{"id": "1608.03665", "review": {"conference": "NIPS", "VERSION": "v1", "DATE_OF_SUBMISSION": "12-Aug-2016", "title": "Learning Structured Sparsity in Deep Neural Networks", "abstract": "High demand for computation resources severely hinders deployment of large-scale Deep Neural Networks (DNN) in resource constrained devices. In this work, we propose a Structured Sparsity Learning (SSL) method to regularize the structures (i.e., filters, channels, filter shapes, and layer depth) of DNNs. SSL can: (1) learn a compact structure from a bigger DNN to reduce computation cost; (2) obtain a hardware-friendly structured sparsity of DNN to efficiently accelerate the DNNs evaluation. Experimental results show that SSL achieves on average 5.1x and 3.1x speedups of convolutional layer computation of AlexNet against CPU and GPU, respectively, with off-the-shelf libraries. These speedups are about twice speedups of non-structured sparsity; (3) regularize the DNN structure to improve classification accuracy. The results show that for CIFAR-10, regularization on layer depth can reduce 20 layers of a Deep Residual Network (ResNet) to 18 layers while improve the accuracy from 91.25% to 92.60%, which is still slightly higher than that of original ResNet with 32 layers. For AlexNet, structure regularization by SSL also reduces the error by around ~1%.", "histories": [["v1", "Fri, 12 Aug 2016 03:20:43 GMT  (2809kb,D)", "http://arxiv.org/abs/1608.03665v1", "10 pages, 7 figures, 4 tables"], ["v2", "Mon, 15 Aug 2016 20:46:15 GMT  (2809kb,D)", "http://arxiv.org/abs/1608.03665v2", "10 pages, 7 figures, 4 tables in NIPS 2016"], ["v3", "Fri, 19 Aug 2016 05:58:48 GMT  (2809kb,D)", "http://arxiv.org/abs/1608.03665v3", "10 pages, 7 figures, 4 tables in NIPS 2016"], ["v4", "Tue, 18 Oct 2016 04:03:41 GMT  (2811kb,D)", "http://arxiv.org/abs/1608.03665v4", "Accepted by NIPS 2016"]], "COMMENTS": "10 pages, 7 figures, 4 tables", "reviews": [], "SUBJECTS": "cs.NE cs.LG stat.ML", "authors": ["wei wen", "chunpeng wu", "yandan wang", "yiran chen", "hai li 0001"], "accepted": true, "id": "1608.03665"}, "pdf": {"name": "1608.03665.pdf", "metadata": {"source": "CRF", "title": "Learning Structured Sparsity in Deep Neural Networks", "authors": ["Wei Wen", "Chunpeng Wu", "Yandan Wang"], "emails": ["wew57@pitt.edu", "chw127@pitt.edu", "yaw46@pitt.edu", "yic52@pitt.edu", "hal66@pitt.edu"], "sections": [{"heading": "1 Introduction", "text": "In fact, the fact is that most of them are able to move, to move and to move."}, {"heading": "2 Related works", "text": "This year it has come to the point where it will be able to put itself at the top, \"he said in an interview with the German Press Agency.\" We have to keep ourselves at the top, \"he said.\" We are able to put ourselves at the top, \"he said."}, {"heading": "3 Structured Sparsity Learning Method for DNNs", "text": "We mainly focus on Structured Sparsity Learning (SSL) on sinuous layers to regulate the structure of DNNs. We first propose a generic method for regulating DNN structures in Section 3.1 and then specify the method for the structures of filters, channels, filter shapes and depths in Section 3.2. Variants of formulations are also discussed from the point of view of computing efficiency in Section 3.3."}, {"heading": "3.1 Proposed structured sparsity learning for generic structures", "text": "Suppose the weights of the coil layers in DNN form a sequence of 4-D tensors W (l), RNl \u00b7 Cl \u00b7 Ml \u00b7 Kl, where Nl, Cl, Ml and Kl are the dimensions of the l-th (1 \u2264 l \u2264 L) weight tensor along the axes of filter, channel, volume height and spatial width. L denotes the number of coil layers. (1) Here W represents the collection of all weights in DNN; ED (W) is the loss of data; R (\u00b7) is unstructured regulation applied to each weight, e.g. \"2-norm; and Rg (\u00b7) is the structuring of weights in DNN; ED (W) is the loss of data; R (\u00b7) is the unstructured regulation applied to each weight."}, {"heading": "3.2 Structured sparsity learning for structures of filters, channels, filter shapes and depth", "text": "We examine and formulate the filer-wise, channel-wise, shape-wise and depth-wise structured sparseness in Figure 2. For the sake of simplicity, the R (\u00b7) term of Eq. (1) is omitted in the following formulation expressions. Suppose W (l) nl,:, is the nl-th filter and W (l):, is the cl-th filter of all filters in the l-th layer. The optimization goal of the filter-wise and channel-wise structured sparseness can be defined asE (W) = ED (W) + L-filter (l) nl,: is the cl-th filter of all filters in the l-th layer. The optimization goal of the filter-wise and channel-wise structured sparseness can be defined."}, {"heading": "3.3 Structured sparsity learning for computationally efficient structures", "text": "All the proposed schemes in Section 3.2 can learn a compact DNN to reduce costs in the calculation. In addition, some variants of the formulation of these schemes can be directly applied in structures that can be efficiently computerized. 3D convolution in DNNs is essentially a composition of 2D convolutions. In order to perform an efficient convolution, we have investigated a fine-grained variant of filter-like spareness, namely 2D filter-wise spareness, in order to spatially enforce the group lasso on each 2D filter of W (l) nl,:,:::. The convolution saved is proportional to the percentage of removed 2D filters. The fine-grain version of filter-wise spareness can more efficiently reduce the compilation associated with the conversion: Since the group sizes are much smaller and thus the weight correction gradients are similar, it helps the Lasso group to quickly achieve a high ratio of zero-groups."}, {"heading": "4 Experiments", "text": "In fact, most of us are able to set out in search of new ways to enrich the world. (...) Most of them are able to understand the world. (...) Most of them are able to understand the world. (...) Most of them are able to understand the world. (...) Most of them are not able to understand the world. (...) Most of them are able to understand the world. (...) Most of them are able to understand the world. (...) Most of them are not able to understand the world. (...) Most of them are not able to understand the world. (...) Most of them are able to understand the world. (...) Most of them are able to understand the world. (...)"}, {"heading": "5 Conclusion", "text": "In this paper, we have proposed a Structured Sparsity Learning (SSL) method to regulate filter, channel, filter shape and depth structures in deep neural networks (DNN), which may force the DNN to learn more compact structures dynamically and without loss of accuracy, and the structured compactness of the DNN achieves significant acceleration in DNN evaluation on both the CPU and the GPU using standard libraries, and a variant of SSL can be used as structure regulation to improve the classification accuracy of state-of-the-art DNNs."}], "references": [{"title": "Imagenet classification with deep convolutional neural networks", "author": ["Alex Krizhevsky", "Ilya Sutskever", "Geoffrey E. Hinton"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2012}, {"title": "Rich feature hierarchies for accurate object detection and semantic segmentation", "author": ["Ross Girshick", "Jeff Donahue", "Trevor Darrell", "Jitendra Malik"], "venue": "In The IEEE Conference on Computer Vision and Pattern Recognition (CVPR),", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2014}, {"title": "Very deep convolutional networks for large-scale image recognition", "author": ["Karen Simonyan", "Andrew Zisserman"], "venue": "arXiv preprint arXiv:1409.1556,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2014}, {"title": "Going deeper with convolutions", "author": ["Christian Szegedy", "Wei Liu", "Yangqing Jia", "Pierre Sermanet", "Scott Reed", "Dragomir Anguelov", "Dumitru Erhan", "Vincent Vanhoucke", "Andrew Rabinovich"], "venue": "arXiv preprint arXiv:1409.4842,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2015}, {"title": "Deep residual learning for image recognition", "author": ["Kaiming He", "Xiangyu Zhang", "Shaoqing Ren", "Jian Sun"], "venue": "arXiv preprint arXiv:1512.03385,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2015}, {"title": "Sparse convolutional neural networks", "author": ["Baoyuan Liu", "Min Wang", "Hassan Foroosh", "Marshall Tappen", "Marianna Pensky"], "venue": "In The IEEE Conference on Computer Vision and Pattern Recognition (CVPR),", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2015}, {"title": "Learning both weights and connections for efficient neural network", "author": ["Song Han", "Jeff Pool", "John Tran", "William Dally"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2015}, {"title": "Deep compression: Compressing deep neural network with pruning, trained quantization and huffman coding", "author": ["Song Han", "Huizi Mao", "William J. Dally"], "venue": "arXiv preprint arXiv:1510.00149,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2015}, {"title": "Predicting parameters in deep learning", "author": ["Misha Denil", "Babak Shakibi", "Laurent Dinh", "Marc' Aurelio Ranzato", "Nando de Freitas"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2013}, {"title": "Exploiting linear structure within convolutional networks for efficient evaluation", "author": ["Emily L Denton", "Wojciech Zaremba", "Joan Bruna", "Yann LeCun", "Rob Fergus"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2014}, {"title": "Speeding up convolutional neural networks with low rank expansions", "author": ["Max Jaderberg", "Andrea Vedaldi", "Andrew Zisserman"], "venue": "arXiv preprint arXiv:1405.3866,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2014}, {"title": "Training cnns with low-rank filters for efficient image classification", "author": ["Yani Ioannou", "Duncan P. Robertson", "Jamie Shotton", "Roberto Cipolla", "Antonio Criminisi"], "venue": "arXiv preprint arXiv:1511.06744,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2015}, {"title": "Convolutional neural networks with low-rank regularization", "author": ["Cheng Tai", "Tong Xiao", "Xiaogang Wang", "Weinan E"], "venue": "arXiv preprint arXiv:1511.06067,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2015}, {"title": "Model selection and estimation in regression with grouped variables", "author": ["Ming Yuan", "Yi Lin"], "venue": "Journal of the Royal Statistical Society. Series B (Statistical Methodology),", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2006}, {"title": "Tree-guided group lasso for multi-task regression with structured sparsity", "author": ["Seyoung Kim", "Eric P Xing"], "venue": "In Proceedings of the 27th International Conference on Machine Learning,", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2010}, {"title": "Learning the structure of deep convolutional networks", "author": ["Jiashi Feng", "Trevor Darrell"], "venue": "In The IEEE International Conference on Computer Vision (ICCV),", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2015}, {"title": "cudnn: Efficient primitives for deep learning", "author": ["Sharan Chetlur", "Cliff Woolley", "Philippe Vandermersch", "Jonathan Cohen", "John Tran", "Bryan Catanzaro", "Evan Shelhamer"], "venue": "arXiv preprint arXiv:1410.0759,", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2014}, {"title": "Caffe: Convolutional architecture for fast feature embedding", "author": ["Yangqing Jia", "Evan Shelhamer", "Jeff Donahue", "Sergey Karayev", "Jonathan Long", "Ross Girshick", "Sergio Guadarrama", "Trevor Darrell"], "venue": "arXiv preprint arXiv:1408.5093,", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2014}, {"title": "Gradient-based learning applied to document recognition", "author": ["Yann LeCun", "L\u00e9on Bottou", "Yoshua Bengio", "Patrick Haffner"], "venue": "Proceedings of the IEEE,", "citeRegEx": "20", "shortCiteRegEx": "20", "year": 1998}, {"title": "Batch normalization: Accelerating deep network training by reducing internal covariate shift", "author": ["Sergey Ioffe", "Christian Szegedy"], "venue": "arXiv preprint arXiv:1502.03167,", "citeRegEx": "21", "shortCiteRegEx": "21", "year": 2015}], "referenceMentions": [{"referenceID": 0, "context": "Deep neural networks (DNN), especially deep convolutional neural networks (CNN), made remarkable success in visual tasks[1][2][3][4][5] by leveraging large-scale networks learning from a huge volume of data.", "startOffset": 120, "endOffset": 123}, {"referenceID": 1, "context": "Deep neural networks (DNN), especially deep convolutional neural networks (CNN), made remarkable success in visual tasks[1][2][3][4][5] by leveraging large-scale networks learning from a huge volume of data.", "startOffset": 123, "endOffset": 126}, {"referenceID": 2, "context": "Deep neural networks (DNN), especially deep convolutional neural networks (CNN), made remarkable success in visual tasks[1][2][3][4][5] by leveraging large-scale networks learning from a huge volume of data.", "startOffset": 126, "endOffset": 129}, {"referenceID": 3, "context": "Deep neural networks (DNN), especially deep convolutional neural networks (CNN), made remarkable success in visual tasks[1][2][3][4][5] by leveraging large-scale networks learning from a huge volume of data.", "startOffset": 129, "endOffset": 132}, {"referenceID": 4, "context": "Deep neural networks (DNN), especially deep convolutional neural networks (CNN), made remarkable success in visual tasks[1][2][3][4][5] by leveraging large-scale networks learning from a huge volume of data.", "startOffset": 132, "endOffset": 135}, {"referenceID": 5, "context": "To reduce computation cost, many studies are performed to compress the scale of DNN, including sparsity regularization[6], connection pruning[7][8] and low rank approximation [9][10][11][12][13].", "startOffset": 118, "endOffset": 121}, {"referenceID": 6, "context": "To reduce computation cost, many studies are performed to compress the scale of DNN, including sparsity regularization[6], connection pruning[7][8] and low rank approximation [9][10][11][12][13].", "startOffset": 141, "endOffset": 144}, {"referenceID": 7, "context": "To reduce computation cost, many studies are performed to compress the scale of DNN, including sparsity regularization[6], connection pruning[7][8] and low rank approximation [9][10][11][12][13].", "startOffset": 144, "endOffset": 147}, {"referenceID": 8, "context": "To reduce computation cost, many studies are performed to compress the scale of DNN, including sparsity regularization[6], connection pruning[7][8] and low rank approximation [9][10][11][12][13].", "startOffset": 175, "endOffset": 178}, {"referenceID": 9, "context": "To reduce computation cost, many studies are performed to compress the scale of DNN, including sparsity regularization[6], connection pruning[7][8] and low rank approximation [9][10][11][12][13].", "startOffset": 178, "endOffset": 182}, {"referenceID": 10, "context": "To reduce computation cost, many studies are performed to compress the scale of DNN, including sparsity regularization[6], connection pruning[7][8] and low rank approximation [9][10][11][12][13].", "startOffset": 182, "endOffset": 186}, {"referenceID": 11, "context": "To reduce computation cost, many studies are performed to compress the scale of DNN, including sparsity regularization[6], connection pruning[7][8] and low rank approximation [9][10][11][12][13].", "startOffset": 186, "endOffset": 190}, {"referenceID": 12, "context": "To reduce computation cost, many studies are performed to compress the scale of DNN, including sparsity regularization[6], connection pruning[7][8] and low rank approximation [9][10][11][12][13].", "startOffset": 190, "endOffset": 194}, {"referenceID": 10, "context": "Inspired by the facts that (1) there is redundancy across filters and channels [11]; (2) shapes of filters are usually fixed as cuboid but enabling arbitrary shapes can potentially eliminate unnecessary computation imposed by this fixation; and (3) depth of the network is critical for classification but deeper layers cannot always guarantee a lower error because of the exploding gradients and degradation problem [5], we propose Structured Sparsity Learning (SSL) method to directly learn a compressed structure of deep CNNs by group Lasso regularization during the training.", "startOffset": 79, "endOffset": 83}, {"referenceID": 4, "context": "Inspired by the facts that (1) there is redundancy across filters and channels [11]; (2) shapes of filters are usually fixed as cuboid but enabling arbitrary shapes can potentially eliminate unnecessary computation imposed by this fixation; and (3) depth of the network is critical for classification but deeper layers cannot always guarantee a lower error because of the exploding gradients and degradation problem [5], we propose Structured Sparsity Learning (SSL) method to directly learn a compressed structure of deep CNNs by group Lasso regularization during the training.", "startOffset": 416, "endOffset": 419}, {"referenceID": 6, "context": "[7][8] reduced number of parameters of AlexNet by 9\u00d7 and VGG-16 by 13\u00d7 using connection pruning.", "startOffset": 0, "endOffset": 3}, {"referenceID": 7, "context": "[7][8] reduced number of parameters of AlexNet by 9\u00d7 and VGG-16 by 13\u00d7 using connection pruning.", "startOffset": 3, "endOffset": 6}, {"referenceID": 4, "context": "99% parameters of ResNet-152 in [5] are from fully-connected layers, compression and acceleration on convolutional layers become essential.", "startOffset": 32, "endOffset": 35}, {"referenceID": 5, "context": "[6] achieved >90% sparsity of convolutional layers in AlexNet with 2% accuracy loss, and bypassed the issue shown in Figure 1 by hardcoding the sparse weights into program, achieving layer-wise 4.", "startOffset": 0, "endOffset": 3}, {"referenceID": 8, "context": "[9] predicted 95% parameters in a DNN by exploiting the redundancy across filters and channels.", "startOffset": 0, "endOffset": 3}, {"referenceID": 10, "context": "[11] achieved 4.", "startOffset": 0, "endOffset": 4}, {"referenceID": 9, "context": "[10] achieved 2\u00d7 speedups on both CPUs and GPUs for the first two layers.", "startOffset": 0, "endOffset": 4}, {"referenceID": 12, "context": "[13][12] improved and extended LRA to larger DNNs.", "startOffset": 0, "endOffset": 4}, {"referenceID": 11, "context": "[13][12] improved and extended LRA to larger DNNs.", "startOffset": 4, "endOffset": 8}, {"referenceID": 9, "context": "As number of hyper-parameters in LRA method increases linearly with layer depth [10][13], the search space increases linearly or even polynomially for very deep DNNs.", "startOffset": 80, "endOffset": 84}, {"referenceID": 12, "context": "As number of hyper-parameters in LRA method increases linearly with layer depth [10][13], the search space increases linearly or even polynomially for very deep DNNs.", "startOffset": 84, "endOffset": 88}, {"referenceID": 13, "context": "Group Lasso [14] is an efficient regularization to learn sparse structures.", "startOffset": 12, "endOffset": 16}, {"referenceID": 14, "context": "[15] used group Lasso to regularize the structure of correlation tree for multi-task regression problem and reduced prediction errors.", "startOffset": 0, "endOffset": 4}, {"referenceID": 5, "context": "[6] utilized group Lasso to constrain the scale of the structure of LRA.", "startOffset": 0, "endOffset": 3}, {"referenceID": 15, "context": "[16] learned", "startOffset": 0, "endOffset": 4}, {"referenceID": 13, "context": "Because Group Lasso can effectively zero out all weights in some groups [14][15], we adopt it in our SSL.", "startOffset": 72, "endOffset": 76}, {"referenceID": 14, "context": "Because Group Lasso can effectively zero out all weights in some groups [14][15], we adopt it in our SSL.", "startOffset": 76, "endOffset": 80}, {"referenceID": 4, "context": "Inspired by the structure of highway networks [17] and deep residual networks [5], we propose to leverage the shortcuts across layers to solve this issue.", "startOffset": 78, "endOffset": 81}, {"referenceID": 16, "context": "Convolutional computation in DNNs is commonly converted to modality of GEneral Matrix Multiplication (GEMM) by lowering weight tensors and feature tensors to matrices [18].", "startOffset": 167, "endOffset": 171}, {"referenceID": 17, "context": "For example, in Caffe [19], a 3D filter W (l) nl,:,:,: is reshaped to a row in the weight matrix where each column is the collection of weights W (l) :,cl,ml,kl related to shape-wise sparsity.", "startOffset": 22, "endOffset": 26}, {"referenceID": 18, "context": "In the experiment of MNIST, we examined the effectiveness of SSL in two types of networks: LeNet [20] implemented by Caffe and a multilayer perceptron (MLP) network.", "startOffset": 97, "endOffset": 101}, {"referenceID": 0, "context": "We implemented the ConvNet of [1] and deep residual networks (ResNet) [5] on CIFAR-10.", "startOffset": 30, "endOffset": 33}, {"referenceID": 4, "context": "We implemented the ConvNet of [1] and deep residual networks (ResNet) [5] on CIFAR-10.", "startOffset": 70, "endOffset": 73}, {"referenceID": 0, "context": "[1] as the baseline and implement it using Caffe.", "startOffset": 0, "endOffset": 3}, {"referenceID": 4, "context": "ResNet: To investigate the necessary depth of DNNs required by SSL, we use a 20-layer deep residual networks (ResNet-20) proposed in [5] as the baseline.", "startOffset": 133, "endOffset": 136}, {"referenceID": 19, "context": "Batch normalization [21] is adopted after convolution and before activation.", "startOffset": 20, "endOffset": 24}, {"referenceID": 4, "context": "We use the same data augmentation and training hyper-parameters as that in [5].", "startOffset": 75, "endOffset": 78}, {"referenceID": 4, "context": "ResNet-# is the original ResNet in [5] with # layers.", "startOffset": 35, "endOffset": 38}, {"referenceID": 4, "context": "Compared with original ResNet in [5], SSL learns a ResNet with 14 layers (SSLResNet-14) that reaching a lower error than the one of the baseline with 20 layers (ResNet-20); SSL-ResNet-18 and ResNet-32 achieve an error of 7.", "startOffset": 33, "endOffset": 36}, {"referenceID": 17, "context": "CaffeNet [19] \u2013 the replication of AlexNet [1] with mirror changes, is used in our experiment.", "startOffset": 9, "endOffset": 13}, {"referenceID": 0, "context": "CaffeNet [19] \u2013 the replication of AlexNet [1] with mirror changes, is used in our experiment.", "startOffset": 43, "endOffset": 46}, {"referenceID": 6, "context": "`1-norm is selected as the representative of non-structured sparsity method instead of connection pruning in [7] because `1-norm get a higher sparsity on convolutional layers as the results of AlexNet 3 and AlexNet 4 depicted in Table 4.", "startOffset": 109, "endOffset": 112}, {"referenceID": 5, "context": "We note that at the same accuracy, our average speedup is indeed higher than that of [6] which adopts heavy hardware customization to overcome the negative impact of non-structured sparsity.", "startOffset": 85, "endOffset": 88}, {"referenceID": 6, "context": "3 pruning[7] 42.", "startOffset": 9, "endOffset": 12}], "year": 2016, "abstractText": "High demand for computation resources severely hinders deployment of large-scale Deep Neural Networks (DNN) in resource constrained devices. In this work, we propose a Structured Sparsity Learning (SSL) method to regularize the structures (i.e., filters, channels, filter shapes, and layer depth) of DNNs. SSL can: (1) learn a compact structure from a bigger DNN to reduce computation cost; (2) obtain a hardware-friendly structured sparsity of DNN to efficiently accelerate the DNN\u2019s evaluation. Experimental results show that SSL achieves on average 5.1\u00d7 and 3.1\u00d7 speedups of convolutional layer computation of AlexNet against CPU and GPU, respectively, with off-the-shelf libraries. These speedups are about twice speedups of non-structured sparsity; (3) regularize the DNN structure to improve classification accuracy. The results show that for CIFAR-10, regularization on layer depth can reduce 20 layers of a Deep Residual Network (ResNet) to 18 layers while improve the accuracy from 91.25% to 92.60%, which is still slightly higher than that of original ResNet with 32 layers. For AlexNet, structure regularization by SSL also reduces the error by \u223c 1%.", "creator": "LaTeX with hyperref package"}}}