{"id": "1608.04147", "review": {"conference": "EMNLP", "VERSION": "v1", "DATE_OF_SUBMISSION": "14-Aug-2016", "title": "Numerically Grounded Language Models for Semantic Error Correction", "abstract": "Semantic error detection and correction is an important task for applications such as fact checking, speech-to-text or grammatical error correction. Current approaches generally focus on relatively shallow semantics and do not account for numeric quantities. Our approach uses language models grounded in numbers within the text. Such groundings are easily achieved for recurrent neural language model architectures, which can be further conditioned on incomplete background knowledge bases. Our evaluation on clinical reports shows that numerical grounding improves perplexity by 33% and F1 for semantic error correction by 5 points when compared to ungrounded approaches. Conditioning on a knowledge base yields further improvements.", "histories": [["v1", "Sun, 14 Aug 2016 22:34:22 GMT  (256kb,D)", "http://arxiv.org/abs/1608.04147v1", "accepted to EMNLP 2016"]], "COMMENTS": "accepted to EMNLP 2016", "reviews": [], "SUBJECTS": "cs.CL cs.NE", "authors": ["georgios p spithourakis", "isabelle augenstein", "sebastian riedel"], "accepted": true, "id": "1608.04147"}, "pdf": {"name": "1608.04147.pdf", "metadata": {"source": "CRF", "title": "Numerically Grounded Language Models for Semantic Error Correction", "authors": ["Georgios P. Spithourakis"], "emails": ["s.riedel}@cs.ucl.ac.uk"], "sections": [{"heading": "1 Introduction", "text": "In many real-world scenarios, it is important to identify and potentially correct semantic errors and inconsistencies in the text, for example, when clinicians write reports, some of the statements in the text may be inconsistent with the patient's measurements (Bowman, 2013). Error rates in clinical data range from 2.3% to 26.9% (Goldberg et al., 2008), and many of them are number-based errors (Arts et al., 2008). Similarly, a blogger can make statistical claims that contradict facts recorded in databases (Munger, 2008). Numerical concepts represent 29% of the contradictions in Wikipedia and GoogleNews (De Marneffe et al, 2008) and 8.8% of the contradictory pairs in the originating datasets (Dagan et al., 2006). These inconsistencies may result from clarity, lack of reporting guidelines or negligence."}, {"heading": "2 Methodology", "text": "Our approach to semantic error correction (Figure 1) begins with the formation of a language model (LM) that can be based on numerical quantities mentioned inline with text (Figure 2.1) and / or on a potentially incomplete KB (Figure 2.2). When a document is created for semantic verification, a hypothesis generator suggests corrections that are then evaluated using the trained language model (Figure 2.3). A final decision step is to accept the best scoring hypothesis."}, {"heading": "2.1 Numerically grounded language modelling", "text": "Let us designate {w1,..., wT} a document in which wt is the unified representation of the t-th character and V is the word size. A neural LM uses a matrix, one-RD-V, to derive word embedding, ewt = wt. A hidden state from the previous time step, ht-1, and the current word embedding, ewt, are fed sequentially to the repetition function of an RNN to generate the current hidden state, ht-RD. The conditional probability of the next word is estimated as softmax (Eoutht), where Eout-RV-D is an output embedding matrix. We suggest that a representation, a concatenation, of the numerical value of wt with the input of the RNN repetition function in each time step. Through this numerical representation, the model can link the real numbers as a real value, but as a repetitive function of the numbers (where the repetitive properties are retrieved)."}, {"heading": "2.2 Conditioning on incomplete KBs", "text": "The proposed extension can also be used in conditional language modeling of documents with a knowledge base. Let's look at a set of KB tuples that accompany each document and describe its attributes in the form < Attribute, Value >, with attributes defined by a KB scheme. We can lexicalize the KB by converting its tuples into textual statements in the form \"Attribute: Value.\" An example of how we lexicalize the KB shows Figure 2. The generated tokens can then be interpreted for their word embedding and numerical representations. This approach can flexibly include KB tuples even if values of some attributes are missing."}, {"heading": "2.3 Semantic error correction", "text": "A statistical model selects the most likely correction from a number of possible correction decisions. If the model receives a corrected hypothesis higher than the original document, the correction is accepted.A hypotheses generator function, G, takes the original document, H0, as input and generates a set of corrected candidates G (H0) = {H1,..., HM}.A simple hypotheses generator uses confusion sets of semantically related words to make all possible substitutions.A goal scoring model, s, assigns a score s (Hi) or R to a hypothesis Hi. The score is based on a probability quotient test between the original document (null hypothesis, H0) and each candidate a correction (alternative hypotheses, Hi), i.e. s (Hi) = p (Hi) = p (Hi) p (H0) p (H0) p (H0).The apparent correction is as much more likely as the original documentation."}, {"heading": "3 Data", "text": "Our data set includes 16,003 clinical records from the London Chest Hospital (Table 1), some of which are also included in the report. On average, 7.7 tuples are completed per record. Numerical tokens make up only a small proportion of each set (4.3%), but make up a large part of the unique token vocabulary (> 40%) and have high OOV values. To evaluate the SEC, we generate a \"corrupt\" set of semantic errors from the test part of the \"trusted\" data set (Table 1, last column). We manually create confusion records (Table 2) by searching the development set for words related to numerical quantities and grouping them when they occur in similar contexts. We then generate an erroneous document for each document in the trusted data set by creating a confusion (Table 2) by searching the terms associated with numerical quantities and not correctly replacing the resulting documents."}, {"heading": "4 Results and discussion", "text": "Our base LM is a single-layer long-term memory network (LSTM, Hochreiter and Schmidhuber (1997) with all latent dimensions (internal matrices, input and output embedding), which is set to D = 50. We extend this baseline to a conditional variant by conditioning the lexicalized KB (see Section 2.2). We also derive a numerically grounded model by linking the numerical representation of each token to the inputs of the base LM model (see Section 2.1). Finally, we consider a model that is both grounded and conditioned (g-conditioned). The vocabulary contains the V = 1000 most common tokens in the training set. Out-of-vocabulary tokens are replaced by < num unk > if numerical, and, otherwise. We extract the numerical representations before masking so that the grounded models can form a generative basis."}, {"heading": "4.1 Experiment 1: Numerically grounded LM", "text": "We report on helplessness and adapted helplessness (Ueberla, 1994) of our LMs on the test set for all tokens and token classes (Table 3). Adapted helplessness is not sensitive to OOV rates and therefore allows meaningful comparisons between token classes. In the case of numerical tokens, helplessness is high because they make up a large portion of the vocabulary. Grounded and g-conditioned models achieved a 33.3% and 36.9% improvement in helplessness, respectively, compared to the base LM model. Conditioning without grounding only leads to minor improvements, since most numerical values from the lexicalized KB are not vocabulary. The qualitative example in Figure 3 shows how numerical values influence the probability of tokens based on their history."}, {"heading": "4.2 Experiment 2: Semantic error correction", "text": "We evaluate the SEC systems on the basis of the corrupt data set (section 3) in terms of detection and correction. For proof, we report on precision, recall and F1 values in Table 4. Our G-conditioned model achieves the best results, an overall improvement of Formula 1 by 2 points compared to the base LM model and 7 points compared to the best baseline. The conditional model without grounding performs slightly worse in Formula 1 metrics than the base LM. Note that the random baseline behaves more similar in more hypotheses than always. Our hypotheses generator generates an average of 12 hypotheses per document. The results are never zero because it does not detect errors. To correct, we give the formula MAP the mean accuracy (MAP) in addition to the same formula F1 metrics as in detection (Table 5). The former measures the position of the ranking of the correct hypothesis. The always (never) correct hypothesis never delivers the best MAP points in the top of the M47 baseline, while the MAP never delivers the best results in the M5 without improving the conditional M5 in the M5."}, {"heading": "5 Related Work", "text": "Previous work has based language on sight (Bruni et al., 2014; Socher et al., 2014; Silberer et al., 2014), audio (Kiela and Clark, 2015), video (Fleischman and Roy, 2008), color (McMahan and Stone, 2015), and olfactory perception (Kiela et al., 2015). However, no previous approach has explored inline error numbers as a grounding source. However, our approach to language modeling at SEC is inspired by LM approaches to grammatical error recognition (GEC) (Ng et al., 2013; Felice et al., 2014). Similarly, they derive confusion sentences from semantically related words, replace the target words with alternatives and evaluate them with a LM. Existing semantic error correction approaches do not aim at correcting word error decisions (Dahlmeier and Ng, 2011), nor at collocation errors (Kochmar, 2016), and SEK-Alimantic paragraphs (SEK)."}, {"heading": "6 Conclusion", "text": "We found that the proposed techniques lead to performance improvements in speech modeling tasks, as well as semantic error detection and correction. Numerically grounded models allow us to capture semantic dependencies of words from numbers. In future work, we plan to apply numerically grounded models to other tasks, such as numerical error correction. We will explore alternative ways to derive numerical representations, such as taking verbal number descriptions into account. For the SEC, a traceable hypotheses generator can potentially improve the reach of the system."}, {"heading": "Acknowledgments", "text": "The authors thank the anonymous reviewers for their insightful comments. We also thank Steffen Petersen for providing the dataset and advising on the clinical aspects of this work. This research was supported by the Farr Institute of Health Informatics Research."}], "references": [{"title": "Defining and improving data quality in medical registries: a literature review, case study, and generic framework", "author": ["Nicolette F De Keizer", "Gert-Jan Scheffer"], "venue": "Journal of the American Medical Informatics Association,", "citeRegEx": "Arts et al\\.,? \\Q2002\\E", "shortCiteRegEx": "Arts et al\\.", "year": 2002}, {"title": "Impact of Electronic Health Record Systems on Information Integrity: Quality and Safety Implications", "author": ["Sue Bowman"], "venue": "Perspectives in Health Information Management,", "citeRegEx": "Bowman.,? \\Q2013\\E", "shortCiteRegEx": "Bowman.", "year": 2013}, {"title": "Multimodal distributional semantics", "author": ["Bruni et al.2014] Elia Bruni", "Nam-Khanh Tran", "Marco Baroni"], "venue": "J. Artif. Intell. Res.(JAIR),", "citeRegEx": "Bruni et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Bruni et al\\.", "year": 2014}, {"title": "The pascal recognising textual entailment challenge. In Machine learning challenges. evaluating predictive uncertainty, visual object classification, and recognising tectual entailment", "author": ["Dagan et al.2006] Ido Dagan", "Oren Glickman", "Bernardo Magnini"], "venue": null, "citeRegEx": "Dagan et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Dagan et al\\.", "year": 2006}, {"title": "Correcting Semantic Collocation Errors with L1-induced Paraphrases", "author": ["Dahlmeier", "Ng2011] Daniel Dahlmeier", "Hwee Tou Ng"], "venue": "In Proceedings of EMNLP,", "citeRegEx": "Dahlmeier et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Dahlmeier et al\\.", "year": 2011}, {"title": "Finding Contradictions in Text", "author": ["Anna N Rafferty", "Christopher D Manning"], "venue": "In ACL,", "citeRegEx": "Marneffe et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Marneffe et al\\.", "year": 2008}, {"title": "Grammatical error correction using hybrid systems and type filtering", "author": ["Zheng Yuan", "\u00d8istein E Andersen", "Helen Yannakoudakis", "Ekaterina Kochmar"], "venue": "In CoNLL Shared Task,", "citeRegEx": "Felice et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Felice et al\\.", "year": 2014}, {"title": "Grounded Language Modeling for Automatic Speech Recognition of Sports Video", "author": ["Fleischman", "Roy2008] Michael Fleischman", "Deb Roy"], "venue": "In Proceedings of ACL,", "citeRegEx": "Fleischman et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Fleischman et al\\.", "year": 2008}, {"title": "Analysis of data errors in clinical research databases", "author": ["Andrzej Niemierko", "Alexander Turchin"], "venue": "In AMIA. Citeseer", "citeRegEx": "Goldberg et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Goldberg et al\\.", "year": 2008}, {"title": "Long short-term memory", "author": ["Hochreiter", "Schmidhuber1997] Sepp Hochreiter", "J\u00fcrgen Schmidhuber"], "venue": "Neural computation,", "citeRegEx": "Hochreiter et al\\.,? \\Q1997\\E", "shortCiteRegEx": "Hochreiter et al\\.", "year": 1997}, {"title": "Multi- and Cross-Modal Semantics Beyond Vision: Grounding in Auditory Perception", "author": ["Kiela", "Clark2015] Douwe Kiela", "Stephen Clark"], "venue": "In Proceedings of EMNLP,", "citeRegEx": "Kiela et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Kiela et al\\.", "year": 2015}, {"title": "Grounding Semantics in Olfactory Perception", "author": ["Kiela et al.2015] Douwe Kiela", "Luana Bulat", "Stephen Clark"], "venue": "In Proceedings of ACL,", "citeRegEx": "Kiela et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Kiela et al\\.", "year": 2015}, {"title": "Error Detection in Content Word Combinations", "author": ["Ekaterina Kochmar"], "venue": "Ph.D. thesis,", "citeRegEx": "Kochmar.,? \\Q2016\\E", "shortCiteRegEx": "Kochmar.", "year": 2016}, {"title": "A bayesian model of grounded color semantics. Transactions of the Association for Computational Linguistics, 3:103\u2013115", "author": ["McMahan", "Stone2015] Brian McMahan", "Matthew Stone"], "venue": null, "citeRegEx": "McMahan et al\\.,? \\Q2015\\E", "shortCiteRegEx": "McMahan et al\\.", "year": 2015}, {"title": "Blogging and political information: truth or truthiness? Public Choice, 134(1-2):125\u2013138", "author": ["Michael C Munger"], "venue": null, "citeRegEx": "Munger.,? \\Q2008\\E", "shortCiteRegEx": "Munger.", "year": 2008}, {"title": "The CoNLL-2013 Shared Task on Grammatical Error Correction", "author": ["Hwee Tou Ng", "Siew Mei Wu", "Yuanbin Wu", "Christian Hadiwinoto", "Joel Tetreault"], "venue": null, "citeRegEx": "Ng et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Ng et al\\.", "year": 2013}, {"title": "Learning Grounded Meaning Representations with Autoencoders", "author": ["Silberer", "Lapata2014] Carina Silberer", "Mirella Lapata"], "venue": "In Proceedings of ACL,", "citeRegEx": "Silberer et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Silberer et al\\.", "year": 2014}, {"title": "Grounded Compositional Semantics for Finding and Describing Images with Sentences", "author": ["Andrej Karpathy", "Quoc V. Le", "Christopher D. Manning", "Andrew Y. Ng"], "venue": null, "citeRegEx": "Socher et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Socher et al\\.", "year": 2014}, {"title": "Analysing a simple language model\u00b7 some general conclusions for language models for speech recognition", "author": ["Joerg Ueberla"], "venue": "Computer Speech & Language,", "citeRegEx": "Ueberla.,? \\Q1994\\E", "shortCiteRegEx": "Ueberla.", "year": 1994}, {"title": "Linear) Maps of the Impossible: Capturing semantic anomalies in distributional space", "author": ["Marco Baroni", "Roberto Zamparelli"], "venue": "In Proceedings of the Workshop on Distributional Semantics and Compositionality,", "citeRegEx": "Vecchi et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Vecchi et al\\.", "year": 2011}, {"title": "ADADELTA: An Adaptive Learning Rate Method. CoRR, abs/1212.5701", "author": ["Matthew D. Zeiler"], "venue": null, "citeRegEx": "Zeiler.,? \\Q2012\\E", "shortCiteRegEx": "Zeiler.", "year": 2012}], "referenceMentions": [{"referenceID": 1, "context": "For example, when clinicians compose reports, some statements in the text may be inconsistent with measurements taken from the patient (Bowman, 2013).", "startOffset": 135, "endOffset": 149}, {"referenceID": 8, "context": "9% (Goldberg et al., 2008) and many of them are number-based errors (Arts et al.", "startOffset": 3, "endOffset": 26}, {"referenceID": 0, "context": ", 2008) and many of them are number-based errors (Arts et al., 2002).", "startOffset": 49, "endOffset": 68}, {"referenceID": 14, "context": "Likewise, a blog writer may make statistical claims that contradict facts recorded in databases (Munger, 2008).", "startOffset": 96, "endOffset": 110}, {"referenceID": 3, "context": "8% of contradictory pairs in entailment datasets (Dagan et al., 2006).", "startOffset": 49, "endOffset": 69}, {"referenceID": 20, "context": "scent (AdaDelta) (Zeiler, 2012).", "startOffset": 17, "endOffset": 31}, {"referenceID": 18, "context": "We report perplexity and adjusted perplexity (Ueberla, 1994) of our LMs on the test set for all tokens and token classes (Table 3).", "startOffset": 45, "endOffset": 60}, {"referenceID": 2, "context": "Previous work grounds language on vision (Bruni et al., 2014; Socher et al., 2014; Silberer and Lapata, 2014), audio (Kiela and Clark, 2015), video (Fleischman and Roy, 2008), colour (McMahan and Stone, 2015), and olfactory perception (Kiela et al.", "startOffset": 41, "endOffset": 109}, {"referenceID": 17, "context": "Previous work grounds language on vision (Bruni et al., 2014; Socher et al., 2014; Silberer and Lapata, 2014), audio (Kiela and Clark, 2015), video (Fleischman and Roy, 2008), colour (McMahan and Stone, 2015), and olfactory perception (Kiela et al.", "startOffset": 41, "endOffset": 109}, {"referenceID": 10, "context": ", 2014; Silberer and Lapata, 2014), audio (Kiela and Clark, 2015), video (Fleischman and Roy, 2008), colour (McMahan and Stone, 2015), and olfactory perception (Kiela et al., 2015).", "startOffset": 160, "endOffset": 180}, {"referenceID": 15, "context": "Our language modelling approach to SEC is inspired by LM approaches to grammatical error detection (GEC) (Ng et al., 2013; Felice et al., 2014).", "startOffset": 105, "endOffset": 143}, {"referenceID": 6, "context": "Our language modelling approach to SEC is inspired by LM approaches to grammatical error detection (GEC) (Ng et al., 2013; Felice et al., 2014).", "startOffset": 105, "endOffset": 143}, {"referenceID": 12, "context": "Existing semantic error correction approaches aim at correcting word error choices (Dahlmeier and Ng, 2011), collocation errors (Kochmar, 2016), and semantic anomalies in adjective-noun combinations (Vecchi et al.", "startOffset": 128, "endOffset": 143}, {"referenceID": 19, "context": "Existing semantic error correction approaches aim at correcting word error choices (Dahlmeier and Ng, 2011), collocation errors (Kochmar, 2016), and semantic anomalies in adjective-noun combinations (Vecchi et al., 2011).", "startOffset": 199, "endOffset": 220}], "year": 2016, "abstractText": "Semantic error detection and correction is an important task for applications such as fact checking, speech-to-text or grammatical error correction. Current approaches generally focus on relatively shallow semantics and do not account for numeric quantities. Our approach uses language models grounded in numbers within the text. Such groundings are easily achieved for recurrent neural language model architectures, which can be further conditioned on incomplete background knowledge bases. Our evaluation on clinical reports shows that numerical grounding improves perplexity by 33% and F1 for semantic error correction by 5 points when compared to ungrounded approaches. Conditioning on a knowledge base yields further improvements.", "creator": "LaTeX with hyperref package"}}}