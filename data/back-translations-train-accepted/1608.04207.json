{"id": "1608.04207", "review": {"conference": "iclr", "VERSION": "v1", "DATE_OF_SUBMISSION": "15-Aug-2016", "title": "Fine-grained Analysis of Sentence Embeddings Using Auxiliary Prediction Tasks", "abstract": "There is a lot of research interest in encoding variable length sentences into fixed length vectors, in a way that preserves the sentence meanings. Two common methods include representations based on averaging word vectors, and representations based on the hidden states of recurrent neural networks such as LSTMs. The sentence vectors are used as features for subsequent machine learning tasks or for pre-training in the context of deep learning. However, not much is known about the properties that are encoded in these sentence representations and about the language information they capture. We propose a framework that facilitates better understanding of the encoded representations. We define prediction tasks around isolated aspects of sentence structure (namely sentence length, word content, and word order), and score representations by the ability to train a classifier to solve each prediction task when using the representation as input. We demonstrate the potential contribution of the approach by analyzing different sentence representation mechanisms. The analysis sheds light on the relative strengths of different sentence embedding methods with respect to these low level prediction tasks, and on the effect of the encoded vector's dimensionality on the resulting representations.", "histories": [["v1", "Mon, 15 Aug 2016 08:51:38 GMT  (653kb,D)", "http://arxiv.org/abs/1608.04207v1", null], ["v2", "Sun, 11 Sep 2016 13:22:13 GMT  (653kb,D)", "http://arxiv.org/abs/1608.04207v2", null], ["v3", "Thu, 9 Feb 2017 06:58:50 GMT  (225kb,D)", "http://arxiv.org/abs/1608.04207v3", null]], "reviews": [], "SUBJECTS": "cs.CL", "authors": ["yossi adi", "einat kermany", "yonatan belinkov", "ofer lavi", "yoav goldberg"], "accepted": true, "id": "1608.04207"}, "pdf": {"name": "1608.04207.pdf", "metadata": {"source": "CRF", "title": "Fine-grained Analysis of Sentence Embeddings Using Auxiliary Prediction Tasks", "authors": ["Yossi Adi", "Einat Kermany", "Yonatan Belinkov", "Ofer Lavi", "Yoav Goldberg"], "emails": ["oferl}@il.ibm.com", "belinkov@mit.edu", "yossiadidrum}@gmail.com"], "sections": [{"heading": null, "text": "We propose a framework that facilitates a better understanding of the coded representations. We define prediction tasks around isolated aspects of the sentence structure (namely sentence length, word content and word order) and evaluate representations by the ability to train a classifier to solve any prediction task when the representation is used as input. We show the possible contribution of the approach by analyzing different sentence representation mechanisms. The analysis highlights the relative strengths of different sentence embedding methods in relation to these low-threshold prediction tasks and the effect of the dimensionality of the coded vector on the resulting representations."}, {"heading": "1 Introduction", "text": "In fact, most of them are able to set out in search of a solution that originates in the real world."}, {"heading": "2 Related Work", "text": "Word-level distributed representations have been analyzed both empirically and theoretically quite extensively, for example by Baroni et al. (2014), Levy et al. (2014) and Levy et al. (2015). In contrast, the analysis of sentence-level representations is much more limited. A common approach is to compare the performance of sentence embedding on downstream tasks (Hill et al., 2016). While the resulting analysis shows differences in the performance of different models, it does not adequately explain which linguistic properties of the sentence they capture. Other studies analyze the hidden units learned by neural networks in the formation of a sentence representation model (Elman, 1991; Karpathy et al., 2015; Ka'r et al., 2016). This approach often associates certain linguistic aspects with certain hidden units. Ka'da'r et al. (2016) suggest a methodology for quantifying the contribution of each entered word to a resulting sentence."}, {"heading": "3 Approach", "text": "The main idea of our method is to focus on isolated aspects of the sentence structure and to design experiments to measure the extent to which each aspect is captured in a particular representation. In each experiment, we formulate a predictive task. If we apply a method of sentence representation, we generate training data and train a classifier to predict a certain sentence property (e.g. its length) based on its vector representation. Then, we measure how well we can train a model to perform the task. Basically, if we cannot train a classifier to predict a property of a sentence based on its vector representation, that property is not coded in the representation (or rather not coded in a meaningful way, taking into account the likely use of the representation). Experiments in this work focus on low sentence properties - the sentence length, the identities of the words in a sentence, and the order of the words. We consider these core elements of the future sentence structure, which we have in the tactics, and which are great."}, {"heading": "3.1 The Prediction Tasks", "text": "In fact, most of them will be able to feel as if they are able to save themselves."}, {"heading": "4 Sentence Representation Models", "text": "In view of a sentence s = {w1, w2,..., wN}, we aim to find a sentence representation s with the help of an encoder: ENC: s = {w1, w2,..., wN} 7. The encryption process usually assumes a vector representation wi-Rd for each word in the vocabulary. In general, word and sentence embedding dimensions d and k do not have to be the same. Word vectors can be learned or pre-trained together with other encoder parameters. In the following, we describe various instances of ENC.Continuous Bag-of-words (CBOW) This simple but effective text representation consists of executing an elementary averaging of word vectors using a word embedding method such as word2vec.Despite its ignorance of the word sequence, the CBOW has proved useful for various tasks (?) and is easy to calculate, making it an important model class for consideration."}, {"heading": "5 Experimental Setup", "text": "Sentence Encoders The bag-of-words (CBOW) and encoder-decoder models are trained on 1 million sets from a 2012 Wikipedia dump. We use NLTK (Bird, 2006) for tokenization, and console satz length to be between 5 and 70 words.3 We control for the embedding size k and train word vectors of sizes k '100, 300, 500, 750, 1000}, with the encoder-decoder models, we use inhouse implementation using the Gensim implementation. 3 We control for the embedding size k and train word vectors of sizes k' 100, 300, 750, 1000}."}, {"heading": "6 Results", "text": "In this section we provide a detailed description of our experimental results and their analysis. For each of the three main tests - length, content and order - we examine the performance of different sentence representation models across the embed size."}, {"heading": "6.1 Length Experiments", "text": "We start by examining how well the different representations encode the set length. Figure 1 shows the performance of the different models in the length task, as well as the BLEU achieved by the LSTM encoder decoder (ED). 100 300 500 750 1000 representation dimensions 102030405060708090L e ng thp red icti o na c ura c y05101520253035B L E UED CBOW ED BLEUFigure 1: Length accuracy vs. embedding size for different models; ED BLEU values as reference values. With enough dimensions, the LSTM embedding is very good at recording set length, achieving accuracy between 82% and 87%. Length prediction capability does not correlate with BLEU values: from 300 dimensions downwards, the length accuracies of the LSTM are relatively stable, while the BLEU value of the encoder decoder model or model increases, more dimensions are adhered."}, {"heading": "6.2 Word Content Experiments", "text": "To what extent do the different sentence representations encode the identities of the words in the sentence? Figure 2 illustrates the performance of our models on the word content test. 100 300 500 750 1000 representation dimensions 505560657075808590C o nte nt pre dic tio na c ura c y05101520253035B L E UCBOW ED BLEUFigure 2: Content accuracy vs. embedding size for different models; ED BLEU scores given for reference.All the representations encode a certain amount of word information and clearly exceed the random baseline of 50%. Some trends are worth mentioning. While the capacity of the LSTM encoder to maintain word identities generally increases when the dimensions are added, the performance peaks at 750 dimensions and falls thereafter. This is in contrast to the BLEU score of the respective encoder models. We assume that this occurs because a significant portion of the auto encoder performance or the encoder performance."}, {"heading": "6.3 Word Order Experiments", "text": "Figure 4 shows the performance of the different models in the job test. LSTM encoders are very good at encoding the word sequence, with LSTM1000 allowing word sequence to be restored 91% of the time. Similar to the length test, the accuracy of the LSTM job prediction only loosely correlates with the BLEU values. It is noteworthy that the increase in the representation size helps the LSTM encoder to better encode the information about the job sequence.Surprisingly, the CBOW encodings achieve an accuracy of 70% in word sequence task, 20% above the baseline. This is noteworthy because the CBOW encoder by definition does not attempt to preserve the information about the job sequence.One way to compare ex-100 300 500 750 750 1000 representation dimensions 5060708090O rde rp red icti o na c ura c y05101520253035B UECBOD EUW BLEUED with other order size models is to consider the following job order accuracy:"}, {"heading": "7 Importance of \u201cNatural Languageness\u201d", "text": "In fact, the majority of them are able to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to fight, to fight, to fight, to fight, to fight, to move, to move, to move, to move, to move, to move, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to move, to move, to move, to move, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to fight, to move, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight,"}, {"heading": "8 Skip-Thought Vectors", "text": "In addition to experiments with CBOW and LSTM encoders, we are also experimenting with the skipped vector model (Kiros et al., 2015), which extends the idea of the auto encoder to adjacent sentences. If a sentence si is given, it first encodes it with an RNN, similar to the auto encoder model. However, instead of predicting the original sentence, skipthought predicts the preceding and subsequent sentences si \u2212 1 and si + 1. The encoder and decoder are implemented with gated recurrent units (Cho et al., 2014). Here, we deviate from the controlled environment and use the Model4 provided by the author with the recommended embedding size of 4800. This makes the direct comparison of the models \"unfair.\" However, our goal is not to decide which is the \"best\" model, but rather to show how our method can be used to measure the types of information captured by different representations. Table 1 summarizes the performance of the unskipped tasks."}, {"heading": "9 Conclusion", "text": "Our analysis shows some characteristics of sentence embedding methods: \u2022 CBOW is surprisingly effective - in addition to being very strong in content, it can also predict lengths and word sequences. 300 dimensions perform best, with greatly deteriorated word content prediction results at higher dimensions. \u2022 With enough dimensions, LSTM auto encoders are very effective in encoding word order information, and less so in encoding word content. Increasing the dimensionality of the LSTM encoder does not significantly improve its ability to encode length, but increases its ability to encode content and job information. 500-dimensional embedding is already very effective in encoding word sequences, with little beyond winning. Word selection accuracy reaches peaks at 750 dimensions and drops at 1000, suggesting that larger encoders are not always better. \u2022 The trained LSTM encoder (if it jumps with a similar pattern to the car encoding) is not fully trained."}, {"heading": "Appendix: Technical Details", "text": "We experienced different learning rates (0.1, 0.01, 0.001), dropout rates (0.1, 0.2, 0.3, 0.5) (Hinton et al., 2012), and optimization techniques (AdaGrad (Duchi et al., 2011), AdaDelta (Zeiler, 2012), Adam (Kingma and Ba, 2014), and RMSprop (Tieleman and Hinton, 2012). We also experimented with different stack sizes (8, 16, 32) and found improvements in runtime, but no significant improvement in performance. Based on the coordinated parameters, we trained the encoder decoder models on a single GPU (NVIDIA Tesla K40), with different stack sizes (8, 16, 32), and found that the learning rate was achieved during runtime, but no significant improvement in performance."}], "references": [{"title": "Kyunghyun Cho", "author": ["Dzmitry Bahdanau"], "venue": "and Yoshua Bengio.", "citeRegEx": "Bahdanau et al.2014", "shortCiteRegEx": null, "year": 2014}, {"title": "Georgiana Dinu", "author": ["Marco Baroni"], "venue": "and Germ\u00e1n Kruszewski.", "citeRegEx": "Baroni et al.2014", "shortCiteRegEx": null, "year": 2014}, {"title": "NLTK: the natural language toolkit", "author": ["Steven Bird"], "venue": "In Proceedings of the COLING/ACL on Interactive presentation sessions,", "citeRegEx": "Bird.,? \\Q2006\\E", "shortCiteRegEx": "Bird.", "year": 2006}, {"title": "Holger Schwenk", "author": ["Kyunghyun Cho", "Bart Van Merri\u00ebnboer", "Caglar Gulcehre", "Dzmitry Bahdanau", "Fethi Bougares"], "venue": "and Yoshua Bengio.", "citeRegEx": "Cho et al.2014", "shortCiteRegEx": null, "year": 2014}, {"title": "Koray Kavukcuoglu", "author": ["Ronan Collobert"], "venue": "and Cl\u00e9ment Farabet.", "citeRegEx": "Collobert et al.2011", "shortCiteRegEx": null, "year": 2011}, {"title": "Semi-supervised sequence learning", "author": ["Dai", "Le2015] Andrew M Dai", "Quoc V Le"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Dai et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Dai et al\\.", "year": 2015}, {"title": "Elad Hazan", "author": ["John Duchi"], "venue": "and Yoram Singer.", "citeRegEx": "Duchi et al.2011", "shortCiteRegEx": null, "year": 2011}, {"title": "Distributed representations, simple recurrent networks, and grammatical structure", "author": ["Jeffrey L Elman"], "venue": "Machine learning,", "citeRegEx": "Elman.,? \\Q1991\\E", "shortCiteRegEx": "Elman.", "year": 1991}, {"title": "Antoine Bordes", "author": ["Xavier Glorot"], "venue": "and Yoshua Bengio.", "citeRegEx": "Glorot et al.2011", "shortCiteRegEx": null, "year": 2011}, {"title": "Abdel-rahman Mohamed", "author": ["Alex Graves"], "venue": "and Geoffrey Hinton.", "citeRegEx": "Graves et al.2013", "shortCiteRegEx": null, "year": 2013}, {"title": "Kyunghyun Cho", "author": ["Felix Hill"], "venue": "and Anna Korhonen.", "citeRegEx": "Hill et al.2016", "shortCiteRegEx": null, "year": 2016}, {"title": "Ilya Sutskever", "author": ["Geoffrey E. Hinton", "Nitish Srivastava", "Alex Krizhevsky"], "venue": "and Ruslan Salakhutdinov.", "citeRegEx": "Hinton et al.2012", "shortCiteRegEx": null, "year": 2012}, {"title": "Long short-term memory", "author": ["Hochreiter", "Schmidhuber1997] Sepp Hochreiter", "J\u00fcrgen Schmidhuber"], "venue": "Neural Computation,", "citeRegEx": "Hochreiter et al\\.,? \\Q1997\\E", "shortCiteRegEx": "Hochreiter et al\\.", "year": 1997}, {"title": "Grzegorz Chrupa\u0142a", "author": ["\u00c1kos K\u00e1d\u00e1r"], "venue": "and Afra Alishahi.", "citeRegEx": "K\u00e1d\u00e1r et al.2016", "shortCiteRegEx": null, "year": 2016}, {"title": "Justin Johnson", "author": ["Andrej Karpathy"], "venue": "and Fei-Fei Li.", "citeRegEx": "Karpathy et al.2015", "shortCiteRegEx": null, "year": 2015}, {"title": "Adam: A method for stochastic optimization. arXiv preprint arXiv:1412.6980", "author": ["Kingma", "Ba2014] Diederik Kingma", "Jimmy Ba"], "venue": null, "citeRegEx": "Kingma et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Kingma et al\\.", "year": 2014}, {"title": "Antonio Torralba", "author": ["Ryan Kiros", "Yukun Zhu", "Ruslan R Salakhutdinov", "Richard Zemel", "Raquel Urtasun"], "venue": "and Sanja Fidler.", "citeRegEx": "Kiros et al.2015", "shortCiteRegEx": null, "year": 2015}, {"title": "Sagar Waghmare", "author": ["Nicholas L\u00e9onard"], "venue": "and Yang Wang.", "citeRegEx": "L\u00e9onard et al.2015", "shortCiteRegEx": null, "year": 2015}, {"title": "Linguistic regularities in sparse and explicit word representations", "author": ["Levy", "Goldberg2014] Omer Levy", "Yoav Goldberg"], "venue": "In Proc. of CONLL,", "citeRegEx": "Levy et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Levy et al\\.", "year": 2014}, {"title": "Yoav Goldberg", "author": ["Omer Levy"], "venue": "and Ido Dagan.", "citeRegEx": "Levy et al.2015", "shortCiteRegEx": null, "year": 2015}, {"title": "Minh-Thang Luong", "author": ["Jiwei Li"], "venue": "and Dan Jurafsky.", "citeRegEx": "Li et al.2015", "shortCiteRegEx": null, "year": 2015}, {"title": "Efficient estimation of word representations in vector space. arXiv preprint arXiv:1301.3781", "author": ["Kai Chen", "Greg Corrado", "Jeffrey Dean"], "venue": null, "citeRegEx": "Mikolov et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Mikolov et al\\.", "year": 2013}, {"title": "Distributed representations of words and phrases and their compositionality", "author": ["Ilya Sutskever", "Kai Chen", "Greg S Corrado", "Jeff Dean"], "venue": "In Advances in neural information processing systems,", "citeRegEx": "Mikolov et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Mikolov et al\\.", "year": 2013}, {"title": "Rectified linear units improve restricted boltzmann machines", "author": ["Nair", "Hinton2010] Vinod Nair", "Geoffrey E Hinton"], "venue": "In Proceedings of the 27th International Conference on Machine Learning", "citeRegEx": "Nair et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Nair et al\\.", "year": 2010}, {"title": "Todd Ward", "author": ["Kishore Papineni", "Salim Roukos"], "venue": "and Wei-Jing Zhu.", "citeRegEx": "Papineni et al.2002", "shortCiteRegEx": null, "year": 2002}, {"title": "Oriol Vinyals", "author": ["Ilya Sutskever"], "venue": "and Quoc VV Le.", "citeRegEx": "Sutskever et al.2014", "shortCiteRegEx": null, "year": 2014}, {"title": "Lecture 6.5-rmsprop. COURSERA: Neural networks for machine learning", "author": ["Tieleman", "Hinton2012] Tijmen Tieleman", "Geoffrey Hinton"], "venue": null, "citeRegEx": "Tieleman et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Tieleman et al\\.", "year": 2012}, {"title": "Adadelta: an adaptive learning rate method", "author": ["Matthew D Zeiler"], "venue": "arXiv preprint arXiv:1212.5701", "citeRegEx": "Zeiler.,? \\Q2012\\E", "shortCiteRegEx": "Zeiler.", "year": 2012}], "referenceMentions": [], "year": 2017, "abstractText": "There is a lot of research interest in encoding variable length sentences into fixed length vectors, in a way that preserves the sentence meanings. Two common methods include representations based on averaging word vectors, and representations based on the hidden states of recurrent neural networks such as LSTMs. The sentence vectors are used as features for subsequent machine learning tasks or for pretraining in the context of deep learning. However, not much is known about the properties that are encoded in these sentence representations and about the language information they capture. We propose a framework that facilitates better understanding of the encoded representations. We define prediction tasks around isolated aspects of sentence structure (namely sentence length, word content, and word order), and score representations by the ability to train a classifier to solve each prediction task when using the representation as input. We demonstrate the potential contribution of the approach by analyzing different sentence representation mechanisms. The analysis sheds light on the relative strengths of different sentence embedding methods with respect to these low level prediction tasks, and on the effect of the encoded vector\u2019s dimensionality on the resulting representations.", "creator": "LaTeX with hyperref package"}}}