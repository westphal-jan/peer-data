{"id": "1608.04320", "review": {"conference": "NIPS", "VERSION": "v1", "DATE_OF_SUBMISSION": "15-Aug-2016", "title": "Correlated-PCA: Principal Components' Analysis when Data and Noise are Correlated", "abstract": "Given a matrix of observed data, Principal Components Analysis (PCA) computes a small number of orthogonal directions that contain most of its variability. Provably accurate solutions for PCA have been in use for decades. However, to the best of our knowledge, all existing theoretical guarantees for it assume that the data and the corrupting noise are mutually independent, or at least uncorrelated. This is valid in practice often, but not always. In this paper, we study the PCA problem in the setting where the data and noise can be correlated. Such noise is often referred to as \"data-dependent noise\". We obtain a correctness result for the standard eigenvalue decomposition (EVD) based solution to PCA under simple assumptions on the data-noise correlation. We also develop and analyze a generalization of EVD, called cluster-EVD, and argue that it reduces the sample complexity of EVD in certain regimes.", "histories": [["v1", "Mon, 15 Aug 2016 16:32:57 GMT  (202kb)", "https://arxiv.org/abs/1608.04320v1", "Under submission to IEEE Transactions on Signal Processing, A part of this work will appear at NIPS 2016"], ["v2", "Wed, 2 Nov 2016 17:55:02 GMT  (57kb)", "http://arxiv.org/abs/1608.04320v2", "NIPS 2016 (to appear). Longer version submitted to IEEE Trans. Sig. Proc. is atthis http URL"]], "COMMENTS": "Under submission to IEEE Transactions on Signal Processing, A part of this work will appear at NIPS 2016", "reviews": [], "SUBJECTS": "cs.LG cs.IT math.IT", "authors": ["namrata vaswani", "han guo"], "accepted": true, "id": "1608.04320"}, "pdf": {"name": "1608.04320.pdf", "metadata": {"source": "CRF", "title": null, "authors": [], "emails": ["namrata@iastate.edu", "hanguo@iastate.edu"], "sections": [{"heading": null, "text": "ar Xiv: 160 8.04 320v 2 [cs.L G] 2N ov"}, {"heading": "1 Introduction", "text": "In fact, most of them are able to trump themselves, and they are also able to trump themselves. (...) Most of them are able to trump themselves. (...) Most of them are able to trump themselves. (...) Most of them are able to trump themselves. (...) Most of them are able to trump themselves. (...) Most of them are able to trump themselves. (...) Most of them are able to trump themselves. (...) Most of them are able to trump themselves. (...) Most of them are able to trump themselves. (...) Most of them are able to trump themselves. (...)"}, {"heading": "1.1 Correlated-PCA: Problem Definition", "text": "We are given a time sequence of data vectors, yt that satisfyt = \u03b2 = \u03b2 = \u03b2 = \u03b2 = wt, with wt = Mtn, range (P); at is its projection into this r-dimensional subspace; and wt is data-dependent noise. We refer to Mt as the correlation / data dependency matrix. The goal is to estimate the range (P) in which the following assumptions are made. We make the following assumptions: The subspace projection coefficients are zero mean, mutually independent, and delimited random vectors (r.v.), with a diagonal covariance matrix."}, {"heading": "1.2 Examples of correlated-PCA problems", "text": "Another example of correlated PCA processes is the problem of the missing PCA component (PCA missing)."}, {"heading": "2 Simple EVD", "text": "The EVD calculates the uppermost eigenvectors of the empirical covariance matrix, 1\u03b1 \u2211 \u03b1 t = 1 ytyt, \"of the observed data. The following results can be shown: - Theorem 2.1 (simple EVD result). Let us clearly outline the matrix, which contains all eigenvectors of 1 \u03b1 \u00b2. - Assumption 1.1. - Assumption 1."}, {"heading": "3 Cluster-EVD", "text": "In order to loosen the strong dependence of the above result on f2, we are developing a generalization of Simple-EVD, which we call Cluster-EVD."}, {"heading": "3.1 Clustering assumption", "text": "To explain the assumption, we define the following division of the index sentence {1, 2,.. r} based on the eigenvalues of \u03a3. Let \u03bbi specify its i-th largest eigenvalue. Definition 3.1 (g-state-number partition of {1, 2,.. r}. Let us define G1 = {1, 2,.. r1}, where r1 is the index for the maximum eigenvalue, which is first g.For each k > 1 + 1 > g. Define G1, start with the index of the first (largest) eigenvalue, and keep indexes of the smaller eigenvalues appended to the sentence until the ratio of the maximum eigenvalue to the minimum eigenvalue is first g.For each k > 1 + 1 > g, we define Gk = 1, r. \"We assume that r.\" We assume that r. \""}, {"heading": "3.2 Cluster-EVD algorithm", "text": "The cluster EVD approach is summarized in algorithm 1. I Its main idea is as follows: We start with the calculation of the empirical covariance matrix of the first set of \u03b1 observed data points, D-1: = 1 alpha eigenvalue algorithm 1 eigenvalue algorithm-EVD parameters: \u03b1, g-eigenvalue.Let us set the first cluster G-1, let us start with the index of the first (largest) eigenvalue and let us add indices of the smaller eigenvalue algorithm 1 eigenvalue.eigenvalue parameter: \u03b1, g-eigenvalue.Let us set the first cluster G-0 [.]. Let us set the flag stop-eigenvalue1. Repeat1. Let us leave G-value, k: = [G-eigenvalue algorithm 1, G-1, G-eigenvalue.1] and let us find the second cluster. & lt."}, {"heading": "3.3 Main result", "text": "We give the performance guarantee for algorithm 1 at this point. Its parameters are set as follows: Then we set g to a value slightly higher than g. This is necessary to take into account the fact that \u03bb-i is not equal to i-th eigenvalue, but is within a small margin of it. For the same reason, we must also use a \"zero\" threshold that is greater than zero, but smaller than \u03bb \u2212. We set a value large enough to ensure that SE (P, P) \u2264 r\u0432 holds a sufficiently high probability. Theorem 3.3 (cluster EVD result). Let's consider algorithm 1. Select such a value so that r2\u0432 \u2212 0.0001, and r2\u0432 f \u2264 0.01. Let's assume that yt respect (2) and the following value can be adhered to. 1. Assumption 1.1 and assumption 3.2 based on PCs + 2.0."}, {"heading": "4 Discussion", "text": "In the cluster EVD (c-\u03b2 VD) result, theorem 3.3 is if q is small enough (e.g. if q \u2264 1 / \u221a f), and if (r2\u0445) f \u2264 0.01, however, it is clear that the maximum in the maximum complexity (.,.) expression is achieved either by (g +) 2. Thus, in this regime c-EVD is replaced by 1, and so its sample complexity, \u03b1 C r 2 (11 logn + log.) (r.2) is 2 g 2 and its sample complexity. In situations where the condition number f is very large but g + is small (the cluster EVD assumption holds good), the complexity of the sample is greater than C r211 logn (r.2) f2. In situations where the condition number f is very large but g + is."}, {"heading": "5 Numerical Experiments", "text": "We use the PCA-SDDC problem as our case study. We compare EVD = \u03b2 tq = 10 PCP (c-EVD) with PCP [15], solved with [24], and with Alt-Min-RPCA [17] (implemented using codes from the authors website). Both PCP and Alt-Min-RPCA are selected as the uppermost eigenvectors of the estimated L. To show the advantage of EVD or c-EVD, we allow the data from P to be sparse. These were selected as the first r = 5 columns of the identity matrix. We uniformly generate at the iid with zero mean and covariance matrix. We generate it as diag (100, 100, 100, 100, 0.1, 0.1, 0.1). Thus, the condition number f = 1000. The clustering assumption holds with 2, g + = 0.001 and 0.001."}, {"heading": "6 Conclusions and Future Work", "text": "We investigated the problem of PCA in the noise correlated with the data (data-dependent noise) and obtained sample complexity limits for the most commonly used PCA solution, the simple EVD. We also developed and analyzed a generalization of the EVD, called cluster EVD, which has lower sample complexity under additional assumptions. We provided a detailed comparison of our results with those for other approaches to solving their sample applications - PCA with missing data and PCA with sparse data-dependent corruption. We used the Matrix Hoeffding Inequality [20] to obtain our results. As explained in Section 2, it should be possible to improve the sample complexity limits if they are replaced by [21, Theorem 5.39] or Matrix Bernstein. Furthermore, as in Section [5] (for ReProCS), the interdependence of PCA results can be slightly improved by a more practical assumption of the following authoritative model that changes."}, {"heading": "7 More examples of Assumption 1.2", "text": "Assumption 1.3 is a simple example of a support modification model that ensures that if M2, t = ITt, the assumption applies to M2, t according to Assumption 1.2. If there are k objects instead of an object, and each of their carriers fulfills Assumption 1.3, then it is again possible, with some modifications, to show that both the PCA missing and the PCA-SDDC problems satisfy Assumption 1.2. Note also that Assumption 1.3 does not have to be contiguous at all (they do not have to correspond to the support of one or fewer objects). Likewise, we can replace the condition that Tt must be constant for at most \u03b2 points in Assumption 1.3 with | {t: Tt = T [k]}. Third, the requirement that the object (s) always move in one or more directions may seem too strict."}, {"heading": "8 Proof of Theorem 2.1", "text": "This result also follows as a consequence of Theorem 3.3. We prove it separately at first, since its detection is short and less notation-intensive. It will help us to understand much easier the detection of Theorem 3.3. Both results are based on the most closely tested Sin Constant Theorem 8.1 of Davis's eigenconstants and Kahan's sin constant theorem [19]. Let us consider two Hermitic eigenconstants D and D. Let us suppose that D can be dissected asD = [E E] [A 0 A] [E] [E] [E] [E] [E], where [E] is an orthonormal matrix. Let us assume that D [F] can be dissected asD = [F] [E] [F 0 \"."}, {"heading": "8.2 Proof of Theorem 2.1", "text": "We use the sinful liability theory [19] of Corollary 8.2. Let us apply it with D \u03b2 = 1\u03b1 fic t ytyt \"and D = 1 \u03b1 p.\" So let us apply F = P. \"\u2212\" Apply. \"\u2212\" Apply. \"Then it is easy to see that the fault H: = 1 \u03b1 t\" atat. \"P\" + P \"0P,\" and so we have E = P \u2212 \u2212 \"Scope\" and A \"= 0. Moreover, it is easy to see that the fault H: = 1 \u03b1 t\" ytyt. \"\u2212\" Scope \"T.\" \u2212 \"Satisfaction\" t. \"\u2212\" Scope H = 1 \"Scope.\" \u2212 \"Scope\" Scope. \"\u2212 P\" Scope \"Scope.\" \u2212 \"Scope\" Scope. \""}, {"heading": "9 Proof of Theorem 3.3", "text": "In paragraph 9.2 we give a sequence of lemmas in a generalised form (so that they are applicable to various other problems). The proof of theorem 3.3 is provided in paragraph 9.3 and follows easily by applying it. One of the lemmas of paragraph 9.2 is proved in paragraph 10, while the others are proved there themselves."}, {"heading": "9.1 Overall idea", "text": "We must bind SE (P, P). From algorithm 1, P, K, G, K, K, K, K, K, K, K, K, K, K, K, K, K, K, K, K, K, K, K, K, K, K, K, K, K, K, K, K, K, K, K, K, K, K, K, K, K, K, K, K, K, K, K, K, K, K, K, K, K, K, K, K, K, K, K, K, K, K, K, K, K, K, K, K, K, K, K, K, K, K, K, K, K, K, K, K, K, K, K, K, K, K, K, K, K, K, K, K, K, K, K, K, K, K, K, K, K, K, K, K, K, K, K, K, K, K, K, K, K, K, K, K, K, K, K, K, K, K, K, K, K, K, K, K, K, K, K, K, K, K, K, K, K, K, K, K, K, K, K, K, K, K, K, K, K, K, K, K, K, K, K, K, K, K, K, K, K, K, K, K, K, K, K, K, K, K, K, K, K, K, K, K, K, K, K, K, K, K, K, K, K, K, K, K, K, K, K, K, K, K, K, K, K, K, K, K, K, K, K, K, K, K, K, K, K, K, K, K, K, K, K, K, K, K, K, K, K, K, K, K, K, K, K, K, K, K, K, K, K, K, K, K, K, K, K, K, K, K, K, K, K, K, K, K"}, {"heading": "9.2 Main lemmas - generalized form", "text": "In this section, we give a sequence of estimates that apply to a generic problem, in which we engage in a generic problem, in which we engage in a generic problem, in which we engage in a generic problem, in which we engage in a generic problem, in which we engage in a generic problem, in which we engage in a generic problem, in which we engage in a generic problem. (Gdet, Gcur, Gundet) The goal is that we engage in a generic problem (Gcur, Gundet). (Gcur, Gcur, Gundet) The goal is that we engage in a generic problem (Gcur, Gcur) and in a generic problem. (Gcur) The goal is to estimate the estimation errors (Gcur) and limit the estimation errors."}, {"heading": "9.3 Proof of Theorem 3.3", "text": "The theorem is a direct consequence of the use of (9) and the application of Lemma 9.8 for each of the k-steps with the substitutions given in Definition 9.2; together with the corresponding selection of \u03b1. Detailed proof can be found in paragraph 11."}, {"heading": "10 Proof of Hoeffding lemma, Lemma 9.6", "text": "The following Lemma, which is a modification of [3, Lemma 8.15], is used in our proof. It is provided in Section 11. The proof is provided in Section 11: [3, Lemma 2.10]. Lemma 10.1. \u2212 Setpoint: 2.0. \u2212 Setpoint: 2.0. \u2212 Setpoint: 2.0. \u2212 Setpoint: 2.0. \u2212 Setpoint: 2.0. \u2212 Setpoint: 2.0. \u2212 Setpoint: 2.0. \u2212 Setpoint: 2.0. \u2212 Setpoint: 2.0. \u2212 Setpoint: 2.0. \u2212 Setpoint: 2.0. \u2212 Setpoint: 2.0. \u2212 Setpoint: 2.0. \u2212 Setpoint: 2.0. \u2212 Setpoint: 2.0. \u2212 Setpoint: 2.0. \u2212 Setpoint: 2.0. \u2212 Setpoint: 2.0. \u2212 Setpoint: 2.0. \u2212 Setpoint: 2.0. \u2212 Setpoint: 2.0. \u2212 Setpoint: 2.0. \u2212 Setpoint: 2.0. \u2212 Setpoint: 2.0."}, {"heading": "11 Detailed Proof of Theorem 3.3 and Proof of Lemma 10.1", "text": "The proof for Theorem 3.3 indicates that we must indicate that this problem cannot be solved. Let's assume that the substitutions (max.) given in Definition 9.2 cannot be solved. Let's assume that the substitutions (max.) given in Definition 9.2 cannot be solved. (Sun.) Let's assume that the use of (9), (Sun.) Let's assume that the algorithm 1) does not stop (k + 1) -th step. (Sun.) Let's assume that the use of (9), (Sun.) Let's assume that (Sun.) Let's assume that the algorithm 1) does not stop (k + 1) -th step. (Sun.) Let's assume that the algorithms 1, (Sun.) Let's assume. (Sun.) Let's assume that the algorithms 1, (Sun.) Let's. (Sun.) Let's. (Sun.) Let's. (Sun.) Let's say. (Sun.) Let's. (Sun.)"}], "references": [{"title": "Finite sample approximation results for principal component analysis: A matrix perturbation approach", "author": ["B. Nadler"], "venue": "The Annals of Statistics, vol. 36, no. 6, 2008.", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2008}, {"title": "Real-time robust principal components\u2019 pursuit", "author": ["C. Qiu", "N. Vaswani"], "venue": "Allerton Conf. on Communication, Control, and Computing, 2010.", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2010}, {"title": "Recursive robust pca or recursive sparse recovery in large but structured noise", "author": ["C. Qiu", "N. Vaswani", "B. Lois", "L. Hogben"], "venue": "IEEE Trans. Info. Th., pp. 5007\u20135039, August 2014.", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2014}, {"title": "Online matrix completion and online robust pca", "author": ["B. Lois", "N. Vaswani"], "venue": "IEEE Intl. Symp. Info. Th. (ISIT), 2015.", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2015}, {"title": "Online (and Offline) Robust PCA: Novel Algorithms and Performance Guarantees", "author": ["J. Zhan", "B. Lois", "H. Guo", "N. Vaswani"], "venue": "Intnl. Conf. Artif. Intell. and Stat. (AISTATS), 2016.", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2016}, {"title": "Stochastic optimization of pca with capped msg", "author": ["R. Arora", "A. Cotter", "N. Srebro"], "venue": "Adv. Neural Info. Proc. Sys. (NIPS), 2013, pp. 1815\u20131823.", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2013}, {"title": "A stochastic pca and svd algorithm with an exponential convergence rate", "author": ["O. Shamir"], "venue": "arXiv:1409.2848, 2014.", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2014}, {"title": "Online principal components analysis", "author": ["C. Boutsidis", "D. Garber", "Z. Karnin", "E. Liberty"], "venue": "Proc. ACM-SIAM Symposium on Discrete Algorithms (SODA), 2015, pp. 887\u2013901.", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2015}, {"title": "The fast convergence of incremental pca", "author": ["A. Balsubramani", "S. Dasgupta", "Y. Freund"], "venue": "Adv. Neural Info. Proc. Sys. (NIPS), 2013, pp. 3174\u20133182.", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2013}, {"title": "Online pca with spectral bounds", "author": ["Z. Karnin", "E. Liberty"], "venue": "Proce. Conference on Computational Learning Theory (COLT), 2015, pp. 505\u2013509.", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2015}, {"title": "Memory limited, streaming pca", "author": ["I. Mitliagkas", "C. Caramanis", "P. Jain"], "venue": "Adv. Neural Info. Proc. Sys. (NIPS), 2013, pp. 2886\u20132894.", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2013}, {"title": "Matrix rank minimization with applications", "author": ["M. Fazel"], "venue": "PhD thesis, Stanford Univ, 2002.", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2002}, {"title": "Exact matrix completion via convex optimization", "author": ["E.J. Candes", "B. Recht"], "venue": "Found. of Comput. Math, , no. 9, pp. 717\u2013772, 2008.", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2008}, {"title": "Robust principal component analysis", "author": ["E.J. Cand\u00e8s", "X. Li", "Y. Ma", "J. Wright"], "venue": "Journal of ACM, vol. 58, no. 3, 2011.", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2011}, {"title": "Rank-sparsity incoherence for matrix decomposition", "author": ["V. Chandrasekaran", "S. Sanghavi", "P.A. Parrilo", "A.S. Willsky"], "venue": "SIAM Journal on Optimization, vol. 21, 2011.", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2011}, {"title": "Robust matrix decomposition with sparse corruptions", "author": ["D. Hsu", "S.M. Kakade", "T. Zhang"], "venue": "IEEE Trans. Info. Th., Nov. 2011.", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2011}, {"title": "Non-convex robust pca", "author": ["P. Netrapalli", "U N Niranjan", "S. Sanghavi", "A. Anandkumar", "P. Jain"], "venue": "Neural Info. Proc. Sys. (NIPS), 2014.", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2014}, {"title": "Provably correct recursive projected compressive sensing (reprocs) for dynamic robust pca: A correlated-pca reformulation", "author": ["N. Vaswani", "B. Lois", "P. Narayanamurthy"], "venue": "http://www.ece.iastate.edu/long_RobSubTrack_3.pdf, submitted to ICASSP 2017.", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2017}, {"title": "The rotation of eigenvectors by a perturbation. iii", "author": ["C. Davis", "W.M. Kahan"], "venue": "SIAM J. Numer. Anal., vol. 7, pp. 1\u201346, Mar. 1970.", "citeRegEx": "19", "shortCiteRegEx": null, "year": 1970}, {"title": "User-friendly tail bounds for sums of random matrices", "author": ["J.A. Tropp"], "venue": "Found. Comput. Math., vol. 12, no. 4, 2012.", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2012}, {"title": "Introduction to the non-asymptotic analysis of random matrices", "author": ["R. Vershynin"], "venue": "Compressed sensing, pp. 210\u2013268, 2012.", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2012}, {"title": "Eigenvalue computation in the 20th century", "author": ["G.H. Golub", "H.A. Van der Vorst"], "venue": "Journal of Computational and Applied Mathematics, vol. 123, no. 1, pp. 35\u201365, 2000.", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2000}, {"title": "Low-rank matrix completion using alternating minimization", "author": ["P. Netrapalli", "P. Jain", "S. Sanghavi"], "venue": "Symposium on Theory of Computing (STOC), 2013.", "citeRegEx": "23", "shortCiteRegEx": null, "year": 2013}, {"title": "Alternating direction algorithms for l1 problems in compressive sensing", "author": ["Z. Lin", "M. Chen", "Y. Ma"], "venue": "Tech. Rep., University of Illinois at Urbana-Champaign, November 2009. 9", "citeRegEx": "24", "shortCiteRegEx": null, "year": 2009}], "referenceMentions": [{"referenceID": 0, "context": ", see [1] and references therein.", "startOffset": 6, "endOffset": 9}, {"referenceID": 1, "context": "We first encountered it while solving the dynamic robust PCA problem in the Recursive Projected Compressive Sensing (ReProCS) framework [2, 3, 4, 5].", "startOffset": 136, "endOffset": 148}, {"referenceID": 2, "context": "We first encountered it while solving the dynamic robust PCA problem in the Recursive Projected Compressive Sensing (ReProCS) framework [2, 3, 4, 5].", "startOffset": 136, "endOffset": 148}, {"referenceID": 3, "context": "We first encountered it while solving the dynamic robust PCA problem in the Recursive Projected Compressive Sensing (ReProCS) framework [2, 3, 4, 5].", "startOffset": 136, "endOffset": 148}, {"referenceID": 4, "context": "We first encountered it while solving the dynamic robust PCA problem in the Recursive Projected Compressive Sensing (ReProCS) framework [2, 3, 4, 5].", "startOffset": 136, "endOffset": 148}, {"referenceID": 5, "context": "Some other somewhat related recent works include [6, 7] that study stochastic optimization based techniques for PCA; and [8, 9, 10, 11] that study online PCA.", "startOffset": 49, "endOffset": 55}, {"referenceID": 6, "context": "Some other somewhat related recent works include [6, 7] that study stochastic optimization based techniques for PCA; and [8, 9, 10, 11] that study online PCA.", "startOffset": 49, "endOffset": 55}, {"referenceID": 7, "context": "Some other somewhat related recent works include [6, 7] that study stochastic optimization based techniques for PCA; and [8, 9, 10, 11] that study online PCA.", "startOffset": 121, "endOffset": 135}, {"referenceID": 8, "context": "Some other somewhat related recent works include [6, 7] that study stochastic optimization based techniques for PCA; and [8, 9, 10, 11] that study online PCA.", "startOffset": 121, "endOffset": 135}, {"referenceID": 9, "context": "Some other somewhat related recent works include [6, 7] that study stochastic optimization based techniques for PCA; and [8, 9, 10, 11] that study online PCA.", "startOffset": 121, "endOffset": 135}, {"referenceID": 10, "context": "Some other somewhat related recent works include [6, 7] that study stochastic optimization based techniques for PCA; and [8, 9, 10, 11] that study online PCA.", "startOffset": 121, "endOffset": 135}, {"referenceID": 11, "context": ", [12, 13], PCA-missing can also be solved by first solving the low-rank matrix completion problem to recoverL, followed by PCA on the completed matrix.", "startOffset": 2, "endOffset": 10}, {"referenceID": 12, "context": ", [12, 13], PCA-missing can also be solved by first solving the low-rank matrix completion problem to recoverL, followed by PCA on the completed matrix.", "startOffset": 2, "endOffset": 10}, {"referenceID": 13, "context": "Another example where correlated-PCA occurs is that of robust PCA (low-rank + sparse formulation) [14, 15, 16] when the sparse component\u2019s magnitude is correlated with lt.", "startOffset": 98, "endOffset": 110}, {"referenceID": 14, "context": "Another example where correlated-PCA occurs is that of robust PCA (low-rank + sparse formulation) [14, 15, 16] when the sparse component\u2019s magnitude is correlated with lt.", "startOffset": 98, "endOffset": 110}, {"referenceID": 15, "context": "Another example where correlated-PCA occurs is that of robust PCA (low-rank + sparse formulation) [14, 15, 16] when the sparse component\u2019s magnitude is correlated with lt.", "startOffset": 98, "endOffset": 110}, {"referenceID": 13, "context": "One key application where it occurs is in foreground-background separation for videos consisting of a slow changing background sequence (modeled as lying close to a low-dimensional subspace) and a sparse foreground image sequence consisting typically of one or more moving objects [14].", "startOffset": 281, "endOffset": 285}, {"referenceID": 13, "context": "An alternative solution approach for PCA-SDDC is to use an RPCA solution such as principal components\u2019 pursuit (PCP) [14, 15] or Alternating-Minimization (Alt-Min-RPCA) [17] to first recover the matrix L followed by PCA on L.", "startOffset": 117, "endOffset": 125}, {"referenceID": 14, "context": "An alternative solution approach for PCA-SDDC is to use an RPCA solution such as principal components\u2019 pursuit (PCP) [14, 15] or Alternating-Minimization (Alt-Min-RPCA) [17] to first recover the matrix L followed by PCA on L.", "startOffset": 117, "endOffset": 125}, {"referenceID": 16, "context": "An alternative solution approach for PCA-SDDC is to use an RPCA solution such as principal components\u2019 pursuit (PCP) [14, 15] or Alternating-Minimization (Alt-Min-RPCA) [17] to first recover the matrix L followed by PCA on L.", "startOffset": 169, "endOffset": 173}, {"referenceID": 2, "context": "A third example where correlated-PCA and its generalization, correlated-PCA with partial subspace knowledge, occurs is in the subspace update step of Recursive Projected Compressive Sensing (ReProCS) for dynamic robust PCA [3, 5].", "startOffset": 223, "endOffset": 229}, {"referenceID": 4, "context": "A third example where correlated-PCA and its generalization, correlated-PCA with partial subspace knowledge, occurs is in the subspace update step of Recursive Projected Compressive Sensing (ReProCS) for dynamic robust PCA [3, 5].", "startOffset": 223, "endOffset": 229}, {"referenceID": 17, "context": "We refer the reader to [18] to understand this application.", "startOffset": 23, "endOffset": 27}, {"referenceID": 3, "context": "The following lemma [4] shows that, with Assumption 1.", "startOffset": 20, "endOffset": 23}, {"referenceID": 3, "context": "[[4], Lemmas 5.", "startOffset": 1, "endOffset": 4}, {"referenceID": 3, "context": "7, or [4].", "startOffset": 6, "endOffset": 9}, {"referenceID": 18, "context": "Proof: The proof involves a careful application of the sin \u03b8 theorem [19] to bound the subspace error, followed by using matrix Hoeffding [20] to obtain high probability bounds on each of the terms in the sin \u03b8 bound.", "startOffset": 69, "endOffset": 73}, {"referenceID": 19, "context": "Proof: The proof involves a careful application of the sin \u03b8 theorem [19] to bound the subspace error, followed by using matrix Hoeffding [20] to obtain high probability bounds on each of the terms in the sin \u03b8 bound.", "startOffset": 138, "endOffset": 142}, {"referenceID": 19, "context": "As a result we can apply the matrix Hoeffding inequality [20] to bound the perturbation between the observed data\u2019s empirical covariance matrix and that of the true data.", "startOffset": 57, "endOffset": 61}, {"referenceID": 20, "context": "39 of [21] in places where one can apply a concentration of measure result to \u2211 t atat \u2032/\u03b1 (which is at r \u00d7 r matrix), and by matrix Bernstein [20] elsewhere, it should be possible to further reduce the sample complexity to cmax((qf)r logn, f(r+ logn)).", "startOffset": 6, "endOffset": 10}, {"referenceID": 19, "context": "39 of [21] in places where one can apply a concentration of measure result to \u2211 t atat \u2032/\u03b1 (which is at r \u00d7 r matrix), and by matrix Bernstein [20] elsewhere, it should be possible to further reduce the sample complexity to cmax((qf)r logn, f(r+ logn)).", "startOffset": 143, "endOffset": 147}, {"referenceID": 21, "context": "This assumption can be understood as a generalization of the eigen-gap condition needed by the block power method, which is a fast algorithm for obtaining the k top eigenvectors of a matrix [22].", "startOffset": 190, "endOffset": 194}, {"referenceID": 2, "context": "Algorithm 1 is related to, but significantly different from, the ones introduced in [3, 5] for the subspace deletion step of ReProCS.", "startOffset": 84, "endOffset": 90}, {"referenceID": 4, "context": "Algorithm 1 is related to, but significantly different from, the ones introduced in [3, 5] for the subspace deletion step of ReProCS.", "startOffset": 84, "endOffset": 90}, {"referenceID": 2, "context": "The one introduced in [3] assumed that the clusters were known to the algorithm (which is unrealistic).", "startOffset": 22, "endOffset": 25}, {"referenceID": 4, "context": "The one studied in [5] has an automatic cluster estimation approach, but, one that needs a larger lower bound on \u03b1 compared to what Algorithm 1 needs.", "startOffset": 19, "endOffset": 22}, {"referenceID": 12, "context": ", nuclear norm minimization (NNM) [13] or alternating minimization (Alt-Min-MC) [23].", "startOffset": 34, "endOffset": 38}, {"referenceID": 22, "context": ", nuclear norm minimization (NNM) [13] or alternating minimization (Alt-Min-MC) [23].", "startOffset": 80, "endOffset": 84}, {"referenceID": 13, "context": "Similarly, for PCA-SDDC, this can be done by solving any of the recent provably correct RPCA techniques such as principal components\u2019 pursuit (PCP) [14, 15, 16] or alternating minimization (Alt-Min-RPCA) [17].", "startOffset": 148, "endOffset": 160}, {"referenceID": 14, "context": "Similarly, for PCA-SDDC, this can be done by solving any of the recent provably correct RPCA techniques such as principal components\u2019 pursuit (PCP) [14, 15, 16] or alternating minimization (Alt-Min-RPCA) [17].", "startOffset": 148, "endOffset": 160}, {"referenceID": 15, "context": "Similarly, for PCA-SDDC, this can be done by solving any of the recent provably correct RPCA techniques such as principal components\u2019 pursuit (PCP) [14, 15, 16] or alternating minimization (Alt-Min-RPCA) [17].", "startOffset": 148, "endOffset": 160}, {"referenceID": 16, "context": "Similarly, for PCA-SDDC, this can be done by solving any of the recent provably correct RPCA techniques such as principal components\u2019 pursuit (PCP) [14, 15, 16] or alternating minimization (Alt-Min-RPCA) [17].", "startOffset": 204, "endOffset": 208}, {"referenceID": 16, "context": "If we use the time complexity from [17], then finding the span of the top k singular vectors of an n \u00d7m matrix takes O(nmk) time.", "startOffset": 35, "endOffset": 39}, {"referenceID": 16, "context": "Thus, if \u03b8 is a constant, both simple-EVD and c-EVD need O(n\u03b1r) time, whereas, Alt-Min-RPCA needs O(n\u03b1r) time per iteration [17].", "startOffset": 124, "endOffset": 128}, {"referenceID": 14, "context": "We compare EVD and cluster-EVD (c-EVD) with PCP [15], solved using [24], and with Alt-Min-RPCA [17] (implemented using code", "startOffset": 48, "endOffset": 52}, {"referenceID": 23, "context": "We compare EVD and cluster-EVD (c-EVD) with PCP [15], solved using [24], and with Alt-Min-RPCA [17] (implemented using code", "startOffset": 67, "endOffset": 71}, {"referenceID": 16, "context": "We compare EVD and cluster-EVD (c-EVD) with PCP [15], solved using [24], and with Alt-Min-RPCA [17] (implemented using code", "startOffset": 95, "endOffset": 99}, {"referenceID": 19, "context": "We used the matrix Hoeffding inequality [20] to obtain our results.", "startOffset": 40, "endOffset": 44}, {"referenceID": 4, "context": "Moreover, as done in [5] (for ReProCS), the mutual independence of lt\u2019s can be easily replaced by a more practical assumption of lt\u2019s following autoregressive model with almost no change to our assumptions.", "startOffset": 21, "endOffset": 24}, {"referenceID": 17, "context": "The solution to the latter problem helps to greatly simplify the proof of correctness of ReProCS for online dynamic RPCA [18].", "startOffset": 121, "endOffset": 125}, {"referenceID": 0, "context": "References [1] B.", "startOffset": 11, "endOffset": 14}, {"referenceID": 1, "context": "[2] C.", "startOffset": 0, "endOffset": 3}, {"referenceID": 2, "context": "[3] C.", "startOffset": 0, "endOffset": 3}, {"referenceID": 3, "context": "[4] B.", "startOffset": 0, "endOffset": 3}, {"referenceID": 4, "context": "[5] J.", "startOffset": 0, "endOffset": 3}, {"referenceID": 5, "context": "[6] R.", "startOffset": 0, "endOffset": 3}, {"referenceID": 6, "context": "[7] O.", "startOffset": 0, "endOffset": 3}, {"referenceID": 7, "context": "[8] C.", "startOffset": 0, "endOffset": 3}, {"referenceID": 8, "context": "[9] A.", "startOffset": 0, "endOffset": 3}, {"referenceID": 9, "context": "[10] Z.", "startOffset": 0, "endOffset": 4}, {"referenceID": 10, "context": "[11] I.", "startOffset": 0, "endOffset": 4}, {"referenceID": 11, "context": "[12] M.", "startOffset": 0, "endOffset": 4}, {"referenceID": 12, "context": "[13] E.", "startOffset": 0, "endOffset": 4}, {"referenceID": 13, "context": "[14] E.", "startOffset": 0, "endOffset": 4}, {"referenceID": 14, "context": "[15] V.", "startOffset": 0, "endOffset": 4}, {"referenceID": 15, "context": "[16] D.", "startOffset": 0, "endOffset": 4}, {"referenceID": 16, "context": "[17] P.", "startOffset": 0, "endOffset": 4}, {"referenceID": 17, "context": "[18] N.", "startOffset": 0, "endOffset": 4}, {"referenceID": 18, "context": "[19] C.", "startOffset": 0, "endOffset": 4}, {"referenceID": 19, "context": "[20] J.", "startOffset": 0, "endOffset": 4}, {"referenceID": 20, "context": "[21] R.", "startOffset": 0, "endOffset": 4}, {"referenceID": 21, "context": "[22] G.", "startOffset": 0, "endOffset": 4}, {"referenceID": 22, "context": "[23] P.", "startOffset": 0, "endOffset": 4}, {"referenceID": 23, "context": "[24] Z.", "startOffset": 0, "endOffset": 4}, {"referenceID": 18, "context": "1 sin \u03b8 theorem Davis and Kahan\u2019s sin \u03b8 theorem [19] studies the rotation of eigenvectors by perturbation.", "startOffset": 48, "endOffset": 52}, {"referenceID": 18, "context": "1 (sin \u03b8 theorem [19]).", "startOffset": 17, "endOffset": 21}, {"referenceID": 18, "context": "1 We use the sin \u03b8 theorem [19] from Corollary 8.", "startOffset": 27, "endOffset": 31}, {"referenceID": 19, "context": "In the next lemma, we bound the terms in the bound on SE(P\u0302 ,P ) using the matrix Hoeffding inequality [20].", "startOffset": 103, "endOffset": 107}, {"referenceID": 19, "context": "The following corollaries of the matrix Hoeffding inequality [20], proved in [3], will be used in the proof.", "startOffset": 61, "endOffset": 65}, {"referenceID": 2, "context": "The following corollaries of the matrix Hoeffding inequality [20], proved in [3], will be used in the proof.", "startOffset": 77, "endOffset": 80}, {"referenceID": 2, "context": "1 ([3], Lemma 2.", "startOffset": 3, "endOffset": 6}], "year": 2016, "abstractText": "Given a matrix of observed data, Principal Components Analysis (PCA) computes a small number of orthogonal directions that contain most of its variability. Provably accurate solutions for PCA have been in use for a long time. However, to the best of our knowledge, all existing theoretical guarantees for it assume that the data and the corrupting noise are mutually independent, or at least uncorrelated. This is valid in practice often, but not always. In this paper, we study the PCA problem in the setting where the data and noise can be correlated. Such noise is often also referred to as \u201cdata-dependent noise\u201d. We obtain a correctness result for the standard eigenvalue decomposition (EVD) based solution to PCA under simple assumptions on the data-noise correlation. We also develop and analyze a generalization of EVD, cluster-EVD, that improves upon EVD in certain regimes.", "creator": "LaTeX with hyperref package"}}}