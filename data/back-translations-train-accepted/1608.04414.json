{"id": "1608.04414", "review": {"conference": "NIPS", "VERSION": "v1", "DATE_OF_SUBMISSION": "15-Aug-2016", "title": "Generalization of ERM in Stochastic Convex Optimization: The Dimension Strikes Back", "abstract": "In stochastic convex optimization the goal is to minimize a convex function $F(x) \\doteq {\\mathbf E}_{{\\mathbf f}\\sim D}[{\\mathbf f}(x)]$ over a convex set $\\cal K \\subset {\\mathbb R}^d$ where $D$ is some unknown distribution and each $f(\\cdot)$ in the support of $D$ is convex over $\\cal K$. The optimization is commonly based on i.i.d.~samples $f^1,f^2,\\ldots,f^n$ from $D$. A standard approach to such problems is empirical risk minimization (ERM) that optimizes $F_S(x) \\doteq \\frac{1}{n}\\sum_{i\\leq n} f^i(x)$. Here we consider the question of how many samples are necessary for ERM to succeed and the closely related question of uniform convergence of $F_S$ to $F$ over $\\cal K$. We demonstrate that in the standard $\\ell_p/\\ell_q$ setting of Lipschitz-bounded functions over a $\\cal K$ of bounded radius, ERM requires sample size that scales linearly with the dimension $d$. This nearly matches standard upper bounds and improves on $\\Omega(\\log d)$ dependence proved for $\\ell_2/\\ell_2$ setting by Shalev-Shwartz et al. (2009). In stark contrast, these problems can be solved using dimension-independent number of samples for $\\ell_2/\\ell_2$ setting and $\\log d$ dependence for $\\ell_1/\\ell_\\infty$ setting using other approaches. We also demonstrate that for a more general class of range-bounded (but not Lipschitz-bounded) stochastic convex programs an even stronger gap appears already in dimension 2.", "histories": [["v1", "Mon, 15 Aug 2016 21:19:51 GMT  (17kb)", "http://arxiv.org/abs/1608.04414v1", null], ["v2", "Fri, 28 Oct 2016 00:46:58 GMT  (20kb)", "http://arxiv.org/abs/1608.04414v2", "Added a lower bound construction based on efficiently computable functions"], ["v3", "Mon, 26 Dec 2016 06:37:48 GMT  (299kb,D)", "http://arxiv.org/abs/1608.04414v3", "Added illustrations of functions used in some of the constructions"]], "reviews": [], "SUBJECTS": "cs.LG stat.ML", "authors": ["vitaly feldman"], "accepted": true, "id": "1608.04414"}, "pdf": {"name": "1608.04414.pdf", "metadata": {"source": "CRF", "title": "Generalization of ERM in Stochastic Convex Optimization: The Dimension Strikes Back", "authors": ["Vitaly Feldman"], "emails": [], "sections": [{"heading": null, "text": "ar Xiv: 160 8.04 414v 1 [cs.L G] 15 Aug 2In stochastic convex optimization, the goal is to minimize a convex function F (x). = Ef \u0445 D [f (x)] via a convex group K-Rd, where D is an unknown distribution and each f (\u00b7) in the support of D is convex over K. Optimization is usually based on i.i.d. samples f1, f2,..., fn of D. A standard approach to such problems is empirical risk minimization (ERM) optimizing FS (x). = 1 n \u00b2 i \u2264 n f i (x). Here we consider the question of how many samples are necessary for the success of the ERM and the closely related question of uniform convergence from FS to F over K. We show that in the default setting p / q the Lipschitz-limited function over a radius-limited K, which requires a sample size of K to ERM with an early convergence of K, even requires a sample size of K to ERM."}, {"heading": "1 Introduction", "text": "The goal is to find a hypothesis that defines (approximately) the expected loss in relation to some distribution patterns (z, y) a function from K to R given by f (z, y). The goal is to find a hypothesis that (approximates) the expected loss in relation to some distribution patterns (z, y)."}, {"heading": "1.1 Overview of Results", "text": "In this work we essentially increase the complexity in the worst case. < < < / p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p \"p\" p > p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p."}, {"heading": "2 Preliminaries", "text": "For an integer n = 1 we leave [n]. = {1,.., n}. Random variables are denoted by bold letters, e.g. f. For a convex body (i.e., compact convex set with non-empty interior) K Rd we consider problems of formmin K (FD). = minx \u00b2 K (FD (x). = Ef \u0445 D [f (x)]}, where f is a random variable p, defined by a set of convex, sub-differentiable functions F on K and distributed according to an unknown probability distribution D. We denounce F \u00b2 = minK (FD). For an approximation parameter x > 0 the goal is to find x \u00b2 K so that FD (x)."}, {"heading": "3 Lower Bounds for Lipschitz-Bounded SCO", "text": "In this section, we present our most important lower limits for SCO of Lipschitz-delimited convex functions. For comparison purposes, we begin by formally specifying some known limits to the complexity of the sample in solving such problems: Uniform convergence upper limit: The following uniform convergence limits can easily be derived from the argument of the standard coverage number (e.g. [SN05, SS09]) Theorem 3.1. For p values, K-Bdp (R) can be derived and any distributions to the L-Lipschitz to K function relative to the number p (not necessarily convex) are supported, then for each individual sample p > 0 and n \u2265 n1 = O (d \u00b7 (LR) 2 \u00b7 log (dLR) 2 \u00b7 distributions to the L-Lipschitz to K function relative to the number p (not necessarily convex)."}, {"heading": "3.1 Non-smooth construction", "text": "We will start with a simpler, lower GV- and 1-Lipitz-solution. (...) We will settle for a simpler GV- and 1-Lipitz-solution. (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...).). (...). (...). (...).). (...). (...).). (...). (...).). (...).). (...). (...). (...).). (...).).). (...). (...).). (...). (...).). (...). (...). (...). (...).). (...).). (...). (...).). (...). (...).). (...). (...). (...). (...).). (...). (...). (...).). (...). (...).). (...). (...). (...). (...). (...).).). (...). (...).). (...)"}, {"heading": "3.2 Smoothness does not help", "text": "Expanding the lower limit to smooth features. < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < <"}, {"heading": "3.3 \u21131 Regularization does not help", "text": "Next, we show that the lower limit is valid even with an additional regulation term, with an additional validity term, namely for positive quantities. (Note that for positive quantities, the construction is shifted to the positive orthant (i.e., x is such that xi is 0 for all i [d]). In this orthant, the subgradient of the regulation term is simply 1 x, where 1 x is the entire 1's vector. We can add a linear term to each function in our distribution that reduces the analysis to unregulated cases. Formally, we define the following function family. For V'W, horizon V (x) is the entire 1's vector. = hV (x \u2212 1 x) is the linear term in our distribution, x x x. (3) Note that over Bd2 (2), horizon V (x) is the state."}, {"heading": "3.4 Dependence on \u01eb", "text": "Note: The upper limit for uniform convergence scales such as O (d / 2). We first observe that our design implies a lower limit for uniform convergence (d / 2). Note: The upper limit for uniform convergence scales such as O (d / 2)."}, {"heading": "4 Range-Bounded Convex Optimization", "text": "As we have outlined in the introduction, SCO is solvable in the more general constellation, where instead of the Lipschitz limit and the radius of K there is a limit to the range of functions in support of distribution. < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < <"}, {"heading": "5 Discussion", "text": "One way to get around our lower limits is to use additional structural assumptions. For example, uniform convergence for generalized linear regression problems provides near-optimal limits for sample complexity [KST08]. A natural question is whether there are more general classes of functions that capture most of the practically relevant SCO problems and have dimensionally independent (or log d scaling) uniform convergence limits. Note that the functions constructed in our lower limits have a description of size exponentially in d and are therefore unlikely to be applicable to natural function classes. An alternative approach is to bypass uniform convergence (and possibly ERM) as a whole. Among a large number of techniques designed to ensure generalization, the most common ones are based on notions of stability [BE02, SSS10]. Known analyses based on stability, however, often do not provide the strongest known generalization assumptions that require, for example, a very high probability of this problem remaining stable."}, {"heading": "Acknowledgements", "text": "I am grateful to Ken Clarkson, Sasha Rakhlin and Thomas Steinke for their discussions and insightful comments on this work."}], "references": [{"title": "Escaping the local minima via simulated annealing: Optimization of approximately convex functions", "author": ["Alexandre Belloni", "Tengyuan Liang", "Hariharan Narayanan", "Alexander Rakhlin"], "venue": "In COLT,", "citeRegEx": "Belloni et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Belloni et al\\.", "year": 2015}, {"title": "Non-strongly-convex smooth stochastic approximation with convergence rate o(1/n)", "author": ["Francis R. Bach", "Eric Moulines"], "venue": "In NIPS,", "citeRegEx": "Bach and Moulines.,? \\Q2013\\E", "shortCiteRegEx": "Bach and Moulines.", "year": 2013}, {"title": "Convex optimization: Algorithms and complexity", "author": ["S\u00e9bastien Bubeck"], "venue": "Foundations and Trends in Machine Learning,", "citeRegEx": "Bubeck.,? \\Q2015\\E", "shortCiteRegEx": "Bubeck.", "year": 2015}, {"title": "On the generalization ability of on-line learning algorithms", "author": ["N. Cesa-Bianchi", "A. Conconi", "C. Gentile"], "venue": "IEEE Transactions on Information Theory,", "citeRegEx": "Cesa.Bianchi et al\\.,? \\Q2004\\E", "shortCiteRegEx": "Cesa.Bianchi et al\\.", "year": 2004}, {"title": "Generalization in adaptive data analysis and holdout", "author": ["Cynthia Dwork", "Vitaly Feldman", "Moritz Hardt", "Toniann Pitassi", "Omer Reingold", "Aaron Roth"], "venue": "reuse. CoRR,", "citeRegEx": "Dwork et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Dwork et al\\.", "year": 2015}, {"title": "Statistical query algorithms for stochastic convex optimization", "author": ["Vitaly Feldman", "Cristobal Guzman", "Santosh Vempala"], "venue": "CoRR, abs/1512.09170,", "citeRegEx": "Feldman et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Feldman et al\\.", "year": 2015}, {"title": "Train faster, generalize better: Stability of stochastic gradient descent", "author": ["Moritz Hardt", "Benjamin Recht", "Yoram Singer"], "venue": "CoRR, abs/1509.01240,", "citeRegEx": "Hardt et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Hardt et al\\.", "year": 2015}, {"title": "On the complexity of linear prediction: Risk bounds, margin bounds, and regularization", "author": ["S. Kakade", "K. Sridharan", "A. Tewari"], "venue": "In NIPS,", "citeRegEx": "Kakade et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Kakade et al\\.", "year": 2008}, {"title": "Robust stochastic approximation approach to stochastic programming", "author": ["A. Nemirovski", "A. Juditsky", "G. Lan", "A. Shapiro"], "venue": "SIAM J. Optim.,", "citeRegEx": "Nemirovski et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Nemirovski et al\\.", "year": 2009}, {"title": "Sequential probability assignment with binary alphabets and large classes of experts", "author": ["Alexander Rakhlin", "Karthik Sridharan"], "venue": "CoRR, abs/1501.07340,", "citeRegEx": "Rakhlin and Sridharan.,? \\Q2015\\E", "shortCiteRegEx": "Rakhlin and Sridharan.", "year": 2015}, {"title": "Making gradient descent optimal for strongly convex stochastic optimization", "author": ["Alexander Rakhlin", "Ohad Shamir", "Karthik Sridharan"], "venue": "In ICML,", "citeRegEx": "Rakhlin et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Rakhlin et al\\.", "year": 2012}, {"title": "On complexity of stochastic programming problems", "author": ["A. Shapiro", "A. Nemirovski"], "venue": null, "citeRegEx": "Shapiro and Nemirovski.,? \\Q2005\\E", "shortCiteRegEx": "Shapiro and Nemirovski.", "year": 2005}, {"title": "Understanding Machine Learning: From Theory to Algorithms", "author": ["Shai Shalev-Shwartz", "Shai Ben-David"], "venue": null, "citeRegEx": "Shalev.Shwartz and Ben.David.,? \\Q2014\\E", "shortCiteRegEx": "Shalev.Shwartz and Ben.David.", "year": 2014}, {"title": "Stochastic convex optimization", "author": ["S. Shalev-Shwartz", "O. Shamir", "N. Srebro", "K. Sridharan"], "venue": "In COLT,", "citeRegEx": "Shalev.Shwartz et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Shalev.Shwartz et al\\.", "year": 2009}, {"title": "Learnability, stability and uniform convergence", "author": ["Shai Shalev-Shwartz", "Ohad Shamir", "Nathan Srebro", "Karthik Sridharan"], "venue": "The Journal of Machine Learning Research,", "citeRegEx": "Shalev.Shwartz et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Shalev.Shwartz et al\\.", "year": 2010}, {"title": "Stochastic gradient descent for non-smooth optimization: Convergence results and optimal averaging schemes", "author": ["Ohad Shamir", "Tong Zhang"], "venue": "In ICML,", "citeRegEx": "Shamir and Zhang.,? \\Q2013\\E", "shortCiteRegEx": "Shamir and Zhang.", "year": 2013}, {"title": "Statistical Learning Theory", "author": ["V. Vapnik"], "venue": "Wiley-Interscience, New York,", "citeRegEx": "Vapnik.,? \\Q1998\\E", "shortCiteRegEx": "Vapnik.", "year": 1998}], "referenceMentions": [], "year": 2017, "abstractText": "In stochastic convex optimization the goal is to minimize a convex function F (x) . = Ef\u223cD[f(x)] over a convex set K \u2282 R where D is some unknown distribution and each f(\u00b7) in the support of D is convex over K. The optimization is commonly based on i.i.d. samples f, f, . . . , f from D. A standard approach to such problems is empirical risk minimization (ERM) that optimizes FS(x) . = 1 n \u2211 i\u2264n f (x). Here we consider the question of how many samples are necessary for ERM to succeed and the closely related question of uniform convergence of FS to F over K. We demonstrate that in the standard lp/lq setting of Lipschitz-bounded functions over a K of bounded radius, ERM requires sample size that scales linearly with the dimension d. This nearly matches standard upper bounds and improves on \u03a9(log d) dependence proved for l2/l2 setting in [SSSS09]. In stark contrast, these problems can be solved using dimension-independent number of samples for l2/l2 setting and log d dependence for l1/l\u221e setting using other approaches. We also demonstrate that for a more general class of rangebounded (but not Lipschitz-bounded) stochastic convex programs an even stronger gap appears already in dimension 2.", "creator": "LaTeX with hyperref package"}}}