{"id": "1608.04426", "review": {"conference": "AAAI", "VERSION": "v1", "DATE_OF_SUBMISSION": "15-Aug-2016", "title": "Regularization for Unsupervised Deep Neural Nets", "abstract": "Unsupervised neural networks, such as restricted Boltzmann machines (RBMs) and deep belief networks (DBNs), are powerful tools for feature selection and pattern recognition tasks. We demonstrate that overfitting occurs in such models just as in deep feedforward neural networks, and discuss possible regularization methods to reduce overfitting. We consider weight decay, model averaging methods, and backward elimination, and propose revised model averaging methods to improve their efficiency. We also discuss the asymptotic convergence properties of these methods. Finally, we compare the performance of these methods using likelihood and classification error rates on various pattern recognition data sets.", "histories": [["v1", "Mon, 15 Aug 2016 22:28:05 GMT  (61kb,D)", "http://arxiv.org/abs/1608.04426v1", null], ["v2", "Tue, 30 Aug 2016 23:41:53 GMT  (66kb,D)", "http://arxiv.org/abs/1608.04426v2", null], ["v3", "Mon, 5 Sep 2016 16:53:55 GMT  (66kb,D)", "http://arxiv.org/abs/1608.04426v3", null], ["v4", "Fri, 17 Feb 2017 02:49:12 GMT  (147kb,D)", "http://arxiv.org/abs/1608.04426v4", null]], "reviews": [], "SUBJECTS": "cs.LG cs.NE", "authors": ["baiyang wang", "diego klabjan"], "accepted": true, "id": "1608.04426"}, "pdf": {"name": "1608.04426.pdf", "metadata": {"source": "CRF", "title": "Regularization for Unsupervised Deep Neural Nets", "authors": ["Baiyang Wang", "Diego Klabjan"], "emails": ["baiyang@u.northwestern.edu,", "d-klabjan@northwestern.edu"], "sections": [{"heading": "1 Introduction", "text": "In fact, it is as if we are able to determine ourselves how to behave. (...) It is not as if we are able to behave. (...) It is as if we are able to behave. (...) It is as if we are able to behave. (...) It is as if we are able to outdo ourselves. (...) It is as if we are able to outdo ourselves. (...) It is as if we are able to outdo ourselves. \"(...) It is as if we are able to outdo ourselves. (...). (...) It is as if we are able to outdo ourselves. (...) It is as if we are able to outdo ourselves.\" (...) It is as if we are able to outdo ourselves. (...)"}, {"heading": "2 Related Works", "text": "We discuss a simple FFNN regulation with a single level of input \u0131 = (\u01311,., \u0131I) T and a single level of output o = (o1,.., oJ) T-2, 1) J. The weight matrix W is therefore of size J \u00b7 I. We assume that the relation E (o) = a (W \u00b7 \u0131), (1) where a () is the activation function, like the sigmoid function (x) = 1 + e \u2212 x) applied elementary properties (1) is modified into [18] asE (o | m) = a (m? (W \u00b7 \u0131))), m = (m1,., mJ) T iid \"Bernoulli (p), (2) where? denotes element-wise multiplication, whereby we achieve the dropout regulation (DO) for neural networks. In dropout we minimize the objective function \u2212 lDO\""}, {"heading": "3 RBM Regularization", "text": "For a restricted Boltzmann machine, we let v = (v1, \u00b7 \u00b7 \u00b7, vJ) T + {0, 1} J denote the visible vector, and h = (h1, \u00b7 \u00b7, hI) T + {0, 1} I denote the hidden vector. Each vj, j = 1,.., J is a visible node and each hi, i = 1..., I is a hidden node. (6) The common probability is P (v, h) = e \u2212 E (v, h) / \u2211, \u03b7 e \u2212 E (\u03bd, p), E (v, h) = \u2212 bT v \u2212 cTh \u2212 hTWv. (6) We let it happen = (b, c, W), i.e. it is a vector containing all components of b, c, W. In reality, we only have data from visible vector v, so that the model calibration consists of finding a vector."}, {"heading": "3.1 Weight Decay Regularization", "text": "The most commonly used term is L2 (comb regression), or L1 (LASSO [20]). In CD-k we add the term weight decomposition regulation. In CD-k we look at a more general form of weight decomposition regulation. Suppose we have obtained an estimate W \u00b2 W of CD without regulation. Instead of adding the term W \u00b2 ss, we add the term \u00b5IJ \u00b2 i, j | s / | W \u00b2 ij | s to the negative log probability. Apparently, this adjustment adapts to the different scales of the components of W. While for an RBM all components of W are equal, we will show later that this approach can be a better empirical approach."}, {"heading": "3.2 Model Averaging Regularization", "text": "As discussed in [18], to characterize a dropout (DO) RBM, we only have to apply the following conditional distributions: PDO (hi = 1 | v, m) = mi \u00b7 \u03c3 (ci + Wi \u00b7 v), PDO (vj = 1 | h, m) = \u03c3 (bj + WT \u00b7 j h). (9) Therefore, with a fixed mask m \u00b2 {0, 1} I actually have an RBM with all visible nodes v and hidden nodes {hi: mi = 1}. Hidden nodes {hi: mi = 0} are fixed to zero, so that they have no effect on the conditional RBM. For the purpose of selecting the characteristics, we determine that EDO (hi | v) = p \u00b7 expansion (ci + Wi \u00b7 v). (10) Hence, if we use EDO (hi: mi = 0} RBZ (hi | v) as output characteristics, the probability of any hidden node is multiplied by only the node."}, {"heading": "3.3 Backward Elimination", "text": "We note that due to the high dimensionality and non-linearity of the RBMs, it seems to be the most possible variant among the step-by-step regression techniques. To estimate the W value of W without regulation, we consider the implementation of a fixed mask m = (mij) I \u00b7 J wheremij = 1 | W value, Q = Q100 (1 \u2212 p)% (| W value), (11) i.e. Q is the 100 (1 \u2212 p)% left percentile of all | W values, and p value (0, 1) is some fixed portion of the retained weights. Then we recalibrate the weights and distortions that fix the mask m, resulting in a simple backward applied elimination procedure that erases 100 (1 \u2212 p)% of all weights."}, {"heading": "3.4 Hybrid Regularization", "text": "We present two such hybrid approaches, namely partial DropConnect (PDC) and partial dropout (PDO). The first is a mixture of DropConnect and BE. The rationale for these methods stems from some of the convergence results given in Section 4. Suppose we have an estimate W \u00b2 of W without regulation. Instead of implementing a fixed mask m, we implement DropConnect regulation with different retention probabilities pij for each weight Wij. We let Q = Q100 (1 \u2212 q)% (| W \u00b2 |), andpij = 1 | W \u00b2 Q + p \u00b7 1 | W \u00b2 ij | < Q. (12) Therefore, we stab a different m = (mij) I \u00b2 J \u00b2 Bernoulli (pij) per minibatch, which means that we could always be 100% empiq \u00b7 a \u00b2 of the remaining weight variable."}, {"heading": "4 Theoretical Considerations", "text": "Here we discuss the convergence properties of different regulatory methods, if the number of data examples N > D (\u00b5) (we assume all regulatory coefficients and parameter estimates with (N) if there are N examples. We assume that \u03d1 = (b, c, W) is generated from an RBM with a \"true\" parameter. We designate each regulated estimate of \u03b8 (N). Let us leave A = {d: \u03b8d 6 = 0} and \u03b8d: d \u00b2 A}. It is shown in [26] that AL1 guarantees asymptotic normality and the identification of set A for linear regression. We show that similar results apply to L2 + AL1 for RBMs."}, {"heading": "5 Extension to Other Networks", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "5.1 Deep Belief Networks", "text": "We look at the multilayer network under P (v, h1,.., hL) = P (v | h1) P (h1 | h2) \u00b7 \u00b7 P (hL \u2212 2 | hL \u2212 1) P (hL \u2212 1, hL), (14) where each probability on the right is one of an RBM. To train the weights of RBM (v, h1),..., RBM (hL \u2212 1, hL), we only need to perform a greedy layered training approach, i.e. we first train the weights of RBM (v, h1) and then use E (h1 | v) to train RBM (h1, h2) etc. The weights of the RBMs are used to initialize a deep FFNN with fine gradients of Delett."}, {"heading": "5.2 Other RBM Variants", "text": "In contrast to a DBN that trains the model (14), a DBM [14] forms an undirected graphical model, the main difference being that in the training sequence for a DBM, both the visible and the hidden nodes in the deepest hidden layer are doubled; the weights for the doubled nodes are bound to the original weights; in addition, the original characteristics E (hL | v) can be added; for a replicated Softmax model (RSM) [15], E (v, h) = \u2212 bT v \u2212 C \u00b7 cTh \u2212 hTWv, 1T v = C, v NJ, h {0, 1} I, (15) and for a Gausian RBM [13], E (v, h) = J \u00b2 j = 1 (vj \u2212 aj) 2 / (2\u03c32j) \u2212 cTh \u2212 hTW (v / \u03c3), v \u00b2 RJ, {0} RJ, {1} can be easily expanded to the Element (I / M)."}, {"heading": "6 Data Studies", "text": "We compare the empirical performance of the above regulation methods using the following data sets: MNIST, NORB (image recognition); 20 newsgroups, Reuters21578 (text classification); ISOLET (speech recognition) All results are obtained with GeForce GTX TITAN X in Theano."}, {"heading": "6.1 Experiment Settings", "text": "We consider the following unsupervised neural network structures: DBN / DBM for MNIST; DBN for NORB; RSM plus logistic regression for 20 newsgroups and Reuters21578; GRBM for ISOLET. CD-1 will be performed for the rest of the work. The following regularization methods are taken into account: None (no regularization); DO; DC; L2; L2 + AL1; BE; IBE (r = 3); PDO; PDC. The number of advance periods is 100 per shift and the number of finetuning periods is 300, with a finetuning learning rate of 0.1. For the last five regularization methods that need to be recalibrated, we cut the 100 epochs in half (4 quarters for IBE) and the number of finetuning epochs to 300, with a finetuning learning rate of 100, which we will need to recalibrate for the last five quarters."}, {"heading": "6.2 The MNIST Data Set", "text": "The MNIST dataset consists of 282 pixels of handwritten 0-9 digits. There are 50,000 training examples, 10,000 validations, and 10,000 test examples. First, we look at the probability of the test data of a 500-node RBM for MNIST. There are two model fit criteria: pseudo-liquidity and AIS liquidity. The former is a sum of conditional liquidity, while the latter directly estimates P (v). Figure 1 uses log scale. Here, p = 0.9 for DO, and Far-A = 4 for L2. These numbers tend to be representative of the model fit process. Pseudo-liquidity is an optimistic estimate of the model. We observe that Dropout is the other two epochs for DO liquidity."}, {"heading": "6.3 The NORB Data Set", "text": "None DO DC L2 L2 + AL1 BE IBE PDO PDC m. 11.00% 11.15% 11.19% 10.93% 10.91% 11.04% 11.14% 10.95% 10.81% sd. 0.15% 0.12% 0.10% 0.18% 0.18% 0.20% 0.15% 0.13%"}, {"heading": "6.4 The 20 Newsgroups Data Set", "text": "The data set of the 20 newsgroups is a collection of news documents with 20 categories. There are 11,293 training examples, of which 6,293 are randomly evaluated, and 7,528 test examples. We use the stemmed version, retain the most common 5,000 words, and train an RSM with 1,000 hidden nodes in a single level. We consider this a simple case of deep learning, as it is a two-step process. The pre-training learning rate is 0.02 and the batch size is 50. We use logistic regression to classify the trained features, i.e. hidden values of the RSM, as in [19]. This setting is quite challenging for unattended neural networks. In Table 4, Dropout intersects best with other regulation methods that bring improvements, other than DropConnect."}, {"heading": "6.5 The Reuters21578 Data Set", "text": "The Reuters21578 dataset is a collection of newswire articles. We adopt the stemmed R-52 version with 52 categories, 6,532 training examples, of which 1,032 validation examples are randomly conducted, and 2,568 test examples. We retain the most common 2,000 words and train an RSM with 500 hidden nodes in a single shift. The pre-training learning rate is 0.1 and the batch size 50. We make the learning rate large because the cost function is quite bumpy. Table 5 shows that PDC works best and PDO improves the performance of Dropout."}, {"heading": "6.6 The ISOLET Data Set", "text": "The ISOLET dataset consists of voice recordings of the Latin alphabet (a-z). There are 6,138 training examples, of which 638 validation examples are performed randomly, and 1,559 test examples. We train a Gaussian RBM with 1,000 hidden nodes with a pre-training learning rate of 0.005, lot size 20, and initialize an FFNN that can be considered a single-layer DBN. Table 6 shows that all regulation methods work better than none, with PDC again being the best."}, {"heading": "6.7 Summary", "text": "The most robust methods that bring improvements for all six cases are L2, L2 + AL1 and PDC. PDO can bring improvements for dropout when dropout is not suitable for the network structure. PDC proves to be the most stable method of all and therefore the recommended choice."}, {"heading": "7 Conclusion", "text": "Regularization for deep learning has aroused great interest, and in this paper we extend regularization to unsupervised deep learning, i.e. for DBNs and DBMs. We have proposed several approaches, demonstrated their performance, and empirically compared the various techniques. In the future, we suggest that it would be interesting to consider more variants of modelling regularization for supervised deep learning, as well as better hybrid regularization methods."}, {"heading": "P (A 6\u2282 A\u0302(N)) + lim", "text": "(D), (D), (D), (D), (D), (D), (D), (D), (D), (D), (D), (D), (D), (D), (D), (D), (D), (G), (G), (G), (G), (G), (G), (G), (G), (G), (G), (G), (G), (G), (G), (G), (G), (G), (G), (G), (G), (G), (G), (G), (G), (G), (G), (G), (G), (G), (G), (G), G (G), G (G), G (G), G (G), G (G), G (G), G (G), G (G), G (G), G (G), G (G), G (G), G (G), G (G), G (G (G), G (G), G (G), G (G (G), G (G), G (G), G (G (G), G (G), G (G), G (G (G), G (G), G (G), (G (G), (G), (G), (G), (G (G), (G), (G), (G), (G (G), (G), (G (G), (G), (G (G), (G), (G), (G (G), (G), (G (G), (G), (G (G), (G (G), (G), (G (G), (G), (G (G), (G), (G (G), (G), (G), (G (G (G), (G), (G), (G (G), (G), (G), (G (G), (G (G"}], "references": [{"title": "Learning deep architectures for AI", "author": ["Y. Bengio"], "venue": "Technical Report, https://www.iro.umontreal", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2007}, {"title": "On the asymptotics of constrained M-estimation", "author": ["C. Geyer"], "venue": "The Annals of Statistics", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 1994}, {"title": "A fast learning algorithm for deep belief nets", "author": ["G. Hinton", "S. Osindero", "Teh", "Y.-W"], "venue": "Neural Computation", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2006}, {"title": "Asymptotics for Lasso-type estimators. The Annals of Statistics 28(5):1356-1378", "author": ["K. Knight", "W. Fu"], "venue": null, "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2000}, {"title": "On Bahadur efficiency and maximum likelihood estimation", "author": ["X. Shen"], "venue": "Statistica Sinica 11(2):479-498", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2001}, {"title": "The adaptive Lasso and its oracle properties", "author": ["H. Zou"], "venue": "Journal of the American Statistical Association 101: 1418-1429", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2006}], "referenceMentions": [{"referenceID": 4, "context": "The most basic one is the restricted Boltzmann machine (RBM) [5, 13], a Markov random field model with a bipartite structure containing a layer of hidden nodes and a layer of visible nodes.", "startOffset": 61, "endOffset": 68}, {"referenceID": 4, "context": "These models can be calibrated with a combination of the stochastic gradient descent and the contrastive divergence (CD) algorithm [5, 13] (or the PCD algorithm [21]).", "startOffset": 131, "endOffset": 138}, {"referenceID": 3, "context": "For regularizing unsupervised neural networks, sparse-RBM-type models [4, 10] encourage a smaller proportion of 1-valued hidden nodes.", "startOffset": 70, "endOffset": 77}, {"referenceID": 1, "context": "Therefore, Dropout can be viewed approximately as the adaptive L regularization [2, 22, 24].", "startOffset": 80, "endOffset": 91}, {"referenceID": 1, "context": "A recursive approximation of Dropout is provided in [2] using normalized weighted geometric means to show its averaging properties.", "startOffset": 52, "endOffset": 55}, {"referenceID": 0, "context": "For instance, Standout [1] varies Dropout probabilities for different nodes which constitute a binary belief network.", "startOffset": 23, "endOffset": 26}, {"referenceID": 4, "context": "The right-hand side of (8) is approximated by contrastive divergence with k steps of Gibbs sampling (CD-k) [5, 13].", "startOffset": 107, "endOffset": 114}, {"referenceID": 2, "context": "It is shown in [3, 8] that adding layers to a DBN improves the likelihood given symmetry of the weights of two adjacent layers.", "startOffset": 15, "endOffset": 21}, {"referenceID": 5, "context": "9 for DO/DC/BE/IBE; \u03bb = 10\u22125 \u223c 10\u22124 for L, similar to [7]; \u03bc = 0.", "startOffset": 54, "endOffset": 57}], "year": 2017, "abstractText": "Unsupervised neural networks, such as restricted Boltzmann machines (RBMs) and deep belief networks (DBNs), are powerful tools for feature selection and pattern recognition tasks. We demonstrate that overfitting occurs in such models just as in deep feedforward neural networks, and discuss possible regularization methods to reduce overfitting. We consider weight decay, model averaging methods, and backward elimination, and propose revised model averaging methods to improve their efficiency. We also discuss the asymptotic convergence properties of these methods. Finally, we compare the performance of these methods using likelihood and classification error rates on various pattern recognition data sets.", "creator": "LaTeX with hyperref package"}}}