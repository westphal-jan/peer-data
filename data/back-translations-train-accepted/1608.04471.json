{"id": "1608.04471", "review": {"conference": "NIPS", "VERSION": "v1", "DATE_OF_SUBMISSION": "16-Aug-2016", "title": "Stein Variational Gradient Descent: A General Purpose Bayesian Inference Algorithm", "abstract": "We propose a general purpose variational inference algorithm that forms a natural counterpart of gradient descent for optimization. Our method iteratively transports a set of particles to match the target distribution, by applying a form of functional gradient descent that minimizes the KL divergence. Empirical studies are performed on various real world models and datasets, on which our method is competitive with existing state-of-the-art methods. The derivation of our method is based on a new theoretical result that connects the derivative of KL divergence under smooth transforms with Stein's identity and a recently proposed kernelized Stein discrepancy, which is of independent interest.", "histories": [["v1", "Tue, 16 Aug 2016 03:24:20 GMT  (3890kb,D)", "http://arxiv.org/abs/1608.04471v1", "To appear in NIPS 2016"], ["v2", "Fri, 19 Aug 2016 05:13:47 GMT  (3890kb,D)", "http://arxiv.org/abs/1608.04471v2", "To appear in NIPS 2016"]], "COMMENTS": "To appear in NIPS 2016", "reviews": [], "SUBJECTS": "stat.ML cs.LG", "authors": ["qiang liu", "dilin wang"], "accepted": true, "id": "1608.04471"}, "pdf": {"name": "1608.04471.pdf", "metadata": {"source": "CRF", "title": "Stein Variational Gradient Descent: A General Purpose Bayesian Inference Algorithm", "authors": ["Qiang Liu", "Dilin Wang"], "emails": ["dilin.wang.gr}@dartmouth.edu"], "sections": [{"heading": "1 Introduction", "text": "This year, it is closer than ever before in the history of the country."}, {"heading": "2 Background", "text": "Preliminary Let x be a continuous random variable or parameter of interest taking values in X-Rd = > q functions are (unfortunately) a series of i.i.d. observations. With previous p0 (x), Bayesian's inference of x includes reasoning with the posterior distribution p (x): = p \u00b2 (x) / Z with p \u00b2 (x) for convenience. We have dropped conditioning on data {Dk} in p (x) for convenience. Let k (x) x \u00b2 s for convenience. Let k (x): X \u00b2 s be for a positive definitive kernel. The reproducing kernel Hilbert Space (RKHS) H of k (x, x \u00b2) is the closing of the linear chip (f)."}, {"heading": "3 Variational Inference Using Smooth Transforms", "text": "\"For the first time in my life, I have been able to do this for the first time. I have been able to do it for the first time. I have been able to do it for the first time. I have been able to do it for the first time. I have been able to do it for the last few years. I have been able to do it for the first time. I have been able to do it for the first time. I have been able to do it for the first time. I have been able to do it for the last few years. I have been able to do it for the first time. I have been able to do it for the second time. I have been able to do it for the first time. I have been able to do it for the first time. I have been able to do it for the first time."}, {"heading": "3.1 Stein Operator as the Derivative of KL Divergence", "text": "To explain how we minimize the KL divergence in (4), we consider an incremental q transformation formed by a small disturbance of the identity card: T (x) = x-x-x (x), where \u03c6 (x) is a smooth function that characterizes the direction of the disturbance and the scalar represents the disturbance magnitude. The following result, which forms the basis of our method, draws a revealing connection between the stone operator and the derivation of the KL divergence. r.t. The perturbation magnitude is guaranteed to be a one-to-one function theorem.Let T (x) = x + digit (z) determine the density of z = T (x) when we have the KL divergence."}, {"heading": "3.2 Stein Variational Gradient Descent", "text": "In order to implement the iterative method (7) in practice, the expectation for the calculation of \"q q\" = \"q\" = \"q\" = \"q\" = \"q\" = \"q\" = \"q\" = \"q\" = \"q\" = \"q\" = \"q\" = \"q\" = \"q\" = \"q\" = \"q\" = \"x\" = \"x\" = \"=\" x \"=\" x \"=\" = \"x\" = \"=\" x \"=\" = \"x\" = \"=\" x \"=\" = \"x\" = \"=\" x \"=\" = \"=\" x \"=\" = \"=\" = \"=\" x \"=\" = \"=\" = \"=\" = \"\" = \"\" \"=\" \"\" \"=\" \"=\" \"\" \"\" = \"\" \"\" \"\" = \"\" \"\" = \"\" = \"\" \"\" = \"\" = \"\" = \"\" = \"=\" = \"=\" = \"=\" = \"=\" = \"=\" = \"=\" = \"=\" = \"=\" = \"=\" = \"=\" = \"=\" = \"=\" = \"=\" = \"=\" = \"=\" = \"=\" = \"=\" = \"=\" = \"=\" = \"=\" = \"=\" = \"=\" = \"=\" = \"=\" = \"=\" = \"=\" = \"=\" = \"=\" = \"=\" = \"=\" = \"=\" = \"=\" = \"=\" = \"=\" = \"=\" = \"=\" = \"=\" = \"=\" = \"=\" = \"=\" = \"=\" = \"=\" = \"\" = \"=\" = \"=\" = \"\" = \"=\" = \"=\" = \"=\" = \"=\" = \"=\" = \"=\" \"=\" = \"\" \"\" = \"=\" = \"\" = \"\" = \"\" \"=\" \"=\" \"=\" \"=\" = \"=\" \"=\" = \"=\" \"\" = \"=\" \"\" \"\" = \"=\" = \"=\" = \"\" = \"=\" \"=\" \"\" \"=\" \"=\" = \"=\" = \""}, {"heading": "4 Related Works", "text": "Our work largely refers to Rezende and Mohamed [13], who also consider variational conclusions about the set of transformed random variables, but focuses on transformations of the parametric form T (x) = f '(\u00b7 \u00b7 (f1 (f0 (x))), where fi (\u00b7) is a predefined simple parametric transformation and \"a predefined length,\" essentially creating a forward-facing neural network with \"layers whose invertedness requires further conditions beyond the parameters and needs to be determined on a case-by-case basis.\" The similar idea is also discussed in Marzouk et al. [14], which also transforms particle forms that are parameterized in a special way to ensure the inverted and computer-assisted tractability of the jacobic matrix. Recently, Tran et al. [18] have constructed a ationvarial family that achieves a universal approximation that is based on a jacogenic, but not on a gausal-wide process."}, {"heading": "5 Experiments", "text": "We test our algorithms against examples of the toy world and the real world, where we find our method to outperform a variety of basic methods.Our code is available at http: / / github.com / DartML / Stein-Gradient-Descent.For all our experiments, we use RBF kernel k (x, x \u2032) = exp (\u2212 1h), and take the bandwidth to be h = med2 / log n, where med is the median the pairwise distance between the current points {xi} ni = 1; this is based on the intuition that we would improved j (xi, xj) or exp."}, {"heading": "6 Conclusion", "text": "We propose a simple general-purpose variation algorithm for a quick and scalable Bayesian inference. Future directions include more theoretical understanding of our method, more practical applications in deep-learning models, and other potential applications of our basic theorem in Section 3.1.6https: / / github.com / HIPS / Probabilistic-Backpropagation"}, {"heading": "A Proof of Theorem 3.1", "text": "Lemma A.1. Let q and p be two smooth densities, and T = T (x) a one-to-one transformation on X, which is indexed by parameters, and T is differentiable. Let's define q [T] as the density of z = T (x), if x \u00b2 q and sp = x log p (x), we have proof that the density of z = T \u2212 1 (x) > T (x) > T (x) + track (x) = q (T (x)) \u2212 1 \u00b7 xT (x)]. Let's say by p [T \u2212 1] (z) the density of z = T \u2212 1 (x) > T \u2212 1 (x), thenq [T \u2212 1] (x) = q (T (x), \u00b7 det (x)."}, {"heading": "B Proof of Theorem 3.3", "text": "Let Hd = H \u00b7 \u00b7 \u00b7 H be a vector value RKHS, and F [f] q [f] q [f] q (f] q (f) q (f) f (f), f [f] f [f] f [\u00b7 g] is a function inHd (f + g) = F [f] + < f [f] f (f] f (f) \u2212 f (f) f (f) f (f) f (f), f (f) f (f) f (f) f (f), f (f) f (f) f (f), f (f) f (f), f (f) f (f), f (f) f (f), f (f) (f), f (f) (f), f (f), f (f) (f (f), f (f), f (f (f), f (f (f), f (f (f), f (f), f (f (f), f (f (f), f (f (f), f (f (f), f (f), f (f (f), f (f (f), f (f), f (f (f), f (f), f (f (f), f (f (f), f (f (f), f (f (f), f (f (f (f), f (f (f), f (f (f (f), f (f (f), f (f (f (f), f (f (f), f (f (f), f (f (f), f (f (f (f (f), f (f (f), f (f (f), f (f), f (f (f (f (f), f (f), f (f (f), f (f (f (f), f (f (f), f), f (f), f (f (f), f (f (f), f (f (f), f), f (f (f (f), f (f (f), f (f), f (f)"}, {"heading": "C Connection with de Bruijn\u2019s identity and Fisher Divergence", "text": "If we take in (5) that (5) is reduced to KL (q [T] | | p) = 0 = \u2212 F (q, p), where F (q, p) is the Fisher divergence between p and q, we can show that asF (q, p) = Eq (x x x log p) = 0 = \u2212 F (q, p) is defined. Note that this can be treated as a deterministic version of de Bruijn's identity [35, 36] which establishes a similar connection between KL and Fisher divergence, but uses a randomized linear transformation T (x) = x + \u0432\u0430\u0441\u0442\u0435\u0441\u0442\u0435\u0441\u0442\u043e\u0441\u0442\u043e\u0441\u043e\u0441\u043e\u0441\u0442\u043e\u0441\u043e\u0441\u043e\u0441\u043e\u0441\u043e\u0441\u0442\u043e\u0441\u0442\u043e\u0441\u043e\u0441\u043e\u0441\u043e\u0441\u043e\u0441\u043e\u0441\u043e\u0441\u043e\u0441\u043e\u0441\u043e\u0441\u043e\u0441\u043e\u0441\u043e\u0441\u043e\u0441\u043e\u0441\u043e\u0441\u043e\u0441\u043e\u0441\u043e\u0441\u043e\u043d\u0438\u043d\u0438\u043d\u0438\u0439, which is a standard Gaussian noise."}, {"heading": "D Additional Experiments", "text": "We collect additional experimental results that cannot fit into the main paper.D.1 Bayesian Logistic Regression on Small Datasets We consider the Bayesian logistic regression model for binary classification, on which regression weights w are mapped with a Gaussian previous p0 (w) = N (w, \u03b1 \u2212 1) and p0 (\u03b1) = E (\u03b1, a, b), and apply to the trailing p (x | D), where x = [w, log\u03b1]. The hyperparameter is assumed to be a = 1 and b = 0.01. This setting is the same as for Gershman et al. [5]. We compare our algorithm with the No-U-Turn-Sampler (NUTS) 7 [29] and non-parametric variation conclusions (NPV) 8 on the 8 datasets (N > 500), as for Gershman et al. [5], in which we use 100 particles."}], "references": [{"title": "Stochastic variational inference", "author": ["M.D. Hoffman", "D.M. Blei", "C. Wang", "J. Paisley"], "venue": "JMLR,", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2013}, {"title": "Bayesian learning via stochastic gradient Langevin dynamics", "author": ["M. Welling", "Y.W. Teh"], "venue": "ICML,", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2011}, {"title": "Firefly Monte Carlo: Exact MCMC with subsets of data", "author": ["D. Maclaurin", "R.P. Adams"], "venue": "UAI,", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2014}, {"title": "Black box variational inference", "author": ["R. Ranganath", "S. Gerrish", "D.M. Blei"], "venue": "AISTATS,", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2014}, {"title": "Nonparametric variational inference", "author": ["S. Gershman", "M. Hoffman", "D. Blei"], "venue": "ICML,", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2012}, {"title": "Automatic variational inference in STAN", "author": ["A. Kucukelbir", "R. Ranganath", "A. Gelman", "D. Blei"], "venue": "NIPS,", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2015}, {"title": "Provable Bayesian inference via particle mirror descent", "author": ["B. Dai", "N. He", "H. Dai", "L. Song"], "venue": "AISTATS,", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2016}, {"title": "A kernelized Stein discrepancy for goodness-of-fit tests and model evaluation", "author": ["Q. Liu", "J.D. Lee", "M.I. Jordan"], "venue": "ICML,", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2016}, {"title": "Control functionals for Monte Carlo integration", "author": ["C.J. Oates", "M. Girolami", "N. Chopin"], "venue": "Journal of the Royal Statistical Society, Series B,", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2016}, {"title": "A kernel test of goodness-of-fit", "author": ["K. Chwialkowski", "H. Strathmann", "A. Gretton"], "venue": "ICML,", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2016}, {"title": "Measuring sample quality with Stein\u2019s method", "author": ["J. Gorham", "L. Mackey"], "venue": "NIPS, pages 226\u2013234,", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2015}, {"title": "Optimal transport: old and new, volume 338", "author": ["C. Villani"], "venue": "Springer Science & Business Media,", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2008}, {"title": "Variational inference with normalizing flows", "author": ["D.J. Rezende", "S. Mohamed"], "venue": "ICML,", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2015}, {"title": "An introduction to sampling via measure transport", "author": ["Y. Marzouk", "T. Moselhy", "M. Parno", "A. Spantini"], "venue": "arXiv preprint arXiv:1602.05023,", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2016}, {"title": "Mean field simulation for Monte Carlo integration", "author": ["P. Del Moral"], "venue": "CRC Press,", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2013}, {"title": "Probability and related topics in physical sciences, volume 1", "author": ["M. Kac"], "venue": "American Mathematical Soc.,", "citeRegEx": "16", "shortCiteRegEx": null, "year": 1959}, {"title": "Random features for large-scale kernel machines", "author": ["A. Rahimi", "B. Recht"], "venue": "NIPS, pages 1177\u20131184,", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2007}, {"title": "Variational Gaussian process", "author": ["D. Tran", "R. Ranganath", "D.M. Blei"], "venue": "ICLR,", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2016}, {"title": "Doubly stochastic variational Bayes for non-conjugate inference", "author": ["M. Titsias", "M. L\u00e1zaro-Gredilla"], "venue": "ICML, pages 1971\u20131979,", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2014}, {"title": "Affine independent variational inference", "author": ["E. Challis", "D. Barber"], "venue": "NIPS,", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2012}, {"title": "Variational Gaussian copula inference", "author": ["S. Han", "X. Liao", "D.B. Dunson", "L. Carin"], "venue": "AISTATS,", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2016}, {"title": "Copula variational inference", "author": ["D. Tran", "D.M. Blei", "E.M. Airoldi"], "venue": "NIPS,", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2015}, {"title": "Approximating posterior distributions in belief networks using mixtures", "author": ["C.M.B.N. Lawrence", "T.J.M.I. Jordan"], "venue": "NIPS,", "citeRegEx": "23", "shortCiteRegEx": null, "year": 1998}, {"title": "Improving the mean field approximation via the use of mixture distributions", "author": ["T.S. Jaakkola", "M.I. Jordon"], "venue": "Learning in graphical models, pages 163\u2013173. MIT Press,", "citeRegEx": "24", "shortCiteRegEx": null, "year": 1999}, {"title": "Variational inference in probabilistic models", "author": ["N.D. Lawrence"], "venue": "PhD thesis, University of Cambridge,", "citeRegEx": "25", "shortCiteRegEx": null, "year": 2001}, {"title": "Variational particle approximations", "author": ["T.D. Kulkarni", "A. Saeedi", "S. Gershman"], "venue": "arXiv preprint arXiv:1402.5715,", "citeRegEx": "26", "shortCiteRegEx": null, "year": 2014}, {"title": "Monte Carlo statistical methods", "author": ["C. Robert", "G. Casella"], "venue": "Springer Science & Business Media,", "citeRegEx": "27", "shortCiteRegEx": null, "year": 2013}, {"title": "Sequential Monte Carlo methods in practice", "author": ["A. Smith", "A. Doucet", "N. de Freitas", "N. Gordon"], "venue": "Springer Science & Business Media,", "citeRegEx": "28", "shortCiteRegEx": "28", "year": 2013}, {"title": "The No-U-Turn sampler: Adaptively setting path lengths in Hamiltonian Monte Carlo", "author": ["M.D. Hoffman", "A. Gelman"], "venue": "The Journal of Machine Learning Research, 15(1):1593\u20131623,", "citeRegEx": "29", "shortCiteRegEx": null, "year": 2014}, {"title": "Probabilistic backpropagation for scalable learning of Bayesian neural networks", "author": ["J.M. Hern\u00e1ndez-Lobato", "R.P. Adams"], "venue": "ICML,", "citeRegEx": "30", "shortCiteRegEx": null, "year": 2015}, {"title": "Use of exchangeable pairs in the analysis of simulations. In Stein\u2019s Method, pages 1\u201325", "author": ["C. Stein", "P. Diaconis", "S. Holmes", "G. Reinert"], "venue": "Institute of Mathematical Statistics,", "citeRegEx": "31", "shortCiteRegEx": "31", "year": 2004}, {"title": "Stochastic expectation propagation", "author": ["Y. Li", "J.M. Hern\u00e1ndez-Lobato", "R.E. Turner"], "venue": "NIPS,", "citeRegEx": "32", "shortCiteRegEx": null, "year": 2015}, {"title": "Variational inference with Renyi divergence", "author": ["Y. Li", "R.E. Turner"], "venue": "arXiv preprint arXiv:1602.02311,", "citeRegEx": "33", "shortCiteRegEx": null, "year": 2016}, {"title": "Dropout as a Bayesian approximation: Representing model uncertainty in deep learning", "author": ["Y. Gal", "Z. Ghahramani"], "venue": "arXiv preprint arXiv:1506.02142,", "citeRegEx": "34", "shortCiteRegEx": null, "year": 2015}, {"title": "Elements of information theory", "author": ["T.M. Cover", "J.A. Thomas"], "venue": "John Wiley & Sons,", "citeRegEx": "35", "shortCiteRegEx": null, "year": 2012}, {"title": "Interpretation and generalization of score matching", "author": ["S. Lyu"], "venue": "UAI, pages 359\u2013366,", "citeRegEx": "36", "shortCiteRegEx": null, "year": 2009}], "referenceMentions": [{"referenceID": 7, "context": "smooth variable transforms and a recently introduced kernelized Stein discrepancy [8\u201310], which allows us to derive a closed form solution for the optimal smooth perturbation direction that gives the steepest descent on the KL divergence within the unit ball of a reproducing kernel Hilbert space (RKHS).", "startOffset": 82, "endOffset": 88}, {"referenceID": 8, "context": "smooth variable transforms and a recently introduced kernelized Stein discrepancy [8\u201310], which allows us to derive a closed form solution for the optimal smooth perturbation direction that gives the steepest descent on the KL divergence within the unit ball of a reproducing kernel Hilbert space (RKHS).", "startOffset": 82, "endOffset": 88}, {"referenceID": 9, "context": "smooth variable transforms and a recently introduced kernelized Stein discrepancy [8\u201310], which allows us to derive a closed form solution for the optimal smooth perturbation direction that gives the steepest descent on the KL divergence within the unit ball of a reproducing kernel Hilbert space (RKHS).", "startOffset": 82, "endOffset": 88}, {"referenceID": 10, "context": "Traditionally, F is taken to be sets of functions with bounded Lipschitz norms, which unfortunately casts a challenging functional optimization problem that is computationally intractable or requires special considerations (see Gorham and Mackey [11] and reference therein).", "startOffset": 246, "endOffset": 250}, {"referenceID": 7, "context": "[8], KSD is defined as", "startOffset": 0, "endOffset": 3}, {"referenceID": 7, "context": "The optimal solution of (2) has been shown [8\u201310] to be \u03c6(x) = \u03c6q,p(x)/||\u03c6 \u2217 q,p||Hd , where \u03c6q,p(\u00b7) = Ex\u223cq[Apk(x, \u00b7)], for which we have S(q, p) = ||\u03c6 \u2217 q,p||2Hd .", "startOffset": 43, "endOffset": 49}, {"referenceID": 8, "context": "The optimal solution of (2) has been shown [8\u201310] to be \u03c6(x) = \u03c6q,p(x)/||\u03c6 \u2217 q,p||Hd , where \u03c6q,p(\u00b7) = Ex\u223cq[Apk(x, \u00b7)], for which we have S(q, p) = ||\u03c6 \u2217 q,p||2Hd .", "startOffset": 43, "endOffset": 49}, {"referenceID": 9, "context": "The optimal solution of (2) has been shown [8\u201310] to be \u03c6(x) = \u03c6q,p(x)/||\u03c6 \u2217 q,p||Hd , where \u03c6q,p(\u00b7) = Ex\u223cq[Apk(x, \u00b7)], for which we have S(q, p) = ||\u03c6 \u2217 q,p||2Hd .", "startOffset": 43, "endOffset": 49}, {"referenceID": 11, "context": "We refer the readers to Villani [12] for in-depth discussion on this topic.", "startOffset": 32, "endOffset": 36}, {"referenceID": 15, "context": "In addition, the distribution of each particle x`i0 , for any fixed i0, also tends to q`, and is independent with any other finite subset of particles as n\u2192\u221e, a phenomenon called propagation of chaos [16].", "startOffset": 200, "endOffset": 204}, {"referenceID": 16, "context": "If there is a need for very large n, one can approximate the summation \u2211n i=1 in (8) by subsampling the particles, or using a random feature expansion of the kernel k(x, x\u2032) [17].", "startOffset": 174, "endOffset": 178}, {"referenceID": 12, "context": "Our work is mostly related to Rezende and Mohamed [13], which also considers variational inference over the set of transformed random variables, but focuses on transforms of parametric form T (x) = f`(\u00b7 \u00b7 \u00b7 (f1(f0(x)))) where fi(\u00b7) is a predefined simple parametric transform and ` a predefined length; this essentially creates a feedforward neural network with ` layers, whose invertibility requires further conditions on the parameters and need to be established case by case.", "startOffset": 50, "endOffset": 54}, {"referenceID": 13, "context": "[14], which also considers transforms parameterized in special ways to ensure the invertible and the computational tractability of the Jacobian matrix.", "startOffset": 0, "endOffset": 4}, {"referenceID": 17, "context": "[18] constructed a variational family that achieves universal approximation based on Gaussian process (equivalent to a single-layer, infinitely-wide neural network), which does not have a Jacobian matrix but needs to calculate the inverse of the kernel matrix of the Gaussian process.", "startOffset": 0, "endOffset": 4}, {"referenceID": 18, "context": "Several other works also leverage variable transforms in variational inference, but with more limited forms; examples include affine transforms [19, 20], and recently the copula models that correspond to element-wise transforms over the individual variables [21, 22].", "startOffset": 144, "endOffset": 152}, {"referenceID": 19, "context": "Several other works also leverage variable transforms in variational inference, but with more limited forms; examples include affine transforms [19, 20], and recently the copula models that correspond to element-wise transforms over the individual variables [21, 22].", "startOffset": 144, "endOffset": 152}, {"referenceID": 20, "context": "Several other works also leverage variable transforms in variational inference, but with more limited forms; examples include affine transforms [19, 20], and recently the copula models that correspond to element-wise transforms over the individual variables [21, 22].", "startOffset": 258, "endOffset": 266}, {"referenceID": 21, "context": "Several other works also leverage variable transforms in variational inference, but with more limited forms; examples include affine transforms [19, 20], and recently the copula models that correspond to element-wise transforms over the individual variables [21, 22].", "startOffset": 258, "endOffset": 266}, {"referenceID": 22, "context": "[23\u201326, 5].", "startOffset": 0, "endOffset": 10}, {"referenceID": 23, "context": "[23\u201326, 5].", "startOffset": 0, "endOffset": 10}, {"referenceID": 24, "context": "[23\u201326, 5].", "startOffset": 0, "endOffset": 10}, {"referenceID": 25, "context": "[23\u201326, 5].", "startOffset": 0, "endOffset": 10}, {"referenceID": 4, "context": "[23\u201326, 5].", "startOffset": 0, "endOffset": 10}, {"referenceID": 4, "context": "[5] by approximating the entropy using Jensen\u2019s inequality and the expectation term using Taylor approximation.", "startOffset": 0, "endOffset": 3}, {"referenceID": 6, "context": ", 27, 28], as well as a recent particle mirror descent for optimizing the variational objective function [7]; compared with these methods, our method does not have the weight degeneration problem, and is much more \u201cparticle-efficient\u201d in that we reduce to MAP with only one single particle.", "startOffset": 105, "endOffset": 108}, {"referenceID": 6, "context": "[7], tend to experience weight degeneracy on this toy example due to the ill choice of q0(x).", "startOffset": 0, "endOffset": 3}, {"referenceID": 4, "context": "[5], which assigns the regression weights w with a Gaussian prior p0(w|\u03b1) = N (w,\u03b1\u22121) and p0(\u03b1) = Gamma(\u03b1, 1, 0.", "startOffset": 0, "endOffset": 3}, {"referenceID": 28, "context": "We compared our algorithm with the no-U-turn sampler (NUTS)1 [29] and non-parametric variational inference (NPV)2 [5] on the 8 datasets (N > 500) used in Gershman et al.", "startOffset": 61, "endOffset": 65}, {"referenceID": 4, "context": "We compared our algorithm with the no-U-turn sampler (NUTS)1 [29] and non-parametric variational inference (NPV)2 [5] on the 8 datasets (N > 500) used in Gershman et al.", "startOffset": 114, "endOffset": 117}, {"referenceID": 4, "context": "[5], and find they tend to give very similar results on these (relatively simple) datasets; see Appendix for more details.", "startOffset": 0, "endOffset": 3}, {"referenceID": 1, "context": "Because NUTS and NPV do not have mini-batch option in their code, we instead compare with the stochastic gradient Langevin dynamics (SGLD) by Welling and Teh [2], the particle mirror descent (PMD) by Dai et al.", "startOffset": 158, "endOffset": 161}, {"referenceID": 6, "context": "[7], and the doubly stochastic variational inference (DSVI) by Titsias and L\u00e1zaro-Gredilla [19].", "startOffset": 0, "endOffset": 3}, {"referenceID": 18, "context": "[7], and the doubly stochastic variational inference (DSVI) by Titsias and L\u00e1zaro-Gredilla [19].", "startOffset": 91, "endOffset": 95}, {"referenceID": 1, "context": "55 for both as suggested by Welling and Teh [2] for fair comparison; 5 we select a using a validation set within the training set.", "startOffset": 44, "endOffset": 47}, {"referenceID": 6, "context": "[7] which we find works most efficiently for PMD.", "startOffset": 0, "endOffset": 3}, {"referenceID": 29, "context": "Bayesian Neural Network We compare our algorithm with the probabilistic back-propagation (PBP) algorithm by Hern\u00e1ndez-Lobato and Adams [30] on Bayesian neural networks.", "startOffset": 135, "endOffset": 139}, {"referenceID": 30, "context": "[31]).", "startOffset": 0, "endOffset": 4}], "year": 2017, "abstractText": "We propose a general purpose variational inference algorithm that forms a natural counterpart of gradient descent for optimization. Our method iteratively transports a set of particles to match the target distribution, by applying a form of functional gradient descent that minimizes the KL divergence. Empirical studies are performed on various real world models and datasets, on which our method is competitive with existing state-of-the-art methods. The derivation of our method is based on a new theoretical result that connects the derivative of KL divergence under smooth transforms with Stein\u2019s identity and a recently proposed kernelized Stein discrepancy, which is of independent interest.", "creator": "LaTeX with hyperref package"}}}