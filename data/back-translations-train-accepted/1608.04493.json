{"id": "1608.04493", "review": {"conference": "NIPS", "VERSION": "v1", "DATE_OF_SUBMISSION": "16-Aug-2016", "title": "Dynamic Network Surgery for Efficient DNNs", "abstract": "Deep learning has become a ubiquitous technology to improve machine intelligence. However, most of the existing deep models are structurally very complex, making them difficult to be deployed on the mobile platforms with limited computational power. In this paper, we propose a novel network compression method called dynamic network surgery, which can remarkably reduce the network complexity by making on-the-fly connection pruning. Unlike the previous methods which accomplish this task in a greedy way, we properly incorporate connection splicing into the whole process to avoid incorrect pruning and make it as a continual network maintenance. The effectiveness of our method is proved with experiments. Without any accuracy loss, our method can efficiently compress the number of parameters in LeNet-5 and AlexNet by a factor of $\\bm{108}\\times$ and $\\bm{17.7}\\times$ respectively, proving that it outperforms the recent pruning method by considerable margins. Code will be made publicly available.", "histories": [["v1", "Tue, 16 Aug 2016 06:23:05 GMT  (1327kb,D)", "http://arxiv.org/abs/1608.04493v1", "Accepted by NIPS 2016"], ["v2", "Thu, 10 Nov 2016 00:17:25 GMT  (1229kb,D)", "http://arxiv.org/abs/1608.04493v2", "Accepted by NIPS 2016"]], "COMMENTS": "Accepted by NIPS 2016", "reviews": [], "SUBJECTS": "cs.NE cs.CV cs.LG", "authors": ["yiwen guo", "anbang yao", "yurong chen"], "accepted": true, "id": "1608.04493"}, "pdf": {"name": "1608.04493.pdf", "metadata": {"source": "CRF", "title": "Dynamic Network Surgery for Efficient DNNs", "authors": ["Yiwen Guo", "Anbang Yao", "Yurong Chen"], "emails": ["yiwen.guo@intel.com", "anbang.yao@intel.com", "yurong.chen@intel.com"], "sections": [{"heading": "1 Introduction", "text": "In fact, it is the case that most of them are in a position to go into another world, in which they are able to move, in which they are able to move, in which they move, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live."}, {"heading": "2 Related Works", "text": "In fact, it is a reactionary project, which is a reactionary project, which is a reactionary project, which is a reactionary project, which is a reactionary project, which is a reactionary project, and which is a reactionary project."}, {"heading": "3 Dynamic Network Surgery", "text": "In this section we will highlight the intuition of our method and present its details for implementation. To simplify the explanations, we will only talk about the twisted layers and the completely connected layers. However, as stated in [8], our cutting method can also be applied to other layer types as long as their underlying mathematical operations are internal products in vector spaces."}, {"heading": "3.1 Notations", "text": "First, we clarify the notations in this thesis. Suppose a DNN model can be represented as {Wk: 0 \u2264 k \u2264 C}, where Wk denotes a matrix of connection weights in the kest layer. For fully connected layers with p-dimensional input and q-dimensional output, the size of Wk is simply qk \u00b7 pk. For the revolutionary layers with learnable cores, we unfold the coefficients of each core into a vector and concatenate them all to Wk as a matrix. To represent a frugal model where a portion of the connections are cut off, we use {Wk, Tk: 0 \u2264 k \u2264 C}. Each Tk is a binary matrix with entries indicating the state of the network connections, i.e. whether they are cropped or not. Therefore, these additional matrices can be considered mask matrices."}, {"heading": "3.2 Pruning and Splicing", "text": "Since our goal is the network loss function, the desired model is learned from its dense reference. (Apparently, the key to this is to give up unimportant parameters and keep the important ones. (Note: The network connection between the networked neurons may be redundant, but it becomes crucial once the others are removed.) Therefore, it should be more appropriate to perform a learning process and continuously maintain the network structure. (Taking the network layer as an example, we suggest solving the following optimization problem: min Wk, TkL = hk (i, j) k (i, j) k = hk (i, j) I, (1) where L (\u00b7) displays the network loss function."}, {"heading": "3.3 Parameter Importance", "text": "Since the degree of parameter significance influences the state of the network connections, the function hk (\u00b7), \u0430 0 \u2264 k \u2264 C, can be crucial for our dynamic network surgery. We tested several candidates and finally found the absolute value of the input as the best choice, as stated in [9]. That is, the relatively small parameters are temporarily truncated, while the others are largely maintained or spliced in each iteration of algorithm 1. Obviously, the thresholds have a significant impact on the final compression rate. For a certain layer, a single threshold can be set based on the average absolute value and the variance of its connection weights. However, to improve the robustness of our method, we use two thresholds ak and bk by importing a small margin t and putting bk as ak + t into equation (3). For the parameters outside this range, we set their function outputs as the corresponding entries in k, which means neither Tk nor W (nor does this parameter be split)."}, {"heading": "3.4 Convergence Acceleration", "text": "In view of the fact that algorithm 1 is somewhat more complicated than the standard method of backflow, we will take a few more steps to increase its convergence. First, we propose to slow down the frequency of backflow and splitting, as these processes lead to a change in the network structure, which can be done by stochastically triggering the update scheme of Tk, with a probability of p = \u03c3 (iter) instead of doing it all the time. \u03c3 (\u00b7) should not be monotonous and \u03c3 (0) = 1 should satisfy. After a prolonged reduction, the probability of p can even be set to zero, i.e. there will be no more backflow or splitting. Another possible reason for the slow convergence is the problem of the disappearing gradient. As a large percentage of connections is cut away, the network structure should become much simpler and probably even much \"thinner\" by using our method."}, {"heading": "4 Experimental Results", "text": "In this section, we will analyze the proposed method experimentally and apply it to some popular network models.2 For fair comparison and easy reproduction, all reference models will be trained through the GPU implementation of the Caffe package [12] with.prototxt files provided by the community. 2 We will also follow the standard experimental settings for the SGD method, including training batch size, base learning rate, learning guidelines, and maximum number of training siterations. Once the reference models are available, we will apply our method directly to reduce their model complexity. A brief summary of the compression results is presented in Table 1.2, except for the simulation experiment and the LeNet 300-100 experiments that we create ourselves. Prototxt files, as they are not available in the Caffe model zoo."}, {"heading": "4.1 The Exclusive-OR Problem", "text": "It is a non-linear classification problem, as illustrated in Figure 3 (a). In this experiment, we turn to a more complicated one than Figure 3 (b), in which some gouges are confused with the original data. (0), (1), (1), (2), (3), (4), (5), (5), (5), (5), (5), (5), (5), (5), (5), (5), (5), (5), (5), (5), (5), (5), (5), (5), (5), (5), (5), (5), (5), (5), (5), (5), (5), (5), (5), (5), (5), (5), (5), (5), (5), (5), (5), (5, (5), (5), (5), (5), (5), (5), (5), (5), (5), (5), (5), (5), (5, (5), (5), (5), (5), (5), (5), (5, (5), (5), (5), (5), (5, (5), (5), (5), (5), (5, (5), (5), (5), (5), (5, (5), (5), (5), (5), (5), (5), (5), (5), (5, (5), (5), (5), (5, (5), (5, (5), (5), (5), (5, (5), (5), (5), (5, (5), (5), (5), (5, (5), (5), (5), (5, (5), (5), (5), (5), (5), (5), (5), ("}, {"heading": "4.2 The MNIST database", "text": "MNIST is a database of handwritten digits and it is widely used to evaluate experimentally the methods of machine learning. Same goes for [9], we test our method on two network models: LeNet-5 and LeNet300-100. LeNet-5 is a conventional CNN model that consists of 4 learnable layers, including 2 conventional layers and 2 fully connected layers. It is designed by LeCun et al. [15] for the detection of documents. With 431K parameters that need to be learned, we train this model for 10,000 iterations and get a prediction error rate of 0.91%. LeNet-300-100, as described in [15], is a classical forward-oriented neural network with three fully connected layers and 267K learnable parameters. It is also trained for 10,000 iterations that follow the same learning policy as LeNet-5. Well-trained net-100 models we are able to prioritize the two of these net-100% error models, with these two models of the 28 being a primitive model."}, {"heading": "4.3 ImageNet and AlexNet", "text": "In the final experiment, we apply our method to AlexNet [13], which won the ILSVRC 2012 Classification Contest. As in previous experiments, we first train the reference model. Without data magnification, we obtain a reference model with 61M well-learned parameters after 450K iterations of training (i.e. approximately 90 epochs), and then perform network surgery on it. AlexNet consists of 8 learnable layers that are considered deep. So, we trim the revolutionary layers and fully connected layers separately, as discussed above in Section 3.4. The size of the training program, the base learning rate, and the learning policy remain the same as the reference training process. We perform 320K iterations for the Convolutionary layers and 380K iterations for the fully connected layers, meaning that 700 K iterations in total (i.e. approximately 140 epochs) are compressed."}, {"heading": "5 Conclusions", "text": "In this thesis, we have investigated the way DNNs are compressed and proposed a new method called Dynamic Network Surgery. Unlike previous methods, where intersection and retraining are performed alternately, our method incorporates connection splicing into surgery and implements the entire process dynamically. By using our method, most parameters in the DNN models can be deleted while the predictive accuracy does not decrease. Experimental results show that our method compresses the number of parameters in LeNet-5 and AlexNet by a factor of 108 x and 17.7 x, respectively, which is considerably higher than the most recent cutting method. Furthermore, the learning efficiency of our method is better, so that fewer epochs are needed."}], "references": [{"title": "Compressing neural networks with the hashing trick", "author": ["Wenlin Chen", "James T. Wilson", "Stephen Tyree", "Kilian Q. Weinberger", "Yixin Chen"], "venue": "In ICML,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2015}, {"title": "BinaryConnect: Training deep neural networks with binary weights during propagations", "author": ["Matthieu Courbariaux", "Yoshua Bengio", "Jean-Pierre David"], "venue": "In NIPS,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2015}, {"title": "Binarized neural networks: Training deep neural networks with weights and activations constrained to +1 or -1", "author": ["Matthieu Courbariaux", "Itay Hubara", "Daniel Soudry", "Ran El-Yaniv", "Yoshua Bengio"], "venue": "arXiv preprint arXiv:1602.02830v3,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2016}, {"title": "Predicting parameters in deep learning", "author": ["Misha Denil", "Babak Shakibi", "Laurent Dinh", "Marc\u2019Aurelio Ranzato", "Nando de Freitas"], "venue": "In NIPS,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2013}, {"title": "Exploiting linear structure within convolutional networks for efficient evaluation", "author": ["Emily L. Denton", "Wojciech Zaremba", "Joan Bruna", "Yann LeCun", "Rob Fergus"], "venue": "In NIPS,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2014}, {"title": "Compressing deep convolutional networks using vector quantization", "author": ["Yunchao Gong", "Liu Liu", "Ming Yang", "Lubomir Bourdev"], "venue": "arXiv preprint arXiv:1412.6115,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2014}, {"title": "EIE: Efficient inference engine on compressed deep neural network", "author": ["Song Han", "Xingyu Liu", "Huizi Mao", "Jing Pu", "Ardavan Pedram", "Mark A Horowitz", "William J. Dally"], "venue": null, "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2016}, {"title": "Deep compression: Compressing deep neural networks with pruning, trained quantization and Huffman coding", "author": ["Song Han", "Huizi Mao", "William J. Dally"], "venue": "In ICLR,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2016}, {"title": "Learning both weights and connections for efficient neural networks", "author": ["Song Han", "Jeff Pool", "John Tran", "William J. Dally"], "venue": "In NIPS,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2015}, {"title": "Second order derivatives for network pruning: Optimal brain surgeon", "author": ["Babak Hassibi", "David G. Stork"], "venue": "In NIPS,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 1993}, {"title": "Deep residual learning for image recognition", "author": ["Kaiming He", "Xiangyu Zhang", "Shaoqing Ren", "Jian Sun"], "venue": null, "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2016}, {"title": "Caffe: Convolutional architecture for fast feature embedding", "author": ["Yangqing Jia", "Evan Shelhamer", "Jeff Donahue", "Sergey Karayev", "Jonathan Long", "Ross Girshick", "Sergio Guadarrama", "Trevor Darrell"], "venue": "In ACM MM,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2014}, {"title": "Imagenet classification with deep convolutional neural networks", "author": ["Alex Krizhevsky", "Ilya Sutskever", "Geoffrey E. Hinton"], "venue": "In NIPS,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2012}, {"title": "Speeding-up convolutional neural networks using fine-tuned cp-decomposition", "author": ["Vadim Lebedev", "Yaroslav Ganin", "Maksim Rakhuba", "Ivan Oseledets", "Victor Lempitsky"], "venue": "In ICLR,", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2015}, {"title": "Gradient-based learning applied to document recognition", "author": ["Yann LeCun", "L\u00e9on Bottou", "Yoshua Bengio", "Patrick Haffner"], "venue": "Proceedings of the IEEE,", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 1998}, {"title": "Optimal brain damage", "author": ["Yann LeCun", "John S Denker", "Sara A Solla", "Richard E Howard", "Lawrence D Jackel"], "venue": "In NIPS,", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 1989}, {"title": "Molecular Cell Biology: Neurotransmitters, Synapses, and Impulse Transmission", "author": ["Harvey Lodish", "Arnold Berk", "S Lawrence Zipursky", "Paul Matsudaira", "David Baltimore", "James Darnell"], "venue": null, "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2000}, {"title": "Fast training of convolutional networks through FFTs", "author": ["Michael Mathieu", "Mikael Henaff", "Yann LeCun"], "venue": "arXiv preprint arXiv:1312.5851,", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2013}, {"title": "Very deep convolutional networks for large-scale image recognition", "author": ["Karen Simonyan", "Andrew Zisserman"], "venue": "In ICLR,", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2014}, {"title": "Improving the speed of neural networks on CPUs", "author": ["Vincent Vanhoucke", "Andrew Senior", "Mark Z. Mao"], "venue": "In NIPS Workshop,", "citeRegEx": "20", "shortCiteRegEx": "20", "year": 2011}, {"title": "Deep fried convnets", "author": ["Zichao Yang", "Marcin Moczulski", "Misha Denil", "Nando de Freitas", "Alex Smola", "Le Song", "Ziyu Wang"], "venue": "In ICCV,", "citeRegEx": "21", "shortCiteRegEx": "21", "year": 2015}, {"title": "Efficient and accurate approximations of nonlinear convolutional networks", "author": ["Xiangyu Zhang", "Jianhua Zou", "Xiang Ming", "Kaiming He", "Jian Sun"], "venue": "In CVPR,", "citeRegEx": "22", "shortCiteRegEx": "22", "year": 2015}], "referenceMentions": [{"referenceID": 12, "context": "As a family of brain inspired models, deep neural networks (DNNs) have substantially advanced a variety of artificial intelligence tasks including image classification [13, 19, 11], natural language processing, speech recognition and face recognition.", "startOffset": 168, "endOffset": 180}, {"referenceID": 18, "context": "As a family of brain inspired models, deep neural networks (DNNs) have substantially advanced a variety of artificial intelligence tasks including image classification [13, 19, 11], natural language processing, speech recognition and face recognition.", "startOffset": 168, "endOffset": 180}, {"referenceID": 10, "context": "As a family of brain inspired models, deep neural networks (DNNs) have substantially advanced a variety of artificial intelligence tasks including image classification [13, 19, 11], natural language processing, speech recognition and face recognition.", "startOffset": 168, "endOffset": 180}, {"referenceID": 12, "context": "For instance, AlexNet [13] designed by Krizhevsky et al.", "startOffset": 22, "endOffset": 26}, {"referenceID": 14, "context": "has 61 million parameters to win the ILSVRC 2012 classification competition, which is over 100 times more than that of LeCun\u2019s conventional model [15] (e.", "startOffset": 146, "endOffset": 150}, {"referenceID": 18, "context": ", LeNet-5), let alone the much more complex models like VGGNet [19].", "startOffset": 63, "endOffset": 67}, {"referenceID": 8, "context": "Moreover, the battery capacity can be another bottleneck [9].", "startOffset": 57, "endOffset": 60}, {"referenceID": 3, "context": "Although DNN models normally require a vast number of parameters to guarantee their superior performance, significant redundancies have been reported in their parameterizations [4].", "startOffset": 177, "endOffset": 180}, {"referenceID": 8, "context": "[9] recently propose to make \"lossless\" DNN compression by deleting unimportant parameters and retraining the remaining ones (as illustrated in Figure 1(b)), somehow similar to a surgery process.", "startOffset": 0, "endOffset": 3}, {"referenceID": 8, "context": "This leads to two main issues in [9] (and some other classical methods [16, 10] as well).", "startOffset": 33, "endOffset": 36}, {"referenceID": 15, "context": "This leads to two main issues in [9] (and some other classical methods [16, 10] as well).", "startOffset": 71, "endOffset": 79}, {"referenceID": 9, "context": "This leads to two main issues in [9] (and some other classical methods [16, 10] as well).", "startOffset": 71, "endOffset": 79}, {"referenceID": 8, "context": "As in the paper [9], several iterations of alternate pruning and retraining are necessary to get a fair compression rate on AlexNet, while each retraining process consists of millions of iterations, which can be very time consuming.", "startOffset": 16, "endOffset": 19}, {"referenceID": 16, "context": "In our method, pruning and splicing naturally constitute a circular procedure and dynamically divide the network connections into two categories, akin to the synthesis of excitatory and inhibitory neurotransmitter in human nervous systems [17].", "startOffset": 239, "endOffset": 243}, {"referenceID": 8, "context": "\u2019s method [9], using AlexNet as an example.", "startOffset": 10, "endOffset": 13}, {"referenceID": 8, "context": "[9] needs more than 4800K iterations to get a fair compression rate (9\u00d7), while our method runs only 700K iterations to yield a significantly better result (17.", "startOffset": 0, "endOffset": 3}, {"referenceID": 19, "context": "[20] analyse the effectiveness of data layout, batching and the usage of Intel fixed-point instructions, making a 3\u00d7 speedup on x86 CPUs.", "startOffset": 0, "endOffset": 4}, {"referenceID": 17, "context": "[18] explore the fast Fourier transforms (FFTs) on GPUs and improve the speed of CNNs by performing convolution calculations in the frequency domain.", "startOffset": 0, "endOffset": 4}, {"referenceID": 3, "context": "[4] propose to approximate parameter matrices with appropriately constructed low-rank decompositions.", "startOffset": 0, "endOffset": 3}, {"referenceID": 4, "context": "Following similar ideas, some subsequent methods can provide more significant speedups [5, 22, 14].", "startOffset": 87, "endOffset": 98}, {"referenceID": 21, "context": "Following similar ideas, some subsequent methods can provide more significant speedups [5, 22, 14].", "startOffset": 87, "endOffset": 98}, {"referenceID": 13, "context": "Following similar ideas, some subsequent methods can provide more significant speedups [5, 22, 14].", "startOffset": 87, "endOffset": 98}, {"referenceID": 5, "context": "[6] explore several such methods and point out the effectiveness of product quantization.", "startOffset": 0, "endOffset": 3}, {"referenceID": 0, "context": "[1] handles network compression by grouping its parameters into hash buckets.", "startOffset": 0, "endOffset": 3}, {"referenceID": 1, "context": "The recently proposed BinaryConnect [2] and Binarized Neural Networks [3] are able to compress DNNs by a factor of 32\u00d7, while a noticeable accuracy loss is sort of inevitable.", "startOffset": 36, "endOffset": 39}, {"referenceID": 2, "context": "The recently proposed BinaryConnect [2] and Binarized Neural Networks [3] are able to compress DNNs by a factor of 32\u00d7, while a noticeable accuracy loss is sort of inevitable.", "startOffset": 70, "endOffset": 73}, {"referenceID": 15, "context": "\u2019s [16], which makes use of the second derivatives of loss function to balance training loss and model complexity.", "startOffset": 3, "endOffset": 7}, {"referenceID": 9, "context": "As an extension, Hassibi and Stork [10] propose to take non-diagonal elements of the Hessian matrix into consideration, producing compression results with less accuracy loss.", "startOffset": 35, "endOffset": 39}, {"referenceID": 8, "context": "[9] explore the magnitude-based pruning in conjunction with retraining, and report promising compression results without accuracy loss.", "startOffset": 0, "endOffset": 3}, {"referenceID": 6, "context": "It has also been validated that the sparse matrix-vector multiplication can further be accelerated by certain hardware design, making it more efficient than traditional CPU and GPU calculations [7].", "startOffset": 194, "endOffset": 197}, {"referenceID": 8, "context": "\u2019s method [9] is mostly its potential risk of irretrievable network damage and learning inefficiency.", "startOffset": 10, "endOffset": 13}, {"referenceID": 8, "context": "Our research on network pruning is partly inspired by [9], not only because it can be very effective to compress DNNs, but also because it makes no assumption on the network structure.", "startOffset": 54, "endOffset": 57}, {"referenceID": 7, "context": "[8] have already tested such combinations and obtained excellent results.", "startOffset": 0, "endOffset": 3}, {"referenceID": 7, "context": "However, as claimed in [8], our pruning method can also be applied to some other layer types as long as their underlying mathematical operations are inner products on vector spaces.", "startOffset": 23, "endOffset": 26}, {"referenceID": 16, "context": "Pruning and splicing constitute a circular procedure by constantly updating the connection weights and setting different entries in Tk, which is analogical to the synthesis of excitatory and inhibitory neurotransmitter in human nervous system [17].", "startOffset": 243, "endOffset": 247}, {"referenceID": 8, "context": "candidates and finally found the absolute value of the input to be the best choice, as claimed in [9].", "startOffset": 98, "endOffset": 101}, {"referenceID": 8, "context": "We resolve this problem by pruning the convolutional layers and fully connected layers separately, in the dynamic way still, which is somehow similar to [9].", "startOffset": 153, "endOffset": 156}, {"referenceID": 11, "context": "For fair comparison and easy reproduction, all the reference models are trained by the GPU implementation of Caffe package [12] with .", "startOffset": 123, "endOffset": 127}, {"referenceID": 8, "context": "However, if the pruning operations are conducted only on the initial parameter magnitude (as in [9]), then probably four hidden neurons will be finally kept, which is obviously not the optimal compression result.", "startOffset": 96, "endOffset": 99}, {"referenceID": 8, "context": "Same with [9], we test our method on two network models: LeNet-5 and LeNet300-100.", "startOffset": 10, "endOffset": 13}, {"referenceID": 14, "context": "[15] for document recognition.", "startOffset": 0, "endOffset": 4}, {"referenceID": 14, "context": "LeNet-300-100, as described in [15], is a classical feedforward neural network with three fully connected layers and 267K learnable parameters.", "startOffset": 31, "endOffset": 35}, {"referenceID": 8, "context": "Table 2: Compare our compression results on LeNet-5 and LeNet-300-100 with that of [9].", "startOffset": 83, "endOffset": 86}, {"referenceID": 8, "context": "The percentage of remaining parameters after applying Han et al\u2019s method [9] and our method are shown in the last two columns.", "startOffset": 73, "endOffset": 76}, {"referenceID": 8, "context": "% [9] Params.", "startOffset": 2, "endOffset": 5}, {"referenceID": 8, "context": "\u2019s [9] in Table 2.", "startOffset": 3, "endOffset": 6}, {"referenceID": 8, "context": "\u2019s models [9], our compressed models will also be undoubtedly much faster than theirs.", "startOffset": 10, "endOffset": 13}, {"referenceID": 12, "context": "In the final experiment, we apply our method to AlexNet [13], which wins the ILSVRC 2012 classification competition.", "startOffset": 56, "endOffset": 60}, {"referenceID": 20, "context": "Model Top-1 error Top-5 error Epochs Compression Fastfood 32 (AD) [21] 41.", "startOffset": 66, "endOffset": 70}, {"referenceID": 20, "context": "93% 2\u00d7 Fastfood 16 (AD) [21] 42.", "startOffset": 24, "endOffset": 28}, {"referenceID": 8, "context": "7\u00d7 Naive Cut [9] 57.", "startOffset": 13, "endOffset": 16}, {"referenceID": 8, "context": "[9] 42.", "startOffset": 0, "endOffset": 3}, {"referenceID": 8, "context": "\u2019s method [9] and the adaptive fastfood transform method [21].", "startOffset": 10, "endOffset": 13}, {"referenceID": 20, "context": "\u2019s method [9] and the adaptive fastfood transform method [21].", "startOffset": 57, "endOffset": 61}, {"referenceID": 8, "context": "\u2019s [9], since they achieve the second best compression rate.", "startOffset": 3, "endOffset": 6}, {"referenceID": 8, "context": "As shown in Table 4, our method compresses more parameters on almost every single layer in AlexNet, which means both the storage requirement and the number of FLOPs are better reduced when compared with [9].", "startOffset": 203, "endOffset": 206}, {"referenceID": 8, "context": "Table 4: Compare our method with [9] on AlexNet.", "startOffset": 33, "endOffset": 36}, {"referenceID": 8, "context": "% [9] Params.", "startOffset": 2, "endOffset": 5}], "year": 2016, "abstractText": "Deep learning has become a ubiquitous technology to improve machine intelligence. However, most of the existing deep models are structurally very complex, making them difficult to be deployed on the mobile platforms with limited computational power. In this paper, we propose a novel network compression method called dynamic network surgery, which can remarkably reduce the network complexity by making on-the-fly connection pruning. Unlike the previous methods which accomplish this task in a greedy way, we properly incorporate connection splicing into the whole process to avoid incorrect pruning and make it as a continual network maintenance. The effectiveness of our method is proved with experiments. Without any accuracy loss, our method can efficiently compress the number of parameters in LeNet-5 and AlexNet by a factor of 108\u00d7 and 17.7\u00d7 respectively, proving that it outperforms the recent pruning method by considerable margins. The code will be made publicly available.", "creator": "LaTeX with hyperref package"}}}