{"id": "1608.04631", "review": {"conference": "EMNLP", "VERSION": "v1", "DATE_OF_SUBMISSION": "16-Aug-2016", "title": "Neural versus Phrase-Based Machine Translation Quality: a Case Study", "abstract": "Within the field of Statistical Machine Translation (SMT), the neural approach (NMT) has recently emerged as the first technology able to challenge the long-standing dominance of phrase-based approaches (PBMT). In particular, at the IWSLT 2015 evaluation campaign, NMT outperformed well established state-of-the-art PBMT systems on English-German, a language pair known to be particularly hard because of morphology and syntactic differences. To understand in what respects NMT provides better translation quality than PBMT, we perform a detailed analysis of neural versus phrase-based SMT outputs, leveraging high quality post-edits performed by professional translators on the IWSLT data. For the first time, our analysis provides useful insights on what linguistic phenomena are best modeled by neural models -- such as the reordering of verbs -- while pointing out other aspects that remain to be improved.", "histories": [["v1", "Tue, 16 Aug 2016 15:04:18 GMT  (43kb,D)", "http://arxiv.org/abs/1608.04631v1", "Conference on Empirical Methods in Natural Language Processing (EMNLP), November 1-5, 2016, Austin, Texas, USA"], ["v2", "Sun, 9 Oct 2016 09:20:08 GMT  (187kb,D)", "http://arxiv.org/abs/1608.04631v2", "Conference on Empirical Methods in Natural Language Processing (EMNLP), November 1-5, 2016, Austin, Texas, USA"]], "COMMENTS": "Conference on Empirical Methods in Natural Language Processing (EMNLP), November 1-5, 2016, Austin, Texas, USA", "reviews": [], "SUBJECTS": "cs.CL", "authors": ["luisa bentivogli", "arianna bisazza", "mauro cettolo", "marcello federico"], "accepted": true, "id": "1608.04631"}, "pdf": {"name": "1608.04631.pdf", "metadata": {"source": "CRF", "title": "Neural versus Phrase-Based Machine Translation Quality: a Case Study", "authors": ["Luisa Bentivogli", "Arianna Bisazza", "Mauro Cettolo"], "emails": [], "sections": [{"heading": "1 Introduction", "text": "In fact, we are able to go in search of a solution that puts us in the position we are in and in search of a solution that enables us to find a solution that enables us to find a solution that enables us to find a solution that enables us to find a solution that enables us to find a solution that enables us to find a solution that enables us to find a solution and that enables us to find a solution that enables us to find a solution that enables us to find a solution that enables us to find a solution that is able to find a solution. \""}, {"heading": "2 Previous Work", "text": "To date, NMT systems have only been evaluated by BLEU in single reference systems (Bahdanau et al., 2015; Sutskever et al., 2014; Luong et al., 2015; Jean et al., 2015a; Gu'lc error analysis techniques (Bahdanau et al., 2015).The Montreal NMT system submitted to WMT 2015 (Jean et al., 2015b) was part of a manual evaluation experiment in which a large number of non-professional annotators were asked to evaluate the results of several MT systems (Bojar et al., 2015).The results for the Montreal system were very positive - first place in German, English-Czech and Czech-English - which confirmed and reinforced the previously published BLEU results. Unfortunately, neither BLEU nor manual ranking judgments say which translation aspects are better modelled by different MT frameworks."}, {"heading": "3 Experimental Setting", "text": "We are conducting a series of analyses on the data and results of the IWSLT 2015 MT En-De task, which consists of the translation of manual transcripts of English TED presentations into German. The evaluation data is available to the public via the WIT3 repository (Cettolo et al., 2012).3"}, {"heading": "3.1 Task Data", "text": "TED Talks4 is a collection of rather short speeches (max. 18 minutes per lecture, equivalent to approximately 2,500 words) on a variety of topics. All of the talks have captions that are translated into many languages by volunteers around the world. TED Talks are not only a popular benchmark for spoken language technology, but also present interesting research challenges. Translating TED Talks implies dealing with spoken rather than written language, which is therefore expected to be structurally less complex, formal, and fluent (Ruiz and Federico, 2014). Moreover, since human translations of the lectures must follow the structure and rhythm of English captions, a lower degree of reformulation and reordering is expected than with ordinary translations of written documents. As for the English-German language pair, the two languages are interesting because, although they belong to the same language family, they differ significantly in the degree of flexibility, morphological variation, and word order, especially in the reordering of verbs."}, {"heading": "3.2 Evaluation Data", "text": "Five systems participated in the MT-En-De task and were evaluated manually on a representative subset3wit3.fbk.eu 4http: / / www.ted.com / of the official 2015 test set. The Human Evaluation (HE) set includes the first half of each of the 12 test lectures with a total of 600 sentences and about 10K words. Five professional translators were asked to post-edit the MT edition using the minimum edits required to translate it into a fluent sentence with the same meaning as the source sentence. Data was prepared so that all translators were able to post-edit the five MT editions equally, i.e. 120 sentences for each rated system. The resulting evaluation data consists of five new reference translations for each sentence in the KE set. Each of these references represents the targeted translation of the system output from which it was derived, but the other four additional translations can also be used to evaluate each MT system."}, {"heading": "3.3 MT Systems", "text": "Our analysis focuses on the first four top ranking systems, which include NMT (Luong and Manning, 2015) and three different phrase-based approaches: standard phrase-based (Ha et al., 2015), hierarchical (Jehl et al., 2015) and a combination of phrase-based and syntax-based (Huck and Birch, 2015). Table 1 presents an overview of each system as well as figures on the training data used. 5The phrase + syntax-based (PBSY) system totals com-5Detailed information on training data was kindly provided by the participating teams.bines the results of a string-to-tree decoder trained with the GHKM algorithm, using those of two standard phrase-based systems, which include adapted phrase tables and language models, enriched with morphological information, hierarchical lexicalized reordering models, and different variations of the operative sequence model."}, {"heading": "3.4 Translation Edit Rate Measures", "text": "Naturally, the Translation Edit Rate (TER) (Snover et al., 2006) fits within our assessment framework, where it tracks the edits performed by post-editors. In addition, the TER shift operations are reliable indicators of the reordering of errors we are particularly interested in. We use the available post-edits in two different ways: (i) for Human-Targeted TER (HTER) we calculate the TER between machine translation and its manually edited version (Targeted Reference), (ii) for Multi-Reference TER (mTER) we calculate the TER against the narrowest translation of all available post-edits (i.e. targeted and additional references) for each sentence."}, {"heading": "4 Overall Translation Quality", "text": "Table 2 presents the overall results of the system according to HTER and mTER, as well as BLEU, calculated on the basis of the original TED Talks reference translation. We see that NMT significantly outperforms all other approaches in terms of both BLEU and TER values. Looking at the mTER results, the gain NMT achieves over the second best system (PBSY) is 26%. It is also noteworthy that mTER is significantly lower than HTER for each system. This reduction shows that using all available post boards as references for TER is a viable method of controlling and overcoming editor variability, thereby ensuring a more reliable and informative evaluation of the real overall performance of MT systems. Therefore, the two following analyses are based on mTER. In particular, we examine how specific features of input documents affect the overall translation quality of the system, focusing on (i) the sentence length and (ii) the conversations from those."}, {"heading": "4.1 Translation quality by sentence length", "text": "Following previous work (Cho et al., 2014a; Pouget-Abadie et al., 2014; Bahdanau et al., 2015; Luong et al., 2015), we examine how sentence length affects overall translation quality. Figure 1 shows mTER values compared to sentence length from the source. NMT performs significantly better in each length range than any PBMT system, with statistically significant differences. As a general trend, the performance of all approaches deteriorates with increasing sentence length. However, when sentences exceed 35 words, we see that NMT quality performs significantly worse than in PBMT systems. Looking at the percentage decrease compared to the previous length range (26-35), we see that the percentage for NMT (-15.4) is much higher than the average percentage for the three PBMT systems (-7.9)."}, {"heading": "4.2 Translation quality by talk", "text": "As we have seen in Section 3.1, the TED data set is very heterogeneous, as it consists of lectures covering different topics and delivered by speakers of different styles. It is therefore interesting to assess the translation quality at the conversation level.Figure 2 shows the mTER values for each of the twelve lectures included in the HE sentence, sorted by ascending order of the NMT values. In all lectures, the NMT system outperforms the PBMT systems in a statistically significant manner. We analyzed various factors that could affect the translation quality to understand whether they correlate with such performance differences. We examined three characteristics that are typically considered indicators of complexity (see (Franc'ois and Fairon, 2012), namely (i) the length of the lecture, (ii) its average sentence length, and (iii) the font ratio NR, which reflects the better relationship between the 6xical word and the mean number of the word."}, {"heading": "5 Analysis of Translation Errors", "text": "Various error taxonomies have been developed in the literature covering different levels of granularity (Flanagan, 1994; Vilar et al., 2006; Farru's Cabeceran et al., 2010; Stymne and Ahrenberg, 2012; Lommel et al., 2014). We focus on three error categories, namely (i) morphology errors, (ii) lexical errors and (iii) word order errors. As for lexical errors, a number of existing taxonomies continue to differentiate between translation errors due to missing words, additional words or wrong lexical choices. Given the proven difficulty of distinguishing between these three subclasses (Popovic's and Ney, 2011; Fishel et al., 2012), we prefer to rely on a coarser linguistic error classification in which all these subclasses (Popovic's and Ney, 2011; Fishel et al, 2012) are identical by translation method."}, {"heading": "5.1 Morphology errors", "text": "A morphology error occurs when a generated word form is incorrect but its basic form (lemmas) is correct. Thus, we assess the ability of systems to handle morphology by comparing the HTER value calculated on the surface forms (i.e., morphologically inflected words) with the HTER value obtained on the corresponding lemmas. The additional matches counted on lemmas in terms of word forms indicate morphology errors. The closer the two HTER values are, the more accurate the system is in dealing with morphology. To perform this analysis, the lemmatized (and POS-marked) version of both MT editions and corresponding post-edits were created with the German parser ParZu (Sennrich et al., 2013). Then the HTER-based evaluation was slightly adjusted to be more suitable for accurate detection of morphology errors. First, the interphology was not removed from the morphology error, as it is not subject to morphology errors."}, {"heading": "5.2 Lexical errors", "text": "Another important feature of MT systems is their ability to select lexical appropriate words. To compare systems under this aspect, we consider HTER results at the level of the lemmas as a way to abstract from morphology errors and focus only on actual lexical selection problems. Evaluation of the lexical data to identify morphology errors fits this purpose, as their driving assumptions (i.e. punctuation can be excluded and lexical errors in the wrong order are not errors) also apply to lexical errors. The lexical column in Table 3 shows that NMT outperforms the other systems. Specifically, the NMT value (18.7) is 3.8% better than the second best (PBSY, 22.5) absolute points. This corresponds to a relative gain of about 17%, which means that NMT makes at least 17% less lexical errors than any PBMT system. Similar to morphology errors, this can be considered a remarkable improvement over the state of the art."}, {"heading": "5.3 Word order errors", "text": "The first three columns of Table 4 each show: (i) the number of words generated by each system (ii) the number of shifts required to align each system output with the corresponding post-edit; and (iii) the corresponding percentage of shift errors. Note that the shift error rates are included in the HTER results shown in Table 2. We can see in the table that the percentage of shift errors in the NMT translations is definitely lower than for the other systems. The gap between NMT and the second-best system (PBSY) is about 50%. It should be remembered that these numbers refer only to shifts detected by HTER (groups of) words in the MT output and post edits that are identical but occur in different positions. Words that had to be shifted and modified at the same time."}, {"heading": "6 Fine-grained Word Order Error Analysis", "text": "This year, it has reached the point where it will be able to put itself at the top without being able to put itself at the top."}, {"heading": "7 Conclusions", "text": "We analyzed the results of four state-of-the-art MT systems that participated in the Anglo-German evaluation campaign of the IWSLT 2015. Our selected runs were produced by three phrase-based MT systems and a neural MT system. The analysis used high-quality post-processing of the MT results, which enabled us to profile systems in terms of reliable metrics for post-processing effort and translation error types.The results of the analysis confirm that NMT has significantly advanced the state of the art, especially in a language pair that includes rich morphological predictions and significant word reordering. To summarize our results: (i) NMT generates expenditures that significantly reduce the overall cost of post-processing with respect to the best PBMT system (-26%); (ii) NMT outperforms PBMT systems in all sentence lengths, although their performance appears to be less error-prone with text inputs than its competitors (iv) (iv)."}, {"heading": "Acknowledgments", "text": "FBK authors were supported by the CRACKER and QT21 projects, which were funded by the Horizon 2020 research and innovation programme of the European Union with funding numbers 645357 and 645452 respectively. AB's work was partly funded by the Dutch Organisation for Scientific Research (NWO) under project numbers 639.022.213 and 612.001.218."}], "references": [{"title": "Neural machine translation by jointly learning to align and translate", "author": ["Kyunghyun Cho", "Yoshua Bengio"], "venue": "In Proc. of ICLR,", "citeRegEx": "Bahdanau et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Bahdanau et al\\.", "year": 2015}, {"title": "Metrics for MT evaluation: evaluating reordering", "author": ["Miles Osborne", "Phil Blunsom"], "venue": "Machine Translation,", "citeRegEx": "Birch et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Birch et al\\.", "year": 2010}, {"title": "Reordering Metrics for Statistical Machine Translation", "author": ["Alexandra Birch"], "venue": "Ph.D. thesis, School of Informatics,", "citeRegEx": "Birch.,? \\Q2011\\E", "shortCiteRegEx": "Birch.", "year": 2011}, {"title": "Efficient solutions for word reordering in German-English phrase-based statistical machine translation", "author": ["Bisazza", "Federico2013] Arianna Bisazza", "Marcello Federico"], "venue": "In Proc. of WMT, Sofia, Bulgaria", "citeRegEx": "Bisazza et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Bisazza et al\\.", "year": 2013}, {"title": "Semantic representation of negation using focus detection", "author": ["Blanco", "Moldovan2011] Eduardo Blanco", "Dan Moldovan"], "venue": "In Proc. of ACL-HLT,", "citeRegEx": "Blanco et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Blanco et al\\.", "year": 2011}, {"title": "Analyzing error types in English-Czech machine translation", "author": ["Ondrej Bojar"], "venue": "The Prague Bulletin of Mathematical Linguistic,", "citeRegEx": "Bojar.,? \\Q2011\\E", "shortCiteRegEx": "Bojar.", "year": 2011}, {"title": "WIT: Web Inventory of Transcribed and Translated Talks", "author": ["Christian Girardi", "Marcello Federico"], "venue": "In Proc. of EAMT,", "citeRegEx": "Cettolo et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Cettolo et al\\.", "year": 2012}, {"title": "The IWSLT 2015 evaluation campaign", "author": ["Jan Niehues", "Sebastian St\u00fcker", "Luisa Bentivogli", "Roldano Cattoni", "Marcello Federico"], "venue": "In Proc. of IWSLT, Da Nang, Vietnam", "citeRegEx": "Cettolo et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Cettolo et al\\.", "year": 2015}, {"title": "On the properties of neural machine translation: encoder\u2013 decoder approaches", "author": ["Cho et al.2014a] Kyunghyun Cho", "Bart van Merri\u00ebnboer", "Dzmitry Bahdanau", "Yoshua Bengio"], "venue": "In Proc. of SSST-8,", "citeRegEx": "Cho et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Cho et al\\.", "year": 2014}, {"title": "Learning phrase representations using RNN encoder\u2013decoder for statistical machine translation", "author": ["Cho et al.2014b] Kyunghyun Cho", "Bart van Merrienboer", "Caglar Gulcehre", "Dzmitry Bahdanau", "Fethi Bougares", "Holger Schwenk", "Yoshua Bengio"], "venue": null, "citeRegEx": "Cho et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Cho et al\\.", "year": 2014}, {"title": "On the origin of errors: a finegrained analysis of MT and PE errors and their relationship", "author": ["Daems et al.2014] Joke Daems", "Lieve Macken", "Sonia Vandepitte"], "venue": "In Proc. of LREC,", "citeRegEx": "Daems et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Daems et al\\.", "year": 2014}, {"title": "A simple, fast, and effective reparameterization of IBM model 2", "author": ["Dyer et al.2013] Chris Dyer", "Victor Chahuneau", "Noah A. Smith"], "venue": "In Proc. of NACL-HLT,", "citeRegEx": "Dyer et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Dyer et al\\.", "year": 2013}, {"title": "Linguistic-based evaluation criteria to identify statistical machine translation errors", "author": ["Marta Ruiz Costa-Juss\u00e0", "Jos\u00e9 Bernardo Mari\u00f1o Acebal", "Jos\u00e9 Adri\u00e1n Rodr\u0131\u0301guez Fonollosa"], "venue": "In Proc. of EAMT,", "citeRegEx": "Cabeceran et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Cabeceran et al\\.", "year": 2010}, {"title": "Assessing the impact of translation errors on machine translation quality with mixed-effects models", "author": ["Matteo Negri", "Luisa Bentivogli", "Marco Turchi"], "venue": "In Proc. of EMNLP,", "citeRegEx": "Federico et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Federico et al\\.", "year": 2014}, {"title": "Terra: a collection of translation error-annotated corpora", "author": ["Fishel et al.2012] Mark Fishel", "Ondrej Bojar", "Maja Popovi\u0107"], "venue": "In Proc. of LREC,", "citeRegEx": "Fishel et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Fishel et al\\.", "year": 2012}, {"title": "Error classification for MT evaluation", "author": ["Mary Flanagan"], "venue": "In Proc. of AMTA,", "citeRegEx": "Flanagan.,? \\Q1994\\E", "shortCiteRegEx": "Flanagan.", "year": 1994}, {"title": "An \u201cAI readability\u201d formula for French as a foreign language", "author": ["Fran\u00e7ois", "Fairon2012] Thomas Fran\u00e7ois", "C\u00e9drick Fairon"], "venue": "In Proc. of EMNLP-CoNLL,", "citeRegEx": "Fran\u00e7ois et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Fran\u00e7ois et al\\.", "year": 2012}, {"title": "On using monolingual corpora in neural machine translation. CoRR, abs/1503.03535", "author": ["Orhan Firat", "Kelvin Xu", "Kyunghyun Cho", "Lo\u0131\u0308c Barrault", "Huei-Chi Lin", "Fethi Bougares", "Holger Schwenk", "Yoshua Bengio"], "venue": null, "citeRegEx": "G\u00fcl\u00e7ehre et al\\.,? \\Q2015\\E", "shortCiteRegEx": "G\u00fcl\u00e7ehre et al\\.", "year": 2015}, {"title": "The KIT translation systems for IWSLT", "author": ["Ha et al.2015] Thanh-Le Ha", "Jan Niehues", "Eunah Cho", "Mohammed Mediani", "Alex Waibel"], "venue": "In Proc. of IWSLT, Da Nang, Vietnam", "citeRegEx": "Ha et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Ha et al\\.", "year": 2015}, {"title": "The Edinburgh machine translation systems for IWSLT", "author": ["Huck", "Birch2015] Matthias Huck", "Alexandra Birch"], "venue": "In Proc. of IWSLT, Da Nang, Vietnam", "citeRegEx": "Huck et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Huck et al\\.", "year": 2015}, {"title": "Measuring machine translation errors in new domains. Transactions of the Association for Computational Linguistics, 1:429\u2013440", "author": ["Irvine et al.2013] Ann Irvine", "John Morgan", "Marine Carpuat", "Hal Daum\u00e9 III", "Dragos Munteanu"], "venue": null, "citeRegEx": "Irvine et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Irvine et al\\.", "year": 2013}, {"title": "On using very large target vocabulary for neural machine translation", "author": ["Jean et al.2015a] S\u00e9bastien Jean", "Kyunghyun Cho", "Roland Memisevic", "Yoshua Bengio"], "venue": "In Proc. of ACL-IJCNLP,", "citeRegEx": "Jean et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Jean et al\\.", "year": 2015}, {"title": "2015b. Montreal neural machine translation systems for WMT15", "author": ["Jean et al.2015b] S\u00e9bastien Jean", "Orhan Firat", "Kyunghyun Cho", "Roland Memisevic", "Yoshua Bengio"], "venue": "In Proc. of WMT,", "citeRegEx": "Jean et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Jean et al\\.", "year": 2015}, {"title": "The Heidelberg university English-German translation system for IWSLT", "author": ["Jehl et al.2015] Laura Jehl", "Patrick Simianer", "Julian Hitschler", "Stefan Riezler"], "venue": "In Proc. of IWSLT, Da Nang, Vietnam", "citeRegEx": "Jehl et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Jehl et al\\.", "year": 2015}, {"title": "Comparing human perceptions of post-editing effort with postediting operations", "author": ["Maarit Koponen"], "venue": "In Proc. of WMT,", "citeRegEx": "Koponen.,? \\Q2012\\E", "shortCiteRegEx": "Koponen.", "year": 2012}, {"title": "Using a new analytic measure for the annotation and analysis of MT errors on real data", "author": ["Lommel et al.2014] Arle Lommel", "Aljoscha Burchardt", "Maja Popovi\u0107", "Kim Harris", "Eleftherios Avramidis", "Hans Uszkoreit"], "venue": "In Proc. of EAMT,", "citeRegEx": "Lommel et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Lommel et al\\.", "year": 2014}, {"title": "Stanford neural machine translation systems for spoken language domains", "author": ["Luong", "Manning2015] Minh-Thang Luong", "Christopher D Manning"], "venue": "In Proc. of IWSLT, Da Nang, Vietnam", "citeRegEx": "Luong et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Luong et al\\.", "year": 2015}, {"title": "Effective approaches to attention-based neural machine translation", "author": ["Luong et al.2015] Thang Luong", "Hieu Pham", "Christopher D. Manning"], "venue": "In Proc. of EMNLP,", "citeRegEx": "Luong et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Luong et al\\.", "year": 2015}, {"title": "Neural Reranking Improves Subjective Quality of Machine Translation: NAIST at WAT2015", "author": ["Neubig et al.2015] Graham Neubig", "Makoto Morishita", "Satoshi Nakamura"], "venue": "In Proc. of WAT2015,", "citeRegEx": "Neubig et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Neubig et al\\.", "year": 2015}, {"title": "Towards automatic error analysis of machine translation output", "author": ["Popovi\u0107", "Ney2011] Maja Popovi\u0107", "Hermann Ney"], "venue": "Computational Linguistics,", "citeRegEx": "Popovi\u0107 et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Popovi\u0107 et al\\.", "year": 2011}, {"title": "Learning from human judgments of machine translation output", "author": ["Popovi\u0107 et al.2013] Maja Popovi\u0107", "Eleftherios Avramidis", "Aljoscha Burchardt", "Sabine Hunsicker", "Sven Schmeier", "Cindy Tscherwinka", "David Vilar", "Hans Uszkoreit"], "venue": null, "citeRegEx": "Popovi\u0107 et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Popovi\u0107 et al\\.", "year": 2013}, {"title": "Hjerson: an open source tool for automatic error classification of machine translation output", "author": ["Maja Popovi\u0107"], "venue": "The Prague Bulletin of Mathematical Linguistic,", "citeRegEx": "Popovi\u0107.,? \\Q2011\\E", "shortCiteRegEx": "Popovi\u0107.", "year": 2011}, {"title": "Overcoming the curse of sentence length for neural machine translation using automatic segmentation", "author": ["Dzmitry Bahdanau", "Bart van Merrienboer", "Kyunghyun Cho", "Yoshua Bengio"], "venue": "In Proc. of SSST-8, Doha,", "citeRegEx": "Pouget.Abadie et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Pouget.Abadie et al\\.", "year": 2014}, {"title": "Complexity of spoken versus written language for machine translation", "author": ["Ruiz", "Federico2014] Nicholas Ruiz", "Marcello Federico"], "venue": "In Proc. of EAMT,", "citeRegEx": "Ruiz et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Ruiz et al\\.", "year": 2014}, {"title": "Exploiting synergies between open resources for German dependency parsing", "author": ["Martin Volk", "Gerold Schneider"], "venue": null, "citeRegEx": "Sennrich et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Sennrich et al\\.", "year": 2013}, {"title": "A study of translation edit rate with targeted human annotation", "author": ["Bonnie Dorr", "Rich Schwartz", "Linnea Micciulla", "John Makhoul"], "venue": "In Proc. of AMTA,", "citeRegEx": "Snover et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Snover et al\\.", "year": 2006}, {"title": "On the practice of error analysis for machine translation evaluation", "author": ["Stymne", "Ahrenberg2012] Sara Stymne", "Lars Ahrenberg"], "venue": "In Proc. of LREC,", "citeRegEx": "Stymne et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Stymne et al\\.", "year": 2012}, {"title": "Sequence to sequence learning with neural networks", "author": ["Oriol Vinyals", "Quoc V. Le"], "venue": "In Proc. of NIPS,", "citeRegEx": "Sutskever et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Sutskever et al\\.", "year": 2014}, {"title": "Accelerated DP based search for statistical translation", "author": ["Stephan Vogel", "Hermann Ney", "Alexander Zubiaga", "Hassan Sawaf"], "venue": "In Proc. of Eurospeech,", "citeRegEx": "Tillmann et al\\.,? \\Q1997\\E", "shortCiteRegEx": "Tillmann et al\\.", "year": 1997}, {"title": "Error analysis of statistical machine translation output", "author": ["Vilar et al.2006] David Vilar", "Jia Xu", "Luis Fernando d\u2019Haro", "Hermann Ney"], "venue": "In Proc. of LREC,", "citeRegEx": "Vilar et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Vilar et al\\.", "year": 2006}, {"title": "Addicter: what is wrong with my translations", "author": ["Zeman et al.2011] Daniel Zeman", "Mark Fishel", "Jan Berka", "Ondrej Bojar"], "venue": "The Prague Bulletin of Mathematical Linguistic,", "citeRegEx": "Zeman et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Zeman et al\\.", "year": 2011}], "referenceMentions": [{"referenceID": 37, "context": "Such rapid progress stems from the improvement of the recurrent neural network encoderdecoder model, originally proposed in (Sutskever et al., 2014; Cho et al., 2014b), with the use of the at-", "startOffset": 124, "endOffset": 167}, {"referenceID": 0, "context": "tention mechanism (Bahdanau et al., 2015).", "startOffset": 18, "endOffset": 41}, {"referenceID": 0, "context": "To date, NMT systems have only been evaluated by BLEU in single-reference setups (Bahdanau et al., 2015; Sutskever et al., 2014; Luong et al., 2015; Jean et al., 2015a; G\u00fcl\u00e7ehre et al., 2015).", "startOffset": 81, "endOffset": 191}, {"referenceID": 37, "context": "To date, NMT systems have only been evaluated by BLEU in single-reference setups (Bahdanau et al., 2015; Sutskever et al., 2014; Luong et al., 2015; Jean et al., 2015a; G\u00fcl\u00e7ehre et al., 2015).", "startOffset": 81, "endOffset": 191}, {"referenceID": 26, "context": "To date, NMT systems have only been evaluated by BLEU in single-reference setups (Bahdanau et al., 2015; Sutskever et al., 2014; Luong et al., 2015; Jean et al., 2015a; G\u00fcl\u00e7ehre et al., 2015).", "startOffset": 81, "endOffset": 191}, {"referenceID": 17, "context": "To date, NMT systems have only been evaluated by BLEU in single-reference setups (Bahdanau et al., 2015; Sutskever et al., 2014; Luong et al., 2015; Jean et al., 2015a; G\u00fcl\u00e7ehre et al., 2015).", "startOffset": 81, "endOffset": 191}, {"referenceID": 20, "context": ", 2015), which is very costly and complex, and fully automatic error analysis (Popovi\u0107 and Ney, 2011; Irvine et al., 2013), which is noisy and biased towards one or few arbitrary reference translations.", "startOffset": 78, "endOffset": 122}, {"referenceID": 31, "context": "(Popovi\u0107, 2011) or on output-reference alignment (Zeman et al.", "startOffset": 0, "endOffset": 15}, {"referenceID": 40, "context": "(Popovi\u0107, 2011) or on output-reference alignment (Zeman et al., 2011).", "startOffset": 49, "endOffset": 69}, {"referenceID": 31, "context": "Regarding error classification, Hjerson (Popovi\u0107, 2011) detects five main types of word-level errors as defined in (Vilar et al.", "startOffset": 40, "endOffset": 55}, {"referenceID": 39, "context": "Regarding error classification, Hjerson (Popovi\u0107, 2011) detects five main types of word-level errors as defined in (Vilar et al., 2006): morphological, reordering, missing words, extra words, and lexical choice errors.", "startOffset": 115, "endOffset": 135}, {"referenceID": 20, "context": "Irvine et al. (2013) propose another word-level error analysis technique specifically focused on lexical choice and aimed at understanding the effects of domain differences on MT.", "startOffset": 0, "endOffset": 21}, {"referenceID": 5, "context": "Previous error analyses based on manually postedited translations were presented in (Bojar, 2011; Koponen, 2012; Popovi\u0107 et al., 2013).", "startOffset": 84, "endOffset": 134}, {"referenceID": 24, "context": "Previous error analyses based on manually postedited translations were presented in (Bojar, 2011; Koponen, 2012; Popovi\u0107 et al., 2013).", "startOffset": 84, "endOffset": 134}, {"referenceID": 30, "context": "Previous error analyses based on manually postedited translations were presented in (Bojar, 2011; Koponen, 2012; Popovi\u0107 et al., 2013).", "startOffset": 84, "endOffset": 134}, {"referenceID": 6, "context": "Evaluation data are publicly available through the WIT3 repository (Cettolo et al., 2012).", "startOffset": 67, "endOffset": 89}, {"referenceID": 18, "context": "Our analysis focuses on the first four top-ranking systems, which include NMT (Luong and Manning, 2015) and three different phrase-based approaches: standard phrase-based (Ha et al., 2015), hierarchical (Jehl et al.", "startOffset": 171, "endOffset": 188}, {"referenceID": 23, "context": ", 2015), hierarchical (Jehl et al., 2015) and a combination of phrasebased and syntax-based (Huck and Birch, 2015).", "startOffset": 22, "endOffset": 41}, {"referenceID": 35, "context": "The Translation Edit Rate (TER) (Snover et al., 2006) naturally fits our evaluation framework, where it traces the edits done by post-editors.", "startOffset": 32, "endOffset": 53}, {"referenceID": 32, "context": "Following previous work (Cho et al., 2014a; Pouget-Abadie et al., 2014; Bahdanau et al., 2015; Luong et al., 2015), we investigate how sentence length affects overall translation quality.", "startOffset": 24, "endOffset": 114}, {"referenceID": 0, "context": "Following previous work (Cho et al., 2014a; Pouget-Abadie et al., 2014; Bahdanau et al., 2015; Luong et al., 2015), we investigate how sentence length affects overall translation quality.", "startOffset": 24, "endOffset": 114}, {"referenceID": 26, "context": "Following previous work (Cho et al., 2014a; Pouget-Abadie et al., 2014; Bahdanau et al., 2015; Luong et al., 2015), we investigate how sentence length affects overall translation quality.", "startOffset": 24, "endOffset": 114}, {"referenceID": 15, "context": "granularity have been developed (Flanagan, 1994; Vilar et al., 2006; Farr\u00fas Cabeceran et al., 2010; Stymne and Ahrenberg, 2012; Lommel et al., 2014).", "startOffset": 32, "endOffset": 148}, {"referenceID": 39, "context": "granularity have been developed (Flanagan, 1994; Vilar et al., 2006; Farr\u00fas Cabeceran et al., 2010; Stymne and Ahrenberg, 2012; Lommel et al., 2014).", "startOffset": 32, "endOffset": 148}, {"referenceID": 25, "context": "granularity have been developed (Flanagan, 1994; Vilar et al., 2006; Farr\u00fas Cabeceran et al., 2010; Stymne and Ahrenberg, 2012; Lommel et al., 2014).", "startOffset": 32, "endOffset": 148}, {"referenceID": 14, "context": "However, given the proved difficulty of disambiguating between these three subclasses (Popovi\u0107 and Ney, 2011; Fishel et al., 2012), we prefer to rely on a more coarse-grained linguistic error classification where lexical errors include all of them (Farr\u00fas Cabeceran et al.", "startOffset": 86, "endOffset": 130}, {"referenceID": 34, "context": "responding post-edits was produced with the German parser ParZu (Sennrich et al., 2013).", "startOffset": 64, "endOffset": 87}, {"referenceID": 38, "context": "Note that the TER score calculated by setting to 0 the cost of shifts approximates the Position-independent Error Rate (Tillmann et al., 1997).", "startOffset": 119, "endOffset": 142}, {"referenceID": 1, "context": "output and the post-edit performed by HTER, we run an additional assessment using KRS, or Kendall Reordering Score (Birch et al., 2010), which measures the similarity between the source-reference reorderings and the source-MT output reorderings.", "startOffset": 115, "endOffset": 135}, {"referenceID": 11, "context": "To compute the word alignments required by KRS, we used the FastAlign tool (Dyer et al., 2013).", "startOffset": 75, "endOffset": 94}, {"referenceID": 2, "context": "To put our results into perspective, note that Birch (2011) reports a difference of 5 KRS points between the translations of a PBMT system and those produced by four human translators tested against each other, in a Chinese-English experiment.", "startOffset": 47, "endOffset": 60}], "year": 2017, "abstractText": "Within the field of Statistical Machine Translation (SMT), the neural approach (NMT) has recently emerged as the first technology able to challenge the long-standing dominance of phrase-based approaches (PBMT). In particular, at the IWSLT 2015 evaluation campaign, NMT outperformed well established state-ofthe-art PBMT systems on English-German, a language pair known to be particularly hard because of morphology and syntactic differences. To understand in what respects NMT provides better translation quality than PBMT, we perform a detailed analysis of neural versus phrase-based SMT outputs, leveraging high quality post-edits performed by professional translators on the IWSLT data. For the first time, our analysis provides useful insights on what linguistic phenomena are best modeled by neural models \u2013 such as the reordering of verbs \u2013 while pointing out other aspects that remain to be improved.", "creator": "LaTeX with hyperref package"}}}