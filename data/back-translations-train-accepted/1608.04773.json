{"id": "1608.04773", "review": {"conference": "ICML", "VERSION": "v1", "DATE_OF_SUBMISSION": "16-Aug-2016", "title": "Faster Principal Component Regression and Stable Matrix Chebyshev Approximation", "abstract": "We solve principle component regression (PCR) by providing an efficient algorithm to project any vector onto the subspace formed by the top principle components of a matrix. Our algorithm does not require any explicit construction of the top principle components, and therefore is suitable for large-scale PCR instances.", "histories": [["v1", "Tue, 16 Aug 2016 20:48:02 GMT  (2532kb,D)", "https://arxiv.org/abs/1608.04773v1", null], ["v2", "Mon, 24 Apr 2017 19:35:38 GMT  (2557kb,D)", "http://arxiv.org/abs/1608.04773v2", "title changed and minor revisions"]], "reviews": [], "SUBJECTS": "stat.ML cs.DS cs.LG math.NA math.OC", "authors": ["zeyuan allen-zhu", "yuanzhi li"], "accepted": true, "id": "1608.04773"}, "pdf": {"name": "1608.04773.pdf", "metadata": {"source": "CRF", "title": "Faster Principal Component Regression and Stable Matrix Chebyshev Approximation", "authors": ["Zeyuan Allen-Zhu", "Yuanzhi Li"], "emails": ["zeyuan@csail.mit.edu", "yuanzhil@cs.princeton.edu"], "sections": [{"heading": null, "text": "We solve the regression of the main components (PCR) to a multiplicative accuracy of 1 + \u03b3 by reducing the problem to O-shaped black box calls of the burr regression. Therefore, our algorithm does not require explicit construction of the top main components and is suitable for large-scale PCR instances. In contrast, an earlier result requires O-shaped black box calls. We achieve this result by developing a generally stable recurrence formula for matrix-chebyshev polynomials and a degree-optimal polynomic approximation to the matrix drawing function. Our techniques can be of independent interest, especially in the development of iterative methods."}, {"heading": "1 Introduction", "text": "In machine learning and statistics, it is often desirable to present a large set of data in a more tractable, low-dimensional form without losing too much information. One of the most robust methods to achieve this goal is the Principal Component Projection (PCP): PCP: project vectors to the range of the top main components of a matrix.It is well known that PCP reduces noise and increases efficiency in downstream tasks. One of the main applications is the Principal Component Regression (PCR): PCR: linear regression, but limited to the subspace of the top main components. Classical algorithms for PCP or PCR rely on a Principal Component Analysis (PCA) solver to first restore the top main components; with these available components, the tasks of PCP and PCR become trivial because the projection matrix can be constructed explicitly."}, {"heading": "1.1 Approximating PCP Without PCA", "text": "In this paper, we propose the following concept of PCP approximation: In view of a data matrix A-Rd-Rd-x-d (with singular values not exceeding 1) and a threshold \u03bb-0, we say that an algorithm solves PCP if it projects - informally speaking - up to a multiplicative error of 1 \u00b1 \u03b5 (see Def. 3.1 for a formal definition) 1. any eigenvector for A > A with values in [\u03bb (1 + \u03b3), 1] to Kong, 2. any eigenvector for A > A with values in [0, \u03bb (1 \u2212 \u03b3)] to ~ 0, 3. any eigenvector for A > A with values in [\u03bb (1 \u2212 \u03b3), \u03bb (1 + \u03b3), 1] to \"somewhere between ~ 0 and \u03bd.\" Such a definition also extends to PCR approximations of A > A > A (see Def 3.2)."}, {"heading": "1.2 From PCP to Polynomial Approximation", "text": "The main technique of Frostig et al. is to construct a polynomial to approximate the drawing function sgn (x): [\u2212 1, 1] \u2192 {\u00b1 1}: sgn (x) \u2212 def = {+ 1, \u2212 0; \u2212 1, x < 0.Above all, since each polynomial g (x) is satisfactory, each polynomial g (x) must be satisfactory (x) \u2212 sgn (x) -def = 0, \u2212 1), and (1,1): each polynomial g (x) must be satisfactory (\u2212 2), (1,2) the problem of the (\u03b3) approximate PCP can be due to the compression of the matrix polynomial g (S) for S def = (A > A + \u03bbI) \u2212 1 (cf."}, {"heading": "1.3 Our Results and Main Ideas", "text": "We provide an efficient and stable polynomial approximation to the matrix drawing function, which has an approximately optimal degree O (\u03b3 \u2212 1 log (1 / \u03b3\u03b5)). On a high level, we construct a polynomial q (x), which is approximately the same (1 + \u03ba \u2212 x 2) \u2212 1 / 2 for some readers, however (1 + 2) \u2212 1 / 2 has no singular dot on [\u2212 1, 1] so we can apply the Chebyshev approximation theory to obtain something q (x) of degree O (1 / 1 log (1 / 3))). However, to construct q (x), we first note that (1 + 2) does not have a stable polynomial approximation theory on [\u2212 1] Chebyshev to prove something q (x) of degree O (1 / 2 log)."}, {"heading": "1.4 Related Work", "text": "There are a few attempts to reduce the costs of PCA when PCR is solved, for example by approximating the AP\u03bb approximation method, where P\u03bb is the PCP projection matrix [6, 7]. However, they cost a running time that includes linear scales with the number of major components above \u03bb.A significant number of papers have focused on the case of PCA [2, 4, 18] and its online variant [3]. Unfortunately, all of these methods require a running time that scales at least linearly in relation to the number of major components.More related to this paper is the work on the matrix drawing function, which plays an important role in control theory and quantum chromodynamics. Several results have addressed Krylov methods for applying the drawing function in the so-called crylov subspace, without explicitly constructing any approximate polynomial methods [21, 24]."}, {"heading": "2 Preliminaries", "text": "We denote the indicator function of the event e with 1 [e], with 2 the euclidean norm of a vector v, with 3 the Moore-Penrose pseudo-inverse of a symmetrical matrix M, and with 3 the matrix function applied to M, which is equal to Udiag {f (D1),.., f (Dd)} U > if M = Udiag {D1,.., Dd} U > its own composition. Throughout the paper, matrix A is the property composition of property components d. \"We denote the property composition of property components d.\" We denote the largest single component of A. Following the tradition of [13] and adherence to the emergency component \u00b7 R > property composition of property components of property components d. \""}, {"heading": "2.1 Ridge Regression", "text": "Definition 2.3. A black box algorithm ApxRidge (A, \u03bb, u) is an \u03b5-approximate comb regression solver if it is equivalent to solving well-conditioned linear systems for each u-Rd, \"ApxRidge (A, \u03bb, u) \u2212 (A > A + \u03bbI) \u2212 1u\" \u2264 \u03b5. \"7Comb regression is synonymous with solving well-conditioned linear systems or minimizing strongly convex and smooth targets f (y) def = 12y > (A > A + \u03bbI) y \u2212 u > y. Comment 2.4. There is extensive literature on efficient algorithms for solving comb regressions. Especially (1) Conjugate gradients [22] or accelerated gradient declines [20] yield fastest full gradient methods; (2) SVRG [17] and their acceleration Katyusha [1] yield the fastest stochastic gradient method; and (3) NUCD5 [5] is the fastest full gradient method."}, {"heading": "2.2 Chebyshev Polynomials", "text": "Definition 2.5. Chebyshev polynomials of type 1 and 2 are {Tn (x)} n \u2265 0 and {Un (x)} n \u2265 0 whereT0 (x) def = 1, T1 (x) def = x, Tn + 1 (x) def = 2x \u00b7 Tn (x) \u2212 Tn (x) \u2212 Tn \u2212 1 (x) U0 (x) def = 1, U1 (x) def = 2, Un + 1 (x) def = 2 \u00b7 Un (x) \u2212 1 (x) Fact 2.6 ([23]) suffice ddxTn (x) = nUn \u2212 1 (x) for n."}, {"heading": "3 Approximate PCP and PCR", "text": "We formalize our notions of an approach to PCP and PCR and offer a reduction from PCR to PCP."}, {"heading": "3.1 Our Notions of Approximation", "text": "It should be remembered that Frostig et al. [13] only work with matrices A that meet the eigengap assumption (> eigengap assumptions, that is, A has no singular value in the span [permanency]. Their approximation guarantees are very simple: \u2022 eigengap approximation to PCP on vector B (eigenaccuracy on vector B) is too strong and impossible to satisfy for matrices that do not have a large eigengap around the projection threshold. In this paper, we project the following more general (but nonetheless very significant) approximation concepts. Definition 3.1. An algorithm B (permanence) is (permanence) conditionality fear on PCP x."}, {"heading": "3.2 Reductions from PCR to PCP", "text": "If the PCP solution (A > b) is calculated accurately, then this calculation is by definition problematic if this requirement is approximate.The following approach has been proposed to improve the accuracy of PCR by solving a linear system: \"Calculate p (((A > A + \u03bbI) \u2212 1), where p (x) is a polynomial that approaches the function x1 \u2212 \u03b2x.\" This is a good approximation of (A > A) because the composition of the functions x1 \u2212 \u03bbx and 1 + x isexaktly x \u2212 1. Frostig et al. Selected p (x) = pm (x) = pm (x) = m = 1 \u03bb t \u2212 1xt, which is a truncated PCR series, and used the following method to compress the gap."}, {"heading": "4 Property of Chebyshev Approximation Outside [\u22121, 1]", "text": "The classical Chebyshev approximation theory (such as Lemma 2.10) speaks only about the behaviors of pn (x) or gn (x) at interval [\u2212 1, 1]. However, for the purpose of this work we must also consider its value for x > 1. We prove the following general problem in appendix B and believe that it could be of independent interest: (we denote it by f (k) (x) the k-th derivative of f at x) Lemma 4.1. The assumption f (z) is analytical for each k \u00b2 0, f (k) (0) \u2265 0, p \u00b2 n \u00b2 (x) and qn (x) becomes the degree-n Chebyshev drunken series and Chebyshev drunken interpolation of f (x), we have the minimum y-degree of f (0)."}, {"heading": "6 Stable Computation of Matrix Chebyshev Polynomials", "text": "In this section, we show that any polynomial that is a weighted sum of Chebyshev polynomials with limited coefficients can be calculated stable when applied to matrices with approximate calculations, first by generalizing Clenshaw's backward method to matrix cases in Section 6.1 to calculate a matrix variant of the Chebyshev sum, and then by analyzing its stability in Section 6.2 using elloit's forward-backward transformation [8]. Note 6.1. We would like to point out that although Chebyshev polynomials are known to be stable under error conditions when calculated on scalars [14], it is not immediately clear why they also apply to matrices. Remember that Chebyshev polynomials Tn + 1 (x) = 2xTn \u2212 1 (x) in which case we cannot exceed this mere Tn + coordance factor (M)."}, {"heading": "6.1 Clenshaw\u2019s Method in Matrix Form", "text": "In the scalar case, Clenshaw's method (sometimes referred to as relapse) is one of the most widely used implementations for Chebyshev polynomials. We now generalize it to matrices. Consider any calculation of the form ~ sN def = N \u2211 k = 0Tk (M) ~ ck-Rd, where M-Rd \u00b7 d is symmetrical and each ~ ck is in Rd. (6.1) (Note that for PCP and PCR purposes it is sufficient to consider ~ ck = c \"k,\" where c \"k\" R is a scalar, and vice versa. However, we need to work on this more general form for our stability analysis.) The sN vector can be calculated using the following method: Lemma = c \"k\" R (relapse recidence)."}, {"heading": "6.2 Inexact Clenshaw\u2019s Method in Matrix Form", "text": "We show that using the backward recursion formula, the Chebyshev sum of (6,1) can be calculated stable. We define the following model to capture the error in terms of matrix vector multiplications. Definition 6,3 (inaccurate backward recursion). Let M be an approximate algorithm that meets (u) - M (u) - 2 (u) - vector multiplications for each u-Rd. Then define an inaccurate backward recursion to beb-N + 1 def = 0, b-N def = ~ cN, and b-N def = 0 \u2212 1,., 0}: b-r def = 2M (b-r + 1) \u2212 b-r + 2 + ~ cr-Rd, and define the output as s-N def = b-0 \u2212 M (b-b1).The following theorem gives an error analysis to our backward recursion."}, {"heading": "7 Algorithms and Main Theorems for PCP and PCR", "text": "We are now ready to present our most important theorems for PCP and PCR. First, we state a simple fact: Fact 7.1. (P\u03bb) = I + sgn (S) 2, where S def = 2 (A > A + \u03bbI) \u2212 1A > A \u2212 I = (A > A + \u03bbI) \u2212 1 (A > A \u2212 \u03bbI). In other words, for each vector introduced in Section 5, the exact PCP solution P\u03bb (B) is the same as computing (P\u03bb) = I + sgn (S) 2. Therefore, we can use our polynomial gn (x) introduced in Section 5, and gn (S), and gn (S), sgn (S). Finally, in order to calculate gn (S), we must multiply S to deg (gn) vectors; whenever we do so, we can call comb regression."}, {"heading": "7.1 Our Pseudo Codes", "text": "First, we can calculate setpoints for any number of setpoints (A = 7.5%). This simply uses an oracle call for comb regression, see algorithm 1.Next, because we are interested in (\u03b3, \u03b5) approximate PCP, we want gn (x) to be close to sgn (x) on all eigenvalues of A > A that are outside [(1 \u2212), or equivalent to all eigenvalues of S outside the range [\u2212 (1 +) \u2212 11 + (1 +), 1 \u2212 (1 \u2212), 1 \u2212 (1 \u2212), 1 \u2212 (1 \u2212), 2 \u2212 (1 \u2212), MultS (A \u2212), 1 \u2212), or equivalent to all eigenvalues outside the range [\u2212 (1 +) \u2212 11 + (1 \u2212), 1 \u2212 (1 \u2212), 1 \u2212 (1 \u2212), 2, (1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1."}, {"heading": "7.2 Our Main Theorems", "text": "I'm not sure what I'm going to do with it, I'm not going to do it, I'm not going to do it, I'm going to do it, I'm going to do it, I'm going to do it, I'm going to do it, I'm going to do it, I'm going to do it, I'm going to do it, I'm going to do it, I'm going to do it, I'm going to do it, I'm going to do it, I'm going to do it, I'm going to do it, I'm going to do it, I'm going to do it, I'm going to do it, I'm going to do it, I'm going to do it, I'm going to do it, I'm going to do it, I'm going to do it, I'm going to do it, I'm going to do it, I'm going to do it, I'm going to do it, I'm going to do it, I'm going to do it, I'm going to do it, I'm going to do it, I'm going to do it, I'm going to do it, I'm going to do it, I'm going to do it, I'm going to do it, '."}, {"heading": "8 Experiments", "text": "We look at synthetic and real data sets. \u2022 We create the synthetic data sets in the same way as [13]. That is, we create a 3000 x 2000 dimensional matrix A via SVD A = UVV >, where U and V are random orthonormal matrices and contain random singular values. Of the 2000 singular values, we let half of them be randomly selected from [0, \u221a 0.1 (1 \u2212 a) and the other half of the PCD shapes be randomly selected. [0 \u2212 a) and the other half of the PCD shapes be randomly selected (1 \u2212 a). We create vector b by adding Ax to a random \"true\" x that correlates to the main components of A. We look at the eigenvalue threshold as 0.01, and use a = 0.02, 0.1 in our experiments. We call these data sets random."}, {"heading": "8.1 Evaluation 1: With Eigengap Assumption", "text": "In the first evaluation, we consider matrices that meet the self-gap assumption. > To simulate a self-gap, we use random datasets with a value of 0.01, 0.02, 0.1 and present our results in Figure 2 with respect to the following three performance measures: \u2022 Regression error: x x x x x x x 2; where x is the output of a PCR algorithm and x x b is the exact PCR solution. \u2022 Denoising Error: (I \u2212 P.P.P.P.P.P.P.P.P.P.P.P.P.P.P.P.P.P.P.P.P.P.P.P.P.P.P.P.P.P.P.P.P.P.P.P.P.P.P.P.P.P.P.P.P.P.P.P.P.P.P.P.P.P.P.P.P.P.P.P.P.P.P.P.P.P.P.P.P.P.P.P..P.P.P.P.P......P.P.P..P.P.P.P...P.P.P.......P..P.P.P.P........P.P...P.P..P.P.P.P.P.P.P.P..P.P.P.P.P..P..P......P.P....P.P...P...P..P..P...P..P.P....P.P...P..P.P.P..P.P.P..P.P....P.P.P..P......P.P.P..P..P.P....P.......P....P..P..P.P.P...P.P.P...P.P.P.P."}, {"heading": "8.2 Evaluation 2: Without Eigengap Assumption", "text": "In our second evaluation, we consider scenarios where there is no significant eigengap around the projection threshold \u03bb. We consider dataset random-a for a = 0 and dataset mnist. This 12Of course, if the true projection matrix P\u03bb is explicitly stated, we consider a good iteration to stop it. However, the whole PCP problem is how to calculate P\u03bb without explicitly constructing it. We emphasize here that in gapless scenarios regression errors, projection errors or even the quantity this method represents can be replaced by \u2022 denoising error (small)."}, {"heading": "8.3 Evaluation 3: Stability Test", "text": "In our third evaluation, we check that our method continues to work well even when comb regressions are calculated with moderate errors. We consider two types of errors in our experiments: Note. Although it seems that our method is more affected by errors than FMMS, we emphasize that this is due to the fact that FMMS is too slow and still operates in a very low accuracy regime in the diagrams. (For example, as a stable algorithm, FMMS should not be affected by errors of the order of 10 \u2212 6 if the desired accuracy exceeds 10 \u2212 4.) \u2022 Comb SVRG: We perform the SVRG method for 50 passes to solve each comb regression. \u2212 Comb-10 \u2212 k: we perform exact comb regression, but randomly add the noise [\u2212 k, 10 \u2212 k] per coordinate. We present our results in Figure 4. For cleanliness, we only compare the denosis of errors and only on data sets."}, {"heading": "9 Conclusion", "text": "We summarize our contributions. \u2022 We present approximate concepts for PCP and PCR that are not based on any inherent tension. \u2022 Our concepts are reduced to standard concepts based on the self-gap assumption. \u2022 We design an approximate optimal polynomial approximation g (x) to sgn (x) satisfactory (1,1) and (1,2). \u2022 We develop general stable relapse formulas for matrix-chebyshev polynomials; as a logical consequence, our g (x) can be applied to matrices in a stable manner. \u2022 We obtain faster, detectable PCA-free algorithms for PCP and PCR than known results."}, {"heading": "Acknowledgements", "text": "We thank Yin Tat Lee for proposing the new title to us, and anonymous arbitrators for useful suggestions. Z. Allen-Zhu is partially supported by an NSF scholarship, no. CCF-1412958, and a Microsoft research scholarship, no. 0518584. Any opinions, findings and conclusions or recommendations expressed in this material are those of the author (s) and do not necessarily reflect the views of NSF or Microsoft.Appendix"}, {"heading": "A Proof of Lemma 3.5", "text": "Let us take into consideration a new exact sequence: K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K (K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K"}, {"heading": "B Appendix for Section 4", "text": "Lemma (z) is (1) n (1) n (1) n (1) n (1) n (1) n (1) n (1) n (1) n (1) n (1) n (1) n (1) n (1) n (1) n (1) n (1) n (1) n (1) n (1) n (1) n (1) n (1) n (1) n (1) n (1) n (1) n (1) n (1) n (1) n) n (1) n (1) n) n (1) n n (1) n n n (1) n n n (1) n n n (1) n n (1) n n (1) n n (1) n n (1) n (n) n (1) n (1) n (n) n (1) n (1) n (1) n (n) n (1) n (1) n (1) n n n (1) n n (1) n (1) n (1) n n (1) n (1) n (1) n (1) n n (1) n (1) n (1) n (1) n) n (1) n (1) n (1) n) n (1) n (1) n) n (1) n (1) n (1) n) n (1) n (1) n) n (1) n n (1) n (1) n) n (1) n n (1) n n (1) n n n (1) n (1) n) n (1) n (1) n n n (1) n n n (1) n n n (1) n n n n (1) n (1) n n n (1) n n n (1) n n n n (1) n (1) n (1) n n n (1) n n (1) n n n n (1) n) n n (1) n n n n (1) n n (1) n n n n (1) n n n (1) n n (1) n (1) n n n n n (1) n (1) n n n"}, {"heading": "C Proof of Lemma 5.4", "text": "Let us first take note of the following problem, which results from Lemma 2.10 together with the alias Lemma 2.8: Lemma C.1. Let us assume that f (z) is analytical on f, then it is the interpolation of f, then it is the interpolation of f, then it is the interpolation of f, then it is the interpolation of f, then it is the interpolation of f, then it is the interpolation of f (z) = (1 + 2) \u2212 1 / 2, then we have Lemma 5.4. Let us leave qn (x) = 0 ckTk (x) = 0 ckTk (x) the degree n Chebyshev interpolation of f (x) = (1 + 2) \u2212 1 / 2 on [\u2212 2]."}, {"heading": "D Appendix for Section 6", "text": "It is also easy to check whether the br., br., b., b., b., b., b., b., b., b., b., b., b., b., b., b., b., b., b., b., b., b., b., b., b., b., b., b., b., b., b., b., b., b., b., b., b., b., b., b., b., b., b., b., b., b., b., b., b., b., b., b., b., b., b., b., b., b., b., b., b., b., b., b., b., b."}, {"heading": "E Appendix for Section 7", "text": "Fakt 7.1. (P\u03bb), (P), (P), (P), (P), (P), (P), (P), (P), (P), (P), (P), (P), (P), (P), (P), (P), (P), (P), (P), (P), (P), (P), (P), (P), (P), (P), (P), (P), (P), (P), (P), (P), (P), (P), (P), (P), (P), (P), (P), (P), (P), (P), (P), (P), (P), (P), (P), (P), (P), (P), (P), (P), (P), (P), (P), (P), (P), (P), (P), (P), (P), (P), (P), (P), (P), (P), (P), (P), (P), (P, (P), (P), (P), (P), (P), (P), (P, (P), (P), (P), (P, (P), (P), (P, (P), (P), (P, (P), (P), (P, (P), (P), (P, (P), (P), (P, (P), (P), (P, (P), (P, (P), (P, (P), (P), (P, (P), (P, (P), (P), (, (P), (P), (P, (P), (P), (P), (P), (P, (, (, (P), (P), (P), (, P), (P), (, (P), (P), (P), (,"}], "references": [{"title": "Katyusha: The First Direct Acceleration of Stochastic Gradient Methods", "author": ["Zeyuan Allen-Zhu"], "venue": "In STOC,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2017}, {"title": "Doubly Accelerated Methods for Faster CCA and Generalized Eigendecomposition", "author": ["Zeyuan Allen-Zhu", "Yuanzhi Li"], "venue": "ArXiv e-prints,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2016}, {"title": "First Efficient Convergence for Streaming k-PCA: a Global, Gap- Free, and Near-Optimal Rate", "author": ["Zeyuan Allen-Zhu", "Yuanzhi Li"], "venue": "ArXiv e-prints,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2016}, {"title": "LazySVD: Even Faster SVD Decomposition Yet Without Agonizing Pain", "author": ["Zeyuan Allen-Zhu", "Yuanzhi Li"], "venue": "In NIPS,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2016}, {"title": "Even faster accelerated coordinate descent using non-uniform sampling", "author": ["Zeyuan Allen-Zhu", "Peter Richt\u00e1rik", "Zheng Qu", "Yang Yuan"], "venue": "In ICML,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2016}, {"title": "Faster SVD-truncated regularized least-squares", "author": ["Christos Boutsidis", "Malik Magdon-Ismail"], "venue": "IEEE International Symposium on Information Theory,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2014}, {"title": "Computing truncated singular value decomposition least squares solutions by rank revealing QR-factorizations", "author": ["Tony F Chan", "Per Christian Hansen"], "venue": "SIAM Journal on Scientific and Statistical Computing,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 1990}, {"title": "Error analysis of an algorithm for summing certain finite series", "author": ["David Elliott"], "venue": "Journal of the Australian Mathematical Society,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 1968}, {"title": "Uniform approximation of sgn x by polynomials and entire functions", "author": ["Alexandre Eremenko", "Peter Yuditskii"], "venue": "Journal d\u2019Analyse Mathe\u0301matique,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2007}, {"title": "Polynomials of the best uniform approximation to sgn (x) on two intervals", "author": ["Alexandre Eremenko", "Peter Yuditskii"], "venue": "Journal d\u2019Analyse Mathe\u0301matique,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2011}, {"title": "LIBSVM Data: Classification, Regression and Multi-label", "author": ["Rong-En Fan", "Chih-Jen Lin"], "venue": null, "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2015}, {"title": "Principal Component Projection Without Principal Component Analysis", "author": ["Roy Frostig", "Cameron Musco", "Christopher Musco", "Aaron Sidford"], "venue": "In ICML,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2016}, {"title": "Numerical Methods for Special Functions. Society for Industrial and Applied Mathematics, jan", "author": ["Amparo Gil", "Javier Segura", "Nico M. Temme"], "venue": null, "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2007}, {"title": "Approximating the spectral sums of large-scale matrices using chebyshev approximations", "author": ["Insu Han", "Dmitry Malioutov", "Haim Avron", "Jinwoo Shin"], "venue": "arXiv preprint arXiv:1606.00942,", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2016}, {"title": "Functions of Matrices", "author": ["N. Higham"], "venue": "Society for Industrial and Applied Mathematics,", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2008}, {"title": "Accelerating stochastic gradient descent using predictive variance reduction", "author": ["Rie Johnson", "Tong Zhang"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2013}, {"title": "Randomized block krylov methods for stronger and faster approximate singular value decomposition", "author": ["Cameron Musco", "Christopher Musco"], "venue": "In NIPS,", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2015}, {"title": "Computing fundamental matrix decompositions accurately via the matrix sign function in two iterations: The power of zolotarev\u2019s functions", "author": ["Yuji Nakatsukasa", "Roland W Freund"], "venue": "SIAM Review,", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2016}, {"title": "Introductory Lectures on Convex Programming Volume: A Basic course, volume I", "author": ["Yurii Nesterov"], "venue": "Kluwer Academic Publishers,", "citeRegEx": "20", "shortCiteRegEx": "20", "year": 2004}, {"title": "Model order reduction: theory, research aspects and applications, volume", "author": ["Wilhelmus H.A. Schilders", "Henk A. Van der Vorst", "Joost Rommes"], "venue": null, "citeRegEx": "21", "shortCiteRegEx": "21", "year": 2008}, {"title": "An introduction to the conjugate gradient method without the agonizing pain", "author": ["Jonathan Richard Shewchuk"], "venue": null, "citeRegEx": "22", "shortCiteRegEx": "22", "year": 1994}, {"title": "Approximation Theory and Approximation Practice", "author": ["Lloyd N. Trefethen"], "venue": null, "citeRegEx": "23", "shortCiteRegEx": "23", "year": 2013}, {"title": "Numerical methods for the qcdd overlap operator", "author": ["Jasper van den Eshof", "Andreas Frommer", "Th Lippert", "Klaus Schilling", "Henk A. van der Vorst"], "venue": "i. sign-function and error bounds. Computer Physics Communications,", "citeRegEx": "24", "shortCiteRegEx": "24", "year": 2002}], "referenceMentions": [{"referenceID": 16, "context": "For instance, to project a vector onto the top 1000 principal components of a high-dimensional dataset, even the most efficient Krylov-based [18] or Lanczos-based [4] methods require a running time that is proportional to 1000 \u00d7 40 = 4 \u00d7 104 times the input matrix sparsity, if the Krylov or Lanczos method is executed for 40 iterations.", "startOffset": 141, "endOffset": 145}, {"referenceID": 3, "context": "For instance, to project a vector onto the top 1000 principal components of a high-dimensional dataset, even the most efficient Krylov-based [18] or Lanczos-based [4] methods require a running time that is proportional to 1000 \u00d7 40 = 4 \u00d7 104 times the input matrix sparsity, if the Krylov or Lanczos method is executed for 40 iterations.", "startOffset": 163, "endOffset": 166}, {"referenceID": 11, "context": "[13] that approximate PCP and PCR be solved with a running time independent of the number of principal components above threshold \u03bb.", "startOffset": 0, "endOffset": 4}, {"referenceID": 16, "context": "This is known as the eigengap assumption, which is rarely satisfied in practice [18].", "startOffset": 80, "endOffset": 84}, {"referenceID": 15, "context": "In other words, \u2022 to project any vector \u03c7 \u2208 Rd to top principal components, we can compute g(S)\u03c7 instead; and Ridge regression is often considered as an easy-to-solve machine learning problem: using for instance SVRG [17], one can usually solve ridge regression to an 10\u22128 accuracy with at most 40 passes of the data.", "startOffset": 217, "endOffset": 221}, {"referenceID": 15, "context": "Efficient routines such as SVRG [17] solve ridge regression and thus compute Su for any u \u2208 Rd, with running times only logarithmically in 1/\u03b5\u2032.", "startOffset": 32, "endOffset": 36}, {"referenceID": 8, "context": "1) is \u0398 ( \u03b3\u22121 log(1/\u03b5) ) [9, 10].", "startOffset": 25, "endOffset": 32}, {"referenceID": 9, "context": "1) is \u0398 ( \u03b3\u22121 log(1/\u03b5) ) [9, 10].", "startOffset": 25, "endOffset": 32}, {"referenceID": 5, "context": "There are a few attempts to reduce the cost of PCA when solving PCR, by for instance approximating the matrix AP\u03bb where P\u03bb is the PCP projection matrix [6, 7].", "startOffset": 152, "endOffset": 158}, {"referenceID": 6, "context": "There are a few attempts to reduce the cost of PCA when solving PCR, by for instance approximating the matrix AP\u03bb where P\u03bb is the PCP projection matrix [6, 7].", "startOffset": 152, "endOffset": 158}, {"referenceID": 1, "context": "A significant number of papers have focused on the low-rank case of PCA [2, 4, 18] and its online variant [3].", "startOffset": 72, "endOffset": 82}, {"referenceID": 3, "context": "A significant number of papers have focused on the low-rank case of PCA [2, 4, 18] and its online variant [3].", "startOffset": 72, "endOffset": 82}, {"referenceID": 16, "context": "A significant number of papers have focused on the low-rank case of PCA [2, 4, 18] and its online variant [3].", "startOffset": 72, "endOffset": 82}, {"referenceID": 2, "context": "A significant number of papers have focused on the low-rank case of PCA [2, 4, 18] and its online variant [3].", "startOffset": 106, "endOffset": 109}, {"referenceID": 19, "context": "Several results have addressed Krylov methods for applying the sign function in the so-called Krylov subspace, without explicitly constructing any approximate polynomial [21, 24].", "startOffset": 170, "endOffset": 178}, {"referenceID": 22, "context": "Several results have addressed Krylov methods for applying the sign function in the so-called Krylov subspace, without explicitly constructing any approximate polynomial [21, 24].", "startOffset": 170, "endOffset": 178}, {"referenceID": 14, "context": "4 Other iterative methods have also been proposed, see Section 5 of textbook [16].", "startOffset": 77, "endOffset": 81}, {"referenceID": 17, "context": "[19]) provide rational approximations to the matrix sign function as opposed to polynomial approximations.", "startOffset": 0, "endOffset": 4}, {"referenceID": 11, "context": "[13] differ from these cited works, because we have only accessed an approximate ridge regression oracle, so ensuring a polynomial approximation to the sign function and ensuring its stability are crucial.", "startOffset": 0, "endOffset": 4}, {"referenceID": 20, "context": "Perhaps the most celebrated example is to approximate S\u22121 using polynomials on S, used in the analysis of conjugate gradient [22].", "startOffset": 125, "endOffset": 129}, {"referenceID": 13, "context": "[15] used Chebyshev polynomials to approximate the trace of the matrix sign function, i.", "startOffset": 0, "endOffset": 4}, {"referenceID": 11, "context": "Following the tradition of [13] and keeping the notations light, we assume without loss of generality that \u03c3max(A) \u2264 1.", "startOffset": 27, "endOffset": 31}, {"referenceID": 20, "context": "Most notably, (1) Conjugate gradient [22] or accelerated gradient descent [20] gives fastest full-gradient methods; (2) SVRG [17] and its acceleration Katyusha [1] give the fastest stochastic-gradient method; and (3) NUACDM [5] gives the fastest coordinate-descent method.", "startOffset": 37, "endOffset": 41}, {"referenceID": 18, "context": "Most notably, (1) Conjugate gradient [22] or accelerated gradient descent [20] gives fastest full-gradient methods; (2) SVRG [17] and its acceleration Katyusha [1] give the fastest stochastic-gradient method; and (3) NUACDM [5] gives the fastest coordinate-descent method.", "startOffset": 74, "endOffset": 78}, {"referenceID": 15, "context": "Most notably, (1) Conjugate gradient [22] or accelerated gradient descent [20] gives fastest full-gradient methods; (2) SVRG [17] and its acceleration Katyusha [1] give the fastest stochastic-gradient method; and (3) NUACDM [5] gives the fastest coordinate-descent method.", "startOffset": 125, "endOffset": 129}, {"referenceID": 0, "context": "Most notably, (1) Conjugate gradient [22] or accelerated gradient descent [20] gives fastest full-gradient methods; (2) SVRG [17] and its acceleration Katyusha [1] give the fastest stochastic-gradient method; and (3) NUACDM [5] gives the fastest coordinate-descent method.", "startOffset": 160, "endOffset": 163}, {"referenceID": 4, "context": "Most notably, (1) Conjugate gradient [22] or accelerated gradient descent [20] gives fastest full-gradient methods; (2) SVRG [17] and its acceleration Katyusha [1] give the fastest stochastic-gradient method; and (3) NUACDM [5] gives the fastest coordinate-descent method.", "startOffset": 224, "endOffset": 227}, {"referenceID": 21, "context": "6 ([23]).", "startOffset": 3, "endOffset": 7}, {"referenceID": 21, "context": "2 of [23]).", "startOffset": 5, "endOffset": 9}, {"referenceID": 21, "context": "2 of [23]).", "startOffset": 5, "endOffset": 9}, {"referenceID": 11, "context": "[13] work only with matrices A that satisfy the eigengap assumption, that is, A has no singular value in the range [ \u221a \u03bb(1\u2212 \u03b3), \u221a \u03bb(1 + \u03b3)].", "startOffset": 0, "endOffset": 4}, {"referenceID": 11, "context": "[13], this computation is problematic if \u03be is only approximate.", "startOffset": 0, "endOffset": 4}, {"referenceID": 11, "context": "[13] showed that Lemma 3.", "startOffset": 0, "endOffset": 4}, {"referenceID": 0, "context": "\u2022 gn(x) \u2208 [0, 1] for every x \u2208 [0, \u03b1] and gn(x) \u2208 [\u22121, 0] for every x \u2208 [\u2212\u03b1, 0].", "startOffset": 10, "endOffset": 16}, {"referenceID": 8, "context": "Note that our degree n = O ( \u03b1\u22121 log(1/\u03b1\u03b5) ) is near-optimal, because the minimum degree for a polynomial to satisfy even only the first item is \u0398 ( \u03b1\u22121 log(1/\u03b5) ) [9, 10].", "startOffset": 164, "endOffset": 171}, {"referenceID": 9, "context": "Note that our degree n = O ( \u03b1\u22121 log(1/\u03b1\u03b5) ) is near-optimal, because the minimum degree for a polynomial to satisfy even only the first item is \u0398 ( \u03b1\u22121 log(1/\u03b5) ) [9, 10].", "startOffset": 164, "endOffset": 171}, {"referenceID": 8, "context": "However, the results of [9, 10] are not constructive, and thus may not lead to stable matrix polynomials.", "startOffset": 24, "endOffset": 31}, {"referenceID": 9, "context": "However, the results of [9, 10] are not constructive, and thus may not lead to stable matrix polynomials.", "startOffset": 24, "endOffset": 31}, {"referenceID": 7, "context": "2 with the help from Elloit\u2019s forward-backward transformation [8].", "startOffset": 62, "endOffset": 65}, {"referenceID": 12, "context": "We wish to point out that although Chebyshev polynomials are known to be stable under error when computed on scalars [14], it is not immediately clear why it holds also for matrices.", "startOffset": 117, "endOffset": 121}, {"referenceID": 11, "context": "[13].", "startOffset": 0, "endOffset": 4}, {"referenceID": 11, "context": "In contrast, the number of ridge-regression oracle calls was \u0398(\u03b3\u22122 log 1 \u03b3\u03b5) for PCP and \u0398(\u03b3 \u22122 log 1 \u03b3\u03bb\u03b5) for PCR in [13].", "startOffset": 118, "endOffset": 122}, {"referenceID": 11, "context": "In the same way as [13], we conclude this paper with an empirical evaluation to demonstrate our theorems.", "startOffset": 19, "endOffset": 23}, {"referenceID": 11, "context": "\u2022 We generate the synthetic dataset in the same way as [13].", "startOffset": 55, "endOffset": 59}, {"referenceID": 10, "context": "\u2022 As for the real-life dataset, we use mnist [11].", "startOffset": 45, "endOffset": 49}, {"referenceID": 11, "context": "[13] (which we call FMMS for short) and minimized the number of calls to ridge regression in our implementations.", "startOffset": 0, "endOffset": 4}, {"referenceID": 11, "context": "The x-axis of these plots represent the number of calls to ridge regression, and in Figure 2 we use exact implementations of ridge regression similar to the experiments in [13].", "startOffset": 172, "endOffset": 176}, {"referenceID": 11, "context": "Note that the horizontal axis starts with 0 for projection performances (second and third column) and with 10 This is a cheap procedure and for instance can be done by power method [13].", "startOffset": 181, "endOffset": 185}, {"referenceID": 15, "context": "\u2022 ridge-SVRG: we run the SVRG [17] method for 50 passes to solve each ridge regression.", "startOffset": 30, "endOffset": 34}, {"referenceID": 2, "context": "The update rule of sk tells us that \u2200i \u2208 [3], k \u2265 1: vi,k = 1 \u03bb k \u2211", "startOffset": 41, "endOffset": 44}, {"referenceID": 21, "context": "1 of [23]).", "startOffset": 5, "endOffset": 9}, {"referenceID": 12, "context": "120) of [14].", "startOffset": 8, "endOffset": 12}], "year": 2017, "abstractText": "We solve principal component regression (PCR), up to a multiplicative accuracy 1 + \u03b3, by reducing the problem to \u00d5(\u03b3\u22121) black-box calls of ridge regression. Therefore, our algorithm does not require any explicit construction of the top principal components, and is suitable for large-scale PCR instances. In contrast, previous result requires \u00d5(\u03b3\u22122) such black-box calls. We obtain this result by developing a general stable recurrence formula for matrix Chebyshev polynomials, and a degree-optimal polynomial approximation to the matrix sign function. Our techniques may be of independent interests, especially when designing iterative methods.", "creator": "LaTeX with hyperref package"}}}