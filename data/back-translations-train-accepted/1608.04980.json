{"id": "1608.04980", "review": {"conference": "iclr", "VERSION": "v1", "DATE_OF_SUBMISSION": "17-Aug-2016", "title": "Mollifying Networks", "abstract": "The optimization of deep neural networks can be more challenging than traditional convex optimization problems due to the highly non-convex nature of the loss function, e.g. it can involve pathological landscapes such as saddle-surfaces that can be difficult to escape for algorithms based on simple gradient descent. In this paper, we attack the problem of optimization of highly non-convex neural networks by starting with a smoothed -- or \\textit{mollified} -- objective function that gradually has a more non-convex energy landscape during the training. Our proposition is inspired by the recent studies in continuation methods: similar to curriculum methods, we begin learning an easier (possibly convex) objective function and let it evolve during the training, until it eventually goes back to being the original, difficult to optimize, objective function. The complexity of the mollified networks is controlled by a single hyperparameter which is annealed during the training. We show improvements on various difficult optimization tasks and establish a relationship with recent works on continuation methods for neural networks and mollifiers.", "histories": [["v1", "Wed, 17 Aug 2016 14:37:34 GMT  (347kb,D)", "http://arxiv.org/abs/1608.04980v1", null]], "reviews": [], "SUBJECTS": "cs.LG cs.NE", "authors": ["caglar gulcehre", "marcin moczulski", "francesco visin", "yoshua bengio"], "accepted": true, "id": "1608.04980"}, "pdf": {"name": "1608.04980.pdf", "metadata": {"source": "CRF", "title": "Mollifying Networks", "authors": ["Caglar Gulcehre", "Marcin Moczulski", "Francesco Visin", "Yoshua Bengio"], "emails": [], "sections": [{"heading": "1 Introduction", "text": "Deep neural networks - i.e. convolutional networks (LeCun et al., 1989), LSTMs (Hochreiter and Schmidhuber, 1997a) or GRUs (Cho et al., 2014) - achieve state-of-the-art results on a range of challenging tasks as object classification and detection (Szegedy et al., 2014), semantic segmentation (Visin et al., 2015), speech recognition (Hinton et al., 2012), statistic machine translation (Sutskever et al., 2014), playing Atari et al., 2013) and Go (Silver et al., 2016)."}, {"heading": "2 Mollifying Objective Functions", "text": "In this section, we first describe continuation and annealing methods, then introduce mollifiers and show how they can be used to facilitate optimization as a continuation method that gradually reduces the degree of smoothing applied to the training goal of a neural network."}, {"heading": "2.1 Continuation and Annealing Methods", "text": "This method is very successful in overcoming difficult optimization problems involving non-convex objective functions, and possibly taking into account points of non-differentiability. In machine learning, it is possible to consider a sequence of optimization problems corresponding to the optimization problem of interest (see Fig. 1). These methods are very successful in overcoming difficult optimization problems involving non-convex objective functions."}, {"heading": "2.2 Generalized and Noisy Mollifiers", "text": "We present a slight generalization of the concept of mollifiers, which includes the approach we have explored here, which aims at optimization by means of a continuation method using stochastic gradient lineage. Definition 2.3. (Generalized Mollifier) A generalized mollifier is a transformation T\u03c3 (f) in Rk to Rk of a function f that: lim \u03c3 \u2192 0 T\u03c3f = f, (4) f0 = lim \u03c3 \u2192 T\u03c3f is a convex (T\u03c3f) (x) x exists. (3) Furthermore, we consider noisy mollififiers as the expected value of a stochastic function \u03c6 (x) under a source of deviation with deviation. (T\u03c3f) = Esch (x) = Mollifier is a deviation of the mollifier form. (4) We call a stochastic function."}, {"heading": "3 Method", "text": "We focus on improving the optimization of neural networks with Schmidtanh (\u00b7) and sigmoid (\u00b7) activation functions, as these are known to be particularly difficult to optimize and play a crucial role in models that include gating (e.g. LSTM, GRU) and piecemeal linear activations such as ReLUs. However, the general principles we present in this paper can also be easily adapted to the other activation functions. We propose a novel learning algorithm to mitigate the cost of a neural network that addresses an important drawback of the previously proposed noisy training methods: as noise increases, it can dominate the learning process and cause the algorithm to perform a random walk on the energy landscape of the objective function. Conversely, in this paper we suggest that noise becomes greater than the SGD properties (e.g. convex), but still has significant objective function. To this end, we define the desired network behavior in those cases where noise is very large or very small."}, {"heading": "3.1 Simplifying the Objective Function for Feedforward Networks", "text": "For each unit of each layer, we copy either the activation (output) of the corresponding unit of the previous layer (the identity path in Figure 2) or the output of a non-linear transformation of the previous layer (Figure 2). (Wl) (8) is a weight matrix applied to hl-1 and is a vector of binary decisions for each unit (the revolutionary path in Figure 2): h) l = 1, 0, 0; Wl) (8) \u03c6 (hl-1, 1, 1; Wl); (Wl) = 1 + (1 \u2212 2) h: l (9) hl = 1, 0; Wl). (10) To decide which path to take, for each unit in the network, a binary stochastic decision is made by drawing from a binomial random variable with the probability."}, {"heading": "3.2 Mollifying LSTMs and GRUs", "text": "Similarly, it is possible to smooth the objective functions of the LSTM and GRU networks by starting the optimization process with a simpler objective function such as optimizing a word2vec, BoW-LM or CRF lens function at the beginning of the training and gradually increasing the difficulty of optimization by increasing the capacity of the network.For GRUs, we set the update gate to 1t - where t is the glow time step - and reset the gate to 1 when the noise is very high. In this way, the LSTM behaves like a BOW model. To achieve this behavior, we can set the exit gate to 1 and the entrance gate to 1t and forget the gate to 1 \u2212 1 t when the noise is very high."}, {"heading": "3.3 Annealing Schedule for p", "text": "We used a different schedule for each layer of the network, so that the noise in the lower layers is annealed more quickly, similar to the linear declining probability of layers in Huang et al. (2016b). In our experiments, we used an annealing plan similar to the inverse sigmoid rule in Bengio et al. (2015) with plt, plt = 1 \u2212 e \u2212 kvtl tL (14) with the hyperparameter k \u2265 0 in the tth update for the lth layer, where L is the number of layers of the model. We stopped annealing when the expected depth pt = \u2211 L i = 1 p l t reaches a certain threshold. vt is a moving average of loss 1 of the network, so the behavior of the loss / optimization can directly affect the annealing behavior of the network. Thus, we have: lim vt \u2192 \u221e plt = 1 and lim vt \u2192 0 plt = 0. (15) This has the following characteristic: When the model becomes apparent during a high level of introspection, the introspection becomes stronger."}, {"heading": "4 Experiments", "text": "In this section, we mainly focus on building models that are difficult to optimize, especially deep MLPs with sigmoid or tanh activation capabilities. Details of the experimental method are described in Appendix C."}, {"heading": "4.1 Deep MLP Experiments", "text": "We are experimenting with a 40-dimensional parity problem with 6-layer MLP using sigmoid activation function. All models are initialized with glorot initialization Glorot et al. (2011) and trained with SGD with dynamics. We are comparing an MLP with residual connections using batch normalization and a melted network with sigmoid activation function. As shown in Figure 4, the melted network faster converges. Deep Pentomino Pentomino is a toy image dataset where each image has 3 pentomino blocks. The task is to predict whether there is another shape in the image or not (Validity Theory and Bengio, 2013). The best reported result in this task is hyperparametry accuracy (Gulcehre, 2014). The same model as our models have a better shape in the image or not (Validity and Bengio, 2013)."}, {"heading": "4.2 LSTM Experiments", "text": "We trained a word2vec model on Wikipedia with embedded size 500 (Mikolov et al., 2014) with a vocabulary of size 374557. LSTM Language Modeling We evaluate our model based on language modeling LSTM. Our base model is a 2-layer stacked LSTM without regulation. We observed that molten model converges faster and achieves better results. We provide the results for modeling the PTB language in Table 2."}, {"heading": "5 Conclusion", "text": "We propose a novel neural network formation method inspired by the idea of continuation, smoothing techniques, and recent advances in non-convex optimization algorithms, which facilitates learning by starting from a simpler model that solves a well-preserved problem and gradually moves to a more complicated environment. We show improvements in very deep models, difficult-to-optimize tasks, and comparisons with powerful techniques such as batch normalization and residual connections. Our future work will include testing this method on large language tasks that require a long familiarization period, such as machine translation and voice modeling. It is also fascinating to understand how generalization performance is affected by molten networks, as noise injected during training can act as a regulator."}, {"heading": "A Monte-Carlo Estimate of Mollification", "text": "LK (\u03b8) = (L \u0445 K) (\u03b8) = \u0442 C L (\u03b8 \u2212 \u0432) K (\u0432) d\u0432, which can be estimated by a Monte Carlo: \u2248 1 N \u2211 i = 1 L (\u03b8 \u2212 \u0432 (i)), whereby the number (i) \"yields\" \u0445 LK (\u03b8) \u0445 1 N \u0445 i = 1 \u0445 L (\u03b8 \u2212 \u0432 (i)) \u0445 \u03b8 is a realization of the random noise variable. (16) Therefore, introducing additive noise into the input range of L (\u03b8) is tantamount to calming down."}, {"heading": "B Linearizing ReLU Activation Function", "text": "Instead of the complicated equation 10. We can use a simpler equation as in equation 17 to achieve the linearization of the activation function if we have a very high noise in the activation function: si = minimal (| xi |, p\u03c3 (xi) |] (17) \u0432 (xi, amouni, wi) = f (xi) \u2212 si (18)"}, {"heading": "C Experimental Details", "text": "C.1 MNISTThe weights of the models are initialized with Glorot & Bengio initialization Glorot et al. (2011). We use the learning rate of 4e \u2212 4 together with RMSProp. We initialize ai parameters of the attenuated activation function by sampling them from a uniform distribution, U [\u2212 2, 2]. We used 100 hidden units at each level with a minibatch size 500.C.2 PentominoWe train a 6-layer MLP with sigmoid activation function using SGD and dynamics. We used 200 units per shift with sigmoid activation functions. We use a learning rate of 1e \u2212 3.C.3 CIFAR10We use the same model with the same hyperparameters for ResNet, melted networks and dynamics. We rent the same units per shift with sigmoid activation functions."}, {"heading": "D Derivation of the Noisy Activations for the Gating", "text": "So let us suppose that zlt = x l + p l t\u03c3 (x), (19) t = E [f (zlt)], provided f (\u00b7) behaves similarly to a linear function (20) E [f (zlt)] \u2248 f (E (z l t)), since we use hard sigmoid for f (\u00b7). (21) f \u2212 1 (t) \u2248 E [zlt] (22) (23) As in Equation 19, we can write the expectation of this equation as follows: f \u2212 1 (t) \u2012 xlt + plt\u043c (x) E-sigmoid (x)."}], "references": [{"title": "Numerical Continuation Methods", "author": ["E.L. Allgower", "K. Georg"], "venue": "An Introduction. Springer-Verlag,", "citeRegEx": "Allgower and Georg.,? \\Q1980\\E", "shortCiteRegEx": "Allgower and Georg.", "year": 1980}, {"title": "Neural machine translation by jointly learning to align and translate", "author": ["D. Bahdanau", "K. Cho", "Y. Bengio"], "venue": "arXiv preprint arXiv:1409.0473,", "citeRegEx": "Bahdanau et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Bahdanau et al\\.", "year": 2014}, {"title": "Scheduled sampling for sequence prediction with recurrent neural networks", "author": ["S. Bengio", "O. Vinyals", "N. Jaitly", "N. Shazeer"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Bengio et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Bengio et al\\.", "year": 2015}, {"title": "Estimating or propagating gradients through stochastic neurons", "author": ["Y. Bengio"], "venue": "Technical Report arXiv:1305.2982, Universite de Montreal,", "citeRegEx": "Bengio.,? \\Q2013\\E", "shortCiteRegEx": "Bengio.", "year": 2013}, {"title": "Curriculum learning", "author": ["Y. Bengio", "J. Louradour", "R. Collobert", "J. Weston"], "venue": "In Proceedings of the 26th annual international conference on machine learning,", "citeRegEx": "Bengio et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Bengio et al\\.", "year": 2009}, {"title": "Une approche th\u00e9orique de l\u2019apprentissage connexioniste; applications \u00e0 la reconnaissance de la parole", "author": ["L. Bottou"], "venue": "PhD thesis, Universite\u0301 de Paris XI,", "citeRegEx": "Bottou.,? \\Q1991\\E", "shortCiteRegEx": "Bottou.", "year": 1991}, {"title": "Online algorithms and stochastic approximations", "author": ["L. Bottou"], "venue": "Online Learning in Neural Networks", "citeRegEx": "Bottou.,? \\Q1998\\E", "shortCiteRegEx": "Bottou.", "year": 1998}, {"title": "Smoothing methods for nonsmooth, nonconvex minimization", "author": ["X. Chen"], "venue": "Math. Program. Ser. B,", "citeRegEx": "Chen.,? \\Q2012\\E", "shortCiteRegEx": "Chen.", "year": 2012}, {"title": "Learning phrase representations using rnn encoder-decoder for statistical machine translation", "author": ["K. Cho", "B. Van Merri\u00ebnboer", "C. Gulcehre", "D. Bahdanau", "F. Bougares", "H. Schwenk", "Y. Bengio"], "venue": "arXiv preprint arXiv:1406.1078,", "citeRegEx": "Cho et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Cho et al\\.", "year": 2014}, {"title": "The loss surface of multilayer", "author": ["A. Choromanska", "M. Henaff", "M. Mathieu", "G.B. Arous", "Y. LeCun"], "venue": null, "citeRegEx": "Choromanska et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Choromanska et al\\.", "year": 2014}, {"title": "Identifying and attacking the saddle point problem in high-dimensional non-convex optimization", "author": ["Y. Dauphin", "R. Pascanu", "C. Gulcehre", "K. Cho", "S. Ganguli", "Y. Bengio"], "venue": "In NIPS\u20192014,", "citeRegEx": "Dauphin et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Dauphin et al\\.", "year": 2014}, {"title": "Partial differential equations", "author": ["L.C. Evans"], "venue": "Graduate Studies in Mathematics,", "citeRegEx": "Evans.,? \\Q1998\\E", "shortCiteRegEx": "Evans.", "year": 1998}, {"title": "Deep sparse rectifier neural networks", "author": ["X. Glorot", "A. Bordes", "Y. Bengio"], "venue": "In AISTATS,", "citeRegEx": "Glorot et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Glorot et al\\.", "year": 2011}, {"title": "Adaptive computation time for recurrent neural networks", "author": ["A. Graves"], "venue": "arXiv preprint arXiv:1603.08983,", "citeRegEx": "Graves.,? \\Q2016\\E", "shortCiteRegEx": "Graves.", "year": 2016}, {"title": "Knowledge matters: Importance of prior information for optimization", "author": ["\u00c7. G\u00fcl\u00e7ehre", "Y. Bengio"], "venue": "arXiv preprint arXiv:1301.4083,", "citeRegEx": "G\u00fcl\u00e7ehre and Bengio.,? \\Q2013\\E", "shortCiteRegEx": "G\u00fcl\u00e7ehre and Bengio.", "year": 2013}, {"title": "Learned-norm pooling for deep feedforward and recurrent neural networks", "author": ["C. Gulcehre", "K. Cho", "R. Pascanu", "Y. Bengio"], "venue": "In Machine Learning and Knowledge Discovery in Databases,", "citeRegEx": "Gulcehre et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Gulcehre et al\\.", "year": 2014}, {"title": "Noisy activation functions", "author": ["C. Gulcehre", "M. Moczulski", "M. Denil", "Y. Bengio"], "venue": null, "citeRegEx": "Gulcehre et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Gulcehre et al\\.", "year": 2016}, {"title": "Deep residual learning for image recognition", "author": ["K. He", "X. Zhang", "S. Ren", "J. Sun"], "venue": "arXiv preprint arXiv:1512.03385,", "citeRegEx": "He et al\\.,? \\Q2015\\E", "shortCiteRegEx": "He et al\\.", "year": 2015}, {"title": "Deep neural networks for acoustic modeling in speech recognition", "author": ["G. Hinton", "L. Deng", "D. Yu", "G. Dahl", "A. rahman Mohamed", "N. Jaitly", "A. Senior", "V. Vanhoucke", "P. Nguyen", "T. Sainath", "B. Kingsbury"], "venue": "Signal Processing Magazine,", "citeRegEx": "Hinton et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Hinton et al\\.", "year": 2012}, {"title": "Long short-term memory", "author": ["S. Hochreiter", "J. Schmidhuber"], "venue": "Neural Computation,", "citeRegEx": "Hochreiter and Schmidhuber.,? \\Q1997\\E", "shortCiteRegEx": "Hochreiter and Schmidhuber.", "year": 1997}, {"title": "Deep networks with stochastic depth", "author": ["G. Huang", "Y. Sun", "Z. Liu", "D. Sedra", "K. Weinberger"], "venue": "arXiv preprint arXiv:1603.09382,", "citeRegEx": "Huang et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Huang et al\\.", "year": 2016}, {"title": "Deep networks with stochastic depth", "author": ["G. Huang", "Y. Sun", "Z. Liu", "D. Sedra", "K.Q. Weinberger"], "venue": "CoRR, abs/1603.09382,", "citeRegEx": "Huang et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Huang et al\\.", "year": 2016}, {"title": "Batch normalization: Accelerating deep network training by reducing internal covariate", "author": ["S. Ioffe", "C. Szegedy"], "venue": "shift. CoRR,", "citeRegEx": "Ioffe and Szegedy.,? \\Q2015\\E", "shortCiteRegEx": "Ioffe and Szegedy.", "year": 2015}, {"title": "Grid long short-term memory", "author": ["N. Kalchbrenner", "I. Danihelka", "A. Graves"], "venue": "arXiv preprint arXiv:1507.01526,", "citeRegEx": "Kalchbrenner et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Kalchbrenner et al\\.", "year": 2015}, {"title": "Optimization by simulated annealing", "author": ["S. Kirkpatrick", "C.D.G. Jr.", "M.P. Vecchi"], "venue": null, "citeRegEx": "Kirkpatrick et al\\.,? \\Q1983\\E", "shortCiteRegEx": "Kirkpatrick et al\\.", "year": 1983}, {"title": "Backpropagation applied to handwritten zip code recognition", "author": ["Y. LeCun", "B. Boser", "J.S. Denker", "D. Henderson", "R.E. Howard", "W. Hubbard", "L.D. Jackel"], "venue": "Neural Comput.,", "citeRegEx": "LeCun et al\\.,? \\Q1989\\E", "shortCiteRegEx": "LeCun et al\\.", "year": 1989}, {"title": "Playing atari with deep reinforcement learning", "author": ["V. Mnih", "K. Kavukcuoglu", "D. Silver", "A. Graves", "I. Antonoglou", "D. Wierstra"], "venue": "Technical report,", "citeRegEx": "Mnih et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Mnih et al\\.", "year": 2013}, {"title": "Training recurrent neural networks by diffusion", "author": ["H. Mobahi"], "venue": "arXiv preprint arXiv:1601.04114,", "citeRegEx": "Mobahi.,? \\Q2016\\E", "shortCiteRegEx": "Mobahi.", "year": 2016}, {"title": "Adding gradient noise improves learning for very deep networks", "author": ["A. Neelakantan", "L. Vilnis", "Q.V. Le", "I. Sutskever", "L. Kaiser", "K. Kurach", "J. Martens"], "venue": "CoRR, abs/1511.06807,", "citeRegEx": "Neelakantan et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Neelakantan et al\\.", "year": 2015}, {"title": "Mastering the game of go with deep neural networks and tree", "author": ["D. Silver", "A. Huang", "C.J. Maddison", "A. Guez", "L. Sifre", "G. Van Den Driessche", "J. Schrittwieser", "I. Antonoglou", "V. Panneershelvam", "M. Lanctot"], "venue": "search. Nature,", "citeRegEx": "Silver et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Silver et al\\.", "year": 2016}, {"title": "Training very deep networks", "author": ["R.K. Srivastava", "K. Greff", "J. Schmidhuber"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Srivastava et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Srivastava et al\\.", "year": 2015}, {"title": "Sequence to sequence learning with neural networks", "author": ["I. Sutskever", "O. Vinyals", "Q.V. Le"], "venue": "In Advances in neural information processing systems,", "citeRegEx": "Sutskever et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Sutskever et al\\.", "year": 2014}, {"title": "Going deeper with convolutions", "author": ["C. Szegedy", "W. Liu", "Y. Jia", "P. Sermanet", "S. Reed", "D. Anguelov", "D. Erhan", "V. Vanhoucke", "A. Rabinovich"], "venue": "Technical report,", "citeRegEx": "Szegedy et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Szegedy et al\\.", "year": 2014}, {"title": "Reseg: A recurrent neural network for object segmentation", "author": ["F. Visin", "K. Kastner", "A. Courville", "Y. Bengio", "M. Matteucci", "K. Cho"], "venue": "arXiv preprint arXiv:1511.07053,", "citeRegEx": "Visin et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Visin et al\\.", "year": 2015}, {"title": "Learning to execute", "author": ["W. Zaremba", "I. Sutskever"], "venue": "arXiv preprint arXiv:1410.4615,", "citeRegEx": "Zaremba and Sutskever.,? \\Q2014\\E", "shortCiteRegEx": "Zaremba and Sutskever.", "year": 2014}], "referenceMentions": [{"referenceID": 25, "context": "convolutional networks (LeCun et al., 1989), LSTMs (Hochreiter and Schmidhuber, 1997a) or GRUs (Cho et al.", "startOffset": 23, "endOffset": 43}, {"referenceID": 8, "context": ", 1989), LSTMs (Hochreiter and Schmidhuber, 1997a) or GRUs (Cho et al., 2014) \u2013 achieve state of the art results on a range of challenging tasks like object classification and detection (Szegedy et al.", "startOffset": 59, "endOffset": 77}, {"referenceID": 32, "context": ", 2014) \u2013 achieve state of the art results on a range of challenging tasks like object classification and detection (Szegedy et al., 2014), semantic segmentation (Visin et al.", "startOffset": 116, "endOffset": 138}, {"referenceID": 33, "context": ", 2014), semantic segmentation (Visin et al., 2015), speech recognition (Hinton et al.", "startOffset": 31, "endOffset": 51}, {"referenceID": 18, "context": ", 2015), speech recognition (Hinton et al., 2012), statistical machine translation (Sutskever et al.", "startOffset": 28, "endOffset": 49}, {"referenceID": 31, "context": ", 2012), statistical machine translation (Sutskever et al., 2014; Bahdanau et al., 2014), playing Atari (Mnih et al.", "startOffset": 41, "endOffset": 88}, {"referenceID": 1, "context": ", 2012), statistical machine translation (Sutskever et al., 2014; Bahdanau et al., 2014), playing Atari (Mnih et al.", "startOffset": 41, "endOffset": 88}, {"referenceID": 26, "context": ", 2014), playing Atari (Mnih et al., 2013) and Go (Silver et al.", "startOffset": 23, "endOffset": 42}, {"referenceID": 29, "context": ", 2013) and Go (Silver et al., 2016).", "startOffset": 15, "endOffset": 36}, {"referenceID": 6, "context": "When trained with variants of SGD (Bottou, 1998) deep models can be hard to optimize due to their highly non-linear and nonconvex nature (Choromanska et al.", "startOffset": 34, "endOffset": 48}, {"referenceID": 9, "context": "When trained with variants of SGD (Bottou, 1998) deep models can be hard to optimize due to their highly non-linear and nonconvex nature (Choromanska et al., 2014; Dauphin et al., 2014).", "startOffset": 137, "endOffset": 185}, {"referenceID": 10, "context": "When trained with variants of SGD (Bottou, 1998) deep models can be hard to optimize due to their highly non-linear and nonconvex nature (Choromanska et al., 2014; Dauphin et al., 2014).", "startOffset": 137, "endOffset": 185}, {"referenceID": 22, "context": "A number of approaches were proposed to alleviate the difficulty of optimization: addressing the problem of the internal covariate shift with Batch Normalization (Ioffe and Szegedy, 2015), learning with a curriculum (Bengio et al.", "startOffset": 162, "endOffset": 187}, {"referenceID": 4, "context": "A number of approaches were proposed to alleviate the difficulty of optimization: addressing the problem of the internal covariate shift with Batch Normalization (Ioffe and Szegedy, 2015), learning with a curriculum (Bengio et al., 2009) and recently training with diffusion (Mobahi, 2016) - a form of continuation method.", "startOffset": 216, "endOffset": 237}, {"referenceID": 27, "context": ", 2009) and recently training with diffusion (Mobahi, 2016) - a form of continuation method.", "startOffset": 45, "endOffset": 59}, {"referenceID": 28, "context": "At the same time, the impact of noise injection on the behavior of modern deep models has been explored in (Neelakantan et al., 2015) and it has been recently shown that noisy activation functions improve performance on a wide variety of tasks (Gulcehre et al.", "startOffset": 107, "endOffset": 133}, {"referenceID": 16, "context": ", 2015) and it has been recently shown that noisy activation functions improve performance on a wide variety of tasks (Gulcehre et al., 2016).", "startOffset": 118, "endOffset": 141}, {"referenceID": 17, "context": "Skip connections allow to train very deep residual and highway architectures (He et al., 2015; Srivastava et al., 2015) by skipping layers or block of layers.", "startOffset": 77, "endOffset": 119}, {"referenceID": 30, "context": "Skip connections allow to train very deep residual and highway architectures (He et al., 2015; Srivastava et al., 2015) by skipping layers or block of layers.", "startOffset": 77, "endOffset": 119}, {"referenceID": 1, "context": ", 2014; Bahdanau et al., 2014), playing Atari (Mnih et al., 2013) and Go (Silver et al., 2016). When trained with variants of SGD (Bottou, 1998) deep models can be hard to optimize due to their highly non-linear and nonconvex nature (Choromanska et al., 2014; Dauphin et al., 2014). A number of approaches were proposed to alleviate the difficulty of optimization: addressing the problem of the internal covariate shift with Batch Normalization (Ioffe and Szegedy, 2015), learning with a curriculum (Bengio et al., 2009) and recently training with diffusion (Mobahi, 2016) - a form of continuation method. At the same time, the impact of noise injection on the behavior of modern deep models has been explored in (Neelakantan et al., 2015) and it has been recently shown that noisy activation functions improve performance on a wide variety of tasks (Gulcehre et al., 2016). In this paper, we connect the ideas of curriculum learning and continuation methods with those arising from models with skip connections and with layers that compute near-identity transformations. Skip connections allow to train very deep residual and highway architectures (He et al., 2015; Srivastava et al., 2015) by skipping layers or block of layers. Similarly, it is now well known that it is possible to stochastically change the depth of a network during training (Huang et al., 2016b) and still converge. In this work, we introduce the idea of mollification \u2013 a form of differentiable smoothing of the loss function connected to noisy activations \u2013 which can be interpreted as a form adaptive noise injection that only depends on a single hyperparameter. Inspired by Huang et al. (2016b), we exploit the \u2217 This work was done while these students were interning at the MILA lab.", "startOffset": 8, "endOffset": 1672}, {"referenceID": 0, "context": "Continuation methods (Allgower and Georg, 1980), address a complex optimization problem by smoothing the original function, turning it into a different problem that is easier to optimize.", "startOffset": 21, "endOffset": 47}, {"referenceID": 4, "context": "In machine learning, approaches based on curriculum learning (Bengio et al., 2009) are inspired by this principle to define a sequence of gradually more difficult training tasks (or training distributions) that converge to the task of interest.", "startOffset": 61, "endOffset": 82}, {"referenceID": 7, "context": "Gradient-based optimization over a sequence of mollified objective functions has been shown to converge (Chen, 2012).", "startOffset": 104, "endOffset": 116}, {"referenceID": 11, "context": "We can define a weak gradient of a non-differentiable function by convolving it with a mollifier (Evans, 1998): \u2207(L \u2217K)(\u03b8) = (L \u2217 \u2207K)(\u03b8).", "startOffset": 97, "endOffset": 110}, {"referenceID": 24, "context": "Gradually reducing the noise during training is related to a form of simulated annealing (Kirkpatrick et al., 1983).", "startOffset": 89, "endOffset": 115}, {"referenceID": 26, "context": "Similarly to the analysis in Mobahi (2016), we can write a Monte-Carlo estimate of LK(\u03b8) = (L \u2217 K)(\u03b8) \u2248 1 N \u2211N i=1 L(\u03b8 \u2212 \u03be).", "startOffset": 29, "endOffset": 43}, {"referenceID": 24, "context": "This is related to the work of Mobahi (2016), who recently introduced analytic smooths of neural network non-linearities in order to help training recurrent networks.", "startOffset": 31, "endOffset": 45}, {"referenceID": 24, "context": "This is related to the work of Mobahi (2016), who recently introduced analytic smooths of neural network non-linearities in order to help training recurrent networks. The differences with the work presented here are twofold: we use a noisy mollifier (rather than an analytic smooth of the network non-linearities) and we introduce (in the next section) a particular form of the noisy mollifier that empirically proved to work very well. Mobahi (2016) also makes a link between continuation or annealing methods and noise injection, although an earlier form of that observation was already made by Bottou (1991) in the context of gradually decreasing the learning rate when doing stochastic gradient descent.", "startOffset": 31, "endOffset": 451}, {"referenceID": 4, "context": "Mobahi (2016) also makes a link between continuation or annealing methods and noise injection, although an earlier form of that observation was already made by Bottou (1991) in the context of gradually decreasing the learning rate when doing stochastic gradient descent.", "startOffset": 160, "endOffset": 174}, {"referenceID": 3, "context": "The idea of injecting noise into a hard-saturating non-linearity was previously used in Bengio (2013) to help backpropagate signals through semi-hard decisions (with the \u201cnoisy rectifier\u201d stochastic non-linearity).", "startOffset": 88, "endOffset": 102}, {"referenceID": 19, "context": "Due to the noise induced by the backpropagation through the noisy units Hochreiter and Schmidhuber (1997b), SGD is more likely to converge to a flatter-minima because the noise will help SGD escape from sharper local minima.", "startOffset": 72, "endOffset": 107}, {"referenceID": 17, "context": "This is similar to the linearly decaying probability of layers in Huang et al. (2016b). In our experiments, we use an annealing schedule similar to inverse sigmoid rule in Bengio et al.", "startOffset": 66, "endOffset": 87}, {"referenceID": 2, "context": "In our experiments, we use an annealing schedule similar to inverse sigmoid rule in Bengio et al. (2015) with pt, pt = 1\u2212 e\u2212 kvtl tL (14)", "startOffset": 84, "endOffset": 105}, {"referenceID": 13, "context": "1 Deep MLP Experiments Deep Parity Experiments Training neural networks on a high-dimensional parity problem can be challenging (Graves, 2016; Kalchbrenner et al., 2015).", "startOffset": 128, "endOffset": 169}, {"referenceID": 23, "context": "1 Deep MLP Experiments Deep Parity Experiments Training neural networks on a high-dimensional parity problem can be challenging (Graves, 2016; Kalchbrenner et al., 2015).", "startOffset": 128, "endOffset": 169}, {"referenceID": 12, "context": "All the models are initialized with Glorot initialization Glorot et al. (2011) and trained with SGD with momentum.", "startOffset": 58, "endOffset": 79}, {"referenceID": 14, "context": "The task is to predict whether if there is a different shape in the image or not (G\u00fcl\u00e7ehre and Bengio, 2013).", "startOffset": 81, "endOffset": 108}, {"referenceID": 15, "context": "15% accuracy (Gulcehre et al., 2014).", "startOffset": 13, "endOffset": 36}, {"referenceID": 20, "context": "We adapted the hyperparameters of the Stochastic depth network from Huang et al. (2016a) and we used the same hyperparameters for our algorithm.", "startOffset": 68, "endOffset": 89}], "year": 2016, "abstractText": "The optimization of deep neural networks can be more challenging than traditional convex optimization problems due to the highly non-convex nature of the loss function, e.g. it can involve pathological landscapes such as saddle-surfaces that can be difficult to escape for algorithms based on simple gradient descent. In this paper, we attack the problem of optimization of highly non-convex neural networks by starting with a smoothed \u2013 or mollified \u2013 objective function which becomes more complex as the training proceeds. Our proposition is inspired by the recent studies in continuation methods: similar to curriculum methods, we begin learning an easier (possibly convex) objective function and let it evolve during the training, until it eventually goes back to being the original, difficult to optimize, objective function. The complexity of the mollified networks is controlled by a single hyperparameter which is annealed during the training. We show improvements on various difficult optimization tasks and establish a relationship between recent works on continuation methods for neural networks and mollifiers.", "creator": "LaTeX with hyperref package"}}}