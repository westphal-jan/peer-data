{"id": "1608.05258", "review": {"conference": "NIPS", "VERSION": "v1", "DATE_OF_SUBMISSION": "18-Aug-2016", "title": "Parameter Learning for Log-supermodular Distributions", "abstract": "We consider log-supermodular models on binary variables, which are probabilistic models with negative log-densities which are submodular. These models provide probabilistic interpretations of common combinatorial optimization tasks such as image segmentation. In this paper, we focus primarily on parameter estimation in the models from known upper-bounds on the intractable log-partition function. We show that the bound based on separable optimization on the base polytope of the submodular function is always inferior to a bound based on \"perturb-and-MAP\" ideas. Then, to learn parameters, given that our approximation of the log-partition function is an expectation (over our own randomization), we use a stochastic subgradient technique to maximize a lower-bound on the log-likelihood. This can also be extended to conditional maximum likelihood. We illustrate our new results in a set of experiments in binary image denoising, where we highlight the flexibility of a probabilistic model to learn with missing data.", "histories": [["v1", "Thu, 18 Aug 2016 13:55:41 GMT  (24kb)", "http://arxiv.org/abs/1608.05258v1", null]], "reviews": [], "SUBJECTS": "stat.ML cs.LG", "authors": ["tatiana shpakova", "francis r bach"], "accepted": true, "id": "1608.05258"}, "pdf": {"name": "1608.05258.pdf", "metadata": {"source": "CRF", "title": "Parameter Learning for Log-supermodular Distributions", "authors": ["Tatiana Shpakova"], "emails": [], "sections": [{"heading": null, "text": "ar Xiv: 160 8.05 258v 1 [stat.ML] 1 8"}, {"heading": "1 Introduction", "text": "Submodular functions provide efficient and flexible tools for learning on discrete data. Several common combinatorial optimization tasks, such as clustering, image segmentation or document summarization, can be accomplished by minimizing or maximizing a submodular function [1, 8, 14]. In practice, however, it is not always easy to define an appropriate submodular function for a problem at hand, and the availability of exact minimization algorithms and approximate maximization algorithms is guaranteed [12]. In practice, it is not easy to define an appropriate submodular function for a problem. Considering fully labeled data, such as their foreground / background segmentation in image segmentation, structured output prediction methods such as the structured SVM we use, it is common to have missing data, and (b) submodular function minimization within a larger model."}, {"heading": "2 Submodular functions and log-supermodular models", "text": "In this section, we review the relevant theory of submodular functions and recall typical examples of logged supermodular distributions."}, {"heading": "2.1 Submodular functions", "text": "We consider submodular functions on the vertices of the hypercube {0, 1} D. This hypercube representation corresponds to the potential set of {1,.., D}. In fact, we can pass from a vertice of the hypercube to a set by looking at the indices of components equal to one and two by using the indicator vector of the set. For all two vertices of the hypercube, x, y, 0, 1} D, a function f: {0, 1} D \u2192 R is submodular if f (x) + f (y) > f (min {x, y}) + f (max {x, y}), where the min and max operations are taken componentally and correspond to the intersection and union of the associated sets."}, {"heading": "2.2 Log-supermodular distributions", "text": "Log supermodular models are introduced in [5] to model probability distributions on a hypercube, where f: {0, 1} D \u2192 R is a submodular function, so that f (0) = 0 and the partition function Z (f) = 3 (0.1) D is exp (\u2212 f (x)). In general, it is more convenient to bypass the convex log partition function A (f) = 5 (f) = 5 (0.1) D exp (\u2212 f (x)). In general, the calculation of the partition function Z (f) or the log partition function A (f) is insoluble because it includes simple binary Markov random fields - the exact calculation is known as # P -hard [10]."}, {"heading": "2.3 Examples", "text": "Essentially, all submodular functions used in the minimization context can be used as negative log densities [5, 6]. In computer vision, the most common examples are graph sections, which are essentially binary Markov random fields with attractive potentials, but also higher order potentials have been considered [11]. In our experiments, we use graph sections where submodular function minimization can be performed using max flow techniques and is thus efficient [4]. Note that there are extensions of submodular functions to continuous domains that could also be considered [2]."}, {"heading": "3 Upper-bounds on the log-partition function", "text": "In this section we will discuss the most important existing boundaries of the log partition function for log supermodular densities. These boundaries use several properties of submodular functions, in particular the Lov\u00e1sz extension and the base polytopic. Note that lower boundaries can be used on the basis of submodular maximization aspects and superdifferentials [5] to emphasize the density of various boundaries that we present in Figure 1."}, {"heading": "3.1 Base polytope relaxation with L-Field [5]", "text": "This method takes advantage of the fact that any submodular function f (x) can be limited by a modular function s (x), i.e. a linear function of x (0, 1) D in hypercube representation. The submodular function and its lower boundary are also connected by f (x) = maxs (f) s x, which results in: A (f) = log \u00b2 x (0.1) D exp (\u2212 f (x)) = log \u00b2 D min (f) exp (\u2212 s x), which leads to less than min s \u00b2 B (f) log \u00b2 x (0.1) D exp (\u2212 s x) = min s \u00b2 B (f) D (f) x (f) d), since the polytopic (f \u2212 m \u2212 d) d (can be achieved by its membership or by automatic interpretation."}, {"heading": "3.2 \u201cPertub-and-MAP\u201d with logistic distributions", "text": "The main idea is to disrupt the log density, find the maximum a-posteriori configuration (i.e., perform optimization), and then calculate the average over several random errors [9, 17, 19]. The Gumbel distribution on R, whose cumulative distribution function is F (z) = exp (\u2212 exp (\u2212 (z + c))), where c is the Euler constant, is particularly useful. In fact, if {g (y)} y (0,1} D is a collection of independent random variables g (y) = exp (\u2212 exp (\u2212 z + c)), where c is the Euler constant, then the random variable maxy [0,1} D g (y) \u2212 f (y) is such that we have logZ (f) = d = logistic distributions."}, {"heading": "3.3 Comparison of bounds", "text": "In this area, we are in a position to go in search of a solution based on the needs of the people. (...) We have to ask ourselves the question to what extent we are able to find a solution. (...) We have to deal with the question to what extent we are able to find a solution. (...) We have to deal with the question to what extent we are able to find a solution. (...) We have to deal with the question. \"(...) We have to deal with the question.\" (...) We have to deal with the question. \"(...)\" We have to deal with the question. \"(...)\" We have to deal with the question. \"(...)\" We have to deal with the question. \"(...) We have to deal with the question.\""}, {"heading": "3.4 From bounds to approximate inference", "text": "Since linear functions are submodular functions with a convex upper limit for the log partition function, we can derive an approximate marginal probability for each xd function. In fact, according to [9], we consider an exponential family model p (x | t) = exp (\u2212 f (x) + s x \u2212 A (f \u2212 t), where f \u2212 tis is the function x 7 \u2192 f (x) \u2212 t x. If f is assumed to be fixed, it can be regarded as an exponential family with the base size exp (\u2212 f (x)), sufficient statistics x, and A (f \u2212 t) as a log partition function. It is known that the expectation of sufficient statistics under the exponential family model Ep (x | t) x is the log partition function gradient exp (\u2212 f (x)), sufficient statistics x, and A (f) \u2212 log partition function."}, {"heading": "4 Parameter learning through maximum likelihood", "text": "One advantage of log supermodular probability models is the ability to learn the model parameters from the data using the maximum probability principle. In this section we consider that we are given N observations x1,.., xN parameters {0, 1} D, e.g. binary images as shown in Figure 2. We consider a submodular function f (x) as f (x) = \u2211 K = 1 \u03b1kfk (x) \u2212 t x. The modular term t x is explicitly taken into account with t-RD, and the submodular K base functions are represented as f (x), so that the function f remains submodular."}, {"heading": "4.1 Learning with the L-field approximation", "text": "In this section we show that if we replace A (f) with AL field (f), we get a degenerated solution. In fact, we have AL field (\u03b1, t) = min s-B (f) D field (1 + e-sd) = min s-Kk = 1-KfK) D field (1 + e-sd + td), which means that Eq. (4) turns out to be min-RK +, t-RDmin s-B (Kk = 1-KfK) K field (K = 1\u03b1KfK) K field (1NN-K = 1fk (xn)) -t-t (1NN-n = 1xn) + D field (1 + e-sd + td). The minimum in relation to the negative function can be executed in closed form."}, {"heading": "4.2 Learning with the logistic approximation with stochastic gradients", "text": "empempempempempempempempempempempempempempempempempempempempempempempempempempempempempempempempempempempempempempempempempempempempempempempempempempempempempempempempempempempempempempempempempempempempempempempempempempempempempempempempempempempempempempempempempempempempempempempempempempempempempempempempempempempempempempempempempempempempempempempempempempempempempempempempempempempempempempempempempempempempempempempempempempempempempempempempempempempempempempempempempempempempempempempempempempempempempempempempempempempempempempempempempempempempempempempempempempempempempempempempempempempempempempempempempempempempempempempempempempempempempempempempempempempempempempempempempempempempempempempempempempempempempempempempempempempempempempempempempempempempempempempempempempempempempempempempempempempempempempempempempempempempempempempempempempempempempempempempempempempempempempempempempempempempempempempempempempempempempempempempempempempempempempempempempempempempempempempempempempempempempempempempempempempempempempempempempempempempempempempempempempempempempempempempempempempempempempempempempempempempempempempempempempempempempempempempempempempempempempempempempempempempempempempempempempempempempempempempempempempempempempempempempempempempempempempempempempempempempempempempempempempempempempempempempempempempempempempempempempempempempempempempempempempempempempempempempempempempempempempempempempempempempempempempempempempempempempempempempempempempempempempempempempempempempemp"}, {"heading": "4.3 Extension to conditional maximum likelihood", "text": "In our experiments in section 5 we consider a common model of two binary vectors x, z and RD as following (x, z | \u03b1, t, \u03c0) = p (x | \u03b1, t) p (z | x, \u03c0) = exp (\u2212 f (x) \u2212 A (f))) D = 1\u03c0 \u03b4 (zd 6 = xd) d (1 \u2212 \u03c0d) \u03b4 (zd = xd), (6) which corresponds to the sampling x from a log supermodular model and z which shifts the values of x with probability \u03c0d for each d, i.e. a noisy observation of x. We have: log p (x, z \u2212 \u03b1, t) = \u2212 f (x) \u2212 Ms (f) + \u2211 Dd = 1 {\u2212 log (1 + eud) + xdud + zdud + zdud \u2212 z, which can lead to an observation of z."}, {"heading": "4.4 Missing data through maximum likelihood", "text": "In the model of Equation (6), we now assume that we have observed only the problem of noise output z, and we want to perform parameter learning for \u03b1, t, \u03c0. This is a latent variable model for which ML can be applied without further ado. We have: log p (z | \u03b1, t, \u03c0) = log \u2211 x {0,1} p (z, x | \u03b1, t) = log \u2211 x (0,1} D exp (\u2212 f (x) \u2212 A (f))). In practice, we will assume that the probability of noise \u03c0 (zd = xd) d (1 \u2212 \u03c0d) \u03b4 (zd = xd) = A (f \u2212 u + 2u \u0445 z) \u2212 A (f) + z u \u2212 \u2211 Dd = 1 log (1 + e ud). In practice, we assume that the probability of noise GDP (and thus u) is uniform across all elements."}, {"heading": "5 Experiments", "text": "The aim of our experiments is to demonstrate the ability of our approach to removing noise in binary images according to the experimental setup of [9]. We look at the training sample of Ntrain = 100 images of size D = 50 x 50 and the test sample of Ntest = 100 binary images containing a horse silhouette from the Weizmann horse database [3]. First, we add some noise by rotating pixel values independent of the probability \u03c0. In Figure 2, we provide an example from the experimental sample: the original, the noisy and the denozed image (according to our algorithm). We look at the model from section 4.3 with the two functions f1 (x), f2 (x), which are horizontal or vertical intersection functions with binary weights, together with a modular concept of dimension D. For minimization, we use graph sections [4] while dealing with positive or attractive potentials."}, {"heading": "5.1 Supervised image denoising", "text": "We assume that we observe N = 100 pairs (xi, zi) of original noisy images, i = 1,.., N. We perform parameter conclusions with maximum probability by means of stochastic subgradient lineage (over the logistic samples), whereby regularization is determined by the square \"2 norm,\" one parameter for t, one for \u03b1, both by cross-validation. In view of our estimates, we can denode a new image by calculating the \"maximum limit,\" e.g. the maximum a posteriori maxx p (x | z, \u03b1, t) by a single diagram section or the calculation of \"averages\" with 100 logistic samples. To calculate the error, we use the normalized hamming distance and 100 test images. The results are presented in Table 1, where we compare the two types of decoding, as well as a structured output SVM (SVM-Struct [22], which is applied to the same problem."}, {"heading": "5.2 Unsupervised image denoising", "text": "We now consider only N = 100 noise images z1,.., zN to learn the model without the original images, and we use the latent model from Section 4.4. We apply stochastic subgradient descent for the difference between the two convex functions Alogistic to learn the model parameters and use fixed regularization parameters equal to 10 \u2212 2."}, {"heading": "10% 1.9% 0.4% 2.1% 0.4% 6.8% 2.2% 7.0% 2.0%", "text": "We look at two situations with a known noise level \u03c0 or with common learning with \u03b1 and t. The error was calculated using either maximum margins and mean margins. Note that structured SVMs cannot be used here because there is no monitoring. Results are given in Table 2. One explanation for better performance of maximum margins in this case is that the unattended approach tends to smooth the result and maximum margins correct this a bit. If the noise level is known, performance is not greatly deteriorated compared to monitored learning, which shows the ability of probable models to make parameter estimates with missing data. If the noise level is unknown and also learned, the results are worse, even better than a trivial response to moderate noise levels (5% and 10%), but no better than the output of noise for extreme values (1% and 20%). In a demanding, completely unattended standard case, the results are statistically significant (which is 2.2% to our)."}, {"heading": "6 Conclusion", "text": "In this paper, we outlined how approximate conclusions based on stochastic gradients and \"perturb-and-MAP\" ideas could be used to learn parameters of logged supermodular models, allowing us to benefit from the versatility of probabilistic modeling, especially with regard to estimating parameters with missing data. While our experiments focused on simple denociation of binary images, exploration of larger applications of computer vision (such as [21, 24]) should also demonstrate the benefits of mixing probabilistic modeling with submodular functions."}, {"heading": "Acknowledgements", "text": "This work was funded by MacSeNet Innovative Training Network. We would like to thank Sesh Kumar, Anastasia Podosinnikova and Anton Osokin for interesting discussions in connection with this work."}], "references": [{"title": "Learning with submodular functions: a convex optimization perspective", "author": ["F. Bach"], "venue": "Foundations and Trends in Machine Learning,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2013}, {"title": "Submodular functions: from discrete to continuous domains", "author": ["F. Bach"], "venue": "Technical Report 1511.00394,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2015}, {"title": "Combining Top-down and Bottom-up Segmentation", "author": ["E. Borenstein", "E. Sharon", "S. Ullman"], "venue": "In Proc. ECCV,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2004}, {"title": "Fast approximate energy minimization via graph cuts", "author": ["Y. Boykov", "O. Veksler", "R. Zabih"], "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2001}, {"title": "From MAP to Marginals: Variational Inference in Bayesian Submodular Models", "author": ["J. Djolonga", "A. Krause"], "venue": "In Adv. NIPS,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2014}, {"title": "Scalable Variational Inference in Log-supermodular Models", "author": ["J. Djolonga", "A. Krause"], "venue": "In Proc. ICML,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2015}, {"title": "Submodular Functions and Optimization", "author": ["S. Fujishige"], "venue": "Annals of discrete mathematics. Elsevier,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2005}, {"title": "Adaptive Submodularity: Theory and Applications in Active Learning and Stochastic Optimization", "author": ["D. Golovin", "A. Krause"], "venue": "Journal of Artificial Intelligence Research,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2011}, {"title": "On the Partition Function and Random Maximum A-Posteriori Perturbations", "author": ["T. Hazan", "T. Jaakkola"], "venue": "In Proc. ICML,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2012}, {"title": "Polynomial-time approximation algorithms for the Ising model", "author": ["M. Jerrum", "A. Sinclair"], "venue": "SIAM Journal on Computing,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 1993}, {"title": "Robust higher order potentials for enforcing label consistency", "author": ["P. Kohli", "L. Ladicky", "P.H.S. Torr"], "venue": "International Journal of Computer Vision,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2009}, {"title": "Submodular function maximization. In Tractability: Practical Approaches to Hard Problems", "author": ["Andreas Krause", "Daniel Golovin"], "venue": null, "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2014}, {"title": "Conditional random fields: Probabilistic models for segmenting and labeling sequence data", "author": ["J. Lafferty", "A. McCallum", "F. Pereira"], "venue": "In Proc. ICML,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2001}, {"title": "A class of submodular functions for document summarization", "author": ["H. Lin", "J. Bilmes"], "venue": "In Proc. NAACL/HLT,", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2011}, {"title": "A generalized logistic distribution", "author": ["S. Nadarajah", "S. Kotz"], "venue": "International Journal of Mathematics and Mathematical Sciences,", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2005}, {"title": "Robust stochastic approximation approach to stochastic programming", "author": ["A. Nemirovski", "A. Juditsky", "G. Lan", "A. Shapiro"], "venue": "SIAM Journal on Optimization,", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2009}, {"title": "Perturb-and-map random fields: Using discrete optimization to learn and sample from energy models", "author": ["G. Papandreou", "A. Yuille"], "venue": "In Proc. ICCV,", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2011}, {"title": "Learning CRFs using graph cuts", "author": ["M. Szummer", "P. Kohli", "D. Hoiem"], "venue": "In Proc. ECCV,", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2008}, {"title": "Randomized optimum models for structured prediction", "author": ["D. Tarlow", "R.P. Adams", "R.S. Zemel"], "venue": "In Proc. AISTATS,", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2012}, {"title": "Learning probabilistic submodular diversity models via noise contrastive estimation", "author": ["S. Tschiatschek", "J. Djolonga", "A. Krause"], "venue": "In Proc. AISTATS,", "citeRegEx": "21", "shortCiteRegEx": "21", "year": 2016}, {"title": "Large margin methods for structured and interdependent output variables", "author": ["I. Tsochantaridis", "T. Thomas Joachims", "Y. Altun", "Y. Singer"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "22", "shortCiteRegEx": "22", "year": 2005}, {"title": "Graphical models, exponential families, and variational inference", "author": ["M.J. Wainwright", "M.I. Jordan"], "venue": "Foundations and Trends in Machine Learning,", "citeRegEx": "23", "shortCiteRegEx": "23", "year": 2008}, {"title": "Higher-order inference for multi-class log-supermodular models", "author": ["J. Zhang", "J. Djolonga", "A. Krause"], "venue": "In Proc. ICCV,", "citeRegEx": "24", "shortCiteRegEx": "24", "year": 2015}], "referenceMentions": [{"referenceID": 0, "context": "Several common combinatorial optimization tasks, such as clustering, image segmentation, or document summarization, can be achieved by the minimization or the maximization of a submodular function [1, 8, 14].", "startOffset": 197, "endOffset": 207}, {"referenceID": 7, "context": "Several common combinatorial optimization tasks, such as clustering, image segmentation, or document summarization, can be achieved by the minimization or the maximization of a submodular function [1, 8, 14].", "startOffset": 197, "endOffset": 207}, {"referenceID": 13, "context": "Several common combinatorial optimization tasks, such as clustering, image segmentation, or document summarization, can be achieved by the minimization or the maximization of a submodular function [1, 8, 14].", "startOffset": 197, "endOffset": 207}, {"referenceID": 11, "context": "The key benefit of submodularity is the ability to model notions of diminishing returns, and the availability of exact minimization algorithms and approximate maximization algorithms with precise approximation guarantees [12].", "startOffset": 221, "endOffset": 225}, {"referenceID": 17, "context": ", images and their foreground/background segmentations in image segmentation, structured-output prediction methods such as the structured-SVM may be used [18].", "startOffset": 154, "endOffset": 158}, {"referenceID": 4, "context": "Log-supermodular models, with negative log-densities equal to a submodular function, are a first important step toward probabilistic modelling on discrete data with submodular functions [5].", "startOffset": 186, "endOffset": 189}, {"referenceID": 5, "context": "Several bounds have been proposed, that are accompanied with variational approximate inference [6].", "startOffset": 95, "endOffset": 98}, {"referenceID": 8, "context": "\u2013 In Section 3, we review existing variational bounds for the log-partition function and show that the bound of [9], based on \u201cperturb-and-MAP\u201d ideas, formally dominates the bounds proposed by [5, 6].", "startOffset": 112, "endOffset": 115}, {"referenceID": 4, "context": "\u2013 In Section 3, we review existing variational bounds for the log-partition function and show that the bound of [9], based on \u201cperturb-and-MAP\u201d ideas, formally dominates the bounds proposed by [5, 6].", "startOffset": 193, "endOffset": 199}, {"referenceID": 5, "context": "\u2013 In Section 3, we review existing variational bounds for the log-partition function and show that the bound of [9], based on \u201cperturb-and-MAP\u201d ideas, formally dominates the bounds proposed by [5, 6].", "startOffset": 193, "endOffset": 199}, {"referenceID": 4, "context": "1, we show that for parameter learning via maximum likelihood the existing bound of [5, 6] typically leads to a degenerate solution while the one based on \u201cperturb-and-MAP\u201d ideas and logistic samples [9] does not.", "startOffset": 84, "endOffset": 90}, {"referenceID": 5, "context": "1, we show that for parameter learning via maximum likelihood the existing bound of [5, 6] typically leads to a degenerate solution while the one based on \u201cperturb-and-MAP\u201d ideas and logistic samples [9] does not.", "startOffset": 84, "endOffset": 90}, {"referenceID": 8, "context": "1, we show that for parameter learning via maximum likelihood the existing bound of [5, 6] typically leads to a degenerate solution while the one based on \u201cperturb-and-MAP\u201d ideas and logistic samples [9] does not.", "startOffset": 200, "endOffset": 203}, {"referenceID": 0, "context": "Most widely used submodular functions are cuts, concave functions of subset cardinality, mutual information, set covers, and certain functions of eigenvalues of submatrices [1, 7].", "startOffset": 173, "endOffset": 179}, {"referenceID": 6, "context": "Most widely used submodular functions are cuts, concave functions of subset cardinality, mutual information, set covers, and certain functions of eigenvalues of submatrices [1, 7].", "startOffset": 173, "endOffset": 179}, {"referenceID": 0, "context": "In this paper, we are going to use a few properties of such submodular functions (see [1, 7] and references therein).", "startOffset": 86, "endOffset": 92}, {"referenceID": 6, "context": "In this paper, we are going to use a few properties of such submodular functions (see [1, 7] and references therein).", "startOffset": 86, "endOffset": 92}, {"referenceID": 4, "context": "2 Log-supermodular distributions Log-supermodular models are introduced in [5] to model probability distributions on a hypercube, x \u2208 {0, 1}D, and are defined as p(x) = 1 Z(f) exp(\u2212f(x)), where f : {0, 1}D \u2192 R is a submodular function such that f(0) = 0 and the partition function is Z(f) = \u2211 x\u2208{0,1}D exp(\u2212f(x)).", "startOffset": 75, "endOffset": 78}, {"referenceID": 9, "context": "In general, calculation of the partition function Z(f) or the log-partition function A(f) is intractable, as it includes simple binary Markov random fields\u2014 the exact calculation is known to be #P -hard [10].", "startOffset": 203, "endOffset": 207}, {"referenceID": 4, "context": "3 Examples Essentially, all submodular functions used in the minimization context can be used as negative log-densities [5, 6].", "startOffset": 120, "endOffset": 126}, {"referenceID": 5, "context": "3 Examples Essentially, all submodular functions used in the minimization context can be used as negative log-densities [5, 6].", "startOffset": 120, "endOffset": 126}, {"referenceID": 10, "context": "In computer vision, the most common examples are graph-cuts, which are essentially binary Markov random fields with attractive potentials, but higher-order potentials have been considered as well [11].", "startOffset": 196, "endOffset": 200}, {"referenceID": 3, "context": "In our experiments, we use graph-cuts, where submodular function minimization may be performed with max-flow techniques and is thus efficient [4].", "startOffset": 142, "endOffset": 145}, {"referenceID": 1, "context": "Note that there are extensions of submodular functions to continuous domains that could be considered as well [2].", "startOffset": 110, "endOffset": 113}, {"referenceID": 4, "context": "Note that lower bounds based on submodular maximization aspects and superdifferentials [5] can be used to highlight the tightness of various bounds, which we present in Figure 1.", "startOffset": 87, "endOffset": 90}, {"referenceID": 4, "context": "1 Base polytope relaxation with L-Field [5] This method exploits the fact that any submodular function f(x) can be lower bounded by a modular function s(x), i.", "startOffset": 40, "endOffset": 43}, {"referenceID": 0, "context": "Moreover, it has a nice interpretation through the convex duality as the logistic function log(1 + e\u2212sd) may be represented as max\u03bcd\u2208[0,1]\u2212\u03bcdsd \u2212 \u03bcd log\u03bcd \u2212 (1 \u2212 \u03bcd) log(1\u2212 \u03bcd), leading to: AL-field(f) = min s\u2208B(f) max \u03bc\u2208[0,1]D \u2212\u03bc\u22a4s+H(\u03bc) = max \u03bc\u2208[0,1]D H(\u03bc)\u2212 f(\u03bc), where H(\u03bc) = \u2212 \u2211D d=1 { \u03bcd log\u03bcd + (1 \u2212 \u03bcd) log(1 \u2212 \u03bcd) }", "startOffset": 133, "endOffset": 138}, {"referenceID": 0, "context": "Moreover, it has a nice interpretation through the convex duality as the logistic function log(1 + e\u2212sd) may be represented as max\u03bcd\u2208[0,1]\u2212\u03bcdsd \u2212 \u03bcd log\u03bcd \u2212 (1 \u2212 \u03bcd) log(1\u2212 \u03bcd), leading to: AL-field(f) = min s\u2208B(f) max \u03bc\u2208[0,1]D \u2212\u03bc\u22a4s+H(\u03bc) = max \u03bc\u2208[0,1]D H(\u03bc)\u2212 f(\u03bc), where H(\u03bc) = \u2212 \u2211D d=1 { \u03bcd log\u03bcd + (1 \u2212 \u03bcd) log(1 \u2212 \u03bcd) }", "startOffset": 221, "endOffset": 226}, {"referenceID": 0, "context": "Moreover, it has a nice interpretation through the convex duality as the logistic function log(1 + e\u2212sd) may be represented as max\u03bcd\u2208[0,1]\u2212\u03bcdsd \u2212 \u03bcd log\u03bcd \u2212 (1 \u2212 \u03bcd) log(1\u2212 \u03bcd), leading to: AL-field(f) = min s\u2208B(f) max \u03bc\u2208[0,1]D \u2212\u03bc\u22a4s+H(\u03bc) = max \u03bc\u2208[0,1]D H(\u03bc)\u2212 f(\u03bc), where H(\u03bc) = \u2212 \u2211D d=1 { \u03bcd log\u03bcd + (1 \u2212 \u03bcd) log(1 \u2212 \u03bcd) }", "startOffset": 246, "endOffset": 251}, {"referenceID": 5, "context": "Finally, [6] shows the remarkable result that the minimizer s \u2208 B(f) may be obtained by minimizing a simpler function on B(f), namely the squared Euclidean norm, thus leading to algorithms such as the minimum-norm-point algorithm [7].", "startOffset": 9, "endOffset": 12}, {"referenceID": 6, "context": "Finally, [6] shows the remarkable result that the minimizer s \u2208 B(f) may be obtained by minimizing a simpler function on B(f), namely the squared Euclidean norm, thus leading to algorithms such as the minimum-norm-point algorithm [7].", "startOffset": 230, "endOffset": 233}, {"referenceID": 8, "context": ", perform optimization), and then average over several random perturbations [9, 17, 19].", "startOffset": 76, "endOffset": 87}, {"referenceID": 16, "context": ", perform optimization), and then average over several random perturbations [9, 17, 19].", "startOffset": 76, "endOffset": 87}, {"referenceID": 18, "context": ", perform optimization), and then average over several random perturbations [9, 17, 19].", "startOffset": 76, "endOffset": 87}, {"referenceID": 8, "context": "The main problem is that we need 2 such variables, and a key contribution of [9] is to show that if we consider a factored collection {gd(yd)}yd\u2208{0,1},d=1,.", "startOffset": 77, "endOffset": 80}, {"referenceID": 14, "context": "Writing gd(yd) = [gd(1)\u2212 gd(0)]yd + gd(0) and using the fact that (a) gd(0) has zero expectation and (b) the difference between two independent Gumbel distributions has a logistic distribution (with cumulative distribution function z 7\u2192 (1 + e\u2212z)\u22121) [15], we get the following upper-bound: ALogistic(f) = Ez1,.", "startOffset": 250, "endOffset": 254}, {"referenceID": 8, "context": "The first inequality was shown by [9].", "startOffset": 34, "endOffset": 37}, {"referenceID": 4, "context": "In this case, the divide-and-conquer algorithm can be applied for L-Field [5].", "startOffset": 74, "endOffset": 77}, {"referenceID": 4, "context": "We compare the upper-bounds on the logpartition function AL-field and Alogistic, with the setup used by [5].", "startOffset": 104, "endOffset": 107}, {"referenceID": 2, "context": "The centers are sampled from N([3, 3], I) and N([\u22123,\u22123], I), respectively.", "startOffset": 31, "endOffset": 37}, {"referenceID": 2, "context": "The centers are sampled from N([3, 3], I) and N([\u22123,\u22123], I), respectively.", "startOffset": 31, "endOffset": 37}, {"referenceID": 4, "context": "The logistic upper bound is obtained using 100 logistic samples: the logistic upper-bound Alogistic is close to the superdifferential lower bound from [5] and is indeed significantly lower than the bound AL-field.", "startOffset": 151, "endOffset": 154}, {"referenceID": 8, "context": "Indeed, following [9], we consider an exponential family model p(x|t) = exp(\u2212f(x) + s\u22a4x\u2212A(f \u2212 t)), where f \u2212 t", "startOffset": 18, "endOffset": 21}, {"referenceID": 21, "context": "It is known that the expectation of the sufficient statistics under the exponential family model Ep(x|t)x is the gradient of the log-partition function [23].", "startOffset": 152, "endOffset": 156}, {"referenceID": 5, "context": "For the L-field bound, at t = 0, we have \u2202tdAL-field(f \u2212 t) = \u03c3(sd), where s\u2217 is the minimizer of \u2211D d=1 log(1 + e \u2212sd), thus recovering the interpretation of [6] from another point of view.", "startOffset": 159, "endOffset": 162}, {"referenceID": 8, "context": "For the logistic bound, this is the inference mechanism from [9], with \u2202tdAlogistic(f \u2212 t) = Ezy \u2217(z), where y\u2217(z) is the maximizer of maxy\u2208{0,1}D z \u22a4y \u2212 f(y).", "startOffset": 61, "endOffset": 64}, {"referenceID": 8, "context": "= \u3008y\u2217(z, t, \u03b1)\u3009logistic, the expected values of the sufficient statistics match between the data and the optimizers used for the logistic approximation [9].", "startOffset": 152, "endOffset": 155}, {"referenceID": 0, "context": "\u2208 [0, 1], regularizer \u03a9(t, \u03b1).", "startOffset": 2, "endOffset": 8}, {"referenceID": 15, "context": "Since our cost function is convex and Lipschitz-continuous, the averaged iterates are converging to the global optimum [16] at rate 1/ \u221a H (for function values).", "startOffset": 119, "endOffset": 123}, {"referenceID": 12, "context": "Note that our conditional ML estimation can be seen as a form of approximate conditional random fields [13].", "startOffset": 103, "endOffset": 107}, {"referenceID": 17, "context": "While supervised learning can be achieved by other techniques such as structured-output-SVMs [18, 20, 22], our probabilistic approach also applies when we do not observe the original image, which we now consider.", "startOffset": 93, "endOffset": 105}, {"referenceID": 20, "context": "While supervised learning can be achieved by other techniques such as structured-output-SVMs [18, 20, 22], our probabilistic approach also applies when we do not observe the original image, which we now consider.", "startOffset": 93, "endOffset": 105}, {"referenceID": 8, "context": "5 Experiments The aim of our experiments is to demonstrate the ability of our approach to removing noise in binary images, following the experimental set-up of [9].", "startOffset": 160, "endOffset": 163}, {"referenceID": 2, "context": "We consider the training sample of Ntrain = 100 images of size D = 50 \u00d7 50, and the test sample of Ntest = 100 binary images, containing a horse silhouette from the Weizmann horse database [3].", "startOffset": 189, "endOffset": 192}, {"referenceID": 3, "context": "To perform minimization we use graph-cuts [4] as we deal with positive or attractive potentials.", "startOffset": 42, "endOffset": 45}, {"referenceID": 2, "context": "(a) original image (b) noisy image (c) denoised image Figure 2: Denoising of a horse image from the Weizmann horse database [3].", "startOffset": 124, "endOffset": 127}, {"referenceID": 20, "context": "Results are presented in Table 1, where we compare the two types of decoding, as well as a structured output SVM (SVM-Struct [22]) applied to the same problem.", "startOffset": 125, "endOffset": 129}], "year": 2016, "abstractText": "We consider log-supermodular models on binary variables, which are probabilistic models with negative log-densities which are submodular. These models provide probabilistic interpretations of common combinatorial optimization tasks such as image segmentation. In this paper, we focus primarily on parameter estimation in the models from known upper-bounds on the intractable log-partition function. We show that the bound based on separable optimization on the base polytope of the submodular function is always inferior to a bound based on \u201cperturb-and-MAP\u201d ideas. Then, to learn parameters, given that our approximation of the log-partition function is an expectation (over our own randomization), we use a stochastic subgradient technique to maximize a lower-bound on the log-likelihood. This can also be extended to conditional maximum likelihood. We illustrate our new results in a set of experiments in binary image denoising, where we highlight the flexibility of a probabilistic model to learn with missing data.", "creator": "LaTeX with hyperref package"}}}