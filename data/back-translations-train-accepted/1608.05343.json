{"id": "1608.05343", "review": {"conference": "ICML", "VERSION": "v1", "DATE_OF_SUBMISSION": "18-Aug-2016", "title": "Decoupled Neural Interfaces using Synthetic Gradients", "abstract": "Training directed neural networks typically requires forward-propagating data through a computation graph, followed by backpropagating error signal, to produce weight updates. All layers, or more generally, modules, of the network are therefore locked, in the sense that they must wait for the remainder of the network to execute forwards and propagate error backwards before they can be updated. In this work we break this constraint by decoupling modules by introducing a model of the future computation of the network graph. These models predict what the result of the modelled subgraph will produce using only local information. In particular we focus on modelling error gradients: by using the modelled synthetic gradient in place of true backpropagated error gradients we decouple subgraphs, and can update them independently and asynchronously i.e. we realise decoupled neural interfaces. We show results for feed-forward models, where every layer is trained asynchronously, recurrent neural networks (RNNs) where predicting one's future gradient extends the time over which the RNN can effectively model, and also a hierarchical RNN system with ticking at different timescales. Finally, we demonstrate that in addition to predicting gradients, the same framework can be used to predict inputs, resulting in models which are decoupled in both the forward and backwards pass -- amounting to independent networks which co-learn such that they can be composed into a single functioning corporation.", "histories": [["v1", "Thu, 18 Aug 2016 17:29:09 GMT  (7694kb,D)", "http://arxiv.org/abs/1608.05343v1", null], ["v2", "Mon, 3 Jul 2017 10:52:04 GMT  (6993kb,D)", "http://arxiv.org/abs/1608.05343v2", null]], "reviews": [], "SUBJECTS": "cs.LG", "authors": ["max jaderberg", "wojciech marian czarnecki", "simon osindero", "oriol vinyals", "alex graves", "david silver", "koray kavukcuoglu"], "accepted": true, "id": "1608.05343"}, "pdf": {"name": "1608.05343.pdf", "metadata": {"source": "CRF", "title": "Decoupled Neural Interfaces using Synthetic Gradients", "authors": ["Max Jaderberg", "Wojciech Marian Czarnecki", "Simon Osindero", "Oriol Vinyals", "Alex Graves", "Koray Kavukcuoglu"], "emails": [], "sections": [{"heading": null, "text": "The training of directed neural networks typically requires the forward propagation of data by a computational graph, followed by the back propagation of error signals to generate weight updates. Therefore, all layers, or more generally, modules of the network are locked, in the sense that they must wait for the rest of the network to perform forward and propagate errors backwards before they can be updated. In this work, we break down this limitation by decoupling modules by introducing a model of the future computation of the network graph. These models predict what the result of the modelled sub-graph will produce only using local information. Specifically, we focus on the modelling of error courses: by using the modelled synthetic gradient instead of true backward propagated error courses, we decouple subgraphs and can use them independently and asynchronously to update them, i.e. we realize decoupled neural networks, i.e. we can also demonstrate a forward-oriented intersection for which each model is trained to perform an asynchronous, i.e."}, {"heading": "1 Introduction", "text": "These modules are connected by steered edges, creating a forward processing diagram that defines the flow of data from network inputs through each module that produces network outputs. (i) Forward Locking - no module can process its incoming data before the previous nodes in the steered diagrams are executed; (ii) Update Locking - no module can be updated until all dependent modules are executed in forward mode; and also in many credit allocation algorithms (including backpropagation [18]) we have (ii) Backward Locking - no module can be updated until all dependent modules are executed."}, {"heading": "2 Decoupled Neural Interfaces", "text": "We begin by describing the high-level communication protocol used to enable asynchronous learning agents to communicate.As shown in Fig. 1 (a), sender A sends a message hA to recipient B. B has a model MB of use of message hA. B's model of use MB is used to predict the feedback: an error signal \u03b4 A = MB (hA, sB, c) based on the message hA, the current state of B, sB, and potentially any other information c that this module is secret in training like the license plate information. Feedback from A is sent back to A, which allows A to be updated immediately. Over time, B can update the true utility value of A, sB's utility model to adjust the true utility value of B, thus reducing the disparity between BBA and SB."}, {"heading": "2.1 Synthetic Gradient for Feed-Forward Networks", "text": "We apply the above process to the case of a forward-facing neural mesh consisting of N layers fi, i layer executed forward (1,.., N), each of which takes an input xi and produces an output hi = fi (xi). The layers are connected in a chain, so xi = hi \u2212 1 as shown in Figure 1 (b). The complete network can be called FN1 - including the forward graph from layer 1 to layer N. Define the loss imposed on the output of the mesh as L (hN). Each layer fi has parameters that can be trained together to minimize L (hN) with a gradient-based execution."}, {"heading": "2.2 Synthetic Gradient for Recurrent Networks", "text": "An RNN applied to infinite stream prediction can be regarded as an infinitely unrolled, recurring core module f with parameters \u03b8, so that the forward graph F \u221e 1 = (fi) \u221e i = 1, where fi = f \u0432i and the core module produce output yi and state hi based on some inputs xi: yi, hi \u2212 1). Of course, at a certain point in time, one cannot calculate an update of the form of the DNI system. (t + T) This process can take place without infinite future time dependence. Instead, one generally considers a tractable time horizontal. (t) Another sequence of NIV models cannot be calculated. (t + T) This process is very long. (T + 1)"}, {"heading": "3 Experiments", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "3.1 Feed-Forward Networks", "text": "This year it is so far that it will only take a few days until it is ready, until it is ready."}, {"heading": "3.2 Recurrent Neural Networks", "text": "In fact, it is the case that most people are able to survive themselves if they do not go into another world, in which they do not go into another world, in which they do not go into another world, in which they do not live, in which they do not live, but in another world in which they do not live, in which they do not live, in which they do not live, in which they do not live, in which they do not live, in which they do not live, in which they do not live, in which they do not live, in which they do not live, in which they do not live, in which they do not live, in which they do not live, in which they do not live, in which they do not live, in which they do not live, in which they do not live, in which they do not live, in which they do not live, in which they do not live, in which they do not live, in which they do not live, in which they do not live, in which they do not live, in which they do not live, in which they do not live, in which they do not live, in which they do not live, in which they do not live, in which they do not live, in which they do not live, in which they do not live, in which they do not live, in which they do not live, in which they do not live, in which they do not live, in which they do not live, in which they do not live, in which they do not live, in which they do not live, in which they do not live in which they do not live, in which they do not live, in which they do not live, in which they do not live, in which they do not live, in which they do not live, in which they do not live, in which they do not live, in which they do not live, in which they do not live, in which they do not live, in which they do not live, in which they do not live, in which they do not live, in which they do not live, in which they do not live, in which they do not live, in which they do not live, in which they do not live, in which they do not live, in which they do not live, in which they do not live, in which they do not live, in which they do not live, in"}, {"heading": "3.3 Multi-Network System", "text": "In this section, we will examine the use of DNI for communication between arbitrary graphs of networks. As a simple proof-of-concept, we will consider a system of two RNNs, Network A and Network B, in which Network B is executed at a slower speed than Network A, and must use the communication of Network A to accomplish its task. Experimental setup is illustrated and described in Fig. 8 (a). Full experimental details can be found in Section A and Network B, as the communication between Network A and Network B results in Network A being updated end-to-end, with complete back-propagation through all connections, requiring the common Network A-Network B system to be unrolled for T 2 periods before a single weight update is performed on both Network A and Network B. We are training the same system, but using DNI to create a learnable bridge between Network A and Network B, so that Network A and Network B are not updated."}, {"heading": "4 Conclusion", "text": "In this thesis, we introduced a method, DNI with synthetic gradients, which allows decoupled communication between components so that they can be independently updated. We demonstrated this on networks that are decoupled across all layers - which means they cannot be trained sequentially and without locking. We also showed great benefits from the extended time horizon that DNI-enabled RNNNs are able to model, as well as faster convergence. Finally, we demonstrated the application to a multi-network system: a communicating pair of slow-ticking RNNNNs can be decoupled, which greatly speeds up learning. Ours is the first time that neural network modules have been decoupled and the locking of updates has been broken off. This important result opens exciting possibilities for exploration - including improving the foundations laid here and applying to modular, decoupled and asynchronous model architectures."}, {"heading": "A Appendix", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "A.1 Feed-Forward Implementation Details", "text": "In this section, we give the implementation details of the experimental setup in the experiments of Sect. 3.1.Conditional DNI (cDNI) To provide DNI module with the label information in FCN, we simply integrated the single-layer representation of an example into the input of the synthetic gradient model. Consequently, for both MNIST and CIFAR-10 experiments, each cDNI module takes ten additional binary inputs. For Constitutional networks, we add label information in the form of a hot encoded channel mask, so that we simply link ten additional channels to the activations, nine of which are filled with zeros, and one (according to the sample label) is with ones.Common Details will run all experiments for 500k iterations and optimized with Adam [11] with batch size of 256. The learning rate has been reduced by a factor of 10 to 300k 400k and 400k learning plan."}, {"heading": "A.2 Feed-Forward Additional Experiments", "text": "Individual DNI We consider the formation of an FCN for the MNIST digit classification with a network of 6 layers (5 hidden layers, one classification layer), but the division of the network into two unlocked subnets by inserting a single DNI at a variable position, as illustrated in Fig. 1 (c). 9 (a) shows the results of a different depth at which the DNI is inserted. If we train this 6 layer FCN with vanilla backpropagation, we achieve a test error of 1.6%. The inclusion of a single DNI between two layers results in errors, depending on whether the DNI is after the first layer or the penultimate layer. If we decouple the layers without DNI by simply reversing a gradient between them, this results in poor performance - between 2.9% and 23.7% error, depending on whether the DNI runs after layer 1 and layer 5."}, {"heading": "A.2.1 Underfitting of Synthetic Gradient Models", "text": "Taking a closer look at the learning curves of the DNI model (see Figure 13 for training error diagrams on CIFAR-10 with the CNN model), it is easy to see that the big test error (and its degradation with depth) is actually an effect of underfit, not a lack of generalization or lack of convergence of the learning process. One possible explanation is the fact that due to the lack of label signal in the DNI module, the network is over-regulated, as DNI attempts to model an expected gradient via the label distribution in each iteration. This is obviously a more difficult problem than modelling the actual gradient, and due to the underfit to this partial problem, the entire network also matches the problem at hand. Once label information is introduced into the cDNI model, the network matches the training data much better, but the use of synthetic gradients still acts like a regulator, which also leads to a reduced test error, which may also indicate that the labeling method may be modified by the conditioner."}, {"heading": "A.3 RNN Implementation Details", "text": "All RNN experiments are performed with a recurrent core of the LSTM, where the output for a last linear layer is used to model the task. In the case of DNI and DNI + Aux, the output of the LSTM is also used as input for a synthetic layer defect model with the same number of units as the LSTM, with a definitive linear projection to double the number of units of the LSTM (to generate the synthetic gradient of the output and the cell state).The synthetic gradient is scaled by a factor of 0.1 when consumed by the model (we found that this reliably results in a stable training)."}, {"heading": "A.4 Multi-Network Implementation Details", "text": "The two RNNs in this experiment, Network A and Network B, are both 256-unit LSTMs that use a batch normalization as described in [5]. Network A takes a 28 x 28 MNIST digit as input and has a two-layer FCN (each layer has 256 units and consists of linear batch normalization and ReLU), the output of which is passed to its LSTM as input. Network B takes the message from Network A as input to its LSTM and uses the output of its LSTM for a linear classifier to classify the number of odd numbers, as well as the input to another linear layer with batch normalization that sends the message to Network B. Network B takes the message from Network A as input to its LSTM and uses the output of its LSTM for a linear classifier to classify the number of odd numbers that should run through the 25th layer of a linear number of numbers that Adam's Network A's data model has a 25th layer followed by the only 25th layer of a 25th."}], "references": [{"title": "Kickback cuts backprop\u2019s red-tape: Biologically plausible credit assignment in neural networks", "author": ["D. Balduzzi", "H. Vanchinathan", "J. Buhmann"], "venue": "arXiv preprint arXiv:1411.6191,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2014}, {"title": "Direct gradient-based reinforcement learning", "author": ["J. Baxter", "P.L. Bartlett"], "venue": "In Circuits and Systems,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2000}, {"title": "How auto-encoders could provide credit assignment in deep networks via target propagation", "author": ["Y. Bengio"], "venue": "arXiv preprint arXiv:1407.7906,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2014}, {"title": "Distributed optimization of deeply nested systems", "author": ["M A Carreira-Perpin\u00e1n", "W Wang"], "venue": "In AISTATS,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2014}, {"title": "Recurrent batch normalization", "author": ["T. Cooijmans", "N. Ballas", "C. Laurent", "A. Courville"], "venue": "arXiv preprint arXiv:1603.09025,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2016}, {"title": "Deep sparse rectifier neural networks", "author": ["X. Glorot", "A. Bordes", "Y. Bengio"], "venue": "In International Conference on Artificial Intelligence and Statistics,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2011}, {"title": "Generating sequences with recurrent neural networks", "author": ["A. Graves"], "venue": "arXiv preprint arXiv:1308.0850,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2013}, {"title": "Neural turing machines", "author": ["A. Graves", "G. Wayne", "I. Danihelka"], "venue": "arXiv preprint arXiv:1410.5401,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2014}, {"title": "Long short-term memory", "author": ["Sepp Hochreiter", "J\u00fcrgen Schmidhuber"], "venue": "Neural computation,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 1997}, {"title": "Batch normalization: Accelerating deep network training by reducing internal covariate shift", "author": ["S. Ioffe", "C. Szegedy"], "venue": null, "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2015}, {"title": "Adam: A method for stochastic optimization", "author": ["Diederik Kingma", "Jimmy Ba"], "venue": "arXiv preprint arXiv:1412.6980,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2014}, {"title": "Learning multiple layers of features from tiny", "author": ["A. Krizhevsky", "G. Hinton"], "venue": null, "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2009}, {"title": "Gradient-based learning applied to document recognition", "author": ["Y. LeCun", "L. Bottou", "Y. Bengio", "P. Haffner"], "venue": "Proceedings of the IEEE,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 1998}, {"title": "The mnist database of handwritten digits", "author": ["Y. LeCun", "C. Cortes", "C. Burges"], "venue": null, "citeRegEx": "14", "shortCiteRegEx": "14", "year": 1998}, {"title": "Difference target propagation", "author": ["D. Lee", "S. Zhang", "A. Fischer", "Y. Bengio"], "venue": "In Machine Learning and Knowledge Discovery in Databases,", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2015}, {"title": "Building a large annotated corpus of english: The penn treebank", "author": ["M.P. Marcus", "M.A. Marcinkiewicz", "B. Santorini"], "venue": "Computational linguistics,", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 1993}, {"title": "Training recurrent networks online without backtracking", "author": ["Yann Ollivier", "Guillaume Charpiat"], "venue": "arXiv preprint arXiv:1507.07680,", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2015}, {"title": "Learning representations by back-propagating", "author": ["D.E. Rumelhart", "G.E. Hinton", "R.J. Williams"], "venue": "errors. Nature,", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 1986}, {"title": "Training neural networks without gradients: A scalable admm approach", "author": ["G Taylor", "R Burmeister", "Z Xu", "B Singh", "A Patel", "T Goldstein"], "venue": null, "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2016}, {"title": "Policy gradient coagent networks", "author": ["P.S. Thomas"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "20", "shortCiteRegEx": "20", "year": 2011}, {"title": "Simple statistical gradient-following algorithms for connectionist reinforcement learning", "author": ["R.J. Williams"], "venue": "Machine learning,", "citeRegEx": "21", "shortCiteRegEx": "21", "year": 1992}, {"title": "A learning algorithm for continually running fully recurrent neural networks", "author": ["R.J. Williams", "D. Zipser"], "venue": "Neural computation,", "citeRegEx": "22", "shortCiteRegEx": "22", "year": 1989}], "referenceMentions": [{"referenceID": 17, "context": "This process results in several forms of locking, namely: (i) Forward Locking \u2013 no module can process its incoming data before the previous nodes in the directed forward graph have executed; (ii) Update Locking \u2013 no module can be updated before all dependent modules have executed in forwards mode; also, in many credit-assignment algorithms (including backpropagation [18]) we have (iii) Backwards Locking \u2013 no module can be updated before all dependent modules have executed in both forwards mode and backwards mode.", "startOffset": 369, "endOffset": 373}, {"referenceID": 1, "context": "Our synthetic gradient model is most analogous to a value function which is used for gradient ascent [2] or a value function used for bootstrapping.", "startOffset": 101, "endOffset": 104}, {"referenceID": 2, "context": "target propagation [3, 15] removes the reliance on passing gradients between layers, by instead generating target activations which should be fitted to.", "startOffset": 19, "endOffset": 26}, {"referenceID": 14, "context": "target propagation [3, 15] removes the reliance on passing gradients between layers, by instead generating target activations which should be fitted to.", "startOffset": 19, "endOffset": 26}, {"referenceID": 20, "context": "REINFORCE [21] (considering all activations are actions), Kickback [1], and Policy Gradient Coagent Networks [20] \u2013 but still remain update locked since they require rewards to be generated by an output (or a global critic).", "startOffset": 10, "endOffset": 14}, {"referenceID": 0, "context": "REINFORCE [21] (considering all activations are actions), Kickback [1], and Policy Gradient Coagent Networks [20] \u2013 but still remain update locked since they require rewards to be generated by an output (or a global critic).", "startOffset": 67, "endOffset": 70}, {"referenceID": 19, "context": "REINFORCE [21] (considering all activations are actions), Kickback [1], and Policy Gradient Coagent Networks [20] \u2013 but still remain update locked since they require rewards to be generated by an output (or a global critic).", "startOffset": 109, "endOffset": 113}, {"referenceID": 21, "context": "While Real-Time Recurrent Learning [22] or approximations such as [17] may seem a promising way to remove update locking, these methods require maintaining the full (or approximate) gradient of the current state with respect to the parameters.", "startOffset": 35, "endOffset": 39}, {"referenceID": 16, "context": "While Real-Time Recurrent Learning [22] or approximations such as [17] may seem a promising way to remove update locking, these methods require maintaining the full (or approximate) gradient of the current state with respect to the parameters.", "startOffset": 66, "endOffset": 70}, {"referenceID": 3, "context": "Other works such as [4, 19] allow training of layers in parallel without backpropagation, but in practice are not scalable to more complex and generic network architectures.", "startOffset": 20, "endOffset": 27}, {"referenceID": 18, "context": "Other works such as [4, 19] allow training of layers in parallel without backpropagation, but in practice are not scalable to more complex and generic network architectures.", "startOffset": 20, "endOffset": 27}, {"referenceID": 17, "context": "Each layer fi has parameters \u03b8i that can be trained jointly to minimise L(hN ) with a gradient-based update rule \u03b8i \u2190 \u03b8i \u2212 \u03b1 \u03b4i \u2202hi \u2202\u03b8i ; \u03b4i = \u2202L \u2202hi (1) where \u03b1 is the learning rate and \u2202L \u2202hi is computed with backpropagation [18].", "startOffset": 227, "endOffset": 231}, {"referenceID": 21, "context": "Note that the target gradient of the hidden state that is regressed to by the synthetic gradient model is slightly stale, a similar consequence of online training as seen in RTRL [22].", "startOffset": 179, "endOffset": 183}, {"referenceID": 13, "context": "Every layer DNI We first look at training an FCN for MNIST digit classification [14].", "startOffset": 80, "endOffset": 84}, {"referenceID": 9, "context": "For an FCN, \u201clayer\u201d refers to a linear transformation followed by batch-normalisation [10] and a rectified linear non-linearity (ReLU) [6].", "startOffset": 86, "endOffset": 90}, {"referenceID": 5, "context": "For an FCN, \u201clayer\u201d refers to a linear transformation followed by batch-normalisation [10] and a rectified linear non-linearity (ReLU) [6].", "startOffset": 135, "endOffset": 138}, {"referenceID": 11, "context": "We perform experiments where we vary the depth of the model (between 3 and 6 layers), on MNIST digit classification and CIFAR-10 object recognition [12].", "startOffset": 148, "endOffset": 152}, {"referenceID": 12, "context": "This framework can be easily applied to CNNs [13].", "startOffset": 45, "endOffset": 49}, {"referenceID": 8, "context": "For all experiments we use an LSTM [9] of the form in [7], whose output is used for the task at hand, and additionally as input to the synthetic gradient model (which is shared over all timesteps).", "startOffset": 35, "endOffset": 38}, {"referenceID": 6, "context": "For all experiments we use an LSTM [9] of the form in [7], whose output is used for the task at hand, and additionally as input to the synthetic gradient model (which is shared over all timesteps).", "startOffset": 54, "endOffset": 57}, {"referenceID": 7, "context": "Copy and Repeat Copy We first look at two synthetic tasks \u2013 Copy and Repeat Copy tasks from [8].", "startOffset": 92, "endOffset": 95}], "year": 2016, "abstractText": "Training directed neural networks typically requires forward-propagating data through a computation graph, followed by backpropagating error signal, to produce weight updates. All layers, or more generally, modules, of the network are therefore locked, in the sense that they must wait for the remainder of the network to execute forwards and propagate error backwards before they can be updated. In this work we break this constraint by decoupling modules by introducing a model of the future computation of the network graph. These models predict what the result of the modelled subgraph will produce using only local information. In particular we focus on modelling error gradients: by using the modelled synthetic gradient in place of true backpropagated error gradients we decouple subgraphs, and can update them independently and asynchronously i.e. we realise decoupled neural interfaces. We show results for feed-forward models, where every layer is trained asynchronously, recurrent neural networks (RNNs) where predicting one\u2019s future gradient extends the time over which the RNN can effectively model, and also a hierarchical RNN system with ticking at different timescales. Finally, we demonstrate that in addition to predicting gradients, the same framework can be used to predict inputs, resulting in models which are decoupled in both the forward and backwards pass \u2013 amounting to independent networks which co-learn such that they can be composed into a single functioning corporation.", "creator": "LaTeX with hyperref package"}}}