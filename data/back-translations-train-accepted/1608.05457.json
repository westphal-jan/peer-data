{"id": "1608.05457", "review": {"conference": "EMNLP", "VERSION": "v1", "DATE_OF_SUBMISSION": "19-Aug-2016", "title": "Who did What: A Large-Scale Person-Centered Cloze Dataset", "abstract": "We have constructed a new \"Who-did-What\" dataset of over 200,000 fill-in-the-gap (cloze) multiple choice reading comprehension problems constructed from the LDC English Gigaword newswire corpus. The WDW dataset has a variety of novel features. First, in contrast with the CNN and Daily Mail datasets (Hermann et al., 2015) we avoid using article summaries for question formation. Instead, each problem is formed from two independent articles --- an article given as the passage to be read and a separate article on the same events used to form the question. Second, we avoid anonymization --- each choice is a person named entity. Third, the problems have been filtered to remove a fraction that are easily solved by simple baselines, while remaining 84% solvable by humans. We report performance benchmarks of standard systems and propose the WDW dataset as a challenge task for the community.", "histories": [["v1", "Fri, 19 Aug 2016 00:13:10 GMT  (27kb)", "http://arxiv.org/abs/1608.05457v1", "To appear at EMNLP 2016. Our dataset is available at tticnlp.github.io/who_did_what"]], "COMMENTS": "To appear at EMNLP 2016. Our dataset is available at tticnlp.github.io/who_did_what", "reviews": [], "SUBJECTS": "cs.CL", "authors": ["takeshi onishi", "hai wang", "mohit bansal", "kevin gimpel", "david a mcallester"], "accepted": true, "id": "1608.05457"}, "pdf": {"name": "1608.05457.pdf", "metadata": {"source": "CRF", "title": "Who did What: A Large-Scale Person-Centered Cloze Dataset", "authors": ["Takeshi Onishi", "Hai Wang", "Mohit Bansal Kevin Gimpel", "David McAllester"], "emails": ["tonishi@ttic.edu", "haiwang@ttic.edu", "mbansal@ttic.edu", "kgimpel@ttic.edu", "mcallester@ttic.edu"], "sections": [{"heading": null, "text": "ar Xiv: 160 8.05 457v 1 [cs.C L] 19 A"}, {"heading": "1 Introduction", "text": "The researchers differentiate between the problem of the general knowledge question and the problem of reading comprehension (Hermann et al., 2015; Hill et al., 2016). Reading comprehension is more difficult than knowledge-based or IR-based questions, which are answered in two ways. First, reading comprehension systems must derive answers from a given unstructured passage rather than from structured knowledge sources such as Freebase (Bollacker et al., 2008) or the Google Knowledge Graph (Singhal, 2012).1 Available at tticnlp.io / who _ did _ whatSecond, machine comprehension systems cannot exploit the large level of redundancy available on the web to find statements that strongly syntactically correspond to the question (Yang et al., 2015). In contrast, a machine comprehension system must use the individual formulations in the given passage that present a poor syntactic agreement with the question."}, {"heading": "2 Related Work", "text": "The MCTest dataset (Richardson et al., 2013) consists of 660 fictional stories, each with 4 multiple choice questions. This dataset is too small to train systems for the general problem of reading comprehension. The synthetic question bAbI to answer datasets (Weston et al., 2016) contains passages describing a series of actions in a simulation, followed by a question. For these synthetic data, a logical algorithm can be written to precisely solve the problems (and in fact, it is used to generate basic truth answers). The Children's Book Test (CBT) dataset, created by Hill et al. (2016), consists of 113,719 entity problems called cloze-style. Each problem consists of 20 consecutive sentences of a children's story, a 21st sentence in which a word has been deleted, and a list of ten decisions for the deleted word."}, {"heading": "3 Dataset Construction", "text": "We describe the construction of our who-didWhat dataset in more detail. We outline the procedure below and provide more specific details in the appendix. To create a problem, we first create the question by selecting a random article - the \"Question Article\" - from the Gigaword Corpus and removing the first sentence of that article - the \"Question Sentence\" - as the source of the Cloze question. The hope is that the first sentence of an article contains prominent people and events that are likely to be discussed in other independent articles. In order to turn the question into a Cloze question, we extract named entities using the Stanford system (Finkel et al., 2005) and analyze the sentence using the Stanford PCFG parser (Klein and Manning, 2003). The named entities are candidates for deletion to create a Cloze problem. For each named entity, we then identify a noun phrase guided by that person in the automatic party."}, {"heading": "4 Performance Benchmarks", "text": "We report on the performance of several systems to characterize our dataset: \u2022 Word overlap: Select the option c inserted to the question q that is most similar to all sentences in the passage, i.e., CosSim (bag (c + q), bag (s)). \u2022 Sliding window and distance baselines (and their combination) by Richardson et al. (2013). \u2022 Semantic features: NLP feature-based system by Wang et al. (2015). \u2022 Attention Sum (AS) reader: GRU with a pointing mechanism (Hermann et al., 2015). \u2022 Stanford reader: An attentive reader modified with a two-dimensional term (Chen et al., 2016). \u2022 Attention Sum (AS) reader: GRU with a pointing mechanism (Kadlec et al., 2016). \u2022 Stanford reader: An attentive reader modified with a two-dimensional term (Chen et al., 2016). \u2022 Attention."}, {"heading": "5 Conclusion", "text": "We presented a large, person-centred Cloze dataset, which is scalable and flexible for neural methods, and which differs in many ways from existing large Cloze datasets, providing a significant enhancement of training and test data for machine understanding."}, {"heading": "6 Appendix", "text": "We include pseudo code for generating questions (Alg. 1) and multiple choice answers (Alg. 2). Data: an Article A result: either zero if no question of A, or a cloze question q and the deleted person named q q q contains the first sentence of Article A. If not 10 \u2264 | s | \u2264 120 then zero E. The amount of person NEs e in s that e does not contain more than three words. Named entities that share a word with a previously named entity are deleted. If | E | < 2 then there is zero T, constituent parse tree of s for e E, starting with the end of s dob The node in T for person NE. While b.category, NNP, NNNPS} and b.head = e and no element of the b.clob entity has the category SBAR dob."}], "references": [{"title": "Freebase: a collaboratively created graph database for structuring human knowledge", "author": ["Kurt Bollacker", "Colin Evans", "Praveen Paritosh", "Tim Sturge", "Jamie Taylor."], "venue": "Proceedings of the 2008 ACM SIGMOD international conference on Management of", "citeRegEx": "Bollacker et al\\.,? 2008", "shortCiteRegEx": "Bollacker et al\\.", "year": 2008}, {"title": "A thorough examination of the CNN/Daily Mail reading comprehension task", "author": ["Danqi Chen", "Jason Bolton", "Christopher D. Manning."], "venue": "Proceedings of the", "citeRegEx": "Chen et al\\.,? 2016", "shortCiteRegEx": "Chen et al\\.", "year": 2016}, {"title": "2016), and result marked IV is from (Dhingra", "author": ["Kadlec"], "venue": null, "citeRegEx": "Kadlec,? \\Q2016\\E", "shortCiteRegEx": "Kadlec", "year": 2016}, {"title": "Gated-attention readers for text comprehension", "author": ["Bhuwan Dhingra", "Hanxiao Liu", "William W. Cohen", "Ruslan Salakhutdinov."], "venue": "CoRR, abs/1606.01549.", "citeRegEx": "Dhingra et al\\.,? 2016", "shortCiteRegEx": "Dhingra et al\\.", "year": 2016}, {"title": "Incorporating non-local information into information extraction systems by Gibbs sampling", "author": ["Jenny Rose Finkel", "Trond Grenager", "Christopher Manning."], "venue": "Proceedings of the 43rd Annual Meeting on Association for Computational Linguistics, pages", "citeRegEx": "Finkel et al\\.,? 2005", "shortCiteRegEx": "Finkel et al\\.", "year": 2005}, {"title": "Teaching machines to read and comprehend", "author": ["Karl Moritz Hermann", "Tom\u00e1s Kocisk\u00fd", "Edward Grefenstette", "Lasse Espeholt", "Will Kay", "Mustafa Suleyman", "Phil Blunsom."], "venue": "Advances in Neural Information Processing Systems, pages 1684\u20131692.", "citeRegEx": "Hermann et al\\.,? 2015", "shortCiteRegEx": "Hermann et al\\.", "year": 2015}, {"title": "The Goldilocks principle: Reading children\u2019s books with explicit memory representations", "author": ["Felix Hill", "Antoine Bordes", "Sumit Chopra", "Jason Weston."], "venue": "Proceedings of International Conference on Learning Representations.", "citeRegEx": "Hill et al\\.,? 2016", "shortCiteRegEx": "Hill et al\\.", "year": 2016}, {"title": "Text understanding with the attention sum reader network", "author": ["Rudolf Kadlec", "Martin Schmid", "Ond\u0159ej Bajgar", "Jan Kleindienst."], "venue": "Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 908\u2013918.", "citeRegEx": "Kadlec et al\\.,? 2016", "shortCiteRegEx": "Kadlec et al\\.", "year": 2016}, {"title": "Introducing the knowledge graph", "author": ["Amit Singhal"], "venue": null, "citeRegEx": "Singhal.,? \\Q2012\\E", "shortCiteRegEx": "Singhal.", "year": 2012}, {"title": "Towards AI-complete ques", "author": ["Tomas Mikolov"], "venue": null, "citeRegEx": "Mikolov.,? \\Q2016\\E", "shortCiteRegEx": "Mikolov.", "year": 2016}], "referenceMentions": [{"referenceID": 5, "context": "First, in contrast with the CNN and Daily Mail datasets (Hermann et al., 2015) we avoid using article summaries for question formation.", "startOffset": 56, "endOffset": 78}, {"referenceID": 5, "context": "Researchers distinguish the problem of general knowledge question answering from that of reading comprehension (Hermann et al., 2015; Hill et al., 2016).", "startOffset": 111, "endOffset": 152}, {"referenceID": 6, "context": "Researchers distinguish the problem of general knowledge question answering from that of reading comprehension (Hermann et al., 2015; Hill et al., 2016).", "startOffset": 111, "endOffset": 152}, {"referenceID": 0, "context": "First, reading comprehension systems must infer answers from a given unstructured passage rather than structured knowledge sources such as Freebase (Bollacker et al., 2008) or the Google Knowledge Graph (Singhal, 2012).", "startOffset": 148, "endOffset": 172}, {"referenceID": 8, "context": ", 2008) or the Google Knowledge Graph (Singhal, 2012).", "startOffset": 38, "endOffset": 53}, {"referenceID": 5, "context": "Our dataset differs from the CNN and Daily Mail comprehension tasks (Hermann et al., 2015) in that it forms questions from two distinct articles rather than summary points.", "startOffset": 68, "endOffset": 90}, {"referenceID": 1, "context": "Humans can answer questions in our dataset with an 84% success rate compared to the estimates of 75% for CNN (Chen et al., 2016) and 82% for the CBT named entities task (Hill et al.", "startOffset": 109, "endOffset": 128}, {"referenceID": 6, "context": ", 2016) and 82% for the CBT named entities task (Hill et al., 2016).", "startOffset": 48, "endOffset": 67}, {"referenceID": 5, "context": "For example, the Attentive Reader (Hermann et al., 2015) achieves 63% on CNN but only 55% on Who-didWhat and the Attention Sum Reader (Kadlec et al.", "startOffset": 34, "endOffset": 56}, {"referenceID": 7, "context": ", 2015) achieves 63% on CNN but only 55% on Who-didWhat and the Attention Sum Reader (Kadlec et al., 2016) achieves 70% on CNN but only 59% on Whodid-What.", "startOffset": 85, "endOffset": 106}, {"referenceID": 6, "context": "The Children\u2019s Book Test (CBT) dataset, created by Hill et al. (2016), consists of 113,719 cloze-style named entity problems.", "startOffset": 51, "endOffset": 70}, {"referenceID": 5, "context": "Above we referenced the comprehension datasets created from CNN and Daily Mail articles by Hermann et al. (2015). The CNN and Daily Mail datasets together consist of 1.", "startOffset": 91, "endOffset": 113}, {"referenceID": 4, "context": "To convert the question sentence to a cloze question, we first extract named entities using the Stanford NER system (Finkel et al., 2005) and parse the sentence using the Stanford PCFG parser (Klein and Manning, 2003).", "startOffset": 116, "endOffset": 137}, {"referenceID": 5, "context": "\u2022 Attentive Reader: LSTM with attention mechanism (Hermann et al., 2015).", "startOffset": 50, "endOffset": 72}, {"referenceID": 1, "context": "\u2022 Stanford Reader: An attentive reader modified with a bilinear term (Chen et al., 2016).", "startOffset": 69, "endOffset": 88}, {"referenceID": 7, "context": "\u2022 Attention Sum (AS) Reader: GRU with a pointattention mechanism (Kadlec et al., 2016).", "startOffset": 65, "endOffset": 86}, {"referenceID": 3, "context": "\u2022 Gated-Attention (GA) Reader: Attention Sum Reader with gated layers (Dhingra et al., 2016).", "startOffset": 70, "endOffset": 92}], "year": 2016, "abstractText": "We have constructed a new \u201cWho-did-What\u201d dataset of over 200,000 fill-in-the-gap (cloze) multiple choice reading comprehension problems constructed from the LDC English Gigaword newswire corpus. The WDW dataset has a variety of novel features. First, in contrast with the CNN and Daily Mail datasets (Hermann et al., 2015) we avoid using article summaries for question formation. Instead, each problem is formed from two independent articles \u2014 an article given as the passage to be read and a separate article on the same events used to form the question. Second, we avoid anonymization \u2014 each choice is a person named entity. Third, the problems have been filtered to remove a fraction that are easily solved by simple baselines, while remaining 84% solvable by humans. We report performance benchmarks of standard systems and propose the WDW dataset as a challenge task for the community.1", "creator": "dvips(k) 5.991 Copyright 2011 Radical Eye Software"}}}