{"id": "1608.05604", "review": {"conference": "EMNLP", "VERSION": "v1", "DATE_OF_SUBMISSION": "19-Aug-2016", "title": "Modeling Human Reading with Neural Attention", "abstract": "When humans read text, they fixate some words and skip others. However, there have been few attempts to explain skipping behavior with computational models, as most existing work has focused on predicting reading times (e.g.,~using surprisal). In this paper, we propose a novel approach that models both skipping and reading, using an unsupervised architecture that combines a neural attention with autoencoding, trained on raw text using reinforcement learning. Our model explains human reading behavior as a tradeoff between precision of language understanding (encoding the input accurately) and economy of attention (fixating as few words as possible). We evaluate the model on the Dundee eye-tracking corpus, showing that it accurately predicts skipping behavior and reading times, is competitive with surprisal, and captures known qualitative features of human reading.", "histories": [["v1", "Fri, 19 Aug 2016 14:03:46 GMT  (40kb)", "http://arxiv.org/abs/1608.05604v1", "To appear in Proceedings of EMNLP 2016"], ["v2", "Mon, 24 Apr 2017 09:38:46 GMT  (40kb)", "http://arxiv.org/abs/1608.05604v2", "EMNLP 2016, pp. 85-95, Austin, TX"]], "COMMENTS": "To appear in Proceedings of EMNLP 2016", "reviews": [], "SUBJECTS": "cs.CL", "authors": ["michael hahn 0001", "frank keller"], "accepted": true, "id": "1608.05604"}, "pdf": {"name": "1608.05604.pdf", "metadata": {"source": "CRF", "title": "Modeling Human Reading with Neural Attention", "authors": ["Michael Hahn", "Frank Keller"], "emails": ["s1582047@inf.ed.ac.uk", "keller@inf.ed.ac.uk"], "sections": [{"heading": null, "text": "ar Xiv: 160 8.05 604v 1 [cs.C L] 19 A"}, {"heading": "1 Introduction", "text": "In fact, it is as if most people who are able to help themselves, to help themselves, put themselves in a difficult situation in which they can no longer live. (...) It is not as if they are able to help themselves. (...) It is not as if they are able to help themselves. (...) It is as if they are doing it, as if they were doing it. (...) It is as if they are doing it, as if they are doing it, as if they are doing it. (...) It is as if they are doing it, as if they are doing it. (...) It is as if they are doing it, as if they are doing it, as if they are doing it. (...) It is as if they are doing it, as if they are doing it. (...) It is as if they are doing it, if they are doing it, if they are doing it. (...) It is as if they are doing it, if they are doing it, if they are doing it. (...) It is as if they are doing it, if they are doing it, if they are doing it, if they are doing it, if they are doing it, if they are doing it."}, {"heading": "2 Related Work", "text": "A number of attention-based neural network architectures have recently been proposed in the literature and show promising results in both NLP and computer vision (e.g. Mnih et al., 2014; Bahdanau et al., 2015). Such architectures include a mechanism that allows the network to dynamically focus on a limited part of the input. Attention is also a central concept in cognitive science, where it designates the focus of cognitive processing. In both speech processing and visual processing, attention is known to be limited to a limited area of the visual field and quickly shifted by eye movements (Henderson, 2003). Attention-based neural architectures deal with either soft attention or hard attention. Soft attention distributes real attention values across the input, enabling continuous training with gradients. Hard attention mechanisms enable discreet decisions about which parts of the input should be focused on, and can be trained with snih."}, {"heading": "3 The NEAT Reading Model", "text": "Starting point of our model is the Tradeoff Hypothesis (see Section 1): Reading optimizes a trade-off between precision of speech understanding and economics of attention. We make this idea clear by proposing NEAT (NEural Attention Tradeoff), a model that reads text and then tries to reconstruct it. During reading, the network selects which words to process and which to skip. The Tradeoff Hypothesis is formalized by a training goal that combines the accuracy of the reconstruction with the economy of attention and encourages the network to look at words only to the extent necessary to reconstruct the sentence."}, {"heading": "3.1 Architecture", "text": "We use a neural sequence-to-sequence architecture (Sutskever et al., 2014) with a hard attention mechanism. We illustrate the model in Figure 1, which is based on a three-word sequence. \u2212 The most basic components are therefore the reader, the R, and the decoder. Both of them are recurrent neural networks with long short-term memory (LSTM, Hochreiter and Schmidhuber, 1997), which transform the word sequence into time steps R0,., R3 in the figure. It goes over the input sequence that reads a word, and converts the word sequence into a sequence of vectors h0, h3. Each vector hi acts as a fixed-dimensional encoding of the word sequence w1,., wi that has been read so far."}, {"heading": "3.2 Model Objective", "text": "In view of the network parameters \u03b8 and a sequence w of words, the network stochastically chooses a sequence \u03c9 according to (1) and results in a loss L (\u03c9 | w, \u03b8) for speech modelling and reconstruction: L (\u03c9 | w, \u03b8) = \u2212 \u2211 i log PR (wi | w1,..., i \u2212 1, \u03c91,..., i \u2212 1; \u03b8) \u2212 \u2211 i logPDecoder (wi | w1,..., i \u2212 1; hN) (3), where PR (wi,...) denotes the output of the reader after reading wi \u2212 1, and PDecoder (wi |...; hN) the output of the decoder at the time i \u2212 1, where hN is the vector representation created by the reader network for the entire input sequence. In order to implement the tradeoff hypothesis, we train NEAT to solve speech modelling and reconstruction with minimal attention, i.e. the network minimizes the expected loss of the word network: Q \u00b7 the entire input sequence, wi-Q for the input sequence."}, {"heading": "3.3 Training", "text": "In order to train the reader and the decoder, we temporarily remove the attention network A by specifying one \u03c9 \u00b2 binom (n, p) (n sequence length, p a hyperparameter) sequence for each input sequence. In reality, NEAT is trained to perform reconstruction and speech modelling when sounds are present in the input. After R and the decoder have been trained, we fix their parameters and train A using the REINFORCE rule (Williams, 1992), which performs a stochastic gradient decrease using the 1 | B \u00b2 w \u00b2 s estimate."}, {"heading": "4 Methods", "text": "Our goal is to assess how well NEAT predicts human fixation behavior and reading times. Furthermore, we want to show that known qualitative properties emerge from the trade-off hypothesis, even though NEAT has no prior knowledge of useful functions."}, {"heading": "4.1 Training Setup", "text": "For the loss estimator U, we use a bidirectional LSTM with 20 memory cells. The input data is divided into sequences of 50 tokens, which are used as NEAT input sequences, without taking into account sentence boundaries. Word embedding has 100 dimensions, is shared between the reader and the attention network, and is only trained during reader training. Vocabulary consists of the 10,000 most common words from the training corpus. NEAT was trained on the training set of the Daily Mail section of the corpus described by Hermann et al. (2015), which consists of 195,462 Daily Mail articles containing approximately 200 million tokens. Recurring networks and the attention network were each trained for an epoch. Weights are pulled from the uniform distribution for initialization."}, {"heading": "4.2 Corpus", "text": "For the evaluation, we used the English section of the Dundee corpus (Kennedy and Pynte, 2005), which consisted of 20 texts from The Independent, annotated with eye movement data from ten English natives. Each native speaker read all 20 texts and answered a comprehension question after each text. We divided the Dundee corpus into a development and a test set, with texts 1-3 forming the development set. The development set consisted of 78,300 tokens and the test set of 281,911 tokens. For the evaluation, we removed the data points removed from Demberg and Keller (2008), which consisted mainly of words at the beginning or end of lines, outliers and cases of trace loss. Furthermore, we removed datapoints where the word was outside the vocabulary of the model, and these datapoints mapped the positions 1-3 or 48-50 of a sequence when splitting the data."}, {"heading": "5 Results and Discussion", "text": "In this section, we will consider the following baseline for the attention network: Random attention is defined by \u03c9 \u0445 binome (n, p), with p = 0.62, the human fixation rate in the development sentence. For full attention, we take \u03c9 = 1, i.e., all words are fixed. In addition, we derive fixation predictions from full surprise, word frequency, and word length by selecting a threshold so that the resulting fixation rate matches the human fixation rate in the development sentence."}, {"heading": "5.1 Quantitative Properties", "text": "This probability is not efficiently calculated, so we approach it by sampling a sequence \u03c9 and predicting the probabilities more randomly. Figure 2 shows heat maps of the simulated and human fixation probabilities, each for the beginning of a text from the Dundee corpus. While some differences between simulated and human fixation probabilities can be noted, there are similarities in the general qualitative characteristics of the two heat maps. In particular, function words and short words are less likely to be fixed than content words and longer words in the two simulated and human data."}, {"heading": "5.2 Qualitative Properties", "text": "We will examine the second key question we have defined in Section 1 and examine the qualitative characteristics of the simulated fixation sequences. We will focus on comparing the predictions of NEAT with those of the word frequency, which are comparable to the task of predicting fixation sequences (see Section 5.1). We will show that NEAT nevertheless makes relevant predictions that go beyond the frequency. This effect is also seen in NEAT. Predictors derived from the word frequency treat the decision whether words are fixed independently or skipped when the previous word is skipped (Rayner, 1998). This effect is also seen in NEAT, both in the human data and in the simulated fixation data."}, {"heading": "6 Conclusions", "text": "We investigated the hypothesis that human reading strategies optimize a trade-off between the precision of speech comprehension and the economics of attention. We made this idea clear in NEAT, a hard-attention neural reading architecture that can be trained consistently to optimize this trade-off. Experiments on the Dundee corpus show that NEAT provides accurate predictions of human skipping behavior, and it predicts reading times even though it only has access to 60.4% of the words in the corpus to estimate surprises. Finally, we found that known qualitative properties of skipping appear in our model, even though they are not explicitly included in the architecture, such as context dependence on fixations, differential skip rates across parts of the language, and correlations with other known predictors of human reading behavior."}], "references": [{"title": "Learning wake-sleep recurrent attention models", "author": ["Ba", "Jimmy", "Ruslan R. Salakhutdinov", "Roger B. Grosse", "Brendan J. Frey."], "venue": "Advances in Neural Information Processing Systems. pages 2575\u20132583.", "citeRegEx": "Ba et al\\.,? 2015", "shortCiteRegEx": "Ba et al\\.", "year": 2015}, {"title": "Neural machine translation by jointly learning to align and translate", "author": ["Bahdanau", "Dzmitry", "Kyunghyun Cho", "Yoshua Bengio."], "venue": "Proceedings of the International Conference on Learning Representations.", "citeRegEx": "Bahdanau et al\\.,? 2015", "shortCiteRegEx": "Bahdanau et al\\.", "year": 2015}, {"title": "The Dundee treebank", "author": ["Barrett", "Maria", "\u017deljko Agi\u0107", "Anders S\u00f8gaard."], "venue": "Proceedings of the 14th International Workshop on Treebanks and Linguistic Theories. pages 242\u2013248.", "citeRegEx": "Barrett et al\\.,? 2015", "shortCiteRegEx": "Barrett et al\\.", "year": 2015}, {"title": "A rational model of eye movement control in reading", "author": ["Bicknell", "Klinton", "Roger Levy."], "venue": "Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics. pages 1168\u20131178.", "citeRegEx": "Bicknell et al\\.,? 2010", "shortCiteRegEx": "Bicknell et al\\.", "year": 2010}, {"title": "What your eyes do while your mind is reading", "author": ["P.A. Carpenter", "M.A. Just."], "venue": "K. Rayner, editor, Eye Movements in Reading, Academic Press, New York, pages 275\u2013307.", "citeRegEx": "Carpenter and Just.,? 1983", "shortCiteRegEx": "Carpenter and Just.", "year": 1983}, {"title": "Data from eye-tracking corpora as evidence for theories of syntactic processing complexity", "author": ["Demberg", "Vera", "Frank Keller."], "venue": "Cognition 109(2):193\u2013210.", "citeRegEx": "Demberg et al\\.,? 2008", "shortCiteRegEx": "Demberg et al\\.", "year": 2008}, {"title": "A dynamical model of saccade generation in reading based on spatially distributed lexical processing", "author": ["Engbert", "Ralf", "Andr\u00e9 Longtin", "Reinhold Kliegl."], "venue": "Vision Research 42(5):621\u2013636.", "citeRegEx": "Engbert et al\\.,? 2002", "shortCiteRegEx": "Engbert et al\\.", "year": 2002}, {"title": "SWIFT: A dynamical model of saccade generation during reading", "author": ["Engbert", "Ralf", "Antje Nuthmann", "Eike M. Richter", "Reinhold Kliegl."], "venue": "Psychological Review 112(4):777\u2013813.", "citeRegEx": "Engbert et al\\.,? 2005", "shortCiteRegEx": "Engbert et al\\.", "year": 2005}, {"title": "Insensitivity of the human sentence-processing system to hierarchical structure", "author": ["S.L. Frank", "R. Bod."], "venue": "Psychological Science 22:829\u2013834.", "citeRegEx": "Frank and Bod.,? 2011", "shortCiteRegEx": "Frank and Bod.", "year": 2011}, {"title": "A probabilistic Earley parser as a psycholinguistic model", "author": ["Hale", "John."], "venue": "Proceedings of Conference of the North American Chapter of the Association for Computational Linguistics. volume 2, pages 159\u2013166.", "citeRegEx": "Hale and John.,? 2001", "shortCiteRegEx": "Hale and John.", "year": 2001}, {"title": "Predicting word fixations in text with a CRF model for capturing general reading strategies among readers", "author": ["Hara", "Tadayoshi", "Daichi Mochihashi Yoshinobu Kano", "Akiko Aizawa."], "venue": "Proceedings of the 1st Workshop on Eye-tracking and", "citeRegEx": "Hara et al\\.,? 2012", "shortCiteRegEx": "Hara et al\\.", "year": 2012}, {"title": "Human gaze control in realworld scene perception", "author": ["Henderson", "John."], "venue": "Trends in Cognitive Sciences 7:498\u2013504.", "citeRegEx": "Henderson and John.,? 2003", "shortCiteRegEx": "Henderson and John.", "year": 2003}, {"title": "Teaching machines to read and comprehend", "author": ["Hermann", "Karl Moritz", "Tom\u00e1\u0161 Ko\u010disk\u1ef3", "Edward Grefenstette", "Lasse Espeholt", "Will Kay", "Mustafa Suleyman", "Phil Blunsom."], "venue": "ArXiv:1506.03340.", "citeRegEx": "Hermann et al\\.,? 2015", "shortCiteRegEx": "Hermann et al\\.", "year": 2015}, {"title": "Parafoveal-onfoveal effects in normal reading", "author": ["Kennedy", "Alan", "Jo\u00ebl Pynte."], "venue": "Vision Research 45(2):153\u2013168.", "citeRegEx": "Kennedy et al\\.,? 2005", "shortCiteRegEx": "Kennedy et al\\.", "year": 2005}, {"title": "Expectation-based syntactic comprehension", "author": ["Levy", "Roger."], "venue": "Cognition 106(3):1126\u20131177.", "citeRegEx": "Levy and Roger.,? 2008", "shortCiteRegEx": "Levy and Roger.", "year": 2008}, {"title": "With blinkers on: Robust prediction of eye movements across readers", "author": ["Matthies", "Franz", "Anders S\u00f8gaard."], "venue": "Proceedings of the Conference on Empirical Methods in Natural Language Processing. pages 803\u2013807.", "citeRegEx": "Matthies et al\\.,? 2013", "shortCiteRegEx": "Matthies et al\\.", "year": 2013}, {"title": "Eye movements reveal the on-line computation of lexical probabilities during reading", "author": ["McDonald", "Scott A.", "Richard C. Shillcock."], "venue": "Psychological Science 14(6):648\u2013652.", "citeRegEx": "McDonald et al\\.,? 2003a", "shortCiteRegEx": "McDonald et al\\.", "year": 2003}, {"title": "Low-level predictive inference in reading: the influence of transitional probabilities on eye movements", "author": ["McDonald", "Scott A.", "Richard C. Shillcock."], "venue": "Vision Research 43(16):1735\u20131751.", "citeRegEx": "McDonald et al\\.,? 2003b", "shortCiteRegEx": "McDonald et al\\.", "year": 2003}, {"title": "Recurrent neural network based language model", "author": ["Mikolov", "Tom\u00e1\u0161", "Martin Karafi\u00e1t", "Luk\u00e1\u0161 Burget", "Jan \u010cernock\u00fd", "Sanjeev Khudanpur."], "venue": "Proceedings of Interspeech. pages 1045\u20131048.", "citeRegEx": "Mikolov et al\\.,? 2010", "shortCiteRegEx": "Mikolov et al\\.", "year": 2010}, {"title": "Learning where to look: Modeling eye movements in reading", "author": ["Nilsson", "Mattias", "Joakim Nivre."], "venue": "Proceedings of the 13th Conference on Computational Natural Language Learning. pages 93\u2013101.", "citeRegEx": "Nilsson et al\\.,? 2009", "shortCiteRegEx": "Nilsson et al\\.", "year": 2009}, {"title": "Towards a data-driven model of eye movement control in reading", "author": ["Nilsson", "Mattias", "Joakim Nivre."], "venue": "Proceedings of the Workshop on Cognitive Modeling and Computational Linguistics. pages 63\u201371.", "citeRegEx": "Nilsson et al\\.,? 2010", "shortCiteRegEx": "Nilsson et al\\.", "year": 2010}, {"title": "A universal part-of-speech tagset", "author": ["Petrov", "Slav", "Dipanjan Das", "Ryan T. McDonald."], "venue": "Proceedings of the 8th International Conference on Language Resources and Evaluation. pages 2089\u20132096.", "citeRegEx": "Petrov et al\\.,? 2012", "shortCiteRegEx": "Petrov et al\\.", "year": 2012}, {"title": "Eye movements in reading and information processing: 20 years of research", "author": ["K. Rayner"], "venue": "Psychological Bulletin 124(3):372\u2013422.", "citeRegEx": "Rayner,? 1998", "shortCiteRegEx": "Rayner", "year": 1998}, {"title": "Eye movements in reading: Models and data", "author": ["Rayner", "Keith."], "venue": "Journal of Eye Movement Research 2(5):1\u201310.", "citeRegEx": "Rayner and Keith.,? 2009", "shortCiteRegEx": "Rayner and Keith.", "year": 2009}, {"title": "Models of the reading process", "author": ["Rayner", "Keith", "Erik D. Reichle."], "venue": "Wiley Interdisciplinary Reviews: Cognitive Science 1(6):787\u2013799.", "citeRegEx": "Rayner et al\\.,? 2010", "shortCiteRegEx": "Rayner et al\\.", "year": 2010}, {"title": "Toward a model of eye movement control in reading", "author": ["E.D. Reichle", "A. Pollatsek", "D.L. Fisher", "K. Rayner."], "venue": "Psychological Review 105(1):125\u2013157.", "citeRegEx": "Reichle et al\\.,? 1998", "shortCiteRegEx": "Reichle et al\\.", "year": 1998}, {"title": "Using EZ Reader to model the effects of higher level language processing on eye movements during reading", "author": ["E.D. Reichle", "T. Warren", "K. McConnell."], "venue": "Psychonomic Bulletin & Review 16(1):1\u201321.", "citeRegEx": "Reichle et al\\.,? 2009", "shortCiteRegEx": "Reichle et al\\.", "year": 2009}, {"title": "The EZ Reader model of eyemovement control in reading: Comparisons to other models", "author": ["Reichle", "Erik D.", "Keith Rayner", "Alexander Pollatsek."], "venue": "Behavioral and Brain Sciences 26(04):445\u2013476.", "citeRegEx": "Reichle et al\\.,? 2003", "shortCiteRegEx": "Reichle et al\\.", "year": 2003}, {"title": "The effect of word predictability on reading time is logarithmic", "author": ["Smith", "Nathaniel J.", "Roger Levy."], "venue": "Cognition 128(3):302\u2013319.", "citeRegEx": "Smith et al\\.,? 2013", "shortCiteRegEx": "Smith et al\\.", "year": 2013}, {"title": "Sequence to sequence learning with neural networks", "author": ["Sutskever", "Ilya", "Oriol Vinyals", "Quoc VV Le."], "venue": "Advances in Neural Information Processing Systems. pages 3104\u20133112.", "citeRegEx": "Sutskever et al\\.,? 2014", "shortCiteRegEx": "Sutskever et al\\.", "year": 2014}, {"title": "Syntactic parsing", "author": ["Van Gompel", "Roger PG", "Martin J. Pickering."], "venue": "The Oxford Handbook of Psycholinguistics, Oxford University Press, pages 289\u2013307.", "citeRegEx": "Gompel et al\\.,? 2007", "shortCiteRegEx": "Gompel et al\\.", "year": 2007}, {"title": "Simple statistical gradient-following algorithms for connectionist reinforcement learning", "author": ["Williams", "Ronald J."], "venue": "Machine Learning 8(34):229\u2013256.", "citeRegEx": "Williams and J.,? 1992", "shortCiteRegEx": "Williams and J.", "year": 1992}, {"title": "Show, attend and tell: Neural image caption generation with visual attention", "author": ["Xu", "Kelvin", "Jimmy Ba", "Ryan Kiros", "Aaron Courville", "Ruslan Salakhutdinov", "Richard Zemel", "Yoshua Bengio."], "venue": "ArXiv:1502.03044.", "citeRegEx": "Xu et al\\.,? 2015", "shortCiteRegEx": "Xu et al\\.", "year": 2015}], "referenceMentions": [{"referenceID": 22, "context": "Saccades are the rapid jumps that occur between fixations, typically lasting 20\u201340 ms and spanning 7\u2013 9 characters (Rayner, 1998).", "startOffset": 115, "endOffset": 129}, {"referenceID": 10, "context": "More recent approaches use machine learning models trained on eye-tracking data to predict human reading patterns (Nilsson and Nivre, 2009, 2010; Hara et al., 2012; Matthies and S\u00f8gaard, 2013).", "startOffset": 114, "endOffset": 192}, {"referenceID": 6, "context": ", 1998, 2003, 2009), SWIFT (Engbert et al., 2002, 2005), or the Bayesian Model of Bicknell and Levy (2010). More recent approaches use machine learning models trained on eye-tracking data to predict human reading patterns (Nilsson and Nivre, 2009, 2010; Hara et al.", "startOffset": 28, "endOffset": 107}, {"referenceID": 8, "context": "While surprisal has been shown to correlate with word-by-word reading times (McDonald and Shillcock, 2003a,b; Demberg and Keller, 2008; Frank and Bod, 2011; Smith and Levy, 2013), it cannot explain other aspects of human reading, such as reverse saccades, re-fixations, or skipping.", "startOffset": 76, "endOffset": 178}, {"referenceID": 1, "context": "A range of attention-based neural network architectures have recently been proposed in the literature, showing promise in both NLP and computer vision (e.g., Mnih et al., 2014; Bahdanau et al., 2015).", "startOffset": 151, "endOffset": 199}, {"referenceID": 1, "context": "In NLP, soft attention can mitigate the difficulty of compressing long sequences into fixed-dimensional vectors, with applications in machine translation (Bahdanau et al., 2015) and question answering (Hermann et al.", "startOffset": 154, "endOffset": 177}, {"referenceID": 12, "context": ", 2015) and question answering (Hermann et al., 2015).", "startOffset": 31, "endOffset": 53}, {"referenceID": 0, "context": "In computer vision, both types of attention can be used for selecting regions in an image (Ba et al., 2015; Xu et al., 2015).", "startOffset": 90, "endOffset": 124}, {"referenceID": 32, "context": "In computer vision, both types of attention can be used for selecting regions in an image (Ba et al., 2015; Xu et al., 2015).", "startOffset": 90, "endOffset": 124}, {"referenceID": 29, "context": "We use a neural sequence-to-sequence architecture (Sutskever et al., 2014) with a hard attention mechanism.", "startOffset": 50, "endOffset": 74}, {"referenceID": 18, "context": ",i\u22121,hN) over the vocabulary in the i-th step, as is common in neural language modeling (Mikolov et al., 2010).", "startOffset": 88, "endOffset": 110}, {"referenceID": 32, "context": "To make learning more stable, we add an entropy term encouraging the distribution to be smooth, following Xu et al. (2015). The parameter updates to A are thus:", "startOffset": 106, "endOffset": 123}, {"referenceID": 12, "context": "We trained NEAT on the training set of the Daily Mail section of the corpus described by Hermann et al. (2015), which consists of 195,462 articles from the Daily Mail newspaper, containing approximately 200 million tokens.", "startOffset": 89, "endOffset": 111}, {"referenceID": 22, "context": "Fixations of Successive Words While predictors derived from word frequency treat the decision whether to fixate or skip words as independent, humans are more likely to fixate a word when the previous word was skipped (Rayner, 1998).", "startOffset": 217, "endOffset": 231}, {"referenceID": 4, "context": "Parts of Speech Part of speech categories are known to be a predictor of fixation probabilities, with content words being more likely to be fixated than function words (Carpenter and Just, 1983).", "startOffset": 168, "endOffset": 194}, {"referenceID": 21, "context": "In Table 4, we give the simulated fixation probabilities and the human fixation probabilities estimated from the Dundee corpus for the tags of the Universal PoS tagset (Petrov et al., 2012), using the PoS annotation of Barrett et al.", "startOffset": 168, "endOffset": 189}, {"referenceID": 2, "context": ", 2012), using the PoS annotation of Barrett et al. (2015). We again compare with the probabilities of a threshold predictor derived from", "startOffset": 37, "endOffset": 59}], "year": 2016, "abstractText": "When humans read text, they fixate some words and skip others. However, there have been few attempts to explain skipping behavior with computational models, as most existing work has focused on predicting reading times (e.g., using surprisal). In this paper, we propose a novel approach that models both skipping and reading, using an unsupervised architecture that combines a neural attention with autoencoding, trained on raw text using reinforcement learning. Our model explains human reading behavior as a tradeoff between precision of language understanding (encoding the input accurately) and economy of attention (fixating as few words as possible). We evaluate the model on the Dundee eye-tracking corpus, showing that it accurately predicts skipping behavior and reading times, is competitive with surprisal, and captures known qualitative features of human reading.", "creator": "LaTeX with hyperref package"}}}