{"id": "1608.05745", "review": {"conference": "NIPS", "VERSION": "v1", "DATE_OF_SUBMISSION": "19-Aug-2016", "title": "RETAIN: An Interpretable Predictive Model for Healthcare using Reverse Time Attention Mechanism", "abstract": "Accuracy and interpretation are two goals of any successful predictive models. Most existing works have to suffer the tradeoff between the two by either picking complex black box models such as recurrent neural networks (RNN) or relying on less accurate traditional models with better interpretation such as logistic regression. To address this dilemma, we present REverse Time AttentIoN model (RETAIN) for analyzing EHR data that achieves high accuracy while remaining clinically interpretable. RETAIN is a two-level neural attention model that can find influential past visits and significant clinical variables within those visits (e.g,. key diagnoses). RETAIN mimics physician practice by attending the EHR data in a reverse time order so that more recent clinical visits will likely get higher attention. Experiments on a large real EHR dataset of 14 million visits from 263K patients over 8 years confirmed the comparable predictive accuracy and computational scalability to the state-of-the-art methods such as RNN. Finally, we demonstrate the clinical interpretation with concrete examples from RETAIN.", "histories": [["v1", "Fri, 19 Aug 2016 21:54:46 GMT  (3233kb,D)", "http://arxiv.org/abs/1608.05745v1", "Accepted at Neural Information Processing Systems (NIPS) 2016"], ["v2", "Tue, 30 Aug 2016 06:03:43 GMT  (3234kb,D)", "http://arxiv.org/abs/1608.05745v2", "Accepted at Neural Information Processing Systems (NIPS) 2016"], ["v3", "Wed, 14 Sep 2016 19:45:03 GMT  (3236kb,D)", "http://arxiv.org/abs/1608.05745v3", "Accepted at Neural Information Processing Systems (NIPS) 2016"], ["v4", "Sun, 26 Feb 2017 15:13:31 GMT  (3233kb,D)", "http://arxiv.org/abs/1608.05745v4", "Accepted at Neural Information Processing Systems (NIPS) 2016"]], "COMMENTS": "Accepted at Neural Information Processing Systems (NIPS) 2016", "reviews": [], "SUBJECTS": "cs.LG cs.AI cs.NE", "authors": ["edward choi", "mohammad taha bahadori", "jimeng sun", "joshua kulas", "andy schuetz", "walter f stewart"], "accepted": true, "id": "1608.05745"}, "pdf": {"name": "1608.05745.pdf", "metadata": {"source": "CRF", "title": "RETAIN: Interpretable Predictive Model in Healthcare using Reverse Time Attention Mechanism", "authors": ["Edward Choi", "Mohammad Taha Bahadori", "Andy Schuetz", "Walter F. Stewart", "Jimeng Sun"], "emails": ["mp2893@gatech.edu},", "bahadori@gatech.edu},", "schueta1@sutterhealth.org,", "stewarwf@sutterhealth.org,", "jsun@cc.gatech.edu"], "sections": [{"heading": null, "text": "To solve this dilemma, we introduce the REverse Time Attack Model (RETAIN) for analyzing EHR data, which achieves high accuracy while remaining clinically interpretable. RETAIN is a two-step neural attention model that can find influential past visits and significant clinical variables (e.g. key diagnoses) within these visits. RETAIN mimics the physician's office by recording EHR data in reverse order, so that newer clinical visits are likely to receive greater attention. Experiments with a large real EHR data set of 14 million visits by 263K patients over a period of 8 years confirmed comparable predictive accuracy and computer-based scalability to modern methods such as RNN. Finally, we demonstrate clinical interpretation using concrete examples from RETAIN."}, {"heading": "1 Introduction", "text": "The widespread use of electronic systems (EHR) has opened up the possibility of improving the quality of clinical services."}, {"heading": "2 Methodology", "text": "In this section, we first describe the structure of the sequential EHR data, our notation and a general form of predictive analysis in health care using EHR. Then, we describe the details of the RETAIN.EHR structure and our notation. EHR data of each patient are modeled as a time-described sequence of multivariate observations. Suppose we use r different variables, the n-te patient of all N-patients may be \"referred to by a sequence of T (n) -tuples (t (n) i) i (c) i) i (c) i) c (c) i) i) i) i) i) i) i) i) i) e (n) i) e (n) i) e (n) e (n) e (n) e (n) e (n) e (c) e (c) c) c (c) e (c) c) c) e (c) e (c) e) c) c) e (c) e) c) c) c) e (c) e) c) c) c) c) c) c) c) c) c) c) c) c) c) c) c) c) c) c) c) c) c) c) c) c) c) c) c) c) c) c) c) c) c) c) c) c) c) c) c) c) c) c) c) c) c) c) c) c) c) c) c) c) c) c) c) c) c) c) c) c) c) c) c) c) c) c) c) c) c) c) c) c) c) c) c) c) c) c) c) c) c) c) c) c) c) c) c) c) c) c) c) c) c) c) c) c) c) c) c) c) c) c) c) c) c) c) c) c) c) c) c) c) c) c) c) c) c) c) c) c) c) c) c) c) c) c) c) c) c) c))"}, {"heading": "2.1 Preliminaries on Neural Attention Models", "text": "Attention is based on neural network models, which have recently received a lot of attention in image processing. In general attention, a specific focus on the respective model, processing natural language (2, 22, 32) and speech recognition (12) allows us to see the need for an attention mechanism in the translation task of the language as follows: the representation of the entire sentence with a fixed size vector is inefficient; the neural translation engine usually finds it difficult to translate the given sentence, which is represented by a single vector. To find the j-th word in the target language, we generate attention for i = 1.,., S for each word in the original sentence. Then we calculate the context vector cj = to use it to predict the j-th word in the target language."}, {"heading": "4 Experiments", "text": "In this section we show that while RETAIN competes with RNNs in terms of performance, we can interpret the knowledge gained from it. Due to lack of space, we report only on the results of learning to diagnose (L2D) tasks and move the results of disease progression modelling (DPM) to Appendix B."}, {"heading": "4.1 Experimental setting", "text": "Patients are middle-aged adults selected for a heart failure study. We extracted visiting strategies consisting of diagnosis, medication and procedure codes from the records, drug orders, procedure instructions and problem lists. In order to reduce the dimensionality and at the same time obtain clinical information, we used existing medical groups to aggregate the codes into input variables. Details of the medical groups are given in Appendix A. The statistics of the data set are provided in Table 1. Implementation Details: We implemented RETAIN with Theano 0.8 [4]. To train the model, we used Adadelta [37] with the mini-batch of 100 patients. Training was carried out in a machine equipped with Intel Xeon E5-2630, 256GB RAM, two Nvidia Tesla K80 and CUDA 7.5.Baselines."}, {"heading": "4.2 Heart Failure Prediction", "text": "Objective: In view of a visit sequence x1,., xT, we attempt to predict whether the patient will be diagnosed with heart failure (RF). This can be considered a specific case of DPM, in which we make a prediction for a single disease at the end of the sequence. However, as this is an approximate prediction task, we use the logistic sigmoid function instead of the Softmax in step 5 Cohort construction: From the source data set, 3,884 cases are selected and approximately 10 controls are selected for each case (28,903 controls). The selection criteria for the case / control are described in full in the supplementary section. Cases have index data to indicate the date on which they will be diagnosed with RF. Controls have the same index data as their corresponding cases. We extract diagnostic codes, medication codes and procedure codes in the 18-month window preceding the index date.Training: The patient-cohort ratio was divided into training, 0.75 and validation in a set of 0.75."}, {"heading": "4.3 Model Interpretation for Heart Failure Prediction", "text": "We show the interpretative ability of RETAIN by studying its behavior in the RF prediction task. We select an RF patient from the test set and calculate the contribution of variables (medical codes in this case) to the preparation of the binary prediction. Figure 3a is the visualization of the contributions of variables in each visit. The patient suffered from skin problems, skin diseases (SD), benign neoplasms (BN), excision of the skin lesion (ESL), for some time before the symptoms of RF, arrhythmia (CD), heart valve disease (HVD) and coronary arteriosclerosis (CA), which are then diagnosed with HF at the end. We can see that the skin related2RNN are used two layers of RNN + 3R that perform an embedding for the visit of HVD."}, {"heading": "5 Conclusion", "text": "Given the power of relapsing neural networks to analyze sequential data, we have proposed RETAIN, which preserves the predictive power of RNN while allowing for a higher degree of interpretation. RETAIN's key idea is to improve predictive accuracy through a sophisticated process of attention generation, while keeping the presentation part simple for interpretation, making the entire algorithm precise and interpretable. RETAIN educates two RNN in reverse order to efficiently generate the corresponding attention variables. In future work, we plan to develop an interactive visualization system for RETAIN and evaluate RETAIN in healthcare applications."}, {"heading": "A Details of the experiment settings", "text": "We are also the random search with m, p and q fixed at 256.The final value we used to train RETAIN for predicting heart failure is m, p = 128, dropoutvi. We are also the random search with m, p, 0.8}. We are also the random search with m, p and q fixed at 256.The final value we used to train RETAIN for predicting heart failure."}, {"heading": "B Results on disease progression modeling", "text": "Goal: Given a sequence of visits x1,.., xT, the goal of the DPM is to predict for each time step i, the codes that occur on the next visit x2,.., xT + 1. However, since we are interested in the progression of the disease, we create a separate set of labels y1,.., yT, which do not contain non-diagnostic codes such as drug codes or procedural codes. Therefore, yi will contain diagnostic codes from the next visit xi + 1 Dataset: We divide the entire data set described in Table 1 into 0.75: 0.10: 0.15 ratio, respectively, for training, validation set, and testset.Baseline: We use the same baseline models that we use for RF forecasting. However, since we predict 283 binary labels, we replace the logistic regression function with the Softmax function. The drop-out and the L2 regulatory policies remain the same."}], "references": [{"title": "Multiple object recognition with visual attention", "author": ["J. Ba", "V. Mnih", "K. Kavukcuoglu"], "venue": "ICLR", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2015}, {"title": "Neural machine translation by jointly learning to align and translate", "author": ["D. Bahdanau", "K. Cho", "Y. Bengio"], "venue": "ICLR", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2015}, {"title": "Learning long-term dependencies with gradient descent is difficult", "author": ["Y. Bengio", "P. Simard", "P. Frasconi"], "venue": "Neural Networks, IEEE Transactions on, 5(2):157\u2013166", "citeRegEx": "3", "shortCiteRegEx": null, "year": 1994}, {"title": "Theano: a CPU and GPU math expression compiler", "author": ["J. Bergstra", "O. Breuleux", "F. Bastien", "P. Lamblin", "R. Pascanu", "G. Desjardins", "J. Turian", "D. Warde-Farley", "Y. Bengio"], "venue": "Proceedings of SciPy", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2010}, {"title": "The impact of ehealth on the quality and safety of health care: a systematic overview", "author": ["A.D. Black", "J. Car", "C. Pagliari", "C. Anandan", "K. Cresswell", "T. Bokun", "B. McKinstry", "R. Procter", "A. Majeed", "A. Sheikh"], "venue": "PLoS Med, 8(1):e1000387", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2011}, {"title": "Intelligible models for healthcare: Predicting pneumonia risk and hospital 30-day readmission", "author": ["R. Caruana", "Y. Lou", "J. Gehrke", "P. Koch", "M. Sturm", "N. Elhadad"], "venue": "KDD", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2015}, {"title": "Systematic review: impact of health information technology on quality", "author": ["B. Chaudhry", "J. Wang", "S. Wu", "M. Maglione", "W. Mojica", "E. Roth", "S.C. Morton", "P.G. Shekelle"], "venue": "efficiency, and costs of medical care. Annals of internal medicine, 144(10):742\u2013752", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2006}, {"title": "Distilling knowledge from deep networks with applications to healthcare domain", "author": ["Z. Che", "S. Purushotham", "R. Khemani", "Y. Liu"], "venue": "arXiv preprint arXiv:1512.03542", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2015}, {"title": "Learning phrase representations using rnn encoder-decoder for statistical machine translation", "author": ["K. Cho", "B. Van Merri\u00ebnboer", "C. Gulcehre", "D. Bahdanau", "F. Bougares", "H. Schwenk", "Y. Bengio"], "venue": "EMNLP", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2014}, {"title": "Multi-layer representation learning for medical concepts", "author": ["E. Choi", "M.T. Bahadori", "E. Searles", "C. Coffey", "J. Sun"], "venue": "KDD", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2016}, {"title": "Doctor ai: Predicting clinical events via recurrent neural networks", "author": ["E. Choi", "M.T. Bahadori", "J. Sun"], "venue": "arXiv preprint arXiv:1511.05942", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2015}, {"title": "Attention-based models for speech recognition", "author": ["J.K. Chorowski", "D. Bahdanau", "D. Serdyuk", "K. Cho", "Y. Bengio"], "venue": "NIPS, pages 577\u2013585", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2015}, {"title": "Visualizing higher-layer features of a deep network", "author": ["D. Erhan", "Y. Bengio", "A. Courville", "P. Vincent"], "venue": "University of Montreal", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2009}, {"title": "Predicting clinical events by combining static and dynamic information using recurrent neural networks", "author": ["C. Esteban", "O. Staeck", "Y. Yang", "V. Tresp"], "venue": "arXiv preprint arXiv:1602.02685", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2016}, {"title": "Clinical predictors of progression to Alzheimer disease in amnestic mild cognitive impairment", "author": ["A.S. Fleisher", "B.B. Sowell", "C. Taylor", "A.C. Gamst", "R.C. Petersen", "L.J. Thal", "f. t. A.D.C. Study"], "venue": null, "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2007}, {"title": "Bringing cohort studies to the bedside: framework for a \u2019green button\u2019 to support clinical decision-making", "author": ["B. Gallego", "S.R. Walter", "R.O. Day", "A.G. Dunn", "V. Sivaraman", "N. Shah", "C.A. Longhurst", "E. Coiera"], "venue": "Journal of Comparative Effectiveness Research,", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2015}, {"title": "Sequence learning with recurrent networks: analysis of internal representations", "author": ["J. Ghosh", "V. Karamcheti"], "venue": "Aerospace Sensing, pages 449\u2013460. International Society for Optics and Photonics", "citeRegEx": "19", "shortCiteRegEx": null, "year": 1992}, {"title": "Costs and benefits of health information technology: new trends from the literature", "author": ["C.L. Goldzweig", "A. Towfigh", "M. Maglione", "P.G. Shekelle"], "venue": "Health affairs, 28(2):w282\u2013w293", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2009}, {"title": "Draw: A recurrent neural network for image generation", "author": ["K. Gregor", "I. Danihelka", "A. Graves", "D. Wierstra"], "venue": "arXiv preprint arXiv:1502.04623", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2015}, {"title": "Teaching machines to read and comprehend", "author": ["K.M. Hermann", "T. Kocisky", "E. Grefenstette", "L. Espeholt", "W. Kay", "M. Suleyman", "P. Blunsom"], "venue": "NIPS, pages 1684\u20131692", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2015}, {"title": "Long short-term memory", "author": ["S. Hochreiter", "J. Schmidhuber"], "venue": "Neural computation, 9(8):1735\u20131780", "citeRegEx": "23", "shortCiteRegEx": null, "year": 1997}, {"title": "Information. Medi-span electronic drug file (med-file", "author": ["D.W.K. C"], "venue": "v2. http://www.wolterskluwercdi. com/drug-data/medi-span-electronic-drug-file/", "citeRegEx": "24", "shortCiteRegEx": "24", "year": 2016}, {"title": "Use of electronic health records in us hospitals", "author": ["A.K. Jha", "C.M. DesRoches", "E.G. Campbell", "K. Donelan", "S.R. Rao", "T.G. Ferris", "A. Shields", "S. Rosenbaum", "D. Blumenthal"], "venue": "N Engl J Med", "citeRegEx": "25", "shortCiteRegEx": null, "year": 2009}, {"title": "Visualizing and understanding recurrent networks", "author": ["A. Karpathy", "J. Johnson", "F.-F. Li"], "venue": "arXiv preprint arXiv:1506.02078", "citeRegEx": "26", "shortCiteRegEx": null, "year": 2015}, {"title": "Use of diverse electronic medical record systems to identify genetic risk for type 2 diabetes within a genome-wide association study", "author": ["A.N. Kho", "M.G. Hayes", "L. Rasmussen-Torvik", "J.A. Pacheco", "W.K. Thompson", "L.L. Armstrong", "J.C. Denny", "P.L. Peissig", "A.W. Miller", "W.-Q. Wei", "S.J. Bielinski", "C.G. Chute", "C.L. Leibson", "G.P. Jarvik", "D.R. Crosslin", "C.S. Carlson", "K.M. Newton", "W.A. Wolf", "R.L. Chisholm", "W.L. Lowe"], "venue": null, "citeRegEx": "27", "shortCiteRegEx": "27", "year": 2012}, {"title": "Building high-level features using large scale unsupervised learning", "author": ["Q.V. Le"], "venue": "ICASSP", "citeRegEx": "28", "shortCiteRegEx": null, "year": 2013}, {"title": "A simple way to initialize recurrent networks of rectified linear units", "author": ["Q.V. Le", "N. Jaitly", "G.E. Hinton"], "venue": "arXiv preprint arXiv:1504.00941", "citeRegEx": "29", "shortCiteRegEx": null, "year": 2015}, {"title": "Learning to Diagnose with LSTM Recurrent Neural Networks", "author": ["Z.C. Lipton", "D.C. Kale", "C. Elkan", "R. Wetzell"], "venue": "ICLR", "citeRegEx": "30", "shortCiteRegEx": null, "year": 2016}, {"title": "et al", "author": ["V. Mnih", "N. Heess", "A. Graves"], "venue": "Recurrent models of visual attention. In NIPS", "citeRegEx": "31", "shortCiteRegEx": null, "year": 2014}, {"title": "A neural attention model for abstractive sentence summarization", "author": ["A.M. Rush", "S. Chopra", "J. Weston"], "venue": "EMNLP", "citeRegEx": "32", "shortCiteRegEx": null, "year": 2015}, {"title": "Learning individual and population level traits from clinical temporal data", "author": ["S. Saria", "D. Koller", "A. Penn"], "venue": "NIPS, Predictive Models in Personalized Medicine workshop", "citeRegEx": "33", "shortCiteRegEx": null, "year": 2010}, {"title": "A probabilistic graphical model for individualizing prognosis in chronic", "author": ["P. Schulam", "S. Saria"], "venue": "complex diseases. In AMIA, volume 2015, page 143", "citeRegEx": "34", "shortCiteRegEx": null, "year": 2015}, {"title": "Supervised patient similarity measure of heterogeneous patient records", "author": ["J. Sun", "F. Wang", "J. Hu", "S. Edabollahi"], "venue": "ACM SIGKDD Explorations Newsletter, 14(1):16\u201324", "citeRegEx": "35", "shortCiteRegEx": null, "year": 2012}, {"title": "Show", "author": ["K. Xu", "J. Ba", "R. Kiros", "A. Courville", "R. Salakhutdinov", "R. Zemel", "Y. Bengio"], "venue": "attend and tell: Neural image caption generation with visual attention. In ICML", "citeRegEx": "36", "shortCiteRegEx": null, "year": 2015}, {"title": "Adadelta: an adaptive learning rate method", "author": ["M.D. Zeiler"], "venue": "arXiv preprint arXiv:1212.5701", "citeRegEx": "37", "shortCiteRegEx": null, "year": 2012}], "referenceMentions": [{"referenceID": 6, "context": "Several systematic reviews have underlined the care quality improvement in hospitals using predictive analysis [7, 25, 5, 20].", "startOffset": 111, "endOffset": 125}, {"referenceID": 22, "context": "Several systematic reviews have underlined the care quality improvement in hospitals using predictive analysis [7, 25, 5, 20].", "startOffset": 111, "endOffset": 125}, {"referenceID": 4, "context": "Several systematic reviews have underlined the care quality improvement in hospitals using predictive analysis [7, 25, 5, 20].", "startOffset": 111, "endOffset": 125}, {"referenceID": 17, "context": "Several systematic reviews have underlined the care quality improvement in hospitals using predictive analysis [7, 25, 5, 20].", "startOffset": 111, "endOffset": 125}, {"referenceID": 5, "context": "There is a common belief that one has to trade accuracy for interpretation in favor to simpler models [6].", "startOffset": 102, "endOffset": 105}, {"referenceID": 24, "context": "via decision trees [27]), 2) case-based reasoning by finding similar patients (e.", "startOffset": 19, "endOffset": 23}, {"referenceID": 15, "context": "via k-nearest neighbors [18] and distance metric learning [35]), and 3) identifying a list of risk factors (e.", "startOffset": 24, "endOffset": 28}, {"referenceID": 32, "context": "via k-nearest neighbors [18] and distance metric learning [35]), and 3) identifying a list of risk factors (e.", "startOffset": 58, "endOffset": 62}, {"referenceID": 14, "context": "via LASSO coefficients [15]).", "startOffset": 23, "endOffset": 27}, {"referenceID": 30, "context": "Latent-variable time-series models, such as [33, 34], do capture the temporality, but often have limited interpretation due to abstract state variables.", "startOffset": 44, "endOffset": 52}, {"referenceID": 31, "context": "Latent-variable time-series models, such as [33, 34], do capture the temporality, but often have limited interpretation due to abstract state variables.", "startOffset": 44, "endOffset": 52}, {"referenceID": 27, "context": "Recently, recurrent neural networks (RNN) have been successfully applied in modeling sequential EHR data to perform various predictive analysis such as learning to diagnose [30] and disease progression modeling [11, 14].", "startOffset": 173, "endOffset": 177}, {"referenceID": 10, "context": "Recently, recurrent neural networks (RNN) have been successfully applied in modeling sequential EHR data to perform various predictive analysis such as learning to diagnose [30] and disease progression modeling [11, 14].", "startOffset": 211, "endOffset": 219}, {"referenceID": 13, "context": "Recently, recurrent neural networks (RNN) have been successfully applied in modeling sequential EHR data to perform various predictive analysis such as learning to diagnose [30] and disease progression modeling [11, 14].", "startOffset": 211, "endOffset": 219}, {"referenceID": 16, "context": "While there have been several attempts at directly interpreting RNNs [19, 26, 8], none of them provide the level of interpretation that can serve the healthcare applications.", "startOffset": 69, "endOffset": 80}, {"referenceID": 23, "context": "While there have been several attempts at directly interpreting RNNs [19, 26, 8], none of them provide the level of interpretation that can serve the healthcare applications.", "startOffset": 69, "endOffset": 80}, {"referenceID": 7, "context": "While there have been several attempts at directly interpreting RNNs [19, 26, 8], none of them provide the level of interpretation that can serve the healthcare applications.", "startOffset": 69, "endOffset": 80}, {"referenceID": 27, "context": "In the case of learning to diagnose (L2D) [30], the input vector xi consists of measurements (possibly continuous) collected by monitoring devices.", "startOffset": 42, "endOffset": 46}, {"referenceID": 2, "context": "In the rest of this section, we will use the abstract symbol RNN to denote any recurrent neural network variants that can cope with the vanishing gradient problem [3], such as LSTM [23], GRU [9], and IRNN [29], with any depth (number of hidden layers).", "startOffset": 163, "endOffset": 166}, {"referenceID": 20, "context": "In the rest of this section, we will use the abstract symbol RNN to denote any recurrent neural network variants that can cope with the vanishing gradient problem [3], such as LSTM [23], GRU [9], and IRNN [29], with any depth (number of hidden layers).", "startOffset": 181, "endOffset": 185}, {"referenceID": 8, "context": "In the rest of this section, we will use the abstract symbol RNN to denote any recurrent neural network variants that can cope with the vanishing gradient problem [3], such as LSTM [23], GRU [9], and IRNN [29], with any depth (number of hidden layers).", "startOffset": 191, "endOffset": 194}, {"referenceID": 26, "context": "In the rest of this section, we will use the abstract symbol RNN to denote any recurrent neural network variants that can cope with the vanishing gradient problem [3], such as LSTM [23], GRU [9], and IRNN [29], with any depth (number of hidden layers).", "startOffset": 205, "endOffset": 209}, {"referenceID": 0, "context": "1 Preliminaries on Neural Attention Models Attention based neural network models have recently gained much attraction in image processing [1, 31, 21, 36], natural language processing [2, 22, 32] and speech recognition [12].", "startOffset": 138, "endOffset": 153}, {"referenceID": 28, "context": "1 Preliminaries on Neural Attention Models Attention based neural network models have recently gained much attraction in image processing [1, 31, 21, 36], natural language processing [2, 22, 32] and speech recognition [12].", "startOffset": 138, "endOffset": 153}, {"referenceID": 18, "context": "1 Preliminaries on Neural Attention Models Attention based neural network models have recently gained much attraction in image processing [1, 31, 21, 36], natural language processing [2, 22, 32] and speech recognition [12].", "startOffset": 138, "endOffset": 153}, {"referenceID": 33, "context": "1 Preliminaries on Neural Attention Models Attention based neural network models have recently gained much attraction in image processing [1, 31, 21, 36], natural language processing [2, 22, 32] and speech recognition [12].", "startOffset": 138, "endOffset": 153}, {"referenceID": 1, "context": "1 Preliminaries on Neural Attention Models Attention based neural network models have recently gained much attraction in image processing [1, 31, 21, 36], natural language processing [2, 22, 32] and speech recognition [12].", "startOffset": 183, "endOffset": 194}, {"referenceID": 19, "context": "1 Preliminaries on Neural Attention Models Attention based neural network models have recently gained much attraction in image processing [1, 31, 21, 36], natural language processing [2, 22, 32] and speech recognition [12].", "startOffset": 183, "endOffset": 194}, {"referenceID": 29, "context": "1 Preliminaries on Neural Attention Models Attention based neural network models have recently gained much attraction in image processing [1, 31, 21, 36], natural language processing [2, 22, 32] and speech recognition [12].", "startOffset": 183, "endOffset": 194}, {"referenceID": 11, "context": "1 Preliminaries on Neural Attention Models Attention based neural network models have recently gained much attraction in image processing [1, 31, 21, 36], natural language processing [2, 22, 32] and speech recognition [12].", "startOffset": 218, "endOffset": 222}, {"referenceID": 1, "context": "The need for attention mechanism can be seen in the language translation task [2]: Representing the entire sentence with one fixed-size vector is inefficient; the neural translation machine usually finds it difficult to translate the given sentence represented by a single vector.", "startOffset": 78, "endOffset": 81}, {"referenceID": 12, "context": "We can easily choose a more sophisticated but still interpretable representation such as multilayer perceptron (MLP) [13, 28] which has been used for representation learning in EHR data [10].", "startOffset": 117, "endOffset": 125}, {"referenceID": 25, "context": "We can easily choose a more sophisticated but still interpretable representation such as multilayer perceptron (MLP) [13, 28] which has been used for representation learning in EHR data [10].", "startOffset": 117, "endOffset": 125}, {"referenceID": 9, "context": "We can easily choose a more sophisticated but still interpretable representation such as multilayer perceptron (MLP) [13, 28] which has been used for representation learning in EHR data [10].", "startOffset": 186, "endOffset": 190}, {"referenceID": 1, "context": "Overall, our attention mechanism can be viewed as the inverted architecture of the standard attention mechanism for NLP [2] where the words are encoded using RNN and generate the attention weights using MLP.", "startOffset": 120, "endOffset": 123}, {"referenceID": 3, "context": "8 [4].", "startOffset": 2, "endOffset": 5}, {"referenceID": 34, "context": "For training the model, we used Adadelta [37] with the mini-batch of 100 patients.", "startOffset": 41, "endOffset": 45}], "year": 2017, "abstractText": "Accuracy and interpretation are two goals of any successful predictive models. Most existing works have to suffer the tradeoff between the two by either picking complex black box models such as recurrent neural networks (RNN) or relying on less accurate traditional models with better interpretation such as logistic regression. To address this dilemma, we present REverse Time AttentIoN model (RETAIN) for analyzing EHR data that achieves high accuracy while remaining clinically interpretable. RETAIN is a two-level neural attention model that can find influential past visits and significant clinical variables within those visits (e.g,. key diagnoses). RETAIN mimics physician practice by attending the EHR data in a reverse time order so that more recent clinical visits will likely get higher attention. Experiments on a large real EHR dataset of 14 million visits from 263K patients over 8 years confirmed the comparable predictive accuracy and computational scalability to the state-of-the-art methods such as RNN. Finally, we demonstrate the clinical interpretation with concrete examples from RETAIN.", "creator": "LaTeX with hyperref package"}}}