{"id": "1608.05995", "review": {"conference": "NIPS", "VERSION": "v1", "DATE_OF_SUBMISSION": "21-Aug-2016", "title": "A Non-convex One-Pass Framework for Generalized Factorization Machine and Rank-One Matrix Sensing", "abstract": "We develop an efficient alternating framework for learning Factorization Machine (FM) on steaming data with provable guarantees. When the feature is $d$-dimension and the target second order coefficient matrix in FM is of rank $k$, our algorithm converges linearly, achieves $O(\\epsilon)$ recovery error after retrieving $O(k^{3}d\\log(1/\\epsilon))$ training instances, consumes $O(kd)$ memory in one-pass of dataset and only requires matrix-vector product operations in each iteration. The key ingredient of our framework is a construction of an estimation sequence endowed with a so-called Conditionally Independent RIP condition. As special cases of FM, our framework can be applied to symmetric or asymmetric rank-one matrix sensing problems, such as inductive matrix completion and phase retrieval.", "histories": [["v1", "Sun, 21 Aug 2016 20:28:29 GMT  (30kb)", "https://arxiv.org/abs/1608.05995v1", "accepted by NIPS 2016"], ["v2", "Fri, 9 Sep 2016 17:54:50 GMT  (20kb)", "http://arxiv.org/abs/1608.05995v2", "accepted by NIPS 2016"], ["v3", "Mon, 12 Sep 2016 21:43:05 GMT  (20kb)", "http://arxiv.org/abs/1608.05995v3", "accepted by NIPS 2016"], ["v4", "Wed, 14 Sep 2016 02:24:22 GMT  (20kb)", "http://arxiv.org/abs/1608.05995v4", "accepted by NIPS 2016"], ["v5", "Tue, 25 Oct 2016 21:23:23 GMT  (20kb)", "http://arxiv.org/abs/1608.05995v5", "accepted by NIPS 2016"]], "COMMENTS": "accepted by NIPS 2016", "reviews": [], "SUBJECTS": "stat.ML cs.LG", "authors": ["ming lin", "jieping ye"], "accepted": true, "id": "1608.05995"}, "pdf": {"name": "1608.05995.pdf", "metadata": {"source": "CRF", "title": "A Non-convex One-Pass Framework for Generalized Factorization Machine and Rank-One Matrix Sensing", "authors": ["Ming Lin", "Jieping Ye"], "emails": ["linmin@umich.edu", "jpye@umich.edu"], "sections": [{"heading": null, "text": "ar Xiv: 160 8.05 995v 5 [st"}, {"heading": "1 Introduction", "text": "This year, only once in its history will there be such a process."}, {"heading": "2 Generalized Factorization Machine (gFM)", "text": "In this section we first present the necessary notation and background of FM and its generalized version gFM."}, {"heading": "2.1 gFM and Rank-One Matrix Sensing", "text": "While the recovery capability of rank-one matrix scanning has been somewhat detectable lately despite the arithmetic task, it is not the case for gFM. Therefore, it is important to discuss the differences between gFM and rank-one matrix scanning to give us a better understanding of the fundamental barriers to the development of detectable gFM algorithms. In the rank-one matrix scanning problem, a relaxed attitude assumes that the sensor operator defined by Aasyi (M) is asymmetric = ui Mvi are independent random vectors. Under this setting, the recovery capability of alternative methods is detectable [Jain and Dhillon, 2013]. However, existing analyses cannot be generalized to their symmetrical counterpart, as ui and vi are not dependent on these frameworks."}, {"heading": "3 One-Pass gFM", "text": "We will focus on the intuition of our algorithms (1) (1). A rigorous theoretical analysis will be presented in the next section. (1) The one-pass gFM is a mini-batch algorithm. (1) In each mini-batch it will require n-training instances and then alternatively updates. (1) The iteration will continue until T mini-batch updates are performed. Since gFM is dealing with a non-convex learning problem, the conventional gradient descendant framework hardly works to show global convergence. Instead, our method is based on a construction of an estimate sequence. Intuitively, if w-batch updates = 0, we will show that 1nA (M), 2M + tr (M) I and tr (M)."}, {"heading": "4 Theoretical Analysis", "text": "In this section, we give the answer to the question of what we have to do to solve our problems. (...) The answer to the question of what we have to do to solve the problems is: \"We have to solve them.\" (...) The answer to the question of what we have to do is: \"We have to solve them.\" (...) The answer to the question of what we have to do is: \"We have to solve them.\" (...) The answer to the question of what we have to do is: \"We have to understand them.\" (...) The answer to the question of what we have to do is. \"(...) The answer to the question of what we have to do is\" we do. \"(...) The answer to the question of what we have to do is\" we do not. \"(...) The answer to the question that we have to do is. (...) The answer to the question that we have to ask ourselves is.\" (...)"}, {"heading": "4.1 Noisy Case", "text": "In this subsection, we analyze the performance of gFM in a rough environment. Let's assume that M + K + K + K + K + K = K + K = K + K = K + K = K + K = K + K + K = K + K + K = K + K + K = K + K + K + K + K + K + K + K + K + K + K + K + K + K + K + K + K + K + K + K + K + K + K = K + K + K + K + K + K + K + K + K + K + K + K + K + K + K + K + K + K + K + K + K + K + K + K + K + K + K + K + K + K + K + K + K + K + K + K + K + K + K + K + K + K + K + K + K + K + K + K + K + K + K + K + K + K + K + K + K + K + K + K + K + K + K + K + K + K + K + K + K + K + K + K + K + K + K + K + K + K + K + K + K + K + K + K + K + K + K + K + K + K + K + K + K + K + K + K + K + K + K + K + K + K + K + K + K + K + K + K + K + K + K + K + K + K + K + K + K + K + K + K + K + K + K + K + K + K + K + K + K + K + K + K + K + K + K + K + K + K + K + K + K + K + K + K + K + K + K + K + K + K + K + K + K + K + K + K + K + K + K + K + K + K + K + K + K + K + K + K + K + K + K + K + K + K + K + K + K + K + K + K + K + K + K + K + K + K + K + K + K + K + K + K + K + K + K + K + K + K +"}, {"heading": "Then for any t \u2265 0,", "text": "The convergence rate is controlled by q, a convergence rate depends on the convergence rate of gFM under noisy environments. Theorem 15 specifies the convergence rate of gFM. Theorem 15 specifies the convergence rate of gFM under noisy environments. We have specified the convergence rate of gFM. We have set the limit of 2 + 2 because the index of the recovery error whose convergence rate is linear. The convergence rate is controlled by q, a convergence rate depends on the convergence rate. Theorem 15 specifies the convergence rate of gFM under noisy conditions. Theorem 15 specifies the convergence rate of gFM. The final recovery error is affected by a minor problem because the convergence rate is linear. The convergence rate is controlled by q, a constant convergence rate depends on the convergence rate."}, {"heading": "5 Conclusion", "text": "In this paper, we propose a demonstrably efficient algorithm for solving the generalized factorization machine (gFM) and rank-one matrix capture. Our method is based on a single-pass system for alternating updating. The proposed algorithm is capable of learning gFM within the O (kd) memory on steaming data, has a linear convergence rate, and requires only the implementation of a matrix vector product. The algorithm requires no more than O (k3d log (1 / B) instances to achieve a recovery error."}, {"heading": "A Preliminary", "text": "In this section we present some important theorems and lemmas in our analysis: The following concentration inequalities are well known. Lemma 16. Let xi have a zero mean sub-Gaussian distribution with variance proxy \u03c3 2. Denote Sn = \u2211 n = 1 aixi for a fixed sequence {ai}. ThenPr (| Sn | > t) \u2264 2 exp (\u2212 t 22\u03c32 (\u2211 n i = 1 a 2 i))), that is, with a probability of at least 1 \u2212 \u03b7, | Sn | \u2248 n = 1ai (x 2 i \u2212 1) \u2264 2 log (2 / \u03b7). Consequence 17. Let xi \u0445 N (0, 1) be a standard Gaussian distribution. Then, with a probability of at least 1 \u2212 \u03b7, n \u0445 i = 1ai (x 2 i \u2212 1) \u2264 2 \u221a 2 log (2 / \u03b7).For random matrix ESrix we have inequalities."}, {"heading": "Define", "text": "Z = n \u00b2 i = 1Si, \u03c3 2 = 1 n max (E \u00b2 (Z \u2212 EZ) (Z \u2212 EZ) 2, E \u00b2 (Z \u2212 EZ) (Z \u2212 EZ) 2).The n \u2265 max (\u03c32, L) / \u043c provided with a probability of at least 1 \u2212 \u03b4, for each 0 < 1, 1n \u00b2 Z \u2212 EZ \u00b2 2 \u2264 9,000 log (d1 + d2) / \u043c) 2. And for each n, 1 n \u00b2 Z \u2212 EZ \u00b2 2 \u2264 4 3 l log (((d1 + d2) / \u043c) + 3 \u221a 2 \u03c32n log (((((d1 + d2) / \u043c)).On the basis of the matrix Bernstein inequality we can bind the covariance estimator. Episode 19 (matrix Bernstein quality for covariance stimulator [?])) + 3 \u04322n log (((d1 + d2) / \u043c) / \u043c).On the basis of the matrix Bernstein inequality we can bind the covariant."}, {"heading": "B Proof of Lemmas", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "B.1 Proof of Lemma 5", "text": "Detection. Since the Gaussian distribution is immutable, it also follows the standard Gaussian distribution. It is easy to see that E (xi'Mxi) = xi'Mxi = tr (M). Defineai, xi'Mxi \u2212 tr (M) = d'j (x '2 i, j \u2212 1) According to sequence 17 for a fixed i, with a probability of at least 1 \u2212 2, | ai | 2 M \u00b2 F \u00b2 (2 / 2). Defineai, xi Mxi \u2212 tr (M) = d \u00b2 j (x '2 i, j \u2212 1)."}, {"heading": "B.2 Proof of Lemma 6", "text": "Proof. Define Random Variables = EEa2i = Ea2i = Ea2i = Ea2i = Ea2i = Ea2i = Ea2i = Ea2i = Ea2i = Ea2i = Ea2i = Ea2i = Ea2i = Ea2i = Ea2i = Ea2i = Ea2i = Ea2i = Ea2i = Ea2i = Ea2i = Ea2i = Ea2a = Ea2a = Ea2i = Ea2i = Ea2i = Ea2i = Ea2a = Ea2i = Ea2i = Ea2i = Ea2i = Ea2i = Ea2i = Ea2i = Ea2i = Ea2i = Ea2i = Ea2i = Ea2i"}, {"heading": "B.3 Proof of Lemma 8", "text": "Proof 2 (2n / 2), 2d log (2n / 2), 2d log (2n / 2), 2d log (2n / 2), 2d log (2n / 2), 2d log (2n / 2), 2d log (2n / 2), 2d log (2n / 2), 2d log (2n / 2), 2d log (2n / 2), 2d log (2), 2d log (2n / 2), 2d log (2n), 2d log (2d log (2n / 2), 2d log (2n / 2), 2d log (2n), 2d log (2), 2d log (2), 2c log (2), 2c log (2), 2d log (2), 2d log (2), 2c log (2), 2c (2), 2c (2), 2c, 2c, 2c, 2c, 2c (2, 2c, 2c, 2c, 2c, 2c, 2c, 2c, 2c, 2c, 2c (2, 2, 2c, 2c, 2, 2c, 2c, 2c, 2c, 2c, 2c, 2c (2, 2, 2, 2c, 2c, 2, 2c, 2c, 2c, 2c, 2, 2c, 2c, 2c, 2, 2c, 2c, 2c, 2c, 2, 2c, 2c, 2c (2, 2, 2, 2, 2c, 2c, 2, 2, 2c, 2c, 2c, 2c, 2c, 2, 2, 2c, 2, 2c, 2, 2c, 2, 2, 2c, 2c, 2c, 2, 2c, 2, 2, 2c, 2, 2, 2c, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2d log (2), 2, 2, 2, 2d log (2n / 2), 2d log (2"}, {"heading": "B.5 Proof of Lemma 9", "text": "According to Conclusion 19, at d \u2265 8 log (2n / \u03b7), b = 2 \u2264 2dTherefore, with a probability of at least 1 \u2212 \u03b7, b = 1 n XX 2 \u2264 9\u0445 \u221a log (2d / \u03b7) / n for n \u2265 2d / \u0445 2. Significantly c = 9\u0438 log (2d / \u03b7) / n, then if n \u2265 Cd / \u03b42, b = I \u2212 1 n XX 2 \u2264 \u03b4."}, {"heading": "B.6 Proof of Lemma 11", "text": "(T) (T) (T) (T) (T) (T) (T) (T) (T) (T) (T) (T) (T) (T) (T) (T) (T) (T) (T) (T) (T) (T) (T) (T) (T) (T) (T) (T) (T) (T) (T) (T) (T) (T) (T) (T) (T) (T) (T) (T) (T) (T) (T) (T) (T) (T) (T) (T) (T) (T) (T) (T) (T) (T) (T) (T) (T) (T) (T) (T) (T) (T) (T) (T) (T) (T) (T) (T) (T) (T) (T) (T) (T) (T) (T) (T) (T) (T) (T) (T) (T) (T) (T) (T) (T) (T) (T) (T) (T) (T) (T) (T) (T) (T) (T) (T) (T) (T) (T) (T) (T) (T (T) (T) (T) (T) (T (T) (T) (T) (T) (T) (T (T) (T) (T) (T) (T (T) (T) (T) (T) (T) (T) (T) (T) (T (T) (T) (T) (T) (T) (T) (T (T) (T) (T (T) (T) (T) (T) (T (T) (T) (T) (T) (T) (T) (T) (T (T) (T (T) (T) (T) (T) (T) (T) (T) (T) (T (T) (T) (T (T) (T) (T) (T) (T) (T (T) (T) (T) (T) ("}, {"heading": "C Proof of Theorem 4", "text": "It is easy to check if EBi = 2M + tr (M) 2 + 2M \u2212 tr (M) 2 + 2M \u2212 tr (M) 2 + 2M (Mxixi) 2 + 2M (Mxixi) 2 + 2M (Mxixi) 2 + 2M (Mxixi) 2 2 (Mxixi) 2 i (Mxixi) 2 i (Mxixi) 2 i (Mxixi) 2 2 2 2 2 2 2 2 2 2 2 2 2"}, {"heading": "D Proof of Lemma 15", "text": "We assume that n \u00b2 s \u00b2 s \u00b2 Ck3d / 2 \u00b2. To prove this, Eq. (5) \u2022 1 2n A \u00b2 A (m \u00b2 \u2212 M) \u2212 1 2 tr (m \u00b2 k \u2212 M) \u2022 1 (m \u00b2 k \u2212 M) \u2022 2 \u2264 1 2n A (m \u00b2 k \u2212 M) \u2212 1 2 tr (m \u00b2 k \u2212 M) I \u2212 (m \u00b2 k \u2212 M) \u2022 2 + 1 2n A (m \u00b2 k \u2212 M) \u2022 2 \u2264 1 2n A (m \u00b2) \u2022 2 + 1 (m \u00b2 k \u2212 M).The latest inequality can be traced back to theorem 4. To tie the first term in the latest inequality, one must find random MatrixBi = xixi \u00b2 M \u00b2 s \u00b2 s \u00b2 xixi M \u00b2 s \u00b2 s \u00b2 (Theorem 4, EBi = 2m \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 (m \u00b2 m \u00b2 s \u00b2), M \u00b2 m \u00b2 s \u00b2 xi \u2212 M \u00b2 s \u00b2 s \u00b2 s s \u00b2 s), M \u00b2 (m \u00b2 m \u00b2 s + m), M \u00b2 s \u00b2 (m \u00b2)."}, {"heading": "E Proof of Lemma 14", "text": "First we exceeded the limit + 1. According to the assumption, if 2 (t + r) \u2264 1 (t + r) \u2264 2 (t + r) \u2264 2 (t + r) \u2264 2 (t + r) \u2264 2 (t + r) \u2264 2 (t + r) \u2264 2 (t + r) \u04452 (t + r) \u04452 (t + r) \u04452 (k + 1t + r) \u2012 k + 12 (t + r) \u2012 k + 1 (t + r) \u2012 k + 2 (t + r) \u2012 k + 1t + 1t (t + r) \u2012 k + 1t (t + r) \u2012 k + 1t (t + 1t + 2t + r) \u2012 k + 1 (t + 2t + 2t + 2t + 2t) \u2012 k + 1 (t + 2t + 2t + 2 (t + r) \u2012 t + 2t (t + 2t) \u2012 t (t + 2t) \u2012 t (t) \u2012 t + 2t (t) \u2012 t."}, {"heading": "F Proof of Lemma 15", "text": "Abbreviatec = 4 + 2 + 2 + 2 + 3 + 3 + 3 + 3 + 3 + 4 + 4 + 4 + 4 + 4 + 4 + 4 + 5 + 5 + 5 + 5 + 5 + 5 + 5 + 5 + 5 + 5 + 5 + 5 + 5 + 5 + 5 + 5 + 5 + 5 + 5 + 5 + 5 + 5 + 5 + 5 + 5 + 5 + 5 + 5 + 5 + 5 + 5 + 5 + 5 + 5 + 5 + 5 + 5 + 5 + 5 + 5 + 5 + 5 + 5 + 5 + 5 + 5 + 5 + 5 + 5 + 5 + 5 + 5 + 5 + 5 + 4 + 5 + 5 + 5 + 5 + 5 + 5 + 5 + 5 + 5 + 5 + 5 + 5 + 5 + 5 + 5 + 5 + 5 + 5 + 5 + 5 + 5 + 5 + 5 + 5 + 5 + 5 + 5 + 5 + 5 + 5 + 5 + 5 + 5 + 5 + 5 + 5 + 5 + 5 + 5 + 5 + 5 + 5 + 5 + 5 + 5 + 5 + 5 + 5 + 5 + 5 + 5 + 5 + 5 + 5 + 5 + 5 + 5 + 5 + 5 + 5 + 5 + 5 + 5 + 5 + 5 + 5 + 5 + 5 + 5 + 5 + 5 + 5 + 5 + 5 + 5 + 5 + 5 + 5 + 5 + 5 + 5 + 5 + 5 + 5 + 5 + 5 + 5 + 5 + 5 + 5 + 5 + 5 + 5 + 5 + 5 + 5 + 5 + 5 + 5 + 5 + 5 + 5 + 5 + 5 + 5 + 5 + 5 + 5 + 5 + 5 + 5 + 5 + 5 + 5 + 5 + 5 + 5 + 5 + 5 + 5 + 5 + 5 + 5 + 5 + 5 + 5 + 5 + 5 + 5 + 5 + 5 + 5 + 5 + 5 + 5 + 5 + 5 + 5 + 5 + 5 + 5 + 5 + 5 + 5 + 5 + 5 + 5 + 5 + 5 + 5 + 5 + 5 + 5 + 5 + 5 + 5 + 5 + 5 + 5 + 5 + 5 + 5 + 5 + 5 + 5 + 5 + 5 + 5 + 5 + 5 + 5 + 5 + 5 + 5 + 5 + 5 + 5 + 5 + 5 + 5 + 5 +"}], "references": [{"title": "Polynomial Networks and Factorization Machines: New Insights and Efficient Training Algorithms", "author": ["Mathieu Blondel", "Masakazu Ishihata", "Akinori Fujino", "Naonori Ueda"], "venue": null, "citeRegEx": "Blondel et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Blondel et al\\.", "year": 2016}, {"title": "ROP: Matrix recovery via rank-one projections", "author": ["T. Tony Cai", "Anru Zhang"], "venue": "The Annals of Statistics,", "citeRegEx": "Cai and Zhang.,? \\Q2015\\E", "shortCiteRegEx": "Cai and Zhang.", "year": 2015}, {"title": "Exact matrix completion via convex optimization", "author": ["Emmanuel J Cand\u00e8s", "Benjamin Recht"], "venue": "Foundations of Computational mathematics,", "citeRegEx": "Cand\u00e8s and Recht.,? \\Q2009\\E", "shortCiteRegEx": "Cand\u00e8s and Recht.", "year": 2009}, {"title": "Phase Retrieval via Matrix Completion", "author": ["Emmanuel J. Candes", "Yonina Eldar", "Thomas Strohmer", "Vlad Voroninski"], "venue": null, "citeRegEx": "Candes et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Candes et al\\.", "year": 2011}, {"title": "Exact and stable covariance estimation from quadratic sampling via convex programming", "author": ["Yuxin Chen", "Yuejie Chi", "Andrea J. Goldsmith"], "venue": "Information Theory, IEEE Transactions on,", "citeRegEx": "Chen et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Chen et al\\.", "year": 2015}, {"title": "An overview of low-rank matrix recovery from incomplete observations", "author": ["Mark A. Davenport", "Justin Romberg"], "venue": null, "citeRegEx": "Davenport and Romberg.,? \\Q2016\\E", "shortCiteRegEx": "Davenport and Romberg.", "year": 2016}, {"title": "The Noisy Power Method: A Meta Algorithm", "author": ["Moritz Hardt", "Eric Price"], "venue": null, "citeRegEx": "Hardt and Price.,? \\Q2013\\E", "shortCiteRegEx": "Hardt and Price.", "year": 2013}, {"title": "Low-rank Matrix Completion", "author": ["Prateek Jain", "Praneeth Netrapalli", "Sujay Sanghavi"], "venue": null, "citeRegEx": "Jain et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Jain et al\\.", "year": 2013}, {"title": "Low rank matrix recovery", "author": ["Richard Kueng", "Holger Rauhut", "Ulrich Terstiege"], "venue": null, "citeRegEx": "Kueng et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Kueng et al\\.", "year": 2011}, {"title": "Matrix Perturbation Theory", "author": ["G.W. Stewart", "Ji-guang Sun"], "venue": null, "citeRegEx": "Stewart and Sun.,? \\Q1990\\E", "shortCiteRegEx": "Stewart and Sun.", "year": 1990}], "referenceMentions": [{"referenceID": 3, "context": ", 2014] as a special case, where the latter one has been studied widely in context such as inductive matrix completion [Jain and Dhillon, 2013] and phase retrieval [Candes et al., 2011].", "startOffset": 164, "endOffset": 185}, {"referenceID": 6, "context": "Then we can construct an estimation sequence via noisy power iteration [Hardt and Price, 2013].", "startOffset": 71, "endOffset": 94}, {"referenceID": 1, "context": ", 2015, Cai and Zhang, 2015, Kueng et al., 2014] as a special case, where the latter one has been studied widely in context such as inductive matrix completion [Jain and Dhillon, 2013] and phase retrieval [Candes et al., 2011]. Despite of the popularity of FMs in industry, there is rare theoretical study of learning guarantees for FMs. One of the main challenges in developing a provable FM algorithm is to handle its symmetric rank-one matrix sensing operator. For conventional matrix sensing problems where the matrix sensing operator is RIP, there are several alternating methods with provable guarantees [Hardt, 2013, Jain et al., 2013, Hardt and Wootters, 2014, Zhao et al., 2015a,b]. However, for a symmetric rank-one matrix sensing operator, the RIP condition doesn\u2019t hold trivially which turns out to be the main difficulty in designing efficient provable FM solvers. In rank-one matrix sensing, when the sensing operator is asymmetric, the problem is also known as inductive matrix completion which can be solved via alternating minimization with a global linear convergence rate [Jain and Dhillon, 2013, Zhong et al., 2015]. For symmetric rank-one matrix sensing operators, we are not aware of any efficient solver by the time of writing this paper. In a special case when the target matrix is of rank one, the problem is called \u201cphase retrieval\u201d whose convex solver is first proposed by Candes et al. [2011] then alternating methods are provided in [Lee et al.", "startOffset": 8, "endOffset": 1421}, {"referenceID": 0, "context": "Some authors also call gFM as second order polynomial network [Blondel et al., 2016].", "startOffset": 62, "endOffset": 84}, {"referenceID": 1, "context": "For example, the sensing operator Aasy(\u00b7) is unbiased ( EAasy(\u00b7) = 0) but the symmetric sensing operator is clearly not [Cai and Zhang, 2015].", "startOffset": 120, "endOffset": 141}, {"referenceID": 2, "context": "In conventional matrix sensing, this construction is possible when the sensing matrix satisfies the Restricted Isometric Property (RIP) [Cand\u00e8s and Recht, 2009]:", "startOffset": 136, "endOffset": 160}, {"referenceID": 1, "context": "However, in gFM and symmetric rank-one matrix sensing, the l2-norm RIP condition cannot be satisfied with high probability [Cai and Zhang, 2015].", "startOffset": 123, "endOffset": 144}, {"referenceID": 9, "context": "To bound \u03b10 , we need the following lemma which directly follows Wely\u2019s and Wedin\u2019s theorems [Stewart and Sun, 1990].", "startOffset": 93, "endOffset": 116}], "year": 2016, "abstractText": "We develop an efficient alternating framework for learning a generalized version of Factorization Machine (gFM) on steaming data with provable guarantees. When the instances are sampled from d dimensional random Gaussian vectors and the target second order coefficient matrix in gFM is of rank k, our algorithm converges linearly, achieves O(\u01eb) recovery error after retrieving O(kd log(1/\u01eb)) training instances, consumes O(kd) memory in one-pass of dataset and only requires matrix-vector product operations in each iteration. The key ingredient of our framework is a construction of an estimation sequence endowed with a so-called Conditionally Independent RIP condition (CI-RIP). As special cases of gFM, our framework can be applied to symmetric or asymmetric rank-one matrix sensing problems, such as inductive matrix completion and phase retrieval.", "creator": "LaTeX with hyperref package"}}}