{"id": "1608.06043", "review": {"conference": "acl", "VERSION": "v1", "DATE_OF_SUBMISSION": "22-Aug-2016", "title": "Context Gates for Neural Machine Translation", "abstract": "In neural machine translation (NMT), generation of a target word depends on both source and target contexts. We find that source contexts have a direct impact on the adequacy of a translation while target contexts on the fluency. Intuitively, generation of a content word should rely more on the source context and generation of a functional word should rely more on the target context. Due to lack of effective control on the influence from source and target contexts, conventional NMT tends to yield fluent but inadequate translations. To address this problem, we propose to use context gates to dynamically control the ratios at which source and target contexts contribute to the generation of target words. In this way, we can enhance the adequacy of NMT while keeping the fluency unchanged. Experiments show that our approach significantly improves upon a standard attention-based NMT system by +2.3 BLEU points.", "histories": [["v1", "Mon, 22 Aug 2016 03:19:27 GMT  (775kb,D)", "http://arxiv.org/abs/1608.06043v1", "Our code is publicly available atthis https URL"], ["v2", "Tue, 6 Dec 2016 03:09:50 GMT  (821kb,D)", "http://arxiv.org/abs/1608.06043v2", "Accepted by TACL 2016"], ["v3", "Wed, 8 Mar 2017 07:14:27 GMT  (821kb,D)", "http://arxiv.org/abs/1608.06043v3", "Accepted by TACL 2017"]], "COMMENTS": "Our code is publicly available atthis https URL", "reviews": [], "SUBJECTS": "cs.CL", "authors": ["zhaopeng tu", "yang liu", "zhengdong lu", "xiaohua liu", "hang li"], "accepted": true, "id": "1608.06043"}, "pdf": {"name": "1608.06043.pdf", "metadata": {"source": "CRF", "title": "Context Gates for Neural Machine Translation", "authors": ["Zhaopeng Tu", "Yang Liu", "Zhengdong Lu", "Xiaohua Liu", "Hang Li"], "emails": ["tu.zhaopeng@huawei.com", "lu.zhengdong@huawei.com", "liuxiaohua3@huawei.com", "hangli.hl@huawei.com", "liuyang2011@tsinghua.edu.cn"], "sections": [{"heading": "1 Introduction", "text": "In fact, it is such that most of them will be able to move into another world, in which they are able to live, in which they are able to live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live."}, {"heading": "2 Neural Machine Translation", "text": "Suppose that x = x1,. xj,. xJ represents a source sentence and y = y1,.. yi,. yI is a target sentence. NMT directly models the probability of translation from the source sentence into the target sentence word by word: P (y | x) = I, i = 1P (yi | y < i, x) (1), where y < i = y1,. \u2212 si, the probability of generating the i-th word yi is calculated by using a recursive neural network (RNN) (RNN), x) (1), where y < i = yi \u2212 source (yi \u2212 1, si) (2), where g (\u00b7) is a softmax function, yi \u2212 1 is the previously generated word, ti is the i-th decoding hidden state, and si is the i-th source."}, {"heading": "2.1 Effects of Source and Target Contexts", "text": "First, we examine empirically our hypothesis: whether source and target contexts correlate with translation adequacy and fluidity? Figure 2 (a) shows the translation lengths with different scaling ratios for source and target contexts. For example, the pair (1.0, 0.5) leads to full utilization of the effect of the source context while halving the effect of the target context. Reducing the effect of the target context (i.e. the lines (0.8, 1.0) and (0.5, 1.0) lead to shorter translations. When the effect of the target context occurs, most of the generated translations reach the maximum length, which is three times as long as the length of the source sentence in this paper. Figure 2 (b) shows the results of the manual evaluation of 200 source sentences randomly sampled from the test sentences. Reducing the effect of the source context (i.e. 0.8, 0.5, lower) leads to a lower (1.0) and (1.0) translation."}, {"heading": "3 Context Gate", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "3.1 Architecture", "text": "x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x"}, {"heading": "3.2 Integrating Context Gate into NMT", "text": "Next, we look at how to integrate context gates into an NMT model.The context gate can determine the amount of context information used in generating words in each step of decoding. For example, Tor assigns higher weights and lower weights to the source context and then feeds it into the activation layer for decoding, which could correct the insufficient translation that misses the translation of \"gua-ngdo-ng\" due to a higher impact from the target context. We have three strategies for integrating context gates into the NMT that either affect one of the translation contexts or affect both contexts, as illustrated in Figure 4. The first two strategies are inspired by the concept of the source context in LSTM (Hochreiter and Schmidhuber, 1997), which affects the amount of memory content used in GATI or affects both contexts, as shown in Figure 1."}, {"heading": "4 Related Work", "text": "In this context, it should be noted that this is an attempt to get a grip on the effects of the crisis on the economy. (...) In this context, it should be noted that the ECB is able to get a grip on the effects of the crisis. (...) In this context, it should be noted that the ECB is able to get a grip on the effects of the crisis. (...). \"(...).\" (...). \"(...).\" (...). \"(...).\" (...). \"(...).\" (...). \"(...).\" (...). \"(...).\" (...). \"(...).\" (...). \"(...).\" (...). \"(...).\" (...). (...). (...). (.). (.). (.). (.). (...). (.). (.). (.). (.). (.). (.). (.).). (.). (.).). (.). (.). (.).). (.). (.).). (.).). (.). (.).). (.).). (.).). (.).). (.). (.). (.).).). (.). (.).). (.). (. (.). (.). (.).). (.).). (.). (. (.).). (.). (. (.).). (.).). (.).). (.). (.). (. (.). (.). (.).). (.). (. (.).).). (.). (.).). (.).). (. (.). (. (.).).).).). (.). (). (.).).). (.). (. (.).).). (.). (.). (). (.). ()."}, {"heading": "5 Experiments", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "5.1 Setup", "text": "The training data set consists of 1.25M pairs of sentences extracted from LDC Corpora4, with 27.9M Chinese words and 34.5m English words. We select the NIST 2002 (MT02) data sets as development sets and the NIST 2005 (MT05), 2006 (MT06) and 2008 (MT08) data sets as test sets. We use the case-insensitive 4-gram NIST BLEU Score (Papineni et al., 2002) as evaluation yardstick and character test (Collins et al., 2005) for statistical significance tests. For efficient training of neural networks, we limit source and target vocabularies to the most common 30K words in Chinese and English, covering approximately 97.7% and 99.3% of the data in the two languages."}, {"heading": "5.2 Translation Quality", "text": "This year, it is as far as ever in the history of the city, where it is as far as never before in the history of the city."}, {"heading": "5.3 Alignment Quality", "text": "According to Tu et al. (2016), we use Alignment error rate (AER) (Och and Ney, 2003) and its variant SAER to measure alignment quality. We find that Context Gate does not improve alignment quality when used alone. However, when used in conjunction with the coverage mechanism, it leads to better alignments, especially one-to-one alignments, by selecting the source word with the highest likelihood of alignment per target (i.e., AER score), one possible reason for this is that better estimated decoding states (from Context Gate) and coverage information help generate more concentrated alignments, as shown in Figure 6."}, {"heading": "5.4 Architecture Analysis", "text": "Table 5 shows a detailed analysis of the architectural components measured in the BLEU score. Several observations can be made: \u2022 Operation Granularity (lines 2 and 3): Element-by-element multiplication (i.e. Context Gate (source) exceeds the vector scalar (i.e. gating scalar), indicating that precise control of each element in the context vector increases translation performance. \u2022 Goal strategy (lines 3 and 4): When fed only with previous decryption state si \u2212 1, Context Gate (both) consistently exceeds the Context Gate (source), indicating that joint control of information from the source and destination side is important in assessing the meaning of the contexts. \u2022 Peephole connections (lines 4 and 5): Peepholes play an important role in the context gate, which improves performance in the BLEU score by 0.57. \u2022 Previously generated word (lines 5 and 6) delivers a more clear signal to the pregeneric word \u2212 1."}, {"heading": "5.5 Effects on Long Sentences", "text": "Figure 7 shows the BLEU score and the average length of translations for each group. As can be seen, NMT prefers short translations to long source sentences (i.e. > 40), which may be due to the fact that the source context is not used sufficiently. Context gate can alleviate this problem by balancing the source and target context, thereby favouring the decoder over long translations. In fact, including context gate increases translation performance for all source set groups. Consider this source sentence from the test set as an example: zho, uliu, zhe, zhe, ngshi, m\u0131, m\u0131, ng, as the supermarket increases translation performance for all source set groups."}, {"heading": "6 Conclusion", "text": "Based on this observation, we propose to use context gates in NMT to dynamically control contributions from the source and target contexts in the generation of target sentences in order to improve the adequacy of NMT. By giving NMT the ability to select an appropriate amount of information from the source and target contexts, the serious translation problem that NMT suffers from can be alleviated. Experimental results show that NMT with context gates achieves consistent and significant improvements in translation quality compared to various variants of NMT models. In principle, the proposed context generic is applicable to all learning tasks from sequence to sequence, where information from the source sequence is transformed into the target sequence (according to appropriateness) and the target sequence is generated (according to fluctuation)."}], "references": [{"title": "Neural machine translation by jointly learning to align and translate", "author": ["Dzmitry Bahdanau", "Kyunghyun Cho", "Yoshua Bengio."], "venue": "ICLR 2015.", "citeRegEx": "Bahdanau et al\\.,? 2015", "shortCiteRegEx": "Bahdanau et al\\.", "year": 2015}, {"title": "Learning phrase representations using rnn encoder-decoder for statistical machine translation", "author": ["Kyunghyun Cho", "Bart van Merrienboer", "Caglar Gulcehre", "Fethi Bougares", "Holger Schwenk", "Yoshua Bengio."], "venue": "EMNLP 2014.", "citeRegEx": "Cho et al\\.,? 2014", "shortCiteRegEx": "Cho et al\\.", "year": 2014}, {"title": "Empirical evaluation of gated recurrent neural networks on sequence modeling", "author": ["Junyoung Chung", "Caglar Gulcehre", "KyungHyun Cho", "Yoshua Bengio."], "venue": "arXiv.", "citeRegEx": "Chung et al\\.,? 2014", "shortCiteRegEx": "Chung et al\\.", "year": 2014}, {"title": "Clause restructuring for statistical machine translation", "author": ["M. Collins", "P. Koehn", "I. Ku\u010derov\u00e1."], "venue": "ACL 2005.", "citeRegEx": "Collins et al\\.,? 2005", "shortCiteRegEx": "Collins et al\\.", "year": 2005}, {"title": "Recurrent nets that time and count", "author": ["Felix A Gers", "J\u00fcrgen Schmidhuber."], "venue": "IJCNN 2000. IEEE.", "citeRegEx": "Gers and Schmidhuber.,? 2000", "shortCiteRegEx": "Gers and Schmidhuber.", "year": 2000}, {"title": "Long shortterm memory", "author": ["S. Hochreiter", "J. Schmidhuber."], "venue": "Neural Computation.", "citeRegEx": "Hochreiter and Schmidhuber.,? 1997", "shortCiteRegEx": "Hochreiter and Schmidhuber.", "year": 1997}, {"title": "On using very large target vocabulary for neural machine translation", "author": ["S\u00e9bastien Jean", "Kyunghyun Cho", "Roland Memisevic", "Yoshua Bengio."], "venue": "ACL 2015.", "citeRegEx": "Jean et al\\.,? 2015", "shortCiteRegEx": "Jean et al\\.", "year": 2015}, {"title": "Recurrent continuous translation models", "author": ["Nal Kalchbrenner", "Phil Blunsom."], "venue": "EMNLP 2013.", "citeRegEx": "Kalchbrenner and Blunsom.,? 2013", "shortCiteRegEx": "Kalchbrenner and Blunsom.", "year": 2013}, {"title": "Moses: open source toolkit for statistical machine translation", "author": ["Zens", "Chris Dyer", "Ondrej Bojar", "Alexandra Constantin", "Evan Herbst."], "venue": "ACL 2007.", "citeRegEx": "Zens et al\\.,? 2007", "shortCiteRegEx": "Zens et al\\.", "year": 2007}, {"title": "Effective approaches to attentionbased neural machine translation", "author": ["Minh-Thang Luong", "Hieu Pham", "Christopher D Manning."], "venue": "EMNLP 2015.", "citeRegEx": "Luong et al\\.,? 2015", "shortCiteRegEx": "Luong et al\\.", "year": 2015}, {"title": "A systematic comparison of various statistical alignment models", "author": ["Franz J. Och", "Hermann Ney."], "venue": "Computational Linguistics, 29(1):19\u201351.", "citeRegEx": "Och and Ney.,? 2003", "shortCiteRegEx": "Och and Ney.", "year": 2003}, {"title": "Bleu: a method for automatic evaluation of machine translation", "author": ["Kishore Papineni", "Salim Roukos", "Todd Ward", "WeiJing Zhu."], "venue": "ACL 2002.", "citeRegEx": "Papineni et al\\.,? 2002", "shortCiteRegEx": "Papineni et al\\.", "year": 2002}, {"title": "Fluency, adequacy, or hter?: exploring different human judgments with a tunable mt metric", "author": ["Matthew Snover", "Nitin Madnani", "Bonnie J Dorr", "Richard Schwartz."], "venue": "Proceedings of the Fourth Workshop on Statistical Machine Translation, pages 259\u2013268.", "citeRegEx": "Snover et al\\.,? 2009", "shortCiteRegEx": "Snover et al\\.", "year": 2009}, {"title": "Sequence to sequence learning with neural networks", "author": ["Ilya Sutskever", "Oriol Vinyals", "Quoc VV Le."], "venue": "NIPS 2014.", "citeRegEx": "Sutskever et al\\.,? 2014", "shortCiteRegEx": "Sutskever et al\\.", "year": 2014}, {"title": "Modeling Coverage for Neural Machine Translation", "author": ["Zhaopeng Tu", "Zhengdong Lu", "Yang Liu", "Xiaohua Liu", "Hang Li."], "venue": "ACL 2016.", "citeRegEx": "Tu et al\\.,? 2016", "shortCiteRegEx": "Tu et al\\.", "year": 2016}, {"title": "Show, Attend and Tell: Neural Image Caption Generation with Visual Attention", "author": ["Kelvin Xu", "Jimmy Ba", "Ryan Kiros", "Kyunghyun Cho", "Aaron Courville", "Ruslan Salakhutdinov", "Richard Zemel", "Yoshua Bengio."], "venue": "ICML 2015.", "citeRegEx": "Xu et al\\.,? 2015", "shortCiteRegEx": "Xu et al\\.", "year": 2015}], "referenceMentions": [{"referenceID": 7, "context": "Neural machine translation (NMT) (Kalchbrenner and Blunsom, 2013; Sutskever et al., 2014; Bahdanau et al., 2015) has made significant progress in the past several years, whose goal is to construct and utilize a single large neural network to accomplish the entire translation task.", "startOffset": 33, "endOffset": 112}, {"referenceID": 13, "context": "Neural machine translation (NMT) (Kalchbrenner and Blunsom, 2013; Sutskever et al., 2014; Bahdanau et al., 2015) has made significant progress in the past several years, whose goal is to construct and utilize a single large neural network to accomplish the entire translation task.", "startOffset": 33, "endOffset": 112}, {"referenceID": 0, "context": "Neural machine translation (NMT) (Kalchbrenner and Blunsom, 2013; Sutskever et al., 2014; Bahdanau et al., 2015) has made significant progress in the past several years, whose goal is to construct and utilize a single large neural network to accomplish the entire translation task.", "startOffset": 33, "endOffset": 112}, {"referenceID": 5, "context": "Several techniques in NMT are proved to be very effective, such as gating (Hochreiter and Schmidhuber, 1997; Cho et al., 2014) and attention (Bahdanau et al.", "startOffset": 74, "endOffset": 126}, {"referenceID": 1, "context": "Several techniques in NMT are proved to be very effective, such as gating (Hochreiter and Schmidhuber, 1997; Cho et al., 2014) and attention (Bahdanau et al.", "startOffset": 74, "endOffset": 126}, {"referenceID": 0, "context": ", 2014) and attention (Bahdanau et al., 2015) which can model long-distance dependencies and complicated alignment relations ar X iv :1 60 8.", "startOffset": 22, "endOffset": 45}, {"referenceID": 9, "context": "With the encoder-decoder framework as well as the gating and attention techniques, it is reported that the performance of NMT has surpassed the performance of traditional SMT in terms of BLEU score (Luong et al., 2015).", "startOffset": 198, "endOffset": 218}, {"referenceID": 0, "context": "Table 1 shows an example, in which an attentionbased NMT system (Bahdanau et al., 2015) generates a fluent yet inadequate translation (e.", "startOffset": 64, "endOffset": 87}, {"referenceID": 12, "context": "Fluency measures whether the translation is fluent, while adequacy measures whether the translation is faithful to the original sentence (Snover et al., 2009).", "startOffset": 137, "endOffset": 158}, {"referenceID": 0, "context": "3 BLEU points over a standard attention-based NMT system (Bahdanau et al., 2015).", "startOffset": 57, "endOffset": 80}, {"referenceID": 1, "context": "It can be either a vanilla RNN unit using tanh function, or a sophisticated gated RNN unit such as GRU (Cho et al., 2014) or LSTM (Hochreiter and Schmidhuber, 1997).", "startOffset": 103, "endOffset": 121}, {"referenceID": 5, "context": ", 2014) or LSTM (Hochreiter and Schmidhuber, 1997).", "startOffset": 16, "endOffset": 50}, {"referenceID": 1, "context": ", si \u2261 hJ ) (Cho et al., 2014; Sutskever et al., 2014), or a dynamic vector that selectively summarizes certain parts of the source sentence at each decoding step (e.", "startOffset": 12, "endOffset": 54}, {"referenceID": 13, "context": ", si \u2261 hJ ) (Cho et al., 2014; Sutskever et al., 2014), or a dynamic vector that selectively summarizes certain parts of the source sentence at each decoding step (e.", "startOffset": 12, "endOffset": 54}, {"referenceID": 0, "context": ", si = \u2211J j=1 \u03b1i,jhj in which \u03b1i,j is alignment probability calculated by an attention model) (Bahdanau et al., 2015).", "startOffset": 94, "endOffset": 117}, {"referenceID": 14, "context": "The recently proposed coverage based technique can address this problem (Tu et al., 2016).", "startOffset": 72, "endOffset": 89}, {"referenceID": 5, "context": "Inspired by the success of gated units in RNN (Hochreiter and Schmidhuber, 1997; Cho et al., 2014), we propose using a context gate to dynamically control the amount of information flowing from the source and target contexts and thus balance the fluency and adequacy of NMT at each decoding step.", "startOffset": 46, "endOffset": 98}, {"referenceID": 1, "context": "Inspired by the success of gated units in RNN (Hochreiter and Schmidhuber, 1997; Cho et al., 2014), we propose using a context gate to dynamically control the amount of information flowing from the source and target contexts and thus balance the fluency and adequacy of NMT at each decoding step.", "startOffset": 46, "endOffset": 98}, {"referenceID": 5, "context": "The first two strategies are inspired by the concept of output gate in LSTM (Hochreiter and Schmidhuber, 1997), which controls the amount of memory content utilized.", "startOffset": 76, "endOffset": 110}, {"referenceID": 15, "context": "Comparison to (Xu et al., 2015) \u2013 The context gate takes inspiration form the gating scalar model proposed by (Xu et al.", "startOffset": 14, "endOffset": 31}, {"referenceID": 15, "context": ", 2015) \u2013 The context gate takes inspiration form the gating scalar model proposed by (Xu et al., 2015) to perform the image caption generation task.", "startOffset": 86, "endOffset": 103}, {"referenceID": 15, "context": "1 Xu et al. (2015) uses a scalar that is shared by all elements in the source context, while we employ a gate that outputs a distinct weight for each element.", "startOffset": 2, "endOffset": 19}, {"referenceID": 4, "context": "It has been shown that peephole connections make precise timings easier to learn (Gers and Schmidhuber, 2000).", "startOffset": 81, "endOffset": 109}, {"referenceID": 15, "context": "Figure 5: Comparison to Gating Scalar proposed by (Xu et al., 2015).", "startOffset": 50, "endOffset": 67}, {"referenceID": 13, "context": "Comparison to Gated RNN \u2013 State-of-the-art NMT models (Sutskever et al., 2014; Bahdanau et al., 2015) generally employ a gated unit (e.", "startOffset": 54, "endOffset": 101}, {"referenceID": 0, "context": "Comparison to Gated RNN \u2013 State-of-the-art NMT models (Sutskever et al., 2014; Bahdanau et al., 2015) generally employ a gated unit (e.", "startOffset": 54, "endOffset": 101}, {"referenceID": 14, "context": "Comparison to Coverage Mechanism \u2013 Recently, Tu et al. (2016) propose adding a coverage mechanism into NMT to alleviate the overtranslation and under-translation problems, which directly affects the translation adequacy.", "startOffset": 45, "endOffset": 62}, {"referenceID": 11, "context": "We use the case-insensitive 4-gram NIST BLEU score (Papineni et al., 2002) as evaluation metric, and sign-test (Collins et al.", "startOffset": 51, "endOffset": 74}, {"referenceID": 3, "context": ", 2002) as evaluation metric, and sign-test (Collins et al., 2005) for statistical significance test.", "startOffset": 44, "endOffset": 66}, {"referenceID": 14, "context": "\u201cNMT-Coverage\u201d denotes attention-based NMT with a coverage mechanism to indicate whether a source word is translated or not (Tu et al., 2016).", "startOffset": 124, "endOffset": 141}, {"referenceID": 0, "context": "\u2022 NMT (Bahdanau et al., 2015): an attentionbased NMT model with default setting.", "startOffset": 6, "endOffset": 29}, {"referenceID": 14, "context": "\u2022 NMT-Coverage (Tu et al., 2016)6: an improved attention-based NMT model with a coverage mechanism.", "startOffset": 15, "endOffset": 32}, {"referenceID": 6, "context": ",, in (Jean et al., 2015)), but here we focus on the generic models.", "startOffset": 6, "endOffset": 25}, {"referenceID": 2, "context": ", tanh, Row 2), which is consistent with the results in other work (Chung et al., 2014).", "startOffset": 67, "endOffset": 87}, {"referenceID": 14, "context": "Over NMT-Coverage (GRU) We finally test on a stronger baseline, which employs a coverage mechanism to indicate whether a source word has already been translated or not (Tu et al., 2016).", "startOffset": 168, "endOffset": 185}, {"referenceID": 1, "context": "Please refer to (Cho et al., 2014) for more details.", "startOffset": 16, "endOffset": 34}, {"referenceID": 10, "context": "(2016), we use alignment error rate (AER) (Och and Ney, 2003) and its variant SAER to measure the alignment quality.", "startOffset": 42, "endOffset": 61}, {"referenceID": 13, "context": "Following Tu et al. (2016), we use alignment error rate (AER) (Och and Ney, 2003) and its variant SAER to measure the alignment quality.", "startOffset": 10, "endOffset": 27}, {"referenceID": 0, "context": "We follow Bahdanau et al. (2015) to group sentences of similar lengths together.", "startOffset": 10, "endOffset": 33}], "year": 2017, "abstractText": "In neural machine translation (NMT), generation of a target word depends on both source and target contexts. We find that source contexts have a direct impact on the adequacy of a translation while target contexts on the fluency. Intuitively, generation of a content word should rely more on the source context and generation of a functional word should rely more on the target context. Due to lack of effective control on the influence from source and target contexts, conventional NMT tends to yield fluent but inadequate translations. To address this problem, we propose to use context gates to dynamically control the ratios at which source and target contexts contribute to the generation of target words. In this way, we can enhance the adequacy of NMT while keeping the fluency unchanged. Experiments show that our approach significantly improves upon a standard attention-based NMT system by +2.3 BLEU points.", "creator": "LaTeX with hyperref package"}}}