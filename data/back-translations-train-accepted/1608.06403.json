{"id": "1608.06403", "review": {"conference": "NIPS", "VERSION": "v1", "DATE_OF_SUBMISSION": "23-Aug-2016", "title": "Phased Exploration with Greedy Exploitation in Stochastic Combinatorial Partial Monitoring Games", "abstract": "Partial monitoring games are repeated games where the learner receives feedback that might be different from adversary's move or even the reward gained by the learner. Recently, a general model of combinatorial partial monitoring (CPM) games was proposed \\cite{lincombinatorial2014}, where the learner's action space can be exponentially large and adversary samples its moves from a bounded, continuous space, according to a fixed distribution. The paper gave a confidence bound based algorithm (GCB) that achieves $O(T^{2/3}\\log T)$ distribution independent and $O(\\log T)$ distribution dependent regret bounds. The implementation of their algorithm depends on two separate offline oracles and the distribution dependent regret additionally requires existence of a unique optimal action for the learner. Adopting their CPM model, our first contribution is a Phased Exploration with Greedy Exploitation (PEGE) algorithmic framework for the problem. Different algorithms within the framework achieve $O(T^{2/3}\\sqrt{\\log T})$ distribution independent and $O(\\log^2 T)$ distribution dependent regret respectively. Crucially, our framework needs only the simpler \"argmax\" oracle from GCB and the distribution dependent regret does not require existence of a unique optimal action. Our second contribution is another algorithm, PEGE2, which combines gap estimation with a PEGE algorithm, to achieve an $O(\\log T)$ regret bound, matching the GCB guarantee but removing the dependence on size of the learner's action space. However, like GCB, PEGE2 requires access to both offline oracles and the existence of a unique optimal action. Finally, we discuss how our algorithm can be efficiently applied to a CPM problem of practical interest: namely, online ranking with feedback at the top.", "histories": [["v1", "Tue, 23 Aug 2016 07:14:18 GMT  (23kb)", "http://arxiv.org/abs/1608.06403v1", "Appearing in NIPS 2016"]], "COMMENTS": "Appearing in NIPS 2016", "reviews": [], "SUBJECTS": "cs.GT cs.AI", "authors": ["sougata chaudhuri", "ambuj tewari"], "accepted": true, "id": "1608.06403"}, "pdf": {"name": "1608.06403.pdf", "metadata": {"source": "CRF", "title": "Phased Exploration with Greedy Exploitation in Stochastic Combinatorial Partial Monitoring Games", "authors": ["Sougata Chaudhur", "Ambuj Tewari"], "emails": ["sougata@umich.edu", "tewaria@umich.edu"], "sections": [{"heading": null, "text": "ar Xiv: 160 8.06 403v 1 [cs.G T] 23 APartial monitoring games are repeat games where the learner gets feedback that may be exponentially large and adversary samples its moves from a bounded, continuous space, according to a fixed distribution. In the paper, a trust-based partial monitoring algorithm (GCB) was proposed, which achieves an O (T 2 / 3 logT) distribution independently and an O (log T) distribution dependent on regrets Bounds. Implementation of their algorithm depends on two separate offline oracles and distributional regret, in addition to the presence of a unique optimal action for the learner. Adapting their CPM model, our first contribution is a Phased Exploration with Greedy Exploitation (PEGE) algorithm to solve the problem. Different algorithms within the problem require a unique Optimal Action for the learner."}, {"heading": "1 Introduction", "text": "This year it is as far as it has ever been until the next round."}, {"heading": "2 Preliminaries and Assumptions", "text": "The online game is played between a learner and an opponent, indexed by t = 1, 2, over discrete rounds. The learner's action set is called X, which can be exponentially large. The opponent's action set is the infinite amount [0, 1] n. The opponent fixes a distribution p to [0, 1] n before the start of the game (the opponent's strategy), with p, which the learner is unaware of. In each round of the game, opposing examples (t) n become corresponding p, with the result that the distribution p [0, 1] n is set before the start of the game p. The learner selects x (t) and gets reward r (t). However, the learner may not know whether he wants either both (t) (as in a complete information game) or r (t), or r (t), which he (t) wants."}, {"heading": "3 Phased Exploration with Greedy Exploitation", "text": "Algorithm 1 (PEGE) uses the classic idea of exploration in phases that are progressively further apart from each other. Between the exploration phases, we greedily select measures by fully trusting the current estimates; the constant \u03b2 controls how much we explore in a given phase, and the constant \u03b1 together with the function C (\u00b7) determines how much we yield; this idea is a classic in bandit literature [9-11], but has not been applied to the best of our knowledge to the CPM framework. Algorithm 1 The PEGE algorithmic framework 1: Inputs: \u03b1, \u03b2 and function C (\u00b7) (for determining the degree of exploitation in each phase)."}, {"heading": "2: For b = 1, 2, . . . ,", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "3: Exploration", "text": "4: For i = 1 to 5: For j = 1 to b\u03b2 6: Let tj = 2: 1 and 1: 2: End for 9: End for 10: End for 10: End for 10: End for 10: End for 10: End for 10: End for 10: End for 10: End for 10: End for 10: End for 10: End for 10: End for 10: End for 10: End for 11: End for 11: End for 11: End for 10: End for 11: End for 11: End for 11: End for 11: End for 11: End for 11: End for 11: End for 11: End for 11: End for 11: End for 11: End for 11: End for 11: End for 11: End for 11: End for 11: End for 11: End for 11: End for 11: End for 11: End for 11: End for 11: End for 11: End for 11: End for 10: End for 10: End for 10: End for 10: End for 10: End for 10: End for 11: End for 10: End for 11: End for 11: End for 11: End for 11: End for 11: End for 11: End for 11: End for 11: End for 11: End for 11: End for 11: End for 11: End for 11: End for 11: End for 11: End for 11: End for 11: End for 11: End for 11: End for 11: End for 11: End for 11: End for 11: End for 11: End for 11: End for 11: End for 11: End for 11: End for 11: End for 11: End for 11: End for 11: End for 11: End for 11: End for 11: End for 11: End for 11: End for 11: End for 11: End for 11: End for 11: End for 11: End for 11: End for 11: End for 11: End for 11: End for 11: End for 11: End for 11: End for 11: End for 11: End for 11: End for 11: End for 11: End for 11: End for 11: End for 11: End for 11: End for 11: End for 11: End for 11: End for 11: End for 11: End for 11: End for 11: End for 11: End for 11: End for 11: End for 11:"}, {"heading": "4 Combining Gap Estimation with PEGE", "text": "Algorithm 2 tries to estimate the gap within a constant multiplication factor. However, if there is no definite optimal action or the actual gap is small, estimating the gap can take a lot of time. To prevent this, the algorithm also takes a threshold T0 as input and definitively stops when the threshold T0 is reached. The result below assures us that the algorithm is highly likely to behave as expected. That is, if there is a definite optimal action and the gap is large enough to be estimated with a given confidence before the threshold T0 enters into force, it gives an estimate in the range."}, {"heading": "12: If argmaxx\u2208X r\u0304(x, \u03b8\u0302(b)) is unique:", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "13: x\u0302(b) = argmaxx\u2208X r\u0304(x, \u03b8\u0302(b))", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "14: x\u0302\u2212(b) = argmaxx\u2208X ,x 6=x\u0302(b) r\u0304(x, \u03b8\u0302(b)) (need not be unique)", "text": "15: If r \"(x\" b \"), \u03b8\" b \") \u2212 r\" (x \"b\"), \u03b8 \"b\" (b) > 6w \"(b): 16: STOP and Output\" c \"(x\" b \") \u2212 r\" (b) \u2212 r \"(x\" b \"), \u03b8\" (b), \u03b8 \"(b) 17: End If 18: End If 19: STOP and Output\" Threshold exceeded \"21: End If 22: End For2.\" Assumption 4 holds and T0 \u2264 T1 (g). Then with probability at least 1 \u2212 \u03b4. \"The algorithm either outputs\" threshold exceeded \"or outputs\" threshold exceeded \"the 1 2: End For2.\" Furthermore, it must be the case that the algorithm is \"stopped\" at an episode b. < b \"Algorithm\" we. \""}, {"heading": "5 Comparison with GCB Algorithm", "text": "We offer a detailed comparison of our results with those for GCB [1]. (a) While we use the same CPM model, our work is inspired by forced exploration technology, while GCB is inspired by the trust-based technique, both of which are classic in bandit literature. (b) An instance of our PEGE framework gives an O (T 2 / 3 \u221a logT) distribution-independent regret (theorem 1) that does not require a call to the arg-secondmax oracle, which is of considerable practical advantage over GCB, since even for linear optimization problems over polyhedra standard routines usually have no option to calculate measures that achieve a second maximum value for the objective function. (c) Further instantiation of the PEGE framework gives an O (log2 T) distribution-dependent regret (theorem 2) that requires neither a specific call."}, {"heading": "6 Application to Online Ranking", "text": "The problem was investigated in a non-stochastic environment, i.e. it was assumed that a forgotten opponent generates reward vectors. Furthermore, the scope of action of the learner is exponentially large in the number of elements to be evaluated. The paper has established the connection of the problem to PM games (but not the combinatorial PM games) and an efficient algorithm for the specific problem is proposed at hand. However, a careful reading of the paper shows that their algorithmic techniques can deal with the CPM model that we have discussed so far, but in the non-stochastic setting of the reward function is linear in both learning and counter movements. The reward function is limited to a limited space of vectors and feedback is a linear transformation of adequate movement. In this section we give a brief description of the problem and show how efficiently the problem of online ranking can be solved with feedback."}, {"heading": "7 Appendix", "text": "We first specify the large deviation inequality for the vector-weighted nominal value, which is the generalization of Azuma-Hoeffding inequality for the vector-weighted nominal value = nominal value (nominal value) = nominal value (nominal value) = nominal value (nominal value) = nominal value (nominal value) = nominal value (nominal value) = nominal value (nominal value) = nominal value (nominal value) = nominal value (nominal value) = nominal value (nominal value) = nominal value (nominal value) = nominal value (nominal value) = nominal value (nominal value) = nominal value (nominal value) = nominal value (nominal value) = (nominal value) = (nominal value) = (nominal value) = (nominal value) = (nominal value) = (nominal value) = (nominal value) = (nominal value) = (nominal value)."}, {"heading": "7.1 Proof of Results in Section 3", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "7.1.1 Proof of Theorem 1", "text": "We reaffirm first the theoretical regret: If algorithm PEGE with the parameters C (a) = log a, \u03b1 = 2 and \u03b2 = 0, and the online game is played over T rounds, we get the following limit of the expected regret: R (T) \u2264 Rmax | T 2 / 3 + 2R\u03b2\u03c3T 2 / 3 270 + 2 logT + Rmax (13), where the constant according to Eq. 3.Proof. Let algorithm PEGE run forK phases, with parameters initialized as C (a) = log a, \u03b1 = 1 / 2 and \u03b2 = 0.Exploration repentance: During each exploration phase, the expected regret is limited by Rmax. | Rmax, where Rmax is given as in assumption 3. Thus, total expected repentance due to exploration is K | Rmax.Exploitation repentance: Leave x."}, {"heading": "7.1.2 Proof of Theorem 2", "text": "We repeat the theorem before we prove it: Distribution dependence remorse: When the PEGE algorithm is initialized with the parameters C (a) = h \u00b7 a, for a tuning parameter h > 0, \u03b1 = 1 and \u03b2 = 1, and the online game is played via T rounds, we get the following limit for the expected remorse: R (T) \u2264 x x x (logTh) 2 + 4 \u221a 2\u03c0e2R \u00b2 = h \u00b7 a, \u03b1 = 1 and \u03b2 = 1 in algorithm PEGE) 2. (18) Proof. Let us allow the total number of phases that the algorithm performs for K. We refer K to the total time T as (after substituting the parameters C (a) = h \u00b7 a, \u03b1 = 1 and \u03b2 = 1 in algorithm PEGE): T = K i = 1 | Proof."}, {"heading": "7.2 Proof of Results in Section 4", "text": "The following theorem refers to the version of PEGE that calls algorithm 3 line 8. It is required for the detection of theorem 4.Theorem 7. (Distribution Dependent Regret, version 2) If algorithm 1 is initialized with parameters C (a) = h \u00b7 a, for a tuning parameter 0 < h < \u2206 24R2\u03b22\u03c3, \u03b1 = 1 and \u03b2 = 0, and the online game is played over T rounds, we get the following limit for the expected regret: R (T) \u2264 x \u00b2 x \u00b2 x \u00b2 x \u00b2 x \u00b2 x \u00b2 x \u00b2 x \u00b2 x \u00b2 x \u00b2 x \u00b2 x \u00b2 \u00b2 \u00b2 x \u00b2 \u00b2 \u00b2 \u00b2 \u2212 h (20) Note: Compared to theorem 2, regret has a better dependence on T - O (log T) instead of O (log2 T) - but it also has a disadvantage. If the tuning parameter h is set incorrectly, say h \u00b2 \u00b2 solution \u00b2 once \u00b2 \u00b2 is selected, then the \u00b2 h \u00b2 is not bound."}, {"heading": "7.2.1 Proof of Theorem 3", "text": "Proof that adoption 1 to adoption 3 takes the form eLb > Mb = b = b = b = b and b = b = b (< b). (< b). (< b). (< b). (< b). (< b). (< b). (< b). (2e2b). (2e2b). (2e2b). (2e2b). (2e2b). (2e2b). (2e2b). (2b2). (2b). (2x.). (x). (x). (x). (x). (x). (x). (x). (x). (x). (x). (x). (x). (x). (x). (x). (x). (x)."}, {"heading": "7.2.2 Proof of Theorem 4", "text": "From Theorem 3 we know that with probability at least 1 \u2212 \u03b4, algorithm 2 outputs \"threshold exceeded\" in this case. Due to Eq. (22), however, we also have, for an optimal act x * (T0), a maximum act x * (x), a maximum act x (x), a maximum act x * (x), a maximum act x * (x), a maximum act x (x), a maximum (x), a maximum (x), a maximum (x), a maximum (x), a maximum (x), a maximum (x), a maximum (x), a maximum (x), a maximum (x), a maximum (x), a maximum (x), a maximum (x), a maximum (x), a maximum (x), a maximum (x), a maximum (x), a maximum (x), a maximum (x, a), a maximum (x, a), a maximum (x, a maximum (x), a maximum (x, a), a maximum (x, a), a maximum (x, a), a maximum (x, a), a maximum (x, a), a maximum (x, a), a maximum (x, a), a maximum (x, a (x, a), a maximum (x, a), a maximum (x, a), a (x, a (x, a), a maximum (x, a), a (x, a), a maximum (x, a (x, a), a maximum (x, a), a maximum (x, a), a (x, a (x, a), a (x, a (x, a), a (x, a (x, a), a (x, a), a (x, a (x, a), a (x, a (x, a), a (x, a (x, a), a (x, a (x, a), a (x, a), a (x, a), a (x, a (x, a), a (x, a (x, a (x, a), a (x, a), a (x, a (x, a)"}], "references": [{"title": "Combinatorial partial monitoring game with linear feedback and its applications", "author": ["Tian Lin", "Bruno Abrahao", "Robert Kleinberg", "John Lui", "Wei Chen"], "venue": "In Proceedings of the 31th International Conference on Machine Learning,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2014}, {"title": "Discrete prediction games with arbitrary feedback and loss", "author": ["Antonio Piccolboni", "Christian Schindelhauer"], "venue": "In COLT,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2001}, {"title": "Regret minimization under partial monitoring", "author": ["Nicolo Cesa-Bianchi", "G\u00e1bor Lugosi", "Gilles Stoltz"], "venue": "Mathematics of Operations Research,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2006}, {"title": "Partial monitoring\u2013classification, regret bounds, and algorithms", "author": ["Gabor Bartok"], "venue": "Mathematics of Operations Research,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2014}, {"title": "Regret lower bound and optimal algorithm in finite stochastic partial monitoring", "author": ["Junpei Komiyama", "Junya Honda", "Hiroshi Nakagawa"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2015}, {"title": "Finite-time analysis of the multiarmed bandit problem", "author": ["Peter Auer", "Nicolo Cesa-Bianchi", "Paul Fischer"], "venue": "Machine learning,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2002}, {"title": "Combinatorial multi-armed bandit: General framework and applications", "author": ["Wei Chen", "Yajun Wang", "Yang Yuan"], "venue": "In Proceedings of the 30th International Conference on Machine Learning,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2013}, {"title": "Tight regret bounds for stochastic combinatorial semi-bandits", "author": ["Branislav Kveton", "Zheng Wen", "Azin Ashkan", "Csaba Szepesvari"], "venue": "In Proceedings of the Eighteenth International Conference on Artificial Intelligence and Statistics,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2015}, {"title": "Some aspects of the sequential design of experiments", "author": ["Herbert Robbins"], "venue": "In Herbert Robbins Selected Papers,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 1985}, {"title": "Certainty equivalence control with forcing: revisited", "author": ["Rajeev Agrawal", "Demosthenis Teneketzis"], "venue": "Systems & control letters,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 1989}, {"title": "Linearly parameterized bandits", "author": ["Paat Rusmevichientong", "John N Tsitsiklis"], "venue": "Mathematics of Operations Research,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2010}, {"title": "Online ranking with top-1 feedback", "author": ["Sougata Chaudhuri", "Ambuj Tewari"], "venue": "In Proceedings of the 18th International Conference on Artificial Intelligence and Statistics,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2015}], "referenceMentions": [{"referenceID": 0, "context": "Recently, a general model of combinatorial partial monitoring (CPM) games was proposed [1], where the learner\u2019s action space can be exponentially large and adversary samples its moves from a bounded, continuous space, according to a fixed distribution.", "startOffset": 87, "endOffset": 90}, {"referenceID": 1, "context": "Starting with early work in the 2000s [2, 3], this body of research reached a culmination point with a comprehensive and complete classification of finite PM games [4].", "startOffset": 38, "endOffset": 44}, {"referenceID": 2, "context": "Starting with early work in the 2000s [2, 3], this body of research reached a culmination point with a comprehensive and complete classification of finite PM games [4].", "startOffset": 38, "endOffset": 44}, {"referenceID": 3, "context": "Starting with early work in the 2000s [2, 3], this body of research reached a culmination point with a comprehensive and complete classification of finite PM games [4].", "startOffset": 164, "endOffset": 167}, {"referenceID": 4, "context": "We refer the reader to these works for more references and also note that newer results continue to appear [5].", "startOffset": 107, "endOffset": 110}, {"referenceID": 3, "context": "The classification is governed by global and local observability properties pertaining to a game [4].", "startOffset": 97, "endOffset": 100}, {"referenceID": 5, "context": "Another line of work has extended traditional multi-armed bandit problem (MAB) [6] to include combinatorial action spaces for learner (CMAB) [7, 8].", "startOffset": 79, "endOffset": 82}, {"referenceID": 6, "context": "Another line of work has extended traditional multi-armed bandit problem (MAB) [6] to include combinatorial action spaces for learner (CMAB) [7, 8].", "startOffset": 141, "endOffset": 147}, {"referenceID": 7, "context": "Another line of work has extended traditional multi-armed bandit problem (MAB) [6] to include combinatorial action spaces for learner (CMAB) [7, 8].", "startOffset": 141, "endOffset": 147}, {"referenceID": 0, "context": "While finite PM and CMAB problems have witnessed a lot of activity, there is only one paper [1] on combinatorial partial monitoring (CPM) games, to the best of our knowledge.", "startOffset": 92, "endOffset": 95}, {"referenceID": 5, "context": "Inspired by the classic confidence bound algorithms for MABs, such as UCB [6], the authors proposed a Global Confidence Bound (GCB) algorithm that enjoyed two types of regret bound.", "startOffset": 74, "endOffset": 77}, {"referenceID": 0, "context": "We adopt the CPM model proposed earlier [1].", "startOffset": 40, "endOffset": 43}, {"referenceID": 8, "context": "This technique was already used in the classic paper of Robbins [9] and has also been called \u201cforcing with certainty equivalence\u201d in the control theory literature [10].", "startOffset": 64, "endOffset": 67}, {"referenceID": 9, "context": "This technique was already used in the classic paper of Robbins [9] and has also been called \u201cforcing with certainty equivalence\u201d in the control theory literature [10].", "startOffset": 163, "endOffset": 167}, {"referenceID": 10, "context": "We develop a Phased Exploration with Greedy Exploitation (PEGE) algorithmic framework (Section 3) borrowing the PEGE terminology from work on linearly parameterized bandits [11].", "startOffset": 173, "endOffset": 177}, {"referenceID": 0, "context": "Thus, though we have adopted the CPM model [1], our regret bounds are meaningful for countably infinite or even continuous learner\u2019s action space, whereas GCB regret bound has an explicit logarithmic dependence on |X |.", "startOffset": 43, "endOffset": 46}, {"referenceID": 11, "context": "Finally, we discuss how our algorithms can be efficiently applied in the CPM problem of online ranking with feedback restricted to top ranked items (Section 6), a setting already considered [12] but analyzed in a non-stochastic setting.", "startOffset": 190, "endOffset": 194}, {"referenceID": 0, "context": "The adversary\u2019s action set is the infinite set [0, 1].", "startOffset": 47, "endOffset": 53}, {"referenceID": 0, "context": "The adversary fixes a distribution p on [0, 1] before start of the game (adversary\u2019s strategy), with p unknown to the learner.", "startOffset": 40, "endOffset": 46}, {"referenceID": 0, "context": "At each round of the game, adversary samples \u03b8(t) \u2208 [0, 1] according to p, with E\u03b8(t)\u223cp[\u03b8(t)] = \u03b8 p .", "startOffset": 52, "endOffset": 58}, {"referenceID": 0, "context": "That is, the expected reward is a function of x and \u03b8 p , which is always satisfied if r(x, \u03b8) is a linear function of \u03b8, or if distribution p happens to be any distribution with support [0, 1] and fully parameterized by its mean \u03b8 p .", "startOffset": 187, "endOffset": 193}, {"referenceID": 1, "context": "Then, with non-zero probability, the learner can suffer \u03a9(T ) regret and no learner strategy can guarantee a sub-linear in T regret (the intuition forms the base of the global observability condition in [2]).", "startOffset": 203, "endOffset": 206}, {"referenceID": 0, "context": "The continuity assumption, along with the fact that adversary\u2019s moves are in [0, 1], implies boundedness of expected reward for any learner\u2019s action and any adversary\u2019s action.", "startOffset": 77, "endOffset": 83}, {"referenceID": 0, "context": "We denote Rmax = maxx\u2208X ,\u03b8\u2208[0,1]n r\u0304(x, \u03b8).", "startOffset": 27, "endOffset": 32}, {"referenceID": 8, "context": "This idea is classic in the bandit literature [9\u201311] but has not been applied to the CPM framework to the best of our knowledge.", "startOffset": 46, "endOffset": 52}, {"referenceID": 9, "context": "This idea is classic in the bandit literature [9\u201311] but has not been applied to the CPM framework to the best of our knowledge.", "startOffset": 46, "endOffset": 52}, {"referenceID": 10, "context": "This idea is classic in the bandit literature [9\u201311] but has not been applied to the CPM framework to the best of our knowledge.", "startOffset": 46, "endOffset": 52}, {"referenceID": 9, "context": ", earlier bounds [10] are asymptotic) whereas here we prove it in the CPM setting.", "startOffset": 17, "endOffset": 21}, {"referenceID": 3, "context": "For PM games that are globally observable but not locally observable, such a distribution independent O(T ) bound is known to be optimal [4].", "startOffset": 137, "endOffset": 140}, {"referenceID": 0, "context": "We provide a detailed comparison of our results with those obtained for GCB [1].", "startOffset": 76, "endOffset": 79}, {"referenceID": 0, "context": "Thus, though we have adopted the CPM model [1], our algorithms achieve meaningful regret bounds for countably infinite or even continuous learner\u2019s action space.", "startOffset": 43, "endOffset": 46}, {"referenceID": 11, "context": "A recent paper studied an interesting problem of online ranking with feedback restricted to top ranked items [12].", "startOffset": 109, "endOffset": 113}, {"referenceID": 0, "context": "Adversary\u2019s move is an n dimensional relevance vector, and thus, is restricted to {0, 1}n (finite space of size 2) contained in [0, 1].", "startOffset": 128, "endOffset": 134}, {"referenceID": 0, "context": "Assumption 1 is satisfied because the reward function is linear in \u03b8 and r\u0304(x, \u03b8 p) = f(x) \u00b7 \u03b8 p , where Ep[\u03b8] = \u03b8 \u2217 p \u2208 [0, 1].", "startOffset": 121, "endOffset": 127}, {"referenceID": 0, "context": "The learner now ranks the items by producing an n dimensional score vector x \u2208 [0, 1] and sorting items according to their scores.", "startOffset": 79, "endOffset": 85}, {"referenceID": 0, "context": "References [1] Tian Lin, Bruno Abrahao, Robert Kleinberg, John Lui, and Wei Chen.", "startOffset": 11, "endOffset": 14}, {"referenceID": 1, "context": "[2] Antonio Piccolboni and Christian Schindelhauer.", "startOffset": 0, "endOffset": 3}, {"referenceID": 2, "context": "[3] Nicolo Cesa-Bianchi, G\u00e1bor Lugosi, and Gilles Stoltz.", "startOffset": 0, "endOffset": 3}, {"referenceID": 3, "context": "[4] Gabor Bartok et al.", "startOffset": 0, "endOffset": 3}, {"referenceID": 4, "context": "[5] Junpei Komiyama, Junya Honda, and Hiroshi Nakagawa.", "startOffset": 0, "endOffset": 3}, {"referenceID": 5, "context": "[6] Peter Auer, Nicolo Cesa-Bianchi, and Paul Fischer.", "startOffset": 0, "endOffset": 3}, {"referenceID": 6, "context": "[7] Wei Chen, Yajun Wang, and Yang Yuan.", "startOffset": 0, "endOffset": 3}, {"referenceID": 7, "context": "[8] Branislav Kveton, Zheng Wen, Azin Ashkan, and Csaba Szepesvari.", "startOffset": 0, "endOffset": 3}, {"referenceID": 8, "context": "[9] Herbert Robbins.", "startOffset": 0, "endOffset": 3}, {"referenceID": 9, "context": "[10] Rajeev Agrawal and Demosthenis Teneketzis.", "startOffset": 0, "endOffset": 4}, {"referenceID": 10, "context": "[11] Paat Rusmevichientong and John N Tsitsiklis.", "startOffset": 0, "endOffset": 4}, {"referenceID": 11, "context": "[12] Sougata Chaudhuri and Ambuj Tewari.", "startOffset": 0, "endOffset": 4}], "year": 2016, "abstractText": "Partial monitoring games are repeated games where the learner receives feedback that might be different from adversary\u2019s move or even the reward gained by the learner. Recently, a general model of combinatorial partial monitoring (CPM) games was proposed [1], where the learner\u2019s action space can be exponentially large and adversary samples its moves from a bounded, continuous space, according to a fixed distribution. The paper gave a confidence bound based algorithm (GCB) that achieves O(T 2/3 logT ) distribution independent and O(log T ) distribution dependent regret bounds. The implementation of their algorithm depends on two separate offline oracles and the distribution dependent regret additionally requires existence of a unique optimal action for the learner. Adopting their CPM model, our first contribution is a Phased Exploration with Greedy Exploitation (PEGE) algorithmic framework for the problem. Different algorithms within the framework achieve O(T 2/3 \u221a logT ) distribution independent and O(log T ) distribution dependent regret respectively. Crucially, our framework needs only the simpler \u201cargmax\u201d oracle from GCB and the distribution dependent regret does not require existence of a unique optimal action. Our second contribution is another algorithm, PEGE2, which combines gap estimation with a PEGE algorithm, to achieve an O(log T ) regret bound, matching the GCB guarantee but removing the dependence on size of the learner\u2019s action space. However, like GCB, PEGE2 requires access to both offline oracles and the existence of a unique optimal action. Finally, we discuss how our algorithm can be efficiently applied to a CPM problem of practical interest: namely, online ranking with feedback at the top.", "creator": "LaTeX with hyperref package"}}}