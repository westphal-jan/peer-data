{"id": "1608.06794", "review": {"conference": "EMNLP", "VERSION": "v1", "DATE_OF_SUBMISSION": "24-Aug-2016", "title": "Improving Sparse Word Representations with Distributional Inference for Semantic Composition", "abstract": "Distributional models are derived from co-occurrences in a corpus, where only a small proportion of all possible plausible co-occurrences will be observed. This results in a very sparse vector space, requiring a mechanism for inferring missing knowledge. Most methods face this challenge in ways that render the resulting word representations uninterpretable, with the consequence that semantic composition becomes hard to model. In this paper we explore an alternative which involves explicitly inferring unobserved co-occurrences using the distributional neighbourhood. We show that distributional inference improves sparse word representations on several word similarity benchmarks and demonstrate that our model is competitive with the state-of-the-art for adjective-noun, noun-noun and verb-object compositions while being fully interpretable.", "histories": [["v1", "Wed, 24 Aug 2016 12:38:45 GMT  (53kb,D)", "http://arxiv.org/abs/1608.06794v1", "To appear at EMNLP 2016"]], "COMMENTS": "To appear at EMNLP 2016", "reviews": [], "SUBJECTS": "cs.CL", "authors": ["thomas kober", "julie weeds", "jeremy reffin", "david j weir"], "accepted": true, "id": "1608.06794"}, "pdf": {"name": "1608.06794.pdf", "metadata": {"source": "CRF", "title": "Improving Sparse Word Representations with Distributional Inference for Semantic Composition", "authors": ["Thomas Kober", "Julie Weeds", "Jeremy Reffin", "David Weir"], "emails": ["d.j.weir}@sussex.ac.uk"], "sections": [{"heading": "1 Introduction", "text": "In fact, the fact is that most of them will be able to feel how they are, able to behave."}, {"heading": "2 Related Work", "text": "Our method follows the distribution smoothing approach of Dagan et al. (1994) and Dagan et al. (1997). In this paper, the authors address the smoothing of the probability estimate for invisible words in bigrams. This is achieved by measuring which unobserved bigrams are more likely than others based on the Kullback-Leibler divergence between bigrams distributions. This has resulted in significantly improved performance in language modelling for speech recognition tasks, as well as word-sense disambiguity in machine translation (Dagan et al., 1994; Dagan et al., 1997). More recently, they have used a distribution approach for smoothing derivatively related words, such as oldish - old, as a back-off strategy in the case of data sparseness. However, none of these approaches have used distributional inferences and distribution models."}, {"heading": "3 Background", "text": "In fact, the majority of people who are able to feel able to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to, to move, to move, to, to, to move, to, to"}, {"heading": "Composition with APTs", "text": "In fact, most of them are able to play by the rules that they need for their work, and they are able to play by the rules that they need for their work."}, {"heading": "4 Distributional Inference", "text": "However, following Dagan et al. (1994) and Dagan et al. (1997), we propose a simple, unsupervised algorithm to enrich sparse vector representations with their nearest neighbors. We show that our distribution algorithm improves performance for untyped and typed models on several comparative scales for word similarities and competes with the state of the art in semantic composition. As shown in the following algorithm 1, we iterate over all word vectors w in a given distribution model M and add the vector representations of the nearest neighbors n, determined by cosmic similarity, to represent the enriched word vector w. The parameter \u03b1 in line 4 scales the contribution of the original word vector w in a given distribution model M and adds the vector representations of the nearest neighbors n: In this work, we have always decided that the model is identical to the number of neighbors used to influence the distribution."}, {"heading": "Static Top n Neighbour Retrieval", "text": "Perhaps the easiest way is to select the uppermost n most similar neighbors for each word in the vector space and enrich the corresponding vector representations with it."}, {"heading": "Density based Neighbour Retrieval", "text": "This approach is rooted in the estimation of nuclear density (Parzen, 1962), but instead of defining a static global Parzen window, we specify the window size for each word individually, depending on the distance to its nearest neighbor plus a threshold. For example, if the cosinal distance between the target vector and its uppermost neighbor is 0.5, we use a window size of 0.5 + for that word. In our experiments, we typically define proportional to the distance to the nearest neighbor (e.g. = 0.5 x 0.1)."}, {"heading": "WordNet based Neighbour Retrieval", "text": "Instead of using the intrinsic structure of our distribution vector space, we retrieve neighbors by querying WordNet (Fellbaum, 1998) and treat synsets with agreed PoS tags as the closest neighbors of any target vector. This restricts the retrieved neighbors to synonyms only."}, {"heading": "5 Experiments", "text": "Our model is based on a Wikipedia dump cleaned up in October 2013 that excludes all pages with less than 20 page views, resulting in a corpus of about 0.6 billion tokens (Wilson, 2015).The corpus is downsized, tokenized, lemmatized, PoS tagged, and analyzed with the Stanford NLP tools using universal dependencies (Manning et al., 2014; de Marneffe et al., 2014).We then build our APT model with first-, second-, and third-order relationships. We remove distributional features with a number of less than 10 and vectors with less than 50 non-zero entries, and then convert the raw counts into PPMI weights. The untyped vector space model is based on the same reduced, linked, and lemmatized Wikipedia corpus. We discard terms with a frequency of less than 50, and apply PPMI to the coherent codes."}, {"heading": "Shifted PPMI", "text": "We are investigating a number of different values for displacement of the PPMI values, as these have significant effects on the performance of the APT model. Therefore, the effect of displacement of the PPMI values for untyped vector space models has already been investigated in Levy and Goldberg (2014) and Levy et al. (2015), so we present only results for the APT model. As shown in Eq.1, PMI is defined as the protocol of the ratio of the common probability of observing a word w and a context c together, and as the product of the respective limits of their separate observation. In our APT model, a context c is defined as a dependency relationship together with a word.PMI (w, c) = Protocol P (w, c) P (w) P (w) P (c) SPPMI (w, c) = max (PMI (w, c) \u2212 log k, 0) (1) Since PMI is negative unlimited, PPMI is used to ensure that all values are greater or equal than the value threshold of the PMI (before the PMI and the PMI shift)."}, {"heading": "5.1 Word Similarity Experiments", "text": "This year, it has reached the point where it will be able to retaliate until it is able to retaliate."}, {"heading": "5.2 Composition Experiments", "text": "For this experiment, we use the dimensions of our vector-space models to be meaningful and interpretable. However, the problem of missing information is amplified in the compositional settings, since many compatible dimensions between words are not observed in the source corpus. Therefore, it is crucial that the distributive analysis is able to inject some of the missing information in order to improve the effect of the distributed information on the compositional process. For the experiments that comprise the semantic composition, we have the elementary representations of the phrase before composition. In these experiments, we first perform a qualitative analysis for our APT model and observe the effect of the distributed information on the closest neighbors of the composed compositions of the composed composition, noun-noun-noun-objective composition. In these experiments, we show how the neighborhood changes, in which composed phrases are embedded in concrete phrases, underline the composition and the composition between them."}, {"heading": "6 Conclusion and Future Work", "text": "One of the biggest challenges in numerical-based models is dealing with extreme scarcity and lack of information. This work contributes to a number of results related to this challenge, in particular a simple, unattended algorithm for enriching sparse word representations by using their distributional neighborliness. We have shown that our APT model competes with typed and untyped vector space models on a number of word similarity datasets. We have shown that distributional inference improves the performance of typed and untyped VSMs for semantic composition and that our APT model competes with the state of the art for adjective-noun, noun-noun and verb-object compositions while they are fully interpretable. With our method, we are able to bridge the gap in performance between low-dimensional, non-interpretable and high-dimensional interpretation tasks. Finally, we have shown the different behavior of compositional compositional formation by examining the completeness of the compositional structure for a full-scale interpretation, which we need to study."}, {"heading": "Acknowledgments", "text": "This work was funded by the UK EPSRC project EP / IO37458 / 1 \"A Unified Model of Compositional and Distributional Compositional Semantics: Theory and Applications.\" We would like to thank Miroslav Batchkarov for valuable discussions on earlier drafts of this work and our anonymous reviewers for their helpful comments."}], "references": [{"title": "A study on similarity and relatedness using distributional and wordnet-based approaches", "author": ["Eneko Agirre", "Enrique Alfonseca", "Keith Hall", "Jana Kravalova", "Marius Pasca", "Aitor Soroa."], "venue": "Proceedings of NAACL-HLT, pages 19\u201327, Boulder, Colorado, June.", "citeRegEx": "Agirre et al\\.,? 2009", "shortCiteRegEx": "Agirre et al\\.", "year": 2009}, {"title": "Distributional memory: A general framework for corpus-based semantics", "author": ["Marco Baroni", "Alessandro Lenci."], "venue": "Computational Linguistics, 36(4):673\u2013 721, December.", "citeRegEx": "Baroni and Lenci.,? 2010", "shortCiteRegEx": "Baroni and Lenci.", "year": 2010}, {"title": "How we blessed distributional semantic evaluation", "author": ["Marco Baroni", "Alessandro Lenci."], "venue": "Proceedings of GEMS Workshop, GEMS \u201911, pages 1\u201310, Stroudsburg, PA, USA. Association for Computational Linguistics.", "citeRegEx": "Baroni and Lenci.,? 2011", "shortCiteRegEx": "Baroni and Lenci.", "year": 2011}, {"title": "Nouns are vectors, adjectives are matrices: Representing adjective-noun constructions in semantic space", "author": ["Marco Baroni", "Roberto Zamparelli."], "venue": "Proceedings of EMNLP, pages 1183\u20131193, Cambridge, MA, October. Association for Computational", "citeRegEx": "Baroni and Zamparelli.,? 2010", "shortCiteRegEx": "Baroni and Zamparelli.", "year": 2010}, {"title": "A comparison of vector-based representations for semantic composition", "author": ["William Blacoe", "Mirella Lapata."], "venue": "Proceedings of EMNLP, pages 546\u2013 556, Jeju Island, Korea, July. Association for Computational Linguistics.", "citeRegEx": "Blacoe and Lapata.,? 2012", "shortCiteRegEx": "Blacoe and Lapata.", "year": 2012}, {"title": "Multimodal distributional semantics", "author": ["Elia Bruni", "Nam Khanh Tran", "Marco Baroni."], "venue": "J. Artif. Int. Res., 49(1):1\u201347, January.", "citeRegEx": "Bruni et al\\.,? 2014", "shortCiteRegEx": "Bruni et al\\.", "year": 2014}, {"title": "Extracting semantic representations from word co-occurrence statistics: A computational study", "author": ["John A. Bullinaria", "Joseph P. Levy."], "venue": "Behavior Research Methods, pages 510\u2013526.", "citeRegEx": "Bullinaria and Levy.,? 2007", "shortCiteRegEx": "Bullinaria and Levy.", "year": 2007}, {"title": "Extracting semantic representations from word co-occurrence statistics: stop-lists, stemming, and svd", "author": ["John A. Bullinaria", "Joseph P. Levy."], "venue": "Behavior Research Methods, 44(3):890\u2013907.", "citeRegEx": "Bullinaria and Levy.,? 2012", "shortCiteRegEx": "Bullinaria and Levy.", "year": 2012}, {"title": "Word association norms, mutual information, and lexicography", "author": ["Kenneth Ward Church", "Patrick Hanks."], "venue": "Computational Linguistics, 16(1):22\u201329, March.", "citeRegEx": "Church and Hanks.,? 1990", "shortCiteRegEx": "Church and Hanks.", "year": 1990}, {"title": "Mathematical foundations for a compositional distributional model of meaning", "author": ["Bob Coecke", "Mehrnoosh Sadrzadeh", "Stephen Clark."], "venue": "CoRR, abs/1003.4394.", "citeRegEx": "Coecke et al\\.,? 2010", "shortCiteRegEx": "Coecke et al\\.", "year": 2010}, {"title": "From Distributional to Semantic Similarity", "author": ["James Curran."], "venue": "Ph.D. thesis, University of Edinburgh.", "citeRegEx": "Curran.,? 2004", "shortCiteRegEx": "Curran.", "year": 2004}, {"title": "Similarity-based estimation of word cooccurrence probabilities", "author": ["Ido Dagan", "Fernando Pereira", "Lillian Lee."], "venue": "Proceedings of ACL, pages 272\u2013278, Las Cruces, New Mexico, USA, June. Association for Computational Linguistics.", "citeRegEx": "Dagan et al\\.,? 1994", "shortCiteRegEx": "Dagan et al\\.", "year": 1994}, {"title": "Similarity-based methods for word sense disambiguation", "author": ["Ido Dagan", "Lillian Lee", "Fernando Pereira."], "venue": "Proceedings of ACL, pages 56\u201363, Madrid, Spain, July. Association for Computational Linguistics.", "citeRegEx": "Dagan et al\\.,? 1997", "shortCiteRegEx": "Dagan et al\\.", "year": 1997}, {"title": "Universal stanford dependencies: A cross-linguistic typology", "author": ["Marie-Catherine de Marneffe", "Timothy Dozat", "Natalia Silveira", "Katri Haverinen", "Filip Ginter", "Joakim Nivre", "Christopher D. Manning."], "venue": "Proceedings of LREC, pages 4585\u20134592, Reykjavik,", "citeRegEx": "Marneffe et al\\.,? 2014", "shortCiteRegEx": "Marneffe et al\\.", "year": 2014}, {"title": "Indexing by latent semantic analysis", "author": ["Scott Deerwester", "Susan T. Dumais", "George W. Furnas", "Thomas K. Landauer", "Richard Harshman."], "venue": "J. Amer. Soc. Inf. Sci., 41(6):391\u2013407.", "citeRegEx": "Deerwester et al\\.,? 1990", "shortCiteRegEx": "Deerwester et al\\.", "year": 1990}, {"title": "A structured vector space model for word meaning in context", "author": ["Katrin Erk", "Sebastian Pad\u00f3."], "venue": "Proceedings of EMNLP, pages 897\u2013906, Honolulu, Hawaii, October. Association for Computational Linguistics.", "citeRegEx": "Erk and Pad\u00f3.,? 2008", "shortCiteRegEx": "Erk and Pad\u00f3.", "year": 2008}, {"title": "WordNet: an electronic lexical database", "author": ["Christiane Fellbaum", "editor"], "venue": null, "citeRegEx": "Fellbaum and editor.,? \\Q1998\\E", "shortCiteRegEx": "Fellbaum and editor.", "year": 1998}, {"title": "Placing search in context: The concept revisited", "author": ["Lev Finkelstein", "Evgeniy Gabrilovich", "Yossi Matias", "Ehud Rivlin", "Zach Solan", "Gadi Wolfman", "Eytan Ruppin."], "venue": "Proceedings of WWW, WWW \u201901, pages 406\u2013414, New York, NY, USA. ACM.", "citeRegEx": "Finkelstein et al\\.,? 2001", "shortCiteRegEx": "Finkelstein et al\\.", "year": 2001}, {"title": "Die Grundlagen der Arithmetik: Eine logisch mathematische Untersuchung \u00fcber den Begriff der Zahl", "author": ["Gottlob Frege."], "venue": "W. Koebner.", "citeRegEx": "Frege.,? 1884", "shortCiteRegEx": "Frege.", "year": 1884}, {"title": "Multi-step regression learning for compositional distributional semantics", "author": ["Edward Grefenstette", "Georgiana Dinu", "Yao-Zhong Zhang", "Mehrnoosh Sadrzadeh", "Marco Baroni."], "venue": "Proceedings of IWCS.", "citeRegEx": "Grefenstette et al\\.,? 2013", "shortCiteRegEx": "Grefenstette et al\\.", "year": 2013}, {"title": "Explorations in Automatic Thesaurus Discovery", "author": ["Gregory Grefenstette."], "venue": "Kluwer Academic Publishers, Norwell, MA, USA.", "citeRegEx": "Grefenstette.,? 1994", "shortCiteRegEx": "Grefenstette.", "year": 1994}, {"title": "Jointly learning word representations and composition functions using predicate-argument structures", "author": ["Kazuma Hashimoto", "Pontus Stenetorp", "Makoto Miwa", "Yoshimasa Tsuruoka."], "venue": "Proceedings of", "citeRegEx": "Hashimoto et al\\.,? 2014", "shortCiteRegEx": "Hashimoto et al\\.", "year": 2014}, {"title": "Simlex-999: Evaluating semantic models with (genuine) similarity estimation", "author": ["Felix Hill", "Roi Reichart", "Anna Korhonen."], "venue": "Computational Linguistics, 41(4):665\u2013695, December.", "citeRegEx": "Hill et al\\.,? 2015", "shortCiteRegEx": "Hill et al\\.", "year": 2015}, {"title": "A large scale evaluation of distributional semantic models: Parameters, interactions and model selection", "author": ["Gabriella Lapesa", "Stefan Evert."], "venue": "TACL, 2:531\u2013 545.", "citeRegEx": "Lapesa and Evert.,? 2014", "shortCiteRegEx": "Lapesa and Evert.", "year": 2014}, {"title": "The forest convolutional network: Compositional distributional semantics with a neural chart and without binarization", "author": ["Phong Le", "Willem Zuidema."], "venue": "Proceedings of EMNLP, pages 1155\u20131164, Lisbon, Portugal, September. Association for Computational", "citeRegEx": "Le and Zuidema.,? 2015", "shortCiteRegEx": "Le and Zuidema.", "year": 2015}, {"title": "Neural word embedding as implicit matrix factorization", "author": ["Omer Levy", "Yoav Goldberg."], "venue": "Proceedings of NIPS, pages 2177\u20132185.", "citeRegEx": "Levy and Goldberg.,? 2014", "shortCiteRegEx": "Levy and Goldberg.", "year": 2014}, {"title": "Improving distributional similarity with lessons learned from word embeddings", "author": ["Omer Levy", "Yoav Goldberg", "Ido Dagan."], "venue": "TACL, 3:211\u2013225.", "citeRegEx": "Levy et al\\.,? 2015", "shortCiteRegEx": "Levy et al\\.", "year": 2015}, {"title": "Automatic retrieval and clustering of similar words", "author": ["Dekang Lin."], "venue": "Proceedings of ACL, pages 768\u2013 774, Montreal, Quebec, Canada, August. Association for Computational Linguistics.", "citeRegEx": "Lin.,? 1998", "shortCiteRegEx": "Lin.", "year": 1998}, {"title": "Producing high-dimensional semantic spaces from lexical cooccurrence", "author": ["Kevin Lund", "Curt Burgess."], "venue": "Behavior Research Methods, Instruments, & Computers, 28(2):203\u2013208.", "citeRegEx": "Lund and Burgess.,? 1996", "shortCiteRegEx": "Lund and Burgess.", "year": 1996}, {"title": "The Stanford CoreNLP natural language processing toolkit", "author": ["Christopher D. Manning", "Mihai Surdeanu", "John Bauer", "Jenny Finkel", "Steven J. Bethard", "David McClosky."], "venue": "Proceedings of ACL - System Demonstrations, pages 55\u201360.", "citeRegEx": "Manning et al\\.,? 2014", "shortCiteRegEx": "Manning et al\\.", "year": 2014}, {"title": "Distributed representations of words and phrases and their compositionality", "author": ["Tomas Mikolov", "Ilya Sutskever", "Kai Chen", "Greg S Corrado", "Jeff Dean."], "venue": "Proceedings of NIPS, pages 3111\u20133119.", "citeRegEx": "Mikolov et al\\.,? 2013", "shortCiteRegEx": "Mikolov et al\\.", "year": 2013}, {"title": "Vector-based models of semantic composition", "author": ["Jeff Mitchell", "Mirella Lapata."], "venue": "In Proceedings of ACL-08: HLT, pages 236\u2013244.", "citeRegEx": "Mitchell and Lapata.,? 2008", "shortCiteRegEx": "Mitchell and Lapata.", "year": 2008}, {"title": "Composition in distributional models of semantics", "author": ["Jeff Mitchell", "Mirella Lapata."], "venue": "Cognitive Science, 34(8):1388\u20131429.", "citeRegEx": "Mitchell and Lapata.,? 2010", "shortCiteRegEx": "Mitchell and Lapata.", "year": 2010}, {"title": "Discriminative neural sentence modeling by tree-based convolution", "author": ["Lili Mou", "Hao Peng", "Ge Li", "Yan Xu", "Lu Zhang", "Zhi Jin."], "venue": "Proceedings of EMNLP, pages 2315\u20132325, Lisbon, Portugal, September. Association for Computational Linguistics.", "citeRegEx": "Mou et al\\.,? 2015", "shortCiteRegEx": "Mou et al\\.", "year": 2015}, {"title": "Co-occurrence vectors from corpora vs", "author": ["Yoshiki Niwa", "Yoshihiko Nitta."], "venue": "distance vectors from dictionaries. In Proceedings of Coling, COLING \u201994,", "citeRegEx": "Niwa and Nitta.,? 1994", "shortCiteRegEx": "Niwa and Nitta.", "year": 1994}, {"title": "Dependencybased construction of semantic space models", "author": ["Sebastian Pad\u00f3", "Mirella Lapata."], "venue": "Computational Linguistics, 33(2):161\u2013199.", "citeRegEx": "Pad\u00f3 and Lapata.,? 2007", "shortCiteRegEx": "Pad\u00f3 and Lapata.", "year": 2007}, {"title": "Derivational smoothing for syntactic distributional semantics", "author": ["Sebastian Pad\u00f3", "Jan \u0160najder", "Britta Zeller."], "venue": "Proceedings of ACL, pages 731\u2013735, Sofia, Bulgaria, August. Association for Computational Linguistics.", "citeRegEx": "Pad\u00f3 et al\\.,? 2013", "shortCiteRegEx": "Pad\u00f3 et al\\.", "year": 2013}, {"title": "A practical and linguistically-motivated approach to compositional distributional semantics", "author": ["Denis Paperno", "Nghia The Pham", "Marco Baroni."], "venue": "Proceedings of ACL, pages 90\u201399, Baltimore, Maryland, June. Association for Computational Linguistics.", "citeRegEx": "Paperno et al\\.,? 2014", "shortCiteRegEx": "Paperno et al\\.", "year": 2014}, {"title": "On estimation of a probability density function and mode", "author": ["Emanuel Parzen."], "venue": "Ann. Math. Statist., 33(3):1065\u20131076, 09.", "citeRegEx": "Parzen.,? 1962", "shortCiteRegEx": "Parzen.", "year": 1962}, {"title": "Glove: Global vectors for word representation", "author": ["Jeffrey Pennington", "Richard Socher", "Christopher Manning."], "venue": "Proceedings of EMNLP, pages 1532\u2013 1543, Doha, Qatar, October. Association for Computational Linguistics.", "citeRegEx": "Pennington et al\\.,? 2014", "shortCiteRegEx": "Pennington et al\\.", "year": 2014}, {"title": "The Word-space model", "author": ["Magnus Sahlgren."], "venue": "Ph.D. thesis, University of Stockholm (Sweden).", "citeRegEx": "Sahlgren.,? 2006", "shortCiteRegEx": "Sahlgren.", "year": 2006}, {"title": "Uncovering distributional differences between synonyms and antonyms in a word space model", "author": ["Silke Scheible", "Sabine Schulte im Walde", "Sylvia Springorum."], "venue": "Proceedings of IJCNLP, pages 489\u2013 497, Nagoya, Japan, October. Asian Federation of Nat-", "citeRegEx": "Scheible et al\\.,? 2013", "shortCiteRegEx": "Scheible et al\\.", "year": 2013}, {"title": "Semantic compositionality through recursive matrix-vector spaces", "author": ["Richard Socher", "Brody Huval", "Christopher D. Manning", "Andrew Y. Ng."], "venue": "Proceedings of EMNLP, pages 1201\u20131211, Jeju Island, Korea, July. Association for Computational Linguistics.", "citeRegEx": "Socher et al\\.,? 2012", "shortCiteRegEx": "Socher et al\\.", "year": 2012}, {"title": "Improved semantic representations from tree-structured long short-term memory networks", "author": ["Kai Sheng Tai", "Richard Socher", "Christopher D. Manning."], "venue": "Proceedings of ACL, pages 1556\u20131566, Beijing, China, July. Association for Computational", "citeRegEx": "Tai et al\\.,? 2015", "shortCiteRegEx": "Tai et al\\.", "year": 2015}, {"title": "Contextualizing semantic representations using syntactically enriched vector models", "author": ["Stefan Thater", "Hagen F\u00fcrstenau", "Manfred Pinkal."], "venue": "Proceedings of ACL, pages 948\u2013957, Uppsala, Sweden, July. Association for Computational Linguistics.", "citeRegEx": "Thater et al\\.,? 2010", "shortCiteRegEx": "Thater et al\\.", "year": 2010}, {"title": "Word meaning in context: A simple and effective vector model", "author": ["Stefan Thater", "Hagen F\u00fcrstenau", "Manfred Pinkal."], "venue": "Proceedings of IJCNLP, pages 1134\u20131143, Chiang Mai, Thailand, November. Asian Federation of Natural Language Processing.", "citeRegEx": "Thater et al\\.,? 2011", "shortCiteRegEx": "Thater et al\\.", "year": 2011}, {"title": "From frequency to meaning: Vector space models of semantics", "author": ["Peter D. Turney", "Patrick Pantel."], "venue": "J. Artif. Int. Res., 37(1):141\u2013188, January.", "citeRegEx": "Turney and Pantel.,? 2010", "shortCiteRegEx": "Turney and Pantel.", "year": 2010}, {"title": "Distributional composition using higher-order dependency vectors", "author": ["Julie Weeds", "David Weir", "Jeremy Reffin."], "venue": "Proceedings of the 2nd Workshop on Continuous Vector Space Models and their Compositionality, pages 11\u201320, Gothenburg, Sweden, April.", "citeRegEx": "Weeds et al\\.,? 2014", "shortCiteRegEx": "Weeds et al\\.", "year": 2014}, {"title": "Aligning packed dependency trees: a theory of composition for distributional semantics", "author": ["David Weir", "Julie Weeds", "Jeremy Reffin", "Thomas Kober."], "venue": "Computational Linguistics, in press.", "citeRegEx": "Weir et al\\.,? 2016", "shortCiteRegEx": "Weir et al\\.", "year": 2016}, {"title": "From paraphrase database to compositional paraphrase model and back", "author": ["John Wieting", "Mohit Bansal", "Kevin Gimpel", "Karen Livescu."], "venue": "TACL, 3:345\u2013358.", "citeRegEx": "Wieting et al\\.,? 2015", "shortCiteRegEx": "Wieting et al\\.", "year": 2015}, {"title": "The unknown perils of mining wikipedia", "author": ["Benjamin Wilson."], "venue": "https://blog.lateral.io/2015/06/theunknown-perils-of-mining-wikipedia/, June.", "citeRegEx": "Wilson.,? 2015", "shortCiteRegEx": "Wilson.", "year": 2015}, {"title": "Squibs: When the whole is not greater than the combination of its parts: A \u201ddecompositional\u201d look at compositional distributional semantics", "author": ["Fabio Massimo Zanzotto", "Lorenzo Ferrone", "Marco Baroni."], "venue": "Computational Linguistics, 41(1):165\u2013173.", "citeRegEx": "Zanzotto et al\\.,? 2015", "shortCiteRegEx": "Zanzotto et al\\.", "year": 2015}], "referenceMentions": [{"referenceID": 7, "context": "volved the use of various techniques for dimensionality reduction (Bullinaria and Levy, 2012; Lapesa and Evert, 2014) or the use of low-dimensional and dense neural word embeddings (Mikolov et al.", "startOffset": 66, "endOffset": 117}, {"referenceID": 23, "context": "volved the use of various techniques for dimensionality reduction (Bullinaria and Levy, 2012; Lapesa and Evert, 2014) or the use of low-dimensional and dense neural word embeddings (Mikolov et al.", "startOffset": 66, "endOffset": 117}, {"referenceID": 30, "context": "volved the use of various techniques for dimensionality reduction (Bullinaria and Levy, 2012; Lapesa and Evert, 2014) or the use of low-dimensional and dense neural word embeddings (Mikolov et al., 2013; Pennington et al., 2014).", "startOffset": 181, "endOffset": 228}, {"referenceID": 39, "context": "volved the use of various techniques for dimensionality reduction (Bullinaria and Levy, 2012; Lapesa and Evert, 2014) or the use of low-dimensional and dense neural word embeddings (Mikolov et al., 2013; Pennington et al., 2014).", "startOffset": 181, "endOffset": 228}, {"referenceID": 48, "context": "Our contributions are as follows: we show that typed and untyped sparse word representations, enriched by distributional inference, lead to performance improvements on several word similarity benchmarks, and that a higher-order dependency-typed vector space model, based on \u201cAnchored Packed Dependency Trees (APTs)\u201d (Weir et al., 2016), is competitive with the state-of-the-art for adjective-noun, noun-noun and verb-object compositions.", "startOffset": 316, "endOffset": 335}, {"referenceID": 11, "context": "Our method follows the distributional smoothing approach of Dagan et al. (1994) and Dagan et al.", "startOffset": 60, "endOffset": 80}, {"referenceID": 11, "context": "Our method follows the distributional smoothing approach of Dagan et al. (1994) and Dagan et al. (1997). In these works the authors are concerned with smoothing the probability estimate for", "startOffset": 60, "endOffset": 104}, {"referenceID": 11, "context": "guage modelling for speech recognition task, as well as for word-sense disambiguation in machine translation (Dagan et al., 1994; Dagan et al., 1997).", "startOffset": 109, "endOffset": 149}, {"referenceID": 12, "context": "guage modelling for speech recognition task, as well as for word-sense disambiguation in machine translation (Dagan et al., 1994; Dagan et al., 1997).", "startOffset": 109, "endOffset": 149}, {"referenceID": 8, "context": "guage modelling for speech recognition task, as well as for word-sense disambiguation in machine translation (Dagan et al., 1994; Dagan et al., 1997). More recently Pad\u00f3 et al. (2013) used a distributional approach for smoothing derivationally related words, such as oldish \u2013 old, as a back-off strategy in case of data sparsity.", "startOffset": 110, "endOffset": 184}, {"referenceID": 3, "context": "Starting from simple pointwise additive and multiplicative approaches to composition, such as Mitchell and Lapata (2008; 2010), and Blacoe and Lapata (2012), to tensor based models, such as Baroni and Zamparelli (2010), Coecke et al.", "startOffset": 132, "endOffset": 157}, {"referenceID": 3, "context": "Starting from simple pointwise additive and multiplicative approaches to composition, such as Mitchell and Lapata (2008; 2010), and Blacoe and Lapata (2012), to tensor based models, such as Baroni and Zamparelli (2010), Coecke et al.", "startOffset": 190, "endOffset": 219}, {"referenceID": 3, "context": "Starting from simple pointwise additive and multiplicative approaches to composition, such as Mitchell and Lapata (2008; 2010), and Blacoe and Lapata (2012), to tensor based models, such as Baroni and Zamparelli (2010), Coecke et al. (2010), Grefenstette et al.", "startOffset": 190, "endOffset": 241}, {"referenceID": 3, "context": "Starting from simple pointwise additive and multiplicative approaches to composition, such as Mitchell and Lapata (2008; 2010), and Blacoe and Lapata (2012), to tensor based models, such as Baroni and Zamparelli (2010), Coecke et al. (2010), Grefenstette et al. (2013) and Paperno et al.", "startOffset": 190, "endOffset": 269}, {"referenceID": 3, "context": "Starting from simple pointwise additive and multiplicative approaches to composition, such as Mitchell and Lapata (2008; 2010), and Blacoe and Lapata (2012), to tensor based models, such as Baroni and Zamparelli (2010), Coecke et al. (2010), Grefenstette et al. (2013) and Paperno et al. (2014), and neural network based approaches, such as Socher et al.", "startOffset": 190, "endOffset": 295}, {"referenceID": 3, "context": "Starting from simple pointwise additive and multiplicative approaches to composition, such as Mitchell and Lapata (2008; 2010), and Blacoe and Lapata (2012), to tensor based models, such as Baroni and Zamparelli (2010), Coecke et al. (2010), Grefenstette et al. (2013) and Paperno et al. (2014), and neural network based approaches, such as Socher et al. (2012), Le and Zuidema (2015), Mou et al.", "startOffset": 190, "endOffset": 362}, {"referenceID": 3, "context": "Starting from simple pointwise additive and multiplicative approaches to composition, such as Mitchell and Lapata (2008; 2010), and Blacoe and Lapata (2012), to tensor based models, such as Baroni and Zamparelli (2010), Coecke et al. (2010), Grefenstette et al. (2013) and Paperno et al. (2014), and neural network based approaches, such as Socher et al. (2012), Le and Zuidema (2015), Mou et al.", "startOffset": 190, "endOffset": 385}, {"referenceID": 3, "context": "Starting from simple pointwise additive and multiplicative approaches to composition, such as Mitchell and Lapata (2008; 2010), and Blacoe and Lapata (2012), to tensor based models, such as Baroni and Zamparelli (2010), Coecke et al. (2010), Grefenstette et al. (2013) and Paperno et al. (2014), and neural network based approaches, such as Socher et al. (2012), Le and Zuidema (2015), Mou et al. (2015) and Tai et al.", "startOffset": 190, "endOffset": 404}, {"referenceID": 3, "context": "Starting from simple pointwise additive and multiplicative approaches to composition, such as Mitchell and Lapata (2008; 2010), and Blacoe and Lapata (2012), to tensor based models, such as Baroni and Zamparelli (2010), Coecke et al. (2010), Grefenstette et al. (2013) and Paperno et al. (2014), and neural network based approaches, such as Socher et al. (2012), Le and Zuidema (2015), Mou et al. (2015) and Tai et al. (2015). Zanzotto et al.", "startOffset": 190, "endOffset": 426}, {"referenceID": 3, "context": "Starting from simple pointwise additive and multiplicative approaches to composition, such as Mitchell and Lapata (2008; 2010), and Blacoe and Lapata (2012), to tensor based models, such as Baroni and Zamparelli (2010), Coecke et al. (2010), Grefenstette et al. (2013) and Paperno et al. (2014), and neural network based approaches, such as Socher et al. (2012), Le and Zuidema (2015), Mou et al. (2015) and Tai et al. (2015). Zanzotto et al. (2015) provide a decompositional analysis of how similarity is affected by distributional composition, and link compositional models to convolution kernels.", "startOffset": 190, "endOffset": 450}, {"referenceID": 3, "context": "Starting from simple pointwise additive and multiplicative approaches to composition, such as Mitchell and Lapata (2008; 2010), and Blacoe and Lapata (2012), to tensor based models, such as Baroni and Zamparelli (2010), Coecke et al. (2010), Grefenstette et al. (2013) and Paperno et al. (2014), and neural network based approaches, such as Socher et al. (2012), Le and Zuidema (2015), Mou et al. (2015) and Tai et al. (2015). Zanzotto et al. (2015) provide a decompositional analysis of how similarity is affected by distributional composition, and link compositional models to convolution kernels. Most closely related to our approach of composition are the works of Thater et al. (2010), Thater et al.", "startOffset": 190, "endOffset": 690}, {"referenceID": 3, "context": "Starting from simple pointwise additive and multiplicative approaches to composition, such as Mitchell and Lapata (2008; 2010), and Blacoe and Lapata (2012), to tensor based models, such as Baroni and Zamparelli (2010), Coecke et al. (2010), Grefenstette et al. (2013) and Paperno et al. (2014), and neural network based approaches, such as Socher et al. (2012), Le and Zuidema (2015), Mou et al. (2015) and Tai et al. (2015). Zanzotto et al. (2015) provide a decompositional analysis of how similarity is affected by distributional composition, and link compositional models to convolution kernels. Most closely related to our approach of composition are the works of Thater et al. (2010), Thater et al. (2011) and Weeds et al.", "startOffset": 190, "endOffset": 712}, {"referenceID": 3, "context": "Starting from simple pointwise additive and multiplicative approaches to composition, such as Mitchell and Lapata (2008; 2010), and Blacoe and Lapata (2012), to tensor based models, such as Baroni and Zamparelli (2010), Coecke et al. (2010), Grefenstette et al. (2013) and Paperno et al. (2014), and neural network based approaches, such as Socher et al. (2012), Le and Zuidema (2015), Mou et al. (2015) and Tai et al. (2015). Zanzotto et al. (2015) provide a decompositional analysis of how similarity is affected by distributional composition, and link compositional models to convolution kernels. Most closely related to our approach of composition are the works of Thater et al. (2010), Thater et al. (2011) and Weeds et al. (2014), which aim to provide a general model of compositionality in a typed distributional vector space.", "startOffset": 190, "endOffset": 736}, {"referenceID": 3, "context": "Starting from simple pointwise additive and multiplicative approaches to composition, such as Mitchell and Lapata (2008; 2010), and Blacoe and Lapata (2012), to tensor based models, such as Baroni and Zamparelli (2010), Coecke et al. (2010), Grefenstette et al. (2013) and Paperno et al. (2014), and neural network based approaches, such as Socher et al. (2012), Le and Zuidema (2015), Mou et al. (2015) and Tai et al. (2015). Zanzotto et al. (2015) provide a decompositional analysis of how similarity is affected by distributional composition, and link compositional models to convolution kernels. Most closely related to our approach of composition are the works of Thater et al. (2010), Thater et al. (2011) and Weeds et al. (2014), which aim to provide a general model of compositionality in a typed distributional vector space. In this paper we adopt the approach to distributional composition introduced by Weir et al. (2016), whose APT framework is based on a higher-order dependency-typed", "startOffset": 190, "endOffset": 933}, {"referenceID": 1, "context": "be categorised into untyped proximity based models and typed models (Baroni and Lenci, 2010).", "startOffset": 68, "endOffset": 92}, {"referenceID": 1, "context": "be categorised into untyped proximity based models and typed models (Baroni and Lenci, 2010). Examples of the former include Deerwester et al. (1990); Lund and Burgess (1996); Curran (2004); Sahlgren (2006); Bullinaria and", "startOffset": 69, "endOffset": 150}, {"referenceID": 1, "context": "be categorised into untyped proximity based models and typed models (Baroni and Lenci, 2010). Examples of the former include Deerwester et al. (1990); Lund and Burgess (1996); Curran (2004); Sahlgren (2006); Bullinaria and", "startOffset": 69, "endOffset": 175}, {"referenceID": 1, "context": "be categorised into untyped proximity based models and typed models (Baroni and Lenci, 2010). Examples of the former include Deerwester et al. (1990); Lund and Burgess (1996); Curran (2004); Sahlgren (2006); Bullinaria and", "startOffset": 69, "endOffset": 190}, {"referenceID": 1, "context": "be categorised into untyped proximity based models and typed models (Baroni and Lenci, 2010). Examples of the former include Deerwester et al. (1990); Lund and Burgess (1996); Curran (2004); Sahlgren (2006); Bullinaria and", "startOffset": 69, "endOffset": 207}, {"referenceID": 46, "context": "Levy (2007) and Turney and Pantel (2010). These models count the number of times every word in a large corpus co-occurs with other words within a specified spatial context window, without leveraging the structural information of the text.", "startOffset": 16, "endOffset": 41}, {"referenceID": 8, "context": "In both kinds of models, the raw counts are usually transformed by Positive Pointwise Mutual Information (PPMI) or a variant of it (Church and Hanks, 1990; Niwa and Nitta, 1994; Scheible et al., 2013; Levy and Goldberg, 2014).", "startOffset": 131, "endOffset": 225}, {"referenceID": 34, "context": "In both kinds of models, the raw counts are usually transformed by Positive Pointwise Mutual Information (PPMI) or a variant of it (Church and Hanks, 1990; Niwa and Nitta, 1994; Scheible et al., 2013; Levy and Goldberg, 2014).", "startOffset": 131, "endOffset": 225}, {"referenceID": 41, "context": "In both kinds of models, the raw counts are usually transformed by Positive Pointwise Mutual Information (PPMI) or a variant of it (Church and Hanks, 1990; Niwa and Nitta, 1994; Scheible et al., 2013; Levy and Goldberg, 2014).", "startOffset": 131, "endOffset": 225}, {"referenceID": 25, "context": "In both kinds of models, the raw counts are usually transformed by Positive Pointwise Mutual Information (PPMI) or a variant of it (Church and Hanks, 1990; Niwa and Nitta, 1994; Scheible et al., 2013; Levy and Goldberg, 2014).", "startOffset": 131, "endOffset": 225}, {"referenceID": 18, "context": "Early proponents of that approach are Grefenstette (1994) and Lin (1998).", "startOffset": 38, "endOffset": 58}, {"referenceID": 18, "context": "Early proponents of that approach are Grefenstette (1994) and Lin (1998). More recent work by Pad\u00f3 and Lapata (2007), Erk and Pad\u00f3 (2008) and Weir et al.", "startOffset": 38, "endOffset": 73}, {"referenceID": 18, "context": "Early proponents of that approach are Grefenstette (1994) and Lin (1998). More recent work by Pad\u00f3 and Lapata (2007), Erk and Pad\u00f3 (2008) and Weir et al.", "startOffset": 38, "endOffset": 117}, {"referenceID": 14, "context": "More recent work by Pad\u00f3 and Lapata (2007), Erk and Pad\u00f3 (2008) and Weir et al.", "startOffset": 44, "endOffset": 64}, {"referenceID": 14, "context": "More recent work by Pad\u00f3 and Lapata (2007), Erk and Pad\u00f3 (2008) and Weir et al. (2016) uses dependency paths to build a structured vector space model.", "startOffset": 44, "endOffset": 87}, {"referenceID": 8, "context": "In both kinds of models, the raw counts are usually transformed by Positive Pointwise Mutual Information (PPMI) or a variant of it (Church and Hanks, 1990; Niwa and Nitta, 1994; Scheible et al., 2013; Levy and Goldberg, 2014). In the following we will give an explanation of the theory of composition with APTs as introduced by Weir et al. (2016), which we adopt in this paper.", "startOffset": 132, "endOffset": 347}, {"referenceID": 48, "context": "following Weir et al. (2016), align and aggregate the resulting parse trees according to their dependency type as shown in Figure 1.", "startOffset": 10, "endOffset": 29}, {"referenceID": 18, "context": "Composition is linguistically motivated by the principle of compositionality, which states that the meaning of a complex expression is fully determined by its structure and the meanings of its constituents (Frege, 1884).", "startOffset": 206, "endOffset": 219}, {"referenceID": 18, "context": "Composition is linguistically motivated by the principle of compositionality, which states that the meaning of a complex expression is fully determined by its structure and the meanings of its constituents (Frege, 1884). Many simple approaches to semantic composition neglect the structure and lose information in the composition process. For example, the phrases house boat and boat house have the exact same representation when composition is done via a pointwise arithmetic operation. Despite performing well in a number of studies, this commutativity is not desirable for a fine grained understanding of the semantics of natural language. When performing composition with APTs, we adopt the method introduced by Weir et al. (2016) which views distributional composition as a process of contextualisation.", "startOffset": 207, "endOffset": 735}, {"referenceID": 48, "context": "Offsetting can be seen as shifting the current viewpoint in the APT data structure and is necessary for aligning the feature spaces for composition (Weir et al., 2016).", "startOffset": 148, "endOffset": 167}, {"referenceID": 11, "context": "Following Dagan et al. (1994) and Dagan et al.", "startOffset": 10, "endOffset": 30}, {"referenceID": 11, "context": "Following Dagan et al. (1994) and Dagan et al. (1997), we propose a simple unsupervised algorithm for enriching sparse vector representations with their nearest neighbours.", "startOffset": 10, "endOffset": 54}, {"referenceID": 38, "context": "This approach has its roots in kernel density estimation (Parzen, 1962), however instead of defining a static global parzen window, we set the window size for every word individually, depending on the distance to its nearest neighbour, plus a threshold.", "startOffset": 57, "endOffset": 71}, {"referenceID": 50, "context": "6 billion tokens (Wilson, 2015).", "startOffset": 17, "endOffset": 31}, {"referenceID": 29, "context": "The corpus is lowercased, tokenised, lemmatised, PoS tagged and dependency parsed with the Stanford NLP tools, using universal dependencies (Manning et al., 2014; de Marneffe et al., 2014).", "startOffset": 140, "endOffset": 188}, {"referenceID": 25, "context": "The effect of shifting PPMI scores for untyped vector space models has already been explored in Levy and Goldberg (2014), and Levy et al.", "startOffset": 96, "endOffset": 121}, {"referenceID": 25, "context": "The effect of shifting PPMI scores for untyped vector space models has already been explored in Levy and Goldberg (2014), and Levy et al. (2015), thus we only present results for the APT model.", "startOffset": 96, "endOffset": 145}, {"referenceID": 5, "context": "We first evaluate our models on 3 word similarity benchmarks, MEN (Bruni et al., 2014), which", "startOffset": 66, "endOffset": 86}, {"referenceID": 22, "context": "meronymy or holonymy) between terms, SimLex-999 (Hill et al., 2015), which is testing for substitutability (e.", "startOffset": 48, "endOffset": 67}, {"referenceID": 17, "context": "synonymy, antonymy, hyponymy and hypernymy), and WordSim-353 (Finkelstein et al., 2001), where we", "startOffset": 61, "endOffset": 87}, {"referenceID": 0, "context": "use the version of Agirre et al. (2009), who split the dataset into a relatedness and a substitutability subset.", "startOffset": 19, "endOffset": 40}, {"referenceID": 0, "context": "use the version of Agirre et al. (2009), who split the dataset into a relatedness and a substitutability subset. Baroni and Lenci (2011) have shown that untyped models are typically better at capturing relatedness, whereas typed models are better at encoding substitutability.", "startOffset": 19, "endOffset": 137}, {"referenceID": 31, "context": "Table 5: Nearest neighbours AN, NN and VO pairs in the Mitchell and Lapata (2010) dataset, with and without distributional", "startOffset": 55, "endOffset": 82}, {"referenceID": 31, "context": "Table 5 shows a small number of example phrases together with their top 3 nearest neighbours, computed from the union of all words in the Wikipedia corpus and all phrase pairs in the Mitchell and Lapata (2010) dataset.", "startOffset": 183, "endOffset": 210}, {"referenceID": 31, "context": "for semantic composition, we evaluate our model on the composition dataset of Mitchell and Lapata (2010), consisting of 108 adjective-noun, 108 noun-noun, and 108 verb-object pairs.", "startOffset": 78, "endOffset": 105}, {"referenceID": 4, "context": "Blacoe and Lapata (2012) showed that an intersective composition function such as pointwise multiplication represents a competitive and robust approach in comparison to more sophisticated composition methods.", "startOffset": 0, "endOffset": 25}, {"referenceID": 4, "context": "Blacoe and Lapata (2012) showed that an intersective composition function such as pointwise multiplication represents a competitive and robust approach in comparison to more sophisticated composition methods. For the final set of experiments on the Mitchell and Lapata (2010) dataset, we present results the APT model and the untyped model, using composition by union and composition by intersection, with and without distributional inference.", "startOffset": 0, "endOffset": 276}, {"referenceID": 4, "context": "Blacoe and Lapata (2012) showed that an intersective composition function such as pointwise multiplication represents a competitive and robust approach in comparison to more sophisticated composition methods. For the final set of experiments on the Mitchell and Lapata (2010) dataset, we present results the APT model and the untyped model, using composition by union and composition by intersection, with and without distributional inference. We compare our models with the best performing untyped VSMs of Mitchell and Lapata (2010), and Blacoe and Lapata (2012), the best performing APT model of Weir et al.", "startOffset": 0, "endOffset": 534}, {"referenceID": 4, "context": "Blacoe and Lapata (2012) showed that an intersective composition function such as pointwise multiplication represents a competitive and robust approach in comparison to more sophisticated composition methods. For the final set of experiments on the Mitchell and Lapata (2010) dataset, we present results the APT model and the untyped model, using composition by union and composition by intersection, with and without distributional inference. We compare our models with the best performing untyped VSMs of Mitchell and Lapata (2010), and Blacoe and Lapata (2012), the best performing APT model of Weir et al.", "startOffset": 0, "endOffset": 564}, {"referenceID": 4, "context": "Blacoe and Lapata (2012) showed that an intersective composition function such as pointwise multiplication represents a competitive and robust approach in comparison to more sophisticated composition methods. For the final set of experiments on the Mitchell and Lapata (2010) dataset, we present results the APT model and the untyped model, using composition by union and composition by intersection, with and without distributional inference. We compare our models with the best performing untyped VSMs of Mitchell and Lapata (2010), and Blacoe and Lapata (2012), the best performing APT model of Weir et al. (2016), as", "startOffset": 0, "endOffset": 617}, {"referenceID": 21, "context": "well as with the recently published state-of-the-art methods by Hashimoto et al. (2014), and Wieting et al.", "startOffset": 64, "endOffset": 88}, {"referenceID": 21, "context": "well as with the recently published state-of-the-art methods by Hashimoto et al. (2014), and Wieting et al. (2015), who are using neural network based approaches.", "startOffset": 64, "endOffset": 115}, {"referenceID": 31, "context": "Table 7: Results for the Mitchell and Lapata (2010) dataset.", "startOffset": 25, "endOffset": 52}, {"referenceID": 21, "context": "distributional inference considerably improves upon the best results for APT models without distributional inference and for untyped count-based models, and is competitive with the state-of-the-art neural network based models of Hashimoto et al. (2014)", "startOffset": 229, "endOffset": 253}, {"referenceID": 46, "context": "and Wieting et al. (2015). Distributional inference also improves upon the performance of an untyped VSM where composition by pointwise multiplication is outperforming the models of Mitchell and Lapata (2010), and Blacoe and Lapata (2012).", "startOffset": 4, "endOffset": 26}, {"referenceID": 30, "context": "Distributional inference also improves upon the performance of an untyped VSM where composition by pointwise multiplication is outperforming the models of Mitchell and Lapata (2010), and Blacoe and Lapata (2012).", "startOffset": 155, "endOffset": 182}, {"referenceID": 4, "context": "Distributional inference also improves upon the performance of an untyped VSM where composition by pointwise multiplication is outperforming the models of Mitchell and Lapata (2010), and Blacoe and Lapata (2012). Table 7", "startOffset": 187, "endOffset": 212}, {"referenceID": 21, "context": "Unlike the models of Hashimoto et al. (2014) and Wieting et al.", "startOffset": 21, "endOffset": 45}, {"referenceID": 21, "context": "Unlike the models of Hashimoto et al. (2014) and Wieting et al. (2015), the elementary word representations, as well as the representations for composed phrases and the composition process in our models are fully interpretable2.", "startOffset": 21, "endOffset": 71}], "year": 2016, "abstractText": "Distributional models are derived from cooccurrences in a corpus, where only a small proportion of all possible plausible cooccurrences will be observed. This results in a very sparse vector space, requiring a mechanism for inferring missing knowledge. Most methods face this challenge in ways that render the resulting word representations uninterpretable, with the consequence that semantic composition becomes hard to model. In this paper we explore an alternative which involves explicitly inferring unobserved co-occurrences using the distributional neighbourhood. We show that distributional inference improves sparse word representations on several word similarity benchmarks and demonstrate that our model is competitive with the state-of-the-art for adjectivenoun, noun-noun and verb-object compositions while being fully interpretable.", "creator": "LaTeX with hyperref package"}}}