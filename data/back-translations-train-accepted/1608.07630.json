{"id": "1608.07630", "review": {"conference": "NIPS", "VERSION": "v1", "DATE_OF_SUBMISSION": "26-Aug-2016", "title": "Global Analysis of Expectation Maximization for Mixtures of Two Gaussians", "abstract": "Expectation Maximization (EM) is among the most popular algorithms for estimating parameters of statistical models. However, EM, which is an iterative algorithm based on the maximum likelihood principle, is generally only guaranteed to find stationary points of the likelihood objective, and these points may be far from any maximizer. This article addresses this disconnect between the statistical principles behind EM and its algorithmic properties. Specifically, it provides a global analysis of EM for specific models in which the observations comprise an i.i.d. sample from a mixture of two Gaussians. This is achieved by (i) studying the sequence of parameters from idealized execution of EM in the infinite sample limit, and fully characterizing the limit points of the sequence in terms of the initial parameters; and then (ii) based on this convergence analysis, establishing statistical consistency (or lack thereof) for the actual sequence of parameters produced by EM.", "histories": [["v1", "Fri, 26 Aug 2016 23:53:43 GMT  (111kb,D)", "http://arxiv.org/abs/1608.07630v1", null]], "reviews": [], "SUBJECTS": "math.ST cs.LG stat.CO stat.ML stat.TH", "authors": ["ji xu", "daniel j hsu", "arian maleki"], "accepted": true, "id": "1608.07630"}, "pdf": {"name": "1608.07630.pdf", "metadata": {"source": "CRF", "title": "Global Analysis of Expectation Maximization for Mixtures of Two Gaussians", "authors": ["Ji Xu", "Daniel Hsu", "Arian Maleki"], "emails": ["jixu@cs.columbia.edu,", "djhsu@cs.columbia.edu,", "arian@stat.columbia.edu"], "sections": [{"heading": null, "text": "There is no better way to estimate these parameters for many standard statistical models. Despite their appealing properties, the calculation of MLE is often intractable. In fact, this is the case for many latent variable models {f (Y; z; p)} in which the latent variables z are not observed. For each setting of the parameters, the marginal distribution of the observed data Y (for discrete z) f (Y; p) f (Y; z; p).It is this marginalization over latent variables that typically causes the computational difficulty. Moreover, many algorithms based on the MLE principle are known only to find stationary points of probability."}], "references": [{"title": "On spectral learning of mixtures of distributions", "author": ["D. Achlioptas", "F. McSherry"], "venue": "In Eighteenth Annual Conference on Learning Theory,", "citeRegEx": "Achlioptas and McSherry.,? \\Q2005\\E", "shortCiteRegEx": "Achlioptas and McSherry.", "year": 2005}, {"title": "Learning mixtures of separated nonspherical Gaussians", "author": ["S. Arora", "R. Kannan"], "venue": "The Annals of Applied Probability,", "citeRegEx": "Arora and Kannan.,? \\Q2005\\E", "shortCiteRegEx": "Arora and Kannan.", "year": 2005}, {"title": "Statistical guarantees for the EM algorithm: From population to sample-based analysis", "author": ["S. Balakrishnan", "M.J. Wainwright", "B. Yu"], "venue": null, "citeRegEx": "Balakrishnan et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Balakrishnan et al\\.", "year": 2014}, {"title": "Statistical mechanics of the maximum-likelihood density estimation", "author": ["N. Barkai", "H. Sompolinsky"], "venue": "Physical Review E,", "citeRegEx": "Barkai and Sompolinsky.,? \\Q1994\\E", "shortCiteRegEx": "Barkai and Sompolinsky.", "year": 1994}, {"title": "Polynomial learning of distribution families", "author": ["M. Belkin", "K. Sinha"], "venue": "In Fifty-First Annual IEEE Symposium on Foundations of Computer Science,", "citeRegEx": "Belkin and Sinha.,? \\Q2010\\E", "shortCiteRegEx": "Belkin and Sinha.", "year": 2010}, {"title": "Isotropic PCA and affine-invariant clustering", "author": ["S.C. Brubaker", "S. Vempala"], "venue": "In Forty-Ninth Annual IEEE Symposium on Foundations of Computer Science,", "citeRegEx": "Brubaker and Vempala.,? \\Q2008\\E", "shortCiteRegEx": "Brubaker and Vempala.", "year": 2008}, {"title": "Learning mixtures of product distributions using correlations and independence", "author": ["K. Chaudhuri", "S. Rao"], "venue": "In Twenty-First Annual Conference on Learning Theory,", "citeRegEx": "Chaudhuri and Rao.,? \\Q2008\\E", "shortCiteRegEx": "Chaudhuri and Rao.", "year": 2008}, {"title": "Multi-view clustering via canonical correlation analysis", "author": ["K. Chaudhuri", "S.M. Kakade", "K. Livescu", "K. Sridharan"], "venue": "In ICML,", "citeRegEx": "Chaudhuri et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Chaudhuri et al\\.", "year": 2009}, {"title": "Learning mixtures of gaussians using the k-means algorithm", "author": ["K. Chaudhuri", "S. Dasgupta", "A. Vattani"], "venue": "CoRR, abs/0912.0086,", "citeRegEx": "Chaudhuri et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Chaudhuri et al\\.", "year": 2009}, {"title": "On EM algorithms and their proximal generalizations", "author": ["S. Chr\u00e9tien", "A.O. Hero"], "venue": "ESAIM: Probability and Statistics,", "citeRegEx": "Chr\u00e9tien and Hero.,? \\Q2008\\E", "shortCiteRegEx": "Chr\u00e9tien and Hero.", "year": 2008}, {"title": "Expected maximum log likelihood estimation", "author": ["D. Conniffe"], "venue": "Journal of the Royal Statistical Society. Series D,", "citeRegEx": "Conniffe.,? \\Q1987\\E", "shortCiteRegEx": "Conniffe.", "year": 1987}, {"title": "Learning mixutres of Gaussians", "author": ["S. Dasgupta"], "venue": "In Fortieth Annual IEEE Symposium on Foundations of Computer Science,", "citeRegEx": "Dasgupta.,? \\Q1999\\E", "shortCiteRegEx": "Dasgupta.", "year": 1999}, {"title": "A probabilistic analysis of EM for mixtures of separated, spherical Gaussians", "author": ["S. Dasgupta", "L. Schulman"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Dasgupta and Schulman.,? \\Q2007\\E", "shortCiteRegEx": "Dasgupta and Schulman.", "year": 2007}, {"title": "Maximum-likelihood from incomplete data via the EM algorithm", "author": ["A.P. Dempster", "N.M. Laird", "D.B. Rubin"], "venue": "J. Royal Statist. Soc. Ser. B,", "citeRegEx": "Dempster et al\\.,? \\Q1977\\E", "shortCiteRegEx": "Dempster et al\\.", "year": 1977}, {"title": "On the mathematical foundations of theoretical statistics", "author": ["R.A. Fisher"], "venue": "Philosophical Transactions of the Royal Society,", "citeRegEx": "Fisher.,? \\Q1922\\E", "shortCiteRegEx": "Fisher.", "year": 1922}, {"title": "Tight bounds for learning a mixture of two gaussians", "author": ["M. Hardt", "E. Price"], "venue": "In Proceedings of the Forty-Seventh Annual ACM on Symposium on Theory of Computing,", "citeRegEx": "Hardt and Price.,? \\Q2015\\E", "shortCiteRegEx": "Hardt and Price.", "year": 2015}, {"title": "Learning mixtures of spherical Gaussians: moment methods and spectral decompositions", "author": ["D. Hsu", "S.M. Kakade"], "venue": "In Fourth Innovations in Theoretical Computer Science,", "citeRegEx": "Hsu and Kakade.,? \\Q2013\\E", "shortCiteRegEx": "Hsu and Kakade.", "year": 2013}, {"title": "Efficiently learning mixtures of two Gaussians", "author": ["A.T. Kalai", "A. Moitra", "G. Valiant"], "venue": "In Forty-second ACM Symposium on Theory of Computing,", "citeRegEx": "Kalai et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Kalai et al\\.", "year": 2010}, {"title": "The spectral method for general mixture models", "author": ["R. Kannan", "H. Salmasian", "S. Vempala"], "venue": "SIAM Journal on Computing,", "citeRegEx": "Kannan et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Kannan et al\\.", "year": 2008}, {"title": "Oracle inequalities in empirical risk minimization and sparse recovery problems", "author": ["V. Koltchinskii"], "venue": "In E\u0301cole d\u2032e\u0301te\u0301 de probabilite\u0301s de Saint-Flour XXXVIII,", "citeRegEx": "Koltchinskii.,? \\Q2011\\E", "shortCiteRegEx": "Koltchinskii.", "year": 2011}, {"title": "Least squares quantization in PCM", "author": ["S.P. Lloyd"], "venue": "IEEE Trans. Information Theory,", "citeRegEx": "Lloyd.,? \\Q1982\\E", "shortCiteRegEx": "Lloyd.", "year": 1982}, {"title": "Some methods for classification and analysis of multivariate observations", "author": ["J.B. MacQueen"], "venue": "In Proceedings of the fifth Berkeley Symposium on Mathematical Statistics and Probability,", "citeRegEx": "MacQueen.,? \\Q1967\\E", "shortCiteRegEx": "MacQueen.", "year": 1967}, {"title": "Settling the polynomial learnability of mixtures of Gaussians", "author": ["A. Moitra", "G. Valiant"], "venue": "In Fifty-First Annual IEEE Symposium on Foundations of Computer Science,", "citeRegEx": "Moitra and Valiant.,? \\Q2010\\E", "shortCiteRegEx": "Moitra and Valiant.", "year": 2010}, {"title": "Mixture densities, maximum likelihood and the EM algorithm", "author": ["R.A. Redner", "H.F. Walker"], "venue": "SIAM Review,", "citeRegEx": "Redner and Walker.,? \\Q1984\\E", "shortCiteRegEx": "Redner and Walker.", "year": 1984}, {"title": "An analysis of the EM algorithm and entropy-like proximal point methods", "author": ["P. Tseng"], "venue": "Mathematics of Operations Research,", "citeRegEx": "Tseng.,? \\Q2004\\E", "shortCiteRegEx": "Tseng.", "year": 2004}, {"title": "A spectral algorithm for learning mixtures models", "author": ["S. Vempala", "G. Wang"], "venue": "Journal of Computer and System Sciences,", "citeRegEx": "Vempala and Wang.,? \\Q2004\\E", "shortCiteRegEx": "Vempala and Wang.", "year": 2004}, {"title": "On the convergence properties of the EM algorithm", "author": ["C.F.J. Wu"], "venue": "The Annals of Statistics,", "citeRegEx": "Wu.,? \\Q1983\\E", "shortCiteRegEx": "Wu.", "year": 1983}, {"title": "On convergence properties of the EM algorithm for Gaussian mixtures", "author": ["L. Xu", "M.I. Jordan"], "venue": "Neural Computation,", "citeRegEx": "Xu and Jordan.,? \\Q1996\\E", "shortCiteRegEx": "Xu and Jordan.", "year": 1996}, {"title": "\u03b4. For the last claim, we borrow a technique in the proof of corollary 2 in B.2", "author": ["Balakrishnan"], "venue": null, "citeRegEx": "Balakrishnan,? \\Q2014\\E", "shortCiteRegEx": "Balakrishnan", "year": 2014}, {"title": "\u03bei\u3008yi \u2212 xa,xb\u3009\u3008yi,uj\u3009 is symmetric in terms of xb and constraints of xb is symmetric. The correctness of the last inequality is shown in B.2 of Balakrishnan et al", "author": ["Balakrishnan"], "venue": "EY ,\u03bee", "citeRegEx": "Balakrishnan,? \\Q2014\\E", "shortCiteRegEx": "Balakrishnan", "year": 2014}], "referenceMentions": [{"referenceID": 14, "context": "1 Introduction Since Fisher\u2019s 1922 paper (Fisher, 1922), maximum likelihood estimators (MLE) have become one of the most popular tools in many areas of science and engineering.", "startOffset": 41, "endOffset": 55}, {"referenceID": 13, "context": "1 Expectation Maximization Among the algorithms mentioned above, Expectation Maximization (EM) has attracted more attention for the simplicity of its iterations, and its good performance in practice (Dempster et al., 1977; Redner and Walker, 1984).", "startOffset": 199, "endOffset": 247}, {"referenceID": 23, "context": "1 Expectation Maximization Among the algorithms mentioned above, Expectation Maximization (EM) has attracted more attention for the simplicity of its iterations, and its good performance in practice (Dempster et al., 1977; Redner and Walker, 1984).", "startOffset": 199, "endOffset": 247}, {"referenceID": 26, "context": "Although EM is billed as a procedure for maximum likelihood estimation, it is known that with certain initializations, the final parameters returned by EM may be far from the MLE, both in parameter distance and in log-likelihood value (Wu, 1983).", "startOffset": 235, "endOffset": 245}, {"referenceID": 26, "context": "Several works characterize local convergence of EM to stationary points of the log-likelihood objective under certain regularity conditions (Wu, 1983; Tseng, 2004; Chr\u00e9tien and Hero, 2008).", "startOffset": 140, "endOffset": 188}, {"referenceID": 24, "context": "Several works characterize local convergence of EM to stationary points of the log-likelihood objective under certain regularity conditions (Wu, 1983; Tseng, 2004; Chr\u00e9tien and Hero, 2008).", "startOffset": 140, "endOffset": 188}, {"referenceID": 9, "context": "Several works characterize local convergence of EM to stationary points of the log-likelihood objective under certain regularity conditions (Wu, 1983; Tseng, 2004; Chr\u00e9tien and Hero, 2008).", "startOffset": 140, "endOffset": 188}, {"referenceID": 11, "context": "Several of these algorithms, starting with that of Dasgupta (1999), assume that the means of the mixture components are well-separated\u2014roughly at distance either d\u03b1 or k\u03b2 for some \u03b1, \u03b2 > 0 for a mixture of k Gaussians in Rd (Dasgupta, 1999; Arora and Kannan, 2005; Dasgupta and Schulman, 2007; Vempala and Wang, 2004; Kannan et al., 2008; Achlioptas and McSherry, 2005; Chaudhuri and Rao, 2008; Brubaker and Vempala, 2008; Chaudhuri et al., 2009a).", "startOffset": 224, "endOffset": 447}, {"referenceID": 1, "context": "Several of these algorithms, starting with that of Dasgupta (1999), assume that the means of the mixture components are well-separated\u2014roughly at distance either d\u03b1 or k\u03b2 for some \u03b1, \u03b2 > 0 for a mixture of k Gaussians in Rd (Dasgupta, 1999; Arora and Kannan, 2005; Dasgupta and Schulman, 2007; Vempala and Wang, 2004; Kannan et al., 2008; Achlioptas and McSherry, 2005; Chaudhuri and Rao, 2008; Brubaker and Vempala, 2008; Chaudhuri et al., 2009a).", "startOffset": 224, "endOffset": 447}, {"referenceID": 12, "context": "Several of these algorithms, starting with that of Dasgupta (1999), assume that the means of the mixture components are well-separated\u2014roughly at distance either d\u03b1 or k\u03b2 for some \u03b1, \u03b2 > 0 for a mixture of k Gaussians in Rd (Dasgupta, 1999; Arora and Kannan, 2005; Dasgupta and Schulman, 2007; Vempala and Wang, 2004; Kannan et al., 2008; Achlioptas and McSherry, 2005; Chaudhuri and Rao, 2008; Brubaker and Vempala, 2008; Chaudhuri et al., 2009a).", "startOffset": 224, "endOffset": 447}, {"referenceID": 25, "context": "Several of these algorithms, starting with that of Dasgupta (1999), assume that the means of the mixture components are well-separated\u2014roughly at distance either d\u03b1 or k\u03b2 for some \u03b1, \u03b2 > 0 for a mixture of k Gaussians in Rd (Dasgupta, 1999; Arora and Kannan, 2005; Dasgupta and Schulman, 2007; Vempala and Wang, 2004; Kannan et al., 2008; Achlioptas and McSherry, 2005; Chaudhuri and Rao, 2008; Brubaker and Vempala, 2008; Chaudhuri et al., 2009a).", "startOffset": 224, "endOffset": 447}, {"referenceID": 18, "context": "Several of these algorithms, starting with that of Dasgupta (1999), assume that the means of the mixture components are well-separated\u2014roughly at distance either d\u03b1 or k\u03b2 for some \u03b1, \u03b2 > 0 for a mixture of k Gaussians in Rd (Dasgupta, 1999; Arora and Kannan, 2005; Dasgupta and Schulman, 2007; Vempala and Wang, 2004; Kannan et al., 2008; Achlioptas and McSherry, 2005; Chaudhuri and Rao, 2008; Brubaker and Vempala, 2008; Chaudhuri et al., 2009a).", "startOffset": 224, "endOffset": 447}, {"referenceID": 0, "context": "Several of these algorithms, starting with that of Dasgupta (1999), assume that the means of the mixture components are well-separated\u2014roughly at distance either d\u03b1 or k\u03b2 for some \u03b1, \u03b2 > 0 for a mixture of k Gaussians in Rd (Dasgupta, 1999; Arora and Kannan, 2005; Dasgupta and Schulman, 2007; Vempala and Wang, 2004; Kannan et al., 2008; Achlioptas and McSherry, 2005; Chaudhuri and Rao, 2008; Brubaker and Vempala, 2008; Chaudhuri et al., 2009a).", "startOffset": 224, "endOffset": 447}, {"referenceID": 6, "context": "Several of these algorithms, starting with that of Dasgupta (1999), assume that the means of the mixture components are well-separated\u2014roughly at distance either d\u03b1 or k\u03b2 for some \u03b1, \u03b2 > 0 for a mixture of k Gaussians in Rd (Dasgupta, 1999; Arora and Kannan, 2005; Dasgupta and Schulman, 2007; Vempala and Wang, 2004; Kannan et al., 2008; Achlioptas and McSherry, 2005; Chaudhuri and Rao, 2008; Brubaker and Vempala, 2008; Chaudhuri et al., 2009a).", "startOffset": 224, "endOffset": 447}, {"referenceID": 5, "context": "Several of these algorithms, starting with that of Dasgupta (1999), assume that the means of the mixture components are well-separated\u2014roughly at distance either d\u03b1 or k\u03b2 for some \u03b1, \u03b2 > 0 for a mixture of k Gaussians in Rd (Dasgupta, 1999; Arora and Kannan, 2005; Dasgupta and Schulman, 2007; Vempala and Wang, 2004; Kannan et al., 2008; Achlioptas and McSherry, 2005; Chaudhuri and Rao, 2008; Brubaker and Vempala, 2008; Chaudhuri et al., 2009a).", "startOffset": 224, "endOffset": 447}, {"referenceID": 17, "context": "More recent work employs the method-of-moments, which permit the means of the mixture components to be arbitrarily close, provided that the sample size is sufficiently large (Kalai et al., 2010; Belkin and Sinha, 2010; Moitra and Valiant, 2010; Hsu and Kakade, 2013; Hardt and Price, 2015).", "startOffset": 174, "endOffset": 289}, {"referenceID": 4, "context": "More recent work employs the method-of-moments, which permit the means of the mixture components to be arbitrarily close, provided that the sample size is sufficiently large (Kalai et al., 2010; Belkin and Sinha, 2010; Moitra and Valiant, 2010; Hsu and Kakade, 2013; Hardt and Price, 2015).", "startOffset": 174, "endOffset": 289}, {"referenceID": 22, "context": "More recent work employs the method-of-moments, which permit the means of the mixture components to be arbitrarily close, provided that the sample size is sufficiently large (Kalai et al., 2010; Belkin and Sinha, 2010; Moitra and Valiant, 2010; Hsu and Kakade, 2013; Hardt and Price, 2015).", "startOffset": 174, "endOffset": 289}, {"referenceID": 16, "context": "More recent work employs the method-of-moments, which permit the means of the mixture components to be arbitrarily close, provided that the sample size is sufficiently large (Kalai et al., 2010; Belkin and Sinha, 2010; Moitra and Valiant, 2010; Hsu and Kakade, 2013; Hardt and Price, 2015).", "startOffset": 174, "endOffset": 289}, {"referenceID": 15, "context": "More recent work employs the method-of-moments, which permit the means of the mixture components to be arbitrarily close, provided that the sample size is sufficiently large (Kalai et al., 2010; Belkin and Sinha, 2010; Moitra and Valiant, 2010; Hsu and Kakade, 2013; Hardt and Price, 2015).", "startOffset": 174, "endOffset": 289}, {"referenceID": 21, "context": "(2009b) uses these symmetries to prove that a variant of Lloyd\u2019s algorithm (MacQueen, 1967; Lloyd, 1982) (which may be regarded as a hard-assignment version of EM) very quickly converges to the subspace spanned by the two mixture component means, without any separation assumption.", "startOffset": 75, "endOffset": 104}, {"referenceID": 20, "context": "(2009b) uses these symmetries to prove that a variant of Lloyd\u2019s algorithm (MacQueen, 1967; Lloyd, 1982) (which may be regarded as a hard-assignment version of EM) very quickly converges to the subspace spanned by the two mixture component means, without any separation assumption.", "startOffset": 75, "endOffset": 104}, {"referenceID": 2, "context": "3 Background and Related Work The EM algorithm was formally introduced by Dempster et al. (1977) as a general iterative method for computing parameter estimates from incomplete data.", "startOffset": 74, "endOffset": 97}, {"referenceID": 1, "context": "Several works characterize local convergence of EM to stationary points of the log-likelihood objective under certain regularity conditions (Wu, 1983; Tseng, 2004; Chr\u00e9tien and Hero, 2008). However, these analyses do not distinguish between global maximizers and other stationary points (except, e.g., when the likelihood function is unimodal). Thus, as an optimization algorithm for maximizing the log-likelihood objective, the \u201cworst-case\u201d performance of EM is somewhat discouraging. For a more optimistic perspective on EM, one may consider a \u201cbest-case\u201d analysis, where (i) the data are an iid sample from a distribution in the given model, (ii) the sample size is sufficiently large, and (iii) the starting point for EM is sufficiently close to the parameters of the data generating distribution. Conditions (i) and (ii) are ubiquitous in (asymptotic) statistical analyses, and (iii) is a generous assumption that may be satisfied in certain cases. Redner and Walker (1984) show that in such a favorable scenario, EM converges to the MLE almost surely for a broad class of mixture models.", "startOffset": 164, "endOffset": 979}, {"referenceID": 0, "context": "Moreover, recent work of Balakrishnan et al. (2014) gives non-asymptotic convergence guarantees in certain models; importantly, these results permit one to quantify the accuracy of a pilot estimator required to effectively initialize EM.", "startOffset": 25, "endOffset": 52}, {"referenceID": 0, "context": "Moreover, recent work of Balakrishnan et al. (2014) gives non-asymptotic convergence guarantees in certain models; importantly, these results permit one to quantify the accuracy of a pilot estimator required to effectively initialize EM. Thus, EM may be used in a tractable two-stage estimation procedures given a first-stage pilot estimator that can be efficiently computed. Indeed, for the special case of Gaussian mixtures, researchers in theoretical computer science and machine learning have developed efficient algorithms that deliver the highly accurate parameter estimates under appropriate conditions. Several of these algorithms, starting with that of Dasgupta (1999), assume that the means of the mixture components are well-separated\u2014roughly at distance either d\u03b1 or k\u03b2 for some \u03b1, \u03b2 > 0 for a mixture of k Gaussians in Rd (Dasgupta, 1999; Arora and Kannan, 2005; Dasgupta and Schulman, 2007; Vempala and Wang, 2004; Kannan et al.", "startOffset": 25, "endOffset": 678}, {"referenceID": 0, "context": ", 2008; Achlioptas and McSherry, 2005; Chaudhuri and Rao, 2008; Brubaker and Vempala, 2008; Chaudhuri et al., 2009a). More recent work employs the method-of-moments, which permit the means of the mixture components to be arbitrarily close, provided that the sample size is sufficiently large (Kalai et al., 2010; Belkin and Sinha, 2010; Moitra and Valiant, 2010; Hsu and Kakade, 2013; Hardt and Price, 2015). In particular, Hardt and Price (2015) characterize the information-theoretic limits of parameter estimation for mixtures of two Gaussians, and that they are achieved by a variant of the original method-of-moments of Pearson (1894).", "startOffset": 8, "endOffset": 447}, {"referenceID": 0, "context": ", 2008; Achlioptas and McSherry, 2005; Chaudhuri and Rao, 2008; Brubaker and Vempala, 2008; Chaudhuri et al., 2009a). More recent work employs the method-of-moments, which permit the means of the mixture components to be arbitrarily close, provided that the sample size is sufficiently large (Kalai et al., 2010; Belkin and Sinha, 2010; Moitra and Valiant, 2010; Hsu and Kakade, 2013; Hardt and Price, 2015). In particular, Hardt and Price (2015) characterize the information-theoretic limits of parameter estimation for mixtures of two Gaussians, and that they are achieved by a variant of the original method-of-moments of Pearson (1894). Most relevant to this paper are works that specifically analyze EM (or variants thereof) for Gaussian mixture models, especially when the mixture components are well-separated.", "startOffset": 8, "endOffset": 640}, {"referenceID": 0, "context": ", 2008; Achlioptas and McSherry, 2005; Chaudhuri and Rao, 2008; Brubaker and Vempala, 2008; Chaudhuri et al., 2009a). More recent work employs the method-of-moments, which permit the means of the mixture components to be arbitrarily close, provided that the sample size is sufficiently large (Kalai et al., 2010; Belkin and Sinha, 2010; Moitra and Valiant, 2010; Hsu and Kakade, 2013; Hardt and Price, 2015). In particular, Hardt and Price (2015) characterize the information-theoretic limits of parameter estimation for mixtures of two Gaussians, and that they are achieved by a variant of the original method-of-moments of Pearson (1894). Most relevant to this paper are works that specifically analyze EM (or variants thereof) for Gaussian mixture models, especially when the mixture components are well-separated. Xu and Jordan (1996) show favorable convergence properties (akin to super-linear convergence near the MLE) for well-separated mixtures.", "startOffset": 8, "endOffset": 839}, {"referenceID": 0, "context": ", 2008; Achlioptas and McSherry, 2005; Chaudhuri and Rao, 2008; Brubaker and Vempala, 2008; Chaudhuri et al., 2009a). More recent work employs the method-of-moments, which permit the means of the mixture components to be arbitrarily close, provided that the sample size is sufficiently large (Kalai et al., 2010; Belkin and Sinha, 2010; Moitra and Valiant, 2010; Hsu and Kakade, 2013; Hardt and Price, 2015). In particular, Hardt and Price (2015) characterize the information-theoretic limits of parameter estimation for mixtures of two Gaussians, and that they are achieved by a variant of the original method-of-moments of Pearson (1894). Most relevant to this paper are works that specifically analyze EM (or variants thereof) for Gaussian mixture models, especially when the mixture components are well-separated. Xu and Jordan (1996) show favorable convergence properties (akin to super-linear convergence near the MLE) for well-separated mixtures. In a related but different vein, Dasgupta and Schulman (2007) analyze a variant of EM with a particular initialization scheme, and proves fast convergence to the true parameters, again for well-separated mixtures in high-dimensions.", "startOffset": 8, "endOffset": 1016}, {"referenceID": 0, "context": ", 2008; Achlioptas and McSherry, 2005; Chaudhuri and Rao, 2008; Brubaker and Vempala, 2008; Chaudhuri et al., 2009a). More recent work employs the method-of-moments, which permit the means of the mixture components to be arbitrarily close, provided that the sample size is sufficiently large (Kalai et al., 2010; Belkin and Sinha, 2010; Moitra and Valiant, 2010; Hsu and Kakade, 2013; Hardt and Price, 2015). In particular, Hardt and Price (2015) characterize the information-theoretic limits of parameter estimation for mixtures of two Gaussians, and that they are achieved by a variant of the original method-of-moments of Pearson (1894). Most relevant to this paper are works that specifically analyze EM (or variants thereof) for Gaussian mixture models, especially when the mixture components are well-separated. Xu and Jordan (1996) show favorable convergence properties (akin to super-linear convergence near the MLE) for well-separated mixtures. In a related but different vein, Dasgupta and Schulman (2007) analyze a variant of EM with a particular initialization scheme, and proves fast convergence to the true parameters, again for well-separated mixtures in high-dimensions. For mixtures of two Gaussians, it is possible to exploit symmetries to get sharper analyses. Indeed, Chaudhuri et al. (2009b) uses these symmetries to prove that a variant of Lloyd\u2019s algorithm (MacQueen, 1967; Lloyd, 1982) (which may be regarded as a hard-assignment version of EM) very quickly converges to the subspace spanned by the two mixture component means, without any separation assumption.", "startOffset": 8, "endOffset": 1313}, {"referenceID": 0, "context": ", 2008; Achlioptas and McSherry, 2005; Chaudhuri and Rao, 2008; Brubaker and Vempala, 2008; Chaudhuri et al., 2009a). More recent work employs the method-of-moments, which permit the means of the mixture components to be arbitrarily close, provided that the sample size is sufficiently large (Kalai et al., 2010; Belkin and Sinha, 2010; Moitra and Valiant, 2010; Hsu and Kakade, 2013; Hardt and Price, 2015). In particular, Hardt and Price (2015) characterize the information-theoretic limits of parameter estimation for mixtures of two Gaussians, and that they are achieved by a variant of the original method-of-moments of Pearson (1894). Most relevant to this paper are works that specifically analyze EM (or variants thereof) for Gaussian mixture models, especially when the mixture components are well-separated. Xu and Jordan (1996) show favorable convergence properties (akin to super-linear convergence near the MLE) for well-separated mixtures. In a related but different vein, Dasgupta and Schulman (2007) analyze a variant of EM with a particular initialization scheme, and proves fast convergence to the true parameters, again for well-separated mixtures in high-dimensions. For mixtures of two Gaussians, it is possible to exploit symmetries to get sharper analyses. Indeed, Chaudhuri et al. (2009b) uses these symmetries to prove that a variant of Lloyd\u2019s algorithm (MacQueen, 1967; Lloyd, 1982) (which may be regarded as a hard-assignment version of EM) very quickly converges to the subspace spanned by the two mixture component means, without any separation assumption. Lastly, for the specific case of our Model 1, Balakrishnan et al. (2014) proves linear convergence of EM (as well as a gradient-based variant of EM) when started in a sufficiently small neighborhood around the true parameters; here, the size of the neighborhood grows with the separation between the two", "startOffset": 8, "endOffset": 1660}, {"referenceID": 2, "context": "Furthermore, the fact that \u03b7\u2217 is a global maximizer of Q(\u03b7 | \u03b7\u2217) is known as the self-consistency property (Balakrishnan et al., 2014).", "startOffset": 107, "endOffset": 134}, {"referenceID": 2, "context": "EM (e.g., Dasgupta and Schulman, 2007; Balakrishnan et al., 2014); our results show that they are not really necessary in the large sample limit.", "startOffset": 3, "endOffset": 65}, {"referenceID": 3, "context": "This would be consistent with results from statistical physics on the MLE for Gaussian mixtures, which characterize the behavior when dn \u221d n as n\u2192\u221e (Barkai and Sompolinsky, 1994).", "startOffset": 148, "endOffset": 178}, {"referenceID": 19, "context": "To simplify the final expression even further, we use the following lemma from Koltchinskii (2011) Lemma 24.", "startOffset": 79, "endOffset": 99}, {"referenceID": 2, "context": "2 in Balakrishnan et al. (2014). Let", "startOffset": 5, "endOffset": 32}, {"referenceID": 2, "context": "2 of Balakrishnan et al. (2014). For part 1, as shown in B.", "startOffset": 5, "endOffset": 32}, {"referenceID": 2, "context": "2 of Balakrishnan et al. (2014). For part 1, as shown in B.2 of Balakrishnan et al. (2014) we have EY ,\u03bee 1 n \u2211n i=1 \u03beiyiy > i \u2016op \u2264 EY ,\u03beej\u2208[M ] 1 n \u2211n i=1 \u03bei\u3008yi,uj\u2032 \u30092 (115) Recall that yi = \u03b6i\u03b8 ? + \u03c9i and (112), we have EY e\u3008yi,uj\u3009 = E\u03b6e ?jE\u03c9eij \u2264 e \u2016\u03b8?\u20162+1 2 .", "startOffset": 5, "endOffset": 91}], "year": 2016, "abstractText": "Expectation Maximization (EM) is among the most popular algorithms for estimating parameters of statistical models. However, EM, which is an iterative algorithm based on the maximum likelihood principle, is generally only guaranteed to find stationary points of the likelihood objective, and these points may be far from any maximizer. This article addresses this disconnect between the statistical principles behind EM and its algorithmic properties. Specifically, it provides a global analysis of EM for specific models in which the observations comprise an i.i.d. sample from a mixture of two Gaussians. This is achieved by (i) studying the sequence of parameters from idealized execution of EM in the infinite sample limit, and fully characterizing the limit points of the sequence in terms of the initial parameters; and then (ii) based on this convergence analysis, establishing statistical consistency (or lack thereof) for the actual sequence of parameters produced by EM.", "creator": "LaTeX with hyperref package"}}}