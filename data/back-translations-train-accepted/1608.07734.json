{"id": "1608.07734", "review": {"conference": "AAAI", "VERSION": "v1", "DATE_OF_SUBMISSION": "27-Aug-2016", "title": "Learning Bayesian Networks with Incomplete Data by Augmentation", "abstract": "We present new algorithms for learning Bayesian networks from data with missing values without the assumption that data are missing at random (MAR). An exact Bayesian network learning algorithm is obtained by recasting the problem into a standard Bayesian network learning problem without missing data. To the best of our knowledge, this is the first exact algorithm for this problem. As expected, the exact algorithm does not scale to large domains. We build on the exact method to create a new approximate algorithm using a hill-climbing technique. This algorithm scales to large domains so long as a suitable standard structure learning method for complete data is available. We perform a wide range of experiments to demonstrate the benefits of learning Bayesian networks without assuming MAR.", "histories": [["v1", "Sat, 27 Aug 2016 18:41:47 GMT  (42kb)", "http://arxiv.org/abs/1608.07734v1", null], ["v2", "Sun, 9 Oct 2016 01:50:25 GMT  (177kb)", "http://arxiv.org/abs/1608.07734v2", null]], "reviews": [], "SUBJECTS": "cs.AI stat.ML", "authors": ["tameem adel", "cassio polpo de campos"], "accepted": true, "id": "1608.07734"}, "pdf": {"name": "1608.07734.pdf", "metadata": {"source": "CRF", "title": "Learning Bayesian Networks without Assuming Missing at Random", "authors": ["Tameem Adel", "Cassio P. de Campos"], "emails": ["tameem.hesham@gmail.com"], "sections": [{"heading": null, "text": "ar Xiv: 160 8.07 734v 1 [cs.A I] 2 7A uWe present new algorithms for learning Bayesian networks from data with missing values without assuming that data is randomly missing (MAR).An exact Bayesian network learning algorithm is created by turning the problem into a Bayesian network standard learning problem without missing data. To our knowledge, this is the first exact algorithm for this problem. As expected, the exact algorithm does not scale to large areas. We build on the exact method to create a new approximate algorithm using a technique of mountaineering, which scales to large areas as long as a suitable standard structure learning method for complete data is available. We conduct a wide range of experiments to demonstrate the benefits of learning Bayesian networks without adopting MAR."}, {"heading": "1. Introduction", "text": "In fact, it is the case that one is able to find a solution that is capable of finding a solution that is capable of finding a solution and that is able to find a solution that is capable of finding a solution that is capable of finding a solution that is capable of finding a solution that is capable of finding a solution that is capable of finding a solution that is capable of finding a solution that is capable of finding a solution, that is capable of finding a solution that is capable of finding a solution that is capable of finding a solution."}, {"heading": "2. Bayesian Network Structure Learning", "text": "It is about the question of to what extent it is in a position to realize, that it is in a position to realize, that it is in a position to realize, that it is in a position to realize, that it is in a position to realize, that it is in a position to realize, that it is in a position to realize, that it is in a position to realize, that it is in a position, that it is in a position to realize, that it is in a position, that it is in a position, that it is in a position, that it is in a position, that it is in a position, that it is in a position, that it is in a position, that it is in a position, that it is in a position, that it is in a position, that it is in a position, that it is in a position, that it is in a position, that it is in a position, that it is in a position, that it is in a position, that it is in a position, that it is in a position, that it is in a position, that it is in a position, that it is in a position, that it is in a position, that it is in a position, that it is in a position, that it is in a position, that it is in a position, that it is in a position, that it is in a position, that it is in a position, that it is in a position, that it is in a position, that it is in a position, that it is in a position, that it is in a position, that it is in a position, that it is in a position, that it is in a position, that it is in a position, it is in a position, it is in a position, that it is in a position, it is in a position, that it is in a position, that it is in a position, that it is in a position, that it is in a way, it is in a way, it is in a way, it is in a way, that it is in a way, it is in a way, it is in a way, it is in a way, it is in a way, it is in a way, it is in a way, it is in a way, it is in a way, it is in a way, it is in a way, it is in a way, it is in a"}, {"heading": "2.1. Optimal (Exact) Learning Algorithm", "text": "We assume that a standard structure learning algorithm for complete data is available to us, based on the framework of two basic optimizations: (i) parent identification and (ii) structure optimization. Step (i) concerns building a list of parents for each variable, while step (ii) optimizes parent selection for each variable in a way that maximizes the overall result while ensuring that the graph is a DAG. This latter step can be addressed by exact or approximate methods Bartlett and Cussens (2013); Scanagatta et al al. (2015) (in our experiments we will apply an exact method that ensures that the quality of the results is only affected / related to the proper handling of the missing data, but for very large domains any approximate method could also be used).The exact algorithm to solve the equation (1) is based on parent principles."}, {"heading": "2.2. Approximate Algorithm", "text": "Although locally involved in the variables involved in the evaluation of a parent, the exact method takes into account all possible completions of the data. This is fine with a few missing values per variable, but if there are many missing values, especially within the same variable, the exact method becomes mathematically unfeasible. We propose an approximate algorithm based on a mountainous idea. We start with an initial conjecture Z0 (or several random conjectures) for the completion of all the missing values in the dataset. Then we perform exactly the same steps of the exact algorithm, but we limit the completions only to those that differ from the current conjectures Zh. At most, there are (R \u00b7 m) t completions Z \u00b2 h, so HD (Zh, Z \u00b2 h) perform an exact method of the exact algorithm, but we handle the exact method, but applying such a constraint during the transformation was explained in the previous section."}, {"heading": "3. Experiments", "text": "We conduct experiments with both simulated and real data, and the most important evaluation variable used is the accuracy of adding missing data, either in the form of missing values distributed across the data, or in the form of a binary classification problem where only the class variable can contain missing values. We perform most of our experiments with binary data for exposure reasons, although the algorithms are general and can be used with any categorical data (as shown in the last experimental setting). To test the significance, we perform a paired t test with a significance level of 5%. In all result tables, a result in bold refers to an accuracy value that is significantly better than its competitors, while two results belonging to the same experiment, bold means that each of them is significantly better than the rest of the competitors. For structural optimization, we use the exact solver referred to as Gnilp (1998 and we compare the approximate values of the same experiment.org. / Expusbn.com)."}, {"heading": "3.1. Well-known Bayesian Networks", "text": "We conduct experiments with real but small data sets to compare both exact and approximate NonMAR algorithms. First, we use the original Bavarian Breast Cancer Network Model (Almeida et al., 2014), which contains 8 binary variables, we simulate 100 instances of data. This model was learned from cancer patients at the University of Wisconsin Medical Hospital. Characteristics (Bavarian network nodes) include breast density, mass density, architectural distortions, and others, in addition to the diagnostic variable whose binary value refers to benign or malignant (DOrsi et al., 2003). We include two missing values per variable, resulting in a total of 16 missing values. These missing values are generated in a NonMAR manner by removing random values that are identical, that is, during the generation we force that all missing values are zero, or that all missing values are one."}, {"heading": "3.2. (LUng CAncer Simple set) LUCAS Dataset", "text": "The LUCAS dataset contains data from the causal Bayesian LUCAS network (Fogelman-Soulie, 2008) with 11 binary variables and the binary class variables, and contains 2000 instances. In this experiment, we perform an analysis of both missing MAR and NonMAR data to understand whether the advantages we have previously seen are relevant only in the case of NonMAR. Thus, we perform two experiments: (i) NonMAR setting by randomly generating missing values, all of which belong to the same data value (we repeat both to zero and to one value in succession); (ii) MAR setting by randomly generating missing values independent of their respective original values. These simulations are repeated 100 times. First, we generate two missing values per variable (24 missing values); a comparison between the imputational accuracy values of the NonMAR algorithm and structural EM values in the first two rows of Table 2 (called \"All Over\" in the re-significant MAR)."}, {"heading": "3.3. SPECT Dataset", "text": "The Single Proton Emission Computed Tomography (SPECT) dataset consists of binary data representing a partial diagnosis of SPECT images (Lichman, 2013).Each patient (data instance) is divided into one of two categories, normal and abnormal. SPECT data consists of a total of 267 instances and 23 variables (22 binary variables and one binary class variable).We generate NonMAR missing data of different proportions, using only one specific value at a time (missing data proportions across all data are 3%, 5% and 10%).These randomly generated datasets are given as input for the NonMAR approach algorithm as well as for structural EM. We note that there is a large discrepancy in the number of data values containing each of the two binary values: approximately 67% of the SPECT data has a value of 0, whereas only 33% of the data has a value."}, {"heading": "3.4. Smoking Cessation Study Dataset", "text": "The data set used in this experiment comes from a quit-smoking study as described in Gruder et al. (1993) and has been used in other work, notably in Hedeker et al. (2007).The quit-smoking data set is a binary data set consisting of 489 patient records (instances), the missing data being inherently contained therein, i.e. there is no need to simulate missing datasets.The data set contains 4 variables, including the class variable relating to smoking or non-smoking. All missing values are in the class variable. There are a total of 372 patient records with observed classes, consisting of 294 smoke and 78 non-smoking records, as well as 117 data sets with missing class labels. We do not have basic truth values for the missing values. The experiment we are conducting here is a semi-supervised learning (SSL) experiment, in which we hide the performance of a pond label (we use algorithms to evaluate the values we observed: the labels)."}, {"heading": "3.5. Car Evaluation Dataset", "text": "The data set Car Evaluation (Blake and Merz, 1998; Lichman, 2013) contains 1728 cases and 7 variables consisting of 6 attributes and one class. The 6 attributes relate to the following: purchase, maintenance, doors, persons, trunks and safety. The class variable refers to the acceptance of the car and can have exactly one of the following values: unacceptable, acceptable, good, very good. All variables are categorized with 3 or 4 states. The data were derived from a hierarchical decision model originally developed by Bohanec and Rajkovic (1988). Similar to Section 3.2, a NonMAR classification task is performed by including missing values, all belonging to a category of class variables at the same time (this is repeated for each label). Due to the class imbalance (unacceptable: 1210 cases, acceptable: 384, good: 69, v-good: 65), we conducted 10 experiments in which 100 randomly selected instances of the SVM table are compared with each other."}, {"heading": "4. Conclusions", "text": "We define an optimization task to address the problem and propose a new exact algorithm for translating the task into a structural learning problem without missing data. Inspired by the exact procedure, we develop an approximate algorithm that uses structural optimization as a partial call. In our experiments, we chose an exact structural optimizer to clearly identify the differences in the experimental results obtained by the different treatments of the missing data. However, any existing algorithm can easily be plugged into our framework so that the proposed approximate method can be scaled to areas with hundreds or even thousands of variables. We intend to explore this path in future work."}], "references": [{"title": "Expertbayes: Automatically refining manually built Bayesian networks", "author": ["E. Almeida", "P. Ferreira", "T. Vinhoza", "I. Dutra", "Y. Wu", "E. Burnside"], "venue": "Machine Learning and Applications (ICMLA)", "citeRegEx": "Almeida et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Almeida et al\\.", "year": 2014}, {"title": "Advances in Bayesian network learning using integer programming", "author": ["M. Bartlett", "J. Cussens"], "venue": "Conference on Uncertainty in artificial intelligence (UAI)", "citeRegEx": "Bartlett and Cussens,? \\Q2013\\E", "shortCiteRegEx": "Bartlett and Cussens", "year": 2013}, {"title": "UCI machine learning repository of machine learning databases", "author": ["C. Blake", "C. Merz"], "venue": null, "citeRegEx": "Blake and Merz,? \\Q1998\\E", "shortCiteRegEx": "Blake and Merz", "year": 1998}, {"title": "Knowledge acquisition and explanation for multi-attribute decision making", "author": ["M. Bohanec", "V. Rajkovic"], "venue": "Intl. Workshop on Expert Systems and their Applications", "citeRegEx": "Bohanec and Rajkovic,? \\Q1988\\E", "shortCiteRegEx": "Bohanec and Rajkovic", "year": 1988}, {"title": "Learning Bayesian network equivalence classes from incomplete data", "author": ["H. Borchani", "N.B. Amor", "K. Mellouli"], "venue": "Lecture Notes in Comp", "citeRegEx": "Borchani et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Borchani et al\\.", "year": 2006}, {"title": "Learning Bayesian networks is NP-complete", "author": ["D. Chickering"], "venue": "Learning from Data,", "citeRegEx": "Chickering,? \\Q1996\\E", "shortCiteRegEx": "Chickering", "year": 1996}, {"title": "A Bayesian method for the induction of probabilistic networks from data", "author": ["G. Cooper", "E. Herskovits"], "venue": "Machine Learning", "citeRegEx": "Cooper and Herskovits,? \\Q1992\\E", "shortCiteRegEx": "Cooper and Herskovits", "year": 1992}, {"title": "Efficient structure learning of Bayesian networks using constraints", "author": ["C. de Campos", "Q. Ji"], "venue": "Journal of Machine Learning Research (JMLR)", "citeRegEx": "Campos and Ji,? \\Q2011\\E", "shortCiteRegEx": "Campos and Ji", "year": 2011}, {"title": "BI-RADS: Mammography", "author": ["C. DOrsi", "L. Bassett", "W Berg"], "venue": "American College of Radiology", "citeRegEx": "DOrsi et al\\.,? \\Q2003\\E", "shortCiteRegEx": "DOrsi et al\\.", "year": 2003}, {"title": "Mining massive data sets for security: Advances in data mining, search, social networks and text mining, and their applications to security", "author": ["F. Fogelman-Soulie"], "venue": null, "citeRegEx": "Fogelman.Soulie,? \\Q2008\\E", "shortCiteRegEx": "Fogelman.Soulie", "year": 2008}, {"title": "The Bayesian structural em algorithm", "author": ["N. Friedman"], "venue": "Conference on Uncertainty in artificial intelligence (UAI)", "citeRegEx": "Friedman,? \\Q1998\\E", "shortCiteRegEx": "Friedman", "year": 1998}, {"title": "Bayesian network classifiers", "author": ["N. Friedman", "D. Geiger", "M. Goldszmidt"], "venue": "Machine Learning", "citeRegEx": "Friedman et al\\.,? \\Q1997\\E", "shortCiteRegEx": "Friedman et al\\.", "year": 1997}, {"title": "Effects of social support and relapse prevention training as adjuncts to a televised smoking cessation intervention", "author": ["R. Warnecke", "R. Burzette", "T. Miller"], "venue": "J Consult Clin Psychol", "citeRegEx": "J. et al\\.,? \\Q1993\\E", "shortCiteRegEx": "J. et al\\.", "year": 1993}, {"title": "The WEKA data mining software: An update", "author": ["M. Hall", "E. Frank", "G. Holmes", "B. Pfahringer", "P. Reutemann", "I. Witten"], "venue": "SIGKDD Explor. Newsl", "citeRegEx": "Hall et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Hall et al\\.", "year": 2009}, {"title": "Learning Bayesian networks: The combination of knowledge and statistical data", "author": ["D. Heckerman", "D. Geiger", "D. Chickering"], "venue": "Machine Learning", "citeRegEx": "Heckerman et al\\.,? \\Q1995\\E", "shortCiteRegEx": "Heckerman et al\\.", "year": 1995}, {"title": "Analysis of binary outcomes with missing data: missing = smoking, last observation carried forward", "author": ["D. Hedeker", "J. Mermelstein", "H. Demirtas"], "venue": null, "citeRegEx": "Hedeker et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Hedeker et al\\.", "year": 2007}, {"title": "Parent assignment is hard for the mdl, aic, and nml", "author": ["M. Koivisto"], "venue": "costs. conference on Learning Theory (COLT)", "citeRegEx": "Koivisto,? \\Q2006\\E", "shortCiteRegEx": "Koivisto", "year": 2006}, {"title": "Local computations with probabilities on graphical structures and their application to expert systems", "author": ["S. Lauritzen", "D. Speigelhalter"], "venue": "Royal statistical Society B", "citeRegEx": "Lauritzen and Speigelhalter,? \\Q1988\\E", "shortCiteRegEx": "Lauritzen and Speigelhalter", "year": 1988}, {"title": "Bayesian network structural learning and incomplete data", "author": ["P. Leray", "O. Francois"], "venue": "Intl. and Interdisc. Conf. on Adaptive Knowledge Repr. and Reasoning (AKRR),", "citeRegEx": "Leray and Francois,? \\Q2005\\E", "shortCiteRegEx": "Leray and Francois", "year": 2005}, {"title": "Statistical analysis with missing data", "author": ["R. Little", "D. Rubin"], "venue": null, "citeRegEx": "Little and Rubin,? \\Q1987\\E", "shortCiteRegEx": "Little and Rubin", "year": 1987}, {"title": "Estimating dependency structure as a hidden variable", "author": ["M. Meila", "M. Jordan"], "venue": "Advances in Neural Information Processing Systems (NIPS),", "citeRegEx": "Meila and Jordan,? \\Q1998\\E", "shortCiteRegEx": "Meila and Jordan", "year": 1998}, {"title": "Laplacian support vector machines trained in the primal", "author": ["S. Melacci", "M. Belkin"], "venue": "Journal of Machine Learning Research (JMLR)", "citeRegEx": "Melacci and Belkin,? \\Q2011\\E", "shortCiteRegEx": "Melacci and Belkin", "year": 2011}, {"title": "Learning Bayesian networks from incomplete databases", "author": ["M. Ramoni", "P. Sebastiani"], "venue": "Conference on Uncertainty in artificial intelligence (UAI)", "citeRegEx": "Ramoni and Sebastiani,? \\Q1997\\E", "shortCiteRegEx": "Ramoni and Sebastiani", "year": 1997}, {"title": "Bayesian network data imputation with application to survival tree analysis", "author": ["P. Rancoita", "M. Zaffalon", "E. Zucca", "F. Bertoni", "C. de Campos"], "venue": "Computational Statistics & Data Analysis", "citeRegEx": "Rancoita et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Rancoita et al\\.", "year": 2016}, {"title": "Learning Bayesian networks from incomplete data: An efficient method for generating approximate predictive distributions", "author": ["C. Riggelsen"], "venue": null, "citeRegEx": "Riggelsen,? \\Q2006\\E", "shortCiteRegEx": "Riggelsen", "year": 2006}, {"title": "Learning Bayesian network models from incomplete data using importance sampling", "author": ["C. Riggelsen", "A. Feelders"], "venue": null, "citeRegEx": "Riggelsen and Feelders,? \\Q2005\\E", "shortCiteRegEx": "Riggelsen and Feelders", "year": 2005}, {"title": "Um estudo do comportamento de redes Bayesianas no prognstico da sobrevivencia no cancro da prostata", "author": ["A. Sarabando"], "venue": "M.Sc. thesis, Universidade do Porto", "citeRegEx": "Sarabando,? \\Q2011\\E", "shortCiteRegEx": "Sarabando", "year": 2011}, {"title": "Learning Bayesian networks with thousands of variables", "author": ["M. Scanagatta", "C. de Campos", "G. Corani", "M. Zaffalon"], "venue": "Advances in Neural Information Processing Systems (NIPS),", "citeRegEx": "Scanagatta et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Scanagatta et al\\.", "year": 2015}], "referenceMentions": [{"referenceID": 19, "context": "Results can be incomplete in an industrial experiment due to mechanical breakdowns not necessarily related to the performed experiment (Little and Rubin, 1987).", "startOffset": 135, "endOffset": 159}, {"referenceID": 19, "context": "While data missingness in the above examples can mostly be assumed to be generated by a random process which depends only on the observed data, usually referred to as missing at random (MAR) (Little and Rubin, 1987; Rancoita et al., 2016), this assumption might miserably fail in other examples.", "startOffset": 191, "endOffset": 238}, {"referenceID": 23, "context": "While data missingness in the above examples can mostly be assumed to be generated by a random process which depends only on the observed data, usually referred to as missing at random (MAR) (Little and Rubin, 1987; Rancoita et al., 2016), this assumption might miserably fail in other examples.", "startOffset": 191, "endOffset": 238}, {"referenceID": 14, "context": "Given a dataset with categorical random variables, the Bayesian network structure learning problem refers to finding the best network structure (a directed acyclic graph, or DAG) according to a score function based on the data (Heckerman et al., 1995).", "startOffset": 227, "endOffset": 251}, {"referenceID": 5, "context": "As well known, learning a Bayesian network from complete data is NP-complete (Chickering, 1996), and the task becomes even harder with incomplete data.", "startOffset": 77, "endOffset": 95}, {"referenceID": 4, "context": "Many other algorithms have used ideas from structural EM and deal separately with the missing values and the structure optimization using complete data (Borchani et al., 2006; Leray and Francois, 2005; Meila and Jordan, 1998; Ramoni and Sebastiani, 1997; Riggelsen, 2006; Riggelsen and Feelders, 2005).", "startOffset": 152, "endOffset": 301}, {"referenceID": 18, "context": "Many other algorithms have used ideas from structural EM and deal separately with the missing values and the structure optimization using complete data (Borchani et al., 2006; Leray and Francois, 2005; Meila and Jordan, 1998; Ramoni and Sebastiani, 1997; Riggelsen, 2006; Riggelsen and Feelders, 2005).", "startOffset": 152, "endOffset": 301}, {"referenceID": 20, "context": "Many other algorithms have used ideas from structural EM and deal separately with the missing values and the structure optimization using complete data (Borchani et al., 2006; Leray and Francois, 2005; Meila and Jordan, 1998; Ramoni and Sebastiani, 1997; Riggelsen, 2006; Riggelsen and Feelders, 2005).", "startOffset": 152, "endOffset": 301}, {"referenceID": 22, "context": "Many other algorithms have used ideas from structural EM and deal separately with the missing values and the structure optimization using complete data (Borchani et al., 2006; Leray and Francois, 2005; Meila and Jordan, 1998; Ramoni and Sebastiani, 1997; Riggelsen, 2006; Riggelsen and Feelders, 2005).", "startOffset": 152, "endOffset": 301}, {"referenceID": 24, "context": "Many other algorithms have used ideas from structural EM and deal separately with the missing values and the structure optimization using complete data (Borchani et al., 2006; Leray and Francois, 2005; Meila and Jordan, 1998; Ramoni and Sebastiani, 1997; Riggelsen, 2006; Riggelsen and Feelders, 2005).", "startOffset": 152, "endOffset": 301}, {"referenceID": 25, "context": "Many other algorithms have used ideas from structural EM and deal separately with the missing values and the structure optimization using complete data (Borchani et al., 2006; Leray and Francois, 2005; Meila and Jordan, 1998; Ramoni and Sebastiani, 1997; Riggelsen, 2006; Riggelsen and Feelders, 2005).", "startOffset": 152, "endOffset": 301}, {"referenceID": 22, "context": "However, that algorithm works as a data augmentation approach related to MAR (Ramoni and Sebastiani, 1997) instead of considering NonMAR.", "startOffset": 77, "endOffset": 106}, {"referenceID": 9, "context": "The seminal algorithm in Friedman (1998) introduced an iterative method based on the Expectation-Maximization (EM) technique, referred to as structural EM.", "startOffset": 25, "endOffset": 41}, {"referenceID": 4, "context": "Many other algorithms have used ideas from structural EM and deal separately with the missing values and the structure optimization using complete data (Borchani et al., 2006; Leray and Francois, 2005; Meila and Jordan, 1998; Ramoni and Sebastiani, 1997; Riggelsen, 2006; Riggelsen and Feelders, 2005). In Rancoita et al. (2016), structures are learned from incomplete data using a structural EM whose maximization step is performed by an anytime method, and the \u2018expectation\u2019 step imputes the missing values using expected means, or modes, of the current estimated joint distribution.", "startOffset": 153, "endOffset": 329}, {"referenceID": 6, "context": "We consider here the score function sD to be the Bayesian Dirichlet Equivalent Uniform (BDeu) criterion (Buntine, 1991; Cooper and Herskovits, 1992) (other decomposable scores could be used too), so we have sD(G) = \u2211 i sD(Xi,PAi).", "startOffset": 104, "endOffset": 148}, {"referenceID": 5, "context": "Hardness is obtained by realizing that this problem generalizes the structure learning problem without missing data, which is NP-hard Chickering (1996). Pertinence in NP holds since given G and Z, the score function sD can be computed in polynomial time.", "startOffset": 134, "endOffset": 152}, {"referenceID": 16, "context": "This step has no known polynomial-time solution if we do not impose a maximum number of parents (Koivisto, 2006), so we will assume that such a bound k is given.", "startOffset": 96, "endOffset": 112}, {"referenceID": 27, "context": "We compute the candidate list by using one of the available approaches (de Campos and Ji, 2011; Scanagatta et al., 2015) to guide the search, but for each candidate to be evaluated, the corresponding variables in the dataset might contain missing values.", "startOffset": 71, "endOffset": 120}, {"referenceID": 1, "context": "This latter step can be tackled by exact or approximate methods Bartlett and Cussens (2013); Scanagatta et al.", "startOffset": 64, "endOffset": 92}, {"referenceID": 1, "context": "This latter step can be tackled by exact or approximate methods Bartlett and Cussens (2013); Scanagatta et al. (2015) (in our experiments we will employ an exact method such that we are sure that the quality of results is only affected/related to the proper treatment of the missing data, but for very large domains any approximate method could be used too).", "startOffset": 64, "endOffset": 118}, {"referenceID": 27, "context": "(Scanagatta et al., 2015)).", "startOffset": 0, "endOffset": 25}, {"referenceID": 1, "context": "For structure optimization, we use the exact solver referred to as Gobnilp (Bartlett and Cussens, 2013) with the code available from https://www.", "startOffset": 75, "endOffset": 103}, {"referenceID": 10, "context": "We perform comparisons among the two proposed NonMAR algorithms (exact and approximate) and the structural Expectation-Maximization (EM) algorithm (Friedman, 1998).", "startOffset": 147, "endOffset": 163}, {"referenceID": 23, "context": "com/cassiopc/csdadataimputation (Rancoita et al., 2016).", "startOffset": 32, "endOffset": 55}, {"referenceID": 0, "context": "Bayesian network model for Breast Cancer (Almeida et al., 2014), which contains 8 binary variables, we simulate 100 data instances.", "startOffset": 41, "endOffset": 63}, {"referenceID": 8, "context": "Features (Bayesian network nodes) include breast density, mass density, architectural distortion and others, in addition to the diagnosis variable whose binary value refers to benign or malignant (DOrsi et al., 2003).", "startOffset": 196, "endOffset": 216}, {"referenceID": 11, "context": "Second, we use the Bayesian network that has been learned from the Prostate Cancer data by the Tree Augmented Naive Bayes (TAN) (Friedman et al., 1997), implemented by WEKA (Hall et al.", "startOffset": 128, "endOffset": 151}, {"referenceID": 13, "context": ", 1997), implemented by WEKA (Hall et al., 2009).", "startOffset": 29, "endOffset": 48}, {"referenceID": 26, "context": "The Prostate Cancer data were acquired during three different moments in time (Sarabando, 2011; Almeida et al., 2014), during a medical appointment, after performing auxiliary exams, and five years after a radical prostatectomy.", "startOffset": 78, "endOffset": 117}, {"referenceID": 0, "context": "The Prostate Cancer data were acquired during three different moments in time (Sarabando, 2011; Almeida et al., 2014), during a medical appointment, after performing auxiliary exams, and five years after a radical prostatectomy.", "startOffset": 78, "endOffset": 117}, {"referenceID": 17, "context": "Third, the well-known ASIA network is used (Lauritzen and Speigelhalter, 1988).", "startOffset": 43, "endOffset": 78}, {"referenceID": 9, "context": "The LUCAS dataset contains data of the LUCAS causal Bayesian network (Fogelman-Soulie, 2008) with 11 binary variables, as well as the binary class variable, and contains 2000 instances.", "startOffset": 69, "endOffset": 92}, {"referenceID": 15, "context": "It has been further utilized in other works, most notably Hedeker et al. (2007). The smoking cessation dataset is a binary dataset consisting of 489 patient records (instances) with the missing data being inherently therein, i.", "startOffset": 58, "endOffset": 80}, {"referenceID": 21, "context": "We compare the performance of the NonMAR approximate algorithm against an equivalent procedure using structural EM (labels are then chosen based on the posterior distribution), and also against a semi-supervised learner in the form of a Laplacian SVM (Melacci and Belkin, 2011) whose code is available online.", "startOffset": 251, "endOffset": 277}, {"referenceID": 2, "context": "The Car Evaluation dataset (Blake and Merz, 1998; Lichman, 2013) contains 1728 instances and 7 variables consisting of 6 attributes and a class.", "startOffset": 27, "endOffset": 64}, {"referenceID": 2, "context": "The Car Evaluation dataset (Blake and Merz, 1998; Lichman, 2013) contains 1728 instances and 7 variables consisting of 6 attributes and a class. The 6 attributes refer to the following: buying, maintenance, doors, persons, luggage boots and safety. The class variable refers to the car acceptability and can have exactly one of the following values: unacceptable, acceptable, good, very good. All variables are categorical with 3 or 4 states. The data were derived from a hierarchical decision model originally developed by Bohanec and Rajkovic (1988). Similar to Section 3.", "startOffset": 28, "endOffset": 552}], "year": 2017, "abstractText": "We present new algorithms for learning Bayesian networks from data with missing values without the assumption that data are missing at random (MAR). An exact Bayesian network learning algorithm is obtained by recasting the problem into a standard Bayesian network learning problem without missing data. To the best of our knowledge, this is the first exact algorithm for this problem. As expected, the exact algorithm does not scale to large domains. We build on the exact method to create a new approximate algorithm using a hill-climbing technique. This algorithm scales to large domains so long as a suitable standard structure learning method for complete data is available. We perform a wide range of experiments to demonstrate the benefits of learning Bayesian networks without assuming MAR.", "creator": "LaTeX with hyperref package"}}}