{"id": "1608.07905", "review": {"conference": "iclr", "VERSION": "v1", "DATE_OF_SUBMISSION": "29-Aug-2016", "title": "Machine Comprehension Using Match-LSTM and Answer Pointer", "abstract": "Machine comprehension of text is an important problem in natural language processing. A recently released dataset, the Stanford Question Answering Dataset (SQuAD), offers a large number of real questions and their answers created by humans through crowdsourcing. SQuAD provides a challenging testbed for evaluating machine comprehension algorithms, partly because compared with previous datasets, in SQuAD the answers do not come from a small set of candidate answers and they have variable lengths. We propose an end-to-end neural architecture for the task. The architecture is based on match-LSTM, a model we proposed previously for textual entailment, and Pointer Net, a sequence-to-sequence model proposed by Vinyals et al.(2015) to constrain the output tokens to be from the input sequences. We propose two ways of using Pointer Net for our task. Our experiments show that both of our two models substantially outperform the best results obtained by Rajpurkar et al.(2016) using logistic regression and manually crafted features.", "histories": [["v1", "Mon, 29 Aug 2016 03:42:50 GMT  (428kb,D)", "http://arxiv.org/abs/1608.07905v1", "12 pages; 3 figures"], ["v2", "Mon, 7 Nov 2016 03:39:40 GMT  (389kb,D)", "http://arxiv.org/abs/1608.07905v2", "11 pages; 3 figures"]], "COMMENTS": "12 pages; 3 figures", "reviews": [], "SUBJECTS": "cs.CL cs.AI", "authors": ["shuohang wang", "jing jiang"], "accepted": true, "id": "1608.07905"}, "pdf": {"name": "1608.07905.pdf", "metadata": {"source": "CRF", "title": "MACHINE COMPREHENSION USING MATCH-LSTM", "authors": ["ANSWER POINTER", "Shuohang Wang", "Jing Jiang"], "emails": ["shwang.2014@phdis.smu.edu.sg", "jingjiang@smu.edu.sg"], "sections": [{"heading": "1 INTRODUCTION", "text": "In fact, it is in such a way that most people are able to recognize themselves and understand what they are doing in order to change the world and to change it. (...) In fact, it is in such a way that the world is able to change the world. (...) It is in such a way that the world in which we live remains in itself. (...) It is in such a way that the world, in the world, in the world, in the world, in the world, in the world, in the world, in the world, in the world, in the world, in the world, in the world, in the world, in the world, in the world, in the world, in the world, in the world, in the world, in the world, in the world, in the world, in the world, in the world, in the world, in the world, in the world, in the world, in the world, in the world, in the world, in the world, in the world, in the world, in the world, in the world, in the world, in the world, in the world, in the world, in the world, in the world, in the world, in the world, in the world, in the world, in the world, in, in the world, in the, in the world, in the world, in the world, in the, in, in the world, in, in the world, in, in the, in the world, in the, in the, in the, in, in the world, in the, in the, in the, in the, in the, in the world, in the, in the, in the, in the, in the, in the, in, in the, in the, in the, in the, in the, in the world, in, in, in the, in the, in the, in the world, in the, in the, in, in the, in the, in the, in the, in the, in the, in the world, in the, in the, in the, in the, in the, in the, in the, in the, in the, in the, in the world, in the, in the, in the, in the world, in the, in the, in the, in the, in the, in the, in the, in the, in"}, {"heading": "2 METHOD", "text": "In this section we will first briefly discuss Match-LSTM and Pointer Net. These two existing works form the basis of our method. Afterwards, we will present our continuous neural architecture for machine understanding."}, {"heading": "2.1 MATCH-LSTM", "text": "In a recent paper on natural language learning, we proposed a match-LSTM model for predicting text influences (Wang & Jiang, 2016), in which two sentences are given for text influences, one being a premise and the other a hypothesis. To predict whether the premise implies the hypothesis, the match-LSTM model passes through the symbols of the hypothesis one by one, and at each position of the hypothesis an attention mechanism is used to obtain a weighted vector representation of the premise, which is then combined with a vector representation of the current token of the hypothesis and fed into an LSTM we call Match-LSTM. The matchLSTM essentially aggregates, one by one, the assignment of the attention-weighted premise to each token of the hypothesis and uses the aggregated matching to make a final prediction."}, {"heading": "2.2 POINTER NET", "text": "Vinyals et al. (2015) proposed a pointer network (Ptr-Net) to solve a specific type of problem where we want to generate an output sequence whose characters must come from the input sequence. Instead of selecting an output symbol from a fixed vocabulary, Ptr-Net uses an attention mechanism as a pointer to select a position from the input sequence as the output symbol. The pointer mechanism has inspired some recent work on language processing (Gu et al., 2016; Kadlec et al., 2016)."}, {"heading": "2.3 OUR METHOD", "text": "The problem we are trying to solve is formulated as follows: We are given a piece of text to which we refer as a passage, and a question related to the passage. The passage is represented by matrix P, Rd, and P, where P represents the length (number of characters) of the passage, and d represents the dimensionality of the word embedding. Likewise, the question is represented by matrix Q, Rd, and Q, where Q is the length of the question. Our goal is to identify a sequence from the passage as the answer to the question, and d is the dimensionality of the word embedding. Since the output tokens come from input, we would like to take over the Pointer Net for this problem."}, {"heading": "3 EXPERIMENTS", "text": "In this section, we present our experimental results and perform some analyses to better understand how our models work."}, {"heading": "3.1 DATA", "text": "We use the Stanford Question Answering Dataset (SQuAD) v1.02 to conduct our experiments. SQuAD passages come from 536 Wikipedia articles covering a wide range of topics. Each passage is a single paragraph from a Wikipedia article, and each passage has about 5 related questions. In total, there are 23,215 passages and 107,785 questions. Data was divided into a training set (with 87,636 question-answer pairs), a development set (with 10,600 question-answer pairs) and a test set that is not published. To evaluate our model on the test set, we had to submit our trained model Rajpurkar et al. (2016), which helped us maintain performance on the test set."}, {"heading": "3.2 EXPERIMENT SETTINGS", "text": "We use word embeddings from GloVe (Pennington et al., 2014) to initialize the model. Words that are not found in GloVe are initialized as zero vectors, the word embeddings are not updated during the training of the model. The dimensionality l of the hidden layers is set to 150. We use ADAMAX (Kingma & Ba, 2015) with the coefficients \u03b21 = 0.9 and \u03b22 = 0.999 to optimize the model. Each update is calculated by a minibatch of 30 instances. We do not use L2 regulation. We use the development kit to adjust the hyperparameters that match the learning rate and the failure.The performance is measured using two metrics: percentage of exact match with the answers of the floor and word levels F1, when we compare the tokens in the answers to the answers to the question that best match the answers to the question in Formula 1."}, {"heading": "3.3 RESULTS", "text": "The results of our models as well as the results of two baselines from Rajpurkar et al. (2016) are shown in Table 2. We see that our two models have significantly exceeded the logistic regression model from Rajpurkar et al. (2016), which is based on carefully designed features. Furthermore, our boundary model has exceeded the sequence model, achieving an exact match score of 59.5% and an F1 score of 70.3%. Especially in terms of accurate match score, the boundary model has a clear advantage over the sequence model. Improving our models over the logistic regression model shows that our end-to-end models of neural networks without much feature engineering are very effective in this task and this dataset."}, {"heading": "3.4 FURTHER ANALYSES", "text": "To better understand the strengths and weaknesses of our models, we perform some further analysis of the results discussed below. Firstly, we suspect that longer answers are more difficult to predict. To verify this hypothesis, we plot the performance both in terms of exact match and F1 points in terms of response length to the set of development. We also show the number of question-answer pairs with different response lengths. The two diagrams are shown in the first row of Figure 2. We can see from Figure (2) in the upper right corner of Figure 2 that there are more short answers than long answers in the dataset. And from Figure (1) in the upper left corner, we see that our models work much better for short answers than for long answers. For example, for questions whose answers contain more than 9 tokens, the F1 point of the boundary model drops to around 56% and the exact mapping falls to just 30%, compared to F1 points and exact mapping numbers to almost 72%."}, {"heading": "4 RELATED WORK", "text": "Machine understanding of text has received a lot of attention in recent years, and increasingly researchers are developing data-driven, end-to-end models of neural networks for this task. We will first review the recently published datasets and then some end-to-end models for this task. 2Rajpurkar et al. (2016) have just published a v1.1 of the same dataset. We have yet to use the new version of the data to train our models."}, {"heading": "4.1 DATASETS", "text": "A number of data sets for studying machine comprehension were created Cloze-style by removing a single token from a sentence in the original corpus. Questions created this way are not real questions that begin with \"when,\" \"where,\" etc. For example, Hermann et al. (2015) creates CNN and Daily Mail-style questions that are short summaries of the main points of CNN and Daily Mail news articles. The task is to predict a missing token that has been removed from a climax based on the corresponding news article. Hill et al. (2016) creates the Children's Book Test Data Set, which is based on children's stories. Each query is the 21st sentence removed from a story with one word. The task is to use the first 20 sentences to predict the missing word in the query. Cui et al. (2016) publishes two similar data sets in Chinese, the People Daily Dataset."}, {"heading": "4.2 END-TO-END NEURAL NETWORK MODELS FOR MACHINE COMPREHENSION", "text": "There are a number of studies that suggest end-to-end models of neural networks for machine understanding (Hermann et al., 2015). A common approach is to use relapsing neural networks (RNNs) to process the given text and question to predict or generate the answers (Hermann et al., 2015; Chen et al., 2016). Given that answers often come from the given passage, Pointer Network has been used in some studies to copy tokens from the given passage as answers (Kadlec et al., 2016; Trischler et al., 2016). Compared with existing work, we use Match-LSTM to match a question and a particular passage, and we use Pointer Network in a different way to copy answers from the given passage (Kadlec et al., 2016; Trischler et al., 2016)."}, {"heading": "5 CONCLUSIONS", "text": "In this paper, we proposed a three-layer architecture of neural networks to solve the problem of machine understanding defined in the Stanford Question Answering (SQuAD) dataset. We developed two models for the problem, both using match-LSTM and pointer network. Experiments with the SQuAD dataset showed that our two models significantly exceeded the best performance achieved by (Rajpurkar et al., 2016). In particular, our second model, the boundary model, achieved an exact match score of 59.5% and an F1 score of 70.3% on the test dataset. Our further analyses showed that our models are better suited to short-answer questions and certain types of questions such as \"when.\" In the future, we plan to continue to deal with the different types of questions and focus on questions that currently have low performance, such as the \"why\" questions. We also plan to test how our models could be applied to other datasets of machine understanding."}, {"heading": "6 ACKNOWLEDGMENTS", "text": "We thank Pranav Rajpurkar for testing our model on the hidden test dataset and Percy Liang for helping with the Dockerfile for Codalab."}], "references": [{"title": "A thorough examination of the CNN/Daily Mail reading comprehension task", "author": ["Danqi Chen", "Jason Bolton", "Christopher D. Manning"], "venue": "In Proceedings of the Conference on Association for Computational Linguistics,", "citeRegEx": "Chen et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Chen et al\\.", "year": 2016}, {"title": "Consensus attention-based neural networks for chinese reading comprehension", "author": ["Yiming Cui", "Ting Liu", "Zhipeng Chen", "Shijin Wang", "Guoping Hu"], "venue": "In arXiv preprint arXiv:1607.02250,", "citeRegEx": "Cui et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Cui et al\\.", "year": 2016}, {"title": "Incorporating copying mechanism in sequence-to-sequence learning", "author": ["Jiatao Gu", "Zhengdong Lu", "Hang Li", "Victor O.K. Li"], "venue": "In Proceedings of the Conference on Association for Computational Linguistics,", "citeRegEx": "Gu et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Gu et al\\.", "year": 2016}, {"title": "Teaching machines to read and comprehend", "author": ["Karl Moritz Hermann", "Tomas Kocisky", "Edward Grefenstette", "Lasse Espeholt", "Will Kay", "Mustafa Suleyman", "Phil Blunsom"], "venue": "In Proceedings of the Conference on Advances in Neural Information Processing Systems,", "citeRegEx": "Hermann et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Hermann et al\\.", "year": 2015}, {"title": "WIKIREADING: A novel large-scale language understanding task over wikipedia", "author": ["Daniel Hewlett", "Alexandre Lacoste", "Llion Jones", "Illia Polosukhin", "Andrew Fandrianto", "Jay Han", "Matthew Kelcey", "David Berthelot"], "venue": "In Proceedings of the Conference on Association for Computational Linguistics,", "citeRegEx": "Hewlett et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Hewlett et al\\.", "year": 2016}, {"title": "The Goldilocks principle: Reading children\u2019s books with explicit memory representations", "author": ["Felix Hill", "Antoine Bordes", "Sumit Chopra", "Jason Weston"], "venue": "In Proceedings of the International Conference on Learning Representations,", "citeRegEx": "Hill et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Hill et al\\.", "year": 2016}, {"title": "Long short-term memory", "author": ["Sepp Hochreiter", "J\u00fcrgen Schmidhuber"], "venue": "Neural computation,", "citeRegEx": "Hochreiter and Schmidhuber.,? \\Q1997\\E", "shortCiteRegEx": "Hochreiter and Schmidhuber.", "year": 1997}, {"title": "Text understanding with the attention sum reader network", "author": ["Rudolf Kadlec", "Martin Schmid", "Ondrej Bajgar", "Jan Kleindienst"], "venue": "In Proceedings of the Conference on Association for Computational Linguistics,", "citeRegEx": "Kadlec et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Kadlec et al\\.", "year": 2016}, {"title": "Adam: A method for stochastic optimization", "author": ["Diederik Kingma", "Jimmy Ba"], "venue": "In Proceedings of the International Conference on Learning Representations,", "citeRegEx": "Kingma and Ba.,? \\Q2015\\E", "shortCiteRegEx": "Kingma and Ba.", "year": 2015}, {"title": "Ask me anything: Dynamic memory networks for natural language processing", "author": ["Ankit Kumar", "Ozan Irsoy", "Jonathan Su", "James Bradbury", "Robert English", "Brian Pierce", "Peter Ondruska", "Ishaan Gulrajani", "Richard Socher"], "venue": "In Proceedings of the International Conference on Machine Learning,", "citeRegEx": "Kumar et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Kumar et al\\.", "year": 2016}, {"title": "GloVe: Global vectors for word representation", "author": ["Jeffrey Pennington", "Richard Socher", "Christopher D Manning"], "venue": "In Proceedings of the Conference on Empirical Methods in Natural Language Processing,", "citeRegEx": "Pennington et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Pennington et al\\.", "year": 2014}, {"title": "SQuAD: 100,000+ questions for machine comprehension of text", "author": ["Pranav Rajpurkar", "Jian Zhang", "Konstantin Lopyrev", "Percy Liang"], "venue": "In Proceedings of the Conference on Empirical Methods in Natural Language Processing,", "citeRegEx": "Rajpurkar et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Rajpurkar et al\\.", "year": 2016}, {"title": "MCTest: A challenge dataset for the open-domain machine comprehension of text", "author": ["Matthew Richardson", "Christopher JC Burges", "Erin Renshaw"], "venue": "In Proceedings of the Conference on Empirical Methods in Natural Language Processing,", "citeRegEx": "Richardson et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Richardson et al\\.", "year": 2013}, {"title": "End-to-end memory networks", "author": ["Sainbayar Sukhbaatar", "Jason Weston", "Rob Fergus"], "venue": "In Proceedings of the Conference on Advances in neural information processing systems,", "citeRegEx": "Sukhbaatar et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Sukhbaatar et al\\.", "year": 2015}, {"title": "MovieQA: Understanding stories in movies through question-answering", "author": ["Makarand Tapaswi", "Yukun Zhu", "Rainer Stiefelhagen", "Antonio Torralba", "Raquel Urtasun", "Sanja Fidler"], "venue": "In Proceedings of IEEE Conference on Computer Vision and Pattern Recognition,", "citeRegEx": "Tapaswi et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Tapaswi et al\\.", "year": 2016}, {"title": "Natural language comprehension with the EpiReader", "author": ["Adam Trischler", "Zheng Ye", "Xingdi Yuan", "Kaheer Suleman"], "venue": "In Proceedings of the Conference on Empirical Methods in Natural Language Processing,", "citeRegEx": "Trischler et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Trischler et al\\.", "year": 2016}, {"title": "Learning natural language inference with LSTM", "author": ["Shuohang Wang", "Jing Jiang"], "venue": "In Proceedings of the Conference on the North American Chapter of the Association for Computational Linguistics,", "citeRegEx": "Wang and Jiang.,? \\Q2016\\E", "shortCiteRegEx": "Wang and Jiang.", "year": 2016}, {"title": "Towards AI-complete question answering: A set of prerequisite toy tasks", "author": ["Jason Weston", "Antoine Bordes", "Sumit Chopra", "Alexander M Rush", "Bart van Merri\u00ebnboer", "Armand Joulin", "Tomas Mikolov"], "venue": "In Proceedings of the International Conference on Learning Representations,", "citeRegEx": "Weston et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Weston et al\\.", "year": 2016}, {"title": "Attention-based convolutional neural network for machine comprehension", "author": ["Wenpeng Yin", "Sebastian Ebert", "Hinrich Sch\u00fctze"], "venue": "arXiv preprint arXiv:1602.04341,", "citeRegEx": "Yin et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Yin et al\\.", "year": 2016}], "referenceMentions": [{"referenceID": 11, "context": "Our experiments show that both of our two models substantially outperform the best results obtained by Rajpurkar et al. (2016) using logistic regression and manually crafted features.", "startOffset": 103, "endOffset": 127}, {"referenceID": 12, "context": "While the ability of a machine to understand text can be assessed in many different ways, in recent years, several benchmark datasets have been created to focus on answering questions as a way to evaluate machine comprehension (Richardson et al., 2013; Hermann et al., 2015; Hill et al., 2016; Weston et al., 2016; Rajpurkar et al., 2016).", "startOffset": 227, "endOffset": 338}, {"referenceID": 3, "context": "While the ability of a machine to understand text can be assessed in many different ways, in recent years, several benchmark datasets have been created to focus on answering questions as a way to evaluate machine comprehension (Richardson et al., 2013; Hermann et al., 2015; Hill et al., 2016; Weston et al., 2016; Rajpurkar et al., 2016).", "startOffset": 227, "endOffset": 338}, {"referenceID": 5, "context": "While the ability of a machine to understand text can be assessed in many different ways, in recent years, several benchmark datasets have been created to focus on answering questions as a way to evaluate machine comprehension (Richardson et al., 2013; Hermann et al., 2015; Hill et al., 2016; Weston et al., 2016; Rajpurkar et al., 2016).", "startOffset": 227, "endOffset": 338}, {"referenceID": 17, "context": "While the ability of a machine to understand text can be assessed in many different ways, in recent years, several benchmark datasets have been created to focus on answering questions as a way to evaluate machine comprehension (Richardson et al., 2013; Hermann et al., 2015; Hill et al., 2016; Weston et al., 2016; Rajpurkar et al., 2016).", "startOffset": 227, "endOffset": 338}, {"referenceID": 11, "context": "While the ability of a machine to understand text can be assessed in many different ways, in recent years, several benchmark datasets have been created to focus on answering questions as a way to evaluate machine comprehension (Richardson et al., 2013; Hermann et al., 2015; Hill et al., 2016; Weston et al., 2016; Rajpurkar et al., 2016).", "startOffset": 227, "endOffset": 338}, {"referenceID": 12, "context": "In some datasets, a question is a multiple choice question, whose correct answer is to be chosen from a set of provided candidate answers (Richardson et al., 2013; Hill et al., 2016).", "startOffset": 138, "endOffset": 182}, {"referenceID": 5, "context": "In some datasets, a question is a multiple choice question, whose correct answer is to be chosen from a set of provided candidate answers (Richardson et al., 2013; Hill et al., 2016).", "startOffset": 138, "endOffset": 182}, {"referenceID": 3, "context": "Alternatively, in some other datasets, a question does not provide candidate answers and the machine is expected to answer the question using any token or sequence of tokens from the given text (Hermann et al., 2015; Rajpurkar et al., 2016).", "startOffset": 194, "endOffset": 240}, {"referenceID": 11, "context": "Alternatively, in some other datasets, a question does not provide candidate answers and the machine is expected to answer the question using any token or sequence of tokens from the given text (Hermann et al., 2015; Rajpurkar et al., 2016).", "startOffset": 194, "endOffset": 240}, {"referenceID": 3, "context": "Moreover, unlike some other datasets whose questions and answers were created automatically in Cloze style (Hermann et al., 2015; Hill et al., 2016), the questions and answers in SQuAD were created by humans through crowdsourcing, which makes the dataset more realistic.", "startOffset": 107, "endOffset": 148}, {"referenceID": 5, "context": "Moreover, unlike some other datasets whose questions and answers were created automatically in Cloze style (Hermann et al., 2015; Hill et al., 2016), the questions and answers in SQuAD were created by humans through crowdsourcing, which makes the dataset more realistic.", "startOffset": 107, "endOffset": 148}, {"referenceID": 3, "context": "Alternatively, in some other datasets, a question does not provide candidate answers and the machine is expected to answer the question using any token or sequence of tokens from the given text (Hermann et al., 2015; Rajpurkar et al., 2016). Presumably, questions without given candidate answers are more challenging because the candidate answer set essentially includes all possible tokens or token sequences from the text and is thus much larger. Furthermore, questions whose answers span multiple tokens are more challenging than those with single-token answers. The Stanford Question Answering Dataset (SQuAD) introduced recently by Rajpurkar et al. (2016) contains such challenging questions, i.", "startOffset": 195, "endOffset": 661}, {"referenceID": 3, "context": "Recently, with the advances of applying neural network models in NLP, there has been much interest in building end-to-end neural architectures for various NLP tasks, including several pieces of work on machine comprehension (Hermann et al., 2015; Hill et al., 2016; Yin et al., 2016; Kadlec et al., 2016; Cui et al., 2016).", "startOffset": 224, "endOffset": 322}, {"referenceID": 5, "context": "Recently, with the advances of applying neural network models in NLP, there has been much interest in building end-to-end neural architectures for various NLP tasks, including several pieces of work on machine comprehension (Hermann et al., 2015; Hill et al., 2016; Yin et al., 2016; Kadlec et al., 2016; Cui et al., 2016).", "startOffset": 224, "endOffset": 322}, {"referenceID": 18, "context": "Recently, with the advances of applying neural network models in NLP, there has been much interest in building end-to-end neural architectures for various NLP tasks, including several pieces of work on machine comprehension (Hermann et al., 2015; Hill et al., 2016; Yin et al., 2016; Kadlec et al., 2016; Cui et al., 2016).", "startOffset": 224, "endOffset": 322}, {"referenceID": 7, "context": "Recently, with the advances of applying neural network models in NLP, there has been much interest in building end-to-end neural architectures for various NLP tasks, including several pieces of work on machine comprehension (Hermann et al., 2015; Hill et al., 2016; Yin et al., 2016; Kadlec et al., 2016; Cui et al., 2016).", "startOffset": 224, "endOffset": 322}, {"referenceID": 1, "context": "Recently, with the advances of applying neural network models in NLP, there has been much interest in building end-to-end neural architectures for various NLP tasks, including several pieces of work on machine comprehension (Hermann et al., 2015; Hill et al., 2016; Yin et al., 2016; Kadlec et al., 2016; Cui et al., 2016).", "startOffset": 224, "endOffset": 322}, {"referenceID": 5, "context": "However, given the properties of previous machine comprehension datasets, existing end-to-end neural architectures for the task either rely on the candidate answers (Hill et al., 2016; Yin et al., 2016) or assume that the answer is a single token (Hermann et al.", "startOffset": 165, "endOffset": 202}, {"referenceID": 18, "context": "However, given the properties of previous machine comprehension datasets, existing end-to-end neural architectures for the task either rely on the candidate answers (Hill et al., 2016; Yin et al., 2016) or assume that the answer is a single token (Hermann et al.", "startOffset": 165, "endOffset": 202}, {"referenceID": 3, "context": ", 2016) or assume that the answer is a single token (Hermann et al., 2015; Kadlec et al., 2016; Cui et al., 2016), which make these methods unsuitable for the SQuAD dataset.", "startOffset": 52, "endOffset": 113}, {"referenceID": 7, "context": ", 2016) or assume that the answer is a single token (Hermann et al., 2015; Kadlec et al., 2016; Cui et al., 2016), which make these methods unsuitable for the SQuAD dataset.", "startOffset": 52, "endOffset": 113}, {"referenceID": 1, "context": ", 2016) or assume that the answer is a single token (Hermann et al., 2015; Kadlec et al., 2016; Cui et al., 2016), which make these methods unsuitable for the SQuAD dataset.", "startOffset": 52, "endOffset": 113}, {"referenceID": 11, "context": "Experiments on the SQuAD dataset show that our two models both outperform the best performance reported by Rajpurkar et al. (2016).", "startOffset": 107, "endOffset": 131}, {"referenceID": 2, "context": "The pointer mechanism has inspired some recent work on language processing (Gu et al., 2016; Kadlec et al., 2016).", "startOffset": 75, "endOffset": 113}, {"referenceID": 7, "context": "The pointer mechanism has inspired some recent work on language processing (Gu et al., 2016; Kadlec et al., 2016).", "startOffset": 75, "endOffset": 113}, {"referenceID": 11, "context": "To evaluate our model on the test set, we had to submit our trained model to Rajpurkar et al. (2016), who helped us obtain the performance on the test set.", "startOffset": 77, "endOffset": 101}, {"referenceID": 10, "context": "We use word embeddings from GloVe (Pennington et al., 2014) to initialize the model.", "startOffset": 34, "endOffset": 59}, {"referenceID": 11, "context": "The results of our models as well as the results of two baselines given by Rajpurkar et al. (2016) are shown in Table 2.", "startOffset": 75, "endOffset": 99}, {"referenceID": 11, "context": "The results of our models as well as the results of two baselines given by Rajpurkar et al. (2016) are shown in Table 2. We can see that both of our two models have clearly outperformed the logistic regression model by Rajpurkar et al. (2016), which relies on carefully designed features.", "startOffset": 75, "endOffset": 243}, {"referenceID": 2, "context": "For example, Hermann et al. (2015) created questions in Cloze style from CNN and Daily Mail highlights, which are short summaries of the major points of news articles from CNN and Daily Mail.", "startOffset": 13, "endOffset": 35}, {"referenceID": 2, "context": "For example, Hermann et al. (2015) created questions in Cloze style from CNN and Daily Mail highlights, which are short summaries of the major points of news articles from CNN and Daily Mail. The task is to predict a missing token removed from a highlight based on the corresponding news article. Hill et al. (2016) created the Children\u2019s Book Test dataset, which is based on children\u2019s stories.", "startOffset": 13, "endOffset": 316}, {"referenceID": 1, "context": "Cui et al. (2016) released two similar datasets in Chinese, the People Daily dataset and the Children\u2019s Fairy Tale dataset, following the work by Hermann et al.", "startOffset": 0, "endOffset": 18}, {"referenceID": 1, "context": "Cui et al. (2016) released two similar datasets in Chinese, the People Daily dataset and the Children\u2019s Fairy Tale dataset, following the work by Hermann et al. (2015) and by Hill et al.", "startOffset": 0, "endOffset": 168}, {"referenceID": 1, "context": "Cui et al. (2016) released two similar datasets in Chinese, the People Daily dataset and the Children\u2019s Fairy Tale dataset, following the work by Hermann et al. (2015) and by Hill et al. (2016).", "startOffset": 0, "endOffset": 194}, {"referenceID": 11, "context": "Similar to these two datasets, the SQuAD dataset (Rajpurkar et al., 2016) was also created by human annotators.", "startOffset": 49, "endOffset": 73}, {"referenceID": 11, "context": "Richardson et al. (2013) created the well-known MCTest dataset, which is based on short fictional stories and has candidate answers provided for each question.", "startOffset": 0, "endOffset": 25}, {"referenceID": 11, "context": "Richardson et al. (2013) created the well-known MCTest dataset, which is based on short fictional stories and has candidate answers provided for each question. Tapaswi et al. (2016) created the MovieQA dataset, which uses multiple sources of information including video clips, plots, subtitles, scripts and DVS of movies for question answering.", "startOffset": 0, "endOffset": 182}, {"referenceID": 4, "context": "Hewlett et al. (2016) created a WikiReading dataset based on Wikipedia articles and the open knowledge base Wikidata.", "startOffset": 0, "endOffset": 22}, {"referenceID": 4, "context": "Hewlett et al. (2016) created a WikiReading dataset based on Wikipedia articles and the open knowledge base Wikidata. Although the dataset was also created for natural language understanding, the queries in this dataset are not complete natural language sentences but properties such as \u201cgender,\u201d \u201ccountry\u201d and \u201cdate of birth,\u201d and in total the dataset has 884 unique properties. It is therefore quite different from those datasets in which the questions are long sentences and have much more variations. Weston et al. (2016) created a question answering dataset that contains questions requiring different levels of intelligence such as fact chaining, counting, deduction and induction.", "startOffset": 0, "endOffset": 526}, {"referenceID": 3, "context": "A common approach is to use recurrent neural networks (RNNs) to process the given text and the question in order to predict or generate the answers (Hermann et al., 2015).", "startOffset": 148, "endOffset": 170}, {"referenceID": 3, "context": "Attention mechanism is also widely used on top of RNNs in order to match the question with the given passage (Hermann et al., 2015; Chen et al., 2016).", "startOffset": 109, "endOffset": 150}, {"referenceID": 0, "context": "Attention mechanism is also widely used on top of RNNs in order to match the question with the given passage (Hermann et al., 2015; Chen et al., 2016).", "startOffset": 109, "endOffset": 150}, {"referenceID": 7, "context": "Given that answers often come from the given passage, Pointer Network has been adopted in a few studies in order to copy tokens from the given passage as answers (Kadlec et al., 2016; Trischler et al., 2016).", "startOffset": 162, "endOffset": 207}, {"referenceID": 15, "context": "Given that answers often come from the given passage, Pointer Network has been adopted in a few studies in order to copy tokens from the given passage as answers (Kadlec et al., 2016; Trischler et al., 2016).", "startOffset": 162, "endOffset": 207}, {"referenceID": 13, "context": ", 2015) have also been applied to machine comprehension (Sukhbaatar et al., 2015; Kumar et al., 2016; Hill et al., 2016), but its scalability when applied to a large dataset is still an issue.", "startOffset": 56, "endOffset": 120}, {"referenceID": 9, "context": ", 2015) have also been applied to machine comprehension (Sukhbaatar et al., 2015; Kumar et al., 2016; Hill et al., 2016), but its scalability when applied to a large dataset is still an issue.", "startOffset": 56, "endOffset": 120}, {"referenceID": 5, "context": ", 2015) have also been applied to machine comprehension (Sukhbaatar et al., 2015; Kumar et al., 2016; Hill et al., 2016), but its scalability when applied to a large dataset is still an issue.", "startOffset": 56, "endOffset": 120}, {"referenceID": 11, "context": "Experiments on the SQuAD dataset showed that both of our models could substantially outperform the best performance achieved by (Rajpurkar et al., 2016).", "startOffset": 128, "endOffset": 152}], "year": 2016, "abstractText": "Machine comprehension of text is an important problem in natural language processing. A recently released dataset, the Stanford Question Answering Dataset (SQuAD), offers a large number of real questions and their answers created by humans through crowdsourcing. SQuAD provides a challenging testbed for evaluating machine comprehension algorithms, partly because compared with previous datasets, in SQuAD the answers do not come from a small set of candidate answers and they have variable lengths. We propose an end-to-end neural architecture for the task. The architecture is based on match-LSTM, a model we proposed previously for textual entailment, and Pointer Net, a sequence-to-sequence model proposed by Vinyals et al. (2015) to constrain the output tokens to be from the input sequences. We propose two ways of using Pointer Net for our task. Our experiments show that both of our two models substantially outperform the best results obtained by Rajpurkar et al. (2016) using logistic regression and manually crafted features.", "creator": "LaTeX with hyperref package"}}}