{"id": "1608.08710", "review": {"conference": "iclr", "VERSION": "v1", "DATE_OF_SUBMISSION": "31-Aug-2016", "title": "Pruning Filters for Efficient ConvNets", "abstract": "Convolutional Neural Networks (CNNs) are extensively used in image and video recognition, natural language processing and other machine learning applications. The success of CNNs in these areas corresponds with a significant increase in the number of parameters and computation costs. Recent approaches towards reducing these overheads involve pruning and compressing the weights of various layers without hurting the overall CNN performance. However, using model compression to generate sparse CNNs mostly reduces parameters from the fully connected layers and may not significantly reduce the final computation costs.", "histories": [["v1", "Wed, 31 Aug 2016 02:29:59 GMT  (2623kb,D)", "http://arxiv.org/abs/1608.08710v1", null], ["v2", "Thu, 15 Sep 2016 02:12:36 GMT  (2624kb,D)", "http://arxiv.org/abs/1608.08710v2", null], ["v3", "Fri, 10 Mar 2017 17:57:56 GMT  (7203kb,D)", "http://arxiv.org/abs/1608.08710v3", "Published as a conference paper at ICLR 2017"]], "reviews": [], "SUBJECTS": "cs.CV cs.LG", "authors": ["hao li", "asim kadav", "igor durdanovic", "hanan samet", "hans peter graf"], "accepted": true, "id": "1608.08710"}, "pdf": {"name": "1608.08710.pdf", "metadata": {"source": "CRF", "title": "Pruning Filters for Efficient ConvNets", "authors": ["Hao Li", "Asim Kadav", "Igor Durdanovic"], "emails": ["haoli@cs.umd.edu", "asim@nec-labs.com", "igord@nec-labs.com", "hjs@cs.umd.edu", "hpg@nec-labs.com"], "sections": [{"heading": "1 Introduction", "text": "The ImageNet challenge has led to significant advances in the exploration of various architectural choices in CNNs [1, 2]. The general trend in recent years has been that networks have grown deeper, with a general increase in the number of parameters and folding operations. These high-capacity networks have significant inference costs, especially when used with embedded sensors or mobile devices, where computing and power resources can be limited. In addition to accuracy, computing efficiency and small network sizes are critical enabling factors [3] for these applications. In addition, web services such as image search and image classification of APIs, which are based on a time budget that often serves hundreds of thousands of images per second, will benefit significantly from reduced inference time."}, {"heading": "2 Related Work", "text": "There is a significant amount of work to be done to reduce storage capacity and networking costs [7]. Optimal Brain Damage [7] uses a theoretically justified unit of measurement to reduce weights and weights."}, {"heading": "3 Pruning Filters and Feature Maps", "text": "Let ni specify the number of input channels for the ith-Convolutionary Layer and hi / wi specify the height / width of the input tag card. The Convolutionary Layer transforms the 3D input tag cards xi-Rni-hi-wi into the 3D output tag cards xi + 1-Rni-hi + 1-wi + 1, which are used as input tag cards for the next Convolutionary Layer. This is achieved by applying ni + 1 filter Fi, j-Rni-Rni-k-k to the ni input channels in which a filter generates a feature card. Each filter consists of ni two-dimensional cores K-Rk-k (e.g. 3-3-3-3). All filters together form the core matrix Fi-Rni-Rni-k-k-k-k + 1-k-k-k. The operations of the constitutional layer are ni + 1nik-k-k-k (e.g. 3-3-3-3-3). All filters together form the core matrix Fi-Rni-k-k-k-k + 1-k-k-k. The operations of the constitutional layer are cut + 1-k-k-k-k-k-k (e.g. 3-3-3-3-3-3-3-3-3-3-3)."}, {"heading": "3.1 Determining which filters to prune", "text": "To do this, we measure the importance of a filter in each layer by calculating its absolute weight sum \u2211 | Fi, j |. Since the number of input channels ni are the sameacross filters, we also represent the average size of its core weights. This value gives an expectation of the order of magnitude of the output feature card. Filters with lower core weights tend to generate feature cards with weak activations compared to the other filters in this layer. Figure 2 (a) illustrates the distribution of this filter weight for each layer in a VGG-16 network trained on the basis of the CIFAR-10 dataset. Filters in one layer are ordered by their sum of core weights, which is divided by the maximum value of the filters."}, {"heading": "3.2 Determining single layer\u2019s sensitivity to pruning", "text": "To understand the sensitivity of each layer, we cut each layer independently and test the accuracy of the resulting cut network using the validation set. Figure 2 (b) shows the layers that maintain their accuracy when the filters are cut away, and they correspond to filters with larger slopes in Figure 2 (a). On the contrary, layers with relatively shallow slopes (such as Figure 2 (a)) are more sensitive to cutting. We empirically determine the number of filters to be cut for each layer based on their sensitivity to cutting. For layers that are sensitive to cutting, we trim a smaller percentage of these layers or skip them completely."}, {"heading": "3.3 Pruning filters across multiple layers", "text": "We will now discuss how to trim filters across the network. Previous work trims the weights on a layer basis, followed by iterative retraining and compensation for any loss of accuracy [4]. However, understanding how to trim multiple filters at once can be useful: 1) For deep networks, a holistic approach on a layer basis can be extremely time consuming. 2) Trimming layers across the network gives a holistic view of the robustness of the network, resulting in a smaller network. 3) For complex networks, a holistic approach may be necessary. For example, for ResNet, which trims the identities of cards or trims the second layer of each residual blocker result in other layers. To trim the filters over multiple layers, we will use the following two criteria for filter selection: \u2022 Independent trimming determines which filters should be trimmed on each layer, regardless of other layers."}, {"heading": "3.4 Retraining pruned networks to regain accuracy", "text": "There are two strategies for trimming the filters over multiple layers: 1. Trim and retrain once: Trim and retrain filters from multiple layers at once until the original accuracy is restored. 2. Trim and retrain: Trim and retrain filter layer by layer or filter by filter and then iteratively retrain: The model is retrained before trimming the next layer for the weights to adapt to the changes in the trimming process. We find that for layers that are resistant to trimming, the trimming and retraining, if the strategy can be applied to trim significant parts of the network, and any loss of accuracy can be restored by retraining for a short period (less than the initial training times)."}, {"heading": "4 Experiments", "text": "We trim two types of networks: simple CNNs (VGG-16 to CIFAR-10) and residual networks (ResNet-56 / 110 to CIFAR-10 and ResNet-34 to ImageNet). Unlike AlexNet, which is often used to demonstrate efficiency gains, both VGG and residual networks have fewer parameters in the fully connected layers. Therefore, trimming a large percentage of parameters from these networks is a challenge. We implement our filter trimming methods in Torch. When the filters are trimmed, a new model with fewer filters is created and the remaining parameters of the modified layers and unaffected layers are copied into a new model. In addition, when a convolutionary layer is trimmed, the weights of the subsequent batch normalization layer are removed. In order to obtain the basic accuracy for each network, we train each model from scratch and follow the same pre-processing net and resopter parameters as a 0,001 to 0,001 for each school popup time."}, {"heading": "4.1 VGG-16 on CIFAR-10", "text": "VGG-16 [11] is a large-capacity network originally designed for the Imagenet dataset, but recent work [21] has shown that a slightly modified version produces state-of-the-art results on the CIFAR-10 dataset. However, VGG-16 on CIFAR-10 consists of 13 convolutional layers and 2 fully connected layers. We use the model described in [21], but use batch normalization [22] after each convolutional layer and the first linear layer, instead of the dropout layer [23]. Detailed configuration of the model is shown in Table 2. For VGG-16 on CIFAR-10, unlike ImageNet, the fully connected layers do not introduce large portions of the parameters due to the small size of the input channels. If the last convolutional layer is not trimmed, the input layer on the linear layer is trimmed."}, {"heading": "4.2 ResNet-56/110 on CIFAR-10", "text": "ResNets for CIFAR-10 have three levels of residual blocks for function boards with sizes 32 \u00b7 32, 16 \u00b7 16 and 8 \u00b7 8. Each level has the same number of residual blocks. As the function boards increase, the interface layer provides an identity mapping with an additional zero padding for the increased dimensions. Since there is no projection screen for selecting the identity characteristics, we only consider trimming the first layer of the residual block. As shown in Figure 6, most layers are robust for trimming back. For ResNet-110, trimming some individual layers without retraining even improves performance. In addition, layers sensitive to trimming (layers 20, 38 and 54 for ResNet-56, layers 36, 38 and 74 for ResNet-110) will be located near the residual blocks where the ResNet cards are modified."}, {"heading": "4.3 ResNet-34 on ImageNet", "text": "ResNets for ImageNet has four levels of residual blocks for feature cards with sizes of 56 x 56, 28 x 28, 14 x 14 and 7 x 7. ResNet-34 on ImageNet uses the projection shortcuts when scanning the feature cards downwards. First, we trim the first layer of each residual block. Figure 7 shows the sensitivity of the first layer of each residual block. Similar to ResNet-56 / 110, the first and last residual blocks of each stage are more sensitive to the cut than the intermediate blocks (i.e. layer 2, 8, 14, 16, 26, 28, 30, 32). We skip these layers and trim the remaining layers at each stage equally. In Table 1, we compare two configurations of cut percentages for the first three layers: (A) p1 = 30%, p2 = 30%, p3 = 30%, p2 = 60%, 3 = 34%, the first layers printed with the second layer."}, {"heading": "5 Discussion and Conclusions", "text": "Instead of static pruning and then re-training the network, we plan to explore dynamic pruning and re-training. Instead of statically capturing the sensitivity of each layer and then pruning it, we need to carefully reduce the less useful filter weights to zero. In summary, the current CNNs are deep and often loaded with high training and inference costs. In this post, we cut function boards and corresponding filters with relatively small weight sizes from the pre-trained networks and re-train them. The CNNs produced are computationally efficient networks with reduced computing costs and do not require sparse computing libraries. We find that our techniques achieve a reduction of about 10% to 30% in FLOP for VGGNet and Low ResNet without compromising the original accuracy."}], "references": [{"title": "Imagenet Classification with Deep Convolutional Neural Networks", "author": ["A. Krizhevsky", "I. Sutskever", "G.E. Hinton"], "venue": null, "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2012}, {"title": "Rethinking the Inception Architecture for Computer Vision", "author": ["C. Szegedy", "V. Vanhoucke", "S. Ioffe", "J. Shlens", "Z. Wojna"], "venue": "arXiv preprint arXiv:1512.00567", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2015}, {"title": "Learning both Weights and Connections for Efficient Neural Network", "author": ["S. Han", "J. Pool", "J. Tran", "W. Dally"], "venue": null, "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2015}, {"title": "Going Deeper with Convolutions", "author": ["C. Szegedy", "W. Liu", "Y. Jia", "P. Sermanet", "S. Reed", "D. Anguelov", "D. Erhan", "V. Vanhoucke", "A. Rabinovich"], "venue": null, "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2015}, {"title": "EIE: Efficient Inference Engine on Compressed Deep Neural Network", "author": ["S. Han", "X. Liu", "H. Mao", "J. Pu", "A. Pedram", "M.A. Horowitz", "W.J. Dally"], "venue": null, "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2016}, {"title": "Optimal brain damage", "author": ["Y. Le Cun", "J.S. Denker", "S.A. Solla"], "venue": "In: NIPS", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 1989}, {"title": "Second Order Derivatives for Network Pruning: Optimal Brain Surgeon", "author": ["B. Hassibi", "D.G. Stork"], "venue": null, "citeRegEx": "8", "shortCiteRegEx": "8", "year": 1993}, {"title": "Diversity Networks. In: ICLR", "author": ["Z. Mariet", "S. Sra"], "venue": null, "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2016}, {"title": "A Deep Neural Network Compression Pipeline: Pruning, Quantization, Huffman Encoding", "author": ["S. Han", "H. Mao", "W.J. Dally"], "venue": null, "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2016}, {"title": "Very Deep Convolutional Networks for Large-Scale Image Recognition", "author": ["K. Simonyan", "A. Zisserman"], "venue": "ICLR", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2015}, {"title": "SqueezeNet: AlexNet-level accuracy with 50x fewer parameters and < 1MB model size", "author": ["F. Iandola", "M. Moskewicz", "K. Ashraf", "S. Han", "W. Dally", "K. Kurt"], "venue": null, "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2016}, {"title": "Convolutional Neural Networks at Constrained Time Cost", "author": ["K. He", "J. Sun"], "venue": null, "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2015}, {"title": "Deep Residual Learning for Image Recognition", "author": ["K. He", "X. Zhang", "S. Ren", "J. Sun"], "venue": "arXiv preprint arXiv:1512.03385", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2015}, {"title": "Network in Network", "author": ["M. Lin", "Q. Chen", "S. Yan"], "venue": "arXiv preprint arXiv:1312.4400", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2013}, {"title": "Deep Networks with Stochastic Depth", "author": ["G. Huang", "Y. Sun", "Z. Liu", "D. Sedra", "K. Weinberger"], "venue": "arXiv preprint", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2016}, {"title": "Structured Pruning of Deep Convolutional Neural Networks", "author": ["S. Anwar", "K. Hwang", "W. Sung"], "venue": "arXiv preprint arXiv:1512.08571", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2015}, {"title": "Channel-Level Acceleration of Deep Face Representations", "author": ["A. Polyak", "L. Wolf"], "venue": "IEEE Access", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2015}, {"title": "Fast Training of Convolutional Networks through FFTs", "author": ["M. Mathieu", "M. Henaff", "Y. LeCun"], "venue": "arXiv preprint arXiv:1312.5851", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2013}, {"title": "Fast Algorithms for Convolutional Neural Networks", "author": ["A. Lavin"], "venue": "arXiv preprint arXiv:1509.09308", "citeRegEx": "20", "shortCiteRegEx": "20", "year": 2015}, {"title": "Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift", "author": ["S. Ioffe", "C. Szegedy"], "venue": "arXiv preprint arXiv:1502.03167", "citeRegEx": "22", "shortCiteRegEx": "22", "year": 2015}, {"title": "Dropout: A Simple Way to Prevent Neural Networks from Overfitting", "author": ["N. Srivastava", "G. Hinton", "A. Krizhevsky", "I. Sutskever", "R. Salakhutdinov"], "venue": null, "citeRegEx": "23", "shortCiteRegEx": "23", "year": 2014}, {"title": "Visualizing and Understanding Convolutional Networks", "author": ["M.D. Zeiler", "R. Fergus"], "venue": "ECCV", "citeRegEx": "24", "shortCiteRegEx": "24", "year": 2014}], "referenceMentions": [{"referenceID": 0, "context": "The ImageNet challenge has led to significant advancements in exploring various architectural choices in CNNs [1, 2].", "startOffset": 110, "endOffset": 116}, {"referenceID": 1, "context": "For these applications, in addition to accuracy, computational efficiency and small network sizes are crucial enabling factors [3].", "startOffset": 127, "endOffset": 130}, {"referenceID": 2, "context": "Recent work in improving the efficiency of large and deep CNNs includes pruning parameters across these deep networks [4].", "startOffset": 118, "endOffset": 121}, {"referenceID": 3, "context": "Second, pruning parameters creates a sparse networks and modern libraries that provide speedup using sparse operations over CNNs are limited [5].", "startOffset": 141, "endOffset": 144}, {"referenceID": 4, "context": "This reduces the computational overheads from convolution operations and does not involve using any sparse libraries or any specialized hardware [6].", "startOffset": 145, "endOffset": 148}, {"referenceID": 5, "context": "There has been a significant amount of work on reducing the storage and computational cost by model compression [7, 8, 4, 9].", "startOffset": 112, "endOffset": 124}, {"referenceID": 6, "context": "There has been a significant amount of work on reducing the storage and computational cost by model compression [7, 8, 4, 9].", "startOffset": 112, "endOffset": 124}, {"referenceID": 2, "context": "There has been a significant amount of work on reducing the storage and computational cost by model compression [7, 8, 4, 9].", "startOffset": 112, "endOffset": 124}, {"referenceID": 7, "context": "There has been a significant amount of work on reducing the storage and computational cost by model compression [7, 8, 4, 9].", "startOffset": 112, "endOffset": 124}, {"referenceID": 5, "context": "Optimal Brain Damage [7] uses a theoretically justified saliency measure to prune weights and Optimal Brain Surgeon [8] removes unimportant weights determined by the second-order derivative information.", "startOffset": 21, "endOffset": 24}, {"referenceID": 6, "context": "Optimal Brain Damage [7] uses a theoretically justified saliency measure to prune weights and Optimal Brain Surgeon [8] removes unimportant weights determined by the second-order derivative information.", "startOffset": 116, "endOffset": 119}, {"referenceID": 2, "context": "[4, 10] report 90% compression rates on AlexNet [2] and VGGNet [11] by pruning weights with small magnitudes, followed by retraining without hurting the accuracy.", "startOffset": 0, "endOffset": 7}, {"referenceID": 8, "context": "[4, 10] report 90% compression rates on AlexNet [2] and VGGNet [11] by pruning weights with small magnitudes, followed by retraining without hurting the accuracy.", "startOffset": 0, "endOffset": 7}, {"referenceID": 0, "context": "[4, 10] report 90% compression rates on AlexNet [2] and VGGNet [11] by pruning weights with small magnitudes, followed by retraining without hurting the accuracy.", "startOffset": 48, "endOffset": 51}, {"referenceID": 9, "context": "[4, 10] report 90% compression rates on AlexNet [2] and VGGNet [11] by pruning weights with small magnitudes, followed by retraining without hurting the accuracy.", "startOffset": 63, "endOffset": 67}, {"referenceID": 10, "context": "They also demonstrate that the convolutional layers can be compressed and accelerated [12], but with the additionally required sparse BLAS libraries and even additional hardware [6].", "startOffset": 86, "endOffset": 90}, {"referenceID": 4, "context": "They also demonstrate that the convolutional layers can be compressed and accelerated [12], but with the additionally required sparse BLAS libraries and even additional hardware [6].", "startOffset": 178, "endOffset": 181}, {"referenceID": 7, "context": "[9] reduce the network redundancy by identifying a subset of diverse neurons that does not require retraining.", "startOffset": 0, "endOffset": 3}, {"referenceID": 3, "context": "Recent winners of the ImageNet competition have consistently used very deep CNNs [5, 3, 13, 14].", "startOffset": 81, "endOffset": 95}, {"referenceID": 1, "context": "Recent winners of the ImageNet competition have consistently used very deep CNNs [5, 3, 13, 14].", "startOffset": 81, "endOffset": 95}, {"referenceID": 11, "context": "Recent winners of the ImageNet competition have consistently used very deep CNNs [5, 3, 13, 14].", "startOffset": 81, "endOffset": 95}, {"referenceID": 12, "context": "Recent winners of the ImageNet competition have consistently used very deep CNNs [5, 3, 13, 14].", "startOffset": 81, "endOffset": 95}, {"referenceID": 13, "context": "The fully connected layers have also been replaced with average pooling layers [15], which reduces the number of parameters significantly.", "startOffset": 79, "endOffset": 83}, {"referenceID": 12, "context": "Nevertheless, as the networks continue to become deeper, the computational costs of convolutional layers continue to dominate [14].", "startOffset": 126, "endOffset": 130}, {"referenceID": 14, "context": "[16] reduce the training times for deep ResNets by randomly skipping some layers.", "startOffset": 0, "endOffset": 4}, {"referenceID": 15, "context": "Filter pruning has been previously explored in [17, 18].", "startOffset": 47, "endOffset": 55}, {"referenceID": 16, "context": "Filter pruning has been previously explored in [17, 18].", "startOffset": 47, "endOffset": 55}, {"referenceID": 15, "context": "[17] introduce a three-level pruning of the weights and locate the pruning candidates using particle filtering techniques.", "startOffset": 0, "endOffset": 4}, {"referenceID": 16, "context": "[18] detect the less activated feature maps with sample input data for face detection applications.", "startOffset": 0, "endOffset": 4}, {"referenceID": 17, "context": "Other approaches to reduce the convolutional overheads including using FFT based convolutions that compute the convolutions in Fourier domain with low overheads [19].", "startOffset": 161, "endOffset": 165}, {"referenceID": 18, "context": "These costs can be further reduced by using minimal filtering algorithms [20].", "startOffset": 73, "endOffset": 77}, {"referenceID": 2, "context": "Relationship to pruning weights: Pruning filters with low absolute weight sum values is similar to pruning low magnitude weights [4].", "startOffset": 129, "endOffset": 132}, {"referenceID": 2, "context": "Previous work prunes the weights on a layer by layer basis, followed by iteratively retraining and compensating for any loss of accuracy [4].", "startOffset": 137, "endOffset": 140}, {"referenceID": 12, "context": "However, for more complex network architectures such as Residual networks [14], pruning filters may not be straight forward.", "startOffset": 74, "endOffset": 78}, {"referenceID": 12, "context": "To get the baseline accuracies for each network, we train each model from scratch and follow the same pre-processing and hyper-parameters as ResNet [14].", "startOffset": 148, "endOffset": 152}, {"referenceID": 2, "context": "Past work has reported up to 3X original training times to retrain pruned networks [4].", "startOffset": 83, "endOffset": 86}, {"referenceID": 9, "context": "VGG-16 [11] is a large capacity network originally designed for the Imagenet dataset.", "startOffset": 7, "endOffset": 11}, {"referenceID": 19, "context": "We use the model described in [21] but use the Batch Normalization [22] after each convolutional layer and the first linear layer, instead of the Dropout layer [23].", "startOffset": 67, "endOffset": 71}, {"referenceID": 20, "context": "We use the model described in [21] but use the Batch Normalization [22] after each convolutional layer and the first linear layer, instead of the Dropout layer [23].", "startOffset": 160, "endOffset": 164}, {"referenceID": 21, "context": "Unlike previous work [24, 4], we find that the first layer is quite robust to pruning as", "startOffset": 21, "endOffset": 28}, {"referenceID": 2, "context": "Unlike previous work [24, 4], we find that the first layer is quite robust to pruning as", "startOffset": 21, "endOffset": 28}], "year": 2016, "abstractText": "Convolutional Neural Networks (CNNs) are extensively used in image and video recognition, natural language processing and other machine learning applications. The success of CNNs in these areas corresponds with a significant increase in the number of parameters and computation costs. Recent approaches towards reducing these overheads involve pruning and compressing the weights of various layers without hurting the overall CNN performance. However, using model compression to generate sparse CNNs mostly reduces parameters from the fully connected layers and may not significantly reduce the final computation costs. In this paper, we present a compression technique for CNNs, where we prune the filters from CNNs that are identified as having a small effect on the output accuracy. By removing whole planes in the network, together with their connecting convolution kernels, the computational costs are reduced significantly. In contrast to other techniques proposed for pruning networks, this approach does not result in sparse connectivity patterns. Hence, our techniques do not need the support of sparse convolution libraries and can work with the most efficient BLAS operations for matrix multiplications. In our results, we show that even simple filter pruning techniques can reduce inference costs for VGG-16 by up to 34% and ResNet-110 by upto 38% while regaining close to the original accuracy by retraining the networks.", "creator": "LaTeX with hyperref package"}}}