{"id": "1608.08868", "review": {"conference": "EMNLP", "VERSION": "v1", "DATE_OF_SUBMISSION": "31-Aug-2016", "title": "Demographic Dialectal Variation in Social Media: A Case Study of African-American English", "abstract": "Though dialectal language is increasingly abundant on social media, few resources exist for developing NLP tools to handle such language. We conduct a case study of dialectal language in online conversational text by investigating African-American English (AAE) on Twitter. We propose a distantly supervised model to identify AAE-like language from demographics associated with geo-located messages, and we verify that this language follows well-known AAE linguistic phenomena. In addition, we analyze the quality of existing language identification and dependency parsing tools on AAE-like text, demonstrating that they perform poorly on such text compared to text associated with white speakers. We also provide an ensemble classifier for language identification which eliminates this disparity and release a new corpus of tweets containing AAE-like language.", "histories": [["v1", "Wed, 31 Aug 2016 14:12:01 GMT  (80kb,D)", "http://arxiv.org/abs/1608.08868v1", "To be published in EMNLP 2016, 15 pages"]], "COMMENTS": "To be published in EMNLP 2016, 15 pages", "reviews": [], "SUBJECTS": "cs.CL", "authors": ["su lin blodgett", "lisa green", "brendan t o'connor"], "accepted": true, "id": "1608.08868"}, "pdf": {"name": "1608.08868.pdf", "metadata": {"source": "CRF", "title": "Demographic Dialectal Variation in Social Media: A Case Study of African-American English", "authors": ["Su Lin Blodgett", "Lisa Green", "Brendan O\u2019Connor"], "emails": [], "sections": [{"heading": null, "text": "Data and software resources are available at: http: / / slanglab.cs.umass.edu / TwitterAAE (This is an extended version of our EMNLP 2016 paper, including the appendix at the end.)"}, {"heading": "1 Introduction", "text": "These varieties or dialects differ from the standard variant in syntax (sentence structure), phonology (sound structure) and the stock of words and phrases (lexicon)."}, {"heading": "2 Identifying AAE from Demographics", "text": "The presence of AAE on social media and the generation of AAE-like texts for NLP tasks has recently aroused interest in sociolinguistic and natural language research; Jones (2015) shows that non-standard AAE orthography on Twitter is consistent with historical patterns of African-American migration in the US, while J\u00f8rgensen et al. (2015) examine the extent to which it supports sociolinguistic hypotheses about AAE: http: / / linguistlaura.blogspot.co.uk / 2016 / 06 / using-twitter-for-linguistic-research.htmlwell-known sociolinguistics hypotheses about AAE. However, both find an AE-like language on Twitter through keyword searches that may not produce broad corporations that reflect the general use of AAE."}, {"heading": "2.1 Twitter and Census data", "text": "In order to create a corpus of demographically associated dialectical language, we turn to Twitter, whose public messages contain large amounts of casual conversation and dialectical speech (Eisenstein, 2015). It is well known that Twitter can be used to study both geographic dialectical variants 2 and minority languages.3Some methods exist to link messages to authors \"races; one way is to use graphical statistics to identify African-American associated names used in (non-social media) social science studies (Sweeney, 2013; Bertrand Mullainathan, 2003). However, metadata about authors is relatively limited to Twitter and most other social media services, and many names supplied are obviously not real.Instead, we turn to geographic locations and induce a distantly monitored mapping between authors and the demographic neighborhoods in which they live (O Marshall et al, 2010a; al, Eisenstein, 2014; Stewart, 2011b)."}, {"heading": "2.2 Direct Word-Demographic Analysis", "text": "Given a number of messages and demographic data associated with their authors, a number of methods could be used to establish statistical relationships between language and demographics.Direct word demographic analysis methods use the \u03c0 (census) u quantities to calculate word-level statistics in a single iteration.An intuitive approach4See appendix for more details.is for calculating the average demographic data per word. For a token in the corpus indexed by t (across the entire corpus), let u (t) be the author of the message containing this token, and be the word token. The average demographic of the word type w is: 5\u03c0 (softcount) w \u0445 t 1 {wt = w} \u03c0 (census) u (t) \u0445 t 1 {wt = w} We note that terms with the highest \u017ew, AA values (high puff) w numbers are unrelated to Average Quantity of American Population (2013), Average Quantity of American Population (Average Quantity) and Average of American Population (Average Quantity) 7.0000."}, {"heading": "2.3 Mixed-Membership Demographic-Language Model", "text": "Direct word demography analysis provides useful validation that the demographic information may lead to dialectical corporations, and the seeding approach can compile a set of users with heavy dialectical use. However, the approach requires a number of ad hoc thresholds that authors who only occasionally use demographically oriented language cannot grasp, and cannot differentiate language usage at the news level. To address these concerns, we are developing a mixed membership model for demographic issues and language usage in social media. The model directly associates each of the four demographic variables with a topic; i.e., an unigram-language model of vocabulary.6 The model as-5 \u03c0w, which has the taste of \"soft counts\" in multinomial EM. By changing the denominator to compare the results (t), it calculates an unigram model that summarizes the vocabulary."}, {"heading": "3 Linguistic Validation", "text": "Since validating our AAalned text by manual verification is impractical, we turn to the well-studied phonological and syntactical phenomena that traditionally distinguish AAE from SAE. We validate our model by reproducing these phenomena, and document a variety of other ways in which our AA-aligned text differs from SAE."}, {"heading": "3.1 Lexical-Level Variation", "text": "We begin by examining how many AA and white-oriented lexical items differ from a standard dictionary. We used SCOWL's largest list of Level 1 variants as a dictionary, totaling 627,685 words. 8We calculated for each word w in the vocabulary of the model the ratio p (w) = p (w | z = k) p (w | z 6 = k) 8http: / / wordlist.aspell.net / with the p (. |.) probabilities from averaged Gibbs samples of the sufficient statistical counting tables Nwk. We strongly chose A- and white-oriented words as such, with rAA (w) \u2265 2 and rwhite (w) \u2265 2 respectively. We found that 58.2% of the strongly white-oriented words were not in our dictionary, but a full 79.1% of the strongly AA-oriented words were not."}, {"heading": "3.2 Internet-Specific Orthographic Variation", "text": "We performed an \"open vocabulary\" universal analysis by classifying all words in the vocabulary using rAA (w) and searching them and examples of their use. Among the words with high rAA, we observe a number of Internet-specific orthographic variations, which we divide into three types: abbreviations (e.g. llh, kmsl), abbreviations (e.g. dwn, dnt), and spelling variants that do not correlate with the pronunciation of the word (e.g. axx, bxtch). These variations do not reflect features attested in literature; rather, they appear to be purely orthographic variations that are very specific to AAE-speaking communities on the Internet. They may highlight previously unknown linguistic phenomena; for example, we note that thoe (SAE, however) often appears in the role of a discourse marker and does not appear in its standard SAE use (e.g. Girl OutMadison Ofit THE), the new features that are evident in the midst of many phenomena."}, {"heading": "3.3 Phonological Variation", "text": "Many phonological characteristics are closely related to AAE (Green, 2002). Although there is no perfect correlation between orthographic variation and human pronunciation, Eisenstein (2013) shows that some real phonological phenomena, including a number of AAE characteristics, are accurately reflected in orthographic variation on social media. We therefore validate our model by verifying that spellings reflecting known phonological characteristics of AAE closely correspond to the AA topics. We selected 31 variants of SAE words from previous studies of AAE phonology on Twitter (J\u00f8rgensen et al., 2015; Jones, 2015). These variations show a number of certified phonological characteristics of AAE, such as dehotacization (e.g. Brotha), the deletion of initial g and d (e.g. iont) and the realization of spoken th as d (e.g. dey) of one of these five AAA-1 words is the most common of these."}, {"heading": "3.4 Syntactic Variation", "text": "We further confirm our model by verifying that it reproduces known AAE synthesis constructions by examining three well-certified AAE-aspect or preverbal markers: habitual be, future gone, and completive done (Green, 2002). Table 2 shows examples of each construction. To search for the constructions, we tagged the corpora with the ARK Twitter POS tagger (Gimpel et al., 2011; Owoputi et al., 2013), 9 which J\u00f8rgensen et al. (2015) showed similar accuracy rates on both AAE and non-AAE tweets, as opposed to other POS taggers. We searched for each construction by looking for sequences of unigrams and POS tags that characterize the construction; e.g. habitual sequences are searched for the sequences that declare O-be-V and O-b-V."}, {"heading": "4 Lang ID Tools on AAE", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "4.1 Evaluation of Existing Classifiers", "text": "The task of classifying the most important world language in which a message is written is a crucial first step in almost all web or social media word processing pipelines. For example, to analyze the opinions of US Twitter users, one could throw away all non-English tweets before performing an English Twitter emotion analysis. Hughes et al. (2006) Assessment methods for speech recognition in social media are challenging, as messages are short and also use non-standard and multiple (frequently related) languages (Baldwin et al., 2013). Researchers have tried to model the code switch in social media (Rosner and Farrugia, 2007; Solorio and Liu, 2008; Maharjan et al., 2015; King and Abney, 2013), and current workshops have focused on code systems (Solorio et al., 2014) and general language identification (Zubiaga et al., 2014)."}, {"heading": "4.2 Adapting Language Identification for AAE", "text": "Natural language processing tools can be improved to better support dialects; for example, J\u00f8rgensenet al. (2016) uses domain customization methods to improve the POS marking of AAE corpora. In this section, we provide a fix to language identification to correctly identify AAE and other social media messages as English."}, {"heading": "4.2.1 Ensemble Classifier", "text": "We observed that messages where our model suggests a high probability of AAE, white-aligned, or \"hispanic\" -aligned language are almost always written in English, so we construct a simple ensemble classifier by combining it with langid.py.s. For a new message ~ w, we predict their demographic-linguistic proportions with our trained model based on a symmetrical alphabet before demographic-thematic proportions (see appendix for details). The ensemble classifier is as follows: \u2022 Calculate langid.pys prediction y. \u2022 If y is English, accept it as English. \u2022 If y is non-English and at least one of the symbols of the demographic model is included in the vocabulary: Slip on Twitter and return English only if the combined AA, hispanic, and white posterior probabilities are at least 0.9. Otherwise, we return the non-English y."}, {"heading": "4.2.2 Evaluation", "text": "Our analysis from Section 4.1 indicates that this method would correct erroneous false negatives for AAE messages in the total number specified for the model. We further confirm this by testing the classifier on a sample of 2.2 million geolocalized tweets sent in the United States in 2014 that are not in the training set. In addition to performance throughout the sample, we verify the performance of our classifier on messages whose posterior probability of using AA or white-associated terms was greater than 0.8 within the sample that we will refer to in this section as high AA or high white messages. The accuracy of our classifier is generally high, with 100% of all manually annotated samples of 200 messages from each sample. 10 Because we are concerned about the overall retrieval of the system, we imply the retrieval (Table 4) by assuming that all high AA and high white messages are actually English (total number)."}, {"heading": "5 Dependency Parser Evaluation", "text": "Given the lexical and syntactical variation of AAE compared to SAE, we assume that syntactical analysis tools also have differentiated accuracy. J\u00f8rgensen et al. (2015) show this for part-of-speech tagging and find that SAE-trained market participants exhibited varying accuracy on AAE versus non-AAE tweets. We evaluate a publicly available syntactic dependency saver on our AAE and white-aligned corpora. Syntactic parsing for tweets has received some research attention; Foster et al. (2011) create a corpus of constituent trees for English tweets, and Kong et al. (2014) s Tweeboparser is trained on a Twitter corpus commenting on a user-defined, unlabeled dependency formalism; since its data has been consistently sampled from tweets, we expect small differences between demographic groups."}, {"heading": "6 Discussion and Conclusion", "text": "We have presented a remotely monitored probabilistic model that uses demographic correlations of a dialect and its speaker communities to detect dialectical language on Twitter. Our model can also bridge the gap between the performance of NLP tools in terms of dialectic and standard text, which is a case study in dialectization, characterization, and ultimately language-technological adaptation for the dialect. In the case of AAE, dialectization is strongly supported because AAE speakers are strongly associated with a demographic group for which high-precision government records (the U.S. Census) exist that we use to identify speaker communities. The notion of a non-standard dialectization implies that dialectization is in some way underrepresented or underestimated, and therefore inherently difficult to detect; and of course, many other language communities and groups will not necessarily be officially recognized. An interesting direction for future research would be to combine remote supervision with unsupervision."}, {"heading": "A Census demographics (\u00a72.1)", "text": "These four \"races\" (non-Hispanic whites, Hispanics, non-Hispanic African-Americans, and Asians) are often used in sociological studies in the U.S. The census also covers other categories, such as Native Americans. The exact options used in the census are somewhat complicated (e.g. Hispanic is not a \"race\" but a separate variable); in a small minority of cases, these values do not yield a one, so we normalize them for analysis and discard the small fraction of cases where their total is less than 0.5. For simplicity's sake, we sometimes refer to these four variables as races; this is a simplification since the census considers race and ethnicity as separate variables, and the relationship between the actual concepts of race and ethnicity is strained on many levels."}, {"heading": "B Unicode ranges for emoji removal (\u00a74.1)", "text": "We have observed that the emoji symbols often produced strange results, so we have pre-processed the data (improving the predictions of langid.py) to remove emoji and other symbols by removing all characters that fall within certain Unicode ranges. We have not been able to find effective pre-existing solutions so that each level contains a number of blocks of different sizes. Today, the first three levels are most commonly used (Basic Multilingual, Supplemental Ideographic). We remove all characters from the following ranges. \u2022 10000-1FFFF: The entire Supplemental Multilingual Plane that contains emoji and other symbols."}, {"heading": "D Syntactic dependency annotations (\u00a75)", "text": "The SyntaxNet model assumes grammatical relationships based on Stanford Dependencies version 3.3.0; 17 so we tried to comment on messages with this formalism, as described in a 2013 revision to de Marneffe and Manning (2008). 18 For each message, we analyzed it and presented the output in the Brat annotation software19 alongside an uncommented copy of the message, which we added as an uncommented copy of the message. This allowed us to see the proposed analysis to improve annotation speed and conformity with the grammatical standard. In difficult cases, we parsed short, standard English toy sets to confirm which relationships should be used to capture specific syntactic constructs. Sometimes, this clearly contradicted the annotation standards (probably due to the discrepancy between the annotations it was trained against the version of dependencies that we looked at manually); we typically shifted the interpretation of the parameters in such cases."}, {"heading": "E Annotation materials", "text": "We deliver our comments with online materials 22 as well as working instructions on difficult cases. Comments are formatted in plain text by Brat. 22http: / / slanglab.cs.umass.edu / TwitterAAE /"}], "references": [{"title": "Globally normalized transition-based neural networks", "author": ["Daniel Andor", "Chris Alberti", "David Weiss", "Aliaksei Severyn", "Alessandro Presta", "Kuzman Ganchev", "Slav Petrov", "Michael Collins"], "venue": "arXiv preprint arXiv:1603.06042,", "citeRegEx": "Andor et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Andor et al\\.", "year": 2016}, {"title": "On smoothing and inference for topic models", "author": ["Arthur Asuncion", "Max Welling", "Padhraic Smyth", "Yee Whye Teh"], "venue": "In Uncertainty in Artificial Intelligence,", "citeRegEx": "Asuncion et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Asuncion et al\\.", "year": 2009}, {"title": "How noisy social media text, how diffrnt social media sources", "author": ["Timothy Baldwin", "Paul Cook", "Marco Lui", "Andrew MacKinlay", "Li Wang"], "venue": "In International Joint Conference on Natural Language Processing,", "citeRegEx": "Baldwin et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Baldwin et al\\.", "year": 2013}, {"title": "Are Emily and Greg more employable than Lakisha and Jamal? A field experiment on labor market discrimination", "author": ["Marianne Bertrand", "Sendhil Mullainathan"], "venue": "Technical report, National Bureau of Economic Research,", "citeRegEx": "Bertrand and Mullainathan.,? \\Q2003\\E", "shortCiteRegEx": "Bertrand and Mullainathan.", "year": 2003}, {"title": "A fast and accurate dependency parser using neural networks", "author": ["Danqi Chen", "Christopher Manning"], "venue": "In Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP),", "citeRegEx": "Chen and Manning.,? \\Q2014\\E", "shortCiteRegEx": "Chen and Manning.", "year": 2014}, {"title": "Stanford typed dependencies manual", "author": ["M.C. de Marneffe", "C.D. Manning"], "venue": "Technical report, last revised April 2015 edition,", "citeRegEx": "Marneffe and Manning.,? \\Q2008\\E", "shortCiteRegEx": "Marneffe and Manning.", "year": 2008}, {"title": "Universal Stanford dependencies: A cross-linguistic typology", "author": ["Marie-Catherine de Marneffe", "Timothy Dozat", "Natalia Silveira", "Katri Haverinen", "Filip Ginter", "Joakim Nivre", "Christopher D. Manning"], "venue": "In Proceedings of LREC,", "citeRegEx": "Marneffe et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Marneffe et al\\.", "year": 2014}, {"title": "Mapping dialectal variation by querying social media", "author": ["Gabriel Doyle"], "venue": "In Proceedings of EACL,", "citeRegEx": "Doyle.,? \\Q2014\\E", "shortCiteRegEx": "Doyle.", "year": 2014}, {"title": "Phonological factors in social media writing", "author": ["Jacob Eisenstein"], "venue": "In Proc. of the Workshop on Language Analysis in Social Media,", "citeRegEx": "Eisenstein.,? \\Q2013\\E", "shortCiteRegEx": "Eisenstein.", "year": 2013}, {"title": "Identifying regional dialects in online social media", "author": ["Jacob Eisenstein"], "venue": "Handbook of Dialectology. Wiley,", "citeRegEx": "Eisenstein.,? \\Q2015\\E", "shortCiteRegEx": "Eisenstein.", "year": 2015}, {"title": "Sparse additive generative models of text", "author": ["Jacob Eisenstein", "Amr Ahmed", "Eric P. Xing"], "venue": "In Proceedings of ICML,", "citeRegEx": "Eisenstein et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Eisenstein et al\\.", "year": 2011}, {"title": "Identifying code switching in informal Arabic text", "author": ["Heba Elfardy", "Mohamed Al-Badrashiny", "Mona Diab. Aida"], "venue": "Proceedings of EMNLP 2014,", "citeRegEx": "Elfardy et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Elfardy et al\\.", "year": 2014}, {"title": "Mladeni\u0107. Mining the web to create minority language corpora", "author": ["Rayid Ghani", "Rosie Jones", "Dunja"], "venue": "In Proceedings of the Tenth International Conference on Information and Knowledge Management,", "citeRegEx": "Ghani et al\\.,? \\Q2001\\E", "shortCiteRegEx": "Ghani et al\\.", "year": 2001}, {"title": "African American English: A Linguistic Introduction", "author": ["Lisa J. Green"], "venue": null, "citeRegEx": "Green.,? \\Q2002\\E", "shortCiteRegEx": "Green.", "year": 2002}, {"title": "Finding scientific topics", "author": ["T.L. Griffiths", "M. Steyvers"], "venue": "Proceedings of the National Academy of Sciences of the United States of America,", "citeRegEx": "Griffiths and Steyvers.,? \\Q2004\\E", "shortCiteRegEx": "Griffiths and Steyvers.", "year": 2004}, {"title": "Understanding US regional linguistic variation with Twitter data analysis", "author": ["Yuan Huang", "Diansheng Guo", "Alice Kasakoff", "Jack Grieve"], "venue": "Computers, Environment and Urban Systems,", "citeRegEx": "Huang et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Huang et al\\.", "year": 2015}, {"title": "Toward a description of African American Vernacular English dialect regions using \u201cBlack Twitter", "author": ["Taylor Jones"], "venue": "American Speech,", "citeRegEx": "Jones.,? \\Q2015\\E", "shortCiteRegEx": "Jones.", "year": 2015}, {"title": "Learning a POS tagger for AAVE-like language", "author": ["Anna J\u00f8rgensen", "Dirk Hovy", "Anders S\u00f8gaard"], "venue": "In Proceedings of NAACL. Association for Computational Linguistics,", "citeRegEx": "J\u00f8rgensen et al\\.,? \\Q2016\\E", "shortCiteRegEx": "J\u00f8rgensen et al\\.", "year": 2016}, {"title": "Labeling the languages of words in mixed-language documents using weakly supervised methods", "author": ["Ben King", "Steven P Abney"], "venue": "In Proceedings of HLT-NAACL,", "citeRegEx": "King and Abney.,? \\Q2013\\E", "shortCiteRegEx": "King and Abney.", "year": 2013}, {"title": "A dependency parser for tweets", "author": ["Lingpeng Kong", "Nathan Schneider", "Swabha Swayamdipta", "Archna Bhatia", "Chris Dyer", "Noah A. Smith"], "venue": "In Proceedings of the 2014 Conference on Empirical Methods", "citeRegEx": "Kong et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Kong et al\\.", "year": 2014}, {"title": "langid. py: An off-the-shelf language identification tool", "author": ["M. Lui", "T. Baldwin"], "venue": "In Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics (ACL", "citeRegEx": "Lui and Baldwin.,? \\Q2012\\E", "shortCiteRegEx": "Lui and Baldwin.", "year": 2012}, {"title": "Minority language Twitter: Part-of-speech tagging and analysis of Irish tweets", "author": ["Teresa Lynn", "Kevin Scannell", "Eimear Maguire"], "venue": "Proceedings of ACL-IJCNLP 2015,", "citeRegEx": "Lynn et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Lynn et al\\.", "year": 2015}, {"title": "Developing language-tagged corpora for code-switching tweets. In The 9th Linguistic Annotation Workshop held in conjuncion with NAACL", "author": ["Suraj Maharjan", "Elizabeth Blair", "Steven Bethard", "Thamar Solorio"], "venue": null, "citeRegEx": "Maharjan et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Maharjan et al\\.", "year": 2015}, {"title": "Arabic dialect identification using a parallel multidialectal corpus", "author": ["Shervin Malmasi", "Eshrag Refaee", "Mark Dras"], "venue": "In International Conference of the Pacific Association for Computational Linguistics,", "citeRegEx": "Malmasi et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Malmasi et al\\.", "year": 2015}, {"title": "Topic models conditioned on arbitrary features with Dirichlet-Multinomial regression", "author": ["David Mimno", "Andrew McCallum"], "venue": "In Uncertainty in Artificial Intelligence,", "citeRegEx": "Mimno and McCallum.,? \\Q2008\\E", "shortCiteRegEx": "Mimno and McCallum.", "year": 2008}, {"title": "Fightin\u2019 Words: Lexical feature selection and evaluation for identifying the content of political conflict", "author": ["B.L. Monroe", "M.P. Colaresi", "K.M. Quinn"], "venue": "Political Analysis,", "citeRegEx": "Monroe et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Monroe et al\\.", "year": 2008}, {"title": "A mixture model of demographic lexical variation", "author": ["Brendan O\u2019Connor", "Jacob Eisenstein", "Eric P. Xing", "Noah A. Smith"], "venue": "In NIPS Workshop on Machine Learning for Social Computing,", "citeRegEx": "O.Connor et al\\.,? \\Q2010\\E", "shortCiteRegEx": "O.Connor et al\\.", "year": 2010}, {"title": "TweetMotif: Exploratory search and topic summarization for Twitter", "author": ["Brendan O\u2019Connor", "Michel Krieger", "David Ahn"], "venue": "In Proceedings of the International AAAI Conference on Weblogs and Social Media,", "citeRegEx": "O.Connor et al\\.,? \\Q2010\\E", "shortCiteRegEx": "O.Connor et al\\.", "year": 2010}, {"title": "Improved part-of-speech tagging for online conversational text with word clusters", "author": ["Olutobi Owoputi", "Brendan O\u2019Connor", "Chris Dyer", "Kevin Gimpel", "Nathan Schneider", "Noah A. Smith"], "venue": "In Proceedings of NAACL,", "citeRegEx": "Owoputi et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Owoputi et al\\.", "year": 2013}, {"title": "Confounds and consequences in geotagged Twitter data", "author": ["Umashanthi Pavalanathan", "Jacob Eisenstein"], "venue": "In Proceedings of Empirical Methods for Natural Language Processing (EMNLP),", "citeRegEx": "Pavalanathan and Eisenstein.,? \\Q2015\\E", "shortCiteRegEx": "Pavalanathan and Eisenstein.", "year": 2015}, {"title": "African American Vernacular English: Features, Evolution, Educational Implications", "author": ["John Russell Rickford"], "venue": "Wiley-Blackwell,", "citeRegEx": "Rickford.,? \\Q1999\\E", "shortCiteRegEx": "Rickford.", "year": 1999}, {"title": "A tagging algorithm for mixed language identification in a noisy domain", "author": ["Mike Rosner", "Paulseph-John Farrugia"], "venue": "In Eighth Annual Conference of the International Speech Communication Association,", "citeRegEx": "Rosner and Farrugia.,? \\Q2007\\E", "shortCiteRegEx": "Rosner and Farrugia.", "year": 2007}, {"title": "Learning to predict code-switching points", "author": ["Thamar Solorio", "Yang Liu"], "venue": "In Proceedings of the Conference on Empirical Methods in Natural Language Processing,", "citeRegEx": "Solorio and Liu.,? \\Q2008\\E", "shortCiteRegEx": "Solorio and Liu.", "year": 2008}, {"title": "Now we stronger than ever: AfricanAmerican syntax in Twitter", "author": ["Ian Stewart"], "venue": "Proceedings of EACL,", "citeRegEx": "Stewart.,? \\Q2014\\E", "shortCiteRegEx": "Stewart.", "year": 2014}, {"title": "Discrimination in online ad delivery", "author": ["Latanya Sweeney"], "venue": "ACM Queue,", "citeRegEx": "Sweeney.,? \\Q2013\\E", "shortCiteRegEx": "Sweeney.", "year": 2013}, {"title": "Multinomial inverse regression for text analysis", "author": ["Matt Taddy"], "venue": "Journal of the American Statistical Association,", "citeRegEx": "Taddy.,? \\Q2013\\E", "shortCiteRegEx": "Taddy.", "year": 2013}, {"title": "Diwersy. N-gram language models and pos distribution for the identification of Spanish varieties", "author": ["Marcos Zampieri", "Binyam Gebrekidan Gebre", "Sascha"], "venue": "Proceedings of TALN2013,", "citeRegEx": "Zampieri et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Zampieri et al\\.", "year": 2013}, {"title": "Overview of TweetLID: Tweet language identification", "author": ["Arkaitz Zubiaga", "Inaki San Vincente", "Pablo Gamallo", "Jose Ramom Pichel", "Inaki Algeria", "Nora Aranberri", "Aitzol Ezeiza", "Victor Fresno"], "venue": "SEPLN", "citeRegEx": "Zubiaga et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Zubiaga et al\\.", "year": 2014}, {"title": "Spanish Society for Natural Language Processing", "author": ["Tweet Language Identification Workshop", "Girona", "Spain", "September"], "venue": "URL http://ceur-ws. org/Vol-1228/.", "citeRegEx": "Workshop et al\\.,? 2014", "shortCiteRegEx": "Workshop et al\\.", "year": 2014}, {"title": "CVB0 has the advantage of simplicity and rapid convergence; \u03b8\u0302 converges to within", "author": ["Asuncion"], "venue": null, "citeRegEx": "Asuncion,? \\Q2009\\E", "shortCiteRegEx": "Asuncion", "year": 2009}, {"title": "2014)\u2019s view that a tweet consists of a sequence of one or more disconnected utterances. We sought to exclude minor utterances like \u201cNo\u201d in \u201cNo. I do not see it", "author": ["Kong"], "venue": null, "citeRegEx": "Kong,? \\Q2014\\E", "shortCiteRegEx": "Kong", "year": 2014}], "referenceMentions": [{"referenceID": 13, "context": "Not all African-Americans speak AAE, and not all speakers of AAE are African-American; nevertheless, speakers of this variety have close ties with specific communities of African-Americans (Green, 2002).", "startOffset": 189, "endOffset": 202}, {"referenceID": 17, "context": "In fact, its presence in social media is attracting increasing interest for natural language processing (J\u00f8rgensen et al., 2016) and sociolinguistic (Stewart, 2014; Eisenstein, 2015; Jones, 2015) research.", "startOffset": 104, "endOffset": 128}, {"referenceID": 33, "context": ", 2016) and sociolinguistic (Stewart, 2014; Eisenstein, 2015; Jones, 2015) research.", "startOffset": 28, "endOffset": 74}, {"referenceID": 9, "context": ", 2016) and sociolinguistic (Stewart, 2014; Eisenstein, 2015; Jones, 2015) research.", "startOffset": 28, "endOffset": 74}, {"referenceID": 16, "context": ", 2016) and sociolinguistic (Stewart, 2014; Eisenstein, 2015; Jones, 2015) research.", "startOffset": 28, "endOffset": 74}, {"referenceID": 16, "context": "The presence of AAE in social media and the generation of resources of AAE-like text for NLP tasks has attracted recent interest in sociolinguistic and natural language processing research; Jones (2015) shows that nonstandard AAE orthography on Twitter aligns with historical patterns of AfricanAmerican migration in the U.", "startOffset": 190, "endOffset": 203}, {"referenceID": 16, "context": "The presence of AAE in social media and the generation of resources of AAE-like text for NLP tasks has attracted recent interest in sociolinguistic and natural language processing research; Jones (2015) shows that nonstandard AAE orthography on Twitter aligns with historical patterns of AfricanAmerican migration in the U.S., while J\u00f8rgensen et al. (2015) investigate to what extent it supports", "startOffset": 190, "endOffset": 357}, {"referenceID": 17, "context": "More recently, J\u00f8rgensen et al. (2016) generated a large unlabeled corpus of text from hip-hop lyrics, subtitles from The Wire and The Boondocks, and tweets from a region of the southeast U.", "startOffset": 15, "endOffset": 39}, {"referenceID": 9, "context": "In order to create a corpus of demographicallyassociated dialectal language, we turn to Twitter, whose public messages contain large amounts of casual conversation and dialectal speech (Eisenstein, 2015).", "startOffset": 185, "endOffset": 203}, {"referenceID": 34, "context": "Some methods exist to associate messages with authors\u2019 races; one possibility is to use birth record statistics to identify African-American-associated names, which has been used in (non-social media) social science studies (Sweeney, 2013; Bertrand and Mullainathan, 2003).", "startOffset": 224, "endOffset": 272}, {"referenceID": 3, "context": "Some methods exist to associate messages with authors\u2019 races; one possibility is to use birth record statistics to identify African-American-associated names, which has been used in (non-social media) social science studies (Sweeney, 2013; Bertrand and Mullainathan, 2003).", "startOffset": 224, "endOffset": 272}, {"referenceID": 33, "context": "Instead, we turn to geo-location and induce a distantly supervised mapping between authors and the demographics of the neighborhoods they live in (O\u2019Connor et al., 2010a; Eisenstein et al., 2011b; Stewart, 2014).", "startOffset": 146, "endOffset": 211}, {"referenceID": 15, "context": "For example, of American English (Huang et al., 2015; Doyle, 2014).", "startOffset": 33, "endOffset": 66}, {"referenceID": 7, "context": "For example, of American English (Huang et al., 2015; Doyle, 2014).", "startOffset": 33, "endOffset": 66}, {"referenceID": 7, "context": ", 2015; Doyle, 2014). For example, Lynn et al. (2015) develop POS corpora and taggers for Irish tweets; see also related work in \u00a74.", "startOffset": 8, "endOffset": 54}, {"referenceID": 29, "context": "Geolocated users are a particular sample of the userbase (Pavalanathan and Eisenstein, 2015), but we expect it is reasonable to compare users of different races within this group.", "startOffset": 57, "endOffset": 92}, {"referenceID": 31, "context": "We find that terms with the highest \u03c0w,AA values (denoting high average African-American demographics of their authors\u2019 locations) are very non-standard, while Stewart (2014) and Eisenstein (2013) find large \u03c0w,AA associated with certain AAE linguistic features.", "startOffset": 160, "endOffset": 175}, {"referenceID": 8, "context": "We find that terms with the highest \u03c0w,AA values (denoting high average African-American demographics of their authors\u2019 locations) are very non-standard, while Stewart (2014) and Eisenstein (2013) find large \u03c0w,AA associated with certain AAE linguistic features.", "startOffset": 179, "endOffset": 197}, {"referenceID": 14, "context": "We fit the model with collapsed Gibbs sampling (Griffiths and Steyvers, 2004) with repeated sample updates for each token t in the corpus,", "startOffset": 47, "endOffset": 77}, {"referenceID": 25, "context": "This model has broadly similar goals as nonlatent, log-linear generative models of text that condition on document-level covariates (Monroe et al., 2008; Eisenstein et al., 2011a; Taddy, 2013).", "startOffset": 132, "endOffset": 192}, {"referenceID": 35, "context": "This model has broadly similar goals as nonlatent, log-linear generative models of text that condition on document-level covariates (Monroe et al., 2008; Eisenstein et al., 2011a; Taddy, 2013).", "startOffset": 132, "endOffset": 192}, {"referenceID": 24, "context": "This model is also related to topic models where the selection of \u03b8 conditions on covariates (Mimno and McCallum, 2008; Ramage et al., 2011; Roberts et al., 2013), though it is much simpler without full latent topic learning.", "startOffset": 93, "endOffset": 162}, {"referenceID": 13, "context": "Many phonological features are closely associated with AAE (Green, 2002).", "startOffset": 59, "endOffset": 72}, {"referenceID": 8, "context": "While there is not a perfect correlation between orthographic variations and people\u2019s pronunciations, Eisenstein (2013) shows that some genuine phonological phenomena, including a number of AAE features, are accurately reflected in orthographic variation on social media.", "startOffset": 102, "endOffset": 120}, {"referenceID": 16, "context": "We selected 31 variants of SAE words from previous studies of AAE phonology on Twitter (J\u00f8rgensen et al., 2015; Jones, 2015).", "startOffset": 87, "endOffset": 124}, {"referenceID": 30, "context": "dey) (Rickford, 1999).", "startOffset": 5, "endOffset": 21}, {"referenceID": 13, "context": "We further validate our model by verifying that it reproduces well-known AAE syntactic constructions, investigating three well-attested AAE aspectual or preverbal markers: habitual be, future gone, and completive done (Green, 2002).", "startOffset": 218, "endOffset": 231}, {"referenceID": 28, "context": "To search for the constructions, we tagged the corpora using the ARK Twitter POS tagger (Gimpel et al., 2011; Owoputi et al., 2013),9 which J\u00f8rgensen et al.", "startOffset": 88, "endOffset": 131}, {"referenceID": 17, "context": ", 2013),9 which J\u00f8rgensen et al. (2015) show has similar accuracy rates on both AAE and non-AAE tweets, unlike other POS taggers.", "startOffset": 16, "endOffset": 40}, {"referenceID": 2, "context": "(2006) review language identification methods; social media language identification is challenging since messages are short, and also use non-standard and multiple (often related) languages (Baldwin et al., 2013).", "startOffset": 190, "endOffset": 212}, {"referenceID": 31, "context": "Researchers have sought to model code-switching in social media language (Rosner and Farrugia, 2007; Solorio and Liu, 2008; Maharjan et al., 2015; Zampieri et al., 2013; King and Abney, 2013), and recent workshops have focused on code-switching (Solorio et al.", "startOffset": 73, "endOffset": 191}, {"referenceID": 32, "context": "Researchers have sought to model code-switching in social media language (Rosner and Farrugia, 2007; Solorio and Liu, 2008; Maharjan et al., 2015; Zampieri et al., 2013; King and Abney, 2013), and recent workshops have focused on code-switching (Solorio et al.", "startOffset": 73, "endOffset": 191}, {"referenceID": 22, "context": "Researchers have sought to model code-switching in social media language (Rosner and Farrugia, 2007; Solorio and Liu, 2008; Maharjan et al., 2015; Zampieri et al., 2013; King and Abney, 2013), and recent workshops have focused on code-switching (Solorio et al.", "startOffset": 73, "endOffset": 191}, {"referenceID": 36, "context": "Researchers have sought to model code-switching in social media language (Rosner and Farrugia, 2007; Solorio and Liu, 2008; Maharjan et al., 2015; Zampieri et al., 2013; King and Abney, 2013), and recent workshops have focused on code-switching (Solorio et al.", "startOffset": 73, "endOffset": 191}, {"referenceID": 18, "context": "Researchers have sought to model code-switching in social media language (Rosner and Farrugia, 2007; Solorio and Liu, 2008; Maharjan et al., 2015; Zampieri et al., 2013; King and Abney, 2013), and recent workshops have focused on code-switching (Solorio et al.", "startOffset": 73, "endOffset": 191}, {"referenceID": 37, "context": ", 2014) and general language identification (Zubiaga et al., 2014).", "startOffset": 44, "endOffset": 66}, {"referenceID": 23, "context": "For Arabic dialect classification, work has developed corpora in both traditional and Romanized script (Cotterell et al., 2014; Malmasi et al., 2015) and tools that use n-gram and morphological analysis to identify code-switching between dialects and with English (Elfardy et al.", "startOffset": 103, "endOffset": 149}, {"referenceID": 11, "context": ", 2015) and tools that use n-gram and morphological analysis to identify code-switching between dialects and with English (Elfardy et al., 2014).", "startOffset": 122, "endOffset": 144}, {"referenceID": 20, "context": "Lui and Baldwin (2012) develop langid.", "startOffset": 0, "endOffset": 23}, {"referenceID": 12, "context": ", 2012) or minority language data from the web (Ghani et al., 2001).", "startOffset": 47, "endOffset": 67}, {"referenceID": 17, "context": "J\u00f8rgensen et al. (2015) demonstrate this for part-ofspeech tagging, finding that SAE-trained taggers had disparate accuracy on AAE versus non-AAE tweets.", "startOffset": 0, "endOffset": 24}, {"referenceID": 19, "context": "(2011) create a corpus of constituent trees for English tweets, and Kong et al. (2014)\u2019s Tweeboparser is trained on a Twitter corpus annotated with a customized unlabeled dependency formalism; since its data was uniformly sampled from tweets, we expect it may have low disparity between demographic groups.", "startOffset": 68, "endOffset": 87}, {"referenceID": 0, "context": "We focus on widely used syntactic representations, testing the SyntaxNet neural network-based dependency parser (Andor et al., 2016),11 which reports state-of-the-art results, including for web corpora.", "startOffset": 112, "endOffset": 132}, {"referenceID": 4, "context": "We test the Stanford CoreNLP neural network dependency parser (Chen and Manning, 2014) using the english SD model that outputs this formalism;13 its disparity is worse.", "startOffset": 62, "endOffset": 86}, {"referenceID": 4, "context": "We test the Stanford CoreNLP neural network dependency parser (Chen and Manning, 2014) using the english SD model that outputs this formalism;13 its disparity is worse. Soni et al. (2014) used a similar parser14 on Twitter text; our analysis suggests this approach may suffer from errors caused by the parser.", "startOffset": 63, "endOffset": 188}], "year": 2016, "abstractText": "Though dialectal language is increasingly abundant on social media, few resources exist for developing NLP tools to handle such language. We conduct a case study of dialectal language in online conversational text by investigating African-American English (AAE) on Twitter. We propose a distantly supervised model to identify AAE-like language from demographics associated with geo-located messages, and we verify that this language follows well-known AAE linguistic phenomena. In addition, we analyze the quality of existing language identification and dependency parsing tools on AAE-like text, demonstrating that they perform poorly on such text compared to text associated with white speakers. We also provide an ensemble classifier for language identification which eliminates this disparity and release a new corpus of tweets containing AAE-like language. Data and software resources are available at: http://slanglab.cs.umass.edu/TwitterAAE (This is an expanded version of our EMNLP 2016 paper, including the appendix at end.)", "creator": "LaTeX with hyperref package"}}}