{"id": "1608.08967", "review": {"conference": "NIPS", "VERSION": "v1", "DATE_OF_SUBMISSION": "31-Aug-2016", "title": "Robustness of classifiers: from adversarial to random noise", "abstract": "Several recent works have shown that state-of-the-art classifiers are vulnerable to worst-case (i.e., adversarial) perturbations of the datapoints. On the other hand, it has been empirically observed that these same classifiers are relatively robust to random noise. In this paper, we propose to study a \\textit{semi-random} noise regime that generalizes both the random and worst-case noise regimes. We propose the first quantitative analysis of the robustness of nonlinear classifiers in this general noise regime. We establish precise theoretical bounds on the robustness of classifiers in this general regime, which depend on the curvature of the classifier's decision boundary. Our bounds confirm and quantify the empirical observations that classifiers satisfying curvature constraints are robust to random noise. Moreover, we quantify the robustness of classifiers in terms of the subspace dimension in the semi-random noise regime, and show that our bounds remarkably interpolate between the worst-case and random noise regimes. We perform experiments and show that the derived bounds provide very accurate estimates when applied to various state-of-the-art deep neural networks and datasets. This result suggests bounds on the curvature of the classifiers' decision boundaries that we support experimentally, and more generally offers important insights onto the geometry of high dimensional classification problems.", "histories": [["v1", "Wed, 31 Aug 2016 17:54:34 GMT  (6040kb,D)", "http://arxiv.org/abs/1608.08967v1", "Accepted to NIPS 2016"]], "COMMENTS": "Accepted to NIPS 2016", "reviews": [], "SUBJECTS": "cs.LG cs.CV stat.ML", "authors": ["alhussein fawzi", "seyed-mohsen moosavi-dezfooli", "pascal frossard"], "accepted": true, "id": "1608.08967"}, "pdf": {"name": "1608.08967.pdf", "metadata": {"source": "CRF", "title": "Robustness of classifiers: from adversarial to random noise", "authors": ["Alhussein Fawzi"], "emails": [], "sections": [{"heading": "1 Introduction", "text": "Another important feature of a classifier that is often overlooked is its robustness in noisy regimes when data samples are disturbed by noise. In other words, a sufficiently small disturbance of a datapoint should ideally not cause the estimated label of a classifier to change. In these cases, it is critical that the classifiers have good robustness properties. In other words, a sufficiently small disturbance of a datapoint should ideally not cause the label of a classifier to change. State-of-the-art networks have recently shown that they are very unstable to the worst cases of data (or equivalent, advalently small perturbations of a datapoint)."}, {"heading": "2 Definitions and notations", "text": "In fact, it is the case that it is a question of a way and a way, in which it is a question, to what extent it is a question, to what extent it is a question, to what extent it is a question, to what extent it is a question, to what extent it is about the question, to what extent it is about the question, to what extent it is about the question, to what extent it is about the question, to what extent it is about the question, to what extent it is about the question, to what extent it is about the question, to what extent it is about the question, to what extent it is about the question, to what extent it is about the question, to what extent it is about the question, to what extent it is about the question, to what extent it is about the question, to what extent it is about the question, to what extent it is about the question, to what extent it is about the question, to what extent it is about the question, to what extent it is about the question, to what extent it is about the question, to what extent it is about the question, to what extent it is about the question, to what extent it is about the question, to what extent it is about the question, to what extent it is about the question, to what extent it is about the question, to what extent it is about the question, to what it is about the question, to what extent it is about the question, to what it is about the question, to what is about the question, to what extent it is about the question, to what is about the question, to what extent it is about the question, to what is about the question, to what extent it is about the question, to what it is about the question, to what is to what extent it is the question to what is the question, to what extent it is about the question, to what is about the question, to what is to what extent, to what is to what is the question, to what extent, to what extent the question is the question, to what extent, to what extent it is the question, to what is to what extent, to what extent, to what is the question, to what is the question, to what is the question is to what is to what is the question, to what is to what is the question, to what is to what is to what is the question, to what is the question"}, {"heading": "3 Robustness of affine classifiers", "text": "We first assume that f is an affine classification, i.e., f (x) = W > x + b for a particular W = [w1.. wL] and b) RL. The following result indicates a precise relationship between the robustness to semi-random noise and the robustness to adversarial perturbations that are randomly selected. (1 / 2) The random results of the random m-dimensional subspace of Rd, and random is an L-class affiner classifier. (1 + 2) The random results of the random, random, random, random, random, random, random, random, random, random, random, random, random, random, random, random, random, random, random, random, random, random, random, random, random, random,"}, {"heading": "4 Robustness of general classifiers", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "4.1 Decision boundary curvature", "text": "We look at the general case where f is a nonlinear classification. We derive the relationships between the random and semi-random properties that we imagine. (We define the paired boundary between the two ranges, in which only classes i and j are considered.) We define the boundary between the two ranges, in which only classes i and j are considered. Formally, the decision looks like this: Bi, j = {x) Rd: fi (x) \u2212 fj The boundary between two regions of the Rd, namelyRi andRi, where the estimated label of binary classification is respectively i and j. Specifically, we are haveRi = {x) > fj (x) > fj (x)."}, {"heading": "4.2 Robustness to random and semi-random noise", "text": "We are now able to make the decision between the classes k and k, in which the classes k and k are considered. (...) To simplify the notation, we can make the decision between the classes k and k only if the classes k and k are considered as such. (...) To simplify the notation, we need to make the decision between the classes k and k. (...) In the case of the binary classes k and k. (...) in which the semi-random ratio and adversarial (...) the semi-random ratio and adversarial (...) the semi-random ratio between the classes k and k. (...) The semi-random ratio ratio ratio ratio ratio ratio ratio ratio ratio ratio ratio ratio ratio ratio ratio ratio ratio ratio ratio ratio ratio ratio ratio ratio ratio ratio ratio ratio ratio ratio ratio ratio ratio ratio ratio ratio ratio ratio ratio ratio ratio ratio ratio ratio ratio ratio ratio ratio ratio ratio ratio ratio ratio ratio ratio ratio ratio ratio ratio ratio ratio ratio ratio ratio ratio ratio ratio ratio ratio ratio ratio ratio ratio ratio ratio ratio ratio ratio ratio ratio ratio ratio ratio ratio ratio ratio ratio ratio ratio ratio ratio ratio ratio ratio ratio ratio ratio ratio ratio ratio ratio ratio ratio ratio ratio ratio ratio ratio ratio ratio ratio ratio ratio ratio ratio ratio ratio ratio ratio ratio ratio ratio ratio ratio ratio ratio ratio ratio ratio ratio ratio ratio ratio ratio ratio ratio ratio ratio ratio ratio ratio ratio ratio ratio ratio ratio ratio ratio ratio ratio ratio ratio ratio ratio ratio ratio ratio ratio ratio ratio ratio ratio ratio ratio ratio ratio ratio ratio ratio ratio ratio ratio ratio ratio ratio ratio ratio ratio ratio ratio ratio ratio ratio ratio ratio ratio ratio ratio ratio ratio ratio ratio ratio ratio ratio ratio ratio ratio ratio ratio ratio ratio ratio ratio ratio ratio ratio ratio ratio ratio ratio ratio ratio ratio ratio ratio ratio ratio ratio ratio ratio ratio ratio ratio ratio ratio ratio ratio ratio ratio ratio ratio ratio ratio ratio ratio ratio ratio ratio ratio ratio ratio ratio ratio ratio ratio ratio ratio ratio ratio ratio ratio ratio ratio ratio ratio ratio ratio ratio ratio ratio ratio ratio ratio ratio ratio ratio ratio ratio ratio ratio ratio ratio ratio ratio ratio ratio ratio ratio ratio ratio ratio ratio ratio ratio ratio ratio ratio ratio ratio ratio ratio ratio ratio ratio ratio ratio ratio ratio ratio ratio ratio"}, {"heading": "5 Experiments", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "5.1 Experimental results", "text": "\"We have to deal with the question of what the future is like,\" he says. \"We have to deal with the question of what the future is like.\" \"We have to deal with the question of what the future is like.\" \"We have to deal with the question of what the future is like.\" \"We have to deal with the question of what the future is like.\" \"We have to deal with the question of what the future is like.\" \"\" We have to deal with the question of what the future is like. \"\" \"\" We have to deal with the question of what the future is like. \"\" \"\" We have to deal with the future. \"\" \"\" We have to deal with the future. \"\" \"\" \"\" We have to deal with the future. \"\" \"\" \"\" We have to deal with.. \"\" \"\" \"\" \"We have to deal with the future.\" \"\" \"\" We have to deal with....... \"We have to....\" We have to deal with the future. \"\" \"\" We have to deal with...... \"We have to deal with....\" We have to deal with..... \"We have to...\" We have to deal with.... \"We have to deal with...\" We have to deal with.... \"We have to deal with.....\" We have to deal with... \"We have to deal with...\" We have to deal with.... \"We have to deal with....\" We have to deal with... \"We have to deal with....\" We have to deal with. \"We have to deal with..\" The future. \"\" We have to deal with...... \"We have to deal with...\" We have to deal with.. \"We have to deal with the future..\" We have to deal with... \"We have to deal with the future......\" We have to deal with the future.. \"\" he says.... \"\" he says. \"\" We have to deal with the question of what the future. \"he said.\" he said. \"he said.\" We have to the question of"}, {"heading": "6 Conclusion", "text": "In this paper, we have accurately characterized the robustness of classifiers in a novel semi-random noise regime that generalizes the random noise regime, and our limitations relate to the robustness of this system in the face of hostile interference. Our limitations depend on the curvature of the decision limit, the data dimension, and the dimension of the subspace to which the interference belongs. In particular, our results show that classifiers are robust in high-dimensional classification problems when there is little curvature of the decision limit to random noise (even if the robustness to opposite interference is relatively low). Furthermore, our results for semi-random noise, which is predominantly random and only slightly counterproductive (i.e. the subspace dimension is small), show that the subspace dimension of papers remains susceptible to such interference."}, {"heading": "Acknowledgments", "text": "We thank the anonymous reviewers for their helpful comments. We thank Omar Fawzi and Louis Merlin for the fruitful discussions. We also thank NVIDIA Corporation for their support with the donation of the Tesla K40 GPU used for this research. This work was partially supported by the Hasler Foundation, Switzerland, within the framework of the CORA project."}], "references": [{"title": "Robust optimization in machine learning", "author": ["C. Caramanis", "S. Mannor", "H. Xu"], "venue": "Optimization for machine learning,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2012}, {"title": "Return of the devil in the details: Delving deep into convolutional nets", "author": ["K. Chatfield", "K. Simonyan", "A. Vedaldi", "A. Zisserman"], "venue": "In British Machine Vision Conference", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2014}, {"title": "An elementary proof of a theorem of johnson and lindenstrauss", "author": ["S. Dasgupta", "A. Gupta"], "venue": "Random Structures & Algorithms,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2003}, {"title": "Analysis of classifiers\u2019 robustness to adversarial perturbations. CoRR, abs/1502.02590", "author": ["A. Fawzi", "O. Fawzi", "P. Frossard"], "venue": null, "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2015}, {"title": "Manitest: Are classifiers really invariant", "author": ["A. Fawzi", "P. Frossard"], "venue": "In British Machine Vision Conference (BMVC),", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2015}, {"title": "Explaining and harnessing adversarial examples", "author": ["I.J. Goodfellow", "J. Shlens", "C. Szegedy"], "venue": "In International Conference on Learning Representations (ICLR)", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2015}, {"title": "Towards deep neural network architectures robust to adversarial examples. arXiv preprint arXiv:1412.5068", "author": ["S. Gu", "L. Rigazio"], "venue": null, "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2014}, {"title": "Deep neural networks for acoustic modeling in speech recognition: The shared views of four research groups", "author": ["G.E. Hinton", "L. Deng", "D. Yu", "G.E. Dahl", "A. Mohamed", "N. Jaitly", "A. Senior", "V. Vanhoucke", "P. Nguyen", "T.N. Sainath", "B. Kingsbury"], "venue": "IEEE Signal Process. Mag.,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2012}, {"title": "Learning with a strong adversary. CoRR, abs/1511.03034", "author": ["R. Huang", "B. Xu", "D. Schuurmans", "C. Szepesv\u00e1ri"], "venue": null, "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2015}, {"title": "Imagenet classification with deep convolutional neural networks. In Advances in neural information processing systems (NIPS), pages 1097\u20131105", "author": ["A. Krizhevsky", "I. Sutskever", "G.E. Hinton"], "venue": null, "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2012}, {"title": "A robust minimax approach to classification", "author": ["G. Lanckriet", "L. Ghaoui", "C. Bhattacharyya", "M. Jordan"], "venue": "The Journal of Machine Learning Research,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2003}, {"title": "Manifolds and differential geometry, volume 107", "author": ["J.M. Lee"], "venue": null, "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2009}, {"title": "Foveation-based mechanisms alleviate adversarial examples. arXiv preprint arXiv:1511.06292", "author": ["Y. Luo", "X. Boix", "G. Roig", "T. Poggio", "Q. Zhao"], "venue": null, "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2015}, {"title": "Deepfool: a simple and accurate method to fool deep neural networks", "author": ["Moosavi-Dezfooli", "S.-M", "A. Fawzi", "P. Frossard"], "venue": "In IEEE Conference on Computer Vision and Pattern Recognition (CVPR)", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2016}, {"title": "Adversarial manipulation of deep representations", "author": ["S. Sabour", "Y. Cao", "F. Faghri", "D.J. Fleet"], "venue": "In International Conference on Learning Representations (ICLR)", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2016}, {"title": "Understanding adversarial training: Increasing local stability of neural nets through robust optimization. arXiv preprint arXiv:1511.05432", "author": ["U. Shaham", "Y. Yamada", "S. Negahban"], "venue": null, "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2015}, {"title": "Very deep convolutional networks for large-scale image recognition", "author": ["K. Simonyan", "A. Zisserman"], "venue": "In International Conference on Learning Representations (ICLR)", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2014}, {"title": "Intriguing properties of neural networks", "author": ["C. Szegedy", "W. Zaremba", "I. Sutskever", "J. Bruna", "D. Erhan", "I. Goodfellow", "R. Fergus"], "venue": "In International Conference on Learning Representations (ICLR)", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2014}, {"title": "Exploring the space of adversarial images", "author": ["P. Tabacof", "E. Valle"], "venue": "IEEE International Joint Conference on Neural Networks", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2016}, {"title": "Robustness and regularization of support vector machines", "author": ["H. Xu", "C. Caramanis", "S. Mannor"], "venue": "The Journal of Machine Learning Research,", "citeRegEx": "20", "shortCiteRegEx": "20", "year": 2009}, {"title": "Suppressing the unusual: towards robust cnns using symmetric activation functions", "author": ["Q. Zhao", "L.D. Griffin"], "venue": "arXiv preprint arXiv:1603.05145", "citeRegEx": "21", "shortCiteRegEx": "21", "year": 2016}], "referenceMentions": [{"referenceID": 9, "context": "State-of-the-art classifiers, especially deep networks, have shown impressive classification performance on many challenging benchmarks in visual tasks [10] and speech processing [8].", "startOffset": 152, "endOffset": 156}, {"referenceID": 7, "context": "State-of-the-art classifiers, especially deep networks, have shown impressive classification performance on many challenging benchmarks in visual tasks [10] and speech processing [8].", "startOffset": 179, "endOffset": 182}, {"referenceID": 17, "context": "State-of-the-art deep neural networks have recently been shown to be very unstable to worst-case perturbations of the data (or equivalently, adversarial perturbations) [18].", "startOffset": 168, "endOffset": 172}, {"referenceID": 17, "context": "Despite the importance of this result, the worst-case noise regime that is studied in [18] only represents a very specific type of noise.", "startOffset": 86, "endOffset": 90}, {"referenceID": 19, "context": "The robustness properties of SVM classifiers have been studied in [20] for example, and robust optimization approaches for constructing robust classifiers have been proposed to minimize the worst possible empirical error under noise disturbance [1, 11].", "startOffset": 66, "endOffset": 70}, {"referenceID": 0, "context": "The robustness properties of SVM classifiers have been studied in [20] for example, and robust optimization approaches for constructing robust classifiers have been proposed to minimize the worst possible empirical error under noise disturbance [1, 11].", "startOffset": 245, "endOffset": 252}, {"referenceID": 10, "context": "The robustness properties of SVM classifiers have been studied in [20] for example, and robust optimization approaches for constructing robust classifiers have been proposed to minimize the worst possible empirical error under noise disturbance [1, 11].", "startOffset": 245, "endOffset": 252}, {"referenceID": 17, "context": "More recently, following the recent results on the instability of deep neural networks to worst-case perturbations [18], several works have provided explanations of the phenomenon [4, 6, 15, 19], and designed more robust networks [7, 9, 21, 14, 16, 13].", "startOffset": 115, "endOffset": 119}, {"referenceID": 3, "context": "More recently, following the recent results on the instability of deep neural networks to worst-case perturbations [18], several works have provided explanations of the phenomenon [4, 6, 15, 19], and designed more robust networks [7, 9, 21, 14, 16, 13].", "startOffset": 180, "endOffset": 194}, {"referenceID": 5, "context": "More recently, following the recent results on the instability of deep neural networks to worst-case perturbations [18], several works have provided explanations of the phenomenon [4, 6, 15, 19], and designed more robust networks [7, 9, 21, 14, 16, 13].", "startOffset": 180, "endOffset": 194}, {"referenceID": 14, "context": "More recently, following the recent results on the instability of deep neural networks to worst-case perturbations [18], several works have provided explanations of the phenomenon [4, 6, 15, 19], and designed more robust networks [7, 9, 21, 14, 16, 13].", "startOffset": 180, "endOffset": 194}, {"referenceID": 18, "context": "More recently, following the recent results on the instability of deep neural networks to worst-case perturbations [18], several works have provided explanations of the phenomenon [4, 6, 15, 19], and designed more robust networks [7, 9, 21, 14, 16, 13].", "startOffset": 180, "endOffset": 194}, {"referenceID": 6, "context": "More recently, following the recent results on the instability of deep neural networks to worst-case perturbations [18], several works have provided explanations of the phenomenon [4, 6, 15, 19], and designed more robust networks [7, 9, 21, 14, 16, 13].", "startOffset": 230, "endOffset": 252}, {"referenceID": 8, "context": "More recently, following the recent results on the instability of deep neural networks to worst-case perturbations [18], several works have provided explanations of the phenomenon [4, 6, 15, 19], and designed more robust networks [7, 9, 21, 14, 16, 13].", "startOffset": 230, "endOffset": 252}, {"referenceID": 20, "context": "More recently, following the recent results on the instability of deep neural networks to worst-case perturbations [18], several works have provided explanations of the phenomenon [4, 6, 15, 19], and designed more robust networks [7, 9, 21, 14, 16, 13].", "startOffset": 230, "endOffset": 252}, {"referenceID": 13, "context": "More recently, following the recent results on the instability of deep neural networks to worst-case perturbations [18], several works have provided explanations of the phenomenon [4, 6, 15, 19], and designed more robust networks [7, 9, 21, 14, 16, 13].", "startOffset": 230, "endOffset": 252}, {"referenceID": 15, "context": "More recently, following the recent results on the instability of deep neural networks to worst-case perturbations [18], several works have provided explanations of the phenomenon [4, 6, 15, 19], and designed more robust networks [7, 9, 21, 14, 16, 13].", "startOffset": 230, "endOffset": 252}, {"referenceID": 12, "context": "More recently, following the recent results on the instability of deep neural networks to worst-case perturbations [18], several works have provided explanations of the phenomenon [4, 6, 15, 19], and designed more robust networks [7, 9, 21, 14, 16, 13].", "startOffset": 230, "endOffset": 252}, {"referenceID": 18, "context": "In [19], the authors provide an interesting empirical analysis of the adversarial instability, and show that adversarial examples are not isolated points, but rather occupy dense regions of the pixel space.", "startOffset": 3, "endOffset": 7}, {"referenceID": 4, "context": "In [5], state-of-the-art classifiers are shown to be vulnerable to geometrically constrained adversarial examples.", "startOffset": 3, "endOffset": 6}, {"referenceID": 3, "context": "In [4], a formal relation between the robustness to random noise, and the worst-case robustness is established in the case of linear classifiers.", "startOffset": 3, "endOffset": 6}, {"referenceID": 3, "context": "Our result therefore generalizes [4] in many aspects, as we study general nonlinear classifiers, and robustness to semi-random noise.", "startOffset": 33, "endOffset": 36}, {"referenceID": 5, "context": "Finally, it should be noted that the authors in [6] conjecture that the \u201chigh linearity\u201d of classification models explains their instability to adversarial perturbations.", "startOffset": 48, "endOffset": 51}, {"referenceID": 17, "context": "When S = R, r\u2217(x0) := rRd(x0) is the adversarial (or worst-case) perturbation defined in [18], which corresponds to the (unconstrained) perturbation of minimal norm that changes the label of the datapoint x0.", "startOffset": 89, "endOffset": 93}, {"referenceID": 11, "context": "There are many notions of curvature that one can define on hypersurfaces [12].", "startOffset": 73, "endOffset": 77}, {"referenceID": 1, "context": "The VGG-F and VGG-19 are respectively introduced in [2, 17].", "startOffset": 52, "endOffset": 59}, {"referenceID": 16, "context": "The VGG-F and VGG-19 are respectively introduced in [2, 17].", "startOffset": 52, "endOffset": 59}, {"referenceID": 13, "context": "We have used a similar approach to [14] to find subspace minimal perturbations.", "startOffset": 35, "endOffset": 39}, {"referenceID": 17, "context": "It should be noted that the robustness of neural networks to adversarial perturbations has previously been observed empirically in [18], but we provide here a quantitative and generic explanation for this phenomenon.", "startOffset": 131, "endOffset": 135}, {"referenceID": 5, "context": "It should be noted that a related empirical observation was made in [6]; our work however provides a precise quantitative analysis on the relation between the curvature and the robustness in the semi-random noise regime.", "startOffset": 68, "endOffset": 71}], "year": 2016, "abstractText": null, "creator": "LaTeX with hyperref package"}}}