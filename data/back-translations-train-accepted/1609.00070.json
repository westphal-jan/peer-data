{"id": "1609.00070", "review": {"conference": "ACL", "VERSION": "v1", "DATE_OF_SUBMISSION": "1-Sep-2016", "title": "How Much is 131 Million Dollars? Putting Numbers in Perspective with Compositional Descriptions", "abstract": "How much is 131 million US dollars? To help readers put such numbers in context, we propose a new task of automatically generating short descriptions known as perspectives, e.g. \"$131 million is about the cost to employ everyone in Texas over a lunch period\". First, we collect a dataset of numeric mentions in news articles, where each mention is labeled with a set of rated perspectives. We then propose a system to generate these descriptions consisting of two steps: formula construction and description generation. In construction, we compose formulae from numeric facts in a knowledge base and rank the resulting formulas based on familiarity, numeric proximity and semantic compatibility. In generation, we convert a formula into natural language using a sequence-to-sequence recurrent neural network. Our system obtains a 15.2% F1 improvement over a non-compositional baseline at formula construction and a 12.5 BLEU point improvement over a baseline description generation.", "histories": [["v1", "Thu, 1 Sep 2016 00:20:41 GMT  (1726kb,D)", "http://arxiv.org/abs/1609.00070v1", null]], "reviews": [], "SUBJECTS": "cs.CL", "authors": ["arun tejasvi chaganty", "percy liang"], "accepted": true, "id": "1609.00070"}, "pdf": {"name": "1609.00070.pdf", "metadata": {"source": "CRF", "title": "How Much is 131 Million Dollars? Putting Numbers in Perspective with Compositional Descriptions", "authors": ["Arun Tejasvi Chaganty", "Percy Liang"], "emails": ["chaganty@cs.stanford.edu", "pliang@cs.stanford.edu"], "sections": [{"heading": "1 Introduction", "text": "When we cite a number, such as \"Cristiano Ronaldo, the player who has chosen a person\" (Figure 1), it is often difficult to grasp the magnitude of the large (or small) absolute values, such as about $131 million. Studies have shown that providing relative comparisons or perspectives leads to either the cost that everyone incurs during a lunch break. They improve understanding when measured in terms of memory preservation or the capture of outliers (Barrio et al., 2016).Previous work in the HCI community relies either on manually generated perspectives (Barrio et al., 2016) or presents itself on a fact derived from a knowledge base (Chiacchieri, 2013)."}, {"heading": "2 Problem statement", "text": "Input to the perspective generation task is a set of s that contains a numerical mention x: a range of tokens within the set that represent a size with the value x.value and the unit x.unit. In Figure 1, the numerical mention x is \"$131 million,\" x.value = 1.31e8 and x.unit = $. The output is a description y that puts x into perspective. We have access to a knowledge base K with numerical tuples t = (t.value, t.unit, t.description). Table 1 has a few examples of tuples in our knowledge base. Units (e.g. $/ per / yr) are fractions that are composed of either basic units (length, area, volume, mass, time) or ordinal units (e.g. cars, people, etc.). The first step of our task, described in Section 4, is to construct a formula f over numerical tuples in K that has the same value and unit as the numerical mention, mass, time) or ordal units (e.g. cars, people, etc.). The first step of our task, described in Section 4, is to construct a formula f over numerical tuples in K that consists of a unit that has the same numerical value and unit as the numerical mention, and the unit is an arial one, and the unit is an arial mention is an arial one, and the unit is an arial mention of a formula consists of 1."}, {"heading": "3 Dataset construction", "text": "In fact, it is the case that it is a matter of a way in which people are able to determine for themselves what they want and what they want to do. (...) It is not the first time that people are able to determine for themselves. (...) It is also not the first time that people are able to determine for themselves what they want. (...) It is not the first time that people are able to decide for themselves. (...) It is the second time that they are able to do it. (...) It is the second time that people are able to determine for themselves what they want. (...) It is the first time that they are able to determine for themselves. (...) It is the second time that they are able to do it. (...) It is the second time that they do it. (...) It is the second time that they do it. (...) It is the first time that they do it. (...) It is the second time that they do it. (...) It is the first time that they do it."}, {"heading": "4 Formula selection", "text": "In fact, it is in such a way that one sees oneself in a position to live in a country in which most people are able to integrate themselves, and in which most of them are able to live in a country in which they are able to live, to live, to live, to live, to live, to live, to live, to live, to live, to live, to live, to live, to live, to live, to live, to live, to live, to live, to live, to live, to live, to live, to live, to live, to live, to live, to live, to live, to live, to live, to live, to live, to live, to live, to live, to live, to live, to live, to live, to live, to live, to live, to live, to live, to live, to live, to live, to live, to live, to live, to live, to live, to live, to live, to live, to live, to live, to live"}, {"heading": "5 Perspective generation", "text": "Our next goal is to generate natural language descriptions, also known as perspectives, in the face of a formula. Our approach models the task as a sequence-to-sequence translation task of formulas into the natural language. We first describe a rule-based baseline and then describe a recursive neural network (RNN) with an attention-based copy mechanism (Jia and Liang, 2016).Baseline. As a simple approach to generating perspectives, we simply combine tuples in the formula with the neutral prepositions of and for, e.g. \"1 / 5 of the cost of an employee for the population of Texas for the time taken for lunch.\" Sequence-to-sequence sequence sequence sequence sequence sequence sequence sequence sequence sequence sequence sequence sequence sequence sequence sequence sequence sequence sequence sequence sequence sequence sequence sequence sequence sequence sequence sequence sequence sequence sequence sequence sequence is combined with the descriptions of its tuptors and descriptions of its sequence sequence sequence sequence-to-perspective sequence sequence sequence sequence is combined with the model sequence sequence sequence sequence-to-to-perspective sequence sequence sequence sequence (the model of our sequence to-to-sequence sequence is the model of the model of the sequence sequence in *)."}, {"heading": "6 Human evaluation", "text": "In addition to the automatic evaluations for each component of the system, we also conducted a final human evaluation based on an independent set of 211 mentions using the same methodology as in Section 3. Crowdworkers were asked to choose between perspectives generated by our entire system (LR + RNN) and those generated by selecting the numerically narrowest tuples in the knowledge base (BASELINE). They were also asked to indicate whether either both or none of the perspectives described in Section 4 (Table 7c) seemed useful. 8 Table 7 summarizes the results of the evaluation and an error analysis conducted by the authors. Errors were characterized either as errors in the generation (e.g. Table 6) or as violations of the criteria in the selection of good formulas (Table 7c). The other category mainly contains cases where the performance generated by LR + RNN appears reasonable according to the above criteria, but not by a majority of the numerically selected workers, for example, some of the larger quantities described most frequently."}, {"heading": "7 Related work and discussion", "text": "We have proposed a new task of perspective generation. Compositionality is the key component of our approach that enables us to synthesize information across multiple information sources. At the same time, compositionality also poses problems for both formula selection and description generation.On the formula selection side, we need to compile facts that make sense. For semantic compatibility between mention and description, we originally relied on simple word vectors (Mikolov et al., 2013), but more complex forms of semantic relationships on larger text units could yield better outcomes (Bowman et al., 2015).On the description generation side, there is a long series of work in generating natural language descriptions of structured data or logical forms Wong and Mooney (2007); Chen and Mooney (2008); Lu and Ng (2012); Angeli et al. (2010). We are leaning on the latest developments in neural sequence sequence sequence models (Wang et 2014, Bahever; Wang et 2014)."}, {"heading": "Acknowledgments", "text": "We would like to thank Glen Chiacchieri for providing information about the Dictionary of Numbers, Maneesh Agarwala for useful discussions and references, Robin Jia for providing code for the Sequence-to-Sequence RNN and the anonymous reviewer for their constructive feedback. This work was partially supported by the Sloan Research Fellowship for the second author."}], "references": [{"title": "A simple domain-independent probabilistic approach to generation", "author": ["G. Angeli", "P. Liang", "D. Klein."], "venue": "Empirical Methods in Natural Language Processing (EMNLP).", "citeRegEx": "Angeli et al\\.,? 2010", "shortCiteRegEx": "Angeli et al\\.", "year": 2010}, {"title": "Neural machine translation by jointly", "author": ["D. Bahdanau", "K. Cho", "Y. Bengio"], "venue": null, "citeRegEx": "Bahdanau et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Bahdanau et al\\.", "year": 2014}, {"title": "Improving the comprehension of numbers in the news", "author": ["P.J. Barrio", "D.G. Goldstein", "J.M. Hofman."], "venue": "Conference on Human Factors in Computing Systems (CHI).", "citeRegEx": "Barrio et al\\.,? 2016", "shortCiteRegEx": "Barrio et al\\.", "year": 2016}, {"title": "An empirical investigation of statistical significance in NLP", "author": ["T. Berg-Kirkpatrick", "D. Burkett", "D. Klein."], "venue": "Empirical Methods in Natural Language Processing (EMNLP). pages 995\u20131005.", "citeRegEx": "Berg.Kirkpatrick et al\\.,? 2012", "shortCiteRegEx": "Berg.Kirkpatrick et al\\.", "year": 2012}, {"title": "Freebase: a collaboratively created graph database for structuring human knowledge", "author": ["K. Bollacker", "C. Evans", "P. Paritosh", "T. Sturge", "J. Taylor."], "venue": "International Conference on Management of Data (SIGMOD). pages 1247\u2013", "citeRegEx": "Bollacker et al\\.,? 2008", "shortCiteRegEx": "Bollacker et al\\.", "year": 2008}, {"title": "A large annotated corpus for learning natural language inference", "author": ["S. Bowman", "G. Angeli", "C. Potts", "C.D. Manning."], "venue": "Empirical Methods in Natural Language Processing (EMNLP).", "citeRegEx": "Bowman et al\\.,? 2015", "shortCiteRegEx": "Bowman et al\\.", "year": 2015}, {"title": "Learning to sportscast: A test of grounded language acquisition", "author": ["D.L. Chen", "R.J. Mooney."], "venue": "International Conference on Machine Learning (ICML). pages 128\u2013135.", "citeRegEx": "Chen and Mooney.,? 2008", "shortCiteRegEx": "Chen and Mooney.", "year": 2008}, {"title": "Using concrete scales: A practical framework for effective visual depiction of complex measures", "author": ["F. Chevalier", "R. Vuillemot", "G. Gali."], "venue": "IEEE Transactions on Visualization and Computer Graphics 19:2426\u20132435.", "citeRegEx": "Chevalier et al\\.,? 2013", "shortCiteRegEx": "Chevalier et al\\.", "year": 2013}, {"title": "Dictionary of numbers", "author": ["G. Chiacchieri."], "venue": "http://www.dictionaryofnumbers. com/.", "citeRegEx": "Chiacchieri.,? 2013", "shortCiteRegEx": "Chiacchieri.", "year": 2013}, {"title": "Identifying relations for open information extraction", "author": ["A. Fader", "S. Soderland", "O. Etzioni."], "venue": "Empirical Methods in Natural Language Processing (EMNLP).", "citeRegEx": "Fader et al\\.,? 2011", "shortCiteRegEx": "Fader et al\\.", "year": 2011}, {"title": "Data recombination for neural semantic parsing", "author": ["R. Jia", "P. Liang."], "venue": "Association for Computational Linguistics (ACL).", "citeRegEx": "Jia and Liang.,? 2016", "shortCiteRegEx": "Jia and Liang.", "year": 2016}, {"title": "Developing a sense of scale: Looking backward", "author": ["M.G. Jones", "A.R. Taylor."], "venue": "Journal of Research in Science Teaching 46:460\u2013475.", "citeRegEx": "Jones and Taylor.,? 2009", "shortCiteRegEx": "Jones and Taylor.", "year": 2009}, {"title": "Generating personalized spatial analogies for distances and areas", "author": ["Y. Kim", "J. Hullman", "M. Agarwala."], "venue": "Conference on Human Factors in Computing Systems (CHI).", "citeRegEx": "Kim et al\\.,? 2016", "shortCiteRegEx": "Kim et al\\.", "year": 2016}, {"title": "A probabilistic forestto-string model for language generation from typed lambda calculus expressions", "author": ["W. Lu", "H.T. Ng."], "venue": "Empirical Methods in Natural Language Processing (EMNLP). pages 1611\u20131622.", "citeRegEx": "Lu and Ng.,? 2012", "shortCiteRegEx": "Lu and Ng.", "year": 2012}, {"title": "Effective approaches to attention-based neural machine translation", "author": ["M. Luong", "H. Pham", "C.D. Manning."], "venue": "Empirical Methods in Natural Language Processing (EMNLP). pages 1412\u20131421.", "citeRegEx": "Luong et al\\.,? 2015", "shortCiteRegEx": "Luong et al\\.", "year": 2015}, {"title": "Efficient estimation of word representations in vector space", "author": ["T. Mikolov", "K. Chen", "G. Corrado", "Jeffrey."], "venue": "arXiv .", "citeRegEx": "Mikolov et al\\.,? 2013", "shortCiteRegEx": "Mikolov et al\\.", "year": 2013}, {"title": "Innumeracy: Mathematical illiteracy and its consequences", "author": ["J.A. Paulos."], "venue": "Macmillan.", "citeRegEx": "Paulos.,? 1988", "shortCiteRegEx": "Paulos.", "year": 1988}, {"title": "Proofiness: How you\u2019re being fooled by the numbers", "author": ["C. Seife."], "venue": "Penguin.", "citeRegEx": "Seife.,? 2010", "shortCiteRegEx": "Seife.", "year": 2010}, {"title": "Sequence to sequence learning with neural networks", "author": ["I. Sutskever", "O. Vinyals", "Q.V. Le."], "venue": "Advances in Neural Information Processing Systems (NIPS). pages 3104\u20133112.", "citeRegEx": "Sutskever et al\\.,? 2014", "shortCiteRegEx": "Sutskever et al\\.", "year": 2014}, {"title": "Framing of numeric quantities", "author": ["K.H. Teigen."], "venue": "The Wiley Blackwell Handbook of Judgment and Decision Making pages 568\u2013589.", "citeRegEx": "Teigen.,? 2015", "shortCiteRegEx": "Teigen.", "year": 2015}, {"title": "Accuracy of scale conceptions in science: Mental maneuverings across many orders of spatial magnitude", "author": ["T.R. Tretter", "M.G. Jones", "J. Minogue."], "venue": "Journal of Research in Science Teaching 43:1061\u20131085.", "citeRegEx": "Tretter et al\\.,? 2006", "shortCiteRegEx": "Tretter et al\\.", "year": 2006}, {"title": "Building a semantic parser overnight", "author": ["Y. Wang", "J. Berant", "P. Liang."], "venue": "Association for Computational Linguistics (ACL).", "citeRegEx": "Wang et al\\.,? 2015", "shortCiteRegEx": "Wang et al\\.", "year": 2015}, {"title": "Generation by inverting a semantic parser that uses statistical machine translation", "author": ["Y.W. Wong", "R.J. Mooney."], "venue": "Human Language Technology and North American Association for Computational Linguistics (HLT/NAACL).", "citeRegEx": "Wong and Mooney.,? 2007", "shortCiteRegEx": "Wong and Mooney.", "year": 2007}], "referenceMentions": [{"referenceID": 16, "context": "] a $131 million\u201d (Figure 1), it is often difficult to comprehend the scale of large (or small) absolute values like $131 million (Paulos, 1988; Seife, 2010).", "startOffset": 130, "endOffset": 157}, {"referenceID": 17, "context": "] a $131 million\u201d (Figure 1), it is often difficult to comprehend the scale of large (or small) absolute values like $131 million (Paulos, 1988; Seife, 2010).", "startOffset": 130, "endOffset": 157}, {"referenceID": 2, "context": "Studies have shown that providing relative comparisons, or perspectives, such as \u201cabout the cost to employ everyone in Texas over a lunch period\u201d significantly improves comprehension when measured in terms of memory retention or outlier detection (Barrio et al., 2016).", "startOffset": 247, "endOffset": 268}, {"referenceID": 2, "context": "Previous work in the HCI community has relied on either manually generated perspectives (Barrio et al., 2016) or present a fact as is from a knowledge base (Chiacchieri, 2013).", "startOffset": 88, "endOffset": 109}, {"referenceID": 8, "context": ", 2016) or present a fact as is from a knowledge base (Chiacchieri, 2013).", "startOffset": 54, "endOffset": 73}, {"referenceID": 10, "context": "Jones and Taylor (2009) find that students learning to appreciate scale do so mainly by anchoring with familiar concepts, e.", "startOffset": 0, "endOffset": 24}, {"referenceID": 7, "context": "Unitization and anchoring have also been proposed by Chevalier et al. (2013) as the basis of a design methodology for constructing visual perspectives called concrete scales.", "startOffset": 53, "endOffset": 77}, {"referenceID": 20, "context": "Conception of scale quickly fails with quantities that exceed \u201chuman scales\u201d (Tretter et al., 2006): numbers that are significantly away from 1/10 and 10.", "startOffset": 77, "endOffset": 99}, {"referenceID": 20, "context": "The most common technique cited by those who do well at scale cognition tests is reasoning in terms of familiar objects (Tretter et al., 2006; Jones and Taylor, 2009; Chevalier et al., 2013).", "startOffset": 120, "endOffset": 190}, {"referenceID": 11, "context": "The most common technique cited by those who do well at scale cognition tests is reasoning in terms of familiar objects (Tretter et al., 2006; Jones and Taylor, 2009; Chevalier et al., 2013).", "startOffset": 120, "endOffset": 190}, {"referenceID": 7, "context": "The most common technique cited by those who do well at scale cognition tests is reasoning in terms of familiar objects (Tretter et al., 2006; Jones and Taylor, 2009; Chevalier et al., 2013).", "startOffset": 120, "endOffset": 190}, {"referenceID": 12, "context": "their location, it is possible to personalize the chosen tuples (Kim et al., 2016).", "startOffset": 64, "endOffset": 82}, {"referenceID": 15, "context": "The word vectors at the token level are computed using word2vec (Mikolov et al., 2013).", "startOffset": 64, "endOffset": 86}, {"referenceID": 10, "context": "We first describe a rulebased baseline and then describe a recurrent neural network (RNN) with an attention-based copying mechanism (Jia and Liang, 2016).", "startOffset": 132, "endOffset": 153}, {"referenceID": 10, "context": "Our system is based on the model described in Jia and Liang (2016). Given a sequence of input tokens (x = (xi)), the model computes a contextdependent vector (b = (bi)) for each token using a bidirectional RNN with LSTM units.", "startOffset": 46, "endOffset": 67}, {"referenceID": 3, "context": "Significance results are computed by the bootstrap test as described in Berg-Kirkpatrick et al. (2012) using the output of classifiers trained on the entire training set.", "startOffset": 72, "endOffset": 103}, {"referenceID": 10, "context": "We refer the reader to Jia and Liang (2016) for more details.", "startOffset": 23, "endOffset": 44}, {"referenceID": 15, "context": "For semantic compatibility between the mention and description, we have relied on simple word vectors (Mikolov et al., 2013), but more sophisticated forms of semantic relations on larger units of text might yield better results (Bowman et al.", "startOffset": 102, "endOffset": 124}, {"referenceID": 5, "context": ", 2013), but more sophisticated forms of semantic relations on larger units of text might yield better results (Bowman et al., 2015).", "startOffset": 111, "endOffset": 132}, {"referenceID": 18, "context": "We lean on the recent developments of neural sequence-to-sequence models (Sutskever et al., 2014; Bahdanau et al., 2014; Luong et al., 2015).", "startOffset": 73, "endOffset": 140}, {"referenceID": 1, "context": "We lean on the recent developments of neural sequence-to-sequence models (Sutskever et al., 2014; Bahdanau et al., 2014; Luong et al., 2015).", "startOffset": 73, "endOffset": 140}, {"referenceID": 14, "context": "We lean on the recent developments of neural sequence-to-sequence models (Sutskever et al., 2014; Bahdanau et al., 2014; Luong et al., 2015).", "startOffset": 73, "endOffset": 140}, {"referenceID": 4, "context": "Using Freebase (Bollacker et al., 2008) or even open information extraction (Fader et al.", "startOffset": 15, "endOffset": 39}, {"referenceID": 9, "context": ", 2008) or even open information extraction (Fader et al., 2011) would dramatically increase the number of facts and therefore the scope of possible perspectives.", "startOffset": 44, "endOffset": 64}, {"referenceID": 19, "context": "We think perspective generation is an exciting setting to study aspects of numeric framing (Teigen, 2015).", "startOffset": 91, "endOffset": 105}, {"referenceID": 2, "context": ", 2013), but more sophisticated forms of semantic relations on larger units of text might yield better results (Bowman et al., 2015). On the description generation side, there is a long line of work in generating natural language descriptions of structured data or logical forms Wong and Mooney (2007); Chen and Mooney (2008); Lu and Ng (2012); Angeli et al.", "startOffset": 112, "endOffset": 302}, {"referenceID": 2, "context": ", 2013), but more sophisticated forms of semantic relations on larger units of text might yield better results (Bowman et al., 2015). On the description generation side, there is a long line of work in generating natural language descriptions of structured data or logical forms Wong and Mooney (2007); Chen and Mooney (2008); Lu and Ng (2012); Angeli et al.", "startOffset": 112, "endOffset": 326}, {"referenceID": 2, "context": ", 2013), but more sophisticated forms of semantic relations on larger units of text might yield better results (Bowman et al., 2015). On the description generation side, there is a long line of work in generating natural language descriptions of structured data or logical forms Wong and Mooney (2007); Chen and Mooney (2008); Lu and Ng (2012); Angeli et al.", "startOffset": 112, "endOffset": 344}, {"referenceID": 0, "context": "On the description generation side, there is a long line of work in generating natural language descriptions of structured data or logical forms Wong and Mooney (2007); Chen and Mooney (2008); Lu and Ng (2012); Angeli et al. (2010). We lean on the recent developments of neural sequence-to-sequence models (Sutskever et al.", "startOffset": 211, "endOffset": 232}, {"referenceID": 0, "context": "On the description generation side, there is a long line of work in generating natural language descriptions of structured data or logical forms Wong and Mooney (2007); Chen and Mooney (2008); Lu and Ng (2012); Angeli et al. (2010). We lean on the recent developments of neural sequence-to-sequence models (Sutskever et al., 2014; Bahdanau et al., 2014; Luong et al., 2015). Our problem bears some similarity to the semantic parsing work of Wang et al. (2015), who connect generated canonical utterances (representing logical forms) to real utterances.", "startOffset": 211, "endOffset": 460}], "year": 2016, "abstractText": "How much is 131 million US dollars? To help readers put such numbers in context, we propose a new task of automatically generating short descriptions known as perspectives, e.g. \u201c$131 million is about the cost to employ everyone in Texas over a lunch period\u201d. First, we collect a dataset of numeric mentions in news articles, where each mention is labeled with a set of rated perspectives. We then propose a system to generate these descriptions consisting of two steps: formula construction and description generation. In construction, we compose formulae from numeric facts in a knowledge base and rank the resulting formulas based on familiarity, numeric proximity and semantic compatibility. In generation, we convert a formula into natural language using a sequence-to-sequence recurrent neural network. Our system obtains a 15.2% F1 improvement over a non-compositional baseline at formula construction and a 12.5 BLEU point improvement over a baseline description generation.", "creator": "TeX"}}}