{"id": "1609.00150", "review": {"conference": "NIPS", "VERSION": "v1", "DATE_OF_SUBMISSION": "1-Sep-2016", "title": "Reward Augmented Maximum Likelihood for Neural Structured Prediction", "abstract": "A key problem in structured output prediction is direct optimization of the task reward function that matters for test evaluation. This paper presents a simple and computationally efficient approach to incorporate task reward into a maximum likelihood framework. We establish a connection between the log-likelihood and regularized expected reward objectives, showing that at a zero temperature, they are approximately equivalent in the vicinity of the optimal solution. We show that optimal regularized expected reward is achieved when the conditional distribution of the outputs given the inputs is proportional to their exponentiated (temperature adjusted) rewards. Based on this observation, we optimize conditional log-probability of edited outputs that are sampled proportionally to their scaled exponentiated reward. We apply this framework to optimize edit distance in the output label space. Experiments on speech recognition and machine translation for neural sequence to sequence models show notable improvements over a maximum likelihood baseline by using edit distance augmented maximum likelihood.", "histories": [["v1", "Thu, 1 Sep 2016 09:00:19 GMT  (69kb)", "http://arxiv.org/abs/1609.00150v1", "NIPS 2016"], ["v2", "Sat, 10 Sep 2016 01:07:04 GMT  (55kb)", "http://arxiv.org/abs/1609.00150v2", "NIPS 2016"], ["v3", "Wed, 4 Jan 2017 18:10:36 GMT  (48kb)", "http://arxiv.org/abs/1609.00150v3", "NIPS 2016"]], "COMMENTS": "NIPS 2016", "reviews": [], "SUBJECTS": "cs.LG", "authors": ["mohammad norouzi", "samy bengio", "zhifeng chen", "navdeep jaitly", "mike schuster", "yonghui wu", "dale schuurmans"], "accepted": true, "id": "1609.00150"}, "pdf": {"name": "1609.00150.pdf", "metadata": {"source": "CRF", "title": "Reward Augmented Maximum Likelihood for Neural Structured Prediction", "authors": ["Mohammad Norouzi", "Samy Bengio", "Zhifeng Chen", "Navdeep Jaitly", "Mike Schuster", "Yonghui Wu", "Dale Schuurmans"], "emails": [], "sections": [{"heading": null, "text": "ar Xiv: 160 9.00 150v 1 [cs.L G] 1S ep"}, {"heading": "1 Introduction", "text": "This year is the highest in the history of the country."}, {"heading": "2 Reward augmented maximum likelihood", "text": "Faced with a dataset of input-output pairs, D \u2261 {(i), y \u00b2 (i)), a reward is preferred (i), Ni = 1, structured output models learn a parametric score function (y) that achieves different output hypotheses, y-Y results. In a probabilistic model, the score function is normalized, while in a large margin model, the score value may not be normalized. In both cases, once the score function is learned, an input x is given, the model predicts a maximum score performance, y-score results (x) = argmax y results. (1) If this optimization is intractable, an approximate inference is used (e.g. beam search). We use a reward function r (y, y \u00b2) to evaluate different outputs against outputs."}, {"heading": "2.1 Optimization", "text": "The optimization of the reward model increases the maximum probability (RML) on the basis of the probability ratio, the LRML method incorporates several procedures that can be expressed depending on the results of the samples from q (y | y; \u03c4). (9) To estimate the progression of the sample from q (y | y *), a minibatch of examples for the SGD therefore uses a randomized sample from y (y | y *) and then optimizes the probability of logic of these samples by following the mean gradient. (9) At a temperature threshold of 0, this is reduced to always the same y pattern (hence ML training without sampling. In contrast, the progression of the LRL method has a lower probability by following the mean gradient.) At a temperature threshold of 0, this reduces the differences between the y models (hence ML training without sampling. The dissemination of the LL method has a positive effect."}, {"heading": "2.2 Sampling from the exponentiated payoff distribution", "text": "To calculate the gradient of the model, using the RML approach, one has to scan auxiliary results from the exposed payout distribution, q (y | y \u0445; \u03c4). This sampling is the price we have to pay to learn with rewards. It should be compared to loss-increased inferences in structured methods of large margins and sampling from the model in RL. We believe that sampling expenditures proportional to exposed rewards is more efficient and effective in many cases. Experiments in this paper use reward values defined either by negative hammering distance or negative processing distance. We stamp out q (y | y) by performing a stratified sampling where we first select a certain distance and then sampling an output with that distance value. Here, we focus on processing distance sampling, as a hammering-special sampling is an easier one."}, {"heading": "3 RML analysis", "text": "In the framework of the RML, we will find the model parameters by minimizing the target (7) rather than optimizing the RL target (2).The difference lies in minimizing the difference between DKL (2) and DKL (2).We will characterize the difference between the two divergences, the difference between the two divergences, DKL (q) and DKL (p), and use this analysis to motivate the RML approximation.We will first consider the KL divergence in its more general form as a Bregman divergence that will clarify some of the key characteristics."}, {"heading": "4 Related Work", "text": "This year it has come to the point that it has never come as far as this year."}, {"heading": "5 Experiments", "text": "We compare our approach, the Augmented Maximum Probability (RML) reward, with the standard Maximum Probability (ML) training on sequence prediction tasks using state-of-the-art attention-based relapsing neural networks [35, 3]. Our experiments show that the RML approach significantly exceeds the ML baseline for both speech recognition and machine translation tasks."}, {"heading": "5.1 Speech recognition", "text": "For speech recognition experiments, we use the TIMIT dataset; a standard benchmark for clean phone recognition = 61. This dataset includes recordings of various speakers who read ten phonetically rich sentences covering the most important dialects of American English. We use the standard train / dev / test splits proposed by the Kaldi toolkit [29].Since the sequence prediction model we use uses an attention-based encoder decoder recurrent model of [7] with three 256-dimensional LSTM layers for encryption and a 256-dimensional LSTM layer for decoding, we do not modify the neural network architecture or its gradient computing in any way, but we only modify the output targets in the network for gradient computation and SGD update.The input to the network static sequence 123 is a standard localence response dimension."}, {"heading": "5.2 Machine translation", "text": "We evaluate the effectiveness of the proposed approach on WMT '14 in relation to three layers of attention that we propose. Translation quality is evaluated on the basis of the tokenized BLEU score to be consistent with previous work on neural machine translation. [35, 3, 26] The models are learned on the basis of the full 36M sentence pairs of WMT' 14 and evaluated on the basis of 3003 sentence pairs from the latest 2014 test set. It is possible to efficiently and easily trace the insertions and deletion sampling processes back to such a large corpus rate, but we are considering the sentences on the basis of substitution (no insertion or deletion).One can assign the insertions and deletion sampling according to the exposed set of BLEU scores, but we are relying on the conditional sequence model that we allow a substitution (no insertion or deletion)."}, {"heading": "6 Conclusion", "text": "We introduced a learning algorithm for structured output predictions that generalizes the maximum probability training by enabling direct optimization of the metric for task evaluation. Our method is computationally efficient and easy to implement, and only requires an increase in the output targets used to train a maximum probability model. We present a method for sampling output extensions with increasing processing distance and show how the use of such extended outputs for training significantly improves the maximum probability for both machine translation and speech recognition tasks. We believe that this framework applies to a wide range of probability models with arbitrary reward functions. In future work, we intend to examine the applicability of this framework to other probability models for tasks with different evaluation metrics."}, {"heading": "A Proofs", "text": "Proposition 1: For each two-fold differentiable, strictly convex closed potential F, and p, q q = q (F): DF (q \u00b2 p) = DF (p \u00b2 p) = DF (p \u00b2 p) \u2212 p (p \u2212 p) \u2212 p (f) (f) (f) (f) (f) (f) (f) (f) (f) (f) (f) (f) (f) (f) (f) (f) (f) (f) (f) (f) (f) (f) (f) (f) (f), (f), (f), (f), (f), (f), (f), (f), (f), (f), (f), (f), (f), (f, f, f, (f, f, f, f, f, f (f), (f), (f, f, f, f (f), (f), (f), (f), (f), (f), (f, f, f (, f), f (, f), (, f (, f), f (, f (, f), f (, f), f (, f (, f), f (, f (, f), f (, f (, f (, f), f (, f (, f), f (, f (, f), f (, f (, f), f (, f (, f (, f), f (, f), f (, f (, f (, f), f (, f (, f), f (, f), f (, f (, f (, f), f (, f (, f), f (, f), f (, f (, f (, f), f (,), f (, f (,), f (, f (, f (, f), f (, f (, f), f (, f), f (,), f (, f (,),), (, f (,), f (, f (,), f (,), f (, f (, f (,"}], "references": [], "referenceMentions": [], "year": 2017, "abstractText": "A key problem in structured output prediction is direct optimization of the task<lb>reward function that matters for test evaluation. This paper presents a simple and<lb>computationally efficient approach to incorporate task reward into a maximum<lb>likelihood framework. We establish a connection between the log-likelihood and<lb>regularized expected reward objectives, showing that at a zero temperature, they<lb>are approximately equivalent in the vicinity of the optimal solution. We show<lb>that optimal regularized expected reward is achieved when the conditional distri-<lb>bution of the outputs given the inputs is proportional to their exponentiated (tem-<lb>perature adjusted) rewards. Based on this observation, we optimize conditional<lb>log-probability of edited outputs that are sampled proportionally to their scaled<lb>exponentiated reward. We apply this framework to optimize edit distance in the<lb>output label space. Experiments on speech recognition and machine translation<lb>for neural sequence to sequence models show notable improvements over a maxi-<lb>mum likelihood baseline by using edit distance augmented maximum likelihood.", "creator": "LaTeX with hyperref package"}}}