{"id": "1609.00629", "review": {"conference": "NIPS", "VERSION": "v1", "DATE_OF_SUBMISSION": "2-Sep-2016", "title": "SEBOOST - Boosting Stochastic Learning Using Subspace Optimization Techniques", "abstract": "We present SEBOOST, a technique for boosting the performance of existing stochastic optimization methods. SEBOOST applies a secondary optimization process in the subspace spanned by the last steps and descent directions. The method was inspired by the SESOP optimization method for large-scale problems, and has been adapted for the stochastic learning framework. It can be applied on top of any existing optimization method with no need to tweak the internal algorithm. We show that the method is able to boost the performance of different algorithms, and make them more robust to changes in their hyper-parameters. As the boosting steps of SEBOOST are applied between large sets of descent steps, the additional subspace optimization hardly increases the overall computational burden. We introduce two hyper-parameters that control the balance between the baseline method and the secondary optimization process. The method was evaluated on several deep learning tasks, demonstrating promising results.", "histories": [["v1", "Fri, 2 Sep 2016 14:48:16 GMT  (422kb,D)", "http://arxiv.org/abs/1609.00629v1", null]], "reviews": [], "SUBJECTS": "cs.CV cs.LG stat.ML", "authors": ["elad richardson", "rom herskovitz", "boris ginsburg", "michael zibulevsky"], "accepted": true, "id": "1609.00629"}, "pdf": {"name": "1609.00629.pdf", "metadata": {"source": "CRF", "title": "SEBOOST \u2013 Boosting Stochastic Learning Using Subspace Optimization Techniques", "authors": ["Elad Richardson", "Rom Herskovitz", "Boris Ginsburg", "Michael Zibulevsky"], "emails": ["eladrich@cs.technion.ac.il", "mzib@cs.technion.ac.il", "fornoch@gmail.com", "boris.ginsburg@gmail.com"], "sections": [{"heading": "1 Introduction", "text": "Since some objective functions that we want to optimize, a vanilla gradient descent method would simply take a firm step toward the current gradient, in many learning problems the goal or loss of function is averaged over the set of given training examples. In this scenario, calculating the loss over the entire training set would be expensive and is therefore approximated on a small batch, resulting in a stochastic algorithm that requires relatively few calculations per step. SGD algorithms \"simplicity and efficiency have made them a standard selection for many learning tasks, specifically for deep learning [9, 6, 5, 10]. Although vanilla SGD has no memory of previous steps, they are usually used in some way, for example with dynamics [13]. Alternatively, the AdaGrad method uses the previous gradients to adaptively normalize each component in the new gradient."}, {"heading": "2 The algorithm", "text": "While our algorithm tries to find the balance between SGD and SESOP, we start with a brief check of the original algorithms and then switch to the SEBOOST algorithm."}, {"heading": "2.1 Vanilla SGD", "text": "For many different major optimization problems, the application of complex optimization methods is not practicable. Therefore, popular optimization methods for these problems are usually based on a stochastic estimate of the gradient. Let Minx-Rn-f (x) be a minimization problem and let g (x) be the gradient of f (x). The general stochastic approach applies the following optimization rule: + 1 = xk \u2212 \u03b7g * (xk), where xi is the result of ith iteration, \u03b7 is the learning rate and g * (xk) is an approximation of g (xk), which was achieved with only a small subset (minibatch) of training data. These stochastic descent methods have proven themselves in many different problems, especially in the context of deep learning algorithms, which offer a combination of simplicity and speed. Note that the Vanilla SGD algorithm does not use memory of previous Iterations to inform the Iteration process, which has previous Iterations on top of D."}, {"heading": "2.2 Vanilla SESOP", "text": "The SEquential Subspace Optimization Method [11, 15] is an optimization technology used for large-scale optimization problems. The core idea of SESOP is to perform the optimization of the objective function in the subspace, which is spanned by the current gradient direction and a series of directions derived from the previous optimization steps. Following the notations in Section 2.1, a subspace structure for SESOP is usually defined on the basis of the following directions: 1. Gradients: current gradients and [optionally] older {g (xi): i = k, k \u2212 1,... k \u2212 s1} 2. Previous directions: {pi = xi \u2212 xi \u2212 xi \u2212 1: i = k, k \u2212 s2}. In the SESOP formulation, the current gradient and the last step are mandatory and any other proposition can be used to enrich the subspace. Theoretically, one can enrich the subspace with two previous romium directions: This gradient = 12."}, {"heading": "2.3 The SEBOOST algorithm", "text": "As explained in Section 2.1, stochastic learning methods are usually better adapted to the task than many other optimization methods involved, but when properly applied, these methods can still be used to speed up the optimization process and achieve faster convergence rates. We suggest starting with an SGD algorithm as a starting point and then applying a SESOP-like optimization method over it. The subspace for the SESOP algorithm is derived from the downward directions of the baseline using the previous iterations. A description of the method is given in Algorithm 1. Note that the subset of training data used for secondary optimization in step 7 is not necessarily the same as the subspace of the baseline in step 2, as in Section 3. Also, note that in step 8 the last added direction is changed to incorporate the xxx.P subdimension in order to integrate the step of secondary optimization into the subspace."}, {"heading": "2.4 Enriching the subspace", "text": "Although the core elements of our optimization superspace are the directions of the last M \u2212 1 external steps and the new stochastic cumulative direction, many more elements can be added to enrich the subspace. Anchor Points Since only the last (M \u2212 1) directions are stored in our subspace, the subspace has only knowledge of the recent history of the optimization process. Subspace could also benefit from directions that depend on previous directions. For example, one could imagine the general descent achieved by the algorithm p = xk0 \u2212 x00 as a possible direction, or the descent in the second half of the optimization process p = xk0 \u2212 x k / 2 0. We formulate this idea by selecting anchor points during the descent process, which we rarely repair and update. For each ai anchor point, ai shows the direction p = xk0 \u2212 ai, the direction of the subspace is added. Various techniques can be selected to change the anchor points and change the anchor points."}, {"heading": "3 Experiments", "text": "After the recent increase in interest in deep learning tasks, we are focusing our assessment on various neural network problems. We start with a small but challenging regression problem and then move on to the known problems of the MNIST autoencoder and the CIFAR-10 classifier. For each problem, we compare the results of the basic stochastic methods with our enhanced variants, retaining the original qualities of these algorithms. Note that the purpose of our work is not to compete directly with existing methods, but rather to show that SEBOOST can improve each learning method compared to its original variant while preserving the original qualities of these algorithms."}, {"heading": "3.1 Simple regression", "text": "The data set in question consists of a set of 20,000 values simulating a continuous function f: R6 \u2192 R. The data set was divided into 18,000 training examples and 2,000 test examples. The problem was solved using a tiny neural network with the architecture 6 \u2192 L 12 \u2192 L 8 \u2192 L 4 \u2192 L 1. Although the network size is very small, the resulting optimization problem remains challenging and gives clear indications of the behavior of SEBOOST. Figure 1 shows the optimization process for the different methods. In all examples, the increased variant converged faster. Note that the different variants of SEBOOST behave differently, regulated by the corresponding baseline."}, {"heading": "3.2 MNIST autoencoder", "text": "One of the classical formulations for neural networks is that of an autoencoder, a network that tries to learn an efficient representation for a given dataset. An autoencoder usually consists of two parts, the encoder that takes the input and generates the compact representation, and the decoder that takes the representation and tries to reconstruct the original input. In our experiment, the MNIST dataset was used with 60,000 training images of the size 28 x 28 x 28 and 10,000 test images. The encoder was defined as a three-layer network with an architecture of the form 784 \u2192 L 200 \u2192 L 100 \u2192 L 64, with a suitable decoder 64 \u2192 L 100 \u2192 L 200 \u2192 L 784. Figure 3 shows the optimization process for the autoencoder problem. A similar trend as in Experiment 3.1 is evident, SEBOOST is able to significantly improve SGD and NAG, although not as clearly."}, {"heading": "3.3 CIFAR-10 classifier", "text": "A standard benchmark for classification purposes is the CIFAR 10 dataset. To test SEBOOST's ability to deal with large and modern networks, the ResNet [6] architecture, which won the ILSVRC classification task in 2015.Figure 4 shows the optimization process and accuracy achieved for ResNet Depth 32. Note that we did not manually optimize the learning rate as in the original work. While AdaGrad is not evaluated for this experiment, SGD and NAG achieve a significant increase and achieve a better minimum. The boosting step was applied only once per epoch, with too frequent boosting steps resulting in less stable optimization and higher minimums, while the application of rare steps also led to higher minimums. Experiment 3.4 shows similar results for MNIST and discusses them."}, {"heading": "3.4 Understanding the hyper-parameters", "text": "SEBOOST introduces two hyperparameters: \"the number of basic steps between each sub-space optimization and M the number of old directions to be used. The purpose of the following two experiments is to measure the impact of these parameters on the result achieved and to give some intuition as to their significance. All experiments are based on the MNIST autoencoder problem described in Section 3.2. First, let's consider the parameter\" that controls the balance between the baseline of the SGD algorithm and the more involved optimization process. Let's take small values of the \"results in more steps of the secondary optimization process, but each direction in the subspace is then composed of fewer steps of the stochastic algorithm, making it less stable. Also, remembering that our secondary optimization is more expensive than the regular optimization steps, impeding the application of the algorithm too often.\""}, {"heading": "3.5 Investigating the subspace", "text": "One of the key components of SEBOOST is the structure of the subspace in which the optimization is applied. The purpose of the following two experiments is to see how changes in the base algorithm or the addition of additional directions affect the algorithm. All experiments are based on the MNIST autoencoder problem defined in Section 3.2. Another interesting experiment is to see how our algorithm is affected by changes in the hyperparameters of the base algorithm. Figure 6a shows the effect of the learning rate on the base algorithms and their increased variants. It can be seen that the change in the original baseline affects our algorithm, but the effects are much smaller, which shows that the algorithm has a degree of robustness compared to the original learning rate."}, {"heading": "4 Conclusion", "text": "In this paper, we introduced SEBOOST, a technique for increasing stochastic learning algorithms through a secondary optimization process. Secondary optimization is applied in the subspace, which is spanned by the preceding descent steps, which can be expanded with additional directions. We evaluated SEBOOST using various deep learning tasks and showed the results of our methods compared to their original foundations. We believe that the flexibility of SEBOOST could make them useful for different learning tasks. Although this is not the focus of our work, an interesting research direction for SEBOOST is the parallel calculation. Similar to [2, 14] one can look at a framework consisting of a single master and a group of workers, with each worker optimizing a local model and replacing the space involved with a secondary optimization process."}, {"heading": "Acknowledgements", "text": "The research that led to these results was funded by the European Research Council under the European Union's Seventh Framework Programme, ERC Funding Agreement No 320649, and supported by the Intel Collaborative Research Institute for Computational Intelligence (ICRI-CI)."}], "references": [{"title": "Torch7: A matlab-like environment for machine learning", "author": ["Ronan Collobert", "Koray Kavukcuoglu", "Cl\u00e9ment Farabet"], "venue": "In BigLearn, NIPS Workshop, number EPFL-CONF-192376,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2011}, {"title": "Large scale distributed deep networks", "author": ["Jeffrey Dean", "Greg Corrado", "Rajat Monga", "Kai Chen", "Matthieu Devin", "Mark Mao", "Andrew Senior", "Paul Tucker", "Ke Yang", "Quoc V Le"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2012}, {"title": "Adaptive subgradient methods for online learning and stochastic optimization", "author": ["John Duchi", "Elad Hazan", "Yoram Singer"], "venue": "The Journal of Machine Learning Research,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2011}, {"title": "Coordinate and subspace optimization methods for linear least squares with non-quadratic regularization", "author": ["Michael Elad", "Boaz Matalon", "Michael Zibulevsky"], "venue": "Applied and Computational Harmonic Analysis,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2007}, {"title": "Rich feature hierarchies for accurate object detection and semantic segmentation", "author": ["Ross Girshick", "Jeff Donahue", "Trevor Darrell", "Jitendra Malik"], "venue": "In Proceedings of the IEEE conference on computer vision and pattern recognition,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2014}, {"title": "Deep residual learning for image recognition", "author": ["Kaiming He", "Xiangyu Zhang", "Shaoqing Ren", "Jian Sun"], "venue": "arXiv preprint arXiv:1512.03385,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2015}, {"title": "Methods of conjugate gradients for solving linear systems, volume", "author": ["Magnus Rudolph Hestenes", "Eduard Stiefel"], "venue": "NBS,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 1952}, {"title": "Adam: A method for stochastic optimization", "author": ["Diederik Kingma", "Jimmy Ba"], "venue": "arXiv preprint arXiv:1412.6980,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2014}, {"title": "Imagenet classification with deep convolutional neural networks. In Advances in neural information processing", "author": ["Alex Krizhevsky", "Ilya Sutskever", "Geoffrey E Hinton"], "venue": null, "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2012}, {"title": "Fully convolutional networks for semantic segmentation", "author": ["Jonathan Long", "Evan Shelhamer", "Trevor Darrell"], "venue": "In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2015}, {"title": "Sequential subspace optimization method for large-scale unconstrained problems", "author": ["Guy Narkiss", "Michael Zibulevsky"], "venue": "Technion-IIT, Department of Electrical Engineering,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2005}, {"title": "Orth-method for smooth convex optimization", "author": ["Arkadi Nemirovski"], "venue": "Izvestia AN SSSR, Transl.: Eng. Cybern. Soviet J. Comput. Syst. Sci,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 1982}, {"title": "On the importance of initialization and momentum in deep learning", "author": ["Ilya Sutskever", "James Martens", "George Dahl", "Geoffrey Hinton"], "venue": "In Proceedings of the 30th international conference on machine learning", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2013}, {"title": "Deep learning with elastic averaging sgd", "author": ["Sixin Zhang", "Anna E Choromanska", "Yann LeCun"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2015}, {"title": "Speeding-up convergence via sequential subspace optimization: Current state and future directions", "author": ["Michael Zibulevsky"], "venue": "arXiv preprint arXiv:1401.0159,", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2013}, {"title": "L1-l2 optimization in signal and image processing", "author": ["Michael Zibulevsky", "Michael Elad"], "venue": "Signal Processing Magazine, IEEE,", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2010}], "referenceMentions": [{"referenceID": 8, "context": "The simplicity and efficiency of SGD algorithms have made them a standard choice for many learning tasks, and specifically for deep learning [9, 6, 5, 10] .", "startOffset": 141, "endOffset": 154}, {"referenceID": 5, "context": "The simplicity and efficiency of SGD algorithms have made them a standard choice for many learning tasks, and specifically for deep learning [9, 6, 5, 10] .", "startOffset": 141, "endOffset": 154}, {"referenceID": 4, "context": "The simplicity and efficiency of SGD algorithms have made them a standard choice for many learning tasks, and specifically for deep learning [9, 6, 5, 10] .", "startOffset": 141, "endOffset": 154}, {"referenceID": 9, "context": "The simplicity and efficiency of SGD algorithms have made them a standard choice for many learning tasks, and specifically for deep learning [9, 6, 5, 10] .", "startOffset": 141, "endOffset": 154}, {"referenceID": 12, "context": "Although the vanilla SGD has no memory of previous steps, they are usually utilized in some way, for example using momentum [13].", "startOffset": 124, "endOffset": 128}, {"referenceID": 2, "context": "Alternatively, the AdaGrad method uses the previous gradients in order to normalize each component in the new gradient adaptively [3], while the ADAM method uses them to estimate an adaptive moment [8].", "startOffset": 130, "endOffset": 133}, {"referenceID": 7, "context": "Alternatively, the AdaGrad method uses the previous gradients in order to normalize each component in the new gradient adaptively [3], while the ADAM method uses them to estimate an adaptive moment [8].", "startOffset": 198, "endOffset": 201}, {"referenceID": 10, "context": "In this work we utilize the knowledge of previous steps in spirit of the Sequential Subspace Optimization (SESOP) framework [11].", "startOffset": 124, "endOffset": 128}, {"referenceID": 3, "context": "Several such extensions were introduced over the years to different fields, such as PCD-SESOP and SSF-SESOP, showing state-of-the-art results in their matching fields [4, 16, 15].", "startOffset": 167, "endOffset": 178}, {"referenceID": 15, "context": "Several such extensions were introduced over the years to different fields, such as PCD-SESOP and SSF-SESOP, showing state-of-the-art results in their matching fields [4, 16, 15].", "startOffset": 167, "endOffset": 178}, {"referenceID": 14, "context": "Several such extensions were introduced over the years to different fields, such as PCD-SESOP and SSF-SESOP, showing state-of-the-art results in their matching fields [4, 16, 15].", "startOffset": 167, "endOffset": 178}, {"referenceID": 10, "context": "2 Vanilla SESOP The SEquential Subspace OPtimization Method [11, 15] is an optimization technique used for large scale optimization problems.", "startOffset": 60, "endOffset": 68}, {"referenceID": 14, "context": "2 Vanilla SESOP The SEquential Subspace OPtimization Method [11, 15] is an optimization technique used for large scale optimization problems.", "startOffset": 60, "endOffset": 68}, {"referenceID": 11, "context": "This will provide optimal worst case complexity of the method (see also [12].", "startOffset": 72, "endOffset": 76}, {"referenceID": 12, "context": "The chosen baselines were SGD with momentum, Nesterov\u2019s Accelerated Gradient (NAG) [13] and AdaGrad [3].", "startOffset": 83, "endOffset": 87}, {"referenceID": 2, "context": "The chosen baselines were SGD with momentum, Nesterov\u2019s Accelerated Gradient (NAG) [13] and AdaGrad [3].", "startOffset": 100, "endOffset": 103}, {"referenceID": 6, "context": "The Conjugate Gradient (CG) [7] was used for the subspace optimization.", "startOffset": 28, "endOffset": 31}, {"referenceID": 0, "context": "Our algorithm was implemented and evaluated using the Torch7 framework [1], and will be publicly available.", "startOffset": 71, "endOffset": 74}, {"referenceID": 5, "context": "In order to check SEBOOST\u2019s ability to deal with large and modern networks the ResNet [6] architecture, winner of the ILSVRC 2015 classification task, is used.", "startOffset": 86, "endOffset": 89}, {"referenceID": 1, "context": "Similarly to [2, 14], one can look at a framework composed of a single master and a set of workers, where each worker optimizes a local model and the master saves a global set of parameters which is based on the workers.", "startOffset": 13, "endOffset": 20}, {"referenceID": 13, "context": "Similarly to [2, 14], one can look at a framework composed of a single master and a set of workers, where each worker optimizes a local model and the master saves a global set of parameters which is based on the workers.", "startOffset": 13, "endOffset": 20}], "year": 2016, "abstractText": "We present SEBOOST, a technique for boosting the performance of existing stochastic optimization methods. SEBOOST applies a secondary optimization process in the subspace spanned by the last steps and descent directions. The method was inspired by the SESOP optimization method for large-scale problems, and has been adapted for the stochastic learning framework. It can be applied on top of any existing optimization method with no need to tweak the internal algorithm. We show that the method is able to boost the performance of different algorithms, and make them more robust to changes in their hyper-parameters. As the boosting steps of SEBOOST are applied between large sets of descent steps, the additional subspace optimization hardly increases the overall computational burden. We introduce two hyper-parameters that control the balance between the baseline method and the secondary optimization process. The method was evaluated on several deep learning tasks, demonstrating promising results.", "creator": "LaTeX with hyperref package"}}}