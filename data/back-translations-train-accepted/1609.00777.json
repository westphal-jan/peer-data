{"id": "1609.00777", "review": {"conference": "ACL", "VERSION": "v1", "DATE_OF_SUBMISSION": "3-Sep-2016", "title": "Towards End-to-End Reinforcement Learning of Dialogue Agents for Information Access", "abstract": "This paper proposes \\emph{KB-InfoBot}---a dialogue agent that provides users with an entity from a knowledge base (KB) by interactively asking for its attributes. All components of the KB-InfoBot are trained in an end-to-end fashion using reinforcement learning. Goal-oriented dialogue systems typically need to interact with an external database to access real-world knowledge (e.g. movies playing in a city). Previous systems achieved this by issuing a symbolic query to the database and adding retrieved results to the dialogue state. However, such symbolic operations break the differentiability of the system and prevent end-to-end training of neural dialogue agents. In this paper, we address this limitation by replacing symbolic queries with an induced \"soft\" posterior distribution over the KB that indicates which entities the user is interested in. We also provide a modified version of the episodic REINFORCE algorithm, which allows the KB-InfoBot to explore and learn both the policy for selecting dialogue acts and the posterior over the KB for retrieving the correct entities. Experimental results show that the end-to-end trained KB-InfoBot outperforms competitive rule-based baselines, as well as agents which are not end-to-end trainable.", "histories": [["v1", "Sat, 3 Sep 2016 01:02:51 GMT  (2704kb,D)", "http://arxiv.org/abs/1609.00777v1", null], ["v2", "Mon, 31 Oct 2016 21:39:31 GMT  (2654kb,D)", "http://arxiv.org/abs/1609.00777v2", null], ["v3", "Thu, 20 Apr 2017 17:26:35 GMT  (2748kb,D)", "http://arxiv.org/abs/1609.00777v3", "Accepted at ACL 2017"]], "reviews": [], "SUBJECTS": "cs.CL cs.LG", "authors": ["bhuwan dhingra", "lihong li", "xiujun li", "jianfeng gao", "yun-nung chen", "faisal ahmed 0001", "li deng"], "accepted": true, "id": "1609.00777"}, "pdf": {"name": "1609.00777.pdf", "metadata": {"source": "CRF", "title": "End-to-End Reinforcement Learning of Dialogue Agents for Information Access", "authors": ["Bhuwan Dhingra", "Lihong Li", "Xiujun Li", "Jianfeng Gao", "Yun-Nung Chen", "Faisal Ahmed", "Li Deng"], "emails": ["bdhingra@andrew.cmu.edu", "lihongli@microsoft.com", "xiul@microsoft.com", "jfgao@microsoft.com", "vivic@microsoft.com", "fiahmed@microsoft.com", "deng@microsoft.com"], "sections": [{"heading": "1 Introduction", "text": "This year it has come to the point that it will be able to put itself at the top, \"he said in an interview with the taz.\" It's as if we will be able to get angry, \"he said.\" But it's not as if we are able to get angry, \"he said."}, {"heading": "2 Related Work", "text": "In fact, most of them are able to survive themselves, and they do not. (...) Most of them are able to survive themselves. (...) Most of them are able to survive themselves. (...) Most of them are not able to survive themselves. (...) Most of them are able to survive themselves. (...) Most of them are not able to survive themselves. (...) Most of them are able to survive themselves. (...) (...) () (...) () () () () () () () () () () () () () () () () () () () () ()) () () ()) () () () ()) () () () () () () ()) () ()) () () () ()) () () () ()) () () () () ()) () () () () ()) () () () () () () () () () () () () () () () () () () () () () () () ()) () () () () () () () () () () () () ()) () () () () () () () () () ()) () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () (() () () (() () () (() () () ((() () () (() () () () (((() () (() (() (() (() () (() () () ((() () () (() (((()) (((()) () ((((()))) ((((())) (((((())"}, {"heading": "3 Probabilistic Framework for KB Lookup", "text": "In this section, we describe a probabilistic framework for querying a KB given the agent's beliefs about the slots or attributes in the KB."}, {"heading": "3.1 Entity-Centric Knowledge Base (EC-KB)", "text": "A knowledge base consists of triples of the form (h, r, t) that denotes the relationship between head h and tail t. In this thesis, we assume that the KB-InfoBot has access to a domain-specific company-centered knowledge base (EC-KB) (Zwicklbauer et al., 2013) in which all header units are of a certain type and the relationships correspond to the attributes of those header units. Examples of this type are movies, persons, or scientific papers. Such a KB can be converted to a table format whose rows correspond to the unique header units, columns to the unique relationship types (henceforth slots), and some of the entries may be missing. A small example is shown in Figure 2."}, {"heading": "3.2 Notations and Assumptions", "text": "Let T specify the KB table described above and Ti, j the jth slot value of the ith unit. 1 \u2264 i \u2264 N and 1 \u2264 j \u2264 M. We let V j specify the vocabulary of each slot, i.e. the set of all the different values in the j-th column. Note that the user still knows the actual value of Ti, j and we assume that it is in V. < Mj = {i: Ti, j = 0} denotes the set of units for which the value of slot j is missing. Note that the user still knows the actual value of Ti, j and we assume that it is in V."}, {"heading": "3.3 Soft-KB Lookup", "text": "We assume that all probabilities depend on user input U t1 and drop them from the following notation. On our assumption of the independence of slot values: ptT (G = i) and M-j = 1 Pr (Gj = i), (2) where Pr (Gj = i) denotes the rear probability of the user target for slot j pointing to Ti, j. We can marginalize this by \u03a6j to get: Pr (Gj = i) = 1 (Gj = i)."}, {"heading": "4 End-to-End KB-InfoBot", "text": "It is not the first time that we have gone in search of a solution, which we have put on the flags. (...) It is the first time that we have put on the flags. (...) It is the second time that we have put on the flags. (...) It is the second time that we have put on the flags. (...) It is the second time that we have put on the flags. (...) It is the second time that we have put on the flags. (...) It is the third time that we have put on the flags. (...) It is the third time that we have put on the flags. (...) It is the first time that we have put on the flags. (...) It is the second time that we have put on the flags. (...) It is the third time that we have put on the flags. (...) It is the third time that we have put on the flags. (...) It is the third time that we have put on the flags. (...) It is the third time that we have put on the flags."}, {"heading": "5 Training", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "5.1 Reinforcement Learning", "text": "This formulation leads to a modified version of the episodic REINFORCE algorithm (Williams, 1992), which we describe below. We can describe the expected discounted return of the agent within the framework of the policy system as follows: J (\u03b8) = E [H] = 0 \u03b3hrh] (16) Here, the expectation lies above all possible paths of the dialogue. (Hindi) denotes the parameters of the end-to-end system. (H) is the maximum length of an episode. (H) is the discounting factor. and rh is the reward observed in turn h. We can use the probability quotient trick (Peters and Bagnell, 2011) to describe the course of the target as follows. (D) is the maximum length of an episode. (D) is the discounting factor, and rh is the reward observed in turn h. (Peters and Bagnell, 2011) we use the probability quotient to describe the course of the trick as follows (D)."}, {"heading": "5.2 Imitation Learning", "text": "In theory, both the faith trackers and the political network can be trained from the ground up on the basis of the reinforcement learning objective described above. In practice, however, with a moderately large KB, the agent almost always fails if he proceeds from a random initialization. In this case, lending credit to the agent is difficult because he does not know whether he is failing due to a wrong sequence of actions or a wrong set of results from the KB. Therefore, at the beginning of the training, we have an imitated learning phase in which the faith trackers and the political network are trained to imitate a simple, hand-designed rule-based agent. The rule-based agent is described in more detail in the next paragraph. Here, we specify the imitated learning objective with which the KB-InfoBot.Suppose that p-tj and q-t are trained to imitate the belief set of a rule-based agent and a rule-based agent imitated agent, and the loss function in the denial itial-j-learning j-j-j-j-j-TB-j-j-j-qqqj [) is [P-j-j-j-qqqj-j-j-j]."}, {"heading": "5.3 User Simulator", "text": "In order to evaluate the performance of the KB-InfoBot, we use a rules-based stochastic simulated user. At the beginning of each dialog, the simulated user randomly tries a target unit from the EC-KB and a random combination of informative slots for which he knows the value of the target. The remaining slot values are unknown to the user. The user initiates the dialog by providing the agent with a subset of his informative slots and asking for a unit that corresponds to him. Later, when the agent requests the value of a slot, the user fulfils it by informing the agent that he does not know this value. If the agent shares the results from the KB, the simulated user checks if his target is among them and provides an appropriate reward. We convert dialog actions from the user into natural speech expressions by using a separately trained natural language generator (NLG)."}, {"heading": "6 Experiments and Results", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "6.1 Baselines", "text": "In fact, most people who are able to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to edit, to edit, to edit, to edit, to edit, to edit, to edit, to edit, to edit, to edit, to edit, to edit, to edit, to edit, to edit, to edit, to accelerate, to accelerate, to accelerate, to accelerate, to accelerate, to accelerate, to accelerate. \""}, {"heading": "6.2 Movies-KB", "text": "In our experiments, we used a film-centric knowledge base built with the IMDBPy1 package. We selected a subset of movies released after 2007 and retained 6 slots. Statistics for this KB are in Table 1. The original KB was modified to reduce the number of actors and directors to make the task more difficult. In addition, we randomly remove 20% of the values from the copy of the KB of 1http: / / imdbpy.sourceforge.net / agent to simulate a real scenario in which the KB might be incomplete."}, {"heading": "6.3 Hyperparameters", "text": "We use a hidden state size of d = 100 for all GRUs, a learning rate of 0.05 for the learning phase of imitation and 0.005 for the learning phase of amplification, and a minibatch size of 128. The maximum length of a dialog is limited to 10 spins, 2 after which the dialog is considered a failure; the input vocabulary is computed from the vocabulary and bigrams of the NLG in the KB and its size is 3078. The agent receives a positive reward if the user target is in the upper R = 5 that he delivers; this reward is calculated as 2 (1 \u2212 (r \u2212 1) / R), where r is the actual rank of the target. For a failed dialog, the agent receives a reward of \u2212 1, and for each turn he receives a reward of \u2212 0.1, because we want him to complete the task in the shortest time. The discounting factor is set to 0.99."}, {"heading": "6.4 Performance Comparison", "text": "We compare each of the models discussed along three metrics: the average rewards achieved, the success rate (where success is defined as delivering the benefit among the top R results), and the average number of revolutions per dialog. Figure 4 shows how the reinforcement learning agents perform over the course of the training, a number that was generated by fixing the model all 100 times and performing 2000 simulations while selecting greedy policies. Figure 2 shows that the 2A rotation consists of one user action and one agent action.The performance of each model consists of over another 5,000 simulations after selecting the best model during the training, and greedy policies without exploration. Soft KB versions exceed their HardKB counterparts, which in turn exceed the NoKB versions in terms of average rewards. The main advantage comes from a reduced number of average revolutions. The similar success rate for all base agents is expected to exceed their HardKB counterparts, which in turn exceed the NoKB versions in terms of average rewards."}, {"heading": "7 Conclusion", "text": "We have shown that starting from a learning phase of imitation, where the agent learns to imitate a rules-based belief tracker and a rules-based policy, he can successfully improve himself by learning the reinforcement. Performance gains are particularly high when the noise in user input is high. A KB InfoBot is a special kind of goal-oriented dialogue agent. Future work should focus on extending the techniques described here to other more general dialogue agents, such as restaurant reservation agents or flight booking agents. We have also ignored scalability issues in our work, which would be a concern for knowledge bases in the real world. This is another direction for future research."}, {"heading": "A Sample Dialogues", "text": "Table 3 shows some sample dialogs between the User Simulator and SimpleRL SoftKB and End2End RL Agents. User comments are generated using the NLG described in the text. Critics \"rating slot value is a common source of error in the User Simulator, and therefore all learned guidelines tend to request this value multiple times."}, {"heading": "B Posterior Derivation", "text": "Here we present a derivative for Equation 5, i.e., for the case where i-Mj | = 1 N, (21), in which we assume that all missing values are equally probable, and estimate the previous probability that the target is in this slot. In the case where i = v 6, Mj | = 1 N, (21), in which we assume that all missing values are equally probable, and estimate the previous probability that the target is in this slot. In the case where i = v 6, Mj: Pr (Gj = i) = Pr (Gj = 6, Mj) Pr (Gj = i | Gj 6, Mj) = (1 \u2212 | Mj | N) \u00b7 ptj (v) # jv, (22), in which the second term comes from the assumption that the probability is above the Mj number, the probability that the Mj number is equal to the Mj number (Mj)."}], "references": [{"title": "A survey of robot learning from demonstration", "author": ["Sonia Chernova", "Manuela Veloso", "Brett Browning"], "venue": "Robotics and autonomous systems,", "citeRegEx": "Argall et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Argall et al\\.", "year": 2009}, {"title": "Learning end-to-end goal-oriented dialog. arXiv preprint arXiv:1605.07683", "author": ["Bordes", "Weston2016] Antoine Bordes", "Jason Weston"], "venue": null, "citeRegEx": "Bordes et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Bordes et al\\.", "year": 2016}, {"title": "End-to-end memory networks with knowledge carryover for multi-turn spoken language understanding", "author": ["Chen et al.2016] Yun-Nung Chen", "Dilek Hakkani-T\u00fcr", "Gokhan Tur", "Jianfeng Gao", "Li Deng"], "venue": "In Proceedings of The 17th Annual Meeting", "citeRegEx": "Chen et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Chen et al\\.", "year": 2016}, {"title": "Learning phrase representations using rnn encoder-decoder for statistical", "author": ["Bart Van Merri\u00ebnboer", "Caglar Gulcehre", "Dzmitry Bahdanau", "Fethi Bougares", "Holger Schwenk", "Yoshua Bengio"], "venue": null, "citeRegEx": "Merri\u00ebnboer et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Merri\u00ebnboer et al\\.", "year": 2014}, {"title": "Tensorlog: A differentiable deductive database. arXiv preprint arXiv:1605.06523", "author": ["William W Cohen"], "venue": null, "citeRegEx": "Cohen.,? \\Q2016\\E", "shortCiteRegEx": "Cohen.", "year": 2016}, {"title": "Simpleds: A simple deep reinforcement learning dialogue system", "author": ["Heriberto Cuay\u00e1huitl"], "venue": "International Workshop on Spoken Dialogue Systems (IWSDS)", "citeRegEx": "Cuay\u00e1huitl.,? \\Q2016\\E", "shortCiteRegEx": "Cuay\u00e1huitl.", "year": 2016}, {"title": "Back-off action selection in summary space-based POMDP dialogue systems", "author": ["Ga\u0161i\u0107 et al.2009] Milica Ga\u0161i\u0107", "Fabrice Lefevre", "Filip Jurcicek", "Simon Keizer", "Francois Mairesse", "Blaise Thomson", "Kai Yu", "Steve Young"], "venue": null, "citeRegEx": "Ga\u0161i\u0107 et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Ga\u0161i\u0107 et al\\.", "year": 2009}, {"title": "Multidomain joint semantic frame parsing using bidirectional RNN-LSTM", "author": ["Gokhan Tur", "Asli Celikyilmaz", "Yun-Nung Chen", "Jianfeng Gao", "Li Deng", "Ye-Yi Wang"], "venue": null, "citeRegEx": "Hakkani.T\u00fcr et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Hakkani.T\u00fcr et al\\.", "year": 2016}, {"title": "Word-based dialog state tracking with recurrent neural networks", "author": ["Blaise Thomson", "Steve Young"], "venue": "In Proceedings of the 15th Annual Meeting of the Special Interest Group on Discourse and Dialogue", "citeRegEx": "Henderson et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Henderson et al\\.", "year": 2014}, {"title": "Machine learning for dialog state tracking: A review", "author": ["Matthew Henderson"], "venue": null, "citeRegEx": "Henderson.,? \\Q2015\\E", "shortCiteRegEx": "Henderson.", "year": 2015}, {"title": "Lecture 6a overview of mini\u2013batch gradient descent", "author": ["N Srivastava", "Kevin Swersky"], "venue": "Coursera Lecture slides https://class. coursera", "citeRegEx": "Hinton et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Hinton et al\\.", "year": 2012}, {"title": "Deep reinforcement learning for dialogue generation", "author": ["Li et al.2016] Jiwei Li", "Will Monroe", "Alan Ritter", "Michel Galley", "Jianfeng Gao", "Dan Jurafsky"], "venue": null, "citeRegEx": "Li et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Li et al\\.", "year": 2016}, {"title": "Policy gradient methods", "author": ["Peters", "Bagnell2011] Jan Peters", "J Andrew Bagnell"], "venue": "In Encyclopedia of Machine Learning,", "citeRegEx": "Peters et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Peters et al\\.", "year": 2011}, {"title": "Mastering the game of Go with deep neural networks and tree search", "author": ["Nal Kalchbrenner", "Ilya Sutskever", "Timothy Lillicrap", "Madeleine Leach", "Koray Kavukcuoglu", "Thore Graepel", "Demis Hassabis."], "venue": "Nature, 529:484\u2013489.", "citeRegEx": "Kalchbrenner et al\\.,? 2016", "shortCiteRegEx": "Kalchbrenner et al\\.", "year": 2016}, {"title": "Semantically conditioned lstm-based natural language generation for spoken dialogue systems", "author": ["Wen et al.2015] Tsung-Hsien Wen", "Milica Ga\u0161i\u0107", "Nikola Mrk\u0161i\u0107", "Pei-Hao Su", "David Vandyke", "Steve Young"], "venue": null, "citeRegEx": "Wen et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Wen et al\\.", "year": 2015}, {"title": "Conditional generation and snapshot learning in neural dialogue systems", "author": ["Wen et al.2016a] Tsung-Hsien Wen", "Milica Ga\u0161i\u0107", "Nikola Mrk\u0161i\u0107", "Lina M. Rojas-Barahona", "Pei-Hao Su", "Stefan Ultes", "David Vandyke", "Steve Young"], "venue": null, "citeRegEx": "Wen et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Wen et al\\.", "year": 2016}, {"title": "2016b. A network-based end-to-end trainable task-oriented dialogue system", "author": ["Wen et al.2016b] Tsung-Hsien Wen", "Milica Ga\u0161i\u0107", "Nikola Mrk\u0161i\u0107", "Lina M. Rojas-Barahona", "PeiHao Su", "Stefan Ultes", "David Vandyke", "Steve Young"], "venue": null, "citeRegEx": "Wen et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Wen et al\\.", "year": 2016}, {"title": "Scaling up POMDPs for dialog management: The \u201cSummary POMDP", "author": ["Williams", "Young2005] Jason D Williams", "Steve Young"], "venue": "IEEE Workshop on Automatic Speech Recognition and Understanding,", "citeRegEx": "Williams et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Williams et al\\.", "year": 2005}, {"title": "End-to-end lstm-based dialog control optimized with supervised and reinforcement learning", "author": ["Williams", "Zweig2016] Jason D Williams", "Geoffrey Zweig"], "venue": "arXiv preprint arXiv:1606.01269", "citeRegEx": "Williams et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Williams et al\\.", "year": 2016}, {"title": "Simple statistical gradient-following algorithms for connectionist reinforcement learning", "author": ["Ronald J Williams"], "venue": "Machine learning,", "citeRegEx": "Williams.,? \\Q1992\\E", "shortCiteRegEx": "Williams.", "year": 1992}, {"title": "A probabilistic framework for representing dialog systems and entropy-based dialog management through dynamic stochastic state evolution", "author": ["Wu et al.2015] Ji Wu", "Miao Li", "Chin-Hui Lee"], "venue": null, "citeRegEx": "Wu et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Wu et al\\.", "year": 2015}, {"title": "Spoken language understanding using long shortterm memory neural networks", "author": ["Yao et al.2014] Kaisheng Yao", "Baolin Peng", "Yu Zhang", "Dong Yu", "Geoffrey Zweig", "Yangyang Shi"], "venue": "In Spoken Language Technology Workshop (SLT), 2014 IEEE,", "citeRegEx": "Yao et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Yao et al\\.", "year": 2014}, {"title": "Neural generative question answering", "author": ["Yin et al.2016a] Jun Yin", "Xin Jiang", "Zhengdong Lu", "Lifeng Shang", "Hang Li", "Xiaoming Li"], "venue": "International Joint Conference on Artificial Intelligence", "citeRegEx": "Yin et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Yin et al\\.", "year": 2016}, {"title": "Neural enquirer: Learning to query tables", "author": ["Yin et al.2016b] Pengcheng Yin", "Zhengdong Lu", "Hang Li", "Ben Kao"], "venue": "International Joint Conference on Artificial Intelligence", "citeRegEx": "Yin et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Yin et al\\.", "year": 2016}, {"title": "POMDPbased statistical spoken dialog systems: A review", "author": ["Young et al.2013] Steve Young", "Milica Ga\u0161i\u0107", "Blaise Thomson", "Jason D Williams"], "venue": "Proceedings of the IEEE,", "citeRegEx": "Young et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Young et al\\.", "year": 2013}, {"title": "Reinforcement learning neural Turing machines-revised", "author": ["Zaremba", "Sutskever2015] Wojciech Zaremba", "Ilya Sutskever"], "venue": "arXiv preprint arXiv:1505.00521", "citeRegEx": "Zaremba et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Zaremba et al\\.", "year": 2015}, {"title": "Towards end-to-end learning for dialog state tracking and management using deep reinforcement learning", "author": ["Zhao", "Eskenazi2016] Tiancheng Zhao", "Maxine Eskenazi"], "venue": "arXiv preprint arXiv:1606.02560", "citeRegEx": "Zhao et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Zhao et al\\.", "year": 2016}, {"title": "Do we need entity-centric knowledge bases for entity disambiguation", "author": ["Christin Seifert", "Michael Granitzer"], "venue": "In Proceedings of the 13th International Conference on Knowledge Management and", "citeRegEx": "Zwicklbauer et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Zwicklbauer et al\\.", "year": 2013}], "referenceMentions": [{"referenceID": 21, "context": "A typical goal-oriented dialogue system consists of four basic components: a language understanding (LU) module for predicting user intents and extracting associated slots (Yao et al., 2014; Hakkani-T\u00fcr et al., 2016; Chen et al., 2016), a dialogue state tracker which tracks the user goal and dialogue history (Henderson et al.", "startOffset": 172, "endOffset": 235}, {"referenceID": 7, "context": "A typical goal-oriented dialogue system consists of four basic components: a language understanding (LU) module for predicting user intents and extracting associated slots (Yao et al., 2014; Hakkani-T\u00fcr et al., 2016; Chen et al., 2016), a dialogue state tracker which tracks the user goal and dialogue history (Henderson et al.", "startOffset": 172, "endOffset": 235}, {"referenceID": 2, "context": "A typical goal-oriented dialogue system consists of four basic components: a language understanding (LU) module for predicting user intents and extracting associated slots (Yao et al., 2014; Hakkani-T\u00fcr et al., 2016; Chen et al., 2016), a dialogue state tracker which tracks the user goal and dialogue history (Henderson et al.", "startOffset": 172, "endOffset": 235}, {"referenceID": 8, "context": ", 2016), a dialogue state tracker which tracks the user goal and dialogue history (Henderson et al., 2014; Henderson, 2015), a dialogue policy which selects the next system action based on the current state (Young et al.", "startOffset": 82, "endOffset": 123}, {"referenceID": 9, "context": ", 2016), a dialogue state tracker which tracks the user goal and dialogue history (Henderson et al., 2014; Henderson, 2015), a dialogue policy which selects the next system action based on the current state (Young et al.", "startOffset": 82, "endOffset": 123}, {"referenceID": 24, "context": ", 2014; Henderson, 2015), a dialogue policy which selects the next system action based on the current state (Young et al., 2013), and a natural language generator (NLG) for converting dialogue acts into natural language (Wen et al.", "startOffset": 108, "endOffset": 128}, {"referenceID": 14, "context": ", 2013), and a natural language generator (NLG) for converting dialogue acts into natural language (Wen et al., 2015; Wen et al., 2016a).", "startOffset": 99, "endOffset": 136}, {"referenceID": 0, "context": "Hence, at the beginning of training, we first have an imitation-learning phase (Argall et al., 2009) where both the belief tracker and policy network are trained to mimic a rule-based agent.", "startOffset": 79, "endOffset": 100}, {"referenceID": 19, "context": "Third, we present a modified version of the episodic REINFORCE (Peters and Bagnell, 2011; Williams, 1992) update rule for training the above model based on user feedback, which allows the agent to explore both the set of possible dialogue acts at each turn and the set of possible entity results from the KB at the final turn.", "startOffset": 63, "endOffset": 105}, {"referenceID": 24, "context": "Statistical goal-oriented dialogue systems have long been modeled as partially observable Markov decision processes (POMDPs) (Young et al., 2013), which are trained using reinforcement learning based on user feedback.", "startOffset": 125, "endOffset": 145}, {"referenceID": 4, "context": "A separate line of work\u2013TensorLog (Cohen, 2016)\u2013investigates reasoning over KB facts in a differentiable manner to derive new facts.", "startOffset": 34, "endOffset": 47}, {"referenceID": 27, "context": "In this work we assume that the KB-InfoBot has access to a domain-specific entity-centric knowledge base (EC-KB) (Zwicklbauer et al., 2013) where all head entities are of a particular type, and the relations correspond to attributes of these head entities.", "startOffset": 113, "endOffset": 139}, {"referenceID": 8, "context": "It is common to use recurrent neural networks for belief tracking (Henderson et al., 2014; Wen et al., 2016b) since the output distribution at turn t depends on all user inputs till that turn.", "startOffset": 66, "endOffset": 109}, {"referenceID": 6, "context": "To improve efficiency we extract summary statistics from the belief states, similar to previous work in dialogue policy management (Williams and Young, 2005; Ga\u0161i\u0107 et al., 2009).", "startOffset": 131, "endOffset": 177}, {"referenceID": 19, "context": "This formulation leads to a modified version of the episodic REINFORCE algorithm (Williams, 1992) which we describe below.", "startOffset": 81, "endOffset": 97}, {"referenceID": 10, "context": "This expectation is estimated using a mini-batch of B dialogues, and we use RMSProp (Hinton et al., 2012) updates to train the parameters \u03b8.", "startOffset": 84, "endOffset": 105}, {"referenceID": 14, "context": "Then, a post-processing scan is used to replace the slot placeholders with their actual values, which is similar to the decoder module in (Wen et al., 2015; Wen et al., 2016a).", "startOffset": 138, "endOffset": 175}, {"referenceID": 20, "context": "All these agents are variants of the EMDM strategy proposed in (Wu et al., 2015), with the difference being in the way the entropy is computed.", "startOffset": 63, "endOffset": 80}], "year": 2016, "abstractText": "This paper proposes KB-InfoBot\u2014a dialogue agent that provides users with an entity from a knowledge base (KB) by interactively asking for its attributes. All components of the KB-InfoBot are trained in an end-to-end fashion using reinforcement learning. Goal-oriented dialogue systems typically need to interact with an external database to access real-world knowledge (e.g. movies playing in a city). Previous systems achieved this by issuing a symbolic query to the database and adding retrieved results to the dialogue state. However, such symbolic operations break the differentiability of the system and prevent end-to-end training of neural dialogue agents. In this paper, we address this limitation by replacing symbolic queries with an induced \u201csoft\u201d posterior distribution over the KB that indicates which entities the user is interested in. We also provide a modified version of the episodic REINFORCE algorithm, which allows the KBInfoBot to explore and learn both the policy for selecting dialogue acts and the posterior over the KB for retrieving the correct entities. Experimental results show that the end-to-end trained KB-InfoBot outperforms competitive rule-based baselines, as well as agents which are not end-to-end trainable.", "creator": "LaTeX with hyperref package"}}}