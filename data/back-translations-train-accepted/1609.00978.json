{"id": "1609.00978", "review": {"conference": "NIPS", "VERSION": "v1", "DATE_OF_SUBMISSION": "4-Sep-2016", "title": "Local Maxima in the Likelihood of Gaussian Mixture Models: Structural Results and Algorithmic Consequences", "abstract": "We provide two fundamental results on the population (infinite-sample) likelihood function of Gaussian mixture models with $M \\geq 3$ components. Our first main result shows that the population likelihood function has bad local maxima even in the special case of equally-weighted mixtures of well-separated and spherical Gaussians. We prove that the log-likelihood value of these bad local maxima can be arbitrarily worse than that of any global optimum, thereby resolving an open question of Srebro (2007). Our second main result shows that the EM algorithm (or a first-order variant of it) with random initialization will converge to bad critical points with probability at least $1-e^{-\\Omega(M)}$. We further establish that a first-order variant of EM will not converge to strict saddle points almost surely, indicating that the poor performance of the first-order method can be attributed to the existence of bad local maxima rather than bad saddle points. Overall, our results highlight the necessity of careful initialization when using the EM algorithm in practice, even when applied in highly favorable settings.", "histories": [["v1", "Sun, 4 Sep 2016 19:34:56 GMT  (211kb)", "http://arxiv.org/abs/1609.00978v1", "Neural Information Processing Systems (NIPS) 2016"]], "COMMENTS": "Neural Information Processing Systems (NIPS) 2016", "reviews": [], "SUBJECTS": "stat.ML cs.LG math.OC", "authors": ["chi jin", "yuchen zhang", "sivaraman balakrishnan", "martin j wainwright", "michael i jordan"], "accepted": true, "id": "1609.00978"}, "pdf": {"name": "1609.00978.pdf", "metadata": {"source": "CRF", "title": "Local Maxima in the Likelihood of Gaussian Mixture Models: Structural Results and Algorithmic Consequences", "authors": ["Chi Jin", "Yuchen Zhang", "Sivaraman Balakrishnan", "Martin J. Wainwright", "Michael Jordan"], "emails": ["chijin@cs.berkeley.edu,", "yuczhang@berkeley.edu,", "siva@stat.cmu.edu", "wainwrig@berkeley.edu,", "jordan@cs.berkeley.edu"], "sections": [{"heading": null, "text": "ar Xiv: 160 9,00 978v 1 [st"}, {"heading": "1 Introduction", "text": "In fact, it is as if most of us are able to surpass ourselves, and that they do not. (...) It is not as if they do it. (...) It is not as if they do it. (...) It is as if they do it. (...) It is as if they do it. (...) It is as if they do it. (...) It is as if they do it. (...) It is as if they do it. (...) It is as if they do it. (...) It is as if they do it. (...) It is as if they do it. (...) It is as if they do it. (...). (...). (.). (.). (.). (.). (.). (.). (.). (.) (.) (.) (.) (.) \"(.)\" (.) \"(.)\" (. \"(.)\" (.) \"(.)\" (.) \"(.)\" (.). \"(.)\" (.). \"(.).\" (.) \"(.).\" (.). \"(.)\" (.). \"(.)\" (.). \"(.).).\" (. \"(.).).\" (.). \"(.).\" (.).). \"(.\" (.).). \"(.).\" (.). \"(.).\" (.).). \"(.\" (.).). \"(.).\" (.). \"(.).).\" (.). \"(.\" (.).). \"(.).\" (.). \"(.\" (.).). \"(.).\" (.).). \"(.).\" (.). \"(.).\" (.). \"(.).\" (.). \"(.).\"). \"(.).\" (.). \"(.).).\" (.). \"(.).).\" (.). \"(.).).\"). \").\" ("}, {"heading": "2 Background and Preliminaries", "text": "In this section we formally define the Gaussian mixing model that we examine in the work. Afterwards we describe the EM algorithm, the EM algorithm of first order as well as the shape of the random initialization that we analyze. In the entire essay we use [M] to describe the sentence {1, 2, \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????"}, {"heading": "2.1 Gaussian Mixture Models", "text": "A d-dimensional Gaussian mixing model (GMM) with M components can be specified by a collection \u00b5 * = {\u00b5 \u043a i,..., p-dimensional mixing model (GMM) with M components. (..) The density function of a Gaussian mixing model results in the sum of the non-negative mixing weights in one, and a collection \u0440 * 1,... (...) The density function of a Gaussian mixing model is derived from the formp (x-x-quantitative, p-quantitative, p-quantitative) = M-quantitative (x-quantitative), p-quantitative, p-quantitative, p-quantitative, p-quantitative. (.) In this paper, we focus on the idealized situation in which each mixing component is equally weighted, and the covariance of each mixing component is identity. (This leads to a mixing model with a GMM)."}, {"heading": "2.2 Expectation-Maximization Algorithm", "text": "A natural way to estimate the mean vectors is to try to maximize the log probability defined by the samples. Instead of trying to directly maximize the log probability, the EM algorithm is calculated by iteratively maximizing a lower limit on the log probability. This is done by switching between two steps: 1. E-step: For each i-level [M] and each i-level [n], the number of members is calculated by iteratively maximizing a lower limit on the log probability."}, {"heading": "2.3 Random Initialization", "text": "Since the protocol probability function is not concave, the point to which the EM algorithm converges depends on the initial value of \u00b5. In practice, it is standard to choose these values by some form of random initialization. For example, one method is to initialize the mean vectors by evenly sampling from the dataset {x} n = 1. This scheme is intuitively reasonable because it automatically adapts to the locations of the true centers. If the true centers have great mutual distances, the initialized centers will also be scattered. Conversely, if the true centers are concentrated in a small region of space, the initialized centers will also be close to each other. In practice, initializing \u00b5 by uniformally pulling from the data is often more meaningful than pulling \u00b5 from a fixed distribution. In this paper, we analyze the EM algorithm and its variants at population level."}, {"heading": "3 Main results", "text": "We now turn to the statements of our main findings and discuss some of their consequences."}, {"heading": "3.1 Structural properties", "text": "In our first main result (theorem 1), for each M \u2265 3 we have an M component mix of Gaussians in the dimension d = 1, for which the population probability has a bad local maximum, whose log probability is arbitrarily inferior to that obtained by the true parameters. This result provides a negative response to the conjecture of Srebro [21].Theorem 1. For each M \u2265 3 and each constant Cgap > 0 there is a well-separated uniform mixture of M variance of spherical Gaussian GMM (\u00b5) and a local maximum \u00b5 constellation such as this. Theorem 1 and each constant Cgap > 0 underlying intuition. To illustrate this configuration, we give a geometrical description of our construction for M = 3. Assuming that the true centers \u00b5 1, \u00b5 2 and \u00b5 3 are so that the distances between 9 and 9 micrometers are much smaller than the respective components."}, {"heading": "3.2 Algorithmic consequences", "text": "An important implication of Theorem 1 is that any iterative algorithm, such as GMS or gradient ascent, that tries to maximize the likelihood of local updates cannot be globally convergent - that is, cannot converge to (almost) globally optimal solutions from arbitrary initialization. In fact, if such an algorithm is initialized at the local level, they will remain trapped. However, it could be argued that this conclusion is overly pessimistic, since we have only shown that these algorithms fail at a certain point (adversarially chosen). In fact, the very existence of bad local minima suggests no need for practical concerns unless it can be shown that a typical optimization algorithm often converges to one of them. The following result shows that the EM algorithm, when applied to population probability and randomly initialized according to the scheme described in Section 2.2, becomes highly likely to conform to a critical point."}, {"heading": "4 Proofs", "text": "This section is dedicated to the proofs of theorems 1 to 4. Certain technical aspects of the proofs are moved to the Appendix."}, {"heading": "4.1 Proof of Theorem 1", "text": "In this section we will prove theorem 1. The proof consists of three parts: starting from case M = 3, the first part shows the existence of a local maximum for certain GMMs, while the second part shows that this local maximum has a significantly lower probability than the global maximum. In the third part, the general case of M > 3 mixtures is extended."}, {"heading": "4.1.1 Existence of a local maximum", "text": "In this section, we prove the presence of a local maximum by first building a family of GMMs with an internal function (= > \u00b53) and then proving the presence of local maxima in the borderline case, if \u03b3 \u2192 + \u221e. From the continuity of the log probability function, we can then conclude that there is a finite \u03b3 whose corresponding log probability has local maximum values. Let's start by considering the special case of M = 3 components in dimension d = 1. For the parameters R > 0 and \u03b31, we assume that the true centers \u00b5 1 = \u2212 R are given, the local probability 1 = \u2212 R, \u00b5 2 = R, \u00b5 3 = R. By constructing the two centers \u00b5 1 and \u00b5 2 will be relatively close together, while the third center \u00b5 3 is far away from the first two centers. We first assert that with sufficient probability, there is a local minimum, a local minimum, a local minimum exists."}, {"heading": "4.1.2 Log-likelihood at a local maximum", "text": "To prove that the protocol probability of a local maximum can be arbitrarily inferior to the protocol probability of the global maximum, we consider the limit if R \u2192 \u221e. In this case, the limit of the global maximum will belong to the local maxima in the closed sentence D. We have previously established the existence of such a local maximum, knowing that either \"\u00b5\" = (\"\u00b5\" 1, \"\u00b5\" 2, \"\u00b5\" 3) must be one of the local maxima in the closed sentence D. Without losing generality, we can assume that \"\u00b5\" 2 \u2212 \u00b5 \"2 = 2R.\" From the definition of the sentence D, we can also see that \"\u00b5\" 2 \u2212 \u00b5 \"1 | > R or\" \u00b5 1 \u2212 \u00b5 \"1 | > R\" must be true."}, {"heading": "4.1.3 Extension to the case M > 3", "text": "We will now give an overview of how this argument can be extended to the general setting of M > 3. Consider a GMM with true centering \u00b5 1 = (2i \u2212 k) R k \u2212 2, for i = 1, \u00b7 \u00b7 \u00b7, M \u2212 1 and \u00b5 \u0445 M = \u03b3R, for a parameter \u03b3 > 0 to be selected. We claim that if \u03b3 is sufficiently large, there is at least a local maximum in the closed setDM = {(\u00b51, \u00b7 \u00b7 \u00b7, \u00b5M) | \u00b51 \u2264 \u03b3R3, \u00b52 \u2265 2\u03b3R3, \u00b7, \u00b5M \u2265 2\u03b3R3}. The proof follows from an identical argument as in M = 3."}, {"heading": "4.2 Proof of Theorem 2", "text": "In this section, we will prove theorem 2. We will first present an important technical problem that deals with the behavior of the GMM algorithm (GMM) for a particular configuration of true and initial centers. We will then prove the theorem by constructing a bad example and applying this problem recursively. Proof of this problem will be given in Appendix A. We will focus on the one-dimensional constellation during this proof. We will also use Bx to represent the complementarity of the interval. Bx (\u03b4) to denote an interval centered on x, that is, Bx (\u03b4), integer. We will also use Bx to represent the complementarity of the interval. Bx, i.e. Bx \u2212 \u03b4), the initial distribution, that is, Bx, integer. First, let us define a class of GMM that we call diffuse GMMs, which we call a diffuse model of GMM that we call GMM."}, {"heading": "4.3 Proof of Theorem 3", "text": "We now begin with the proof of theorem 3. The proof follows from a design similar to the proof of theorem 2, and we develop here only the main ideas. Specifically, it is easy to verify that in order to prove the result, we only need to establish the analogue of Lemma 1 for the EM algorithm of the first order. Intuitively, we first argue that the EM updates of the first order can be regarded as less aggressive versions of the corresponding EM updates, and we use this fact to argue that Lemma 1 continues to apply to the EM algorithm of the first order. Specifically, we can compare the update of the EM algorithm: \u00b5new, EMi = E\u00b5 wi (X) \u00b7 X E\u00b5 wi (X) with the update of the EM algorithm of the first order: \u00b5new, first-order EMi + sE\u00b5 wi (X \u2212 \u00b5i).If we choose the resize change for any parameter EM-\u00b5i, we choose Lemma."}, {"heading": "4.4 Proof of Theorem 4", "text": "In this section, we will prove theorem 4 (new) + 1 (new). In the course of this proof, we will use the fact that the first order is EM updates with the increment s (0, 1), which affects the Hessian state map. (11) To reflect on the behavior of the first order EM algorithm, we will first provide a result concerning the Hessian state map. We can now prove the theorem assertions that the first order EM algorithm is converted with the increment s (0, 1) to a critical point. By a Taylor extension of the log likelihood function, which we haveL (new) + < L (new) + < L (new)."}, {"heading": "Acknowledgements", "text": "This work was partially supported by the Office of Naval Research MURI grant DOD-002888, the Air Force Office of Scientific Research Grant AFOSR-FA9550-14-1-001, the Mathematical Data Science Program of the Office of Naval Research under grant number N00014-15-1-2670, and the National Science Foundation Grant CIF-31712-23800."}, {"heading": "A Proofs of Technical Lemmas", "text": "Most of this section is devoted to the proof of Lemma 1 based on a number of technical lemmas.A.1 Proof of Lemma 1Underlying our proof is the following auxiliary results: Lemma 4. \u2212 \u2212 Suppose the true distribution is a GMM (\u00b5) with M components and that all true centers are located in (\u2212 \u221e, \u2212 10a) with at least one center in (a, 3a), with a > logM + 3. (b) The current configuration of the centers has the property that for each true center there is one such center in (\u2212 10a), there is one current center in (a, \u2212) that has such a center that there is such a center in (a, 4a)."}], "references": [{"title": "Identifiability of parameters in latent structure models with many observed variables", "author": ["Elizabeth S Allman", "Catherine Matias", "John A Rhodes"], "venue": "Annals of Statistics,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2009}, {"title": "Maximum likelihood estimates for Gaussian mixtures are transcendental", "author": ["Carlos Am\u00e9ndola", "Mathias Drton", "Bernd Sturmfels"], "venue": "In International Conference on Mathematical Aspects of Computer and Information Sciences,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2015}, {"title": "Learning mixtures of separated nonspherical Gaussians", "author": ["Sanjeev Arora", "Ravi Kannan"], "venue": "The Annals of Applied Probability,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2005}, {"title": "Statistical guarantees for the EM algorithm: From population to sample-based analysis", "author": ["Sivaraman Balakrishnan", "Martin J Wainwright", "Bin Yu"], "venue": "Annals of Statistics,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2015}, {"title": "Polynomial learning of distribution families", "author": ["Mikhail Belkin", "Kaushik Sinha"], "venue": "In 51st Annual IEEE Symposium on Foundations of Computer Science,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2010}, {"title": "Learning mixtures of product distributions using correlations and independence", "author": ["Kamalika Chaudhuri", "Satish Rao"], "venue": "In 21st Annual Conference on Learning Theory,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2008}, {"title": "Optimal rate of convergence for finite mixture models", "author": ["Jiahua Chen"], "venue": "Annals of Statistics,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 1995}, {"title": "A probabilistic analysis of EM for mixtures of separated, spherical Gaussians", "author": ["Sanjoy Dasgupta", "Leonard Schulman"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2007}, {"title": "Maximum likelihood from incomplete data via the EM algorithm", "author": ["Arthur P Dempster", "Nan M Laird", "Donald B Rubin"], "venue": "Journal of the Royal Statistical Society, Series B,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 1977}, {"title": "The \u201cautomatic\u201d robustness of minimum distance functionals", "author": ["David L Donoho", "Richard C Liu"], "venue": "Annals of Statistics,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 1988}, {"title": "Escaping from saddle points\u2014online stochastic gradient for tensor decomposition", "author": ["Rong Ge", "Furong Huang", "Chi Jin", "Yang Yuan"], "venue": "In 28th Annual Conference on Learning Theory,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2015}, {"title": "Rates of convergence for the Gaussian mixture sieve", "author": ["Christopher R Genovese", "Larry Wasserman"], "venue": "Annals of Statistics,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2000}, {"title": "Entropies and rates of convergence for maximum likelihood and Bayes estimation for mixtures of normal densities", "author": ["Subhashis Ghosal", "Aad W Van Der Vaart"], "venue": "Annals of Statistics,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2001}, {"title": "Identifiability and optimal rates of convergence for parameters of multiple types in finite mixtures", "author": ["Nhat Ho", "XuanLong Nguyen"], "venue": "arXiv preprint arXiv:1501.02497,", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2015}, {"title": "Learning mixtures of spherical Gaussians: Moment methods and spectral decompositions", "author": ["Daniel Hsu", "Sham M Kakade"], "venue": "In Proceedings of the 4th Conference on Innovations in Theoretical Computer Science,", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2013}, {"title": "Gradient descent converges to minimizers", "author": ["Jason D Lee", "Max Simchowitz", "Michael I Jordan", "Benjamin Recht"], "venue": "In 29th Annual Conference on Learning Theory,", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2016}, {"title": "Regularized M-estimators with nonconvexity: Statistical and algorithmic theory for local optima", "author": ["Po-Ling Loh", "Martin J Wainwright"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2013}, {"title": "Settling the polynomial learnability of mixtures of Gaussians", "author": ["Ankur Moitra", "Gregory Valiant"], "venue": "In 51st Annual IEEE Symposium on Foundations of Computer Science,", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2010}, {"title": "Gradient descent converges to minimizers: The case of non-isolated critical points", "author": ["Ioannis Panageas", "Georgios Piliouras"], "venue": "arXiv preprint arXiv:1605.00405,", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2016}, {"title": "On the saddle point problem for non-convex optimization", "author": ["Razvan Pascanu", "Yann N Dauphin", "Surya Ganguli", "Yoshua Bengio"], "venue": "arXiv preprint arXiv:1405.4604,", "citeRegEx": "20", "shortCiteRegEx": "20", "year": 2014}, {"title": "Are there local maxima in the infinite-sample likelihood of Gaussian mixture estimation", "author": ["Nathan Srebro"], "venue": "In 20th Annual Conference on Learning Theory,", "citeRegEx": "21", "shortCiteRegEx": "21", "year": 2007}, {"title": "Identifiability of finite mixtures", "author": ["Henry Teicher"], "venue": "The Annals of Mathematical Statistics,", "citeRegEx": "22", "shortCiteRegEx": "22", "year": 1963}, {"title": "Statistical Analysis of Finite Mixture Distributions", "author": ["D Michael Titterington"], "venue": null, "citeRegEx": "23", "shortCiteRegEx": "23", "year": 1985}, {"title": "A spectral algorithm for learning mixtures of distributions", "author": ["Santosh Vempala", "Grant Wang"], "venue": "In The 43rd Annual IEEE Symposium on Foundations of Computer Science,", "citeRegEx": "24", "shortCiteRegEx": "24", "year": 2002}], "referenceMentions": [{"referenceID": 20, "context": "We prove that the log-likelihood value of these bad local maxima can be arbitrarily worse than that of any global optimum, thereby resolving an open question of Srebro [21].", "startOffset": 168, "endOffset": 172}, {"referenceID": 22, "context": "Their ability to model data as arising from underlying subpopulations provides essential flexibility in a wide range of applications Titterington [23].", "startOffset": 146, "endOffset": 150}, {"referenceID": 21, "context": "Early work [22] studied the identifiability of finite mixture models, and this problem has continued to attract significant interest (see the recent paper of Allman et al.", "startOffset": 11, "endOffset": 15}, {"referenceID": 0, "context": "[1] for a recent", "startOffset": 0, "endOffset": 3}, {"referenceID": 11, "context": "More recent theoretical work has focused on issues related to the use of GMMs for the density estimation problem [12, 13].", "startOffset": 113, "endOffset": 121}, {"referenceID": 12, "context": "More recent theoretical work has focused on issues related to the use of GMMs for the density estimation problem [12, 13].", "startOffset": 113, "endOffset": 121}, {"referenceID": 6, "context": "Focusing on rates of convergence for parameter estimation in GMMs, Chen [7] established the surprising result that when the number of mixture components is unknown, then the standard \u221a n-rate for regular parametric models is not achievable.", "startOffset": 72, "endOffset": 75}, {"referenceID": 13, "context": "Recent investigations [14] into exact-fitted, under-fitted and over-fitted GMMs have characterized the achievable rates of convergence in these settings.", "startOffset": 22, "endOffset": 26}, {"referenceID": 8, "context": "From an algorithmic perspective, the dominant practical method for estimating GMMs is the Expectation-Maximization (EM) algorithm [9].", "startOffset": 130, "endOffset": 133}, {"referenceID": 2, "context": "Broadly, these algorithms are based either on clustering [3, 6, 8, 24] or on the method of moments [5, 15, 18].", "startOffset": 57, "endOffset": 70}, {"referenceID": 5, "context": "Broadly, these algorithms are based either on clustering [3, 6, 8, 24] or on the method of moments [5, 15, 18].", "startOffset": 57, "endOffset": 70}, {"referenceID": 7, "context": "Broadly, these algorithms are based either on clustering [3, 6, 8, 24] or on the method of moments [5, 15, 18].", "startOffset": 57, "endOffset": 70}, {"referenceID": 23, "context": "Broadly, these algorithms are based either on clustering [3, 6, 8, 24] or on the method of moments [5, 15, 18].", "startOffset": 57, "endOffset": 70}, {"referenceID": 4, "context": "Broadly, these algorithms are based either on clustering [3, 6, 8, 24] or on the method of moments [5, 15, 18].", "startOffset": 99, "endOffset": 110}, {"referenceID": 14, "context": "Broadly, these algorithms are based either on clustering [3, 6, 8, 24] or on the method of moments [5, 15, 18].", "startOffset": 99, "endOffset": 110}, {"referenceID": 17, "context": "Broadly, these algorithms are based either on clustering [3, 6, 8, 24] or on the method of moments [5, 15, 18].", "startOffset": 99, "endOffset": 110}, {"referenceID": 7, "context": "Dasgupta and Schulman [8] analyzed a two-round variant of EM, which involved over-fitting the mixture and then pruning extra centers.", "startOffset": 22, "endOffset": 25}, {"referenceID": 3, "context": "[4] studied the local convergence of the EM algorithm for a mixture of two Gaussians with \u03a9(1)-separation.", "startOffset": 0, "endOffset": 3}, {"referenceID": 9, "context": "In fact, maximum likelihood has favorable properties in this regard\u2014maximumlikelihood estimates are well known to be robust to perturbations in the Kullback-Leibler metric of the generative model [10].", "startOffset": 196, "endOffset": 200}, {"referenceID": 20, "context": "For this favorable setting, Srebro [21] conjectured that any local maximum of the likelihood function is a global maximum in the limit of infinite samples\u2014in other words, that there are no bad local maxima for the population GMM likelihood function.", "startOffset": 35, "endOffset": 39}, {"referenceID": 1, "context": "For suitably small sample sizes, it is known [2] that configurations of the samples can be constructed which lead to the likelihood function having an unbounded number of local maxima.", "startOffset": 45, "endOffset": 48}, {"referenceID": 20, "context": "The conjecture of Srebro [21] avoids this by requiring that the samples come from the specified GMM, as well as by considering the (infinite-sample-size) population setting.", "startOffset": 25, "endOffset": 29}, {"referenceID": 20, "context": "A mixture of two spherical Gaussians: A Gaussian mixture model with a single component is simply a Gaussian, so the conjecture of Srebro [21] holds trivially in this case.", "startOffset": 137, "endOffset": 141}, {"referenceID": 20, "context": "Our first contribution is a negative answer to the open question of Srebro [21].", "startOffset": 75, "endOffset": 79}, {"referenceID": 20, "context": "On the basis of empirical evidence, Srebro [21] conjectured that this population log-likelihood is in fact well-behaved, in the sense of having no spurious local optima.", "startOffset": 43, "endOffset": 47}, {"referenceID": 3, "context": "[4], is the first-order EM algorithm.", "startOffset": 0, "endOffset": 3}, {"referenceID": 3, "context": "(6) This update turns out to be equivalent to gradient ascent on the population likelihood L with step size s > 0 (see the paper [4] for details).", "startOffset": 129, "endOffset": 132}, {"referenceID": 20, "context": "This result provides a negative answer to the conjecture of Srebro [21].", "startOffset": 67, "endOffset": 71}, {"referenceID": 14, "context": "This result strongly suggests that that effective initialization schemes, such as those based on pilot estimators utilizing the method of moments [15, 18], are critical to finding good maxima in general GMMs.", "startOffset": 146, "endOffset": 154}, {"referenceID": 17, "context": "This result strongly suggests that that effective initialization schemes, such as those based on pilot estimators utilizing the method of moments [15, 18], are critical to finding good maxima in general GMMs.", "startOffset": 146, "endOffset": 154}, {"referenceID": 3, "context": "[4].", "startOffset": 0, "endOffset": 3}, {"referenceID": 19, "context": "[20] argue that for high-dimensional optimization problems, the principal difficulty is the proliferation of saddle points, not the existence of poor local maxima.", "startOffset": 0, "endOffset": 4}, {"referenceID": 15, "context": "In our setting, however, we can leverage recent results on gradient methods [16, 19] to show that the first-order EM algorithm cannot converge to strict saddle points.", "startOffset": 76, "endOffset": 84}, {"referenceID": 18, "context": "In our setting, however, we can leverage recent results on gradient methods [16, 19] to show that the first-order EM algorithm cannot converge to strict saddle points.", "startOffset": 76, "endOffset": 84}, {"referenceID": 10, "context": "Definition 1 (Strict saddle point [11]).", "startOffset": 34, "endOffset": 38}, {"referenceID": 15, "context": "The proof of Theorem 4 is based on recent work [16, 19] on the asymptotic performance of gradient methods.", "startOffset": 47, "endOffset": 55}, {"referenceID": 18, "context": "The proof of Theorem 4 is based on recent work [16, 19] on the asymptotic performance of gradient methods.", "startOffset": 47, "endOffset": 55}, {"referenceID": 0, "context": "Consequently, there must exist some \u03b8i \u2208 [0, 1] such that \u03bc first-order EM i = \u03b8i\u03bci + (1\u2212 \u03b8i)\u03bc new, EM i .", "startOffset": 41, "endOffset": 47}, {"referenceID": 15, "context": "We do this via a technique that has been used in recent papers [16, 19], exploiting the stable manifold theorem from dynamical systems theory.", "startOffset": 63, "endOffset": 71}, {"referenceID": 18, "context": "We do this via a technique that has been used in recent papers [16, 19], exploiting the stable manifold theorem from dynamical systems theory.", "startOffset": 63, "endOffset": 71}, {"referenceID": 15, "context": "With these definitions in place, we can state an intermediate result: Lemma 3 ([16, 19]).", "startOffset": 79, "endOffset": 87}, {"referenceID": 18, "context": "With these definitions in place, we can state an intermediate result: Lemma 3 ([16, 19]).", "startOffset": 79, "endOffset": 87}], "year": 2016, "abstractText": "We provide two fundamental results on the population (infinite-sample) likelihood function of Gaussian mixture models with M \u2265 3 components. Our first main result shows that the population likelihood function has bad local maxima even in the special case of equally-weighted mixtures of well-separated and spherical Gaussians. We prove that the log-likelihood value of these bad local maxima can be arbitrarily worse than that of any global optimum, thereby resolving an open question of Srebro [21]. Our second main result shows that the EM algorithm (or a first-order variant of it) with random initialization will converge to bad critical points with probability at least 1 \u2212 e. We further establish that a first-order variant of EM will not converge to strict saddle points almost surely, indicating that the poor performance of the first-order method can be attributed to the existence of bad local maxima rather than bad saddle points. Overall, our results highlight the necessity of careful initialization when using the EM algorithm in practice, even when applied in highly favorable settings.", "creator": "LaTeX with hyperref package"}}}