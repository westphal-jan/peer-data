{"id": "1609.01000", "review": {"conference": "ICML", "VERSION": "v1", "DATE_OF_SUBMISSION": "4-Sep-2016", "title": "Convexified Convolutional Neural Networks", "abstract": "We describe the class of convexified convolutional neural networks (CCNNs), which capture the parameter sharing of convolutional neural networks in a convex manner. By representing the nonlinear convolutional filters as vectors in a reproducing kernel Hilbert space, the CNN parameters can be represented as a low-rank matrix, which can be relaxed to obtain a convex optimization problem. For learning two-layer convolutional neural networks, we prove that the generalization error obtained by a convexified CNN converges to that of the best possible CNN. For learning deeper networks, we train CCNNs in a layer-wise manner. Empirically, CCNNs achieve performance competitive with CNNs trained by backpropagation, SVMs, fully-connected neural networks, stacked denoising auto-encoders, and other baseline methods.", "histories": [["v1", "Sun, 4 Sep 2016 23:57:43 GMT  (145kb,D)", "http://arxiv.org/abs/1609.01000v1", "29 pages"]], "COMMENTS": "29 pages", "reviews": [], "SUBJECTS": "cs.LG", "authors": ["yuchen zhang", "percy liang", "martin j wainwright"], "accepted": true, "id": "1609.01000"}, "pdf": {"name": "1609.01000.pdf", "metadata": {"source": "CRF", "title": "Convexified Convolutional Neural Networks", "authors": ["Yuchen Zhang", "Percy Liang", "Martin J. Wainwright"], "emails": ["zhangyuc@cs.stanford.edu.", "pliang@cs.stanford.edu.", "wainwrig@eecs.berkeley.edu."], "sections": [{"heading": "1 Introduction", "text": "There are two main advantages of a CNN over a fully connected neural network: (i) that any nonlinear convolution method is only based on a local patch of the input, and (ii) that the same filter is applied to each patch. However, as with most neural networks, the standard approach to forming CNNs is based on solving a nonconvex optimization problem known to be NP-hard. In practice, researchers use some of the stochastic gradient method, in which gradients are calculated using back propagation [7]. This approach has two drawbacks: (i) the convergence rate, which can only be slow to a local optimum-optimum convergence convergence method."}, {"heading": "2 Background and problem set-up", "text": "In this section we formalize the class of convex neural networks to be learned and describe the associated non-convex optimization problem."}, {"heading": "2.1 Convolutional neural networks.", "text": "x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x"}, {"heading": "2.2 Empirical risk minimization.", "text": "Given an input-output pair (x, y) and a CNN-f, we have L (f (x); y) denote the loss that occurs when the output y is predicted by f (x). We assume that the loss function L convex and L-Lipschitz specify any value of their second argument in their first argument. As a concrete example of multi-class classification with d2 classes, the output vector y takes values in the discrete set [d2] = {1, 2,.., d2}.1 Average merge and multiple channels are also integral parts of CNNs, but these do not present any new technical challenges, so we can extend these extensions to Section 4.For example, for a vector f (x) = (f1 (x),., fd2 (y) and Rd2 of the classification values, the associated multiclassical logistical loss for a pair (x, y) by L (f).empy: we can solve a problem (x) (1)."}, {"heading": "3 Convexifying CNNs", "text": "Let us now turn to the development of the class of convexified CNNs. We begin in Section 3.1 with an illustration of the procedure for the specific case of the linear activation function. Although the linear case is not of practical interest, it provides an intuition for our more general convexification procedure, which is described in Section 3.2 and applies to nonlinear activation functions. In particular, we show how embedding the nonlinear problem in a appropriately selected reproducing Hilbert space (RKHS) allows us to reduce ourselves back to the linear setting."}, {"heading": "3.1 Linear activation functions: low rank relaxations", "text": "In order to find a solution to our problem, we must start by looking at the simple case of linear activation (t), in which case it is as if we are focusing on the reactionary, reactionary, reactionary, reactionary, reactionary, reactionary, reactionary, reactionary, reactionary, reactionary, reactionary, reactionary, reactionary, reactionary, reactionary, reactionary, reactionary, reactionary, reactionary, reactionary, reactionary, reactionary, reactionary, reactionary, reactionary, reactionary, reactionary, reactionary, reactionary, reactionary, reactionary, reactionary, reactionary, reactionary, reactionary, reactionary, reactionary, reactionary, reactionary, reactionary, reactionary, reactionary, reactionary, reactionary, reactionary, reactionary, reactionary, reactionary, reactionary, reactionary, reactionary, reactionary, reactionary, reactionary, reactionary, reactionary, reactionary, reactionary, reactionary, reactionary, reactionary, reactionary, reactionary, reactionary, reactionary, reactionary, reactionary, reactionary, reactionary, reactionary, reactionary, reactionary, reactionary, reactionary, reactionary, reactionary, reactionary, reactionary, reactionary, reactionary, reactionary, reactionary, reactionary, reactionary, reactionary, reactionary, reactionary, reactionary, reactionary, reactionary, reactionary, reactionary, reactionary, reactionary, reactionary, reactionary, reactionary, reactionary, reactionary, reactionary, reactionary, reactionary, reactionary, reactionary, reactionary, reactionary, reactionary, reactionary, reactionary, reactionary, reactionary, reactionary, reactionary, reactionary, reactionary, reactionary, reactionary, reac"}, {"heading": "3.2 Nonlinear activations: RKHS filters", "text": "For non-linear activation functions (xi), we can convert the class of CNN filters to a reproducing kernel (RKHS). As we show, this relaxation allows us to reduce the problem to the linear activation function. Let's show that the filter h: z 7 \"(< w, z >) is included in the RKHS, which is provided by the kernel function K. See Section 3.4 for the choice of kernel function and activation function. Let's show that the filter h: z 7\" (z, z >) is included in the RKHS."}, {"heading": "3.3 Algorithm", "text": "The algorithm for learning a two-layer CCNN is summarized in algorithm 1; it is a formalization of the steps described in Section 3.2. To solve the optimization problem (12), the simplest approach is to lower via a projected gradient: at iteration t it forms the new matrix At + 1 based on the previous iteration At: At + 1 = [A] p (At). (13) Here it denotes the gradient of the objective function defined in (12), and in this case it denotes the Euclidean matrix projection on the nuclear standard ball {A] p: This nuclear standard projection can be achieved by first calculating the singular value substitution of A, and then the vector of the singular values is projected onto the '1 sphere."}, {"heading": "3.4 Theoretical results", "text": "In this section, we will consider the generalization errors of algorithm 1, proving that CNN has the best possible generalization errors. (We will focus on the case of binary classification, where the output dimension is d2 = 1.) We will consider core functions whose associated RKHS function is large enough to contain any function that takes the following form: z 7 \u2192 q, z >), where q is an arbitrary polynomial function and w-Rd1 is an arbitrary vector. As a concrete example, we will consider the inverse polynomial structure: K (z, z): 1 \u2212 2 \u2212 < z >, z \u00b2, z \u00b2."}, {"heading": "4 Learning multi-layer CCNNs", "text": "In this section, we describe a heuristic method for learning multi-layer CNNs. The idea is to estimate the parameters of the revolutionary layers incrementally from bottom to top. Before introducing the multi-layer algorithm, we introduce two extensions, average pooling and multi-channel inputs. Average pooling is a technique to reduce the output dimension of the revolutionary layer from dimensions P \u00b7 r to dimensions P \u2032 r with P \u2032 < P. Suppose that the filter hj is applied to all patch vectors, the output vector produces Hj (x): (hj (x))."}, {"heading": "5 Experiments", "text": "In this section, we compare the CCNN approach with other methods. The results are reported on the MNIST dataset and its variations on digit recognition, and on the CIFAR 10 dataset for object classification."}, {"heading": "5.1 MNIST and variations", "text": "Since the basic MNIST digits are relatively easy to classify, we also consider more difficult variations as such. These variations are known to be difficult for methods without a convolution mechanism (for example, see the paper [44]). Figure 3 shows a number of sample images from these different data sets. All images are 28 \u00d7 28 in size. For all data sets, we use 10,000 images for training, 2,000 images for validation, and 50,000 images for testing. This 10k / 2k / 50k partitioning is standard for MNIST variations [43].Implementation details.For the CCNN method and the baseline CNN method, we use 10,000 images each for training and three-layer models with different layers each. The models with k convolutionary layers are denoted by CCNN-k and CNN-k. Each revolutionary layer is constructed on 5 \u00d7 5 patches with unit stripping, followed by 2 \u00d7 2 average pooling."}, {"heading": "5.2 CIFAR-10", "text": "To test CCNN's capability in complex classification tasks, we report on its performance on the CIFAR-10 Dataset [24]. The dataset consists of 60,000 images divided into 10 classes. Each image has 32 x 32 pixels in RGB colors. We use 50k images for training and 10k images for testing. We train CNN and CCNN models with two, three and four layers. Each revolutionary layer is built on 5 x 5 fields of CNN, followed by 3 x 3 average pooling with two pixels. We train 32, 64 filters for the three revolutionary layers from bottom to top."}, {"heading": "6 Related work", "text": "With the empirical success of deep neural networks, interest in theoretical understanding has increased. Bengio et al. [5] showed how to formulate neural network training as a convex optimization problem involving an infinite number of parameters. However, this perspective encourages the gradual insertion of neurons into the network, the generalization error of which Bach investigated [3]. Zhang et al. [47] suggest a polynomial ensemble method for learning fully networked neural networks, but their approach does not address parameter release or the revolutionary setting. Other relevant work on learning fully networked networks include [35, 23, 29]. Aslan et al. [1, 2] propose a method for learning multi-layered latent variable models. They showed that the proposed method represents a convex relaxation for learning fully networked neural networks."}, {"heading": "7 Conclusion", "text": "In this paper, we demonstrated how convex optimization can be used to optimize CNNs efficiently and understand them statistically. Our convex relaxation consists of two parts: the nuclear standard stress for handling parameter distributions and the RKHS relaxation for dealing with nonlinearity. In the two-layer CCNN, we demonstrated that its generalization error converges with that of the best possible two-layer CNN. We treated multilayer CCNNs only heuristically, but observed that adding Morelayers improves performance in practice. In real-world data experiments, we showed that CCNN exceeds traditional CNN at the same depth, is computationally efficient, and can be combined with traditional CNN to achieve better performance. A major open problem is formally investigating the convex relaxation of deep CNNs."}, {"heading": "Acknowledgements", "text": "This work was partially supported by the Office of Naval Research MURI grant DOD-002888 =. < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < &lt"}, {"heading": "B Convex relaxation for nonlinear activation", "text": "In this appendix, we provide a detailed derivation of the problem for nonlinear activation functions that we outlined earlier in Section 3.2. (< wj, z >) Appendix A shows that we have a sufficiently smooth activation function that we are able to find some core functions (< wj, z > Rd1)."}, {"heading": "C Proof of Theorem 1", "text": "Since it is a one-dimensional class, we can use simplified notation (A, A, A, P) for the matrix (A1, A, J, P)."}], "references": [{"title": "Convex two-layer modeling", "author": ["\u00d6. Aslan", "H. Cheng", "X. Zhang", "D. Schuurmans"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2013}, {"title": "Convex deep learning via normalized kernels", "author": ["\u00d6. Aslan", "X. Zhang", "D. Schuurmans"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2014}, {"title": "Breaking the curse of dimensionality with convex neural networks", "author": ["F. Bach"], "venue": "arXiv preprint arXiv:1412.8690,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2014}, {"title": "Rademacher and Gaussian complexities: Risk bounds and structural results", "author": ["P.L. Bartlett", "S. Mendelson"], "venue": "The Journal of Machine Learning Research,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2003}, {"title": "Convex neural networks", "author": ["Y. Bengio", "N.L. Roux", "P. Vincent", "O. Delalleau", "P. Marcotte"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2005}, {"title": "Training a 3-node neural network is NP-complete", "author": ["A.L. Blum", "R.L. Rivest"], "venue": "Neural Networks,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 1992}, {"title": "Online learning and stochastic approximations", "author": ["L. Bottou"], "venue": "On-line learning in neural networks,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 1998}, {"title": "Invariant scattering convolution networks. Pattern Analysis and Machine Intelligence", "author": ["J. Bruna", "S. Mallat"], "venue": "IEEE Transactions on,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2013}, {"title": "Pcanet: A simple deep learning baseline for image classification", "author": ["T.-H. Chan", "K. Jia", "S. Gao", "J. Lu", "Z. Zeng", "Y. Ma"], "venue": "IEEE Transactions on Image Processing,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2015}, {"title": "A fast and accurate dependency parser using neural networks", "author": ["D. Chen", "C.D. Manning"], "venue": "In Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP),", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2014}, {"title": "The loss surface of multilayer networks", "author": ["A. Choromanska", "M. Henaff", "M. Mathieu", "G.B. Arous", "Y. LeCun"], "venue": null, "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2014}, {"title": "An analysis of single-layer networks in unsupervised feature learning", "author": ["A. Coates", "H. Lee", "A.Y. Ng"], "venue": "Ann Arbor,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2010}, {"title": "Toward deeper understanding of neural networks: The power of initialization and a dual view on expressivity", "author": ["A. Daniely", "R. Frostig", "Y. Singer"], "venue": "arXiv preprint arXiv:1602.05897,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2016}, {"title": "Identifying and attacking the saddle point problem in high-dimensional non-convex optimization", "author": ["Y.N. Dauphin", "R. Pascanu", "C. Gulcehre", "K. Cho", "S. Ganguli", "Y. Bengio"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2014}, {"title": "On the Nystr\u00f6m method for approximating a Gram matrix for improved kernel-based learning", "author": ["P. Drineas", "M.W. Mahoney"], "venue": "The Journal of Machine Learning Research,", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2005}, {"title": "Efficient projections onto the `1ball for learning in high dimensions", "author": ["J. Duchi", "S. Shalev-Shwartz", "Y. Singer", "T. Chandra"], "venue": "In Proceedings of the 25th International Conference on Machine Learning,", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2008}, {"title": "Adaptive subgradient methods for online learning and stochastic optimization", "author": ["J. Duchi", "E. Hazan", "Y. Singer"], "venue": "The Journal of Machine Learning Research,", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2011}, {"title": "Compressed sensing: theory and applications", "author": ["Y.C. Eldar", "G. Kutyniok"], "venue": null, "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2012}, {"title": "An empirical study of learning speed in back-propagation networks", "author": ["S.E. Fahlman"], "venue": "Journal of Heuristics,", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 1988}, {"title": "Global optimality in tensor factorization, deep learning, and beyond", "author": ["B.D. Haeffele", "R. Vidal"], "venue": "arXiv preprint arXiv:1506.07540,", "citeRegEx": "20", "shortCiteRegEx": "20", "year": 2015}, {"title": "Deep neural networks for acoustic modeling in speech recognition: The shared views of four research groups", "author": ["G. Hinton", "L. Deng", "D. Yu", "G.E. Dahl", "A.-r. Mohamed", "N. Jaitly", "A. Senior", "V. Vanhoucke", "P. Nguyen", "T.N. Sainath"], "venue": null, "citeRegEx": "21", "shortCiteRegEx": "21", "year": 2012}, {"title": "Suitable mlp network activation functions for breast cancer and thyroid disease detection", "author": ["I. Isa", "Z. Saad", "S. Omar", "M. Osman", "K. Ahmad", "H.M. Sakim"], "venue": "In 2010 Second International Conference on Computational Intelligence, Modelling and Simulation,", "citeRegEx": "22", "shortCiteRegEx": "22", "year": 2010}, {"title": "Generalization bounds for neural networks through tensor factorization", "author": ["M. Janzamin", "H. Sedghi", "A. Anandkumar"], "venue": null, "citeRegEx": "23", "shortCiteRegEx": "23", "year": 2015}, {"title": "Learning multiple layers of features from tiny images", "author": ["A. Krizhevsky", "G. Hinton"], "venue": "Master Thesis,", "citeRegEx": "24", "shortCiteRegEx": "24", "year": 2009}, {"title": "Imagenet classification with deep convolutional neural networks", "author": ["A. Krizhevsky", "I. Sutskever", "G.E. Hinton"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "25", "shortCiteRegEx": "25", "year": 2012}, {"title": "Face recognition: A convolutional neural-network approach", "author": ["S. Lawrence", "C.L. Giles", "A.C. Tsoi", "A.D. Back"], "venue": "Neural Networks, IEEE Transactions on,", "citeRegEx": "26", "shortCiteRegEx": "26", "year": 1997}, {"title": "Fastfood-approximating kernel expansions in loglinear time", "author": ["Q. Le", "T. Sarl\u00f3s", "A. Smola"], "venue": "In Proceedings of the International Conference on Machine Learning,", "citeRegEx": "27", "shortCiteRegEx": "27", "year": 2013}, {"title": "Gradient-based learning applied to document recognition", "author": ["Y. LeCun", "L. Bottou", "Y. Bengio", "P. Haffner"], "venue": "Proceedings of the IEEE,", "citeRegEx": "28", "shortCiteRegEx": "28", "year": 1998}, {"title": "On the computational efficiency of training neural networks", "author": ["R. Livni", "S. Shalev-Shwartz", "O. Shamir"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "29", "shortCiteRegEx": "29", "year": 2014}, {"title": "Convolutional kernel networks", "author": ["J. Mairal", "P. Koniusz", "Z. Harchaoui", "C. Schmid"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "30", "shortCiteRegEx": "30", "year": 2014}, {"title": "On some extensions of Bernstein\u2019s inequality for self-adjoint operators", "author": ["S. Minsker"], "venue": "arXiv preprint arXiv:1112.5448,", "citeRegEx": "31", "shortCiteRegEx": "31", "year": 2011}, {"title": "Human-level control through deep reinforcement learning", "author": ["V. Mnih", "K. Kavukcuoglu", "D. Silver", "A.A. Rusu", "J. Veness", "M.G. Bellemare", "A. Graves", "M. Riedmiller", "A.K. Fidjeland", "G. Ostrovski"], "venue": "Nature, 518(7540):529\u2013533,", "citeRegEx": "32", "shortCiteRegEx": "32", "year": 2015}, {"title": "Random features for large-scale kernel machines", "author": ["A. Rahimi", "B. Recht"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "33", "shortCiteRegEx": "33", "year": 2007}, {"title": "On the quality of the initial basin in overspecified neural networks", "author": ["I. Safran", "O. Shamir"], "venue": "arXiv preprint arXiv:1511.04210,", "citeRegEx": "34", "shortCiteRegEx": "34", "year": 2015}, {"title": "Provable methods for training neural networks with sparse connectivity", "author": ["H. Sedghi", "A. Anandkumar"], "venue": null, "citeRegEx": "35", "shortCiteRegEx": "35", "year": 2014}, {"title": "Learning kernel-based halfspaces with the 0-1 loss", "author": ["S. Shalev-Shwartz", "O. Shamir", "K. Sridharan"], "venue": "SIAM Journal on Computing,", "citeRegEx": "36", "shortCiteRegEx": "36", "year": 2011}, {"title": "Mastering the game of Go with deep neural networks and tree", "author": ["D. Silver", "A. Huang", "C.J. Maddison", "A. Guez", "L. Sifre", "G. Van Den Driessche", "J. Schrittwieser", "I. Antonoglou", "V. Panneershelvam", "M. Lanctot"], "venue": "search. Nature,", "citeRegEx": "37", "shortCiteRegEx": "37", "year": 2016}, {"title": "Learning invariant representations with local transformations", "author": ["K. Sohn", "H. Lee"], "venue": "In Proceedings of the 29th International Conference on Machine Learning", "citeRegEx": "38", "shortCiteRegEx": "38", "year": 2012}, {"title": "Neural networks with periodic and monotonic activation functions: a comparative study in classification problems", "author": ["J.M. Sopena", "E. Romero", "R. Alquezar"], "venue": "In ICANN", "citeRegEx": "39", "shortCiteRegEx": "39", "year": 1999}, {"title": "Dropout: A simple way to prevent neural networks from overfitting", "author": ["N. Srivastava", "G. Hinton", "A. Krizhevsky", "I. Sutskever", "R. Salakhutdinov"], "venue": "The Journal of Machine Learning Research,", "citeRegEx": "40", "shortCiteRegEx": "40", "year": 1929}, {"title": "Support vector machines", "author": ["I. Steinwart", "A. Christmann"], "venue": null, "citeRegEx": "41", "shortCiteRegEx": "41", "year": 2008}, {"title": "Deep learning using linear support vector machines", "author": ["Y. Tang"], "venue": "arXiv preprint arXiv:1306.0239,", "citeRegEx": "42", "shortCiteRegEx": "42", "year": 2013}, {"title": "Stacked denoising autoencoders: Learning useful representations in a deep network with a local denoising criterion", "author": ["P. Vincent", "H. Larochelle", "I. Lajoie", "Y. Bengio", "P.-A. Manzagol"], "venue": "The Journal of Machine Learning Research,", "citeRegEx": "44", "shortCiteRegEx": "44", "year": 2010}, {"title": "End-to-end text recognition with convolutional neural networks", "author": ["T. Wang", "D.J. Wu", "A. Coates", "A.Y. Ng"], "venue": "In Pattern Recognition (ICPR),", "citeRegEx": "45", "shortCiteRegEx": "45", "year": 2012}, {"title": "A proximal stochastic gradient method with progressive variance reduction", "author": ["L. Xiao", "T. Zhang"], "venue": "SIAM Journal on Optimization,", "citeRegEx": "46", "shortCiteRegEx": "46", "year": 2014}, {"title": "Learning halfspaces and neural networks with random initialization", "author": ["Y. Zhang", "J.D. Lee", "M.J. Wainwright", "M.I. Jordan"], "venue": "arXiv preprint arXiv:1511.07948,", "citeRegEx": "47", "shortCiteRegEx": "47", "year": 2015}, {"title": "`1-regularized neural networks are improperly learnable in polynomial time", "author": ["Y. Zhang", "J.D. Lee", "M.I. Jordan"], "venue": "In Proceedings on the 33rd International Conference on Machine Learning,", "citeRegEx": "48", "shortCiteRegEx": "48", "year": 2016}], "referenceMentions": [{"referenceID": 27, "context": "Convolutional neural networks (CNNs) [28] have proven successful across many tasks in machine learning and artificial intelligence, including image classification [28, 25], face recognition [26], speech recognition [21], text classification [45], and game playing [32, 37].", "startOffset": 37, "endOffset": 41}, {"referenceID": 27, "context": "Convolutional neural networks (CNNs) [28] have proven successful across many tasks in machine learning and artificial intelligence, including image classification [28, 25], face recognition [26], speech recognition [21], text classification [45], and game playing [32, 37].", "startOffset": 163, "endOffset": 171}, {"referenceID": 24, "context": "Convolutional neural networks (CNNs) [28] have proven successful across many tasks in machine learning and artificial intelligence, including image classification [28, 25], face recognition [26], speech recognition [21], text classification [45], and game playing [32, 37].", "startOffset": 163, "endOffset": 171}, {"referenceID": 25, "context": "Convolutional neural networks (CNNs) [28] have proven successful across many tasks in machine learning and artificial intelligence, including image classification [28, 25], face recognition [26], speech recognition [21], text classification [45], and game playing [32, 37].", "startOffset": 190, "endOffset": 194}, {"referenceID": 20, "context": "Convolutional neural networks (CNNs) [28] have proven successful across many tasks in machine learning and artificial intelligence, including image classification [28, 25], face recognition [26], speech recognition [21], text classification [45], and game playing [32, 37].", "startOffset": 215, "endOffset": 219}, {"referenceID": 43, "context": "Convolutional neural networks (CNNs) [28] have proven successful across many tasks in machine learning and artificial intelligence, including image classification [28, 25], face recognition [26], speech recognition [21], text classification [45], and game playing [32, 37].", "startOffset": 241, "endOffset": 245}, {"referenceID": 31, "context": "Convolutional neural networks (CNNs) [28] have proven successful across many tasks in machine learning and artificial intelligence, including image classification [28, 25], face recognition [26], speech recognition [21], text classification [45], and game playing [32, 37].", "startOffset": 264, "endOffset": 272}, {"referenceID": 36, "context": "Convolutional neural networks (CNNs) [28] have proven successful across many tasks in machine learning and artificial intelligence, including image classification [28, 25], face recognition [26], speech recognition [21], text classification [45], and game playing [32, 37].", "startOffset": 264, "endOffset": 272}, {"referenceID": 5, "context": "However, as with most neural networks, the standard approach to training CNNs is based on solving a nonconvex optimization problem that is known to be NP-hard [6].", "startOffset": 159, "endOffset": 162}, {"referenceID": 6, "context": "In practice, researchers use some flavor of stochastic gradient method, in which gradients are computed via backpropagation [7].", "startOffset": 124, "endOffset": 127}, {"referenceID": 18, "context": "This approach has two drawbacks: (i) the rate of convergence, which is at best only to a local optimum, can be slow due to nonconvexity (for instance, see the paper [19]), and (ii) its statistical properties are very difficult to understand, as the actual performance is determined by some combination of the CNN architecture along with the optimization algorithm.", "startOffset": 165, "endOffset": 169}, {"referenceID": 46, "context": "This approach is inspired by our earlier work [48], involving a subset of the current authors, in which we developed this relaxation step for fully-connected neural networks.", "startOffset": 46, "endOffset": 50}, {"referenceID": 46, "context": "Our results show that the sample complexity for CCNNs is significantly lower than that of the convexified fully-connected neural network [48], highlighting the importance of parameter sharing.", "startOffset": 137, "endOffset": 141}, {"referenceID": 15, "context": "[16].", "startOffset": 0, "endOffset": 4}, {"referenceID": 16, "context": "There are other efficient optimization algorithms for solving the problem (12), such as the proximal adaptive gradient method [17] and the proximal SVRG method [46].", "startOffset": 126, "endOffset": 130}, {"referenceID": 44, "context": "There are other efficient optimization algorithms for solving the problem (12), such as the proximal adaptive gradient method [17] and the proximal SVRG method [46].", "startOffset": 160, "endOffset": 164}, {"referenceID": 14, "context": "Setting m = nP allows us to solve the exact kernelized problem, but to improve the computation efficiency, we can use Nystr\u00f6m approximation [15] or random feature approximation [33]; both are randomized methods to obtain a tall-and-thin matrix Q \u2208 RnP\u00d7m such that K \u2248 QQ>.", "startOffset": 140, "endOffset": 144}, {"referenceID": 32, "context": "Setting m = nP allows us to solve the exact kernelized problem, but to improve the computation efficiency, we can use Nystr\u00f6m approximation [15] or random feature approximation [33]; both are randomized methods to obtain a tall-and-thin matrix Q \u2208 RnP\u00d7m such that K \u2248 QQ>.", "startOffset": 177, "endOffset": 181}, {"referenceID": 26, "context": "The random feature approximation takes O(mnPd1) time, but can be improved to O(mnP log d1) time using the fast Hadamard transform [27].", "startOffset": 130, "endOffset": 134}, {"referenceID": 35, "context": "[36] for learning halfspaces, and by Zhang et al.", "startOffset": 0, "endOffset": 4}, {"referenceID": 46, "context": "[48] for learning fully-connected neural networks.", "startOffset": 0, "endOffset": 4}, {"referenceID": 9, "context": ", used by [10, 29]).", "startOffset": 10, "endOffset": 18}, {"referenceID": 28, "context": ", used by [10, 29]).", "startOffset": 10, "endOffset": 18}, {"referenceID": 38, "context": ", used by [39, 22]).", "startOffset": 10, "endOffset": 18}, {"referenceID": 21, "context": ", used by [39, 22]).", "startOffset": 10, "endOffset": 18}, {"referenceID": 3, "context": "Combining this bound with the classical Rademacher complexity theory [4], we conclude that the generalization loss of f\u0302ccnn converges to the least possible generalization error of Fccnn.", "startOffset": 69, "endOffset": 72}, {"referenceID": 46, "context": "[48].", "startOffset": 0, "endOffset": 4}, {"referenceID": 0, "context": "We define the multi-channel patch vector as a concatenation of patch vectors for each channel: zp(x) := (zp(x[1]), .", "startOffset": 109, "endOffset": 112}, {"referenceID": 42, "context": "These variations are known to be hard for methods without a convolution mechanism (for instance, see the paper [44]).", "startOffset": 111, "endOffset": 115}, {"referenceID": 32, "context": "The feature matrix Z(x) is constructed via random feature approximation [33] with dimension m = 500 for the first convolutional layer and m = 1000 for the second.", "startOffset": 72, "endOffset": 76}, {"referenceID": 11, "context": "Before training each CCNN layer, we preprocess the input vectors zp(xi) using local contrast normalization and ZCA whitening [12].", "startOffset": 125, "endOffset": 129}, {"referenceID": 37, "context": "The CCNN-2 model is compared against methods that report the state-of-the-art results on these datasets, including the translation-invariant RBM model (TIRBM) [38], the stacked denoising auto-encoder with three hidden layers (SDAE-3) [44], the ScatNet-2 model [8] and the PCANet-2 model [9].", "startOffset": 159, "endOffset": 163}, {"referenceID": 42, "context": "The CCNN-2 model is compared against methods that report the state-of-the-art results on these datasets, including the translation-invariant RBM model (TIRBM) [38], the stacked denoising auto-encoder with three hidden layers (SDAE-3) [44], the ScatNet-2 model [8] and the PCANet-2 model [9].", "startOffset": 234, "endOffset": 238}, {"referenceID": 7, "context": "The CCNN-2 model is compared against methods that report the state-of-the-art results on these datasets, including the translation-invariant RBM model (TIRBM) [38], the stacked denoising auto-encoder with three hidden layers (SDAE-3) [44], the ScatNet-2 model [8] and the PCANet-2 model [9].", "startOffset": 260, "endOffset": 263}, {"referenceID": 8, "context": "The CCNN-2 model is compared against methods that report the state-of-the-art results on these datasets, including the translation-invariant RBM model (TIRBM) [38], the stacked denoising auto-encoder with three hidden layers (SDAE-3) [44], the ScatNet-2 model [8] and the PCANet-2 model [9].", "startOffset": 287, "endOffset": 290}, {"referenceID": 42, "context": "basic rand rot img img+rot SVMrbf [44] 3.", "startOffset": 34, "endOffset": 38}, {"referenceID": 42, "context": "18% NN-1 [44] 4.", "startOffset": 9, "endOffset": 13}, {"referenceID": 37, "context": "28% TIRBM [38] - 4.", "startOffset": 10, "endOffset": 14}, {"referenceID": 42, "context": "50% SDAE-3 [44] 2.", "startOffset": 11, "endOffset": 15}, {"referenceID": 7, "context": "76% ScatNet-2 [8] 1.", "startOffset": 14, "endOffset": 17}, {"referenceID": 8, "context": "48% PCANet-2 [9] 1.", "startOffset": 13, "endOffset": 16}, {"referenceID": 17, "context": "To see their impact on the parameter matrix, we compute the effective rank (ratio between the nuclear norm and the spectral norm, see [18]) of matrix \u00c2.", "startOffset": 134, "endOffset": 138}, {"referenceID": 23, "context": "2 CIFAR-10 In order to test the capability of CCNN in complex classification tasks, we report its performance on the CIFAR-10 dataset [24].", "startOffset": 134, "endOffset": 138}, {"referenceID": 24, "context": "It was known that the generalization performance of the CNN can be improved by training on random crops of the original image [25], so we train the CNN on random 24\u00d724 patches of the image, and test on the central 24\u00d724 patch.", "startOffset": 126, "endOffset": 130}, {"referenceID": 26, "context": "We compare the CCNN against other baseline methods that don\u2019t involve nonconvex optimization: the kernel SVM with Fastfood Fourier features (SVMFastfood) [27], the PCANet-2 model [9] and the convolutional kernel networks (CKN) [30].", "startOffset": 154, "endOffset": 158}, {"referenceID": 8, "context": "We compare the CCNN against other baseline methods that don\u2019t involve nonconvex optimization: the kernel SVM with Fastfood Fourier features (SVMFastfood) [27], the PCANet-2 model [9] and the convolutional kernel networks (CKN) [30].", "startOffset": 179, "endOffset": 182}, {"referenceID": 29, "context": "We compare the CCNN against other baseline methods that don\u2019t involve nonconvex optimization: the kernel SVM with Fastfood Fourier features (SVMFastfood) [27], the PCANet-2 model [9] and the convolutional kernel networks (CKN) [30].", "startOffset": 227, "endOffset": 231}, {"referenceID": 26, "context": "52% SVMFastfood [27] 36.", "startOffset": 16, "endOffset": 20}, {"referenceID": 8, "context": "90% PCANet-2 [9] 22.", "startOffset": 13, "endOffset": 16}, {"referenceID": 29, "context": "86% CKN [30] 21.", "startOffset": 8, "endOffset": 12}, {"referenceID": 24, "context": "[25]).", "startOffset": 0, "endOffset": 4}, {"referenceID": 4, "context": "[5] showed how to formulate neural network training as a convex optimization problem involving an infinite number of parameters.", "startOffset": 0, "endOffset": 3}, {"referenceID": 2, "context": "This perspective encourages incrementally adding neurons to the network, whose generalization error was studied by Bach [3].", "startOffset": 120, "endOffset": 123}, {"referenceID": 45, "context": "[47] propose a polynomial-time ensemble method for learning fully-connected neural networks, but their approach handles neither parameter sharing nor the convolutional setting.", "startOffset": 0, "endOffset": 4}, {"referenceID": 34, "context": "Other relevant works for learning fully-connected networks include [35, 23, 29].", "startOffset": 67, "endOffset": 79}, {"referenceID": 22, "context": "Other relevant works for learning fully-connected networks include [35, 23, 29].", "startOffset": 67, "endOffset": 79}, {"referenceID": 28, "context": "Other relevant works for learning fully-connected networks include [35, 23, 29].", "startOffset": 67, "endOffset": 79}, {"referenceID": 0, "context": "[1, 2] propose a method for learning multi-layer latent variable models.", "startOffset": 0, "endOffset": 6}, {"referenceID": 1, "context": "[1, 2] propose a method for learning multi-layer latent variable models.", "startOffset": 0, "endOffset": 6}, {"referenceID": 13, "context": "Under certain assumptions on the data distribution, it can be shown that any local minimum of a two-layer fully connected neural network has an objective value that is close to the global minimum value [14, 11].", "startOffset": 202, "endOffset": 210}, {"referenceID": 10, "context": "Under certain assumptions on the data distribution, it can be shown that any local minimum of a two-layer fully connected neural network has an objective value that is close to the global minimum value [14, 11].", "startOffset": 202, "endOffset": 210}, {"referenceID": 33, "context": "Similar results have also been established for over-specified neural networks [34], or neural networks that has a certain parallel topology [20].", "startOffset": 78, "endOffset": 82}, {"referenceID": 19, "context": "Similar results have also been established for over-specified neural networks [34], or neural networks that has a certain parallel topology [20].", "startOffset": 140, "endOffset": 144}, {"referenceID": 29, "context": "[30] present convolutional kernel networks.", "startOffset": 0, "endOffset": 4}, {"referenceID": 7, "context": "The ScatNet method [8] uses translation and deformation-invariant filters constructed by wavelet analysis; however, these filters are independent of the data, unlike the analysis in this paper.", "startOffset": 19, "endOffset": 22}, {"referenceID": 12, "context": "[13] show that a randomly initialized CNN can extract features as powerful as kernel methods, but it is not clear how to provably improve the model from a random initialization.", "startOffset": 0, "endOffset": 4}, {"referenceID": 46, "context": "[48].", "startOffset": 0, "endOffset": 4}, {"referenceID": 3, "context": "We refer the reader to Bartlett and Mendelson [4] for an introduction to the theoretical properties of Rademacher complexity.", "startOffset": 46, "endOffset": 49}, {"referenceID": 3, "context": "Thus, the theory of Rademacher complexity [4] guarantees that E[L(Fccnn(X);Y )] \u2264 inf f\u2208Fccnn E[L(f(x); y)] + 2L \u00b7 Rn(Fccnn) + c \u221a n , (33)", "startOffset": 42, "endOffset": 45}], "year": 2016, "abstractText": "We describe the class of convexified convolutional neural networks (CCNNs), which capture the parameter sharing of convolutional neural networks in a convex manner. By representing the nonlinear convolutional filters as vectors in a reproducing kernel Hilbert space, the CNN parameters can be represented as a low-rank matrix, which can be relaxed to obtain a convex optimization problem. For learning two-layer convolutional neural networks, we prove that the generalization error obtained by a convexified CNN converges to that of the best possible CNN. For learning deeper networks, we train CCNNs in a layer-wise manner. Empirically, CCNNs achieve performance competitive with CNNs trained by backpropagation, SVMs, fully-connected neural networks, stacked denoising auto-encoders, and other baseline methods.", "creator": "LaTeX with hyperref package"}}}