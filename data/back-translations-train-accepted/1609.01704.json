{"id": "1609.01704", "review": {"conference": "iclr", "VERSION": "v1", "DATE_OF_SUBMISSION": "6-Sep-2016", "title": "Hierarchical Multiscale Recurrent Neural Networks", "abstract": "Learning both hierarchical and temporal representation has been among the long-standing challenges of recurrent neural networks. Multiscale recurrent neural networks have been considered as a promising approach to resolve this issue, yet there has been a lack of empirical evidence showing that this type of models can actually capture the temporal dependencies by discovering the latent hierarchical structure of the sequence. In this paper, we propose a novel multiscale approach, called the hierarchical multiscale recurrent neural networks, which can capture the latent hierarchical structure in the sequence by encoding the temporal dependencies with different timescales using a novel update mechanism. We show some evidence that our proposed multiscale architecture can discover underlying hierarchical structure in the sequences without using explicit boundary information. We evaluate our proposed model on character-level language modelling and handwriting sequence modelling.", "histories": [["v1", "Tue, 6 Sep 2016 19:37:57 GMT  (2030kb,D)", "http://arxiv.org/abs/1609.01704v1", null], ["v2", "Wed, 7 Sep 2016 18:33:08 GMT  (2030kb,D)", "http://arxiv.org/abs/1609.01704v2", null], ["v3", "Mon, 31 Oct 2016 06:37:30 GMT  (1143kb,D)", "http://arxiv.org/abs/1609.01704v3", null], ["v4", "Wed, 16 Nov 2016 07:44:27 GMT  (1143kb,D)", "http://arxiv.org/abs/1609.01704v4", null], ["v5", "Wed, 14 Dec 2016 18:41:53 GMT  (1144kb,D)", "http://arxiv.org/abs/1609.01704v5", null], ["v6", "Wed, 8 Mar 2017 07:33:38 GMT  (1144kb,D)", "http://arxiv.org/abs/1609.01704v6", null], ["v7", "Thu, 9 Mar 2017 05:22:52 GMT  (1144kb,D)", "http://arxiv.org/abs/1609.01704v7", null]], "reviews": [], "SUBJECTS": "cs.LG", "authors": ["junyoung chung", "sungjin ahn", "yoshua bengio"], "accepted": true, "id": "1609.01704"}, "pdf": {"name": "1609.01704.pdf", "metadata": {"source": "CRF", "title": "Hierarchical Multiscale Recurrent Neural Networks", "authors": ["Junyoung Chung", "Sungjin Ahn", "Yoshua Bengio"], "emails": ["junyoung.chung@umontreal.ca"], "sections": [{"heading": "1 Introduction", "text": "One of the most important foundations of learning in the deep neural networks as well as in the human brain regions (Bengio, 2009; LeCun et al., 2015) is to achieve hierarchical representation with increasing levels of abstraction; a stack of layers of representation learned in a way that has an overarching or unsupervised objective leads to profound changes such as generalization of examples (Hoffman et al., 2016), sharing knowledge between different tasks (Yosinski et al., 2014), and discovery of origin factors (Kingma and Welling, 2013). The remarkable recent successes of neural networks are based on this ability to learn hierarchical structures (Krizhevsky et al.)."}, {"heading": "2 Related Work", "text": "This year, it is as far as ever in the history of the city, where it is as far as never before."}, {"heading": "3 Hierarchical Multiscale Recurrent Neural Networks", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "3.1 Motivation", "text": "In fact, it is the case that most of them are able to abide by the rules they have given themselves, and that they are able to abide by the rules they have given themselves. (7) In fact, it is the case that they are able to abide by the rules. (7) In fact, it is the case that they are able to abide by the rules. (7) In fact, it is the case that they are able to abide by the rules. (7) In fact, it is the case that they are able to break the rules. (7)"}, {"heading": "3.2 The Proposed Model", "text": "In fact, it is so that it is able to fix and correct the mentioned errors."}, {"heading": "3.3 Computing Gradient of Boundary Detector", "text": "Training neural networks with discrete variables requires greater effort, since the default backward propagation is no longer applicable due to non-differentiability. Among a few methods for forming a neural network with discrete variables, such as the REINFORCE (Williams, 1992; Mnih and Gregor, 2014) and the Straight Through estimator (Bengio et al., 2013), we use the Straight Through estimator to train our model. However, the Straight Through estimator is a distorted estimator, because the non-differentiable function used in forward gear (the step function in our case) is replaced by a differentiable function in reverse (the hard sigmoid function in our case). However, the Straight Through estimator is much simpler and often works more efficiently in practice than other impartial but highly variable estimators, such as the REINFORCE. The Straight Through estimator has been used to decrease the Tribaux function until 2015, and the Tilt has also been used in Courbaux (2015)."}, {"heading": "4 Experiments", "text": "We evaluate the proposed model based on two tasks: character-level speech modeling and handwriting sequence generation. Character-level speech modeling is a representative example of discrete sequence modeling, where the discrete symbols form a unique hierarchical multi-scale structure. Real sequence performance is tested on the task of generating handwriting sequences, where there is a relatively clear hierarchical multi-scale structure compared to other data such as speech signals."}, {"heading": "4.1 Character-Level Language modelling", "text": "A sequence task aims to learn the probability distribution over the sequences by minimizing the negative log units with the probability of training sequences. (10), where the number of training sequences, and Tn is the length of the n-th sequence. A word at the time of the sequence n is denoted by xnt, and x n < t denotes all previous words at the time t, N is the number of training sequences, and Tn is the length of the n-th sequence. (1) Penn Treebank, (2). We evaluate our model on three benchmark levels: (1) Penn Treebank, (2) Penn Treebank, (2) Text8 and Hutter Price Wikipedia. We use the bits-per-character (BPC), E [\u2212 log2 p (xt + 1 x), like the rating metric.Model We use a model consisting of an RNN module and an output module."}, {"heading": "4.2 Handwriting Sequence Generation", "text": "We extend the rating of the HM-LSTM to a real-rated sequence, which proves useful as the IAMOnDB (Liwicki and Bunke, 2005) dataset. The IAM-OnDB dataset consists of 12, 179 handwritten examples, each of which has a sequence of (x, y) coordinates and a binary indicator for the pinboard position that tells us (x1, y1: Tn, p1: Tn) where n is an index of a sequence, the model (xt, yt, pt), and the goal is to predict (xt + 1, yt, pt + 1). The pen-up (pt = 1) indicates an end of a stroke, and the pen-down (pt = 0) indicates that a stroke is in progress (xt, yt, pt, pt) that a large shift in the (x, y) is coordinated to start a new stroke."}, {"heading": "5 Conclusion", "text": "In this thesis, we proposed a novel multi-scale RNN architecture that can capture the latent hierarchical structure of the sequences. We introduced two types of new operations into the RNN, namely the COPY and FLUSH operations. To implement these operations, we introduced a set of binary variables and a novel update rule depending on the states of these binary variables. Each binary variable is learned to find segments at its level, so we call this binary variable a limit detector. At character level, the HM-LSTM achieved state-of-the-art results for Penn Treebank and Text8 datasets, and an equilibrium result with the state of the art Wikipedia Hutter Price dataset. Also, the HM-LSTM exceeded the standard LSTM in the task of generating handwriting sequences. Our results and analyses suggest that the proposed HM RNN can reveal the lateral structure of these sequences."}, {"heading": "Acknowledgments", "text": "The authors thank Alex Graves, Tom Schaul and Hado van Hasselt for their fruitful comments and discussions. We thank the following research funding and computer support agencies: Ubisoft, Samsung, NSERC, Calcul Qu\u00e9bec, Compute Canada, the Canada Research Chairs and CIFAR. The authors thank the developers of Theano (Team et al., 2016). JC thanks Arnaud Bergenon and Fr\u00e9d\u00e9ric Bastien for their technical support. JC also thanks Guillaume Alain, Kyle Kastner and David Ha for providing useful code parts."}], "references": [{"title": "Layer normalization", "author": ["J.L. Ba", "J.R. Kiros", "G.E. Hinton"], "venue": "arXiv preprint arXiv:1607.06450.", "citeRegEx": "Ba et al\\.,? 2016", "shortCiteRegEx": "Ba et al\\.", "year": 2016}, {"title": "End-to-end attention-based large vocabulary speech recognition", "author": ["D. Bahdanau", "J. Chorowski", "D. Serdyuk", "Y Bengio"], "venue": "In 2016 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP),", "citeRegEx": "Bahdanau et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Bahdanau et al\\.", "year": 2016}, {"title": "Learning deep architectures for ai", "author": ["Y. Bengio"], "venue": "Foundations and trends R", "citeRegEx": "Bengio,? 2009", "shortCiteRegEx": "Bengio", "year": 2009}, {"title": "Estimating or propagating gradients through stochastic neurons for conditional computation", "author": ["Y. Bengio", "N. L\u00e9onard", "A. Courville"], "venue": "arXiv preprint arXiv:1308.3432.", "citeRegEx": "Bengio et al\\.,? 2013", "shortCiteRegEx": "Bengio et al\\.", "year": 2013}, {"title": "Mixture density networks", "author": ["C.M. Bishop"], "venue": null, "citeRegEx": "Bishop,? \\Q1994\\E", "shortCiteRegEx": "Bishop", "year": 1994}, {"title": "Listen, attend and spell: A neural network for large vocabulary conversational speech recognition", "author": ["W. Chan", "N. Jaitly", "Q. Le", "O. Vinyals"], "venue": "2016 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), pages 4960\u20134964. IEEE.", "citeRegEx": "Chan et al\\.,? 2016", "shortCiteRegEx": "Chan et al\\.", "year": 2016}, {"title": "Learning phrase representations using RNN encoder-decoder for statistical machine translation", "author": ["K. Cho", "B. van Merrienboer", "C. Gulcehre", "F. Bougares", "H. Schwenk", "Y. Bengio"], "venue": "In Proceedings of the Empiricial Methods in Natural Language Processing (EMNLP", "citeRegEx": "Cho et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Cho et al\\.", "year": 2014}, {"title": "Gated feedback recurrent neural networks", "author": ["J. Chung", "C. Gulcehre", "K. Cho", "Y. Bengio"], "venue": "Proceedings of the 32nd International Conference on Machine Learning.", "citeRegEx": "Chung et al\\.,? 2015a", "shortCiteRegEx": "Chung et al\\.", "year": 2015}, {"title": "A recurrent latent variable model for sequential data", "author": ["J. Chung", "K. Kastner", "L. Dinh", "K. Goel", "A. Courville", "Y. Bengio"], "venue": "Advances in Neural Information Processing Systems.", "citeRegEx": "Chung et al\\.,? 2015b", "shortCiteRegEx": "Chung et al\\.", "year": 2015}, {"title": "A character-level decoder without explicit segmentation for neural machine translation", "author": ["J. Chung", "K. Cho", "Y. Bengio"], "venue": "arXiv preprint arXiv:1603.06147.", "citeRegEx": "Chung et al\\.,? 2016", "shortCiteRegEx": "Chung et al\\.", "year": 2016}, {"title": "Recurrent batch normalization", "author": ["T. Cooijmans", "N. Ballas", "C. Laurent", "A. Courville"], "venue": "arXiv preprint arXiv:1603.09025.", "citeRegEx": "Cooijmans et al\\.,? 2016", "shortCiteRegEx": "Cooijmans et al\\.", "year": 2016}, {"title": "Binaryconnect: Training deep neural networks with binary weights during propagations", "author": ["M. Courbariaux", "Y. Bengio", "David", "J.-P."], "venue": "Advances in Neural Information Processing Systems, pages 3123\u20133131.", "citeRegEx": "Courbariaux et al\\.,? 2015", "shortCiteRegEx": "Courbariaux et al\\.", "year": 2015}, {"title": "Hierarchical recurrent neural networks for long-term dependencies", "author": ["S. El Hihi", "Y. Bengio"], "venue": "Advances in Neural Information Processing Systems, pages 493\u2013499. Citeseer.", "citeRegEx": "Hihi and Bengio,? 1995", "shortCiteRegEx": "Hihi and Bengio", "year": 1995}, {"title": "Sequence labelling in structured domains with hierarchical recurrent neural networks", "author": ["S. Fern\u00e1ndez", "A. Graves", "J. Schmidhuber"], "venue": "Proceedings of the 20th international joint conference on Artifical intelligence, pages 774\u2013779. Morgan Kaufmann Publishers Inc.", "citeRegEx": "Fern\u00e1ndez et al\\.,? 2007", "shortCiteRegEx": "Fern\u00e1ndez et al\\.", "year": 2007}, {"title": "Generating sequences with recurrent neural networks", "author": ["A. Graves"], "venue": "arXiv preprint arXiv:1308.0850.", "citeRegEx": "Graves,? 2013", "shortCiteRegEx": "Graves", "year": 2013}, {"title": "Unconstrained on-line handwriting recognition with recurrent neural networks", "author": ["A. Graves", "M. Liwicki", "H. Bunke", "J. Schmidhuber", "S. Fern\u00e1ndez"], "venue": "Advances in Neural Information Processing Systems, pages 577\u2013584.", "citeRegEx": "Graves et al\\.,? 2008", "shortCiteRegEx": "Graves et al\\.", "year": 2008}, {"title": "Speech recognition with deep recurrent neural networks", "author": ["A. Graves", "Mohamed", "A.-R.", "G. Hinton"], "venue": "Acoustics, Speech and Signal Processing (ICASSP), 2013 IEEE International Conference on, pages 6645\u20136649. IEEE.", "citeRegEx": "Graves et al\\.,? 2013", "shortCiteRegEx": "Graves et al\\.", "year": 2013}, {"title": "Long short-term memory", "author": ["S. Hochreiter", "J. Schmidhuber"], "venue": "Neural computation, 9(8), 1735\u20131780.", "citeRegEx": "Hochreiter and Schmidhuber,? 1997", "shortCiteRegEx": "Hochreiter and Schmidhuber", "year": 1997}, {"title": "One-shot adaptation of supervised deep convolutional models", "author": ["J. Hoffman", "E. Tzeng", "J. Donahue", "Y. Jia", "K. Saenko", "T. Darrell"], "venue": "arXiv preprint arXiv:1312.6204.", "citeRegEx": "Hoffman et al\\.,? 2013", "shortCiteRegEx": "Hoffman et al\\.", "year": 2013}, {"title": "Deep networks with stochastic depth", "author": ["G. Huang", "Y. Sun", "Z. Liu", "D. Sedra", "K. Weinberger"], "venue": "arXiv preprint arXiv:1603.09382.", "citeRegEx": "Huang et al\\.,? 2016", "shortCiteRegEx": "Huang et al\\.", "year": 2016}, {"title": "The human knowledge compression contest", "author": ["M. Hutter"], "venue": null, "citeRegEx": "Hutter,? \\Q2012\\E", "shortCiteRegEx": "Hutter", "year": 2012}, {"title": "Batch normalization: Accelerating deep network training by reducing internal covariate shift", "author": ["S. Ioffe", "C. Szegedy"], "venue": "arXiv preprint arXiv:1502.03167.", "citeRegEx": "Ioffe and Szegedy,? 2015", "shortCiteRegEx": "Ioffe and Szegedy", "year": 2015}, {"title": "Grid long short-term memory", "author": ["N. Kalchbrenner", "I. Danihelka", "A. Graves"], "venue": "arXiv preprint arXiv:1507.01526.", "citeRegEx": "Kalchbrenner et al\\.,? 2015", "shortCiteRegEx": "Kalchbrenner et al\\.", "year": 2015}, {"title": "Convolutional neural networks for sentence classification", "author": ["Y. Kim"], "venue": "arXiv preprint arXiv:1408.5882.", "citeRegEx": "Kim,? 2014", "shortCiteRegEx": "Kim", "year": 2014}, {"title": "Character-aware neural language models", "author": ["Y. Kim", "Y. Jernite", "D. Sontag", "A.M. Rush"], "venue": "arXiv preprint arXiv:1508.06615.", "citeRegEx": "Kim et al\\.,? 2015", "shortCiteRegEx": "Kim et al\\.", "year": 2015}, {"title": "Adam: A method for stochastic optimization", "author": ["D.P. Kingma", "J. Ba"], "venue": "arXiv preprint arXiv:1412.6980.", "citeRegEx": "Kingma and Ba,? 2014", "shortCiteRegEx": "Kingma and Ba", "year": 2014}, {"title": "Auto-encoding variational bayes", "author": ["D.P. Kingma", "M. Welling"], "venue": "arXiv preprint arXiv:1312.6114.", "citeRegEx": "Kingma and Welling,? 2013", "shortCiteRegEx": "Kingma and Welling", "year": 2013}, {"title": "Bursty and hierarchical structure in streams", "author": ["J. Kleinberg"], "venue": "Data Mining and Knowledge Discovery, 7(4), 373\u2013397.", "citeRegEx": "Kleinberg,? 2003", "shortCiteRegEx": "Kleinberg", "year": 2003}, {"title": "A clockwork rnn", "author": ["J. Koutn\u00edk", "K. Greff", "F. Gomez", "J. Schmidhuber"], "venue": "Proceedings of the 31st International Conference on Machine Learning (ICML 2014).", "citeRegEx": "Koutn\u00edk et al\\.,? 2014", "shortCiteRegEx": "Koutn\u00edk et al\\.", "year": 2014}, {"title": "Imagenet classification with deep convolutional neural networks", "author": ["A. Krizhevsky", "I. Sutskever", "G.E. Hinton"], "venue": "Advances in Neural Information Processing Systems, pages 1097\u20131105.", "citeRegEx": "Krizhevsky et al\\.,? 2012", "shortCiteRegEx": "Krizhevsky et al\\.", "year": 2012}, {"title": "Regularizing rnns by stabilizing activations", "author": ["D. Krueger", "R. Memisevic"], "venue": "arXiv preprint arXiv:1511.08400.", "citeRegEx": "Krueger and Memisevic,? 2015", "shortCiteRegEx": "Krueger and Memisevic", "year": 2015}, {"title": "Zoneout: Regularizing rnns by randomly preserving hidden activations", "author": ["D. Krueger", "T. Maharaj", "J. Kram\u00e1r", "M. Pezeshki", "N. Ballas", "N.R. Ke", "A. Goyal", "Y. Bengio", "H. Larochelle", "A Courville"], "venue": "arXiv preprint arXiv:1606.01305", "citeRegEx": "Krueger et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Krueger et al\\.", "year": 2016}, {"title": "Deep learning", "author": ["Y. LeCun", "Y. Bengio", "G. Hinton"], "venue": "Nature, 521(7553), 436\u2013444.", "citeRegEx": "LeCun et al\\.,? 2015", "shortCiteRegEx": "LeCun et al\\.", "year": 2015}, {"title": "Character-based neural machine translation", "author": ["W. Ling", "I. Trancoso", "C. Dyer", "A.W. Black"], "venue": "arXiv preprint arXiv:1511.04586.", "citeRegEx": "Ling et al\\.,? 2015", "shortCiteRegEx": "Ling et al\\.", "year": 2015}, {"title": "Iam-ondb-an on-line english sentence database acquired from handwritten text on a whiteboard", "author": ["M. Liwicki", "H. Bunke"], "venue": "Eighth International Conference on Document Analysis and Recognition (ICDAR\u201905), pages 956\u2013961. IEEE.", "citeRegEx": "Liwicki and Bunke,? 2005", "shortCiteRegEx": "Liwicki and Bunke", "year": 2005}, {"title": "Large text compression benchmark", "author": ["M. Mahoney"], "venue": "URL: http://www. mattmahoney. net/text/text. html.", "citeRegEx": "Mahoney,? 2009", "shortCiteRegEx": "Mahoney", "year": 2009}, {"title": "Building a large annotated corpus of english: The penn treebank", "author": ["M.P. Marcus", "M.A. Marcinkiewicz", "B. Santorini"], "venue": "Computational linguistics, 19(2), 313\u2013330.", "citeRegEx": "Marcus et al\\.,? 1993", "shortCiteRegEx": "Marcus et al\\.", "year": 1993}, {"title": "Subword language modeling with neural networks", "author": ["T. Mikolov", "I. Sutskever", "A. Deoras", "Le", "H.-S.", "S. Kombrink", "J. Cernocky"], "venue": "Preprint.", "citeRegEx": "Mikolov et al\\.,? 2012", "shortCiteRegEx": "Mikolov et al\\.", "year": 2012}, {"title": "Neural variational inference and learning in belief networks", "author": ["A. Mnih", "K. Gregor"], "venue": "Proceedings of the 31st International Conference on Machine Learning (ICML-14), pages 1791\u20131799.", "citeRegEx": "Mnih and Gregor,? 2014", "shortCiteRegEx": "Mnih and Gregor", "year": 2014}, {"title": "Human-level control through deep reinforcement learning", "author": ["V. Mnih", "K. Kavukcuoglu", "D. Silver", "A.A. Rusu", "J. Veness", "M.G. Bellemare", "A. Graves", "M. Riedmiller", "A.K. Fidjeland", "G Ostrovski"], "venue": null, "citeRegEx": "Mnih et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Mnih et al\\.", "year": 2015}, {"title": "Strategic attentive writer for learning macro-actions", "author": ["V. Mnih", "J. Agapiou", "S. Osindero", "A. Graves", "O. Vinyals", "K Kavukcuoglu"], "venue": "arXiv preprint arXiv:1606.04695", "citeRegEx": "Mnih et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Mnih et al\\.", "year": 2016}, {"title": "Induction of multiscale temporal structure", "author": ["M.C. Mozer"], "venue": "Advances in neural information processing systems, pages 275\u2013275.", "citeRegEx": "Mozer,? 1993", "shortCiteRegEx": "Mozer", "year": 1993}, {"title": "Rectified linear units improve restricted boltzmann machines", "author": ["V. Nair", "G.E. Hinton"], "venue": "Proceedings of the 27th International Conference on Machine Learning (ICML-10), pages 807\u2013814.", "citeRegEx": "Nair and Hinton,? 2010", "shortCiteRegEx": "Nair and Hinton", "year": 2010}, {"title": "Regularization and nonlinearities for neural language models: when are they needed? arXiv preprint arXiv:1301.5650", "author": ["M. Pachitariu", "M. Sahani"], "venue": null, "citeRegEx": "Pachitariu and Sahani,? \\Q2013\\E", "shortCiteRegEx": "Pachitariu and Sahani", "year": 2013}, {"title": "On the difficulty of training recurrent neural networks", "author": ["R. Pascanu", "T. Mikolov", "Y. Bengio"], "venue": "arXiv preprint arXiv:1211.5063.", "citeRegEx": "Pascanu et al\\.,? 2012", "shortCiteRegEx": "Pascanu et al\\.", "year": 2012}, {"title": "Recurrent memory array structures", "author": ["K.M. Rocki"], "venue": "arXiv preprint arXiv:1607.03085.", "citeRegEx": "Rocki,? 2016a", "shortCiteRegEx": "Rocki", "year": 2016}, {"title": "Surprisal-driven feedback in recurrent networks", "author": ["K.M. Rocki"], "venue": "arXiv preprint arXiv:1608.06027.", "citeRegEx": "Rocki,? 2016b", "shortCiteRegEx": "Rocki", "year": 2016}, {"title": "One-shot learning with memory-augmented neural networks", "author": ["A. Santoro", "S. Bartunov", "M. Botvinick", "D. Wierstra", "T. Lillicrap"], "venue": "arXiv preprint arXiv:1605.06065.", "citeRegEx": "Santoro et al\\.,? 2016", "shortCiteRegEx": "Santoro et al\\.", "year": 2016}, {"title": "Neural sequence chunkers", "author": ["J. Schmidhuber"], "venue": null, "citeRegEx": "Schmidhuber,? \\Q1991\\E", "shortCiteRegEx": "Schmidhuber", "year": 1991}, {"title": "Learning complex, extended sequences using the principle of history compression", "author": ["J. Schmidhuber"], "venue": "Neural Computation, 4(2), 234\u2013242. Schmidhuber, J. (2015). Deep learning in neural networks: An overview. Neural Networks, 61, 85\u2013117.", "citeRegEx": "Schmidhuber,? 1992", "shortCiteRegEx": "Schmidhuber", "year": 1992}, {"title": "Mastering the game of go with deep neural networks and tree", "author": ["D. Silver", "A. Huang", "C.J. Maddison", "A. Guez", "L. Sifre", "G. Van Den Driessche", "J. Schrittwieser", "I. Antonoglou", "V. Panneershelvam", "M Lanctot"], "venue": "search. Nature,", "citeRegEx": "Silver et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Silver et al\\.", "year": 2016}, {"title": "A hierarchical recurrent encoder-decoder for generative context-aware query suggestion", "author": ["A. Sordoni", "Y. Bengio", "H. Vahabi", "C. Lioma", "J. Grue Simonsen", "Nie", "J.-Y."], "venue": "Proceedings of the 24th ACM International on Conference on Information and Knowledge Management, pages 553\u2013562. ACM.", "citeRegEx": "Sordoni et al\\.,? 2015", "shortCiteRegEx": "Sordoni et al\\.", "year": 2015}, {"title": "Dropout: a simple way to prevent neural networks from overfitting", "author": ["N. Srivastava", "G.E. Hinton", "A. Krizhevsky", "I. Sutskever", "R. Salakhutdinov"], "venue": "Journal of Machine Learning Research, 15(1), 1929\u20131958.", "citeRegEx": "Srivastava et al\\.,? 2014", "shortCiteRegEx": "Srivastava et al\\.", "year": 2014}, {"title": "Generating text with recurrent neural networks", "author": ["I. Sutskever", "J. Martens", "G.E. Hinton"], "venue": "Proceedings of the 28th International Conference on Machine Learning (ICML\u201911), pages 1017\u20131024.", "citeRegEx": "Sutskever et al\\.,? 2011", "shortCiteRegEx": "Sutskever et al\\.", "year": 2011}, {"title": "Sequence to sequence learning with neural networks", "author": ["I. Sutskever", "O. Vinyals", "Q.V. Le"], "venue": "Advances in Neural Information Processing Systems, pages 3104\u20133112.", "citeRegEx": "Sutskever et al\\.,? 2014", "shortCiteRegEx": "Sutskever et al\\.", "year": 2014}, {"title": "Theano: A python framework for fast computation of mathematical expressions. arXiv preprint arXiv:1605.02688", "author": ["T.T.D. Team", "R. Al-Rfou", "G. Alain", "A. Almahairi", "C. Angermueller", "D. Bahdanau", "N. Ballas", "F. Bastien", "J. Bayer", "A Belikov"], "venue": null, "citeRegEx": "Team et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Team et al\\.", "year": 2016}, {"title": "Show and tell: A neural image caption generator", "author": ["O. Vinyals", "A. Toshev", "S. Bengio", "D. Erhan"], "venue": "Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 3156\u20133164.", "citeRegEx": "Vinyals et al\\.,? 2015", "shortCiteRegEx": "Vinyals et al\\.", "year": 2015}, {"title": "Simple statistical gradient-following algorithms for connectionist reinforcement learning", "author": ["R.J. Williams"], "venue": "Machine learning, 8(3-4), 229\u2013256.", "citeRegEx": "Williams,? 1992", "shortCiteRegEx": "Williams", "year": 1992}, {"title": "On multiplicative integration with recurrent neural networks", "author": ["Y. Wu", "S. Zhang", "Y. Zhang", "Y. Bengio", "R. Salakhutdinov"], "venue": "arXiv preprint arXiv:1606.06630.", "citeRegEx": "Wu et al\\.,? 2016", "shortCiteRegEx": "Wu et al\\.", "year": 2016}, {"title": "How transferable are features in deep neural networks", "author": ["J. Yosinski", "J. Clune", "Y. Bengio", "H. Lipson"], "venue": "In Advances in neural information processing systems,", "citeRegEx": "Yosinski et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Yosinski et al\\.", "year": 2014}, {"title": "Architectural complexity measures of recurrent neural networks", "author": ["S. Zhang", "Y. Wu", "T. Che", "Z. Lin", "R. Memisevic", "R. Salakhutdinov", "Y. Bengio"], "venue": "arXiv preprint arXiv:1602.08210.", "citeRegEx": "Zhang et al\\.,? 2016", "shortCiteRegEx": "Zhang et al\\.", "year": 2016}, {"title": "Recurrent highway networks", "author": ["J.G. Zilly", "R.K. Srivastava", "J. Koutn\u00edk", "J. Schmidhuber"], "venue": "arXiv preprint arXiv:1607.03474.", "citeRegEx": "Zilly et al\\.,? 2016", "shortCiteRegEx": "Zilly et al\\.", "year": 2016}], "referenceMentions": [{"referenceID": 2, "context": "One of the key principles of learning in deep neural networks as well as in the human brain (Bengio, 2009; LeCun et al., 2015; Schmidhuber, 2015) is to obtain a hierarchical representation with increasing levels of abstraction.", "startOffset": 92, "endOffset": 145}, {"referenceID": 32, "context": "One of the key principles of learning in deep neural networks as well as in the human brain (Bengio, 2009; LeCun et al., 2015; Schmidhuber, 2015) is to obtain a hierarchical representation with increasing levels of abstraction.", "startOffset": 92, "endOffset": 145}, {"referenceID": 18, "context": "A stack of representation layers, learned from the data in a way to optimize a supervised or unsupervised target task, make deep neural networks entertain advantages such as generalization to unseen examples (Hoffman et al., 2013; Santoro et al., 2016), sharing learned knowledge among multiple tasks (Yosinski et al.", "startOffset": 208, "endOffset": 252}, {"referenceID": 47, "context": "A stack of representation layers, learned from the data in a way to optimize a supervised or unsupervised target task, make deep neural networks entertain advantages such as generalization to unseen examples (Hoffman et al., 2013; Santoro et al., 2016), sharing learned knowledge among multiple tasks (Yosinski et al.", "startOffset": 208, "endOffset": 252}, {"referenceID": 59, "context": ", 2016), sharing learned knowledge among multiple tasks (Yosinski et al., 2014) and discovering disentangling factors of variation (Kingma and Welling, 2013).", "startOffset": 56, "endOffset": 79}, {"referenceID": 26, "context": ", 2014) and discovering disentangling factors of variation (Kingma and Welling, 2013).", "startOffset": 59, "endOffset": 85}, {"referenceID": 29, "context": "The remarkable recent successes of the deep convolutional neural networks are particularly based on this ability to learn hierarchical representation for spatial data (Krizhevsky et al., 2012; Mnih et al., 2015; Silver et al., 2016).", "startOffset": 167, "endOffset": 232}, {"referenceID": 39, "context": "The remarkable recent successes of the deep convolutional neural networks are particularly based on this ability to learn hierarchical representation for spatial data (Krizhevsky et al., 2012; Mnih et al., 2015; Silver et al., 2016).", "startOffset": 167, "endOffset": 232}, {"referenceID": 50, "context": "The remarkable recent successes of the deep convolutional neural networks are particularly based on this ability to learn hierarchical representation for spatial data (Krizhevsky et al., 2012; Mnih et al., 2015; Silver et al., 2016).", "startOffset": 167, "endOffset": 232}, {"referenceID": 6, "context": "For modelling temporal data, the recent resurgence of recurrent neural networks (RNN) has led to remarkable advances (Graves et al., 2008, 2013; Cho et al., 2014; Sutskever et al., 2014; Vinyals et al., 2015).", "startOffset": 117, "endOffset": 208}, {"referenceID": 54, "context": "For modelling temporal data, the recent resurgence of recurrent neural networks (RNN) has led to remarkable advances (Graves et al., 2008, 2013; Cho et al., 2014; Sutskever et al., 2014; Vinyals et al., 2015).", "startOffset": 117, "endOffset": 208}, {"referenceID": 56, "context": "For modelling temporal data, the recent resurgence of recurrent neural networks (RNN) has led to remarkable advances (Graves et al., 2008, 2013; Cho et al., 2014; Sutskever et al., 2014; Vinyals et al., 2015).", "startOffset": 117, "endOffset": 208}, {"referenceID": 48, "context": "However, unlike the spatial data, learning both hierarchical and temporal representation has been among the long-standing challenges of RNNs in spite of the fact that hierarchical multiscale structures naturally exist in many temporal data (Schmidhuber, 1991; Mozer, 1993; El Hihi and Bengio, 1995; Kleinberg, 2003; Koutn\u00edk et al., 2014; Chung et al., 2016).", "startOffset": 240, "endOffset": 357}, {"referenceID": 41, "context": "However, unlike the spatial data, learning both hierarchical and temporal representation has been among the long-standing challenges of RNNs in spite of the fact that hierarchical multiscale structures naturally exist in many temporal data (Schmidhuber, 1991; Mozer, 1993; El Hihi and Bengio, 1995; Kleinberg, 2003; Koutn\u00edk et al., 2014; Chung et al., 2016).", "startOffset": 240, "endOffset": 357}, {"referenceID": 27, "context": "However, unlike the spatial data, learning both hierarchical and temporal representation has been among the long-standing challenges of RNNs in spite of the fact that hierarchical multiscale structures naturally exist in many temporal data (Schmidhuber, 1991; Mozer, 1993; El Hihi and Bengio, 1995; Kleinberg, 2003; Koutn\u00edk et al., 2014; Chung et al., 2016).", "startOffset": 240, "endOffset": 357}, {"referenceID": 28, "context": "However, unlike the spatial data, learning both hierarchical and temporal representation has been among the long-standing challenges of RNNs in spite of the fact that hierarchical multiscale structures naturally exist in many temporal data (Schmidhuber, 1991; Mozer, 1993; El Hihi and Bengio, 1995; Kleinberg, 2003; Koutn\u00edk et al., 2014; Chung et al., 2016).", "startOffset": 240, "endOffset": 357}, {"referenceID": 9, "context": "However, unlike the spatial data, learning both hierarchical and temporal representation has been among the long-standing challenges of RNNs in spite of the fact that hierarchical multiscale structures naturally exist in many temporal data (Schmidhuber, 1991; Mozer, 1993; El Hihi and Bengio, 1995; Kleinberg, 2003; Koutn\u00edk et al., 2014; Chung et al., 2016).", "startOffset": 240, "endOffset": 357}, {"referenceID": 49, "context": "A promising approach to model such hierarchical and temporal representation is the multiscale RNNs (Schmidhuber, 1992; El Hihi and Bengio, 1995; Koutn\u00edk et al., 2014).", "startOffset": 99, "endOffset": 166}, {"referenceID": 28, "context": "A promising approach to model such hierarchical and temporal representation is the multiscale RNNs (Schmidhuber, 1992; El Hihi and Bengio, 1995; Koutn\u00edk et al., 2014).", "startOffset": 99, "endOffset": 166}, {"referenceID": 28, "context": "The most popular approach is to set the timescales as hyperparameters (El Hihi and Bengio, 1995; Koutn\u00edk et al., 2014; Bahdanau et al., 2016) instead of treating them as dynamic variables that can be learned from the data (Schmidhuber, 1991, 1992; Chung et al.", "startOffset": 70, "endOffset": 141}, {"referenceID": 1, "context": "The most popular approach is to set the timescales as hyperparameters (El Hihi and Bengio, 1995; Koutn\u00edk et al., 2014; Bahdanau et al., 2016) instead of treating them as dynamic variables that can be learned from the data (Schmidhuber, 1991, 1992; Chung et al.", "startOffset": 70, "endOffset": 141}, {"referenceID": 51, "context": "While this is trivial if the hierarchical boundary structure is provided (Sordoni et al., 2015), it has been a challenge for an RNN to discover the latent hierarchical structure in temporal data without explicit boundary information.", "startOffset": 73, "endOffset": 95}, {"referenceID": 17, "context": "The UPDATE operation is similar to the usual update rule of the long short-term memory (LSTM) (Hochreiter and Schmidhuber, 1997), except that it is executed sparsely according to the detected boundaries.", "startOffset": 94, "endOffset": 128}, {"referenceID": 6, "context": "Unlike the leaky integration of the LSTM or gated recurrent unit (GRU) (Cho et al., 2014), the COPY retains the whole states without any loss of information.", "startOffset": 71, "endOffset": 89}, {"referenceID": 3, "context": "We find that the straight-through estimator (Bengio et al., 2013; Courbariaux et al., 2015) is efficient for training this model containing discrete variables.", "startOffset": 44, "endOffset": 91}, {"referenceID": 11, "context": "We find that the straight-through estimator (Bengio et al., 2013; Courbariaux et al., 2015) is efficient for training this model containing discrete variables.", "startOffset": 44, "endOffset": 91}, {"referenceID": 17, "context": "The LSTM (Hochreiter and Schmidhuber, 1997) is probably the most popular RNN that employs the multiscale update concept.", "startOffset": 9, "endOffset": 43}, {"referenceID": 13, "context": "That is, the hidden units of the LSTM or stacked LSTM (Fern\u00e1ndez et al., 2007) have different forget and update rates and can operate with different timescales, however unlike to our model, these timescales are not organized hierarchically.", "startOffset": 54, "endOffset": 78}, {"referenceID": 28, "context": "A more recent model, the clockwork RNN (CW-RNN) (Koutn\u00edk et al., 2014) extends the hierarchical RNN (El Hihi and Bengio, 1995) and tries to solve the issue of using soft timescales in the LSTM, by explicitly assigning hard timescales.", "startOffset": 48, "endOffset": 70}, {"referenceID": 7, "context": "More recently, in the Gated-Feedback RNNs (GF-RNN) (Chung et al., 2015a) and the Biscale RNNs (BiS-RNN) (Chung et al.", "startOffset": 51, "endOffset": 72}, {"referenceID": 9, "context": ", 2015a) and the Biscale RNNs (BiS-RNN) (Chung et al., 2016), the authors proposed to model layer-wise timescales more explicitly by having additional gating units to stacked RNN architecture.", "startOffset": 40, "endOffset": 60}, {"referenceID": 31, "context": "The COPY operation used in our model can be related to Zoneout (Krueger et al., 2016) which is a recurrent generalization of stochastic depth (Huang et al.", "startOffset": 63, "endOffset": 85}, {"referenceID": 19, "context": ", 2016) which is a recurrent generalization of stochastic depth (Huang et al., 2016).", "startOffset": 64, "endOffset": 84}, {"referenceID": 33, "context": "Two notable early attempts inspiring our model are Schmidhuber (1992) and El Hihi and Bengio (1995).", "startOffset": 51, "endOffset": 70}, {"referenceID": 1, "context": "Two notable early attempts inspiring our model are Schmidhuber (1992) and El Hihi and Bengio (1995). In these works, it is advocated to stack multiple layers of RNNs in a decreasing order of update frequency for computational and learning efficiency.", "startOffset": 86, "endOffset": 100}, {"referenceID": 1, "context": "Two notable early attempts inspiring our model are Schmidhuber (1992) and El Hihi and Bengio (1995). In these works, it is advocated to stack multiple layers of RNNs in a decreasing order of update frequency for computational and learning efficiency. In Schmidhuber (1992), the author shows a model that can self-organize a hierarchical multiscale structure.", "startOffset": 86, "endOffset": 273}, {"referenceID": 1, "context": "Two notable early attempts inspiring our model are Schmidhuber (1992) and El Hihi and Bengio (1995). In these works, it is advocated to stack multiple layers of RNNs in a decreasing order of update frequency for computational and learning efficiency. In Schmidhuber (1992), the author shows a model that can self-organize a hierarchical multiscale structure. Particularly in El Hihi and Bengio (1995), the advantages of incorporating a priori knowledge, \u201ctemporal dependencies are structured hierarchically\", into the RNN architecture is studied.", "startOffset": 86, "endOffset": 401}, {"referenceID": 1, "context": "Two notable early attempts inspiring our model are Schmidhuber (1992) and El Hihi and Bengio (1995). In these works, it is advocated to stack multiple layers of RNNs in a decreasing order of update frequency for computational and learning efficiency. In Schmidhuber (1992), the author shows a model that can self-organize a hierarchical multiscale structure. Particularly in El Hihi and Bengio (1995), the advantages of incorporating a priori knowledge, \u201ctemporal dependencies are structured hierarchically\", into the RNN architecture is studied. The authors propose an RNN architecture that updates each layer with a fixed but different rate, called a hierarchical RNN. The LSTM (Hochreiter and Schmidhuber, 1997) is probably the most popular RNN that employs the multiscale update concept. That is, the hidden units of the LSTM or stacked LSTM (Fern\u00e1ndez et al., 2007) have different forget and update rates and can operate with different timescales, however unlike to our model, these timescales are not organized hierarchically. Although the update of the LSTM by leaky integration helps to reduce the difficulty of learning long-term dependency by mitigating the vanishing gradient problem, the model still remains computationally expensive because it has to perform the update at every time step for each unit. Also, in practice, the long-term dependency captured by the LSTM units are still limited to a few hundred time steps due to the leaky integration by which the contents to memorize for a long-term is gradually diluted at every time step. However, our model is less prone to this problem because it learns a hierarchical structure such that, by design, high-level layers perform less frequent updates than low-level layers. We hypothesize that this property mitigates the vanishing gradient problem more efficiently while also being computationally more efficient. A more recent model, the clockwork RNN (CW-RNN) (Koutn\u00edk et al., 2014) extends the hierarchical RNN (El Hihi and Bengio, 1995) and tries to solve the issue of using soft timescales in the LSTM, by explicitly assigning hard timescales. In the CW-RNN, hidden units are partitioned into several modules, and different timescales are assigned to the modules such that a module i updates its hidden units at every 2(i\u22121)-th time step. The CW-RNN is computationally more efficient than the standard RNN including the LSTM since hidden units are updated only at the assigned clock rates. However, finding proper timescales in CW-RNN remains as a challenge, unlike our model, which learns the intrinsic timescales from the data. More recently, in the Gated-Feedback RNNs (GF-RNN) (Chung et al., 2015a) and the Biscale RNNs (BiS-RNN) (Chung et al., 2016), the authors proposed to model layer-wise timescales more explicitly by having additional gating units to stacked RNN architecture. While the GF-RNN proposed a general framework where information can flow across multiple RNN layers via a gating mechanism, the BiS-RNN architecture is applied to character-level machine translation where two different timescales (e.g., character-level and word-level) seem to clearly exist in the data. However, similar to the LSTM, these two approaches rely on the soft gating mechanism, making the model suffer from the curse of update at every time step. Other forms of Hierarchical RNN (HRNN) architectures have been used in the case where the hierarchical boundary structure is explicitly observed in the data and easy to obtain. In Ling et al. (2015), the HRNN architecture is used for neural machine translation by modelling the characters and words using the first and second RNN layers, respectively, after obtaining the word boundary via tokenization.", "startOffset": 86, "endOffset": 3516}, {"referenceID": 1, "context": "Two notable early attempts inspiring our model are Schmidhuber (1992) and El Hihi and Bengio (1995). In these works, it is advocated to stack multiple layers of RNNs in a decreasing order of update frequency for computational and learning efficiency. In Schmidhuber (1992), the author shows a model that can self-organize a hierarchical multiscale structure. Particularly in El Hihi and Bengio (1995), the advantages of incorporating a priori knowledge, \u201ctemporal dependencies are structured hierarchically\", into the RNN architecture is studied. The authors propose an RNN architecture that updates each layer with a fixed but different rate, called a hierarchical RNN. The LSTM (Hochreiter and Schmidhuber, 1997) is probably the most popular RNN that employs the multiscale update concept. That is, the hidden units of the LSTM or stacked LSTM (Fern\u00e1ndez et al., 2007) have different forget and update rates and can operate with different timescales, however unlike to our model, these timescales are not organized hierarchically. Although the update of the LSTM by leaky integration helps to reduce the difficulty of learning long-term dependency by mitigating the vanishing gradient problem, the model still remains computationally expensive because it has to perform the update at every time step for each unit. Also, in practice, the long-term dependency captured by the LSTM units are still limited to a few hundred time steps due to the leaky integration by which the contents to memorize for a long-term is gradually diluted at every time step. However, our model is less prone to this problem because it learns a hierarchical structure such that, by design, high-level layers perform less frequent updates than low-level layers. We hypothesize that this property mitigates the vanishing gradient problem more efficiently while also being computationally more efficient. A more recent model, the clockwork RNN (CW-RNN) (Koutn\u00edk et al., 2014) extends the hierarchical RNN (El Hihi and Bengio, 1995) and tries to solve the issue of using soft timescales in the LSTM, by explicitly assigning hard timescales. In the CW-RNN, hidden units are partitioned into several modules, and different timescales are assigned to the modules such that a module i updates its hidden units at every 2(i\u22121)-th time step. The CW-RNN is computationally more efficient than the standard RNN including the LSTM since hidden units are updated only at the assigned clock rates. However, finding proper timescales in CW-RNN remains as a challenge, unlike our model, which learns the intrinsic timescales from the data. More recently, in the Gated-Feedback RNNs (GF-RNN) (Chung et al., 2015a) and the Biscale RNNs (BiS-RNN) (Chung et al., 2016), the authors proposed to model layer-wise timescales more explicitly by having additional gating units to stacked RNN architecture. While the GF-RNN proposed a general framework where information can flow across multiple RNN layers via a gating mechanism, the BiS-RNN architecture is applied to character-level machine translation where two different timescales (e.g., character-level and word-level) seem to clearly exist in the data. However, similar to the LSTM, these two approaches rely on the soft gating mechanism, making the model suffer from the curse of update at every time step. Other forms of Hierarchical RNN (HRNN) architectures have been used in the case where the hierarchical boundary structure is explicitly observed in the data and easy to obtain. In Ling et al. (2015), the HRNN architecture is used for neural machine translation by modelling the characters and words using the first and second RNN layers, respectively, after obtaining the word boundary via tokenization. A similar HRNN architecture is also adopted in Sordoni et al. (2015) to model dialogue utterances.", "startOffset": 86, "endOffset": 3790}, {"referenceID": 1, "context": "Two notable early attempts inspiring our model are Schmidhuber (1992) and El Hihi and Bengio (1995). In these works, it is advocated to stack multiple layers of RNNs in a decreasing order of update frequency for computational and learning efficiency. In Schmidhuber (1992), the author shows a model that can self-organize a hierarchical multiscale structure. Particularly in El Hihi and Bengio (1995), the advantages of incorporating a priori knowledge, \u201ctemporal dependencies are structured hierarchically\", into the RNN architecture is studied. The authors propose an RNN architecture that updates each layer with a fixed but different rate, called a hierarchical RNN. The LSTM (Hochreiter and Schmidhuber, 1997) is probably the most popular RNN that employs the multiscale update concept. That is, the hidden units of the LSTM or stacked LSTM (Fern\u00e1ndez et al., 2007) have different forget and update rates and can operate with different timescales, however unlike to our model, these timescales are not organized hierarchically. Although the update of the LSTM by leaky integration helps to reduce the difficulty of learning long-term dependency by mitigating the vanishing gradient problem, the model still remains computationally expensive because it has to perform the update at every time step for each unit. Also, in practice, the long-term dependency captured by the LSTM units are still limited to a few hundred time steps due to the leaky integration by which the contents to memorize for a long-term is gradually diluted at every time step. However, our model is less prone to this problem because it learns a hierarchical structure such that, by design, high-level layers perform less frequent updates than low-level layers. We hypothesize that this property mitigates the vanishing gradient problem more efficiently while also being computationally more efficient. A more recent model, the clockwork RNN (CW-RNN) (Koutn\u00edk et al., 2014) extends the hierarchical RNN (El Hihi and Bengio, 1995) and tries to solve the issue of using soft timescales in the LSTM, by explicitly assigning hard timescales. In the CW-RNN, hidden units are partitioned into several modules, and different timescales are assigned to the modules such that a module i updates its hidden units at every 2(i\u22121)-th time step. The CW-RNN is computationally more efficient than the standard RNN including the LSTM since hidden units are updated only at the assigned clock rates. However, finding proper timescales in CW-RNN remains as a challenge, unlike our model, which learns the intrinsic timescales from the data. More recently, in the Gated-Feedback RNNs (GF-RNN) (Chung et al., 2015a) and the Biscale RNNs (BiS-RNN) (Chung et al., 2016), the authors proposed to model layer-wise timescales more explicitly by having additional gating units to stacked RNN architecture. While the GF-RNN proposed a general framework where information can flow across multiple RNN layers via a gating mechanism, the BiS-RNN architecture is applied to character-level machine translation where two different timescales (e.g., character-level and word-level) seem to clearly exist in the data. However, similar to the LSTM, these two approaches rely on the soft gating mechanism, making the model suffer from the curse of update at every time step. Other forms of Hierarchical RNN (HRNN) architectures have been used in the case where the hierarchical boundary structure is explicitly observed in the data and easy to obtain. In Ling et al. (2015), the HRNN architecture is used for neural machine translation by modelling the characters and words using the first and second RNN layers, respectively, after obtaining the word boundary via tokenization. A similar HRNN architecture is also adopted in Sordoni et al. (2015) to model dialogue utterances. However, in many cases, hierarchical boundary information is not explicitly observed or expensive to obtain. Also, it is unclear how to deploy more layers than the number of boundary levels that is explicitly observed in the data. While the above models focus on the online prediction problem, where a prediction needs to be made by using only the past data, in some cases, predictions are made after observing the whole sequence. In this setting, the input sequence can be regarded as 1-D spatial data, using convolutional neural networks with 1-D kernels as proposed in Kim (2014) and Kim et al.", "startOffset": 86, "endOffset": 4403}, {"referenceID": 1, "context": "Two notable early attempts inspiring our model are Schmidhuber (1992) and El Hihi and Bengio (1995). In these works, it is advocated to stack multiple layers of RNNs in a decreasing order of update frequency for computational and learning efficiency. In Schmidhuber (1992), the author shows a model that can self-organize a hierarchical multiscale structure. Particularly in El Hihi and Bengio (1995), the advantages of incorporating a priori knowledge, \u201ctemporal dependencies are structured hierarchically\", into the RNN architecture is studied. The authors propose an RNN architecture that updates each layer with a fixed but different rate, called a hierarchical RNN. The LSTM (Hochreiter and Schmidhuber, 1997) is probably the most popular RNN that employs the multiscale update concept. That is, the hidden units of the LSTM or stacked LSTM (Fern\u00e1ndez et al., 2007) have different forget and update rates and can operate with different timescales, however unlike to our model, these timescales are not organized hierarchically. Although the update of the LSTM by leaky integration helps to reduce the difficulty of learning long-term dependency by mitigating the vanishing gradient problem, the model still remains computationally expensive because it has to perform the update at every time step for each unit. Also, in practice, the long-term dependency captured by the LSTM units are still limited to a few hundred time steps due to the leaky integration by which the contents to memorize for a long-term is gradually diluted at every time step. However, our model is less prone to this problem because it learns a hierarchical structure such that, by design, high-level layers perform less frequent updates than low-level layers. We hypothesize that this property mitigates the vanishing gradient problem more efficiently while also being computationally more efficient. A more recent model, the clockwork RNN (CW-RNN) (Koutn\u00edk et al., 2014) extends the hierarchical RNN (El Hihi and Bengio, 1995) and tries to solve the issue of using soft timescales in the LSTM, by explicitly assigning hard timescales. In the CW-RNN, hidden units are partitioned into several modules, and different timescales are assigned to the modules such that a module i updates its hidden units at every 2(i\u22121)-th time step. The CW-RNN is computationally more efficient than the standard RNN including the LSTM since hidden units are updated only at the assigned clock rates. However, finding proper timescales in CW-RNN remains as a challenge, unlike our model, which learns the intrinsic timescales from the data. More recently, in the Gated-Feedback RNNs (GF-RNN) (Chung et al., 2015a) and the Biscale RNNs (BiS-RNN) (Chung et al., 2016), the authors proposed to model layer-wise timescales more explicitly by having additional gating units to stacked RNN architecture. While the GF-RNN proposed a general framework where information can flow across multiple RNN layers via a gating mechanism, the BiS-RNN architecture is applied to character-level machine translation where two different timescales (e.g., character-level and word-level) seem to clearly exist in the data. However, similar to the LSTM, these two approaches rely on the soft gating mechanism, making the model suffer from the curse of update at every time step. Other forms of Hierarchical RNN (HRNN) architectures have been used in the case where the hierarchical boundary structure is explicitly observed in the data and easy to obtain. In Ling et al. (2015), the HRNN architecture is used for neural machine translation by modelling the characters and words using the first and second RNN layers, respectively, after obtaining the word boundary via tokenization. A similar HRNN architecture is also adopted in Sordoni et al. (2015) to model dialogue utterances. However, in many cases, hierarchical boundary information is not explicitly observed or expensive to obtain. Also, it is unclear how to deploy more layers than the number of boundary levels that is explicitly observed in the data. While the above models focus on the online prediction problem, where a prediction needs to be made by using only the past data, in some cases, predictions are made after observing the whole sequence. In this setting, the input sequence can be regarded as 1-D spatial data, using convolutional neural networks with 1-D kernels as proposed in Kim (2014) and Kim et al. (2015) for language modelling and sentence classification.", "startOffset": 86, "endOffset": 4425}, {"referenceID": 1, "context": "Two notable early attempts inspiring our model are Schmidhuber (1992) and El Hihi and Bengio (1995). In these works, it is advocated to stack multiple layers of RNNs in a decreasing order of update frequency for computational and learning efficiency. In Schmidhuber (1992), the author shows a model that can self-organize a hierarchical multiscale structure. Particularly in El Hihi and Bengio (1995), the advantages of incorporating a priori knowledge, \u201ctemporal dependencies are structured hierarchically\", into the RNN architecture is studied. The authors propose an RNN architecture that updates each layer with a fixed but different rate, called a hierarchical RNN. The LSTM (Hochreiter and Schmidhuber, 1997) is probably the most popular RNN that employs the multiscale update concept. That is, the hidden units of the LSTM or stacked LSTM (Fern\u00e1ndez et al., 2007) have different forget and update rates and can operate with different timescales, however unlike to our model, these timescales are not organized hierarchically. Although the update of the LSTM by leaky integration helps to reduce the difficulty of learning long-term dependency by mitigating the vanishing gradient problem, the model still remains computationally expensive because it has to perform the update at every time step for each unit. Also, in practice, the long-term dependency captured by the LSTM units are still limited to a few hundred time steps due to the leaky integration by which the contents to memorize for a long-term is gradually diluted at every time step. However, our model is less prone to this problem because it learns a hierarchical structure such that, by design, high-level layers perform less frequent updates than low-level layers. We hypothesize that this property mitigates the vanishing gradient problem more efficiently while also being computationally more efficient. A more recent model, the clockwork RNN (CW-RNN) (Koutn\u00edk et al., 2014) extends the hierarchical RNN (El Hihi and Bengio, 1995) and tries to solve the issue of using soft timescales in the LSTM, by explicitly assigning hard timescales. In the CW-RNN, hidden units are partitioned into several modules, and different timescales are assigned to the modules such that a module i updates its hidden units at every 2(i\u22121)-th time step. The CW-RNN is computationally more efficient than the standard RNN including the LSTM since hidden units are updated only at the assigned clock rates. However, finding proper timescales in CW-RNN remains as a challenge, unlike our model, which learns the intrinsic timescales from the data. More recently, in the Gated-Feedback RNNs (GF-RNN) (Chung et al., 2015a) and the Biscale RNNs (BiS-RNN) (Chung et al., 2016), the authors proposed to model layer-wise timescales more explicitly by having additional gating units to stacked RNN architecture. While the GF-RNN proposed a general framework where information can flow across multiple RNN layers via a gating mechanism, the BiS-RNN architecture is applied to character-level machine translation where two different timescales (e.g., character-level and word-level) seem to clearly exist in the data. However, similar to the LSTM, these two approaches rely on the soft gating mechanism, making the model suffer from the curse of update at every time step. Other forms of Hierarchical RNN (HRNN) architectures have been used in the case where the hierarchical boundary structure is explicitly observed in the data and easy to obtain. In Ling et al. (2015), the HRNN architecture is used for neural machine translation by modelling the characters and words using the first and second RNN layers, respectively, after obtaining the word boundary via tokenization. A similar HRNN architecture is also adopted in Sordoni et al. (2015) to model dialogue utterances. However, in many cases, hierarchical boundary information is not explicitly observed or expensive to obtain. Also, it is unclear how to deploy more layers than the number of boundary levels that is explicitly observed in the data. While the above models focus on the online prediction problem, where a prediction needs to be made by using only the past data, in some cases, predictions are made after observing the whole sequence. In this setting, the input sequence can be regarded as 1-D spatial data, using convolutional neural networks with 1-D kernels as proposed in Kim (2014) and Kim et al. (2015) for language modelling and sentence classification. Also, in Chan et al. (2016) and Bahdanau et al.", "startOffset": 86, "endOffset": 4505}, {"referenceID": 1, "context": "(2016) and Bahdanau et al. (2016), the authors proposed to obtain high-level representation of the sequences of reduced length by repeatedly merging or pooling the lower level representation of the sequences.", "startOffset": 11, "endOffset": 34}, {"referenceID": 52, "context": "While the focus of Zoneout is to propose a regularization technique similar to dropout (Srivastava et al., 2014) (where the regularization strength is controlled by a hyperparameter), our model learns (a) to dynamically determine when to copy from the context inputs and (b) to discover the latent hierarchical multiscale structure and representation.", "startOffset": 87, "endOffset": 112}, {"referenceID": 51, "context": ", spaces to separate words and periods to separate sentences (Sordoni et al., 2015; Ling et al., 2015).", "startOffset": 61, "endOffset": 102}, {"referenceID": 33, "context": ", spaces to separate words and periods to separate sentences (Sordoni et al., 2015; Ling et al., 2015).", "startOffset": 61, "endOffset": 102}, {"referenceID": 28, "context": "In Section 2, we discussed the limitations of the existing RNN models under this setting, which either have to update all units at every time step or use fixed update frequencies (El Hihi and Bengio, 1995; Koutn\u00edk et al., 2014; Bahdanau et al., 2016).", "startOffset": 179, "endOffset": 250}, {"referenceID": 1, "context": "In Section 2, we discussed the limitations of the existing RNN models under this setting, which either have to update all units at every time step or use fixed update frequencies (El Hihi and Bengio, 1995; Koutn\u00edk et al., 2014; Bahdanau et al., 2016).", "startOffset": 179, "endOffset": 250}, {"referenceID": 57, "context": "Among a few methods for training a neural network with discrete variables such as the REINFORCE (Williams, 1992; Mnih and Gregor, 2014) and the straight-through estimator (Bengio et al.", "startOffset": 96, "endOffset": 135}, {"referenceID": 38, "context": "Among a few methods for training a neural network with discrete variables such as the REINFORCE (Williams, 1992; Mnih and Gregor, 2014) and the straight-through estimator (Bengio et al.", "startOffset": 96, "endOffset": 135}, {"referenceID": 3, "context": "Among a few methods for training a neural network with discrete variables such as the REINFORCE (Williams, 1992; Mnih and Gregor, 2014) and the straight-through estimator (Bengio et al., 2013), we use the straight-through estimator to train our model.", "startOffset": 171, "endOffset": 192}, {"referenceID": 2, "context": "Among a few methods for training a neural network with discrete variables such as the REINFORCE (Williams, 1992; Mnih and Gregor, 2014) and the straight-through estimator (Bengio et al., 2013), we use the straight-through estimator to train our model. The straight-through estimator is a biased estimator because the non-differentiable function used in the forward pass (the step function in our case) is replaced by a differentiable function during the backward pass (the hard sigmoid function in our case). The straight-through estimator, however, is much simpler and often works more efficiently in practice than other unbiased but high-variance estimators such as the REINFORCE. The straight-through estimator has also been used in Courbariaux et al. (2015) and Mnih et al.", "startOffset": 172, "endOffset": 762}, {"referenceID": 2, "context": "Among a few methods for training a neural network with discrete variables such as the REINFORCE (Williams, 1992; Mnih and Gregor, 2014) and the straight-through estimator (Bengio et al., 2013), we use the straight-through estimator to train our model. The straight-through estimator is a biased estimator because the non-differentiable function used in the forward pass (the step function in our case) is replaced by a differentiable function during the backward pass (the hard sigmoid function in our case). The straight-through estimator, however, is much simpler and often works more efficiently in practice than other unbiased but high-variance estimators such as the REINFORCE. The straight-through estimator has also been used in Courbariaux et al. (2015) and Mnih et al. (2016). The Slope Annealing Trick.", "startOffset": 172, "endOffset": 785}, {"referenceID": 30, "context": "Penn Treebank Model BPC Norm-stabilized RNN (Krueger and Memisevic, 2015) 1.", "startOffset": 44, "endOffset": 73}, {"referenceID": 28, "context": "48 Clockwork RNN (Koutn\u00edk et al., 2014) 1.", "startOffset": 17, "endOffset": 39}, {"referenceID": 37, "context": "46 HF-MRNN (Mikolov et al., 2012) 1.", "startOffset": 11, "endOffset": 33}, {"referenceID": 58, "context": "41 MI-RNN (Wu et al., 2016) 1.", "startOffset": 10, "endOffset": 27}, {"referenceID": 37, "context": "39 ME n-gram (Mikolov et al., 2012) 1.", "startOffset": 13, "endOffset": 35}, {"referenceID": 10, "context": "37 Batch-normalized LSTM (Cooijmans et al., 2016) 1.", "startOffset": 25, "endOffset": 49}, {"referenceID": 31, "context": "32 Zoneout RNN (Krueger et al., 2016) 1.", "startOffset": 15, "endOffset": 37}, {"referenceID": 60, "context": "Text8 Model BPC td-LSTM (Zhang et al., 2016) 1.", "startOffset": 24, "endOffset": 44}, {"referenceID": 37, "context": "63 HF-MRNN (Mikolov et al., 2012) 1.", "startOffset": 11, "endOffset": 33}, {"referenceID": 58, "context": "54 MI-RNN (Wu et al., 2016) 1.", "startOffset": 10, "endOffset": 27}, {"referenceID": 43, "context": "52 Skipping-RNN (Pachitariu and Sahani, 2013) 1.", "startOffset": 16, "endOffset": 45}, {"referenceID": 58, "context": "48 MI-LSTM (Wu et al., 2016) 1.", "startOffset": 11, "endOffset": 28}, {"referenceID": 10, "context": "44 Batch-normalized LSTM (Cooijmans et al., 2016) 1.", "startOffset": 25, "endOffset": 49}, {"referenceID": 46, "context": "Hutter Prize Wikipedia Model BPC SF-LSTM (Rocki, 2016b)\u2217 1.", "startOffset": 41, "endOffset": 55}, {"referenceID": 14, "context": "39 Stacked LSTM (Graves, 2013) 1.", "startOffset": 16, "endOffset": 30}, {"referenceID": 53, "context": "67 MRNN (Sutskever et al., 2011) 1.", "startOffset": 8, "endOffset": 32}, {"referenceID": 7, "context": "60 GF-LSTM (Chung et al., 2015a) 1.", "startOffset": 11, "endOffset": 32}, {"referenceID": 22, "context": "58 Grid-LSTM (Kalchbrenner et al., 2015) 1.", "startOffset": 13, "endOffset": 40}, {"referenceID": 0, "context": "47 Layer-normalized LSTM (Ba et al., 2016)\u2020 1.", "startOffset": 25, "endOffset": 42}, {"referenceID": 58, "context": "46 MI-LSTM (Wu et al., 2016) 1.", "startOffset": 11, "endOffset": 28}, {"referenceID": 61, "context": "44 Recurrent Highway Networks (Zilly et al., 2016) 1.", "startOffset": 30, "endOffset": 50}, {"referenceID": 45, "context": "42 Recurrent Memory Array Structures (Rocki, 2016a) 1.", "startOffset": 37, "endOffset": 51}, {"referenceID": 0, "context": "(\u2020) This model is implemented by the authors as the standard LSTM architecture using layer normalization (Ba et al., 2016).", "startOffset": 105, "endOffset": 122}, {"referenceID": 42, "context": "where L = 3 and ReLU(x) = max(0, x) (Nair and Hinton, 2010).", "startOffset": 36, "endOffset": 59}, {"referenceID": 36, "context": "Penn Treebank We process the Penn Treebank dataset (Marcus et al., 1993) by following the procedure introduced in Mikolov et al.", "startOffset": 51, "endOffset": 72}, {"referenceID": 25, "context": "We train the model using Adam (Kingma and Ba, 2014) with an initial learning rate of 0.", "startOffset": 30, "endOffset": 51}, {"referenceID": 44, "context": "The norm of the gradient is clipped with a threshold of 1 (Pascanu et al., 2012).", "startOffset": 58, "endOffset": 80}, {"referenceID": 21, "context": "We apply batch normalization (Ioffe and Szegedy, 2015; Cooijmans et al., 2016).", "startOffset": 29, "endOffset": 78}, {"referenceID": 10, "context": "We apply batch normalization (Ioffe and Szegedy, 2015; Cooijmans et al., 2016).", "startOffset": 29, "endOffset": 78}, {"referenceID": 32, "context": "Penn Treebank We process the Penn Treebank dataset (Marcus et al., 1993) by following the procedure introduced in Mikolov et al. (2012). Each update is done by using a mini-batch of 64 examples of length 100 to prevent the memory overflow problem when unfolding the RNN in time for backpropagation.", "startOffset": 52, "endOffset": 136}, {"referenceID": 35, "context": "Text8 The Text8 dataset (Mahoney, 2009) consists of 100M characters extracted from the Wikipedia corpus.", "startOffset": 24, "endOffset": 39}, {"referenceID": 36, "context": "to compare with other previous works, we follow the data splits used in Mikolov et al. (2012); Cooijmans et al.", "startOffset": 72, "endOffset": 94}, {"referenceID": 10, "context": "(2012); Cooijmans et al. (2016); Wu et al.", "startOffset": 8, "endOffset": 32}, {"referenceID": 10, "context": "(2012); Cooijmans et al. (2016); Wu et al. (2016). We use 1024 units for each HM-LSTM layer and 2048 units for the embedding layer of the output module.", "startOffset": 8, "endOffset": 50}, {"referenceID": 20, "context": "Hutter Prize Wikipedia The third dataset for language modelling is the Hutter Prize Wikipedia (also known as enwik8) dataset (Hutter, 2012).", "startOffset": 125, "endOffset": 139}, {"referenceID": 0, "context": "We use the same model size as in the Text8 experiments but apply layer normalization (Ba et al., 2016) to the model.", "startOffset": 85, "endOffset": 102}, {"referenceID": 13, "context": "(2012) and Graves (2013) where the first 90M characters are used to train the model, the next 5M characters for validation, and the remainders for the test set.", "startOffset": 11, "endOffset": 25}, {"referenceID": 34, "context": "We extend the evaluation of the HM-LSTM to a real-valued sequence modelling task using IAMOnDB (Liwicki and Bunke, 2005) dataset.", "startOffset": 95, "endOffset": 120}, {"referenceID": 14, "context": "Note that this process makes our numbers in Table 4 to become not directly comparable to the previous works (Graves, 2013; Chung et al., 2015b).", "startOffset": 108, "endOffset": 143}, {"referenceID": 8, "context": "Note that this process makes our numbers in Table 4 to become not directly comparable to the previous works (Graves, 2013; Chung et al., 2015b).", "startOffset": 108, "endOffset": 143}, {"referenceID": 4, "context": "We use the mixture density network (Bishop, 1994) as the output layer following Graves (2013), and use 400 units for each HM-LSTM layer and 800 units for the embedding layer of the output module.", "startOffset": 35, "endOffset": 49}, {"referenceID": 4, "context": "We use the mixture density network (Bishop, 1994) as the output layer following Graves (2013), and use 400 units for each HM-LSTM layer and 800 units for the embedding layer of the output module.", "startOffset": 36, "endOffset": 94}, {"referenceID": 55, "context": "The authors thank the developers of Theano (Team et al., 2016).", "startOffset": 43, "endOffset": 62}], "year": 2016, "abstractText": "Learning both hierarchical and temporal representation has been among the longstanding challenges of recurrent neural networks. Multiscale recurrent neural networks have been considered as a promising approach to resolve this issue, yet there has been a lack of empirical evidence showing that this type of models can actually capture the temporal dependencies by discovering the latent hierarchical structure of the sequence. In this paper, we propose a novel multiscale approach, called the hierarchical multiscale recurrent neural network, which can capture the latent hierarchical structure in the sequence by encoding the temporal dependencies with different timescales using a novel update mechanism. We show some evidence that our proposed multiscale architecture can discover underlying hierarchical structure in the sequences without using explicit boundary information. We evaluate our proposed model on character-level language modelling and handwriting sequence generation.", "creator": "LaTeX with hyperref package"}}}