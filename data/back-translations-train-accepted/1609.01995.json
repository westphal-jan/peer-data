{"id": "1609.01995", "review": {"conference": "ICML", "VERSION": "v1", "DATE_OF_SUBMISSION": "7-Sep-2016", "title": "Unifying Task Specification in Reinforcement Learning", "abstract": "Reinforcement learning tasks are typically specified as Markov decision processes. This formalism has been highly successful, though specifications often couple the dynamics of the environment and the learning objective. This lack of modularity can complicate generalization of the task specification, as well as obfuscate connections between different task settings, such as episodic and continuing. In this work, we introduce the RL task formalism, that provides a unification through simple constructs including a generalization to transition-based discounting. Through a series of examples, we demonstrate the generality and utility of this formalism. Finally, we extend standard learning constructs, including Bellman operators, and extend some seminal theoretical results, including approximation errors bounds. Overall, we provide a well-understood and sound formalism on which to build theoretical results and simplify algorithm use and development.", "histories": [["v1", "Wed, 7 Sep 2016 14:27:56 GMT  (71kb,D)", "http://arxiv.org/abs/1609.01995v1", null], ["v2", "Wed, 1 Mar 2017 02:36:21 GMT  (80kb,D)", "http://arxiv.org/abs/1609.01995v2", "Modification of contraction result for weight dpi"], ["v3", "Fri, 7 Jul 2017 09:55:23 GMT  (81kb,D)", "http://arxiv.org/abs/1609.01995v3", "Modification of contraction result for weight dpi"]], "reviews": [], "SUBJECTS": "cs.AI", "authors": ["martha white"], "accepted": true, "id": "1609.01995"}, "pdf": {"name": "1609.01995.pdf", "metadata": {"source": "CRF", "title": "Unifying task specification in reinforcement learning", "authors": ["Martha White"], "emails": ["martha@indiana.edu"], "sections": [{"heading": "1 Introduction", "text": "This interaction is typically specified by a Markov Decision Process (MDP), which includes a transition model, a reward model, and potentially discount parameters \u03b3, which specify a discount on the sum of future values in the return. Domains are typically divided into two cases: episodic problems (finite horizon) and ongoing problems (finite horizon). In episodic problems, the actor reaches a final state and is returned to a starting state. In ongoing problems, the actor's interaction is continuous, with a discount to ensure a finite total reward (e.g. a constant total reward < 1). This formalism has a long and successful tradition, but is limited in the problems that can be specified. Progressively, there are additions to specify a broader range of goals, including options. [Sutton et al, 1999], government computing."}, {"heading": "2 Generalized problem formulation", "text": "We assume that the agent interacts with an environment formalized by a Markov decision-making process (MDP =): (S, A, Pr), in which S is the group of states, n = | S |; A is the group of measures; and Pr: S \u00b7 A \u00b7 S \u2192 [0, 1] is the transition probability function, in which Pr (s, a, s \u00b2) is the probability of transition from state to state s \u00b2 when taking action. A reinforcement learning task (RL task) is specified at the top of this transition dynamic, as the tuple (P, r, g, i), where P is a series of strategies: S \u00b7 A \u2192 [0, 1]; the reward function r: S \u00b7 A \u00b7 S \u2192 R specifies the reward received by (s \u00b2) is defined as the tuple (P, g, i), with the group P, g, strategies i being the group of the state sp."}, {"heading": "2.1 Unifying specification of episodic and continuing problems", "text": "The RL task specification allows for episodic and persistent problems that can easily be encrypted with just one change in the transition process = 1 x discounting. Prior approaches, including the absorbent state1We describe another probabilistic generalization in Appendix A; much of the treatment remains the same, but the notation becomes cumbersome and the utility obfuscated.formulation [Sutton and Barto, 1998] and state discounting [Sutton, 1995, Maei and Sutton, 2010, Sutton et al.] [van Hasselt, 2011, Section 2.1.1], require special cases or changes to the set of states and underlying MDP, coupling task specification and the dynamics of the environment. We demonstrate how transition control can be seamlessly specified episodic or continued tasks in an MDP. Consider the chain world with three states s1, s2 and s3 in Figure 1. The starting state is 2 and the two measures and the two poppies are."}, {"heading": "2.2 Options as RL tasks", "text": "The options framework [Sutton et al., 1999] generally covers a wide range of settings, with discussions about macro actions, option models, option interruption, and intra-option value learning. These concepts at the time merited their own language, but current generalizations can now be more conveniently occupied as RL subtasks. Proposal 1. An option defined as the tupel [Sutton et al., 1999, section 2] (Sutton, \u03b2, I) with policies: S \u00d7 A \u2192 [0, 1], termination function \u03b2: S \u2192 [0, 1] and an initiation group from which the option can be executed can be executed as an RL task. Proof evidence: This evidence is mainly defining. The discount function can be used (s, a, s \u2032) = 1 \u2212 \u03b2 (s \u2032) for these tasks."}, {"heading": "2.3 General value functions and predictive questions", "text": "In a similar spirit of abstraction as options, general value functions have been introduced for individual predictive or goal-oriented questions about the world [Sutton et al., 2011]. The idea is to encode predictive knowledge in the form of value function predictions: with a collection or horde of predictive demons, this represents knowledge [Sutton et al., 2011, Modayil et al., 2014, White, 2015]. Work on Horde [Sutton et al., 2011] and subsequent [Modayil et al., 2014] provides numerous examples of the usefulness of question types that can be specified by general value functions, and thus by RL tasks, because general value functions can naturally be specified as RL tasks. Generalizing to RL tasks offers additional benefits for predictive knowledge. Separation into underlying MDP dynamics and task specification is particularly useful in non-political learning, with horde formalism, where many demons (value functions) are learned outside politics."}, {"heading": "3 Demonstration in the taxi domain", "text": "In fact, the agent's goal is to move a passenger from a source platform to a destination platform, as illustrated in Figure 2. The agent receives a negative reward of -1 at each step, except for successful pickups and drop-offs, which give a reward of -0.1 and -0.2. We change the domain to include the orientation of the taxi, with additional costs for continuing in the current orientation. This encodes that turning right, left or backward is more expensive than continuing forward, with additional negative rewards of -0.2 and -0.2. These additional costs are multiplied by a factor of 2 if there is a passenger in the vehicle."}, {"heading": "4 Objectives and algorithms", "text": "With an intuition for the expediency of specifying problems as RL tasks, we now turn to generalizing some central algorithmic concepts to enable learning for RL tasks. First, we generalize the definition of the Bellman operator for the value function. For a policy \u03c0: S \u00b7 A \u2192 [0, 1], we define P\u03c0, P\u03c0, \u03b3 \u00b2 s \u00b2 s \u00b7 n and r\u03c0, indexed by states s, s \u00b2 s, s \u00b2 s \u00b2 s, s \u00b2 (s \u00b2 s, s \u00b2): = a) Pr (s, a, s \u00b2 ton (s, s \u00b2 s \u00b2 s), Pp \u00b2 s \u00b2 s, s \u00b2 s \u00b2 s \u00b2 s \u00b2 s, s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s (s), (s \u00b2 s): (s \u00b2 s (s), \"s (s), s (s), s (s,\" s, s (s, s, s, s, s, s, s, s, s, \"s, s, s, s, s, s, s, s, s (s, s, s, s, s, s, s, s, s, s, s, s, s, s, s, s, s, s, s, s, s, s,\" (s, s, s, s, s, s, s, s, s, s, s, s, s, s, s, s, s, \")."}, {"heading": "5 Analysis of contraction properties and convergence for RL tasks", "text": "In this section, we present a general approach to incorporating transition-based discounts into approximate limits. Most of the earlier limits assumed a constant discount discount. For example, the ETD was introduced at state-determined discounts; however, Hallak et al. [2015] analyzed the approximation error limits of the ETD using a constant discount discount rate calculation \u03b3c. By applying matrix norms to P\u03c0 instead, we generalize earlier approximation limits to both the episodic and the ongoing case."}, {"heading": "5.1 List of Assumptions", "text": "Define the set of bounded vectors for the general space of the value functions V = > V = > Rn: + V = + Dp < \u00b2. We assume everywhere that we have a subspace Fv \u00b2 V of possible solutions, as created by linear function approximation, e.g. Fv = {Xw | w \u00b2 Rd, + 2 < \u00b2}.A1. Action space A and state space S are finite. A2. For given policies \u00b5, \u03c0: S \u00b7 A \u2192 [0, 1] there exist unique invariant distributions dp and dp = dp \u00b2 and dp \u00b2 s = dp \u00b2. This assumption is typically fulfilled by assuming an ergodic Markov chain for politics. A3. There are transitions s, a, s \u00b2 so that a negative distribution dp (s \u00b2, s \u00b2) the matrisp = dp \u00b2 s \u00b2 s \u00b2 and p \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 prognoses."}, {"heading": "5.2 Approximation bound", "text": "First, we prove that the generalized Bellman operator in Eq.1 has a contraction and bound bias = > Biased bias due to the fact that the limit of [Tsitsiklis and Van Roy, 1997] for constant discount and constant trace parameters to which general transition parameters refer; the normalized difference to the true value function could be defined by multiple weighting; a known result is that the Bellman operator represents a contraction for constant defaults and constant defaults [Tsitsiklis and Van Roy, 1997]; more recently, this difference was generalized for a variant of ETD to M, still with constant parameters [Hallak et al., 2015]. We extend this result to constant defaults and presets and for both drops and the transition-based emphasis of the matrix M.Lemma 1. For definition D = pre-empt or D = irical D = D."}, {"heading": "5.3 Properties of learning algorithms", "text": "To be a practical formalism, a method of learning value functions and guidelines for RL tasks is needed. To evaluate policy, it has been shown that ETD and ELSTD, the least square version of ETD that uses the A and b values defined above with D = M. An important part of this evidence is convergence based on the expectation that A is clearly positive. In particular, convergence is extended to ELSTDQ in anticipation of suitable increments (see [Yu, 2015]). If A is clearly positive, the iterative update is convergent wt + 1 = wt + t (b \u2212 Awt)."}, {"heading": "6 Discussion and conclusion", "text": "For this reason, we would like to highlight and summarize the technical contributions, which include 1) the introduction of RL task formalism and transitional discounts; 2) general limits of approximation applicable to both episodic and ongoing tasks; 3) an explicit characterization of the relationship between government and transition-based discounting. This work provides a relatively complete characterization of RL task formalism as a basis for application in practice and theory through intuition based on simple examples and simple but fundamental theoretical extensions."}, {"heading": "A More general formulation with probabilistic discounts", "text": "When introducing transition discounting, we could have assumed instead that we had a more general probability model: Pr (r, \u03b3 | s, a, s \u2032). Now, both reward and discounting are not only functions of states and action, but also stochastical. This generalization does not change the treatment in this paper much. This is because when we use the expectations for the value function, the Bellman operator and the A matrix, we are back to \u03b3 (s, a, s \u2032). To see why, v\u03c0 (s) = \u2211 a, s \u2032 \u03c0 (s, s \u2032) E [r, a, s \u2032 p \u00b2 s, a, s \u00b2 s \u00b2 s \u00b2 s, s \u00b2 s \u00b2, s \u00b2, s \u00b2 s \u00b2 (s, s \u00b2, p \u00b2), p \u00b2 (p \u00b2), p \u00b2 (s \u00b2)."}, {"heading": "B Relationship between state-based and transition-based discounting", "text": "The MDPs are equivalent in the sense that learned policies and value functions learned in both MDP's would have equal values if evaluated on the states in the original transition-based MDP's. This equality ignores the practicability of learning in the larger induced MDP's, and at the end of this section we discuss the benefits of more compact transition-based MDP.B.1 equivalence results by introducing hypothetical states for each transition. The key then is to prove that the stationary distribution for the state-based MDP's, with additional hypothetical states, even with functional approximation, is achieved. For each triplet's, a new hypothetical state fsas' is added, with a set of F's-based additional states."}, {"heading": "C Discounting and average reward for control", "text": "One of the main reasons for this view is that the discounting is useful for asking predictive questions, but for control purposes, the end goal is the average reward. One of the main reasons for this view is that it has already been shown that optimizing the expected return at a constant discount equals optimizing the average reward. This can easily be seen by extending the expected return weighting according to the stationary distribution for a policy, since constant discount ratio \u03b3c < 1, d\u03c0v\u03c0 = d\u03c0 (r\u03c0 + P\u03c0) = d\u03c0r\u03c0 + \u03b3cd\u03c0r\u03c0 + \u03b3cd\u03c0v\u03c0 = \u21d2 d\u03c0v\u03c0 = 11 \u2212 \u03b3c d\u03c0r\u03c0. (6) Therefore, the constant \u043ac < 1 simply scales the average reward target, so that the optimization provides either the same policy. However, this argument does not extend to the transition-based discounting because it (s \u2032 s, an optimal way that can not significantly alter the weighting of the return)."}, {"heading": "D Algorithms", "text": "We show how to write generalized pseudo-codes for two algorithms: true-online TD (\u03bb) and ELSTDQ (\u03bb). We choose these two algorithms because they generally show how to extend to transition-based solutions and previously had a few unclear points in their implementation. For TO-TD, the pseudo-code for episodic tasks was given by Seijen and Sutton [2014], rather than more general, and it was handled carefully at the beginning of episodes, which is not necessary. LSTDQ was typically written for a set of data only, without sampling importance; we provide an ELSTDQ variant with importance here, where LSTDQ is a special case where M = 1.There are a few more implementation details that deserve clarification. We use the notation for the first phase (st, at, st + 1) and the second phase (t)."}, {"heading": "E Lemmas", "text": "To bind the maximum eigenvalues of the discount-weighted transition matrix, we must first address the following problem (>): < < < this problem is interesting independently, since it explicitly confirms the previous assumption. < 1.Proof: Part 1: First, we show that r (M) < 1 forM (M) < 1 forM (Mkl) if i = k, j = l Mkl otherwise.for all i, j, and all 0 < B < Mij < Mij (M) = psp (M) = 1 forM (M) is irreducible. We know that M (M) is irreducible. We know that connectivity is not changed (since no additional entries are made)."}, {"heading": "F Convergence of emphatic algorithms for the RL task formalism", "text": "We start with the expectation of ETDs for transition-based discounts, and the results for state-based MDPs should be automatically extended to transition-based MDPs, due to the equivalence shown in Section B.1. \u2212 \u2212 \u2212 In an effort to generalize the spelling of theoretical analysis similarly to the more general transition-based MDP settings, however, we explicitly provide evidence for transition-based MDPs. \u2212 \u2212 Assuming that the value function is approximated by linear function: v (s) = x (s) > w. For X with linear independent columns (i.e. linear independent characteristics), with an interest function i: (0, sp.) and M = diag (m) for m = (I \u2212 Pliquonal) \u2212 1 (d)), the matrix A is positively definite.A: = X > M (I > M) is positively defined."}], "references": [{"title": "Neuro-dynamic programming", "author": ["Dimitri P Bertsekas", "John N Tsitsiklis"], "venue": "Athena Scientific Press,", "citeRegEx": "Bertsekas and Tsitsiklis.,? \\Q1996\\E", "shortCiteRegEx": "Bertsekas and Tsitsiklis.", "year": 1996}, {"title": "Hierarchical Reinforcement Learning with the MAXQ Value Function Decomposition", "author": ["Thomas G Dietterich"], "venue": "Journal of Artificial Intelligence Research,", "citeRegEx": "Dietterich.,? \\Q2000\\E", "shortCiteRegEx": "Dietterich.", "year": 2000}, {"title": "An object-oriented representation for efficient reinforcement learning", "author": ["Carlos Diuk", "Andre Cohen", "Michael L Littman"], "venue": "In International Conference on Machine Learning,", "citeRegEx": "Diuk et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Diuk et al\\.", "year": 2008}, {"title": "Generalized Emphatic Temporal Difference Learning: Bias-Variance Analysis", "author": ["Assaf Hallak", "Aviv Tamar", "R\u00e9mi Munos", "Shie Mannor"], "venue": "CoRR abs/1509.05172,", "citeRegEx": "Hallak et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Hallak et al\\.", "year": 2015}, {"title": "Bias-Variance error bounds for temporal difference updates", "author": ["Michael J Kearns", "Satinder P Singh"], "venue": "In Annual Conference on Learning Theory,", "citeRegEx": "Kearns and Singh.,? \\Q2000\\E", "shortCiteRegEx": "Kearns and Singh.", "year": 2000}, {"title": "GQ (\u03bb): A general gradient algorithm for temporal-difference prediction learning with eligibility traces", "author": ["H Maei", "R Sutton"], "venue": "In AGI,", "citeRegEx": "Maei and Sutton.,? \\Q2010\\E", "shortCiteRegEx": "Maei and Sutton.", "year": 2010}, {"title": "Emphatic temporal-difference learning", "author": ["A Rupam Mahmood", "Huizhen Yu", "Martha White", "Richard S Sutton"], "venue": "In European Workshop on Reinforcement Learning,", "citeRegEx": "Mahmood et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Mahmood et al\\.", "year": 2015}, {"title": "Multi-timescale nexting in a reinforcement learning robot. Adaptive Behavior - Animals, Animats, Software Agents, Robots", "author": ["Joseph Modayil", "Adam White", "Richard S Sutton"], "venue": "Adaptive Systems,", "citeRegEx": "Modayil et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Modayil et al\\.", "year": 2014}, {"title": "Fast gradient-descent methods for temporal-difference learning with linear function approximation", "author": ["R Sutton", "H Maei", "D Precup", "S Bhatnagar"], "venue": "International Conference on Machine Learning,", "citeRegEx": "Sutton et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Sutton et al\\.", "year": 2009}, {"title": "Between MDPs and semi-MDPs: A framework for temporal abstraction in reinforcement learning", "author": ["Richard S Sutton", "Doina Precup", "Satinder Singh"], "venue": "Artificial intelligence,", "citeRegEx": "Sutton et al\\.,? \\Q1999\\E", "shortCiteRegEx": "Sutton et al\\.", "year": 1999}, {"title": "A new Q(lambda) with interim forward view and Monte Carlo equivalence", "author": ["Richard S Sutton", "Ashique Rupam Mahmood", "Doina Precup", "Hado van Hasselt"], "venue": "In International Conference on Machine Learning,", "citeRegEx": "Sutton et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Sutton et al\\.", "year": 2014}, {"title": "An emphatic approach to the problem of off-policy temporal-difference learning", "author": ["Richard S Sutton", "Ashique Rupam Mahmood", "Martha White"], "venue": "The Journal of Machine Learning Research,", "citeRegEx": "Sutton et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Sutton et al\\.", "year": 2016}, {"title": "TD models: Modeling the world at a mixture of time scales", "author": ["R.S. Sutton"], "venue": "In International Conference on Machine Learning,", "citeRegEx": "Sutton.,? \\Q1995\\E", "shortCiteRegEx": "Sutton.", "year": 1995}, {"title": "Reinforcement Learning: An Introduction", "author": ["R.S. Sutton", "A G Barto"], "venue": "MIT press,", "citeRegEx": "Sutton and Barto.,? \\Q1998\\E", "shortCiteRegEx": "Sutton and Barto.", "year": 1998}, {"title": "Horde: A scalable real-time architecture for learning knowledge from unsupervised sensorimotor interaction", "author": ["R.S. Sutton", "J Modayil", "M Delp", "T Degris", "P.M. Pilarski", "A White", "D Precup"], "venue": "In International Conference on Autonomous Agents and Multiagent Systems,", "citeRegEx": "Sutton et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Sutton et al\\.", "year": 2011}, {"title": "On the Rate of Convergence and Error Bounds for LSTD(\u03bb)", "author": ["Manel Tagorti", "Bruno Scherrer"], "venue": "In International Conference on Machine Learning,", "citeRegEx": "Tagorti and Scherrer.,? \\Q2015\\E", "shortCiteRegEx": "Tagorti and Scherrer.", "year": 2015}, {"title": "An analysis of temporal-difference learning with function approximation", "author": ["J.N. Tsitsiklis", "B Van Roy"], "venue": "IEEE Transactions on Automatic Control,", "citeRegEx": "Tsitsiklis and Roy.,? \\Q1997\\E", "shortCiteRegEx": "Tsitsiklis and Roy.", "year": 1997}, {"title": "Insights in Reinforcement Learning", "author": ["Hado Philip van Hasselt"], "venue": "PhD thesis, Hado van Hasselt,", "citeRegEx": "Hasselt.,? \\Q2011\\E", "shortCiteRegEx": "Hasselt.", "year": 2011}, {"title": "True online TD(lambda)", "author": ["Harm van Seijen", "Rich Sutton"], "venue": "In International Conference on Machine Learning,", "citeRegEx": "Seijen and Sutton.,? \\Q2014\\E", "shortCiteRegEx": "Seijen and Sutton.", "year": 2014}, {"title": "Developing a predictive approach to knowledge", "author": ["Adam White"], "venue": "PhD thesis, University of Alberta,", "citeRegEx": "White.,? \\Q2015\\E", "shortCiteRegEx": "White.", "year": 2015}, {"title": "Least Squares Temporal Difference Methods: An Analysis under General Conditions", "author": ["Huizhen Yu"], "venue": "SIAM Journal on Control and Optimization,", "citeRegEx": "Yu.,? \\Q2012\\E", "shortCiteRegEx": "Yu.", "year": 2012}, {"title": "On convergence of emphatic temporal-difference learning", "author": ["Huizhen Yu"], "venue": "In Annual Conference on Learning Theory,", "citeRegEx": "Yu.,? \\Q2015\\E", "shortCiteRegEx": "Yu.", "year": 2015}], "referenceMentions": [{"referenceID": 9, "context": "Progressively there have been additions to specify a broader range of objectives, including options [Sutton et al., 1999], state-based discounting [Sutton, 1995, Sutton et al.", "startOffset": 100, "endOffset": 121}, {"referenceID": 13, "context": "formulation [Sutton and Barto, 1998] and state-based discounting [Sutton, 1995, Maei and Sutton, 2010, Sutton et al.", "startOffset": 12, "endOffset": 36}, {"referenceID": 9, "context": "2 Options as RL tasks The options framework [Sutton et al., 1999] generically covers a wide range of settings, with discussion about macro-actions, option models, interrupting options and intra-option value learning.", "startOffset": 44, "endOffset": 65}, {"referenceID": 9, "context": "This relation is important for practically using these subtasks as part of semi-Markov decision processes, for which we have standard learning and planning algorithms [Sutton et al., 1999].", "startOffset": 167, "endOffset": 188}, {"referenceID": 14, "context": "3 General value functions and predictive questions In a similar spirit of abstraction as options, general value functions were introduced for single predictive or goal-oriented questions about the world [Sutton et al., 2011].", "startOffset": 203, "endOffset": 224}, {"referenceID": 14, "context": "The work on Horde [Sutton et al., 2011] and nexting [Modayil et al.", "startOffset": 18, "endOffset": 39}, {"referenceID": 7, "context": ", 2011] and nexting [Modayil et al., 2014] provide numerous examples of the utility of the types of questions that can be specified by general value functions, and so by RL tasks, because general value functions can naturally can be specified as an RL task.", "startOffset": 20, "endOffset": 42}, {"referenceID": 4, "context": "The trace parameter \u03bb : S\u00d7S \u2192 [0, 1] influences the fixed point and provides a modified (biased) return, called the \u03bb-return; this parameter is typically motivated as a bias-variance trade-off parameter [Kearns and Singh, 2000].", "startOffset": 203, "endOffset": 227}, {"referenceID": 3, "context": "For example, ETD was introduced with state-based \u03b3s; however, Hallak et al. [2015] analyzed approximation error bounds of ETD using a constant discount \u03b3c.", "startOffset": 62, "endOffset": 83}, {"referenceID": 3, "context": "A well-known result is that for D = D\u03c0 the Bellman operator is a contraction for constant \u03b3c and \u03bbc [Tsitsiklis and Van Roy, 1997]; recently, this has been generalized for a variant of ETD to M, still with constant parameters [Hallak et al., 2015].", "startOffset": 226, "endOffset": 247}, {"referenceID": 21, "context": "For policy evaluation, ETD and ELSTD, the least-squares version of ETD that uses the above defined A and b with D = M, have both been shown to converge with probability one [Yu, 2015].", "startOffset": 173, "endOffset": 183}, {"referenceID": 21, "context": "In particular, for appropriate step-sizes \u03b1t (see [Yu, 2015]), if A is positive definite, the iterative update is convergent wt+1 = wt + \u03b1t(b\u2212Awt).", "startOffset": 50, "endOffset": 60}, {"referenceID": 15, "context": "Convergence rate of LSTD(\u03bb) Tagorti and Scherrer [2015] recently provided convergence rates for LSTD(\u03bb) for continuing tasks, for some \u03b3c < 1.", "startOffset": 28, "endOffset": 56}], "year": 2017, "abstractText": "Reinforcement learning tasks are typically specified as Markov decision processes. This formalism has been highly successful, though specifications often couple the dynamics of the environment and the learning objective. This lack of modularity can complicate generalization of the task specification, as well as obfuscate connections between different task settings, such as episodic and continuing. In this work, we introduce the RL task formalism, that provides a unification through simple constructs including a generalization to transition-based discounting. Through a series of examples, we demonstrate the generality and utility of this formalism. Finally, we extend standard learning constructs, including Bellman operators, and extend some seminal theoretical results, including approximation errors bounds. Overall, we provide a well-understood and sound formalism on which to build theoretical results and simplify algorithm use and development.", "creator": "LaTeX with hyperref package"}}}