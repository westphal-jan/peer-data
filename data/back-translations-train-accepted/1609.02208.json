{"id": "1609.02208", "review": {"conference": "NIPS", "VERSION": "v1", "DATE_OF_SUBMISSION": "7-Sep-2016", "title": "Breaking the Bandwidth Barrier: Geometrical Adaptive Entropy Estimation", "abstract": "Estimators of information theoretic measures such as entropy and mutual information are a basic workhorse for many downstream applications in modern data science. State of the art approaches have been either geometric (nearest neighbor (NN) based) or kernel based (with a globally chosen bandwidth). In this paper, we combine both these approaches to design new estimators of entropy and mutual information that outperform state of the art methods. Our estimator uses local bandwidth choices of $k$-NN distances with a finite $k$, independent of the sample size. Such a local and data dependent choice improves performance in practice, but the bandwidth is vanishing at a fast rate, leading to a non-vanishing bias. We show that the asymptotic bias of the proposed estimator is universal; it is independent of the underlying distribution. Hence, it can be pre-computed and subtracted from the estimate. As a byproduct, we obtain a unified way of obtaining both kernel and NN estimators. The corresponding theoretical contribution relating the asymptotic geometry of nearest neighbors to order statistics is of independent mathematical interest.", "histories": [["v1", "Wed, 7 Sep 2016 22:11:39 GMT  (712kb,D)", "http://arxiv.org/abs/1609.02208v1", "24 pages 8 figures"]], "COMMENTS": "24 pages 8 figures", "reviews": [], "SUBJECTS": "cs.IT cs.LG math.IT stat.ML", "authors": ["weihao gao", "sewoong oh", "pramod viswanath"], "accepted": true, "id": "1609.02208"}, "pdf": {"name": "1609.02208.pdf", "metadata": {"source": "CRF", "title": "Breaking the Bandwidth Barrier: Geometrical Adaptive Entropy Estimation", "authors": ["Weihao Gao", "Sewoong Oh", "Pramod Viswanath"], "emails": ["wgao9@illinois.edu", "swoh@illinois.edu", "pramodv@illinois.edu"], "sections": [{"heading": "1 Introduction", "text": "In fact, it is one of the most important questions of modern data science; a common theme among the various approaches is to extract maximum \"informative\" characteristics by means of information theory metrics (entropy, mutual information and its variations), while in fact there is the primary reason for the popularity of information theory, that it is inescapable to one-to-one transformations, and that it obeys natural axioms such as data processing. Such an approach is evident in many fields of application, as varied as it is as computer biology [17] and information drive [23], the quotations representing a mere matrix of recent work. Within the mainstream, machine learning, systematic clustering and hierarchical information extraction are carried out in more recent works by [37, 35]. The fundamental workhorse in all these methods is the compilation of mutual information (mulpairing and)."}, {"heading": "2 Local likelihood density estimation (LLDE)", "text": "Given that this is a very basic statistical task, the local probability of increasing the local logarithm probability is higher than the local logarithm probability: Lx (f) = n K (also referred to as the kernel), a degree p (Xj) \u2212 n K (u \u2212 x h) f (u) f (u) f (u) du, where the maximization goes beyond an exponential polynomial family, locally approximated f (u) near x: loge fa, x (u) near x (u) near x: a0 + < a1, u \u2212 x (u \u2212 x)."}, {"heading": "6 Breaking the bandwidth barrier", "text": "While k-NN distance-based bandwidth is routine in practical applications [31], perhaps the most important finding of this work is that it also proves to be the \"right\" mathematical choice for the purpose of asymptotically unbiased estimation of an integral functionality such as entropy. We briefly discuss the following ramifications. Traditionally, if the goal is to estimate f (x), it is generally known that the bandwidth should satisfy h \u2192 0 and nhd \u2192 \u221e in order for KDEs to be consistent. As a rule of thumb, h = 1,06\u03c3 n \u2212 1 / 5 is suggested if d = 1 is the sample standard deviation [41, chapter 6.3]. On the other hand, in estimating entropy, as well as other integral functionalities, it is known that the substitution estimator of the form \u2212 d \u2212 d is a subversion of the standard deviation [41, chapter 6.3]."}, {"heading": "7 Discussion", "text": "The topic of estimating an integral function of unknown density from i.i.d. samples is a classic one in statistics and we will link some relevant topics from the literature in the context of the results of this manuscript."}, {"heading": "7.1 Uniform order statistics and NN distances", "text": "The expression for the asymptotic bias in (13), which is independent of the underlying distribution, forms the main result of this work and depends crucially on Lemma 3.1. Precisely, the problem implies that the quantities of Si converge in (10) in the distribution to S'i's in (12). There are two parts to this convergence result (29) for an overview of the results: the nearest neighboring distances converge to uniform order statistics and the directions converge to the nearest neighbors \u2212 \u2212 \u2212 the former indicators have been extensively studied, see [29] for an overview of the results. The latter is a new result that we give in Lemma 3.1 and which we have intuitively assumed in Section 9. The probability density fX in the neighborhood of a sample Xi (as defined by the distance to the k'th closest neighbor) and convergence to a uniform distribution over a sphere (decreasing from radius with the rate k = i) \u2212 n."}, {"heading": "7.2 Convergence rate of the bias", "text": "Determining the convergence rate of the KL estimator is a difficult problem and, despite the work of the last three decades, is not fully solved; the convergence rate of the variance O (1 / n) is determined in [3, 18, 2, 4] under various assumptions; determining the convergence rate of the bias is more difficult; it was first investigated in [10, 11], where the convergence of the root n is shown in a dimension with limited support and it is assumed that f (x) is limited at the bottom. [36] is the first evidence of a convergence rate of the root in the square of O (1 / n) for general densities with unlimited support in one dimension and exponentially decreasing tail, such as the Gaussian density. These assumptions are loosened in [5], where zeros and fattails in f (x) are permitted. In general d dimensions, the convergence of the manifest size is shown when convergence is loosened in a density, such as in a density of the palate."}, {"heading": "7.3 Ensemble estimators", "text": "Recent work [34, 25, 26, 1] has suggested ensemble estimators that use known estimators based on kernel density estimators and k-NN methods and construct a new estimate using the weighted linear combination of these methods with different bandwidth or k. By correctly selecting the weights that can be analytically calculated by solving a simple linear program, an increase in the convergence rate can be achieved. [1] An intuitive explanation for this phenomenon is provided in [1] in the context of the k-NN methods; it is interesting to investigate whether such a phenomenon persists in the k-LNN scenario examined in this paper. It is also interesting to see whether the leading terms (in terms of sample size n) of the bias have a multiplicative constant that depends only on the unknown distribution."}, {"heading": "8 Proofs", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "8.1 Proof of proposition 2.1", "text": "First, we prove the derivative of the LLDE with the degree p = 2 in Equation (7). The gradient of the local probability, which is evaluated at the maximizer, is zero [21], which results in a calculating tool for calculating the maximizer: 1n n n \u00b2 j = 1 K (Xj \u2212 x h) = 1 K (u \u2212 x h) ea0 + a T 1 (u \u2212 x) + (u \u2212 x) T a2 (u \u2212 x) du, (16) 1n n \u00b2 n \u00b2 j = 1 Xj \u2212 x h K (Xj \u2212 x h) = 1 K (u \u2212 x h) ea0 + a T 1 (u \u2212 x) + (u \u2212 x) T a2 (u \u2212 x) du, (17) 1n \u00b2 n \u00b2 n \u00b2 j = 1 (Xj \u2212 x) K (Xj \u2212 x Sh) (Xj \u2212 x S2 K (u \u2212 x)."}, {"heading": "9 Proof of Lemma 3.1", "text": "Let us first introduce some notations. Define Sd \u2212 1. (f). (f). (f). (f). (f). (f). (f). (f). (f). (f). (F). (F). (F). (F). (F). (F). (F). (F). (F). (F). (F). (F). (F). (F). (F). (F). (F). (F). (F). (F). (F). (F). (F). (F). (F). (F). (F). (F). (F). (F). (F). (F). (F). (F). (F). (F). (F). (F). (F). (F). (F). (F). (F). (F). (F). (F). (F). (F). (F). (F). (F). (F). (F). (F). (F). (F). (F). (F). (F). (F). (F). (F). (F). (F). (F). (F). (F). (F). (F). (F). (F). (F). (F). (F). (F). (F). (F). (F). (F). (F). (F). (F). (F). (F). (F). (F). (F). (F). (F). (F). (F). (F). (F). (F). (F). (F).). (F).). (F. (F). (F.). (F.).). (F. (F.).).).).). (F. (F.). (F.).). (.). (F.).). (."}, {"heading": "10 Proof of Theorem 1", "text": "We define the new notations by the estimation asH (n) k = 1n (n) n (n) n (n) n (n) n (n) n (n) n (n) n (n) n (n) n (n) s (n) s (n) s (n) s (n) s (n) s (n) s (n) n (n) n (n) n (n) n (n) s (n) s (n) s (n) s (n) s (n) s (n) s (n) s (n) s (n) s (n) s (n) s (n) s (n) s (n) s) n (n) s (n) s) n (n) s (n) n (n) s (n) n (n) n (n) n (n) n (n) n (n) n (n) n (n (n) n (n) n (n) n (n (n) n (n) n (n) n (n) n (n (n) n (n) n (n) n (n (n) s) n (n (n) s) n (n (n) n (n) n (n (n) s) n (n (n (n) s) n (n (n (n) n (n) n (n (n) s) n (n (n) n) n (n) n (n) n (n (n) n) n) n (n) n (n (n) n (n) n (n (n (n) n) n (n (n) n (n) n (n) n) n (n (n) n (n) n (n) n (n (n) n (n (n) n (n) n) n (n (n) n (n) n (n) n) n (n) n (n (n) n (n (n) n (n) n (n) n (n (n) n (n) n (n) n (n) n (n (n) n) n (n (n) n) n (n (n (n) n (n (n) n) n)"}, {"heading": "10.1 Proof of Lemma 10.1", "text": "Firstly, because the expectation of E-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-"}, {"heading": "11 Proof of Theorem 2", "text": "The proposed estimator is a solution to a maximization problem a = 67 = arg maxa LXi (fa, Xi). From [21] we know that the maximizer is a fixed point of a series of nonlinear equations of the form \u2211 j 6 = i (Xj \u2212 Xi) \u03b1k, i K (Xj \u2212 Xi \u03c1k, i) = n\u03c1dk, i e \u2212 Xi (u \u2212 Xi) \u03b1k, i K (u \u2212 Xi \u03c1k, i) e < u \u2212 x, a1 > + \u00b7 \u00b7 + ap [(u \u2212 x), \u00b7 \u00b7 p, (u \u2212 x)] 1 \u03c1dk, i \u2212 Xi \u0432\u0430k, i \u2212 p] where the high phrase p denotes the \u03b1-th order tensor product. From the evidence for theorem 1, specifically (48) and (50) p \u2212 d, we know that the left side d converges to a value that depends only on k, and K. Let it be raised by a certain product Tensk (o \u2212 d)."}, {"heading": "Acknowledgement", "text": "This work is supported by NSF SaTC award CNS-1527754, NSF CISE award CCF-1553452, NSF CISE award CCF-1617745. We thank the anonymous critics for their constructive feedback."}], "references": [{"title": "Efficient multivariate entropy estimation via k-nearest neighbour distances", "author": ["T.B. Berrett", "R.J. Samworth", "M. Yuan"], "venue": "arXiv preprint arXiv:1606.00304", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2016}, {"title": "Lectures on the Nearest Neighbor Method", "author": ["G. Biau", "L. Devroye"], "venue": "Springer", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2016}, {"title": "Sums of functions of nearest neighbor distances", "author": ["P.J. Bickel", "L. Breiman"], "venue": "moment bounds, limit theorems and a goodness of fit test. The Annals of Probability, pages 185\u2013214", "citeRegEx": "3", "shortCiteRegEx": null, "year": 1983}, {"title": "On the kozachenko\u2013leonenko entropy estimator, 2016", "author": ["S. Delattre", "N. Fournier"], "venue": null, "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2016}, {"title": "On entropy estimation by m-spacing method", "author": ["F. El Haje Hussein", "Y. Golubev"], "venue": "Journal of Mathematical Sciences,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2009}, {"title": "Efficient estimation of mutual information for strongly dependent variables", "author": ["S. Gao", "G. Ver Steeg", "A. Galstyan"], "venue": "International Conference on Artificial Intelligence and Statistics (AISTATS)", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2015}, {"title": "Estimating mutual information by local gaussian approximation", "author": ["S. Gao", "G. Ver Steeg", "A. Galstyan"], "venue": "31st Conference on Uncertainty in Artificial Intelligence (UAI)", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2015}, {"title": "Demystifying fixed k-nearest neighbor information estimators", "author": ["W. Gao", "S. Oh", "P. Viswanath"], "venue": "arXiv preprint arXiv:1604.03006", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2016}, {"title": "A new class of random vector entropy estimators and its applications in testing statistical hypotheses", "author": ["M.N. Goria", "N.N. Leonenko", "V.V. Mergel", "P.L. Novi Inverardi"], "venue": "Nonparametric Statistics, 17(3):277\u2013297", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2005}, {"title": "Limit theorems for sums of general functions of m-spacings", "author": ["P. Hall"], "venue": "Mathematical Proceedings of the Cambridge Philosophical Society, volume 96, pages 517\u2013532. Cambridge Univ Press", "citeRegEx": "10", "shortCiteRegEx": null, "year": 1984}, {"title": "On powerful distributional tests based on sample spacings", "author": ["P. Hall"], "venue": "Journal of Multivariate Analysis, 19(2):201\u2013224", "citeRegEx": "11", "shortCiteRegEx": null, "year": 1986}, {"title": "Locally parametric nonparametric density estimation", "author": ["N. Hjort", "M. Jones"], "venue": "The Annals of Statistics, pages 1619\u20131647", "citeRegEx": "12", "shortCiteRegEx": null, "year": 1996}, {"title": "Estimation of entropy and other functionals of a multivariate density", "author": ["H. Joe"], "venue": "Annals of the Institute of Statistical Mathematics, 41(4):683\u2013697", "citeRegEx": "13", "shortCiteRegEx": null, "year": 1989}, {"title": "Nonparametric von mises estimators for entropies", "author": ["K. Kandasamy", "A. Krishnamurthy", "B. Poczos", "L. Wasserman"], "venue": "divergences and mutual informations. In NIPS, pages 397\u2013405", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2015}, {"title": "Sample estimate of the entropy of a random vector", "author": ["L.F. Kozachenko", "N.N. Leonenko"], "venue": "Problemy Peredachi Informatsii, 23(2):9\u201316", "citeRegEx": "15", "shortCiteRegEx": null, "year": 1987}, {"title": "Estimating mutual information", "author": ["A. Kraskov", "H. St\u00f6gbauer", "P. Grassberger"], "venue": "Physical review E, 69(6):066138", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2004}, {"title": "Conditional density-based analysis of t cell signaling in single-cell data", "author": ["S. Krishnaswamy", "M. Spitzer", "M. Mingueneau", "S. Bendall", "O. Litvin", "E. Stone", "D. Peer", "G. Nolan"], "venue": "Science, 346:1250689", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2014}, {"title": "A class of r\u00e9nyi information estimators for multidimensional densities", "author": ["N. Leonenko", "L. Pronzato", "V. Savani"], "venue": "The Annals of Statistics, 36(5):2153\u20132182", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2008}, {"title": "Exponential concentration for mutual information estimation with application to forests", "author": ["H. Liu", "L. Wasserman", "J.D. Lafferty"], "venue": "NIPS, pages 2537\u20132545", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2012}, {"title": "Local regression and likelihood", "author": ["C. Loader"], "venue": "Springer Science & Business Media", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2006}, {"title": "Local likelihood density estimation", "author": ["C.R. Loader"], "venue": "The Annals of Statistics, 24(4):1602\u20131618", "citeRegEx": "21", "shortCiteRegEx": null, "year": 1996}, {"title": "Nonparametric k-nearest-neighbor entropy estimator", "author": ["D. Lombardi", "S. Pant"], "venue": "Physical Review E, 93(1):013310", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2016}, {"title": "Introduction to information retrieval", "author": ["C.D. Manning", "P. Raghavan", "H. Sch\u00fctze"], "venue": "volume 1. Cambridge university press Cambridge", "citeRegEx": "23", "shortCiteRegEx": null, "year": 2008}, {"title": "kn-nearest neighbor estimators of entropy", "author": ["R.M. Mnatsakanov", "N. Misra", "S. Li", "E.J. Harner"], "venue": "Mathematical Methods of Statistics, 17(3):261\u2013277", "citeRegEx": "24", "shortCiteRegEx": null, "year": 2008}, {"title": "Ensemble estimation of multivariate f-divergence", "author": ["K.R. Moon", "A.O. Hero"], "venue": "2014 IEEE International Symposium on Information Theory, pages 356\u2013360. IEEE", "citeRegEx": "25", "shortCiteRegEx": null, "year": 2014}, {"title": "Nonparametric ensemble estimation of distributional functionals", "author": ["K.R. Moon", "K. Sricharan", "K. Greenewald", "A.O. Hero III"], "venue": "arXiv preprint arXiv:1601.06884", "citeRegEx": "26", "shortCiteRegEx": null, "year": 2016}, {"title": "Submanifold density estimation", "author": ["A. Ozakin", "A.G. Gray"], "venue": "Advances in Neural Information Processing Systems, pages 1375\u20131382", "citeRegEx": "27", "shortCiteRegEx": null, "year": 2009}, {"title": "Estimation of r\u00e9nyi entropy and mutual information based on generalized nearest-neighbor graphs", "author": ["D. P\u00e1l", "B. P\u00f3czos", "C. Szepesv\u00e1ri"], "venue": "Advances in Neural Information Processing Systems, pages 1849\u20131857", "citeRegEx": "28", "shortCiteRegEx": null, "year": 2010}, {"title": "Approximate distributions of order statistics: with applications to nonparametric statistics", "author": ["R-D Reiss"], "venue": "Springer Science & Business Media", "citeRegEx": "29", "shortCiteRegEx": null, "year": 2012}, {"title": "Detecting novel associations in large data sets", "author": ["D. Reshef", "Y. Reshef", "H. Finucane", "S. Grossman", "G. McVean", "P. Turnbaugh", "E. Lander", "M. Mitzenmacher", "P. Sabeti"], "venue": "science, 334(6062):1518\u20131524", "citeRegEx": "30", "shortCiteRegEx": null, "year": 2011}, {"title": "Density estimation", "author": ["S.J. Sheather"], "venue": "Statistical Science, 19(4):588\u2013597", "citeRegEx": "31", "shortCiteRegEx": null, "year": 2004}, {"title": "Nearest neighbor estimates of entropy", "author": ["H. Singh", "N. Misra", "V. Hnizdo", "A. Fedorowicz", "E. Demchuk"], "venue": "American journal of mathematical and management sciences, 23(3-4):301\u2013321", "citeRegEx": "32", "shortCiteRegEx": null, "year": 2003}, {"title": "Analysis of k-nearest neighbor distances with application to entropy estimation", "author": ["S. Singh", "B. P\u00f3czos"], "venue": "arXiv preprint arXiv:1603.08578", "citeRegEx": "33", "shortCiteRegEx": null, "year": 2016}, {"title": "Ensemble estimators for multivariate entropy estimation", "author": ["K. Sricharan", "D. Wei", "A.O. Hero"], "venue": "IEEE Transactions on Information Theory, 59(7):4374\u20134388", "citeRegEx": "34", "shortCiteRegEx": null, "year": 2013}, {"title": "The information sieve", "author": ["G. Ver Steeg", "A. Galstyan"], "venue": "to appear in ICML, arXiv:1507.02284", "citeRegEx": "35", "shortCiteRegEx": null, "year": 2016}, {"title": "Root-n consistent estimators of entropy for densities with unbounded support", "author": ["A.B. Tsybakov", "E.C. Van der Meulen"], "venue": "Scandinavian Journal of Statistics,", "citeRegEx": "36", "shortCiteRegEx": "36", "year": 1996}, {"title": "Discovering structure in high-dimensional data through correlation explanation", "author": ["G. Ver Steeg", "A. Galstyan"], "venue": "Advances in Neural Information Processing Systems, pages 577\u2013585", "citeRegEx": "37", "shortCiteRegEx": null, "year": 2014}, {"title": "Locally weighted full covariance gaussian density estimation", "author": ["P. Vincent", "Y. Bengio"], "venue": "Technical report, Technical report 1240", "citeRegEx": "38", "shortCiteRegEx": null, "year": 2003}, {"title": "Divergence estimation for multidimensional densities via-nearest-neighbor distances", "author": ["Q. Wang", "S.R. Kulkarni", "S. Verd\u00fa"], "venue": "Information Theory, IEEE Transactions on, 55(5):2392\u20132405", "citeRegEx": "39", "shortCiteRegEx": null, "year": 2009}, {"title": "Universal estimation of information measures for analog sources", "author": ["Q. Wang", "S.R. Kulkarni", "S. Verd\u00fa"], "venue": "Foundations and Trends in Communications and Information Theory, 5(3):265\u2013353", "citeRegEx": "40", "shortCiteRegEx": null, "year": 2009}, {"title": "All of nonparametric statistics", "author": ["L. Wasserman"], "venue": "Springer Science & Business Media", "citeRegEx": "41", "shortCiteRegEx": null, "year": 2006}], "referenceMentions": [{"referenceID": 16, "context": "Such an approach is evident in many applications, as varied as computational biology [17], sociology [30] and information retrieval [23], with the citations representing a mere smattering of recent works.", "startOffset": 85, "endOffset": 89}, {"referenceID": 29, "context": "Such an approach is evident in many applications, as varied as computational biology [17], sociology [30] and information retrieval [23], with the citations representing a mere smattering of recent works.", "startOffset": 101, "endOffset": 105}, {"referenceID": 22, "context": "Such an approach is evident in many applications, as varied as computational biology [17], sociology [30] and information retrieval [23], with the citations representing a mere smattering of recent works.", "startOffset": 132, "endOffset": 136}, {"referenceID": 36, "context": "Within mainstream machine learning, a systematic effort at unsupervised clustering and hierarchical information extraction is conducted in recent works of [37, 35].", "startOffset": 155, "endOffset": 163}, {"referenceID": 34, "context": "Within mainstream machine learning, a systematic effort at unsupervised clustering and hierarchical information extraction is conducted in recent works of [37, 35].", "startOffset": 155, "endOffset": 163}, {"referenceID": 39, "context": "While these estimation questions have been studied in the past three decades (and summarized in [40]), the renewed importance of estimating information theoretic measures in a sample-efficient manner is persuasively argued in a recent work [6], where the authors note that existing estimators perform poorly in several key scenarios of central interest (especially when the high dimensional random variables are strongly related to each other).", "startOffset": 96, "endOffset": 100}, {"referenceID": 5, "context": "While these estimation questions have been studied in the past three decades (and summarized in [40]), the renewed importance of estimating information theoretic measures in a sample-efficient manner is persuasively argued in a recent work [6], where the authors note that existing estimators perform poorly in several key scenarios of central interest (especially when the high dimensional random variables are strongly related to each other).", "startOffset": 240, "endOffset": 243}, {"referenceID": 15, "context": "The widely used estimator of mutual information is the one by Kraskov and St\u00f6gbauer and Grassberger [16] and christened the KSG estimator (nomenclature based on the authors, cf.", "startOffset": 100, "endOffset": 104}, {"referenceID": 5, "context": "[6]) \u2013 while this estimator works well in practice (and performs much better than other approaches such as those based on kernel density estimation procedures), it \u2217Coordinated Science Lab and Department of Electrical and Computer Engineering \u2020Coordinated Science Lab and Department of Industrial and Enterprise Systems Engineering", "startOffset": 0, "endOffset": 3}, {"referenceID": 14, "context": "The basic issue is that the KSG estimator (and the underlying differential entropy estimator based on nearest neighbor distances by Kozachenko and Leonenko (KL) [15]) does not take advantage of the fact that the samples could lie in a smaller dimensional subspace (more generally, manifold) despite the high dimensionality of the data itself.", "startOffset": 161, "endOffset": 165}, {"referenceID": 6, "context": "Ameliorating this deficiency is the central theme of recent works [7, 6, 22], each of which aims to improve upon the classical KL (differential) entropy estimator of [15].", "startOffset": 66, "endOffset": 76}, {"referenceID": 5, "context": "Ameliorating this deficiency is the central theme of recent works [7, 6, 22], each of which aims to improve upon the classical KL (differential) entropy estimator of [15].", "startOffset": 66, "endOffset": 76}, {"referenceID": 21, "context": "Ameliorating this deficiency is the central theme of recent works [7, 6, 22], each of which aims to improve upon the classical KL (differential) entropy estimator of [15].", "startOffset": 66, "endOffset": 76}, {"referenceID": 14, "context": "Ameliorating this deficiency is the central theme of recent works [7, 6, 22], each of which aims to improve upon the classical KL (differential) entropy estimator of [15].", "startOffset": 166, "endOffset": 170}, {"referenceID": 5, "context": "A local SVD is used to heuristically improve the density estimate at each sample point in [6], while a local Gaussian density (with empirical mean and covariance weighted by NN distances) is heuristically used for the same purpose in [22].", "startOffset": 90, "endOffset": 93}, {"referenceID": 21, "context": "A local SVD is used to heuristically improve the density estimate at each sample point in [6], while a local Gaussian density (with empirical mean and covariance weighted by NN distances) is heuristically used for the same purpose in [22].", "startOffset": 234, "endOffset": 238}, {"referenceID": 6, "context": "An effort towards a systematic study is initiated in [7] which connects the aforementioned heuristic efforts of [6, 22] to the local log-likelihood density estimation methods [12, 21] from theoretical statistics.", "startOffset": 53, "endOffset": 56}, {"referenceID": 5, "context": "An effort towards a systematic study is initiated in [7] which connects the aforementioned heuristic efforts of [6, 22] to the local log-likelihood density estimation methods [12, 21] from theoretical statistics.", "startOffset": 112, "endOffset": 119}, {"referenceID": 21, "context": "An effort towards a systematic study is initiated in [7] which connects the aforementioned heuristic efforts of [6, 22] to the local log-likelihood density estimation methods [12, 21] from theoretical statistics.", "startOffset": 112, "endOffset": 119}, {"referenceID": 11, "context": "An effort towards a systematic study is initiated in [7] which connects the aforementioned heuristic efforts of [6, 22] to the local log-likelihood density estimation methods [12, 21] from theoretical statistics.", "startOffset": 175, "endOffset": 183}, {"referenceID": 20, "context": "An effort towards a systematic study is initiated in [7] which connects the aforementioned heuristic efforts of [6, 22] to the local log-likelihood density estimation methods [12, 21] from theoretical statistics.", "startOffset": 175, "endOffset": 183}, {"referenceID": 20, "context": "Equation (9) of [21]).", "startOffset": 16, "endOffset": 20}, {"referenceID": 6, "context": "Indeed, such an elaborate numerical effort is one of the key impediments for the entropy estimator of [7] to be practically valuable.", "startOffset": 102, "endOffset": 105}, {"referenceID": 40, "context": "samples is missing in the theoretical statistics literature (despite local log-likelihood methods for regression and density estimation being standard textbook fare [41, 20]).", "startOffset": 165, "endOffset": 173}, {"referenceID": 19, "context": "samples is missing in the theoretical statistics literature (despite local log-likelihood methods for regression and density estimation being standard textbook fare [41, 20]).", "startOffset": 165, "endOffset": 173}, {"referenceID": 15, "context": "This effort allows us to connect disparate threads of ideas from seemingly different arenas: NN methods, local log-likelihood methods, asymptotic order statistics and sample-dependent heuristic, but inspired, methods for mutual information estimation suggested in the work of [16].", "startOffset": 276, "endOffset": 280}, {"referenceID": 6, "context": "2], significantly improves the computational efficiency upon similar endeavors in the recent efforts of [7, 22, 38].", "startOffset": 104, "endOffset": 115}, {"referenceID": 21, "context": "2], significantly improves the computational efficiency upon similar endeavors in the recent efforts of [7, 22, 38].", "startOffset": 104, "endOffset": 115}, {"referenceID": 37, "context": "2], significantly improves the computational efficiency upon similar endeavors in the recent efforts of [7, 22, 38].", "startOffset": 104, "endOffset": 115}, {"referenceID": 28, "context": "Since the bandwidth is data dependent and vanishes too fast (because k is fixed), the estimator has a bias, which we derive a closed form expression for and show that it is independent of the underlying distribution and hence can be easily corrected: this is our main theoretical contribution, and involves new theorems on asymptotic statistics of nearest neighbors generalizing classical work in probability theory [29], which might be of independent mathematical interest.", "startOffset": 416, "endOffset": 420}, {"referenceID": 5, "context": "Generalized view: We show that seemingly very different approaches to entropy estimation \u2013 recent works of [6, 7, 22] and the classical work of fixed k-NN estimator of Kozachenko and Leonenko [15] \u2013 can all be cast in the local log-likelihood framework as specific kernel and sample dependent bandwidth choices.", "startOffset": 107, "endOffset": 117}, {"referenceID": 6, "context": "Generalized view: We show that seemingly very different approaches to entropy estimation \u2013 recent works of [6, 7, 22] and the classical work of fixed k-NN estimator of Kozachenko and Leonenko [15] \u2013 can all be cast in the local log-likelihood framework as specific kernel and sample dependent bandwidth choices.", "startOffset": 107, "endOffset": 117}, {"referenceID": 21, "context": "Generalized view: We show that seemingly very different approaches to entropy estimation \u2013 recent works of [6, 7, 22] and the classical work of fixed k-NN estimator of Kozachenko and Leonenko [15] \u2013 can all be cast in the local log-likelihood framework as specific kernel and sample dependent bandwidth choices.", "startOffset": 107, "endOffset": 117}, {"referenceID": 14, "context": "Generalized view: We show that seemingly very different approaches to entropy estimation \u2013 recent works of [6, 7, 22] and the classical work of fixed k-NN estimator of Kozachenko and Leonenko [15] \u2013 can all be cast in the local log-likelihood framework as specific kernel and sample dependent bandwidth choices.", "startOffset": 192, "endOffset": 196}, {"referenceID": 14, "context": "Thus our work is a strict mathematical generalization of the classical work of [15].", "startOffset": 79, "endOffset": 83}, {"referenceID": 15, "context": "Mutual Information estimation: The inspired work of [16] constructs a mutual information estimator that subtly altered (in a sample dependent way) the three KL entropy estimation terms, leading to superior empirical performance.", "startOffset": 52, "endOffset": 56}, {"referenceID": 20, "context": "Local likelihood density estimators [21, 12] constitute state of the art and are specified by a weight function K : R \u2192 R (also called a kernel), a degree p \u2208 Z of the polynomial approximation, and the bandwidth h \u2208 R, and maximizes the local log-likelihood: Lx(f) = n \u2211", "startOffset": 36, "endOffset": 44}, {"referenceID": 11, "context": "Local likelihood density estimators [21, 12] constitute state of the art and are specified by a weight function K : R \u2192 R (also called a kernel), a degree p \u2208 Z of the polynomial approximation, and the bandwidth h \u2208 R, and maximizes the local log-likelihood: Lx(f) = n \u2211", "startOffset": 36, "endOffset": 44}, {"referenceID": 20, "context": "Concretely, for p = 0, it is known that LDDE reduces to the standard Kernel Density Estimator (KDE) [21]:", "startOffset": 100, "endOffset": 104}, {"referenceID": 21, "context": "Somewhat surprisingly, this result has eluded prior works [22, 38] and [7] which specifically attempted the evaluation for p = 2.", "startOffset": 58, "endOffset": 66}, {"referenceID": 37, "context": "Somewhat surprisingly, this result has eluded prior works [22, 38] and [7] which specifically attempted the evaluation for p = 2.", "startOffset": 58, "endOffset": 66}, {"referenceID": 6, "context": "Somewhat surprisingly, this result has eluded prior works [22, 38] and [7] which specifically attempted the evaluation for p = 2.", "startOffset": 71, "endOffset": 74}, {"referenceID": 11, "context": "LLDE provides a principled approach to automatically correct for the boundary bias, which takes effect only for p \u2265 2 [12, 31].", "startOffset": 118, "endOffset": 126}, {"referenceID": 30, "context": "LLDE provides a principled approach to automatically correct for the boundary bias, which takes effect only for p \u2265 2 [12, 31].", "startOffset": 118, "endOffset": 126}, {"referenceID": 27, "context": "As noted in [28], such an assumption is common in the literature on consistency of k-NN estimators, where it has been implicitly assumed in existing analyses of entropy estimators including [15, 9, 18, 39], without explicitly stating that such assumptions are being made.", "startOffset": 12, "endOffset": 16}, {"referenceID": 14, "context": "As noted in [28], such an assumption is common in the literature on consistency of k-NN estimators, where it has been implicitly assumed in existing analyses of entropy estimators including [15, 9, 18, 39], without explicitly stating that such assumptions are being made.", "startOffset": 190, "endOffset": 205}, {"referenceID": 8, "context": "As noted in [28], such an assumption is common in the literature on consistency of k-NN estimators, where it has been implicitly assumed in existing analyses of entropy estimators including [15, 9, 18, 39], without explicitly stating that such assumptions are being made.", "startOffset": 190, "endOffset": 205}, {"referenceID": 17, "context": "As noted in [28], such an assumption is common in the literature on consistency of k-NN estimators, where it has been implicitly assumed in existing analyses of entropy estimators including [15, 9, 18, 39], without explicitly stating that such assumptions are being made.", "startOffset": 190, "endOffset": 205}, {"referenceID": 38, "context": "As noted in [28], such an assumption is common in the literature on consistency of k-NN estimators, where it has been implicitly assumed in existing analyses of entropy estimators including [15, 9, 18, 39], without explicitly stating that such assumptions are being made.", "startOffset": 190, "endOffset": 205}, {"referenceID": 5, "context": "Naturally, all the techniques we develop in this paper generalize to any estimators that depend on the nearest neighbor statistics {Z`,i}i,`\u2208[n] \u2013 and the value of such a general result is demonstrated later (in Section 4) when we evaluate the bias in similarly inspired entropy estimators [6, 7, 22, 15].", "startOffset": 290, "endOffset": 304}, {"referenceID": 6, "context": "Naturally, all the techniques we develop in this paper generalize to any estimators that depend on the nearest neighbor statistics {Z`,i}i,`\u2208[n] \u2013 and the value of such a general result is demonstrated later (in Section 4) when we evaluate the bias in similarly inspired entropy estimators [6, 7, 22, 15].", "startOffset": 290, "endOffset": 304}, {"referenceID": 21, "context": "Naturally, all the techniques we develop in this paper generalize to any estimators that depend on the nearest neighbor statistics {Z`,i}i,`\u2208[n] \u2013 and the value of such a general result is demonstrated later (in Section 4) when we evaluate the bias in similarly inspired entropy estimators [6, 7, 22, 15].", "startOffset": 290, "endOffset": 304}, {"referenceID": 14, "context": "Naturally, all the techniques we develop in this paper generalize to any estimators that depend on the nearest neighbor statistics {Z`,i}i,`\u2208[n] \u2013 and the value of such a general result is demonstrated later (in Section 4) when we evaluate the bias in similarly inspired entropy estimators [6, 7, 22, 15].", "startOffset": 290, "endOffset": 304}, {"referenceID": 14, "context": "The idea of using k-NN distance as bandwidth for entropy estimation was originally proposed by Kozachenko and Leonenko in [15], and is a special case of the k-LNN method we propose with degree 0 and a step kernel.", "startOffset": 122, "endOffset": 126}, {"referenceID": 12, "context": "Another popular resubstitution entropy estimator is to use KDE in (3) [13], which is a special case of the k-LNN method with degree 0, and the Gaussian kernel is used in simulations.", "startOffset": 70, "endOffset": 74}, {"referenceID": 13, "context": "As comparison, we also study a new estimator [14] based on von Mises expansion (as opposed to simple re-substitution) which has an improved convergence rate in the large sample regime.", "startOffset": 45, "endOffset": 49}, {"referenceID": 14, "context": "4 Universality of the k-LNN approach In this section, we show that Theorem 1 holds universally for a general family of entropy estimators, specified by the choice of k \u2208 Z, degree p \u2208 Z, and a kernel K : R \u2192 R, thus allowing a unified view of several seemingly disparate entropy estimators [15, 6, 7, 22].", "startOffset": 290, "endOffset": 304}, {"referenceID": 5, "context": "4 Universality of the k-LNN approach In this section, we show that Theorem 1 holds universally for a general family of entropy estimators, specified by the choice of k \u2208 Z, degree p \u2208 Z, and a kernel K : R \u2192 R, thus allowing a unified view of several seemingly disparate entropy estimators [15, 6, 7, 22].", "startOffset": 290, "endOffset": 304}, {"referenceID": 6, "context": "4 Universality of the k-LNN approach In this section, we show that Theorem 1 holds universally for a general family of entropy estimators, specified by the choice of k \u2208 Z, degree p \u2208 Z, and a kernel K : R \u2192 R, thus allowing a unified view of several seemingly disparate entropy estimators [15, 6, 7, 22].", "startOffset": 290, "endOffset": 304}, {"referenceID": 21, "context": "4 Universality of the k-LNN approach In this section, we show that Theorem 1 holds universally for a general family of entropy estimators, specified by the choice of k \u2208 Z, degree p \u2208 Z, and a kernel K : R \u2192 R, thus allowing a unified view of several seemingly disparate entropy estimators [15, 6, 7, 22].", "startOffset": 290, "endOffset": 304}, {"referenceID": 14, "context": "[15] showed (in a remarkable result at the time) that the asymptotic bias is independent of the dimension d and can be computed exactly to be log n\u2212 \u03c8(n) + \u03c8(k)\u2212 log k and \u03c8(k) is the digamma function defined as \u03c8(x) = \u0393\u22121(x)d\u0393(x)/dx.", "startOffset": 0, "endOffset": 4}, {"referenceID": 7, "context": "The dimension independent nature of this asymptotic bias term (of O(n\u22121/2) for d = 1 in [36, Theorem 1] and O(n\u22121/d) for general d in [8]) is special to the choice of p = 0 and the step kernel; we explain this in detail in Section 11, later in the paper.", "startOffset": 134, "endOffset": 137}, {"referenceID": 5, "context": "Analogously, the estimator in [6] can be viewed as a special case with p = 0 and an ellipsoidal step kernel.", "startOffset": 30, "endOffset": 33}, {"referenceID": 15, "context": "In [16], Kraskov and St\u00f6gbauer and Grassberger introduced \u00ceKSG(X;Y ) by coupling the choices of the bandwidths.", "startOffset": 3, "endOffset": 7}, {"referenceID": 15, "context": "Inspired by [16], we introduce the following novel mutual information estimator we denote by \u00ceLNN\u2212KSG(X;Y ).", "startOffset": 12, "endOffset": 16}, {"referenceID": 0, "context": "samples from two random variables X and Y , where X is uniform over [0, 1] and Y = X + U , where U is uniform over [0, 0.", "startOffset": 68, "endOffset": 74}, {"referenceID": 5, "context": "Mutual information estimators have been recently proposed in [6, 7, 22] based on local likelihood maximization.", "startOffset": 61, "endOffset": 71}, {"referenceID": 6, "context": "Mutual information estimators have been recently proposed in [6, 7, 22] based on local likelihood maximization.", "startOffset": 61, "endOffset": 71}, {"referenceID": 21, "context": "Mutual information estimators have been recently proposed in [6, 7, 22] based on local likelihood maximization.", "startOffset": 61, "endOffset": 71}, {"referenceID": 0, "context": "In Figure 6, we test the mutual information estimators for Y = f(X) + U , where X is uniformly distributed over [0, 1] and U is uniformly distributed over [0, \u03b8], independent of X, for some noise level \u03b8.", "startOffset": 112, "endOffset": 118}, {"referenceID": 6, "context": "Similar simulation were studied in [7].", "startOffset": 35, "endOffset": 38}, {"referenceID": 0, "context": "Here Xi\u2019s are uniformly distributed over [0, 1] and U is uniformly distributed over [\u22123/2, 3/2], independently of Xi\u2019s.", "startOffset": 41, "endOffset": 47}, {"referenceID": 5, "context": "Similar simulation were studied in [6].", "startOffset": 35, "endOffset": 38}, {"referenceID": 30, "context": "6 Breaking the bandwidth barrier While k-NN distance based bandwidth are routine in practical usage [31], the main finding of this work is that they also turn out to be the \u201ccorrect\" mathematical choice for the purpose of asymptotically unbiased estimation of an integral functional such as the entropy: \u2212 \u222b f(x) log f(x); we briefly discuss the ramifications below.", "startOffset": 100, "endOffset": 104}, {"referenceID": 18, "context": "On the other hand, when estimating entropy, as well as other integral functionals, it is known that resubstitution estimators of the form \u2212(1/n) \u2211n i=1 log f\u0302(Xi) achieve variances scaling as O(1/n) independent of the bandwidth [19].", "startOffset": 228, "endOffset": 232}, {"referenceID": 18, "context": "The bottleneck in choosing such a small bandwidth is the bias, scaling as O(h + (nhd)\u22121 + En) [19], where the lower order dependence on n, dubbed En, is generally not known.", "startOffset": 94, "endOffset": 98}, {"referenceID": 17, "context": "Following recent advances in [18, 33], the proposed local estimator has a potential to be extended to, for example, Renyi entropy, but with a multiplicative bias as opposed to additive.", "startOffset": 29, "endOffset": 37}, {"referenceID": 32, "context": "Following recent advances in [18, 33], the proposed local estimator has a potential to be extended to, for example, Renyi entropy, but with a multiplicative bias as opposed to additive.", "startOffset": 29, "endOffset": 37}, {"referenceID": 28, "context": "The former has been extensively studied, for example see [29] for a survey of results.", "startOffset": 57, "endOffset": 61}, {"referenceID": 14, "context": "based estimators, such as the entropy estimator of [15].", "startOffset": 51, "endOffset": 55}, {"referenceID": 14, "context": "In the seminal paper, [15] introduced resubstitution entropy estimators of the form \u0124(X) = \u2212(1/n) \u2211n i=1 log f\u0302n(Xi) with f\u0302n(x) = k/(nCd \u03c1 d k,x) (as defined in (4)).", "startOffset": 22, "endOffset": 26}, {"referenceID": 14, "context": "For k = 1 this was proved in the original paper of [15], which later was extended in [32, 9] to general k.", "startOffset": 51, "endOffset": 55}, {"referenceID": 31, "context": "For k = 1 this was proved in the original paper of [15], which later was extended in [32, 9] to general k.", "startOffset": 85, "endOffset": 92}, {"referenceID": 8, "context": "For k = 1 this was proved in the original paper of [15], which later was extended in [32, 9] to general k.", "startOffset": 85, "endOffset": 92}, {"referenceID": 14, "context": "This mysterious bias term Bk,d = (\u03c8(k)\u2212 log(k)) whose original proofs in [15, 32, 9] provided little explanation for, can be alternatively proved with both rigor and intuition by making connections to uniform order statistics.", "startOffset": 73, "endOffset": 84}, {"referenceID": 31, "context": "This mysterious bias term Bk,d = (\u03c8(k)\u2212 log(k)) whose original proofs in [15, 32, 9] provided little explanation for, can be alternatively proved with both rigor and intuition by making connections to uniform order statistics.", "startOffset": 73, "endOffset": 84}, {"referenceID": 8, "context": "This mysterious bias term Bk,d = (\u03c8(k)\u2212 log(k)) whose original proofs in [15, 32, 9] provided little explanation for, can be alternatively proved with both rigor and intuition by making connections to uniform order statistics.", "startOffset": 73, "endOffset": 84}, {"referenceID": 2, "context": "The O(1/n) convergence rate of the variance is established in [3, 18, 2, 4] under various assumptions.", "startOffset": 62, "endOffset": 75}, {"referenceID": 17, "context": "The O(1/n) convergence rate of the variance is established in [3, 18, 2, 4] under various assumptions.", "startOffset": 62, "endOffset": 75}, {"referenceID": 1, "context": "The O(1/n) convergence rate of the variance is established in [3, 18, 2, 4] under various assumptions.", "startOffset": 62, "endOffset": 75}, {"referenceID": 3, "context": "The O(1/n) convergence rate of the variance is established in [3, 18, 2, 4] under various assumptions.", "startOffset": 62, "endOffset": 75}, {"referenceID": 9, "context": "It has been first studied in [10, 11], where root-n consistency is shown in 1-dimension with bounded support and assuming f(x) is bounded below.", "startOffset": 29, "endOffset": 37}, {"referenceID": 10, "context": "It has been first studied in [10, 11], where root-n consistency is shown in 1-dimension with bounded support and assuming f(x) is bounded below.", "startOffset": 29, "endOffset": 37}, {"referenceID": 35, "context": "[36] is the first to prove a root mean squared error convergence rate of O(1/ \u221a n) for general densities with unbounded support in 1-dimension and exponentially decaying tail, such as the Gaussian density.", "startOffset": 0, "endOffset": 4}, {"referenceID": 4, "context": "These assumptions are relaxed in [5], where zeroes and fat tails are allowed in f(x).", "startOffset": 33, "endOffset": 36}, {"referenceID": 7, "context": "In general d-dimensions, [8, 33] prove bounds on the convergence rate of the bias for finite k = O(1), and [24, 1] for k = \u03a9(log n).", "startOffset": 25, "endOffset": 32}, {"referenceID": 32, "context": "In general d-dimensions, [8, 33] prove bounds on the convergence rate of the bias for finite k = O(1), and [24, 1] for k = \u03a9(log n).", "startOffset": 25, "endOffset": 32}, {"referenceID": 23, "context": "In general d-dimensions, [8, 33] prove bounds on the convergence rate of the bias for finite k = O(1), and [24, 1] for k = \u03a9(log n).", "startOffset": 107, "endOffset": 114}, {"referenceID": 0, "context": "In general d-dimensions, [8, 33] prove bounds on the convergence rate of the bias for finite k = O(1), and [24, 1] for k = \u03a9(log n).", "startOffset": 107, "endOffset": 114}, {"referenceID": 26, "context": "This is made precise in [27], which also shows improved convergence rates for distributions whose support is on low dimensional manifolds.", "startOffset": 24, "endOffset": 28}, {"referenceID": 26, "context": "However, the estimator in [27] critically uses the geodesic distances between the sample points on the manifold.", "startOffset": 26, "endOffset": 30}, {"referenceID": 33, "context": "3 Ensemble estimators Recent works [34, 25, 26, 1] have proposed ensemble estimators, which use known estimators based on kernel density estimators and k-NN methods and construct a new estimate by taking the weighted linear combination of those methods with varying bandwidth or k, respectively.", "startOffset": 35, "endOffset": 50}, {"referenceID": 24, "context": "3 Ensemble estimators Recent works [34, 25, 26, 1] have proposed ensemble estimators, which use known estimators based on kernel density estimators and k-NN methods and construct a new estimate by taking the weighted linear combination of those methods with varying bandwidth or k, respectively.", "startOffset": 35, "endOffset": 50}, {"referenceID": 25, "context": "3 Ensemble estimators Recent works [34, 25, 26, 1] have proposed ensemble estimators, which use known estimators based on kernel density estimators and k-NN methods and construct a new estimate by taking the weighted linear combination of those methods with varying bandwidth or k, respectively.", "startOffset": 35, "endOffset": 50}, {"referenceID": 0, "context": "3 Ensemble estimators Recent works [34, 25, 26, 1] have proposed ensemble estimators, which use known estimators based on kernel density estimators and k-NN methods and construct a new estimate by taking the weighted linear combination of those methods with varying bandwidth or k, respectively.", "startOffset": 35, "endOffset": 50}, {"referenceID": 0, "context": "An intuitive explanation for this phenomenon is provided in [1] in the context of k-NN methods; it is interesting to explore if such a phenomenon continues in the k-LNN scenario studied in this paper.", "startOffset": 60, "endOffset": 63}, {"referenceID": 20, "context": "The gradient of the local likelihood evaluated at the maximizer is zero [21], which gives a computational tool for finding the maximizer:", "startOffset": 72, "endOffset": 76}, {"referenceID": 28, "context": "5 of [29] to show that this term vanishes for m = O(log n) and as n grows.", "startOffset": 5, "endOffset": 9}, {"referenceID": 28, "context": "5, [29]).", "startOffset": 3, "endOffset": 7}, {"referenceID": 27, "context": "As noted in [28] this is common in the literature on consistency of k-NN estimators, where the same assumptions have been implicitly made without explicitly stating as such, in existing analyses of entropy estimators including [15, 9, 18, 39].", "startOffset": 12, "endOffset": 16}, {"referenceID": 14, "context": "As noted in [28] this is common in the literature on consistency of k-NN estimators, where the same assumptions have been implicitly made without explicitly stating as such, in existing analyses of entropy estimators including [15, 9, 18, 39].", "startOffset": 227, "endOffset": 242}, {"referenceID": 8, "context": "As noted in [28] this is common in the literature on consistency of k-NN estimators, where the same assumptions have been implicitly made without explicitly stating as such, in existing analyses of entropy estimators including [15, 9, 18, 39].", "startOffset": 227, "endOffset": 242}, {"referenceID": 17, "context": "As noted in [28] this is common in the literature on consistency of k-NN estimators, where the same assumptions have been implicitly made without explicitly stating as such, in existing analyses of entropy estimators including [15, 9, 18, 39].", "startOffset": 227, "endOffset": 242}, {"referenceID": 38, "context": "As noted in [28] this is common in the literature on consistency of k-NN estimators, where the same assumptions have been implicitly made without explicitly stating as such, in existing analyses of entropy estimators including [15, 9, 18, 39].", "startOffset": 227, "endOffset": 242}, {"referenceID": 27, "context": "This assumption can be avoided for Renyi entropy as in the proof of consistency in [28] or for sharper results such as the convergence rate of the bias with respect to the sample size but with more assumptions as in [8, 33, 1].", "startOffset": 83, "endOffset": 87}, {"referenceID": 7, "context": "This assumption can be avoided for Renyi entropy as in the proof of consistency in [28] or for sharper results such as the convergence rate of the bias with respect to the sample size but with more assumptions as in [8, 33, 1].", "startOffset": 216, "endOffset": 226}, {"referenceID": 32, "context": "This assumption can be avoided for Renyi entropy as in the proof of consistency in [28] or for sharper results such as the convergence rate of the bias with respect to the sample size but with more assumptions as in [8, 33, 1].", "startOffset": 216, "endOffset": 226}, {"referenceID": 0, "context": "This assumption can be avoided for Renyi entropy as in the proof of consistency in [28] or for sharper results such as the convergence rate of the bias with respect to the sample size but with more assumptions as in [8, 33, 1].", "startOffset": 216, "endOffset": 226}, {"referenceID": 20, "context": "From [21] we know that the maximizer is a fixed point of a series of non-linear equations of the form \u2211", "startOffset": 5, "endOffset": 9}], "year": 2016, "abstractText": "Estimators of information theoretic measures such as entropy and mutual information are a basic workhorse for many downstream applications in modern data science. State of the art approaches have been either geometric (nearest neighbor (NN) based) or kernel based (with a globally chosen bandwidth). In this paper, we combine both these approaches to design new estimators of entropy and mutual information that outperform state of the art methods. Our estimator uses local bandwidth choices of k-NN distances with a finite k, independent of the sample size. Such a local and data dependent choice improves performance in practice, but the bandwidth is vanishing at a fast rate, leading to a non-vanishing bias. We show that the asymptotic bias of the proposed estimator is universal; it is independent of the underlying distribution. Hence, it can be precomputed and subtracted from the estimate. As a byproduct, we obtain a unified way of obtaining both kernel and NN estimators. The corresponding theoretical contribution relating the asymptotic geometry of nearest neighbors to order statistics is of independent mathematical interest.", "creator": "LaTeX with hyperref package"}}}