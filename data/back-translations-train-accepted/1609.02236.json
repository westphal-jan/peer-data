{"id": "1609.02236", "review": {"conference": "AAAI", "VERSION": "v1", "DATE_OF_SUBMISSION": "8-Sep-2016", "title": "Latent Dependency Forest Models", "abstract": "Probabilistic modeling is one of the foundations of modern machine learning and artificial intelligence. In this paper, we propose a novel type of probabilistic models named latent dependency forest models (LDFMs). A LDFM models the dependencies between random variables with a forest structure that can change dynamically based on the variable values. It is therefore capable of modeling context-specific independence. We parameterize a LDFM using a first-order non-projective dependency grammar. Learning LDFMs from data can be formulated purely as a parameter learning problem, and hence the difficult problem of model structure learning is circumvented. Our experimental results show that LDFMs are competitive with existing probabilistic models.", "histories": [["v1", "Thu, 8 Sep 2016 00:57:19 GMT  (276kb,D)", "https://arxiv.org/abs/1609.02236v1", "7 pages, 2 figures, conference"], ["v2", "Sun, 20 Nov 2016 15:51:35 GMT  (436kb,D)", "http://arxiv.org/abs/1609.02236v2", "10 pages, 3 figures, conference"]], "COMMENTS": "7 pages, 2 figures, conference", "reviews": [], "SUBJECTS": "cs.AI", "authors": ["shanbo chu", "yong jiang", "kewei tu"], "accepted": true, "id": "1609.02236"}, "pdf": {"name": "1609.02236.pdf", "metadata": {"source": "CRF", "title": "Latent Dependency Forest Models", "authors": ["Shanbo Chu", "Yong Jiang", "Kewei Tu"], "emails": ["tukw}@shanghaitech.edu.cn"], "sections": [{"heading": "Introduction", "text": "In fact, most of them are able to survive on their own, and they see themselves able to survive on their own."}, {"heading": "Related Work", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "Probabilistic Modeling", "text": "In fact, it is the case that most of them are able to move into another world, in which they are able to move, in which they are able to live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they, in which they live, in which they live, in which they live, in which they live, in which they live."}, {"heading": "Dependency Grammars", "text": "In natural language processing (NLP), dependency grammatics (DGs) are a simple flexible mechanism for encoding words and their syntactic dependencies by directed graphs. In directed graphs, derived from a sentence, each word is anode, and the dependency relationships between words are the edges. In a non-projective dependency graph, one edge can cross with other edges. Non-projection occurs due to dependencies over long distances or in languages with flexible order. In most cases, the dependency structure is assumed to be directed tree. Figure 2 is an example of a non-projective dependency tree for the sentence A hearing is scheduled today. Each edge associates a word with its modifier and is labeled with the specific syntactic function of the dependence, e.g. SBJ for the subject. Parsing the dependencies (finding the dependency tree) is an important task in the 2005 NJP and McDonald maximum efficiency models (the first McDonald algorithms)."}, {"heading": "Latent Dependency Forest Models", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "Basics", "text": "We assume that the actual dependency relationships between the variables always form a forest structure (a series of trees). By adding an edge from the dummy root node to the root of each tree, we assume that the root of each dependency tree is forest. We obtain a single tree structure, which extends over the graph Gx = Q = any tree thickness. We assume that the actual dependency relationships between the variables always form a forest structure (a series of trees). By assuming an edge from the dummy root node Vx, we assume that the root of each tree of the dependency is forest."}, {"heading": "LDFM", "text": "LDFM requires that the weight of each dependence (xi, xj) is the conditional probability to generate the variable Xj and set its value to xj, since the assignment Xi = xi or the root node is given. We designate this probability by wxj | xi. however, we specify the constraint 0 \u2264 wxj | xi \u2264 1 and the normalization condition \u2211 j 6 = i \u2211 xj wxj | xi = 1 for each xi (or \u2211 j 6 = i \u0109xj wxj | 1 for continuous variables). An assignment x = (x1, x2,.., xn) is generated recursively in a top-down manner in which the assignment of the assignment of the assignment of the assignment of the assignment of the assignment of the assignment of the assignment of the assignment of the assignment of the assignment of the assignment of the assignment of the assignment of the assignment of the assignment of the assignment of the assignment of the assignment of the assignment of the assignment of the assignment of the assignment of the assignment of the assignment of the assignment of the assignment of the assignment of the assignment of the assignment of the assignment of the assignment of the assignment of the assignment of the assignment of the assignment of the assignment of the assignment of the assignment of the assignment of the assignment of the assignment of the assignment of the assignment of the assignment of the assignment of the assignment of the assignment of the assignment of the assignment of the assignment of the assignment of the assignment of the assignment of the assignment of the assignment of the Xj requires the value of the value of the assignment of the assignment of the assignment of the assignment of the assignment of the assignment of the assignment of the assignment of the assignment of the assignment of the assignment of the assign"}, {"heading": "LDFM-S", "text": "It is the way in which people move in the individual countries of the world in the way in which they move in the individual countries of the world, in the most varied expressions in which they move, in the way in which they move, in the way in which they move, in the way in which they move, in the way in which they move themselves, in the way in which they move themselves, in the way in which they move themselves, in the way in which they move themselves, in the way in which they move themselves, in the way in which they move themselves, in which they move themselves, in which they themselves, in which they themselves, in which they themselves, in which they themselves, in which they are themselves, in which they themselves, in which they themselves, in which they themselves, in which they, in which they, in which they, in which they, in which they, in which they, in which they, in which they, in which they, in which they, in which they, in which they, in which they, in which they, in which they, in which they, in which they, in which they, in which they, in which they, in which they, in which they, in which they, in which they, in which they, in which they, in which they, in which they, in which they, in which they, in which they, in which they, in which they, in which they, in which they, in which they, in which they, in which they, in which they, in which they, in which they, in which they, in which they, in which they, in which they, in which they, in which they, in which they, in which they, in which they, in which they, in which they, in which they, in which they, they, in which they, in which they, they, in which they, in which they, in which they, in which, in which they, in which they, in which they, they, in which they, in which they, in which they, in which they, in which they, in which they, they, in which they, in which they, in which they, in which they, in which they, in which they, in which they, in which they, in which they, in which they, in which they, in which they, they, in which they, in which they, in which"}, {"heading": "Supplementary Material", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "An Example of Using LDFMs to Model CSI", "text": "The allocation of variables can affect distribution across dependency structures, allowing LDFMs to model CSI to a certain extent. Here is an example of using LDFM-S to model three binary variables X1, X2, and X3. Figure 3 gives an example of using LDFM-S to model CSI. The conditional probabilities of the two variables X2 and X3 can be calculated using the formula in the LDFM-S section and are shown in Table 3. If X1 = T, X2, and X3 are highly interdependent, they are only slightly interdependent at X1 = F."}, {"heading": "The Derivation Details", "text": "We show the details of the derivation of the probability to create an assignment x, which is discussed in the LDFM subsection in the main text. T * is the uniformly generated tree structure and M is an assignment from the n variables to the n nodes of the tree structure T, \u03b2 is the constant value of p (T). \u03b2n! is a constant w.r.t. x. Here we have n!, because for each surrounding tree T of Gx each permutation of the n variables is generated differently (i.e. corresponds to another < T, M > pair)."}, {"heading": "The Evaluation Results of LDFM-S", "text": "We report on the results of LDFM-S and LDFM, which were formed on the 5000 sample data sets and evaluated using Gibbs sampling using two different portions of the division of the query and evidence object variables in Table 4. It can be seen that LDFM-S performs similarly to LDFM in most datasets, but achieves significantly better results with the Win95pts dataset and significantly worse results with the Water dataset. Therefore, it may depend on the dataset whether modelling distributions via tree structures is useful."}, {"heading": "More Evaluation Results", "text": "In Table 5 we report on the evaluation results of the other two components (30% query, 40% evidence and 20% query, 30% evidence).References [Adel, Balduzzi, and Ghodsi 2015] Adel, T.; Balduzzi, D.; and Ghodsi, A. 2015. Learning the structure of sum-product networks via an svd-based algorithm. In 31. Conference on Uncertainty in Artificial Intelligence (UAI).Ph.D. Dissertation, Computer Science Department, Stanford University, Stanford, CA. [Tanner and Wong 1987] Tanner, M. A., and Wong, W. H. 1987. The calculation of posterior distributions by data augmentation. Journal of the American gramstical Association 82 (398: 534] and Wong 1987, Tunified Tarids."}], "references": [{"title": "Learning the structure of sum-product networks via an svd-based algorithm", "author": ["Balduzzi Adel", "T. Ghodsi 2015] Adel", "D. Balduzzi", "A. Ghodsi"], "venue": "In 31st Conference on Uncertainty in Artificial Intelligence (UAI)", "citeRegEx": "Adel et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Adel et al\\.", "year": 2015}, {"title": "and Pedro", "author": ["R. Gens"], "venue": "D.", "citeRegEx": "Gens and Pedro 2013", "shortCiteRegEx": null, "year": 2013}, {"title": "and Satta", "author": ["R. McDonald"], "venue": "G.", "citeRegEx": "McDonald and Satta 2007", "shortCiteRegEx": null, "year": 2007}, {"title": "Non-projective dependency parsing using spanning tree algorithms", "author": ["McDonald"], "venue": "In Proceedings of the conference on Human Language Technology and Empirical Methods in Natural Language Processing,", "citeRegEx": "McDonald,? \\Q2005\\E", "shortCiteRegEx": "McDonald", "year": 2005}, {"title": "M", "author": ["M. Meila", "Jordan"], "venue": "I.", "citeRegEx": "Meila and Jordan 2001", "shortCiteRegEx": null, "year": 2001}, {"title": "Learning selective sum-product networks", "author": ["Gens Peharz", "R. Domingos 2014] Peharz", "R. Gens", "P. Domingos"], "venue": "In LTPM workshop", "citeRegEx": "Peharz et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Peharz et al\\.", "year": 2014}, {"title": "and Domingos", "author": ["H. Poon"], "venue": "P.", "citeRegEx": "Poon and Domingos 2011", "shortCiteRegEx": null, "year": 2011}, {"title": "Cutset networks: A simple, tractable, and scalable approach for improving the accuracy of chow-liu trees", "author": ["Kothalkar Rahman", "T. Gogate 2014] Rahman", "P. Kothalkar", "V. Gogate"], "venue": "In Joint European Conference on Machine Learning and Knowledge Discovery in Databases,", "citeRegEx": "Rahman et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Rahman et al\\.", "year": 2014}, {"title": "and Lowd", "author": ["A. Rooshenas"], "venue": "D.", "citeRegEx": "Rooshenas and Lowd 2014", "shortCiteRegEx": null, "year": 2014}, {"title": "V", "author": ["Spitkovsky"], "venue": "I.", "citeRegEx": "Spitkovsky 2013", "shortCiteRegEx": null, "year": 2013}, {"title": "W", "author": ["M.A. Tanner", "Wong"], "venue": "H.", "citeRegEx": "Tanner and Wong 1987", "shortCiteRegEx": null, "year": 1987}, {"title": "and Honavar", "author": ["K. Tu"], "venue": "V.", "citeRegEx": "Tu and Honavar 2012", "shortCiteRegEx": null, "year": 2012}, {"title": "Modified dirichlet distribution: Allowing negative parameters to induce stronger sparsity", "author": ["K. Tu 2016a] Tu"], "venue": "In Conference on Empirical Methods in Natural Language Processing (EMNLP", "citeRegEx": "Tu,? \\Q2016\\E", "shortCiteRegEx": "Tu", "year": 2016}, {"title": "Stochastic And-Or grammars: A unified framework and logic perspective", "author": ["K. Tu 2016b] Tu"], "venue": null, "citeRegEx": "Tu,? \\Q2016\\E", "shortCiteRegEx": "Tu", "year": 2016}, {"title": "W", "author": ["Tutte"], "venue": "T., ed.", "citeRegEx": "Tutte 1984", "shortCiteRegEx": null, "year": 1984}], "referenceMentions": [], "year": 2016, "abstractText": "Probabilistic modeling is one of the foundations of modern machine learning and artificial intelligence. In this paper, we propose a novel type of probabilistic models named latent dependency forest models (LDFMs). A LDFM models the dependencies between random variables with a forest structure that can change dynamically based on the variable values. It is therefore capable of modeling context-specific independence. We parameterize a LDFM using a first-order non-projective dependency grammar. Learning LDFMs from data can be formulated purely as a parameter learning problem, and hence the difficult problem of model structure learning is circumvented. Our experimental results show that LDFMs are competitive with existing probabilistic models.", "creator": "LaTeX with hyperref package"}}}