{"id": "1609.02612", "review": {"conference": "NIPS", "VERSION": "v1", "DATE_OF_SUBMISSION": "8-Sep-2016", "title": "Generating Videos with Scene Dynamics", "abstract": "We capitalize on large amounts of unlabeled video in order to learn a model of scene dynamics for both video recognition tasks (e.g. action classification) and video generation tasks (e.g. future prediction). We propose a generative adversarial network for video with a spatio-temporal convolutional architecture that untangles the scene's foreground from the background. Experiments suggest this model can generate tiny videos up to a second at full frame rate better than simple baselines, and we show its utility at predicting plausible futures of static images. Moreover, experiments and visualizations show the model internally learns useful features for recognizing actions with minimal supervision, suggesting scene dynamics are a promising signal for representation learning. We believe generative video models can impact many applications in video understanding and simulation.", "histories": [["v1", "Thu, 8 Sep 2016 22:29:52 GMT  (1926kb,D)", "http://arxiv.org/abs/1609.02612v1", "NIPS 2016. See more atthis http URL"], ["v2", "Mon, 17 Oct 2016 03:13:10 GMT  (1927kb,D)", "http://arxiv.org/abs/1609.02612v2", "NIPS 2016. See more atthis http URL"], ["v3", "Wed, 26 Oct 2016 13:58:10 GMT  (1927kb,D)", "http://arxiv.org/abs/1609.02612v3", "NIPS 2016. See more atthis http URL"]], "COMMENTS": "NIPS 2016. See more atthis http URL", "reviews": [], "SUBJECTS": "cs.CV cs.GR cs.LG", "authors": ["carl vondrick", "hamed pirsiavash", "antonio torralba 0001"], "accepted": true, "id": "1609.02612"}, "pdf": {"name": "1609.02612.pdf", "metadata": {"source": "CRF", "title": "Generating Videos with Scene Dynamics", "authors": ["Carl Vondrick", "Hamed Pirsiavash", "Antonio Torralba"], "emails": ["vondrick@mit.edu", "hpirsiav@umbc.edu", "torralba@mit.edu"], "sections": [{"heading": "1 Introduction", "text": "In fact, most people who are able to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move and to move."}, {"heading": "1.1 Related Work", "text": "This work builds on early work in generative video models. [26] Previous work, however, mainly focused on small patches and evaluated them for clustering videos. Here, we are developing a generative video model for natural scenes using state-of-the-art adversarial learning methods. [27, 11, 2, 6, 21] However, we are interested in creating short videos with realistic temporal semantics rather than recognizing or retrieving them. Our technical approach builds on recent work in generative adversarial networks for image modeling [8, 28, 44, 25] which we are extending to video."}, {"heading": "2 Generative Models for Video", "text": "In this section, we present a generative model for videos. We propose to use generative opposing networks [8], which have proven to be good at image generation [28, 4]."}, {"heading": "2.1 Review: Generative Adversarial Networks", "text": "The main idea behind generative hostile networks [8] is to train two networks: a generator network G tries to produce a video, and a discriminator network D tries to distinguish between \"real\" videos and \"fake\" generated videos. It is possible to train these networks against each other in a min / max game, in which the generator tries to deceive the discriminator as much as possible, while the discriminator simultaneously tries to detect which examples are fake: min wG max wDEx \u0445 px (x) [logD (x; wD)] + Ez \u0445 pz (z) [log (1 \u2212 D (G (z; wG; wG))))] (1), in which z is a latent \"code,\" which is often scanned from a simple distribution (such as a normal distribution) and x-px (x) samples from the data distribution. In practice, since we do not know the true distribution of data px (x), we can estimate the expectations by drawing IS from our networks (like a normal distribution) and we can sample the networks Gx from the data (x) on the basis of the Gx), and (x) we can calculate the GIS from the networks on the networks (x)."}, {"heading": "2.2 Generator Network", "text": "In most cases, this code can be scanned from a distribution (e.g. Gaussian). In view of a code z, we want to produce a video.We design the architecture of the generator network with a few principles in mind. First, we want the network to be invariant for translations into space and time. Second, we want a low-dimensional z to be able to produce a high-dimensional output (video). Third, we want to adopt a stationary camera and exploit the property that normally only objects move. We are interested in modeling object movements, not the motion of cameras. Furthermore, modeling the background is important for video detection tasks [41], it can be helpful in video creation. We explore two different network architectures: A stream architecture: We combine spatio-temporal constellations [13, 37] with broken volumes."}, {"heading": "2.3 Discriminator Network", "text": "The discriminator must be able to solve two problems: First, he must be able to classify realistic scenes from synthetically generated scenes, and second, he must be able to detect realistic movements between images. We designed the discriminator so that he can solve both of these tasks with the same model. We use a five-layer spatio-temporal winding network with cores 4 x 4 x 4 x 4, so that the hidden layers can learn both visual models and motion models. We design the architecture in such a way that it is opposed to the foreground current in the generator by replacing fractional stepwise cut waves with stepwise cut waves (for underpinning instead of superpinning) and replacing the last layer to output a binary classification (real or not)."}, {"heading": "2.4 Learning and Implementation", "text": "Our implementation is based on a modified version of [28] in Torch7. We used a numerically more stable implementation of cross-entropy loss to prevent overflow. We use the Adam [14] optimizer and a fixed learning rate of 0.0002 and a pulse term of 0.5. The latent code has 100 dimensions, which we extract from a normal distribution. We use a batch size of 64. We initialize all weights with zero mean Gaussian noise with a standard deviation of 0.01. We normalize all videos in the range [\u2212 1, 1]. We use batch normalization [10] followed by ReLU activation after each shift in the generator, except for the output layers that use Tanh. Following [28] we also use batch normalization [10], followed by the ReLU activation functions after each shift in the generator."}, {"heading": "3 Experiments", "text": "We experiment with the Generative Adversarial Network for Video (VGAN) for both generation and recognition tasks and show some qualitative examples online."}, {"heading": "3.1 Unlabeled Video Dataset", "text": "From this pool, we created two datasets: Unfiltered Unlabeled Videos: We use these videos directly, without filtering, to learn how to display them. The dataset spans over 5,000 hours. Filtered Unlabeled Videos: To evaluate generations, we use the pre-trained model Places2 [49] to automatically filter the videos by scene category. As image / video generation is a difficult problem, we compiled this dataset to better diagnose the strengths and weaknesses of approaches. We experimented with four scene categories: golf course, nursing rooms (babies), beaches and station.Stabilization: Since we are interested in the movement of objects rather than camera movement, we stabilize the camera motion for both datasets."}, {"heading": "3.2 Video Generation", "text": "We both believe that this project is a project, that it is a project, that it is a project, that it is a project, that it is a project, that it is first and foremost a project."}, {"heading": "3.3 Video Representation Learning", "text": "We are also experimenting with using our model as a way to learn unattended representations for video. However, we train our two-stream model with over 5,000 hours of unfiltered, unlabeled videos from Flickr. We then refine the discriminator to the task of interest (e.g. action detection) by using a relatively small set of labeled videos, replacing the last layer (which is a binary classifier) with a K-Way Softmax classifier. We also add dropout [33] to the penultimate layer to reduce overload. Action classification: We evaluated the performance in classifying actions on UCF101 [32]. We report accuracy in Figure 4a. Initializing the network with the weights learned from the generative opposing network results in a randomly initialized network, indicating that it has learned useful internal representation for video."}, {"heading": "3.4 Future Generation", "text": "We are investigating whether our approach can be used to generate the future of a static image."}], "references": [{"title": "Learning sound representations from unlabeled video", "author": ["Yusuf Aytar", "Carl Vondrick", "Antonio Torralba"], "venue": null, "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2016}, {"title": "Watching unlabeled video helps learn new human actions from very few labeled snapshots", "author": ["Chao-Yeh Chen", "Kristen Grauman"], "venue": "In CVPR,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2013}, {"title": "Deep generative image models using a laplacian pyramid of adversarial networks", "author": ["Emily L Denton", "Soumith Chintala", "Rob Fergus"], "venue": "In NIPS,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2015}, {"title": "Unsupervised visual representation learning by context prediction", "author": ["Carl Doersch", "Abhinav Gupta", "Alexei A Efros"], "venue": "In ICCV,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2015}, {"title": "Statistical learning of higher-order temporal structure from visual shape sequences", "author": ["J\u00f3zsef Fiser", "Richard N Aslin"], "venue": "JEP,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2002}, {"title": "Recurrent network models for human dynamics", "author": ["Katerina Fragkiadaki", "Sergey Levine", "Panna Felsen", "Jitendra Malik"], "venue": "In ICCV,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2015}, {"title": "Generative adversarial nets", "author": ["Ian Goodfellow", "Jean Pouget-Abadie", "Mehdi Mirza", "Bing Xu", "David Warde-Farley", "Sherjil Ozair", "Aaron Courville", "Yoshua Bengio"], "venue": "In NIPS,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2014}, {"title": "Dimensionality reduction by learning an invariant mapping", "author": ["Raia Hadsell", "Sumit Chopra", "Yann LeCun"], "venue": "In CVPR,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2006}, {"title": "Batch normalization: Accelerating deep network training by reducing internal covariate", "author": ["Sergey Ioffe", "Christian Szegedy"], "venue": "shift. arXiv,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2015}, {"title": "Discovering states and transformations in image collections", "author": ["Phillip Isola", "Joseph J Lim", "Edward H Adelson"], "venue": "In CVPR,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2015}, {"title": "Learning image representations tied to ego-motion", "author": ["Dinesh Jayaraman", "Kristen Grauman"], "venue": "In ICCV,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2015}, {"title": "3d convolutional neural networks for human action recognition", "author": ["Shuiwang Ji", "Wei Xu", "Ming Yang", "Kai Yu"], "venue": null, "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2013}, {"title": "Adam: A method for stochastic optimization", "author": ["Diederik Kingma", "Jimmy Ba"], "venue": null, "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2014}, {"title": "Building high-level features using large scale unsupervised learning", "author": ["Quoc V Le"], "venue": "CASSP,", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2013}, {"title": "Object recognition from local scale-invariant features", "author": ["David G Lowe"], "venue": "In ICCV,", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 1999}, {"title": "Deep multi-scale video prediction beyond mean square error", "author": ["Michael Mathieu", "Camille Couprie", "Yann LeCun"], "venue": null, "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2015}, {"title": "Conditional generative adversarial nets", "author": ["Mehdi Mirza", "Simon Osindero"], "venue": null, "citeRegEx": "20", "shortCiteRegEx": "20", "year": 2014}, {"title": "Unsupervised learning using sequential verification for action recognition", "author": ["Ishan Misra", "C Lawrence Zitnick", "Martial Hebert"], "venue": null, "citeRegEx": "21", "shortCiteRegEx": "21", "year": 2016}, {"title": "Deep learning from temporal coherence in video", "author": ["Hossein Mobahi", "Ronan Collobert", "Jason Weston"], "venue": "In ICML,", "citeRegEx": "22", "shortCiteRegEx": "22", "year": 2009}, {"title": "The open world of micro-videos", "author": ["Phuc Xuan Nguyen", "Gregory Rogez", "Charless Fowlkes", "Deva Ramanan"], "venue": null, "citeRegEx": "23", "shortCiteRegEx": "23", "year": 2016}, {"title": "Ambient sound provides supervision for visual learning", "author": ["Andrew Owens", "Jiajun Wu", "Josh H McDermott", "William T Freeman", "Antonio Torralba"], "venue": null, "citeRegEx": "24", "shortCiteRegEx": "24", "year": 2016}, {"title": "Context encoders: Feature learning by inpainting", "author": ["Deepak Pathak", "Philipp Krahenbuhl", "Jeff Donahue", "Trevor Darrell", "Alexei A Efros"], "venue": null, "citeRegEx": "25", "shortCiteRegEx": "25", "year": 2016}, {"title": "Recursive estimation of generative models of video", "author": ["Nikola Petrovic", "Aleksandar Ivanovic", "Nebojsa Jojic"], "venue": "In CVPR,", "citeRegEx": "26", "shortCiteRegEx": "26", "year": 2006}, {"title": "Seeing the arrow of time", "author": ["Lyndsey Pickup", "Zheng Pan", "Donglai Wei", "YiChang Shih", "Changshui Zhang", "Andrew Zisserman", "Bernhard Scholkopf", "William Freeman"], "venue": "In CVPR,", "citeRegEx": "27", "shortCiteRegEx": "27", "year": 2014}, {"title": "Unsupervised representation learning with deep convolutional generative adversarial networks", "author": ["Alec Radford", "Luke Metz", "Soumith Chintala"], "venue": null, "citeRegEx": "28", "shortCiteRegEx": "28", "year": 2015}, {"title": "Learning temporal embeddings for complex video analysis", "author": ["Vignesh Ramanathan", "Kevin Tang", "Greg Mori", "Li Fei-Fei"], "venue": "In CVPR,", "citeRegEx": "29", "shortCiteRegEx": "29", "year": 2015}, {"title": "Video (language) modeling: a baseline for generative models of natural videos", "author": ["MarcAurelio Ranzato", "Arthur Szlam", "Joan Bruna", "Michael Mathieu", "Ronan Collobert", "Sumit Chopra"], "venue": null, "citeRegEx": "30", "shortCiteRegEx": "30", "year": 2014}, {"title": "Two-stream convolutional networks for action recognition in videos", "author": ["Karen Simonyan", "Andrew Zisserman"], "venue": "In NIPS,", "citeRegEx": "31", "shortCiteRegEx": "31", "year": 2014}, {"title": "Ucf101: A dataset of 101 human actions classes from videos", "author": ["Khurram Soomro", "Amir Roshan Zamir", "Mubarak Shah"], "venue": "in the wild. arXiv,", "citeRegEx": "32", "shortCiteRegEx": "32", "year": 2012}, {"title": "Dropout: a simple way to prevent neural networks from overfitting", "author": ["Nitish Srivastava", "Geoffrey E Hinton", "Alex Krizhevsky", "Ilya Sutskever", "Ruslan Salakhutdinov"], "venue": null, "citeRegEx": "33", "shortCiteRegEx": "33", "year": 2014}, {"title": "Unsupervised learning of video representations using lstms", "author": ["Nitish Srivastava", "Elman Mansimov", "Ruslan Salakhutdinov"], "venue": null, "citeRegEx": "34", "shortCiteRegEx": "34", "year": 2015}, {"title": "A note on the evaluation of generative models", "author": ["Lucas Theis", "A\u00e4ron van den Oord", "Matthias Bethge"], "venue": null, "citeRegEx": "35", "shortCiteRegEx": "35", "year": 2015}, {"title": "Yfcc100m: The new data in multimedia research", "author": ["Bart Thomee", "David A Shamma", "Gerald Friedland", "Benjamin Elizalde", "Karl Ni", "Douglas Poland", "Damian Borth", "Li-Jia Li"], "venue": null, "citeRegEx": "36", "shortCiteRegEx": "36", "year": 2016}, {"title": "Learning spatiotemporal features with 3d convolutional networks", "author": ["Du Tran", "Lubomir Bourdev", "Rob Fergus", "Lorenzo Torresani", "Manohar Paluri"], "venue": null, "citeRegEx": "37", "shortCiteRegEx": "37", "year": 2014}, {"title": "Efficiently scaling up crowdsourced video annotation", "author": ["Carl Vondrick", "Donald Patterson", "Deva Ramanan"], "venue": null, "citeRegEx": "38", "shortCiteRegEx": "38", "year": 2013}, {"title": "Anticipating visual representations from unlabeled video", "author": ["Carl Vondrick", "Hamed Pirsiavash", "Antonio Torralba"], "venue": null, "citeRegEx": "39", "shortCiteRegEx": "39", "year": 2015}, {"title": "Patch to the future: Unsupervised visual prediction", "author": ["Jacob Walker", "Arpan Gupta", "Martial Hebert"], "venue": "In CVPR,", "citeRegEx": "40", "shortCiteRegEx": "40", "year": 2014}, {"title": "Action recognition with improved trajectories", "author": ["Heng Wang", "Cordelia Schmid"], "venue": "In ICCV,", "citeRegEx": "41", "shortCiteRegEx": "41", "year": 2013}, {"title": "Towards good practices for very deep two-stream convnets", "author": ["Limin Wang", "Yuanjun Xiong", "Zhe Wang", "Yu Qiao"], "venue": null, "citeRegEx": "42", "shortCiteRegEx": "42", "year": 2015}, {"title": "Unsupervised learning of visual representations using videos", "author": ["Xiaolong Wang", "Abhinav Gupta"], "venue": "In ICCV,", "citeRegEx": "43", "shortCiteRegEx": "43", "year": 2015}, {"title": "Generative image modeling using style and structure adversarial networks", "author": ["Xiaolong Wang", "Abhinav Gupta"], "venue": null, "citeRegEx": "44", "shortCiteRegEx": "44", "year": 2016}, {"title": "Empirical evaluation of rectified activations in convolutional network", "author": ["Bing Xu", "Naiyan Wang", "Tianqi Chen", "Mu Li"], "venue": null, "citeRegEx": "45", "shortCiteRegEx": "45", "year": 2015}, {"title": "A data-driven approach for event prediction", "author": ["Jenny Yuen", "Antonio Torralba"], "venue": "In ECCV", "citeRegEx": "46", "shortCiteRegEx": "46", "year": 2010}, {"title": "Object detectors emerge in deep scene cnns", "author": ["Bolei Zhou", "Aditya Khosla", "Agata Lapedriza", "Aude Oliva", "Antonio Torralba"], "venue": null, "citeRegEx": "48", "shortCiteRegEx": "48", "year": 2014}, {"title": "Learning deep features for scene recognition using places database", "author": ["Bolei Zhou", "Agata Lapedriza", "Jianxiong Xiao", "Antonio Torralba", "Aude Oliva"], "venue": "In NIPS,", "citeRegEx": "49", "shortCiteRegEx": "49", "year": 2014}, {"title": "Temporal perception and prediction in ego-centric video", "author": ["Yipin Zhou", "Tamara L Berg"], "venue": "In ICCV,", "citeRegEx": "50", "shortCiteRegEx": "50", "year": 2015}], "referenceMentions": [{"referenceID": 6, "context": "To do this, we capitalize on recent advances in generative adversarial networks [8, 28, 4], which we extend to video.", "startOffset": 80, "endOffset": 90}, {"referenceID": 24, "context": "To do this, we capitalize on recent advances in generative adversarial networks [8, 28, 4], which we extend to video.", "startOffset": 80, "endOffset": 90}, {"referenceID": 2, "context": "To do this, we capitalize on recent advances in generative adversarial networks [8, 28, 4], which we extend to video.", "startOffset": 80, "endOffset": 90}, {"referenceID": 22, "context": "This paper builds upon early work in generative video models [26].", "startOffset": 61, "endOffset": 65}, {"referenceID": 6, "context": "Here, we develop a generative video model for natural scenes using state-of-the-art adversarial learning methods [8, 28].", "startOffset": 113, "endOffset": 120}, {"referenceID": 24, "context": "Here, we develop a generative video model for natural scenes using state-of-the-art adversarial learning methods [8, 28].", "startOffset": 113, "endOffset": 120}, {"referenceID": 23, "context": "Conceptually, our work is related to studies into fundamental roles of time in computer vision [27, 11, 2, 6, 21].", "startOffset": 95, "endOffset": 113}, {"referenceID": 9, "context": "Conceptually, our work is related to studies into fundamental roles of time in computer vision [27, 11, 2, 6, 21].", "startOffset": 95, "endOffset": 113}, {"referenceID": 4, "context": "Conceptually, our work is related to studies into fundamental roles of time in computer vision [27, 11, 2, 6, 21].", "startOffset": 95, "endOffset": 113}, {"referenceID": 17, "context": "Conceptually, our work is related to studies into fundamental roles of time in computer vision [27, 11, 2, 6, 21].", "startOffset": 95, "endOffset": 113}, {"referenceID": 6, "context": "Our technical approach builds on recent work in generative adversarial networks for image modeling [8, 28, 4, 44, 25], which we extend to video.", "startOffset": 99, "endOffset": 117}, {"referenceID": 24, "context": "Our technical approach builds on recent work in generative adversarial networks for image modeling [8, 28, 4, 44, 25], which we extend to video.", "startOffset": 99, "endOffset": 117}, {"referenceID": 2, "context": "Our technical approach builds on recent work in generative adversarial networks for image modeling [8, 28, 4, 44, 25], which we extend to video.", "startOffset": 99, "endOffset": 117}, {"referenceID": 40, "context": "Our technical approach builds on recent work in generative adversarial networks for image modeling [8, 28, 4, 44, 25], which we extend to video.", "startOffset": 99, "endOffset": 117}, {"referenceID": 21, "context": "Our technical approach builds on recent work in generative adversarial networks for image modeling [8, 28, 4, 44, 25], which we extend to video.", "startOffset": 99, "endOffset": 117}, {"referenceID": 15, "context": "Most notably, [19] also uses adversarial networks for video frame prediction.", "startOffset": 14, "endOffset": 18}, {"referenceID": 26, "context": "Our work is also related to efforts to predict the future in video [30, 19, 40, 46, 39, 15, 7, 50].", "startOffset": 67, "endOffset": 98}, {"referenceID": 15, "context": "Our work is also related to efforts to predict the future in video [30, 19, 40, 46, 39, 15, 7, 50].", "startOffset": 67, "endOffset": 98}, {"referenceID": 36, "context": "Our work is also related to efforts to predict the future in video [30, 19, 40, 46, 39, 15, 7, 50].", "startOffset": 67, "endOffset": 98}, {"referenceID": 42, "context": "Our work is also related to efforts to predict the future in video [30, 19, 40, 46, 39, 15, 7, 50].", "startOffset": 67, "endOffset": 98}, {"referenceID": 35, "context": "Our work is also related to efforts to predict the future in video [30, 19, 40, 46, 39, 15, 7, 50].", "startOffset": 67, "endOffset": 98}, {"referenceID": 5, "context": "Our work is also related to efforts to predict the future in video [30, 19, 40, 46, 39, 15, 7, 50].", "startOffset": 67, "endOffset": 98}, {"referenceID": 45, "context": "Our work is also related to efforts to predict the future in video [30, 19, 40, 46, 39, 15, 7, 50].", "startOffset": 67, "endOffset": 98}, {"referenceID": 33, "context": "We use spatio-temporal 3D convolutions to model videos [37], but we use fractionally strided convolutions [47] instead because we are interested in generation.", "startOffset": 55, "endOffset": 59}, {"referenceID": 27, "context": "We also use two-streams to model video [31], but apply them for video generation instead of action recognition.", "startOffset": 39, "endOffset": 43}, {"referenceID": 13, "context": "Finally, this paper is related to a growing body of work that capitalizes on large amounts of unlabeled video for visual recognition tasks [16, 43, 34, 12, 21, 22, 3, 29, 23, 24, 17, 38, 39, 1].", "startOffset": 139, "endOffset": 193}, {"referenceID": 39, "context": "Finally, this paper is related to a growing body of work that capitalizes on large amounts of unlabeled video for visual recognition tasks [16, 43, 34, 12, 21, 22, 3, 29, 23, 24, 17, 38, 39, 1].", "startOffset": 139, "endOffset": 193}, {"referenceID": 30, "context": "Finally, this paper is related to a growing body of work that capitalizes on large amounts of unlabeled video for visual recognition tasks [16, 43, 34, 12, 21, 22, 3, 29, 23, 24, 17, 38, 39, 1].", "startOffset": 139, "endOffset": 193}, {"referenceID": 10, "context": "Finally, this paper is related to a growing body of work that capitalizes on large amounts of unlabeled video for visual recognition tasks [16, 43, 34, 12, 21, 22, 3, 29, 23, 24, 17, 38, 39, 1].", "startOffset": 139, "endOffset": 193}, {"referenceID": 17, "context": "Finally, this paper is related to a growing body of work that capitalizes on large amounts of unlabeled video for visual recognition tasks [16, 43, 34, 12, 21, 22, 3, 29, 23, 24, 17, 38, 39, 1].", "startOffset": 139, "endOffset": 193}, {"referenceID": 18, "context": "Finally, this paper is related to a growing body of work that capitalizes on large amounts of unlabeled video for visual recognition tasks [16, 43, 34, 12, 21, 22, 3, 29, 23, 24, 17, 38, 39, 1].", "startOffset": 139, "endOffset": 193}, {"referenceID": 1, "context": "Finally, this paper is related to a growing body of work that capitalizes on large amounts of unlabeled video for visual recognition tasks [16, 43, 34, 12, 21, 22, 3, 29, 23, 24, 17, 38, 39, 1].", "startOffset": 139, "endOffset": 193}, {"referenceID": 25, "context": "Finally, this paper is related to a growing body of work that capitalizes on large amounts of unlabeled video for visual recognition tasks [16, 43, 34, 12, 21, 22, 3, 29, 23, 24, 17, 38, 39, 1].", "startOffset": 139, "endOffset": 193}, {"referenceID": 19, "context": "Finally, this paper is related to a growing body of work that capitalizes on large amounts of unlabeled video for visual recognition tasks [16, 43, 34, 12, 21, 22, 3, 29, 23, 24, 17, 38, 39, 1].", "startOffset": 139, "endOffset": 193}, {"referenceID": 20, "context": "Finally, this paper is related to a growing body of work that capitalizes on large amounts of unlabeled video for visual recognition tasks [16, 43, 34, 12, 21, 22, 3, 29, 23, 24, 17, 38, 39, 1].", "startOffset": 139, "endOffset": 193}, {"referenceID": 34, "context": "Finally, this paper is related to a growing body of work that capitalizes on large amounts of unlabeled video for visual recognition tasks [16, 43, 34, 12, 21, 22, 3, 29, 23, 24, 17, 38, 39, 1].", "startOffset": 139, "endOffset": 193}, {"referenceID": 35, "context": "Finally, this paper is related to a growing body of work that capitalizes on large amounts of unlabeled video for visual recognition tasks [16, 43, 34, 12, 21, 22, 3, 29, 23, 24, 17, 38, 39, 1].", "startOffset": 139, "endOffset": 193}, {"referenceID": 0, "context": "Finally, this paper is related to a growing body of work that capitalizes on large amounts of unlabeled video for visual recognition tasks [16, 43, 34, 12, 21, 22, 3, 29, 23, 24, 17, 38, 39, 1].", "startOffset": 139, "endOffset": 193}, {"referenceID": 6, "context": "We propose to use generative adversarial networks [8], which have been shown to have good performance on image generation [28, 4].", "startOffset": 50, "endOffset": 53}, {"referenceID": 24, "context": "We propose to use generative adversarial networks [8], which have been shown to have good performance on image generation [28, 4].", "startOffset": 122, "endOffset": 129}, {"referenceID": 2, "context": "We propose to use generative adversarial networks [8], which have been shown to have good performance on image generation [28, 4].", "startOffset": 122, "endOffset": 129}, {"referenceID": 6, "context": "The main idea behind generative adversarial networks [8] is to train two networks: a generator network G tries to produce a video, and a discriminator network D tries to distinguish between \u201creal\u201c videos and \u201cfake\u201d generated videos.", "startOffset": 53, "endOffset": 56}, {"referenceID": 37, "context": "Moreover, since modeling that the background is stationary is important in video recognition tasks [41], it may be helpful in video generation as well.", "startOffset": 99, "endOffset": 103}, {"referenceID": 11, "context": "One Stream Architecture: We combine spatio-temporal convolutions [13, 37] with fractionally strided convolutions [47, 28] to generate video.", "startOffset": 65, "endOffset": 73}, {"referenceID": 33, "context": "One Stream Architecture: We combine spatio-temporal convolutions [13, 37] with fractionally strided convolutions [47, 28] to generate video.", "startOffset": 65, "endOffset": 73}, {"referenceID": 24, "context": "One Stream Architecture: We combine spatio-temporal convolutions [13, 37] with fractionally strided convolutions [47, 28] to generate video.", "startOffset": 113, "endOffset": 121}, {"referenceID": 24, "context": "We use an architecture inspired by [28], except extended in time.", "startOffset": 35, "endOffset": 39}, {"referenceID": 24, "context": "For f(z), we use the same network as the one-stream architecture, and for b(z) we use a similar generator architecture to [28].", "startOffset": 122, "endOffset": 126}, {"referenceID": 24, "context": "Our implementation is based off a modified version of [28] in Torch7.", "startOffset": 54, "endOffset": 58}, {"referenceID": 12, "context": "We use the Adam [14] optimizer and a fixed learning rate of 0.", "startOffset": 16, "endOffset": 20}, {"referenceID": 8, "context": "We use batch normalization [10] followed by the ReLU activation functions after every layer in the generator, except the output layers, which uses tanh.", "startOffset": 27, "endOffset": 31}, {"referenceID": 24, "context": "Following [28], we also use batch normalization in the discriminator except for the first layer and we instead use leaky ReLU [45].", "startOffset": 10, "endOffset": 14}, {"referenceID": 41, "context": "Following [28], we also use batch normalization in the discriminator except for the first layer and we instead use leaky ReLU [45].", "startOffset": 126, "endOffset": 130}, {"referenceID": 32, "context": "We downloaded over two million videos from Flickr [36] by querying for popular Flickr tags as well as querying for common English words.", "startOffset": 50, "endOffset": 54}, {"referenceID": 44, "context": "Filtered Unlabeled Videos: To evaluate generations, we use the Places2 pre-trained model [49] to automatically filter the videos by scene category.", "startOffset": 89, "endOffset": 93}, {"referenceID": 14, "context": "We extract SIFT keypoints [18], use RANSAC to estimate a homography (rotation, translation, scale) between adjacent frames, and warp frames to minimize background motion.", "startOffset": 26, "endOffset": 30}, {"referenceID": 26, "context": "Baseline: Since to our knowledge there are no existing large-scale generative models of video ([30] requires an input frame), we develop a simple but reasonable baseline for this task.", "startOffset": 95, "endOffset": 99}, {"referenceID": 31, "context": "We designed this experiment following advice from [35], which advocates evaluating generative models for the task at hand.", "startOffset": 50, "endOffset": 54}, {"referenceID": 29, "context": "We also add dropout [33] to the penultimate layer to reduce overfitting.", "startOffset": 20, "endOffset": 24}, {"referenceID": 28, "context": "Action Classification: We evaluated performance on classifying actions on UCF101 [32].", "startOffset": 81, "endOffset": 85}, {"referenceID": 28, "context": "Interestingly, while a randomly initialized network under-performs hand-crafted STIP features [32], the network initialized with our model significantly", "startOffset": 94, "endOffset": 98}, {"referenceID": 28, "context": "9% STIP Features [32] 43.", "startOffset": 17, "endOffset": 21}, {"referenceID": 7, "context": "9% Temporal Coherence [9] 45.", "startOffset": 22, "endOffset": 25}, {"referenceID": 17, "context": "4% Shuffle and Learn [21] 50.", "startOffset": 21, "endOffset": 25}, {"referenceID": 38, "context": "1% ImageNet Supervision [42] 91.", "startOffset": 24, "endOffset": 28}, {"referenceID": 28, "context": "Figure 4: Video Representation Learning: We evaluate the representation learned by the discriminator for action classification on UCF101 [32].", "startOffset": 137, "endOffset": 141}, {"referenceID": 17, "context": "Moreover, our model slightly outperforms another unsupervised video representation [21] despite using an order of magnitude fewer learned parameters and only 64 \u00d7 64 videos.", "startOffset": 83, "endOffset": 87}, {"referenceID": 17, "context": "Finally, our model slightly outperforms another recent unsupervised video representation learning approach [21].", "startOffset": 107, "endOffset": 111}, {"referenceID": 16, "context": "We can do this by attaching a fivelayer convolutional network to the front of the generator which encodes the image into the latent space, similar to a conditional generative adversarial network [20].", "startOffset": 195, "endOffset": 199}, {"referenceID": 43, "context": "(a) hidden unit that fires on \u201cperson\u201d (b) hidden unit that fires on \u201ctrain tracks\u201d Figure 6: Visualizing Representation: We visualize some hidden units in the encoder of the future generator, following the technique from [48].", "startOffset": 222, "endOffset": 226}, {"referenceID": 26, "context": "[30, 19] can generate video, but they require multiple input frames and empirically become blurry after extrapolating many frames.", "startOffset": 0, "endOffset": 8}, {"referenceID": 15, "context": "[30, 19] can generate video, but they require multiple input frames and empirically become blurry after extrapolating many frames.", "startOffset": 0, "endOffset": 8}, {"referenceID": 36, "context": "[40, 46] can predict optic flow from a single image, but they do not generate several frames of motion and may be susceptible to warping artifacts.", "startOffset": 0, "endOffset": 8}, {"referenceID": 42, "context": "[40, 46] can predict optic flow from a single image, but they do not generate several frames of motion and may be susceptible to warping artifacts.", "startOffset": 0, "endOffset": 8}, {"referenceID": 20, "context": "These visualizations suggest that scaling up future generation might be a promising supervisory signal for object recognition and complementary to [24, 5, 43].", "startOffset": 147, "endOffset": 158}, {"referenceID": 3, "context": "These visualizations suggest that scaling up future generation might be a promising supervisory signal for object recognition and complementary to [24, 5, 43].", "startOffset": 147, "endOffset": 158}, {"referenceID": 39, "context": "These visualizations suggest that scaling up future generation might be a promising supervisory signal for object recognition and complementary to [24, 5, 43].", "startOffset": 147, "endOffset": 158}], "year": 2016, "abstractText": "We capitalize on large amounts of unlabeled video in order to learn a model of scene dynamics for both video recognition tasks (e.g. action classification) and video generation tasks (e.g. future prediction). We propose a generative adversarial network for video with a spatio-temporal convolutional architecture that untangles the scene\u2019s foreground from the background. Experiments suggest this model can generate tiny videos up to a second at full frame rate better than simple baselines, and we show its utility at predicting plausible futures of static images. Moreover, experiments and visualizations show the model internally learns useful features for recognizing actions with minimal supervision, suggesting scene dynamics are a promising signal for representation learning. We believe generative video models can impact many applications in video understanding and simulation.", "creator": "LaTeX with hyperref package"}}}