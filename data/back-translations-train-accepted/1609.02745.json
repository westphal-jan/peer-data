{"id": "1609.02745", "review": {"conference": "EMNLP", "VERSION": "v1", "DATE_OF_SUBMISSION": "9-Sep-2016", "title": "A Hierarchical Model of Reviews for Aspect-based Sentiment Analysis", "abstract": "Opinion mining from customer reviews has become pervasive in recent years. Sentences in reviews, however, are usually classified independently, even though they form part of a review's argumentative structure. Intuitively, sentences in a review build and elaborate upon each other; knowledge of the review structure and sentential context should thus inform the classification of each sentence. We demonstrate this hypothesis for the task of aspect-based sentiment analysis by modeling the interdependencies of sentences in a review with a hierarchical bidirectional LSTM. We show that the hierarchical model outperforms two non-hierarchical baselines, obtains results competitive with the state-of-the-art, and outperforms the state-of-the-art on five multilingual, multi-domain datasets without any hand-engineered features or external resources.", "histories": [["v1", "Fri, 9 Sep 2016 11:16:15 GMT  (912kb,D)", "http://arxiv.org/abs/1609.02745v1", "To be published at EMNLP 2016, 7 pages"]], "COMMENTS": "To be published at EMNLP 2016, 7 pages", "reviews": [], "SUBJECTS": "cs.CL cs.LG", "authors": ["sebastian ruder", "parsa ghaffari", "john g breslin"], "accepted": true, "id": "1609.02745"}, "pdf": {"name": "1609.02745.pdf", "metadata": {"source": "CRF", "title": "A Hierarchical Model of Reviews for Aspect-based Sentiment Analysis", "authors": ["Sebastian Ruder", "Parsa Ghaffari", "John G. Breslin"], "emails": ["sebastian.ruder@insight-centre.org", "john.breslin@insight-centre.org", "sebastian@aylien.com", "parsa@aylien.com"], "sections": [{"heading": null, "text": "Opinion mining from customer reviews has become ubiquitous in recent years. However, sentences in reviews are usually independently classified even though they are part of the argumentative structure of a review. Intuitively, sentences in a review build on each other and deepen each other; knowledge of the review structure and sentence context should therefore determine the classification of each sentence. We demonstrate this hypothesis for the task of aspect-based sentiment analysis by modelling the interdependence of sentences in a review using a hierarchical bi-directional LSTM. We show that the hierarchical model outperforms two non-hierarchical baselines, achieves results that compete with the state of the art, and outperforms the state of the art in five multilingual, multi-domain datasets without any manual features or external resources."}, {"heading": "1 Introduction", "text": "Sentiment Analysis (Pang and Lee, 2008) is used to assess public opinion regarding products, analyze customer satisfaction, and identify trends. As customer reviews become more prevalent, the finer-grained aspect-based sentiment analysis (ABSA) has gained popularity because it allows for more detailed analysis of aspects of a product or service. Reviews - simply with any coherent text - have an underlying structure. Visualization of the discourse structure according to Rhetorical Structure Theory (RST) (Mann and Thompson, 1988) for the example summary in Figure 1 shows that sentences and clauses related to different rhetorical relationships are related, such as elaboration and background. Intuitively, knowledge of the relationships and the feeling of surrounding judgments should shape the mood of the current judgement. If a restaurant reviewer has shown a positive sentiment toward the quality of the food, it is likely that his opinion is not based drastically on the course of the judgments or negative judgments in the most recent reviews or judgments in the hierarchical judgments."}, {"heading": "2 Related Work", "text": "The Zhang and Lan model (2015) is the only approach we know that takes more than one sentence into account, but it is less meaningful than ours as it only extracts characteristics from the previous and subsequent sentence without any idea of structure. Neural network-based approaches include an LSTM that determines the mood toward a target word based on its position (Tang et al., 2015) and a recursive neural network that needs to parse trees (Nguyen and Shirai, 2015). In contrast, our model requires no feature engineering, no positional information, and no parser output, which is often not available for resource-archaic languages."}, {"heading": "3 Model", "text": "In the following, we will present the various components of our hierarchical bidirectional LSTM architecture shown in Figure 2."}, {"heading": "3.1 Sentence and Aspect Representation", "text": "Each review consists of sentences supplemented to length l by the insertion of padding symbols. Each review, in turn, is supplemented to length h by the insertion of sentences containing only padding symbols. We present each sentence as a concatenation of its word embeddings x1: l, where xt-Rk is the k-dimensional vector of the t-th word in the sentence. Each sentence is associated with an aspect. Aspects consist of an entity and an attribute, e.g. FOOD # QUALITY. Similar to the entity representation of Socher et al. (2013), we represent each aspect a as an average of its entity and attribute embeddings 12 (xe + xa), with xe, xa-Rm being the m-dimensional entity and attribute embeddings respectively 1."}, {"heading": "3.2 LSTM", "text": "We use a Long Short-Term Memory (LSTM) (Hochreiter and Schmidhuber, 1997) that adds input, output, and oblivion grids to a recurring cell, allowing it to model long-term dependencies that are essential for capturing emotions. For the t-th word in a sentence, the LSTM input takes the word embed xt, the previous output ht \u2212 1, and the cell state ct \u2212 1, and computes the next output and cell state ct. Both h and c are initialized with zeros."}, {"heading": "3.3 Bidirectional LSTM", "text": "A bi-directional LSTM (Bi-LSTM) (Graves et al., 2013) allows us to look to the future by using a forward directed LSTM that processes the sequence in chronological order, and a backward directed LSTM that processes the sequence in reverse order. Output at a given time step is then the concatenation of the corresponding states of the forward and backward directed LSTM.1The embedding resulted in slightly better results than the use of a separate embedding for each aspect."}, {"heading": "3.4 Hierarchical Bidirectional LSTM", "text": "Stacking a Bi-LSTM on the verification level on the record level Bi-LSTM results in the hierarchical bi-directional LSTM (H-LSTM) in Figure 2. Forward and backward LSTMs receive the record starting with the first or last word embedding x1 or xl, respectively. The final output hl of both LSTMs is then linked to the aspect vector a2 and fed as input into the assessment level forward and backwards. The outputs of both LSTMs are concatenated and fed into a final Softmax layer, which outputs a probability distribution over sentiments3 for each sentence."}, {"heading": "4 Experiments", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "4.1 Datasets", "text": "For our experiments, we looked at data sets in five areas (restaurants, hotels, laptops, telephones, cam-2We experimented with other interactions, such as recalculating word embeddings by their similarity to aspects, an attention-like mechanism, as well as summation and multiplication, but found that simple concatenation yields the best results.3The sentiment classes are positive, negative and neutral.eras) and eight languages (English, Spanish, French, Russian, Dutch, Turkish, Arabic, Chinese) from the most recent aspect-based sentiment analysis task (Pontiki et al., 2016) using the provided pull / test splits. In total, there are 11 domain-language data sets with 300-400 ratings for 1250-6000 sentences4. Each sentence is commented with no, one or more domain-specific aspects and a sentiment value for each aspect."}, {"heading": "4.2 Training Details", "text": "Our LSTMs have a layer and output size of 200 dimensions. We use 300-dimensional word embeddings. We use pre-formed GloVe (Pennington et al., 2014) embeddings for English, while we train embedders on frWaC5 for French and on Leipzig Corpora Collection6 for all other languages.7 Entity4Exact dataset statistics can be seen in (Pontiki et al., 2016). 5http: / / wacky.sslmit.unibo.it / doku.php? id = corpora 6http: / / corpora2.informatik.uni-leipzig. de / download.html 7Using 64-dimensional polyglot embeddings (Al-Rfou et al., 2013) generally yielded worse performance. And attribute embeddings of aspects have 15 dimensions and are initialized sequentas."}, {"heading": "4.3 Comparison models", "text": "We compare our model using random (H-LSTM) and pre-trained word embedding (HP-LSTM) with the best model of the SemEval 2016 Aspectbased Sentiment Analysis task (Pontiki et al., 2016) for each domain language pair (Best) and with the two best single models in the competition: IIT-TUDA (Kumar et al., 2016), which uses large sentiment lexicon for each language, and XRCE (Brun et al., 2016), which uses a Parser aug-8Labeling with a NONE aspect and predicts a neutral, slightly reduced performance. Supplemented with handmade, domain-specific rules. To determine that the hierarchical nature of our model is the deciding factor, we additionally compare it with the convolutionary neural network at the sentence level by Ruder et al. (2016) (CNN) and with a Bi-LSTM (STM), which is identical to our first model 9."}, {"heading": "5 Results and Discussion", "text": "We present our results in Table 1. Taking into account the structure of verification, our hierarchical model achieves better results than the sentence level CNN and the sentence level Bi-LSTM baselines for almost all domain language pairs. We highlight examples in which this improves the predictions in Table 2. In addition, our model shows results that compete with the best single models of the competition, while it does not require expensive handmade features or external resources, demonstrating its language and domain independence. Overall, our model compares favorably with the state of the art, especially for languages with limited resources, where few handmade features are available. It exceeds the state of the art in four and five sets of data, using randomly initialized and pre-trained embeddings. To ensure that the additional parameters do not take into account the difference, we increase the number of layers and dimensions of LSTM, which does not affect the results."}, {"heading": "5.1 Pre-trained embeddings", "text": "In line with previous research findings (Collobert et al., 2011), we observe significant progress in initializing our word vectors through pre-trained embeddings in almost all languages. Pre-trained embeddings improve the performance of our model for all languages except Russian, Arabic, and Chinese, and help keep it state-of-the-art in Dutch telephones. We publish our pre-trained multilingual embeddings so that they facilitate future research in the field of multilingual mood analysis and text classification10."}, {"heading": "5.2 Leveraging additional information", "text": "Since annotations are expensive in many real-world applications, it is important to learn from only a few examples. Our model was designed with this goal in mind and is able to extract additional information inherent in the training data. By using the structure of the report, our model is able to inform and improve its sentiment predictions, as shown in Table 2. However, the large performance differences from the state of the art for the Turkish dataset, in which only 1104 records are available for training, and the performance gaps for resource-intensive languages such as English, Spanish, and French show the limitations of an approach such as ours, which only uses data available during training time. While the use of pre-trained word embeddings is a 10https: / / s3.amazonaws.com / aylien-main / data / index.htmleffective way to mitigate this deficit, for high-resource languages, the sole use of uncontrolled language information is a 10https: / / s3.amazonaws.com / aylien-main / data / index.htmleffective way to improve the use of uncontrolled language information in 2016, not sufficient to cope with large-scale approaches."}, {"heading": "6 Conclusion", "text": "In this paper, we have presented a hierarchical assessment model for aspect-based sentiment analysis. We show that by taking into account the structure of the review and the sentential context for its predictions, the model is able to outperform models based only on sentence information, and achieve performance that is competitive with models that utilize large external resources and handcrafted features. Our model achieves state-of-the-art results for aspect-based sentiment analysis on 5 out of 11 datasets."}, {"heading": "Acknowledgments", "text": "We thank the anonymous reviewers Nicolas P\u00e9cheux and Hugo Larochelle for their constructive feedback. This publication is the result of research funded by the Irish Research Council (IRC) under grant number EBPPG / 2014 / 30 and with Aylien Ltd. as Enterprise Partner, as well as research supported by a Science Foundation Ireland (SFI) grant number SFI / 12 / RC / 2289.11."}], "references": [{"title": "Polyglot: Distributed Word Representations for Multilingual NLP", "author": ["Rami Al-Rfou", "Bryan Perozzi", "Steven Skiena."], "venue": "Proceedings of the Seventeenth Conference on Computational Natural Language Learning, pages 183\u2013192.", "citeRegEx": "Al.Rfou et al\\.,? 2013", "shortCiteRegEx": "Al.Rfou et al\\.", "year": 2013}, {"title": "XRCE at SemEval-2016 Task 5: Feedbacked Ensemble Modelling on Syntactico-Semantic Knowledge for Aspect Based Sentiment Analysis", "author": ["Caroline Brun", "Julien Perez", "Claude Roux."], "venue": "Proceedings of the 10th International Workshop on Semantic Evaluation", "citeRegEx": "Brun et al\\.,? 2016", "shortCiteRegEx": "Brun et al\\.", "year": 2016}, {"title": "Natural Language Processing (almost) from Scratch", "author": ["Ronan Collobert", "Jason Weston", "Leon Bottou", "Michael Karlen", "Koray Kavukcuoglu", "Pavel Kuksa."], "venue": "Journal of Machine Learning Research, 12(Aug):2493\u20132537.", "citeRegEx": "Collobert et al\\.,? 2011", "shortCiteRegEx": "Collobert et al\\.", "year": 2011}, {"title": "Extraction of Salient Sentences from Labelled Documents", "author": ["Misha Denil", "Alban Demiraj", "Nando de Freitas."], "venue": "arXiv preprint arXiv:1412.6815, pages 1\u20139.", "citeRegEx": "Denil et al\\.,? 2014", "shortCiteRegEx": "Denil et al\\.", "year": 2014}, {"title": "Speech Recognition with Deep Recurrent Neural Networks", "author": ["Alex Graves", "Abdel-rahman Mohamed", "Geoffrey Hinton."], "venue": "IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), (3):6645\u20136649.", "citeRegEx": "Graves et al\\.,? 2013", "shortCiteRegEx": "Graves et al\\.", "year": 2013}, {"title": "Inducing Domain-Specific Sentiment Lexicons from Unlabeled Corpora", "author": ["William L. Hamilton", "Kevin Clark", "Jure Leskovec", "Dan Jurafsky."], "venue": "Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics.", "citeRegEx": "Hamilton et al\\.,? 2016", "shortCiteRegEx": "Hamilton et al\\.", "year": 2016}, {"title": "Long Short-Term Memory", "author": ["Sepp Hochreiter", "J\u00fcrgen Schmidhuber."], "venue": "Neural Computation, 9(8):1735\u20131780.", "citeRegEx": "Hochreiter and Schmidhuber.,? 1997", "shortCiteRegEx": "Hochreiter and Schmidhuber.", "year": 1997}, {"title": "Harnessing Deep Neural Networks with Logic Rules", "author": ["Zhiting Hu", "Xuezhe Ma", "Zhengzhong Liu", "Eduard Hovy", "Eric Xing."], "venue": "Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics, pages 1\u201318.", "citeRegEx": "Hu et al\\.,? 2016", "shortCiteRegEx": "Hu et al\\.", "year": 2016}, {"title": "Adam: a Method for Stochastic Optimization", "author": ["Diederik P. Kingma", "Jimmy Lei Ba."], "venue": "International Conference on Learning Representations, pages 1\u201313.", "citeRegEx": "Kingma and Ba.,? 2015", "shortCiteRegEx": "Kingma and Ba.", "year": 2015}, {"title": "From Group to Individual Labels using Deep Features", "author": ["Dimitrios Kotzias", "Misha Denil", "Nando de Freitas", "Padhraic Smyth."], "venue": "Proceedings of the 21th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, pages 597\u2014-606.", "citeRegEx": "Kotzias et al\\.,? 2015", "shortCiteRegEx": "Kotzias et al\\.", "year": 2015}, {"title": "IIT-TUDA at SemEval2016 Task 5: Beyond Sentiment Lexicon: Combining Domain Dependency and Distributional Semantics Features for Aspect Based Sentiment Analysis", "author": ["Ayush Kumar", "Sarah Kohail", "Amit Kumar", "Asif Ekbal", "Chris Biemann."], "venue": "Pro-", "citeRegEx": "Kumar et al\\.,? 2016", "shortCiteRegEx": "Kumar et al\\.", "year": 2016}, {"title": "Sequential Short-Text Classification with Recurrent and Convolutional Neural Networks", "author": ["Ji Young Lee", "Franck Dernoncourt."], "venue": "Proceedings of NAACL-HLT 2016.", "citeRegEx": "Lee and Dernoncourt.,? 2016", "shortCiteRegEx": "Lee and Dernoncourt.", "year": 2016}, {"title": "A Hierarchical Neural Autoencoder for Paragraphs and Documents", "author": ["Jiwei Li", "Minh-Thang Luong", "Daniel Jurafsky."], "venue": "Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics, pages 1106\u20131115.", "citeRegEx": "Li et al\\.,? 2015", "shortCiteRegEx": "Li et al\\.", "year": 2015}, {"title": "Rhetorical Structure Theory: Toward a functional theory of text organization", "author": ["William C. Mann", "Sandra A. Thompson"], "venue": null, "citeRegEx": "Mann and Thompson.,? \\Q1988\\E", "shortCiteRegEx": "Mann and Thompson.", "year": 1988}, {"title": "PhraseRNN: Phrase Recursive Neural Network for Aspect-based Sentiment Analysis", "author": ["Thien Hai Nguyen", "Kiyoaki Shirai."], "venue": "Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, (September):2509\u20132514.", "citeRegEx": "Nguyen and Shirai.,? 2015", "shortCiteRegEx": "Nguyen and Shirai.", "year": 2015}, {"title": "Opinion Mining and Sentiment Analysis", "author": ["Bo Pang", "Lillian Lee."], "venue": "Foundations and trends in information retrieval, 2(1-2):1\u2013135.", "citeRegEx": "Pang and Lee.,? 2008", "shortCiteRegEx": "Pang and Lee.", "year": 2008}, {"title": "Glove: Global Vectors for Word Representation", "author": ["Jeffrey Pennington", "Richard Socher", "Christopher D. Manning."], "venue": "Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing, pages 1532\u20131543.", "citeRegEx": "Pennington et al\\.,? 2014", "shortCiteRegEx": "Pennington et al\\.", "year": 2014}, {"title": "SemEval-2014 Task 4: Aspect Based Sentiment Analysis", "author": ["Maria Pontiki", "Dimitrios Galanis", "John Pavlopoulos", "Haris Papageorgiou", "Ion Androutsopoulos", "Suresh Manandhar."], "venue": "Proceedings of the 8th International Workshop on Semantic Evaluation (Se-", "citeRegEx": "Pontiki et al\\.,? 2014", "shortCiteRegEx": "Pontiki et al\\.", "year": 2014}, {"title": "SemEval-2015 Task 12: Aspect Based Sentiment Analysis", "author": ["Maria Pontiki", "Dimitris Galanis", "Haris Papageorgiou", "Suresh Manandhar", "Ion Androutsopoulos."], "venue": "Proceedings of the 9th International Workshop on Semantic Evaluation (SemEval 2015), pages", "citeRegEx": "Pontiki et al\\.,? 2015", "shortCiteRegEx": "Pontiki et al\\.", "year": 2015}, {"title": "SemEval-2016 Task 5: Aspect-Based Sentiment Analysis", "author": ["Evgeny Kotelnikov", "Nuria Bel", "Salud Mar\u00eda Jim\u00e9nezZafra", "G\u00fcl\u015fen Eryi\u011fit."], "venue": "Proceedings of the 10th International Workshop on Semantic Evaluation, San Diego, California. Association for Com-", "citeRegEx": "Kotelnikov et al\\.,? 2016", "shortCiteRegEx": "Kotelnikov et al\\.", "year": 2016}, {"title": "INSIGHT-1 at SemEval-2016 Task 5: Deep Learning for Multilingual Aspect-based Sentiment Analysis", "author": ["Sebastian Ruder", "Parsa Ghaffari", "John G. Breslin."], "venue": "Proceedings of the 10th International Workshop on Semantic Evaluation (SemEval 2016).", "citeRegEx": "Ruder et al\\.,? 2016", "shortCiteRegEx": "Ruder et al\\.", "year": 2016}, {"title": "A Hierarchical Latent Variable Encoder-Decoder Model for Generating Dialogues", "author": ["Iulian Vlad Serban", "Alessandro Sordoni", "Ryan Lowe", "Laurent Charlin", "Joelle Pineau", "Aaron Courville", "Yoshua Bengio."], "venue": "Proceedings of the Advances in Neural Information", "citeRegEx": "Serban et al\\.,? 2016", "shortCiteRegEx": "Serban et al\\.", "year": 2016}, {"title": "UNITN: Training Deep Convolutional Neural Network for Twitter Sentiment Classification", "author": ["Aliaksei Severyn", "Alessandro Moschitti."], "venue": "Proceedings of the 9th International Workshop on Semantic Evaluation (SemEval 2015), pages 464\u2013469.", "citeRegEx": "Severyn and Moschitti.,? 2015", "shortCiteRegEx": "Severyn and Moschitti.", "year": 2015}, {"title": "Reasoning With Neural Tensor Networks for Knowledge Base Completion", "author": ["Richard Socher", "Danqi Chen", "Christopher D. Manning", "Andrew Y. Ng."], "venue": "Proceedings of the Advances in Neural Information Processing Systems 26 (NIPS 2013), pages 1\u201310.", "citeRegEx": "Socher et al\\.,? 2013", "shortCiteRegEx": "Socher et al\\.", "year": 2013}, {"title": "Sequence to sequence learning with neural networks", "author": ["Ilya Sutskever", "Oriol Vinyals", "Quoc V. Le."], "venue": "Advances in Neural Information Processing Systems, page 9.", "citeRegEx": "Sutskever et al\\.,? 2014", "shortCiteRegEx": "Sutskever et al\\.", "year": 2014}, {"title": "Target-Dependent Sentiment Classification with Long Short Term Memory", "author": ["Duyu Tang", "Bing Qin", "Xiaocheng Feng", "Ting Liu."], "venue": "arXiv preprint arXiv:1512.01100.", "citeRegEx": "Tang et al\\.,? 2015", "shortCiteRegEx": "Tang et al\\.", "year": 2015}, {"title": "Target-Dependent Twitter Sentiment Classification with Rich Automatic Features", "author": ["Duy-tin Vo", "Yue Zhang."], "venue": "IJCAI International Joint Conference on Artificial Intelligence, pages 1347\u20131353.", "citeRegEx": "Vo and Zhang.,? 2015", "shortCiteRegEx": "Vo and Zhang.", "year": 2015}, {"title": "ECNU: Extracting Effective Features from Multiple Sequential Sentences for Target-dependent Sentiment Analysis in Reviews", "author": ["Zhihua Zhang", "Man Lan."], "venue": "Proceedings of the 9th International Workshop on Semantic Evaluation (SemEval 2015), pages 736\u2013741.", "citeRegEx": "Zhang and Lan.,? 2015", "shortCiteRegEx": "Zhang and Lan.", "year": 2015}], "referenceMentions": [{"referenceID": 15, "context": "Sentiment analysis (Pang and Lee, 2008) is used to gauge public opinion towards products, to analyze customer satisfaction, and to detect trends.", "startOffset": 19, "endOffset": 39}, {"referenceID": 13, "context": "A visualization of the discourse structure according to Rhetorical Structure Theory (RST) (Mann and Thompson, 1988) for the example review in Figure 1 reveals that sentences Elaboration Background", "startOffset": 90, "endOffset": 115}, {"referenceID": 22, "context": "Neural network-based architectures that have recently become popular for sentiment analysis and ABSA, such as convolutional neural networks (Severyn and Moschitti, 2015), LSTMs (Vo and Zhang, 2015), and recursive neural networks (Nguyen and Shirai, 2015), however, are only able to consider intra-sentence relations such as Background in Figure 1 and fail to capture inter-sentence relations, e.", "startOffset": 140, "endOffset": 169}, {"referenceID": 26, "context": "Neural network-based architectures that have recently become popular for sentiment analysis and ABSA, such as convolutional neural networks (Severyn and Moschitti, 2015), LSTMs (Vo and Zhang, 2015), and recursive neural networks (Nguyen and Shirai, 2015), however, are only able to consider intra-sentence relations such as Background in Figure 1 and fail to capture inter-sentence relations, e.", "startOffset": 177, "endOffset": 197}, {"referenceID": 14, "context": "Neural network-based architectures that have recently become popular for sentiment analysis and ABSA, such as convolutional neural networks (Severyn and Moschitti, 2015), LSTMs (Vo and Zhang, 2015), and recursive neural networks (Nguyen and Shirai, 2015), however, are only able to consider intra-sentence relations such as Background in Figure 1 and fail to capture inter-sentence relations, e.", "startOffset": 229, "endOffset": 254}, {"referenceID": 17, "context": "Past approaches use classifiers with expensive hand-crafted features based on n-grams, parts-of-speech, negation words, and sentiment lexica (Pontiki et al., 2014; Pontiki et al., 2015).", "startOffset": 141, "endOffset": 185}, {"referenceID": 18, "context": "Past approaches use classifiers with expensive hand-crafted features based on n-grams, parts-of-speech, negation words, and sentiment lexica (Pontiki et al., 2014; Pontiki et al., 2015).", "startOffset": 141, "endOffset": 185}, {"referenceID": 25, "context": "Neural network-based approaches include an LSTM that determines sentiment towards a target word based on its position (Tang et al., 2015) as well as a recursive neural network that requires parse trees (Nguyen and Shirai, 2015).", "startOffset": 118, "endOffset": 137}, {"referenceID": 14, "context": ", 2015) as well as a recursive neural network that requires parse trees (Nguyen and Shirai, 2015).", "startOffset": 72, "endOffset": 97}, {"referenceID": 16, "context": "Past approaches use classifiers with expensive hand-crafted features based on n-grams, parts-of-speech, negation words, and sentiment lexica (Pontiki et al., 2014; Pontiki et al., 2015). The model by Zhang and Lan (2015) is the only approach we are aware of that considers more than one sentence.", "startOffset": 142, "endOffset": 221}, {"referenceID": 9, "context": "Hierarchical models have been used predominantly for representation learning and generation of paragraphs and documents: Li et al. (2015) use a hierarchical LSTM-based autoencoder to reconstruct reviews and paragraphs of Wikipedia articles.", "startOffset": 121, "endOffset": 138}, {"referenceID": 9, "context": "Hierarchical models have been used predominantly for representation learning and generation of paragraphs and documents: Li et al. (2015) use a hierarchical LSTM-based autoencoder to reconstruct reviews and paragraphs of Wikipedia articles. Serban et al. (2016) use a hierarchical recurrent encoder-decoder with latent variables for dialogue generation.", "startOffset": 121, "endOffset": 262}, {"referenceID": 3, "context": "Denil et al. (2014) use a hierarchical ConvNet to extract salient sentences from reviews, while Kotzias et al.", "startOffset": 0, "endOffset": 20}, {"referenceID": 3, "context": "Denil et al. (2014) use a hierarchical ConvNet to extract salient sentences from reviews, while Kotzias et al. (2015) use the same architecture to learn sentence-level labels from review-level labels using a novel cost function.", "startOffset": 0, "endOffset": 118}, {"referenceID": 3, "context": "Denil et al. (2014) use a hierarchical ConvNet to extract salient sentences from reviews, while Kotzias et al. (2015) use the same architecture to learn sentence-level labels from review-level labels using a novel cost function. The model of Lee and Dernoncourt (2016) is perhaps the most similar to ours.", "startOffset": 0, "endOffset": 269}, {"referenceID": 23, "context": "Similarly to the entity representation of Socher et al. (2013), we represent every aspect a as the average of its entity and attribute embeddings 12(xe + xa) where xe, xa \u2208 R m are the m-dimensional entity and attribute embeddings respectively1.", "startOffset": 42, "endOffset": 63}, {"referenceID": 6, "context": "We use a Long Short-Term Memory (LSTM) (Hochreiter and Schmidhuber, 1997), which adds input, output, and forget gates to a recurrent cell, which allow it to model long-range dependencies that are essential for capturing sentiment.", "startOffset": 39, "endOffset": 73}, {"referenceID": 4, "context": "A Bidirectional LSTM (Bi-LSTM) (Graves et al., 2013) allows us to look ahead by employing a forward LSTM, which processes the sequence in chronological order, and a backward LSTM, which processes the sequence in reverse order.", "startOffset": 31, "endOffset": 52}, {"referenceID": 16, "context": "We use pre-trained GloVe (Pennington et al., 2014) embeddings for English, while we train embeddings on frWaC5 for French and on the Leipzig Corpora Collection6 for all other languages.", "startOffset": 25, "endOffset": 50}, {"referenceID": 0, "context": "html Using 64-dimensional Polyglot embeddings (Al-Rfou et al., 2013) yielded generally worse performance.", "startOffset": 46, "endOffset": 68}, {"referenceID": 24, "context": "We remove sentences with no aspect8 and ignore predictions for all sentences that have been added as padding to a review so as not to force our model to learn meaningless predictions, as is commonly done in sequenceto-sequence learning (Sutskever et al., 2014).", "startOffset": 236, "endOffset": 260}, {"referenceID": 8, "context": "We train our model to minimize the cross-entropy loss, using stochastic gradient descent, the Adam update rule (Kingma and Ba, 2015), mini-batches of size 10, and early stopping with a patience of 10.", "startOffset": 111, "endOffset": 132}, {"referenceID": 10, "context": ", 2016) for each domain-language pair (Best) as well as against the two best single models of the competition: IIT-TUDA (Kumar et al., 2016), which uses large sentiment lexicons for every language, and XRCE (Brun et al.", "startOffset": 120, "endOffset": 140}, {"referenceID": 1, "context": ", 2016), which uses large sentiment lexicons for every language, and XRCE (Brun et al., 2016), which uses a parser aug-", "startOffset": 74, "endOffset": 93}, {"referenceID": 20, "context": "In order to ascertain that the hierarchical nature of our model is the deciding factor, we additionally compare against the sentence-level convolutional neural network of Ruder et al. (2016) (CNN) and against a sentence-level Bi-LSTM (LSTM), which is identical to the first layer of our model.", "startOffset": 171, "endOffset": 191}, {"referenceID": 2, "context": "In line with past research (Collobert et al., 2011), we observe significant gains when initializing our word vectors with pre-trained embeddings across almost all languages.", "startOffset": 27, "endOffset": 51}, {"referenceID": 10, "context": "html effective way to mitigate this deficit, for highresource languages, solely leveraging unsupervised language information is not enough to perform onpar with approaches that make use of large external resources (Kumar et al., 2016) and meticulously hand-crafted features (Brun et al.", "startOffset": 214, "endOffset": 234}, {"referenceID": 1, "context": ", 2016) and meticulously hand-crafted features (Brun et al., 2016).", "startOffset": 47, "endOffset": 66}, {"referenceID": 5, "context": "In light of the diversity of domains in the context of aspect-based sentiment analysis and many other applications, domain-specific lexicons (Hamilton et al., 2016) are often preferred.", "startOffset": 141, "endOffset": 164}, {"referenceID": 7, "context": "by constraining them with rules (Hu et al., 2016) is thus an important research avenue, which we leave for future work.", "startOffset": 32, "endOffset": 49}, {"referenceID": 8, "context": "We experimented with using sentiment lexicons by Kumar et al. (2016) but were not able to significantly improve upon our results with pre-trained embeddings11.", "startOffset": 49, "endOffset": 69}, {"referenceID": 26, "context": "We tried bucketing and embedding of sentiment scores as well as filtering and pooling as in (Vo and Zhang, 2015)", "startOffset": 92, "endOffset": 112}], "year": 2016, "abstractText": "Opinion mining from customer reviews has become pervasive in recent years. Sentences in reviews, however, are usually classified independently, even though they form part of a review\u2019s argumentative structure. Intuitively, sentences in a review build and elaborate upon each other; knowledge of the review structure and sentential context should thus inform the classification of each sentence. We demonstrate this hypothesis for the task of aspect-based sentiment analysis by modeling the interdependencies of sentences in a review with a hierarchical bidirectional LSTM. We show that the hierarchical model outperforms two non-hierarchical baselines, obtains results competitive with the state-of-the-art, and outperforms the state-of-the-art on five multilingual, multi-domain datasets without any handengineered features or external resources.", "creator": "LaTeX with hyperref package"}}}