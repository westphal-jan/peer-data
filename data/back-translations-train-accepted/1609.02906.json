{"id": "1609.02906", "review": {"conference": "NIPS", "VERSION": "v1", "DATE_OF_SUBMISSION": "9-Sep-2016", "title": "Robust Spectral Detection of Global Structures in the Data by Learning a Regularization", "abstract": "Spectral methods are popular in detecting global structures in the given data that can be represented as a matrix. However when the data matrix is sparse or noisy, classic spectral methods usually fail to work, due to localization of eigenvectors (or singular vectors) induced by the sparsity or noise. In this work, we propose a general method to solve the localization problem by learning a regularization matrix from the localized eigenvectors. Using matrix perturbation analysis, we demonstrate that the learned regularizations suppress down the eigenvalues associated with localized eigenvectors and enable us to recover the informative eigenvectors representing the global structure. We show applications of our method in several inference problems: community detection in networks, clustering from pairwise similarities, rank estimation and matrix completion problems. Using extensive experiments, we illustrate that our method solves the localization problem and works down to the theoretical detectability limits in different kinds of synthetic data. This is in contrast with existing spectral algorithms based on data matrix, non-backtracking matrix, Laplacians and those with rank-one regularizations, which perform poorly in the sparse case with noise.", "histories": [["v1", "Fri, 9 Sep 2016 19:48:29 GMT  (5430kb,D)", "http://arxiv.org/abs/1609.02906v1", "13 pages, 9 figures, Neural Information Processing Systems 2016"]], "COMMENTS": "13 pages, 9 figures, Neural Information Processing Systems 2016", "reviews": [], "SUBJECTS": "stat.ML cs.LG cs.SI physics.soc-ph", "authors": ["pan zhang"], "accepted": true, "id": "1609.02906"}, "pdf": {"name": "1609.02906.pdf", "metadata": {"source": "CRF", "title": "Robust Spectral Detection of Global Structures in the Data by Learning a Regularization", "authors": ["Pan Zhang"], "emails": ["panzhang@itp.ac.cn"], "sections": [{"heading": "1 Introduction", "text": "The task is usually difficult to solve, as modern datasets usually have a large dimensionality. However, if the dataset can be represented as a matrix, spectral methods are more popular, as they provide a natural way to reduce the dimensionality of data using eigenvectors or singular vectors. However, in many situations where we do not have enough measurements, the data matrix is economical, i.e. the standard spectral methods do not usually work well. An example is community detection in sparse networks, where the task leads to partition nodes in groups where there are many edges, nodes within the same group and comparatively few edge connections in different groups."}, {"heading": "2 Regularization as a unified framework", "text": "We see that the above three methods for detecting the community problem in sparse graphs, i.e. trimming, non-backtracking / Bethe Hessian, and rank-one regularizations, can be understood as performing different types of regularizations. In this context, we consider a regularized matrix L = A + R. \"(1) Here matrix A\" is the data matrix or its (symmetrical) variance, such as A \"D \u2212 1 / 2AD \u2212 1 / 2 with D, which denotes the eigenmatrix of degrees, and matrix R\" is a regulation matrix or its (symmetrical) variance, such as A \"D \u2212 1 / 2AD \u2212 2 with D, which denotes the eigenmatrix of degrees, and the matrix R\" is a regulation matrix. \"(2, 11, 16-18, 23] falls naturally within this framework, since they make R a Rangeins introductory matrix, we see a controllable matrix, where T \u2212 11x is a feasible regulation."}, {"heading": "3 Learning regularizations from localized eigenvectors", "text": "The reason why informative eigenvectors are hidden in mass is that some random eigenvectors have large eigenvalues due to the localization that represents the local structures of the system. On the complementary side, if these eigenvectors are not localized, they should have smaller eigenvalues than the informative eigenvalues that reveal the global structures of the graph. This is the main assumption that our idea is based on eigenvalues. In this thesis, we use the Inverse Participation Ratio (IPR), I (v) = 1 v 4 i to quantify the amount of localization of a (normalized) eigenvector."}, {"heading": "4 Numerical evaluations", "text": "In this section, we confirm our approach with experiments on various inference problems, such as community detection problems, clusters of sparse paired entries, rank estimation, and matrix completion from a few entries. We will compare the performance of spectral algorithms using the X-Laplacian with recently proposed state-of-the-art spectral methods in the sparse regime."}, {"heading": "4.1 Community Detection", "text": "In fact, it is the case that one is able to find a solution that is capable of finding a solution that is capable of finding a solution, and that is able to find a solution that is capable of finding a solution that is capable of finding a solution, that is able to find a solution that is capable of finding a solution, that is able to find a solution that is capable of finding a solution, that is able to find a solution that is capable of finding a solution, that is able to find a solution that is capable of finding a solution. \""}, {"heading": "4.2 Clustering from sparse pairwise measurements", "text": "This year it is more than ever before in the history of the city."}, {"heading": "4.3 Rank estimation and Matrix Completion", "text": "The last problem we consider in this work to evaluate the X-Laplacian is the completion of a low ranking matrix of a few entries. This problem has many applications, including the famous collaborative filtering. One problem closely related to it is the ranking of the revealed entries. In fact, estimating the property of the matrix is usually the first step before the matrix is completed. The problem is defined as follows: Leave Atrue = UV T, where we select a matrix Rn \u00b7 r and V-Rm \u00b7 r to obtain the matrix autonomy. Few, say c-mn, interventions of the matrix Atrue, which we include a matrix Rn \u00b7 r and V-Rm, which contains a subset of Atrue, with other elements that are zero. Many algorithms have been proposed for matrix completion, including the nuclear norm minimization [5 and based on the definitions]."}, {"heading": "5 Conclusion and discussion", "text": "We introduced the X-Laplacian, a general approach to detecting latent global structures in a given data matrix. It is a completely data-driven approach that learns different forms of regulation for different data to solve the problem of localizing eigenvectors or singular vectors. The mechanics of de-localizing eigenvectors during the learning of regulations was illustrated by matrix disturbance analysis. We validated our method using extensive numerical experiments and showed that it exceeds state-of-the-art algorithms on various inference problems in the sparse regime and with errors. In this paper we discuss the X-Laplacian directly using data matrix A, but this is not the only choice. In fact, we tested approaches that use different variants of A, such as A, and also found theoretical work."}, {"heading": "6 Appendix", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "6.1 Perturbation analysis", "text": "If we assume that matrix LX is not poorly conditioned, and the first eigenvectors that are important to us are different, then we have (LX + L) (Ui + U) (Ui + U) (Ui + U) (Ui + U). (4) Since LX is a real symmetrical matrix, we can use LXui = eigenvector Iui, and hold only the first order terms, then we have haveL (Ui + LX). (4) Since LX is a real symmetrical matrix, we can use as a weighted sum of eigenvectors of LX, asu (n)."}, {"heading": "6.2 Detailed process of learning a regularization", "text": "Fig. 6.2 shows the evolution of eigenvalues, overlaps and inverse participation ratio (IPR) for the second, third and fourth eigenvectors during the learning of the X-Laplacian for a network generated by the stochastic block model. The network has a community structure with 3 groups, with the first three eigenvectors of the adjacence matrix located (see left panel at t = 0) and does not show the underlying community structure (see right panel at t small. We can also see from the left panel that the IPR decreases from them as learning progresses. From the middle panel of the figure we see that all 3 eigenvalues decrease, while the spectral gap D3 \u2212 D4 increases during learning. Interestingly, at t = 4 there is an exchange of the positions of the third eigenvector and the fourth eigenvector, resulting in a bump in the IPR and an increase in recognition accuracy (characterized by overlapping at 4)."}, {"heading": "6.3 Additional numerical evaluations on community detections", "text": "In fact, it is the case that it will be able to implement the erroneous orders, which will be able to reform the erroneous orders."}, {"heading": "6.4 Additional numerical evaluations on spectral clustering using pairwise similarity measurements", "text": "In this section, we compare the performance of spectral algorithms using the data matrix, the Bethe-Hessian and the X-Laplacian, using the model recently proposed in [25], which generates pairs of measurements between two groups of nodes from different probability distributions. Two distribution pins and the pout are selected as Gaussian with unit variance, meaning 0.75 and \u2212 0.75 respectively. At the top of the network, we add two different types of noise, i.e. cliques and hubs to the topology of the random chart. And from the illustrations, we can see that the results are qualitatively similar to the right panel of Figure 4 in the main text, where X-Laplaker outperforms both Bethe-Hessian and X-Laplacian in the reconstruction of the planted partition."}], "references": [{"title": "The political blogosphere and the 2004 us election: divided they blog", "author": ["L.A. Adamic", "N. Glance"], "venue": "Proceedings of the 3rd international workshop on Link discovery, pages 36\u201343. ACM", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2005}, {"title": "et al", "author": ["A.A. Amini", "A. Chen", "P.J. Bickel", "E. Levina"], "venue": "Pseudo-likelihood methods for community detection in large sparse networks. The Annals of Statistics, 41(4):2097\u20132122", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2013}, {"title": "Atomic vibrations in vitreous silica", "author": ["R. Bell", "P. Dean"], "venue": "Discussions of the Faraday society, 50:55\u201361", "citeRegEx": "3", "shortCiteRegEx": null, "year": 1970}, {"title": "A singular value thresholding algorithm for matrix completion", "author": ["J.-F. Cai", "E.J. Cand\u00e8s", "Z. Shen"], "venue": "SIAM Journal on Optimization, 20(4):1956\u20131982", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2010}, {"title": "Exact matrix completion via convex optimization", "author": ["E.J. Cand\u00e8s", "B. Recht"], "venue": "Foundations of Computational mathematics, 9(6):717\u2013772", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2009}, {"title": "Graph partitioning via adaptive spectral techniques", "author": ["A. COJA-OGHLAN"], "venue": "Combinatorics, Probability and Computing, 19:227\u2013284,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2010}, {"title": "Asymptotic analysis of the stochastic block model for modular networks and its algorithmic applications", "author": ["A. Decelle", "F. Krzakala", "C. Moore", "L. Zdeborov\u00e1"], "venue": "Phys. Rev. E,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2011}, {"title": "Zeta functions of finite graphs and representations of p-adic groups", "author": ["K.-i. Hashimoto"], "venue": "Advanced Studies in Pure Mathematics,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 1989}, {"title": "Stochastic blockmodels: First steps", "author": ["P.W. Holland", "K.B. Laskey", "S. Leinhardt"], "venue": "Social networks, 5(2):109\u2013137", "citeRegEx": "9", "shortCiteRegEx": null, "year": 1983}, {"title": "Phase transitions in semidefinite relaxations", "author": ["A. Javanmard", "A. Montanari", "F. Ricci-Tersenghi"], "venue": "Proceedings of the National Academy of Sciences, 113(16):E2218", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2016}, {"title": "Impact of regularization on spectral clustering", "author": ["A. Joseph", "B. Yu"], "venue": "arXiv preprint arXiv:1312.1733", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2013}, {"title": "Stochastic blockmodels and community structure in networks", "author": ["B. Karrer", "M.E.J. Newman"], "venue": "Phys. Rev. E,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2011}, {"title": "Low-rank matrix completion with noisy observations: a quantitative comparison", "author": ["R.H. Keshavan", "A. Montanari", "S. Oh"], "venue": "Communication, Control, and Computing, 2009. Allerton 2009. 47th Annual Allerton Conference on, pages 1216\u20131222. IEEE", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2009}, {"title": "Matrix completion from a few entries", "author": ["R.H. Keshavan", "S. Oh", "A. Montanari"], "venue": "Information Theory, 2009. ISIT 2009. IEEE International Symposium on, pages 324\u2013328. IEEE", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2009}, {"title": "Spectral redemption in clustering sparse networks", "author": ["F. Krzakala", "C. Moore", "E. Mossel", "J. Neeman", "A. Sly", "L. Zdeborov\u00e1", "P. Zhang"], "venue": "Proc. Natl. Acad. Sci. USA, 110(52):20935\u201320940", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2013}, {"title": "Sparse random graphs: regularization and concentration of the laplacian", "author": ["C.M. Le", "E. Levina", "R. Vershynin"], "venue": "arXiv preprint arXiv:1502.03049", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2015}, {"title": "Concentration and regularization of random graphs", "author": ["C.M. Le", "R. Vershynin"], "venue": "arXiv preprint arXiv:1506.00669", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2015}, {"title": "et al", "author": ["J. Lei", "A. Rinaldo"], "venue": "Consistency of spectral clustering in stochastic block models. The Annals of Statistics, 43(1):215\u2013237", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2014}, {"title": "and Pertinence", "author": ["U.V. Luxburg", "M. Belkin", "O. Bousquet"], "venue": "A tutorial on spectral clustering. Stat. Comput", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2007}, {"title": "Community detection thresholds and the weak ramanujan property", "author": ["L. Massouli\u00e9"], "venue": "Proceedings of the 46th Annual ACM Symposium on Theory of Computing, pages 694\u2013703. ACM", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2014}, {"title": "Stochastic block models and reconstruction", "author": ["E. Mossel", "J. Neeman", "A. Sly"], "venue": "arXiv preprint arXiv:1202.1499", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2012}, {"title": "et al", "author": ["A.Y. Ng", "M.I. Jordan", "Y. Weiss"], "venue": "On spectral clustering: Analysis and an algorithm. Advances in neural information processing systems, 2:849\u2013856", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2002}, {"title": "Regularized spectral clustering under the degree-corrected stochastic blockmodel", "author": ["T. Qin", "K. Rohe"], "venue": "Advances in Neural Information Processing Systems, pages 3120\u20133128", "citeRegEx": "23", "shortCiteRegEx": null, "year": 2013}, {"title": "Spectral clustering of graphs with the bethe hessian", "author": ["A. Saade", "F. Krzakala", "L. Zdeborov\u00e1"], "venue": "Advances in Neural Information Processing Systems, pages 406\u2013414", "citeRegEx": "24", "shortCiteRegEx": null, "year": 2014}, {"title": "Matrix completion from fewer entries: Spectral detectability and rank estimation", "author": ["A. Saade", "F. Krzakala", "L. Zdeborov\u00e1"], "venue": "C. Cortes, N. Lawrence, D. Lee, M. Sugiyama, and R. Garnett, editors, Advances in Neural Information Processing Systems 28, pages 1261\u20131269. Curran Associates, Inc.", "citeRegEx": "25", "shortCiteRegEx": null, "year": 2015}, {"title": "Clustering from sparse pairwise measurements", "author": ["A. Saade", "M. Lelarge", "F. Krzakala", "L. Zdeborov\u00e1"], "venue": "arXiv preprint arXiv:1601.06683", "citeRegEx": "26", "shortCiteRegEx": null, "year": 2016}, {"title": "Triangular stochastic block model", "author": ["P. Zhang"], "venue": "unpublished", "citeRegEx": "28", "shortCiteRegEx": null, "year": 2016}], "referenceMentions": [{"referenceID": 18, "context": "Other standard matrices for spectral clustering [19, 22], e.", "startOffset": 48, "endOffset": 56}, {"referenceID": 21, "context": "Other standard matrices for spectral clustering [19, 22], e.", "startOffset": 48, "endOffset": 56}, {"referenceID": 5, "context": "A simple way to ease the pain of localization induced by high degree or weight is trimming [6, 13] which sets to zero columns or rows with a large degree or weight.", "startOffset": 91, "endOffset": 98}, {"referenceID": 12, "context": "A simple way to ease the pain of localization induced by high degree or weight is trimming [6, 13] which sets to zero columns or rows with a large degree or weight.", "startOffset": 91, "endOffset": 98}, {"referenceID": 5, "context": "However trimming throws away part of the information, thus does not work all the way down to the theoretical limit in the community detection problem [6, 15].", "startOffset": 150, "endOffset": 157}, {"referenceID": 14, "context": "However trimming throws away part of the information, thus does not work all the way down to the theoretical limit in the community detection problem [6, 15].", "startOffset": 150, "endOffset": 157}, {"referenceID": 24, "context": "It also performs worse than other methods in matrix completion problem [25].", "startOffset": 71, "endOffset": 75}, {"referenceID": 14, "context": "One kind of methods use new linear operators related to the belief propagation and Bethe free energy, such as the nonbacktracking matrix [15] and Bethe Hessian [24].", "startOffset": 137, "endOffset": 141}, {"referenceID": 23, "context": "One kind of methods use new linear operators related to the belief propagation and Bethe free energy, such as the nonbacktracking matrix [15] and Bethe Hessian [24].", "startOffset": 160, "endOffset": 164}, {"referenceID": 1, "context": "Another kind of methods add to the data matrix or its variance a rank-one regularization matrix [2, 11, 16\u201318, 23].", "startOffset": 96, "endOffset": 114}, {"referenceID": 10, "context": "Another kind of methods add to the data matrix or its variance a rank-one regularization matrix [2, 11, 16\u201318, 23].", "startOffset": 96, "endOffset": 114}, {"referenceID": 15, "context": "Another kind of methods add to the data matrix or its variance a rank-one regularization matrix [2, 11, 16\u201318, 23].", "startOffset": 96, "endOffset": 114}, {"referenceID": 16, "context": "Another kind of methods add to the data matrix or its variance a rank-one regularization matrix [2, 11, 16\u201318, 23].", "startOffset": 96, "endOffset": 114}, {"referenceID": 17, "context": "Another kind of methods add to the data matrix or its variance a rank-one regularization matrix [2, 11, 16\u201318, 23].", "startOffset": 96, "endOffset": 114}, {"referenceID": 22, "context": "Another kind of methods add to the data matrix or its variance a rank-one regularization matrix [2, 11, 16\u201318, 23].", "startOffset": 96, "endOffset": 114}, {"referenceID": 9, "context": "Moreover its performance is sensitive to the noise in the data [10].", "startOffset": 63, "endOffset": 67}, {"referenceID": 1, "context": "The rank-one regularization approaches [2, 11, 16\u201318, 23] fall naturally into this framework as they set R to be a rank-one matrix, \u2212\u03b611 , with \u03b6 being a tunable parameter controlling strength of regularizations.", "startOffset": 39, "endOffset": 57}, {"referenceID": 10, "context": "The rank-one regularization approaches [2, 11, 16\u201318, 23] fall naturally into this framework as they set R to be a rank-one matrix, \u2212\u03b611 , with \u03b6 being a tunable parameter controlling strength of regularizations.", "startOffset": 39, "endOffset": 57}, {"referenceID": 15, "context": "The rank-one regularization approaches [2, 11, 16\u201318, 23] fall naturally into this framework as they set R to be a rank-one matrix, \u2212\u03b611 , with \u03b6 being a tunable parameter controlling strength of regularizations.", "startOffset": 39, "endOffset": 57}, {"referenceID": 16, "context": "The rank-one regularization approaches [2, 11, 16\u201318, 23] fall naturally into this framework as they set R to be a rank-one matrix, \u2212\u03b611 , with \u03b6 being a tunable parameter controlling strength of regularizations.", "startOffset": 39, "endOffset": 57}, {"referenceID": 17, "context": "The rank-one regularization approaches [2, 11, 16\u201318, 23] fall naturally into this framework as they set R to be a rank-one matrix, \u2212\u03b611 , with \u03b6 being a tunable parameter controlling strength of regularizations.", "startOffset": 39, "endOffset": 57}, {"referenceID": 22, "context": "The rank-one regularization approaches [2, 11, 16\u201318, 23] fall naturally into this framework as they set R to be a rank-one matrix, \u2212\u03b611 , with \u03b6 being a tunable parameter controlling strength of regularizations.", "startOffset": 39, "endOffset": 57}, {"referenceID": 7, "context": "However we can link them using the theory of graph zeta function [8] which says that an eigenvalue \u03bc of the non-backtracking operator satisfies the following quadratic eigenvalue equation,", "startOffset": 65, "endOffset": 68}, {"referenceID": 23, "context": "Actually a range of parameters work well in practice, like those estimated from the spin-glass transition of the system [24].", "startOffset": 120, "endOffset": 124}, {"referenceID": 2, "context": "IPR has been used frequently in physics, for example for distinguishing the extended state from the localized state when applied on the wave function [3].", "startOffset": 150, "endOffset": 153}, {"referenceID": 8, "context": "First we use synthetic networks generated by the stochastic block model [9], and its variant with noise [10].", "startOffset": 72, "endOffset": 75}, {"referenceID": 9, "context": "First we use synthetic networks generated by the stochastic block model [9], and its variant with noise [10].", "startOffset": 104, "endOffset": 108}, {"referenceID": 6, "context": "Given the average degree of the graph, there is a so-called detectability transition \u2217 = cout/cin = ( \u221a c \u2212 1)/( \u221a c \u2212 1 + q) [7] , beyond which point it is not possible to obtain any information about the planted partition.", "startOffset": 126, "endOffset": 129}, {"referenceID": 14, "context": "It is also known spectral algorithms based on the non-backtracking matrix succeed all the way down to the transition [15].", "startOffset": 117, "endOffset": 121}, {"referenceID": 19, "context": "This transition was recently established rigorously in the case of q = 2 [20, 21].", "startOffset": 73, "endOffset": 81}, {"referenceID": 20, "context": "This transition was recently established rigorously in the case of q = 2 [20, 21].", "startOffset": 73, "endOffset": 81}, {"referenceID": 23, "context": "In addition to the non-backtracking matrix, X-Laplacian, and the adjacency matrix, we put into comparison the results obtained using other classic and newly proposed matrices, including Bethe Hessian [24], Normalized Laplacian (N.", "startOffset": 200, "endOffset": 204}, {"referenceID": 9, "context": "We have tested other kinds of noisy models, including the noisy stochastic block model, as proposed in [10].", "startOffset": 103, "endOffset": 107}, {"referenceID": 9, "context": "Our results show that the X-Laplacian works well (see appendices) while all other spectral methods do not work at all on this dataset [10].", "startOffset": 134, "endOffset": 138}, {"referenceID": 11, "context": "Moreover, in addition to the classic stochastic block model, we have extensively evaluated our method on networks generated by the degree-corrected stochastic block model [12], and the stochastic block model with extensive triangles.", "startOffset": 171, "endOffset": 175}, {"referenceID": 0, "context": "For example on the political blogs network [1], spectral clustering using the adjacency matrix gives 83 mis-classified labels among totally 1222 labels, while the X-Laplacian gives only 50 mis-classified labels.", "startOffset": 43, "endOffset": 46}, {"referenceID": 25, "context": "In this section we use the generative model recently proposed in [26], since there is a theoretical limit that can be used to evaluate algorithms.", "startOffset": 65, "endOffset": 69}, {"referenceID": 25, "context": "The model in [26] first assigns items hidden clusters {ti} \u2208 {1, 2}, then generates similarity between a randomly sampled pairs of items according to probability distribution,", "startOffset": 13, "endOffset": 17}, {"referenceID": 25, "context": "spectral clustering using the Bethe Hessian [26], achieve partial recovery of the planted clusters.", "startOffset": 44, "endOffset": 48}, {"referenceID": 25, "context": "The model used to generate pairwise measurements is proposed in [26], see text for detailed descriptions.", "startOffset": 64, "endOffset": 68}, {"referenceID": 4, "context": "Many algorithms have been proposed for matrix completion, including nuclear norm minimization [5] and methods based on the singular value decomposition [4] etc.", "startOffset": 94, "endOffset": 97}, {"referenceID": 3, "context": "Many algorithms have been proposed for matrix completion, including nuclear norm minimization [5] and methods based on the singular value decomposition [4] etc.", "startOffset": 152, "endOffset": 155}, {"referenceID": 13, "context": "singular values [14].", "startOffset": 16, "endOffset": 20}, {"referenceID": 24, "context": "Indeed in [25] authors reported that their approach based on the Bethe Hessian outperforms trimming+SVD when the topology of revealed entries is a sparse random graph.", "startOffset": 10, "endOffset": 14}, {"referenceID": 24, "context": "Moreover, authors in [25] show that the number of negative eigenvalues of the Bethe Hessian gives a more accurate estimate of the rank of A than that based on trimming+SVD.", "startOffset": 21, "endOffset": 25}, {"referenceID": 13, "context": "Thus the correct rank can be estimated using the value minimizing consecutive eigenvalues, as suggested in [14].", "startOffset": 107, "endOffset": 111}], "year": 2016, "abstractText": "Spectral methods are popular in detecting global structures in the given data that can be represented as a matrix. However when the data matrix is sparse or noisy, classic spectral methods usually fail to work, due to localization of eigenvectors (or singular vectors) induced by the sparsity or noise. In this work, we propose a general method to solve the localization problem by learning a regularization matrix from the localized eigenvectors. Using matrix perturbation analysis, we demonstrate that the learned regularizations suppress down the eigenvalues associated with localized eigenvectors and enable us to recover the informative eigenvectors representing the global structure. We show applications of our method in several inference problems: community detection in networks, clustering from pairwise similarities, rank estimation and matrix completion problems. Using extensive experiments, we illustrate that our method solves the localization problem and works down to the theoretical detectability limits in different kinds of synthetic data. This is in contrast with existing spectral algorithms based on data matrix, non-backtracking matrix, Laplacians and those with rank-one regularizations, which perform poorly in the sparse case with noise.", "creator": "LaTeX with hyperref package"}}}