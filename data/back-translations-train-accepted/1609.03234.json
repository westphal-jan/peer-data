{"id": "1609.03234", "review": {"conference": "icml", "VERSION": "v1", "DATE_OF_SUBMISSION": "12-Sep-2016", "title": "Reduced Space and Faster Convergence in Imperfect-Information Games via Regret-Based Pruning", "abstract": "Counterfactual Regret Minimization (CFR) is the most popular iterative algorithm for solving zero-sum imperfect-information games. Regret-Based Pruning (RBP) is an improvement that allows poorly-performing actions to be temporarily pruned, thus speeding up CFR. We introduce Total RBP, a new form of RBP that reduces the space requirements of CFR as actions are pruned. We prove that in zero-sum games it asymptotically prunes any action that is not part of a best response to some Nash equilibrium. This leads to provably faster convergence and lower space requirements. Experiments show that Total RBP results in an order of magnitude reduction in space, and the reduction factor increases with game size.", "histories": [["v1", "Mon, 12 Sep 2016 00:30:54 GMT  (293kb,D)", "http://arxiv.org/abs/1609.03234v1", null]], "reviews": [], "SUBJECTS": "cs.GT cs.AI", "authors": ["noam brown", "tuomas sandholm"], "accepted": true, "id": "1609.03234"}, "pdf": {"name": "1609.03234.pdf", "metadata": {"source": "CRF", "title": "Reduced Space and Faster Convergence in Imperfect- Information Games via Regret-Based Pruning", "authors": ["Noam Brown"], "emails": ["noamb@cmu.edu", "sandholm@cs.cmu.edu"], "sections": [{"heading": "1 Introduction", "text": "This year it is more than ever before."}, {"heading": "2 Background", "text": "This section presents the notation used in the rest of the paper. An imperfect form of information has a finite number of players, P (h) is the set of all possible stories (nodes) in the game tree, represented as a sequence of actions, and contains the empty story. A (h) is the action available in a story, and P (h) is the player acting in that story, where c is an action that represents an action with a fixed probability. A (h) is an action known to all players. The story h \"reaches after action a in h is a child of h, represented by h,\" while h is \"the action of the parents of h.\" More generally h \"is an ancestor of h\" (and h is a descendant of h \"), represented by h.\" @ h \"if there is an action of h. Z\" H \"is an endless story of actions for which no actions are available."}, {"heading": "2.1 Counterfactual Regret Minimization", "text": "Counterfactual Regret Minimization (CFR) is a popular algorithm for extended form games, in which the strategy vector for each amount of information is determined using a repentance minimization algorithm (BR = > RT). We use repentance matching (RT) [6] as a repentance minimization algorithm. Therefore, RTFR analysis often uses counterfactual information. Informally, this is the expected benefit of a set of information when the player tries to achieve it. I have given a strategy profile for player i on the information set that is defined as asv\u03c3 (I). (I) = h h h \"I (h) perceive counterfactual actions \u2212 z.\" (Z) ui. \"The counterfactual value of an action a is v\u03c3 (I, a). (I, a) h\" h \"h\" h \"h.\""}, {"heading": "2.2 Partial Pruning and Interval Regret-Based Pruning", "text": "(I) (I) (I) (I) (I) (I) (I) (I) (I) (I) (I) (I) (I) (I) (I) (I) (I) (I) (I) (I) (I) (I) (I) (I) (I) (I) (I) (I) (I) (I) (I) (I) (I) (I) (I) (I) (I) (I) (I) (I) (I) (I) (I) (I) (I) (I) (I) (I) (I) (I) (I) (I) (I) (I) (I) (I) (I) (I) (I) (I) (I) (I) (I) (I) (I) (I) (I) (I) (I) (I) (I) (I) (I) (I) (I) (I) (I) (I) (I) (I) (I) (I) (I) (I) (I) (I) (I) (I) (I) (I) (I) (I) (I) (I) (I) (I) (I) (I (I) (I) (I) (I) (I) (I (I) (I) (I) (I (I) (I) (I (I) (I) (I) (I (I) (I) (I) (I) (I) (I) (I (I) (I) (I (I) (I) (I) (I) (I) (I) (I (I) (I (I) (I) (I) (I) (I) (I) (I (I) (I) (I) (I) (I) (I) (I) (I) (I) (I) (I (I) (I) (I) (I) (I) (I) (I) (I) (I) (I) (I) (I) (I) (I) (I) (I) (I) (I) (I) (I) (I) (I"}, {"heading": "3.1 Total RBP Requires Less Space", "text": "A major advantage of Total RBP is that setting the new repentance according to (7) and (8) requires a two-pronged strategy for all players if they have no knowledge of what the repentance was before they started. So, the space only needs to be reallocated once, and we cannot immediately start repenting again (that is, (5) cannot be kept). Theorem 2 proves that for all the information I have placed, one (I) that is not part of a best possible response to a Nash equilibrium, there is one that can be set for all T-1, an action in Information Set I (and its descendants). (2) Once this TI, it will never be necessary to reserve room for repentance in D-2."}, {"heading": "3.2 Total RBP Has a Better Convergence Bound", "text": "In this section, we demonstrate that Total RBP can perform any iteration faster than CFR alone. Specifically, if an action A (I) is not a CBR to a Nash balance, then actions that are not a counterfactual best response will ultimately perform worse than actions that are, so these suboptimal actions will only accumulate an increasing negative regret. This negative regret allows the action to be safely curtailed for ever longer periods of time. (Since both players converge to a Nash balance, actions that are not a counterfactual best response will ultimately perform worse than actions that are, so these suboptimal actions will accumulate increasing amounts of negative regret. This negative regret allows the action to be curtailed safely for ever longer periods of time (Nash balances)."}, {"heading": "4 Experiments", "text": "The experiments are conducted on Leduc Hold'em [13] and Leduc FR-5 [2]. However, Leduc Hold'em is a common benchmark in imperfect information game solutions because it is small enough to be solved but still strategically complex. In Leduc Hold'em there is a deck consisting of six cards: two each from Jack, Queen, and King. There are two rounds in which each player puts an ante of 1 chip into the pot and receives a single private card. A round of bets then takes place with a two-bet maximum, with player 1 going first. A public common card is then dealt and another round of bets takes place. Player 1 goes first, and there is a two-bet maximum maximum."}, {"heading": "5 Conclusions", "text": "We introduced Total RBP, a new form of contrition-based retrenchment that has been proven to reduce both the space needed to solve a game with imperfect information and the time needed to achieve a -Nash balance, addressing the two biggest bottlenecks in solving large games with imperfect information. Experimentally, Total RBP reduced the space needed to solve a game by an order of magnitude, with the reduction factor increasing with the size of the game."}, {"heading": "A Lemma 1", "text": "Lemma 1 proves that if (5) is fulfilled for any action a = A (I) for iteration T (FR), then the value of the action a and all its progeny can be set to the T-nearly counterfactually best response value for each iteration played so far. Lemma 1. Suppose that the T-nearly counterfactually best response values have been replaced by exact counterfactually best response values. If T (NBV) is changed for each action T \u2212 i, T (I, a), T t = 1 v (I) and one changes T (NBV, T) for each action T (I, a). If T (I, a) is changed for each action T \u2212 T, then the action T (I, a)."}, {"heading": "B Proof of Theorem 1", "text": "Proof. From Lemma 1, we can immediately repent for an \"A\" (I) to \"T\" (I, a) = \"NBV\" = \"T \u2212 i,\" \"T\" (I, a). By constructing \"T,\" Rt (I, a) is guaranteed not to be positive for \"T\" (I, a) and therefore \"T\" (I, a) = 0. Thus, \"T + T\" (I, a) is identical for \"I,\" regardless of what is played in \"D\" (I, a) while \"T\" (I, a) and \"T + T\" (NBV, a). Since (T + T) (NBV, T + I, a), \"T\" (U (I, a) + \"T\" (I, a) and \"T +\" (T, a) = 1 v. \""}, {"heading": "C Proof of Theorem 2", "text": "Evidence. Consider a set of information I and an act a \u00b2 A (I), in which for each opponent Nash equilibrium strategy \u03c3 \u00b2 P (I), CBV \u00b2 P (I) (I, a) < CBV \u2082 \u2212 P (I) (I). Let us let i = P (I). Let us let it. Let it. Let it. Let it. Let it. Let it. Let it. Let it. Let it. Let it. Let it. Let it. Let it. Let it. Let it. Let it. Let it. Let it. Let it. Let it. Let it. Let it. Let it."}, {"heading": "D Proof of Corollary 1", "text": "Proof. Let me use 6 \"IS.\" Then I will use \"D\" (I, \"a\") for some \"I\" and \"A\" (I \") for each opponent's\" Nash \"balance strategy \u03c3\" P \"(I\"), \"CBV \u03c3\" (I \") (I,\" a \") < CBV \u03c3\" P \"(I\") (I \"). Applied to Theorem 2 this means that there is one\" TI, \"\" a \"and\" I, \"one\" > 0 \"so that for T\" TI, \"one,\" CBV \u03c3 \"T\" i \"(I,\" a \") -\" T \"t = 1 v\" t \"(I\"), \"one.\" So (5) always applies to \"T,\" one \"for me\" and one, \"and one,\" and I will always be circumcised. \"Since (8) no knowledge of regret is required, it does not have to be saved for me. Since D\" (\"I,\" a) always applies to \"one,\" one, \"one,\" one T, \"and one."}, {"heading": "E Lemma 2", "text": "Lemma 2. If for all T \u2265 T \u2032 iterations of the CFR with RBP, T (CBV \u03b5 T (I, a) \u2212 \u2211 T t = 1 v\u03c3t (I) \u2264 \u2212 xT for some x > 0, then each story h \"such that h \u00b7 a v h\" for some h \"i must be passed through at most O (ln (T) times. Proof. Let such an action be such that for all T \u2265 T,\" T (CBV), T (I, a) \u2212 \u2211 T t = 1 v\u03c3t (I) \u2264 \u2212 xT for some x > 0. NBV \u03c3 T \u2212 i, T (I, a) \u2264 CBV \u0445T \u2212 i, so that from theorem 1, D (I, a) for m \u00b2 b xTU (I, a)."}, {"heading": "F Proof of Theorem 3", "text": "Proof. Let us consider an h * 6 * S. Then there are some h \u00b7 a v * so that h \u00b7 S but h \u00b7 a 6 * S. Let us leave I = I (h) and i = P (I). Since h \u00b7 a 6 * S but h * S, i.e. for each Nash equilibrium \u03c3 *, CBV \u03c3 * (I, a) < CBV \u03c3 * (I). From Theorem 2 there is a TI, a and \u03b4I, a > 0 such that according to T \u2265 TI, an iteration of CFR, CBV \u0445 T \u2212 i (I, a) \u2212 \u2211 T t = 1 v \u0432t (I) T \u2264 \u2212 \u0432I, a. Therefore, from Lemma 2, h \"must be traversed at most O (ln (T) times."}], "references": [{"title": "Heads-up limit hold\u2019em poker is solved", "author": ["Michael Bowling", "Neil Burch", "Michael Johanson", "Oskari Tammelin"], "venue": "Science,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2015}, {"title": "Regret-based pruning in extensive-form games", "author": ["Noam Brown", "Tuomas Sandholm"], "venue": "In Proceedings of the Annual Conference on Neural Information Processing Systems (NIPS),", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2015}, {"title": "Simultaneous abstraction and equilibrium finding in games", "author": ["Noam Brown", "Tuomas Sandholm"], "venue": "In Proceedings of the International Joint Conference on Artificial Intelligence (IJCAI),", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2015}, {"title": "Strategy-based warm starting for regret minimization in games", "author": ["Noam Brown", "Tuomas Sandholm"], "venue": "In AAAI Conference on Artificial Intelligence (AAAI),", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2016}, {"title": "Lossless abstraction of imperfect information games", "author": ["Andrew Gilpin", "Tuomas Sandholm"], "venue": "Journal of the ACM,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2007}, {"title": "A simple adaptive procedure leading to correlated", "author": ["Sergiu Hart", "Andreu Mas-Colell"], "venue": "equilibrium. Econometrica,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2000}, {"title": "Smoothing techniques for computing Nash equilibria of sequential games", "author": ["Samid Hoda", "Andrew Gilpin", "Javier Pe\u00f1a", "Tuomas Sandholm"], "venue": "Mathematics of Operations Research,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2010}, {"title": "Faster first-order methods for extensive-form game solving", "author": ["Christian Kroer", "Kevin Waugh", "Fatma K\u0131l\u0131n\u00e7-Karzan", "Tuomas Sandholm"], "venue": "In Proceedings of the ACM Conference on Economics and Computation (EC),", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2015}, {"title": "Monte Carlo sampling for regret minimization in extensive games", "author": ["Marc Lanctot", "Kevin Waugh", "Martin Zinkevich", "Michael Bowling"], "venue": "In Proceedings of the Annual Conference on Neural Information Processing Systems (NIPS),", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2009}, {"title": "Refining subgames in large imperfect information games", "author": ["Matej Moravcik", "Martin Schmid", "Karel Ha", "Milan Hladik", "Stephen J Gaukrodger"], "venue": "In Thirtieth AAAI Conference on Artificial Intelligence,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2016}, {"title": "Excessive gap technique in nonsmooth convex minimization", "author": ["Yurii Nesterov"], "venue": "SIAM Journal of Optimization,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2005}, {"title": "An interior point approach to large games of incomplete information", "author": ["Fran\u00e7ois Pays"], "venue": "In AAAI Computer Poker Workshop,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2014}, {"title": "bluff: Opponent modelling in poker", "author": ["Finnegan Southey", "Michael Bowling", "Bryce Larson", "Carmelo Piccione", "Neil Burch", "Darse Billings", "Chris Rayner. Bayes"], "venue": "In Proceedings of the 21st Annual Conference on Uncertainty in Artificial Intelligence (UAI),", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2005}, {"title": "Solving heads-up limit texas hold\u2019em", "author": ["Oskari Tammelin", "Neil Burch", "Michael Johanson", "Michael Bowling"], "venue": "In Proceedings of the 24th International Joint Conference on Artificial Intelligence (IJCAI),", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2015}, {"title": "Abstraction pathologies in extensive games", "author": ["Kevin Waugh", "David Schnizlein", "Michael Bowling", "Duane Szafron"], "venue": "In International Conference on Autonomous Agents and Multi-Agent Systems (AAMAS),", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2009}], "referenceMentions": [{"referenceID": 4, "context": "A linear program can find an exact Nash equilibrium in two-player zero-sum games containing fewer than about 10 nodes [5].", "startOffset": 118, "endOffset": 121}, {"referenceID": 10, "context": "There are a number of such iterative algorithms [11, 7, 12, 8], the most popular of which is Counterfactual Regret Minimization (CFR) [16].", "startOffset": 48, "endOffset": 62}, {"referenceID": 6, "context": "There are a number of such iterative algorithms [11, 7, 12, 8], the most popular of which is Counterfactual Regret Minimization (CFR) [16].", "startOffset": 48, "endOffset": 62}, {"referenceID": 11, "context": "There are a number of such iterative algorithms [11, 7, 12, 8], the most popular of which is Counterfactual Regret Minimization (CFR) [16].", "startOffset": 48, "endOffset": 62}, {"referenceID": 7, "context": "There are a number of such iterative algorithms [11, 7, 12, 8], the most popular of which is Counterfactual Regret Minimization (CFR) [16].", "startOffset": 48, "endOffset": 62}, {"referenceID": 0, "context": "CFR+, a variant of CFR, was used to essentially solve Limit Texas Hold\u2019em, the largest imperfect-information game ever to be essentially solved [1].", "startOffset": 144, "endOffset": 147}, {"referenceID": 13, "context": "This data had to be repeatedly decompressed from disk into memory and then compressed back to disk in order to run CFR+ [14].", "startOffset": 120, "endOffset": 124}, {"referenceID": 1, "context": "Regret Based Pruning (RBP) is an improvement to CFR that greatly reduces the computation time needed to solve large games by temporarily pruning suboptimal actions [2].", "startOffset": 164, "endOffset": 167}, {"referenceID": 3, "context": "While Total RBP still requires enough memory to store the entire game in the early iterations, recent work has shown that these early iterations can be skipped by first solving a low-memory abstraction of the game and then using its solution to warm starting CFR in the full game [4].", "startOffset": 280, "endOffset": 283}, {"referenceID": 2, "context": "Total RBP\u2019s reduction in space is also helpful to the Simultaneous Abstraction and Equilibrium Finding (SAEF) algorithm [3], which starts CFR with a small abstraction of the game and progressively expands the abstraction while also solving the game.", "startOffset": 120, "endOffset": 123}, {"referenceID": 5, "context": "We use regret matching (RM) [6] as the regret-minimization algorithm.", "startOffset": 28, "endOffset": 31}, {"referenceID": 9, "context": "A counterfactual best response [10] (CBR) is a strategy similar to a best response, except that it maximizes counterfactual value even at information sets that it does not reach due to its earlier actions.", "startOffset": 31, "endOffset": 35}, {"referenceID": 14, "context": "In two-player zero-sum games, if both players\u2019 average regret R i T \u2264 , their average strategies \u3008\u03c3\u0304 1 , \u03c3\u0304 2 \u3009 form a 2 -equilibrium [15].", "startOffset": 134, "endOffset": 138}, {"referenceID": 8, "context": "The benefit of this form of pruning, which we refer to as partial pruning, varies depending on the game, but empirical results show a factor of 30 improvement in some games [9].", "startOffset": 173, "endOffset": 176}, {"referenceID": 1, "context": "While partial pruning allows one to prune paths that an opponent reaches with zero probability, interval regret-based pruning (Interval RBP) allows one to also prune paths that the traverser reaches with zero probability [2].", "startOffset": 221, "endOffset": 224}, {"referenceID": 3, "context": "The proof for Theorem 1 draws from recent work on warm starting CFR using only an average strategy profile [4].", "startOffset": 107, "endOffset": 110}, {"referenceID": 12, "context": "The experiments are conducted on Leduc Hold\u2019em [13] and Leduc-5 [2].", "startOffset": 47, "endOffset": 51}, {"referenceID": 1, "context": "The experiments are conducted on Leduc Hold\u2019em [13] and Leduc-5 [2].", "startOffset": 64, "endOffset": 67}], "year": 2016, "abstractText": "Counterfactual Regret Minimization (CFR) is the most popular iterative algorithm for solving zero-sum imperfect-information games. Regret-Based Pruning (RBP) is an improvement that allows poorly-performing actions to be temporarily pruned, thus speeding up CFR. We introduce Total RBP, a new form of RBP that reduces the space requirements of CFR as actions are pruned. We prove that in zero-sum games it asymptotically prunes any action that is not part of a best response to some Nash equilibrium. This leads to provably faster convergence and lower space requirements. Experiments show that Total RBP results in an order of magnitude reduction in space, and the reduction factor increases with game size.", "creator": "LaTeX with hyperref package"}}}