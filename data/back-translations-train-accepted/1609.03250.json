{"id": "1609.03250", "review": {"conference": "NIPS", "VERSION": "v1", "DATE_OF_SUBMISSION": "12-Sep-2016", "title": "DESPOT: Online POMDP Planning with Regularization", "abstract": "The partially observable Markov decision process (POMDP) provides a principled general framework for planning under uncertainty, but solving POMDPs optimally is computationally intractable, due to the \"curse of dimensionality\" and the \"curse of history\". To overcome these challenges, we introduce the Determinized Sparse Partially Observable Tree (DESPOT), a sparse approximation of the standard belief tree, for online planning under uncertainty. A DESPOT focuses online planning on a set of randomly sampled scenarios and compactly captures the \"execution\" of all policies under these scenarios. We show that the best policy obtained from a DESPOT is near-optimal, with a regret bound that depends on the representation size of the optimal policy. Leveraging this result, we give an anytime online planning algorithm, which searches a DESPOT for a policy that optimizes a regularized objective function. Regularization balances the estimated value of a policy under the sampled scenarios and the policy size, thus avoiding overfitting. The algorithm demonstrates strong experimental results, compared with some of the best online POMDP algorithms available. It has also been incorporated into an autonomous driving system for realtime vehicle control. The source code for the algorithm is available at http: //bigbird.comp.nus.edu.sg/pmwiki/farm/appl/.", "histories": [["v1", "Mon, 12 Sep 2016 02:12:13 GMT  (691kb,D)", "https://arxiv.org/abs/1609.03250v1", "35 pages"], ["v2", "Wed, 8 Mar 2017 07:28:31 GMT  (684kb,D)", "http://arxiv.org/abs/1609.03250v2", "36 pages"], ["v3", "Tue, 19 Sep 2017 03:29:57 GMT  (684kb,D)", "http://arxiv.org/abs/1609.03250v3", "36 pages"]], "COMMENTS": "35 pages", "reviews": [], "SUBJECTS": "cs.AI", "authors": ["adhiraj somani", "nan ye", "david hsu", "wee sun lee"], "accepted": true, "id": "1609.03250"}, "pdf": {"name": "1609.03250.pdf", "metadata": {"source": "CRF", "title": "DESPOT: Online POMDP Planning with Regularization", "authors": ["Nan Ye", "Adhiraj Somani", "David Hsu", "Wee Sun Lee"], "emails": ["N.YE@QUT.EDU.AU", "ADHIRAJSOMANI@GMAIL.COM", "DYHSU@COMP.NUS.EDU.SG", "LEEWS@COMP.NUS.EDU.SG"], "sections": [{"heading": null, "text": "To overcome these challenges, we present the Determined Sparse Partially Observable Tree (DESPOT), a sparse approach to the standard belief tree for online planning under uncertainty. A DESPOT focuses online planning on a series of randomly selected scenarios and compactly captures the \"execution\" of all strategies under these scenarios. We show that the best policy gained from a DESPOT is nearly optimal, with the limit of regret depending on the representation size of the optimal policy. Based on this result, we present an online planning algorithm that searches for a DESPOT for a strategy that optimizes a regulated objective function. Regulation balances the estimated value of a policy under the selected scenarios and the size of the policy, thereby avoiding matching of the available vehicle with the available POSPOT results."}, {"heading": "1. Introduction", "text": "This year, it has reached the point where it will be able to put itself at the top of the list."}, {"heading": "2. Background", "text": "In this section we discuss the basics of online POMDP planning and related work."}, {"heading": "2.1 Online POMDP Planning", "text": "A POMDP model is an agent that acts in a partially observable stochastic environment. It can formally be called a tuple (S, A, Z, T, O, R), where S is a set of states, A is a set of actions, and Z is a set of observations. If the agent makes a set of observations, he is placed in a new state in which there is a real reward. (s, a, s).A POMDP agent does not know the true state, but receives observations that provide partial information about the state. Thus, the agent maintains a belief that acts as a probability distribution over S. It begins with an initial belief b0."}, {"heading": "2.2 Related Work", "text": "There are two main approaches to POMDP planning: offline policy and online search for the best way to proceed. In offline planning, the agent is initially dependent on a policy based on all possible future outcomes and executes the calculated policy based on the observations obtained. A major advantage of offline planning is the rapid implementation of strategies to predict policy. Early work on POMDP planning often takes the offline approach. See, the work of Kaelbling, Littman, and Cassandra (1998) and Zhang (2001) Although offline planning algorithms have made great strides in recent years (Pineau et al., 2003; Spaan & Vlassis, 2005; Kurniawati et al., 2008), they still face significant difficulties in scaling-up to very large POMDPs, as they must plan together for all beliefs and future contingencies."}, {"heading": "3. Determinized Sparse Partially Observable Trees", "text": "A DESPOT is a sparse approximation of a standard belief tree. While a standard belief tree records the execution of all actions under all possible scenarios, a DESPOT records the execution of all actions under a series of randomly sampled scenarios (Figure 1b). A DESPOT scenario contains all actions but only the observations that occur under the sampled scenarios. Formally, a scenario for a belief b is constructive by applying a deterministic simulation model to all possible action sequences under sampled scenarios. A scenario is an abstract simulation with some initial states szu0. Formally, a scenario for a belief b is an infinite random sequence."}, {"heading": "4. Online Planning with DESPOTs", "text": "In accordance with the standard online planning framework (Section 2.1), our algorithm iterates over two main steps: action selection and faith actualization. In faith actualization, we use a standard method of particle filtering, sequential resampling (SIR) (Gordon, Salmond, & Smith, 1993). We now present two methods of action selection: In Section 4.1, we describe a conceptually simple dynamic programming method that fully constructs a DESPOT before the optimal action is found. In the case of very large POMDPs, the construction of DESPOT is completely impractical. In Sections 4.2 to 4.4, we describe a DESPOT algorithm that performs heuristic searches at any time. The algorithm available at any time constructs a DESPOT step by step under the guidance of a hayristic and scales in practice up to very large POMDPs. In Section 4.5, we show that the algorithm is graded to an optimal algorithm, even if the algorithm is consistent with an algorithmic policy, and if the algorithm is algorithmic is algorithmic."}, {"heading": "4.1 Dynamic Programming", "text": "We construct a fixed DESPOT D with K randomly sampled scenarios and want to derive a policy from D that maximizes the regulated empirical value (6) using the sampled scenarios: max. \u00b7 D (b0) \u2212 f = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p p = p = p = p = p p = p = p = p = p p = p p = p p = p = p = p = p p p = p p p p p = p = p p = p p p p = p p p p p"}, {"heading": "4.2 Anytime Heuristic Search", "text": "This is usually impractical, as there are exponentially many nodes. To scale this, we now introduce a forward-facing search that prevents DESPOT from being constructed completely in advance. It selects the action by constructing a DESPOT D incrementally. We describe the main components of the algorithms below. Full pseudo-coding will perform the heuristic search in Appendix B.To, which maintains a lower limit (b) and an upper limit (b) at the optimal RWDU. We describe the main components of the algorithms below. (b) Full pseudo-coding will occur in Appendix B.To."}, {"heading": "4.2.1 FORWARD EXPLORATION", "text": "Let's (b) = \u00b5 (b) \u2212 (b) explain the gap between the upper and lower RWDU boundary at a node b. (b) Each exploration aims to reduce the current gap (b0) at root b0 \u2032 \u2032 \u2032 to \u2032 (b0) for a given constant 0 < < 1 (algorithm 2). An exploration begins at root b0. At each node along the exploration path, we select the branch of action corresponding to the upper boundary \u00b5 (b, a): a) = arg max a \"\u00b5 (b, a) = arg max a\" A \"(b, a) = arg max a\".A \".A\".A \".Z\".Z \".Z.\" \""}, {"heading": "4.2.2 TERMINATION OF EXPLORATION AND PRUNING", "text": "We end exploration at a node b under three conditions (algorithm 2, line 1): firstly, \u2206 (b) > D, i.e., the maximum tree height is exceeded; secondly, E (b) < 0, indicating that the expected gap is reached at b and further exploration from b may be unprofitable; b is eventually blocked by a pedigree b \u2032: | \u03a6b \u2032 | K \u03b3 (b \u2032) (U (b \u2032) \u2212 L0 (b \u2032))) \u2264 (b \u2032, b), (12), where \"(b \u2032, b) the number of nodes on the path from b \u2032 to b \u2032. The intuition behind this last condition is that there are not enough studied scenarios at the pedigree b \u2032. Further expansion b, and thus enlargement of the policy sub-tree at b \u2032, b \u00b2, may lead to an overadjustment and reduction of the regulated utility at b \u2032."}, {"heading": "4.2.3 BACKUP", "text": "s principle (algorithm 5): \"(b) = max {'0 (b), maxa-a {\u03c1 (b, a) + x-z-Zb, a-z (b),\" (b) = max {' 0 (b), maxa-z (b, a)}, U (b) = max-a-A {1 | \u03a6b-z (b, a) + x-z-Zb, a-b (b), where b-z is a child of b-z (b, a, a, z)."}, {"heading": "4.2.4 RUNNING TIME", "text": "Suppose that the always available search algorithm calls EXPLORE N times. EXPLORE traverses a path from the root to a leaf node of a DESPOTD, visiting mostD + K \u2212 1 nodes, because a path has at most D nodes, and at most K \u2212 1 nodes that are not on the path, can be added due to node extensions (algorithm 2). At each node, the following steps dominate the runtime. Checking the pruning condition (line 1) takes a total of time O (D2) and per node O (D). Adding a new node to D and initializing the boundaries (lines 3 and 8) take time O (I) (assuming I am an upper limit of the cost). Selecting the action branch (line 4) takes time O (A | A). Choosing the observation branch (line 5) requires min time."}, {"heading": "4.3 Initial Upper Bounds", "text": "There are naturally many alternatives. The flexibility in the construction of upper and lower limits for improved performance is a strength of DESPOT.The simplest scenario is the uninformed limit U0 (b) = upper limit U0 (b) = upper limit Umax / (1 \u2212 3). (13) While this limit is loosely calculated, it is easy to provide good results, if it is combined with suitable lower limits U0 (b) = upper limit Umax / (1 \u2212 3). (13) While this limit is easy to calculate and can provide good results, if it is combined with suitable lower limits U0 (b). (Yoon et al., 2008) offers a principle method to construct an upper limit algorithmically. In view of a set scenario????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????"}, {"heading": "4.4 Initial Lower Bounds and Default Policies", "text": "The DESPOT algorithm requires a default policy \u03c00. The simplest default policy is a fixed-action policy with the highest expected total discount reward (Smith & Simmons, 2004). One can also manage a better policy that chooses an action based on the past history of actions and observations (Silver & Veness, 2010). However, it is often not easy to determine what the next action should be given the past history of actions and observations. As in the case of ceilings, it is often more intuitive to work with states than with beliefs. We describe a class of methods that we call scenario-based politics. In a scenario-based policy, we construct a figure f: S 7 \u2192 A that specifies an action in a given state. We then specify a function that maps a belief in a state: B 7 \u2192 S and make the default policy count as the ultimate policy (b) = f (b)."}, {"heading": "4.5 Analysis", "text": "The dynamic programming environment builds a complete DESPOT D. The main objective of the analysis is to show that the optimal regulated policy of D. \"We start with some lemmas to justify the decisions that are made in the respective optimal regulated policy. Lemma 4.1 says that the excessive uncertainty at a node b is limited by the sum of the excessive uncertainty about its children, which has the highest upper limit (b, a). This provides a greedy means to reduce the excessive uncertainty by recursive exploration of the branch of action."}, {"heading": "5. Experiments", "text": "We now compare the always available DESPOT algorithm with three state-of-the-art POMDP algorithms (Section 5.1) and examine the impact of regulation (Section 5.2) and initial limits (Section 5.3) on the performance of our algorithm."}, {"heading": "5.1 Performance Comparison", "text": "We compare DESPOT with SARSOP (Kurniawati et al., 2008), AEMS2 (Ross & Chaib-Draa, 2007; Ross et al., 2008), and POMCP (Silver & Veness, 2010). SARSOP is one of the fastest offline POMDP algorithms. Although it cannot compete with online algorithms on scalability, it often delivers better results on POMDP's moderate size and helps calibrate the performance of online algorithms. AEMS2 is an early successful online POMDP algorithm. Again, it is not designed to target very large government and observation spaces and is used here as a calibration on medium-sized problems. POMCP scales in practice (Silver & Veness, 2010) and allows us to calibrate the performance of DESPOT algorithms on very large problems. We implemented DESPOT and EMS2 ourselves."}, {"heading": "5.1.1 TAG", "text": "A robot and a target operate in a grid with 29 possible positions (Figure 4a). The aim of the robot is to find and mark the target that intentionally runs away. They start from random starting positions. The robot knows its own position, but can only observe the position of the target if they are in the same grid cell. The robot can either stay in the same position or move into the four adjacent positions and pay a price of \u2212 1 for each turn. It can also try to mark the target. It will be rewarded + 10 if the attempt is successful, and will otherwise be punished. To successfully complete the task, a good policy uses the dynamics of the target to \"push\" it against a corner of the environment. For DESPOT we use the Forbearance optimization, which is tied to the initial upper limit of U0, and initializes the forbearance optimization by setting the optimal MDP value (Section 4.3)."}, {"heading": "5.1.2 LASER TAG", "text": "Theorem 3.1 suggests that DESPOT can work well even if the observation room is large, provided there is a small good policy. We are now looking at Laser Tag, an extended version of Tag with a large observation room. In Laser Tag, the robot moves in a 7 x 11 rectangular grid with obstacles randomly placed in eight grid cells (Figure 4b). The behavior of the robot and the target remains the same as before. However, the robot does not know its own position accurately and is initially evenly distributed across the grid. To locate it, it is equipped with a laser rangefinder that measures the distances in eight directions. The side length of each cell is 1. The laser reading in each direction is generated by a normal distribution centered in the true distance of the robot to the next obstacle in that direction."}, {"heading": "5.1.3 ROCK SAMPLE", "text": "Next, consider the Rock Sample, a well-established benchmark with a large state space (Smith & Simmons, 2004).In RS (n, k), a robot rover moves on an n \u00d7 n grid containing k rocks, each of which can be good or bad (Figure 4c).The goal of the robot is to visit and try the good rocks, and leave the eastern border upon completion. At each step, the robot can move into an adjacent cell, sampling a rock or sampling a rock. Sampling gives a reward of + 10 if the rock is good and \u2212 10 otherwise. Moving and sampling have a reward of 0. Moving or sampling do not generate observation, or equivalent zero observation is produced. Sampling a rock produces an observation, GOOD or BAD, with the probability that it will be decreased exponentially with the distance of the robot from the rock."}, {"heading": "5.1.4 POCMAN", "text": "Pocman (Silver & Veness, 2010) is a partially observable variant of the popular video game Pacman (Figure 4d). In Pocman, one agent and four ghosts move in a 17 x 19 maze populated with food pellets. Each agent moves at a cost of \u2212 1. Each food pellet offers a reward of + 10. If the agent is captured by a ghost, the game ends with a penalty of \u2212 100. In addition, there are four power pills. Within the next 15 steps after consuming a power pill, the agent can eat a ghost and receive a reward of + 25. A ghost chases the agent if the agent is at a distance of \u2212 100, but runs away if the agent has a power pill. The agent does not know the exact ghost locations, but receives information about whether he sees a ghost in each direction, whether he hears a ghost, whether he hears a ghost at a distance of 2, whether he feels a wall."}, {"heading": "5.1.5 BRIDGE CROSSING", "text": "The experiments above suggest that both POMCP and DESPOT can handle very large POMDPs. However, the UCT search strategy (Kocsis & Szepesvari, 2006) on which POMCP relies is very bad behavior (Coquelin & Munos, 2007) We have designed Bridge Crossing, a very simple domain to illustrate this. In Bridge Crossing, a person attempts to cross a narrow bridge over a mountain pass in the dark, starting at one end of the bridge, but is uncertain whether his exact starting position is due to the darkness. At any time, he can call for rescue and end the attempt, the POMDP model gives the man 10 discredited positions x, 1."}, {"heading": "5.2 Benefits of Regularization", "text": "In fact, the number of people who are able to move is higher than ever before, and the number of people who are able to move is higher than ever before, is higher than ever before."}, {"heading": "5.3 Effect of Initial Bounds", "text": "This year, it has come to the point that it has never come as far as it has this year."}, {"heading": "6. Discussion", "text": "A key idea of DESPOT is to use a series of randomly tested scenarios as an approximate representation of uncertainty and plan over the tested scenarios. This works well if there is a compact, near-optimal policy. The basic idea goes beyond POMDP planning and applies to other planning tasks under uncertainty, such as MDP planning and faith space MDP planning. The search strategy of the always available DESPOT algorithm has several desirable properties. It is asymptotically solid and complete (Theorem 4.2). It is robust against imperfect heuristics (Theorem 4.2). It is also flexible and allows easy inclusion of domain knowledge. However, there are many alternatives, such as the UCT strategy used in POMCP. While UCT performs very poorly at worst, it is easier to implement, an important practical consideration. Furthermore, it avoids the overhead of calculating the upper and lower limits during the search and could therefore be more efficient."}, {"heading": "7. Conclusions", "text": "This paper introduces DESPOT, a new approach to online POMDP planning. The main idea is to plan based on a series of sampled scenarios, avoiding overmatch with the samples. Theoretical analysis shows that a DESPOT compactly captures the \"execution\" of all strategies under the sampled scenarios and delivers a near-optimal policy, provided there is a small, near-optimal policy. Analysis justifies our general planning approach based on sampled scenarios and the need for regulation. Experimental results indicate strong performance of the always available DESPOT algorithm in practice. For moderately large POMDPs, DESPOT competes with SARSOP and AEMS2, but it scales much better. For large POMDPs with up to 1056 states, DESPOT matches POMCP and sometimes performs better than POMCP."}, {"heading": "Acknowledgments", "text": "The work was carried out mainly during N. Ye's time at the Department of Computer Science at the National University of Singapore. We are grateful to the anonymous reviewers for reading the manuscript carefully and providing many suggestions that have made a significant contribution to improving the work. Partly, the work is supported by the National Research Foundation Singapore through the SMART IRG program, the US Air Force Research Laboratory under agreements FA2386-12-1-4031 and FA2386-15-1-4010, a research grant from the Vice Chancellor of Queensland University of Technology and a scholarship from the Australian Laureate Fellowship (FL110100281) of the Australian Research Council."}, {"heading": "Appendix A. Proofs", "text": "The evidence of theory 3.1We become the following problem from (Haussler, 1992, p. 103): We become the following problem from (Lemma 9, part (2).Lemma A.1 (Hausslers border) Let us Z1, \u2212 \u2212 \u2212 Zn # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # #"}, {"heading": "Appendix B. Pseudocode for Anytime DESPOT", "text": "Algorithm 6 Anytime DESPOTInput\u03b2: Initial amounf.0 > a > b: The target gap between \u00b5 (b0) and '(b0).b: The target gap rate decreases. K: The number of sampled scenarios.D: The maximum depth of the DESPOT.\u03bb: Regularization constants. Tmax: The maximum online planning time per step. b: The maximum online planning time per step. b: The maximum number of sampled scenarios.D: The maximum depth of the DESPOT.\u03bb (b, a). 5: if L0 (b) >' (b, a): the maximum online planning time per step."}], "references": [{"title": "Monte carlo value iteration for continuous-state POMDPs", "author": ["H. Bai", "D. Hsu", "W.S. Lee", "V. Ngo"], "venue": "Algorithmic Foundations of Robotics IX,", "citeRegEx": "Bai et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Bai et al\\.", "year": 2011}, {"title": "Intention-Aware Online POMDP Planning for Autonomous Driving in a Crowd", "author": ["H. Bai", "S. Cai", "N. Ye", "D. Hsu", "W. Lee"], "venue": "In Proc. IEEE Int. Conf. on Robotics & Automation", "citeRegEx": "Bai et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Bai et al\\.", "year": 2015}, {"title": "Rollout algorithms for stochastic scheduling problems", "author": ["D.P. Bertsekas", "D.A. Castanon"], "venue": "J. Heuristics,", "citeRegEx": "Bertsekas and Castanon,? \\Q1999\\E", "shortCiteRegEx": "Bertsekas and Castanon", "year": 1999}, {"title": "A Theorem on Trees", "author": ["A. Cayley"], "venue": "Quart. J. Math, 23, 376\u2013378.", "citeRegEx": "Cayley,? 1889", "shortCiteRegEx": "Cayley", "year": 1889}, {"title": "MOMDPs: A Solution for Modelling Adaptive Management Problems", "author": ["I. Chad\u00e8s", "J. Carwardine", "T. Martin", "S. Nicol", "R. Sabbadin", "O. Buffet"], "venue": "In Proc. AAAI Conf. on Artificial Intelligence", "citeRegEx": "Chad\u00e8s et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Chad\u00e8s et al\\.", "year": 2012}, {"title": "A framework for simulation-based network control via hindsight optimization", "author": ["E. Chong", "R. Givan", "H. Chang"], "venue": "In Proc. IEEE Conf. on Decision & Control,", "citeRegEx": "Chong et al\\.,? \\Q2000\\E", "shortCiteRegEx": "Chong et al\\.", "year": 2000}, {"title": "Bandit algorithms for tree search", "author": ["Coquelin", "P.-A", "R. Munos"], "venue": "In Proc. Conf. on Uncertainty in Artificial Intelligence", "citeRegEx": "Coquelin et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Coquelin et al\\.", "year": 2007}, {"title": "Combining online and offline knowledge in UCT", "author": ["S. Gelly", "D. Silver"], "venue": "In Proc. Int. Conf. on Machine Learning", "citeRegEx": "Gelly and Silver,? \\Q2007\\E", "shortCiteRegEx": "Gelly and Silver", "year": 2007}, {"title": "Novel approach to nonlinear/non-Gaussian Bayesian state estimation", "author": ["N. Gordon", "D. Salmond", "A. Smith"], "venue": "IEE Proc. F on Radar & Signal Processing,", "citeRegEx": "Gordon et al\\.,? \\Q1993\\E", "shortCiteRegEx": "Gordon et al\\.", "year": 1993}, {"title": "Planning treatment of ischemic heart disease with partially observable Markov decision processes", "author": ["M. Hauskrecht", "H. Fraser"], "venue": "Artificial Intelligence in Medicine,", "citeRegEx": "Hauskrecht and Fraser,? \\Q2000\\E", "shortCiteRegEx": "Hauskrecht and Fraser", "year": 2000}, {"title": "Decision theoretic generalizations of the PAC model for neural net and other learning applications", "author": ["D. Haussler"], "venue": "Information and Computation, 100(1), 78\u2013150.", "citeRegEx": "Haussler,? 1992", "shortCiteRegEx": "Haussler", "year": 1992}, {"title": "Efficient planning under uncertainty with macro-actions", "author": ["R. He", "E. Brunskill", "N. Roy"], "venue": "J. Artificial Intelligence Research,", "citeRegEx": "He et al\\.,? \\Q2011\\E", "shortCiteRegEx": "He et al\\.", "year": 2011}, {"title": "Probability inequalities for sums of bounded random variables", "author": ["W. Hoeffding"], "venue": "J. American statistical association, 58(301), 13\u201330.", "citeRegEx": "Hoeffding,? 1963", "shortCiteRegEx": "Hoeffding", "year": 1963}, {"title": "Planning and acting in partially observable stochastic domains", "author": ["L.P. Kaelbling", "M.L. Littman", "A.R. Cassandra"], "venue": "Artificial intelligence,", "citeRegEx": "Kaelbling et al\\.,? \\Q1998\\E", "shortCiteRegEx": "Kaelbling et al\\.", "year": 1998}, {"title": "Approximate planning in large POMDPs via reusable trajectories", "author": ["M. Kearns", "Y. Mansour", "A. Ng"], "venue": "In Advances in Neural Information Processing Systems (NIPS),", "citeRegEx": "Kearns et al\\.,? \\Q1999\\E", "shortCiteRegEx": "Kearns et al\\.", "year": 1999}, {"title": "A sparse sampling algorithm for near-optimal planning in large Markov decision processes", "author": ["M. Kearns", "Y. Mansour", "A.Y. Ng"], "venue": "Machine Learning,", "citeRegEx": "Kearns et al\\.,? \\Q2002\\E", "shortCiteRegEx": "Kearns et al\\.", "year": 2002}, {"title": "Bandit based Monte-Carlo planning", "author": ["L. Kocsis", "C. Szepesvari"], "venue": "In Proc. Eur. Conf. on Machine Learning,", "citeRegEx": "Kocsis and Szepesvari,? \\Q2006\\E", "shortCiteRegEx": "Kocsis and Szepesvari", "year": 2006}, {"title": "SARSOP: Efficient point-based POMDP planning by approximating optimally reachable belief spaces", "author": ["H. Kurniawati", "D. Hsu", "W. Lee"], "venue": "In Proc. Robotics: Science and Systems", "citeRegEx": "Kurniawati et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Kurniawati et al\\.", "year": 2008}, {"title": "Nonapproximability results for partially observable Markov decision processes", "author": ["C. Lusena", "J. Goldsmith", "M. Mundhenk"], "venue": "J. Artif. Intell. Res. (JAIR),", "citeRegEx": "Lusena et al\\.,? \\Q2001\\E", "shortCiteRegEx": "Lusena et al\\.", "year": 2001}, {"title": "On the undecidability of probabilistic planning and infinite-horizon partially observable Markov decision problems", "author": ["O. Madani", "S. Hanks", "A. Condon"], "venue": "In Proc. AAAI Conf. on Artificial Intelligence,", "citeRegEx": "Madani et al\\.,? \\Q1999\\E", "shortCiteRegEx": "Madani et al\\.", "year": 1999}, {"title": "PEGASUS: A policy search method for large MDPs and POMDPs", "author": ["A. Ng", "M. Jordan"], "venue": "In Proc. Conf. on Uncertainty in Artificial Intelligence,", "citeRegEx": "Ng and Jordan,? \\Q2000\\E", "shortCiteRegEx": "Ng and Jordan", "year": 2000}, {"title": "The complexity of Markov decision processes", "author": ["C.H. Papadimitriou", "J.N. Tsitsiklis"], "venue": "Mathematics of Operations Research,", "citeRegEx": "Papadimitriou and Tsitsiklis,? \\Q1987\\E", "shortCiteRegEx": "Papadimitriou and Tsitsiklis", "year": 1987}, {"title": "An online POMDP algorithm for complex multiagent environments", "author": ["S. Paquet", "L. Tobin", "B. Chaib-Draa"], "venue": "In Proceedings of the fourth international joint conference on Autonomous agents and multiagent systems,", "citeRegEx": "Paquet et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Paquet et al\\.", "year": 2005}, {"title": "Point-based value iteration: An anytime algorithm for POMDPs", "author": ["J. Pineau", "G. Gordon", "S. Thrun"], "venue": "In Proc. Int. Jnt. Conf. on Artificial Intelligence,", "citeRegEx": "Pineau et al\\.,? \\Q2003\\E", "shortCiteRegEx": "Pineau et al\\.", "year": 2003}, {"title": "Exploiting structure to efficiently solve large scale partially observable Markov decision processes", "author": ["P. Poupart"], "venue": "Ph.D. thesis, University of Toronto.", "citeRegEx": "Poupart,? 2005", "shortCiteRegEx": "Poupart", "year": 2005}, {"title": "AEMS: An anytime online search algorithm for approximate policy refinement in large POMDPs", "author": ["S. Ross", "B. Chaib-Draa"], "venue": "In Proc. Int. Jnt. Conf. on Artificial Intelligence,", "citeRegEx": "Ross and Chaib.Draa,? \\Q2007\\E", "shortCiteRegEx": "Ross and Chaib.Draa", "year": 2007}, {"title": "Online planning algorithms for POMDPs", "author": ["S. Ross", "J. Pineau", "S. Paquet", "B. Chaib-Draa"], "venue": "J. Artificial Intelligence Research,", "citeRegEx": "Ross et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Ross et al\\.", "year": 2008}, {"title": "Coastal navigation: mobile robot navigation with uncertainty in dynamic environments", "author": ["N. Roy", "W. Burgard", "D. Fox", "S. Thrun"], "venue": "In Proc. IEEE Int. Conf. on Robotics & Automation", "citeRegEx": "Roy et al\\.,? \\Q1999\\E", "shortCiteRegEx": "Roy et al\\.", "year": 1999}, {"title": "Markovian decision processes with probabilistic observation of states", "author": ["J. Satia", "R. Lave"], "venue": "Management Science,", "citeRegEx": "Satia and Lave,? \\Q1973\\E", "shortCiteRegEx": "Satia and Lave", "year": 1973}, {"title": "Monte-Carlo planning in large POMDPs", "author": ["D. Silver", "J. Veness"], "venue": "In Advances in Neural Information Processing Systems (NIPS)", "citeRegEx": "Silver and Veness,? \\Q2010\\E", "shortCiteRegEx": "Silver and Veness", "year": 2010}, {"title": "The optimal control of partially observable Markov processes over a finite horizon", "author": ["R.D. Smallwood", "E.J. Sondik"], "venue": "Operations Research,", "citeRegEx": "Smallwood and Sondik,? \\Q1973\\E", "shortCiteRegEx": "Smallwood and Sondik", "year": 1973}, {"title": "Heuristic search value iteration for POMDPs", "author": ["T. Smith", "R. Simmons"], "venue": "In Proc. Conf. on Uncertainty in Artificial Intelligence,", "citeRegEx": "Smith and Simmons,? \\Q2004\\E", "shortCiteRegEx": "Smith and Simmons", "year": 2004}, {"title": "Point-based POMDP algorithms: Improved analysis and implementation", "author": ["T. Smith", "R. Simmons"], "venue": "In Proc. Conf. on Uncertainty in Artificial Intelligence", "citeRegEx": "Smith and Simmons,? \\Q2005\\E", "shortCiteRegEx": "Smith and Simmons", "year": 2005}, {"title": "DESPOT: POMDP planning with regularization", "author": ["A. Somani", "N. Ye", "D. Hsu", "W. Lee"], "venue": "In Advances in Neural Information Processing Systems (NIPS)", "citeRegEx": "Somani et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Somani et al\\.", "year": 2013}, {"title": "Perseus: Randomized point-based value iteration for POMDPs", "author": ["M. Spaan", "N. Vlassis"], "venue": "J. Artificial Intelligence Research,", "citeRegEx": "Spaan and Vlassis,? \\Q2005\\E", "shortCiteRegEx": "Spaan and Vlassis", "year": 2005}, {"title": "Monte Carlo Bayesian reinforcement learning", "author": ["Y. Wang", "K. Won", "D. Hsu", "W. Lee"], "venue": "In Proc. Int. Conf. on Machine Learning", "citeRegEx": "Wang et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Wang et al\\.", "year": 2012}, {"title": "Probabilistic planning via determinization", "author": ["YE", "HSU SOMANI", "S. LEE Yoon", "A. Fern", "R. Givan", "S. Kambhampati"], "venue": null, "citeRegEx": "YE et al\\.,? \\Q2008\\E", "shortCiteRegEx": "YE et al\\.", "year": 2008}], "referenceMentions": [{"referenceID": 24, "context": "There has been substantial progress in the last decade (Pineau, Gordon, & Thrun, 2003; Smith & Simmons, 2004; Poupart, 2005; Kurniawati, Hsu, & Lee, 2008; Silver & Veness, 2010; Bai, Hsu, Lee, & Ngo, 2011).", "startOffset": 55, "endOffset": 205}, {"referenceID": 23, "context": "Although offline planning algorithms have made major progress in recent years (Pineau et al., 2003; Spaan & Vlassis, 2005; Smith & Simmons, 2005; Kurniawati et al., 2008), they still face significant difficulty in scaling up to very large POMDPs, as they must plan for all beliefs and future contingencies.", "startOffset": 78, "endOffset": 170}, {"referenceID": 17, "context": "Although offline planning algorithms have made major progress in recent years (Pineau et al., 2003; Spaan & Vlassis, 2005; Smith & Simmons, 2005; Kurniawati et al., 2008), they still face significant difficulty in scaling up to very large POMDPs, as they must plan for all beliefs and future contingencies.", "startOffset": 78, "endOffset": 170}, {"referenceID": 26, "context": "A recent survey lists three main ideas for online planning via belief tree search (Ross et al., 2008): heuristic search, branch-and-bound pruning, and Monte Carlo sampling.", "startOffset": 82, "endOffset": 101}, {"referenceID": 26, "context": "Heuristic search employs a heuristic to guide the belief tree search (Ross & Chaib-Draa, 2007; Ross et al., 2008).", "startOffset": 69, "endOffset": 113}, {"referenceID": 15, "context": "The sparse sampling (SS) algorithm (Kearns et al., 2002) and the DESPOT algorithm both construct sparse approximations to a belief tree.", "startOffset": 35, "endOffset": 56}, {"referenceID": 23, "context": "A recent survey lists three main ideas for online planning via belief tree search (Ross et al., 2008): heuristic search, branch-and-bound pruning, and Monte Carlo sampling. Heuristic search employs a heuristic to guide the belief tree search (Ross & Chaib-Draa, 2007; Ross et al., 2008). This idea dates back to the early work of Satia and Lave (1973) .", "startOffset": 83, "endOffset": 352}, {"referenceID": 23, "context": "A recent survey lists three main ideas for online planning via belief tree search (Ross et al., 2008): heuristic search, branch-and-bound pruning, and Monte Carlo sampling. Heuristic search employs a heuristic to guide the belief tree search (Ross & Chaib-Draa, 2007; Ross et al., 2008). This idea dates back to the early work of Satia and Lave (1973) . Branch-and-bound pruning maintains upper and lower bounds on the value at each belief tree node and use them to prune suboptimal subtrees and improve computational efficiency (Paquet, Tobin, & Chaib-Draa, 2005). This idea is also present in earlier work on offline POMDP planning (Smith & Simmons, 2004). Monte Carlo sampling explores only a randomly sampled subset of observation branches at each node of the belief tree (Bertsekas & Castanon, 1999; Yoon, Fern, Givan, & Kambhampati, 2008; Kearns, Mansour, & Ng, 2002; Silver & Veness, 2010). Our DESPOT algorithm contains all three ideas, but is most closely associated with Monte Carlo sampling. Below we examine some of the earlier Monte Carlo sampling algorithms and DESPOT\u2019s connection with them. The rollout algorithm (Bertsekas & Castanon, 1999) is an early example of Monte Carlo sampling for planning under uncertainty. It is originally designed for Markov decision processes (MDPs), but can be easily adapted to solve POMDPs as well. It estimates the value of a default heuristic policy by performing K simulations and then chooses the best action by one-step lookahead search over the estimated values. Although a one-step lookahead policy improves over the default policy, it may be far from the optimum because of the very short, one-step search horizon. Like the rollout algorithm, the hindsight optimization algorithm (HO) (Chong, Givan, & Chang, 2000; Yoon et al., 2008) is intended for MDPs, but can be adapted for POMDPs. While both HO and DESPOT sample K scenarios for planning, HO builds one tree with O(|A|D) nodes for each scenario, independent of others, and thus K trees in total. It searches each tree for an optimal plan and averages the values of theseK optimal plans to choose a best action. HO and related algorithms have been quite successful in recent international probabilistic planning competitions. However, HO plans for each scenario independently; it optimizes an upper bound on the value of a POMDP and not the true value itself. In contrast, DESPOT captures allK scenarios in a single tree ofO(|A|DK) nodes and hedges against all K scenarios simultaneously during the planning. It converges to the true optimal value of the POMDP as K grows. The work of Kearns, Mansour, and Ng (1999) and that of Ng and Jordan (2000) use sampled scenarios as well, but for offline POMDP policy computation.", "startOffset": 83, "endOffset": 2629}, {"referenceID": 18, "context": "The work of Kearns, Mansour, and Ng (1999) and that of Ng and Jordan (2000) use sampled scenarios as well, but for offline POMDP policy computation.", "startOffset": 55, "endOffset": 76}, {"referenceID": 14, "context": "The bound benefits from the existence of a small near-optimal policy and naturally leads to a regularized objective function for online planning; in contrast, the algorithms by Kearns et al. (1999) and Ng and Jordan (2000) do not exploit the existence of good small policies within the class of policies under consideration.", "startOffset": 177, "endOffset": 198}, {"referenceID": 14, "context": "The bound benefits from the existence of a small near-optimal policy and naturally leads to a regularized objective function for online planning; in contrast, the algorithms by Kearns et al. (1999) and Ng and Jordan (2000) do not exploit the existence of good small policies within the class of policies under consideration.", "startOffset": 177, "endOffset": 223}, {"referenceID": 17, "context": "It selects the action by incrementally constructing a DESPOT D rooted at the current belief b0, using heuristic search (Smith & Simmons, 2004; Kurniawati et al., 2008), and approximating the optimal RWDU \u03bd(b0).", "startOffset": 119, "endOffset": 167}, {"referenceID": 33, "context": "This algorithm differs from an earlier version (Somani et al., 2013) in a subtle, but important way.", "startOffset": 47, "endOffset": 68}, {"referenceID": 30, "context": "5, the work of Smith and Simmons (2005) and the work of Kurniawati et al.", "startOffset": 15, "endOffset": 40}, {"referenceID": 17, "context": "5, the work of Smith and Simmons (2005) and the work of Kurniawati et al. (2008) for justifications of this strategy.", "startOffset": 56, "endOffset": 81}, {"referenceID": 17, "context": "We compare DESPOT with SARSOP (Kurniawati et al., 2008), AEMS2 (Ross & Chaib-Draa, 2007; Ross et al.", "startOffset": 30, "endOffset": 55}, {"referenceID": 26, "context": ", 2008), AEMS2 (Ross & Chaib-Draa, 2007; Ross et al., 2008), and POMCP (Silver & Veness, 2010).", "startOffset": 15, "endOffset": 59}, {"referenceID": 17, "context": "We used the APPL package for SARSOP (Kurniawati et al., 2008).", "startOffset": 36, "endOffset": 61}, {"referenceID": 17, "context": "95 as in SARSOP (Kurniawati et al., 2008).", "startOffset": 16, "endOffset": 41}, {"referenceID": 26, "context": "We thus report both, with the results in earlier work (Ross et al., 2008; Silver & Veness, 2010) in parentheses.", "startOffset": 54, "endOffset": 96}, {"referenceID": 23, "context": "Tag is a standard POMDP benchmark introduced by Pineau et al. (2003). A robot and a target operate in a grid with 29 possible positions (Figure 4a).", "startOffset": 48, "endOffset": 69}, {"referenceID": 3, "context": "By Cayley\u2019s formula (1889), the number of trees with i labeled nodes is ii\u22122, thus |\u03a0i| \u2264 i(i\u22122).", "startOffset": 3, "endOffset": 27}, {"referenceID": 3, "context": "By Cayley\u2019s formula (1889), the number of trees with i labeled nodes is ii\u22122, thus |\u03a0i| \u2264 i(i\u22122). Therefore |\u03a0i| \u2264 ii\u22122 \u00b7 |A|i \u00b7 |Z|i\u22121 \u2264 ii\u22122(|A||Z|)i. In the following, we often abbreviate V\u03c0(b0) and V\u0302\u03c0(b0) as V\u03c0 and V\u0302\u03c0 respectively, since we will only consider the true and empirical values for a fixed but arbitrary b0. Our proof follows a line of reasoning similar to that of Wang, Won, Hsu, and Lee (2012).", "startOffset": 3, "endOffset": 414}, {"referenceID": 12, "context": "Hoeffding\u2019s inequality (1963) gives us", "startOffset": 0, "endOffset": 30}], "year": 2017, "abstractText": "The partially observable Markov decision process (POMDP) provides a principled general framework for planning under uncertainty, but solving POMDPs optimally is computationally intractable, due to the \u201ccurse of dimensionality\u201d and the \u201ccurse of history\u201d. To overcome these challenges, we introduce the Determinized Sparse Partially Observable Tree (DESPOT), a sparse approximation of the standard belief tree, for online planning under uncertainty. A DESPOT focuses online planning on a set of randomly sampled scenarios and compactly captures the \u201cexecution\u201d of all policies under these scenarios. We show that the best policy obtained from a DESPOT is near-optimal, with a regret bound that depends on the representation size of the optimal policy. Leveraging this result, we give an anytime online planning algorithm, which searches a DESPOT for a policy that optimizes a regularized objective function. Regularization balances the estimated value of a policy under the sampled scenarios and the policy size, thus avoiding overfitting. The algorithm demonstrates strong experimental results, compared with some of the best online POMDP algorithms available. It has also been incorporated into an autonomous driving system for real-time vehicle control. The source code for the algorithm is available online.", "creator": "TeX"}}}