{"id": "1609.04309", "review": {"conference": "ICML", "VERSION": "v1", "DATE_OF_SUBMISSION": "14-Sep-2016", "title": "Efficient softmax approximation for GPUs", "abstract": "We propose an approximate strategy to efficiently train neural network based language models over very large vocabularies. Our approach, called adaptive softmax, circumvents the linear dependency on the vocabulary size by exploiting the unbalanced word distribution to form clusters that explicitly minimize the expectation of computational complexity. Our approach further reduces the computational cost by exploiting the specificities of modern architectures and matrix-matrix vector operations, making it particularly suited for graphical processing units. Our experiments carried out on standard benchmarks, such as EuroParl and One Billion Word, show that our approach brings a large gain in efficiency over standard approximations while achieving an accuracy close to that of the full softmax.", "histories": [["v1", "Wed, 14 Sep 2016 15:15:08 GMT  (116kb,D)", "http://arxiv.org/abs/1609.04309v1", null], ["v2", "Tue, 13 Dec 2016 21:42:39 GMT  (121kb,D)", "http://arxiv.org/abs/1609.04309v2", null], ["v3", "Mon, 19 Jun 2017 16:33:04 GMT  (1887kb,D)", "http://arxiv.org/abs/1609.04309v3", "Accepted to ICML 2017"]], "reviews": [], "SUBJECTS": "cs.CL cs.LG", "authors": ["edouard grave", "armand joulin", "moustapha ciss\u00e9", "david grangier", "herv\u00e9 j\u00e9gou"], "accepted": true, "id": "1609.04309"}, "pdf": {"name": "1609.04309.pdf", "metadata": {"source": "CRF", "title": "Efficient softmax approximation for GPUs", "authors": ["\u00c9douard Grave", "Armand Joulin", "Moustapha Ciss\u00e9", "David Grangier", "Herv\u00e9 J\u00e9gou"], "emails": ["egrave@fb.com", "ajoulin@fb.com", "moustaphacisse@fb.com", "grangier@fb.com", "rvj@fb.com"], "sections": [{"heading": null, "text": "Our approach, called adaptive softmax, circumvents the linear dependence on vocabulary size by exploiting the unbalanced word distribution to form clusters that explicitly minimize the expectation of computational complexity. Our approach further reduces computational costs by taking advantage of the specifics of modern architectures and matrix-matrix vector operations, making it particularly suitable for graphical processing units. Our experiments, conducted with standard benchmarks such as EuroParl and One Billion Word, show that our approach delivers great efficiency gains over standard approximations, while achieving accuracy close to full Softmax."}, {"heading": "1 Introduction", "text": "In fact, we are able to assert ourselves, we are able to assert ourselves, we are able to assert ourselves in the region."}, {"heading": "2 Related work", "text": "This year, it is as far as ever in the history of the city, where it is as far as never before."}, {"heading": "3 Preliminaries on language modeling", "text": "The goal of language modeling is to learn a probability distribution via a sequence of words from a given dictionary. Common distribution is defined as the product of the conditional distribution of symbols that have not been considered in their past. (1) This problem is traditionally addressed with non-parameter models based on counting statistics. (2) In particular, the sophisticated N-gram models [2, 23, 24] perform well in practice, especially when associated with cache models. (1) Recently, parametric models based on neural networks have gained popularity. (3, 22, 33) They are mostly either feedforward networks [33] or urrent networks."}, {"heading": "4 Our approach: the adaptive softmax", "text": "In this section, we propose adaptive Softmax, a simple acceleration technology for calculating probability distributions across words. Adaptive Softmax is inspired by the class-based hierarchical Softmax, in which word classes are built to minimize computational complexity. Our method is designed to be efficient for GPUs commonly used to form neural networks. To clarify the intuition behind our method, we present it first in the simple case, where we simply divide our dictionary into two different clusters, before analyzing a more general case."}, {"heading": "4.1 GPU computational model", "text": "The bottleneck of the model described in the previous section is the matrix multiplication between the matrix representing the hidden states (the size B \u00b7 d, where B denotes the stack size) and the matrix of the word representations of the size d \u00b7 k. For a fixed size d of the hidden layer, we denote the complexity of this multiplication with g (k, B, d) and simplify notation wherever some parameters are set. Figure 1 reports empirical timings as a function of k for typical parameters of B > k0 and d for two GPU models, namely K40 and Maxwell. We observe that the complexity g (k) is constant for low values of k until a certain inflection point k0 \u2248 50, and then affin relation to values k > k0 affin such a cost function of the form g (k) = max (c + perck0, c + ecuk), hierarchical (0 \u2212 kk)."}, {"heading": "4.2 Intuition: the two-clusters case", "text": "This year it is more than ever before."}, {"heading": "4.3 General case", "text": "Let us now consider the more general case in which the dictatorial system is structured as V = J + J = V1. (PJ + Vj = 6) We will look at the hierarchical model shown in Figure 2, where the subdictatorial Vh is retrieved at the first level, and the others at the second level. We will now examine the calculation costs C of the prediction (equivalent, backwards). We will denote from pi = w the probability P (w) and the dimensionality d of the hidden layer to analyze complexity as a function of the subdictatorial quantities and probabilities. (We will denote from pi = the probability P (w) and the dimensionality d of the hidden layer d of the hidden layer to analyze complexity as a function of the subdictatorial quantities and probabilities. (The expected cost C is decomposed as C = Ch)."}, {"heading": "5 Experiments", "text": "This chapter presents a series of experiments aimed at analyzing the trade-off between the actual complexity and effectiveness of different strategies, in particular the approach presented in the previous section. First, we describe our evaluation protocol, then we evaluate some of the characteristics of our model, and finally we compare it against standard benchmarks against standard baselines. We evaluate our method on standard datasets and use perplexity (ppl) as a benchmark, as a function of training time or the number of training dates (epochs). The datasets have different vocabulary sizes, in different languages, which allow us to better understand the strengths and weaknesses of the different approaches. Text81 is a standard compression dataset containing a pre-edited version of the first 100 million characters from Wikipedia in English."}, {"heading": "6 Conclusion", "text": "In this paper, we have proposed a simple but efficient approach to the Softmax classifier, which, to our knowledge, is the first speed-optimizing approach that achieves performance equivalent to the exact model, by explicitly taking into account the computational complexity of parallel systems and combining it with a few important observations, namely maintaining a short list of common words in the root node [39] and reducing the capacity of rare words [8]. In all our experiments with GPU, our method consistently maintains low perplexity while maintaining a speed of 2 x to 10 x compared to the exact model. This type of acceleration enables us to deal with extremely large companies in a reasonable amount of time and without the need for a large number of GPUs. We believe that our approach is general enough to be applicable to other parallel computing architectures and other losses, as well as to other areas where class distributions are unbalanced."}, {"heading": "Acknowledgments", "text": "The authors would like to thank Jeff Johnson for his help with GPU benchmarking and Tomas Mikolov for insightful discussions."}], "references": [{"title": "When and why are log-linear models self-normalizing", "author": ["J. Andreas", "D. Klein"], "venue": "ACL,", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2014}, {"title": "A maximum likelihood approach to continuous speech recognition", "author": ["L.R. Bahl", "F. Jelinek", "R.L. Mercer"], "venue": "PAMI,", "citeRegEx": "2", "shortCiteRegEx": null, "year": 1983}, {"title": "Neural probabilistic language models", "author": ["Y. Bengio", "H. Schwenk", "J.-S. Sen\u00e9cal", "F. Morin", "J.-L. Gauvain"], "venue": "Innovations in Machine Learning.", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2006}, {"title": "Adaptive importance sampling to accelerate training of a neural probabilistic language model", "author": ["Y. Bengio", "J.-S. Sen\u00e9cal"], "venue": "Neural Networks,", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2008}, {"title": "Quick training of probabilistic neural nets by importance sampling", "author": ["Y. Bengio", "J.-S. Sen\u00e9cal"], "venue": "In AISTATS,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2003}, {"title": "Class-based n-gram models of natural language", "author": ["P.F. Brown", "P.V. Desouza", "R.L. Mercer", "V.J.D. Pietra", "J.C. Lai"], "venue": "Computational linguistics,", "citeRegEx": "6", "shortCiteRegEx": null, "year": 1992}, {"title": "One billion word benchmark for measuring progress in statistical language modeling", "author": ["C. Chelba", "T. Mikolov", "M. Schuster", "Q. Ge", "T. Brants", "P. Koehn", "T. Robinson"], "venue": "arXiv preprint arXiv:1312.3005,", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2013}, {"title": "Strategies for training large vocabulary neural language models", "author": ["W. Chen", "D. Grangier", "M. Auli"], "venue": "arXiv preprint arXiv:1512.04906,", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2015}, {"title": "Empirical evaluation of gated recurrent neural networks on sequence modeling", "author": ["J. Chung", "C. Gulcehre", "K. Cho", "Y. Bengio"], "venue": "arXiv preprint arXiv:1412.3555,", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2014}, {"title": "Fast and robust neural network joint models for statistical machine translation", "author": ["J. Devlin", "R. Zbib", "Z. Huang", "T. Lamar", "R.M. Schwartz", "J. Makhoul"], "venue": "ACL,", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2014}, {"title": "Adaptive subgradient methods for online learning and stochastic optimization", "author": ["J. Duchi", "E. Hazan", "Y. Singer"], "venue": "JMLR,", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2011}, {"title": "Finding structure in time", "author": ["J.L. Elman"], "venue": "Cognitive science,", "citeRegEx": "12", "shortCiteRegEx": null, "year": 1990}, {"title": "Classes for fast maximum entropy training", "author": ["J. Goodman"], "venue": "ICASSP,", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2001}, {"title": "A bit of progress in language modeling", "author": ["J.T. Goodman"], "venue": "Computer Speech & Language,", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2001}, {"title": "Speech recognition with deep recurrent neural networks", "author": ["A. Graves", "A.-r. Mohamed", "G. Hinton"], "venue": "In ICASSP,", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2013}, {"title": "Noise-contrastive estimation: A new estimation principle for unnormalized statistical models", "author": ["M. Gutmann", "A. Hyv\u00e4rinen"], "venue": "International Conference on Artificial Intelligence and Statistics,", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2010}, {"title": "Deep neural networks for acoustic modeling in speech recognition: The shared views of four research groups", "author": ["G. Hinton", "L. Deng", "D. Yu", "G.E. Dahl", "A.-r. Mohamed", "N. Jaitly", "A. Senior", "V. Vanhoucke", "P. Nguyen", "T.N. Sainath"], "venue": "Signal Processing Magazine,", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2012}, {"title": "Long short-term memory", "author": ["S. Hochreiter", "J. Schmidhuber"], "venue": "Neural computation,", "citeRegEx": "18", "shortCiteRegEx": null, "year": 1997}, {"title": "On using very large target vocabulary for neural machine translation", "author": ["S. Jean", "K. Cho", "R. Memisevic", "Y. Bengio"], "venue": null, "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2015}, {"title": "Blackout: Speeding up recurrent neural network language models with very large vocabularies", "author": ["S. Ji", "S. Vishwanathan", "N. Satish", "M.J. Anderson", "P. Dubey"], "venue": "arXiv preprint arXiv:1511.06909,", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2015}, {"title": "Learning visual features from large weakly supervised data", "author": ["A. Joulin", "L. van der Maaten", "A. Jabri", "N. Vasilache"], "venue": "arXiv preprint arXiv:1511.02251,", "citeRegEx": "21", "shortCiteRegEx": "21", "year": 2015}, {"title": "Exploring the limits of language modeling", "author": ["R. Jozefowicz", "O. Vinyals", "M. Schuster", "N. Shazeer", "Y. Wu"], "venue": "arXiv preprint arXiv:1602.02410,", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2016}, {"title": "Estimation of probabilities from sparse data for the language model component of a speech recognizer", "author": ["S.M. Katz"], "venue": "ICASSP,", "citeRegEx": "23", "shortCiteRegEx": null, "year": 1987}, {"title": "Improved backing-off for m-gram language modeling", "author": ["R. Kneser", "H. Ney"], "venue": "ICASSP,", "citeRegEx": "24", "shortCiteRegEx": null, "year": 1995}, {"title": "Europarl: A parallel corpus for statistical machine translation", "author": ["P. Koehn"], "venue": "MT summit,", "citeRegEx": "25", "shortCiteRegEx": null, "year": 2005}, {"title": "A cache-based natural language model for speech recognition", "author": ["R. Kuhn", "R. De Mori"], "venue": "PAMI,", "citeRegEx": "26", "shortCiteRegEx": null, "year": 1990}, {"title": "Structured output layer neural network language model", "author": ["H.-S. Le", "I. Oparin", "A. Allauzen", "J.-L. Gauvain", "F. Yvon"], "venue": "ICASSP,", "citeRegEx": "27", "shortCiteRegEx": null, "year": 2011}, {"title": "A simple way to initialize recurrent networks of rectified linear units", "author": ["Q.V. Le", "N. Jaitly", "G.E. Hinton"], "venue": "arXiv preprint arXiv:1504.00941,", "citeRegEx": "28", "shortCiteRegEx": null, "year": 2015}, {"title": "Efficient estimation of word representations in vector space", "author": ["T. Mikolov", "K. Chen", "G. Corrado", "J. Dean"], "venue": "arXiv preprint arXiv:1301.3781,", "citeRegEx": "29", "shortCiteRegEx": null, "year": 2013}, {"title": "Empirical evaluation and combination of advanced language modeling techniques", "author": ["T. Mikolov", "A. Deoras", "S. Kombrink", "L. Burget", "J. Cernock\u1ef3"], "venue": "INTERSPEECH,", "citeRegEx": "30", "shortCiteRegEx": null, "year": 2011}, {"title": "Strategies for training large scale neural network language models", "author": ["T. Mikolov", "A. Deoras", "D. Povey", "L. Burget", "J. \u010cernock\u1ef3"], "venue": "ASRU,", "citeRegEx": "31", "shortCiteRegEx": null, "year": 2011}, {"title": "Learning longer memory in recurrent neural networks", "author": ["T. Mikolov", "A. Joulin", "S. Chopra", "M. Mathieu", "M. Ranzato"], "venue": "arXiv preprint arXiv:1412.7753,", "citeRegEx": "32", "shortCiteRegEx": null, "year": 2014}, {"title": "Recurrent neural network based language model", "author": ["T. Mikolov", "M. Karafi\u00e1t", "L. Burget", "J. Cernock\u1ef3", "S. Khudanpur"], "venue": "INTERSPEECH,", "citeRegEx": "33", "shortCiteRegEx": null, "year": 2010}, {"title": "Extensions of recurrent neural network language model", "author": ["T. Mikolov", "S. Kombrink", "L. Burget", "J.H. \u010cernock\u1ef3", "S. Khudanpur"], "venue": "ICASSP,", "citeRegEx": "34", "shortCiteRegEx": null, "year": 2011}, {"title": "Context dependent recurrent neural network language model", "author": ["T. Mikolov", "G. Zweig"], "venue": "SLT,", "citeRegEx": "35", "shortCiteRegEx": null, "year": 2012}, {"title": "A scalable hierarchical distributed language model", "author": ["A. Mnih", "G.E. Hinton"], "venue": "NIPS,", "citeRegEx": "36", "shortCiteRegEx": null, "year": 2009}, {"title": "A fast and simple algorithm for training neural probabilistic language models", "author": ["A. Mnih", "Y.W. Teh"], "venue": "arXiv preprint arXiv:1206.6426,", "citeRegEx": "37", "shortCiteRegEx": null, "year": 2012}, {"title": "Hierarchical probabilistic neural network language model", "author": ["F. Morin", "Y. Bengio"], "venue": "Aistats,", "citeRegEx": "38", "shortCiteRegEx": null, "year": 2005}, {"title": "Continuous space language models", "author": ["H. Schwenk"], "venue": "Computer Speech & Language, pages 492\u2013518,", "citeRegEx": "39", "shortCiteRegEx": null, "year": 2007}, {"title": "Large, pruned or continuous space language models on a gpu for statistical machine translation", "author": ["H. Schwenk", "A. Rousseau", "M. Attik"], "venue": "NAACL-HLT Workshop,", "citeRegEx": "40", "shortCiteRegEx": null, "year": 2012}, {"title": "Sparse non-negative matrix language modeling for skip-grams", "author": ["N. Shazeer", "J. Pelemans", "C. Chelba"], "venue": "Proceedings of Interspeech, pages 1428\u20131432,", "citeRegEx": "41", "shortCiteRegEx": null, "year": 2015}, {"title": "Sequence to sequence learning with neural networks", "author": ["I. Sutskever", "O. Vinyals", "Q.V. Le"], "venue": "Advances in neural information processing systems,", "citeRegEx": "42", "shortCiteRegEx": null, "year": 2014}, {"title": "Decoding with large-scale neural language models improves translation", "author": ["A. Vaswani", "Y. Zhao", "V. Fossum", "D. Chiang"], "venue": "EMNLP,", "citeRegEx": "43", "shortCiteRegEx": null, "year": 2013}, {"title": "Efficient exact gradient update for training deep networks with very large sparse targets", "author": ["P. Vincent", "A. de Br\u00e9bisson", "X. Bouthillier"], "venue": "In NIPS,", "citeRegEx": "44", "shortCiteRegEx": "44", "year": 2015}, {"title": "Backpropagation through time: what it does and how to do", "author": ["P.J. Werbos"], "venue": null, "citeRegEx": "45", "shortCiteRegEx": "45", "year": 1990}, {"title": "An efficient gradient-based algorithm for on-line training of recurrent network trajectories", "author": ["R.J. Williams", "J. Peng"], "venue": "Neural computation,", "citeRegEx": "46", "shortCiteRegEx": null, "year": 1990}, {"title": "Human behavior and the principle of least effort", "author": ["G.K. Zipf"], "venue": null, "citeRegEx": "47", "shortCiteRegEx": "47", "year": 1949}, {"title": "Speed regularization and optimality in word classing", "author": ["G. Zweig", "K. Makarychev"], "venue": "ICASSP,", "citeRegEx": "48", "shortCiteRegEx": null, "year": 2013}], "referenceMentions": [{"referenceID": 39, "context": "This problem is key to natural language processing, with applications in machine translation [40, 42, 43] or automatic speech recognition [15, 17].", "startOffset": 93, "endOffset": 105}, {"referenceID": 41, "context": "This problem is key to natural language processing, with applications in machine translation [40, 42, 43] or automatic speech recognition [15, 17].", "startOffset": 93, "endOffset": 105}, {"referenceID": 42, "context": "This problem is key to natural language processing, with applications in machine translation [40, 42, 43] or automatic speech recognition [15, 17].", "startOffset": 93, "endOffset": 105}, {"referenceID": 14, "context": "This problem is key to natural language processing, with applications in machine translation [40, 42, 43] or automatic speech recognition [15, 17].", "startOffset": 138, "endOffset": 146}, {"referenceID": 16, "context": "This problem is key to natural language processing, with applications in machine translation [40, 42, 43] or automatic speech recognition [15, 17].", "startOffset": 138, "endOffset": 146}, {"referenceID": 21, "context": "In particular, Neural Network Language Models (NNLMs) have received a renewed interest in recent years, by achieving state of the art performance on standard benchmarks [22, 33].", "startOffset": 169, "endOffset": 177}, {"referenceID": 32, "context": "In particular, Neural Network Language Models (NNLMs) have received a renewed interest in recent years, by achieving state of the art performance on standard benchmarks [22, 33].", "startOffset": 169, "endOffset": 177}, {"referenceID": 1, "context": "These approaches are more costly but generalize better than traditional non-parametric models [2, 24].", "startOffset": 94, "endOffset": 101}, {"referenceID": 23, "context": "These approaches are more costly but generalize better than traditional non-parametric models [2, 24].", "startOffset": 94, "endOffset": 101}, {"referenceID": 1, "context": "Statistical language models assign a probability to words given their history [2].", "startOffset": 78, "endOffset": 81}, {"referenceID": 6, "context": "For instance, the vocabulary of the One Billion Word benchmark [7] contains around 800K words.", "startOffset": 63, "endOffset": 66}, {"referenceID": 2, "context": "In standard NNLMs, such as feedforward networks [3] or recurrent networks [33], computing this probability over the whole vocabulary is the bottleneck.", "startOffset": 48, "endOffset": 51}, {"referenceID": 32, "context": "In standard NNLMs, such as feedforward networks [3] or recurrent networks [33], computing this probability over the whole vocabulary is the bottleneck.", "startOffset": 74, "endOffset": 78}, {"referenceID": 4, "context": "Many solutions have been proposed to reduce the complexity of this expensive step [5, 13, 16].", "startOffset": 82, "endOffset": 93}, {"referenceID": 12, "context": "Many solutions have been proposed to reduce the complexity of this expensive step [5, 13, 16].", "startOffset": 82, "endOffset": 93}, {"referenceID": 15, "context": "Many solutions have been proposed to reduce the complexity of this expensive step [5, 13, 16].", "startOffset": 82, "endOffset": 93}, {"referenceID": 4, "context": "We distinguish (i) the methods that consider the original distribution and aim at providing approximations of the probabilities, or of a subset of them [5, 20], from (ii) the approaches that compute exact probabilities for an approximate model yielding a lower computational cost, such as the popular hierarchical softmax [13, 36, 38].", "startOffset": 152, "endOffset": 159}, {"referenceID": 19, "context": "We distinguish (i) the methods that consider the original distribution and aim at providing approximations of the probabilities, or of a subset of them [5, 20], from (ii) the approaches that compute exact probabilities for an approximate model yielding a lower computational cost, such as the popular hierarchical softmax [13, 36, 38].", "startOffset": 152, "endOffset": 159}, {"referenceID": 12, "context": "We distinguish (i) the methods that consider the original distribution and aim at providing approximations of the probabilities, or of a subset of them [5, 20], from (ii) the approaches that compute exact probabilities for an approximate model yielding a lower computational cost, such as the popular hierarchical softmax [13, 36, 38].", "startOffset": 322, "endOffset": 334}, {"referenceID": 35, "context": "We distinguish (i) the methods that consider the original distribution and aim at providing approximations of the probabilities, or of a subset of them [5, 20], from (ii) the approaches that compute exact probabilities for an approximate model yielding a lower computational cost, such as the popular hierarchical softmax [13, 36, 38].", "startOffset": 322, "endOffset": 334}, {"referenceID": 37, "context": "We distinguish (i) the methods that consider the original distribution and aim at providing approximations of the probabilities, or of a subset of them [5, 20], from (ii) the approaches that compute exact probabilities for an approximate model yielding a lower computational cost, such as the popular hierarchical softmax [13, 36, 38].", "startOffset": 322, "endOffset": 334}, {"referenceID": 4, "context": "Many methods have been proposed to approximate the softmax efficiently [5, 13, 16, 38].", "startOffset": 71, "endOffset": 86}, {"referenceID": 12, "context": "Many methods have been proposed to approximate the softmax efficiently [5, 13, 16, 38].", "startOffset": 71, "endOffset": 86}, {"referenceID": 15, "context": "Many methods have been proposed to approximate the softmax efficiently [5, 13, 16, 38].", "startOffset": 71, "endOffset": 86}, {"referenceID": 37, "context": "Many methods have been proposed to approximate the softmax efficiently [5, 13, 16, 38].", "startOffset": 71, "endOffset": 86}, {"referenceID": 7, "context": "[8] for a comparative study.", "startOffset": 0, "endOffset": 3}, {"referenceID": 30, "context": "For the sake of completeness, we refer the reader to other strategies that can speed-up the training of language models in complementary manners [31].", "startOffset": 145, "endOffset": 149}, {"referenceID": 12, "context": "The Hierarchical Softmax (HSM) is an approximation of the softmax function introduced in Goodman [13].", "startOffset": 97, "endOffset": 101}, {"referenceID": 12, "context": "This approach is generally used with a two-level tree [13, 34] but has also been extended to deeper hierarchies [38, 36].", "startOffset": 54, "endOffset": 62}, {"referenceID": 33, "context": "This approach is generally used with a two-level tree [13, 34] but has also been extended to deeper hierarchies [38, 36].", "startOffset": 54, "endOffset": 62}, {"referenceID": 37, "context": "This approach is generally used with a two-level tree [13, 34] but has also been extended to deeper hierarchies [38, 36].", "startOffset": 112, "endOffset": 120}, {"referenceID": 35, "context": "This approach is generally used with a two-level tree [13, 34] but has also been extended to deeper hierarchies [38, 36].", "startOffset": 112, "endOffset": 120}, {"referenceID": 5, "context": "In general, the hierarchy structure is built on word similarities [6, 27, 29] or frequency binning [34].", "startOffset": 66, "endOffset": 77}, {"referenceID": 26, "context": "In general, the hierarchy structure is built on word similarities [6, 27, 29] or frequency binning [34].", "startOffset": 66, "endOffset": 77}, {"referenceID": 28, "context": "In general, the hierarchy structure is built on word similarities [6, 27, 29] or frequency binning [34].", "startOffset": 66, "endOffset": 77}, {"referenceID": 33, "context": "In general, the hierarchy structure is built on word similarities [6, 27, 29] or frequency binning [34].", "startOffset": 99, "endOffset": 103}, {"referenceID": 28, "context": "[29] proposes an optimal hierarchy by constructing a Huffman coding based on frequency.", "startOffset": 0, "endOffset": 4}, {"referenceID": 47, "context": "[48] constructs their hierachy in order to explicitly reduce the computational complexity.", "startOffset": 0, "endOffset": 4}, {"referenceID": 26, "context": "The idea of keeping a short-list of the most frequent words has been explored before [27, 39].", "startOffset": 85, "endOffset": 93}, {"referenceID": 38, "context": "The idea of keeping a short-list of the most frequent words has been explored before [27, 39].", "startOffset": 85, "endOffset": 93}, {"referenceID": 26, "context": "[27] combines a short-list with a hierachical softmax based on word representation.", "startOffset": 0, "endOffset": 4}, {"referenceID": 7, "context": "[8].", "startOffset": 0, "endOffset": 3}, {"referenceID": 21, "context": "Sampling based approaches have been successfully applied to approximate the softmax function over large dictionaries in different domains, such as language modeling [22], machine translation [19] and computer vision [21].", "startOffset": 165, "endOffset": 169}, {"referenceID": 18, "context": "Sampling based approaches have been successfully applied to approximate the softmax function over large dictionaries in different domains, such as language modeling [22], machine translation [19] and computer vision [21].", "startOffset": 191, "endOffset": 195}, {"referenceID": 20, "context": "Sampling based approaches have been successfully applied to approximate the softmax function over large dictionaries in different domains, such as language modeling [22], machine translation [19] and computer vision [21].", "startOffset": 216, "endOffset": 220}, {"referenceID": 3, "context": "In particular, importance sampling [4, 5] selects a subset of negative targets to approximate the softmax normalization.", "startOffset": 35, "endOffset": 41}, {"referenceID": 4, "context": "In particular, importance sampling [4, 5] selects a subset of negative targets to approximate the softmax normalization.", "startOffset": 35, "endOffset": 41}, {"referenceID": 4, "context": "Different schemes have been proposed for sampling, such as the unigram and bigram distribution [5] or more recently, a power-raised distribution of the unigram [20, 29].", "startOffset": 95, "endOffset": 98}, {"referenceID": 19, "context": "Different schemes have been proposed for sampling, such as the unigram and bigram distribution [5] or more recently, a power-raised distribution of the unigram [20, 29].", "startOffset": 160, "endOffset": 168}, {"referenceID": 28, "context": "Different schemes have been proposed for sampling, such as the unigram and bigram distribution [5] or more recently, a power-raised distribution of the unigram [20, 29].", "startOffset": 160, "endOffset": 168}, {"referenceID": 15, "context": "Popular methods are Noise Contrastive Estimation [16, 37, 43] or a penalization on the normalization function [1, 10].", "startOffset": 49, "endOffset": 61}, {"referenceID": 36, "context": "Popular methods are Noise Contrastive Estimation [16, 37, 43] or a penalization on the normalization function [1, 10].", "startOffset": 49, "endOffset": 61}, {"referenceID": 42, "context": "Popular methods are Noise Contrastive Estimation [16, 37, 43] or a penalization on the normalization function [1, 10].", "startOffset": 49, "endOffset": 61}, {"referenceID": 0, "context": "Popular methods are Noise Contrastive Estimation [16, 37, 43] or a penalization on the normalization function [1, 10].", "startOffset": 110, "endOffset": 117}, {"referenceID": 9, "context": "Popular methods are Noise Contrastive Estimation [16, 37, 43] or a penalization on the normalization function [1, 10].", "startOffset": 110, "endOffset": 117}, {"referenceID": 15, "context": "Noise Contrastive Estimation [16] replaces the softmax by a binary classifier distinguishing the original distribution", "startOffset": 29, "endOffset": 33}, {"referenceID": 36, "context": "While the original formulation still requires to compute the softmax normalization, Mnih and Teh [37] shows that good performance can be achieved even without it.", "startOffset": 97, "endOffset": 101}, {"referenceID": 43, "context": "[44] have also proposed an efficient way to train model with high dimensional output space.", "startOffset": 0, "endOffset": 4}, {"referenceID": 1, "context": "The joint distribution is defined as a product of conditional distribution of tokens given their past [2].", "startOffset": 102, "endOffset": 105}, {"referenceID": 13, "context": "This problem is traditionally addressed with non-parameteric models based on counting statistics [14].", "startOffset": 97, "endOffset": 101}, {"referenceID": 1, "context": "In particular, smoothed N-gram models [2, 23, 24] achieve good performance in practice [30], especially when they are associated with cache models [26].", "startOffset": 38, "endOffset": 49}, {"referenceID": 22, "context": "In particular, smoothed N-gram models [2, 23, 24] achieve good performance in practice [30], especially when they are associated with cache models [26].", "startOffset": 38, "endOffset": 49}, {"referenceID": 23, "context": "In particular, smoothed N-gram models [2, 23, 24] achieve good performance in practice [30], especially when they are associated with cache models [26].", "startOffset": 38, "endOffset": 49}, {"referenceID": 29, "context": "In particular, smoothed N-gram models [2, 23, 24] achieve good performance in practice [30], especially when they are associated with cache models [26].", "startOffset": 87, "endOffset": 91}, {"referenceID": 25, "context": "In particular, smoothed N-gram models [2, 23, 24] achieve good performance in practice [30], especially when they are associated with cache models [26].", "startOffset": 147, "endOffset": 151}, {"referenceID": 2, "context": "More recently, parametric models based on neural networks have gained popularity for language modeling [3, 22, 33].", "startOffset": 103, "endOffset": 114}, {"referenceID": 21, "context": "More recently, parametric models based on neural networks have gained popularity for language modeling [3, 22, 33].", "startOffset": 103, "endOffset": 114}, {"referenceID": 32, "context": "More recently, parametric models based on neural networks have gained popularity for language modeling [3, 22, 33].", "startOffset": 103, "endOffset": 114}, {"referenceID": 2, "context": "They are mostly either feedforward networks [3] or recurrent networks [33].", "startOffset": 44, "endOffset": 47}, {"referenceID": 32, "context": "They are mostly either feedforward networks [3] or recurrent networks [33].", "startOffset": 70, "endOffset": 74}, {"referenceID": 10, "context": "This model is parameterized by the weight matrices P , A and B and is routinely learned with an optimization scheme such as stochastic gradient descent or Adagrad [11].", "startOffset": 163, "endOffset": 167}, {"referenceID": 11, "context": "A Recurrent network [12] extends a feedforward network in that the current state of the hidden layer also depends on its previous state.", "startOffset": 20, "endOffset": 24}, {"referenceID": 44, "context": "Computing the exact gradient for this model is challenging but it is possible to compute an efficient and stable approximation of it, using a truncated back-propagation through time [45, 46] and norm clipping [33].", "startOffset": 182, "endOffset": 190}, {"referenceID": 45, "context": "Computing the exact gradient for this model is challenging but it is possible to compute an efficient and stable approximation of it, using a truncated back-propagation through time [45, 46] and norm clipping [33].", "startOffset": 182, "endOffset": 190}, {"referenceID": 32, "context": "Computing the exact gradient for this model is challenging but it is possible to compute an efficient and stable approximation of it, using a truncated back-propagation through time [45, 46] and norm clipping [33].", "startOffset": 209, "endOffset": 213}, {"referenceID": 11, "context": "Since the model introduced by Elman [12], many extensions have been proposed, such as Longer Short Term Memory (LSTM) [18], Gated recurrent units [9] or structurally constrained network [32].", "startOffset": 36, "endOffset": 40}, {"referenceID": 17, "context": "Since the model introduced by Elman [12], many extensions have been proposed, such as Longer Short Term Memory (LSTM) [18], Gated recurrent units [9] or structurally constrained network [32].", "startOffset": 118, "endOffset": 122}, {"referenceID": 8, "context": "Since the model introduced by Elman [12], many extensions have been proposed, such as Longer Short Term Memory (LSTM) [18], Gated recurrent units [9] or structurally constrained network [32].", "startOffset": 146, "endOffset": 149}, {"referenceID": 31, "context": "Since the model introduced by Elman [12], many extensions have been proposed, such as Longer Short Term Memory (LSTM) [18], Gated recurrent units [9] or structurally constrained network [32].", "startOffset": 186, "endOffset": 190}, {"referenceID": 21, "context": "These models have been successfully used in the context of language modeling [22, 33, 35].", "startOffset": 77, "endOffset": 89}, {"referenceID": 32, "context": "These models have been successfully used in the context of language modeling [22, 33, 35].", "startOffset": 77, "endOffset": 89}, {"referenceID": 34, "context": "These models have been successfully used in the context of language modeling [22, 33, 35].", "startOffset": 77, "endOffset": 89}, {"referenceID": 21, "context": "In this work, we focus on the standard word level LSTM architecture since it has obtained state of the art performance on the challenging One Billion Word Benchmark [22].", "startOffset": 165, "endOffset": 169}, {"referenceID": 21, "context": "When the vocabulary is large, this step is computationally expensive and often dominates the computation of the whole model [22, 32], as discussed in introduction and related work.", "startOffset": 124, "endOffset": 132}, {"referenceID": 31, "context": "When the vocabulary is large, this step is computationally expensive and often dominates the computation of the whole model [22, 32], as discussed in introduction and related work.", "startOffset": 124, "endOffset": 132}, {"referenceID": 12, "context": "A simple approach [13] to reduce this computational cost is to assign each word w of the vocabulary to a unique class C(w) and to factorize the probability distribution over words as", "startOffset": 18, "endOffset": 22}, {"referenceID": 46, "context": "In natural languages, the distribution of the words notoriously follows a Zipf law [47].", "startOffset": 83, "endOffset": 87}, {"referenceID": 33, "context": "Similar to the frequency binning hierarchical softmax [34], this information can be exploited to reduce the computation cost.", "startOffset": 54, "endOffset": 58}, {"referenceID": 33, "context": "These two clusters can be organized in two different ways: either they are both leaves of a 2-level tree [34], or the head cluster is kept as a short-list in the root node [27].", "startOffset": 105, "endOffset": 109}, {"referenceID": 26, "context": "These two clusters can be organized in two different ways: either they are both leaves of a 2-level tree [34], or the head cluster is kept as a short-list in the root node [27].", "startOffset": 172, "endOffset": 176}, {"referenceID": 7, "context": "[8], we exploit this observation to further reduce the computational cost of our classifier.", "startOffset": 0, "endOffset": 3}, {"referenceID": 7, "context": "Unlike [8], we share the state of hidden layer across clusters and simply reduce the input size of the classifiers by applying a projection matrix.", "startOffset": 7, "endOffset": 10}, {"referenceID": 33, "context": "We observe empirically that putting all the clusters in the leaves of the tree leads to a significant drop of performance (around 5\u2212 10% performance drop) [34, 48].", "startOffset": 155, "endOffset": 163}, {"referenceID": 47, "context": "We observe empirically that putting all the clusters in the leaves of the tree leads to a significant drop of performance (around 5\u2212 10% performance drop) [34, 48].", "startOffset": 155, "endOffset": 163}, {"referenceID": 31, "context": "It has been recently used for language modeling [32] and has a vocabulary of 44k words.", "startOffset": 48, "endOffset": 52}, {"referenceID": 24, "context": "\u2022 Europarl2 is a machine translation corpus, containing 20 languages [25].", "startOffset": 69, "endOffset": 73}, {"referenceID": 6, "context": "[7].", "startOffset": 0, "endOffset": 3}, {"referenceID": 10, "context": "We use Adagrad [11], with a step size of 0.", "startOffset": 15, "endOffset": 19}, {"referenceID": 6, "context": "Interpolated Kneser-Ney 5-gram [7] 67.", "startOffset": 31, "endOffset": 34}, {"referenceID": 7, "context": "6 Feedforward NN + D-Softmax [8] 91.", "startOffset": 29, "endOffset": 32}, {"referenceID": 27, "context": "2 4-layer IRNN-512 [28] 69.", "startOffset": 19, "endOffset": 23}, {"referenceID": 19, "context": "4 RNN-2048 + BlackOut sampling [20] 68.", "startOffset": 31, "endOffset": 35}, {"referenceID": 40, "context": "3 Sparse Non-negative Matrix factorization [41] 52.", "startOffset": 43, "endOffset": 47}, {"referenceID": 6, "context": "9 RNN-1024 + MaxEnt 9-gram [7] 51.", "startOffset": 27, "endOffset": 30}, {"referenceID": 21, "context": "3 2-layer LSTM-8192-1024 + CNN inputs [22] 30.", "startOffset": 38, "endOffset": 42}, {"referenceID": 30, "context": "Our method is compared to: (1) the full softmax, (2) the hierarchical softmax (HSM) with frequency binning [31], (3) importance sampling [5, 4] and (4) the differentiated softmax [8].", "startOffset": 107, "endOffset": 111}, {"referenceID": 4, "context": "Our method is compared to: (1) the full softmax, (2) the hierarchical softmax (HSM) with frequency binning [31], (3) importance sampling [5, 4] and (4) the differentiated softmax [8].", "startOffset": 137, "endOffset": 143}, {"referenceID": 3, "context": "Our method is compared to: (1) the full softmax, (2) the hierarchical softmax (HSM) with frequency binning [31], (3) importance sampling [5, 4] and (4) the differentiated softmax [8].", "startOffset": 137, "endOffset": 143}, {"referenceID": 7, "context": "Our method is compared to: (1) the full softmax, (2) the hierarchical softmax (HSM) with frequency binning [31], (3) importance sampling [5, 4] and (4) the differentiated softmax [8].", "startOffset": 179, "endOffset": 182}, {"referenceID": 7, "context": "For the negative sampling method, we used a number of samples equal to 20% of the size of the vocabulary [8].", "startOffset": 105, "endOffset": 108}, {"referenceID": 7, "context": "[8], where each word cluster uses a disjoint subset of the hidden representation.", "startOffset": 0, "endOffset": 3}, {"referenceID": 7, "context": "[8] obtains similar results but is slower by a factor \u00d71.", "startOffset": 0, "endOffset": 3}, {"referenceID": 21, "context": "[22] achieves a lower perplexity, but with a model 8\u00d7 bigger than ours and trained over 32 GPUs during 3 weeks.", "startOffset": 0, "endOffset": 4}, {"referenceID": 38, "context": "This is achieved by explicitly taking into account the computational complexity of parallel systems and combining it with a few important observations, namely keeping a short-list of frequent words in the root node [39] and reducing the capacity of rare words [8].", "startOffset": 215, "endOffset": 219}, {"referenceID": 7, "context": "This is achieved by explicitly taking into account the computational complexity of parallel systems and combining it with a few important observations, namely keeping a short-list of frequent words in the root node [39] and reducing the capacity of rare words [8].", "startOffset": 260, "endOffset": 263}], "year": 2016, "abstractText": "We propose an approximate strategy to efficiently train neural network based language models over very large vocabularies. Our approach, called adaptive softmax, circumvents the linear dependency on the vocabulary size by exploiting the unbalanced word distribution to form clusters that explicitly minimize the expectation of computational complexity. Our approach further reduces the computational cost by exploiting the specificities of modern architectures and matrix-matrix vector operations, making it particularly suited for graphical processing units. Our experiments carried out on standard benchmarks, such as EuroParl and One Billion Word, show that our approach brings a large gain in efficiency over standard approximations while achieving an accuracy close to that of the full softmax.", "creator": "LaTeX with hyperref package"}}}