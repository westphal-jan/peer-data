{"id": "1609.04508", "review": {"conference": "AAAI", "VERSION": "v1", "DATE_OF_SUBMISSION": "15-Sep-2016", "title": "Column Networks for Collective Classification", "abstract": "Relational learning deals with data that are characterized by relational structures. An important task is collective classification, which is to jointly classify networked objects. While it holds a great promise to produce a better accuracy than non-collective classifiers, collective classification is computational challenging and has not leveraged on the recent breakthroughs of deep learning. We present Column Network (CLN), a novel deep learning model for collective classification in multi-relational domains. CLN has many desirable theoretical properties: (i) it encodes multi-relations between any two instances; (ii) it is deep and compact, allowing complex functions to be approximated at the network level with a small set of free parameters; (iii) local and relational features are learned simultaneously; (iv) long-range, higher-order dependencies between instances are supported naturally; and (v) crucially, learning and inference are efficient, linear in the size of the network and the number of relations. We evaluate CLN on multiple real-world applications: (a) delay prediction in software projects, (b) PubMed Diabetes publication classification and (c) film genre classification. In all applications, CLN demonstrates a higher accuracy than state-of-the-art rivals.", "histories": [["v1", "Thu, 15 Sep 2016 04:45:11 GMT  (726kb,D)", "https://arxiv.org/abs/1609.04508v1", null], ["v2", "Tue, 29 Nov 2016 03:59:26 GMT  (280kb,D)", "http://arxiv.org/abs/1609.04508v2", "Accepted at AAAI'17"]], "reviews": [], "SUBJECTS": "cs.LG cs.AI stat.ML", "authors": ["trang pham", "truyen tran 0001", "dinh q phung", "svetha venkatesh"], "accepted": true, "id": "1609.04508"}, "pdf": {"name": "1609.04508.pdf", "metadata": {"source": "META", "title": "Column Networks for Collective Classification", "authors": ["Trang Pham", "Truyen Tran", "Dinh Phung", "Svetha Venkatesh"], "emails": ["phtra@deakin.edu.au", "truyen.tran@deakin.edu.au", "dinh.phung@deakin.edu.au", "svetha.venkatesh@deakin.edu.au"], "sections": [{"heading": "1 Introduction", "text": "In fact, it is as if most people are able to understand themselves and their environment and to understand what they are doing. (...) It is not as if they are able to change the world. (...) It is as if they are able to change the world. (...) It is as if they are able to change the world. (...) It is as if they are able to change the world. (...) It is as if they are able to change the world. (...) It is as if they are able to change the world. (...) It is as if they are able to change the world. (...) It is as if they are able to change the world. (...) It is as if they are able to change the world. (...) It is as if they are. (...) It is so. \"(...) It is so.\" (...) It is so. \"(...) It is so.\" (...) It is so. \"(...) It is as if it is.\" (...) It is as if it is."}, {"heading": "2 Preliminaries", "text": "Notation convention: We use uppercase letters for matrices and bold lowercase letters for vectors. The sigmoid function of a scalar x is defined as \u03c3 (x) = [1 + exp (\u2212 x)] \u2212 1, x-R. A function g of a vector x is defined as g (x) = (g (x1),..., g (xn). The operator \u0445 is used to denote elemental multiplication. We use uppercase t (e.g. ht) to denote layers or computational steps in neural networks, and subscript i for the ith element in a set (e.g. hti is the hidden activation at the level t of unit i in a graph)."}, {"heading": "2.1 Collective Classification in Multi-relational Setting", "text": "We describe the collective classification setting under multiple relationships. In view of a graph of entities G = {E, R, X, Y}, where E = {e1,..., eN} are N entities that connect through relationships in R. Any entity can be unidirectional or bidirectional. Example: Film A and movie B can be linked by a unidirectional relationship (A, B) and two bidirectional relationships: Same entities and relationships can be represented in an entity graph in which a node exists an entity and an edge between two nodes if they have at least one relationship. Furthermore, ej is a neighbor of ei if there is a connection between ej and ei."}, {"heading": "3.1 Architecture", "text": "Inspired by the column organization in neocortex (Mountcastle 1997), the CLN has a mini-column per entity (or data instance) that resembles a sensory sensory sensation field. Each column is a feedback network that passes information from a lower layer to a higher layer of its own and higher layers of its neighbors (see Fig. 2 for an in-depth discussion). The nature of intercolumn communication is dictated by the relationships between the two entities. Multiple layers establish far-reaching dependencies (see Fig. 3.4 for in-depth discussions), somewhat similar to the strategy used in stacked learning as described in Sec. 2.2. The main difference is that in CLNthe intermediate steps there are no output-class probabilities, but higher abstractions of instance traits and relative traits."}, {"heading": "3.2 Highway Network as Mini-Column", "text": "We are now specifying the detail of a mini-pillar that we are implementing by expanding a recently introduced feedback network called the Highway Network (Srivastava, Greff and Schmidhuber 2015). Remember that traditional feedback networks have great difficulty learning with a large number of layers, due to the nested nonlinear structure that prevents the simple transmission of information and slopes along the arithmetic path. Highway networks solve this problem by partially opening the gate that allows previous states to propagate through layers, as follows: ht = 1 \u0445 h + 2 \u0445 ht \u2212 1 (5), where h-t is a nonlinear candidate function of ht \u2212 1 and where 1, \u03b12 \u0445 (0.1) are learnable gates. Since the gates are never completely closed, data signals and error patterns can spread very far into a deep network."}, {"heading": "3.3 Parameter Sharing for Compactness", "text": "For networks used on the data superhighway, the number of parameters increases with the number of hidden layers. In the CLN, the number is multiplied by the number of relationships (see Equation (4)). In a deep CLN with many relationships, the number of parameters can grow faster than the size of the training data, resulting in overmatch and high memory requirements. To meet this challenge, we are borrowing the idea of parameter sharing in the Recurrent Neural Network (RNN), which means that layers have identical parameters. There is empirical evidence supporting this strategy in non-relational data (Liao and Poggio 2016; Pham et al. 2016). In parameter sharing, the depth of the CLN can grow without increasing the model size, which can lead to good performance on small and disks. See 4."}, {"heading": "3.4 Capturing Long-range Dependencies", "text": "An important feature of our proposed deep CLN is the ability to detect far-reaching dependencies even though only a local exchange of states takes place, as shown in Equation (4,6). Consider the example in Figure 2, where x1 is modeled in h13 and h 1 3 in h24, so even though e1 has no direct connection to e4, but information from e1 is still embedded in h24 to h13. More generally, after k hidden layers, a hidden activation of a unit may contain information about its extended neighbors of radius k. If the number of layers is large, the representation of a unit on the top layer contains not only its local characteristics and its directed neighbors, but also the information of the entire diagram. In highway networks, all these layers of representation are accumulated by layers and used to predict output labels."}, {"heading": "3.5 Training with mini-batch", "text": "As described in paragraph 3.1, hti is a function of h t \u2212 1i and the previous layer of its neighborhood. hti can therefore contain information about the entire graph if the network is deep enough, requiring full-batch training that is expensive and non-scalable. We propose a very simple but efficient approach that allows mini-batch training; for each mini-batch, the neighboring activations are temporarily frozen to scalars, i.e. gradients are not spread through this \"blanket.\" After the parameter update, the activations are recalculated as usual. Experiments showed that the process has been converged and its performance is comparable to the whole-batch training method.4 Experiments and Results In this section, we report on three real applications of CLN to networked data: Software Delay Estimation, PubMed Paper Classification, and Film Genre Classification."}, {"heading": "4.1 Baselines", "text": "For comparison, we used a comprehensive set of basic methods that include: (a) those designed for collective classification, and (b) deep neural networks for non-collective classification; for the former, we used NetKit1, an open source toolkit for networked data classification (Macskassy and Provost 2007); NetKit provides a classification framework consisting of three components: a local classifier, a relational classifier, and a collective inference method; in our experiments, the local classifier is the logistic regression (LR) for all settings; relational classifiers are (i) weighted-vote relational neighbors (wvRN), (ii) logistic regression link-based classifiers with normalized values (nbD), and (ii) logistic regression are nIC-based absolute classifiers with numerical values (C)."}, {"heading": "4.2 Experiment Settings", "text": "We report on three variants of CLN: a basic version that uses standard feedback Neural Network as a mini-column (CLNFNN) and two versions of CLN-HWN that use motorway networks with common parameters (CLN-HWN-full for full-batch mode and CLN-HWN-mini for mini-batch mode, as described in sections 3.2, 3.3 and 3.5).All neural networks use ReLU in the hidden layers. To match hyper parameters, we look for (i) the number of hidden layers: 2, 6, 10,..., 30, (ii) hidden dimensions and (iii) optimizers: Adam or RMSprop."}, {"heading": "4.3 Software Delay Prediction", "text": "This task is to predict a potential delay for a problem, which is a unit of the task in an iterative software development lifecycle (Choetkiertikul et al. 2015). The prediction point is when the problem planning is complete. Due to the interdependencies between the problems, predicting the delay for a problem must take into account all related problems. We use the largest data set reported in (Choetkiertikul et al. 2015), the JBoss, which contains 8,206 problems. Each problem is a vector of 15 features and connects to other problems over 12 relationships (unidirectional such as blocked-by or bidirectional such as the same developer). The task is to predict whether a software problem is at risk of obtaining delays (i.e. binary classification). Figure 3 visualizes CLN-HWN full performance with different number of layers as blocked-by or bidirectional as the developer."}, {"heading": "4.4 PubMed Publication Classification", "text": "We used the Pubmed Diabetes dataset, which consists of 19,717 scientific publications and 44,338 citation links between the3. Each publication is described by a TF / IDF-weighted word vector from a dictionary consisting of 500 unique words. We conducted experiments in which each publication was divided into one of three classes: Diabetes Melitus - Experimental, Diabetes Melitus Type 1 and Diabetes Mellitus Type 2. Visualization of hidden layers We randomly selected 2 samples from each class and visualized their activation of ReLU units through 10 layers of CLN-HWN (Fig. 4). Interestingly, the activation strength appears to increase with higher layers, suggesting that learning characteristics are more differentiated the closer they come to the results. For each class, a number of hidden units in each layer is disabled. Images of samples from the same class show similar patterns, while numbers from samples from the same class are very different."}, {"heading": "4.5 Film Genre Prediction", "text": "The task is to reconcile the various strands and plot lines in the individual plot lines and plot lines of the individual plot lines and plot lines. (...) It is important to reconcile the different plot lines and plot lines of the individual plot lines. (...) It is important to reconcile the different plot lines and plot lines of the individual plot lines and plot lines of the individual plot lines. (...) It is important to reconcile the different plot lines and plot lines of the individual plot lines. (...) It is important to reconcile the plot lines and plot lines of the individual plot lines and plot lines of the individual plot lines of the plot lines. (...) It is important to reconcile the plot lines and plot lines of the plot lines of the plot lines of the plot lines and plot lines of the plot lines of the plot lines. \"(...) It is important to reconcile the different plot lines and plot lines of the plot lines of the plot lines of the plot lines of the plot lines and plot lines of the plot lines of the plot lines of the plot lines."}, {"heading": "Acknowledgement", "text": "Dinh Phung is partially supported by the Australian Research Council as part of the Discovery Project DP150100031."}], "references": [{"title": "Structured prediction energy networks", "author": ["D. Belanger", "A. McCallum"], "venue": "ICML.", "citeRegEx": "Belanger and McCallum,? 2016", "shortCiteRegEx": "Belanger and McCallum", "year": 2016}, {"title": "Predicting delays in software projects using networked classification", "author": ["M. Choetkiertikul", "H.K. Dam", "T. Tran", "A. Ghose"], "venue": "30th IEEE/ACM International Conference on Automated Software Engineering.", "citeRegEx": "Choetkiertikul et al\\.,? 2015", "shortCiteRegEx": "Choetkiertikul et al\\.", "year": 2015}, {"title": "Structured machine learning: the next ten years", "author": ["T.G. Dietterich", "P. Domingos", "L. Getoor", "S. Muggleton", "P. Tadepalli"], "venue": "Machine Learning 73(1):3\u201323.", "citeRegEx": "Dietterich et al\\.,? 2008", "shortCiteRegEx": "Dietterich et al\\.", "year": 2008}, {"title": "Neural conditional random fields", "author": ["T. Do", "T Arti"], "venue": "In International Conference on Artificial Intelligence and Statistics,", "citeRegEx": "Do and Arti,? \\Q2010\\E", "shortCiteRegEx": "Do and Arti", "year": 2010}, {"title": "Structured learning via logistic regression", "author": ["J. Domke"], "venue": "Advances in Neural Information Processing Systems, 647\u2013 655.", "citeRegEx": "Domke,? 2013", "shortCiteRegEx": "Domke", "year": 2013}, {"title": "Using probabilistic relational models for collaborative filtering", "author": ["L. Getoor", "M. Sahami"], "venue": "Workshop on Web Usage Analysis and User Profiling (WEBKDD\u201999).", "citeRegEx": "Getoor and Sahami,? 1999", "shortCiteRegEx": "Getoor and Sahami", "year": 1999}, {"title": "The movielens datasets: History and context", "author": ["F.M. Harper", "J.A. Konstan"], "venue": "ACM Transactions on Interactive Intelligent Systems (TiiS) 5(4):19.", "citeRegEx": "Harper and Konstan,? 2016", "shortCiteRegEx": "Harper and Konstan", "year": 2016}, {"title": "Training products of experts by minimizing contrastive divergence", "author": ["G. Hinton"], "venue": "Neural Computation 14:1771\u20131800.", "citeRegEx": "Hinton,? 2002", "shortCiteRegEx": "Hinton", "year": 2002}, {"title": "Stacked Graphical Models for Efficient Inference in Markov Random Fields", "author": ["Z. Kou", "W.W. Cohen"], "venue": "SDM, 533\u2013538. SIAM.", "citeRegEx": "Kou and Cohen,? 2007", "shortCiteRegEx": "Kou and Cohen", "year": 2007}, {"title": "Conditional random fields: Probabilistic models for segmenting and labeling sequence data", "author": ["J. Lafferty", "A. McCallum", "F. Pereira"], "venue": "Proceedings of the International Conference on Machine learning (ICML), 282\u2013289.", "citeRegEx": "Lafferty et al\\.,? 2001", "shortCiteRegEx": "Lafferty et al\\.", "year": 2001}, {"title": "Deep learning", "author": ["Y. LeCun", "Y. Bengio", "G. Hinton"], "venue": "Nature 521(7553):436\u2013444.", "citeRegEx": "LeCun et al\\.,? 2015", "shortCiteRegEx": "LeCun et al\\.", "year": 2015}, {"title": "Bridging the gaps between residual learning, recurrent neural networks and visual cortex", "author": ["Q. Liao", "T. Poggio"], "venue": "arXiv preprint arXiv:1604.03640.", "citeRegEx": "Liao and Poggio,? 2016", "shortCiteRegEx": "Liao and Poggio", "year": 2016}, {"title": "Classification in networked data: A toolkit and a univariate case study", "author": ["S. Macskassy", "F. Provost"], "venue": "The Journal of Machine Learning Research 8:935\u2013983.", "citeRegEx": "Macskassy and Provost,? 2007", "shortCiteRegEx": "Macskassy and Provost", "year": 2007}, {"title": "Human-level control through deep reinforcement learning", "author": ["V. Mnih", "K. Kavukcuoglu", "D. Silver", "A.A. Rusu", "J. Veness", "M.G. Bellemare", "A. Graves", "M. Riedmiller", "A.K. Fidjeland", "G Ostrovski"], "venue": "Nature", "citeRegEx": "Mnih et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Mnih et al\\.", "year": 2015}, {"title": "Recurrent neural collective classification", "author": ["D.D. Monner", "J Reggia"], "venue": "Neural Networks and Learning Systems, IEEE Transactions on 24(12):1932\u20131943", "citeRegEx": "Monner and Reggia,? \\Q2013\\E", "shortCiteRegEx": "Monner and Reggia", "year": 2013}, {"title": "The columnar organization of the neocortex", "author": ["V.B. Mountcastle"], "venue": "Brain 120(4):701\u2013722.", "citeRegEx": "Mountcastle,? 1997", "shortCiteRegEx": "Mountcastle", "year": 1997}, {"title": "Iterative classification in relational data", "author": ["J. Neville", "D. Jensen"], "venue": "Proc. AAAI-2000 Workshop on Learning Statistical Models from Relational Data, 13\u201320.", "citeRegEx": "Neville and Jensen,? 2000", "shortCiteRegEx": "Neville and Jensen", "year": 2000}, {"title": "Relational dependency networks", "author": ["J. Neville", "D. Jensen"], "venue": "Journal of Machine Learning Research 8(Mar):653\u2013 692.", "citeRegEx": "Neville and Jensen,? 2007", "shortCiteRegEx": "Neville and Jensen", "year": 2007}, {"title": "Advanced mean field methods: Theory and practice", "author": ["M. Opper", "D. Saad"], "venue": "Massachusetts Institute of Technology Press (MIT Press).", "citeRegEx": "Opper and Saad,? 2001", "shortCiteRegEx": "Opper and Saad", "year": 2001}, {"title": "Probabilistic Reasoning in Intelligent Systems: Networks of Plausible Inference", "author": ["J. Pearl"], "venue": "San Francisco, CA: Morgan Kaufmann.", "citeRegEx": "Pearl,? 1988", "shortCiteRegEx": "Pearl", "year": 1988}, {"title": "Faster training of very deep networks via p-norm gates", "author": ["T. Pham", "T. Tran", "D. Phung", "S. Venkatesh"], "venue": "ICPR\u201916.", "citeRegEx": "Pham et al\\.,? 2016", "shortCiteRegEx": "Pham et al\\.", "year": 2016}, {"title": "Markov logic networks", "author": ["M. Richardson", "P. Domingos"], "venue": "Machine Learning 62:107\u2013136.", "citeRegEx": "Richardson and Domingos,? 2006", "shortCiteRegEx": "Richardson and Domingos", "year": 2006}, {"title": "Deep learning in neural networks: An overview", "author": ["J. Schmidhuber"], "venue": "Neural Networks 61:85\u2013117.", "citeRegEx": "Schmidhuber,? 2015", "shortCiteRegEx": "Schmidhuber", "year": 2015}, {"title": "Collective classification in network data", "author": ["P. Sen", "G. Namata", "M. Bilgic", "L. Getoor", "B. Galligher", "T. Eliassi-Rad"], "venue": "AI magazine 29(3):93.", "citeRegEx": "Sen et al\\.,? 2008", "shortCiteRegEx": "Sen et al\\.", "year": 2008}, {"title": "Training very deep networks", "author": ["R.K. Srivastava", "K. Greff", "J. Schmidhuber"], "venue": "Advances in neural information processing systems, 2377\u20132385.", "citeRegEx": "Srivastava et al\\.,? 2015", "shortCiteRegEx": "Srivastava et al\\.", "year": 2015}, {"title": "Piecewise pseudolikelihood for efficient CRF training", "author": ["C. Sutton", "A. McCallum"], "venue": "Proceedings of the International Conference on Machine Learning (ICML), 863\u2013 870.", "citeRegEx": "Sutton and McCallum,? 2007", "shortCiteRegEx": "Sutton and McCallum", "year": 2007}, {"title": "Discriminative probabilistic models for relational data", "author": ["B. Taskar", "A. Pieter", "D. Koller"], "venue": "Proceedings of the 18th Conference on Uncertainty in Artificial Intelligence (UAI), 485\u201349. Morgan Kaufmann.", "citeRegEx": "Taskar et al\\.,? 2002", "shortCiteRegEx": "Taskar et al\\.", "year": 2002}, {"title": "Joint training of a convolutional network and a graphical model for human pose estimation", "author": ["J.J. Tompson", "A. Jain", "Y. LeCun", "C. Bregler"], "venue": "Advances in neural information processing systems, 1799\u20131807.", "citeRegEx": "Tompson et al\\.,? 2014", "shortCiteRegEx": "Tompson et al\\.", "year": 2014}, {"title": "Tree-based Iterated Local Search for Markov Random Fields with Applications in Image Analysis", "author": ["T. Tran", "S.V. Dinh Phung"], "venue": "Journal of Heuristics DOI:10.1007/s10732014-9270-1.", "citeRegEx": "Tran and Phung,? 2014", "shortCiteRegEx": "Tran and Phung", "year": 2014}, {"title": "Neural choice by elimination via highway networks", "author": ["T. Tran", "D. Phung", "S. Venkatesh"], "venue": "Trends and Applications in Knowledge Discovery and Data Mining, Lecture Notes in Computer Science 9794:15\u201325.", "citeRegEx": "Tran et al\\.,? 2016", "shortCiteRegEx": "Tran et al\\.", "year": 2016}, {"title": "Sequential labeling using deep-structured conditional random fields", "author": ["D. Yu", "S. Wang", "L. Deng"], "venue": "Selected Topics in Signal Processing, IEEE Journal of 4(6):965\u2013973.", "citeRegEx": "Yu et al\\.,? 2010", "shortCiteRegEx": "Yu et al\\.", "year": 2010}, {"title": "Conditional random fields as recurrent neural networks", "author": ["S. Zheng", "S. Jayasumana", "B. Romera-Paredes", "V. Vineet", "Z. Su", "D. Du", "C. Huang", "P.H. Torr"], "venue": "Proceedings of the IEEE International Conference on Computer Vision, 1529\u20131537.", "citeRegEx": "Zheng et al\\.,? 2015", "shortCiteRegEx": "Zheng et al\\.", "year": 2015}], "referenceMentions": [{"referenceID": 12, "context": "A canonical task in learning from this data type is collective classification in which networked data instances are classified simultaneously rather than independently to exploit the dependencies in the data (Macskassy and Provost 2007; Neville and Jensen 2007; Richardson and Domingos 2006; Sen et al. 2008).", "startOffset": 208, "endOffset": 308}, {"referenceID": 17, "context": "A canonical task in learning from this data type is collective classification in which networked data instances are classified simultaneously rather than independently to exploit the dependencies in the data (Macskassy and Provost 2007; Neville and Jensen 2007; Richardson and Domingos 2006; Sen et al. 2008).", "startOffset": 208, "endOffset": 308}, {"referenceID": 21, "context": "A canonical task in learning from this data type is collective classification in which networked data instances are classified simultaneously rather than independently to exploit the dependencies in the data (Macskassy and Provost 2007; Neville and Jensen 2007; Richardson and Domingos 2006; Sen et al. 2008).", "startOffset": 208, "endOffset": 308}, {"referenceID": 23, "context": "A canonical task in learning from this data type is collective classification in which networked data instances are classified simultaneously rather than independently to exploit the dependencies in the data (Macskassy and Provost 2007; Neville and Jensen 2007; Richardson and Domingos 2006; Sen et al. 2008).", "startOffset": 208, "endOffset": 308}, {"referenceID": 25, "context": "For tractable learning, we often resort to surrogate loss functions such as (structured) pseudo-likelihood (Sutton and McCallum 2007), approximate gradient (Hinton 2002), or iterative schemes, stacked learning (Choetkiertikul et al.", "startOffset": 107, "endOffset": 133}, {"referenceID": 7, "context": "For tractable learning, we often resort to surrogate loss functions such as (structured) pseudo-likelihood (Sutton and McCallum 2007), approximate gradient (Hinton 2002), or iterative schemes, stacked learning (Choetkiertikul et al.", "startOffset": 156, "endOffset": 169}, {"referenceID": 1, "context": "For tractable learning, we often resort to surrogate loss functions such as (structured) pseudo-likelihood (Sutton and McCallum 2007), approximate gradient (Hinton 2002), or iterative schemes, stacked learning (Choetkiertikul et al. 2015; Kou and Cohen 2007; Macskassy and Provost 2007; Neville and Jensen 2000).", "startOffset": 210, "endOffset": 311}, {"referenceID": 8, "context": "For tractable learning, we often resort to surrogate loss functions such as (structured) pseudo-likelihood (Sutton and McCallum 2007), approximate gradient (Hinton 2002), or iterative schemes, stacked learning (Choetkiertikul et al. 2015; Kou and Cohen 2007; Macskassy and Provost 2007; Neville and Jensen 2000).", "startOffset": 210, "endOffset": 311}, {"referenceID": 12, "context": "For tractable learning, we often resort to surrogate loss functions such as (structured) pseudo-likelihood (Sutton and McCallum 2007), approximate gradient (Hinton 2002), or iterative schemes, stacked learning (Choetkiertikul et al. 2015; Kou and Cohen 2007; Macskassy and Provost 2007; Neville and Jensen 2000).", "startOffset": 210, "endOffset": 311}, {"referenceID": 16, "context": "For tractable learning, we often resort to surrogate loss functions such as (structured) pseudo-likelihood (Sutton and McCallum 2007), approximate gradient (Hinton 2002), or iterative schemes, stacked learning (Choetkiertikul et al. 2015; Kou and Cohen 2007; Macskassy and Provost 2007; Neville and Jensen 2000).", "startOffset": 210, "endOffset": 311}, {"referenceID": 13, "context": "Deep neural networks, on the other hand, offer automatic feature learning, which is arguably the key behind recent record-breaking successes in vision, speech, games and NLP (LeCun, Bengio, and Hinton 2015; Mnih et al. 2015).", "startOffset": 174, "endOffset": 224}, {"referenceID": 0, "context": "With known challenges in relational learning, can we design a deep neural network that is efficient and accurate for collective classification? There has been recent work that combines deep learning with structured prediction but the main learning and inference problems for general multirelational settings remain open (Belanger and McCallum 2016; Do, Arti, and others 2010; Tompson et al. 2014; Yu, Wang, and Deng 2010; Zheng et al. 2015).", "startOffset": 320, "endOffset": 440}, {"referenceID": 27, "context": "With known challenges in relational learning, can we design a deep neural network that is efficient and accurate for collective classification? There has been recent work that combines deep learning with structured prediction but the main learning and inference problems for general multirelational settings remain open (Belanger and McCallum 2016; Do, Arti, and others 2010; Tompson et al. 2014; Yu, Wang, and Deng 2010; Zheng et al. 2015).", "startOffset": 320, "endOffset": 440}, {"referenceID": 31, "context": "With known challenges in relational learning, can we design a deep neural network that is efficient and accurate for collective classification? There has been recent work that combines deep learning with structured prediction but the main learning and inference problems for general multirelational settings remain open (Belanger and McCallum 2016; Do, Arti, and others 2010; Tompson et al. 2014; Yu, Wang, and Deng 2010; Zheng et al. 2015).", "startOffset": 320, "endOffset": 440}, {"referenceID": 15, "context": "The design of CLN is partly inspired by the columnar organization of neocortex (Mountcastle 1997), in which cortical neurons are organized in vertical, layered mini-columns, each of which is responsible for a small receptive field.", "startOffset": 79, "endOffset": 97}, {"referenceID": 11, "context": "But unlike the original highway nets, CLN\u2019s hidden layers share the same set of parameters, allowing the depth to grow without introducing new parameters (Liao and Poggio 2016; Pham et al. 2016).", "startOffset": 154, "endOffset": 194}, {"referenceID": 20, "context": "But unlike the original highway nets, CLN\u2019s hidden layers share the same set of parameters, allowing the depth to grow without introducing new parameters (Liao and Poggio 2016; Pham et al. 2016).", "startOffset": 154, "endOffset": 194}, {"referenceID": 12, "context": "A popular strategy is to employ approximate but efficient iterative methods (Macskassy and Provost 2007).", "startOffset": 76, "endOffset": 104}, {"referenceID": 1, "context": "1) is a multi-step learning procedure for collective classification (Choetkiertikul et al. 2015; Kou and Cohen 2007; Yu, Wang, and Deng 2010).", "startOffset": 68, "endOffset": 141}, {"referenceID": 8, "context": "1) is a multi-step learning procedure for collective classification (Choetkiertikul et al. 2015; Kou and Cohen 2007; Yu, Wang, and Deng 2010).", "startOffset": 68, "endOffset": 141}, {"referenceID": 1, "context": "In (Choetkiertikul et al. 2015), each relation produces one set of contextual features, where all features of the same relation are averaged:", "startOffset": 3, "endOffset": 31}, {"referenceID": 15, "context": "Inspired by the columnar organization in neocortex (Mountcastle 1997), the CLN has one mini-column per entity (or data instance), which is akin to a sensory receptive field.", "startOffset": 51, "endOffset": 69}, {"referenceID": 20, "context": "Other gating options exists, for example, the p-norm gates where \u03b1p1 +\u03b1 p 2 = 1 for p > 0 (Pham et al. 2016).", "startOffset": 90, "endOffset": 108}, {"referenceID": 11, "context": "There has been empirical evidence supporting this strategy in non-relational data (Liao and Poggio 2016; Pham et al. 2016).", "startOffset": 82, "endOffset": 122}, {"referenceID": 20, "context": "There has been empirical evidence supporting this strategy in non-relational data (Liao and Poggio 2016; Pham et al. 2016).", "startOffset": 82, "endOffset": 122}, {"referenceID": 12, "context": "For the former, we used NetKit1, an open source toolkit for classification in networked data (Macskassy and Provost 2007).", "startOffset": 93, "endOffset": 121}, {"referenceID": 11, "context": "For deep neural nets, following the latest results in (Liao and Poggio 2016; Pham et al. 2016), we implemented highway network with shared parameters among layers (HWNnoRel).", "startOffset": 54, "endOffset": 94}, {"referenceID": 20, "context": "For deep neural nets, following the latest results in (Liao and Poggio 2016; Pham et al. 2016), we implemented highway network with shared parameters among layers (HWNnoRel).", "startOffset": 54, "endOffset": 94}, {"referenceID": 1, "context": "This task is to predict potential delay for an issue, which is an unit of task in an iterative software development lifecycle (Choetkiertikul et al. 2015).", "startOffset": 126, "endOffset": 154}, {"referenceID": 1, "context": "We use the largest dataset reported in (Choetkiertikul et al. 2015), the JBoss, which contains 8,206 issues.", "startOffset": 39, "endOffset": 67}, {"referenceID": 1, "context": "(*) Result reported in (Choetkiertikul et al. 2015).", "startOffset": 23, "endOffset": 51}, {"referenceID": 6, "context": "We used the MovieLens Latest Dataset (Harper and Konstan 2016) which consists of 33,000 movies.", "startOffset": 37, "endOffset": 62}, {"referenceID": 5, "context": "Started in the late 1990s, SRL has advanced significantly with noticeable works such as Probabilistic Relational Models (Getoor and Sahami 1999), Conditional Random Fields (Lafferty, McCallum, and Pereira 2001), Relational Markov Network (Taskar, Pieter, and Koller 2002) and Markov Logic Networks (Richardson and Domingos 2006).", "startOffset": 120, "endOffset": 144}, {"referenceID": 21, "context": "Started in the late 1990s, SRL has advanced significantly with noticeable works such as Probabilistic Relational Models (Getoor and Sahami 1999), Conditional Random Fields (Lafferty, McCallum, and Pereira 2001), Relational Markov Network (Taskar, Pieter, and Koller 2002) and Markov Logic Networks (Richardson and Domingos 2006).", "startOffset": 298, "endOffset": 328}, {"referenceID": 2, "context": "Collective classification is a canonical task in SRL, also known in various forms as structured prediction (Dietterich et al. 2008) and classification on networked data (Macskassy and Provost 2007).", "startOffset": 107, "endOffset": 131}, {"referenceID": 12, "context": "2008) and classification on networked data (Macskassy and Provost 2007).", "startOffset": 43, "endOffset": 71}, {"referenceID": 12, "context": "Two key components of collective classifiers are relational classifier and collective inference (Macskassy and Provost 2007).", "startOffset": 96, "endOffset": 124}, {"referenceID": 12, "context": "Examples are wvRN (Macskassy and Provost 2007), logistic based (Domke 2013) or stacked graphical learning (Choetkiertikul et al.", "startOffset": 18, "endOffset": 46}, {"referenceID": 4, "context": "Examples are wvRN (Macskassy and Provost 2007), logistic based (Domke 2013) or stacked graphical learning (Choetkiertikul et al.", "startOffset": 63, "endOffset": 75}, {"referenceID": 1, "context": "Examples are wvRN (Macskassy and Provost 2007), logistic based (Domke 2013) or stacked graphical learning (Choetkiertikul et al. 2015; Kou and Cohen 2007).", "startOffset": 106, "endOffset": 154}, {"referenceID": 8, "context": "Examples are wvRN (Macskassy and Provost 2007), logistic based (Domke 2013) or stacked graphical learning (Choetkiertikul et al. 2015; Kou and Cohen 2007).", "startOffset": 106, "endOffset": 154}, {"referenceID": 19, "context": "This is a subject of AI with abundance of solutions including message passing algorithms (Pearl 1988), variational meanfield (Opper and Saad 2001) and discrete optimization (Tran and Dinh Phung 2014).", "startOffset": 89, "endOffset": 101}, {"referenceID": 18, "context": "This is a subject of AI with abundance of solutions including message passing algorithms (Pearl 1988), variational meanfield (Opper and Saad 2001) and discrete optimization (Tran and Dinh Phung 2014).", "startOffset": 125, "endOffset": 146}, {"referenceID": 8, "context": "Among existing collective classifiers, the closest to ours is stacked graphical learning where collective inference is bypassed through stacking (Kou and Cohen 2007; Yu, Wang, and Deng 2010).", "startOffset": 145, "endOffset": 190}, {"referenceID": 22, "context": "The other area is Deep Learning (DL), where the current wave has offered compact and efficient ways to build multilayered networks for function approximation (via feedforward networks) and program construction (via recurrent networks) (LeCun, Bengio, and Hinton 2015; Schmidhuber 2015).", "startOffset": 235, "endOffset": 285}, {"referenceID": 0, "context": "However, much less attention has been paid to general networked data (Monner, Reggia, and others 2013), although there has been work on pairing structured outputs with deep networks (Belanger and McCallum 2016; Do, Arti, and others 2010; Tompson et al. 2014; Yu, Wang, and Deng 2010; Zheng et al. 2015).", "startOffset": 182, "endOffset": 302}, {"referenceID": 27, "context": "However, much less attention has been paid to general networked data (Monner, Reggia, and others 2013), although there has been work on pairing structured outputs with deep networks (Belanger and McCallum 2016; Do, Arti, and others 2010; Tompson et al. 2014; Yu, Wang, and Deng 2010; Zheng et al. 2015).", "startOffset": 182, "endOffset": 302}, {"referenceID": 31, "context": "However, much less attention has been paid to general networked data (Monner, Reggia, and others 2013), although there has been work on pairing structured outputs with deep networks (Belanger and McCallum 2016; Do, Arti, and others 2010; Tompson et al. 2014; Yu, Wang, and Deng 2010; Zheng et al. 2015).", "startOffset": 182, "endOffset": 302}, {"referenceID": 11, "context": "Parameter sharing in feedforward networks was recently analyzed in (Liao and Poggio 2016; Pham et al. 2016).", "startOffset": 67, "endOffset": 107}, {"referenceID": 20, "context": "Parameter sharing in feedforward networks was recently analyzed in (Liao and Poggio 2016; Pham et al. 2016).", "startOffset": 67, "endOffset": 107}, {"referenceID": 15, "context": "It somewhat resembles the columnar structure in neocortex (Mountcastle 1997), where each narrow deep network plays a role of a mini-column.", "startOffset": 58, "endOffset": 76}, {"referenceID": 20, "context": "We wish to emphasize that although we use highway networks in actual implementation due to its excellent performance (Pham et al. 2016; Srivastava, Greff, and Schmidhuber 2015; Tran, Phung, and Venkatesh 2016), any feedforward networks can be potentially be used in our architecture.", "startOffset": 117, "endOffset": 209}], "year": 2016, "abstractText": "Relational learning deals with data that are characterized by relational structures. An important task is collective classification, which is to jointly classify networked objects. While it holds a great promise to produce a better accuracy than non-collective classifiers, collective classification is computationally challenging and has not leveraged on the recent breakthroughs of deep learning. We present Column Network (CLN), a novel deep learning model for collective classification in multi-relational domains. CLN has many desirable theoretical properties: (i) it encodes multi-relations between any two instances; (ii) it is deep and compact, allowing complex functions to be approximated at the network level with a small set of free parameters; (iii) local and relational features are learned simultaneously; (iv) long-range, higher-order dependencies between instances are supported naturally; and (v) crucially, learning and inference are efficient with linear complexity in the size of the network and the number of relations. We evaluate CLN on multiple real-world applications: (a) delay prediction in software projects, (b) PubMed Diabetes publication classification and (c) film genre classification. In all of these applications, CLN demonstrates a higher accuracy than state-of-the-art rivals.", "creator": "TeX"}}}