{"id": "1609.04938", "review": {"conference": "ICML", "VERSION": "v1", "DATE_OF_SUBMISSION": "16-Sep-2016", "title": "Image-to-Markup Generation with Coarse-to-Fine Attention", "abstract": "Building on recent advances in image caption generation and optical character recognition (OCR), we present a general-purpose, deep learning-based system to decompile an image into presentational markup. While this task is a well-studied problem in OCR, our method takes an inherently different, data-driven approach. Our model does not require any knowledge of the underlying markup language, and is simply trained end-to-end on real-world example data. The model employs a convolutional network for text and layout recognition in tandem with an attention-based neural machine translation system. To train and evaluate the model, we introduce a new dataset of real-world rendered mathematical expressions paired with LaTeX markup, as well as a synthetic dataset of web pages paired with HTML snippets. Experimental results show that the system is surprisingly effective at generating accurate markup for both datasets. While a standard domain-specific LaTeX OCR system achieves around 25% accuracy, our model reproduces the exact rendered image on 75% of examples.", "histories": [["v1", "Fri, 16 Sep 2016 08:14:50 GMT  (814kb,D)", "http://arxiv.org/abs/1609.04938v1", null], ["v2", "Tue, 13 Jun 2017 22:48:53 GMT  (323kb,D)", "http://arxiv.org/abs/1609.04938v2", "Accepted by ICML 2017"]], "reviews": [], "SUBJECTS": "cs.CV cs.CL cs.LG cs.NE", "authors": ["yuntian deng", "anssi kanervisto", "jeffrey ling", "alexander m rush"], "accepted": true, "id": "1609.04938"}, "pdf": {"name": "1609.04938.pdf", "metadata": {"source": "CRF", "title": "What You Get Is What You See: A Visual Markup Decompiler", "authors": ["Yuntian Deng", "Anssi Kanervisto", "Alexander M. Rush"], "emails": ["dengyuntian@gmail.com", "anssk@student.uef.fi", "srush@seas.harvard.edu"], "sections": [{"heading": "Introduction", "text": "In fact, it is the case that you are able to play by the rules without being able to fulfil them."}, {"heading": "Model", "text": "It is not only the way in which people in the USA, in Europe, in the USA, in Europe, in Europe, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA,"}, {"heading": "Data", "text": "While there are some datasets for the image tomarkup generation problem (Mouchere et al. 2012; 2013; Lin et al. 2012), they are all too small to train a data-driven system. Therefore, we present two new datasets for this task: a preliminary web page dataset with HTML snippets and a large real mathematical expression dataset in LaTeX."}, {"heading": "Web Page-to-HTML", "text": "Our preliminary dataset is a synthetically generated collection of \"web pages\" that is used as a test of whether the model can learn relative spatial positioning. We generate a dataset consisting of 100k unique HTML snippets and corresponding rendered images. The images are assembled using a web kit and produce images the size of 100 x 100. The task is to derive the HTML markup based on the rendered image. The HTML markup is generated using a simple context-free grammar. The grammar recursively generates div's each with a fixed frame, a random width and a random float (left or right). Each div can be divided either recursively vertically or horizontally. 2 The maximum depth of the recursion is limited to 2 nested layers of div's. Finally, we set multiple constraints on the width of the elements to2Additionally, each div contains a randomly generated character of a span array."}, {"heading": "Math-to-LaTeX", "text": "In fact, most of them are able to play by the rules that they have established in recent years, and they are able to play by the rules that they have set themselves."}, {"heading": "Experimental Setup", "text": "To test this approach, we compare the proposed model with several other classic OCR baselines, neural models and ablations on the HTML and LaTeX decompilation tasks."}, {"heading": "Baselines", "text": "The currently best available OCR-based mathematical expression recognition system is the InftyReader system, a proprietary commercial implementation of the INFTY system from (Suzuki et al. 2003), which combines character recognition and structural analysis phases. Additionally, we experimented with the open source AbiWord OCR system, which includes a Tex generation mode on the KaTeX parser https: / / khan.github.io / KaTeX / (Wen 2002). However, we found that this system performs too poorly in this task. For a neural model, a natural comparison with standard image caption approaches (Xu et al. 2015) is required. Since our model is based on this approach, we compare by removing the RNN encoder, i.e. replacing V with V, and increasing the number of CNN parameters so that the number of parameters is approximately the same."}, {"heading": "Evaluation", "text": "Our core evaluation method is to verify the accuracy of the rendered output image x compared to the true image x.The main evaluation method specifies the column-by-column processing distance between the gold and the predicted images. We explicitly discredit generated columns and compare the processed spacing sequences.6 The end result is the total number of processing distances used divided by the maximum number in the dataset. In addition, we verify the exact match with the original image as well as the value after removing the spaces column.6 We also include the standard metrics for text generation, the perplexity of the conditional language model and the BLEU value (Papineni et al. 2002). Note that both metrics are sensitive to the fact that the markup languages have an unclear ambiguity, so that a deterministic perplexity of 1 would be impossible."}, {"heading": "Implementation Details", "text": "The CNN specifications are summarized in Table 2. The model uses single-layer LSTMs for all RNNs. We use a bidirectional RNN for the encoder. The hidden state of the RNN encoder is of size 256, decoder RNN of 512, and token embedding of size 80. The model has a total of 9.48 million parameters. To learn the parameters, we use a stochastic gradient descent in the minibatch.6In practice, we have found that the LaTeX renderer often misaligns identical expressions by several pixels. To correct this, only misalignments of \u2265 5 pixels wide are \"exact\" match errors."}, {"heading": "Model Train Perp Test Perp Exact Match", "text": "We train the model for 12 epochs and use the validation perplexity to select the best model. During the test phase, we use beam search with beam size 5. The system is run with Torch (Collobert, Kavukcuoglu and Farabet 2011) based on the Seq2seq-attn NMT system 7. Experiments are performed with a 12GB NVidia Titan X GPU.HTML. All images start as 100 x 100 color input, which are then sampled to grayscale images of size 64 x 64. We then normalize pixels to range [\u2212 1, 1]. During the training, we only use training instances with less than 100 output tokens to speed up the training process (the batch size is set to 100)."}, {"heading": "Results", "text": "The model is able to achieve a perplexity of 1.06 and an exact match accuracy of over 97.5%. These results suggest that the model is able to identify and generate the correct output based on spatial terms. The most important experimental results on mathematical expressions are in Table 3. These results are due to ambiguities in the underlying markup language. Typical errors are in Table 4. The few problems occur in terms of font size and match with the relative sizes of Divs.The most important experimental results on mathematical expressions are in Table 3. These results compare several different systems to the task of decompiling rendered LaTeX. The classic INFTY system is able to perform fairly well in terms of text accuracy, but leads poorly on the more rigorous image metrics. Our reimplementation of image capture works CNNENC much better by increasing the standard codification to over 75% with our very high codification."}, {"heading": "Conclusion and Future Work", "text": "We have introduced a visual attention-based model, WYGIWYS, for OCR of presentation markup. The model acts as a \"visual decompiler\" for markup such as HTML and LaTeX. We also introduce a new dataset IM2LATEX-100K that provides a test bed for this task of image-to-markup generation. These papers offer a new view of the task of structured text OCR and show that data-driven models can be surprisingly effective, even without knowledge of the underlying language. Possible future instructions for this work include: scaling the system to run across entire web pages or to decompile documents, using a similar approach to handwritten mathematical expressions or HTML from informal sketches, or combining these methods with neural inference engines such as MemNNs (Weston, Chopra and Bordes 2014) for more complicated markup or HTML variables."}], "references": [{"title": "Syntax-directed recognition of handprinted two-dimensional mathematics", "author": ["R.H. Anderson"], "venue": "Symposium on Interactive Systems for Experimental Applied Mathematics: Proceedings of the Association for Computing Machinery Inc. Symposium, 436\u2013459. ACM.", "citeRegEx": "Anderson,? 1967", "shortCiteRegEx": "Anderson", "year": 1967}, {"title": "Neural machine translation by jointly learning to align and translate", "author": ["D. Bahdanau", "K. Cho", "Y. Bengio"], "venue": "arXiv preprint arXiv:1409.0473.", "citeRegEx": "Bahdanau et al\\.,? 2014", "shortCiteRegEx": "Bahdanau et al\\.", "year": 2014}, {"title": "A syntactic approach for handwritten mathematical formula recognition", "author": ["A. Belaid", "Haton", "J.-P."], "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence (1):105\u2013111.", "citeRegEx": "Belaid et al\\.,? 1984", "shortCiteRegEx": "Belaid et al\\.", "year": 1984}, {"title": "Mathematical expression recognition: a survey", "author": ["K. Chan", "D. Yeung"], "venue": "IJDAR 3(1):3\u201315.", "citeRegEx": "Chan and Yeung,? 2000", "shortCiteRegEx": "Chan and Yeung", "year": 2000}, {"title": "Deep, big, simple neural nets for handwritten digit recognition", "author": ["D.C. Ciresan", "U. Meier", "L.M. Gambardella", "J. Schmidhuber"], "venue": "Neural computation 22(12):3207\u20133220.", "citeRegEx": "Ciresan et al\\.,? 2010", "shortCiteRegEx": "Ciresan et al\\.", "year": 2010}, {"title": "Torch7: A matlab-like environment for machine learning", "author": ["R. Collobert", "K. Kavukcuoglu", "C. Farabet"], "venue": "BigLearn, NIPS Workshop, number EPFL-CONF-192376.", "citeRegEx": "Collobert et al\\.,? 2011", "shortCiteRegEx": "Collobert et al\\.", "year": 2011}, {"title": "Overview of the 2003 kdd cup", "author": ["J. Gehrke", "P. Ginsparg", "J. Kleinberg"], "venue": "ACM SIGKDD Explorations Newsletter 5(2):149\u2013151.", "citeRegEx": "Gehrke et al\\.,? 2003", "shortCiteRegEx": "Gehrke et al\\.", "year": 2003}, {"title": "Long short-term memory", "author": ["S. Hochreiter", "J. Schmidhuber"], "venue": "Neural computation 9(8):1735\u20131780.", "citeRegEx": "Hochreiter and Schmidhuber,? 1997", "shortCiteRegEx": "Hochreiter and Schmidhuber", "year": 1997}, {"title": "Batch normalization: Accelerating deep network training by reducing internal covariate shift", "author": ["S. Ioffe", "C. Szegedy"], "venue": "Proceedings of The 32nd International Conference on Machine Learning, 448\u2013456.", "citeRegEx": "Ioffe and Szegedy,? 2015", "shortCiteRegEx": "Ioffe and Szegedy", "year": 2015}, {"title": "Deep structured output learning for unconstrained text recognition", "author": ["M. Jaderberg", "K. Simonyan", "A. Vedaldi", "A. Zisserman"], "venue": "ICLR.", "citeRegEx": "Jaderberg et al\\.,? 2015", "shortCiteRegEx": "Jaderberg et al\\.", "year": 2015}, {"title": "Reading text in the wild with convolutional neural networks", "author": ["M. Jaderberg", "K. Simonyan", "A. Vedaldi", "A. Zisserman"], "venue": "International Journal of Computer Vision 116(1):1\u201320.", "citeRegEx": "Jaderberg et al\\.,? 2016", "shortCiteRegEx": "Jaderberg et al\\.", "year": 2016}, {"title": "Deep visual-semantic alignments for generating image descriptions", "author": ["A. Karpathy", "L. Fei-Fei"], "venue": "Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, 3128\u20133137.", "citeRegEx": "Karpathy and Fei.Fei,? 2015", "shortCiteRegEx": "Karpathy and Fei.Fei", "year": 2015}, {"title": "Visualizing and understanding recurrent networks", "author": ["A. Karpathy", "J. Johnson", "F.-F. Li"], "venue": "arXiv preprint arXiv:1506.02078.", "citeRegEx": "Karpathy et al\\.,? 2015", "shortCiteRegEx": "Karpathy et al\\.", "year": 2015}, {"title": "Recursive recurrent nets with attention modeling for ocr in the wild", "author": ["Lee", "C.-Y.", "S. Osindero"], "venue": "arXiv preprint arXiv:1603.03101.", "citeRegEx": "Lee et al\\.,? 2016", "shortCiteRegEx": "Lee et al\\.", "year": 2016}, {"title": "Performance evaluation of mathematical formula identification", "author": ["X. Lin", "L. Gao", "Z. Tang", "X. Lin", "X. Hu"], "venue": "Document Analysis Systems (DAS), 2012 10th IAPR International Workshop on, 287\u2013291. IEEE.", "citeRegEx": "Lin et al\\.,? 2012", "shortCiteRegEx": "Lin et al\\.", "year": 2012}, {"title": "Effective approaches to attention-based neural machine translation", "author": ["M.-T. Luong", "H. Pham", "C.D. Manning"], "venue": "EMNLP.", "citeRegEx": "Luong et al\\.,? 2015", "shortCiteRegEx": "Luong et al\\.", "year": 2015}, {"title": "Ambiguity and constraint in mathematical expression recognition", "author": ["E.G. Miller", "P.A. Viola"], "venue": "AAAI/IAAI, 784\u2013791.", "citeRegEx": "Miller and Viola,? 1998", "shortCiteRegEx": "Miller and Viola", "year": 1998}, {"title": "Icfhr 2012 competition on recognition of on-line mathematical expressions (crohme 2012)", "author": ["H. Mouchere", "C. Viard-Gaudin", "D.H. Kim", "J.H. Kim", "U. Garain"], "venue": "Frontiers in Handwriting Recognition (ICFHR), 2012 International Conference on, 811\u2013816. IEEE.", "citeRegEx": "Mouchere et al\\.,? 2012", "shortCiteRegEx": "Mouchere et al\\.", "year": 2012}, {"title": "Icdar 2013 crohme: Third international competition on recognition of online handwritten mathematical expressions", "author": ["H. Mouchere", "C. Viard-Gaudin", "R. Zanibbi", "U. Garain", "D.H. Kim", "J.H. Kim"], "venue": "2013 12th International Conference on Document Analysis and Recognition, 1428\u2013", "citeRegEx": "Mouchere et al\\.,? 2013", "shortCiteRegEx": "Mouchere et al\\.", "year": 2013}, {"title": "Bleu: a method for automatic evaluation of machine translation", "author": ["K. Papineni", "S. Roukos", "T. Ward", "W.-J. Zhu"], "venue": "Proceedings of the 40th annual meeting on association for computational linguistics, 311\u2013318. Association for Computational Linguistics.", "citeRegEx": "Papineni et al\\.,? 2002", "shortCiteRegEx": "Papineni et al\\.", "year": 2002}, {"title": "An end-to-end trainable neural network for image-based sequence recognition and its application to scene text recognition", "author": ["B. Shi", "X. Bai", "C. Yao"], "venue": "arXiv preprint arXiv:1507.05717.", "citeRegEx": "Shi et al\\.,? 2015", "shortCiteRegEx": "Shi et al\\.", "year": 2015}, {"title": "Infty: an integrated ocr system for mathematical documents", "author": ["M. Suzuki", "F. Tamari", "R. Fukuda", "S. Uchida", "T. Kanahori"], "venue": "Proceedings of the 2003 ACM symposium on Document engineering, 95\u2013104. ACM.", "citeRegEx": "Suzuki et al\\.,? 2003", "shortCiteRegEx": "Suzuki et al\\.", "year": 2003}, {"title": "Grammar as a foreign language", "author": ["O. Vinyals", "\u0141. Kaiser", "T. Koo", "S. Petrov", "I. Sutskever", "G. Hinton"], "venue": "Advances in Neural Information Processing Systems, 2755\u2013 2763.", "citeRegEx": "Vinyals et al\\.,? 2015a", "shortCiteRegEx": "Vinyals et al\\.", "year": 2015}, {"title": "Show and tell: A neural image caption generator", "author": ["O. Vinyals", "A. Toshev", "S. Bengio", "D. Erhan"], "venue": "Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, 3156\u20133164.", "citeRegEx": "Vinyals et al\\.,? 2015b", "shortCiteRegEx": "Vinyals et al\\.", "year": 2015}, {"title": "End-toend text recognition with convolutional neural networks", "author": ["T. Wang", "D.J. Wu", "A. Coates", "A.Y. Ng"], "venue": "Pattern Recognition (ICPR), 2012 21st International Conference on, 3304\u20133308. IEEE.", "citeRegEx": "Wang et al\\.,? 2012", "shortCiteRegEx": "Wang et al\\.", "year": 2012}, {"title": "Abiword: Open source\u2019s answer to microsoft word", "author": ["H. Wen"], "venue": "Linux Dev Center, downloaded from http://www. linuxdevcenter. com/lpt/a/1636 1\u20133.", "citeRegEx": "Wen,? 2002", "shortCiteRegEx": "Wen", "year": 2002}, {"title": "Memory networks", "author": ["J. Weston", "S. Chopra", "A. Bordes"], "venue": "arXiv preprint arXiv:1410.3916.", "citeRegEx": "Weston et al\\.,? 2014", "shortCiteRegEx": "Weston et al\\.", "year": 2014}, {"title": "Show, attend and tell: Neural image caption generation with visual attention", "author": ["K. Xu", "J. Ba", "R. Kiros", "K. Cho", "A. Courville", "R. Salakhudinov", "R. Zemel", "Y. Bengio"], "venue": "Proceedings of The 32nd International Conference on Machine Learning, 2048\u20132057.", "citeRegEx": "Xu et al\\.,? 2015", "shortCiteRegEx": "Xu et al\\.", "year": 2015}], "referenceMentions": [{"referenceID": 0, "context": "Optical character recognition (OCR) is most commonly used to recognize natural language from an image; however, as early as the work of (Anderson 1967), there has been research interest in converting images into structured language or markup that defines both the text itself and its presentational semantics.", "startOffset": 136, "endOffset": 151}, {"referenceID": 3, "context": "The primary focus of this work is OCR for mathematical expressions, and how to handle presentational aspects such as sub and superscript notation, special symbols, and nested fractions (Belaid and Haton 1984; Chan and Yeung 2000).", "startOffset": 185, "endOffset": 229}, {"referenceID": 16, "context": "The most effective systems combine specialized character segmentation with grammars of the underlying mathematical layout language (Miller and Viola 1998).", "startOffset": 131, "endOffset": 154}, {"referenceID": 21, "context": "A prime example of this approach is the INFTY system that is used to convert printed mathematical expressions to LaTeX and other markup formats (Suzuki et al. 2003).", "startOffset": 144, "endOffset": 164}, {"referenceID": 4, "context": "handwriting recognition (Ciresan et al. 2010), OCR in natural scenes (Jaderberg et al.", "startOffset": 24, "endOffset": 45}, {"referenceID": 9, "context": "2010), OCR in natural scenes (Jaderberg et al. 2015; 2016; Wang et al. 2012) and image caption generation (Karpathy and Fei-Fei 2015; Vinyals et al.", "startOffset": 29, "endOffset": 76}, {"referenceID": 24, "context": "2010), OCR in natural scenes (Jaderberg et al. 2015; 2016; Wang et al. 2012) and image caption generation (Karpathy and Fei-Fei 2015; Vinyals et al.", "startOffset": 29, "endOffset": 76}, {"referenceID": 11, "context": "2012) and image caption generation (Karpathy and Fei-Fei 2015; Vinyals et al. 2015b).", "startOffset": 35, "endOffset": 84}, {"referenceID": 23, "context": "2012) and image caption generation (Karpathy and Fei-Fei 2015; Vinyals et al. 2015b).", "startOffset": 35, "endOffset": 84}, {"referenceID": 22, "context": "While results from language modeling suggest that neural models can consistently generate syntactically correct markup (Karpathy, Johnson, and Li 2015; Vinyals et al. 2015a), it is unclear whether the full solution can be learned from markup-image pairs.", "startOffset": 119, "endOffset": 173}, {"referenceID": 27, "context": "Similar to work in image captioning (Xu et al. 2015), the model incorporates a multi-layer convolutional network over the image with an attention-based recurrent neural network decoder.", "startOffset": 36, "endOffset": 52}, {"referenceID": 9, "context": ") Unlike some recent OCR work (Jaderberg et al. 2015; Lee and Osindero 2016), we do not use final fully-connected layers (Ioffe and Szegedy 2015), since we want to preserve the locality of CNN features in order to use visual attention.", "startOffset": 30, "endOffset": 76}, {"referenceID": 8, "context": "2015; Lee and Osindero 2016), we do not use final fully-connected layers (Ioffe and Szegedy 2015), since we want to preserve the locality of CNN features in order to use visual attention.", "startOffset": 73, "endOffset": 97}, {"referenceID": 17, "context": "This network architecture is now standard; we model it specifically after the network used by Shi et al. (2015) for OCR from images (specification is given in Table 2.", "startOffset": 94, "endOffset": 112}, {"referenceID": 27, "context": "Row Encoder In attention-based image captioning (Xu et al. 2015), the image feature grid can be directly fed into the decoder.", "startOffset": 48, "endOffset": 64}, {"referenceID": 21, "context": "The current best-available OCR-based mathematical expression recognition system is the InftyReader system, a proprietary commercial implementation of the INFTY system of (Suzuki et al. 2003).", "startOffset": 170, "endOffset": 190}, {"referenceID": 25, "context": "contains a Tex generation mode (Wen 2002).", "startOffset": 31, "endOffset": 41}, {"referenceID": 27, "context": "For a neural model, a natural comparison is to standard image captioning approaches (Xu et al. 2015).", "startOffset": 84, "endOffset": 100}, {"referenceID": 19, "context": "6 We also include standard intrinsic text generation metrics, conditional language model perplexity and BLEU score (Papineni et al. 2002).", "startOffset": 115, "endOffset": 137}], "year": 2016, "abstractText": "Building on recent advances in image caption generation and optical character recognition (OCR), we present a generalpurpose, deep learning-based system to decompile an image into presentational markup. While this task is a wellstudied problem in OCR, our method takes an inherently different, data-driven approach. Our model does not require any knowledge of the underlying markup language, and is simply trained end-to-end on real-world example data. The model employs a convolutional network for text and layout recognition in tandem with an attention-based neural machine translation system. To train and evaluate the model, we introduce a new dataset of real-world rendered mathematical expressions paired with LaTeX markup, as well as a synthetic dataset of web pages paired with HTML snippets. Experimental results show that the system is surprisingly effective at generating accurate markup for both datasets. While a standard domainspecific LaTeX OCR system achieves around 25% accuracy, our model reproduces the exact rendered image on 75% of examples.", "creator": "TeX"}}}