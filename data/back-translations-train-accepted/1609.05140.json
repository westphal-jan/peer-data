{"id": "1609.05140", "review": {"conference": "AAAI", "VERSION": "v1", "DATE_OF_SUBMISSION": "16-Sep-2016", "title": "The Option-Critic Architecture", "abstract": "Temporal abstraction is key to scaling up learning and planning in reinforcement learning. While planning with temporally extended actions is well understood, creating such abstractions autonomously from data has remained challenging. We tackle this problem in the framework of options [Sutton, Precup &amp; Singh, 1999; Precup, 2000]. We derive policy gradient theorems for options and propose a new option-critic architecture capable of learning both the internal policies and the termination conditions of options, in tandem with the policy over options, and without the need to provide any additional rewards or subgoals. Experimental results in both discrete and continuous environments showcase the flexibility and efficiency of the framework.", "histories": [["v1", "Fri, 16 Sep 2016 17:05:55 GMT  (263kb,D)", "http://arxiv.org/abs/1609.05140v1", null], ["v2", "Sat, 3 Dec 2016 02:47:51 GMT  (295kb,D)", "http://arxiv.org/abs/1609.05140v2", "Accepted to the Thirthy-first AAAI Conference On Artificial Intelligence (AAAI), 2017"]], "reviews": [], "SUBJECTS": "cs.AI", "authors": ["pierre-luc bacon", "jean harb", "doina precup"], "accepted": true, "id": "1609.05140"}, "pdf": {"name": "1609.05140.pdf", "metadata": {"source": "CRF", "title": "The Option-Critic Architecture", "authors": ["Pierre-Luc Bacon", "Jean Harb", "Doina Precup"], "emails": ["dprecup}@cs.mcgill.ca"], "sections": [{"heading": "Introduction", "text": "In fact, the majority of people who are able to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to fight, to move, to move, to fight, to fight, to fight, to move, to move, to move, to move, to move, to move, to move, to move, to fight, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move"}, {"heading": "Preliminaries and Notation", "text": "A Markov Decision Process consists of a set of states S, a set of measures A > RespectA, a transitional function P: S \u00b7 A \u2192 (S \u2192 [0, 1]) and a reward function r: S \u00b7 A \u2192 R. For convenience, we develop our ideas under the assumption of discrete states and approaches to action. However, our results extend to continuous spaces using common dimensional assumptions (some of our empirical results are contained in continuous tasks). A (Markovian stationary) policy is a probability distribution through measures tied to states: S \u00b7 A \u2192 [0, 1]. In discounted problems, the value function of a policy measure is defined as an expected return: V\u03c0 (s) = E\u03c0 policy."}, {"heading": "Learning Options", "text": "We take a continuous perspective on the problem of learning options. At all times, we want to incorporate all available experience into each component of our system: value function and policy on options, intra-option policy and termination functions. To achieve this goal, we focus on learning option policy and termination functions, provided they are represented by differentiated parameterized functional approximations (repeating this approach). We look at the call-and-return option execution model, in which an agent picks up the option according to his policy on options, then follows the intra-option policy until termination (as dictated by \u03b2\u03c9) at which point this approach is applied. Let us designate the intra-option policy of the option policy of the option parameterized states as such and the termination function of the option parameters. We present two new results for learning options, which we expect to be the blueprint of the political outcomes (both of which are derived from the 2000 Sutton)."}, {"heading": "Experiments", "text": "We first experimented with a four-space navigation task (Sutton, Precup and Singh 1999), the goal being to evaluate the ability of a predetermined option learned fully autonomously to recover from a sudden change in the environment. (Sutton, Precup and Singh 1999) presented a similar experiment for a series of predetermined options; the options in our results were not prioritized. First, the target is in the East Gate and the initial state is drawn uniformly from all other cells. After 1000 episodes, the target moves to a random location in the lower right space. Primitive movements can fail with a 1 / 3 probability, with the agent randomly passing over to one of the empty adjacent cells. The discount factor was 0.99, and the reward was + 1 at the target and 0 otherwise. We decided to parameterise the intra-option policy with Boltzmann distributions and termination with sigmoid functions."}, {"heading": "Pinball Domain", "text": "In the Pinball domain (Konidaris and Barto 2009), a ball must be guided through a labyrinth of arbitrarily shaped polygons to a specified destination. The state space is continuous and corresponds to the position and speed of the ball in the x-y plane. At each step, the agent must choose between five individual primitive actions: moving the ball faster or slower, in a vertical or horizontal direction, or taking the zero action. Collisions with obstacles are elastic and can be used to the advantage of the agent. To learn the critic, we used the intra option Q-Learning with linear functional approximation and Fourier bases (Konidaris and Barto 2009) of order 3. We experimented with 2, 3 or 4 options. All other options were the same as before. In (Konidaris and Barto 2009), an option can only be used and updated after a pregnancy period of 10 episodes. Since Learning fully integrated into the option-critics, 40 episodes can already be found near the target options (learn the options)."}, {"heading": "Arcade Learning Environment", "text": "We used the Option Critic Architecture in the Arcade Learning Environment (ALE) (ALE) (Bellemare et al. 2013) with an in-depth neural network to approach the critics, intra-options strategies and termination functions. We used the same configuration as (Mnih et al. 2013) for the first 3 constituent layers of the network. We then fed the results of the third layer into a dense common layer of 512 neurons as shown in Figure 4.We represented the intra-options strategies as a linear-softmax of the fourth (dense) layer to achieve a probability distribution of the individual actions under current observation. Termination functions were similarly defined as sigmoid functions, with one neuron per termination.The critic network was trained with intra-option Qlearning with experiential replay."}, {"heading": "Related Work", "text": "Since option discovery has received a lot of attention lately, we will now discuss in more detail the place of our approach in relation to others. (Comanici and Precup 2010) also used a gradient-based approach to improve only the termination function for Semi-Markov options; termination was modeled through a logistical distribution via a cumulative measurement of the characteristics observed since initiation. (Silver and Ciosek 2012) Dynamically linked options into longer time sequences by relying on compositivity properties. Previous work on linear options (Sorg and Singh 2010) also used compositionality to plan linear expectation models for options. As in this work, we rely on Bellman equations and compositionality, but in conjunction with policy gradients. (Levy and Shimkin 2011) also rely on political gradient methods by explicitly treating augmented state space and the treatment of events as additional control measures."}, {"heading": "Discussion", "text": "We developed a general, gradient-based approach to learning simultaneously the intra-option policy and termination conditions, as well as the options policy, to optimize a performance target for the given task. Our ALE experiments show successful end-to-end option training in the presence of non-linear functional approximation. As mentioned earlier, our approach only requires specifying the number of options. However, if you wanted to use additional pseudo-rewards, the options critique framework would easily include them. Internal policies and termination gradients would simply have to be taken in relation to the pseudo-rewards, rather than the task reward. A simple instance of this idea we used in some of the experiments is to use additional rewards that are actually extended over time by adding a penalty whenever a switch event occurs."}, {"heading": "Augmented Process", "text": "If the \u03c9t option is initiated or executed at a certain point in time t in state st, then the discounted probability of the transition to (st + 1, \u03c9t + 1) is in one step: P (1) \u03b3 (st + 1, \u03c9t + 1 | st, \u03c9t) = \u2211 a \u03c0\u03c9t (a | st) \u03b3 P (st + 1 | st, a) ((1 \u2212 \u03b2\u03c9t (st + 1))) 1\u03c9t = \u03c9t + 1 + \u03b2\u03c9t (st + 1) \u043d\u044bt (\u03c9t + 1 | st + 1)))). If the process is conditioned by (st, \u03c9t \u2212 1), the discounted probability of the transition to st + 1, \u03c9t: P (st + 1) \u03b3 (st + 1, \u03c9t \u2212 st, \u03c9t \u2212 t \u2212 st)."}, {"heading": "Proof of the Intra-Option Policy Gradient Theorem", "text": "Take the gradient of the option value function: \"Qis\" (s) \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \""}, {"heading": "Proof of the Termination Gradient Theorem", "text": "The expected sum of discounted rewards, beginning with (s1), is stated as follows: U (s1, s1) = \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \""}, {"heading": "Algorithm Outline", "text": "We write \u03b1, \u03b1\u03b8 and \u03b1\u043d for the critic's learning rates, the intra-option policies and the termination functions. The tabular setting is assumed only for clarity of presentation."}, {"heading": "Four-Rooms Domain", "text": "For all Boltzmann guidelines, we set the temperature parameter to 0.001. We also used the following method for selecting the learning rates and have a fair comparison between algorithms. First, we chose the highest possible learning rate (0.5) for the SARSA agent, which would be stable and has low variance. Then, we used the same learning rate for the critic and performed a search for smaller values of learning rates for the actor components: 0.5 / (2k) for k = 1 to 5. We received 0.5 for the critic, 0.25 for the intra-option policy gradient, and 0.25 for the termination gradient."}, {"heading": "Pinball", "text": "In this area, a drag coefficient of 0.995 effectively stops ball movements after a limited number of steps if the zero action is chosen repeatedly. Each push action results in a penalty of \u2212 5 with no cost of action \u2212 1. The episode ends with + 10000 reward if the agent reaches the target. We interrupted each episode with more than 10000 steps and set the discount factor to 0.99. We used Boltzmann guidelines for the intra option policy and linear sigmoid functions for the termination functions. Learning rates were 0.01 for the critic and 0.001 for the intra and termination grades. We used an epsilon-greedy policy for options with = 0.01."}, {"heading": "Arcade Learning Environments", "text": "We used 32 turn filters of size 8 x 8 and 4 in the first layer, 64 filters of size 4 x 4 with a step of 2 in the second and 64 3 x 3 filters with a step of 1 in the third layer. We set the learning rate for the intra option policies and the termination gradient to 0.00025 and used RMSProp or the Critics. The behavior specialization when using 2 options in the game Seaquest is visible in Figure 9, which shows that measures 10 (PLAYER A UPFIRE in ALE) are dominant in option 1, while measures 13 (PLAYER A DOWNFIRE) are only part of option 2. Distribution over the primitive measures is also complementary over the two options."}], "references": [{"title": "L", "author": ["Baird"], "venue": "C.", "citeRegEx": "Baird 1993", "shortCiteRegEx": null, "year": 1993}, {"title": "M", "author": ["Bellemare"], "venue": "G.; Naddaf, Y.; Veness, J.; and Bowling, M.", "citeRegEx": "Bellemare et al. 2013", "shortCiteRegEx": null, "year": 2013}, {"title": "and Precup", "author": ["G. Comanici"], "venue": "D.", "citeRegEx": "Comanici and Precup 2010", "shortCiteRegEx": null, "year": 2010}, {"title": "A", "author": ["O. \u015eim\u015fek", "Barto"], "venue": "G.", "citeRegEx": "\u015eim\u015fek and Barto 2009", "shortCiteRegEx": null, "year": 2009}, {"title": "Probabilistic inference for determining options in reinforcement learning. Machine Learning, Special Issue 104(2):337\u2013357", "author": ["Daniel"], "venue": null, "citeRegEx": "Daniel,? \\Q2016\\E", "shortCiteRegEx": "Daniel", "year": 2016}, {"title": "J", "author": ["V.R. Konda", "Tsitsiklis"], "venue": "N.", "citeRegEx": "Konda and Tsitsiklis 2000", "shortCiteRegEx": null, "year": 2000}, {"title": "and Barto", "author": ["G. Konidaris"], "venue": "A.", "citeRegEx": "Konidaris and Barto 2009", "shortCiteRegEx": null, "year": 2009}, {"title": "A", "author": ["G. Konidaris", "S. Kuindersma", "R.A. Grupen", "Barto"], "venue": "G.", "citeRegEx": "Konidaris et al. 2011", "shortCiteRegEx": null, "year": 2011}, {"title": "Hierarchical deep reinforcement learning: Integrating temporal abstraction and intrinsic motivation", "author": ["Kulkarni"], "venue": "In Advances in Neural Information Processing Systems", "citeRegEx": "Kulkarni,? \\Q2016\\E", "shortCiteRegEx": "Kulkarni", "year": 2016}, {"title": "and Shimkin", "author": ["K.Y. Levy"], "venue": "N.", "citeRegEx": "Levy and Shimkin 2011", "shortCiteRegEx": null, "year": 2011}, {"title": "T", "author": ["Mankowitz, D.J.", "Mann"], "venue": "A.; and Mannor, S.", "citeRegEx": "Mankowitz. Mann. and Mannor 2016", "shortCiteRegEx": null, "year": 2016}, {"title": "D", "author": ["Mann, T.A.", "Mankowitz"], "venue": "J.; and Mannor, S.", "citeRegEx": "Mann. Mankowitz. and Mannor 2014", "shortCiteRegEx": null, "year": 2014}, {"title": "A", "author": ["A. McGovern", "Barto"], "venue": "G.", "citeRegEx": "McGovern and Barto 2001", "shortCiteRegEx": null, "year": 2001}, {"title": "Q-cut - dynamic discovery of sub-goals in reinforcement learning", "author": ["Mannor Menache", "I. Shimkin 2002] Menache", "S. Mannor", "N. Shimkin"], "venue": null, "citeRegEx": "Menache et al\\.,? \\Q2002\\E", "shortCiteRegEx": "Menache et al\\.", "year": 2002}, {"title": "Playing atari with deep reinforcement learning", "author": ["A. M"], "venue": "CoRR abs/1312.5602", "citeRegEx": "M.,? \\Q2013\\E", "shortCiteRegEx": "M.", "year": 2013}, {"title": "T", "author": ["V. Mnih", "A.P. Badia", "M. Mirza", "A. Graves", "Lillicrap"], "venue": "P.; Harley, T.; Silver, D.; and Kavukcuoglu, K.", "citeRegEx": "Mnih et al. 2016", "shortCiteRegEx": null, "year": 2016}, {"title": "A", "author": ["S. Niekum", "Barto"], "venue": "G.", "citeRegEx": "Niekum and Barto 2011", "shortCiteRegEx": null, "year": 2011}, {"title": "M", "author": ["Puterman"], "venue": "L.", "citeRegEx": "Puterman 1994", "shortCiteRegEx": null, "year": 1994}, {"title": "and Ciosek", "author": ["D. Silver"], "venue": "K.", "citeRegEx": "Silver and Ciosek 2012", "shortCiteRegEx": null, "year": 2012}, {"title": "S", "author": ["J. Sorg", "Singh"], "venue": "P.", "citeRegEx": "Sorg and Singh 2010", "shortCiteRegEx": null, "year": 2010}, {"title": "and Precup", "author": ["M. Stolle"], "venue": "D.", "citeRegEx": "Stolle and Precup 2002", "shortCiteRegEx": null, "year": 2002}, {"title": "Y", "author": ["R.S. Sutton", "D.A. McAllester", "S.P. Singh", "Mansour"], "venue": "2000. Policy gradient methods for reinforcement learning with function approximation. In Advances in Neural Information Processing Systems 12. 1057\u2013", "citeRegEx": "Sutton et al. 2000", "shortCiteRegEx": null, "year": 1063}, {"title": "S", "author": ["R.S. Sutton", "D. Precup", "Singh"], "venue": "P.", "citeRegEx": "Sutton. Precup. and Singh 1999", "shortCiteRegEx": null, "year": 1999}, {"title": "R", "author": ["Sutton"], "venue": "S.", "citeRegEx": "Sutton 1984", "shortCiteRegEx": null, "year": 1984}, {"title": "Bias in natural actor-critic algorithms", "author": ["P. Thomas"], "venue": "In ICML,", "citeRegEx": "Thomas,? \\Q2014\\E", "shortCiteRegEx": "Thomas", "year": 2014}, {"title": "A", "author": ["Vezhnevets"], "venue": "S.; Mnih, V.; Agapiou, J.; Osindero, S.; Graves, A.; Vinyals, O.; and Kavukcuoglu, K.", "citeRegEx": "Vezhnevets et al. 2016", "shortCiteRegEx": null, "year": 2016}], "referenceMentions": [{"referenceID": 0, "context": "References [Baird 1993] Baird, L.", "startOffset": 11, "endOffset": 23}, {"referenceID": 1, "context": "[Bellemare et al. 2013] Bellemare, M.", "startOffset": 0, "endOffset": 23}, {"referenceID": 2, "context": "[Comanici and Precup 2010] Comanici, G.", "startOffset": 0, "endOffset": 26}, {"referenceID": 3, "context": "[\u015eim\u015fek and Barto 2009] \u015eim\u015fek, O.", "startOffset": 0, "endOffset": 23}, {"referenceID": 5, "context": "[Konda and Tsitsiklis 2000] Konda, V.", "startOffset": 0, "endOffset": 27}, {"referenceID": 6, "context": "[Konidaris and Barto 2009] Konidaris, G.", "startOffset": 0, "endOffset": 26}, {"referenceID": 7, "context": "[Konidaris et al. 2011] Konidaris, G.", "startOffset": 0, "endOffset": 23}, {"referenceID": 9, "context": "[Levy and Shimkin 2011] Levy, K.", "startOffset": 0, "endOffset": 23}, {"referenceID": 10, "context": "[Mankowitz, Mann, and Mannor 2016] Mankowitz, D.", "startOffset": 0, "endOffset": 34}, {"referenceID": 11, "context": "[Mann, Mankowitz, and Mannor 2014] Mann, T.", "startOffset": 0, "endOffset": 34}, {"referenceID": 12, "context": "[McGovern and Barto 2001] McGovern, A.", "startOffset": 0, "endOffset": 25}, {"referenceID": 15, "context": "[Mnih et al. 2016] Mnih, V.", "startOffset": 0, "endOffset": 18}, {"referenceID": 16, "context": "[Niekum and Barto 2011] Niekum, S.", "startOffset": 0, "endOffset": 23}, {"referenceID": 17, "context": "[Puterman 1994] Puterman, M.", "startOffset": 0, "endOffset": 15}, {"referenceID": 18, "context": "[Silver and Ciosek 2012] Silver, D.", "startOffset": 0, "endOffset": 24}, {"referenceID": 19, "context": "[Sorg and Singh 2010] Sorg, J.", "startOffset": 0, "endOffset": 21}, {"referenceID": 20, "context": "[Stolle and Precup 2002] Stolle, M.", "startOffset": 0, "endOffset": 24}, {"referenceID": 21, "context": "[Sutton et al. 2000] Sutton, R.", "startOffset": 0, "endOffset": 20}, {"referenceID": 22, "context": "[Sutton, Precup, and Singh 1999] Sutton, R.", "startOffset": 0, "endOffset": 32}, {"referenceID": 23, "context": "[Sutton 1984] Sutton, R.", "startOffset": 0, "endOffset": 13}, {"referenceID": 24, "context": "[Thomas 2014] Thomas, P.", "startOffset": 0, "endOffset": 13}, {"referenceID": 25, "context": "[Vezhnevets et al. 2016] Vezhnevets, A.", "startOffset": 0, "endOffset": 24}], "year": 2016, "abstractText": "Temporal abstraction is key to scaling up learning and planning in reinforcement learning. While planning with temporally extended actions is well understood, creating such abstractions autonomously from data has remained challenging. We tackle this problem in the framework of options [Sutton, Precup & Singh, 1999; Precup, 2000]. We derive policy gradient theorems for options and propose a new option-critic architecture capable of learning both the internal policies and the termination conditions of options, in tandem with the policy over options, and without the need to provide any additional rewards or subgoals. Experimental results in both discrete and continuous environments showcase the flexibility and efficiency of the framework. Introduction Temporal abstraction allows representing knowledge about course of actions that take place at different time scales. In reinforcement learning, options (Sutton, Precup, and Singh 1999; Precup 2000) provide a framework for defining such courses of action and for seamlessly learning and planning with them. Discovering temporal abstractions autonomously has been the subject of extensive research efforts in the last 15 years (McGovern and Barto 2001; Stolle and Precup 2002; Menache, Mannor, and Shimkin 2002; \u015eim\u015fek and Barto 2009; Silver and Ciosek 2012), but approaches that can be used naturally with continuous state and/or action spaces have only recently started to become feasible (Konidaris et al. 2011; Niekum and Barto 2011; Mann, Mannor, and Precup ; Mankowitz, Mann, and Mannor 2016; Kulkarni et al. 2016; Vezhnevets et al. 2016; Daniel et al. 2016). The majority of the existing work has focused on finding subgoals (useful states that an agent should reach) and subsequently learning policies to achieve them. This idea has lead to interesting methods but ones which are also difficult to scale up given their \u201ccombinatorial\u201d flavor. Additionally, learning policies associated with subgoals can be expensive in terms of data and computation time; in the worst case, it can be as expensive as solving the entire task. We present an alternative view, which blurs the line between the problem of discovering options from that of learning options. Based on the policy gradient theorem (Sutton et al. 2000), we derive new results which enable a gradual learning process of the intra-option policies and termination functions, simultaneously with the policy over them. This approach works naturally with both linear and non-linear function approximators, under discrete or continuous state and action spaces. Existing methods for learning options are considerably slower when learning from a single task: much of the benefit will come from re-using the learned options in similar tasks. In contrast, we show that our approach is capable of successfully learning options within a single task without incurring any slowdown and while still providing re-use speedups. We start by reviewing background related to the two main ingredients of our work: policy gradient methods and options. We then describe the core ideas of our approach: the intra-option policy and termination gradient theorems. Additional technical details are included in the appendix. We present experimental results showing that our approach learns meaningful temporally extended behaviors in an effective manner. As opposed to other methods, we only need to specify the number of desired options; it is not necessary to have subgoals, extra rewards, demonstrations, multiple problems or any other special accommodations (however, the approach can work with pseudo-reward functions if desired). To our knowledge, this is the first end-to-end approach for learning options that scales to very large domains at comparable efficiency. Preliminaries and Notation A Markov Decision Process consists of a set of states S, a set of actionsA, a transition function P : S\u00d7A \u2192 (S \u2192 [0, 1]) and a reward function r : S \u00d7 A \u2192 R. For convenience, we develop our ideas assuming discrete state and action sets. However, our results extend to continuous spaces using usual measure-theoretic assumptions (some of our empirical results are in continuous tasks). A (Markovian stationary) policy is a probability distribution over actions conditioned on states, \u03c0 : S \u00d7 A \u2192 [0, 1]. In discounted problems, the value function of a policy \u03c0 is defined as the expected return: V\u03c0(s) = E\u03c0 [ \u2211\u221e t=0 \u03b3 rt+1 | s0 = s] and its action-value function as Q\u03c0(s, a) = E\u03c0 [ \u2211\u221e t=0 \u03b3 rt+1 | s0 = s, a0 = a], where \u03b3 \u2208 [0, 1) is the discount factor. A policy \u03c0 is greedy with respect to a given action-value function Q if \u03c0(s, a) > 0 iff a = argmaxa\u2032 Q(s, a \u2032). In a discrete MDP, there is at least one optimal policy which is greedy with rear X iv :1 60 9. 05 14 0v 1 [ cs .A I] 1 6 Se p 20 16 spect to its own action-value function. Policy gradient methods (Sutton et al. 2000; Konda and Tsitsiklis 2000) address the problem of finding a good policy by performing stochastic gradient descent to optimize a performance objective over a given family of parametrized stochastic policies, \u03c0\u03b8. The policy gradient theorem (Sutton et al. 2000) provides expressions for the gradient of the average reward and discounted reward objectives with respect to \u03b8. In the discounted setting, the objective is defined with respect to a designated start state (or distribution) s0: \u03c1(\u03b8, s0) = E\u03c0\u03b8 [ \u2211 t=0 \u03b3 rt+1 | s0]. The policy gradient theorem shows that: \u2202\u03c1(\u03b8,s0) \u2202\u03b8 = \u2211 s \u03bc\u03c0\u03b8 (s | s0) \u2211 a \u2202\u03c0\u03b8(a|s) \u2202\u03b8 Q\u03c0\u03b8 (s, a), where \u03bc\u03c0\u03b8 (s | s0) = \u2211\u221e t=0 \u03b3 t P (st = s | s0) is a discounted weighting of the states along the trajectories starting from s0. In practice, the policy gradient is estimated from samples along the on-policy stationary distribution. (Thomas 2014) showed that neglecting the discount factor in this stationary distribution makes the usual policy gradient estimator biased. However, correcting for this discrepancy also reduces data efficiency. For simplicity, we build on the framework of (Sutton et al. 2000) and discuss how to extend our results according to (Thomas 2014). The options framework (Sutton, Precup, and Singh 1999; Precup 2000) formalizes the idea of temporally extended actions. A Markovian option \u03c9 \u2208 \u03a9 is a triple (I\u03c9, \u03c0\u03c9, \u03b2\u03c9) in which I\u03c9 \u2286 S is an initiation set, \u03c0\u03c9 is an intra-option policy, and \u03b2\u03c9 : S \u2192 [0, 1] is a termination function. We also assume that \u2200s \u2208 S,\u2200\u03c9 \u2208 \u03a9 : s \u2208 I\u03c9 (i.e., all options are available everywhere), an assumption made in the majority of options discovery algorithms. We will discuss how to dispense with this assumption in the final section. (Sutton, Precup, and Singh 1999; Precup 2000) show that an MDP endowed with a set of options becomes a Semi-Markov Decision Process (Puterman 1994, chapter 11), which has a corresponding optimal value function over options V\u03a9(s) and option-value function Q\u03a9(s, \u03c9). Learning and planning algorithms for MDPs have their counterparts in this setting. However, the existence of the underlying MDP offers the possibility of learning about many different options in parallel : the idea of intra-option learning, which we leverage in our work. Learning Options We adopt a continual perspective on the problem of learning options. At any time, we would like to distill all of the available experience into every component of our system: value function and policy over options, intra-option policies and termination functions. To achieve this goal, we focus on learning option policies and termination functions, assuming they are represented using differentiable parameterized function approximators. We consider the call-and-return option execution model, in which an agent picks option \u03c9 according to its policy over options \u03c0\u03a9 , then follows the intra-option policy \u03c0\u03c9 until termination (as dictated by \u03b2\u03c9), at which point this procedure is repeated. Let \u03c0\u03c9,\u03b8 denote the intra-option policy of option \u03c9 parametrized by \u03b8 and \u03b2\u03c9,\u03b8, the termination function of \u03c9 parameterized by \u03b8. We present two new results for learning options, obtained using as blueprint the policy gradient theorem (Sutton et al. 2000). Both results are derived under the assumption that the goal is to learn options that maximize the expected return in the current task. However, if one wanted to add extra information to the objective function, this could readily be done so long as it comes in the form of an additive differentiable function. Suppose we aim to optimize directly the discounted return, expected over all the trajectories starting at a designated state s0 and option \u03c90, then: \u03c1(\u03a9, \u03b8, \u03b8, s0, \u03c90) = E\u03a9,\u03b8,\u03c9 [ \u2211\u221e t=0 \u03b3 rt+1 | s0, \u03c90]. Note that this return depends on the policy over options, as well as the parameters of the option policies and termination functions. We will take gradients of this objective with respect to \u03b8 and \u03b8. In order to do this, we will manipulate equations similar to those used in intra-option learning (Sutton, Precup, and Singh 1999, section 8). Specifically, the definition of the option-value function can be written as: Q\u03a9(s, \u03c9) = E \u03a9,\u03b8,\u03b8 [ \u221e \u2211 t=0 \u03b3rt+1 \u2223\u2223\u2223\u2223 s0 = s, \u03c90 = \u03c9 ]", "creator": "LaTeX with hyperref package"}}}