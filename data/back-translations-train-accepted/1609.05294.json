{"id": "1609.05294", "review": {"conference": "AAAI", "VERSION": "v1", "DATE_OF_SUBMISSION": "17-Sep-2016", "title": "Sparse Boltzmann Machines with Structure Learning as Applied to Text Analysis", "abstract": "We are interested in exploring the possibility and benefits of structure learning for deep models. As the first step, this paper investigates the matter for Restricted Boltzmann Machines (RBMs). We conduct the study with Replicated Softmax, a variant of RBMs for unsupervised text analysis. We present a method for learning what we call Sparse Boltzmann Machines, where each hidden unit is connected to a subset of the visible units instead of all of them. Empirical results show that the method yields models with significantly improved model fit and interpretability as compared with RBMs where each hidden unit is connected to all visible units.", "histories": [["v1", "Sat, 17 Sep 2016 08:17:36 GMT  (175kb,D)", "http://arxiv.org/abs/1609.05294v1", null], ["v2", "Tue, 20 Sep 2016 11:51:20 GMT  (175kb,D)", "http://arxiv.org/abs/1609.05294v2", null]], "reviews": [], "SUBJECTS": "cs.LG", "authors": ["zhourong chen", "nevin l zhang", "dit-yan yeung", "peixian chen"], "accepted": true, "id": "1609.05294"}, "pdf": {"name": "1609.05294.pdf", "metadata": {"source": "CRF", "title": "Sparse Boltzmann Machines with Structure Learning as Applied to Text Analysis", "authors": ["Zhourong Chen", "Nevin L. Zhang", "Dit-Yan Yeung", "Peixian Chen"], "emails": [], "sections": [{"heading": "Introduction", "text": "This year is the highest in the history of the country."}, {"heading": "Related Works", "text": "The authors use sparse RBMs to build sparse Deep Belief Networks and extract some interesting features. However, sparse RBMs were not defined in their paper from the perspective of sparse connections: Xiv: 160 9.05 294v 1 [cs.L G] 17 September 2016 but sparse activations of hidden units. And it was achieved by adding a term of regulation to the objective function in training the parameters. There is no structural learning. Network circumcision is also a potential method to optimize the structure of a neural network. Biased weight loss was the early approach to circumcision. Later, Optimal Brain Damage (Cun, Denker and Solla, 1990) and Optimal Brain Surgeon (Hassibi, Stork and Com, 1993) were said to be the inadequate parameters of circumcision possibly not the best strategies and they characterized the influences based on the loss of networks (based directly on tangible methods)."}, {"heading": "Restricted Boltzmann Machines", "text": "An example is shown in Figure 1. In the simplest case, all units are assumed to be binary. An energy function is defined across all units as follows: E (v, h) = vocabulary between units on the same level, while there are no connections between units on the same level: E (v, h) = vocabulary between units on the same level. An example is shown in Figure 1. \u2212 An example is shown in Figure 1. P \u2212 Kjv k = 1 vkbk \u2212 F (1), where all units are considered binary. An energy function is defined across all units as follows: E (v, h) = vocabulary on the same level."}, {"heading": "Sparse Boltzmann Machines", "text": "In this section, we will introduce our new models, Sparse Boltzmann Machines (SBMs). An SBM is a two-layer, undirected graphical model with a layer of K visible units {v1,..., vK} and a layer of F hidden units {h1,..., hF}. The hidden units in SBMs are directly linked to a tree structure, while each hidden unit is also individually connected to a subset of visible units. See Figure 2 for an example of SBM. In SBM, the number of hidden units and the connections are both learned from data. A technical difference between SBMs and RBMs is that there are direct connections between the hidden units in SBMs. We call them hidden connections. The reason why we introduce the hidden connections in our models is that the hidden connections provide a way to relate a hidden unit with a visible unit without a direct connection. For example, Figure 2 Hidden Unit is not directly connected to h1."}, {"heading": "Parameter Learning", "text": "SBMs can also be used for text analysis as RBMs. (Un = U = U = U = U = U = U = U = U = U = U = U = U = U = U = U = U = U = U = U = U = U (U = U = U). (U = U = h). (U = U = h). (U = h). (U = h). (U = h). (U = h). (U = h). (U = h). (U = h). (U = h). (U = h). (U = h). (U = h). (U = h). (U = h). (U = h). (U = h). (U = h). (U = h). (U = h). (U = h). (U = h). (U = h)."}, {"heading": "Structure Learning", "text": "In fact, it is so that one sees oneself in a position to surpass oneself. (...) It is not so that one is able to surpass oneself. (...) \"It is as if.\" (...) \"It is as if.\" (...) \"It is as if.\" (...) \"It is as if.\" (...) \"It is as if.\" (...) \"It is as if.\" (...) \"It is as if.\" (...) \"It is as if.\" (...) \"It is as if.\" (...). \"(...)\" It is as if. \"(...)\" It is as if. \"(...).\" (It is. \"(It is.)\" (It is. \"(It is.)\" (It is. \"(It is.\" (It is.). \"(It is.\" (It is. \"(It is.)\" (It is. \"(It is.\" It. \"It is.\" (It is. \"It.\" It. \"It is.\" (It is. \"It.\" It. \"It is.\" It. \"(It is.\" It is. \"It.\" It. \"It is.\" (It. \"It.\" It. \"It.\" It is. \"It.\" It. \")\" (it. \"It is.\" It. (It. \"It is.\" It. \"It is.\" (it. \"It is.\" It. \").\" (it. (it. \"it.\" It is. \"(it.\" it. \"is.\" It is. \"). (it.\" (it. \"It is.\"). (it. \"(it.\" is. \"(it.\" it. \"is.\") \"it. (it.\" It is. \"it.\" (it. \"It is.\" it. \"(it.\" is. \"is.\"). (it. (it.). (it. \"it. (it.\" it. \"is.\" is. \"is.\" it. \"it.\" is. (it. \"is.).\" it. (it. (it. \"is.\""}, {"heading": "Experiments", "text": "In this section we test the performance of our Sparse Boltzmann machines on the basis of three text data sets of different scales: Algorithm 1 SBM-SFC (T) Inputs: T - Diagram of an HLTM, U - Collection of training documents, M - Number of new connections for each hidden unit. Outputs: Chart T \"of a corresponding SBM. 1: T\" \u2190, HL - Chart of the top latent layer in T 2: V. \"add\" Variables in T 3: T. \"add\" (HL), T. \"add\" Units (V) 4: for variable Z in HL do 5: VZ \"observed variables in sub-tree with roots in Z 6: T.\" add \"edges (Z, VZ), I 7: for V\" in (V \u2212 VZ), T. \"add Units (V), T.\" add Units (V) 4: for variable Z in HL do 5: VZ \"observed variables in sub-tree with roots in Z.\" Z \""}, {"heading": "Datasets", "text": "The NIPS essays consist of 1,740 NIPS essays published between 1987 and 1999. Randomly sampling, we sample 1,640 essays as training data, 50 as validation data, and the remaining 50 as test data. We process the data and select 1,000 most common words in the entire corpus. In this way, each document is presented as a vector of 1,000 dimensions, each element being the number of times the word occurs in current documents. CiteULike's article collection includes 16,980 articles. We also randomly divide it into training data with 12,000 articles, validation data with 1,000 articles, and test data with 3,980 articles. 2,000 words with the highest average TF-IDF values are selected to represent the articles. The New York Times data set includes 300,000 documents from which we randomly select 290,000 documents for training, 1,000 for validation, and 9,000 for testing with the highest WF-10,000 documents."}, {"heading": "Training", "text": "The batch sizes of the NIPS, CiteULike and New York Times datasets are 10, 100 and 1,000, respectively. Model parameters are updated after each mini batch. If we assume that going through all mini batches counts as one epoch, we set the maximum number of training periods to 50. And we train all1Available at http: / / www.cs.nyu.edu / \u0445 roweis / data.html 2Available at http: / / www.wanghao.in / data / ctrsr datasets.rar 3Available at http: / / archive.ics.uci.edu / ml / datasets / Bag + of + of + Wordsthe models using the Contrastive Divergence algorithm with T = 10 full Gibbs steps."}, {"heading": "Evaluations", "text": "Since the exact calculation of this value (due to the partition function) is difficult, Annealed Importance Sampling (AIS) (Neal, 2001; Salakhutdinov and Murray, 2008) was used in Hinton and Salakhutdinov (2009) to estimate the partition function of Replicated Softmax. In our experiments, we extend AIS to Sparse Boltzmann Machines (Neal, 2001; Salakhutdinov and Murray, 2008). In AIS, we use 500 \"inverse temperatures\" \u03b2k at intervals of 0 to 0.5, 3,000 \u03b2k at intervals of 0.5 to 0.9 and 6,500 \u03b2k at intervals of 0.9 to 1.0, with a total of 10,000 intermediate distributions. The estimates are based on an average of over 100 AIS passes for each document endured. We then calculate the average perplexity per word as exp \u2212 1N N N N Un login the score = 1 PN value for the lower data (D1 PN value)."}, {"heading": "Results", "text": "This year, it has reached the point where it will be able to retaliate until it is able to retaliate."}, {"heading": "Conclusions", "text": "In this paper, we have developed a method to determine the number of hidden units and the connections between units for models with a single hidden layer. Models obtained with this method are significantly better in terms of the likelihood of lasting longer than RBMs, where the hidden and observed units are fully connected, even if the number of hidden units in RBMs is optimized by heldout validation. Compared to redundancy, our method is more efficient and able to determine the number of hidden units, and it also produces more interpretable models. In the future, we will generalize the method of structural learning to models with multiple hidden layers."}], "references": [{"title": "Latent dirichlet allocation", "author": ["D.M. Blei", "A.Y. Ng", "M.I. Jordan"], "venue": "Journal of Machine Learning Research 3:993\u20131022.", "citeRegEx": "Blei et al\\.,? 2003", "shortCiteRegEx": "Blei et al\\.", "year": 2003}, {"title": "Progressive EM for latent tree models and hierarchical topic detection", "author": ["P. Chen", "N.L. Zhang", "L.K.M. Poon", "Z. Chen"], "venue": "Proceedings of the Thirtieth AAAI Conference on Artificial Intelligence, 1498\u20131504.", "citeRegEx": "Chen et al\\.,? 2016", "shortCiteRegEx": "Chen et al\\.", "year": 2016}, {"title": "Optimal brain damage", "author": ["Y.L. Cun", "J.S. Denker", "S.A. Solla"], "venue": "Advances in Neural Information Processing Systems, 598\u2013605.", "citeRegEx": "Cun et al\\.,? 1990", "shortCiteRegEx": "Cun et al\\.", "year": 1990}, {"title": "Learning both weights and connections for efficient neural network", "author": ["S. Han", "J. Pool", "J. Tran", "W. Dally"], "venue": "Advances in Neural Information Processing Systems 28. Curran Associates, Inc. 1135\u20131143.", "citeRegEx": "Han et al\\.,? 2015", "shortCiteRegEx": "Han et al\\.", "year": 2015}, {"title": "Second order derivatives for network pruning: Optimal brain surgeon", "author": ["B. Hassibi", "D.G. Stork", "S.C.R. Com"], "venue": "Advances in Neural Information Processing Systems 5, 164\u2013171.", "citeRegEx": "Hassibi et al\\.,? 1993", "shortCiteRegEx": "Hassibi et al\\.", "year": 1993}, {"title": "Replicated softmax: an undirected topic model", "author": ["G.E. Hinton", "R.R. Salakhutdinov"], "venue": "Advances in Neural Information Processing Systems 22. 1607\u20131614.", "citeRegEx": "Hinton and Salakhutdinov,? 2009", "shortCiteRegEx": "Hinton and Salakhutdinov", "year": 2009}, {"title": "Deep neural networks for acoustic modeling in speech recognition: The shared views of four research groups", "author": ["G.E. Hinton", "L. Deng", "D. Yu", "G.E. Dahl", "A.-r. Mohamed", "N. Jaitly", "A. Senior", "V. Vanhoucke", "P. Nguyen", "T. N Sainath"], "venue": "IEEE Signal Processing", "citeRegEx": "Hinton et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Hinton et al\\.", "year": 2012}, {"title": "Training products of experts by minimizing contrastive divergence", "author": ["G.E. Hinton"], "venue": "Neural Computation 14(8):1771\u20131800.", "citeRegEx": "Hinton,? 2002", "shortCiteRegEx": "Hinton", "year": 2002}, {"title": "Imagenet classification with deep convolutional neural networks", "author": ["A. Krizhevsky", "I. Sutskever", "G.E. Hinton"], "venue": "Advances in Neural Information Processing Systems, 1097\u20131105.", "citeRegEx": "Krizhevsky et al\\.,? 2012", "shortCiteRegEx": "Krizhevsky et al\\.", "year": 2012}, {"title": "Gradient-based learning applied to document recognition", "author": ["Y. Lecun", "L. Bottou", "Y. Bengio", "P. Haffner"], "venue": "Proceedings of the IEEE, 2278\u20132324.", "citeRegEx": "Lecun et al\\.,? 1998", "shortCiteRegEx": "Lecun et al\\.", "year": 1998}, {"title": "Sparse deep belief net model for visual area v2", "author": ["H. Lee", "C. Ekanadham", "A.Y. Ng"], "venue": "Advances in Neural Information Processing Systems, 873\u2013880.", "citeRegEx": "Lee et al\\.,? 2008", "shortCiteRegEx": "Lee et al\\.", "year": 2008}, {"title": "Hierarchical latent tree analysis for topic detection", "author": ["T. Liu", "N.L. Zhang", "P. Chen"], "venue": "Machine Learning and Knowledge Discovery in Databases 2014, 256\u2013272.", "citeRegEx": "Liu et al\\.,? 2014", "shortCiteRegEx": "Liu et al\\.", "year": 2014}, {"title": "Strategies for training large scale neural network language models", "author": ["T. Mikolov", "A. Deoras", "D. Povey", "L. Burget", "J. \u010cernock\u1ef3"], "venue": "IEEE Workshop on Automatic Speech Recognition and Understanding, 196\u2013 201.", "citeRegEx": "Mikolov et al\\.,? 2011", "shortCiteRegEx": "Mikolov et al\\.", "year": 2011}, {"title": "Efficient estimation of word representations in vector space", "author": ["T. Mikolov", "K. Chen", "G. Corrado", "J. Dean"], "venue": "International Conference on Learning Representations Workshops.", "citeRegEx": "Mikolov et al\\.,? 2013a", "shortCiteRegEx": "Mikolov et al\\.", "year": 2013}, {"title": "Distributed representations of words and phrases and their compositionality", "author": ["T. Mikolov", "I. Sutskever", "K. Chen", "G.S. Corrado", "J. Dean"], "venue": "Advances in Neural Information Processing Systems 26, 3111\u20133119.", "citeRegEx": "Mikolov et al\\.,? 2013b", "shortCiteRegEx": "Mikolov et al\\.", "year": 2013}, {"title": "Machine learning: a probabilistic perspective", "author": ["K.P. Murphy"], "venue": null, "citeRegEx": "Murphy,? \\Q2012\\E", "shortCiteRegEx": "Murphy", "year": 2012}, {"title": "Annealed importance sampling", "author": ["R.M. Neal"], "venue": "Statistics and Computing 11(2):125\u2013139.", "citeRegEx": "Neal,? 2001", "shortCiteRegEx": "Neal", "year": 2001}, {"title": "On the quantitative analysis of deep belief networks", "author": ["R. Salakhutdinov", "I. Murray"], "venue": "Proceedings of the 25th International Conference on Machine Learning, 872\u2013879.", "citeRegEx": "Salakhutdinov and Murray,? 2008", "shortCiteRegEx": "Salakhutdinov and Murray", "year": 2008}, {"title": "Parallel distributed processing: Explorations in the microstructure of cognition, vol", "author": ["P. Smolensky"], "venue": "1. chapter Information Processing in Dynamical Systems: Foundations of Harmony Theory, 194\u2013281.", "citeRegEx": "Smolensky,? 1986", "shortCiteRegEx": "Smolensky", "year": 1986}, {"title": "Data-free parameter pruning for deep neural networks", "author": ["S. Srinivas", "R.V. Babu"], "venue": "Proceedings of the British Machine Vision Conference, 31.1\u201331.12.", "citeRegEx": "Srinivas and Babu,? 2015", "shortCiteRegEx": "Srinivas and Babu", "year": 2015}, {"title": "Dropout: A simple way to prevent neural networks from overfitting", "author": ["N. Srivastava", "G.E. Hinton", "A. Krizhevsky", "I. Sutskever", "R. Salakhutdinov"], "venue": "Journal of Machine Learning Research 15(1):1929\u20131958.", "citeRegEx": "Srivastava et al\\.,? 2014", "shortCiteRegEx": "Srivastava et al\\.", "year": 2014}, {"title": "Sequence to sequence learning with neural networks", "author": ["I. Sutskever", "O. Vinyals", "Q.V. Le"], "venue": "Advances in Neural Information Processing Systems, 3104\u20133112.", "citeRegEx": "Sutskever et al\\.,? 2014", "shortCiteRegEx": "Sutskever et al\\.", "year": 2014}, {"title": "Regularization of neural networks using dropconnect", "author": ["L. Wan", "M. Zeiler", "S. Zhang", "Y.L. Cun", "R. Fergus"], "venue": "Proceedings of the 30th International Conference on Machine Learning, 1058\u20131066.", "citeRegEx": "Wan et al\\.,? 2013", "shortCiteRegEx": "Wan et al\\.", "year": 2013}], "referenceMentions": [{"referenceID": 6, "context": "It has produced superior results in a range of applications, including image classification (Krizhevsky, Sutskever, and Hinton, 2012), speech recognition (Hinton et al., 2012; Mikolov et al., 2011), language translation (Sutskever, Vinyals, and Le, 2014) and so on.", "startOffset": 154, "endOffset": 197}, {"referenceID": 12, "context": "It has produced superior results in a range of applications, including image classification (Krizhevsky, Sutskever, and Hinton, 2012), speech recognition (Hinton et al., 2012; Mikolov et al., 2011), language translation (Sutskever, Vinyals, and Le, 2014) and so on.", "startOffset": 154, "endOffset": 197}, {"referenceID": 9, "context": "(2015) have shown that many weak connections in the fully connected layers of Convolutional Neural Networks (CNNs) (Lecun et al., 1998) can be pruned without incurring any accuracy loss.", "startOffset": 115, "endOffset": 135}, {"referenceID": 20, "context": "One method to address the problem is dropout (Srivastava et al., 2014), which randomly drops out units (while keeping full connectivity) during training.", "startOffset": 45, "endOffset": 70}, {"referenceID": 3, "context": "How can one learn sparse deep models? One method is to first learn a fully connected model and then prune weak connections (Han et al., 2015).", "startOffset": 123, "endOffset": 141}, {"referenceID": 2, "context": "In fact, Han et al. (2015) have shown that many weak connections in the fully connected layers of Convolutional Neural Networks (CNNs) (Lecun et al.", "startOffset": 9, "endOffset": 27}, {"referenceID": 2, "context": "(2015) have shown that many weak connections in the fully connected layers of Convolutional Neural Networks (CNNs) (Lecun et al., 1998) can be pruned without incurring any accuracy loss. The convolutional layers of CNNs are sparse, and the fact is considered one of the key factors that lead to the success of CNNs. Moreover, it is well known that overfitting is a serious problem in deep models. One method to address the problem is dropout (Srivastava et al., 2014), which randomly drops out units (while keeping full connectivity) during training. The possibility of randomly dropping connections has also been explored in Wan et al. (2013). Sparseness offers an interesting alternative.", "startOffset": 118, "endOffset": 644}, {"referenceID": 3, "context": "With respect to deep neural networks, Han et al. (2015) proposed to compress a network through a threestep process: train, prune connections, and retrain.", "startOffset": 38, "endOffset": 56}, {"referenceID": 3, "context": "With respect to deep neural networks, Han et al. (2015) proposed to compress a network through a threestep process: train, prune connections, and retrain. We call it redundancy pruning. In contrast, Srinivas and Babu (2015) proposed to prune redundant neurons directly.", "startOffset": 38, "endOffset": 224}, {"referenceID": 18, "context": "An Restricted Boltzmann Machine (RBM) (Smolensky, 1986) is a two-layer undirected graphical model with a layer of K visible units {v, .", "startOffset": 38, "endOffset": 55}, {"referenceID": 7, "context": "The model parameters of an RBM are learned using the Contrastive Divergence algorithm (Hinton, 2002), which maximizes the data likelihood via stochastic gradient descent.", "startOffset": 86, "endOffset": 100}, {"referenceID": 5, "context": "In Hinton and Salakhutdinov (2009), RBM was used for topic modeling and the proposed model was called Replicated Softmax.", "startOffset": 3, "endOffset": 35}, {"referenceID": 1, "context": "Figure 4: An example HLTM from Chen et al. (2016).", "startOffset": 31, "endOffset": 50}, {"referenceID": 15, "context": "Nevertheless, since the hidden units in Sparse Boltzmann Machines are linked as a tree structure, we can easily compute the value of P (hj |Un) and P (h|Un) by conducting message propagation (Murphy, 2012) in the model.", "startOffset": 191, "endOffset": 205}, {"referenceID": 1, "context": "Recently, Liu, Zhang, and Chen (2014) and Chen et al. (2016) proposed a method, called HLTA, for learning a Hierarchical Latent Tree Model (HLTM) from data.", "startOffset": 42, "endOffset": 61}, {"referenceID": 16, "context": "As exactly computing these value is intractable (due to the partition function), Annealed Importance Sampling (AIS) (Neal, 2001; Salakhutdinov and Murray, 2008) was used in Hinton and Salakhutdinov (2009) to estimate the partition function of Replicated Softmax.", "startOffset": 116, "endOffset": 160}, {"referenceID": 17, "context": "As exactly computing these value is intractable (due to the partition function), Annealed Importance Sampling (AIS) (Neal, 2001; Salakhutdinov and Murray, 2008) was used in Hinton and Salakhutdinov (2009) to estimate the partition function of Replicated Softmax.", "startOffset": 116, "endOffset": 160}, {"referenceID": 5, "context": "As exactly computing these value is intractable (due to the partition function), Annealed Importance Sampling (AIS) (Neal, 2001; Salakhutdinov and Murray, 2008) was used in Hinton and Salakhutdinov (2009) to estimate the partition function of Replicated Softmax.", "startOffset": 173, "endOffset": 205}, {"referenceID": 5, "context": "Due to the high computation cost, we follow the experiments in Hinton and Salakhutdinov (2009) and randomly sample 50 documents from the validation data to calculate the score.", "startOffset": 63, "endOffset": 95}, {"referenceID": 3, "context": "Comparisons with Redundancy Pruning We also compare our method with the redundancy pruning method which produces Replicated Softmax with sparse connections (Han et al., 2015).", "startOffset": 156, "endOffset": 174}], "year": 2016, "abstractText": "We are interested in exploring the possibility and benefits of structure learning for deep models. As the first step, this paper investigates the matter for Restricted Boltzmann Machines (RBMs). We conduct the study with Replicated Softmax, a variant of RBMs for unsupervised text analysis. We present a method for learning what we call Sparse Boltzmann Machines, where each hidden unit is connected to a subset of the visible units instead of all of them. Empirical results show that the method yields models with significantly improved model fit and interpretability as compared with RBMs where each hidden unit is connected to all visible units.", "creator": "LaTeX with hyperref package"}}}