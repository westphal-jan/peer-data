{"id": "1609.05473", "review": {"conference": "AAAI", "VERSION": "v1", "DATE_OF_SUBMISSION": "18-Sep-2016", "title": "SeqGAN: Sequence Generative Adversarial Nets with Policy Gradient", "abstract": "As a new way of training generative models, Generative Adversarial Nets (GAN) that uses a discriminative model to guide the training of the generative model has enjoyed considerable success in generating real-valued data. However, it has limitations when the goal is for generating sequences of discrete tokens. A major reason lies in that the discrete outputs from the generative model make it difficult to pass the gradient update from the discriminative model to the generative model. Also, the discriminative model can only assess a complete sequence, while for a partially generated sequence, it is non-trivial to balance its current score and the future one once the entire sequence has been generated. In this paper, we propose a sequence generation framework, called SeqGAN, to solve the problems. Modeling the data generator as a stochastic policy in reinforcement learning (RL), SeqGAN bypasses the generator differentiation problem by directly performing gradient policy update. The RL reward signal comes from the GAN discriminator judged on a complete sequence, and is passed back to the intermediate state-action steps using Monte Carlo search. Extensive experiments on synthetic data and real-world tasks demonstrate significant improvements over strong baselines.", "histories": [["v1", "Sun, 18 Sep 2016 11:42:23 GMT  (1073kb,D)", "http://arxiv.org/abs/1609.05473v1", null], ["v2", "Tue, 20 Sep 2016 09:44:18 GMT  (1074kb,D)", "http://arxiv.org/abs/1609.05473v2", null], ["v3", "Sun, 25 Sep 2016 13:06:24 GMT  (1075kb,D)", "http://arxiv.org/abs/1609.05473v3", null], ["v4", "Mon, 24 Oct 2016 13:19:26 GMT  (1075kb,D)", "http://arxiv.org/abs/1609.05473v4", null], ["v5", "Fri, 9 Dec 2016 14:37:13 GMT  (888kb,D)", "http://arxiv.org/abs/1609.05473v5", "The Thirty-First AAAI Conference on Artificial Intelligence (AAAI 2017)"], ["v6", "Fri, 25 Aug 2017 16:22:57 GMT  (1079kb,D)", "http://arxiv.org/abs/1609.05473v6", "The Thirty-First AAAI Conference on Artificial Intelligence (AAAI 2017)"]], "reviews": [], "SUBJECTS": "cs.LG cs.AI", "authors": ["lantao yu", "weinan zhang", "jun wang", "yong yu"], "accepted": true, "id": "1609.05473"}, "pdf": {"name": "1609.05473.pdf", "metadata": {"source": "CRF", "title": "SeqGAN: Sequence Generative Adversarial Nets with Policy Gradient", "authors": ["Lantao Yu", "Weinan Zhang", "Jun Wang", "Yong Yu"], "emails": ["yulantao@apex.sjtu.edu.cn,", "wnzhang@apex.sjtu.edu.cn,", "yyu@apex.sjtu.edu.cn,", "j.wang@cs.ucl.ac.uk"], "sections": [{"heading": "Introduction", "text": "The generation of sequential synthetic data that mimic the real problem is an important problem in the unattended learning strategy. Recently, we have repeatedly seen neural networks (RNNs) with long-term short-term memory (LSTM) cells (Hochreiter and Schmidhuber 1997) that have shown excellent performance ranging from natural language generations to handwriting generation (Wen et al. 2015). However, the most common approach in the formation of an RNN is to maximize the log predictive probability of any real token in the training sequence. Previous observed tokens (Salakhutdinov 2009). However, as argued in Bengio et al 2015; Ranzato et al. 2015), the maximum approaches to exposure bias in the inference stage suffer: the model generates an iterratic sequence and predicts that the next tokens will be conditioned so that those who are never observed in the training data can never observe a discrepancy between training and inferences."}, {"heading": "Related Work", "text": "The basic generative models have recently attracted significant attention, and the ability to learn over large (unlabeled) sets of data gives them more potential and vitality (Salakhutdinov 2009; Bengio et al. 2013). (Hinton, Osindero, and Teh 2006) First proposed to use the contrasting divergence algorithm to efficiently train deep faith networks (DBN). (Bengio et al. 2013) proposed denociation of autoencoders (DAE) that learn data distribution in a supervised learning method. Both DBN and DAE learn a low dimensional representation (coding) for each data instance and generate it from a decoding network. (Recently, variational autoencoder) that combines deep learning with statistical inferences that are intended to represent a data instance in a latent hidden space; and Kingelling; and Wma; 2014 and Wierstra, they are still using it for carding."}, {"heading": "Sequence Generative Adversarial Nets", "text": "The sequence generation problem is described as follows: Given a set of structured sequences from the real world, we create a \u03b8-parameterized generative model to generate a sequence Y1: T = (y1,.., yT), yt-Y, where Y is the vocabulary of the candidate marks. We interpret this problem on the basis of reinforcement learning. In the timeframe t, the state s is the currently produced mark (y1,.., yt-1) and the action a is the next mark to be selected. Thus, the political model G\u03b8 (yt | Y1: t \u2212 1) is stochastical, while the state transition is dissuasive after an action has been selected, i.e., s-1 for the next state s-1 = = = 1 for the next state s' = Y1: t if the current state s = Y1: t \u2212 1 and the action a = sequential; i.e., for the next transition, the next state is deterrent for the next state = 1:"}, {"heading": "SeqGAN via Policy Gradient", "text": "Following (Sutton et al. 1999), if there is no interim reward, the goal of the generator model (politics) is, like Silver (politics) G\u03b8 (yt | Y1: t \u2212 1) to generate a sequence from the starting states \u2212 \u2212 \u2212 1), (1) where RT is the reward for a complete sequence. Note that the reward from the discriminator D\u03c6 (which we will discuss later) is the action value function of a sequence, i.e. the expected accumulative reward from a state, and then the following policy. The rationality of the objective function for a sequence is that we proceed from a given starting state, the goal of the generator is to generate a sequence that considers the discriminator as real.The next question is how we value this function."}, {"heading": "The Generative Model for Sequences", "text": "We use recurrent neural networks (RNNs) (Hochreiter and Schmidhuber 1997) as a generative model. An RNN maps input by recursively using representations x1,..., xT of the sequence x1,..., xT into a sequence of hidden states h1,..., hT by using the update function g recursively.ht = g (ht \u2212 1, xt) (9) In addition, a Softmax output layer z maps the hidden states into the output token distribution (yt | x1,.., xt) = z (ht) = softmax (c + V ht), (10), where the parameters are a bias vector c and a weight ma-trix V. to deal with the common problem of disappearing and exploding gradients (Goodfellow, Bengio, and Courville 2016) of backpropagation over time, we use the Long Short Schmidshort (STLq), the most (M) and High Memory cells (STLm) (1997)."}, {"heading": "The Discriminative Model for Sequences", "text": "Deep discriminative models such as Deep Neural Network (DNN) (Vesely) et al. 2013), Convolutional Neural Network (CNN) (Kim 2014) and Recurrent Convolutional Neural Network (RCNN) (Lai et al. 2015) have shown high performance in complicated sequence classification tasks. In this essay, we choose CNN as our discriminator as CNN has recently shown great effectiveness in text sequence classification (Zhang and LeCun 2015). Most discriminatory models can only perform one classification well for an entire sequence, rather than for the unfinished one. In this essay, we also focus on the situation in which the discriminator predicts the probability that a finished sequence will be real.11The generated sequence has a fixed length T. We can use CNN padding or the RNN discriminator (Bahdanau et al. 2016) for the variable length sequence generation.We initially represent an input sequence, x1."}, {"heading": "Synthetic Data Experiments", "text": "To test the effectiveness and broaden our understanding of SeqGAN, we perform a simulated test with synthetic data. To simulate the structured sequences of the real world, we consider a language model to capture the dependence of the tokens. We use a randomly initialized LSTM as the true model, also called oracle, to generate the real data distribution p (xt | x1,..., xt \u2212 1) for the following experiments."}, {"heading": "Evaluation Metric", "text": "The advantage of such an oracle is, firstly, that it provides the training data set and, secondly, that it evaluates the exact performance of the generative models, which will not be possible with real data. We know that MLE tries to minimize the transverse entropy between the true data distribution p and our approximation q, i.e. \u2212 Ex-p log q (x). However, the most accurate method for evaluating generative models is to take some samples from them and have human observers verify them based on their prior knowledge. We assume that the human observer has learned an accurate model of the natural distribution phuman (x). In order to then increase the chance of passing the turning test, we actually have to minimize the exactly opposite average negative log probability \u2212 Ex-q log Phuman (x) (Husza-r 2015), with the role of p and q being swapped. In our synthetic data experiments, we can consider the oracle as the human observer for real problems."}, {"heading": "Training Setting", "text": "To set up the synthetic data experiments, we first initialize the parameters of an LSTM network that follows the normal distribution N (0, 1), as an oracle that describes the real data distribution Goracle (xt | x1,.., xt \u2212 1). Then we use it to generate 10,000 sequences of length 20 as a training set S for the generative models. In the SeqGAN algorithm, the training set for the discriminator consists of the examples generated with the label 0 and the instances of S with the label 1. For different tasks, it is necessary to design a specific structure for the revolutionary layer and in our synthetic data experiments, the core size is from 1 to T and the number of each grain size is between 100 and 2002. Dropout (Srivastava et al. 2014) and L2 regularization are used to avoid an overmatch of the LSampling capability. Four generative models are compared with SeqGAN."}, {"heading": "Results", "text": "Since the evaluation metric is fundamentally instructive, we can understand the effects of SeqGAN b y or acleSeqGAN (a) g-steps = 100, d-steps = 1, k = 100 50 100 150 200 epoches8.708.89 9.009.5010.00N LL b y or acleSeqGAN (b) g-steps = 30, d-steps = 1, k = 300 50 100 150 200 epoches8.70 8,819.009.5010.00N LL y or acleSeqGAN (b) g-steps = 30, d-steps = 1, k = 300 50 150 200 epoches8.70 8,819.009.5010.00N LL y or acleSeqGAN (c) g-steps = 1, d-steps = 100 100 150 250 epoches.739.0050.00b = GAN-steps = 3, d-steps = GLQAN-leq."}, {"heading": "Discussion", "text": "In our experiments with synthetic data, we find that the stability of SeqGAN depends on the training strategy. Specifically, the g-steps, d-steps and k-parameters in algorithm 1 have a major impact on the convergence and performance of SeqGAN. Figure 3 shows the effect of these parameters. In Figure 3 (a), the g-steps are much larger than the d-steps and k-number of epochs, which means that we train the generator for many times until we update the discriminator. This strategy leads to rapid convergence, but as the generator improves rapidly, the discriminator cannot be fully trained and will therefore gradually provide a misleading signal. In Figure 3 (b), with more discriminator training periods, the unstable training process is mitigated. In Figure 3 (c), we train the generator for only one epoch and then, before the discriminator is deceived, we immediately update it based on more realistic examples."}, {"heading": "Real-world Scenarios", "text": "In addition to the previous experiments, we are also testing SeqGAN on various tasks from the real world, e.g. poetry composition, speech generation and music generation."}, {"heading": "Text Generation", "text": "For text generation scenarios, we use the proposed SeqGAN to generate Chinese poems and Barack Obama's political speeches. To focus on a fully automated solution and remain general, we have not used prior knowledge of specific structural rules in Chinese poems, such as specific phonological rules. In Obama's political speech, we use a corpus4, which is a collection of 11,092 paragraphs of Obama's political speeches. We use BLEU score as a benchmark to measure the degree of similarity between the generated texts and the human-generated texts. BLEU was originally intended to automatically assess the quality of machine translation (Papineni et al. 2002). The key point is to compare the similarity between the results of the machine and the references of the human person."}, {"heading": "Music Generation", "text": "For music composition, we use the Nottingham5 data set as training data, a collection of 695 folk tunes in midi file format. In our thesis, we use 88 numbers to represent 88 pitches corresponding to the 88 keys of the piano. By sampling the pitch for all 0.4s6, we transform the midi files into sequences of numbers from 1 to 88 with the length 32. BLEU is used as evaluation yardstick to model the suitability of the individual piano key patterns. To model the suitability of the continuous tonal value patterns, the mean square error (MSE) (Manaris et al. 2007) is used for evaluation. Table 4 shows that SeqGAN clearly exceeds the MLE in both metrics in the task of music production."}, {"heading": "Conclusion", "text": "In this paper, we proposed a sequence generation method, SeqGAN, to effectively train generative opposing networks to generate structured sequences using a policy gradient. To our knowledge, this is the first work to expand GANs to generate sequences of discrete tokens. In our synthetic data experiments, we used an oracle evaluation mechanism to explicitly demonstrate the superiority of SeqGAN over strong baselines. For three real-world scenarios, i.e. poetry, language, and music generation, SeqGAN demonstrated excellent performance in generating the creative sequences. We also conducted a series of experiments to investigate the robustness and stability of SeqGAN training. For future work, we plan to build a Monte Carlo tree search and value network (Silver et al. 2016) to improve decision-making for large-scale data and long-term planning measures."}, {"heading": "Proof for Eq. (6)", "text": "In this section, we provide the detailed derivative of Eq (6) in the database. (1) As in SEQUENCE GENERATIVE GENERATIVE GENERATIVE GENERATIVE GENERATIVE GENERATIVE BUSINESS (1: t \u2212 1 and the action a = yt; for other next states, the transition is deterministic, after an action has been selected, s, s, s, s, s, s, s, s, s, s, s, s, s, s, s, s, s, s, s, s, s, s, s, s, s, s, s, s, s, s, s, s, s, s, s, as, s, s, s, s, s, we, s, s, s, s, s, re, s, s, s, s, s, s, s, s, s, s, s, s, s, s, s, s, s, s."}, {"heading": "More Ablation Study", "text": "In the DISCUSSION section of the SYNTHETIC DATA EXPERIMENTS section of our paper, we will discuss the ablation study of three SeqGAN hyperparameters, i.e. g-steps, d-steps and k-epoch number. At this point, we will conduct further experiments to investigate the performance of SeqGAN when the supervised pre-training is insufficient. As described in our paper, we will begin the contrary training process after the convergence of MLE supervised pre-training. If we pre-train the generative model with conventional MLE methods for only 20 epochs, which is far from convergence, then the contrarian training process improves the generator quite slowly and unsteadily. The reason for this is that in SeqGAN the discriminatory model provides reward strategies when the generator acts almost randomly, the generator by providing the required reward procedure."}], "references": [{"title": "and Precup", "author": ["P. Bachman"], "venue": "D.", "citeRegEx": "Bachman and Precup 2015", "shortCiteRegEx": null, "year": 2015}, {"title": "An actor-critic algorithm for sequence prediction", "author": ["Bahdanau"], "venue": null, "citeRegEx": "Bahdanau,? \\Q2016\\E", "shortCiteRegEx": "Bahdanau", "year": 2016}, {"title": "Neural machine translation by jointly learning to align and translate", "author": ["Cho Bahdanau", "D. Bengio 2014] Bahdanau", "K. Cho", "Y. Bengio"], "venue": null, "citeRegEx": "Bahdanau et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Bahdanau et al\\.", "year": 2014}, {"title": "Generalized denoising auto-encoders as generative models", "author": ["Bengio"], "venue": null, "citeRegEx": "Bengio,? \\Q2013\\E", "shortCiteRegEx": "Bengio", "year": 2013}, {"title": "Scheduled sampling for sequence prediction with recurrent neural networks", "author": ["Bengio"], "venue": null, "citeRegEx": "Bengio,? \\Q2015\\E", "shortCiteRegEx": "Bengio", "year": 2015}, {"title": "S", "author": ["C.B. Browne", "E. Powley", "D. Whitehouse", "Lucas"], "venue": "M.; et al.", "citeRegEx": "Browne et al. 2012", "shortCiteRegEx": null, "year": 2012}, {"title": "Learning phrase representations using RNN encoder-decoder for statistical machine", "author": ["Cho"], "venue": null, "citeRegEx": "Cho,? \\Q2014\\E", "shortCiteRegEx": "Cho", "year": 2014}, {"title": "E", "author": ["Denton"], "venue": "L.; Chintala, S.; Fergus, R.; et al.", "citeRegEx": "Denton et al. 2015", "shortCiteRegEx": null, "year": 2015}, {"title": "P", "author": ["Glynn"], "venue": "W.", "citeRegEx": "Glynn 1990", "shortCiteRegEx": null, "year": 1990}, {"title": "Generative adversarial nets", "author": ["Goodfellow", "I others 2014] Goodfellow"], "venue": "In NIPS,", "citeRegEx": "Goodfellow and Goodfellow,? \\Q2014\\E", "shortCiteRegEx": "Goodfellow and Goodfellow", "year": 2014}, {"title": "A", "author": ["I. Goodfellow", "Y. Bengio", "Courville"], "venue": "2016. Deep learning.", "citeRegEx": "Goodfellow. Bengio. and Courville 2016", "shortCiteRegEx": null, "year": 2015}, {"title": "Generating chinese classical poems with statistical machine translation models", "author": ["Zhou He", "J. Jiang 2012] He", "M. Zhou", "L. Jiang"], "venue": null, "citeRegEx": "He et al\\.,? \\Q2012\\E", "shortCiteRegEx": "He et al\\.", "year": 2012}, {"title": "G", "author": ["Hinton"], "venue": "E.; Osindero, S.; and Teh, Y.-W.", "citeRegEx": "Hinton. Osindero. and Teh 2006", "shortCiteRegEx": null, "year": 2006}, {"title": "and Schmidhuber", "author": ["S. Hochreiter"], "venue": "J.", "citeRegEx": "Hochreiter and Schmidhuber 1997", "shortCiteRegEx": null, "year": 1997}, {"title": "and Ba", "author": ["D. Kingma"], "venue": "J.", "citeRegEx": "Kingma and Ba 2014", "shortCiteRegEx": null, "year": 2014}, {"title": "and Welling", "author": ["D.P. Kingma"], "venue": "M.", "citeRegEx": "Kingma and Welling 2014", "shortCiteRegEx": null, "year": 2014}, {"title": "Recurrent convolutional neural networks for text classification", "author": ["Lai"], "venue": null, "citeRegEx": "Lai,? \\Q2015\\E", "shortCiteRegEx": "Lai", "year": 2015}, {"title": "A corpus-based hybrid approach to music analysis and composition", "author": ["Manaris"], "venue": "In NCAI,", "citeRegEx": "Manaris,? \\Q2007\\E", "shortCiteRegEx": "Manaris", "year": 2007}, {"title": "Bleu: a method for automatic evaluation of machine translation", "author": ["Papineni"], "venue": null, "citeRegEx": "Papineni,? \\Q2002\\E", "shortCiteRegEx": "Papineni", "year": 2002}, {"title": "J", "author": ["Quinlan"], "venue": "R.", "citeRegEx": "Quinlan 1996", "shortCiteRegEx": null, "year": 1996}, {"title": "Sequence level training with recurrent neural networks. arXiv:1511.06732", "author": ["Ranzato"], "venue": null, "citeRegEx": "Ranzato,? \\Q2015\\E", "shortCiteRegEx": "Ranzato", "year": 2015}, {"title": "D", "author": ["Rezende"], "venue": "J.; Mohamed, S.; and Wierstra, D.", "citeRegEx": "Rezende. Mohamed. and Wierstra 2014", "shortCiteRegEx": null, "year": 2014}, {"title": "C", "author": ["D. Silver", "A. Huang", "Maddison"], "venue": "J.; Guez, A.; Sifre, L.; et al.", "citeRegEx": "Silver et al. 2016", "shortCiteRegEx": null, "year": 2016}, {"title": "G", "author": ["Srivastava, N.", "Hinton"], "venue": "E.; Krizhevsky, A.; Sutskever, I.; and Salakhutdinov, R.", "citeRegEx": "Srivastava et al. 2014", "shortCiteRegEx": null, "year": 2014}, {"title": "R", "author": ["Srivastava"], "venue": "K.; Greff, K.; and Schmidhuber, J.", "citeRegEx": "Srivastava. Greff. and Schmidhuber 2015", "shortCiteRegEx": null, "year": 2015}, {"title": "Q", "author": ["I. Sutskever", "O. Vinyals", "Le"], "venue": "V.", "citeRegEx": "Sutskever. Vinyals. and Le 2014", "shortCiteRegEx": null, "year": 2014}, {"title": "S", "author": ["R.S. Sutton", "D.A. McAllester", "Singh"], "venue": "P.; Mansour, Y.; et al.", "citeRegEx": "Sutton et al. 1999", "shortCiteRegEx": null, "year": 1999}, {"title": "Sequence-discriminative training of deep neural networks", "author": ["Vesel\u1ef3"], "venue": null, "citeRegEx": "Vesel\u1ef3,? \\Q2013\\E", "shortCiteRegEx": "Vesel\u1ef3", "year": 2013}, {"title": "Semantically conditioned LSTM-based natural language generation for spoken dialogue systems", "author": ["Wen"], "venue": null, "citeRegEx": "Wen,? \\Q2015\\E", "shortCiteRegEx": "Wen", "year": 2015}, {"title": "Generating chinese classical poems with RNN encoderdecoder", "author": ["Li Yi", "X. Sun 2016] Yi", "R. Li", "M. Sun"], "venue": null, "citeRegEx": "Yi et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Yi et al\\.", "year": 2016}, {"title": "and Lapata", "author": ["X. Zhang"], "venue": "M.", "citeRegEx": "Zhang and Lapata 2014", "shortCiteRegEx": null, "year": 2014}, {"title": "and LeCun", "author": ["X. Zhang"], "venue": "Y.", "citeRegEx": "Zhang and LeCun 2015", "shortCiteRegEx": null, "year": 2015}], "referenceMentions": [], "year": 2016, "abstractText": "As a new way of training generative models, Generative Adversarial Nets (GAN) that uses a discriminative model to guide the training of the generative model has enjoyed considerable success in generating real-valued data. However, it has limitations when the goal is for generating sequences of discrete tokens. A major reason lies in that the discrete outputs from the generative model make it difficult to pass the gradient update from the discriminative model to the generative model. Also, the discriminative model can only assess a complete sequence, while for a partially generated sequence, it is non-trivial to balance its current score and the future one once the entire sequence has been generated. In this paper, we propose a sequence generation framework, called SeqGAN, to solve the problems. Modeling the data generator as a stochastic policy in reinforcement learning (RL), SeqGAN bypasses the generator differentiation problem by directly performing gradient policy update. The RL reward signal comes from the GAN discriminator judged on a complete sequence, and is passed back to the intermediate state-action steps using Monte Carlo search. Extensive experiments on synthetic data and real-world tasks demonstrate significant improvements over strong baselines.", "creator": "LaTeX with hyperref package"}}}