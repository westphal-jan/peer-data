{"id": "1609.05521", "review": {"conference": "AAAI", "VERSION": "v1", "DATE_OF_SUBMISSION": "18-Sep-2016", "title": "Playing FPS Games with Deep Reinforcement Learning", "abstract": "Advances in deep reinforcement learning have allowed autonomous agents to perform well on Atari games, often outperforming humans, using only raw pixels to make their decisions. However, most of these games take place in 2D environments that are fully observable to the agent. In this paper, we present the first architecture to tackle 3D environments in first-person shooter games, that involve partially observable states. Typically, deep reinforcement learning methods only utilize visual input for training. We present a method to augment these models to exploit game feature information such as the presence of enemies or items, during the training phase. Our model is trained to simultaneously learn these features along with minimizing a Q-learning objective, which is shown to dramatically improve the training speed and performance of our agent. Our architecture is also modularized to allow different models to be independently trained for different phases of the game. We show that the proposed architecture substantially outperforms built-in AI agents of the game as well as humans in deathmatch scenarios.", "histories": [["v1", "Sun, 18 Sep 2016 17:52:28 GMT  (2266kb,D)", "http://arxiv.org/abs/1609.05521v1", "The authors contributed equally to this work"]], "COMMENTS": "The authors contributed equally to this work", "reviews": [], "SUBJECTS": "cs.AI cs.LG", "authors": ["guillaume lample", "devendra singh chaplot"], "accepted": true, "id": "1609.05521"}, "pdf": {"name": "1609.05521.pdf", "metadata": {"source": "CRF", "title": "Playing FPS Games with Deep Reinforcement Learning", "authors": ["Guillaume Lample", "Devendra Singh Chaplot"], "emails": ["glample@cs.cmu.edu", "chaplot@cs.cmu.edu"], "sections": [{"heading": "1 Introduction", "text": "In fact, most of them are able to play by the rules they had in the past, and they are able to play by the rules they had in the past."}, {"heading": "2 Background", "text": "In the following we give a short summary of the DQN and DRQN models."}, {"heading": "2.1 Deep Q-Networks", "text": "In each step, an agent observes the current state of the environment, decides on an action according to a policy \u03c0 and observes a reward signal rt. The objective of the agent is to find a policy that maximizes the expected sum of discounted rewards. (The Q function of a particular policy is defined as the expected return from performing an action a in a state s: Q\u03c0 (a) = E [Rt | st = s, at = a] It is common to use a function approximator to parameterize the action value function Q. Specifically, DQN uses a neural network, and the idea is to get an estimate of the Q function, the Q function close to Q-Q function we can expect as an optimal Q function."}, {"heading": "2.2 Deep Recurrent Q-Networks", "text": "The above model assumes that the agent receives a complete observation of the environment at each step - unlike games like Go, Atari games rarely actually provide a complete observation of the environment, as they still contain hidden variables, but the current screen buffer is usually sufficient to derive a very good sequence of actions. However, in partially observable environments, the agent only receives an observation of the environment, which is normally not sufficient to infer the complete state of the system. An FPS game like DOOM, where the field of vision of the agent is limited to 90 around its position, obviously falls into this category. To deal with such environments, Hausknecht and Stone (2015) have introduced the Deep Recurrent Q-Networks (DRQN), which does not estimate Q (st, at), but Q (ot, ht \u2212 1, at), whereby there is an additional input returned by the network at the previous step, which represents the hidden state of the agent. A neural network like a STM can be imputed to the normal network."}, {"heading": "3 Model", "text": "Our first approach to solving the problem was to use a basic DRQN model. Although this model performed well in relatively simple scenarios (where the only available measures were to turn or attack), it did not succeed in agony tasks; the resulting agents fired at will in the hope that an enemy would come under their line of fire. A penalty for using ammunition did not help: with a small penalty, the agents continued shooting, and with a large one, they simply never fired."}, {"heading": "3.1 Game feature augmentation", "text": "The ViZDoom environment provides access to internal variables generated by the game machine. We modify the game machine so that it returns information about the visible units with each frame. Although this internal information is not available, it can be used during training to integrate this information into the frame or not. (A company can be an enemy, a weapon, a weapon, a weapon, etc.)."}, {"heading": "3.2 Divide and conquer", "text": "The death match task is typically divided into two phases, one involving the exploration of the map to collect items and find enemies, and the other consisting of fighting enemies (McPartland and Gallagher 2008; Tastan and Sukthankar 2011). We call these phases the navigation and action phases. After two networks work together, each should be trained to act in a particular phase of the game, naturally leading to better overall performance. Current DQN models do not allow a combination of different networks that are optimized for different tasks. However, the current phase of the game can be determined by predicting whether an enemy is visible in the current frame (action phase) or not (navigation phase), which can be directly derived from the game characteristics in the proposed model architecture. There are various advantages of dividing the task into two phases and forming a different network for each phase. First, this makes the architecture modular and allows different models to train and test independently of each other and both networks can be trained in parallel."}, {"heading": "4 Training", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "4.1 Reward shaping", "text": "The score in the Deathmatch scenario is defined as the number of questions, i.e. number of kills minus number of suicides. If the reward is based only on the score, the repetition table is extremely sparse. State action pairs with rewards that are not zero make it difficult for the agent to learn advantageous actions. Moreover, rewards are extremely delayed and are usually not the result of a particular action: In order to obtain a positive reward, the actor must explore the map to find an enemy and aim him precisely and shoot him with a slow projectile missile. Delaying the reward makes it difficult for the actor to know which action is responsible for which reward. To address the problem of the sparse replay table and delayed rewards, we introduce the reward design, i.e. the modification of the reward function to include small intermediate rewards, in order to accelerate the learning process for the following kills (in addition to the positive for 2003)."}, {"heading": "4.2 Frame skip", "text": "As in most previous approaches, we used the frame skip technique (Bellemare et al. 2012), in which the agent receives only one screen input per k + 1 frame, with k being the number of frames skipped between each step, and the action decided by the network is then repeated over all skipped frames. A higher frame skip rate speeds up training, but can affect performance. Typically, targeting an enemy sometimes requires a rotation of a few degrees, which is impossible if the frame skip rate is too high even for human players, as the agent repeats the rotation action many times and ultimately rotates more than intended. A frame skip of k = 4 proved to be the best compromise."}, {"heading": "4.3 Sequential updates", "text": "To perform the DRQN updates, we use a different approach than the one presented by Hausknecht and Stone (2015). A sequence of n observations o1, o2,..., on is randomly sampled from the replay memory, but instead of updating all action states in the sequence, we consider only those that have sufficient history. In fact, the first states of the sequence are estimated on the basis of an almost non-existent history (since h0 is reinitialized at the beginning of the updates) and could therefore be inaccurate. As a result, their updating could lead to imprecise updates. To avoid this problem, errors from states o1... oh, where h is the minimum historical value for a state to be updated, are not propagated backwards by the network. Errors from states oh + 1.. on \u2212 1 are propagated backwards, and are only used to create a target for the action state on \u2212 1. An illustration of the update process is presented in Figure 3 = 4 and the updates are presented on the summaries where h = 4 and n = 4."}, {"heading": "5 Experiments", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "5.1 Hyperparameters", "text": "All networks were trained using the RMSProp algorithm and size 32 minibatches; the network weights were updated every 4 steps so that experience during training is scanned an average of 8 times (Van Hasselt, Guez and Silver 2015); the replay memory contained the one million most up-to-date frames; the discount factor was set to \u03b3 = 0.99; we used a greedy policy during training where the resolution was reduced linearly from 1 to 0.1 during the first million steps and then set to 0.1; different screen resolutions of the game can lead to a different field of view; in particular, a 4 / 3 resolution provides a 90 degree field of view, while a 16 / 9 resolution in Doom has a 108 degree field of view (as shown in Figure 1); to maximize agent game awareness, we used a 16 / 9 resolution of 440x225, which we increased to 108x60. Although our model had a lower performance through grayscale images, we opted for all of us to experiment."}, {"heading": "5.2 Scenario", "text": "We use the platform ViZDoom (Kempka et al. 2016) to perform all our experiments and evaluate our methods against the Deathmatch Scenario. In this scenario, the agent plays against built-in Doom bots, and the end result is the number of questions, i.e. the number of bots killed by the agent minus the number of suicides committed. We consider two variants of this scenario, adapted to the ViZDoom AI Competition: Limited Deathmatch on a known map. The agent is trained and evaluated on the same map, and the only weapon available is a rocket launcher. Agents can collect Health Packs and Ammo. Complete Deathmatch on unknown maps. The agent is trained and tested on different maps. The agent starts with a pistol, but can also collect various weapons around the map, as well as health packs and ammunition. We use 10 maps for training and 3 maps for testing. Furthermore, we randomize the textures of the maps during the training, while the modelling is improved to demonstrate the generalizability of the task."}, {"heading": "5.3 Evaluation Metrics", "text": "Since the K / D ratio is susceptible to \"camper\" behavior to minimize deaths, we also give the number of deaths to determine whether the pathogen is able to explore the map to find enemies. In addition, we also give the total number of objects collected, the total number of deaths, and the total number of suicides (to analyze the impact of different design decisions). Suicides are caused when the pathogen shoots too close to itself, with a weapon having an explosion radius like a rocket launcher. Since suicides are counted in deaths, they provide a good way to punish the K / D when the pathogen shoots arbitrarily."}, {"heading": "5.4 Results & Analysis", "text": "In this context, it should be noted that the measures in question are measures primarily aimed at combating climate change."}, {"heading": "6 Conclusion", "text": "In this paper, we have introduced a complete architecture for playing death match scenarios in FPS games, introduced a method for extending a DRQN model with high-level game information, and modularised our architecture to incorporate independent networks responsible for different phases of the game. These methods result in dramatic improvements over the standard DRQN model when applied to complicated tasks such as a death match. We have shown that the proposed model can outperform both built-in bots and human players, demonstrating the generalisability of our model to unknown maps. Furthermore, our methods complement the recent improvements in DQN and can easily be combined with duel architectures (Wang, de Freitas and Lanctot 2015) and prioritised playback (Schaul et al. 2015)."}, {"heading": "7 Acknowledgements", "text": "We would like to thank Sandeep Subramanian and Kanthashree Mysore Sathyendra for their valuable comments and suggestions, and the students of Carnegie Mellon University for their useful feedback and help in testing our system."}], "references": [{"title": "Multiple object recognition with visual attention", "author": ["Mnih Ba", "J. Kavukcuoglu 2014] Ba", "V. Mnih", "K. Kavukcuoglu"], "venue": "arXiv preprint arXiv:1412.7755", "citeRegEx": "Ba et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Ba et al\\.", "year": 2014}, {"title": "The arcade learning environment: An evaluation platform for general agents", "author": ["Bellemare"], "venue": "Journal of Artificial Intelligence Research", "citeRegEx": "Bellemare,? \\Q2012\\E", "shortCiteRegEx": "Bellemare", "year": 2012}, {"title": "Learning to communicate to solve riddles with deep distributed recurrent q-networks", "author": ["Foerster"], "venue": "arXiv preprint arXiv:1602.02672", "citeRegEx": "Foerster,? \\Q2016\\E", "shortCiteRegEx": "Foerster", "year": 2016}, {"title": "Q-learning in continuous state and action spaces", "author": ["Wettergreen Gaskett", "C. Zelinsky 1999] Gaskett", "D. Wettergreen", "A. Zelinsky"], "venue": "In Australasian Joint Conference on Artificial Intelligence,", "citeRegEx": "Gaskett et al\\.,? \\Q1999\\E", "shortCiteRegEx": "Gaskett et al\\.", "year": 1999}, {"title": "Deep recurrent q-learning for partially observable mdps", "author": ["Hausknecht", "M. Stone 2015] Hausknecht", "P. Stone"], "venue": "arXiv preprint arXiv:1507.06527", "citeRegEx": "Hausknecht et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Hausknecht et al\\.", "year": 2015}, {"title": "Learning continuous control policies by stochastic value gradients", "author": ["Heess"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Heess,? \\Q2015\\E", "shortCiteRegEx": "Heess", "year": 2015}, {"title": "Vizdoom: A doombased ai research platform for visual reinforcement learning", "author": ["Kempka"], "venue": "arXiv preprint arXiv:1605.02097", "citeRegEx": "Kempka,? \\Q2016\\E", "shortCiteRegEx": "Kempka", "year": 2016}, {"title": "End-to-end training of deep visuomotor policies", "author": ["Levine"], "venue": "Journal of Machine Learning Research", "citeRegEx": "Levine,? \\Q2016\\E", "shortCiteRegEx": "Levine", "year": 2016}, {"title": "Learning to be a bot: Reinforcement learning in shooter games. In AIIDE", "author": ["McPartland", "M. Gallagher 2008] McPartland", "M. Gallagher"], "venue": null, "citeRegEx": "McPartland et al\\.,? \\Q2008\\E", "shortCiteRegEx": "McPartland et al\\.", "year": 2008}, {"title": "Playing atari with deep reinforcement learning", "author": ["Mnih"], "venue": "arXiv preprint arXiv:1312.5602", "citeRegEx": "Mnih,? \\Q2013\\E", "shortCiteRegEx": "Mnih", "year": 2013}, {"title": "Shaping and policy search in reinforcement learning", "author": ["A.Y. Ng"], "venue": "Ph.D. Dissertation,", "citeRegEx": "Ng,? \\Q2003\\E", "shortCiteRegEx": "Ng", "year": 2003}, {"title": "Prioritized experience replay", "author": ["Schaul"], "venue": "arXiv preprint arXiv:1511.05952", "citeRegEx": "Schaul,? \\Q2015\\E", "shortCiteRegEx": "Schaul", "year": 2015}, {"title": "Mastering the game of go with deep neural networks and tree search. Nature 529(7587):484\u2013489", "author": ["Silver"], "venue": null, "citeRegEx": "Silver,? \\Q2016\\E", "shortCiteRegEx": "Silver", "year": 2016}, {"title": "Learning policies for first person shooter games using inverse reinforcement learning", "author": ["Tastan", "B. Sukthankar 2011] Tastan", "G.R. Sukthankar"], "venue": "In AIIDE. Citeseer", "citeRegEx": "Tastan et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Tastan et al\\.", "year": 2011}, {"title": "Deep reinforcement learning with double q-learning", "author": ["Guez Van Hasselt", "H. Silver 2015] Van Hasselt", "A. Guez", "D. Silver"], "venue": null, "citeRegEx": "Hasselt et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Hasselt et al\\.", "year": 2015}, {"title": "Dueling network architectures for deep reinforcement learning", "author": ["de Freitas Wang", "Z. Lanctot 2015] Wang", "N. de Freitas", "M. Lanctot"], "venue": "arXiv preprint arXiv:1511.06581", "citeRegEx": "Wang et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Wang et al\\.", "year": 2015}], "referenceMentions": [{"referenceID": 2, "context": "Foerster et al. (2016) consider", "startOffset": 0, "endOffset": 23}, {"referenceID": 6, "context": "We evaluate our model on the two different tasks adapted from the Visual Doom AI Competition (ViZDoom)2 using the API developed by Kempka et al. (2016) (Figure 1 shows a screenshot of Doom).", "startOffset": 131, "endOffset": 152}, {"referenceID": 10, "context": "the modification of reward function to include small intermediate rewards to speed up the learning process (Ng 2003).", "startOffset": 107, "endOffset": 116}], "year": 2016, "abstractText": "Advances in deep reinforcement learning have allowed autonomous agents to perform well on Atari games, often outperforming humans, using only raw pixels to make their decisions. However, most of these games take place in 2D environments that are fully observable to the agent. In this paper, we present the first architecture to tackle 3D environments in first-person shooter games, that involve partially observable states. Typically, deep reinforcement learning methods only utilize visual input for training. We present a method to augment these models to exploit game feature information such as the presence of enemies or items, during the training phase. Our model is trained to simultaneously learn these features along with minimizing a Q-learning objective, which is shown to dramatically improve the training speed and performance of our agent. Our architecture is also modularized to allow different models to be independently trained for different phases of the game. We show that the proposed architecture substantially outperforms built-in AI agents of the game as well as humans in deathmatch scenarios.", "creator": "LaTeX with hyperref package"}}}