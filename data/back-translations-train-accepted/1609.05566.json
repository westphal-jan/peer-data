{"id": "1609.05566", "review": {"conference": "AAAI", "VERSION": "v1", "DATE_OF_SUBMISSION": "18-Sep-2016", "title": "Label-Free Supervision of Neural Networks with Physics and Domain Knowledge", "abstract": "In many machine learning applications, labeled data is scarce and obtaining more labels is expensive. We introduce a new approach to supervising neural networks by specifying constraints that should hold over the output space, rather than direct examples of input-output pairs. These constraints are derived from prior domain knowledge, e.g., from known laws of physics. We demonstrate the effectiveness of this approach on real world and simulated computer vision tasks. We are able to train a convolutional neural network to detect and track objects without any labeled examples. Our approach can significantly reduce the need for labeled training data, but introduces new challenges for encoding prior knowledge into appropriate loss functions.", "histories": [["v1", "Sun, 18 Sep 2016 23:16:14 GMT  (824kb,D)", "http://arxiv.org/abs/1609.05566v1", null]], "reviews": [], "SUBJECTS": "cs.AI", "authors": ["russell stewart", "stefano ermon"], "accepted": true, "id": "1609.05566"}, "pdf": {"name": "1609.05566.pdf", "metadata": {"source": "CRF", "title": "Label-Free Supervision of Neural Networks with Physics and Domain Knowledge", "authors": ["Russell Stewart", "Stefano Ermon"], "emails": ["ermon}@cs.stanford.edu"], "sections": [{"heading": "Introduction", "text": "In this context, it should be noted that people are often able to learn without direct examples, opting instead for high-level instructions on how a task should be performed or what it will look like after completion. In this work, we ask ourselves whether a similar principle can be applied to teaching machines; can we monitor networks without individual examples by describing only the structure of the desired results? Contemporary methods of learning without labels often fall into the category of unsupervised learning."}, {"heading": "Problem Setup", "text": "It's not just the way we deal with the issue, but also the way we deal with the question of how we should behave. (It's not the way we behave, but the way we behave.) (It's the way we behave.) It's the way we behave. (It's the way we behave.) It's not the way we behave.) It's the way we behave. (It's the way we behave.) It's the way we behave. (It's the way we behave.) It's the way we behave. (It's the way we behave.) It's the way we behave. (It's the way) we behave. (It's the way) we behave. (It's the way we behave.) It's the way we behave. (It's) the way we behave. (It's) the way we behave. (It's) the way we behave. \""}, {"heading": "Experiments", "text": "The goal of our approach is to build a network in which we proceed from the inputs to the outputs that interest us, without needing direct examples of these outputs. In our first two experiments, we have the opportunity to create a map from an image to an object that contains it. It is possible to exploit a structure that is exploited by us over time. In our third experiment, we found an image of an object in free fall that describes whether it is two special objects that exploit the unique causal semantics that exist between these objects. We offer labels only for the purpose of evaluation. In our first experiment, we will record videos of an object that is thrown into each frame, and the goal is to learn the height of the object. Our goal is to obtain a regressive network from height to reach. \"R,\" where height and width is the number of vertical and horizontal pixels per frame."}, {"heading": "Tracking the position of a walking man", "text": "In our second experiment, we are now trying to extend the detection of free-falling objects to other types of motion. We will strive to detect the horizontal position of a person walking across a frame without providing direct labels. To this end, we are taking advantage of the structure that is held over time by assuming that the person is running at a constant speed over short periods of time. We are therefore formulating a structured prediction problem in which we have observed that the constant velocity assumption is approximately holding. Given the similarities to our first experiment with free-falling objects, we might hope to simply remove the notion of gravity from the equation (3) and retrain. In this case, however, this is not possible, as the constraint provides a necessary but not sufficient condition for convergence."}, {"heading": "Related Work", "text": "In this paper, we have presented a new strategy for integrating domain knowledge into three computer vision tasks. Networks in our experiments learn without labels by exploiting high-level instructions in the form of constraints. Constraint Learning is a generalization of supervised learning that allows for more creative methods of supervision. Thus, multiple learning, as described by (Dietterich, Lathrop and Lozano-Pe-rez 1997; Zhou and Xu 2007), offers various physical learning methods that allow for more efficient labeling by providing annotations on image groups and learning opportunities to predict characteristics that hold at least one input in a group, rather than providing individual learning tokens. Ranking Learning allows labels to be placed as orderings between inputs with the goal of finding an embedding of inputs that respect order relationships (Joachims 2002). Productive logical programming approaches can be based on continuous learning and background constraints."}, {"heading": "Conclusion", "text": "We have introduced a new method to use physical and other domain constraints to monitor neural networks. Future challenges include extending these results to larger datasets with multiple objects per image and simplifying the process of selecting sufficiency terms for new and interesting problems. By freeing the operator from collecting labels, our small experiments show promising prospects for the future of forming neural networks with weak monitoring."}, {"heading": "Acknowledgments", "text": "This work was supported by a grant from the SAIL-Toyota Center for AI Research. The authors thank Aditya Grover and Tudor Achim for the helpful conversations."}], "references": [{"title": "G", "author": ["M. Abadi", "A. Agarwal", "P. Barham", "E. Brevdo", "Z. Chen", "C. Citro", "Corrado"], "venue": "S.; Davis, A.; Dean, J.; Devin, M.; et al.", "citeRegEx": "Abadi et al. 2016", "shortCiteRegEx": null, "year": 2016}, {"title": "Constrained clustering: Advances in algorithms, theory, and applications", "author": ["Davidson Basu", "S. Wagstaff 2008] Basu", "I. Davidson", "K. Wagstaff"], "venue": null, "citeRegEx": "Basu et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Basu et al\\.", "year": 2008}, {"title": "Freebase: a collaboratively created graph database for structuring human knowledge", "author": ["Bollacker"], "venue": "In Proceedings of the 2008 ACM SIGMOD international conference on Management of data,", "citeRegEx": "Bollacker,? \\Q2008\\E", "shortCiteRegEx": "Bollacker", "year": 2008}, {"title": "and Kersting", "author": ["L. De Raedt"], "venue": "K.", "citeRegEx": "De Raedt and Kersting 2003", "shortCiteRegEx": null, "year": 2003}, {"title": "R", "author": ["Dietterich, T.G.", "Lathrop"], "venue": "H.; and Lozano-P\u00e9rez, T.", "citeRegEx": "Dietterich. Lathrop. and Lozano.P\u00e9rez 1997", "shortCiteRegEx": null, "year": 1997}, {"title": "C", "author": ["S. Ermon", "R. Le Bras", "S.K. Suram", "J.M. Gregoire", "Gomes"], "venue": "P.; Selman, B.; and van Dover, R. B.", "citeRegEx": "Ermon et al. 2015", "shortCiteRegEx": null, "year": 2015}, {"title": "and Ba", "author": ["D. Kingma"], "venue": "J.", "citeRegEx": "Kingma and Ba 2014", "shortCiteRegEx": null, "year": 2014}, {"title": "and Welling", "author": ["D.P. Kingma"], "venue": "M.", "citeRegEx": "Kingma and Welling 2013", "shortCiteRegEx": null, "year": 2013}, {"title": "From group to individual labels using deep features", "author": ["Kotzias"], "venue": "In ACM SIGKDD", "citeRegEx": "Kotzias,? \\Q2015\\E", "shortCiteRegEx": "Kotzias", "year": 2015}, {"title": "G", "author": ["A. Krizhevsky", "I. Sutskever", "Hinton"], "venue": "E.", "citeRegEx": "Krizhevsky. Sutskever. and Hinton 2012", "shortCiteRegEx": null, "year": 2012}, {"title": "Conditional random fields: Probabilistic models for segmenting and labeling sequence data", "author": ["McCallum Lafferty", "J. Pereira 2001] Lafferty", "A. McCallum", "F. Pereira"], "venue": "In Proceedings of the eighteenth international conference on machine learning, ICML,", "citeRegEx": "Lafferty et al\\.,? \\Q2001\\E", "shortCiteRegEx": "Lafferty et al\\.", "year": 2001}, {"title": "Q", "author": ["Le"], "venue": "V.", "citeRegEx": "Le 2013", "shortCiteRegEx": null, "year": 2013}, {"title": "H", "author": ["D.D. Lee", "Seung"], "venue": "S.", "citeRegEx": "Lee and Seung 2001", "shortCiteRegEx": null, "year": 2001}, {"title": "D", "author": ["Lenat"], "venue": "B.", "citeRegEx": "Lenat 1995", "shortCiteRegEx": null, "year": 1995}, {"title": "A", "author": ["V. Mnih", "K. Kavukcuoglu", "D. Silver", "A.A. Rusu", "J. Veness", "M.G. Bellemare", "A. Graves", "M. Riedmiller", "Fidjeland"], "venue": "K.; Ostrovski, G.; Petersen, S.; Beattie, C.; Sadik, A.; Antonoglou, I.; King, H.; Kumaran, D.; Wierstra, D.; Legg, S.; and Hassabis, D.", "citeRegEx": "Mnih et al. 2015", "shortCiteRegEx": null, "year": 2015}, {"title": "and De Raedt", "author": ["S. Muggleton"], "venue": "L.", "citeRegEx": "Muggleton and De Raedt 1994", "shortCiteRegEx": null, "year": 1994}, {"title": "and Domingos", "author": ["M. Richardson"], "venue": "P.", "citeRegEx": "Richardson and Domingos 2006", "shortCiteRegEx": null, "year": 2006}, {"title": "and Andres", "author": ["I. Shcherbatyi"], "venue": "B.", "citeRegEx": "Shcherbatyi and Andres 2016", "shortCiteRegEx": null, "year": 2016}, {"title": "G", "author": ["Srivastava, N.", "Hinton"], "venue": "E.; Krizhevsky, A.; Sutskever, I.; and Salakhutdinov, R.", "citeRegEx": "Srivastava et al. 2014", "shortCiteRegEx": null, "year": 2014}, {"title": "D", "author": ["Wolpert"], "venue": "H.", "citeRegEx": "Wolpert 2002", "shortCiteRegEx": null, "year": 2002}, {"title": "Clustering with complex constraints-algorithms and applications", "author": ["Zhi"], "venue": null, "citeRegEx": "Zhi,? \\Q2013\\E", "shortCiteRegEx": "Zhi", "year": 2013}, {"title": "and Xu", "author": ["Zhou", "Z.-H."], "venue": "J.-M.", "citeRegEx": "Zhou and Xu 2007", "shortCiteRegEx": null, "year": 2007}, {"title": "Fast training of triplet-based deep binary embedding networks. arXiv preprint arXiv:1603.02844", "author": ["Zhuang"], "venue": null, "citeRegEx": "Zhuang,? \\Q2016\\E", "shortCiteRegEx": "Zhuang", "year": 2016}], "referenceMentions": [], "year": 2016, "abstractText": "In many machine learning applications, labeled data is scarce and obtaining more labels is expensive. We introduce a new approach to supervising neural networks by specifying constraints that should hold over the output space, rather than direct examples of input-output pairs. These constraints are derived from prior domain knowledge, e.g., from known laws of physics. We demonstrate the effectiveness of this approach on real world and simulated computer vision tasks. We are able to train a convolutional neural network to detect and track objects without any labeled examples. Our approach can significantly reduce the need for labeled training data, but introduces new challenges for encoding prior knowledge into appropriate loss functions.", "creator": "LaTeX with hyperref package"}}}