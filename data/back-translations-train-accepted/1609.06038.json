{"id": "1609.06038", "review": {"conference": "ACL", "VERSION": "v1", "DATE_OF_SUBMISSION": "20-Sep-2016", "title": "Enhanced LSTM for Natural Language Inference", "abstract": "Reasoning and inference are central to human and artificial intelligence. Modeling inference in human language is notoriously challenging but is fundamental to natural language understanding and many applications. With the availability of large annotated data, neural network models have recently advanced the field significantly. In this paper, we present a new state-of-the-art result, achieving the accuracy of 88.3% on the standard benchmark, the Stanford Natural Language Inference dataset. This result is achieved first through our enhanced sequential encoding model, which outperforms the previous best model that employs more complicated network architectures, suggesting that the potential of sequential LSTM-based models have not been fully explored yet in previous work. We further show that by explicitly considering recursive architectures, we achieve additional improvement. Particularly, incorporating syntactic parse information contributes to our best result; it improves the performance even when the parse information is added to an already very strong system.", "histories": [["v1", "Tue, 20 Sep 2016 06:59:31 GMT  (287kb,D)", "http://arxiv.org/abs/1609.06038v1", "10 pages, 2 figures"], ["v2", "Tue, 7 Mar 2017 03:34:41 GMT  (507kb,D)", "http://arxiv.org/abs/1609.06038v2", "Update results, add case analysis"], ["v3", "Wed, 26 Apr 2017 17:37:13 GMT  (604kb,D)", "http://arxiv.org/abs/1609.06038v3", "ACL 2017"]], "COMMENTS": "10 pages, 2 figures", "reviews": [], "SUBJECTS": "cs.CL", "authors": ["qian chen", "xiaodan zhu", "zhen-hua ling", "si wei", "hui jiang 0001", "diana inkpen"], "accepted": true, "id": "1609.06038"}, "pdf": {"name": "1609.06038.pdf", "metadata": {"source": "CRF", "title": "Enhancing and Combining Sequential and Tree LSTM for Natural Language Inference", "authors": ["Qian Chen", "Xiaodan Zhu"], "emails": ["cq1231@mail.ustc.edu.cn", "xiaodan.zhu@nrc-cnrc.gc.ca", "zhling@ustc.edu.cn", "siwei@iflytek.com", "hj@cse.yorku.ca"], "sections": [{"heading": "1 Introduction", "text": "Recent years have shown that the problem is a necessary (though not sufficient) condition for a true natural understanding of language, as illustrated in the following example from [MacCartney, 2009], in which architectural inference (NLI) deals with determining whether a naturalistic hypothesis h can be derived from a natural premise of language, as illustrated in the following example from [MacCartney, 2009], in which the hypothesis is considered as a consequence of the premise."}, {"heading": "2 Related Work", "text": "In fact, the majority of them will be able to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to move, to move, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight."}, {"heading": "3 Hybrid Neural Inference Models", "text": "In this section, we present our hybrid networks, which consist of the following components: \"It is a very complex system in which we exploit the basic models based on sequencialisms and structures.\" (\"It is a system in which we exploit the basic concepts and the basic strategies of the individual storylines.\" (\"It is a system in which we provide a performance comparable to the best results [Munkhdalai and Yu, 2016b], but with a fairly simple but very effective architecture.In our notation, we have two sentences a = (a1, a'a) and b =. (b1, b'b =.)."}, {"heading": "4 Experiment Set-up", "text": "Data The Stanford Natural Language Inference (SNLI) corpus [Bowman et al., 2015] focuses on three fundamental relationships between a premise and a potential hypothesis: the premise contains the hypothesis (discord), they contradict each other (contradiction), or they are not related (neutral). The original SNLI corpus also contains \"the other\" category, which includes the pairs of sentences that do not have a consensus among several human annotators. As in previous work, we remove this category. We use the same division as in [Bowman et al., 2015] and as in other previous work. The parse trees used in this paper are produced by Stanford PCFG Parser 3.5.3 [Klein and Manning, 2003] and they are shipped as part of the SNLI corpus. We use classification accuracy as an evaluation metric, just as in previous work."}, {"heading": "5 Results", "text": "In fact, it is a way in which people are able to determine for themselves how they want to behave."}, {"heading": "6 Conclusions", "text": "We present several neural network models to better solve the problem of natural language inference (NLI), which achieve the best results to date on the SNLI benchmark. First, the result is achieved by our advanced sequential inference model, which has already surpassed the best model to date with more complex network architectures, suggesting that the potential of sequential LSTM-based models has not been fully explored in previous work. We also show that by explicitly incorporating recursive architectures, we achieve additional improvements, especially the inclusion of syntactic parse information contributes to our best result; it improves performance even when parse information is added to an already very strong system."}], "references": [{"title": "Neural machine translation by jointly learning to align and translate", "author": ["Dzmitry Bahdanau", "KyungHyun Cho", "Yoshua Bengio"], "venue": "CoRR, abs/1409.0473,", "citeRegEx": "Bahdanau et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Bahdanau et al\\.", "year": 2014}, {"title": "Direct Compositionality", "author": ["Chris Barker", "Pauline Jacobson"], "venue": null, "citeRegEx": "Barker and Jacobson.,? \\Q2007\\E", "shortCiteRegEx": "Barker and Jacobson.", "year": 2007}, {"title": "A large annotated corpus for learning natural language inference", "author": ["Samuel R. Bowman", "Gabor Angeli", "Christopher Potts", "Christopher D. Manning"], "venue": "In EMNLP,", "citeRegEx": "Bowman et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Bowman et al\\.", "year": 2015}, {"title": "A fast unified model for parsing and sentence understanding", "author": ["Samuel R. Bowman", "Jon Gauthier", "Abhinav Rastogi", "Raghav Gupta", "Christopher D. Manning", "Christopher Potts"], "venue": null, "citeRegEx": "Bowman et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Bowman et al\\.", "year": 2016}, {"title": "Listen, attend and spell", "author": ["William Chan", "Navdeep Jaitly", "Quoc Viet Le", "Oriol Vinyals"], "venue": "CoRR, abs/1508.01211,", "citeRegEx": "Chan et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Chan et al\\.", "year": 2015}, {"title": "Distraction-based neural networks for modeling document", "author": ["Qian Chen", "Xiao-Dan Zhu", "Zhen-Hua Ling", "Si Wei", "Hui Jiang"], "venue": "In IJCAI,", "citeRegEx": "Chen et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Chen et al\\.", "year": 2016}, {"title": "Long short-term memory-networks for machine reading", "author": ["Jianpeng Cheng", "Li Dong", "Mirella Lapata"], "venue": "arXiv preprint arXiv:1601.06733,", "citeRegEx": "Cheng et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Cheng et al\\.", "year": 2016}, {"title": "Attention-based models for speech recognition", "author": ["Jan Chorowski", "Dzmitry Bahdanau", "Dmitriy Serdyuk", "KyungHyun Cho", "Yoshua Bengio"], "venue": "In NIPS,", "citeRegEx": "Chorowski et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Chorowski et al\\.", "year": 2015}, {"title": "Long short-term memory", "author": ["S. Hochreiter", "J. Schmidhuber"], "venue": "Neural Computation,", "citeRegEx": "Hochreiter and Schmidhuber.,? \\Q1997\\E", "shortCiteRegEx": "Hochreiter and Schmidhuber.", "year": 1997}, {"title": "Adam: A method for stochastic optimization", "author": ["Diederik P. Kingma", "Lei Jimmy Ba"], "venue": "CoRR, abs/1412.6980,", "citeRegEx": "Kingma and Ba.,? \\Q2014\\E", "shortCiteRegEx": "Kingma and Ba.", "year": 2014}, {"title": "Accurate unlexicalized parsing", "author": ["Dan Klein", "Christopher D. Manning"], "venue": "In ACL,", "citeRegEx": "Klein and Manning.,? \\Q2003\\E", "shortCiteRegEx": "Klein and Manning.", "year": 2003}, {"title": "Learning natural language inference using bidirectional lstm model and inner-attention", "author": ["Yang Liu", "Chengjie Sun", "Lei Lin", "Xiaolong Wang"], "venue": "arXiv preprint arXiv:1605.09090,", "citeRegEx": "Liu et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Liu et al\\.", "year": 2016}, {"title": "Natural Language Inference", "author": ["Bill MacCartney"], "venue": "PhD thesis, Stanford University,", "citeRegEx": "MacCartney.,? \\Q2009\\E", "shortCiteRegEx": "MacCartney.", "year": 2009}, {"title": "Modeling semantic containment and exclusion in natural language inference", "author": ["Bill MacCartney", "Christopher D. Manning"], "venue": "In Proceedings of the 22Nd International Conference on Computational Linguistics - Volume 1,", "citeRegEx": "MacCartney and Manning.,? \\Q2008\\E", "shortCiteRegEx": "MacCartney and Manning.", "year": 2008}, {"title": "Natural language inference by tree-based convolution and heuristic matching", "author": ["Lili Mou", "Rui Men", "Ge Li", "Yan Xu", "Lu Zhang", "Rui Yan", "Zhi Jin"], "venue": "In The 54th Annual Meeting of the Association for Computational Linguistics,", "citeRegEx": "Mou et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Mou et al\\.", "year": 2016}, {"title": "Neural semantic encoders", "author": ["Tsendsuren Munkhdalai", "Hong Yu"], "venue": "arXiv preprint arXiv:1607.04315,", "citeRegEx": "Munkhdalai and Yu.,? \\Q2016\\E", "shortCiteRegEx": "Munkhdalai and Yu.", "year": 2016}, {"title": "Neural tree indexers for text understanding", "author": ["Tsendsuren Munkhdalai", "Hong Yu"], "venue": "arXiv preprint arXiv:1607.04492,", "citeRegEx": "Munkhdalai and Yu.,? \\Q2016\\E", "shortCiteRegEx": "Munkhdalai and Yu.", "year": 2016}, {"title": "A decomposable attention model for natural language inference", "author": ["Ankur P Parikh", "Oscar T\u00e4ckstr\u00f6m", "Dipanjan Das", "Jakob Uszkoreit"], "venue": null, "citeRegEx": "Parikh et al\\.,? \\Q1933\\E", "shortCiteRegEx": "Parikh et al\\.", "year": 1933}, {"title": "Glove: Global vectors for word representation", "author": ["Jeffrey Pennington", "Richard Socher", "Christopher D. Manning"], "venue": "In Empirical Methods in Natural Language Processing (EMNLP),", "citeRegEx": "Pennington et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Pennington et al\\.", "year": 2014}, {"title": "Reasoning about entailment with neural attention", "author": ["Tim Rockt\u00e4schel", "Edward Grefenstette", "Karl Moritz Hermann", "Tom\u00e1s Kocisk\u00fd", "Phil Blunsom"], "venue": "CoRR, abs/1509.06664,", "citeRegEx": "Rockt\u00e4schel et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Rockt\u00e4schel et al\\.", "year": 2015}, {"title": "A neural attention model for abstractive sentence summarization", "author": ["Alexander M. Rush", "Sumit Chopra", "Jason Weston"], "venue": "In EMNLP,", "citeRegEx": "Rush et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Rush et al\\.", "year": 2015}, {"title": "Order-embeddings of images and language", "author": ["Ivan Vendrov", "Ryan Kiros", "Sanja Fidler", "Raquel Urtasun"], "venue": "arXiv preprint arXiv:1511.06361,", "citeRegEx": "Vendrov et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Vendrov et al\\.", "year": 2015}, {"title": "Learning natural language inference with lstm", "author": ["Shuohang Wang", "Jing Jiang"], "venue": "CoRR, abs/1512.08849,", "citeRegEx": "Wang and Jiang.,? \\Q2015\\E", "shortCiteRegEx": "Wang and Jiang.", "year": 2015}, {"title": "Show, attend and tell: Neural image caption generation with visual attention", "author": ["Kelvin Xu", "Lei Jimmy Ba", "Ryan Kiros", "KyungHyun Cho", "Aaron C. Courville", "Ruslan Salakhutdinov", "Richard S. Zemel", "Yoshua Bengio"], "venue": "In ICML,", "citeRegEx": "Xu et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Xu et al\\.", "year": 2015}, {"title": "Long Short-Term Memory over Recursive Structures", "author": ["Xiaodan Zhu", "Parinaz Sobhani", "Hongyu Guo"], "venue": "In Proceedings of International Conference on Machine Learning,", "citeRegEx": "Zhu et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Zhu et al\\.", "year": 2015}], "referenceMentions": [{"referenceID": 13, "context": "As pointed out in [MacCartney and Manning, 2008], \u201ca necessary (if not sufficient) condition for true natural language understanding is a mastery of open-domain natural language inference.", "startOffset": 18, "endOffset": 48}, {"referenceID": 12, "context": "\u201d Specifically, natural language inference (NLI) is concerned with determining whether a naturallanguage hypothesis h can be inferred from a natural-language premise p, as depicted in the following example from [MacCartney, 2009], in which the hypothesis is regarded to be entailed from the premise.", "startOffset": 211, "endOffset": 229}, {"referenceID": 2, "context": "An important contribution is the availability of the large annotated dataset, the Stanford Natural Language Inference (SNLI) dataset, made available by [Bowman et al., 2015].", "startOffset": 152, "endOffset": 173}, {"referenceID": 12, "context": "Early work on natural language inference has been performed on rather small datasets with more conventional methods (refer to [MacCartney, 2009] for a good literature survey).", "startOffset": 126, "endOffset": 144}, {"referenceID": 2, "context": "More recently, [Bowman et al., 2015] made available the SNLI dataset with about 570K human annotated sentence pair.", "startOffset": 15, "endOffset": 36}, {"referenceID": 19, "context": "[Rockt\u00e4schel et al., 2015] proposed neural attention-based model on NLI, which captured the attention information.", "startOffset": 0, "endOffset": 26}, {"referenceID": 0, "context": "In general, attention based models have shown to be effective in a wide range of tasks, including machine translation [Bahdanau et al., 2014], speech recognition [Chorowski et al.", "startOffset": 118, "endOffset": 141}, {"referenceID": 23, "context": ", 2015], image caption [Xu et al., 2015], and text summarization [Rush et al.", "startOffset": 23, "endOffset": 40}, {"referenceID": 24, "context": ", the tree-LSTM [Zhu et al., 2015].", "startOffset": 16, "endOffset": 34}, {"referenceID": 1, "context": "As pointed out in [Barker and Jacobson, 2007] \u201cthe syntax and the semantics work together in tandem\", and natural language inference is very likely to involve both.", "startOffset": 18, "endOffset": 45}, {"referenceID": 8, "context": "ht = ot tanh(ct), (7) where at each word position t, LSTM employs a set of internal vectors: an input gate it, a forget gate ft, an output gate ot, and a memory cell ct, to generate a hidden state ht (refer to [Hochreiter and Schmidhuber, 1997] for details).", "startOffset": 210, "endOffset": 244}, {"referenceID": 14, "context": "First, we leverage heuristic matching [Mou et al., 2016] between original vectors and mimic vectors.", "startOffset": 38, "endOffset": 56}, {"referenceID": 14, "context": "The work of [Mou et al., 2016] used it to compare the premise and hypothesis sentence embeddings, while we extend it to subcomponents of sentences.", "startOffset": 12, "endOffset": 30}, {"referenceID": 1, "context": "As pointed out in [Barker and Jacobson, 2007] \u201cthe syntax and the semantics work together in tandem\", and natural language inference is very likely to involve both of them.", "startOffset": 18, "endOffset": 45}, {"referenceID": 24, "context": "In general, tree-LSTM has recently been proposed to explicitly model tree structures [Zhu et al., 2015].", "startOffset": 85, "endOffset": 103}, {"referenceID": 24, "context": "And as noted in [Zhu et al., 2015], one can always choose to binarize a non-binary tree, and the syntactic information will largely be kept.", "startOffset": 16, "endOffset": 34}, {"referenceID": 2, "context": "Data The Stanford Natural Language Inference (SNLI) corpus [Bowman et al., 2015] focuses on three basic relationship between a premise and a potential hypothesis: the premise entails the hypothesis (entailment), they contradict each other (contradiction), or they are not related (neutral).", "startOffset": 59, "endOffset": 80}, {"referenceID": 2, "context": "We used the same split as in [Bowman et al., 2015] and as in other previous work.", "startOffset": 29, "endOffset": 50}, {"referenceID": 10, "context": "3 [Klein and Manning, 2003] and they are delivered as a part of the SNLI corpus.", "startOffset": 2, "endOffset": 27}, {"referenceID": 9, "context": "We use the Adam method [Kingma and Ba, 2014] for optimization.", "startOffset": 23, "endOffset": 44}, {"referenceID": 18, "context": "We use pre-trained 300-D Glove 840B vectors [Pennington et al., 2014] to initialize our word embeddings.", "startOffset": 44, "endOffset": 69}, {"referenceID": 2, "context": "The first row is a baseline classifier presented in [Bowman et al., 2015] that considers handcrafted features such as BLEU score of the hypothesis with respect to the premise, the overlapped words, and the length difference between them, etc.", "startOffset": 52, "endOffset": 73}, {"referenceID": 3, "context": "The model of [Bowman et al., 2016] encodes the premise and hypothesis with two different LSTMs.", "startOffset": 13, "endOffset": 34}, {"referenceID": 21, "context": "The model in [Vendrov et al., 2015] uses unsupervised \u2019skip-thoughts\u2019 pre-training in GRU encoders.", "startOffset": 13, "endOffset": 35}, {"referenceID": 14, "context": "The approach proposed in [Mou et al., 2016] consider also tree-based CNN to capture sentence-level semantics, while the model of [Bowman et al.", "startOffset": 25, "endOffset": 43}, {"referenceID": 3, "context": ", 2016] consider also tree-based CNN to capture sentence-level semantics, while the model of [Bowman et al., 2016] introduces a Stack-augmented Parser-Interpreter Neural Network (SPINN), which combines parsing and interpretation within a single tree-sequence hybrid model.", "startOffset": 93, "endOffset": 114}, {"referenceID": 11, "context": "The work of [Liu et al., 2016] use BiLSTM to generate sentence representation, and then replace average pooling with intra-attention.", "startOffset": 12, "endOffset": 30}, {"referenceID": 2, "context": "(1) Handcrafted features [Bowman et al., 2015] - 99.", "startOffset": 25, "endOffset": 46}, {"referenceID": 3, "context": "(2) 300D LSTM encoders [Bowman et al., 2016] 3.", "startOffset": 23, "endOffset": 44}, {"referenceID": 21, "context": "6 (3) 1024D pretrained GRU encoders [Vendrov et al., 2015] 15M 98.", "startOffset": 36, "endOffset": 58}, {"referenceID": 14, "context": "4 (4) 300D tree-based CNN encoders [Mou et al., 2016] 3.", "startOffset": 35, "endOffset": 53}, {"referenceID": 3, "context": "1 (5) 300D SPINN-PI encoders [Bowman et al., 2016] 3.", "startOffset": 29, "endOffset": 50}, {"referenceID": 11, "context": "2 (6) 600D BiLSTM intra-attention encoders [Liu et al., 2016] 2.", "startOffset": 43, "endOffset": 61}, {"referenceID": 19, "context": "(8) 100D LSTM with attention [Rockt\u00e4schel et al., 2015] 250k 85.", "startOffset": 29, "endOffset": 55}, {"referenceID": 22, "context": "5 (9) 300D mLSTM [Wang and Jiang, 2015] 1.", "startOffset": 17, "endOffset": 39}, {"referenceID": 6, "context": "1 (10) 450D LSTMN with deep attention fusion [Cheng et al., 2016] 3.", "startOffset": 45, "endOffset": 65}, {"referenceID": 19, "context": "The model marked with [Rockt\u00e4schel et al., 2015] is LSTMs enforcing so called word-by-word attention.", "startOffset": 22, "endOffset": 48}, {"referenceID": 22, "context": "The model in [Wang and Jiang, 2015] extends this idea to explicitly enforce word-by-word matching between the hypothesis and the premise.", "startOffset": 13, "endOffset": 35}, {"referenceID": 6, "context": "Long short-term memory-networks (LSTMN) with deep attention fusion [Cheng et al., 2016] link the current word to previous words stored in memory.", "startOffset": 67, "endOffset": 87}, {"referenceID": 22, "context": "The model of [Munkhdalai and Yu, 2016b] extends the framework in [Wang and Jiang, 2015] to a full n-ary tree model and achieved further improvement.", "startOffset": 65, "endOffset": 87}, {"referenceID": 24, "context": "We ensemble our EBIM model with syntactic tree-LSTM [Zhu et al., 2015] based on binary parse trees, and achieve significant improvement over our best sequential encoding model EBIM, obtaining an accuracy of 88.", "startOffset": 52, "endOffset": 70}, {"referenceID": 24, "context": "Each tree node is implemented with a tree-LSTM block [Zhu et al., 2015].", "startOffset": 53, "endOffset": 71}], "year": 2016, "abstractText": "Reasoning and inference are central to human and artificial intelligence. Modeling inference in human language is notoriously challenging but is fundamental to natural language understanding and many applications. With the availability of large annotated data, neural network models have recently advanced the field significantly. In this paper, we present a new state-of-the-art result, achieving the accuracy of 88.3% on the standard benchmark, the Stanford Natural Language Inference dataset. This result is achieved first through our enhanced sequential encoding model, which outperforms the previous best model that employs more complicated network architectures, suggesting that the potential of sequential LSTM-based models have not been fully explored yet in previous work. We further show that by explicitly considering recursive architectures, we achieve additional improvement. Particularly, incorporating syntactic parse information contributes to our best result; it improves the performance even when the parse information is added to an already very strong system.", "creator": "LaTeX with hyperref package"}}}