{"id": "1609.06082", "review": {"conference": "EMNLP", "VERSION": "v1", "DATE_OF_SUBMISSION": "20-Sep-2016", "title": "Learning Robust Representations of Text", "abstract": "Deep neural networks have achieved remarkable results across many language processing tasks, however these methods are highly sensitive to noise and adversarial attacks. We present a regularization based method for limiting network sensitivity to its inputs, inspired by ideas from computer vision, thus learning models that are more robust. Empirical evaluation over a range of sentiment datasets with a convolutional neural network shows that, compared to a baseline model and the dropout method, our method achieves superior performance over noisy inputs and out-of-domain data.", "histories": [["v1", "Tue, 20 Sep 2016 10:23:47 GMT  (31kb)", "http://arxiv.org/abs/1609.06082v1", "5 pages with 2 pages reference, 2 tables, 1 figure"]], "COMMENTS": "5 pages with 2 pages reference, 2 tables, 1 figure", "reviews": [], "SUBJECTS": "cs.CL", "authors": ["yitong li", "trevor cohn", "timothy baldwin"], "accepted": true, "id": "1609.06082"}, "pdf": {"name": "1609.06082.pdf", "metadata": {"source": "CRF", "title": "Learning Robust Representations of Text", "authors": ["Yitong Li", "Trevor Cohn", "Timothy Baldwin"], "emails": ["yitongl4@student.unimelb.edu.au,", "tcohn@unimelb.edu.au", "tbaldwin@unimelb.edu.au"], "sections": [{"heading": null, "text": "ar Xiv: 160 9.06 082v 1 [cs.C L] 20 SE"}, {"heading": "1 Introduction", "text": "Recent years have shown that the networking of computer systems is capable of meeting people's needs (Krizhevsky et al., 2012), speech recognition (Graves et al., 2013) and natural language processing tasks (Bahdanau et al., 2015; Kalchbrenner et al., 2015; Yih et al., 2014; Bitvai and Cohn, 2015). However, deep models are often too self-conscious for loud test cases, making them vulnerable to hostile attacks (Nguyen et al., 2015; Tabacof and Valle, 2016). Goodfellow et al. (2014) argues that the primary cause of neural networks \"susceptibility to hostile disturbances is their linear nature, due to neural models that are deliberately designed to behave in a largely linear manner to facilitate optimization."}, {"heading": "2 Training for Robustness", "text": "Our method introduces a regularization term during the training to ensure the robustness of the model. We develop our approach on the basis of a general class of parametric models with the following structure. Let x be the input, which is a sequence of (single) words represented by a vector of fixed size continuous values, i.e. a transfer function takes h as input and generates an output distribution, ypred. The training proceeds by means of stochastic descent to minimize a loss function L, which ytruds the difference between ypred and the truth. The purpose of our work is to learn neural models that are more robust to strange or invalid inputs. If small disturbances are applied to x, we want the prediction to remain stable. Text can be very variable, so that the same information can be transmitted with other word selections, different syntactical structures, typographic errors, stylistic changes, etc. This is a particular problem to learn from scenario models, where the next variation models are stable."}, {"heading": "2.1 Conventional Regularization and Dropout", "text": "Traditional methods for learning robust models include the l1 and l2 regulation (Ng, 2004) and the drop-out method (Srivastava et al., 2014). In fact, Wager et al. (2013) have shown that the drop-out regulator is first-class like an L2 regulator applied after scaling the features. Dropout is also synonymous with \"Follow the Perturbed Leader\" (FPL), which disturbs exponential numbers of experts with noise and then, with the expert, predicts minimal disruption losses to the robustness of online learning (van Erven et al., 2014). Given its popularity in deep learning, we take the drop-out as a strong baseline in our assessment. The key idea behind the drop-out is to randomly set units along with their connections to zero during training, thus limiting the extent of coadaptation between units."}, {"heading": "2.2 Robust Regularization", "text": "Our method is inspired by work on the hostile training in computer vision (Goodfellow et al., 2014). An intuitive explanation of our regulation method is that when noise is applied to the data, the variation of the output is less than the noise. We adapt this idea from Rifai et al. (2011) and develop the Jacobic regulation method. The proposed regulation method works as follows: The conventional training attempts to minimize the difference between Ytrue and Ypred. However, in order to make our model robust against noise, we want to minimize the variation of the output when noise is applied to the input. That is, if perturbations are applied to the input x x, there should be as little disruption in the output as possible."}, {"heading": "2.3 Convolutional Network", "text": "For the purposes of this work, we focus exclusively on Convolutionary Neural Networks (CNNs), but emphasize that the method is compatible with other neural architectures and other types of parametric models (not just deep neural networks).The CNN used in this research is based on the model proposed by Kim (2014) and is outlined below. Let's apply the sentence consisting of n words {w1, w2, \u00b7 \u00b7, wn}.A look-up table is applied to S, consisting of word vectors ei Rm corresponding to each word wi, where m is the word vector dimensionality. Thus, sentence S can be used as a matrix ES Rm \u00b7 n by concatenating the word vectors ES = 1 ewi. A revolutionary layer is combined with a number of further revolutionary properties wi, where m is the word vector dimensionality."}, {"heading": "3 Datasets and Experimental Setups", "text": "We experiment with the following datasets, 2 according to Kim (2014): \u2022 MR: Sentence Polarity dataset (Pang and Lee, 2008) 3 \u2022 Subj: Subjectivity dataset (Pang and Lee, 2005) 3 \u2022 CR: Customer Review dataset (Hu and Liu, 2004) 4 \u2022 SST: Stanford Sentiment Treebank, using the 3-class configuration (Socher et al., 2013) 5 In any case, we evaluate on the basis of classification accuracy."}, {"heading": "3.1 Noisifying the Data", "text": "Unlike traditional ratings, we corrupt the test data with noise to evaluate the robustness of our model. We assume that in short texts such as Twitter posts, it is common to see unknown words due to typos, abbreviations, and sociolinguistic ratings of different types (Han and Baldwin, 2011; Eisenstein, 2013). To simulate this, we apply word-level dropout noise to each document by randomly replacing words with a unique sentinel symbol.6 This is applied to each word with a probability \u03b1 {0, 0.1, 0.2, 0.3}. We also experimented with adding different Gaussian noise to the sentence embeddings ES, but found that the results are largely consistent with those for word dropout noise, so we have excluded these results from the paper."}, {"heading": "3.2 Word Vectors and Hyper-parameters", "text": "To set the hyperparameters of CNN, we follow the guidelines of Zhang and Wallace (2015), set word embedding to m = 300 dimensions and initialize on the basis of word2vec pre-training (Mikolov et al., 2013). Words not included in the trained vector table were randomly initialized by the uniform distribution U ([\u2212 0.25, 0.25) m. Window sizes of the filters (t) are set to 3, 4, 5, with 128 filters for each size, resulting in a hidden layer dimension of 384 = 128 x 3. We use the Adam Optimizer (Kingma and Ba, 2015) for training."}, {"heading": "4 Results and Discussions", "text": "Generally speaking, the increase in word level drop-out noise results in a decrease in accuracy for all four datasets, but the relative decrease in accuracy is lower for Robust Regularization than for Word Dropout, and in 15 out of 16 cases (four noise levels over the four datasets), our method achieves the best result. Note that this also includes the case of \u03b1 = 0, where the test data is left in its original form, which shows that Robust Regularization is also an effective means of preventing overadjustment of the model. For each dataset, we also evaluated based on the combination of Word Dropout and Robust Regularization with the fixed parameters \u03b2 = 0.5 and \u03bb = 10 \u2212 2, which are overall the best individual settings. The combined approach works better than any single method for the highest noise level tested over all datasets. This indicates that Rropbust Regularization in the fixed parameters = 10 and the individual parameters = 0.5 \u2212 are the best ones overall."}, {"heading": "4.1 Running Time", "text": "Our method requires second order derivatives and is therefore slightly slower during training. Figure 1 is a diagram of training and test accuracy at different points during training via SST. We can see that the time to convergence is only slightly slower with Robust Regularization than with standard training, at about 30 minutes on a two-core CPU (one fold) with standard training versus 35-40 minutes with Robust Regularization. The convergence time for Robust Regularization is similar to that for Word Dropout."}, {"heading": "5 Conclusions", "text": "In this paper, we present a robust control method that explicitly minimizes the sensitivity of a neural model to small changes in its hidden representation. Based on the evaluation of four mood analysis data sets using Convolutionary Neural Networks, we have found that our method is both superior and complementary to traditional word-level dropouts at different noise levels and in a cross-sectoral assessment. In future work, we plan to apply our control method to other models and tasks to determine how broadly applicable our method is. We will also explore methods for more realistic linguistic noise, such as lexical, syntactical, and semantic noise, to develop models that are robust to the type of data that occur frequently during the test period."}, {"heading": "Acknowledgments", "text": "We would like to thank the anonymous reviewers for their helpful feedback and suggestions. This work was supported by the Australian Research Council (grant number FT130101105) and we would also like to thank the developers of Tensorflow (Abadi et al., 2015) who were used for the experiments in this work."}], "references": [{"title": "TensorFlow: Large-scale machine learning on heterogeneous systems", "author": ["Wattenberg", "Martin Wicke", "Yuan Yu", "Xiaoqiang Zheng."], "venue": "Technical report, Google Research.", "citeRegEx": "Wattenberg et al\\.,? 2015", "shortCiteRegEx": "Wattenberg et al\\.", "year": 2015}, {"title": "Neural machine translation by jointly learning to align and translate", "author": ["Dzmitry Bahdanau", "Kyunghyun Cho", "Yoshua Bengio."], "venue": "Proceedings of the International Conference on Learning Representations.", "citeRegEx": "Bahdanau et al\\.,? 2015", "shortCiteRegEx": "Bahdanau et al\\.", "year": 2015}, {"title": "Non-linear text regression with a deep convolutional neural network", "author": ["Zsolt Bitvai", "Trevor Cohn."], "venue": "Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Pro-", "citeRegEx": "Bitvai and Cohn.,? 2015", "shortCiteRegEx": "Bitvai and Cohn.", "year": 2015}, {"title": "What to do about bad language on the internet", "author": ["Jacob Eisenstein."], "venue": "Proceedings of the 2013 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 359\u2013369.", "citeRegEx": "Eisenstein.,? 2013", "shortCiteRegEx": "Eisenstein.", "year": 2013}, {"title": "Analysis of classifiers\u2019 robustness to adversarial perturbations", "author": ["Alhussein Fawzi", "Omar Fawzi", "Pascal Frossard."], "venue": "arXiv preprint arXiv:1502.02590.", "citeRegEx": "Fawzi et al\\.,? 2015", "shortCiteRegEx": "Fawzi et al\\.", "year": 2015}, {"title": "Explaining and harnessing adversarial examples", "author": ["Ian J. Goodfellow", "Jonathon Shlens", "Christian Szegedy."], "venue": "Proceedings of the International Conference on Learning Representations.", "citeRegEx": "Goodfellow et al\\.,? 2014", "shortCiteRegEx": "Goodfellow et al\\.", "year": 2014}, {"title": "Speech recognition with deep recurrent neural networks", "author": ["Alan Graves", "Abdel-rahman Mohamed", "Geoffrey Hinton."], "venue": "Proceedings of the IEEE International Conference on Acoustics, Speech and Signal Processing, pages 6645\u20136649.", "citeRegEx": "Graves et al\\.,? 2013", "shortCiteRegEx": "Graves et al\\.", "year": 2013}, {"title": "Lexical normalisation of short text messages: Makn sens a #twitter", "author": ["Bo Han", "Timothy Baldwin."], "venue": "Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics, pages 368\u2013378.", "citeRegEx": "Han and Baldwin.,? 2011", "shortCiteRegEx": "Han and Baldwin.", "year": 2011}, {"title": "Mining and summarizing customer reviews", "author": ["Minqing Hu", "Bing Liu."], "venue": "Proceedings of the Tenth ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, pages 168\u2013177.", "citeRegEx": "Hu and Liu.,? 2004", "shortCiteRegEx": "Hu and Liu.", "year": 2004}, {"title": "A convolutional neural network for modelling sentences", "author": ["Nal Kalchbrenner", "Edward Grefenstette", "Phil Blunsom."], "venue": "Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics, pages 655\u2013665.", "citeRegEx": "Kalchbrenner et al\\.,? 2014", "shortCiteRegEx": "Kalchbrenner et al\\.", "year": 2014}, {"title": "Convolutional neural networks for sentence classification", "author": ["Yoon Kim."], "venue": "Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing, pages 1746\u20131751.", "citeRegEx": "Kim.,? 2014", "shortCiteRegEx": "Kim.", "year": 2014}, {"title": "Adam: A method for stochastic optimization", "author": ["Diederik P. Kingma", "Jimmy Ba."], "venue": "Proceedings of the International Conference on Learning Representations.", "citeRegEx": "Kingma and Ba.,? 2015", "shortCiteRegEx": "Kingma and Ba.", "year": 2015}, {"title": "Imagenet classification with deep convolutional neural networks", "author": ["Alex Krizhevsky", "Ilya Sutskever", "Geoffrey E. Hinton."], "venue": "Advances in Neural Information Processing Systems 25, pages 1097\u20131105.", "citeRegEx": "Krizhevsky et al\\.,? 2012", "shortCiteRegEx": "Krizhevsky et al\\.", "year": 2012}, {"title": "Deep learning via Hessian-free optimization", "author": ["James Martens."], "venue": "Proceedings of the 27th International Conference on Machine Learning, pages 735\u2013742.", "citeRegEx": "Martens.,? 2010", "shortCiteRegEx": "Martens.", "year": 2010}, {"title": "Distributed representations of words and phrases and their compositionality", "author": ["Tomas Mikolov", "Ilya Sutskever", "Kai Chen", "Greg S. Corrado", "Jeff Dean."], "venue": "Advances in Neural Information Processing Systems 26, pages 3111\u20133119.", "citeRegEx": "Mikolov et al\\.,? 2013", "shortCiteRegEx": "Mikolov et al\\.", "year": 2013}, {"title": "Feature selection, L1 vs", "author": ["Andrew Y. Ng."], "venue": "L2 regularization, and rotational invariance. In Proceedings of the Twenty-first International Conference on Machine Learning.", "citeRegEx": "Ng.,? 2004", "shortCiteRegEx": "Ng.", "year": 2004}, {"title": "Deep neural networks are easily fooled: High confidence predictions for unrecognizable images", "author": ["Anh Nguyen", "Jason Yosinski", "Jeff Clune."], "venue": "Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition.", "citeRegEx": "Nguyen et al\\.,? 2015", "shortCiteRegEx": "Nguyen et al\\.", "year": 2015}, {"title": "Seeing stars: Exploiting class relationships for sentiment categorization with respect to rating scales", "author": ["Bo Pang", "Lillian Lee."], "venue": "Proceedings of the 43rd Annual Meeting on Association for Computational Linguistics, pages 115\u2013124.", "citeRegEx": "Pang and Lee.,? 2005", "shortCiteRegEx": "Pang and Lee.", "year": 2005}, {"title": "Opinion mining and sentiment analysis", "author": ["Bo Pang", "Lillian Lee."], "venue": "Foundations and Trends in Information Retrieval, 2(1-2):1\u2013135.", "citeRegEx": "Pang and Lee.,? 2008", "shortCiteRegEx": "Pang and Lee.", "year": 2008}, {"title": "Contractive autoencoders: Explicit invariance during feature extraction", "author": ["Salah Rifai", "Pascal Vincent", "Xavier Muller", "Xavier Glorot", "Yoshua Bengio."], "venue": "Proceedings of the 28th International Conference on Machine Learning, pages 833\u2013840.", "citeRegEx": "Rifai et al\\.,? 2011", "shortCiteRegEx": "Rifai et al\\.", "year": 2011}, {"title": "Recursive deep models for semantic compositionality over a sentiment treebank", "author": ["Richard Socher", "Alex Perelygin", "Jean Y. Wu", "Jason Chuang", "Christopher D. Manning", "Andrew Y. Ng", "Christopher Potts."], "venue": "Proceedings of the 2013 Conference on Empiri-", "citeRegEx": "Socher et al\\.,? 2013", "shortCiteRegEx": "Socher et al\\.", "year": 2013}, {"title": "Dropout: A simple way to prevent neural networks from overfitting", "author": ["Nitish Srivastava", "Geoffrey Hinton", "Alex Krizhevsky", "Ilya Sutskever", "Ruslan Salakhutdinov."], "venue": "Journal of Machine Learning Research, 15:1929\u20131958.", "citeRegEx": "Srivastava et al\\.,? 2014", "shortCiteRegEx": "Srivastava et al\\.", "year": 2014}, {"title": "A sensitivity anal", "author": ["Ye Zhang", "Byron Wallace"], "venue": null, "citeRegEx": "Zhang and Wallace.,? \\Q2015\\E", "shortCiteRegEx": "Zhang and Wallace.", "year": 2015}], "referenceMentions": [{"referenceID": 12, "context": "Deep learning has achieved state-of-the-art results across a range of computer vision (Krizhevsky et al., 2012), speech recognition (Graves et al.", "startOffset": 86, "endOffset": 111}, {"referenceID": 6, "context": ", 2012), speech recognition (Graves et al., 2013) and natural language processing tasks (Bahdanau et al.", "startOffset": 28, "endOffset": 49}, {"referenceID": 1, "context": ", 2013) and natural language processing tasks (Bahdanau et al., 2015; Kalchbrenner et al., 2014; Yih et al., 2014; Bitvai and Cohn, 2015).", "startOffset": 46, "endOffset": 137}, {"referenceID": 9, "context": ", 2013) and natural language processing tasks (Bahdanau et al., 2015; Kalchbrenner et al., 2014; Yih et al., 2014; Bitvai and Cohn, 2015).", "startOffset": 46, "endOffset": 137}, {"referenceID": 2, "context": ", 2013) and natural language processing tasks (Bahdanau et al., 2015; Kalchbrenner et al., 2014; Yih et al., 2014; Bitvai and Cohn, 2015).", "startOffset": 46, "endOffset": 137}, {"referenceID": 16, "context": "However, deep models are often overconfident for noisy test instances, making them susceptible to adversarial attacks (Nguyen et al., 2015; Tabacof and Valle, 2016).", "startOffset": 118, "endOffset": 164}, {"referenceID": 1, "context": ", 2013) and natural language processing tasks (Bahdanau et al., 2015; Kalchbrenner et al., 2014; Yih et al., 2014; Bitvai and Cohn, 2015). However, deep models are often overconfident for noisy test instances, making them susceptible to adversarial attacks (Nguyen et al., 2015; Tabacof and Valle, 2016). Goodfellow et al. (2014) argued that the primary cause of neural networks\u2019 vulnerability to adversarial perturbation is their linear nature, due to neural models being intentionally designed to behave in a mostly linear manner to facilitate optimization.", "startOffset": 47, "endOffset": 330}, {"referenceID": 4, "context": "Fawzi et al. (2015) provided a theoretical framework for analyzing the robustness of classifiers to adversarial perturbations, and also showed linear models are usually not robust to adversarial noise.", "startOffset": 0, "endOffset": 20}, {"referenceID": 18, "context": "We empirically demonstrate the effectiveness of the model over text corpora with increasing amounts of artificial masking noise, using a range of sentiment analysis datasets (Pang and Lee, 2008) with a convolutional neural network model (Kim, 2014).", "startOffset": 174, "endOffset": 194}, {"referenceID": 10, "context": "We empirically demonstrate the effectiveness of the model over text corpora with increasing amounts of artificial masking noise, using a range of sentiment analysis datasets (Pang and Lee, 2008) with a convolutional neural network model (Kim, 2014).", "startOffset": 237, "endOffset": 248}, {"referenceID": 21, "context": "In this, we show that our method is superior to dropout (Srivastava et al., 2014) and a baseline method using MAP training.", "startOffset": 56, "endOffset": 81}, {"referenceID": 13, "context": "In this work, we present a regularization method which makes deep learning models more robust to noise, inspired by Rifai et al. (2011). The intuition behind the approach is to stabilize predictions by minimizing the ability of features to perturb predictions, based on high-order derivatives.", "startOffset": 72, "endOffset": 136}, {"referenceID": 13, "context": "In this work, we present a regularization method which makes deep learning models more robust to noise, inspired by Rifai et al. (2011). The intuition behind the approach is to stabilize predictions by minimizing the ability of features to perturb predictions, based on high-order derivatives. Rifai et al. (2011) introduced contractive autoencoders based on similar ideas, using the Frobenius norm of the Jacobian matrix as a penalty term to extract robust features.", "startOffset": 72, "endOffset": 314}, {"referenceID": 12, "context": "Also related, Martens (2010) investigated a second-order optimization method based on Hessian-free approach for training deep auto-encoders.", "startOffset": 14, "endOffset": 29}, {"referenceID": 15, "context": "Conventional methods for learning robust models include l1 and l2 regularization (Ng, 2004), and dropout (Srivastava et al.", "startOffset": 81, "endOffset": 91}, {"referenceID": 21, "context": "Conventional methods for learning robust models include l1 and l2 regularization (Ng, 2004), and dropout (Srivastava et al., 2014).", "startOffset": 105, "endOffset": 130}, {"referenceID": 15, "context": "Conventional methods for learning robust models include l1 and l2 regularization (Ng, 2004), and dropout (Srivastava et al., 2014). In fact, Wager et al. (2013) showed that the dropout regularizer is first-order equivalent to an l2 regularizer applied after scaling the features.", "startOffset": 31, "endOffset": 161}, {"referenceID": 5, "context": "Our method is inspired by the work on adversarial training in computer vision (Goodfellow et al., 2014).", "startOffset": 78, "endOffset": 103}, {"referenceID": 5, "context": "Our method is inspired by the work on adversarial training in computer vision (Goodfellow et al., 2014). In image recognition tasks, small distortions that are indiscernible to humans can significantly distort the predictions of neural networks (Szegedy et al., 2014). An intuitive explanation of our regularization method is, when noise is applied to the data, the variation of the output is kept lower than the noise. We adapt this idea from Rifai et al. (2011) and develop the Jacobian regularization method.", "startOffset": 79, "endOffset": 464}, {"referenceID": 10, "context": "The CNN used in this research is based on the model proposed by Kim (2014), and is outlined below.", "startOffset": 64, "endOffset": 75}, {"referenceID": 18, "context": "We experiment on the following datasets,2 following Kim (2014): \u2022 MR: Sentence polarity dataset (Pang and Lee, 2008)3 \u2022 Subj: Subjectivity dataset (Pang and Lee, 2005)3 \u2022 CR: Customer review dataset (Hu and Liu, 2004)4 \u2022 SST: Stanford Sentiment Treebank, using the 3-class configuration (Socher et al.", "startOffset": 96, "endOffset": 116}, {"referenceID": 17, "context": "We experiment on the following datasets,2 following Kim (2014): \u2022 MR: Sentence polarity dataset (Pang and Lee, 2008)3 \u2022 Subj: Subjectivity dataset (Pang and Lee, 2005)3 \u2022 CR: Customer review dataset (Hu and Liu, 2004)4 \u2022 SST: Stanford Sentiment Treebank, using the 3-class configuration (Socher et al.", "startOffset": 147, "endOffset": 167}, {"referenceID": 8, "context": "We experiment on the following datasets,2 following Kim (2014): \u2022 MR: Sentence polarity dataset (Pang and Lee, 2008)3 \u2022 Subj: Subjectivity dataset (Pang and Lee, 2005)3 \u2022 CR: Customer review dataset (Hu and Liu, 2004)4 \u2022 SST: Stanford Sentiment Treebank, using the 3-class configuration (Socher et al.", "startOffset": 199, "endOffset": 217}, {"referenceID": 20, "context": "We experiment on the following datasets,2 following Kim (2014): \u2022 MR: Sentence polarity dataset (Pang and Lee, 2008)3 \u2022 Subj: Subjectivity dataset (Pang and Lee, 2005)3 \u2022 CR: Customer review dataset (Hu and Liu, 2004)4 \u2022 SST: Stanford Sentiment Treebank, using the 3-class configuration (Socher et al., 2013)5 In each case, we evaluate using classification accuracy.", "startOffset": 287, "endOffset": 308}, {"referenceID": 9, "context": "We experiment on the following datasets,2 following Kim (2014): \u2022 MR: Sentence polarity dataset (Pang and Lee, 2008)3 \u2022 Subj: Subjectivity dataset (Pang and Lee, 2005)3 \u2022 CR: Customer review dataset (Hu and Liu, 2004)4 \u2022 SST: Stanford Sentiment Treebank, using the 3-class configuration (Socher et al.", "startOffset": 52, "endOffset": 63}, {"referenceID": 7, "context": "We assume that when dealing with short text such as Twitter posts, it is common to see unknown words due to typos, abbreviations and sociolinguistic marking of different types (Han and Baldwin, 2011; Eisenstein, 2013).", "startOffset": 176, "endOffset": 217}, {"referenceID": 3, "context": "We assume that when dealing with short text such as Twitter posts, it is common to see unknown words due to typos, abbreviations and sociolinguistic marking of different types (Han and Baldwin, 2011; Eisenstein, 2013).", "startOffset": 176, "endOffset": 217}, {"referenceID": 10, "context": "Refer to Kim (2014) for more details on the datasets.", "startOffset": 9, "endOffset": 20}, {"referenceID": 14, "context": "To set the hyper-parameters of the CNN, we follow the guidelines of Zhang and Wallace (2015), setting word embeddings to m = 300 dimensions and initialising based on word2vec pre-training (Mikolov et al., 2013).", "startOffset": 188, "endOffset": 210}, {"referenceID": 11, "context": "We use the Adam optimizer (Kingma and Ba, 2015) for training.", "startOffset": 26, "endOffset": 47}, {"referenceID": 13, "context": "To set the hyper-parameters of the CNN, we follow the guidelines of Zhang and Wallace (2015), setting word embeddings to m = 300 dimensions and initialising based on word2vec pre-training (Mikolov et al.", "startOffset": 71, "endOffset": 93}], "year": 2016, "abstractText": "Deep neural networks have achieved remarkable results across many language processing tasks, however these methods are highly sensitive to noise and adversarial attacks. We present a regularization based method for limiting network sensitivity to its inputs, inspired by ideas from computer vision, thus learning models that are more robust. Empirical evaluation over a range of sentiment datasets with a convolutional neural network shows that, compared to a baseline model and the dropout method, our method achieves superior performance over noisy inputs and out-of-domain data.1", "creator": "LaTeX with hyperref package"}}}