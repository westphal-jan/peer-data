{"id": "1609.06380", "review": {"conference": "EMNLP", "VERSION": "v1", "DATE_OF_SUBMISSION": "20-Sep-2016", "title": "Recognizing Implicit Discourse Relations via Repeated Reading: Neural Networks with Multi-Level Attention", "abstract": "Recognizing implicit discourse relations is a challenging but important task in the field of Natural Language Processing. For such a complex text processing task, different from previous studies, we argue that it is necessary to repeatedly read the arguments and dynamically exploit the efficient features useful for recognizing discourse relations. To mimic the repeated reading strategy, we propose the neural networks with multi-level attention (NNMA), combining the attention mechanism and external memories to gradually fix the attention on some specific words helpful to judging the discourse relations. Experiments on the PDTB dataset show that our proposed method achieves the state-of-art results. The visualization of the attention weights also illustrates the progress that our model observes the arguments on each level and progressively locates the important words.", "histories": [["v1", "Tue, 20 Sep 2016 22:59:19 GMT  (703kb,D)", "http://arxiv.org/abs/1609.06380v1", "Accepted as long paper at EMNLP2016"]], "COMMENTS": "Accepted as long paper at EMNLP2016", "reviews": [], "SUBJECTS": "cs.CL cs.AI", "authors": ["yang liu", "sujian li"], "accepted": true, "id": "1609.06380"}, "pdf": {"name": "1609.06380.pdf", "metadata": {"source": "CRF", "title": "Recognizing Implicit Discourse Relations via Repeated Reading: Neural Networks with Multi-Level Attention", "authors": ["Yang Liu", "Sujian Li"], "emails": ["lisujian}@pku.edu.cn"], "sections": [{"heading": null, "text": "Recognition of implicit discourse relationships is a demanding but important task in the field of natural language processing. For such a complex word processing task, which differs from previous studies, we argue that it is necessary to re-read the arguments and use the efficient features useful for detecting discourse relationships dynamically. To mimic the repeated reading strategy, we propose multi-level attention neural networks (NNMA), combining the attention mechanism and external reminders to gradually focus attention on some specific words that are helpful for assessing the discourse relationships. Experiments with the PDTB data set show that our proposed method achieves the latest results."}, {"heading": "1 Introduction", "text": "In fact, most of them will be able to play by the rules that they have set themselves in order to play by the rules that they have played by."}, {"heading": "2 Repeated Reading Neural Network with Multi-Level Attention", "text": "To do this, we use the bi-directional Long-Short Term Memory Neural Network (Bi-LSTM) to model each argument, since Bi-LSTM is good at modelling over a sequence of words and can represent each word with more contextual information in mind. Afterwards, several levels of attention are developed to simulate subsequent multiple readings. At each level of attention, an external short-term memory is used to store what has been learned from previous runs and on which words to focus. To determine the useful parts of the arguments, the attention mechanism is used to predict a probability distribution over each word, indicating the extent to which each word should be affected. The overall architecture of our model is illustrated in Figure 1."}, {"heading": "2.1 Representing Arguments with LSTM", "text": "\u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7"}, {"heading": "2.2 Tuning Attention via Repeated Reading", "text": "\"We are the first, we are the first, second, third, third, fourth, fourth, fourth, fifth, fifth, fifth, fifth, fifth, fifth, fifth, fifth, fifth, fifth, fifth, fifth, fifth, fifth, fifth, fifth, fifth, fifth, fifth, fifth, fifth, sixth, sixth, sixth, seventh, seventh, seventh, seventh, seventh, seventh, seventh, seventh, seventh, seventh, seventh, seventh, seventh, seventh, seventh, seventh, seventh, seventh, seventh, seventh, seventh, seventh, seventh, seventh, seventh, seventh, seventh, seventh, seventh, seventh, seventh, seventh, seventh, seventh, seventh, seventh, seventh, seventh, seventh, seventh, seventh, seventh, seventh, seventh, seventh, seventh, seventh, seventh, seventh, seventh, seventh, seventh, seventh, seventh, seventh, seventh, seventh, seventh, seventh, seventh, seventh, seventh, seventh, seventh, seventh, seventh, seventh, seventh, seventh, seventh, seventh, seventh, seventh, seventh, seventh, seventh, seventh, seventh, seventh, seventh, seventh, seventh, seventh, seventh, seventh, seventh, seventh, seventh, seventh, seventh, seventh, seventh, seventh, seventh, seventh, seventh, seventh, seventh, seventh, seventh, seventh, seventh, seventh, seventh, seventh, seventh, seventh, seventh, seventh, seventh, seventh, seventh, seventh, seventh, seventh, seventh, seventh, seventh, seventh, seventh, seventh, seventh, seventh, seventh, seventh, seventh, seventh, seventh, seventh, seventh, seventh, seventh, seventh, seventh, seventh, seventh, seventh, seventh, seventh, seventh, seventh, seventh, seventh, seventh, seventh, seventh, seventh, seventh, seventh, seventh, seventh, seventh, seventh, seventh, seventh, seventh, seventh, seventh, seventh, seventh, seventh, seventh, seventh, seventh, seventh, seventh, seventh, seventh, seventh, seventh, seventh, seventh, seventh, seventh, seventh, seventh, seventh, seventh, seventh, seventh, seventh, seventh, seventh, seventh, seventh, seventh, seventh, seventh, seventh, seventh, seventh, seventh, seventh, seventh, seventh, seventh, seventh, seventh, seventh, seventh, seventh, seventh, seventh, seventh, seventh, seventh, seventh, seventh, seventh, seventh, seventh, seventh, seventh, seventh, seventh, seventh, seventh, seventh, seventh, seventh"}, {"heading": "2.3 Model Training", "text": "To train our model, the training goal is defined as a loss of lateral entropy between the outputs of the Softmax layer and the Ground Truth Class labels. We use stochastic gradient descendence (SGD) with dynamics to train the neural networks. To avoid overmatch, drop-out mode is applied to the top feature vector before the Softmax layer. In addition, we use different learning rates \u03bb and \u03bbe to train the neural network parameters \u0442 and the word embedding to which Ji and Eisenstein refer, 2015).To prevent an overhaul of this task, a small value is set. In the experimental part, we will present the setting of hyperparameters."}, {"heading": "3 Experiments", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "3.1 Preparation", "text": "We evaluate our model based on the Penn Discourse Treebank (PDTB) (Prasad et al., 2008). In our work, we experiment with the top four classes of this corpus as in previous work (Rutherford and Xue, 2015). We extract all implicit relationships of the PDTB and follow the setup of (Rutherford and Xue, 2015). We divide the data into a training set (Sections 2-20), a development set (Sections 0-1) and a test set (Sections 21-22). Table 1 summarizes the statistics of the four PDTB discourse relationships, i.e., comparison, contingency, expansion and temporal.We first convert the tokens in the PDTB into lowercase letters. The word embeddings used to initialize the word representations are provided by GloVe (Pennington et al., 2014), and the dimension of the embeddings is De 50. The hyperparameter problem, we show the two-level dynamics, including the dynamics."}, {"heading": "3.2 Results", "text": "This year it has come to the point where it will be able to put itself at the top, \"he said.\" We have to put ourselves at the top, \"he said.\" We have to put ourselves at the top, \"he said.\" We have to put ourselves at the top, \"he said.\" We have to put ourselves at the top, \"he said.\" We have to put ourselves at the top, \"he said.\" We have to put ourselves at the top. \""}, {"heading": "3.3 Analysis of Attention Levels", "text": "The multiple attention levels in our model greatly enhance the performance of the classification of implicit discourse relationships. In this subsection, we perform both qualitative and quantitative analyses at the attention levels. First, we take a three-level NNMA model for example and analyze its attention distributions at different attention levels by looking at the mean Kullback-Leibler (KL) divergence between any two levels at the training level. In Figure 3, we use klij to name the KL divergence between the ith and the jth attention levels and Smart to name the KL divergence between the uniform distribution and the attention level. We can see that each attention level forms different attention distributions and the difference in the higher levels increases. It can be deduced that the 2nd and 3rd levels give more attention than the NMA and some words gradually neglect in the NMA."}, {"heading": "4 Related Work", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "4.1 Implicit Discourse Relation Classification", "text": "The Penn Discourse Treebank (PDTB) (Prasad et al., 2008), known as the largest discourse corpus, consists of 2159 Wall Street Journal articles. Each document is provided with the predicate argumentation structure, with the predicate being the discourse that is connective (e.g. during) and the arguments encompassing two passages of text around the connective. The discourse corpus can be either explicit or implicit. In the PDTB, a hierarchy of relation labels is provided for commenting (Wang et al., 2012). In our study, we use the four top-level tags, including temporal, contingency, comparison, and expansion. These four core relationships allow us to be theoretically neutral as they are incorporated into almost all discourse theories, sometimes with different names (Wang et al., 2012). Implicit discourse relationship recognition is often treated as a classification problem that represents the classification problem (the TB)."}, {"heading": "4.2 Neural Networks and Attention Mechanism", "text": "Recently, neural network-based methods have become increasingly important in the field of processing natural language (Kim, 2014), based primarily on learning a distributed representation for each word, also known as word embedding (Collobert et al., 2011). Attention mechanisms were first introduced into neural models to solve the alignment problem between different modalities. Graves (2013) designed a neural network to generate handwriting based on text when generating each target word. Tan et al. (2015) proposed an attention-based neural network to model both questions and sentences from the content within the window in order to select the appropriate non-factoid answers. Parallel, the idea was developed to equip the neural model with an external memory."}, {"heading": "5 Conclusion", "text": "As a complex task of word processing, implicit recognition of discourse relationships requires in-depth analysis of the arguments. To this end, we propose for the first time to mimic the repeated reading strategy and dynamically utilize efficient features through multiple reading passes. Following this idea, we design multi-level neural networks (NNMA), with the general level and attention levels representing the first and subsequent reading passes. Using external short-term memories, the NNMA can gradually update the representations of arguments at each level of attention and focus attention on some specific words that provide effective clues to the recognition of discourse relationships. We have conducted experiments on the PDTB and the evaluation results show that our model can achieve state-of-the-art performance in recognizing implicit discourse relations."}], "references": [{"title": "Neural machine translation by jointly learning to align and translate", "author": ["Kyunghyun Cho", "Yoshua Bengio"], "venue": "arXiv preprint arXiv:1409.0473", "citeRegEx": "Bahdanau et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Bahdanau et al\\.", "year": 2014}, {"title": "Probabilistic head-driven parsing for discourse structure", "author": ["Baldridge", "Lascarides2005] Jason Baldridge", "Alex Lascarides"], "venue": "In Proceedings of CoNLL", "citeRegEx": "Baldridge et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Baldridge et al\\.", "year": 2005}, {"title": "Comparing word representations for implicit discourse relation classification", "author": ["Braud", "Denis2015] Chlo\u00e9 Braud", "Pascal Denis"], "venue": "In Proceedings of EMNLP", "citeRegEx": "Braud et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Braud et al\\.", "year": 2015}, {"title": "Natural language processing (almost) from scratch", "author": ["Jason Weston", "L\u00e9on Bottou", "Michael Karlen", "Koray Kavukcuoglu", "Pavel Kuksa"], "venue": "The Journal of Machine Learning Research,", "citeRegEx": "Collobert et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Collobert et al\\.", "year": 2011}, {"title": "Generating sequences with recurrent neural networks. arXiv preprint arXiv:1308.0850", "author": ["Alex Graves"], "venue": null, "citeRegEx": "Graves.,? \\Q2013\\E", "shortCiteRegEx": "Graves.", "year": 2013}, {"title": "One vector is not enough: Entity-augmented distributed semantics for discourse relations. Transactions of the Association for Computational Linguistics, 3:329\u2013344", "author": ["Ji", "Eisenstein2015] Yangfeng Ji", "Jacob Eisenstein"], "venue": null, "citeRegEx": "Ji et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Ji et al\\.", "year": 2015}, {"title": "A latent variable recurrent neural network for discourse relation language models. arXiv preprint arXiv:1603.01913", "author": ["Ji et al.2016] Yangfeng Ji", "Gholamreza Haffari", "Jacob Eisenstein"], "venue": null, "citeRegEx": "Ji et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Ji et al\\.", "year": 2016}, {"title": "A novel discriminative framework for sentence-level discourse analysis", "author": ["Joty et al.2012] Shafiq Joty", "Giuseppe Carenini", "Raymond T Ng"], "venue": "In Proceedings of EMNLP", "citeRegEx": "Joty et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Joty et al\\.", "year": 2012}, {"title": "Convolutional neural networks for sentence classification", "author": ["Yoon Kim"], "venue": "In Proceedings of EMNLP", "citeRegEx": "Kim.,? \\Q2014\\E", "shortCiteRegEx": "Kim.", "year": 2014}, {"title": "Ask me anything: Dynamic memory networks for natural language processing", "author": ["Kumar et al.2015] Ankit Kumar", "Ozan Irsoy", "Jonathan Su", "James Bradbury", "Robert English", "Brian Pierce", "Peter Ondruska", "Ishaan Gulrajani", "Richard Socher"], "venue": null, "citeRegEx": "Kumar et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Kumar et al\\.", "year": 2015}, {"title": "Recognizing implicit discourse relations in the penn discourse treebank", "author": ["Lin et al.2009] Ziheng Lin", "Min-Yen Kan", "Hwee Tou Ng"], "venue": "In Proceedings of EMNLP", "citeRegEx": "Lin et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Lin et al\\.", "year": 2009}, {"title": "Implicit discourse relation classification via multi-task neural network", "author": ["Liu et al.2016] Yang Liu", "Sujian Li", "Xiaodong Zhang", "Zhifang Sui"], "venue": "In Proceedings of AAAI", "citeRegEx": "Liu et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Liu et al\\.", "year": 2016}, {"title": "Linguistic regularities in continuous space word representations", "author": ["Wen-tau Yih", "Geoffrey Zweig"], "venue": "In Proceedings of NAACL", "citeRegEx": "Mikolov et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Mikolov et al\\.", "year": 2013}, {"title": "Improving implicit discourse relation recognition through feature set optimization", "author": ["Park", "Cardie2012] Joonsuk Park", "Claire Cardie"], "venue": "In Proceedings of SigDial", "citeRegEx": "Park et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Park et al\\.", "year": 2012}, {"title": "Glove: Global vectors for word representation", "author": ["Richard Socher", "Christopher D. Manning"], "venue": "In Proceedings of EMNLP", "citeRegEx": "Pennington et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Pennington et al\\.", "year": 2014}, {"title": "Automatic sense prediction for implicit discourse relations in text", "author": ["Pitler et al.2009] Emily Pitler", "Annie Louis", "Ani Nenkova"], "venue": "In Proceedings of ACL", "citeRegEx": "Pitler et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Pitler et al\\.", "year": 2009}, {"title": "The Penn Discourse TreeBank", "author": ["Prasad et al.2008] Rashmi Prasad", "Nikhil Dinesh", "Alan Lee", "Eleni Miltsakaki", "Livio Robaldo", "Aravind K. Joshi", "Bonnie L. Webber"], "venue": "Proceedings of LREC", "citeRegEx": "Prasad et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Prasad et al\\.", "year": 2008}, {"title": "Discovering implicit discourse relations through brown cluster pair representation and coreference patterns", "author": ["Rutherford", "Xue2014] Attapol Rutherford", "Nianwen Xue"], "venue": "In Proceedings of EACL", "citeRegEx": "Rutherford et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Rutherford et al\\.", "year": 2014}, {"title": "Improving the inference of implicit discourse relations via classifying explicit discourse connectives", "author": ["Rutherford", "Xue2015] Attapol T Rutherford", "Nianwen Xue"], "venue": "In Proceedings of NAACL", "citeRegEx": "Rutherford et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Rutherford et al\\.", "year": 2015}, {"title": "Sentence level discourse parsing using syntactic and lexical information", "author": ["Soricut", "Marcu2003] Radu Soricut", "Daniel Marcu"], "venue": "In Proceedings of NAACL", "citeRegEx": "Soricut et al\\.,? \\Q2003\\E", "shortCiteRegEx": "Soricut et al\\.", "year": 2003}, {"title": "An effective discourse parser that uses rich linguistic information", "author": ["Subba", "Di Eugenio2009] Rajen Subba", "Barbara Di Eugenio"], "venue": "In Proceedings of NAACL", "citeRegEx": "Subba et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Subba et al\\.", "year": 2009}, {"title": "End-to-end memory networks", "author": ["Jason Weston", "Rob Fergus"], "venue": "In Proceedings of NIPS", "citeRegEx": "Sukhbaatar et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Sukhbaatar et al\\.", "year": 2015}, {"title": "Lstm-based deep learning models for non-factoid answer selection. arXiv preprint arXiv:1511.04108", "author": ["Tan et al.2015] Ming Tan", "Bing Xiang", "Bowen Zhou"], "venue": null, "citeRegEx": "Tan et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Tan et al\\.", "year": 2015}, {"title": "A long short-term memory model for answer sentence selection in question answering", "author": ["Wang", "Nyberg2015] Di Wang", "Eric Nyberg"], "venue": "In Proceedings of EMNLP", "citeRegEx": "Wang et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Wang et al\\.", "year": 2015}, {"title": "Implicit Discourse Relation Recognition by Selecting Typical Training Examples", "author": ["Wang et al.2012] Xun Wang", "Sujian Li", "Jiwei Li", "Wenjie Li"], "venue": "In Proceedings of COLING", "citeRegEx": "Wang et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Wang et al\\.", "year": 2012}, {"title": "Stacked attention networks for image question answering", "author": ["Yang et al.2015] Zichao Yang", "Xiaodong He", "Jianfeng Gao", "Li Deng", "Alexander J. Smola"], "venue": "arXiv preprint arXiv:1511.02274", "citeRegEx": "Yang et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Yang et al\\.", "year": 2015}, {"title": "Learning to execute. arXiv preprint arXiv:1410.4615", "author": ["Zaremba", "Sutskever2014] Wojciech Zaremba", "Ilya Sutskever"], "venue": null, "citeRegEx": "Zaremba et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Zaremba et al\\.", "year": 2014}, {"title": "Shallow convolutional neural network for implicit discourse relation recognition", "author": ["Zhang et al.2015] Biao Zhang", "Jinsong Su", "Deyi Xiong", "Yaojie Lu", "Hong Duan", "Junfeng Yao"], "venue": "Proceedings of EMNLP", "citeRegEx": "Zhang et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Zhang et al\\.", "year": 2015}, {"title": "Predicting discourse connectives for implicit discourse relation recognition", "author": ["Zhou et al.2010] Zhi-Min Zhou", "Yu Xu", "Zheng-Yu Niu", "Man Lan", "Jian Su", "Chew Lim Tan"], "venue": "In Proceedings of the ICCL,", "citeRegEx": "Zhou et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Zhou et al\\.", "year": 2010}], "referenceMentions": [{"referenceID": 15, "context": "Previous research mainly focus on exploring various kinds of efficient features and machine learning models to classify the implicit discourse relations (Soricut and Marcu, 2003; Baldridge and Lascarides, 2005; Subba and Di Eugenio, 2009; Hernault et al., 2010; Pitler et al., 2009; Joty et al., 2012).", "startOffset": 153, "endOffset": 301}, {"referenceID": 7, "context": "Previous research mainly focus on exploring various kinds of efficient features and machine learning models to classify the implicit discourse relations (Soricut and Marcu, 2003; Baldridge and Lascarides, 2005; Subba and Di Eugenio, 2009; Hernault et al., 2010; Pitler et al., 2009; Joty et al., 2012).", "startOffset": 153, "endOffset": 301}, {"referenceID": 0, "context": "These models mostly combine the attention mechanism that has been originally designed to solve the alignment problem in machine translation (Bahdanau et al., 2014) and the external memory which can be read and written when processing the objects (Sukhbaatar et al.", "startOffset": 140, "endOffset": 163}, {"referenceID": 21, "context": ", 2014) and the external memory which can be read and written when processing the objects (Sukhbaatar et al., 2015).", "startOffset": 90, "endOffset": 115}, {"referenceID": 0, "context": "These models mostly combine the attention mechanism that has been originally designed to solve the alignment problem in machine translation (Bahdanau et al., 2014) and the external memory which can be read and written when processing the objects (Sukhbaatar et al., 2015). For example, Kumar et al. (2015) drew attention to specific facts of the input sequence and processed the sequence via multiple hops to generate an answer.", "startOffset": 141, "endOffset": 306}, {"referenceID": 0, "context": "These models mostly combine the attention mechanism that has been originally designed to solve the alignment problem in machine translation (Bahdanau et al., 2014) and the external memory which can be read and written when processing the objects (Sukhbaatar et al., 2015). For example, Kumar et al. (2015) drew attention to specific facts of the input sequence and processed the sequence via multiple hops to generate an answer. In computation vision, Yang et al. (2015) pointed out that repeatedly giving attention to different regions of an image could gradually lead to more precise image representations.", "startOffset": 141, "endOffset": 471}, {"referenceID": 14, "context": "Here we adopt the pre-trained vectors provided by GloVe (Pennington et al., 2014).", "startOffset": 56, "endOffset": 81}, {"referenceID": 12, "context": "has been found explainable and meaningful in many applications (Mikolov et al., 2013).", "startOffset": 63, "endOffset": 85}, {"referenceID": 16, "context": "We evaluate our model on the Penn Discourse Treebank (PDTB) (Prasad et al., 2008).", "startOffset": 60, "endOffset": 81}, {"referenceID": 14, "context": "The word embeddings used for initializing the word representations are provided by GloVe (Pennington et al., 2014), and the dimension of the embeddings De is 50.", "startOffset": 89, "endOffset": 114}, {"referenceID": 28, "context": "The reason may be that the identification of the \u201cComparison\u201d depends more on some deep analysis such as semantic parsing, according to (Zhou et al., 2010).", "startOffset": 136, "endOffset": 155}, {"referenceID": 27, "context": "\u2022 Zhang2015: Zhang et al. (2015) proposed to use shallow convolutional neural networks to model two arguments respectively.", "startOffset": 13, "endOffset": 33}, {"referenceID": 11, "context": "\u2022 Liu2016: Liu et al. (2016) proposed to better classify the discourse relations by learning from other discourse-related tasks with a multitask neural network.", "startOffset": 11, "endOffset": 29}, {"referenceID": 5, "context": "\u2022 Ji2016: Ji et al. (2016) proposed a neural language model over sequences of words and used the discourse relations as latent variables to connect the adjacent sequences.", "startOffset": 10, "endOffset": 27}, {"referenceID": 16, "context": "The Penn Discourse Treebank (PDTB) (Prasad et al., 2008), known as the largest discourse corpus, is composed of 2159 Wall Street Journal articles.", "startOffset": 35, "endOffset": 56}, {"referenceID": 24, "context": "These four core relations allow us to be theory-neutral, since they are almost included in all discourse theories, sometimes with different names (Wang et al., 2012).", "startOffset": 146, "endOffset": 165}, {"referenceID": 15, "context": "The first work to tackle this task on PDTB is (Pitler et al., 2009).", "startOffset": 46, "endOffset": 67}, {"referenceID": 8, "context": "Extending from this work, Lin et al. (2009) further identified four different feature types representing the context, the constituent parse trees, the dependency parse trees and the raw text respectively.", "startOffset": 26, "endOffset": 44}, {"referenceID": 8, "context": "Extending from this work, Lin et al. (2009) further identified four different feature types representing the context, the constituent parse trees, the dependency parse trees and the raw text respectively. Rutherford and Xue (2014) used brown cluster to replace the word pair features for solving the sparsity problem.", "startOffset": 26, "endOffset": 231}, {"referenceID": 8, "context": "Extending from this work, Lin et al. (2009) further identified four different feature types representing the context, the constituent parse trees, the dependency parse trees and the raw text respectively. Rutherford and Xue (2014) used brown cluster to replace the word pair features for solving the sparsity problem. Ji and Eisenstein (2015) adopted two recursive neural networks to exploit the representation of arguments and entity spans.", "startOffset": 26, "endOffset": 343}, {"referenceID": 8, "context": "Extending from this work, Lin et al. (2009) further identified four different feature types representing the context, the constituent parse trees, the dependency parse trees and the raw text respectively. Rutherford and Xue (2014) used brown cluster to replace the word pair features for solving the sparsity problem. Ji and Eisenstein (2015) adopted two recursive neural networks to exploit the representation of arguments and entity spans. Very recently, Liu et al. (2016) proposed a twodimensional convolutional neural network (CNN) to model the argument pairs and employed a multitask learning framework to boost the performance by learning from other discourse-related tasks.", "startOffset": 26, "endOffset": 475}, {"referenceID": 5, "context": "Ji et al. (2016) considered discourse relations as latent variables connecting two token sequences and trained a discourse informed language model.", "startOffset": 0, "endOffset": 17}, {"referenceID": 8, "context": "Recently, neural network-based methods have gained prominence in the field of natural language processing (Kim, 2014).", "startOffset": 106, "endOffset": 117}, {"referenceID": 3, "context": "Such methods are primarily based on learning a distributed representation for each word, which is also called a word embedding (Collobert et al., 2011).", "startOffset": 127, "endOffset": 151}, {"referenceID": 3, "context": "Graves (2013) designed a neural network to generate handwriting based on a text.", "startOffset": 0, "endOffset": 14}, {"referenceID": 0, "context": "Bahdanau et al. (2014) introduced this idea into machine translation, where their model computed a probabilistic distribution over the input sequence when generating each target word.", "startOffset": 0, "endOffset": 23}, {"referenceID": 0, "context": "Bahdanau et al. (2014) introduced this idea into machine translation, where their model computed a probabilistic distribution over the input sequence when generating each target word. Tan et al. (2015) proposed an attentionbased neural network to model both questions and sentences for selecting the appropriate non-factoid answers.", "startOffset": 0, "endOffset": 202}, {"referenceID": 9, "context": "Kumar et al. (2015) proposed a similar model where a memory was designed to change the gate of the gated recurrent unit for each iteration.", "startOffset": 0, "endOffset": 20}], "year": 2016, "abstractText": "Recognizing implicit discourse relations is a challenging but important task in the field of Natural Language Processing. For such a complex text processing task, different from previous studies, we argue that it is necessary to repeatedly read the arguments and dynamically exploit the efficient features useful for recognizing discourse relations. To mimic the repeated reading strategy, we propose the neural networks with multi-level attention (NNMA), combining the attention mechanism and external memories to gradually fix the attention on some specific words helpful to judging the discourse relations. Experiments on the PDTB dataset show that our proposed method achieves the state-ofart results. The visualization of the attention weights also illustrates the progress that our model observes the arguments on each level and progressively locates the important words.", "creator": "LaTeX with hyperref package"}}}