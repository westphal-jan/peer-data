{"id": "1609.06390", "review": {"conference": "NIPS", "VERSION": "v1", "DATE_OF_SUBMISSION": "21-Sep-2016", "title": "Learning HMMs with Nonparametric Emissions via Spectral Decompositions of Continuous Matrices", "abstract": "Recently, there has been a surge of interest in using spectral methods for estimating latent variable models. However, it is usually assumed that the distribution of the observations conditioned on the latent variables is either discrete or belongs to a parametric family. In this paper, we study the estimation of an $m$-state hidden Markov model (HMM) with only smoothness assumptions, such as H\\\"olderian conditions, on the emission densities. By leveraging some recent advances in continuous linear algebra and numerical analysis, we develop a computationally efficient spectral algorithm for learning nonparametric HMMs. Our technique is based on computing an SVD on nonparametric estimates of density functions by viewing them as \\emph{continuous matrices}. We derive sample complexity bounds via concentration results for nonparametric density estimation and novel perturbation theory results for continuous matrices. We implement our method using Chebyshev polynomial approximations. Our method is competitive with other baselines on synthetic and real problems and is also very computationally efficient.", "histories": [["v1", "Wed, 21 Sep 2016 00:15:44 GMT  (2628kb,D)", "http://arxiv.org/abs/1609.06390v1", "To appear in NIPS 2016"]], "COMMENTS": "To appear in NIPS 2016", "reviews": [], "SUBJECTS": "stat.ML cs.LG", "authors": ["kirthevasan kandasamy", "maruan al-shedivat", "eric p xing"], "accepted": true, "id": "1609.06390"}, "pdf": {"name": "1609.06390.pdf", "metadata": {"source": "CRF", "title": "Learning HMMs with Nonparametric Emissions via Spectral Decompositions of Continuous Matrices", "authors": ["Kirthevasan Kandasamy", "Maruan Al-Shedivat", "Eric P. Xing"], "emails": ["kandasamy@cs.cmu.edu", "alshedivat@cs.cmu.edu", "epxing@cs.cmu.edu"], "sections": [{"heading": "1 Introduction", "text": "This year it is more than ever before."}, {"heading": "2 A Pint-sized Review of Continuous Linear Algebra", "text": "We start with a pint-sized review on continuous linear algebra which treats functions as continuous analogues of matrices. Appendix A contains a quart-sized review. Both sections are based on [5, 6]. While these objects can be considered operators on Hilbert spaces which have been extensively studied over the years, the above line of work (simplified and specialized) could be infinite. A column qmatrix (quasi-matrix) Q-Rm \u00b7 n array of numbers where F (i, j) denotes the entry in line i, column j. m or n could be (countable) infinite. A column qmatrix (quasi-matrix) Q-Rm \u00b7 m is a collection of m functions defined on [a, b] where the row index is continuous and the column index is discrete. WritingQ = [q1, qm] where qj."}, {"heading": "3 Nonparametric HMMs and the Observable Representation", "text": "The notation: The probability that the HMM would be in the first stage is P to denote probabilities of events, while p denotes probability functions (pdf). An HMM characterizes a probability distribution over a sequence of hidden states {ht} t \u2265 0 and observations {xt} t \u2265 0. At a given time, the HMM may be in one of the hidden states, i.e., the HMM will be fully characterized by the initial state distribution, and the observation is in a limited ongoing domain X. Without loss of universality, we take 2 X = [0, 1]. The non-parametric HMM is fully characterized by the initial state distribution. Rm, the state transition matrix T is Rm \u00b7 and the emission densities Oj: X \u2192 R, j [m]."}, {"heading": "4 Spectral Learning of HMMs with Nonparametric Emissions", "text": "The high-level idea of our NP-HMM-SPEC algorithm is as follows: First, we obtain density estimates for P1, P21, P321, which are then used to restore the observable representation b1, b \u221e, B by inserting the expressions in (1). Lemma 2 then gives us a way to estimate the joint and conditional probability densities. For the moment, we assume that we have N i.i.d sequences of triples {X (j)} Nj = 1, where X (j) = (X (j) 1, X (j) 2, X (j) 3) are the observations in the first three time steps. We describe learning from longer sequences in Section 4.3."}, {"heading": "4.1 Kernel Density Estimation", "text": "The first step is the estimation of joint probabilities, which requires a non-parametric density estimation. [17] Although there are several techniques [17], we use the Kernel Density Assessment (KDE) because it is easy to analyze and works well in practice. KDE for P1, P21 and P321 takes the form: P 1 (t) = 1N N N \u00b2 j = 1 h1 K (t \u2212 X (j) 1 h1), P \u00b2 21 (s, t) = 1 N \u00b2 j = 1 h221 K (s \u2212 X (j) 2 h21) K (t \u2212 X (j) 1 h21), P \u00b2 321 (r, s, t) = 1N \u00b2 J = 1 h3321 K (r \u2212 X (j) 3 h321) K (s \u2212 X (j) 2 h321 K (t \u2212 X (j) 1 h321)."}, {"heading": "4.2 The Spectral Algorithm", "text": "Algorithm 1 NP-HMM-SPEC Input: Data {X (j) = (X (j) 1, X (j) 2, X (j) 3)} Nj = 1, number of states m. \u2022 Get estimates P-1, P-21, P-321 for P1, P21, P321 via kernel density estimate (3). \u2022 Calculate the cmatrix SVD of P-21. Leave U-1 = (P > 21U) \u00b7 m the first m on the left singular vectors of P-21. \u2022 Calculate the observable parameters. Note that B-1 = U-21 is a Rm-weighted function.b-1 = (P > 21U) \u2020 P-1, B-1 (x) = (U-P > P-3x1) (U-3x1) (U-P-21) The above algorithm follows the roadmap specified at the beginning of this section."}, {"heading": "4.3 Implementation Details", "text": "This means that we see ourselves in a position to solve the problems we have caused ourselves, by solving them, by solving them, by solving them, by solving them, by solving them, by solving them, by putting them within limits, by placing them within limits, by placing them within limits, by placing them within limits, by placing them within the limits, by placing them within the limits, by placing them within the limits, by placing them within the limits, by placing them within the limits, by placing them within the limits, by placing them within the limits, by placing them within the limits, by placing them within the limits, by placing them within the limits, by placing them within the limits, by placing them within the limits, by placing them within the limits, by placing them within the limits, by placing them within the limits, by placing them within the limits, by placing them within the limits, by placing them within the limits, by placing them within the limits, by placing them within the limits of the limits, by placing them within the limits, by placing them within the limits of the limits, by placing them within the limits of the limits, by the limits, by placing them within the limitations, by the limitations of the limitations, by the limitations of the limitations, by the limitations of the limitations, by the limitations of the limitations, by the limitations of the limitations, by the limitations of the limitations of the limitations, by the limitations of the limitations of the limitations, the limitations of the limitations of the limitations of the limitations of the limitations, the limitations of the limitations of the limitations of the limitations of the limitations of the limitations, the limitations of the limitations of the limitations of the limitations of the limitations of the limitations, the limitations of the limitations of the limitations of the limitations of the limitations of the limitations of the limitations of the limitations of the limitations of the limitations of the limitations of the limitations, the limitations of the limitations of the limitations of the limitations of the limitations of the limitations of the limitations, the limitations of the limitations of the limitations of the limitations of the limitations of the limitations of the limitations of the limitations of the limitations of the limitations of the"}, {"heading": "5 Analysis", "text": "We assume that it is the following regularity condition, which we consider to be inessential for the most important intuitions. If we start with the following regularity condition, we are either insufficiently able to confuse the learners, which makes learning a problem, while we do not make parametric assumptions about emissions, we will consider the emission of pdfs as a linear criterion of independence. We use the H\u00f6lder class, H1 (\u03b2, L), which is used as the standard in the non-parameterization of emissions."}, {"heading": "5.1 Some Perturbation Theory Results for C/Q-matrices", "text": "The first result is an analogue to Weyl's theorem, which limits the difference of the singular values in relation to the operator norm of the disorder. Weyl's theorem has been investigated for general operators [23] and cmatrices [6]. We have given a version in Lemma 21 of Appendix B. In addition, we must also tie the difference in the singular vectors and the pseudo-inverses of truth and estimation. To our knowledge, these results are not yet known. To this end, we specify the following results. \u03c3k (A) denotes the highest singular value of a c / q matrix A.Lemma 6 (simplified Wedin's sine theorem for cmatrices). Let us leave A, A, E, R [0,1] \u00d7 [0,1] \u00d7 [0,1], where A = A + E and rank (A) = m. Let us leave U, U, B] m the first singular theorem for matrices."}, {"heading": "5.2 Concentration Bound for the Kernel Density Estimator", "text": "s assumptions for O, the cores used in KDE must be in order \u03b2. (5) Such cores can be constructed using legendary polynomials [17]. Since N i.i.d samples from a d dimensional density f, where d, 2, 3} and f, P1, P321}, for the appropriate decisions of bandwidths h1, h321}, the KDE f, [P, P, 21, P, 321} refer to the d dimensional density f, where d, 2, 3} and f, [P1, P321}."}, {"heading": "6 Experiments", "text": "Dre rf\u00fc nde eeisrVnlrteeerteerr\u00fc rf\u00fc ide rf\u00fc the rf\u00fc-eaeJnlhsrteeaJngnea-eaJnlrteeaeaJnlrh-eaJnlhsrteeSrteeeeSrteeSrlteeSrmnlhc-eaeSrlrrrrteeaeaeSrlhc-nlrteeSrteeSrteeSrteeeSrteeSrteeSrteeSrteeSrteeSrteeSrteeSrteeSrteeSrrreeeaeSrf\u00fc-eSrrrrf\u00fc-eSrlllteeSrteeSrteec-SrteeSrteeSrteeSrteec-SrteeSrrteeSrrec-SrteeaeSrteerec-SrteeaeSrf\u00fc"}, {"heading": "7 Conclusion", "text": "We have proposed and investigated a method for estimating the observable representation of a Hidden Markov model, whose emission probabilities are smooth non-parametric densities. We derive a limitation of sample complexity for our method. Although our algorithm is similar to existing methods for discrete models, many of the ideas generalizing it to the non-parametric setting are new. Compared to other methods, the proposed approach exhibits some desirable properties: We can restore the joints / conditional densities, our theoretical results are in terms of more interpretable metrics, the method exceeds baselines and is orders of magnitude faster to train. In this exposure, we focused only on one-dimensional observations. The multidimensional case is handled by extending the above ideas and technologies to multivariate functions. Our algorithm and analysis perform up to d-dimensional adjustment, mutatis mutdis. However, the concern is that we have to perform this method in a highly effective way, while we have different technology / q."}, {"heading": "Acknowledgements", "text": "The authors thank Alex Townsend, Arthur Gretton and Ahmed Hefny for the helpful discussions."}, {"heading": "A A Quart-sized Review of Continuous Linear Algebra", "text": "In this section we present sequential analogies of matrices and their factorisations. We offer only a brief overview of what is needed in this exposition. Chapters 3 and 4 of Townsend [6] contain an answer to the following questions: A matrix F = > Q = > Q (Q = > Q) is a m \u00b7 n array of numbers in which F (i, j) is the entry in row i, column j. We will also consider cases in which either m or n is infinite. A column qmatrix (quasi-matrix) Q [a] R [a, b] \u00b7 m is a collection of m functions based on [a, b] where the row index is continuous and the column index discrete. Write Q = [q1, qm] where qj: [a] is the j th function, Q (y, j) denotes the value of the j function."}, {"heading": "B Some Perturbation Theory Results for Continuous Linear Algebra", "text": "We recommend that readers who are not familiar with continuous linear algebra first read the evaluation in Appendix A = > Q = > Q = = 11x. In this section, a matrix (including q / cmatrices) is presented to their eigenvalues. Similarly, we refer to convergence in the operator norm. For all theories, we follow the Stewart and Sun [26] template for matrix cases and therefore try to stick to their notations. Before proceeding, we set the \"cmatrix\" I [0,1] to [0, 1]. For all theories, we follow the template provided by Stewart and Sun [26] for the matrix arguments."}, {"heading": "C Concentration of Kernel Density Estimation", "text": "# # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # #"}, {"heading": "D Analysis of the Spectral Algorithm", "text": "Our proof is a rough generalization of the analysis in Hsu et al. [> >]. Following its submission, we use a few technical terms. We mainly focus on the cases in which our analysis is different. \u2212 b \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 f \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 21) We start with a series of terms - p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p."}, {"heading": "E Addendum to Experiments", "text": "Details on synthetic experiments: Figure 3 shows the emission probabilities used in our synthetic experiments. For the transition matrices, we scanned the entries of the matrix from a U (0, 1) distribution and then calculated the columns to the sum 1. In our implementation, we use a Gaussian kernel for the KDE, which is of order \u03b2 = 2. While nuclei of higher order can be constructed with legendary polynomials [17], the Gaussian kernel was more robust in practice. The bandwidth for the kernel was selected by cross-validation for density estimation.Details on real data sets: Here, we first estimate the model parameters using the training sequence. For a test sequence x1: n, we predict that xt + 1 on the previous x1: t is due to t = 1: n.1. Internet traffic. Training sequence length: 10, 000. Test sequence length: 10. 2. Laser generation. Training sequence length: 10, 1. Test length: 100 000."}], "references": [{"title": "A Tutorial on Hidden Markov Models and Selected Applications in Speech Recognition", "author": ["Lawrence R. Rabiner"], "venue": "In Proceedings of the IEEE,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 1989}, {"title": "A Spectral Algorithm for Learning Hidden Markov Models", "author": ["Daniel J. Hsu", "Sham M. Kakade", "Tong Zhang"], "venue": "In COLT,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2009}, {"title": "A Method of Moments for Mixture Models and Hidden Markov Models", "author": ["Animashree Anandkumar", "Daniel Hsu", "Sham M Kakade"], "venue": "arXiv preprint arXiv:1203.0683,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2012}, {"title": "Reduced-Rank Hidden Markov Models", "author": ["Sajid M. Siddiqi", "Byron Boots", "Geoffrey J. Gordon"], "venue": "In AISTATS,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2010}, {"title": "Continuous analogues of matrix factorizations", "author": ["Alex Townsend", "Lloyd N Trefethen"], "venue": "In Proc. R. Soc. A,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2015}, {"title": "Computing with Functions in Two Dimensions", "author": ["Alex Townsend"], "venue": "PhD thesis, University of Oxford,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2014}, {"title": "An extension of chebfun to two dimensions", "author": ["Townsend", "Alex", "Trefethen", "Lloyd N"], "venue": "SIAM J. Scientific Computing,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2013}, {"title": "Predictive representations of state", "author": ["Michael L Littman", "Richard S Sutton", "Satinder P Singh"], "venue": "In NIPS,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2001}, {"title": "Maximum likelihood from incomplete data via the EM algorithm. JOURNAL OF THE ROYAL STATISTICAL SOCIETY", "author": ["A.P. Dempster", "N.M. Laird", "D.B. Rubin"], "venue": "SERIES B,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 1977}, {"title": "Hidden Markov models and the Baum-Welch algorithm", "author": ["Lloyd R Welch"], "venue": "IEEE Information Theory Society Newsletter,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2003}, {"title": "An EM-like algorithm for semi-and nonparametric estimation in multivariate mixtures", "author": ["Tatiana Benaglia", "Didier Chauveau", "David R Hunter"], "venue": "Journal of Computational and Graphical Statistics,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2009}, {"title": "Hilbert space embeddings of hidden markov models", "author": ["Le Song", "Byron Boots", "Sajid M Siddiqi", "Geoffrey J Gordon", "Alex Smola"], "venue": "In ICML,", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2010}, {"title": "Nonparametric Estimation of Multi-View Latent Variable Models", "author": ["Le Song", "Animashree Anandkumar", "Bo Dai", "Bo Xie"], "venue": "In Proceedings of the 31st International Conference on Machine Learning", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2014}, {"title": "Observable operator models for discrete stochastic time series", "author": ["Herbert Jaeger"], "venue": "Neural Computation,", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2000}, {"title": "Introduction to Nonparametric Estimation", "author": ["Alexandre B. Tsybakov"], "venue": null, "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2008}, {"title": "Chebyshev polynomials in numerical analysis", "author": ["L. Fox", "I.B. Parker"], "venue": "Oxford U.P. cop.,", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 1968}, {"title": "Approximation Theory and Approximation Practice", "author": ["Lloyd N. Trefethen"], "venue": "Society for Industrial and Applied Mathematics,", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2012}, {"title": "Estimation of integral functionals of a density", "author": ["Lucien Birg\u00e9", "Pascal Massart"], "venue": "Ann. of Stat.,", "citeRegEx": "20", "shortCiteRegEx": "20", "year": 1995}, {"title": "Nonparametric Von Mises Estimators for Entropies, Divergences and Mutual Informations", "author": ["Kirthevasan Kandasamy", "Akshay Krishnamurthy", "Barnab\u00e1s P\u00f3czos", "Larry Wasserman", "James Robins"], "venue": "In NIPS,", "citeRegEx": "22", "shortCiteRegEx": "22", "year": 2015}, {"title": "Weyl\u2019s theorem for operator matrices", "author": ["Woo Young Lee"], "venue": "Integral Equations and Operator Theory,", "citeRegEx": "23", "shortCiteRegEx": "23", "year": 1998}, {"title": "Forest Density Estimation", "author": ["Han Liu", "Min Xu", "Haijie Gu", "Anupam Gupta", "John D. Lafferty", "Larry A. Wasserman"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "24", "shortCiteRegEx": "24", "year": 2011}, {"title": "Rates of strong uniform consistency for multivariate kernel density estimators", "author": ["Evarist Gin\u00e9", "Armelle Guillou"], "venue": "In Annales de l\u2019IHP Probabilite\u0301s et statistiques,", "citeRegEx": "25", "shortCiteRegEx": "25", "year": 2002}, {"title": "Matrix Perturbation Theory", "author": ["G.W. Stewart", "Ji-guang Sun"], "venue": null, "citeRegEx": "26", "shortCiteRegEx": "26", "year": 1990}, {"title": "Wide area traffic: the failure of Poisson modeling", "author": ["Vern Paxson", "Sally Floyd"], "venue": "IEEE/ACM Transactions on Networking,", "citeRegEx": "27", "shortCiteRegEx": "27", "year": 1995}, {"title": "Dimensions and entropies of chaotic intensity pulsations in a single-mode far-infrared NH 3 laser", "author": ["U H\u00fcbner", "NB Abraham", "CO Weiss"], "venue": "Physical Review A,", "citeRegEx": "28", "shortCiteRegEx": "28", "year": 1989}, {"title": "Chebfun to three dimensions", "author": ["B. Hashemi", "L.N. Trefethen"], "venue": "In preparation,", "citeRegEx": "30", "shortCiteRegEx": "30", "year": 2016}, {"title": "Predictive State Representations: A New Theory for Modeling Dynamical Systems", "author": ["Satinder Singh", "Michael R. James", "Matthew R. Rudary"], "venue": "In UAI,", "citeRegEx": "31", "shortCiteRegEx": "31", "year": 2004}], "referenceMentions": [{"referenceID": 0, "context": "Hidden Markov models (HMMs) [1] are one of the most popular statistical models for analyzing time series data in various application domains such as speech recognition, medicine, and meteorology.", "startOffset": 28, "endOffset": 31}, {"referenceID": 1, "context": "Recently, spectral methods for estimating parametric latent variable models have gained immense popularity as a viable alternative to the Expectation Maximisation (EM) procedure [2\u20134].", "startOffset": 178, "endOffset": 183}, {"referenceID": 2, "context": "Recently, spectral methods for estimating parametric latent variable models have gained immense popularity as a viable alternative to the Expectation Maximisation (EM) procedure [2\u20134].", "startOffset": 178, "endOffset": 183}, {"referenceID": 3, "context": "Recently, spectral methods for estimating parametric latent variable models have gained immense popularity as a viable alternative to the Expectation Maximisation (EM) procedure [2\u20134].", "startOffset": 178, "endOffset": 183}, {"referenceID": 1, "context": "In the case of discrete HMMs [2], these moments correspond exactly to the joint probabilities of the observations in the sequence.", "startOffset": 29, "endOffset": 32}, {"referenceID": 4, "context": "Our methods leverage some recent advances in continuous linear algebra [5, 6] which views two-dimensional functions as continuous analogues of matrices.", "startOffset": 71, "endOffset": 77}, {"referenceID": 5, "context": "Our methods leverage some recent advances in continuous linear algebra [5, 6] which views two-dimensional functions as continuous analogues of matrices.", "startOffset": 71, "endOffset": 77}, {"referenceID": 6, "context": "Chebyshev polynomial approximations enable efficient computation of algebraic operations on these continuous objects [7, 8].", "startOffset": 117, "endOffset": 123}, {"referenceID": 7, "context": "While we focus on HMMs in this exposition, we believe that the ideas presented in this paper can be easily generalised to estimating other latent variable models and predictive state representations [9] with nonparametric observations using approaches developed by Anandkumar et al.", "startOffset": 199, "endOffset": 202}, {"referenceID": 2, "context": "[3].", "startOffset": 0, "endOffset": 3}, {"referenceID": 8, "context": "Related Work: Parametric HMMs are usually estimated using maximum likelihood principle via EM techniques [10] such as the Baum-Welch procedure [11].", "startOffset": 105, "endOffset": 109}, {"referenceID": 9, "context": "Related Work: Parametric HMMs are usually estimated using maximum likelihood principle via EM techniques [10] such as the Baum-Welch procedure [11].", "startOffset": 143, "endOffset": 147}, {"referenceID": 1, "context": "[2] who showed that discrete HMMs can be learned efficiently, under certain conditions.", "startOffset": 0, "endOffset": 3}, {"referenceID": 3, "context": "[4] show that the same algorithm works under slightly more general assumptions.", "startOffset": 0, "endOffset": 3}, {"referenceID": 2, "context": "[3] proposed a spectral algorithm for estimating more general latent variable models with parametric observations via a moment matching technique.", "startOffset": 0, "endOffset": 3}, {"referenceID": 10, "context": "A commonly used heuristic is the nonparametric EM [12] which lacks theoretical underpinnings.", "startOffset": 50, "endOffset": 54}, {"referenceID": 3, "context": "[4] proposed a heuristic based on kernel smoothing, with no theoretical justification, to modify the discrete algorithm for continuous observations.", "startOffset": 0, "endOffset": 3}, {"referenceID": 11, "context": "[14, 15] developed an RKHS-based procedure for estimating the Hilbert space embedding of an HMM.", "startOffset": 0, "endOffset": 8}, {"referenceID": 12, "context": "[14, 15] developed an RKHS-based procedure for estimating the Hilbert space embedding of an HMM.", "startOffset": 0, "endOffset": 8}, {"referenceID": 4, "context": "Both sections are based on [5, 6].", "startOffset": 27, "endOffset": 33}, {"referenceID": 5, "context": "Both sections are based on [5, 6].", "startOffset": 27, "endOffset": 33}, {"referenceID": 0, "context": "Without loss of generality, we take2 X = [0, 1].", "startOffset": 41, "endOffset": 47}, {"referenceID": 1, "context": "It is well known [2, 16] that the joint probability density of the sequence x1:t can be computed via p(x1:t) = 1mA(xt:1)\u03c0.", "startOffset": 17, "endOffset": 24}, {"referenceID": 13, "context": "It is well known [2, 16] that the joint probability density of the sequence x1:t can be computed via p(x1:t) = 1mA(xt:1)\u03c0.", "startOffset": 17, "endOffset": 24}, {"referenceID": 13, "context": "Observable Representation: The observable representation is a description of an HMM in terms of quantities that depend on the observations [16].", "startOffset": 139, "endOffset": 143}, {"referenceID": 0, "context": "We will find it useful to view both P21, P3x1 \u2208 R[0,1]\u00d7[0,1] as cmatrices.", "startOffset": 49, "endOffset": 54}, {"referenceID": 0, "context": "We will find it useful to view both P21, P3x1 \u2208 R[0,1]\u00d7[0,1] as cmatrices.", "startOffset": 55, "endOffset": 60}, {"referenceID": 0, "context": "We will also need an additional qmatrix U \u2208 R[0,1]\u00d7m such that U>O \u2208 Rm\u00d7m is invertible.", "startOffset": 45, "endOffset": 50}, {"referenceID": 0, "context": "Given one such U , the observable representation of an HMM is described by the parameters b1, b\u221e \u2208 R and B : [0, 1]\u2192 Rm\u00d7m, b1 = U P1, b\u221e = (P > 21U) P1, B(x) = (U P3x1)(U P21) \u2020 (1) As before, for a sequence, xt:1 = {xt, .", "startOffset": 109, "endOffset": 115}, {"referenceID": 0, "context": "B(x) = (U>O)A(x)(U>O)\u22121 \u2200x \u2208 [0, 1].", "startOffset": 29, "endOffset": 35}, {"referenceID": 1, "context": "[2].", "startOffset": 0, "endOffset": 3}, {"referenceID": 14, "context": "While there are several techniques [17], we use kernel density estimation (KDE) since it is easy to analyse and works well in practice.", "startOffset": 35, "endOffset": 39}, {"referenceID": 0, "context": "Here K : [0, 1] \u2192 R is a symmetric function called a smoothing kernel and satisfies (at the very least) \u222b 1 0 K(s)ds = 1, \u222b 1 0 sK(s)ds = 0.", "startOffset": 9, "endOffset": 15}, {"referenceID": 0, "context": "Let \u00db \u2208 R[0,1]\u00d7m be the first m left singular vectors of P\u030221.", "startOffset": 9, "endOffset": 14}, {"referenceID": 1, "context": "[2], the SVD, pseudoinverses and multiplications are with q/c-matrices.", "startOffset": 0, "endOffset": 3}, {"referenceID": 15, "context": "Chebyshev polynomials is a family of orthogonal polynomials on compact intervals, known to be an excellent approximator of one-dimensional functions [18, 19].", "startOffset": 149, "endOffset": 157}, {"referenceID": 16, "context": "Chebyshev polynomials is a family of orthogonal polynomials on compact intervals, known to be an excellent approximator of one-dimensional functions [18, 19].", "startOffset": 149, "endOffset": 157}, {"referenceID": 4, "context": "A recent line of work [5, 8] has extended the Chebyshev technology to two dimensional functions enabling the mentioned operations and factorisations such as QR, LU and SVD [6, Sections 4.", "startOffset": 22, "endOffset": 28}, {"referenceID": 6, "context": "A recent line of work [5, 8] has extended the Chebyshev technology to two dimensional functions enabling the mentioned operations and factorisations such as QR, LU and SVD [6, Sections 4.", "startOffset": 22, "endOffset": 28}, {"referenceID": 1, "context": "Following [2, 4, 14] we assume i.", "startOffset": 10, "endOffset": 20}, {"referenceID": 3, "context": "Following [2, 4, 14] we assume i.", "startOffset": 10, "endOffset": 20}, {"referenceID": 11, "context": "Following [2, 4, 14] we assume i.", "startOffset": 10, "endOffset": 20}, {"referenceID": 0, "context": "T \u2208 Rm\u00d7m and O \u2208 R[0,1]\u00d7m are of rank m.", "startOffset": 18, "endOffset": 23}, {"referenceID": 0, "context": "for all \u03b1 \u2264 b\u03b2c, j \u2208 [m], s, t \u2208 [0, 1] \u2223\u2223\u2223\u2223d\u03b1Oj(s) ds\u03b1 \u2212 dOj(t) dt\u03b1 \u2223\u2223\u2223\u2223 \u2264 L|s\u2212 t|\u03b2\u2212|\u03b1|.", "startOffset": 33, "endOffset": 39}, {"referenceID": 3, "context": "[4] show that the discrete spectral algorithm works under a slightly more general setting.", "startOffset": 0, "endOffset": 3}, {"referenceID": 1, "context": "[2], whose sample complexity bound4 is N & m \u03ba(O) 2", "startOffset": 0, "endOffset": 3}, {"referenceID": 17, "context": "While we do not have any lower bounds, given the current understanding of the difficulty of various nonparametric tasks [20\u201322], we think our bound might be unimprovable.", "startOffset": 120, "endOffset": 127}, {"referenceID": 18, "context": "While we do not have any lower bounds, given the current understanding of the difficulty of various nonparametric tasks [20\u201322], we think our bound might be unimprovable.", "startOffset": 120, "endOffset": 127}, {"referenceID": 0, "context": "This is due to the fact that we want the KDE to concentrate around its expectation in L over [0, 1], instead of just point-wise.", "startOffset": 93, "endOffset": 99}, {"referenceID": 19, "context": "Weyl\u2019s theorem has been studied for general operators [23] and cmatrices [6].", "startOffset": 54, "endOffset": 58}, {"referenceID": 5, "context": "Weyl\u2019s theorem has been studied for general operators [23] and cmatrices [6].", "startOffset": 73, "endOffset": 76}, {"referenceID": 0, "context": "Let A, \u00c3, E \u2208 R[0,1]\u00d7[0,1] where \u00c3 = A+ E and rank(A) = m.", "startOffset": 15, "endOffset": 20}, {"referenceID": 0, "context": "Let A, \u00c3, E \u2208 R[0,1]\u00d7[0,1] where \u00c3 = A+ E and rank(A) = m.", "startOffset": 21, "endOffset": 26}, {"referenceID": 14, "context": "Such kernels can be constructed using Legendre polynomials [17].", "startOffset": 59, "endOffset": 63}, {"referenceID": 1, "context": "[2] provide a more refined bound but we use this form to simplify the comparison.", "startOffset": 0, "endOffset": 3}, {"referenceID": 20, "context": "This slow convergence is also observed in similar concentration bounds for the KDE [24, 25].", "startOffset": 83, "endOffset": 91}, {"referenceID": 21, "context": "This slow convergence is also observed in similar concentration bounds for the KDE [24, 25].", "startOffset": 83, "endOffset": 91}, {"referenceID": 22, "context": "A note on the Proofs: For Lemmas 6, 7 we follow the matrix proof in Stewart and Sun [26] and derive several intermediate results for c/q-matrices in the process.", "startOffset": 84, "endOffset": 88}, {"referenceID": 14, "context": "The main challenge with the KDE concentration result is that we want an L bound \u2013 so usual techniques (such as McDiarmid\u2019s [13, 17]) do not apply.", "startOffset": 123, "endOffset": 131}, {"referenceID": 21, "context": "We use a technical lemma from Gin\u00e9 and Guillou [25] which allows us to bound the L error in terms of the VC characteristics of the class of functions induced by an i.", "startOffset": 47, "endOffset": 51}, {"referenceID": 1, "context": "[2].", "startOffset": 0, "endOffset": 3}, {"referenceID": 1, "context": "NP-HMM-BIN: A naive baseline where we bin the space into n intervals and use the discrete spectral algorithm [2] with n states.", "startOffset": 109, "endOffset": 112}, {"referenceID": 10, "context": "NP-HMM-EM: The Nonparametric EM heuristic of [12].", "startOffset": 45, "endOffset": 49}, {"referenceID": 11, "context": "NP-HMM-HSE: The Hilbert space embedding method of [14].", "startOffset": 50, "endOffset": 54}, {"referenceID": 3, "context": "We could not include the method of [4] in our comparisons since their code was not available and their method isn\u2019t straightforward to implement.", "startOffset": 35, "endOffset": 38}, {"referenceID": 23, "context": "Real Datasets: We compare all the above methods (except NP-HMM-EM which was too slow) on prediction error on 3 real datasets: internet traffic [27], laser generation [28] and sleep data [29].", "startOffset": 143, "endOffset": 147}, {"referenceID": 24, "context": "Real Datasets: We compare all the above methods (except NP-HMM-EM which was too slow) on prediction error on 3 real datasets: internet traffic [27], laser generation [28] and sleep data [29].", "startOffset": 166, "endOffset": 170}, {"referenceID": 6, "context": "That said, some recent advances in this direction are promising [8, 30].", "startOffset": 64, "endOffset": 71}, {"referenceID": 25, "context": "That said, some recent advances in this direction are promising [8, 30].", "startOffset": 64, "endOffset": 71}, {"referenceID": 26, "context": "Recent advances in spectral methods for estimating parametric predictive state representations [31], mixture models [3] and other latent variable models [32] can be generalised to the nonparamatric setting using our ideas.", "startOffset": 95, "endOffset": 99}, {"referenceID": 2, "context": "Recent advances in spectral methods for estimating parametric predictive state representations [31], mixture models [3] and other latent variable models [32] can be generalised to the nonparamatric setting using our ideas.", "startOffset": 116, "endOffset": 119}], "year": 2016, "abstractText": "Recently, there has been a surge of interest in using spectral methods for estimating latent variable models. However, it is usually assumed that the distribution of the observations conditioned on the latent variables is either discrete or belongs to a parametric family. In this paper, we study the estimation of an m-state hidden Markov model (HMM) with only smoothness assumptions, such as H\u00f6lderian conditions, on the emission densities. By leveraging some recent advances in continuous linear algebra and numerical analysis, we develop a computationally efficient spectral algorithm for learning nonparametric HMMs. Our technique is based on computing an SVD on nonparametric estimates of density functions by viewing them as continuous matrices. We derive sample complexity bounds via concentration results for nonparametric density estimation and novel perturbation theory results for continuous matrices. We implement our method using Chebyshev polynomial approximations. Our method is competitive with other baselines on synthetic and real problems and is also very computationally efficient.", "creator": "LaTeX with hyperref package"}}}