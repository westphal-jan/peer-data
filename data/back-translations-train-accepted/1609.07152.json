{"id": "1609.07152", "review": {"conference": "ICML", "VERSION": "v1", "DATE_OF_SUBMISSION": "22-Sep-2016", "title": "Input Convex Neural Networks", "abstract": "This paper presents the input convex neural network architecture. These are scalar-valued (potentially deep) neural networks with constraints on the network parameters such that the output of the network is a convex function of (some of) the inputs. The networks allow for efficient inference via optimization over some inputs to the network given others, and can be applied to settings including structured prediction, data imputation, reinforcement learning, and others. In this paper we lay the basic groundwork for these models, proposing methods for inference, optimization and learning, and analyze their representational power. We show that many existing neural network architectures can be made input-convex with only minor modification, and develop specialized optimization algorithms tailored to this setting. Finally, we highlight the performance of the methods on multi-label prediction, image completion, and reinforcement learning problems, where we show improvement over the existing state of the art in many cases.", "histories": [["v1", "Thu, 22 Sep 2016 20:10:57 GMT  (2074kb,D)", "http://arxiv.org/abs/1609.07152v1", null], ["v2", "Thu, 13 Oct 2016 19:46:58 GMT  (2167kb,D)", "http://arxiv.org/abs/1609.07152v2", null], ["v3", "Wed, 14 Jun 2017 17:59:12 GMT  (2532kb,D)", "http://arxiv.org/abs/1609.07152v3", "ICML 2017"]], "reviews": [], "SUBJECTS": "cs.LG math.OC", "authors": ["brandon amos", "lei xu", "j zico kolter"], "accepted": true, "id": "1609.07152"}, "pdf": {"name": "1609.07152.pdf", "metadata": {"source": "CRF", "title": "Input Convex Neural Networks", "authors": ["Brandon Amos", "Lei Xu"], "emails": ["bamos@cs.cmu.edu", "leonard.xu.thu@gmail.com", "zkolter@cs.cmu.edu"], "sections": [{"heading": "1 Introduction", "text": "In this paper, we propose a new neural network architecture that we call the input problem Convex Neural Network (ICNN). In the simplest case, these are scalar estimated neural networks f (x, y; \u03b8), where x and y comprise the inputs to the network that are built in such a way that the network is convex in (a subset of) inputs that we can use globally and efficiently (because the problem is convex), we solve the optimization problems via the convex inputs to the network that give a fixed value to other inputs. That is, given some fixed x (and possibly some fixed elements of y) that we can globally and efficiently solve the optimization problems that we have f (x, y). (1) We are working while the author is at Carnegie Mellon University. 1We emphasize the term \"input convex,\" because machine learning is typically switched to convex."}, {"heading": "2 Background and related work", "text": "In fact, most of them are able to behave the way they do. (...) Most of them are able to behave the way they do. (...) Most of them are able to behave the way they do. (...) Most of them are able to behave the way they do. (...) Most of them are able to behave the way they do. (...) Most of them are able to behave the way they do. (...) Most of them are able to behave the way they do. (...) Most of them are able to behave the way they do. (...) Most of them are able to behave the way they behave. (...) Most of them are able to behave the way they behave themselves. (...)"}, {"heading": "3 Convex neural network architectures", "text": "Here we formally present the various ICNN architectures and demonstrate their convexity properties in the face of certain parameter space constraints. Our main claim is that the class of (complete and partial) convex input models is rich and allows us to capture complex common models via the input to a network."}, {"heading": "3.1 Fully input convex neural networks", "text": "To begin with, let us consider a fully convex, k-layer, fully connected ICNN, shown in Figure 1. This model defines a neural network above the input y (i.e., without any x-term in this function) using the architectural terms zi + 1 = gi (W (z) i zi + W (y) i + bi), i = 0,., k \u2212 1f (y; \u03b8) = zk (7), where zi denotes the layer activations (with z0, W (z) 0 \u2261 0), \u03b8 = {W (y) 0: k \u2212 1: k \u2212 1, b0: k \u2212 1} are the parameters, and gi are non-linear activation functions. The central result of the convex exposure of the network is the following: Statement 1. The function f is convex in y, assuming that all W (z) 1: k \u2212 1 are not negative, and all functions gi are convex and non-cretionable."}, {"heading": "3.2 Partially input convex architectures", "text": "The FICNN provides a common convexity across all input to the function, which may in fact be a restriction to the permissible class of models; in fact, neural networks derive much of their power from the fact that they are general functional approximation mechanisms. In this section, we propose an extension to pure FICNN, where the neural network (PICNN) is used to create a common model via an input and output sample space and only convexity via the outputs. In this section, we propose an extension to pure FICNN, which is partly input convex neural network (PICNN), which is convex via only some inputs to the network. In fact, when we refer to ICNs, we will mean these partially convex functions. As we will show that these networks generalize both traditional feedforward networks and FICNs, providing significant representative advantages."}, {"heading": "3.3 Convolutional architectures", "text": "Coils are important for many visually structured tasks. We have omitted coils to obtain the previous ICNN notation by using matrix-vector operations. ICNNs can be similarly produced with coils, by treating the convolution as a linear operator. The construction of Convolutionary Layers in ICNs depends on the type of input and output space. If the input and output space are similarly structured (e.g. spatially), the jth characteristic sketch of a Convolutionary PICNN layer i byzji + 1 = gi (zi \u0445 W (z) i, j + (Sx) \u0445 W (x) i, j + (Sy) \u0445 W (y) i) can be defined, where the convolution cores W are equal in size and S scales the input and output size so that they have the same size as the previous characteristic sketch, and if we leave out some of the product specifications above, the product specifications may appear."}, {"heading": "4 Inference in ICNNs", "text": "Unlike traditional neural feedback networks, where predictions are generated by a single forward move in the network, prediction (which we generally call a conclusion here) requires solving the convex optimization problem, which minimizes y-Y f (x, y; \u03b8). (11) While the resulting tasks are convex optimization problems (and are therefore \"easy\" to solve in a certain sense), in practice it is still a matter of solving a potentially very complex optimization problem. Therefore, we are discussing several approaches to (sometimes approximate) solving these optimization problems. We note that in practice we can obtain reasonably precise solutions in many areas by using a method that involves only a small number of forward and backward steps through the network, and thus has a complexity that is at best a constant factor in practice that is worse than that for feedback networks."}, {"heading": "4.1 Exact inference in ICNNs", "text": "Although it is not a practical approach to solving the optimization problems, we first highlight the fact that the consequence problem for the networks presented above (where the nonlinear ones are either ReLU or linear units) can be posed as a linear program. In particular, taking into account the FICNN network in (7), the optimization problem can be written as y, z1,..., zk zksubject to zi + 1 \u2265 W (z) i zi + W (y) i + bi, i = 0.., k \u2212 1zi \u2265 0, i = 1,.,., k \u2212 1. This problem exactly replicates the equations of the FICNN structures, except that we have replaced ReLU and the equality constraint between layers with a positivity optimization on the Ziterms and an inequality. However, because we minimize the last zk term, and because any inequality convex is."}, {"heading": "4.2 Approximate inference in ICNNs", "text": "That is, we begin with a series of approaches to optimizing these nets (ideally, however, those that still exploit the convexity of the resulting problem), focusing specifically on gradient-based approaches that use the fact that we can easily calculate the gradient of an ICNN in terms of its input factors. (11) The simplest gradient-based method for solving (11) is simple (projected sub-) gradient descent, or modifications such as those that use a dynamic term to calculate the gradient function of the network in terms of its parameters. (11) The simplest gradient-based method for solving (11) is simple (projected sub-) gradient descent, or modifications such as those that use a dynamic term. (1964, Rumelhart et al, 1988) or spectral step-size modifications (1988, Barzai, 2000, and Borilgin]."}, {"heading": "4.3 Approximate inference via the bundle entropy method", "text": "In order to overcome these challenges, we have developed a new optimization algorithm for this domain, which we assume to be Y = [0, 1] n (other upper or lower limits can be reached by scaling); the method is also easily extendable to the setting in which elements of Y belong to a higher dimensional probability simplex. (19) In this setting, we consider adding an additional \"barrier\" function for optimization in the form of negative entropy \u2212 H (y), whereas H (y) = \u2212 n, which belong to a higher probability simplex. (19) In other words, we want to problematize optimization instead yf (x, y) \u2212 H (y), where we can additionally add a scaling between f and entropy if we want to problematize practice."}, {"heading": "5 Learning ICNNs", "text": "In general, ICNN learning shapes the energy function of the object in such a way that it delivers the desired values when optimizing via the relevant inputs. That is, for a particular input-output pair (x, y?), our goal is to find ICNN parameters that are so complex that we use the notation f for the entire section to denote the combination of the function of the neuronal network and the regularization term as \u2212 H (y) when it is included, i.e., f (x, y; \u03b8) = f (x, y) \u2212 H (y). (28) Although we are only discussing the regularization of entropy in this work, we emphasize that other regularizers are also possible. Depending on the setting, there are several different approaches that we can apply to ensure that the ICNN achieves the desired goals, and we will consider three approaches below: direct functional adaptation, maximum margins, and differentiation."}, {"heading": "5.1 Direct functional fitting", "text": "Although it is simple enough that there is no need for substantive discussion, we should first point out that in some areas we do not need a specialized method for adapting ICNNs, but can use existing approaches that directly fit the ICNN function, such as the Q-Learning setting, where the aim of our algorithm is to fit Q (s, a) = \u2212 f (s, a) so that Q (s, a) = R (s, a) + \u03b3E [argmaxa \u2032 Q (s \u2032, a \u2032)]]]. (29) In view of some tuples observed (s, a, r, s \u00b2), Q-Learning performs the updating processes: = \u03b8 \u2212 \u03b1 (Q (s, a) \u2212 \u03b3 argmaxa \u2032 Q (s \u2032, a \u2032)) and vice versa."}, {"heading": "5.2 Max-margin structured prediction", "text": "In the more traditional prediction environment, where we do not aim to directly adjust the energy function, but to adapt the predictions made by the system to specific results, there are various ways to learn the ICNN parameters. Such a method is based on the maximum range of structured prediction possibilities [Tsochantaridis et al., 2005, Taskar et al., 2005]. Given some training examples (x, y?), we would like this example to have a common energy that is lower than all other possible values for the margin. That is, we want the function f to satisfy the limitation (x, y?). In view of some training examples (x, y?). (x, y?) Unfortunately, these conditions can be trivially appropriate by setting a constant f? (although the entropy concept slightly alleviates this problem, we can still select an approximately constant function)."}, {"heading": "5.3 Argmin differentiation", "text": "In our last proposed approach, argmin differentiation, we propose to minimize a loss function between the actual results and the results predicted by our model, these predictions themselves being the result of an optimization problem. In order to simplify the notation, we will consider the case in which the approximate solution to the problem is achieved by the entropy method described above. (Gy + h) | Gy + h \u00b2 argmin y \u00b2 argmin y \u00b2 argmin y \u00b2 argmin y \u00b2 argmin y \u00b2 s \u00b2 argmin y \u00b2 s \u00b2 argmin y \u00b2 s \u00b2 s \u00b2 s argmin y \u00b2 s \u00b2 s \u00b2 s argmin y \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s (x, y \u00b2 s).s The safe minimization of the results from operating the entropy bundling method, especially with the final iteration of the method (x, y), our goal is to calculate the parameters in relation to the NN."}, {"heading": "6 Experiments", "text": "Our experiments examine the representational power of ICNs to better understand the interplay between the limitations and accuracy of the model. In this section, we present our results on multi-label classification in the Bibtex dataset [Katakis et al., 2008], image completion using the Olivetti facial dataset [Samaria and Harter, 1994], and continuous learning in the OpenAI gym [Brockman et al., 2016]. Our multi-label classification experiments use a fully networked PICNN. Our image completion experiments use a DQN-style revolutionary PICNN, where the entrance room X is the right side of the image and the output room Y is the left side of the image. Our amplification experiments use a fully networked PICNN where the input room X lies above the possible current state, and the output room Y is the output area of the continuous output code of the end area of our 1995 actuation space for our Oliv1 / actuation point."}, {"heading": "6.1 Synthetic classification examples", "text": "We start with a simple example to illustrate the classification performance of a two-tiered FICNN and PICNN for two-dimensional binary classification tasks from the scikit-learn toolkit [Pedregosa et al., 2011]. Figure 3 shows the classification performance on the dataset. FICNN's energy function, which is fully convex in X \u00d7 Y, is able to capture complex but sometimes restrictive decision boundaries. PICNN, which is not convex in X but convex in Y, overcomes these constraints and can capture more complex decision boundaries."}, {"heading": "6.2 Multi-Label Classification", "text": "Next, we examine how ICNNs perform the multi-label classification using the Bibtex dataset and benchmark presented in Katakis et al. [2008], which maps the text classification from an input range X of 1836 Bag-of-Works (binary) characteristics to an output range Y of 159 binary labels. We use the pull test split of 4880 / 2515 from Katakis et al. [2008] and evaluate with the exampleleaveraged (macro) F1 Score. We use the ARFF version of this dataset from Mulan [Tsoumakas et al., 2011]. Our PICNN architecture for multi-label classification uses fully connected layers with ReLU activation functions and batch normalization [Ioffe and Szegedy, 2015] along with the input path [Tsoumakas et al et al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al"}, {"heading": "6.3 Image completion on the Olivetti faces", "text": "As a test of the system on a structured prediction task over a much more complex output space Y, we apply a Convolutionary DQN-like PICNN to face completion on the sklearn version [Pedregosa et al., 2011] of the Olivetti dataset [Samaria and Harter, 1994], which includes 400 64x64 grayscales. Our PICNN is equivalent to the DQN architecture in Mnih et al. [2015] (unless we have added a batch normalization to the input path) and is above (x, y) pairs in which x (32x64) is the left half of the image and y (32x64) is the right half of the image. The input and output paths are: 32x8 conv (striking 4x2), 64x4x4 conv (striking 2x2), 64x3x3 conv, 64x3 conv, 512 fully connected. Any modern architecture we have not been able to use in place of a QN."}, {"heading": "6.4 Continuous Action Reinforcement Learning", "text": "In this experiment, we present our preliminary ICNN results on standard benchmarks in continuous action enhancement studies from the OpenAI Gym [Brockman et al., 2016], which use the MuJoCo physics simulator [Todorov et al., 2012]. Table 1 shows the dimensionality of continuous control benchmarks from the MuJoCo part of the OpenAI gym. As previously introduced, performing continuous state and action spaces requires S \u00d7 A, we model the (negative) Q function, \u2212 Q (s, a;) as ICNN, and choose actions with the convex optimization problem (4). We use Q Learning to optimize the ICNN with the update in (30). For our preliminary results, we focus only on a few environments with small state and action spaces. All our experiments use a PICNN with two fully connected layers, each containing 200 hidden units."}, {"heading": "7 Conclusion and future work", "text": "By incorporating relatively simple constraints into existing network architectures, we can customize very general convex functions and apply the applied optimization as an inference method. As many existing models already fit into this general framework (e.g. CRF models perform optimization over an output range in which parameters are specified by the output of a neural network), the proposed method is an extension in which the entire inference method is \"learned\" together with the network itself, without explicitly building typical structured prediction architectures. In this work, only a small part of the possible applications of this network has been studied, and the networks offer promising directions for many more areas."}], "references": [{"title": "Tensorflow: Large-scale machine learning on heterogeneous distributed systems", "author": ["Mart\u0131n Abadi", "Ashish Agarwal", "Paul Barham", "Eugene Brevdo", "Zhifeng Chen", "Craig Citro", "Greg S Corrado", "Andy Davis", "Jeffrey Dean", "Matthieu Devin"], "venue": "arXiv preprint arXiv:1603.04467,", "citeRegEx": "Abadi et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Abadi et al\\.", "year": 2016}, {"title": "Two-point step size gradient methods", "author": ["Jonathan Barzilai", "Jonathan M Borwein"], "venue": "IMA Journal of Numerical Analysis,", "citeRegEx": "Barzilai and Borwein.,? \\Q1988\\E", "shortCiteRegEx": "Barzilai and Borwein.", "year": 1988}, {"title": "Structured prediction energy networks", "author": ["David Belanger", "Andrew McCallum"], "venue": null, "citeRegEx": "Belanger and McCallum.,? \\Q2015\\E", "shortCiteRegEx": "Belanger and McCallum.", "year": 2015}, {"title": "Globally trained handwritten word recognizer using spatial representation, convolutional neural networks, and hidden markov models", "author": ["Yoshua Bengio", "Yann LeCun", "Donnie Henderson"], "venue": "Advances in neural information processing systems,", "citeRegEx": "Bengio et al\\.,? \\Q1994\\E", "shortCiteRegEx": "Bengio et al\\.", "year": 1994}, {"title": "Projected newton methods for optimization problems with simple constraints", "author": ["Dimitri P Bertsekas"], "venue": "SIAM Journal on control and Optimization,", "citeRegEx": "Bertsekas.,? \\Q1982\\E", "shortCiteRegEx": "Bertsekas.", "year": 1982}, {"title": "Nonmonotone spectral projected gradient methods on convex sets", "author": ["Ernesto G Birgin", "Jos\u00e9 Mario Mart\u0301\u0131nez", "Marcos Raydan"], "venue": "SIAM Journal on Optimization,", "citeRegEx": "Birgin et al\\.,? \\Q2000\\E", "shortCiteRegEx": "Birgin et al\\.", "year": 2000}, {"title": "Learning deep structured models", "author": ["Liang-Chieh Chen", "Alexander G Schwing", "Alan L Yuille", "Raquel Urtasun"], "venue": "In Proceedings of the International Conference on Machine Learning,", "citeRegEx": "Chen et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Chen et al\\.", "year": 2015}, {"title": "Generic methods for optimization-based modeling", "author": ["Justin Domke"], "venue": "In Proceedings of the Conference on AI and Statistics,", "citeRegEx": "Domke.,? \\Q2012\\E", "shortCiteRegEx": "Domke.", "year": 2012}, {"title": "Adaptive subgradient methods for online learning and stochastic optimization", "author": ["John Duchi", "Elad Hazan", "Yoram Singer"], "venue": "The Journal of Machine Learning Research,", "citeRegEx": "Duchi et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Duchi et al\\.", "year": 2011}, {"title": "Generative adversarial nets", "author": ["Ian Goodfellow", "Jean Pouget-Abadie", "Mehdi Mirza", "Bing Xu", "David Warde-Farley", "Sherjil Ozair", "Aaron Courville", "Yoshua Bengio"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Goodfellow et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Goodfellow et al\\.", "year": 2014}, {"title": "Continuous deep q-learning with model-based acceleration", "author": ["Shixiang Gu", "Timothy Lillicrap", "Ilya Sutskever", "Sergey Levine"], "venue": "In Proceedings of the International Conference on Machine Learning,", "citeRegEx": "Gu et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Gu et al\\.", "year": 2016}, {"title": "Deep residual learning for image recognition", "author": ["Kaiming He", "Xiangyu Zhang", "Shaoqing Ren", "Jian Sun"], "venue": "arXiv preprint arXiv:1512.03385,", "citeRegEx": "He et al\\.,? \\Q2015\\E", "shortCiteRegEx": "He et al\\.", "year": 2015}, {"title": "Densely connected convolutional networks", "author": ["Gao Huang", "Zhuang Liu", "Kilian Q Weinberger"], "venue": "arXiv preprint arXiv:1608.06993,", "citeRegEx": "Huang et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Huang et al\\.", "year": 2016}, {"title": "Batch normalization: Accelerating deep network training by reducing internal covariate shift", "author": ["Sergey Ioffe", "Christian Szegedy"], "venue": "In Proceedings of The 32nd International Conference on Machine Learning,", "citeRegEx": "Ioffe and Szegedy.,? \\Q2015\\E", "shortCiteRegEx": "Ioffe and Szegedy.", "year": 2015}, {"title": "Multilabel text classification for automated tag suggestion", "author": ["Ioannis Katakis", "Grigorios Tsoumakas", "Ioannis Vlahavas"], "venue": "ECML PKDD discovery challenge,", "citeRegEx": "Katakis et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Katakis et al\\.", "year": 2008}, {"title": "Adam: A method for stochastic optimization", "author": ["Diederik Kingma", "Jimmy Ba"], "venue": "arXiv preprint arXiv:1412.6980,", "citeRegEx": "Kingma and Ba.,? \\Q2014\\E", "shortCiteRegEx": "Kingma and Ba.", "year": 2014}, {"title": "Probabilistic graphical models: principles and techniques", "author": ["Daphne Koller", "Nir Friedman"], "venue": "MIT press,", "citeRegEx": "Koller and Friedman.,? \\Q2009\\E", "shortCiteRegEx": "Koller and Friedman.", "year": 2009}, {"title": "Imagenet classification with deep convolutional neural networks. In Advances in neural information processing", "author": ["Alex Krizhevsky", "Ilya Sutskever", "Geoffrey E Hinton"], "venue": null, "citeRegEx": "Krizhevsky et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Krizhevsky et al\\.", "year": 2012}, {"title": "A tutorial on energy-based learning", "author": ["Yann LeCun", "Sumit Chopra", "Raia Hadsell", "M Ranzato", "F Huang"], "venue": "Predicting structured data,", "citeRegEx": "LeCun et al\\.,? \\Q2006\\E", "shortCiteRegEx": "LeCun et al\\.", "year": 2006}, {"title": "Continuous control with deep reinforcement learning", "author": ["Timothy P Lillicrap", "Jonathan J Hunt", "Alexander Pritzel", "Nicolas Heess", "Tom Erez", "Yuval Tassa", "David Silver", "Daan Wierstra"], "venue": "arXiv preprint arXiv:1509.02971,", "citeRegEx": "Lillicrap et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Lillicrap et al\\.", "year": 2015}, {"title": "An extensive experimental comparison of methods for multi-label learning", "author": ["Gjorgji Madjarov", "Dragi Kocev", "Dejan Gjorgjevikj", "Sa\u0161o D\u017eeroski"], "venue": "Pattern Recognition,", "citeRegEx": "Madjarov et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Madjarov et al\\.", "year": 2012}, {"title": "Convex piecewise-linear fitting", "author": ["Alessandro Magnani", "Stephen P Boyd"], "venue": "Optimization and Engineering,", "citeRegEx": "Magnani and Boyd.,? \\Q2009\\E", "shortCiteRegEx": "Magnani and Boyd.", "year": 2009}, {"title": "Human-level control through deep reinforcement learning", "author": ["Volodymyr Mnih", "Koray Kavukcuoglu", "David Silver", "Andrei A Rusu", "Joel Veness", "Marc G Bellemare", "Alex Graves", "Martin Riedmiller", "Andreas K Fidjeland", "Georg Ostrovski"], "venue": "Nature, 518(7540):529\u2013533,", "citeRegEx": "Mnih et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Mnih et al\\.", "year": 2015}, {"title": "Rectified linear units improve restricted boltzmann machines", "author": ["Vinod Nair", "Geoffrey E Hinton"], "venue": "In Proceedings of the 27th International Conference on Machine Learning", "citeRegEx": "Nair and Hinton.,? \\Q2010\\E", "shortCiteRegEx": "Nair and Hinton.", "year": 2010}, {"title": "A guide to NumPy, volume 1", "author": ["Travis E Oliphant"], "venue": "Trelgol Publishing USA,", "citeRegEx": "Oliphant.,? \\Q2006\\E", "shortCiteRegEx": "Oliphant.", "year": 2006}, {"title": "Scikit-learn: Machine learning in python", "author": ["Fabian Pedregosa", "Ga\u00ebl Varoquaux", "Alexandre Gramfort", "Vincent Michel", "Bertrand Thirion", "Olivier Grisel", "Mathieu Blondel", "Peter Prettenhofer", "Ron Weiss", "Vincent Dubourg"], "venue": "The Journal of Machine Learning Research,", "citeRegEx": "Pedregosa et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Pedregosa et al\\.", "year": 2011}, {"title": "Conditional neural fields", "author": ["Jian Peng", "Liefeng Bo", "Jinbo Xu"], "venue": "In Advances in neural information processing systems,", "citeRegEx": "Peng et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Peng et al\\.", "year": 2009}, {"title": "Some methods of speeding up the convergence of iteration methods", "author": ["Boris T Polyak"], "venue": "USSR Computational Mathematics and Mathematical Physics,", "citeRegEx": "Polyak.,? \\Q1964\\E", "shortCiteRegEx": "Polyak.", "year": 1964}, {"title": "Sum-product networks: A new deep architecture", "author": ["Hoifung Poon", "Pedro Domingos"], "venue": "UAI", "citeRegEx": "Poon and Domingos.,? \\Q2011\\E", "shortCiteRegEx": "Poon and Domingos.", "year": 2011}, {"title": "Approximate) subgradient methods for structured prediction", "author": ["Nathan D Ratliff", "J Andrew Bagnell", "Martin Zinkevich"], "venue": "In International Conference on Artificial Intelligence and Statistics,", "citeRegEx": "Ratliff et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Ratliff et al\\.", "year": 2007}, {"title": "Learning representations by back-propagating errors", "author": ["David E Rumelhart", "Geoffrey E Hinton", "Ronald J Williams"], "venue": "Cognitive modeling,", "citeRegEx": "Rumelhart et al\\.,? \\Q1988\\E", "shortCiteRegEx": "Rumelhart et al\\.", "year": 1988}, {"title": "Parameterisation of a stochastic model for human face identification", "author": ["Ferdinando S Samaria", "Andy C Harter"], "venue": "In Applications of Computer Vision,", "citeRegEx": "Samaria and Harter.,? \\Q1994\\E", "shortCiteRegEx": "Samaria and Harter.", "year": 1994}, {"title": "Reverse tdnn: an architecture for trajectory generation", "author": ["Patrice Simard", "Yann LeCun"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Simard and LeCun.,? \\Q1991\\E", "shortCiteRegEx": "Simard and LeCun.", "year": 1991}, {"title": "Very deep convolutional networks for large-scale image recognition", "author": ["Karen Simonyan", "Andrew Zisserman"], "venue": "arXiv preprint arXiv:1409.1556,", "citeRegEx": "Simonyan and Zisserman.,? \\Q2014\\E", "shortCiteRegEx": "Simonyan and Zisserman.", "year": 2014}, {"title": "Bundle methods for machine learning", "author": ["Alex J. Smola", "S.v.n. Vishwanathan", "Quoc V. Le"], "venue": "Advances in Neural Information Processing Systems", "citeRegEx": "Smola et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Smola et al\\.", "year": 2008}, {"title": "Going deeper with convolutions", "author": ["Christian Szegedy", "Wei Liu", "Yangqing Jia", "Pierre Sermanet", "Scott Reed", "Dragomir Anguelov", "Dumitru Erhan", "Vincent Vanhoucke", "Andrew Rabinovich"], "venue": "In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition,", "citeRegEx": "Szegedy et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Szegedy et al\\.", "year": 2015}, {"title": "Learning structured prediction models: A large margin approach", "author": ["Ben Taskar", "Vassil Chatalbashev", "Daphne Koller", "Carlos Guestrin"], "venue": "In Proceedings of the 22nd International Conference on Machine Learning,", "citeRegEx": "Taskar et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Taskar et al\\.", "year": 2005}, {"title": "Mujoco: A physics engine for model-based control", "author": ["Emanuel Todorov", "Tom Erez", "Yuval Tassa"], "venue": "In 2012 IEEE/RSJ International Conference on Intelligent Robots and Systems,", "citeRegEx": "Todorov et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Todorov et al\\.", "year": 2012}, {"title": "Large margin methods for structured and interdependent output variables", "author": ["Ioannis Tsochantaridis", "Thorsten Joachims", "Thomas Hofmann", "Yasemin Altun"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Tsochantaridis et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Tsochantaridis et al\\.", "year": 2005}, {"title": "Mulan: A java library for multi-label learning", "author": ["Grigorios Tsoumakas", "Eleftherios Spyromitros-Xioufis", "Jozef Vilcek", "Ioannis Vlahavas"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Tsoumakas et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Tsoumakas et al\\.", "year": 2011}, {"title": "Python reference manual", "author": ["Guido Van Rossum", "Fred L Drake Jr."], "venue": "Centrum voor Wiskunde en Informatica Amsterdam,", "citeRegEx": "Rossum and Jr.,? \\Q1995\\E", "shortCiteRegEx": "Rossum and Jr.", "year": 1995}, {"title": "Primal-dual interior-point methods", "author": ["Stephen J Wright"], "venue": null, "citeRegEx": "Wright.,? \\Q1997\\E", "shortCiteRegEx": "Wright.", "year": 1997}, {"title": "Conditional random fields as recurrent neural networks", "author": ["Shuai Zheng", "Sadeep Jayasumana", "Bernardino Romera-Paredes", "Vibhav Vineet", "Zhizhong Su", "Dalong Du", "Chang Huang", "Philip HS Torr"], "venue": "In Proceedings of the IEEE International Conference on Computer Vision,", "citeRegEx": "Zheng et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Zheng et al\\.", "year": 2015}], "referenceMentions": [{"referenceID": 18, "context": "Given (typically high-dimensional) structured input and output spaces X \u00d7 Y, we can build a network over (x, y) pairs that encodes the energy function for this pair, following typical energy-based learning formalisms [LeCun et al., 2006].", "startOffset": 217, "endOffset": 237}, {"referenceID": 2, "context": "This is similar in nature to the neural network structured prediction methodology [Belanger and McCallum, 2015], with the difference being that in our setting f is both convex in y, so the optimization can be performed globally, and has a deep network structure over y itself.", "startOffset": 82, "endOffset": 111}, {"referenceID": 3, "context": "Simard and LeCun [1991]), and there was an early appreciation that structured models like hidden Markov models could be combined with the outputs of neural networks [Bengio et al., 1994].", "startOffset": 165, "endOffset": 186}, {"referenceID": 26, "context": "Simard and LeCun [1991]), and there was an early appreciation that structured models like hidden Markov models could be combined with the outputs of neural networks [Bengio et al.", "startOffset": 0, "endOffset": 24}, {"referenceID": 2, "context": "Simard and LeCun [1991]), and there was an early appreciation that structured models like hidden Markov models could be combined with the outputs of neural networks [Bengio et al., 1994]. Much of this earlier work is surveyed and synthesized by LeCun et al. [2006], who give a tutorial on these energy based learning methods.", "startOffset": 166, "endOffset": 265}, {"referenceID": 28, "context": "Finally, although it is more abstract, we feel there is a philosophical similarity between our proposed approach and sum-product networks [Poon and Domingos, 2011]; both settings define networks where inference is accomplished \u201ceasily\u201d either by a sum-product message passing algorithm (by construction) or via convex optimization.", "startOffset": 138, "endOffset": 163}, {"referenceID": 21, "context": "The most similar work of which we are aware specifically fit sums of rectified half-planes to data [Magnani and Boyd, 2009], which is similar to one layer of our rectified linear units.", "startOffset": 99, "endOffset": 123}, {"referenceID": 11, "context": "Some passthrough layers have been recently explored in the deep residual networks [He et al., 2015] and densely connected convolutional networks [Huang et al.", "startOffset": 82, "endOffset": 99}, {"referenceID": 12, "context": ", 2015] and densely connected convolutional networks [Huang et al., 2016], though these differ from those of an ICNN as they pass through hidden layers deeper in the network, whereas to maintain convexity our passthrough layers can only apply to the input directly.", "startOffset": 53, "endOffset": 73}, {"referenceID": 17, "context": "Indeed, modern feedforward architectures such as AlexNet [Krizhevsky et al., 2012], VGG [Simonyan and Zisserman, 2014], and GoogLeNet [Szegedy et al.", "startOffset": 57, "endOffset": 82}, {"referenceID": 33, "context": ", 2012], VGG [Simonyan and Zisserman, 2014], and GoogLeNet [Szegedy et al.", "startOffset": 13, "endOffset": 43}, {"referenceID": 35, "context": ", 2012], VGG [Simonyan and Zisserman, 2014], and GoogLeNet [Szegedy et al., 2015] with ReLUs [Nair and Hinton, 2010] can be made input convex with Proposition 1.", "startOffset": 59, "endOffset": 81}, {"referenceID": 23, "context": ", 2015] with ReLUs [Nair and Hinton, 2010] can be made input convex with Proposition 1.", "startOffset": 19, "endOffset": 42}, {"referenceID": 18, "context": "LeCun et al. [2006] for more details).", "startOffset": 0, "endOffset": 20}, {"referenceID": 41, "context": "Furthermore, most LP solution methods to solve such problems require that we form and invert structured matrices with blocks such as W T i Wi \u2014 the case for most interior-point methods [Wright, 1997] or even approximate algorithms such as the alternating direction method of multipliers [Boyd et al.", "startOffset": 185, "endOffset": 199}, {"referenceID": 7, "context": "The method is also more challenging to integrate with some learning procedures, as we often need to differentiate through an entire chain of the gradient descent algorithm Domke [2012]. Thus, while the method can sometimes work in practice, we have found that other approaches typically far outperform this method, and we will focus on alternative approximate approaches for the remainder of this section.", "startOffset": 172, "endOffset": 185}, {"referenceID": 34, "context": "Bundle method One optimization approach that seems particular promising in this setting is the bundle method [Smola et al., 2008].", "startOffset": 109, "endOffset": 129}, {"referenceID": 29, "context": "The optimization problem (2) is naturally still not convex in \u03b8, but can be solved via the subgradient method for structured prediction [Ratliff et al., 2007].", "startOffset": 136, "endOffset": 158}, {"referenceID": 8, "context": "This method can be easily adapted to use mini-batches instead of a single example per subgradient step, and also adapted to alternative optimization methods like AdaGrad [Duchi et al., 2011] or ADAM [Kingma and Ba, 2014].", "startOffset": 170, "endOffset": 190}, {"referenceID": 15, "context": ", 2011] or ADAM [Kingma and Ba, 2014].", "startOffset": 16, "endOffset": 37}, {"referenceID": 14, "context": "This section presents our results on multi-label classification on the bibtex dataset [Katakis et al., 2008], image completion using the Olivetti face dataset [Samaria and Harter, 1994], and continuous action reinforcement learning in the OpenAI Gym [Brockman et al.", "startOffset": 86, "endOffset": 108}, {"referenceID": 31, "context": ", 2008], image completion using the Olivetti face dataset [Samaria and Harter, 1994], and continuous action reinforcement learning in the OpenAI Gym [Brockman et al.", "startOffset": 58, "endOffset": 84}, {"referenceID": 24, "context": "Our Python [Van Rossum and Drake Jr, 1995] implementation uses numpy [Oliphant, 2006] for linear algebra primitives and TensorFlow [Abadi et al.", "startOffset": 69, "endOffset": 85}, {"referenceID": 0, "context": "Our Python [Van Rossum and Drake Jr, 1995] implementation uses numpy [Oliphant, 2006] for linear algebra primitives and TensorFlow [Abadi et al., 2016] for neural networks and auto-differentiation.", "startOffset": 131, "endOffset": 151}, {"referenceID": 25, "context": "1 Synthetic classification examples We begin with a simple example to illustrate the classification performance of a two-hidden-layer FICNN and PICNN on two-dimensional binary binary classification tasks from the scikit-learn toolkit [Pedregosa et al., 2011].", "startOffset": 234, "endOffset": 258}, {"referenceID": 39, "context": "We use the ARFF version of this dataset from Mulan [Tsoumakas et al., 2011].", "startOffset": 51, "endOffset": 75}, {"referenceID": 13, "context": "Our PICNN architecture for multi-label classification uses fully-connected layers with ReLU activation functions and batch normalization [Ioffe and Szegedy, 2015] along with input path.", "startOffset": 137, "endOffset": 162}, {"referenceID": 2, "context": "SPENs [Belanger and McCallum, 2015] obtain a macro-F1 score of 42.", "startOffset": 6, "endOffset": 35}, {"referenceID": 12, "context": "2 Multi-Label Classification We next study how ICNNs perform on multi-label classification with the bibtex dataset and benchmark presented in Katakis et al. [2008]. This benchmark maps text classification from an input space X of 1836 bag-of-works indicator (binary) features to an output space Y of 159 binary labels.", "startOffset": 142, "endOffset": 164}, {"referenceID": 12, "context": "2 Multi-Label Classification We next study how ICNNs perform on multi-label classification with the bibtex dataset and benchmark presented in Katakis et al. [2008]. This benchmark maps text classification from an input space X of 1836 bag-of-works indicator (binary) features to an output space Y of 159 binary labels. We use the train/test split of 4880/2515 from Katakis et al. [2008] and evaluate with the exampleaveraged (macro) F1 score.", "startOffset": 142, "endOffset": 387}, {"referenceID": 12, "context": "Our PICNN architecture for multi-label classification uses fully-connected layers with ReLU activation functions and batch normalization [Ioffe and Szegedy, 2015] along with input path. As a baseline, we use a fully-connected neural network with batch normalization and ReLU activation functions. Figure 4 compares the training of a feedforward baseline to a PICNN with the same structure (600 fully connected, 159 (#labels) fully connected). While the PICNN does not improve the F1 score, we emphasize that the performance is similar to the feedforward network. Our baseline feedforward network\u2019s best macro F1 score of 45.5 outperforms the best macro-F1 score of 43.4 from a survey presented in Madjarov et al. [2012]. SPENs [Belanger and McCallum, 2015] obtain a macro-F1 score of 42.", "startOffset": 138, "endOffset": 720}, {"referenceID": 25, "context": "3 Image completion on the Olivetti faces As a test of the system on a structured prediction task over a much more complex output space Y, we apply a convolutional DQN-style PICNN to face completion on the sklearn version [Pedregosa et al., 2011] of the Olivetti data set [Samaria and Harter, 1994], which contains 400 64x64 grayscale", "startOffset": 221, "endOffset": 245}, {"referenceID": 31, "context": ", 2011] of the Olivetti data set [Samaria and Harter, 1994], which contains 400 64x64 grayscale", "startOffset": 33, "endOffset": 59}, {"referenceID": 22, "context": "images Our PICNN is equivalent to the DQN architecture in Mnih et al. [2015] (except we have added batch normalization to the input path) and is over (x, y) pairs where x (32x64) is the left half and y (32x64) is the right half of the image.", "startOffset": 58, "endOffset": 77}, {"referenceID": 22, "context": "images Our PICNN is equivalent to the DQN architecture in Mnih et al. [2015] (except we have added batch normalization to the input path) and is over (x, y) pairs where x (32x64) is the left half and y (32x64) is the right half of the image. The input and output paths are: 32x8x8 conv (stride 4x2), 64x4x4 conv (stride 2x2), 64x3x3 conv, 512 fully connected. Any modern architecture can be used in place of a DQN and we have not well-explored the space of possible architectures. Our weak motivation for choosing a DQN architecture comes from its use of strided convolutions over pooling. Our intuition is that networks for image completion do not need (and can be hurt by) the translation-invariance provided by pooling layers present in other modern architectures. This experiment uses the same training/test splits and minimizes the mean squared error (MSE) as in Poon and Domingos [2011] so that our results can be directly compared to (a non-exhaustive list of) other techniques.", "startOffset": 58, "endOffset": 893}, {"referenceID": 28, "context": "Our network improves upon performance of sum-product networks, which obtain an MSE of 942 [Poon and Domingos, 2011].", "startOffset": 90, "endOffset": 115}, {"referenceID": 37, "context": ", 2016] that use the MuJoCo physics simulator [Todorov et al., 2012].", "startOffset": 46, "endOffset": 68}, {"referenceID": 19, "context": "We use Deep Deterministic Policy Gradient (DDPG) [Lillicrap et al., 2015] and Normalized Advantage Functions (NAF) [Gu et al.", "startOffset": 49, "endOffset": 73}, {"referenceID": 10, "context": ", 2015] and Normalized Advantage Functions (NAF) [Gu et al., 2016] as state-of-the-art baselines.", "startOffset": 49, "endOffset": 66}], "year": 2016, "abstractText": "This paper presents the input convex neural network architecture. These are scalar-valued (potentially deep) neural networks with constraints on the network parameters such that the output of the network is a convex function of (some of) the inputs. The networks allow for efficient inference via optimization over some inputs to the network given others, and can be applied to settings including structured prediction, data imputation, reinforcement learning, and others. In this paper we lay the basic groundwork for these models, proposing methods for inference, optimization and learning, and analyze their representational power. We show that many existing neural network architectures can be made input-convex with only minor modification, and develop specialized optimization algorithms tailored to this setting. Finally, we highlight the performance of the methods on multi-label prediction, image completion, and reinforcement learning problems, where we show improvement over the existing state of the art in many cases.", "creator": "LaTeX with hyperref package"}}}