{"id": "1609.07317", "review": {"conference": "EMNLP", "VERSION": "v1", "DATE_OF_SUBMISSION": "23-Sep-2016", "title": "Language as a Latent Variable: Discrete Generative Models for Sentence Compression", "abstract": "In this work we explore deep generative models of text in which the latent representation of a document is itself drawn from a discrete language model distribution. We formulate a variational auto-encoder for inference in this model and apply it to the task of compressing sentences. In this application the generative model first draws a latent summary sentence from a background language model, and then subsequently draws the observed sentence conditioned on this latent summary. In our empirical evaluation we show that generative formulations of both abstractive and extractive compression yield state-of-the-art results when trained on a large amount of supervised data. Further, we explore semi-supervised compression scenarios where we show that it is possible to achieve performance competitive with previously proposed supervised models while training on a fraction of the supervised data.", "histories": [["v1", "Fri, 23 Sep 2016 11:25:41 GMT  (1299kb,D)", "http://arxiv.org/abs/1609.07317v1", "EMNLP 2016"], ["v2", "Fri, 14 Oct 2016 00:21:00 GMT  (1299kb,D)", "http://arxiv.org/abs/1609.07317v2", "EMNLP 2016"]], "COMMENTS": "EMNLP 2016", "reviews": [], "SUBJECTS": "cs.CL cs.AI", "authors": ["yishu miao", "phil blunsom"], "accepted": true, "id": "1609.07317"}, "pdf": {"name": "1609.07317.pdf", "metadata": {"source": "CRF", "title": "Language as a Latent Variable: Discrete Generative Models for Sentence Compression", "authors": ["Yishu Miao", "Phil Blunsom"], "emails": ["phil.blunsom}@cs.ox.ac.uk"], "sections": [{"heading": "1 Introduction", "text": "This year is the highest in the history of the country."}, {"heading": "2 Auto-Encoding Sentence Compression", "text": "The ASC model consists of four recursive neural networks - an encoder, a compressor, a decoder, and a speech model.Let's be the source set and c the compression set. The compression model (encoder compressor) is the inference network q\u03c6 (c | s), which takes source sets s as inputs and generates extractive compressions.The reconstruction1 The language model, the layer connections, and the soft attentions of the decoder are shown in Figure 1 for clarity. The model (compressor decoder) is the generative network context (s | c), which reconstructs source sets based on the latent compressions c. Therefore, the forward pass of the encoder starts at the compressor and ends at the decoder. Like the previous distribution model, the precompression model of the sets is likely to be compressed so that the natural p compression (the precompression) is taken from the samples."}, {"heading": "2.1 Compression", "text": "For the compression model (encoder compressor), q\u03c6 (c | s), we use a pointer network, consisting of a bidirectional LSTM encoder that processes the source records, and an LSTM compressor that generates compressed records by observing the encoded source words. Let's say the words in the source records, hei be the corresponding state output of the encoder. hei are the concatenated hidden states from any direction: hei = f \u2212 \u2192 enc (~ hei \u2212 1, si) | f \u2190 \u2212 enc (~ hei + 1, si) (1) Further, let's say cj be the words in the compressed sentences, hcj the state output of the compressor. We construct the predictive distribution by observing the words in the source records: hcj = fcom (h c \u2212 1, cj \u2212 1) (2) uj \u2212 w = c = the source words (qh), and qqh = the source words (1)."}, {"heading": "2.2 Reconstruction", "text": "For the reconstruction model (compressor decoder) p\u03b8 (s | c), we apply a soft attention sequence equation model to generate the source set s based on the compression samples c \u0445 q\u03c6 (c | s). Let sk be the words in the reconstructed sentences and hdk be the corresponding state products of the decoder: hdk = fdec (h \u0445 c \u2212 1, sk \u2212 1) (5) In this model, we directly use the recurring cell of the compressor to encode the compression profiles: 2: h \u0441c j = fcom (h \u0441c \u2212 1, cj) (6), with the state productions h \u0445 c j corresponding to the word inputs cj being different from the outputs cj in the compression model, because we block the information from the source records. We also run a start symbol s0 for the reconstructed sentence and h0 is triggered by the last state prodj (cgnose)."}, {"heading": "2.3 Inference", "text": "In the ASC model, there are two sets of parameters for the summary and the effectiveness of the compilation of errors that need to be updated during the inference. Due to the non-differentiability of the model, the repair trick of the UAE is not applicable in this case. Therefore, we use the REINFORCE algorithm (Mnih et al., 2014; Mnih and Gregor, 2014) to reduce the variance of the gradient estimator.The variable lower limit of the ASC model is: L = Eq\u03c6 (c) [log p\u03b8 (s | c]]] \u2212 DKL [q\u03c6], (c | s) to reduce the variance of the gradient estimator.The variable lower limit of the ASC model is: L = Eq\u03c6 (c) p (c) dc = log p (s) p (s), by optimizing the lower limit (Eq. 11), the model matches the effectiveness of the summary and the effectiveness of the synopsis."}, {"heading": "3 Forced-attention Sentence Compression", "text": "Although we introduce a biased estimator by using pointer meshes, it is still very difficult for the compression model to generate reasonable sentences in natural language at the early stages of learning, resulting in high variance for the gradient estimator. It neither directly replicates the pointer mesh of the ASC model, nor does it use a typical sequence sequence sequence model, the FSC model uses a forceattention strategy (Figure 2) that encourages the compressor to select words that appear in the source set, but maintains the original full output vocabulary V. The force attention strategy is essentially a combined pointer model that determines whether it selects a word from the source sentence s or predicts a word from any recursive state."}, {"heading": "4 Semi-supervised Training", "text": "Since the automatic encoding set compression model (ASC) grants the ability to use an unlabeled record, we explore a semi-monitored training framework for the ASC and FSC models. In this scenario, we have a labeled record that contains source-compressed parallel records, (s, c), and an unlabeled record that contains only source records, (s, c). The FSC model is trained on L so that we can learn the compression model by maximizing the likelihood of logs. (22) While the ASC model is trained on U's, by maximizing the modified variable lower limit, L = \"s\" U (c \"s) [log (c\" s \"s\" s \"s\" s \"s\" s) [log \"s\" c \"s\" s \"s\" s \"s\" s \"s\" s \"s (c\" s \"s\" s \"s\" s \"s\" s \"s) (c\" s \"s\" s \"s\" s \"s (c\" s \"s\" s \"s\" s \"s\" s) (c \"s\" s \"s\" s \"s\" s (s) (c \"s\" s \"s\" s \"s\" s \"s\" s \"s) (c\" s \"s\" s \"s) (s\" s \"s\" s \"s\" s) (s \"s\" s \"s\" s \"s\" s) (s \"s\" s \"s\" s \"s\" s \"s) (s\" s \"s\" s \"s\" s \"s\" s \"s\" s) (s \"s\" s \"s\" s \"s\" s) (s \"s\" s \"s\" s \"s\" s \"s\" s \"s (s\" s \"s) (s\" s \"s\" s \"s\" s)."}, {"heading": "5 Related Work", "text": "As one of the typical sequencer-to-sequencer tasks, the summary of sentences has been explored through a series of discriminatory encoder-decoder-neural models. Filippova et al. (2015) performs the extractive summary by deletion with LSTMs, while Rush et al. (2015) uses a conventional encoder and a recognized decoder to generate abstract summaries. (2016) Nallapati et al. (2016) improves performance by applying multiple variants of RNN encoder-decoder models. Recent work Gulcehre et al. (2016), Nallapati et al. (2016) and Gu et al."}, {"heading": "6 Experiments", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "6.1 Dataset & Setup", "text": "We evaluate the proposed models on the standard Gigaword3-sentence compression dataset. This dataset was generated by linking the headline of each article with its first sentence to create a source-compression pair. Rush et al. (2015) delivered Skripte4 to filter out outliers, resulting in approximately 3.8M training pairs, a 400K validation set and a 400K test set. In the following experiments, all models are trained on the training set with different data sizes 5 and tested on a 2K subset that is identical to that of Rush et al. (2015) and Nallapati et al. (2016). We decrypt the sets with k = 5 target connections and test with full-length rouge target score.For the ASC- and FSC-models, we use 256 for the dimension of both hidden units as well as reference tables. In the ASC-model, we use a three-chord target-er skier with a point-er target-er."}, {"heading": "6.2 Extractive Summarisation", "text": "The first set of experiments evaluates the models for extractive summary. Here, we refer to the joint3https: / / catalog.ldc.upenn.edu / LDC2012T21 4https: / / github.com / facebook / NAMAS 5Hyperparameters are tuned for validation to maximize the perplexity of the summaries rather than the reconstructed source data sets. ASC + FSC1 and ASC + FSC2 models where ASC is trained on unlabeled data and FSC is trained on labeled data. ASC + FSC1 model uses equally labeled and unlabeled data sets where the unlabeled data sets are the same sets of articles in the labeled data, so there are no additional unlabeled data that will be applied in this case. ASC + FSC1 model uses the full unlabeled data sets in addition to the existing, labeled data that are better than the labeled data, where the labeled data is in this case, the unlabeled data are the ones that are better than the ones in this case."}, {"heading": "6.3 Abstractive Summarisation", "text": "The second set of experiments evaluates the performance of the abstract summary (Table 2). We see that adding the generative objective to the discriminatory model (ASC + FSC1) leads to a significant increase in the overall numbers, while comparing the validation of the performance of the three models in order to transfer the acquired knowledge to the blank data. ASC + FSC1 deals with the same data sets as the blank and described ones."}, {"heading": "7 Discussion", "text": "From the perspective of generative models, a significant contribution to our work is a process of reducing variance for discrete sampling-based variations. The first step is the introduction of two baselines in the control variant due to the fact that the repair trick is not applicable for discrete latent variables. However, it is the second step of using a pointer network as a biased estimator that makes the decisive contribution, resulting in a much smaller state space limited by the length of the source set (usually between 20 and 50 tokens) compared to the full vocabulary. The last step is the application of the FSC model to transfer knowledge from the monitored data to the pointer network."}, {"heading": "8 Conclusion", "text": "In this paper, we presented a generative model for the joint modeling of sequence pairs and evaluated its effectiveness in the task of sentence compression. The variable auto-encoding model provided an effective inference algorithm for this approach and also enabled us to explore combinations of discriminatory (FSC) and generative (ASC) compression models. The evaluation results show that the supervised training of the combination of these models improves the current performance of the Gigaword compression dataset. If we train the supervised FSC model on a small amount of marked data and the unattended ASC model on a large amount of blank data, the combined model is able to effectively monitor previously reported benchmarks trained on much more closely monitored data. These results show that we are able to model language as a discrete latent variable in a varying auto-encoding framework and that the supervised model is both effective in the supervised position and in the supervised situation."}], "references": [{"title": "Volodymyr Mnih", "author": ["Jimmy Ba"], "venue": "and Koray Kavukcuoglu.", "citeRegEx": "Ba et al.2015", "shortCiteRegEx": null, "year": 2015}, {"title": "Kyunghyun Cho", "author": ["Dzmitry Bahdanau"], "venue": "and Yoshua Bengio.", "citeRegEx": "Bahdanau et al.2015", "shortCiteRegEx": null, "year": 2015}, {"title": "Rafal Jozefowicz", "author": ["Samuel R Bowman", "Luke Vilnis", "Oriol Vinyals", "Andrew M Dai"], "venue": "and Samy Bengio.", "citeRegEx": "Bowman et al.2015", "shortCiteRegEx": null, "year": 2015}, {"title": "Kyunghyun Cho", "author": ["Jan K Chorowski", "Dzmitry Bahdanau", "Dmitriy Serdyuk"], "venue": "and Yoshua Bengio.", "citeRegEx": "Chorowski et al.2015", "shortCiteRegEx": null, "year": 2015}, {"title": "Semi-supervised sequence learning", "author": ["Dai", "Le2015] Andrew M Dai", "Quoc V Le"], "venue": "In Proceedings of NIPS,", "citeRegEx": "Dai et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Dai et al\\.", "year": 2015}, {"title": "Miguel Ballesteros", "author": ["Chris Dyer", "Adhiguna Kuncoro"], "venue": "and Noah A Smith.", "citeRegEx": "Dyer et al.2016", "shortCiteRegEx": null, "year": 2016}, {"title": "Koray Kavukcuoglu", "author": ["SM Eslami", "Nicolas Heess", "Theophane Weber", "Yuval Tassa"], "venue": "and Geoffrey E Hinton.", "citeRegEx": "Eslami et al.2016", "shortCiteRegEx": null, "year": 2016}, {"title": "Lukasz Kaiser", "author": ["Katja Filippova", "Enrique Alfonseca", "Carlos A Colmenares"], "venue": "and Oriol Vinyals.", "citeRegEx": "Filippova et al.2015", "shortCiteRegEx": null, "year": 2015}, {"title": "Alex Graves", "author": ["Karol Gregor", "Ivo Danihelka"], "venue": "and Daan Wierstra.", "citeRegEx": "Gregor et al.2015", "shortCiteRegEx": null, "year": 2015}, {"title": "Hang Li", "author": ["Jiatao Gu", "Zhengdong Lu"], "venue": "and Victor OK Li.", "citeRegEx": "Gu et al.2016", "shortCiteRegEx": null, "year": 2016}, {"title": "Bowen Zhou", "author": ["Caglar Gulcehre", "Sungjin Ahn", "Ramesh Nallapati"], "venue": "and Yoshua Bengio.", "citeRegEx": "Gulcehre et al.2016", "shortCiteRegEx": null, "year": 2016}, {"title": "Reducing the dimensionality of data with neural networks", "author": ["Hinton", "Ruslan R Salakhutdinov"], "venue": null, "citeRegEx": "Hinton et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Hinton et al\\.", "year": 2006}, {"title": "Recurrent continuous translation models", "author": ["Kalchbrenner", "Blunsom2013] Nal Kalchbrenner", "Phil Blunsom"], "venue": "In Proceedings of EMNLP", "citeRegEx": "Kalchbrenner et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Kalchbrenner et al\\.", "year": 2013}, {"title": "Adam: A method for stochastic optimization", "author": ["Kingma", "Ba2015] Diederik P. Kingma", "Jimmy Ba"], "venue": "In Proceedings of ICLR", "citeRegEx": "Kingma et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Kingma et al\\.", "year": 2015}, {"title": "Auto-encoding variational bayes", "author": ["Kingma", "Welling2014] Diederik P Kingma", "Max Welling"], "venue": "In Proceedings of ICLR", "citeRegEx": "Kingma et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Kingma et al\\.", "year": 2014}, {"title": "Danilo Jimenez Rezende", "author": ["Diederik P Kingma", "Shakir Mohamed"], "venue": "and Max Welling.", "citeRegEx": "Kingma et al.2014", "shortCiteRegEx": null, "year": 2014}, {"title": "Discrete-state variational autoencoders for joint discovery and factorization of relations", "author": ["Marcheggiani", "Titov2016] Diego Marcheggiani", "Ivan Titov"], "venue": "Transactions of the Association for Computational Linguistics,", "citeRegEx": "Marcheggiani et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Marcheggiani et al\\.", "year": 2016}, {"title": "Lei Yu", "author": ["Yishu Miao"], "venue": "and Phil Blunsom.", "citeRegEx": "Miao et al.2016", "shortCiteRegEx": null, "year": 2016}, {"title": "Neural variational inference and learning in belief networks", "author": ["Mnih", "Gregor2014] Andriy Mnih", "Karol Gregor"], "venue": "In Proceedings of ICML", "citeRegEx": "Mnih et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Mnih et al\\.", "year": 2014}, {"title": "Nicolas Heess", "author": ["Volodymyr Mnih"], "venue": "and Alex Graves.", "citeRegEx": "Mnih et al.2014", "shortCiteRegEx": null, "year": 2014}, {"title": "\u00c7a glar Gul\u00e7ehre", "author": ["Ramesh Nallapati", "Bowen Zhou"], "venue": "and Bing Xiang.", "citeRegEx": "Nallapati et al.2016", "shortCiteRegEx": null, "year": 2016}, {"title": "Sequence level training with recurrent neural networks", "author": ["Sumit Chopra", "Michael Auli", "Wojciech Zaremba"], "venue": null, "citeRegEx": "Ranzato et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Ranzato et al\\.", "year": 2016}, {"title": "Shakir Mohamed", "author": ["Danilo J Rezende"], "venue": "and Daan Wierstra.", "citeRegEx": "Rezende et al.2014", "shortCiteRegEx": null, "year": 2014}, {"title": "Geoffrey E Hinton", "author": ["David E Rumelhart"], "venue": "and Ronald J Williams.", "citeRegEx": "Rumelhart et al.1985", "shortCiteRegEx": null, "year": 1985}, {"title": "Sumit Chopra", "author": ["Alexander M Rush"], "venue": "and Jason Weston.", "citeRegEx": "Rush et al.2015", "shortCiteRegEx": null, "year": 2015}, {"title": "Oriol Vinyals", "author": ["Ilya Sutskever"], "venue": "and Quoc V Le.", "citeRegEx": "Sutskever et al.2014", "shortCiteRegEx": null, "year": 2014}, {"title": "Meire Fortunato", "author": ["Oriol Vinyals"], "venue": "and Navdeep Jaitly.", "citeRegEx": "Vinyals et al.2015", "shortCiteRegEx": null, "year": 2015}, {"title": "Richard Zemel", "author": ["Kelvin Xu", "Jimmy Ba", "Ryan Kiros", "Aaron Courville", "Ruslan Salakhutdinov"], "venue": "and Yoshua Bengio.", "citeRegEx": "Xu et al.2015", "shortCiteRegEx": null, "year": 2015}], "referenceMentions": [], "year": 2016, "abstractText": "In this work we explore deep generative models of text in which the latent representation of a document is itself drawn from a discrete language model distribution. We formulate a variational auto-encoder for inference in this model and apply it to the task of compressing sentences. In this application the generative model first draws a latent summary sentence from a background language model, and then subsequently draws the observed sentence conditioned on this latent summary. In our empirical evaluation we show that generative formulations of both abstractive and extractive compression yield state-of-the-art results when trained on a large amount of supervised data. Further, we explore semi-supervised compression scenarios where we show that it is possible to achieve performance competitive with previously proposed supervised models while training on a fraction of the supervised data.", "creator": "LaTeX with hyperref package"}}}