{"id": "1609.07451", "review": {"conference": "EMNLP", "VERSION": "v1", "DATE_OF_SUBMISSION": "23-Sep-2016", "title": "AMR-to-text generation as a Traveling Salesman Problem", "abstract": "The task of AMR-to-text generation is to generate grammatical text that sustains the semantic meaning for a given AMR graph. We at- tack the task by first partitioning the AMR graph into smaller fragments, and then generating the translation for each fragment, before finally deciding the order by solving an asymmetric generalized traveling salesman problem (AGTSP). A Maximum Entropy classifier is trained to estimate the traveling costs, and a TSP solver is used to find the optimized solution. The final model reports a BLEU score of 22.44 on the SemEval-2016 Task8 dataset.", "histories": [["v1", "Fri, 23 Sep 2016 18:12:12 GMT  (29kb,D)", "http://arxiv.org/abs/1609.07451v1", "accepted by EMNLP 2016"]], "COMMENTS": "accepted by EMNLP 2016", "reviews": [], "SUBJECTS": "cs.CL", "authors": ["linfeng song", "yue zhang", "xiaochang peng", "zhiguo wang", "daniel gildea"], "accepted": true, "id": "1609.07451"}, "pdf": {"name": "1609.07451.pdf", "metadata": {"source": "CRF", "title": "AMR-to-text generation as a Traveling Salesman Problem", "authors": ["Linfeng Song", "Yue Zhang", "Xiaochang Peng", "Zhiguo Wang", "Daniel Gildea"], "emails": [], "sections": [{"heading": null, "text": "AMR-to-text generation as a Traveling Salesman problemLinfeng Song1, Yue Zhang3, Xiaochang Peng1, Zhiguo Wang2 and Daniel Gildea1 1Department of Computer Science, University of Rochester, Rochester, NY 146272IBM T.J. Watson Research Center, Yorktown Heights, NY 10598 3Singapore University of Technology and DesignAbstractThe task of the AMR-to-text generation is to generate grammatical text that maintains the semantic meaning of a given AMR diagram. We tackle the task by first splitting the AMR diagram into smaller fragments and then generalizing the translation for each fragment before finally determining the order by solving an asymmetric generalized travel salesman (AGTSP) problem."}, {"heading": "1 Introduction", "text": "Illustrated in Figure 1, the nodes of an AMR graph (e.g. \"boy,\" \"go-01\" and \"want01\") finally represent concepts, and the edges (e.g. \"ARG0\" and \"ARG1\") represent relationships between concepts. AMR collectively encodes a number of different semantic phenomena, making it useful in applications such as answering questions and semantic machine translations. AMR serves as an intermediate representation for various text-text NLP applications, such as statistical machine translation (SMT) (Jones et al., 2012). The task of AMR-to-Text generation is to generate grammatical text that contains the same setic meaning as a given AMR graph."}, {"heading": "2 Method", "text": "We are reformulating the problem of AMR-to-text generation into an asymmetric generalized wandering vendor problem (AGTSP), a variant of TSP."}, {"heading": "2.1 TSP and its variants", "text": "Faced with an undirected chart GN with n cities, assuming that travel costs are incurred between each pair of cities, TSP tries to find a tour of the minimum total cost that each city visits exactly once. In contrast, the Asymmetric Wanderseller Problem (ATSP) tries to find a tour of the minimum total cost on a directed chart where travel costs are different between two nodes in each direction. Faced with a directed chart GD with n nodes grouped into m groups, the Asymmetric Generalized Wanderseller Problem (AGTSP) tries to find a tour of the minimum total cost that each group visits exactly once."}, {"heading": "2.2 AMR-to-text Generation as AGTSP", "text": "Considering an input AMR A, each node in the AGTSP diagram is represented as (c, r), where c is a concept in A and r = (Asub, Tsub) is a rule consisting of an AMR fragment containing c and a translation of the fragment. We put all nodes containing the same concept into a group, making every concept in the AMR exactly once.To show a brief example, consider the AMR fragment in Figure 1 and the following rules, r1 (w / want-01)."}, {"heading": "2.3 Rule Acquisition", "text": "We extract rules from a corpus of (sentence, AMR) pairs that use the method of Peng et al. (01). (01) Givenan aligned (sentence, AMR) pair, a phrase-fragment pair is a pair ([i, j], f), where [i, j] is a span of the sentence and f represents a connected and rooted AMR fragment. (1) Fragment decomposition forest consisting of all possible phrase-fragment pairs that contain the alignment agreement for phrase-based MT (Koehn et al., 2003). The rules we use for the generation are the result of applying an MCMC process to learn a set of probable phrase-fragment pairs from the forests that contain all possible pairs. (2015) is that while they require the string side to be tight (does not include the unaligned words on both sides), we expand the more narrow phrases to include both sides."}, {"heading": "2.4 Traveling cost", "text": "Considering an AGTSP graph whose nodes are grouped into m-groups, we define the travel costs for a trip T in Equation 1: Cost (ns, ne) = \u2212 m \u2211 i = 0 log p (\"yes\" | nTi, nTi + 1) (1), where nT0 = ns, nTm + 1 = ne and each nTi (i) [1.. m]) belong to a group that is different from all the others. Previous methods (\"yes\" | nj, ni) represent a learned value for a move from nj to ni. Decisions before nTi are independent of the choice of nTi + 1 given nTi due to the Markovian property of the TSP problem. Previous methods (Zaslavskiy et al., 2009) evaluate the travel costs p (nTi + 1 | nTi) using the language model nTi."}, {"heading": "3 Experiments", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "3.1 Setup", "text": "We use the data set from SemEval-2016 Task8 (Meaning Representation Parsing), which contains 16833 training instances, 1368 development instances and 1371 test instances. Each instance consists of an AMR diagram and a sentence representing the same meaning. Rules are extracted from the training data, and hyperparameters are matched to the developer set. For tuning and testing, we filter out sentences that have more than 30 words, resulting in 1103 development instances and 1055 test instances. We train a 4 gram language model (LM) with Gigaword (LDC2011T07) and use BLEU (Papineni et al., 2002) as a benchmark. To solve the AGTSP, we use Or-Tool3.Our graph-to-string rules are reminiscent of phrase-to-string rules reminiscent of phrase-to-string rules in phrase-based MT (BMPT)."}, {"heading": "3.2 Results", "text": "The results are shown in Table 1. Our method (All) exceeds the baseline (PBMT) 3https: / / developers.google.com / optimization / 4http: / / www.statmt.org / moses / 5https: / / github.com / jflanigan / jamr / tree / Generator on both the developer and the test kit. PBMT does not exceed OnlyBigramLM and OnlyInducedRule, demonstrating that our control induction algorithm is effective. We look at rooted and connected fragments from the AMR diagram, and the TSP solver finds better solutions than beam detection, as is consistent with Zaslavskiy et al. (2009). In addition, OnlyInducedRule is significantly better than OnlyConceptRule, demonstrating the importance of induced rules for performance. This also confirms the reason that All Performing Performs exceed PBMT. This result confirms our expectation that the AMR model will be used as an AMR model, which includes the Rule 4."}, {"heading": "3.3 Analysis and Discussions", "text": "We analyze All and JAMR-gen using an example of AMR and show the AMR graph, the reference, and the results in Table 2. First, All and JAMR-gen output a reasonable translation that contains most of the meaning of AMR. Secondly, All does not recognize the \"boy\" as a subject, because the functionality does not include edge labels such as \"ARG0\" and \"ARG1.\" Finally, neither All nor JAMR-gen can deal with the situation when a re-entry node (such as \"b / boy\" in the example diagram of Table 2) needs to be translated twice. This limitation applies to both works."}, {"heading": "4 Related Work", "text": "Our work refers to earlier work on AMR (Banarescu et al., 2013). There was a list of papers on AMR analysis (Flanigan et al., 2014; Wang et al., 2015; Peng et al., 2015; Vanderwende et al., 2015; Pust et al., 2015; Artzi et al., 2015) that predict the AMR structures for a given sentence. In the reverse direction, Flanigan et al. (2016) and our work here apply synchronous rules consisting of AMR diagrams and text to transfer an AMR diagram directly into a sentence. In addition to AMR analysis and generation, there have also been papers that use AMR as a semantic representation in machine translation (Jones et al., 2012)."}, {"heading": "5 Conclusion", "text": "In summary, we showed that a TSP solver with some real properties can be useful for AMR tottext generation. Our method is based on a set of graph-to-string rules, but is significantly better than a PBMT-based baseline. This shows that our control induction algorithm is effective and that the TSP solver provides better solutions than beam search."}, {"heading": "Acknowledgments", "text": "We are grateful for the help of Jeffrey Flanigan, Lin Zhao and Yifan He. This work was funded by NSF IIS-1446996 and a Google Faculty Research Award. Yue Zhang is funded by NSFC61572245 and T2MOE201301 of the Singapore Ministry of Education."}], "references": [{"title": "Broad-coverage CCG semantic parsing with AMR", "author": ["Yoav Artzi", "Kenton Lee", "Luke Zettlemoyer."], "venue": "Conference on Empirical Methods in Natural Language Processing (EMNLP-15), pages 1699\u20131710.", "citeRegEx": "Artzi et al\\.,? 2015", "shortCiteRegEx": "Artzi et al\\.", "year": 2015}, {"title": "Abstract meaning representation for sembanking", "author": ["Laura Banarescu", "Claire Bonial", "Shu Cai", "Madalina Georgescu", "Kira Griffitt", "Ulf Hermjakob", "Kevin Knight", "Philipp Koehn", "Martha Palmer", "Nathan Schneider."], "venue": "Proceedings of the 7th Linguistic", "citeRegEx": "Banarescu et al\\.,? 2013", "shortCiteRegEx": "Banarescu et al\\.", "year": 2013}, {"title": "Broad coverage multilingual deep sentence generation with a stochastic multi-level realizer", "author": ["Bernd Bohnet", "Leo Wanner", "Simon Mill", "Alicia Burga."], "venue": "Proceedings of the 23rd International Conference on Computational Linguistics (COLING-10),", "citeRegEx": "Bohnet et al\\.,? 2010", "shortCiteRegEx": "Bohnet et al\\.", "year": 2010}, {"title": "Smatch: an evaluation metric for semantic feature structures", "author": ["Shu Cai", "Kevin Knight."], "venue": "Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics (ACL-13), pages 748\u2013752.", "citeRegEx": "Cai and Knight.,? 2013", "shortCiteRegEx": "Cai and Knight.", "year": 2013}, {"title": "A discriminative graph-based parser for the abstract meaning representation", "author": ["Jeffrey Flanigan", "Sam Thomson", "Jaime Carbonell", "Chris Dyer", "Noah A. Smith."], "venue": "Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (ACL-", "citeRegEx": "Flanigan et al\\.,? 2014", "shortCiteRegEx": "Flanigan et al\\.", "year": 2014}, {"title": "Generation from abstract meaning representation using tree transducers", "author": ["Jeffrey Flanigan", "Chris Dyer", "Noah A. Smith", "Jaime Carbonell."], "venue": "Proceedings of the 2016 Meeting of the North American chapter of the Association for Computational Linguistics", "citeRegEx": "Flanigan et al\\.,? 2016", "shortCiteRegEx": "Flanigan et al\\.", "year": 2016}, {"title": "Semanticsbased machine translation with hyperedge replacement grammars", "author": ["Bevan Jones", "Jacob Andreas", "Daniel Bauer", "Karl Moritz Hermann", "Kevin Knight."], "venue": "Proceedings of the International Conference on Computational Linguistics (COLING-12),", "citeRegEx": "Jones et al\\.,? 2012", "shortCiteRegEx": "Jones et al\\.", "year": 2012}, {"title": "Statistical phrase-based translation", "author": ["Philipp Koehn", "Franz Josef Och", "Daniel Marcu."], "venue": "Proceedings of the 2003 Meeting of the North American chapter of the Association for Computational Linguistics (NAACL-03), pages 48\u201354.", "citeRegEx": "Koehn et al\\.,? 2003", "shortCiteRegEx": "Koehn et al\\.", "year": 2003}, {"title": "Bleu: a method for automatic evaluation of machine translation", "author": ["Kishore Papineni", "Salim Roukos", "Todd Ward", "WeiJing Zhu."], "venue": "Proceedings of the 40th Annual Conference of the Association for Computational Linguistics (ACL-02), pages 311\u2013318.", "citeRegEx": "Papineni et al\\.,? 2002", "shortCiteRegEx": "Papineni et al\\.", "year": 2002}, {"title": "A synchronous hyperedge replacement grammar based approach for AMR parsing", "author": ["Xiaochang Peng", "Linfeng Song", "Daniel Gildea."], "venue": "Proceedings", "citeRegEx": "Peng et al\\.,? 2015", "shortCiteRegEx": "Peng et al\\.", "year": 2015}, {"title": "Parsing English into abstract meaning representation using syntax-based machine translation", "author": ["Michael Pust", "Ulf Hermjakob", "Kevin Knight", "Daniel Marcu", "Jonathan May."], "venue": "Conference on Empirical Methods in Natural Language Processing (EMNLP-", "citeRegEx": "Pust et al\\.,? 2015", "shortCiteRegEx": "Pust et al\\.", "year": 2015}, {"title": "Building applied natural language generation systems", "author": ["Ehud Reiter", "Robert Dale."], "venue": "Natural Language Engineering, 3(1):57\u201387.", "citeRegEx": "Reiter and Dale.,? 1997", "shortCiteRegEx": "Reiter and Dale.", "year": 1997}, {"title": "Joint morphological generation and syntactic linearization", "author": ["Linfeng Song", "Yue Zhang", "Kai Song", "Qun Liu."], "venue": "Proceedings of the National Conference on Artificial Intelligence (AAAI-14), pages 1522\u2013 1528.", "citeRegEx": "Song et al\\.,? 2014", "shortCiteRegEx": "Song et al\\.", "year": 2014}, {"title": "An AMR parser for English, French, German, Spanish and Japanese and a new AMR-annotated corpus", "author": ["Lucy Vanderwende", "Arul Menezes", "Chris Quirk."], "venue": "Proceedings of the 2015 Meeting of the North American chapter of the Association for Computa-", "citeRegEx": "Vanderwende et al\\.,? 2015", "shortCiteRegEx": "Vanderwende et al\\.", "year": 2015}, {"title": "Improving grammaticality in statistical sentence generation: Introducing a dependency spanning tree algorithm with an argument satisfaction model", "author": ["Stephen Wan", "Mark Dras", "Robert Dale", "C\u00e9cile Paris."], "venue": "Proceedings of the 12th Conference of the European", "citeRegEx": "Wan et al\\.,? 2009", "shortCiteRegEx": "Wan et al\\.", "year": 2009}, {"title": "A transition-based algorithm for AMR parsing", "author": ["Chuan Wang", "Nianwen Xue", "Sameer Pradhan."], "venue": "Proceedings of the 2015 Meeting of the North American chapter of the Association for Computational Linguistics (NAACL-15), pages 366\u2013375.", "citeRegEx": "Wang et al\\.,? 2015", "shortCiteRegEx": "Wang et al\\.", "year": 2015}, {"title": "Perceptron reranking for CCG realization", "author": ["Michael White", "Rajakrishnan Rajkumar."], "venue": "Conference on Empirical Methods in Natural Language Processing (EMNLP-09), pages 410\u2013419.", "citeRegEx": "White and Rajkumar.,? 2009", "shortCiteRegEx": "White and Rajkumar.", "year": 2009}, {"title": "Reining in CCG chart realization", "author": ["Michael White."], "venue": "International Conference on Natural Language Generation (INLG-04), pages 182\u2013191.", "citeRegEx": "White.,? 2004", "shortCiteRegEx": "White.", "year": 2004}, {"title": "Phrase-based statistical machine translation as a traveling salesman problem", "author": ["Mikhail Zaslavskiy", "Marc Dymetman", "Nicola Cancedda."], "venue": "Proceedings of the 47th Annual Meeting of the Association for Computational Linguistics (ACL-09), pages 333\u2013341.", "citeRegEx": "Zaslavskiy et al\\.,? 2009", "shortCiteRegEx": "Zaslavskiy et al\\.", "year": 2009}, {"title": "Discriminative syntax-based word ordering for text generation", "author": ["Yue Zhang", "Stephen Clark."], "venue": "Computational Linguistics, 41(3):503\u2013538.", "citeRegEx": "Zhang and Clark.,? 2015", "shortCiteRegEx": "Zhang and Clark.", "year": 2015}, {"title": "Partial-tree linearization: Generalized word ordering for text synthesis", "author": ["Yue Zhang."], "venue": "Proceedings of the International Joint Conference on Artificial Intelligence (IJCAI-13), pages 2232\u20132238.", "citeRegEx": "Zhang.,? 2013", "shortCiteRegEx": "Zhang.", "year": 2013}], "referenceMentions": [{"referenceID": 1, "context": "Abstract Meaning Representation (AMR) (Banarescu et al., 2013) is a semantic formalism encoding the meaning of a sentence as a rooted, directed graph.", "startOffset": 38, "endOffset": 62}, {"referenceID": 6, "context": "AMR has served as an intermediate representation for various text-to-text NLP applications, such as statistical machine translation (SMT) (Jones et al., 2012).", "startOffset": 138, "endOffset": 158}, {"referenceID": 1, "context": "usually has multiple corresponding sentences, and syntactic structure and function words are abstracted away when transforming a sentence into AMR (Banarescu et al., 2013).", "startOffset": 147, "endOffset": 171}, {"referenceID": 4, "context": "There has been work dealing with text-to-AMR parsing (Flanigan et al., 2014; Wang et al., 2015; Peng et al., 2015; Vanderwende et al., 2015; Pust et al., 2015; Artzi et al., 2015).", "startOffset": 53, "endOffset": 179}, {"referenceID": 15, "context": "There has been work dealing with text-to-AMR parsing (Flanigan et al., 2014; Wang et al., 2015; Peng et al., 2015; Vanderwende et al., 2015; Pust et al., 2015; Artzi et al., 2015).", "startOffset": 53, "endOffset": 179}, {"referenceID": 9, "context": "There has been work dealing with text-to-AMR parsing (Flanigan et al., 2014; Wang et al., 2015; Peng et al., 2015; Vanderwende et al., 2015; Pust et al., 2015; Artzi et al., 2015).", "startOffset": 53, "endOffset": 179}, {"referenceID": 13, "context": "There has been work dealing with text-to-AMR parsing (Flanigan et al., 2014; Wang et al., 2015; Peng et al., 2015; Vanderwende et al., 2015; Pust et al., 2015; Artzi et al., 2015).", "startOffset": 53, "endOffset": 179}, {"referenceID": 10, "context": "There has been work dealing with text-to-AMR parsing (Flanigan et al., 2014; Wang et al., 2015; Peng et al., 2015; Vanderwende et al., 2015; Pust et al., 2015; Artzi et al., 2015).", "startOffset": 53, "endOffset": 179}, {"referenceID": 0, "context": "There has been work dealing with text-to-AMR parsing (Flanigan et al., 2014; Wang et al., 2015; Peng et al., 2015; Vanderwende et al., 2015; Pust et al., 2015; Artzi et al., 2015).", "startOffset": 53, "endOffset": 179}, {"referenceID": 0, "context": ", 2015; Artzi et al., 2015). On the other hand, relatively little work has been done on AMR-to-text generation. One recent exception is Flanigan et al. (2016), who first generate a span-", "startOffset": 8, "endOffset": 159}, {"referenceID": 3, "context": "(2015), and use the rule matching algorithm of Cai and Knight (2013). For decoding the fragments and synthesizing the output, we define a cut to be a subset of matched rules without overlap that covers the AMR, and an ordered cut to be a cut with the rules being ordered.", "startOffset": 47, "endOffset": 69}, {"referenceID": 7, "context": "A fragment decomposition forest consists of all possible phrase-fragment pairs that satisfy the alignment agreement for phrase-based MT (Koehn et al., 2003).", "startOffset": 136, "endOffset": 156}, {"referenceID": 8, "context": "We extract rules from a corpus of (sentence, AMR) pairs using the method of Peng et al. (2015). Given an aligned (sentence, AMR) pair, a phrase-fragment pair is a pair ([i, j], f), where [i, j] is a span of the sentence and f represents a connected and rooted AMR fragment.", "startOffset": 76, "endOffset": 95}, {"referenceID": 7, "context": "A fragment decomposition forest consists of all possible phrase-fragment pairs that satisfy the alignment agreement for phrase-based MT (Koehn et al., 2003). The rules that we use for generation are the result of applying an MCMC procedure to learn a set of likely phrase-fragment pairs from the forests containing all possible pairs. One difference from the work of Peng et al. (2015) is that, while they require the string side to be tight (does not include unaligned words on both sides), we expand the tight phrases to incorporate unaligned words on both sides.", "startOffset": 137, "endOffset": 386}, {"referenceID": 18, "context": "Previous methods (Zaslavskiy et al., 2009) evaluate traveling costs p(nTi+1 |nTi) by using a language model.", "startOffset": 17, "endOffset": 42}, {"referenceID": 18, "context": "Previous methods (Zaslavskiy et al., 2009) evaluate traveling costs p(nTi+1 |nTi) by using a language model. Inevitably some rules may only cover one translation word, making only bigram language models naturally applicable. Zaslavskiy et al. (2009) introduces a method for incorporating a trigram language model.", "startOffset": 18, "endOffset": 250}, {"referenceID": 8, "context": "We train a 4-gram language model (LM) with gigaword (LDC2011T07), and use BLEU (Papineni et al., 2002) as the evaluation metric.", "startOffset": 79, "endOffset": 102}, {"referenceID": 5, "context": "We also compare with JAMRgen5 (Flanigan et al., 2016), which is trained on the same dataset but with a 5-gram LM from gigaword (LDC2011T07).", "startOffset": 30, "endOffset": 53}, {"referenceID": 18, "context": "We consider rooted and connected fragments from the AMR graph, and the TSP solver finds better solutions than beam search, as consistent with Zaslavskiy et al. (2009). In addition, OnlyInducedRule is significantly better than OnlyConceptRule, showing the importance of induced rules on performance.", "startOffset": 142, "endOffset": 167}, {"referenceID": 1, "context": "Our work is related to prior work on AMR (Banarescu et al., 2013).", "startOffset": 41, "endOffset": 65}, {"referenceID": 4, "context": "There has been a list of work on AMR parsing (Flanigan et al., 2014; Wang et al., 2015; Peng et al., 2015; Vanderwende et al., 2015; Pust et al., 2015; Artzi et al., 2015), which predicts the AMR structures for a given sentence.", "startOffset": 45, "endOffset": 171}, {"referenceID": 15, "context": "There has been a list of work on AMR parsing (Flanigan et al., 2014; Wang et al., 2015; Peng et al., 2015; Vanderwende et al., 2015; Pust et al., 2015; Artzi et al., 2015), which predicts the AMR structures for a given sentence.", "startOffset": 45, "endOffset": 171}, {"referenceID": 9, "context": "There has been a list of work on AMR parsing (Flanigan et al., 2014; Wang et al., 2015; Peng et al., 2015; Vanderwende et al., 2015; Pust et al., 2015; Artzi et al., 2015), which predicts the AMR structures for a given sentence.", "startOffset": 45, "endOffset": 171}, {"referenceID": 13, "context": "There has been a list of work on AMR parsing (Flanigan et al., 2014; Wang et al., 2015; Peng et al., 2015; Vanderwende et al., 2015; Pust et al., 2015; Artzi et al., 2015), which predicts the AMR structures for a given sentence.", "startOffset": 45, "endOffset": 171}, {"referenceID": 10, "context": "There has been a list of work on AMR parsing (Flanigan et al., 2014; Wang et al., 2015; Peng et al., 2015; Vanderwende et al., 2015; Pust et al., 2015; Artzi et al., 2015), which predicts the AMR structures for a given sentence.", "startOffset": 45, "endOffset": 171}, {"referenceID": 0, "context": "There has been a list of work on AMR parsing (Flanigan et al., 2014; Wang et al., 2015; Peng et al., 2015; Vanderwende et al., 2015; Pust et al., 2015; Artzi et al., 2015), which predicts the AMR structures for a given sentence.", "startOffset": 45, "endOffset": 171}, {"referenceID": 6, "context": "In addition to AMR parsing and generation, there has also been work using AMR as a semantic representation in machine translation (Jones et al., 2012).", "startOffset": 130, "endOffset": 150}, {"referenceID": 0, "context": ", 2015; Artzi et al., 2015), which predicts the AMR structures for a given sentence. On the reverse direction, Flanigan et al. (2016) and our work here study sentence generation from a given AMR graph.", "startOffset": 8, "endOffset": 134}, {"referenceID": 0, "context": ", 2015; Artzi et al., 2015), which predicts the AMR structures for a given sentence. On the reverse direction, Flanigan et al. (2016) and our work here study sentence generation from a given AMR graph. Different from Flanigan et al. (2016) who map a input AMR graph into a tree before linearization, we apply synchronous rules consisting of AMR graph fragments and text to directly transfer a AMR graph into a sentence.", "startOffset": 8, "endOffset": 240}, {"referenceID": 11, "context": "Our work also belongs to the task of text generation (Reiter and Dale, 1997).", "startOffset": 53, "endOffset": 76}, {"referenceID": 14, "context": "There has been work on generating natural language text from a bag of words (Wan et al., 2009; Zhang and Clark, 2015), surface syntactic trees (Zhang, 2013; Song et al.", "startOffset": 76, "endOffset": 117}, {"referenceID": 19, "context": "There has been work on generating natural language text from a bag of words (Wan et al., 2009; Zhang and Clark, 2015), surface syntactic trees (Zhang, 2013; Song et al.", "startOffset": 76, "endOffset": 117}, {"referenceID": 20, "context": ", 2009; Zhang and Clark, 2015), surface syntactic trees (Zhang, 2013; Song et al., 2014), deep semantic graphs (Bohnet et al.", "startOffset": 56, "endOffset": 88}, {"referenceID": 12, "context": ", 2009; Zhang and Clark, 2015), surface syntactic trees (Zhang, 2013; Song et al., 2014), deep semantic graphs (Bohnet et al.", "startOffset": 56, "endOffset": 88}, {"referenceID": 2, "context": ", 2014), deep semantic graphs (Bohnet et al., 2010) and logical forms (White, 2004; White and Rajkumar, 2009).", "startOffset": 30, "endOffset": 51}, {"referenceID": 17, "context": ", 2010) and logical forms (White, 2004; White and Rajkumar, 2009).", "startOffset": 26, "endOffset": 65}, {"referenceID": 16, "context": ", 2010) and logical forms (White, 2004; White and Rajkumar, 2009).", "startOffset": 26, "endOffset": 65}], "year": 2016, "abstractText": "The task of AMR-to-text generation is to generate grammatical text that sustains the semantic meaning for a given AMR graph. We attack the task by first partitioning the AMR graph into smaller fragments, and then generating the translation for each fragment, before finally deciding the order by solving an asymmetric generalized traveling salesman problem (AGTSP). A Maximum Entropy classifier is trained to estimate the traveling costs, and a TSP solver is used to find the optimized solution. The final model reports a BLEU score of 22.44 on the SemEval-2016 Task8 dataset.", "creator": "TeX"}}}