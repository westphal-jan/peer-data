{"id": "1609.07479", "review": {"conference": "EMNLP", "VERSION": "v1", "DATE_OF_SUBMISSION": "23-Sep-2016", "title": "Incorporating Relation Paths in Neural Relation Extraction", "abstract": "Distantly supervised relation extraction has been widely used to find novel relational facts from plain text. To predict the relation between a pair of two target entities, existing methods solely rely on those direct sentences containing both entities. In fact, there are also many sentences containing only one of the target entities, which provide rich and useful information for relation extraction. To address this issue, we build inference chains between two target entities via intermediate entities, and propose a path-based neural relation extraction model to encode the relational semantics from both direct sentences and inference chains. Experimental results on real-world datasets show that, our model can make full use of those sentences containing only one target entity, and achieves significant and consistent improvements on relation extraction as compared with baselines.", "histories": [["v1", "Fri, 23 Sep 2016 19:59:51 GMT  (287kb,D)", "http://arxiv.org/abs/1609.07479v1", "9 pages, 3 figures, 4 tables"], ["v2", "Tue, 15 Aug 2017 08:56:42 GMT  (287kb,D)", "http://arxiv.org/abs/1609.07479v2", "9 pages, 3 figures, 4 tables. Code and dataset available"], ["v3", "Wed, 13 Sep 2017 19:30:06 GMT  (172kb,D)", "http://arxiv.org/abs/1609.07479v3", "Proceedings of EMNLP 2017. Code and dataset available"]], "COMMENTS": "9 pages, 3 figures, 4 tables", "reviews": [], "SUBJECTS": "cs.CL", "authors": ["wenyuan zeng", "yankai lin", "zhiyuan liu", "maosong sun"], "accepted": true, "id": "1609.07479"}, "pdf": {"name": "1609.07479.pdf", "metadata": {"source": "CRF", "title": "Incorporating Relation Paths in Neural Relation Extraction", "authors": ["Wenyuan Zeng", "Yankai Lin", "Zhiyuan Liu", "Maosong Sun"], "emails": ["(liuzy@tsinghua.edu.cn)."], "sections": [{"heading": "1 Introduction", "text": "In fact, most of them are able to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to"}, {"heading": "2 Related Work", "text": "Although Santos et al., 2010) uses this method in the field of biology, it is considered retrospectively (Mintz et al., 2009) only as a single-label issue. To alleviate this problem, (Riedel et al., 2010) considers each sentence as a training instance and allows multiple instances to share the same label, but prohibits more than one label, namely a multi-label issue. Furthermore, (Hoffmann et al., 2011; Surdeanu et al., 2012) uses multi-label learning in relation extraction. The major drawback of these methods is that they obtain most features directly from NLP tools, and thus the errors generated by NLP tools spread and limit their performance."}, {"heading": "3 Methodology", "text": "For a pair of target units, a set of corresponding direct sentences S = {s1, s2, \u00b7 \u00b7, sn} that contain this entity pair, and a set of relation paths P = {p1, p2, \u00b7 \u00b7 \u00b7, pm}, our model measures the security of each relationship for that entity pair. In this section, we present our model in three parts: (1) Text encoder. For a set of two corresponding target units, we use a CNN to embed the sentence in a semantic space, and measure the probability of relationships given with that sentence. (2) Relation path encoder. For a relationship path between the target units, we measure the probability of the relationship r due to the relation path. (3) Common model. We integrate the information from direct sentences and relation paths, and then predict the reliability of each relationship."}, {"heading": "3.1 Text Encoder", "text": "As shown in Fig. 2, we use a CNN to extract information from text. Faced with a set of sentences from an entity pair, we first transform each sentence into its distributed representation, and then predict the relationship using the most representative sentence through a multi-level learning mechanism."}, {"heading": "3.1.1 Input Vector", "text": "First, we transform the words {w1, w2, \u00b7 \u00b7, wl} in the sentence s into vectors. For each word wi, we use word embedding to encode its syntactic and semantic meanings, and use position embedding to encode its position information. We then link both word embedding and position embedding to form the input vector of wi for CNN.Word Embeddings. Suppose V is the collection of all raw words, and we represent each word by a dw-dimensional vector. Thus, the words become WE through a word embedding matrix, with each column of WE being the representation of a word. Position embedding. The order of words in a sentence is important to understand this sentence. For example, those words that are closer to the target units are usually more informative to follow this relationship."}, {"heading": "3.1.2 Convolution, Max-pooling Layers", "text": "When processing a sentence, it is a great challenge that important information is likely to occur in all parts of that sentence. In addition, the length l of a sentence could also vary greatly. To extract local characteristics, we use CNN to encode all local characteristics regardless of the sentence length. We first apply a folding layer to extract all possible local characteristics, and then select the most important one via the max pooling level. To extract local characteristics, the folding layer first concatenates a sequence of word embeddings within a sliding window to be vector qi of dimension k \u00b7 d: qi = wi \u2212 k + 1: i (1 \u2264 i \u2264 l + k \u2212 1), (1) where k is the size of the window, and we also insert all index words so that they are zero vectors. It then multiplies Qi with a convolution matrix W: qi = wi \u2212 k + 1: i (k \u00d7 d), where DC is the dimension of the sentence embedding."}, {"heading": "3.1.3 Multi-Instance Learning", "text": "Next, we apply a Softmax classifier to the sentence representation s \u03b8 to predict the corresponding relationship. We define the conditional probability of the relationship r as follows: p (r | \u03b8, s) = exp (er) \u2211 nr i = 1 exp (ei), (4) where ei measures how well this sentence matches the relationship ri and nr, the number of relationships is. Specifically, e from: e = Us + v, (5) could be calculated, where U-Rnr \u00b7 dc is the coefficient matrix of relations and v-Rnr is a bias vector. Similar to (Zeng et al., 2015) we define the problem of incorrect labeling in remote monitoring by selecting a sentence from the set of all direct sentences S = {s1, s2, \u00b7 \u00b7 \u00b7 sm} that corresponds to the entity pair (h, t). Similarly to (Zeng et al., 2015) we define the score of this corresponding relationship (max-e), and its corresponding (e-e-e) setting (s)."}, {"heading": "3.2 Relation Path Encoder", "text": "Relationpath encoder measures the probability of the relationship r (h, e), (e, t), and the corresponding relations are rA, rB. Each of (h, e) and (e, t) corresponds to at least one sentence in the text. Our model calculates the probability of the relationship r on p1 as follows: p (r, t), p (rA, rB) = exp (or).nr i = 1 exp (oi), (8) where oi measures how well the relationship r corresponds to the relation path (rA, rB) (rB). Inspired by working on learning the relationship path representation (Lin et al., 2015), our model first transforms the relationship r on its distribution rotation, ivector r relationship r = condition for the relationship A (rcondition A = 1 condition)."}, {"heading": "3.3 Joint Model", "text": "Given any entity pair (h, t), these sentences S, which they directly mention, and the relationship paths P between them, we define the global score function in relation to a possible relationship r as, L (h, r, t) = E (h, r, t | S) + \u03b1G (h, r, t | P), (12) where E (h, r, t | S) models the correlation between r and (h, t) from direct sentences, G (h, r, t | P) models the consequential correlation between relationship r and multiple sentence paths P. \u03b1 corresponds to (1 \u2212 E (h, r, t | S) times a constant \u03b2. This term serves to represent the relative weight between direct sentences and relationship paths, since we do not need to pay much attention to additional information when CNN has already provided a reliable result, namely E (h, r, t | S) times a constant. One of the advantages of this common model is to spread the problem of error."}, {"heading": "3.4 Optimization and Implementation Details", "text": "In this section, we present optimization details of our model. The superordinate objective function is defined as: J (\u03b8) = \u2211 (h, r, t) log (L (h, r, t)), (13), with the summation of the protocol loss of all entities in text and \u03b8 representing the model parameters. To solve this optimization problem, we use stochastic gradient descent (SGD) to maximize our objective function. First, we initialize WE with the results of the Skip-gram model and randomly initialize other parameters. Then, we iterate SGD by selecting a minibatch from the training set, and stop learning when the performance of the model is converged during validation. In test mode, we include this relationship path in the training set to enrich the path information. In the implementation of CNN, we assume the dropout (Srivastaal is defined by CNN to be particularly effective on the output of 2014)."}, {"heading": "4 Experiments", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "4.1 Dataset and Evaluation Metrics", "text": "We are building a novel data set that contains more up-to-date facts and richer relationship structures, such as the number of relationships / relationship paths compared to existing similar data sets, making it more similar to real-world cases and therefore a more appropriate choice for evaluating RE systems. We are creating the data set by aligning Wikidata1 relationships with the1https: / / www.wikidata.org / New York Times Corpus (NYT). Wikidata is a large, growing knowledge base that contains more than 80 million triple-facts and 20 million entities. We first sample a subset of S from Wikidata by maintaining a subset of entities that are also available in freebase, and restrict the relationships according to their frequency, resulting in 4,574,665 triplets with 1,045,385 entities and 99 relations. We randomly divide the subset of the knowledge base into three data groups, including training, testing and validation."}, {"heading": "4.2 Experimental Settings", "text": "In this thesis, we use the word2vec tool 2 to pre-train the word embedding in the NYT corpus. We keep the words that occur more than 100 times in the corpus as vocabulary. We adjust our model to the validation quantity by determining the optimal parameters using the grid search, which is displayed in bold. We select the learning rate for SGD \u03bb {0,1, 0,01, 0,001}, the embedding size dc \u00b2 {50, 60, \u00b7 \u00b7, 230, \u00b7, 300}, the window size k \u00b2 {1, 2, 3, 4, 5} and the minicharge size B \u00b2 {40, 160, 640}. We also choose the embedding size dR \u00b2 {5, 10, \u00b7, 40, \u00b7 \u00b7 \u00b7, 60} and the weight for information from the relation paths \u03b2 \u00b2 {0,5, 1, \u00b7 \u00b7, 5}."}, {"heading": "4.3 Effect of Relation Paths", "text": "To demonstrate the effectiveness of our approach, we are comparing it empirically with other state-of-the-art in-store evaluations methods, including: (1) CNN + rand represents the CNN model reported in (Zeng et al., 2014). (2) CNN + max represents the CNN model with multiple learning instances used in (Zeng et al., 2015). (3) Path + rand / rand is our model with these two multi-instance settings. We implement (1), (2) ourselves, which achieve comparable results to those reported in these papers.Figure 3 shows the precision / recall curves of all methods. From the figure, we can conclude that: (1) Our methods outperform their countermethods and achieve higher precision across almost the entire spectrum of the recall. They also improve memory by 20% without decreasing precision. These results demonstrate the effectiveness of our approach. (2) As the recall rate increases in comparison to our models, improvements in terms of our recall rate increase."}, {"heading": "4.4 Model Robustness under Different Percentages of Noises", "text": "In the task of relation extraction, there are many sounds in the text that may affect the performance of the model, such as those \"no relation\" entity pairs. Therefore, it is important to check the robustness of our model in the presence of mass noise. In this context, we evaluate these models in three settings, testing the same relational facts and different percentages of \"no relation\" sentences. In each experiment, we extract the first 20,000 relational facts according to the predictive values of the model and report on the precision @ top 10%, @ top 20%, @ top 50% and F1 in Table 2. From the table, we can see the following: (1) With respect to all the ratings, our models achieve the best performance compared to other methods in all test settings. It shows the effectiveness of our approach. (2) Although the values of all models decrease as the values of the sounds increase, we find that the values of Path + rand / max decrease much less than their counterparts in all test settings."}, {"heading": "4.5 Effectiveness of Extracted Features in Zero-Shot Situation", "text": "In this experiment, we demonstrate the effectiveness of the extracted characteristics from our model. Since CNN-based models have already been able to successfully extract relationships from individual sets, we put our experiment into a new scenario: predicting the relationship between units that did not appear in the same set. A natural approach is to build a relationship path between this zero-shot pair. We assume that we can make a prediction about (h, t) once we know the information of (h, e) and (e, t). Therefore, we build the training path by extracting all such relationship paths and their sentences from the training text, and similar to testing. To test the effectiveness of characteristics, we encode sentences through CNN + rand / max, path + rand / max, performs the concatenation of the sentence vector and its sentences from the training text and similar to testing. To test the effectiveness of characteristics, we encode vrand + sentences by CNN + rand / max / max, although we could clearly encode the result by encoding CNN + max / max sentences from the training text and similar to testing."}, {"heading": "4.6 Case study", "text": "Table 4 shows some representative sequences from the test data set. These examples cannot be correctly predicted by the original CNN model, but will be corrected later using our model. We show the test instance and its correct relationships, as well as the sequences that the model uses. In the first example, the test sentence does not directly express the spouse of the relationship, the proof of that relationship appears in a broader context in the NYT. However, if we use path # 1 and path # 2, we could easily conclude that Rebecca and Issac are spouses. In the second example, the test sentence also does not show the relationship. However, with the help of an intermediate unit, Dijibouti, our model predicts that Somalia divides the border with Ethiopia. Note that this sequence chain does not always hold, but our model could well capture this uncertainty through a Softmax operation. In general, our model can use common sense from sequence chains."}, {"heading": "5 Conclusion and Future Work", "text": "Compared to existing models of neural relationship extraction, our model is able to use sets that contain both two target units and only one target unit, and is more robust for noisy data extraction. Experimental results from real data sets show that our model achieves significant and consistent improvements in relationship extraction compared to baseline. Codes and data sets are released upon adoption. In the future, we will explore the following directions: (1) Our relationship extraction model only takes into account the relationship path in simple texts. However, the relationship paths in KBs are also important additions for relationship extraction. In the future, we will investigate the combination of relationship paths from simple texts and KBs for relationship extraction. (2) This paper only takes into account the correlations between inference chains and direct sentences. More complicated correlations between relationship paths may exist."}], "references": [{"title": "Dbpedia: A nucleus for a web of open data", "author": ["Auer et al.2007] S\u00f6ren Auer", "Christian Bizer", "Georgi Kobilarov", "Jens Lehmann", "Richard Cyganiak", "Zachary Ives"], "venue": null, "citeRegEx": "Auer et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Auer et al\\.", "year": 2007}, {"title": "Learning deep architectures for ai. Foundations and trends R", "author": ["Yoshua Bengio"], "venue": null, "citeRegEx": "Bengio.,? \\Q2009\\E", "shortCiteRegEx": "Bengio.", "year": 2009}, {"title": "Freebase: a collaboratively created graph database for structuring human knowledge", "author": ["Colin Evans", "Praveen Paritosh", "Tim Sturge", "Jamie Taylor"], "venue": "In Proceedings of KDD,", "citeRegEx": "Bollacker et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Bollacker et al\\.", "year": 2008}, {"title": "Constructing biological knowledge bases by extracting information from text sources", "author": ["Craven et al.1999] Mark Craven", "Johan Kumlien"], "venue": "In Proceedings of ISMB,", "citeRegEx": "Craven and Kumlien,? \\Q1999\\E", "shortCiteRegEx": "Craven and Kumlien", "year": 1999}, {"title": "Deep convolutional neural networks for sentiment analysis of short texts", "author": ["dos Santos", "Ma\u0131ra Gatti"], "venue": "In Proceedings of COLING", "citeRegEx": "Santos et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Santos et al\\.", "year": 2014}, {"title": "Classifying relations by ranking with convolutional neural networks", "author": ["Bing Xiang", "Bowen Zhou"], "venue": "In Proceedings of ACL,", "citeRegEx": "Santos et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Santos et al\\.", "year": 2015}, {"title": "Improving learning and inference in a large knowledgebase using latent syntactic cues", "author": ["Gardner et al.2013] Matt Gardner", "Partha Pratim Talukdar", "Bryan Kisiel", "Tom M Mitchell"], "venue": "In Proceedings of EMNLP,", "citeRegEx": "Gardner et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Gardner et al\\.", "year": 2013}, {"title": "Knowledge-based weak supervision for information extraction of overlapping relations", "author": ["Congle Zhang", "Xiao Ling", "Luke Zettlemoyer", "Daniel S Weld"], "venue": "In Proceedings of ACL-HLT,", "citeRegEx": "Hoffmann et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Hoffmann et al\\.", "year": 2011}, {"title": "Relational retrieval using a combination of path-constrained random walks", "author": ["Lao", "Cohen2010] Ni Lao", "William W Cohen"], "venue": "Machine learning,", "citeRegEx": "Lao et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Lao et al\\.", "year": 2010}, {"title": "Random walk inference and learning in a large scale knowledge base", "author": ["Lao et al.2011] Ni Lao", "Tom Mitchell", "William W Cohen"], "venue": "In Proceedings of EMNLP,", "citeRegEx": "Lao et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Lao et al\\.", "year": 2011}, {"title": "Reading the web with learned syntactic-semantic inference rules", "author": ["Lao et al.2012] Ni Lao", "Amarnag Subramanya", "Fernando Pereira", "William W Cohen"], "venue": "In Proceedings of EMNLP-CoNLL,", "citeRegEx": "Lao et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Lao et al\\.", "year": 2012}, {"title": "Modeling relation paths for representation learning of knowledge bases", "author": ["Lin et al.2015] Yankai Lin", "Zhiyuan Liu", "Maosong Sun"], "venue": "Proceedings of EMNLP", "citeRegEx": "Lin et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Lin et al\\.", "year": 2015}, {"title": "Neural relation extraction with selective attention over instances", "author": ["Lin et al.2016] Yankai Lin", "Shiqi Shen", "Zhiyuan Liu", "Luan Huanbo", "Maosong Sun"], "venue": "Proceedings of ACL", "citeRegEx": "Lin et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Lin et al\\.", "year": 2016}, {"title": "Distant supervision for relation extraction without labeled data", "author": ["Mintz et al.2009] Mike Mintz", "Steven Bills", "Rion Snow", "Dan Jurafsky"], "venue": "In Proceedings of ACL-IJCNLP,", "citeRegEx": "Mintz et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Mintz et al\\.", "year": 2009}, {"title": "End-to-end relation extraction using lstms on sequences and tree structures. arXiv preprint arXiv:1601.00770", "author": ["Miwa", "Bansal2016] Makoto Miwa", "Mohit Bansal"], "venue": null, "citeRegEx": "Miwa et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Miwa et al\\.", "year": 2016}, {"title": "Compositional vector space models for knowledge base inference", "author": ["Benjamin Roth", "Andrew McCallum"], "venue": "In 2015 AAAI Spring Symposium Series", "citeRegEx": "Neelakantan et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Neelakantan et al\\.", "year": 2015}, {"title": "Modeling relations and their mentions without labeled text", "author": ["Limin Yao", "Andrew McCallum"], "venue": "In Proceedings of ECML-PKDD,", "citeRegEx": "Riedel et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Riedel et al\\.", "year": 2010}, {"title": "A neural attention model for abstractive sentence summarization", "author": ["Sumit Chopra", "Jason Weston"], "venue": "Proceedings of EMNLP", "citeRegEx": "Rush et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Rush et al\\.", "year": 2015}, {"title": "Semantic compositionality through recursive matrixvector spaces", "author": ["Brody Huval", "Christopher D Manning", "Andrew Y Ng"], "venue": "In Proceedings of EMNLP-CoNLL,", "citeRegEx": "Socher et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Socher et al\\.", "year": 2012}, {"title": "Parsing with compositional vector grammars", "author": ["John Bauer", "Christopher D Manning", "Andrew Y Ng"], "venue": "In Proceedings of ACL. Citeseer", "citeRegEx": "Socher et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Socher et al\\.", "year": 2013}, {"title": "Dropout: A simple way to prevent neural networks from overfitting", "author": ["Geoffrey Hinton", "Alex Krizhevsky", "Ilya Sutskever", "Ruslan Salakhutdinov"], "venue": null, "citeRegEx": "Srivastava et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Srivastava et al\\.", "year": 2014}, {"title": "Yago: a core of semantic knowledge", "author": ["Gjergji Kasneci", "Gerhard Weikum"], "venue": "In Proceedings of WWW,", "citeRegEx": "Suchanek et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Suchanek et al\\.", "year": 2007}, {"title": "Multi-instance multi-label learning for relation extraction", "author": ["Julie Tibshirani", "Ramesh Nallapati", "Christopher D Manning"], "venue": "In Proceedings of EMNLP,", "citeRegEx": "Surdeanu et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Surdeanu et al\\.", "year": 2012}, {"title": "Sequence to sequence learning with neural networks", "author": ["Oriol Vinyals", "Quoc V Le"], "venue": "In Proceedings of NIPS,", "citeRegEx": "Sutskever et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Sutskever et al\\.", "year": 2014}, {"title": "Relation classification via convolutional deep neural network", "author": ["Zeng et al.2014] Daojian Zeng", "Kang Liu", "Siwei Lai", "Guangyou Zhou", "Jun Zhao"], "venue": "In Proceedings of COLING,", "citeRegEx": "Zeng et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Zeng et al\\.", "year": 2014}, {"title": "Distant supervision for relation extraction via piecewise convolutional neural networks", "author": ["Zeng et al.2015] Daojian Zeng", "Kang Liu", "Yubo Chen", "Jun Zhao"], "venue": "In Proceedings of EMNLP", "citeRegEx": "Zeng et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Zeng et al\\.", "year": 2015}], "referenceMentions": [{"referenceID": 2, "context": "Typical KBs such as Freebase (Bollacker et al., 2008), DBpedia (Auer et al.", "startOffset": 29, "endOffset": 53}, {"referenceID": 0, "context": ", 2008), DBpedia (Auer et al., 2007) and YAGO (Suchanek et al.", "startOffset": 17, "endOffset": 36}, {"referenceID": 21, "context": ", 2007) and YAGO (Suchanek et al., 2007) usually describe knowledge as multi-relational data and represent them as triple facts.", "startOffset": 17, "endOffset": 40}, {"referenceID": 13, "context": "(Mintz et al., 2009) automatically generates training data by aligning a KB with plain text.", "startOffset": 0, "endOffset": 20}, {"referenceID": 18, "context": "Recently, neural models have been successfully applied to classify relations based on plain text (Socher et al., 2012; Zeng et al., 2014; dos Santos et al., 2015).", "startOffset": 97, "endOffset": 162}, {"referenceID": 24, "context": "Recently, neural models have been successfully applied to classify relations based on plain text (Socher et al., 2012; Zeng et al., 2014; dos Santos et al., 2015).", "startOffset": 97, "endOffset": 162}, {"referenceID": 25, "context": "(Zeng et al., 2015) further proposes neural models with distant supervision for relation extraction, and achieved the state-of-the-art performance.", "startOffset": 0, "endOffset": 19}, {"referenceID": 13, "context": "Afterwards, (Mintz et al., 2009) aligns plain text with Freebase.", "startOffset": 12, "endOffset": 32}, {"referenceID": 16, "context": "To alleviate this issue, (Riedel et al., 2010) regards each sentence as a training instance and allows multiple instances to share the same label but disallows more than one label, namely a multi-instance single-label problem.", "startOffset": 25, "endOffset": 46}, {"referenceID": 7, "context": "Further, (Hoffmann et al., 2011; Surdeanu et al., 2012) adopt multi-instance multi-label learning in relation extraction.", "startOffset": 9, "endOffset": 55}, {"referenceID": 22, "context": "Further, (Hoffmann et al., 2011; Surdeanu et al., 2012) adopt multi-instance multi-label learning in relation extraction.", "startOffset": 9, "endOffset": 55}, {"referenceID": 1, "context": "Recently, deep learning (Bengio, 2009) has been successfully applied in various areas, including computer vision, speech recognition and so on.", "startOffset": 24, "endOffset": 38}, {"referenceID": 19, "context": "Meanwhile, its effectiveness has also been verified in many NLP tasks such as sentiment analysis (dos Santos and Gatti, 2014), parsing (Socher et al., 2013), summarization (Rush et al.", "startOffset": 135, "endOffset": 156}, {"referenceID": 17, "context": ", 2013), summarization (Rush et al., 2015) and machine translation (Sutskever et al.", "startOffset": 23, "endOffset": 42}, {"referenceID": 23, "context": ", 2015) and machine translation (Sutskever et al., 2014).", "startOffset": 32, "endOffset": 56}, {"referenceID": 18, "context": "(Socher et al., 2012) uses a recursive neural net-", "startOffset": 0, "endOffset": 21}, {"referenceID": 24, "context": "work in relation extraction, and (Zeng et al., 2014; dos Santos et al., 2015) adopt CNNs for relation extraction.", "startOffset": 33, "endOffset": 77}, {"referenceID": 25, "context": "Hence, (Zeng et al., 2015) combines at-least-one multi-instance learning with neural network model to extract relations on distant supervision data.", "startOffset": 7, "endOffset": 26}, {"referenceID": 12, "context": "Further, (Lin et al., 2016) adopts a sentence-level selective attention mechanism to model distant supervised relation extraction.", "startOffset": 9, "endOffset": 27}, {"referenceID": 10, "context": "Path Ranking algorithm (PRA) (Lao and Cohen, 2010) has been adopted for expert finding (Lao and Cohen, 2010) and information retrieval (Lao et al., 2012).", "startOffset": 135, "endOffset": 153}, {"referenceID": 9, "context": "PRA has also been used for relation classification based on KB structure (Lao et al., 2011; Gardner et al., 2013).", "startOffset": 73, "endOffset": 113}, {"referenceID": 6, "context": "PRA has also been used for relation classification based on KB structure (Lao et al., 2011; Gardner et al., 2013).", "startOffset": 73, "endOffset": 113}, {"referenceID": 15, "context": "(Neelakantan et al., 2015; Lin et al., 2015) further learn recurrent neural networks to represent unseen relation paths based on all involved relations.", "startOffset": 0, "endOffset": 44}, {"referenceID": 11, "context": "(Neelakantan et al., 2015; Lin et al., 2015) further learn recurrent neural networks to represent unseen relation paths based on all involved relations.", "startOffset": 0, "endOffset": 44}, {"referenceID": 25, "context": "In fact, other neural RE models such as PCNN (Zeng et al., 2015), RNN (Socher et al.", "startOffset": 45, "endOffset": 64}, {"referenceID": 18, "context": ", 2015), RNN (Socher et al., 2012), LSTM (Miwa and Bansal, 2016) can also be easily adopted as text encoder.", "startOffset": 13, "endOffset": 34}, {"referenceID": 25, "context": "Similar to (Zeng et al., 2015), we define the score function of this entity pair and its corresponding relation r as in max-one setting:", "startOffset": 11, "endOffset": 30}, {"referenceID": 11, "context": "Inspired by work on relation path representation learning (Lin et al., 2015), our model first transforms relation r to its distributed", "startOffset": 58, "endOffset": 76}, {"referenceID": 20, "context": "In the implementation of CNN, we adopt dropout (Srivastava et al., 2014) upon the output layer.", "startOffset": 47, "endOffset": 72}, {"referenceID": 13, "context": "Following previous work (Mintz et al., 2009), we evaluate our model using held-out evaluation which approximately measures the precision without timeconsuming manual evaluation.", "startOffset": 24, "endOffset": 44}, {"referenceID": 25, "context": "com/p/word2vec/ other parameters which have little effect on the system performance, we follow the settings used in (Zeng et al., 2015): word embedding size dw is 50, position embedding size dp is 5 and dropout rate p is 0.", "startOffset": 116, "endOffset": 135}, {"referenceID": 24, "context": "To demonstrate the effect of our approach, we empirically compare it with other state-of-the-art methods via held-out evaluation including: (1) CNN+rand represents the CNN model reported in (Zeng et al., 2014).", "startOffset": 190, "endOffset": 209}, {"referenceID": 25, "context": "(2) CNN+max represents the CNN model with multi-instances learning used in (Zeng et al., 2015).", "startOffset": 75, "endOffset": 94}], "year": 2016, "abstractText": "Distantly supervised relation extraction has been widely used to find novel relational facts from plain text. To predict the relation between a pair of two target entities, existing methods solely rely on those direct sentences containing both entities. In fact, there are also many sentences containing only one of the target entities, which provide rich and useful information for relation extraction. To address this issue, we build inference chains between two target entities via intermediate entities, and propose a path-based neural relation extraction model to encode the relational semantics from both direct sentences and inference chains. Experimental results on realworld datasets show that, our model can make full use of those sentences containing only one target entity, and achieves significant and consistent improvements on relation extraction as compared with baselines.", "creator": "LaTeX with hyperref package"}}}