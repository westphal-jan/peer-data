{"id": "1609.07561", "review": {"conference": "EMNLP", "VERSION": "v1", "DATE_OF_SUBMISSION": "24-Sep-2016", "title": "Distilling an Ensemble of Greedy Dependency Parsers into One MST Parser", "abstract": "We introduce two first-order graph-based dependency parsers achieving a new state of the art. The first is a consensus parser built from an ensemble of independently trained greedy LSTM transition-based parsers with different random initializations. We cast this approach as minimum Bayes risk decoding (under the Hamming cost) and argue that weaker consensus within the ensemble is a useful signal of difficulty or ambiguity. The second parser is a \"distillation\" of the ensemble into a single model. We train the distillation parser using a structured hinge loss objective with a novel cost that incorporates ensemble uncertainty estimates for each possible attachment, thereby avoiding the intractable cross-entropy computations required by applying standard distillation objectives to problems with structured outputs. The first-order distillation parser matches or surpasses the state of the art on English, Chinese, and German.", "histories": [["v1", "Sat, 24 Sep 2016 02:58:26 GMT  (44kb,D)", "http://arxiv.org/abs/1609.07561v1", "10 pages. To appear at EMNLP 2016"]], "COMMENTS": "10 pages. To appear at EMNLP 2016", "reviews": [], "SUBJECTS": "cs.CL", "authors": ["adhiguna kuncoro", "miguel ballesteros", "lingpeng kong", "chris dyer", "noah a smith"], "accepted": true, "id": "1609.07561"}, "pdf": {"name": "1609.07561.pdf", "metadata": {"source": "CRF", "title": "Distilling an Ensemble of Greedy Dependency Parsers into One MST Parser", "authors": ["Adhiguna Kuncoro", "Miguel Ballesteros", "Lingpeng Kong", "Chris Dyer", "Noah A. Smith"], "emails": ["akuncoro@cs.cmu.edu", "cdyer@cs.cmu.edu", "lingpenk@cs.cmu.edu", "miguel.ballesteros@upf.edu,", "nasmith@cs.washington.edu"], "sections": [{"heading": "1 Introduction", "text": "Neural network dependency savers achieve state-of-the-art performance (Dyer et al., 2015; Weiss et al., 2015; Andor et al., 2016), but the training involves a gradient descent to non-convex targets that is unstable in terms of initial parameter values. For some tasks, an ensemble of neural networks was found from various random initializations to improve performance via individual models (Sutskever et al., 2014; Vinyals et al., 2015, among others).In \u00a7 3, we apply this idea to build a firstorder graph-based ensemble parser (Sagaeand Lavie, 2006) that seeks a consensus between 20 randomly initialized LSTM parsers (Dyer et al., 2015), and achieve almost the best-reported performance on the standard Penn Treebank Stanford dependencies task (94.51 UAS, 92.70 LAS)."}, {"heading": "2 Notation and Definitions", "text": "We suppress dependency arc at a time. (We suppress dependency arc at a time. (We suppression dependency arc) There is only one dependency arc at a time. (We suppression dependency arc) There is only one dependency arc at a time. (We suppression dependency arc at a time.) We suppress dependency arc at a time. (We suppress dependency arc at a time. (We suppress dependency arc at a time.) There are different ways to integrate them. (We suppression arc labels at a time.) (We suppress dependency arc at a time.) We suppress dependency arc at a time. (We suppress dependency arc labels at a time.) (We suppress dependency arc at a time.) There are different ways to integrate them. (We suppress dependency arc labels at a time.) (We suppress dependency arc at a time.)"}, {"heading": "3 Consensus and Minimum Bayes Risk", "text": "Despite the recent success of the Neural Network Dependency Parser, most previous work reports exclusively on the performance of a single model. Putting together neural network models trained from different random starting points is a standard technique for a variety of problems, such as machine translation (Sutskever et al., 2014) and constituency parsing (Vinyals et al., 2015). We aim to investigate the benefits of the ensemble of independently trained neural network dependency parsers by applying the Sagae and Lavie parser sembling method (2006) to a collection of strong neural network base parsers. Here, each base parser is an example of the greedy, transient-based parser by Dyer et al. (2015), known as the stack LSTM parser, formed from another random initial estimate. In the face of a sentence defined as the FOG-Parser (Eq) 1."}, {"heading": "4 What is Ensemble Uncertainty?", "text": "While previous work has already shown the value of dependence on parsing (Sagae and Lavie, 2006; Surdeanu and Manning, 2010), we consider whether the posterior margins estimated by p (h, m).x are a sign of difficulty or whether the default data is split (02-21 for development, 23 for testing).We suspect that discrepancies among the base parsers about the attachment of xm (i.e., the uncertainty in the hindquarters).We use the default data, 22 for development, 23 for testing."}, {"heading": "5 Distilling the Ensemble", "text": "Although it is a purely procedural approach, it requires the individual movements to be deciphered. To reduce computational costs, we introduce a method of \"distilling\" the ensemble into a single parser, using a novel cost function to integrate this knowledge from the ensemble system into the system. While models combining the results of other parsing models have been proposed in advance (Martins et al., Nivre and McDonald, 2008), these works are associated with the scores or results of the first stage."}, {"heading": "5.1 Distillation Cost Function", "text": "The natural place where this additional information can be used when a parser performs multiple functions in the cost function is an attractive training, the hamming cost encodes hard targets: the correct attachment should get a higher score than all incorrectly predicted. Our distillation cost function aims to reduce the cost of decisions that seem more difficult - based on ensemble uncertainty - or where there may be several plausible attachments. (h, m) The new cost function is defined by CD (y, y). The new cost function is defined by CD (y, y)."}, {"heading": "5.2 Distilled Parser", "text": "This is a natural choice because it makes the cost function explicit and central to learning. [6] This approach to building a model is good for an FOG parser, but not for a transitional parser, which is made up of a collection of classifiers that opt for good transitions - not to win whole trees for good attachment. [7] Transition-based approaches are therefore inappropriate for our proposed distillation cost function, even if they are asymptomatically faster. [8] We are dealing with a FOG parameter that takes responsibility for itself."}, {"heading": "6 Experiments", "text": "In fact, most of us are able to play by the rules that they have imposed on themselves, and they are able to play by the rules, \"he said in an interview with the New York Times."}, {"heading": "7 Related Work", "text": "Our work on the dependence sparers is based on Sagae and Lavie (2006) and Surdeanu and Manning (2010); an additional contribution of this work is to show that the normalized ensemble tunings correspond to MBR analysis. Petrov (2010) proposed a similar model combination with random initializations for the analysis of phrase structure, using a single neural network to simulate a large ensemble of classifiers. More recently, Ba and Caruana demonstrated that a single neural network closely interconnects the performance of an ensemble of deep neural networks in terms of phoneme recognition and object recognition."}, {"heading": "8 Conclusions", "text": "We show that an ensemble of 20 greedy stack LSTMs (Dyer et al., 2015) can achieve state-of-the-art accuracy in English dependency parsing. This approach corresponds to minimal Bayes risk decoding, and we suspect that the posterior arc appendices quantify an idea of uncertainty that may indicate difficulties or ambiguities. As operating an ensemble is computationally expensive, we proposed a discriminatory formation of a graph-based model with a novel cost function that distills ensemble uncertainty. Deriving a cost function from a statistical model and extending distillation to structured predictions are new contributions. This distilled model, which was trained to simulate the slower ensemble parser, improves the state of the art in Chinese and German."}, {"heading": "Acknowledgments", "text": "We thank Swabha Swayamdipta, Sam Thomson, Jesse Dodge, Dallas Card, Yuichiro Sawai, Graham Neubig, and the anonymous reviewers for useful feedback. We also thank Juntao Yu and Bernd Bohnet for reprinting the parser Bohnet and Nivre (2012) in Chinese with gold tags. This work was partially supported by the Defense Advanced Research Projects Agency (DARPA) Information Innovation Office (I2O) under the Low Resource Languages for Emergent Incidents (LORELEI) program, issued by DARPA / I2O under contract no. HR001115-C-0114; partially supported by contract no. W911NF-15-1-0543 with DARPA and the Army Research Office (ARO). Authorized for public publication, distribution unlimited. The views expressed are those of the authors and do not reflect the official policy or position of the Department of Defense or the U.S. government."}], "references": [{"title": "Globally normalized transition-based neural networks", "author": ["Andor et al.2016] Daniel Andor", "Chris Alberti", "David Weiss", "Aliaksei Severyn", "Alessandro Presta", "Kuzman Ganchev", "Slav Petrov", "Michael Collins"], "venue": "In Proc. of ACL", "citeRegEx": "Andor et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Andor et al\\.", "year": 2016}, {"title": "Do deep nets really need to be deep", "author": ["Ba", "Caruana2014] Jimmy Ba", "Rich Caruana"], "venue": "In Proc. of NIPS", "citeRegEx": "Ba et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Ba et al\\.", "year": 2014}, {"title": "Improved transition-based parsing by modeling characters instead of words with LSTMs", "author": ["Chris Dyer", "Noah A. Smith"], "venue": "In Proc. of EMNLP", "citeRegEx": "Ballesteros et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Ballesteros et al\\.", "year": 2015}, {"title": "Training with exploration improves a greedy stack-LSTM parser", "author": ["Yoav Goldberg", "Chris Dyer", "Noah A. Smith"], "venue": "In Proc. of EMNLP", "citeRegEx": "Ballesteros et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Ballesteros et al\\.", "year": 2016}, {"title": "A transition-based system for joint part-ofspeech tagging and labeled non-projective dependency parsing", "author": ["Bohnet", "Nivre2012] Bernd Bohnet", "Joakim Nivre"], "venue": "In Proc. of EMNLP-CoNLL", "citeRegEx": "Bohnet et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Bohnet et al\\.", "year": 2012}, {"title": "A fast and accurate dependency parser using neural networks", "author": ["Chen", "Manning2014] Danqi Chen", "Christopher Manning"], "venue": "In Proc. of EMNLP", "citeRegEx": "Chen et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Chen et al\\.", "year": 2014}, {"title": "Stanford typed dependencies manual", "author": ["De Marneffe", "Christopher D. Manning"], "venue": null, "citeRegEx": "Marneffe et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Marneffe et al\\.", "year": 2008}, {"title": "Transition-based dependency parsing with stack long short-term memory", "author": ["Dyer et al.2015] Chris Dyer", "Miguel Ballesteros", "Wang Ling", "Austin Matthews", "Noah A. Smith"], "venue": "In Proc. of ACL", "citeRegEx": "Dyer et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Dyer et al\\.", "year": 2015}, {"title": "Three new probabilistic models for dependency parsing: An exploration", "author": ["Jason M. Eisner"], "venue": "In Proc. of COLING", "citeRegEx": "Eisner.,? \\Q1996\\E", "shortCiteRegEx": "Eisner.", "year": 1996}, {"title": "Softmax-margin CRFs: Training loglinear models with cost functions", "author": ["Gimpel", "Smith2010] Kevin Gimpel", "Noah A Smith"], "venue": "In Proc. of NAACL", "citeRegEx": "Gimpel et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Gimpel et al\\.", "year": 2010}, {"title": "Distilling the knowledge in a neural network. CoRR, abs/1503.02531", "author": ["Oriol Vinyals", "Jeffrey Dean"], "venue": null, "citeRegEx": "Hinton et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Hinton et al\\.", "year": 2015}, {"title": "Sequence-level knowledge distillation", "author": ["Kim", "Rush2016] Yoon Kim", "Alexander M. Rush"], "venue": "In Proc. of EMNLP", "citeRegEx": "Kim et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Kim et al\\.", "year": 2016}, {"title": "Simple and accurate dependency parsing using bidirectional LSTM feature representations. TACL", "author": ["Kiperwasser", "Yoav Goldberg"], "venue": null, "citeRegEx": "Kiperwasser et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Kiperwasser et al\\.", "year": 2016}, {"title": "Efficient third-order dependency parsers", "author": ["Koo", "Collins2010] Terry Koo", "Michael Collins"], "venue": "In Proc. of ACL", "citeRegEx": "Koo et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Koo et al\\.", "year": 2010}, {"title": "The inside-outside recursive neural network model for dependency parsing", "author": ["Le", "Zuidema2014] Phong Le", "Willem Zuidema"], "venue": "In Proc. of EMNLP", "citeRegEx": "Le et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Le et al\\.", "year": 2014}, {"title": "Two/too simple adaptations of word2vec for syntax problems", "author": ["Ling et al.2015] Wang Ling", "Chris Dyer", "Alan W Black", "Isabel Trancoso"], "venue": "In Proc. of NAACL", "citeRegEx": "Ling et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Ling et al\\.", "year": 2015}, {"title": "Fourth-order dependency parsing", "author": ["Ma", "Zhao2012] Xuezhe Ma", "Hai Zhao"], "venue": "In Proc. of COLING", "citeRegEx": "Ma et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Ma et al\\.", "year": 2012}, {"title": "Stacking dependency parsers", "author": ["Dipanjan Das", "Noah A. Smith", "Eric P. Xing"], "venue": "In Proc. of EMNLP", "citeRegEx": "Martins et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Martins et al\\.", "year": 2008}, {"title": "Polyhedral outer approximations with application to natural language parsing", "author": ["Noah A. Smith", "Eric P. Xing"], "venue": "In Proc. of ICML", "citeRegEx": "Martins et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Martins et al\\.", "year": 2009}, {"title": "Turning on the turbo: Fast third-order non-projective turbo parsers", "author": ["Mariana S.C. Almeida", "Noah A. Smith"], "venue": "In Proc. of ACL", "citeRegEx": "Martins et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Martins et al\\.", "year": 2013}, {"title": "Online largemargin training of dependency parsers", "author": ["Koby Crammer", "Fernando Pereira"], "venue": "In Proc. of ACL", "citeRegEx": "McDonald et al\\.,? \\Q2005\\E", "shortCiteRegEx": "McDonald et al\\.", "year": 2005}, {"title": "Nonprojective dependency parsing using spanning tree algorithms", "author": ["Fernando Pereira", "Kiril Ribarov", "Jan Haji\u010d"], "venue": "In Proc. of EMNLP", "citeRegEx": "McDonald et al\\.,? \\Q2005\\E", "shortCiteRegEx": "McDonald et al\\.", "year": 2005}, {"title": "Integrating graph-based and transitionbased dependency parsers", "author": ["Nivre", "McDonald2008] Joakim Nivre", "Ryan McDonald"], "venue": "In Proc. of ACL", "citeRegEx": "Nivre et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Nivre et al\\.", "year": 2008}, {"title": "An efficient algorithm for projective dependency parsing", "author": ["Joakim Nivre"], "venue": "In Proc. of IWPT", "citeRegEx": "Nivre.,? \\Q2003\\E", "shortCiteRegEx": "Nivre.", "year": 2003}, {"title": "Products of random latent variable grammars", "author": ["Slav Petrov"], "venue": "In Proc.of NAACL", "citeRegEx": "Petrov.,? \\Q2010\\E", "shortCiteRegEx": "Petrov.", "year": 2010}, {"title": "Parser combination by reparsing", "author": ["Sagae", "Lavie2006] Kenji Sagae", "Alon Lavie"], "venue": "In Proc. of NAACL", "citeRegEx": "Sagae et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Sagae et al\\.", "year": 2006}, {"title": "Minimum risk annealing for training log-linear models", "author": ["Smith", "Eisner2006] David A. Smith", "Jason Eisner"], "venue": "In Proc. of ACL", "citeRegEx": "Smith et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Smith et al\\.", "year": 2006}, {"title": "Ensemble models for dependency parsing: Cheap and good", "author": ["Surdeanu", "Manning2010] Mihai Surdeanu", "Christopher D. Manning"], "venue": "In Proc. of NAACL", "citeRegEx": "Surdeanu et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Surdeanu et al\\.", "year": 2010}, {"title": "Sequence to sequence learning with neural networks", "author": ["Oriol Vinyals", "Quoc V. Le"], "venue": "In Proc. of NIPS", "citeRegEx": "Sutskever et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Sutskever et al\\.", "year": 2014}, {"title": "Target language adaptation of discriminative transfer parsers", "author": ["Ryan T. McDonald", "Joakim Nivre"], "venue": "In Proc. of NAACL", "citeRegEx": "T\u00e4ckstr\u00f6m et al\\.,? \\Q2013\\E", "shortCiteRegEx": "T\u00e4ckstr\u00f6m et al\\.", "year": 2013}, {"title": "Learning structured prediction models: A large margin approach", "author": ["Taskar et al.2005] Ben Taskar", "Vassil Chatalbashev", "Daphne Koller", "Carlos Guestrin"], "venue": "In Proc. of ICML", "citeRegEx": "Taskar et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Taskar et al\\.", "year": 2005}, {"title": "Bayes risk minimization in natural language parsing", "author": ["Titov", "Henderson2006] Ivan Titov", "James Henderson"], "venue": "Technical report, University of Geneva", "citeRegEx": "Titov et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Titov et al\\.", "year": 2006}, {"title": "Feature-rich part-of-speech tagging with a cyclic dependency network", "author": ["Dan Klein", "Christopher D. Manning", "Yoram Singer"], "venue": "In Proc. of NAACL", "citeRegEx": "Toutanova et al\\.,? \\Q2003\\E", "shortCiteRegEx": "Toutanova et al\\.", "year": 2003}, {"title": "Large margin methods for structured and interdependent output variables. JMLR", "author": ["Thorsten Joachims", "Thomas Hofmann", "Yasemin Altun"], "venue": null, "citeRegEx": "Tsochantaridis et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Tsochantaridis et al\\.", "year": 2005}, {"title": "Grammar as a foreign language", "author": ["Lukasz Kaiser", "Terry Koo", "Slav Petrov", "Ilya Sutskever", "Geoffrey Hinton"], "venue": "In Proc. of NIPS", "citeRegEx": "Vinyals et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Vinyals et al\\.", "year": 2015}, {"title": "Graph-based dependency parsing with bidirectional LSTM", "author": ["Wang", "Chang2016] Wenhui Wang", "Baobao Chang"], "venue": "In Proc. of ACL", "citeRegEx": "Wang et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Wang et al\\.", "year": 2016}, {"title": "Structured training for neural network transition-based parsing", "author": ["Weiss et al.2015] David Weiss", "Chris Alberti", "Michael Collins", "Slav Petrov"], "venue": "In Proc. of ACL", "citeRegEx": "Weiss et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Weiss et al\\.", "year": 2015}, {"title": "Building a large-scale annotated chinese corpus", "author": ["Xue et al.2002] Nianwen Xue", "Fu-Dong Chiou", "Martha Palmer"], "venue": "In Proc. of COLING", "citeRegEx": "Xue et al\\.,? \\Q2002\\E", "shortCiteRegEx": "Xue et al\\.", "year": 2002}, {"title": "Incremental recurrent neural network dependency parser with search-based discriminative training", "author": ["Yazdani", "Henderson2015] Majid Yazdani", "James Henderson"], "venue": "In Proc. of CoNLL", "citeRegEx": "Yazdani et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Yazdani et al\\.", "year": 2015}, {"title": "A tale of two parsers: Investigating and combining graph-based and transition-based dependency parsing using beam-search", "author": ["Zhang", "Clark2008] Yue Zhang", "Stephen Clark"], "venue": "In Proc. of EMNLP", "citeRegEx": "Zhang et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Zhang et al\\.", "year": 2008}, {"title": "Transition-based dependency parsing with rich non-local features", "author": ["Zhang", "Nivre2011] Yue Zhang", "Joakim Nivre"], "venue": "In Proc. of ACL", "citeRegEx": "Zhang et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Zhang et al\\.", "year": 2011}, {"title": "A re-ranking model for dependency parser with recursive convolutional neural network", "author": ["Zhu et al.2015] Chenxi Zhu", "Xipeng Qiu", "Xinchi Chen", "Xuanjing Huang"], "venue": "In Proc. of ACL-IJCNLP", "citeRegEx": "Zhu et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Zhu et al\\.", "year": 2015}], "referenceMentions": [{"referenceID": 7, "context": "Neural network dependency parsers achieve state of the art performance (Dyer et al., 2015; Weiss et al., 2015; Andor et al., 2016), but training them involves gradient descent on non-convex objectives, which is unstable with respect to initial parameter values.", "startOffset": 71, "endOffset": 130}, {"referenceID": 36, "context": "Neural network dependency parsers achieve state of the art performance (Dyer et al., 2015; Weiss et al., 2015; Andor et al., 2016), but training them involves gradient descent on non-convex objectives, which is unstable with respect to initial parameter values.", "startOffset": 71, "endOffset": 130}, {"referenceID": 0, "context": "Neural network dependency parsers achieve state of the art performance (Dyer et al., 2015; Weiss et al., 2015; Andor et al., 2016), but training them involves gradient descent on non-convex objectives, which is unstable with respect to initial parameter values.", "startOffset": 71, "endOffset": 130}, {"referenceID": 7, "context": "In \u00a73, we apply this idea to build a firstorder graph-based (FOG) ensemble parser (Sagae and Lavie, 2006) that seeks consensus among 20 randomly-initialized stack LSTM parsers (Dyer et al., 2015), achieving nearly the best-reported performance on the standard Penn Treebank Stanford", "startOffset": 176, "endOffset": 195}, {"referenceID": 10, "context": "We address this issue in \u00a75 by distilling the ensemble into a single FOG parser with discriminative training by defining a new cost function, inspired by the notion of \u201csoft targets\u201d (Hinton et al., 2015).", "startOffset": 183, "endOffset": 204}, {"referenceID": 19, "context": "The distilled model performs almost as well as the ensemble consensus and much better than (i) a strong LSTM FOG parser trained using the conventional Hamming cost function, (ii) recently published strong LSTM FOG parsers (Kiperwasser and Goldberg, 2016; Wang and Chang, 2016), and (iii) many higher-order graph-based parsers (Koo and Collins, 2010; Martins et al., 2013; Le and Zuidema, 2014).", "startOffset": 326, "endOffset": 393}, {"referenceID": 20, "context": ") To define s, McDonald et al. (2005a) used hand-engineered features of the surrounding and in-between context of xh and xm; more recently, Kiperwasser and Goldberg (2016) used a bidirectional LSTM followed by a single hid-", "startOffset": 15, "endOffset": 39}, {"referenceID": 20, "context": ") To define s, McDonald et al. (2005a) used hand-engineered features of the surrounding and in-between context of xh and xm; more recently, Kiperwasser and Goldberg (2016) used a bidirectional LSTM followed by a single hid-", "startOffset": 15, "endOffset": 172}, {"referenceID": 8, "context": ", 2005b) or, under a projectivity constraint, a dynamic programming algorithm (Eisner, 1996), in O(n2) or O(n3) runtime, respectively.", "startOffset": 78, "endOffset": 92}, {"referenceID": 23, "context": "An alternative that runs in linear time is transition-based parsing, which recasts parsing as a sequence of actions that manipulate auxiliary data structures to incrementally build a parse tree (Nivre, 2003).", "startOffset": 194, "endOffset": 207}, {"referenceID": 0, "context": "Unlike FOG parsers, transition-based parsers allow the use of scoring functions with history-based features, so that attachment decisions can interact more freely; the best performing parser at the time of this writing employ neural networks (Andor et al., 2016).", "startOffset": 242, "endOffset": 262}, {"referenceID": 30, "context": "(Taskar et al., 2005; Tsochantaridis et al., 2005), and if S\u03b8 is (sub)differentiable, many algorithms are available.", "startOffset": 0, "endOffset": 50}, {"referenceID": 33, "context": "(Taskar et al., 2005; Tsochantaridis et al., 2005), and if S\u03b8 is (sub)differentiable, many algorithms are available.", "startOffset": 0, "endOffset": 50}, {"referenceID": 18, "context": "Variants have been used extensively in training graph-based parsers (McDonald et al., 2005b; Martins et al., 2009), which typically make use of Hamming cost, so that the inner max can be solved efficiently using FOG parsing with a slightly revised local scoring function:", "startOffset": 68, "endOffset": 114}, {"referenceID": 28, "context": "work models trained from different random starting points is a standard technique in a variety of problems, such as machine translation (Sutskever et al., 2014) and constituency parsing (Vinyals et al.", "startOffset": 136, "endOffset": 160}, {"referenceID": 34, "context": ", 2014) and constituency parsing (Vinyals et al., 2015).", "startOffset": 33, "endOffset": 55}, {"referenceID": 7, "context": "greedy, transition-based parser of Dyer et al. (2015), known as the stack LSTM parser, trained from a different random initial estimate.", "startOffset": 35, "endOffset": 54}, {"referenceID": 34, "context": "An alternative to building an ensemble of stack LSTM parsers in this way would be to average the softmax decisions at each timestep (transition), similar to Vinyals et al. (2015). John saw the woman with a telescope 19", "startOffset": 157, "endOffset": 179}, {"referenceID": 0, "context": "Model UAS LAS UEM Andor et al. (2016) 94.", "startOffset": 18, "endOffset": 38}, {"referenceID": 7, "context": "the base parsers instantiate the greedy stack LSTM parser (Dyer et al., 2015).", "startOffset": 58, "endOffset": 77}, {"referenceID": 0, "context": "perform comparably to the best previously reported, due to Andor et al. (2016), which uses a beam (width 32) for training and decoding.", "startOffset": 59, "endOffset": 79}, {"referenceID": 7, "context": "We use the standard data split (02\u201321 for training, 22 for development, 23 for test), automatically predicted partof-speech tags, same pretrained word embedding as Dyer et al. (2015), and recommended hyperparameters; https:// github.", "startOffset": 164, "endOffset": 183}, {"referenceID": 10, "context": "(2006) and Hinton et al. (2015); they were likewise motivated by a desire to create a simpler model that was cheaper to run at test time.", "startOffset": 11, "endOffset": 32}, {"referenceID": 7, "context": "The first difference is that we fix the pretrained word embeddings and compose them with learned embeddings and POS tag embeddings (Dyer et al., 2015), allowing the model to simultaneously leverage pretrained vectors and learn a taskspecific representation.", "startOffset": 131, "endOffset": 150}, {"referenceID": 7, "context": "The first difference is that we fix the pretrained word embeddings and compose them with learned embeddings and POS tag embeddings (Dyer et al., 2015), allowing the model to simultaneously leverage pretrained vectors and learn a taskspecific representation.7 Unlike Kiperwasser and Goldberg (2016), we did not observe any degradation by incorporating the pretrained vectors.", "startOffset": 132, "endOffset": 298}, {"referenceID": 0, "context": "Alternatives that do not use cost functions include probabilistic parsers, whether locally normalized like the stack LSTM parser used within our ensemble, or globally normalized, as in Andor et al. (2016); cost functions can be incorporated in such cases with minimum risk training (Smith and Eisner, 2006) or softmax margin (Gimpel and Smith, 2010).", "startOffset": 185, "endOffset": 205}, {"referenceID": 0, "context": "Alternatives that do not use cost functions include probabilistic parsers, whether locally normalized like the stack LSTM parser used within our ensemble, or globally normalized, as in Andor et al. (2016); cost functions can be incorporated in such cases with minimum risk training (Smith and Eisner, 2006) or softmax margin (Gimpel and Smith, 2010). To our understanding, Kiperwasser and Goldberg (2016) initialized with pretrained vectors and backpropagated during training.", "startOffset": 185, "endOffset": 405}, {"referenceID": 37, "context": "0, Penn Chinese Treebank (Xue et al., 2002), and German CoNLL 2009 (Haji\u010d et al.", "startOffset": 25, "endOffset": 43}, {"referenceID": 32, "context": "(2015), we use predicted tags with the Stanford tagger (Toutanova et al., 2003) for English and gold tags for Chinese.", "startOffset": 55, "endOffset": 79}, {"referenceID": 7, "context": "Like Chen and Manning (2014) and Dyer et al. (2015), we use predicted tags with the Stanford tagger (Toutanova et al.", "startOffset": 33, "endOffset": 52}, {"referenceID": 15, "context": "All models were augmented with pretrained structured-skipgram (Ling et al., 2015) embeddings; for English we used the Gigaword corpus and 100 dimensions, for Chinese Gi-", "startOffset": 62, "endOffset": 81}, {"referenceID": 23, "context": "For Chinese, it achieves the best published scores, for German the best published UAS scores, and just after Bohnet and Nivre (2012)", "startOffset": 120, "endOffset": 133}, {"referenceID": 19, "context": "Zhang and Nivre (2011) Transition (beam) - - 86.", "startOffset": 10, "endOffset": 23}, {"referenceID": 19, "context": "Zhang and Nivre (2011) Transition (beam) - - 86.0 84.4 - Bohnet and Nivre (2012)\u2020 Transition (beam) - - 87.", "startOffset": 10, "endOffset": 81}, {"referenceID": 19, "context": "Zhang and Nivre (2011) Transition (beam) - - 86.0 84.4 - Bohnet and Nivre (2012)\u2020 Transition (beam) - - 87.3 85.9 91.37 89.38 Chen and Manning (2014) Transition (greedy) X 91.", "startOffset": 10, "endOffset": 150}, {"referenceID": 4, "context": "4 - Dyer et al. (2015) Transition (greedy) X 93.", "startOffset": 4, "endOffset": 23}, {"referenceID": 4, "context": "4 - Dyer et al. (2015) Transition (greedy) X 93.1 90.9 87.2 85.7 - Weiss et al. (2015) Transition (beam) X 94.", "startOffset": 4, "endOffset": 87}, {"referenceID": 4, "context": "4 - Dyer et al. (2015) Transition (greedy) X 93.1 90.9 87.2 85.7 - Weiss et al. (2015) Transition (beam) X 94.0 92.0 - - - Yazdani and Henderson (2015) Transition (beam) - - - - 89.", "startOffset": 4, "endOffset": 152}, {"referenceID": 1, "context": "0 Ballesteros et al. (2015) Transition (greedy) 91.", "startOffset": 2, "endOffset": 28}, {"referenceID": 1, "context": "0 Ballesteros et al. (2015) Transition (greedy) 91.63 89.44 85.30 83.72 88.83 86.10 Ballesteros et al. (2016) Transition (greedy) X 93.", "startOffset": 2, "endOffset": 110}, {"referenceID": 1, "context": "0 Ballesteros et al. (2015) Transition (greedy) 91.63 89.44 85.30 83.72 88.83 86.10 Ballesteros et al. (2016) Transition (greedy) X 93.56 91.42 87.65 86.21 - Kiperwasser and Goldberg (2016) Transition (greedy) X 93.", "startOffset": 2, "endOffset": 190}, {"referenceID": 0, "context": "1 - Andor et al. (2016) Transition (beam) X 94.", "startOffset": 4, "endOffset": 24}, {"referenceID": 17, "context": "74 - - Martins et al. (2013) Graph (3rd order) 93.", "startOffset": 7, "endOffset": 29}, {"referenceID": 17, "context": "74 - - Martins et al. (2013) Graph (3rd order) 93.1 - - - - Le and Zuidema (2014) Reranking/blend X 93.", "startOffset": 7, "endOffset": 82}, {"referenceID": 17, "context": "74 - - Martins et al. (2013) Graph (3rd order) 93.1 - - - - Le and Zuidema (2014) Reranking/blend X 93.8 91.5 - - - Zhu et al. (2015) Reranking/blend X - - 85.", "startOffset": 7, "endOffset": 134}, {"referenceID": 17, "context": "74 - - Martins et al. (2013) Graph (3rd order) 93.1 - - - - Le and Zuidema (2014) Reranking/blend X 93.8 91.5 - - - Zhu et al. (2015) Reranking/blend X - - 85.7 - - Kiperwasser and Goldberg (2016) Graph (1st order) 93.", "startOffset": 7, "endOffset": 197}, {"referenceID": 17, "context": "74 - - Martins et al. (2013) Graph (3rd order) 93.1 - - - - Le and Zuidema (2014) Reranking/blend X 93.8 91.5 - - - Zhu et al. (2015) Reranking/blend X - - 85.7 - - Kiperwasser and Goldberg (2016) Graph (1st order) 93.1 91.0 86.6 85.1 - Wang and Chang (2016) Graph (1st order) X 94.", "startOffset": 7, "endOffset": 259}, {"referenceID": 24, "context": "Petrov (2010) proposed a similar model combination with random initializations for phrase-structure parsing, using products of constituent marginals.", "startOffset": 0, "endOffset": 14}, {"referenceID": 10, "context": "Our work is closer to Hinton et al. (2015), in the sense that we do not simply compress the ensemble and hit the \u201csoft target,\u201d but also the \u201chard target\u201d at the same time10.", "startOffset": 22, "endOffset": 43}, {"referenceID": 7, "context": "We demonstrate that an ensemble of 20 greedy stack LSTMs (Dyer et al., 2015) can achieve state of the art accuracy on English dependency parsing.", "startOffset": 57, "endOffset": 76}, {"referenceID": 23, "context": "We also thank Juntao Yu and Bernd Bohnet for re-running the parser of Bohnet and Nivre (2012) on Chinese with gold tags.", "startOffset": 81, "endOffset": 94}], "year": 2016, "abstractText": "We introduce two first-order graph-based dependency parsers achieving a new state of the art. The first is a consensus parser built from an ensemble of independently trained greedy LSTM transition-based parsers with different random initializations. We cast this approach as minimum Bayes risk decoding (under the Hamming cost) and argue that weaker consensus within the ensemble is a useful signal of difficulty or ambiguity. The second parser is a \u201cdistillation\u201d of the ensemble into a single model. We train the distillation parser using a structured hinge loss objective with a novel cost that incorporates ensemble uncertainty estimates for each possible attachment, thereby avoiding the intractable crossentropy computations required by applying standard distillation objectives to problems with structured outputs. The first-order distillation parser matches or surpasses the state of the art on English, Chinese, and German.", "creator": "LaTeX with hyperref package"}}}