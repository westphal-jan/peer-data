{"id": "1609.07730", "review": {"conference": "AAAI", "VERSION": "v1", "DATE_OF_SUBMISSION": "25-Sep-2016", "title": "Lattice-Based Recurrent Neural Network Encoders for Neural Machine Translation", "abstract": "Neural machine translation (NMT) heavily relies on word level modelling to learn semantic representations of input sentences. However, for languages without natural word delimiters (e.g., Chinese) where input sentences have to be tokenized first, conventional NMT is confronted with two issues: 1) it is difficult to find an optimal tokenization granularity for source sentence modelling, and 2) errors in 1-best tokenizations may propagate to the encoder of NMT. To handle these issues, we propose word-lattice based Recurrent Neural Network (RNN) encoders for NMT, which generalize the standard RNN to word lattice topology. The proposed encoders take as input a word lattice that compactly encodes multiple tokenizations, and learn to generate new hidden states from arbitrarily many inputs and hidden states in preceding time steps. As such, the word-lattice based encoders not only alleviate the negative impact of tokenization errors but also are more expressive and flexible to embed input sentences. Experiment results on Chinese-English translation demonstrate the superiorities of the proposed encoders over the conventional encoder.", "histories": [["v1", "Sun, 25 Sep 2016 10:59:01 GMT  (3718kb,D)", "http://arxiv.org/abs/1609.07730v1", null], ["v2", "Fri, 9 Dec 2016 13:03:42 GMT  (3845kb,D)", "http://arxiv.org/abs/1609.07730v2", null]], "reviews": [], "SUBJECTS": "cs.CL", "authors": ["jinsong su", "zhixing tan", "deyi xiong", "rongrong ji", "xiaodong shi", "yang liu"], "accepted": true, "id": "1609.07730"}, "pdf": {"name": "1609.07730.pdf", "metadata": {"source": "CRF", "title": "Lattice-Based Recurrent Neural Network Encoders for Neural Machine Translation", "authors": ["Jinsong Su", "Zhixing Tan", "Deyi Xiong", "Yang Liu"], "emails": ["jssu@xmu.edu.cn,", "playinf@stu.xmu.edu.cn", "dyxiong@suda.edu.cn,", "liuyang2011@tsinghua.edu.cn"], "sections": [{"heading": "1 Introduction", "text": "This year is the highest in the history of the country."}, {"heading": "2 Neural Machine Translation", "text": "The dominant NMT model is an attention-based model (Bahdanau et al., 2015), which includes an encoder network and a decoder network with attention mechanism. Generally, the encoder is a bidirectional RNN. The forward RNN reads a source set x = x1, x2... xI automatically from left to right and applies the recurring activation function to learn the semantic representation of the word sequence x1: i as \u2212 \u2192 h i = \u03c6 (\u2212 \u2192 h i \u2212 1, ~ xi). Likewise, the backward RNN scans the source set in the reverse direction and learns the semantic representation of \u2212 h \u2212 i of the word sequence xi: I. Finally, we link the hidden states of the two RNNNs to obtain the source annotation hi = [\u2212 h Ti, \u2012 h Ti \u2212 h Ti] T, where the information about the i-word in relation to all the source text is encoded by Tj, where Tj is encoded by the source text, where Ti is encoded by the source word and where \u2212 N is generated in the source text."}, {"heading": "3 Word-Lattice based RNN Encoders", "text": "In this section, we will examine how to build word grids into the NMT RNN encoder. First, we will briefly look at the standard GRU that is the basis of our encoder units, then we will describe how to create the word grid of each input set, and finally, we will describe word grid-based GRU encoders in detail."}, {"heading": "3.1 The Standard GRU", "text": "GRU is a novel hidden entity motivated by LSTM. As illustrated in Figure 2 (a), GRU has two gates in the t time step: 1) a reset gate rt, which specifies how the new input xt can be combined with the previous memory ht \u2212 1, and 2) an update gate zt, which defines how much of the previous memory is to be retained. Formally, the GRU transition equations are as follows: rt = \u03c3 (W (r) xt + U (r) ht \u2212 1), (6) zt = \u03c3 (W (z) xt + U (z) ht \u2212 1), (7) h-t = \u03c6 (Wxt + U (rt ht \u2212 1))), (8) ht = zt ht \u2212 1 + (1 \u2212 zt) h-t, (9) where the logistic sigmoid function is concerned, denotes the element-wise multiplication, and W \u0445 and U \u0445 are the weight matrices, corresponding to this 1997, we apply the STM or the GRM method to the conventional one."}, {"heading": "3.2 Word Lattice", "text": "As shown in Figure 1, a word grid is a directed graph G = < V, E >, where V is the node set and E is the edge set. In view of the word grid of a string c1: N = c1... cN, the node vi-V denotes the position between ci and ci + 1, edge ei: j-E deviates from vi and reaches vj from left to right, covering the sequence ci + 1: j recognized as a possible word. To create word grids, we train many word segments by using several corpora with different annotation standards, such as the Peking University corpus (PKU), the Microsoft Research Corpus (MSR), and the Penn Chinese Treebank 6.0 (CTB). For each input set, we will tokenize using these different segments and generate a word grid by merging the predicted tokenizations divided into different segments."}, {"heading": "3.3 Word-lattice based RNN with GRU encoders", "text": "To address the above problem, we propose that the Gr & # 246; & # 223; e-Gr & # 246; & # 223; e-Gr & # 246; & # 223; -Gr & # 246; & # 223; -Gr & # 246; -Gr & # 223; -Gr & # 246; & # 223; -Gr & # 246; & # 223; -Gr & # 246; & # 223; -Gr & # 246; -Gr & # 246; -Gr & # 246; -Gr & # 246; & # 246-246; & # 246; & # 246; & # 246; & # 246-246; & # 246; & # 246; & # 246-246; & # 246; & # 246-246; & # 246; & # 246-246; & # 246; & # 246-246; & # 246; & # 246-246; & # 246-246; & # 246; & # 246-246; & # 246; & # 246-246; & # 246; & # 246-246; & # 246; & # 246-246; & # 246; & # 246-246; & # 246; & # 246-246; & # 246; & # 246-246; & # 246; & # 246; & # 246; & # 246-246; & # 246; & # 246-246; & # 246; & # 246; & # 246; & # 246; & # 246; & # 246; & # 246-246; & # 246; & # 246; & # 246; & # 246; & # 246;"}, {"heading": "4 Experiments", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "4.1 Setup", "text": "Our training data consists of 1.25M set pairs extracted from LDC2002E18, LDC2003E07, LDC2003E14, Hansard's part from LDC2004T07, LDC2004T08 and LDC2005T06, with 27.9M Chinese words and 34.5 M English words. We chose the NIST 2005 dataset as validation set and the NIST 2002, 2003, 2004, 2006 and 2008 datasets as test sets. The evaluation metric is case-insensitive BLEU (Papineni et al., 2002), as calculated by the Multi-bleu.perl script. To mitigate the effects of instability in NMT training, we trained NMT systems five times for each experiment and reported the average BLEU values."}, {"heading": "4.2 Baselines", "text": "We refer to the attention-based NMT system with the proposed encoders as LatticeNMT, which has four variants with different network units and composition operations. We compared them with the following state-of-the-art SMT and 2http: / / nlp.stanford.edu / software / segmenter.html # DownloadNMT systems: \u2022 Moses3: an open source phrase translation system with standard configurations and a 4 gram language model trained on the target part of the training data. \u2022 RNNSearch (Bahdanau et al., 2015): an attention-based NMT system that has slightly better performance and faster speed than GroundHog.4 In addition to the RNNSearch with word-based segmentation, we also compared the one with characteristic segmentation to investigate whether word boundary information is helpful for NMT \u2022 Multiple source-based MT also has a number of pre-signage points."}, {"heading": "4.3 Overall Performance", "text": "The mentionedlcihsrc\u00fcehsrc\u00fceBnhei nvo nlrf\u00fc ide eerwdnei rf\u00fc ide eerwdnei rf\u00fc ide eerwdnei eaeerdBnn, nvo the ihsc nvo nlrf\u00fc ide eerwdnei eerwdnei eerwdnei eerwdne.ndU nI \"s tsi hacu, nn sasds,\" nn os os os os os os rf\u00fc ide eerwdnei eerwdnei eaJrha, nn os os os os os os os, \"lsa os os os os os rf\u00fc, nvo os os os os os os rf\u00fc.\" nI \"s"}, {"heading": "4.4 Analysis", "text": "This year it is more than ever before in the history of the city."}, {"heading": "5 Related Work", "text": "This year, it has come to the point where it is a purely reactionary project, which is an attempt, which is first and foremost an attempt."}, {"heading": "6 Conclusions and Future Work", "text": "Compared to standard RNN encoders, our encoders use the input and previous hidden states simultaneously, depending on multiple tokenizations, for source rate modeling, thereby not only reducing the error spread of 1-best tokenizations, but also being more meaningful and flexible than the standard encoder. Experimental results in the Sino-English translation show that our encoders lead to significant improvements over a variety of bases. In the future, we plan to continue our work in the following directions: In this work, our network structures depend on the word grids of the source sentences. We will expand our models to include the segmentation model in the source rate representation. In this way, tokenization and translation can work together. In addition, we are interested in better combination strategies to improve our encoders."}], "references": [{"title": "Neural versus phrasebased machine translation quality: a case study", "author": ["Luisa Bentivogli", "Arianna Bisazza", "Mauro Cettolo", "Marcello Federico."], "venue": "arXiv:1608.04631.", "citeRegEx": "Bentivogli et al\\.,? 2016", "shortCiteRegEx": "Bentivogli et al\\.", "year": 2016}, {"title": "Sentence modeling with gated recursive neural network", "author": ["Xinchi Chen", "Xipeng Qiu", "Chenxi Zhu", "Shiyu Wu", "Xuanjing Huang."], "venue": "Proc. of EMNLP2015 Short Paper, pages 655\u2013665.", "citeRegEx": "Chen et al\\.,? 2015", "shortCiteRegEx": "Chen et al\\.", "year": 2015}, {"title": "Learning phrase representations using rnn encoder\u2013decoder for statistical machine translation", "author": ["Kyunghyun Cho", "Bart van Merrienboer", "Caglar Gulcehre", "Dzmitry Bahdanau", "Fethi Bougares", "Holger Schwenk", "Yoshua Bengio."], "venue": "Proc. of", "citeRegEx": "Cho et al\\.,? 2014", "shortCiteRegEx": "Cho et al\\.", "year": 2014}, {"title": "A character-level decoder without explicit segmentation for neural machine translation", "author": ["Junyoung Chung", "KyungHyun Cho", "Yoshua Bengio."], "venue": "Proc. of ACL2016, pages 1693\u20131703.", "citeRegEx": "Chung et al\\.,? 2016", "shortCiteRegEx": "Chung et al\\.", "year": 2016}, {"title": "Natural language processing (almost) from scratch", "author": ["Ronan Collobert", "Jason Weston", "L\u2019eon Bottou", "Michael Karlen", "Koray Kavukcuoglu", "Pavel Kuksa"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Collobert et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Collobert et al\\.", "year": 2011}, {"title": "Character-based neural machine translation", "author": ["Marta R. Costa-Juss\u2018a", "Jos\u2563e A.R. Fonollosa"], "venue": "In Proc. of ACL2016 Shot Paper,", "citeRegEx": "Costa.Juss.a and Fonollosa.,? \\Q2016\\E", "shortCiteRegEx": "Costa.Juss.a and Fonollosa.", "year": 2016}, {"title": "Generalizing word lattice translation", "author": ["Christopher Dyer", "Smaranda Muresan", "Philip Resnik."], "venue": "Proc. of ACL2008, pages 1012\u20131020.", "citeRegEx": "Dyer et al\\.,? 2008", "shortCiteRegEx": "Dyer et al\\.", "year": 2008}, {"title": "Multi-dimensional recurrent neural networks", "author": ["Alex Graves", "Santiago Fernandez", "J\u306argen Schmidhuber."], "venue": "Proc. of ICANN2007, pages 549\u2013558.", "citeRegEx": "Graves et al\\.,? 2007", "shortCiteRegEx": "Graves et al\\.", "year": 2007}, {"title": "Generating sequences with recurrent neural networks", "author": ["Alex Graves."], "venue": "arXiv:1308.0850v5.", "citeRegEx": "Graves.,? 2013", "shortCiteRegEx": "Graves.", "year": 2013}, {"title": "The role of syntax in vector space models of compositional semantics", "author": ["Karl M. Hermann", "Phil Blunsom."], "venue": "Proc. of ACL2013, pages 894\u2013904.", "citeRegEx": "Hermann and Blunsom.,? 2013", "shortCiteRegEx": "Hermann and Blunsom.", "year": 2013}, {"title": "Long short-term memory", "author": ["Sepp Hochreiter", "Jurgen Schmidhuber."], "venue": "Neural Computation, pages 1735\u20131780.", "citeRegEx": "Hochreiter and Schmidhuber.,? 1997", "shortCiteRegEx": "Hochreiter and Schmidhuber.", "year": 1997}, {"title": "Convolutional neural network architectures for matching natural language sentences", "author": ["Baotian Hu", "Zhengdong Lu", "Hang Li", "Qingcai Chen."], "venue": "Proc. of NIPS2014, pages 2042\u20132050.", "citeRegEx": "Hu et al\\.,? 2014", "shortCiteRegEx": "Hu et al\\.", "year": 2014}, {"title": "A neural network for factoid question answering over paragraphs", "author": ["Mohit Iyyer", "Jordan Boyd Graber", "Leonardo Claudino", "Richard Socher", "Hal Daum\u2019e III"], "venue": "In Proc. of EMNLP2014,", "citeRegEx": "Iyyer et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Iyyer et al\\.", "year": 2014}, {"title": "Word lattice reranking for chinese word segmentation and part-of-speech tagging", "author": ["Wenbin Jiang", "HaiTao Mi", "Qun Liu."], "venue": "Proc. of COLING2008, pages 385\u2013392.", "citeRegEx": "Jiang et al\\.,? 2008", "shortCiteRegEx": "Jiang et al\\.", "year": 2008}, {"title": "Recurrent continuous translation models", "author": ["Nal Kalchbrenner", "Phil Blunsom."], "venue": "Proc. of EMNLP2013, pages 1700\u20131709.", "citeRegEx": "Kalchbrenner and Blunsom.,? 2013", "shortCiteRegEx": "Kalchbrenner and Blunsom.", "year": 2013}, {"title": "A convolutional neural network for modelling sentences", "author": ["Nal Kalchbrenner", "Edward Grefenstette", "Phil Blunsom."], "venue": "Proc. of ACL2014, pages 655\u2013665.", "citeRegEx": "Kalchbrenner et al\\.,? 2014", "shortCiteRegEx": "Kalchbrenner et al\\.", "year": 2014}, {"title": "Grid long short-term memory", "author": ["Nal Kalchbrenner", "Ivo Danihelka", "Alex Graves."], "venue": "arXiv:1507.01526.", "citeRegEx": "Kalchbrenner et al\\.,? 2015", "shortCiteRegEx": "Kalchbrenner et al\\.", "year": 2015}, {"title": "Convolutional neural networks for sentence classification", "author": ["Yoon Kim."], "venue": "Proc. of EMNLP2014 Short Paper, pages 1746\u20131751.", "citeRegEx": "Kim.,? 2014", "shortCiteRegEx": "Kim.", "year": 2014}, {"title": "Statistical significance tests for machine translation evaluation", "author": ["Philipp Koehn."], "venue": "Proc. of EMNLP2004, pages 388\u2013395.", "citeRegEx": "Koehn.,? 2004", "shortCiteRegEx": "Koehn.", "year": 2004}, {"title": "Compositional distributional semantics with long short term memory", "author": ["Phong Le", "Willem Zuidema."], "venue": "Proc. of SEM2015, pages 10\u201319.", "citeRegEx": "Le and Zuidema.,? 2015a", "shortCiteRegEx": "Le and Zuidema.", "year": 2015}, {"title": "The forest convolutional network-compositional distributional semantics with a neural chart and without binarization", "author": ["Phong Le", "Willem Zuidema."], "venue": "Proc. of EMNLP2015, pages 1155\u20131164.", "citeRegEx": "Le and Zuidema.,? 2015b", "shortCiteRegEx": "Le and Zuidema.", "year": 2015}, {"title": "Naver machine translation system for wat 2015", "author": ["Hyoung-Gyu Lee", "JaeSong Lee", "Jun-Seok Kim", "Chang-Ki. Lee."], "venue": "Proc. of WAT2015, pages 69\u201373.", "citeRegEx": "Lee et al\\.,? 2015", "shortCiteRegEx": "Lee et al\\.", "year": 2015}, {"title": "Character-based neural machine translation", "author": ["Wang Ling", "Isabel Trancoso", "Chris Dyer", "Alan W. Black."], "venue": "arXiv:1511.04586.", "citeRegEx": "Ling et al\\.,? 2015", "shortCiteRegEx": "Ling et al\\.", "year": 2015}, {"title": "Multi-timescale long short-term memory neural network for modelling sentences and documents", "author": ["Pengfei Liu", "Xipeng Qiu", "Xinchi Chen", "Shiyu Wu", "Xuanjing Huang."], "venue": "Proc. of EMNLP2015, pages 2326\u20132335.", "citeRegEx": "Liu et al\\.,? 2015", "shortCiteRegEx": "Liu et al\\.", "year": 2015}, {"title": "Recurrent neural network based language model", "author": ["Tomas Mikolov", "Martin Karafiat", "Lukas Burget", "Jan Cernocky", "Sanjeev Khudanpur."], "venue": "Proc. of INTERSPEECH2010, pages 1045\u20131048.", "citeRegEx": "Mikolov et al\\.,? 2010", "shortCiteRegEx": "Mikolov et al\\.", "year": 2010}, {"title": "Discriminative neural sentence modeling by tree-based convolution", "author": ["Lili Mou", "Hao Peng", "Ge Li", "Yan Xu", "Lu Zhang", "Zhi Jin."], "venue": "Proc. of EMNLP2015, pages 2315\u20132325.", "citeRegEx": "Mou et al\\.,? 2015", "shortCiteRegEx": "Mou et al\\.", "year": 2015}, {"title": "Bleu: A method for automatic evaluation of machine translation", "author": ["Kishore Papineni", "Salim Roukos", "Todd Ward", "WeiJing Zhu."], "venue": "Proc. of ACL2002, pages 311\u2013318.", "citeRegEx": "Papineni et al\\.,? 2002", "shortCiteRegEx": "Papineni et al\\.", "year": 2002}, {"title": "Parsing natural scenes and natural language with recursive neural networks", "author": ["Richard Socher", "Cliff Chiung-Yu Lin", "Andrew Y. Ng", "Christopher D. Manning."], "venue": "Proc. of ICML2011, pages 129\u2013136.", "citeRegEx": "Socher et al\\.,? 2011", "shortCiteRegEx": "Socher et al\\.", "year": 2011}, {"title": "Higher order recurrent neural networks", "author": ["Rohollah Soltani", "Hui Jiang."], "venue": "arXiv:1605.00064.", "citeRegEx": "Soltani and Jiang.,? 2016", "shortCiteRegEx": "Soltani and Jiang.", "year": 2016}, {"title": "Training very deep networks", "author": ["Rupesh K. Srivastava", "Klaus Greff", "Jurgen Schmidhuber."], "venue": "Proc. of NIPS2015, pages 2368\u20132376.", "citeRegEx": "Srivastava et al\\.,? 2015", "shortCiteRegEx": "Srivastava et al\\.", "year": 2015}, {"title": "Sequence to sequence learning with neural networks", "author": ["Ilya Sutskever", "Oriol Vinyals", "Quoc V. Le."], "venue": "Proc. of NIPS2014, pages 3104\u20133112.", "citeRegEx": "Sutskever et al\\.,? 2014", "shortCiteRegEx": "Sutskever et al\\.", "year": 2014}, {"title": "Improved semantic representations from tree-structured long short-term memory networks", "author": ["Kai Sheng Tai", "Richard Socher", "Christopher D. Manning."], "venue": "Proc. of ACL2015, pages 1556\u20131566.", "citeRegEx": "Tai et al\\.,? 2015", "shortCiteRegEx": "Tai et al\\.", "year": 2015}, {"title": "A lattice-based framework for joint chinese word segmentation, pos tagging and parsing", "author": ["Zhiguo Wang", "Chengqing Zong", "Nianwen Xue."], "venue": "Proc. of ACL2013 Short Paper, pages 623\u2013627.", "citeRegEx": "Wang et al\\.,? 2013", "shortCiteRegEx": "Wang et al\\.", "year": 2013}, {"title": "Long short-term memory over tree structures", "author": ["Xiaodan Zhu", "Parinaz Sobhani", "Hongyu Guo."], "venue": "Proc. of ICML2015, pages 1604\u20131612.", "citeRegEx": "Zhu et al\\.,? 2015", "shortCiteRegEx": "Zhu et al\\.", "year": 2015}], "referenceMentions": [{"referenceID": 14, "context": "In the last two years, NMT (Kalchbrenner and Blunsom, 2013; Sutskever et al., 2014; Bahdanau et al., 2015) has obtained state-of-the-art translation performance on some language pairs.", "startOffset": 27, "endOffset": 106}, {"referenceID": 30, "context": "In the last two years, NMT (Kalchbrenner and Blunsom, 2013; Sutskever et al., 2014; Bahdanau et al., 2015) has obtained state-of-the-art translation performance on some language pairs.", "startOffset": 27, "endOffset": 106}, {"referenceID": 2, "context": "In contrast to conventional statistical machine translation that models latent structures and correspondences between the source and target language in a pipeline, NMT trains a unified encoder-decoder (Cho et al., 2014; Sutskever et al., 2014) neural network for translation, where an encoder maps the input sentence into a fixed-length vector, and a decoder generates a translation from the encoded vector.", "startOffset": 201, "endOffset": 243}, {"referenceID": 30, "context": "In contrast to conventional statistical machine translation that models latent structures and correspondences between the source and target language in a pipeline, NMT trains a unified encoder-decoder (Cho et al., 2014; Sutskever et al., 2014) neural network for translation, where an encoder maps the input sentence into a fixed-length vector, and a decoder generates a translation from the encoded vector.", "startOffset": 201, "endOffset": 243}, {"referenceID": 24, "context": "Most NMT systems adopt RNNs (Mikolov et al., 2010) such as Long Short-Term Memory (LSTM) (Hochreiter and Schmidhuber, 1997) and Gated Recurrent Unit (GRU) (Cho et al.", "startOffset": 28, "endOffset": 50}, {"referenceID": 10, "context": ", 2010) such as Long Short-Term Memory (LSTM) (Hochreiter and Schmidhuber, 1997) and Gated Recurrent Unit (GRU) (Cho et al.", "startOffset": 46, "endOffset": 80}, {"referenceID": 2, "context": ", 2010) such as Long Short-Term Memory (LSTM) (Hochreiter and Schmidhuber, 1997) and Gated Recurrent Unit (GRU) (Cho et al., 2014) to learn vector representations of source sentences and to generate target sentences due to the strong capability of RNNs in capturing long-distance dependencies.", "startOffset": 112, "endOffset": 130}, {"referenceID": 22, "context": "Very recently, we have also seen successful approaches in character-level NMT (Ling et al., 2015; Costa-Juss\u2018a and Fonollosa, 2016; Chung et al., 2016).", "startOffset": 78, "endOffset": 151}, {"referenceID": 5, "context": "Very recently, we have also seen successful approaches in character-level NMT (Ling et al., 2015; Costa-Juss\u2018a and Fonollosa, 2016; Chung et al., 2016).", "startOffset": 78, "endOffset": 151}, {"referenceID": 3, "context": "Very recently, we have also seen successful approaches in character-level NMT (Ling et al., 2015; Costa-Juss\u2018a and Fonollosa, 2016; Chung et al., 2016).", "startOffset": 78, "endOffset": 151}, {"referenceID": 6, "context": "many tokenizations, which has been successfully used in a variety of NLP tasks (Dyer et al., 2008; Jiang et al., 2008; Wang et al., 2013).", "startOffset": 79, "endOffset": 137}, {"referenceID": 13, "context": "many tokenizations, which has been successfully used in a variety of NLP tasks (Dyer et al., 2008; Jiang et al., 2008; Wang et al., 2013).", "startOffset": 79, "endOffset": 137}, {"referenceID": 32, "context": "many tokenizations, which has been successfully used in a variety of NLP tasks (Dyer et al., 2008; Jiang et al., 2008; Wang et al., 2013).", "startOffset": 79, "endOffset": 137}, {"referenceID": 10, "context": "Similar to LSTM, GRU overcomes exploding or vanishing gradient problem (Hochreiter and Schmidhuber, 1997) of the conventional RNN.", "startOffset": 71, "endOffset": 105}, {"referenceID": 26, "context": "The evaluation metric is case-insensitive BLEU (Papineni et al., 2002) as calculated by the multi-bleu.", "startOffset": 47, "endOffset": 70}, {"referenceID": 18, "context": "Furthermore, we conducted paired bootstrap sampling (Koehn, 2004) to test the significance in BLEU score differences.", "startOffset": 52, "endOffset": 65}, {"referenceID": 8, "context": "We apply Rmsprop (Graves, 2013) (momentum = 0, \u03c1 = 0.", "startOffset": 17, "endOffset": 31}, {"referenceID": 0, "context": "However, translating long sequences still remains a great challenge for the attention-based NMT (Bentivogli et al., 2016).", "startOffset": 96, "endOffset": 121}, {"referenceID": 24, "context": "The most typical sequence models are RNNs (Mikolov et al., 2010) with LSTM (Hochreiter and Schmidhuber, 1997; Sutskever et al.", "startOffset": 42, "endOffset": 64}, {"referenceID": 10, "context": ", 2010) with LSTM (Hochreiter and Schmidhuber, 1997; Sutskever et al., 2014; Le and Zuidema, 2015a; Zhu et al., 2015; Liu et al., 2015) or GRU (Cho et al.", "startOffset": 18, "endOffset": 135}, {"referenceID": 30, "context": ", 2010) with LSTM (Hochreiter and Schmidhuber, 1997; Sutskever et al., 2014; Le and Zuidema, 2015a; Zhu et al., 2015; Liu et al., 2015) or GRU (Cho et al.", "startOffset": 18, "endOffset": 135}, {"referenceID": 19, "context": ", 2010) with LSTM (Hochreiter and Schmidhuber, 1997; Sutskever et al., 2014; Le and Zuidema, 2015a; Zhu et al., 2015; Liu et al., 2015) or GRU (Cho et al.", "startOffset": 18, "endOffset": 135}, {"referenceID": 33, "context": ", 2010) with LSTM (Hochreiter and Schmidhuber, 1997; Sutskever et al., 2014; Le and Zuidema, 2015a; Zhu et al., 2015; Liu et al., 2015) or GRU (Cho et al.", "startOffset": 18, "endOffset": 135}, {"referenceID": 23, "context": ", 2010) with LSTM (Hochreiter and Schmidhuber, 1997; Sutskever et al., 2014; Le and Zuidema, 2015a; Zhu et al., 2015; Liu et al., 2015) or GRU (Cho et al.", "startOffset": 18, "endOffset": 135}, {"referenceID": 2, "context": ", 2015) or GRU (Cho et al., 2014; Chen et al., 2015).", "startOffset": 15, "endOffset": 52}, {"referenceID": 1, "context": ", 2015) or GRU (Cho et al., 2014; Chen et al., 2015).", "startOffset": 15, "endOffset": 52}, {"referenceID": 7, "context": "Further, some researchers extend standard RNNs to non-sequential ones, such as multi-dimensional RNNs (Graves et al., 2007), grid RNNs (Kalchbrenner et al.", "startOffset": 102, "endOffset": 123}, {"referenceID": 16, "context": ", 2007), grid RNNs (Kalchbrenner et al., 2015) and higher order RNNs (Soltani and Jiang, 2016).", "startOffset": 19, "endOffset": 46}, {"referenceID": 28, "context": ", 2015) and higher order RNNs (Soltani and Jiang, 2016).", "startOffset": 30, "endOffset": 55}, {"referenceID": 27, "context": "In topological models, sentence representations are composed following given topological structures over words (Socher et al., 2011; Hermann and Blunsom, 2013; Iyyer et al., 2014; Mou et al., 2015; Tai et al., 2015; Le and Zuidema, 2015b).", "startOffset": 111, "endOffset": 238}, {"referenceID": 9, "context": "In topological models, sentence representations are composed following given topological structures over words (Socher et al., 2011; Hermann and Blunsom, 2013; Iyyer et al., 2014; Mou et al., 2015; Tai et al., 2015; Le and Zuidema, 2015b).", "startOffset": 111, "endOffset": 238}, {"referenceID": 12, "context": "In topological models, sentence representations are composed following given topological structures over words (Socher et al., 2011; Hermann and Blunsom, 2013; Iyyer et al., 2014; Mou et al., 2015; Tai et al., 2015; Le and Zuidema, 2015b).", "startOffset": 111, "endOffset": 238}, {"referenceID": 25, "context": "In topological models, sentence representations are composed following given topological structures over words (Socher et al., 2011; Hermann and Blunsom, 2013; Iyyer et al., 2014; Mou et al., 2015; Tai et al., 2015; Le and Zuidema, 2015b).", "startOffset": 111, "endOffset": 238}, {"referenceID": 31, "context": "In topological models, sentence representations are composed following given topological structures over words (Socher et al., 2011; Hermann and Blunsom, 2013; Iyyer et al., 2014; Mou et al., 2015; Tai et al., 2015; Le and Zuidema, 2015b).", "startOffset": 111, "endOffset": 238}, {"referenceID": 20, "context": "In topological models, sentence representations are composed following given topological structures over words (Socher et al., 2011; Hermann and Blunsom, 2013; Iyyer et al., 2014; Mou et al., 2015; Tai et al., 2015; Le and Zuidema, 2015b).", "startOffset": 111, "endOffset": 238}, {"referenceID": 4, "context": "In addition to the aforementioned models, convolutional neural networks are also widely used to model sentences (Collobert et al., 2011; Kalchbrenner et al., 2014; Kim, 2014; Hu et al., 2014).", "startOffset": 112, "endOffset": 191}, {"referenceID": 15, "context": "In addition to the aforementioned models, convolutional neural networks are also widely used to model sentences (Collobert et al., 2011; Kalchbrenner et al., 2014; Kim, 2014; Hu et al., 2014).", "startOffset": 112, "endOffset": 191}, {"referenceID": 17, "context": "In addition to the aforementioned models, convolutional neural networks are also widely used to model sentences (Collobert et al., 2011; Kalchbrenner et al., 2014; Kim, 2014; Hu et al., 2014).", "startOffset": 112, "endOffset": 191}, {"referenceID": 11, "context": "In addition to the aforementioned models, convolutional neural networks are also widely used to model sentences (Collobert et al., 2011; Kalchbrenner et al., 2014; Kim, 2014; Hu et al., 2014).", "startOffset": 112, "endOffset": 191}, {"referenceID": 30, "context": "Concerning NMT, the conventional NMT relies almost exclusively on word-level source sentence modelling with explicit tokenizations (Sutskever et al., 2014; Cho et al., 2014; Bahdanau et al., 2015), which tends to suffer from the problem of unknown words.", "startOffset": 131, "endOffset": 196}, {"referenceID": 2, "context": "Concerning NMT, the conventional NMT relies almost exclusively on word-level source sentence modelling with explicit tokenizations (Sutskever et al., 2014; Cho et al., 2014; Bahdanau et al., 2015), which tends to suffer from the problem of unknown words.", "startOffset": 131, "endOffset": 196}, {"referenceID": 29, "context": "In this respect, Costa-Juss\u00e0 and Fonollosa (2016) applied character-based embeddings in combination with convolutional and highway layers (Srivastava et al., 2015) to produce word embeddings.", "startOffset": 138, "endOffset": 163}, {"referenceID": 2, "context": ", 2014; Cho et al., 2014; Bahdanau et al., 2015), which tends to suffer from the problem of unknown words. To address this problem, researchers have proposed alternative characterbased modelling. In this respect, Costa-Juss\u00e0 and Fonollosa (2016) applied character-based embeddings in combination with convolutional and highway layers (Srivastava et al.", "startOffset": 8, "endOffset": 246}, {"referenceID": 2, "context": ", 2014; Cho et al., 2014; Bahdanau et al., 2015), which tends to suffer from the problem of unknown words. To address this problem, researchers have proposed alternative characterbased modelling. In this respect, Costa-Juss\u00e0 and Fonollosa (2016) applied character-based embeddings in combination with convolutional and highway layers (Srivastava et al., 2015) to produce word embeddings. Similarly, Ling et al. (2015) used a bidirectional LSTM to generate semantic representations of words based on character embeddings.", "startOffset": 8, "endOffset": 418}, {"referenceID": 2, "context": ", 2014; Cho et al., 2014; Bahdanau et al., 2015), which tends to suffer from the problem of unknown words. To address this problem, researchers have proposed alternative characterbased modelling. In this respect, Costa-Juss\u00e0 and Fonollosa (2016) applied character-based embeddings in combination with convolutional and highway layers (Srivastava et al., 2015) to produce word embeddings. Similarly, Ling et al. (2015) used a bidirectional LSTM to generate semantic representations of words based on character embeddings. A slightly different approach was proposed by Lee et al. (2015), where they explicitly marked each character with its relative location in a word.", "startOffset": 8, "endOffset": 585}, {"referenceID": 2, "context": ", 2014; Cho et al., 2014; Bahdanau et al., 2015), which tends to suffer from the problem of unknown words. To address this problem, researchers have proposed alternative characterbased modelling. In this respect, Costa-Juss\u00e0 and Fonollosa (2016) applied character-based embeddings in combination with convolutional and highway layers (Srivastava et al., 2015) to produce word embeddings. Similarly, Ling et al. (2015) used a bidirectional LSTM to generate semantic representations of words based on character embeddings. A slightly different approach was proposed by Lee et al. (2015), where they explicitly marked each character with its relative location in a word. Recently, Chung et al. (2016) evaluated a character-level decoder without explicit segmentations for NMT.", "startOffset": 8, "endOffset": 698}, {"referenceID": 26, "context": "ours are those proposed by Tai et al., (2015), Le et al.", "startOffset": 27, "endOffset": 46}, {"referenceID": 26, "context": "ours are those proposed by Tai et al., (2015), Le et al., (2015b), Kalchbrenner et al.", "startOffset": 27, "endOffset": 66}, {"referenceID": 15, "context": ", (2015b), Kalchbrenner et al., (2015), Soltani and Jiang (2016).", "startOffset": 11, "endOffset": 39}, {"referenceID": 15, "context": ", (2015b), Kalchbrenner et al., (2015), Soltani and Jiang (2016). Tai et al.", "startOffset": 11, "endOffset": 65}, {"referenceID": 15, "context": ", (2015b), Kalchbrenner et al., (2015), Soltani and Jiang (2016). Tai et al., (2015) presented treestructured LSTMs, while Le and Zuidema (2015b) further introduced the forest convolutional network for sentence modelling.", "startOffset": 11, "endOffset": 85}, {"referenceID": 15, "context": ", (2015b), Kalchbrenner et al., (2015), Soltani and Jiang (2016). Tai et al., (2015) presented treestructured LSTMs, while Le and Zuidema (2015b) further introduced the forest convolutional network for sentence modelling.", "startOffset": 11, "endOffset": 146}, {"referenceID": 15, "context": ", (2015b), Kalchbrenner et al., (2015), Soltani and Jiang (2016). Tai et al., (2015) presented treestructured LSTMs, while Le and Zuidema (2015b) further introduced the forest convolutional network for sentence modelling. Kalchbrenner et al., (2015) studied grid RNNs where the inputs are arranged in a multi-dimensional grid.", "startOffset": 11, "endOffset": 250}, {"referenceID": 15, "context": ", (2015b), Kalchbrenner et al., (2015), Soltani and Jiang (2016). Tai et al., (2015) presented treestructured LSTMs, while Le and Zuidema (2015b) further introduced the forest convolutional network for sentence modelling. Kalchbrenner et al., (2015) studied grid RNNs where the inputs are arranged in a multi-dimensional grid. In higher order RNNs proposed by Soltani and Jiang (2016), more memory units are used to record more preceding states, which are all recurrently fed to the hidden layers as feedbacks through different weighted paths.", "startOffset": 11, "endOffset": 385}], "year": 2016, "abstractText": "Neural machine translation (NMT) heavily relies on word-level modelling to learn semantic representations of input sentences. However, for languages without natural word delimiters (e.g., Chinese) where input sentences have to be tokenized first, conventional NMT is confronted with two issues: 1) it is difficult to find an optimal tokenization granularity for source sentence modelling, and 2) errors in 1-best tokenizations may propagate to the encoder of NMT. To handle these issues, we propose word-lattice based Recurrent Neural Network (RNN) encoders for NMT, which generalize the standard RNN to word lattice topology. The proposed encoders take as input a word lattice that compactly encodes multiple tokenizations, and learn to generate new hidden states from arbitrarily many inputs and hidden states in preceding time steps. As such, the word-lattice based encoders not only alleviate the negative impact of tokenization errors but also are more expressive and flexible to embed input sentences. Experiment results on ChineseEnglish translation demonstrate the superiorities of the proposed encoders over the conventional encoder.", "creator": "TeX"}}}