{"id": "1609.07843", "review": {"conference": "iclr", "VERSION": "v1", "DATE_OF_SUBMISSION": "26-Sep-2016", "title": "Pointer Sentinel Mixture Models", "abstract": "Recent neural network sequence models with softmax classifiers have achieved their best language modeling performance only with very large hidden states and large vocabularies. Even then they struggle to predict rare or unseen words even if the context makes the prediction unambiguous. We introduce the pointer sentinel mixture architecture for neural sequence models which has the ability to either reproduce a word from the recent context or produce a word from a standard softmax classifier. Our pointer sentinel-LSTM model achieves state of the art language modeling performance on the Penn Treebank (70.9 perplexity) while using far fewer parameters than a standard softmax LSTM. In order to evaluate how well language models can exploit longer contexts and deal with more realistic vocabularies and larger corpora we also introduce the freely available WikiText corpus.", "histories": [["v1", "Mon, 26 Sep 2016 04:06:13 GMT  (654kb,D)", "http://arxiv.org/abs/1609.07843v1", null]], "reviews": [], "SUBJECTS": "cs.CL cs.AI", "authors": ["stephen merity", "caiming xiong", "james bradbury", "richard socher"], "accepted": true, "id": "1609.07843"}, "pdf": {"name": "1609.07843.pdf", "metadata": {"source": "META", "title": "Pointer Sentinel Mixture Models", "authors": ["Stephen Merity", "Caiming Xiong", "James Bradbury", "Richard Socher"], "emails": ["SMERITY@SALESFORCE.COM", "CXIONG@SALESFORCE.COM", "JAMES.BRADBURY@SALESFORCE.COM", "RSOCHER@SALESFORCE.COM"], "sections": [{"heading": "1. Introduction", "text": "For example, imagine a new person being introduced, and two paragraphs later, the context would allow you to accurately predict that person's name as the next word. For standard neural sequence models, to predict that name, they would have to encrypt the name, store it in its hidden state for many time steps, and then decrypt it when appropriate. As the hidden state is limited in its capacity, and the optimization of such models suffers from the disappearing gradient problem, this is a lossy process when executed over many time carpets, especially for rare words with soft attention or memory components that have been proposed to deal with this challenge and achieve the goal of making relevant hidden1Available for downloading from WikiText sitep (Yellen) g (Ypptvelles) (Ypptvelles) (Ypptvocelles) (1)."}, {"heading": "2. The Pointer Sentinel for Language Modeling", "text": "Considering a word sequence w1,., wN \u2212 1, our task is to predict the next word wN.2.1. The softmax-RNN ComponentRecurrent Neural Networks (RNNNs) have been widely used in language modeling because of their ability, at least in theory, to maintain long-term dependencies. RNNs use the chain rule to factorize the hidden probabilities of the RNN over a sequence of tokens: p (w1,.., wN) = 1 p (wi | w1,., wi \u2212 1). Specifically, at each step i, we calculate the hidden state hi according to the previous hidden state hi \u2212 1 and the input xi so that hi = RNN (xi, hi \u2212 1) = 1 p (xi, hi \u2212 1). If all the words of the N \u2212 1 vocabulary have been processed by the RNN, the final state hN \u2212 1 is fed into a soft problem that can maximize the probability of the vocabulary."}, {"heading": "2.2. The Pointer Network Component", "text": "This year it has come to the point where we will be able to find ourselves in the position we are in. () D \"i\" r \"i\" n \"i\" n, in which we are able. () D \"i\" n \"i\" n, in which we are able to find ourselves. () D \"i\" n \"i\" n \"i\" n \"n\" i \"n\" n \"i\" n, in which we are able. \"() D\" i \"n\" n \"i\" n \"n\" i \"n\" n \"n\" n \"i\" n \"n\" i \"n\" n \"i\" n \"n\" n. (n \"i\" n \"i\" n \"n\" i \"n\" i \"n\" i \"n\" i \"n\" i \"n\" i \"n\" i \"n\" n \"i\" n. \"n.\""}, {"heading": "2.3. The Pointer Sentinel Mixture Model", "text": "While pointer networks have proven effective, they cannot predict output words that are not present in the input, a common scenario in speech modeling. We suggest solving this by using a blending model that combines a standard Softmax component with a pointer component. Our blending model has two basic distributions: the Softmax vocabulary of the RNN output and the position vocabulary of the pointer model. We call these the RNN component and the pointer component, respectively. To combine the two base distributions, we use a g = p rating function (zi = k | xi), where zi is the latent variable that indicates which base distribution the data point belongs to. Since we have only two base distributions, g can generate a scalar in the range [0, 1]. A value of 0 implies that only the pointer is used, and 1 means that the soft Nmax model is used."}, {"heading": "2.4. Details of the Gating Function", "text": "In order to calculate the new pointer sentinel gate g, we modify the pointer component. In particular, we add an additional element to z, the vector of attention values according to Equation 3. This element is calculated using an internal product between the query and the sentinel 2 vector s-RH. This change can be summarized by changing Equation 4 to: a = Softmax ([z; qT s]. (7) We define a probability mass assigned to g as attention distribution both by the words in the pointer window and by the sentinel state. We interpret the last element of this vector as the gate value: g = a [V + 1]. Each probability mass assigned to g is assigned to the standard Softmax vocabulary of the RNN. The final updated, normalized pointer probability via the vocabulary of this vector becomes directly the GNN (Gxi-V) component (then: \u2212 11)."}, {"heading": "2.5. Motivation for the Sentinel as Gating Function", "text": "In order to make the best possible decision about which component the gating function should use, as much context as possible must be present. As we increase both the number of timetables and the window of words for the pointer component to be considered, the hidden state of the RNN alone does not guarantee that the value of the pointers is inserted at the end of a search space to ensure that a search algorithm is terminated if no matching element is found. Our Sentinel value ends the pointer search space and distributes the rest of the probability mass into the RNN vocabulary. Recall the exact identity or order of the words you have recently seen (Adi et al., 2016). This is an obvious limitation of coding a variable length sequence into a fixed dimensionality vector. In our task, where we may want a pointer window in which the length L is in the hundreds, the exact modeling of all this hidden state information within the N. must be present."}, {"heading": "2.6. Pointer Sentinel Loss Function", "text": "We minimize the cross-entropy loss of \u2212 \u2211 j y-ij log p (yij | xi), where y-i is a uniform encoding of the correct output. Since y-i is a hot word, only a single mixed probability p (yij) has to be calculated to calculate the loss, which can result in a much more efficient GPU implementation. At the prediction time, if we want to have all the values for p (yi | xi), a maximum of L-word probabilities has to be mixed, since there is a maximum of L-unique words in the pointer window of length L. This mixing can occur on the CPU, where random access indexing is more efficient than the GPU. Following the pointer sum network, the goal is to set probability mass from the attention mechanism to the correct output y-i to all the correct output y-i, if it exists in the input model, so the probability of our component I is assigned to \u2212 instead (In the case of our mixing log, the probability of our whole component I is assigned \u2212)."}, {"heading": "2.7. Parameters and Computation Time", "text": "The blending model pointer monitor LSTM results in a relatively small increase in parameters and computing time, especially compared to the size of the models required to achieve similar performance with standard LSTM models. The only two additional parameters that the model requires are those for the calculation q, in particular W-RH-H and b-RH, and the sentinel vector embedding, s-RH. This is independent of the depth of the RNN, as the pointer component only interacts with the output of the final RNN layer. The additional H2 + 2H parameters are insignificant compared to the 8H2 + 4H parameters of a single LSTM layer. Furthermore, most modern models require multiple LSTM layers. In terms of additional calculation, a pointer guard LSTM of window size L only requires the calculation of the query q (a linear layer with tanh activation), a total quantity of L and the product calibration for the internal L."}, {"heading": "3. Related Work", "text": "This year, it has reached the point where it will be able to retaliate."}, {"heading": "4. WikiText - A Benchmark for Language Modeling", "text": "We first describe the most commonly used language modeling dataset and its pre-processing, then motivate the need for a new benchmark dataset."}, {"heading": "4.1. Penn Treebank", "text": "To compare our model with the many newer neural language models, we perform word-level prediction experiments with the Penn Treebank (PTB) dataset (Marcus et al., 1993), which was pre-processed by Mikolov et al. (2010) The dataset consists of 929k training words, 73k validation words, and 82k test words. In pre-processing performed by Mikolov et al. (2010), words were reduced in size, numbers replaced by N, line breaks replaced by < eos >, and all other punctuations removed. Vocabulary consists of the most common 10k words, which are replaced by a < unk > token. For complete statistics, see Table 1."}, {"heading": "4.2. Reasons for a New Dataset", "text": "While the processed version of the aforementioned PTB has often been used for language modeling, it has many limitations. PTB tokens are all lowercase, with no punctuation and limited to a vocabulary of just 10k. These limitations mean that PTB is unrealistic for real-world use, especially when it comes to much larger vocabulary with many rare words. Figure 3 illustrates this with a zippy diagram above the training section of PTB. The curve stops abruptly when you hit the 10k vocabulary. Given that accurately predicting rare words, such as named units, is an important task for many applications, the lack of a long tail for the vocabulary is problematic. Other major language modeling datasets exist. Unfortunately, they either have restrictive licenses that prevent widespread use, or they have randomized sentences (Chelba et al., 2013), which allow for most of these to be used unrealistically and for longer-term use."}, {"heading": "4.3. Construction and Pre-processing", "text": "These articles have been reviewed by people and are considered well-written, factually accurate, comprehensive in reporting, neutral in viewpoint and stable, resulting in 23,805 good articles and 4,790 featured articles. The text for each article has been extracted using the Wikipedia API. Extracting the raw text from the Wikipedia marker is not trivial due to the large number of macros used. These macros are also widely used and include metric conversion, abbreviations, language notation and date porting.Once extracted, certain sections that primarily contained lists have been removed by default. Other minor errors, such as sorting keys and editing buttons that have leaked from the HTML, have also been removed. Mathematical formulas and LATEX code have been replaced by < Formula > Token. < Normalization and vocabulary have been performed using the Moses Tokenizer (Koehkenizer)."}, {"heading": "4.4. Statistics", "text": "The full WikiText dataset is over 103 million words in size, a hundred times larger than the PTB. It is also one-tenth the size of the One Billion Word benchmark (Chelba et al., 2013), one of the largest publicly available language modeling benchmarks, while it consists of articles that allow the capture and use of long-term dependencies found in many real-world tasks. WikiText-103 comes in two different sizes: WikiText-2 and WikiText-103. Both have punctuation, original text, a larger vocabulary and numbers. WikiText-2 is twice the size of the Penn Treebank dataset. WikiText-103 contains all the extracted articles. Both datasets use the same articles for validation and verification, with the only difference in vocabularies. For complete statistics, see Table 1."}, {"heading": "5. Experiments", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "5.1. Training Details", "text": "Since the pointer sentinel mixing model uses the results of the RNN of up to L timelines, this poses a challenge for the training. If we do not regenerate the outdated historical results of the RNN when we update the gradients, the back propagation through these outdated results can lead to incorrect gradient updates. If we regenerate all outdated results of the RNN, the training process is much slower. As we cannot give theoretical guarantees about the effects of the outdated results on gradient updates, we opt for the window of the RNN output generated by the pointer component after each gradient update. We also use truncated backward propagation through time (BPTT) in different ways for many other RNN language models."}, {"heading": "5.2. Model Details", "text": "Our experimental setup mirrors that of Zaremba et al. (2014) and Gal (2015). We have increased the number of timesteps used during training from 35 to 100, according to the length of window L. The batch size has been increased from 20 to 32. We also halve the learning rate if the validation perplexity is worse than the previous iteration. We stop training if the validation perplexity does not improve for three epochs or when 64 epochs are reached. The gradients are recalculated if their global standard is above 1 (Pascanu et al., 2013b).3 We evaluate the configuration of the middle model, which has a hidden size of H = 650 and a double-layered LSTM. We compare it with the large model configuration u-3The highly aggressive truncation is probably due to the increased BPTT length. Even with such truncating of previous batches, there may be excessive perplexity, although this occurs rapidly."}, {"heading": "5.3. Comparison over Penn Treebank", "text": "Table 2 compares the sentinel-LSTM pointer with a variety of other models on the Penn Treebank dataset. The sentinel-LSTM pointer achieves the lowest perplexity, followed by the most recent Recurrent Highway Networks (Zilly et al., 2016).The sentinel-LSTM median pointer also achieves lower perplexity than the major LSTM models. Note that the most powerful large variational LSTM model uses computationally intensive Monte Carlo (MC) dropout average.The sentinel-LSTM median pointer is a general improvement for any sequence model that uses dropout, but comes with significantly increased test time costs. In Gal (2015), it requires a reevaluation of the test model with 1000 different dropout masks. The sentinel-LSTM pointer is able to achieve these results with far fewer parameters than other models of comparable performance, especially with less than one-third of the STeps used in the STM."}, {"heading": "5.4. Comparison over WikiText-2", "text": "Since WikiText-2 is introduced in this dataset, there are no existing baselines. We provide two baselines to compare the pointer guard LSTM with those: our variation LSTM with zoneout and the middle variation LSTM used in Gal (2015).4 Attempts to perform the large model variant Gal (2015), a two-layered LSTM with hidden size 1500, resulted in memory errors on a 12GB K80 GPU, probably due to the larger vocabulary size. We selected the best hyperparameters from PTB experiments for all models. Table 3 shows a similar gain that the pointer guard LSTM achieved over the variable LSTM models. Gal's variable LSTM (2015) again beats the variable LSTM that was used as the basis for our experiments."}, {"heading": "6. Analysis", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "6.1. Impact on Rare Words", "text": "One hypothesis as to why the sentinel-LSTM pointer can outperform an LSTM is that the pointer component allows the model to effectively reproduce rare words. An RNN may be able to make better use of the hidden state capacity by moving to the pointer component. The pointer component may also allow a sharper selection of a single word than only with Softmax. Figure 4 shows the improvement in confusion when comparing the LSTM with the sentinel-LSTM pointer with words that are split by buckets depending on frequency. It shows that the sentinel-LSTM pointer has greater improvements as words become rarely.Even on the Penn Treebank, where there is a relative lack of rare words, since only the most common 10k words are selected, we can see the sentinel-LSTM pointer that enhances the Lyeel-LSTM pointer as words become rarely.Even on the Penn Treebank, where there is a relative lack of rare words, since only the most common 10k words are selected, we can see the STM pointer that enhances the STINEL-Lyeel-LSTM pointer directly, while the STINM is the STINSTINEL-STINT.While the STINEL is the most directly beneficial word by STINSTINT.We can see the STINTENT directly by STINTER directly using the STINSTM, the STINEL is the STINTENT component by STINT.While the STM is the STINTENT directly using the STM is the STINTENT component, the STM is the most beneficial word, the STINGENT is relatively few."}, {"heading": "6.2. Qualitative Analysis of Pointer Usage", "text": "In a qualitative analysis, we visualized the use of the gate and the attention of the pointer for a variety of examples in the validation group, focusing on predictions where the gate primarily uses the pointer component. As expected, the pointer component is heavily used for rare names such as Seidman (23 times in training), Iverson (7 times in training) and Rosenthal (3 times in training).The pointer component was also heavily used when it came to other named entity names such as Honeywell (8 times in training) and Integrated (41 times in training, although due to the reduction of words these include integrated circuits, fully integrated and other generic use).Surprisingly, the pointer component was also used for many commonly used tokens. When selecting the unit of measurement (tons, kilograms,...) or the short scale of numbers (thousands, millions, billions, etc.), the pointer component was frequently used for many tokens."}, {"heading": "7. Conclusion", "text": "We introduced WikiText's Pointer Modeling Model and Voice Modeling Dataset. This model delivers state-of-the-art results in voice modeling over Penn Treebank, while using few additional parameters and little additional computational complexity at prediction time.We also motivated the need to move from Penn Treebank to a new Voice Modeling Dataset for Long-Term Dependencies that provides WikiText-2 and WikiText-103 as potential options. We hope this new dataset can serve as a platform for improving the use of rare words and long-term dependencies in voice modeling."}], "references": [{"title": "Fine-grained Analysis of Sentence Embeddings Using Auxiliary Prediction Tasks", "author": ["Adi", "Yossi", "Kermany", "Einat", "Belinkov", "Yonatan", "Lavi", "Ofer", "Goldberg", "Yoav"], "venue": "arXiv preprint arXiv:1608.04207,", "citeRegEx": "Adi et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Adi et al\\.", "year": 2016}, {"title": "Neural Machine Translation by Jointly Learning to Align and Translate", "author": ["Bahdanau", "Dzmitry", "Cho", "Kyunghyun", "Bengio", "Yoshua"], "venue": "In ICLR,", "citeRegEx": "Bahdanau et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Bahdanau et al\\.", "year": 2015}, {"title": "One Billion Word Benchmark for Measuring Progress in Statistical Language Modeling", "author": ["Chelba", "Ciprian", "Mikolov", "Tomas", "Schuster", "Mike", "Ge", "Qi", "Brants", "Thorsten", "Koehn", "Phillipp", "Robinson", "Tony"], "venue": "arXiv preprint arXiv:1312.3005,", "citeRegEx": "Chelba et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Chelba et al\\.", "year": 2013}, {"title": "Language Modeling with Sum-Product Networks", "author": ["Cheng", "Wei-Chen", "Kok", "Stanley", "Pham", "Hoai Vu", "Chieu", "Hai Leong", "Chai", "Kian Ming Adam"], "venue": "In INTERSPEECH,", "citeRegEx": "Cheng et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Cheng et al\\.", "year": 2014}, {"title": "A Theoretically Grounded Application of Dropout in Recurrent Neural Networks", "author": ["Gal", "Yarin"], "venue": "arXiv preprint arXiv:1512.05287,", "citeRegEx": "Gal and Yarin.,? \\Q2015\\E", "shortCiteRegEx": "Gal and Yarin.", "year": 2015}, {"title": "Long ShortTerm Memory", "author": ["Hochreiter", "Sepp", "Schmidhuber", "J\u00fcrgen"], "venue": "Neural Computation,", "citeRegEx": "Hochreiter et al\\.,? \\Q1997\\E", "shortCiteRegEx": "Hochreiter et al\\.", "year": 1997}, {"title": "Text Understanding with the Attention Sum Reader Network", "author": ["Kadlec", "Rudolf", "Schmid", "Martin", "Bajgar", "Ondrej", "Kleindienst", "Jan"], "venue": "arXiv preprint arXiv:1603.01547,", "citeRegEx": "Kadlec et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Kadlec et al\\.", "year": 2016}, {"title": "Ask me anything: Dynamic memory networks for natural language processing", "author": ["Kumar", "Ankit", "Irsoy", "Ozan", "Ondruska", "Peter", "Iyyer", "Mohit", "Bradbury", "James", "Gulrajani", "Ishaan", "Zhong", "Victor", "Paulus", "Romain", "Socher", "Richard"], "venue": null, "citeRegEx": "Kumar et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Kumar et al\\.", "year": 2016}, {"title": "Latent Predictor Networks for Code Generation", "author": ["Ling", "Wang", "Grefenstette", "Edward", "Hermann", "Karl Moritz", "Kocisk\u00fd", "Tom\u00e1s", "Senior", "Andrew", "Fumin", "Blunsom", "Phil"], "venue": null, "citeRegEx": "Ling et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Ling et al\\.", "year": 2016}, {"title": "Building a Large Annotated Corpus of English: The Penn Treebank", "author": ["Marcus", "Mitchell P", "Santorini", "Beatrice", "Marcinkiewicz", "Mary Ann"], "venue": "Computational Linguistics,", "citeRegEx": "Marcus et al\\.,? \\Q1993\\E", "shortCiteRegEx": "Marcus et al\\.", "year": 1993}, {"title": "Context dependent recurrent neural network language model", "author": ["Mikolov", "Tomas", "Zweig", "Geoffrey"], "venue": "In SLT,", "citeRegEx": "Mikolov et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Mikolov et al\\.", "year": 2012}, {"title": "Recurrent neural network based language model", "author": ["Mikolov", "Tomas", "Karafi\u00e1t", "Martin", "Burget", "Luk\u00e1s", "Cernock\u00fd", "Jan", "Khudanpur", "Sanjeev"], "venue": "In INTERSPEECH,", "citeRegEx": "Mikolov et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Mikolov et al\\.", "year": 2010}, {"title": "On the difficulty of training recurrent neural networks", "author": ["Pascanu", "Razvan", "Mikolov", "Tomas", "Bengio", "Yoshua"], "venue": "In ICML,", "citeRegEx": "Pascanu et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Pascanu et al\\.", "year": 2013}, {"title": "A Maximum Entropy Approach to Adaptive Statistical Language Modeling", "author": ["Rosenfeld", "Roni"], "venue": null, "citeRegEx": "Rosenfeld and Roni.,? \\Q1996\\E", "shortCiteRegEx": "Rosenfeld and Roni.", "year": 1996}, {"title": "End-To-End Memory Networks", "author": ["Sukhbaatar", "Sainbayar", "Szlam", "Arthur", "Weston", "Jason", "Fergus", "Rob"], "venue": "In NIPS,", "citeRegEx": "Sukhbaatar et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Sukhbaatar et al\\.", "year": 2015}, {"title": "Dynamic Memory Networks for Visual and Textual Question Answering", "author": ["Xiong", "Caiming", "Merity", "Stephen", "Socher", "Richard"], "venue": "In ICML,", "citeRegEx": "Xiong et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Xiong et al\\.", "year": 2016}, {"title": "Recurrent neural network regularization", "author": ["Zaremba", "Wojciech", "Sutskever", "Ilya", "Vinyals", "Oriol"], "venue": "arXiv preprint arXiv:1409.2329,", "citeRegEx": "Zaremba et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Zaremba et al\\.", "year": 2014}, {"title": "Recurrent Highway Networks", "author": ["Zilly", "Julian Georg", "Srivastava", "Rupesh Kumar", "Koutn\u0131\u0301k", "Jan", "Schmidhuber", "J\u00fcrgen"], "venue": "arXiv preprint arXiv:1607.03474,", "citeRegEx": "Zilly et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Zilly et al\\.", "year": 2016}], "referenceMentions": [{"referenceID": 11, "context": "The softmax-RNN Component Recurrent neural networks (RNNs) have seen widespread use for language modeling (Mikolov et al., 2010) due to their ability to, at least in theory, retain long term dependencies.", "startOffset": 106, "endOffset": 128}, {"referenceID": 6, "context": "This technique, referred to as pointer sum attention, has been used for question answering (Kadlec et al., 2016).", "startOffset": 91, "endOffset": 112}, {"referenceID": 0, "context": "accurately recall the identity or order of words it has recently seen (Adi et al., 2016).", "startOffset": 70, "endOffset": 88}, {"referenceID": 11, "context": "Beyond n-grams, neural sequence models such as recurrent neural networks have been shown to achieve state of the art results (Mikolov et al., 2010).", "startOffset": 125, "endOffset": 147}, {"referenceID": 16, "context": "A variety of RNN regularization methods have been explored, including a number of dropout variations (Zaremba et al., 2014; Gal, 2015) which prevent overfitting of complex LSTM language models.", "startOffset": 101, "endOffset": 134}, {"referenceID": 17, "context": "Other work has improved language modeling performance by modifying the RNN architecture to better handle increased recurrence depth (Zilly et al., 2016).", "startOffset": 132, "endOffset": 152}, {"referenceID": 1, "context": "els have also added a soft attention or memory component (Bahdanau et al., 2015; Sukhbaatar et al., 2015; Cheng et al., 2016; Kumar et al., 2016; Xiong et al., 2016; Ahn et al., 2016).", "startOffset": 57, "endOffset": 183}, {"referenceID": 14, "context": "els have also added a soft attention or memory component (Bahdanau et al., 2015; Sukhbaatar et al., 2015; Cheng et al., 2016; Kumar et al., 2016; Xiong et al., 2016; Ahn et al., 2016).", "startOffset": 57, "endOffset": 183}, {"referenceID": 7, "context": "els have also added a soft attention or memory component (Bahdanau et al., 2015; Sukhbaatar et al., 2015; Cheng et al., 2016; Kumar et al., 2016; Xiong et al., 2016; Ahn et al., 2016).", "startOffset": 57, "endOffset": 183}, {"referenceID": 15, "context": "els have also added a soft attention or memory component (Bahdanau et al., 2015; Sukhbaatar et al., 2015; Cheng et al., 2016; Kumar et al., 2016; Xiong et al., 2016; Ahn et al., 2016).", "startOffset": 57, "endOffset": 183}, {"referenceID": 6, "context": "A drawback to soft attention is that if, for instance, January and March are both equally attended candidates, the attention mechanism may blend the two vectors, resulting in a context vector closest to February (Kadlec et al., 2016).", "startOffset": 212, "endOffset": 233}, {"referenceID": 8, "context": ", 2015), code generation (Ling et al., 2016), summarization (Gu et al.", "startOffset": 25, "endOffset": 44}, {"referenceID": 6, "context": ", 2016), question answering (Kadlec et al., 2016).", "startOffset": 28, "endOffset": 49}, {"referenceID": 8, "context": "Extending this concept further, the latent predictor network (Ling et al., 2016) generates an output sequence conditioned on an arbitrary number of base models where each base model may have differing granularity.", "startOffset": 61, "endOffset": 80}, {"referenceID": 1, "context": "els have also added a soft attention or memory component (Bahdanau et al., 2015; Sukhbaatar et al., 2015; Cheng et al., 2016; Kumar et al., 2016; Xiong et al., 2016; Ahn et al., 2016). These mechanisms allow for the retrieval and use of relevant previous hidden states. Soft attention mechanisms need to first encode the relevant word into a state vector and then decode it again, even if the output word is identical to the input word used to compute that hidden state or memory. A drawback to soft attention is that if, for instance, January and March are both equally attended candidates, the attention mechanism may blend the two vectors, resulting in a context vector closest to February (Kadlec et al., 2016). Even with attention, the standard softmax classifier being used in these models often struggles to correctly predict rare or previously unknown words. Attention-based pointer mechanisms were introduced in Vinyals et al. (2015) where the pointer network is able to select elements from the input as output.", "startOffset": 58, "endOffset": 943}, {"referenceID": 1, "context": "els have also added a soft attention or memory component (Bahdanau et al., 2015; Sukhbaatar et al., 2015; Cheng et al., 2016; Kumar et al., 2016; Xiong et al., 2016; Ahn et al., 2016). These mechanisms allow for the retrieval and use of relevant previous hidden states. Soft attention mechanisms need to first encode the relevant word into a state vector and then decode it again, even if the output word is identical to the input word used to compute that hidden state or memory. A drawback to soft attention is that if, for instance, January and March are both equally attended candidates, the attention mechanism may blend the two vectors, resulting in a context vector closest to February (Kadlec et al., 2016). Even with attention, the standard softmax classifier being used in these models often struggles to correctly predict rare or previously unknown words. Attention-based pointer mechanisms were introduced in Vinyals et al. (2015) where the pointer network is able to select elements from the input as output. In the above example, only January or March would be available as options, as February does not appear in the input. The use of pointer networks have been shown to help with geometric problems (Vinyals et al., 2015), code generation (Ling et al., 2016), summarization (Gu et al., 2016; G\u00fcl\u00e7ehre et al., 2016), question answering (Kadlec et al., 2016). While pointer networks improve performance on rare words and long-term dependencies they are unable to select words that do not exist in the input. G\u00fcl\u00e7ehre et al. (2016) introduce a pointer softmax model that can generate output from either the vocabulary softmax of an RNN or the location softmax of the pointer network.", "startOffset": 58, "endOffset": 1545}, {"referenceID": 1, "context": "els have also added a soft attention or memory component (Bahdanau et al., 2015; Sukhbaatar et al., 2015; Cheng et al., 2016; Kumar et al., 2016; Xiong et al., 2016; Ahn et al., 2016). These mechanisms allow for the retrieval and use of relevant previous hidden states. Soft attention mechanisms need to first encode the relevant word into a state vector and then decode it again, even if the output word is identical to the input word used to compute that hidden state or memory. A drawback to soft attention is that if, for instance, January and March are both equally attended candidates, the attention mechanism may blend the two vectors, resulting in a context vector closest to February (Kadlec et al., 2016). Even with attention, the standard softmax classifier being used in these models often struggles to correctly predict rare or previously unknown words. Attention-based pointer mechanisms were introduced in Vinyals et al. (2015) where the pointer network is able to select elements from the input as output. In the above example, only January or March would be available as options, as February does not appear in the input. The use of pointer networks have been shown to help with geometric problems (Vinyals et al., 2015), code generation (Ling et al., 2016), summarization (Gu et al., 2016; G\u00fcl\u00e7ehre et al., 2016), question answering (Kadlec et al., 2016). While pointer networks improve performance on rare words and long-term dependencies they are unable to select words that do not exist in the input. G\u00fcl\u00e7ehre et al. (2016) introduce a pointer softmax model that can generate output from either the vocabulary softmax of an RNN or the location softmax of the pointer network. Not only does this allow for producing OoV words which are not in the input, the pointer softmax model is able to better deal with rare and unknown words than a model only featuring an RNN softmax. Rather than constructing a mixture model as in our work, they use a switching network to decide which component to use. For neural machine translation, the switching network is conditioned on the representation of the context of the source text and the hidden state of the decoder. The pointer network is not used as a source of information for switching network as in our model. The pointer and RNN softmax are scaled according to the switching network and the word or location with the highest final attention score is selected for output. Although this approach uses both a pointer and RNN component, it is not a mixture model and does not combine the probabilities for a word if it occurs in both the pointer location softmax and the RNN vocabulary softmax. In our model the word probability is a mix of both the RNN and pointer components, allowing for better predictions when the context may be ambiguous. Extending this concept further, the latent predictor network (Ling et al., 2016) generates an output sequence conditioned on an arbitrary number of base models where each base model may have differing granularity. In their task of code generation, the output could be produced one character at a time using a standard softmax or instead copy entire words from referenced text fields using a pointer network. As opposed to G\u00fcl\u00e7ehre et al. (2016), all states which produce the same output are merged by summing their probabilities.", "startOffset": 58, "endOffset": 3252}, {"referenceID": 9, "context": "Penn Treebank In order to compare our model to the many recent neural language models, we conduct word-level prediction experiments on the Penn Treebank (PTB) dataset (Marcus et al., 1993), pre-processed by Mikolov et al.", "startOffset": 167, "endOffset": 188}, {"referenceID": 9, "context": "Penn Treebank In order to compare our model to the many recent neural language models, we conduct word-level prediction experiments on the Penn Treebank (PTB) dataset (Marcus et al., 1993), pre-processed by Mikolov et al. (2010). The dataset consists of 929k training words, 73k validation words, and 82k test words.", "startOffset": 168, "endOffset": 229}, {"referenceID": 9, "context": "Penn Treebank In order to compare our model to the many recent neural language models, we conduct word-level prediction experiments on the Penn Treebank (PTB) dataset (Marcus et al., 1993), pre-processed by Mikolov et al. (2010). The dataset consists of 929k training words, 73k validation words, and 82k test words. As part of the pre-processing performed by Mikolov et al. (2010), words were lower-cased, numbers were replaced with N, newlines were replaced with \u3008eos\u3009, and all other punctuation was removed.", "startOffset": 168, "endOffset": 382}, {"referenceID": 2, "context": "Unfortunately, they either have restrictive licensing which prevents widespread use or have randomized sentence ordering (Chelba et al., 2013) which is unrealistic for most language use and prevents the effective learning and evaluation of longer term dependencies.", "startOffset": 121, "endOffset": 142}, {"referenceID": 2, "context": "Following Chelba et al. (2013) a vocabulary was constructed by discarding all words with a count below 3.", "startOffset": 10, "endOffset": 31}, {"referenceID": 2, "context": "It is also a tenth the size of the One Billion Word Benchmark (Chelba et al., 2013), one of the largest publicly available language modeling benchmarks, whilst consisting of articles that allow for the capture and usage of longer term dependencies as might be found in many real world tasks.", "startOffset": 62, "endOffset": 83}, {"referenceID": 15, "context": "Model Details Our experimental setup reflects that of Zaremba et al. (2014) and Gal (2015).", "startOffset": 54, "endOffset": 76}, {"referenceID": 15, "context": "Model Details Our experimental setup reflects that of Zaremba et al. (2014) and Gal (2015). We increased the number of timesteps used during training from 35 to 100, matching the length of the window L.", "startOffset": 54, "endOffset": 91}, {"referenceID": 17, "context": "The pointer sentinel-LSTM achieves the lowest perplexity, followed by the recent Recurrent Highway Networks (Zilly et al., 2016).", "startOffset": 108, "endOffset": 128}, {"referenceID": 17, "context": "The pointer sentinel-LSTM achieves the lowest perplexity, followed by the recent Recurrent Highway Networks (Zilly et al., 2016). The medium pointer sentinel-LSTM model also achieves lower perplexity than the large LSTM models. Note that the best performing large variational LSTM model uses computationally intensive Monte Carlo (MC) dropout averaging. Monte Carlo dropout averaging is a general improvement for any sequence model that uses dropout but comes at a greatly increased test time cost. In Gal (2015) it requires rerunning the test model with 1000 different dropout masks.", "startOffset": 109, "endOffset": 513}, {"referenceID": 11, "context": "0 Pascanu et al. (2013a) - Deep RNN 6M \u2212 107.", "startOffset": 2, "endOffset": 25}, {"referenceID": 3, "context": "5 Cheng et al. (2014) - Sum-Prod Net 5M\u2021 \u2212 100.", "startOffset": 2, "endOffset": 22}, {"referenceID": 3, "context": "5 Cheng et al. (2014) - Sum-Prod Net 5M\u2021 \u2212 100.0 Zaremba et al. (2014) - LSTM (medium) 20M 86.", "startOffset": 2, "endOffset": 71}, {"referenceID": 3, "context": "5 Cheng et al. (2014) - Sum-Prod Net 5M\u2021 \u2212 100.0 Zaremba et al. (2014) - LSTM (medium) 20M 86.2 82.7 Zaremba et al. (2014) - LSTM (large) 66M 82.", "startOffset": 2, "endOffset": 123}, {"referenceID": 3, "context": "5 Cheng et al. (2014) - Sum-Prod Net 5M\u2021 \u2212 100.0 Zaremba et al. (2014) - LSTM (medium) 20M 86.2 82.7 Zaremba et al. (2014) - LSTM (large) 66M 82.2 78.4 Gal (2015) - Variational LSTM (medium, untied) 20M 81.", "startOffset": 2, "endOffset": 163}, {"referenceID": 3, "context": "5 Cheng et al. (2014) - Sum-Prod Net 5M\u2021 \u2212 100.0 Zaremba et al. (2014) - LSTM (medium) 20M 86.2 82.7 Zaremba et al. (2014) - LSTM (large) 66M 82.2 78.4 Gal (2015) - Variational LSTM (medium, untied) 20M 81.9\u00b1 0.2 79.7\u00b1 0.1 Gal (2015) - Variational LSTM (medium, untied, MC) 20M \u2212 78.", "startOffset": 2, "endOffset": 234}, {"referenceID": 3, "context": "5 Cheng et al. (2014) - Sum-Prod Net 5M\u2021 \u2212 100.0 Zaremba et al. (2014) - LSTM (medium) 20M 86.2 82.7 Zaremba et al. (2014) - LSTM (large) 66M 82.2 78.4 Gal (2015) - Variational LSTM (medium, untied) 20M 81.9\u00b1 0.2 79.7\u00b1 0.1 Gal (2015) - Variational LSTM (medium, untied, MC) 20M \u2212 78.6\u00b1 0.1 Gal (2015) - Variational LSTM (large, untied) 66M 77.", "startOffset": 2, "endOffset": 301}, {"referenceID": 3, "context": "5 Cheng et al. (2014) - Sum-Prod Net 5M\u2021 \u2212 100.0 Zaremba et al. (2014) - LSTM (medium) 20M 86.2 82.7 Zaremba et al. (2014) - LSTM (large) 66M 82.2 78.4 Gal (2015) - Variational LSTM (medium, untied) 20M 81.9\u00b1 0.2 79.7\u00b1 0.1 Gal (2015) - Variational LSTM (medium, untied, MC) 20M \u2212 78.6\u00b1 0.1 Gal (2015) - Variational LSTM (large, untied) 66M 77.9\u00b1 0.3 75.2\u00b1 0.2 Gal (2015) - Variational LSTM (large, untied, MC) 66M \u2212 73.", "startOffset": 2, "endOffset": 371}, {"referenceID": 3, "context": "5 Cheng et al. (2014) - Sum-Prod Net 5M\u2021 \u2212 100.0 Zaremba et al. (2014) - LSTM (medium) 20M 86.2 82.7 Zaremba et al. (2014) - LSTM (large) 66M 82.2 78.4 Gal (2015) - Variational LSTM (medium, untied) 20M 81.9\u00b1 0.2 79.7\u00b1 0.1 Gal (2015) - Variational LSTM (medium, untied, MC) 20M \u2212 78.6\u00b1 0.1 Gal (2015) - Variational LSTM (large, untied) 66M 77.9\u00b1 0.3 75.2\u00b1 0.2 Gal (2015) - Variational LSTM (large, untied, MC) 66M \u2212 73.4\u00b1 0.0 Kim et al. (2016) - CharCNN 19M \u2212 78.", "startOffset": 2, "endOffset": 444}, {"referenceID": 3, "context": "5 Cheng et al. (2014) - Sum-Prod Net 5M\u2021 \u2212 100.0 Zaremba et al. (2014) - LSTM (medium) 20M 86.2 82.7 Zaremba et al. (2014) - LSTM (large) 66M 82.2 78.4 Gal (2015) - Variational LSTM (medium, untied) 20M 81.9\u00b1 0.2 79.7\u00b1 0.1 Gal (2015) - Variational LSTM (medium, untied, MC) 20M \u2212 78.6\u00b1 0.1 Gal (2015) - Variational LSTM (large, untied) 66M 77.9\u00b1 0.3 75.2\u00b1 0.2 Gal (2015) - Variational LSTM (large, untied, MC) 66M \u2212 73.4\u00b1 0.0 Kim et al. (2016) - CharCNN 19M \u2212 78.9 Zilly et al. (2016) - Variational RHN 32M 72.", "startOffset": 2, "endOffset": 485}, {"referenceID": 16, "context": "For our models and the models of Zaremba et al. (2014) and Gal (2015), medium and large refer to a 650 and 1500 units two layer LSTM respectively.", "startOffset": 33, "endOffset": 55}, {"referenceID": 16, "context": "For our models and the models of Zaremba et al. (2014) and Gal (2015), medium and large refer to a 650 and 1500 units two layer LSTM respectively.", "startOffset": 33, "endOffset": 70}, {"referenceID": 16, "context": "For our models and the models of Zaremba et al. (2014) and Gal (2015), medium and large refer to a 650 and 1500 units two layer LSTM respectively. The medium pointer sentinel-LSTM model achieves lower perplexity than the large LSTM model of Gal (2015) while using a third of the parameters and without using the computationally expensive Monte Carlo (MC) dropout averaging at test time.", "startOffset": 33, "endOffset": 252}, {"referenceID": 16, "context": "For our models and the models of Zaremba et al. (2014) and Gal (2015), medium and large refer to a 650 and 1500 units two layer LSTM respectively. The medium pointer sentinel-LSTM model achieves lower perplexity than the large LSTM model of Gal (2015) while using a third of the parameters and without using the computationally expensive Monte Carlo (MC) dropout averaging at test time. Parameter numbers with \u2021 are estimates based upon our understanding of the model and with reference to Kim et al. (2016).", "startOffset": 33, "endOffset": 508}], "year": 2016, "abstractText": "Recent neural network sequence models with softmax classifiers have achieved their best language modeling performance only with very large hidden states and large vocabularies. Even then they struggle to predict rare or unseen words even if the context makes the prediction unambiguous. We introduce the pointer sentinel mixture architecture for neural sequence models which has the ability to either reproduce a word from the recent context or produce a word from a standard softmax classifier. Our pointer sentinelLSTM model achieves state of the art language modeling performance on the Penn Treebank (70.9 perplexity) while using far fewer parameters than a standard softmax LSTM. In order to evaluate how well language models can exploit longer contexts and deal with more realistic vocabularies and larger corpora we also introduce the freely available WikiText corpus.1", "creator": "LaTeX with hyperref package"}}}