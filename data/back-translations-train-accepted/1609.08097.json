{"id": "1609.08097", "review": {"conference": "EMNLP", "VERSION": "v1", "DATE_OF_SUBMISSION": "26-Sep-2016", "title": "Creating Causal Embeddings for Question Answering with Minimal Supervision", "abstract": "A common model for question answering (QA) is that a good answer is one that is closely related to the question, where relatedness is often determined using general-purpose lexical models such as word embeddings. We argue that a better approach is to look for answers that are related to the question in a relevant way, according to the information need of the question, which may be determined through task-specific embeddings. With causality as a use case, we implement this insight in three steps. First, we generate causal embeddings cost-effectively by bootstrapping cause-effect pairs extracted from free text using a small set of seed patterns. Second, we train dedicated embeddings over this data, by using task-specific contexts, i.e., the context of a cause is its effect. Finally, we extend a state-of-the-art reranking approach for QA to incorporate these causal embeddings. We evaluate the causal embedding models both directly with a casual implication task, and indirectly, in a downstream causal QA task using data from Yahoo! Answers. We show that explicitly modeling causality improves performance in both tasks. In the QA task our best model achieves 37.3% P@1, significantly outperforming a strong baseline by 7.7% (relative).", "histories": [["v1", "Mon, 26 Sep 2016 17:50:15 GMT  (97kb,D)", "http://arxiv.org/abs/1609.08097v1", "To appear in EMNLP 2016"]], "COMMENTS": "To appear in EMNLP 2016", "reviews": [], "SUBJECTS": "cs.CL", "authors": ["rebecca sharp", "mihai surdeanu", "peter jansen", "peter clark", "michael hammond"], "accepted": true, "id": "1609.08097"}, "pdf": {"name": "1609.08097.pdf", "metadata": {"source": "CRF", "title": "Creating Causal Embeddings for Question Answering with Minimal Supervision", "authors": ["Rebecca Sharp", "Mihai Surdeanu", "Peter Jansen", "Peter Clark", "Michael Hammond"], "emails": ["hammond}@email.arizona.edu,", "PeterC@allenai.org", "P@1,"], "sections": [{"heading": "1 Introduction", "text": "In fact, most people who are able to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to"}, {"heading": "2 Related Work", "text": "We have noted the need for specialized approaches in QA, Oh et. al (2013) to embed a specific causal component in their system and note that it improves overall performance, but their model is limited by the need for lexical overlaps between a causal construction found in their knowledge base and the question itself. Here, we are developing a causal component that uses specialized word embeddings to gain the robustness of lexical variations. However, there has been a huge amount of work that shows that word embeddings from distributional similarity are useful in many tasks, including answering questions that are inter alia (Fried et al), 2015; Yih et al al al, 2013). However, Levy and Goldberg point out that there are limitations on the type of semantic knowledge encoded in these general purposes."}, {"heading": "3 Approach", "text": "We focus on recalculating answers to causal questions using task-specific distribution similarity methods, and our approach works in three steps: (1) we start by bootstrapping a large number of cause-and-effect pairs from free text using a small number of syntactic and superficial patterns (Section 4); (2) we then use these bootstrapping pairs to build multiple task-specific embedding models (and other distribution similarity models) (Section 5); we evaluate these models directly against a task to identify causal relationships (Section 6); (3) finally, we integrate these models into a reranking framework for causal QA and show that the resulting approach works better than the reranchor without these task-specific models, even if it is trained on the same data (Section 7)."}, {"heading": "4 Extracting Cause-Effect Tuples", "text": "This year, it has reached the point where it will be able to retaliate until it is able to retaliate."}, {"heading": "5 Models", "text": "We use the extracted causal tuples to train three different distribution-specific similarity models, while we could build several of these multiple words. A causal embedding model (cEmbed) is used for each of these models: The first distribution-specific similarity model we use is based on the skipped word embedding algorithm by Mikolov et al. (2013), which uses a variety of language processing tasks including QA (Yih et al., 2013; Fried et al., 2015), we use the variant implemented by Levy and Goldberg (2014), which modifies the original algorithm to use a variety of language processing tasks rather than using linear, contextual, contexts. Our novel contribution is to make this context taskspecific, the context of a cause being its effect. Furthermore, these contexts are generated from tuples that are minimized themselves, which superimpose the scope of the supervision."}, {"heading": "6 Direct Evaluation: Ranking Word Pairs", "text": "We begin the evaluation of our models with a direct evaluation to determine whether the proposed approaches better capture causality than universally accepted word embeddings, and whether their robustness improves after a simple database search. To perform this evaluation, we follow the protocol of Levy and Goldberg (2014). Specifically, we create a collection of word pairs, half of which are causally related, while the other half consists of other relationships, which are then ranked according to our models and multiple baselines, with the goal of ranking the causal pairs above the others. Embeddings models rank the pairs based on the cosine similarity between the target vector for the causal word and the context vector of the effect word. The alignment model ranks the pairs based on the probability P (Effect | Cause) given by IBM Model 1, and CNN ranks the pairs based on the value of the output received by the network."}, {"heading": "6.1 Data", "text": "To avoid prejudice against our extraction methods, we evaluate our models using an external set of term storage networks (LSTM) (Hochreiter and Schmidhuber, 1997) and note that they consistently undercut this CNN architecture. Our guess is that CNNs work better because LSTMs are more sensitive to the general word order than CNNs that only capture local contexts, and that we have relatively little training data, which prevents the LSTMs from generalizing well. Word pairs originating from SemEval 2010 task 8 (Hendrickx et al., 2009) originally used multiple classification of semantic relationships between nominals. We used a total of 1730 nominal pairs, of which 865 stemmed from the cause-effect relationship (e.g. (dancing, happiness) and an equal number randomly selected from the other eight relationships (e.g. juice, grapefruit) from the relationship between entity and origin."}, {"heading": "6.2 Baselines", "text": "We compared our distribution similarity models with three basic lines: Vanilla Embeddings Model (vEmbed): a standard word2vec model trained with the skip gram algorithm and a slider window of 5, using the original texts from which our causal pairs were extracted.7 As with the cEmbed model, SemEval pairs were ranked based on the cosinal similarity between the vector representations of their reasons.Look-up Baseline: A given SemEval pair was ranked by the number of appearances in our extracted cause-effect tuples. Random: Pairs were randomly shuffled."}, {"heading": "6.3 Results", "text": "Figure 2 shows the precision recall (PR) curve for each of the models and baselines. As expected, the causal models are better able to analyze causal7All embedding models here, including this baseline and our causal variants, producing embedding vectors of 200 dimensions. Couples than the vanilla embedding baseline (vEmbed), which in turn exceeds the random baseline. Our look-up baseline, which ranks pairs by their frequency in our causal database, shows high precision for this task, but has coverage for only 35% of the causal SemEval pairings. Some models perform better at the low recall percentage of the curve (e.g. the look-up baseline and cCNN), while the embedding and alignment models exhibit higher and more consistent performance in the PR curve."}, {"heading": "7 Indirect Evaluation: QA Task", "text": "The main objective of our work is to investigate the effects of a tailor-made causal embedding model for QA. After our direct evaluation, which only evaluates the degree to which our models directly encode causality, we evaluate each of our proposed causal models for their contribution to a downstream real QA task. Our QA system uses a standard re-ranking approach (Jansen et al., 2014). In this architecture, candidates \"responses are first extracted and evaluated using a flat candidate retrieval component (CR component), which uses only information recovery techniques, and then reclassified using a\" learning-to-ranking \"approach. Specifically, we used SVM rank8, a ranking classifier from Support Vector Machines, and evaluated candidates\" responses with a set of characteristics that can be derived from both the initial CR score and from the set of models introduced by us (see the table 2)."}, {"heading": "7.1 Data", "text": "We evaluated using a series of causal questions taken from Yahoo! Answers corpus9, with simple surface patterns such as What causes... and What8 http: / / www.cs.cornell.edu / people / tj / svm _ light / svm _ rank.html9Available freely through Yahoo! \"s Webscope program (research-data-requests @ yahoo-inc.com) is the result of... 10. We extracted a total of 3031 questions, each with at least four candidate answers, and evaluated performance using five-fold cross-validation, using three folds for training, one for development, and one for testing."}, {"heading": "7.2 Models and Features", "text": "We evaluate the contribution of the bidirectional and noise-conscious causal embedding of models (cEmbedBi and cEmbedBiNoise) as well as the causal alignment model (cAlign) and the causal CNN (cCNN). These models are compared against three baselines: the vanilla embedding (vEmbed), the lookup baseline (LU) and additionally a vanilla embedding model (vAlign), which contains over 65k question-answer pairs from Yahoo! Answers. Features11 we use for the various models are: The embedding of model features: For our vanilla and causal embedding models we use the same set of features as Fried et al al al al al al al al al al. (2015): the maximum, minimum and average cosmic similarity between question and answer words, as well as the general similarity between the compositional question and answer of vectors."}, {"heading": "7.3 Results", "text": "The overall results are summarized in Table 2. Lines 1-5 in the table show that each of our demand lines performs better than CR itself, except for vAlign, which suggests that the vanilla alignment model does not generate accurate predictions for causal issues. Note that using this threshold is better than simply using the total number of partnerships. The strongest starting point was CR + vEmbed (line 3), the vanilla embeddings that were trained via Gigaword. Therefore, we consider this to be the basis to beat \"beat\" and perform statistical significance of all proposed models in relation to it."}, {"heading": "7.4 Error Analysis", "text": "For simplicity, we used the combination model CR + vEmbed + cEmbedBi. When examining the learned characteristic weights of the model, we found that the characteristic of the general similarity of vanilla had the highest weight, followed by the causal overall similarity and the causal maximum similarity characteristics. This indicates that even when answering causal questions, the general topical similarity between question and answer is still useful and complementary to the causal similarity characteristics. To determine sources of error, we randomly selected 20 questions that were answered incorrectly and analyzed them according to the categories shown in Table 3. We found that for 70% of the questions, the answer chosen by our system was as good or better than the gold answer, often the case of the community question that answers the data. While the maximum causal similarity characteristic is useful, it may be better than the answer of other (although the nature of the question may be better) due to the embedding of noise, low frequency."}, {"heading": "8 Conclusion", "text": "We have trained three popular models (embedding, alignment, and CNN) on causal tuples extracted from free text with minimal monitoring by bootstrapping cause-and-effect pairs, and assessed their performance both directly (i.e., the extent to which they capture causality) and indirectly (i.e., their use in the real world in answering a high-level question); we have shown that models that include knowledge of causality are best suited to both tasks; our analysis suggests that the models that perform best in the real QA task perform consistently on the precision recall curve in direct evaluation; in QA, where the vocabulary is much larger, precision with high memory must be balanced, and this can best be achieved through our causal embedding model; and we have shown that vanilla and causal embedding models address different information needs."}, {"heading": "Acknowledgments", "text": "We thank the Allen Institute for Artificial Intelligence for funding this work. In addition, this work was partly funded by the Defense Advanced Research Projects Agency (DARPA) under the ARO contract W911NF-14-1-0395."}], "references": [{"title": "Bridging the lexical chasm: Statistical approaches to answer finding", "author": ["Berger et al.2000] A. Berger", "R. Caruana", "D. Cohn", "D. Freytag", "V. Mittal"], "venue": "In Proc. of the 23rd Annual International ACM SIGIR Conference on Research & Development on Informa-", "citeRegEx": "Berger et al\\.,? \\Q2000\\E", "shortCiteRegEx": "Berger et al\\.", "year": 2000}, {"title": "Question answering with subgraph embeddings", "author": ["Bordes et al.2014] A. Bordes", "S. Chopra", "J. Weston"], "venue": "arXiv preprint arXiv:1406.3676", "citeRegEx": "Bordes et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Bordes et al\\.", "year": 2014}, {"title": "The mathematics of statistical machine translation: Parameter estimation", "author": ["Brown et al.1993] P.F. Brown", "S.A. Della Pietra", "V.J. Della Pietra", "R.L. Mercer"], "venue": null, "citeRegEx": "Brown et al\\.,? \\Q1993\\E", "shortCiteRegEx": "Brown et al\\.", "year": 1993}, {"title": "A fast and accurate dependency parser using neural networks", "author": ["Chen", "Manning2014] D. Chen", "C.D. Manning"], "venue": "In Proc. of the Conferenc on Empirical Methods for Natural Language Processing (EMNLP)", "citeRegEx": "Chen et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Chen et al\\.", "year": 2014}, {"title": "Keras. https:// github.com/fchollet/keras", "author": ["F. Chollet"], "venue": null, "citeRegEx": "Chollet.,? \\Q2015\\E", "shortCiteRegEx": "Chollet.", "year": 2015}, {"title": "A study of the knowledge base requirements for passing an elementary science test", "author": ["Clark et al.2013] P. Clark", "P. Harrison", "N. Balasubramanian"], "venue": "In Proc. of the 2013 workshop on Automated Knowledge Base Construction (AKBC),", "citeRegEx": "Clark et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Clark et al\\.", "year": 2013}, {"title": "A lightweight tool for automatically extracting causal relationships from text", "author": ["Cole et al.2005] S.V. Cole", "M.D. Royal", "M.G. Valtorta", "M.N. Huhns", "J.B. Bowles"], "venue": "In SoutheastCon,", "citeRegEx": "Cole et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Cole et al\\.", "year": 2005}, {"title": "Minimally supervised event causality identification", "author": ["Do et al.2011] Q.X. Do", "Y.S. Chan", "D. Roth"], "venue": "In Proc. of the Conference on Empirical Methods in Natural Language Processing (EMNLP),", "citeRegEx": "Do et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Do et al\\.", "year": 2011}, {"title": "A noisy-channel approach to question answering", "author": ["Echihabi", "Marcu2003] A. Echihabi", "D. Marcu"], "venue": "In Proc. of the 41st Annual Meeting of the Association for Computational Linguistics (ACL),", "citeRegEx": "Echihabi et al\\.,? \\Q2003\\E", "shortCiteRegEx": "Echihabi et al\\.", "year": 2003}, {"title": "Search needs a shake-up", "author": ["O. Etzioni"], "venue": "Nature,", "citeRegEx": "Etzioni.,? \\Q2011\\E", "shortCiteRegEx": "Etzioni.", "year": 2011}, {"title": "Problems with evaluation of word embeddings using word similarity tasks. arXiv preprint arXiv:1605.02276", "author": ["Faruqui et al.2016] M. Faruqui", "Y. Tsvetkov", "R. Rastogi", "C. Dyer"], "venue": null, "citeRegEx": "Faruqui et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Faruqui et al\\.", "year": 2016}, {"title": "Building Watson: An overview of the DeepQA project", "author": ["J.W. Murdock", "E. Nyberg", "J. Prager"], "venue": "AI magazine,", "citeRegEx": "Murdock et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Murdock et al\\.", "year": 2010}, {"title": "Semantic role labeling with neural network factors", "author": ["O. T\u00e4ckstr\u00f6m", "K. Ganchev", "D. Das"], "venue": "In Proc. of the 2015 Conference on Empirical Methods in Natural Language Processing (EMNLP),", "citeRegEx": "FitzGerald et al\\.,? \\Q2015\\E", "shortCiteRegEx": "FitzGerald et al\\.", "year": 2015}, {"title": "Higher-order lexical semantic models for non-factoid answer reranking", "author": ["Fried et al.2015] D. Fried", "P. Jansen", "G. Hahn-Powell", "M. Surdeanu", "P. Clark"], "venue": "Transactions of the Association for Computational Linguistics,", "citeRegEx": "Fried et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Fried et al\\.", "year": 2015}, {"title": "Text mining for causal relations", "author": ["Girju", "Moldovan2002] R. Girju", "D.I. Moldovan"], "venue": "In FLAIRS Conference,", "citeRegEx": "Girju et al\\.,? \\Q2002\\E", "shortCiteRegEx": "Girju et al\\.", "year": 2002}, {"title": "Automatic acquisition of hyponyms from large text corpora", "author": ["M.A. Hearst"], "venue": "In Proc. of the 14th conference on Computational linguistics (COLING),", "citeRegEx": "Hearst.,? \\Q1992\\E", "shortCiteRegEx": "Hearst.", "year": 1992}, {"title": "Semeval-2010 task 8: Multi-way classification of semantic relations between pairs of nominals", "author": ["S.N. Kim", "Z. Kozareva", "P. Nakov", "D. \u00d3 S\u00e9aghdha", "S. Pad\u00f3", "M. Pennacchiotti", "L. Romano", "S. Szpakowicz"], "venue": null, "citeRegEx": "Hendrickx et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Hendrickx et al\\.", "year": 2009}, {"title": "Long short-term memory", "author": ["Hochreiter", "Schmidhuber1997] S. Hochreiter", "J. Schmidhuber"], "venue": "Neural computation,", "citeRegEx": "Hochreiter et al\\.,? \\Q1997\\E", "shortCiteRegEx": "Hochreiter et al\\.", "year": 1997}, {"title": "Deep unordered composition rivals syntactic methods for text classification", "author": ["Iyyer et al.2015] Mohit Iyyer", "Varun Manjunatha", "Jordan Boyd-Graber", "Hal Daum\u00e9 III"], "venue": "In Proceedings of the Association for Computational Linguistics", "citeRegEx": "Iyyer et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Iyyer et al\\.", "year": 2015}, {"title": "Discourse complements lexical semantics for non-factoid answer reranking", "author": ["Jansen et al.2014] P. Jansen", "M. Surdeanu", "P. Clark"], "venue": "In Proc. of the 52nd Annual Meeting of the Association for Computational Linguistics (ACL)", "citeRegEx": "Jansen et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Jansen et al\\.", "year": 2014}, {"title": "Automatic extraction of cause-effect information from newspaper text without knowledge-based inferencing", "author": ["Khoo et al.1998] C.S.G. Khoo", "J. Kornfilt", "R.N. Oddy", "S.H. Myaeng"], "venue": "Literary and Linguistic Computing,", "citeRegEx": "Khoo et al\\.,? \\Q1998\\E", "shortCiteRegEx": "Khoo et al\\.", "year": 1998}, {"title": "Specializing word embeddings for similarity or relatedness", "author": ["Kiela et al.2015] D. Kiela", "F. Hill", "S. Clark"], "venue": "In Proc. of the 2015 Conference on Empirical Methods in Natural Language Processing (EMNLP)", "citeRegEx": "Kiela et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Kiela et al\\.", "year": 2015}, {"title": "Adam: A method for stochastic optimization. arXiv preprint arXiv:1412.6980", "author": ["Kingma", "Ba2014] D. Kingma", "J. Ba"], "venue": null, "citeRegEx": "Kingma et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Kingma et al\\.", "year": 2014}, {"title": "Dependency-based word embeddings", "author": ["Levy", "Goldberg2014] O. Levy", "Y. Goldberg"], "venue": "In Proc. of the 52nd Annual Meeting of the Association for Computational Linguistics (ACL),", "citeRegEx": "Levy et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Levy et al\\.", "year": 2014}, {"title": "Do supervised distributional methods really learn lexical inference relations", "author": ["Levy et al.2015] O. Levy", "S. Remus", "C. Biemann", "I. Dagan", "I. Ramat-Gan"], "venue": "In Proc. of the Conference of the North American Chapter of the Association for Computational Lin-", "citeRegEx": "Levy et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Levy et al\\.", "year": 2015}, {"title": "The Stanford CoreNLP natural language processing toolkit", "author": ["M. Surdeanu", "J. Bauer", "J. Finkel", "S.J. Bethard", "D. McClosky"], "venue": "In Proc. of the 52nd Annual Meeting of the Association for Computational Linguistics (ACL)", "citeRegEx": "Manning et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Manning et al\\.", "year": 2014}, {"title": "Distributed representations of words and phrases and their compositionality", "author": ["Mikolov et al.2013] T. Mikolov", "I. Sutskever", "K. Chen", "G.S. Corrado", "J. Dean"], "venue": "In Advances in neural information processing systems,", "citeRegEx": "Mikolov et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Mikolov et al\\.", "year": 2013}, {"title": "Annotated gigaword", "author": ["Napoles et al.2012] C. Napoles", "M. Gormley", "B. Van Durme"], "venue": "In Proc. of the Joint Workshop on Automatic Knowledge Base Construction and Web-scale Knowledge Extraction,", "citeRegEx": "Napoles et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Napoles et al\\.", "year": 2012}, {"title": "A systematic comparison of various statistical alignment models", "author": ["Och", "Ney2003] F.J. Och", "H. Ney"], "venue": "Computational Linguistics,", "citeRegEx": "Och et al\\.,? \\Q2003\\E", "shortCiteRegEx": "Och et al\\.", "year": 2003}, {"title": "Whyquestion answering using intra-and inter-sentential causal relations", "author": ["Oh et al.2013] J.-H. Oh", "K. Torisawa", "C. Hashimoto", "M. Sano", "S. De Saeger", "K. Ohtake"], "venue": "In The 51st Annual Meeting of the Association", "citeRegEx": "Oh et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Oh et al\\.", "year": 2013}, {"title": "Relation extraction with matrix factorization and universal schemas", "author": ["Riedel et al.2013] S. Riedel", "L. Yao", "A. McCallum", "B.M. Marlin"], "venue": "In Proc. of Annual Meeting of the North American Chapter of the Association for Computational Linguistics (NAACL)", "citeRegEx": "Riedel et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Riedel et al\\.", "year": 2013}, {"title": "Statistical machine translation for query expansion in answer retrieval", "author": ["Riezler et al.2007] S. Riezler", "A. Vasserman", "I. Tsochantaridis", "V. Mittal", "Y. Liu"], "venue": "In Proc. of the 45th Annual Meeting of the Association", "citeRegEx": "Riezler et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Riezler et al\\.", "year": 2007}, {"title": "Automatically generating extraction patterns from untagged text", "author": ["E. Riloff"], "venue": "In Proc. of the National Conference on Artificial Intelligence (AAAI),", "citeRegEx": "Riloff.,? \\Q1996\\E", "shortCiteRegEx": "Riloff.", "year": 1996}, {"title": "Spinning straw into gold", "author": ["Sharp et al.2015] R. Sharp", "P. Jansen", "M. Surdeanu", "P. Clark"], "venue": "In Proc. of the Conference of the North American Chapter of the Association for Computational Linguistics - Human Language Technologies", "citeRegEx": "Sharp et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Sharp et al\\.", "year": 2015}, {"title": "Automatic question answering using the web: Beyond the factoid", "author": ["Soricut", "Brill2006] R. Soricut", "E. Brill"], "venue": "Journal of Information Retrieval - Special Issue on Web Information Retrieval,", "citeRegEx": "Soricut et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Soricut et al\\.", "year": 2006}, {"title": "Learning to rank answers to non-factoid questions from web collections", "author": ["Surdeanu et al.2011] M. Surdeanu", "M. Ciaramita", "H. Zaragoza"], "venue": null, "citeRegEx": "Surdeanu et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Surdeanu et al\\.", "year": 2011}, {"title": "Odin\u2019s runes: A rule language for information extraction", "author": ["G. Hahn-Powell", "M. Surdeanu"], "venue": "In Proc. of the 10th International Conference on Language Resources and Evaluation (LREC)", "citeRegEx": "ValenzuelaEsc\u00e1rcega et al\\.,? \\Q2016\\E", "shortCiteRegEx": "ValenzuelaEsc\u00e1rcega et al\\.", "year": 2016}, {"title": "Distributed representations for unsupervised semantic role labeling", "author": ["Woodsend", "Lapata2015] K. Woodsend", "M. Lapata"], "venue": "In Proc. of the 2015 Conference on Empirical Methods in Natural Language Processing (EMNLP)", "citeRegEx": "Woodsend et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Woodsend et al\\.", "year": 2015}, {"title": "Multi level causal relation identification using extended features", "author": ["Yang", "Mao2014] X. Yang", "K. Mao"], "venue": "Expert Systems with Applications,", "citeRegEx": "Yang et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Yang et al\\.", "year": 2014}, {"title": "Joint relational embeddings for knowledge-based question answering", "author": ["Yang et al.2014] M.-C. Yang", "N. Duan", "M. Zhou", "H.-C. Rim"], "venue": "In Proc. of the Conference on Empirical Methods in Natural Language Processing (EMNLP),", "citeRegEx": "Yang et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Yang et al\\.", "year": 2014}, {"title": "Semi-markov phrasebased monolingual alignment", "author": ["Yao et al.2013] X. Yao", "B. Van Durme", "C. CallisonBurch", "P. Clark"], "venue": "In Proc. of the Conference on Empirical Methods in Natural Language Processing (EMNLP)", "citeRegEx": "Yao et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Yao et al\\.", "year": 2013}, {"title": "Question answering using enhanced lexical semantic models", "author": ["Yih et al.2013] W. Yih", "M. Chang", "C. Meek", "A. Pastusiak"], "venue": "In Proc. of the 51st Annual Meeting of the Association for Computational Linguistics (ACL)", "citeRegEx": "Yih et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Yih et al\\.", "year": 2013}], "referenceMentions": [{"referenceID": 9, "context": ", finding short answers to natural language questions, is one of the most important but challenging tasks on the road towards natural language understanding (Etzioni, 2011).", "startOffset": 157, "endOffset": 172}, {"referenceID": 41, "context": "such as word embeddings (Yih et al., 2013; Jansen et al., 2014; Fried et al., 2015).", "startOffset": 24, "endOffset": 83}, {"referenceID": 19, "context": "such as word embeddings (Yih et al., 2013; Jansen et al., 2014; Fried et al., 2015).", "startOffset": 24, "endOffset": 83}, {"referenceID": 13, "context": "such as word embeddings (Yih et al., 2013; Jansen et al., 2014; Fried et al., 2015).", "startOffset": 24, "endOffset": 83}, {"referenceID": 5, "context": "in any given question set, and that are best addressed individually (Chu-Carroll et al., 2004; Ferrucci et al., 2010; Clark et al., 2013).", "startOffset": 68, "endOffset": 137}, {"referenceID": 26, "context": "Levy and Goldberg (2014) have modified the algorithm of Mikolov et al. (2013) to use an arbitrary, rather than linear, context.", "startOffset": 56, "endOffset": 78}, {"referenceID": 10, "context": "uations (Faruqui et al., 2016): we show that they have limited coverage and may align poorly with real-world tasks.", "startOffset": 8, "endOffset": 30}, {"referenceID": 13, "context": "There has been a vast body of work which demonstrates that word embeddings derived from distributional similarity are useful in many tasks, including question answering \u2013 see inter alia (Fried et al., 2015; Yih et al., 2013).", "startOffset": 186, "endOffset": 224}, {"referenceID": 41, "context": "There has been a vast body of work which demonstrates that word embeddings derived from distributional similarity are useful in many tasks, including question answering \u2013 see inter alia (Fried et al., 2015; Yih et al., 2013).", "startOffset": 186, "endOffset": 224}, {"referenceID": 13, "context": "There has been a vast body of work which demonstrates that word embeddings derived from distributional similarity are useful in many tasks, including question answering \u2013 see inter alia (Fried et al., 2015; Yih et al., 2013). However, Levy and Goldberg (2015) note that there are limitations on the type of semantic knowledge which is encoded in these general-purpose similarity embeddings.", "startOffset": 187, "endOffset": 260}, {"referenceID": 12, "context": "Customized embeddings have been created for a variety of tasks, including semantic role labeling (FitzGerald et al., 2015; Woodsend and Lapata, 2015), and binary relation extraction (Riedel et al.", "startOffset": 97, "endOffset": 149}, {"referenceID": 30, "context": ", 2015; Woodsend and Lapata, 2015), and binary relation extraction (Riedel et al., 2013).", "startOffset": 67, "endOffset": 88}, {"referenceID": 1, "context": "In QA, embeddings have been customized to have question words that are close to either their answer words (Bordes et al., 2014), or to structured knowl-", "startOffset": 106, "endOffset": 127}, {"referenceID": 38, "context": "edge base entries (Yang et al., 2014).", "startOffset": 18, "endOffset": 37}, {"referenceID": 21, "context": "Additionally, embeddings have been customized to distinguish functional similarity from relatedness (Levy and Goldberg, 2014; Kiela et al., 2015).", "startOffset": 100, "endOffset": 145}, {"referenceID": 10, "context": "Recently, Faruqui et al.(2016) discussed issues surrounding the evaluation of similarity word embeddings, including the lack of correlation between their performance on word-similarity tasks and \u201cdownstream\u201d or real-world tasks like QA, text classification, etc.", "startOffset": 10, "endOffset": 31}, {"referenceID": 15, "context": "(2002) use modified Hearst patterns (Hearst, 1992) to extract a large number of potential cause-effect tuples, where both causes and effects must be nouns.", "startOffset": 36, "endOffset": 50}, {"referenceID": 12, "context": "With respect to extracting causal relations from text, Girju et al. (2002) use modified Hearst patterns (Hearst, 1992) to extract a large number of potential cause-effect tuples, where both causes and effects must be nouns.", "startOffset": 55, "endOffset": 75}, {"referenceID": 6, "context": "However, Cole et al. (2005) show that these nominal-based causal relations account for a relatively small percentage of all causal relations, and for this reason, (Yang and Mao, 2014) allow for more elaborate argument structures in their causal extraction by identifying verbs, and then following the syntactic subtree of the verbal arguments to construct their candidate causes and effects.", "startOffset": 9, "endOffset": 28}, {"referenceID": 6, "context": "However, Cole et al. (2005) show that these nominal-based causal relations account for a relatively small percentage of all causal relations, and for this reason, (Yang and Mao, 2014) allow for more elaborate argument structures in their causal extraction by identifying verbs, and then following the syntactic subtree of the verbal arguments to construct their candidate causes and effects. Additionally, Do et al. (2011) observe that nouns as well as verbs can signal causality.", "startOffset": 9, "endOffset": 423}, {"referenceID": 33, "context": "Because the success of embedding models depends on large training datasets (Sharp et al., 2015), and such datasets do not exist for open-domain causality, we opted to bootstrap a large number of cause-effect pairs from a small set of patterns.", "startOffset": 75, "endOffset": 95}, {"referenceID": 27, "context": "(1) Pre-processing: Much of the text we use to extract causal relation tuples comes from the Annotated Gigaword (Napoles et al., 2012).", "startOffset": 112, "endOffset": 134}, {"referenceID": 25, "context": "We additionally use text from the Simple English Wikipedia1, which we processed using the Stanford CoreNLP toolkit (Manning et al., 2014) and the dependency parser of Chen and Manning (2014).", "startOffset": 115, "endOffset": 137}, {"referenceID": 25, "context": "We additionally use text from the Simple English Wikipedia1, which we processed using the Stanford CoreNLP toolkit (Manning et al., 2014) and the dependency parser of Chen and Manning (2014).", "startOffset": 116, "endOffset": 191}, {"referenceID": 20, "context": "Different patterns have varying probabilities of signaling causation (Khoo et al., 1998).", "startOffset": 69, "endOffset": 88}, {"referenceID": 41, "context": "(2013), which has been shown to improve a variety of language processing tasks including QA (Yih et al., 2013; Fried et al., 2015).", "startOffset": 92, "endOffset": 130}, {"referenceID": 13, "context": "(2013), which has been shown to improve a variety of language processing tasks including QA (Yih et al., 2013; Fried et al., 2015).", "startOffset": 92, "endOffset": 130}, {"referenceID": 25, "context": "Causal Embedding Model (cEmbed): The first distributional similarity model we use is based on the skip-gram word-embedding algorithm of Mikolov et al. (2013), which has been shown to improve a variety of language processing tasks including QA (Yih et al.", "startOffset": 136, "endOffset": 158}, {"referenceID": 13, "context": ", 2013; Fried et al., 2015). In particular, we use the variant implemented by Levy and Goldberg (2014) which modifies the original algorithm to use an arbitrary, rather than linear, context.", "startOffset": 8, "endOffset": 103}, {"referenceID": 0, "context": "Causal Alignment Model (cAlign): Monolingual alignment (or translation) models have been shown to be successful in QA (Berger et al., 2000; Echihabi and Marcu, 2003; Soricut and Brill, 2006; Riezler et al., 2007; Surdeanu et al., 2011; Yao et al., 2013), and recent work has shown that they can be successfully trained with less data than embedding models (Sharp et al.", "startOffset": 118, "endOffset": 253}, {"referenceID": 31, "context": "Causal Alignment Model (cAlign): Monolingual alignment (or translation) models have been shown to be successful in QA (Berger et al., 2000; Echihabi and Marcu, 2003; Soricut and Brill, 2006; Riezler et al., 2007; Surdeanu et al., 2011; Yao et al., 2013), and recent work has shown that they can be successfully trained with less data than embedding models (Sharp et al.", "startOffset": 118, "endOffset": 253}, {"referenceID": 35, "context": "Causal Alignment Model (cAlign): Monolingual alignment (or translation) models have been shown to be successful in QA (Berger et al., 2000; Echihabi and Marcu, 2003; Soricut and Brill, 2006; Riezler et al., 2007; Surdeanu et al., 2011; Yao et al., 2013), and recent work has shown that they can be successfully trained with less data than embedding models (Sharp et al.", "startOffset": 118, "endOffset": 253}, {"referenceID": 40, "context": "Causal Alignment Model (cAlign): Monolingual alignment (or translation) models have been shown to be successful in QA (Berger et al., 2000; Echihabi and Marcu, 2003; Soricut and Brill, 2006; Riezler et al., 2007; Surdeanu et al., 2011; Yao et al., 2013), and recent work has shown that they can be successfully trained with less data than embedding models (Sharp et al.", "startOffset": 118, "endOffset": 253}, {"referenceID": 33, "context": ", 2013), and recent work has shown that they can be successfully trained with less data than embedding models (Sharp et al., 2015).", "startOffset": 110, "endOffset": 130}, {"referenceID": 4, "context": "We train the network using a binary cross entropy objective function and the Adam optimizer (Kingma and Ba, 2014), using the Keras library (Chollet, 2015) operating over Theano (Theano Development Team, 2016), a popular deep-learning framework.", "startOffset": 139, "endOffset": 154}, {"referenceID": 17, "context": "In designing our CNN, we attempted to minimize architectural and hyperparameter tuning by taking inspiration from Iyyer et al. (2015), preferring simpler architectures.", "startOffset": 114, "endOffset": 134}, {"referenceID": 32, "context": "For this, we first score the tuples by their causal PMI and then scale these scores by the overall frequency of the tuple (Riloff, 1996), to account for the PMI bias toward low-frequency items.", "startOffset": 122, "endOffset": 136}, {"referenceID": 16, "context": "word pairs drawn from the SemEval 2010 Task 8 (Hendrickx et al., 2009), originally a multi-way clas-", "startOffset": 46, "endOffset": 70}, {"referenceID": 19, "context": "Our QA system uses a standard reranking approach (Jansen et al., 2014).", "startOffset": 49, "endOffset": 70}, {"referenceID": 13, "context": "Embedding model features: For both our vanilla and causal embedding models, we use the same set of features as Fried et al. (2015): the maximum,", "startOffset": 111, "endOffset": 131}, {"referenceID": 35, "context": "Alignment model features: We use the same global alignment probability, p(Q|A) of Surdeanu et al. (2011). In our causal alignment model, we adapt this to causality as p(Effect|Cause), and again we first determine the direction of the causal relation implied in the question.", "startOffset": 82, "endOffset": 105}, {"referenceID": 13, "context": "Jensen-Shannon distance, proposed more recently by Fried et al. (2015), in our vanilla alignment", "startOffset": 51, "endOffset": 71}], "year": 2016, "abstractText": "A common model for question answering (QA) is that a good answer is one that is closely related to the question, where relatedness is often determined using generalpurpose lexical models such as word embeddings. We argue that a better approach is to look for answers that are related to the question in a relevant way, according to the information need of the question, which may be determined through task-specific embeddings. With causality as a use case, we implement this insight in three steps. First, we generate causal embeddings cost-effectively by bootstrapping cause-effect pairs extracted from free text using a small set of seed patterns. Second, we train dedicated embeddings over this data, by using task-specific contexts, i.e., the context of a cause is its effect. Finally, we extend a state-of-the-art reranking approach for QA to incorporate these causal embeddings. We evaluate the causal embedding models both directly with a casual implication task, and indirectly, in a downstream causal QA task using data from Yahoo! Answers. We show that explicitly modeling causality improves performance in both tasks. In the QA task our best model achieves 37.3% P@1, significantly outperforming a strong baseline by 7.7% (relative).", "creator": "LaTeX with hyperref package"}}}