{"id": "1609.08194", "review": {"conference": "EMNLP", "VERSION": "v1", "DATE_OF_SUBMISSION": "26-Sep-2016", "title": "Online Segment to Segment Neural Transduction", "abstract": "We introduce an online neural sequence to sequence model that learns to alternate between encoding and decoding segments of the input as it is read. By independently tracking the encoding and decoding representations our algorithm permits exact polynomial marginalization of the latent segmentation during training, and during decoding beam search is employed to find the best alignment path together with the predicted output sequence. Our model tackles the bottleneck of vanilla encoder-decoders that have to read and memorize the entire input sequence in their fixed-length hidden states before producing any output. It is different from previous attentive models in that, instead of treating the attention weights as output of a deterministic function, our model assigns attention weights to a sequential latent variable which can be marginalized out and permits online generation. Experiments on abstractive sentence summarization and morphological inflection show significant performance gains over the baseline encoder-decoders.", "histories": [["v1", "Mon, 26 Sep 2016 21:13:49 GMT  (30kb)", "http://arxiv.org/abs/1609.08194v1", "EMNLP 2016"]], "COMMENTS": "EMNLP 2016", "reviews": [], "SUBJECTS": "cs.CL cs.AI cs.NE", "authors": ["lei yu", "jan buys", "phil blunsom"], "accepted": true, "id": "1609.08194"}, "pdf": {"name": "1609.08194.pdf", "metadata": {"source": "CRF", "title": "Online Segment to Segment Neural Transduction", "authors": ["Lei Yu", "Jan Buys"], "emails": ["phil.blunsom}@cs.ox.ac.uk"], "sections": [{"heading": null, "text": "ar Xiv: 160 9.08 194v 1 [cs.C L] 26 SeWe introduce an online neural sequence into the sequence model that learns to switch between encoding and decoding segments of input during reading. By independently tracking the encoding and decoding of representations, our algorithm enables precise polynomial marginalization of latent segmentation during training and during decoding the bar search to find the best alignment path along with the predicted output sequence. Our model addresses the bottleneck of vanilla encoder decoders that need to read and memorize the entire input sequence in its hidden states before they are output. It differs from previous attentive models in that our model does not treat attention weights as an output of a deterministic function, but assigns them to a sequential variable that can be marginalized and can be generated online."}, {"heading": "1 Introduction", "text": "The fact is that most of us are able to put ourselves in a different world, in which they are able to move, in which they move, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they, in which they live, in which they live, in which they, in which they live, in which they live, in which they live, in which they live."}, {"heading": "2 Model", "text": "Let xI1 be the input sequence of length I and y J 1 be the output sequence of length J. Let yj denote the jth character of y. Our goal is to model the conditional distribution p (y | x) = J-J = 1p (yj | y-1, x). (1) We introduce a hidden alignment sequence aJ1 in which each aj = i corresponds to an input position i-x.... I}, which we want to focus on when generating yj. Then p (y | x) is calculated by marginalizing all hidden alignments, p (y | x) = a sequence of p (y, a-x) (2)."}, {"heading": "2.1 Probabilities of Output Word Predictions", "text": "The input set x is encoded with a Recurrent Neural Network (RNN), in particular with an LSTM (Hochreiter and Schmidhuber, 1997).The encoder can be either a unidirectional or bidirectional LSTM. If a unidirectional encoder is used, the model is able to read input and generate output symbols online. The hidden state vectors are calculated as ashes \u2192 i = RNN (h \u2192 i \u2212 1, v (e) (xi))))), (3) h \u2190 i = RNN (h \u2190 i + 1, v (e) (xi), (4) where v (e) (xi) denotes the vector representation of the token x, and h \u2192 i and h \u2190 i are the forward or backward hidden states. For a bidirectional encoder, they are linked as Hi = [h \u2192 i; h \u00b7 i \u00b2 i], while the output value is the encox."}, {"heading": "2.2 Transition Probabilities", "text": "Since the alignments must necessarily be monotonous, we can treat the transition from timestep j to j + 1 as a sequence of shift and emission operations. Specifically, at each input position a decision is made on displacement or emission by the model; when the operation is emitted, the next output word is generated; otherwise, the model shifts to the next input word. While multinomial distribution is an alternative for parameterizing alignments, the shift / emit parameterization does not set a limit for the jump value, since a multinomial distribution would tend the model toward shorter jump variables that a multinomial model would have to learn. We describe two methods for modelling the alignment transition probability. The first approach is independent of the input or output words. To parameterize the alignment probability in relation to the shift and emission probability, we will use a geometric distribution that is independent of \u2212 aj (or \u2212 first)."}, {"heading": "3 Training and Decoding", "text": "Since there is an exponential number of possible alignments, it is mathematically difficult to explicitly calculate and then add each p (y, a | x) to obtain the conditional probability p (y | x). Instead, we approach the problem with a dynamic programming algorithm similar to the forward-backward algorithm for HMMs (Rabiner, 1989)."}, {"heading": "3.1 Training", "text": "For an input x and output y, the forward variable \u03b1 (i, j) = p (i, j) = p (i, j) = j (forward, j 1 | x). The value of \u03b1 (i, j) is calculated by adding up the probabilities of each path that could lead to this cell. (11) For j [2, J], i [1, I]: \u03b1 (i, j) = p (yj, sj) \u00b7 (12) i (k, j \u2212 1) p (aj = i, s1).The backward variables, defined as \u03b2 (i, j) = 1, I]: \u03b1 (i, j) = p (yj | hi, sj) \u00b7 p (12) i \u00b2 s (k, j \u2212 1) p (aj \u2212 1).The backward variables, defined as \u03b2 (i, j) = j (j), j (j) = education (j) = p (yy), j), j = y (j)."}, {"heading": "3.2 Decoding", "text": "Algorithm 1 DP search algorithm input: source set x output: best output set y * initialization: Q-RI \u00b7 Jmax, bp-NI \u00b7 Jmax, W-NI \u00b7 Jmax, Iend, Jend. for i [1, I] doQ [i, 1] p (i, s1) p (a1 = i) p (y, s1) p (i, s1) p (2, Jmax] dofor i [i, 1] p (i, s1) p (y, s1) p (y, s1) end for j [2, Jmax] dofor i [1, I] do Q [i, j] p [i, j] p [i, k] p [k] p (aj = i \u2212 1) p (y] p (i, sj] p [i, j] p [i] p [i] p [i, j \"j\" j \"j\" j \"j\" j [j \"j] j [j\" j \"p] j, j\" j \"j\" p [i] j [i] p [i] j [j \"p] j."}, {"heading": "4 Experiments", "text": "We evaluate the effectiveness of our model against two representative tasks for processing natural language, sentence compression and morphological diffraction. The primary objective of this evaluation is to assess whether our proposed architecture is capable of surpassing the basic encoder decoder model by overcoming its bottleneck in encoding. We also compare our results with an attention model to see if our alternative alignment strategy is capable of offering similar benefits in processing input online."}, {"heading": "4.1 Abstractive Sentence Summarisation", "text": "This year, it is closer than ever before to being able to take the lead."}, {"heading": "4.2 Morphological Inflection", "text": "The morphological diffraction generation is the task of predicting the inflected form of a certain lexical element based on a morphological attribute. Transformation from a base form to a inflected form usually involves concatenation with a prefix or a suffix and substitution of some characters. For example, the inflected form of a German stem is abbreviated when the case is dative and the number is plural. In our experiments we use the same dataset as Faruqui et al. (2016) This dataset was originally created by Durrett and DeNero (2013) from Wiktionary, which contains diffraction models for German nouns (de-N), German verbs (de-V), Spanish verbs (es-V), Finnish noun and adjectives (fi-NA), and Finnish verbs (fi-V). It was further expanded by Nicolai et al. (2015) by adding Dutch verbs (V) and French verbs (V)."}, {"heading": "5 Alignment Quality", "text": "Figure 3 shows visualizations of the segment alignments generated by our model for example examples from both tasks. We see that the model is able to learn the correct matches between the segments of the input and output sequences. For example, the alignment follows an almost diagonal path for the example in Figure 3c, where the input and output sequences are identical. In Figure 3b, it learns to add the prefix \"ge\" at the beginning of the sequence and replace \"en\" with \"t\" after copying \"zock.\" We observe that the model is robust on long phrase images. As shown in Figure 3a, the matching between \"Wall Street Journal Asia,\" the Asian edition of the US business newspaper, and \"Wall Street Journal Asia,\" shows that our model learns to ignore phrase modifiers that contain additional information."}, {"heading": "6 Related Work", "text": "This year, we have reached the point where we feel we are in a position to take the lead without being able to try to find a solution."}, {"heading": "7 Conclusion", "text": "We have proposed a novel segment to segment neural transduction models that address the limitations of vanilla encoder decoders, which must read and store an entire input sequence in a fixed-length context vector before they produce output. By introducing latent segmentation, which determines input and output sequence matches, our model learns to jointly generate and align. During the training, hidden alignment is marginalized by dynamic programming, and during decoding, the best alignment path is generated in addition to the predicted output sequence. By using a unidirectional LSTM as an encoder, our model is able to perform online generation. Experiments on two representative tasks of processing natural language, abstract sentence summary, and morphological flexibility generation have shown that our model significantly exceeds encode base lines, while we need much smaller work in the future."}, {"heading": "Acknowledgments", "text": "We thank Chris Dyer, Karl Moritz Hermann, Edward Grefenstette, Toma's, Ko'cisky, Gabor Melis, Yishu Miao and many others for their helpful comments."}], "references": [{"title": "Kyunghyun Cho", "author": ["Dzmitry Bahdanau"], "venue": "and Yoshua Bengio.", "citeRegEx": "Bahdanau et al.2015", "shortCiteRegEx": null, "year": 2015}, {"title": "Holger Schwenk", "author": ["Kyunghyun Cho", "Bart van Merrienboer", "\u00c7aglar G\u00fcl\u00e7ehre", "Dzmitry Bahdanau", "Fethi Bougares"], "venue": "and Yoshua Bengio.", "citeRegEx": "Cho et al.2014", "shortCiteRegEx": null, "year": 2014}, {"title": "and Alexander M", "author": ["Sumit Chopra", "Michael Auli"], "venue": "Rush.", "citeRegEx": "Chopra et al.2016", "shortCiteRegEx": null, "year": 2016}, {"title": "Supervised learning of complete morphological paradigms", "author": ["Durrett", "DeNero2013] Greg Durrett", "John DeNero"], "venue": "In Proceedings of HLT-NAACL", "citeRegEx": "Durrett et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Durrett et al\\.", "year": 2013}, {"title": "Graham Neubig", "author": ["Manaal Faruqui", "Yulia Tsvetkov"], "venue": "and Chris Dyer.", "citeRegEx": "Faruqui et al.2016", "shortCiteRegEx": null, "year": 2016}, {"title": "Ke Chen", "author": ["David Graff", "Junbo Kong"], "venue": "and Kazuaki Maeda.", "citeRegEx": "Graff et al.2003", "shortCiteRegEx": null, "year": 2003}, {"title": "Faustino Gomez", "author": ["Alex Graves", "Santiago Fern\u00e1ndez"], "venue": "and J\u00fcrgen Schmidhuber.", "citeRegEx": "Graves et al.2006", "shortCiteRegEx": null, "year": 2006}, {"title": "Greg Wayne", "author": ["Alex Graves"], "venue": "and Ivo Danihelka.", "citeRegEx": "Graves et al.2014", "shortCiteRegEx": null, "year": 2014}, {"title": "Sequence transduction with recurrent neural networks. arXiv preprint arXiv:1211.3711", "author": ["Alex Graves"], "venue": null, "citeRegEx": "Graves.,? \\Q2012\\E", "shortCiteRegEx": "Graves.", "year": 2012}, {"title": "Learning to transduce with unbounded memory", "author": ["Grefenstette", "Karl Moritz Hermann", "Mustafa Suleyman", "Phil Blunsom"], "venue": "In Proceedings of NIPS,", "citeRegEx": "Grefenstette et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Grefenstette et al\\.", "year": 2015}, {"title": "Bowen Zhou", "author": ["\u00c7aglar G\u00fcl\u00e7ehre", "Sungjin Ahn", "Ramesh Nallapati"], "venue": "and Yoshua Bengio.", "citeRegEx": "G\u00fcl\u00e7ehre et al.2016", "shortCiteRegEx": null, "year": 2016}, {"title": "Long short-term memory", "author": ["Hochreiter", "Schmidhuber1997] Sepp Hochreiter", "J\u00fcrgen Schmidhuber"], "venue": "Neural computation,", "citeRegEx": "Hochreiter et al\\.,? \\Q1997\\E", "shortCiteRegEx": "Hochreiter et al\\.", "year": 1997}, {"title": "Ilya Sutskever", "author": ["Navdeep Jaitly", "David Sussillo", "Quoc V. Le", "Oriol Vinyals"], "venue": "and Samy Bengio.", "citeRegEx": "Jaitly et al.2016", "shortCiteRegEx": null, "year": 2016}, {"title": "Recurrent continuous translation models", "author": ["Kalchbrenner", "Blunsom2013] Nal Kalchbrenner", "Phil Blunsom"], "venue": "In Proceedings of EMNLP", "citeRegEx": "Kalchbrenner et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Kalchbrenner et al\\.", "year": 2013}, {"title": "Adam: A method for stochastic optimization", "author": ["Kingma", "Ba2015] Diederik P. Kingma", "Jimmy Ba"], "venue": "In Proceedings of ICIR", "citeRegEx": "Kingma et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Kingma et al\\.", "year": 2015}, {"title": "Chris Dyer", "author": ["Lingpeng Kong"], "venue": "and Noah A Smith.", "citeRegEx": "Kong et al.2016", "shortCiteRegEx": null, "year": 2016}, {"title": "Ishaan Gulrajani", "author": ["Ankit Kumar", "Ozan Irsoy", "Jonathan Su", "James Bradbury", "Robert English", "Brian Pierce", "Peter Ondruska"], "venue": "and Richard Socher.", "citeRegEx": "Kumar et al.2016", "shortCiteRegEx": null, "year": 2016}, {"title": "and Christopher D", "author": ["Thang Luong", "Hieu Pham"], "venue": "Manning.", "citeRegEx": "Luong et al.2015", "shortCiteRegEx": null, "year": 2015}, {"title": "Matthew Gormley", "author": ["Courtney Napoles"], "venue": "and Benjamin Van Durme.", "citeRegEx": "Napoles et al.2012", "shortCiteRegEx": null, "year": 2012}, {"title": "Colin Cherry", "author": ["Garrett Nicolai"], "venue": "and Grzegorz Kondrak.", "citeRegEx": "Nicolai et al.2015", "shortCiteRegEx": null, "year": 2015}, {"title": "A tutorial on hidden markov models and selected applications in speech recognition", "author": ["Lawrence R Rabiner"], "venue": "Proceedings of the IEEE,", "citeRegEx": "Rabiner.,? \\Q1989\\E", "shortCiteRegEx": "Rabiner.", "year": 1989}, {"title": "Ryan Cotterell", "author": ["Pushpendre Rastogi"], "venue": "and Jason Eisner.", "citeRegEx": "Rastogi et al.2016", "shortCiteRegEx": null, "year": 2016}, {"title": "Sumit Chopra", "author": ["Alexander M. Rush"], "venue": "and Jason Weston.", "citeRegEx": "Rush et al.2015", "shortCiteRegEx": null, "year": 2015}, {"title": "Oriol Vinyals", "author": ["Ilya Sutskever"], "venue": "and Quoc V Le.", "citeRegEx": "Sutskever et al.2014", "shortCiteRegEx": null, "year": 2014}, {"title": "Hermann Ney", "author": ["Christoph Tillmann", "Stephan Vogel"], "venue": "and Alex Zubiaga.", "citeRegEx": "Tillmann et al.1997", "shortCiteRegEx": null, "year": 1997}, {"title": "Meire Fortunato", "author": ["Oriol Vinyals"], "venue": "and Navdeep Jaitly.", "citeRegEx": "Vinyals et al.2015", "shortCiteRegEx": null, "year": 2015}, {"title": "Hermann Ney", "author": ["Stephan Vogel"], "venue": "and Christoph Tillmann.", "citeRegEx": "Vogel et al.1996", "shortCiteRegEx": null, "year": 1996}, {"title": "Sumit Chopra", "author": ["Jason Weston"], "venue": "and Antoine Bordes.", "citeRegEx": "Weston et al.2015", "shortCiteRegEx": null, "year": 2015}], "referenceMentions": [], "year": 2016, "abstractText": "We introduce an online neural sequence to sequence model that learns to alternate between encoding and decoding segments of the input as it is read. By independently tracking the encoding and decoding representations our algorithm permits exact polynomial marginalization of the latent segmentation during training, and during decoding beam search is employed to find the best alignment path together with the predicted output sequence. Our model tackles the bottleneck of vanilla encoder-decoders that have to read and memorize the entire input sequence in their fixedlength hidden states before producing any output. It is different from previous attentive models in that, instead of treating the attention weights as output of a deterministic function, our model assigns attention weights to a sequential latent variable which can be marginalized out and permits online generation. Experiments on abstractive sentence summarization and morphological inflection show significant performance gains over the baseline encoder-decoders.", "creator": "LaTeX with hyperref package"}}}