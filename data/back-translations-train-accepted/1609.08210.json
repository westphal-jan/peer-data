{"id": "1609.08210", "review": {"conference": "EMNLP", "VERSION": "v1", "DATE_OF_SUBMISSION": "26-Sep-2016", "title": "Learning to Translate for Multilingual Question Answering", "abstract": "In multilingual question answering, either the question needs to be translated into the document language, or vice versa. In addition to direction, there are multiple methods to perform the translation, four of which we explore in this paper: word-based, 10-best, context-based, and grammar-based. We build a feature for each combination of translation direction and method, and train a model that learns optimal feature weights. On a large forum dataset consisting of posts in English, Arabic, and Chinese, our novel learn-to-translate approach was more effective than a strong baseline (p&lt;0.05): translating all text into English, then training a classifier based only on English (original or translated) text.", "histories": [["v1", "Mon, 26 Sep 2016 22:12:50 GMT  (359kb,D)", "http://arxiv.org/abs/1609.08210v1", "12 pages. To appear in EMNLP'16"]], "COMMENTS": "12 pages. To appear in EMNLP'16", "reviews": [], "SUBJECTS": "cs.CL cs.AI", "authors": ["ferhan t\u00fcre", "elizabeth boschee"], "accepted": true, "id": "1609.08210"}, "pdf": {"name": "1609.08210.pdf", "metadata": {"source": "CRF", "title": "Learning to Translate for Multilingual Question Answering", "authors": ["Ferhan Ture", "Elizabeth Boschee"], "emails": ["ture@cable.comcast.com", "eboschee@bbn.com"], "sections": [{"heading": "1 Introduction", "text": "The majority of QS pipelines consist of three main stages: (a) the preliminary processing of the question and the collection, (b) the questioning of the candidates in the collection, (c) the questioning of the answers relating to their relevance in the question and (c) the answering of the questions, (c) the answering of the questions, (c) the answering of the questions, (c) the answering of the questions, (c) the answering of the answers to the questions, (c) the answering of the questions, (c) the answering of the questions, (c) the answering of the questions, (c) the giving of the answers to the questions, (c) the giving of the giving of the levy on the levy on the levy on the levy on the levy on the levy on the levy on the levy on the levy on the levy on the levy on the levy on the levy on the levy on the levy on the levy on the levy on the levy on the levy on the levy on the levy on the levy on the levy on the levy on the levy on the levy on the levy on the levy on the levy on the levy on the levy on the levy on the levy on the levy on the levy on the levy on the levy on the levy on the levy on the levy on the levy on the levy on the levy on the levy on the levy on the levy on the levy on the levy on the levy on the levy on the cession, the levy on the levy on the levy on the levy on the levy on the levy on the levy on the levy on the levy on the cession on the levy on the levy on the levy on the levy on the levy on the levy on the levy on the cession, the levy on the cession of the levy on the levy on the levy on the levy on the levy on the levy on the levy on the levy on the levy on the levy on the levy on the levy on the cession of the levy on the cession of the levy on the"}, {"heading": "2 Related Work", "text": "Most previous work relied on rule-based approaches, where a set of rules for each type of question were worked out manually, or IR-like approaches, where each pair of question and candidate answers were evaluated using retrieval functions (e.g. BM25 (Robertson et al., 2004)). On the other hand, the formation of a ranking classifier enables answers to various characteristics extracted from the question, candidate response, and surrounding context (Madnani et al., 2007; Zhang et al., 2007). An explicit comparison at TREC confirmed the superiority of machine learning (ML-based) approaches (F measure 35.9% vs. 38.7%) (Zhang et al., 2007). Learning-torank approaches have also been successfully applied to QA (Agarwal et al., 2001)."}, {"heading": "3 Approach", "text": "Our work focuses on a particular phase of the QA pipeline, namely the answer ranking: Faced with a natural language question q and k candidates answer s1,..., sk, we evaluate each answer in terms of its relevance to q. In our case, candidates answer sentences extracted from all documents retrieved in the previous phase of the pipeline (with Indri (Metzler and Croft, 2005), after which sentence and answer could be used interchangeably. While our approach is not language-specific, we assume (for simplicity) that the questions are in English, while the sentences are either in English, Arabic or Chinese. Non-English answers are translated back into English before returning to the user. Our approach is not limited to each question type, factual or non-factoid. Our main motivation is to provide good QA quality for each multilingual web collection (which means finding answers to questions where there is no single answer, and for which human agreement is low."}, {"heading": "3.1 Representation", "text": "In MLQA, since questions and answers are asked in different languages, we translate most approaches into both an intermediate language and a multiple system (usually English). Due to the error-prone nature of MT, valuable information is often \"lost in translation.\" These errors are particularly noteworthy when we translate informal text or less studied languages (Van der Wees et al., 2015). Translation Guidelines: We perform a two-way translation to better preserve the original meaning: in addition to translating each non-English sentence into English, we also translate English questions into Arabic and Chinese (using multiple translation methods described below). For each question and answer pair, we have two \"views\": comparing the question with the original sentence (i.e., collection language (CL) view and comparing the original question with the translated sentence (i.e., question language (QL) view).For each question and answer pair, we have two \"views\": comparing the question with the original sentence (i.e., collection language (CL) view and comparing the original question with the translated sentence (i.e., question language (QL) view)."}, {"heading": "3.2 Features", "text": "Considering two different translation directions (CL and QL) and four different translation methods (Word q = grammar, 10-best, context), our strategy is to use a machine learning process to determine how helpful each signal is in relation to the final task. To this end, we have introduced separate characteristics of similarity between question and answer based on each combination of translation direction and method. To calculate a single real vector that represents the question in the collective language (LexCL), we start with the probability structure that represents the question translation (e.g. Figure 1 is such a structure if the translation method is grammatically based). For each word in the collective vocabulary, we calculate a weight by capturing its probability via the terms in the probability structure."}, {"heading": "3.3 Data Selection", "text": "In order to learn a model of machine learning with our novel characteristics, we need positive and negative examples of question-and-answer pairs (i.e., (q, s). To this end, our approach is to label the sentences in the source language (i.e., Arabic or Chinese) or in the question language (i.e., translated into English). In this section, we will examine whether it is useful to distinguish between these two independently generated labels, and whether this redundancy can be used to improve the machine learning process. We suspect two reasons why the selection of training data based on language MLQA might benefit: i) The translation of non-English candidate answers may be poor in quality, so that annotators may judge some relevant answers as not relevant."}, {"heading": "3.4 Language-specific Ranking", "text": "The quality of resources, grammar, etc., and other internal dynamics differ greatly between languages. We assume that there is no one-size-fits-all model, so the parameters that work best for retrieving English may not be as useful when evaluating sentences in Arabic and / or Chinese. Our proposed solution is to use a separate classifier tailored to each collection, and to retrieve three monolingual rankings (i.e. in English, Arabic and Chinese). In addition to comparing the individual language-specific classifiers, we use this idea to suggest an approach to MLQA."}, {"heading": "4 Evaluation", "text": "In fact, it is the case that most people are able to understand themselves and understand what is at stake: the question of how they should behave and the question of how they should behave. (...) It is the question of how they should behave. (...) It is the question of how they should behave. (...) It is the question of how they should behave. (...) It is the question of how they should behave. (...) It is the question of how they should behave. (...) It is the question of how they should behave. (...) It is the question of how they should behave. (...) It is the question of how they should behave. (...) It is the question of how they should behave. (...) It is the question of how they should behave. (...) It is the question of how they should behave. (...) It is the question of how they should behave. (...) It is the question of how they should behave. (...) It is the question of how they should behave."}, {"heading": "5 Conclusions", "text": "We have introduced L2T, a novel approach to MLQA, inspired by recent successes in CLIR research. To our knowledge, this is the first use of probabilistic translation methods for this task, and the first attempt to use machine learning to learn the optimal question translation. We have also proposed L2CT, which uses language-specific classifiers to treat the ranking of English, Arabic and Chinese as three separate subtasks by applying a separate classifier for each language. While post-retrieval merging has been investigated in the past, we have not found any work specifically applying this idea to create a language-aware ranking for MLQA. Our experimental analysis shows the importance of data selection when dealing with annotations at the source and translated text, and the effects of combining translation methods significantly improved effectiveness for Chinese-only, English-only, and mixed language fixation."}, {"heading": "Acknowledgements", "text": "Jacob Devlin has been very helpful in designing and implementing the context-based question translation approach, and we would also like to thank the anonymous reviewers for their helpful feedback."}], "references": [{"title": "Dublin city university at qaclef 2008", "author": ["SisayFissaha Adafre", "Josef van Genabith."], "venue": "Carol Peters, Thomas Deselaers, Nicola Ferro, Julio Gonzalo, GarethJ.F. Jones, Mikko Kurimo, Thomas Mandl, Anselmo Pe\u00f1as, and Vivien Petras, editors, Evaluating", "citeRegEx": "Adafre and Genabith.,? 2009", "shortCiteRegEx": "Adafre and Genabith.", "year": 2009}, {"title": "Learning to Rank for Robust Question Answering", "author": ["Arvind Agarwal", "Hema Raghavan", "Karthik Subbian", "Prem Melville", "Richard D Lawrence", "David C Gondek", "James Fan."], "venue": "Proceedings of the 21st ACM International Conference on Informa-", "citeRegEx": "Agarwal et al\\.,? 2012", "shortCiteRegEx": "Agarwal et al\\.", "year": 2012}, {"title": "A prototype question answering system using syntactic and semantic information for answer retrieval", "author": ["Enrique Alfonseca", "Marco De Boni", "Jos\u00e9-Luis JaraValencia", "Suresh Manandhar."], "venue": "TREC.", "citeRegEx": "Alfonseca et al\\.,? 2001", "shortCiteRegEx": "Alfonseca et al\\.", "year": 2001}, {"title": "Piqasso: Pisa question answering system", "author": ["Giuseppe Attardi", "Antonio Cisternino", "Francesco Formica", "Maria Simi", "Alessandro Tommasi."], "venue": "TREC.", "citeRegEx": "Attardi et al\\.,? 2001", "shortCiteRegEx": "Attardi et al\\.", "year": 2001}, {"title": "Learning to rank with (a lot of) word features", "author": ["Bing Bai", "Jason Weston", "David Grangier", "Ronan Collobert", "Kunihiko Sadamasa", "Yanjun Qi", "Olivier Chapelle", "Kilian Q. Weinberger."], "venue": "Inf. Retr., 13(3):291\u2013314.", "citeRegEx": "Bai et al\\.,? 2010", "shortCiteRegEx": "Bai et al\\.", "year": 2010}, {"title": "Learning concept importance using a weighted dependence model", "author": ["Michael Bendersky", "Donald Metzler", "W. Bruce Croft."], "venue": "Proceedings of the Third ACM International Conference on Web Search and Data Mining, WSDM \u201910, pages 31\u201340, New York, NY,", "citeRegEx": "Bendersky et al\\.,? 2010", "shortCiteRegEx": "Bendersky et al\\.", "year": 2010}, {"title": "Automatic information extraction", "author": ["Elizabeth Boschee", "Ralph Weischedel", "Alex Zamanian."], "venue": "Proceedings of the International Conference on Intelligence Analysis, volume 71.", "citeRegEx": "Boschee et al\\.,? 2005", "shortCiteRegEx": "Boschee et al\\.", "year": 2005}, {"title": "Data-intensive question answering", "author": ["Eric Brill", "Jimmy Lin", "Michele Banko", "Susan Dumais", "Andrew Ng."], "venue": "In Proceedings of the Tenth Text REtrieval Conference (TREC, pages 393\u2013400.", "citeRegEx": "Brill et al\\.,? 2001", "shortCiteRegEx": "Brill et al\\.", "year": 2001}, {"title": "Joint Question Clustering and Relevance Prediction for Open Domain Non-factoid Question Answering", "author": ["Snigdha Chaturvedi", "Vittorio Castelli", "Radu Florian", "Ramesh M Nallapati", "Hema Raghavan."], "venue": "Proceedings of the 23rd International Conference", "citeRegEx": "Chaturvedi et al\\.,? 2014", "shortCiteRegEx": "Chaturvedi et al\\.", "year": 2014}, {"title": "Question answering passage retrieval using dependency relations", "author": ["Hang Cui", "Renxu Sun", "Keya Li", "Min-Yen Kan", "TatSeng Chua."], "venue": "Proceedings", "citeRegEx": "Cui et al\\.,? 2005", "shortCiteRegEx": "Cui et al\\.", "year": 2005}, {"title": "Probabilistic structured query methods", "author": ["Kareem Darwish", "Douglas W. Oard."], "venue": "SIGIR.", "citeRegEx": "Darwish and Oard.,? 2003", "shortCiteRegEx": "Darwish and Oard.", "year": 2003}, {"title": "Fast and robust neural network joint models for statistical machine translation", "author": ["Jacob Devlin", "Rabih Zbib", "Zhongqiang Huang", "Thomas Lamar", "Richard M. Schwartz", "John Makhoul."], "venue": "Proceedings of the 52nd Annual Meeting of the Association for Com-", "citeRegEx": "Devlin et al\\.,? 2014", "shortCiteRegEx": "Devlin et al\\.", "year": 2014}, {"title": "UAlacant: Using Online Machine Translation for Cross-lingual Textual Entailment", "author": ["Miquel Espl\u00e0-Gomis", "Felipe S\u00e1nchez-Mart\u0131\u0301nez", "Mikel L Forcada"], "venue": "In Proceedings of the First Joint Conference on Lexical and Computational Semantics - Volume", "citeRegEx": "Espl\u00e0.Gomis et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Espl\u00e0.Gomis et al\\.", "year": 2012}, {"title": "Architecture and Evaluation of BRUJA, a Multilingual Question Answering System", "author": ["M \u00c1 Garc\u0131\u0301a-Cumbreras", "F Mart\u0131\u0301nez-Santiago", "L A Ure\u00f1a L\u00f3pez"], "venue": "Inf. Retr.,", "citeRegEx": "Garc\u0131\u0301a.Cumbreras et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Garc\u0131\u0301a.Cumbreras et al\\.", "year": 2012}, {"title": "Question answering based on temporal inference", "author": ["Sanda Harabagiu", "Cosmin Adrian Bejan."], "venue": "Proceedings of the AAAI-2005 workshop on inference for textual question answering, pages 27\u201334.", "citeRegEx": "Harabagiu and Bejan.,? 2005", "shortCiteRegEx": "Harabagiu and Bejan.", "year": 2005}, {"title": "Efficient question answering with question decomposition and multiple answer streams", "author": ["Sven Hartrumpf", "Ingo Gl\u00c3ckner", "Johannes Leveling."], "venue": "Carol Peters, Thomas Deselaers, Nicola Ferro, Julio Gonzalo, GarethJ.F. Jones, Mikko Kurimo, Thomas", "citeRegEx": "Hartrumpf et al\\.,? 2009", "shortCiteRegEx": "Hartrumpf et al\\.", "year": 2009}, {"title": "Toward semantics-based answer pinpointing", "author": ["Eduard Hovy", "Laurie Gerber", "Ulf Hermjakob", "ChinYew Lin", "Deepak Ravichandran."], "venue": "Proceedings of the first international conference on Human language technology research, pages 1\u20137. Association for", "citeRegEx": "Hovy et al\\.,? 2001", "shortCiteRegEx": "Hovy et al\\.", "year": 2001}, {"title": "Syntactic and semantic decomposition strategies for question answering from multiple resources", "author": ["Boris Katz", "Gary Borchardt", "Sue Felshin."], "venue": "Proceedings of the AAAI 2005 workshop on inference for textual question answering, pages 35\u201341.", "citeRegEx": "Katz et al\\.,? 2005", "shortCiteRegEx": "Katz et al\\.", "year": 2005}, {"title": "Combining Evidence with a Probabilistic Framework", "author": ["Jeongwoo Ko", "Luo Si", "Eric Nyberg"], "venue": null, "citeRegEx": "Ko et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Ko et al\\.", "year": 2010}, {"title": "Probabilistic Models for Answerranking in Multilingual Question-answering", "author": ["Jeongwoo Ko", "Luo Si", "Eric Nyberg", "Teruko Mitamura."], "venue": "ACM Trans. Inf. Syst., 28(3):16:1\u2014-16:37, July.", "citeRegEx": "Ko et al\\.,? 2010b", "shortCiteRegEx": "Ko et al\\.", "year": 2010}, {"title": "A survey on question answering technology from an information retrieval perspective", "author": ["Oleksandr Kolomiyets", "Marie-Francine Moens."], "venue": "Information Sciences, 181(24):5412\u20135434.", "citeRegEx": "Kolomiyets and Moens.,? 2011", "shortCiteRegEx": "Kolomiyets and Moens.", "year": 2011}, {"title": "Description of the ntou complex qa system", "author": ["Chuan-Jie Lin", "Yu-Min Kuo."], "venue": "Proceedings of NTCIR-8 Workshop.", "citeRegEx": "Lin and Kuo.,? 2010", "shortCiteRegEx": "Lin and Kuo.", "year": 2010}, {"title": "Finding what matters in questions", "author": ["Xiaoqiang Luo", "Hema Raghavan", "Vittorio Castelli", "Sameer Maskey", "Radu Florian."], "venue": "Proceedings of NAACLHLT\u201913.", "citeRegEx": "Luo et al\\.,? 2013", "shortCiteRegEx": "Luo et al\\.", "year": 2013}, {"title": "TREC 2007 ciQA Task: University of Maryland", "author": ["N Madnani", "Jimmy Lin", "Bonnie J Dorr."], "venue": "Proceedings of TREC.", "citeRegEx": "Madnani et al\\.,? 2007", "shortCiteRegEx": "Madnani et al\\.", "year": 2007}, {"title": "The miracle team at the clef 2008 multilingual question answering track", "author": ["\u00c3ngel Martinez-Gonzalez", "Cesar de Pablo-Sanchez", "Concepcion Polo-Bayo", "Mar\u00c3aTeresa Vicente-Diez", "Paloma Martinez-Fernandez", "Jose Luis MartinezFernandez."], "venue": "Carol Pe-", "citeRegEx": "Martinez.Gonzalez et al\\.,? 2009", "shortCiteRegEx": "Martinez.Gonzalez et al\\.", "year": 2009}, {"title": "Towards Cross-lingual Textual Entailment", "author": ["Yashar Mehdad", "Matteo Negri", "Marcello Federico."], "venue": "Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the Association for Computational Linguistics, HLT \u201910,", "citeRegEx": "Mehdad et al\\.,? 2010", "shortCiteRegEx": "Mehdad et al\\.", "year": 2010}, {"title": "A Markov random field model for term dependencies", "author": ["Donald Metzler", "W Bruce Croft."], "venue": "Proceedings of the 28th annual international ACM SIGIR conference on Research and development in information retrieval, SIGIR \u201905, pages 472\u2013479, New York,", "citeRegEx": "Metzler and Croft.,? 2005", "shortCiteRegEx": "Metzler and Croft.", "year": 2005}, {"title": "Keyword translation accuracy and cross-lingual question answering in chinese and japanese", "author": ["Teruko Mitamura", "Mengqiu Wang", "Hideki Shima", "Frank Lin."], "venue": "Proceedings of the Workshop on Multilingual Question Answering, MLQA \u201906, pages 31\u201338,", "citeRegEx": "Mitamura et al\\.,? 2006", "shortCiteRegEx": "Mitamura et al\\.", "year": 2006}, {"title": "Using Coreference for Question Answering", "author": ["Thomas S Morton."], "venue": "Proceedings of the Workshop on Coreference and Its Applications, CorefApp \u201999, pages 85\u201389, Stroudsburg, PA, USA. Association for Computational Linguistics.", "citeRegEx": "Morton.,? 1999", "shortCiteRegEx": "Morton.", "year": 1999}, {"title": "Integrating logic forms and anaphora resolution in the aliqan system", "author": ["Rafael Munoz-Terol", "Marcel Puchol-Blasco", "Maria Pardino", "Jose Manuel Gomez", "Sandra Roger", "Katia Vila", "Antonio Ferrandez", "Jesus Peral", "Patricio MartinezBarco."], "venue": "Evaluating Systems", "citeRegEx": "Munoz.Terol et al\\.,? 2009", "shortCiteRegEx": "Munoz.Terol et al\\.", "year": 2009}, {"title": "Cross-language information retrieval", "author": ["Jian-Yun Nie."], "venue": "Synthesis Lectures on Human Language Technologies, 3(1):1\u2013125.", "citeRegEx": "Nie.,? 2010", "shortCiteRegEx": "Nie.", "year": 2010}, {"title": "A systematic comparison of various statistical alignment models", "author": ["Franz J. Och", "Hermann Ney."], "venue": "Computational Linguistics, 29(1):19\u201351.", "citeRegEx": "Och and Ney.,? 2003", "shortCiteRegEx": "Och and Ney.", "year": 2003}, {"title": "Minimum error rate training in statistical machine translation", "author": ["Franz Josef Och."], "venue": "Proceedings of the 41st Annual Meeting on Association for Computational Linguistics - Volume 1, ACL \u201903, pages 160\u2013 167, Stroudsburg, PA, USA. Association for Compu-", "citeRegEx": "Och.,? 2003", "shortCiteRegEx": "Och.", "year": 2003}, {"title": "Whu question answering system at ntcir-8 aclia task", "author": ["Han Ren", "Donghong Ji", "Jing Wan."], "venue": "Proceedings of NTCIR-8 Workshop.", "citeRegEx": "Ren et al\\.,? 2010", "shortCiteRegEx": "Ren et al\\.", "year": 2010}, {"title": "Simple {BM25} extension to multiple weighted fields", "author": ["Stephen Robertson", "Hugo Zaragoza", "Michael Taylor."], "venue": "Proc. CIKM, pages 42\u201349.", "citeRegEx": "Robertson et al\\.,? 2004", "shortCiteRegEx": "Robertson et al\\.", "year": 2004}, {"title": "Dfki-lt at qaclef 2008", "author": ["Bogdan Sacaleanu", "G\u00fcnter Neumann", "Christian Spurk."], "venue": "Carol Peters and et al., editors, CLEF 2008 Working Notes, Working Notes. Springer Verlag.", "citeRegEx": "Sacaleanu et al\\.,? 2008", "shortCiteRegEx": "Sacaleanu et al\\.", "year": 2008}, {"title": "Evaluation of Complex Temporal Questions in CLEF-QA", "author": ["E Saquete", "J L Vicedo", "P Mart\u0131\u0301nez-Barco", "R Mu\u00f1oz", "F Llopis"], "venue": "In Proceedings of the 5th Conference on Cross-Language Evaluation Forum:", "citeRegEx": "Saquete et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Saquete et al\\.", "year": 2005}, {"title": "Combining multiple strategies for effective monolingual and cross-language retrieval", "author": ["Jacques Savoy."], "venue": "Information Retrieval, 7(1-2):121\u2013148.", "citeRegEx": "Savoy.,? 2004", "shortCiteRegEx": "Savoy.", "year": 2004}, {"title": "Using semantic roles to improve question answering", "author": ["Dan Shen", "Mirella Lapata."], "venue": "EMNLPCoNLL, pages 12\u201321. Citeseer.", "citeRegEx": "Shen and Lapata.,? 2007", "shortCiteRegEx": "Shen and Lapata.", "year": 2007}, {"title": "Bootstrap pattern learning for open-domain clqa", "author": ["Hideki Shima", "Teruko Mitamura."], "venue": "Proceedings of NTCIR-8 Workshop.", "citeRegEx": "Shima and Mitamura.,? 2010", "shortCiteRegEx": "Shima and Mitamura.", "year": 2010}, {"title": "A study of learning a merge model for multilingual information retrieval", "author": ["Ming-Feng Tsai", "Yu-Ting Wang", "Hsin-Hsi Chen."], "venue": "Proceedings of the 31st", "citeRegEx": "Tsai et al\\.,? 2008", "shortCiteRegEx": "Tsai et al\\.", "year": 2008}, {"title": "Exploiting representations from statistical machine translation for crosslanguage information retrieval", "author": ["Ferhan Ture", "Jimmy Lin."], "venue": "ACM Trans. Inf. Syst., 32(4):19:1\u201319:32, October.", "citeRegEx": "Ture and Lin.,? 2014", "shortCiteRegEx": "Ture and Lin.", "year": 2014}, {"title": "What\u2019s in a domain? analyzing genre and topic differences in statistical machine translation", "author": ["Marlies Van der Wees", "Arianna Bisazza", "Wouter Weerkamp", "Christof Monz."], "venue": "Proceedings of the 53rd Annual Meeting of the Association for Computational", "citeRegEx": "Wees et al\\.,? 2015", "shortCiteRegEx": "Wees et al\\.", "year": 2015}, {"title": "Michigan State University at the 2007 TREC ciQA Task", "author": ["Chen Zhang", "Matthew Gerber", "Tyler Baldwin", "Steven Emelander", "Joyce Chai", "Rong Jin."], "venue": "Proceedings of the Sixteenth Text Retrieval Conference, Gaithersburg, Maryland, November.", "citeRegEx": "Zhang et al\\.,? 2007", "shortCiteRegEx": "Zhang et al\\.", "year": 2007}], "referenceMentions": [{"referenceID": 41, "context": "However, recent advances in cross-lingual IR (CLIR) show that one can do better by representing the translation space as a probability distribution (Ture and Lin, 2014).", "startOffset": 148, "endOffset": 168}, {"referenceID": 34, "context": ", BM25 (Robertson et al., 2004)).", "startOffset": 7, "endOffset": 31}, {"referenceID": 23, "context": "the other hand, training a classifier for ranking candidate answers allows the exploitation of various features extracted from the question, candidate answer, and surrounding context (Madnani et al., 2007; Zhang et al., 2007).", "startOffset": 183, "endOffset": 225}, {"referenceID": 43, "context": "the other hand, training a classifier for ranking candidate answers allows the exploitation of various features extracted from the question, candidate answer, and surrounding context (Madnani et al., 2007; Zhang et al., 2007).", "startOffset": 183, "endOffset": 225}, {"referenceID": 43, "context": "7%) (Zhang et al., 2007).", "startOffset": 4, "endOffset": 24}, {"referenceID": 1, "context": "Learning-torank approaches have also been applied to QA successfully (Agarwal et al., 2012).", "startOffset": 69, "endOffset": 91}, {"referenceID": 7, "context": "Previous ML-based approaches have introduced useful features from many aspects of natural language, including lexical (Brill et al., 2001; Attardi et al., 2001), syntactic (Alfonseca et al.", "startOffset": 118, "endOffset": 160}, {"referenceID": 3, "context": "Previous ML-based approaches have introduced useful features from many aspects of natural language, including lexical (Brill et al., 2001; Attardi et al., 2001), syntactic (Alfonseca et al.", "startOffset": 118, "endOffset": 160}, {"referenceID": 2, "context": ", 2001), syntactic (Alfonseca et al., 2001; Katz et al., 2005), semantic (Cui et al.", "startOffset": 19, "endOffset": 62}, {"referenceID": 17, "context": ", 2001), syntactic (Alfonseca et al., 2001; Katz et al., 2005), semantic (Cui et al.", "startOffset": 19, "endOffset": 62}, {"referenceID": 9, "context": ", 2005), semantic (Cui et al., 2005; Katz et al., 2005; Alfonseca et al., 2001; Hovy et al., 2001), and discourse features, such as coreference resolution (Morton, 1999), or identifying temporal/spatial references (Saquete et al.", "startOffset": 18, "endOffset": 98}, {"referenceID": 17, "context": ", 2005), semantic (Cui et al., 2005; Katz et al., 2005; Alfonseca et al., 2001; Hovy et al., 2001), and discourse features, such as coreference resolution (Morton, 1999), or identifying temporal/spatial references (Saquete et al.", "startOffset": 18, "endOffset": 98}, {"referenceID": 2, "context": ", 2005), semantic (Cui et al., 2005; Katz et al., 2005; Alfonseca et al., 2001; Hovy et al., 2001), and discourse features, such as coreference resolution (Morton, 1999), or identifying temporal/spatial references (Saquete et al.", "startOffset": 18, "endOffset": 98}, {"referenceID": 16, "context": ", 2005), semantic (Cui et al., 2005; Katz et al., 2005; Alfonseca et al., 2001; Hovy et al., 2001), and discourse features, such as coreference resolution (Morton, 1999), or identifying temporal/spatial references (Saquete et al.", "startOffset": 18, "endOffset": 98}, {"referenceID": 28, "context": ", 2001), and discourse features, such as coreference resolution (Morton, 1999), or identifying temporal/spatial references (Saquete et al.", "startOffset": 64, "endOffset": 78}, {"referenceID": 36, "context": ", 2001), and discourse features, such as coreference resolution (Morton, 1999), or identifying temporal/spatial references (Saquete et al., 2005; Harabagiu and Bejan, 2005), which are especially useful for \u201cwhy\u201d and \u201chow\u201d questions (Kolomiyets and Moens, 2011).", "startOffset": 123, "endOffset": 172}, {"referenceID": 14, "context": ", 2001), and discourse features, such as coreference resolution (Morton, 1999), or identifying temporal/spatial references (Saquete et al., 2005; Harabagiu and Bejan, 2005), which are especially useful for \u201cwhy\u201d and \u201chow\u201d questions (Kolomiyets and Moens, 2011).", "startOffset": 123, "endOffset": 172}, {"referenceID": 20, "context": ", 2005; Harabagiu and Bejan, 2005), which are especially useful for \u201cwhy\u201d and \u201chow\u201d questions (Kolomiyets and Moens, 2011).", "startOffset": 94, "endOffset": 122}, {"referenceID": 38, "context": "Additionally, semantic role labeling and dependency trees are other forms of semantic analysis used widely in NLP applications (Shen and Lapata, 2007; Cui et al., 2005).", "startOffset": 127, "endOffset": 168}, {"referenceID": 9, "context": "Additionally, semantic role labeling and dependency trees are other forms of semantic analysis used widely in NLP applications (Shen and Lapata, 2007; Cui et al., 2005).", "startOffset": 127, "endOffset": 168}, {"referenceID": 33, "context": ", 2009; Lin and Kuo, 2010; Shima and Mitamura, 2010), with few notable exceptions that took term importance (Ren et al., 2010), or semantics (Munoz-Terol et al.", "startOffset": 108, "endOffset": 126}, {"referenceID": 29, "context": ", 2010), or semantics (Munoz-Terol et al., 2009) into account.", "startOffset": 22, "endOffset": 48}, {"referenceID": 22, "context": "ponent (Luo et al., 2013; Chaturvedi et al., 2014).", "startOffset": 7, "endOffset": 50}, {"referenceID": 8, "context": "ponent (Luo et al., 2013; Chaturvedi et al., 2014).", "startOffset": 7, "endOffset": 50}, {"referenceID": 19, "context": "Extending this idea to MLQA appears as a logical next step, yet most prior work relies solely on the one-best translation of questions or answers (Ko et al., 2010b; Garc\u0131\u0301a-Cumbreras et al., 2012; Chaturvedi et al., 2014), or selects the", "startOffset": 146, "endOffset": 221}, {"referenceID": 13, "context": "Extending this idea to MLQA appears as a logical next step, yet most prior work relies solely on the one-best translation of questions or answers (Ko et al., 2010b; Garc\u0131\u0301a-Cumbreras et al., 2012; Chaturvedi et al., 2014), or selects the", "startOffset": 146, "endOffset": 221}, {"referenceID": 8, "context": "Extending this idea to MLQA appears as a logical next step, yet most prior work relies solely on the one-best translation of questions or answers (Ko et al., 2010b; Garc\u0131\u0301a-Cumbreras et al., 2012; Chaturvedi et al., 2014), or selects the", "startOffset": 146, "endOffset": 221}, {"referenceID": 35, "context": "best translation out of few options (Sacaleanu et al., 2008; Mitamura et al., 2006).", "startOffset": 36, "endOffset": 83}, {"referenceID": 27, "context": "best translation out of few options (Sacaleanu et al., 2008; Mitamura et al., 2006).", "startOffset": 36, "endOffset": 83}, {"referenceID": 25, "context": "Mehdad et al. reported improvements by including the top ten translations (instead of the single best) and computing a distance-based entailment score with each (2010).", "startOffset": 0, "endOffset": 168}, {"referenceID": 30, "context": "argue that using MT as a black box is more convenient (and modular) (2012), there are potential benefits from a closer integration between statistical MT and multilingual retrieval (Nie, 2010).", "startOffset": 181, "endOffset": 192}, {"referenceID": 30, "context": "argue that using MT as a black box is more convenient (and modular) (2012), there are potential benefits from a closer integration between statistical MT and multilingual retrieval (Nie, 2010).", "startOffset": 48, "endOffset": 75}, {"referenceID": 40, "context": ", (Tsai et al., 2008)), and has shown to improve multilingual QA performance as well (Garc\u0131\u0301a-Cumbreras et al.", "startOffset": 2, "endOffset": 21}, {"referenceID": 13, "context": ", 2008)), and has shown to improve multilingual QA performance as well (Garc\u0131\u0301a-Cumbreras et al., 2012).", "startOffset": 71, "endOffset": 103}, {"referenceID": 7, "context": ", (Brill et al., 2001) for monolingual, (Ko et al.", "startOffset": 2, "endOffset": 22}, {"referenceID": 19, "context": ", 2001) for monolingual, (Ko et al., 2010a; Ko et al., 2010b) for multilingual QA).", "startOffset": 25, "endOffset": 61}, {"referenceID": 26, "context": "tences extracted from all documents retrieved in the previous stage of the pipeline (using Indri (Metzler and Croft, 2005)).", "startOffset": 97, "endOffset": 122}, {"referenceID": 8, "context": "1) We can accurately classify questions via template patterns (Chaturvedi et al. argue that this does not hold for non-factoid questions (2014)) 2) We can accurately determine the relevance of", "startOffset": 63, "endOffset": 144}, {"referenceID": 42, "context": "an answer, based on its automatic translation into English (Wees et al. show how recall decreases when translating user-generated text (2015))", "startOffset": 60, "endOffset": 142}, {"referenceID": 41, "context": "language IR (CLIR) has shown that incorporating probabilities from the internal representations of an MT system to \u201ctranslate\u201d the question can accomplish this, outperforming standard one-best translation (Ture and Lin, 2014).", "startOffset": 205, "endOffset": 225}, {"referenceID": 11, "context": "and Lin, 2014), while fourth method is a novel query translation method adapted from the neural network translation model described in (Devlin et al., 2014).", "startOffset": 135, "endOffset": 156}, {"referenceID": 32, "context": "Word: In MT, a word alignment is a many-tomany mapping between source- and target-language words, learned without supervision, at the beginning of the training pipeline (Och, 2003).", "startOffset": 169, "endOffset": 180}, {"referenceID": 10, "context": "bilities for CLIR (Darwish and Oard, 2003).", "startOffset": 18, "endOffset": 42}, {"referenceID": 41, "context": "Ture and Lin exploited this to obtain word translation probabilities from the top 10 translations of the question (2014). For each question word w, we can extract which grammar rules were used to produce the translation", "startOffset": 0, "endOffset": 121}, {"referenceID": 11, "context": "Context: Neural network-based MT models learn context-dependent word translation probabilities \u2013 the probability of a target word is dependent on the source word it aligns to, as well as a 5-word window of context (Devlin et al., 2014).", "startOffset": 214, "endOffset": 235}, {"referenceID": 37, "context": "8 Two common approaches are uniform and alternate merging (Savoy, 2004):", "startOffset": 58, "endOffset": 71}, {"referenceID": 6, "context": "All questions and forum posts were processed with an information extraction (IE) toolkit (Boschee et al., 2005), which performs sentence-splitting, named entity recognition, coreference resolution, parsing, and part-ofspeech tagging.", "startOffset": 89, "endOffset": 111}, {"referenceID": 11, "context": "tems (Devlin et al., 2014).", "startOffset": 5, "endOffset": 26}, {"referenceID": 31, "context": "GIZA++ (Och and Ney, 2003) (five iterations of IBM Models 1\u20134 and HMM).", "startOffset": 7, "endOffset": 26}, {"referenceID": 8, "context": "single similarity score between question and translated text, and matching the performance of the system by Chaturvedi et al. on the BOLT evaluation (2014). Baseline MAP values are reported on the leftmost column of Table 2.", "startOffset": 108, "endOffset": 156}, {"referenceID": 5, "context": "peared in learning-to-rank literature for monolingual IR (Bendersky et al., 2010), but not for multilingual retrieval.", "startOffset": 57, "endOffset": 81}, {"referenceID": 4, "context": "One potential next step is to learn bilingual embeddings directly for the task of QA, for which we have started adapting some related work (Bai et al., 2010).", "startOffset": 139, "endOffset": 157}], "year": 2016, "abstractText": "In multilingual question answering, either the question needs to be translated into the document language, or vice versa. In addition to direction, there are multiple methods to perform the translation, four of which we explore in this paper: word-based, 10-best, contextbased, and grammar-based. We build a feature for each combination of translation direction and method, and train a model that learns optimal feature weights. On a large forum dataset consisting of posts in English, Arabic, and Chinese, our novel learn-to-translate approach was more effective than a strong baseline (p < 0.05): translating all text into English, then training a classifier based only on English (original or translated) text.", "creator": "TeX"}}}