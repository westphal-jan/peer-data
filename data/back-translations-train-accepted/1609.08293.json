{"id": "1609.08293", "review": {"conference": "EMNLP", "VERSION": "v1", "DATE_OF_SUBMISSION": "27-Sep-2016", "title": "The Effects of Data Size and Frequency Range on Distributional Semantic Models", "abstract": "This paper investigates the effects of data size and frequency range on distributional semantic models. We compare the performance of a number of representative models for several test settings over data of varying sizes, and over test items of various frequency. Our results show that neural network-based models underperform when the data is small, and that the most reliable model over data of varying sizes and frequency ranges is the inverted factorized model.", "histories": [["v1", "Tue, 27 Sep 2016 07:38:29 GMT  (28kb)", "http://arxiv.org/abs/1609.08293v1", "Accepted at EMNLP 2016"]], "COMMENTS": "Accepted at EMNLP 2016", "reviews": [], "SUBJECTS": "cs.CL", "authors": ["magnus sahlgren", "alessandro lenci"], "accepted": true, "id": "1609.08293"}, "pdf": {"name": "1609.08293.pdf", "metadata": {"source": "CRF", "title": null, "authors": [], "emails": ["mange@[gavagai|sics].se", "alessandro.lenci@unipi.it"], "sections": [{"heading": null, "text": "ar Xiv: 160 9.08 293v 1 [cs.C L] 27 SE"}, {"heading": "1 Introduction", "text": "The various parameters of the DSM - e.g. the size of context windows, weighting schemes, dimensionality reduction techniques and similarity measures - have been thoroughly studied (Weeds et al., 2004; Sahlgren, 2006; Riordan and Jones, 2011; Bullinaria et al., 2015) and are now well understood; the effects of the different processing models - matrix-based models, neural networks and hash methods - have also recently attracted considerable attention, with sometimes conflicting conclusions (Baroni et al., 2014; Levy et al., 2015; Schnabel et al., 2015; Sahlgren et al."}, {"heading": "2 Distributional Semantic Models", "text": "DSMs could be classified in many different ways, such as the type of context and the method of building distribution vectors. Since our main goal here is to understand the effects of data size and frequency range on the different models, we focus primarily on the differences in the processing models, hence the following typology of DSMs."}, {"heading": "Explicit matrix models", "text": "We include what could be called an explicit model, in which each vector dimension corresponds to a particular context (Levy and Goldberg, 2014); the base model is a simple co-occurrence matrix F (hereinafter referred to as CO for co-occurrence); we also include the model resulting from the application of positive point-twise mutual information (PPMI) to the co-occurrence matrix; PPMI is defined as simply discarding all negative values of the PMI, calculated as: PMI (a, b) = log fab \u00b7 Tfafb (1), where fab is the co-occurrence number of the word a and word b, fa and fb are the individual frequencies of the words, and T is the number of tokens in the data. 1"}, {"heading": "Factorized matrix models", "text": "This type of model applies an additional factorization of weighted co-event numbers. We include here two variants of applying Singular Value Decomposition (SVD) to the PPMI-weighted co-event matrix; a version that discards all but the first few hundred latent dimensions (TSVD for truncated SVD), and a version that removes the first hundred latent dimensions (ISVD for inverted SVD) instead. SVD is defined by default: F = UVT (2) 1We also experimented with smoothed PPMI that raises and normalizes the context to the power of \u03b1 (Levy et al., 2015), counteracting the tendency of mutual information to favor rare events: f (b) = UVV T (2) 1We also experimented with smoothed PPMI that raises the context to the power of \u03b1 and normalizes it (Levy, 2015)."}, {"heading": "Hashing models", "text": "Another approach to reducing the dimensionality of DSMs is to use a hashing method such as Random Indexing (RI) (Kanerva et al., 2000), which accumulates distribution vectors ~ d (a) online: ~ d (a) \u2190 ~ d (ai) + c \u2211 j = \u2212 c, j 6 = 0w (x (i + j)) \u03c0j ~ r (x (i + j)))) (3) where c is the extension of the context window, w (b) is a weight that quantifies the meaning of the context term b, 2 ~ rd (b) is a sparse random index vector that acts as a fingerprint of the context term b, and \u03c0j is a permutation that rotates the random index vectors one step left or right depending on the position of the context elements within the context windows, enabling the model to take into account the word order (Sahlgret, 2008)."}, {"heading": "Neural network models", "text": "There are many variations of DSMs that use neural networks as a processing model, from simple recursive networks (Elman, 1990) to more complex deep architectures (Collobert and Weston, 2008).The incomparably popular model for neural networks is the one implemented in the word2vec library that uses the Softmax method to predict b (Mikolov et al., 2013): p (b | a) = exp (~ b \u00b7 a) \u2211 b \u2032 C exp (~ b \u2032 \u00b7 a) (4), where C is the set of context words, and ~ b (b) and ~ a is the vector representations for the context or target words. We include two versions of this general model: Continuous Bag of Words (CBOW) 2We use w (b) = e \u2212 \u03bb \u00b7 f (b) V, where f (b) the frequency of the context element is 2016, V is the total number of words seen so far (i.e., the current contextual size)."}, {"heading": "3 Experiment setup", "text": "Since our main focus in this work is on the performance of the DSMs mentioned above on data of varying sizes, we use a large corpus as a starting point and divide the data into vessels of varying sizes. We opt for the ukWaC corpus (Ferraresi et al., 2008), which consists of approximately 1.6 billion words after tokenization and lemmatization. We produce partial corpora by taking the first million, 10 million, 100 million, and 1 billion words. As the coexistence matrix from the 1 billion-word ukWaC sample is very large (more than 4,000,000 x 4,000,000), we edit the coexistence matrix to 50,000 dimensions before factoring, removing only rare contexts. 3 For comparison, we use 200 dimensions for TSVD, 2,800 (3,000-200) dimensions for ISVOD, 2,000 dimensions for RI, and 200 dimensions for CBOW and GSNS."}, {"heading": "4 Comparison by data size", "text": "Table 1 summarizes the results across the different test settings.5 The most notable aspect of these results is that neural network models do not provide competitive results for the smaller data, confirming the results of Asr et al. (2016).The best results for the smallest data are produced by the factored models, with both TSVD and ISVD producing peak values in different test environments.However, it should be noted that even the peaks for the smallest data sets are below average; only two models (PPMI and TSVD) manage to exceed the 25% random baseline for the TOEFL tests, and none of the models manage to exceed the random baseline for the ESL test. The ISVD model consistently delivers good results; it delivers the best overall results for the 10 million and 100 million word data, and is competitive with the SGNS data based on 1 billion word data, the standard model shows that all the differences between the standard SNS and their average results are evident."}, {"heading": "5 Comparison by frequency range", "text": "To study how each model handles different frequency ranges, we divide the test elements into three different classes, which contain about a third of the frequency mass of the test elements. This division was produced by collecting all the test elements into a common vocabulary, and then we sort this vocabulary according to its frequency in the ukWaC 1 Billionaire Body. We divide the vocabulary into 3 equal parts; the HIGH range with frequencies from 3,515,086 (\"do\") to 16,830 (\"organism\"), the MEDIUM range with frequencies between 16,795 (\"desirable\") and 729 (\"prickly\"), and the LOW range with frequencies from 728 (\"boardwalk\") to 16,830 (\"organism\"), the MEDIUM range with frequencies between 16,795 (\"desirable\") and 729 (\"prickly\")."}, {"heading": "6 Conclusion", "text": "Our experiments confirm the findings of Asr et al. (2016), which show that neural network-based models are sub-optimal for using them for smaller amounts of data. On the other hand, our results show that none of the standard DSMs works well in situations with small data. It could be an interesting new research direction to investigate how DSMs can be designed that are applicable to small data scenarios. Our results show that the inverted factored model (ISVD) produces the most robust results across data of different sizes and across different test scenarios. We interpret these results as further confirmation of the results of Bullinaria and Levy (2012), and O \ufffd sterlund et al. (2015), concluding that the inverted factored model represents a robust competitive alternative to the widely used neural network models SGNS and CBOW."}, {"heading": "7 Acknowledgements", "text": "This research was supported by the Swedish Research Council under contract 2014-28199."}], "references": [{"title": "Comparing predictive and co-occurrence based models of lexical semantics trained on childdirected speech", "author": ["Asr et al.2016] Fatemeh Asr", "Jon Willits", "Michael Jones"], "venue": "Proceedings of CogSci", "citeRegEx": "Asr et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Asr et al\\.", "year": 2016}, {"title": "Don\u2019t count, predict! a systematic comparison of context-counting vs. context-predicting semantic vectors", "author": ["Baroni et al.2014] Marco Baroni", "Georgiana Dinu", "Germ\u00e1n Kruszewski"], "venue": "In Proceedings of ACL,", "citeRegEx": "Baroni et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Baroni et al\\.", "year": 2014}, {"title": "Multimodal distributional semantics", "author": ["Bruni et al.2014] Elia Bruni", "Nam Khanh Tran", "Marco Baroni"], "venue": "Journal of Artificial Intelligence Research,", "citeRegEx": "Bruni et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Bruni et al\\.", "year": 2014}, {"title": "Extracting semantic representations from word co-occurrence statistics: stop-lists, stemming, and svd", "author": ["Bullinaria", "Levy2012] John Bullinaria", "Joseph P. Levy"], "venue": "Behavior Research Methods,", "citeRegEx": "Bullinaria et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Bullinaria et al\\.", "year": 2012}, {"title": "A unified architecture for natural language processing: Deep neural networks with multitask learning", "author": ["Collobert", "Weston2008] Ronan Collobert", "Jason Weston"], "venue": "In Proceedings of ICML,", "citeRegEx": "Collobert et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Collobert et al\\.", "year": 2008}, {"title": "Finding structure in time", "author": ["Jeffrey L. Elman"], "venue": "Cognitive Science,", "citeRegEx": "Elman.,? \\Q1990\\E", "shortCiteRegEx": "Elman.", "year": 1990}, {"title": "Introducing and evaluating ukwac, a very large web-derived corpus of english", "author": ["Eros Zanchetta", "Marco Baroni", "Silvia Bernardini"], "venue": "Proceedings of WAC-4,", "citeRegEx": "Ferraresi et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Ferraresi et al\\.", "year": 2008}, {"title": "Simlex-999: Evaluating semantic models with (genuine) similarity estimation", "author": ["Hill et al.2015] Felix Hill", "Roi Reichart", "Anna Korhonen"], "venue": null, "citeRegEx": "Hill et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Hill et al\\.", "year": 2015}, {"title": "Random indexing of text samples for latent semantic analysis", "author": ["Jan Kristofersson", "Anders Holst"], "venue": "In Proceedings of CogSci,", "citeRegEx": "Kanerva et al\\.,? \\Q2000\\E", "shortCiteRegEx": "Kanerva et al\\.", "year": 2000}, {"title": "A solution to platos problem: The latent semantic analysis theory of acquisition, induction, and representation of knowledge", "author": ["Landauer", "Dumais1997] Thomas K Landauer", "Susan T. Dumais"], "venue": "Psychological Review,", "citeRegEx": "Landauer et al\\.,? \\Q1997\\E", "shortCiteRegEx": "Landauer et al\\.", "year": 1997}, {"title": "Linguistic regularities in sparse and explicit word representations", "author": ["Levy", "Goldberg2014] Omer Levy", "Yoav Goldberg"], "venue": "In Proceedings of CoNLL,", "citeRegEx": "Levy et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Levy et al\\.", "year": 2014}, {"title": "Improving distributional similarity with lessons learned from word embeddings. Transactions of the Association for Computational Linguistics, 3:211\u2013225", "author": ["Levy et al.2015] Omer Levy", "Yoav Goldberg", "Ido Dagan"], "venue": null, "citeRegEx": "Levy et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Levy et al\\.", "year": 2015}, {"title": "Better word representations with recursive neural networks for morphology", "author": ["Richard Socher", "Christopher D. Manning"], "venue": "In Proceedings of CoNLL,", "citeRegEx": "Luong et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Luong et al\\.", "year": 2013}, {"title": "Distributed representations of words and phrases and their compositionality", "author": ["Ilya Sutskever", "Kai Chen", "Greg S. Corrado", "Jeff Dean"], "venue": "In Proceedings of NIPS,", "citeRegEx": "Mikolov et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Mikolov et al\\.", "year": 2013}, {"title": "Factorization of latent variables in distributional semantic models", "author": ["David \u00d6dling", "Magnus Sahlgren"], "venue": "In Proceedings of EMNLP,", "citeRegEx": "\u00d6sterlund et al\\.,? \\Q2015\\E", "shortCiteRegEx": "\u00d6sterlund et al\\.", "year": 2015}, {"title": "Redundancy in perceptual and linguistic experience: Comparing feature-based and distributional models of semantic representation", "author": ["Riordan", "Jones2011] Brian Riordan", "Michael N. Jones"], "venue": "Topics in Cognitive Science,", "citeRegEx": "Riordan et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Riordan et al\\.", "year": 2011}, {"title": "Permutations as a means to encode order in word space", "author": ["Anders Holst", "Pentti Kanerva"], "venue": "In Proceedings of CogSci,", "citeRegEx": "Sahlgren et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Sahlgren et al\\.", "year": 2008}, {"title": "The Gavagai Living Lexicon", "author": ["Amaru Cuba Gyllensten", "Fredrik Espinoza", "Ola Hamfors", "Anders Holst", "Jussi Karlgren", "Fredrik Olsson", "Per Persson", "Akshay Viswanathan"], "venue": "Proceedings of LREC", "citeRegEx": "Sahlgren et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Sahlgren et al\\.", "year": 2016}, {"title": "The WordSpace Model", "author": ["Magnus Sahlgren"], "venue": "Phd thesis,", "citeRegEx": "Sahlgren.,? \\Q2006\\E", "shortCiteRegEx": "Sahlgren.", "year": 2006}, {"title": "Evaluation methods for unsupervised word embeddings", "author": ["Igor Labutov", "David Mimno", "Thorsten Joachims"], "venue": "In Proceedings of EMNLP,", "citeRegEx": "Schnabel et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Schnabel et al\\.", "year": 2015}, {"title": "Characterising measures of lexical distributional similarity", "author": ["Weeds et al.2004] Julie Weeds", "David Weir", "Diana McCarthy"], "venue": "In Proceedings of COLING,", "citeRegEx": "Weeds et al\\.,? \\Q2004\\E", "shortCiteRegEx": "Weeds et al\\.", "year": 2004}], "referenceMentions": [{"referenceID": 20, "context": "size of context windows, weighting schemes, dimensionality reduction techniques, and similarity measures \u2014 have been thoroughly studied (Weeds et al., 2004; Sahlgren, 2006; Riordan and Jones, 2011; Bullinaria and Levy, 2012; Levy et al., 2015), and are now well understood.", "startOffset": 136, "endOffset": 243}, {"referenceID": 18, "context": "size of context windows, weighting schemes, dimensionality reduction techniques, and similarity measures \u2014 have been thoroughly studied (Weeds et al., 2004; Sahlgren, 2006; Riordan and Jones, 2011; Bullinaria and Levy, 2012; Levy et al., 2015), and are now well understood.", "startOffset": 136, "endOffset": 243}, {"referenceID": 11, "context": "size of context windows, weighting schemes, dimensionality reduction techniques, and similarity measures \u2014 have been thoroughly studied (Weeds et al., 2004; Sahlgren, 2006; Riordan and Jones, 2011; Bullinaria and Levy, 2012; Levy et al., 2015), and are now well understood.", "startOffset": 136, "endOffset": 243}, {"referenceID": 1, "context": "The impact of various processing models \u2014 matrix-based models, neural networks, and hashing methods \u2014 have also enjoyed considerable attention lately, with at times conflicting conclusions (Baroni et al., 2014; Levy et al., 2015; Schnabel et al., 2015; \u00d6sterlund et al., 2015; Sahlgren et al., 2016).", "startOffset": 189, "endOffset": 299}, {"referenceID": 11, "context": "The impact of various processing models \u2014 matrix-based models, neural networks, and hashing methods \u2014 have also enjoyed considerable attention lately, with at times conflicting conclusions (Baroni et al., 2014; Levy et al., 2015; Schnabel et al., 2015; \u00d6sterlund et al., 2015; Sahlgren et al., 2016).", "startOffset": 189, "endOffset": 299}, {"referenceID": 19, "context": "The impact of various processing models \u2014 matrix-based models, neural networks, and hashing methods \u2014 have also enjoyed considerable attention lately, with at times conflicting conclusions (Baroni et al., 2014; Levy et al., 2015; Schnabel et al., 2015; \u00d6sterlund et al., 2015; Sahlgren et al., 2016).", "startOffset": 189, "endOffset": 299}, {"referenceID": 14, "context": "The impact of various processing models \u2014 matrix-based models, neural networks, and hashing methods \u2014 have also enjoyed considerable attention lately, with at times conflicting conclusions (Baroni et al., 2014; Levy et al., 2015; Schnabel et al., 2015; \u00d6sterlund et al., 2015; Sahlgren et al., 2016).", "startOffset": 189, "endOffset": 299}, {"referenceID": 17, "context": "The impact of various processing models \u2014 matrix-based models, neural networks, and hashing methods \u2014 have also enjoyed considerable attention lately, with at times conflicting conclusions (Baroni et al., 2014; Levy et al., 2015; Schnabel et al., 2015; \u00d6sterlund et al., 2015; Sahlgren et al., 2016).", "startOffset": 189, "endOffset": 299}, {"referenceID": 0, "context": "The only previous work in this direction that we are aware of is Asr et al. (2016), who report that on small data (the CHILDES corpus), simple matrix-based models outperform neural network-based ones.", "startOffset": 65, "endOffset": 83}, {"referenceID": 11, "context": "We also experimented with smoothed PPMI, which raises the context counts to the power of \u03b1 and normalizes them (Levy et al., 2015), thereby countering the tendency of mutual information to favor infrequent events: f(b) = #(b) \u03b1 \u2211 b #(b) \u03b1 , but it did not lead to any consistent improvements compared to PPMI.", "startOffset": 111, "endOffset": 130}, {"referenceID": 8, "context": "A different approach to reduce the dimensionality of DSMs is to use a hashing method such as Random Indexing (RI) (Kanerva et al., 2000), which accumulates distributional vectors ~ d(a) in an online fashion:", "startOffset": 114, "endOffset": 136}, {"referenceID": 16, "context": "where c is the extension of the context window, w(b) is a weight that quantifies the importance of context term b,2 ~rd(b) is a sparse random index vector that acts as a fingerprint of context term b, and \u03c0 is a permutation that rotates the random index vectors one step to the left or right, depending on the position of the context items within the context windows, thus enabling the model to take word order into account (Sahlgren et al., 2008).", "startOffset": 424, "endOffset": 447}, {"referenceID": 5, "context": "There are many variations of DSMs that use neural networks as processing model, ranging from simple recurrent networks (Elman, 1990) to more complex deep architectures (Collobert and Weston, 2008).", "startOffset": 119, "endOffset": 132}, {"referenceID": 13, "context": "The incomparably most popular neural network model is the one implemented in the word2vec library, which uses the softmax for predicting b given a (Mikolov et al., 2013):", "startOffset": 147, "endOffset": 169}, {"referenceID": 17, "context": "the current size of the growing vocabulary), and \u03bb is a constant that we set to 60 (Sahlgren et al., 2016).", "startOffset": 83, "endOffset": 106}, {"referenceID": 6, "context": "We opt for the ukWaC corpus (Ferraresi et al., 2008), which comprises some 1.", "startOffset": 28, "endOffset": 52}, {"referenceID": 16, "context": "These dimensionalities have been reported to perform well for the respective models (Landauer and Dumais, 1997; Sahlgren et al., 2008; Mikolov et al., 2013; \u00d6sterlund et al., 2015).", "startOffset": 84, "endOffset": 180}, {"referenceID": 13, "context": "These dimensionalities have been reported to perform well for the respective models (Landauer and Dumais, 1997; Sahlgren et al., 2008; Mikolov et al., 2013; \u00d6sterlund et al., 2015).", "startOffset": 84, "endOffset": 180}, {"referenceID": 14, "context": "These dimensionalities have been reported to perform well for the respective models (Landauer and Dumais, 1997; Sahlgren et al., 2008; Mikolov et al., 2013; \u00d6sterlund et al., 2015).", "startOffset": 84, "endOffset": 180}, {"referenceID": 18, "context": "All DSMs use the same parameters as far as possible with a narrow context window of \u00b12 words, which has been shown to produce good results in semantic tasks (Sahlgren, 2006; Bullinaria and Levy, 2012).", "startOffset": 157, "endOffset": 200}, {"referenceID": 7, "context": "We use five standard benchmark tests in these experiments; two multiple-choice vocabulary tests (the TOEFL synonyms and the ESL synonyms), and three similarity/relatedness rating benchmarks (SimLex-999 (SL) (Hill et al., 2015), MEN (Bruni et al.", "startOffset": 207, "endOffset": 226}, {"referenceID": 2, "context": ", 2015), MEN (Bruni et al., 2014), and Stanford Rare Words (RW) (Luong et al.", "startOffset": 13, "endOffset": 33}, {"referenceID": 12, "context": ", 2014), and Stanford Rare Words (RW) (Luong et al., 2013)).", "startOffset": 38, "endOffset": 58}, {"referenceID": 0, "context": "The most notable aspect of these results is that the neural networks models do not produce competitive results for the smaller data, which corroborates the results by Asr et al. (2016). The best results for the smallest data are produced by the factorized models, with both TSVD and ISVD producing top scores in different test settings.", "startOffset": 167, "endOffset": 185}, {"referenceID": 7, "context": "In the case of SL, this confirms the results in (Hill et al., 2015), and might be due to the general bias of DSMs towards semantic relatedness, rather than genuine semantic similarity, as represented in SL.", "startOffset": 48, "endOffset": 67}, {"referenceID": 0, "context": "Our experiments confirm the results of Asr et al. (2016), who show that neural network-based models are suboptimal to use for smaller amounts of data.", "startOffset": 39, "endOffset": 57}, {"referenceID": 14, "context": "We interpret this finding as further corroborating the results of Bullinaria and Levy (2012), and \u00d6sterlund et al. (2015), with the conclusion that the inverted factorized model is a robust competitive alternative to the widely used SGNS and CBOW neural network-based models.", "startOffset": 98, "endOffset": 122}], "year": 2016, "abstractText": "This paper investigates the effects of data size and frequency range on distributional semantic models. We compare the performance of a number of representative models for several test settings over data of varying sizes, and over test items of various frequency. Our results show that neural network-based models underperform when the data is small, and that the most reliable model over data of varying sizes and frequency ranges is the inverted factorized model.", "creator": "gnuplot 4.6 patchlevel 4"}}}