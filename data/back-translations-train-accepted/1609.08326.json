{"id": "1609.08326", "review": {"conference": "ICML", "VERSION": "v1", "DATE_OF_SUBMISSION": "27-Sep-2016", "title": "Asynchronous Stochastic Gradient Descent with Delay Compensation", "abstract": "With the fast development of deep learning, people have started to train very big neural networks using massive data. Asynchronous Stochastic Gradient Descent (ASGD) is widely used to fulfill this task, which, however, is known to suffer from the problem of delayed gradient. That is, when a local worker adds the gradient it calculates to the global model, the global model may have been updated by other workers and this gradient becomes \"delayed\". We propose a novel technology to compensate this delay, so as to make the optimization behavior of ASGD closer to that of sequential SGD. This is done by leveraging Taylor expansion of the gradient function and efficient approximators to the Hessian matrix of the loss function. We call the corresponding new algorithm Delay Compensated ASGD (DC-ASGD). We evaluated the proposed algorithm on CIFAR-10 and ImageNet datasets, and experimental results demonstrate that DC-ASGD can outperform both synchronous SGD and ASGD, and nearly approaches the performance of sequential SGD.", "histories": [["v1", "Tue, 27 Sep 2016 09:22:03 GMT  (327kb,D)", "http://arxiv.org/abs/1609.08326v1", "7 pages, 5 figures"], ["v2", "Mon, 12 Jun 2017 17:53:10 GMT  (383kb,D)", "http://arxiv.org/abs/1609.08326v2", "20 pages, 5 figures"], ["v3", "Tue, 13 Jun 2017 09:02:32 GMT  (384kb,D)", "http://arxiv.org/abs/1609.08326v3", "20 pages, 5 figures"], ["v4", "Wed, 14 Jun 2017 12:45:50 GMT  (384kb,D)", "http://arxiv.org/abs/1609.08326v4", "20 pages, 5 figures"]], "COMMENTS": "7 pages, 5 figures", "reviews": [], "SUBJECTS": "cs.LG cs.DC", "authors": ["shuxin zheng", "qi meng", "taifeng wang", "wei chen", "nenghai yu", "zhiming ma", "tie-yan liu"], "accepted": true, "id": "1609.08326"}, "pdf": {"name": "1609.08326.pdf", "metadata": {"source": "CRF", "title": "Asynchronous Stochastic Gradient Descent with Delay Compensation for Distributed Deep Learning", "authors": ["Shuxin Zheng", "Qi Meng", "Taifeng Wang", "Wei Chen", "Nenghai Yu", "Zhi-Ming Ma", "Tie-Yan Liu"], "emails": ["zhengsx@mail.ustc.edu.cn,", "ynh@ustc.edu.cn", "qimeng13@pku.edu.cn", "tie-yan.liu}@microsoft.com", "mazm@amt.ac.cn"], "sections": [{"heading": "1 Introduction", "text": "This means that we must attribute the availability of large amounts of data and powerful computing resources that allow humans to train very deep and large DNN models in parallel Chen and Huo. (2015); Chen et al. (2016); Chen et al. (2016); Chen et al. (2016); Chen et al. (2016); Chen et al. (Stochastic Gradient al. (SGD) is a popular optimization method for training neural networks Bottou (2012); K\u00f6nigs and Ba (2015); Chen et al. (2016)."}, {"heading": "2 Problem Setting", "text": "In this section, we present DNN and its parallel training by the ASGD method. Faced with a multi-level classification problem that we call the input space, we will use X = Rd = 1, Y = {1,..., K} as the output space and P as the common underlying distribution via X \u00b7 Y. Here, however, d denotes the dimension of the input space, and K denotes the number of categories in the output space. Our goal is to learn a neural network model that O-F: X \u00d7 Y \u2192 R parameterized by w-Rn. Specifically, the neural network models have hierarchical structures in which each node performs linear combination and non-linear activation via its connected nodes in the lower layer. The parameters are the weights for the edges between two layers."}, {"heading": "3 Delay Compensation using Taylor Expansion and Hessian Approximation", "text": "The question then is whether we can find a way to bridge this gap. In this section we show that this gap can be illustrated by Taylor expansion, and based on this knowledge we can design effective algorithms to compensate for the delay. Gradient decomposition by means of Taylor expansion. Taylor expansion of g (wt + 5) can be as a result of Folland (2005), g (wt) and g (wt) we (wt + 1) we (wt + 2) In, where (wt + 2) we (wt) 2) 2 = (wt + 2)."}, {"heading": "Approximation of Hessian matrix", "text": "The calculation of the exact Hessian matrix is too computationally and spatially expensive, especially for large models. Alternatively, we want to find some approximators that are theoretically very close to the Hessian matrix, but can be calculated without additional complexity (i.e., only using what we already have during the previous training process). To this end, in Section 3 we can construct an unbiased approximation to the Hessian matrix based on the existing first order gradients and second order derivatives (which are the elements in the Hessian matrix). Based on the theory, we can construct an unbiased approximation to the Hessian matrix."}, {"heading": "1 repeat", "text": "2: \"Push gm to the parameter server. (5 to forever; according to the deduction above, our DC-ASGD is represented in algorithms 1 and 2 (using option-I and option-II to refer to Eqn. (9) and (10) respectively) Here we assume that DC-ASGD is implemented by the parameter server framework (although it can also be implemented in other frameworks). According to algorithm 1, local worker m pulls the latest global model wt from the parameter server, calculates its gradient gm \u2212 and sends it back to the server. Meanwhile, the parameter server will store a backup model wbak (m) when the worker m wt pulls the latest global model wt from the parameter server, calculates its gradient gm parameter gm and sends it back to the server."}, {"heading": "5 Experiments", "text": "In this section we evaluate our proposed algorithms. We used two sets of data: CIFAR-10 Krizhevsky and Hinton (2009) and ImageNet ILSVRC 2013 Russakovsky et al. (2015). The experiments were conducted on a GPU cluster connected to InfiniBand. Each node has four K40 Tesla GPU processors. We treat each GPU as a separate local worker. To parallelise the ResNet algorithm running on each worker, we chose ResNet He et al. (2016), as it generates state-of-the-art accuracy in many image-related tasks and is available for implementation by open source projects. 5 For parallelization of ResNet across machines, we used an open source parameter server.6 We implemented DC-ASGD et al. (with both approximators) on this experimental platform."}, {"heading": "Experimental Results on CIFAR-10", "text": "The CIFAR-10 dataset consists of a training set of 50k images and a test set of 10k images in 10 classes. We trained a 20-layer ResNet model on this dataset (without data augmentation). For all the algorithms that were examined, we conducted training for 160 iterations, with a mini-batch size of 128 and an initial learning rate of \u03b7 = 0.5, which is ten times after 80 and 120 iterations according to practice in He et al. (2016). For DC-ASGD, we need to set the parameters \u03bb1 and \u03bb2. Actually, Theorem 3.4 provides feasible ranges for these two parameters. By some simple calculations and empirical observations, we are relatively broad and we make sure that we GDC-G2 = 0.4 first in our experiments and are increased."}, {"heading": "Experimental Results on ImageNet", "text": "The ImageNet dataset is much larger, containing 1.28 million training images and 50k validation images in 1000 categories. We trained a 50-layer ResNet model Er et al. (2016) on this dataset. According to the previous subsection, we seem to be a better approximation for DC-ASGD than \u03bb2g g g. Therefore, in this large-scale experiment, we implemented only \u03bb1 | g | in DC-ASGD. For all the algorithms in this experiment, we performed a training for 120 iterations, with a mini batch size of 32, and an initial learning rate of \u03b7 = 0.1 (reduced by ten times after each 30 iterations). Initially, we opted for \u03bb1 = 5 (according to our theorem) for a larger GAS1 being preferred for classification tasks with more categories, and increased it tenfold after each 30 iterations. As the training on the ImageNet dataset is very time-consuming, we employed GM = 25 in our PU."}, {"heading": "Experimental Results on the influence of \u03bb", "text": "In this section we will show how the parameter \u03bb influences our DC-ASGD algorithm. In Theorem 3.4 we provide a practicable reasonable range for \u03bb, outside of this range, an excessive value of this parameter will lead to an incorrect gradient direction and too small will almost make the compensatory influence disappear. We compare the performance of the respective sequential SGD, ASGD and DC-ASGD with different values of \u03bb18. The results are shown in Figure 5. A correct \u03bb1 will lead to a significantly better accuracy. As the DC ASGD decreases, it will gradually deteriorate to ASGD.6. Conclusion In this paper we have presented a theoretical analysis of the problem of delayed gradients in the asynchronous parallelization of stochastic gradient decrease (SGD) algorithms and propose a novel algorithm called Delay Compensated asynchronous SGD (DC-ASGD) to address the problem."}], "references": [{"title": "Distributed delayed stochastic optimization", "author": ["A. Agarwal", "J.C. Duchi"], "venue": "NIPS.", "citeRegEx": "Agarwal and Duchi,? 2011", "shortCiteRegEx": "Agarwal and Duchi", "year": 2011}, {"title": "Revisiting asynchronous linear solvers: Provable convergence rate through randomization", "author": ["H. Avron", "A. Druinsky", "A. Gupta"], "venue": "JACM.", "citeRegEx": "Avron et al\\.,? 2015", "shortCiteRegEx": "Avron et al\\.", "year": 2015}, {"title": "Neural machine translation by jointly learning to align and translate", "author": ["D Bahdanau"], "venue": "In ICLR", "citeRegEx": "Bahdanau,? \\Q2015\\E", "shortCiteRegEx": "Bahdanau", "year": 2015}, {"title": "Distributed representations of words and phrases and their compositionality", "author": ["D. Bahdanau", "K. Cho", "Y. Bengio"], "venue": "NIPS.", "citeRegEx": "Bahdanau et al\\.,? 2013", "shortCiteRegEx": "Bahdanau et al\\.", "year": 2013}, {"title": "Improving the convergence of back-propagation learning with second order methods", "author": ["S. Becker", "Y. Le Cun"], "venue": "Tech.Rep.", "citeRegEx": "Becker and Cun,? 1988", "shortCiteRegEx": "Becker and Cun", "year": 1988}, {"title": "Stochastic gradient descent tricks", "author": ["L. Bottou"], "venue": "Neural Networks: Tricks of the Trade. Springer.", "citeRegEx": "Bottou,? 2012", "shortCiteRegEx": "Bottou", "year": 2012}, {"title": "Scalable training of deep learning machines by incremental block training with intrablock parallel optimization and blockwise model-update filtering", "author": ["K. Chen", "Q. Huo"], "venue": "ICASSP.", "citeRegEx": "Chen and Huo,? 2016", "shortCiteRegEx": "Chen and Huo", "year": 2016}, {"title": "Revisiting distributed synchronous sgd", "author": ["J. Chen", "R. Monga", "S. Bengio", "R. Jozefowicz"], "venue": "ICLR.", "citeRegEx": "Chen et al\\.,? 2016", "shortCiteRegEx": "Chen et al\\.", "year": 2016}, {"title": "Higher-order derivatives and taylor\u2019s formula in several variables", "author": ["G. Folland"], "venue": null, "citeRegEx": "Folland,? \\Q2005\\E", "shortCiteRegEx": "Folland", "year": 2005}, {"title": "The elements of statistical learning", "author": ["J. Friedman", "T. Hastie", "R. Tibshirani"], "venue": "Springer series in statistics Springer, Berlin.", "citeRegEx": "Friedman et al\\.,? 2001", "shortCiteRegEx": "Friedman et al\\.", "year": 2001}, {"title": "Deep residual learning for image recognition", "author": ["K. He", "X. Zhang", "S. Ren", "J. Sun"], "venue": "CVPR.", "citeRegEx": "He et al\\.,? 2016", "shortCiteRegEx": "He et al\\.", "year": 2016}, {"title": "Adam: A method for stochastic optimization", "author": ["D. Kingma", "J. Ba"], "venue": "ICLR.", "citeRegEx": "Kingma and Ba,? 2015", "shortCiteRegEx": "Kingma and Ba", "year": 2015}, {"title": "Learning multiple layers of features from tiny images", "author": ["A. Krizhevsky", "G. Hinton"], "venue": null, "citeRegEx": "Krizhevsky and Hinton,? \\Q2009\\E", "shortCiteRegEx": "Krizhevsky and Hinton", "year": 2009}, {"title": "Imagenet classification with deep convolutional neural networks", "author": ["A. Krizhevsky", "I. Sutskever", "G.E. Hinton"], "venue": "NIPS.", "citeRegEx": "Krizhevsky et al\\.,? 2012", "shortCiteRegEx": "Krizhevsky et al\\.", "year": 2012}, {"title": "Asynchronous parallel stochastic gradient for nonconvex optimization", "author": ["X. Lian", "Y. Huang", "Y. Li", "J. Liu"], "venue": "NIPS.", "citeRegEx": "Lian et al\\.,? 2015", "shortCiteRegEx": "Lian et al\\.", "year": 2015}, {"title": "Delay-tolerant algorithms for asynchronous distributed online learning", "author": ["B. McMahan", "M. Streeter"], "venue": "NIPS.", "citeRegEx": "McMahan and Streeter,? 2014", "shortCiteRegEx": "McMahan and Streeter", "year": 2014}, {"title": "Hogwild: A lock-free approach to parallelizing stochastic gradient descent", "author": ["B. Recht", "C. Re", "S. Wright", "F. Niu"], "venue": "NIPS.", "citeRegEx": "Recht et al\\.,? 2011", "shortCiteRegEx": "Recht et al\\.", "year": 2011}, {"title": "Long shortterm memory recurrent neural network architectures for large scale acoustic modeling", "author": ["H. Sak", "A.W. Senior", "F. Beaufays"], "venue": "INTERSPEECH.", "citeRegEx": "Sak et al\\.,? 2014", "shortCiteRegEx": "Sak et al\\.", "year": 2014}, {"title": "Very deep multilingual convolutional neural networks for lvcsr", "author": ["T. Sercu", "C. Puhrsch", "B. Kingsbury", "Y. LeCun"], "venue": "ICASSP.", "citeRegEx": "Sercu et al\\.,? 2016", "shortCiteRegEx": "Sercu et al\\.", "year": 2016}, {"title": "Inception-v4, inception-resnet and the impact of residual connections on learning", "author": ["C. Szegedy", "S. Ioffe", "V. Vanhoucke"], "venue": "arXiv preprint arXiv:1602.07261.", "citeRegEx": "Szegedy et al\\.,? 2016", "shortCiteRegEx": "Szegedy et al\\.", "year": 2016}, {"title": "Deep learning with elastic averaging sgd", "author": ["S. Zhang", "A.E. Choromanska", "Y. LeCun"], "venue": "NIPS.", "citeRegEx": "Zhang et al\\.,? 2015", "shortCiteRegEx": "Zhang et al\\.", "year": 2015}], "referenceMentions": [{"referenceID": 12, "context": "1 Introduction Deep Neural Networks (DNN) have pushed the frontiers of many applications, such as speech recognition Sak, Senior, and Beaufays (2014); Sercu et al. (2016), computer vision Krizhevsky, Sutskever, and Hinton (2012); He et al.", "startOffset": 151, "endOffset": 171}, {"referenceID": 12, "context": "1 Introduction Deep Neural Networks (DNN) have pushed the frontiers of many applications, such as speech recognition Sak, Senior, and Beaufays (2014); Sercu et al. (2016), computer vision Krizhevsky, Sutskever, and Hinton (2012); He et al.", "startOffset": 151, "endOffset": 229}, {"referenceID": 6, "context": "(2016), computer vision Krizhevsky, Sutskever, and Hinton (2012); He et al. (2016); Szegedy, Ioffe, and Vanhoucke (2016), and natural language processing Bahdanau and others (2015); Bahdanau, Cho, and Bengio (2013).", "startOffset": 66, "endOffset": 83}, {"referenceID": 6, "context": "(2016), computer vision Krizhevsky, Sutskever, and Hinton (2012); He et al. (2016); Szegedy, Ioffe, and Vanhoucke (2016), and natural language processing Bahdanau and others (2015); Bahdanau, Cho, and Bengio (2013).", "startOffset": 66, "endOffset": 121}, {"referenceID": 2, "context": "(2016); Szegedy, Ioffe, and Vanhoucke (2016), and natural language processing Bahdanau and others (2015); Bahdanau, Cho, and Bengio (2013).", "startOffset": 78, "endOffset": 105}, {"referenceID": 2, "context": "(2016); Szegedy, Ioffe, and Vanhoucke (2016), and natural language processing Bahdanau and others (2015); Bahdanau, Cho, and Bengio (2013). Part of the success of DNN should be attributed to the availability of big data and powerful computational resources, which allows people to train very deep and big DNN models in parallelChen and Huo (2016); Zhang, Choromanska, and LeCun (2015); Chen et al.", "startOffset": 78, "endOffset": 139}, {"referenceID": 2, "context": "(2016); Szegedy, Ioffe, and Vanhoucke (2016), and natural language processing Bahdanau and others (2015); Bahdanau, Cho, and Bengio (2013). Part of the success of DNN should be attributed to the availability of big data and powerful computational resources, which allows people to train very deep and big DNN models in parallelChen and Huo (2016); Zhang, Choromanska, and LeCun (2015); Chen et al.", "startOffset": 78, "endOffset": 347}, {"referenceID": 2, "context": "(2016); Szegedy, Ioffe, and Vanhoucke (2016), and natural language processing Bahdanau and others (2015); Bahdanau, Cho, and Bengio (2013). Part of the success of DNN should be attributed to the availability of big data and powerful computational resources, which allows people to train very deep and big DNN models in parallelChen and Huo (2016); Zhang, Choromanska, and LeCun (2015); Chen et al.", "startOffset": 78, "endOffset": 385}, {"referenceID": 2, "context": "(2016); Szegedy, Ioffe, and Vanhoucke (2016), and natural language processing Bahdanau and others (2015); Bahdanau, Cho, and Bengio (2013). Part of the success of DNN should be attributed to the availability of big data and powerful computational resources, which allows people to train very deep and big DNN models in parallelChen and Huo (2016); Zhang, Choromanska, and LeCun (2015); Chen et al. (2016). Stochastic Gradient Descent (SGD) is a popular optimization algorithm to train neural networks Bottou (2012); Kingma and Ba (2015); Dean et al.", "startOffset": 78, "endOffset": 405}, {"referenceID": 2, "context": "(2016); Szegedy, Ioffe, and Vanhoucke (2016), and natural language processing Bahdanau and others (2015); Bahdanau, Cho, and Bengio (2013). Part of the success of DNN should be attributed to the availability of big data and powerful computational resources, which allows people to train very deep and big DNN models in parallelChen and Huo (2016); Zhang, Choromanska, and LeCun (2015); Chen et al. (2016). Stochastic Gradient Descent (SGD) is a popular optimization algorithm to train neural networks Bottou (2012); Kingma and Ba (2015); Dean et al.", "startOffset": 78, "endOffset": 515}, {"referenceID": 2, "context": "(2016); Szegedy, Ioffe, and Vanhoucke (2016), and natural language processing Bahdanau and others (2015); Bahdanau, Cho, and Bengio (2013). Part of the success of DNN should be attributed to the availability of big data and powerful computational resources, which allows people to train very deep and big DNN models in parallelChen and Huo (2016); Zhang, Choromanska, and LeCun (2015); Chen et al. (2016). Stochastic Gradient Descent (SGD) is a popular optimization algorithm to train neural networks Bottou (2012); Kingma and Ba (2015); Dean et al.", "startOffset": 78, "endOffset": 537}, {"referenceID": 2, "context": "(2016); Szegedy, Ioffe, and Vanhoucke (2016), and natural language processing Bahdanau and others (2015); Bahdanau, Cho, and Bengio (2013). Part of the success of DNN should be attributed to the availability of big data and powerful computational resources, which allows people to train very deep and big DNN models in parallelChen and Huo (2016); Zhang, Choromanska, and LeCun (2015); Chen et al. (2016). Stochastic Gradient Descent (SGD) is a popular optimization algorithm to train neural networks Bottou (2012); Kingma and Ba (2015); Dean et al. (2012). As for the parallelization of SGD algorithms (suppose we use M machines for the parallelization), one can choose to do it in either a synchronous or asynchronous way.", "startOffset": 78, "endOffset": 557}, {"referenceID": 2, "context": "(2016); Szegedy, Ioffe, and Vanhoucke (2016), and natural language processing Bahdanau and others (2015); Bahdanau, Cho, and Bengio (2013). Part of the success of DNN should be attributed to the availability of big data and powerful computational resources, which allows people to train very deep and big DNN models in parallelChen and Huo (2016); Zhang, Choromanska, and LeCun (2015); Chen et al. (2016). Stochastic Gradient Descent (SGD) is a popular optimization algorithm to train neural networks Bottou (2012); Kingma and Ba (2015); Dean et al. (2012). As for the parallelization of SGD algorithms (suppose we use M machines for the parallelization), one can choose to do it in either a synchronous or asynchronous way. In synchronous SGD (SSGD) Chen et al. (2016), local workers compute the gradients over a mini-batch of their own data, and add these gradients to the", "startOffset": 78, "endOffset": 770}, {"referenceID": 14, "context": "This problem has been well known, and some researchers have analyzed its negative effect to the convergence speed Lian et al. (2015); Avron, Druinsky, and Gupta (2015).", "startOffset": 114, "endOffset": 133}, {"referenceID": 14, "context": "This problem has been well known, and some researchers have analyzed its negative effect to the convergence speed Lian et al. (2015); Avron, Druinsky, and Gupta (2015). In this paper, we want to tackle the challenges of delayed gradient so as to make ASGD both efficient and more mathematically sound.", "startOffset": 114, "endOffset": 168}, {"referenceID": 7, "context": "Recently, people proposed to partially resolve this problem by using additional backup workers Chen et al. (2016). However, this technique requires additional computation resource, and works on the assumption that the majority of workers train almost equally fast.", "startOffset": 95, "endOffset": 114}, {"referenceID": 13, "context": "Actually, this problem of delayed gradient has been well known Lian et al. (2015); Avron, Druinsky, and Gupta (2015); Agarwal and Duchi (2011); Recht et al.", "startOffset": 63, "endOffset": 82}, {"referenceID": 13, "context": "Actually, this problem of delayed gradient has been well known Lian et al. (2015); Avron, Druinsky, and Gupta (2015); Agarwal and Duchi (2011); Recht et al.", "startOffset": 63, "endOffset": 117}, {"referenceID": 0, "context": "(2015); Avron, Druinsky, and Gupta (2015); Agarwal and Duchi (2011); Recht et al.", "startOffset": 43, "endOffset": 68}, {"referenceID": 0, "context": "(2015); Avron, Druinsky, and Gupta (2015); Agarwal and Duchi (2011); Recht et al. (2011), and many practical observations indicate that it usually costs ASGD more iterations to converge than sequential SGD, and sometimes, the converged model of ASGD cannot reach the same accuracy as that obtained by sequential SGD, especially when the number of workers becomes large.", "startOffset": 43, "endOffset": 89}, {"referenceID": 0, "context": "(2015); Avron, Druinsky, and Gupta (2015); Agarwal and Duchi (2011); Recht et al. (2011), and many practical observations indicate that it usually costs ASGD more iterations to converge than sequential SGD, and sometimes, the converged model of ASGD cannot reach the same accuracy as that obtained by sequential SGD, especially when the number of workers becomes large. Researchers have also tried to improve ASGD from different perspectives Zhang, Choromanska, and LeCun (2015); McMahan and Streeter (2014); Ho et al.", "startOffset": 43, "endOffset": 479}, {"referenceID": 0, "context": "(2015); Avron, Druinsky, and Gupta (2015); Agarwal and Duchi (2011); Recht et al. (2011), and many practical observations indicate that it usually costs ASGD more iterations to converge than sequential SGD, and sometimes, the converged model of ASGD cannot reach the same accuracy as that obtained by sequential SGD, especially when the number of workers becomes large. Researchers have also tried to improve ASGD from different perspectives Zhang, Choromanska, and LeCun (2015); McMahan and Streeter (2014); Ho et al.", "startOffset": 43, "endOffset": 508}, {"referenceID": 0, "context": "(2015); Avron, Druinsky, and Gupta (2015); Agarwal and Duchi (2011); Recht et al. (2011), and many practical observations indicate that it usually costs ASGD more iterations to converge than sequential SGD, and sometimes, the converged model of ASGD cannot reach the same accuracy as that obtained by sequential SGD, especially when the number of workers becomes large. Researchers have also tried to improve ASGD from different perspectives Zhang, Choromanska, and LeCun (2015); McMahan and Streeter (2014); Ho et al. (2013), however, to the best of our knowledge, there is no sound solution to compensate the delayed gradient while keeping the high efficiency of ASGD yet.", "startOffset": 43, "endOffset": 526}, {"referenceID": 8, "context": "Gradient decomposition using Taylor expansion The Taylor expansion of g(wt+\u03c4 ) at wt can be written as follows Folland (2005),", "startOffset": 111, "endOffset": 126}, {"referenceID": 14, "context": "Following the proof of ASGD Lian et al. (2015) and under the same assumptions, we can get the following theorem, which describe the convergence rate of DC-ASGD.", "startOffset": 28, "endOffset": 47}, {"referenceID": 10, "context": "We used two datasets: CIFAR-10 Krizhevsky and Hinton (2009), and ImageNet ILSVRC 2013 Russakovsky et al.", "startOffset": 31, "endOffset": 60}, {"referenceID": 10, "context": "We used two datasets: CIFAR-10 Krizhevsky and Hinton (2009), and ImageNet ILSVRC 2013 Russakovsky et al. (2015). The experiments were conducted on a GPU cluster interconnected with InfiniBand.", "startOffset": 31, "endOffset": 112}, {"referenceID": 9, "context": "For the DNN algorithm running on each worker, we chose ResNet He et al. (2016) since it produces the state-of-the-art accuracy in many image related tasks and its implementation is available through open-source projects.", "startOffset": 62, "endOffset": 79}, {"referenceID": 7, "context": "In addition, we also implemented ASGD and SSGD, which have been used in many previous works as baselines Chen et al. (2016); Dean et al.", "startOffset": 105, "endOffset": 124}, {"referenceID": 7, "context": "In addition, we also implemented ASGD and SSGD, which have been used in many previous works as baselines Chen et al. (2016); Dean et al. (2012). Furthermore, for the experiments on CIFAR-10, we used the sequential SGD algorithm as a reference model to examine the accuracy of parallel algorithms.", "startOffset": 105, "endOffset": 144}, {"referenceID": 10, "context": "5 which were reduced by ten times after 80 and 120 iterations following the practice in He et al. (2016). For DC-ASGD, we need to set parameters \u03bb1 and \u03bb2.", "startOffset": 88, "endOffset": 105}, {"referenceID": 10, "context": "We trained a 50-layer ResNet model He et al. (2016) on this dataset.", "startOffset": 35, "endOffset": 52}, {"referenceID": 7, "context": "Please note this time the accuracy of SSGD is quite good (which is consistent with a separate observation in Chen et al. (2016)).", "startOffset": 109, "endOffset": 128}], "year": 2016, "abstractText": "With the fast development of deep learning, people have started to train very big neural networks using massive data. Asynchronous Stochastic Gradient Descent (ASGD) is widely used to fulfill this task, which, however, is known to suffer from the problem of delayed gradient. That is, when a local worker adds the gradient it calculates to the global model, the global model may have been updated by other workers and this gradient becomes \u201cdelayed\u201d. We propose a novel technology to compensate this delay, so as to make the optimization behavior of ASGD closer to that of sequential SGD. This is done by leveraging Taylor expansion of the gradient function and efficient approximators to the Hessian matrix of the loss function. We call the corresponding new algorithm Delay Compensated ASGD (DC-ASGD). We evaluated the proposed algorithm on CIFAR-10 and ImageNet datasets, and experimental results demonstrate that DC-ASGD can outperform both synchronous SGD and ASGD, and nearly approaches the performance of sequential SGD.", "creator": "LaTeX with hyperref package"}}}