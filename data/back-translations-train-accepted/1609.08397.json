{"id": "1609.08397", "review": {"conference": "AAAI", "VERSION": "v1", "DATE_OF_SUBMISSION": "27-Sep-2016", "title": "Generalization Error Bounds for Optimization Algorithms via Stability", "abstract": "Many machine learning tasks can be formulated as Regularized Empirical Risk Minimization (R-ERM), and solved by optimization algorithms such as gradient descent (GD), stochastic gradient descent (SGD), and stochastic variance reduction (SVRG). Conventional analysis on these optimization algorithms focuses on their convergence rates during the training process, however, people in the machine learning community may care more about the generalization performance of the learned model on unseen test data. In this paper, we investigate on this issue, by using stability as a tool. In particular, we decompose the generalization error for R-ERM, and derive its upper bound for both convex and non-convex cases. In convex cases, we prove that the generalization error can be bounded by the convergence rate of the optimization algorithm and the stability of the R-ERM process, both in expectation (in the order of $\\mathcal{O}((1/n)+\\mathbb{E}\\rho(T))$, where $\\rho(T)$ is the convergence error and $T$ is the number of iterations) and in high probability (in the order of $\\mathcal{O}\\left(\\frac{\\log{1/\\delta}}{\\sqrt{n}}+\\rho(T)\\right)$ with probability $1-\\delta$). For non-convex cases, we can also obtain a similar expected generalization error bound. Our theorems indicate that 1) along with the training process, the generalization error will decrease for all the optimization algorithms under our investigation; 2) Comparatively speaking, SVRG has better generalization ability than GD and SGD. We have conducted experiments on both convex and non-convex problems, and the experimental results verify our theoretical findings.", "histories": [["v1", "Tue, 27 Sep 2016 13:10:57 GMT  (850kb,D)", "http://arxiv.org/abs/1609.08397v1", null]], "reviews": [], "SUBJECTS": "stat.ML cs.LG", "authors": ["qi meng", "yue wang", "wei chen", "taifeng wang", "zhiming ma", "tie-yan liu"], "accepted": true, "id": "1609.08397"}, "pdf": {"name": "1609.08397.pdf", "metadata": {"source": "CRF", "title": "Generalization Error Bounds for Optimization Algorithms via Stability", "authors": ["Qi Meng", "Yue Wang", "Wei Chen", "Taifeng Wang", "Zhi-Ming Ma", "Tie-Yan Liu"], "emails": ["qimeng13@pku.edu.cn", "11271012@bjtu.edu.cn", "tie-yan.liu}@microsoft.com", "mazm@amt.ac.cn"], "sections": [{"heading": null, "text": "Our theorems indicate that 1) along with the training process, generalization errors will decrease for all optimization algorithms we have studied; 2) SVRG, in comparison, has better generalization capability than GD and SGD. We have conducted experiments on both convex and non-convex problems, and the experimental results confirm our theoretical results."}, {"heading": "1 Introduction", "text": "In fact, most people are able to survive on their own."}, {"heading": "2 Preliminaries", "text": "In this section, we briefly present the R-ERM problem and the common optimization algorithms for its solution."}, {"heading": "2.1 R-ERM and its Stability", "text": "Let us suppose that we know the prediction model from training, and we will apply this model. (...) Let us assume that we apply the prediction modalities for classification to practice. (...) Let us assume that we apply the prediction modalities for classification to practice. (...) Let us assume that we apply the prediction modalities to practice. (...) Let us assume that the prediction practice is applicable in practice. (...) Let us assume that the prediction practice is applicable in practice. (...) Let us assume that the prediction practice is applicable in practice. (...) Let us assume that the risks are applicable in practice. (...) Let us assume that the risks are applicable in practice. (...) Let us assume that the risks are applicable in practice."}, {"heading": "2.2 Optimization Algorithms", "text": "In fact, the fact is that most of us will be able to be able to be able to be able to be able to be able to be able to be able to be able to be able to be able to be able to be able to be able to be able to be able to be able to be able to be able to be able to be able to be able to be able to be able to be able to be able to be able to be able to be able to be able to be able to be able to be able to be able to be able to be able to be able to be."}, {"heading": "3 Generalization Analysis", "text": "In this section, we will analyze the generalization error for optimization algorithms using stability as a tool.3Second-order methods can obtain quadratic convergence rates (Nocedal and Wright 2006). However, compared to second-order methods, the computational complexity of second-order methods could be much higher due to the computation of second-order information.First, we introduce the definition of the generalization error and its decomposition. Then, we will prove the limitations of generalization errors of optimization algorithms in both convex and non-convex cases. The evidence details of all lemmats and theorems are placed in the complementary materials due to space constraints."}, {"heading": "3.1 Generalization Error and its Decomposition", "text": "It is clear that the minimization of RrS (f) in F is equivalent to the minimization of RS (f) in Fc (f) in Fc (f) in F (f) in F (f) in F (f) in F (f) in F (f) in F (f) in F (f) in F (f) in F (f) in F (f) in F (f). It is clear that the minimization of RrS (f) in F (f) is the minimization of RS (f) in F (f)."}, {"heading": "3.2 Expected Generalization Bounds for Convex Case", "text": "The following theorems indicate an expected generalization error (T) and the well-investigated error (VII). (P) Theorem 3.1 Consider an R-ERM problem when the loss function L-ERM process (T) + 0-ERM process (T) + 0-ERM process (T) + 0-ERM process (T) + 0-ERM process (T) + 0-ERM process (T) + 0-ERM process (T). (T) are the convergence errors defined in Eqn 16.1. (18) Where \u03b20, \u03b21 we are the uniform stability and output stability of the R-ERM process as defined in 2.1 and 2.2, 0-ERM process (T). (T) are the convergence errors that we have defined in Theorem 3.1, we can see that the generalization errors are due to the stability 0 and the dimension errors (1) of the convergence algorithms (1)."}, {"heading": "3.3 High-Probability Generalization Bounds for Convex Case", "text": "Theorem 3.5 For an R-ERM problem, if the loss function L-Lipschitz is continuous, \u03b3-smooth and convex in relation to the forecast output vector, and 0 \u2264 l (f-S, Fc, z) \u2264 M for any z-Z and S-Zn, then with a probability of at least 1 \u2212 \u03b4, we have E \u2264 Eapp + 2\u03b20 + 0 (T) + \u03b32 \u03c11 (T) + 2\u03b3\u03b21 (T) + (4n\u03b20 + 2M + (4n\u03b3\u03b21 + L), we have E \u2264 Eapp + 2\u03b20 (T), the upper Eapp + 2n. The highly probable limit is consistent with the expected probability given in the previous subsection. That is, the highly probable generalization will also decrease."}, {"heading": "3.4 Expected Generalization Bounds for Nonconvex Case", "text": "In this subsection, we look at the case where the loss function is convex w.r.t., the predictive output vector, but non-convex w.r.t. the model parameter. This case can cover deep neural networks that are state of the art today in AI techniques.Definition 3.8 If we look at the non-convex case, the definition of the convergence error differs a little, like equation. (17) It measures whether the solution is close to a critical point that is also defined as following and further categorized as following.Definition 3.8 If we look at the objective RrS and parameterw. If the definition RrS (w) = 0, say w is a critical point of RrS; if the RrS (w) has at least a strictly negative eigenvalue, say w is a strict saddle point. If each critical point w is either a local minimum or we say a strict saddle point, then Rrx is a strict one."}, {"heading": "4 Sufficient Training and Optimal Generalization Error", "text": "In this section we will discuss further the generalization error. In particular, we will explore the sufficient training iterations or optimal generalization error in view of the size of the training data. As shown in Section 3, the generalization error boundaries consist of an estimation error related to the training data size n and an optimization error related to the training siteration T. In the face of a machine learning task with fixed training size n, in the early stages of the training process (i.e., T is relatively small), the optimization error will dominate the generalization error; if T becomes larger than a threshold, the optimization error will be smaller than the estimation error (i.e. O (1 / n) and then the estimation error will dominate the generalization error. We call this threshold sufficient training siteration and the corresponding training time sufficient training time. The generalization error with the optimization algorithm x is called the optimal generalization error."}, {"heading": "5 Experiments", "text": "In this section, we report on experimental results to validate our theoretical results. We conducted experiments on three tasks: linear regression, logistic regression, and fully connected neural networks, whose objective functions are the least square loss, logistic loss, and cross-entropy loss. For each task, we report on three figures. The horizontal axis of each figure corresponds to the number of data runs, and the vertical axis corresponds to the results for convex problems, and the third task is used to verify our theory of nonconvex problems. The horizontal axis of each figure corresponds to the number of data runs, and the vertical axis corresponds to the training loss, the test loss, and the log-scaled test loss. For linear regression, we independently sample data instances with the size n = 40,000 from a 100 \u2212 dimensional gauss distribution. We use half of them as training data and the others as test data."}, {"heading": "6 Conclusion", "text": "In this paper, we have investigated the generalization error limits for optimization algorithms in order to solve R-ERM problems by using stability as a tool. In the case of convex problems, we have reached both expectation limits and maximum probability limits. Some of our results can be extended to the non-convex case. Broadly speaking, our theoretical analysis has shown: (1) With the training process, the generalization error will decrease; (2) SVRG exceeds GD and SGD in most cases. We have verified the theoretical results through experiments on linear regression, logistic regression, and fully networked neural networks. In the future, we plan to investigate the stability of R-ERM with other regularization terms, such as the L1 regulator, which is normally associated with uneven optimization methods."}, {"heading": "7 Appendices", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "7.1 Proofs of Lemma 3.2, Lemma 3.3 and", "text": "Theoretically, we are able to find a solution that is tailored to people's needs. (...) Theoretically, we are able to meet people's needs. (...) Theoretically, we are able to satisfy people's needs. (...) Theoretically, we are able to satisfy people's needs. (...) Theoretically, we are able to satisfy people's needs. (...) Theoretically, we are able to satisfy people's needs. (...) Theoretically, we are able to satisfy people's needs. (...) Theoretically, we are able to satisfy people's needs. (...) Theoretically, we are able to satisfy people's needs. (...) Theoretically, we are able to satisfy people's needs. (...) Theoretically, we are able to satisfy people's needs. (...) Theoretically, we are able to satisfy people's needs. (...) Theoretically, we are able to satisfy people's needs. (...) Theoretically, we are able to satisfy people's needs."}, {"heading": "7.2 Proof of Theorem 3.5", "text": "Theorem f & # 252; for an R & # 214; ERM problem we have to use the following theorems, the McDiarmid.Theorem (McDiarmid, 1989): Let S and Si are two sets of data, which can only use one point each j & # 246;. & # 8222; Let S & # 8220; and Si are two different sets of data. & # 8220; Let F & # 8222; Let S & # 8222; Let F & # 8220;. & # 8220; Let S and Si are two different sets of data. & # 8222; Let S & # 8220; and Si are only one point j & # 8220;."}, {"heading": "7.3 Proof of Theorem 3.9", "text": "\"We must be prepared not to take on more than half of the costs,\" he says, \"but we must do more than we do.\" (\"We must do more.\") (\"We must do more.\") (\"We must do more.\") (\"We must do more.\") (\"We must do more.\") (\"We must do more.\") (\"We must do more.\") (\"We must do more.\") (\"We must do more.\" (\"We must do more.\") (\"We must do more.\") (\"We must do more.\" (\"We must.\"). (\"We.\"). (\"We.\" (\"We must do more.\"). (\"We must do more.\"). (\"We must do more.\"). (\"We must do more.\" (. \"We.\"). (\"We.\" We. \"We.\" (. \"We.\" We. \"). (\" We. \"We.\" We. \"We.\" (. \"We.\"). (. \"We.\" We. \"). (\" We. \"We.\" We. \"(.\" We. \").\" We. \"(.\" We. \"We.\"). (. \"We.\" We. (. \").\" We. (. \"We.\" We. (. \"). (.\" We. \"We. (.\" We. \"). (.\" We. (. \"We.\"). (. \"We.\" We. \"(.\"). (\"We. (\"). (\"We. (\" We. \"). (\" We. \"We.\" We. \"). (. (.\"). (\"We. (\"). (\"). (\" We. (. (\"We. (\" We. \"). (\" We. \"). (\" We. (. (\"We.\"). (. (. \"). (\" We. (. (\"We.\"). (\"We.\"). (\"We.\"). (. (\"We. (.\" We. (. \"). (\"). (\"We."}], "references": [{"title": "and Bottou", "author": ["O. Bousquet"], "venue": "L.", "citeRegEx": "Bousquet and Bottou 2008", "shortCiteRegEx": null, "year": 2008}, {"title": "and Elisseeff", "author": ["O. Bousquet"], "venue": "A.", "citeRegEx": "Bousquet and Elisseeff 2002", "shortCiteRegEx": null, "year": 2002}, {"title": "R", "author": ["Byrd"], "venue": "H.; Hansen, S.; Nocedal, J.; and Singer, Y.", "citeRegEx": "Byrd et al. 2016", "shortCiteRegEx": null, "year": 2016}, {"title": "On the generalization ability of on-line learning algorithms", "author": ["Conconi Cesa-Bianchi", "N. Gentile 2004] Cesa-Bianchi", "A. Conconi", "C. Gentile"], "venue": "IEEE Transactions on Information Theory 50(9):2050\u20132057", "citeRegEx": "Cesa.Bianchi et al\\.,? \\Q2004\\E", "shortCiteRegEx": "Cesa.Bianchi et al\\.", "year": 2004}, {"title": "and Wagner", "author": ["L. Devroye"], "venue": "T.", "citeRegEx": "Devroye and Wagner 1979", "shortCiteRegEx": null, "year": 1979}, {"title": "S", "author": ["R. Frostig", "R. Ge", "Kakade"], "venue": "M.; and Sidford, A.", "citeRegEx": "Frostig et al. 2015", "shortCiteRegEx": null, "year": 2015}, {"title": "and Lan", "author": ["S. Ghadimi"], "venue": "G.", "citeRegEx": "Ghadimi and Lan 2013", "shortCiteRegEx": null, "year": 2013}, {"title": "Train faster, generalize better: Stability of stochastic gradient descent", "author": ["Recht Hardt", "M. Singer 2015] Hardt", "B. Recht", "Y. Singer"], "venue": "arXiv preprint arXiv:1509.01240", "citeRegEx": "Hardt et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Hardt et al\\.", "year": 2015}, {"title": "and Zhang", "author": ["R. Johnson"], "venue": "T.", "citeRegEx": "Johnson and Zhang 2013", "shortCiteRegEx": null, "year": 2013}, {"title": "and Tewari", "author": ["S.M. Kakade"], "venue": "A.", "citeRegEx": "Kakade and Tewari 2009", "shortCiteRegEx": null, "year": 2009}, {"title": "and Ron", "author": ["M. Kearns"], "venue": "D.", "citeRegEx": "Kearns and Ron 1999", "shortCiteRegEx": null, "year": 1999}, {"title": "Gradient descent converges to minimizers", "author": ["B. Recht"], "venue": "University of California,", "citeRegEx": "Recht,? \\Q2016\\E", "shortCiteRegEx": "Recht", "year": 2016}, {"title": "Asynchronous parallel stochastic gradient for nonconvex optimization", "author": ["Lian"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Lian,? \\Q2015\\E", "shortCiteRegEx": "Lian", "year": 2015}, {"title": "Learning theory: stability is sufficient for generalization and necessary and sufficient for consistency of empirical risk minimization", "author": ["Mukherjee"], "venue": null, "citeRegEx": "Mukherjee,? \\Q2006\\E", "shortCiteRegEx": "Mukherjee", "year": 2006}, {"title": "and Wright", "author": ["J. Nocedal"], "venue": "S.", "citeRegEx": "Nocedal and Wright 2006", "shortCiteRegEx": null, "year": 2006}, {"title": "Making gradient descent optimal for strongly convex stochastic optimization. arXiv preprint arXiv:1109.5647", "author": ["Shamir Rakhlin", "A. Sridharan 2011] Rakhlin", "O. Shamir", "K. Sridharan"], "venue": null, "citeRegEx": "Rakhlin et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Rakhlin et al\\.", "year": 2011}, {"title": "S", "author": ["Reddi"], "venue": "J.; Hefny, A.; Sra, S.; P\u00f3cz\u00f3s, B.; and Smola, A.", "citeRegEx": "Reddi et al. 2016", "shortCiteRegEx": null, "year": 2016}, {"title": "Learnability, stability and uniform convergence", "author": ["Shalev-Shwartz"], "venue": "Journal of Machine Learning Research 11(Oct):2635\u20132670", "citeRegEx": "Shalev.Shwartz,? \\Q2010\\E", "shortCiteRegEx": "Shalev.Shwartz", "year": 2010}, {"title": "Communication-efficient distributed optimization using an approximate newton-type method", "author": ["Srebro Shamir", "O. Zhang 2014] Shamir", "N. Srebro", "T. Zhang"], "venue": "In ICML,", "citeRegEx": "Shamir et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Shamir et al\\.", "year": 2014}, {"title": "and Kotz", "author": ["V.N. Vapnik"], "venue": "S.", "citeRegEx": "Vapnik and Kotz 1982", "shortCiteRegEx": null, "year": 1982}, {"title": "and Vapnik", "author": ["V.N. Vapnik"], "venue": "V.", "citeRegEx": "Vapnik and Vapnik 1998", "shortCiteRegEx": null, "year": 1998}, {"title": "G", "author": ["Wahba"], "venue": "2000. An introduction to model building with reproducing kernel hilbert spaces. Statistics Department TR", "citeRegEx": "Wahba 2000", "shortCiteRegEx": null, "year": 1020}], "referenceMentions": [], "year": 2016, "abstractText": "Many machine learning tasks can be formulated as Regularized Empirical Risk Minimization (R-ERM), and solved by optimization algorithms such as gradient descent (GD), stochastic gradient descent (SGD), and stochastic variance reduction (SVRG). Conventional analysis on these optimization algorithms focuses on their convergence rates during the training process, however, people in the machine learning community may care more about the generalization performance of the learned model on unseen test data. In this paper, we investigate on this issue, by using stability as a tool. In particular, we decompose the generalization error for R-ERM, and derive its upper bound for both convex and non-convex cases. In convex cases, we prove that the generalization error can be bounded by the convergence rate of the optimization algorithm and the stability of the R-ERM process, both in expectation (in the order of O(1/n) + E\u03c1(T )), where \u03c1(T ) is the convergence error and T is the number of iterations) and in high probability (in the order of O ( log 1/\u03b4 \u221a n + \u03c1(T ) ) with probability 1\u2212 \u03b4). For non-convex cases, we can also obtain a similar expected generalization error bound. Our theorems indicate that 1) along with the training process, the generalization error will decrease for all the optimization algorithms under our investigation; 2) Comparatively speaking, SVRG has better generalization ability than GD and SGD. We have conducted experiments on both convex and non-convex problems, and the experimental results verify our theoretical findings.", "creator": "LaTeX with hyperref package"}}}