{"id": "1609.08435", "review": {"conference": "AAAI", "VERSION": "v1", "DATE_OF_SUBMISSION": "27-Sep-2016", "title": "Asynchronous Stochastic Proximal Optimization Algorithms with Variance Reduction", "abstract": "Regularized empirical risk minimization (R-ERM) is an important branch of machine learning, since it constrains the capacity of the hypothesis space and guarantees the generalization ability of the learning algorithm. Two classic proximal optimization algorithms, i.e., proximal stochastic gradient descent (ProxSGD) and proximal stochastic coordinate descent (ProxSCD) have been widely used to solve the R-ERM problem. Recently, variance reduction technique was proposed to improve ProxSGD and ProxSCD, and the corresponding ProxSVRG and ProxSVRCD have better convergence rate. These proximal algorithms with variance reduction technique have also achieved great success in applications at small and moderate scales. However, in order to solve large-scale R-ERM problems and make more practical impacts, the parallel version of these algorithms are sorely needed. In this paper, we propose asynchronous ProxSVRG (Async-ProxSVRG) and asynchronous ProxSVRCD (Async-ProxSVRCD) algorithms, and prove that Async-ProxSVRG can achieve near linear speedup when the training data is sparse, while Async-ProxSVRCD can achieve near linear speedup regardless of the sparse condition, as long as the number of block partitions are appropriately set. We have conducted experiments on a regularized logistic regression task. The results verified our theoretical findings and demonstrated the practical efficiency of the asynchronous stochastic proximal algorithms with variance reduction.", "histories": [["v1", "Tue, 27 Sep 2016 13:40:00 GMT  (628kb,D)", "http://arxiv.org/abs/1609.08435v1", null]], "reviews": [], "SUBJECTS": "cs.LG", "authors": ["qi meng", "wei chen", "jingcheng yu", "taifeng wang", "zhiming ma", "tie-yan liu"], "accepted": true, "id": "1609.08435"}, "pdf": {"name": "1609.08435.pdf", "metadata": {"source": "CRF", "title": "Asynchronous Stochastic Proximal Optimization Algorithms with Variance Reduction", "authors": ["Qi Meng", "Wei Chen", "Jingcheng Yu", "Taifeng Wang", "Zhi-Ming Ma", "Tie-Yan Liu"], "emails": ["qimeng13@pku.edu.cn", "tie-yan.liu}@microsoft.com", "JingchengYu.94@gmail.com", "mazm@amt.ac.cn"], "sections": [{"heading": "1 Introduction", "text": "In fact, it is so that most of them are able to survive themselves if they do not see themselves able to survive themselves. In fact, it is so that they are able to survive themselves and that they are able to survive themselves. In fact, it is so that they are able to survive themselves and that they are able to survive themselves. In fact, it is so that they are able to survive themselves and that they are able to survive themselves. In fact, it is so that they are able to survive themselves and survive themselves."}, {"heading": "3 Asynchronous Proximal Algorithms with Variance Reduction", "text": "This year, it is only a matter of time before an agreement is reached."}, {"heading": "4 Convergence Analysis", "text": "In this section we will show the convergence rates of asynchronous parallel algorithms with variance reduction (in this section we will introduce the following assumptions used in the theoretical analysis for asynchronous parallel algorithms (Recht et al. 2011; Reddi et al. 2015). Assumption 1: (convexity) F (x) and R (x) are very common in asynchronous parallel algorithms (Recht et al.). Assumption 1: (convexity) F (x) and R (x) are convex and R (x) is blockable. The objective function P (x) is strongly convex, i.e., we have P (y) P (x): convexity P (x)."}, {"heading": "5 Experiments", "text": "This year it is more than ever before."}], "references": [{"title": "J", "author": ["A. Agarwal", "Duchi"], "venue": "C.", "citeRegEx": "Agarwal and Duchi 2011", "shortCiteRegEx": null, "year": 2011}, {"title": "Q", "author": ["J. Dean", "G. Corrado", "R. Monga", "K. Chen", "M. Devin", "M. Mao", "A. Senior", "P. Tucker", "K. Yang", "Le"], "venue": "V.; et al.", "citeRegEx": "Dean et al. 2012", "shortCiteRegEx": null, "year": 2012}, {"title": "H", "author": ["Feyzmahdavian"], "venue": "R.; Aytekin, A.; and Johansson, M.", "citeRegEx": "Feyzmahdavian. Aytekin. and Johansson 2015", "shortCiteRegEx": null, "year": 2015}, {"title": "and Zhang", "author": ["R. Johnson"], "venue": "T.", "citeRegEx": "Johnson and Zhang 2013", "shortCiteRegEx": null, "year": 2013}, {"title": "Sparse online learning via truncated gradient", "author": ["Li Langford", "J. Zhang 2009] Langford", "L. Li", "T. Zhang"], "venue": null, "citeRegEx": "Langford et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Langford et al\\.", "year": 2009}, {"title": "Asynchronous parallel stochastic gradient for nonconvex optimization", "author": ["Lian"], "venue": null, "citeRegEx": "Lian,? \\Q2015\\E", "shortCiteRegEx": "Lian", "year": 2015}, {"title": "S", "author": ["J. Liu", "Wright"], "venue": "J.", "citeRegEx": "Liu and Wright 2015", "shortCiteRegEx": null, "year": 2015}, {"title": "S", "author": ["Liu, J.", "Wright"], "venue": "J.; R\u00e9, C.; Bittorf, V.; and Sridhar, S.", "citeRegEx": "Liu et al. 2013", "shortCiteRegEx": null, "year": 2013}, {"title": "S", "author": ["Liu, J.", "Wright"], "venue": "J.; and Sridhar, S.", "citeRegEx": "Liu. Wright. and Sridhar 2014", "shortCiteRegEx": null, "year": 2014}, {"title": "M", "author": ["H. Mania", "X. Pan", "D. Papailiopoulos", "B. Recht", "K. Ramchandran", "Jordan"], "venue": "I.", "citeRegEx": "Mania et al. 2015", "shortCiteRegEx": null, "year": 2015}, {"title": "Making gradient descent optimal for strongly convex stochastic optimization. arXiv preprint arXiv:1109.5647", "author": ["Shamir Rakhlin", "A. Sridharan 2011] Rakhlin", "O. Shamir", "K. Sridharan"], "venue": null, "citeRegEx": "Rakhlin et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Rakhlin et al\\.", "year": 2011}, {"title": "Hogwild: A lock-free approach to parallelizing stochastic gradient descent", "author": ["Recht"], "venue": null, "citeRegEx": "Recht,? \\Q2011\\E", "shortCiteRegEx": "Recht", "year": 2011}, {"title": "A", "author": ["S.J. Reddi", "A. Hefny", "S. Sra", "B. P\u00f3czos", "Smola"], "venue": "J.", "citeRegEx": "Reddi et al. 2015", "shortCiteRegEx": null, "year": 2015}, {"title": "and Tewari", "author": ["S. Shalev-Shwartz"], "venue": "A.", "citeRegEx": "Shalev.Shwartz and Tewari 2011", "shortCiteRegEx": null, "year": 2011}, {"title": "Communication-efficient distributed optimization using an approximate newton-type method", "author": ["Srebro Shamir", "O. Zhang 2014] Shamir", "N. Srebro", "T. Zhang"], "venue": "In Proceedings of the 31st International Conference on Machine Learning", "citeRegEx": "Shamir et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Shamir et al\\.", "year": 2014}, {"title": "Scaling up stochastic dual coordinate ascent", "author": ["Tran"], "venue": "In Proceedings of the 21th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining,", "citeRegEx": "Tran,? \\Q2015\\E", "shortCiteRegEx": "Tran", "year": 2015}, {"title": "S", "author": ["Wright"], "venue": "J.", "citeRegEx": "Wright 2015", "shortCiteRegEx": null, "year": 2015}, {"title": "and Zhang", "author": ["L. Xiao"], "venue": "T.", "citeRegEx": "Xiao and Zhang 2014", "shortCiteRegEx": null, "year": 2014}, {"title": "Accelerated mini-batch randomized block coordinate descent method", "author": ["Zhao"], "venue": null, "citeRegEx": "Zhao,? \\Q2014\\E", "shortCiteRegEx": "Zhao", "year": 2014}, {"title": "2015), where constant \u03b70 and \u03c30 specify the scale and speed of decay", "author": ["t+\u03c30 (Reddi"], "venue": null, "citeRegEx": ".Reddi,? \\Q2015\\E", "shortCiteRegEx": ".Reddi", "year": 2015}], "referenceMentions": [{"referenceID": 0, "context": "References [Agarwal and Duchi 2011] Agarwal, A.", "startOffset": 11, "endOffset": 35}, {"referenceID": 1, "context": "[Dean et al. 2012] Dean, J.", "startOffset": 0, "endOffset": 18}, {"referenceID": 2, "context": "[Feyzmahdavian, Aytekin, and Johansson 2015] Feyzmahdavian, H.", "startOffset": 0, "endOffset": 44}, {"referenceID": 3, "context": "[Johnson and Zhang 2013] Johnson, R.", "startOffset": 0, "endOffset": 24}, {"referenceID": 6, "context": "[Liu and Wright 2015] Liu, J.", "startOffset": 0, "endOffset": 21}, {"referenceID": 7, "context": "[Liu et al. 2013] Liu, J.", "startOffset": 0, "endOffset": 17}, {"referenceID": 8, "context": "[Liu, Wright, and Sridhar 2014] Liu, J.", "startOffset": 0, "endOffset": 31}, {"referenceID": 9, "context": "[Mania et al. 2015] Mania, H.", "startOffset": 0, "endOffset": 19}, {"referenceID": 12, "context": "[Reddi et al. 2015] Reddi, S.", "startOffset": 0, "endOffset": 19}, {"referenceID": 13, "context": "[Shalev-Shwartz and Tewari 2011] Shalev-Shwartz, S.", "startOffset": 0, "endOffset": 32}, {"referenceID": 16, "context": "[Wright 2015] Wright, S.", "startOffset": 0, "endOffset": 13}, {"referenceID": 17, "context": "[Xiao and Zhang 2014] Xiao, L.", "startOffset": 0, "endOffset": 21}], "year": 2016, "abstractText": "Regularized empirical risk minimization (R-ERM) is an important branch of machine learning, since it constrains the capacity of the hypothesis space and guarantees the generalization ability of the learning algorithm. Two classic proximal optimization algorithms, i.e., proximal stochastic gradient descent (ProxSGD) and proximal stochastic coordinate descent (ProxSCD) have been widely used to solve the R-ERM problem. Recently, variance reduction technique was proposed to improve ProxSGD and ProxSCD, and the corresponding ProxSVRG and ProxSVRCD have better convergence rate. These proximal algorithms with variance reduction technique have also achieved great success in applications at small and moderate scales. However, in order to solve large-scale RERM problems and make more practical impacts, the parallel version of these algorithms are sorely needed. In this paper, we propose asynchronous ProxSVRG (Async-ProxSVRG) and asynchronous ProxSVRCD (Async-ProxSVRCD) algorithms, and prove that Async-ProxSVRG can achieve near linear speedup when the training data is sparse, while AsyncProxSVRCD can achieve near linear speedup regardless of the sparse condition, as long as the number of block partitions are appropriately set. We have conducted experiments on a regularized logistic regression task. The results verified our theoretical findings and demonstrated the practical efficiency of the asynchronous stochastic proximal algorithms with variance reduction.", "creator": "LaTeX with hyperref package"}}}