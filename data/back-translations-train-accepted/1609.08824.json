{"id": "1609.08824", "review": {"conference": "acl", "VERSION": "v1", "DATE_OF_SUBMISSION": "28-Sep-2016", "title": "Equation Parsing: Mapping Sentences to Grounded Equations", "abstract": "Identifying mathematical relations expressed in text is essential to understanding a broad range of natural language text from election reports, to financial news, to sport commentaries to mathematical word problems. This paper focuses on identifying and understanding mathematical relations described within a single sentence. We introduce the problem of Equation Parsing -- given a sentence, identify noun phrases which represent variables, and generate the mathematical equation expressing the relation described in the sentence. We introduce the notion of projective equation parsing and provide an efficient algorithm to parse text to projective equations. Our system makes use of a high precision lexicon of mathematical expressions and a pipeline of structured predictors, and generates correct equations in $70\\%$ of the cases. In $60\\%$ of the time, it also identifies the correct noun phrase $\\rightarrow$ variables mapping, significantly outperforming baselines. We also release a new annotated dataset for task evaluation.", "histories": [["v1", "Wed, 28 Sep 2016 08:54:05 GMT  (190kb)", "http://arxiv.org/abs/1609.08824v1", null]], "reviews": [], "SUBJECTS": "cs.CL", "authors": ["subhro roy", "shyam upadhyay", "dan roth"], "accepted": true, "id": "1609.08824"}, "pdf": {"name": "1609.08824.pdf", "metadata": {"source": "CRF", "title": null, "authors": [], "emails": ["danr}@illinois.edu"], "sections": [{"heading": null, "text": "ar Xiv: 160 9.08 824v 1 [cs.C L] 28 Identifying mathematical relationships expressed in texts is essential for understanding a wide range of natural language texts, from election reports to financial news to sports commentary and mathematical word problems. This essay focuses on identifying and understanding mathematical relationships described within a sentence. We introduce the problem of equation parsing - a sentence given, identify noun phrases that represent variables, and generate the mathematical equation that expresses the relationship described in the sentence. We introduce the concept of parsing projective equations and provide an efficient algorithm for parsing text for projective equations. Our system uses a high-precision lexicon of mathematical expressions and a pipeline of structured predictors, generating correct equations in 70% of the cases. In 60% of the time it also identifies the correct noun phrase \u2192 conceptual lines, which clearly exceeds the base order."}, {"heading": "1 Introduction", "text": "This year, it will only take one year for an agreement to be reached."}, {"heading": "2 Related Work", "text": "The work that has the most to do with this work is (Madaan et al., 2016), which focuses on extracting relation triples where one of the arguments is a number. In contrast, our work deals with several variables and complex equations that incorporate them. Recently, there has been much work on automatic mathematical solving of word problems (Kushman et al., 2014; Roy et al., 2015; Hosseini et al., 2014; Roy and Roth, 2015) These solvers cannot handle sentences individually; they require input to be a complete mathematical word problem, and even then they focus only on retrieving a series of response values without mentioning which corresponds to each response value. Our work is also conceptually related to work on semantic parsing - the allocation of text of natural language to a formal representation of meaning (Wong and Mooney, 2007; Clarke et al., 2010 Ckowski and Roth, 2013; Roth, 2011)."}, {"heading": "3 The Equation Parsing Task", "text": "s use N as a set of noun phrases in sentence x.The output of the problem is the mathematical equation described in x, together with a mapping of each variable in the equation to the corresponding noun sentence in n. In this illustration we are talking about the \"grounding\" of the variables; the noun phrase represents what the variable in the equation stands for. Table 1 gives an example of the input and output of the equation when parsing the text in Example 2. Since an equation can be written in different forms, we use the form that most corresponds to text as the target output. For example, we choose 1 = 3 \u00d7 V2 and not V2 = V1 \u2212 3. In cases where multiple equation forms seem equally likely to be the target equation, we randomly select one of them and keep this choice consistent throughout the set."}, {"heading": "3.1 Equation Parse Representation", "text": "In this section, we present an equation axis for a set. \"An equation of a set x is a pair (T, E) in which T represents a set of triggers extracted from x, and E represents an equation tree formed with the set T as the trigger. We will describe these terms in detail.\" Trigger \"is a sentence that mentions a mathematical relationship, a trigger can be either a quantity list expressed in x, or a variable trigger set corresponding to a variable.\" A quantitative trigger is a tuple (q, s), where q is the numerical value of the quantity mentioned in the text, and s is the range of text x that refers to quantity. \"Variable trigger is a tuple (l, s) where l represents the marker of the variable, and s represents the variable phrase that represents the variable.\""}, {"heading": "4 Projectivity", "text": "For each leaf n of an equation tree T, we define a function Location (\u00b7) to specify the position of the corresponding trigger in the text. We also define it for each node n of the equation tree T, functions Span-Start (n) and Span-End (n) to specify the minimum span of the text containing the leaves of the subtree that are rooted at n. We define it as follows: Span-Start (n) = Location (n) if n is a Leafmin (Span-Start (lc (n))), Span-Start (rc (n)))), otherwise (2) Span-End (n) = Location (n) if n is a Leafmax (Span-End (lc (n)))), Span-End (rc (n)))))))), otherwise (3) An equation tree T is denoted as a projective equation iff for each node n, either Span-End (n)."}, {"heading": "5 Predicting Equation Parse", "text": "When analyzing a set of equations, three components are predicted - Quantity Trigger List, Variable Trigger List, and Equation Tree. We develop three structured prediction modules to predict each of the above components. All of our prediction modules take a similar form: given input x and output y, we learn a scoring function fw (x, y), which evaluates how likely the output y given input x. The scoring function fw (x, y) is linear, fw (y) = wT\u03c6 (x, y), where \u03c6 (x, y) is a feature vector extracted from x and y. The inference problem, i.e. the prediction y \u0445 for an input x is then: y \u0445 = argmaxy Y fw (y), where Y is the set of all allowed values of y."}, {"heading": "5.1 Predicting Quantity Trigger List", "text": "In view of the input text and the quantities mentioned therein, the role of this step is to determine for each quantity in the text whether it should be part of the final equation. For example, in Example 5 in Section 1, both \"5\" and \"10\" are irrelevant to the final equation \"V1 + V2 = 54.\" Likewise, in Example 4, the number \"two\" is irrelevant to the equation \"V1 + V2 = 80.\" We define for each quantity q in the sentence a Boolean value relevance (q), which is set to true if q is relevant to the final equation, and to wrong otherwise. For structured classification, the input x of the sentence together with a number of quantities recognized therein, and the output y is the relevance value for all quantities in the sentence. We have empirically found that the prediction of all the relevance values collectively is better than the prediction of a quantitative feature (the quantitative characteristics used for each characteristic)."}, {"heading": "5.2 Predicting Variable Trigger List", "text": "The goal of this step is to predict the variable trigger list for the equation. Our structured classifier takes the set x as input, and the output y is either one or two noun phrases representing variables in the final equation. As we have previously shown, several principles could be valid for each given variable, so there may be several valid trigger lists. For each set x, we construct a set of Y valid initial structures. Each element in Y corresponds to a valid trigger list. Finally, we aim to output only one of the elements of Y. We have modified the standard structured prediction algorithms to consider \"superset supervision\" and consider several gold structures for an input x. We assume that access to N training examples of the form: (x1, Y1), (x2, Y2), (xN, YN), where each Yi is a set of starting data for the xi."}, {"heading": "5.3 Predicting Equation Tree", "text": "It is natural to assume that the syntactic part of the set could be very useful for making all the predictions we parse in the equation. However, it turns out that this is not the case - large parts of the syntactic set will not be part of the equation, so we need the above modules to address them. However, in the next task of predicting the equation tree we have tried to restrict the output space using the syntactic tree; we have found that even enforcing this weak level of expectation is not productive."}, {"heading": "5.4 Alternatives", "text": "A natural approach might be to learn together to predict all three components, to grasp the dependencies between them. To investigate this, we developed a structured SVM that predicts all components together by combining the characteristics of each component. We use approximate conclusions by first enumerating possible trigger lists and then equation trees and finding the best scoring structure. However, this method has not surpassed the pipeline method; the poorer performance of joint learning is due to the fact that (1) the search space for the common model is too large to do well given our record size of 385, and (2) that our independent classifiers are good enough to support better joint inference, a trade-off that is strongly supported in the literature (Punyakanok et al., 2005; Sutton and McCallum, 2007). Another option is to impose limitations between trigger list predictions, such as that triggers should not overlap with quantity triggers."}, {"heading": "6 Experimental Results", "text": "We now describe the dataset and the annotation method used. We then evaluate the performance of the system in predicting the trigger list, the equation tree, and the complete equation analysis."}, {"heading": "6.1 Dataset", "text": "We created a new dataset that consisted of 385 sentences extracted from algebra word problems and financial news headlines. For algebra word problems, we used the MIT dataset (Kushman et al., 2014) and two high school math textbooks, Elementary Algebra (College of Redwoods) and Beginning and Intermediate Algebra (Tyler Wallace). Financial news headlines were extracted from MarketWatch's The Latest News Feed during February 2015. All sentences of information describing a mathematical relationship between at most two (possibly Korean) variables were selected. Next, we edited sentences that require a multiple use of a number to create the equation, removing only a few time-related sentences, such as \"In 10 years, John will be twice as old as his son.\" We found empirically that about 97% of sentences describing a reformulation fell within the scope of our dataset's normalized anators were paired with the notation."}, {"heading": "6.2 Equation Parsing Modules", "text": "In this section, we evaluate the performance of the individual modules of the equation analysis process. We report on accuracy - a fraction of the correct predictions. Table 3 shows the 5-fold cross-validation accuracy of the various modules. In any case, we also report on accuracy by removing each feature group individually. In addition, we also show the effect of lexicon, projectivity, agreement with syntactic parse constraints, and the use of lexicon as a feature instead of hard constraints for predicting the equation tree. For all our experiments, we use the Stanford Parser (Socher et al., 2013), the Illinois POS Tagger (Roth and Zelenko, 1998), and the Illinois-SL Structured Prediction Package (Chang et al., 2015)."}, {"heading": "6.3 Equation Parsing Results", "text": "In this section, we evaluate the performance of our system using the general equation analysis task. We report on equation accuracy - the fraction of the sentences for which the system corrected the equation, and Equation + Grounding Accuracy - the fraction of sentences for which the system correctly got both the equation and the grounding of the variables. Table 4 shows the overall performance of our system on a 5-fold cross-validation. We compare against Joint Learning - a system that collectively learns to predict all relevant components of an equation parcel (Section 5.4). We also compare with SPF (Artzi and Zettlemoyer, 2013), a publicly available semantic parser that can learn from sentence form pairs. We train SPF with sentence equation pairs and a seed lexicon for mathematical terms (similar to ours) and report on equation accuracy. Our structured predictor pipeline approach is superior to both common learning and SPF predictions. SPF solves only a few sentences, with SPS equations connected."}, {"heading": "6.4 Error Analysis", "text": "When predicting variable trigger lists, about 25% of the errors were due to the predictor selecting a range within the correct range, for example, when the target term is \"the cost of a children's ticket,\" our predictor only chose \"children's ticket.\" Although this may be sufficient for downstream tasks, we consider it incorrect in our current assessment. Another 25% of the errors were due to the selection of units that are not involved in the relationship. In \"A cattle breeder breeds five times as many cows as horses,\" for example, our predictor chose \"a cattle breeder\" and \"cows\" as variables, whereas the relationship between \"cows\" and \"horses\" exists. In predicting the equation tree, we found that 35% of the errors were due to rare mathematical concepts expressed in the text. For example, \"$7 below the price\" $7 should be subtracted from the price."}, {"heading": "7 Conclusion", "text": "This paper examines methods of identifying and understanding mathematical relationships expressed in the text. We introduce the equation analysis task, which involves creating an equation from a set and figuring out what the variables represent. We define the concept of projectivity and construct a high-precision lexicon and use it to reduce the equation search space. Our experimental results are quite satisfactory and raise a few interesting questions. In particular, it suggests that predicting equation analyses using a pipeline of structured predictors works better than jointly trained alternatives, and as discussed, it also demonstrates the limitation of the current NLP tools to support these tasks. Our current formulation has one crucial limitation: We will only deal with expressions described within a set. Our future work will focus on removing this limitation to express relationships across multiple sets and multiple relationships expressed in the same set. Code and dataset are available athttpage / cedpage / comp.co.u / gil.info /.info /.cedu /.cil.view /.cil."}, {"heading": "Acknowledgements", "text": "This work is funded by DARPA under contract number FA8750-13-2-0008 and funded by the Allen Institute for Artificial Intelligence (allenai.org)."}, {"heading": "A Features", "text": "A.1 Quantity Trigger List PredictionThe feature function \u03c6 (x, y) used for the classification generates the following features: 1. Neighborhood features: For each quantity q in the input sentence, we add unigrams and bigrams generate from a window around q, part of speech tags of neighbor tokens of q. We associate these features with relevance (q).2. Quantity features: For each quantity q, we add unigrams and bigrams of the phrase represent the quantity. We also add a feature that indicates whether the number is associated with the number one or two, and whether it is the only number in the sentence. These features are also associated with relevance (q).A.2 Variable Trigger List PredictionThe features \u03c6 (x, y) used for variable trigger prediction: 1. Variable features: 1. Variable features: Unigrams and bigrams generated from the nouphrase."}, {"heading": "B Annotation Guidelines", "text": "For each variable in the equation, commentators were asked to mark ranges of text that best describe what the variable represents, and they were asked to comment on associated units if there was no exact variable description. For example, in Example 3 (Section 1), the relationship between the speed of the bird and the speed of the wind was kept. However, \"speed\" is not explicitly mentioned in the sentence. In such cases, commentators were asked to comment on the associated units \"the wind\" and \"a bird\" as variable, and the guidelines also instructed commentators to select the longest possible mention if they felt the mention limit was unclear. Consequently, in the sentence \"City Rentals Hire a Medium-Size Car for $18.95 plus $0.21 per Mile,\" the phrase \"City Rentals a Medium-Size Car\" was marked as variable."}, {"heading": "C Lexicon", "text": "We construct a high-precision list of rules to analyze sentences that describe mathematical concepts, such as \"difference of,\" \"greater than,\" etc. If each non-leaf node contains n of a projective equation tree, we define the following terms: 1. MidSpan (n): The string from min (span-end (lc (n)), span-end (rc (n))) to max (span-start (lc (n)), span-start (rc (n)), span-start (rc (n))), span-start (n)) and from the closest trigger position on the left.3. RightSpan (n): The string of max (span-end (lc (n), span-end (n), span-start (n), should. \""}], "references": [{"title": "Learning structured perceptrons for coreference resolution with latent antecedents and non-local features", "author": ["Bj\u00f6rkelund", "Kuhn2014] Anders Bj\u00f6rkelund", "Jonas Kuhn"], "venue": "In Proceedings of the 52nd Annual Meeting of the Association", "citeRegEx": "Bj\u00f6rkelund et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Bj\u00f6rkelund et al\\.", "year": 2014}, {"title": "Semantic Parsing Freebase: Towards Opendomain Semantic Parsing", "author": ["Cai", "Yates2013] Qingqing Cai", "Alexander Yates"], "venue": "In Proceedings of the Second Joint Conference on Lexical and Computational Semantics (*SEM)", "citeRegEx": "Cai et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Cai et al\\.", "year": 2013}, {"title": "Illinoissl: A JAVA library for structured prediction", "author": ["Chang et al.2015] Kai-Wei Chang", "Shyam Upadhyay", "Ming-Wei Chang", "Vivek Srikumar", "Dan Roth"], "venue": "In Arxiv Preprint,", "citeRegEx": "Chang et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Chang et al\\.", "year": 2015}, {"title": "Driving semantic parsing from the world\u2019s response", "author": ["Clarke et al.2010] J. Clarke", "D. Goldwasser", "M. Chang", "D. Roth"], "venue": "In Proc. of the Conference on Computational Natural Language Learning (CoNLL),", "citeRegEx": "Clarke et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Clarke et al\\.", "year": 2010}, {"title": "Learning from natural instructions", "author": ["Goldwasser", "Roth2011] D. Goldwasser", "D. Roth"], "venue": "In Proc. of the International Joint Conference on Artificial Intelligence (IJCAI)", "citeRegEx": "Goldwasser et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Goldwasser et al\\.", "year": 2011}, {"title": "Learning to solve arithmetic word problems with verb categorization", "author": ["Hannaneh Hajishirzi", "Oren Etzioni", "Nate Kushman"], "venue": "In Proc. of the Conference on Empirical Methods in Natural Language Process-", "citeRegEx": "Hosseini et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Hosseini et al\\.", "year": 2014}, {"title": "Learning to automatically solve algebra word problems", "author": ["Kushman et al.2014] N. Kushman", "L. Zettlemoyer", "R. Barzilay", "Y. Artzi"], "venue": null, "citeRegEx": "Kushman et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Kushman et al\\.", "year": 2014}, {"title": "Scaling semantic parsers with on-the-fly ontology matching", "author": ["Eunsol Choi", "Yoav Artzi", "Luke Zettlemoyer"], "venue": "In Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing", "citeRegEx": "Kwiatkowski et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Kwiatkowski et al\\.", "year": 2013}, {"title": "Numerical relation extraction with minimal supervision", "author": ["Madaan et al.2016] A. Madaan", "A. Mittal", "Mausam", "G. Ramakrishnan", "S. Sarawagi"], "venue": "In Proc. of the Conference on Artificial Intelligence (AAAI)", "citeRegEx": "Madaan et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Madaan et al\\.", "year": 2016}, {"title": "Nonprojective dependency parsing using spanning tree algorithms", "author": ["Fernando Pereira", "Kiril Ribarov", "Jan Haji\u010d"], "venue": "In Proceedings of the Conference on Human Language Technology and Empirical Methods", "citeRegEx": "McDonald et al\\.,? \\Q2005\\E", "shortCiteRegEx": "McDonald et al\\.", "year": 2005}, {"title": "Learning and inference over constrained output", "author": ["D. Roth", "W. Yih", "D. Zimak"], "venue": "In Proc. of the International Joint Conference on Artificial Intelligence (IJCAI),", "citeRegEx": "Punyakanok et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Punyakanok et al\\.", "year": 2005}, {"title": "Part of speech tagging using a network of linear separators", "author": ["Roth", "Zelenko1998] D. Roth", "D. Zelenko"], "venue": "In Coling-Acl, The 17th International Conference on Computational Linguistics,", "citeRegEx": "Roth et al\\.,? \\Q1998\\E", "shortCiteRegEx": "Roth et al\\.", "year": 1998}, {"title": "Solving general arithmetic word problems", "author": ["Roy", "Roth2015] S. Roy", "D. Roth"], "venue": "In Proc. of the Conference on Empirical Methods in Natural Language Processing (EMNLP)", "citeRegEx": "Roy et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Roy et al\\.", "year": 2015}, {"title": "Reasoning about quantities in natural language", "author": ["Roy et al.2015] S. Roy", "T. Vieira", "D. Roth"], "venue": "Transactions of the Association for Computational Linguistics,", "citeRegEx": "Roy et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Roy et al\\.", "year": 2015}, {"title": "Parsing With Compositional Vector Grammars", "author": ["John Bauer", "Christopher D. Manning", "Andrew Y. Ng"], "venue": null, "citeRegEx": "Socher et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Socher et al\\.", "year": 2013}, {"title": "Piecewise pseudolikelihood for efficient training of conditional random fields", "author": ["Sutton", "McCallum2007] C. Sutton", "A. McCallum"], "venue": "In Zoubin Ghahramani, editor, Proceedings of the International Conference on Machine Learning (ICML),", "citeRegEx": "Sutton et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Sutton et al\\.", "year": 2007}, {"title": "Learning synchronous grammars for semantic parsing with lambda calculus", "author": ["Wong", "Mooney2007] Y.-W. Wong", "R. Mooney"], "venue": "In Proceedings of the Annual Meeting of the Association for Computational Linguistics (ACL),", "citeRegEx": "Wong et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Wong et al\\.", "year": 2007}], "referenceMentions": [{"referenceID": 7, "context": "The task can be seen as a form of semantic parsing (Goldwasser and Roth, 2011; Kwiatkowski et al., 2013) where instead of mapping a sentence to a logical form, we want to map", "startOffset": 51, "endOffset": 104}, {"referenceID": 8, "context": "The work most related to this paper is (Madaan et al., 2016), which focuses on extracting relation triples where one of the arguments is a number.", "startOffset": 39, "endOffset": 60}, {"referenceID": 6, "context": "There has been a lot of recent work in automatic math word problem solving (Kushman et al., 2014; Roy et al., 2015; Hosseini et al., 2014; Roy and Roth, 2015).", "startOffset": 75, "endOffset": 158}, {"referenceID": 12, "context": "There has been a lot of recent work in automatic math word problem solving (Kushman et al., 2014; Roy et al., 2015; Hosseini et al., 2014; Roy and Roth, 2015).", "startOffset": 75, "endOffset": 158}, {"referenceID": 5, "context": "There has been a lot of recent work in automatic math word problem solving (Kushman et al., 2014; Roy et al., 2015; Hosseini et al., 2014; Roy and Roth, 2015).", "startOffset": 75, "endOffset": 158}, {"referenceID": 3, "context": "Our work is also conceptually related to work on semantic parsing \u2013 mapping natural language text to a formal meaning representation (Wong and Mooney, 2007; Clarke et al., 2010; Cai and Yates, 2013; Kwiatkowski et al., 2013; Goldwasser and Roth, 2011).", "startOffset": 133, "endOffset": 251}, {"referenceID": 7, "context": "Our work is also conceptually related to work on semantic parsing \u2013 mapping natural language text to a formal meaning representation (Wong and Mooney, 2007; Clarke et al., 2010; Cai and Yates, 2013; Kwiatkowski et al., 2013; Goldwasser and Roth, 2011).", "startOffset": 133, "endOffset": 251}, {"referenceID": 9, "context": "This is more general than the definition of projective trees used in dependency parsing (McDonald et al., 2005).", "startOffset": 88, "endOffset": 111}, {"referenceID": 10, "context": "This tradeoff is strongly supported in the literature (Punyakanok et al., 2005; Sutton and McCallum, 2007).", "startOffset": 54, "endOffset": 106}, {"referenceID": 6, "context": "For algebra word problems, we used the MIT dataset (Kushman et al., 2014), and two high school mathematics textbooks, Elementary Algebra (College of Redwoods) and Beginning and Intermediate Algebra (Tyler Wallace).", "startOffset": 51, "endOffset": 73}, {"referenceID": 14, "context": "For all our experiments, we use the Stanford Parser (Socher et al., 2013), the Illinois POS tagger (Roth and Zelenko, 1998) and the Illinois-SL structured prediction package (Chang et al.", "startOffset": 52, "endOffset": 73}, {"referenceID": 2, "context": ", 2013), the Illinois POS tagger (Roth and Zelenko, 1998) and the Illinois-SL structured prediction package (Chang et al., 2015).", "startOffset": 108, "endOffset": 128}], "year": 2016, "abstractText": "Identifying mathematical relations expressed in text is essential to understanding a broad range of natural language text from election reports, to financial news, to sport commentaries to mathematical word problems. This paper focuses on identifying and understanding mathematical relations described within a single sentence. We introduce the problem of Equation Parsing \u2013 given a sentence, identify noun phrases which represent variables, and generate the mathematical equation expressing the relation described in the sentence. We introduce the notion of projective equation parsing and provide an efficient algorithm to parse text to projective equations. Our system makes use of a high precision lexicon of mathematical expressions and a pipeline of structured predictors, and generates correct equations in 70% of the cases. In 60% of the time, it also identifies the correct noun phrase \u2192 variables mapping, significantly outperforming baselines. We also release a new annotated dataset for task evaluation.", "creator": "LaTeX with hyperref package"}}}