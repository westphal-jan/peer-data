{"id": "1609.09106", "review": {"conference": "iclr", "VERSION": "v1", "DATE_OF_SUBMISSION": "27-Sep-2016", "title": "HyperNetworks", "abstract": "This work explores hypernetworks: an approach of using a small network, also known as a hypernetwork, to generate the weights for a larger network. Hypernetworks provide an abstraction that is similar to what is found in nature: the relationship between a genotype - the hypernetwork - and a phenotype - the main network. Though they are also reminiscent of HyperNEAT in evolution, our hypernetworks are trained end-to-end with backpropagation and thus are usually faster. The focus of this work is to make hypernetworks useful for deep convolutional networks and long recurrent networks, where hypernetworks can be viewed as relaxed form of weight-sharing across layers. Our main result is that hypernetworks can generate non-shared weights for LSTM and achieve state-of-art results on a variety of language modeling tasks with Character-Level Penn Treebank and Hutter Prize Wikipedia datasets, challenging the weight-sharing paradigm for recurrent networks. Our results also show that hypernetworks applied to convolutional networks still achieve respectable results for image recognition tasks compared to state-of-the-art baseline models while requiring fewer learnable parameters.", "histories": [["v1", "Tue, 27 Sep 2016 05:57:00 GMT  (4158kb,D)", "http://arxiv.org/abs/1609.09106v1", null], ["v2", "Thu, 27 Oct 2016 02:04:56 GMT  (2538kb,D)", "http://arxiv.org/abs/1609.09106v2", null], ["v3", "Fri, 28 Oct 2016 00:28:32 GMT  (2538kb,D)", "http://arxiv.org/abs/1609.09106v3", null], ["v4", "Thu, 1 Dec 2016 10:08:15 GMT  (2564kb,D)", "http://arxiv.org/abs/1609.09106v4", null]], "reviews": [], "SUBJECTS": "cs.LG", "authors": ["david ha", "rew dai", "quoc v le"], "accepted": true, "id": "1609.09106"}, "pdf": {"name": "1609.09106.pdf", "metadata": {"source": "CRF", "title": null, "authors": ["David Ha", "Andrew Dai", "Quoc V. Le"], "emails": ["hadavid@google.com", "adai@google.com", "qvl@google.com"], "sections": [{"heading": "1 INTRODUCTION", "text": "In this paper, we consider an approach of using a small network (called the \"hypernetwork\") to generate the weights for a larger network (called the main network); the behavior of the main network is the same with any usual neural network: it learns to assign some raw input to its desired targets; whereas the hypernetwork takes a series of inputs that contain information about the structure of the weights and generate the weight for that layer (see Figure 1). Figure 1: A hypernetwork generates the weights for a preceding network. Black connections and parameters are assigned to the main network, whereas orange connections and parameters are associated with the hypernetwork. HyperNEAT (Stanley et al., 2009) is an example of a hypernetwork in which the inputs are virtual coordinates for each weight in the main network. In this work, we focus on a more powerful approach in which the input of an embedded network is specified as the total weight of a given layer."}, {"heading": "2 RELATED WORK", "text": "The concept of using a neural network to generate weights for another neural network has its origins in the evolutionary computation. It is often difficult for evolutionary algorithms to work directly in large search spaces consisting of millions of weight parameters. A more efficient method is to first develop a smaller network to generate the structure of weights for a larger network, and to search within the much smaller parameterized weight space. An example of this is the work on Hypercubebased NEAT or HyperNEAT Framework (Stanley et al., 2009). Within the framework of HyperNEAT, compositional Pattern-Producing Networks (CPPNs) are developed to define the weight structure of a much larger main network. Closely related to our approach is a simplified variant of HyperNEAT, in which the structure is fixed and the weights are developed by Discrete Cosine Cosine Transform."}, {"heading": "3 METHODS", "text": "Although hypernetworks can be used to learn weight distribution within a layer in a fully connected network, the focus of this work is to make hypernetworks useful for very deep revolutionary networks and very long recurrent networks. In this context, we consider revolutionary networks and recurrent networks to be two ends of the spectrum. On the one hand, recurrent networks can be considered multi-layer weight distribution, making them inflexible and difficult to learn due to dwindling gradients. On the other hand, revolutionary networks enjoy the flexibility of not having weight distribution at the expense of many parameters when the networks are deep. Hypernetworks can be considered a form of relaxed weight distribution and thus find a balance between the two ends."}, {"heading": "3.1 STATIC HYPERNETWORK: A WEIGHT FACTORIZATION APPROACH FOR DEEP CONVOLUTIONAL NETWORKS", "text": "This year is the highest in the history of the country."}, {"heading": "3.2 DYNAMIC HYPERNETWORK: ADAPTIVE WEIGHT GENERATION FOR RECURRENT NETWORKS", "text": "In the previous section, we outlined a method of using a single hypernetwork to generate weights for a deep revolutionary network. In extreme cases, we can force zj to be identical across layers, thereby imposing a hard weight distribution across all layers, which is often found in recursive networks. Therefore, when used for recurrent networks, hypernetworks can be considered a form of relaxed weight distribution, which is a compromise between hard weight distribution and no weight distribution. While weight distribution reduces the number of model parameters, it can also limit the expressivity of the model. The relaxed weight distribution method via hypernetworks allows us to control the trade between the number of model parameters and the expressiveness of the model. In static hypernet models, a vector zj is learned during training and changes during inference, and we do not want to create a neperdynamic for each step, by which we can also explore the indentation during the set of weights."}, {"heading": "3.2.1 HYPERRNN", "text": "In fact, it is as it is in the real world in the real world of Wnlrlr.nreeNu nI \"r nlrrlreeaeNn,\" so nlrlteeaeaNr \"o, n os os os os os os os.\" nreD nI \"s nlrrrleaeNn, o os os os os\" nlrsrteeeeu, n os os os os os. \"n\" r \"o, nn os os os os os os os os os os os os os os os os.\" n \"r\" n \"o os os os os os os os.\" n \"n\" z \"nI\" n \"z\" n \"o\" o \"z\" z \"z\" o \"z\" z \"o\" o \"o\" os os os os os os os os os os os os os os os os. \"n\" n \"n\" os os os os os os os os os. \"n\" n, n \"n\" n \"n\" n \"n, n\" n \"n\" n, n \"n\" n \"n, n\" n \"n\" n, n \"n\" n, n \"n, n\" n, n \"n, n, n, n, n, n, n, n, n, n, n, n, n, n, n, n, n, n, n, n, n, n, n, n, n, n, n, n, n, n, n, in the real world of Wnlrrrrrrrrrrrrrrrrrrrrrrr.nos, n, n\" n, n, n, n, n, n, n, n, n, n, n, n, n, n, n, n, in the real world."}, {"heading": "3.2.2 HYPERLSTM", "text": "In this area, we will address the question of whether we will be able to change the world. (...) We will focus on the foundations of the LSTM architecture. (...) We will focus on the foundations of the LSTM architecture. (...) We will focus on the foundations of the LSTM architecture. (...) We will focus on the foundations of the LSTM architecture. (...) We will focus on the foundations of the LSTM architecture. (...) We will focus on the foundations of the LSTM architecture. (...) We will focus on the foundations of the LSTM policy. (...) We will focus on the foundations of the LSTM policy. (...) We will focus on the foundations of the STM policy. (...) We will focus on the foundations of the STM policy. (...) We will focus on the STM policy."}, {"heading": "4 EXPERIMENTS", "text": "In the following experiments we will measure the performance of static hypernetworks in image recognition with MNIST and CIFAR-10 and the performance of dynamic hypernetworks in speech modeling with Penn Treebank and Hutter Prize Wikipedia (enwik8) datasets and handwriting."}, {"heading": "4.1 USING STATIC HYPERNETWORKS TO GENERATE FILTERS FOR CONVOLUTIONAL NETWORKS AND MNIST", "text": "We start with the application of a hypernetwork to generate the filters for a revolutionary network on MNIST. Our main revolutionary network is a small two-layer network, and the hypernetwork is used to generate the core for the second layer (7x7x16x16), which contains most of the trainable parameters in the system. Our weight matrix is summarized by embedding the size Nz = 4. Table 1 shows a comparison of the accuracy and number of parameters for a normal revolutionary network and a hypernetwork. See Appendix A.3.1 for more details on the experimental setup."}, {"heading": "Model Test Error Params of 2nd Kernel", "text": "We see that we can achieve similar accuracy in this MNIST classification task, while we are able to represent the entire weight matrix with four numbers and train a generative network to generate a series of weights. In this example, there are 4240 parameters in the generative hypernet. We see the weight matrix that this network generates through the hypernet in Figure 2. Now the question is whether we can also train a deep coil network by using a single hypernetwork that generates a lot of weights for each layer, on a dataset that is more difficult than MNIST."}, {"heading": "4.2 STATIC HYPERNETWORKS FOR RESIDUAL NETWORK ARCHITECTURE AND CIFAR-10", "text": "The remaining network architectures (He et al., 2016a; Zagoruyko & Komodakis, 2016) are popular for image recognition tasks because they can accommodate very deep networks while maintaining an effective gradient flow across layers using skip connections. The original resnet and subsequent derivatives (Zhang et al., 2016; Huang et al., 2016a) achieved state-of-the-art image recognition performance on a variety of public datasets. While residual networks can be very deep and in some experiments as deep as 1001 layers (He et al., 2016b), it is important to understand whether some of these layers share common characteristics and can be effectively reduced by introducing a weight distribution across many layers of a deep feed-forward network. If we force a weight distribution across many layers of a deep feed-forward network, the network can pass many characteristics on to that of a recurring network, and we want to spread this idea over all."}, {"heading": "Model Test Error Param Count", "text": "We have obtained classification accuracy figures similar to those measured in (Zagoruyko & Komodakis, 2016) with our own implementation.We also note that the weights generated by the hypernetwork are used in a batch normalization setting without changing the original model.In principle, hypernetworks can also be applied to newer variants of residual networks with more skip connections, such as DenseNets and ResNets of Resnets.From the results, we see that enforcing a relaxed weight distribution on the deep residual network costs us approximately 1.5% classification accuracy, while drastically reducing the number of parameters in the model as a trade-off. One reason for this decrease in accuracy for the residual network, but not for the MNIST case, is that different layers of a deep network are trained to extract different levels of functionality, and require different types of filters to function optimally."}, {"heading": "4.3 HYPERLSTM FOR CHARACTER-LEVEL PENN TREEBANK LANGUAGE MODELING", "text": "The HyperLSTM model is based on the prediction task on Penn Treebank Corpus (Marcus et al., 1993) using the tensile / validation / test division outlined in (Mikolov et al., 2012). As the data set is relatively small, we tend to apply both the input and output layers with a shelf life probability of 0.90. Unlike previous approaches (Graves, 2013; Ognawala & Bayer, 2014) to the use of weight noise during training, we also apply the recursive layer (Henaff et al., 2016) with the same dropout probability. We compare our model with the basic LSTM cell (Hochreiter & Schmidhuber, 1997), stacked LSTM cells (Graves, 2013), and LSTM with the applied layer normalization. In addition, we experiment with the application of layer normalization on the layer normalization."}, {"heading": "4.4 HYPERLSTM FOR HUTTER PRIZE WIKIPEDIA LANGUAGE MODELING", "text": "We train our model on the larger and more challenging hutter price that we achieve near LSTM to accomplish this task. (Hutter, 2012) We point out that we apply the HyperLSTM techniques to the larger and more difficult hutter techniques that we have compared near this task. (Hutter, 2012) We point out that in most regions in which we move, there are different types of Appendix A.3.50Based on the results of version 2 of version 2 on the time of printing language models. http: / arxiv.org / abs / 1609.01704v2 1We do not compare with methods that use dynamic ratings. 2We see that HyperLSTM is once again competitive with layer standard LSTM, and when we combine both techniques, the HyperSTM techniques, the HyperSTM techniques, the LSTM techniques."}, {"heading": "4.5 HYPERLSTM FOR HANDWRITING SEQUENCE GENERATION", "text": "This year, as never before in the history of a country in which it is a country, in which it is a country, in which it is not a country, in which it is a country, in which it is a country, in which it is a country, in which it is not a country, in which it is a country, in which it is a country, in which it is a country, in which it is not a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which is a country, in which it is a country, in which is a country, in which it is a country, in which is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which is a country, in which is a country, in which is a country, in which is a"}, {"heading": "4.6 CONCLUSION", "text": "In this paper, we presented a method for using a hypernetwork to generate weights for another neural network. Our hypernetworks are trained end-to-end with back propagation, making them efficient and scalable. We focused on two uses of hypernetworks: static hypernetworks to generate weights for a revolutionary network, dynamic hypernetworks to generate weights for recurring networks. We found that the method works well, but uses fewer parameters. In image recognition, speech modelling and handwriting, hypernetworks are competitive or sometimes better than state-of-the-art models."}, {"heading": "ACKNOWLEDGMENTS", "text": "We thank Jeff Dean, Geoffrey Hinton, Mike Schuster and the Google Brain team for their help with the project."}, {"heading": "A APPENDIX", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "A.1 HYPERNETWORKS TO LEARN FILTERS FOR A FULLY CONNECTED NETWORKS", "text": "We have conducted an experiment in which the hypernetwork receives the x, y positions of both the input pixel and the weight and predicts the value of the hidden weight matrix in a fully connected network that learns to classify MNIST digits. In this experiment, the fully connected network (784-256-10) has a hidden layer of 16 x 16 units, with the hypernetwork being a predefined small feedback network. The weights of the hidden layer have 784 x 256 = 200704 parameters, while the hypernetwork is a four-layer feed-forward relay network with 801 parameters that would generate the 786 x 256 weight matrix. The result of this experiment is in Figure 8. We would like to emphasize that although the network can learn evolution-like filters during end-to-end training, its performance is rather poor: The best accuracy for this experiment is 93.5% complete, compared to 93.5%."}, {"heading": "A.2 KERNEL GENERATION EXAMPLES WITH STATIC HYPERNETNETWORK", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "A.2.1 FILTER VISUALIZATIONS FOR RESIDUAL NETWORKS", "text": "Figures 9 and 10 are examples of visualizations for different cores in a deep residual network. Note that the 32x32x3x3 kernel generated by the hypernetwork was constructed by concatenating 4 basic cores."}, {"heading": "A.3 EXPERIMENT SETUP DETAILS AND HYPER PARAMETERS", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "A.3.1 USING STATIC HYPERNETWORKS TO GENERATE FILTERS FOR CONVOLUTIONAL NETWORKS AND MNIST", "text": "We train the network with a 55000 / 5000 / 10000 split for training, validation and test sets and use the 5000 validation samples to stop early, and train the network with Adam (Kingma & Ba, 2015) at a learning rate of 0.001 on mini batches of size 1000."}, {"heading": "A.3.2 STATIC HYPERNETWORKS FOR RESIDUAL NETWORK ARCHITECTURE AND", "text": "In fact, it is not that we see ourselves as being in a position to be in a position to be in a position to be in a position to be in a position to be in a position to be in a position to be in a position to be in a position to be in a position to be in a position to be in a position to be in a position to be in a position to be in a position to be in a position to be in a position to be in a position to be in a position to be in a position to be in a position to be in a position to be in a position to be in a position to be in a position to be in a position to be in a position to be in a position to be in a position to be in a position to be in a position to be in a position to be in a position to be in a position to be in a position to be in a position to be in a position to be in a position to be in a position to be in a position to be in a position to be in a position to be in a position to be in a position to be in a position to be in a position to be in a position to be in a position to be in a position to be in a position to be in a position to be in a position to be in a position to be in a position to be in a position to a position to be in a position to be in a position to be in a position to be in a position to a position to be in a position to be in a position to a position to a position to a position to be in a position to be in a position to a position to a position to a position to be a position to a position to be a position to be in a position to be a position to a position to a position to be a position to be in a situation to a situation to be a situation to be a situation to a situation to a situation to be a situation to a situation to be a situation to a situation to a situation to a situation to be a situation to a situation to a situation to be a situation to be a situation to be a situation to a situation to be a situation to a situation to be a situation to a situation to be a situation to be a situation to a situation to be a situation to a situation to a situation to a situation to a situation to a situation to a situation to a situation to a situation to a situation to a situation to be a situation to"}, {"heading": "A.3.4 CHARACTER-LEVEL PENN TREEBANK", "text": "We trained the model with Adam (Kingma & Ba, 2015) with a learning rate of 0.001 and a gradient clipping of 1.0. During evaluation, we generate the entire sequence and do not use information about previous test errors to predict, e.g. dynamic evaluation (Graves, 2013; Rocki, 2016b). For base models, orthogonal initialization (Henaff et al., 2016) is performed for all weights."}, {"heading": "A.3.5 HUTTER PRIZE WIKIPEDIA", "text": "Since enwik8 is a larger data set than Penn Treebank, we will use 1800 units for our networks, unless stated otherwise in the table. Furthermore, we will conduct training on sequences of length 250. Our HyperLSTM cell will consist of 256 units, and we will have a signal size of 32. Our setup is similar to the previous experiment, using the same minibatch size, learning rate, weight initialization, gradient clipping parameters, and optimizer. We do not use dropouts for the input and output layers, but still apply the same failure probability for all models, unless specified otherwise in the table. For base models, the orthogonal initialization (Henaff et al., 2016) is performed for all weight layers. As in (Chung et al., 2015), we train on the first 90M characters of the data set, we will use the next 5M as the validation set for the early stop and the last M as the test set."}, {"heading": "A.3.6 HANDWRITING SEQUENCE GENERATION", "text": "We will use the same model architecture described in (Graves, 2013) and use a Mixture Density Network Layer (Bishop, 1994) to create a mixture of bi-variable Gaussian distributions to model the position of the pin at each step. We normalize the data and use the same tension / validation distribution as per (Graves, 2013) in this experiment. We remove samples below 300 in length as we have found that these samples contain a lot of recording errors and noise. After pre-processing, since the data set is small, we will apply data augmentation of the selected uniformly of + / - 10% and apply random scaling of the samples used for training. One concern we would like to address is the lack of a test set in the data distribution methodology developed in (Graves, 2013). In this task, the qualitative evaluation of the generated handwriting samples will be as important as the quantitative evaluation of the results of the calculation."}], "references": [{"title": "Mixture density networks", "author": ["Christopher M. Bishop"], "venue": "Technical report,", "citeRegEx": "Bishop.,? \\Q1994\\E", "shortCiteRegEx": "Bishop.", "year": 1994}, {"title": "Gated feedback recurrent neural networks", "author": ["Junyoung Chung", "Caglar G\u00fcl\u00e7ehre", "Kyunghyun Cho", "Yoshua Bengio"], "venue": "arXiv preprint arXiv:1502.02367,", "citeRegEx": "Chung et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Chung et al\\.", "year": 2015}, {"title": "Hierarchical multiscale recurrent neural networks", "author": ["Junyoung Chung", "Sungjin Ahn", "Yoshua Bengio"], "venue": "arXiv preprint arXiv:1609.01704,", "citeRegEx": "Chung et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Chung et al\\.", "year": 2016}, {"title": "Fast and accurate deep network learning by exponential linear units (ELUs)", "author": ["Djork-Arn\u00e9 Clevert", "Thomas Unterthiner", "Sepp Hochreiter"], "venue": "arXiv preprint arXiv:1511.07289,", "citeRegEx": "Clevert et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Clevert et al\\.", "year": 2015}, {"title": "Convolution by evolution: Differentiable pattern producing networks", "author": ["Chrisantha Fernando", "Dylan Banarse", "Malcolm Reynolds", "Frederic Besse", "David Pfau", "Max Jaderberg", "Marc Lanctot", "Daan Wierstra"], "venue": "arXiv preprint arXiv:1606.02580,", "citeRegEx": "Fernando et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Fernando et al\\.", "year": 2016}, {"title": "Generating sequences with recurrent neural networks", "author": ["Alex Graves"], "venue": null, "citeRegEx": "Graves.,? \\Q2013\\E", "shortCiteRegEx": "Graves.", "year": 2013}, {"title": "Deep residual learning for image recognition", "author": ["Kaiming He", "Xiangyu Zhang", "Shaoqing Ren", "Jian Sun"], "venue": "In CVPR,", "citeRegEx": "He et al\\.,? \\Q2016\\E", "shortCiteRegEx": "He et al\\.", "year": 2016}, {"title": "Identity mappings in deep residual networks", "author": ["Kaiming He", "Xiangyu Zhang", "Shaoqing Ren", "Jian Sun"], "venue": "arXiv preprint arXiv:1603.05027,", "citeRegEx": "He et al\\.,? \\Q2016\\E", "shortCiteRegEx": "He et al\\.", "year": 2016}, {"title": "Orthogonal RNNs and long-memory tasks", "author": ["Mikael Henaff", "Arthur Szlam", "Yann LeCun"], "venue": "In ICML,", "citeRegEx": "Henaff et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Henaff et al\\.", "year": 2016}, {"title": "Improving neural networks by preventing co-adaptation of feature detectors", "author": ["Geoffrey E Hinton", "Nitish Srivastava", "Alex Krizhevsky", "Ilya Sutskever", "Ruslan R Salakhutdinov"], "venue": "arXiv preprint arXiv:1207.0580,", "citeRegEx": "Hinton et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Hinton et al\\.", "year": 2012}, {"title": "Long short-term memory", "author": ["Sepp Hochreiter", "Juergen Schmidhuber"], "venue": "Neural Computation,", "citeRegEx": "Hochreiter and Schmidhuber.,? \\Q1997\\E", "shortCiteRegEx": "Hochreiter and Schmidhuber.", "year": 1997}, {"title": "Densely connected convolutional networks", "author": ["Gao Huang", "Zhuang Liu", "Kilian Q. Weinberger"], "venue": "arXiv preprint arXiv:1608.06993,", "citeRegEx": "Huang et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Huang et al\\.", "year": 2016}, {"title": "Deep networks with stochastic depth", "author": ["Gao Huang", "Yu Sun", "Zhuang Liu", "Daniel Sedra", "Kilian Weinberger"], "venue": "arXiv preprint arXiv:1603.09382,", "citeRegEx": "Huang et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Huang et al\\.", "year": 2016}, {"title": "The human knowledge compression contest", "author": ["Marcus Hutter"], "venue": "URL http://prize. hutter1.net/", "citeRegEx": "Hutter.,? \\Q2012\\E", "shortCiteRegEx": "Hutter.", "year": 2012}, {"title": "Grid long short-term memory", "author": ["Nal Kalchbrenner", "Ivo Danihelka", "Alex Graves"], "venue": "In ICLR,", "citeRegEx": "Kalchbrenner et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Kalchbrenner et al\\.", "year": 2016}, {"title": "Adam: A method for stochastic optimization", "author": ["Diederik Kingma", "Jimmy Ba"], "venue": "In ICLR,", "citeRegEx": "Kingma and Ba.,? \\Q2015\\E", "shortCiteRegEx": "Kingma and Ba.", "year": 2015}, {"title": "Evolving neural networks in compressed weight space", "author": ["Jan Koutnik", "Faustino Gomez", "J\u00fcrgen Schmidhuber"], "venue": "In GECCO,", "citeRegEx": "Koutnik et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Koutnik et al\\.", "year": 2010}, {"title": "Zoneout: Regularizing RNNs by randomly preserving hidden activations", "author": ["David Krueger", "Tegan Maharaj", "J\u00e1nos Kram\u00e1r", "Mohammad Pezeshki", "Nicolas Ballas", "Nan Rosemary Ke", "Anirudh Goyal", "Yoshua Bengio", "Hugo Larochelle", "Aaron Courville"], "venue": "arXiv preprint arXiv:1606.01305,", "citeRegEx": "Krueger et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Krueger et al\\.", "year": 2016}, {"title": "Handwritten digit recognition with a back-propagation network", "author": ["Y. LeCun", "B. Boser", "J.S. Denker", "D. Henderson", "R.E. Howard", "W. Hubbard", "L.D. Jackel"], "venue": "In Advances in neural information processing systems,", "citeRegEx": "LeCun et al\\.,? \\Q1990\\E", "shortCiteRegEx": "LeCun et al\\.", "year": 1990}, {"title": "IAM-OnDB - an on-line English sentence database acquired from handwritten text on a whiteboard", "author": ["Marcus Liwicki", "Horst Bunke"], "venue": "In ICDAR,", "citeRegEx": "Liwicki and Bunke.,? \\Q2005\\E", "shortCiteRegEx": "Liwicki and Bunke.", "year": 2005}, {"title": "Building a large annotated corpus of english: The penn treebank", "author": ["Mitchell P Marcus", "Mary Ann Marcinkiewicz", "Beatrice Santorini"], "venue": "Computational linguistics,", "citeRegEx": "Marcus et al\\.,? \\Q1993\\E", "shortCiteRegEx": "Marcus et al\\.", "year": 1993}, {"title": "Subword language modeling with neural networks", "author": ["Tom\u00e1\u0161 Mikolov", "Ilya Sutskever", "Anoop Deoras", "Hai-Son Le", "Stefan Kombrink", "Jan Cernocky"], "venue": null, "citeRegEx": "Mikolov et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Mikolov et al\\.", "year": 2012}, {"title": "Regularizing recurrent networks-on injected noise and normbased methods", "author": ["Saahil Ognawala", "Justin Bayer"], "venue": "arXiv preprint arXiv:1410.5684,", "citeRegEx": "Ognawala and Bayer.,? \\Q2014\\E", "shortCiteRegEx": "Ognawala and Bayer.", "year": 2014}, {"title": "Recurrent memory array structures", "author": ["Kamil Rocki"], "venue": "arXiv preprint arXiv:1607.03085,", "citeRegEx": "Rocki.,? \\Q2016\\E", "shortCiteRegEx": "Rocki.", "year": 2016}, {"title": "Surprisal-driven feedback in recurrent networks", "author": ["Kamil Rocki"], "venue": "arXiv preprint arXiv:1608.06027,", "citeRegEx": "Rocki.,? \\Q2016\\E", "shortCiteRegEx": "Rocki.", "year": 2016}, {"title": "Fitnets: Hints for thin deep nets", "author": ["Adriana Romero", "Nicolas Ballas", "Samira Ebrahimi Kahou", "Antoine Chassang", "Carlo Gatta", "Yoshua Bengio"], "venue": "arXiv preprint arXiv:1412.6550,", "citeRegEx": "Romero et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Romero et al\\.", "year": 2014}, {"title": "Recurrent dropout without memory loss", "author": ["Stanislaw Semeniuta", "Aliases Severyn", "Erhardt Barth"], "venue": null, "citeRegEx": "Semeniuta et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Semeniuta et al\\.", "year": 2016}, {"title": "Training very deep networks", "author": ["Rupesh Srivastava", "Klaus Greff", "J\u00fcrgen Schmidhuber"], "venue": "In NIPS,", "citeRegEx": "Srivastava et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Srivastava et al\\.", "year": 2015}, {"title": "A hypercube-based encoding for evolving large-scale neural networks", "author": ["Kenneth O. Stanley", "David B. D\u2019Ambrosio", "Jason Gauci"], "venue": "Artificial Life,", "citeRegEx": "Stanley et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Stanley et al\\.", "year": 2009}, {"title": "Generating text with recurrent neural networks", "author": ["Ilya Sutskever", "James Martens", "Geoffrey E. Hinton"], "venue": "In ICML,", "citeRegEx": "Sutskever et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Sutskever et al\\.", "year": 2011}, {"title": "On multiplicative integration with recurrent neural networks", "author": ["Yuhuai Wu", "Saizheng Zhang", "Ying Zhang", "Yoshua Bengio", "Ruslan Salakhutdinov"], "venue": null, "citeRegEx": "Wu et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Wu et al\\.", "year": 2016}, {"title": "Fast algorithms for hierarchically semiseparable matrices", "author": ["Jianlin Xia", "Shivkumar Chandrasekaran", "Ming Gu", "Xiaoye S. Li"], "venue": "Numerical Linear Algebra with Applications,", "citeRegEx": "Xia et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Xia et al\\.", "year": 2010}, {"title": "Wide residual networks", "author": ["Sergey Zagoruyko", "Nikos Komodakis"], "venue": "In BMVC,", "citeRegEx": "Zagoruyko and Komodakis.,? \\Q2016\\E", "shortCiteRegEx": "Zagoruyko and Komodakis.", "year": 2016}, {"title": "Residual networks of residual networks: Multilevel residual networks", "author": ["Ke Zhang", "Miao Sun", "Tony X. Han", "Xingfang Yuan", "Liru Guo", "Tao Liu"], "venue": "arXiv preprint arXiv:1608.02908,", "citeRegEx": "Zhang et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Zhang et al\\.", "year": 2016}, {"title": "Recurrent highway networks", "author": ["Julian Zilly", "Rupesh Srivastava", "Jan Koutn\u0131\u0301k", "J\u00fcrgen Schmidhuber"], "venue": "arXiv preprint arXiv:1607.03474,", "citeRegEx": "Zilly et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Zilly et al\\.", "year": 2016}], "referenceMentions": [{"referenceID": 28, "context": "HyperNEAT (Stanley et al., 2009) is an example of a hypernetwork where the inputs are a set of virtual coordinates for each weight in the main network.", "startOffset": 10, "endOffset": 32}, {"referenceID": 18, "context": "On image classification with CIFAR-10, hypernetworks, when being used to generate weights for a deep convnet (LeCun et al., 1990), obtain respectable results compared to state-of-the-art models while having fewer learnable parameters.", "startOffset": 109, "endOffset": 129}, {"referenceID": 28, "context": "An instance of this is the work on Hypercubebased NEAT or HyperNEAT framework (Stanley et al., 2009).", "startOffset": 78, "endOffset": 100}, {"referenceID": 16, "context": "Closely related to our approach is a simplified variation of HyperNEAT, where the structure is fixed and the weights are evolved through Discrete Cosine Transform is called Compressed Weight Search (Koutnik et al., 2010).", "startOffset": 198, "endOffset": 220}, {"referenceID": 4, "context": "Even more closely related to our approach is Differentiable Pattern Producing Networks (DPPNs), where the structure is evolved but the weights are learned (Fernando et al., 2016).", "startOffset": 155, "endOffset": 178}, {"referenceID": 4, "context": "Using this setting, hypernetworks can approximately recover the convolutional architecture without explicitly being told to do so, a similar result obtained by \u201dConvolution by Evolution\u201d (Fernando et al., 2016).", "startOffset": 187, "endOffset": 210}, {"referenceID": 31, "context": ") is similar to the hierarchically semiseparable matrix approach proposed by Xia et al. (2010). Note that even though it seems redundant to have a two-layered linear hypernetwork as that is equivalent to a one-layered hypernetwork, the fact that Wout and Bout are shared makes our two-layered hypernetwork more compact than a one-layered hypernetwork.", "startOffset": 77, "endOffset": 95}, {"referenceID": 9, "context": "In our implementation, the cell and hidden state update equations for the main LSTM will incorporate a single dropout (Hinton et al., 2012) gate, as developed in Recurrent Dropout without Memory Loss (Semeniuta et al.", "startOffset": 118, "endOffset": 139}, {"referenceID": 26, "context": ", 2012) gate, as developed in Recurrent Dropout without Memory Loss (Semeniuta et al., 2016), as we found this to help regularize the entire model during training:", "startOffset": 68, "endOffset": 92}, {"referenceID": 33, "context": "The original resnet and subsequent derivatives (Zhang et al., 2016; Huang et al., 2016a) achieved state-of-the-art image recognition performance on a variety of public datasets.", "startOffset": 47, "endOffset": 88}, {"referenceID": 25, "context": "81% FitNet (Romero et al., 2014) 8.", "startOffset": 11, "endOffset": 32}, {"referenceID": 27, "context": "22% Highway Networks (Srivastava et al., 2015) 7.", "startOffset": 21, "endOffset": 46}, {"referenceID": 3, "context": "72% ELU (Clevert et al., 2015) 6.", "startOffset": 8, "endOffset": 30}, {"referenceID": 33, "context": "5 M ResNet of ResNet 58-4 (Zhang et al., 2016) 3.", "startOffset": 26, "endOffset": 46}, {"referenceID": 20, "context": "The HyperLSTM model is evaluated on character level prediction task on the Penn Treebank corpus (Marcus et al., 1993) using the train/validation/test split outlined in (Mikolov et al.", "startOffset": 96, "endOffset": 117}, {"referenceID": 21, "context": ", 1993) using the train/validation/test split outlined in (Mikolov et al., 2012).", "startOffset": 58, "endOffset": 80}, {"referenceID": 5, "context": "Unlike previous approaches (Graves, 2013; Ognawala & Bayer, 2014) of applying weight noise during training, we instead also apply dropout to the recurrent layer (Henaff et al.", "startOffset": 27, "endOffset": 65}, {"referenceID": 8, "context": "Unlike previous approaches (Graves, 2013; Ognawala & Bayer, 2014) of applying weight noise during training, we instead also apply dropout to the recurrent layer (Henaff et al., 2016) with the same dropout probability.", "startOffset": 161, "endOffset": 182}, {"referenceID": 5, "context": "We compare our model to the basic LSTM cell (Hochreiter & Schmidhuber, 1997), stacked LSTM cells (Graves, 2013), and LSTM with layer normalization applied.", "startOffset": 97, "endOffset": 111}, {"referenceID": 5, "context": "Using the setup in (Graves, 2013), we use networks with 1000 units and train the network to predict the next character.", "startOffset": 19, "endOffset": 33}, {"referenceID": 21, "context": "Model1 Test Validation Param Count ME n-gram (Mikolov et al., 2012) 1.", "startOffset": 45, "endOffset": 67}, {"referenceID": 26, "context": "32 Recurrent Dropout LSTM (Semeniuta et al., 2016) 1.", "startOffset": 26, "endOffset": 50}, {"referenceID": 17, "context": "338 Zoneout RNN (Krueger et al., 2016) 1.", "startOffset": 16, "endOffset": 38}, {"referenceID": 2, "context": "30 HM-LSTM0 (Chung et al., 2016) 1.", "startOffset": 12, "endOffset": 32}, {"referenceID": 26, "context": "Our implementation of Recurrent Dropout Basic LSTM cell reproduced similar results as (Semeniuta et al., 2016), where they have also experimented with different dropout settings.", "startOffset": 86, "endOffset": 110}, {"referenceID": 13, "context": "We train our model on the larger and more challenging Hutter Prize Wikipedia dataset, also known as enwik8 (Hutter, 2012) consisting of a sequence of 100M characters composed of 205 unique characters.", "startOffset": 107, "endOffset": 121}, {"referenceID": 5, "context": "Model1 enwik8 Param Count Stacked LSTM (Graves, 2013) 1.", "startOffset": 39, "endOffset": 53}, {"referenceID": 29, "context": "0 M MRNN (Sutskever et al., 2011) 1.", "startOffset": 9, "endOffset": 33}, {"referenceID": 1, "context": "60 GF-RNN (Chung et al., 2015) 1.", "startOffset": 10, "endOffset": 30}, {"referenceID": 14, "context": "0 M Grid-LSTM (Kalchbrenner et al., 2016) 1.", "startOffset": 14, "endOffset": 41}, {"referenceID": 30, "context": "8 M MI-LSTM (Wu et al., 2016) 1.", "startOffset": 12, "endOffset": 29}, {"referenceID": 34, "context": "44 Recurrent Highway Networks (Zilly et al., 2016) 1.", "startOffset": 30, "endOffset": 50}, {"referenceID": 2, "context": "40 HM-LSTM0 (Chung et al., 2016) 1.", "startOffset": 12, "endOffset": 32}, {"referenceID": 5, "context": "2 of (Graves, 2013).", "startOffset": 5, "endOffset": 19}, {"referenceID": 5, "context": "In this task, we note that data augmentation and applying recurrent dropout improved the performance of all models, compared to the original setup by (Graves, 2013).", "startOffset": 150, "endOffset": 164}, {"referenceID": 5, "context": "Our implementation, to replicate setup of (Graves, 2013).", "startOffset": 42, "endOffset": 56}, {"referenceID": 5, "context": "Model Log-Loss Param Count LSTM, 900 units (Graves, 2013) -1,026 3-Layer LSTM, 400 units (Graves, 2013) -1,041 3-Layer LSTM, 400 units, adaptive weight noise (Graves, 2013) -1,058 LSTM, 900 units, no dropout, no data augmentation.", "startOffset": 43, "endOffset": 57}, {"referenceID": 5, "context": "Model Log-Loss Param Count LSTM, 900 units (Graves, 2013) -1,026 3-Layer LSTM, 400 units (Graves, 2013) -1,041 3-Layer LSTM, 400 units, adaptive weight noise (Graves, 2013) -1,058 LSTM, 900 units, no dropout, no data augmentation.", "startOffset": 89, "endOffset": 103}, {"referenceID": 5, "context": "Model Log-Loss Param Count LSTM, 900 units (Graves, 2013) -1,026 3-Layer LSTM, 400 units (Graves, 2013) -1,041 3-Layer LSTM, 400 units, adaptive weight noise (Graves, 2013) -1,058 LSTM, 900 units, no dropout, no data augmentation.", "startOffset": 158, "endOffset": 172}], "year": 2016, "abstractText": "This work explores hypernetworks: an approach of using a small network, also known as a hypernetwork, to generate the weights for a larger network. Hypernetworks provide an abstraction that is similar to what is found in nature: the relationship between a genotype \u2013 the hypernetwork \u2013 and a phenotype \u2013 the main network. Though they are also reminiscent of HyperNEAT in evolution, our hypernetworks are trained end-to-end with backpropagation and thus are usually faster. The focus of this work is to make hypernetworks useful for deep convolutional networks and long recurrent networks, where hypernetworks can be viewed as relaxed form of weight-sharing across layers. Our main result is that hypernetworks can generate non-shared weights for LSTM and achieve state-of-art results on a variety of language modeling tasks with Character-Level Penn Treebank and Hutter Prize Wikipedia datasets, challenging the weight-sharing paradigm for recurrent networks. Our results also show that hypernetworks applied to convolutional networks still achieve respectable results for image recognition tasks compared to state-of-the-art baseline models while requiring fewer learnable parameters.", "creator": "LaTeX with hyperref package"}}}