{"id": "1609.09444", "review": {"conference": "AAAI", "VERSION": "v1", "DATE_OF_SUBMISSION": "29-Sep-2016", "title": "Contextual RNN-GANs for Abstract Reasoning Diagram Generation", "abstract": "Understanding, predicting, and generating object motions and transformations is a core problem in artificial intelligence. Modeling sequences of evolving images may provide better representations and models of motion and may ultimately be used for forecasting, simulation, or video generation. Diagrammatic Abstract Reasoning is an avenue in which diagrams evolve in complex patterns and one needs to infer the underlying pattern sequence and generate the next image in the sequence. For this, we develop a novel Contextual Generative Adversarial Network based on Recurrent Neural Networks (Context-RNN-GANs), where both the generator and the discriminator modules are based on contextual history (modeled as RNNs) and the adversarial discriminator guides the generator to produce realistic images for the particular time step in the image sequence. We evaluate the Context-RNN-GAN model (and its variants) on a novel dataset of Diagrammatic Abstract Reasoning, where it performs competitively with 10th-grade human performance but there is still scope for interesting improvements as compared to college-grade human performance. We also evaluate our model on a standard video next-frame prediction task, achieving improved performance over comparable state-of-the-art.", "histories": [["v1", "Thu, 29 Sep 2016 17:56:32 GMT  (176kb,D)", "http://arxiv.org/abs/1609.09444v1", null], ["v2", "Tue, 6 Dec 2016 13:14:09 GMT  (173kb,D)", "http://arxiv.org/abs/1609.09444v2", "To Appear in AAAI-17 and NIPS Workshop on Adversarial Training"]], "reviews": [], "SUBJECTS": "cs.CV cs.AI cs.LG", "authors": ["viveka kulharia", "arnab ghosh", "amitabha mukerjee", "vinay p namboodiri", "mohit bansal"], "accepted": true, "id": "1609.09444"}, "pdf": {"name": "1609.09444.pdf", "metadata": {"source": "CRF", "title": "Contextual RNN-GANs for Abstract Reasoning Diagram Generation", "authors": ["Arnab Ghosh", "Viveka Kulharia", "Amitabha Mukerjee", "Vinay Namboodiri", "Mohit Bansal"], "emails": ["mbansal@cs.unc.edu"], "sections": [{"heading": "Introduction", "text": "In fact, it is a purely mental game, in which it is a matter of understanding the world as it is and as it is."}, {"heading": "Related Work", "text": "(Stern, 1914) introduced IQ tests to measure an individual's success in adapting to a specific situation under certain conditions. Visual problems in intelligence tests are among the earliest and continuously explored problems in the field of artificial intelligence. It has therefore been considered in terms of propositional logic, starting with (Evans, 1964) and more recently by (Prade and Richard, 2011). Recently, there has also been a significant interest in building systems that compete with humans in a variety of tasks (Antol et al., 2015), physics-based problems (Novak and Bulko, 1992), repetition and symmetry recognition (Novak and Bulko, 1992), visual questions that respond to a variety of tasks (Antol et al., 2015), and verbal reasoning and analogy (Mikolov et al al al al., 2013; Wang et al, 2015).Our task is closely related to the problem of progressions."}, {"heading": "Models", "text": "We first describe our primary Context RNN GAN model and then briefly discuss two simplifications of this model, namely RNN-GAN and a regular RNN. The main motivation for using a reverse loss was the shortcomings of L-2 and L-1 losses (Fig. 3): \u2022 When using an L-2 loss function, some of the generated images were superimposed on the components and were too confusing. \u2022 When using an L-1 loss function, some components of the actual diagrams were missing, although it was sharper than when using an L-2 loss."}, {"heading": "Context-RNN-GAN", "text": "In fact, the first basic principle underlying our model is the same as that of the original GAN model of (Goodfellow et al., 2014), which consists in the discriminator and the generator trying to play the following minimax game: min\u03b2max\u03b8F (\u03b2) = Ex \u0445 pdata (y = x)] + Exempor p\u03b2 (y = 0 | x)], the pdata's distribution, p\u03b2 is generator's distribution with the vector of parameters for the generator, and as the art of the art, we is discriminator's distribution with the art of the art. \"We is discriminator's from the art of the art of the art of the art.\""}, {"heading": "RNN-GAN", "text": "The RNN-GAN model is a simplified version of the ContextRNN-GAN model, in which the discriminator is merely a multilayer perceptron (i.e. a fully connected network) that receives the generated (and real) images only in the current timeframe and not a previous context image. The aim of this discriminator is to classify whether the image provided to it is an image from the distribution of the data set or whether it is generated by the generator, i.e. the discriminator is replaced by D (x1.. xt, yt) to only D (yt)."}, {"heading": "RNN", "text": "Finally, the simplest model is a regular RNN (also with GRU units, similar to the generator modules above), which merely models the image sequence by trying to predict the characteristics of each image taking into account the characteristics of all previous images in the sequence. We have tried L1 and L2 loss functions."}, {"heading": "Feed-forward Baseline", "text": "The feed-forward network baseline is simply a fully connected multi-layer perceptron with 2 layers and is trained using the first 4 images x1.. xt \u2212 1 to predict the fifth image xt; and during the test, the last 4 images x2.. xt are used to predict the characteristics of the response image xt + 1."}, {"heading": "Feature Representations", "text": "In this section, we discuss the various methods of image embedding that we have used to create input functions for the sequencers, such as the Context RNN GAN discussed above. Raw Pixels As the first baseline, each individual image was used in the same dimension of 128 x 128 pixel values, which are used as characteristics that occurred in the order between the individual models. The characteristics of each model were modified to show the various edges and corners of the image. (Dalal and Triggs, 2005) are achieved by concatenating histograms of events of orientation in each area."}, {"heading": "Experimental Setup", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "Dataset", "text": "We collected the data from several IQ test books and online resources (Aggarwal, 2016), (Sijwali and Sijwali, 2016), (Gupta, 2016) and (Jha and Siddiquee, 2011). We collected approximately 1500 training problems (15,000 images) and a commented test set with 100 problems, and then used transformations such as rotation and mirror reflections on the axes to increase the data eightfold, resulting in a total of 12,000 problem sequences to be trained, each of which contained a sequence of 5 diagrams. Each test problem consists of five digits for the input sequence and five as a response choice, i.e. in a multiple choice setup to allow easier quantitative evaluation. All proposed models were trained only on the five digits of the question part, i.e. the training program consisting of (12000x5) images was used, while the (sixth) correct training was not used for the first 50% of the answer figure."}, {"heading": "Training Details", "text": "The best (validated) hyperparameters for the context RNNGAN were a GRU with two layers of 400 hidden units each for the generator and a GRU with one layer and 500 hidden units for the discriminator. The best (validated) hyperparameters for the RNN-GAN model were a GRU with two layers of 400 hidden units each for the generator and a multi-layer perceptron (MLP). The final models of ContextRNN-GAN and the RNN-GAN models use \u03bbadv = 0.05, \u03bbp = 1, and they use p = 1 for the DAT-DAR task and p = 2 for the next-frame prediction task, similar (Mathieu et al., 2015) and based on empirical evidence from experiments. The best hyperparameters of the regular RNN were a GRU with a hidden layer of 1000 hidden units each for both L1 and L2 based functions."}, {"heading": "Evaluation Metrics", "text": "In addition to the qualitative evaluation by visualizing the generated images, we also perform a quantitative evaluation by comparing the generated image with the embedding of each of the five response images (based on the cosine distance) and returning the closest matching image. Accuracy is then determined by the number of correct decisions made by the model in relation to the total size of the test set."}, {"heading": "Results and Analysis", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "Human Performance on DAR", "text": "In order to test human competence using our DAT-DAR dataset, we conducted experiments on two groups of individuals, which were divided into groups of 12 tasks and given as much time as necessary to answer all the questions. In addition, they were given an example of a problem with an explanation of the answer. 3 Advanced Students: 21 students from the computer science faculty of a leading university participated in the first experiment. 10th grade students from a prestigious high school: 48 students from the 10th grade of a prestigious high school participated in the second experiment.As shown in Table 1, our diagrammatic abstract brain task is quite a challenge even for humans. To enable sincere participation, we have set up a reward-based system for high-performing personalities."}, {"heading": "Features Accuracy", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "RNN", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "RNN-GAN", "text": "The best performance of high-performing undergraduate students is about 44% and the accuracy of 10th grade students is 37%."}, {"heading": "Model Performance on DAR", "text": "In Table 2, we first show the basic results for a regular RNN-GAN model (and its simpler RNN-GAN variant). As shown in Table 2, our primary context RNN-GAN functions, combined with our novel Siamese CNN functions, provide the best results, even competitive with 10th grade human performance. However, there is still plenty of scope for interesting models and feature improvements compared to collective LNN-GAN models (and beyond), making this a new challenging task and dataset for the communist metropolises.Next-Frame Generation on Moving NISN-T videos compared to the popular NISM videos in 2015."}, {"heading": "Qualitiative Generation Visualization", "text": "We also present quantitative evaluations based on visualizations of the images generated by our model for both the DAR task (fig. 4) and the Moving MNIST task (fig. 5). We show cases where the generated image closely matches the correct response image. Thus, for example, in the DAR task (fig. 4), the model is able to generate correct sequence diagrams with alternating arrow directions, number and type of lines in different corners and sides, multiple shapes that interact in different ways, etc. Similarly, in the MNIST task (fig. 5), our model is able to generate correct digits that move in different amounts and directions."}, {"heading": "Conclusion", "text": "We presented a novel Context RNN GAN model that can generate images for sequential reasoning scenarios such as our task of diagrammatic abstract thinking. Combined with useful characteristics such as those of Siamese CNNs, our model performs competitively compared to 10th grade people, but there is still scope for interesting improvements over college-level human performance, making it a novel challenging task for the generational community. Our sequential GAN model is also generic enough to be useful for tasks such as the next frame generations in a video (where we also get strong results on the Moving MNIST dataset) and other similarly important AI tasks such as prediction and simulation."}, {"heading": "Acknowledgment", "text": "We would like to thank Dhruv Batra (Georgia Tech), Kundan Kumar (IIT Kanpur), Devi Parikh (Georgia Tech), Deepak Pathak (UC Berkeley), Greg Shakhnarovich (TTI-Chicago) and Shubham Tulsiani (UC Berkeley) for their helpful feedback."}], "references": [{"title": "A Modern Approach To Non-Verbal Reasoning", "author": ["Aggarwal", "D.R.S. 2016] Aggarwal"], "venue": null, "citeRegEx": "Aggarwal and Aggarwal,? \\Q2016\\E", "shortCiteRegEx": "Aggarwal and Aggarwal", "year": 2016}, {"title": "VQA: Visual question answering", "author": ["Antol et al", "S. 2015] Antol", "A. Agrawal", "J. Lu", "M. Mitchell", "D. Batra", "C.L. Zitnick", "D. Parikh"], "venue": null, "citeRegEx": "al. et al\\.,? \\Q2015\\E", "shortCiteRegEx": "al. et al\\.", "year": 2015}, {"title": "Differential aptitude tests", "author": ["Bennett et al", "G. 1947] Bennett", "H. Seashore", "A. Wesman"], "venue": "Psychological Corporation", "citeRegEx": "al. et al\\.,? \\Q1947\\E", "shortCiteRegEx": "al. et al\\.", "year": 1947}, {"title": "Pulling it all together via psychometric AI", "author": ["Bringsjord", "Schimanski", "S. 2004] Bringsjord", "B. Schimanski"], "venue": "In Proc. of AAAI symposium: achieving human-level intelligence through integrated systems and Research,", "citeRegEx": "Bringsjord et al\\.,? \\Q2004\\E", "shortCiteRegEx": "Bringsjord et al\\.", "year": 2004}, {"title": "Learning a similarity metric discriminatively, with application to face verification", "author": ["Chopra et al", "S. 2005] Chopra", "R. Hadsell", "Y. LeCun"], "venue": null, "citeRegEx": "al. et al\\.,? \\Q2005\\E", "shortCiteRegEx": "al. et al\\.", "year": 2005}, {"title": "Histograms of oriented gradients for human detection", "author": ["Dalal", "Triggs", "N. 2005] Dalal", "B. Triggs"], "venue": null, "citeRegEx": "Dalal et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Dalal et al\\.", "year": 2005}, {"title": "Deep generative image models using a laplacian pyramid of adversarial networks", "author": ["Denton et al", "E.L. 2015] Denton", "S. Chintala", "R Fergus"], "venue": null, "citeRegEx": "al. et al\\.,? \\Q2015\\E", "shortCiteRegEx": "al. et al\\.", "year": 2015}, {"title": "A program for the solution of a class of geometric-analogy intelligence-test questions", "author": ["Evans", "T.G. 1964] Evans"], "venue": null, "citeRegEx": "Evans and Evans,? \\Q1964\\E", "shortCiteRegEx": "Evans and Evans", "year": 1964}, {"title": "The structure-mapping engine: Algorithm and examples", "author": ["Falkenhainer et al", "B. 1989] Falkenhainer", "K.D. Forbus", "D. Gentner"], "venue": "Artificial intelligence,", "citeRegEx": "al. et al\\.,? \\Q1989\\E", "shortCiteRegEx": "al. et al\\.", "year": 1989}, {"title": "Draw: A recurrent neural network for image generation", "author": ["Gregor et al", "K. 2015] Gregor", "I. Danihelka", "A. Graves", "D. Wierstra"], "venue": "arXiv preprint arXiv:1502.04623", "citeRegEx": "al. et al\\.,? \\Q2015\\E", "shortCiteRegEx": "al. et al\\.", "year": 2015}, {"title": "Generating images with recurrent adversarial networks. arXiv preprint arXiv:1602.05110", "author": ["Im et al", "D.J. 2016] Im", "C.D. Kim", "H. Jiang", "R. Memisevic"], "venue": null, "citeRegEx": "al. et al\\.,? \\Q2016\\E", "shortCiteRegEx": "al. et al\\.", "year": 2016}, {"title": "Magical Book Series Non-Verbal Reasoning", "author": ["Jha", "Siddiquee", "P.N. 2011] Jha", "J.R. Siddiquee"], "venue": "BSC Publication Co. Pvt. Ltd.,", "citeRegEx": "Jha et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Jha et al\\.", "year": 2011}, {"title": "Adam: A method for stochastic optimization. arXiv preprint arXiv:1412.6980", "author": ["Kingma", "Ba", "D. 2014] Kingma", "J. Ba"], "venue": null, "citeRegEx": "Kingma et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Kingma et al\\.", "year": 2014}, {"title": "Imagenet classification with deep convolutional neural networks", "author": ["Krizhevsky et al", "A. 2012] Krizhevsky", "I. Sutskever", "G.E. Hinton"], "venue": "In NIPS,", "citeRegEx": "al. et al\\.,? \\Q2012\\E", "shortCiteRegEx": "al. et al\\.", "year": 2012}, {"title": "Deep multi-scale video prediction beyond mean square error. arXiv preprint arXiv:1511.05440", "author": ["Mathieu et al", "M. 2015] Mathieu", "C. Couprie", "Y. LeCun"], "venue": null, "citeRegEx": "al. et al\\.,? \\Q2015\\E", "shortCiteRegEx": "al. et al\\.", "year": 2015}, {"title": "Efficient estimation of word representations in vector space. arXiv preprint arXiv:1301.3781", "author": ["Mikolov et al", "T. 2013] Mikolov", "K. Chen", "G. Corrado", "J. Dean"], "venue": null, "citeRegEx": "al. et al\\.,? \\Q2013\\E", "shortCiteRegEx": "al. et al\\.", "year": 2013}, {"title": "Conditional generative adversarial nets. arXiv preprint arXiv:1411.1784", "author": ["Mirza", "Osindero", "M. 2014] Mirza", "S. Osindero"], "venue": null, "citeRegEx": "Mirza et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Mirza et al\\.", "year": 2014}, {"title": "Human-level control through deep reinforcement learning", "author": ["Mnih et al", "V. 2015] Mnih", "K. Kavukcuoglu", "D. Silver", "A.A. Rusu", "J. Veness", "M.G. Bellemare", "A. Graves", "M Riedmiller"], "venue": null, "citeRegEx": "al. et al\\.,? \\Q2015\\E", "shortCiteRegEx": "al. et al\\.", "year": 2015}, {"title": "Uses of diagrams in solving physics problems", "author": ["Novak", "Bulko", "G. 1992] Novak", "W. Bulko"], "venue": "In AAAI Symposium on Reasoning with Diagrammatic Representations", "citeRegEx": "Novak et al\\.,? \\Q1992\\E", "shortCiteRegEx": "Novak et al\\.", "year": 1992}, {"title": "Context encoders: Feature learning by inpainting", "author": ["Pathak et al", "D. 2016] Pathak", "P. Kr\u00e4henb\u00fchl", "J. Donahue", "T. Darrell", "A. Efros"], "venue": null, "citeRegEx": "al. et al\\.,? \\Q2016\\E", "shortCiteRegEx": "al. et al\\.", "year": 2016}, {"title": "Spatio-temporal video autoencoder with differentiable memory", "author": ["Patraucean et al", "V. 2015] Patraucean", "A. Handa", "R. Cipolla"], "venue": "arXiv preprint arXiv:1511.06309", "citeRegEx": "al. et al\\.,? \\Q2015\\E", "shortCiteRegEx": "al. et al\\.", "year": 2015}, {"title": "Recursive estimation of generative models of video", "author": ["Petrovic et al", "N. 2006] Petrovic", "A. Ivanovic", "N. Jojic"], "venue": "In Proc. of CVPR,", "citeRegEx": "al. et al\\.,? \\Q2006\\E", "shortCiteRegEx": "al. et al\\.", "year": 2006}, {"title": "Analogy-making for solving iq tests: A logical view", "author": ["Prade", "Richard", "H. 2011] Prade", "G. Richard"], "venue": "In CaseBased Reasoning Research and Development,", "citeRegEx": "Prade et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Prade et al\\.", "year": 2011}, {"title": "Unsupervised representation learning with deep convolutional generative adversarial networks. arXiv preprint arXiv:1511.06434", "author": ["Radford et al", "A. 2015] Radford", "L. Metz", "S. Chintala"], "venue": null, "citeRegEx": "al. et al\\.,? \\Q2015\\E", "shortCiteRegEx": "al. et al\\.", "year": 2015}, {"title": "Video (language) modeling: a baseline for generative models of natural videos. arXiv preprint arXiv:1412.6604", "author": ["Ranzato et al", "M. 2014] Ranzato", "A. Szlam", "J. Bruna", "M. Mathieu", "R. Collobert", "S. Chopra"], "venue": null, "citeRegEx": "al. et al\\.,? \\Q2014\\E", "shortCiteRegEx": "al. et al\\.", "year": 2014}, {"title": "Generative adversarial text to image synthesis", "author": ["Reed et al", "S. 2016] Reed", "Z. Akata", "X. Yan", "L. Logeswaran", "B. Schiele", "H. Lee"], "venue": "arXiv preprint arXiv:1605.05396", "citeRegEx": "al. et al\\.,? \\Q2016\\E", "shortCiteRegEx": "al. et al\\.", "year": 2016}, {"title": "Solving geometry problems: Combining text and diagram interpretation", "author": ["Seo et al", "M. 2015] Seo", "H. Hajishirzi", "A. Farhadi", "O. Etzioni", "C. Malcolm"], "venue": "Proceedings of EMNLP", "citeRegEx": "al. et al\\.,? \\Q2015\\E", "shortCiteRegEx": "al. et al\\.", "year": 2015}, {"title": "Overfeat: Integrated recognition, localization and detection using convolutional networks. arXiv preprint arXiv:1312.6229", "author": ["Sermanet et al", "P. 2013] Sermanet", "D. Eigen", "X. Zhang", "M. Mathieu", "R. Fergus", "Y. LeCun"], "venue": null, "citeRegEx": "al. et al\\.,? \\Q2013\\E", "shortCiteRegEx": "al. et al\\.", "year": 2013}, {"title": "NonVerbal Reasoning. Arihant Publications (India) LTD", "author": ["Sijwali", "B. 2016] Sijwali", "I. Sijwali"], "venue": null, "citeRegEx": "Sijwali et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Sijwali et al\\.", "year": 2016}, {"title": "Mastering the game of Go with deep neural networks and tree search. Nature, 529(7587):484\u2013489", "author": ["Silver et al", "D. 2016] Silver", "A. Huang", "C.J. Maddison", "A. Guez", "L. Sifre", "G. Van Den Driessche", "J Schrittwieser"], "venue": null, "citeRegEx": "al. et al\\.,? \\Q2016\\E", "shortCiteRegEx": "al. et al\\.", "year": 2016}, {"title": "Unsupervised learning of video representations using lstms", "author": ["Srivastava et al", "N. 2015] Srivastava", "E. Mansimov", "R. Salakhutdinov"], "venue": "CoRR, abs/1502.04681,", "citeRegEx": "al. et al\\.,? \\Q2015\\E", "shortCiteRegEx": "al. et al\\.", "year": 2015}, {"title": "Extracting and composing robust features with denoising autoencoders", "author": ["Vincent et al", "P. 2008] Vincent", "H. Larochelle", "Y. Bengio", "Manzagol", "P.-A"], "venue": "In ICML,", "citeRegEx": "al. et al\\.,? \\Q2008\\E", "shortCiteRegEx": "al. et al\\.", "year": 2008}, {"title": "Generating videos with scene dynamics", "author": ["Vondrick et al", "C. 2016] Vondrick", "H. Pirsiavash", "A. Torralba"], "venue": "arXiv preprint arXiv:1609.02612", "citeRegEx": "al. et al\\.,? \\Q2016\\E", "shortCiteRegEx": "al. et al\\.", "year": 2016}, {"title": "Solving verbal comprehension questions in iq test by knowledge-powered word embedding", "author": ["Wang et al", "H. 2015] Wang", "B. Gao", "J. Bian", "F. Tian", "Liu", "T.-Y"], "venue": "arXiv preprint arXiv:1505.07909", "citeRegEx": "al. et al\\.,? \\Q2015\\E", "shortCiteRegEx": "al. et al\\.", "year": 2015}, {"title": "Generative image modeling using style and structure adversarial networks. arXiv preprint arXiv:1603.05631", "author": ["Wang", "Gupta", "X. 2016] Wang", "A. Gupta"], "venue": null, "citeRegEx": "Wang et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Wang et al\\.", "year": 2016}, {"title": "Sketch-a-net that beats humans", "author": ["Yu et al", "Q. 2015] Yu", "Y. Yang", "Song", "Y.-Z", "T. Xiang", "T.M. Hospedales"], "venue": "In Proc. of BMVC,", "citeRegEx": "al. et al\\.,? \\Q2015\\E", "shortCiteRegEx": "al. et al\\.", "year": 2015}, {"title": "Generative visual manipulation on the natural image manifold", "author": ["Zhu et al", "J. 2016] Zhu", "P. Kr\u00e4henb\u00fchl", "E. Shechtman", "A. Efros"], "venue": null, "citeRegEx": "al. et al\\.,? \\Q2016\\E", "shortCiteRegEx": "al. et al\\.", "year": 2016}], "referenceMentions": [], "year": 2017, "abstractText": "Understanding, predicting, and generating object motions and transformations is a core problem in artificial intelligence. Modeling sequences of evolving images may provide better representations and models of motion and may ultimately be used for forecasting, simulation, or video generation. Diagrammatic Abstract Reasoning is an avenue in which diagrams evolve in complex patterns and one needs to infer the underlying pattern sequence and generate the next image in the sequence. For this, we develop a novel Contextual Generative Adversarial Network based on Recurrent Neural Networks (Context-RNN-GANs), where both the generator and the discriminator modules are based on contextual history (modeled as RNNs) and the adversarial discriminator guides the generator to produce realistic images for the particular time step in the image sequence. We evaluate the Context-RNN-GAN model (and its variants) on a novel dataset of Diagrammatic Abstract Reasoning, where it performs competitively with 10th-grade human performance but there is still scope for interesting improvements as compared to college-grade human performance. We also evaluate our model on a standard video next-frame prediction task, achieving improved performance over comparable state-of-the-art.", "creator": "TeX"}}}