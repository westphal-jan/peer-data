{"id": "1609.09481", "review": {"conference": "NIPS", "VERSION": "v1", "DATE_OF_SUBMISSION": "29-Sep-2016", "title": "Fast learning rates with heavy-tailed losses", "abstract": "We study fast learning rates when the losses are not necessarily bounded and may have a distribution with heavy tails. To enable such analyses, we introduce two new conditions: (i) the envelope function $\\sup_{f \\in \\mathcal{F}}|\\ell \\circ f|$, where $\\ell$ is the loss function and $\\mathcal{F}$ is the hypothesis class, exists and is $L^r$-integrable, and (ii) $\\ell$ satisfies the multi-scale Bernstein's condition on $\\mathcal{F}$. Under these assumptions, we prove that learning rate faster than $O(n^{-1/2})$ can be obtained and, depending on $r$ and the multi-scale Bernstein's powers, can be arbitrarily close to $O(n^{-1})$. We then verify these assumptions and derive fast learning rates for the problem of vector quantization by $k$-means clustering with heavy-tailed distributions. The analyses enable us to obtain novel learning rates that extend and complement existing results in the literature from both theoretical and practical viewpoints.", "histories": [["v1", "Thu, 29 Sep 2016 19:46:13 GMT  (18kb)", "http://arxiv.org/abs/1609.09481v1", "Advances in Neural Information Processing Systems (NIPS 2016): 11 pages"]], "COMMENTS": "Advances in Neural Information Processing Systems (NIPS 2016): 11 pages", "reviews": [], "SUBJECTS": "stat.ML cs.LG", "authors": ["vu c dinh", "lam s ho", "binh t nguyen", "duy m h nguyen"], "accepted": true, "id": "1609.09481"}, "pdf": {"name": "1609.09481.pdf", "metadata": {"source": "CRF", "title": "Fast learning rates with heavy-tailed losses", "authors": ["Vu Dinh", "Duy Nguyen", "Binh T. Nguyen"], "emails": [], "sections": [{"heading": null, "text": "ar Xiv: 160 9.09 481v 1 [stat.ML] 2 9SE p"}, {"heading": "1 Introduction", "text": "The rate at which a learning algorithm solves this problem includes Brownlees al. (2015), which play a central role in machine learning. (Recent advances have refined our theoretical understanding of the conditions under which fast learning rates are possible, leading to the development of robust algorithms that can automatically adapt to data with hidden structures and achieve faster rates whenever possible.) However, the literature has mainly focused on limited losses and little was known about the rates of learning in the unlimited cases, especially in cases where the distribution of loss has heavy tails (van Erven et al.), where most previous work on unlimited losses in the context of learning density is carried out (van Erven et al., 2015; Zhang, 2006a, b), of which the fast rate proofs implicitly address the central condition (Green Forest, 2012) and cannot be extended to address losses with polynoal tails (van Erven et al)."}, {"heading": "2 Mathematical framework", "text": "Let's use the loss class hypothesis (Z = f), which we consider an optimal hypothesis. (F) Let's use the loss class hypothesis (F = f), which we consider to be an optimal hypothesis. (F) Let's use the loss zone hypothesis (F = f), which we consider to be a non-negative function. (F) Let's use the loss zone hypothesis (F = f), which we consider to be a non-supervised learning situation, there is no output (Y = f) and the loss zone hypothesis (X, f) depending on the application. (F) Nevertheless, Pn (F) and Pn (F) we can similarly misuse the notation to denote the losses (f)."}, {"heading": "3 Fast learning rates with heavy-tailed losses", "text": "The derivation of the fast learning rate with high losses proceeds as follows: First, we will use the assumption of an integrable envelope function to prove a localization-based result that allows us to reduce the analyses from the separable parametric classes F to their final net result F. The multi-scale amber condition is then used to derive a rapid inequality that helps distinguish the optimal hypothesis from alternative hypotheses in F. The two results are then combined to obtain fast learning rates."}, {"heading": "3.1 Preliminaries", "text": "Throughout this section, G1 < \u2212 Z \u2212 Z becomes the projection of K (t, K (g0), L2 (P), (P), (P), (K), (P), (K), (K), (K), (K), (K), (K), (K / t), (K), (K), (2), (K), (Z), (K), (K), (K), (K), (K), (K), (K), (K), (K), (K), (K), (K), (K), (K), (K), (K), (K), (K), (K), (K), (), (K)."}, {"heading": "3.2 Local analysis of the empirical loss", "text": "The preliminary results of the study show that we are able to integrate ourselves in the region. (Pn \u2212 P) u as follows: \"A\" (Pn \u2212 P) u \"K\" (Pn \u2212 P) u \"K\" (Pn \u2212 P) u \"K\" (Pn \u2212 P) u \"K\" (Pn \u2212 P) u \"K\" (Pn \u2212 P) u \"K\" (Pn \u2212 P) u \"K\" (Pn \u2212 P) u \"(Pn \u2212 P) u\" K \"(Pn \u2212 P) u\" K \"(Pn \u2212 P) u\" K \"(Pn \u2212 P) u\" K \"n\" (Pn \u2212 P) (Pn \u2212 P) u (Pn \u2212 P) u (Pn \u2212 P) u (Pn \u2212 P) u (Pn \u2212 P) u \"K\" n (Pn) (Pn \u2212 P) (Pn \u2212 P) \u2212 P) u \"K\" (Pn) (Pn \u2212 P) (Pn \u2212 P) (Pn \u2212 P) \u2212 P \"n (Pn) (Pn \u2212 P) (Pn \u2212 P) n\" n \"n\" n \"K\" n \"n\" n \"n\" n (Pu u \u2212 P) u (Pu \u2212 P \u2212 P) (Pn \u2212 P \u2212 P \u2212 P) \u2212 P (Pn) (Pn \u2212 P \u2212 P \u2212 P \u2212 P \u2212 P \u2212 P) u \"n (Pn) (Pn) (Pn \u2212 P \u2212 P \u2212 P \u2212 P \u2212 P \u2212 P \u2212 P \u2212 P \u2212 P \u2212 P \u2212 P \u2212 P) u\" K \"n\" n \"n\" n \"n\" n \"n\" n \"n\" K \"n\" n \"n\" n \"n\" n \"n\" n \"n\" n \"n\" n \"n\" n \"n\" n \"n\" n \"n\" n \"n\" n \"n\" n \"n\" n \"n\" K \"n\" n \"n\" n \"n\" n \"n) (Pn) (Pn \u2212 P \u2212 P \u2212 P \u2212 P \u2212 P \u2212 P \u2212 P \u2212 P \u2212 P \u2212 P \u2212 P \u2212 P \u2212 P \u2212 P \u2212 P \u2212 P\" n) (\u2212 P \u2212 P \u2212"}, {"heading": "3.3 Fast learning rates with heavy-tailed losses", "text": "Theorem 3.2. Given a0, \u03b4 > 0. Under the multiple scale (B, \u03b2, I) -Amber's condition and assuming that r, 4C, consider0 < \u03b2 < (1 \u2212 2, C / r) / (2 \u2212 2, C / r) / (2 \u2212 3, P, P, P, P, P, P, Pnf, F, F, F, F, F, F, F, F, F, F, F, F, F, F, F, F, F, F, F, F, F, F, F, F, F, F, F, F, F, F, F, F, F, F, F, F, F, F, F, F, F, F, F, F, F, F, F, F, F, F, F, F, F, F, F, F, F, F, F, F, F, F, F, F, F, F"}, {"heading": "3.4 Verifying the multi-scale Bernstein\u2019s condition", "text": "In practice, the most difficult condition for rapid learning rates is the multi-level function of Amber. In this section, we derive some approaches to verify the state. We first extend the result of Mendelson (2008) to prove that the state of Amber is automatically fulfilled for functions that are relatively far from f. We remember that R (f) = E (f) is called a risk function. Lemma 3,6. Assuming 2,3, we define M = W r / (r \u2212 2) and \u03b3 = (r \u2212 2) / (r \u2212 1). Then, if \u03b1 > M and R (f)."}, {"heading": "3.5 Comparison to related work", "text": "Theory 3.3 dictates that under our conditions, the problem of learning with high losses (i.e. with high losses) does not affect the convergence rates up to orderO (1 \u2212 2 \u00b7 C / r) / (2 \u2212 min {\u03b3})) (3.4), where the number of failures in the same constellation is responsible for limited losses, and r is the degree of integrability of the loss. (The analysis there was carried out under the aspect of weak stochastic miscibility, which is equivalent to the standard D condition for limited losses (van Erven et al, 2015). We note that if the loss is limited, r = 3.4), the convergence rate in Mehta and Williamson (2014) is reduced."}, {"heading": "5 Discussion and future work", "text": "We have shown that rapid learning rates for severe losses can be achieved for hypotheses classes with an integral envelope, if the loss meets the state of Amber on several levels. We then review these conditions and obtain new convergence rates for k-means, which group together with severe losses. The analyses expand and supplement existing results in the literature from both a theoretical and a practical point of view. We also introduce a new assumption at fast rates, the state of Amber on several levels, and offer a clear way to verify the assumption in practice. We believe that the state of Amber on several levels is the correct assumption to study fast rates for unlimited losses, for its ability to separate the behavior of the risk function on microscopic and macroscopic scales, for which the distinction can only be observed in an unlimited environment. There are several opportunities for improvement. First, we would like to consider hypotheses classes with polynomial entropy."}], "references": [{"title": "Individual convergence rates in empirical vector quantizer design", "author": ["Andr\u00e1s Antos", "L\u00e1szl\u00f3 Gy\u00f6rfi", "Andr\u00e1s Gy\u00f6rgy"], "venue": "IEEE Transactions on Information Theory,", "citeRegEx": "Antos et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Antos et al\\.", "year": 2005}, {"title": "The minimax distortion redundancy in empirical quantizer design", "author": ["Peter L Bartlett", "Tam\u00e1s Linder", "G\u00e1bor Lugosi"], "venue": "IEEE Transactions on Information Theory,", "citeRegEx": "Bartlett et al\\.,? \\Q1998\\E", "shortCiteRegEx": "Bartlett et al\\.", "year": 1998}, {"title": "A framework for statistical clustering with constant time approximation algorithms for k-median and k-means clustering", "author": ["Shai Ben-David"], "venue": "Machine Learning,", "citeRegEx": "Ben.David.,? \\Q2007\\E", "shortCiteRegEx": "Ben.David.", "year": 2007}, {"title": "Concentration inequalities: A nonasymptotic theory of independence", "author": ["St\u00e9phane Boucheron", "G\u00e1bor Lugosi", "Pascal Massart"], "venue": "OUP Oxford,", "citeRegEx": "Boucheron et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Boucheron et al\\.", "year": 2013}, {"title": "Introduction to statistical learning theory", "author": ["Olivier Bousquet", "St\u00e9phane Boucheron", "G\u00e1bor Lugosi"], "venue": "In Advanced lectures on machine learning,", "citeRegEx": "Bousquet et al\\.,? \\Q2004\\E", "shortCiteRegEx": "Bousquet et al\\.", "year": 2004}, {"title": "Empirical risk minimization for heavy-tailed losses", "author": ["Christian Brownlees", "Emilien Joly", "G\u00e1bor Lugosi"], "venue": "The Annals of Statistics,", "citeRegEx": "Brownlees et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Brownlees et al\\.", "year": 2015}, {"title": "Relative deviation learning bounds and generalization with unbounded loss functions", "author": ["Corinna Cortes", "Spencer Greenberg", "Mehryar Mohri"], "venue": null, "citeRegEx": "Cortes et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Cortes et al\\.", "year": 2013}, {"title": "Learning from non-iid data: Fast rates for the one-vs-all multiclass plug-in classifiers", "author": ["Vu Dinh", "Lam Si Tung Ho", "Nguyen Viet Cuong", "Duy Nguyen", "Binh T Nguyen"], "venue": "In Theory and Applications of Models of Computation,", "citeRegEx": "Dinh et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Dinh et al\\.", "year": 2015}, {"title": "The safe Bayesian: learning the learning rate via the mixability gap", "author": ["Peter Gr\u00fcnwald"], "venue": "In Proceedings of the 23rd international conference on Algorithmic Learning Theory,", "citeRegEx": "Gr\u00fcnwald.,? \\Q2012\\E", "shortCiteRegEx": "Gr\u00fcnwald.", "year": 2012}, {"title": "Fast learning from \u03b1-mixing observations", "author": ["Hanyuan Hang", "Ingo Steinwart"], "venue": "Journal of Multivariate Analysis,", "citeRegEx": "Hang and Steinwart.,? \\Q2014\\E", "shortCiteRegEx": "Hang and Steinwart.", "year": 2014}, {"title": "Loss minimization and parameter estimation with heavy tails", "author": ["Daniel Hsu", "Sivan Sabato"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Hsu and Sabato.,? \\Q2016\\E", "shortCiteRegEx": "Hsu and Sabato.", "year": 2016}, {"title": "General nonexact oracle inequalities for classes with a sub-exponential envelope", "author": ["Guillaume Lecu\u00e9", "Shahar Mendelson"], "venue": "The Annals of Statistics,", "citeRegEx": "Lecu\u00e9 and Mendelson.,? \\Q2012\\E", "shortCiteRegEx": "Lecu\u00e9 and Mendelson.", "year": 2012}, {"title": "Learning sub-Gaussian classes: Upper and minimax bounds", "author": ["Guillaume Lecu\u00e9", "Shahar Mendelson"], "venue": null, "citeRegEx": "Lecu\u00e9 and Mendelson.,? \\Q2013\\E", "shortCiteRegEx": "Lecu\u00e9 and Mendelson.", "year": 2013}, {"title": "New concentration inequalities for suprema of empirical processes", "author": ["Johannes Lederer", "Sara van de Geer"], "venue": null, "citeRegEx": "Lederer and Geer.,? \\Q2020\\E", "shortCiteRegEx": "Lederer and Geer.", "year": 2020}, {"title": "Fast rates for empirical vector quantization", "author": ["Cl\u00e9ment Levrard"], "venue": "Electronic Journal of Statistics,", "citeRegEx": "Levrard.,? \\Q2013\\E", "shortCiteRegEx": "Levrard.", "year": 2013}, {"title": "Rates of convergence in the source coding theorem, in empirical quantizer design, and in universal lossy source coding", "author": ["Tam\u00e1s Linder", "G\u00e1bor Lugosi", "Kenneth Zeger"], "venue": "IEEE Transactions on Information Theory,", "citeRegEx": "Linder et al\\.,? \\Q1994\\E", "shortCiteRegEx": "Linder et al\\.", "year": 1994}, {"title": "From stochastic mixability to fast rates", "author": ["Nishant A Mehta", "Robert C Williamson"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Mehta and Williamson.,? \\Q2014\\E", "shortCiteRegEx": "Mehta and Williamson.", "year": 2014}, {"title": "Obtaining fast error rates in nonconvex situations", "author": ["Shahar Mendelson"], "venue": "Journal of Complexity,", "citeRegEx": "Mendelson.,? \\Q2008\\E", "shortCiteRegEx": "Mendelson.", "year": 2008}, {"title": "A central limit theorem for k-means clustering", "author": ["David Pollard"], "venue": "The Annals of Probability,", "citeRegEx": "Pollard.,? \\Q1982\\E", "shortCiteRegEx": "Pollard.", "year": 1982}, {"title": "Fast learning from non-iid observations", "author": ["Ingo Steinwart", "Andreas Christmann"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Steinwart and Christmann.,? \\Q2009\\E", "shortCiteRegEx": "Steinwart and Christmann.", "year": 2009}, {"title": "Moment-based uniform deviation bounds for k-means and friends", "author": ["Matus J Telgarsky", "Sanjoy Dasgupta"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Telgarsky and Dasgupta.,? \\Q2013\\E", "shortCiteRegEx": "Telgarsky and Dasgupta.", "year": 2013}, {"title": "Fast rates in statistical and online learning", "author": ["Tim van Erven", "Peter D Gr\u00fcnwald", "Nishant A Mehta", "Mark D Reid", "Robert C Williamson"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Erven et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Erven et al\\.", "year": 2015}, {"title": "From \u01eb-entropy to KL-entropy: Analysis of minimum information complexity density estimation", "author": ["Tong Zhang"], "venue": "The Annals of Statistics,", "citeRegEx": "Zhang.,? \\Q2006\\E", "shortCiteRegEx": "Zhang.", "year": 2006}, {"title": "Information-theoretic upper and lower bounds for statistical estimation", "author": ["Tong Zhang"], "venue": "IEEE Transactions on Information Theory,", "citeRegEx": "Zhang.,? \\Q2006\\E", "shortCiteRegEx": "Zhang.", "year": 2006}], "referenceMentions": [{"referenceID": 8, "context": ", 2015; Zhang, 2006a,b), of which the proofs of fast rates implicitly employ the central condition (Gr\u00fcnwald, 2012) and cannot be extended to address losses with polynomial tails (van Erven et al.", "startOffset": 99, "endOffset": 115}, {"referenceID": 5, "context": "Efforts to resolve this issue include Brownlees et al. (2015), which proposes using some robust mean estimators to replace empirical means, and Cortes et al.", "startOffset": 38, "endOffset": 62}, {"referenceID": 5, "context": "Efforts to resolve this issue include Brownlees et al. (2015), which proposes using some robust mean estimators to replace empirical means, and Cortes et al. (2013), which derives relative deviation and generalization bounds for unbounded losses with the assumption that L-diameter of the hypothesis class is bounded.", "startOffset": 38, "endOffset": 165}, {"referenceID": 5, "context": "Efforts to resolve this issue include Brownlees et al. (2015), which proposes using some robust mean estimators to replace empirical means, and Cortes et al. (2013), which derives relative deviation and generalization bounds for unbounded losses with the assumption that L-diameter of the hypothesis class is bounded. However, results about fast learning rates were not obtained in both approaches. Fast learning rates are derived in Lecu\u00e9 and Mendelson (2013) for sub-Gaussian losses and in Lecu\u00e9 and Mendelson (2012) for hypothesis classes that have sub-exponential envelope functions.", "startOffset": 38, "endOffset": 461}, {"referenceID": 5, "context": "Efforts to resolve this issue include Brownlees et al. (2015), which proposes using some robust mean estimators to replace empirical means, and Cortes et al. (2013), which derives relative deviation and generalization bounds for unbounded losses with the assumption that L-diameter of the hypothesis class is bounded. However, results about fast learning rates were not obtained in both approaches. Fast learning rates are derived in Lecu\u00e9 and Mendelson (2013) for sub-Gaussian losses and in Lecu\u00e9 and Mendelson (2012) for hypothesis classes that have sub-exponential envelope functions.", "startOffset": 38, "endOffset": 519}, {"referenceID": 0, "context": "The result can be viewed as an extension of the result from Antos et al. (2005) and Levrard (2013) to cases when the source distribution has unbounded support, and produces a more favorable convergence rate than that of Telgarsky and Dasgupta (2013) under similar settings.", "startOffset": 60, "endOffset": 80}, {"referenceID": 0, "context": "The result can be viewed as an extension of the result from Antos et al. (2005) and Levrard (2013) to cases when the source distribution has unbounded support, and produces a more favorable convergence rate than that of Telgarsky and Dasgupta (2013) under similar settings.", "startOffset": 60, "endOffset": 99}, {"referenceID": 0, "context": "The result can be viewed as an extension of the result from Antos et al. (2005) and Levrard (2013) to cases when the source distribution has unbounded support, and produces a more favorable convergence rate than that of Telgarsky and Dasgupta (2013) under similar settings.", "startOffset": 60, "endOffset": 250}, {"referenceID": 3, "context": "2 is satisfied (Boucheron et al., 2013; Bousquet et al., 2004).", "startOffset": 15, "endOffset": 62}, {"referenceID": 4, "context": "2 is satisfied (Boucheron et al., 2013; Bousquet et al., 2004).", "startOffset": 15, "endOffset": 62}, {"referenceID": 3, "context": "2 is satisfied (Boucheron et al., 2013; Bousquet et al., 2004). Both Bernstein\u2019s condition and the assumption of separable parametric hypothesis class are standard assumptions frequently used to obtain faster learning rates in agnostic settings. A review about the Bernstein\u2019s condition and its applications is Mendelson (2008), while fast learning rates for bounded losses on hypothesis classes satisfying Assumptions 2.", "startOffset": 16, "endOffset": 328}, {"referenceID": 3, "context": "2 is satisfied (Boucheron et al., 2013; Bousquet et al., 2004). Both Bernstein\u2019s condition and the assumption of separable parametric hypothesis class are standard assumptions frequently used to obtain faster learning rates in agnostic settings. A review about the Bernstein\u2019s condition and its applications is Mendelson (2008), while fast learning rates for bounded losses on hypothesis classes satisfying Assumptions 2.2 were previously studied in Mehta and Williamson (2014) under the stochastic mixability condition.", "startOffset": 16, "endOffset": 478}, {"referenceID": 3, "context": "2 is satisfied (Boucheron et al., 2013; Bousquet et al., 2004). Both Bernstein\u2019s condition and the assumption of separable parametric hypothesis class are standard assumptions frequently used to obtain faster learning rates in agnostic settings. A review about the Bernstein\u2019s condition and its applications is Mendelson (2008), while fast learning rates for bounded losses on hypothesis classes satisfying Assumptions 2.2 were previously studied in Mehta and Williamson (2014) under the stochastic mixability condition. Fast learning rate for hypothesis classes with envelope functions were studied in Lecu\u00e9 and Mendelson (2012), but under a much stronger assumption that the envelope function is sub-exponential.", "startOffset": 16, "endOffset": 630}, {"referenceID": 3, "context": "2 is satisfied (Boucheron et al., 2013; Bousquet et al., 2004). Both Bernstein\u2019s condition and the assumption of separable parametric hypothesis class are standard assumptions frequently used to obtain faster learning rates in agnostic settings. A review about the Bernstein\u2019s condition and its applications is Mendelson (2008), while fast learning rates for bounded losses on hypothesis classes satisfying Assumptions 2.2 were previously studied in Mehta and Williamson (2014) under the stochastic mixability condition. Fast learning rate for hypothesis classes with envelope functions were studied in Lecu\u00e9 and Mendelson (2012), but under a much stronger assumption that the envelope function is sub-exponential. Under these assumptions, we illustrate that fast rates for heavy-tailed losses can be obtained. Throughout the analyses, two recurrent analytical techniques are worth mentioning. The first comes from the simple observation that in the standard derivation of fast learning rates for bounded losses, the boundedness assumption is used in multiple places only to provide reverse-Holder-type inequalities, where the L2-norm are upper bounded by the L1-norm. This use of the boundedness assumption can be simply relieved by the assumption that the Lr-norm of the loss is bounded, which implies \u2016u\u2016L2 \u2264 \u2016u\u2016(r\u22122)/(2r\u22122) L1 \u2016u\u2016 r/(2r\u22122) Lr . The second technique relies on the following results of Lederer and van de Geer (2014) on concentration inequalities for suprema of empirical unbounded processes.", "startOffset": 16, "endOffset": 1436}, {"referenceID": 15, "context": "The arguments to extend the bound from countable classes to separable classes are standard (see, for example, Lemma 12 of Mehta and Williamson (2014)).", "startOffset": 122, "endOffset": 150}, {"referenceID": 3, "context": "2 in Boucheron et al. (2013)), we have", "startOffset": 5, "endOffset": 29}, {"referenceID": 17, "context": "We first extend the result of Mendelson (2008) to prove that the (standard) Bernstein\u2019s condition is automatically satisfied for functions that are relatively far way from f\u2217 under the integrability condition of the envelope function (proof in the Appendix).", "startOffset": 30, "endOffset": 47}, {"referenceID": 14, "context": "We recall that convergence rate of O(n\u22121/(2\u2212\u03b3)) is obtained in Mehta and Williamson (2014) under the same setting but for bounded losses.", "startOffset": 63, "endOffset": 91}, {"referenceID": 14, "context": "We recall that convergence rate of O(n\u22121/(2\u2212\u03b3)) is obtained in Mehta and Williamson (2014) under the same setting but for bounded losses. (The analysis there was done under the \u03b3weakly stochastic mixability condition, which is equivalent with the standard \u03b3-Bernstein\u2019s condition for bounded losses (van Erven et al., 2015)). We note that if the loss is bounded, r = \u221e and (3.4) reduces to the convergence rate obtained in Mehta and Williamson (2014). Fast learning rates for unbounded loses are previously derived in Lecu\u00e9 and Mendelson (2013) for sub-Gaussian losses and in Lecu\u00e9 and Mendelson (2012) for hypothesis classes that have sub-exponential envelope functions.", "startOffset": 63, "endOffset": 451}, {"referenceID": 11, "context": "Fast learning rates for unbounded loses are previously derived in Lecu\u00e9 and Mendelson (2013) for sub-Gaussian losses and in Lecu\u00e9 and Mendelson (2012) for hypothesis classes that have sub-exponential envelope functions.", "startOffset": 66, "endOffset": 93}, {"referenceID": 11, "context": "Fast learning rates for unbounded loses are previously derived in Lecu\u00e9 and Mendelson (2013) for sub-Gaussian losses and in Lecu\u00e9 and Mendelson (2012) for hypothesis classes that have sub-exponential envelope functions.", "startOffset": 66, "endOffset": 151}, {"referenceID": 11, "context": "Fast learning rates for unbounded loses are previously derived in Lecu\u00e9 and Mendelson (2013) for sub-Gaussian losses and in Lecu\u00e9 and Mendelson (2012) for hypothesis classes that have sub-exponential envelope functions. In Lecu\u00e9 and Mendelson (2013), the Bernstein\u2019s condition is not directly imposed, but is replaced by condition (ii) of Lemma 3.", "startOffset": 66, "endOffset": 250}, {"referenceID": 11, "context": "Fast learning rates for unbounded loses are previously derived in Lecu\u00e9 and Mendelson (2013) for sub-Gaussian losses and in Lecu\u00e9 and Mendelson (2012) for hypothesis classes that have sub-exponential envelope functions. In Lecu\u00e9 and Mendelson (2013), the Bernstein\u2019s condition is not directly imposed, but is replaced by condition (ii) of Lemma 3.7 with m = 2 on the whole hypothesis class, while the assumption of sub-Gaussian hypothesis class validates condition (i). This implies the standard Bernstein\u2019s condition with \u03b3 = 1 and makes the convergence rate O(n\u22121) consistent with our result (note that for sub-Gaussian losses, r can be chosen arbitrary large). The analysis of Lecu\u00e9 and Mendelson (2012) concerns about non-exact oracle inequalities (rather than the sharp oracle inequalities we investigate in this paper) and can not be directly compared with our results.", "startOffset": 66, "endOffset": 707}, {"referenceID": 18, "context": "The rate of convergence of k-means clustering has drawn considerable attention in the statistics and machine learning literatures (Pollard, 1982; Bartlett et al., 1998; Linder et al., 1994; Ben-David, 2007).", "startOffset": 130, "endOffset": 206}, {"referenceID": 1, "context": "The rate of convergence of k-means clustering has drawn considerable attention in the statistics and machine learning literatures (Pollard, 1982; Bartlett et al., 1998; Linder et al., 1994; Ben-David, 2007).", "startOffset": 130, "endOffset": 206}, {"referenceID": 15, "context": "The rate of convergence of k-means clustering has drawn considerable attention in the statistics and machine learning literatures (Pollard, 1982; Bartlett et al., 1998; Linder et al., 1994; Ben-David, 2007).", "startOffset": 130, "endOffset": 206}, {"referenceID": 2, "context": "The rate of convergence of k-means clustering has drawn considerable attention in the statistics and machine learning literatures (Pollard, 1982; Bartlett et al., 1998; Linder et al., 1994; Ben-David, 2007).", "startOffset": 130, "endOffset": 206}, {"referenceID": 0, "context": "Fast learning rates for k-means clustering (O(1/n)) have also been derived by Antos et al. (2005) in the case when the source distribution is supported on a finite set of points, and by Levrard (2013) under the assumptions that the source distribution has bounded support and satisfies the so-called Pollard\u2019s regularity condition, which dictates that P has a continuous density with respect to the Lebesgue measure and the Hessian matrix of the mapping C \u2192 R(C) is positive definite at C\u2217.", "startOffset": 78, "endOffset": 98}, {"referenceID": 0, "context": "Fast learning rates for k-means clustering (O(1/n)) have also been derived by Antos et al. (2005) in the case when the source distribution is supported on a finite set of points, and by Levrard (2013) under the assumptions that the source distribution has bounded support and satisfies the so-called Pollard\u2019s regularity condition, which dictates that P has a continuous density with respect to the Lebesgue measure and the Hessian matrix of the mapping C \u2192 R(C) is positive definite at C\u2217.", "startOffset": 78, "endOffset": 201}, {"referenceID": 0, "context": "Fast learning rates for k-means clustering (O(1/n)) have also been derived by Antos et al. (2005) in the case when the source distribution is supported on a finite set of points, and by Levrard (2013) under the assumptions that the source distribution has bounded support and satisfies the so-called Pollard\u2019s regularity condition, which dictates that P has a continuous density with respect to the Lebesgue measure and the Hessian matrix of the mapping C \u2192 R(C) is positive definite at C\u2217. Little is known about the finite-sample performance of empirically designed quantizers under possibly heavy-tailed distributions. In Telgarsky and Dasgupta (2013), a convergence rate ofO(n\u22121/2+2/r) are derived, where r is the number of moments of X that are assumed to be finite.", "startOffset": 78, "endOffset": 654}, {"referenceID": 0, "context": "Fast learning rates for k-means clustering (O(1/n)) have also been derived by Antos et al. (2005) in the case when the source distribution is supported on a finite set of points, and by Levrard (2013) under the assumptions that the source distribution has bounded support and satisfies the so-called Pollard\u2019s regularity condition, which dictates that P has a continuous density with respect to the Lebesgue measure and the Hessian matrix of the mapping C \u2192 R(C) is positive definite at C\u2217. Little is known about the finite-sample performance of empirically designed quantizers under possibly heavy-tailed distributions. In Telgarsky and Dasgupta (2013), a convergence rate ofO(n\u22121/2+2/r) are derived, where r is the number of moments of X that are assumed to be finite. Brownlees et al. (2015) uses some robust mean estimators to replace empirical means and derives a convergence rate of O(n\u22121/2) assuming only that the variance of X is finite.", "startOffset": 78, "endOffset": 795}, {"referenceID": 5, "context": "Following the framework of Brownlees et al. (2015), we consider G = {l(C, x) = min yi\u2208C \u2016x\u2212 yi\u2016, C \u2208 F = (\u2212\u03c1, \u03c1)d\u00d7k} for some \u03c1 > 0 with the regular Euclidean metric.", "startOffset": 27, "endOffset": 51}, {"referenceID": 15, "context": "while standard results about VC-dimension of k-means clustering hypothesis class guarantees that C \u2264 k(d+ 1) (Linder et al., 1994).", "startOffset": 109, "endOffset": 130}, {"referenceID": 19, "context": "Similarly, the condition of independent and identically distributed observations can be replaced with mixing properties (Steinwart and Christmann, 2009; Hang and Steinwart, 2014; Dinh et al., 2015).", "startOffset": 120, "endOffset": 197}, {"referenceID": 9, "context": "Similarly, the condition of independent and identically distributed observations can be replaced with mixing properties (Steinwart and Christmann, 2009; Hang and Steinwart, 2014; Dinh et al., 2015).", "startOffset": 120, "endOffset": 197}, {"referenceID": 7, "context": "Similarly, the condition of independent and identically distributed observations can be replaced with mixing properties (Steinwart and Christmann, 2009; Hang and Steinwart, 2014; Dinh et al., 2015).", "startOffset": 120, "endOffset": 197}, {"referenceID": 6, "context": "While the condition of integrable envelope is an improvement from the condition of sub-exponential envelope previously investigated in the literature, it would be interesting to see if the rates retain under weaker conditions, for example, the assumption that the L-diameter of the hypothesis class is bounded (Cortes et al., 2013).", "startOffset": 310, "endOffset": 331}, {"referenceID": 5, "context": "Finally, the recent work of Brownlees et al. (2015); Hsu and Sabato (2016) about robust estimators as alternatives of ERM to study heavy-tailed losses has yielded more favorable learning rates under weaker conditions, and we would like to extend the result in this paper to study such estimators.", "startOffset": 28, "endOffset": 52}, {"referenceID": 5, "context": "Finally, the recent work of Brownlees et al. (2015); Hsu and Sabato (2016) about robust estimators as alternatives of ERM to study heavy-tailed losses has yielded more favorable learning rates under weaker conditions, and we would like to extend the result in this paper to study such estimators.", "startOffset": 28, "endOffset": 75}], "year": 2016, "abstractText": "We study fast learning rates when the losses are not necessarily bounded and may have a distribution with heavy tails. To enable such analyses, we introduce two new conditions: (i) the envelope function supf\u2208F |l \u25e6 f |, where l is the loss function and F is the hypothesis class, exists and is Lintegrable, and (ii) l satisfies the multi-scale Bernstein\u2019s condition on F . Under these assumptions, we prove that learning rate faster than O(n\u22121/2) can be obtained and, depending on r and the multi-scale Bernstein\u2019s powers, can be arbitrarily close to O(n\u22121). We then verify these assumptions and derive fast learning rates for the problem of vector quantization by k-means clustering with heavy-tailed distributions. The analyses enable us to obtain novel learning rates that extend and complement existing results in the literature from both theoretical and practical viewpoints.", "creator": "LaTeX with hyperref package"}}}