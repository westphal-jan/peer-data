{"id": "1610.00085", "review": {"conference": "AAAI", "VERSION": "v1", "DATE_OF_SUBMISSION": "1-Oct-2016", "title": "Latent Tree Analysis", "abstract": "Latent tree analysis seeks to model the correlations among a set of random variables using a tree of latent variables. It was proposed as an improvement to latent class analysis --- a method widely used in social sciences and medicine to identify homogeneous subgroups in a population. It provides new and fruitful perspectives on a number of machine learning areas, including cluster analysis, topic detection, and deep probabilistic modeling. This paper gives an overview of the research on latent tree analysis and various ways it is used in practice.", "histories": [["v1", "Sat, 1 Oct 2016 04:26:48 GMT  (1383kb,D)", "http://arxiv.org/abs/1610.00085v1", "7 pages, 5 figures"]], "COMMENTS": "7 pages, 5 figures", "reviews": [], "SUBJECTS": "cs.LG", "authors": ["nevin l zhang", "leonard k m poon"], "accepted": true, "id": "1610.00085"}, "pdf": {"name": "1610.00085.pdf", "metadata": {"source": "CRF", "title": "Latent Tree Analysis", "authors": ["Nevin L. Zhang", "Leonard K. M. Poon"], "emails": ["lzhang@cse.ust.hk", "kmpoon@eduhk.hk"], "sections": [{"heading": null, "text": "In fact, most of them are able to survive on their own if they do not put themselves in a position to survive on their own."}, {"heading": "Preliminaries", "text": "A latent tree model (LTM) is a tree-structured Bayesian network (Pearl, 1988) in which the leaf nodes represent observed variables and the internal nodes represent latent variables. An example is shown in Figure 1 (a). All variables are assumed to be discrete. Model parameters include a marginal distribution for the root Y1 and a conditional distribution for each of the other nodes due to its parentage. The product of the distributions defines a common distribution across all variables. By changing the root from Y1 to Y2 in Figure 1 (a) we obtain another model in Figure 1 (b). The two models are equivalent in the sense that they have the same set of distributions over the observed variables X1,.. X5 (Zhang, 2004). It is not possible to distinguish between equivalent models based on data."}, {"heading": "Learning Latent Tree Models", "text": "To match an LTM to a data set, one must determine: (1) the number of latent variables, (2) the number of possible states for each latent variable, (3) the connections between all variables, and (4) the probability distributions. There are three commonly used algorithms for learning LTMs: EAST (Expansion, Adjustment and Simplification until Termination) (Chen et al., 2012), BI (Bridged Islands) (Liu et al., 2013), and CLRG (Chow-Liu and Recursive Grouping) (Choi et al., 2011). EAST is a search-based algorithm aimed at finding the model with the highest BICScore. It is the slowest of the three and finds better models, measured by probability, than the other two algorithms."}, {"heading": "Improving Latent Class Analysis", "text": "This year it has come to the point that it has never come as far as this year."}, {"heading": "Multidimensional Clustering", "text": "Complex data generally has multiple facets and can be shared meaningfully in several ways. For example, a student population can be confined in one way to academic achievement and another to extracurricular activities. Movie ratings can be grouped based on feelings (positive or negative) or genres (corruption, action, war, etc.) The respondents in a social survey can be clustered based on demographic information or views on social issues. Efforts are being made to develop clustering algorithms that produce multiple partitions of data, with each partition based solely or primarily on a different subset of attributes. We call them multidimensional clustering methods, which should not be confused with multiview methods that combine information from different views of data to improve the quality of a single partition.) There are sequential methods that aim to obtain additional partitions of data that are novel."}, {"heading": "Hierarchical Topic Detection", "text": "This year it is so far that it will only take one year to move on to the next round."}, {"heading": "Deep Probabilistic Modeling", "text": "This year, we have reached the point where we feel we are able to live in a country where most people are able to live in a country where they are able, where they are able to move, and where they are able to move."}, {"heading": "Other Applications", "text": "LTMs are used as a tool for generalizing probabilities about discrete variables. Here, one works with a common distribution over a set of variables and draws conclusions to calculate the posterior distribution of variables (Pearl, 1988). It is technically challenging that the number of variables is exponential. The idea is to build an LTM with the observed variables, and it follows that the inference with the LTMs has two attractive properties."}, {"heading": "Conclusions", "text": "Latent Tree Analysis is a novel tool for modeling correlations. It has proven useful for unidimensional clustering, multidimensional clustering, hierarchical topic recognition, deep probabilistic modeling, probabilistic inference, and spectral clustering, and is potentially useful in other areas as well, since modeling correlations between variables is a fundamental task in data analysis.The research in this article was supported by the Hong Kong Research Grants Council under funding programs 16202515 and 16212516, as well as by the Education University of Hong Kong under the RG90 / 2014-2015R project. We thank all of our collaborators, especially Tao Chen, Yi Wang, Tengfei Liu, Raphael Mourad, April H. Liu, Chen Fu, Peixian Chen, Zhourong Chen."}], "references": [{"title": "Coala: A novel approach for the extraction of an alternate clustering of high quality and high dissimilarity", "author": ["E. Bae", "J. Bailey"], "venue": "ICDM, 53\u201362.", "citeRegEx": "Bae and Bailey,? 2006", "shortCiteRegEx": "Bae and Bailey", "year": 2006}, {"title": "Model-based multidimensional clustering of categorical data", "author": ["T. Chen", "N.L. Zhang", "T. Liu", "K.M. Poon", "Y. Wang"], "venue": "Artificial Intelligence 176:2246\u20132269.", "citeRegEx": "Chen et al\\.,? 2012", "shortCiteRegEx": "Chen et al\\.", "year": 2012}, {"title": "Progressive em for latent tree models and hierarchical topic detection", "author": ["P. Chen", "N. Zhang", "L. Poon", "Z. Chen"], "venue": "AAAI.", "citeRegEx": "Chen et al\\.,? 2016a", "shortCiteRegEx": "Chen et al\\.", "year": 2016}, {"title": "Sparse boltzmann machines with structure learning as applied to text analysis", "author": ["Z. Chen", "N.L. Zhang", "D.-Y. Yeung", "P. Chen"], "venue": "arXiv preprint arXiv:1609.05294", "citeRegEx": "Chen et al\\.,? 2016b", "shortCiteRegEx": "Chen et al\\.", "year": 2016}, {"title": "Learning latent tree graphical models", "author": ["M.J. Choi", "V.Y.F. Tan", "A. Anandkumar", "A.S. Willsky"], "venue": "Journal of Machine Learning Research 11:1771\u20131812.", "citeRegEx": "Choi et al\\.,? 2011", "shortCiteRegEx": "Choi et al\\.", "year": 2011}, {"title": "Approximating discrete probability distributions with dependence trees", "author": ["C. Chow", "C. Liu"], "venue": "IEEE transactions on Information Theory 14(3):462\u2013467.", "citeRegEx": "Chow and Liu,? 1968", "shortCiteRegEx": "Chow and Liu", "year": 1968}, {"title": "Latent class and latent transition analysis", "author": ["L.M. Collins", "S.T. Lanza"], "venue": "Wiley.", "citeRegEx": "Collins and Lanza,? 2010", "shortCiteRegEx": "Collins and Lanza", "year": 2010}, {"title": "Non-redundant multi-view clustering via orthogonalization", "author": ["Y. Cui", "X.Z. Fern", "J.G. Dy"], "venue": "ICDM, 133\u2013142.", "citeRegEx": "Cui et al\\.,? 2007", "shortCiteRegEx": "Cui et al\\.", "year": 2007}, {"title": "Biological Sequence Analysis: Probabilistic Models of Proteins and Nucleic Acids", "author": ["R. Durbin", "S.R. Eddy", "A. Krogh", "G. Mitchison"], "venue": "Cambridge University Press.", "citeRegEx": "Durbin et al\\.,? 1998", "shortCiteRegEx": "Durbin et al\\.", "year": 1998}, {"title": "A few logs suffice to build (almost) all trees (i)", "author": ["P.L. Erdos", "M.A. Steel", "L.A. Sz\u00e9kely", "T.J. Warnow"], "venue": "Random Structures and Algorithms 14(2):153\u2013184.", "citeRegEx": "Erdos et al\\.,? 1999", "shortCiteRegEx": "Erdos et al\\.", "year": 1999}, {"title": "Identification and classification of TCM syndrome types among patients with vascular mild cognitive impairment using latent tree analysis", "author": ["C. Fu", "N.L. Zhang", "B.X. Chen", "Z.R. Chen", "X.L. Jin", "R.J. Guo", "Z.G. Chen", "Y.L. Zhang"], "venue": "arXiv preprint arXiv:1601.06923.", "citeRegEx": "Fu et al\\.,? 2016", "shortCiteRegEx": "Fu et al\\.", "year": 2016}, {"title": "Non-redundant data clustering", "author": ["D. Gondek", "T. Hofmann"], "venue": "Knowledge and Information Systems 12(1):1\u201324.", "citeRegEx": "Gondek and Hofmann,? 2007", "shortCiteRegEx": "Gondek and Hofmann", "year": 2007}, {"title": "Learning both weights and connections for efficient neural network", "author": ["S. Han", "J. Pool", "J. Tran", "W. Dally"], "venue": "NIPS, volume 1135\u20131143, 3.", "citeRegEx": "Han et al\\.,? 2015", "shortCiteRegEx": "Han et al\\.", "year": 2015}, {"title": "A fast learning algorithm for deep belief nets", "author": ["G.E. Hinton", "S. Osindero", "Y.-W. Teh"], "venue": "Neural computation 18(7):1527\u2013 1554.", "citeRegEx": "Hinton et al\\.,? 2006", "shortCiteRegEx": "Hinton et al\\.", "year": 2006}, {"title": "Scalable latent tree model and its application to health analytics. NIPS 2015 Machine Learning for Healthcare workshop", "author": ["F. Huang", "I. Perros", "R. Chen", "J. Sun", "A Anandkumar"], "venue": null, "citeRegEx": "Huang et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Huang et al\\.", "year": 2015}, {"title": "Simultaneous unsupervised learning of disparate clusterings", "author": ["P. Jain", "R. Meka", "I.S. Dhillon"], "venue": "Statistical Analysis and Data Mining 1(3):195\u2013210.", "citeRegEx": "Jain et al\\.,? 2008", "shortCiteRegEx": "Jain et al\\.", "year": 2008}, {"title": "Latent trees for estimating intensity of facial action units", "author": ["S. Kaltwang", "S. Todorovic", "M. Pantic"], "venue": "CVPR, 296\u2013304.", "citeRegEx": "Kaltwang et al\\.,? 2015", "shortCiteRegEx": "Kaltwang et al\\.", "year": 2015}, {"title": "Latent tree copulas", "author": ["S. Kirshner"], "venue": "Sixth European Workshop on Probabilistic Graphical Models, Granada.", "citeRegEx": "Kirshner,? 2012", "shortCiteRegEx": "Kirshner", "year": 2012}, {"title": "Latent variable models and factor analysis", "author": ["M. Knott", "D.J. Bartholomew"], "venue": "Number 7. Edward Arnold.", "citeRegEx": "Knott and Bartholomew,? 1999", "shortCiteRegEx": "Knott and Bartholomew", "year": 1999}, {"title": "Latent Structure Analysis", "author": ["P.F. Lazarsfeld", "N.W. Henry"], "venue": "Boston: Houghton Mifflin.", "citeRegEx": "Lazarsfeld and Henry,? 1968", "shortCiteRegEx": "Lazarsfeld and Henry", "year": 1968}, {"title": "Convolutional networks for images, speech, and time series", "author": ["Y. LeCun", "Y. Bengio"], "venue": "The handbook of brain theory and neural networks 3361(10):1995.", "citeRegEx": "LeCun and Bengio,? 1995", "shortCiteRegEx": "LeCun and Bengio", "year": 1995}, {"title": "Subtypes of major depression: latent class analysis in depressed han chinese women. Psychological medicine", "author": ["Y. Li", "S. Aggen", "S. Shi", "J. Gao", "M. Tao", "K. Zhang", "X. Wang", "C. Gao", "L. Yang", "Y Liu"], "venue": null, "citeRegEx": "Li et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Li et al\\.", "year": 2014}, {"title": "Greedy learning of latent tree models for multidimensional clustering", "author": ["T. Liu", "N.L. Zhang", "P. Chen", "A.H. Liu", "K.M. Poon", "Y. Wang"], "venue": "Machine Learning 98(1-2):301\u2013330.", "citeRegEx": "Liu et al\\.,? 2013", "shortCiteRegEx": "Liu et al\\.", "year": 2013}, {"title": "Unidimensional clustering of discrete data using latent tree models", "author": ["A.H. Liu", "L.K. Poon", "N.L. Zhang"], "venue": "AAAI, 2771\u20132777.", "citeRegEx": "Liu et al\\.,? 2015", "shortCiteRegEx": "Liu et al\\.", "year": 2015}, {"title": "Hierarchical latent tree analysis for topic detection", "author": ["T. Liu", "N.L. Zhang", "P. Chen"], "venue": "ECML/PKDD, volume 8725 of Lecture Notes in Computer Science. Springer. 256\u2013272.", "citeRegEx": "Liu et al\\.,? 2014", "shortCiteRegEx": "Liu et al\\.", "year": 2014}, {"title": "Optimizing semantic coherence in topic models", "author": ["D. Mimno", "H.M. Wallach", "E. Talley", "M. Leenders", "A. McCallum"], "venue": "Proceedings of the Conference on Empirical Methods in Natural Language Processing, 262\u2013272. Association for Computational Linguistics.", "citeRegEx": "Mimno et al\\.,? 2011", "shortCiteRegEx": "Mimno et al\\.", "year": 2011}, {"title": "A survey on latent tree models and applications", "author": ["R. Mourad", "C. Sinoquet", "N.L. Zhang", "T. Liu", "P Leray"], "venue": "J. Artif. Intell", "citeRegEx": "Mourad et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Mourad et al\\.", "year": 2013}, {"title": "Multiple non-redundant spectral clustering views", "author": ["D. Niu", "J.G. Dy", "M.I. Jordan"], "venue": "ICML-10, 831\u2013838.", "citeRegEx": "Niu et al\\.,? 2010", "shortCiteRegEx": "Niu et al\\.", "year": 2010}, {"title": "Nested hierarchical dirichlet processes", "author": ["J. Paisley", "C. Wang", "D.M. Blei", "M Jordan"], "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence", "citeRegEx": "Paisley et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Paisley et al\\.", "year": 2015}, {"title": "Probabilistic reasoning in intelligent systems: Networks of plausible inference", "author": ["J. Pearl"], "venue": "Morgan Kaufmann.", "citeRegEx": "Pearl,? 1988", "shortCiteRegEx": "Pearl", "year": 1988}, {"title": "Variable selection in model-based clustering: To do or to facilitate", "author": ["L. Poon", "N.L. Zhang", "T. Chen", "Y. Wang"], "venue": "ICML-10, 887\u2013894.", "citeRegEx": "Poon et al\\.,? 2010", "shortCiteRegEx": "Poon et al\\.", "year": 2010}, {"title": "A modelbased approach to rounding in spectral clustering", "author": ["L.K. Poon", "A.H. Liu", "T. Liu", "N.L. Zhang"], "venue": "UAI.", "citeRegEx": "Poon et al\\.,? 2012", "shortCiteRegEx": "Poon et al\\.", "year": 2012}, {"title": "A principled and flexible framework for finding alternative clusterings", "author": ["Z. Qi", "I. Davidson"], "venue": "KDD, 717\u2013726.", "citeRegEx": "Qi and Davidson,? 2009", "shortCiteRegEx": "Qi and Davidson", "year": 2009}, {"title": "Deep boltzmann machines", "author": ["R. Salakhutdinov", "G.E. Hinton"], "venue": "AISTATS, volume 1, 3.", "citeRegEx": "Salakhutdinov and Hinton,? 2009", "shortCiteRegEx": "Salakhutdinov and Hinton", "year": 2009}, {"title": "Nonparametric latent tree graphical models: Inference, estimation, and structure learning", "author": ["L. Song", "H. Liu", "A. Parikh", "E. Xing"], "venue": "arXiv preprint arXiv:1401.3940.", "citeRegEx": "Song et al\\.,? 2014", "shortCiteRegEx": "Song et al\\.", "year": 2014}, {"title": "Dropout: A simple way to prevent neural networks from overfitting", "author": ["N. Srivastava", "G. Hinton", "A. Krizhevsky", "I. Sutskever", "R. Salakhutdinov"], "venue": "J. Mach. Learn. Res. 15(1):1929\u2013 1958.", "citeRegEx": "Srivastava et al\\.,? 2014", "shortCiteRegEx": "Srivastava et al\\.", "year": 2014}, {"title": "Latent class models in diagnostic studies when there is no reference standard \u2013 a systematic review", "author": ["M. van Smeden", "C.A. Naaktgeboren", "J.B. Reitsma", "K.G. Moons", "J.A. de Groot"], "venue": "American journal of epidemiology 179(4):423\u201331", "citeRegEx": "Smeden et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Smeden et al\\.", "year": 2013}, {"title": "A tutorial on spectral clustering", "author": ["U. Von Luxburg"], "venue": "Statistics and computing 17(4):395\u2013416.", "citeRegEx": "Luxburg,? 2007", "shortCiteRegEx": "Luxburg", "year": 2007}, {"title": "Latent tree models and approximate inference in Bayesian networks", "author": ["Y. Wang", "N.L. Zhang", "T. Chen"], "venue": "AAAI.", "citeRegEx": "Wang et al\\.,? 2008", "shortCiteRegEx": "Wang et al\\.", "year": 2008}, {"title": "Latent tree ensemble of pairwise copulas for spatial extremes analysis", "author": ["H. Yu", "J. Huang", "J. Dauwels"], "venue": "2016 IEEE International Symposium on Information Theory (ISIT), 1730\u2013 1734.", "citeRegEx": "Yu et al\\.,? 2016", "shortCiteRegEx": "Yu et al\\.", "year": 2016}, {"title": "Latent tree models and diagnosis in traditional chinese medicine", "author": ["N.L. Zhang", "S. Yuan", "T. Chen", "Y. Wang"], "venue": "Artificial intelligence in medicine 42(3):229\u2013245.", "citeRegEx": "Zhang et al\\.,? 2008", "shortCiteRegEx": "Zhang et al\\.", "year": 2008}, {"title": "Hierarchical latent class models for cluster analysis", "author": ["N.L. Zhang"], "venue": "AAAI.", "citeRegEx": "Zhang,? 2002", "shortCiteRegEx": "Zhang", "year": 2002}, {"title": "Hierarchical latent class models for cluster analysis", "author": ["N.L. Zhang"], "venue": "Journal of Machine Learning Research 5(6):697\u2013723.", "citeRegEx": "Zhang,? 2004", "shortCiteRegEx": "Zhang", "year": 2004}], "referenceMentions": [{"referenceID": 19, "context": "The first one is latent class models (LCMs) (Lazarsfeld and Henry, 1968; Knott and Bartholomew, 1999), which are LTMs with a single latent variable.", "startOffset": 44, "endOffset": 101}, {"referenceID": 18, "context": "The first one is latent class models (LCMs) (Lazarsfeld and Henry, 1968; Knott and Bartholomew, 1999), which are LTMs with a single latent variable.", "startOffset": 44, "endOffset": 101}, {"referenceID": 8, "context": "The second class is probabilistic phylogenetic trees (Durbin et al., 1998), which are a tool for determining the evolution history of a set of species.", "startOffset": 53, "endOffset": 74}, {"referenceID": 1, "context": "In other words, LTA performs multidimensional clustering (Chen et al., 2012; Liu et al., 2013).", "startOffset": 57, "endOffset": 94}, {"referenceID": 22, "context": "In other words, LTA performs multidimensional clustering (Chen et al., 2012; Liu et al., 2013).", "startOffset": 57, "endOffset": 94}, {"referenceID": 2, "context": "This leads to an alternative method for hierarchical topic detection (Liu, Zhang, and Chen, 2014; Chen et al., 2016a), which has been shown to find more meaningful topics and topic hierarchies than the state-of-the-art method based on latent Dirichlet allocation (Paisley et al.", "startOffset": 69, "endOffset": 117}, {"referenceID": 28, "context": ", 2016a), which has been shown to find more meaningful topics and topic hierarchies than the state-of-the-art method based on latent Dirichlet allocation (Paisley et al., 2015).", "startOffset": 154, "endOffset": 176}, {"referenceID": 3, "context": "Extension of LTA might offer one solution (Chen et al., 2016b).", "startOffset": 42, "endOffset": 62}, {"referenceID": 40, "context": "The term \u201clatent tree models\u201d first appeared in (Zhang et al., 2008; Wang, Zhang, and Chen, 2008).", "startOffset": 48, "endOffset": 97}, {"referenceID": 26, "context": "The exposition are more conceptual and less technical than (Mourad et al., 2013).", "startOffset": 59, "endOffset": 80}, {"referenceID": 26, "context": "Mourad et al. (2013) surveyed the research on latent tree models as of 2012 in details.", "startOffset": 0, "endOffset": 21}, {"referenceID": 29, "context": "A latent tree model (LTM) is a tree-structured Bayesian network (Pearl, 1988), where the leaf nodes represent observed variables and the internal nodes represent latent variables.", "startOffset": 64, "endOffset": 77}, {"referenceID": 42, "context": ", X5 (Zhang, 2004).", "startOffset": 5, "endOffset": 18}, {"referenceID": 4, "context": "In the literature, there are variations of LTMs where some internal nodes are observed (Choi et al., 2011) and/or the variables are continuous (Poon et al.", "startOffset": 87, "endOffset": 106}, {"referenceID": 30, "context": ", 2011) and/or the variables are continuous (Poon et al., 2010; Kirshner, 2012; Song et al., 2014).", "startOffset": 44, "endOffset": 98}, {"referenceID": 17, "context": ", 2011) and/or the variables are continuous (Poon et al., 2010; Kirshner, 2012; Song et al., 2014).", "startOffset": 44, "endOffset": 98}, {"referenceID": 34, "context": ", 2011) and/or the variables are continuous (Poon et al., 2010; Kirshner, 2012; Song et al., 2014).", "startOffset": 44, "endOffset": 98}, {"referenceID": 42, "context": "For any irregular LTM, there is a regular model that has fewer parameters and represents that same set of distributions over the observed variables (Zhang, 2004).", "startOffset": 148, "endOffset": 161}, {"referenceID": 1, "context": "There are three commonly used algorithms for learning LTMs: EAST (Expansion, Adjustment and Simplification until Termination) (Chen et al., 2012), BI (Bridged Islands) (Liu et al.", "startOffset": 126, "endOffset": 145}, {"referenceID": 22, "context": ", 2012), BI (Bridged Islands) (Liu et al., 2013), and CLRG (Chow-Liu and Recursive Grouping) (Choi et al.", "startOffset": 30, "endOffset": 48}, {"referenceID": 4, "context": ", 2013), and CLRG (Chow-Liu and Recursive Grouping) (Choi et al., 2011).", "startOffset": 52, "endOffset": 71}, {"referenceID": 22, "context": "It is the slowest among the three and finds better models, as measured by held-out likelihood, than the other two algorithms (Liu et al., 2013).", "startOffset": 125, "endOffset": 143}, {"referenceID": 5, "context": "The latent variables in the islands are linked up using Chow-Liu\u2019s algorithm (Chow and Liu, 1968) to form a global model.", "startOffset": 77, "endOffset": 97}, {"referenceID": 9, "context": "Information distances, as defined in (Erdos et al., 1999), between pairs of variables in the patch are estimated from data.", "startOffset": 37, "endOffset": 57}, {"referenceID": 22, "context": "Empirical results reported in (Liu et al., 2013) show that BI consistently yields better models than CLRG in terms of held-out likelihood.", "startOffset": 30, "endOffset": 48}, {"referenceID": 2, "context": "BI scales up well if progressive EM is used to estimate the parameters of the intermediate models, and was able to handle a text dataset with 10,000 distinct words (variables) and 300,000 documents in around 11 hours on a single machine (Chen et al., 2016a).", "startOffset": 237, "endOffset": 257}, {"referenceID": 14, "context": "5 hours on a single machine (Huang et al., 2015).", "startOffset": 28, "endOffset": 48}, {"referenceID": 6, "context": "LCA is hence a technique for cluster analysis and it is widely used in social, behavioral and health sciences (Collins and Lanza, 2010).", "startOffset": 110, "endOffset": 135}, {"referenceID": 21, "context": "In medical research, it is used to identify subtypes of diseases, for instance major depression, where good standards are not available (van Smeden et al., 2013; Li et al., 2014).", "startOffset": 136, "endOffset": 178}, {"referenceID": 10, "context": "We illustrate the point first using an example from (Fu et al., 2016), where the", "startOffset": 52, "endOffset": 69}, {"referenceID": 41, "context": "Following this practice, Liu, Poon, and Zhang (2015) have compared LTM-based cluster analysis with LCA on 30 datasets from UCI, and found that the LTM-based method outperforms LCA in most cases and often significantly.", "startOffset": 40, "endOffset": 53}, {"referenceID": 11, "context": "t a previous partition, which can, for instance, be obtained using K-means (Cui, Fern, and Dy, 2007; Gondek and Hofmann, 2007; Qi and Davidson, 2009; Bae and Bailey, 2006).", "startOffset": 75, "endOffset": 171}, {"referenceID": 32, "context": "t a previous partition, which can, for instance, be obtained using K-means (Cui, Fern, and Dy, 2007; Gondek and Hofmann, 2007; Qi and Davidson, 2009; Bae and Bailey, 2006).", "startOffset": 75, "endOffset": 171}, {"referenceID": 0, "context": "t a previous partition, which can, for instance, be obtained using K-means (Cui, Fern, and Dy, 2007; Gondek and Hofmann, 2007; Qi and Davidson, 2009; Bae and Bailey, 2006).", "startOffset": 75, "endOffset": 171}, {"referenceID": 1, "context": "As such, LTA is a natural tool for multidimensional clustering (Chen et al., 2012; Liu et al., 2013).", "startOffset": 63, "endOffset": 100}, {"referenceID": 22, "context": "As such, LTA is a natural tool for multidimensional clustering (Chen et al., 2012; Liu et al., 2013).", "startOffset": 63, "endOffset": 100}, {"referenceID": 1, "context": "We illustrate the use of LTA for multidimensional clustering using an example from (Chen et al., 2012), where a survey dataset from ICAC \u2014 Hong Kong\u2019s anti-corruption agency \u2014 was analyzed using the EAST algorithm.", "startOffset": 83, "endOffset": 102}, {"referenceID": 41, "context": "Liu, Zhang, and Chen (2014) propose a method to analyze text data and obtain models such as the one shown in Figure 4 (a).", "startOffset": 5, "endOffset": 28}, {"referenceID": 28, "context": "Nested hierarchical Dirichlet process (nHDP) (Paisley et al., 2015) is the state-of-theart method for hierarchical topic detection.", "startOffset": 45, "endOffset": 67}, {"referenceID": 2, "context": "Empirical results reported in (Liu, Zhang, and Chen, 2014; Chen et al., 2016a) show that HLTA significantly outperforms nHDP in terms of topic quality as measured by the topic coherence score proposed by Mimno et al.", "startOffset": 30, "endOffset": 78}, {"referenceID": 36, "context": "Liu, Zhang, and Chen (2014) use the BI algorithm to learn flat models, which does not scale up.", "startOffset": 5, "endOffset": 28}, {"referenceID": 1, "context": "Chen et al. (2016a) improves HLTA by using progressive EM (PEM) to estimate parameters of the intermediate models.", "startOffset": 0, "endOffset": 20}, {"referenceID": 1, "context": "Chen et al. (2016a) improves HLTA by using progressive EM (PEM) to estimate parameters of the intermediate models. The idea is to estimate the parameters in steps and, in each step, EM is run on a submodel that involves only 3 or 4 observed variables. PEM is efficient because a dataset, when projected onto 3 or 4 binary variables, consists of only 8 or 16 distinct cases no matter how large it is. Topic detection has been one of the most active research areas in machine learning. Nested hierarchical Dirichlet process (nHDP) (Paisley et al., 2015) is the state-of-theart method for hierarchical topic detection. Empirical results reported in (Liu, Zhang, and Chen, 2014; Chen et al., 2016a) show that HLTA significantly outperforms nHDP in terms of topic quality as measured by the topic coherence score proposed by Mimno et al. (2011), and HLTA finds more meaningful topic hierarchies.", "startOffset": 0, "endOffset": 840}, {"referenceID": 20, "context": "(2015) have shown that many weak connections in the fully-connected layers of Convolutional Neural Networks (CNNs) (LeCun and Bengio, 1995) can be pruned without incurring any accuracy loss.", "startOffset": 115, "endOffset": 139}, {"referenceID": 35, "context": "One method to address the problem is dropout (Srivastava et al., 2014), which randomly drops out units (while keeping full connectivity) during training.", "startOffset": 45, "endOffset": 70}, {"referenceID": 12, "context": "In fact, Han et al. (2015) have shown that many weak connections in the fully-connected layers of Convolutional Neural Networks (CNNs) (LeCun and Bengio, 1995) can be pruned without incurring any accuracy loss.", "startOffset": 9, "endOffset": 27}, {"referenceID": 12, "context": "How can one learn sparse deep models? One method is to first learn a fully connected model and then prune weak connections (Han et al., 2015).", "startOffset": 123, "endOffset": 141}, {"referenceID": 2, "context": "To do so, it partitions all the variables into groups such that the variables in each group are strongly correlated and the correlations can be properly modeled using a single latent variable (Liu, Zhang, and Chen, 2014; Chen et al., 2016a).", "startOffset": 192, "endOffset": 240}, {"referenceID": 33, "context": "(2016b) have developed and tested the idea in the context of RBMs, which have a single hidden layer and are building blocks of Deep Belief Networks (Hinton, Osindero, and Teh, 2006) and Deep Boltzmann Machines (Salakhutdinov and Hinton, 2009).", "startOffset": 210, "endOffset": 242}, {"referenceID": 1, "context": "To do so, it partitions all the variables into groups such that the variables in each group are strongly correlated and the correlations can be properly modeled using a single latent variable (Liu, Zhang, and Chen, 2014; Chen et al., 2016a). It introduces a latent variable for each group and links up the latent variables to form a flat LTM. Then it converts the latent variables into observed variables via data completion and repeats the process to produce a hierarchy. The output of HLTA is a deep tree model with a layer of observed variables at the bottom and multiple layers of latent variables on top (see Figure 4 (a)). To obtain a non-tree sparse deep model, we can use the tree model as a skeleton and introduce additional connections to model the residual correlations not captured by the tree. Chen et al. (2016b) have developed and tested the idea in the context of RBMs, which have a single hidden layer and are building blocks of Deep Belief Networks (Hinton, Osindero, and Teh, 2006) and Deep Boltzmann Machines (Salakhutdinov and Hinton, 2009).", "startOffset": 221, "endOffset": 827}, {"referenceID": 29, "context": "Bayesian networks (Pearl, 1988) alleviate the problem by representing the joint distribution in a factorized form.", "startOffset": 18, "endOffset": 31}, {"referenceID": 31, "context": "LTMs also have a role to play in spectral clustering (Poon et al., 2012).", "startOffset": 53, "endOffset": 72}, {"referenceID": 29, "context": "Bayesian networks (Pearl, 1988) alleviate the problem by representing the joint distribution in a factorized form. LTMs offer an alternative method. The idea is to build an LTM with the variables in question as observed variables, and make inference with the LTM. LTMs have two attractive properties. On one hand, they are computationally simple to work with because they are tree structured. On the other hand, they can represent complex relationship among the observed variables. Those two properties are exploited in (Wang, Zhang, and Chen, 2008; Kaltwang, Todorovic, and Pantic, 2015; Yu, Huang, and Dauwels, 2016) for efficient probabilistic inference in various domains. LTMs also have a role to play in spectral clustering (Poon et al., 2012). In spectral clustering (Von Luxburg, 2007), one defines a similarity matrix for a collection of data points, transforms the matrix to get a Laplacian matrix, finds the eigenvectors of the Laplacian matrix, and obtains a partition of the data using the leading eigenvectors. The last step is sometimes referred to as rounding. Rounding amounts to clustering the data points using the eigenvectors as features.What is unique about the problem is that one needs to determine how many leading eigenvectors to use. To solve the problem using LTMs, Poon et al. (2012) binarize the eigenvectors and build a collection of LTMs.", "startOffset": 19, "endOffset": 1313}], "year": 2016, "abstractText": "Latent tree analysis seeks to model the correlations among a set of random variables using a tree of latent variables. It was proposed as an improvement to latent class analysis \u2014 a method widely used in social sciences and medicine to identify homogeneous subgroups in a population. It provides new and fruitful perspectives on a number of machine learning areas, including cluster analysis, topic detection, and deep probabilistic modeling. This paper gives an overview of the research on latent tree analysis and various ways it is used in practice. Much of machine learning is about modeling and utilizing correlations among variables. In classification, the task is to establish relationships between attributes and class variables so that unseen data can be classified accurately. In Bayesian networks, dependencies among variables are represented as directed acyclic graphs and the graphs are used to facilitate efficient probabilistic inference. In topic models, word cooccurrences are accounted for by assuming that all words are generated probabilistically from the same set of topics, and the generation process is reverted via statistical inference to determine the topics. In deep belief networks, correlations among observed units are modeled using multiple levels of hidden units, and the top-level hidden units are used as a representation of the data for further analysis. Latent tree analysis (LTA) seeks to model the correlations among a set of observed variables using a tree model, called latent tree model (LTM), where the leaf nodes represent observed variables and the internal nodes represent latent variables.The dependence between two observed variables is explained by the path between them. Despite their simplicity, LTMs subsume two classes of models widely used in academic research. The first one is latent class models (LCMs) (Lazarsfeld and Henry, 1968; Knott and Bartholomew, 1999), which are LTMs with a single latent variable. They are used for categorical data clustering in social sciences and medicine. The second class is probabilistic phylogenetic trees (Durbin et al., 1998), which are a tool for determining the evolution history of a set of species. Phylogenetic trees are special LTMs where the model structures are binary (bifurcating) trees and all the Copyright c \u00a9 2017, Association for the Advancement of Artificial Intelligence (www.aaai.org). All rights reserved. variables have the same number of possible states. LTA also provides new and fruitful perspectives on a number of machine learning areas. One area is cluster analysis. Here finite mixture models such as LCMs are commonly used. A finite mixture model has one latent variable and consequently it gives one soft partition of data. An LTM typically has multiple latent variables and hence LTA yields multiple soft partitions of data simultaneously. In other words, LTA performs multidimensional clustering (Chen et al., 2012; Liu et al., 2013). It is interesting because complex data usually have multiple facets and can be meaningfully clustered in multiple ways. Another area is topic detection. Applying LTA to text data, we can partition a collection of documents in multiple ways. The document clusters in the partitions can be interpreted as topics. Furthermore, it is possible to learn hierarchical LTMs where the latent variables are organized into multiple layers. This leads to an alternative method for hierarchical topic detection (Liu, Zhang, and Chen, 2014; Chen et al., 2016a), which has been shown to find more meaningful topics and topic hierarchies than the state-of-the-art method based on latent Dirichlet allocation (Paisley et al., 2015). The third area is deep probabilistic modeling. Hierarchical LTM and deep belief network (DBN) (Hinton, Osindero, and Teh, 2006) are similar in that they both consist of multiple layers of variables, with an observed layer at the bottom and multiple layers of hidden units on top of it. One difference is that, in DBN, units from adjacent layers are fully connected, while HLTM is tree-structured. It would be interesting to explore the middle ground between the two extreme and develop algorithms for learning what might be called sparse DBNs. Learning structures for deep models is an interesting open problem. Extension of LTA might offer one solution (Chen et al., 2016b). The concept of latent tree models was introduced in (Zhang, 2002, 2004), where they were referred to as hierarchical latent class models. The term \u201clatent tree models\u201d first appeared in (Zhang et al., 2008; Wang, Zhang, and Chen, 2008). Mourad et al. (2013) surveyed the research on latent tree models as of 2012 in details. This paper provides a concise overview of the methodology. The exposition are more conceptual and less technical than (Mourad et al., 2013). Developments after 2012 are also included. ar X iv :1 61 0. 00 08 5v 1 [ cs .L G ] 1 O ct 2 01 6", "creator": "LaTeX with hyperref package"}}}