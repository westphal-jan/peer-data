{"id": "1610.00527", "review": {"conference": "ICML", "VERSION": "v1", "DATE_OF_SUBMISSION": "3-Oct-2016", "title": "Video Pixel Networks", "abstract": "We propose a probabilistic video model, the Video Pixel Network (VPN), that estimates the discrete joint distribution of the raw pixel values in a video. The model and the neural architecture reflect the time, space and color structure of video tensors and encode it as a four-dimensional dependency chain. The VPN approaches the best possible performance on the Moving MNIST benchmark, a leap over the previous state of the art, and the generated videos show only minor deviations from the ground truth. The VPN also produces detailed samples on the action-conditional Robotic Pushing benchmark and generalizes to the motion of novel objects.", "histories": [["v1", "Mon, 3 Oct 2016 13:06:40 GMT  (5347kb,D)", "http://arxiv.org/abs/1610.00527v1", "16 pages"]], "COMMENTS": "16 pages", "reviews": [], "SUBJECTS": "cs.CV cs.LG", "authors": ["nal kalchbrenner", "a\u00e4ron van den oord", "karen simonyan", "ivo danihelka", "oriol vinyals", "alex graves", "koray kavukcuoglu"], "accepted": true, "id": "1610.00527"}, "pdf": {"name": "1610.00527.pdf", "metadata": {"source": "CRF", "title": "Video Pixel Networks", "authors": ["Nal Kalchbrenner", "A\u00e4ron van den Oord", "Karen Simonyan", "Ivo Danihelka", "Oriol Vinyals", "Alex Graves", "Koray Kavukcuoglu"], "emails": ["nalk@google.com", "avdnoord@google.com", "simonyan@google.com", "danihelka@google.com", "vinyals@google.com", "gravesa@google.com", "korayk@google.com"], "sections": [{"heading": "1 Introduction", "text": "It is about the question of whether and to what extent people in the individual countries will be able to play by the rules. (...) It is about the question of whether and how they should play by the rules. (...) It is about the way in which they play by the rules. (...) It is about the way in which they play by the rules. (...) It is about the way in which they play by the rules. (...) It is about the way in which they play by the rules. (...) It is about the way in which they play by the rules. (...) It is about the way in which they play by the rules. \"(...) It is about the way in which they play by the rules. (...) It is about the way in which they play by the rules. (...) It is about the way in which they play by the rules. (...) It is about the way in which they play by the rules. (...) It is about the way in which they play by the rules."}, {"heading": "2 Model", "text": "In this section, we define the probability model implemented by Video Pixel Networks < < p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p < p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p"}, {"heading": "2.1 Baseline Model", "text": "We compare the VPN model to a base model that encodes the temporal dependencies in videos from previous images to the next, but ignores the spatial dependencies between pixels within an image and the dependencies between color channels. In this case, the common distribution is factored in by introducing assumptions of independence: p (x) \u2248 T-T = 0 N-i = 0 N-j = 0 p (xt, i, j, B | x < t,:,:,:) p (xt, i, j, G | x < t,:,:,:) p (xt, i, j, R | x < t,:,:,::). (2) Figure 1 illustrates the conditioning structure in the base model. The green channel value of pixel x depends only on the values of pixels in previous images. Various models have been proposed that resemble our base model insofar as they only include the 2015, Sanza, S5ff."}, {"heading": "2.2 Remarks on the Factorization", "text": "To illustrate the properties of the two factorizations, let's assume that a model must predict the value of a pixel x and the value of the adjacent pixel y in a frame F. The transition to the frame F from the previous frames F < is not deterministic. Suppose the previous frames F < represent a robotic arm, and in the current frame F the robotic arm is about to move either to the left or to the right. The base model estimates p (x | F <) and p (y | F <) as distributions with two modes, one for the robot moving to the left and one for the robot moving to the right. Sampling independently of p (x | F <) and p (y | F <) can result in two inconsistent pixel values coming from different modes, one pixel moving the robot moving to the left and the other moving the robot moving to the right."}, {"heading": "3 Architecture", "text": "In this section, we construct a network architecture capable of efficiently calculating the factorized distribution in Section 2. The architecture consists of two parts: the first part models the temporal dimension of the data and consists of Resolution Preserving CNN encoders whose outputs are given to a Convolutional LSTM; the second part models the spatial and color dimensions of the video and consists of pixel CNN architectures (van den Oord et al., 2016b; c) conditioned on the outputs of the CNN encoders."}, {"heading": "3.1 Resolution Preserving CNN Encoders", "text": "Faced with a series of video images F0,..., FT, the VPN first encodes each of the first T-images F0,..., FT \u2212 1 with a CNN encoder. These images form the story that conditions the generated images. Each of the CNN encoders consists of k (k = 8 in the experiments) residual blocks (section 4) and the spatial resolution of the input images is preserved in all layers in all blocks. Preservation of the resolution is crucial as it allows the model to condition any pixel that needs to be generated without loss of imaging capacity. The results of the T CNN encoders, which are computed in parallel during the training, are given as input to a Convolutional LSTM that also preserves the resolution. This part of the VPN calculates the temporal dependencies of the video sensor and is represented in Fig. 1 by the shaded blocks."}, {"heading": "3.2 PixelCNN Decoders", "text": "The second part of the VPN architecture calculates dependencies along the spatial and color dimensions. The T outputs of the first part of the architecture provide representations for the contexts that condition the generation of a portion of the T + 1 frames F0,..., FT; if you generate all the T + 1 frames, the first frame F0 does not get context representation. These context representations are used to condition decoder neural networks that are PixelCNNs. PixelCNNs consist of a Softmax layer over 256 intensity values for each color channel in each pixel. Figure 1 shows the two parts of the VPN architecture as discrete random variables. Since we treat the pixel values as discrete random variables, the last layer of the PixelCNN decoder is a softmax layer over 256 intensity values in each color channel."}, {"heading": "3.3 Architecture of Baseline Model", "text": "Unlike PixelCNNs, the decoders in the baseline model are CNNs that do not use weights masking, so the frame to be predicted cannot be specified as input. As shown in Fig. 1, the resulting neural network captures the time dependencies, but ignores spatial dependencies and color channel dependencies within the generated frames. As with VPNs, we create the neural architecture of the baseline model that is preserved at all levels."}, {"heading": "4 Network Building Blocks", "text": "In this section we describe two basic operations that are used as building blocks of the VPN: The first is the Multiplicative Unit (MU, Section 4.1), which contains multiplicative interactions inspired by LSTM gates (Hochreiter and Schmidhuber, 1997); the second is the Residual Multiplicative Block (RMB, Section 4.2), which consists of several layers of MUs."}, {"heading": "4.1 Multiplicative Units", "text": "A multiplicative unit (Fig. 2) is constructed by incorporating LSTM-like gates into a twisted layer. At an input h of size N \u00b7 N \u00b7 c, where c corresponds to the number of channels, we first pass through four twisted layers to obtain an update u and three gates g1 \u2212 3. The input, update and gates are then combined in the following way: g1 = \u03c3 (W1 \u0445 h) g2 = \u03c3 (W2 \u0445 h) g3 = \u03c3 (W3 \u0445 h) (3) u = tanh (W4 \u0445 h) MU (h; W) = g1 tanh (g2 h + g3 u), \u03c3 being the sigmoid nonlinearity and component-based multiplication. Distortions are omitted for clarity reasons. In our experiments, the W1 \u2212 4 shaft weights use a kernel of size 3 \u00d7 3. In contrast to LSTM networks, \u03c3 is the sigmoid nonlinearity and the hidden states."}, {"heading": "4.2 Residual Multiplicative Blocks", "text": "To facilitate simple gradient propagation through many layers of the network, we stack two MU layers in a multiplicative residual block (Fig. 3) in which the input has a residual connection to the output (He et al., 2016). To increase computing efficiency, the number of channels in MU layers within the block is halved. In the case of an input layer h of size N \u00b7 N \u00b7 2c with 2c channels, we first apply a 1 \u00b7 1 revolutionary layer that reduces the number of channels to c; for this layer, no activation function is used, followed by two consecutive MU layers, each with a revolutionary core of size 3 \u00d7 3. We then project the function card back to 2c channels using another 1 \u00d7 1 revolutionary layer. Finally, the input layer h is added to the total output, which forms a residual connection. Such a layer structure is similar to the bottle envelope residual unit (He, 2016)."}, {"heading": "4.3 Dilated Convolutions", "text": "One way to increase the receptive field without much impact on computational complexity is to use extended waves (Chen et al., 2014; Yu and Koltun, 2015), which allow the receptive field to grow exponentially, as opposed to linearly in the number of layers. In the variant of the VPN that uses dilation, the dilation rates within each RMB are the same, but they double from one RMB to the next up to a selected maximum size and then repeat (van den Oord et al., 2016a). Especially in the CNN encoders, we use two repetitions of the dilation scheme [1, 2, 4, 8], for a total of 8 RMB. We do not use dilation in the decoders."}, {"heading": "5 Moving MNIST", "text": "The moving MNIST dataset consists of sequences of 20 images of the size 64 x 64, representing two potentially overlapping MNIST numbers moving at constant speed and bouncing off walls. Training sequences are generated on-the-fly with numbers from the MNIST training set, without limiting the number of training sequences generated (our models observe 19.2M training sequences before convergence). The test set is fixed and consists of 10,000 sequences containing digits from the MNIST test set. 10 of the 20 images are used as context and the remaining 10 images are generated. To make our results comparable, we only use the same sigmoid cross-entropy loss for this dataset as in previous work (Srivastava et al., 2015a). The loss is defined as: H (z, y) = \u2212 zi log yi (1 \u2212 zi) (5 log \u2212 where the NISH numbers are limited."}, {"heading": "5.1 Implementation Details", "text": "The VPNs with and without dilatation, as well as the baseline model, have 8 RMBs in the encoders and 12 RMBs in the decoders; for the network variants that use ReLUs, we double the number of residual blocks to 16 and 24, respectively, to equal the size of the receptive fields in the two model variants; the number of channels in the blocks is c = 128, while the Convolutionary LSTM has 256 channels; the top layer before output has 768 channels; we train the models for 300,000 steps with 20-frame sequences to predict the last 10 frames of each sequence; each step corresponds to a stack of 64 sequences; we use RMSProp for optimization with an initial learning rate of 3 \u00b7 10 \u2212 4; and multiply the learning rate by 0.3 when learning flat lines."}, {"heading": "5.2 Results", "text": "Our basic model reaches 110.1 Nats / Frame, which is significantly better than the previous state of the art (Patraucean et al., 2015). We attribute these increases to architectural features and in particular to the resolution-preserving aspect of the network. Furthermore, the VPN reaches 87.6 Nats / Frames, which is close to the lower limit of 86.3 Nats / Frames. Table 2 reports on the results of architectural variants of the VPNs. The model with dilated turns improves over its non-dilated counterpart, as it can more easily affect the relatively large numbers moving in the 64 \u00d7 64 frames. In the case of Moving MNIST, MUs do not lead to a significant improvement in performance compared to the pure use of ReLUs, possibly due to the relatively low complexity of the task. A significant improvement results from the robotic push frame."}, {"heading": "6 Robotic Pushing", "text": "The Robotic Pushing Data Set consists of sequences of 20 64 x 64 frames, representing camera shots of a robot arm pushing objects in a basket. Data consists of a training set of 50,000 sequences, a validation set, and two test sets of 1500 sequences each, one containing a subset of objects seen during training, and the other containing novel objects not seen during training. Each frame in the video sequence is paired with the state of the robot on that frame and with the desired action to reach the next frame. Transitions are not deterministic, as the robot arm in a frame cannot reach the desired state in a frame due to occlusion by the objects encountered on its trajectory. 2 frames, 2 states, and 2 actions are used as context; the desired 18 future actions are also given."}, {"heading": "6.1 Implementation Details", "text": "For this data set, both the VPN and the base model use the softmax cross-entropy loss as defined in Section 2. As for Moving MNIST, the models have 8 RMBs in the encoders and 12 RMBs in the decoders; the ReLU variants have 16 residual blocks in the encoders and 24 in the decoders. The number of channels in the RMBs is c = 128, the Convolutionary LSTM has 256 channels and the top layer before output has 1536 channels. We use RMSProp with an initial learning rate of 10 \u2212 4. We train for 275000 steps with a stack size of 64 sequences per step. Each training sequence is achieved by selecting a random subsequence of 12 frames along with the corresponding states and actions. We use the first 2 frames in the sequence as context and predict the other 10 frames. Statuses and actions come as vectors of 5 rejects."}, {"heading": "6.2 Results", "text": "The best variant of the VPN has a > 65% reduction in the negative log probability over the base model. This shows that the models have learned to generalize not only new action sequences, but also new objects. In addition, we see that the use of multiplicative interactions in the VPN represents a significant improvement over the use of ReLUs. Figure 5-9 shows that the models not only generalize new action sequences, but also new objects. In addition, we see that the use of multiplicative interactions in the VPN represents a significant improvement over the use of ReLUs. Figure 5 contains random samples of the VPN on the validation with seen objects (along with the corresponding basic probability)."}, {"heading": "7 Conclusion", "text": "We introduced the Video Pixel Network, a profound generative model of video data that models the factorization of the common probability of video. We have shown that, despite the lack of specific motion priorities or replacement losses, the VPN approaches the lower loss limit of the Moving MNIST benchmark, which is a major improvement over the previous state of the art. In robotic pushing, the VPN achieves significantly better probabilities than the basic model, which lacks the quadruple dependency structure; the VPN produces videos that are artifact-free and will be highly detailed for many frames in the future; the quadruple dependency structure provides a robust and generic method of generating videos without systematic artifacts."}, {"heading": "Acknowledgments", "text": "The authors thank Chelsea Finn for her advice on robotic pushing and Lasse Espeholt for her helpful remarks."}], "references": [{"title": "Semantic image segmentation with deep convolutional nets and fully connected crfs", "author": ["Liang-Chieh Chen", "George Papandreou", "Iasonas Kokkinos", "Kevin Murphy", "Alan L. Yuille"], "venue": "CoRR, abs/1412.7062,", "citeRegEx": "Chen et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Chen et al\\.", "year": 2014}, {"title": "Unsupervised learning for physical interaction through video", "author": ["Chelsea Finn", "Ian J. Goodfellow", "Sergey Levine"], "venue": "prediction. CoRR,", "citeRegEx": "Finn et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Finn et al\\.", "year": 2016}, {"title": "Identity mappings in deep residual networks", "author": ["Kaiming He", "Xiangyu Zhang", "Shaoqing Ren", "Jian Sun"], "venue": null, "citeRegEx": "He et al\\.,? \\Q2016\\E", "shortCiteRegEx": "He et al\\.", "year": 2016}, {"title": "Long short-term memory", "author": ["Sepp Hochreiter", "J\u00fcrgen Schmidhuber"], "venue": "Neural computation,", "citeRegEx": "Hochreiter and Schmidhuber.,? \\Q1997\\E", "shortCiteRegEx": "Hochreiter and Schmidhuber.", "year": 1997}, {"title": "Recurrent continuous translation models", "author": ["Nal Kalchbrenner", "Phil Blunsom"], "venue": "In EMNLP, pages 1700\u20131709,", "citeRegEx": "Kalchbrenner and Blunsom.,? \\Q2013\\E", "shortCiteRegEx": "Kalchbrenner and Blunsom.", "year": 2013}, {"title": "Grid long short-term memory", "author": ["Nal Kalchbrenner", "Ivo Danihelka", "Alex Graves"], "venue": "International Conference on Learning Representations,", "citeRegEx": "Kalchbrenner et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Kalchbrenner et al\\.", "year": 2016}, {"title": "Deep multi-scale video prediction beyond mean square error", "author": ["Micha\u00ebl Mathieu", "Camille Couprie", "Yann LeCun"], "venue": "CoRR, abs/1511.05440,", "citeRegEx": "Mathieu et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Mathieu et al\\.", "year": 2015}, {"title": "Actionconditional video prediction using deep networks in atari games", "author": ["Junhyuk Oh", "Xiaoxiao Guo", "Honglak Lee", "Richard L. Lewis", "Satinder P. Singh"], "venue": "In NIPS,", "citeRegEx": "Oh et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Oh et al\\.", "year": 2015}, {"title": "Spatio-temporal video autoencoder with differentiable memory", "author": ["Viorica Patraucean", "Ankur Handa", "Roberto Cipolla"], "venue": "CoRR, abs/1511.06309,", "citeRegEx": "Patraucean et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Patraucean et al\\.", "year": 2015}, {"title": "Video (language) modeling: a baseline for generative models of natural videos", "author": ["Marc\u2019Aurelio Ranzato", "Arthur Szlam", "Joan Bruna", "Micha\u00ebl Mathieu", "Ronan Collobert", "Sumit Chopra"], "venue": "CoRR, abs/1412.6604,", "citeRegEx": "Ranzato et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Ranzato et al\\.", "year": 2014}, {"title": "Convolutional LSTM network: A machine learning approach for precipitation nowcasting", "author": ["Xingjian Shi", "Zhourong Chen", "Hao Wang", "Dit-Yan Yeung", "Wai-Kin Wong", "Wang-chun Woo"], "venue": "In NIPS,", "citeRegEx": "Shi et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Shi et al\\.", "year": 2015}, {"title": "Unsupervised learning of video representations using lstms", "author": ["Nitish Srivastava", "Elman Mansimov", "Ruslan Salakhutdinov"], "venue": "In ICML,", "citeRegEx": "Srivastava et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Srivastava et al\\.", "year": 2015}, {"title": "Wavenet: A generative model for raw audio", "author": ["Aaron van den Oord", "Sander Dieleman", "Heiga Zen", "Karen Simonyan", "Oriol Vinyals", "Alex Graves", "Nal Kalchbrenner", "Andrew Senior", "Koray Kavukcuoglu"], "venue": "CoRR, abs/1609.03499,", "citeRegEx": "Oord et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Oord et al\\.", "year": 2016}, {"title": "Pixel recurrent neural networks", "author": ["A\u00e4ron van den Oord", "Nal Kalchbrenner", "Koray Kavukcuoglu"], "venue": "In ICML,", "citeRegEx": "Oord et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Oord et al\\.", "year": 2016}, {"title": "Conditional image generation with pixelcnn decoders", "author": ["A\u00e4ron van den Oord", "Nal Kalchbrenner", "Oriol Vinyals", "Lasse Espeholt", "Alex Graves", "Koray Kavukcuoglu"], "venue": "In NIPS,", "citeRegEx": "Oord et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Oord et al\\.", "year": 2016}, {"title": "Generating videos with scene", "author": ["Carl Vondrick", "Hamed Pirsiavash", "Antonio Torralba"], "venue": "dynamics. CoRR,", "citeRegEx": "Vondrick et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Vondrick et al\\.", "year": 2016}, {"title": "Multi-scale context aggregation by dilated convolutions", "author": ["Fisher Yu", "Vladlen Koltun"], "venue": "CoRR, abs/1511.07122,", "citeRegEx": "Yu and Koltun.,? \\Q2015\\E", "shortCiteRegEx": "Yu and Koltun.", "year": 2015}], "referenceMentions": [{"referenceID": 7, "context": "Current approaches range from mean squared error models based on deep neural networks (Srivastava et al., 2015a; Oh et al., 2015), to models that predict quantized image patches (Ranzato et al.", "startOffset": 86, "endOffset": 129}, {"referenceID": 9, "context": ", 2015), to models that predict quantized image patches (Ranzato et al., 2014), incorporate motion priors (Patraucean et al.", "startOffset": 56, "endOffset": 78}, {"referenceID": 8, "context": ", 2014), incorporate motion priors (Patraucean et al., 2015; Finn et al., 2016) or use adversarial losses (Mathieu et al.", "startOffset": 35, "endOffset": 79}, {"referenceID": 1, "context": ", 2014), incorporate motion priors (Patraucean et al., 2015; Finn et al., 2016) or use adversarial losses (Mathieu et al.", "startOffset": 35, "endOffset": 79}, {"referenceID": 6, "context": ", 2016) or use adversarial losses (Mathieu et al., 2015; Vondrick et al., 2016).", "startOffset": 34, "endOffset": 79}, {"referenceID": 15, "context": ", 2016) or use adversarial losses (Mathieu et al., 2015; Vondrick et al., 2016).", "startOffset": 34, "endOffset": 79}, {"referenceID": 3, "context": "The outputs of the encoders are combined over time with a convolutional LSTM that also preserves the resolution (Hochreiter and Schmidhuber, 1997; Shi et al., 2015).", "startOffset": 112, "endOffset": 164}, {"referenceID": 10, "context": "The outputs of the encoders are combined over time with a convolutional LSTM that also preserves the resolution (Hochreiter and Schmidhuber, 1997; Shi et al., 2015).", "startOffset": 112, "endOffset": 164}, {"referenceID": 8, "context": "8 nats/frame (Patraucean et al., 2015).", "startOffset": 13, "endOffset": 38}, {"referenceID": 1, "context": "The second benchmark is the Robotic Pushing dataset (Finn et al., 2016) where, given two natural video frames showing a robotic arm pushing objects, the task is to predict the following 18 frames.", "startOffset": 52, "endOffset": 71}, {"referenceID": 9, "context": "Various models have been proposed that are similar to our baseline model in that they capture the temporal dependencies only (Ranzato et al., 2014; Srivastava et al., 2015a; Oh et al., 2015)", "startOffset": 125, "endOffset": 190}, {"referenceID": 7, "context": "Various models have been proposed that are similar to our baseline model in that they capture the temporal dependencies only (Ranzato et al., 2014; Srivastava et al., 2015a; Oh et al., 2015)", "startOffset": 125, "endOffset": 190}, {"referenceID": 4, "context": ", 2016a), and it is in the order of 10 for language tasks such as machine translation (Kalchbrenner and Blunsom, 2013).", "startOffset": 86, "endOffset": 118}, {"referenceID": 3, "context": "1) that contains multiplicative interactions inspired by LSTM (Hochreiter and Schmidhuber, 1997) gates.", "startOffset": 62, "endOffset": 96}, {"referenceID": 5, "context": ", 2015b) and Grid LSTM (Kalchbrenner et al., 2016), there is no setting of the gates such that MU(h;W) simply returns the input h; the input is always processed with a non-linearity.", "startOffset": 23, "endOffset": 50}, {"referenceID": 2, "context": "3) where the input has a residual (additive skip) connection to the output (He et al., 2016).", "startOffset": 75, "endOffset": 92}, {"referenceID": 2, "context": "Such a layer structure is similar to the bottleneck residual unit of (He et al., 2016).", "startOffset": 69, "endOffset": 86}, {"referenceID": 2, "context": "We also experimented with a standard residual block of (He et al., 2016) which uses ReLU non-linearities \u2013 see Sect.", "startOffset": 55, "endOffset": 72}, {"referenceID": 10, "context": "Model Test (Shi et al., 2015) 367.", "startOffset": 11, "endOffset": 29}, {"referenceID": 8, "context": "2 (Patraucean et al., 2015) 179.", "startOffset": 2, "endOffset": 27}, {"referenceID": 0, "context": "One way to increase the receptive field without much effect on the computational complexity is to use dilated convolutions (Chen et al., 2014; Yu and Koltun, 2015), which make the receptive field grow exponentially, as opposed to linearly, in the number of layers.", "startOffset": 123, "endOffset": 163}, {"referenceID": 16, "context": "One way to increase the receptive field without much effect on the computational complexity is to use dilated convolutions (Chen et al., 2014; Yu and Koltun, 2015), which make the receptive field grow exponentially, as opposed to linearly, in the number of layers.", "startOffset": 123, "endOffset": 163}, {"referenceID": 8, "context": "1 nats/frame, which is significantly better than the previous state of the art (Patraucean et al., 2015).", "startOffset": 79, "endOffset": 104}], "year": 2016, "abstractText": "We propose a probabilistic video model, the Video Pixel Network (VPN), that estimates the discrete joint distribution of the raw pixel values in a video. The model and the neural architecture reflect the time, space and color structure of video tensors and encode it as a four-dimensional dependency chain. The VPN approaches the best possible performance on the Moving MNIST benchmark, a leap over the previous state of the art, and the generated videos show only minor deviations from the ground truth. The VPN also produces detailed samples on the action-conditional Robotic Pushing benchmark and generalizes to the motion of novel objects.", "creator": "LaTeX with hyperref package"}}}