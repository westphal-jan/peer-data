{"id": "1610.01132", "review": {"conference": "NIPS", "VERSION": "v1", "DATE_OF_SUBMISSION": "4-Oct-2016", "title": "A Non-generative Framework and Convex Relaxations for Unsupervised Learning", "abstract": "We give a novel formal theoretical framework for unsupervised learning with two distinctive characteristics. First, it does not assume any generative model and based on a worst-case performance metric. Second, it is comparative, namely performance is measured with respect to a given hypothesis class. This allows to avoid known computational hardness results and improper algorithms based on convex relaxations. We show how several families of unsupervised learning models, which were previously only analyzed under probabilistic assumptions and are otherwise provably intractable, can be efficiently learned in our framework by convex optimization.", "histories": [["v1", "Tue, 4 Oct 2016 19:22:44 GMT  (344kb)", "https://arxiv.org/abs/1610.01132v1", "to appear in NIPS 2016"], ["v2", "Wed, 5 Oct 2016 00:30:02 GMT  (345kb)", "http://arxiv.org/abs/1610.01132v2", "to appear in NIPS 2016"], ["v3", "Tue, 27 Dec 2016 20:59:01 GMT  (351kb)", "http://arxiv.org/abs/1610.01132v3", "NIPS 2016"]], "COMMENTS": "to appear in NIPS 2016", "reviews": [], "SUBJECTS": "cs.LG cs.DS stat.ML", "authors": ["elad hazan", "tengyu ma"], "accepted": true, "id": "1610.01132"}, "pdf": {"name": "1610.01132.pdf", "metadata": {"source": "CRF", "title": "A Non-generative Framework and Convex Relaxations for Unsupervised Learning", "authors": ["Elad Hazan", "Tengyu Ma"], "emails": [], "sections": [{"heading": null, "text": "ar Xiv: 161 0.01 132v 3 [cs.L G] 27 Dec 201 6"}, {"heading": "1 Introduction", "text": "This year, it is closer than ever before in the history of the country."}, {"heading": "1.1 Previous work", "text": "The vast majority of work on unsupervised learning processes, both theoretical and applied, focuses on generic models that are demonstrably difficult to solve."}, {"heading": "2 A formal framework for unsupervised learning", "text": "The base constructs itself in an unsupervised learning environment. (F) The base constructs itself in an unsupervised learning environment. (F) The base constructs itself in an unsupervised learning environment. (F) The base constructs itself in an inadequate learning environment. (F) The base constructs itself in an inadequate learning environment. (F) The base constructs itself in an inadequate learning environment. (F) The base constructs itself in an inadequate learning environment. (F) The base constructs itself in an inadequate learning environment. (F) The base constructs itself in an inadequate learning environment. (F) The base constructs itself in an insufficient learning environment. (F) The base constructs itself in an insufficient learning environment. (F) The base constructs itself in an insufficient learning environment. (F) The base constructs itself. (F) The base constructs itself in an insufficient learning environment. (F) The base constructs itself. (F) The base constructs itself in an insufficient learning environment."}, {"heading": "3 Spectral autoencoders: unsupervised learning of algebraic manifolds", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "3.1 Algebraic manifolds", "text": "The goal of the spectral autoencoder hypothesis class that we define from now on is to learn the representation of data that lies on a low-dimensional algebraic variety / multiplicity. Linear diversity, or linear multiplicity defined by the roots of linear equations, is simply a linear subspace. If the data is located in a linear subspace or close enough to it, then PCA is effectively learning its concise representation. An extension of linear multiplicity is the root group of low-grade polynomial equations. Formally, we leave k, s, integers and let c1,.... cds \u2212 k, Rd s, be a series of vectors in ds dimension, and consider the algebraic variety M = {x, Rd: in tight rows [ds \u2212 k], < ci, s > = 0}.Note that here any constraint < ci, Rd s, and we consider the algebraic variety M = {x, Rd: in tight rows [ds \u2212 k], < ci, s > = 0}."}, {"heading": "3.2 Warm up: PCA and kernel PCA", "text": "In this section, we illustrate our framework for agnostic unattended learning by showing how PCA and kernel PCA can be efficiently learned within our model. Results of this subsection are not new and are given for illustrative purposes.The PCA-related hypothesis works with domain X = Rd and range Y = Rk for some k < d via linear operators. In the kernel PCA, the linear encryption operator applies to the sec tensor power x s of the data. That is, encryption and decoding are parameterized by a linear operator A-Rk \u00d7 ds, Hpcak, s = {(hA, gA): hA (x) = Ax s, gA (y) = A \u2020 y}, where A \u2020 denotes the pseudo-inverse of A. The natural loss function here is the euclidean norm, (g, h)."}, {"heading": "3.3 Spectral Autoencoders", "text": "In this section, we assume that the data will be normalized to Euclidean norm 1, and consider the following class of hypotheses, which of course generalize PCA: Definition 3.1 (spectral autoencoder). We define the class Hsak, s as the following set of all hypotheses (g, h): h (x) = Ax s, A Rk \u00d7 ds g (y) = vmax (of), B Rd s, k \"1. We note that this notion is more general than kernel PCA: Adopting some (g, h) s, Ax s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s.\""}, {"heading": "3.4 Learnability of polynomial spectral decoding", "text": "For simplicity, we focus on the case when s = 2. Ideally, we would like to learn the best encoding decoding schemes for each data distribution. Although there are technical difficulties to achieve such a general result, it would be a natural attempt to optimize the loss function f (A, B), and in fact it might even be non-continuous (if not poorly defined)! Here we make another feasibility assumption that the data distribution D error.Definition 3.2 we say that the data distribution D (k) is spectral decodable on a regular basis if there is A (R) -2 and B (Rd2) -k with a reasonable reconstruction. Definition 3.2 we say that the data distribution D (k) is spectral decodable on a regular basis if there is A (k)."}, {"heading": "4 A family of optimization encodings and efficient dictionary learning", "text": "In this section, we provide efficient algorithms for learning a family of unattended learning algorithms commonly known as \"dictionary learning.\" Contrary to previous approaches, we do not construct an actual \"dictionary,\" but rather improperly learn a comparable coding via convex relaxations. We consider another family of codes motivated by matrix-based unattended learning models such as theme models, dictionary learning, and PCA. This family is described by a matrix that exhibits low complexity according to a particular standard."}, {"heading": "4.1 Improper dictionary learning: overview", "text": "We assume that the maximum number of columns in the A-class can be replaced by others in most cases. (...) We assume that the maximum number of columns in the A-class cannot be reached in most cases. (...) We assume that the maximum number of columns in the A-class cannot be reached in most cases. (...) We assume that the number of A-classes in the order of magnitude of the A-class is higher than that of the A-class. (...) We consider Hkdict to be parameterized. (...) In this case, the order of magnitude of the individual entries of x, Hdictk = {(hA, gA): A, Rd \u00b7 r. (...) We assume that the number of A-classes is higher than that of the A-classes. (...) We assume that the A-class is larger than that of the A-class. (...) We assume that the number of A-classes is larger than the A-class. (...) We assume that the A-class is larger than the number of A-classes. (...) We assume that the A-class is larger than the A-class. (...)"}, {"heading": "5 Analysis of Improper Dictionary Learning", "text": "In this section we give the complete proof of the theorems and lemas in Section 4. We begin with general results on denocialization, Rademacher's breadth, and factorizable norms, and continue to narrow down our attitudes specifically in Section 5.4."}, {"heading": "5.1 Guarantees of denoising", "text": "In this section we give the guarantees for the error caused by the denoting step. Let us remember that \u03b5 is the optimal reconstruction error that can be achieved by the optimal (correct) dictionary (equation (4.1). Lemma 5.1. Let us define Z in equation (4.2). Then we have that 1Nd E X-X-1 [| Z-X-1] 6 \u03b5-1 (5.1) proof. Let us leave Y-1 = A-hA (X), where hA-X-X denotes the collection of the coding of X with hA-1. Since Y-Y we have that 1Nd E [| Z-X-1] 6 1Nd E [| X-Y-Y-Y-Y-1] -Q is that Y-Y is a practicable solution of optimization (4.2)."}, {"heading": "5.2 Sampling Rademacher Width of a Set", "text": "As long as the intrinsic complexity of set Q is low, we can compress it by random sampling. The idea of looking at a reconstruction error is the test error of a supervised learning problem that began with the work of Srebro and Shraibman [SS05] and was used for other completion problems, e.g. [BM15]. We use the terminology \"wheel maker width\" instead of \"wheel maker width\" to emphasize that the notation below is the property of a series of vectors (instead of a hypotheses class), and for each set W-RD and an integer m we define its sampling of wheel maker width (SRW) as, SRWm (W) = E-RWm, [1m sup x-W < x-W < x-W > DIS), (5,2) where it is random subset [D] of size < a > size."}, {"heading": "5.3 Factorable norms", "text": "In this subsection, we generally define the factorable norms from which we obtain a convex set Q that meets the condition (4,4) (see dim \u03b2 5,3). For all two norms defined on matrices of each dimension, we can define the following quantity: A (Z) = inf Z = A (A), A), A), B (5,3), s, s, s, t > 1), s, q, q, s, s () to denote the function p ()."}, {"heading": "5.4 Sampling Rademacher width of level set of \u03931,\u221e,1,1", "text": "Here we give a Rademacher breadth, which is limited to the specific group in which we are interested, namely the level groups of \"A,\" \"A,\" \"A,\" \"A,\" \"A,\" \"A,\" \"A,\" \"A,\" \"A,\" \"A,\" \"A,\" \"A,\" \"A,\" \"A,\" \"A,\" \"\" A, \"\" \"A,\" \"\" A, \"\" \"A,\" \"\" A, \"\" \"A,\" \"A,\" \"A,\" A, \"A,\" A, \"A,\" \"A,\" A, \"A,\" \"A,\" \"A,\" \"A,\" \"A,\" \"A,\" \"A,\" A, \"A,\" A, \"\" A, \"A,\" A, \"A,\" A, \"A,\" A, \"A,\" A, \"A,\" A, \"A,\" A, \"\" A, \"A,\" A, \"A,\" A, \"\" A, \"A,\" A, \"A,\" A, \"\" A, \"A,\" A, \"A,\" A, \"A,\" A, \"\" A, \"A,\" A, \"A,\" A, \"A,\" A, \"\" A, \"A,\" A, \"A,\" A, \"A,\" A, \"A,\" \"A,\" A, \"A,\" A, \"A,\" \"A,\" A, \"A,\" A, \"A,\" \"A,\" A, \"A,\" A."}, {"heading": "5.5 Convex Relaxation for \u03931,\u221e,1,1 norm", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "5.5.1 Sum-of-squares relaxation", "text": "Here we will briefly present the basic ideas of sum-of-squares (Lasserre) used for this work. We refer to the extensive study [Las15, Lau09, BS14] for detailed discussions on the sum of squares proofs and their application to algorithm design (Par00, Las01]. Recently, there has been a popular research line on the application of x x x x algorithms for machine learning [BKS15b, BKS14, BM16, MW15, GMS15, HSS15, HSS16, HSS16 x x x. Here our technique is most related to that of [BM16], with the main difference that we deal with standard constraints that are not typically within the SoS framework. Let us leave R [x] d denotes the set of all real polynomials of the degree with n variables x1, xn."}, {"heading": "5.5.2 Relaxation for \u03931,\u221e,1,1 norm", "text": "In this section, we introduce ourselves: \"A,\" \"A,\" \"A,\" \"A,\" \"A,\" \"A,\" \"A,\" \"A,\" \"A,\" \"A,\" \"A,\" \"A,\" \"A,\" \"A,\" \"A,\" \"A,\" \"A,\" \"A,\" \"A,\" \"A,\" \"A,\" \"A,\" \"A,\" \"A,\" \"A,\" \"A,\" \"\" A, \"\" A, \"\" A, \"A,\" A, \"A,\" A, \"A,\" A, \"A,\" A, \"A,\" A, \"\" A, \"A,\" A, \"A,\" A, \"A,\" A, \"A,\" A, \"A,\" A, \"A,\" A, \"A,\" A, \"A,\" A, \"A,\" A, \"A,\" A, \"A,\" A, \"A,\" A, \"A,\" A, \"A,\" A, \"A,\" A, \"A,\" A, \"A,\" A, \"A,\" A, \"A,\" A, \"A,\" A, A, A, \"A, A, A, A, A, A, A, A, A, A, A, A, A, A, A, A, A, A, A, A, A, A, A, A, A, A, A, A, A, A, A, A, A, A, A, A, A, A, A, A, A, A, A, A, A, A, A, A, A, A, A, A, A, A, A, A, A, A, A, A, A, A, A, A, A, A, A, A, A, A, A, A, A, A, A, A, A, A, A, A, A, A, A, A, A, A, A, A, A, A, A, A, A, A, A, A, A, A, A, A, A, A, A, A, A, A, A, A, A, A, A, A, A, A"}, {"heading": "5.6 Proof of Theorem 4.1", "text": "In this subsection, we prove that we will get a good construction. (...) Essentially, the idea here is to reduce the problem of encoding an example to the problem of encoding an example. (...) To decipher an example x, we still prefer efficient decoding steps. (...) Since in this case the encoding contains more noise from the data, we prefer the current scheme of encoding. (...) We will prove that algorithm 2 with Q = Qpsos and the arrangement 2 / pd - 1 \u00b7 log d) gives the desired encoding / decoding. (...) In fact, the most likely encoding length is O (k2r2 / p / 2)."}, {"heading": "6 Conclusions", "text": "We have defined a new framework for unattended learning that replaces generative assumptions with notions of reconstruction errors and encoding lengths. This framework is comparative and enables learning of certain hypotheses classes in relation to unknown distribution by other hypotheses classes. We demonstrate their usefulness by providing new polynomic time algorithms for two unattended hypotheses classes. First, we provide new polynomic time algorithms for dictionary models in a much broader range of parameters and assumptions. Another domain is the class of spectral encodings, for which we consider a new class of models that strictly encompass PCA and kernel PCA. This new class, unlike previous spectral models, is capable of learning algebraic manifolds. We provide efficient learning algorithms for this class based on convex relaxations."}, {"heading": "Acknowledgements", "text": "We thank Sanjeev Arora for many illuminating discussions and crucial observations in earlier stages of this work, including that a representation that preserves information for all classifiers requires lossless compression."}, {"heading": "A Proof of Theorem 2.1", "text": "The proof for theorem 2.1. [MRT12, theorem 3.1] again problem 3.1] This design defect is sufficient for each hypothesis f \u00b2 H, loss D (f) 6 loss S (f) + 2Rm (H) + 2Rm (H) + 2Rm (H) + 2Rm (H), loss D (f \u00b2 ERM) 6 loss S (f \u00b2 ERM) + 2Rm (H) + 2Rm (H) + 2Rm (H) + 2Rm (H), loss S (f \u00b2 ERM) + 2Rm (H) + 2Rm (H) + 2Rm (H), loss S (f \u00b2) + 2Rm (H), loss S (H) + 2Rm (H) + 2Rm (H) + 2Rm (H) + 2Rm (H), that we (by definition of ERM) 6 loss S (f \u00b2 ERM) + 6Rm (H) + 6Rm (H) + 6Rm (H) (3.12), that after learning ability (RTH) x (M1)."}, {"heading": "B Proof of Theorem 3.1", "text": "We assume that without loss of generality s = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1: 1 = 1 = 1: 1 = 1: 1: 1: 1: 1: 1: 1: 1: 1: 1: 1: 1: 1: 1: 1: 1: 1: 1: 1, \"1: 1: 1: 1: 1: 1,\" 1: 1: 1: 1: 1: 1."}, {"heading": "C Shorter codes with relaxed objective for Polynomial Spectral Components Analysis", "text": "It's not just the way we play by the rules, but also the way we play by the rules. (...) It's the way we play by the rules. (...) It's the way we play by the rules. (...) It's the way we play by the rules. (...) It's the way we play by the rules. (...) It's the way we play by the rules. (...) It's the way we play by the rules. (...) It's the way we play by the rules. (...) It's the way we play by the rules. (...) It's the way we play by the rules. (...) It's the way we play by the rules. (...) It's the way we play by the rules. (...) It's the way we play by the rules, the way we play by the rules. (...) It's the way we play by the rules."}, {"heading": "D Toolbox", "text": "Lemma D.1. Let p > 2 be a power of 2 and u = [u1,..., un] and v = [v1,..., vn] be indeterminate, then there is a SoS proof that we have for p = 4 p \u2212 1 j p6 (iupi) (vpi) p \u2212 1 (vpi) proof sketch. Inequality results from repeated application of Cauchy-Black. For example, for p = 4 (iu4i) (v4i) 3 > (iu2i v 2 i) 2 (v4i) 2 (v4i) 2 (from Cauchy-Black) > (iuiv 3 i) 4 (again from Cauchy-Black) For p = 2s with s > 2, the statement can be proved inductively."}], "references": [{"title": "Tight bounds for universal compression of large alphabets", "author": ["Jayadev Acharya", "Hirakendu Das", "Ashkan Jafarpour", "Alon Orlitsky", "Ananda Theertha Suresh"], "venue": "In Proceedings of the 2013 IEEE International Symposium on Information", "citeRegEx": "Acharya et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Acharya et al\\.", "year": 2013}, {"title": "K-svd: Design of dictionaries for sparse representation", "author": ["Michal Aharon", "Michael Elad", "Alfred Bruckstein"], "venue": "In IN: PROCEEDINGS OF SPARS05,", "citeRegEx": "Aharon et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Aharon et al\\.", "year": 2005}, {"title": "Tensor decompositions for learning latent variable models", "author": ["Animashree Anandkumar", "Rong Ge", "Daniel Hsu", "Sham M. Kakade", "Matus Telgarsky"], "venue": "J. Mach. Learn. Res.,", "citeRegEx": "Anandkumar et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Anandkumar et al\\.", "year": 2014}, {"title": "Learning topic models\u2013going beyond svd", "author": ["Sanjeev Arora", "Rong Ge", "Ankur Moitra"], "venue": "In Foundations of Computer Science (FOCS),", "citeRegEx": "Arora et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Arora et al\\.", "year": 2012}, {"title": "New algorithms for learning incoherent and overcomplete dictionaries", "author": ["Sanjeev Arora", "Rong Ge", "Ankur Moitra"], "venue": "arXiv preprint arXiv:1308.6273,", "citeRegEx": "Arora et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Arora et al\\.", "year": 2013}, {"title": "Simple, efficient, and neural algorithms for sparse coding", "author": ["Sanjeev Arora", "Rong Ge", "Tengyu Ma", "Ankur Moitra"], "venue": "In Proceedings of The 28th Conference on Learning Theory, COLT", "citeRegEx": "Arora et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Arora et al\\.", "year": 2015}, {"title": "A discriminative framework for clustering via similarity functions", "author": ["Maria-Florina Balcan", "Avrim Blum", "Santosh Vempala"], "venue": "In Proceedings of the Fortieth Annual ACM Symposium on Theory of Computing,", "citeRegEx": "Balcan et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Balcan et al\\.", "year": 2008}, {"title": "Measures of clustering quality: A working set of axioms for clustering", "author": ["Shai Ben-David", "Margareta Ackerman"], "venue": "Advances in Neural Information Processing Systems", "citeRegEx": "Ben.David and Ackerman.,? \\Q2009\\E", "shortCiteRegEx": "Ben.David and Ackerman.", "year": 2009}, {"title": "Rate distortion theory: A mathematical basis for data compression", "author": ["Toby Berger"], "venue": null, "citeRegEx": "Berger.,? \\Q1971\\E", "shortCiteRegEx": "Berger.", "year": 1971}, {"title": "Rounding sum-of-squares relaxations", "author": ["Boaz Barak", "Jonathan A. Kelner", "David Steurer"], "venue": "In STOC,", "citeRegEx": "Barak et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Barak et al\\.", "year": 2014}, {"title": "Dictionary learning and tensor decomposition via the sum-of-squares method", "author": ["Boaz Barak", "Jonathan A. Kelner", "David Steurer"], "venue": "In Proceedings of the Forty-Seventh Annual ACM on Symposium on Theory of Computing,", "citeRegEx": "Barak et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Barak et al\\.", "year": 2015}, {"title": "Dictionary learning and tensor decomposition via the sum-of-squares method", "author": ["Boaz Barak", "Jonathan A. Kelner", "David Steurer"], "venue": "In Proceedings of the Forty-seventh Annual ACM Symposium on Theory of Computing,", "citeRegEx": "Barak et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Barak et al\\.", "year": 2015}, {"title": "Tensor prediction, rademacher complexity and random 3-xor", "author": ["Boaz Barak", "Ankur Moitra"], "venue": "CoRR, abs/1501.06521,", "citeRegEx": "Barak and Moitra.,? \\Q2015\\E", "shortCiteRegEx": "Barak and Moitra.", "year": 2015}, {"title": "Noisy tensor completion via the sum-of-squares hierarchy", "author": ["Boaz Barak", "Ankur Moitra"], "venue": "In Proceedings of the 29th Conference on Learning Theory, COLT 2016,", "citeRegEx": "Barak and Moitra.,? \\Q2016\\E", "shortCiteRegEx": "Barak and Moitra.", "year": 2016}, {"title": "Latent dirichlet allocation", "author": ["David M. Blei", "Andrew Y. Ng", "Michael I. Jordan"], "venue": "J. Mach. Learn. Res.,", "citeRegEx": "Blei et al\\.,? \\Q2003\\E", "shortCiteRegEx": "Blei et al\\.", "year": 2003}, {"title": "Sum-of-squares proofs and the quest toward optimal algorithms", "author": ["Boaz Barak", "David Steurer"], "venue": "In Proceedings of International Congress of Mathematicians (ICM),", "citeRegEx": "Barak and Steurer.,? \\Q2014\\E", "shortCiteRegEx": "Barak and Steurer.", "year": 2014}, {"title": "Elements of Information Theory (Wiley Series in Telecommunications and Signal Processing)", "author": ["Thomas M. Cover", "Joy A. Thomas"], "venue": null, "citeRegEx": "Cover and Thomas.,? \\Q2006\\E", "shortCiteRegEx": "Cover and Thomas.", "year": 2006}, {"title": "Uncertainty principles and ideal atomic decomposition", "author": ["D.L. Donoho", "X. Huo"], "venue": "IEEE Trans. Inf. Theor.,", "citeRegEx": "Donoho and Huo.,? \\Q2006\\E", "shortCiteRegEx": "Donoho and Huo.", "year": 2006}, {"title": "Decomposing overcomplete 3rd order tensors using sum-ofsquares algorithms. In Approximation, Randomization, and Combinatorial Optimization", "author": ["Rong Ge", "Tengyu Ma"], "venue": "Algorithms and Techniques,", "citeRegEx": "Ge and Ma.,? \\Q2015\\E", "shortCiteRegEx": "Ge and Ma.", "year": 2015}, {"title": "Projection-free online learning", "author": ["Elad Hazan", "Satyen Kale"], "venue": "In Proceedings of the 29th International Conference on Machine Learning,", "citeRegEx": "Hazan and Kale.,? \\Q2012\\E", "shortCiteRegEx": "Hazan and Kale.", "year": 2012}, {"title": "Learning mixtures of spherical gaussians: moment methods and spectral decompositions", "author": ["Daniel Hsu", "Sham M Kakade"], "venue": "In Proceedings of the 4th conference on Innovations in Theoretical Computer Science,", "citeRegEx": "Hsu and Kakade.,? \\Q2013\\E", "shortCiteRegEx": "Hsu and Kakade.", "year": 2013}, {"title": "Tensor principal component analysis via sum-of-square proofs", "author": ["Samuel B. Hopkins", "Jonathan Shi", "David Steurer"], "venue": "In Proceedings of The 28th Conference on Learning Theory, COLT", "citeRegEx": "Hopkins et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Hopkins et al\\.", "year": 2015}, {"title": "Fast spectral algorithms from sum-of-squares proofs: tensor decomposition and planted sparse vectors", "author": ["Samuel B. Hopkins", "Tselil Schramm", "Jonathan Shi", "David Steurer"], "venue": "In Proceedings of the 48th Annual ACM SIGACT Symposium on Theory of Computing,", "citeRegEx": "Hopkins et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Hopkins et al\\.", "year": 2016}, {"title": "A lower bound on compression of unknown alphabets", "author": ["Nikola Jevtic", "Alon Orlitsky", "Narayana P. Santhanam"], "venue": "Theor. Comput. Sci.,", "citeRegEx": "Jevtic et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Jevtic et al\\.", "year": 2005}, {"title": "An impossibility theorem for clustering", "author": ["Jon M. Kleinberg"], "venue": "Advances in Neural Information Processing Systems", "citeRegEx": "Kleinberg.,? \\Q2003\\E", "shortCiteRegEx": "Kleinberg.", "year": 2003}, {"title": "Global optimization with polynomials and the problem of moments", "author": ["Jean B. Lasserre"], "venue": "SIAM Journal on Optimization,", "citeRegEx": "Lasserre.,? \\Q2001\\E", "shortCiteRegEx": "Lasserre.", "year": 2001}, {"title": "An introduction to polynomial and semi-algebraic optimization. Cambridge Texts in Applied Mathematics", "author": ["Jean Bernard Lasserre"], "venue": null, "citeRegEx": "Lasserre.,? \\Q2015\\E", "shortCiteRegEx": "Lasserre.", "year": 2015}, {"title": "Sums of squares, moment matrices and optimization over polynomials", "author": ["Monique Laurent"], "venue": "Emerging Applications of Algebraic Geometry,", "citeRegEx": "Laurent.,? \\Q2009\\E", "shortCiteRegEx": "Laurent.", "year": 2009}, {"title": "Vanishing component analysis", "author": ["Roi Livni", "David Lehavi", "Sagi Schein", "Hila Nachlieli", "Shai Shalev-Shwartz", "Amir Globerson"], "venue": "In Proceedings of the 30th International Conference on Machine Learning,", "citeRegEx": "Livni et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Livni et al\\.", "year": 2013}, {"title": "Foundations of machine learning", "author": ["Mehryar Mohri", "Afshin Rostamizadeh", "Ameet Talwalkar"], "venue": "MIT press,", "citeRegEx": "Mohri et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Mohri et al\\.", "year": 2012}, {"title": "Polynomial-time tensor decompositions with sum-of-squares", "author": ["Tengyu Ma", "Jonathan Shi", "David Steurer"], "venue": "In FOCS 2016,", "citeRegEx": "Ma et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Ma et al\\.", "year": 2016}, {"title": "Sum-of-squares lower bounds for sparse PCA", "author": ["Tengyu Ma", "Avi Wigderson"], "venue": "In Advances in Neural Information Processing Systems 28: Annual Conference on Neural Information Processing Systems", "citeRegEx": "Ma and Wigderson.,? \\Q2015\\E", "shortCiteRegEx": "Ma and Wigderson.", "year": 2015}, {"title": "Sparse approximate solutions to linear systems", "author": ["B.K. Natarajan"], "venue": "SIAM J. Comput.,", "citeRegEx": "Natarajan.,? \\Q1995\\E", "shortCiteRegEx": "Natarajan.", "year": 1995}, {"title": "Universal compression of memoryless sources over unknown alphabets", "author": ["Alon Orlitsky", "Narayana P. Santhanam", "Junan Zhang"], "venue": "IEEE Trans. Information Theory,", "citeRegEx": "Orlitsky et al\\.,? \\Q2004\\E", "shortCiteRegEx": "Orlitsky et al\\.", "year": 2004}, {"title": "Structured Semidefinite Programs and Semialgebraic Geometry Methods in Robustness and Optimization", "author": ["Pablo A. Parrilo"], "venue": "PhD thesis, California Institute of Technology,", "citeRegEx": "Parrilo.,? \\Q2000\\E", "shortCiteRegEx": "Parrilo.", "year": 2000}, {"title": "Compressive feature learning", "author": ["Hristo S Paskov", "Robert West", "John C Mitchell", "Trevor Hastie"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Paskov et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Paskov et al\\.", "year": 2013}, {"title": "Modeling by shortest data", "author": ["Jorma Rissanen"], "venue": "description. Automatica,", "citeRegEx": "Rissanen.,? \\Q1978\\E", "shortCiteRegEx": "Rissanen.", "year": 1978}, {"title": "Learning Deep Generative Models", "author": ["Ruslan Salakhutdinov"], "venue": "PhD thesis,", "citeRegEx": "Salakhutdinov.,? \\Q2009\\E", "shortCiteRegEx": "Salakhutdinov.", "year": 2009}, {"title": "Rank, trace-norm and max-norm", "author": ["Nathan Srebro", "Adi Shraibman"], "venue": "In Learning Theory, 18th Annual Conference on Learning Theory, COLT 2005, Bertinoro,", "citeRegEx": "Srebro and Shraibman.,? \\Q2005\\E", "shortCiteRegEx": "Srebro and Shraibman.", "year": 2005}, {"title": "Learning and Generalization with the Information Bottleneck, pages 92\u2013107", "author": ["Ohad Shamir", "Sivan Sabato", "Naftali Tishby"], "venue": null, "citeRegEx": "Shamir et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Shamir et al\\.", "year": 2008}, {"title": "Regression shrinkage and selection via the lasso", "author": ["Robert Tibshirani"], "venue": "Journal of the Royal Statistical Society. Series B (Methodological),", "citeRegEx": "Tibshirani.,? \\Q1996\\E", "shortCiteRegEx": "Tibshirani.", "year": 1996}, {"title": "Banach-Mazur distances and finite-dimensional operator ideals. Pitman monographs and surveys in pure and applied mathematics", "author": ["N. Tomczak-Jaegermann"], "venue": "Longman Scientific & Technical,", "citeRegEx": "Tomczak.Jaegermann.,? \\Q1989\\E", "shortCiteRegEx": "Tomczak.Jaegermann.", "year": 1989}, {"title": "The information bottleneck method", "author": ["Naftali Tishby", "Fernando C.N. Pereira", "William Bialek"], "venue": "CoRR, physics/0004057,", "citeRegEx": "Tishby et al\\.,? \\Q2000\\E", "shortCiteRegEx": "Tishby et al\\.", "year": 2000}, {"title": "A theory of the learnable", "author": ["Leslie G Valiant"], "venue": "Communications of the ACM,", "citeRegEx": "Valiant.,? \\Q1984\\E", "shortCiteRegEx": "Valiant.", "year": 1984}, {"title": "Singular vectors under random perturbation", "author": ["Van Vu"], "venue": "Random Structures and Algorithms,", "citeRegEx": "Vu.,? \\Q2011\\E", "shortCiteRegEx": "Vu.", "year": 2011}], "referenceMentions": [], "year": 2016, "abstractText": "We give a novel formal theoretical framework for unsupervised learning with two distinctive characteristics. First, it does not assume any generative model and based on a worst-case performance metric. Second, it is comparative, namely performance is measured with respect to a given hypothesis class. This allows to avoid known computational hardness results and improper algorithms based on convex relaxations. We show how several families of unsupervised learning models, which were previously only analyzed under probabilistic assumptions and are otherwise provably intractable, can be efficiently learned in our framework by convex optimization.", "creator": "LaTeX with hyperref package"}}}