{"id": "1610.02136", "review": {"conference": "iclr", "VERSION": "v1", "DATE_OF_SUBMISSION": "7-Oct-2016", "title": "A Baseline for Detecting Misclassified and Out-of-Distribution Examples in Neural Networks", "abstract": "We consider the two related problems of detecting if an example is misclassified or out-of-distribution. We present a simple baseline that utilizes probabilities from softmax distributions. Correctly classified examples tend to have greater maximum softmax probabilities than erroneously classified and out-of-distribution examples, allowing for their detection. We assess performance by defining several tasks in computer vision, natural language processing, and automatic speech recognition, showing the effectiveness of this baseline across all. We then show the baseline can sometimes be surpassed, demonstrating the room for future research on these underexplored detection tasks.", "histories": [["v1", "Fri, 7 Oct 2016 04:06:01 GMT  (55kb,D)", "https://arxiv.org/abs/1610.02136v1", null], ["v2", "Thu, 23 Mar 2017 18:11:25 GMT  (59kb,D)", "http://arxiv.org/abs/1610.02136v2", "Published as a conference paper at ICLR 2017. 1 Figure and 1 Appendix on Confidence Calibration"]], "reviews": [], "SUBJECTS": "cs.NE cs.CV cs.LG", "authors": ["dan hendrycks", "kevin gimpel"], "accepted": true, "id": "1610.02136"}, "pdf": {"name": "1610.02136.pdf", "metadata": {"source": "CRF", "title": null, "authors": ["NEURAL NETWORKS", "Dan Hendrycks", "Kevin Gimpel"], "emails": ["dan@ttic.edu", "kgimpel@ttic.edu"], "sections": [{"heading": "1 INTRODUCTION", "text": "In fact, it is so that most people are able to know themselves and to understand how they have behaved. (...) It is as if they were able to outwit themselves. (...) \"It is so.\" (...) \"It is so.\" (...) \"It is so.\" (...) \"It is so.\" (...) \"It is so.\" (...) \"It is so.\" (...) \"It is so.\" (...) \"(...)\" It is so. \"(...)\" It is so. \"(...)\" It is so. \"(...)\" It is so. \"(...)\" (It is. \"(...)\" (It is. \"(It is.)\" (It is. \"(It is.)\" (It is. \"(It is.)\" (It is. \"(It is.)\" (It is. \"(It is.)\" (It is. \"(It is.)\" (It is. \"(It is.)\" (It is. \"(It is.)\" (It is. \"(It is.\" It is. \"(It is.\" It is. \")\" (It. \"It is.\" (It is. \")\" (It. \"It is.\" (It is. \"It is.\") \"(It is.\" (It is. \"(It is.\" It. \")\" (It is. \"(It is.\") \"(It is.\" (It is. \")\" (It. \"(It is.\" It is. \"(It is.\" (It is. \")\" (It is. \")\" (It is. \"(It is.\" (It is. \")\" (It is. \"(It is.\") \"(It is.\") \"(It is.\" (It is. \"It.\" (It is. \")\" (It is. \")\" (It is. \"(It is.\" (It is. \").\" (It. \"(It is.\" It is. \"(It is.\"). \"(It is.\" It is. \"It is.\" (It is. \").\" (It is. \"It is.\""}, {"heading": "2 PROBLEM FORMULATION AND EVALUATION", "text": "This year, it is as if it were a reactionary party that is able to choose a party that is able to choose a party."}, {"heading": "3 SOFTMAX PREDICTION PROBABILITY AS A BASELINE", "text": "In what follows, we retrieve the maximum / predicted class probability from a softmax distribution, detecting whether an example is incorrectly classified or not distributed.3 Specifically, we separate correctly and incorrectly classified test sets, and calculate for each example the softmax probability of the predicted class, i.e. the maximum softmax probability. From these two groups, we obtain the range under PR and ROC curves. These ranges summarize the performance of a binary classifier that discriminates against values / values (in this case maximum probabilities from the softmaxes) across various thresholds. This description treats correctly classified examples as the positive class, denounced \"success\" or \"succ\" in our tables. In \"Error\" or \"Err\" we treat the incorrectly classified examples as the positive class; to do so, we refer to the incorrectly classified examples as positive and take the negatives of the predicted classes as the score.In the classification rate, we treat the \"predetermined examples\" with the probability."}, {"heading": "3.1 COMPUTER VISION", "text": "In the following computer systems we will use three datasets: MNIST, CIFAR-10, and CIFAR100 (Krizhevsky, 2009). MNIST is a dataset of handwritten digits consisting of 60000 trainers and 10000 test examples. Meanwhile, CIFAR-10 has color images belonging to 10 different classes, and 10000 test examples. CIFAR-100 is more difficult because there are 100 different classes with 50000 test examples. In Table 1 we see that correctly classified and incorrectly classified examples are sufficient to allow reliable discrimination."}, {"heading": "3.2 NATURAL LANGUAGE PROCESSING", "text": "Let's turn to a variety of tasks and architectures used in processing natural language."}, {"heading": "3.2.1 SENTIMENT CLASSIFICATION", "text": "The first NLP task is the binary sentiment classification using the IMDB dataset (Maas et al., 2011), a dataset of polarized film reviews with 25,000 training and 25,000 test reviews. This task allows us to determine whether classifiers trained on a relatively small dataset still produce informative soft-max distributions. For this task, we use a linear classifier that takes the average of traceable randomly initialized word vectors with dimension 50 (Joulin et al., 2016; Iyyer et al., 2015) as input. We train with Adam for 15 epochs and stop early, based on 5000 training reviews provided. Here, too, Table 3 shows that the Softmax distributions differ between correctly and wrongly classified examples, so that predictive truths allow us to reliably identify which examples are right and wrong.Now we are using the Customer Review (2002, Hu & IMDB) and non-distributable data sets (2004, Hu & IMDB)."}, {"heading": "3.2.2 TEXT CATEGORIZATION", "text": "We turn to text categorization tasks to determine whether Softmax distributions are useful for identifying similar but uncommon examples. In the following text categorization tasks, we train classifiers to predict the topic of the text they are processing. In the 20 Newsgroups dataset (Lang, 1995), there are 20 different newsgroup subjects with a total of 20,000 documents for the entire dataset. The Reuters 8 dataset (Lewis et al., 2004) includes eight different news subjects with a total of nearly 8,000 stories. The Reuters 52 dataset includes 52 news subjects with just over 9,000 news stories; this dataset can only have three stories for a single subject. For the 20 newsgroups datasets, we train a linear classifier to 30-dimensional word vectors for 20 epochs. Meanwhile, Reuters 8 and Retuers use 52 single-layer neural networks with a bag of word entry and a GELU non-linearity, all of which are epochal for Adam 5."}, {"heading": "3.2.3 PART-OF-SPEECH TAGGING", "text": "For social media, we use tweets with POS comments (Hochreiter & Schmidhuber, 1997) with three layers, 128 neurons per layer, with randomly initialized word vectors; for the WSJ tagger, we train a bidirectional short-term memory with recurring neural networks (Hochreiter & Schmidhuber, 1997) with three layers, 128 neurons per layer, with randomly initialized word vectors, and this is used on 90% of the corpus for 10 epochs of stochastic gradient descent with a stack size of 32. The tweet tagger is simpler because it is a two-layer neural network with a GELU nonlinearity, while weight initialization by (Hendrycks & Gimpel, 2016c prepoc) is word distribution."}, {"heading": "3.3 AUTOMATIC SPEECH RECOGNITION", "text": "Our sequence prediction system uses a bidirectional LSTM with two layers and a truncated GELU nonlinearity, optimized for 60 epochs with RMS Prop, trained on 80% of the TIMIT corpus (Garofolo et al., 1993). The LSTM is trained on connectionistic time classification (CTC) (Graves et al., 2006) to predict sequences of phones equipped with MFCCs, energy, and first and second deltas of a 25ms frame. When trained with CTC, the LSTM learns to briefly increase the probabilities of its phone identification while predicting otherwise mostly empty symbols. In this way, the Softmax is used differently from typical classification problems that provide a clear test for our detection methods."}, {"heading": "4 ABNORMALITY DETECTION WITH AUXILIARY DECODERS", "text": "To demonstrate this, we use the learned internal representations of neural networks. We start with the training of a normal classifier and add an auxiliary decoder that reconstructs the input, as shown in Figure 1. Auxiliary decoders are sometimes known to increase the classification performance (Zhang et al., 2016). Decoder and scorer are jointly trained on examples in the distribution, then the blue layers are frozen in Figure 1. Then we train red layers on clean and low-noise training examples, and the sigmoid output of the red layers is evaluated on how normal the input is. As a result, low-noise examples are in the normal class, clean examples in the normal class, and the sigmoid output is trained on which class an input belongs to. Consequently, after the training, we have a normal classifier, an auxiliary decoder, a deviation module, and what we call a deviation from the possible normality."}, {"heading": "4.1 TIMIT", "text": "We test the Abnormalization Module by revisiting the TIMIT task with a different architecture and showing how these auxiliary components can greatly improve detection. The system is a three-layer, 1024-neuron classification system with an auxiliary decoder and abnormality module. This network takes 11 frames as input and has to predict the phone of the center frame, 26 features per frame. Weights are initialized by (Hendrycks & Gimpel, 2016c) This network trains for 20 epochs, and the abnormality module trains for two. The Abnormalization Module sees clean examples and, as negative examples, TIMIT examples distorted with either white noise (noise with its spectral density proportional to 1 / f2), or pink examples of train detection."}, {"heading": "4.2 MNIST", "text": "Finally, like in a previous experiment, we train an MNIST classifier with three layers of 256 width. This time, we also use an auxiliary decoder and an anomaly module instead of relying solely on Softmax statistics. In abnormal examples, we blur, twist or add Gaussian noise to the training images. Gains from the Anomaly module are shown in Table 11, and there is a consistent improvement in detection outside the sample compared to Softmax prediction realities. Even in strongly dissimilar examples, the Anomaly module can further improve detection."}, {"heading": "5 DISCUSSION AND FUTURE WORK", "text": "The anomaly module shows that in some cases the baseline can be exceeded by using the representations of a network, which suggests a variety of research directions. Some promising future paths may take advantage of intra-class variance: If the distance from one example to another of the same predicted class is unusually high, it may be outside of the distribution (Giryes et al., 2015). Another way is to feed in a vector that summarizes the activations of a layer into an RNN, a vector for each layer. The RNN may find that the activation patterns are abnormal for examples outside of distribution. Others may make the detections fine-grained: Is the example outside of distribution a known - unknown or an unknown - unknown example? Another way is not only to detect correct classifications, but to output the probability of correct detection. In Appendix B, we show a data base and evaluation metrics, the future research to be tested for the evaluation of the multiplicity of U.I.AR, but rather a multiplicity of classifications."}, {"heading": "6 CONCLUSION", "text": "We demonstrated a softmax probability basis for detecting errors and distribution errors across multiple architectures and numerous datasets, and then introduced the anomaly module, which provided better values for distinguishing between normal and abnormal examples in tested cases. The anomaly module shows that the baseline can be exceeded in some cases, and this implies that there is room for future research. We hope that other researchers will investigate architectures that make predictions for anomaly estimates, and that other more reliable methods for detecting errors and distributed inputs will follow, because it seems extremely important to us to know when a machine learning system is failing."}, {"heading": "ACKNOWLEDGMENTS", "text": "We would like to thank John Wieting, Hao Tang, Karen Livescu, Greg Shakhnarovich and our reviewers for their suggestions. We would also like to thank NVIDIA Corporation for donating several TITAN X GPUs used in this research."}, {"heading": "A ABNORMALITY MODULE EXAMPLE", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "B METRICS FOR MEANINGFUL CONFIDENCE VALUES", "text": "During this work, we show that the softmax probability of a predicted class is consistently high = 0.5% and is therefore a poor proxy for estimating the \"confidence\" in a classification. In addition to the fact that we are often high, the softmax prediction probability must be limited below a value greater than zero. However, to see this, note that a binary sentiment classifier has the minimum confidence in an out-of-distribution example of 50%, as the highest Softmax probability must be at least 50%. However, for an out-of-distribution example, we may want less confidence when reflecting the probability of correct classification. To assess how well a confidence model reflects the probability of correct classification, we present two metrics. First, we set the confidence model by c: X \u2192 [0, 1] where X is a set of inputs. An output close to 1 of c reflects high reliability, or a high probability \u2192 that the Y is based on its classification."}], "references": [{"title": "Concrete problems in ai safety", "author": ["Dario Amodei", "Chris Olah", "Jacob Steinhardt", "Paul Christiano", "John Schulman", "Dan Man\u00e9"], "venue": null, "citeRegEx": "Amodei et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Amodei et al\\.", "year": 2016}, {"title": "The relationship between precision-recall and ROC curves", "author": ["Jesse Davis", "Mark Goadrich"], "venue": "In International Conference on Machine Learning (ICML),", "citeRegEx": "Davis and Goadrich.,? \\Q2006\\E", "shortCiteRegEx": "Davis and Goadrich.", "year": 2006}, {"title": "An introduction to ROC analysis", "author": ["Tom Fawcett"], "venue": "Pattern Recognition Letters,", "citeRegEx": "Fawcett.,? \\Q2005\\E", "shortCiteRegEx": "Fawcett.", "year": 2005}, {"title": "TIMIT Acoustic-Phonetic Continuous Speech Corpus", "author": ["John Garofolo", "Lori Lamel", "William Fisher", "Jonathan Fiscus", "David Pallett", "Nancy Dahlgren", "Victor Zue"], "venue": "Linguistic Data Consortium,", "citeRegEx": "Garofolo et al\\.,? \\Q1993\\E", "shortCiteRegEx": "Garofolo et al\\.", "year": 1993}, {"title": "Part-of-Speech Tagging for Twitter: Annotation, Features, and Experiments", "author": ["Kevin Gimpel", "Nathan Schneider", "Brendan O\u2032Connor", "Dipanjan Das", "Daniel Mills", "Jacob Eisenstein", "Michael Heilman", "Dani Yogatama", "Jeffrey Flanigan", "Noah A. Smith"], "venue": "Association for Computational Linguistics (ACL),", "citeRegEx": "Gimpel et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Gimpel et al\\.", "year": 2011}, {"title": "Deep neural networks with random gaussian weights: A universal classification strategy", "author": ["Raja Giryes", "Guillermo Sapiro", "Alex M. Bronstein"], "venue": null, "citeRegEx": "Giryes et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Giryes et al\\.", "year": 2015}, {"title": "Explaining and harnessing adversarial examples", "author": ["Ian J. Goodfellow", "Jonathon Shlens", "Christian Szegedy"], "venue": "In International Conference on Learning Representations (ICLR),", "citeRegEx": "Goodfellow et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Goodfellow et al\\.", "year": 2015}, {"title": "Connectionist temporal classification: Labeling unsegmented sequence data with recurrent neural networks", "author": ["Alex Graves", "Santiago Fern\u00e1ndez", "Faustino Gomez", "J\u00fcrgen Schmidhuber"], "venue": "In International Conference on Machine Learning (ICML),", "citeRegEx": "Graves et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Graves et al\\.", "year": 2006}, {"title": "Methods for detecting adversarial images and a colorful saliency", "author": ["Dan Hendrycks", "Kevin Gimpel"], "venue": "map. arXiv,", "citeRegEx": "Hendrycks and Gimpel.,? \\Q2016\\E", "shortCiteRegEx": "Hendrycks and Gimpel.", "year": 2016}, {"title": "Bridging nonlinearities and stochastic regularizers with Gaussian error linear units. arXiv, 2016b", "author": ["Dan Hendrycks", "Kevin Gimpel"], "venue": null, "citeRegEx": "Hendrycks and Gimpel.,? \\Q2016\\E", "shortCiteRegEx": "Hendrycks and Gimpel.", "year": 2016}, {"title": "Adjusting for dropout variance in batch normalization and weight initialization", "author": ["Dan Hendrycks", "Kevin Gimpel"], "venue": null, "citeRegEx": "Hendrycks and Gimpel.,? \\Q2016\\E", "shortCiteRegEx": "Hendrycks and Gimpel.", "year": 2016}, {"title": "The Aurora experimental framework for the performance evaluation of speech recognition systems under noisy conditions", "author": ["Hans-G\u00fcnter Hirsch", "David Pearce"], "venue": "ISCA ITRW ASR2000,", "citeRegEx": "Hirsch and Pearce.,? \\Q2000\\E", "shortCiteRegEx": "Hirsch and Pearce.", "year": 2000}, {"title": "Long short-term memory", "author": ["Sepp Hochreiter", "J\u00fcrgen Schmidhuber"], "venue": "Neural Computation,", "citeRegEx": "Hochreiter and Schmidhuber.,? \\Q1997\\E", "shortCiteRegEx": "Hochreiter and Schmidhuber.", "year": 1997}, {"title": "Mining and Summarizing Customer Reviews", "author": ["Minqing Hu", "Bing Liu"], "venue": "Knowledge Discovery and Data Mining (KDD),", "citeRegEx": "Hu and Liu.,? \\Q2004\\E", "shortCiteRegEx": "Hu and Liu.", "year": 2004}, {"title": "Iii. Deep Unordered Composition Rivals Syntactic Methods for Text Classification", "author": ["Mohit Iyyer", "Varun Manjunatha", "Jordan Boyd-Graber", "Hal Daum\u00e9"], "venue": "Association for Computational Linguistics (ACL),", "citeRegEx": "Iyyer et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Iyyer et al\\.", "year": 2015}, {"title": "Bag of tricks for efficient text classification", "author": ["Armand Joulin", "Edouard Grave", "Piotr Bojanowski", "Tomas Mikolov"], "venue": null, "citeRegEx": "Joulin et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Joulin et al\\.", "year": 2016}, {"title": "Adam: A Method for Stochastic Optimization", "author": ["Diederik Kingma", "Jimmy Ba"], "venue": "International Conference for Learning Representations (ICLR),", "citeRegEx": "Kingma and Ba.,? \\Q2015\\E", "shortCiteRegEx": "Kingma and Ba.", "year": 2015}, {"title": "Learning Multiple Layers of Features from Tiny Images", "author": ["Alex Krizhevsky"], "venue": null, "citeRegEx": "Krizhevsky.,? \\Q2009\\E", "shortCiteRegEx": "Krizhevsky.", "year": 2009}, {"title": "Human-level concept learning through probabilistic program induction", "author": ["Brenden M. Lake", "Ruslan Salakhutdinov", "Joshua B. Tenenbaum"], "venue": null, "citeRegEx": "Lake et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Lake et al\\.", "year": 2015}, {"title": "Newsweeder: Learning to filter netnews", "author": ["Ken Lang"], "venue": "In International Conference on Machine Learning (ICML),", "citeRegEx": "Lang.,? \\Q1995\\E", "shortCiteRegEx": "Lang.", "year": 1995}, {"title": "Rcv1: A new benchmark collection for text categorization research", "author": ["David D. Lewis", "Yiming Yang", "Tony G. Rose", "Fan Li"], "venue": "Journal of Machine Learning Research (JMLR),", "citeRegEx": "Lewis et al\\.,? \\Q2004\\E", "shortCiteRegEx": "Lewis et al\\.", "year": 2004}, {"title": "Sgdr: Stochastic gradient descent with restarts", "author": ["Ilya Loshchilov", "Frank Hutter"], "venue": null, "citeRegEx": "Loshchilov and Hutter.,? \\Q2016\\E", "shortCiteRegEx": "Loshchilov and Hutter.", "year": 2016}, {"title": "Learning word vectors for sentiment analysis", "author": ["Andrew L. Maas", "Raymond E. Daly", "Peter T. Pham", "Dan Huang", "Andrew Y. Ng", "Christopher Potts"], "venue": "In Association for Computational Linguistics (ACL),", "citeRegEx": "Maas et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Maas et al\\.", "year": 2011}, {"title": "Foundations of Statistical Natural Language Processing", "author": ["Chris Manning", "Hinrich Sch\u00fctze"], "venue": null, "citeRegEx": "Manning and Sch\u00fctze.,? \\Q1999\\E", "shortCiteRegEx": "Manning and Sch\u00fctze.", "year": 1999}, {"title": "Building a large annotated corpus of English: The Penn Treebank", "author": ["Mitchell P. Marcus", "Mary Ann Marcinkiewicz", "Beatrice Santorini"], "venue": "Computational linguistics,", "citeRegEx": "Marcus et al\\.,? \\Q1993\\E", "shortCiteRegEx": "Marcus et al\\.", "year": 1993}, {"title": "Deep neural networks are easily fooled: High confidence predictions for unrecognizable images", "author": ["Anh Nguyen", "Jason Yosinski", "Jeff Clune"], "venue": "In Computer Vision and Pattern Recognition (CVPR),", "citeRegEx": "Nguyen et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Nguyen et al\\.", "year": 2015}, {"title": "Posterior calibration and exploratory analysis for natural language processing models", "author": ["Khanh Nguyen", "Brendan O\u2019Connor"], "venue": "In Empirical Methods in Natural Language Processing (EMNLP),", "citeRegEx": "Nguyen and O.Connor.,? \\Q2015\\E", "shortCiteRegEx": "Nguyen and O.Connor.", "year": 2015}, {"title": "Improved part-of-speech tagging for online conversational text with word clusters", "author": ["Olutobi Owoputi", "Brendan O\u2019Connor", "Chris Dyer", "Kevin Gimpel", "Nathan Schneider", "Noah A. Smith"], "venue": "In North American Chapter of the Association for Computational Linguistics (NAACL),", "citeRegEx": "Owoputi et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Owoputi et al\\.", "year": 2013}, {"title": "Thumbs up? sentiment classification using machine learning techniques", "author": ["Bo Pang", "Lillian Lee", "Shivakumar Vaithyanathan"], "venue": "In Empirical Methods in Natural Language Processing (EMNLP),", "citeRegEx": "Pang et al\\.,? \\Q2002\\E", "shortCiteRegEx": "Pang et al\\.", "year": 2002}, {"title": "The case against accuracy estimation for comparing induction algorithms", "author": ["Joan Pastor-Pellicer", "Francisco Zamora-Mart\u0131\u0301nez", "Salvador Espa\u00f1a-Boquera", "Mar\u0131\u0301a Jos\u00e9 CastroBleda"], "venue": "In International Conference on Machine Learning (ICML),", "citeRegEx": "Pastor.Pellicer et al\\.,? \\Q1998\\E", "shortCiteRegEx": "Pastor.Pellicer et al\\.", "year": 1998}, {"title": "The case against accuracy estimation for comparing induction algorithms", "author": ["Foster Provost", "Tom Fawcett", "Ron Kohavi"], "venue": "In International Conference on Machine Learning (ICML),", "citeRegEx": "Provost et al\\.,? \\Q1998\\E", "shortCiteRegEx": "Provost et al\\.", "year": 1998}, {"title": "The precision-recall plot is more informative than the ROC plot when evaluating binary classifiers on imbalanced datasets", "author": ["Takaya Saito", "Marc Rehmsmeier"], "venue": "In PLoS ONE", "citeRegEx": "Saito and Rehmsmeier.,? \\Q2015\\E", "shortCiteRegEx": "Saito and Rehmsmeier.", "year": 2015}, {"title": "Investigation of deep neural networks for noise robust speech recognition", "author": ["Michael L. Seltzer", "Dong Yu", "Yongqiang Wang"], "venue": "In IEEE International Conference on Acoustics, Speech, and Signal Processing (ICASSP),", "citeRegEx": "Seltzer et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Seltzer et al\\.", "year": 2013}, {"title": "Unsupervised risk estimation using only conditional independence structure", "author": ["Jacob Steinhardt", "Percy Liang"], "venue": "In Neural Information Processing Systems (NIPS),", "citeRegEx": "Steinhardt and Liang.,? \\Q2016\\E", "shortCiteRegEx": "Steinhardt and Liang.", "year": 2016}, {"title": "Thchs-30 : A free chinese speech corpus", "author": ["Dong Wang", "Xuewei Zhang"], "venue": "In Technical Report,", "citeRegEx": "Wang and Zhang.,? \\Q2015\\E", "shortCiteRegEx": "Wang and Zhang.", "year": 2015}, {"title": "Confidence measures for hybrid hmm/ann speech recognition", "author": ["Gethin Williams", "Steve Renals"], "venue": "In Proceedings of EuroSpeech,", "citeRegEx": "Williams and Renals.,? \\Q1997\\E", "shortCiteRegEx": "Williams and Renals.", "year": 1997}, {"title": "Sun database: Large-scale scene recognition from abbey to zoo", "author": ["Jianxiong Xiao", "James Hays", "Krista A. Ehinger", "Aude Oliva", "Antonio Torralba"], "venue": "In IEEE Conference on Computer Vision and Pattern Recognition (CVPR),", "citeRegEx": "Xiao et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Xiao et al\\.", "year": 2010}, {"title": "Calibration of confidence measures in speech recognition", "author": ["Dong Yu", "Jinyu Li", "Li Deng"], "venue": "In IEEE Transactions on Audio, Speech, and Language,", "citeRegEx": "Yu et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Yu et al\\.", "year": 2010}, {"title": "Wide residual networks", "author": ["Sergey Zagoruyko", "Nikos Komodakis"], "venue": "British Machine Vision Conference,", "citeRegEx": "Zagoruyko and Komodakis.,? \\Q2016\\E", "shortCiteRegEx": "Zagoruyko and Komodakis.", "year": 2016}, {"title": "Augmenting supervised neural networks with unsupervised objectives for large-scale image classification", "author": ["Yuting Zhang", "Kibok Lee", "Honglak Lee"], "venue": "In International Conference on Machine Learning (ICML),", "citeRegEx": "Zhang et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Zhang et al\\.", "year": 2016}], "referenceMentions": [{"referenceID": 6, "context": "Worse, these classifiers often fail silently by providing highconfidence predictions while being woefully incorrect (Goodfellow et al., 2015; Amodei et al., 2016).", "startOffset": 116, "endOffset": 162}, {"referenceID": 0, "context": "Worse, these classifiers often fail silently by providing highconfidence predictions while being woefully incorrect (Goodfellow et al., 2015; Amodei et al., 2016).", "startOffset": 116, "endOffset": 162}, {"referenceID": 0, "context": "More generally and importantly, estimating when a model is in error is of great concern to AI Safety (Amodei et al., 2016).", "startOffset": 101, "endOffset": 122}, {"referenceID": 37, "context": "This is consistent with a great deal of anecdotal evidence from researchers (Nguyen & O\u2019Connor, 2015; Yu et al., 2010; Provost et al., 1998; Nguyen et al., 2015).", "startOffset": 76, "endOffset": 161}, {"referenceID": 30, "context": "This is consistent with a great deal of anecdotal evidence from researchers (Nguyen & O\u2019Connor, 2015; Yu et al., 2010; Provost et al., 1998; Nguyen et al., 2015).", "startOffset": 76, "endOffset": 161}, {"referenceID": 25, "context": "This is consistent with a great deal of anecdotal evidence from researchers (Nguyen & O\u2019Connor, 2015; Yu et al., 2010; Provost et al., 1998; Nguyen et al., 2015).", "startOffset": 76, "endOffset": 161}, {"referenceID": 30, "context": "If the negative class is far more likely than the positive class, a model may always guess the negative class and obtain high accuracy, which can be misleading (Provost et al., 1998).", "startOffset": 160, "endOffset": 182}, {"referenceID": 2, "context": "Moreover, the AUROC can be interpreted as the probability that a positive example has a greater detector score/value than a negative example (Fawcett, 2005).", "startOffset": 141, "endOffset": 156}, {"referenceID": 17, "context": "In the following computer vision tasks, we use three datasets: MNIST, CIFAR-10, and CIFAR100 (Krizhevsky, 2009).", "startOffset": 93, "endOffset": 111}, {"referenceID": 36, "context": "For CIFAR-10 and CIFAR-100, we use realistic images from the Scene UNderstanding dataset (SUN), which consists of 397 different scenes (Xiao et al., 2010).", "startOffset": 135, "endOffset": 154}, {"referenceID": 18, "context": "Omniglot (Lake et al., 2015) images are handwritten characters rather than the handwritten digits in MNIST.", "startOffset": 9, "endOffset": 28}, {"referenceID": 22, "context": "The first NLP task is binary sentiment classification using the IMDB dataset (Maas et al., 2011), a dataset of polarized movie reviews with 25000 training and 25000 test reviews.", "startOffset": 77, "endOffset": 96}, {"referenceID": 15, "context": "For this task we use a linear classifier taking as input the average of trainable, randomly initialized word vectors with dimension 50 (Joulin et al., 2016; Iyyer et al., 2015).", "startOffset": 135, "endOffset": 176}, {"referenceID": 14, "context": "For this task we use a linear classifier taking as input the average of trainable, randomly initialized word vectors with dimension 50 (Joulin et al., 2016; Iyyer et al., 2015).", "startOffset": 135, "endOffset": 176}, {"referenceID": 28, "context": "Now we use the Customer Review (Hu & Liu, 2004) and Movie Review (Pang et al., 2002) datasets as out-of-distribution examples.", "startOffset": 65, "endOffset": 84}, {"referenceID": 19, "context": "In the 20 Newsgroups dataset (Lang, 1995), there are 20 different newsgroup subjects with a total of 20000 documents for the whole dataset.", "startOffset": 29, "endOffset": 41}, {"referenceID": 20, "context": "The Reuters 8 (Lewis et al., 2004) dataset has eight different news subjects with nearly 8000 stories in total.", "startOffset": 14, "endOffset": 34}, {"referenceID": 24, "context": "We use the Wall Street Journal portion of the Penn Treebank (Marcus et al., 1993) which contains 45 distinct POS tags.", "startOffset": 60, "endOffset": 81}, {"referenceID": 4, "context": "For social media, we use POS-annotated tweets (Gimpel et al., 2011; Owoputi et al., 2013) which contain 25 tags.", "startOffset": 46, "endOffset": 89}, {"referenceID": 27, "context": "For social media, we use POS-annotated tweets (Gimpel et al., 2011; Owoputi et al., 2013) which contain 25 tags.", "startOffset": 46, "endOffset": 89}, {"referenceID": 27, "context": "The tweet tagger is simpler, as it is twolayer neural network with a GELU nonlinearity, a weight initialization according to (Hendrycks & Gimpel, 2016c), pretrained word vectors trained on a corpus of 56 million tweets (Owoputi et al., 2013), and a hidden layer size of 256, all while training on 1000 tweets for 30 epochs with Adam and early stopping with 327 validation tweets.", "startOffset": 219, "endOffset": 241}, {"referenceID": 3, "context": "Our sequence prediction system uses a bidirectional LSTM with two-layers and a clipped GELU nonlinearity, optimized for 60 epochs with RMSProp trained on 80% of the TIMIT corpus (Garofolo et al., 1993).", "startOffset": 178, "endOffset": 201}, {"referenceID": 7, "context": "The LSTM is trained with connectionist temporal classification (CTC) (Graves et al., 2006) for predicting sequences of phones given MFCCs, energy, and first and second deltas of a 25ms frame.", "startOffset": 69, "endOffset": 90}, {"referenceID": 39, "context": "Auxiliary decoders are sometimes known to increase classification performance (Zhang et al., 2016).", "startOffset": 78, "endOffset": 98}, {"referenceID": 32, "context": "It is worth mentioning that fully connected deep neural networks are noise robust (Seltzer et al., 2013), yet the abnormality module can still detect whether an example is out-of-distribution.", "startOffset": 82, "endOffset": 104}, {"referenceID": 5, "context": "Some promising future avenues may utilize the intra-class variance: if the distance from an example to another of the same predicted class is abnormally high, it may be out-of-distribution (Giryes et al., 2015).", "startOffset": 189, "endOffset": 210}], "year": 2017, "abstractText": "We consider the two related problems of detecting if an example is misclassified or out-of-distribution. We present a simple baseline that utilizes probabilities from softmax distributions. Correctly classified examples tend to have greater maximum softmax probabilities than erroneously classified and out-of-distribution examples, allowing for their detection. We assess performance by defining several tasks in computer vision, natural language processing, and automatic speech recognition, showing the effectiveness of this baseline across all. We then show the baseline can sometimes be surpassed, demonstrating the room for future research on these underexplored detection tasks.", "creator": "LaTeX with hyperref package"}}}