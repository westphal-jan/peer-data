{"id": "1610.02242", "review": {"conference": "iclr", "VERSION": "v1", "DATE_OF_SUBMISSION": "7-Oct-2016", "title": "Temporal Ensembling for Semi-Supervised Learning", "abstract": "In this paper, we present a simple and efficient method for training deep neural networks in a semi-supervised setting where only a small portion of training data is labeled. We introduce temporal ensembling, where we form a consensus prediction of the unknown labels under multiple instances of the network-in-training on different epochs, and most importantly, under different regularization and input augmentation conditions. This ensemble prediction can be expected to be a better predictor for the unknown labels than the output of the network at the most recent training epoch, and can thus be used as a target for training. Using our method, we set new records for two standard semi-supervised learning benchmarks, reducing the classification error rate from 18.63% to 12.89% in CIFAR-10 with 4000 labels and from 18.44% to 6.83% in SVHN with 500 labels.", "histories": [["v1", "Fri, 7 Oct 2016 12:15:42 GMT  (21kb,D)", "http://arxiv.org/abs/1610.02242v1", null], ["v2", "Mon, 7 Nov 2016 13:27:40 GMT  (99kb,D)", "http://arxiv.org/abs/1610.02242v2", "This version was submitted to ICLR 2017. The two methods are now closer to each other and use similar parameters. Also added more experimental results and a test with corrupted labels. Code released"], ["v3", "Wed, 15 Mar 2017 14:22:41 GMT  (101kb,D)", "http://arxiv.org/abs/1610.02242v3", "Final ICLR 2017 version. Includes new results for CIFAR-100 with additional unlabeled data from Tiny Images dataset"]], "reviews": [], "SUBJECTS": "cs.NE cs.LG", "authors": ["samuli laine", "timo aila"], "accepted": true, "id": "1610.02242"}, "pdf": {"name": "1610.02242.pdf", "metadata": {"source": "CRF", "title": "Temporal Ensembling for Semi-Supervised Learning", "authors": ["Samuli Laine", "Timo Aila"], "emails": ["slaine@nvidia.com", "taila@nvidia.com"], "sections": [{"heading": "1 Introduction", "text": "It has long been known that an ensemble of multiple neural networks generally provides better predictions than a single network within the ensemble. This effect has also been used indirectly when a single network is trained by dropout [20], Dropconnect [23] or stochastic depth [4] regulation methods, and in swapout networks [17], where training is always focused on a specific subset of the network, and therefore the entire network can be regarded as the implicit interaction of such trained subnetworks. We expand this idea by making ensemble predictions during the training, using the results of a single network at different training stages and under different regulatory and input conditions. Our training still operates on a single network, but the predictions made at different epochs correspond to an ensemble prediction of a large number of individual subnetworks, as failure regulation occurs at different training stages and under different conditions."}, {"heading": "2 \u03a0-Model for Enforcing Consistency", "text": "Our first step towards temporal similarity is to recognize that even with unlabeled training data, we can form a network so that it is consistent over multiple extensions of the same input, which is achieved by duplicating the network while sharing all traceable parameters between the two branches. We feed the training input into these two branches under two different extensions and downtime conditions. We then define a loss function that punishes differences in output between the two branches. This approach is similar to the PCB model [13], but is simpler conceptually. Unlike this model, the comparison is made directly on network outputs, i.e. after softmax activation, and there is no additional mapping between the two branches as opposed to the learned denoizing functions in the PCB architecture."}, {"heading": "3 Temporal Ensembling", "text": "We will describe our method (algorithm 1) in the traditional context of education protocols as follows: we are able, in two separate phases, to classify the training under a set of augmentations and to capture these predictions, and then we can expect the network to be identified as training targets using the predictions we have just received. Since the training targets obtained in this way are based on a single evaluation of augmented inputs, it can be expected that they will still be weighted with the same asymmetric loss functions by aggregating the predictions of multiple network calls into an ensemble prediction. Furthermore, it unifies the described and unlabelled inputs to use the same asymmetric loss function, albeit with a weighting between known and inferred labels. Finally, the temporary analogy lets us imagine that we have multiple copies of the same network as in our model."}, {"heading": "4 Results", "text": "Our network structure is shown in Table 3, and the test setup and all training parameters are described in detail in Appendix A. We test the N model and temporal similarity in CIFAR-10 and SVHN image classification tasks, indicating the mean and standard deviation of 10 runs from different random seeds."}, {"heading": "4.1 CIFAR-10", "text": "Unfortunately, it is difficult to make a true apple-to-apple comparison between the semi-monitored methods because it is often not specified whether dataset augmentation has been enabled or how many training examples have been used (73257 or 604388 in SVHN). Of our comparison methods, the only one that explicitly states that dataset augmentation for CIFAR-10 was not used."}, {"heading": "4.2 SVHN", "text": "The Streetview house number dataset (SVHN) consists of 32 x 32 pixel RGB images of real house numbers, and the task is to classify the middle digit. In SVHN, we chose to use only the official 73257 training examples, because otherwise the dataset would have been too simple. Even with this choice, the error rate for all labels is only 2.88% (compared to 1.69% for 604388 examples).Table 2 compares our method with the previous state of the art. For the most commonly used 1000 labels, we observe an improvement of 2.6 percentage points, from 8.11% to 5.49%. In this case, the time fade-in has not brought any advantage over the E model, and we suspect that the error rate is closer to the fully labeled case. Therefore, we have also examined the behavior with 500 labels, where we obtained an error rate of less than half of the Salane imet al [another 1.31 percent improvement with the E-time model]."}, {"heading": "5 Related Work", "text": "This year, it has reached the point where it will be able to take the lead in the EU in order to win the Presidency of the Council of the European Union."}, {"heading": "A Network architecture, test setup, and training parameters", "text": "It is heavily inspired by ConvPoolCNN-C [19] and the improvements of Salimans and Kingma [15]. All layers of data were initialized after him and al. [3], and we applied weight normalization and only mediocre batch normalization [15] to distinguish all of them. We used leaky ReLU [9] with a maximum learning rate of 0.003, except for temporal ensemblement in the SVHN case, where a maximum learning rate of 0.001, and chose max pooling because there were consistently better results than striped convolutions in our experiments. All networks were associated with Adam [5] with a maximum learning rate of 0.003, except for temporal ensemblement in the SVHN case, where a maximum learning rate of 0.001 and 0.999 as suggested in the newspaper."}], "references": [{"title": "Bagging predictors", "author": ["L. Breiman"], "venue": "Machine Learning, 24(2)", "citeRegEx": "1", "shortCiteRegEx": null, "year": 1996}, {"title": "et al", "author": ["S. Dieleman", "J. Schl\u00fcter", "C. Raffel", "E. Olson", "S.K. S\u00f8nderby"], "venue": "Lasagne: First release.", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2015}, {"title": "Delving deep into rectifiers: Surpassing human-level performance on imagenet classification", "author": ["K. He", "X. Zhang", "S. Ren", "J. Sun"], "venue": "CoRR, abs/1502.01852", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2015}, {"title": "Deep networks with stochastic depth", "author": ["G. Huang", "Y. Sun", "Z. Liu", "D. Sedra", "K.Q. Weinberger"], "venue": "CoRR, abs/1603.09382", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2016}, {"title": "Adam: A method for stochastic optimization", "author": ["D.P. Kingma", "J. Ba"], "venue": "CoRR, abs/1412.6980", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2014}, {"title": "Semi-supervised learning with deep generative models", "author": ["D.P. Kingma", "S. Mohamed", "D. Jimenez Rezende", "M. Welling"], "venue": "In Advances in Neural Information Processing Systems", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2014}, {"title": "Generalizing pooling functions in convolutional neural networks: Mixed", "author": ["C.-Y. Lee", "P.W. Gallagher", "Z. Tu"], "venue": "gated, and tree. CoRR, abs/1509.08985", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2015}, {"title": "Auxiliary deep generative models", "author": ["L. Maal\u00f8e", "C.K. S\u00f8nderby", "S.K. S\u00f8nderby", "O. Winther"], "venue": "CoRR, abs/1602.05473", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2016}, {"title": "Rectifier nonlinearities improve neural network acoustic models", "author": ["A.L. Maas", "A.Y. Hannun", "A. Ng"], "venue": "Proc. International Conference on Machine Learning (ICML), volume 30", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2013}, {"title": "All you need is a good init", "author": ["D. Mishkin", "J. Matas"], "venue": "Proc. International Conference on Learning Representations (ICLR)", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2016}, {"title": "Distributional smoothing with virtual adversarial training", "author": ["T. Miyato", "S. Maeda", "M. Koyama", "K. Nakae", "S. Ishii"], "venue": "Proc. International Conference on Learning Representations (ICLR)", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2016}, {"title": "Making neural networks robust to label noise: a loss correction approach", "author": ["G. Patrini", "A. Rozza", "A. Menon", "R. Nock", "L. Qu"], "venue": "CoRR, abs/1609.03683", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2016}, {"title": "Semi-supervised learning with ladder networks", "author": ["A. Rasmus", "M. Berglund", "M. Honkala", "H. Valpola", "T. Raiko"], "venue": "In Advances in Neural Information Processing Systems", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2015}, {"title": "Training deep neural networks on noisy labels with bootstrapping", "author": ["S.E. Reed", "H. Lee", "D. Anguelov", "C. Szegedy", "D. Erhan", "A. Rabinovich"], "venue": "CoRR, abs/1412.6596", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2014}, {"title": "Weight normalization: A simple reparameterization to accelerate training of deep neural networks", "author": ["T. Salimans", "D.P. Kingma"], "venue": "CoRR, abs/1602.07868", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2016}, {"title": "Improved techniques for training GANs", "author": ["T. Salimans", "I.J. Goodfellow", "W. Zaremba", "V. Cheung", "A. Radford", "X. Chen"], "venue": "CoRR, abs/1606.03498", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2016}, {"title": "Swapout: Learning an ensemble of deep architectures", "author": ["S. Singh", "D. Hoiem", "D.A. Forsyth"], "venue": "CoRR, abs/1605.06465", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2016}, {"title": "Unsupervised and semi-supervised learning with categorical generative adversarial networks", "author": ["J.T. Springenberg"], "venue": "Proc. International Conference on Learning Representations (ICLR)", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2016}, {"title": "Striving for simplicity: The all convolutional net", "author": ["J.T. Springenberg", "A. Dosovitskiy", "T. Brox", "M.A. Riedmiller"], "venue": "CoRR, abs/1412.6806", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2014}, {"title": "Dropout: A simple way to prevent neural networks from overfitting", "author": ["N. Srivastava", "G. Hinton", "A. Krizhevsky", "I. Sutskever", "R. Salakhutdinov"], "venue": "Journal of Machine Learning Research, 15:1929\u20131958", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2014}, {"title": "Training convolutional networks with noisy labels", "author": ["S. Sukhbaatar", "J. Bruna", "M. Paluri", "L. Bourdev", "R. Fergus"], "venue": "CoRR, abs/1406.2080", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2014}, {"title": "Regularization of neural networks using dropconnect", "author": ["L. Wan", "M. Zeiler", "S. Zhang", "Y.L. Cun", "R. Fergus"], "venue": "Proc. International Conference on Machine Learning (ICML), 28(3):1058\u20131066", "citeRegEx": "23", "shortCiteRegEx": null, "year": 2013}, {"title": "Bootstrapping via graph propagation", "author": ["M. Whitney", "A. Sarkar"], "venue": "Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics: Long Papers - Volume 1, ACL \u201912", "citeRegEx": "24", "shortCiteRegEx": null, "year": 2012}, {"title": "Unsupervised word sense disambiguation rivaling supervised methods", "author": ["D. Yarowsky"], "venue": "Proceedings of the 33rd Annual Meeting on Association for Computational Linguistics, ACL \u201995", "citeRegEx": "25", "shortCiteRegEx": null, "year": 1995}, {"title": "Semi-supervised learning literature survey", "author": ["X. Zhu"], "venue": "Technical Report 1530, Computer Sciences, University of Wisconsin-Madison", "citeRegEx": "26", "shortCiteRegEx": null, "year": 2005}], "referenceMentions": [{"referenceID": 19, "context": "This effect has also been indirectly exploited when training a single network through dropout [20], dropconnect [23], or stochastic depth [4] regularization methods, and in swapout networks [17], where training always focuses on a particular subset of the network, and thus the complete network can be seen as an implicit ensemble of such trained subnetworks.", "startOffset": 94, "endOffset": 98}, {"referenceID": 21, "context": "This effect has also been indirectly exploited when training a single network through dropout [20], dropconnect [23], or stochastic depth [4] regularization methods, and in swapout networks [17], where training always focuses on a particular subset of the network, and thus the complete network can be seen as an implicit ensemble of such trained subnetworks.", "startOffset": 112, "endOffset": 116}, {"referenceID": 3, "context": "This effect has also been indirectly exploited when training a single network through dropout [20], dropconnect [23], or stochastic depth [4] regularization methods, and in swapout networks [17], where training always focuses on a particular subset of the network, and thus the complete network can be seen as an implicit ensemble of such trained subnetworks.", "startOffset": 138, "endOffset": 141}, {"referenceID": 16, "context": "This effect has also been indirectly exploited when training a single network through dropout [20], dropconnect [23], or stochastic depth [4] regularization methods, and in swapout networks [17], where training always focuses on a particular subset of the network, and thus the complete network can be seen as an implicit ensemble of such trained subnetworks.", "startOffset": 190, "endOffset": 194}, {"referenceID": 12, "context": "Our \u03a0-model can be seen as a simplification of the \u0393-model of the ladder network [13], a previously presented network architecture for semi-supervised learning.", "startOffset": 81, "endOffset": 85}, {"referenceID": 13, "context": "[14] targeted for training with noisy labels.", "startOffset": 0, "endOffset": 4}, {"referenceID": 12, "context": "This approach is similar to the \u0393-model of the ladder network [13], but conceptually simpler.", "startOffset": 62, "endOffset": 66}, {"referenceID": 12, "context": "Finally, temporal ensembling lets us dispose of the notion of having multiple copies of the same network as in our \u03a0-model, or in the \u0393-model [13].", "startOffset": 142, "endOffset": 146}, {"referenceID": 12, "context": "Conv-Large, \u0393-model [13] 20.", "startOffset": 20, "endOffset": 24}, {"referenceID": 17, "context": "47 CatGAN [18] 19.", "startOffset": 10, "endOffset": 14}, {"referenceID": 15, "context": "[16] 18.", "startOffset": 0, "endOffset": 4}, {"referenceID": 5, "context": "Error rate (%) with # labels Model 500 1000 All (73257) DGN [6] 36.", "startOffset": 60, "endOffset": 63}, {"referenceID": 10, "context": "10 Virtual Adversarial [11] 24.", "startOffset": 23, "endOffset": 27}, {"referenceID": 7, "context": "63 Auxiliary Deep Generative Model [8] 22.", "startOffset": 35, "endOffset": 38}, {"referenceID": 7, "context": "86 Skip Deep Generative Model [8] 16.", "startOffset": 30, "endOffset": 33}, {"referenceID": 15, "context": "[16] 18.", "startOffset": 0, "endOffset": 4}, {"referenceID": 12, "context": "Of our comparison methods the \u0393-model [13] is the only one that explicitly says that augmentation was not used for CIFAR-10.", "startOffset": 38, "endOffset": 42}, {"referenceID": 6, "context": "When all labels are used for training, our network approximately matches the state-of-the-art error rate for a single model in CIFAR-10 with augmentation [7, 10] at 6.", "startOffset": 154, "endOffset": 161}, {"referenceID": 9, "context": "When all labels are used for training, our network approximately matches the state-of-the-art error rate for a single model in CIFAR-10 with augmentation [7, 10] at 6.", "startOffset": 154, "endOffset": 161}, {"referenceID": 14, "context": "04% and without augmentation [15] at 7.", "startOffset": 29, "endOffset": 33}, {"referenceID": 15, "context": "[16] with the \u03a0-model, and a further improvement of 1.", "startOffset": 0, "endOffset": 4}, {"referenceID": 24, "context": "There is a vast body of previous work on semi-supervised learning [26].", "startOffset": 66, "endOffset": 70}, {"referenceID": 12, "context": "\u0393-model is a subset of a ladder network [13] that introduces lateral connections into an encoderdecoder type network architecture, targeted at semi-supervised learning.", "startOffset": 40, "endOffset": 44}, {"referenceID": 0, "context": "In bootstrap aggregating, or bagging, multiple networks are trained independently based on subsets of training data [1].", "startOffset": 116, "endOffset": 119}, {"referenceID": 23, "context": "The general technique of inferring new labels from partially labeled data is often referred to as bootstrapping or self-training, and it was first proposed by Yarowsky [25] in the context of linguistic analysis.", "startOffset": 168, "endOffset": 172}, {"referenceID": 22, "context": "Whitney and Sarkar [24] analyze Yarowsky\u2019s algorithm and propose a novel graph-based label propagation approach.", "startOffset": 19, "endOffset": 23}, {"referenceID": 20, "context": "[21] and Patrini et al.", "startOffset": 0, "endOffset": 4}, {"referenceID": 11, "context": "[12].", "startOffset": 0, "endOffset": 4}, {"referenceID": 13, "context": "[14] presented a simple bootstrapping method that trains a classifier with the target composed of a convex combination of the previous epoch output and the known but potentially noisy labels.", "startOffset": 0, "endOffset": 4}, {"referenceID": 7, "context": "Generative Adversarial Networks (GAN) have been recently used for semi-supervised learning with promising results [8, 18, 16].", "startOffset": 114, "endOffset": 125}, {"referenceID": 17, "context": "Generative Adversarial Networks (GAN) have been recently used for semi-supervised learning with promising results [8, 18, 16].", "startOffset": 114, "endOffset": 125}, {"referenceID": 15, "context": "Generative Adversarial Networks (GAN) have been recently used for semi-supervised learning with promising results [8, 18, 16].", "startOffset": 114, "endOffset": 125}], "year": 2017, "abstractText": "In this paper, we present a simple and efficient method for training deep neural networks in a semi-supervised setting where only a small portion of training data is labeled. We introduce temporal ensembling, where we form a consensus prediction of the unknown labels under multiple instances of the network-in-training on different epochs, and most importantly, under different regularization and input augmentation conditions. This ensemble prediction can be expected to be a better predictor for the unknown labels than the output of the network at the most recent training epoch, and can thus be used as a target for training. Using our method, we set new records for two standard semi-supervised learning benchmarks, reducing the classification error rate from 18.63% to 12.89% in CIFAR-10 with 4000 labels and from 18.44% to 6.83% in SVHN with 500 labels.", "creator": "LaTeX with hyperref package"}}}