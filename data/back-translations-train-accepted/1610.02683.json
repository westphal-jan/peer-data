{"id": "1610.02683", "review": {"conference": "EMNLP", "VERSION": "v1", "DATE_OF_SUBMISSION": "9-Oct-2016", "title": "Interpreting Neural Networks to Improve Politeness Comprehension", "abstract": "We present an interpretable neural network approach to predicting and understanding politeness in natural language requests. Our models are based on simple convolutional neural networks directly on raw text, avoiding any manual identification of complex sentiment or syntactic features, while performing better than such feature-based models from previous work. More importantly, we use the challenging task of politeness prediction as a testbed to next present a much-needed understanding of what these successful networks are actually learning. For this, we present several network visualizations based on activation clusters, first derivative saliency, and embedding space transformations, helping us automatically identify several subtle linguistics markers of politeness theories. Further, this analysis reveals multiple novel, high-scoring politeness strategies which, when added back as new features, reduce the accuracy gap between the original featurized system and the neural model, thus providing a clear quantitative interpretation of the success of these neural networks.", "histories": [["v1", "Sun, 9 Oct 2016 14:42:58 GMT  (1181kb,D)", "http://arxiv.org/abs/1610.02683v1", "To appear at EMNLP 2016"]], "COMMENTS": "To appear at EMNLP 2016", "reviews": [], "SUBJECTS": "cs.CL cs.AI", "authors": ["malika aubakirova", "mohit bansal"], "accepted": true, "id": "1610.02683"}, "pdf": {"name": "1610.02683.pdf", "metadata": {"source": "CRF", "title": "Interpreting Neural Networks to Improve Politeness Comprehension", "authors": ["Malika Aubakirova", "Mohit Bansal"], "emails": ["aubakirova@uchicago.edu", "mbansal@cs.unc.edu"], "sections": [{"heading": "1 Introduction", "text": "Politeness theories (Brown and Levinson, 1987; Gu, 1990; Bargiela-Chiappini, 2003) incorporate important components such as modality, indirection, deference, and impersonalization. Positive politeness strategies therefore focus on giving the listener a good feeling through offers, promises, and jokes. Negative examples of politeness include courtesy seeking, commands, and requests. Distinguishing between courtesy types is a highly non-trivial task, as it depends on factors such as context, relative power, and culture. Danescu-Niculescu-Mizil et al. (2013) proposed a useful computational framework for predicting courtesy types in natural language requirements by outlining various lexical and syntactical features over important politeness theories, e.g. the first or second person begin vs. plural. However, the manual identification of such politeness functions is very challenging because there are multiple theories and polite in language."}, {"heading": "2 Related Work", "text": "Danescu-Niculescu-Mizil et al. (2013) presented one of the first useful datasets and computerized approaches for courtesy theories (Brown and Levinson, 1987; Goldsmith, 2007; Ka'da'r and Haugh, 2013; Locher and Watts, 2005), using manually defined lexical and syntactical features. Extensive previous work has also used models of the neural network for other tasks of sentiment analysis (Pang et al., 2011; Socher et al., 2013; Moraes et al., 2013; Dong et al., 2014; dos Santos and Gatti et al, 2014; Kalchbrenner et al., 2014). None of the above methods focused on the visualization and understanding of the inner workings of these neural networks."}, {"heading": "3 Approach", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "3.1 Convolutional Neural Networks", "text": "For a sentence v1: n (where each word vi is a d-dimming vector), a filter m applied to a window with t-words generates a convolution characteristic ci = f (m * vi: i + t \u2212 1 + b), where f is a nonlinear function and b is a distorted term. A characteristic card c * Rn \u2212 t + 1 is applied to every possible window of words, so that c = [c1,..., cn \u2212 t + 1]. This wavy layer is then followed by a max-over-pooling operation (Collobert et al., 2011), which results in C = max {c} of the respective filter. To obtain several characteristics, we use several filters of different window sizes. The result is then passed to a fully connected Softmax layer, which outputs probabilities via labels."}, {"heading": "4 Experimental Setup", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "4.1 Datasets", "text": "We used the two sets of data published by DanescuNiculescu-Mizil et al. (2013): Wikipedia (Wiki) and Stack Exchange (SE), which contained community requests with courtesy names; their \"feature development\" was done on the wiki record, and SE was used as the domain for \"feature transfer.\" We use a simpler, train validation test-based setup for these records instead of the original leave-one-out cross-validation system, which makes training extremely slow for any neural network or any major classification.21 The same techniques can also be applied to RNN models. 2The result trends and visualizations using cross-validation were similar to our current results in preliminary experiments. We will publish our exact data split details."}, {"heading": "4.2 Training Details", "text": "Our tuned hyperparameter values (on the Wiki developer set) are a mini batch size of 32, a learning rate of 0.001 for the Adam (Kingma and Ba, 2015) optimizer, a drop-out rate of 0.5, CNN filter windows of 3, 4 and 5 with 75 feature maps and ReLU as a nonlinear function (Nair and Hinton, 2010), valid pads and increments for all folding layers. We followed Danescu-Niculescu-Mizil et al. (2013) when using SE only as a transfer domain, i.e. we do not re-tune hyperparameters or features on that domain and simply use the values selected from the wiki setting. The split and other training details are included in the supplement."}, {"heading": "5 Results", "text": "Table 1 first presents our reproduced results of classification accuracy tests (two terms: positive or negative politeness) for the vocabulary and linguistic characteristics of models based on language by Danescu-Niculescu-Mizil et al. (2013) (for our dataset partitions) as well as the performance of our CNN model. As we have seen, without using manually defined, theoretically inspired linguistic characteristics, the simple CNN model performs better than function-based methods. 3Next, we also show how the linguistic trait base improves when we add our newly discovered characteristics (plus correcting some existing characteristics) uncovered by the analysis in Sec. 6. Thus, this reduces the performance gap between the linguistic trait base and CNN and provides a quantitative rationale for the success of the CNN model. More details in Sec. 6."}, {"heading": "6 Analysis and Visualization", "text": "We present the primary interest and contribution of this work: conducting an important qualitative and quantitative analysis of what our neural networks learn."}, {"heading": "6.1 Activation Clusters", "text": "Activation clustering is a non-parametric approach (adopted by Girshick et al. (2014)) from Computing3For reference, human performance on the original task set by Danescu-Niculescu-Mizil et al. (2013) was 86.72% and 80.89% respectively on the Wiki- and SE-datasets. 4We use only the wiki train / developer sets for all analyses. Each CNN unit activates on one dataset and then analyzes the best-rated samples in each cluster. We track which neurons are maximally activated, which Wikipedia requests, and analyze the most frequent requests in the cluster of each neuron to understand what each neuron responds to."}, {"heading": "6.1.1 Rediscovering Existing Strategies", "text": "We note that the various activation clusters of our neural network automatically rediscover a series of courtesy theory strategies examined in Danescu-Niculescu-Mizil et al. (2013) (see Table 3 in their paper).We present here some of such strategies with their supporting examples, and the rest (e.g. gratitude, greeting, positive lexicon and counterfactual modality) is presented in the supplement. The majority of each category is indicated by (+) and (-). Homage (+) A way to share the burden of a request made to the addressee. Examples of activation clusters: {\"so far good work on rewriting...\"; \"hey, good work on the new pages...\"; \"Direct question (-) questions imposed on the converter in a direct manner, with the demand for a factual answer. Examples of activation clusters: {\" what about radio, fist and air? \""}, {"heading": "6.1.2 Extending Existing Strategies", "text": "We also found that certain activation clusters were interesting extensions of the courtesy strategies indicated in previous work. Gratitude (+) Our CNN learns a special shade of gratitude, namely distinguishing a cluster from the Bigram Thank You for. Activation cluster examples: {\"thank you for the good advice.\"; \"thank you for letting me know.\"} Counterfactual modal (+) sentences with Would you / Could you group yourself as expected; but in addition, the cluster contains requests with \"Do you mind\" as well as patchy 3-gram phrases like \"Can you... please?,\" which presumably implies that the combined nation of a later request with future-oriented variants in the request can / will have a similar effect as the conditional variants would / could do. Activation cluster examples: \"Can this be reported... grid, please?\"; do you have a different look at it? \"}"}, {"heading": "6.1.3 Discovering Novel Strategies", "text": "??????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????"}, {"heading": "6.2 First Derivative Saliency", "text": "Inspired by the visualization of neural networks in computer vision (Simonyan et al., 2014), the first derivative highlighting method indicates how much each input unit contributes to the final decision of the classifier. If E is the embedding of the input, y is the true designation, and Sy (E) is the output of neural networks, then we consider gradients \u2202 Sy (E) \u2202 e. Each image in Fig. 1 is a heatmap of orders of magnitude of the derivative in absolute value in relation to each dimension. The first heatmap receives signals from please (request strategy) and could you (counterfactual modal strategy), but effectively puts much more mass on help. This is probably due to the nature of Wikipedia requests, so the meaning boils down to asking for help, which reduces the social distance. In the second illustration, the highest emphasis is placed on the reason why you are, conceivably, used by Wikipedia administrators as the indicator."}, {"heading": "6.3 Embedding Space Transformations", "text": "We selected keywords from Danescu-NiculescuMizil et al. (2013) and from our new activation clusters (Sec. 6.1) and plotted (via PCA) their embedded-6See Table 3 by Danescu-Niculescu-Mizil et al. (2013) for the polarity values of the various strategies to help us gain insights into specific mood transformations. Fig. 2 shows that the most positive key terms such as hi, appreciate and great are clustered even more closely after training. The key thanks is given a distinct position on a positive spectrum, which underlines its importance for the decision-making of the NN (also represented by the outstanding heatmaps in Sec. 6.2). The indeterminate pronoun is somewhat close to a direct question about the keys for the courtesy strategy, why and what. Please note, as Danescu-Niculescuzil et Mizil et al's positive (2013) sensitivity may differ between the word, because it is always a stronger one."}, {"heading": "6.4 Quantitative Analysis", "text": "In this section, we present quantitative measures of the importance and polarity of the newly discovered courtesy strategies in the sections above, as well as how they explain some of the improved performance of the neural model. In Table 3 by Danescu-Niculescu-Mizil et al. (2013), the pronoun courtesy strategy with the highest percentage in the top quartile is the 2nd person (30%). Our extended Table 2 shows that our newly discovered strategy for indefinite pronouns represents a higher percentage (39%), with a courtesy value of -0.13. In addition, our punctuation strategy also proves to be a strategy for negative courtesy and in the top three of all strategies (after Gratitude and Deference), it has a value of -0.71, while the second negative courtesy strategy (Direct Start) has a much lower value of -0.43. Finally, in terms of accuracy, our newly discovered characteristics for indefinite pronouns and punctuation models automatically improve the success rate of this system (Table 7)."}, {"heading": "7 Conclusion", "text": "Our simple CNN model improves on the previous work with manually defined features. More importantly, we understand the reasons for these improvements using three visualization techniques and discover some new high-score courtesy strategies that in turn quantitatively explain part of the performance gap between the Featurized and Neural models."}, {"heading": "Acknowledgments", "text": "This work was supported by an IBM Faculty Award, a Bloomberg Research Grant, and an NVIDIA GPU donation to MB.7. Our NN visualizations also resulted in an interesting feature correction. As a result of \"With Discovered Features\" in Table 1, we also removed the existing pronoun features (# 14-18) based on the observation that they had weaker activation and highlighting contributions (and lower top quartile%) than the new feature for indeterminate pronouns. This correction and the addition of the two new features helped improve accuracy by 50-50%. Appendix This is complementary material to the main work, in which we present more detailed analysis and visualization examples as well as our datasets and training details."}, {"heading": "A Activation Clusters", "text": "In fact, most of them are able to determine for themselves what they want and what they want to do."}, {"heading": "B First Derivative Saliency", "text": "In the first heatmap, we see a clear example of the Positive Lexicon Courtesy Strategy. The key captures most of the weight for final decision-making. Note that the question mark in particular does not have any influence in this case. In the third example, we see that these markers still carry a lot of weight. For example, other words like real, successful and a personal pronoun have very little impact. Overall, this illustrates the Direct Question Strategy, as most of the focus is on the negatively predicted label. In the third example, we see that these markers still carry a lot of weight. In the discussion about embedding space translations, the gravity key is compared with a preposition for a much stronger polarity than other positive courtesy keys in the fourth heatmap."}, {"heading": "C Dataset and Training Details", "text": "We divide the Wikipedia and Stack Exchange datasets from Danescu-Niculescu-Mizil et al. (2013) into training, validation and test sets, each with 70%, 10% and 20% of the data (after random shuffling). Therefore, the final split for Wikipedia is 1523, 218 and 436; and for Stack Exchange it is 2298, 328 and 657. We will make the dataset split indexes publicly available. For words not present in the pre-trained word2vec embedding set, we use uniform unit scaling initialization. We implement our model with a Python version of TensorFlow (Abadi al., 2015), hyperrate, air parameters, optimization, miniature rate, 0.01 and miniature adjustment. We use our model with a Python version of Tensorlow (Abadi, 2015)."}], "references": [{"title": "TensorFlow: Large-scale machine learning on heterogeneous systems. Software available from tensorflow.org", "author": ["sudevan", "Fernanda Vi\u00e9gas", "Oriol Vinyals", "Pete Warden", "Martin Wattenberg", "Martin Wicke", "Yuan Yu", "Xiaoqiang Zheng"], "venue": null, "citeRegEx": "sudevan et al\\.,? \\Q2015\\E", "shortCiteRegEx": "sudevan et al\\.", "year": 2015}, {"title": "Face and politeness: new (insights) for old (concepts)", "author": ["Francesca Bargiela-Chiappini"], "venue": "Journal of pragmatics,", "citeRegEx": "Bargiela.Chiappini.,? \\Q2003\\E", "shortCiteRegEx": "Bargiela.Chiappini.", "year": 2003}, {"title": "Nltk: the natural language toolkit", "author": ["Steven Bird"], "venue": "In Proceedings of the COLING/ACL on Interactive presentation sessions,", "citeRegEx": "Bird.,? \\Q2006\\E", "shortCiteRegEx": "Bird.", "year": 2006}, {"title": "Politeness: Some universals in language usage, volume 4", "author": ["Brown", "Levinson1987] Penelope Brown", "Stephen C Levinson"], "venue": null, "citeRegEx": "Brown et al\\.,? \\Q1987\\E", "shortCiteRegEx": "Brown et al\\.", "year": 1987}, {"title": "A neural network based approach for sentiment classification in the blogosphere", "author": ["Chen et al.2011] Long-Sheng Chen", "Cheng-Hsiang Liu", "Hui-Ju Chiu"], "venue": "Journal of Informetrics,", "citeRegEx": "Chen et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Chen et al\\.", "year": 2011}, {"title": "Natural language processing (almost) from scratch", "author": ["Jason Weston", "L\u00e9on Bottou", "Michael Karlen", "Koray Kavukcuoglu", "Pavel Kuksa"], "venue": "The Journal of Machine Learning Research,", "citeRegEx": "Collobert et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Collobert et al\\.", "year": 2011}, {"title": "A computational approach to politeness with application to social factors", "author": ["Moritz Sudhof", "Dan Jurafsky", "Jure Leskovec", "Christopher Potts"], "venue": "Proceedings of ACL", "citeRegEx": "DanescuNiculescu.Mizil et al\\.,? \\Q2013\\E", "shortCiteRegEx": "DanescuNiculescu.Mizil et al\\.", "year": 2013}, {"title": "Adaptive recursive neural network for target-dependent twitter sentiment classification", "author": ["Dong et al.2014] Li Dong", "Furu Wei", "Chuanqi Tan", "Duyu Tang", "Ming Zhou", "Ke Xu"], "venue": "In Proceedings of ACL,", "citeRegEx": "Dong et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Dong et al\\.", "year": 2014}, {"title": "Deep convolutional neural networks for sentiment analysis of short texts", "author": ["dos Santos", "Maira Gatti"], "venue": "In Proceedings of COLING,", "citeRegEx": "Santos et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Santos et al\\.", "year": 2014}, {"title": "Visualizing higher-layer features of a deep network", "author": ["Erhan et al.2009] Dumitru Erhan", "Yoshua Bengio", "Aaron Courville", "Pascal Vincent"], "venue": null, "citeRegEx": "Erhan et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Erhan et al\\.", "year": 2009}, {"title": "Twitter brand sentiment analysis: A hybrid system using n-gram analysis and dynamic artificial neural network", "author": ["Ghiassi et al.2013] M Ghiassi", "J Skinner", "D Zimbra"], "venue": "Expert Systems with applications,", "citeRegEx": "Ghiassi et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Ghiassi et al\\.", "year": 2013}, {"title": "Rich feature hierarchies for accurate object detection and semantic segmentation", "author": ["Jeff Donahue", "Trevor Darrell", "Jitendra Malik"], "venue": "In Proceedings of CVPR,", "citeRegEx": "Girshick et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Girshick et al\\.", "year": 2014}, {"title": "Twitter sentiment classification using distant supervision", "author": ["Go et al.2009] Alec Go", "Richa Bhayani", "Lei Huang"], "venue": "CS224N Project", "citeRegEx": "Go et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Go et al\\.", "year": 2009}, {"title": "Politeness phenomena in modern chinese", "author": ["Yueguo Gu"], "venue": "Journal of pragmatics,", "citeRegEx": "Gu.,? \\Q1990\\E", "shortCiteRegEx": "Gu.", "year": 1990}, {"title": "Understanding politeness", "author": ["K\u00e1d\u00e1r", "Haugh2013] D\u00e1niel Z K\u00e1d\u00e1r", "Michael Haugh"], "venue": null, "citeRegEx": "K\u00e1d\u00e1r et al\\.,? \\Q2013\\E", "shortCiteRegEx": "K\u00e1d\u00e1r et al\\.", "year": 2013}, {"title": "Representation of linguistic form and function in recurrent neural networks. arXiv preprint arXiv:1602.08952", "author": ["K\u00e1d\u00e1r et al.2016] Akos K\u00e1d\u00e1r", "Grzegorz Chrupa\u0142a", "Afra Alishahi"], "venue": null, "citeRegEx": "K\u00e1d\u00e1r et al\\.,? \\Q2016\\E", "shortCiteRegEx": "K\u00e1d\u00e1r et al\\.", "year": 2016}, {"title": "A convolutional neural network for modelling sentences", "author": ["Edward Grefenstette", "Phil Blunsom"], "venue": "In Proceedings of ACL", "citeRegEx": "Kalchbrenner et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Kalchbrenner et al\\.", "year": 2014}, {"title": "Visualizing and understanding recurrent networks", "author": ["Justin Johnson", "Fei-Fei Li"], "venue": "In Proceedings of ICLR Workshop", "citeRegEx": "Karpathy et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Karpathy et al\\.", "year": 2016}, {"title": "Sentiment classification of movie reviews using contextual valence shifters", "author": ["Kennedy", "Inkpen2006] Alistair Kennedy", "Diana Inkpen"], "venue": "Computational intelligence,", "citeRegEx": "Kennedy et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Kennedy et al\\.", "year": 2006}, {"title": "Adam: A method for stochastic optimization", "author": ["Kingma", "Ba2015] Diederik Kingma", "Jimmy Ba"], "venue": "In Proceedings of ICLR", "citeRegEx": "Kingma et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Kingma et al\\.", "year": 2015}, {"title": "Imagenet classification with deep convolutional neural networks", "author": ["Ilya Sutskever", "Geoffrey E Hinton"], "venue": "In Proceedings of NIPS,", "citeRegEx": "Krizhevsky et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Krizhevsky et al\\.", "year": 2012}, {"title": "Visualizing and understanding neural models in NLP", "author": ["Li et al.2016] Jiwei Li", "Xinlei Chen", "Eduard Hovy", "Dan Jurafsky"], "venue": "In Proceedings of NAACL", "citeRegEx": "Li et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Li et al\\.", "year": 2016}, {"title": "Politeness theory and relational work", "author": ["Locher", "Watts2005] Miriam A Locher", "Richard J Watts"], "venue": "Journal of Politeness Research. Language, Behaviour,", "citeRegEx": "Locher et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Locher et al\\.", "year": 2005}, {"title": "Understanding deep image representations by inverting them", "author": ["Mahendran", "Vedaldi2015] Aravindh Mahendran", "Andrea Vedaldi"], "venue": "In Proceedings of CVPR,", "citeRegEx": "Mahendran et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Mahendran et al\\.", "year": 2015}, {"title": "Documentlevel sentiment classification: An empirical comparison between svm and ann", "author": ["Joao Francisco Valiati", "Wilson P Gavi\u00e3O Neto"], "venue": "Expert Systems with Applications,", "citeRegEx": "Moraes et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Moraes et al\\.", "year": 2013}, {"title": "Rectified linear units improve restricted boltzmann machines", "author": ["Nair", "Hinton2010] Vinod Nair", "Geoffrey E Hinton"], "venue": "In Proceedings of ICML,", "citeRegEx": "Nair et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Nair et al\\.", "year": 2010}, {"title": "A sentimental education: Sentiment analysis using subjectivity summarization based on minimum cuts", "author": ["Pang", "Lee2004] Bo Pang", "Lillian Lee"], "venue": "In Proceedings of ACL,", "citeRegEx": "Pang et al\\.,? \\Q2004\\E", "shortCiteRegEx": "Pang et al\\.", "year": 2004}, {"title": "Thumbs up?: sentiment classification using machine learning techniques", "author": ["Pang et al.2002] Bo Pang", "Lillian Lee", "Shivakumar Vaithyanathan"], "venue": "In Proceedings of EMNLP,", "citeRegEx": "Pang et al\\.,? \\Q2002\\E", "shortCiteRegEx": "Pang et al\\.", "year": 2002}, {"title": "Evaluating the visualization of what a deep neural network has learned", "author": ["Samek et al.2016] Wojciech Samek", "Alexander Binder", "Gr\u00e9goire Montavon", "Sebastian Bach", "KlausRobert M\u00fcller"], "venue": "IEEE Transactions on Neural Networks and Learning Systems", "citeRegEx": "Samek et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Samek et al\\.", "year": 2016}, {"title": "Deep inside convolutional networks: Visualising image classification models and saliency maps", "author": ["Andrea Vedaldi", "Andrew Zisserman"], "venue": "In Proceedings of ICLR Workshop", "citeRegEx": "Simonyan et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Simonyan et al\\.", "year": 2014}, {"title": "Recursive deep models for semantic compositionality over a sentiment treebank", "author": ["Alex Perelygin", "Jean Y Wu", "Jason Chuang", "Christopher D Manning", "Andrew Y Ng", "Christopher Potts"], "venue": "Proceedings of EMNLP", "citeRegEx": "Socher et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Socher et al\\.", "year": 2013}, {"title": "Visualizing and understanding convolutional networks", "author": ["Zeiler", "Fergus2014] Matthew D Zeiler", "Rob Fergus"], "venue": "In Proceedings of ECCV,", "citeRegEx": "Zeiler et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Zeiler et al\\.", "year": 2014}], "referenceMentions": [{"referenceID": 13, "context": "Politeness theories (Brown and Levinson, 1987; Gu, 1990; Bargiela-Chiappini, 2003) include key components such as modality, indirection, deference, and impersonalization.", "startOffset": 20, "endOffset": 82}, {"referenceID": 1, "context": "Politeness theories (Brown and Levinson, 1987; Gu, 1990; Bargiela-Chiappini, 2003) include key components such as modality, indirection, deference, and impersonalization.", "startOffset": 20, "endOffset": 82}, {"referenceID": 1, "context": "Politeness theories (Brown and Levinson, 1987; Gu, 1990; Bargiela-Chiappini, 2003) include key components such as modality, indirection, deference, and impersonalization. Positive politeness strategies focus on making the hearer feel good through offers, promises, and jokes. Negative politeness examples include favor seeking, orders, and requests. Differentiating among politeness types is a highly nontrivial task, because it depends on factors such as a context, relative power, and culture. Danescu-Niculescu-Mizil et al. (2013) proposed a useful computational framework for predicting politeness in natural language requests by designing various lexical and syntactic features about key politeness theories, e.", "startOffset": 57, "endOffset": 534}, {"referenceID": 9, "context": "To this end, we present several visualization strategies: activation clustering, first derivative saliency, and embedding space transformations, some of which are inspired by similar strategies in computer vision (Erhan et al., 2009; Simonyan et al., 2014; Girshick et al., 2014), and have also been recently adopted in NLP for recurrent neural networks (Li et al.", "startOffset": 213, "endOffset": 279}, {"referenceID": 29, "context": "To this end, we present several visualization strategies: activation clustering, first derivative saliency, and embedding space transformations, some of which are inspired by similar strategies in computer vision (Erhan et al., 2009; Simonyan et al., 2014; Girshick et al., 2014), and have also been recently adopted in NLP for recurrent neural networks (Li et al.", "startOffset": 213, "endOffset": 279}, {"referenceID": 11, "context": "To this end, we present several visualization strategies: activation clustering, first derivative saliency, and embedding space transformations, some of which are inspired by similar strategies in computer vision (Erhan et al., 2009; Simonyan et al., 2014; Girshick et al., 2014), and have also been recently adopted in NLP for recurrent neural networks (Li et al.", "startOffset": 213, "endOffset": 279}, {"referenceID": 21, "context": ", 2014), and have also been recently adopted in NLP for recurrent neural networks (Li et al., 2016; K\u00e1d\u00e1r et al., 2016).", "startOffset": 82, "endOffset": 119}, {"referenceID": 15, "context": ", 2014), and have also been recently adopted in NLP for recurrent neural networks (Li et al., 2016; K\u00e1d\u00e1r et al., 2016).", "startOffset": 82, "endOffset": 119}, {"referenceID": 27, "context": "Substantial previous work has employed machine learning models for other sentiment analysis style tasks (Pang et al., 2002; Pang and Lee, 2004; Kennedy and Inkpen, 2006; Go et al., 2009; Ghiassi et al., 2013).", "startOffset": 104, "endOffset": 208}, {"referenceID": 12, "context": "Substantial previous work has employed machine learning models for other sentiment analysis style tasks (Pang et al., 2002; Pang and Lee, 2004; Kennedy and Inkpen, 2006; Go et al., 2009; Ghiassi et al., 2013).", "startOffset": 104, "endOffset": 208}, {"referenceID": 10, "context": "Substantial previous work has employed machine learning models for other sentiment analysis style tasks (Pang et al., 2002; Pang and Lee, 2004; Kennedy and Inkpen, 2006; Go et al., 2009; Ghiassi et al., 2013).", "startOffset": 104, "endOffset": 208}, {"referenceID": 4, "context": "work has also applied neural network based models to sentiment analysis tasks (Chen et al., 2011; Socher et al., 2013; Moraes et al., 2013; Dong et al., 2014; dos Santos and Gatti, 2014; Kalchbrenner et al., 2014).", "startOffset": 78, "endOffset": 213}, {"referenceID": 30, "context": "work has also applied neural network based models to sentiment analysis tasks (Chen et al., 2011; Socher et al., 2013; Moraes et al., 2013; Dong et al., 2014; dos Santos and Gatti, 2014; Kalchbrenner et al., 2014).", "startOffset": 78, "endOffset": 213}, {"referenceID": 24, "context": "work has also applied neural network based models to sentiment analysis tasks (Chen et al., 2011; Socher et al., 2013; Moraes et al., 2013; Dong et al., 2014; dos Santos and Gatti, 2014; Kalchbrenner et al., 2014).", "startOffset": 78, "endOffset": 213}, {"referenceID": 7, "context": "work has also applied neural network based models to sentiment analysis tasks (Chen et al., 2011; Socher et al., 2013; Moraes et al., 2013; Dong et al., 2014; dos Santos and Gatti, 2014; Kalchbrenner et al., 2014).", "startOffset": 78, "endOffset": 213}, {"referenceID": 16, "context": "work has also applied neural network based models to sentiment analysis tasks (Chen et al., 2011; Socher et al., 2013; Moraes et al., 2013; Dong et al., 2014; dos Santos and Gatti, 2014; Kalchbrenner et al., 2014).", "startOffset": 78, "endOffset": 213}, {"referenceID": 20, "context": "There have been a number of visualization techniques explored for neural networks in computer vision (Krizhevsky et al., 2012; Simonyan et al., 2014; Zeiler and Fergus, 2014; Samek et al., 2016; Mahendran and Vedaldi, 2015).", "startOffset": 101, "endOffset": 223}, {"referenceID": 29, "context": "There have been a number of visualization techniques explored for neural networks in computer vision (Krizhevsky et al., 2012; Simonyan et al., 2014; Zeiler and Fergus, 2014; Samek et al., 2016; Mahendran and Vedaldi, 2015).", "startOffset": 101, "endOffset": 223}, {"referenceID": 28, "context": "There have been a number of visualization techniques explored for neural networks in computer vision (Krizhevsky et al., 2012; Simonyan et al., 2014; Zeiler and Fergus, 2014; Samek et al., 2016; Mahendran and Vedaldi, 2015).", "startOffset": 101, "endOffset": 223}, {"referenceID": 17, "context": "There have been a number of visualization techniques explored for neural networks in computer vision (Krizhevsky et al., 2012; Simonyan et al., 2014; Zeiler and Fergus, 2014; Samek et al., 2016; Mahendran and Vedaldi, 2015). Recently in NLP, Li et al. (2016) successfully adopt computer vision techniques, namely first-order saliency, and present representation plotting for sentiment compositionality across RNN variants.", "startOffset": 102, "endOffset": 259}, {"referenceID": 14, "context": "Similarly, K\u00e1d\u00e1r et al. (2016) analyze the omission scores and top-k contexts of hidden units of a multimodal RNN.", "startOffset": 11, "endOffset": 31}, {"referenceID": 14, "context": "Similarly, K\u00e1d\u00e1r et al. (2016) analyze the omission scores and top-k contexts of hidden units of a multimodal RNN. Karpathy et al. (2016) visualize character-level language models.", "startOffset": 11, "endOffset": 138}, {"referenceID": 5, "context": "This convolutional layer is then followed by a max-over-pooling operation (Collobert et al., 2011) that gives C = max{c} of the particular filter.", "startOffset": 74, "endOffset": 98}, {"referenceID": 6, "context": "We used the two datasets released by DanescuNiculescu-Mizil et al. (2013): Wikipedia (Wiki) and Stack Exchange (SE), containing community requests with politeness labels.", "startOffset": 37, "endOffset": 74}, {"referenceID": 11, "context": "Activation clustering is a non-parametric approach (adopted from Girshick et al. (2014)) of computing", "startOffset": 65, "endOffset": 88}, {"referenceID": 29, "context": "Inspired from neural network visualization in computer vision (Simonyan et al., 2014), the first derivative saliency method indicates how much each input", "startOffset": 62, "endOffset": 85}, {"referenceID": 6, "context": "Finally, in terms of accuracies, our newly discovered features of Indefinite Pronouns and Punctuation improved the featurized system of DanescuNiculescu-Mizil et al. (2013) (see Table 1).", "startOffset": 136, "endOffset": 173}, {"referenceID": 2, "context": "All sentence tokenization is done using NLTK (Bird, 2006).", "startOffset": 45, "endOffset": 57}], "year": 2016, "abstractText": "We present an interpretable neural network approach to predicting and understanding politeness in natural language requests. Our models are based on simple convolutional neural networks directly on raw text, avoiding any manual identification of complex sentiment or syntactic features, while performing better than such feature-based models from previous work. More importantly, we use the challenging task of politeness prediction as a testbed to next present a much-needed understanding of what these successful networks are actually learning. For this, we present several network visualizations based on activation clusters, first derivative saliency, and embedding space transformations, helping us automatically identify several subtle linguistics markers of politeness theories. Further, this analysis reveals multiple novel, high-scoring politeness strategies which, when added back as new features, reduce the accuracy gap between the original featurized system and the neural model, thus providing a clear quantitative interpretation of the success of these neural networks.", "creator": "LaTeX with hyperref package"}}}