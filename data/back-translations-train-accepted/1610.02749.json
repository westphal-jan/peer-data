{"id": "1610.02749", "review": {"conference": "AAAI", "VERSION": "v1", "DATE_OF_SUBMISSION": "10-Oct-2016", "title": "A Dynamic Window Neural Network for CCG Supertagging", "abstract": "Combinatory Category Grammar (CCG) supertagging is a task to assign lexical categories to each word in a sentence. Almost all previous methods use fixed context window sizes as input features. However, it is obvious that different tags usually rely on different context window sizes. These motivate us to build a supertagger with a dynamic window approach, which can be treated as an attention mechanism on the local contexts. Applying dropout on the dynamic filters can be seen as drop on words directly, which is superior to the regular dropout on word embeddings. We use this approach to demonstrate the state-of-the-art CCG supertagging performance on the standard test set.", "histories": [["v1", "Mon, 10 Oct 2016 01:47:50 GMT  (783kb,D)", "http://arxiv.org/abs/1610.02749v1", "8 pages, 3 figures"]], "COMMENTS": "8 pages, 3 figures", "reviews": [], "SUBJECTS": "cs.CL", "authors": ["huijia wu", "jiajun zhang", "chengqing zong"], "accepted": true, "id": "1610.02749"}, "pdf": {"name": "1610.02749.pdf", "metadata": {"source": "CRF", "title": "A Dynamic Window Neural Network for CCG Supertagging", "authors": ["Huijia Wu", "Jiajun Zhang", "Chengqing Zong"], "emails": ["huijia.wu@nlpr.ia.ac.cn", "jjzhang@nlpr.ia.ac.cn", "cqzong@nlpr.ia.ac.cn"], "sections": [{"heading": "Introduction", "text": "In fact, most of them will be able to move into a different world, in which they are able to move, in which they move, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they, in which they, in which they live, in which they, in which they, in which they live, in which they, in which they, in which they live, in which they, in which they, in which they live, in which they, in which they live."}, {"heading": "Background", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "Category Notation", "text": "CCG uses a number of lexical categories to represent components (Steedman, 2000). Specifically, a fixed finite set is used as the basis for constructing other categories described in Table 1. The basic categories could be used to create an infinite set of functional categories by applying the following recursive definition: \u2022 N, NP, PP, S-C \u2022 X / Y, X-Y-C, if the functional category X, Y-CEach specifies some arguments; the combination of the arguments can form a new category according to the orders (Steedman and Baldridge, 2011). The argument could be either basic or functional, and the orders are determined by the forward slash / and backward slash. A category X / Y is a forward function that could accept an argument Y to the right and get X, while the backward functioning radio gate X\\ Y should accept its argument to the left."}, {"heading": "Neuron Based Supertaggers", "text": "CCG supertagging is an approach to assigning lexical categories for each word in a sentence. The problem can be formulated by P (c | w; \u03b8), where w = [w1,.., wT] specifies the T-words in a sentence, and c = [c1,..., cT] specifies the corresponding lexical categories. Note that the length of the words and categories are the same. We denote vectors with bold font and matrices with uppercase letters. Bias terms in neural networks are omitted for readability. Network inputs are the representation of each token in an order. Our inputs include concatenating word representations, letter representations and uppercase representations. To reduce sparseness, all words are displayed as uppercase letters together with uppercase letters and letter representations."}, {"heading": "Network Outputs.", "text": "Since our goal is to assign CCG categories to each word, we use a Softmax activation function g (\u00b7): yt = g (W hyht) (3) at the output level. yt-RK represents a probability distribution across all possible category. yk (t) = exp (hk) \u2211 k \u2032 exp (hk \u2032) is the k-th dimension of yt corresponding to the k-th lexical category in the lexicon. ht-RH is the output of the hidden layer in due course."}, {"heading": "Inputs to Outputs Mappings.", "text": "Since CCG supertagging can be treated either as a point or as a sequential estimation problem corresponding to two types of neural networks: MLPs or RNNs. For convenience, we will only talk about gated RNNNs, which are special types of RNs with logisticians in the hidden units for controlling the flow of information. There are many types of gated RNNNs, such as long-term short-term memory (Hochreiter and Schmidhuber, 1997) and gated recurrent unit (Cho et al., 2014). We focus on LSTMs only in this work.LSTMs replace the hidden units in vanilla RNNNs with intricate blocks designed to act as: c-t = residential block (W xcxt + W hght \u2212 1) ct = ft \u2212 t (5) ht = f (ct), where cell gates are executed, and the input of Gate H (W xcent + W = H-weight), c-H (-)."}, {"heading": "A Dynamic Window Approach", "text": "In this section we will introduce a dynamic window approach for supertagging. Specifically, we will add logistic gates to each token in the context window to filter the unnecessary information. We can add the equation (2) to: x-bd = rt'xt (7): = [rt \u2212 bd / 2cfwt \u2212 bd / 2c;...; rt + bd / 2cfwt + bd / 2c] (8) Here we denote an elementary scalar vector product. rt: = [rt \u2212 bd / 2c,..., rt + bd / 2c]."}, {"heading": "Design of the Gates", "text": "We use a forward-facing neural network to learn rt: rt = \u03c3 (W xrxt) (9), where \u03c3 (\u00b7) is the sigmoid function defined as \u03c3 (z) = 1 + e \u2212 z. This function is to ensure that the values of rt are between 0 and 1. xt network input, as defined in Equation (2). rt-Rd is the output of the dynamic window model, where d is the window size. Wxr-Rd \u00b7 I is the weight to learn. A disadvantage of the sigmoid function is that when a neuron is almost saturated, its derivative becomes small, causing the connection weights to change very slowly. If some neurons are saturated in rt, the states will be stable and may not generalize well. To further improve the sparseness in the context window, we add a dropout mask (Srivastava et al., 2014) to rt: li (t)."}, {"heading": "Embedded into MLPs", "text": "For MLPs with this approach, we can use the filtered context as input: ht = f (W xhx-t) (13) yt = g (W hyht) (14), where x-t is defined in Equation (7). We use the filtered context as input into the hidden plane. Wxh-RH-I and Why-RK-H are the weight parameters of MLP."}, {"heading": "Embedded into Vanilla RNNs", "text": "The similar approach can be applied to RNNs with minor changes. For each hidden state, we have two types of input: one is from the input layer, the other is from the hidden (Elman, 1990) or the output layer (Jordan, 1986). The recurring weight can disappear or explode if its eigenvalues differ from 1. To avoid these problems, we add a gate to the recurring input to reset it: r-t = \u03c3 (W xrxt) (15) st = \u03c3 (B xsxt) (16), where st-R is a scalar between 0 and 1 that is used to reset the recurring input. Wxs-R1 \u00d7 I is the corresponding hidden output weight. Intuitively, when st is near zero, the recurring input layer Wycyt \u2212 1 disappears, resulting in an MLP.N.Let's take an output of the Jordanian type RNN as an example: h-J-J-J-J-J-J putt = J-J-J-J-J-J-J-J-W \u2212 J-J-J-J-J-J-J-J-J-J-J-J-J-J-J-J-J-J-J-J-J-J-J-J-J-W \u2212 W."}, {"heading": "Embedded into Gated RNNs", "text": "For gated RNNs, we use a two-stacked bi-directional LSTM (bi-LSTM) to model the task. The architecture can be defined as follows: \u2212 \u2192 ht = LSTM (\u2212 \u2192 xt, \u2212 \u2192 ht \u2212 1, \u2212 \u2212 \u2192 ct \u2212 1) (19) \u00b7 \u2212 ht = LSTM (\u2190 \u2212 xt, \u2190 \u2212 ht \u2212 1, \u2190 \u2212 ct \u2212 1) (20) yt = g (\u2212 ht, \u2190 \u2212 ht) (21) where LSTM (\u00b7) is the LSTM calculation. \u2212 \u2192 xt and \u2190 \u2212 xt are the forward and backward input sequences, respectively. The output of the two hidden layers \u2212 \u2192 ht and \u2190 \u2212 ht in a birectional LSTM are stacked on top of each other: hlt = f l (hl \u2212 1t, h l t \u2212 1) (22), where the t-th is the hidden state of the l-th layer."}, {"heading": "Discussion", "text": "The main idea of the attention mechanism is to focus on parts of the memories that are used to store the information for prediction, such as the input or the hidden units. From this perspective, our dynamic window method can be considered an attention-based system. Supertagging is also a special kind of sequence sequence problem, where the input and output sequences have the same length. Therefore, we do not need to use an encoder to store the input, and use a different decoder to generate the output.The difference between the two attention sequence mechanisms lies in the type of memory. In the encoder decoder architecture, the attention model is viewed by a weighted average of the output of the encoder, because they use an encoder and a decoder to model the outputs with variable length, and the memories are the hidden states of the encoder, while the encoder's task is just to handle the supercode."}, {"heading": "Experiments", "text": "We divide our experiments into two steps: First, we compare the performance of our models with the existing approaches; the comparisons do not include externally labeled data and POS labels; then we describe quantitative results that confirm the effectiveness of our dynamic window approach; and we conduct experiments with MLPs and RNNs with and without this method to make comparisons."}, {"heading": "Dataset and Pre-Processing", "text": "Our experiments are conducted with CCGBank (Hockenmaier and Steedman, 2007), a translation from Penn Treebank (Marcus et al., 1993) to CCG with a coverage of 99.4%. We follow the standard splits and use Sections 02-21 for training, Section 00 for development, and Section 23 for testing. We use a complete category set of 1285 keywords. All digits are mapped into the same digit \"9,\" and all words are lowercase."}, {"heading": "Network Configuration", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "Initialization.", "text": "In our experiments, there are two types of weights: recurring and non-recurring weights. In non-recurring weights, we initialize word embeddings with the pre-formed 200-dimensional GolVe vectors (Pennington et al., 2014). Other weights, we initialize with the Gaussian distribution N (0, 1 \u221a fan-in), scaled by a factor of 0.1, where an-in is the number of units in the input layer. In recurring weight matrices, we initialize with random orthogonal matrices through SVD (Saxe et al., 2013) to avoid unstable gradients. Orthogonal initialization for recurring weights is important in our experiments, which requires about 2% relative power gain compared to other methods such as the Xavier initialization (Glorot et al., 2010)."}, {"heading": "Hyperparameters.", "text": "For MLPs, we use a window size of 4, while for vanilla RNNs and gated RNNs, a window size of 1 is sufficient to capture local contexts; the word embedding dimension is 200; the size of character embedding and uppercase embedding is set to 5; we set the maximum number of letters of a word to 5 to eliminate complexity, which means that we link the leftmost 5 letters and the rightmost 5 letters as character representations; the number of cells of the stacked Bi-LSTM is set to 512; we also tried 400 cells or 600 cells and found that this number does not affect performance as much; all stacked hidden layers have the same number of cells; the output layer has 1286 neurons, which is the number of tags in the training set with a RARE symbol."}, {"heading": "Training.", "text": "We train the networks with the back-propagation algorithm and use the stochastic gradient descent algorithm (SGD) with an equal learning rate of 0.02 for all layers. We have also tried other optimization methods, such as Momentum (Plaut and others, 1986), Adadelta (Zeiler, 2012) or Adam (Kingma and Ba, 2014), but none of them works as well as SGD. Gradient clipping is not used. In our experiments we use online learning, which means that the parameters are updated successively with each training sequence. We use negative log probability costs to evaluate performance. In a training set {(xn, tn) Nn = 1}, the objective function can be written as follows: C = \u2212 1 N-N-N-N-N-N = 1 logytn (23), where tn-N is the true target for sample n, and ytn the t output in the softest layer given xn input."}, {"heading": "Regularization.", "text": "We add a failover mask with a drop rate of 0.5 to our filtergate rt, which is helpful to improve performance. Figure 2 shows such comparisons on a forward-facing LSTM. The behavior of other types of neural networks is similar. We can see that dropouts on the words directly eq. (12) are slightly better than the dynamic window without dropouts eq. (9), while dropouts on the dynamic window result in a much better improvement. We also apply dropouts to the output of the hidden layer with a drop rate of 0.5. At test time, weights are scaled by a factor of 1 \u2212 p."}, {"heading": "Results on Supertagging Accuracy", "text": "Table 2 shows comparisons of accuracy on CCGBank. We find that stacked Bi-LSTM (with depth 2) performs best than other neural models. Our dynamic window approach provides the highest (+ 11%) relative performance gain. Drawing level information (+ 6% relative accuracy) and dropout (+ 8% relative accuracy) are also helpful to improve performance."}, {"heading": "On the Usage of Dynamic Filters", "text": "Performance is 94.05% (Table 3, line 2), which indicates that folding is recommended to work directly with words rather than with word embedding. This can be formulated as follows: x-bd = rt xt (24): = [rt \u2212 bd / 2c fwt \u2212 bd / 2c;...; rt + bd / 2c fwt + bd / 2c] (25) Here we refer to an elementary product. We can use an MLP instead of a single-layer network that deserves the dynamic filters: ut = \u03c3 (W xuxt) (26) rt = \u03c3 (W urut) (27), adding a hidden layer ut-ut-Ru to learn rt. But performance is 94.23% (Table 3, line 3) with additional computing costs. The weighted average on the input di- = 1 rifi is a standard attention method that results in a poor result of 94.23% (weighted to table 3)."}, {"heading": "Effects of the Dynamic Window Approach", "text": "Figure 3 shows the effectiveness of our dynamic window approach. Taking a forward-facing LSTM as an example, we can first observe 20 epochs in which the LSTM + Dyn performs worse than the original LSTM model, as many useful input contexts are filtered with this approach, but after 20 epochs the LSTM starts to overperform as the performance of the LSTM + Dyn continues to increase."}, {"heading": "Visualizations", "text": "To understand this mechanism, we randomly select a few words to visualize the dynamic activities during the training. All visualizations are displayed by means of an MLP on CCGBank section 02-21. Figure 4 shows the different dynamic activities for the words. Each sub-figure has 9 blocks (a window size of 4), each of which shows an activation of a filter li (t), i (1,...., 9). After the Convergence Training, the items that are far from the middle words are gradually removed from the contexts, while the attention is gradually directed to the items near the central words. We can observe that their dynamic activities differ for different words."}, {"heading": "Related Work", "text": "The discrete property of the model makes the features independent from each other. Lewis and Steedman (2014) propose a semisupervised supertagging model using a multi-character perceptron (MLP) based on Collobert (2011) and conditional random field (CRF) proposed by Turian et al. (2010). Without using POS tags, they use the pro-trained word embeddings with 2-character suffix and capitalization as features to represent the word embedding encodes the word similarities and provides a better representation than log-linear models. However, MLP based supertagger ignores the sequential information, and their CRF based model can capture this but takes far more computational complexity than the MLLP model."}, {"heading": "Conclusion", "text": "Our model uses logistical gates to filter the context window around the middle word. This attention mechanism shows effectiveness for both MLPs and RNNs. We observed that using dropouts in the dynamic window will greatly improve generalization performance. We also visualized the activation of the filters, which is useful to understand the dynamic activities. Although our work focuses mainly on CCG supertagging, this method can easily be applied to other sequence tagging tasks such as POS tagging and Entity Recognition (NER)."}], "references": [{"title": "Broadcoverage CCG semantic parsing with amr", "author": ["Yoav Artzi", "Kenton Lee", "Luke Zettlemoyer."], "venue": "Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, pages 1699\u20131710. Association for Computational Linguistics.", "citeRegEx": "Artzi et al\\.,? 2015", "shortCiteRegEx": "Artzi et al\\.", "year": 2015}, {"title": "Neural machine translation by jointly learning to align and translate", "author": ["Dzmitry Bahdanau", "Kyunghyun Cho", "Yoshua Bengio."], "venue": "arXiv preprint arXiv:1409.0473.", "citeRegEx": "Bahdanau et al\\.,? 2014", "shortCiteRegEx": "Bahdanau et al\\.", "year": 2014}, {"title": "Wide-coverage semantic representations from a CCG parser", "author": ["Johan Bos", "Stephen Clark", "Mark Steedman", "James R Curran", "Julia Hockenmaier."], "venue": "Proceedings of the 20th international conference on Computational Linguistics, page 1240. Association for Computational Linguistics.", "citeRegEx": "Bos et al\\.,? 2004", "shortCiteRegEx": "Bos et al\\.", "year": 2004}, {"title": "Towards wide-coverage semantic interpretation", "author": ["Johan Bos."], "venue": "Proceedings of Sixth International Workshop on Computational Semantics IWCS, volume 6, pages 42\u201353.", "citeRegEx": "Bos.,? 2005", "shortCiteRegEx": "Bos.", "year": 2005}, {"title": "Wide-coverage semantic analysis with boxer", "author": ["Johan Bos."], "venue": "Proceedings of the 2008 Conference on Semantics in Text", "citeRegEx": "Bos.,? 2008", "shortCiteRegEx": "Bos.", "year": 2008}, {"title": "Learning phrase representations using rnn encoder-decoder for statistical machine translation", "author": ["Kyunghyun Cho", "Bart Van Merri\u00ebnboer", "Caglar Gulcehre", "Dzmitry Bahdanau", "Fethi Bougares", "Holger Schwenk", "Yoshua Bengio."], "venue": "arXiv preprint arXiv:1406.1078.", "citeRegEx": "Cho et al\\.,? 2014", "shortCiteRegEx": "Cho et al\\.", "year": 2014}, {"title": "Wide-coverage efficient statistical parsing with ccg and log-linear models", "author": ["Stephen Clark", "James R Curran."], "venue": "Computational Linguistics, 33(4):493\u2013552.", "citeRegEx": "Clark and Curran.,? 2007", "shortCiteRegEx": "Clark and Curran.", "year": 2007}, {"title": "Natural language processing (almost) from scratch", "author": ["Ronan Collobert", "Jason Weston", "L\u00e9on Bottou", "Michael Karlen", "Koray Kavukcuoglu", "Pavel Kuksa."], "venue": "The Journal of Machine Learning Research, 12:2493\u20132537.", "citeRegEx": "Collobert et al\\.,? 2011", "shortCiteRegEx": "Collobert et al\\.", "year": 2011}, {"title": "Semi-supervised sequence learning", "author": ["Andrew M Dai", "Quoc V Le."], "venue": "Advances in Neural Information Processing Systems, pages 3061\u20133069.", "citeRegEx": "Dai and Le.,? 2015", "shortCiteRegEx": "Dai and Le.", "year": 2015}, {"title": "Finding structure in time", "author": ["Jeffrey L Elman."], "venue": "Cognitive science, 14(2):179\u2013211.", "citeRegEx": "Elman.,? 1990", "shortCiteRegEx": "Elman.", "year": 1990}, {"title": "A theoretically grounded application of dropout in recurrent neural networks", "author": ["Yarin Gal."], "venue": "arXiv preprint arXiv:1512.05287.", "citeRegEx": "Gal.,? 2015", "shortCiteRegEx": "Gal.", "year": 2015}, {"title": "Understanding the difficulty of training deep feedforward neural networks", "author": ["Xavier Glorot", "Yoshua Bengio."], "venue": "Aistats, volume 9, pages 249\u2013256.", "citeRegEx": "Glorot and Bengio.,? 2010", "shortCiteRegEx": "Glorot and Bengio.", "year": 2010}, {"title": "Lstm can solve hard long time lag problems", "author": ["Sepp Hochreiter", "J\u00fcrgen Schmidhuber."], "venue": "Advances in neural information processing systems, pages 473\u2013479.", "citeRegEx": "Hochreiter and Schmidhuber.,? 1997", "shortCiteRegEx": "Hochreiter and Schmidhuber.", "year": 1997}, {"title": "Ccgbank: a corpus of CCG derivations and dependency structures extracted from the penn treebank", "author": ["Julia Hockenmaier", "Mark Steedman."], "venue": "Computational Linguistics, 33(3):355\u2013396.", "citeRegEx": "Hockenmaier and Steedman.,? 2007", "shortCiteRegEx": "Hockenmaier and Steedman.", "year": 2007}, {"title": "Attractor dynamics and parallellism in a connectionist sequential machine", "author": ["Michael I Jordan"], "venue": null, "citeRegEx": "Jordan.,? \\Q1986\\E", "shortCiteRegEx": "Jordan.", "year": 1986}, {"title": "Adam: A method for stochastic optimization", "author": ["Diederik Kingma", "Jimmy Ba."], "venue": "arXiv preprint arXiv:1412.6980.", "citeRegEx": "Kingma and Ba.,? 2014", "shortCiteRegEx": "Kingma and Ba.", "year": 2014}, {"title": "Inducing probabilistic CCG grammars from logical form with higher-order unification", "author": ["Tom Kwiatkowski", "Luke Zettlemoyer", "Sharon Goldwater", "Mark Steedman."], "venue": "Proceedings of the 2010 conference on empirical methods in natural language processing, pages 1223\u20131233. Association", "citeRegEx": "Kwiatkowski et al\\.,? 2010", "shortCiteRegEx": "Kwiatkowski et al\\.", "year": 2010}, {"title": "Lexical generalization in CCG grammar induction for semantic parsing", "author": ["Tom Kwiatkowski", "Luke Zettlemoyer", "Sharon Goldwater", "Mark Steedman."], "venue": "Proceedings of the Conference on Empirical Methods in Natural Language Processing, pages 1512\u20131523. Association for Computational", "citeRegEx": "Kwiatkowski et al\\.,? 2011", "shortCiteRegEx": "Kwiatkowski et al\\.", "year": 2011}, {"title": "Combining distributional and logical semantics", "author": ["Mike Lewis", "Mark Steedman."], "venue": "Transactions of the Association for Computational Linguistics, 1:179\u2013192.", "citeRegEx": "Lewis and Steedman.,? 2013", "shortCiteRegEx": "Lewis and Steedman.", "year": 2013}, {"title": "Improved CCG parsing with semi-supervised supertagging", "author": ["Mike Lewis", "Mark Steedman."], "venue": "Transactions of the Association for Computational Linguistics, 2:327\u2013338.", "citeRegEx": "Lewis and Steedman.,? 2014", "shortCiteRegEx": "Lewis and Steedman.", "year": 2014}, {"title": "Lstm ccg parsing", "author": ["Mike Lewis", "Kenton Lee", "Luke Zettlemoyer."], "venue": "Proceedings of the 15th Annual Conference of the North American Chapter of the Association for Computational Linguistics.", "citeRegEx": "Lewis et al\\.,? 2016", "shortCiteRegEx": "Lewis et al\\.", "year": 2016}, {"title": "Not all contexts are created equal: Better word representations with variable attention", "author": ["Wang Ling", "Lin Chu-Cheng", "Yulia Tsvetkov", "Silvio Amir", "Ram\u00f3n Fernandez Astudillo", "Chris Dyer", "Alan W Black", "Isabel Trancoso."], "venue": "EMNLP.", "citeRegEx": "Ling et al\\.,? 2015", "shortCiteRegEx": "Ling et al\\.", "year": 2015}, {"title": "Building a large annotated corpus of english: The penn treebank", "author": ["Mitchell P Marcus", "Mary Ann Marcinkiewicz", "Beatrice Santorini."], "venue": "Computational linguistics, 19(2):313\u2013330.", "citeRegEx": "Marcus et al\\.,? 1993", "shortCiteRegEx": "Marcus et al\\.", "year": 1993}, {"title": "Glove: Global vectors for word representation", "author": ["Jeffrey Pennington", "Richard Socher", "Christopher D Manning."], "venue": "EMNLP, volume 14, pages 1532\u201343.", "citeRegEx": "Pennington et al\\.,? 2014", "shortCiteRegEx": "Pennington et al\\.", "year": 2014}, {"title": "Experiments on learning by back propagation", "author": ["David C Plaut"], "venue": null, "citeRegEx": "Plaut,? \\Q1986\\E", "shortCiteRegEx": "Plaut", "year": 1986}, {"title": "Exact solutions to the nonlinear dynamics of learning in deep linear neural networks", "author": ["Andrew M Saxe", "James L McClelland", "Surya Ganguli."], "venue": "arXiv preprint arXiv:1312.6120.", "citeRegEx": "Saxe et al\\.,? 2013", "shortCiteRegEx": "Saxe et al\\.", "year": 2013}, {"title": "Dropout: A simple way to prevent neural networks from overfitting", "author": ["Nitish Srivastava", "Geoffrey Hinton", "Alex Krizhevsky", "Ilya Sutskever", "Ruslan Salakhutdinov."], "venue": "The Journal of Machine Learning Research, 15(1):1929\u20131958.", "citeRegEx": "Srivastava et al\\.,? 2014", "shortCiteRegEx": "Srivastava et al\\.", "year": 2014}, {"title": "Combinatory categorial grammar", "author": ["Mark Steedman", "Jason Baldridge."], "venue": "Non-Transformational Syntax: Formal and Explicit Models of Grammar. Wiley-Blackwell.", "citeRegEx": "Steedman and Baldridge.,? 2011", "shortCiteRegEx": "Steedman and Baldridge.", "year": 2011}, {"title": "The syntactic process, volume 24", "author": ["Mark Steedman."], "venue": "MIT Press.", "citeRegEx": "Steedman.,? 2000", "shortCiteRegEx": "Steedman.", "year": 2000}, {"title": "Word representations: a simple and general method for semisupervised learning", "author": ["Joseph Turian", "Lev Ratinov", "Yoshua Bengio."], "venue": "Proceedings of the 48th annual meeting of the association for computational linguistics, pages 384\u2013394. Association for Computational Linguistics.", "citeRegEx": "Turian et al\\.,? 2010", "shortCiteRegEx": "Turian et al\\.", "year": 2010}, {"title": "Supertagging with lstms", "author": ["Ashish Vaswani", "Yonatan Bisk", "Kenji Sagae", "Ryan Musa."], "venue": "Proceedings of the Human Language Technology Conference of the NAACL.", "citeRegEx": "Vaswani et al\\.,? 2016", "shortCiteRegEx": "Vaswani et al\\.", "year": 2016}, {"title": "CCG supertagging with a recurrent neural network", "author": ["Wenduan Xu", "Michael Auli", "Stephen Clark."], "venue": "Volume 2: Short Papers, page 250.", "citeRegEx": "Xu et al\\.,? 2015", "shortCiteRegEx": "Xu et al\\.", "year": 2015}, {"title": "Expected f-measure training for shift-reduce parsing with recurrent neural networks", "author": ["Wenduan Xu", "Michael Auli", "Stephen Clark."], "venue": "Proceedings of NAACL-HLT, pages 210\u2013220.", "citeRegEx": "Xu et al\\.,? 2016", "shortCiteRegEx": "Xu et al\\.", "year": 2016}, {"title": "Adadelta: An adaptive learning rate method", "author": ["Matthew D Zeiler."], "venue": "arXiv preprint arXiv:1212.5701.", "citeRegEx": "Zeiler.,? 2012", "shortCiteRegEx": "Zeiler.", "year": 2012}, {"title": "Online learning of relaxed CCG grammars for parsing to logical form", "author": ["Luke S Zettlemoyer", "Michael Collins."], "venue": "EMNLP-CoNLL, pages 678\u2013687.", "citeRegEx": "Zettlemoyer and Collins.,? 2007", "shortCiteRegEx": "Zettlemoyer and Collins.", "year": 2007}], "referenceMentions": [{"referenceID": 34, "context": "CCG provides an elegant solution for a wide range of semantic analysis, such as semantic parsing (Zettlemoyer and Collins, 2007; Kwiatkowski et al., 2010; Kwiatkowski et al., 2011; Artzi et al., 2015), semantic representations (Bos et al.", "startOffset": 97, "endOffset": 200}, {"referenceID": 16, "context": "CCG provides an elegant solution for a wide range of semantic analysis, such as semantic parsing (Zettlemoyer and Collins, 2007; Kwiatkowski et al., 2010; Kwiatkowski et al., 2011; Artzi et al., 2015), semantic representations (Bos et al.", "startOffset": 97, "endOffset": 200}, {"referenceID": 17, "context": "CCG provides an elegant solution for a wide range of semantic analysis, such as semantic parsing (Zettlemoyer and Collins, 2007; Kwiatkowski et al., 2010; Kwiatkowski et al., 2011; Artzi et al., 2015), semantic representations (Bos et al.", "startOffset": 97, "endOffset": 200}, {"referenceID": 0, "context": "CCG provides an elegant solution for a wide range of semantic analysis, such as semantic parsing (Zettlemoyer and Collins, 2007; Kwiatkowski et al., 2010; Kwiatkowski et al., 2011; Artzi et al., 2015), semantic representations (Bos et al.", "startOffset": 97, "endOffset": 200}, {"referenceID": 2, "context": ", 2015), semantic representations (Bos et al., 2004; Bos, 2005; Bos, 2008; Lewis and Steedman, 2013), and semantic compositions, all of which heavily depend on the supertagging and parsing performance.", "startOffset": 34, "endOffset": 100}, {"referenceID": 3, "context": ", 2015), semantic representations (Bos et al., 2004; Bos, 2005; Bos, 2008; Lewis and Steedman, 2013), and semantic compositions, all of which heavily depend on the supertagging and parsing performance.", "startOffset": 34, "endOffset": 100}, {"referenceID": 4, "context": ", 2015), semantic representations (Bos et al., 2004; Bos, 2005; Bos, 2008; Lewis and Steedman, 2013), and semantic compositions, all of which heavily depend on the supertagging and parsing performance.", "startOffset": 34, "endOffset": 100}, {"referenceID": 18, "context": ", 2015), semantic representations (Bos et al., 2004; Bos, 2005; Bos, 2008; Lewis and Steedman, 2013), and semantic compositions, all of which heavily depend on the supertagging and parsing performance.", "startOffset": 34, "endOffset": 100}, {"referenceID": 6, "context": "Existing algorithms on CCG supertagging range from point estimation (Clark and Curran, 2007; Lewis and Steedman, 2014) to sequential estimation (Xu et al.", "startOffset": 68, "endOffset": 118}, {"referenceID": 19, "context": "Existing algorithms on CCG supertagging range from point estimation (Clark and Curran, 2007; Lewis and Steedman, 2014) to sequential estimation (Xu et al.", "startOffset": 68, "endOffset": 118}, {"referenceID": 31, "context": "Existing algorithms on CCG supertagging range from point estimation (Clark and Curran, 2007; Lewis and Steedman, 2014) to sequential estimation (Xu et al., 2015; Lewis et al., 2016; Vaswani et al., 2016), which predict the most probable supertag of the current word according to the context in a fixed size window.", "startOffset": 144, "endOffset": 203}, {"referenceID": 20, "context": "Existing algorithms on CCG supertagging range from point estimation (Clark and Curran, 2007; Lewis and Steedman, 2014) to sequential estimation (Xu et al., 2015; Lewis et al., 2016; Vaswani et al., 2016), which predict the most probable supertag of the current word according to the context in a fixed size window.", "startOffset": 144, "endOffset": 203}, {"referenceID": 30, "context": "Existing algorithms on CCG supertagging range from point estimation (Clark and Curran, 2007; Lewis and Steedman, 2014) to sequential estimation (Xu et al., 2015; Lewis et al., 2016; Vaswani et al., 2016), which predict the most probable supertag of the current word according to the context in a fixed size window.", "startOffset": 144, "endOffset": 203}, {"referenceID": 13, "context": "For a particular word, the number of its categories may vary from 1 to 130 in CCGBank 02-21 (Hockenmaier and Steedman, 2007).", "startOffset": 92, "endOffset": 124}, {"referenceID": 31, "context": "To overcome these problems, we notice that Xu et al. (2015) use dropout in the embedding layer to make the input contexts sparse.", "startOffset": 43, "endOffset": 60}, {"referenceID": 1, "context": "This method is naturally an extension to the encoderdecoder with the attention mechanism (Bahdanau et al., 2014), which can be interpreted as focusing on parts of the memories when making decisions.", "startOffset": 89, "endOffset": 112}, {"referenceID": 28, "context": "CCG uses a set of lexical categories to represent constituents (Steedman, 2000).", "startOffset": 63, "endOffset": 79}, {"referenceID": 27, "context": "Combining the arguments can form a new category according to the orders (Steedman and Baldridge, 2011).", "startOffset": 72, "endOffset": 102}, {"referenceID": 12, "context": "There are many kinds of gated RNNs, such as long short-term memory, (Hochreiter and Schmidhuber, 1997) and gated recurrent unit (Cho et al.", "startOffset": 68, "endOffset": 102}, {"referenceID": 5, "context": "There are many kinds of gated RNNs, such as long short-term memory, (Hochreiter and Schmidhuber, 1997) and gated recurrent unit (Cho et al., 2014).", "startOffset": 128, "endOffset": 146}, {"referenceID": 26, "context": "To further improve the sparsity in the context window, we add a dropout mask (Srivastava et al., 2014) on rt:", "startOffset": 77, "endOffset": 102}, {"referenceID": 8, "context": "Since r\u0303t acts on each word feature in the context window, this dropout can be viewed as drop on words directly (Dai and Le, 2015).", "startOffset": 112, "endOffset": 130}, {"referenceID": 9, "context": "For each hidden state we have two types of inputs: one is from the input layer, the other is from the hidden (Elman, 1990) or the output layer (Jordan, 1986).", "startOffset": 109, "endOffset": 122}, {"referenceID": 14, "context": "For each hidden state we have two types of inputs: one is from the input layer, the other is from the hidden (Elman, 1990) or the output layer (Jordan, 1986).", "startOffset": 143, "endOffset": 157}, {"referenceID": 13, "context": "Our experiments are performed on CCGBank (Hockenmaier and Steedman, 2007), which is a translation from Penn Treebank (Marcus et al.", "startOffset": 41, "endOffset": 73}, {"referenceID": 22, "context": "Our experiments are performed on CCGBank (Hockenmaier and Steedman, 2007), which is a translation from Penn Treebank (Marcus et al., 1993) to CCG with a coverage 99.", "startOffset": 117, "endOffset": 138}, {"referenceID": 23, "context": "For non-recurrent weights, we initialize word embeddings with the pretrained 200-dimensional GolVe vectors (Pennington et al., 2014).", "startOffset": 107, "endOffset": 132}, {"referenceID": 25, "context": "For recurrent weight matrices, we initialize with random orthogonal matrices through SVD (Saxe et al., 2013)to avoid unstable gradients.", "startOffset": 89, "endOffset": 108}, {"referenceID": 11, "context": "Orthogonal initialization for recurrent weights is important in our experiments, which takes about 2% relative performance gain than other methods such as Xavier initialization (Glorot and Bengio, 2010).", "startOffset": 177, "endOffset": 202}, {"referenceID": 33, "context": "We also tried other optimization methods, such as momentum (Plaut and others, 1986), Adadelta (Zeiler, 2012), or Adam (Kingma and Ba, 2014), but none of them perform as well as SGD.", "startOffset": 94, "endOffset": 108}, {"referenceID": 15, "context": "We also tried other optimization methods, such as momentum (Plaut and others, 1986), Adadelta (Zeiler, 2012), or Adam (Kingma and Ba, 2014), but none of them perform as well as SGD.", "startOffset": 118, "endOffset": 139}, {"referenceID": 18, "context": "Lewis and Steedman (2014) propose a semisupervised supertagging model using a multi-layer perceptron (MLP) based on Collobert (2011) and conditional random field (CRF) proposed by Turian et al.", "startOffset": 0, "endOffset": 26}, {"referenceID": 18, "context": "Lewis and Steedman (2014) propose a semisupervised supertagging model using a multi-layer perceptron (MLP) based on Collobert (2011) and conditional random field (CRF) proposed by Turian et al.", "startOffset": 0, "endOffset": 133}, {"referenceID": 18, "context": "Lewis and Steedman (2014) propose a semisupervised supertagging model using a multi-layer perceptron (MLP) based on Collobert (2011) and conditional random field (CRF) proposed by Turian et al. (2010). Without using POS tags, they use the per-trained word embeddings with 2-character suffix and capitalization as features to represent the word.", "startOffset": 0, "endOffset": 201}, {"referenceID": 28, "context": "Recently, Xu et al. (2015) design an Elman-type RNN to capture these dependencies, and use a fixed size window for each word as MLPs.", "startOffset": 10, "endOffset": 27}, {"referenceID": 9, "context": "(2015) design an Elman-type RNN to capture these dependencies, and use a fixed size window for each word as MLPs. The recurrent matrix in RNN can restore the historical information, which makes it outperform the MLP based model. But RNNs may suffer from the gradient vanishing/exploding problems and are not good at capturing long-range dependencies in practice. Vaswani et al. (2016) and Lewis et al.", "startOffset": 17, "endOffset": 385}, {"referenceID": 9, "context": "(2015) design an Elman-type RNN to capture these dependencies, and use a fixed size window for each word as MLPs. The recurrent matrix in RNN can restore the historical information, which makes it outperform the MLP based model. But RNNs may suffer from the gradient vanishing/exploding problems and are not good at capturing long-range dependencies in practice. Vaswani et al. (2016) and Lewis et al. (2016) shows the effectiveness of bi-LSTMs in supertagging, but they do not use a context window for the inputs.", "startOffset": 17, "endOffset": 409}, {"referenceID": 8, "context": "Dropout on the dynamic window is similar to (Dai and Le, 2015), which randomly drop words in the input sentences.", "startOffset": 44, "endOffset": 62}, {"referenceID": 1, "context": "Their attention mechanism is similar to Bahdanau et al. (2014), while ours was not originally designed as a weighted average but a gated concatenation.", "startOffset": 40, "endOffset": 63}, {"referenceID": 1, "context": "Their attention mechanism is similar to Bahdanau et al. (2014), while ours was not originally designed as a weighted average but a gated concatenation. Dropout on the dynamic window is similar to (Dai and Le, 2015), which randomly drop words in the input sentences. Gal (2015) also use dropout on words, but using a fixed mask rather a random one.", "startOffset": 40, "endOffset": 277}], "year": 2016, "abstractText": "Combinatory Category Grammar (CCG) supertagging is a task to assign lexical categories to each word in a sentence. Almost all previous methods use fixed context window sizes as input features. However, it is obvious that different tags usually rely on different context window sizes. These motivate us to build a supertagger with a dynamic window approach, which can be treated as an attention mechanism on the local contexts. Applying dropout on the dynamic filters can be seen as drop on words directly, which is superior to the regular dropout on word embeddings. We use this approach to demonstrate the state-of-the-art CCG supertagging performance on the standard test set.", "creator": "LaTeX with hyperref package"}}}