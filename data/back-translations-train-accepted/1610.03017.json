{"id": "1610.03017", "review": {"conference": "aaai", "VERSION": "v1", "DATE_OF_SUBMISSION": "10-Oct-2016", "title": "Fully Character-Level Neural Machine Translation without Explicit Segmentation", "abstract": "Most existing machine translation systems operate at the level of words, relying on explicit segmentation to extract tokens. We introduce a neural machine translation (NMT) model that maps a source character sequence to a target character sequence without any segmentation. We employ a character-level convolutional network with max-pooling at the encoder to reduce the length of source representation, allowing the model to be trained at a speed comparable to subword-level models while capturing local regularities. Our character-to-character model outperforms a recently proposed baseline with a subword-level encoder on WMT'15 DE-EN and CS-EN, and gives comparable performance on FI-EN and RU-EN. We then demonstrate that it is possible to share a single character-level encoder across multiple languages by training a model on a many-to-one translation task. In this multilingual setting, the character-level encoder significantly outperforms the subword-level encoder on all the language pairs. We also observe that the quality of the multilingual character-level translation even surpasses the models trained and tuned on one language pair, namely on CS-EN, FI-EN and RU-EN.", "histories": [["v1", "Mon, 10 Oct 2016 18:19:34 GMT  (380kb,D)", "http://arxiv.org/abs/1610.03017v1", "14 pages, 2 figures"], ["v2", "Tue, 1 Nov 2016 17:51:32 GMT  (415kb,D)", "http://arxiv.org/abs/1610.03017v2", "15 pages, 2 figures"], ["v3", "Tue, 13 Jun 2017 03:32:34 GMT  (326kb,D)", "http://arxiv.org/abs/1610.03017v3", "Transactions of the Association for Computational Linguistics (TACL), 2017"]], "COMMENTS": "14 pages, 2 figures", "reviews": [], "SUBJECTS": "cs.CL cs.LG", "authors": ["jason lee", "kyunghyun cho", "thomas hofmann"], "accepted": true, "id": "1610.03017"}, "pdf": {"name": "1610.03017.pdf", "metadata": {"source": "CRF", "title": "Fully Character-Level Neural Machine Translation without Explicit Segmentation", "authors": ["Jason Lee", "Kyunghyun Cho", "Thomas Hofmann"], "emails": ["jasonlee@inf.ethz.ch", "kyunghyun.cho@nyu.edu", "thomas.hofmann@inf.ethz.ch"], "sections": [{"heading": "1 Introduction", "text": "The majority of this work was completed while the author was at New York University (Jackendoff, 1992), one reason being that sequences are much longer when represented in characters. Despite their remarkable success, the NMT models suffer from several major weaknesses. For one thing, they are unable to model rare words emerging from the vocabulary, which makes them a rich morphology, such as Czech, Finnish, and Turkish."}, {"heading": "2 Background: Attentional Neural Machine Translation", "text": "Neural machine translation (NMT) is a recently proposed approach to machine translation that builds a single neural network that inputs a source sentence X = (x1,.) and translates it Y = (y1,.., yTy), where xt and yt \u00b2 are source and target symbols (Bahdanau et al., 2015; Sutskever et al., 2015; Luong et al., 2015; Attentional NMT models have three components: an encoder, a decoder, and an attention mechanism. Encoder Faced with a source set X, the encoder constructs a continuous representation that summarizes its meaning with a recursive neural network (RNN). A bidirectional RNN is often implemented as proposed in (Bahdanau et al., 2015)."}, {"heading": "3 Fully Character-Level Translation", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "3.1 Why Character-Level?", "text": "The advantages of character-level translation over word-level translation are already well known. Chung et al. (2016) present three main arguments: character-level models (1) do not suffer from vocabulary-level problems, (2) are able to model different, rare morphological variants of a word, and (3) do not require segmentation. In particular, text segmentation is highly trivial for many languages, and problematic even for English, since word tokenizers are either designed manually or trained from a corpus, using an objective function that has nothing to do with the translation task, making the overall system suboptimal. At this point, we present two additional arguments for character-level translation: First, a character system can easily be applied to a multilingual translation situation. Between European languages where the majority of alphabets overlap, we cannot simply identify morphemes that are represented at character-level."}, {"heading": "3.2 Related Work", "text": "To address these limitations associated with word-level translation, a recent line of research has explored the use of sub-word information. Costa-jussa and Fonollosa (2016) replaced the word search table with revolutionary and highway layers on top of letter embeddings, while still segmenting source words into words. Target sentences were also segmented into words, and predictions were made at word level. Similarly, Ling et al. (2015) used a bi-directional LSTM to compose letter embeddings in word embeddings. On the target page, another LSTM takes the hidden state of the decoder and generates the target word by letter. While this system is completely open, it also requires word embeddings in word embeddings. Character embeddings in word embeddings are also clearly different embeddings."}, {"heading": "3.3 Challenges", "text": "Sentences are on average 6 (DE, CS and RU) to 8 (FI) times longer when represented in characters, which presents three major challenges to achieving a complete translation at character level. (1) Training / decoding latency For the decoder, the string to be generated is much longer, but each Softmax operation at character level costs considerably less compared to a Softmax operation at word or subword level. Chung et al. (2016) report that the character decoding is only 14% slower than decoding at subword level. On the other hand, the computational complexity of the attention mechanism grows squarely in terms of sentence length, as it has to take care of each source code for each target token. This makes a na\u00efve approach at character level, as in (Luong and Manning, 2016), making the computational complexity of the coding mechanism more difficult. Comparing the reduction of the length of the key with the duration of the long encoding is also a function (the function of decreasing the length of the key)."}, {"heading": "4 Fully Character-Level NMT", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "4.1 Encoder", "text": "We are designing an encoder that addresses all of the challenges mentioned above by getting used to the challenges mentioned above in a way that allows us to move on two levels: on one side, on the other, on the other, on the one side, on the fourth, on the fourth, on the fourth, on the fourth, on the fourth, on the fourth, on the fourth, on the fourth, on the fourth, on the fourth, on the fourth, on the fourth, on the fourth, on the fourth, on the fourth, on the fourth, on the fourth, on the fifth, on the fifth, on the fifth, on the fifth, on the ninth, on the fourth, on the fourth, on the fourth, on the fourth, on the fourth, on the fourth, on the fourth, on the fourth, on the fourth, on the fifth, on the fifth, on the ninth, on the ninth, on the ninth, on the ninth, on the ninth, on the ninth, on the ninth, on the ninth, on the ninth, on the ninth, on the ninth, on the ninth, on the ninth, on the ninth, on the ninth, on the ninth on the ninth, on the ninth, on the ninth, on the ninth on the ninth, on the ninth, on the ninth on the ninth, on the ninth, on the ninth on the ninth, on the ninth, on the ninth on the ninth, on the ninth on the fourth, on the fourth, on the fourth, on the fourth, on the fourth, on the fourth, on the fourth, on the fourth, on the fourth, on the fourth, on the fourth, on the fourth, on the fourth, on the fourth, on the fourth, on the fourth, on the fourth, on the fourth, on the fourth, on the fourth, on the fourth, on the fourth, on the fourth, on the fourth, on the fourth, on the fourth, on the fourth, on the fourth, on the fourth, on the fourth, on the fourth, on the fourth, on the fourth, on the fourth, on the fourth, on the fourth, on the fourth, on the fourth, on the fourth, on the fourth, on the fourth, on the fourth, on the fourth, on the fourth, on the fourth, on the fourth, on the fourth, on the fourth, on the fourth, on the fourth, on the fourth, on the fourth"}, {"heading": "4.2 Attention and Decoder", "text": "Similar to the attention model in (Bahdanau et al., 2015), a single-layer feedback network calculates the attention value of the next target character generated with each source segment representation. A two-layer character decoder then takes the source context vector out of the attention mechanism and predicts each target character."}, {"heading": "5 Experiment Settings", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "5.1 Task and Models", "text": "We evaluate the proposed character-to-character translation model (char2char) using subword-level baselines (bpe2bpe and bpe2char) on the WMT '15 DE \u2192 EN, CS \u2192 EN, FI \u2192 EN and RU \u2192 EN translation tasks.1 We compare them in two different scenarios: 1) a bilingual environment in which we train a model using data from a single language pair; and 2) a multilingual environment in which the task is manyto-one translation: we train a single model using data from all four language pairs. Therefore, our baselines and models are: (a) bilingual bpe2bpe: from (Firat et al., 2016a). (b) bilingual bpe2char: from (Chung et al., 2016). (c) bilingual char2char (d) multilingual char2char (e) we build all models ourselves (unlike the models we report for the 1 and the 1)."}, {"heading": "5.2 Datasets and Preprocessing", "text": "We use all available parallel data for the four language pairs of WMT '15: DE-EN, CS-EN, FI-EN and RU-EN.For baselines bpe2char, we only use sentence pairs where the source is no longer than 50 subword symbols. For our char2char models, we only use pairs where the source set is no longer than 450 chars. For all language pairs except FI-EN, we use newtest-2013 as development set and newtest-2014 and newtest-2015 as test kits. For FI-EN, we use newsdev-2015 and newtest-2015 as development and test kits. We tokenize each corpus with a script from Moses.2When we train bilingual bpe2char models, we extract 20,000 BPE operations from each source and target corpus with a script (Sennrich et al., 2015). This results in a source set of 20k \u2212 for each language."}, {"heading": "5.3 Training Details", "text": "Each model will be trained with stochastic gradient descent and Adam (Kingma and Ba, 2014) with learning rate 0.0001 and minibatch size 64. Training will continue until the BLEU score is validated on the validation set1http: / / www.statmt.org / wmt15 / translatio n-task.html2This is unnecessary for char2char models but has been done for comparison; the gradient standard will be truncated to a threshold of 1 (Pascanu et al., 2013); all weights will be initialized using a uniform distribution [\u2212 0.01, 0.01]; each model will be trained on a single pre-2016 GTX Titan X GPU with 12 GB of RAM."}, {"heading": "5.4 Decoding Details", "text": "Ab (Chung et al., 2016) uses a two-layer attention decoder with 1024 GRU units for all our experiments. To decode, we use the beam search with length normalization to punish shorter hypotheses. The width of our beam is 20 for char2char models and 5 for bpe2char models."}, {"heading": "5.5 Training Multilingual Models", "text": "In fact, most people who are able to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to"}, {"heading": "6 Quantitative Analysis", "text": "In this section, we first establish our main typotheses for the introduction of character and multilingualism models and examine whether our observations support our hypotheses or not. From our empirical results, we want to check whether the complete translation takes place at subword level in detail. (1) In which framework the multilingual translation is advantageous and (3) whether the translation on multilingualism level provides superior performance. (1) We discuss every hypothesis in detail. (2) In a bilingual environment, the character and multiple translations are carried out. (2) The char2char model clearly carries both subwords on DE-EN level. (Table 5) and CS-EN-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V V-V-V-V-V-V-V-V-V V-V-V-V-V-V-V V-V-V-V-V V-V-V-V-V-V-V-V-V-V-V-V-V-V V-V-V-V-V-V-V-V-V-V-V V-V-V-V V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V"}, {"heading": "7 Qualitative Analysis", "text": "This year we have reached the stage where we are able to say thank you, we are able to put ourselves in a position to be at the top. \""}, {"heading": "8 Conclusion", "text": "We propose a full character-level NMT model that accepts a sequence of characters in the source language and prints a sequence of characters in the target language. What is noteworthy about this model is the lack of explicit hard-coded knowledge about words and their boundaries, and that the model learns these concepts solely from a translation task. Our empirical results show that the character-level model works both as well as or better than the subword-level translation models. We observe a particularly large improvement in FI-EN translation when the model has been trained to translate multiple languages, with the results showing that the character model has more efficient model capacities across different languages than the subword-level models. We observe a particularly large improvement in FI-EN translation when the model has been trained to translate multiple languages, indicating positive cross-border translation to a language with limited resources."}, {"heading": "Acknowledgements", "text": "KC would like to thank Facebook, Google (Google Faculty Award 2016) and NVidia (GPU Center of Excellence 2015-2016) for this work, which was partially supported by Samsung Advanced Institute of Technology (Neural Machine Translation). JL was supported by the Qualcomm Innovation Fellowship and thanks David Yenicelik and Kevin Wallimann for their contribution in designing the qualitative analysis. The authors would also like to thank Prof. Zheng Zhang (NYU Shanghai) for fruitful discussions and comments."}, {"heading": "A Supplementary Examples", "text": "We show additional sample translations in five scenarios: spelling errors (Table 8), rare and long words (Table 9), one-time words (Table 10), morphological inflections (Table 11) and intrasentence code switching (Table 12)."}], "references": [{"title": "Neural machine translation by jointly learning to align and translate", "author": ["Dzmitry Bahdanau", "Kyunghyun Cho", "Yoshua Bengio."], "venue": "Proceedings of the International Conference on Learning Representations (ICLR).", "citeRegEx": "Bahdanau et al\\.,? 2015", "shortCiteRegEx": "Bahdanau et al\\.", "year": 2015}, {"title": "On the properties of neural machine translation: Encoder-decoder approaches", "author": ["Kyunghyun Cho", "Bart van Merri\u00ebnboer", "Dzmitry Bahdanau", "Yoshua Bengio."], "venue": "Proceedings of the 8th Workshop on Syntax, Semantics, and Structure in Statistical Trans-", "citeRegEx": "Cho et al\\.,? 2014a", "shortCiteRegEx": "Cho et al\\.", "year": 2014}, {"title": "Learning phrase representations using rnn encoder-decoder for statistical machine translation", "author": ["Kyunghyun Cho", "Bart van Merri\u00ebnboer", "Caglar Gulcehre", "Fethi Bougares", "Holger Schwenk", "Yoshua Bengio."], "venue": "Proceedings of the Empiricial Methods in Nat-", "citeRegEx": "Cho et al\\.,? 2014b", "shortCiteRegEx": "Cho et al\\.", "year": 2014}, {"title": "A character-level decoder without explicit segmentation for neural machine translation", "author": ["Junyoung Chung", "Kyunghyun Cho", "Yoshua Bengio."], "venue": "Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics.", "citeRegEx": "Chung et al\\.,? 2016", "shortCiteRegEx": "Chung et al\\.", "year": 2016}, {"title": "Character-based neural machine translation", "author": ["Marta R. Costa-Juss\u00e1", "Jos\u00e8 A.R. Fonollosa."], "venue": "Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics, page 357.", "citeRegEx": "Costa.Juss\u00e1 and Fonollosa.,? 2016", "shortCiteRegEx": "Costa.Juss\u00e1 and Fonollosa.", "year": 2016}, {"title": "Multi-way, multilingual neural machine translation with a shared attention mechanism", "author": ["Orhan Firat", "Kyunghyun Cho", "Yoshua Bengio."], "venue": "Proceedings of the 2016 Conference of the North American Chapter of the Association for Computational Linguis-", "citeRegEx": "Firat et al\\.,? 2016a", "shortCiteRegEx": "Firat et al\\.", "year": 2016}, {"title": "Multilingual language processing from bytes", "author": ["Dan Gillick", "Cliff Brunk", "Oriol Vinyals", "Amarnag Subramanya."], "venue": "arXiv preprint arXiv:1512.00103.", "citeRegEx": "Gillick et al\\.,? 2015", "shortCiteRegEx": "Gillick et al\\.", "year": 2015}, {"title": "Long short-term memory", "author": ["Sepp Hochreiter", "J\u00fcrgen Schmidhuber."], "venue": "Neural Computation, 9(8):1735\u2013 1780.", "citeRegEx": "Hochreiter and Schmidhuber.,? 1997", "shortCiteRegEx": "Hochreiter and Schmidhuber.", "year": 1997}, {"title": "Semantic Structures, volume 18", "author": ["Ray S. Jackendoff."], "venue": "MIT press.", "citeRegEx": "Jackendoff.,? 1992", "shortCiteRegEx": "Jackendoff.", "year": 1992}, {"title": "On using very large target vocabulary for neural machine translation", "author": ["S\u00e9bastien Jean", "Kyunghyun Cho", "Roland Memisevic", "Yoshua Bengio."], "venue": "Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics.", "citeRegEx": "Jean et al\\.,? 2015", "shortCiteRegEx": "Jean et al\\.", "year": 2015}, {"title": "Character-aware neural language models", "author": ["Yoon Kim", "Yacine Jernite", "David Sontag", "Alexander M. Rush."], "venue": "arXiv preprint arXiv:1508.06615.", "citeRegEx": "Kim et al\\.,? 2015", "shortCiteRegEx": "Kim et al\\.", "year": 2015}, {"title": "Adam: A method for stochastic optimization", "author": ["Diederik Kingma", "Jimmy Ba."], "venue": "Proceedings of the 3rd International Conference for Learning Representations (ICLR).", "citeRegEx": "Kingma and Ba.,? 2014", "shortCiteRegEx": "Kingma and Ba.", "year": 2014}, {"title": "Character-based neural machine translation", "author": ["Wang Ling", "Isabel Trancoso", "Chris Dyer", "Alan W Black."], "venue": "arXiv preprint arXiv:1511.04586.", "citeRegEx": "Ling et al\\.,? 2015", "shortCiteRegEx": "Ling et al\\.", "year": 2015}, {"title": "Achieving open vocabulary neural machine translation with hybrid word-character models", "author": ["Minh-Thang Luong", "Christopher D. Manning."], "venue": "Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics.", "citeRegEx": "Luong and Manning.,? 2016", "shortCiteRegEx": "Luong and Manning.", "year": 2016}, {"title": "Effective approaches to attentionbased neural machine translation", "author": ["Minh-Thang Luong", "Hieu Pham", "Christopher D. Manning."], "venue": "Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics.", "citeRegEx": "Luong et al\\.,? 2015", "shortCiteRegEx": "Luong et al\\.", "year": 2015}, {"title": "On the difficulty of training recurrent neural networks", "author": ["Razvan Pascanu", "Tomas Mikolov", "Yoshua Bengio."], "venue": "Proceedings of the 30th International Conference on Machine Learning (ICML).", "citeRegEx": "Pascanu et al\\.,? 2013", "shortCiteRegEx": "Pascanu et al\\.", "year": 2013}, {"title": "Neural machine translation of rare words with subword units", "author": ["Rico Sennrich", "Barry Haddow", "Alexandra Birch."], "venue": "Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics.", "citeRegEx": "Sennrich et al\\.,? 2015", "shortCiteRegEx": "Sennrich et al\\.", "year": 2015}, {"title": "Training very deep networks", "author": ["Rupesh Kumar Srivastava", "Klaus Greff", "J\u00fcrgen Schmidhuber."], "venue": "Advances in Neural Information Processing Systems (NIPS 2015), volume 28.", "citeRegEx": "Srivastava et al\\.,? 2015", "shortCiteRegEx": "Srivastava et al\\.", "year": 2015}, {"title": "Sequence to sequence learning with neural networks", "author": ["Ilya Sutskever", "Oriol Vinyals", "Quoc V Le."], "venue": "Advances in Neural Information Processing Systems (NIPS 2015), volume 28.", "citeRegEx": "Sutskever et al\\.,? 2015", "shortCiteRegEx": "Sutskever et al\\.", "year": 2015}, {"title": "Polyglot neural language models: A case study in cross-lingual phonetic representation learning", "author": ["Yulia Tsvetkov", "Sunayana Sitaram", "Manaal Faruqui", "Guillaume Lample", "Patrick Littell", "David Mortensen", "Alan W Black", "Lori Levin", "Chris Dyer."], "venue": "Pro-", "citeRegEx": "Tsvetkov et al\\.,? 2016", "shortCiteRegEx": "Tsvetkov et al\\.", "year": 2016}, {"title": "Efficient Character-level Document Classification by Combining Convolution and Recurrent Layers", "author": ["Yijun Xiao", "Kyunghyun Cho."], "venue": "arXiv preprint arXiv:1602.00367.", "citeRegEx": "Xiao and Cho.,? 2016", "shortCiteRegEx": "Xiao and Cho.", "year": 2016}, {"title": "Character-level convolutional networks for text classification", "author": ["Xiang Zhang", "Junbo Zhao", "Yann LeCun."], "venue": "Advances in Neural Information Processing Systems (NIPS 2015), volume 28.", "citeRegEx": "Zhang et al\\.,? 2015", "shortCiteRegEx": "Zhang et al\\.", "year": 2015}], "referenceMentions": [{"referenceID": 8, "context": "ing (Jackendoff, 1992), one reason behind this is that sequences are significantly longer when represented in characters, compounding the problem of data sparsity and modeling long-range dependencies.", "startOffset": 4, "endOffset": 22}, {"referenceID": 0, "context": "This has driven NMT research to be almost exclusively word-level (Bahdanau et al., 2015; Sutskever et al., 2015).", "startOffset": 65, "endOffset": 112}, {"referenceID": 18, "context": "This has driven NMT research to be almost exclusively word-level (Bahdanau et al., 2015; Sutskever et al., 2015).", "startOffset": 65, "endOffset": 112}, {"referenceID": 9, "context": "If one uses a large vocabulary to combat this (Jean et al., 2015), the complexity of training and decoding grows linearly with respect to the target vocabulary size, leading to a vicious cycle.", "startOffset": 46, "endOffset": 65}, {"referenceID": 0, "context": ", yTy), where xt and yt\u2032 are source and target symbols (Bahdanau et al., 2015; Sutskever et al., 2015; Luong et al., 2015; Cho et al., 2014a).", "startOffset": 55, "endOffset": 141}, {"referenceID": 18, "context": ", yTy), where xt and yt\u2032 are source and target symbols (Bahdanau et al., 2015; Sutskever et al., 2015; Luong et al., 2015; Cho et al., 2014a).", "startOffset": 55, "endOffset": 141}, {"referenceID": 14, "context": ", yTy), where xt and yt\u2032 are source and target symbols (Bahdanau et al., 2015; Sutskever et al., 2015; Luong et al., 2015; Cho et al., 2014a).", "startOffset": 55, "endOffset": 141}, {"referenceID": 1, "context": ", yTy), where xt and yt\u2032 are source and target symbols (Bahdanau et al., 2015; Sutskever et al., 2015; Luong et al., 2015; Cho et al., 2014a).", "startOffset": 55, "endOffset": 141}, {"referenceID": 0, "context": "A bidirectional RNN is often implemented as proposed in (Bahdanau et al., 2015).", "startOffset": 56, "endOffset": 79}, {"referenceID": 7, "context": "a source embedding lookup table, and \u2212\u2192 fenc and \u2190\u2212 fenc are recurrent activation functions such as long short-term memory units (LSTMs, (Hochreiter and Schmidhuber, 1997)) or gated recurrent units (GRUs, (Cho et al.", "startOffset": 137, "endOffset": 171}, {"referenceID": 2, "context": "a source embedding lookup table, and \u2212\u2192 fenc and \u2190\u2212 fenc are recurrent activation functions such as long short-term memory units (LSTMs, (Hochreiter and Schmidhuber, 1997)) or gated recurrent units (GRUs, (Cho et al., 2014b)).", "startOffset": 205, "endOffset": 224}, {"referenceID": 0, "context": "Attention First introduced in (Bahdanau et al., 2015), the attention mechanism lets the decoder attend more to different source symbols for each target symbol.", "startOffset": 30, "endOffset": 53}, {"referenceID": 3, "context": "Chung et al. (2016) present three main arguments: character level models (1) do not suffer from outof-vocabulary issues, (2) are able to model different, rare morphological variants of a word, and (3) do not require segmentation.", "startOffset": 0, "endOffset": 20}, {"referenceID": 12, "context": "Similarly, Ling et al. (2015) employed a bidirectional LSTM to compose character embeddings into word embeddings.", "startOffset": 11, "endOffset": 30}, {"referenceID": 13, "context": "Most recently, Luong and Manning (2016) proposed a hybrid scheme that consults character-level information whenever the model encounters an outof-vocabulary word.", "startOffset": 15, "endOffset": 40}, {"referenceID": 16, "context": "Sennrich et al. (2015) introduced a subword-level NMT model that is capable of open-vocabulary translation using subword-level segmentation based on the byte pair encoding (BPE) algorithm.", "startOffset": 0, "endOffset": 23}, {"referenceID": 3, "context": "Perhaps the work that is closest to our end goal is (Chung et al., 2016), which used a subword-level encoder from (Sennrich et al.", "startOffset": 52, "endOffset": 72}, {"referenceID": 16, "context": ", 2016), which used a subword-level encoder from (Sennrich et al., 2015) and a fully character-level decoder (bpe2char).", "startOffset": 49, "endOffset": 72}, {"referenceID": 21, "context": "Outside NMT, our work is based on a few existing approaches that applied convolutional networks to text, most notably in text classification (Zhang et al., 2015; Xiao and Cho, 2016).", "startOffset": 141, "endOffset": 181}, {"referenceID": 20, "context": "Outside NMT, our work is based on a few existing approaches that applied convolutional networks to text, most notably in text classification (Zhang et al., 2015; Xiao and Cho, 2016).", "startOffset": 141, "endOffset": 181}, {"referenceID": 19, "context": "Also, we drew inspiration for our multilingual models from previous work that showed the possibility of training a single recurrent model for multiple languages in domains other than translation (Tsvetkov et al., 2016; Gillick et al., 2015).", "startOffset": 195, "endOffset": 240}, {"referenceID": 6, "context": "Also, we drew inspiration for our multilingual models from previous work that showed the possibility of training a single recurrent model for multiple languages in domains other than translation (Tsvetkov et al., 2016; Gillick et al., 2015).", "startOffset": 195, "endOffset": 240}, {"referenceID": 3, "context": "Chung et al. (2016) report that characterlevel decoding is only 14% slower than subwordlevel decoding.", "startOffset": 0, "endOffset": 20}, {"referenceID": 13, "context": "This makes a naive character-level approach, such as in (Luong and Manning, 2016), computationally prohibitive.", "startOffset": 56, "endOffset": 81}, {"referenceID": 10, "context": "Inspired by the character-level language model from (Kim et al., 2015), our encoder first reduces the source sentence length with a series of convolutional, pooling and highway layers.", "startOffset": 52, "endOffset": 70}, {"referenceID": 17, "context": "Highway network A sequence of segment embeddings from the max pooling layer is fed into a highway network (Srivastava et al., 2015).", "startOffset": 106, "endOffset": 131}, {"referenceID": 0, "context": "Similarly to the attention model in (Bahdanau et al., 2015), a single-layer feedforward network computes the attention score of next target character to be generated with every source segment representation.", "startOffset": 36, "endOffset": 59}, {"referenceID": 5, "context": "(a) bilingual bpe2bpe: from (Firat et al., 2016a).", "startOffset": 28, "endOffset": 49}, {"referenceID": 3, "context": "(b) bilingual bpe2char: from (Chung et al., 2016).", "startOffset": 29, "endOffset": 49}, {"referenceID": 5, "context": "We train all the models ourselves other than (a), for which we report the results from (Firat et al., 2016a).", "startOffset": 87, "endOffset": 108}, {"referenceID": 16, "context": "2 When training bilingual bpe2char models, we extract 20,000 BPE operations from each of the source and target corpus using a script from (Sennrich et al., 2015).", "startOffset": 138, "endOffset": 161}, {"referenceID": 11, "context": "Each model is trained using stochastic gradient descent and Adam (Kingma and Ba, 2014) with learning rate 0.", "startOffset": 65, "endOffset": 86}, {"referenceID": 15, "context": "The norm of the gradient is clipped with a threshold of 1 (Pascanu et al., 2013).", "startOffset": 58, "endOffset": 80}, {"referenceID": 3, "context": "As from (Chung et al., 2016), a two-layer characterlevel attentional decoder with 1024 GRU units is used for all our experiments.", "startOffset": 8, "endOffset": 28}, {"referenceID": 5, "context": "Following (Firat et al., 2016a; Firat et al., 2016b), each minibatch is balanced, in that the proportion of each language pair in a single minibatch corresponds to that of the full corpus.", "startOffset": 10, "endOffset": 52}, {"referenceID": 5, "context": "(\u2217) results are taken from (Firat et al., 2016a).", "startOffset": 27, "endOffset": 48}], "year": 2016, "abstractText": "Most existing machine translation systems operate at the level of words, relying on explicit segmentation to extract tokens. We introduce a neural machine translation (NMT) model that maps a source character sequence to a target character sequence without any segmentation. We employ a character-level convolutional network with max-pooling at the encoder to reduce the length of source representation, allowing the model to be trained at a speed comparable to subword-level models while capturing local regularities. Our character-to-character model outperforms a recently proposed baseline with a subwordlevel encoder on WMT\u201915 DE-EN and CSEN, and gives comparable performance on FIEN and RU-EN. We then demonstrate that it is possible to share a single characterlevel encoder across multiple languages by training a model on a many-to-one translation task. In this multilingual setting, the character-level encoder significantly outperforms the subword-level encoder on all the language pairs. We also observe that the quality of the multilingual character-level translation even surpasses the models trained and tuned on one language pair, namely on CSEN, FI-EN and RU-EN.", "creator": "LaTeX with hyperref package"}}}