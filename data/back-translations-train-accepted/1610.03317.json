{"id": "1610.03317", "review": {"conference": "nips", "VERSION": "v1", "DATE_OF_SUBMISSION": "11-Oct-2016", "title": "A Greedy Approach for Budgeted Maximum Inner Product Search", "abstract": "Maximum Inner Product Search (MIPS) is an important task in many machine learning applications such as the prediction phase of a low-rank matrix factorization model for a recommender system. There have been some works on how to perform MIPS in sub-linear time recently. However, most of them do not have the flexibility to control the trade-off between search efficient and search quality. In this paper, we study the MIPS problem with a computational budget. By carefully studying the problem structure of MIPS, we develop a novel Greedy-MIPS algorithm, which can handle budgeted MIPS by design. While simple and intuitive, Greedy-MIPS yields surprisingly superior performance compared to state-of-the-art approaches. As a specific example, on a candidate set containing half a million vectors of dimension 200, Greedy-MIPS runs 200x faster than the naive approach while yielding search results with the top-5 precision greater than 75\\%.", "histories": [["v1", "Tue, 11 Oct 2016 13:10:48 GMT  (6874kb,D)", "http://arxiv.org/abs/1610.03317v1", null]], "reviews": [], "SUBJECTS": "cs.DS cs.LG", "authors": ["hsiang-fu yu", "cho-jui hsieh", "qi lei", "inderjit s dhillon"], "accepted": true, "id": "1610.03317"}, "pdf": {"name": "1610.03317.pdf", "metadata": {"source": "CRF", "title": "A Greedy Approach for Budgeted Maximum Inner Product Search", "authors": ["Hsiang-Fu Yu", "Cho-Jui Hsieh", "Inderjit S. Dhillon"], "emails": ["rofuyu@cs.utexas.edu", "chohsieh@cs.ucdavis.edu", "leiqi@ices.utexas.edu", "inderjit@cs.utexas.edu"], "sections": [{"heading": "1 Introduction", "text": "In fact, it is a way in which most people are able, in which they are able, to understand the world in which they live, to understand and to understand what they want."}, {"heading": "2 Existing Approaches for Fast MIPS", "text": "Due to its broad applicability, several algorithms have been proposed to design efficient algorithms for MIPS. Most existing approaches consider reducing the MIPS problem to the closest search problem (NNS), with the goal of identifying the closest candidates of the given query and applying an existing efficient NNS algorithm to solve the reduced problem [1, 2, 11, 13, 14]. [2] is the first MIPS work to apply such MIPS-to-NNS reduction. Variants of MIPS-to-NNS reduction have also been proposed in [13, 14]. Experimental results in [2] show the superiority of NNS reduction over traditional industry-bound search approaches for MIPS [9, 12]. Fast MIPS approaches with sampling programs have recently become popular [3, 5]. Different sampling programs have been suggested to deal with the MIPS problem with different limitations."}, {"heading": "2.1 Approaches with Nearest Neighbor Search Reduction", "text": "(8). (8). (8). (8). (8). (8). (8). (8). (8). (8). (8). (8). (8). (8). (8). (8). (8). (8). (8). (8). (8). (8). (8). (8). (8). (8). (8). (8). (8). (8). (8). (8). (8). (8). (8). (8). (8). (8). (8). (8). (8). (8). (8). (8). (8). (8). (8). (8). (8). (8). (8). (8). (8). (8). (8). (8). (8). (8). (8). (8). (8). (8). (8). (8). (8). (8). (8). (8). (8). (8). (8). (8). (8). (8). (8). (8). (8). (8). (8). (8). (8). (8). (8). (8). (8). (8). (8). (8). (8). (8). (8). (8). (8). (8). (8). (8). (8). (8). (8). (8). (8. (8). (8). (8). (8). (8). (8). (8). (8). (8). (8). (8). (8). (8). (8). (8). (8). (8). (8. (8). (8). (8). (8). (8). (8). (8). (8). (8)."}, {"heading": "2.2 Sampling-based Approaches", "text": "The idea of the sample-based MIPS approach is first proposed in [5] as an approach to performing approximate matrix-matrix multiplications, and its applicability to MIPS problems has only recently been explored [3]. The idea behind a sample-based approach called sample-MSIPS is to design an efficient sampling method so that the j-th candidate is selected with the probability p (j, t): p (j) \u0445 h > j w.Every time a pair (j, t) is sampled, we increase the number for the j-th element by one, but at the end of the sampling process, [n] \u00d7 [k] with the probability p (j, t): p (j, t) \u0445 hjtw.Every time a pair (j, t) is sampled, we increase the number for the j-th element by one."}, {"heading": "3 Budgeted Maximum Inner Product Search", "text": "The basic idea behind the fast, approximate MIPS approaches is to trade the search quality for the shorter latency of the query: the shorter the search latency, the lower the search quality. In most existing fast MIPS approaches, the trade-off depends on the approach-specific parameters, such as the depth of the PCA tree in [2] or the number of hash functions in [11, 13, 14]. Such approach-specific parameters are usually required to construct approach-specific data structures before a query is made, which means that the target conflict is reasonably fixed for all queries. Specifically, the calculation costs for all queries are fixed. However, in many real-world scenarios, each query request may have a different calculation budget, which raises the question: Can we design a fast MIPS approach that supports the dynamic adjustment of the target conflict in the query phase? In this section, we formally define the MIPS problem for the first phase of the MIPS calculation extension, which is a standard for the 3.MIPS extension."}, {"heading": "3.1 Essential Components for Fast MIPS Approaches", "text": "Before diving into the details of budgeted MIPS, we first review the essential components in the fast MIPS algorithms: \u2022 Before each query request: - Query-Independent Data Structure Construction: A pre-processing procedure is performed on the entire candidate sets to create a custom data structure D to store information about H, such as the LSH hash tables [11, 13, 14], space partition trees (e.g. KD tree or PCA tree [2]), or cluster centrifuges [1]. \u2022 For each query request: - Query-dependent pre-processing: In some approaches, a query-dependent pre-processing is required. For example, vector augmentation is required in all approaches with the MIPS NNS reduction."}, {"heading": "3.2 Budgeted MIPS: Problem Definition", "text": "This year it is more than ever before in the history of the city."}, {"heading": "4.2 A Greedy Procedure to Candidate Screening", "text": "To better describe the idea of the proposed algorithm Greedy-MIPS, we consider the following definition (4): definition 1. The rank of an item x under a row of items X = {x1,..) is defined as follows: # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # #"}, {"heading": "4.2.1 Query-dependent Pre-processing", "text": "In Greedy-MIPS we divide nk entries from (j, t) into k groups. The t-th group contains n entries: {(j, t): j = 1,., n}. Here we need an iterator that plays a role similar to the pointer that holds the index j (r], r = 1,., n) in the sorted order induced by the conditional order. (.) n can be used to construct such an iterator, called conditionalIterator, that one index j at a time in the desired sorted order. ConditionalIterator must support current () to support the current index j and getNext () to advance the iterator."}, {"heading": "4.2.2 Candidate Screening", "text": "Recall the requirements for a viable candidate screening procedure to support budgeted MIPS: 1) the flexibility to control the size | C (w) | \u2264 B; and 2) \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7"}, {"heading": "4.2.3 Connection to Sampling-based MIPS Approaches", "text": "Sample-MSIPS, as already mentioned, is essentially a sampling algorithm with a substitution scheme to draw Z entries in such a way that (j, t) is sampled with probability proportional to zjt. Thus, sample-MSIPS can be regarded as a traversal of (j, t) entries in a stratified random order determined by a distribution of the values of {zjt}, while the core idea of Greedy-MIPS in a few perspectives on iterate (j, t) entries of Z in a greedy order caused by the order of {zjt}. Next, we will discuss the differences between Greedy-MIPS and sample-MSIPS in a few perspectives: Algorithm 6 pseudo-code of a selection tree used for Greedy-MIPS.class SelectionTree: def constructor (w, k, iters): \u00b7 O (k)."}, {"heading": "5 Experimental Results", "text": "In this section, we perform extensive empirical comparisons to compare Greedy-MIPS with other state-of-the-art, fast MIPS approaches on both real and synthetic datasets.2This setting is used in experiments in [3]. \u2022 We use Netflix and yahoo music as our recommendation systems in the real world. There are 17, 770 and 624, 961 items in Netflix or yahoo music. In particular, we obtain the low-rank model (W, H) through standard low-rank factorization: min W, H, H, (i, j), (Aij, j), (Aij \u2212 w > i hj) 2 + \u03bb m, i | n, j = 1, j, j = 1, j, j, j, j, j, j, j, j, j, j, j, j, j, j, j, j, j, j, j, j, j, j, j, j, j, j, j, j, j, j, j, j, j"}, {"heading": "5.1 Experimental Settings and Evaluation Criteria", "text": "All experiments are performed on a Linux machine with 20 cores and 256 GB of memory. We make sure that only a single core / thread is used for our experiments. To have a fair comparison, all are compared with the selection tree in algorithm 6: 0. - The improved Greedy MIPS in algorithm 5: 0. - The improved Greedy MIPS in algorithm 5: 0. - The improved Greedy MIPS in algorithm 5: 0."}, {"heading": "5.2 Experimental Results", "text": "In Figure 4, we show the comparison between the three variants of Greedy-MIPS on netflix and yahoo-music. For comparison with other MIPS approaches, we use Greedy-MIPS to denote the results of the version with the combination of Algorithm 5 and Algorithm 6.Results on Real-World Data Sets. Comparison results for Netflix and yahoo-music are shown in Figure 5. The first, second and third columns show the results with k = 50, k = 100 and k = 200, respectively. It is clear to observe that at fixed acceleration, Greedy-MIPS predictions are achieved with significantly higher search quality. In particular, for yahoo-music data with k = 200, Greedy-MIPS comparisons are faster than MIPS-MIPS with various MIPS-200S forms."}, {"heading": "6 Conclusions", "text": "In this paper, we examine the calculation problem in the predictive phase of many MF-based models: a maximum internal product search problem (MIPS) with a very large number of candidate embeddings. By carefully studying the problem structure of MIPS, we develop a novel Greedy MIPS algorithm that can handle budgeted MIPS according to plan. Although simple and intuitive, Greedy-MIPS delivers surprisingly superior performance compared to most modern approaches. As a concrete example, Greedy-MIPS leads 200 times faster than the naive approach on a candidate set that contains half a million vectors of dimension 200 and delivers search results with a top-5 precision of more than 75%."}], "references": [{"title": "Clustering is efficient for approximate maximum inner product search, 2016", "author": ["Alex Auvolat", "Sarath Chandar", "Pascal Vincent", "Hugo Larochelle", "Yoshua Bengio"], "venue": "arXiv preprint arXiv:1507.05910", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2016}, {"title": "Speeding up the xbox recommender system using a euclidean transformation for inner-product spaces", "author": ["Yoram Bachrach", "Yehuda Finkelstein", "Ran Gilad-Bachrach", "Liran Katzir", "Noam Koenigstein", "Nir Nice", "Ulrich Paquet"], "venue": "In Proceedings of the 8th ACM Conference on Recommender Systems,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2014}, {"title": "Diamond sampling for approximate maximum all-pairs dot-product (MAD) search", "author": ["Grey Ballard", "Seshadhri Comandur", "Tamara Kolda", "Ali Pinar"], "venue": "In Proceedings of the IEEE International Conference on Data Mining,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2015}, {"title": "A learning-rate schedule for stochastic gradient methods to matrix factorization", "author": ["Wei-Sheng Chin", "Yong Zhuang", "Yu-Chin Juan", "Chih-Jen Lin"], "venue": "In Proceedings of the Pacific-Asia Conference on Knowledge Discovery and Data Mining (PAKDD),", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2015}, {"title": "Approximating matrix multiplication for pattern recognition tasks", "author": ["Edith Cohen", "David D. Lewis"], "venue": "In Proceedings of the Eighth Annual ACM-SIAM Symposium on Discrete Algorithms,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 1997}, {"title": "Introduction to Algorithms", "author": ["Thomas H. Cormen", "Charles E. Leiserson", "Ronald L Rivest"], "venue": null, "citeRegEx": "6", "shortCiteRegEx": "6", "year": 1990}, {"title": "The Yahoo! music dataset and KDD- Cup\u201911", "author": ["Gideon Dror", "Noam Koenigstein", "Yehuda Koren", "Markus Weimer"], "venue": "In JMLR Workshop and Conference Proceedings: Proceedings of KDD Cup 2011 Competition,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2012}, {"title": "The Art of Cmoputer Programming, Volumne 3: Sorting and Searching", "author": ["Donald E. Knuth"], "venue": "Addison-Wesley, 2nd edition,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 1998}, {"title": "Efficient retrieval of recommendations in a matrix factorization framework", "author": ["Noam Koenigstein", "Parikshit Ram", "Yuval Shavitt"], "venue": "In Proceedings of the 21st ACM International Conference on Information and Knowledge Management,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2012}, {"title": "Matrix factorization techniques for recommender systems", "author": ["Yehuda Koren", "Robert M. Bell", "Chris Volinsky"], "venue": "IEEE Computer,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2009}, {"title": "On symmetric and asymmetric lshs for inner product search", "author": ["Behnam Neyshabur", "Nathan Srebro"], "venue": "In Proceedings of the International Conference on Machine Learning,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 1926}, {"title": "Maximum inner-product search using cone trees", "author": ["Parikshit Ram", "Alexander G. Gray"], "venue": "In Proceedings of the 18th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2012}, {"title": "Asymmetric lsh (ALSH) for sublinear time maximum inner product search (MIPS)", "author": ["Anshumali Shrivastava", "Ping Li"], "venue": "Advances in Neural Information Processing Systems", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2014}, {"title": "Improved asymmetric locality senstive hashing lsh (ALSH) for maximum inner product search (MIPS)", "author": ["Anshumali Shrivastava", "Ping Li"], "venue": "In Proceedings of the Thirty-First Conference on Uncertainty in Artificial Intelligence (UAI),", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2015}, {"title": "Large scale image annotation: learning to rank with joint word-image embeddings", "author": ["Jason Weston", "Samy Bengio", "Nicolas Usunier"], "venue": null, "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2010}, {"title": "Parallel matrix factorization for recommender systems", "author": ["Hsiang-Fu Yu", "Cho-Jui Hsieh", "Si Si", "Inderjit S. Dhillon"], "venue": "Knowledge and Information Systems,", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2014}, {"title": "A scalable asynchronous distributed algorithm for topic modeling", "author": ["Hsiang-Fu Yu", "Cho-Jui Hsieh", "Hyokun Yun", "S.V.N. Vishwanathan", "Inderjit S. Dhillon"], "venue": "In Proceedings of the International World Wide Web Conference,", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2015}, {"title": "Large-scale multi-label learning with missing labels", "author": ["Hsiang-Fu Yu", "Prateek Jain", "Purushottam Kar", "Inderjit S. Dhillon"], "venue": "In Proceedings of the International Conference on Machine Learning,", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2014}], "referenceMentions": [{"referenceID": 6, "context": "Matrix factorization (MF) based recommender system [7, 10] is one of the most important applications.", "startOffset": 51, "endOffset": 58}, {"referenceID": 9, "context": "Matrix factorization (MF) based recommender system [7, 10] is one of the most important applications.", "startOffset": 51, "endOffset": 58}, {"referenceID": 5, "context": "When both m and n are large, the prediction procedure is extremely time consuming; it is even slower than the training 1When only the largest B elements are required, the sorting procedure can be reduced to O(n+ B logB) on average using a selection algorithm [6].", "startOffset": 259, "endOffset": 262}, {"referenceID": 14, "context": "In addition, MIPS can be found in many other machine learning applications, such as the prediction for a multi-class or multi-label classifier [15, 18], an object detector, a structure SVM predicator, and many others.", "startOffset": 143, "endOffset": 151}, {"referenceID": 17, "context": "In addition, MIPS can be found in many other machine learning applications, such as the prediction for a multi-class or multi-label classifier [15, 18], an object detector, a structure SVM predicator, and many others.", "startOffset": 143, "endOffset": 151}, {"referenceID": 1, "context": "There is a recent line of research on accelerating MIPS for large n, such as [2, 3, 9, 11\u201313].", "startOffset": 77, "endOffset": 93}, {"referenceID": 2, "context": "There is a recent line of research on accelerating MIPS for large n, such as [2, 3, 9, 11\u201313].", "startOffset": 77, "endOffset": 93}, {"referenceID": 8, "context": "There is a recent line of research on accelerating MIPS for large n, such as [2, 3, 9, 11\u201313].", "startOffset": 77, "endOffset": 93}, {"referenceID": 10, "context": "There is a recent line of research on accelerating MIPS for large n, such as [2, 3, 9, 11\u201313].", "startOffset": 77, "endOffset": 93}, {"referenceID": 11, "context": "There is a recent line of research on accelerating MIPS for large n, such as [2, 3, 9, 11\u201313].", "startOffset": 77, "endOffset": 93}, {"referenceID": 12, "context": "There is a recent line of research on accelerating MIPS for large n, such as [2, 3, 9, 11\u201313].", "startOffset": 77, "endOffset": 93}, {"referenceID": 1, "context": "Our contributions can be summarized as follows: \u2022 We carefully study the MIPS problem and develop Greedy-MIPS, which is a novel algorithm without any nearest neighbor search reduction that is essential in many state-of-the-art approaches [2, 11, 13].", "startOffset": 238, "endOffset": 249}, {"referenceID": 10, "context": "Our contributions can be summarized as follows: \u2022 We carefully study the MIPS problem and develop Greedy-MIPS, which is a novel algorithm without any nearest neighbor search reduction that is essential in many state-of-the-art approaches [2, 11, 13].", "startOffset": 238, "endOffset": 249}, {"referenceID": 12, "context": "Our contributions can be summarized as follows: \u2022 We carefully study the MIPS problem and develop Greedy-MIPS, which is a novel algorithm without any nearest neighbor search reduction that is essential in many state-of-the-art approaches [2, 11, 13].", "startOffset": 238, "endOffset": 249}, {"referenceID": 2, "context": "To the best of our knowledge, among existing MIPS approaches, only the sampling approaches proposed in [3, 5] support the similar flexibility under a limited situation where all the candidates and query vectors are non-negative.", "startOffset": 103, "endOffset": 109}, {"referenceID": 4, "context": "To the best of our knowledge, among existing MIPS approaches, only the sampling approaches proposed in [3, 5] support the similar flexibility under a limited situation where all the candidates and query vectors are non-negative.", "startOffset": 103, "endOffset": 109}, {"referenceID": 0, "context": "Most of existing approaches consider to reduce the MIPS problem to the nearest neighbor search problem (NNS), where the goal is to identify the nearest candidates of the given query, and apply an existing efficient NNS algorithm to solve the reduced problem [1, 2, 11, 13, 14].", "startOffset": 258, "endOffset": 276}, {"referenceID": 1, "context": "Most of existing approaches consider to reduce the MIPS problem to the nearest neighbor search problem (NNS), where the goal is to identify the nearest candidates of the given query, and apply an existing efficient NNS algorithm to solve the reduced problem [1, 2, 11, 13, 14].", "startOffset": 258, "endOffset": 276}, {"referenceID": 10, "context": "Most of existing approaches consider to reduce the MIPS problem to the nearest neighbor search problem (NNS), where the goal is to identify the nearest candidates of the given query, and apply an existing efficient NNS algorithm to solve the reduced problem [1, 2, 11, 13, 14].", "startOffset": 258, "endOffset": 276}, {"referenceID": 12, "context": "Most of existing approaches consider to reduce the MIPS problem to the nearest neighbor search problem (NNS), where the goal is to identify the nearest candidates of the given query, and apply an existing efficient NNS algorithm to solve the reduced problem [1, 2, 11, 13, 14].", "startOffset": 258, "endOffset": 276}, {"referenceID": 13, "context": "Most of existing approaches consider to reduce the MIPS problem to the nearest neighbor search problem (NNS), where the goal is to identify the nearest candidates of the given query, and apply an existing efficient NNS algorithm to solve the reduced problem [1, 2, 11, 13, 14].", "startOffset": 258, "endOffset": 276}, {"referenceID": 1, "context": "[2] is the first MIPS work which adopts such a MIPS-to-NNS reduction.", "startOffset": 0, "endOffset": 3}, {"referenceID": 12, "context": "Variants MIPS-to-NNS reduction are also proposed in [13, 14].", "startOffset": 52, "endOffset": 60}, {"referenceID": 13, "context": "Variants MIPS-to-NNS reduction are also proposed in [13, 14].", "startOffset": 52, "endOffset": 60}, {"referenceID": 1, "context": "Experimental results in [2] show the superiority of the NNS reduction over the traditional branch-and-bound search approaches for MIPS [9, 12].", "startOffset": 24, "endOffset": 27}, {"referenceID": 8, "context": "Experimental results in [2] show the superiority of the NNS reduction over the traditional branch-and-bound search approaches for MIPS [9, 12].", "startOffset": 135, "endOffset": 142}, {"referenceID": 11, "context": "Experimental results in [2] show the superiority of the NNS reduction over the traditional branch-and-bound search approaches for MIPS [9, 12].", "startOffset": 135, "endOffset": 142}, {"referenceID": 2, "context": "Fast MIPS approaches with sampling schemes have become popular recently [3, 5].", "startOffset": 72, "endOffset": 78}, {"referenceID": 4, "context": "Fast MIPS approaches with sampling schemes have become popular recently [3, 5].", "startOffset": 72, "endOffset": 78}, {"referenceID": 1, "context": "In 1(b), the reduction proposed in [2] is applied to w and {hj}: \u0175 = [w; 0]> and \u0125j = [hj ; \u221a M \u2212 \u2016hj\u2016], \u2200j, where M = maxj \u2016hj\u2016.", "startOffset": 35, "endOffset": 38}, {"referenceID": 1, "context": "We briefly introduce the concept of the reduction proposed in [2].", "startOffset": 62, "endOffset": 65}, {"referenceID": 1, "context": "To handle the situation where candidates have different lengths, [2] proposes the following transform to reduce the original MIPS problem with H and w in a k dimensional space to a new NNS problem with", "startOffset": 65, "endOffset": 68}, {"referenceID": 13, "context": "In [14], another MIPS-to-NNS reduction has been proposed.", "startOffset": 3, "endOffset": 7}, {"referenceID": 13, "context": "In the procedure by [14], all the hj vectors are assumed (or scaled) to have \u2016hj\u2016 \u2264 U, \u2200j, where U < 1 is a positive constant.", "startOffset": 20, "endOffset": 24}, {"referenceID": 13, "context": "Because U < 1, [14] shows that with the transform (3), we have \u2225\u2225\u0125j \u2225\u2225 2 = k\u0304/4 + \u2016hj\u2016 k\u0304+1 , with the second term vanishing as k\u0304 \u2192 \u221e.", "startOffset": 15, "endOffset": 19}, {"referenceID": 10, "context": "Both transforms show a similar empirical performance in [11].", "startOffset": 56, "endOffset": 60}, {"referenceID": 10, "context": "In [11, 13, 14], various locality sensitive hashing schemes have been considered.", "startOffset": 3, "endOffset": 15}, {"referenceID": 12, "context": "In [11, 13, 14], various locality sensitive hashing schemes have been considered.", "startOffset": 3, "endOffset": 15}, {"referenceID": 13, "context": "In [11, 13, 14], various locality sensitive hashing schemes have been considered.", "startOffset": 3, "endOffset": 15}, {"referenceID": 1, "context": "In [2], a PCA-tree based approach is proposed, and shows better performance than LSH-based approaches, which is consistent to the empirical observations in [1] and our experimental results shown in Section 5.", "startOffset": 3, "endOffset": 6}, {"referenceID": 0, "context": "In [2], a PCA-tree based approach is proposed, and shows better performance than LSH-based approaches, which is consistent to the empirical observations in [1] and our experimental results shown in Section 5.", "startOffset": 156, "endOffset": 159}, {"referenceID": 0, "context": "In [1], a simple K-means clustering algorithm is proposed to handled the transformed NNS problem.", "startOffset": 3, "endOffset": 6}, {"referenceID": 4, "context": "The idea of the sampling-based MIPS approach is first proposed in [5] as an approach to perform approximate matrix-matrix multiplications.", "startOffset": 66, "endOffset": 69}, {"referenceID": 2, "context": "Its applicability on MIPS problems is studied very recently [3].", "startOffset": 60, "endOffset": 63}, {"referenceID": 2, "context": "Diamond-MSIPS, a diamond sampling scheme proposed in [3], is an extension of Sample-MSIPS to handle the maximum squared inner product search problem (MSIPS) where the goal is to identify candidate vectors with largest values of ( hj w )2 .", "startOffset": 53, "endOffset": 56}, {"referenceID": 1, "context": "In most existing fast MIPS approaches, the trade-off depends on the approach-specific parameters such as the depth of the PCA tree in [2] or the number of hash functions in [11, 13, 14].", "startOffset": 134, "endOffset": 137}, {"referenceID": 10, "context": "In most existing fast MIPS approaches, the trade-off depends on the approach-specific parameters such as the depth of the PCA tree in [2] or the number of hash functions in [11, 13, 14].", "startOffset": 173, "endOffset": 185}, {"referenceID": 12, "context": "In most existing fast MIPS approaches, the trade-off depends on the approach-specific parameters such as the depth of the PCA tree in [2] or the number of hash functions in [11, 13, 14].", "startOffset": 173, "endOffset": 185}, {"referenceID": 13, "context": "In most existing fast MIPS approaches, the trade-off depends on the approach-specific parameters such as the depth of the PCA tree in [2] or the number of hash functions in [11, 13, 14].", "startOffset": 173, "endOffset": 185}, {"referenceID": 10, "context": "Before diving into the details of budgeted MIPS, we first review the essential components in fast MIPS algorithms: \u2022 Before any query request: \u2013 Query-Independent Data Structure Construction: A pre-processing procedure is performed on the entire candidate sets to construct an approach-specific data structure D to store information about H, such as the LSH hash tables [11, 13, 14], space partition trees (e.", "startOffset": 370, "endOffset": 382}, {"referenceID": 12, "context": "Before diving into the details of budgeted MIPS, we first review the essential components in fast MIPS algorithms: \u2022 Before any query request: \u2013 Query-Independent Data Structure Construction: A pre-processing procedure is performed on the entire candidate sets to construct an approach-specific data structure D to store information about H, such as the LSH hash tables [11, 13, 14], space partition trees (e.", "startOffset": 370, "endOffset": 382}, {"referenceID": 13, "context": "Before diving into the details of budgeted MIPS, we first review the essential components in fast MIPS algorithms: \u2022 Before any query request: \u2013 Query-Independent Data Structure Construction: A pre-processing procedure is performed on the entire candidate sets to construct an approach-specific data structure D to store information about H, such as the LSH hash tables [11, 13, 14], space partition trees (e.", "startOffset": 370, "endOffset": 382}, {"referenceID": 1, "context": ", KD-tree or PCA-tree [2]), or cluster centroids [1].", "startOffset": 22, "endOffset": 25}, {"referenceID": 0, "context": ", KD-tree or PCA-tree [2]), or cluster centroids [1].", "startOffset": 49, "endOffset": 52}, {"referenceID": 0, "context": "For example, a vector augmentation is required in all approaches with the MIPS-to-NNS reduction [1, 2, 11, 13].", "startOffset": 96, "endOffset": 110}, {"referenceID": 1, "context": "For example, a vector augmentation is required in all approaches with the MIPS-to-NNS reduction [1, 2, 11, 13].", "startOffset": 96, "endOffset": 110}, {"referenceID": 10, "context": "For example, a vector augmentation is required in all approaches with the MIPS-to-NNS reduction [1, 2, 11, 13].", "startOffset": 96, "endOffset": 110}, {"referenceID": 12, "context": "For example, a vector augmentation is required in all approaches with the MIPS-to-NNS reduction [1, 2, 11, 13].", "startOffset": 96, "endOffset": 110}, {"referenceID": 1, "context": "In addition, [2] also requires another normalization.", "startOffset": 13, "endOffset": 16}, {"referenceID": 1, "context": "As mentioned earlier, most recently proposed efficient algorithms such as PCA-MIPS [2] and LSH-MIPS [11, 13, 14] adopt the approach to reduce the MIPS problem to an instance of NNS problem, and apply various search space partition data structures or techniques (e.", "startOffset": 83, "endOffset": 86}, {"referenceID": 10, "context": "As mentioned earlier, most recently proposed efficient algorithms such as PCA-MIPS [2] and LSH-MIPS [11, 13, 14] adopt the approach to reduce the MIPS problem to an instance of NNS problem, and apply various search space partition data structures or techniques (e.", "startOffset": 100, "endOffset": 112}, {"referenceID": 12, "context": "As mentioned earlier, most recently proposed efficient algorithms such as PCA-MIPS [2] and LSH-MIPS [11, 13, 14] adopt the approach to reduce the MIPS problem to an instance of NNS problem, and apply various search space partition data structures or techniques (e.", "startOffset": 100, "endOffset": 112}, {"referenceID": 13, "context": "As mentioned earlier, most recently proposed efficient algorithms such as PCA-MIPS [2] and LSH-MIPS [11, 13, 14] adopt the approach to reduce the MIPS problem to an instance of NNS problem, and apply various search space partition data structures or techniques (e.", "startOffset": 100, "endOffset": 112}, {"referenceID": 0, "context": "Unlike the most recent approaches [1, 2, 11, 13, 14], Greedy-MIPS is an approach without any reduction to a NNS problem.", "startOffset": 34, "endOffset": 52}, {"referenceID": 1, "context": "Unlike the most recent approaches [1, 2, 11, 13, 14], Greedy-MIPS is an approach without any reduction to a NNS problem.", "startOffset": 34, "endOffset": 52}, {"referenceID": 10, "context": "Unlike the most recent approaches [1, 2, 11, 13, 14], Greedy-MIPS is an approach without any reduction to a NNS problem.", "startOffset": 34, "endOffset": 52}, {"referenceID": 12, "context": "Unlike the most recent approaches [1, 2, 11, 13, 14], Greedy-MIPS is an approach without any reduction to a NNS problem.", "startOffset": 34, "endOffset": 52}, {"referenceID": 13, "context": "Unlike the most recent approaches [1, 2, 11, 13, 14], Greedy-MIPS is an approach without any reduction to a NNS problem.", "startOffset": 34, "endOffset": 52}, {"referenceID": 0, "context": "hs[1] \u2265 hs[2] \u2265 \u00b7 \u00b7 \u00b7 \u2265 hs[n], \u2022 Candidate screening phase: for any given w 6= 0 and B > 0,", "startOffset": 2, "endOffset": 5}, {"referenceID": 1, "context": "hs[1] \u2265 hs[2] \u2265 \u00b7 \u00b7 \u00b7 \u2265 hs[n], \u2022 Candidate screening phase: for any given w 6= 0 and B > 0,", "startOffset": 10, "endOffset": 13}, {"referenceID": 0, "context": "return { first B elements: {s[1], .", "startOffset": 29, "endOffset": 32}, {"referenceID": 0, "context": "One way to store a ranking \u03c0(\u00b7) induced by X is by a sorted index array s[r] of size |X | such that \u03c0(xs[1]) \u2264 \u03c0(xs[2]) \u2264 \u00b7 \u00b7 \u00b7 \u2264 \u03c0(xs[|X |]).", "startOffset": 104, "endOffset": 107}, {"referenceID": 1, "context": "One way to store a ranking \u03c0(\u00b7) induced by X is by a sorted index array s[r] of size |X | such that \u03c0(xs[1]) \u2264 \u03c0(xs[2]) \u2264 \u00b7 \u00b7 \u00b7 \u2264 \u03c0(xs[|X |]).", "startOffset": 115, "endOffset": 118}, {"referenceID": 0, "context": ", n such that \u03c0t+(st[1]) \u2264 \u03c0t+(st[2]) \u2264 \u00b7 \u00b7 \u00b7 \u2264 \u03c0t+(st[n]), (7)", "startOffset": 20, "endOffset": 23}, {"referenceID": 1, "context": ", n such that \u03c0t+(st[1]) \u2264 \u03c0t+(st[2]) \u2264 \u00b7 \u00b7 \u00b7 \u2264 \u03c0t+(st[n]), (7)", "startOffset": 33, "endOffset": 36}, {"referenceID": 0, "context": "\u03c0t\u2212(st[1]) \u2265 \u03c0t\u2212(st[2]) \u2265 \u00b7 \u00b7 \u00b7 \u2265 \u03c0t\u2212(st[n]).", "startOffset": 6, "endOffset": 9}, {"referenceID": 1, "context": "\u03c0t\u2212(st[1]) \u2265 \u03c0t\u2212(st[2]) \u2265 \u00b7 \u00b7 \u00b7 \u2265 \u03c0t\u2212(st[n]).", "startOffset": 19, "endOffset": 22}, {"referenceID": 0, "context": "iters[1].", "startOffset": 5, "endOffset": 8}, {"referenceID": 1, "context": "current() = iters[2].", "startOffset": 17, "endOffset": 20}, {"referenceID": 5, "context": "(b) End of iteration-1: C(w) = [6]", "startOffset": 31, "endOffset": 34}, {"referenceID": 5, "context": "(c) End of iteration-2: C(w) = [6,1] 5.", "startOffset": 31, "endOffset": 36}, {"referenceID": 0, "context": "(c) End of iteration-2: C(w) = [6,1] 5.", "startOffset": 31, "endOffset": 36}, {"referenceID": 5, "context": "(d) End of iteration-3: C(w) = [6, 1,7] Figure 3: Illustration of Algorithm 5 with w = [1, 1, 0.", "startOffset": 31, "endOffset": 39}, {"referenceID": 0, "context": "(d) End of iteration-3: C(w) = [6, 1,7] Figure 3: Illustration of Algorithm 5 with w = [1, 1, 0.", "startOffset": 31, "endOffset": 39}, {"referenceID": 6, "context": "(d) End of iteration-3: C(w) = [6, 1,7] Figure 3: Illustration of Algorithm 5 with w = [1, 1, 0.", "startOffset": 31, "endOffset": 39}, {"referenceID": 3, "context": "The sorted index arrays are shown in the upper part of circles on the right plot for each sub-figure; for example, s1[4] = 7, s2[1] = 6, and s3[5] = 5.", "startOffset": 117, "endOffset": 120}, {"referenceID": 0, "context": "The sorted index arrays are shown in the upper part of circles on the right plot for each sub-figure; for example, s1[4] = 7, s2[1] = 6, and s3[5] = 5.", "startOffset": 128, "endOffset": 131}, {"referenceID": 4, "context": "The sorted index arrays are shown in the upper part of circles on the right plot for each sub-figure; for example, s1[4] = 7, s2[1] = 6, and s3[5] = 5.", "startOffset": 143, "endOffset": 146}, {"referenceID": 1, "context": "pop() is executed and 7 = iters[2].", "startOffset": 31, "endOffset": 34}, {"referenceID": 1, "context": "current() is appended into C(w), we need to advance iters[2] twice because the index j = 1 has been included in C(w).", "startOffset": 57, "endOffset": 60}, {"referenceID": 0, "context": "first: buf[i]\u2190 buf[2i] else: buf[i]\u2190 buf[2i+ 1] def top(): return buf[1] \u00b7 \u00b7 \u00b7O(1) def updateValue(t, z): \u00b7 \u00b7 \u00b7O(log k) i\u2190 K\u0304 + t buf[i]\u2190 (z, t) while i > 1: i\u2190 bi/2c if buf[2i].", "startOffset": 69, "endOffset": 72}, {"referenceID": 2, "context": "Instead of hj w, Diamond-MSIPS is designed for the MSIPS problem which is to identify candidates with largest ( hj w )2 or \u2223h>j w \u2223\u2223 values [3].", "startOffset": 140, "endOffset": 143}, {"referenceID": 2, "context": "2This setting is used in the experiments in [3]", "startOffset": 44, "endOffset": 47}, {"referenceID": 15, "context": "We use the CCD++ [16] algorithm implemented in LIBPMF to solve the above optimization problem and obtain the user embeddings {wi} and item embeddings {hj}.", "startOffset": 17, "endOffset": 21}, {"referenceID": 3, "context": "We use the same \u03bb used in [4].", "startOffset": 26, "endOffset": 29}, {"referenceID": 1, "context": "\u2022 NNS-based MIPS approaches: \u2013 PCA-MIPS: the approach proposed in [2], which is shown to be the state-of-the-art among treebased approaches [2].", "startOffset": 66, "endOffset": 69}, {"referenceID": 1, "context": "\u2022 NNS-based MIPS approaches: \u2013 PCA-MIPS: the approach proposed in [2], which is shown to be the state-of-the-art among treebased approaches [2].", "startOffset": 140, "endOffset": 143}, {"referenceID": 1, "context": "We implement a complete PCA-Tree with the neighborhood boosting techniques described in [2].", "startOffset": 88, "endOffset": 91}, {"referenceID": 10, "context": "\u2013 LSH-MIPS: the approach proposed in [11, 13].", "startOffset": 37, "endOffset": 45}, {"referenceID": 12, "context": "\u2013 LSH-MIPS: the approach proposed in [11, 13].", "startOffset": 37, "endOffset": 45}, {"referenceID": 1, "context": "We use the nearest neighbor transform function proposed in [2, 11] and use the random projection scheme as the LSH function as suggested in [11].", "startOffset": 59, "endOffset": 66}, {"referenceID": 10, "context": "We use the nearest neighbor transform function proposed in [2, 11] and use the random projection scheme as the LSH function as suggested in [11].", "startOffset": 59, "endOffset": 66}, {"referenceID": 10, "context": "We use the nearest neighbor transform function proposed in [2, 11] and use the random projection scheme as the LSH function as suggested in [11].", "startOffset": 140, "endOffset": 144}, {"referenceID": 2, "context": "\u2022 Diamond-MSIPS: the sampling scheme proposed in [3] for the maximum squared inner product search.", "startOffset": 49, "endOffset": 52}, {"referenceID": 2, "context": "As it shows better performance than LSH-MIPS in [3] in terms of MIPS problems, we also include Diamond-MSIPS into our comparison.", "startOffset": 48, "endOffset": 51}, {"referenceID": 16, "context": "F+Tree [17] is implemented as the multinomial sampling procedure.", "startOffset": 7, "endOffset": 11}], "year": 2016, "abstractText": "Maximum Inner Product Search (MIPS) is an important task in many machine learning applications such as the prediction phase of a low-rank matrix factorization model for a recommender system. There have been some works on how to perform MIPS in sub-linear time recently. However, most of them do not have the flexibility to control the trade-off between search efficient and search quality. In this paper, we study the MIPS problem with a computational budget. By carefully studying the problem structure of MIPS, we develop a novel Greedy-MIPS algorithm, which can handle budgeted MIPS by design. While simple and intuitive, Greedy-MIPS yields surprisingly superior performance compared to state-of-the-art approaches. As a specific example, on a candidate set containing half a million vectors of dimension 200, Greedy-MIPS runs 200x faster than the naive approach while yielding search results with the top-5 precision greater than 75%.", "creator": "LaTeX with hyperref package"}}}