{"id": "1610.03946", "review": {"conference": "EMNLP", "VERSION": "v1", "DATE_OF_SUBMISSION": "13-Oct-2016", "title": "A Neural Network for Coordination Boundary Prediction", "abstract": "We propose a neural-network based model for coordination boundary prediction. The network is designed to incorporate two signals: the similarity between conjuncts and the observation that replacing the whole coordination phrase with a conjunct tends to produce a coherent sentences. The modeling makes use of several LSTM networks. The model is trained solely on conjunction annotations in a Treebank, without using external resources. We show improvements on predicting coordination boundaries on the PTB compared to two state-of-the-art parsers; as well as improvement over previous coordination boundary prediction systems on the Genia corpus.", "histories": [["v1", "Thu, 13 Oct 2016 06:42:51 GMT  (435kb,D)", "http://arxiv.org/abs/1610.03946v1", "EMNLP 2016"]], "COMMENTS": "EMNLP 2016", "reviews": [], "SUBJECTS": "cs.CL", "authors": ["jessica ficler", "yoav goldberg"], "accepted": true, "id": "1610.03946"}, "pdf": {"name": "1610.03946.pdf", "metadata": {"source": "CRF", "title": "A Neural Network for Coordination Boundary Prediction", "authors": ["Jessica Ficler"], "emails": ["jessica.ficler@gmail.com", "yoav.goldberg@gmail.com"], "sections": [{"heading": "1 Introduction", "text": "Coordination is a common syntactical phenomenon that occurs in 38.8% of sentences in Penn Treebank (PTB) (Marcus et al., 1993) and 60.71% of sentences in Genia Treebank (Ohta et al., 2002). However, predicting the correct subjunctive remains one of the biggest challenges for the state's syntactic parsers. Both Berkeley and Zpar Phrase Structure Parsers (Petrov et al., 2006; Zhang and Clark, 2011) reach F1 values of about 69% when evaluated for their ability to restore coordination boundaries in the PTB test group. For example, in: \"He has the blessing of the government to build churches and [spread unificationism] in this country.\" The conjunctions are incorrectly predicted by both parties: \"He has the blessing of the government to build churches and spread unificationism in this country.\""}, {"heading": "2 Background", "text": "Coordination is a very common syntactical construction in which several proposition elements (so-called conjunctions) are linked to each other. For example, in: \"The Jon Bon Jovi Soul Foundation [was founded in 2006] and1 [exists to combat problems that drive (families) and2 (individuals) into economic despair].\" The coordinator und1 connects the conjunctions surrounded by square brackets, and the coordinator and2 connects the conjunctions surrounded by round brackets. Coordination between NPs and between VPs is the most common, but other grammatical functions can also be coordinated: \"[relatively active] ADJP, but [unfocused] ADJP\"; \"in] IN and [outside] the market.\" While coordination usually takes place between elements of the same syntactic category, cross-category conjunctions are also possible: (\"Alice will visit the Earth [tomorrow] NP or [in the next decade] PP.\" Less common coordinations do not include constituent elements on Earth [then] equal to [Alice [1988] or four visited [Alice]]."}, {"heading": "2.1 Symmetry between conjuncts", "text": "Coordinated conjunctions tend to be semantically related and have a similar syntactical structure. For example, (a) and (b) the conjunctions contain similar words (China / Asia, brands / yen) and have identical syntactical structures. PPINforNPNNPChinaCCandPINforNPNNPAsia (a) NPNPCD1.8690NNSmarksCCandNPCD139.75NNSyen (b) symmetry also applies to larger conjunctions, such as in: (c) NPNNincomePPIN fromNPNPQP429.9 trillionNNS-RubelPRN (US $693.4) CC andNPVBZexpenditure PIN fromNPNPQP489.9 trillionNNS-RubelPRN (US $790.2) similarity between conjunctions has been used in previous work on coordination disambiguity (NPNPQP489.9 trillionNNS-RubelPRN, 2009, Shimbo)."}, {"heading": "2.2 Replaceability", "text": "Replacing a conjunction with the entire coordination phrase usually results in a coherent sentence (Huddleston et al., 2002). In \"Ethan has [new products] and [a new strategy],\" the exchange leads to: \"Ethan has developed new products\"; and \"Ethan has developed a new strategy,\" both valid sentences. Conjunctive substitution also applies to conjunctions of different syntactic types, for example: \"Inactivation of tumor suppressor genes [alone] or [in combination] appears to be crucial for the development of scourges such as cancer.\" While symmetry and interchangeability are strong features of coordination, neither principle is universal. Coordination between syntactically dissimilar conjunctions is possible (\"tomorrow and for the whole of the next decade\"), and the replacement principle fails in cases of ellipse, gapping, and others (\"The Bank employs [8,000 people in Spain and 2,000 abroad].\")"}, {"heading": "2.3 Coordination in the PTB", "text": "The coordination comments in Penn Treebank (Marcus et al., 1993) are inconsistent (Hogan, 2007) and there is a lack of internal structure for NP with nominal modifiers (Bies et al., 1995). Furthermore, conjunctions in PTB are not explicitly marked, which led to earlier work to disambiguate coordination (Shimbo and Hara, 2007; Hara et al., 2009; Hanamoto et al., 2012) to use the Genia tree bank of biomedical text (Ohta et al., 2002), which explicitly identifies coordination phrases. However, the use of the Genia corpus is not ideal as it is located in a specialized area and much smaller than the PTB. In this work, we rely on a version of the PTB published by Ficler and Goldberg (2016), which manually remedies the above deficiencies."}, {"heading": "2.4 Neural Networks and Notation", "text": "We use w1: n to specify a list of vectors w1, w2,.. wn and wn: 1 to indicate the inverse list. We use \u0445 for vector concatenation. If a single symbol w is used as the input of a neural network, the corresponding embedding vector is assumed. A multi-layer perceptron (MLP) is a nonlinear classifier. In this thesis, we take MLP as a classifier with a single hidden layer: MLP (x) = V \u00b7 g (Wx + b), where x is the input of the network, g is an activation function like ReLU or sigmoid, and W, V and b are detectable parameters. Recurring neural networks (RNNNNs) (Elman, 1990) allow the representation of arbitrarily large sequences, g is an activation function like ReLU or Sigmoid, and W, V and b are detectable parameters M (RNNNNNNNNNNNNNNs) which allow the representation of STi (in both STi)."}, {"heading": "3 Task Definition and Architecture", "text": "If there is a coordination word in a sentence, the task of the coordination prediction is to return the two associated conjunctions, or NOT if the word does not function as a coordination cycle of a relevant type. Figure 1 provides an example. Our system works in three phases: First, we determine whether the coordination word is actually part of a conjunction of a desired type. Then we extract a ranking of candidate conjunctions in which a candidate a1We look at and, or, but, not as coordination words. If there are more than two coordinated elements (conjunctions), we focus on the two conjunctions closest to the coordinator.Spreading pair of the form ((((i, j), (l, m))). Then the candidates are evaluated and the highest pair of points is returned. Section 4 describes the scoring model that represents the main contribution of this work."}, {"heading": "4 Candidate Conjunctions Scoring", "text": "Our scoring model takes into account two signals, symmetry between conjunctions and the possibility of replacing the entire coordination phrase with the participating conjunctions."}, {"heading": "4.1 The Symmetry Component", "text": "This year it is more than ever before."}, {"heading": "4.2 The Replacement Component", "text": "The replacement component is based on the observation that in many cases the coordinating sentence can be replaced by either one of its conjunctions > 3 points, while maintaining a grammatically and semantically coherent sentence at the same time (Section 2.2). Trying such a replacement on false conjunctions, the resulting sentence is likely to be either syntactically or semantically incorrect. (Example: in the following erroneous analysis: \"Rudolph Agnew, [55 years old] and [former chairman] of Consolidated Gold Fields PLC,\" which replaces the conjunction with the first conjunction, results in semantically incoherent consequences. \"Rudolph Agnew, 55 years old of Consolidated Golden Fields, PLC.\" 4Our goal is to distinguish substitutes resulting from correct conjunctions resulting from those resulting from erroneous conjunctions. To this end, we focus on the connecting points. A connecting point in a resulting sentence is the point at which the sentence splits into two sequences that were not connected in the original sentence."}, {"heading": "4.3 Parser based Features", "text": "In addition to the symmetry and replacement signals, we also include some values derived from the Berkeley parser. As described in Section 5, a list of conjunction candidates is extracted from the CKY chart of the parser. 5Usually, candidates are sorted in descending order by multiplying the internal and external values of the candidate spans: 6I (i, j) \u00b7 O (i, j) \u00b7 I (l, m) \u00b7 I (l, m) \u00b7 I (l, m) \u00b7 I (l, m). Each candidate {(i, j), (l, m)} is assigned two numerical attributes based on this rank: its position in the ranking and the ratio between its score and the score of the neighboring higher-placed candidate. We add an additional binary attribute indicating whether the candidate is in the 1-best tree predicted by the parser. These three attributes are referred to as Feat (l, m)."}, {"heading": "4.4 Final Scoring and Training", "text": "Finally, the score of a candidate {(i, j), (l, m)} in a sentence with the words w1: n and POS tags p1: n is calculated as follows: SCORE (w1: n, p1: n, {(i, j), (l, m)} = MLP (Sym (vPathi: j, v Path l: m) - Repl (w1: n, i, j, l, m) - Repl (p1: n, i, j, l, m) - tricks (i, j, l, m), where vPathi: j and v Path l: m are the vectors resulting from the path LSTMs, and Sym, Repl and Feats are the networks defined in sections 4.1 - 4.3 above. The network is trained together and tries to minimize a paired ranking loss function, with the loss for each training case by: Loss = max (0, y \u2212 number \u2212 candidate \u2212 highest in all cases)."}, {"heading": "5 Candidates Extraction and Supporting Classifiers", "text": "Candidate extraction We extract candidates extends on the basis of the inside-outside probabilities assigned by the Berkeley parser TB 3. Specifically, to obtain 6Inside-Outside probabilities (Goodman, 1998) represent the probability of a span with a given non-terminal symbol NP. The inner probability I (N, i, j) is the probability of generating words wi, wi + 1,..., wj given the fact that the root is the non-terminal N. The outer probability O (N, i, j) is the probability of generating words w1, w2,..., wi \u2212 1, the non-terminal N and the words wj + 1, wj + 2,..., wn with the root S. Candidates for conjunction span we collect span that are marked with COORD, are next to the coordinating word, and have non-zero within or outside the probabilities M."}, {"heading": "6 Experiments", "text": "We evaluate our models for their ability to identify conjunctive boundaries in the extended Penn Treebank (Ficler and Goldberg, 2016) and Genia Treebank (Ohta et al., 2002) 7. In evaluating PTB, we compare the conjunctive boundary predictions of generative7http: / / www-tsujii.is.s.u-tokyo.ac.jp / GENIABerkeley-Parser (Petrov et al., 2006) and the discriminatory Zpar-Parser (Zhang and Clark, 2011).8"}, {"heading": "6.1 Evaluation on PTB", "text": "Baseline Our baseline is the performance of the Berkeley and Zpar parsers on the task presented in Section 78, namely: for a particular coordination word, determine the two spans that are connected by it, and return NONE if the coordinator does not cooperate spans or connects spans that are not of the expected kind. We convert predicted trees into conjunction forecasts by taking the two sentences that lie immediately adjacent to the coordinator on both sides (Ignore phrases that contain exclusively punctuation). For example, in the following Zparse tree is the conjunction forecast (\"February 8, 1990,\" \"May 10, 1990.\"). NPNP-8, 1990CCandNP-May 10, 1990, ADJPrespectivelyCases in which the coordination word is the most leftmost or right-most non-ctuation element in its formulation (e.g.: PRN (P) and) (S was painful) we consider the 8P results not to be a little."}, {"heading": "6.2 Evaluation on Genia", "text": "To compare our model with previous work, we also evaluate the Genia Tree Bank (Beta), a collection of constituency trees for 4529 sets of medline abstracts. The Genia Tree Bank Coordination Note explicitly identifies coordination expressions with a special function label (COOD), making the corpus an appealing resource for earlier work on coordination boundary forecasting (Shimbo and Hara, 2007; Hara et al., 2009; Hanamoto et al., 2012). Following Hara et al. (2009), we evaluate the models \"ability to predict the range of the entire coordination boundary forecast without taking into account individual conjunctions. In\" For example, my plan is to visit Seychelles, ko Samui and Sardinia by the end of the year, \"the goal is to\" restore Seychelles, ko Samui and Sardinia. \"This is a recall measure."}, {"heading": "6.3 Technical Details", "text": "The neural networks (candidate scoring model and supporting classifiers) are implemented with the pyCNN package.10.In the supporting models, we use size 50 words and the sigmoid activation function. The LSTMs also have a dimension of 50. The models are trained using SGD for 10 iterations over the train set, randomly shuffling the samples before each iteration.We select the model with the highest F1 score on the development set.All LSTMs in the candidate scoring model have a dimension of 50. The input vectors for the 10https: / / github.com / clab / cnn / tree / master / pycnnsymmetry LSTM also have a size 50. The MLP in the candidate scoring model uses the Relu activation function, and the model is trained using the Adam optimizer. Words and POS embeddings are split between the symmetry and replacement components."}, {"heading": "7 Analysis", "text": "Our model combines four signals: symmetry, word level replacement, POS level replacement, and features of the Berkeley parser. Table 4 shows the PTB developer performance of each sub-model in isolation. On its own, the signals of each component are relatively weak and rarely exceed the parsers, but they provide complementary information, as shown by the strong performance of the common model. Figure 4 lists the correct and incorrect predictions of each component, suggesting that the individual models actually capture the patterns they should capture - although these patterns do not always lead to correct predictions."}, {"heading": "8 Related Work", "text": "Hogan (2007) integrated this principle into a generative parsing model by modifying the generative process of coordinated NPs to condition the properties of the first conjunction in generating the second. Shimbo and Hara (2007) proposed a discriminatory sequence alignment model to detect similar conjunctions, focusing on disambiguating non-nested coordination based on the learned edit distance between two conjunctions, and Hara et al. (2009) extended their work to include nested coordinations as well. In this work, the discriminatory edit distance model is similar in the spirit of our symmetry components, but is limited to sequences of POS tags and uses a sequence algorithm. We compare our results with Hara et al.'s in Section 6.2. Hanamoto et al. (2012)."}, {"heading": "9 Conclusions", "text": "Our model is based on the observation that (a) conjunctions tend to be similar and (b) that replacing the coordination phrase with a conjunction leads to a coherent set. Our models are based on syntactical information and do not include resources outside the training tree banks, but improve the task of predicting coordination limits compared to modern parsers."}, {"heading": "Acknowledgments", "text": "This work was supported by the Israeli Science Foundation (grant number 1555 / 15) and the German Research Foundation via the German-Israeli Project Cooperation (DIP, grant number DA 1600 / 1- 1)."}], "references": [{"title": "Bracketing guidelines for treebank ii style penn treebank project", "author": ["Bies et al.1995] Ann Bies", "Mark Ferguson", "Karen Katz", "Robert MacIntyre", "Victoria Tredinnick", "Grace Kim", "Mary Ann Marcinkiewicz", "Britta Schasberger"], "venue": null, "citeRegEx": "Bies et al\\.,? \\Q1995\\E", "shortCiteRegEx": "Bies et al\\.", "year": 1995}, {"title": "Tag, dynamic programming, and the perceptron for efficient, feature-rich parsing", "author": ["Michael Collins", "Terry Koo"], "venue": "In Proceedings of the Twelfth Conference on Computational Natural Language Learning,", "citeRegEx": "Carreras et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Carreras et al\\.", "year": 2008}, {"title": "Learning a similarity metric discriminatively, with application to face verification", "author": ["Chopra et al.2005] Sumit Chopra", "Raia Hadsell", "Yann LeCun"], "venue": "In Computer Vision and Pattern Recognition,", "citeRegEx": "Chopra et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Chopra et al\\.", "year": 2005}, {"title": "Type raising, functional composition, and non-constituent conjunction", "author": ["David Dowty"], "venue": "In Categorial grammars and natural language structures,", "citeRegEx": "Dowty.,? \\Q1988\\E", "shortCiteRegEx": "Dowty.", "year": 1988}, {"title": "Finding structure in time", "author": ["Jeffrey L Elman"], "venue": "Cognitive science,", "citeRegEx": "Elman.,? \\Q1990\\E", "shortCiteRegEx": "Elman.", "year": 1990}, {"title": "Coordination annotation extension in the penn tree bank. Association for Computational Linguistics", "author": ["Ficler", "Goldberg2016] Jessica Ficler", "Yoav Goldberg"], "venue": null, "citeRegEx": "Ficler et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Ficler et al\\.", "year": 2016}, {"title": "Parsing insideout. arXiv preprint cmp-lg/9805007", "author": ["Joshua Goodman"], "venue": null, "citeRegEx": "Goodman.,? \\Q1998\\E", "shortCiteRegEx": "Goodman.", "year": 1998}, {"title": "Coordination structure analysis using dual decomposition", "author": ["Takuya Matsuzaki", "Jun\u2019ichi Tsujii"], "venue": "In Proceedings of the 13th Conference of the European Chapter of the Association for Computational Linguistics,", "citeRegEx": "Hanamoto et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Hanamoto et al\\.", "year": 2012}, {"title": "Coordinate structure analysis with global structural constraints and alignment-based local features", "author": ["Hara et al.2009] Kazuo Hara", "Masashi Shimbo", "Hideharu Okuma", "Yuji Matsumoto"], "venue": "In Proceedings of the Joint Conference of the 47th Annual", "citeRegEx": "Hara et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Hara et al\\.", "year": 2009}, {"title": "Long short-term memory", "author": ["Hochreiter", "Schmidhuber1997] Sepp Hochreiter", "J\u00fcrgen Schmidhuber"], "venue": "Neural computation,", "citeRegEx": "Hochreiter et al\\.,? \\Q1997\\E", "shortCiteRegEx": "Hochreiter et al\\.", "year": 1997}, {"title": "Coordinate noun phrase disambiguation in a generative parsing model. Association for Computational Linguistics", "author": ["Deirdre Hogan"], "venue": null, "citeRegEx": "Hogan.,? \\Q2007\\E", "shortCiteRegEx": "Hogan.", "year": 2007}, {"title": "The cambridge grammar of english", "author": ["Geoffrey K Pullum"], "venue": null, "citeRegEx": "Huddleston and Pullum,? \\Q2002\\E", "shortCiteRegEx": "Huddleston and Pullum", "year": 2002}, {"title": "Coordination disambiguation without any similarities", "author": ["Kawahara", "Kurohashi2008] Daisuke Kawahara", "Sadao Kurohashi"], "venue": "In Proceedings of the 22nd International Conference on Computational Linguistics-Volume", "citeRegEx": "Kawahara et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Kawahara et al\\.", "year": 2008}, {"title": "Building a large annotated corpus of english: The penn treebank", "author": ["Mary Ann Marcinkiewicz", "Beatrice Santorini"], "venue": null, "citeRegEx": "Marcus et al\\.,? \\Q1993\\E", "shortCiteRegEx": "Marcus et al\\.", "year": 1993}, {"title": "Efficient estimation of word representations in vector space. arXiv preprint arXiv:1301.3781", "author": ["Kai Chen", "Greg Corrado", "Jeffrey Dean"], "venue": null, "citeRegEx": "Mikolov et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Mikolov et al\\.", "year": 2013}, {"title": "The genia corpus: An annotated research abstract corpus in molecular biology domain", "author": ["Ohta et al.2002] Tomoko Ohta", "Yuka Tateisi", "JinDong Kim"], "venue": "In Proceedings of the second international conference on Human Language Technology Research,", "citeRegEx": "Ohta et al\\.,? \\Q2002\\E", "shortCiteRegEx": "Ohta et al\\.", "year": 2002}, {"title": "Learning accurate, compact, and interpretable tree annotation", "author": ["Petrov et al.2006] Slav Petrov", "Leon Barrett", "Romain Thibaux", "Dan Klein"], "venue": "In Proceedings of the 21st International Conference on Computational Linguistics and the 44th annual meeting", "citeRegEx": "Petrov et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Petrov et al\\.", "year": 2006}, {"title": "Using ltag based features in parse reranking", "author": ["Shen et al.2003] Libin Shen", "Anoop Sarkar", "Aravind K Joshi"], "venue": "In Proceedings of the 2003 conference on Empirical methods in natural language processing,", "citeRegEx": "Shen et al\\.,? \\Q2003\\E", "shortCiteRegEx": "Shen et al\\.", "year": 2003}, {"title": "A discriminative learning model for coordinate conjunctions", "author": ["Shimbo", "Hara2007] Masashi Shimbo", "Kazuo Hara"], "venue": "In EMNLP-CoNLL,", "citeRegEx": "Shimbo et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Shimbo et al\\.", "year": 2007}, {"title": "Syntactic processing using the generalized perceptron and beam search", "author": ["Zhang", "Clark2011] Yue Zhang", "Stephen Clark"], "venue": "Computational linguistics,", "citeRegEx": "Zhang et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Zhang et al\\.", "year": 2011}], "referenceMentions": [{"referenceID": 13, "context": "8% of the sentences in the Penn Treebank (PTB) (Marcus et al., 1993), and in 60.", "startOffset": 47, "endOffset": 68}, {"referenceID": 15, "context": "71% of the sentences in the Genia Treebank (Ohta et al., 2002).", "startOffset": 43, "endOffset": 62}, {"referenceID": 16, "context": "Both the Berkeley and Zpar phrase-structure parsers (Petrov et al., 2006; Zhang and Clark, 2011) achieve F1 scores of around 69% when evaluated on their ability to recover coordination boundaries on the PTB test set.", "startOffset": 52, "endOffset": 96}, {"referenceID": 10, "context": "Semantic signals (which are likely to be based on resources external to the treebank) are also relevant for coordination disambiguation (Kawahara and Kurohashi, 2008; Hogan, 2007) and provide complementary information.", "startOffset": 136, "endOffset": 179}, {"referenceID": 3, "context": "Less common coordinations involve non-constituent elements \u201c[equal to] or [higher than]\u201d, argument clusters (\u201cAlice visited [4 planets] [in 2014] and [3 more] [since then]\u201d), and gapping (\u201c[Bob lives on Earth] and [Alice on Saturn]\u201d) (Dowty, 1988).", "startOffset": 234, "endOffset": 247}, {"referenceID": 10, "context": "Similarity between conjuncts was used as a guiding principle in previous work on coordination disambiguation (Hogan, 2007; Shimbo and Hara, 2007; Hara et al., 2009).", "startOffset": 109, "endOffset": 164}, {"referenceID": 8, "context": "Similarity between conjuncts was used as a guiding principle in previous work on coordination disambiguation (Hogan, 2007; Shimbo and Hara, 2007; Hara et al., 2009).", "startOffset": 109, "endOffset": 164}, {"referenceID": 13, "context": "3 Coordination in the PTB Coordination annotation in the Penn Treebank (Marcus et al., 1993) is inconsistent (Hogan, 2007) and lacks internal structure for NPs with nominal modifiers (Bies et al.", "startOffset": 71, "endOffset": 92}, {"referenceID": 10, "context": ", 1993) is inconsistent (Hogan, 2007) and lacks internal structure for NPs with nominal modifiers (Bies et al.", "startOffset": 24, "endOffset": 37}, {"referenceID": 0, "context": ", 1993) is inconsistent (Hogan, 2007) and lacks internal structure for NPs with nominal modifiers (Bies et al., 1995).", "startOffset": 98, "endOffset": 117}, {"referenceID": 8, "context": "These deficiencies led previous works on coordination disambiguation (Shimbo and Hara, 2007; Hara et al., 2009; Hanamoto et al., 2012) to use the Genia treebank of biomedical text (Ohta et al.", "startOffset": 69, "endOffset": 134}, {"referenceID": 7, "context": "These deficiencies led previous works on coordination disambiguation (Shimbo and Hara, 2007; Hara et al., 2009; Hanamoto et al., 2012) to use the Genia treebank of biomedical text (Ohta et al.", "startOffset": 69, "endOffset": 134}, {"referenceID": 15, "context": ", 2012) to use the Genia treebank of biomedical text (Ohta et al., 2002) which explicitly marks coordination phrases.", "startOffset": 53, "endOffset": 72}, {"referenceID": 4, "context": "Recurrent Neural Networks (RNNs) (Elman, 1990) allow the representation of arbitrary sized sequences.", "startOffset": 33, "endOffset": 46}, {"referenceID": 2, "context": "This architecture is similar to Siamese Networks, which are used for learning similarity functions in vision tasks (Chopra et al., 2005).", "startOffset": 115, "endOffset": 136}, {"referenceID": 1, "context": "2 For example, the projections for the first conjunct in Figure 2 are: Similar in spirit to the spines used in Carreras et al. (2008) and Shen et al.", "startOffset": 111, "endOffset": 134}, {"referenceID": 1, "context": "2 For example, the projections for the first conjunct in Figure 2 are: Similar in spirit to the spines used in Carreras et al. (2008) and Shen et al. (2003). VP", "startOffset": 111, "endOffset": 157}, {"referenceID": 14, "context": "3 In both approaches, the POS embeddings are initialized with vectors that are pre-trained by running word2vec (Mikolov et al., 2013) on the POS sequences in PTB training set.", "startOffset": 111, "endOffset": 133}, {"referenceID": 6, "context": "Inside-Outside probabilities (Goodman, 1998) represent the probability of a span with a given non-terminal symbol.", "startOffset": 29, "endOffset": 44}, {"referenceID": 15, "context": "We evaluate our models on their ability to identify conjunction boundaries in the extended Penn Treebank (Ficler and Goldberg, 2016) and Genia Treebank (Ohta et al., 2002)7.", "startOffset": 152, "endOffset": 171}, {"referenceID": 16, "context": "Berkeley parser (Petrov et al., 2006) and the discriminative Zpar parser (Zhang and Clark, 2011).", "startOffset": 16, "endOffset": 37}, {"referenceID": 8, "context": "When evaluating on the Genia treebank, we compare to the results of the discriminative coordination-prediction model of Hara et al. (2009).8", "startOffset": 120, "endOffset": 139}, {"referenceID": 7, "context": "Another relevant model in the literature is (Hanamoto et al., 2012), however the results are not directly comparable as they use a slightly different definition of conjuncts, and evaluate on a subset of the Genia treebank, containing only trees that were properly converted to an HPSG formalism.", "startOffset": 44, "endOffset": 67}, {"referenceID": 8, "context": "The Genia treebank coordination annotation explicitly marks coordination phrases with a special function label (COOD), making the corpus an appealing resource for previous work on coordination boundary prediction (Shimbo and Hara, 2007; Hara et al., 2009; Hanamoto et al., 2012).", "startOffset": 213, "endOffset": 278}, {"referenceID": 7, "context": "The Genia treebank coordination annotation explicitly marks coordination phrases with a special function label (COOD), making the corpus an appealing resource for previous work on coordination boundary prediction (Shimbo and Hara, 2007; Hara et al., 2009; Hanamoto et al., 2012).", "startOffset": 213, "endOffset": 278}, {"referenceID": 7, "context": ", 2009; Hanamoto et al., 2012). Following Hara et al. (2009), we evaluate the models\u2019 ability to predict the span of the entire coordination phrase, disregarding the individual conjuncts.", "startOffset": 8, "endOffset": 61}, {"referenceID": 7, "context": ", 2009; Hanamoto et al., 2012). Following Hara et al. (2009), we evaluate the models\u2019 ability to predict the span of the entire coordination phrase, disregarding the individual conjuncts. For example, in \u201cMy plan is to visit Seychelles, ko Samui and Sardinia by the end of the year\u201d the goal is to recover \u201cSeychelles, ko Samui and Sardinia\u201d. This is a recall measure. We follow the exact protocol of Hara et al. (2009) and train and evaluate the model on 3598 coordination phrases in Genia Treebank Beta and report the micro-averaged results of a five-fold cross validation run.", "startOffset": 8, "endOffset": 420}, {"referenceID": 8, "context": "by Hara et al. (2009), syntactic parsers do not perform well on the Genia treebank.", "startOffset": 3, "endOffset": 22}, {"referenceID": 8, "context": "Hogan (2007) incorporated this principle in a generative parsing model by changing the generative process of coordinated NPs to condition on properties of the first conjunct when generating the second one.", "startOffset": 0, "endOffset": 13}, {"referenceID": 8, "context": "Hogan (2007) incorporated this principle in a generative parsing model by changing the generative process of coordinated NPs to condition on properties of the first conjunct when generating the second one. Shimbo and Hara (2007) proposed a discriminative sequence alignment model to detect similar conjuncts.", "startOffset": 0, "endOffset": 229}, {"referenceID": 7, "context": "Their work was extended by Hara et al. (2009) to handle nested coordinations as well.", "startOffset": 27, "endOffset": 46}, {"referenceID": 7, "context": "Hanamoto et al. (2012) extended the previous method with dual decomposition and HPSG parsing.", "startOffset": 0, "endOffset": 23}, {"referenceID": 7, "context": "Hanamoto et al. (2012) extended the previous method with dual decomposition and HPSG parsing. In contrast to these symmetry-directed efforts, Kawahara et al. (2008) focuses on the dependency relations that surround the conjuncts.", "startOffset": 0, "endOffset": 165}], "year": 2016, "abstractText": "We propose a neural-network based model for coordination boundary prediction. The network is designed to incorporate two signals: the similarity between conjuncts and the observation that replacing the whole coordination phrase with a conjunct tends to produce a coherent sentences. The modeling makes use of several LSTM networks. The model is trained solely on conjunction annotations in a Treebank, without using external resources. We show improvements on predicting coordination boundaries on the PTB compared to two state-of-the-art parsers; as well as improvement over previous coordination boundary prediction systems on the Genia corpus.", "creator": "LaTeX with hyperref package"}}}