{"id": "1610.03950", "review": {"conference": "ACL", "VERSION": "v1", "DATE_OF_SUBMISSION": "13-Oct-2016", "title": "Compressing Neural Language Models by Sparse Word Representations", "abstract": "Neural networks are among the state-of-the-art techniques for language modeling. Existing neural language models typically map discrete words to distributed, dense vector representations. After information processing of the preceding context words by hidden layers, an output layer estimates the probability of the next word. Such approaches are time- and memory-intensive because of the large numbers of parameters for word embeddings and the output layer. In this paper, we propose to compress neural language models by sparse word representations. In the experiments, the number of parameters in our model increases very slowly with the growth of the vocabulary size, which is almost imperceptible. Moreover, our approach not only reduces the parameter space to a large extent, but also improves the performance in terms of the perplexity measure.", "histories": [["v1", "Thu, 13 Oct 2016 06:55:54 GMT  (2356kb,D)", "http://arxiv.org/abs/1610.03950v1", "ACL-16, pages 226--235"]], "COMMENTS": "ACL-16, pages 226--235", "reviews": [], "SUBJECTS": "cs.CL cs.LG", "authors": ["yunchuan chen", "lili mou", "yan xu", "ge li", "zhi jin"], "accepted": true, "id": "1610.03950"}, "pdf": {"name": "1610.03950.pdf", "metadata": {"source": "CRF", "title": "Compressing Neural Language Models by Sparse Word Representations", "authors": ["Yunchuan Chen", "Lili Mou", "Yan Xu", "Ge Li", "Zhi Jin"], "emails": ["chenyunchuan11@mails.ucas.ac.cn", "doublepower.mou@gmail.com,", "xuyan14@pku.edu.cn", "lige@pku.edu.cn", "zhijin@pku.edu.cn"], "sections": [{"heading": null, "text": "Existing neural language models typically map discrete words to distributed, dense vector representations. After processing the previous context words through hidden layers, an output layer estimates the likelihood of the next word. Such approaches are time-consuming and memory intensive due to the large number of parameters for word embedding and the output layer. In this paper, we propose to compress neural language models using sparse word representations. In the experiments, the number of parameters in our model increases very slowly with the almost imperceptible growth of the vocabulary size. In addition, our approach not only greatly reduces parameter space, but also improves performance in terms of perplexity measurement. 1"}, {"heading": "1 Introduction", "text": "This year it is more than ever before."}, {"heading": "2 Background", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "2.1 Standard Neural LMs", "text": "Language modeling aims to minimize the common probability of a corpus (Jurafsky and Martin, 2014). Traditional n-gram models impose a Markov assumption that a word depends only on previous n-1 words and regardless of its position. In estimating the parameters, researchers have proposed various smoothing techniques, including back-off models, to alleviate the problem of data economy. Bengio et al. (2003) propose to use a feedforward neural network (FFNN) to replace multinomic parameter estimation in n-gram models. Recurring neural networks (RNNNs) can also be used for speech modeling; they are particularly capable of capturing long ranges of dependencies in sentences (Mikolov et al., 2010; Sundermeyer etal., 2015) In the above models, we can see that a neural LM consists of three main parts, namely, encoding and coding."}, {"heading": "2.2 Complexity Concerns of Neural LMs", "text": "Neural network-based LMs can capture a more precise semantics of natural language than N-gram models because the regularity of the embedding subnet extracts meaningful semantics of a word and the high capacity of the encoding subnet allows complicated information processing. Nevertheless, neural LMs also suffer from several drawbacks, mainly due to complexity. Formation of neural LMs is typically time-consuming, especially when the vocabulary size is large. However, the normalization factor in Equation (1) contributes most to time complexity. Morin and Bengio (2005) suggest hierarchical softmax by using a Bayesian network so that the probability is self-normalized. Sampling techniques - for example meaning sampling (Bengio and Sene) contribute most to time complexity."}, {"heading": "2.3 Related Work", "text": "Jaderberg et al. (2014) compress neural models by matrix factorization, Gong et al. (2014) by quantization. In NLP, Mou et al. (2015a) learn to embed sub-spaces by supervised training. Our work is little, if any, similar to the above methods, since we compress embeddings and output weights with sparse word representations. Existing model compression typically works with a compromise of performance. On the contrary, our model improves perplexity measurement after compression. Sparse word representations. We use sparse word codes to compress neural LMs. Faruqui et al. (2015) suggest a sparse coding method to represent each word with a sparse diffector. Through sparse word representations, we can use sparse word codes to achieve a sparse result when we do not estimate the simulators as well as we estimate the compression of words."}, {"heading": "3 Our Proposed Model", "text": "In this section, we describe our compressed language model in detail. Section 3.1 formalizes the sparse representation of words that serves as the premise of our model. On this basis, we compress the subnets embedding and predicting in Section 3.2 and 3.3 respectively. Finally, Section 3.4 introduces NCE to parameter estimation, where we further propose the mechanism of ZRegression to stabilize our model."}, {"heading": "3.1 Sparse Representations of Words", "text": "The first subset B is a base set that contains a fixed number of common words (8k in our experiments). C = V / B is a set of unusual words, so we would like to use a few common words to represent this rare word. Our intuition is that a word can often be defined by a few other words, and that rare words should be defined by common words, so it is reasonable to use a few common words to represent the rare word. Most work in literature (Lee et al., 2006; Yang et al., 2011) represents each unusual word with a sparse, linear combination of embeddings. The sparse coefficients are called sparse coefficients for a particular word. We first have a word representation model like SkipGram (Mikolov et al., 2013) to obtain a set of embeddings for each word."}, {"heading": "3.2 Parameter Compression for the", "text": "The embedding of SubnetOne is the main source of LM parameters in the Embedding subnet, which uses a list of words (history / context) as input and prints dense, low-dimensional vector representations of the words. We use the sparse representation of the above-mentioned words to construct a compressed embedding subnet, where the number of parameters is independent of the vocabulary size. By solving the optimization problem (5) for each word, we obtain a non-negative, sparse code x-RB for each word that indicates the degree to which the word is related to ordinary words in B. Then the embedding of a word by w = Ux.2https: / / code.google.com / archive / p / word2vecWe would like to point out that embedding a word is not economical, as U is a dense matrix that serves as the common parameter for learning all the word vector settings."}, {"heading": "3.3 Parameter Compression for the", "text": "In fact, we will be able to retaliate, and we will be able to retaliate, \"he said in an interview with\" Welt am Sonntag. \""}, {"heading": "3.4 Noise-Contrastive Estimation with", "text": "Compared to the maximum probability estimate of softmax, NCE greatly reduces the computational complexity. We therefore propose the Zregression mechanism to stabilize the training. NCE generates a few negative samples for each positive data sample. In training, we only need to calculate the unnormalised probability of these positive and negative samples. Interesting readers refer to (Gutmann and Hyv\u00e4rinen, 2012) for more information. Formally, the estimated probability of the word wi with the history / context is h isP (w | h; h) = 1 Zh P 0 (wi; h; s) exp (s; s; h; s), where the probability of Z (10), where we use the parameters and Zh is a context-dependent normalization factor."}, {"heading": "4 Evaluation", "text": "In this section we first describe our data set in Section 4.1. We evaluate our learned sparse codes of rare words in Section 4.2 and the compressed language model in Section 4.3. Section 4.4 provides an in-depth analysis of the Zregression mechanism."}, {"heading": "4.1 Dataset", "text": "We used the freely available Wikipedia4 dump (2014) as a data set. We extracted simple sentences from the dump and removed all markups. Furthermore, we performed several steps of pre-processing, such as text normalization, sentence splitting and tokenization. Sentences were randomly mixed so that no information could be used across sentences, i.e. we did not consider cached language models. The resulting corpus contains about 1.6 billion running words. The corpus was divided into three parts for training, validation and testing. As it is typically time consuming to train neural networks, we stitched a subset of 100 million running words to train neural LMs, but the entire training set was used to train the backward n-gram models. We selected hyperparameters based on the validation set and reported on the model performance on the test set. Table 2 presents some statistics from our data set."}, {"heading": "4.2 Qualitative Analysis of Sparse Codes", "text": "To get the sparse codes of the words, we chose 8k ordinary words as a \"dictionary,\" i.e. B = 8000.4http: / / en.wikipedia.orgWe had 2k-42k unusual words in different settings. We first pre-trained word embedding of both rare and common words and got them in Equation (5) 200d vectors U and W. The dimension was set in advance and not coordinated. As there is no analytical solution for the target, we optimized it by Adam (Kingma and Ba, 2014), which is a gradient-based method. To filter out small coefficients around zero, we simply set a value to 0 if it is less than 0.015 \u00b7 max {v \u00b2 x}. W\u03b1 in Equation (6) the word was set to 1 because we considered the matching loss and the spareness penalty to be equally important. We set w\u03b2 in Equation (7) to 0.1, and this hyperparameter is insensitive."}, {"heading": "4.3 Quantitative Analysis of Compressed Language Models", "text": "We then used the pre-calculated sparse codes to compress neural LMs, allowing a quantitative analysis of the learned sparse word representations. We take perplexity as a performance measurement of a language model defined by PPL = 2 \u2212 1 N \u2211 N i = 1 log2p (wi | hi), where N is the number of running words in the test corpus."}, {"heading": "4.3.1 Settings", "text": "We used LSTM-RNN as a subnet for encoding, which is a dominant class of neural networks for speech modeling (Sundermeyer et al., 2015; Karpathy et al., 2015).The hidden layer was 200d. We used the Adam algorithm to train our neural models.The learning rate was selected by validation from {0.001, 0.002, 0.004, 0.008}.The parameters were updated with the same predefined hyperparameters or tuned to the same candidate set, so our comparison is fair.We list our compressed LMs and competing methods as follows: \u2022 KN3. We used the modified LCE variant and the baseline theory form STM; we used the STRano word and the compressed LMs and the competing methods as follows."}, {"heading": "4.3.2 Performance", "text": "This year it is more than ever before."}, {"heading": "5 Conclusion", "text": "In this paper, we proposed an approach to presenting rare words with sparse linear combinations of common words. Based on such combinations, we succeeded in compressing an LSTM language model (LM) in which memory does not increase with vocabulary size except for a bias and a sparse code for each word. Furthermore, our experimental results show that the compressed LM performed better than the uncompressed basic LM."}, {"heading": "Acknowledgments", "text": "This research is supported by China's National Basic Research Programme (the 973 Programme) under grant numbers 2015CB352201, China's National Science Foundation under grant numbers 61232015, 91318301, 61421091 and 61502014, and China Post-Doctoral Foundation under grant numbers 2015M580927."}], "references": [{"title": "When and why are log-linear models self-normalizing", "author": ["Andreas", "Klein2014] Jacob Andreas", "Dan Klein"], "venue": "In Proceedings of the Annual Meeting of the North American Chapter of the Association", "citeRegEx": "Andreas et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Andreas et al\\.", "year": 2014}, {"title": "Quick training of probabilistic neural nets by importance sampling", "author": ["Bengio", "Sen\u00e9cal2003] Yoshua Bengio", "JeanS\u00e9bastien Sen\u00e9cal"], "venue": "In Proceedings of the Ninth International Workshop on Artificial Intelligence and Statistics", "citeRegEx": "Bengio et al\\.,? \\Q2003\\E", "shortCiteRegEx": "Bengio et al\\.", "year": 2003}, {"title": "A neural probabilistic language model", "author": ["Bengio et al.2003] Yoshua Bengio", "R\u00e9jean Ducharme", "Pascal Vincent", "Christian Jauvin"], "venue": "The Journal of Machine Learning Research,", "citeRegEx": "Bengio et al\\.,? \\Q2003\\E", "shortCiteRegEx": "Bengio et al\\.", "year": 2003}, {"title": "Strategies for training large vocabulary neural language models. arXiv preprint arXiv:1512.04906", "author": ["Chen et al.2015] Welin Chen", "David Grangier", "Michael Auli"], "venue": null, "citeRegEx": "Chen et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Chen et al\\.", "year": 2015}, {"title": "Fast and robust neural network joint models for statistical machine", "author": ["Devlin et al.2014] Jacob Devlin", "Rabih Zbib", "Zhongqiang Huang", "Thomas Lamar", "Richard M Schwartz", "John Makhoul"], "venue": null, "citeRegEx": "Devlin et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Devlin et al\\.", "year": 2014}, {"title": "Sparse overcomplete word vector representations", "author": ["Yulia Tsvetkov", "Dani Yogatama", "Chris Dyer", "Noah A. Smith"], "venue": "In Proceedings of the 53rd Annual Meeting of the Association for Computational Linguis-", "citeRegEx": "Faruqui et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Faruqui et al\\.", "year": 2015}, {"title": "Compressing deep convolutional networks using vector quantization", "author": ["Gong et al.2014] Yunchao Gong", "Liu Liu", "Ming Yang", "Lubomir Bourdev"], "venue": "arXiv preprint arXiv:1412.6115", "citeRegEx": "Gong et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Gong et al\\.", "year": 2014}, {"title": "Noise-contrastive estimation of unnormalized statistical models, with applications to natural image statistics", "author": ["Gutmann", "Hyv\u00e4rinen2012] Michael Gutmann", "Aapo Hyv\u00e4rinen"], "venue": "The Journal of Machine Learning Research,", "citeRegEx": "Gutmann et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Gutmann et al\\.", "year": 2012}, {"title": "Document summarization based on data reconstruction", "author": ["He et al.2012] Zhanying He", "Chun Chen", "Jiajun Bu", "Can Wang", "Lijun Zhang", "Deng Cai", "Xiaofei He"], "venue": "In Proceedings of the 26th AAAI Conference on Artificial Intelligence,", "citeRegEx": "He et al\\.,? \\Q2012\\E", "shortCiteRegEx": "He et al\\.", "year": 2012}, {"title": "Distilling the knowledge in a neural network. arXiv preprint arXiv:1503.02531", "author": ["Oriol Vinyals", "Jeff Dean"], "venue": null, "citeRegEx": "Hinton et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Hinton et al\\.", "year": 2015}, {"title": "Speeding up convolutional neural networks with low rank expansions", "author": ["Andrea Vedaldi", "Andrew Zisserman"], "venue": "In Proceedings of the British Machine Vision Conference", "citeRegEx": "Jaderberg et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Jaderberg et al\\.", "year": 2014}, {"title": "On using very large target vocabulary for neural machine translation", "author": ["Jean et al.2014] S\u00e9bastien Jean", "Kyunghyun Cho", "Roland Memisevic", "Yoshua Bengio"], "venue": "arXiv preprint arXiv:1412.2007", "citeRegEx": "Jean et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Jean et al\\.", "year": 2014}, {"title": "Visualizing and understanding recurrent networks. arXiv preprint arXiv:1506.02078", "author": ["Justin Johnson", "Fei-Fei Li"], "venue": null, "citeRegEx": "Karpathy et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Karpathy et al\\.", "year": 2015}, {"title": "Adam: A method for stochastic optimization. arXiv preprint arXiv:1412.6980", "author": ["Kingma", "Ba2014] Diederik P Kingma", "Jimmy Ba"], "venue": null, "citeRegEx": "Kingma et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Kingma et al\\.", "year": 2014}, {"title": "Efficient sparse coding algorithms", "author": ["Lee et al.2006] Honglak Lee", "Alexis Battle", "Rajat Raina", "Andrew Y Ng"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Lee et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Lee et al\\.", "year": 2006}, {"title": "Linguistic regularities in sparse and explicit word representations", "author": ["Levy", "Goldberg2014] Omer Levy", "Yoav Goldberg"], "venue": "In Proceedings of the Eighteenth Conference on Natural Language Learning,", "citeRegEx": "Levy et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Levy et al\\.", "year": 2014}, {"title": "Recurrent neural network based language model", "author": ["Martin Karafi\u00e1t", "Lukas Burget", "Jan Cernock\u1ef3", "Sanjeev Khudanpur"], "venue": "In INTERSPEECH,", "citeRegEx": "Mikolov et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Mikolov et al\\.", "year": 2010}, {"title": "Strategies for training large scale neural network language models", "author": ["Anoop Deoras", "Daniel Povey", "Lukas Burget", "Jan Cernock\u00fd"], "venue": "In Proceedings of the IEEE Workshop on Automatic Speech Recognition", "citeRegEx": "Mikolov et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Mikolov et al\\.", "year": 2011}, {"title": "Efficient estimation of word representations in vector space. arXiv preprint arXiv:1301.3781", "author": ["Kai Chen", "Greg Corrado", "Jeffrey Dean"], "venue": null, "citeRegEx": "Mikolov et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Mikolov et al\\.", "year": 2013}, {"title": "Three new graphical models for statistical language modelling", "author": ["Mnih", "Hinton2007] Andriy Mnih", "Geoffrey Hinton"], "venue": "In Proceedings of the 24th International Conference on Machine learning,", "citeRegEx": "Mnih et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Mnih et al\\.", "year": 2007}, {"title": "A fast and simple algorithm for training neural probabilistic language models. arXiv preprint arXiv:1206.6426", "author": ["Mnih", "Teh2012] Andriy Mnih", "Yee-Whye Teh"], "venue": null, "citeRegEx": "Mnih et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Mnih et al\\.", "year": 2012}, {"title": "Hierarchical probabilistic neural network language model", "author": ["Morin", "Bengio2005] Fr\u00e9deric Morin", "Yoshua Bengio"], "venue": "In Proceedings of the International Workshop on Artificial Intelligence and Statistics,", "citeRegEx": "Morin et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Morin et al\\.", "year": 2005}, {"title": "Distilling word embeddings: An encoding approach", "author": ["Mou et al.2015a] Lili Mou", "Ge Li", "Yan Xu", "Lu Zhang", "Zhi Jin"], "venue": "arXiv preprint arXiv:1506.04488", "citeRegEx": "Mou et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Mou et al\\.", "year": 2015}, {"title": "2015b. Backward and forward language modeling for constrained natural language generation", "author": ["Mou et al.2015b] Lili Mou", "Rui Yan", "Ge Li", "Lu Zhang", "Zhi Jin"], "venue": "arXiv preprint arXiv:1512.06612", "citeRegEx": "Mou et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Mou et al\\.", "year": 2015}, {"title": "A neural attention model for abstractive sentence summarization", "author": ["Sumit Chopra", "Jason Weston"], "venue": "In Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing,", "citeRegEx": "Rush et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Rush et al\\.", "year": 2015}, {"title": "A neural network approach to context-sensitive generation", "author": ["Michel Galley", "Michael Auli", "Chris Brockett", "Yangfeng Ji", "Margaret Mitchell", "Jian-Yun Nie", "Jianfeng Gao", "Bill Dolan"], "venue": null, "citeRegEx": "Sordoni et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Sordoni et al\\.", "year": 2015}, {"title": "SRILM\u2014An extensible language modeling toolkit", "author": ["Andreas Stolcke"], "venue": "In INTERSPEECH,", "citeRegEx": "Stolcke,? \\Q2002\\E", "shortCiteRegEx": "Stolcke", "year": 2002}, {"title": "From feedforward to recurrent LSTM neural networks for language modeling", "author": ["Hermann Ney", "Ralf Schl\u00fcter"], "venue": "IEEE/ACM Transactions on Audio, Speech and Language Processing,", "citeRegEx": "Sundermeyer et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Sundermeyer et al\\.", "year": 2015}, {"title": "Robust sparse coding for face recognition", "author": ["Yang et al.2011] Meng Yang", "Lei Zhang", "Jian Yang", "David Zhang"], "venue": "In Proceedings of the 2011 IEEE Conference on Computer Vision and Pattern Recognition,", "citeRegEx": "Yang et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Yang et al\\.", "year": 2011}], "referenceMentions": [{"referenceID": 17, "context": "In recent years, neural network-based LMs have achieved significant breakthroughs: they can model language more precisely than traditional n-gram statistics (Mikolov et al., 2011); it is even possible to generate new sentences from a neural LM, benefiting various downstream tasks like machine translation, summarization, and dialogue systems (Devlin et al.", "startOffset": 157, "endOffset": 179}, {"referenceID": 4, "context": ", 2011); it is even possible to generate new sentences from a neural LM, benefiting various downstream tasks like machine translation, summarization, and dialogue systems (Devlin et al., 2014; Rush et al., 2015; Sordoni et al., 2015; Mou et al., 2015b).", "startOffset": 171, "endOffset": 252}, {"referenceID": 24, "context": ", 2011); it is even possible to generate new sentences from a neural LM, benefiting various downstream tasks like machine translation, summarization, and dialogue systems (Devlin et al., 2014; Rush et al., 2015; Sordoni et al., 2015; Mou et al., 2015b).", "startOffset": 171, "endOffset": 252}, {"referenceID": 25, "context": ", 2011); it is even possible to generate new sentences from a neural LM, benefiting various downstream tasks like machine translation, summarization, and dialogue systems (Devlin et al., 2014; Rush et al., 2015; Sordoni et al., 2015; Mou et al., 2015b).", "startOffset": 171, "endOffset": 252}, {"referenceID": 9, "context": "First, with a wider application of neural networks in resourcerestricted systems (Hinton et al., 2015), such approach is too memory-consuming and may fail to be deployed in mobile phones or embedded systems.", "startOffset": 81, "endOffset": 102}, {"referenceID": 18, "context": "The Embedding subnet maps a word to a dense vector, representing some abstract features of the word (Mikolov et al., 2013).", "startOffset": 100, "endOffset": 122}, {"referenceID": 1, "context": "We may either leverage FFNNs (Bengio et al., 2003) or RNNs (Mikolov et al.", "startOffset": 29, "endOffset": 50}, {"referenceID": 16, "context": ", 2003) or RNNs (Mikolov et al., 2010) as the Encoding subnet, but RNNs typically yield a better performance (Sundermeyer et al.", "startOffset": 16, "endOffset": 38}, {"referenceID": 27, "context": ", 2010) as the Encoding subnet, but RNNs typically yield a better performance (Sundermeyer et al., 2015).", "startOffset": 78, "endOffset": 104}, {"referenceID": 11, "context": "Sampling techniques\u2014for example, importance sampling (Bengio and Sen\u00e9cal, 2003), noise-contrastive estimation (Gutmann and Hyv\u00e4rinen, 2012), and target sampling (Jean et al., 2014)\u2014are applied to avoid computation over the entire vocabulary.", "startOffset": 161, "endOffset": 180}, {"referenceID": 3, "context": "Chen et al. (2015) propose the differentiated softmax model by assigning fewer parameters to rare words than to frequent words.", "startOffset": 0, "endOffset": 19}, {"referenceID": 8, "context": "(2006) and Hinton et al. (2015) use a well-trained large network to guide the training of a small network for model compression.", "startOffset": 11, "endOffset": 32}, {"referenceID": 8, "context": "(2006) and Hinton et al. (2015) use a well-trained large network to guide the training of a small network for model compression. Jaderberg et al. (2014) compress neural models by matrix factorization, Gong et al.", "startOffset": 11, "endOffset": 153}, {"referenceID": 6, "context": "(2014) compress neural models by matrix factorization, Gong et al. (2014) by quantization.", "startOffset": 55, "endOffset": 74}, {"referenceID": 6, "context": "(2014) compress neural models by matrix factorization, Gong et al. (2014) by quantization. In NLP, Mou et al. (2015a) learn an embedding subspace by supervised training.", "startOffset": 55, "endOffset": 118}, {"referenceID": 5, "context": "Faruqui et al. (2015) propose a sparse coding method to represent each word with a sparse vector.", "startOffset": 0, "endOffset": 22}, {"referenceID": 14, "context": "Following most work in the literature (Lee et al., 2006; Yang et al., 2011), we represent each uncommon word with a sparse, linear combination of com-", "startOffset": 38, "endOffset": 75}, {"referenceID": 28, "context": "Following most work in the literature (Lee et al., 2006; Yang et al., 2011), we represent each uncommon word with a sparse, linear combination of com-", "startOffset": 38, "endOffset": 75}, {"referenceID": 18, "context": "We first train a word representation model like SkipGram (Mikolov et al., 2013) to obtain a set of embeddings for each word in the vocabulary, including both common words and rare words.", "startOffset": 57, "endOffset": 79}, {"referenceID": 8, "context": "The nonnegative regularizer is applied as in He et al. (2012) due to psychological interpretation concerns.", "startOffset": 45, "endOffset": 62}, {"referenceID": 27, "context": "We leveraged LSTM-RNN as the Encoding subnet, which is a prevailing class of neural networks for language modeling (Sundermeyer et al., 2015; Karpathy et al., 2015).", "startOffset": 115, "endOffset": 164}, {"referenceID": 12, "context": "We leveraged LSTM-RNN as the Encoding subnet, which is a prevailing class of neural networks for language modeling (Sundermeyer et al., 2015; Karpathy et al., 2015).", "startOffset": 115, "endOffset": 164}, {"referenceID": 25, "context": "We adopted the modified Kneser-Ney smoothing technique to train a 3-gram LM; we used the SRILM toolkit (Stolcke and others, 2002) in out experiment. \u2022 LBL5. A Log-BiLinear model introduced in Mnih and Hinton (2007). We used 5 preceding words as context.", "startOffset": 104, "endOffset": 215}, {"referenceID": 25, "context": "We adopted the modified Kneser-Ney smoothing technique to train a 3-gram LM; we used the SRILM toolkit (Stolcke and others, 2002) in out experiment. \u2022 LBL5. A Log-BiLinear model introduced in Mnih and Hinton (2007). We used 5 preceding words as context. \u2022 LSTM-s. A standard LSTM-RNN language model which is applied in Sundermeyer et al. (2015) and Karpathy et al.", "startOffset": 104, "endOffset": 345}, {"referenceID": 12, "context": "(2015) and Karpathy et al. (2015). We implemented the LM ourselves based on Theano (Theano Development Team, 2016) and also used NCE for training.", "startOffset": 11, "endOffset": 34}], "year": 2016, "abstractText": "Neural networks are among the state-ofthe-art techniques for language modeling. Existing neural language models typically map discrete words to distributed, dense vector representations. After information processing of the preceding context words by hidden layers, an output layer estimates the probability of the next word. Such approaches are timeand memory-intensive because of the large numbers of parameters for word embeddings and the output layer. In this paper, we propose to compress neural language models by sparse word representations. In the experiments, the number of parameters in our model increases very slowly with the growth of the vocabulary size, which is almost imperceptible. Moreover, our approach not only reduces the parameter space to a large extent, but also improves the performance in terms of the perplexity measure.1", "creator": "LaTeX with hyperref package"}}}