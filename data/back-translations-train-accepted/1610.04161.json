{"id": "1610.04161", "review": {"conference": "iclr", "VERSION": "v1", "DATE_OF_SUBMISSION": "13-Oct-2016", "title": "Why Deep Neural Networks for Function Approximation?", "abstract": "Recently there has been much interest in understanding why deep neural networks are preferred to shallow networks. In this paper, we show that, for a large class of piecewise smooth functions, the number of neurons needed by a shallow network to approximate a function is exponentially larger than the corresponding number of neurons needed by a deep network for a given degree of function approximation. First, we consider univariate functions on a bounded interval and require a neural network to achieve an approximation error of $\\varepsilon$ uniformly over the interval. We show that shallow networks (i.e., networks whose depth does not depend on $\\varepsilon$) require $\\Omega(\\text{poly}(1/\\varepsilon))$ neurons while deep networks (i.e., networks whose depth grows with $1/\\varepsilon$) require $\\mathcal{O}(\\text{polylog}(1/\\varepsilon))$ neurons. We then extend these results to certain classes of important multivariate functions. Our results are derived for neural networks which use a combination of rectifier linear units (ReLUs) and binary step units, two of the most popular type of activation functions. Our analysis builds on this simple observation that the binary approximation of a real number in the interval $[0,1]$ can be represented by a deep neural network which uses a \"small\" number of neurons.", "histories": [["v1", "Thu, 13 Oct 2016 16:34:30 GMT  (340kb,D)", "http://arxiv.org/abs/1610.04161v1", null], ["v2", "Fri, 3 Mar 2017 20:43:04 GMT  (628kb,D)", "http://arxiv.org/abs/1610.04161v2", "The paper is published at the 5th International Conference on Learning Representations (ICLR)"]], "reviews": [], "SUBJECTS": "cs.LG cs.NE", "authors": ["shiyu liang", "r srikant"], "accepted": true, "id": "1610.04161"}, "pdf": {"name": "1610.04161.pdf", "metadata": {"source": "CRF", "title": "Why Deep Neural Networks?", "authors": ["Shiyu Liang"], "emails": [], "sections": [{"heading": null, "text": "In this paper, we show that the number of neurons that a flat network requires to approach a function is exponentially greater than the corresponding number of neurons that a deep network requires to approach a function. First, we look at univariate functions at a limited interval and need a neural network to achieve an approximation error of \u03b5 uniformly over the interval. We show that flat networks (i.e. networks whose depth does not depend on \u03b5) require neurons, while deep networks (i.e. networks whose depth grows with 1 / \u03b5) require type O neurons (polylog (1 / \u03b5). We then extend these results to certain classes of important multivariate functions. Our results are derived for neural networks that have a combination of linear units of the rectifier (ReLUs) and binary step units (Binary Step Units), which represent a very popular number of functions that are most common in the \"deep interval formation.\""}, {"heading": "1 Introduction", "text": "Neural networks have aroused considerable interest from the machine learning community, particularly due to their recent empirical successes (see the surveys (Bengio, 2009).Neural networks are used to build state-of-the-art systems in various applications, such as image cognition, speech recognition, natural language process, and others (see Krizhevsky et al. 2012; Goodfellow et al. 2013; Wan et al. 2013, for example).The finding that neural networks are universal approximators is one of the most frequently cited theoretical results to justify the use of neural networks in these applications. Countless results have shown that the universal approximation capability of neural networks with a hidden layer in approximations of different functional classes (see e.g. Cybenko 1989; Hornik et al. 1989; Chui and Li 1992; Barron 1993).All of these results and many others provide upper limits of network size and claim that small network errors can be achieved if the approximation size is sufficient."}, {"heading": "2 Preliminaries and problem statement", "text": "In this section we present definitions of feedforward neural networks and formally present the problem."}, {"heading": "2.1 Feedforward Neural Networks", "text": "A forward-facing neural network is composed of layers of computational units and defines a unique function f: Rd \u2192 R. Let L denote the number of hidden layers, Nl the number of units of layer l, vector x = (x (1),..., x (d))) the input of the neural mesh, zlj the output of the jth unit in layer l, w l i, j the weight of the connecting edge unit i in layer l and unit j in layer l + 1, blj the preload of the unit j in layer. Then the output between the layers of the forward neural mesh can be characterized by the following iterations: zl + 1j = planning depth (Nl = 1wli, jz l i + b l + 1 j), l [L \u2212 1], j [Nl + 1], with input layer: z1w0i = 1w0i."}, {"heading": "2.2 Problem Statement", "text": "In this paper, we focus on limits to the size of the feed network function approximation. Specifically, we aim to answer the following two questions: Given a function f with continuous derivatives of sufficient order and an error limit of \u03b5 > 0, if we use a multi-layer neural network f \u00b2 of depth L and size N, then 1 there are upper limits for L (\u03b5) and N (\u03b5) such as this limit for F (N, L), f (x), f \u00b2 (x) \u2212 f \u00b2 (x) (x). Is there a lower limit for N (L, \u03b5) for a fixed depth L such as this subliminal limit f \u00b2 F (N, L), f \u00b2 f (x) \u2212 f \u00b2 (x)? The first question is what depth and size are sufficient to guarantee a proximation? The second question asks for a fixed depth L as the minimum size of a neural network is required to guarantee an approximation."}, {"heading": "3 Upper bounds on function approximations", "text": "In this section we will introduce upper limits of the size of the multilayered neural mesh that are sufficient to approximate the function. Before we give the results, some notations and terms merit further explanation. First, the upper limit of the network size represents the number of neurons that are most needed to approximate a given function with a certain error. Second, the concept of approximation is the L \u221e distance: for two functions f and g, the L \u00b2 distance between these two functions is the maximum point of disagreement over the cube [0, 1]."}, {"heading": "3.1 Approximation of univariate functions", "text": "In this section we present all the results of the approach to univariate functions. First, we present a theory of the size of the mesh to approximate a simple square function. As part of the proof, we present the structure of the multilayer feedforward neural network and show how the neural network parameters are selected. Results for the approximation of general functions can be found in Theorem 2 and Theorem 1. For the function f (x) = x2, x [0, 1] there is a multilayer neural network f (x) with O (log 1\u03b5) layers, O (log 1\u03b5) binary steps and O (log 1\u03b5) rectifier linear units, so that there is f (x) \u2212 f (x) a multilayer neural network f (x).Proof. The proof consists of three parts. For each x [0, 1] we use the multilayer neural network for approximation x."}, {"heading": "3.2 Approximation of multivariate functions", "text": "First we present a theorem on the upper limit of neural network size (the upper limit of neural network size) and finally we show the upper limit of neural network size (the upper limit of neural network size) for the upper limit of linear combination, multiplication and composition of multivariate functions with sufficient smoothness.Theorem 8: Let W = {w, Rd = 1}. Let W = {s, Rd = 1}."}, {"heading": "4 Lower bounds on function approximations", "text": "This section introduces lower network size boundaries in function for certain function classes. Next, by combining the lower boundaries and upper boundaries shown in the previous section, we can analytically show the benefits of lower neural networks versus flatter units. \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # #"}, {"heading": "5 Conclusions", "text": "In this paper, we demonstrated that an exponentially large number of neurons is required for functional approximation using flat networks compared to deep networks. Results were generated for a large class of smooth univariate and multivariate functions, and our results were obtained for the case of feedback-forward neural networks with ReLUs and binary step units."}, {"heading": "Appendix A Proof of Corollary 5", "text": "Proof: According to theorem 4, for each hi, i = 1,..., k, there is a multilayer neural network h = 1, i (x).Then the approximation error is limited by | f (x) \u2212 f (x).Now let us calculate the size of the multilayer neural network f (x).Let us calculate N = 2, i = 0 xi 2i be the binary extension of x. Since h (x) \u2212 h (x) \u2212 h (x) | h (x).Now let us calculate the size of the multilayer neural network f (x).Let N = 2, i = 0 xi 2i be the binary extension of x."}, {"heading": "Appendix B Proof of Corollary 6", "text": "Da f (x) = h1 (x) h2 (x) h2 (x)... hk (x), then the derivative of the f (n) order n isf (n) = h (n) k (n) k (n) k (n) k (n) k (n) k (n) k (n) k (n) k (n) k (n) k (n) k (n) k (n) k (n) k (n) k (n) k (n) k (n) k (n) k (n) k (n) k (n) k (n) k (n) k (n) k (n) k (n) k (n) k (n) k (n) k (n) k n n n n n n n (n) k k k k k n n (n) k k k k k k n (n) k k k) k (n) k n (n) k n (n) k n (n) k n (n) k) k k k k k n (n) k) k k k k k k k k n (n) k) k (n (n) k) k) k (n (n) k) k n (n) k n (n) k) k n (n (n) k) k) k n (n (n) k) k n (n) k) k) k) k k k k k k k k k k k k k k k k k k k k n n (n n n n n (n) k) k k) k n (n) k) k n (n) k) k n (n) k) k n (n) k (n) k n) k (n) k (n) k n n n) k (n) k (n) k n) k (n) k (n) k n) k (n) k (n) k) k (n) k (n) k) k (n) k) k (n) k) k n n n n n n) k (n) k (n n) k (n) k) k) k (n) k) k) k (n) k) k (n n n n n) k (n) k"}, {"heading": "Appendix C Proof of Corollary 7", "text": "The evidence that we are considering the cases for Fm-1 is not provided. \u2212 m (1) definition function Fm = h1 \u00d7 m (1) definition function Fm = h1 \u00d7 m (1) definition function Fm = h1 \u00d7 m (1) definition definition function Fm = h1 definition definition definition definition definition definition definition definition definition definition definition definition definition definition definition function (1) (1) definition definition function Fm (1) (1) definition definition function definition definition definition definition definition function (1) (1) definition definition definition definition function definition definition definition definition definition definition definition definition function (1) (1) definition definition definition definition definition definition definition definition definition definition definition definition definition definition definition definition definition function (1) (1) (1) (1) definition definition definition function Fm (1) (1) definition definition function (1) definition definition function (definition definition definition definition definition definition definition function) (1) (1) (definition definition definition definition function) function (1) (definition definition definition function), (1), (1) definition definition definition function (1), definition function (1) (definition definition definition definition definition definition function (1), definition function (1), definition function (1), definition definition function (1), definition definition function (1), definition definition function (1), definition definition function (1), definition function (1), definition function (1), definition definition definition function (1), definition function (1), definition function (1), definition function (1), (1), definition definition function (definition definition definition function), (1), (1), (1), definition function (1), definition function (1), (definition function), (1), (definition definition definition function), (1)."}, {"heading": "Appendix D Proof of Theorem 8", "text": "The proof consists of two parts. As before, we first use the deep structure shown in Figure 1 (gl = gl = gl = 1) to find the deep structure shown in Figure 1 to find the binary expansion for each x (k), k), k (k) = 0 x (k) and wi = 1 (wi1,..., wid). We could now use the deep structure shown in Figure 1 to find the binary expansion for each x (k), k (k) = 0 x (k) = 0 x (k), r 2r denote the binary expansion of x (k) r is the rth bit in the binary expansion of x (k). Obviously, we can all n-bit binary expansions of all x (k), k), k (d), k (d), we need a multi-layered neural network with n layers and dn binary units in the entirety."}, {"heading": "Appendix E Proof of Theorem 9", "text": "Proof: For each multinomic function g with multi-index \u03b1, g\u03b1 (x) = x\u03b1 it follows from theorem 4 that a deep neural network g-\u03b1 of size O (| \u03b1 | log | \u03b1 | d\u03b5) and depth O (| \u03b1 | + log | \u03b1 | d\u03b5) exists, so that the deep neural network g-\u03b1 (x) | \u2264 \u03b5.If we leave the deep neural network bef-x (x) = \u2211 \u03b1: | \u03b1 | \u2264 p C\u03b1g-\u03b1 (x) and thus | f (x) \u2212 f-f-\u03b1: | \u2264 p | C\u03b1 | \u00b7 g\u03b1 (x) \u2212 g-\u03b1 (x) | = \u03b5.Since the total number of the multinomic network is limited by p (p + d \u2212 1 d \u2212 1), the size of the deep neural network is thus upper limit (p + d \u2212 1 d \u2212 1). (8) If the total size of the multinomic network is limited by p (ond \u2212 1), then the neural network is limited by p \u00b2 and the order \u2212 2 (p \u00b2)."}, {"heading": "Appendix F Proof of Corollary 11", "text": "Theorem 4 shows that there are d multilayer neural networks g (1) (x (1)),..., g (x (d)) without layers and O (d) without layers and O (d) with binary steps and O (d) with linear units as a whole. (9) It follows that there is a deep neural network f (0, 1) with layers O (log 1) +... + g (log 1) with binary steps and O ((log 1) 2) so that | e \u2212 dx \u2212 d (f) with layers O (log 1) and O (log 1) with two layers and O (log 1) 2 so that | e \u2212 dx \u2212 f \u2212 f (f), f \u2212 f \u2212 f \u2212 d \u2212 d = layers and f \u2212 d \u2212 d (1) with two layers and f \u2212 d \u2212 d \u2212 d \u2212 d (1) with two layers."}, {"heading": "Appendix G Proof of Corollary 12", "text": "From theorem 4 it follows that there is then a multi-layer neural network with O (log 1\u03b5) layers, O (log 1\u03b5) binary step units and O (log 1\u03b5) 2) linear rectifier units, so that | g (t) \u2212 g (t) | \u2264 \u03b5 [0, 1]. If we define the deep network f (log 1\u03b5) = g (t), then the approximation error of f (x) \u2212 f (x) \u2212 g (t) \u2212 g (t). Now we have proved the logical sequence."}], "references": [{"title": "Learning polynomials with neural networks", "author": ["A. Andoni", "R. Panigrahy", "G. Valiant", "L. Zhang"], "venue": "In ICML,", "citeRegEx": "Andoni et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Andoni et al\\.", "year": 2014}, {"title": "Universal approximation bounds for superpositions of a sigmoidal function", "author": ["A.R. Barron"], "venue": "IEEE Transactions on Information theory,", "citeRegEx": "Barron.,? \\Q1993\\E", "shortCiteRegEx": "Barron.", "year": 1993}, {"title": "Learning deep architectures for ai", "author": ["Y. Bengio"], "venue": "Foundations and trends in Machine Learning,", "citeRegEx": "Bengio.,? \\Q2009\\E", "shortCiteRegEx": "Bengio.", "year": 2009}, {"title": "Approximation by ridge functions and neural networks with one hidden layer", "author": ["C.K. Chui", "X. Li"], "venue": "Journal of Approximation Theory,", "citeRegEx": "Chui and Li.,? \\Q1992\\E", "shortCiteRegEx": "Chui and Li.", "year": 1992}, {"title": "Approximation by superpositions of a sigmoidal function", "author": ["G. Cybenko"], "venue": "Mathematics of control, signals and systems,", "citeRegEx": "Cybenko.,? \\Q1989\\E", "shortCiteRegEx": "Cybenko.", "year": 1989}, {"title": "Shallow vs. deep sumproduct networks", "author": ["O. Delalleau", "Y. Bengio"], "venue": "In NIPS,", "citeRegEx": "Delalleau and Bengio.,? \\Q2011\\E", "shortCiteRegEx": "Delalleau and Bengio.", "year": 2011}, {"title": "The power of depth for feedforward neural networks", "author": ["R. Eldan", "O. Shamir"], "venue": "arXiv preprint arXiv:1512.03965,", "citeRegEx": "Eldan and Shamir.,? \\Q2015\\E", "shortCiteRegEx": "Eldan and Shamir.", "year": 2015}, {"title": "Numerical methods for special functions", "author": ["A. Gil", "J. Segura", "N.M. Temme"], "venue": null, "citeRegEx": "Gil et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Gil et al\\.", "year": 2007}, {"title": "Approximation capabilities of multilayer feedforward networks", "author": ["K. Hornik"], "venue": "Neural networks,", "citeRegEx": "Hornik.,? \\Q1991\\E", "shortCiteRegEx": "Hornik.", "year": 1991}, {"title": "Multilayer feedforward networks are universal approximators", "author": ["K. Hornik", "M. Stinchcombe", "H. White"], "venue": "Neural networks,", "citeRegEx": "Hornik et al\\.,? \\Q1989\\E", "shortCiteRegEx": "Hornik et al\\.", "year": 1989}, {"title": "Imagenet classification with deep convolutional neural networks", "author": ["A. Krizhevsky", "I. Sutskever", "G.E. Hinton"], "venue": "In NIPS,", "citeRegEx": "Krizhevsky et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Krizhevsky et al\\.", "year": 2012}, {"title": "Benefits of depth in neural networks", "author": ["M. Telgarsky"], "venue": "arXiv preprint arXiv:1602.04485,", "citeRegEx": "Telgarsky.,? \\Q2016\\E", "shortCiteRegEx": "Telgarsky.", "year": 2016}, {"title": "Regularization of neural networks using dropconnect", "author": ["L. Wan", "M. Zeiler", "S. Zhang", "Y. LeCun", "R. Fergus"], "venue": "In ICML,", "citeRegEx": "Wan et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Wan et al\\.", "year": 2013}], "referenceMentions": [{"referenceID": 2, "context": "Neural networks have drawn significant interest from the machine learning community, especially due to their recent empirical successes (see the surveys (Bengio, 2009)).", "startOffset": 153, "endOffset": 167}, {"referenceID": 9, "context": "Numerous results have shown the universal approximation property of neural networks with one hidden layer in approximations of different function classes, (see, e.g., Cybenko 1989; Hornik et al. 1989; Hornik 1991; Chui and Li 1992; Barron 1993).", "startOffset": 155, "endOffset": 244}, {"referenceID": 1, "context": "1989; Hornik 1991; Chui and Li 1992; Barron 1993). All these results and many others provide upper bounds on the network size and assert that small approximation error can be achieved if the network size is sufficiently large. More recently, there has been much interest in understanding the approximation capabilities of deep versus shallow networks. Delalleau and Bengio (2011) and Telgarsky (2016) have shown that there exist deep networks which cannot be approximated by shallow networks unless they use an exponentially larger amount of units or neurons.", "startOffset": 37, "endOffset": 380}, {"referenceID": 1, "context": "1989; Hornik 1991; Chui and Li 1992; Barron 1993). All these results and many others provide upper bounds on the network size and assert that small approximation error can be achieved if the network size is sufficiently large. More recently, there has been much interest in understanding the approximation capabilities of deep versus shallow networks. Delalleau and Bengio (2011) and Telgarsky (2016) have shown that there exist deep networks which cannot be approximated by shallow networks unless they use an exponentially larger amount of units or neurons.", "startOffset": 37, "endOffset": 401}, {"referenceID": 1, "context": "1989; Hornik 1991; Chui and Li 1992; Barron 1993). All these results and many others provide upper bounds on the network size and assert that small approximation error can be achieved if the network size is sufficiently large. More recently, there has been much interest in understanding the approximation capabilities of deep versus shallow networks. Delalleau and Bengio (2011) and Telgarsky (2016) have shown that there exist deep networks which cannot be approximated by shallow networks unless they use an exponentially larger amount of units or neurons. Eldan and Shamir (2015) have shown that, to approximate a specific function, a two-layer network requires an exponential number of neurons in the input dimension, while a three-layer network requires a polynomial number of neurons.", "startOffset": 37, "endOffset": 584}, {"referenceID": 7, "context": "The proof can be found in the reference (Gil et al., 2007).", "startOffset": 40, "endOffset": 58}, {"referenceID": 0, "context": "We note that both Andoni et al.(2014) and Barron(1993) showed the sizes of the networks in grow exponentially with respect to p if only 3-layer neural networks are allowed to use in approximating polynomials.", "startOffset": 18, "endOffset": 38}, {"referenceID": 0, "context": "We note that both Andoni et al.(2014) and Barron(1993) showed the sizes of the networks in grow exponentially with respect to p if only 3-layer neural networks are allowed to use in approximating polynomials.", "startOffset": 18, "endOffset": 55}, {"referenceID": 0, "context": "Similar results on approximating multivariate polynomials were obtained by Andoni et al.(2014) and Barron(1993).", "startOffset": 75, "endOffset": 95}, {"referenceID": 0, "context": "Similar results on approximating multivariate polynomials were obtained by Andoni et al.(2014) and Barron(1993). Barron(1993) showed that on can use a 3-layer neural network to approximate any multivariate polynomial with degree p, dimension d and network size d/\u03b5.", "startOffset": 75, "endOffset": 112}, {"referenceID": 0, "context": "Similar results on approximating multivariate polynomials were obtained by Andoni et al.(2014) and Barron(1993). Barron(1993) showed that on can use a 3-layer neural network to approximate any multivariate polynomial with degree p, dimension d and network size d/\u03b5.", "startOffset": 75, "endOffset": 126}, {"referenceID": 0, "context": "Similar results on approximating multivariate polynomials were obtained by Andoni et al.(2014) and Barron(1993). Barron(1993) showed that on can use a 3-layer neural network to approximate any multivariate polynomial with degree p, dimension d and network size d/\u03b5. Andoni et al.(2014) showed that one could use the gradient descent to train a 3-layer neural network of size d/\u03b5 to approximate any multivariate polynomial.", "startOffset": 75, "endOffset": 286}, {"referenceID": 0, "context": "Similar results on approximating multivariate polynomials were obtained by Andoni et al.(2014) and Barron(1993). Barron(1993) showed that on can use a 3-layer neural network to approximate any multivariate polynomial with degree p, dimension d and network size d/\u03b5. Andoni et al.(2014) showed that one could use the gradient descent to train a 3-layer neural network of size d/\u03b5 to approximate any multivariate polynomial. However, Theorem 9 shows that the deep neural network could reduce the network size from O (1/\u03b5) to O ( log 1\u03b5 ) for the same \u03b5 error. Besides, for a fixed input dimension d, the size of the 3layer neural network used by Andoni et al.(2014) and Barron(1993) grows exponentially with respect to the degree p.", "startOffset": 75, "endOffset": 664}, {"referenceID": 0, "context": "Similar results on approximating multivariate polynomials were obtained by Andoni et al.(2014) and Barron(1993). Barron(1993) showed that on can use a 3-layer neural network to approximate any multivariate polynomial with degree p, dimension d and network size d/\u03b5. Andoni et al.(2014) showed that one could use the gradient descent to train a 3-layer neural network of size d/\u03b5 to approximate any multivariate polynomial. However, Theorem 9 shows that the deep neural network could reduce the network size from O (1/\u03b5) to O ( log 1\u03b5 ) for the same \u03b5 error. Besides, for a fixed input dimension d, the size of the 3layer neural network used by Andoni et al.(2014) and Barron(1993) grows exponentially with respect to the degree p.", "startOffset": 75, "endOffset": 681}, {"referenceID": 11, "context": "Further, Telgarsky (2016) has shown that the maximum number of break points that a multilayer neural network of depth L and size N could have is (N/L).", "startOffset": 9, "endOffset": 26}], "year": 2016, "abstractText": "Recently there has been much interest in understanding why deep neural networks are preferred to shallow networks. In this paper, we show that, for a large class of piecewise smooth functions, the number of neurons needed by a shallow network to approximate a function is exponentially larger than the corresponding number of neurons needed by a deep network for a given degree of function approximation. First, we consider univariate functions on a bounded interval and require a neural network to achieve an approximation error of \u03b5 uniformly over the interval. We show that shallow networks (i.e., networks whose depth does not depend on \u03b5) require \u03a9(poly(1/\u03b5)) neurons while deep networks (i.e., networks whose depth grows with 1/\u03b5) require O(polylog(1/\u03b5)) neurons. We then extend these results to certain classes of important multivariate functions. Our results are derived for neural networks which use a combination of rectifier linear units (ReLUs) and binary step units, two of the most popular type of activation functions. Our analysis builds on this simple observation that the binary approximation of a real number in the interval [0, 1] can be represented by a deep neural network which uses a \u201csmall\u201d number of neurons.", "creator": "LaTeX with hyperref package"}}}