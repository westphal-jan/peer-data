{"id": "1610.04325", "review": {"conference": "iclr", "VERSION": "v1", "DATE_OF_SUBMISSION": "14-Oct-2016", "title": "Hadamard Product for Low-rank Bilinear Pooling", "abstract": "Bilinear models provide rich representations compared to linear models. They have been applied in various visual tasks, such as object recognition, segmentation, and visual question-answering, to get state-of-the-art performances taking advantage of the expanded representations. However, bilinear representations tend to be high-dimensional, limiting the applicability to computationally complex tasks. We propose low-rank bilinear neural networks using Hadamard product (element-wise multiplication), commonly implemented in many scientific computing frameworks. We show that our model outperforms compact bilinear pooling in visual question-answering tasks, having a better parsimonious property.", "histories": [["v1", "Fri, 14 Oct 2016 04:29:52 GMT  (80kb,D)", "http://arxiv.org/abs/1610.04325v1", "13 pages, 1 figure, &amp; appendix included"], ["v2", "Tue, 1 Nov 2016 05:31:27 GMT  (80kb,D)", "http://arxiv.org/abs/1610.04325v2", "13 pages, 1 figure, &amp; appendix included"], ["v3", "Tue, 14 Feb 2017 05:22:01 GMT  (83kb,D)", "http://arxiv.org/abs/1610.04325v3", "13 pages, 1 figure, &amp; appendix. ICLR 2017 accepted"], ["v4", "Sun, 26 Mar 2017 16:22:47 GMT  (83kb,D)", "http://arxiv.org/abs/1610.04325v4", "13 pages, 1 figure, &amp; appendix. ICLR 2017 accepted"]], "COMMENTS": "13 pages, 1 figure, &amp; appendix included", "reviews": [], "SUBJECTS": "cs.CV cs.AI cs.NE", "authors": ["jin-hwa kim", "kyoung-woon on", "woosang lim", "jeonghee kim", "jung-woo ha", "byoung-tak zhang"], "accepted": true, "id": "1610.04325"}, "pdf": {"name": "1610.04325.pdf", "metadata": {"source": "CRF", "title": null, "authors": ["Jin-Hwa Kim"], "emails": ["jhkim@bi.snu.ac.kr", "kwon@bi.snu.ac.kr", "jeonghee.kim@navercorp.com", "jungwoo.ha@navercorp.com", "btzhang@bi.snu.ac.kr"], "sections": [{"heading": null, "text": "Bilinear models provide comprehensive representation compared to linear models. They have been used in various visual tasks such as object recognition, segmentation, and visual issues to obtain state-of-the-art representation that benefits from advanced representation. However, bilinear representation tends to be high-dimensional, limiting its applicability to computationally complex tasks. We propose low-level bilinear neural networks that use Hadamard products (elemental multiplication), which are commonly implemented in many scientific computing frameworks. We show that our model outperforms compact bilinear pooling in answering visual questions and has better parsimonial characteristics."}, {"heading": "1 INTRODUCTION", "text": "This year, it has reached the point where it will be able to put itself at the forefront, in the way that it has been in recent years: in the way that it has to be done, in the way that it has to be done, in the way that it has to be done, in the way that it has to be done, in the way that it has to be done, in the way that it has to be done, in the way that it has to be able to do it."}, {"heading": "2 LOW-RANK BILINEAR MODEL", "text": "Bilinear models use a square extension of the linear transformation taking into account all pairs of characteristics. fi = N \u2211 j = 1 M \u2211 k = 1 wijkxjyk + bi = x TWiy + bi (1), where x and y are input vectors, Wi \u0441RN \u00b7 M is a weight matrix for output fi, and bi is a bias for output fi. Note that the number of parameters is L \u00b7 (N \u00b7 M + 1), including a bias vector b, where L is the number of output qualities. Pirsiavash et al. (2009) propose a low-grade bilinear method to reduce the rank of the weight matrix Wi in order to have fewer parameters for regulation. They rewrite the weight matrix as Wi = UiV T i, where U-RN \u00b7 d and V-RM \u00b7 d impose a constraint on the rank W dimensions."}, {"heading": "3 LOW-RANK BILINEAR NEURAL NETWORKS", "text": "A low-level bilinear model in Eq.3 can be implemented with two linear mappings without bias for embedding two input vectors, Hadamard product for multiplicative learning of common representations, and linear mapping with bias for projecting the common representations into an output vector for a given output dimension. Now, we discuss possible variations of low-level bilinear models inspired by studies of neural networks."}, {"heading": "3.1 FULL MODEL", "text": "In Eq.3, linear projections, U and V, can have their own bias vectors. As a result, linear models for each input vector, x and y, are integrated in an additive form called the complete model of linear regression in statistics: f = PT (((UTx + bx) \u0445 (VTy + by))) + b = PT (UTx \u0445 VTy) + U \u2032 Tx + V \u2032 Ty + b \u2032. (4) Here U \u2032 T = diag (by) \u00b7 UT, V \u2032 T = diag (bx) \u00b7 VT and b \u2032 = b + PT (bx \u0445 by)."}, {"heading": "3.2 NONLINEAR ACTIVATION", "text": "The application of nonlinear activation functions could help to increase the representative capacity of the model. The first candidate is the application of nonlinear activation functions directly after linear mappings for input vectors. f = PT (\u03c3 (UTx) \u03c3 (VTy))) + b (5), where \u03c3 refers to any nonlinear activation function that maps any real values into a finite interval, e.g. Sigmoid or Tanh. If two inputs come from different modalities, statistics of two inputs can be very different, which can lead to interference. As the gradient directly depends on the other input in the Hadamard product from two inputs, the additional application of an activation function according to the Hadamard product is not appropriate, since activation functions occur twice in the calculation of gradients. However, the application of the activation function would be an alternative choice only after the Hadamard product (We explore this option in Section T5: T\u03c3 (Ty) (Tf) (Ty = 6)."}, {"heading": "3.3 SHORTCUT CONNECTION", "text": "To avoid this unfortunate situation, we add abbreviations as studied in the context of residual learning (He et al., 2015).f = PT (\u03c3 (UTx) \u0445 \u03c3 (VTy)) + hx (x) + hy (y) + b (7), where hx and hy are abbreviations. Linear projection is a linear mapping of abbreviations. Note that this formulation is a generalized form of single-layered MRN (Kim et al., 2016b). Note, however, that the abbreviations are not used in our proposed model, as explained in Section 6."}, {"heading": "4 MULTIMODAL LOW-RANK BILINEAR ATTENTION NETWORKS", "text": "In this section, we apply low-level bilinear pooling to propose an attention mechanism for visual question-answering tasks, based on the interpretation of the previous section. We assumed that input is a question that embeds the vector q and a series of visual feature vectors F over the S \u00b7 S grid space."}, {"heading": "4.1 LOW-RANK BILINEAR POOLING IN ATTENTION MECHANISM", "text": "The attention mechanism uses an attention probability distribution \u03b1 over the S \u00d7 S lattice space. If \u03b1 is a coefficient vector for the linear combination of several visual features within the same lattice space, it is called (deterministic) soft attention (Bahdanau et al., 2014), compared to (stochastic) hard attention, in which visual features are examined according to a specific distribution (Xu et al., 2015). Here, the attention probability distribution \u03b1 for soft attention using the low bilinear pooling is defined as \u03b1 = softmax (PT\u03b1 (UTqq \u00b7 1T) (VTFFT)) (8), which is a hyperbolic tangent function, Uq, RN \u00d7 d, q, q, RN, 1, RS2, VF, RS2 \u00d7 M. If G > 1, several looks are explicitly expressed in this line, so that RF, RF, SF, SF, SQ, SQ, and RF, SQ, and RF, SQ, and RF, and so on."}, {"heading": "4.2 MULTIMODAL LOW-RANK BILINEAR ATTENTION NETWORKS", "text": "The accompanying visual feature v is a linear combination of visual feature vectors Fi with coefficients \u03b1g, i. Each attention probability distribution \u03b1g is for a fleeting glance g. For G > 1, v is the concatenation of the resulting vectors v-g.v = Gng = 1 S2 \u2211 s = 1 \u03b1g, sFs (9), where f denotes concatenation of vectors.The posterior probability distribution of the answers is an output of a Softmax function whose input is the result of another two-dimensional linkage of q and v-asp (a | q, F; E) = Softmax (PTo (\u03c3 (WTqq), (VTv))) (10) and the predicted response a-isa = arg max a-a-p (a-q, F; E) (11), which involves a series of candidate responses and an aggregation of whole model parameters."}, {"heading": "5 EXPERIMENTS", "text": "In this section, we perform six experiments to select the proposed model, Multimodal Low-Rank Bilinear Attention Networks (MLB). Each experiment controls other factors besides one factor to assess the impact on accuracy. Based on MRN (Kim et al., 2016b), we start our evaluations with an initial option of G = 1 and shortcuts of MRN called Multimodal Attention Residual Networks (MARN). We attribute this choice to the attention mechanism for visual characteristics, which provides more capacity to learn visual characteristics. We use the same hyperparameters of MRN (Kim et al., 2016b), without explicit mention of this.The VQA dataset al Dataset al al al al al al al al al al al al al al al al."}, {"heading": "6 RESULTS", "text": "The six experiments are performed one after the other to restrict architectural decisions. Each experiment determines experimental variables one by one. See Table 1, which divides the six sectors by means rules."}, {"heading": "6.1 SIX EXPERIMENT RESULTS", "text": "Number of learning blocks Although, MRN (Kim et al., 2016b) has the three-block architecture, MARN shows the best performance with two-block-layer models (63.92%). For the multiple-insight models in the next experiment, we choose single-block-layer model for its simplicity to expand, and competitive performance (63.79%). Number of glimmers Compared with the results of Fukui et al. (2016), four-insight MARN (64.61%) is better than other comparison models. However, for an economical choice, two-insight MARN (64.53%) is selected for later experiments. We speculate that multiple glances are one of the key factors for the competitiveness of MCB (Fukui et al., 2016), based on a large distance in accuracy, compared to a look at the accuracy that Krishotal mode MARN (2014.79%) is used."}, {"heading": "6.2 COMPARISON WITH STATE-OF-THE-ART", "text": "The overall accuracy of our model is approximately 1.9% higher than the next best model (Noh & Han, 2016) in the VQA's Open Ended task. Major improvements result from yes or no (Y / N) and other (ETC) answers. In Table 3, we also report on the accuracy of our ensemble model to compare it with other ensemble models on the VQA test standard, which ranked 1st to 5th in the VQA Challenge 20162. We beat the previous state of the art by a margin of 0.42%."}, {"heading": "7 RELATED WORKS", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "7.1 COMPACT BILINEAR POOLING", "text": "Compact bilinear pooling (Gao et al., 2015) approximates full bilinear pooling = full bilinear pooling using a stamping-based computerion, Tensor Sketch Projection (Charikar et al., 2002; Pham & Pagh, 2013): B (x, h, s) = B (x, h, s); B (y, h, s) (15) = FFT \u2212 1 (FFT (x, h, s). (16) 2http: / / visualqa.org / challenge.htmlwhere denotes outer product, B (v, h, s)."}, {"heading": "7.2 MULTIMODAL RESIDUAL NETWORKS", "text": "MRN (Kim et al., 2016b) is an implicit LB attention model that does not exhibit an explicit attention mechanism. Instead, they propose a new method of visualizing implicit attention generated by the Hadamard product, even though the visual input features are the results of a fully connected layer that does not exhibit explicit spatial information. F (k) (q, v) = \u03c3 (W (k) q) q)."}, {"heading": "8 CONCLUSIONS", "text": "We propose low-level bilinear pooling to replace compact bilinear pooling, which has a fan-out structure and requires complex calculations. Low-level bilinear pooling has a flexible structure and better economical properties using linear mapping and Hadamard products compared to compact bilinear pooling. We achieve new, state-of-the-art results on the VQA dataset using a similar architecture from Fukui et al. (2016) and replace compact bilinear pooling with low-level bilinear pooling. We believe that our method could be applicable to other bilinear learning tasks."}, {"heading": "ACKNOWLEDGMENTS", "text": "The authors thank Patrick Emaase for the helpful comments and editing. This work was supported by Naver Corp and in part by the Korean government (IITP-R0126-16-1072-SW.StarLab, KEIT-10044009-HRI.MESSI, KEIT-10060086-RISF, ADD-UD130070ID-BMRR)."}, {"heading": "A EXPERIMENT DETAILS", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "A.1 PREPROCESSING", "text": "We follow the process of pre-processing by Kim et al. (2016b). Here we comment on some details and modifications."}, {"heading": "A.1.1 QUESTION EMBEDDING", "text": "Based on previous studies (Noh et al., 2015; Kim et al., 2016b), a word embedding matrix and a GRU with a pre-trained model of the Skip-thought Vector (Kiros et al., 2015) are initialized. Consequently, question vectors have 2400 dimensions. Kim et al. (2016a) is used for the GRU to efficiently calculate variable length questions. Furthermore, the Bayesian dropout (Gal.) implemented in Le-onard et al. (2015) is used for regulation during training. A.2 VISION EMBEDINGResNet-152 networks (He et al., 2015) are used for featureextraction. The dimensionality of an input is 3 x 448 x 448 x 448 outputs with last dimensions."}, {"heading": "A.3 HYPERPARAMETERS", "text": "The hyperparameters used in the MLB of Table 2 are described in Table 4. Batch size is 200 and the number of iterations is set at 250K. For data-enhanced models, a simplified early stop is used, starting from 250K to 350K iteration for all 25K iterations (250K, 275K, 300K, 325K and 350K; no more than five points) to avoid exhaustive input to the VQA test development server."}, {"heading": "A.4 MODEL SCHEMA", "text": "Figure 1 shows a schematic diagram of MLB in which \u0443 denotes Hadamard product and \u03a3 a linear combination of visual feature vectors using coefficients, i.e. the output of the Softmax function. If G > 1, the Softmax function is applied to each row vector of an output matrix (Eq.8), and we concatenate the resulting vectors of linear G combinations (Eq.9)."}, {"heading": "A.5 ENSEMBLE OF SEVEN MODELS", "text": "The test results for individual models, which consist of our ensemble model, are shown in Table 5."}, {"heading": "LinearLinear", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "MODEL GLIMPSE ALL Y/N NUM ETC", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "B OTHER RELATED WORKS", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "B.1 HIGHER-ORDER BOLTZMANN MACHINES", "text": "A similar model is found in a study of higher order Boltzmann machines (Memisevic & Hinton, 2007; 2010): They propose a factoring method for the three-way energy function to capture correlations between input, output and hidden representations. \u2212 E (y, h; x) = [f (\u2211 i xiw x if) (\u0445 j yjw y jf) (\u0445 k hkw h kf) + \u2211 k whkhk + \u2211 j wyj yj = (xTWx-yTWy-hTWh) 1 + hTwh + yTwy (19) Apart from biases, the parameter tensor of higher order unfactored Boltzmann machines is replaced by three matrices, Wx, Wx, RI, RJ, F and Wh, RK, F."}, {"heading": "B.2 MULTIPLICATIVE INTEGRATION WITH RECURRENT NEURAL NETWORKS", "text": "Most relapsing neural networks, including vanilla RNNs, long short term memory networks (Hochreiter & Schmidhuber, 1997), and gated recurrent units (Cho et al., 2014), have a common expression as follows:???? (Wx + Uh + b) (20), where?? is a nonlinear function,?? Rd \u00b7 n,? Rn,? Rd \u00b7 m,? Rm, and? Rd is a bias vector. Note that x is usually an input state vector, and h is a hidden state vector in relapsing neural networks.? Wu et al. (2016c) suggest a new design to replace the additive expression with a multiplicative expression using the Hadamard product ascription (Wx-Uh + b). (21) In addition, a general formulation of this multiplicative integration can be described as follows:"}], "references": [{"title": "Learning to Compose Neural Networks for Question Answering", "author": ["Jacob Andreas", "Marcus Rohrbach", "Trevor Darrell", "Dan Klein"], "venue": "arXiv preprint arXiv:1601.01705,", "citeRegEx": "Andreas et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Andreas et al\\.", "year": 2016}, {"title": "VQA: Visual Question Answering", "author": ["Stanislaw Antol", "Aishwarya Agrawal", "Jiasen Lu", "Margaret Mitchell", "Dhruv Batra", "C. Lawrence Zitnick", "Devi Parikh"], "venue": "International Conference on Computer Vision,", "citeRegEx": "Antol et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Antol et al\\.", "year": 2015}, {"title": "Neural machine translation by jointly learning to align and translate", "author": ["Dzmitry Bahdanau", "Kyunghyun Cho", "Yoshua Bengio"], "venue": "arXiv preprint arXiv:1409.0473,", "citeRegEx": "Bahdanau et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Bahdanau et al\\.", "year": 2014}, {"title": "Finding frequent items in data streams", "author": ["Moses Charikar", "Kevin Chen", "Martin Farach-Colton"], "venue": "In International Colloquium on Automata, Languages, and Programming,", "citeRegEx": "Charikar et al\\.,? \\Q2002\\E", "shortCiteRegEx": "Charikar et al\\.", "year": 2002}, {"title": "Learning Phrase Representations using RNN Encoder-Decoder for Statistical Machine Translation", "author": ["Kyunghyun Cho", "Bart Van Merri\u00ebnboer", "Caglar Gulcehre", "Dzmitry Bahdanau", "Fethi Bougares", "Holger Schwenk", "Yoshua Bengio"], "venue": "In Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP),", "citeRegEx": "Cho et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Cho et al\\.", "year": 2014}, {"title": "Multimodal Compact Bilinear Pooling for Visual Question Answering and Visual Grounding", "author": ["Akira Fukui", "Dong Huk Park", "Daylen Yang", "Anna Rohrbach", "Trevor Darrell", "Marcus Rohrbach"], "venue": "arXiv preprint arXiv:1606.01847,", "citeRegEx": "Fukui et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Fukui et al\\.", "year": 2016}, {"title": "A Theoretically Grounded Application of Dropout in Recurrent Neural Networks", "author": ["Yarin Gal"], "venue": "arXiv preprint arXiv:1512.05287,", "citeRegEx": "Gal.,? \\Q2015\\E", "shortCiteRegEx": "Gal.", "year": 2015}, {"title": "Compact Bilinear Pooling", "author": ["Yang Gao", "Oscar Beijbom", "Ning Zhang", "Trevor Darrell"], "venue": "arXiv preprint arXiv:1511.06062,", "citeRegEx": "Gao et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Gao et al\\.", "year": 2015}, {"title": "Deep Residual Learning for Image Recognition", "author": ["Kaiming He", "Xiangyu Zhang", "Shaoqing Ren", "Jian Sun"], "venue": "arXiv preprint arXiv:1512.03385,", "citeRegEx": "He et al\\.,? \\Q2015\\E", "shortCiteRegEx": "He et al\\.", "year": 2015}, {"title": "Long Short-Term Memory", "author": ["Sepp Hochreiter", "J\u00fcrgen Schmidhuber"], "venue": "Neural computation,", "citeRegEx": "Hochreiter and Schmidhuber.,? \\Q1997\\E", "shortCiteRegEx": "Hochreiter and Schmidhuber.", "year": 1997}, {"title": "A Focused Dynamic Attention Model for Visual Question Answering", "author": ["Ilija Ilievski", "Shuicheng Yan", "Jiashi Feng"], "venue": "arXiv preprint arXiv:1604.01485,", "citeRegEx": "Ilievski et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Ilievski et al\\.", "year": 2016}, {"title": "Spatial Transformer Networks", "author": ["Max Jaderberg", "Karen Simonyan", "Andrew Zisserman", "Koray Kavukcuoglu"], "venue": "In Advances in Neural Information Processing Systems", "citeRegEx": "Jaderberg et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Jaderberg et al\\.", "year": 2015}, {"title": "Visual Question Answering: Datasets, Algorithms, and Future Challenges", "author": ["Kushal Kafle", "Christopher Kanan"], "venue": "arXiv preprint arXiv:1610.01465,", "citeRegEx": "Kafle and Kanan.,? \\Q2016\\E", "shortCiteRegEx": "Kafle and Kanan.", "year": 2016}, {"title": "Answer-Type Prediction for Visual Question Answering", "author": ["Kushal Kafle", "Christopher Kanan"], "venue": "IEEE Conference on Computer Vision and Pattern Recognition,", "citeRegEx": "Kafle and Kanan.,? \\Q2016\\E", "shortCiteRegEx": "Kafle and Kanan.", "year": 2016}, {"title": "TrimZero: A Torch Recurrent Module for Efficient Natural Language Processing", "author": ["Jin-Hwa Kim", "Jeonghee Kim", "Jung-Woo Ha", "Byoung-Tak Zhang"], "venue": "In Proceedings of KIIS Spring Conference,", "citeRegEx": "Kim et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Kim et al\\.", "year": 2016}, {"title": "Multimodal Residual Learning for Visual QA", "author": ["Jin-Hwa Kim", "Sang-Woo Lee", "Dong-Hyun Kwak", "Min-Oh Heo", "Jeonghee Kim", "Jung-Woo Ha", "Byoung-Tak Zhang"], "venue": "arXiv preprint arXiv:1606.01455,", "citeRegEx": "Kim et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Kim et al\\.", "year": 2016}, {"title": "Skip-Thought Vectors", "author": ["Ryan Kiros", "Yukun Zhu", "Ruslan Salakhutdinov", "Richard S. Zemel", "Antonio Torralba", "Raquel Urtasun", "Sanja Fidler"], "venue": "In Advances in Neural Information Processing Systems", "citeRegEx": "Kiros et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Kiros et al\\.", "year": 2015}, {"title": "Visual genome: Connecting language and vision using crowdsourced dense image annotations", "author": ["Ranjay Krishna", "Yuke Zhu", "Oliver Groth", "Justin Johnson", "Kenji Hata", "Joshua Kravitz", "Stephanie Chen", "Yannis Kalantidis", "Li-Jia Li", "David A Shamma", "Michael Bernstein", "Li Fei-Fei"], "venue": "arXiv preprint arXiv:1602.07332,", "citeRegEx": "Krishna et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Krishna et al\\.", "year": 2016}, {"title": "Recurrent Library for Torch", "author": ["Nicholas L\u00e9onard", "Sagar Waghmare", "Yang Wang", "Jin-Hwa Kim"], "venue": "arXiv preprint arXiv:1511.07889,", "citeRegEx": "L\u00e9onard et al\\.,? \\Q2015\\E", "shortCiteRegEx": "L\u00e9onard et al\\.", "year": 2015}, {"title": "Bilinear CNN Models for Fine-grained Visual Recognition", "author": ["Tsung-Yu Lin", "Aruni RoyChowdhury", "Subhransu Maji"], "venue": "In IEEE International Conference on Computer Vision, pp", "citeRegEx": "Lin et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Lin et al\\.", "year": 2015}, {"title": "Deeper LSTM and normalized CNN Visual Question Answering model", "author": ["Jiasen Lu", "Xiao Lin", "Dhruv Batra", "Devi Parikh"], "venue": "https://github.com/VT-vision-lab/VQA_LSTM_CNN,", "citeRegEx": "Lu et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Lu et al\\.", "year": 2015}, {"title": "Hierarchical Question-Image Co-Attention for Visual Question Answering", "author": ["Jiasen Lu", "Jianwei Yang", "Dhruv Batra", "Devi Parikh"], "venue": "arXiv preprint arXiv:1606.00061,", "citeRegEx": "Lu et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Lu et al\\.", "year": 2016}, {"title": "Ask Your Neurons: A Deep Learning Approach to Visual Question Answering", "author": ["Mateusz Malinowski", "Marcus Rohrbach", "Mario Fritz"], "venue": "arXiv preprint arXiv:1605.02697,", "citeRegEx": "Malinowski et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Malinowski et al\\.", "year": 2016}, {"title": "Unsupervised learning of image transformations", "author": ["Roland Memisevic", "Geoffrey E Hinton"], "venue": "In IEEE Conference on Computer Vision and Pattern Recognition,", "citeRegEx": "Memisevic and Hinton.,? \\Q2007\\E", "shortCiteRegEx": "Memisevic and Hinton.", "year": 2007}, {"title": "Learning to represent spatial transformations with factored higher-order Boltzmann machines", "author": ["Roland Memisevic", "Geoffrey E Hinton"], "venue": "Neural computation,", "citeRegEx": "Memisevic and Hinton.,? \\Q2010\\E", "shortCiteRegEx": "Memisevic and Hinton.", "year": 2010}, {"title": "Training Recurrent Answering Units with Joint Loss Minimization for VQA", "author": ["Hyeonwoo Noh", "Bohyung Han"], "venue": "arXiv preprint arXiv:1606.03647,", "citeRegEx": "Noh and Han.,? \\Q2016\\E", "shortCiteRegEx": "Noh and Han.", "year": 2016}, {"title": "Image Question Answering using Convolutional Neural Network with Dynamic Parameter Prediction", "author": ["Hyeonwoo Noh", "Paul Hongsuck Seo", "Bohyung Han"], "venue": "arXiv preprint arXiv:1511.05756,", "citeRegEx": "Noh et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Noh et al\\.", "year": 2015}, {"title": "Fast and scalable polynomial kernels via explicit feature maps", "author": ["Ninh Pham", "Rasmus Pagh"], "venue": "In Proceedings of the 19th ACM SIGKDD international conference on Knowledge discovery and data mining,", "citeRegEx": "Pham and Pagh.,? \\Q2013\\E", "shortCiteRegEx": "Pham and Pagh.", "year": 2013}, {"title": "Bilinear classifiers for visual recognition", "author": ["Hamed Pirsiavash", "Deva Ramanan", "Charless C. Fowlkes"], "venue": "Advances in Neural Information Processing Systems", "citeRegEx": "Pirsiavash et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Pirsiavash et al\\.", "year": 2009}, {"title": "Separating style and content with bilinear models", "author": ["Joshua B Tenenbaum", "William T Freeman"], "venue": "Neural computation,", "citeRegEx": "Tenenbaum and Freeman.,? \\Q2000\\E", "shortCiteRegEx": "Tenenbaum and Freeman.", "year": 2000}, {"title": "Visual Question Answering: A Survey of Methods and Datasets", "author": ["Qi Wu", "Damien Teney", "Peng Wang", "Chunhua Shen", "Anthony Dick", "Anton van den Hengel"], "venue": "arXiv preprint arXiv:1607.05910,", "citeRegEx": "Wu et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Wu et al\\.", "year": 2016}, {"title": "Ask Me Anything: Free-form Visual Question Answering Based on Knowledge from External Sources", "author": ["Qi Wu", "Peng Wang", "Chunhua Shen", "Anthony Dick", "Anton van den Hengel"], "venue": "In IEEE Conference on Computer Vision and Pattern Recognition,", "citeRegEx": "Wu et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Wu et al\\.", "year": 2016}, {"title": "On Multiplicative Integration with Recurrent Neural Networks. 2016c", "author": ["Yuhuai Wu", "Saizheng Zhang", "Ying Zhang", "Yoshua Bengio", "Ruslan Salakhutdinov"], "venue": null, "citeRegEx": "Wu et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Wu et al\\.", "year": 2016}, {"title": "Dynamic Memory Networks for Visual and Textual Question Answering", "author": ["Caiming Xiong", "Stephen Merity", "Richard Socher"], "venue": "arXiv preprint arXiv:1603.01417,", "citeRegEx": "Xiong et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Xiong et al\\.", "year": 2016}, {"title": "Ask, Attend and Answer: Exploring Question-Guided Spatial Attention for Visual Question Answering", "author": ["Huijuan Xu", "Kate Saenko"], "venue": "In European Conference on Computer Vision,", "citeRegEx": "Xu and Saenko.,? \\Q2016\\E", "shortCiteRegEx": "Xu and Saenko.", "year": 2016}, {"title": "Show, Attend and Tell : Neural Image Caption Generation with Visual Attention", "author": ["Kelvin Xu", "Aaron Courville", "Richard S Zemel", "Yoshua Bengio"], "venue": "In Proceedings of the 32nd International Conference on Machine Learning,", "citeRegEx": "Xu et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Xu et al\\.", "year": 2015}, {"title": "Stacked Attention Networks for Image Question Answering", "author": ["Zichao Yang", "Xiaodong He", "Jianfeng Gao", "Li Deng", "Alex Smola"], "venue": "arXiv preprint arXiv:1511.02274,", "citeRegEx": "Yang et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Yang et al\\.", "year": 2015}, {"title": "Simple Baseline for Visual Question Answering", "author": ["Bolei Zhou", "Yuandong Tian", "Sainbayar Sukhbaatar", "Arthur Szlam", "Rob Fergus"], "venue": "arXiv preprint arXiv:1512.02167,", "citeRegEx": "Zhou et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Zhou et al\\.", "year": 2015}, {"title": "2016a) is used for the GRU. Moreover, for regularization, Bayesian Dropout (Gal, 2015) which is implemented in L\u00e9onard et al", "author": ["Kim"], "venue": null, "citeRegEx": "Kim,? \\Q2015\\E", "shortCiteRegEx": "Kim", "year": 2015}, {"title": "2016c) propose a new design to replace the additive expression with a multiplicative expression using Hadamard product", "author": ["Wu"], "venue": null, "citeRegEx": "Wu,? \\Q2016\\E", "shortCiteRegEx": "Wu", "year": 2016}], "referenceMentions": [{"referenceID": 19, "context": "Recently, a successful application of this technique is used for fine-grained visual recognition (Lin et al., 2015).", "startOffset": 97, "endOffset": 115}, {"referenceID": 7, "context": "Compact bilinear pooling (Gao et al., 2015) reduces the quadratic expansion of dimensionality by two orders of magnitude, retaining the performance of the full bilinear pooling.", "startOffset": 25, "endOffset": 43}, {"referenceID": 3, "context": "This approximation uses sampling-based computation, Tensor Sketch Projection (Charikar et al., 2002; Pham & Pagh, 2013), which utilizes an useful property that \u03a8(x\u2297 y, h, s) = \u03a8(x, h, s) \u2217\u03a8(y, h, s), which means the projection of outer product of two vectors is the convolution of two projected vectors.", "startOffset": 77, "endOffset": 119}, {"referenceID": 7, "context": "Practical choices are 10K and 16K for 512 and 4096-dimensional inputs, respectively (Gao et al., 2015; Fukui et al., 2016).", "startOffset": 84, "endOffset": 122}, {"referenceID": 5, "context": "Practical choices are 10K and 16K for 512 and 4096-dimensional inputs, respectively (Gao et al., 2015; Fukui et al., 2016).", "startOffset": 84, "endOffset": 122}, {"referenceID": 8, "context": "We also explore to add non-linearity using non-linear activation functions into the low-rank bilinear neural networks, and shortcut connections inspired by deep residual learning (He et al., 2015).", "startOffset": 179, "endOffset": 196}, {"referenceID": 28, "context": "Pirsiavash et al. (2009) suggest a low-rank bilinear method to reduce the rank of the weight matrix Wi to have less number of parameters for regularization.", "startOffset": 0, "endOffset": 25}, {"referenceID": 8, "context": "To avoid this unfortunate situation, we add shortcut connections as explored in residual learning (He et al., 2015).", "startOffset": 98, "endOffset": 115}, {"referenceID": 2, "context": "space, it is called (deterministic) soft attention (Bahdanau et al., 2014), compared to (stochastic) hard attention, which samples visual features following a given distribution (Xu et al.", "startOffset": 51, "endOffset": 74}, {"referenceID": 35, "context": ", 2014), compared to (stochastic) hard attention, which samples visual features following a given distribution (Xu et al., 2015).", "startOffset": 111, "endOffset": 128}, {"referenceID": 2, "context": "space, it is called (deterministic) soft attention (Bahdanau et al., 2014), compared to (stochastic) hard attention, which samples visual features following a given distribution (Xu et al., 2015). Here, using the low-rank bilinear pooling, the attention probability distribution \u03b1 for the soft attention is defined as \u03b1 = softmax ( P\u03b1 ( \u03c3(Uqq \u00b7 1 ) \u25e6 \u03c3(V FF ) )) (8) where \u03b1 \u2208 RG\u00d7S2 , P\u03b1 \u2208 Rd\u00d7G, \u03c3 is a hyperbolic tangent function, Uq \u2208 RN\u00d7d, q \u2208 R , 1 \u2208 RS2 , VF \u2208 RM\u00d7d, and F \u2208 R \u00d7M . If G > 1, multiple glimpses are explicitly expressed as in Fukui et al. (2016), conceptually similar to Jaderberg et al.", "startOffset": 52, "endOffset": 566}, {"referenceID": 2, "context": "space, it is called (deterministic) soft attention (Bahdanau et al., 2014), compared to (stochastic) hard attention, which samples visual features following a given distribution (Xu et al., 2015). Here, using the low-rank bilinear pooling, the attention probability distribution \u03b1 for the soft attention is defined as \u03b1 = softmax ( P\u03b1 ( \u03c3(Uqq \u00b7 1 ) \u25e6 \u03c3(V FF ) )) (8) where \u03b1 \u2208 RG\u00d7S2 , P\u03b1 \u2208 Rd\u00d7G, \u03c3 is a hyperbolic tangent function, Uq \u2208 RN\u00d7d, q \u2208 R , 1 \u2208 RS2 , VF \u2208 RM\u00d7d, and F \u2208 R \u00d7M . If G > 1, multiple glimpses are explicitly expressed as in Fukui et al. (2016), conceptually similar to Jaderberg et al. (2015). And, the softmax function applies to each row vector of \u03b1.", "startOffset": 52, "endOffset": 615}, {"referenceID": 1, "context": "The VQA dataset (Antol et al., 2015) is used as a primary dataset, and, for data augmentation, question-answering annotations of Visual Genome (Krishna et al.", "startOffset": 16, "endOffset": 36}, {"referenceID": 17, "context": ", 2015) is used as a primary dataset, and, for data augmentation, question-answering annotations of Visual Genome (Krishna et al., 2016) are used.", "startOffset": 114, "endOffset": 136}, {"referenceID": 1, "context": "The VQA dataset (Antol et al., 2015) is used as a primary dataset, and, for data augmentation, question-answering annotations of Visual Genome (Krishna et al., 2016) are used. Validation is performed on the VQA test-dev split, and model comparison is based on the results of the VQA test-standard split. For the comprehensive reviews of VQA tasks, please refer to Wu et al. (2016a) and Kafle & Kanan (2016a).", "startOffset": 17, "endOffset": 382}, {"referenceID": 1, "context": "The VQA dataset (Antol et al., 2015) is used as a primary dataset, and, for data augmentation, question-answering annotations of Visual Genome (Krishna et al., 2016) are used. Validation is performed on the VQA test-dev split, and model comparison is based on the results of the VQA test-standard split. For the comprehensive reviews of VQA tasks, please refer to Wu et al. (2016a) and Kafle & Kanan (2016a).", "startOffset": 17, "endOffset": 408}, {"referenceID": 14, "context": "Number of Learning Blocks Kim et al. (2016b) argue that three-block layered MRN shows the best performance among one to four-block layered models, taking advantage of residual learning.", "startOffset": 26, "endOffset": 45}, {"referenceID": 5, "context": "Number of Glimpses Fukui et al. (2016) show that the attention mechanism of two glimpses was an optimal choice.", "startOffset": 19, "endOffset": 39}, {"referenceID": 5, "context": "Since Fukui et al. (2016) only report the accuracy of the ensemble model on the test-standard, the test-dev results of their single models are included in the last sector.", "startOffset": 6, "endOffset": 26}, {"referenceID": 5, "context": "76 MCB+Att (Fukui et al., 2016) 69.", "startOffset": 11, "endOffset": 31}, {"referenceID": 5, "context": "8 MCB+Att+GloVe (Fukui et al., 2016) 70.", "startOffset": 16, "endOffset": 36}, {"referenceID": 5, "context": "6 MCB+Att+Glove+VG (Fukui et al., 2016) 70.", "startOffset": 19, "endOffset": 39}, {"referenceID": 1, "context": "Answer Sampling VQA (Antol et al., 2015) dataset has ten answers from unique persons for each question, while Visual Genome (Krishna et al.", "startOffset": 20, "endOffset": 40}, {"referenceID": 17, "context": ", 2015) dataset has ten answers from unique persons for each question, while Visual Genome (Krishna et al., 2016) dataset has a single answer for each question.", "startOffset": 91, "endOffset": 113}, {"referenceID": 1, "context": "Answer Sampling VQA (Antol et al., 2015) dataset has ten answers from unique persons for each question, while Visual Genome (Krishna et al., 2016) dataset has a single answer for each question. Since difficult or ambiguous questions may have divided answers, the probabilistic sampling from the distribution of answers can be utilized to optimize for the multiple answers. An instance 1 can be found in Fukui et al. (2016). We simplify the procedure as follows:", "startOffset": 21, "endOffset": 423}, {"referenceID": 1, "context": "We define the divided answers as having at least three answers which are the secondly frequent one, for the evaluation metric of VQA (Antol et al., 2015), accuracy(ak) = min (|ak|/3, 1) .", "startOffset": 133, "endOffset": 153}, {"referenceID": 37, "context": "iBOWIMG (Zhou et al., 2015) 55.", "startOffset": 8, "endOffset": 27}, {"referenceID": 26, "context": "97 DPPnet (Noh et al., 2015) 57.", "startOffset": 10, "endOffset": 28}, {"referenceID": 20, "context": "69 Deeper LSTM+Normalized CNN (Lu et al., 2015) 58.", "startOffset": 30, "endOffset": 47}, {"referenceID": 22, "context": "48 Ask Your Neuron (Malinowski et al., 2016) 58.", "startOffset": 19, "endOffset": 44}, {"referenceID": 36, "context": "32 SAN (Yang et al., 2015) 58.", "startOffset": 7, "endOffset": 26}, {"referenceID": 0, "context": "42 D-NMN (Andreas et al., 2016) 59.", "startOffset": 9, "endOffset": 31}, {"referenceID": 10, "context": "83 FDA (Ilievski et al., 2016) 59.", "startOffset": 7, "endOffset": 30}, {"referenceID": 33, "context": "56 DMN+ (Xiong et al., 2016) 60.", "startOffset": 8, "endOffset": 28}, {"referenceID": 21, "context": "33 HieCoAtt (Lu et al., 2016) 62.", "startOffset": 12, "endOffset": 29}, {"referenceID": 8, "context": "Since the usefulness of shortcut connections is linked to the network depth (He et al., 2015).", "startOffset": 76, "endOffset": 93}, {"referenceID": 17, "context": "Data Augmentation The data augmentation with Visual Genome (Krishna et al., 2016) question answer annotations is explored.", "startOffset": 59, "endOffset": 81}, {"referenceID": 17, "context": "Visual Genome (Krishna et al., 2016) originally provides 1.", "startOffset": 14, "endOffset": 36}, {"referenceID": 5, "context": "We speculate that multiple glimpses are one of key factors for the competitive performance of MCB (Fukui et al., 2016), based on a large margin in accuracy, compared to one-glimpse MARN (63.", "startOffset": 98, "endOffset": 118}, {"referenceID": 5, "context": "Number of Glimpses Compared with the results of Fukui et al. (2016), four-glimpse MARN (64.", "startOffset": 48, "endOffset": 68}, {"referenceID": 17, "context": "Data Augmentation Data augmentation using Visual Genome (Krishna et al., 2016) question answer annotations significantly improves the performance by 0.", "startOffset": 56, "endOffset": 78}, {"referenceID": 5, "context": "26 MCB (Fukui et al., 2016) 66.", "startOffset": 7, "endOffset": 27}, {"referenceID": 1, "context": "29 Human (Antol et al., 2015) 83.", "startOffset": 9, "endOffset": 29}, {"referenceID": 7, "context": "Compact bilinear pooling (Gao et al., 2015) approximates full bilinear pooling using a samplingbased computation, Tensor Sketch Projection (Charikar et al.", "startOffset": 25, "endOffset": 43}, {"referenceID": 3, "context": ", 2015) approximates full bilinear pooling using a samplingbased computation, Tensor Sketch Projection (Charikar et al., 2002; Pham & Pagh, 2013): \u03a8(x\u2297 y, h, s) = \u03a8(x, h, s) \u2217\u03a8(y, h, s) (15) = FFT\u22121(FFT(\u03a8(x, h, s) \u25e6 FFT(\u03a8(y, h, s)) (16) http://visualqa.", "startOffset": 103, "endOffset": 145}, {"referenceID": 5, "context": "Although the dimensions of x and y are different from each other, one can easily generalize compact bilinear pooling for multimodal learning (Fukui et al., 2016).", "startOffset": 141, "endOffset": 161}, {"referenceID": 5, "context": "MCB (Fukui et al., 2016) for VQA tasks needs to set the dimension of output d to 16,000, to reduce the bias induced by the fixed random variables h and s.", "startOffset": 4, "endOffset": 24}, {"referenceID": 21, "context": "Recent state-of-the-art methods use a variant of an explicit attention mechanism in their models (Lu et al., 2016; Noh & Han, 2016; Fukui et al., 2016).", "startOffset": 97, "endOffset": 151}, {"referenceID": 5, "context": "Recent state-of-the-art methods use a variant of an explicit attention mechanism in their models (Lu et al., 2016; Noh & Han, 2016; Fukui et al., 2016).", "startOffset": 97, "endOffset": 151}, {"referenceID": 5, "context": ", 2016; Noh & Han, 2016; Fukui et al., 2016). Note that shortcut connections of MRN are not used in the proposed Multimodal Low-rank Bilinear (MLB) model. Since, it does not have any performance gain due to not stacking multiple layers in MLB. We leave the study of residual learning for MLB for future work, which may leverage the excellency of bilinear models as suggested in Wu et al. (2016a).", "startOffset": 25, "endOffset": 396}, {"referenceID": 5, "context": "We achieve new state-of-the-art results on the VQA dataset using a similar architecture of Fukui et al. (2016), replacing compact bilinear pooling with low-rank bilinear pooling.", "startOffset": 91, "endOffset": 111}], "year": 2017, "abstractText": "Bilinear models provide rich representations compared to linear models. They have been applied in various visual tasks, such as object recognition, segmentation, and visual question-answering, to get state-of-the-art performances taking advantage of the expanded representations. However, bilinear representations tend to be high-dimensional, limiting the applicability to computationally complex tasks. We propose low-rank bilinear neural networks using Hadamard product (element-wise multiplication), commonly implemented in many scientific computing frameworks. We show that our model outperforms compact bilinear pooling in visual question-answering tasks, having a better parsimonious property.", "creator": "LaTeX with hyperref package"}}}