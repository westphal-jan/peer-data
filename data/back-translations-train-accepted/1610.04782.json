{"id": "1610.04782", "review": {"conference": "ICML", "VERSION": "v1", "DATE_OF_SUBMISSION": "15-Oct-2016", "title": "An Adaptive Test of Independence with Analytic Kernel Embeddings", "abstract": "A new computationally efficient dependence measure, and an adaptive statistical test of independence, are proposed. The dependence measure is the difference between analytic embeddings of the joint distribution and the product of the marginals, evaluated at a finite set of locations (features). These features are chosen so as to maximize a lower bound on the test power, resulting in a test that is data-efficient, and that runs in linear time (with respect to the sample size n). The optimized features can be interpreted as evidence to reject the null hypothesis, indicating regions in the joint domain where the joint distribution and the product of the marginals differ most. Consistency of the independence test is established, for an appropriate choice of features. In real-world benchmarks, independence tests using the optimized features perform comparably to the state-of-the-art quadratic-time HSIC test, and outperform competing O(n) and O(n log n) tests.", "histories": [["v1", "Sat, 15 Oct 2016 20:19:48 GMT  (1678kb,D)", "http://arxiv.org/abs/1610.04782v1", "8 pages of main text"]], "COMMENTS": "8 pages of main text", "reviews": [], "SUBJECTS": "stat.ML cs.LG", "authors": ["wittawat jitkrittum", "zolt\u00e1n szab\u00f3 0001", "arthur gretton"], "accepted": true, "id": "1610.04782"}, "pdf": {"name": "1610.04782.pdf", "metadata": {"source": "CRF", "title": "An Adaptive Test of Independence with Analytic Kernel Embeddings", "authors": ["Wittawat Jitkrittum", "Zolt\u00e1n Szab\u00f3", "Arthur Gretton"], "emails": ["wittawat@gatsby.ucl.ac.uk", "zoltan.szabo@polytechnique.edu", "arthur.gretton@gmail.com"], "sections": [{"heading": null, "text": "A new computationally efficient dependence measurement and an adaptive statistical independence test are proposed. Dependence measurement is the difference between the analytical embedding of the common distribution and the product of the marginals, which are evaluated at a limited number of locations (characteristics). These characteristics are chosen to maximize a lower limit of test performance, resulting in a test that is data-efficient and runs in linear time (in terms of sample size n). The optimized characteristics can be interpreted as evidence of the rejection of the null hypothesis and indicate regions in the common area where the common distribution and the product of the marginals differ most. For an appropriate selection of characteristics, the consistency of the independence test is determined. In the real world, independence tests that use the optimized characteristics provide comparable performance to the state-of-the-art quadratictime HSIC test and perform better than competing tests (O - log and n)."}, {"heading": "1 Introduction", "text": "In fact, it is such that most of them will be able to move into another world, in which they are able to move, in which they move, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they, in which they, in which they live, in which they, in which they live, in which they, in which they live, in which they, in which they, in which they live."}, {"heading": "2 Independence Criteria and Statistical Tests", "text": "We introduce two test statistics: first, the Finite Set Independence Criterion (FSIC), which is based on the principle that dependence can be measured by the covariance between data characteristics. Next, we propose a normalized version of this statistic (NFSIC), with a simpler asymptotic distribution at Pxy = PxPy. We show how to select characteristics for the latter statistic to maximize a lower limit on the performance of the corresponding statistical test."}, {"heading": "2.1 The Finite Set Independence Criterion", "text": "We start with the introduction of the Hilbert-Schmidt criterion (HSIC) as in Gretton et al. (2005), because our unnormalized statistics are constructed along similar lines. Let's look at two random variables X-X-Rdx and Y-Y-Rdy. Denote by Pxy the common distribution between X and Y; Px and Py are the marginal distributions of X and Y. Let's look at the tensor product so that (a b) c = a < b >. Assume that k: X-X \u2192 R and l: Y \u00b7 Y \u2192 R are positively defined kernels associated with the reproduction of Hilbert spaces (RKHS) Hk and Hl \u00b7 HS are the normative expectations of Hl \u2192 Hk Hilbert-Schmidt operators. Then the HSIC is defined between X and Y."}, {"heading": "2.2 Normalized FSIC and Adaptive Test", "text": "For the purposes of an independence test, we consider a normalized variant of F-SIC2, which we call N-FSIC2, whose tractable asymptotic zero distribution is \u03c72 (J), the chi-quadratic distribution with J degrees of freedom. We then show that the independence test defined by N-FSIC2 is consistent, and these results are given in Theorem 5 (Independence test using N-FSIC2 is consistent). Let us be a consistent assessment of what is based on the common sample Zn."}, {"heading": "2. \u03a3 is invertible almost surely with respect to VJ =", "text": "{(vi, wi)} Ji = 1 from an absolutely continuous distribution. 3. Limn \u2192 \u221e \u03b3n = 0."}, {"heading": "Then, for any k, l and VJ satisfying the assumptions,", "text": "This means that the independence test based on N-FSIC2 based on N-FSIC2 is consistent (see Proposition 4). Proof 2 builds on the result in Proposition 2, which states that u 6 = 0 under H1; it follows the convergence of u-targets to u-targets. Full proof can be found in Appendix E.Theorem 5 that if H1 holds, the statistic can be arbitrarily large, as n can be increased, so that H0 can be discarded to a fixed threshold."}, {"heading": "1. There exist finite Bk and Bl such that", "text": "\"We are not in a position to bring about a solution,\" he said. \"We are not in a position to bring about a solution.\" \"We are not in a position to bring about a solution.\" \"No.\" \"No.\" \"No.\" \"No.\" \"No.\" \"No.\" \"No.\" \"No.\" \"No.\" \"No.\" \"No.\" \"\" No. \"\" No. \"\" No. \"\" \"No.\" \"\" No. \"\" \"No.\" \"\" No. \"\" \"No.\" \"No.\" \"No.\" \"No.\" \"No.\" No. \"\" No. \"\" No. \"\" No. \"No.\" No. \"No.\" No. \"No.\" No. \"No.\" No. \"No.\" No. \"No.\" No. \"No.\" No. \"No.\" No. \"No.\" No. \"No.\" No. \"No.\" No. \"No.\" No. \"No.\" No. \"No.\" No. \"No.\" No. \"No.\" No. \"\" No. \"No.\" \"No.\" No. \"No.\""}, {"heading": "3 Experiments", "text": "In this section we will empirically examine the performance of the proposed method on toys (Section 3.1) as well as on other HSIC problems (Section 3.2).We are interested in the performance of linear time tests for challenging problems requiring a large sample size in order to accurately show the dependency.The entire code can be found at http ps: / / github.com / wittawatj / fsic-test.We compare the proposed NFSIC tests with optimization (NFSIC opt) on five multivariate non-parametric tests. The N-FSIC2 test without optimization (NFSIC-med) functions as a baseline so that the effect of parameter optimization is clearly discernible. For pedagogical reasons we consider the original HSIC test by Gretton et al. [2005] denoted by QHSIC, which is a square time test. Nystr\u00f6m HSIC (Nystr\u00f6m HSIC) uses an approximation to the kernel matrices of X and Y when the HSIC is calculated."}, {"heading": "3.1 Toy Problems", "text": "We look at three toy problems: Same Gaussian (SG), Sinusoid (Sin) and Gaussian sign (GSign). The two variables are drawn independently from each other from the standard multivariate normal distribution, i.e., the HSH index N (0, Idx) and the Y index N (0, Idy), where Id is the d \u00b7 d identity matrix. This problem represents a case in which H0 holds. 2. FSF index is the probability density of Pxy. In the sinusoid problem, the dependence of X and Y is characterized by (X, Y). The dependence of XY is by (X, Y)."}, {"heading": "3.2 Real Problems", "text": "We are now examining the performance of our proposed test for real problems. Million Song Data (MSD) We are looking at a subset of Million Song Data2 [Bertin-Mahieux et al., 2011], in which each song (X) of 515,345 is represented by 90 characteristics, of which 12 characteristics are timbre average (across all segments) of the song, and 78 characteristics are timbre covariance. Most songs are western commercial tracks from 1922 to 2011. The goal is to detect the dependence between each song and its year of release (Y). We are using \u03b1 = 0.01, and repeating for 300 studies, in which the full sample is subsampled to n points in each study. Other settings are the same as in the toy problems. To ensure that the type I error is correct, we are using the permutation approach in the NFSIC tests to calculate the threshold. Figure 4b shows the test power as n rises from 500 to 2000. To simulate the case where we are showing the dependence on the H0 in the problem."}, {"heading": "Acknowledgement", "text": "We thank the Gatsby Charitable Foundation for its financial support. Most of this work was carried out by Zolt\u00e1n Szab\u00f3 as a research associate at the Gatsby Computational Neuroscience Unit at University College London.3VideoStory46K dataset: https: / / ivi.fnwi.uva.nl / isis / mediamill / datasets / videostory.php."}, {"heading": "R. J. Serfling. Approximation Theorems of Mathematical", "text": "Statistics. John Wiley & Sons, 2009.A. Smola, A. Gretton, L. Song, and B. Sch\u00f6lkopf. A hilbert space embedding for distributions. In International Conference on Algorithmic Learning Theory (ALT), pp. 13-31, 2007.B. K. Sriperumbudur, A. Gretton, K. Fukumizu, B. Sch\u00f6lkopf, and G. R. G. Lanckriet. Hilbert Space Embeddings and Metrics on Probability Measures. Journal of Machine Learning Research, 11: 1517-1561, 2010.I. Steinwart and A. Christmann. Support vector machines. Springer Science & Business Media, 2008.G. J. Sz\u00e9kely and M. L. Rizzo. Brownian distance covariance. The Annals of Applied Statistics, 3 (4): 1236-1265, 2009.G. J. Sz\u00e9kely, M. L. Rizzo, and N. K. Bakirov. Measuring and testing dependence by correlation of distances of the Peters."}, {"heading": "A Type-I Errors", "text": "In this section, we show that all tests in real problems have correct type I errors (i.e. the probability of rejecting H0 if it is true). We permutate the joint sample so that the dependence is broken to simulate cases in which H0 holds. Results are shown in Figure 5."}, {"heading": "B Redundant Test Locations", "text": "In Figure 6, t1 is fixed to the red star, while t2 is varied along the horizontal line; the objective value of the sinusoid problem described in Section 3.1 as a function of (t1, t2) is shown in the figure below; it can be seen that \u03bb-n decreases sharply when t2 is in the neighborhood of t1; this property implies that two locations too close do not maximize the objective function (i.e. the second feature does not contain additional information if it coincides with the first); in J > 2, the target decreases sharply when two locations are in the same neighborhood."}, {"heading": "C Test Power vs. J", "text": "Here we show empirically that this statement is not always true. Consider the example of a sinusoid toy with \u03c9 = 2 described in Section 3.1 (see also the left figure of Figure 7). By design, X and Y become dependent on this problem. We perform the NFSIC test with a sample size of n = 800, with J varying from 1 to 600. For each value of J, the test is repeated 500 times. In each experiment, the sample is redrawn and the J test sites are drawn by uniform (\u2212 \u03c0, \u03c0) 2. There is no optimization of the test sites. We use Gaussian cores for both X and Y and use mean heuristics to set the Gaussian widths to 1.8. Figure 7 shows the test sites as J sites. We observe that the test sites x4 do not rise monotonously because J-100 is a trading location. If J = 1, the difference of Gaussian widths to J-J and J-size is not increased appropriately J-J-size = J-J-strength."}, {"heading": "D Proof of Proposition 3", "text": "Let us recall sentence 3, sentence (A product of the Gaussian nuclei is characteristic and analytical). Let us leave k (x, x) = exp (\u2212 (x \u2212 x \u2032) > A (x \u2212 x \u2032) and l (y, y \u2032) = exp (\u2212 (y \u2212 y \u2032) > B (y \u2212 y \u2032))) Gaussian nuclei on Rdx \u00b7 Rdx or Rdy \u00b7 Rdy on positive definitive matrices A and B. Then g (x, y \u2032), (x \u2032, y \u2032)) = k (x, x \u2032) l (y, y \u2032) on characteristic and analytical (Rdx \u00b7 Rdy) \u00b7 (Rdx \u00d7 Rdy). Proof: z: (x >, y >) > and z \u2032: (x \u2032, y \u2032) > analytically on e.g."}, {"heading": "E Proof of Theorem 5", "text": "This is the independence test based on N-FSIC2."}, {"heading": "F Proof of Theorem 7", "text": "Recall Theorem 7, Theorem 7 (A lower limit for test performance).Let NFSIC2 (X, Y): = \u03bbn: = nu > \u03a3 \u2212 1u. Let K be a kernel class for k, L be a kernel class for l, and V be a collection with each element being a set of J locations."}, {"heading": "F.1 Notations", "text": "Let < A, B > F: = tr (A > B) be the inner product of Frobenius (V = V = V = V = V), and vice versa (A > F: = V = V) be the Frobenius norm. Write z: = (x, y) to a pair of X \u00b7 Y. We write t: = (v, w) to a pair of test sites of X \u00b7 Y. For abbreviation, an expectation about (x, y) (i.e., E (x, y) is written as Ez or Exy. Define k (x, v): = k (x, v) \u2212 Ex \u2032 k (x, v), and l (y, w): = l (y, w) \u2212 Ey \u2032 l (y \u2032 l (y \u2032, w). Let B2 (r): (r), xxxxx."}, {"heading": "F.2 Proof", "text": "+ + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + +"}, {"heading": "F.2.5 Bounding", "text": "To keep the notations clear, we will define the following abbreviations: b-b-b-b-b-b-b-b-b-b-b-b-b-b-b-b-b-b-b-b-b-b-b-b-b-b-b-b-b-b-b-b-b-b-b-b-b-b-b-b-b-b-b-b-b-b-b-b-b-b-b-b-b-b-b-b-b-b-b-b-b-b-b-b-b-b-b-b-b-b-b-b-b-b-b-b-b-b-b-b-b-b-b-b-b-b-b-b-b-b-b-b-b-b-b-b-b-b-b-b-b-b-b-b-b-b-b-b-b-b-b-b-b-b-b-b-b-b-b-b-b-b-b-b-b-b-b-b-b-b-b-b-b-b-b-b-b-b-b-b-b-b-b-b-b-b-b-b-b-b-b-b-b-b-b-b-b-b-b-b-b-b-b-b-b-b-b-b-b-b-b-b-b-b-b-b-b-b-b-b-b-b-b-b-b-b-b-b-b-b-b-b-b-b-b-b-b-b-b-b-b-b-b-b-b-b-b-b-b-b-b-b-b-b-b-b-b-b-b-b-b-b-b-b-b-b-b-b-b-b-b-b-b-b-b-b-b-b-b-b-b-b-b-b-b-b-b"}, {"heading": "Bounding", "text": "For the year ended December 31, 2009, the Company recorded a net loss of $0.1 million, or $0.01 per diluted share, compared to a net loss of $0.1 million, or $0.01 per diluted share, for the same period in 2007, or $0.01 per diluted share, for the same period in 2007, or $0.01 per diluted share, for the same period in 2007, or $0.01 per diluted share, for the same period in 2007, or $0.01 per diluted share, or $0.01 per diluted share, for the same period in 2007, or $0.01 per diluted share, or $0.01 per diluted share, or $0.01 per diluted share, or $0.01 per diluted share, or $0.01 per diluted share, or $0.01 per diluted share, or $0.01 per diluted share, or $0.01 per diluted $0.01 per diluted share, or $0.01 per diluted $0.01 per diluted $0.01 per diluted share, or $0.01 per diluted $0.01 per diluted $0.01 per diluted share, or $0.01 per diluted $0.01 per diluted $0.01 per diluted share, or $0.01 per diluted $0.01 per diluted share, or $0.01 per diluted $0.01 per diluted share, or $0.01 per diluted $0.01 per diluted share, or $0.01 per diluted $0.01 per diluted $0.01 per diluted $0.01 per diluted share, or $0.01 per diluted $0.01 per diluted $0.01 per diluted $0.01 per diluted $0.01 per diluted share, or $0.01 per diluted $0.01 per diluted $0.01 per diluted $0.01 per diluted $0.01 per diluted $0.01 per diluted $0.01 per diluted $0.01 per diluted $0.01 per diluted share, or $0.01 per diluted $0.01 per diluted $0.01 per diluted $0.01 per diluted $0.01 per diluted $0.01 per diluted $0.01 per diluted $0.01 per diluted diluted $0.01 diluted diluted $0.01 diluted diluted $0.00.00) diluted diluted $0.00.0"}, {"heading": "Bounding", "text": "Let us leave f1 (x, y) = ababa (x, v) l (x, v) k (x, v \") and f2 (b) = l (y, w\").We note that | f1 (x, y) | \u2264 (BBk, Bl) + and | f2 (y) | \u2264 (BBk, Bl) +. Thus, in Lemma we have 9 with E = 2 P (v, y) (v, y) | \u2264 (BBk, Bl) | \u2264 (BBk, Bl). Thus, in Lemma, we have 9 with E = 2 P (v, y)."}, {"heading": "Bounding", "text": "Leave f1 (x, y) = ab = k (x, v) l (y, w), f2 (x) = a \"= k (x, v\") and f3 (y) = b \"= l (y, w\"). We can see that | f1 (x, y) |, | f2 (x) |, | f3 (y) | \u2264 (B, Bk, Bl) +. Thus we have through Lemma 9 with E = 3, P (thus through Lemma 9 with E = 3)."}, {"heading": "Bounding", "text": "Similarly, other terms can be derived. \u2212 k \u2212 k \u2212 k \u2212 k \u2212 k \u2212 k \u2212 k \u2212 k \u2212 k \u2212 k \u2212 k \u2212 k \u2212 k \u2212 k \u2212 k \u2212 k (last term). \u2212 k \u2212 k \u2212 k (last term). \u2212 k \u2212 k (last term). \u2212 k \u2212 k (last term). \u2212 k \u2212 k (last term)."}, {"heading": "F.2.6 Union Bound for", "text": "(22) that the (n), (n), (n), (n), (n), (n), (n), (n), (n), (n), (n), (n), (n), (n), (n), (n), (n), (n), (n), (n), (n), (n), (n), (n), (n), (n), (n), (n), (n), (n), (n), (n), (n), (n, (n), (n), (n), (n), (n, (n), (n), (n, (n), (n), (n, (n), (n, (n), (n, (n), (n, (n), (n, (n), (n, (n), (n, (n), (n, (n), (n, (n), (n, (n), (n, (n), (n), (n, (n), (n), (n, (n), (n), (n, (n), (n, (n), (n), (n, (n), (n, (n), (n, (n), (n, (n), (n, (n), (n, (n), (n, (n, (n), (n, (n, (n), (n, (n), (n), (n, (n, (n, (n), (n, (n), (n, (n, (n), (n, (n), (n, (n), (n, (n), (n, (n, (n), (n), (n, (n, (), (), (n, (), (, (n, (, (n, n, n, n, n, n, n), (, (, n, n, (, n, n, n, n, n, n, n, n), (, (, n"}, {"heading": "G Helper Lemmas", "text": "In this section, the main results are shown in this working method. Lemma 8 (product to sum) (result to sum) (result to sum) (result to sum) (result to sum) (result to sum) (result to sum) (result to sum) (result to sum) (result to sum) (result to sum) (result to sum) (result to sum) (result to sum) (result to sum) (result to sum) (result to sum) (result to sum). (result to sum). (result to sum). (result to sum). (result to sum). (result to sum). (result to sum). (result to sum). (result to sum). (result to sum). (result to sum)."}, {"heading": "H External Lemmas", "text": "In this section we provide well-known results referred to in this thesis. Lemma 10 (Chwialkowski et al. [2015, Lemma 1]). If k is a limited, analytical nucleus (in the sense of definition 1) on Rd \u00b7 Rd, then all functions in the RKHS defined by k are analytical. Lemma 11 (Chwialkowski et al. [2015, Lemma 3]. Let us assign an injective mapping from the space of probability measurements into a space of analytical functions on Rd. Defined2VJ (P, Q) = J \u2211 j = 1 | P] (vj) \u2212 Hoxy \u2212 statistical (2, where VJ = {vi} Ji = 1 vector-rated i.e. random variables from a distribution that is absolutely continuous in terms of lebesgue measurement."}], "references": [{"title": "Fast Two-Sample Testing with Analytic Representations of Probability Measures", "author": ["K.P. Chwialkowski", "A. Ramdas", "D. Sejdinovic", "A. Gretton"], "venue": "In Advances in Neural Information Processing Systems (NIPS),", "citeRegEx": "Chwialkowski et al\\.,? \\Q1981\\E", "shortCiteRegEx": "Chwialkowski et al\\.", "year": 1981}, {"title": "Interpretable Distribution Features with Maximum Testing Power. 2016", "author": ["W. Jitkrittum", "Z. Szab\u00f3", "K. Chwialkowski", "A. Gretton"], "venue": "URL http://arxiv.org/abs/1605.06796", "citeRegEx": "Jitkrittum et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Jitkrittum et al\\.", "year": 2016}, {"title": "Fourier analysis on groups", "author": ["W. Rudin"], "venue": null, "citeRegEx": "Rudin.,? \\Q2011\\E", "shortCiteRegEx": "Rudin.", "year": 2011}, {"title": "Approximation Theorems of Mathematical Statistics", "author": ["R.J. Serfling"], "venue": null, "citeRegEx": "Serfling.,? \\Q2009\\E", "shortCiteRegEx": "Serfling.", "year": 2009}], "referenceMentions": [{"referenceID": 0, "context": "We show that our test is consistent, despite a finite number of analytic features being used, via a generalization of arguments in Chwialkowski et al. [2015]. As in recent work on two-sample testing by Jitkrittum et al.", "startOffset": 131, "endOffset": 158}, {"referenceID": 0, "context": "We show that our test is consistent, despite a finite number of analytic features being used, via a generalization of arguments in Chwialkowski et al. [2015]. As in recent work on two-sample testing by Jitkrittum et al. [2016], our test is adaptive in the sense that we choose our features on a held-out validation set to optimize a lower bound on the test power.", "startOffset": 131, "endOffset": 227}, {"referenceID": 0, "context": "Using the same argument as in Chwialkowski et al. [2015], since k and l are analytic, \u03c1 is also analytic, and the set of roots R := {(v,w) | \u03c1(v,w) = 0} has Lebesgue measure zero.", "startOffset": 30, "endOffset": 57}, {"referenceID": 0, "context": "The proof of the second claim has a very similar structure to the proof of Proposition 2 of Chwialkowski et al. [2015]. Assume that H1 holds.", "startOffset": 92, "endOffset": 119}, {"referenceID": 2, "context": "Lemma 12 (Bochner\u2019s theorem [Rudin, 2011]).", "startOffset": 28, "endOffset": 41}], "year": 2016, "abstractText": "A new computationally efficient dependence measure, and an adaptive statistical test of independence, are proposed. The dependence measure is the difference between analytic embeddings of the joint distribution and the product of the marginals, evaluated at a finite set of locations (features). These features are chosen so as to maximize a lower bound on the test power, resulting in a test that is data-efficient, and that runs in linear time (with respect to the sample size n). The optimized features can be interpreted as evidence to reject the null hypothesis, indicating regions in the joint domain where the joint distribution and the product of the marginals differ most. Consistency of the independence test is established, for an appropriate choice of features. In real-world benchmarks, independence tests using the optimized features perform comparably to the state-of-the-art quadratictime HSIC test, and outperform competing O(n) and O(n log n) tests.", "creator": "LaTeX with hyperref package"}}}