{"id": "1610.04989", "review": {"conference": "EMNLP", "VERSION": "v1", "DATE_OF_SUBMISSION": "17-Oct-2016", "title": "Cached Long Short-Term Memory Neural Networks for Document-Level Sentiment Classification", "abstract": "Recently, neural networks have achieved great success on sentiment classification due to their ability to alleviate feature engineering. However, one of the remaining challenges is to model long texts in document-level sentiment classification under a recurrent architecture because of the deficiency of the memory unit. To address this problem, we present a Cached Long Short-Term Memory neural networks (CLSTM) to capture the overall semantic information in long texts. CLSTM introduces a cache mechanism, which divides memory into several groups with different forgetting rates and thus enables the network to keep sentiment information better within a recurrent unit. The proposed CLSTM outperforms the state-of-the-art models on three publicly available document-level sentiment analysis datasets.", "histories": [["v1", "Mon, 17 Oct 2016 07:28:06 GMT  (139kb,D)", "http://arxiv.org/abs/1610.04989v1", "Published as long paper of EMNLP2016"]], "COMMENTS": "Published as long paper of EMNLP2016", "reviews": [], "SUBJECTS": "cs.CL cs.NE", "authors": ["jiacheng xu", "danlu chen", "xipeng qiu", "xuanjing huang"], "accepted": true, "id": "1610.04989"}, "pdf": {"name": "1610.04989.pdf", "metadata": {"source": "CRF", "title": "Cached Long Short-Term Memory Neural Networks for Document-Level Sentiment Classification", "authors": ["Jiacheng Xu", "Danlu Chen", "Xipeng Qiu", "Xuanjing Huang"], "emails": ["jcxu13@fudan.edu.cn", "dlchen13@fudan.edu.cn", "xpqiu@fudan.edu.cn", "xjhuang@fudan.edu.cn"], "sections": [{"heading": null, "text": "Recently, neural networks have achieved great successes in sentiment classification due to their ability to mitigate feature engineering, but one of the remaining challenges is to model long texts in sentiment classification at the document level under a recurring architecture because the storage unit is insufficient. To address this problem, we present a Cached Long Short-Term Memory Neural Networks (CLSTM) to capture all semantic information in long texts. CLSTM introduces a cache mechanism that splits memory into multiple groups at different forgetfulness rates, allowing the network to better hold sentiment information within a recurring unit. CLSTM proposes to outperform the state-of-the-art models on three publicly available document-level sentimental analysis datasets."}, {"heading": "1 Introduction", "text": "This year, we have reached the point where it is only half way through until we reach an agreement in the next few days."}, {"heading": "2 Related Work", "text": "In this section, we present briefly related work in two areas: first, we discuss the existing approaches to classifying mood at the documentation level; second, we discuss some variants of the LSTM that address the problem of storing long-term information."}, {"heading": "2.1 Document-level Sentiment Classification", "text": "The most difficult task in sentiment analysis (Pang and Lee, 2008) is to derive the sentiment polarity or intensity of an entire document; the most difficult part is that not every part of the document is equally informative in order to derive the mood of the entire document (Pang and Lee, 2004; Yessenalina et al., 2010) Most of these methods have been studied and researched over years (Wilson et al., 2005; Pang and Lee, 2008; Pak and Paroubek, 2010; Yessenalina et al., 2010; Moraes et al., 2013); most of these methods are based on traditional machine learning algorithms and require effective handcrafted traits. Recently, neural network-based methods have become dominant due to their ability to learn discriminatory traits from data (Socher et al., 2013; Le and Mikolov et al., 2015a)."}, {"heading": "2.2 Memory Augmented Recurrent Models", "text": "Although it is generally accepted that LSTM has more durable storage units than RNNs, it still suffers from the \"forgetting\" of information that is too far away from the current point (Le et al., 2015; Karpathy et al., 2015). Such a scaling problem of LSTMs is crucial to expand some previous work at the sentence level on sentiment analysis at the document level. Various models have been proposed to increase the capacity of LSTMs to store long-distance information (Le et al., 2015; Salehinejad, 2016) and two types of approaches are gaining in appeal. One is to extend LSTM by an external memory (Sukhbaatar et al., 2015; Monz, 2016), but they are of poor performance on time due to the huge external storage matrix. Unlike these methods, we fully exploit the potential of LSTM's internal storage by adjusting its formatting rates."}, {"heading": "3 Long Short-Term Memory Networks", "text": "Long-term memory network (LSTM) (Hochreiter and Schmidhuber, 1997) is a typical recursive neural network that alleviates the problem of gradient diffusion and explosion. LSTM can capture the long dependencies in a sequence by introducing a memory unit and a gate mechanism aimed at determining how the information held in the memory cell is used and updated. Formally, the update of each LSTM component can be formalized as: i (t) = \u03c3 (Wix (t) + Uih (t \u2212 1), (1) f (t) = \u03c3 (Wox (t) + Uoh (t \u2212 1), (3) c (t) = tanh (t \u2212 1)), (t) = gate (Wcx (t) + Uch (t \u2212 1)."}, {"heading": "4 Cached Long Short-Term Memory Neural Network", "text": "The LSTM study shows that the long-term and short-term dependencies at the level of LSTM tasks can be neglected, because the LSTM tasks are not able to capture the long-term and short-term dependencies of the LSTM tasks, but in those areas where the LSTM approaches (and hence the memory structures) are not yet as advanced as they are. (D) The LSTM approaches are proposed by Greff et al to capture information in a longer step by introducing a cache mechanism to achieve better control and balance. (D) In addition, we adopt a specific variant of the LSTM approaches. (2015) The Coupled Input and Forget Gate LSTM (CIFG-LSTM).Coupled Input and Forget Gate LSTM studies show that the LSTM approaches are comparable."}, {"heading": "5 Training", "text": "The goal of our model is to minimize the intersection error of the predicted and true distributions. In addition, the goal contains an L2 regularization term for all parameters. Suppose we have m pairs of tension and markings (w (i) 1: Ti, y (i) mi = 1, the goal is to minimize the objective function J (\u03b8): J (\u03b8) = \u2212 1 m \u2211 i = 1 logp (i) y (i) + \u03bb 2 | | \u03b8 | | 2, (16), where \u03b8 denotes all traceable parameters of our model."}, {"heading": "6 Experiment", "text": "In this section, we examine the empirical results of our model based on three datasets for classifying mood at the document level. The results show that the proposed model outperforms competing models in several respects when it comes to modelling long texts."}, {"heading": "6.1 Datasets", "text": "Most existing film review classification data sets, such as the Stanford Sentiment Treebank (Socher et al., 2013), consist of multi-sentence short paragraphs that cannot evaluate the effectiveness of the model under the circumstances of encoding long texts. We evaluate our model using three popular real-world data sets, Yelp 2013, Yelp 2014 and IMDB. Table 1 shows the statistical information of the three data sets. All of these data sets can be made publicly available1. We process and divide the data sets in the same way as Tang et al. (2015b). \u2022 Yelp 2013 and Yelp 2014 are review data sets derived from Yelp Dataset Challenge2 of 2013 and 2014, respectively. The sentimental polarity of each review is 1 star to 5 stars, reflecting consumer attitudes and opinions toward restaurants. \u2022 IMDB is a popular movie review data set with 849 seconds of reviews, each ranging from an average of 4.1 to 6 seconds in length."}, {"heading": "6.2 Evaluation Metrics", "text": "We use Accuracy (Acc.) and MSE as evaluation measures for mood classification. Accuracy is a standard measurement of the overall result of the classification, and Mean Squared Error (MSE) is used to determine the discrepancies between predicted sentiment labels and ground truth labels."}, {"heading": "6.3 Baseline Models", "text": "We compare our model, CLSTM and B-CLSTM, with the following basic methods: 1http: / / ir.hit.edu.cn / \u02dc dytang / paper / acl2015 / dataset.7z2http: / / www.yelp.com / dataset _ challenge \u2022 CBOW summarizes the word vectors and applies a nonlinearity, followed by a Softmax classification layer. \u2022 JMARS is one of the most advanced recommendation algorithms that uses users and aspects of verification through collaborative filtering and topic modeling. \u2022 CNN UPNN (CNN) (Tang et al., 2015b) can be downgraded as CNN (Kim, 2014). Multiple filters are sensitive to capture various semantic characteristics during the generation of a representation in a bottom-up mode. \u2022 RNN is a basic sequential model for model texts (Elman, 1991) \u2022 STLber is a gateway network with a 1997 gateway BLM and BLM-BLM-longer mechanism."}, {"heading": "6.4 Hyper-parameters and Initialization", "text": "For parameter configuration, we choose the parameters for validation mainly according to the classification accuracy, since MSE always has a strong correlation with accuracy. The dimension of the pre-formed word vectors is 50. We use 120 as the dimension of hidden units and select the weight drop below {5e \u2212 4, 1e \u2212 4, 1e \u2212 5}. We use Adagrad (Duchi et al., 2011) as the optimizer and its initial learning rate is 0.01. Batch size is chosen below {32, 64, 128} to be efficient. For CLSTM, the number of memory groups is selected according to each data set, which will be discussed later. We keep the total number of hidden units unchanged. In view of a total of 120 neurons, for example, there are four memory groups and each of them has 30 neurons. This makes the model comparable to (B) LSTM. Table 3 shows the optimal hyperparameter configurations for each data set."}, {"heading": "6.5 Results", "text": "The classification accuracy and mean square error (MSE) of our models compared to other competitive models are shown in Table 2. Comparing our models with other neural network models, we have several important insights. 1. Among all unidirectional sequential models, RNN fails to capture and store semantic characteristics, while vanilla LSTM retains sentimental messages much longer than RNN. It shows that internal memory plays a key role in text modeling. CIFG-LSTM.2. Using the bidirectional architecture, models could look backwards and forwards from a global perspective to capture characteristics. In mood analysis, when users express their opinions at the beginning of their review, they may forget these cluds.3. The proposed CLSTM proposes the CIFG-LSTM and Vanilla LSTM, even surpassing the bidirectional models."}, {"heading": "6.6 Rate of Convergence", "text": "We compare the convergence rates of our models, including CIFG-LSTM, CIFG-BLSTM and BCLSTM, and the base models (LSTM and BLSTM). We configure the hyperparameter to ensure that each competing model has roughly the same number of parameters, and different models have shown different convergence rates in Figure 3. In terms of convergence rate, B-CLSTM beats other competing models. The reason why BCLSTM converges faster is that splitting memory groups during the training process can be considered a better initialization and limitation."}, {"heading": "6.7 Effectiveness on Grouping Memory", "text": "Figure 4 shows the best predictive accuracy (Y-axis) achieved during validation with different numbers of memory groups on all data sets. We can see from the chart that our model exceeds the basic method. In Yelp 2013, when we split memory into 4 groups, we achieve the best result of all memory groups tested. We can observe the declining trends when we select more than 5 groups. To make fair comparisons, we determine the total number of neurons in our model to be identical to vanilla LSTM. Therefore, the more groups we divide, the fewer neurons belong to each group, resulting in a worm capacity, than those that have sufficient neurons for each group."}, {"heading": "6.8 Sensitivity on Document Length", "text": "We also examine the performance of our model on IMDB when it encodes documents of different lengths. Test samples are divided into 10 groups in terms of length. Figure 5 allows us to draw several elaborate conclusions. 1. Bidirectional models perform much better than their counterparts. 2. The overall performance of B-CLSTM is better than CIFG-BLSTM. This means that our model is adaptive for both short texts and long documents. Furthermore, our model shows a slightly better performance than CIFG-LSTM compared to CIFG-BLSTM.3. CBOW is slightly better than CIFG-LSTM because LSTM forgets a large amount of information during unidirectional propagation."}, {"heading": "7 Conclusion", "text": "In this paper, we address the problem of effective analysis of the feeling of texts at the document level in an RNN architecture. Similar to the human memory structure, low-forgetfulness memory captures global semantic traits, while high-forgetfulness memory captures local semantic traits. Empirical results from three real-world document level review datasets show that our model significantly outperforms modern models. In future work, we will develop a strategy to dynamically adjust forgetfulness rates for fine-grained sentiment analysis at the document level."}, {"heading": "Acknowledgments", "text": "We appreciate the constructive work of Xinchi Chen. We would also like to thank the anonymous critics for their valuable comments, which were partially funded by the National Natural Science Foundation of China (No. 61532011 and 61672162), the National High Technology Research and Development Program of China (No. 2015AA015408)."}], "references": [], "referenceMentions": [], "year": 2016, "abstractText": "Recently, neural networks have achieved great<lb>success on sentiment classification due to their<lb>ability to alleviate feature engineering. How-<lb>ever, one of the remaining challenges is to<lb>model long texts in document-level sentiment<lb>classification under a recurrent architecture<lb>because of the deficiency of the memory unit.<lb>To address this problem, we present a Cached<lb>Long Short-Term Memory neural networks<lb>(CLSTM) to capture the overall semantic in-<lb>formation in long texts. CLSTM introduces<lb>a cache mechanism, which divides memory<lb>into several groups with different forgetting<lb>rates and thus enables the network to keep<lb>sentiment information better within a recur-<lb>rent unit. The proposed CLSTM outperforms<lb>the state-of-the-art models on three publicly<lb>available document-level sentiment analysis<lb>datasets.", "creator": "LaTeX with hyperref package"}}}