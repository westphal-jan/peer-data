{"id": "1610.05120", "review": {"conference": "ICML", "VERSION": "v1", "DATE_OF_SUBMISSION": "17-Oct-2016", "title": "Lazifying Conditional Gradient Algorithms", "abstract": "Conditional gradient algorithms (also often called Frank-Wolfe algorithms) are popular due to their simplicity of only requiring a linear optimization oracle and more recently they also gained significant traction for online learning. While simple in principle, in many cases the actual implementation of the linear optimization oracle is costly. We show a general method to lazify various conditional gradient algorithms, which in actual computations leads to several orders of magnitude of speedup in wall-clock time. This is achieved by using a faster separation oracle instead of a linear optimization oracle, relying only on few linear optimization oracle calls.", "histories": [["v1", "Mon, 17 Oct 2016 14:01:25 GMT  (587kb)", "http://arxiv.org/abs/1610.05120v1", "18 pages and 16 pages of computational results"], ["v2", "Fri, 30 Dec 2016 16:53:44 GMT  (4189kb,D)", "http://arxiv.org/abs/1610.05120v2", "18 pages and 16 pages of computational results"]], "COMMENTS": "18 pages and 16 pages of computational results", "reviews": [], "SUBJECTS": "cs.DS cs.LG", "authors": ["g\u00e1bor braun", "sebastian pokutta", "daniel zink"], "accepted": true, "id": "1610.05120"}, "pdf": {"name": "1610.05120.pdf", "metadata": {"source": "META", "title": "Lazifying Conditional Gradient Algorithms", "authors": ["G\u00e1bor Braun", "Sebastian Pokutta"], "emails": ["gabor.braun@gatech.edu", "sebastian.pokutta@gatech.edu", "daniel.zink@gatech.edu"], "sections": [{"heading": null, "text": "ar Xiv: 161 0.05 120v 1 [cs.D S] 1 7Conditional gradient algorithms (also often referred to as Frank Wolfe algorithms) are popular for their simplicity in requiring only a linear optimization oracle, and more recently they have gained traction for online learning. Although they are simple in principle, the actual implementation of the oracle of linear optimization is costly in many cases. We show a general method of calming various conditional gradient algorithms that results in several orders of magnitude of acceleration in the wall clock time in actual calculations, achieved by using a faster separation oracle instead of a linear optimizer oracle based on only a few linear optimization oracle calls."}, {"heading": "1 Introduction", "text": "It is an important technology that leads to significant advantages from both a theoretical and an application perspective. Standard gradient-based methods are often applied to many real-world problems due to their simplicity and ease of application. To maintain feasibility, it is necessary that it is potentially very expensive, especially for complex realizable regions in very large dimensions. Therefore, we have attracted a lot of attention recently, especially to the Frank Wolfe algorithm [Frank and Wolfe, 1956] (also known as conditional gradient descent [Levitin and Polyak, 1966]. See also [Jaggi, 2013] for an overview) and its online version [Hazan and Kale, 2012]. These methods shy away from project planning and tend to use a linear one in order to stay within the feasible region."}, {"heading": "2 Preliminaries", "text": "A function f is L-Lipschitz if (y) -f (x) | \u2264 L-y-x-x for all x, y-dom f. A convex function f is smooth with curvature at most C if f (g (1) -f (x) + \u03b3 f (x) (y \u2212 x) + C2 / 2 for all x, y-dom f and 0 \u2264 1. A function f is S-strongly convex if f (y) -f (x) (y \u2212 x) + S2 (y \u2212 x) + C2 for all x, y-dom f. Unless otherwise specified, Lipschitz will measure continuity and strong convexity in the norm. Furthermore, Br (x): = {y \u2212 y \u2212 y \u2212 x \u00b2 for all x, y-dom f."}, {"heading": "3 Lazy Offline Conditional Gradients", "text": "We will first show how the Frank Wolfe algorithms in Garber and Hazan [2013] and Garber and Meshi [2016] can be defused by a weak separation oracle like in Oracle 1. Note that the two cases in Oracle 1 are not mutually exclusive: the oracle could return a y-P with c (x \u2212 y) > \u03a6 / K, while still c (x \u2212 z) \u2264 \u0439 stands for all z-P."}, {"heading": "3.1 Lazy Conditional Gradient: a basic example", "text": "Starting with the simplest convergence rate O (1 / T), its simplicity makes it an illustrative example of the main idea of lacification. The lazy algorithm (algorithm 1) maintains an upper limit in terms of convergence rate, which underpins its eagerness for progress in the search for an improved point. Step size is then chosen to be (approximately) minimized. The lazy algorithms (algorithm 1) are subject to an upper limit. (C.Theorem 3.1). (C.f., [Jaggi, 2013, Theorem 1]. Step size is chosen to be (approximately) minimized."}, {"heading": "3.2 Lazy Pairwise Conditional Gradients", "text": "In this section we provide a lazy variant (algorithm 2) of the Garber and Meshi Pairwise Conditional Gradient Algorithm [2016], which uses separation instead of linear optimization. We assume that the feasible region is a 0 / 1 polytopic, in the form P = {x \u00b2 Rn \u00b2 x \u00b2 0, Ax = b \u00b2.Algorithm 2 Lazy Pairwise Conditional Gradients (LPCG) Require: polytopic P, smooth and S-strong convex function f with curvature C, accuracy K > 1, formance K = b}.Algorithm 2 Lazy Pairwise Conditional Gradients (LPCG) Require: polytopic P, smooth and S-strong convex function f with curvature C > 1, accuracy K > 1, formance K > t nonincreasing step-sizes Ensure: xt points 1: x1: x1 @ P arbitrary gradients and voice 0."}, {"heading": "3.3 Lazy Local Conditional Gradients", "text": "In this section we provide a lazy version (Algorithm3) of conditional gradient algorithms (cf. Garber and Hazan [2013]. Let P'Rn be any polytopic, D denotes an upper boundary based on smoothness: Remember that a convex function f (y) \u2212 f (x). \u2212 f (x) < f (x)."}, {"heading": "4 Lazy Online Conditional Gradients", "text": "In this section we lacify the online gradient algorithm of Hazan and Kale (2012) over arbitrary polytopes P = {x-Rn | Ax-Rn), resulting in algorithm 5. \u2212 In this section we slightly improve the constant factors by providing [Hazan and Kale, 2012, Lemma 3.1] with a better estimate of solving a square inequality resulting from strong contexts. \u2212 In this section we improve the constant factors by providing [Hazan and Kale, 2012, Lemma 3.1] with a better estimate (about solving a square inequality resulting from strong contexts. \u2212 In this section we do 2: \"The normative factors K, b, S, s; diameter D Ensure: xt points 1 to T \u2212 1."}, {"heading": "4.1 Stochastic and Adversarial Versions", "text": "In addition to the offline algorithms in Section 3, we will now deduce different versions from the online case (the cases presented here are similar to those in Hazan and Kale [2012] and therefore provide them without proof. (For stochastic cost functions ft), we will obtain boundaries from theorem 4.1 (i) similar to [Hazan and Kale, 2012, theorems 4.1 and 4.3] (excluding the cost of correcting an inaccuracy in the original reasoning).The proof is analogous and is therefore omitted, but note that the results of [Hazan and Kale] (excluding [Hazan and Kale] 4.1 \u2212 Y2 \u2212 Y1 \u2212 Y2 \u2212 Y1 \u2212 Y2 \u2212 Y2 \u2212 Y2 \u2212 Y2 Y2 Y2 Y2 Y2 Y2 Y2 Y2 Y2 Y2 Y2 Y2 Y2 Y2 Y2 Y2. Let us anticipate the convex functions of i.d with E [ft] = f."}, {"heading": "5 Experiments", "text": "We implemented and compared algorithm 2 (LPCG) with the pairwise conditional gradient algorithm (PCG) in Garber and Meshi [2016], and we implemented and compared algorithm 5 (LOCG) with the online frank wolf algorithm (OCG) of Hazan and Kale [2012]. We did not implement the variant in Garber and Hazan [2013] because the variant in Garber and Meshi [2016] is more efficient for the cases considered here; our method applies to both, as has already been shown. We implemented all algorithms in Python 2.7 with critical functions cythoned for performance, and used Numpy and MKL for arithmetic operations. We used these packages from the Anaconda 2.5 distribution. We used Gurobi 6.5 [Gurobi Optimization, 2016] as a black box solver for the weak separator, once we found a feasible solution, by using a callback option."}, {"heading": "5.1 Computational results", "text": "We looked at polytopes where the underlying optimization problem is simple (spanning trees) and where it is NP-hard (Maximum Cut, Traveling Salesman Problem, Quadratic Unconstrained Boolean Optimization [Dash, 2013], as well as various examples from MIPLIB [Achterberg et al., 2006, Koch et al., 2011]). Note that in real-world examples, state-of-the-art solutions such as Gurobi or CPLEX can solve these NP-hard problems in a reasonable amount of time and thus enable real-world learning via complex structure.We tested two types of loss functions cx + b with c, + 1] n and b [0, 1] n and b [0, 1] n with optimization of the square functions of the form b \u2212 x [22] n, similar to those in [Hazan and Kale, 2012]."}, {"heading": "5.1.1 Online Results", "text": "For the online gradient algorithms, the left column uses linear loss functions, the right uses quadratic loss functions over the same polytopy. We used the flow-based formulation for Hamilton's cycles in graphs, i.e., the Travelling Salesman (TSP) problem for graphs with 11 and 16 nodes (Figures 1 and 2). While the oracle problem can be solved in a reasonable amount of time for these instances, we used the standard formulation of the cut polytop for graphs with 23 and 28 nodes (Figures 3 and 4). Another set of NP-hard instances on which we tested our algorithm are the square unrestricted Boolean Optimization (QUBO) instances defined on Chimera graphs [Dash, 2013], which are available athttp: / / researcher.watson.ibm.com / / files / imchera-data.we"}, {"heading": "5.1.2 Offline Results", "text": "We tested the paired conditional gradient algorithmMIPLIB instanceseil33-2, air04, eilB101, nw04, disctom, m100n500k4r1 (Figures 12, 13 and 14) with square loss functions only, as linear optimization immediately finds the optimum of a linear loss function. As we inherit structural limitations of the PCG to the feasible region, the problem repertoire is limited in this case. Similar to the online case, we observe a significant acceleration of the LPCG compared to the PCG due to the shorter iteration of the rotten algorithm LPCG."}, {"heading": "5.2 Performance improvements, parameter sensitivity, and tuning", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "5.2.1 Effect of caching", "text": "As already mentioned, lazy algorithms have two improvements: caching and early termination. Figure 15 shows the effect of caching by comparing OCG (no caching, no early termination), LOCG (caching and early termination only) and LOCG (early termination only). We did not include a caching OCG variant because caching without early termination makes little sense: in each iteration, a new linear optimization problem must be solved; previous solutions can hardly be reused as they are probably not optimal for the new linear optimization problem."}, {"heading": "5.2.2 Effect of rate amplification", "text": "A major difference between the rotten and the non-rotten algorithms is that rotten algorithms converge at a certain rate, which is controlled by setting \u03a6t in line 8 of algorithm 5, line 4 of algorithm 3 and line 4 of algorithm 2. In contrast, the non-rotten algorithms can in principle converge faster, since they are parameter-free and automatically \"adapt\" to the underlying functional family. However, we are free to choose a more aggressive rate, which we could try to prove with rotten algorithms. This can be done simply by scaling \u03b1 t with an additional term t \u2212 \u03b1, just before transferring it to the weak separation routine, where \u03b1 is an adjustment coefficient of the convergence rate.We considered the rate as an amplification rate only for the online LOCG algorithm, with the subexential convergence rate being much more suitable for improvements, but we always considered the following two options:"}], "references": [{"title": "Regret in online combinatorial optimization", "author": ["J.-Y. Audibert", "S. Bubeck", "G. Lugosi"], "venue": "Mathematics of Operations Research,", "citeRegEx": "Audibert et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Audibert et al\\.", "year": 2013}, {"title": "Following the perturbed leader for online structured learning", "author": ["A. Cohen", "T. Hazan"], "venue": "In Proceedings of the 32nd International Conference on Machine Learning", "citeRegEx": "Cohen and Hazan.,? \\Q2015\\E", "shortCiteRegEx": "Cohen and Hazan.", "year": 2015}, {"title": "A note on QUBO instances defined on", "author": ["S. Dash"], "venue": "Chimera graphs. preprint arXiv:1306.1202,", "citeRegEx": "Dash.,? \\Q2013\\E", "shortCiteRegEx": "Dash.", "year": 2013}, {"title": "An algorithm for quadratic programming", "author": ["M. Frank", "P. Wolfe"], "venue": "Naval research logistics quarterly,", "citeRegEx": "Frank and Wolfe.,? \\Q1956\\E", "shortCiteRegEx": "Frank and Wolfe.", "year": 1956}, {"title": "A linearly convergent conditional gradient algorithm with applications to online and stochastic optimization", "author": ["D. Garber", "E. Hazan"], "venue": "arXiv preprint arXiv:1301.4666,", "citeRegEx": "Garber and Hazan.,? \\Q2013\\E", "shortCiteRegEx": "Garber and Hazan.", "year": 2013}, {"title": "Linear-memory and decomposition-invariant linearly convergent conditional gradient algorithm for structured polytopes", "author": ["D. Garber", "O. Meshi"], "venue": "arXiv preprint,", "citeRegEx": "Garber and Meshi.,? \\Q2016\\E", "shortCiteRegEx": "Garber and Meshi.", "year": 2016}, {"title": "Solving combinatorial games using products, projections and lexicographically optimal bases", "author": ["S. Gupta", "M. Goemans", "P. Jaillet"], "venue": "arXiv preprint arXiv:1603.00522,", "citeRegEx": "Gupta et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Gupta et al\\.", "year": 2016}, {"title": "Introduction to online convex optimization", "author": ["E. Hazan"], "venue": "Foundations and Trends in Optimization,", "citeRegEx": "Hazan.,? \\Q2016\\E", "shortCiteRegEx": "Hazan.", "year": 2016}, {"title": "Projection-free online learning", "author": ["E. Hazan", "S. Kale"], "venue": "arXiv preprint arXiv:1206.4657,", "citeRegEx": "Hazan and Kale.,? \\Q2012\\E", "shortCiteRegEx": "Hazan and Kale.", "year": 2012}, {"title": "Revisiting Frank\u2013Wolfe: Projection-free sparse convex optimization", "author": ["M. Jaggi"], "venue": "In Proceedings of the 30th International Conference on Machine Learning", "citeRegEx": "Jaggi.,? \\Q2013\\E", "shortCiteRegEx": "Jaggi.", "year": 2013}, {"title": "Efficient algorithms for online decision problems", "author": ["A. Kalai", "S. Vempala"], "venue": "Journal of Computer and System Sciences,", "citeRegEx": "Kalai and Vempala.,? \\Q2005\\E", "shortCiteRegEx": "Kalai and Vempala.", "year": 2005}, {"title": "Mathematical Programming Computation, 3(2):103\u2013163, 2011", "author": ["T. Koch", "T. Achterberg", "E. Andersen", "O. Bastert", "T. Berthold", "R.E. Bixby", "E. Danna", "G. Gamrath", "A.M. Gleixner", "S. Heinz", "A. Lodi", "H. Mittelmann", "T. Ralphs", "D. Salvagnin", "D.E. Steffy", "K. Wolter. MIPLIB"], "venue": "doi: 10.1007/s12532-011-0025-9. URL http://mpc.zib.de/index.php/MPC/article/view/56/28.", "citeRegEx": "Koch et al\\.,? 2010", "shortCiteRegEx": "Koch et al\\.", "year": 2010}, {"title": "On the global linear convergence of Frank\u2013Wolfe optimization variants", "author": ["S. Lacoste-Julien", "M. Jaggi"], "venue": "Advances in Neural Information Processing Systems,", "citeRegEx": "Lacoste.Julien and Jaggi.,? \\Q2015\\E", "shortCiteRegEx": "Lacoste.Julien and Jaggi.", "year": 2015}, {"title": "Constrained minimization methods", "author": ["E.S. Levitin", "B.T. Polyak"], "venue": "USSR Computational mathematics and mathematical physics,", "citeRegEx": "Levitin and Polyak.,? \\Q1966\\E", "shortCiteRegEx": "Levitin and Polyak.", "year": 1966}], "referenceMentions": [{"referenceID": 3, "context": "Hence projectionfree methods gained a lot of attention recently, in particular the Frank-Wolfe algorithm [Frank and Wolfe, 1956] (also known as conditional gradient descent [Levitin and Polyak, 1966]; see also [Jaggi, 2013] for an overview) and its online version [Hazan and Kale, 2012].", "startOffset": 105, "endOffset": 128}, {"referenceID": 13, "context": "Hence projectionfree methods gained a lot of attention recently, in particular the Frank-Wolfe algorithm [Frank and Wolfe, 1956] (also known as conditional gradient descent [Levitin and Polyak, 1966]; see also [Jaggi, 2013] for an overview) and its online version [Hazan and Kale, 2012].", "startOffset": 173, "endOffset": 199}, {"referenceID": 9, "context": "Hence projectionfree methods gained a lot of attention recently, in particular the Frank-Wolfe algorithm [Frank and Wolfe, 1956] (also known as conditional gradient descent [Levitin and Polyak, 1966]; see also [Jaggi, 2013] for an overview) and its online version [Hazan and Kale, 2012].", "startOffset": 210, "endOffset": 223}, {"referenceID": 8, "context": "Hence projectionfree methods gained a lot of attention recently, in particular the Frank-Wolfe algorithm [Frank and Wolfe, 1956] (also known as conditional gradient descent [Levitin and Polyak, 1966]; see also [Jaggi, 2013] for an overview) and its online version [Hazan and Kale, 2012].", "startOffset": 264, "endOffset": 286}, {"referenceID": 3, "context": "Hence projectionfree methods gained a lot of attention recently, in particular the Frank-Wolfe algorithm [Frank and Wolfe, 1956] (also known as conditional gradient descent [Levitin and Polyak, 1966]; see also [Jaggi, 2013] for an overview) and its online version [Hazan and Kale, 2012]. These methods eschew the projection step and rather use a linear optimization oracle to stay within the feasible region. While convergence rates and regret bounds are often suboptimal, in many cases the gain due to only having to solve a single linear optimization problem over the feasible region in every iteration still leads to significant computational advantages (see e.g., [Hazan and Kale, 2012, Section 5]). This led to conditional gradients algorithms being often used for e.g., online optimization andmore generallymachine learning, especially because they also naturally generate sparse distributions over the extreme points of the feasible region. Further increasing the relevance of these methods, it was shown recently in Garber and Hazan [2013], Lacoste-Julien and Jaggi [2015], Garber and Meshi [2016] that conditional gradient methods often achieve linear convergence.", "startOffset": 106, "endOffset": 1048}, {"referenceID": 3, "context": "Hence projectionfree methods gained a lot of attention recently, in particular the Frank-Wolfe algorithm [Frank and Wolfe, 1956] (also known as conditional gradient descent [Levitin and Polyak, 1966]; see also [Jaggi, 2013] for an overview) and its online version [Hazan and Kale, 2012]. These methods eschew the projection step and rather use a linear optimization oracle to stay within the feasible region. While convergence rates and regret bounds are often suboptimal, in many cases the gain due to only having to solve a single linear optimization problem over the feasible region in every iteration still leads to significant computational advantages (see e.g., [Hazan and Kale, 2012, Section 5]). This led to conditional gradients algorithms being often used for e.g., online optimization andmore generallymachine learning, especially because they also naturally generate sparse distributions over the extreme points of the feasible region. Further increasing the relevance of these methods, it was shown recently in Garber and Hazan [2013], Lacoste-Julien and Jaggi [2015], Garber and Meshi [2016] that conditional gradient methods often achieve linear convergence.", "startOffset": 106, "endOffset": 1081}, {"referenceID": 3, "context": "Hence projectionfree methods gained a lot of attention recently, in particular the Frank-Wolfe algorithm [Frank and Wolfe, 1956] (also known as conditional gradient descent [Levitin and Polyak, 1966]; see also [Jaggi, 2013] for an overview) and its online version [Hazan and Kale, 2012]. These methods eschew the projection step and rather use a linear optimization oracle to stay within the feasible region. While convergence rates and regret bounds are often suboptimal, in many cases the gain due to only having to solve a single linear optimization problem over the feasible region in every iteration still leads to significant computational advantages (see e.g., [Hazan and Kale, 2012, Section 5]). This led to conditional gradients algorithms being often used for e.g., online optimization andmore generallymachine learning, especially because they also naturally generate sparse distributions over the extreme points of the feasible region. Further increasing the relevance of these methods, it was shown recently in Garber and Hazan [2013], Lacoste-Julien and Jaggi [2015], Garber and Meshi [2016] that conditional gradient methods often achieve linear convergence.", "startOffset": 106, "endOffset": 1106}, {"referenceID": 5, "context": "separation is significantly weaker than approximate minimization; the latter has been already considered in Jaggi [2013]. A (weak) separation oracle can be realized by a single call to a linear optimization oracle, however with two important differences.", "startOffset": 108, "endOffset": 121}, {"referenceID": 7, "context": ", 2013, Neu and Bart\u00f3k, 2013]), see [Hazan, 2016] for an exhaustive overview.", "startOffset": 36, "endOffset": 49}, {"referenceID": 2, "context": "Related Work In the offline setting we mimick the same setups as in Garber and Hazan [2013], Garber and Meshi [2016] respectively.", "startOffset": 68, "endOffset": 92}, {"referenceID": 2, "context": "Related Work In the offline setting we mimick the same setups as in Garber and Hazan [2013], Garber and Meshi [2016] respectively.", "startOffset": 68, "endOffset": 117}, {"referenceID": 2, "context": "Related Work In the offline setting we mimick the same setups as in Garber and Hazan [2013], Garber and Meshi [2016] respectively. In the online setup we mimick the setup of Hazan and Kale [2012]. Combinatorial convex optimization has been investigated in a long line of works (see e.", "startOffset": 68, "endOffset": 196}, {"referenceID": 4, "context": "We will first show how the Frank-Wolfe style algorithms in Garber and Hazan [2013] and Garber and Meshi [2016] can be lazified by means of a weak separation oracle as given in Oracle 1.", "startOffset": 59, "endOffset": 83}, {"referenceID": 4, "context": "We will first show how the Frank-Wolfe style algorithms in Garber and Hazan [2013] and Garber and Meshi [2016] can be lazified by means of a weak separation oracle as given in Oracle 1.", "startOffset": 59, "endOffset": 111}, {"referenceID": 9, "context": "1 Lazy Conditional Gradient: a basic example We start with lazifying the simplest Conditional Gradent algorithm, adapting the argument of the non-lazy version from Jaggi [2013]. While the vanilla version has suboptimal convergence rate O(1/T), its simplicity makes it an illustrative example of the main idea of lazification.", "startOffset": 164, "endOffset": 177}, {"referenceID": 5, "context": "2 Lazy Pairwise Conditional Gradients In this section we provide a lazy variant (Algorithm 2) of the Pairwise Conditional Gradient algorithm from Garber and Meshi [2016], using separation instead of linear optimization.", "startOffset": 146, "endOffset": 170}, {"referenceID": 4, "context": "3 Lazy Local Conditional Gradients In this sectionwe provide a lazy version (Algorithm3) of the conditional gradient algorithmfromGarber and Hazan [2013]. Let P \u2286 Rn be any polytope, D denote an upper bound on the l2-diameter of P, and \u03bc \u2265 1 be the affine invariant of P from Garber and Hazan [2013].", "startOffset": 130, "endOffset": 154}, {"referenceID": 4, "context": "3 Lazy Local Conditional Gradients In this sectionwe provide a lazy version (Algorithm3) of the conditional gradient algorithmfromGarber and Hazan [2013]. Let P \u2286 Rn be any polytope, D denote an upper bound on the l2-diameter of P, and \u03bc \u2265 1 be the affine invariant of P from Garber and Hazan [2013]. As the algorithm is not affine invariant by nature, we need a non-invariant version of smoothness: Recall that a convex function f is \u03b2-smooth if f (y)\u2212 f (x) \u2264 \u2207 f (x)(y \u2212 x) + \u03b2\u2016y \u2212 x\u2016/2.", "startOffset": 130, "endOffset": 300}, {"referenceID": 4, "context": "As an intermediary step, we first implement a local weak separation oracle in Algorithm4, a local version of Oracle 1, analogously to the local linear optimization oracle in Garber and Hazan [2013]. To this end, we recall a technical lemma from Garber and Hazan [2013].", "startOffset": 174, "endOffset": 198}, {"referenceID": 4, "context": "As an intermediary step, we first implement a local weak separation oracle in Algorithm4, a local version of Oracle 1, analogously to the local linear optimization oracle in Garber and Hazan [2013]. To this end, we recall a technical lemma from Garber and Hazan [2013]. Lemma 3.", "startOffset": 174, "endOffset": 269}, {"referenceID": 7, "context": "In this section we lazify the online conditional gradient algorithm of Hazan and Kale [2012] over arbitrary polytopes P = {x \u2208 Rn | Ax \u2264 b}, resulting in Algorithm 5.", "startOffset": 71, "endOffset": 93}, {"referenceID": 7, "context": "The presented cases here are similar to those in Hazan and Kale [2012] and thus we state them without proof.", "startOffset": 49, "endOffset": 71}, {"referenceID": 4, "context": ", Garber and Hazan [2013]).", "startOffset": 2, "endOffset": 26}, {"referenceID": 4, "context": "We implemented and compared Algorithm 2 (LPCG) to the Pairwise Conditional Gradient Algorithm (PCG) in Garber and Meshi [2016] and we also implemented and compared Algorithm 5 (LOCG) to the Online Frank-WolfeAlgorithm (OCG) ofHazan and Kale [2012].", "startOffset": 103, "endOffset": 127}, {"referenceID": 4, "context": "We implemented and compared Algorithm 2 (LPCG) to the Pairwise Conditional Gradient Algorithm (PCG) in Garber and Meshi [2016] and we also implemented and compared Algorithm 5 (LOCG) to the Online Frank-WolfeAlgorithm (OCG) ofHazan and Kale [2012]. We did not implement the variant in Garber and Hazan [2013] as for the cases considered here the variant in Garber and Meshi [2016] is more efficient; our method applies to both though as shown earlier.", "startOffset": 103, "endOffset": 248}, {"referenceID": 4, "context": "We did not implement the variant in Garber and Hazan [2013] as for the cases considered here the variant in Garber and Meshi [2016] is more efficient; our method applies to both though as shown earlier.", "startOffset": 36, "endOffset": 60}, {"referenceID": 4, "context": "We did not implement the variant in Garber and Hazan [2013] as for the cases considered here the variant in Garber and Meshi [2016] is more efficient; our method applies to both though as shown earlier.", "startOffset": 36, "endOffset": 132}, {"referenceID": 2, "context": "We considered polytopes where the underlying optimization problem is easy (spanning trees), and where it is NP-hard (Maximum Cut, Traveling Salesman Problem, Quadratic Unconstrained Boolean Optimization [Dash, 2013], as well as various instances from MIPLIB [Achterberg et al.", "startOffset": 203, "endOffset": 215}, {"referenceID": 8, "context": "We tested two types of loss functions: random linear functions cx + b with c \u2208 [\u22121,+1]n and b \u2208 [0, 1] and quadratic functions of the form \u2016b \u2212 x\u201622 with b \u2208 [0, 1]n, similar to those in [Hazan and Kale, 2012].", "startOffset": 187, "endOffset": 209}, {"referenceID": 2, "context": "Another set of NP-hard instances we tested our algorithm on are the quadratic unconstrained boolean optimization (QUBO) instances defined on Chimera graphs [Dash, 2013], which are available athttp://researcher.", "startOffset": 156, "endOffset": 168}], "year": 2017, "abstractText": "Conditional gradient algorithms (also often called Frank-Wolfe algorithms) are popular due to their simplicity of only requiring a linear optimization oracle and more recently they also gained significant traction for online learning. While simple in principle, inmany cases the actual implementation of the linear optimization oracle is costly. We show a general method to lazify various conditional gradient algorithms, which in actual computations leads to several orders of magnitude of speedup in wall-clock time. This is achieved by using a faster separation oracle instead of a linear optimization oracle, relying only on few linear optimization oracle calls.", "creator": "LaTeX with hyperref package"}}}