{"id": "1610.05150", "review": {"conference": "AAAI", "VERSION": "v1", "DATE_OF_SUBMISSION": "17-Oct-2016", "title": "Neural Machine Translation Advised by Statistical Machine Translation", "abstract": "Neural Machine Translation (NMT) is a new approach to machine translation that has made great progress in recent years. However, recent studies show that NMT generally produces fluent but inadequate translations (Tu et al. 2016; He et al. 2016). This is in contrast to conventional Statistical Machine Translation (SMT), which usually yields adequate but non-fluent translations. It is natural, therefore, to leverage the advantages of both models for better translations, and in this work we propose to incorporate SMT model into NMT framework. More specifically, at each decoding step, SMT offers additional recommendations of generated words based on the decoding information from NMT (e.g., the generated partial translation and attention history). Then we employ an auxiliary classifier to score the SMT recommendations and a gating function to combine the SMT recommendations with NMT generations, both of which are jointly trained within the NMT architecture in an end-to-end manner. Experimental results on Chinese-English translation show that the proposed approach achieves significant and consistent improvements over state-of the-art NMT and SMT systems on multiple NIST test sets.", "histories": [["v1", "Mon, 17 Oct 2016 14:50:58 GMT  (91kb,D)", "http://arxiv.org/abs/1610.05150v1", "submitted to AAAI 2017"], ["v2", "Fri, 30 Dec 2016 02:38:35 GMT  (79kb,D)", "http://arxiv.org/abs/1610.05150v2", "Accepted by AAAI 2017"]], "COMMENTS": "submitted to AAAI 2017", "reviews": [], "SUBJECTS": "cs.CL", "authors": ["xing wang", "zhengdong lu", "zhaopeng tu", "hang li", "deyi xiong", "min zhang"], "accepted": true, "id": "1610.05150"}, "pdf": {"name": "1610.05150.pdf", "metadata": {"source": "CRF", "title": "Neural Machine Translation Advised by Statistical Machine Translation", "authors": ["Xing Wang", "Zhengdong Lu", "Zhaopeng Tu", "Hang Li", "Deyi Xiong", "Min Zhang"], "emails": ["xingwsuda@gmail.com,", "minzhang}@suda.edu.cn", "hangli.hl}@huawei.com"], "sections": [{"heading": "Introduction", "text": "This year, it is so far that it will be able to retaliate, \"he said.\" We have to be able to try to find a solution, \"he said.\" We are able to find a solution, \"he said,\" but we have to be able to find a solution. \""}, {"heading": "Background", "text": "In this section we give a brief introduction to NMT and phrase-based SMT, the most popular SMT model."}, {"heading": "Neural Machine Translation", "text": "When a source set x = x1, x2,..., xTx, attention-based NMT (Bahdanau, Cho, and Bengio 2015) encodes into a sequence of vectors, it uses the sequence of vectors to generate a target set y = y1, y2,..., yTy. In the coding step, attention-based NMT uses a bidirectional RNN consisting of forward-directed RNN and backward RNN (Schuster and Paliwal 1997) to encode the source set. Forward-directed RNN reads the source set x in a forward-directed direction and generates a sequence of forward-hidden states \u2212 \u2192 h = [\u2212 h1, \u2212 h2, \u2212 hTx]. Backward-directed RNN reads the source set x in a backward direction, where the source set x, the sequence of backward-hidden states, is named."}, {"heading": "Statistical Machine Translation", "text": "Most SMT models are defined using the log-linear framework (Och and Ney 2002).p (y | x) = exp (\u2211 M m = 1 \u03bbmhm (y, x)) \u2211 y'exp (\u2211 M m = 1 \u03bbmhm (y \u2032, x))) (5), where hm (y, x) is a feature function and \u03bbm its weight. Standard SMT features include the bidirectional translation probabilities, the bidirectional lexical translation probabilities, the language model, the reordering model, the word penalty, and the phrase penalty. The feature weights can be set using the minimal error rate training algorithm (MERT) (Och 2003).During translation, the SMT decoder extends the partial translation (referred to in SMT as the translation hypothesis) y < t = y1, y2,... yt \u2212 1 by selecting a two-word target phrase for a translation."}, {"heading": "NMT with SMT Recommendations", "text": "Unlike the attention-based NMT, which predicts the next word on the basis of vector representations, the prediction of the proposed model is also based on recommendations from an SMT model. The SMT model can be trained separately on a bilingual corpus using the conventional, phrase-based SMT approach (Koehn, Och and Marcu 2003). Given the decoding of information from the NMT, the SMT model makes word recommendations 1 for the next word prediction with SMT characteristics. In order to integrate the SMT recommendations into the proposed model, we use a classifier to evaluate the recommendations and a gate to combine SMT recommendations with NMT word predictions. As shown in Figure 1, the word generation process of the proposed model has three steps: 1. Inheriting from standard attention-based NMT, the NMT classifier (SMT classifier SMT), SMT classifiable SMT (SMT SMT)."}, {"heading": "Generating SMT Recommendations", "text": "Given the previously generated words y < t = y1, y2, yt \u2212 1 of NMT, SMT generates the next word recommendations, and calculates its results from SMTscore (yt | y < t, x) = M \u2211 m = 1\u03bbmhm (yt, xt) (7) where yt is an SMT recommendation and xt is its corresponding source span. hm (yt, xt) is a feature function and \u03bbm is its weight. The SMT model can generate correct word recommendations through extended generated words (partial translation). However, there are two problems with SMT recommendations due to the different word generation mechanisms between SMT and NMT: Coverage Information SMT lacks information about the source text. In conventional SMT, the decoder has a cover word vector to indicate whether a source word / sentence is translated or not."}, {"heading": "Integrating SMT Recommendations into NMT", "text": "To ensure the quality of SMT2In education, since words from the NMT vocabulary V nmt can also occur in already generated words, we should train a new language model by replacing the words with UNK and using this language model to evaluate a target sequence.Recommendations we adopt two strategies to filter low-quality recommendations: 1) Only uppermost Ntm translations for each source word are retained according to their translation results, each of which is calculated as a weighted sum of the translation probabilities. (2) uppermost Nrec recommendations with highest SMT results are selected, each of which is calculated as a weighted sum of SMT words. At the decoding step, the SMT classifier assumes the current hidden state SMltx."}, {"heading": "Handling UNK with SMT recommendations", "text": "For each UNK word, we select the recommendation with the highest SMT score as the definitive replacement. 43Bidirectional translation probabilities and bidirectional lexical translation probabilities. 4In the previously generated words, there is no UNK word, as our model generates the target sentence from left to right. Here, we can use the original language model to evaluate the target sequence. Language models trained on large monolingual corpora help perform correct substitutions. Our method is similar to the method described in (He et al. 2016), which allows both rich contextual information to be used on both the source and target side to deal with UNK. The difference is that our method takes into account the reordering of information and the vector of SMT coverage to generate more reliable recommendations."}, {"heading": "Model Training", "text": "We train the proposed model by minimizing the negative log probability on a set of training data {(xn, yn)} Nn = 1: C (\u03b8) = \u2212 1 N \u2211 n = 1 Ty \u2211 t = 1 log p (ynt | yn < t, xn) (14) The cost function of our proposed model is the same as a traditional attention-based NMT model except that we introduce additional parameters for the SMT classifier and the gate. We optimize our model by minimizing the cost function. In practice, we use a simple pre-training strategy to train our model. We first train a regular attention-based NMT model. We then use this model to initialize the parameters of the encoder and decoder of the proposed model and use random initializations to initialize the parameters of the SMT classifier and gate."}, {"heading": "Experiments", "text": "In this section, we review our approach to machine translation from Chinese to English."}, {"heading": "Setup", "text": "The training is a parallel corpus from the LDC, which attracts the attention of the public. \"We,\" it says in the explanatory statement, \"must deal with the question of how we get a grip on the world.\" \"We,\" he says, \"we.\" \"We.\" \"We.\" \"We.\" \"\" We. \"\" \"\" We. \"\" \"We.\" \"\" \"We.\" \"\" \"We.\" \"\" \"\" We. \"\" \"\" \"\" We. \"\" \"\" \"\" \".\" \"\" \"\" \"\". \"\" \"\" \"\" \"\" \"\" \".\" \"\" \"\" \"\" \"\" \"\" \"\" \".\" \"\" \"\" \"\" \"\" \"\" \"\". \"\" \"\" \"\" \"\" \"\" \"\". \"\" \"\" \"\" \"\" \"\". \"\" \"\" \"\" \"\". \"\" \"\" \"\" \"\". \"\" \"\" \"\" \"\" \".\" \"\" \"\" \"\" \"\" \"\" \"\" \".\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\". \"\" \"\" \"\" \"\" \"\" \"\" \"\" \".\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\". \"\" \"\" \"\" \"\" \"\" \"\" \"\" \""}, {"heading": "Results", "text": "Table 1 reports on the experimental results measured in terms of BLEU score. We find that our implementation RNNSearch * without feedback attention and dropout exceeds Groundhog and Moses * by 1.51 BLEU point and 0.77 BLEU point. RNSearch * + SMTrec with SMT recommendations, but holds target UNK words achieves a gain of up to 1.20 BLEU point over RNNSearch *. Surprisingly, RNNSearch * + SMTrec, UNKReplace, which further replaces UNK word with SMT recommendations during translation, achieves a further improvement over RNNSearch * SMTrec by 1.24 BLEU point. It exceeds the proposed RNSearch * and Moses by 2.44 BLEU point and 3.21 BLEU point respectable.We also conduct additional experiments to validate the effectiveness of the key components of our proposed SMT search model, namely SMT NSMT search, NSMT Trec NT recommendations and generic testing mechanism. We also take the following SMT (we say SMT for three recommendations: SMT)."}, {"heading": "Related Work", "text": "In this section, we briefly review previous studies related to our work. Here, we divide previous work into three categories: Combination of SMT and NMT: Stahlberg et al. (2016) extended the beam search decryption by expanding the search space of NMT with translation hypotheses produced by a syntactic SMT model. Er et al. (2016) improved the NMT system with effective SMT functions. They integrated three SMT functions, namely translation model, word reward, and language model, with the NMT model under the loglinear frame in beam search. Arthur et al. (2016) proposed to include discrete translation reflections in the NMT model. They calculated lexical predictive probability and integrated the probability of word probability associated with the NMT model to predict the next word. Wuebker et al al al al al al al al al al, applied as 2016, used phrase-based interneural and machine translation to complete the machine translation."}, {"heading": "Conclusion", "text": "In this paper, we have presented a novel approach that incorporates the SMT model with attention mechanisms into the NMT. Our proposed model remains the power of end-to-end NMT while mitigating its limitations by using SMT recommendations for better generation in the NMT. Unlike previous work, which normally used a separately trained NMT model as an additional feature, our proposed model, which includes NMT and SMT, is trained in an end-to-end manner. Experimental results from the Chinese-English translation have shown the effectiveness of the proposed model."}], "references": [{"title": "S", "author": ["P. Arthur", "G. Neubig", "Nakamura"], "venue": "2016. Incorporating discrete translation lexicons into neural machine translation. In EMNLP", "citeRegEx": "Arthur. Neubig. and Nakamura 2016", "shortCiteRegEx": null, "year": 2016}, {"title": "Y", "author": ["D. Bahdanau", "K. Cho", "Bengio"], "venue": "2015. Neural machine translation by jointly learning to align and translate. In ICLR", "citeRegEx": "Bahdanau. Cho. and Bengio 2015", "shortCiteRegEx": null, "year": 2015}, {"title": "R", "author": ["P.F. Brown", "V.J.D. Pietra", "S.A.D. Pietra", "Mercer"], "venue": "L.", "citeRegEx": "Brown et al. 1993", "shortCiteRegEx": null, "year": 1993}, {"title": "D", "author": ["Chiang"], "venue": "2005. A hierarchical phrase-based model for statistical machine translation. In ACL", "citeRegEx": "Chiang 2005", "shortCiteRegEx": null, "year": 2015}, {"title": "J", "author": ["R. Chitnis", "DeNero"], "venue": "2015. Variable-length word encodings for neural translation models. In EMNLP", "citeRegEx": "Chitnis and DeNero 2015", "shortCiteRegEx": null, "year": 2015}, {"title": "On the properties of neural machine translation: Encoder-decoder approaches", "author": ["Cho"], "venue": null, "citeRegEx": "Cho,? \\Q2014\\E", "shortCiteRegEx": "Cho", "year": 2014}, {"title": "Learning phrase representations using rnn encoder\u2013decoder for statistical machine translation", "author": ["Cho"], "venue": "EMNLP", "citeRegEx": "Cho,? \\Q2014\\E", "shortCiteRegEx": "Cho", "year": 2014}, {"title": "Y", "author": ["J. Chung", "K. Cho", "Bengio"], "venue": "2016. A character-level decoder without explicit segmentation for neural machine translation. In ACL", "citeRegEx": "Chung. Cho. and Bengio 2016", "shortCiteRegEx": null, "year": 2016}, {"title": "G", "author": ["T. Cohn", "C.D.V. Hoang", "E. Vymolova", "K. Yao", "C. Dyer", "Haffari"], "venue": "2016. Incorporating structural alignment biases into an attentional neural translation model. In NAACL", "citeRegEx": "Cohn et al. 2016", "shortCiteRegEx": null, "year": 2016}, {"title": "2016", "author": ["M.R. Costa-Juss\u00e0", "Fonollosa", "J. A"], "venue": "Character-based neural machine translation. In ACL", "citeRegEx": "Costa.Juss\u00e0 and Fonollosa 2016", "shortCiteRegEx": null, "year": 2016}, {"title": "Implicit distortion and fertility models for attentionbased encoder-decoder nmt model", "author": ["Feng"], "venue": null, "citeRegEx": "Feng,? \\Q2016\\E", "shortCiteRegEx": "Feng", "year": 2016}, {"title": "2016", "author": ["J. Gu", "Z. Lu", "H. Li", "V. O Li"], "venue": "Incorporating copying mechanism in sequence-to-sequence learning. In ACL", "citeRegEx": "Gu et al. 2016", "shortCiteRegEx": null, "year": 2016}, {"title": "H", "author": ["W. He", "Z. He", "H. Wu", "Wang"], "venue": "2016. Improved neural machine translation with smt features. In AAAI", "citeRegEx": "He et al. 2016", "shortCiteRegEx": null, "year": 2016}, {"title": "and Schmidhuber", "author": ["S. Hochreiter"], "venue": "J.", "citeRegEx": "Hochreiter and Schmidhuber 1997", "shortCiteRegEx": null, "year": 1997}, {"title": "Y", "author": ["S. Jean", "K. Cho", "R. Memisevic", "Bengio"], "venue": "2015. On using very large target vocabulary for neural machine translation. In ACL", "citeRegEx": "Jean et al. 2015", "shortCiteRegEx": null, "year": 2015}, {"title": "P", "author": ["N. Kalchbrenner", "Blunsom"], "venue": "2013. Recurrent continuous translation models. In EMNLP", "citeRegEx": "Kalchbrenner and Blunsom 2013", "shortCiteRegEx": null, "year": 2013}, {"title": "D", "author": ["P. Koehn", "F.J. Och", "Marcu"], "venue": "2003. Statistical phrase-based translation. In NAACL", "citeRegEx": "Koehn. Och. and Marcu 2003", "shortCiteRegEx": null, "year": 2003}, {"title": "A", "author": ["W. Ling", "I. Trancoso", "C. Dyer", "Black"], "venue": "W.", "citeRegEx": "Ling et al. 2015", "shortCiteRegEx": null, "year": 2015}, {"title": "2016", "author": ["Luong", "M.-T.", "Manning", "C. D"], "venue": "Achieving open vocabulary neural machine translation with hybrid word-character models. In ACL", "citeRegEx": "Luong and Manning 2016", "shortCiteRegEx": null, "year": 2016}, {"title": "Q", "author": ["M.-T. Luong", "I. Sutskever", "Le"], "venue": "V.; Vinyals, O.; and Zaremba, W.", "citeRegEx": "Luong et al. 2015", "shortCiteRegEx": null, "year": 2015}, {"title": "A deep memory-based architecture for sequenceto-sequence learning", "author": ["Meng"], "venue": "arXiv preprint arXiv:1506.06442", "citeRegEx": "Meng,? \\Q2015\\E", "shortCiteRegEx": "Meng", "year": 2015}, {"title": "and Ney", "author": ["F.J. Och"], "venue": "H.", "citeRegEx": "Och and Ney 2002", "shortCiteRegEx": null, "year": 2002}, {"title": "F", "author": ["Och"], "venue": "J.", "citeRegEx": "Och 2003", "shortCiteRegEx": null, "year": 2003}, {"title": "K", "author": ["M. Schuster", "Paliwal"], "venue": "K.", "citeRegEx": "Schuster and Paliwal 1997", "shortCiteRegEx": null, "year": 1997}, {"title": "Neural machine translation of rare words with subword units", "author": ["Haddow Sennrich", "R. Birch 2016] Sennrich", "B. Haddow", "A. Birch"], "venue": "In 54th ACL,", "citeRegEx": "Sennrich et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Sennrich et al\\.", "year": 2016}, {"title": "Syntactically guided neural machine translation", "author": ["Stahlberg"], "venue": "arXiv preprint arXiv:1605.04569", "citeRegEx": "Stahlberg,? \\Q2016\\E", "shortCiteRegEx": "Stahlberg", "year": 2016}, {"title": "Q", "author": ["I. Sutskever", "O. Vinyals", "Le"], "venue": "V.", "citeRegEx": "Sutskever. Vinyals. and Le 2014", "shortCiteRegEx": null, "year": 2014}, {"title": "P", "author": ["Y. Tang", "F. Meng", "Z. Lu", "H. Li", "Yu"], "venue": "L.", "citeRegEx": "Tang et al. 2016", "shortCiteRegEx": null, "year": 2016}, {"title": "H", "author": ["Z. Tu", "Z. Lu", "Y. Liu", "X. Liu", "Li"], "venue": "2016. Modeling coverage for neural machine translation. In ACL", "citeRegEx": "Tu et al. 2016", "shortCiteRegEx": null, "year": 2016}, {"title": "Models and inference for prefix-constrained machine translation", "author": ["Wuebker"], "venue": "In 54th ACL-Volume", "citeRegEx": "Wuebker,? \\Q2016\\E", "shortCiteRegEx": "Wuebker", "year": 2016}, {"title": "M", "author": ["Zeiler"], "venue": "D.", "citeRegEx": "Zeiler 2012", "shortCiteRegEx": null, "year": 2012}], "referenceMentions": [], "year": 2016, "abstractText": "Neural Machine Translation (NMT) is a new approach to machine translation that has made great progress in recent years. However, recent studies show that NMT generally produces fluent but inadequate translations (Tu et al. 2016; He et al. 2016). This is in contrast to conventional Statistical Machine Translation (SMT), which usually yields adequate but non-fluent translations. It is natural, therefore, to leverage the advantages of both models for better translations, and in this work we propose to incorporate SMT model into NMT framework. More specifically, at each decoding step, SMT offers additional recommendations of generated words based on the decoding information from NMT (e.g., the generated partial translation and attention history). Then we employ an auxiliary classifier to score the SMT recommendations and a gating function to combine the SMT recommendations with NMT generations, both of which are jointly trained within the NMT architecture in an end-to-end manner. Experimental results on Chinese-English translation show that the proposed approach achieves significant and consistent improvements over state-of-the-art NMT and SMT systems on multiple NIST test sets.", "creator": "LaTeX with hyperref package"}}}