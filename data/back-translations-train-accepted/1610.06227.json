{"id": "1610.06227", "review": {"conference": "acl", "VERSION": "v1", "DATE_OF_SUBMISSION": "19-Oct-2016", "title": "Cross-Lingual Syntactic Transfer with Limited Resources", "abstract": "We describe a simple but effective method for cross-lingual syntactic transfer of dependency parsers, in the scenario where a large amount of translation data is not available.The method makes use of three steps: 1) a method for deriving cross-lingual word clusters, that can then be used in a multilingual parser; 2) a method for transferring lexical information from a target language to source language treebanks; 3) a method for integrating these steps with the density-driven annotation projection method of Rasooli and Collins(2015). Experiments show improvements over the state-of-the-art in several languages used in previous work (Rasooli and Collins, 2015;Zhang and Barzilay, 2015; Ammar et al.,2016), in a setting where the only source of translation data is the Bible, a considerably smaller corpus than the Europarl corpus used in previous work. Results using the Europarl corpus as a source of translation data show additional improvements over the results of Rasooli and Collins (2015). We conclude with results on 38 datasets (26 languages) from the Universal Dependencies corpora: 13 datasets(10 languages) have unlabeled attachment ac-curacies of 80% or higher; the average unlabeled accuracy on the 38 datasets is 74.8%.", "histories": [["v1", "Wed, 19 Oct 2016 21:25:39 GMT  (534kb)", "http://arxiv.org/abs/1610.06227v1", null], ["v2", "Sat, 4 Feb 2017 04:05:00 GMT  (49kb)", "http://arxiv.org/abs/1610.06227v2", null]], "reviews": [], "SUBJECTS": "cs.CL", "authors": ["mohammad sadegh rasooli", "michael collins"], "accepted": true, "id": "1610.06227"}, "pdf": {"name": "1610.06227.pdf", "metadata": {"source": "CRF", "title": "Cross-Lingual Syntactic Transfer with Limited Resources", "authors": ["Mohammad Sadegh Rasooli"], "emails": ["rasooli@cs.columbia.edu", "mcollins@cs.columbia.edu"], "sections": [{"heading": null, "text": "ar Xiv: 161 0.06 227v 1 [cs.C L] 19 OWe describe a simple but effective method for the lingual syntactic transfer of dependency savers in a scenario where a large amount of translation data is not available, using three steps: 1) a method for deriving cross-lingual word clusters that can then be used in a multilingual parser; 2) a method for transferring lexical information from a target language into source-language tree banks; 3) a method for integrating these steps with the density-driven projection method for annotations by Rasooli and Collins (2015). Experiments show improvements over the state of the art in multiple languages used in previous work (Rasooli and Collins, 2015; Zhang and Barzilay, 2015; Ammar et al., 2016), in an environment where the only source of translation data is the Bible, a much smaller corpus than the one used in previous work, Europarzar and Ammay;"}, {"heading": "1 Introduction", "text": "Recently there has been a great interest in translingual systems, for which there is a clear motivation to develop models for languages for which there is no syntactic data. Methods for syntactic transfer include annotation methods (Hwa et al., 2005; Ganchev et al., 2009; McDonald et al., 2011; Ma and Xia, 2014; Rasooli and Collins, 2015; Lacroix et al., 2016), learning of delicalized models of universal treebanks (Zeman and Resnik, 2008; McDonald et al., 2011; Ta ckstro \u00bc m et al."}, {"heading": "2 Background", "text": "This section describes the underlying parsing models used in our experiments, the data sets used, and a basic approach based on delicalized parsing models."}, {"heading": "2.1 The Parsing Model", "text": "We assume that the parsing model is a discriminatory linear model in which a sentence x is given and a series of candidates Y (x) parse, the output of the model isy \u0445 (x) = arg max Y (x, y). In our experiments, we use the shift-reducing dependency parser of Rasooli and Tetreault (2015), which is an extension of the approach in (Zhang and Nivre, 2011). The parser is trained using the averaged structured perceptor (Collins, 2002). We assume that the feature vector \u03c6 (x, y) is the concatenation of three feature vectors: \u2022 \u03c6 (p) (x, y) is an unexicalized set of features. Any such feature may depend on the part of the speech (POS) tag in the sentence, but not on the identity of individual words in a sentence (x, y)."}, {"heading": "2.2 Data Assumptions", "text": "We assume that we use the source languages L1... Lm and a single target language Lm + 1. We assume the following data sources: source language tree banks. We assume that the data uses a universal POS set, which is common in all languages. Monolingual data. We have monolingual raw data for each of the (m + 1) languages. We use Di to refer to the monolingual data for the i \"th language. Translation data. We use translation data for all language pairs. We use Bi, j to refer to translation data for the language pair (i, j) where i, j, (m + 1)} and i 6 = j.In our main experiments we use the Google source data and the universal data from McDonald's source text (i, j), the Bible and corpus (i, j) as translation data."}, {"heading": "2.3 A Baseline Approach: Delexicalized Parsers with Self-Training", "text": "Given the data assumption of a universal POS set, the feature vectors \u03c6 (p) (x, y) can be distributed across all languages. A simple approach is then to simply use a delecicalized parser using treebanks T1.... Tm, using the representation \u03c6 (x, y) = \u03c6 (p) (x, y) (see (McDonald et al., 2013; Ta \ufffd ckstro \u00b6 m et al., 2013)). Our basic approach also uses a delecicalized parser with two refinements: WALS properties. We use the six properties from the World Book of Language Structures (WALS) (Dryer and Haspelmath, 2013) to train a subset of 2We also train our most powerful model on the newly released universal treebank v1.3 (Nivre et al al al al al al al al al al al., 2016). See also the six properties from the World Book of Language Structures (WALS) (WALS) (Dryer et al), our most powerful treebank (2013)."}, {"heading": "2.4 Translation Dictionaries", "text": "Our only use of the translation data Bi, j for i, j. (m + 1)} is to construct a translation dictionary t (w, i, j).Here, i and j are two languages, w is a word in the Li language, and the output w \u2032 = t (w, i, j) is a word in the Lj language, which is the most common translation of w into this language.3There has been no effort to optimize this choice; future work might consider more complex common schematics. We define the function t (w, i, j) as follows: We first perform the GIZA + + alignment process (Och and Ney, 2000) on the data Bi, j. We then consider intersections between sentences in the two languages.Finally, for each word w in Li w \u2032 = t (w, i, j) as the most frequently directed word of the target language."}, {"heading": "3 Our Approach", "text": "This section describes our approach to improving on the delicalized approach. The following subsections describe our approach: \u00a7 3.1 describes a method for deriving lingual clusters that allows us to insert cluster features \u03c6 (c) (x, y) into the model. \u00a7 3.2 describes a method for adding lexical features \u03c6 (l) (x, y) to the model. \u00a7 3.3 describes a method for integrating the approach with the density-driven approach of Rasooli and Collins (2015). Finally, \u00a7 4 describes experiments. We show that each of the above steps leads to an improvement in accuracy."}, {"heading": "3.1 Learning Cross-Lingual Clusters", "text": "We now describe a method for learning cross-lingual clusters, following previous work on cross-lingual cluster algorithms (Ta \ufffd ckstro \ufffd m et al., 2012). A clustering is a function C (w), the inputs mapsInputs: 1) Monolingual texts Di for i = 1. (m + 1); 2) a function t (w, i, j), which translates a word w \u00b2 Li to w \u00b2 Lj; and 3) a parameter \u03b1, that is 0 < \u03b1 < 1.each word w in a vocabulary to a cluster C (w, i, j), which is a word w \u00b2 Li to w \u00b2 Lj \u00b2, and 3) a parameter \u03b1, that is 0 < \u03b1 < 1.each word w in a vocabulary to a cluster C (w)."}, {"heading": "3.2 Treebank Lexicalization", "text": "We now describe how lexical representations \u03c6 (l) (x, y) are transferred to the model. Our approach is simple: we take the tree data T1... Tm for the m source languages, together with the translation lexicons t (w, i, m + 1). For each word w in the source tree database, we can look up its translation t (w, i, m + 1) in the lexicon and add this translated form to the underlying sentence. Properties can now consider lexical identities derived in this way. In many cases, the resulting translation will be the NULL word. However, we still use the representations \u03c6 (p, y) and \u03c6 (x, y) as previously defined; these representations are useful when observing the NULL translation. 3.3 Integration with the density-driven projection method of Rasooli and Collins (2015). In this section, we describe a method for integrating our approach."}, {"heading": "4 Experiments", "text": "This section first describes the experimental settings and then reports on the results."}, {"heading": "4.1 Data and Tools", "text": "The data in a first series of experiments, which we are researching on the same order of magnitude as the Bible, are the effects of the Bible on the data we have provided in a first group of experiments. (Each of us has the ability to transfer the data into a different group.) We use the linkage of the three different types of training in the Bible that we apply in the Bible. (We use the linkage of study sentences, Wikipedia data and the Bible monolingual sentences of the Bible monolingual sentences as our monolingual raw texts.) Table 3 shows statistics for the monolingual languages. We use the Bible data from Christodoupoulos and Steedman (2014), which includes data for 100 languages, as the source of the translations. We also have the experiments with the Europarl data (both with the original size of the Bible like the Bible)."}, {"heading": "4.2 Results on the Google Treebank", "text": "In fact, it is the case that most of the people who have lived in the United States in the last ten years are not able to understand the world and to understand why they are able to understand the world and to understand what they have to do to save the world. (...) It is as if the world were not able to save the world. (...) It is as if the world were not able to save the world. (...) It is as if the world was not able to save the world. (...) It is as if the world was in the world of the world, in the world, in the world, in the world, in the world, in the world, in the world, in the world, in the world, in the world, in the world, in the world, in the world, in the world, in the world, in the world, in the world, in the world, in the world, in the world, in the world, in the world, in the world, in the world, in the world, in the world, in the world, in the world, in the world, in the world, in the world, in the world, in the world, in the world in the world, in the world, in the world, in the world, in the world, in the world, in the world, in the world, in the world, in the world, in the world, in the world, in the world, in the world, in the world, in the world, in the world, in the world, in the world, in the world, in the world, in the world, in the world, in the world, in the world, in the world, in the world, in the world, in the, in the world, in the world, in the world, in the world, in the world, in the world, in the world, in the world, in the world, in the world, in the world, in the world, in the, in fact, in fact, it is the fact that most people who are unable to understand the world, why they are unable to understand the world, why they are not able to understand the world, why they are able to understand the world, to understand the world, and why they are able to understand the world, and why they are able to understand the world, the world, and why they are unable to understand the world, the world,"}, {"heading": "4.3 Results on the Universal Dependencies v1.3", "text": "Table 9 provides results on 38 data sets (26 languages) from the newly published Universal Dependency Corpus (Nivre et al., 2016). Given the number of tree banks and to speed up training, we select source languages that have at least 5 of 6 common WALS characteristics with each target language. Our experiments are conducted using the Bible as translation data. As shown in Table 9, our method consistently outperforms the density-driven method of Rasooli and Collins (2015), and in many languages, the accuracy of our method approaches the accuracy of the monitored parser. Accuracy in some languages (e.g. Persian (fa) and Turkish (tr)) is low, suggesting that future work should consider more powerful techniques for these languages."}, {"heading": "5 Analysis", "text": "We conclude this paper with some examples from the English development data (Figures 3 and 4). Wefind interesting examples of where our method predicts the correct tree, including some long sentences (Figures 3a and 3b). Apart from very short sentences with basic grammatical structures, the baseline parser fails completely to restore the correct structure. Figure 2 presents two examples for which the baseline parser has very little accuracy, while our method is perfectly accurate. In the first example, the baseline parser places the root node on an incorrect verb (\"plug\" instead of \"goes\") and then adds false dependencies. In the second example in Figure 2, the baseline parser cannot find the head of the noun phrase (\"diversified fidelity funds\")."}, {"heading": "6 Related Work", "text": "A number of methods (Zeman and Resnik, 2008; McDonald et al., 2011; Cohen et al., 2011; Naseem et al., 2012; Ta \ufffd ckstro \ufffd m et al., 2013; Rosa and Zabokrtsky, 2015) directly learn delectable models that can be trained on universal treebank data from one or more source languages, which are then applied to the target language. Recent work has introduced cross-lingual representations - for example, cross-lingual word embedding - that can be used to improve performance (Zhang and Barzilay, 2015; Guo et al., 2015; Duong et al., 2016; Guo et al., 2016). These cross-lingual representations are usually learned from parallel translation data. We show results for the methods of (Zhang and Barzilay, 2015; Guo et al., 2016; Ammar et al., 2016)."}, {"heading": "7 Conclusions", "text": "We have described a method for linguistic syntactic transfer that is effective in a scenario where a large amount of translation data is not available. We have introduced a simple, direct method for deriving linguistic clusters and transferring lexical information for different languages across tree banks. Experiments with the method show that the method offers improved performance compared to previous work using Europarl, a much larger translation corpus."}, {"heading": "Appendix A Parsing Features", "text": "We used all the features in (Zhang and Nivre, 2011, Tables 1 and 2) that describe features based on the word and part of the speech at different positions on the stack and buffer of the transition system. Furthermore, we extend the features of Zhang and Nivre (2011, Table 1) by clusters: Whenever a feature checks the part of the speech for a word at position 0 of the stack or buffer, we introduce features that replace the part of the speech with the Brown cluster bit string of length 4 and 6. Whenever a feature tests the word identity at position 0 of the stack or buffer, we introduce a cluster feature that replaces the word with the full cluster feature. We take the cross-product of all features that correspond to the choice of 4 or 6-bit string for parts of the speech characteristics."}], "references": [{"title": "Multilingual projection for parsing truly low-resource languages. Transactions of the Association for Computational Linguistics", "author": ["\u017deljko Agi\u0107", "Anders Johannsen", "Barbara Plank", "H\u00e9ctor Alonso Mart\u0131\u0301nez", "Natalie Schluter", "Anders S\u00f8gaard"], "venue": null, "citeRegEx": "Agi\u0107 et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Agi\u0107 et al\\.", "year": 2016}, {"title": "One parser, many languages", "author": ["Waleed Ammar", "George Mulcaire", "Miguel Ballesteros", "Chris Dyer", "Noah A Smith."], "venue": "arXiv preprint arXiv:1602.01595.", "citeRegEx": "Ammar et al\\.,? 2016", "shortCiteRegEx": "Ammar et al\\.", "year": 2016}, {"title": "Classbased n-gram models of natural language", "author": ["Peter F Brown", "Peter V Desouza", "Robert L Mercer", "Vincent J Della Pietra", "Jenifer C Lai."], "venue": "Computational linguistics, 18(4):467\u2013479.", "citeRegEx": "Brown et al\\.,? 1992", "shortCiteRegEx": "Brown et al\\.", "year": 1992}, {"title": "A massively parallel corpus: the bible in 100 languages", "author": ["Christos Christodouloupoulos", "Mark Steedman."], "venue": "Language Resources and Evaluation, pages 1\u201321.", "citeRegEx": "Christodouloupoulos and Steedman.,? 2014", "shortCiteRegEx": "Christodouloupoulos and Steedman.", "year": 2014}, {"title": "Unsupervised structure prediction with non-parallel multilingual guidance", "author": ["Shay B. Cohen", "Dipanjan Das", "Noah A. Smith."], "venue": "Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing, pages 50\u201361, Edinburgh, Scotland,", "citeRegEx": "Cohen et al\\.,? 2011", "shortCiteRegEx": "Cohen et al\\.", "year": 2011}, {"title": "Discriminative training methods for hidden markov models: Theory and experiments with perceptron algorithms", "author": ["Michael Collins."], "venue": "Proceedings of the 2002 Conference on Empirical Methods in Natural Language Processing, pages 1\u20138. Association for", "citeRegEx": "Collins.,? 2002", "shortCiteRegEx": "Collins.", "year": 2002}, {"title": "Cross-lingual transfer for unsupervised dependency parsing without parallel data", "author": ["Long Duong", "Trevor Cohn", "Steven Bird", "Paul Cook."], "venue": "Proceedings of the Nineteenth Conference on Computational Natural Language Learning, pages 113\u2013122, Beijing, China,", "citeRegEx": "Duong et al\\.,? 2015", "shortCiteRegEx": "Duong et al\\.", "year": 2015}, {"title": "Syntactic transfer using a bilingual lexicon", "author": ["Greg Durrett", "Adam Pauls", "Dan Klein."], "venue": "Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning, pages 1\u201311, Jeju Island,", "citeRegEx": "Durrett et al\\.,? 2012", "shortCiteRegEx": "Durrett et al\\.", "year": 2012}, {"title": "Dependency grammar induction via bitext projection constraints", "author": ["Kuzman Ganchev", "Jennifer Gillenwater", "Ben Taskar."], "venue": "Proceedings of the Joint Conference of the 47th Annual Meeting of the ACL and the 4th International Joint Conference on Natural Lan-", "citeRegEx": "Ganchev et al\\.,? 2009", "shortCiteRegEx": "Ganchev et al\\.", "year": 2009}, {"title": "Simple taskspecific bilingual word embeddings", "author": ["Stephan Gouws", "Anders S\u00f8gaard."], "venue": "Proceedings of the 2015 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 1386\u20131390, Den-", "citeRegEx": "Gouws and S\u00f8gaard.,? 2015", "shortCiteRegEx": "Gouws and S\u00f8gaard.", "year": 2015}, {"title": "Cross-lingual dependency parsing based on distributed representations", "author": ["Jiang Guo", "Wanxiang Che", "David Yarowsky", "Haifeng Wang", "Ting Liu."], "venue": "Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th Inter-", "citeRegEx": "Guo et al\\.,? 2015", "shortCiteRegEx": "Guo et al\\.", "year": 2015}, {"title": "A representation learning framework for multi-source transfer parsing", "author": ["Jiang Guo", "Wanxiang Che", "David Yarowsky", "Haifeng Wang", "Ting Liu."], "venue": "The Thirtieth AAAI Conference on Artificial Intelligence (AAAI-16), Phoenix, Arizona, USA.", "citeRegEx": "Guo et al\\.,? 2016", "shortCiteRegEx": "Guo et al\\.", "year": 2016}, {"title": "Structured perceptron with inexact search", "author": ["Liang Huang", "Suphan Fayong", "Yang Guo."], "venue": "Proceedings of the 2012 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 142\u2013", "citeRegEx": "Huang et al\\.,? 2012", "shortCiteRegEx": "Huang et al\\.", "year": 2012}, {"title": "Bootstrapping parsers via syntactic projection across parallel texts", "author": ["Rebecca Hwa", "Philip Resnik", "Amy Weinberg", "Clara Cabezas", "Okan Kolak."], "venue": "Natural language engineering, 11(03):311\u2013325.", "citeRegEx": "Hwa et al\\.,? 2005", "shortCiteRegEx": "Hwa et al\\.", "year": 2005}, {"title": "Moses: Open source toolkit for statistical machine translation", "author": ["Philipp Koehn", "Hieu Hoang", "Alexandra Birch", "Chris Callison-Burch", "Marcello Federico", "Nicola Bertoldi", "Brooke Cowan", "Wade Shen", "Christine Moran", "Richard Zens"], "venue": null, "citeRegEx": "Koehn et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Koehn et al\\.", "year": 2007}, {"title": "Europarl: A parallel corpus for statistical machine translation", "author": ["Philipp Koehn."], "venue": "MT summit, volume 5, pages 79\u201386.", "citeRegEx": "Koehn.,? 2005", "shortCiteRegEx": "Koehn.", "year": 2005}, {"title": "Frustratingly easy cross-lingual transfer for transition-based dependency parsing", "author": ["Oph\u00e9lie Lacroix", "Lauriane Aufrant", "Guillaume Wisniewski", "Fran\u00e7ois Yvon."], "venue": "Proceedings of the 2016 Conference of the North American Chapter of the Association for Com-", "citeRegEx": "Lacroix et al\\.,? 2016", "shortCiteRegEx": "Lacroix et al\\.", "year": 2016}, {"title": "Semi-supervised learning for natural language", "author": ["Percy Liang."], "venue": "Ph.D. thesis, Massachusetts Institute of Technology.", "citeRegEx": "Liang.,? 2005", "shortCiteRegEx": "Liang.", "year": 2005}, {"title": "Unsupervised dependency parsing with transferring distribution via parallel guidance and entropy regularization", "author": ["Xuezhe Ma", "Fei Xia."], "venue": "Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers),", "citeRegEx": "Ma and Xia.,? 2014", "shortCiteRegEx": "Ma and Xia.", "year": 2014}, {"title": "Effective self-training for parsing", "author": ["David McClosky", "Eugene Charniak", "Mark Johnson."], "venue": "Proceedings of the Main Conference on Human Language Technology Conference of the North American Chapter of the Association of Computational Linguistics,", "citeRegEx": "McClosky et al\\.,? 2006", "shortCiteRegEx": "McClosky et al\\.", "year": 2006}, {"title": "Multi-source transfer of delexicalized dependency parsers", "author": ["Ryan McDonald", "Slav Petrov", "Keith Hall."], "venue": "Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing, pages 62\u201372, Edinburgh, Scotland, UK., July. Associ-", "citeRegEx": "McDonald et al\\.,? 2011", "shortCiteRegEx": "McDonald et al\\.", "year": 2011}, {"title": "Selective sharing for multilingual dependency parsing", "author": ["Tahira Naseem", "Regina Barzilay", "Amir Globerson."], "venue": "Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics: Long Papers-Volume 1, pages 629\u2013637. Association", "citeRegEx": "Naseem et al\\.,? 2012", "shortCiteRegEx": "Naseem et al\\.", "year": 2012}, {"title": "Universal dependencies 1.3. LINDAT/CLARIN digital library", "author": ["Gertjan van Noord", "Viktor Varga", "Veronika Vincze", "Jing Xian Wang", "Jonathan North Washington", "Zden\u011bk \u017dabokrtsk\u00fd", "Daniel Zeman", "Hanzhi Zhu"], "venue": null, "citeRegEx": "Noord et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Noord et al\\.", "year": 2016}, {"title": "Giza++: Training of statistical translation models", "author": ["Franz Josef Och", "Hermann Ney"], "venue": null, "citeRegEx": "Och and Ney.,? \\Q2000\\E", "shortCiteRegEx": "Och and Ney.", "year": 2000}, {"title": "Density-driven cross-lingual transfer of dependency parsers", "author": ["Mohammad Sadegh Rasooli", "Michael Collins."], "venue": "Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, pages 328\u2013338, Lisbon, Portugal, September. Associ-", "citeRegEx": "Rasooli and Collins.,? 2015", "shortCiteRegEx": "Rasooli and Collins.", "year": 2015}, {"title": "Yara parser: A fast and accurate dependency parser", "author": ["Mohammad Sadegh Rasooli", "Joel Tetreault."], "venue": "arXiv preprint arXiv:1503.06733.", "citeRegEx": "Rasooli and Tetreault.,? 2015", "shortCiteRegEx": "Rasooli and Tetreault.", "year": 2015}, {"title": "Klcpos3 a language similarity measure for delexicalized parser transfer", "author": ["Rudolf Rosa", "Zdenek Zabokrtsky."], "venue": "Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural", "citeRegEx": "Rosa and Zabokrtsky.,? 2015", "shortCiteRegEx": "Rosa and Zabokrtsky.", "year": 2015}, {"title": "Model-based word embeddings from decompositions of count matrices", "author": ["Karl Stratos", "Michael Collins", "Daniel Hsu."], "venue": "Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Nat-", "citeRegEx": "Stratos et al\\.,? 2015", "shortCiteRegEx": "Stratos et al\\.", "year": 2015}, {"title": "Cross-lingual word clusters for direct transfer of linguistic structure", "author": ["Oscar T\u00e4ckstr\u00f6m", "Ryan McDonald", "Jakob Uszkoreit."], "venue": "Proceedings of the 2012 conference of the North American chapter of the association for computational linguistics: Human language", "citeRegEx": "T\u00e4ckstr\u00f6m et al\\.,? 2012", "shortCiteRegEx": "T\u00e4ckstr\u00f6m et al\\.", "year": 2012}, {"title": "Target language adaptation of discriminative transfer parsers", "author": ["Oscar T\u00e4ckstr\u00f6m", "Ryan McDonald", "Joakim Nivre."], "venue": "Transactions for ACL.", "citeRegEx": "T\u00e4ckstr\u00f6m et al\\.,? 2013", "shortCiteRegEx": "T\u00e4ckstr\u00f6m et al\\.", "year": 2013}, {"title": "Synthetic treebanking for cross-lingual dependency parsing", "author": ["J\u00f6rg Tiedemann", "\u017deljko Agi\u0107."], "venue": "Journal of Artificial Intelligence Research, 55:209\u2013248.", "citeRegEx": "Tiedemann and Agi\u0107.,? 2016", "shortCiteRegEx": "Tiedemann and Agi\u0107.", "year": 2016}, {"title": "Treebank translation for cross-lingual parser induction", "author": ["J\u00f6rg Tiedemann", "\u017deljko Agi\u0107", "Joakim Nivre."], "venue": "Proceedings of the Eighteenth Conference", "citeRegEx": "Tiedemann et al\\.,? 2014", "shortCiteRegEx": "Tiedemann et al\\.", "year": 2014}, {"title": "Improving the cross-lingual pro", "author": ["J\u00f6rg Tiedemann"], "venue": null, "citeRegEx": "Tiedemann.,? \\Q2015\\E", "shortCiteRegEx": "Tiedemann.", "year": 2015}], "referenceMentions": [{"referenceID": 24, "context": "Experiments show improvements over the state-of-the-art in several languages used in previous work (Rasooli and Collins, 2015; Zhang and Barzilay, 2015; Ammar et al., 2016), in a setting where the only source of translation data is the Bible, a considerably smaller corpus than the Europarl corpus used in previous work.", "startOffset": 99, "endOffset": 172}, {"referenceID": 1, "context": "Experiments show improvements over the state-of-the-art in several languages used in previous work (Rasooli and Collins, 2015; Zhang and Barzilay, 2015; Ammar et al., 2016), in a setting where the only source of translation data is the Bible, a considerably smaller corpus than the Europarl corpus used in previous work.", "startOffset": 99, "endOffset": 172}, {"referenceID": 4, "context": "The method makes use of three steps: 1) a method for deriving cross-lingual word clusters, that can then be used in a multilingual parser; 2) a method for transferring lexical information from a target language to source language treebanks; 3) a method for integrating these steps with the density-driven annotation projection method of Rasooli and Collins (2015). Experiments show improvements over the state-of-the-art in several languages used in previous work (Rasooli and Collins, 2015; Zhang and Barzilay, 2015; Ammar et al.", "startOffset": 349, "endOffset": 364}, {"referenceID": 1, "context": "Experiments show improvements over the state-of-the-art in several languages used in previous work (Rasooli and Collins, 2015; Zhang and Barzilay, 2015; Ammar et al., 2016), in a setting where the only source of translation data is the Bible, a considerably smaller corpus than the Europarl corpus used in previous work. Results using the Europarl corpus as a source of translation data show additional improvements over the results of Rasooli and Collins (2015). We conclude with results on 38 datasets (26 languages) from the Universal Dependencies corpora: 13 datasets (10 languages) have unlabeled attachment accuracies of 80% or higher; the average unlabeled accuracy on the 38 datasets is 74.", "startOffset": 153, "endOffset": 463}, {"referenceID": 13, "context": "Methods for syntactic transfer include annotation projection methods (Hwa et al., 2005; Ganchev et al., 2009; McDonald et al., 2011; Ma and Xia, 2014; Rasooli and Collins, 2015; Lacroix et al., 2016; Agi\u0107 et al., 2016), learning of delexicalized models on universal treebanks (Zeman and Resnik, 2008; McDonald et al.", "startOffset": 69, "endOffset": 218}, {"referenceID": 8, "context": "Methods for syntactic transfer include annotation projection methods (Hwa et al., 2005; Ganchev et al., 2009; McDonald et al., 2011; Ma and Xia, 2014; Rasooli and Collins, 2015; Lacroix et al., 2016; Agi\u0107 et al., 2016), learning of delexicalized models on universal treebanks (Zeman and Resnik, 2008; McDonald et al.", "startOffset": 69, "endOffset": 218}, {"referenceID": 20, "context": "Methods for syntactic transfer include annotation projection methods (Hwa et al., 2005; Ganchev et al., 2009; McDonald et al., 2011; Ma and Xia, 2014; Rasooli and Collins, 2015; Lacroix et al., 2016; Agi\u0107 et al., 2016), learning of delexicalized models on universal treebanks (Zeman and Resnik, 2008; McDonald et al.", "startOffset": 69, "endOffset": 218}, {"referenceID": 18, "context": "Methods for syntactic transfer include annotation projection methods (Hwa et al., 2005; Ganchev et al., 2009; McDonald et al., 2011; Ma and Xia, 2014; Rasooli and Collins, 2015; Lacroix et al., 2016; Agi\u0107 et al., 2016), learning of delexicalized models on universal treebanks (Zeman and Resnik, 2008; McDonald et al.", "startOffset": 69, "endOffset": 218}, {"referenceID": 24, "context": "Methods for syntactic transfer include annotation projection methods (Hwa et al., 2005; Ganchev et al., 2009; McDonald et al., 2011; Ma and Xia, 2014; Rasooli and Collins, 2015; Lacroix et al., 2016; Agi\u0107 et al., 2016), learning of delexicalized models on universal treebanks (Zeman and Resnik, 2008; McDonald et al.", "startOffset": 69, "endOffset": 218}, {"referenceID": 16, "context": "Methods for syntactic transfer include annotation projection methods (Hwa et al., 2005; Ganchev et al., 2009; McDonald et al., 2011; Ma and Xia, 2014; Rasooli and Collins, 2015; Lacroix et al., 2016; Agi\u0107 et al., 2016), learning of delexicalized models on universal treebanks (Zeman and Resnik, 2008; McDonald et al.", "startOffset": 69, "endOffset": 218}, {"referenceID": 0, "context": "Methods for syntactic transfer include annotation projection methods (Hwa et al., 2005; Ganchev et al., 2009; McDonald et al., 2011; Ma and Xia, 2014; Rasooli and Collins, 2015; Lacroix et al., 2016; Agi\u0107 et al., 2016), learning of delexicalized models on universal treebanks (Zeman and Resnik, 2008; McDonald et al.", "startOffset": 69, "endOffset": 218}, {"referenceID": 20, "context": ", 2016), learning of delexicalized models on universal treebanks (Zeman and Resnik, 2008; McDonald et al., 2011; T\u00e4ckstr\u00f6m et al., 2013; Rosa and Zabokrtsky, 2015), treebank translation (Tiedemann et al.", "startOffset": 65, "endOffset": 163}, {"referenceID": 29, "context": ", 2016), learning of delexicalized models on universal treebanks (Zeman and Resnik, 2008; McDonald et al., 2011; T\u00e4ckstr\u00f6m et al., 2013; Rosa and Zabokrtsky, 2015), treebank translation (Tiedemann et al.", "startOffset": 65, "endOffset": 163}, {"referenceID": 26, "context": ", 2016), learning of delexicalized models on universal treebanks (Zeman and Resnik, 2008; McDonald et al., 2011; T\u00e4ckstr\u00f6m et al., 2013; Rosa and Zabokrtsky, 2015), treebank translation (Tiedemann et al.", "startOffset": 65, "endOffset": 163}, {"referenceID": 31, "context": ", 2013; Rosa and Zabokrtsky, 2015), treebank translation (Tiedemann et al., 2014; Tiedemann, 2015; Tiedemann and Agi\u0107, 2016) and methods that leverage cross-lingual representations of word clusters, embeddings or dictionaries (T\u00e4ckstr\u00f6m et al.", "startOffset": 57, "endOffset": 124}, {"referenceID": 32, "context": ", 2013; Rosa and Zabokrtsky, 2015), treebank translation (Tiedemann et al., 2014; Tiedemann, 2015; Tiedemann and Agi\u0107, 2016) and methods that leverage cross-lingual representations of word clusters, embeddings or dictionaries (T\u00e4ckstr\u00f6m et al.", "startOffset": 57, "endOffset": 124}, {"referenceID": 30, "context": ", 2013; Rosa and Zabokrtsky, 2015), treebank translation (Tiedemann et al., 2014; Tiedemann, 2015; Tiedemann and Agi\u0107, 2016) and methods that leverage cross-lingual representations of word clusters, embeddings or dictionaries (T\u00e4ckstr\u00f6m et al.", "startOffset": 57, "endOffset": 124}, {"referenceID": 28, "context": ", 2014; Tiedemann, 2015; Tiedemann and Agi\u0107, 2016) and methods that leverage cross-lingual representations of word clusters, embeddings or dictionaries (T\u00e4ckstr\u00f6m et al., 2012; Durrett et al., 2012; Duong et al., 2015; Zhang and Barzilay, 2015; Xiao and Guo, 2015; Guo et al., 2015; Guo et al., 2016; Ammar et al., 2016).", "startOffset": 152, "endOffset": 320}, {"referenceID": 7, "context": ", 2014; Tiedemann, 2015; Tiedemann and Agi\u0107, 2016) and methods that leverage cross-lingual representations of word clusters, embeddings or dictionaries (T\u00e4ckstr\u00f6m et al., 2012; Durrett et al., 2012; Duong et al., 2015; Zhang and Barzilay, 2015; Xiao and Guo, 2015; Guo et al., 2015; Guo et al., 2016; Ammar et al., 2016).", "startOffset": 152, "endOffset": 320}, {"referenceID": 6, "context": ", 2014; Tiedemann, 2015; Tiedemann and Agi\u0107, 2016) and methods that leverage cross-lingual representations of word clusters, embeddings or dictionaries (T\u00e4ckstr\u00f6m et al., 2012; Durrett et al., 2012; Duong et al., 2015; Zhang and Barzilay, 2015; Xiao and Guo, 2015; Guo et al., 2015; Guo et al., 2016; Ammar et al., 2016).", "startOffset": 152, "endOffset": 320}, {"referenceID": 10, "context": ", 2014; Tiedemann, 2015; Tiedemann and Agi\u0107, 2016) and methods that leverage cross-lingual representations of word clusters, embeddings or dictionaries (T\u00e4ckstr\u00f6m et al., 2012; Durrett et al., 2012; Duong et al., 2015; Zhang and Barzilay, 2015; Xiao and Guo, 2015; Guo et al., 2015; Guo et al., 2016; Ammar et al., 2016).", "startOffset": 152, "endOffset": 320}, {"referenceID": 11, "context": ", 2014; Tiedemann, 2015; Tiedemann and Agi\u0107, 2016) and methods that leverage cross-lingual representations of word clusters, embeddings or dictionaries (T\u00e4ckstr\u00f6m et al., 2012; Durrett et al., 2012; Duong et al., 2015; Zhang and Barzilay, 2015; Xiao and Guo, 2015; Guo et al., 2015; Guo et al., 2016; Ammar et al., 2016).", "startOffset": 152, "endOffset": 320}, {"referenceID": 1, "context": ", 2014; Tiedemann, 2015; Tiedemann and Agi\u0107, 2016) and methods that leverage cross-lingual representations of word clusters, embeddings or dictionaries (T\u00e4ckstr\u00f6m et al., 2012; Durrett et al., 2012; Duong et al., 2015; Zhang and Barzilay, 2015; Xiao and Guo, 2015; Guo et al., 2015; Guo et al., 2016; Ammar et al., 2016).", "startOffset": 152, "endOffset": 320}, {"referenceID": 15, "context": "The Bible data contains a much smaller set of sentences (around 24,000) than other translation corpora, for example Europarl (Koehn, 2005), which has around 2 million sentences per language", "startOffset": 125, "endOffset": 138}, {"referenceID": 3, "context": "lation data because it is available for a very broad set of languages: the data from Christodouloupoulos and Steedman (2014) includes data from 100 languages.", "startOffset": 85, "endOffset": 125}, {"referenceID": 24, "context": "\u2022 We describe a method that integrates the above two approaches with the density-driven approach to annotation projection described in (Rasooli and Collins, 2015).", "startOffset": 135, "endOffset": 162}, {"referenceID": 17, "context": "Experiments show that our model outperforms previous work on a set of European languages from the Google universal treebank (McDonald et al., 2013): we achieve 80.9% average unlabeled attachment score (UAS) on these languages; in comparison the work of Zhang and Barzilay (2015), Guo et al.", "startOffset": 125, "endOffset": 279}, {"referenceID": 9, "context": "9% average unlabeled attachment score (UAS) on these languages; in comparison the work of Zhang and Barzilay (2015), Guo et al. (2016) and Ammar et al.", "startOffset": 117, "endOffset": 135}, {"referenceID": 1, "context": "(2016) and Ammar et al. (2016) have UAS of 75.", "startOffset": 11, "endOffset": 31}, {"referenceID": 15, "context": "these previous works make use of the much larger Europarl (Koehn, 2005) corpus to derive lexical", "startOffset": 58, "endOffset": 71}, {"referenceID": 24, "context": "68% absolute improvements over (Rasooli and Collins, 2015).", "startOffset": 31, "endOffset": 58}, {"referenceID": 5, "context": "68% absolute improvements over (Rasooli and Collins, 2015). Finally, we conduct experiments on 38 datasets (26 languages) in the universal dependencies v1.3 (Nivre et al., 2016) corpus. Our method has an average unlabeled dependency accuracy of 74.8% for these languages, compared to an average accuracy of 68.1% for the method of Rasooli and Collins (2015). 13 datasets (10 languages) have accuracies higher than 80.", "startOffset": 44, "endOffset": 358}, {"referenceID": 5, "context": "The parser is trained using the averaged structured perceptron (Collins, 2002).", "startOffset": 63, "endOffset": 78}, {"referenceID": 24, "context": "In our experiments we use the shift-reduce dependency parser of Rasooli and Tetreault (2015), which is an extension of the approach in (Zhang and Nivre, 2011).", "startOffset": 64, "endOffset": 93}, {"referenceID": 2, "context": "Clusters may for example be learned using the Brown clustering algorithm (Brown et al., 1992).", "startOffset": 73, "endOffset": 93}, {"referenceID": 3, "context": ", 2013) as our source language treebanks2 (this treebank provides universal dependency relations and POS tags), Wikipedia data as our monolingual data, and the Bible data from Christodouloupoulos and Steedman (2014) as the source of our translation data.", "startOffset": 176, "endOffset": 216}, {"referenceID": 29, "context": "Tm, using the representation \u03c6(x, y) = \u03c6(p)(x, y) (see (McDonald et al., 2013; T\u00e4ckstr\u00f6m et al., 2013)).", "startOffset": 55, "endOffset": 102}, {"referenceID": 19, "context": "We use self-training (McClosky et al., 2006) to further improve parsing performance.", "startOffset": 21, "endOffset": 44}, {"referenceID": 23, "context": "We first run the GIZA++ alignment process (Och and Ney, 2000) on the data Bi,j .", "startOffset": 42, "endOffset": 61}, {"referenceID": 28, "context": "cross-lingual clustering algorithms (T\u00e4ckstr\u00f6m et al., 2012).", "startOffset": 36, "endOffset": 60}, {"referenceID": 27, "context": "Use the algorithm of Stratos et al. (2015) on D to learn a clustering C.", "startOffset": 21, "endOffset": 43}, {"referenceID": 2, "context": "As one example, the Brown clustering algorithm (Brown et al., 1992) gives a hierarchical clustering.", "startOffset": 47, "endOffset": 67}, {"referenceID": 5, "context": "3 Integration with the Density-Driven Projection Method of Rasooli and Collins (2015)", "startOffset": 71, "endOffset": 86}, {"referenceID": 5, "context": "of Rasooli and Collins (2015), which makes use of density-driven projections.", "startOffset": 15, "endOffset": 30}, {"referenceID": 13, "context": "In annotation projection methods (Hwa et al., 2005; McDonald et al., 2011), it is assumed that we have translation data Bi,j for a source and target language, and that we have a dependency parser in the source language Li.", "startOffset": 33, "endOffset": 74}, {"referenceID": 20, "context": "In annotation projection methods (Hwa et al., 2005; McDonald et al., 2011), it is assumed that we have translation data Bi,j for a source and target language, and that we have a dependency parser in the source language Li.", "startOffset": 33, "endOffset": 74}, {"referenceID": 5, "context": "The density-driven approach of Rasooli and Collins (2015) makes use of various definitions of \u201cdensity\u201d of the projected dependencies.", "startOffset": 43, "endOffset": 58}, {"referenceID": 5, "context": "We integrate our approach with the density-driven approach of Rasooli and Collins (2015) as follows: Consider the treebanks T1 .", "startOffset": 74, "endOffset": 89}, {"referenceID": 5, "context": "We integrate our approach with the density-driven approach of Rasooli and Collins (2015) as follows: Consider the treebanks T1 . . . Tm created using the lexicalization method of \u00a73.2. We add all trees in these treebanks to the set P100 of full trees used to initialize the method of Rasooli and Collins (2015). In addition we make use of the representations \u03c6(p), \u03c6(c) and \u03c6(l), throughout the learning process.", "startOffset": 74, "endOffset": 311}, {"referenceID": 18, "context": "Data In a first set of experiments, we consider the 7 European languages studied in previous work (Ma and Xia, 2014; Zhang and Barzilay, 2015; Guo et al., 2016; Ammar et al., 2016; Lacroix et al., 2016).", "startOffset": 98, "endOffset": 202}, {"referenceID": 11, "context": "Data In a first set of experiments, we consider the 7 European languages studied in previous work (Ma and Xia, 2014; Zhang and Barzilay, 2015; Guo et al., 2016; Ammar et al., 2016; Lacroix et al., 2016).", "startOffset": 98, "endOffset": 202}, {"referenceID": 1, "context": "Data In a first set of experiments, we consider the 7 European languages studied in previous work (Ma and Xia, 2014; Zhang and Barzilay, 2015; Guo et al., 2016; Ammar et al., 2016; Lacroix et al., 2016).", "startOffset": 98, "endOffset": 202}, {"referenceID": 16, "context": "Data In a first set of experiments, we consider the 7 European languages studied in previous work (Ma and Xia, 2014; Zhang and Barzilay, 2015; Guo et al., 2016; Ammar et al., 2016; Lacroix et al., 2016).", "startOffset": 98, "endOffset": 202}, {"referenceID": 3, "context": "We use the Bible data from Christodouloupoulos and Steedman (2014), which includes data for 100 languages, as the source of translations.", "startOffset": 27, "endOffset": 67}, {"referenceID": 17, "context": "Brown Clustering Algorithm We use the off-theshelf Brown clustering tool5 (Liang, 2005) to train monolingual Brown clusters with 500 clusters.", "startOffset": 74, "endOffset": 87}, {"referenceID": 17, "context": "Brown Clustering Algorithm We use the off-theshelf Brown clustering tool5 (Liang, 2005) to train monolingual Brown clusters with 500 clusters. The monolingual Brown clusters are used as features over lexicalized values created in \u03c6(l), and in selftraining experiments. We train our cross-lingual clustering with the off-the-shelf-tool6 from Stratos et al. (2015). We set the window size to 2 with cluster size of 500.", "startOffset": 75, "endOffset": 363}, {"referenceID": 12, "context": "The parser is trained using the the maximum violation update strategy (Huang et al., 2012).", "startOffset": 70, "endOffset": 90}, {"referenceID": 24, "context": "Parsing Model We use the k-beam arc-eager dependency parser of Rasooli and Tetreault (2015), which is similar to the model of Zhang and Nivre (2011).", "startOffset": 63, "endOffset": 92}, {"referenceID": 24, "context": "Parsing Model We use the k-beam arc-eager dependency parser of Rasooli and Tetreault (2015), which is similar to the model of Zhang and Nivre (2011). We modify the parser such that it can use both monolingual and cross-lingual word cluster features.", "startOffset": 63, "endOffset": 149}, {"referenceID": 3, "context": "We excluded languages that are not completely present in the Bible corpus of Christodouloupoulos and Steedman (2014) (Ancient Greek, Basque, Catalan, Galician, Gothic, Irish, Kazakh, Latvian, Old Church Slavonic, and Tamil).", "startOffset": 77, "endOffset": 117}, {"referenceID": 27, "context": "Therefore we use the more efficient algorithm from Stratos et al. (2015) on the larger cross-lingual datasets to obtain word clusters.", "startOffset": 51, "endOffset": 73}, {"referenceID": 3, "context": "Each individual Bible text file from Christodouloupoulos and Steedman (2014) consists of 24720 sentences, except for English datasets, where two translations into English are available, giving double the amount of data.", "startOffset": 37, "endOffset": 77}, {"referenceID": 23, "context": "Word alignment We use the intersected alignments from Giza++ (Och and Ney, 2000) on translation data.", "startOffset": 61, "endOffset": 80}, {"referenceID": 5, "context": "2); 3) integration with the densitydriven method of Rasooli and Collins (2015). It can be seen that each of these three steps gives", "startOffset": 64, "endOffset": 79}, {"referenceID": 4, "context": "\u201cDensity\u201d refers to the method of Rasooli and Collins (2015); \u201cThis paper\u201d gives results using the methods described in sections 3.", "startOffset": 46, "endOffset": 61}, {"referenceID": 3, "context": "The \u201cBible\u201d experiments use the Bible data of Christodouloupoulos and Steedman (2014). The \u201cEuroparl\u201d experiments use the Europarl data of Koehn (2005).", "startOffset": 46, "endOffset": 86}, {"referenceID": 3, "context": "The \u201cBible\u201d experiments use the Bible data of Christodouloupoulos and Steedman (2014). The \u201cEuroparl\u201d experiments use the Europarl data of Koehn (2005). The \u201cEuroparl-Sample\u201d experiments use 25K randomly chosen sentences from Europarl; this gives a similar number of sentences to the Bible data.", "startOffset": 46, "endOffset": 152}, {"referenceID": 11, "context": "Barzilay, 2015), GCY16 (Guo et al., 2016), AMB 16 (Ammar et al.", "startOffset": 23, "endOffset": 41}, {"referenceID": 1, "context": ", 2016), AMB 16 (Ammar et al., 2016), and RC15 (Rasooli and Collins, 2015).", "startOffset": 16, "endOffset": 36}, {"referenceID": 24, "context": ", 2016), and RC15 (Rasooli and Collins, 2015).", "startOffset": 18, "endOffset": 45}, {"referenceID": 5, "context": "33% UAS of Rasooli and Collins (2015).", "startOffset": 23, "endOffset": 38}, {"referenceID": 8, "context": "Comparison to Other Previous Work Table 7 compares the accuracy of our method to the following related work: 1) Zhang and Barzilay (2015), who describe a method that learns cross-lingual embeddings and bilingual dictionaries from Europarl data, and uses these features in a discriminative parsing model; 2) Guo et al. (2016), who describe a method that learns cross-lingual embeddings from Europarl data and uses a shift-reduce neural parser with these representations; 3) Ammar et al.", "startOffset": 307, "endOffset": 325}, {"referenceID": 1, "context": "(2016), who describe a method that learns cross-lingual embeddings from Europarl data and uses a shift-reduce neural parser with these representations; 3) Ammar et al. (2016), who use the same embeddings as Guo et al.", "startOffset": 155, "endOffset": 175}, {"referenceID": 1, "context": "(2016), who describe a method that learns cross-lingual embeddings from Europarl data and uses a shift-reduce neural parser with these representations; 3) Ammar et al. (2016), who use the same embeddings as Guo et al. (2016), within an LSTM-based parser; and 4) Rasooli and Collins (2015) who use the density-driven approach on the Europarl data.", "startOffset": 155, "endOffset": 225}, {"referenceID": 1, "context": "(2016), who describe a method that learns cross-lingual embeddings from Europarl data and uses a shift-reduce neural parser with these representations; 3) Ammar et al. (2016), who use the same embeddings as Guo et al. (2016), within an LSTM-based parser; and 4) Rasooli and Collins (2015) who use the density-driven approach on the Europarl data.", "startOffset": 155, "endOffset": 289}, {"referenceID": 5, "context": "improves the state-of-the-art model of Rasooli and Collins (2015). It is worth noting that our model significantly outperform the UAS reported in other", "startOffset": 51, "endOffset": 66}, {"referenceID": 5, "context": "Rasooli and Collins (2015) do not report results on English.", "startOffset": 12, "endOffset": 27}, {"referenceID": 5, "context": "RC15 refers to the best performing model of Rasooli and Collins (2015).", "startOffset": 56, "endOffset": 71}, {"referenceID": 20, "context": "previous work (McDonald et al., 2011; Ma and Xia, 2014; Lacroix et al., 2016) but we do not put theirs because of space restrictions.", "startOffset": 14, "endOffset": 77}, {"referenceID": 18, "context": "previous work (McDonald et al., 2011; Ma and Xia, 2014; Lacroix et al., 2016) but we do not put theirs because of space restrictions.", "startOffset": 14, "endOffset": 77}, {"referenceID": 16, "context": "previous work (McDonald et al., 2011; Ma and Xia, 2014; Lacroix et al., 2016) but we do not put theirs because of space restrictions.", "startOffset": 14, "endOffset": 77}, {"referenceID": 5, "context": "Although there is a drop in accuracy when using the automatic POS tags, our model has still a high accuracy even higher than those from the density driven approach of Rasooli and Collins (2015).", "startOffset": 179, "endOffset": 194}, {"referenceID": 5, "context": "sooli and Collins (2015) and for many languages the accuracy of our method gets close to the accuracy of the supervised parser.", "startOffset": 10, "endOffset": 25}, {"referenceID": 20, "context": "A number of methods (Zeman and Resnik, 2008; McDonald et al., 2011; Cohen et al., 2011; Naseem et al., 2012; T\u00e4ckstr\u00f6m et al., 2013; Rosa and Zabokrtsky, 2015) directly learn delexicalized models that can be trained on universal treebank data from one or more source languages, then applied to the target language.", "startOffset": 20, "endOffset": 159}, {"referenceID": 4, "context": "A number of methods (Zeman and Resnik, 2008; McDonald et al., 2011; Cohen et al., 2011; Naseem et al., 2012; T\u00e4ckstr\u00f6m et al., 2013; Rosa and Zabokrtsky, 2015) directly learn delexicalized models that can be trained on universal treebank data from one or more source languages, then applied to the target language.", "startOffset": 20, "endOffset": 159}, {"referenceID": 21, "context": "A number of methods (Zeman and Resnik, 2008; McDonald et al., 2011; Cohen et al., 2011; Naseem et al., 2012; T\u00e4ckstr\u00f6m et al., 2013; Rosa and Zabokrtsky, 2015) directly learn delexicalized models that can be trained on universal treebank data from one or more source languages, then applied to the target language.", "startOffset": 20, "endOffset": 159}, {"referenceID": 29, "context": "A number of methods (Zeman and Resnik, 2008; McDonald et al., 2011; Cohen et al., 2011; Naseem et al., 2012; T\u00e4ckstr\u00f6m et al., 2013; Rosa and Zabokrtsky, 2015) directly learn delexicalized models that can be trained on universal treebank data from one or more source languages, then applied to the target language.", "startOffset": 20, "endOffset": 159}, {"referenceID": 26, "context": "A number of methods (Zeman and Resnik, 2008; McDonald et al., 2011; Cohen et al., 2011; Naseem et al., 2012; T\u00e4ckstr\u00f6m et al., 2013; Rosa and Zabokrtsky, 2015) directly learn delexicalized models that can be trained on universal treebank data from one or more source languages, then applied to the target language.", "startOffset": 20, "endOffset": 159}, {"referenceID": 24, "context": "Table 9: Results for the density driven method (Rasooli and Collins, 2015) and ours using the Bible data on the universal dependencies v1.", "startOffset": 47, "endOffset": 74}, {"referenceID": 10, "context": "has introduced cross-lingual representations \u2013for example cross-lingual word-embeddings\u2013 that can be used to improve performance (Zhang and Barzilay, 2015; Guo et al., 2015; Duong et al., 2015; Guo et al., 2016; Ammar et al., 2016).", "startOffset": 129, "endOffset": 231}, {"referenceID": 6, "context": "has introduced cross-lingual representations \u2013for example cross-lingual word-embeddings\u2013 that can be used to improve performance (Zhang and Barzilay, 2015; Guo et al., 2015; Duong et al., 2015; Guo et al., 2016; Ammar et al., 2016).", "startOffset": 129, "endOffset": 231}, {"referenceID": 11, "context": "has introduced cross-lingual representations \u2013for example cross-lingual word-embeddings\u2013 that can be used to improve performance (Zhang and Barzilay, 2015; Guo et al., 2015; Duong et al., 2015; Guo et al., 2016; Ammar et al., 2016).", "startOffset": 129, "endOffset": 231}, {"referenceID": 1, "context": "has introduced cross-lingual representations \u2013for example cross-lingual word-embeddings\u2013 that can be used to improve performance (Zhang and Barzilay, 2015; Guo et al., 2015; Duong et al., 2015; Guo et al., 2016; Ammar et al., 2016).", "startOffset": 129, "endOffset": 231}, {"referenceID": 11, "context": "We show results for the methods of (Zhang and Barzilay, 2015; Guo et al., 2016; Ammar et al., 2016) in Table 7 of this paper.", "startOffset": 35, "endOffset": 99}, {"referenceID": 1, "context": "We show results for the methods of (Zhang and Barzilay, 2015; Guo et al., 2016; Ammar et al., 2016) in Table 7 of this paper.", "startOffset": 35, "endOffset": 99}, {"referenceID": 13, "context": "The annotation projection approach, where dependencies from one language are transferred through translation alignments to another language, has been considered by several authors (Hwa et al., 2005; Ganchev et al., 2009; McDonald et al., 2011; Ma and Xia, 2014; Rasooli and Collins, 2015; Lacroix et al., 2016; Agi\u0107 et al., 2016).", "startOffset": 180, "endOffset": 329}, {"referenceID": 8, "context": "The annotation projection approach, where dependencies from one language are transferred through translation alignments to another language, has been considered by several authors (Hwa et al., 2005; Ganchev et al., 2009; McDonald et al., 2011; Ma and Xia, 2014; Rasooli and Collins, 2015; Lacroix et al., 2016; Agi\u0107 et al., 2016).", "startOffset": 180, "endOffset": 329}, {"referenceID": 20, "context": "The annotation projection approach, where dependencies from one language are transferred through translation alignments to another language, has been considered by several authors (Hwa et al., 2005; Ganchev et al., 2009; McDonald et al., 2011; Ma and Xia, 2014; Rasooli and Collins, 2015; Lacroix et al., 2016; Agi\u0107 et al., 2016).", "startOffset": 180, "endOffset": 329}, {"referenceID": 18, "context": "The annotation projection approach, where dependencies from one language are transferred through translation alignments to another language, has been considered by several authors (Hwa et al., 2005; Ganchev et al., 2009; McDonald et al., 2011; Ma and Xia, 2014; Rasooli and Collins, 2015; Lacroix et al., 2016; Agi\u0107 et al., 2016).", "startOffset": 180, "endOffset": 329}, {"referenceID": 24, "context": "The annotation projection approach, where dependencies from one language are transferred through translation alignments to another language, has been considered by several authors (Hwa et al., 2005; Ganchev et al., 2009; McDonald et al., 2011; Ma and Xia, 2014; Rasooli and Collins, 2015; Lacroix et al., 2016; Agi\u0107 et al., 2016).", "startOffset": 180, "endOffset": 329}, {"referenceID": 16, "context": "The annotation projection approach, where dependencies from one language are transferred through translation alignments to another language, has been considered by several authors (Hwa et al., 2005; Ganchev et al., 2009; McDonald et al., 2011; Ma and Xia, 2014; Rasooli and Collins, 2015; Lacroix et al., 2016; Agi\u0107 et al., 2016).", "startOffset": 180, "endOffset": 329}, {"referenceID": 0, "context": "The annotation projection approach, where dependencies from one language are transferred through translation alignments to another language, has been considered by several authors (Hwa et al., 2005; Ganchev et al., 2009; McDonald et al., 2011; Ma and Xia, 2014; Rasooli and Collins, 2015; Lacroix et al., 2016; Agi\u0107 et al., 2016).", "startOffset": 180, "endOffset": 329}, {"referenceID": 0, "context": ", 2016; Agi\u0107 et al., 2016). The work of Rasooli and Collins (2015) gives the best results of these methods on the Europarl data, as shown in Table 7.", "startOffset": 8, "endOffset": 67}, {"referenceID": 31, "context": "Other recent work (Tiedemann et al., 2014; Tiedemann, 2015; Tiedemann and Agi\u0107, 2016) has considered treebank translation, where a statistical machine translation system (e.", "startOffset": 18, "endOffset": 85}, {"referenceID": 32, "context": "Other recent work (Tiedemann et al., 2014; Tiedemann, 2015; Tiedemann and Agi\u0107, 2016) has considered treebank translation, where a statistical machine translation system (e.", "startOffset": 18, "endOffset": 85}, {"referenceID": 30, "context": "Other recent work (Tiedemann et al., 2014; Tiedemann, 2015; Tiedemann and Agi\u0107, 2016) has considered treebank translation, where a statistical machine translation system (e.", "startOffset": 18, "endOffset": 85}, {"referenceID": 14, "context": ", MOSES (Koehn et al., 2007)) is used to translate a source language treebank into the target language, complete with reordering of the input sentence.", "startOffset": 8, "endOffset": 28}, {"referenceID": 21, "context": "A number of authors have considered incorporating universal syntactic properties such as dependency order by selectively learning syntactic attributes from similar source languages (Naseem et al., 2012; T\u00e4ckstr\u00f6m et al., 2013; Zhang and Barzilay, 2015; Ammar et al., 2016).", "startOffset": 181, "endOffset": 272}, {"referenceID": 29, "context": "A number of authors have considered incorporating universal syntactic properties such as dependency order by selectively learning syntactic attributes from similar source languages (Naseem et al., 2012; T\u00e4ckstr\u00f6m et al., 2013; Zhang and Barzilay, 2015; Ammar et al., 2016).", "startOffset": 181, "endOffset": 272}, {"referenceID": 1, "context": "A number of authors have considered incorporating universal syntactic properties such as dependency order by selectively learning syntactic attributes from similar source languages (Naseem et al., 2012; T\u00e4ckstr\u00f6m et al., 2013; Zhang and Barzilay, 2015; Ammar et al., 2016).", "startOffset": 181, "endOffset": 272}, {"referenceID": 28, "context": "A number of authors (T\u00e4ckstr\u00f6m et al., 2012; Guo et al., 2015; Guo et al., 2016) have introduced methods that learn cross-lingual representations that are then used in syntactic transfer.", "startOffset": 20, "endOffset": 80}, {"referenceID": 10, "context": "A number of authors (T\u00e4ckstr\u00f6m et al., 2012; Guo et al., 2015; Guo et al., 2016) have introduced methods that learn cross-lingual representations that are then used in syntactic transfer.", "startOffset": 20, "endOffset": 80}, {"referenceID": 11, "context": "A number of authors (T\u00e4ckstr\u00f6m et al., 2012; Guo et al., 2015; Guo et al., 2016) have introduced methods that learn cross-lingual representations that are then used in syntactic transfer.", "startOffset": 20, "endOffset": 80}, {"referenceID": 6, "context": "(see Figure 1) which is then used as input to a clustering algorithm is closely related to (Duong et al., 2015; Gouws and S\u00f8gaard, 2015; Wick et al., 2015).", "startOffset": 91, "endOffset": 155}, {"referenceID": 9, "context": "(see Figure 1) which is then used as input to a clustering algorithm is closely related to (Duong et al., 2015; Gouws and S\u00f8gaard, 2015; Wick et al., 2015).", "startOffset": 91, "endOffset": 155}], "year": 2016, "abstractText": "We describe a simple but effective method for cross-lingual syntactic transfer of dependency parsers, in the scenario where a large amount of translation data is not available. The method makes use of three steps: 1) a method for deriving cross-lingual word clusters, that can then be used in a multilingual parser; 2) a method for transferring lexical information from a target language to source language treebanks; 3) a method for integrating these steps with the density-driven annotation projection method of Rasooli and Collins (2015). Experiments show improvements over the state-of-the-art in several languages used in previous work (Rasooli and Collins, 2015; Zhang and Barzilay, 2015; Ammar et al., 2016), in a setting where the only source of translation data is the Bible, a considerably smaller corpus than the Europarl corpus used in previous work. Results using the Europarl corpus as a source of translation data show additional improvements over the results of Rasooli and Collins (2015). We conclude with results on 38 datasets (26 languages) from the Universal Dependencies corpora: 13 datasets (10 languages) have unlabeled attachment accuracies of 80% or higher; the average unlabeled accuracy on the 38 datasets is 74.8%.", "creator": "dvips(k) 5.991 Copyright 2011 Radical Eye Software"}}}