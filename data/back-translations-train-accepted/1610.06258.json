{"id": "1610.06258", "review": {"conference": "NIPS", "VERSION": "v1", "DATE_OF_SUBMISSION": "20-Oct-2016", "title": "Using Fast Weights to Attend to the Recent Past", "abstract": "Until recently, research on artificial neural networks was largely restricted to systems with only two types of variable: Neural activities that represent the current or recent input and weights that learn to capture regularities among inputs, outputs and payoffs. There is no good reason for this restriction. Synapses have dynamics at many different time-scales and this suggests that artificial neural networks might benefit from variables that change slower than activities but much faster than the standard weights. These \"fast weights\" can be used to store temporary memories of the recent past and they provide a neurally plausible way of implementing the type of attention to the past that has recently proved very helpful in sequence-to-sequence models. By using fast weights we can avoid the need to store copies of neural activity patterns.", "histories": [["v1", "Thu, 20 Oct 2016 01:03:20 GMT  (153kb,D)", "http://arxiv.org/abs/1610.06258v1", null], ["v2", "Thu, 27 Oct 2016 19:53:07 GMT  (154kb,D)", "http://arxiv.org/abs/1610.06258v2", null], ["v3", "Mon, 5 Dec 2016 00:14:01 GMT  (162kb,D)", "http://arxiv.org/abs/1610.06258v3", "Added [Schmidhuber 1993] citation to the last paragraph of the introduction. Fixed typo appendix A.1 uniform initialization to 1/\\sqrt{H}"]], "reviews": [], "SUBJECTS": "stat.ML cs.LG cs.NE", "authors": ["jimmy ba", "geoffrey e hinton", "volodymyr mnih", "joel z leibo", "catalin ionescu"], "accepted": true, "id": "1610.06258"}, "pdf": {"name": "1610.06258.pdf", "metadata": {"source": "CRF", "title": "Using Fast Weights to Attend to the Recent Past", "authors": ["Jimmy Ba", "Geoffrey Hinton", "Joel Z. Leibo", "Catalin Ionescu"], "emails": ["jimmy@psi.toronto.edu", "geoffhinton@google.com", "vmnih@google.com", "jzl@google.com", "cdi@google.com"], "sections": [{"heading": "1 Introduction", "text": "Ordinary recursive neural networks typically have two types of memory that have very different timescales, very different capacities and very different computational roles. However, the history of the sequence currently being processed is stored in the hidden activity vector, which acts like a short-term memory that is updated at each time step. However, the capacity of this memory is O (H), where H is the number of hidden units. Long-term memory about how the current input and hidden vectors are converted to the next hidden vector, and a predicted output vector is stored in the weight matrices that connect the hidden units to themselves and the inputs and outputs. These matrices are typically updated at the end of a sequence and their capacity is O (H2) + O (HO), where I and O are the numbers of the input and output units."}, {"heading": "2 Evidence from physiology that temporary memory may not be stored as neural activities", "text": "Processes such as working memory, attention and priming operate on a time scale of 100 ms to minutes, which is simultaneously too slow to be mediated by neuronal activations without dynamic attractor states (10 ms time scale) and too fast to initiate long-term synaptic plasticity mechanisms (minutes to hours). While research on artificial neural networks typically focuses on methods for maintaining temporary states in activation dynamics, this focus may be at odds with evidence that the brain also - or perhaps primarily - maintains transient state information through short-term synaptic plasticity mechanisms Tsodyks et al. [1998], Abbott and Regehr [2004], Barak and Tsodyks [2007]. The brain implements a variety of short-term plasticity mechanisms operating on an interim time scale."}, {"heading": "3 Fast Associative Memory", "text": "One of the main concerns of neural network research in the 1970s and early 1980s is the ability to record all the other Gardner bits, but we have only a slow bias of neural network research in the 1970s and early 1980s. [1969], Kohonen [1972], Anderson and Hinton [1981], Hopfield [1982] was the idea that memories were not stored by somehow preserving copies of patterns of neural activity. Instead, these patterns were reconstructed when they could be stored from information stored in the weights of an associative network and the same weights could store many different memories. An autoassociative memory that has N2 weights cannot be expected to store more real vectors with N components each. How close we can get to this upper limit depends on which memory rule we use."}, {"heading": "3.1 Layer normalized fast weights", "text": "A potential problem with fast associative memory is that the scalar product of two hidden vectors could disappear or explode, depending on the norm of hidden vectors. Recently, the layer normalization Ba et al. [2016] has been shown to be very effective in stabilizing the hidden state dynamics in RNs and reducing the training time. Layer normalization is applied to the vector of summed inputs to all recursive units in a given time step. It uses the mean and variance of the components of that vector to re-center and re-scale these summed inputs. Then, before applying nonlinearity, it includes a learned neuron-specific bias and amplification. We apply the layer normalization to the fast associative memory as follows: hs + 1 (t + 1) = f (LN [Wh) + Cx (+ A)% (A)% (h) of the perforation layer [We found] that the perforation layer [1] is a (h) of the choice of the normalization layer [1] in which we found."}, {"heading": "4 Experimental results", "text": "To demonstrate the effectiveness of fast associative memory, we first investigated the problems of associative recall (Section 4.1) and MNIST classification (Section 4.2). We compared quick weight models with regular RNN and LSTM variants, and then applied the proposed fast weights to a task for facial expression recognition using a fast associative memory model to store processing results on a level while examining a sequence of details on a finer level (Section 4.3). Hyperparameters of the experiments were selected by looking for grids on the validation set. All models were trained on size 128 mini-batches and the Adam optimizer Kingma and Ba [2014]. A description of training protocols and the hyperparameter settings we used can be found in the appendix. Finally, we show that fast weights can also be used effectively to implement memory enhancement with Section 4.4."}, {"heading": "4.1 Associative retrieval", "text": "We start by showing that the method we propose for storing and restoring temporary memories is effective for a toy task for which it is very suitable. Consider a task in which several key-value pairs are presented in a sequence. At the end of the sequence, one of the keys is introduced and the model must predict the value that was temporarily associated with the key. We used strings that contained characters from the English alphabet, along with the digits 0 to 9. To construct a training sequence, we first try a character from the alphabet without substitution. This is the first key. Then, a single digit is scanned as the associated value for that key."}, {"heading": "4.2 Integrating glimpses in visual attention models", "text": "In fact, most of them will be able to move to another world, in which they will be able to move, and in which they will be able to move to another world, in which they will be able to move."}, {"heading": "4.3 Facial expression recognition", "text": "To further investigate the benefits of using fast weights in the multi-level visual attention model, we performed facial expression recognition tasks on the CMU Multi-PIE Face Database Gross et al. [2010]. The dataset was pre-processed to align each face through eyes and nostrils, but it was reduced to 48 x 48 grayscales. The complete dataset contains 15 photos taken by cameras with different viewing angles for each illumination, and the resulting datasets contained > 100 x 000 images. We used only the images taken by the three central cameras that were \u2212 15 x, 15 x, views not discernible since facial expressions."}, {"heading": "4.4 Agents with memory", "text": "While different types of memory and attention have been extensively studied in the monitored learning environment, Graves [2014], Mnih et al. [2014], Bahdanau et al. [2015], the use of such models to learn long-distance dependencies in amplification learning has attracted less attention. We compare different memory architectures on a partially observable variant of the game \"Catch,\" described in Mnih et al. [2014] The game is played on a N \u00b7 N binary pixel screen and each episode consists of N-frames. Each study begins with a single pixel representing a ball appearing somewhere in the first row of the column, and a two-pixel paddle controlled by the agent in the lower row. After observing a frame, the paddle receives stationary or moves it to the right or left of a pixel."}, {"heading": "5 Conclusion", "text": "This paper contributes to machine learning by showing that the performance of RNNs can be improved in a variety of different tasks by introducing a mechanism that allows any new state of hidden units to be attracted to new hidden states, proportional to their scalar products in the current state. By normalizing the layer, this kind of attention works much better. This is a form of attention in the recent past that resembles the attention mechanism recently used to dramatically improve the sequence-to-sequence RNNNs used in machine translation. The paper has interesting implications for computational neuroscience and cognitive science. The ability of humans to apply this very knowledge and the processing apparatus recursively to an entire set and to an embedded set of neuronal connections within this set or to a complex object and to a large part of this object, neuronal connections have long been used to argue that neuronal networks are not a good cognitive model for the recent past."}, {"heading": "A Experimental details", "text": "rE \"s tis rf\u00fc ide nree\u00fcdBr,\" so rf\u00fc ide rf\u00fc ide rf\u00fc ide rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf the rf\u00fc the rf the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf the rf\u00fc the rf the rf the rf\u00fc the rf\u00fc the rf the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf the rf the rf the rf the rf the rf the rf the rf\u00fc the rf\u00fc the r"}], "references": [{"title": "Models of information processing in the brain", "author": ["James A Anderson", "Geoffrey E Hinton"], "venue": "Parallel models of associative memory,", "citeRegEx": "Anderson and Hinton.,? \\Q1981\\E", "shortCiteRegEx": "Anderson and Hinton.", "year": 1981}, {"title": "Multiple object recognition with visual attention", "author": ["J. Ba", "V. Mnih", "K. Kavukcuoglu"], "venue": "In International Conference on Learning Representations,", "citeRegEx": "Ba et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Ba et al\\.", "year": 2015}, {"title": "Neural machine translation by jointly learning to align and translate", "author": ["D. Bahdanau", "K. Cho", "Y. Bengio"], "venue": "In International Conference on Learning Representations,", "citeRegEx": "Bahdanau et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Bahdanau et al\\.", "year": 2015}, {"title": "Persistent activity in neural networks with dynamic synapses", "author": ["Omri Barak", "Misha Tsodyks"], "venue": "PLoS Comput Biol,", "citeRegEx": "Barak and Tsodyks.,? \\Q2007\\E", "shortCiteRegEx": "Barak and Tsodyks.", "year": 2007}, {"title": "Synaptic modifications in cultured hippocampal neurons: dependence on spike timing, synaptic strength, and postsynaptic cell type", "author": ["Guo-qiang Bi", "Mu-ming Poo"], "venue": "The Journal of neuroscience,", "citeRegEx": "Bi and Poo.,? \\Q1998\\E", "shortCiteRegEx": "Bi and Poo.", "year": 1998}, {"title": "Associative long short-term memory", "author": ["Ivo Danihelka", "Greg Wayne", "Benigno Uria", "Nal Kalchbrenner", "Alex Graves"], "venue": "arXiv preprint arXiv:1602.03032,", "citeRegEx": "Danihelka et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Danihelka et al\\.", "year": 2016}, {"title": "The space of interactions in neural network models", "author": ["Elizabeth Gardner"], "venue": "Journal of physics A: Mathematical and general,", "citeRegEx": "Gardner.,? \\Q1988\\E", "shortCiteRegEx": "Gardner.", "year": 1988}, {"title": "Generating sequences with recurrent neural networks", "author": ["A. Graves"], "venue": null, "citeRegEx": "Graves.,? \\Q2014\\E", "shortCiteRegEx": "Graves.", "year": 2014}, {"title": "Neural turing machines", "author": ["Alex Graves", "Greg Wayne", "Ivo Danihelka"], "venue": "arXiv preprint arXiv:1410.5401,", "citeRegEx": "Graves et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Graves et al\\.", "year": 2014}, {"title": "Learning to transduce with unbounded memory", "author": ["Edward Grefenstette", "Karl Moritz Hermann", "Mustafa Suleyman", "Phil Blunsom"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Grefenstette et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Grefenstette et al\\.", "year": 2015}, {"title": "Using fast weights to deblur old memories", "author": ["Geoffrey E Hinton", "David C Plaut"], "venue": "In Proceedings of the ninth annual conference of the Cognitive Science Society,", "citeRegEx": "Hinton and Plaut.,? \\Q1987\\E", "shortCiteRegEx": "Hinton and Plaut.", "year": 1987}, {"title": "Long short-term memory", "author": ["Sepp Hochreiter", "J\u00fcrgen Schmidhuber"], "venue": "Neural computation,", "citeRegEx": "Hochreiter and Schmidhuber.,? \\Q1997\\E", "shortCiteRegEx": "Hochreiter and Schmidhuber.", "year": 1997}, {"title": "Neural networks and physical systems with emergent collective computational abilities", "author": ["John J Hopfield"], "venue": "Proceedings of the national academy of sciences,", "citeRegEx": "Hopfield.,? \\Q1982\\E", "shortCiteRegEx": "Hopfield.", "year": 1982}, {"title": "Adam: a method for stochastic optimization", "author": ["D. Kingma", "J.L. Ba"], "venue": null, "citeRegEx": "Kingma and Ba.,? \\Q2014\\E", "shortCiteRegEx": "Kingma and Ba.", "year": 2014}, {"title": "Correlation matrix memories", "author": ["Teuvo Kohonen"], "venue": "Computers, IEEE Transactions on,", "citeRegEx": "Kohonen.,? \\Q1972\\E", "shortCiteRegEx": "Kohonen.", "year": 1972}, {"title": "Regulation of synaptic efficacy by coincidence of postsynaptic aps and epsps", "author": ["Henry Markram", "Joachim L\u00fcbke", "Michael Frotscher", "Bert Sakmann"], "venue": null, "citeRegEx": "Markram et al\\.,? \\Q1997\\E", "shortCiteRegEx": "Markram et al\\.", "year": 1997}, {"title": "Recurrent models of visual attention", "author": ["V. Mnih", "N. Heess", "A. Graves", "K. Kavukcuoglu"], "venue": "In Neural Information Processing Systems,", "citeRegEx": "Mnih et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Mnih et al\\.", "year": 2014}, {"title": "Asynchronous methods for deep reinforcement learning", "author": ["Volodymyr Mnih", "Adria Puigdomenech Badia", "Mehdi Mirza", "Alex Graves", "Timothy P Lillicrap", "Tim Harley", "David Silver", "Koray Kavukcuoglu"], "venue": "In International Conference on Machine Learning,", "citeRegEx": "Mnih et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Mnih et al\\.", "year": 2016}], "referenceMentions": [{"referenceID": 10, "context": "Long short-term memory networks Hochreiter and Schmidhuber [1997] are a more complicated type of RNN that work better for discovering long-range structure in sequences for two main reasons: First, they compute increments to the hidden activity vector at each time step rather than recomputing the full vector1.", "startOffset": 32, "endOffset": 66}, {"referenceID": 10, "context": "Several researchers Hinton and Plaut [1987], Schmidhuber [1992] have suggested that neural networks could benefit from a third form of memory that has much higher storage capacity than the neural activities but much faster dynamics than the standard \u201cslow\u201d weights.", "startOffset": 20, "endOffset": 44}, {"referenceID": 10, "context": "Several researchers Hinton and Plaut [1987], Schmidhuber [1992] have suggested that neural networks could benefit from a third form of memory that has much higher storage capacity than the neural activities but much faster dynamics than the standard \u201cslow\u201d weights.", "startOffset": 20, "endOffset": 64}, {"referenceID": 3, "context": "[1998], Abbott and Regehr [2004], Barak and Tsodyks [2007]. The brain implements a variety of short-term plasticity mechanisms that operate on intermediate timescale.", "startOffset": 34, "endOffset": 59}, {"referenceID": 3, "context": "[1998], Abbott and Regehr [2004], Barak and Tsodyks [2007]. The brain implements a variety of short-term plasticity mechanisms that operate on intermediate timescale. For example, short term facilitation is implemented by leftover [Ca] in the axon terminal after depolarization while short term depression is implemented by presynaptic neurotransmitter depletion Zucker and Regehr [2002]. Spike-time dependent plasticity can also be invoked on this timescale Markram et al.", "startOffset": 34, "endOffset": 388}, {"referenceID": 3, "context": "[1998], Abbott and Regehr [2004], Barak and Tsodyks [2007]. The brain implements a variety of short-term plasticity mechanisms that operate on intermediate timescale. For example, short term facilitation is implemented by leftover [Ca] in the axon terminal after depolarization while short term depression is implemented by presynaptic neurotransmitter depletion Zucker and Regehr [2002]. Spike-time dependent plasticity can also be invoked on this timescale Markram et al. [1997], Bi and Poo [1998].", "startOffset": 34, "endOffset": 481}, {"referenceID": 3, "context": "[1998], Abbott and Regehr [2004], Barak and Tsodyks [2007]. The brain implements a variety of short-term plasticity mechanisms that operate on intermediate timescale. For example, short term facilitation is implemented by leftover [Ca] in the axon terminal after depolarization while short term depression is implemented by presynaptic neurotransmitter depletion Zucker and Regehr [2002]. Spike-time dependent plasticity can also be invoked on this timescale Markram et al. [1997], Bi and Poo [1998]. These plasticity mechanisms are all synapsespecific.", "startOffset": 34, "endOffset": 500}, {"referenceID": 8, "context": "[1969], Kohonen [1972], Anderson and Hinton [1981], Hopfield [1982] was the idea that memories were not stored by somehow keeping copies of patterns of neural activity.", "startOffset": 8, "endOffset": 23}, {"referenceID": 0, "context": "[1969], Kohonen [1972], Anderson and Hinton [1981], Hopfield [1982] was the idea that memories were not stored by somehow keeping copies of patterns of neural activity.", "startOffset": 24, "endOffset": 51}, {"referenceID": 0, "context": "[1969], Kohonen [1972], Anderson and Hinton [1981], Hopfield [1982] was the idea that memories were not stored by somehow keeping copies of patterns of neural activity.", "startOffset": 24, "endOffset": 68}, {"referenceID": 0, "context": "[1969], Kohonen [1972], Anderson and Hinton [1981], Hopfield [1982] was the idea that memories were not stored by somehow keeping copies of patterns of neural activity. Instead, these patterns were reconstructed when needed from information stored in the weights of an associative network and the very same weights could store many different memories An auto-associative memory that has N weights cannot be expected to store more that N real-valued vectors with N components each. How close we can come to this upper bound depends on which storage rule we use. Hopfield nets use a simple, one-shot, outer-product storage rule and achieve a capacity of approximately 0.15N binary vectors using weights that require log(N) bits each. Much more efficient use can be made of the weights by using an iterative, error correction storage rule to learn weights that can retrieve each bit of a pattern from all the other bits Gardner [1988], but for our purposes maximizing the capacity is less important than having a simple, non-iterative storage rule, so we will use an outer product rule to store hidden activity vectors in fast weights that decay rapidly.", "startOffset": 24, "endOffset": 932}, {"referenceID": 0, "context": "[1969], Kohonen [1972], Anderson and Hinton [1981], Hopfield [1982] was the idea that memories were not stored by somehow keeping copies of patterns of neural activity. Instead, these patterns were reconstructed when needed from information stored in the weights of an associative network and the very same weights could store many different memories An auto-associative memory that has N weights cannot be expected to store more that N real-valued vectors with N components each. How close we can come to this upper bound depends on which storage rule we use. Hopfield nets use a simple, one-shot, outer-product storage rule and achieve a capacity of approximately 0.15N binary vectors using weights that require log(N) bits each. Much more efficient use can be made of the weights by using an iterative, error correction storage rule to learn weights that can retrieve each bit of a pattern from all the other bits Gardner [1988], but for our purposes maximizing the capacity is less important than having a simple, non-iterative storage rule, so we will use an outer product rule to store hidden activity vectors in fast weights that decay rapidly. The usual weights in an RNN will be called slow weights and they will learn by stochastic gradient descent in an objective function taking into account the fact that changes in the slow weights will lead to changes in what gets stored automatically in the fast associative memory. A fast associative memory has several advantages when compared with the type of memory assumed by a Neural Turing Machine (NTM) Graves et al. [2014], Neural Stack Grefenstette et al.", "startOffset": 24, "endOffset": 1582}, {"referenceID": 0, "context": "[1969], Kohonen [1972], Anderson and Hinton [1981], Hopfield [1982] was the idea that memories were not stored by somehow keeping copies of patterns of neural activity. Instead, these patterns were reconstructed when needed from information stored in the weights of an associative network and the very same weights could store many different memories An auto-associative memory that has N weights cannot be expected to store more that N real-valued vectors with N components each. How close we can come to this upper bound depends on which storage rule we use. Hopfield nets use a simple, one-shot, outer-product storage rule and achieve a capacity of approximately 0.15N binary vectors using weights that require log(N) bits each. Much more efficient use can be made of the weights by using an iterative, error correction storage rule to learn weights that can retrieve each bit of a pattern from all the other bits Gardner [1988], but for our purposes maximizing the capacity is less important than having a simple, non-iterative storage rule, so we will use an outer product rule to store hidden activity vectors in fast weights that decay rapidly. The usual weights in an RNN will be called slow weights and they will learn by stochastic gradient descent in an objective function taking into account the fact that changes in the slow weights will lead to changes in what gets stored automatically in the fast associative memory. A fast associative memory has several advantages when compared with the type of memory assumed by a Neural Turing Machine (NTM) Graves et al. [2014], Neural Stack Grefenstette et al. [2015], or Memory Network Weston et al.", "startOffset": 24, "endOffset": 1623}, {"referenceID": 0, "context": "[1969], Kohonen [1972], Anderson and Hinton [1981], Hopfield [1982] was the idea that memories were not stored by somehow keeping copies of patterns of neural activity. Instead, these patterns were reconstructed when needed from information stored in the weights of an associative network and the very same weights could store many different memories An auto-associative memory that has N weights cannot be expected to store more that N real-valued vectors with N components each. How close we can come to this upper bound depends on which storage rule we use. Hopfield nets use a simple, one-shot, outer-product storage rule and achieve a capacity of approximately 0.15N binary vectors using weights that require log(N) bits each. Much more efficient use can be made of the weights by using an iterative, error correction storage rule to learn weights that can retrieve each bit of a pattern from all the other bits Gardner [1988], but for our purposes maximizing the capacity is less important than having a simple, non-iterative storage rule, so we will use an outer product rule to store hidden activity vectors in fast weights that decay rapidly. The usual weights in an RNN will be called slow weights and they will learn by stochastic gradient descent in an objective function taking into account the fact that changes in the slow weights will lead to changes in what gets stored automatically in the fast associative memory. A fast associative memory has several advantages when compared with the type of memory assumed by a Neural Turing Machine (NTM) Graves et al. [2014], Neural Stack Grefenstette et al. [2015], or Memory Network Weston et al. [2014]. First, it is not at all clear how a real brain would implement the more exotic structures in these models e.", "startOffset": 24, "endOffset": 1663}, {"referenceID": 2, "context": "So fast weights act like a kind of attention to the recent past but with the strength of the attention being determined by the scalar product between the current hidden vector and the earlier hidden vector rather than being determined by a separate parameterized computation of the type used in neural machine translation models Bahdanau et al. [2015]. The update rule for the fast memory weight matrix, A, is simply to multiply the current fast weights by a decay rate, \u03bb, and add the outer product of the hidden state vector, h(t), multiplied by a learning rate, \u03b7: A(t+ 1) = \u03bbA(t) + \u03b7h(t)h(t) (1)", "startOffset": 329, "endOffset": 352}, {"referenceID": 1, "context": "Recently, layer normalization Ba et al. [2016] has been shown to be very effective at stablizing the hidden state dynamics in RNNs and reducing training time.", "startOffset": 30, "endOffset": 47}, {"referenceID": 13, "context": "All the models were trained using mini-batches of size 128 and the Adam optimizer Kingma and Ba [2014]. A description of the training protocols and the hyper-parameter settings we used can be found in the Appendix.", "startOffset": 82, "endOffset": 103}, {"referenceID": 5, "context": "Danihelka et al. [2016] extended LSTMs by adding complex associative memory.", "startOffset": 0, "endOffset": 24}, {"referenceID": 15, "context": "Recently, visual attention models Mnih et al. [2014], Ba et al.", "startOffset": 34, "endOffset": 53}, {"referenceID": 1, "context": "[2014], Ba et al. [2015], Xu et al.", "startOffset": 8, "endOffset": 25}, {"referenceID": 1, "context": "[2014], Ba et al. [2015], Xu et al. [2015] have been shown to overcome some of the limitations in ConvNets.", "startOffset": 8, "endOffset": 43}, {"referenceID": 6, "context": "While different kinds of memory and attention have been studied extensively in the supervised learning setting Graves [2014], Mnih et al.", "startOffset": 111, "endOffset": 125}, {"referenceID": 6, "context": "While different kinds of memory and attention have been studied extensively in the supervised learning setting Graves [2014], Mnih et al. [2014], Bahdanau et al.", "startOffset": 111, "endOffset": 145}, {"referenceID": 2, "context": "[2014], Bahdanau et al. [2015], the use of such models for learning long range dependencies in reinforcement learning has received less attention.", "startOffset": 8, "endOffset": 31}, {"referenceID": 2, "context": "[2014], Bahdanau et al. [2015], the use of such models for learning long range dependencies in reinforcement learning has received less attention. We compare different memory architectures on a partially observable variant of the game \u201dCatch\u201d described in Mnih et al. [2014]. The game is played on an N \u00d7N screen of binary pixels and each episode consists of N frames.", "startOffset": 8, "endOffset": 275}, {"referenceID": 2, "context": "[2014], Bahdanau et al. [2015], the use of such models for learning long range dependencies in reinforcement learning has received less attention. We compare different memory architectures on a partially observable variant of the game \u201dCatch\u201d described in Mnih et al. [2014]. The game is played on an N \u00d7N screen of binary pixels and each episode consists of N frames. Each trial begins with a single pixel, representing a ball, appearing somewhere in the first row of the column and a two pixel \u201dpaddle\u201d controlled by the agent in the bottom row. After observing a frame, the agent gets to either keep the paddle stationary or move it right or left by one pixel. The ball descends by a single pixel after each frame. The episode ends when the ball pixel reaches the bottom row and the agent receives a reward of +1 if the paddle touches the ball and a reward of\u22121 if it doesn\u2019t. Solving the fully observable task is straightforward and requires the agent to move the paddle to the column with the ball. We make the task partiallyobservable by providing the agent blank observations after the M th frame. Solving the partiallyobservable version of the game requires remembering the position of the paddle and ball after M frames and moving the paddle to the correct position using the stored information. We used the recently proposed asynchronous advantage actor-critic method Mnih et al. [2016] to train agents with three types of memory on different sizes of the partially observable Catch task.", "startOffset": 8, "endOffset": 1397}], "year": 2016, "abstractText": "Until recently, research on artificial neural networks was largely restricted to systems with only two types of variable: Neural activities that represent the current or recent input and weights that learn to capture regularities among inputs, outputs and payoffs. There is no good reason for this restriction. Synapses have dynamics at many different time-scales and this suggests that artificial neural networks might benefit from variables that change slower than activities but much faster than the standard weights. These \u201cfast weights\u201d can be used to store temporary memories of the recent past and they provide a neurally plausible way of implementing the type of attention to the past that has recently proved very helpful in sequence-to-sequence models. By using fast weights we can avoid the need to store copies of neural activity patterns.", "creator": "LaTeX with hyperref package"}}}