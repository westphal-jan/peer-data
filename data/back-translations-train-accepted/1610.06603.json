{"id": "1610.06603", "review": {"conference": "NIPS", "VERSION": "v1", "DATE_OF_SUBMISSION": "20-Oct-2016", "title": "Combinatorial Multi-Armed Bandit with General Reward Functions", "abstract": "In this paper, we study the stochastic combinatorial multi-armed bandit (CMAB) framework that allows a general nonlinear reward function, whose expected value may not depend only on the means of the input random variables but possibly on the entire distributions of these variables. Our framework enables a much larger class of reward functions such as the $\\max()$ function and nonlinear utility functions. Existing techniques relying on accurate estimations of the means of random variables, such as the upper confidence bound (UCB) technique, do not work directly on these functions. We propose a new algorithm called stochastically dominant confidence bound (SDCB), which estimates the distributions of underlying random variables and their stochastically dominant confidence bounds. We prove that SDCB can achieve $O(\\log T)$ distribution-dependent regret and $\\tilde{O}(\\sqrt{T})$ distribution-independent regret, where $T$ is the time horizon. We apply our results to the $K$-MAX problem and expected utility maximization problems. In particular, for $K$-MAX, we provide the first polynomial-time approximation scheme (PTAS) for its offline problem, and give the first $\\tilde{O}(\\sqrt T)$ bound on the $(1-\\epsilon)$-approximation regret of its online problem, for any $\\epsilon&gt;0$.", "histories": [["v1", "Thu, 20 Oct 2016 20:54:41 GMT  (402kb)", "https://arxiv.org/abs/1610.06603v1", "to appear in NIPS 2016"], ["v2", "Tue, 31 Jan 2017 18:18:05 GMT  (490kb)", "http://arxiv.org/abs/1610.06603v2", "published in Neural Information Processing Systems (NIPS) 2016"], ["v3", "Wed, 1 Feb 2017 04:49:22 GMT  (490kb)", "http://arxiv.org/abs/1610.06603v3", "published in Neural Information Processing Systems (NIPS) 2016"]], "COMMENTS": "to appear in NIPS 2016", "reviews": [], "SUBJECTS": "cs.LG cs.DS stat.ML", "authors": ["wei chen", "wei hu", "fu li", "jian li", "yu liu", "pinyan lu"], "accepted": true, "id": "1610.06603"}, "pdf": {"name": "1610.06603.pdf", "metadata": {"source": "CRF", "title": "Combinatorial Multi-Armed Bandit with General Reward Functions", "authors": ["Wei Chen", "Wei Hu", "Fu Li", "Jian Li", "Yu Liu", "Pinyan Lu"], "emails": ["weic@microsoft.com.", "huwei@cs.princeton.edu.", "fuli.theory.research@gmail.com.", "lapordge@gmail.com."], "sections": [{"heading": null, "text": "ar Xiv: 161 0.06 603v 3 [cs.L G] 1F eb2 01 \u221a T) distributional regret, where T is the time horizon. We apply our results to the K-MAX problem and expected problems in maximizing benefits. In particular, we provide K-MAX with the first polynomial time approximation scheme (PTAS) for its offline problem, and specify the first O-number (\u221a T) linked to the (1 \u2212 B) approximation of its online problem, in all cases where there is an E > 0."}, {"heading": "1 Introduction", "text": "In fact, you see yourself able to compete in a country where most of us are able to compete and where most of us are able to change the world, \"he said."}, {"heading": "2 Setup and Notation", "text": "Problem formulation: Without loss of generality, we assume that a reward problem is negative."}, {"heading": "3 SDCB Algorithm", "text": "3 [0, 1] S is isomorph to [0, 1] S [0, 1] S [0, 1] S [0, 1] S [0, 1] S [0, 1] S [0, 1] S [0, 1] S [0, 1] S. \"S\".S \".S\".S \".S\".S \".S\".S \".S\".S \".S\".S \".S\".S \".S\".S. \".S\".S \".S\".S \".S\".S \".S\".S \".S\".S \".S\".S \".S\".S \".S\" S \".S\".S \"S\".S \".S\".S \"S\".S \".S\".S \".S\".S \".S\" S \".S\".S \"S\".S \".S\" S \".S\".S \"S\".S \".S\" S \".S\".S \"S\".S \".S\" S \".S\".S \"S\".S \"S\".S \"S\" S \".S\" S \".S\" S \".S\" S \".S\" S \"S\".S \"S\".S \"S\".S \"S\" S \".S\" S \".S\" S \"S\".S \"S\" S \".S\" S \".S\" S \"S\".S \"S\".S \"S\" S \".S\" S \"S\".S \"S\".S \"S\" S \".S\" S \".S\".S \"S\".S \"S\" S \".S\".S \".S\" S \"S\".S \".S\" S \"S\" S \"S\".S \".S\" S \"S\".S \".S\" S \"S\" S \".S\".S \"S\" S \".S\" S \".S\" S S \".S\".S \"S\".S \"S\" S \"S\".S \"S\".S S \"S\".S \"S S S\".S \"S S\".S \".S\" S"}, {"heading": "4 Improved SDCB Algorithm by Discretization", "text": "In Section 3, we have shown that our SDCB algorithm reaches near-optimal repentance limits. (However, this algorithm may suffer from large runtime and memory usage.) Note: In such a case, it needs a space to store the empirical CDF results of Arm i, and it is possible that all observed values of Arm i are different (e.g., if the results of Arm i are continuous); in such a case, it takes up space to store the empirical CDF results of Arm i, and both compute the stochastically dominant CDF-Fi values and update F-i the time. (t) The worst use of space of SDCB in T rounds is therefore pronounced (T), and the worst case where the runtime has expired (T 2) (we ignore the dependence of m and K); here, we do not count the time and space used by the offline compilation."}, {"heading": "5 Applications", "text": "We describe the K-MAX problem and the class of expected benefit maximization problems that can be a simple problem, as if we could solve the general benefit maximization problem. (...) It is easy to verify that this reward function maximizes assumptions 2, 3 and 4 with M = 1. Now we look at the appropriate benefit maximizations of most K-MAX problems with the largest expected benefit maximization. (...) Maximize the result by implying that the exact optimal solution is NP-hard, so we can resort to maximizing approximation algorithms. (...) We can show that a simple problem of maximizing benefit in most K-MAX classes of independent arms, with the highest expected reward, can be maximized. (...) It can be implied by a result that we find the exact optimal solution of the NP-hard maximixer so that we can rely on approximation algorithms to maximize usage. (...)"}, {"heading": "Acknowledgments", "text": "Wei Chen was partially supported by the National Natural Science Foundation of China (grant no. 61433014), Jian Li and Yu Liu were partially supported by grants from China's National Basic Research Program: 2015CB358700, 2011CBA00300, 2011CBA00301, and National NSFC grants: 61033001, 61361136003. The authors would like to thank Tor Lattimore for bringing DKW inequality to our attention."}, {"heading": "Appendix", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "A Missing Proofs from Section 3", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "A.1 Proof of Theorem 1", "text": "We present the proof for theorem 1 in four steps. In section A.1.1 we check the L1 distance between two distributions and present a property of it. In section A.1.2 we check the Dvoretzky-Kiefer-Wolfowitz inequality (DKW), which is a strong concentration result for empirical CDFs. In section A.1.3 we demonstrate some important technical shortcomings. Then we complete the proof for theorem 1 in section A.1.4."}, {"heading": "A.1.1 The L1 Distance between Two Probability Distributions", "text": "(Let P be a probability distribution.) Let P (x) = PrX \u0445 P [X = x]. We write P = P1 \u00b7 P2 \u00b7 \u00b7 \u00b7 Pn if the (multivariate) random variable X + P can be written as X = (X1, X2,. \u2212 \u2212 Pn), where X1,. \u2212 Q (x) | Xn are mutually independent and Xi \u0445 Pi (\u0441i [n]). If there are two distributions P and Q, their L1 distance becomes L1 (P, Q) = P1 \u2212 P (x) \u2212 Q (x) \u2212 Q (x) \u2212 supp (P)."}, {"heading": "A.1.2 The DKW Inequality", "text": "Consider a distribution D with CDF (x). Let F-n (x) be the empirical CDF of n i.i.d. samples X1,..., Xn from D, i.e. F-n (x) = 1n-n-n i = 1 {Xi \u2264 x} (x-R).7 Then we have: Lemma 2 (Dvoretzky-Kiefer-Wolfowitz inequality [10, 24]. For each x-0 and each n-Z + we have Pr [sup x-R], F-n (x) \u2212 F (x)."}, {"heading": "A.1.3 Technical Lemmas", "text": "The following problem describes some of the properties of the expected reward rP (S) = EX \u00b7 P [R (X, S)].Lemma 3. \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7"}, {"heading": "A.1.4 Finishing the Proof of Theorem 1", "text": "Lemma 4 is very similar to Lemma 1 in [18]. We now apply the counter argument in [18] = k = k = = k = = k = = = = = = = = = = = = [18] to end the proof of the theorem 1.Of Lemma 4 we know that it is still bound to E [26]. \u2212 \u2212 So it is that limk \u2192 limk = limk = 0. We then choose two decreasing sequences of positive constants 1 = \u03b20 > \u03b21 > \u03b22 >. \u2212 1 > 2 >. \u2212 k = limk \u2192 limk = limk = 0. We opt for {k} and {\u03b2k} something like in Theorem 4 of [18], which does not meet the 6-K-K-K-K-K-K-K-K-K-K. \u2212 k-k-k-K-K-K = limk & ltk < 267) For t-K-K, K-K-26, K-K-K-K."}, {"heading": "A.2 Analysis of Our Algorithm in the Previous CMAB Framework", "text": "We consider the case where the expected reward depends only on the means of the random variables."}, {"heading": "B Missing Proofs from Section 4", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "B.1 Analysis of the Discretization Error", "text": "The following 4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-3-3-3-3-3-3-3-3-3-3-3-3-3-3-3-3-3-3-3-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-"}, {"heading": "B.2 Proof of Theorem 2", "text": "The proof for Theorem 2: Imagine algorithm 2 selects Super-Arm St in the t-th round (1 \u2264 t \u2264 T), then its \u03b1 approach to asReg Alg. 2 D, \u03b1 (T) = T \u00b7 \u03b1 \u00b7 rD (S) \u2212 T \u0445 t = 1E [rD (St)] = T \u00b7 \u03b1 (T) = T \u00b7 \u03b1 (S). \u2212 rD (S) + T (T) = 1E [rD (St) \u2212 T (T)."}, {"heading": "B.3 Proof of Theorem 3", "text": "Then we have 2n \u2212 1 < T \u2264 2n. If we have n \u2264 q = log2 m, then T \u2264 2m and the regret in T rounds is at most 2m \u00b7 \u03b1M. The remorse limit is trivial. Now we assume that n \u2264 q + 1. Using Theorem 2 we have RegAlg. 3 D, \u03b1 (T) \u2264 RegAlg. 3D, \u03b1 (2n) = RegAlg. 2D, \u03b1 (2 q) + n - qReg. Alg. 2 D, \u03b1 (2 k) \u2264 RegAlg. 2D, \u03b1 (2 k) - n \u2212 Xi Xi = qReg. 2 D, \u03b1 (2 k) - (2 k)."}, {"heading": "C.2 PTAS", "text": "In other words, we specify an algorithm that, by specifying a fixed constant 0 < \u03b5 < 1 / 2, can find a solution S of cardinality | K | so that rD (S) \u2265 (1 \u2212 \u03b5) \u00b7 OPT in polynomial time. We first give an overview of our approach and then spell out the details later. 1. (Discretization) We first transform each Xi into a different discrete distribution X-i, so that all X-i are supported on a group of size O (1 / \u03b52). 2. (Calculation signatures) For each Xi, we can calculate a signature Sig (Xi) from X-i, which is a vector of size O (1 / \u03b52). For a group S, we define its signature Sig (S) so that both groups S1 and S2 are the same signatures."}, {"heading": "C.2.1 Discretization", "text": "We first describe the discretization step. We say that a random variable X follows the Bernoulli distribution B (v, q), if X follows the value v with the probability q and the value 0 with the probability 1 \u2212 q. For each discrete distribution we can describe it as the maximum of a series of Bernoulli distributions. Definition 1. Let X define a discrete random variable with the probability 1 \u2212 q., vs. (v1 < v2 < vs) and Pr [X = vj] = pj. We define a series of independent Bernoulli random variables with the support {v1, v2,.)."}, {"heading": "C.2.2 Signatures", "text": "For each Xi we have created his discrediting, with each coordinating a multiple of 4 / 4 / 4 / 4 / 4 / 4 / 4 / 5 / 5 / 5 / 5 / 5 / 5 / 5 / 5 / 5 / 5 / 5 / 5 / 5 / 5 / 5 / 5 / 5 / 5 / 5 / 5 / 5 / 5 / 5 / 5 / 5 / 5 / 5 / 5 / 5 / 5 / 5 / 5 / 5 / 5 / 5 / 5 / 5 / 5 / 5 / 5 / 5 / 5 / 5 / 5 / 5 / 5 / 5 / 5 / 5 / 5 / 5 / 5 / 5 / 5 / 5 / 5 / 5 / 5 / 5 / 5 / 5 / 5 / 5 / 5 / 5 / 5 / 5 / 5 / 5 / 5 / 5 / 5 / 5 / 5 / 5 / 5 / 5 / 5 / 5 / 5 / 5 / 5 / 5 / / 5 / 5 / 5 / / 5 / / 5 / 5 / 5 / / 5 / / 5 / 5 / 5 / / 5 / 5 / 5 / 5 / 5 / 5 / 5 / 5 / 5 / 5 / 5 / 5 / 5 / 5 / 5 / 5 / 5 / 5 / 5 / 5 / 5 / 5 / 5 / 5 / 5 / 5 / 5 / 5 / 5 / 5 / 5 / 5 / 5 / 5 / 5 / 5 / 5 / 5 / 5 / 5 / 5 / 5 / 5 / 5 / 5 / 5 / 5 / 5 / 5 / 5 / 5 / 5 / 5 / 5 / 5 / 5 / 5 / 5 / 5 / 5 / 5 / 5 / 5 / 5 / 5 / 5 / 5 / 5 / 5 / 5 / 5 / 5 / 5 / 5 / 5 / 5 / 5 / 5 / 5 / 5 / 5 / 5 / 5 / 5 / 5 / 5 / 5 / 5 / 5 / 5 / 5 / 5 / 5 / 5 / 5 / 5 / 5 / 5 / 5 / 5 / 5 / 5 / 5 / 5 / 5 / 5 / 5 / 5 / 5 / 5 / 5 / 5 / 5 / 5 / 5 / 5 / 5 / 5 / 5 / 5 / 5 / 5 / 5 / 5 / 5 / 5 / 5 / 5 / 5 / 5 / 5 / 5 / 5 / 5 / 5 / 5 / 5 / 5 / 5 / 5 / 5 / 5 /"}, {"heading": "C.2.3 Enumerating Signatures", "text": "The answer is yes (i.e.), we say that sg is a feasible signature vector and S is a candidate. Finally, we choose the candidate with maximum rD (AK).The answer is yes (i.e.) that we can find such a signature. (i.e.) The answer is yes (i.e.), we say that sg is a feasible signature vector and S is a candidate. (i.e.) The answer is yes (i.e.), we say that sg is a feasible signature vector and S is a candidate. (i.e.) The answer is yes (i.e.), we say that sg is a feasible signature vector and S is a candidate."}, {"heading": "D Empirical Comparison between the SDCB Algorithm and Online", "text": "Submodular maximization on the K-MAX round, then the c-MAX problem (SDCB reward) = = We conduct experiments to compare the SDCB algorithm with the online submodular maximization algorithm = = 4 = 4 = 4 = 4 = 4 = 4 = [26], on the K-MAX problem.Online submodular maximization. First, we briefly describe the online submodular maximization problem found in [26] and the algorithm contained therein {2} {2} {r {2} [0}, when the player selects a feasible super arm Xi Xi Xi Xi Xi, the reward is ft (St). This model covers the K-MAX problem as an instance: Assumption X (t) = (X (t) 1,.,., X (t) m), is the result vector amplified in the t-th round."}], "references": [{"title": "Minimax policies for adversarial and stochastic bandits", "author": ["Jean-Yves Audibert", "S\u00e9bastien Bubeck"], "venue": "In COLT,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2009}, {"title": "Finite-time analysis of the multiarmed bandit problem", "author": ["Peter Auer", "Nicolo Cesa-Bianchi", "Paul Fischer"], "venue": "Machine learning,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2002}, {"title": "The nonstochastic multiarmed bandit problem", "author": ["Peter Auer", "Nicolo Cesa-Bianchi", "Yoav Freund", "Robert E. Schapire"], "venue": "SIAM Journal on Computing,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2002}, {"title": "A utility equivalence theorem for concave functions", "author": ["Anand Bhalgat", "Sanjeev Khanna"], "venue": "In IPCO,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2014}, {"title": "Regret analysis of stochastic and nonstochastic multi-armed bandit problems", "author": ["S\u00e9bastien Bubeck", "Nicol\u00f2 Cesa-Bianchi"], "venue": "Foundations and Trends in Machine Learning,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2012}, {"title": "Combinatorial bandits", "author": ["Nicolo Cesa-Bianchi", "G\u00e1bor Lugosi"], "venue": "Journal of Computer and System Sciences,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2012}, {"title": "Combinatorial pure exploration of multi-armed bandits", "author": ["Shouyuan Chen", "Tian Lin", "Irwin King", "Michael R. Lyu", "Wei Chen"], "venue": "In NIPS,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2014}, {"title": "Combinatorial multi-armed bandit and its extension to probabilistically triggered arms", "author": ["Wei Chen", "Yajun Wang", "Yang Yuan", "Qinshi Wang"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2016}, {"title": "Combinatorial bandits revisited", "author": ["Richard Combes", "M. Sadegh Talebi", "Alexandre Proutiere", "Marc Lelarge"], "venue": "In NIPS,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2015}, {"title": "Asymptotic minimax character of the sample distribution function and of the classical multinomial estimator", "author": ["Aryeh Dvoretzky", "Jack Kiefer", "Jacob Wolfowitz"], "venue": "The Annals of Mathematical Statistics,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 1956}, {"title": "The foundations of expected utility", "author": ["P.C. Fishburn"], "venue": "Dordrecht: Reidel,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 1982}, {"title": "Combinatorial network optimization with unknown variables: Multi-armed bandits with linear rewards and individual observations", "author": ["Yi Gai", "Bhaskar Krishnamachari", "Rahul Jain"], "venue": "IEEE/ACM Transactions on Networking,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2012}, {"title": "Asking the right questions: Model-driven optimization using probes", "author": ["Ashish Goel", "Sudipto Guha", "Kamesh Munagala"], "venue": "In PODS,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2006}, {"title": "How to probe for an extreme value", "author": ["Ashish Goel", "Sudipto Guha", "Kamesh Munagala"], "venue": "ACM Transactions on Algorithms (TALG),", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2010}, {"title": "Thompson sampling for complex online problems", "author": ["Aditya Gopalan", "Shie Mannor", "Yishay mansour"], "venue": "In ICML,", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2014}, {"title": "Matroid bandits: Fast combinatorial optimization with learning", "author": ["Branislav Kveton", "Zheng Wen", "Azin Ashkan", "Hoda Eydgahi", "Brian Eriksson"], "venue": "In UAI,", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2014}, {"title": "Combinatorial cascading bandits", "author": ["Branislav Kveton", "Zheng Wen", "Azin Ashkan", "Csaba Szepesv\u00e1ri"], "venue": "In NIPS,", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2015}, {"title": "Tight regret bounds for stochastic combinatorial semi-bandits", "author": ["Branislav Kveton", "Zheng Wen", "Azin Ashkan", "Csaba Szepesv\u00e1ri"], "venue": "In AISTATS,", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2015}, {"title": "Asymptotically efficient adaptive allocation rules", "author": ["Tze Leung Lai", "Herbert Robbins"], "venue": "Advances in applied mathematics,", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 1985}, {"title": "Maximizing expected utility for stochastic combinatorial optimization problems", "author": ["Jian Li", "Amol Deshpande"], "venue": "In FOCS,", "citeRegEx": "20", "shortCiteRegEx": "20", "year": 2011}, {"title": "Stochastic combinatorial optimization via poisson approximation", "author": ["Jian Li", "Wen Yuan"], "venue": "In STOC,", "citeRegEx": "21", "shortCiteRegEx": "21", "year": 2013}, {"title": "Combinatorial partial monitoring game with linear feedback and its applications", "author": ["Tian Lin", "Bruno Abrahao", "Robert Kleinberg", "John Lui", "Wei Chen"], "venue": "In ICML,", "citeRegEx": "22", "shortCiteRegEx": "22", "year": 2014}, {"title": "Stochastic online greedy learning with semi-bandit feedbacks", "author": ["Tian Lin", "Jian Li", "Wei Chen"], "venue": "In NIPS,", "citeRegEx": "23", "shortCiteRegEx": "23", "year": 2015}, {"title": "The tight constant in the dvoretzky-kiefer-wolfowitz inequality", "author": ["Pascal Massart"], "venue": "The Annals of Probability,", "citeRegEx": "24", "shortCiteRegEx": "24", "year": 1990}, {"title": "An analysis of approximations for maximizing submodular set functions \u2013 I", "author": ["George L. Nemhauser", "Laurence A. Wolsey", "Marshall L. Fisher"], "venue": "Mathematical Programming,", "citeRegEx": "25", "shortCiteRegEx": "25", "year": 1978}, {"title": "An online algorithm for maximizing submodular functions", "author": ["Matthew Streeter", "Daniel Golovin"], "venue": "In NIPS,", "citeRegEx": "26", "shortCiteRegEx": "26", "year": 2008}], "referenceMentions": [{"referenceID": 18, "context": "MAB and its variants have been extensively studied in the literature, with classical results such as tight \u0398(logT ) distribution-dependent and \u0398( \u221a T ) distribution-independent upper and lower bounds on the regret in T rounds [19, 2, 1].", "startOffset": 226, "endOffset": 236}, {"referenceID": 1, "context": "MAB and its variants have been extensively studied in the literature, with classical results such as tight \u0398(logT ) distribution-dependent and \u0398( \u221a T ) distribution-independent upper and lower bounds on the regret in T rounds [19, 2, 1].", "startOffset": 226, "endOffset": 236}, {"referenceID": 0, "context": "MAB and its variants have been extensively studied in the literature, with classical results such as tight \u0398(logT ) distribution-dependent and \u0398( \u221a T ) distribution-independent upper and lower bounds on the regret in T rounds [19, 2, 1].", "startOffset": 226, "endOffset": 236}, {"referenceID": 11, "context": "Therefore CMAB has attracted a lot of attention in online learning research in recent years [12, 8, 22, 15, 7, 16, 18, 17, 23, 9].", "startOffset": 92, "endOffset": 129}, {"referenceID": 7, "context": "Therefore CMAB has attracted a lot of attention in online learning research in recent years [12, 8, 22, 15, 7, 16, 18, 17, 23, 9].", "startOffset": 92, "endOffset": 129}, {"referenceID": 21, "context": "Therefore CMAB has attracted a lot of attention in online learning research in recent years [12, 8, 22, 15, 7, 16, 18, 17, 23, 9].", "startOffset": 92, "endOffset": 129}, {"referenceID": 14, "context": "Therefore CMAB has attracted a lot of attention in online learning research in recent years [12, 8, 22, 15, 7, 16, 18, 17, 23, 9].", "startOffset": 92, "endOffset": 129}, {"referenceID": 6, "context": "Therefore CMAB has attracted a lot of attention in online learning research in recent years [12, 8, 22, 15, 7, 16, 18, 17, 23, 9].", "startOffset": 92, "endOffset": 129}, {"referenceID": 15, "context": "Therefore CMAB has attracted a lot of attention in online learning research in recent years [12, 8, 22, 15, 7, 16, 18, 17, 23, 9].", "startOffset": 92, "endOffset": 129}, {"referenceID": 17, "context": "Therefore CMAB has attracted a lot of attention in online learning research in recent years [12, 8, 22, 15, 7, 16, 18, 17, 23, 9].", "startOffset": 92, "endOffset": 129}, {"referenceID": 16, "context": "Therefore CMAB has attracted a lot of attention in online learning research in recent years [12, 8, 22, 15, 7, 16, 18, 17, 23, 9].", "startOffset": 92, "endOffset": 129}, {"referenceID": 22, "context": "Therefore CMAB has attracted a lot of attention in online learning research in recent years [12, 8, 22, 15, 7, 16, 18, 17, 23, 9].", "startOffset": 92, "endOffset": 129}, {"referenceID": 8, "context": "Therefore CMAB has attracted a lot of attention in online learning research in recent years [12, 8, 22, 15, 7, 16, 18, 17, 23, 9].", "startOffset": 92, "endOffset": 129}, {"referenceID": 7, "context": "Even for studies that do generalize to non-linear reward functions, they typically still assume that the expected reward for choosing a super arm is a function of the expected outcomes from the constituent base arms in this super arm [8, 17].", "startOffset": 234, "endOffset": 241}, {"referenceID": 16, "context": "Even for studies that do generalize to non-linear reward functions, they typically still assume that the expected reward for choosing a super arm is a function of the expected outcomes from the constituent base arms in this super arm [8, 17].", "startOffset": 234, "endOffset": 241}, {"referenceID": 19, "context": "Beyond the K-MAX problem, many expected utility maximization (EUM) problems are studied in stochastic optimization literature [27, 20, 21, 4].", "startOffset": 126, "endOffset": 141}, {"referenceID": 20, "context": "Beyond the K-MAX problem, many expected utility maximization (EUM) problems are studied in stochastic optimization literature [27, 20, 21, 4].", "startOffset": 126, "endOffset": 141}, {"referenceID": 3, "context": "Beyond the K-MAX problem, many expected utility maximization (EUM) problems are studied in stochastic optimization literature [27, 20, 21, 4].", "startOffset": 126, "endOffset": 141}, {"referenceID": 11, "context": "As already mentioned, most relevant to our work are studies on CMAB frameworks, among which [12, 16, 18, 9] focus on linear reward functions while [8, 17] look into nonlinear reward functions.", "startOffset": 92, "endOffset": 107}, {"referenceID": 15, "context": "As already mentioned, most relevant to our work are studies on CMAB frameworks, among which [12, 16, 18, 9] focus on linear reward functions while [8, 17] look into nonlinear reward functions.", "startOffset": 92, "endOffset": 107}, {"referenceID": 17, "context": "As already mentioned, most relevant to our work are studies on CMAB frameworks, among which [12, 16, 18, 9] focus on linear reward functions while [8, 17] look into nonlinear reward functions.", "startOffset": 92, "endOffset": 107}, {"referenceID": 8, "context": "As already mentioned, most relevant to our work are studies on CMAB frameworks, among which [12, 16, 18, 9] focus on linear reward functions while [8, 17] look into nonlinear reward functions.", "startOffset": 92, "endOffset": 107}, {"referenceID": 7, "context": "As already mentioned, most relevant to our work are studies on CMAB frameworks, among which [12, 16, 18, 9] focus on linear reward functions while [8, 17] look into nonlinear reward functions.", "startOffset": 147, "endOffset": 154}, {"referenceID": 16, "context": "As already mentioned, most relevant to our work are studies on CMAB frameworks, among which [12, 16, 18, 9] focus on linear reward functions while [8, 17] look into nonlinear reward functions.", "startOffset": 147, "endOffset": 154}, {"referenceID": 7, "context": "[8] look at general non-linear reward functions and Kveton et al.", "startOffset": 0, "endOffset": 3}, {"referenceID": 16, "context": "[17] consider specific non-linear reward functions in a conjunctive or disjunctive form, but both papers require that the expected reward of playing a super arm is determined by the expected outcomes from base arms.", "startOffset": 0, "endOffset": 4}, {"referenceID": 14, "context": "The only work in combinatorial bandits we are aware of that does not require the above assumption on the expected reward is [15], which is based on a general Thompson sampling framework.", "startOffset": 124, "endOffset": 128}, {"referenceID": 4, "context": "There are extensive studies on the classical MAB problem, for which we refer to a survey by Bubeck and Cesa-Bianchi [5].", "startOffset": 116, "endOffset": 119}, {"referenceID": 25, "context": "[26, 6].", "startOffset": 0, "endOffset": 7}, {"referenceID": 5, "context": "[26, 6].", "startOffset": 0, "endOffset": 7}, {"referenceID": 19, "context": "[27, 20, 21, 4]).", "startOffset": 0, "endOffset": 15}, {"referenceID": 20, "context": "[27, 20, 21, 4]).", "startOffset": 0, "endOffset": 15}, {"referenceID": 3, "context": "[27, 20, 21, 4]).", "startOffset": 0, "endOffset": 15}, {"referenceID": 12, "context": "The K-MAX problem may be traced back to [13], where Goel et al.", "startOffset": 40, "endOffset": 44}, {"referenceID": 0, "context": ",m} is a set of m (base) arms, F \u2286 2 is a set of subsets of E, D is a probability distribution over [0, 1], and R is a reward function defined on [0, 1] \u00d7 F .", "startOffset": 100, "endOffset": 106}, {"referenceID": 0, "context": ",m} is a set of m (base) arms, F \u2286 2 is a set of subsets of E, D is a probability distribution over [0, 1], and R is a reward function defined on [0, 1] \u00d7 F .", "startOffset": 146, "endOffset": 152}, {"referenceID": 0, "context": "Therefore, we can alternatively express R(x, S) as RS(xS), where RS is a function defined on [0, 1] .", "startOffset": 93, "endOffset": 99}, {"referenceID": 19, "context": "We remark that the above independence assumption is also made for past studies on the offline EUM and K-MAX problems [27, 20, 21, 4, 13], so it is not an extra assumption for the online learning case.", "startOffset": 117, "endOffset": 136}, {"referenceID": 20, "context": "We remark that the above independence assumption is also made for past studies on the offline EUM and K-MAX problems [27, 20, 21, 4, 13], so it is not an extra assumption for the online learning case.", "startOffset": 117, "endOffset": 136}, {"referenceID": 3, "context": "We remark that the above independence assumption is also made for past studies on the offline EUM and K-MAX problems [27, 20, 21, 4, 13], so it is not an extra assumption for the online learning case.", "startOffset": 117, "endOffset": 136}, {"referenceID": 12, "context": "We remark that the above independence assumption is also made for past studies on the offline EUM and K-MAX problems [27, 20, 21, 4, 13], so it is not an extra assumption for the online learning case.", "startOffset": 117, "endOffset": 136}, {"referenceID": 0, "context": "There exists M > 0 such that for any x \u2208 [0, 1] and any S \u2208 F , we have 0 \u2264 R(x, S) \u2264 M .", "startOffset": 41, "endOffset": 47}, {"referenceID": 0, "context": "If two vectors x, x \u2208 [0, 1] satisfy xi \u2264 xi (\u2200i \u2208 [m]), then for any S \u2208 F , we have R(x, S) \u2264 R(x, S).", "startOffset": 22, "endOffset": 28}, {"referenceID": 0, "context": "3 SDCB Algorithm [0, 1] is isomorphic to [0, 1]; the coordinates in [0, 1] are indexed by elements in S.", "startOffset": 17, "endOffset": 23}, {"referenceID": 0, "context": "3 SDCB Algorithm [0, 1] is isomorphic to [0, 1]; the coordinates in [0, 1] are indexed by elements in S.", "startOffset": 41, "endOffset": 47}, {"referenceID": 0, "context": "3 SDCB Algorithm [0, 1] is isomorphic to [0, 1]; the coordinates in [0, 1] are indexed by elements in S.", "startOffset": 68, "endOffset": 74}, {"referenceID": 0, "context": "We also maintain the empirical distribution D\u0302i of the observed outcomes from arm i so far, which can be represented by its CDF F\u0302i: for x \u2208 [0, 1], the value of F\u0302i(x) is just the fraction of the observed outcomes from arm i that are no larger than x.", "startOffset": 141, "endOffset": 147}, {"referenceID": 0, "context": "Our algorithm ensures that with high probability we have Fi(x) \u2264 Fi(x) simultaneously for all i \u2208 [m] and all x \u2208 [0, 1], where Fi is the CDF of the outcome distribution Di.", "startOffset": 114, "endOffset": 120}, {"referenceID": 0, "context": "We remark that while Fi(x) is a numerical lower confidence bound on Fi(x) for all x \u2208 [0, 1], at the distribution level, Di serves as a \u201cstochastically dominant (upper) confidence bound\u201d on Di.", "startOffset": 86, "endOffset": 92}, {"referenceID": 17, "context": "The main idea is to reduce our analysis on general reward functions satisfying Assumptions 1-3 to the one in [18] that deals with the summation reward functionR(x, S) = \u2211 i\u2208S xi.", "startOffset": 109, "endOffset": 113}, {"referenceID": 9, "context": "Our analysis relies on the Dvoretzky-Kiefer-Wolfowitz inequality [10, 24], which gives a uniform concentration bound on the empirical CDF of a distribution.", "startOffset": 65, "endOffset": 73}, {"referenceID": 23, "context": "Our analysis relies on the Dvoretzky-Kiefer-Wolfowitz inequality [10, 24], which gives a uniform concentration bound on the empirical CDF of a distribution.", "startOffset": 65, "endOffset": 73}, {"referenceID": 7, "context": "Although our focus is on general reward functions, we note that when SDCB is applied to the previous CMAB framework where the expected reward depends only on the means of the random variables, it can achieve the same regret bounds as the previous combinatorial upper confidence bound (CUCB) algorithm in [8, 18].", "startOffset": 304, "endOffset": 311}, {"referenceID": 17, "context": "Although our focus is on general reward functions, we note that when SDCB is applied to the previous CMAB framework where the expected reward depends only on the means of the random variables, it can achieve the same regret bounds as the previous combinatorial upper confidence bound (CUCB) algorithm in [8, 18].", "startOffset": 304, "endOffset": 311}, {"referenceID": 7, "context": ") Hence, the analysis in [8, 18] can be applied to SDCB, resulting in the same regret bounds.", "startOffset": 25, "endOffset": 32}, {"referenceID": 17, "context": ") Hence, the analysis in [8, 18] can be applied to SDCB, resulting in the same regret bounds.", "startOffset": 25, "endOffset": 32}, {"referenceID": 17, "context": "We further remark that in this case we do not need the three assumptions stated in Section 2 (in particular the independence assumption on Xi\u2019s): the summation reward case just works as in [18] and the nonlinear reward case relies on the properties of monotonicity and bounded smoothness used in [8].", "startOffset": 189, "endOffset": 193}, {"referenceID": 7, "context": "We further remark that in this case we do not need the three assumptions stated in Section 2 (in particular the independence assumption on Xi\u2019s): the summation reward case just works as in [18] and the nonlinear reward case relies on the properties of monotonicity and bounded smoothness used in [8].", "startOffset": 296, "endOffset": 299}, {"referenceID": 0, "context": "There exists C > 0 such that for any S \u2208 F and any x, x \u2208 [0, 1], we have |R(x, S)\u2212R(x\u2032, S)| \u2264 C\u2016xS \u2212 xS\u20161, where \u2016xS \u2212 xS\u20161 = \u2211 i\u2208S |xi \u2212 xi|.", "startOffset": 58, "endOffset": 64}, {"referenceID": 0, "context": "Specifically, we partition [0, 1] into s intervals: I1 = [0, 1s ], I2 = ( 1 s , 2 s ], .", "startOffset": 27, "endOffset": 33}, {"referenceID": 13, "context": "It can be implied by a result in [14] that finding the exact optimal solution is NP-hard, so we resort to approximation algorithms.", "startOffset": 33, "endOffset": 37}, {"referenceID": 25, "context": "Streeter and Golovin [26] study an online submodular maximization problem in the oblivious adversary model.", "startOffset": 21, "endOffset": 25}, {"referenceID": 25, "context": "While the techniques in [26] can only give a bound on the (1 \u2212 1/e)-approximation regret for K-MAX, we can obtain the first \u00d5( \u221a T ) bound on the (1\u2212 \u01eb)-approximation regret for any constant \u01eb > 0, using our PTAS as the offline oracle.", "startOffset": 24, "endOffset": 28}, {"referenceID": 25, "context": "Even when we use the simple greedy algorithm as the oracle, our experiments show that SDCB performs significantly better than the algorithm in [26] (see Appendix D).", "startOffset": 143, "endOffset": 147}, {"referenceID": 10, "context": ", [11]), while linear utility functions correspond to risk-neutrality.", "startOffset": 2, "endOffset": 6}, {"referenceID": 19, "context": "Li and Deshpande [20] obtain a PTAS for the expected utility maximization (EUM) problem for several classes of utility functions (including for example increasing concave functions which typically indicate risk-averseness), and a large class of feasibility constraints (including cardinality constraint, s-t simple paths, matchings, and knapsacks).", "startOffset": 17, "endOffset": 21}, {"referenceID": 20, "context": "Similar results for other utility functions and feasibility constraints can be found in [27, 21, 4].", "startOffset": 88, "endOffset": 99}, {"referenceID": 3, "context": "Similar results for other utility functions and feasibility constraints can be found in [27, 21, 4].", "startOffset": 88, "endOffset": 99}, {"referenceID": 0, "context": "References [1] Jean-Yves Audibert and S\u00e9bastien Bubeck.", "startOffset": 11, "endOffset": 14}, {"referenceID": 1, "context": "[2] Peter Auer, Nicolo Cesa-Bianchi, and Paul Fischer.", "startOffset": 0, "endOffset": 3}, {"referenceID": 2, "context": "[3] Peter Auer, Nicolo Cesa-Bianchi, Yoav Freund, and Robert E.", "startOffset": 0, "endOffset": 3}, {"referenceID": 3, "context": "[4] Anand Bhalgat and Sanjeev Khanna.", "startOffset": 0, "endOffset": 3}, {"referenceID": 4, "context": "[5] S\u00e9bastien Bubeck and Nicol\u00f2 Cesa-Bianchi.", "startOffset": 0, "endOffset": 3}, {"referenceID": 5, "context": "[6] Nicolo Cesa-Bianchi and G\u00e1bor Lugosi.", "startOffset": 0, "endOffset": 3}, {"referenceID": 6, "context": "[7] Shouyuan Chen, Tian Lin, Irwin King, Michael R.", "startOffset": 0, "endOffset": 3}, {"referenceID": 7, "context": "[8] Wei Chen, Yajun Wang, Yang Yuan, and Qinshi Wang.", "startOffset": 0, "endOffset": 3}, {"referenceID": 8, "context": "[9] Richard Combes, M.", "startOffset": 0, "endOffset": 3}, {"referenceID": 9, "context": "[10] Aryeh Dvoretzky, Jack Kiefer, and Jacob Wolfowitz.", "startOffset": 0, "endOffset": 4}, {"referenceID": 10, "context": "[11] P.", "startOffset": 0, "endOffset": 4}, {"referenceID": 11, "context": "[12] Yi Gai, Bhaskar Krishnamachari, and Rahul Jain.", "startOffset": 0, "endOffset": 4}, {"referenceID": 12, "context": "[13] Ashish Goel, Sudipto Guha, and Kamesh Munagala.", "startOffset": 0, "endOffset": 4}, {"referenceID": 13, "context": "[14] Ashish Goel, Sudipto Guha, and Kamesh Munagala.", "startOffset": 0, "endOffset": 4}, {"referenceID": 14, "context": "[15] Aditya Gopalan, Shie Mannor, and Yishay mansour.", "startOffset": 0, "endOffset": 4}, {"referenceID": 15, "context": "[16] Branislav Kveton, Zheng Wen, Azin Ashkan, Hoda Eydgahi, and Brian Eriksson.", "startOffset": 0, "endOffset": 4}, {"referenceID": 16, "context": "[17] Branislav Kveton, Zheng Wen, Azin Ashkan, and Csaba Szepesv\u00e1ri.", "startOffset": 0, "endOffset": 4}, {"referenceID": 17, "context": "[18] Branislav Kveton, Zheng Wen, Azin Ashkan, and Csaba Szepesv\u00e1ri.", "startOffset": 0, "endOffset": 4}, {"referenceID": 18, "context": "[19] Tze Leung Lai and Herbert Robbins.", "startOffset": 0, "endOffset": 4}, {"referenceID": 19, "context": "[20] Jian Li and Amol Deshpande.", "startOffset": 0, "endOffset": 4}, {"referenceID": 20, "context": "[21] Jian Li and Wen Yuan.", "startOffset": 0, "endOffset": 4}, {"referenceID": 21, "context": "[22] Tian Lin, Bruno Abrahao, Robert Kleinberg, John Lui, and Wei Chen.", "startOffset": 0, "endOffset": 4}, {"referenceID": 22, "context": "[23] Tian Lin, Jian Li, and Wei Chen.", "startOffset": 0, "endOffset": 4}, {"referenceID": 23, "context": "[24] Pascal Massart.", "startOffset": 0, "endOffset": 4}, {"referenceID": 24, "context": "[25] George L.", "startOffset": 0, "endOffset": 4}, {"referenceID": 25, "context": "[26] Matthew Streeter and Daniel Golovin.", "startOffset": 0, "endOffset": 4}, {"referenceID": 9, "context": "7 Then we have: Lemma 2 (Dvoretzky-Kiefer-Wolfowitz inequality [10, 24]).", "startOffset": 63, "endOffset": 71}, {"referenceID": 23, "context": "7 Then we have: Lemma 2 (Dvoretzky-Kiefer-Wolfowitz inequality [10, 24]).", "startOffset": 63, "endOffset": 71}, {"referenceID": 0, "context": "Let P = P1 \u00d7 \u00b7 \u00b7 \u00b7 \u00d7 Pm and P \u2032 = P \u2032 1 \u00d7 \u00b7 \u00b7 \u00b7 \u00d7 P \u2032 m be two probability distributions over [0, 1].", "startOffset": 94, "endOffset": 100}, {"referenceID": 0, "context": "(i) If for any i \u2208 [m], x \u2208 [0, 1] we have F \u2032 i (x) \u2264 Fi(x), then for any super arm S \u2208 F , we have rP \u2032(S) \u2265 rP (S).", "startOffset": 28, "endOffset": 34}, {"referenceID": 0, "context": "(ii) If for any i \u2208 [m], x \u2208 [0, 1] we have Fi(x) \u2212 F \u2032 i (x) \u2264 \u039bi (\u039bi > 0), then for any super arm S \u2208 F , we have rP \u2032(S)\u2212 rP (S) \u2264 2M \u2211", "startOffset": 29, "endOffset": 35}, {"referenceID": 0, "context": "If we have F \u2032 i (x) \u2264 Fi(x) for all i \u2208 [m] and x \u2208 [0, 1], then for all i, P \u2032 i has first-order stochastic dominance over Pi.", "startOffset": 53, "endOffset": 59}, {"referenceID": 0, "context": "Recall that the reward function R(x, S) has a monotonicity property (Assumption 3): if x and x are two vectors in [0, 1] such that xi \u2264 xi for all i \u2208 [m], then R(x, S) \u2264 R(x, S) for all S \u2208 F .", "startOffset": 114, "endOffset": 120}, {"referenceID": 0, "context": "Let P \u2032\u2032 = P \u2032\u2032 1 \u00d7 \u00b7 \u00b7 \u00b7 \u00d7 P \u2032\u2032 m be a distribution over [0, 1] such that the CDF of P \u2032\u2032 i is the following: F \u2032\u2032 i (x) = { max{Fi(x)\u2212 \u039bi, 0}, 0 \u2264 x < 1, 1, x = 1.", "startOffset": 58, "endOffset": 64}, {"referenceID": 0, "context": "(2) It is easy to see that F \u2032\u2032 i (x) \u2264 F \u2032 i (x) for all i \u2208 [m] and x \u2208 [0, 1].", "startOffset": 74, "endOffset": 80}, {"referenceID": 17, "context": "The following lemma is similar to Lemma 1 in [18].", "startOffset": 45, "endOffset": 49}, {"referenceID": 0, "context": "there exists i \u2208 [m] such that sup x\u2208[0,1] \u2223", "startOffset": 37, "endOffset": 42}, {"referenceID": 0, "context": "sup x\u2208[0,1] \u2223", "startOffset": 6, "endOffset": 11}, {"referenceID": 0, "context": "\u2223 < ci \u2200i \u2208 [m], x \u2208 [0, 1].", "startOffset": 21, "endOffset": 27}, {"referenceID": 0, "context": "(12) From (11) and (12) we know that Fi(x) \u2264 Fi(x) \u2264 Fi(x) + 2ci for all i \u2208 [m], x \u2208 [0, 1].", "startOffset": 86, "endOffset": 92}, {"referenceID": 17, "context": "4 Finishing the Proof of Theorem 1 Lemma 4 is very similar to Lemma 1 in [18].", "startOffset": 73, "endOffset": 77}, {"referenceID": 17, "context": "We now apply the counting argument in [18] to finish the proof of Theorem 1.", "startOffset": 38, "endOffset": 42}, {"referenceID": 17, "context": "We choose {\u03b1k} and {\u03b2k} as in Theorem 4 of [18], which satisfy \u221a 6 \u221e \u2211", "startOffset": 43, "endOffset": 47}, {"referenceID": 7, "context": "Algorithm 4 CUCB [8, 18] 1: For each arm i, maintain: (i) \u03bc\u0302i, the average of all observed outcomes from arm i so far, and (ii) Ti, the number of observed outcomes from arm i so far.", "startOffset": 17, "endOffset": 24}, {"referenceID": 17, "context": "Algorithm 4 CUCB [8, 18] 1: For each arm i, maintain: (i) \u03bc\u0302i, the average of all observed outcomes from arm i so far, and (ii) Ti, the number of observed outcomes from arm i so far.", "startOffset": 17, "endOffset": 24}, {"referenceID": 7, "context": "We summarize the CUCB algorithm [8, 18] in Algorithm 4.", "startOffset": 32, "endOffset": 39}, {"referenceID": 17, "context": "We summarize the CUCB algorithm [8, 18] in Algorithm 4.", "startOffset": 32, "endOffset": 39}, {"referenceID": 0, "context": "Let P andP \u2032 be two distributions over [0, 1]with CDFs F andF , respectively.", "startOffset": 39, "endOffset": 45}, {"referenceID": 0, "context": "(i) If for all x \u2208 [0, 1] we have F (x) \u2264 F (x), then we have E[Y ] \u2264 E[Y ].", "startOffset": 19, "endOffset": 25}, {"referenceID": 0, "context": "(ii) If for all x \u2208 [0, 1] we have F (x)\u2212 F (x) \u2264 \u039b (\u039b > 0), then we have E[Y ] \u2264 E[Y ] + \u039b.", "startOffset": 20, "endOffset": 26}, {"referenceID": 0, "context": "3 ln t 2Ti,t\u22121 \u2264 Fi(x) \u2264 Fi(x) (24) for all i \u2208 [m] and x \u2208 [0, 1], where Fi is the CDF of Di used in round t of SDCB, and Fi is the CDF of Di.", "startOffset": 60, "endOffset": 66}, {"referenceID": 7, "context": "With this property, the analysis in [8, 18] can also be applied to SDCB, resulting in exactly the same regret bounds.", "startOffset": 36, "endOffset": 43}, {"referenceID": 17, "context": "With this property, the analysis in [8, 18] can also be applied to SDCB, resulting in exactly the same regret bounds.", "startOffset": 36, "endOffset": 43}, {"referenceID": 0, "context": "Let g(x) be a Lipschitz continuous function on [0, 1] such that for any x, x \u2208 [0, 1], we have |g(x)\u2212 g(x\u2032)| \u2264 C\u2016x\u2212 x\u20161, where \u2016x\u2212 x\u20161 = \u2211n i=1 |xi \u2212 xi|.", "startOffset": 47, "endOffset": 53}, {"referenceID": 0, "context": "Let g(x) be a Lipschitz continuous function on [0, 1] such that for any x, x \u2208 [0, 1], we have |g(x)\u2212 g(x\u2032)| \u2264 C\u2016x\u2212 x\u20161, where \u2016x\u2212 x\u20161 = \u2211n i=1 |xi \u2212 xi|.", "startOffset": 79, "endOffset": 85}, {"referenceID": 0, "context": "Let P = P1 \u00d7 \u00b7 \u00b7 \u00b7 \u00d7Pn be a probability distribution over [0, 1].", "startOffset": 58, "endOffset": 64}, {"referenceID": 0, "context": "Define another distribution P\u0303 = P\u03031\u00d7\u00b7 \u00b7 \u00b7\u00d7 P\u0303n over [0, 1] as follows: each P\u0303i (i \u2208 [n]) takes values in { 1s , 2s , .", "startOffset": 53, "endOffset": 59}, {"referenceID": 0, "context": "We define two functions on [0, 1]: h(x1, .", "startOffset": 27, "endOffset": 33}, {"referenceID": 0, "context": ", xk\u22121 \u2208 [0, 1], the function g(x1, .", "startOffset": 9, "endOffset": 15}, {"referenceID": 0, "context": ", xk\u22121, x) on x \u2208 [0, 1] is Lipschitz continuous.", "startOffset": 18, "endOffset": 24}, {"referenceID": 0, "context": ", xk\u22121 \u2208 [0, 1].", "startOffset": 9, "endOffset": 15}, {"referenceID": 0, "context": ", xk\u22121) \u2208 [0, 1] is Lipschitz continuous.", "startOffset": 10, "endOffset": 16}, {"referenceID": 0, "context": "According to Assumption 4, the function RS defined on [0, 1] is Lipschitz continuous.", "startOffset": 54, "endOffset": 60}, {"referenceID": 0, "context": ", vi,si} \u2282 [0, 1], and D = D1 \u00d7 \u00b7 \u00b7 \u00b7 \u00d7Dm is the joint distribution of X = (X1, .", "startOffset": 11, "endOffset": 17}, {"referenceID": 0, "context": "For any x \u2208 [0, 1], let fx(S) = maxi\u2208S xi be a set function defined on 2.", "startOffset": 12, "endOffset": 18}, {"referenceID": 24, "context": "According to the classical result on submodular maximization [25], the greedy algorithm can find a (1\u2212 1/e)-approximate solution to maxS\u2286[m],|S|\u2264K{rD(S)}.", "startOffset": 61, "endOffset": 65}, {"referenceID": 25, "context": "Algorithm 8 Online Submodular Maximization [26] 1: Let A1,A2, .", "startOffset": 43, "endOffset": 47}, {"referenceID": 19, "context": "Using the same trick as in [20], we can extend the dynamic program to a more general class of combinatorial constraints where there is a pseudo-polynomial time for the exact version9 of the deterministic version of the corresponding problem.", "startOffset": 27, "endOffset": 31}, {"referenceID": 25, "context": "D Empirical Comparison between the SDCB Algorithm and Online Submodular Maximization on the K-MAX Problem We perform experiments to compare the SDCB algorithm with the online submodular maximization algorithm in [26], on the K-MAX problem.", "startOffset": 212, "endOffset": 216}, {"referenceID": 25, "context": "First we briefly describe the online submodular maximization problem considered in [26] and the algorithm therein.", "startOffset": 83, "endOffset": 87}, {"referenceID": 2, "context": "It uses K copies of the Exp3 algorithm (see [3] for an introduction).", "startOffset": 44, "endOffset": 47}, {"referenceID": 0, "context": "\u2022 Distribution 4: All Di\u2019s are continuous distributions on [0, 1].", "startOffset": 59, "endOffset": 65}, {"referenceID": 0, "context": "For i \u2208 {1, 2, 3}, Di is the uniform distribution on [0, 1].", "startOffset": 53, "endOffset": 59}], "year": 2017, "abstractText": "In this paper, we study the stochastic combinatorial multi-armed bandit (CMAB) framework that allows a general nonlinear reward function, whose expected value may not depend only on the means of the input random variables but possibly on the entire distributions of these variables. Our framework enables a much larger class of reward functions such as the max() function and nonlinear utility functions. Existing techniques relying on accurate estimations of the means of random variables, such as the upper confidence bound (UCB) technique, do not work directly on these functions. We propose a new algorithm called stochastically dominant confidence bound (SDCB), which estimates the distributions of underlying random variables and their stochastically dominant confidence bounds. We prove that SDCB can achieve O(log T ) distribution-dependent regret and \u00d5( \u221a T ) distribution-independent regret, where T is the time horizon. We apply our results to the K-MAX problem and expected utility maximization problems. In particular, for K-MAX, we provide the first polynomial-time approximation scheme (PTAS) for its offline problem, and give the first \u00d5( \u221a T ) bound on the (1 \u2212 \u01eb)approximation regret of its online problem, for any \u01eb > 0.", "creator": "LaTeX with hyperref package"}}}