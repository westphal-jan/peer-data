{"id": "1610.06664", "review": {"conference": "NIPS", "VERSION": "v1", "DATE_OF_SUBMISSION": "21-Oct-2016", "title": "Stochastic Gradient MCMC with Stale Gradients", "abstract": "Stochastic gradient MCMC (SG-MCMC) has played an important role in large-scale Bayesian learning, with well-developed theoretical convergence properties. In such applications of SG-MCMC, it is becoming increasingly popular to employ distributed systems, where stochastic gradients are computed based on some outdated parameters, yielding what are termed stale gradients. While stale gradients could be directly used in SG-MCMC, their impact on convergence properties has not been well studied. In this paper we develop theory to show that while the bias and MSE of an SG-MCMC algorithm depend on the staleness of stochastic gradients, its estimation variance (relative to the expected estimate, based on a prescribed number of samples) is independent of it. In a simple Bayesian distributed system with SG-MCMC, where stale gradients are computed asynchronously by a set of workers, our theory indicates a linear speedup on the decrease of estimation variance w.r.t. the number of workers. Experiments on synthetic data and deep neural networks validate our theory, demonstrating the effectiveness and scalability of SG-MCMC with stale gradients.", "histories": [["v1", "Fri, 21 Oct 2016 04:18:11 GMT  (1093kb,D)", "http://arxiv.org/abs/1610.06664v1", "NIPS2016"]], "COMMENTS": "NIPS2016", "reviews": [], "SUBJECTS": "stat.ML cs.LG", "authors": ["changyou chen", "nan ding", "chunyuan li", "yizhe zhang", "lawrence carin"], "accepted": true, "id": "1610.06664"}, "pdf": {"name": "1610.06664.pdf", "metadata": {"source": "CRF", "title": "Stochastic Gradient MCMC with Stale Gradients", "authors": ["Changyou Chen", "Nan Ding", "Chunyuan Li", "Yizhe Zhang", "Lawrence Carin"], "emails": ["cc448@duke.edu;", "cl319@duke.edu;", "yz196@duke.edu;", "lcarin@duke.edu;", "dingnan@google.com"], "sections": [{"heading": "1 Introduction", "text": "The pervasiveness of large amounts of data has made scalable machine learning increasingly important, especially for in-depth models. A basic technique is the adoption of stochastic optimization algorithms [1], such as stochastic gradient descent and its extensions [2]. In every iteration of stochastic optimization techniques, a minibatch of data is used to evaluate the gradients of the objective function and the updated model parameters (errors are introduced in the gradients because they are based on minibatches rather than on the entire dataset; since the minibatches are typically selected at random, this results in the term \"stochastic\" gradients). This is highly scalable because processing a minibatch of data in each iteration is relatively cheap compared to analyzing the entire (large) dataset. Under certain conditions, stochastic optimization is guaranteed to a (local) optimization."}, {"heading": "2 Stochastic Gradient MCMC", "text": "In the analysis, we consider algorithms with fixed step variables for simplicity; decreasing step variants can be addressed similar to those in [17]. The goal of SG-MCMC is to generate random samples from a posterior distribution p (D). (D) p (V) p (V) p (D) p (D) p (D) p (V) p (V) p (V) p (V) p (V)))), which are used to evaluate a test function. (D) Here, Rn represents the parameter vector and D = {d1, dN} the data, p (V) is the previous distribution, and p (D) the probability distribution for di. SG-MCMC algorithms are based on a class of stochastic differential equations called ItDiffusion."}, {"heading": "3 Stochastic Gradient MCMC with Stale Gradients", "text": "In this section, we extend SG-MCMC to the setting of the waste gradient, which is usually achieved in asynchronous distributed systems [7, 8, 9], and develop theory for analyzing the convergence properties. 3.1 Stale stochastic gradient MCMC (S2G-MCMC) The setting for S2G-MCMC corresponds to the standard version SG-MCMC described above, except that the stochastic gradient (2) is replaced by a stochastic gradient evaluated with outdated parameters."}, {"heading": "3.2 Convergence analysis", "text": "It is shown that the bias and MSE system of the standard SG-MCMC algorithms were analyzed using a Kth order integrator, where the order of an integrator reflects how accurately a SG-MCMC algorithm approximates the corresponding continuous diffusion. Specifically, when the development xt is induced using a numerical integrator using discrete time h, the integrator is referred to as the Kth order integrator, e.g. the popular Euler method used in SGLD."}, {"heading": "3.3 Extension to multiple parallel chains", "text": "In this section, the theory is extended to the setting with S parallel chains, each running an S2G-MCMC algorithm. After generating samples from the S chains, an aggregation step is required to combine the sample average from each chain, i.e., the sample averages are given in relation to S2G-Ls = 1, where Ls is the number of iterations on the chain s. For the general public, we allow each chain to have different step sizes, e.g. (hs) Ss = 1. We aggregate the sample averages in relation to SL, \u2211 S s = 1 Ts T-Ls, where Ts, Lshs, T \u2212 S s s s = 1 T.Interestingly, the use of multiple chains does not seem to directly improve the convergence rate for the bias, but improves the MSE limit, as stated in Theorem 6.Theorem."}, {"heading": "4 Applications to Distributed SG-MCMC Systems", "text": "Our theory for S2G-MCMC is general, serving as a basic analytical tool for distributed SG-MCMC systems. We propose two simple Bayesian distributed systems with S2G-MCMC below. Single chain distributed SG-MCMC Perhaps the simplest architecture is an asynchronously distributed SG-MCMC system, where a server runs an S2G-MCMC algorithm, with stale gradients calculated asynchronously by W-workers. Detailed operations of the server and workers are described in Appendix A. With our theory, we now explain the convergence property of this simple distributed system with SG-MCMC, i.e., a linear speedup w.r.t. the number of workers on the decrease of deviations while maintaining the same bias level, we rewrite L = WL-2 and 3, where L-2 is the average number of workers on each worker."}, {"heading": "5 Experiments", "text": "We will first use two synthetic experiments to validate the theory, and then apply the distributed architecture for Bayesian deep learning described in Section 4. To quantify the acceleration property, we will use the acceleration of iteration [12], defined as: acceleration of iteration, # iteration with a single working average # iteration on a worker, where # is the iteration number when the same level of precision is achieved, and this acceleration is most consistent with the theory. We will also consider the acceleration of time, defined as: runtime for a single labor error by a worker, where the runtime is recorded with the same accuracy. It is heavily influenced by hardware and therefore not exactly in line with the theory."}, {"heading": "5.1 Synthetic experiments", "text": "In the experiments we set C = 1 / 30, L0 = 500, SO = 1, SO = 2, SO = 15, SO = 15, SO = 15, SO = 15, SO = 15, SO = 15, SO = 15, SO = 15, SO = 15, SO = 15, SO = 15, SO = 15, SO = 15, SO = 15, SO = 15, SO = 15, SO = 15, SO = 15, SO = 15, SO = 15, SO = 15, SO = 15, SO = 15, SO = 15, SO = 15, SO = 15, SO = 15, SO = 15, SO = 15, SO = 15, SO = 15, SO = 15, SO = 15, SO = 15, SO = 15, SO = 15, SO = 15, SO = 15, SO = 15, SO = 15, SO = 15, SO = 15, SO = 15, SO = 15, SO = 15, SO = 15, SO = 15, SO = 15, SO = 15, SO = 15, SO = 15, SO = 19, SO, SO = 19, SO, SO = 19, SO, SO = 19, SO, SO = 19, SO = 19, SO, SO = 19, SO, SO = 19, SO = 19, SO, SO = 19, SO, SO = 19, SO, SO = 19, SO, SO = 19, SO = 19, SO = 19, SO, SO, SO = 19, SO = 19, SO, SO, SO = 19, SO, SO = 19, SO, SO, SO, SO = 15, SO = 15, SO, SO, SO = 15, SO, SO = 15, SO = 15, SO, SO = 15, SO, SO, SO = 15, SO, SO, SO = 15, SO, SO = 15, 15, SO = 15,"}, {"heading": "5.2 Applications to deep learning", "text": "We continue to test S2G-MCMC on Bayesian learning of deep neural networks. The distributed system is based on an MPI (message passing interface) extension of the popular Caffe package for deep learning [32]. We implement the SGHMC algorithm, with point-to-point communication between servers and workers, which is managed by the MPICH library. The algorithm is run on a cluster of five machines. Each machine is equipped with eight 3.60GHz Intel (R) core (TM) i7-4790 CPU cores.We evaluate S2G-MCMC on the above BLR model and two deep Convolutionary Neural Networks (CNN).In all of these models, zero-mean and unit variance Gauss priors are used for weights to capture weight uncertainties, an effective way to deal with the retrofit. [33] We vary the number of net servers {1, net workers {1, net number 7, and IST for each number of net, from 9.5 to net workers}."}, {"heading": "5.2.1 Single-server experiments", "text": "We first test the single server architecture in Section 4 using the three models. Since expectations in bias, MSE or variance are not analytically available in these complex models, we instead record the loss relative to the average number of iterations (defined in Section 4) for each worker and the runtime in Figure 3. As mentioned above, a faster reduction in loss is expected with more workers. For easier visualization, we record the results only with {1, 2, 4, 6, 9} workers; more detailed results are provided in Appendix I. We can see that the loss generally decreases faster with increasing number of workers. In the CIFAR 10 dataset, the final losses of 6 and 9 workers are worst than the loss with 4 workers. It turns out that the accuracy of the average sample value suffers from the increased durability due to increased number of workers. Therefore, a smaller step size should be considered to maintain high accuracy with a large number of workers."}, {"heading": "5.2.2 Multiple-server experiments", "text": "Finally, we test the architecture of multiple servers on the same models. We use the same criterion as the single server setting to measure convergence behavior. Loss versus the average number of iterations for each worker (defined in Section 4) for the three datasets is shown in Figure 4, where we vary the number of servers between {1, 3, 5, 7} and use 2 workers for each server. Graphs Loss versus time and use of different number of workers for each server are provided in the appendix. We can see that in the simple BLR model multiple servers do not appear to show significant acceleration, probably due to the simplicity of the rear, where the sample variance is too small for multiplexers to take effect; while in the more complex deep neural networks, using more servers leads to a faster reduction in loss, especially in the MNIST dataset."}, {"heading": "6 Conclusion", "text": "We extend the theory from the standard S2G-MCMC to the old-fashioned stochastic gradient setting and analyze the effects of durability on the convergence behavior of an S2G-MCMC algorithm. Our theory shows that the estimate variance is independent of durability, resulting in a linear acceleration of the number of workers, although in practice, due to their durability dependence, little acceleration could be achieved in terms of optimal bias and MSE. We test our theory using a simple asynchronously distributed SG-MCMC system with two simulated examples and several deep neural network models. Experimental results confirm the effectiveness and scalability of the proposed S2G-MCMC framework."}, {"heading": "B Assumptions", "text": "First, we must assume that the corresponding SDE of the SG-MCMC will be either elliptical or hypoelliptical; the ellipsis / hypoelliptic describes whether Brownian motion is able to propagate across the entire parameter space; the SDE of the SGLD is elliptical, while for other SGMCMC algorithms such as the SGHMC the hypoellipticality assumption is usually reasonable; if domain x is on the torus, the ellipticity and hypoellipticity of an SDE guarantees the existence of a nice solution to the Poisson equation (5); the assumption is summarized in Assumption 2. The corresponding SDE of an SG-MCMC algorithm is either elliptical or hypoelliptic; if x is extended to the domain of the Rp, we need some assumptions to solve the Poisson equation (5)."}, {"heading": "C Notation", "text": "For the sake of simplicity, we will simplify some spellings which are used in the proof as follows: \u0435\u043dU \u0441l (\u03b8lh), \u0435\u043dU \u0441U \u0442lh, G \u044blh \u0442Ul (\u03b8lh), \u043d\u043e\u043b\u043e\u043b\u043e\u043b\u0435\u0442Ulh, Glh \u0432 (Xlh), \u043alh"}, {"heading": "D Proof of Theorem 2", "text": "In S2G-MCMC, for the l-te iteration, we assume that a stochastic gradient (1) l (1) l (1) l (1) l (1) l (1) l (1) l (1) l (1) l (1) l (1) l (1) l (1) l (1) l (1) l (1) l (1) l (1) l (1) l (1) l (1) l (1) l (1) l) l (1) l (1) l) l (1) l (1) l) l (1) l (1) l) l (1) l (1) l) l (1) l (1) l (1) l (1) l (1) l (1) l (1) l (1) l (1) l (1) l (1) l (1 l) l (1) l (1 l) l (1 l) l (1 l) l (1) l (1) l) l (1 l) l (1) l) l (1 l) l (1) l) l (1) l) l (1 l (1) l) l (1) l (1) l (1) l) l (1) l (1) l) l) l (1 l (1) l) l) l (1 l (1) l) l) l (1 l) l (1 l) l) l (1 l) l (1 l) l (1 l) l) l (1 l) l (1 l) l (1 l (1 l) l) l (1 l) l) l (1 l) l (1 l (1 l) l (1 l) l (1 l) l) l (1 l) l) l (1 l (1 l) l (1 l) l) l (1 l) l (1 l) l (1 l) l) l (1 l (1 l) l (1 l) l) l (1 l) l (1 l (1 l) l) l (1 l (1 l) l) l (1 l) l (1 l) l (1 l (1 l) l) l (1 l) l ("}, {"heading": "E Proof of Theorem 3", "text": "Proof. (1) Proof. (1) Proof. (1) Proof. (1) Proof. (1) Proof. (1) Proof. (1) Proof. (1) Proof. (1) Proof. (1) Proof. (1) Proof. (1) Proof. (1) Proof. (1) Proof. (1) Proof. (1) Proof. (1) Proof. (1) Proof. (1) Proof. (1) Proof. (1) Proof. (1) Proof. (1) Proof. (1) Proof. (1) Proof. (1) Proof. (1) Proof. (1) Proof. (1) Proof. (1) Proof. (1) Proof. (1) Proof. (1) Proof. (1) Proof. (1) Proof. (1) Proof. (1) Proof. (1) Proof. (1) Proof. (1) Proof. (1) Proof. (1) Proof. (1) Proof. (1) Proof. (1) Proof. (1) Proof. (1) Proof. (1) Proof. (1) Proof. (1) Proof. (1) Proof. (1) Proof. (1) Proof. (1) Proof. (1) Proof. (1) Proof. (1) Proof. (1) Proof. (1) Proof. (1) Proof. (1) Proof. (1) Proof. (1) Proof. (1) Proof. (1) Proof. (1) Proof. (1) Proof. (1) Proof. (1) Proof. (1) Proof. (1) Proof. (1) Proof. (1) Proof. (1) Proof. (1) Proof. (1) Proof. (1) Proof. (1) Proof. (1) Proof. (1). (1) Proof. (1) Proof. (1). (1) Proof. (1) Proof. ("}, {"heading": "F Proof of Theorem 4", "text": "In the following we will use the following simple result: Lemma 10. let (M1, \u00b7 l) (1, \u00b7 l) (1, \u00b7 l) (1, \u00b7 l) (1, \u2212 l) (1, \u2212 l) (1, \u2212 l) (1, \u2212 l) (1, \u2212 l) (1, \u2212 l) (1, \u2212 l) (1, \u2212 l) (1, \u2212 l) (1, \u2212 l) (1, l) (1, l) (1, l) (1, l) (1) (1, l) (1, l) (1) (1), l (1) (1) (l) (1) (1), l (1) (1) (1), l (1), l (1) (1), l (1) (1), l (1), l (1), l (1), l (1), l (1, l (1), l (1, l, l (1), l (1, l), l (1, l, l (1), l (1, l), l (1), l (1, l (1), l (1), l (1), l (1), l (1, l (1), l (1), l (1), l (1), l (1, l (1), l (1), l (1), l (1), l (1, l (1), l (1), l (1), l (1), l (1, l (1), l (1), l (1), l (1, l (1), l (1, l (1), l (1), l (1), l (1, l (1, l (1), l (1), l (1), l (1, l (1), l (1, l (1), l (1, l (1), l (1), l (1), l (1, l (1), l (1, l (1), l (1), l (1, l (1), l (1), l (1) (1, l (1,"}, {"heading": "G Proof of Theorem 6", "text": "We separate the proof of the bias and the bias, or the proof of the bias. According to the definition of the bias, we have the proof of the bias and the proof of the bias. After the definition of the bias, we have the proof of the bias and the proof of the bias."}, {"heading": "H Proof of Theorem 7", "text": "Proof. If we follow the proof of theorem 6, we have for the variance E (?? SL? E\u03c6) 2 = E (S? s = 1 Ts T (? Ls? E\u03c6? Ls)) 2 = S? s = 1 T 2s T 2 E (? Ls??? Ls) 2 + 1 Tj T2 E [? Li? E\u03c6 Li] E [? Lj? Lj? E\u03c6 Lj] = S? s = 1 T 2s T 2 E (? Lshs + h2Ks) = D S? s = 1 (Ts T 2 + T 2s T 2 h2Ks). If we replace the variance bound in theory 4 for each server, we have E (? SL? E\u03c6) 2 \u2264 D? s = 1 T 2s T 2 (1 Lshs + h2Ks) = 1 (Ts T 2 h2Ks) = D (1T + S? s = 1 T 2Ks)"}, {"heading": "I Additional Results", "text": "See Figure 6 7 8 9 10 11. The content of the figures is described in the headings."}], "references": [{"title": "Online algorithms and stochastic approximations", "author": ["L. Bottou", "editor"], "venue": null, "citeRegEx": "1", "shortCiteRegEx": "1", "year": 1998}, {"title": "Stochastic gradient descent tricks", "author": ["L. Bottou"], "venue": "Technical report, Microsoft Research, Redmond, WA,", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2012}, {"title": "Bayesian learning via stochastic gradient Langevin dynamics", "author": ["M. Welling", "Y.W. Teh"], "venue": "ICML,", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2011}, {"title": "Stochastic gradient Hamiltonian Monte Carlo", "author": ["T. Chen", "E.B. Fox", "C. Guestrin"], "venue": "ICML,", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2014}, {"title": "Bayesian sampling using stochastic gradient thermostats", "author": ["N. Ding", "Y. Fang", "R. Babbush", "C. Chen", "R.D. Skeel", "H. Neven"], "venue": "NIPS,", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2014}, {"title": "Distributed delayed stochastic optimization", "author": ["A. Agarwal", "J.C. Duchi"], "venue": "NIPS,", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2011}, {"title": "Large scale distributed deep networks", "author": ["J. Dean"], "venue": "In NIPS,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2012}, {"title": "MXNet: A flexible and efficient machine learning library for heterogeneous distributed systems", "author": ["T. Chen"], "venue": null, "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2015}, {"title": "TensorFlow: Large-scale machine learning", "author": ["M. Abadi"], "venue": "on heterogeneous systems,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2015}, {"title": "More effective distributed ML via a stale synchronous parallel parameter server", "author": ["Q. Ho", "J. Cipar", "H. Cui", "J.K. Kim", "S. Lee", "P.B. Gibbons", "G.A. Gibbons", "G.R. Ganger", "E.P. Xing"], "venue": "NIPS,", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2013}, {"title": "Communication efficient distributed machine learning with the parameter server", "author": ["M. Li", "D. Andersen", "A. Smola", "K. Yu"], "venue": "NIPS,", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2014}, {"title": "Asynchronous parallel stochastic gradient for nonconvex optimization", "author": ["X. Lian", "Y. Huang", "Y. Li", "J. Liu"], "venue": "NIPS,", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2015}, {"title": "Scalable inference in latent variable models", "author": ["A. Ahmed", "M. Aly", "J. Gonzalez", "S. Narayanamurthy", "A.J. Smola"], "venue": "WSDM,", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2012}, {"title": "Distributed stochastic gradient MCMC", "author": ["S. Ahn", "B. Shahbaba", "M. Welling"], "venue": "ICML,", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2014}, {"title": "Large-scale distributed Bayesian matrix factorization using stochastic gradient MCMC", "author": ["S. Ahn", "A. Korattikara", "N. Liu", "S. Rajan", "M. Welling"], "venue": "KDD,", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2015}, {"title": "Parallel stochastic gradient Markov chain Monte Carlo for matrix factorisation models", "author": ["U. Simsekli", "H. Koptagel", "Guldas H", "A.Y. Cemgil", "F. Oztoprak", "S. Birbil"], "venue": "Technical report,", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2015}, {"title": "On the convergence of stochastic gradient MCMC algorithms with high-order integrators", "author": ["C. Chen", "N. Ding", "L. Carin"], "venue": "NIPS,", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2015}, {"title": "Hogwild!: A lock-free approach to parallelizing stochastic gradient descent", "author": ["F. Niu", "B. Recht", "C. R\u00e9", "S.J. Wright"], "venue": "NIPS,", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2011}, {"title": "An asynchronous mini-batch algorithm for regularised stochastic optimization", "author": ["H.R. Feyzmahdavian", "A. Aytekin", "M. Johansson"], "venue": "Technical Report arXiv:1505.04824, May", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2015}, {"title": "Asynchronous stochastic convex optimization: the noise is in the noise and SGD don\u2019t care", "author": ["S. Chaturapruek", "J.C. Duchi", "C. R\u00e9"], "venue": "NIPS,", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2015}, {"title": "Bayes and big data: The consensus Monte Carlo algorithm", "author": ["S.L. Scott", "A.W. Blocker", "F.V. Bonassi"], "venue": "Bayes 250,", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2013}, {"title": "Variational consensus Monte Carlo", "author": ["M. Rabinovich", "E. Angelino", "M.I. Jordan"], "venue": "NIPS,", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2015}, {"title": "Asymptotically exact, embarrassingly parallel MCMC", "author": ["W. Neiswanger", "C. Wang", "E.P. Xing"], "venue": "UAI,", "citeRegEx": "23", "shortCiteRegEx": null, "year": 2014}, {"title": "Parallelizing MCMC with random partition trees", "author": ["X. Wang", "F. Guo", "K. Heller", "D. Dunson"], "venue": "NIPS,", "citeRegEx": "24", "shortCiteRegEx": null, "year": 2015}, {"title": "Construction of numerical time-average and stationary measures via Poisson equations", "author": ["J.C. Mattingly", "A.M. Stuart", "M.V. Tretyakov"], "venue": "SIAM Journal on Numerical Analysis, 48(2):552\u2013577,", "citeRegEx": "25", "shortCiteRegEx": null, "year": 2010}, {"title": "A complete recipe for stochastic gradient MCMC", "author": ["Y.A. Ma", "T. Chen", "E.B. Fox"], "venue": "NIPS,", "citeRegEx": "26", "shortCiteRegEx": null, "year": 2015}, {"title": "Stochastic gradient Riemannian Langevin dynamics on the probability simplex", "author": ["S. Patterson", "Y.W. Teh"], "venue": "NIPS,", "citeRegEx": "27", "shortCiteRegEx": null, "year": 2013}, {"title": "Preconditioned stochastic gradient Langevin dynamics for deep neural networks", "author": ["C. Li", "C. Chen", "D. Carlson", "L. Carin"], "venue": "AAAI,", "citeRegEx": "28", "shortCiteRegEx": null, "year": 2016}, {"title": "Deep learning with elastic averaging SGD", "author": ["S. Zhang", "A.E. Choromanska", "Y. Lecun"], "venue": "NIPS,", "citeRegEx": "29", "shortCiteRegEx": null, "year": 2015}, {"title": "Distributed Bayesian learning with stochastic natural-gradient expectation propagation and the posterior server", "author": ["Y.W. Teh", "L. Hasenclever", "T. Lienart", "S. Vollmer", "S. Webb", "B. Lakshminarayanan", "C. Blundell"], "venue": "Technical Report arXiv:1512.09327v1, December", "citeRegEx": "30", "shortCiteRegEx": null, "year": 2015}, {"title": "On Markov chain Monte Carlo methods for tall data", "author": ["R. Bardenet", "A. Doucet", "C. Holmes"], "venue": "Technical Report arXiv:1505.02827, May", "citeRegEx": "31", "shortCiteRegEx": null, "year": 2015}, {"title": "Caffe: Convolutional architecture for fast feature embedding", "author": ["T. Jia", "E. Shelhamer", "J. Donahue", "S. Karayev", "J. Long", "R. Girshick", "S. Guadarrama", "T. Darrell"], "venue": "arXiv preprint arXiv:1408.5093,", "citeRegEx": "32", "shortCiteRegEx": null, "year": 2014}, {"title": "Weight uncertainty in neural networks", "author": ["C. Blundell", "J. Cornebise", "K. Kavukcuoglu", "D. Wierstra"], "venue": "ICML,", "citeRegEx": "33", "shortCiteRegEx": null, "year": 2015}, {"title": "Imagenet classification with deep convolutional neural networks", "author": ["A. Krizhevshy", "I. Sutskever", "G.E. Hinton"], "venue": "NIPS,", "citeRegEx": "34", "shortCiteRegEx": null, "year": 2012}, {"title": "Non-)asymptotic properties of stochastic gradient Langevin dynamics", "author": ["S.J. Vollmer", "K.C. Zygalakis", "Y.W. Teh"], "venue": "Technical Report arXiv:1501.00438, University of Oxford, UK, January", "citeRegEx": "35", "shortCiteRegEx": null, "year": 2015}], "referenceMentions": [{"referenceID": 0, "context": "A basic technique is to adopt stochastic optimization algorithms [1], e.", "startOffset": 65, "endOffset": 68}, {"referenceID": 1, "context": ", stochastic gradient descent and its extensions [2].", "startOffset": 49, "endOffset": 52}, {"referenceID": 0, "context": "Under certain conditions, stochastic optimization is guaranteed to converge to a (local) optima [1].", "startOffset": 96, "endOffset": 99}, {"referenceID": 2, "context": "Because of its scalability, the minibatch strategy has recently been extended to Markov Chain Monte Carlo (MCMC) Bayesian sampling methods, yielding SG-MCMC [3, 4, 5].", "startOffset": 157, "endOffset": 166}, {"referenceID": 3, "context": "Because of its scalability, the minibatch strategy has recently been extended to Markov Chain Monte Carlo (MCMC) Bayesian sampling methods, yielding SG-MCMC [3, 4, 5].", "startOffset": 157, "endOffset": 166}, {"referenceID": 4, "context": "Because of its scalability, the minibatch strategy has recently been extended to Markov Chain Monte Carlo (MCMC) Bayesian sampling methods, yielding SG-MCMC [3, 4, 5].", "startOffset": 157, "endOffset": 166}, {"referenceID": 5, "context": "In order to handle large-scale data, distributed stochastic optimization algorithms have been developed, for example [6], to further improve scalability.", "startOffset": 117, "endOffset": 120}, {"referenceID": 6, "context": "In a distributed setting, a cluster of machines with multiple cores cooperate with each other, typically through an asynchronous scheme, for scalability [7, 8, 9].", "startOffset": 153, "endOffset": 162}, {"referenceID": 7, "context": "In a distributed setting, a cluster of machines with multiple cores cooperate with each other, typically through an asynchronous scheme, for scalability [7, 8, 9].", "startOffset": 153, "endOffset": 162}, {"referenceID": 8, "context": "In a distributed setting, a cluster of machines with multiple cores cooperate with each other, typically through an asynchronous scheme, for scalability [7, 8, 9].", "startOffset": 153, "endOffset": 162}, {"referenceID": 9, "context": "While some theory has been developed to guarantee the convergence of stochastic optimization with stale gradients [10, 11, 12], little analysis has been done in a Bayesian setting, where SG-MCMC is applied.", "startOffset": 114, "endOffset": 126}, {"referenceID": 10, "context": "While some theory has been developed to guarantee the convergence of stochastic optimization with stale gradients [10, 11, 12], little analysis has been done in a Bayesian setting, where SG-MCMC is applied.", "startOffset": 114, "endOffset": 126}, {"referenceID": 11, "context": "While some theory has been developed to guarantee the convergence of stochastic optimization with stale gradients [10, 11, 12], little analysis has been done in a Bayesian setting, where SG-MCMC is applied.", "startOffset": 114, "endOffset": 126}, {"referenceID": 12, "context": "methods, such as [13], usually employ stale statistics instead of stale gradients, where stale statistics are summarized based on outdated parameters, e.", "startOffset": 17, "endOffset": 21}, {"referenceID": 12, "context": ", outdated topic distributions in distributed Gibbs sampling [13].", "startOffset": 61, "endOffset": 65}, {"referenceID": 13, "context": "For existing distributed SG-MCMC methods, typically only standard stochastic gradients are used, for limited problems such as matrix factorization, without rigorous convergence theory [14, 15, 16].", "startOffset": 184, "endOffset": 196}, {"referenceID": 14, "context": "For existing distributed SG-MCMC methods, typically only standard stochastic gradients are used, for limited problems such as matrix factorization, without rigorous convergence theory [14, 15, 16].", "startOffset": 184, "endOffset": 196}, {"referenceID": 15, "context": "For existing distributed SG-MCMC methods, typically only standard stochastic gradients are used, for limited problems such as matrix factorization, without rigorous convergence theory [14, 15, 16].", "startOffset": 184, "endOffset": 196}, {"referenceID": 16, "context": "In this paper, by extending techniques from standard SG-MCMC [17], we develop theory to study the convergence behavior of SG-MCMC with Stale gradients (SG-MCMC).", "startOffset": 61, "endOffset": 65}, {"referenceID": 5, "context": "Representative algorithms include, but are not limited to, the ASYSG-CON [6] and HOGWILD! algorithms [18], and some more recent developments [19, 20].", "startOffset": 73, "endOffset": 76}, {"referenceID": 17, "context": "Representative algorithms include, but are not limited to, the ASYSG-CON [6] and HOGWILD! algorithms [18], and some more recent developments [19, 20].", "startOffset": 101, "endOffset": 105}, {"referenceID": 18, "context": "Representative algorithms include, but are not limited to, the ASYSG-CON [6] and HOGWILD! algorithms [18], and some more recent developments [19, 20].", "startOffset": 141, "endOffset": 149}, {"referenceID": 19, "context": "Representative algorithms include, but are not limited to, the ASYSG-CON [6] and HOGWILD! algorithms [18], and some more recent developments [19, 20].", "startOffset": 141, "endOffset": 149}, {"referenceID": 11, "context": "Furthermore, recent research on stochastic optimization has been extended to non-convex problems with provable convergence rates [12].", "startOffset": 129, "endOffset": 133}, {"referenceID": 20, "context": "In Bayesian learning with MCMC, existing work has focused on running parallel chains on subsets of data [21, 22, 23, 24], and little if any effort has been made to use stale stochastic gradients, the setting considered in this paper.", "startOffset": 104, "endOffset": 120}, {"referenceID": 21, "context": "In Bayesian learning with MCMC, existing work has focused on running parallel chains on subsets of data [21, 22, 23, 24], and little if any effort has been made to use stale stochastic gradients, the setting considered in this paper.", "startOffset": 104, "endOffset": 120}, {"referenceID": 22, "context": "In Bayesian learning with MCMC, existing work has focused on running parallel chains on subsets of data [21, 22, 23, 24], and little if any effort has been made to use stale stochastic gradients, the setting considered in this paper.", "startOffset": 104, "endOffset": 120}, {"referenceID": 23, "context": "In Bayesian learning with MCMC, existing work has focused on running parallel chains on subsets of data [21, 22, 23, 24], and little if any effort has been made to use stale stochastic gradients, the setting considered in this paper.", "startOffset": 104, "endOffset": 120}, {"referenceID": 16, "context": "In the analysis we consider algorithms with fixed-stepsizes for simplicity; decreasingstepsize variants can be addressed similarly as in [17].", "startOffset": 137, "endOffset": 141}, {"referenceID": 24, "context": "SG-MCMC algorithms are based on a class of stochastic differential equations, called It\u00f4 diffusion, defined as d xt = F (xt)dt+ g(xt)dwt , (1) where x \u2208 R represents the model states, typically x augments \u03b8 such that \u03b8 \u2286 x and n \u2264 m; t is the time index, wt \u2208 R is m-dimensional Brownian motion, functions F : R \u2192 R and g : R \u2192 Rm\u00d7m are assumed to satisfy the usual Lipschitz continuity condition [25].", "startOffset": 397, "endOffset": 401}, {"referenceID": 25, "context": "For appropriate functions F and g, the stationary distribution, \u03c1(x), of the It\u00f4 diffusion (1) has a marginal distribution equal to the posterior distribution p(\u03b8|D) [26].", "startOffset": 166, "endOffset": 170}, {"referenceID": 2, "context": "For example, denoting the unnormalized negative log-posterior as U(\u03b8) , \u2212 log p(\u03b8) \u2212 \u2211N i=1 log p(di |\u03b8), the stochastic gradient Langevin dynamic (SGLD) method [3] is based on 1st-order Langevin dynamics, with x = \u03b8, and F (xt) = \u2212\u2207\u03b8U(\u03b8), g(xt) = \u221a 2 In, where In is the n \u00d7 n identity matrix.", "startOffset": 161, "endOffset": 164}, {"referenceID": 3, "context": "The stochastic gradient Hamiltonian Monte Carlo (SGHMC) method [4] is based on 2nd-order Langevin dynamics, with x = (\u03b8,q), and F (xt) = ( q \u2212B q\u2212\u2207\u03b8U(\u03b8) ) , g(xt) = \u221a 2B ( 0 0 0 In ) for a scalar B > 0; q is an auxiliary variable known as the momentum [4, 5].", "startOffset": 63, "endOffset": 66}, {"referenceID": 3, "context": "The stochastic gradient Hamiltonian Monte Carlo (SGHMC) method [4] is based on 2nd-order Langevin dynamics, with x = (\u03b8,q), and F (xt) = ( q \u2212B q\u2212\u2207\u03b8U(\u03b8) ) , g(xt) = \u221a 2B ( 0 0 0 In ) for a scalar B > 0; q is an auxiliary variable known as the momentum [4, 5].", "startOffset": 252, "endOffset": 258}, {"referenceID": 4, "context": "The stochastic gradient Hamiltonian Monte Carlo (SGHMC) method [4] is based on 2nd-order Langevin dynamics, with x = (\u03b8,q), and F (xt) = ( q \u2212B q\u2212\u2207\u03b8U(\u03b8) ) , g(xt) = \u221a 2B ( 0 0 0 In ) for a scalar B > 0; q is an auxiliary variable known as the momentum [4, 5].", "startOffset": 252, "endOffset": 258}, {"referenceID": 4, "context": "SG-MCMC algorithms, such as the stochastic gradient thermostat [5] and variants with Riemannian information geometry [27, 26, 28], are defined similarly.", "startOffset": 63, "endOffset": 66}, {"referenceID": 26, "context": "SG-MCMC algorithms, such as the stochastic gradient thermostat [5] and variants with Riemannian information geometry [27, 26, 28], are defined similarly.", "startOffset": 117, "endOffset": 129}, {"referenceID": 25, "context": "SG-MCMC algorithms, such as the stochastic gradient thermostat [5] and variants with Riemannian information geometry [27, 26, 28], are defined similarly.", "startOffset": 117, "endOffset": 129}, {"referenceID": 27, "context": "SG-MCMC algorithms, such as the stochastic gradient thermostat [5] and variants with Riemannian information geometry [27, 26, 28], are defined similarly.", "startOffset": 117, "endOffset": 129}, {"referenceID": 6, "context": "3 Stochastic Gradient MCMC with Stale Gradients In this section, we extend SG-MCMC to the stale-gradient setting, commonly met in asynchronous distributed systems [7, 8, 9], and develop theory to analyze convergence properties.", "startOffset": 163, "endOffset": 172}, {"referenceID": 7, "context": "3 Stochastic Gradient MCMC with Stale Gradients In this section, we extend SG-MCMC to the stale-gradient setting, commonly met in asynchronous distributed systems [7, 8, 9], and develop theory to analyze convergence properties.", "startOffset": 163, "endOffset": 172}, {"referenceID": 8, "context": "3 Stochastic Gradient MCMC with Stale Gradients In this section, we extend SG-MCMC to the stale-gradient setting, commonly met in asynchronous distributed systems [7, 8, 9], and develop theory to analyze convergence properties.", "startOffset": 163, "endOffset": 172}, {"referenceID": 16, "context": "Bias and MSE In [17], the bias and MSE of the standard SG-MCMC algorithms with a Kth order integrator were analyzed, where the order of an integrator reflects how accurately an SG-MCMC algorithm approximates the corresponding continuous diffusion.", "startOffset": 16, "endOffset": 20}, {"referenceID": 2, "context": ", the popular Euler method used in SGLD [3] is a 1st-order integrator.", "startOffset": 40, "endOffset": 43}, {"referenceID": 16, "context": "In particular, [17] proved the bounds stated in Lemma 1.", "startOffset": 15, "endOffset": 19}, {"referenceID": 16, "context": "Lemma 1 ([17]).", "startOffset": 9, "endOffset": 13}, {"referenceID": 16, "context": "In addition to the assumptions in SG-MCMC [17] (see details in Appendix B), the following additional assumption is imposed.", "startOffset": 42, "endOffset": 46}, {"referenceID": 24, "context": "\u2217The existence of a nice \u03c8 is guaranteed in the elliptic/hypoelliptic SDE settings when x is on a torus [25].", "startOffset": 104, "endOffset": 108}, {"referenceID": 13, "context": "In addition, Theorem 6 with \u03c4 = 0 and K = 1 provides convergence rates for the distributed SGLD algorithm in [14], i.", "startOffset": 109, "endOffset": 113}, {"referenceID": 6, "context": "More advanced architectures More complex architectures could also be designed to reduce communication cost, for example, by extending the downpour [7] and elastic SGD [29] architectures to the SG-MCMC setting.", "startOffset": 147, "endOffset": 150}, {"referenceID": 28, "context": "More advanced architectures More complex architectures could also be designed to reduce communication cost, for example, by extending the downpour [7] and elastic SGD [29] architectures to the SG-MCMC setting.", "startOffset": 167, "endOffset": 171}, {"referenceID": 29, "context": "5 Experiments Our primal goal is to validate the theory, comparing with different distributed architectures and algorithms, such as [30, 31], is beyond the scope of this paper.", "startOffset": 132, "endOffset": 140}, {"referenceID": 30, "context": "5 Experiments Our primal goal is to validate the theory, comparing with different distributed architectures and algorithms, such as [30, 31], is beyond the scope of this paper.", "startOffset": 132, "endOffset": 140}, {"referenceID": 11, "context": "To quantitatively describe the speedup property, we adopt the the iteration speedup [12], defined as: iteration speedup , #iterations with a single worker average #iterations on a worker , where # is the iteration count when the same level of precision is achieved.", "startOffset": 84, "endOffset": 88}, {"referenceID": 31, "context": "The distributed system is developed based on an MPI (message passing interface) extension of the popular Caffe package for deep learning [32].", "startOffset": 137, "endOffset": 141}, {"referenceID": 32, "context": "In all these models, zero mean and unit variance Gaussian priors are employed for the weights to capture weight uncertainties, an effective way to deal with overfitting [33].", "startOffset": 169, "endOffset": 173}, {"referenceID": 33, "context": "LeNet consists of 2 convolutional layers, 2 max pool layers and 2 ReLU nonlinear layers, followed by 2 fully connected layers [34].", "startOffset": 126, "endOffset": 130}, {"referenceID": 0, "context": "References [1] L.", "startOffset": 11, "endOffset": 14}, {"referenceID": 1, "context": "[2] L.", "startOffset": 0, "endOffset": 3}, {"referenceID": 2, "context": "[3] M.", "startOffset": 0, "endOffset": 3}, {"referenceID": 3, "context": "[4] T.", "startOffset": 0, "endOffset": 3}, {"referenceID": 4, "context": "[5] N.", "startOffset": 0, "endOffset": 3}, {"referenceID": 5, "context": "[6] A.", "startOffset": 0, "endOffset": 3}, {"referenceID": 6, "context": "[7] J.", "startOffset": 0, "endOffset": 3}, {"referenceID": 7, "context": "[8] T.", "startOffset": 0, "endOffset": 3}, {"referenceID": 8, "context": "[9] M.", "startOffset": 0, "endOffset": 3}, {"referenceID": 9, "context": "[10] Q.", "startOffset": 0, "endOffset": 4}, {"referenceID": 10, "context": "[11] M.", "startOffset": 0, "endOffset": 4}, {"referenceID": 11, "context": "[12] X.", "startOffset": 0, "endOffset": 4}, {"referenceID": 12, "context": "[13] A.", "startOffset": 0, "endOffset": 4}, {"referenceID": 13, "context": "[14] S.", "startOffset": 0, "endOffset": 4}, {"referenceID": 14, "context": "[15] S.", "startOffset": 0, "endOffset": 4}, {"referenceID": 15, "context": "[16] U.", "startOffset": 0, "endOffset": 4}, {"referenceID": 16, "context": "[17] C.", "startOffset": 0, "endOffset": 4}, {"referenceID": 17, "context": "[18] F.", "startOffset": 0, "endOffset": 4}, {"referenceID": 18, "context": "[19] H.", "startOffset": 0, "endOffset": 4}, {"referenceID": 19, "context": "[20] S.", "startOffset": 0, "endOffset": 4}, {"referenceID": 20, "context": "[21] S.", "startOffset": 0, "endOffset": 4}, {"referenceID": 21, "context": "[22] M.", "startOffset": 0, "endOffset": 4}, {"referenceID": 22, "context": "[23] W.", "startOffset": 0, "endOffset": 4}, {"referenceID": 23, "context": "[24] X.", "startOffset": 0, "endOffset": 4}, {"referenceID": 24, "context": "[25] J.", "startOffset": 0, "endOffset": 4}, {"referenceID": 25, "context": "[26] Y.", "startOffset": 0, "endOffset": 4}, {"referenceID": 26, "context": "[27] S.", "startOffset": 0, "endOffset": 4}, {"referenceID": 27, "context": "[28] C.", "startOffset": 0, "endOffset": 4}, {"referenceID": 28, "context": "[29] S.", "startOffset": 0, "endOffset": 4}, {"referenceID": 29, "context": "[30] Y.", "startOffset": 0, "endOffset": 4}, {"referenceID": 30, "context": "[31] R.", "startOffset": 0, "endOffset": 4}, {"referenceID": 31, "context": "[32] T.", "startOffset": 0, "endOffset": 4}, {"referenceID": 32, "context": "[33] C.", "startOffset": 0, "endOffset": 4}, {"referenceID": 33, "context": "[34] A.", "startOffset": 0, "endOffset": 4}, {"referenceID": 34, "context": "[35] S.", "startOffset": 0, "endOffset": 4}, {"referenceID": 24, "context": "B Assumptions First, following [25], we will need to assume the corresponding SDE of SG-MCMC to be either elliptic or hypoelliptic.", "startOffset": 31, "endOffset": 35}, {"referenceID": 34, "context": "Note (5) can be equivalently written in an integration form [35] using It\u00f4\u2019s formula: 1 t \u222b t", "startOffset": 60, "endOffset": 64}, {"referenceID": 24, "context": "This is satisfied if the SDE is defined in a bounded domain [25].", "startOffset": 60, "endOffset": 64}, {"referenceID": 16, "context": "In the unbounded domain as for SG-MCMC algorithms, it turns out the following boundedness assumptions on \u03c8 suffice [17].", "startOffset": 115, "endOffset": 119}, {"referenceID": 24, "context": "For other SG-MCMC algorithms such as the SGHMC, the hypoellipticity assumption is usually reasonable, see [25] on how to verify hypoellipticity of an SDE.", "startOffset": 106, "endOffset": 110}, {"referenceID": 34, "context": "To ensure the remainder term r`,F,\u03c6(x0) to be bounded, it suffices to make the following assumption on the smoothness and boundedness of F (x) [35, 17].", "startOffset": 143, "endOffset": 151}, {"referenceID": 16, "context": "To ensure the remainder term r`,F,\u03c6(x0) to be bounded, it suffices to make the following assumption on the smoothness and boundedness of F (x) [35, 17].", "startOffset": 143, "endOffset": 151}, {"referenceID": 16, "context": "The basic technique follows [17], thus we skip some derivations for some steps.", "startOffset": 28, "endOffset": 32}, {"referenceID": 16, "context": "Following [17], for an SG-MCMC with a Kth-order integrator, and a test function \u03c6, we have: E[\u03c8(xlh)] = ( I + hL\u0303l ) \u03c8(x(l\u22121)h) (11)", "startOffset": 10, "endOffset": 14}, {"referenceID": 16, "context": "According to [17], the term \u2211 l E[L\u0303 k l \u03c8(x(l\u22121)h)] is bounded by \u2211 l E[L\u0303 k l \u03c8(X(l\u22121)h)]", "startOffset": 13, "endOffset": 17}, {"referenceID": 16, "context": "From the proof of Theorem 3 in [17], A2 and A4 are also bounded, which are summarized in Lemma 9.", "startOffset": 31, "endOffset": 35}], "year": 2016, "abstractText": "Stochastic gradient MCMC (SG-MCMC) has played an important role in largescale Bayesian learning, with well-developed theoretical convergence properties. In such applications of SG-MCMC, it is becoming increasingly popular to employ distributed systems, where stochastic gradients are computed based on some outdated parameters, yielding what are termed stale gradients. While stale gradients could be directly used in SG-MCMC, their impact on convergence properties has not been well studied. In this paper we develop theory to show that while the bias and MSE of an SG-MCMC algorithm depend on the staleness of stochastic gradients, its estimation variance (relative to the expected estimate, based on a prescribed number of samples) is independent of it. In a simple Bayesian distributed system with SG-MCMC, where stale gradients are computed asynchronously by a set of workers, our theory indicates a linear speedup on the decrease of estimation variance w.r.t. the number of workers. Experiments on synthetic data and deep neural networks validate our theory, demonstrating the effectiveness and scalability of SG-MCMC with stale gradients.", "creator": "LaTeX with hyperref package"}}}