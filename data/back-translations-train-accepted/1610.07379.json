{"id": "1610.07379", "review": {"conference": "NIPS", "VERSION": "v1", "DATE_OF_SUBMISSION": "24-Oct-2016", "title": "Truncated Variance Reduction: A Unified Approach to Bayesian Optimization and Level-Set Estimation", "abstract": "We present a new algorithm, truncated variance reduction (TruVaR), that treats Bayesian optimization (BO) and level-set estimation (LSE) with Gaussian processes in a unified fashion. The algorithm greedily shrinks a sum of truncated variances within a set of potential maximizers (BO) or unclassified points (LSE), which is updated based on confidence bounds. TruVaR is effective in several important settings that are typically non-trivial to incorporate into myopic algorithms, including pointwise costs and heteroscedastic noise. We provide a general theoretical guarantee for TruVaR covering these aspects, and use it to recover and strengthen existing results on BO and LSE. Moreover, we provide a new result for a setting where one can select from a number of noise levels having associated costs. We demonstrate the effectiveness of the algorithm on both synthetic and real-world data sets.", "histories": [["v1", "Mon, 24 Oct 2016 12:18:00 GMT  (2536kb,D)", "http://arxiv.org/abs/1610.07379v1", "Accepted to NIPS 2016"]], "COMMENTS": "Accepted to NIPS 2016", "reviews": [], "SUBJECTS": "stat.ML cs.IT cs.LG math.IT", "authors": ["ilija bogunovic", "jonathan scarlett", "andreas krause 0001", "volkan cevher"], "accepted": true, "id": "1610.07379"}, "pdf": {"name": "1610.07379.pdf", "metadata": {"source": "CRF", "title": "Truncated Variance Reduction: A Unified Approach to Bayesian Optimization and Level-Set Estimation", "authors": ["Ilija Bogunovic", "Jonathan Scarlett", "Andreas Krause", "Volkan Cevher"], "emails": ["ilija.bogunovic@epfl.ch,", "jonathan.scarlett@epfl.ch,", "volkan.cevher@epfl.ch,", "krausea@ethz.ch"], "sections": [{"heading": "1 Introduction", "text": "Bayesian Optimization (BO) [1] provides a powerful framework for automating design problems and finds applications in robotics, environmental monitoring, and automated machine learning, to name a few. It tries to find the maximum of an unknown reward function that is expensive to evaluate, based on a sequence of appropriately selected points and noisy observations. Numerous BO algorithms have been presented before; see Section 1.1 for an overview. The Level Set Estimation (LSE) [2] is closely related to BO, with the added twist that instead of looking for a maximizer, one tries to classify the domain into points above or below a certain threshold. This is of considerable interest for applications such as environmental monitoring and sensor networks, which allow one to find all \"sufficiently good\" points rather than the best point alon.While BO and LSE are closely related, they are typically examined in isolation (we provide a significant new algorithm for this treatment)."}, {"heading": "1.1 Previous Work", "text": "Three popular myopic techniques for Bayesian optimization are expected, Improvement (EI), Probability of Improvement (PI), and Gaussian Process Upper Confidence (GP-UCB) [1, 3], each of which chooses the point that maximizes an acquisition function based directly on the current posterior mean and variance. In [4], the GP-UCB PE algorithm for BO was presented, selecting the point with the highest variance among a number of potential maximizers that is updated to confidence limits. Another relevant BO algorithm is BaMSOO [5], which also tracks potential maximizers but instead chooses points based on a global optimization technique called Xiv: 161 0.07 379v 1 [stat.ML] 2 4O ctonline optimization (SOO). An algorithm for level set estimation with GPs is given in [2], which has a set of non-classified points."}, {"heading": "1.2 Contributions", "text": "We present a standardized analysis of Bayean optimization and level estimation using a new Truncated Variance Reduction (TRUVAR) algorithm. The algorithm works by tracking a number of potential maximizers (BO) or unclassified points (LSE), selecting points that reduce uncertainty within that specified threshold to a truncation threshold, and updating the set based on confidence limits. Similar to ES and MRS, the algorithm performs a one-step outlook that is highly advantageous in terms of versatility. However, unlike this previous work, our outlook avoids the computationally costly task of averaging the rear distribution and observations. Similarly, unlike ES and MRS, we offer theoretical limits for TRUVAR that characterize the costs needed to determine a certain accuracy in determining a near-optimal point (BO) or in classifying each point in the domain (LSE), we achieve this standard setting by improving the corresponding Wympatic one, but also by applying some of the corresponding Wympatic ones."}, {"heading": "2 Problem Setup and Proposed Algorithm", "text": "We strive to re-classify an unknown reward function f (x) over a finite domain (BO) with a finite domain D.1 In due course, we query a single point xt (D) and observe a random sample yt = f (xt) + zt, in which each point has a cost threshold (0, \u03c32 (xt) for some known cost functions \u03c32 (\u00b7): D \u2192 R +. So if both (\u00b7) and c (\u00b7) are to be constant, then we recover the standard homoscedastic and unit cost setting. We associate with each point a cost function c: D \u2192 R +. If both values (\u00b7) are constant, then we recover the homoscedastic and unit cost setting. We have a model f (x) as a Gaussian process (GP) [16] with mean zeros and core functions k (x, x), so that k (x) that k (x) for all x x x."}, {"heading": "3 Theoretical Bounds", "text": "In order to present our results for BO and LSE in a consistent manner, we define a term of -accuracy for the two settings. That is, we define this term differently in the two scenarios, but then we provide theorems that apply to both at the same time. All proofs are given in the supplementary material.Definition 3.1. After TRUVAR step t, we use the following terminology: \u2022 For BO, the quantity Mt is -accurate if it contains all true maximums in Ht f (x) > h, and all of its points f (x) \u2212 f (x) \u2264 fulfill. \u2022 For LSE, the triplet (Mt, Ht, Lt) -exact if all points in Ht f (x) > h are fulfilled, all points in Lt f (x) < h, and satisfy all points in Mt | f (x) \u2212 h | \u2264 2. In both cases, the cumulative costs after this time are narrowly defined."}, {"heading": "3.1 General Result", "text": "Preliminary definitions: Let us assume that the {i)} are chosen to ensure valid confidence limits (\u03b2 = \u03b2 = BO), i.e. lt (x) \u2264 f (x) \u2264 ut (x) with high probability; see Theorem 3.1 and its proof for such decisions. In this case, we have after the i-th epoch that all points are either discarded (BO) or classified (LSE) or are known up to the confidence level (1 + \u03b4). For points of such confidence we have (x) lt (x) \u2264 2 (BO) and henceut (x) lt (1 + 6) Prospects with such confidence (x) lt (x) Prospects with such confidence (x) lt (1 + 2) Prospects (x) Prospects with such confidence (x) lt (1 + 2) Prospects (x) Prospects with such confidence (x)"}, {"heading": "3.2 Results for Specific Settings", "text": "Homoscedastic and unit-cost setting: Define the maximum mutual information [3] \u03b3T = \u03b2 x1,..., xT1 2 log det (IT + \u03c3 \u2212 2KT), (16) and consider the case that \u03c32 (x) = \u03c32 and c (x) = potentially 1. In the supplementary material, we provide a theorem with a condition for the -accuracy of the form T \u2264 1, (C1GP T \u03b2T 2 + 1) with C1 = 1log (1 + \u03c3 \u2212 2), which consequently agrees with logarithmic factors. In the following, we present a refined version that has a significantly better dependence on the noise level, thus demonstrating that a more careful analysis of (13) can provide improvements over the standard boundary techniques. Corollary 3.1 fix > 0 and implementable accuracy (0, 1), we define T = 2 log | D | T 6GP."}, {"heading": "4 Experimental Results", "text": "We have written to the flags that we will be able to cover the cost of dissemination of e-mails, e-mails, e-mails, e-mails, e-mails, e-mails, e-mails, e-mails, e-mails, e-mails, e-mails, e-mails, e-mails, e-mails, e-mails, e-mails, e-mails, e-mails, e-mails, e-mails, e-mail, e-mail, e-mail, e-mail, e-mail, e-mail, e-mail, e-mail, e-mail, e-mail, e-mail, e-mail, e-mail, e-mail, e-mail, e-mail, e-mail, e-mail, e-mail, e-mail, e-mail, e-mail, e-mail, e-mail, e-mail, e-mail, e-mail, e-mail, e-mail, e-mail, e-mail, e-mail,"}, {"heading": "5 Conclusion", "text": "We highlight the following aspects in which TRUVAR is versatile: \u2022 Unified optimization and level estimation: These are typically treated separately, while TRUVAR and its theoretical guarantees are essentially identical in both cases \u2022 Costs-related measures: TRUVAR naturally prefers cost-effective points, as these are directly incorporated into the capture function. \u2022 Heteroscedastic noise: TRUVAR selects points that effectively reduce the variance of other points, directly taking advantage of situations where some points are louder than others. \u2022 Sound level selection: We provided novel theoretical guarantees in the event that the algorithm can choose both a point and a noise level, see episode 3.2.Therefore, TRUVAR directly addresses several important aspects that are not trivial to integrate them into myopic algorithms. In addition, compared to other BO algorithms that cost the EU millions of euros to carry out an outlook (e.g. the 665 million SMRI task and 633 million SMRI task), TRUVAR avoids the following aspects where TRUVAR is versatile: \u2022 Unified optimization and level estimation: These are typically treated separately, whereas TRUVAR and its theoretical guarantees are essentially identical in both cases."}, {"heading": "B Further Details of Numerical Experiments", "text": "Other algorithms to be taken into account: We sketch the algorithms that TRUVAR against; full details can be found in the essays quoted. (1) For level set estimation we have the following: \u2022 The GCHK algorithm \u2212 \u2212 \u2212 \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 \u2212 p \u2212 p \u2212 p \u2212 p \u2212 \u2212 \u2212 p \u2212 p \u2212 \u2212 \u2212 \u2212 p \u2212 p \u2212 \u2212 p \u2212 \u2212 \u2212 \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 \u2212 \u2212 \u2212 p \u2212 p \u2212 \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 \u2212 p \u2212 \u2212 p \u2212 p \u2212 p \u2212 \u2212 \u2212 p \u2212 p \u2212 p \u2212 p \u2212 \u2212 \u2212 p \u2212 p \u2212 \u2212 \u2212 \u2212 \u2212 p \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 p \u2212 \u2212 p \u2212 p \u2212 \u2212 p \u2212 p \u2212 p \u2212 \u2212 p \u2212 p \u2212 \u2212 \u2212 \u2212 \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 p \u2212 \u2212"}, {"heading": "C Proof of General Result (Theorem 3.1)", "text": "We start with the following problem of [3].Tagma C.1. [3] For each t, we define \u03b2t = 2 Log = 2 Log = 1 Log = 2 Log = 2 Log = 2 Log = 2 Log = 2 Log = 2 Log = 2 Log = 2 Log = 2 Log = 2 Log = 2 Log = 2 Log = 2 Log = 2 Log = 2 Log = 2 Log = 2 Log = 2 Log = 2 Log = 2 Log = 2 Log = 2 Log = 2 Log = 2 Log = 2 Log = 2 Log = 2 Log = 2 Log = 2 Log = 2 Log = 2 Log = 2 Log = 2 Log = 1 Log = 2 Log = 2 Log = 2 Log = 2 Log = 2 (Log = 2)."}, {"heading": "2(1 + \u03b4)\u03b7(i\u22121) >", "text": "2, which is the same as 4 (1 + \u03b4) \u03b7 (i \u2212 1) > Again, all points in Ht and Lt are correct due to the validity of our confidence limits."}, {"heading": "D Simplified Result for the Homoscedastic and Unit-Cost Setting", "text": "Since we are looking at the unit costs c (x) = 1, 1), 1), 2), 3), 3), 4), 4), 4), 4), 5), 5 (c), 5 (c), 8 (c), 8 (c), 8 (c), 8 (c), 8 (c), 8 (c), 8 (c), 8 (c), 8 (c), 8 (c), 8 (c), 8 (c), 8 (c), 8 (c), 8 (c), 8 (c), 8 (c), 8 (c), 8 (c), 8 (c), 8 (c), 8 (c), 8 (c), 8 (c), 8 (8), 8 (8), 8 (8), 8 (8), 8 (8), 8 (8), 8 (8), 8 (8), 8 (c), 8 (8), 8 (c), 8 (8), 8 (c), 8 (c), 8 (c), 8 (c), 8 (c), 8 (c), 8 (c), 8 (c), 8 (c), 8 (c), 8 (c), 8 (c), 8 (c), 8 (c), 8 (c), 8 (c), 8 (c), 8 (c), 8 (c (8 (c), 8 (c), 8 (8 (c), 8 (c), 8 (8 (8 (8), 8 (8, 8 (c), 8 (8 (8), 8 (8, 8 (8), 8 (8, 8 (c), 8 (8 (8, 8 (8), 8 (c), 8 (8 (8, 8 (c), 8 (8, 8 (8), 8 (c), 8 (8 (8, 8, 8 (c), 8 (c), 8 (8, 8, 8 (c), 8, 8 (c), 8 (c), 8 (8, 8 (c), 8, 8 (c), 8 (c), 8, 8 (c), 8 (c), 8"}, {"heading": "E Proof of Improved Noise Dependence (Corollary 3.1))", "text": "(1) (2) (2) (2) (2) (2) (2) (2) (2) (2) (2) (2) (1) (2) (2) (2) (2) (2) (2) (2) (2) (2) (2) (2) (2) (2) (2) (2) (2) (2) (2) (2) (2) (2) (2) (2) (2) (2) (2) (2) (2) (2) (2) (2) (2) (2) (2) (2) (2) (2) (2) (2) (2) (2) (2) (2) (2) (2) (2) (2) (2) (2) (2) (2) (2) (2) (2) (2) (2) (2) (2) (2) (2) (2) (2) (2) (2) (2) (2) (2) (2) (2) (2) (2) (2) (2) (2) (2) (2) (2) (2) (2) (2) (2) (2) (2) (2) (2) (2) (2) (2) (2) (2) (2) (2) (2) (2) (2) (2) (2) (2) (2) (2) (2) (2) (2) (2) (2) (2) (2) (2) (2) (2) (2) (2) () (2) () (2) (2) (2) (2) () (2) (2) (2) (2) () (2) (2) (2) (2) (2) (2) () (2) (2) (2) (2) (2) () () (2) (2) (2) (2) () (2) (2) (2) () (2) (2) (2) (2) () (2) (2) () () (2) (2) (2) () ("}, {"heading": "F Proof for the Setting of Choosing Noise (Corollary 3.2)", "text": "The evidence follows the same arguments as those of appendices D and E, where C \u0445 in K is delimited upwards in different ways, one for every possible sound level. \u03b2T = 2 log | D | T 2c2max\u03c02 6\u03b4c2minis a simple upper limit to the right side of (14), which results from the fact that \u2211 T = 1 c (xt) \u2264 cmaxT."}], "references": [], "referenceMentions": [], "year": 2016, "abstractText": "We present a new algorithm, truncated variance reduction (TRUVAR), that treats<lb>Bayesian optimization (BO) and level-set estimation (LSE) with Gaussian pro-<lb>cesses in a unified fashion. The algorithm greedily shrinks a sum of truncated<lb>variances within a set of potential maximizers (BO) or unclassified points (LSE),<lb>which is updated based on confidence bounds. TRUVAR is effective in several<lb>important settings that are typically non-trivial to incorporate into myopic algo-<lb>rithms, including pointwise costs and heteroscedastic noise. We provide a general<lb>theoretical guarantee for TRUVAR covering these aspects, and use it to recover<lb>and strengthen existing results on BO and LSE. Moreover, we provide a new result<lb>for a setting where one can select from a number of noise levels having associated<lb>costs. We demonstrate the effectiveness of the algorithm on both synthetic and<lb>real-world data sets.", "creator": "LaTeX with hyperref package"}}}