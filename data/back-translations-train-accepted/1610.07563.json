{"id": "1610.07563", "review": {"conference": "NIPS", "VERSION": "v1", "DATE_OF_SUBMISSION": "24-Oct-2016", "title": "On Multiplicative Multitask Feature Learning", "abstract": "We investigate a general framework of multiplicative multitask feature learning which decomposes each task's model parameters into a multiplication of two components. One of the components is used across all tasks and the other component is task-specific. Several previous methods have been proposed as special cases of our framework. We study the theoretical properties of this framework when different regularization conditions are applied to the two decomposed components. We prove that this framework is mathematically equivalent to the widely used multitask feature learning methods that are based on a joint regularization of all model parameters, but with a more general form of regularizers. Further, an analytical formula is derived for the across-task component as related to the task-specific component for all these regularizers, leading to a better understanding of the shrinkage effect. Study of this framework motivates new multitask learning algorithms. We propose two new learning formulations by varying the parameters in the proposed framework. Empirical studies have revealed the relative advantages of the two new formulations by comparing with the state of the art, which provides instructive insights into the feature learning problem with multiple tasks.", "histories": [["v1", "Mon, 24 Oct 2016 19:27:52 GMT  (7145kb,D)", "http://arxiv.org/abs/1610.07563v1", "Advances in Neural Information Processing Systems 2014"]], "COMMENTS": "Advances in Neural Information Processing Systems 2014", "reviews": [], "SUBJECTS": "cs.LG", "authors": ["xin wang 0023", "jinbo bi", "shipeng yu", "jiangwen sun"], "accepted": true, "id": "1610.07563"}, "pdf": {"name": "1610.07563.pdf", "metadata": {"source": "CRF", "title": "On Multiplicative Multitask Feature Learning", "authors": ["Xin Wang", "Jinbo Bi", "Shipeng Yu", "Jiangwen Sun"], "emails": ["wangxin@engr.uconn.edu", "jinbo@engr.uconn.edu", "javon@engr.uconn.edu", "shipeng.yu@siemens.com"], "sections": [{"heading": "1 Introduction", "text": "These methods are more effective than researching each task individually [16, 11] or identifying when certain tasks are outliers to other tasks [6]. A common MTFL strategy is to impose a block-by-block common regulation of all task parameters in order to reduce the impact of functions on tasks. [16, 11] These methods employ a regulator based on the so-called standard [12, 13, 15, 22] which represents the 24 sums of tasks. [16, 11] or to identify when certain tasks are outliers to other tasks. [17] A widespread MTFL strategy is to impose a block-by-block common regulation of all task parameters in order to reduce the impact of functions on tasks."}, {"heading": "2 The Proposed Multiplicative MTFL", "text": "Considering that T tasks as a whole are indicative for each task t, t-\u03b21, \u00b7 \u00b7 \u00b7, T = \u03b2\u03b2j = any function (Xt tasks), we have sample set (Xt tasks).The data set of Xt contains \"t examples in which the i-th series corresponds to the i-th example of the task t, i-th example xti to the task t, i-i- \u00b2, i-i- \u00b2, and each column represents a characteristic. The vector yt contains yti, the name of the i-th example of the task. We consider functions of the linear form Xt\u03b1t, in which \u03b1t \u00b2 Rd is positive. We define the parameter matrix or the weight matrix A = [\u03b11, \u00b7 \u00b7 \u00b7, \u03b1T] and \u03b1j are the rows, j-th example of the task. A family of multiplicative MTFL methods can be derived by rewriting the parameters of the matrix or formula."}, {"heading": "3 Theoretical Analysis", "text": "The common p regulated MTFL method minimizes the following problem with pre-specified values of p, q and p, for the best parameters. We expand this formula to allow more decisions by regulators. We introduce a new notation, which is an operator applied to a vector. The operator can solve the following optimization problem with pre-specified values of p, q, p, q, 0, which is the \"p standard,\" if p = q and both are positive integers. A common regulated MTFL approach can solve the following problem with pre-specified values of p, q and p, for the best parameters."}, {"heading": "J (3)(\u03b1\u0303tj , c\u0303j). Let \u03c3\u0303j = (c\u0303j)", "text": "k), and we have J (3) (Alpha Formula) (Alpha Formula) (Alpha Formula) (Alpha Formula) (Alpha Formula) (Alpha Formula) (Alpha Formula) (Alpha Formula). Secondly, we demonstrate in a similar way that if we combine the results from the two terms, we can deduce that with Alpha Formula (1), then Alpha Formula (Alpha Formula), then Alpha Formula (Alpha Formula) and Alpha Formula (Alpha Formula) (Alpha Formula) (Alpha Formula) (Alpha Formula) (Alpha Formula) and Alpha Formula (Alpha Formula) the optimal solutions for problem (Alpha Formula) and Alpha Formula (Alpha Formula) are obtained when Alpha Formula (Alpha Formula) and Alpha Formula (Alpha Formula)."}, {"heading": "4 Probabilistic Interpretation", "text": "In this section, we show that the proposed multiplicative formalism with the maximum a = q = q = q = q = q = q = q = q = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = ="}, {"heading": "5 Optimization Algorithm", "text": "The alternative optimization algorithms were used in both early methods [2, 14] to solve the problem (1) alternating between solving two partial problems: solving for \u03b2t with fixed c; solving for c with fixed \u03b2t. The convergence property of such an alternative algorithm was analyzed in [2] that it converges to a local minimizer. However, both partial problems in the existing methods can only be solved with iterative algorithms used in our proof and we derive a closed formula for the second partial problem from this. We are designing a new alternative optimization algorithm that uses the property that both problems (1) and (2) are equivalent to problem (3) and from this we derive a closed solution to the second partial problem. The following theorem characterizes this result."}, {"heading": "6 Two New Formulations", "text": "The two existing methods discussed in [2, 14] use p = k in their formulations, whereby \u03b2tj and \u03b2 have the same degree of shrinkage. To explore other characteristic patterns between the tasks, we propose two new formulations in which p 6 = k. For the joint decisions of p and k, the relationship between optimal c and \u03b2 according to theorem 2 can be calculated and summarized in Table 1.1. If the majority of the characteristics are not relevant for any of the tasks, it requires a savings-inducing standard for c. However, within the relevant characteristics, many characteristics are divided between the tasks. In other words, the characteristics used in each task are not economical in relation to all the characteristics selected by c, which requires a non-economy-inducing standard for \u03b2. Therefore, we use \"1 standard for c and\" 2 standard for all \u03b2's in formulation (1).This formulation corresponds to the common regulation method for min."}, {"heading": "7 Experiments", "text": "In this section, we empirically evaluated the performance of the proposed multiplicative MTFL using the four parameter settings listed in Table 1 on synthetic and real data. [1] The first two settings (p, k) = (2, 2), (1, 1) give the same methods or in [2, 14], and the last two settings each correspond to our new formulations. The least square and logistical regression losses are each used for common tasks. We focus on understanding the shrinkage effects created by the different decisions of regulators in multiplicative MTFL groups. These methods are called MTFL and are compared with the dirty model (DMTL) [9] and robust MTFL (rMTFL) that uses additive decomposition. The first subproblem of the algorithms was solved using CPLEX and individual problems in the first problem."}, {"heading": "8 Conclusion", "text": "In this paper, we examine a general framework of multiplicative multi-task feature learning. By breaking down the model parameter of each task into a product of two components: the cross-task characteristic indicator and task-specific parameters and applying different regulatory mechanisms to the two components, we can select characteristics for individual tasks and also look for the common characteristics between the tasks. We have examined the theoretical properties of this framework when applying different regulatory mechanisms and found that this family of methods creates models that are equivalent to those of the common regulated MTL methods but have a more general form of regulation. Furthermore, an analytical formula for the cross-task component is derived in connection with the task-specific component, which sheds light on the different shrinkage effects in the different regulatory mechanisms, an efficient algorithm is derived to solve the entire family of methods, and our experiments clearly show that there are no empirical results other than specific ones."}, {"heading": "Acknowledgements", "text": "Jinbo Bi and her students Xin Wang and Jiangwen Sun were supported by NSF grants IIS-1320586, DBI-1356655, IIS-1407205 and IIS-1447711."}], "references": [{"title": "Multi-task feature learning", "author": ["A. Argyriou", "T. Evgeniou", "M. Pontil"], "venue": "In Proceedings of NIPS\u201907,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2007}, {"title": "An improved multi-task learning approach with applications in medical diagnosis", "author": ["J. Bi", "T. Xiong", "S. Yu", "M. Dundar", "R.B. Rao"], "venue": "In Proceedings of ECML\u201908,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2008}, {"title": "Integrating low-rank and group-sparse structures for robust multitask learning", "author": ["J. Chen", "J. Zhou", "J. Ye"], "venue": "In Proceedings of KDD\u201911,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2011}, {"title": "Regularized multi\u2013task learning", "author": ["T. Evgeniou", "M. Pontil"], "venue": "In Proceedings of KDD\u201904,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2004}, {"title": "Multi-stage multi-task feature learning", "author": ["P. Gong", "J. Ye", "C. Zhang"], "venue": "In Proceedings of NIPS\u201912,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2012}, {"title": "Robust multi-task feature learning", "author": ["P. Gong", "J. Ye", "C. Zhang"], "venue": "In Proceedings of KDD\u201912,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2012}, {"title": "Multivariate \u03b8-generalized normal distributions", "author": ["I.R. Goodman", "S. Kotz"], "venue": "Journal of Multivariate Analysis,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 1973}, {"title": "Clustered multi-task learning: a convex formulation", "author": ["L. Jacob", "B. Francis", "J.-P. Vert"], "venue": null, "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2008}, {"title": "A dirty model for multi-task learning", "author": ["A. Jalali", "S. Sanghavi", "C. Ruan", "P.K. Ravikumar"], "venue": "In Proceedings of NIPS\u201910,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2010}, {"title": "Learning with whom to share in multi-task feature learning", "author": ["Z. Kang", "K. Grauman", "F. Sha"], "venue": "In Proceedings of ICML\u201911,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2011}, {"title": "Learning task grouping and overlap in multi-task learning", "author": ["A. Kumar", "H. Daume III"], "venue": "In Proceedings of ICML\u201912,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2012}, {"title": "Adaptive multi-task lasso: with application to eQTL detection", "author": ["S. Lee", "J. Zhu", "E. Xing"], "venue": "In Proceedings of NIPS\u201910,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2010}, {"title": "Multi-task feature learning via efficient `1,2-norm minimization", "author": ["J. Liu", "S. Ji", "J. Ye"], "venue": "In Proceedings of UAI\u201909,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2009}, {"title": "Multi-level lasso for sparse multi-task regression", "author": ["A. Lozano", "G. Swirszcz"], "venue": "In Proceedings of ICML\u201912,", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2012}, {"title": "Multi-task feature selection", "author": ["G. Obozinski", "B. Taskar"], "venue": "Technical report, Statistics Department, UC Berkeley,", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2006}, {"title": "Flexible modeling of latent task structures in multitask learning", "author": ["A. Passos", "P. Rai", "J. Wainer", "H. Daume III"], "venue": "In Proceedings of ICML\u201912,", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2012}, {"title": "An efficient projection for l1,infinity regularization", "author": ["A. Quattoni", "X. Carreras", "M. Collins", "T. Darrell"], "venue": "In Proceedings of ICML\u201909,", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2009}, {"title": "Infinite predictor subspace models for multitask learning", "author": ["P. Rai", "H. Daume"], "venue": "In International Conference on Artificial Intelligence and Statistics,", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2010}, {"title": "Convergence of a block coordinate descent method for nondifferentiable minimization", "author": ["P. Tseng"], "venue": "Journal Optimization Theory Applications,", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2001}, {"title": "Simultaneous variable selection", "author": ["B.A. Turlach", "W.N. Wenables", "S.J. Wright"], "venue": null, "citeRegEx": "20", "shortCiteRegEx": "20", "year": 2005}, {"title": "Probabilistic joint feature selection for multi-task learning", "author": ["T. Xiong", "J. Bi", "B. Rao", "V. Cherkassky"], "venue": "In Proceedings of SIAM International Conference on Data Mining (SDM),", "citeRegEx": "21", "shortCiteRegEx": "21", "year": 2006}, {"title": "Probabilistic multi-task feature selection", "author": ["Y. Zhang", "D.-Y. Yeung", "Q. Xu"], "venue": "In Proceedings of NIPS\u201910,", "citeRegEx": "22", "shortCiteRegEx": "22", "year": 2010}, {"title": "Clustered multi-task learning via alternating structure optimization", "author": ["J. Zhou", "J. Chen", "J. Ye"], "venue": "In Proceedings of NIPS\u201911,", "citeRegEx": "23", "shortCiteRegEx": "23", "year": 2011}, {"title": "Exclusive lasso for multi-task feature selection", "author": ["Y. Zhou", "R. Jin", "S.C. Hoi"], "venue": "In Proceedings of UAI\u201910,", "citeRegEx": "24", "shortCiteRegEx": "24", "year": 2010}], "referenceMentions": [{"referenceID": 17, "context": "Either the task parameters can be projected to explore the latent common substructure [18], or a shared low-dimensional representation of data can be formed by feature learning [10].", "startOffset": 86, "endOffset": 90}, {"referenceID": 9, "context": "Either the task parameters can be projected to explore the latent common substructure [18], or a shared low-dimensional representation of data can be formed by feature learning [10].", "startOffset": 177, "endOffset": 181}, {"referenceID": 15, "context": "Recent methods either explore the latent basis that is used to develop the entire set of tasks, or learn how to group the tasks [16, 11], or identify if certain tasks are outliers to other tasks [6].", "startOffset": 128, "endOffset": 136}, {"referenceID": 10, "context": "Recent methods either explore the latent basis that is used to develop the entire set of tasks, or learn how to group the tasks [16, 11], or identify if certain tasks are outliers to other tasks [6].", "startOffset": 128, "endOffset": 136}, {"referenceID": 5, "context": "Recent methods either explore the latent basis that is used to develop the entire set of tasks, or learn how to group the tasks [16, 11], or identify if certain tasks are outliers to other tasks [6].", "startOffset": 195, "endOffset": 198}, {"referenceID": 11, "context": "These methods employ a regularizer based on the so-called `1,p matrix norm [12, 13, 15, 22, 24] that is the sum of the `p norms of the rows in a matrix.", "startOffset": 75, "endOffset": 95}, {"referenceID": 12, "context": "These methods employ a regularizer based on the so-called `1,p matrix norm [12, 13, 15, 22, 24] that is the sum of the `p norms of the rows in a matrix.", "startOffset": 75, "endOffset": 95}, {"referenceID": 14, "context": "These methods employ a regularizer based on the so-called `1,p matrix norm [12, 13, 15, 22, 24] that is the sum of the `p norms of the rows in a matrix.", "startOffset": 75, "endOffset": 95}, {"referenceID": 21, "context": "These methods employ a regularizer based on the so-called `1,p matrix norm [12, 13, 15, 22, 24] that is the sum of the `p norms of the rows in a matrix.", "startOffset": 75, "endOffset": 95}, {"referenceID": 23, "context": "These methods employ a regularizer based on the so-called `1,p matrix norm [12, 13, 15, 22, 24] that is the sum of the `p norms of the rows in a matrix.", "startOffset": 75, "endOffset": 95}, {"referenceID": 14, "context": "Typical choices for p are 2 [15, 4] and\u221e [20] which are used in the very early MTFL methods.", "startOffset": 28, "endOffset": 35}, {"referenceID": 3, "context": "Typical choices for p are 2 [15, 4] and\u221e [20] which are used in the very early MTFL methods.", "startOffset": 28, "endOffset": 35}, {"referenceID": 19, "context": "Typical choices for p are 2 [15, 4] and\u221e [20] which are used in the very early MTFL methods.", "startOffset": 41, "endOffset": 45}, {"referenceID": 12, "context": "Effective algorithms have since then been developed for the `1,2 [13] and `1,\u221e [17] regularization.", "startOffset": 65, "endOffset": 69}, {"referenceID": 16, "context": "Effective algorithms have since then been developed for the `1,2 [13] and `1,\u221e [17] regularization.", "startOffset": 79, "endOffset": 83}, {"referenceID": 21, "context": "Later, the `1,p norm is generalized to include 1 < p \u2264 \u221e with a probabilistic interpretation that the resultant MTFL method solves a relaxed optimization problem with a generalized normal prior for all tasks [22].", "startOffset": 208, "endOffset": 212}, {"referenceID": 4, "context": "Recent research applies the capped `1,1 norm as a nonconvex joint regularizer [5].", "startOffset": 78, "endOffset": 81}, {"referenceID": 8, "context": "To overcome this limitation, one of the most effective strategies is to decompose the model parameters into either summation [9, 3, 6] or multiplication [21, 2, 14] of two components with different regularizers applied to the two components.", "startOffset": 125, "endOffset": 134}, {"referenceID": 2, "context": "To overcome this limitation, one of the most effective strategies is to decompose the model parameters into either summation [9, 3, 6] or multiplication [21, 2, 14] of two components with different regularizers applied to the two components.", "startOffset": 125, "endOffset": 134}, {"referenceID": 5, "context": "To overcome this limitation, one of the most effective strategies is to decompose the model parameters into either summation [9, 3, 6] or multiplication [21, 2, 14] of two components with different regularizers applied to the two components.", "startOffset": 125, "endOffset": 134}, {"referenceID": 20, "context": "To overcome this limitation, one of the most effective strategies is to decompose the model parameters into either summation [9, 3, 6] or multiplication [21, 2, 14] of two components with different regularizers applied to the two components.", "startOffset": 153, "endOffset": 164}, {"referenceID": 1, "context": "To overcome this limitation, one of the most effective strategies is to decompose the model parameters into either summation [9, 3, 6] or multiplication [21, 2, 14] of two components with different regularizers applied to the two components.", "startOffset": 153, "endOffset": 164}, {"referenceID": 13, "context": "To overcome this limitation, one of the most effective strategies is to decompose the model parameters into either summation [9, 3, 6] or multiplication [21, 2, 14] of two components with different regularizers applied to the two components.", "startOffset": 153, "endOffset": 164}, {"referenceID": 8, "context": "Specifically, for the methods that decompose the parameter matrix into summation of two matrices, the dirty model in [9] employs `1,1 and `1,\u221e regularizers to the two components.", "startOffset": 117, "endOffset": 120}, {"referenceID": 2, "context": "A robust MTFL method in [3] uses the trace norm on one component for mining a low-rank structure shared by tasks and a column-wise `1,2-norm on the other component for identifying task outliers.", "startOffset": 24, "endOffset": 27}, {"referenceID": 5, "context": "Another method applies the `1,2-norm both row-wisely to one component and column-wisely to the other [6].", "startOffset": 101, "endOffset": 104}, {"referenceID": 1, "context": "These methods either use the `2-norm penalty on both of the component vectors [2], or the sparse `1-norm on the two components (i.", "startOffset": 78, "endOffset": 81}, {"referenceID": 13, "context": ", multi-level LASSO) [14].", "startOffset": 21, "endOffset": 25}, {"referenceID": 13, "context": "The multi-level LASSO method has been analytically compared to the dirty model [14], showing that the multiplicative decomposition creates better shrinkage on the global and task-specific parameters.", "startOffset": 79, "endOffset": 83}, {"referenceID": 1, "context": "This general form includes all early methods that represent model parameters as a product of two components [2, 14].", "startOffset": 108, "endOffset": 115}, {"referenceID": 13, "context": "This general form includes all early methods that represent model parameters as a product of two components [2, 14].", "startOffset": 108, "endOffset": 115}, {"referenceID": 1, "context": "Unlike the existing methods [2, 14] where the same kind of vector norm is applied to both components, the shrinkage of the global and task-specific parameters differ in the new formulations.", "startOffset": 28, "endOffset": 35}, {"referenceID": 13, "context": "Unlike the existing methods [2, 14] where the same kind of vector norm is applied to both components, the shrinkage of the global and task-specific parameters differ in the new formulations.", "startOffset": 28, "endOffset": 35}, {"referenceID": 1, "context": "In particular, if both p = k = 2, Problem (1) becomes the formulation in [2] and if p = k = 1, Problem (1) becomes the formulation in [14].", "startOffset": 73, "endOffset": 76}, {"referenceID": 13, "context": "In particular, if both p = k = 2, Problem (1) becomes the formulation in [2] and if p = k = 1, Problem (1) becomes the formulation in [14].", "startOffset": 134, "endOffset": 138}, {"referenceID": 14, "context": "In particular, if {p, q} = {2, 1} in Problem (2) as used in the methods of [15] and [1], the `2-norm regularizer is applied to \u03b1 .", "startOffset": 75, "endOffset": 79}, {"referenceID": 0, "context": "In particular, if {p, q} = {2, 1} in Problem (2) as used in the methods of [15] and [1], the `2-norm regularizer is applied to \u03b1 .", "startOffset": 84, "endOffset": 87}, {"referenceID": 1, "context": "Then, this problem is equivalent to Problem (1) when k = 2 and \u03bb = 2 \u221a \u03b31\u03b32, the same formulation in [2].", "startOffset": 101, "endOffset": 104}, {"referenceID": 13, "context": "Our theorem 1 shows that this problem is equivalent to the multi-level LASSO MTFL formulation [14] which is Problem (1) with k = 1 and \u03bb = 2 \u221a \u03b31\u03b32.", "startOffset": 94, "endOffset": 98}, {"referenceID": 6, "context": "Denote z \u223c GN (\u03bc, \u03c1, q) the univariate generalized normal distribution, with the density function p(z) = 1 2\u03c1\u0393(1+1/q) exp(\u2212 |z\u2212\u03bc| \u03c1q ), in which \u03c1 > 0, q > 0, and \u0393(\u00b7) is the Gamma function [7].", "startOffset": 190, "endOffset": 193}, {"referenceID": 21, "context": "assumption, the prior takes the form (also refer to [22] for a similar treatment)", "startOffset": 52, "endOffset": 56}, {"referenceID": 1, "context": "Alternating optimization algorithms have been used in both of the early methods [2, 14] to solve Problem (1) which alternate between solving two subproblems: solve for \u03b2t with fixed c; solve for c with fixed \u03b2t.", "startOffset": 80, "endOffset": 87}, {"referenceID": 13, "context": "Alternating optimization algorithms have been used in both of the early methods [2, 14] to solve Problem (1) which alternate between solving two subproblems: solve for \u03b2t with fixed c; solve for c with fixed \u03b2t.", "startOffset": 80, "endOffset": 87}, {"referenceID": 1, "context": "The convergence property of such an alternating algorithm has been analyzed in [2] that it converges to a local minimizer.", "startOffset": 79, "endOffset": 82}, {"referenceID": 18, "context": "When convex and differentiable losses are used, theoretical results in [19] can be used to prove the convergence of the proposed algorithm.", "startOffset": 71, "endOffset": 75}, {"referenceID": 1, "context": "The two existing methods discussed in [2, 14] use p = k in their formulations, which renders \u03b2 j and cj the same amount of shrinkage.", "startOffset": 38, "endOffset": 45}, {"referenceID": 13, "context": "The two existing methods discussed in [2, 14] use p = k in their formulations, which renders \u03b2 j and cj the same amount of shrinkage.", "startOffset": 38, "endOffset": 45}, {"referenceID": 1, "context": "The first two settings (p, k) = (2, 2), (1, 1) give the same methods respectively in [2, 14], and the last two settings correspond to our new formulations.", "startOffset": 85, "endOffset": 92}, {"referenceID": 13, "context": "The first two settings (p, k) = (2, 2), (1, 1) give the same methods respectively in [2, 14], and the last two settings correspond to our new formulations.", "startOffset": 85, "endOffset": 92}, {"referenceID": 8, "context": "These methods are referred to as MMTFL and are compared with the dirty model (DMTL) [9] and robust MTFL (rMTFL) [6] that use the additive decomposition.", "startOffset": 84, "endOffset": 87}, {"referenceID": 5, "context": "These methods are referred to as MMTFL and are compared with the dirty model (DMTL) [9] and robust MTFL (rMTFL) [6] that use the additive decomposition.", "startOffset": 112, "endOffset": 115}, {"referenceID": 9, "context": "Note that the feature sharing patterns may not be revealed by the recent methods on clustered multitask learning that cluster tasks into groups [10, 8, 23] because no cluster structure is present in Figure 1b, for instance.", "startOffset": 144, "endOffset": 155}, {"referenceID": 7, "context": "Note that the feature sharing patterns may not be revealed by the recent methods on clustered multitask learning that cluster tasks into groups [10, 8, 23] because no cluster structure is present in Figure 1b, for instance.", "startOffset": 144, "endOffset": 155}, {"referenceID": 22, "context": "Note that the feature sharing patterns may not be revealed by the recent methods on clustered multitask learning that cluster tasks into groups [10, 8, 23] because no cluster structure is present in Figure 1b, for instance.", "startOffset": 144, "endOffset": 155}, {"referenceID": 0, "context": "Two benchmark data sets, the Sarcos [1] and the USPS data sets [10], were used for regression and classification tests respectively.", "startOffset": 36, "endOffset": 39}, {"referenceID": 9, "context": "Two benchmark data sets, the Sarcos [1] and the USPS data sets [10], were used for regression and classification tests respectively.", "startOffset": 63, "endOffset": 67}], "year": 2016, "abstractText": "We investigate a general framework of multiplicative multitask feature learning which decomposes each task\u2019s model parameters into a multiplication of two components. One of the components is used across all tasks and the other component is task-specific. Several previous methods have been proposed as special cases of our framework. We study the theoretical properties of this framework when different regularization conditions are applied to the two decomposed components. We prove that this framework is mathematically equivalent to the widely used multitask feature learning methods that are based on a joint regularization of all model parameters, but with a more general form of regularizers. Further, an analytical formula is derived for the across-task component as related to the taskspecific component for all these regularizers, leading to a better understanding of the shrinkage effect. Study of this framework motivates new multitask learning algorithms. We propose two new learning formulations by varying the parameters in the proposed framework. Empirical studies have revealed the relative advantages of the two new formulations by comparing with the state of the art, which provides instructive insights into the feature learning problem with multiple tasks.", "creator": "LaTeX with hyperref package"}}}