{"id": "1610.07584", "review": {"conference": "NIPS", "VERSION": "v1", "DATE_OF_SUBMISSION": "24-Oct-2016", "title": "Learning a Probabilistic Latent Space of Object Shapes via 3D Generative-Adversarial Modeling", "abstract": "We study the problem of 3D object generation. We propose a novel framework, namely 3D Generative Adversarial Network (3D-GAN), which generates 3D objects from a probabilistic space by leveraging recent advances in volumetric convolutional networks and generative adversarial nets. The benefits of our model are three-fold: first, the use of an adversarial criterion, instead of traditional heuristic criteria, enables the generator to capture object structure implicitly and to synthesize high-quality 3D objects; second, the generator establishes a mapping from a low-dimensional probabilistic space to the space of 3D objects, so that we can sample objects without a reference image or CAD models, and explore the 3D object manifold; third, the adversarial discriminator provides a powerful 3D shape descriptor which, learned without supervision, has wide applications in 3D object recognition. Experiments demonstrate that our method generates high-quality 3D objects, and our unsupervisedly learned features achieve impressive performance on 3D object recognition, comparable with those of supervised learning methods.", "histories": [["v1", "Mon, 24 Oct 2016 19:53:41 GMT  (8667kb,D)", "http://arxiv.org/abs/1610.07584v1", "NIPS 2016. The first two authors contributed equally to this work"], ["v2", "Wed, 4 Jan 2017 18:35:52 GMT  (8669kb,D)", "http://arxiv.org/abs/1610.07584v2", "NIPS 2016. The first two authors contributed equally to this work"]], "COMMENTS": "NIPS 2016. The first two authors contributed equally to this work", "reviews": [], "SUBJECTS": "cs.CV cs.LG", "authors": ["jiajun wu 0001", "chengkai zhang", "tianfan xue", "bill freeman", "josh tenenbaum"], "accepted": true, "id": "1610.07584"}, "pdf": {"name": "1610.07584.pdf", "metadata": {"source": "CRF", "title": "Learning a Probabilistic Latent Space of Object Shapes via 3D Generative-Adversarial Modeling", "authors": ["Jiajun Wu", "Chengkai Zhang", "Tianfan Xue", "William T. Freeman", "Joshua B. Tenenbaum"], "emails": ["jbt}@mit.edu"], "sections": [{"heading": "1 Introduction", "text": "In fact, the question is that most of them are able to survive themselves if they do not find themselves able to survive themselves, \"he told the German Press Agency in an interview with\" Welt am Sonntag, \"in which he dealt with the question:\" What is that? \"The question is only:\" What is that actually? \"The question is only:\" What is that? \"The question is only:\" What is that actually? \"The question is:\" What is that actually? \"The question is:\" What is that actually? \"The question is:\" What is that? \"The answer?\" The question \"The question\" The question \"The question\" The question \"The question\" The question \"The question\" The question \"The question\" The question \"The question\" The question \"The question\" The question \"The question\" The question \"The question\" The question \"The question\" The question \"The question\" The question \"The question\" The question \"The question\" The question \"The question\" The question \"The question\" The question \"The question\" The question \"The question\" The question \"The question\" The question \"The question\" The question \"The question\" The question \"The question\" The question \"The question\" The question \"The question\" The question \"The question\" The question \"The question\" The question \"The question\" The question \"The question\" The question \"The question\" The question \"The question\" The question \"The question\" The question \"The question\" The question \"The question\" The question \"The question\" The question \"The question\" The question \"The question\" The question \"The question\" The question \"The question\" The question \"The question\" The question \"The question\" The question \"The question\" The question \"The question\" The question \"The question\" The question \"The question\" The question \"The question\" The question \"The question\" The question \"The question\" The question \"The question\" The question \"The question\" The question \"The question\" The question \"The question\" The question \""}, {"heading": "2 Related Work", "text": "In this context, it is important that we see ourselves in a position to show that we are able to change the world, by changing the world, by changing the world, by changing the world, by changing it, by changing it, by changing it, by changing it, by changing it, by changing it, by changing it, by changing it, by changing it, by changing it, by changing it, by changing it, by changing it, by changing it, by changing it, by changing it, by changing it, by changing it, by changing it, by changing it, by changing it, by changing it, by changing it, by changing it, by changing it, by changing it, by changing it, by changing it, by changing it, by changing it, by changing it, by changing it, by changing it, by changing it, by changing it, by changing it, by changing it."}, {"heading": "3 Models", "text": "In this section, we present our model for 3D object generation. First, we discuss how to build our framework, 3D Generative Adversarial Network (3D-GAN), using previous advances in volumetric coil networks and generative opposing networks. Then, we show how to simultaneously train a variational autoencoder [Kingma and Welling, 2014] so that our framework can capture a mapping from a 2D image to a 3D object."}, {"heading": "3.1 3D Generative Adversarial Network (3D-GAN)", "text": "As suggested in Goodfellow et al. [2014], the Generative Adversarial Network (GAN) consists of a generator and a discriminator, where the discriminator tries to classify real objects and objects synthesized by the generator, and the generator tries to confuse the discriminator. In our 3D Generative Adversarial Network (3D-GAN), generator G maps a 200-dimensional latent vector z that is randomly sampled from a probable latent space to represent an object G (z) in the 3D vocal space. discriminator D gives a confidence value that shows whether a 3D object x is real or synthetic. [2014] we use binary cross-entropy as a classification loss and present our general addressable loss function asL3D-GAN = logD (x) + log."}, {"heading": "3.2 3D-VAE-GAN", "text": "We discussed how to create 3D objects by scanning a latent vector z and mapping it to the object space. In practice, it would also be helpful to derive these latent vectors from observations. For example, if there is a mapping of a 2D image onto the latent image, we can then restore the 3D object that corresponds to this 2D image. Following this idea, we present 3D-VAE-GAN as an extension to 3D-GAN. Let's add an additional image encoder E that requires a 2D image x as input and outputs of the latent representation vector z (3D image), which is proposed by VAE-GAN as an extension to 3D-GAN, 2016] that connects VAE and GAN by connecting the VAE decoder with the GAN generator. The 3D-VAE-GAN therefore consists of three components: an image encoder, a 3D decoder, a 3D-GAN decoder, and a GAN-GAN divider."}, {"heading": "4 Evaluation", "text": "In this section, we evaluate our framework from various points of view. First, we show qualitative results of generated 3D objects, then we evaluate the unattended learned representation by the discriminator, using it as features for 3D object classification, showing both qualitative and quantitative results on the popular benchmark ModelNet [Wu et al., 2015], evaluating our 3D-VAE-GAN for 3D object reconstruction from a single image, and showing both qualitative and quantitative results on the IKEA dataset [Lim et al., 2013]."}, {"heading": "4.1 3D Object Generation", "text": "Figure 2 shows 3D objects generated by our 3D-GAN. For this experiment, we train a 3D-GAN for each object category. For generating random 200-dimensional vectors that follow an even distribution over [0, 1], and render the largest connected component of each created object. We compare 3D-GAN with Wu et al. [2015], the state of the art in 3D object synthesis from a probabilistic space and with a volumetric autoencoder whose variants have been used by several newer methods [Girdhar et al., 2016, Sharma et al., 2016]. Since an auto-encoder does not restrict the distribution of its latent representation, we calculate the empirical distribution p0 (z) of the vector z of all training examples representing a Gaussian distribution g0 to p0, and the sample from g0. Our algorithm produces 3D objects with much higher quality and finer detail."}, {"heading": "4.2 3D Object Classification", "text": "We then evaluate the representations we learned through our discriminator. A typical way of evaluating representations we learned without supervision is to use them as attributes for classification. To obtain attributes for an input 3D object, we link the responses of the second, third and fourth folding layers in the discriminator and apply the maximum pooling of core sizes {8, 4, 2}, respectively. We use a linear SVM for classification. Data We train a single 3D GAN on the seven major object categories (chairs, sofas, tables, boats, airplanes, guns and cars) of ShapeNet [Chang et al., 2015]. We use ModelNet et al al., 2015] for # objects per class in training 10 40 80 160 fullA ccur (%) 6570758083D-GAN VoxNet VConv-DAEFigure."}, {"heading": "4.3 Single Image 3D Reconstruction", "text": "As an application, we show that the 3D-UAE-GAN performs well in 3D reconstruction of individual images. After previous work [Girdhar et al., 2016], we test it on the IKEA dataset [Lim et al., 2013] and show both qualitative and quantitative results. The IKEA dataset consists of images with IKEA objects. We crop the images so that the objects in the images are centered. Our test set consists of 1, 039 objects cropped out of 759 images (provided by the author). The IKEA dataset is challenging because all images are captured in the CCA dataset, there are two typically used tension / test columns. Qi et al. [2016], Shi et al. [2015], Maturana and Scherer [2015] used the tension / test gap in the dataset we follow; Wu et al."}, {"heading": "5 Analyzing Learned Representations", "text": "In this section, we will delve deeply into the representations that both the generator and the discriminator have learned from 3D-GAN. We will start with the 200-dimensional object vector from which the generator generates various objects. We will then visualize neurons in the discriminator and show that these units capture informative semantic knowledge about the objects, which justifies their good performance in the object classification described in Section 4."}, {"heading": "5.1 The Generative Representation", "text": "We examine three methods for understanding the latent space of vectors for object creation. First, we visualize what represents an individual dimension of the vector; then we explore the possibility of interpolation between two object vectors and observe how the generated objects change; finally, we present how to apply formmarithmetics in latent space. Visualization of the object vector In order to visualize the semantic meaning of each dimension, we gradually increase its value and observe how it affects the generated 3D object. In Figure 5, each column corresponds to one dimension of the object vector, with the red region marking the voxels affected by variable values of this dimension. We observe that some dimensions in the object vector carry semantic knowledge about the object, e.g. the thickness or width of the surfaces. Interpolation We show results of interpolation between two object vectors in Figure 6. Previous work showed the interpolation between two 2D images of the same category [quitskiovy] that traverse the object."}, {"heading": "5.2 The Discriminative Representation", "text": "Specifically, we want to show which input objects and which part of them generate the highest intensity values for each neuron. To do this, we traverse all the training objects for each neuron in the penultimate coil of the discriminator and show those that most activate the unit. To visualize the parts that trigger the activation, we continue to use guided reverse propagation [Springenberg et al., 2015]. Figure 9 shows the results. There are two main observations: First, the objects that trigger the strongest activation exhibit very similar shapes, showing that the neuron is selective in terms of the entire object shape; second, the parts that activate the neuron are shown in red, consistent across these objects, suggesting that the neuron also learns semantic knowledge about object parts."}, {"heading": "6 Conclusion", "text": "In this article, we proposed 3D-GAN for 3D object generation and 3D-UAE-GAN for learning an image for 3D model mapping. We demonstrated that our models are capable of creating novel objects and reconstructing 3D objects from images. We demonstrated that the discriminator in GAN, which was learned without supervision, can be used as an informative feature representation for 3D objects and provided impressive performance in form classification. We also examined the latent space of object vectors and presented results on object interpolation, formarithmetics and neuron visualization. Recognition This work is supported by NSF grants # 1212849 and # 1447476, ONR MURI N00014-16-1-2007, the Center for Brain, Minds and (NSF STC award CCF-1231216), Toyota Research Institute, Adobe Research, Shell Research and a hardware donation from Nvidia."}], "references": [{"title": "A morphable model for the synthesis of 3d faces", "author": ["Volker Blanz", "Thomas Vetter"], "venue": "In SIGGRAPH,", "citeRegEx": "Blanz and Vetter.,? \\Q1999\\E", "shortCiteRegEx": "Blanz and Vetter.", "year": 1999}, {"title": "Shapenet: An information-rich 3d model repository", "author": ["Angel X Chang", "Thomas Funkhouser", "Leonidas Guibas"], "venue": null, "citeRegEx": "Chang et al\\.,? \\Q1982\\E", "shortCiteRegEx": "Chang et al\\.", "year": 1982}, {"title": "3d-r2n2: A unified approach", "author": ["Christopher B Choy", "Danfei Xu", "JunYoung Gwak", "Kevin Chen", "Silvio Savarese"], "venue": "CGF,", "citeRegEx": "Choy et al\\.,? \\Q2003\\E", "shortCiteRegEx": "Choy et al\\.", "year": 2003}, {"title": "multi-view 3d object reconstruction", "author": ["Emily L Denton", "Soumith Chintala", "Rob Fergus"], "venue": "In ECCV,", "citeRegEx": "Denton et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Denton et al\\.", "year": 2016}, {"title": "Learning a predictable and generative", "author": ["Rohit Girdhar", "David F Fouhey", "Mikel Rodriguez", "Abhinav Gupta"], "venue": null, "citeRegEx": "Girdhar et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Girdhar et al\\.", "year": 2015}, {"title": "Generative adversarial nets", "author": ["Courville", "Yoshua Bengio"], "venue": "In NIPS,", "citeRegEx": "Courville and Bengio.,? \\Q2014\\E", "shortCiteRegEx": "Courville and Bengio.", "year": 2014}, {"title": "Analysis and synthesis of 3d shape families", "author": ["Haibin Huang", "Evangelos Kalogerakis", "Benjamin Marlin"], "venue": null, "citeRegEx": "Huang et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Huang et al\\.", "year": 2016}, {"title": "deep-learned generative models of surfaces. CGF", "author": ["Daniel Jiwoong Im", "Chris Dongjoo Kim", "Hui Jiang", "Roland Memisevic"], "venue": null, "citeRegEx": "Im et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Im et al\\.", "year": 2015}, {"title": "A probabilistic model for", "author": ["Evangelos Kalogerakis", "Siddhartha Chaudhuri", "Daphne Koller", "Vladlen Koltun"], "venue": "arXiv preprint arXiv:1602.05110,", "citeRegEx": "Kalogerakis et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Kalogerakis et al\\.", "year": 2016}, {"title": "component-based shape synthesis", "author": ["Abhishek Kar", "Shubham Tulsiani", "Joao Carreira", "Jitendra Malik"], "venue": "ACM TOG,", "citeRegEx": "Kar et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Kar et al\\.", "year": 2012}, {"title": "Rotation invariant spherical harmonic", "author": ["Michael Kazhdan", "Thomas Funkhouser", "Szymon Rusinkiewicz"], "venue": null, "citeRegEx": "Kazhdan et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Kazhdan et al\\.", "year": 2015}, {"title": "Adam: A method for stochastic optimization", "author": ["Diederik P Kingma", "Jimmy Ba"], "venue": "SGP,", "citeRegEx": "Kingma and Ba.,? \\Q2003\\E", "shortCiteRegEx": "Kingma and Ba.", "year": 2003}, {"title": "Precomputed real-time texture synthesis with markovian generative adversarial", "author": ["Chuan Li", "Michael Wand"], "venue": null, "citeRegEx": "Li and Wand.,? \\Q2016\\E", "shortCiteRegEx": "Li and Wand.", "year": 2016}, {"title": "embeddings of shapes and images via cnn image purification", "author": ["Joseph J. Lim", "Hamed Pirsiavash", "Antonio Torralba"], "venue": "ACM TOG,", "citeRegEx": "Lim et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Lim et al\\.", "year": 2015}, {"title": "Voxnet: A 3d convolutional neural network for real-time object", "author": ["Daniel Maturana", "Sebastian Scherer"], "venue": null, "citeRegEx": "Maturana and Scherer.,? \\Q2013\\E", "shortCiteRegEx": "Maturana and Scherer.", "year": 2013}, {"title": "multi-view cnns for object classification on 3d data", "author": ["Alec Radford", "Luke Metz", "Soumith Chintala"], "venue": "In CVPR, 2016", "citeRegEx": "Radford et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Radford et al\\.", "year": 2016}, {"title": "Orientation-boosted voxel nets for 3d object", "author": ["Nima Sedaghat", "Mohammadreza Zolfaghari", "Thomas Brox"], "venue": "ICLR,", "citeRegEx": "Sedaghat et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Sedaghat et al\\.", "year": 2016}, {"title": "Vconv-dae: Deep volumetric shape learning without object", "author": ["Abhishek Sharma", "Oliver Grau", "Mario Fritz"], "venue": "recognition. arXiv preprint arXiv:1604.03351,", "citeRegEx": "Sharma et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Sharma et al\\.", "year": 2016}, {"title": "Deeppano: Deep panoramic representation for 3-d", "author": ["Baoguang Shi", "Song Bai", "Zhichao Zhou", "Xiang Bai"], "venue": "labels. arXiv preprint arXiv:1604.03755,", "citeRegEx": "Shi et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Shi et al\\.", "year": 2016}, {"title": "Striving for simplicity", "author": ["Jost Tobias Springenberg", "Alexey Dosovitskiy", "Thomas Brox", "Martin Riedmiller"], "venue": "IEEE SPL,", "citeRegEx": "Springenberg et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Springenberg et al\\.", "year": 2015}, {"title": "The all convolutional net", "author": ["Hang Su", "Subhransu Maji", "Evangelos Kalogerakis", "Erik Learned-Miller"], "venue": "In ICLR Workshop,", "citeRegEx": "Su et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Su et al\\.", "year": 2015}, {"title": "Render for cnn: Viewpoint estimation in images", "author": ["Hao Su", "Charles R Qi", "Yangyan Li", "Leonidas Guibas"], "venue": "ICCV, 2015a", "citeRegEx": "Su et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Su et al\\.", "year": 2015}, {"title": "cnns trained with rendered 3d model views. In ICCV, 2015b", "author": ["Johan WH Tangelder", "Remco C Veltkamp"], "venue": null, "citeRegEx": "Tangelder and Veltkamp.,? \\Q2015\\E", "shortCiteRegEx": "Tangelder and Veltkamp.", "year": 2015}, {"title": "A survey on shape correspondence", "author": ["Oliver Van Kaick", "Hao Zhang", "Ghassan Hamarneh", "Daniel Cohen-Or"], "venue": null, "citeRegEx": "Kaick et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Kaick et al\\.", "year": 2008}, {"title": "Generative image modeling using style and structure adversarial networks", "author": ["Xiaolong Wang", "Abhinav Gupta"], "venue": "CGF,", "citeRegEx": "Wang and Gupta.,? \\Q2011\\E", "shortCiteRegEx": "Wang and Gupta.", "year": 2011}, {"title": "Single image 3d interpreter network. In ECCV, 2016", "author": ["Zhirong Wu", "Shuran Song", "Aditya Khosla", "Fisher Yu", "Linguang Zhang", "Xiaoou Tang", "Jianxiong Xiao"], "venue": null, "citeRegEx": "Wu et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Wu et al\\.", "year": 2016}, {"title": "shapenets: A deep representation for volumetric shapes", "author": ["Yu Xiang", "Wongun Choi", "Yuanqing Lin", "Silvio Savarese"], "venue": "In CVPR,", "citeRegEx": "Xiang et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Xiang et al\\.", "year": 2015}, {"title": "scene recognition from abbey to zoo", "author": ["Jun-Yan Zhu", "Philipp Kr\u00e4henb\u00fchl", "Eli Shechtman", "Alexei A Efros"], "venue": "In CVPR,", "citeRegEx": "Zhu et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Zhu et al\\.", "year": 2010}], "referenceMentions": [{"referenceID": 15, "context": ", 2015] and DC-GAN [Radford et al., 2016] adopted GAN with convolutional networks for image synthesis, and achieved impressive performance.", "startOffset": 19, "endOffset": 41}, {"referenceID": 0, "context": ", 2011, Blanz and Vetter, 1999, Kalogerakis et al., 2012, Chaudhuri et al., 2011, Kar et al., 2015, Bansal et al., 2016, Wu et al., 2016]. Since decades ago, AI and vision researchers have made inspiring attempts to design or learn 3D object representations, mostly based on meshes and skeletons. Many of these shape synthesis algorithms are nonparametric and they synthesize new objects by retrieving and combining shapes and parts from a database. Recently, Huang et al. [2015] explored generating 3D shapes with pre-trained templates and producing both object structure and surface geometry.", "startOffset": 8, "endOffset": 480}, {"referenceID": 0, "context": ", 2011, Blanz and Vetter, 1999, Kalogerakis et al., 2012, Chaudhuri et al., 2011, Kar et al., 2015, Bansal et al., 2016, Wu et al., 2016]. Since decades ago, AI and vision researchers have made inspiring attempts to design or learn 3D object representations, mostly based on meshes and skeletons. Many of these shape synthesis algorithms are nonparametric and they synthesize new objects by retrieving and combining shapes and parts from a database. Recently, Huang et al. [2015] explored generating 3D shapes with pre-trained templates and producing both object structure and surface geometry. Our framework synthesizes objects without explicitly borrow parts from a repository, and requires no supervision during training. Deep learning for 3D data The vision community have witnessed rapid development of deep networks for various tasks [He et al., 2016]. In the field of 3D object recognition, Li et al. [2015], Su et al.", "startOffset": 8, "endOffset": 915}, {"referenceID": 0, "context": ", 2011, Blanz and Vetter, 1999, Kalogerakis et al., 2012, Chaudhuri et al., 2011, Kar et al., 2015, Bansal et al., 2016, Wu et al., 2016]. Since decades ago, AI and vision researchers have made inspiring attempts to design or learn 3D object representations, mostly based on meshes and skeletons. Many of these shape synthesis algorithms are nonparametric and they synthesize new objects by retrieving and combining shapes and parts from a database. Recently, Huang et al. [2015] explored generating 3D shapes with pre-trained templates and producing both object structure and surface geometry. Our framework synthesizes objects without explicitly borrow parts from a repository, and requires no supervision during training. Deep learning for 3D data The vision community have witnessed rapid development of deep networks for various tasks [He et al., 2016]. In the field of 3D object recognition, Li et al. [2015], Su et al. [2015b], Girdhar et al.", "startOffset": 8, "endOffset": 934}, {"referenceID": 0, "context": ", 2011, Blanz and Vetter, 1999, Kalogerakis et al., 2012, Chaudhuri et al., 2011, Kar et al., 2015, Bansal et al., 2016, Wu et al., 2016]. Since decades ago, AI and vision researchers have made inspiring attempts to design or learn 3D object representations, mostly based on meshes and skeletons. Many of these shape synthesis algorithms are nonparametric and they synthesize new objects by retrieving and combining shapes and parts from a database. Recently, Huang et al. [2015] explored generating 3D shapes with pre-trained templates and producing both object structure and surface geometry. Our framework synthesizes objects without explicitly borrow parts from a repository, and requires no supervision during training. Deep learning for 3D data The vision community have witnessed rapid development of deep networks for various tasks [He et al., 2016]. In the field of 3D object recognition, Li et al. [2015], Su et al. [2015b], Girdhar et al. [2016] proposed to learn a joint embedding of 3D shapes and synthesized images, Su et al.", "startOffset": 8, "endOffset": 957}, {"referenceID": 0, "context": ", 2011, Blanz and Vetter, 1999, Kalogerakis et al., 2012, Chaudhuri et al., 2011, Kar et al., 2015, Bansal et al., 2016, Wu et al., 2016]. Since decades ago, AI and vision researchers have made inspiring attempts to design or learn 3D object representations, mostly based on meshes and skeletons. Many of these shape synthesis algorithms are nonparametric and they synthesize new objects by retrieving and combining shapes and parts from a database. Recently, Huang et al. [2015] explored generating 3D shapes with pre-trained templates and producing both object structure and surface geometry. Our framework synthesizes objects without explicitly borrow parts from a repository, and requires no supervision during training. Deep learning for 3D data The vision community have witnessed rapid development of deep networks for various tasks [He et al., 2016]. In the field of 3D object recognition, Li et al. [2015], Su et al. [2015b], Girdhar et al. [2016] proposed to learn a joint embedding of 3D shapes and synthesized images, Su et al. [2015a], Qi et al.", "startOffset": 8, "endOffset": 1048}, {"referenceID": 0, "context": ", 2011, Blanz and Vetter, 1999, Kalogerakis et al., 2012, Chaudhuri et al., 2011, Kar et al., 2015, Bansal et al., 2016, Wu et al., 2016]. Since decades ago, AI and vision researchers have made inspiring attempts to design or learn 3D object representations, mostly based on meshes and skeletons. Many of these shape synthesis algorithms are nonparametric and they synthesize new objects by retrieving and combining shapes and parts from a database. Recently, Huang et al. [2015] explored generating 3D shapes with pre-trained templates and producing both object structure and surface geometry. Our framework synthesizes objects without explicitly borrow parts from a repository, and requires no supervision during training. Deep learning for 3D data The vision community have witnessed rapid development of deep networks for various tasks [He et al., 2016]. In the field of 3D object recognition, Li et al. [2015], Su et al. [2015b], Girdhar et al. [2016] proposed to learn a joint embedding of 3D shapes and synthesized images, Su et al. [2015a], Qi et al. [2016] focused on learning discriminative representations for 3D object recognition, Xiang et al.", "startOffset": 8, "endOffset": 1066}, {"referenceID": 0, "context": ", 2011, Blanz and Vetter, 1999, Kalogerakis et al., 2012, Chaudhuri et al., 2011, Kar et al., 2015, Bansal et al., 2016, Wu et al., 2016]. Since decades ago, AI and vision researchers have made inspiring attempts to design or learn 3D object representations, mostly based on meshes and skeletons. Many of these shape synthesis algorithms are nonparametric and they synthesize new objects by retrieving and combining shapes and parts from a database. Recently, Huang et al. [2015] explored generating 3D shapes with pre-trained templates and producing both object structure and surface geometry. Our framework synthesizes objects without explicitly borrow parts from a repository, and requires no supervision during training. Deep learning for 3D data The vision community have witnessed rapid development of deep networks for various tasks [He et al., 2016]. In the field of 3D object recognition, Li et al. [2015], Su et al. [2015b], Girdhar et al. [2016] proposed to learn a joint embedding of 3D shapes and synthesized images, Su et al. [2015a], Qi et al. [2016] focused on learning discriminative representations for 3D object recognition, Xiang et al. [2015], Choy et al.", "startOffset": 8, "endOffset": 1164}, {"referenceID": 0, "context": ", 2011, Blanz and Vetter, 1999, Kalogerakis et al., 2012, Chaudhuri et al., 2011, Kar et al., 2015, Bansal et al., 2016, Wu et al., 2016]. Since decades ago, AI and vision researchers have made inspiring attempts to design or learn 3D object representations, mostly based on meshes and skeletons. Many of these shape synthesis algorithms are nonparametric and they synthesize new objects by retrieving and combining shapes and parts from a database. Recently, Huang et al. [2015] explored generating 3D shapes with pre-trained templates and producing both object structure and surface geometry. Our framework synthesizes objects without explicitly borrow parts from a repository, and requires no supervision during training. Deep learning for 3D data The vision community have witnessed rapid development of deep networks for various tasks [He et al., 2016]. In the field of 3D object recognition, Li et al. [2015], Su et al. [2015b], Girdhar et al. [2016] proposed to learn a joint embedding of 3D shapes and synthesized images, Su et al. [2015a], Qi et al. [2016] focused on learning discriminative representations for 3D object recognition, Xiang et al. [2015], Choy et al. [2016] discussed 3D object reconstruction from in-the-wild images, possibly with a recurrent network, and Girdhar et al.", "startOffset": 8, "endOffset": 1184}, {"referenceID": 0, "context": ", 2011, Blanz and Vetter, 1999, Kalogerakis et al., 2012, Chaudhuri et al., 2011, Kar et al., 2015, Bansal et al., 2016, Wu et al., 2016]. Since decades ago, AI and vision researchers have made inspiring attempts to design or learn 3D object representations, mostly based on meshes and skeletons. Many of these shape synthesis algorithms are nonparametric and they synthesize new objects by retrieving and combining shapes and parts from a database. Recently, Huang et al. [2015] explored generating 3D shapes with pre-trained templates and producing both object structure and surface geometry. Our framework synthesizes objects without explicitly borrow parts from a repository, and requires no supervision during training. Deep learning for 3D data The vision community have witnessed rapid development of deep networks for various tasks [He et al., 2016]. In the field of 3D object recognition, Li et al. [2015], Su et al. [2015b], Girdhar et al. [2016] proposed to learn a joint embedding of 3D shapes and synthesized images, Su et al. [2015a], Qi et al. [2016] focused on learning discriminative representations for 3D object recognition, Xiang et al. [2015], Choy et al. [2016] discussed 3D object reconstruction from in-the-wild images, possibly with a recurrent network, and Girdhar et al. [2016], Sharma et al.", "startOffset": 8, "endOffset": 1305}, {"referenceID": 0, "context": ", 2011, Blanz and Vetter, 1999, Kalogerakis et al., 2012, Chaudhuri et al., 2011, Kar et al., 2015, Bansal et al., 2016, Wu et al., 2016]. Since decades ago, AI and vision researchers have made inspiring attempts to design or learn 3D object representations, mostly based on meshes and skeletons. Many of these shape synthesis algorithms are nonparametric and they synthesize new objects by retrieving and combining shapes and parts from a database. Recently, Huang et al. [2015] explored generating 3D shapes with pre-trained templates and producing both object structure and surface geometry. Our framework synthesizes objects without explicitly borrow parts from a repository, and requires no supervision during training. Deep learning for 3D data The vision community have witnessed rapid development of deep networks for various tasks [He et al., 2016]. In the field of 3D object recognition, Li et al. [2015], Su et al. [2015b], Girdhar et al. [2016] proposed to learn a joint embedding of 3D shapes and synthesized images, Su et al. [2015a], Qi et al. [2016] focused on learning discriminative representations for 3D object recognition, Xiang et al. [2015], Choy et al. [2016] discussed 3D object reconstruction from in-the-wild images, possibly with a recurrent network, and Girdhar et al. [2016], Sharma et al. [2016] explored autoencoder-based networks for learning voxel-based object representations.", "startOffset": 8, "endOffset": 1327}, {"referenceID": 0, "context": ", 2011, Blanz and Vetter, 1999, Kalogerakis et al., 2012, Chaudhuri et al., 2011, Kar et al., 2015, Bansal et al., 2016, Wu et al., 2016]. Since decades ago, AI and vision researchers have made inspiring attempts to design or learn 3D object representations, mostly based on meshes and skeletons. Many of these shape synthesis algorithms are nonparametric and they synthesize new objects by retrieving and combining shapes and parts from a database. Recently, Huang et al. [2015] explored generating 3D shapes with pre-trained templates and producing both object structure and surface geometry. Our framework synthesizes objects without explicitly borrow parts from a repository, and requires no supervision during training. Deep learning for 3D data The vision community have witnessed rapid development of deep networks for various tasks [He et al., 2016]. In the field of 3D object recognition, Li et al. [2015], Su et al. [2015b], Girdhar et al. [2016] proposed to learn a joint embedding of 3D shapes and synthesized images, Su et al. [2015a], Qi et al. [2016] focused on learning discriminative representations for 3D object recognition, Xiang et al. [2015], Choy et al. [2016] discussed 3D object reconstruction from in-the-wild images, possibly with a recurrent network, and Girdhar et al. [2016], Sharma et al. [2016] explored autoencoder-based networks for learning voxel-based object representations. Many of these network structure can be used on 3D shape classification [Su et al., 2015a, Qi et al., 2016, Sharma et al., 2016, Maturana and Scherer, 2015], 3D shape retrieval [Shi et al., 2015, Su et al., 2015a], and single image 3D reconstruction [Kar et al., 2015, Bansal et al., 2016, Choy et al., 2016, Girdhar et al., 2016], most of which use full supervision. In comparison, our framework requires no supervision for training, is able to generate objects from a probabilistic space, and comes with a rich discriminative 3D shape representation. Learning with an adversarial net Generative Adversarial Nets (GAN) [Goodfellow et al., 2014] proposed to incorporate an adversarial discriminator into the procedure of generative modeling. More recently, LAPGAN [Denton et al., 2015] and DC-GAN [Radford et al., 2016] adopted GAN with convolutional networks for image synthesis, and achieved impressive performance. Researchers have also explored the use of GAN for other vision problems. To name a few, Wang and Gupta [2016] discussed how to model image style and structure with sequential GANs, Li and Wand [2016] and", "startOffset": 8, "endOffset": 2439}, {"referenceID": 0, "context": ", 2011, Blanz and Vetter, 1999, Kalogerakis et al., 2012, Chaudhuri et al., 2011, Kar et al., 2015, Bansal et al., 2016, Wu et al., 2016]. Since decades ago, AI and vision researchers have made inspiring attempts to design or learn 3D object representations, mostly based on meshes and skeletons. Many of these shape synthesis algorithms are nonparametric and they synthesize new objects by retrieving and combining shapes and parts from a database. Recently, Huang et al. [2015] explored generating 3D shapes with pre-trained templates and producing both object structure and surface geometry. Our framework synthesizes objects without explicitly borrow parts from a repository, and requires no supervision during training. Deep learning for 3D data The vision community have witnessed rapid development of deep networks for various tasks [He et al., 2016]. In the field of 3D object recognition, Li et al. [2015], Su et al. [2015b], Girdhar et al. [2016] proposed to learn a joint embedding of 3D shapes and synthesized images, Su et al. [2015a], Qi et al. [2016] focused on learning discriminative representations for 3D object recognition, Xiang et al. [2015], Choy et al. [2016] discussed 3D object reconstruction from in-the-wild images, possibly with a recurrent network, and Girdhar et al. [2016], Sharma et al. [2016] explored autoencoder-based networks for learning voxel-based object representations. Many of these network structure can be used on 3D shape classification [Su et al., 2015a, Qi et al., 2016, Sharma et al., 2016, Maturana and Scherer, 2015], 3D shape retrieval [Shi et al., 2015, Su et al., 2015a], and single image 3D reconstruction [Kar et al., 2015, Bansal et al., 2016, Choy et al., 2016, Girdhar et al., 2016], most of which use full supervision. In comparison, our framework requires no supervision for training, is able to generate objects from a probabilistic space, and comes with a rich discriminative 3D shape representation. Learning with an adversarial net Generative Adversarial Nets (GAN) [Goodfellow et al., 2014] proposed to incorporate an adversarial discriminator into the procedure of generative modeling. More recently, LAPGAN [Denton et al., 2015] and DC-GAN [Radford et al., 2016] adopted GAN with convolutional networks for image synthesis, and achieved impressive performance. Researchers have also explored the use of GAN for other vision problems. To name a few, Wang and Gupta [2016] discussed how to model image style and structure with sequential GANs, Li and Wand [2016] and", "startOffset": 8, "endOffset": 2529}, {"referenceID": 7, "context": "[2016] used GAN for texture synthesis and image editing, respectively, and Im et al. [2016] developed a recurrent adversarial network for image generation.", "startOffset": 75, "endOffset": 92}, {"referenceID": 14, "context": "Network structure Inspired by Radford et al. [2016], we design an all-convolutional neural network to generate 3D objects.", "startOffset": 30, "endOffset": 52}, {"referenceID": 23, "context": "We compare 3D-GAN with Wu et al. [2015], the state-of-the-art in 3D object synthesis from a probabilistic space, and with a volumetric autoencoder, whose variants have been employed by multiple recent methods [Girdhar et al.", "startOffset": 23, "endOffset": 40}, {"referenceID": 25, "context": "Objects generated by Wu et al. [2015] (30\u00d7 30\u00d7 30)", "startOffset": 21, "endOffset": 38}, {"referenceID": 16, "context": "0% ORION [Sedaghat et al., 2016] 93.", "startOffset": 9, "endOffset": 32}, {"referenceID": 17, "context": "4% VConv-DAE [Sharma et al., 2016] 75.", "startOffset": 13, "endOffset": 34}, {"referenceID": 14, "context": "testing, following Sharma et al. [2016], Maturana and Scherer [2015], Qi et al.", "startOffset": 19, "endOffset": 40}, {"referenceID": 13, "context": "[2016], Maturana and Scherer [2015], Qi et al.", "startOffset": 8, "endOffset": 36}, {"referenceID": 13, "context": "[2016], Maturana and Scherer [2015], Qi et al. [2016]. Specifically, we evaluate our model on both ModelNet10 and ModelNet40, two subsets of ModelNet that are often used as benchmarks for 3D object classification.", "startOffset": 8, "endOffset": 54}, {"referenceID": 16, "context": "[2016], Shi et al. [2015], Maturana and Scherer [2015] used the train/test split included in the dataset, which we also follow; Wu et al.", "startOffset": 8, "endOffset": 26}, {"referenceID": 14, "context": "[2015], Maturana and Scherer [2015] used the train/test split included in the dataset, which we also follow; Wu et al.", "startOffset": 8, "endOffset": 36}, {"referenceID": 14, "context": "[2015], Maturana and Scherer [2015] used the train/test split included in the dataset, which we also follow; Wu et al. [2015], Su et al.", "startOffset": 8, "endOffset": 126}, {"referenceID": 14, "context": "[2015], Maturana and Scherer [2015] used the train/test split included in the dataset, which we also follow; Wu et al. [2015], Su et al. [2015a], Sharma et al.", "startOffset": 8, "endOffset": 145}, {"referenceID": 14, "context": "[2015], Maturana and Scherer [2015] used the train/test split included in the dataset, which we also follow; Wu et al. [2015], Su et al. [2015a], Sharma et al. [2016] used 80 training points and 20 test points in each category for experiments, possibly with viewpoint augmentation.", "startOffset": 8, "endOffset": 167}, {"referenceID": 4, "context": "Following Girdhar et al. [2016], we evaluate results at resolution 20\u00d7 20\u00d7 20, use the average precision as our evaluation metric, and attempt to align each prediction with the ground-truth over permutations, flips, and translational alignments (up to 10%), as IKEA ground truth objects are not in a canonical viewpoint.", "startOffset": 10, "endOffset": 32}, {"referenceID": 14, "context": ", 2015, Radford et al., 2016]. Here we show interpolations both within and across object categories. We observe that for both cases walking over the latent space gives smooth transitions between objects. Arithmetic Another way of exploring the learned representations is to show arithmetic in the latent space. Previously, Dosovitskiy et al. [2015], Radford et al.", "startOffset": 8, "endOffset": 349}, {"referenceID": 14, "context": ", 2015, Radford et al., 2016]. Here we show interpolations both within and across object categories. We observe that for both cases walking over the latent space gives smooth transitions between objects. Arithmetic Another way of exploring the learned representations is to show arithmetic in the latent space. Previously, Dosovitskiy et al. [2015], Radford et al. [2016] presented that their generative nets are able to encode semantic knowledge of chair or face images in its latent space; Girdhar et al.", "startOffset": 8, "endOffset": 372}, {"referenceID": 4, "context": "[2016] presented that their generative nets are able to encode semantic knowledge of chair or face images in its latent space; Girdhar et al. [2016] also showed that the learned representation for 3D objects behave similarly.", "startOffset": 127, "endOffset": 149}, {"referenceID": 4, "context": "[2016] presented that their generative nets are able to encode semantic knowledge of chair or face images in its latent space; Girdhar et al. [2016] also showed that the learned representation for 3D objects behave similarly. We show our shape arithmetic in Figure 8. Different from Girdhar et al. [2016], all of our objects are randomly sampled, requiring no existing 3D CAD models as input.", "startOffset": 127, "endOffset": 305}, {"referenceID": 4, "context": "[2016] presented that their generative nets are able to encode semantic knowledge of chair or face images in its latent space; Girdhar et al. [2016] also showed that the learned representation for 3D objects behave similarly. We show our shape arithmetic in Figure 8. Different from Girdhar et al. [2016], all of our objects are randomly sampled, requiring no existing 3D CAD models as input. \u2020For methods from Girdhar et al. [2016], the mean values in the last column are higher than the originals in their paper, because we compute per-class accuracy instead of per-instance accuracy.", "startOffset": 127, "endOffset": 433}, {"referenceID": 19, "context": "We further use guided back-propagation [Springenberg et al., 2015] to visualize the parts that produce the activation.", "startOffset": 39, "endOffset": 66}], "year": 2016, "abstractText": "We study the problem of 3D object generation. We propose a novel framework, namely 3D Generative Adversarial Network (3D-GAN), which generates 3D objects from a probabilistic space by leveraging recent advances in volumetric convolutional networks and generative adversarial nets. The benefits of our model are three-fold: first, the use of an adversarial criterion, instead of traditional heuristic criteria, enables the generator to capture object structure implicitly and to synthesize high-quality 3D objects; second, the generator establishes a mapping from a low-dimensional probabilistic space to the space of 3D objects, so that we can sample objects without a reference image or CAD models, and explore the 3D object manifold; third, the adversarial discriminator provides a powerful 3D shape descriptor which, learned without supervision, has wide applications in 3D object recognition. Experiments demonstrate that our method generates high-quality 3D objects, and our unsupervisedly learned features achieve impressive performance on 3D object recognition, comparable with those of supervised learning methods.", "creator": "LaTeX with hyperref package"}}}