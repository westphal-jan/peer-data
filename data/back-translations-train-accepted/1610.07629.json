{"id": "1610.07629", "review": {"conference": "iclr", "VERSION": "v1", "DATE_OF_SUBMISSION": "24-Oct-2016", "title": "A Learned Representation For Artistic Style", "abstract": "The diversity of painting styles represents a rich visual vocabulary for the construction of an image. The degree to which one may learn and parsimoniously capture this visual vocabulary measures our understanding of the higher level features of paintings, if not images in general. In this work we investigate the construction of a single, scalable deep network that can parsimoniously capture the artistic style of a diversity of paintings. We demonstrate that such a network generalizes across a diversity of artistic styles by reducing a painting to a point in an embedding space. Importantly, this model permits a user to explore new painting styles by arbitrarily combining the styles learned from individual paintings. We hope that this work provides a useful step towards building rich models of paintings and offers a window on to the structure of the learned representation of artistic style.", "histories": [["v1", "Mon, 24 Oct 2016 20:06:54 GMT  (47073kb,D)", "http://arxiv.org/abs/1610.07629v1", "9 pages. 15 pages of Appendix"], ["v2", "Mon, 5 Dec 2016 16:24:40 GMT  (47075kb,D)", "http://arxiv.org/abs/1610.07629v2", "9 pages. 15 pages of Appendix"], ["v3", "Fri, 9 Dec 2016 01:20:17 GMT  (49442kb,D)", "http://arxiv.org/abs/1610.07629v3", "9 pages. 15 pages of Appendix"], ["v4", "Tue, 27 Dec 2016 23:05:51 GMT  (49443kb,D)", "http://arxiv.org/abs/1610.07629v4", "9 pages. 15 pages of Appendix"], ["v5", "Thu, 9 Feb 2017 16:29:09 GMT  (49443kb,D)", "http://arxiv.org/abs/1610.07629v5", "9 pages. 15 pages of Appendix, International Conference on Learning Representations (ICLR) 2017"]], "COMMENTS": "9 pages. 15 pages of Appendix", "reviews": [], "SUBJECTS": "cs.CV cs.LG", "authors": ["vincent dumoulin", "jonathon shlens", "manjunath kudlur"], "accepted": true, "id": "1610.07629"}, "pdf": {"name": "1610.07629.pdf", "metadata": {"source": "CRF", "title": null, "authors": ["ARTISTIC STYLE", "Vincent Dumoulin"], "emails": ["vi.dumoulin@gmail.com,", "shlens@google.com,", "keveman@google.com"], "sections": [{"heading": "1 INTRODUCTION", "text": "And I think it's important to say that I'm not going to talk about it, and I'm not going to talk about it, and I'm not going to talk about it, but I'm going to talk about it, and I'm going to talk about it, and I'm going to talk about it, and I'm going to talk about it, and I'm going to talk about it, and I'm going to talk about it, and I'm going to talk about it, and I'm going to talk about it, and I'm going to talk about it, and I'm going to talk about it, and I'm going to talk about it, and I'm going to talk about it, and I'm going to talk about it, and I'm going to talk about it, and I'm going to talk about it, and I'm going to talk about it, and I'm going to talk about it, and I'm going to talk about it, and I'm going to talk about it, and I'm going to talk about it, and I'm going to talk about it, and I'm going to talk about it, and I'm going to talk about it, and I'm going to talk about it, and I'm going to talk about it, and I'm going to talk about it, and I'm going to talk about it, and I'm going to talk about it, and I'm going to talk about it, and I'm going to talk about it, and I'm going to talk about it and I'm going to talk about it, and I'm going to talk about it, and I'm going to talk about it, and I'm going to talk about it, and I'm going to talk about it, and I'm going to talk about it, and I'm going to talk about it, and I'm going to talk about it, and I'm going to talk about it, and I'm going to talk about it, and I'm going to talk about it, and I'm going to talk about it, and I'm going to talk about it, and I'm going to talk about it and I'm going to talk about it, and I'm going to talk about it, and I'm going to talk about it, and I'm going to talk about it, and I'm going to talk about it, and I'm going to talk about it, and I'm going to talk about it, and I'm going to talk"}, {"heading": "2 STYLE TRANSFER WITH DEEP NETWORKS", "text": "The first point is motivated by the empirical observation that high-level features in classifiers tend to correspond to higher levels of abstraction (see Zeiler & Fergus (2014) for visualizations, see Johnson et al. (2016); the second point is motivated by the observation that high-level features in classifications tend to correspond to higher levels of abstraction (see Zeiler & Fergus (2014) for visualizations, see Johnson et al. (see Johnson et al.)."}, {"heading": "2.1 FEEDFORWARD STYLE TRANSFER NETWORKS", "text": "To speed up the process outlined above, a style transmission network T is introduced (Figure 2), which requires a content image c as input and outputs the pastiche image p directly; the network is trained on many content images using the same loss function as above, i.e. L (s, c) = \u03bbsLs (T (c)) + \u03bbcLc (T (c)). (4) While feedback-style transmission networks solve the problem of speed at test time, they also suffer from the fact that they are bound to a particular style, which means that for each style to be imitated, a separate network needs to be trained. The real impact of this limitation is that it becomes prohibitive to implement a style transmission application on a memory-limited device such as a smartphone."}, {"heading": "2.2 N-STYLES FEEDFORWARD STYLE TRANSFER NETWORKS", "text": "Our work stems from the intuition that many styles probably share a certain degree of calculation, but differ in the color palette used. In this case, it seems very wasteful to treat a number of N-impressionist paintings as completely separate styles.To take this into account, we propose to train a single conditional style transfer network T (c, s) for N-styles.The conditional network is given both a content image and the identity of the style to apply and produce a pastiche that corresponds to this style.While the idea on paper is simple, the open question remains how conditioning should be done. In researching this question, we found a very surprising fact about the role of normalization in the style transfer network: to model a style, it is sufficient to specialize parameters after normalization on each specific style."}, {"heading": "3 EXPERIMENTAL RESULTS", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "3.1 METHODOLOGY", "text": "Unless otherwise noted, all style transmission networks were trained using the hyperparameters outlined in the table in the appendix. We used the same network architecture as in Johnson et al. (2016), except for two important details: zero padding is replaced by mirror padding, and transposed coils (sometimes called deconvolutions) are replaced by related upsampling followed by folding. Using mirror padding avoids marginal patterns that are sometimes caused by zero padding in SAME-padded coils, while replacing transposed coils avoids checkerboard patterns, as discussed in Odena et al. (2016). We note that with these two improvements in network training, total loss of variation is no longer required that was previously used to remove radio frequency noise, as in Johnson et al. (2016). The evaluation images used for this work were reduced to 125x12x12."}, {"heading": "3.2 TRAINING A SINGLE NETWORK ON N STYLES PRODUCES STYLIZATIONS COMPARABLE", "text": "TO INDEPENDENTLY-TRAINED MODELsAs the first test, we trained a 10-style model on stylistically similar images, namely 10 Impressionist paintings by Claude Monet. Figure 4 shows the result of applying the trained network to evaluation images for a subset of styles, showing the full results in the appendix. The model captures different color palettes and textures. We emphasize that 99.8% of the parameters are shared across all styles, as opposed to 0.2% of the parameters, which are unique for each painting style. To get a sense of what is done by folding 10 styles into a single network, we trained a separate, uniform network for each style and compared it with the 10-style network in terms of style transfer quality and training speed (Figure 5).The left column compares the learning curves for styles and content losses between each style and the 10-style network."}, {"heading": "3.3 THE N-STYLES MODEL IS FLEXIBLE ENOUGH TO CAPTURE VERY DIFFERENT STYLES", "text": "Figure 1a shows the result of applying the trained network to evaluation images for a subset of styles. Once again, the full results are presented in the appendix. The model appears to be able to model all 32 styles, despite the enormous variation in color palette and spatial scale of the painting styles."}, {"heading": "3.4 THE TRAINED NETWORK GENERALIZES ACROSS PAINTING STYLES", "text": "Since all weights in the transformer network are divided between styles, one way to give a trained network a new style is to hold on to the trained weights and learn a new set of \u03b3 and \u03b2 parameters. To test the efficiency of this approach, we used it to gradually integrate Monet's Blossom Plum Trees into the network, which is trained in 32 different styles. Figure 6 shows that this is much faster than learning a new network from scratch (left), producing comparable willows: even after eight times fewer parameter updates than its single-style counterpart, the fine model produces comparable willows (right)."}, {"heading": "3.5 THE TRAINED NETWORK CAN ARBITRARILY COMBINE PAINTING STYLES", "text": "The conditional normalization of instances approach raises some interesting questions about style representation. By learning a different set of \u03b3 and \u03b2 parameters for each style, we learn how to embed styles in a certain way. To probe the benefits of this embedding, we tried convex combinations of gamma and \u03b2 values of very different styles (Figure 1b; Figure 7, left column). Using a single convex combination produces a smooth transition from one style to another. Suppose (Observation 1, \u03b21) and (Observation 2, \u03b22) are the parameters that correspond to two different styles. We use \u03b3 = \u03b1 \u00d7 1 + (1 \u2212 \u03b1) \u00d7 2 and \u03b2 = \u03b1 \u00d7 \u03b21 + (1 \u2212 \u03b1) \u00b7 \u03b22 to stylize an image. Figure 7 (right column) shows the loss of style from the transformer network for a particular source image, in relation to the biennial pressure of a clown's head and proportional to the 1-year print."}, {"heading": "4 DISCUSSION", "text": "It seems very surprising that such a small proportion of the parameters of the network can have such an impact on the entire process of style transfer. It could be that the network architecture is over-specified for the task. Another interpretation could be that the revolutionary weights of the style transfer network encode transformations that represent \"style elements.\" Scaling and shifting factors would then provide a way for each style to inhibit or improve the expression of different style elements to form a global style identity. While this work does not attempt to verify this hypothesis, we think that this would be a very promising direction of research to understand the calculation behind the style transfer networks, as well as the representation of images in general."}, {"heading": "ACKNOWLEDGMENTS", "text": "We would like to thank Fred Bertsch, Douglas Eck, Cinjon Resnick and the rest of the Google Magenta team for their feedback; Peyman Milanfar, Michael Elad, Feng Yang, Jon Barron, Bhavik Singh, Jennifer Daniel, and the Google Brain team for their critical suggestions and advice. Finally, we would like to thank the Google Cultural Institute, whose curated collection of art photos has been very helpful in finding exciting style images to train."}], "references": [{"title": "Image quilting for texture synthesis and transfer", "author": ["Alexei A Efros", "William T Freeman"], "venue": "In Proceedings of the 28th annual conference on Computer graphics and interactive techniques,", "citeRegEx": "Efros and Freeman.,? \\Q2001\\E", "shortCiteRegEx": "Efros and Freeman.", "year": 2001}, {"title": "Texture synthesis by non-parametric sampling", "author": ["Alexei A Efros", "Thomas K Leung"], "venue": "In Computer Vision,", "citeRegEx": "Efros and Leung.,? \\Q1999\\E", "shortCiteRegEx": "Efros and Leung.", "year": 1999}, {"title": "Style-transfer via texture-synthesis", "author": ["Michael Elad", "Peyman Milanfar"], "venue": "arXiv preprint arXiv:1609.03057,", "citeRegEx": "Elad and Milanfar.,? \\Q2016\\E", "shortCiteRegEx": "Elad and Milanfar.", "year": 2016}, {"title": "Split and match: Example-based adaptive patch sampling for unsupervised style transfer", "author": ["Oriel Frigo", "Neus Sabater", "Julie Delon", "Pierre Hellier"], "venue": null, "citeRegEx": "Frigo et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Frigo et al\\.", "year": 2016}, {"title": "Texture synthesis using convolutional neural networks", "author": ["Leon Gatys", "Alexander S Ecker", "Matthias Bethge"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Gatys et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Gatys et al\\.", "year": 2015}, {"title": "A neural algorithm of artistic style", "author": ["Leon A Gatys", "Alexander S Ecker", "Matthias Bethge"], "venue": "arXiv preprint arXiv:1508.06576,", "citeRegEx": "Gatys et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Gatys et al\\.", "year": 2015}, {"title": "Preserving color in neural artistic style transfer", "author": ["Leon A Gatys", "Matthias Bethge", "Aaron Hertzmann", "Eli Shechtman"], "venue": "arXiv preprint arXiv:1606.05897,", "citeRegEx": "Gatys et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Gatys et al\\.", "year": 2016}, {"title": "Image analogies", "author": ["Aaron Hertzmann", "Charles E Jacobs", "Nuria Oliver", "Brian Curless", "David H Salesin"], "venue": "In Proceedings of the 28th annual conference on Computer graphics and interactive techniques,", "citeRegEx": "Hertzmann et al\\.,? \\Q2001\\E", "shortCiteRegEx": "Hertzmann et al\\.", "year": 2001}, {"title": "Perceptual losses for real-time style transfer and super-resolution", "author": ["Justin Johnson", "Alexandre Alahi", "Li Fei-Fei"], "venue": "arXiv preprint arXiv:1603.08155,", "citeRegEx": "Johnson et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Johnson et al\\.", "year": 2016}, {"title": "Visual pattern discrimination", "author": ["Bela Julesz"], "venue": "IRE Trans. Info Theory,", "citeRegEx": "Julesz.,? \\Q1962\\E", "shortCiteRegEx": "Julesz.", "year": 1962}, {"title": "Adam: A method for stochastic optimization", "author": ["Diederik Kingma", "Jimmy Ba"], "venue": "arXiv preprint arXiv:1412.6980,", "citeRegEx": "Kingma and Ba.,? \\Q2014\\E", "shortCiteRegEx": "Kingma and Ba.", "year": 2014}, {"title": "Texture optimization for examplebased synthesis", "author": ["Vivek Kwatra", "Irfan Essa", "Aaron Bobick", "Nipun Kwatra"], "venue": "ACM Transactions on Graphics (ToG),", "citeRegEx": "Kwatra et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Kwatra et al\\.", "year": 2005}, {"title": "Real-time texture synthesis by patch-based sampling", "author": ["Lin Liang", "Ce Liu", "Ying-Qing Xu", "Baining Guo", "Heung-Yeung Shum"], "venue": "ACM Transactions on Graphics (ToG),", "citeRegEx": "Liang et al\\.,? \\Q2001\\E", "shortCiteRegEx": "Liang et al\\.", "year": 2001}, {"title": "Avoiding checkerboard artifacts in neural networks", "author": ["Augustus Odena", "Christopher Olah", "Vincent Dumoulin"], "venue": "Distill,", "citeRegEx": "Odena et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Odena et al\\.", "year": 2016}, {"title": "A parametric texture model based on joint statistics of complex wavelet coefficients", "author": ["Javier Portilla", "Eero Simoncelli"], "venue": "International Journal of Computer Vision,", "citeRegEx": "Portilla and Simoncelli.,? \\Q1999\\E", "shortCiteRegEx": "Portilla and Simoncelli.", "year": 1999}, {"title": "Natural image statistics and neural representation", "author": ["Eero Simoncelli", "Bruno Olshausen"], "venue": "Annual Review of Neuroscience,", "citeRegEx": "Simoncelli and Olshausen.,? \\Q2001\\E", "shortCiteRegEx": "Simoncelli and Olshausen.", "year": 2001}, {"title": "Very deep convolutional networks for large-scale image recognition", "author": ["Karen Simonyan", "Andrew Zisserman"], "venue": "arXiv preprint arXiv:1409.1556,", "citeRegEx": "Simonyan and Zisserman.,? \\Q2014\\E", "shortCiteRegEx": "Simonyan and Zisserman.", "year": 2014}, {"title": "Texture networks: Feedforward synthesis of textures and stylized images", "author": ["Dmitry Ulyanov", "Vadim Lebedev", "Andrea Vedaldi", "Victor Lempitsky"], "venue": "arXiv preprint arXiv:1603.03417,", "citeRegEx": "Ulyanov et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Ulyanov et al\\.", "year": 2016}, {"title": "Instance normalization: The missing ingredient for fast stylization", "author": ["Dmitry Ulyanov", "Andrea Vedaldi", "Victor Lempitsky"], "venue": "arXiv preprint arXiv:1607.08022,", "citeRegEx": "Ulyanov et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Ulyanov et al\\.", "year": 2016}, {"title": "Fast texture synthesis using tree-structured vector quantization", "author": ["Li-Yi Wei", "Marc Levoy"], "venue": "In Proceedings of the 27th annual conference on Computer graphics and interactive techniques,", "citeRegEx": "Wei and Levoy.,? \\Q2000\\E", "shortCiteRegEx": "Wei and Levoy.", "year": 2000}, {"title": "Visualizing and understanding convolutional networks", "author": ["Matthew D Zeiler", "Rob Fergus"], "venue": "In European Conference on Computer Vision,", "citeRegEx": "Zeiler and Fergus.,? \\Q2014\\E", "shortCiteRegEx": "Zeiler and Fergus.", "year": 2014}, {"title": "Claude Monet, V\u00e9theuil (1879)", "author": ["Claude Monet", "Three Fishing Boats"], "venue": "Claude Monet, V\u00e9theuil (1902). 13", "citeRegEx": "Monet and Boats,? 1886", "shortCiteRegEx": "Monet and Boats", "year": 1886}, {"title": "Ernst Ludwig Kirchner, Boy with Sweets (1918)", "author": ["Roy Lichtenstein", "Bicentennial Print"], "venue": "14", "citeRegEx": "Lichtenstein and Print,? 1975", "shortCiteRegEx": "Lichtenstein and Print", "year": 1975}, {"title": "Opus 196 (1889). Paul Klee, Colors from a Distance (1932)", "author": ["Paul Signac", "Cassis", "Cap Lombard"], "venue": "Frederic Edwin Church, Cotopaxi (1855)", "citeRegEx": "Signac et al\\.,? \\Q1932\\E", "shortCiteRegEx": "Signac et al\\.", "year": 1932}, {"title": "Egon Schiele, Edith with Striped Dress, Sitting (1915)", "author": ["Henri de Toulouse-Lautrec", "Divan Japonais"], "venue": "16", "citeRegEx": "Toulouse.Lautrec and Japonais,? 1893", "shortCiteRegEx": "Toulouse.Lautrec and Japonais", "year": 1893}, {"title": "Head of a Clown (ca", "author": ["Georges Rouault"], "venue": "Giorgio de Chirico, Horses on the seashore (1927/1928)", "citeRegEx": "Rouault,? \\Q1907\\E", "shortCiteRegEx": "Rouault", "year": 1907}, {"title": "Severini Gino, Ritmo plastico del 14 luglio (1913)", "author": ["Juan Gris", "Portrait of Pablo Picasso"], "venue": "19", "citeRegEx": "Gris and Picasso,? 1912", "shortCiteRegEx": "Gris and Picasso", "year": 1912}, {"title": "John Ruskin, Trees in a Lane (1847)", "author": ["Claude Monet", "Three Fishing Boats"], "venue": "Giuseppe Cades, Tullia about to Ride over the Body of Her Father in Her Chariot (about 1770-1775). 23", "citeRegEx": "Monet and Boats,? 1886", "shortCiteRegEx": "Monet and Boats", "year": 1886}], "referenceMentions": [{"referenceID": 5, "context": "Efros & Freeman (2001) and Liang et al. (2001) extend this idea to \u201cgrowing\u201d textures one patch at a time, and Efros & Freeman (2001) uses the approach to implement \u201ctexture transfer\u201d, i.", "startOffset": 27, "endOffset": 47}, {"referenceID": 5, "context": "Efros & Freeman (2001) and Liang et al. (2001) extend this idea to \u201cgrowing\u201d textures one patch at a time, and Efros & Freeman (2001) uses the approach to implement \u201ctexture transfer\u201d, i.", "startOffset": 27, "endOffset": 134}, {"referenceID": 5, "context": "Kwatra et al. (2005) approaches the texture synthesis problem from an energy minimization perspective, progressively refining the texture using an EMlike algorithm.", "startOffset": 0, "endOffset": 21}, {"referenceID": 3, "context": "Hertzmann et al. (2001) introduces the concept of \u201cimage analogies\u201d: given a pair of \u201cunfiltered\u201d and \u201cfiltered\u201d versions of an examplar image, a target image is processed to create an analogous \u201cfiltered\u201d result.", "startOffset": 0, "endOffset": 24}, {"referenceID": 3, "context": "More recently, Frigo et al. (2016) treats style transfer as a local texture transfer (using an adaptive patch partition) followed by a global color transfer, and Elad & Milanfar (2016) extends Kwatra\u2019s energy-based method into a style transfer algorithm by taking content similarity into account.", "startOffset": 15, "endOffset": 35}, {"referenceID": 3, "context": "More recently, Frigo et al. (2016) treats style transfer as a local texture transfer (using an adaptive patch partition) followed by a global color transfer, and Elad & Milanfar (2016) extends Kwatra\u2019s energy-based method into a style transfer algorithm by taking content similarity into account.", "startOffset": 15, "endOffset": 185}, {"referenceID": 3, "context": "More recently, Frigo et al. (2016) treats style transfer as a local texture transfer (using an adaptive patch partition) followed by a global color transfer, and Elad & Milanfar (2016) extends Kwatra\u2019s energy-based method into a style transfer algorithm by taking content similarity into account. On the machine learning side, it has been shown that a trained classifier can be used as a feature extractor to drive texture synthesis and style transfer. Gatys et al. (2015a) uses the VGG-19 network (Simonyan & Zisserman, 2014) to extract features from a texture image and a synthesized texture.", "startOffset": 15, "endOffset": 474}, {"referenceID": 3, "context": "More recently, Frigo et al. (2016) treats style transfer as a local texture transfer (using an adaptive patch partition) followed by a global color transfer, and Elad & Milanfar (2016) extends Kwatra\u2019s energy-based method into a style transfer algorithm by taking content similarity into account. On the machine learning side, it has been shown that a trained classifier can be used as a feature extractor to drive texture synthesis and style transfer. Gatys et al. (2015a) uses the VGG-19 network (Simonyan & Zisserman, 2014) to extract features from a texture image and a synthesized texture. The two sets of features are compared and the synthesized texture is modified by gradient descent so that the two sets of features are as close as possible. Gatys et al. (2015b) extends this idea to style transfer by adding the constraint that the synthesized image also be close to a content image with respect to another set of features extracted by the trained VGG-19 classifier.", "startOffset": 15, "endOffset": 773}, {"referenceID": 3, "context": "More recently, Frigo et al. (2016) treats style transfer as a local texture transfer (using an adaptive patch partition) followed by a global color transfer, and Elad & Milanfar (2016) extends Kwatra\u2019s energy-based method into a style transfer algorithm by taking content similarity into account. On the machine learning side, it has been shown that a trained classifier can be used as a feature extractor to drive texture synthesis and style transfer. Gatys et al. (2015a) uses the VGG-19 network (Simonyan & Zisserman, 2014) to extract features from a texture image and a synthesized texture. The two sets of features are compared and the synthesized texture is modified by gradient descent so that the two sets of features are as close as possible. Gatys et al. (2015b) extends this idea to style transfer by adding the constraint that the synthesized image also be close to a content image with respect to another set of features extracted by the trained VGG-19 classifier. While very flexible, this algorithm is expensive to run due to the optimization loop being carried. Ulyanov et al. (2016a) and Johnson et al.", "startOffset": 15, "endOffset": 1101}, {"referenceID": 3, "context": "More recently, Frigo et al. (2016) treats style transfer as a local texture transfer (using an adaptive patch partition) followed by a global color transfer, and Elad & Milanfar (2016) extends Kwatra\u2019s energy-based method into a style transfer algorithm by taking content similarity into account. On the machine learning side, it has been shown that a trained classifier can be used as a feature extractor to drive texture synthesis and style transfer. Gatys et al. (2015a) uses the VGG-19 network (Simonyan & Zisserman, 2014) to extract features from a texture image and a synthesized texture. The two sets of features are compared and the synthesized texture is modified by gradient descent so that the two sets of features are as close as possible. Gatys et al. (2015b) extends this idea to style transfer by adding the constraint that the synthesized image also be close to a content image with respect to another set of features extracted by the trained VGG-19 classifier. While very flexible, this algorithm is expensive to run due to the optimization loop being carried. Ulyanov et al. (2016a) and Johnson et al. (2016) tackle this problem by introducing a feedforward style transfer network, which is trained to go from content to pastiche image in one pass.", "startOffset": 15, "endOffset": 1127}, {"referenceID": 6, "context": "with respect to color preservation (Gatys et al., 2016) or style transfer quality (Ulyanov et al.", "startOffset": 35, "endOffset": 55}, {"referenceID": 9, "context": "A visual texture is conjectured to be spatially homogenous and consist of repeated structural motifs whose minimal sufficient statistics are captured by lower order statistical measurements (Julesz, 1962; Portilla & Simoncelli, 1999).", "startOffset": 190, "endOffset": 233}, {"referenceID": 5, "context": "The first point is motivated by the empirical observation that high-level features in classifiers tend to correspond to higher levels of abstractions (see Zeiler & Fergus (2014) for visualizations; see Johnson et al. (2016) for style transfer features).", "startOffset": 202, "endOffset": 224}, {"referenceID": 8, "context": "In constrast, a single-style network requires N feed forward passes to perform N style transfers (Johnson et al., 2016; Ulyanov et al., 2016a).", "startOffset": 97, "endOffset": 142}, {"referenceID": 16, "context": "Building off the instance normalization technique proposed in Ulyanov et al. (2016b), we augment the \u03b3 and \u03b2 parameters so that they\u2019re N \u00d7 C matrices, where N is the number of styles being modeled and C is the number of output feature maps.", "startOffset": 62, "endOffset": 85}, {"referenceID": 8, "context": "We used the same network architecture as in Johnson et al. (2016), except for two key details: zero-padding is replaced with mirror-padding, and transposed convolutions (also sometimes called deconvolutions) are replaced with nearest-neighbor upsampling followed by a convolution.", "startOffset": 44, "endOffset": 66}, {"referenceID": 8, "context": "We used the same network architecture as in Johnson et al. (2016), except for two key details: zero-padding is replaced with mirror-padding, and transposed convolutions (also sometimes called deconvolutions) are replaced with nearest-neighbor upsampling followed by a convolution. The use of mirror-padding avoids border patterns sometimes caused by zero-padding in SAME-padded convolutions, while the replacement for transposed convolutions avoids checkerboard patterning, as discussed in in Odena et al. (2016). We find that with these two improvements training the network no longer requires a total variation loss that was previously employed to remove high frequency noise as proposed in Johnson et al.", "startOffset": 44, "endOffset": 513}, {"referenceID": 8, "context": "We used the same network architecture as in Johnson et al. (2016), except for two key details: zero-padding is replaced with mirror-padding, and transposed convolutions (also sometimes called deconvolutions) are replaced with nearest-neighbor upsampling followed by a convolution. The use of mirror-padding avoids border patterns sometimes caused by zero-padding in SAME-padded convolutions, while the replacement for transposed convolutions avoids checkerboard patterning, as discussed in in Odena et al. (2016). We find that with these two improvements training the network no longer requires a total variation loss that was previously employed to remove high frequency noise as proposed in Johnson et al. (2016). The evaluation images used for this work were resized such that their smaller side has size 512.", "startOffset": 44, "endOffset": 715}], "year": 2016, "abstractText": "The diversity of painting styles represents a rich visual vocabulary for the construction of an image. The degree to which one may learn and parsimoniously capture this visual vocabulary measures our understanding of the higher level features of paintings, if not images in general. In this work we investigate the construction of a single, scalable deep network that can parsimoniously capture the artistic style of a diversity of paintings. We demonstrate that such a network generalizes across a diversity of artistic styles by reducing a painting to a point in an embedding space. Importantly, this model permits a user to explore new painting styles by arbitrarily combining the styles learned from individual paintings. We hope that this work provides a useful step towards building rich models of paintings and offers a window on to the structure of the learned representation of artistic style.", "creator": "LaTeX with hyperref package"}}}