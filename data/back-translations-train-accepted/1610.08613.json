{"id": "1610.08613", "review": {"conference": "NIPS", "VERSION": "v1", "DATE_OF_SUBMISSION": "27-Oct-2016", "title": "Can Active Memory Replace Attention?", "abstract": "Several mechanisms to focus attention of a neural network on selected parts of its input or memory have been used successfully in deep learning models in recent years. Attention has improved image classification, image captioning, speech recognition, generative models, and learning algorithmic tasks, but it had probably the largest impact on neural machine translation.", "histories": [["v1", "Thu, 27 Oct 2016 04:28:29 GMT  (18kb)", "http://arxiv.org/abs/1610.08613v1", null], ["v2", "Tue, 7 Mar 2017 04:04:33 GMT  (18kb)", "http://arxiv.org/abs/1610.08613v2", null]], "reviews": [], "SUBJECTS": "cs.LG cs.CL", "authors": ["lukasz kaiser", "samy bengio"], "accepted": true, "id": "1610.08613"}, "pdf": {"name": "1610.08613.pdf", "metadata": {"source": "CRF", "title": null, "authors": [], "emails": ["lukaszkaiser@google.com", "bengio@google.com"], "sections": [{"heading": null, "text": "ar Xiv: 161 0.08 613v 1 [cs.L G] 27 Oct 2"}, {"heading": "1 Introduction", "text": "This year, as never before in the history of a country in which it is a country in which it is a country in which it is a country in which it is a country, a country in which it is a country, a country in which it is a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country"}, {"heading": "2 Active Memory Models", "text": "In the previous section, we used the term active memory in general, referring to each model in which each part of the memory undergoes an active change at each step. This is in contrast to attention models, in which only a small part of the memory is changed at each step, or where the memory remains constant. The exact implementation of an active change in the memory may vary from model to model. In the present work, we will focus on the most common ways that implement this change, all based on evolution."}, {"heading": "2.1 The Markovian Neural GPU", "text": "The basic model of the neural GPU delivers very poor results in neural machine translation: the confusion per word in WMT1 is not less than 30 (good models in this task are less than 4), and the BLEU values are also very bad (below 5, while good models are higher than 20). Which part of the model is responsible for such bad results? It turns out that the main culprit is the output generator. As can be seen in Figure 3 above, it does not work for more difficult problems in the real world, where there could be several possible output symbols for each input.The most basic way to mitigate this problem is to make each output symbol dependent on the previous output, like the toy tasks for which the neural GPU was designed. \u2212 However, it does not work for more difficult problems in the real world, where there could be several possible output symbols for each input.The most basic way to make the output symbol dependent on the previous problem is to clarify the output state \u2212 The most basic method is to make this output only \u2212."}, {"heading": "2.2 The Extended Neural GPU", "text": "The Markovian Neural GPU delivers much better results in neural machine translation than the base model: its confusion per word reaches about 12 and its BLEU values improve a bit. But these results are still far from those achieved by models with attention. 1See section 3 for more details on experimental setting. Could it be that the Markovian dependence of results is too weak for this problem that a fully recurring dependence of the state is required for good performance? We test this by adding an active memory decoder to the base model, as shown in Figure 5. The definition of the Advanced Neural GPU follows the base model, up to sfin = sn. We consider sn as the starting point for the active memory decoder, i.e., we use d0 = sn. In the active memory decoder, we use a separate output band tensor p of the same shape as your 0, i.e., we define the form of the GRw = all."}, {"heading": "2.3 Related Models", "text": "A revolutionary architecture has already been used to achieve good results in neural machine translation at the word level in [20] and more recently in [21]. This model uses a standard RNN on top of the convolution to generate the output and thus avoid the problem of output dependence, but the state of this RNN has a fixed size, and in the first case the sentence representation generated by the revolutionary network is also a fixed size vector. Therefore, while these models are superficially similar to the active memory, they are more similar to fixed memory models. The first suffers from all limitations of sequence sequence models without attention [4, 6] which we discussed earlier. Another recently introduced model, the Grid-LSTM [22], could be less related to active memory as it does not use any convolutions at all, but in fact it is a major extension of the active memory model - the memory on the diagonal of the STM cells running."}, {"heading": "3 Experiments", "text": "Dre rf\u00fc ide eeisrteeGsrteeeeeeeeteeteeteeteeteeteeteeteeteeteeteeteeteerrrrrsrrsrrsrrsrrsrsrrsrrsrrsrsrrsrrrsrteeteeteeteeteeteeteeteeteeteeteeteeteeteeteeteeteeteeteeteeteeteeteeteeteeteeteeteeteeteeteeteeteeteeteeteeteeteeteeteeteeteeteeteeteeteeteeteeteeteeteeteeteeteeteeteerrrsrsrrrrsrsrsrsrsrsrsrsrsrsrsrsrsrsrsrsrteeteeteeteeteeteeteeteetelllllteeteeteeteeteeteeteeteeteeteeteeteeteeteeteeteeteeteeteeteerrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrleteeteeteeteeteeteeteeteeteeteeteeteeteeteeteeteeteeteeteeteeteeteeeteeteeteeteeteeteeteeteeteeteeeteeteeteeteeteeteeteeteeteeteeteeteeteeteeteeteeeteeteeteeteeteeteeteeteeteeteeteeteeteeteeteeteeteeeteeteeteeteeteeteeteeteeteeteeteeteeteeteeteeteeteetee"}, {"heading": "4 Discussion", "text": "A pure neural GPU model yields 3.5, a Markovian 2.5, and only a fully dependent model trained under teacher coercion yields 1.3. Recurring dependence in generating the power distribution proves to be the key to achieving good performance. We find it instructive that the problem of dependencies in power distribution can be decoupled from the specifics of the model or model class. In previous work, such dependence (and teacher coercion training) has always been used in LSTM and GRU models, but very rarely in other models. We show that it can be advantageous to consider this problem separately from the model architecture. It allows us to create the extended neural GPU, and this way of thinking may also prove fruitful for other classes of models."}], "references": [{"title": "Imagenet classification with deep convolutional neural network", "author": ["Alex Krizhevsky", "Ilya Sutskever", "Geoffrey Hinton"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2012}, {"title": "Context-dependent pre-trained deep neural networks for large-vocabulary speech recognition", "author": ["George E. Dahl", "Dong Yu", "Li Deng", "Alex Acero"], "venue": "IEEE Transactions on Audio, Speech & Language Processing,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2012}, {"title": "Long short-term memory", "author": ["Sepp Hochreiter", "J\u00fcrgen Schmidhuber"], "venue": "Neural computation,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 1997}, {"title": "Sequence to sequence learning with neural networks", "author": ["Ilya Sutskever", "Oriol Vinyals", "Quoc VV Le"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2014}, {"title": "Neural machine translation by jointly learning to align and translate", "author": ["Dzmitry Bahdanau", "Kyunghyun Cho", "Yoshua Bengio"], "venue": "CoRR, abs/1409.0473,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2014}, {"title": "Learning phrase representations using rnn encoder-decoder for statistical machine", "author": ["Kyunghyun Cho", "Bart van Merrienboer", "Caglar Gulcehre", "Fethi Bougares", "Holger Schwenk", "Yoshua Bengio"], "venue": "translation. CoRR,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2014}, {"title": "Grammar as a foreign language", "author": ["Vinyals", "Kaiser", "Koo", "Petrov", "Sutskever", "Hinton"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2015}, {"title": "Show, attend and tell: Neural image caption generation with visual attention", "author": ["Kelvin Xu", "Jimmy Lei Ba", "Ryan Kiros", "Kyunghyun Cho", "Aaron Courville", "Ruslan Salakhutdinov", "Richard S. Zemel", "Yoshua Bengio"], "venue": "In ICML,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2015}, {"title": "Draw: A recurrent neural network for image generation", "author": ["Karol Gregor", "Ivo Danihelka", "Alex Graves", "Danilo Jimenez Rezende", "Daan Wierstra"], "venue": "CoRR, abs/1502.04623,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2015}, {"title": "Simple statistical gradient-following algorithms for connectionist reinforcement learning", "author": ["Ronald J. Williams"], "venue": "Machine Learning,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 1992}, {"title": "Neural GPUs learn algorithms", "author": ["\u0141ukasz Kaiser", "Ilya Sutskever"], "venue": "In International Conference on Learning Representations (ICLR),", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2016}, {"title": "Inferring algorithmic patterns with stack-augmented recurrent nets", "author": ["A. Joulin", "T. Mikolov"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2015}, {"title": "Deep residual learning for image recognition", "author": ["Kaiming He", "Xiangyu Zhang", "Shaoqing Ren", "Jian Sun"], "venue": null, "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2016}, {"title": "Bridging the gaps between residual learning, recurrent neural networks and visual cortex", "author": ["Qianli Liao", "Tomaso Poggio"], "venue": null, "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2016}, {"title": "One-shot generalization in deep generative models", "author": ["Danilo Jimenez Rezende", "Shakir Mohamed", "Ivo Danihelka", "Karol Gregor", "Daan Wierstra"], "venue": null, "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2016}, {"title": "Fast algorithms for convolutional neural networks", "author": ["Andrew Lavin", "Scott Gray"], "venue": "CoRR, abs/1509.09308,", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2015}, {"title": "On the properties of neural machine translation: Encoder-decoder approaches", "author": ["K. Cho", "B. van Merrienboer", "D. Bahdanau", "Y. Bengio"], "venue": "CoRR, abs/1409.1259,", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2014}, {"title": "Recurrent continuous translation models", "author": ["Nal Kalchbrenner", "Phil Blunsom"], "venue": "In Proceedings EMNLP", "citeRegEx": "20", "shortCiteRegEx": "20", "year": 2013}, {"title": "Encoding source language with convolutional neural network for machine translation", "author": ["Fandong Meng", "Zhengdong Lu", "Mingxuan Wang", "Hang Li", "Wenbin Jiang", "Qun Liu"], "venue": "In ACL,", "citeRegEx": "21", "shortCiteRegEx": "21", "year": 2015}, {"title": "Grid long short-term memory", "author": ["Nal Kalchbrenner", "Ivo Danihelka", "Alex Graves"], "venue": "In International Conference on Learning Representations,", "citeRegEx": "22", "shortCiteRegEx": "22", "year": 2016}, {"title": "Convolutional LSTM network: A machine learning approach for precipitation nowcasting", "author": ["Xingjian Shi", "Zhourong Chen", "Hao Wang", "Dit-Yan Yeung", "Wai kin Wong", "Wang chun Woo"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "23", "shortCiteRegEx": "23", "year": 2015}, {"title": "Variable rate image compression with recurrent neural networks", "author": ["George Toderici", "Sean M. O\u2019Malley", "Sung Jin Hwang", "Damien Vincent", "David Minnen", "Shumeet Baluja", "Michele Covell", "Rahul Sukthankar"], "venue": "In International Conference on Learning Representations,", "citeRegEx": "24", "shortCiteRegEx": "24", "year": 2016}, {"title": "Adam: A method for stochastic optimization", "author": ["Diederik P. Kingma", "Jimmy Ba"], "venue": "CoRR, abs/1412.6980,", "citeRegEx": "25", "shortCiteRegEx": "25", "year": 2014}, {"title": "Tensorflow: Large-scale machine learning on heterogeneous distributed systems", "author": ["Mart\u00edn Abadi", "Ashish Agarwal", "Paul Barham", "Eugene Brevdo", "Zhifeng Chen", "Craig Citro", "Greg Corrado", "Andy Davis", "Jeffrey Dean", "Matthieu Devin", "Sanjay Ghemawat", "Ian Goodfellow", "Andrew Harp", "Geoffrey Irving", "Michael Isard", "Yangqing Jia", "Rafal Jozefowicz", "Lukasz Kaiser", "Manjunath Kudlur", "Josh Levenberg", "Dan Man\u00e9", "Rajat Monga", "Sherry Moore", "Derek Murray", "Chris Olah", "Mike Schuster", "Jonathon Shlens", "Benoit Steiner", "Ilya Sutskever", "Kunal Talwar", "Paul Tucker", "Vincent Vanhoucke", "Vijay Vasudevan", "Fernanda Vi\u00e9gas", "Oriol Vinyals", "Pete Warden", "Martin Wattenberg", "Martin Wicke", "Yuan Yu", "Xiaoqiang Zheng"], "venue": null, "citeRegEx": "26", "shortCiteRegEx": "26", "year": 2015}, {"title": "Modeling coverage for neural machine translation", "author": ["Zhaopeng Tu", "Zhengdong Lu", "Yang Liu", "Xiaohua Liu", "Hang Li"], "venue": "CoRR, abs/1601.04811,", "citeRegEx": "27", "shortCiteRegEx": "27", "year": 2016}], "referenceMentions": [{"referenceID": 0, "context": "Recent successes of deep neural networks have spanned many domains, from computer vision [1] to speech recognition [2] and many other tasks.", "startOffset": 89, "endOffset": 92}, {"referenceID": 1, "context": "Recent successes of deep neural networks have spanned many domains, from computer vision [1] to speech recognition [2] and many other tasks.", "startOffset": 115, "endOffset": 118}, {"referenceID": 2, "context": "In particular, sequence-to-sequence recurrent neural networks (RNNs) with long short-term memory (LSTM) cells [3] have proven especially successful at natural language processing (NLP) tasks, including machine translation [4, 5, 6].", "startOffset": 110, "endOffset": 113}, {"referenceID": 3, "context": "In particular, sequence-to-sequence recurrent neural networks (RNNs) with long short-term memory (LSTM) cells [3] have proven especially successful at natural language processing (NLP) tasks, including machine translation [4, 5, 6].", "startOffset": 222, "endOffset": 231}, {"referenceID": 4, "context": "In particular, sequence-to-sequence recurrent neural networks (RNNs) with long short-term memory (LSTM) cells [3] have proven especially successful at natural language processing (NLP) tasks, including machine translation [4, 5, 6].", "startOffset": 222, "endOffset": 231}, {"referenceID": 5, "context": "In particular, sequence-to-sequence recurrent neural networks (RNNs) with long short-term memory (LSTM) cells [3] have proven especially successful at natural language processing (NLP) tasks, including machine translation [4, 5, 6].", "startOffset": 222, "endOffset": 231}, {"referenceID": 3, "context": "While a pure sequence-to-sequence recurrent neural network can already obtain good translation results [4, 6], it suffers from the fact that the whole sentence to be translated needs to be encoded into a single fixed-size vector.", "startOffset": 103, "endOffset": 109}, {"referenceID": 5, "context": "While a pure sequence-to-sequence recurrent neural network can already obtain good translation results [4, 6], it suffers from the fact that the whole sentence to be translated needs to be encoded into a single fixed-size vector.", "startOffset": 103, "endOffset": 109}, {"referenceID": 6, "context": "This clearly manifests itself in the degradation of translation quality on longer sentences (see Figure 6) and hurts even more when there is less training data [7].", "startOffset": 160, "endOffset": 163}, {"referenceID": 4, "context": "In [5], a successful mechanism to overcome this problem was presented: a neural model of attention.", "startOffset": 3, "endOffset": 6}, {"referenceID": 7, "context": "Image models can benefit from attention too; for instance, image captioning models can focus on the relevant parts of the image when describing it [8]; generative models for images yield especially good results with", "startOffset": 147, "endOffset": 150}, {"referenceID": 8, "context": "attention, as was demonstrated by the DRAW model [9], where the network focuses on a part of the image to produce at a given time.", "startOffset": 49, "endOffset": 52}, {"referenceID": 7, "context": "In the extreme case, also known as hard attention [8], one of the memory elements is selected and the selection is trained using the REINFORCE algorithm (since this is not differentiable) [11].", "startOffset": 50, "endOffset": 53}, {"referenceID": 9, "context": "In the extreme case, also known as hard attention [8], one of the memory elements is selected and the selection is trained using the REINFORCE algorithm (since this is not differentiable) [11].", "startOffset": 188, "endOffset": 192}, {"referenceID": 10, "context": "If only a single attention mechanism is present, the model will have a hard time learning this task and will not generalize properly, as was demonstrated in [12, 13].", "startOffset": 157, "endOffset": 165}, {"referenceID": 11, "context": "If only a single attention mechanism is present, the model will have a hard time learning this task and will not generalize properly, as was demonstrated in [12, 13].", "startOffset": 157, "endOffset": 165}, {"referenceID": 10, "context": "A solution to this problem, already proposed in the recent literature (for instance, the Neural GPU from [12]), is to allow the model to access and change all its memory at each decoding step.", "startOffset": 105, "endOffset": 109}, {"referenceID": 12, "context": "Residual networks [14], the currently best performing model on the ImageNet task, falls into this category.", "startOffset": 18, "endOffset": 22}, {"referenceID": 13, "context": "In [15] it was shown that the weights of different layers of a residual network can be tied (so it becomes recurrent), without degrading performance.", "startOffset": 3, "endOffset": 7}, {"referenceID": 14, "context": "Other models that operate on the whole canvas at each step were presented in [16, 17].", "startOffset": 77, "endOffset": 85}, {"referenceID": 10, "context": "But what about non-image models? The Neural GPUs [12] demonstrated that active memory yields superior results on algorithmic tasks.", "startOffset": 49, "endOffset": 53}, {"referenceID": 15, "context": ", [18]) on optimizing convolutions.", "startOffset": 2, "endOffset": 6}, {"referenceID": 12, "context": "While residual models have been successful in image analysis [14] and generation [16], they might suffer from the vanishing gradient problem in the same way as recurrent neural networks do.", "startOffset": 61, "endOffset": 65}, {"referenceID": 14, "context": "While residual models have been successful in image analysis [14] and generation [16], they might suffer from the vanishing gradient problem in the same way as recurrent neural networks do.", "startOffset": 81, "endOffset": 85}, {"referenceID": 2, "context": "Therefore, in the same spirit as LSTM gates [3] and GRU gates [19] improve over pure RNNs, one can introduce convolutional LSTM and GRU operators.", "startOffset": 44, "endOffset": 47}, {"referenceID": 16, "context": "Therefore, in the same spirit as LSTM gates [3] and GRU gates [19] improve over pure RNNs, one can introduce convolutional LSTM and GRU operators.", "startOffset": 62, "endOffset": 66}, {"referenceID": 10, "context": "Let us focus on the convolutional GRU, which we define in the same way as in [12], namely: CGRU(s) = u\u2299 s+ (1\u2212 u)\u2299 tanh(U \u2217 (r \u2299 s) +B), where u = \u03c3(U \u2032 \u2217 s+B) and r = \u03c3(U \u2032\u2032 \u2217 s+B).", "startOffset": 77, "endOffset": 81}, {"referenceID": 10, "context": "As a baseline for our investigation of active memory models, we will use the Neural GPU model from [12], depicted in Figure 3, and defined as follows.", "startOffset": 99, "endOffset": 103}, {"referenceID": 3, "context": "Also, as is standard in recurrent networks [4], we use teacher forcing, i.", "startOffset": 43, "endOffset": 46}, {"referenceID": 17, "context": "A convolutional architecture has already been used to obtain good results in word-level neural machine translation in [20] and more recently in [21].", "startOffset": 118, "endOffset": 122}, {"referenceID": 18, "context": "A convolutional architecture has already been used to obtain good results in word-level neural machine translation in [20] and more recently in [21].", "startOffset": 144, "endOffset": 148}, {"referenceID": 3, "context": "The first one suffers from all the limitations of sequence-to-sequence models without attention [4, 6] that we discussed before.", "startOffset": 96, "endOffset": 102}, {"referenceID": 5, "context": "The first one suffers from all the limitations of sequence-to-sequence models without attention [4, 6] that we discussed before.", "startOffset": 96, "endOffset": 102}, {"referenceID": 19, "context": "Another recently introduced model, the Grid LSTM [22], might look less related to active memory, as it does not use convolutions at all.", "startOffset": 49, "endOffset": 53}, {"referenceID": 19, "context": "The experiments from [22] are only performed on a very small dataset of 44K short sentences.", "startOffset": 21, "endOffset": 25}, {"referenceID": 7, "context": "In image processing, in addition to the captioning [8] and generative models [16, 17] that we mentioned before, there are several other active memory models.", "startOffset": 51, "endOffset": 54}, {"referenceID": 14, "context": "In image processing, in addition to the captioning [8] and generative models [16, 17] that we mentioned before, there are several other active memory models.", "startOffset": 77, "endOffset": 85}, {"referenceID": 20, "context": "They use convolutional LSTMs, an architecture similar to CGRU, and have recently been used for weather prediction [23] and image compression [24], in both cases surpassing the state-of-the-art.", "startOffset": 114, "endOffset": 118}, {"referenceID": 21, "context": "They use convolutional LSTMs, an architecture similar to CGRU, and have recently been used for weather prediction [23] and image compression [24], in both cases surpassing the state-of-the-art.", "startOffset": 141, "endOffset": 145}, {"referenceID": 22, "context": "For the results presented in this paper we used the Adam optimizer [25] with \u03b5 = 10 and gradients norm clipped to 1.", "startOffset": 67, "endOffset": 71}, {"referenceID": 4, "context": "This is the same task that was used to introduce attention [5], but \u2013 to avoid the problem with the UNK token \u2013 we spell-out each word that is not in the vocabulary.", "startOffset": 59, "endOffset": 62}, {"referenceID": 4, "context": "As a baseline, we use a GRU model with attention that is almost identical to the original one from [5], except that it has 2 layers of GRU cells, each with 1024 units.", "startOffset": 99, "endOffset": 102}, {"referenceID": 6, "context": "This model is identical as the one in [7], except that is uses GRU cells instead of LSTM cells.", "startOffset": 38, "endOffset": 41}, {"referenceID": 23, "context": "Our model was implemented using TensorFlow [26].", "startOffset": 43, "endOffset": 47}, {"referenceID": 24, "context": "We experimented with more elaborate methods following [27] but it did not improve our results.", "startOffset": 54, "endOffset": 58}, {"referenceID": 4, "context": "We plot the curves for the Extended Neural GPU model, the long baseline GRU model with attention, and \u2013 for comparison \u2013 we add the numbers for a non-attention model from Figure 2 of [5].", "startOffset": 183, "endOffset": 186}, {"referenceID": 6, "context": "In addition to the main large-scale translation task, we tested the Extended Neural GPU on English constituency parsing, the same task as in [7].", "startOffset": 141, "endOffset": 144}, {"referenceID": 6, "context": "3 reported in [7], but we didn\u2019t use any of their optimizations (no early stopping, no POS-tag substitution, no special tuning).", "startOffset": 14, "endOffset": 17}], "year": 2016, "abstractText": "Several mechanisms to focus attention of a neural network on selected parts of its input or memory have been used successfully in deep learning models in recent years. Attention has improved image classification, image captioning, speech recognition, generative models, and learning algorithmic tasks, but it had probably the largest impact on neural machine translation. Recently, similar improvements have been obtained using alternative mechanisms that do not focus on a single part of a memory but operate on all of it in parallel, in a uniform way. Such mechanism, which we call active memory, improved over attention in algorithmic tasks, image processing, and in generative modelling. So far, however, active memory has not improved over attention for most natural language processing tasks, in particular for machine translation. We analyze this shortcoming in this paper and propose an extended model of active memory that matches existing attention models on neural machine translation and generalizes better to longer sentences. We investigate this model and explain why previous active memory models did not succeed. Finally, we discuss when active memory brings most benefits and where attention can be a better choice.", "creator": "LaTeX with hyperref package"}}}