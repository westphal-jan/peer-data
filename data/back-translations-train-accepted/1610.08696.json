{"id": "1610.08696", "review": {"conference": "NIPS", "VERSION": "v1", "DATE_OF_SUBMISSION": "27-Oct-2016", "title": "Learning Bound for Parameter Transfer Learning", "abstract": "We consider a transfer-learning problem by using the parameter transfer approach, where a suitable parameter of feature mapping is learned through one task and applied to another objective task. Then, we introduce the notion of the local stability of parametric feature mapping and parameter transfer learnability, and thereby derive a learning bound for parameter transfer algorithms. As an application of parameter transfer learning, we discuss the performance of sparse coding in self-taught learning. Although self-taught learning algorithms with plentiful unlabeled data often show excellent empirical performance, their theoretical analysis has not been studied. In this paper, we also provide the first theoretical learning bound for self-taught learning.", "histories": [["v1", "Thu, 27 Oct 2016 10:50:55 GMT  (38kb)", "http://arxiv.org/abs/1610.08696v1", "This paper was accepted at NIPS 2016 as a poster presentation"], ["v2", "Wed, 9 Nov 2016 01:08:40 GMT  (37kb)", "http://arxiv.org/abs/1610.08696v2", "This paper was accepted at NIPS 2016 as a poster presentation"], ["v3", "Wed, 18 Jan 2017 04:41:17 GMT  (37kb)", "http://arxiv.org/abs/1610.08696v3", "This paper was accepted at NIPS 2016 as a poster presentation"]], "COMMENTS": "This paper was accepted at NIPS 2016 as a poster presentation", "reviews": [], "SUBJECTS": "stat.ML cs.LG", "authors": ["wataru kumagai"], "accepted": true, "id": "1610.08696"}, "pdf": {"name": "1610.08696.pdf", "metadata": {"source": "CRF", "title": "Learning Bound for Parameter Transfer Learning", "authors": ["Wataru Kumagai"], "emails": ["kumagai@kanagawa-u.ac.jp"], "sections": [{"heading": null, "text": "ar Xiv: 161 0.08 696v 1 [stat.ML] 2 7O ct"}, {"heading": "1 Introduction", "text": "This year, it is only a matter of time before there is an agreement."}, {"heading": "1.1 Related Work", "text": "In this paper, we look at the parameter transfer approach. This approach can be applied to several remarkable algorithms, such as sparse coding, multiple learning and deep learning since the dictionary, weights on cores and weights on the neural network are considered parameters in each case. Then, these parameters are typically trained or matched to samples that are not necessarily taken from a target region. In the parameter transfer region, a number of samples in the source region is often required to accurately estimate the parameter to be transferred. In the source region, it is desirable to be able to use unmarked samples. Self-taught learning is similar to the case where only unmarked samples are given in the source region, while marked samples are available in the target region. In this sense, self-taught learning is compatible with the parameter approach."}, {"heading": "2 Learning Bound for Parameter Transfer Learning", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "2.1 Problem Setting of Parameter Transfer Learning", "text": "We formulate parameter transfer learning in this area (h). We first introduce notations and terminology relating to transfer-specific learning processes (h). (h) We assume that there is an effective distribution method (h). (h) We assume that the distribution method in the region where the distribution method is used is not located in the region itself. (h) We assume that the distribution method in the region where the distribution method and the distribution method are used is a pair. (h) In addition, we let H = {h \u2192 Y} be a hypothesis and a hypothesis: Z \u00d7 Z \u2192 R \u00b2 represent a loss function. Then, the expected risk and empirical risk are defined by R (h): = Ez = (x, y).P [z, h] and R (h)."}, {"heading": "2.2 Learning Bound Based on Stability and Learnability", "text": "We reintroduce the local stability and the parameter transfer learnability as below. These terms are essential to derive a learning boundary in Theorem 1.Definition 1 (Local stability).A parametric attribute that is used is referred to as local stable if there are uniform patterns: X \u2192 R > 0 for each uniform pattern and definition > 0 so that the permissible radius of the disturbance is limited at x. For samples Xn = {x1,.. xn}, which we refer to as a uniform source (Xn): = minj."}, {"heading": "2.3 Proof of Learning Bound", "text": "We assign theorem 1 in this subsection: In this proof, we omit the subscript T for simplicity. (< w,) In addition, we specify the system requirements for the system requirements simply by the system requirements. (< w,) We have specified the system requirements for the system requirements. (< w,) We have specified the system requirements for the system requirements. (< w, w, w, w, w, w, w) We have specified the system requirements for the system requirements. (< w, w, w) We have specified the system requirements for the system requirements. (< w, w, w) We have specified the system requirements for the system requirements. (< w, w, w, w, w, w) We have specified the system requirements. (< w, w, w) We have specified the system requirements for the system requirements. (< w, w, w, w) We have specified the system requirements for the system requirements. (< w, w, w, w) We have specified the system requirements for the system requirements. (< w, w, w, w, w, w, w, w, w, w, w, w, w, w) We have specified the system requirements for the system requirements. () We have specified the system requirements. (< p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p"}, {"heading": "3 Stability and Learnability in Sparse Coding", "text": "In this section, we look at the sparse encoding in the field of self-taught learning, where the source region essentially consists of sample space XS without the YS tag space. We assume that the sample spaces in both regions are Rd. Then, the sparse encoding method discussed here consists of a two-step process in which a dictionary is learned in the source region, and then a sparse encoding with the learned dictionary for a predictive task in the target region. First, we show that the sparse encoding satisfies local stability in Section 3.1, and second, we explain that appropriate dictionary learning algorithms satisfy the parameter transfer ability in Section 3.4. As a consequence of Theorem 1, we obtain the learning limit of autodidactic learning algorithms based on sparse encoding. We note that the results in this section are useful, independent of transfer algorithms."}, {"heading": "3.1 Local Stability of Sparse Representation", "text": "We show the local stability of the sparse representation under a sparse model. A sparse representation with dictatorial parameter D of a sample x-x-x is expressed as follows: D (x): = argmin z-Rm1 2 x-Dz 2 x-Dz 2 x-Dz. Generally, however, the (p, q) -induced norm for p, q-D 2 x-D agrees with the one in Definition 3 by Vainsencher et al. (2011).where f > 0 is a regulation parameter. This situation corresponds to the case where D and D = D agree with stability in this general definition. We prepare some notes on the stability of the sparse representation. The following margin and incoherence were introduced by Mehta and Gray."}, {"heading": "3.2 Sparse Modeling and Margin Bound", "text": "In this subsection, we assume a sparse structure for samples and give a lower limit for the k margin used in (5). The result obtained in this section plays an essential role in showing the learning ability of the parameter in Section 3.4. Assumption 1 (model). There is a dictionary matrix D *, so each sample x is independent of a representation a and noise asx = D * a +. In addition, we make the following three assumptions on the model above. Assumption 2 (dictionary). The dictionary matrix D * is independent of a representation a and noise asx."}, {"heading": "3.3 Proof of Margin Bound", "text": "We provide a sketch of the proof of theorem 3. We designate the first term, the second term, and the sum of the third and fourth terms of (6) by (1), (2) and (3). From assumption 1 and (3), a sample is represented as x = D \"a\" and \"a.\" Without the loss of generality, we assume that the first m \u2212 k \"components of a\" 0 \"and the last k\" components are not 0. \"Since Mk, D\" (x) min1 \"j\" m \u2212 k \"and\" a \"(3) min1.\""}, {"heading": "3.4 Transfer Learnability for Dictionary Learning", "text": "If the true dictionary D \u00b2 exists as in supposition 1, we show that the output D \u00b2 N of a suitable dictionary learning algorithm from N \u2212 unlabeled samples fulfills the parameter transfer margin for the sparse coding D. Then, Theorem 1 guarantees self-taught learning, since the discussion in this section does not assume the labeling margin in the source region. This situation corresponds to the case in which the parameter transfer margin for the sparse coding D is fulfilled by focusing on the permissible radius of the disturbance in (5) under some assumptions."}, {"heading": "4 Conclusion", "text": "We derived a learning boundary (theorem 1) for a problem of parameter transfer based on the local stability and learning ability of parameter transfer, which are newly introduced in this paper. Then, by applying it to a sparse coding-based algorithm under a sparse model (assumptions 1-4), we obtained the first theoretical guarantee of learning tied to autodidactic learning. Although we consider only sparse coding, the framework of parameter transfer encompasses other promising algorithms such as multiple kernel learning and deep neural networks, and therefore our results are expected to be effective in analyzing the theoretical performance of these algorithms. Finally, we find that our learning boundary can be applied to settings other than autodidactic learning, since theorem 1 encompasses the case where labeled samples are available in the source region."}, {"heading": "A Appendix: Lemma for Proof of Theorem 1", "text": "In this subsection, we leave out the T for simplicity subscript."}, {"heading": "B Appendix: Proof of Sparse Coding Stability", "text": "The proof for theorem 2 is almost the same as that of theorem 1 in Mehta and Gray (2012). However, since part of the proof cannot be applied to our attitude, we provide the complete proof for theorem 2 in this section. \u2212 Lemma 2. Allow an answer to the question whether there is an answer to this question. \u2212 22 \u2212 \u2212 -22 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 22 \u2212 \u2212 \u2212 \u2212 2a-2a-2a-2a-2a-2a-2a-2a-2a-2a-2a-a-2a-2a-2a-2a-2a-a-a-2a-a-a-2a-a-a-2a-a-a-2a-a-2a-a-2a-a-2a-a-2a-a-a-2a-a-a-2a-a-a-2a-a-2a-a-2a-a-a-2a-a-2a-a-a-2a-a-2a-a-2a-a-2a-a-2a-a-2a-a-2a-a-2a-2a-a-a-2a-2a-a-2a-a-2a-2a-a-2a-2a-a-2a-2a-a-a-a-2a-2a-a-2a-a-2a-a-a-2a-2a-a-2a-a-2a-a-a-2a-a-2a-a-a-2a-a-a-2a-2a-2a-a-a-a-2a-a-2a-a-a-2a-a-a-2a-a-a-a-2a-2a-2a-a-a-a-2a-a-2a-a-a-2a-a-2a-a-a-2a-a-a-2a-2a-a-2a-2a-a-2a-2a-a-2a-2a-a-2a-2a"}, {"heading": "C Appendix: Proof of Margin Bound", "text": "In this evidence, we have given the following example: When a dictionary is in a general position, a LASSO solution for an incoherent dictionary is unique due to the fact that Lemma 3 is in Tibshirani et al. (2013).Lemma 9. If a dictionary is a LASSO solution for an incoherent dictionary, then a LASSO solution for an incoherent dictionary is unique due to the fact that Lemma 3 is in Tibshirani et al. (2013).Lemma 9. If a dictionary is a LASSO solution for a incoherent dictionary."}], "references": [{"title": "Simple, efficient, and neural algorithms for sparse coding,", "author": ["S. Arora", "R. Ge", "T. Ma", "A. Moitra"], "venue": "arXiv preprint arXiv:1503.00778", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2015}, {"title": "On the lasso and dantzig selector equivalence,", "author": ["M.S. Asif", "J. Romberg"], "venue": "Information Sciences and Systems (CISS),", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2010}, {"title": "A model of inductive bias learning,", "author": ["J. Baxter"], "venue": "J. Artif. Intell. Res.(JAIR),", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2000}, {"title": "An analysis of single-layer networks in unsupervised feature learning,", "author": ["A. Coates", "A.Y. Ng", "H. Lee"], "venue": "in International conference on artificial intelligence and statistics,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2011}, {"title": "Self-taught clustering,", "author": ["W. Dai", "Q. Yang", "G.-R. Xue", "Y. Yu"], "venue": "Proceedings of the 25th international conference on Machine learning,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2008}, {"title": "Building high-level features using large scale unsupervised learning,", "author": ["V. Q"], "venue": "in Acoustics, Speech and Signal Processing (ICASSP),", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2013}, {"title": "Exponential Family Sparse Coding with Application to Self-taught Learning,", "author": ["H. Lee", "R. Raina", "A. Teichman", "A.Y. Ng"], "venue": "in IJCAI,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2009}, {"title": "Supervised dictionary learning,\u201d in Advances in neural information processing", "author": ["J. Mairal", "J. Ponce", "G. Sapiro", "A. Zisserman", "F.R. Bach"], "venue": null, "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2009}, {"title": "Transfer bounds for linear feature learning,", "author": ["A. Maurer"], "venue": "Machine learning,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2009}, {"title": "Sparse coding for multitask and transfer learning,", "author": ["A. Maurer", "M. Pontil", "B. Romera-Paredes"], "venue": "arXiv preprint arXiv:1209.0738", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2012}, {"title": "A unified framework for highdimensional analysis of M -estimators with decomposable regularizers,", "author": ["S. Negahban", "B. Yu", "M.J. Wainwright", "P.K. Ravikumar"], "venue": "Advances in Neural Information Processing Systems,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2009}, {"title": "A survey on transfer learning,", "author": ["S.J. Pan", "Q. Yang"], "venue": "Knowledge and Data Engineering, IEEE Transactions on,", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2010}, {"title": "Self-taught learning: transfer learning from unlabeled data,", "author": ["R. Raina", "A. Battle", "H. Lee", "B. Packer", "A.Y. Ng"], "venue": "Proceedings of the 24th international conference on Machine learning,", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2007}, {"title": "Fast rates for regularized objectives,", "author": ["K. Sridharan", "S. Shalev-Shwartz", "N. Srebro"], "venue": "Advances in Neural Information Processing Systems,", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2009}, {"title": "The lasso problem and uniqueness,", "author": ["R.J. Tibshirani"], "venue": "Electronic Journal of Statistics,", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2013}, {"title": "The sample complexity of dictionary learning,", "author": ["D. Vainsencher", "S. Mannor", "A.M. Bruckstein"], "venue": "The Journal of Machine Learning Research,", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2011}, {"title": "Robust and discriminative self-taught learning,", "author": ["H. Wang", "F. Nie", "H. Huang"], "venue": "Proceedings of The 30th International Conference on Machine Learning,", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2013}, {"title": "On model selection consistency of Lasso,", "author": ["P. Zhao", "B. Yu"], "venue": "The Journal of Machine Learning Research,", "citeRegEx": "20", "shortCiteRegEx": "20", "year": 2006}], "referenceMentions": [{"referenceID": 0, "context": "References [1] S.", "startOffset": 11, "endOffset": 14}, {"referenceID": 1, "context": "[2] M.", "startOffset": 0, "endOffset": 3}, {"referenceID": 2, "context": "[3] J.", "startOffset": 0, "endOffset": 3}, {"referenceID": 3, "context": "[4] A.", "startOffset": 0, "endOffset": 3}, {"referenceID": 4, "context": "[5] W.", "startOffset": 0, "endOffset": 3}, {"referenceID": 5, "context": "[6] Q.", "startOffset": 0, "endOffset": 3}, {"referenceID": 6, "context": "[7] H.", "startOffset": 0, "endOffset": 3}, {"referenceID": 7, "context": "[8] J.", "startOffset": 0, "endOffset": 3}, {"referenceID": 8, "context": "[9] A.", "startOffset": 0, "endOffset": 3}, {"referenceID": 9, "context": "[10] A.", "startOffset": 0, "endOffset": 4}, {"referenceID": 10, "context": "[13] S.", "startOffset": 0, "endOffset": 4}, {"referenceID": 11, "context": "[14] S.", "startOffset": 0, "endOffset": 4}, {"referenceID": 12, "context": "[15] R.", "startOffset": 0, "endOffset": 4}, {"referenceID": 13, "context": "[16] K.", "startOffset": 0, "endOffset": 4}, {"referenceID": 14, "context": "[17] R.", "startOffset": 0, "endOffset": 4}, {"referenceID": 15, "context": "[18] D.", "startOffset": 0, "endOffset": 4}, {"referenceID": 16, "context": "[19] H.", "startOffset": 0, "endOffset": 4}, {"referenceID": 17, "context": "[20] P.", "startOffset": 0, "endOffset": 4}], "year": 2017, "abstractText": "We consider a transfer-learning problem by using the parameter transfer approach, where a suitable parameter of feature mapping is learned through one task and applied to another objective task. Then, we introduce the notion of the local stability of parametric feature mapping and parameter transfer learnability, and thereby derive a learning bound for parameter transfer algorithms. As an application of parameter transfer learning, we discuss the performance of sparse coding in selftaught learning. Although self-taught learning algorithms with plentiful unlabeled data often show excellent empirical performance, their theoretical analysis has not been studied. In this paper, we also provide the first theoretical learning bound for self-taught learning.", "creator": "LaTeX with hyperref package"}}}