{"id": "1610.09001", "review": {"conference": "NIPS", "VERSION": "v1", "DATE_OF_SUBMISSION": "27-Oct-2016", "title": "SoundNet: Learning Sound Representations from Unlabeled Video", "abstract": "We learn rich natural sound representations by capitalizing on large amounts of unlabeled sound data collected in the wild. We leverage the natural synchronization between vision and sound to learn an acoustic representation using two-million unlabeled videos. Unlabeled video has the advantage that it can be economically acquired at massive scales, yet contains useful signals about natural sound. We propose a student-teacher training procedure which transfers discriminative visual knowledge from well established visual recognition models into the sound modality using unlabeled video as a bridge. Our sound representation yields significant performance improvements over the state-of-the-art results on standard benchmarks for acoustic scene/object classification. Visualizations suggest some high-level semantics automatically emerge in the sound network, even though it is trained without ground truth labels.", "histories": [["v1", "Thu, 27 Oct 2016 20:23:39 GMT  (5939kb,D)", "http://arxiv.org/abs/1610.09001v1", "NIPS 2016"]], "COMMENTS": "NIPS 2016", "reviews": [], "SUBJECTS": "cs.CV cs.LG cs.SD", "authors": ["yusuf aytar", "carl vondrick", "antonio torralba 0001"], "accepted": true, "id": "1610.09001"}, "pdf": {"name": "1610.09001.pdf", "metadata": {"source": "CRF", "title": "SoundNet: Learning Sound Representations from Unlabeled Video", "authors": ["Yusuf Aytar", "Carl Vondrick"], "emails": ["yusuf@csail.mit.edu", "vondrick@mit.edu", "torralba@mit.edu"], "sections": [{"heading": "1 Introduction", "text": "The fields of object recognition, speech recognition, and machine translation have been revolutionized by the emergence of massive labeled data sets [31, 42, 10] and learned deep representations [17, 33, 10, 35]. However, we believe that large-scale sound data can also significantly advance the natural understanding of sound. In this paper, we attribute this in part to the lack of large labeled sound data sets, which are often both expensive and ambiguous to collect. We suggest that large-scale sound data can also promote a natural understanding of sound. In this paper, we use the sounds collected in the wild to learn semantically rich sound representations. We propose to use the natural synchronization between vision and sound to learn an acoustic representation of blank videos."}, {"heading": "1.1 Related Work", "text": "Another major advantage of our approach is that we extend our acoustic network by a considerable categorical diversity [5, 37]. We focus on the understanding of natural, wild sounds. However, the classification of the acoustic scene, the classification of sound extracts into existing acoustic scene / object categories, is largely based on the application of a variety of generic classifiers (SVMs, GMMs, etc.) to artisanal sound functions (MFCC, spectrograms, etc.). Although they are not monitored [20] and monitored [27, 23, 6, 12] deep learning methods are applied to sound classification, the models are limited by the amount of available, labelled natural sound data. We differ from existing literature in that we form a deeply convolutive network of data sets (2M videos)."}, {"heading": "2 Large Unlabeled Video Dataset", "text": "Although there are a variety of sources on the Internet (e.g. YouTube, Flickr), we chose Flickr videos because they are natural, non-professionally edited short clips that capture various sounds in everyday situations in the wild. We have downloaded over two million videos from Flickr by asking for popular keywords [36] and dictionary words, which resulted in over a year of continuous natural sound and video that we use for training. The length of each video varies from a few seconds to several minutes. We show a small selection of images from the video dataset in Figure 2. We want to process raw sound waves, so the only post-processing we did for the videos was to convert audio to MP3s, reduce the sampling rate to 22 kHz, and convert to single-channel sound. Although this slightly degrades the quality of the sound, it allows us to work more efficiently on large datasets that have already been sorted from 256 to 256."}, {"heading": "3 Learning Sound Representations", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "3.1 Deep Convolutional Sound Network", "text": "We propose to use a series of one-dimensional windings, followed by non-linearity (i.e. ReLU layer), to process sound. Winding networks are good for audio signals for several reasons. First, like images [19], we want our network to be invariant for translations, a property that reduces the number of parameters we need to learn and increases efficiency. Second, winding networks allow us to stack layers, which allows us to detect concepts at a higher level through a series of winding detectors at a lower level. Variable Lengths Input / Output: Since sound can vary in time length, we want our network to be able to handle inputs of variable length. To do this, we use a completely revolutionary network that is immutable for the location."}, {"heading": "3.2 Visual Transfer into Sound", "text": "We model the learning problem from the perspective of a student / teacher. In our case, state-of-the-art networks for vision will teach our network to recognize scenes and objects in sound. Let's let xi-RD be a waveform and yi-R3 \u00b7 T \u00b7 W \u00b7 H be its corresponding video for 1 \u2264 i \u2264 N, in which W, H, T are width, height and number of images scanned in the video. During learning, we aim to use the rear probabilities of a teacher-vision network gk (yi) to train our student network fk (xi) to recognize predetermined concepts. Since we want to transfer knowledge from both object and scene networks, k enumerates the concepts we transmit. During learning, we optimize the mine networks Kk = 1-N = 1DKL (gk (Qyj categories) and Pjk networks can be used as a distraction tool."}, {"heading": "3.3 Sound Classification", "text": "Although we train SoundNet to classify visual categories, the categories we want to detect may not appear in visual models (e.g. sneezing), so we use a different strategy to assign a semantic meaning to sounds. We ignore the output level of our network and use internal representation as a feature for the training classifiers, using a small amount of wired sound data for the interesting concepts. We select a layer in the network to use as features, and train a linear SVM. For classification into several classes, we use a one-on-all strategy. We perform cross-validations to select the hyperparameter of margin control. To ensure robustness, we follow a standard data augmentation process where each training sample is divided into overlapping audio snippets of fixed length, on which we calculate features and use for training."}, {"heading": "3.4 Implementation", "text": "Our approach is implemented in Torch7. We used the Adam [16] optimizer and a fixed learning rate of 0.001 and an impulse term of 0.9 during our experiments. We experimented with several lot sizes and found 64 that yielded good results. We initialized all weights to zero, the mean Gaussian noise with a standard deviation of 0.01. After each fold, we used the lot normalization [15] and corrected linear activation units [17]. We trained the network for 100,000 iterations. Optimization typically took a day on a GPU."}, {"heading": "4 Experiments", "text": "Experimental Setup: We split the blank video data set into a training set and a pre-set validation set. We use 2,000 000 videos for training and the remaining 140,000 videos for validation. After training the network, we use the hidden representation as a feature extractor to learn on smaller, labeled sound data sets. We extract features for a specific layer and train an SVM on the task of interest. For training the SVM, we use the standard training / test splits of the data sets. We report on classification accuracy. Baselines:: In addition to published baselines on standard data sets, we explore an additional baseline that was trained on our unlabeled videos. We experimented with a revolutionary auto-encoder for sound, trained on our video data set. We use an auto-encoder with 4 encoder layers and 4 decoder layers. For the auto-coders of the first four layers, we have used the same kind of auto-coders for loss."}, {"heading": "4.1 Acoustic Scene Classification", "text": "In fact, it is so that most people are able to determine for themselves what they want and what they do not want. (...) It is not so that people are able to decide what they want and what they do not want. (...) It is not so that they want it. (...) It is not so that they want it. (...) It is not so that they want it. (...) It is so that they want it. (...) It is so. \"(...).\" \"(...).\" (...). \"(.).\" (.). \"(.).\" (.). \"(.).\" (.). \"(.).\" (.). \"(.).\" (.). \"(.).\" (. \"(.).\" (.). \"(.\" (.). \"(.).\" (. \"(.).\" (. \"(.).\" (. \"(.).\" (.). \"(.\" (.). \"(.).\" (. \"(.).\" (.). \"(.\" (.). \"(.).\" (.). \"(.).\" (.). \"(.\"). \"(.).\" (.). \"(.).\" (.). \"(.).\" (.). \"(.).\" (.). \"(.).\" (.). \"(.).\" (.). \"(.\"). \"(.).\" (. \"(.).).\" (.). \"(.).\" (.). \"(.).\" (.).). \"(.\" (.). \"(.).).\" (.). \"(.).\" (. \").\" (.)."}, {"heading": "4.2 Ablation Analysis", "text": "In order to better understand our approach, we perform an ablation analysis in Table 5 and Table 6. Comparison of loss and teacher network (Table 5): We have tried to conduct training with different subsets of target categories. Generally, performance improves as visual monitoring increases. As expected, our results suggest that using ImageNet and Places networks as supervision is better than using a single one. This suggests that progress in the area of acoustic understanding can be encouraged by building stronger vision models. We have also experimented with using \"2 losses on the target starting from KL losses, which were significant. Comparison of network depth (Table 5) shows the effects of network depth. We use five layers version of SoundNet (instead of the full eight) as feature extractor. The five-layer SoundNet architecture is 8% worse than the eight-layer architecture that suggests depth."}, {"heading": "4.3 Multi-Modal Recognition", "text": "This new dataset consists of 44 categories of 6 large groups of concepts (i.e. city, nature, work / home, music / entertainment, sports and vehicles).To demonstrate the semantic relevance of the characteristics, we performed a two-dimensional t-SNE [38] embedding and visualization of our datasets in Figure 4. The visual characteristics are associated with fc7 characteristics of the two VGG networks trained with the help of ImageNet and Places2. The visual characteristics of the two VGG networks are also associated with ImageNet and Places2 datas.We calculated the visual characteristics from uniformly selected 4 frames and calculated the middle characteristic as the final visual representation.The sound characteristics are compared with considerable characteristics performed with the help of GS2 datasets."}, {"heading": "4.4 Visualizations", "text": "The learned filters are manifold, including low and high frequencies, wavelike patterns, increasing and decreasing amplitude filters. We also visualize some of the hidden units in the last hidden layer (Conve7) of our sound representation by finding inputs that maximally activate a hidden unit. This visualization is shown in Figure 6. Note that visual frames are not used in the calculation of activations; they are included in the image for visualization purposes only."}, {"heading": "5 Conclusion", "text": "Our results show that unlabeled video transfer is a strong paradigm for learning sound. All of our experiments suggest that you can achieve better performance simply by downloading more videos, creating deeper networks, and using richer vision models. Acknowledgements: We thank MIT TIG, especially Garrett Wollman, for helping us store 26TB of video. We are grateful for the GPUs donated by NVidia. This work has been supported by the NSF Grant # 1524817 at AT and the Google PhD Fellowship at CV."}], "references": [{"title": "Tabula rasa: Model transfer for object category detection", "author": ["Yusuf Aytar", "Andrew Zisserman"], "venue": "In ICCV,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2011}, {"title": "Part level transfer regularization for enhancing exemplar svms", "author": ["Yusuf Aytar", "Andrew Zisserman"], "venue": "CVIU,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2015}, {"title": "Do deep nets really need to be deep", "author": ["Jimmy Ba", "Rich Caruana"], "venue": "In NIPS,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2014}, {"title": "Acoustic scene classification: Classifying environments from the sounds", "author": ["Daniele Barchiesi", "Dimitrios Giannoulis", "Dan Stowell", "Mark D Plumbley"], "venue": null, "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2015}, {"title": "The million song dataset", "author": ["Thierry Bertin-Mahieux", "Daniel PW Ellis", "Brian Whitman", "Paul Lamere"], "venue": "In ISMIR,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2011}, {"title": "Polyphonic sound event detection using multi label deep neural networks", "author": ["Emre Cakir", "Toni Heittola", "Heikki Huttunen", "Tuomas Virtanen"], "venue": "In IJCNN,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2015}, {"title": "Learning aligned cross-modal representations from weakly aligned data", "author": ["Lluis Castrejon", "Yusuf Aytar", "Carl Vondrick", "Hamed Pirsiavash", "Antonio Torralba"], "venue": null, "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2016}, {"title": "Watching unlabeled video helps learn new human actions from very few labeled snapshots", "author": ["Chao-Yeh Chen", "Kristen Grauman"], "venue": "In CVPR,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2013}, {"title": "Cross modal distillation for supervision transfer", "author": ["Saurabh Gupta", "Judy Hoffman", "Jitendra Malik"], "venue": "arXiv preprint arXiv:1507.00448,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2015}, {"title": "Deep speech: Scaling up end-to-end speech recognition", "author": ["Awni Hannun", "Carl Case", "Jared Casper", "Bryan Catanzaro", "Greg Diamos", "Erich Elsen", "Ryan Prenger", "Sanjeev Satheesh", "Shubho Sengupta", "Adam Coates"], "venue": "arXiv preprint arXiv:1412.5567,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2014}, {"title": "Cnn architectures for large-scale audio classification", "author": ["Shawn Hershey", "Sourish Chaudhuri", "Daniel PW Ellis", "Jort F Gemmeke", "Aren Jansen", "R Channing Moore", "Manoj Plakal", "Devin Platt", "Rif A Saurous", "Bryan Seybold"], "venue": "arXiv, 2016", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2016}, {"title": "Comparing time and frequency domain for audio event recognition using deep learning", "author": ["Lars Hertel", "Huy Phan", "Alfred Mertins"], "venue": null, "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2016}, {"title": "Distilling the knowledge in a neural network", "author": ["Geoffrey Hinton", "Oriol Vinyals", "Jeff Dean"], "venue": null, "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2015}, {"title": "Audio-visual deep learning for noise robust speech recognition", "author": ["Jing Huang", "Brian Kingsbury"], "venue": "In ICASSP,", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2013}, {"title": "Batch normalization: Accelerating deep network training by reducing internal covariate", "author": ["Sergey Ioffe", "Christian Szegedy"], "venue": "shift. arXiv,", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2015}, {"title": "Adam: A method for stochastic optimization", "author": ["Diederik Kingma", "Jimmy Ba"], "venue": "arXiv preprint arXiv:1412.6980,", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2014}, {"title": "Imagenet classification with deep convolutional neural networks", "author": ["Alex Krizhevsky", "Ilya Sutskever", "Geoffrey E Hinton"], "venue": "In NIPS,", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2012}, {"title": "Figure-ground segmentation by transferring window masks", "author": ["Daniel Kuettel", "Vittorio Ferrari"], "venue": "In CVPR,", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2012}, {"title": "Gradient-based learning applied to document recognition", "author": ["Yann LeCun", "L\u00e9on Bottou", "Yoshua Bengio", "Patrick Haffner"], "venue": null, "citeRegEx": "19", "shortCiteRegEx": "19", "year": 1998}, {"title": "Unsupervised feature learning for audio classification using convolutional deep belief networks", "author": ["Honglak Lee", "Peter Pham", "Yan Largman", "Andrew Y Ng"], "venue": "In NIPS,", "citeRegEx": "20", "shortCiteRegEx": "20", "year": 2009}, {"title": "Auditory scene classification using machine learning techniques", "author": ["David Li", "Jason Tam", "Derek Toub"], "venue": "AASP Challenge,", "citeRegEx": "21", "shortCiteRegEx": "21", "year": 2013}, {"title": "Fully convolutional networks for semantic segmentation", "author": ["Jonathan Long", "Evan Shelhamer", "Trevor Darrell"], "venue": "In CVPR,", "citeRegEx": "22", "shortCiteRegEx": "22", "year": 2015}, {"title": "Robust sound event classification using deep neural networks", "author": ["Ian McLoughlin", "Haomin Zhang", "Zhipeng Xie", "Yan Song", "Wei Xiao"], "venue": "ASL,", "citeRegEx": "23", "shortCiteRegEx": "23", "year": 2015}, {"title": "Multimodal deep learning", "author": ["Jiquan Ngiam", "Aditya Khosla", "Mingyu Kim", "Juhan Nam", "Honglak Lee", "Andrew Y Ng"], "venue": "In ICML,", "citeRegEx": "24", "shortCiteRegEx": "24", "year": 2011}, {"title": "The open world of micro-videos", "author": ["Phuc Xuan Nguyen", "Gregory Rogez", "Charless Fowlkes", "Deva Ramamnan"], "venue": null, "citeRegEx": "25", "shortCiteRegEx": "25", "year": 2016}, {"title": "Visually indicated sounds", "author": ["Andrew Owens", "Phillip Isola", "Josh McDermott", "Antonio Torralba", "Edward H Adelson", "William T Freeman"], "venue": "arXiv preprint arXiv:1512.08512,", "citeRegEx": "26", "shortCiteRegEx": "26", "year": 2015}, {"title": "Environmental sound classification with convolutional neural networks", "author": ["Karol J Piczak"], "venue": "In MLSP,", "citeRegEx": "27", "shortCiteRegEx": "27", "year": 2015}, {"title": "Esc: Dataset for environmental sound classification", "author": ["Karol J Piczak"], "venue": "In ACM Multimedia,", "citeRegEx": "28", "shortCiteRegEx": "28", "year": 2015}, {"title": "Histogram of gradients of time-frequency representations for audio scene classification", "author": ["Alain Rakotomamonjy", "Gilles Gasso"], "venue": "TASLP,", "citeRegEx": "29", "shortCiteRegEx": "29", "year": 2015}, {"title": "Recurrence quantification analysis features for environmental sound recognition", "author": ["Guido Roma", "Waldo Nogueira", "Perfecto Herrera"], "venue": "In WASPAA,", "citeRegEx": "30", "shortCiteRegEx": "30", "year": 2013}, {"title": "Imagenet large scale visual recognition challenge", "author": ["Olga Russakovsky", "Jia Deng", "Hao Su", "Jonathan Krause", "Sanjeev Satheesh", "Sean Ma", "Zhiheng Huang", "Andrej Karpathy", "Aditya Khosla", "Michael Bernstein"], "venue": null, "citeRegEx": "31", "shortCiteRegEx": "31", "year": 2015}, {"title": "Unsupervised feature learning for urban sound classification", "author": ["Justin Salamon", "Juan Pablo Bello"], "venue": "In ICASSP,", "citeRegEx": "32", "shortCiteRegEx": "32", "year": 2015}, {"title": "Very deep convolutional networks for large-scale image recognition", "author": ["Karen Simonyan", "Andrew Zisserman"], "venue": "arXiv preprint arXiv:1409.1556,", "citeRegEx": "33", "shortCiteRegEx": "33", "year": 2014}, {"title": "Detection and classification of acoustic scenes and events", "author": ["Dan Stowell", "Dimitrios Giannoulis", "Emmanouil Benetos", "Mathieu Lagrange", "Mark D Plumbley"], "venue": "TM,", "citeRegEx": "34", "shortCiteRegEx": "34", "year": 2015}, {"title": "Sequence to sequence learning with neural networks", "author": ["Ilya Sutskever", "Oriol Vinyals", "Quoc V Le"], "venue": "In NIPS,", "citeRegEx": "35", "shortCiteRegEx": "35", "year": 2014}, {"title": "Yfcc100m: The new data in multimedia research", "author": ["Bart Thomee", "David A Shamma", "Gerald Friedland", "Benjamin Elizalde", "Karl Ni", "Douglas Poland", "Damian Borth", "Li-Jia Li"], "venue": "Communications of the ACM,", "citeRegEx": "36", "shortCiteRegEx": "36", "year": 2016}, {"title": "Deep content-based music recommendation", "author": ["Aaron Van den Oord", "Sander Dieleman", "Benjamin Schrauwen"], "venue": "In NIPS,", "citeRegEx": "37", "shortCiteRegEx": "37", "year": 2013}, {"title": "Visualizing data using t-sne", "author": ["Laurens Van der Maaten", "Geoffrey Hinton"], "venue": null, "citeRegEx": "38", "shortCiteRegEx": "38", "year": 2008}, {"title": "Anticipating visual representations from unlabeled video", "author": ["Carl Vondrick", "Hamed Pirsiavash", "Antonio Torralba"], "venue": null, "citeRegEx": "39", "shortCiteRegEx": "39", "year": 2016}, {"title": "Generating videos with scene dynamics", "author": ["Carl Vondrick", "Hamed Pirsiavash", "Antonio Torralba"], "venue": null, "citeRegEx": "40", "shortCiteRegEx": "40", "year": 2016}, {"title": "Dense optical flow prediction from a static image", "author": ["Jacob Walker", "Abhinav Gupta", "Martial Hebert"], "venue": "In ICCV,", "citeRegEx": "41", "shortCiteRegEx": "41", "year": 2015}, {"title": "Learning deep features for scene recognition using places database", "author": ["Bolei Zhou", "Agata Lapedriza", "Jianxiong Xiao", "Antonio Torralba", "Aude Oliva"], "venue": "In NIPS,", "citeRegEx": "42", "shortCiteRegEx": "42", "year": 2014}], "referenceMentions": [{"referenceID": 30, "context": "The fields of object recognition, speech recognition, machine translation have been revolutionized by the emergence of massive labeled datasets [31, 42, 10] and learned deep representations [17, 33, 10, 35].", "startOffset": 144, "endOffset": 156}, {"referenceID": 41, "context": "The fields of object recognition, speech recognition, machine translation have been revolutionized by the emergence of massive labeled datasets [31, 42, 10] and learned deep representations [17, 33, 10, 35].", "startOffset": 144, "endOffset": 156}, {"referenceID": 9, "context": "The fields of object recognition, speech recognition, machine translation have been revolutionized by the emergence of massive labeled datasets [31, 42, 10] and learned deep representations [17, 33, 10, 35].", "startOffset": 144, "endOffset": 156}, {"referenceID": 16, "context": "The fields of object recognition, speech recognition, machine translation have been revolutionized by the emergence of massive labeled datasets [31, 42, 10] and learned deep representations [17, 33, 10, 35].", "startOffset": 190, "endOffset": 206}, {"referenceID": 32, "context": "The fields of object recognition, speech recognition, machine translation have been revolutionized by the emergence of massive labeled datasets [31, 42, 10] and learned deep representations [17, 33, 10, 35].", "startOffset": 190, "endOffset": 206}, {"referenceID": 9, "context": "The fields of object recognition, speech recognition, machine translation have been revolutionized by the emergence of massive labeled datasets [31, 42, 10] and learned deep representations [17, 33, 10, 35].", "startOffset": 190, "endOffset": 206}, {"referenceID": 34, "context": "The fields of object recognition, speech recognition, machine translation have been revolutionized by the emergence of massive labeled datasets [31, 42, 10] and learned deep representations [17, 33, 10, 35].", "startOffset": 190, "endOffset": 206}, {"referenceID": 4, "context": "Sound Recognition: Although large-scale audio understanding has been extensively studied in the context of music [5, 37] and speech recognition [10], we focus on understanding natural, in-the-wild sounds.", "startOffset": 113, "endOffset": 120}, {"referenceID": 36, "context": "Sound Recognition: Although large-scale audio understanding has been extensively studied in the context of music [5, 37] and speech recognition [10], we focus on understanding natural, in-the-wild sounds.", "startOffset": 113, "endOffset": 120}, {"referenceID": 9, "context": "Sound Recognition: Although large-scale audio understanding has been extensively studied in the context of music [5, 37] and speech recognition [10], we focus on understanding natural, in-the-wild sounds.", "startOffset": 144, "endOffset": 148}, {"referenceID": 3, "context": ") [4, 29, 21, 30, 34, 32].", "startOffset": 2, "endOffset": 25}, {"referenceID": 28, "context": ") [4, 29, 21, 30, 34, 32].", "startOffset": 2, "endOffset": 25}, {"referenceID": 20, "context": ") [4, 29, 21, 30, 34, 32].", "startOffset": 2, "endOffset": 25}, {"referenceID": 29, "context": ") [4, 29, 21, 30, 34, 32].", "startOffset": 2, "endOffset": 25}, {"referenceID": 33, "context": ") [4, 29, 21, 30, 34, 32].", "startOffset": 2, "endOffset": 25}, {"referenceID": 31, "context": ") [4, 29, 21, 30, 34, 32].", "startOffset": 2, "endOffset": 25}, {"referenceID": 19, "context": "Even though there are unsupervised [20] and supervised [27, 23, 6, 12] deep learning methods applied to sound classification, the models are limited by the amount of available labeled natural sound data.", "startOffset": 35, "endOffset": 39}, {"referenceID": 26, "context": "Even though there are unsupervised [20] and supervised [27, 23, 6, 12] deep learning methods applied to sound classification, the models are limited by the amount of available labeled natural sound data.", "startOffset": 55, "endOffset": 70}, {"referenceID": 22, "context": "Even though there are unsupervised [20] and supervised [27, 23, 6, 12] deep learning methods applied to sound classification, the models are limited by the amount of available labeled natural sound data.", "startOffset": 55, "endOffset": 70}, {"referenceID": 5, "context": "Even though there are unsupervised [20] and supervised [27, 23, 6, 12] deep learning methods applied to sound classification, the models are limited by the amount of available labeled natural sound data.", "startOffset": 55, "endOffset": 70}, {"referenceID": 11, "context": "Even though there are unsupervised [20] and supervised [27, 23, 6, 12] deep learning methods applied to sound classification, the models are limited by the amount of available labeled natural sound data.", "startOffset": 55, "endOffset": 70}, {"referenceID": 32, "context": "Another key advantage of our approach is that we supervise our sound recognition network through semantically rich visual discriminative models [33, 17] which proved their robustness on a variety of large scale object/scene categorization challenges[31, 42].", "startOffset": 144, "endOffset": 152}, {"referenceID": 16, "context": "Another key advantage of our approach is that we supervise our sound recognition network through semantically rich visual discriminative models [33, 17] which proved their robustness on a variety of large scale object/scene categorization challenges[31, 42].", "startOffset": 144, "endOffset": 152}, {"referenceID": 30, "context": "Another key advantage of our approach is that we supervise our sound recognition network through semantically rich visual discriminative models [33, 17] which proved their robustness on a variety of large scale object/scene categorization challenges[31, 42].", "startOffset": 249, "endOffset": 257}, {"referenceID": 41, "context": "Another key advantage of our approach is that we supervise our sound recognition network through semantically rich visual discriminative models [33, 17] which proved their robustness on a variety of large scale object/scene categorization challenges[31, 42].", "startOffset": 249, "endOffset": 257}, {"referenceID": 25, "context": "[26] also investigates the relation between vision and sound modalities, but focuses on producing sound from image sequences.", "startOffset": 0, "endOffset": 4}, {"referenceID": 10, "context": "Concurrent work [11] also explores video as a form of weak labeling for audio event classification.", "startOffset": 16, "endOffset": 20}, {"referenceID": 0, "context": "Transfer Learning: Transfer learning is widely studied within computer vision such as transferring knowledge for object detection [1, 2] and segmentation [18], however transferring from vision to other modalities are only possible recently with the emergence of high performance visual models [33, 17].", "startOffset": 130, "endOffset": 136}, {"referenceID": 1, "context": "Transfer Learning: Transfer learning is widely studied within computer vision such as transferring knowledge for object detection [1, 2] and segmentation [18], however transferring from vision to other modalities are only possible recently with the emergence of high performance visual models [33, 17].", "startOffset": 130, "endOffset": 136}, {"referenceID": 17, "context": "Transfer Learning: Transfer learning is widely studied within computer vision such as transferring knowledge for object detection [1, 2] and segmentation [18], however transferring from vision to other modalities are only possible recently with the emergence of high performance visual models [33, 17].", "startOffset": 154, "endOffset": 158}, {"referenceID": 32, "context": "Transfer Learning: Transfer learning is widely studied within computer vision such as transferring knowledge for object detection [1, 2] and segmentation [18], however transferring from vision to other modalities are only possible recently with the emergence of high performance visual models [33, 17].", "startOffset": 293, "endOffset": 301}, {"referenceID": 16, "context": "Transfer Learning: Transfer learning is widely studied within computer vision such as transferring knowledge for object detection [1, 2] and segmentation [18], however transferring from vision to other modalities are only possible recently with the emergence of high performance visual models [33, 17].", "startOffset": 293, "endOffset": 301}, {"referenceID": 2, "context": "Our method builds upon teacher-student models [3, 9] and dark knowledge transfer [13].", "startOffset": 46, "endOffset": 52}, {"referenceID": 8, "context": "Our method builds upon teacher-student models [3, 9] and dark knowledge transfer [13].", "startOffset": 46, "endOffset": 52}, {"referenceID": 12, "context": "Our method builds upon teacher-student models [3, 9] and dark knowledge transfer [13].", "startOffset": 81, "endOffset": 85}, {"referenceID": 2, "context": "In [3, 13] the basic idea is to compress (i.", "startOffset": 3, "endOffset": 10}, {"referenceID": 12, "context": "In [3, 13] the basic idea is to compress (i.", "startOffset": 3, "endOffset": 10}, {"referenceID": 2, "context": "In [3] and [13] both the teacher and the student are in the same modality, whereas in our approach the teacher operates on vision to train the student model in sound.", "startOffset": 3, "endOffset": 6}, {"referenceID": 12, "context": "In [3] and [13] both the teacher and the student are in the same modality, whereas in our approach the teacher operates on vision to train the student model in sound.", "startOffset": 11, "endOffset": 15}, {"referenceID": 8, "context": "[9] also transfer visual supervision into depth models.", "startOffset": 0, "endOffset": 3}, {"referenceID": 23, "context": "Cross-Modal Learning and Unlabeled Video: Our approach is broadly inspired by efforts to model cross-modal relations [24, 14, 7, 26] and works that leverage large amounts of unlabeled video [25, 41, 8, 40, 39].", "startOffset": 117, "endOffset": 132}, {"referenceID": 13, "context": "Cross-Modal Learning and Unlabeled Video: Our approach is broadly inspired by efforts to model cross-modal relations [24, 14, 7, 26] and works that leverage large amounts of unlabeled video [25, 41, 8, 40, 39].", "startOffset": 117, "endOffset": 132}, {"referenceID": 6, "context": "Cross-Modal Learning and Unlabeled Video: Our approach is broadly inspired by efforts to model cross-modal relations [24, 14, 7, 26] and works that leverage large amounts of unlabeled video [25, 41, 8, 40, 39].", "startOffset": 117, "endOffset": 132}, {"referenceID": 25, "context": "Cross-Modal Learning and Unlabeled Video: Our approach is broadly inspired by efforts to model cross-modal relations [24, 14, 7, 26] and works that leverage large amounts of unlabeled video [25, 41, 8, 40, 39].", "startOffset": 117, "endOffset": 132}, {"referenceID": 24, "context": "Cross-Modal Learning and Unlabeled Video: Our approach is broadly inspired by efforts to model cross-modal relations [24, 14, 7, 26] and works that leverage large amounts of unlabeled video [25, 41, 8, 40, 39].", "startOffset": 190, "endOffset": 209}, {"referenceID": 40, "context": "Cross-Modal Learning and Unlabeled Video: Our approach is broadly inspired by efforts to model cross-modal relations [24, 14, 7, 26] and works that leverage large amounts of unlabeled video [25, 41, 8, 40, 39].", "startOffset": 190, "endOffset": 209}, {"referenceID": 7, "context": "Cross-Modal Learning and Unlabeled Video: Our approach is broadly inspired by efforts to model cross-modal relations [24, 14, 7, 26] and works that leverage large amounts of unlabeled video [25, 41, 8, 40, 39].", "startOffset": 190, "endOffset": 209}, {"referenceID": 39, "context": "Cross-Modal Learning and Unlabeled Video: Our approach is broadly inspired by efforts to model cross-modal relations [24, 14, 7, 26] and works that leverage large amounts of unlabeled video [25, 41, 8, 40, 39].", "startOffset": 190, "endOffset": 209}, {"referenceID": 38, "context": "Cross-Modal Learning and Unlabeled Video: Our approach is broadly inspired by efforts to model cross-modal relations [24, 14, 7, 26] and works that leverage large amounts of unlabeled video [25, 41, 8, 40, 39].", "startOffset": 190, "endOffset": 209}, {"referenceID": 35, "context": "We downloaded over two million videos from Flickr by querying for popular tags [36] and dictionary words, which resulted in over one year of continuous natural sound and video, which we use for training.", "startOffset": 79, "endOffset": 83}, {"referenceID": 18, "context": "Firstly, like images [19], we desire our network to be invariant to translations, a property that reduces the number of parameters we need to learn and increases efficiency.", "startOffset": 21, "endOffset": 25}, {"referenceID": 36, "context": "While we could have used a global pooling strategy [37] to down-sample variable length inputs to a fixed dimensional vector, such a strategy may unnecessarily discard information useful for high-level representations.", "startOffset": 51, "endOffset": 55}, {"referenceID": 21, "context": "This strategy is similar to a spatial loss in images [22], but instead temporally.", "startOffset": 53, "endOffset": 57}, {"referenceID": 18, "context": "As KL-divergence is differentiable, we optimize it using back-propagation [19] and stochastic gradient descent.", "startOffset": 74, "endOffset": 78}, {"referenceID": 15, "context": "We use the Adam [16] optimizer and a fixed learning rate of 0.", "startOffset": 16, "endOffset": 20}, {"referenceID": 14, "context": "After every convolution, we use batch normalization [15] and rectified linear activation units [17].", "startOffset": 52, "endOffset": 56}, {"referenceID": 16, "context": "After every convolution, we use batch normalization [15] and rectified linear activation units [17].", "startOffset": 95, "endOffset": 99}, {"referenceID": 28, "context": "Method Accuracy RG [29] 69%", "startOffset": 19, "endOffset": 23}, {"referenceID": 20, "context": "LTT [21] 72%", "startOffset": 4, "endOffset": 8}, {"referenceID": 29, "context": "RNH [30] 77%", "startOffset": 4, "endOffset": 8}, {"referenceID": 33, "context": "Ensemble [34] 78% SoundNet 88% Table 3: Acoustic Scene Classification on DCASE: We evaluate classification accuracy on the DCASE dataset.", "startOffset": 9, "endOffset": 13}, {"referenceID": 27, "context": "Accuracy on Method ESC-50 ESC-10 SVM-MFCC [28] 39.", "startOffset": 42, "endOffset": 46}, {"referenceID": 27, "context": "3% Random Forest [28] 44.", "startOffset": 17, "endOffset": 21}, {"referenceID": 26, "context": "Piczak ConvNet [27] 64.", "startOffset": 15, "endOffset": 19}, {"referenceID": 27, "context": "2% Human Performance [28] 81.", "startOffset": 21, "endOffset": 25}, {"referenceID": 33, "context": "We use three standard, publicly available datasets: DCASE Challenge[34], ESC-50 [28], and ESC-10 [28].", "startOffset": 67, "endOffset": 71}, {"referenceID": 27, "context": "We use three standard, publicly available datasets: DCASE Challenge[34], ESC-50 [28], and ESC-10 [28].", "startOffset": 80, "endOffset": 84}, {"referenceID": 27, "context": "We use three standard, publicly available datasets: DCASE Challenge[34], ESC-50 [28], and ESC-10 [28].", "startOffset": 97, "endOffset": 101}, {"referenceID": 33, "context": "DCASE[34]: One of the tasks in the Detection and Classification of Acoustic Scenes and Events Challenge (DCASE)[34] is to recognize scenes from natural sounds.", "startOffset": 5, "endOffset": 9}, {"referenceID": 33, "context": "DCASE[34]: One of the tasks in the Detection and Classification of Acoustic Scenes and Events Challenge (DCASE)[34] is to recognize scenes from natural sounds.", "startOffset": 111, "endOffset": 115}, {"referenceID": 27, "context": "Figure 3: SoundNet confusions on ESC-50 ESC-50 and ESC-10 [28]: The ESC-50 dataset is a collection of 2000 short (5 seconds) environmental sound recordings of equally balanced 50 categories selected from 5 major groups (animals, natural soundscapes, human non-speech sounds, interior/domestic sounds, and exterior/urban noises).", "startOffset": 58, "endOffset": 62}, {"referenceID": 27, "context": "3% [28].", "startOffset": 3, "endOffset": 7}, {"referenceID": 33, "context": "Dataset Model conv4 conv5 pool5 conv6 conv7 conv8 DCASE [34] 8 Layer, AlexNet 84% 85% 84% 83% 78% 68% 8 Layer, VGG 77% 88% 88% 87% 84% 74%", "startOffset": 56, "endOffset": 60}, {"referenceID": 27, "context": "ESC50 [28] 8 Layer, AlexNet 66.", "startOffset": 6, "endOffset": 10}, {"referenceID": 26, "context": "The five layer SoundNet achieves slightly better results than [27] which is also a convolutional network trained with same data but with a different architecture, suggesting our five layer architecture is similar.", "startOffset": 62, "endOffset": 66}, {"referenceID": 37, "context": "Sound Embeddings: In order to show the semantic relevance of the features, we performed a two dimensional t-SNE [38] embedding and visualized our dataset in figure 4.", "startOffset": 112, "endOffset": 116}], "year": 2016, "abstractText": "We learn rich natural sound representations by capitalizing on large amounts of unlabeled sound data collected in the wild. We leverage the natural synchronization between vision and sound to learn an acoustic representation using two-million unlabeled videos. Unlabeled video has the advantage that it can be economically acquired at massive scales, yet contains useful signals about natural sound. We propose a student-teacher training procedure which transfers discriminative visual knowledge from well established visual recognition models into the sound modality using unlabeled video as a bridge. Our sound representation yields significant performance improvements over the state-of-the-art results on standard benchmarks for acoustic scene/object classification. Visualizations suggest some high-level semantics automatically emerge in the sound network, even though it is trained without ground truth labels.", "creator": "LaTeX with hyperref package"}}}