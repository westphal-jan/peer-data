{"id": "1610.09027", "review": {"conference": "NIPS", "VERSION": "v1", "DATE_OF_SUBMISSION": "27-Oct-2016", "title": "Scaling Memory-Augmented Neural Networks with Sparse Reads and Writes", "abstract": "Neural networks augmented with external memory have the ability to learn algorithmic solutions to complex tasks. These models appear promising for applications such as language modeling and machine translation. However, they scale poorly in both space and time as the amount of memory grows --- limiting their applicability to real-world domains. Here, we present an end-to-end differentiable memory access scheme, which we call Sparse Access Memory (SAM), that retains the representational power of the original approaches whilst training efficiently with very large memories. We show that SAM achieves asymptotic lower bounds in space and time complexity, and find that an implementation runs $1,\\!000\\times$ faster and with $3,\\!000\\times$ less physical memory than non-sparse models. SAM learns with comparable data efficiency to existing models on a range of synthetic tasks and one-shot Omniglot character recognition, and can scale to tasks requiring $100,\\!000$s of time steps and memories. As well, we show how our approach can be adapted for models that maintain temporal associations between memories, as with the recently introduced Differentiable Neural Computer.", "histories": [["v1", "Thu, 27 Oct 2016 22:38:05 GMT  (2240kb,D)", "http://arxiv.org/abs/1610.09027v1", "in 30th Conference on Neural Information Processing Systems (NIPS 2016), Barcelona, Spain"]], "COMMENTS": "in 30th Conference on Neural Information Processing Systems (NIPS 2016), Barcelona, Spain", "reviews": [], "SUBJECTS": "cs.LG", "authors": ["jack w rae", "jonathan j hunt", "ivo danihelka", "timothy harley", "andrew w senior", "gregory wayne", "alex graves", "tim lillicrap"], "accepted": true, "id": "1610.09027"}, "pdf": {"name": "1610.09027.pdf", "metadata": {"source": "CRF", "title": "Scaling Memory-Augmented Neural Networks with Sparse Reads and Writes", "authors": ["Jack W Rae", "Jonathan J Hunt", "Tim Harley tharley", "Ivo Danihelka", "Timothy P Lillicrap"], "emails": ["@google.com"], "sections": [{"heading": "1 Introduction", "text": "In fact, one limitation of the LSTM architecture is that the number of parameters increases proportionally to the square of the storage capacity, rendering it unsuitable for problems requiring large amounts of long-term memory. We refer to this class of models as augmented neural networks (MANNs). External storage allows MANNs to find algorithmic solutions to problems that decouple the storage capacity from the number of model parameters. We point out that the memory is augmented."}, {"heading": "2 Background", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "2.1 Attention and content-based addressing", "text": "An external memory M = RN \u00b7 M is a collection of N real vectors or words of fixed size M. A soft reading is defined as a weighted average over memory words, r = N \u2211 i = 1 w (i) M (i), (1) where w-RN is a vector of weights with non-negative entries adding up to one. Memory maintenance is formalized as a problem of calculation w (i). A memory addressable to the content, proposed in [7, 21, 2, 17], is an external memory with an addressing scheme that selects w based on the similarity of memory words with a given query q. Specifically, we define for the read weight w (i) = f (d (q, M (i))) = N j = 1 f (d (q, M (i)))))."}, {"heading": "2.2 Memory Networks", "text": "A newer architecture, Memory Networks, uses a memory that is addressable to content, accessed via a series of read operations [21, 17] and successfully applied to a series of questions [20, 10]. In these tasks, the memory is preloaded by a learned embedding of the provided context, e.g. a paragraph of text, and then, when the question is embedded, the controller repeatedly queries the memory with content-based read operations to obtain an answer."}, {"heading": "2.3 Neural Turing Machine", "text": "The Turing Neural Machine is a recursive neural network that is equipped with content-addressable memory, similar to storage networks, but with the added ability to write to memory over time. Memory is accessed via a controller network, typically an LSTM, and the complete model is differentiable - so it can be written to memory via BPTT.A, Mt \u2190 (1 \u2212 Rt) Mt \u2212 1 + At, (3) consists of a copy of the memory from the previous time step Mt \u2212 1, which has decomposed by the delete matrix Rt specifying outdated or inaccurate content, as well as the addition of new or updated information At. The delete matrix Rt = wWt e T t is selected as an outer product between a set of write weights wWt-1 [0, 1] N and the delete vector et-0 M. The W1 = the addition of the matrix to the outer matrix is the WAT."}, {"heading": "3 Architecture", "text": "This paper introduces Sparse Access Memory (SAM), a new neural storage architecture with two innovations. Most importantly, all write and read operations from external memory are limited to a sparse subset of memory words, which provides similar functionality to the NTM, while enabling computer and memory-efficient operation. Second, we introduce a sparse memory management scheme that tracks memory usage and finds unused memory blocks to record new information. For a memory containing N-words, SAM performs a forward and backward step in LogN time, initialized in LogN space, and consumes space at each step of time. Under some reasonable assumptions, SAM is asymptotically optimal in terms of time and space complexity (Supplementary A)."}, {"heading": "3.1 Read", "text": "The sparse reading process is defined as a weighted average over a selection of words in memory: r \u0439t = K \u2211 i = 1 w = Rt (si) Mt (si), (4) where w = 4 or K = 8. We will refer to sparse analogies of weight vectors w as well as w, and when discussing operations used in both the sparse and dense versions of our model. We want to construct w = Rt so that r = 4 or K = 8. For content-based reading operations where wRt is defined by (2), an effective approach is to keep the K largest non-null entries and set the remaining entries to zero. We can calculate w = Rt naively in O (N) time by reading the K largest values."}, {"heading": "3.2 Write", "text": "The write operation is SAM is an instance of (3) where the write weights w = > Wt are forced to contain a constant number of non-zero entries, through a simple scheme in which the controller writes to either previously read locations to update contextually relevant memory or the least recently accessed location to overwrite obsolete or unused memory with fresh content. Introducing the thrift could be achieved through other schemes. For example, we could use a frugal content-based write scheme in which the controller chooses a query vector qWt and writes to similar words in memory, which would allow direct memory updates, but would cause problems if the memory is empty (and shifts further complexity to the controller). We opted for the previously read / least recently accessed addressing scheme for simplicity and flexibility."}, {"heading": "3.3 Controller", "text": "At each step, the LSTM receives a concatenation of the external inputs, xt, the word rt \u2212 1. The LSTM then creates a vector, pt = (qt, at, \u03b1t, \u03b3t), of read and write parameters for memory access via a linear layer. The word that is read from memory for the current time step rt is then linked to the output of the LSTM, and this vector is fed through a linear layer to form the final output, yt. The complete control flow is illustrated in Supplementary Figure 6."}, {"heading": "3.4 Efficient backpropagation through time", "text": "We have already shown how forward operations in SAM can be efficiently calculated in O (T logN) time. However, given the space complexity of MANNs, a dependence on Mt for calculating the derivatives in the corresponding time step remains. A naive implementation requires that the state of memory is cached at each time step, creating a space expenditure of O (NT) at each time step, greatly limiting the memory size and sequence length. Fortunately, this can be remedied. Since there are only O (1) words at each time step, we instead track the sparse changes that are made to memory at each time step and apply them on the spot to calculate Mt in O (1) time and O (T) space. During the reverse run, we can restore the state of Mt from Mt + 1 in O (1) time by reversing the sparse modifications that are applied at the time step backwards."}, {"heading": "3.5 Approximate nearest neighbors", "text": "When querying the memory, we can use an approximate adjacent index (ANN) to search for the nearest K words via the external memory. Where a linear KNN search inspects each element in memory (where O (N) takes time), an ANN index gets a structure over the data set to allow fast inspection of near points in O (logN) time. In our case, the memory is still a dense tensor that the network works with directly; however, the ANN is a structured view of its contents. Both the memory and the ANN index are guided through the network and kept synchronous during writing. However, there are no gradients with respect to the ANN because its function is fixed. We looked at two types of ANN indices: FLANN's randomized k-d tree implementation [15], which arranges the datapoints in a combination of structured (randomized k-d) points to search YS for YS for nearby YS by means of a YS-sensitive tree search for the YS (using the hash YS)."}, {"heading": "4 Results", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "4.1 Speed and memory benchmarks", "text": "We measured the forward and backward times of the SAM architecture compared to the dense DAM variant and the original NTM (details of setup in Supplementary E. SAM is over 100 times faster than the NTM if the memory contains a million words and an exact linear index is used, and 1600 times faster with the k-d tree (Figure 1a). With an ANN, the model runs in sublinear time in terms of memory size. SAM's memory usage per time step is independent of the number of memory words (Figure 1b), which empirically confirms the assertion of the O (1) space in Supplementary A. For 64K memory words, SAM uses 53 MiB of physical memory to initialize the network, and 7.8 MiB to perform a 100-step forward and backward run, compared to the NTM, which uses 29 GiB."}, {"heading": "4.2 Learning with sparse memory access", "text": "We found that SAM has a huge computational and memory advantage over previous models, but can we really learn from the sparse approximations of SAM? We investigated the learning costs of generating sparseness, and the effect of placing an approximate neighboring index within the network by comparing SAM with its dense variant DAM and some established models, the NTM and the LSTM. We trained each model on three of the original NTM tasks [7]. 1. Copy: Copy a random input sequence of length 1-20, 2. Associative Recall: Give 3-6 random (key, value) pairs and then a random key return the corresponding value. 3. Priority sorting: Given 20 random keys and priority values, the 16 most important keys return in descending order of priority. We chose these tasks because the NTM is known to perform well in them."}, {"heading": "4.3 Scaling with a curriculum", "text": "The computational efficiency of SAM opens up the possibility of training for tasks that require the storage of a large amount of information about long sequences. Here, we show that this is possible in practice by scaling tasks on a large scale over an exponentially increasing curriculum. We parameterized three of the tasks described in Section 4.2: associative retrieval, copy, and priority sorting, with a progressively increasing degree of difficulty characterizing the length of the sequence and the number of entries to store in memory. For example, Layer specifies the input sequence length for the copy task. We exponentially increased the maximum level h when the network begins to learn the basic algorithm. Since the time taken for a forward and backward sequence scale O (T) with the sequence length T could potentially take O (T 2) (T) if the maximum level of the school was doubled from S2 to S2 when required in each school sequence length."}, {"heading": "4.4 Question answering on the Babi tasks", "text": "These are synthetically generated language tasks with a vocabulary of about 150 words that test various aspects of simple thinking such as deduction, induction and coreference. We tested the models (including the Sparse Differentiable Neural Computer described in Supplementary D) on this task. Complete results and training details are available in Supplementary G. The MANNs, with the exception of the NTM, are able to learn solutions comparable to the previous best results, with only 2 of the tasks failing. The SDNC manages to solve all but one task, the best reported result about Babi we know. Remarkable are the best previous results obtained by monitoring the memory search (during the training, annotations are provided to the model indicating which memory should be used to answer a query). More directly comparable previous work with end-to-end memory networks that did not use monitoring [17], we cannot perform this task effectively because we do not know much about the task."}, {"heading": "4.5 Learning on real world data", "text": "Omniglot [12] is a dataset of 1623 characters from 50 different alphabets with 20 examples for each character. This dataset is used to test rapid or one-off learning, as there are few examples for each character, but many different character classes. Following [16] we create episodes in which a subset of characters is randomly selected, rotated and stretched from the dataset and assigned to a randomly selected character. In each step, an example of one of the characters is presented, along with the correct designation of the progressing character. Each character is presented 10 times in an episode (but each presentation can be one of the 20 examples of the character). To successfully complete the task, the model must learn to quickly associate a novel character with the correct character so that it can correctly classify the subsequent examples of the same character class (but each presentation can be one of the 20 examples of the character)."}, {"heading": "5 Discussion", "text": "The scaling of storage systems is an urgent research direction due to the potential for compelling applications with large amounts of memory. We have shown that you can train large memory neural networks via a sparse read / write scheme that uses efficient data structures within the network and achieves significant acceleration during training. Although we have focused on a specific MAN (SAM) that is closely related to the NTM, the approach used here is general and can be applied to many differentiable memory architectures, such as Memory Networks [21]. It should be noted that there are several possible routes towards scalable memory architectures. For example, previous work aiming to scale neural turing machines [22] used amplification learning to train a discrete addressing policy. This approach also touches on a sparse number of storage modules in each step of time, but we can only guess at the number of faults in the hierarchy."}, {"heading": "Acknowledgements", "text": "We thank Vyacheslav Egorov, Edward Grefenstette, Malcolm Reynolds, Fumin Wang and Yori Zwols for their support and the Google DeepMind family for helpful discussions and encouragement."}, {"heading": "A Time and space complexity", "text": "The SAM storage architecture is included in this paper, how it translates the closest neighbors into fixed dimensions."}, {"heading": "C Training details", "text": "To avoid bias in our results, we chose the learning rate that worked best for DAM (and not for SAM), tested learning rates {10 \u2212 6, 5 \u00b7 10 \u2212 5, 10 \u2212 5, 5 \u00b7 10 \u2212 4, 10 \u2212 4} and found that DAM trained best at 10 \u2212 5. We also tested values of K {4, 8, 16} and found no significant performance difference between the values. We used 100 hidden units for the LSTM (including controller LSTMs), a minibatch of 8, 8 asynchronous workers to speed up training, and RMSProp [19] to optimize the controller. We used 4 memory access heads and configured SAM to read from just K = 4 locations per head."}, {"heading": "D Sparse Differentiable Neural Computer", "text": "This year, it has reached the point where it will be able to retaliate until it is able to retaliate."}, {"heading": "E Benchmarking details", "text": "Each model included an LSTM controller with 100 hidden units, an N-memory external memory, with word size 32 and 4 access heads. For speed benchmarks, a minibatch size of 8 was used to ensure fair comparison - so many dense operations (e.g. matrix multiplication) can be efficiently bundled. For memory benchmarks, the minibatch size was set to 1. We used Torch7 [5] to implement SAM, DAM, NTM, DNC and SDNC. For fast, economical tensor operations, Eigen v3 [9] was used, using the provided CSC and CSR formats. All benchmarks were executed on a Linux desktop running Ubuntu 14.04.1 with 32 GiB RAM and an Intel Xeon E5-1650 3.20 GHz processor with power scaling.F on generative memory"}, {"heading": "G Babi results", "text": "The main text for a description of the Babi task and its relevance. Here we report on the best and average results for all models on this task."}], "references": [{"title": "An optimal algorithm for approximate nearest neighbor searching fixed dimensions", "author": ["Sunil Arya", "David M. Mount", "Nathan S. Netanyahu", "Ruth Silverman", "Angela Y. Wu"], "venue": "J. ACM,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 1998}, {"title": "Neural machine translation by jointly learning to align and translate", "author": ["Dzmitry Bahdanau", "Kyunghyun Cho", "Yoshua Bengio"], "venue": "arXiv preprint arXiv:1409.0473,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2014}, {"title": "Lsh forest: self-tuning indexes for similarity search", "author": ["Mayank Bawa", "Tyson Condie", "Prasanna Ganesan"], "venue": "In Proceedings of the 14th international conference on World Wide Web,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2005}, {"title": "Visual long-term memory has a massive storage capacity for object details", "author": ["Timothy F Brady", "Talia Konkle", "George A Alvarez", "Aude Oliva"], "venue": "Proceedings of the National Academy of Sciences,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2008}, {"title": "Torch7: A matlab-like environment for machine learning", "author": ["Ronan Collobert", "Koray Kavukcuoglu", "Cl\u00e9ment Farabet"], "venue": "In BigLearn, NIPS Workshop, number EPFL-CONF-192376,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2011}, {"title": "Speech recognition with deep recurrent neural networks", "author": ["Alex Graves", "Abdel-rahman Mohamed", "Geoffrey Hinton"], "venue": "In Acoustics, Speech and Signal Processing (ICASSP),", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2013}, {"title": "Neural turing machines", "author": ["Alex Graves", "Greg Wayne", "Ivo Danihelka"], "venue": "arXiv preprint arXiv:1410.5401,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2014}, {"title": "Hybrid computing using a neural network with dynamic external memory", "author": ["Alex Graves", "Greg Wayne", "Malcolm Reynolds", "Tim Harley", "Ivo Danihelka", "Agnieszka Grabska-Barwi\u0144ska", "Sergio G\u00f3mez Colmenarejo", "Edward Grefenstette", "Tiago Ramalho", "John Agapiou"], "venue": null, "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2016}, {"title": "The goldilocks principle: Reading children\u2019s books with explicit memory representations", "author": ["Felix Hill", "Antoine Bordes", "Sumit Chopra", "Jason Weston"], "venue": "arXiv preprint arXiv:1511.02301,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2015}, {"title": "Long short-term memory", "author": ["Sepp Hochreiter", "J\u00fcrgen Schmidhuber"], "venue": "Neural computation,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 1997}, {"title": "Human-level concept learning through probabilistic program induction", "author": ["Brenden M Lake", "Ruslan Salakhutdinov", "Joshua B Tenenbaum"], "venue": null, "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2015}, {"title": "Mondrian forests: Efficient online random forests", "author": ["Balaji Lakshminarayanan", "Daniel M Roy", "Yee Whye Teh"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2014}, {"title": "Lower bounds on locality sensitive hashing", "author": ["Rajeev Motwani", "Assaf Naor", "Rina Panigrahy"], "venue": "SIAM Journal on Discrete Mathematics,", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2007}, {"title": "Scalable nearest neighbor algorithms for high dimensional data", "author": ["Marius Muja", "David G. Lowe"], "venue": "Pattern Analysis and Machine Intelligence, IEEE Transactions on,", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2014}, {"title": "Meta-learning with memory-augmented neural networks", "author": ["Adam Santoro", "Sergey Bartunov", "Matthew Botvinick", "Daan Wierstra", "T Lillicrap"], "venue": "In International conference on machine learning,", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2016}, {"title": "End-to-end memory networks", "author": ["Sainbayar Sukhbaatar", "Jason Weston", "Rob Fergus"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2015}, {"title": "Sequence to sequence learning with neural networks", "author": ["Ilya Sutskever", "Oriol Vinyals", "Quoc V Le"], "venue": "In Advances in Neural Information Processing Systems", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2014}, {"title": "Lecture 6.5-rmsprop: Divide the gradient by a running average of its recent magnitude", "author": ["Tijmen Tieleman", "Geoffrey Hinton"], "venue": "COURSERA: Neural Networks for Machine Learning,", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2012}, {"title": "Towards ai-complete question answering: A set of prerequisite toy tasks", "author": ["Jason Weston", "Antoine Bordes", "Sumit Chopra", "Alexander M Rush", "Bart van Merri\u00ebnboer", "Armand Joulin", "Tomas Mikolov"], "venue": "arXiv preprint arXiv:1502.05698,", "citeRegEx": "20", "shortCiteRegEx": "20", "year": 2015}], "referenceMentions": [{"referenceID": 9, "context": "1 Introduction Recurrent neural networks, such as the Long Short-Term Memory (LSTM) [11], have proven to be powerful sequence learning models [6, 18].", "startOffset": 84, "endOffset": 88}, {"referenceID": 5, "context": "1 Introduction Recurrent neural networks, such as the Long Short-Term Memory (LSTM) [11], have proven to be powerful sequence learning models [6, 18].", "startOffset": 142, "endOffset": 149}, {"referenceID": 16, "context": "1 Introduction Recurrent neural networks, such as the Long Short-Term Memory (LSTM) [11], have proven to be powerful sequence learning models [6, 18].", "startOffset": 142, "endOffset": 149}, {"referenceID": 6, "context": "Recent approaches, such as Neural Turing Machines (NTMs) [7] and Memory Networks [21], have addressed this issue by decoupling the memory capacity from the number of model parameters.", "startOffset": 57, "endOffset": 60}, {"referenceID": 6, "context": "To test whether the model is able to learn with this sparse approximation, we examined its performance on a selection of synthetic and natural tasks: algorithmic tasks from the NTM work [7], Babi reasoning tasks used with Memory Networks [17] and Omniglot one-shot classification [16, 12].", "startOffset": 186, "endOffset": 189}, {"referenceID": 15, "context": "To test whether the model is able to learn with this sparse approximation, we examined its performance on a selection of synthetic and natural tasks: algorithmic tasks from the NTM work [7], Babi reasoning tasks used with Memory Networks [17] and Omniglot one-shot classification [16, 12].", "startOffset": 238, "endOffset": 242}, {"referenceID": 14, "context": "To test whether the model is able to learn with this sparse approximation, we examined its performance on a selection of synthetic and natural tasks: algorithmic tasks from the NTM work [7], Babi reasoning tasks used with Memory Networks [17] and Omniglot one-shot classification [16, 12].", "startOffset": 280, "endOffset": 288}, {"referenceID": 10, "context": "To test whether the model is able to learn with this sparse approximation, we examined its performance on a selection of synthetic and natural tasks: algorithmic tasks from the NTM work [7], Babi reasoning tasks used with Memory Networks [17] and Omniglot one-shot classification [16, 12].", "startOffset": 280, "endOffset": 288}, {"referenceID": 7, "context": "Further, in Supplementary D we demonstrate the generality of our approach by describing how to construct a sparse version of the recently published Differentiable Neural Computer [8].", "startOffset": 179, "endOffset": 182}, {"referenceID": 6, "context": "A content addressable memory, proposed in [7, 21, 2, 17], is an external memory with an addressing scheme which selects w based upon the similarity of memory words to a given query q.", "startOffset": 42, "endOffset": 56}, {"referenceID": 1, "context": "A content addressable memory, proposed in [7, 21, 2, 17], is an external memory with an addressing scheme which selects w based upon the similarity of memory words to a given query q.", "startOffset": 42, "endOffset": 56}, {"referenceID": 15, "context": "A content addressable memory, proposed in [7, 21, 2, 17], is an external memory with an addressing scheme which selects w based upon the similarity of memory words to a given query q.", "startOffset": 42, "endOffset": 56}, {"referenceID": 15, "context": "2 Memory Networks One recent architecture, Memory Networks, make use of a content addressable memory that is accessed via a series of read operations [21, 17] and has been successfully applied to a number of question answering tasks [20, 10].", "startOffset": 150, "endOffset": 158}, {"referenceID": 18, "context": "2 Memory Networks One recent architecture, Memory Networks, make use of a content addressable memory that is accessed via a series of read operations [21, 17] and has been successfully applied to a number of question answering tasks [20, 10].", "startOffset": 233, "endOffset": 241}, {"referenceID": 8, "context": "2 Memory Networks One recent architecture, Memory Networks, make use of a content addressable memory that is accessed via a series of read operations [21, 17] and has been successfully applied to a number of question answering tasks [20, 10].", "startOffset": 233, "endOffset": 241}, {"referenceID": 0, "context": "The erase matrix Rt = w t e T t is constructed as the outer product between a set of write weights w t \u2208 [0, 1] and erase vector et \u2208 [0, 1] .", "startOffset": 105, "endOffset": 111}, {"referenceID": 0, "context": "The erase matrix Rt = w t e T t is constructed as the outer product between a set of write weights w t \u2208 [0, 1] and erase vector et \u2208 [0, 1] .", "startOffset": 134, "endOffset": 140}, {"referenceID": 13, "context": "We considered two types of ANN indexes: FLANN\u2019s randomized k-d tree implementation [15] that arranges the datapoints in an ensemble of structured (randomized k-d) trees to search for nearby points via comparison-based search, and one that uses locality sensitive hash (LSH) functions that map points into buckets with distance-preserving guarantees.", "startOffset": 83, "endOffset": 87}, {"referenceID": 6, "context": "We trained each model on three of the original NTM tasks [7].", "startOffset": 57, "endOffset": 60}, {"referenceID": 18, "context": "4 Question answering on the Babi tasks [20] introduced toy tasks they considered a prerequisite to agents which can reason and understand natural language.", "startOffset": 39, "endOffset": 43}, {"referenceID": 15, "context": "More directly comparable previous work with end-to-end memory networks, which did not use supervision [17], fails at 6 of the tasks.", "startOffset": 102, "endOffset": 106}, {"referenceID": 10, "context": "Omniglot [12] is a dataset of 1623 characters taken from 50 different alphabets, with 20 examples of each character.", "startOffset": 9, "endOffset": 13}, {"referenceID": 14, "context": "Following [16], we generate episodes where a subset of characters are randomly selected from the dataset, rotated and stretched, and assigned a randomly chosen label.", "startOffset": 10, "endOffset": 14}, {"referenceID": 14, "context": "Previous results on the Omniglot curriculum [16] task are not identical, since we used 1-hot labels throughout and the training curriculum scaled to longer sequences, but our results with the dense models are comparable (\u2248 0.", "startOffset": 44, "endOffset": 48}, {"referenceID": 11, "context": "Recent work in tree ensemble models, such as Mondrian forests [13], show promising results in maintaining balanced hierarchical set coverage in the online setting.", "startOffset": 62, "endOffset": 66}, {"referenceID": 2, "context": "An alternative approach which may be well-suited is LSH forests [3], which adaptively modifies the number of hashes used.", "startOffset": 64, "endOffset": 67}, {"referenceID": 3, "context": "Humans are able to retain a large, task-dependent set of memories obtained in one pass with a surprising amount of fidelity [4].", "startOffset": 124, "endOffset": 127}], "year": 2016, "abstractText": "Neural networks augmented with external memory have the ability to learn algorithmic solutions to complex tasks. These models appear promising for applications such as language modeling and machine translation. However, they scale poorly in both space and time as the amount of memory grows \u2014 limiting their applicability to real-world domains. Here, we present an end-to-end differentiable memory access scheme, which we call Sparse Access Memory (SAM), that retains the representational power of the original approaches whilst training efficiently with very large memories. We show that SAM achieves asymptotic lower bounds in space and time complexity, and find that an implementation runs 1,000\u00d7 faster and with 3,000\u00d7 less physical memory than non-sparse models. SAM learns with comparable data efficiency to existing models on a range of synthetic tasks and one-shot Omniglot character recognition, and can scale to tasks requiring 100,000s of time steps and memories. As well, we show how our approach can be adapted for models that maintain temporal associations between memories, as with the recently introduced Differentiable Neural Computer.", "creator": "LaTeX with hyperref package"}}}