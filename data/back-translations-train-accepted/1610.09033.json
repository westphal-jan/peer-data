{"id": "1610.09033", "review": {"conference": "NIPS", "VERSION": "v1", "DATE_OF_SUBMISSION": "27-Oct-2016", "title": "Operator Variational Inference", "abstract": "Variational inference is an umbrella term for algorithms which cast Bayesian inference as optimization. Classically, variational inference uses the Kullback-Leibler divergence to define the optimization. Though this divergence has been widely used, the resultant posterior approximation can suffer from undesirable statistical properties. To address this, we reexamine variational inference from its roots as an optimization problem. We use operators, or functions of functions, to design variational objectives. As one example, we design a variational objective with a Langevin-Stein operator. We develop a black box algorithm, operator variational inference (OPVI), for optimizing any operator objective. Importantly, operators enable us to make explicit the statistical and computational tradeoffs for variational inference. We can characterize different properties of variational objectives, such as objectives that admit data subsampling---allowing inference to scale to massive data---as well as objectives that admit variational programs---a rich class of posterior approximations that does not require a tractable density. We illustrate the benefits of OPVI on a mixture model and a generative model of images.", "histories": [["v1", "Thu, 27 Oct 2016 23:32:25 GMT  (150kb,D)", "http://arxiv.org/abs/1610.09033v1", null], ["v2", "Mon, 31 Oct 2016 23:58:43 GMT  (150kb,D)", "http://arxiv.org/abs/1610.09033v2", "Appears in Neural Information Processing Systems, 2016"]], "reviews": [], "SUBJECTS": "stat.ML cs.LG stat.CO stat.ME", "authors": ["rajesh ranganath", "dustin tran", "jaan altosaar", "david m blei"], "accepted": true, "id": "1610.09033"}, "pdf": {"name": "1610.09033.pdf", "metadata": {"source": "CRF", "title": "Operator Variational Inference", "authors": ["Rajesh Ranganath", "Jaan Altosaar", "Dustin Tran", "David M. Blei"], "emails": [], "sections": [{"heading": "1 Introduction", "text": "We evolved in the 1990 \"s, and recent advances in variation assessment have enabled us to orient ourselves in practice without sacrificing efficiency. These innovations have both a scaled analysis and the analytical burdens that traditionally demand their practical application. Considering a model of latent and observed variables p (x, z), variable inference can locate a family of distributions via its latent variables and then find the member of that family closest, p (z | x). This is typically called a miniature variable p (kl), the divergence from the approximation of the family to the posterior, p (z) we are formalized."}, {"heading": "2 Operator Variational Objectives", "text": "We define the variation objectives of the operator and the conditions necessary for a lens to be useful for concluding variations. We develop a new objective, the Langevin-Stein objective, and show how to place the classic kl in this class. In the next section, we develop a general algorithm for optimizing the variation objectives of the operator."}, {"heading": "2.1 Variational Objectives", "text": "Consider a probabilistic model p (x, z) of data x and latent variables. In the face of a dataset x, the approximate Bayesian inference attempts to approximate the posterior distribution p (z | x) applied in all downstream tasks. Variational inference defines a family of approximate distributions q (z) and optimizes a divergence function to find the member of the family closest to the posterior one. The divergence function is the object of variation, a function of both the posterior approximate distribution. Useful variation objects depend on two properties: firstly, the optimization of the function results in a good posterior approximation; secondly, the problem is traceable if the posterior distribution is known to a constant. The classical construction that fulfills these properties is the evidence boundary (elbo), equation (z) [p (z), z \u2212 log \u2212 q (keriz \u2212 keriz \u2212 keriz = keriz \u2212 keriz), and (z \u2212 keriz \u2212 keriz \u2212 keriz \u2212 keriz = keriz \u2212 keriz \u2212 keriz = keriz)."}, {"heading": "2.2 Operator Variational Objectives", "text": "We define a new class of variation targets, operator variation targets. An operator target has three components. The first component is an operator Op, q, which depends on p (z | x) and q (z). (Remember that an operator maps functions to other functions.) The second component is a family of test functions F, in which each f (z).F realizations of the latent variables to real vectors Rd. (a): The operator and a function are designed in expectation Eq (z) [Op, q f) (z)], so that values are close to zero. The third component is a distance function t (a): R \u2192 [0, \u221e), which is applied to the expectation so that the target is not negative. (Our example uses the square function t (a) = a2."}, {"heading": "2.3 Understanding Operator Variational Objectives", "text": "Let us consider operators where Eq (z) [(Op, q f) (z)] has only positive values. In this case, the distance to zero can be measured with the identity t (a) = a, so that the traceability of the operator must be known only up to a constant. This family includes comprehensible forms of familiar divergences such as the kl divergence (elbo), R\u00e9nyis \u03b1 divergence [15] and ig divergence [19]. If the expectation can take positive or negative values, the variation goals of the operator are closely related to the stone divergences [2]. Let us consider a family of scalar test functions that have an expectation of zero in relation to the posterior, Ep (z | x) [f (z)] = 0. When using this family, a stone divergence (p, q) = positive divergences closely related to stone divergences [2], let us consider a family of zero."}, {"heading": "2.4 Langevin-Stein Operator Variational Objective", "text": "It is a class of tractable targets, each of which can be optimized to achieve an approximation to the posterior level.An operator variation target is built to zero by an operator, a function class and a distance function. We are now using this construction to design a new type of variable lens. An operator objective includes a class of functions that have known expectations in terms of an intractable distribution. There are many ways to construct such classes [1, 2]. Here, we construct an operator objective from the method of the Stein generator, which is applied to the Langevin diffusion. Let's call > f the divergence of a vector-depreciated function f, that is, the sum of its individual gradients. Applying the Barbour generator method [2] to the Langevin diffusion gives the operator (Opls f) (z) = z variables p (x), z > f (we call this the stone)."}, {"heading": "2.5 The KL Divergence as an Operator Variational Objective", "text": "Finally, we show how classical methods of variation fall into the family of operators. Thus, for example, the conventional inference of variation minimizes the kl divergence from an approximate family to the trailing family [11]. This can be interpreted as the variation target of the operator, (Op, qKL f) (z) = log q (z) \u2212 log p (z | x) \u0441F. (5) This operator does not use the function family - it maps all functions f trivially to the same function. Since kl is strictly positive, we also use the identity distance t (a) = a. The operator fulfills both conditions. It fulfills the proximity, because KL (p | p) = 0. It fulfills the traceability, because it can be calculated in the operator target from equation to a constant. 2. Traceability results from the fact that log p (z | x) = log p (z, x) \u2212 log p (x)."}, {"heading": "3 Operator Variational Inference", "text": "We are now investigating how to optimize it. We are developing a black box algorithm [29, 20] based on Monte Carlo estimation and stochastic optimization. Our algorithm refers to a general class of models and each operator objective. However, minimizing the operator lens involves two optimizations: minimizing the lens relative to the approximate family Q and maximizing the lens relative to the functional class F (which is part of the object). We capture the familyQ with variable parameters and require it to fulfill properties typically adopted by black box methods [20]: the variable distribution q (z) has a known and tractable density; we can sample from q (z); and we can calculate the score function traceable (z). We capture the functional class F with parameters."}, {"heading": "3.2 Variational Programs", "text": "Considering an operator and a family of variations, algorithm 1 optimizes the corresponding operator objective. Certain operators require the density of q. For example, the kl operator (Eq.5) requires its log density. This potentially limits the construction of rich variable approximations for which the density of q is difficult to calculate.1However, some operators are not dependent on having an analytical density; the Langevin stone (ls) operator (Eq.3) is an example. These operators can be used with a much richer class of variable approximations, those that can be sampled but may not have analytically traceable densities. We call such an approximate family variational programs. Inference with a variant program requires the family to be repairable [12, 23]. (Otherwise, we must use the score function that requires the density derivation."}, {"heading": "4 Empirical Study", "text": "We evaluate the conclusions of variation of the operator on a mixture of Gaussians and compare different options in the lens. Then we analyze logistical factors for images."}, {"heading": "4.1 Mixture of Gaussians", "text": "Consider a one-dimensional mixture of Gaussians as the posterior side of interest, p (z) = 12Normal (z; \u2212 3, 1) + 1 2Normal (z; 3, 1). The posterior side contains several modes. We try to approximate them with three variation goals: Kullback-Leibler (kl) with a Gaussian approximation family, Langevin-Stein (ls) with a Gaussian approximation family and ls with a variation program. Figure 1 shows the posterior approximations. We find that the kl divergence and ls divergence select a single mode and have slightly different deviations. These operators do not yield good results because a single Gaussian is a poor approximation to the mixture. The remaining distribution in Figure 1 comes from the toy variation program described by Equation 10 with the ls operator. Since this program captures different statistical distributions for the positive and negative half of the posterior half, it is possible to capture it."}, {"heading": "4.2 Logistic Factor Analysis", "text": "Logistic factor analysis models binary vector sxi with amatrix of parameters W and biasesb > q = two hidden network functions q > normal (0, 1) xi, k \u0445 Bernoulli (\u03c3 (w > k zi + bk)), where zi has fixed dimensionK and \u03c3 is the sigmoid function. This model captures correlations of entries in xi by W. We apply logistic factor analysis to analyze the binary MNIST dataset [25], which contains 28x28 binary pixel images of handwritten digits. (We set the latent dimensionality to 10.) We fix the model parameters on those learned with varying expectation maximization and focus on comparing posterior inferences. We compare the kl operator with the ls operator and examine two selections of variant models: a fully factorized Gaussian distribution and a variable program."}, {"heading": "5 Summary", "text": "We present operator variation objectives, a broad but comprehensible class of optimization problems for the approximation of rear distributions. Operator objectives are established by an operator, a family of test functions and a distance function. We outline the connection between operator objectives and existing divergences such as the KL divergence and develop a new variation target using the Langevin Stein operator. In general, operator objectives create new possibilities for positioning variable deviations. In the face of an operator object, we develop a black box algorithm for optimization and show which operators enable scalable optimization through data subsampling. Furthermore, unlike the popular evidence distinctions, not all operators are explicitly dependent on approximate density. This allows flexible approximation families, so-called variation programs, where the distribution form is not traceable."}, {"heading": "A Technical Conditions for Langevin-Stein Operators", "text": "Here we specify the conditions that are necessary for the function class F or the rear distribution curve p, so that the operators have an expectation of zero for all f-F. W derives properties by using parts for supports that are unlimited open sets. Then we extend the result to unlimited supports using limits. We start with the Langevin stone operator. Let S be the quantity above which we integrate and let B be its limit. Let v be the unit that is normal for the surface, and vi be the D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D (which is D dimensional). Then we have Sp (OpLS f) dS = VP-D-D-D-D-Log > f + p-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-"}, {"heading": "B Characterizing the zeros of the Langevin-Stein Operators", "text": "A general condition for equality in the distribution results from the equality of probability on all Borel sets. We can build functions that have an expectation of zero with respect to the posterior value that tests this equality. Formally, these functions on A have the form: \u03b4A (z) \u2212 \u03b4A p (y) dyWe show that if the Langevin stone operator fulfills L (q; OpLS, F) = 0, then q in the distribution is equal to p. We do this by showing that the above functions are within the range of OpLS. Extension of the Langevin stone operator results in (OpLS f) = p \u2212 1, zp > f = p \u2212 1, d \u00b2, i = 1, f = 1, p \u00b2, p \u2212 z, the differential equation yields the differential equation A (z)."}, {"heading": "C Operators for Discrete Variables", "text": "There are stone operators that work with discrete variables [1, 14]. We present variation targets to the operator based on a discrete analogy to the Langevin stone operator developed in [14]. For simplicity's sake, let us consider a one-dimensional discrete operator with support {0,..., c}. Let f be a function that is f (0) = 0, then an operator can be defined as (opdiscrete f) (z) = f (z + 1) p (z + 1, x) \u2212 f (z) p (z, x) p (z, x) p (z, x). Since the expectation of this operator in relation to the posterior p (z | x) is a telescoping sum with both endpoints 0, it has a zero expectation. This refers to the Langevin stone operator in one dimension. The Langevin stone operator in one dimension can be written as an OpLS (f = d = discretion)."}, {"heading": "D Proof of Universal Representations", "text": "Consider the optimal form of R in such a way that the transformations of the normal standard moves in the distribution are equal to exact back-row pulls. This means that the reverse cumulative distribution function P \u2212 1 is applied to the uniform pulls. In fact, as in the universal approximation theorem by Tran et al. [28], there is a sequence of parameters {\u03bb1, \u03bb2,.} so that the target of variation of the operator is zero, but the function class is no longer limited to local interpolation. Universal approximations such as neural networks [9] also function on the assumption that p is the unique root and, by satisfying the conditions described in this program, Section N (equality) represents the equality of the distribution in x."}], "references": [{"title": "Zero-variance principle for monte carlo algorithms", "author": ["R. Assaraf", "M. Caffarel"], "venue": "In Phys. Rev. Let", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 1999}, {"title": "Stein\u2019s method and poisson process convergence", "author": ["A.D. Barbour"], "venue": "Journal of Applied Probability", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 1988}, {"title": "The Stan Math Library: Reverse-mode automatic differentiation in C++", "author": ["B. Carpenter", "M.D. Hoffman", "M. Brubaker", "D. Lee", "P. Li", "M. Betancourt"], "venue": "arXiv preprint arXiv:1509.07164", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2015}, {"title": "Probability and Stochastics", "author": ["E. Cinlar"], "venue": null, "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2011}, {"title": "Propagation algorithms for variational Bayesian learning", "author": ["Z. Ghahramani", "M. Beal"], "venue": "In NIPS", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2001}, {"title": "Generative adversarial nets", "author": ["I. Goodfellow", "J. Pouget-Abadie", "M. Mirza", "B. Xu", "D. Warde-Farley", "S. Ozair", "A. Courville", "Y. Bengio"], "venue": "In Neural Information Processing Systems", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2014}, {"title": "Black-box \u03b1-divergence Minimization", "author": ["J.M. Hern\u00e1ndez-Lobato", "Y. Li", "M. Rowland", "D. Hern\u00e1ndez-Lobato", "T. Bui", "R.E. Turner"], "venue": null, "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2015}, {"title": "Stochastic variational inference", "author": ["M. Hoffman", "D. Blei", "C. Wang", "J. Paisley"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2013}, {"title": "Multilayer feedforward networks are universal approximators", "author": ["K. Hornik", "M. Stinchcombe", "H. White"], "venue": "Neural networks,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 1989}, {"title": "Estimation of non-normalized statistical models by score matching", "author": ["A. Hyv\u00e4rinen"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2005}, {"title": "Introduction to variational methods for graphical models", "author": ["M. Jordan", "Z. Ghahramani", "T. Jaakkola", "L. Saul"], "venue": "Machine Learning,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 1999}, {"title": "Auto-encoding variational bayes", "author": ["D. Kingma", "M. Welling"], "venue": null, "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2014}, {"title": "Stochastic Approximation Algorithms and Applications", "author": ["H. Kushner", "G. Yin"], "venue": null, "citeRegEx": "13", "shortCiteRegEx": "13", "year": 1997}, {"title": "Discrete stein characterizations and discrete information distances. arXiv preprint arXiv:1201.0143", "author": ["C. Ley", "Y. Swan"], "venue": null, "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2011}, {"title": "R\u00e9nyi divergence variational inference. arXiv preprint arXiv:1602.02311", "author": ["Y. Li", "R.E. Turner"], "venue": null, "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2016}, {"title": "Auxiliary deep generative models", "author": ["L. Maal\u00f8e", "C.K. S\u00f8nderby", "S.K. S\u00f8nderby", "O. Winther"], "venue": "arXiv preprint arXiv:1602.05473", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2016}, {"title": "Expectation propagation for approximate Bayesian inference. In UAI", "author": ["T.P. Minka"], "venue": null, "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2001}, {"title": "On the chi square and higher-order chi distances for approximating f-divergences", "author": ["F. Nielsen", "R. Nock"], "venue": "arXiv preprint arXiv:1309.3029", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2013}, {"title": "Hierarchical variational models", "author": ["R. Ranganath", "D. Tran", "D.M. Blei"], "venue": "In International Conference on Machine Learning", "citeRegEx": "21", "shortCiteRegEx": "21", "year": 2016}, {"title": "Variational inference with normalizing flows", "author": ["D.J. Rezende", "S. Mohamed"], "venue": "In Proceedings of the 31st International Conference on Machine Learning (ICML-15)", "citeRegEx": "22", "shortCiteRegEx": "22", "year": 2015}, {"title": "Stochastic backpropagation and approximate inference in deep generative models", "author": ["D.J. Rezende", "S. Mohamed", "D. Wierstra"], "venue": "In International Conference on Machine Learning", "citeRegEx": "23", "shortCiteRegEx": "23", "year": 2014}, {"title": "A stochastic approximation method", "author": ["H. Robbins", "S. Monro"], "venue": "The Annals of Mathematical Statistics,", "citeRegEx": "24", "shortCiteRegEx": "24", "year": 1951}, {"title": "On the quantitative analysis of deep belief networks", "author": ["R. Salakhutdinov", "I. Murray"], "venue": "In International Conference on Machine Learning", "citeRegEx": "25", "shortCiteRegEx": "25", "year": 2008}, {"title": "A note on the evaluation of generative models", "author": ["L. Theis", "A. van den Oord", "M. Bethge"], "venue": "In International Conference on Learning Representations", "citeRegEx": "26", "shortCiteRegEx": "26", "year": 2016}, {"title": "Doubly stochastic variational bayes for non-conjugate inference", "author": ["M. Titsias", "M. L\u00e1zaro-Gredilla"], "venue": "In Proceedings of the 31st International Conference on Machine Learning (ICML-14),", "citeRegEx": "27", "shortCiteRegEx": "27", "year": 2014}, {"title": "The variational Gaussian process", "author": ["D. Tran", "R. Ranganath", "D.M. Blei"], "venue": "In ICLR", "citeRegEx": "28", "shortCiteRegEx": "28", "year": 2016}], "referenceMentions": [{"referenceID": 10, "context": "Variational inference is an umbrella term for algorithms that cast Bayesian inference as optimization [11].", "startOffset": 102, "endOffset": 106}, {"referenceID": 7, "context": "Originally developed in the 1990s, recent advances in variational inference have scaled Bayesian computation to massive data [8], provided black box strategies for generic inference in many models [20], and enabled more accurate approximations of a model\u2019s posterior without sacrificing efficiency [22, 21].", "startOffset": 125, "endOffset": 128}, {"referenceID": 19, "context": "Originally developed in the 1990s, recent advances in variational inference have scaled Bayesian computation to massive data [8], provided black box strategies for generic inference in many models [20], and enabled more accurate approximations of a model\u2019s posterior without sacrificing efficiency [22, 21].", "startOffset": 298, "endOffset": 306}, {"referenceID": 18, "context": "Originally developed in the 1990s, recent advances in variational inference have scaled Bayesian computation to massive data [8], provided black box strategies for generic inference in many models [20], and enabled more accurate approximations of a model\u2019s posterior without sacrificing efficiency [22, 21].", "startOffset": 298, "endOffset": 306}, {"referenceID": 16, "context": "An early example is expectation propagation (ep) [17].", "startOffset": 49, "endOffset": 53}, {"referenceID": 6, "context": "ep hinges on local minimization with respect to subsets of data and connects to work on \u03b1-divergence minimization [18, 7].", "startOffset": 114, "endOffset": 121}, {"referenceID": 14, "context": "Li and Turner [15] present a variant of\u03b1-divergences that satisfy the full requirements of opvi.", "startOffset": 14, "endOffset": 18}, {"referenceID": 9, "context": "Score matching [10], a method for estimating models by matching the score function of one distribution to another that can be sampled, also falls into the class of objectives we develop.", "startOffset": 15, "endOffset": 19}, {"referenceID": 4, "context": "Maximizing the elbo is equivalent to minimizing the kl divergence to the posterior, and the expectations are analytic for a large class of models [5].", "startOffset": 146, "endOffset": 149}, {"referenceID": 14, "context": "This family includes tractable forms of familiar divergences like the kl divergence (elbo), R\u00e9nyi\u2019s \u03b1-divergence [15], and the \u03c7-divergence [19].", "startOffset": 113, "endOffset": 117}, {"referenceID": 17, "context": "This family includes tractable forms of familiar divergences like the kl divergence (elbo), R\u00e9nyi\u2019s \u03b1-divergence [15], and the \u03c7-divergence [19].", "startOffset": 140, "endOffset": 144}, {"referenceID": 1, "context": "When the expectation can take positive or negative values, operator variational objectives are closely related to Stein divergences [2].", "startOffset": 132, "endOffset": 135}, {"referenceID": 0, "context": "There are many ways to construct such classes [1, 2].", "startOffset": 46, "endOffset": 52}, {"referenceID": 1, "context": "There are many ways to construct such classes [1, 2].", "startOffset": 46, "endOffset": 52}, {"referenceID": 1, "context": "Applying the generator method of Barbour [2] to Langevin diffusion gives the operator (O ls f)(z) = \u2207z log p(x, z)>f(z) +\u2207>f.", "startOffset": 41, "endOffset": 44}, {"referenceID": 10, "context": "For example, traditional variational inference minimizes the kl divergence from an approximating family to the posterior [11].", "startOffset": 121, "endOffset": 125}, {"referenceID": 8, "context": "In the experiments, we use neural networks, which are flexible enough to approximate a general family of test functions [9].", "startOffset": 120, "endOffset": 123}, {"referenceID": 21, "context": "We will use stochastic optimization [24, 13].", "startOffset": 36, "endOffset": 44}, {"referenceID": 12, "context": "We will use stochastic optimization [24, 13].", "startOffset": 36, "endOffset": 44}, {"referenceID": 11, "context": "An alternative is to use the reparameterization gradient for the second expectation [12, 23].", "startOffset": 84, "endOffset": 92}, {"referenceID": 20, "context": "An alternative is to use the reparameterization gradient for the second expectation [12, 23].", "startOffset": 84, "endOffset": 92}, {"referenceID": 2, "context": "Given a model, operator, and function class parameterization, we can use automatic differentiation to calculate the necessary gradients [3].", "startOffset": 136, "endOffset": 139}, {"referenceID": 7, "context": "With stochastic optimization, data subsampling scales up traditional variational inference to massive data [8, 27].", "startOffset": 107, "endOffset": 114}, {"referenceID": 24, "context": "With stochastic optimization, data subsampling scales up traditional variational inference to massive data [8, 27].", "startOffset": 107, "endOffset": 114}, {"referenceID": 7, "context": "[8] calculate unbiased estimates of the log joint density (and its gradient) by subsampling data and appropriately scaling the sum.", "startOffset": 0, "endOffset": 3}, {"referenceID": 11, "context": "Inference with a variational program requires the family to be reparameterizable [12, 23].", "startOffset": 81, "endOffset": 89}, {"referenceID": 20, "context": "Inference with a variational program requires the family to be reparameterizable [12, 23].", "startOffset": 81, "endOffset": 89}, {"referenceID": 15, "context": "1It is possible to construct rich approximating families with kl(q||p), but this requires the introduction of an auxiliary distribution [16].", "startOffset": 136, "endOffset": 140}, {"referenceID": 22, "context": "We apply logistic factor analysis to analyze the binarizedMNIST data set [25], which contains 28x28 binary pixel images of handwritten digits.", "startOffset": 73, "endOffset": 77}, {"referenceID": 23, "context": "There is no standard for evaluating generative models and their inference algorithms [26].", "startOffset": 85, "endOffset": 89}, {"referenceID": 20, "context": "[23], we consider a missing data problem.", "startOffset": 0, "endOffset": 4}], "year": 2017, "abstractText": "Variational inference is an umbrella term for algorithms which cast Bayesian inference as optimization. Classically, variational inference uses the Kullback-Leibler divergence to define the optimization. Though this divergence has been widely used, the resultant posterior approximation can suffer from undesirable statistical properties. To address this, we reexamine variational inference from its roots as an optimization problem. We use operators, or functions of functions, to design variational objectives. As one example, we design a variational objective with a Langevin-Stein operator. We develop a black box algorithm, operator variational inference (opvi), for optimizing any operator objective. Importantly, operators enable us to make explicit the statistical and computational tradeoffs for variational inference. We can characterize different properties of variational objectives, such as objectives that admit data subsampling\u2014allowing inference to scale to massive data\u2014as well as objectives that admit variational programs\u2014a rich class of posterior approximations that does not require a tractable density. We illustrate the benefits of opvi on a mixture model and a generative model of images.", "creator": "LaTeX with hyperref package"}}}