{"id": "1610.09072", "review": {"conference": "NIPS", "VERSION": "v1", "DATE_OF_SUBMISSION": "28-Oct-2016", "title": "Orthogonal Random Features", "abstract": "We present an intriguing discovery related to Random Fourier Features: in Gaussian kernel approximation, replacing the random Gaussian matrix by a properly scaled random orthogonal matrix significantly decreases kernel approximation error. We call this technique Orthogonal Random Features (ORF), and provide theoretical and empirical justification for this behavior. Motivated by this discovery, we further propose Structured Orthogonal Random Features (SORF), which uses a class of structured discrete orthogonal matrices to speed up the computation. The method reduces the time cost from $\\mathcal{O}(d^2)$ to $\\mathcal{O}(d \\log d)$, where $d$ is the data dimensionality, with almost no compromise in kernel approximation quality compared to ORF. Experiments on several datasets verify the effectiveness of ORF and SORF over the existing methods. We also provide discussions on using the same type of discrete orthogonal structure for a broader range of applications.", "histories": [["v1", "Fri, 28 Oct 2016 03:50:00 GMT  (339kb,D)", "http://arxiv.org/abs/1610.09072v1", "NIPS 2016"]], "COMMENTS": "NIPS 2016", "reviews": [], "SUBJECTS": "cs.LG stat.ML", "authors": ["felix x yu", "ananda theertha suresh", "krzysztof marcin choromanski", "daniel holtmann-rice", "sanjiv kumar"], "accepted": true, "id": "1610.09072"}, "pdf": {"name": "1610.09072.pdf", "metadata": {"source": "CRF", "title": "Orthogonal Random Features", "authors": ["Felix Xinnan Yu", "Ananda Theertha Suresh", "Krzysztof Choromanski", "Daniel Holtmann-Rice", "Sanjiv Kumar"], "emails": ["sanjivk}@google.com"], "sections": [{"heading": "1 Introduction", "text": "Kernel methods are widely used in nonlinear learning [9], but they are computationally expensive for large datasets (K = 2). Kernel approximation is a powerful technique to make kernel methods scalable by mapping input characteristics into a new space where dot products mapping the kernel well [20]. With accurate kernel approximation, efficient linear classifiers can be trained in transformed space while maintaining the expressiveness of nonlinear methods [11, 22].Formally, in the face of a kernel K (\u00b7): Rd \u00b7 Rd \u2192 R, seek kernel approximation methods to find a nonlinear transformation (\u00b7): Rd \u2192 Rd \u2032 so that for each x, y RdK)."}, {"heading": "2 Related Works", "text": "Explicit nonlinear random maps have been designed for many types of nuclei, such as intersecting nuclei [16], generalized RBF nuclei [23], distorted multiplicative histogram nuclei [15], additive nuclei [25], and polynomial nuclei [12, 19]. In this paper, we focus on the approximation of Gaussian nuclei according to the groundbreaking Random Fourier Features (RFF) framework [20], which has been extensively studied both theoretically and empirically [27, 21, 24]. Key to the RFF technique is the Monte Carlo sampling method. It is known that the convergence of MonteCarlo can be improved by careful selection of a deterministic sequence instead of random samples."}, {"heading": "3 Orthogonal Random Features", "text": "The goal is to show the expectation and variance of KRFF (x, y)."}, {"heading": "4 Structured Orthogonal Random Features", "text": "In the previous section, we presented the orthogonal random features (ORF) and provided a theoretical explanation for their effectiveness. Since generating orthogonal matrices in high dimensions can be expensive, we propose here a fast version of ORF by imposing a structure on the orthogonal matrices. This method can offer drastic memory and time savings with minimal compromises in terms of kernel approximation quality. Note that the previous work on fast kernel approximation with structured matrices also does not use structured orthogonal matrices [14, 29, 8]. Let's first introduce a simplified version of ORF: replace S in (2) by a scalar D method. Let's call this method ORF approximation matrix. Thus, the transformation matrix has the following form: WORF-2 = slow Q. (4) Theorem 2. (Appendix B) Let ORF-K2 (the kernel-uncompromised matrix)."}, {"heading": "5 Experiments", "text": "Figure 4 compares the mean square error (MSE) of all methods. For fixed D, the Kernel Approximation MSE has the following sequence: SORFORFORFORFORFORFORFORFORFORFORFORFORFORFORFORFORFORFORFORFORFORFORFORFORFORFORFORFORFORFORFORFORFORFORFORFORFORFORFORFORFORFORFORFORFORFORFORFORFORFORFORFORFORFORFORFORFORFORFORFORFORFORFORFORFORFORFORFORFORFORFORFORFORFORFORFORFORFORFORFORFORFORFORFORFORFORFORFORFORFORFORFORFORFORFORFORFORFORFORFORFORFORFORFORFORFORFORFORFORFORFORFORFORFORFORFORFORFORFORFORFORFORFORFORFORFORFORFORFORFORFORFORFORFORFORFORFORFORFORFORFORFORFORFORFORFORFORFORFORFORFORFORFORFORFORFORFORFORFORFORFORFORFORFORFORFORFORFORFORFORFORFORFORFORFORFORFORFORFORFORFORFORFORFORFORFORFORFORFORFORFORFORFORFORFORFORFORFORFORFORFORFORFORFORFORFORFORFORFORFORFORFORFORFORFORFORFORFORFORFORFORFORFORFORFORFORFORFORFORFORFORFORFORFORFORFOR"}, {"heading": "6 Analysis and General Applicability of the Hadamard-Diagonal Structure", "text": "We provide theoretical discussions about ORF in this section. We first show that SORF is an unbiased estimator of the Gaussian kernel. Theorem 3. (Appendix C) Let KSORF (x, y) be the approximate kernel calculated with linear transformation matrix \u221a dHD1HD3. (Appendix C) Let z = | x \u2212 y | / \u03c3. (Appendix E (x, y)) \u2212 e \u2212 z2 / 2. Let W be a random Gaussian matrix that is almost unbiased and guarantees narrow variance and concentration, similar to ORF, an open question. (The following discussion provides a sketch in this direction.) Let W be a random Gaussian matrix as in RFF, for a given z, the distribution of the heat is N (0, | 2Id)."}, {"heading": "7 Conclusions", "text": "We have shown that the introduction of orthogonality into the transformation matrix can significantly reduce the kernel approximation of random Fourier features in the approximation of Gaussian nuclei. We also proposed a kind of structured orthogonal matrix with substantially lower computing and memory costs. We provided theoretical insights that suggest that the Hadamard diagonal block structure can be widely used to replace random Gaussian matrices in a broader field of application. Our method can also be generalized to other types of nuclei such as general shift-invariant nuclei and polynomic nuclei based on Sch\u00f6nberg's characterization as in [19]."}, {"heading": "Appendix A Variance Reduction via Orthogonal Random Features", "text": "For a vector y (i), its i th coordinate (i) is denoted. Let's remember that in RFF we calculate the product of each number from n to 1, where each number has the same parity as n.A.2 Proof of Lemma 1Let z = (x \u2212 y) / n. Let's be a d-dimensional vector distributing N (0, Id). By Bochner's theorem, E [cos (wT z)] = e-z 2 / 2, and hence RFF results in a d-dimensional vector, the N (0, Id)."}, {"heading": "Appendix B Proof of Theorem 2", "text": "The proof of the theorem is similar to that of Lemma 2 and we outline some key steps (k). We first tied the bias in Lemma 5 and then the variance in Lemma 6. Lemma 5. If w = \u221a, where y is evenly distributed on a sphere of unity, then we can assume that z (e \u2212 z2 / 2 \u2212 e) is along the first coordinate and hence wT z = 2 z44d). One way to create a uniformly distributed random variable on a sphere with radius z is that we d = (1) independent random variables x = (1), x (2), x (d), each distributed N (0, 1) and the setting y (i)."}, {"heading": "Appendix C Proof of Theorem 3", "text": "The proof arises from the following two technical words: Lemma 7. Let us z '( f'i) (f'i) (f'i) (f'i) (f'i) (f'i) (f'i) (f'i) (f'i) (f'i) (f'i) (f'i) (f'i) (f'i) (f'i) (f'i) (f) (f) (f) (f) (f) (f) (f) (f) (f) (f) (f) (f) (f) (f) (f) (f) (f) (f). \"(f) (f) (f) (f) (f) (f) (f) (i) (f) (f) (f) (f) (f) (f) (f) (f) (f) (f) (f) (f) (i).\" (f) (f) (i) (f) (f) (f) (f) (f) (f) (f) (f) (i) (f) (f) (f) (f) (i) (f) (f) (f) (f) (i) (f) (f) (f) (f) (i) (f) (f) (f) (f) (f) (i) (f) (f) (f) (i) (f) (f) (f) (f) (i. \"(f) (i.\" (f) (f (f) (f) (i) (f (f) (f) (f) (f) (i. \"(f) (f) (i.\" (f) (f) (f) (f (f) (f) (f) (f) (f) (f) (f) (f (i. \"(f (f) (f) (f) (f) (f) (i.\" (f) (f (f) (f) (f) (f) (f) (f) (f (f (f) (f) (i. \"(f) (i.\" (f) (f) (f) (f"}, {"heading": "Appendix D Proof of Theorem 4", "text": "To prove that we have the vector to the diagonals of D. Let v = 3HDV (HDV = HDV = HDV = HDV), we use the Hanson-Wright-Inequality.Lemma 9 (HDV = HDV = HDV) -Inequality. Let's (H2) -Inequality (H2) -Inequality (H2) -Inequality (H2) -Itequality (H2) -Itequality (H2) -Itequality (H2) -Itequality (H2) -Itequality (H2) -Itection (H2) -Itection (H2) -Itection (H2) -Itection (H2) -Itection (H2) -Itection (H2) (H2) -Itection (H2) -Itection (H2) -Itection (H2) -Itection (H2) -Itection (H2) -Itection (H2) -Itection (H2) -Itection (H2) -Itection (H2) -Itection (H2) -Itection (H2) -Itection (H2) -Itection (H2) -Itection (H2) -Itection (H2) -2) -Itection (H2) -Itection (H2) -Itection (H2) -2) -Itection (H2) -Itection (H2) -2) -Itection (H2) -2) -2 (H2) -2) (H2) -2)"}, {"heading": "Appendix E Discrete Hadamard-Diagonal Structure in Binary Embedding", "text": "Motivated by recent advances in the use of structured matrices for binary embedding, we show empirically that the same type of structured discrete orthogonal matrices (three blocks of HadamardDiagonal Matrices) can also be applied to approximate angular distances for high-dimensional data. Let's say W-RD-D is a random matrix with i.e. normally distributed entries. The classical LocalitySensitive Hashing (LSH) result shows that the character nonlinear map \u03c6 (x) = 1 \u221a D character (Wx) can be used to approximate the angle, i.e. for any x, y, y, Rd\u03c6 (x) T\u03c6 (y) \u2248 \u03b8 (x, y) / \u03c0.We compare random projections based on Locality Sensitive Hashing (LSH) [4], Circulant Binary Embedding (CBE) [28] and KBE (Binary Embedding)."}], "references": [{"title": "Approximate nearest neighbors and the fast Johnson-Lindenstrauss transform", "author": ["N. Ailon", "B. Chazelle"], "venue": "STOC,", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2006}, {"title": "Practical and optimal lsh for angular distance", "author": ["A. Andoni", "P. Indyk", "T. Laarhoven", "I. Razenshteyn", "L. Schmidt"], "venue": "NIPS,", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2015}, {"title": "Harmonic analysis and the theory of probability", "author": ["S. Bochner"], "venue": "Dover Publications,", "citeRegEx": "3", "shortCiteRegEx": null, "year": 1955}, {"title": "Similarity estimation techniques from rounding algorithms", "author": ["M.S. Charikar"], "venue": "STOC,", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2002}, {"title": "Lecture notes on Stein\u2019s method and applications", "author": ["S. Chatterjee"], "venue": null, "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2007}, {"title": "An exploration of parameter redundancy in deep networks with circulant projections", "author": ["Y. Cheng", "F.X. Yu", "R.S. Feris", "S. Kumar", "A. Choudhary", "S.-F. Chang"], "venue": "ICCV,", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2015}, {"title": "Triplespin-a generic compact paradigm for fast machine learning computations", "author": ["K. Choromanski", "F. Fagan", "C. Gouy-Pailler", "A. Morvan", "T. Sarlos", "J. Atif"], "venue": "arXiv,", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2016}, {"title": "Recycling randomness with structure for sublinear time kernel expansions", "author": ["K. Choromanski", "V. Sindhwani"], "venue": "ICML,", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2015}, {"title": "Support-vector networks", "author": ["C. Cortes", "V. Vapnik"], "venue": "Machine Learning, 20(3):273\u2013297,", "citeRegEx": "9", "shortCiteRegEx": null, "year": 1995}, {"title": "Unified matrix treatment of the fast walsh-hadamard transform", "author": ["B.J. Fino", "V.R. Algazi"], "venue": "IEEE Transactions on Computers, (11):1142\u20131146,", "citeRegEx": "10", "shortCiteRegEx": null, "year": 1976}, {"title": "Training linear SVMs in linear time", "author": ["T. Joachims"], "venue": "KDD,", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2006}, {"title": "Random feature maps for dot product kernels", "author": ["P. Kar", "H. Karnick"], "venue": "AISTATS,", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2012}, {"title": "Fast cross-polytope locality-sensitive hashing", "author": ["C. Kennedy", "R. Ward"], "venue": "arXiv,", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2016}, {"title": "Fastfood \u2013 approximating kernel expansions in loglinear time", "author": ["Q. Le", "T. Sarl\u00f3s", "A. Smola"], "venue": "ICML,", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2013}, {"title": "Random fourier approximations for skewed multiplicative histogram kernels", "author": ["F. Li", "C. Ionescu", "C. Sminchisescu"], "venue": "Pattern Recognition, pages 262\u2013271,", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2010}, {"title": "Max-margin additive classifiers for detection", "author": ["S. Maji", "A.C. Berg"], "venue": "ICCV,", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2009}, {"title": "Aspects of multivariate statistical theory, volume 197", "author": ["R.J. Muirhead"], "venue": "John Wiley & Sons,", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2009}, {"title": "Quasi-Monte Carlo Methods", "author": ["H. Niederreiter"], "venue": "Wiley Online Library,", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2010}, {"title": "Spherical random features for polynomial kernels", "author": ["J. Pennington", "F.X. Yu", "S. Kumar"], "venue": "NIPS,", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2015}, {"title": "Random features for large-scale kernel machines", "author": ["A. Rahimi", "B. Recht"], "venue": "NIPS,", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2007}, {"title": "Generalization properties of learning with random features", "author": ["A. Rudi", "R. Camoriano", "L. Rosasco"], "venue": "arXiv:1602.04474,", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2016}, {"title": "Pegasos: Primal estimated sub-gradient solver for SVM, volume = 127, year", "author": ["S. Shalev-Shwartz", "Y. Singer", "N. Srebro", "A. Cotter"], "venue": "Mathematical Programming,", "citeRegEx": "22", "shortCiteRegEx": "22", "year": 2011}, {"title": "Generalized RBF feature maps for efficient detection", "author": ["V. Sreekanth", "A. Vedaldi", "A. Zisserman", "C. Jawahar"], "venue": "BMVC,", "citeRegEx": "23", "shortCiteRegEx": null, "year": 2010}, {"title": "Optimal rates for random fourier features", "author": ["B. Sriperumbudur", "Z. Szab\u00f3"], "venue": "NIPS,", "citeRegEx": "24", "shortCiteRegEx": null, "year": 2015}, {"title": "Efficient additive kernels via explicit feature maps", "author": ["A. Vedaldi", "A. Zisserman"], "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence, 34(3):480\u2013492,", "citeRegEx": "25", "shortCiteRegEx": null, "year": 2012}, {"title": "Quasi-monte carlo feature maps for shift-invariant kernels", "author": ["J. Yang", "V. Sindhwani", "H. Avron", "M. Mahoney"], "venue": "ICML,", "citeRegEx": "26", "shortCiteRegEx": null, "year": 2014}, {"title": "Nystr\u00f6m method vs random fourier features: A theoretical and empirical comparison", "author": ["T. Yang", "Y.-F. Li", "M. Mahdavi", "R. Jin", "Z.-H. Zhou"], "venue": "NIPS,", "citeRegEx": "27", "shortCiteRegEx": null, "year": 2012}, {"title": "Circulant binary embedding", "author": ["F.X. Yu", "S. Kumar", "Y. Gong", "S.-F. Chang"], "venue": "ICML,", "citeRegEx": "28", "shortCiteRegEx": null, "year": 2014}, {"title": "Compact nonlinear maps and circulant extensions", "author": ["F.X. Yu", "S. Kumar", "H. Rowley", "S.-F. Chang"], "venue": "arXiv:1503.03893,", "citeRegEx": "29", "shortCiteRegEx": null, "year": 2015}, {"title": "Fast orthogonal projection based on kronecker product", "author": ["X. Zhang", "F.X. Yu", "R. Guo", "S. Kumar", "S. Wang", "S.-F. Chang"], "venue": "ICCV,", "citeRegEx": "30", "shortCiteRegEx": null, "year": 2015}], "referenceMentions": [{"referenceID": 8, "context": "1 Introduction Kernel methods are widely used in nonlinear learning [9], but they are computationally expensive for large datasets.", "startOffset": 68, "endOffset": 71}, {"referenceID": 19, "context": "Kernel approximation is a powerful technique to make kernel methods scalable, by mapping input features into a new space where dot products approximate the kernel well [20].", "startOffset": 168, "endOffset": 172}, {"referenceID": 10, "context": "With accurate kernel approximation, efficient linear classifiers can be trained in the transformed space while retaining the expressive power of nonlinear methods [11, 22].", "startOffset": 163, "endOffset": 171}, {"referenceID": 21, "context": "With accurate kernel approximation, efficient linear classifiers can be trained in the transformed space while retaining the expressive power of nonlinear methods [11, 22].", "startOffset": 163, "endOffset": 171}, {"referenceID": 19, "context": "Random Fourier Features [20] are used widely in approximating smooth, shift-invariant kernels.", "startOffset": 24, "endOffset": 28}, {"referenceID": 2, "context": "The second property guarantees that the Fourier transform of K(\u2206) is a nonnegative function [3].", "startOffset": 92, "endOffset": 95}, {"referenceID": 15, "context": "2 Related Works Explicit nonlinear random feature maps have been constructed for many types of kernels, such as intersection kernels [16], generalized RBF kernels [23], skewed multiplicative histogram kernels [15], additive kernels [25], and polynomial kernels [12, 19].", "startOffset": 133, "endOffset": 137}, {"referenceID": 22, "context": "2 Related Works Explicit nonlinear random feature maps have been constructed for many types of kernels, such as intersection kernels [16], generalized RBF kernels [23], skewed multiplicative histogram kernels [15], additive kernels [25], and polynomial kernels [12, 19].", "startOffset": 163, "endOffset": 167}, {"referenceID": 14, "context": "2 Related Works Explicit nonlinear random feature maps have been constructed for many types of kernels, such as intersection kernels [16], generalized RBF kernels [23], skewed multiplicative histogram kernels [15], additive kernels [25], and polynomial kernels [12, 19].", "startOffset": 209, "endOffset": 213}, {"referenceID": 24, "context": "2 Related Works Explicit nonlinear random feature maps have been constructed for many types of kernels, such as intersection kernels [16], generalized RBF kernels [23], skewed multiplicative histogram kernels [15], additive kernels [25], and polynomial kernels [12, 19].", "startOffset": 232, "endOffset": 236}, {"referenceID": 11, "context": "2 Related Works Explicit nonlinear random feature maps have been constructed for many types of kernels, such as intersection kernels [16], generalized RBF kernels [23], skewed multiplicative histogram kernels [15], additive kernels [25], and polynomial kernels [12, 19].", "startOffset": 261, "endOffset": 269}, {"referenceID": 18, "context": "2 Related Works Explicit nonlinear random feature maps have been constructed for many types of kernels, such as intersection kernels [16], generalized RBF kernels [23], skewed multiplicative histogram kernels [15], additive kernels [25], and polynomial kernels [12, 19].", "startOffset": 261, "endOffset": 269}, {"referenceID": 19, "context": "In this paper, we focus on approximating Gaussian kernels following the seminal Random Fourier Features (RFF) framework [20], which has been extensively studied both theoretically and empirically [27, 21, 24].", "startOffset": 120, "endOffset": 124}, {"referenceID": 26, "context": "In this paper, we focus on approximating Gaussian kernels following the seminal Random Fourier Features (RFF) framework [20], which has been extensively studied both theoretically and empirically [27, 21, 24].", "startOffset": 196, "endOffset": 208}, {"referenceID": 20, "context": "In this paper, we focus on approximating Gaussian kernels following the seminal Random Fourier Features (RFF) framework [20], which has been extensively studied both theoretically and empirically [27, 21, 24].", "startOffset": 196, "endOffset": 208}, {"referenceID": 23, "context": "In this paper, we focus on approximating Gaussian kernels following the seminal Random Fourier Features (RFF) framework [20], which has been extensively studied both theoretically and empirically [27, 21, 24].", "startOffset": 196, "endOffset": 208}, {"referenceID": 17, "context": "It is well known that the convergence of MonteCarlo can be largely improved by carefully choosing a deterministic sequence instead of random samples [18].", "startOffset": 149, "endOffset": 153}, {"referenceID": 25, "context": "[26] proposed to use low-displacement rank sequences in RFF.", "startOffset": 0, "endOffset": 4}, {"referenceID": 28, "context": "[29] studied optimizing the sequences in a data-dependent fashion to achieve more compact maps.", "startOffset": 0, "endOffset": 4}, {"referenceID": 25, "context": "Compared to [26], the proposed SORF method achieves both lower kernel approximation error and greatly reduced computation and memory costs.", "startOffset": 12, "endOffset": 16}, {"referenceID": 28, "context": "Furthermore, unlike [29], the results in this paper are data independent.", "startOffset": 20, "endOffset": 24}, {"referenceID": 0, "context": "Structured matrices have been used for speeding up dimensionality reduction [1], binary embedding [28], deep neural networks [6] and kernel approximation [14, 29, 8].", "startOffset": 76, "endOffset": 79}, {"referenceID": 27, "context": "Structured matrices have been used for speeding up dimensionality reduction [1], binary embedding [28], deep neural networks [6] and kernel approximation [14, 29, 8].", "startOffset": 98, "endOffset": 102}, {"referenceID": 5, "context": "Structured matrices have been used for speeding up dimensionality reduction [1], binary embedding [28], deep neural networks [6] and kernel approximation [14, 29, 8].", "startOffset": 125, "endOffset": 128}, {"referenceID": 13, "context": "Structured matrices have been used for speeding up dimensionality reduction [1], binary embedding [28], deep neural networks [6] and kernel approximation [14, 29, 8].", "startOffset": 154, "endOffset": 165}, {"referenceID": 28, "context": "Structured matrices have been used for speeding up dimensionality reduction [1], binary embedding [28], deep neural networks [6] and kernel approximation [14, 29, 8].", "startOffset": 154, "endOffset": 165}, {"referenceID": 7, "context": "Structured matrices have been used for speeding up dimensionality reduction [1], binary embedding [28], deep neural networks [6] and kernel approximation [14, 29, 8].", "startOffset": 154, "endOffset": 165}, {"referenceID": 19, "context": "Method Extra Memory Time Lower error than RFF? Random Fourier Feature (RFF) [20] O(Dd) O(Dd) Compact Nonlinear Map (CNM) [29] O(Dd) O(Dd) Yes (data-dependent) Quasi-Monte Carlo (QMC) [26] O(Dd) O(Dd) Yes Structured (fastfood/circulant) [29, 14] O(D) O(D log d) No Orthogonal Random Feature (ORF) O(Dd) O(Dd) Yes Structured ORF (SORF) O(D) or O(1) O(D log d) Yes Table 1: Comparison of different kernel approximation methods under the framework of Random Fourier Features [20].", "startOffset": 76, "endOffset": 80}, {"referenceID": 28, "context": "Method Extra Memory Time Lower error than RFF? Random Fourier Feature (RFF) [20] O(Dd) O(Dd) Compact Nonlinear Map (CNM) [29] O(Dd) O(Dd) Yes (data-dependent) Quasi-Monte Carlo (QMC) [26] O(Dd) O(Dd) Yes Structured (fastfood/circulant) [29, 14] O(D) O(D log d) No Orthogonal Random Feature (ORF) O(Dd) O(Dd) Yes Structured ORF (SORF) O(D) or O(1) O(D log d) Yes Table 1: Comparison of different kernel approximation methods under the framework of Random Fourier Features [20].", "startOffset": 121, "endOffset": 125}, {"referenceID": 25, "context": "Method Extra Memory Time Lower error than RFF? Random Fourier Feature (RFF) [20] O(Dd) O(Dd) Compact Nonlinear Map (CNM) [29] O(Dd) O(Dd) Yes (data-dependent) Quasi-Monte Carlo (QMC) [26] O(Dd) O(Dd) Yes Structured (fastfood/circulant) [29, 14] O(D) O(D log d) No Orthogonal Random Feature (ORF) O(Dd) O(Dd) Yes Structured ORF (SORF) O(D) or O(1) O(D log d) Yes Table 1: Comparison of different kernel approximation methods under the framework of Random Fourier Features [20].", "startOffset": 183, "endOffset": 187}, {"referenceID": 28, "context": "Method Extra Memory Time Lower error than RFF? Random Fourier Feature (RFF) [20] O(Dd) O(Dd) Compact Nonlinear Map (CNM) [29] O(Dd) O(Dd) Yes (data-dependent) Quasi-Monte Carlo (QMC) [26] O(Dd) O(Dd) Yes Structured (fastfood/circulant) [29, 14] O(D) O(D log d) No Orthogonal Random Feature (ORF) O(Dd) O(Dd) Yes Structured ORF (SORF) O(D) or O(1) O(D log d) Yes Table 1: Comparison of different kernel approximation methods under the framework of Random Fourier Features [20].", "startOffset": 236, "endOffset": 244}, {"referenceID": 13, "context": "Method Extra Memory Time Lower error than RFF? Random Fourier Feature (RFF) [20] O(Dd) O(Dd) Compact Nonlinear Map (CNM) [29] O(Dd) O(Dd) Yes (data-dependent) Quasi-Monte Carlo (QMC) [26] O(Dd) O(Dd) Yes Structured (fastfood/circulant) [29, 14] O(D) O(D log d) No Orthogonal Random Feature (ORF) O(Dd) O(Dd) Yes Structured ORF (SORF) O(D) or O(1) O(D log d) Yes Table 1: Comparison of different kernel approximation methods under the framework of Random Fourier Features [20].", "startOffset": 236, "endOffset": 244}, {"referenceID": 19, "context": "Method Extra Memory Time Lower error than RFF? Random Fourier Feature (RFF) [20] O(Dd) O(Dd) Compact Nonlinear Map (CNM) [29] O(Dd) O(Dd) Yes (data-dependent) Quasi-Monte Carlo (QMC) [26] O(Dd) O(Dd) Yes Structured (fastfood/circulant) [29, 14] O(D) O(D log d) No Orthogonal Random Feature (ORF) O(Dd) O(Dd) Yes Structured ORF (SORF) O(D) or O(1) O(D log d) Yes Table 1: Comparison of different kernel approximation methods under the framework of Random Fourier Features [20].", "startOffset": 471, "endOffset": 475}, {"referenceID": 16, "context": "Q is distributed uniformly on the Stiefel manifold (the space of all orthogonal matrices) based on the Bartlett decomposition theorem [17].", "startOffset": 134, "endOffset": 138}, {"referenceID": 13, "context": "Note that the previous works on fast kernel approximation using structured matrices do not use structured orthogonal matrices [14, 29, 8].", "startOffset": 126, "endOffset": 137}, {"referenceID": 28, "context": "Note that the previous works on fast kernel approximation using structured matrices do not use structured orthogonal matrices [14, 29, 8].", "startOffset": 126, "endOffset": 137}, {"referenceID": 7, "context": "Note that the previous works on fast kernel approximation using structured matrices do not use structured orthogonal matrices [14, 29, 8].", "startOffset": 126, "endOffset": 137}, {"referenceID": 9, "context": "The computation of SORF can also be carried out with almost no extra memory due to the fact that both sign flipping and the Walsh-Hadamard transformation can be efficiently implemented as in-place operations [10].", "startOffset": 208, "endOffset": 212}, {"referenceID": 25, "context": "For fixed D, the kernel approximation MSE exhibits the following ordering: SORF ' ORF < QMC [26] < RFF [20] < Other fast kernel approximations [14, 29].", "startOffset": 92, "endOffset": 96}, {"referenceID": 19, "context": "For fixed D, the kernel approximation MSE exhibits the following ordering: SORF ' ORF < QMC [26] < RFF [20] < Other fast kernel approximations [14, 29].", "startOffset": 103, "endOffset": 107}, {"referenceID": 13, "context": "For fixed D, the kernel approximation MSE exhibits the following ordering: SORF ' ORF < QMC [26] < RFF [20] < Other fast kernel approximations [14, 29].", "startOffset": 143, "endOffset": 151}, {"referenceID": 28, "context": "For fixed D, the kernel approximation MSE exhibits the following ordering: SORF ' ORF < QMC [26] < RFF [20] < Other fast kernel approximations [14, 29].", "startOffset": 143, "endOffset": 151}, {"referenceID": 28, "context": "All other fast kernel approximation methods, such as circulant [29] and FastFood [14] have higher MSE.", "startOffset": 63, "endOffset": 67}, {"referenceID": 13, "context": "All other fast kernel approximation methods, such as circulant [29] and FastFood [14] have higher MSE.", "startOffset": 81, "endOffset": 85}, {"referenceID": 25, "context": "We also include DigitalNet, the best performing method among Quasi-Monte Carlo techniques [26].", "startOffset": 90, "endOffset": 94}, {"referenceID": 13, "context": "The order of time cost for a fixed D is SORF ' Other fast kernel approximations [14, 29] ORF = QMC [26] = RFF [20].", "startOffset": 80, "endOffset": 88}, {"referenceID": 28, "context": "The order of time cost for a fixed D is SORF ' Other fast kernel approximations [14, 29] ORF = QMC [26] = RFF [20].", "startOffset": 80, "endOffset": 88}, {"referenceID": 25, "context": "The order of time cost for a fixed D is SORF ' Other fast kernel approximations [14, 29] ORF = QMC [26] = RFF [20].", "startOffset": 99, "endOffset": 103}, {"referenceID": 19, "context": "The order of time cost for a fixed D is SORF ' Other fast kernel approximations [14, 29] ORF = QMC [26] = RFF [20].", "startOffset": 110, "endOffset": 114}, {"referenceID": 28, "context": "Throughout the experiments, \u03c3 for each dataset is chosen to be the mean distance of the 50th `2 nearest neighbor, which empirically yields good classification results [29].", "startOffset": 167, "endOffset": 171}, {"referenceID": 3, "context": "In the appendix, we show empirically that the same scheme can be successfully applied to angle estimation where the nonlinear map f is a non-smooth sign(\u00b7) function [4].", "startOffset": 165, "endOffset": 168}, {"referenceID": 1, "context": "We note that the HD1HD2HD3 structure has also been recently used in fast cross-polytope LSH [2, 13, 7].", "startOffset": 92, "endOffset": 102}, {"referenceID": 12, "context": "We note that the HD1HD2HD3 structure has also been recently used in fast cross-polytope LSH [2, 13, 7].", "startOffset": 92, "endOffset": 102}, {"referenceID": 6, "context": "We note that the HD1HD2HD3 structure has also been recently used in fast cross-polytope LSH [2, 13, 7].", "startOffset": 92, "endOffset": 102}, {"referenceID": 18, "context": "Our method can also be generalized to other types of kernels such as general shift-invariant kernels and polynomial kernels based on Schoenberg\u2019s characterization as in [19].", "startOffset": 169, "endOffset": 173}], "year": 2016, "abstractText": "We present an intriguing discovery related to Random Fourier Features: in Gaussian kernel approximation, replacing the random Gaussian matrix by a properly scaled random orthogonal matrix significantly decreases kernel approximation error. We call this technique Orthogonal Random Features (ORF), and provide theoretical and empirical justification for this behavior. Motivated by this discovery, we further propose Structured Orthogonal Random Features (SORF), which uses a class of structured discrete orthogonal matrices to speed up the computation. The method reduces the time cost fromO(d) toO(d log d), where d is the data dimensionality, with almost no compromise in kernel approximation quality compared to ORF. Experiments on several datasets verify the effectiveness of ORF and SORF over the existing methods. We also provide discussions on using the same type of discrete orthogonal structure for a broader range of applications.", "creator": "LaTeX with hyperref package"}}}