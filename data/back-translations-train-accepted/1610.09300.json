{"id": "1610.09300", "review": {"conference": "NIPS", "VERSION": "v1", "DATE_OF_SUBMISSION": "28-Oct-2016", "title": "Globally Optimal Training of Generalized Polynomial Neural Networks with Nonlinear Spectral Methods", "abstract": "The optimization problem behind neural networks is highly non-convex. Training with stochastic gradient descent and variants requires careful parameter tuning and provides no guarantee to achieve the global optimum. In contrast we show under quite weak assumptions on the data that a particular class of feedforward neural networks can be trained globally optimal with a linear convergence rate with our nonlinear spectral method. Up to our knowledge this is the first practically feasible method which achieves such a guarantee. While the method can in principle be applied to deep networks, we restrict ourselves for simplicity in this paper to one and two hidden layer networks. Our experiments confirm that these models are rich enough to achieve good performance on a series of real-world datasets.", "histories": [["v1", "Fri, 28 Oct 2016 16:28:23 GMT  (310kb,D)", "http://arxiv.org/abs/1610.09300v1", "Long version of NIPS 2016 paper"]], "COMMENTS": "Long version of NIPS 2016 paper", "reviews": [], "SUBJECTS": "cs.LG math.OC stat.ML", "authors": ["antoine gautier", "quynh n nguyen", "matthias hein 0001"], "accepted": true, "id": "1610.09300"}, "pdf": {"name": "1610.09300.pdf", "metadata": {"source": "CRF", "title": "Globally Optimal Training of Generalized Polynomial Neural Networks with Nonlinear Spectral Methods", "authors": ["A. Gautier", "Q. Nguyen"], "emails": [], "sections": [{"heading": "1 Introduction", "text": "In fact, the fact is that most of them are able to move, to move and to move."}, {"heading": "2 Main result", "text": "We are not able to generalize the number of classes. We use the negative entropy theory defined for the y label: = {K} and classifier f: Rd \u2192 RK as () The function class we use is a feedback neural network with a hidden layer with hidden units. As activation functions we use real forces in the shape of a polyomial, which for us is a definition of feedback neural networks with a hidden layer with hidden units."}, {"heading": "3 From the optimization problem to fixed point theory", "text": "Let us ask whether we will be able to find a solution that we are not satisfied with. (wb, ub) Let us find a solution. (wb, ub) Let us find a solution. (wb, ub) Let us find a solution. (wb, ub) Let us find a solution. (wb, ub) Let us find a solution. (wb, ub) Let us find a solution. (wb, ub) Let us find a solution. (wb, ub) Let us find a solution. (wb, ub) Let us find a solution. (wb, ub) Let us find a solution. (wb1) Let us find a solution. (wb1) Let us find a solution. (wb1) Let us find a solution. (wb1) Let us find a solution."}, {"heading": "4 Application to Neural Networks", "text": "In the previous sections we have outlined the proof of our main result for a general objective function that fulfils certain properties, the purpose of which is to prove that the properties apply to our neural network optimization problem. We remember our objective function of (2), (w, u) = 1 n n n, (w, u), (w, u), (w, u), (K) = 1 n, l + n1, l + n1, l, l, l, l, l = 1 d, m = 1 ulm, and the function class we are considering, (1) fr (x) = fr (w, u) (x) = n1, l, l (d, m = 1 ulmxm)."}, {"heading": "4.1 Neural networks with two hidden layers", "text": "We show how we can expand our framework for neural networks with 2 hidden layers."}, {"heading": "5 Experiments", "text": "\"We have not yet understood that we are in a position,\" he said. \"We have not yet understood what we must do.\" (\"We have not yet understood\"). (\"We have not yet understood.\"). (\"We have not yet understood.\") (\"We.\"). (\"We.\"). (\"We.\"). (\"We.\"). (\"We.\"). (\"We.\"). (\"We.\"). (\"We.\"). (\"We.\" (\"We.\"). (\"We.\"). (\"We.\" (\"We.\"). (\"We.\" (\"We.\"). (\"We.\"). (\"We.\" (\"We.\"). (\"We.\"). (\"We.\" (\"We.\"). (\"We.\"). (\"We.\" (\"We.\"). (\"We.\"). (\"We.\" (\"We.\"). (\"). (\" We. \"(\" We. \"). (\" (\"We.\"). (\"We.\"). (\"(\" We. \"). (\" (\"We.\"). (\"(\" We. \"). (\" (\"). (\" We. (\"We. (\"). (\"). (\" (\"We. (\" We. \"). (. (\"). (\"We. (. (.). (.). (.). (\" We. (.). (.). (. (\"We.). (. (.). (\" We.). (. (.). (. (.). (. (\"We. (.). (. (.). (.). (.). (\" We.). (.). (. (\"We. (.). (. (.). (.). (. (.). (.). (.). (\" We. (. (.). (.). (.). (. (. (.). (.). (.). (. (.). (.). ("}, {"heading": "Acknowledgment", "text": "The authors acknowledge the support of the ERC Start-Up Fund NOLEPRO 307793."}], "references": [{"title": "Neural Network Learning: Theoretical Foundations", "author": ["M. Anthony", "P. Bartlett"], "venue": "Cambridge University Press, New York", "citeRegEx": "1", "shortCiteRegEx": null, "year": 1999}, {"title": "Provable bounds for learning some deep representations", "author": ["S. Arora", "A. Bhaskara", "R. Ge", "T. Ma"], "venue": "ICML", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2014}, {"title": "Nonnegative Matrices in the Mathematical Sciences", "author": ["A. Berman", "R.J. Plemmons"], "venue": "SIAM, Philadelphia", "citeRegEx": "3", "shortCiteRegEx": null, "year": 1994}, {"title": "Nonlinear Programming", "author": ["D.P. Bertsekas"], "venue": "Athena Scientific, Belmont, Mass.", "citeRegEx": "4", "shortCiteRegEx": null, "year": 1999}, {"title": "The loss surfaces of multilayer networks", "author": ["A. Choromanska", "M. Hena", "M. Mathieu", "G.B. Arous", "Y. LeCun"], "venue": "AISTATS", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2015}, {"title": "and Y", "author": ["A Daniely", "R. Frostigy"], "venue": "Singer. Toward deeper understanding of neural networks: The power of initialization and a dual view on expressivity", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2016}, {"title": "The Perron-Frobenius Theorem for Multi-Homogeneous Maps", "author": ["A. Gautier", "F. Tudisco", "M. Hein"], "venue": "preparation", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2016}, {"title": "Global optimality in tensor factorization, deep learning, and beyond", "author": ["B.D. Haeffele", "Rene Vidal"], "venue": null, "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2015}, {"title": "Train faster", "author": ["M. Hardt", "B. Recht", "Y. Singer"], "venue": "generalize better: Stability of stochastic gradient descent. In ICML", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2016}, {"title": "Matrix Analysis", "author": ["R.A. Horn", "C.R. Johnson"], "venue": "Cambridge University Press, New York, second edition", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2013}, {"title": "and A", "author": ["M. Janzamin", "H. Sedghi"], "venue": "Anandkumar. Beating the perils of non-convexity:guaranteed training of neural networks using tensor methods", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2015}, {"title": "An Introduction to Metric Spaces and Fixed Point Theory", "author": ["W.A. Kirk", "M.A. Khamsi"], "venue": "John Wiley, New York", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2001}, {"title": "Deep learning", "author": ["Y. LeCun", "Y. Bengio", "G. Hinton"], "venue": "Nature, 521", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2015}, {"title": "Nonlinear Perron-Frobenius theory", "author": ["B. Lemmens", "R.D. Nussbaum"], "venue": "Cambridge University Press, New York, general edition", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2012}, {"title": "On the computational efficiency of training neural networks", "author": ["R. Livni", "S. Shalev-Shwartz", "O. Shamir"], "venue": "NIPS, pages 855\u2013863", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2014}, {"title": "Deep Learning in Neural Networks: An Overview", "author": ["J. Schmidhuber"], "venue": "Neural Networks, 61:85\u2013117", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2015}, {"title": "Training a single sigmoidal neuron is hard", "author": ["J. Sima"], "venue": "Neural Computation, 14:2709\u20132728", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2002}, {"title": "On certain contraction mappings in a partially ordered vector space", "author": ["A.C. Thompson"], "venue": "Proceedings of the American Mathematical Society, 14:438\u2013443", "citeRegEx": "18", "shortCiteRegEx": null, "year": 1963}], "referenceMentions": [{"referenceID": 12, "context": "1 Introduction Deep learning [13, 16] is currently the state of the art machine learning technique in many application areas such as computer vision or natural language processing.", "startOffset": 29, "endOffset": 37}, {"referenceID": 15, "context": "1 Introduction Deep learning [13, 16] is currently the state of the art machine learning technique in many application areas such as computer vision or natural language processing.", "startOffset": 29, "endOffset": 37}, {"referenceID": 0, "context": "[1], the understanding of the success of training deep neural networks is a currently very active research area [5, 6, 9].", "startOffset": 0, "endOffset": 3}, {"referenceID": 4, "context": "[1], the understanding of the success of training deep neural networks is a currently very active research area [5, 6, 9].", "startOffset": 112, "endOffset": 121}, {"referenceID": 5, "context": "[1], the understanding of the success of training deep neural networks is a currently very active research area [5, 6, 9].", "startOffset": 112, "endOffset": 121}, {"referenceID": 8, "context": "[1], the understanding of the success of training deep neural networks is a currently very active research area [5, 6, 9].", "startOffset": 112, "endOffset": 121}, {"referenceID": 16, "context": "In particular, the problem is even for a single hidden layer in general NP hard, see [17] and references therein.", "startOffset": 85, "endOffset": 89}, {"referenceID": 1, "context": "A recent line of research has directly tackled the optimization problem of neural networks and provided either certain guarantees [2, 15] in terms of the global optimum or proved directly convergence to the global optimum [8, 11].", "startOffset": 130, "endOffset": 137}, {"referenceID": 14, "context": "A recent line of research has directly tackled the optimization problem of neural networks and provided either certain guarantees [2, 15] in terms of the global optimum or proved directly convergence to the global optimum [8, 11].", "startOffset": 130, "endOffset": 137}, {"referenceID": 7, "context": "A recent line of research has directly tackled the optimization problem of neural networks and provided either certain guarantees [2, 15] in terms of the global optimum or proved directly convergence to the global optimum [8, 11].", "startOffset": 222, "endOffset": 229}, {"referenceID": 10, "context": "A recent line of research has directly tackled the optimization problem of neural networks and provided either certain guarantees [2, 15] in terms of the global optimum or proved directly convergence to the global optimum [8, 11].", "startOffset": 222, "endOffset": 229}, {"referenceID": 7, "context": "While providing a lot of interesting insights on the relationship of structured matrix factorization and training of neural networks, Haeffele and Vidal admit themselves in their paper [8] that their results are \u201cchallenging to apply in practice\u201d.", "startOffset": 185, "endOffset": 188}, {"referenceID": 10, "context": "[11] they use a tensor approach and propose a globally optimal algorithm for a feedforward neural network with one hidden layer and squared loss.", "startOffset": 0, "endOffset": 4}, {"referenceID": 14, "context": "Polynomial neural networks have been recently analyzed in [15].", "startOffset": 58, "endOffset": 62}, {"referenceID": 13, "context": "The reason to write this as a maximization problem is that our nonlinear spectral method is inspired by the theory of (sub)-homogeneous nonlinear eigenproblems on convex cones [14] which has its origin in the Perron-Frobenius theory for nonnegative matrices.", "startOffset": 176, "endOffset": 180}, {"referenceID": 6, "context": "In fact our work is motivated by the closely related Perron-Frobenius theory for multihomogeneous problems developed in [7].", "startOffset": 120, "endOffset": 123}, {"referenceID": 2, "context": "30 [3].", "startOffset": 3, "endOffset": 6}, {"referenceID": 9, "context": "26 in [10]) that \u03c1(A) = \u03c1(A ) \u2264 maxi\u2208[K+1](A v)i/vi for any v \u2208 RK+1 ++ .", "startOffset": 6, "endOffset": 10}, {"referenceID": 3, "context": "A necessary and sufficient condition [4] for (w, u) \u2208 S++ being a critical point of \u03a6 is the existence of \u03bbi with \u2207wj\u03a6(w, u) = \u03bbj\u03c8pw(wj) \u2200j \u2208 [K] and \u2207u\u03a6(w, u) = \u03bbK+1\u03c8pu(u).", "startOffset": 37, "endOffset": 40}, {"referenceID": 11, "context": "[12]).", "startOffset": 0, "endOffset": 4}, {"referenceID": 17, "context": "A popular metric for the study of nonlinear eigenvalue problems on the positive orthant is the so-called Thompson metric d : R++ \u00d7 R++ \u2192 R+ [18] defined as d(z, z\u0303) = \u2016 ln(z)\u2212 ln(z\u0303)\u2016\u221e where ln(z) = ( ln(z1), .", "startOffset": 140, "endOffset": 144}, {"referenceID": 13, "context": "2 [14]), we prove:", "startOffset": 2, "endOffset": 6}, {"referenceID": 13, "context": "2 in [14] that (R++, d) is a complete metric space and thus there exists z\u2217 \u2208 R++ such that z converge to z\u2217 w.", "startOffset": 5, "endOffset": 9}, {"referenceID": 13, "context": "6 in [14] implies that the topology of (R++, d) coincide with the norm topology implying that limk\u2192\u221e z = z\u2217 w.", "startOffset": 5, "endOffset": 9}, {"referenceID": 9, "context": "31 in [10], we know that if A has a positive eigenvector \u03b3 \u2208 RK+1 ++ , then max i\u2208[K+1] (A \u03b3)i \u03b3i = \u03c1(A) = min \u03b3\u0303\u2208RK+1 ++ max i\u2208[K+1] (A \u03b3\u0303)i \u03b3\u0303i .", "startOffset": 6, "endOffset": 10}, {"referenceID": 2, "context": "30 [3].", "startOffset": 3, "endOffset": 6}, {"referenceID": 9, "context": "4 in [10]).", "startOffset": 5, "endOffset": 9}, {"referenceID": 10, "context": "However, the other papers which have up to now discussed global optimality for neural networks [11, 8] have not included any results on real datasets.", "startOffset": 95, "endOffset": 102}, {"referenceID": 7, "context": "However, the other papers which have up to now discussed global optimality for neural networks [11, 8] have not included any results on real datasets.", "startOffset": 95, "endOffset": 102}, {"referenceID": 1, "context": "We choose the parameters of our model out of 100 randomly generated combinations of (n1, \u03b1, \u03c1w, \u03c1u) \u2208 [2, 20]\u00d7 [1, 4]\u00d7 (0, 1]2 18", "startOffset": 102, "endOffset": 109}, {"referenceID": 0, "context": "We choose the parameters of our model out of 100 randomly generated combinations of (n1, \u03b1, \u03c1w, \u03c1u) \u2208 [2, 20]\u00d7 [1, 4]\u00d7 (0, 1]2 18", "startOffset": 111, "endOffset": 117}, {"referenceID": 3, "context": "We choose the parameters of our model out of 100 randomly generated combinations of (n1, \u03b1, \u03c1w, \u03c1u) \u2208 [2, 20]\u00d7 [1, 4]\u00d7 (0, 1]2 18", "startOffset": 111, "endOffset": 117}, {"referenceID": 1, "context": "(respectively (n1, n2, \u03b1, \u03b2, \u03c1w, \u03c1v, \u03c1u) \u2208 [2, 10]2 \u00d7 [1, 4]2 \u00d7 (0, 1]2) and pick the best one based on 5-fold cross-validation error.", "startOffset": 43, "endOffset": 50}, {"referenceID": 9, "context": "(respectively (n1, n2, \u03b1, \u03b2, \u03c1w, \u03c1v, \u03c1u) \u2208 [2, 10]2 \u00d7 [1, 4]2 \u00d7 (0, 1]2) and pick the best one based on 5-fold cross-validation error.", "startOffset": 43, "endOffset": 50}, {"referenceID": 0, "context": "(respectively (n1, n2, \u03b1, \u03b2, \u03c1w, \u03c1v, \u03c1u) \u2208 [2, 10]2 \u00d7 [1, 4]2 \u00d7 (0, 1]2) and pick the best one based on 5-fold cross-validation error.", "startOffset": 54, "endOffset": 60}, {"referenceID": 3, "context": "(respectively (n1, n2, \u03b1, \u03b2, \u03c1w, \u03c1v, \u03c1u) \u2208 [2, 10]2 \u00d7 [1, 4]2 \u00d7 (0, 1]2) and pick the best one based on 5-fold cross-validation error.", "startOffset": 54, "endOffset": 60}, {"referenceID": 0, "context": "We noted in our experiments that as \u03b1 is large and our data lies between [0, 1], all units in the network tend to have small values that make the whole objective function relatively small.", "startOffset": 73, "endOffset": 79}], "year": 2016, "abstractText": "The optimization problem behind neural networks is highly non-convex. Training with stochastic gradient descent and variants requires careful parameter tuning and provides no guarantee to achieve the global optimum. In contrast we show under quite weak assumptions on the data that a particular class of feedforward neural networks can be trained globally optimal with a linear convergence rate with our nonlinear spectral method. Up to our knowledge this is the first practically feasible method which achieves such a guarantee. While the method can in principle be applied to deep networks, we restrict ourselves for simplicity in this paper to one and two hidden layer networks. Our experiments confirm that these models are rich enough to achieve good performance on a series of real-world datasets.", "creator": "LaTeX with hyperref package"}}}