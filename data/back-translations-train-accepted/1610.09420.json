{"id": "1610.09420", "review": {"conference": "NIPS", "VERSION": "v1", "DATE_OF_SUBMISSION": "28-Oct-2016", "title": "Dynamic matrix recovery from incomplete observations under an exact low-rank constraint", "abstract": "Low-rank matrix factorizations arise in a wide variety of applications -- including recommendation systems, topic models, and source separation, to name just a few. In these and many other applications, it has been widely noted that by incorporating temporal information and allowing for the possibility of time-varying models, significant improvements are possible in practice. However, despite the reported superior empirical performance of these dynamic models over their static counterparts, there is limited theoretical justification for introducing these more complex models. In this paper we aim to address this gap by studying the problem of recovering a dynamically evolving low-rank matrix from incomplete observations. First, we propose the locally weighted matrix smoothing (LOWEMS) framework as one possible approach to dynamic matrix recovery. We then establish error bounds for LOWEMS in both the {\\em matrix sensing} and {\\em matrix completion} observation models. Our results quantify the potential benefits of exploiting dynamic constraints both in terms of recovery accuracy and sample complexity. To illustrate these benefits we provide both synthetic and real-world experimental results.", "histories": [["v1", "Fri, 28 Oct 2016 22:44:29 GMT  (144kb)", "http://arxiv.org/abs/1610.09420v1", null]], "reviews": [], "SUBJECTS": "stat.ML cs.LG", "authors": ["liangbei xu", "mark a davenport"], "accepted": true, "id": "1610.09420"}, "pdf": {"name": "1610.09420.pdf", "metadata": {"source": "CRF", "title": null, "authors": [], "emails": ["lxu66@gatech.edu", "mdav@gatech.edu"], "sections": [{"heading": null, "text": "ar Xiv: 161 0.09 420v 1 [stat.ML] 2 8"}, {"heading": "1 Introduction", "text": "In fact, the fact is that most of us are able to go in search of a solution that is capable, that they are able to find a solution that is capable of finding a solution, that they are able to find a solution, that they are able, that they are able to find themselves in a position, that they are able to find a solution, that they are able to find a solution, that they are able to find a solution, that they are able to find a solution, that they are able to find a solution, that they are able to find a solution, that they are able to find a solution, that they are able to find a solution, that they are able to find a solution, that they are able to find a solution."}, {"heading": "2 Problem formulation", "text": "The underlying assumption in this work is that our low-rank matrix will change over time, but we assume that we will model this by the following discrete dynamic process: currently t we have a low-rank matrix Xt \u00b7 Rn1 \u00b7 n2 with rank r, which we assume is related to the matrix in previous time steps viaXt = f (Xt) + f (Xt), yt, zt Rmt, (1) where zt represents the measuring noise. We assume that we will observe each Xt by a linear operator. To: Rn1 \u00b7 n2 \u2192 Rmt, yt = yt, yt, zt Rmt, (1) where zt is the measuring noise. In our problem we will assume that we observe up to d time steps, and our goal is to take {Xt} dt = 1 together from {yt} dt = U = 1. The above model is sufficiently flexible to allow us to integrate multiple dynamics."}, {"heading": "3 Recovery error bounds", "text": "T T T T T T T T T T T T T T T T T T T T T T T T T T T T T T T T T T T T T T T T T T T T T T T T T T T T T T T T T T T T T T T T T T T T T T T T T T T T T T T T T T T T T T T T T T T T T T T T T T T T T T T T T T T T T T T T T T T T T T T T T T T T T T T T T T T T T T T T T T T T T T T T T T T T T T T T T T T T T T T T T T T T T T T T T T T T T T T T T T T T T T T T T T T T T T T T T T T T T T T T T T T T T T T T T T T T T T T T T T T T T T T T T T T T T T T T T T T T T T T T T T T T T T T T T T T T T T T T T T T T T T T T T T T T T T T T T T T T T T T T T T T T T T T T T T T T T T T T T T T T T T T T T T T T T T T T T T T T T T T T T T T T T T T T T T T T T T T T T T T T T T T T T T T T T T T T T T T T T T T T T T T T T T T T T T T T T T T T T T T T T T T T T T T T T T T T T T T T T T T T T T T T T T T T T T T T T T T T T T T T T T T T T T T T T T T T T T T T T T T T T T T T T T T T T T T T T T T T T T T T T T T T T T T T T T T T T T T T T T T T T T T T T T T T T T T T T T T T T T T T T T T T T T T T T T"}, {"heading": "3.1 Matrix sensing setting", "text": "We will consider the case in which all operators are involved in the Gaussian measurement, which is defined as follows. (4) We will consider the case in which all operators are involved in the Gaussian measurement (4). (4) We will participate in the Gaussian measurement if we can express any input from A (X) as [A). (4) We are independent of any other. (4) We will also define the Matrix Restricted Isometry Property (RIP) for a linear map A. Definition r = 1,.)., nmin, the isometric constant of A is the smallest quantity, the (1 \u2212 2)."}, {"heading": "3.2 Matrix completion setting", "text": "(We.). (We.). (We.). (We.). (We.). (We.). (We.). (We.). (We.). (We.). (We.). (We.). (We.). (We.). (We.). (We.). (We.). (We.). (We.). (We.). (We.). (We.). (We.). (We.). (We.). (We.). (We.). (We.). (We. (We.). (We.). (We.). (We.). (We.). (We.). (We.). (We.). (We.). (We.). (We.). (We.). (We.). (We.). (We.). (We.). (We. (We.). (We.). (We.). (We. (We.). (We.). (We.). (We.). (We. (We.). (We.). (We.). (We.). (We. (We.). (We.). (We.). (We. (We.). (We.). (We. (We.). (We.). (We. (We.). (We.). (We. (.). (We.). (We. (We. (We.). (We.). (We. (.). (We. (.). (We. (.). (We. (.). (We.). (We. (.). (We.). (. (We. (. (We.). (We.). (We.). (We.). (We. (.). (. (We. (. (.). (We.). (We. (We. (.). (We. (.). (We. (We.).). (We.). (. (We. (We. (We."}, {"heading": "4 An algorithm based on alternating minimization", "text": "As noted in section 2, each rank R matrix can be factored in as X = UV T, where U is n1 \u00b7 r and V is n2 \u00b7 r, so the LOWEMS estimator in (3) can be factored in as X-D = arg min X-C (r) L (X) = arg min X = UV Td \u00b2 t = 11 2 wt \u00b2 At (UV T) \u2212 yt \u00b2 2. (13) The above program can be solved by alternating minimization (see [17]), which alternatively minimizes the objective function via U (or V) while holding V (or U) until a stop criterion is reached. Since the objective function is square, each step in this method is reduced to conventionally weighted smallest squares that can be solved by efficient numerical methods. Theoretical guarantees for global convergence of the alternating minimization of the static matrix sensitization / completion process should be considered as this possible method recently established in this [method]."}, {"heading": "5 Simulations and experiments", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "5.1 Synthetic simulations", "text": "Our synthetic simulations look at both matrix sensors and matrix completion, but with an emphasis on matrix completion. We set n1 = 100, n2 = 50, d = 4 and r = 5. We look at two baselines: baseline one uses only yd to restore Xd and simply ignores y1,.. yd \u2212 1; baseline two uses {yt} dt = 1 with equal weights. Note that both of these examples can be considered special cases of LOWEMS with weights (0,., 0) and (1d, 1 d.,.) each taking into account the formula for optimal selection of weights in (8), it is easy to show that baseline one corresponds to the case in which we do it."}, {"heading": "5.2 Real world experiments", "text": "Next, we test the LOWEMS approach in the context of a recommendation system using the (truncated) Netflix dataset. We eliminate these movies with few ratings, and these users rate few movies, and generate a truncated dataset with 3199 users, 1042 movies, 2462840 ratings, which is a fraction of the visible entries in the rating matrix. All ratings are distributed over a period of 2191 days. To ensure robustness, we also impose a Frobenius standard penalty on the factor matrices U and V in (13). We keep the latest (temporal) 10% of the ratings as a test set. The remaining ratings are divided into a validation set and a training set for cross-validation. We divide the remaining ratings into d-values {1, 3, 6, 8} bins each with the same time interval."}, {"heading": "6 Conclusion", "text": "In this paper, we look at the low-level matrix recovery problem in a novel environment where one of the factor matrices changes over time. We propose the framework for locally weighted matrix smoothing (LOWEMS) and have established error limits for LOWEMS in both matrix capture and matrix completion. Our analysis quantifies how the proposed estimator improves recovery accuracy and reduces sample complexity compared to static recovery methods. Finally, we provide both synthetic and real experimental results to verify our analysis and demonstrate superior empirical performance in leveraging dynamic constraints in a recommendation system."}, {"heading": "A Proof of Proposition 3.1", "text": "Proof. Let us x: = vec (X) - Rn1n2 and L (x) - (X) - (X) - (X) - (X) - (S) - (S) - (S) - (S) - (S) - (S) - (S) - (S) - (S) - (S) - (S) - (S) - (S) - (S) - (S) - (S) - (S) - (S) - (S) - (S) - (S) - (S) - (S) - (S) - (S) - (S) - (S) - (S) - (S) - (S) - (S) - (S) - (S) - (S) - (S) - (S) (S) - (S) (S) (S)."}, {"heading": "B Proof of Theorem 3.4", "text": "The proof consists of the lower limit of the LHS of (4) and the upper limit of the RHS of (4). Suppose the linear operator At: Rn1 \u00b7 n2 \u2192 Rm0 is a random Gaussian total ensemble for all 1 \u2264 t \u2264 d. If m0 > Dnmaxr \u0445 d = 1 w 2 t, the compound operator {\"wtAt\" d = 1 satisfies the rank-2r matrix RIP with constant probability of more than 1 \u2212 C exp (\u2212 cm0), where D, C and c (depending on the ratios) are absolutely positive constants. Proof. See Appendix C. Next problem gives us an upper limit for the stochastic error of 1 \u2212 C exp (\u2212 cm0), where D, C, and c (depending on the ratios) are absolutely positive constants."}, {"heading": "C Proof of Lemma B.1", "text": "Theorem C.1. For independent Xi-subexponentials with parameters (\u03c3i, bi), with an average of 0.2 i and b = maxi bi. We now have lower limits for d = 1 wt = 1 wt = 1 wt = 1 wt = 1 wt = 2 wt = 2 wt = 2 maxi \u00b2 2. Since all At-s are Gaussian random measurements, then a certain measurement is Ati, d \u00b2 2 than m \u2212 10 x b = maxi bi. We now have lower limits for d = 1 wt = 1 wt = 1 maxi \u00b2 2. Since all At-s are Gaussian random measurements, a certain measurement is Ati, d \u00b2 2 than m \u2212 2 and b = maxi \u00b2 b = maxi \u00b2. We now have lower limits for d = 1 wt = 1 wt = 1 maxi \u00b2."}, {"heading": "D Proof of Lemma B.2", "text": "Proof that W = 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0."}, {"heading": "E Proof of Theorem 3.8", "text": "The proof follows the same framework of proof of theorem 7 in [15].Before we engage in the next level (21), we must engage in the next level (22), in which we enter the next level (22) Definition. (22) Definition to the next level (21). (22) Definition to the next level (21). (22) Definition to the next level (21). (22) Definition to the next level (21). (22) Definition to the next level (21). (22) Definition to the next level (21). (22) Definition to the next level (21). (22) Definition to the second level (21). (22) Definition to the first level (22). Definition to the second level (21). (22) Definition to the second level (21). (21). (21. (21.). (21. (21.) to the third level (21.). (21. (21.)."}, {"heading": "F Proof of Lemma E.1", "text": "The proof is almost the same as the proof for Lemma 12 in [15] with some minor changes. (set F = 44rn1n2m0 (E (E)). (set F = 44rn1n2m0 (E). (set F = 44rn1n2m0 (E). (set F = 44rn1n2m0 (E). (set F = 44rn1m0 (E). (set F = 44rn1m0 (E). (set F = 44n2m0). (set F = 44n2m0 (E). (set F = 44rn1n2m0 (E). (set F = 44n2m0). (set F = 44n2m0). (set F = 44n2m0). (set F = 44n2p. (2f) \u2212 (set 244n2p). (244n2p). (2f = 44n2p. \u2212. (2p). (244n2p). (2f = 44n2n2p). (F = 44n2n2p). (2f = 44n2n2p). (F = 44n2n2n2p). (2f = 44n2n2p). (2f = 44n2n2n2p). (F = 44n2n2n2p = 44n2n2p = 44n2p = 44n2n2n2n2m0 (F = 44n). (F = 44nF = 44n2n2n2n2m0). (F = 44n2n2m0). (F = 44nf = 44n2n2m0 (F = 44n2m0). (F = 44n2n2n2n2n2m0 (F = 44n2m0). (F = 44n2m0). (F = 44n2m0). (F = 44nf = 44n2n2m0). (F = 44n2m0). (F = 44n2m0). (F = 44n2m0). (F = 44n2m0"}, {"heading": "G Proof of Lemma F.1", "text": "The proof is almost the same as the proof for Lemma 14 in [15] with some minor changes. By Massart's concentration disparity (see e.g. [2], Theorem 14.2) we finally have the expectation E (HT) + 19512 T) \u2264 exp (\u2212 c5m0T 2 = 1 w 2 t), (40) where c5 = 1 / 128. Next we have bound the expectation E (HT). Using a symmetry argument we get E (HT) \u2264 2E (sup X-T) \u2264 E (r, T), (sup X-T) \u2264 T = 1 \u2264 t = 1 < Ati, X-T 2 2, where it is a wheel-maker variable (independent of i and t)."}], "references": [], "referenceMentions": [], "year": 2016, "abstractText": "Low-rank matrix factorizations arise in a wide variety of applications \u2013 including recommendation systems, topic models, and source separation, to name just a few. In these and many other applications, it has been widely noted that by incorporating temporal information and allowing for the possibility of time-varying models, significant improvements are possible in practice. However, despite the reported superior empirical performance of these dynamic models over their static counterparts, there is limited theoretical justification for introducing these more complex models. In this paper we aim to address this gap by studying the problem of recovering a dynamically evolving low-rank matrix from incomplete observations. First, we propose the locally weighted matrix smoothing (LOWEMS) framework as one possible approach to dynamic matrix recovery. We then establish error bounds for LOWEMS in both the matrix sensing and matrix completion observation models. Our results quantify the potential benefits of exploiting dynamic constraints both in terms of recovery accuracy and sample complexity. To illustrate these benefits we provide both synthetic and real-world experimental results.", "creator": "LaTeX with hyperref package"}}}