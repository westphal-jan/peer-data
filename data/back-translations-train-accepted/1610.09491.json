{"id": "1610.09491", "review": {"conference": "NIPS", "VERSION": "v1", "DATE_OF_SUBMISSION": "29-Oct-2016", "title": "SDP Relaxation with Randomized Rounding for Energy Disaggregation", "abstract": "We develop a scalable, computationally efficient method for the task of energy disaggregation for home appliance monitoring. In this problem the goal is to estimate the energy consumption of each appliance over time based on the total energy-consumption signal of a household. The current state of the art is to model the problem as inference in factorial HMMs, and use quadratic programming to find an approximate solution to the resulting quadratic integer program. Here we take a more principled approach, better suited to integer programming problems, and find an approximate optimum by combining convex semidefinite relaxations randomized rounding, as well as a scalable ADMM method that exploits the special structure of the resulting semidefinite program. Simulation results both in synthetic and real-world datasets demonstrate the superiority of our method.", "histories": [["v1", "Sat, 29 Oct 2016 11:48:28 GMT  (313kb)", "http://arxiv.org/abs/1610.09491v1", null]], "reviews": [], "SUBJECTS": "cs.LG", "authors": ["kiarash shaloudegi", "andr\u00e1s gy\u00f6rgy", "csaba szepesv\u00e1ri", "wilsun xu"], "accepted": true, "id": "1610.09491"}, "pdf": {"name": "1610.09491.pdf", "metadata": {"source": "CRF", "title": null, "authors": ["Wilsun Xu"], "emails": ["k.shaloudegi16@imperial.ac.uk", "a.gyorgy@imperial.ac.uk", "szepesva@ualberta.ca", "wxu@ualberta.ca"], "sections": [{"heading": null, "text": "ar Xiv: 161 0.09 491v 1 [cs.L G] 2"}, {"heading": "1 Introduction", "text": "This year, it will only take one year for an agreement to be reached."}, {"heading": "1.1 Notation", "text": "Throughout the work, we use the following notation: R stands for the set of real numbers, Sn + for the set of n \u00b7 n positive semidefined matrices, I {E} for the indicator function of an event E (i.e., it is 1 if the event is true and otherwise zero), 1 stands for a vector of suitable dimension whose entries are every 1. For an integer K, [K] stands for the set {1, 2,.., K}. N (\u00b5, \u03a3) stands for the Gaussian distribution with the mean \u00b5 and the covariance matrix \u03a3. For a matrix A, track (A) stands for its track, and diag (A) for the vector formed by the diagonal entries of A."}, {"heading": "2 System Model", "text": "Following Kolter and Jaakkola [2012], the energy consumption of the household is modeled using an additional factorial HMM (Ghahramani and Jordan, 1997). (Suppose there are no other appliances in a household.) Each of them is modeled using an HMM: let Pi-RKi \u00b7 Ki denotes the transition probability matrix of the appliance i [M] and assumes that for each state s [Ki] the energy consumption of the appliance is constant. (\u00b5i denotes the corresponding Ki-dimensional column vector (micro, 1,., \u00b5i, Ki) (0, 1} Ki is the indicator vector of the state st, i the application i of time t (i.e., xt, i, s = I {st}), which is total current consumption."}, {"heading": "3 SDP Relaxation and Randomized Rounding", "text": "There are two major challenges to solve the optimization problem (2) precisely: (i) optimization is done via binary vectors xt, i; and (ii) the objective function f, even considering its extension to a convex domain, is generally not convex (due to the second term). As a remedy, we will relax (2) to make it an integer square programming problem, then apply SDP relaxation and randomized rounding to roughly solve the relaxed problem."}, {"heading": "3.1 Approximate Solutions for Integer Quadratic Programming", "text": "In this section, we consider approximate solutions to integer programming (X). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D)."}, {"heading": "4 An Efficient Algorithm for Inference in FHMMs", "text": "To do this, we must solve the problem in a convex way by using the new variables Zt, i = xt, ix + 1, i + 1, i + 1, i with two new ones: Zt, i1 = xt, i and Z't, to simplify the representation, we will assume that Ki = K for all i, i = K, i + 1, i with two new ones: Zt, i1 = xt, i and Z 't. Then problem (2) becomesargmin xt, iT = 1 {122, yt \u00b2."}, {"heading": "5 ADMM Solver for Large-Scale, Sparse Block-Structured SDP Problems", "text": "Considering the relaxation and randomized rounding shown in the previous subsection, all that remains is to find X * t, z * t to initialize algorithm 1. Although internal dot methods can efficiently solve SDP problems, even for problems with sparse constraints such as (4), the runtime to obtain an optimal solution is in the order of n3.5 log (1 / 3) [Nesterov, 2004, Section 4.3.3], which becomes prohibitive in our case because the number of variables with the time horizon T is scaled more linear. As an alternative solution, first-order methods can be used for major problems [Wen et al., 2010]. Since our problem (8) is a problem where the objective function is divisible, ADMM is a promising candidate to find a near-optimal solution. To apply ADMM, we use the MoreYosida square regulation of Xvid et al, 2009, which is well suited for the primary formulation."}, {"heading": "6 Learning the Model", "text": "In the previous section, an algorithm was provided to solve the inference part of our energy breakdown problem. However, in order to be able to perform the inference method, we need to configure the model. To learn the HMMs describing each device, we use the method of Kontorovich et al. [2013] to learn the transition matrix and spectral learning method of Anandkumar et al. [2012] (according to Mattfeld, 2014) to determine the emission parameters. However, when it comes to the specific application of NILM, the problem of unknown, time-varying bias due to the presence of unknown / unmodelled devices in the measured signal must also be addressed. A simple idea that Kolter and Jaakkola [2012] also follow is the use of a \"generic model\" whose contribution to the objective function is downgraded."}, {"heading": "7 Experimental Results", "text": "We evaluate the performance of our algorithm in two configurations: 5 we use a synthetic dataset to test the inference method in a controlled environment, while we use the Kolter and Johnson REDD dataset [2011] to see how the method performs with non-simulated \"real\" data; the performance of our algorithm is compared with the Structured Variation Inference (SVI) method of Ghahramani and Jordan [1997], the Kolter and Jaakkola method [2012] and the method of Zhong et al. [2014]; we will refer to the last two algorithms as KJ and ZGS, respectively."}, {"heading": "7.1 Experimental Results: Synthetic Data", "text": "The synthetic data set was generated randomly (the exact procedure is described in Appendix C.) To evaluate the performance, we use normalized disaggregation errors, such as those proposed by Kolter and Jaakkola [2012] and also applied by Zhong et al. [2014], which measures the reconstruction error for each individual device. Considering the actual output yt, i and the estimated output y-t, i (i.e. y-t, i = \u00b5 i x-t, i), the error size is defined as NDE = \u221a t, i (yt, i \u2212 y-t, i) 2 / \u2211 t, i (yt, i) 2. Figures 1 and 2 show the performance of the algorithms varied as number of HMMs (M) (i.e. number of states, K). Each plot is a report for T = 1000 steps averaged over 100 random models and realizations, showing the mean and standard deviation from NDE at the end of the MDR, each of which is better than the MDR at the other end of the MDR method."}, {"heading": "7.2 Experimental Results: Real Data", "text": "In this section, we compared the 3 best methods on REDD [Kolter and Johnson, 2011]. We use the first half of the data for training and the second half for testing. Each HMM (i.e., 5Our code is available online at http: / / github.com / kiarashshaloudegi / FHMR) is executed separately with the associated circuits, and the HMM corresponds to unregistered devices. In this set of experiments, we monitor the devices that consume more than 100 watts. ADMM-RR is executed for 1000 iterations, and the local search is performed at the end of each 250 iterations, and the result with the greatest probability is chosen."}, {"heading": "8 Conclusion", "text": "FHMMs are commonly used in energy breakdown, but the resulting model has a huge (factorized) state space, making standard FHMM inference algorithms unfeasible even for a handful of devices. In this work, we developed a scalable approximate inference algorithm based on semi-definitive relaxation in combination with randomized rounding that significantly exceeded the state of the art in our experiments. A critical component of our solution is a scalable ADMM method that utilizes the special block diagonal structure of SDP relaxation and provides good initialization for randomized rounding. We expect that our method could prove useful in solving other FHMM inference problems as well as large-scale integer square programming."}, {"heading": "Acknowledgements", "text": "This work was supported in part by the Alberta Innovates Technology Futures through the Alberta Ingenuity Centre for Machine Learning and NSERC. K. is indebted to Pooria Joulani and Mohammad Ajallooeian, who gave many useful technical advice, while all authors are grateful to Zico Kolter for passing on his code."}, {"heading": "A ADMM updates", "text": "In this section we derive the ADMM updates for the regularized L\u00b5 given by (10). If we take derivatives with respect to X and z and set them to zeros, we get \"XL\u00b5\" = D + 1\u00b5 (X \u2212 S) \u2212 A \u03bb \u2212 B \u03bd \u2212 E S + \u00b5 (A \u03bb + B \u03bd \u2212 + W \u2212 D) and zL\u00b5 = d + 1\u00b5 (z \u2212 r) \u2212 C P + P + 1\u00b5 (z \u2212 S \u2212 r) \u2212 Z \u2212 P is then only the standard iteration that K + 1 = argmin P 0 L p p p p p p p p p p p p p p p p p p p p p p, z (S"}, {"heading": "B Discussion of the Derivation in Kolter and Jaakkola [2012] in the Presence of the \u201cGeneric Model\u201d", "text": "The \"generic model\" influences the derivation of the Kolter and Jaakkola algorithm [2012] as follows: The authors of this paper claim to derive the final optimization problem of (9) and (10) given in Equation (15) of their paper as follows: Equation (9) defines the problem minz-Z, Q-A-f1 (z, Q), while (10) defines the problem minz-Z, Q-A-f2 (z, Q), where z-Z, z-Z (z) are variables describing the state of the \"generic model\" over time. The assertion in the paper is that a group B (resulting from its \"one-to-time\" constraint) corresponds minz-Z, z-Z-Z, Q-A-B, z-f1 (z, A) + f2 (Q) to the minimization problem in the equation in general."}, {"heading": "C Generating the Synthetic Dataset", "text": "The synthetic dataset used in the experiments was generated as follows: the power levels corresponding to each on-state (\u00b5) were randomly generated from [100, 4500], with the additional restriction that the difference between two unequal levels must be greater than 100 (to promote identifiability); the levels for off-states were set to 0; the transition matrices for each device were generated in the following way: diagonal elements for off-states were drawn randomly from [0, 35] and for on-states from [0, 30], while non-diagonal elements were selected from [0, 1] to ensure sparse transitions; and finally, the data matrices were normalized to ensure that they were correct transition matrices; the output of each device was subjected to additional Gaussian noise proportional to the energy consumption level of the given on-state [0, 6] and 1 for off-states."}, {"heading": "D Additional Results for the Real-Data Experiment", "text": "In Table 1, we have provided prediction and retrieval values for our experiments with real data. As promised, here are some additional results to these experiments: Table 2 shows the total power consumption assigned to different devices, and Figure 3 shows the amount of power assigned to each device."}], "references": [{"title": "Non-Intrusive Signature Extraction for Major Residential Loads", "author": ["M. Dong", "Meira", "W. Xu", "C.Y. Chung"], "venue": "Smart Meters. IEEE Transactions on Smart Grid,", "citeRegEx": "Dong et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Dong et al\\.", "year": 2012}, {"title": "PALDi: Online Load Disaggregation via Particle Filtering", "author": ["D. Egarter", "V.P. Bhuvana", "W. Elmenreich"], "venue": "IEEE Transactions on Smart Grid,", "citeRegEx": "Egarter et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Egarter et al\\.", "year": 2013}, {"title": "Factorial Hidden Markov Models", "author": ["Z. Ghahramani", "M. Jordan"], "venue": "Monitoring (NILM) Systems. Neurocomputing,", "citeRegEx": "Ghahramani and Jordan.,? \\Q2012\\E", "shortCiteRegEx": "Ghahramani and Jordan.", "year": 2012}, {"title": "Implementing spectral methods for hidden Markov models with real-valued emissions", "author": ["C. Mattfeld"], "venue": "Journal on Optimization,", "citeRegEx": "Mattfeld.,? \\Q2009\\E", "shortCiteRegEx": "Mattfeld.", "year": 2009}, {"title": "Leveraging smart meter data to recognize home appliances", "author": ["M. Weiss", "A. Helfenstein", "F. Mattern", "T. Staake"], "venue": null, "citeRegEx": "Weiss et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Weiss et al\\.", "year": 2007}, {"title": "Note that the projections are done on matrices of small size. Note also that the pseudo-inverses of the matrices involved need only be calculated once", "author": [], "venue": "B Discussion of the Derivation in Kolter and Jaakkola", "citeRegEx": "..,? \\Q2012\\E", "shortCiteRegEx": "..", "year": 2012}], "referenceMentions": [{"referenceID": 0, "context": ", 2010], or ad-hoc heuristic methods [Dong et al., 2012] have been employed.", "startOffset": 37, "endOffset": 56}, {"referenceID": 2, "context": "FHMMs, introduced by Ghahramani and Jordan [1997], are powerful tools for modeling times series generated from multiple independent sources, and are great for modeling speech with multiple people simultaneously talking [Rennie et al.", "startOffset": 21, "endOffset": 50}, {"referenceID": 2, "context": "FHMMs, introduced by Ghahramani and Jordan [1997], are powerful tools for modeling times series generated from multiple independent sources, and are great for modeling speech with multiple people simultaneously talking [Rennie et al., 2009], or energy monitoring which we consider here [Kim et al., 2011]. Doing exact inference in FHMMs is NP hard; therefore, computationally efficient approximate methods have been the subject of study. Classic approaches include sampling methods, such as MCMC or particle filtering [Koller and Friedman, 2009] and variational Bayes methods [Wainwright and Jordan, 2007, Ghahramani and Jordan, 1997]. In practice, both methods are nontrivial to make work and we are not aware of any works that would have demonstrated good results in our application domain with the type of FHMMs we need to work and at practical scales. In this paper we follow the work of Kolter and Jaakkola [2012] to model the NILM problem by FHMMs.", "startOffset": 21, "endOffset": 919}, {"referenceID": 2, "context": "FHMMs, introduced by Ghahramani and Jordan [1997], are powerful tools for modeling times series generated from multiple independent sources, and are great for modeling speech with multiple people simultaneously talking [Rennie et al., 2009], or energy monitoring which we consider here [Kim et al., 2011]. Doing exact inference in FHMMs is NP hard; therefore, computationally efficient approximate methods have been the subject of study. Classic approaches include sampling methods, such as MCMC or particle filtering [Koller and Friedman, 2009] and variational Bayes methods [Wainwright and Jordan, 2007, Ghahramani and Jordan, 1997]. In practice, both methods are nontrivial to make work and we are not aware of any works that would have demonstrated good results in our application domain with the type of FHMMs we need to work and at practical scales. In this paper we follow the work of Kolter and Jaakkola [2012] to model the NILM problem by FHMMs. The distinguishing features of FHMMs in this setting are that (i) the output is the sum of the output of the underlying HMMs (perhaps with some noise), and (ii) the number of transitions are small in comparison to the signal length. FHMMs with the first property are called additive. In this paper we derive an efficient, convex relaxation based method for FHMMs of the above type, which significantly outperforms the state-of-the-art algorithms. Our approach is based on revisiting relaxations to the integer programming formulation of Kolter and Jaakkola [2012]. In particular, we replace the quadratic programming relaxation of Kolter and Jaakkola, 2012 with a relaxation to an semi-definite program (SDP), which, based on the literature of relaxations is expected to be tighter and thus better.", "startOffset": 21, "endOffset": 1519}, {"referenceID": 2, "context": "FHMMs, introduced by Ghahramani and Jordan [1997], are powerful tools for modeling times series generated from multiple independent sources, and are great for modeling speech with multiple people simultaneously talking [Rennie et al., 2009], or energy monitoring which we consider here [Kim et al., 2011]. Doing exact inference in FHMMs is NP hard; therefore, computationally efficient approximate methods have been the subject of study. Classic approaches include sampling methods, such as MCMC or particle filtering [Koller and Friedman, 2009] and variational Bayes methods [Wainwright and Jordan, 2007, Ghahramani and Jordan, 1997]. In practice, both methods are nontrivial to make work and we are not aware of any works that would have demonstrated good results in our application domain with the type of FHMMs we need to work and at practical scales. In this paper we follow the work of Kolter and Jaakkola [2012] to model the NILM problem by FHMMs. The distinguishing features of FHMMs in this setting are that (i) the output is the sum of the output of the underlying HMMs (perhaps with some noise), and (ii) the number of transitions are small in comparison to the signal length. FHMMs with the first property are called additive. In this paper we derive an efficient, convex relaxation based method for FHMMs of the above type, which significantly outperforms the state-of-the-art algorithms. Our approach is based on revisiting relaxations to the integer programming formulation of Kolter and Jaakkola [2012]. In particular, we replace the quadratic programming relaxation of Kolter and Jaakkola, 2012 with a relaxation to an semi-definite program (SDP), which, based on the literature of relaxations is expected to be tighter and thus better. While SDPs are convex and could in theory be solved using interior-point (IP) methods in polynomial time [Malick et al., 2009], IP scales poorly with the size of the problem and is thus unsuitable to our large scale problem which may involve as many a million variables. To address this problem, capitalizing on the structure of our relaxation coming from our FHMM model, we develop a novel variant of ADMM [Boyd et al., 2011] that uses Moreau-Yosida regularization and combine it with a version of randomized rounding that is inspired by the the recent work of Park and Boyd [2015]. Experiments on synthetic and real data confirm that our method significantly outperforms other algorithms from the literature, and we expect that it may find its applications in other FHMM inference problems, too.", "startOffset": 21, "endOffset": 2337}, {"referenceID": 0, "context": ", by Dong et al., 2012, 2013, Figueiredo et al., 2012). This observation was used by Kolter and Jaakkola [2012] to amend the posterior with a term that tries to match the large signal changes to the possible changes in the power level when only the state of a single appliance changes.", "startOffset": 5, "endOffset": 112}, {"referenceID": 3, "context": "[2012] (following Mattfeld, 2014) to determine the emission parameters. However, when it comes to the specific application of NILM, the problem of unknown, time-varying bias also needs to be addressed, which appears due to the presence of unknown/unmodeled appliances in the measured signal. A simple idea, which is also followed by Kolter and Jaakkola [2012], is to use a \u201cgeneric model\u201d whose contribution to the objective function is downweighted.", "startOffset": 18, "endOffset": 360}, {"referenceID": 3, "context": "[2012] (following Mattfeld, 2014) to determine the emission parameters. However, when it comes to the specific application of NILM, the problem of unknown, time-varying bias also needs to be addressed, which appears due to the presence of unknown/unmodeled appliances in the measured signal. A simple idea, which is also followed by Kolter and Jaakkola [2012], is to use a \u201cgeneric model\u201d whose contribution to the objective function is downweighted. Surprisingly, incorporating this idea in the FHMM inference creates some unexpected challenges.4 Therefore, in this work we come up with a practical, heuristic solution tailored to NILM. First we identify all electric events defined by a large change \u2206yt in the power usage (using some adhoc threshold). Then we discard all events that are similar to any possible level change \u2206\u03bc m,k. The remaining large jumps are regarded as coming from a generic HMM model describing the unregistered appliances: they are clustered into K \u2212 1 clusters, and an HMM model is built where each cluster is regarded as power usage coming from a single state of the unregistered appliances. We also allow an \u201coff state\u201d with power usage 0. We drop the subscript t and replace t+ 1 and t\u2212 1 with + and \u2212 signs, respectively. For example, the incorporation of this generic model breaks the derivation of the algorithm of Kolter and Jaakkola [2012]. See Appendix B for a discussion of this.", "startOffset": 18, "endOffset": 1376}, {"referenceID": 2, "context": "The performance of our algorithm is compared to the structured variational inference (SVI) method of Ghahramani and Jordan [1997], the method of Kolter and Jaakkola [2012] and that of Zhong et al.", "startOffset": 101, "endOffset": 130}, {"referenceID": 2, "context": "The performance of our algorithm is compared to the structured variational inference (SVI) method of Ghahramani and Jordan [1997], the method of Kolter and Jaakkola [2012] and that of Zhong et al.", "startOffset": 101, "endOffset": 172}, {"referenceID": 2, "context": "The performance of our algorithm is compared to the structured variational inference (SVI) method of Ghahramani and Jordan [1997], the method of Kolter and Jaakkola [2012] and that of Zhong et al. [2014]; we shall refer to the last two algorithms as KJ and ZGS, respectively.", "startOffset": 101, "endOffset": 204}], "year": 2016, "abstractText": "We develop a scalable, computationally efficient method for the task of energy disaggregation for home appliance monitoring. In this problem the goal is to estimate the energy consumption of each appliance over time based on the total energy-consumption signal of a household. The current state of the art is to model the problem as inference in factorial HMMs, and use quadratic programming to find an approximate solution to the resulting quadratic integer program. Here we take a more principled approach, better suited to integer programming problems, and find an approximate optimum by combining convex semidefinite relaxations randomized rounding, as well as a scalable ADMM method that exploits the special structure of the resulting semidefinite program. Simulation results both in synthetic and real-world datasets demonstrate the superiority of our method.", "creator": "LaTeX with hyperref package"}}}