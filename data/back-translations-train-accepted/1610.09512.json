{"id": "1610.09512", "review": {"conference": "ICML", "VERSION": "v1", "DATE_OF_SUBMISSION": "29-Oct-2016", "title": "Contextual Decision Processes with low Bellman rank are PAC-Learnable", "abstract": "This paper studies systematic exploration for reinforcement learning with rich observations and function approximation. We introduce a new formulation, called contextual decision processes, that unifies and generalizes most prior settings. Our first contribution is a new complexity measure, the Bellman Rank, that we show enables tractable learning of near-optimal behavior in these processes and is naturally small for many well-studied reinforcement learning settings. Our second contribution is a new reinforcement learning algorithm that engages in systematic exploration to learn contextual decision processes with low Bellman Rank. The algorithm provably learns near-optimal behavior with a number of samples that is polynomial in all relevant parameters but independent of the number of unique observations. The algorithm uses Bellman error minimization with optimistic exploration and provides new insights into efficient exploration for reinforcement learning with function approximation.", "histories": [["v1", "Sat, 29 Oct 2016 14:01:58 GMT  (849kb,D)", "http://arxiv.org/abs/1610.09512v1", "43 pages, 1 figure"], ["v2", "Thu, 1 Dec 2016 19:21:45 GMT  (846kb,D)", "http://arxiv.org/abs/1610.09512v2", "42 pages, 1 figure"]], "COMMENTS": "43 pages, 1 figure", "reviews": [], "SUBJECTS": "cs.LG stat.ML", "authors": ["nan jiang", "akshay krishnamurthy", "alekh agarwal", "john langford", "robert e schapire"], "accepted": true, "id": "1610.09512"}, "pdf": {"name": "1610.09512.pdf", "metadata": {"source": "CRF", "title": "Contextual Decision Processes with Low Bellman Rank are PAC-Learnable", "authors": ["Nan Jiang", "Akshay Krishnamurthy", "Alekh Agarwal", "John Langford", "Robert E. Schapire"], "emails": ["nanjiang@umich.edu", "akshay@cs.umass.edu", "alekha@microsoft.com", "jcl@microsoft.com", "schapire@microsoft.com"], "sections": [{"heading": "1 Introduction", "text": "This year, it has reached the stage where it will be able to take the lead."}, {"heading": "2 Contextual Decision Processes (CDPs)", "text": "We introduce a new model, the so-called Contextual Decision Process, as a unified framework for enhanced learning with rich observations. First, we present the model before introducing the relevant notations and definitions."}, {"heading": "2.1 Model and Examples", "text": "Traditionally, Markov Decision Processes (MDPs) are the standard model of the environment in RL, but Markov's assumption is often unrealistic in the real world. While models such as Partially Observable MDPs (POMDPs) allow for non-Markovian environments, the emphasis is typically on the finite and small observation spaces or on the latent structures that allow partial observability to be resolved, making the techniques inappropriate in several practical applications. Contextual Decision Processes use minimal assumptions to capture a very general class of RL problems. Definition 1 (Contextual Decision Processes (CDP)). A (finite-horizon) Contextual Decision Process (CDP for short) is called < X, H, P > tuple, where X is the context space, A is the horizon of the problem, and H is the horizon."}, {"heading": "2.2 Value-based RL and Function Approximation", "text": "Note that a CDP does not make an assumption about the cardinality of the context space with the highest long-term reward of this CDP extension, and we will generally consider continuous context theorems Q =. This makes it critical to generalize across contexts, since the agent may not encounter the same context twice. With this motivation, we consider the value-based RL with function approximation. That is, the agent is given a space of functions F X \u00b7 A \u2192 [0, 1] and uses it to approximate an action value function (or Q value function). Without loss of generality, we assume that f (xH + 1, a).2 For the purpose of the presentation, we assume that F is a finite space with | F < N < for most parts of the paper, we relax this assumption and allow unlimited function classes with limited performance. Optimal value works in MDPs, we will typically fulfill the value-induced by the Bell-state policy, which also specifies such an action."}, {"heading": "3 Bellman Factorization and Bellman Rank", "text": "Development has so far convinced the reader that CDPs are relatively general models for RL. While it is good that all algorithms for CDPs are applied to all subsumed special cases, this generality also presents significant challenges for exemplary efficient learning. In fact, learning in CDPs with large or infinite context spaces is generally difficult, as they subsume MDPs and POMDPs with arbitrarily large state / observation spaces. In the following lower limit, we show that this intuition does not hold: while simple function classes are effectively generalized in superordinate learning areas, where data consists of a fixed distribution, 5 in RL the data consists of trajectories whose distribution depends crucially on the policies of the agent."}, {"heading": "4 Algorithm and Main Results", "text": "In this section, we present our algorithm for learning CDPs that have Bellman factorization with a small Bellman factorization and our main sample complexity warranty. To facilitate the presentation and help convey the main ideas, we assume three simplistic assumptions: 1. We assume that Bellman factorization parameter M is known to the agent.72 We assume that function class F is finite with | F | = N.3. We assume an exact validity (definition 3) and an exact Bellman factorization (definition 5). All three assumptions can be loosened, and we outline these relaxations in Section 5. We are interested in designing an algorithm for PAC learning CDPs. We say that an algorithm learns PAC when F is specified, two parameters, such PAC factorization (0, 1), and the probability of having access to an algorithm that expresses a CDV with at least a \u2212 P-1 policy."}, {"heading": "4.1 Algorithm", "text": "The pseudo-codes for our algorithms, which we call Olive (Optimism Led Iterative Value-Function Elimination), are presented in Algorithm 1. Theorem 1 describes how to eliminate the parameters Nest, Neval, n and \u03c6.At a high level, the algorithm aims to eliminate functions f \u00b2 F that do not meet the validity condition in Definition 3. (6) This is done by Eq \u00b2 and (7) within the loop of the algorithm. Note that since the actions are randomly selected, Eq. (6) produces an unbiased estimate of E (f, ht), the average Bellman error for the function f in politics at present time. (7) eliminates functions that have a high average Bellman error on this distribution, meaning that they do not meet the validity criteria."}, {"heading": "4.2 Sample Complexity", "text": "In fact, it is a way in which people are able to determine for themselves how they have behaved."}, {"heading": "5 Extensions", "text": "We present four important enhancements to our algorithm and analysis."}, {"heading": "5.1 Unknown Bellman Rank", "text": "The first extension deals with the difficulty that M may not be known to the agent (note that algorithm requires 1 M as input parameter). However, a simple procedure described in algorithm 2 can guess the value of M on a doubling plan and handle this situation without consequences for asymptotic sample complexity. 9Algorithm 2 GuessM (F, 0, 0, 0, 0, 0, 0) 1: for i = 1, 2,. do 2: M \u00b2 2i. 3: Call Olive (F, M, 1) with parameters specified in theorem 1.4: Stop the subroutine if t > HM \u2032 log (6H, M) / log (5 / 3) in line 4 (the for-loop). 3: Call Olive (F, M, 1) with parameters specified in theorem 1.4: Stop the subroutine if t > HM \u00b2 log (6H, M) / log (5 for 3) in line 4 (p)."}, {"heading": "5.2 Separation of Policy Class and V-value Class", "text": "In this section, we show that our algorithm allows separate representations of policies and V-value functions. For each f-value pair and each x-value function, we note that the value of f (x, a) is not used by algorithm 1 and its change to arbitrary values does not affect the execution of the algorithm as long as f (x, a) \u2264 f (x, \u03c0f (x))) (so that the value of f does not change). In other words, our algorithm interacts only with f in two forms: 1. f's greedy policy \u03c0f.2. A figure gf: x 7 \u2192 f (x, \u03c0f (x) f (x) f (x))) makes such figures V-value functions to contrast the previous use of Q-value functions.10Hence, the provision of F corresponds to the provision of the following space."}, {"heading": "5.3 Infinite Hypothesis Classes", "text": "The arguments in Section 4 assume that these two dimensions of the VC class are each more numerous."}, {"heading": "5.4 Approximate Validity and Approximate Bellman Rank", "text": "In practice, however, it is difficult to define a functional category that contains strictly valid functions, since the concept of validity depends on the number of RL models that are unknown. A much more realistic situation is that some functions in F are only roughly valid. \u2022 We assume that the average Bellman errors have an exact low factorization (Definition 5). While this is true for a number of RL models (Section 3), it is worth bearing in mind that they are only models of environments that differ from the real environments themselves and are only approximations. Therefore, it is more realistic to assume that an approximate factorization exists when we present Bellman factorization.In this section, the algorithmic idea of Olive is actually robust against both types of approximation errors, and gradient with the extent to which we present the two approximate functions."}, {"heading": "6 Proofs of Main Results", "text": "In this section, we will provide the high-level intuition as well as the most important lemmas that come into play in the examination of Theorem 1. We will also show how the lemmas are put together to prove the theorem. Detailed evidence of the lemmas can be found in Appendix C. At the highest level, the evidence follows an argument common to most sample-efficient RL algorithms. Specifically, we argue that the optimistic policy chosen in Step 5 of Algorithm 1 is either approximately optimal or seeks a context distribution under which it exhibits a major Bellman error, implying that the use of this policy for research leads to learning on a new context distribution. To ensure sample efficiency, we must then determine that this event may not occur too often. This is done by using the Bellman factoring of the process and arguing that the number of times a suboptimal policy is found cannot be greater than (MO / H)."}, {"heading": "6.1 Key Lemmas for Theorem 1", "text": "We begin with the fragmentation of a political loss into the sum of Bellman errors, which we explore with average error. (...) We begin with the fragmentation of a political loss into the sum of Bellman errors. (...) We begin with the fragmentation of a political loss into the sum of Bellman errors. (...) We begin with the fragmentation of the Lilli errors. (...) We begin with the fragmentation of a political loss into the sum of Bellman errors. (...) We begin with the fragmentation of the Lilli errors. (...) We begin with the fragmentation of a political loss. (...) We begin with the fragmentation of a political loss. (...) We begin with the fragmentation of a political loss. (...) We begin with the fragmentation of a political loss. (...) We begin with the fragmentation of a political loss."}, {"heading": "6.2 Proof of Theorem 1", "text": "Suppose we meet the requirements of Lemma 2 (Eq. (17)) and Lemma 3 (Eq. (Eq. (19)); we will show them later by means of concentration imbalances. Applying Lemma 2 in each round t before the algorithm ends, E (ft, \u03c0t, ht) \u2265 2H = 6 \u221a M\u03c6, thanks to the choice of \u03c6. For level h = ht, Eq. (20) is satisfactory. According to Lemma 3, the event ht = h can happen at most in the M protocol (Eq. 2) / protocol 53 times per hour. (Eq. 2) The total number of rounds in the algorithm is thus in the mostHM protocol (Eq. 2) / protocol 53 = HM protocol (6H) / protocol 53. Now we are ready to apply the concentration imbalances to show that these imbalances (Eq. (17) and 19) are held with high probability. We divide the total probability of failure from the following Lemma-V:"}, {"heading": "7 Conclusions and Discussions", "text": "In this paper, we present a new model for RL with rich observations, called Contextual Decision Processes, and a structural property, the Bellman Factorization, of this model, which enables sample-efficient learning. Our unified approach allows us to address several scenarios of practical interest that have largely eluded RL theory to date. By extending our main result, we also show that our techniques are quite robust and gracefully degrade in violation of our assumptions. While the results certainly represent an exciting development for RL theory, we believe that they also raise several other questions that we will examine in future work: 1. Can we obtain a computationally efficient algorithm for an even smaller but novel subset of this scenario? Can related work (for example, in contextual bandits [Dudik et al., 2011, Agarwal., 2014]) apply these strategies continuously to achieve a compatible value in this scenario? Can we apply a more efficient scope for action?"}, {"heading": "Appendix", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "A A New Lower Bound", "text": "In this section, we prove that the cumulative reward H = 1 reward fulfills the assumptions we made in this paper.We first recall some definitions. (Sh, a) The system descriptors are replaced by a transition phase that includes a distribution across states with each state action pair. (Sh, a) The initial situation is characterized by a transition phase in which all transitions of SH are terminal.There is also a reward distribution R that associates a random reward with each state action pair. (s, a) We use r (s, a) the random instantaneous reward for performing a state action. (s, a) We assume that the cumulative reward is associated with each state action pair. (s, a) The random reward for performing a state action is. (s, a) We assume that the cumulative reward is H = 1 (a)."}, {"heading": "B Models with Low Bellman Rank", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "B.1 Proof of Propositon 2", "text": "Let us allow M = | S | and each element of \u03bdh (\u00b7) and \u0394h (\u00b7) to be indexed by s \u00b2 s. Let us explicitly construct \u03bdh (f \u00b2) s = Pr [xh = (s, h) | a1: h \u2212 1 \u0445 \u03c0f \u00b2], and [\u0442h (f \u00b2) s = E [f (xh, ah) \u2212 rh \u2212 f (xh + 1, ah + 1)). In other words, \u0445h (f \u00b2) is the distribution over states induced by \u0421f \"in due course of time step h, and the s-th element of \u0445h is the traditional notion of Bellman's error for the state. It is easy to prove that equality (3) applies. For the limitation of norms, since we have (\u00b7) \u043fh \u00b2 1 = 1 and vice versa (\u00b7 h2) \u00b2, equality (3) applies."}, {"heading": "B.2 Generalization of Li [2009]\u2019s setting", "text": "Li [2009] considers the setting in which the learner is given an abstraction that maps the great state space S in an MDP to a limited abstract state space S in an MDP to be potentially much smaller than | S |, and it is guaranteed that Q? can be expressed as a function of (s), a). Li shows that when delayed Q-Learning is applied to this setting, the example complexity is a polynomial dependence on | S | S |.In the next statement we show that a similar setting for finite horizon problems allows Bellman factorization with a low Bellman rank."}, {"heading": "B.3 POMDP-like models", "text": "The model behaves like a POMDP, except that both the transition function and the reward also depend on observation, i.e. that both the transition and the reward are assumed to be independent of the current observation factor. (This model also generalizes the low-level dynamics described in Proposition 3: If the future hidden state is independent of the current hidden state based on observation, we can treat the hidden state as the low-level factor in Eq. (see Figure 1). Low-level dynamics require us to vectors for x at level h, (x \u00b2), q (x \u00b2), q (x \u00b2) for two M dimensions."}, {"heading": "B.4 Predictive State Representations", "text": "In this subsection we explain and prove the formal version of Proposition 5. Let us first remember the definitions and some basic properties of PSRs that can be found in Singh and al. [2004], Boots et al. [2011]. Let us consider dynamic systems with discrete and finite observation space O and action space A. Such systems can be fully specified by moment matrices PT | H, where H is a set of stories (past events) and T is a set of tests (future events). Elements of T and H are sequences of alternating actions and observations, and the separation of PT | H is indexed by t-T and H on the line and system plate is the probability that the test t would be successful on a particular past phase. For example, if t = aoa \u2032 o \u2032, success of t means seeing o and o \u2032 in the next two steps after observation."}, {"heading": "B.5 Linear Quadratic Regulators", "text": "In this subsection, we will prove that the linear square regulator (LQR) is a replacement for these costs. (See e.g., Anderson and Moore [2007] for a standard reference. (See also: LQR) for a standard reference. (See also: LQR) for a standard reference. (See also: LQR) for a standard reference. (See also: LQR) for a standard reference. (See also: LQR) for a standard reference. (See also: LQR) for a standard reference. (See also: LQR) for a standard reference. (See also: LQR) for a standard reference. (See also: LQR) for a standard reference. (See also: LQR) for a standard reference. (See also: QR) for a standard reference. (See also: QR) for a standard reference. (See also: QR) for a standard reference. (See also: QR) for a standard reference. (See also: QR) for a standard reference."}, {"heading": "C Auxiliary Proofs of the Main Lemmas", "text": "In this appendix, we give the complete evidence for the lemmas outlined in Section 6. Instead of directly analyzing olives and proving theorem 1, we instead focus our analysis on the robust Oliver variant introduced in Section 5.4. Oliver (algorithm 3) with the parameters \u03b8 = 0 and \u03b7 = 0 is exactly olives, and the two analyses are identical. To avoid repetition, we analyze Oliver (algorithm 3) in this appendix and prove the versions of the lemmas that can be used for Theorem 4. To facilitate understanding, readers can easily recreate the detailed evidence of the lemmas in Section 6 for olives by using \u03b8 = 0, \u03b7 = 0, \u2032 =, f? \u03b8 = f?, V? F = V? F?.For understanding, we split the evidence into three parts. The main evidence appears in Chapter 1, and two types of technical lemmas are used from there: (1) a series of 3rd lemmas F. = V?"}, {"heading": "C.1 Main Proofs", "text": "Lemma (Restatement of Lemma 1 from main text for convenience) With Vf = E [f = q = q (x1, \u03c0f (x1)))), we haveVf \u2212 V \u03c0f = E (f, \u03c0f, h). (24) Proof. (1) Proof. (1) Proof. (1) Proof. (2) Definition. (1) Definition. (1) Definition. (1) Definition. (1) Definition. (1) Definition. (1) Definition. (1) Definition. (1) Definition. (1) Definition. (2) Definition. (24), we getH = 1 E (xh, ah)."}, {"heading": "C.2 Lemmas for the volumetric argument", "text": "We adapt the work of Todd [1982] to derive the lemmas we use in C.1 The main result of this section is lemmas 11. Since this section focuses on generic geometric results, we assume a notation that is default for these arguments. The notation used in this section should not be confused with the rest of the paper. Theorem 6 (Theorem 2 by Todd [1982]). Define E = \u2212 w: \u2212 w \u2264 1) and E\u03b2 = {w \u00b2 E: | 1 w \u00b2 for 0 < \u03b2 \u2264 d \u2212 1 / 2. The ellipsoid, E + = = {w \u00b2 Rd > (I \u2212 p)."}, {"heading": "C.3 Deviation Bounds", "text": "Note: The statement of the lemmas in this section, which stand for Oliver, coincides with the deviations for Olive given in section 6. This should not be surprising, since the two algorithms capture data and estimates in the same way at the same time. Lemma 12 (deviation limit for V-F). With the probability of at least 1 \u2212 6 percent, which we have for all f-F-F-F-F-F-F-F-F-F-F-F-F-F-F-F-F-F-F-F-F-F-F-F-F-F-F-F-F-F-F-F-F-F-F-F-F-F-F-F-F-F-F-F-F-F-F-F-F-F-F-F-F-F-F-F-F-F-F-F-F-F-F-F-F-F-F-F-F-F-F-F-F-F-F-F-F-F-F-F-F-F-F-F-F-F-F-F-F-F-F-F-F-F-F-F-F-F-F-F-F-F-F-F-F-F-F-F-F-F-F-F-F-F-F-F-F-F-F-F-F-F-F-F-F-F-F-F-F"}, {"heading": "D Proofs of Extensions", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "D.1 Proof for unknown Bellman Rank (Theorem 2)", "text": "However, as we monitor the number of laps we have each led to Olive and stop the suutine number of actual laps, we cannot assume that the total failure probability is at least 1%. (...) The total failure probability is at most 1%. (...) The total failure probability is at least 1% (...). (...) All highly probable events in the analysis of Olive occur for all i = 1, 2,. (...) While the algorithms return a roughly optimal policy if it proves that the algorithms still need to be completed. (...) If M \u00b2 < M Eq. (20) and Lemma 10 are not applied, we cannot naively apply arguments from the analysis of Olive. (...) We monitor the number of laps in which we have exceeded the suutine number of actual laps."}, {"heading": "D.2.1 Definitions and Basic Lemmas", "text": "The notations X, x, d, in this section are used according to conventions in literature and may not share the semantics with the same symbols used elsewhere in this paper.Definition 12 (VC dimension): Definition of class H (0, 1): Definition of the VC dimension VC dimming (H) is defined as the maximum cardinality of a sentence X = {x1,. \u2212 Definition of class H (H): Definition of class H (X): Definition of class X (X): Definition of class X (X): Definition of class X (X): Definition of class H (X): Definition of class X (X): Definition of class X (X): Definition of class X (X): Definition of class X (X): Definition of class X (X): Definition of class X (X): Definition of class X (X): Definition of H (X): Definition of class H (X): Definition of class H."}, {"heading": "D.2.2 Proof of Lemma 16", "text": "The idea is to deviate boundaries for each of the three terms in the definition of E ((\u03c0, g), (4), (4), (4), (4), (5), (5), (5), (5), (5), (5), (6), (6), (6), (6), (6). Each term takes the form of a value multiplied by a real function, and we first show that the functional space formed by these products has a pseudo-dimension."}, {"heading": "D.3 Proofs for OLIVER", "text": "Remember that the main lemmas for Oliver's analysis have been demonstrated in Appendix C.1, so in the following Theorem 4.Proof of Theorem 4 we will directly prove it. Suppose the preconditions of Lemma 9 (Eq. (25)) and Lemma 10 (Eq. (27) are met; we will show them by invoking the limits of deviation later. In Lemma 9, when the algorithm ends, the value of the output policy is at least V? F, \u03b8 \u2212 \u2212 H\u03b8.Suppose that \u2032 = + 2H (3 \u221a M) + \u03b7 (line 1), so that the sub-optimality in comparison with V? F, \u03b8 is at most + 2H (3 \u221a M) + \u03b7) + HTB (Lemm) + 8H \u221a M (3), which justifies the claim to sub-optimality. It remains to show the sample complexity limit. Applying Lemma 9, ends in each turn before the algorithm."}, {"heading": "Acknowledgements", "text": "Part of this work was completed during the time of NJ and AK at Microsoft Research."}], "references": [{"title": "Contextual bandit learning with predictable rewards", "author": ["Alekh Agarwal", "Miroslav Dud\u0301\u0131k", "Satyen Kale", "John Langford", "Robert E. Schapire"], "venue": "In International Conference on Artificial Intelligence and Statistics (AISTATS),", "citeRegEx": "Agarwal et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Agarwal et al\\.", "year": 2012}, {"title": "Taming the monster: A fast and simple algorithm for contextual bandits", "author": ["Alekh Agarwal", "Daniel Hsu", "Satyen Kale", "John Langford", "Lihong Li", "Robert E. Schapire"], "venue": "In International Conference on Machine Learning (ICML),", "citeRegEx": "Agarwal et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Agarwal et al\\.", "year": 2014}, {"title": "Optimal control: linear quadratic methods", "author": ["Brian D.O. Anderson", "John B. Moore"], "venue": "Courier Corporation,", "citeRegEx": "Anderson and Moore.,? \\Q2007\\E", "shortCiteRegEx": "Anderson and Moore.", "year": 2007}, {"title": "Learning near-optimal policies with bellman-residual minimization based fitted policy iteration and a single sample path", "author": ["Andr\u00e1s Antos", "Csaba Szepesv\u00e1ri", "R\u00e9mi Munos"], "venue": "Machine Learning,", "citeRegEx": "Antos et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Antos et al\\.", "year": 2008}, {"title": "The nonstochastic multiarmed bandit problem", "author": ["Peter Auer", "Nicolo Cesa-Bianchi", "Yoav Freund", "Robert E. Schapire"], "venue": "SIAM Journal on Computing,", "citeRegEx": "Auer et al\\.,? \\Q2002\\E", "shortCiteRegEx": "Auer et al\\.", "year": 2002}, {"title": "Reinforcement learning of POMDPs using spectral methods", "author": ["Kamyar Azizzadenesheli", "Alessandro Lazaric", "Animashree Anandkumar"], "venue": "Conference on Learning Theory (COLT),", "citeRegEx": "Azizzadenesheli et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Azizzadenesheli et al\\.", "year": 2016}, {"title": "Policy iteration based on stochastic factorization", "author": ["Andr\u00e9 da Motta Salles Barreto", "Joelle Pineau", "Doina Precup"], "venue": "Journal of Artificial Intelligence Research,", "citeRegEx": "Barreto et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Barreto et al\\.", "year": 2014}, {"title": "Reinforcement Learning using Kernel-based Stochastic Factorization", "author": ["Andre S Barreto", "Doina Precup", "Joelle Pineau"], "venue": "In Advances in Neural Information Processing Systems (NIPS),", "citeRegEx": "Barreto et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Barreto et al\\.", "year": 2011}, {"title": "Unifying count-based exploration and intrinsic motivation", "author": ["Marc G. Bellemare", "Sriram Srinivasan", "Georg Ostrovski", "Tom Schaul", "David Saxton", "Remi Munos"], "venue": null, "citeRegEx": "Bellemare et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Bellemare et al\\.", "year": 2016}, {"title": "Characterizations of learnability for classes of {0", "author": ["Shai Ben-David", "Nicolo Cesa-Bianchi", "Philip M. Long"], "venue": "n}-valued functions. In Conference on Learning Theory (COLT),", "citeRegEx": "Ben.David et al\\.,? \\Q1992\\E", "shortCiteRegEx": "Ben.David et al\\.", "year": 1992}, {"title": "Closing the learning-planning loop with predictive state representations", "author": ["Byron Boots", "Sajid M. Siddiqi", "Geoffrey J. Gordon"], "venue": "International Journal of Robotics Research,", "citeRegEx": "Boots et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Boots et al\\.", "year": 2011}, {"title": "R-max-a general polynomial time algorithm for near-optimal reinforcement learning", "author": ["Ronen I. Brafman", "Moshe Tennenholtz"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Brafman and Tennenholtz.,? \\Q2003\\E", "shortCiteRegEx": "Brafman and Tennenholtz.", "year": 2003}, {"title": "Sample complexity of episodic fixed-horizon reinforcement learning", "author": ["Christoph Dann", "Emma Brunskill"], "venue": "In Advances in Neural Information Processing Systems (NIPS),", "citeRegEx": "Dann and Brunskill.,? \\Q2015\\E", "shortCiteRegEx": "Dann and Brunskill.", "year": 2015}, {"title": "A probabilistic theory of pattern recognition", "author": ["Luc Devroye", "L\u00e1szl\u00f3 Gy\u00f6rfi", "G\u00e1bor Lugosi"], "venue": null, "citeRegEx": "Devroye et al\\.,? \\Q1996\\E", "shortCiteRegEx": "Devroye et al\\.", "year": 1996}, {"title": "Efficient optimal learning for contextual bandits", "author": ["Miroslav Dudik", "Daniel Hsu", "Satyen Kale", "Nikos Karampatziakis", "John Langford", "Lev Reyzin", "Tong Zhang"], "venue": "In Uncertainty in Artificial Intelligence (UAI),", "citeRegEx": "Dudik et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Dudik et al\\.", "year": 2011}, {"title": "Tree-based batch mode reinforcement learning", "author": ["Damien Ernst", "Pierre Geurts", "Louis Wehenkel"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Ernst et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Ernst et al\\.", "year": 2005}, {"title": "Error propagation for approximate policy and value iteration", "author": ["Amir-Massoud Farahmand", "Csaba Szepesv\u00e1ri", "R\u00e9mi Munos"], "venue": "In Advances in Neural Information Processing Systems (NIPS),", "citeRegEx": "Farahmand et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Farahmand et al\\.", "year": 2010}, {"title": "Decision theoretic generalizations of the PAC model for neural net and other learning applications", "author": ["David Haussler"], "venue": "Information and computation,", "citeRegEx": "Haussler.,? \\Q1992\\E", "shortCiteRegEx": "Haussler.", "year": 1992}, {"title": "Sphere packing numbers for subsets of the boolean n-cube with bounded vapnik-chervonenkis dimension", "author": ["David Haussler"], "venue": "Journal of Combinatorial Theory, Series A,", "citeRegEx": "Haussler.,? \\Q1995\\E", "shortCiteRegEx": "Haussler.", "year": 1995}, {"title": "A generalization of sauer\u2019s lemma", "author": ["David Haussler", "Philip M. Long"], "venue": "Journal of Combinatorial Theory, Series A,", "citeRegEx": "Haussler and Long.,? \\Q1995\\E", "shortCiteRegEx": "Haussler and Long.", "year": 1995}, {"title": "The Malmo Platform for artificial intelligence experimentation", "author": ["Matthew Johnson", "Katja Hofmann", "Tim Hutton", "David Bignell"], "venue": "In International Joint Conference on Artificial Intelligence (IJCAI),", "citeRegEx": "Johnson et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Johnson et al\\.", "year": 2016}, {"title": "Model-based exploration in continuous state spaces", "author": ["Nicholas K. Jong", "Peter Stone"], "venue": "In Abstraction, Reformulation, and Approximation,", "citeRegEx": "Jong and Stone.,? \\Q2007\\E", "shortCiteRegEx": "Jong and Stone.", "year": 2007}, {"title": "Exploration in metric state spaces", "author": ["Sham Kakade", "Michael Kearns", "John Langford"], "venue": "In International Conference on Machine Learning (ICML),", "citeRegEx": "Kakade et al\\.,? \\Q2003\\E", "shortCiteRegEx": "Kakade et al\\.", "year": 2003}, {"title": "Near-optimal reinforcement learning in polynomial time", "author": ["Michael Kearns", "Satinder Singh"], "venue": "Machine Learning,", "citeRegEx": "Kearns and Singh.,? \\Q2002\\E", "shortCiteRegEx": "Kearns and Singh.", "year": 2002}, {"title": "A sparse sampling algorithm for near-optimal planning in large Markov decision processes", "author": ["Michael Kearns", "Yishay Mansour", "Andrew Y. Ng"], "venue": "Machine Learning,", "citeRegEx": "Kearns et al\\.,? \\Q2002\\E", "shortCiteRegEx": "Kearns et al\\.", "year": 2002}, {"title": "Bandit based monte-carlo planning", "author": ["Levente Kocsis", "Csaba Szepesv\u00e1ri"], "venue": "In European Conference on Machine Learning (ECML),", "citeRegEx": "Kocsis and Szepesv\u00e1ri.,? \\Q2006\\E", "shortCiteRegEx": "Kocsis and Szepesv\u00e1ri.", "year": 2006}, {"title": "PAC reinforcement learning with rich observations", "author": ["Akshay Krishnamurthy", "Alekh Agarwal", "John Langford"], "venue": "In Advances in Neural Information Processing Systems (NIPS),", "citeRegEx": "Krishnamurthy et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Krishnamurthy et al\\.", "year": 2016}, {"title": "Least-squares policy iteration", "author": ["Michail G. Lagoudakis", "Ronald Parr"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Lagoudakis and Parr.,? \\Q2003\\E", "shortCiteRegEx": "Lagoudakis and Parr.", "year": 2003}, {"title": "Finite-sample analysis of least-squares policy iteration", "author": ["Alessandro Lazaric", "Mohammad Ghavamzadeh", "R\u00e9mi Munos"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Lazaric et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Lazaric et al\\.", "year": 2012}, {"title": "A unifying framework for computational reinforcement learning theory", "author": ["Lihong Li"], "venue": "PhD thesis, Rutgers, The State University of New Jersey,", "citeRegEx": "Li.,? \\Q2009\\E", "shortCiteRegEx": "Li.", "year": 2009}, {"title": "Predictive representations of state", "author": ["Michael L. Littman", "Richard S. Sutton", "Satinder Singh"], "venue": "In Advances in Neural Information Processing Systems (NIPS),", "citeRegEx": "Littman et al\\.,? \\Q2001\\E", "shortCiteRegEx": "Littman et al\\.", "year": 2001}, {"title": "Human-level control through deep reinforcement learning", "author": ["Volodymyr Mnih", "Koray Kavukcuoglu", "David Silver", "Andrei A Rusu", "Joel Veness", "Marc G Bellemare", "Alex Graves", "Martin Riedmiller", "Andreas K Fidjeland", "Georg Ostrovski", "Stig Petersen", "Charles Beattie", "Amir Sadik", "Ioannis Antonoglou", "Helen King", "Dharshan Kumaran", "Daan Wierstra", "Shane Legg", "Demis Hassabis"], "venue": "Nature,", "citeRegEx": "Mnih et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Mnih et al\\.", "year": 2015}, {"title": "Error bounds for approximate policy iteration", "author": ["R\u00e9mi Munos"], "venue": "In International Conference on Machine Learning (ICML),", "citeRegEx": "Munos.,? \\Q2003\\E", "shortCiteRegEx": "Munos.", "year": 2003}, {"title": "Finite-time bounds for fitted value iteration", "author": ["R\u00e9mi Munos", "Csaba Szepesv\u00e1ri"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Munos and Szepesv\u00e1ri.,? \\Q2008\\E", "shortCiteRegEx": "Munos and Szepesv\u00e1ri.", "year": 2008}, {"title": "On learning sets and functions", "author": ["Balas K. Natarajan"], "venue": "Machine Learning,", "citeRegEx": "Natarajan.,? \\Q1989\\E", "shortCiteRegEx": "Natarajan.", "year": 1989}, {"title": "Model-based reinforcement learning and the eluder dimension", "author": ["Ian Osband", "Benjamin Van Roy"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Osband and Roy.,? \\Q2014\\E", "shortCiteRegEx": "Osband and Roy.", "year": 2014}, {"title": "Some extensions of an inequality of Vapnik and Chervonenkis", "author": ["Dmitriy Panchenko"], "venue": "Electronic Communications in Probability,", "citeRegEx": "Panchenko.,? \\Q2002\\E", "shortCiteRegEx": "Panchenko.", "year": 2002}, {"title": "Efficient PAC-optimal exploration in concurrent, continuous state MDPs with delayed updates", "author": ["Jason Pazis", "Ronald Parr"], "venue": "In Conference on Artificial Intelligence (AAAI),", "citeRegEx": "Pazis and Parr.,? \\Q2016\\E", "shortCiteRegEx": "Pazis and Parr.", "year": 2016}, {"title": "Convergence of Stochastic Processes", "author": ["D Pollard"], "venue": null, "citeRegEx": "Pollard.,? \\Q1984\\E", "shortCiteRegEx": "Pollard.", "year": 1984}, {"title": "Mastering the game of Go with deep neural networks and tree search. Nature, 2016", "author": ["David Silver", "Aja Huang", "Chris J. Maddison", "Arther Guez", "Laurent Sifre", "George van den Driessche", "Julian Schrittwieser", "Ioannis Antonoglou", "Veda Penneershelvam", "Marc Lanctot", "Sander Dieleman", "Dominik Grewe", "John Nham", "Nal Kalchbrenner", "Ilya Sutskever", "Timothy Lillicrap", "Madeleine Leach", "Koray Kavukcuoglu", "Thore Graepel", "Demis Hassabis"], "venue": null, "citeRegEx": "Silver et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Silver et al\\.", "year": 2016}], "referenceMentions": [{"referenceID": 39, "context": ", 2015] and Go [Silver et al., 2016] have sparked a flurry of research interest.", "startOffset": 15, "endOffset": 36}, {"referenceID": 8, "context": ", the use of pseudo-counts in Bellemare et al. [2016]), and combining MCTS with function approximation (e.", "startOffset": 30, "endOffset": 54}, {"referenceID": 8, "context": ", the use of pseudo-counts in Bellemare et al. [2016]), and combining MCTS with function approximation (e.g., Silver et al. [2016]).", "startOffset": 30, "endOffset": 131}, {"referenceID": 8, "context": ", the use of pseudo-counts in Bellemare et al. [2016]), and combining MCTS with function approximation (e.g., Silver et al. [2016]). Both types of approaches often require strong domain knowledge and large amounts of data to be successful. In this work, we study reinforcement learning settings where the agent receives rich sensory observations from the environment, forms complex contexts from these sensorimotor streams for learning and decisionmaking, and uses function approximation to generalize to unseen contexts. Our work departs from existing efforts by aiming at a sample complexity that depends neither on the number of unique contexts nor exponentially on the horizon. Similar goals have been attempted by Wen and Van Roy [2013] and Krishnamurthy et al.", "startOffset": 30, "endOffset": 742}, {"referenceID": 8, "context": ", the use of pseudo-counts in Bellemare et al. [2016]), and combining MCTS with function approximation (e.g., Silver et al. [2016]). Both types of approaches often require strong domain knowledge and large amounts of data to be successful. In this work, we study reinforcement learning settings where the agent receives rich sensory observations from the environment, forms complex contexts from these sensorimotor streams for learning and decisionmaking, and uses function approximation to generalize to unseen contexts. Our work departs from existing efforts by aiming at a sample complexity that depends neither on the number of unique contexts nor exponentially on the horizon. Similar goals have been attempted by Wen and Van Roy [2013] and Krishnamurthy et al. [2016] where attention is restricted to decision processes with special structures.", "startOffset": 30, "endOffset": 774}, {"referenceID": 26, "context": ", POMDPs with large observation spaces and reactive value functions [Krishnamurthy et al., 2016]) and \u201cnew\u201d means that we provide the first sample efficient algorithm (e.", "startOffset": 68, "endOffset": 96}, {"referenceID": 31, "context": "In some application scenarios, partial observability can be resolved by using a small sliding window: for example, in Atari games, it is common to keep track of the last 4 frames of images [Mnih et al., 2015].", "startOffset": 189, "endOffset": 208}, {"referenceID": 27, "context": ", LSPI [Lagoudakis and Parr, 2003] and FQI [Ernst et al.", "startOffset": 7, "endOffset": 34}, {"referenceID": 15, "context": ", LSPI [Lagoudakis and Parr, 2003] and FQI [Ernst et al., 2005]), the Bellman errors are defined as taking the expectation of a squared error unlike in our definition.", "startOffset": 43, "endOffset": 63}, {"referenceID": 26, "context": "The result is due to Krishnamurthy et al. [2016], and we restate it here for completeness.", "startOffset": 21, "endOffset": 49}, {"referenceID": 26, "context": "Proposition 1 (Restatement of Proposition 2 in Krishnamurthy et al. [2016]).", "startOffset": 47, "endOffset": 75}, {"referenceID": 26, "context": "Since F provides no information other than the fact that the true MDP lies in this family, the problem is equivalent to identifying the best arm in a multi-arm bandit with K arms, and the remaining analysis follows exactly the same as in Krishnamurthy et al. [2016].", "startOffset": 238, "endOffset": 266}, {"referenceID": 26, "context": "This result subsumes and generalizes the setting of Krishnamurthy et al. [2016] which requires deterministic transitions in the underlying MDP.", "startOffset": 52, "endOffset": 80}, {"referenceID": 20, "context": "(d) A popular RL experiment setting [Johnson et al., 2016].", "startOffset": 36, "endOffset": 58}, {"referenceID": 30, "context": "Next, we consider Predictive State Representations (PSRs), which are alternative models of partially observable systems with parameters grounded in observable quantities [Littman et al., 2001].", "startOffset": 170, "endOffset": 192}, {"referenceID": 29, "context": "Our last example considers a class of linear control problems well studied in control theory, called Linear Quadratic Regulators (LQRs). In LQRs, we show that the Bellman Rank is bounded by the dimension of the state space. Exploration in this class of problems has been previously considered by Osband and Van Roy [2014]. Again for presentation purposes, we only state an informal result here and defer the formal statement to Appendix B.", "startOffset": 101, "endOffset": 322}, {"referenceID": 26, "context": "The most closely related result is the recent work of Krishnamurthy et al. [2016], who also consider episodic reinforcement learning with infinite observation spaces and function approximation.", "startOffset": 54, "endOffset": 82}, {"referenceID": 3, "context": ", the inherent Bellman error in [Antos et al., 2008]).", "startOffset": 32, "endOffset": 52}, {"referenceID": 12, "context": "Unfortunately, our sample complexity is polynomially worse than the state of the art \u00d5( 2 log(1/\u03b4)) bounds for PAC-learning MDPs [Dann and Brunskill, 2015].", "startOffset": 129, "endOffset": 155}, {"referenceID": 5, "context": "Azizzadenesheli et al. [2016] provided a sample-efficient algorithm in a closely related setting, where both the observation space and the hidden-state space are small in cardinality.", "startOffset": 0, "endOffset": 30}, {"referenceID": 0, "context": "Finally, Contextual Decision Processes also encompass contextual bandits, where the optimal sample complexity is O(K log(N)/ ) [Agarwal et al., 2012].", "startOffset": 127, "endOffset": 149}, {"referenceID": 4, "context": "While no existing MDP lower bounds apply as is, since formulations vary, in Appendix A we adapt ideas from the literature [Auer et al., 2002] to obtain a \u03a9(MKH/ ) sample complexity lower bound for learning the MDPs in Example 1.", "startOffset": 122, "endOffset": 141}, {"referenceID": 34, "context": "Definition 6 (Natarajan dimension [Natarajan, 1989]).", "startOffset": 34, "endOffset": 51}, {"referenceID": 17, "context": "Definition 7 (Pseudo-dimension [Haussler, 1992]).", "startOffset": 31, "endOffset": 47}, {"referenceID": 36, "context": ", using tools from Panchenko [2002]).", "startOffset": 19, "endOffset": 36}, {"referenceID": 11, "context": "The result is similar in spirit to other lower bounds for PAClearning MDPs [Dann and Brunskill, 2015, Krishnamurthy et al., 2016], but we are not aware of any lower bound that applies directly to our setting. There are two main differences between our bound and the lower bound due to Dann and Brunskill [2015] for episodic MDPs.", "startOffset": 76, "endOffset": 311}, {"referenceID": 11, "context": "The result is similar in spirit to other lower bounds for PAClearning MDPs [Dann and Brunskill, 2015, Krishnamurthy et al., 2016], but we are not aware of any lower bound that applies directly to our setting. There are two main differences between our bound and the lower bound due to Dann and Brunskill [2015] for episodic MDPs. First, their bound assumes that the total reward is in [0, H], so the H dependence in the sample complexity is a consequence of scaling the rewards. Second, their MDP is not layered, but instead has M total states shared across all layers. In contrast, our process is layered with M distinct states per layer and total reward bounded in [0, 1]. Intuitively, our additional H dependence arises simply from having MH total states. At a high level, the proof is based on embedding \u0398(MH) independent multi-arm bandit instances into a MDP and requiring that the algorithm identify the best action in \u03a9(MH) of them to produce a near-optimal policy. By appealing to a sample complexity lower bound for best arm identification, this implies that the algorithm requires \u03a9(MHK/ ) samples to identify a near-optimal policy. We rely on a fairly standard lower bound for best arm identification. We reproduce the formal statement from Krishnamurthy et al. [2016], although the proof is based on earlier lower bounds due to Auer et al.", "startOffset": 76, "endOffset": 1280}, {"referenceID": 4, "context": "[2016], although the proof is based on earlier lower bounds due to Auer et al. [2002].", "startOffset": 67, "endOffset": 86}, {"referenceID": 29, "context": "2 Generalization of Li [2009]\u2019s setting Li [2009] considers the setting where the learner is given an abstraction \u03c6 that maps the large state space S in an MDP to some finite abstract state space S\u0304 in an MDP.", "startOffset": 20, "endOffset": 30}, {"referenceID": 29, "context": "2 Generalization of Li [2009]\u2019s setting Li [2009] considers the setting where the learner is given an abstraction \u03c6 that maps the large state space S in an MDP to some finite abstract state space S\u0304 in an MDP.", "startOffset": 20, "endOffset": 50}, {"referenceID": 29, "context": "Proposition 8 (A generalization of [Li, 2009]\u2019s setting).", "startOffset": 35, "endOffset": 45}, {"referenceID": 10, "context": "[2004], Boots et al. [2011]. Consider dynamical systems with discrete and finite observation space O and action space A.", "startOffset": 8, "endOffset": 28}, {"referenceID": 2, "context": "It is well known that in a discrete time LQR, the optimal policy is a non-stationary linear policy \u03c0(x, h) = P?,hx [Anderson and Moore, 2007], where P?,h \u2208 RK\u00d7d is a h-dependent control matrix.", "startOffset": 115, "endOffset": 141}, {"referenceID": 2, "context": ", Anderson and Moore [2007] for a standard reference) admit Bellman Factorization with low Bellman Rank.", "startOffset": 2, "endOffset": 28}, {"referenceID": 18, "context": "Lemma 19 (Bounding covering number by pseudo-dimension [Haussler, 1995]).", "startOffset": 55, "endOffset": 71}, {"referenceID": 38, "context": "Lemma 20 (Uniform deviation bound using covering number [Pollard, 1984]; also see Devroye et al.", "startOffset": 56, "endOffset": 71}, {"referenceID": 13, "context": "Lemma 20 (Uniform deviation bound using covering number [Pollard, 1984]; also see Devroye et al. [1996], Theorem 29.", "startOffset": 82, "endOffset": 104}], "year": 2017, "abstractText": "This paper studies systematic exploration for reinforcement learning with rich observations and function approximation. We introduce a new formulation, called contextual decision processes, that unifies and generalizes most prior settings. Our first contribution is a new complexity measure, the Bellman Rank , that we show enables tractable learning of near-optimal behavior in these processes and is naturally small for many well-studied reinforcement learning settings. Our second contribution is a new reinforcement learning algorithm that engages in systematic exploration to learn contextual decision processes with low Bellman Rank. The algorithm provably learns near-optimal behavior with a number of samples that is polynomial in all relevant parameters but independent of the number of unique observations. The algorithm uses Bellman error minimization with optimistic exploration and provides new insights into efficient exploration for reinforcement learning with function approximation.", "creator": "LaTeX with hyperref package"}}}