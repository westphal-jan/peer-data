{"id": "1610.09716", "review": {"conference": "NIPS", "VERSION": "v1", "DATE_OF_SUBMISSION": "30-Oct-2016", "title": "Doubly Convolutional Neural Networks", "abstract": "Building large models with parameter sharing accounts for most of the success of deep convolutional neural networks (CNNs). In this paper, we propose doubly convolutional neural networks (DCNNs), which significantly improve the performance of CNNs by further exploring this idea. In stead of allocating a set of convolutional filters that are independently learned, a DCNN maintains groups of filters where filters within each group are translated versions of each other. Practically, a DCNN can be easily implemented by a two-step convolution procedure, which is supported by most modern deep learning libraries. We perform extensive experiments on three image classification benchmarks: CIFAR-10, CIFAR-100 and ImageNet, and show that DCNNs consistently outperform other competing architectures. We have also verified that replacing a convolutional layer with a doubly convolutional layer at any depth of a CNN can improve its performance. Moreover, various design choices of DCNNs are demonstrated, which shows that DCNN can serve the dual purpose of building more accurate models and/or reducing the memory footprint without sacrificing the accuracy.", "histories": [["v1", "Sun, 30 Oct 2016 22:07:16 GMT  (705kb,D)", "http://arxiv.org/abs/1610.09716v1", "To appear in NIPS 2016"]], "COMMENTS": "To appear in NIPS 2016", "reviews": [], "SUBJECTS": "cs.LG", "authors": ["shuangfei zhai", "yu cheng", "zhongfei zhang", "weining lu"], "accepted": true, "id": "1610.09716"}, "pdf": {"name": "1610.09716.pdf", "metadata": {"source": "CRF", "title": "Doubly Convolutional Neural Networks", "authors": ["Shuangfei Zhai", "Yu Cheng"], "emails": ["szhai2@binghamton.edu", "chengyu@us.ibm.com", "luwn14@mails.tsinghua.edu.cn", "zhongfei@cs.binghamton.edu"], "sections": [{"heading": "1 Introduction", "text": "In recent years, we have achieved great success in solving many problems in machine learning and computer vision. (...) We are extremely efficient in researching the translation properties of images, which is the key to very deep models without heavy over-adjustment. (...) We have made considerable progress in researching deeper architectures that can lead to even better generalizations and / or parameters. (...) Our intuition is based on the observation of well-trained CNNs, where many of the filters learned are the easily translated versions of each other. (...) To quantify this in a more formal way, we define the correlation between two conventional filters within a layer. (...) We define the correlation between two conventional filters within a layer. (...)"}, {"heading": "2 Model", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "2.1 Convolution", "text": "We define the folding process that occurs with I '+ 1 = I '.I \".I\".I \".I\".I \".I\".I \".I\".I \".I\".I \".I\".I \".I\".I \".I\".I \".I\".I \".I\".I \".I\".I \".I\".I \".I\".I \".I\".I \".I\".I \".I\".I \".I\".I \".I\".I \".I\".I \".I\".I \".I\".I \".I\".I \".I\".I \".I\".I \".I\".I \".I\".I.I. \"I\".I.I \".I\".I.I \".I.I.\" I \".I.I.\" I \".I.I.\" I \"I\".I.\" I \".I.\" I\".\" I \".I.\" I\".I. \"I\".I.I. \"I\" I \".I.\" I\" I \".I.\" I\".\".\" I \"I\".I. \"I\".I. \"I\".I. \"I\".I. \"I\".I. \"I\".I.I. \"I\" I \".I.\" I \".I.\" I \".I.\" I. \"I\""}, {"heading": "2.2 Double convolution", "text": "Next, we introduce the double folding operation and define it as a filter with the size of I \"+ 1 = I\" W, \"as follows: O\" 1i, j \"k\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p"}, {"heading": "3 Related work", "text": "The spirit of the DCNNs is to further advance the idea of parameter division of the revolutionary layers, which is shared by several current efforts. [9] examines the rotation symmetry of certain classes of images and therefore proposes rotating each filter (or alternatively the input) by multiplying 90 cases producing filters four times with the same amount of parameters for a single layer. [10] observes that filters learned from ReLU CNNs often contain pairs of opposite phases in the lower layers. Accordingly, the authors propose the concatenated ReLU, in which the linear activations are linked to their negations and then passed on to ReLU, which effectively doubles the number of filters. [11] proposes the diversified convolutions in which additional filters of larger sizes are produced by extending the basic evolutionary filters that are effective in anticipatory tasks such as image segmentation."}, {"heading": "4 Experiments", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "4.1 Datasets", "text": "We are conducting several experiments with DCNN on three image classification benchmarks: CIFAR-10, CIFAR-100 and ImageNet. CIFAR-10 and CIFAR-100 each contain 50,000 training and 10,000 tests of 32 x 32 large RGB images, drawn evenly from 10 and 100 classes, respectively. ImageNet is the data set used in the ILSVRC-2012 Challenge, which consists of approximately 1.2 million images for training and 50,000 images for validation, sampled from 1,000 classes."}, {"heading": "4.2 Is DCNN an effective architecture?", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "4.2.1 Model specifications", "text": "In the first set of experiments, we examine the effectiveness of DCNN in comparison to two different CNN drafts. > > > The three types of architectures that are subject to evaluation are: (1) CNN: This is equivalent to models that use the standard Convolutionary Layers. A Convolutionary Layer is referred to as C- < c > - < z >, where c, z are the number of filters and the filter size. (2) MaxoutCNN: This is equivalent to the maxout Convolutionary Networks [8] that use the maxout Unit to process dimensions along the channel dimensions (feature) with a stride k. (Feature) Dimensions used as the maxout Convolutionary Layer is called MC- < c > - < z > - ropoutCNN >, where the maxout Unit is used to process the channel dimensions with a stride k. (Feature) Dimensions used as the maxout Convolutionary Layer is used as a < < < <"}, {"heading": "4.2.2 Training protocols", "text": "All models are trained with Adadelta [21] on NVIDIA K40 GPUs. Bath size is set to 200 for CIFAR-10 and CIFAR-100 and 128 for ImageNet. Data augmentation has also been studied. In CIFAR-10 and CIFAR-100, we follow the simple data augmentation as in [2]. For training, 4 pixels are filled on each side of the images, 32 x 32 of which are sampled with random horizontal flipping. For testing, only the original 32 x 32 images are used. In ImageNet, 224 x 224 sections are sampled with random horizontal flipping; the standard color augmentation and the 10-cutout test are also used as in AlexNet [1]."}, {"heading": "4.2.3 Results", "text": "The test errors are summarized in Table 2 and Table 3, which also show the relative # parameters of DCNN and MaxoutCNN compared to the CNN standard. On the moderately large data sets CIFAR-10 and CIFAR-100, DCNN achieves the best results of the three control experiments, with and without data magnification. It is noteworthy that DCNN consistently improves by a margin over the CNN standard. Remarkably, DCNN MaxoutCNN also consistently exceeds CNN by 2.25 times fewer parameters, proving on the one hand that the double-revolutionary layers significantly improve model capacity, and, on the other hand, confirming our hypothesis that the parameter division introduced by double convolution actually acts as a very effective regulator. The results DCNN achieves on the two data sets are also among the best published results compared to [20, 22, 23, 24] and we note that DCNN has no difficulty scaling up to a large data collection, as the net results of the net results are obtained in comparison with the V2 results compared to the other comparative parameters."}, {"heading": "4.3 Does double convolution contribute to every layer?", "text": "In the next series of experiments, we will examine the effect of applying double folding to layers at different depths. To this end, we will replace the revolutionary layers at each level of the CNN standard defined in 4.2.1 with a double revolutionary layer counterpart (e.g. by replacing a layer C-128-3 layer with a layer DC-128-4-3-2 layer).We therefore define DCNN [i-j] as the network resulting from the replacement of the i \u2011 jth revolutionary layer of a CNN with its double revolutionary layer counterpart and traction {DCNN [1-2], DCNN [3-4], DCNN [5-6], DCNN [7-8]}} on CIFAR-10 and CIFAR-100 following the same protocol as that in Section 4.2.2. The results are shown in Table 4."}, {"heading": "5 Conclusion", "text": "We have proposed the Double Convolutionary Neural Networks (DCNNs), which use a novel double-folding operation to provide an additional level of parameter transmission via CNNs. We show that DCNNs generalize standard CNNs and refer to several current proposals that investigate parameter redundancy in CNNs. A DCNN can be easily implemented through modern deep learning libraries by reusing the efficient convolution module. DCNNs can be used to fulfill the dual purpose of 1) improving classification accuracy as a regulated version of maxout networks and 2) being parameter efficient by flexibly varying their architectures. In the extensive experiments with CIFAR-10, CIFAR-100 and ImageNet datasets, we have shown that DCNNs significantly improve over other architectural types. Furthermore, we have shown that introducing the double-convolutionary layer of each one of CNN improves its performance."}], "references": [{"title": "Imagenet classification with deep convolutional neural networks", "author": ["Alex Krizhevsky", "Ilya Sutskever", "Geoffrey E Hinton"], "venue": "In Advances in neural information processing systems,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2012}, {"title": "Very deep convolutional networks for large-scale image recognition", "author": ["Karen Simonyan", "Andrew Zisserman"], "venue": "arXiv preprint arXiv:1409.1556,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2014}, {"title": "Going deeper with convolutions", "author": ["Christian Szegedy", "Wei Liu", "Yangqing Jia", "Pierre Sermanet", "Scott Reed", "Dragomir Anguelov", "Dumitru Erhan", "Vincent Vanhoucke", "Andrew Rabinovich"], "venue": "In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2015}, {"title": "Deep residual learning for image recognition", "author": ["Kaiming He", "Xiangyu Zhang", "Shaoqing Ren", "Jian Sun"], "venue": "arXiv preprint arXiv:1512.03385,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2015}, {"title": "Dropout: A simple way to prevent neural networks from overfitting", "author": ["Nitish Srivastava", "Geoffrey Hinton", "Alex Krizhevsky", "Ilya Sutskever", "Ruslan Salakhutdinov"], "venue": "The Journal of Machine Learning Research,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 1929}, {"title": "Batch normalization: Accelerating deep network training by reducing internal covariate shift", "author": ["Sergey Ioffe", "Christian Szegedy"], "venue": "arXiv preprint arXiv:1502.03167,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2015}, {"title": "Caffe: Convolutional architecture for fast feature embedding", "author": ["Yangqing Jia", "Evan Shelhamer", "Jeff Donahue", "Sergey Karayev", "Jonathan Long", "Ross Girshick", "Sergio Guadarrama", "Trevor Darrell"], "venue": "In Proceedings of the ACM International Conference on Multimedia,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2014}, {"title": "Exploiting cyclic symmetry in convolutional neural networks", "author": ["Sander Dieleman", "Jeffrey De Fauw", "Koray Kavukcuoglu"], "venue": "arXiv preprint arXiv:1602.02660,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2016}, {"title": "Understanding and improving convolutional neural networks via concatenated rectified linear units", "author": ["Wenling Shang", "Kihyuk Sohn", "Diogo Almeida", "Honglak Lee"], "venue": "arXiv preprint arXiv:1603.05201,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2016}, {"title": "Multi-scale context aggregation by dilated convolutions", "author": ["Fisher Yu", "Vladlen Koltun"], "venue": "arXiv preprint arXiv:1511.07122,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2015}, {"title": "Multi-bias non-linear activation in deep neural networks", "author": ["Hongyang Li", "Wanli Ouyang", "Xiaogang Wang"], "venue": "arXiv preprint arXiv:1604.00676,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2016}, {"title": "Deep symmetry networks", "author": ["Robert Gens", "Pedro M Domingos"], "venue": "In Advances in neural information processing systems,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2014}, {"title": "Group equivariant convolutional networks", "author": ["Taco S Cohen", "Max Welling"], "venue": "arXiv preprint arXiv:1602.07576,", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2016}, {"title": "Learning invariant features through topographic filter maps", "author": ["Koray Kavukcuoglu", "Rob Fergus", "Yann LeCun"], "venue": "In Computer Vision and Pattern Recognition,", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2009}, {"title": "Structured transforms for small-footprint deep learning", "author": ["Vikas Sindhwani", "Tara Sainath", "Sanjiv Kumar"], "venue": "Advances in Neural Information Processing Systems", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2015}, {"title": "An exploration of parameter redundancy in deep networks with circulant projections", "author": ["Yu Cheng", "Felix X. Yu", "Rogerio Feris", "Sanjiv Kumar", "Shih-Fu Chang"], "venue": "In International Conference on Computer Vision (ICCV),", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2015}, {"title": "Deep fried convnets", "author": ["Zichao Yang", "Marcin Moczulski", "Misha Denil", "Nando de Freitas", "Alex Smola", "Le Song", "Ziyu Wang"], "venue": "In International Conference on Computer Vision (ICCV),", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2015}, {"title": "Tensorizing neural networks", "author": ["Alexander Novikov", "Dmitry Podoprikhin", "Anton Osokin", "Dmitry Vetrov"], "venue": "In Advances in Neural Information Processing Systems", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2015}, {"title": "Adadelta: an adaptive learning rate method", "author": ["Matthew D Zeiler"], "venue": "arXiv preprint arXiv:1212.5701,", "citeRegEx": "21", "shortCiteRegEx": "21", "year": 2012}, {"title": "Learning activation functions to improve deep neural networks", "author": ["Forest Agostinelli", "Matthew Hoffman", "Peter J. Sadowski", "Pierre Baldi"], "venue": "CoRR, abs/1412.6830,", "citeRegEx": "23", "shortCiteRegEx": "23", "year": 2014}, {"title": "Fast and accurate deep network learning by exponential linear units (elus)", "author": ["Djork-Arn\u00e9 Clevert", "Thomas Unterthiner", "Sepp Hochreiter"], "venue": "arXiv preprint arXiv:1511.07289,", "citeRegEx": "24", "shortCiteRegEx": "24", "year": 2015}], "referenceMentions": [{"referenceID": 0, "context": "While considerable progresses have been achieved by aggressively exploring deeper architectures [1, 2, 3, 4] or novel regularization techniques [5, 6] with the standard \"convolution + pooling\" recipe, we contribute from a different view by providing an alternative to the default convolution module, which can lead to models with even better generalization abilities and/or parameter efficiency.", "startOffset": 96, "endOffset": 108}, {"referenceID": 1, "context": "While considerable progresses have been achieved by aggressively exploring deeper architectures [1, 2, 3, 4] or novel regularization techniques [5, 6] with the standard \"convolution + pooling\" recipe, we contribute from a different view by providing an alternative to the default convolution module, which can lead to models with even better generalization abilities and/or parameter efficiency.", "startOffset": 96, "endOffset": 108}, {"referenceID": 2, "context": "While considerable progresses have been achieved by aggressively exploring deeper architectures [1, 2, 3, 4] or novel regularization techniques [5, 6] with the standard \"convolution + pooling\" recipe, we contribute from a different view by providing an alternative to the default convolution module, which can lead to models with even better generalization abilities and/or parameter efficiency.", "startOffset": 96, "endOffset": 108}, {"referenceID": 3, "context": "While considerable progresses have been achieved by aggressively exploring deeper architectures [1, 2, 3, 4] or novel regularization techniques [5, 6] with the standard \"convolution + pooling\" recipe, we contribute from a different view by providing an alternative to the default convolution module, which can lead to models with even better generalization abilities and/or parameter efficiency.", "startOffset": 96, "endOffset": 108}, {"referenceID": 4, "context": "While considerable progresses have been achieved by aggressively exploring deeper architectures [1, 2, 3, 4] or novel regularization techniques [5, 6] with the standard \"convolution + pooling\" recipe, we contribute from a different view by providing an alternative to the default convolution module, which can lead to models with even better generalization abilities and/or parameter efficiency.", "startOffset": 144, "endOffset": 150}, {"referenceID": 5, "context": "While considerable progresses have been achieved by aggressively exploring deeper architectures [1, 2, 3, 4] or novel regularization techniques [5, 6] with the standard \"convolution + pooling\" recipe, we contribute from a different view by providing an alternative to the default convolution module, which can lead to models with even better generalization abilities and/or parameter efficiency.", "startOffset": 144, "endOffset": 150}, {"referenceID": 0, "context": "Figure 1: Visualization of the 11\u00d7 11 sized first layer filters learned by AlexNet [1].", "startOffset": 83, "endOffset": 86}, {"referenceID": 0, "context": "Figure 2: Illustration of the averaged maximum 1-translation correlation, together with the standard deviation, of each convolutional layer for AlexNet [1] (left), and the 19-layer VGGNet [2] (right), respectively.", "startOffset": 152, "endOffset": 155}, {"referenceID": 1, "context": "Figure 2: Illustration of the averaged maximum 1-translation correlation, together with the standard deviation, of each convolutional layer for AlexNet [1] (left), and the 19-layer VGGNet [2] (right), respectively.", "startOffset": 188, "endOffset": 191}, {"referenceID": 0, "context": "As a concrete example, Figure 1 demonstrates the 3-translation correlation of the first layer filters learned by the AlexNet [1], with the weights obtained from the Caffe model zoo [7].", "startOffset": 125, "endOffset": 128}, {"referenceID": 6, "context": "As a concrete example, Figure 1 demonstrates the 3-translation correlation of the first layer filters learned by the AlexNet [1], with the weights obtained from the Caffe model zoo [7].", "startOffset": 181, "endOffset": 184}, {"referenceID": 1, "context": "We then load the weights of all the convolutional layers of AlexNet as well as the 19-layer VGGNet [2] from the Caffe model zoo, and report the averaged maximum 1-translation correlation of each layer in Figure 2.", "startOffset": 99, "endOffset": 102}, {"referenceID": 7, "context": "[9] explores the rotation symmetry of certain classes of images, and hence proposes to rotate each filter (or alternatively, the input) by a multiplication of 90\u25e6 which produces four times filters with the same amount of parameters for a single layer.", "startOffset": 0, "endOffset": 3}, {"referenceID": 8, "context": "[10] observes that filters learned by ReLU CNNs often contain pairs with opposite phases in the lower layers.", "startOffset": 0, "endOffset": 4}, {"referenceID": 9, "context": "[11] proposes the dilated convolutions, where additional filters with larger sizes are generated by dilating the base convolutional filters, which is shown to be effective in dense prediction tasks such as image segmentation.", "startOffset": 0, "endOffset": 4}, {"referenceID": 10, "context": "[12] proposes a multi-bias activation scheme where k, k \u2264 1, bias terms are learned for each filter, which produces a k times channel size for the convolution output.", "startOffset": 0, "endOffset": 4}, {"referenceID": 11, "context": "Additionally, [13, 14] have investigated the combination of more than one transformations of filters, such as rotation, flipping and distortion.", "startOffset": 14, "endOffset": 22}, {"referenceID": 12, "context": "Additionally, [13, 14] have investigated the combination of more than one transformations of filters, such as rotation, flipping and distortion.", "startOffset": 14, "endOffset": 22}, {"referenceID": 13, "context": "The need of correlated filters in CNNs is also studied in [15], where similar filters are explicitly learned and grouped with a group sparsity penalty.", "startOffset": 58, "endOffset": 62}, {"referenceID": 14, "context": "The work of Vikas and Tara [16] addresses the problem of compressing deep networks by applying structured transforms.", "startOffset": 27, "endOffset": 31}, {"referenceID": 15, "context": "[17] exploits the redundancy in the parametrization of deep architectures by imposing a circulant structure on the projection matrix, while allowing the use of FFT for faster computations.", "startOffset": 0, "endOffset": 4}, {"referenceID": 16, "context": "[18] attempts to obtain the compression of the fully-connected layers of the AlexNettype network with the Fastfood method.", "startOffset": 0, "endOffset": 4}, {"referenceID": 17, "context": "[19] use a multi-linear transform (Tensor-Train decomposition) to attain reduction of the number of parameters in the linear layers of CNNs.", "startOffset": 0, "endOffset": 4}, {"referenceID": 1, "context": "The architectures on the ImageNet dataset are variants of the 16-layer VGGNet [2] (right).", "startOffset": 78, "endOffset": 81}, {"referenceID": 5, "context": "For all the models, we apply batch normalization [6] immediately after each convolution layer, after which ReLU is used as the nonlinearity (including MaxoutCNN, which makes out implementation slightly different from [8]).", "startOffset": 49, "endOffset": 52}, {"referenceID": 1, "context": "Our model design is similar to VGGNet [2] where 3\u00d7 3 filter sizes are used, as well as Network in Network [20] where fully connected layers are completely eliminated.", "startOffset": 38, "endOffset": 41}, {"referenceID": 1, "context": "Note that the architecture we have used on the ImageNet dataset resembles the 16-layer VGGNet [2], but without the fully connected layers.", "startOffset": 94, "endOffset": 97}, {"referenceID": 18, "context": "All the models are trained with Adadelta [21] on NVIDIA K40 GPUs.", "startOffset": 41, "endOffset": 45}, {"referenceID": 1, "context": "On CIFAR-10 and CIFAR-100, We follow the simple data augmentation as in [2].", "startOffset": 72, "endOffset": 75}, {"referenceID": 0, "context": "On ImageNet, 224\u00d7 224 crops are sampled with random horizontal flipping; the standard color augmentation and the 10-crop testing are also applied as in AlexNet [1].", "startOffset": 160, "endOffset": 163}, {"referenceID": 19, "context": "The results achieved by DCNN on the two datasets are also among the best published results compared with [20, 22, 23, 24].", "startOffset": 105, "endOffset": 121}, {"referenceID": 20, "context": "The results achieved by DCNN on the two datasets are also among the best published results compared with [20, 22, 23, 24].", "startOffset": 105, "endOffset": 121}, {"referenceID": 1, "context": "Compared with the results of the 16-layer VGGNet in [2] with multiscale evaluation, our DCNN implementation achieves comparable results, with significantly less parameters.", "startOffset": 52, "endOffset": 55}, {"referenceID": 19, "context": "22% APL [23] 9.", "startOffset": 8, "endOffset": 12}, {"referenceID": 20, "context": "83% ELU [24] 6.", "startOffset": 8, "endOffset": 12}, {"referenceID": 0, "context": "We hence define DCNN[i-j] as the network resulted from replacing the i\u2212 jth convolutional layer of a CNN with its doubly convolutional layer counterpart, and train {DCNN[1-2], DCNN[3-4], DCNN[5-6], DCNN[7-8]} on CIFAR-10 and CIFAR-100 following the same protocol as that in Section 4.", "startOffset": 169, "endOffset": 174}, {"referenceID": 1, "context": "We hence define DCNN[i-j] as the network resulted from replacing the i\u2212 jth convolutional layer of a CNN with its doubly convolutional layer counterpart, and train {DCNN[1-2], DCNN[3-4], DCNN[5-6], DCNN[7-8]} on CIFAR-10 and CIFAR-100 following the same protocol as that in Section 4.", "startOffset": 169, "endOffset": 174}, {"referenceID": 2, "context": "We hence define DCNN[i-j] as the network resulted from replacing the i\u2212 jth convolutional layer of a CNN with its doubly convolutional layer counterpart, and train {DCNN[1-2], DCNN[3-4], DCNN[5-6], DCNN[7-8]} on CIFAR-10 and CIFAR-100 following the same protocol as that in Section 4.", "startOffset": 180, "endOffset": 185}, {"referenceID": 3, "context": "We hence define DCNN[i-j] as the network resulted from replacing the i\u2212 jth convolutional layer of a CNN with its doubly convolutional layer counterpart, and train {DCNN[1-2], DCNN[3-4], DCNN[5-6], DCNN[7-8]} on CIFAR-10 and CIFAR-100 following the same protocol as that in Section 4.", "startOffset": 180, "endOffset": 185}, {"referenceID": 4, "context": "We hence define DCNN[i-j] as the network resulted from replacing the i\u2212 jth convolutional layer of a CNN with its doubly convolutional layer counterpart, and train {DCNN[1-2], DCNN[3-4], DCNN[5-6], DCNN[7-8]} on CIFAR-10 and CIFAR-100 following the same protocol as that in Section 4.", "startOffset": 191, "endOffset": 196}, {"referenceID": 5, "context": "We hence define DCNN[i-j] as the network resulted from replacing the i\u2212 jth convolutional layer of a CNN with its doubly convolutional layer counterpart, and train {DCNN[1-2], DCNN[3-4], DCNN[5-6], DCNN[7-8]} on CIFAR-10 and CIFAR-100 following the same protocol as that in Section 4.", "startOffset": 191, "endOffset": 196}, {"referenceID": 6, "context": "We hence define DCNN[i-j] as the network resulted from replacing the i\u2212 jth convolutional layer of a CNN with its doubly convolutional layer counterpart, and train {DCNN[1-2], DCNN[3-4], DCNN[5-6], DCNN[7-8]} on CIFAR-10 and CIFAR-100 following the same protocol as that in Section 4.", "startOffset": 202, "endOffset": 207}, {"referenceID": 1, "context": "78 VGG-16 [2] 7.", "startOffset": 10, "endOffset": 13}, {"referenceID": 3, "context": "3 ResNet-152 [4] 5.", "startOffset": 13, "endOffset": 16}, {"referenceID": 2, "context": "1 GoogLeNet [3] 7.", "startOffset": 12, "endOffset": 15}, {"referenceID": 0, "context": "26% DCNN[1-2] 9.", "startOffset": 8, "endOffset": 13}, {"referenceID": 1, "context": "26% DCNN[1-2] 9.", "startOffset": 8, "endOffset": 13}, {"referenceID": 2, "context": "91% DCNN[3-4] 9.", "startOffset": 8, "endOffset": 13}, {"referenceID": 3, "context": "91% DCNN[3-4] 9.", "startOffset": 8, "endOffset": 13}, {"referenceID": 4, "context": "27% DCNN[5-6] 9.", "startOffset": 8, "endOffset": 13}, {"referenceID": 5, "context": "27% DCNN[5-6] 9.", "startOffset": 8, "endOffset": 13}, {"referenceID": 6, "context": "58% DCNN[7-8] 9.", "startOffset": 8, "endOffset": 13}, {"referenceID": 0, "context": "72% DCNN[1-8] 8.", "startOffset": 8, "endOffset": 13}, {"referenceID": 1, "context": "72% DCNN[1-8] 8.", "startOffset": 8, "endOffset": 13}, {"referenceID": 2, "context": "72% DCNN[1-8] 8.", "startOffset": 8, "endOffset": 13}, {"referenceID": 3, "context": "72% DCNN[1-8] 8.", "startOffset": 8, "endOffset": 13}, {"referenceID": 4, "context": "72% DCNN[1-8] 8.", "startOffset": 8, "endOffset": 13}, {"referenceID": 5, "context": "72% DCNN[1-8] 8.", "startOffset": 8, "endOffset": 13}, {"referenceID": 6, "context": "72% DCNN[1-8] 8.", "startOffset": 8, "endOffset": 13}], "year": 2016, "abstractText": "Building large models with parameter sharing accounts for most of the success of deep convolutional neural networks (CNNs). In this paper, we propose doubly convolutional neural networks (DCNNs), which significantly improve the performance of CNNs by further exploring this idea. In stead of allocating a set of convolutional filters that are independently learned, a DCNN maintains groups of filters where filters within each group are translated versions of each other. Practically, a DCNN can be easily implemented by a two-step convolution procedure, which is supported by most modern deep learning libraries. We perform extensive experiments on three image classification benchmarks: CIFAR-10, CIFAR-100 and ImageNet, and show that DCNNs consistently outperform other competing architectures. We have also verified that replacing a convolutional layer with a doubly convolutional layer at any depth of a CNN can improve its performance. Moreover, various design choices of DCNNs are demonstrated, which shows that DCNN can serve the dual purpose of building more accurate models and/or reducing the memory footprint without sacrificing the accuracy.", "creator": "LaTeX with hyperref package"}}}