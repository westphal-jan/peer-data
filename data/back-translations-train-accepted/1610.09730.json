{"id": "1610.09730", "review": {"conference": "NIPS", "VERSION": "v1", "DATE_OF_SUBMISSION": "30-Oct-2016", "title": "Active Learning from Imperfect Labelers", "abstract": "We study active learning where the labeler can not only return incorrect labels but also abstain from labeling. We consider different noise and abstention conditions of the labeler. We propose an algorithm which utilizes abstention responses, and analyze its statistical consistency and query complexity under fairly natural assumptions on the noise and abstention rate of the labeler. This algorithm is adaptive in a sense that it can automatically request less queries with a more informed or less noisy labeler. We couple our algorithm with lower bounds to show that under some technical conditions, it achieves nearly optimal query complexity.", "histories": [["v1", "Sun, 30 Oct 2016 23:39:18 GMT  (172kb)", "http://arxiv.org/abs/1610.09730v1", "To appear in NIPS 2016"]], "COMMENTS": "To appear in NIPS 2016", "reviews": [], "SUBJECTS": "cs.LG stat.ML", "authors": ["songbai yan", "kamalika chaudhuri", "tara javidi"], "accepted": true, "id": "1610.09730"}, "pdf": {"name": "1610.09730.pdf", "metadata": {"source": "CRF", "title": "Active Learning from Imperfect Labelers", "authors": ["Songbai Yan"], "emails": ["yansongbai@eng.ucsd.edu", "kamalika@cs.ucsd.edu", "tjavidi@eng.ucsd.edu"], "sections": [{"heading": null, "text": "ar Xiv: 161 0,09 730v 1 [cs.L G] 30 E"}, {"heading": "1 Introduction", "text": "In fact, it is the case that most of us are able to survive ourselves, and that they are able to survive themselves. (...) Most of them are able to survive themselves. (...) Most of them are able to survive themselves. (...) Most of them are able to survive themselves. (...) Most of them are able to survive themselves. (...) Most of them are able to survive themselves. (...) Most of them are able to survive themselves. (...) Most of them are able to survive themselves. (...) Most of them are able to survive themselves. (...) Most of them are able to survive themselves. (...) Most of them are able to survive themselves. (...) Most of them are able to survive themselves. (...)"}, {"heading": "1.1 Related work", "text": "A considerable amount of work has been done in the field of active learning, most of which relates to label swindlers who are not allowed to abstain. Theoretical work on this subject falls largely into two categories - the membership query model [6, 13, 18, 19], in which the student can request a label of any example in the instance room, and the PAC model, in which the student receives a large number of unlabeled examples from an underlying, unlabeled data distribution, and can request labels from a subset of these examples. Our work, and also that of [24], is based on the membership query model. There has also been a lot of work on active learning under different noise models. The problem is relatively simple when the labeler provides the basic truth marking labels - see [8, 9, 12] for work in the PAC model."}, {"heading": "2 Settings", "text": "[A] is the Function indicator: 1 [A] = 1, if A is true, and 0 otherwise. [A] is the Function indicator: 1 [A] = 1, if A is true, and 0 otherwise. [A] is the Function indicator: 1 [A] = 1, and 0. \"[A] is the Function indicator: 1.\" [A] is the Function indicator: 1. \"[D] is the Function indicator: 1.\" [A] is the Function indicator: 1. \"[A] is the Function indicator: 1.\" [D] is the Function indicator: 1. \"[D] is the Function indicator.\" [D] is the Function indicator: 1. \"[D] is the Function indicator: 1.\" [D] is the Function indicator: 1. \"[D] is the Function.\" [D] is the Function. \"D.\" D. \"D.\" D is the Function: 1. \"D is the Function.\" [D] is the Function."}, {"heading": "2.1 Conditions", "text": "s response with increasing severity. Later, we will provide an algorithm whose query complexity improves with increasing severity of the conditions. Condition 1: The response distribution of the label P (Y | X) fulfills: \u2022 (Abstention) For each x condition (0, 1] d condition, x condition, x condition, x condition, x condition, x condition, x condition, x condition, x condition, x condition, x condition, x condition, x condition, x condition, x condition, x condition, x condition, x condition, x condition, x condition, x condition, x condition, x condition, x-condition, x-condition, x-condition, x-condition, x-condition, x-condition, x-condition, x-, x-condition, x-condition, x-condition, x-condition, x-condition, x-condition, x-condition, x-condition."}, {"heading": "3 Learning one-dimensional thresholds", "text": "In this section we will start with the one-dimensional case (d = 1) to illustrate the main idea. We will generalize these results in the next section to a multi-dimensional instance space.If d = 1, the decision boundary g * becomes a point in [0, 1], and the corresponding classifier is a threshold function above [0,1]. In other words, the hypotheses space becomes H = {f\u03b8 (x) = 1 [x > \u03b8]: \u03b8 [0, 1]). We designate the boundary of the basic truth decisions based on the framework in which they are made."}, {"heading": "3.1 Algorithm", "text": "The proposed algorithm is a binary search style presented as an algorithm. (To make it easier, we assume that there is an integer system.) Algorithm 1 adopts a desired precision if it has a specific algorithm for learning thresholds. (For reasons of simplicity, which we do not understand). (For reasons of simplicity, which we do not understand). (For reasons of simplicity, which we do not understand). (For reasons of simplicity, which we do.) Algorithm 1: 1 (for reasons of simplicity). (For reasons of simplicity, which we do not understand). (For reasons of simplicity, which we do not understand). (For reasons of simplicity, which we do not understand). (For reasons of simplicity, which we do not understand). (For reasons of simplicity). (For reasons of simplicity, which we do not understand). (For reasons of simplicity). (For reasons of simplicity, which we do not understand). (For reasons of simplicity). (For reasons of simplicity, which we do not understand). (For reasons of simplicity). (For reasons of simplicity, which we do not understand). (For reasons of simplicity). (For reasons of simplicity). (for reasons of simplicity, which we do not understand)."}, {"heading": "3.2 Analysis", "text": "For algorithm 1 to be statistically consistent, we need only condition 1. Theorem 1. Let us be the basic truth. If the identifier L meets condition 1 and algorithm 1 ceases to deliver the results, then we must condition 1. Theorem 1. Let us be the basic truth. If the identifier L meets condition 1 and the algorithm 1. Condition 2. Label does not apply to 0 / 1, because in contrast to the difference between the numbers of the abstention answers in Uk (Vk) and Mk the difference between the numbers of 0 and 1 remains above a positive constant. Under additional conditions 2. and 3. we can derive upper limits of quantum complexity for our algorithm. (Recall f and \u03b2 are defined in conditions 2. and 3.) Theorem 2. Let us be the basic truth and the results of algorithm 1. Condition 2. Condition 3. Condition 3."}, {"heading": "3.3 Lower Bounds", "text": "In this subsection, we specify lower limits of query complexity in the one-dimensional case and determine an approximate optimality of algorithm 1. In the next section, we will determine correspondingly lower limits for the high-dimensional case. The lower limit in [24] can easily be generalized to condition 2: Theorem 4. ([24] There is a universal constant [0, 1] and a labeler [1] that meets conditions 1 and 2, so that for every active learning algorithm A there is such a condition [0, 1], so that for small [0, 1], [0, A, L,] and a labeler [1f]."}, {"heading": "3.4 Remarks", "text": "Our results confirm the intuition that learning with abstention is easier than learning with loud lettering. This is true, because a loud lettering could mislead the learning algorithm, but an abstention response never does. In particular, our analysis shows that if the markup never abandons the lettering and abandons completely loud lettering with a probability of 1 \u2212 | x \u2212, then the near-optimal lettering query complexity is significantly greater than the near-optimal lettering query complexity associated with a lettering that would only dispense with the probability P (Y = X = X), and that the near-optimal lettering query complexity is significantly greater than the near-optimal lettering query complexity, which would dispense only with the probability P (Y = X = X)."}, {"heading": "4 The multidimensional case", "text": "We follow [6] to get the results from one-dimensional thresholds to the d-dimensional (d > 1) smooth boundary fragment class \u03a3 (K, \u03b3).Algorithm 3 The active learning algorithm for the smooth boundary fragment class 1: Input: \u03b4, preservation, \u03b32: M \u2190 (\u0432 \u2212 1 / \u03b3).L \u2190 {0 M, 1 M,., M \u2212 1 M} d \u2212 13: For each l \u00b2 L, we apply algorithm 1 with parameters, where Iq = [q1q / Md \u2212 1) learns a threshold that g \u00b2 (l) 4: division of the instance space into cells {Iq \u2212 1,.,., M\u03b3 \u2212 1} d \u2212 1, where Iq = [q1q / Md \u2212 1), (q1 + 1), (q1 + 1), (qd \u2212 3), (qd \u2212 3), (qd \u2212 4), (qd \u2212 1, \u2212 1."}, {"heading": "4.1 Lower bounds", "text": "Theorem 6. There are universal constants \u03b40 \u0445 (0, 1), c0 > 0, and a label L that meets conditions 1 and 2, so that for each active learning algorithm A there is a g-shaped constant, so that for each sufficiently small algorithm there is a g-shaped constant, so that for each sufficiently small algorithm there is a g-shaped constant (0, 1), and a labeller L that meets conditions 1, 2, and condition 3 with f (x) = C'x\u03b1 (C '> 0 and 0 < \u03b1 \u2264 2 are constants), so that for each active learning algorithm A there is a g-shaped constant, so that for each sufficiently small algorithm A there is a g-shaped constant, so that for each sufficiently large g-shaped constant there is available."}, {"heading": "4.2 Algorithm and Analysis", "text": "Remember the decision boundary of the smooth boundary fragment class, which can be considered an epigraph of a smooth function [0, 1] d \u2212 1 \u2192 [0, 1]. For d > 1, we can reduce the problem to the one-dimensional problem by discrediting the first d \u2212 1 dimensions of the instance space and then performing a polynomial interpolation. For the sake of simplicity, we assume that g, M / \u03b3 are integral in algorithm 3. We have a similar consistency guarantee and upper boundaries as in the one-dimensional case. Theorem 8. Let g be the basic truth. If the labeler L meets condition 1 and algorithm 3 stops producing g, then it is most likely at least 1 \u2212 2."}, {"heading": "A Proof of query complexities", "text": "n n (n) n (n) n (n) n (n) n (n) n (n) n (n) n (n) n (n) n (n) n (n) n (n) n (n) n (n) n (n) n (n) n (n) n (n) n (n) n (n) n (n) n (n) n (n) n (n) n (n) n (n) n (n) n (n) n (n) n (n) n (n) n (n) n (n) n (n) n (n) n (n) n (n) n (n) n (n) n (n) n (n) n (n) n (n) n (n) n (n) n) n (n) n (n) n (n) n (n) n) n (n) n (n) n (n) n (n) n (n) n (n) n (n) n (n) n (n) n (n) n (n) n (n) n (n) n (n) n (n) n (n) n (n) n (n) n (n) n (n) n (n) n (n) n (n) n) n (n) n (n) n (n) n) n (n (n) n) n (n) n) n (n) n) n (n) n) n (n) n) n (n) n) n (n (n) n) n (n) n) n (n (n) n) n (n) n) n (n) n) n (n) n) n (n) n) n (n) n (n) n) n (n) n) n (n) n) n (n (n) n (n) n (n) n) n (n) n n) n (n) n (n (n) n) n) n (n) n) n (n) n) n (n (n) n) n (n) n) n (n (n) n n n) n n) n n n n n n n n n n n n n n n n n (n"}, {"heading": "Qk and p \u2264 \u03b8\u2217 \u2264 q.", "text": "For each k, k, k, k, k, k, k, k, k, k, k, k, k, k, k, k, k, k, k, k, k, k, k, k, k, k, k, k, k, k, k, k, k, k, k, k, k, k, k, k, k, k, k, k, k, k, k, k, k, k, k, k, k, k, k, k, k, k, k, k, k, k, k, k, k, k, k, k, k, k, k, k, k, k, k, k, k, k, k, k, k, k, k, k, k, k, k, k, k, k, k, k, k, k, k, k, k, k, k, k, k, k, k, k, k, k, k, k, k, k, k, k, k, k, k, k, k, k, k, k, k, k, k, k, k, k, k, k, k, k, k, k, k, k, k, k, k, k, k, k, k, k, k, k, k, k, k, k, k, k, k, k, k, k, k, k, k, k, k, k, k, k, k, k, k, k, k, k, k, k, k, k, k, k, k, k, k, k, k, k, k, k, k, k, k, k, k, k, k, k, k, k, k, k, k, k, k, k, k, k, k, k, k, k, k, k, k, k, k, k, k, k, k, k, k, k, k, k, k, k, k, k, k, k, k, k, k, k, k, k, k, k, k, k, k, k, k, k, k, k, k, k, k, k, k, k, k, k, k, k, k, k, k, k,"}, {"heading": "B Proof of lower bounds", "text": "First, we present some notations for this section. Given is a labeler L and an active learning algorithm A, denoted by PnL, A the distribution of n samples (Xi, Yi) n i = 1, where Yi is drawn from the distribution PL (Y | X) and Xi is drawn from the active learning algorithm based exclusively on the knowledge of {(Xj, Yj)} i \u2212 1j = 1. We will draw the conclusions of PnL, A and PL (Y | X) if it is clear from the context. For a sequence {Xi} i = 1 denote of Xn of the subsequence {X1,., Xn}. Definition 1. P, Q on a countable support, we define the KL divergence as dKL (P, Q) = x) ln P (x) ln P (x) ln P (x) Q (x).For two random variables X, Y, we define the mutual information as X (I)."}, {"heading": "C Technical lemmas", "text": "C.1 Concentration limits In this subsection we define Y1, Y2,. simultaneously a sequence of i.i.d. random variables. Suppose Y1 [\u2212 2], EY1 = 0, Var (Y1) = at least 4. Let us define Vn = nn \u2212 1 (\u2211 n = 1 Y 2 i \u2212 1n (\u0445 n = 1 Yi) 2). It is easy to check whether EVn = n\u03c3n has at least two results from [21] Lemma 6. ([21], Theorem 2). Let us take any 0 < \u03b4 < 1. Then there is an absolute constant D0 with a probability of at least 1 \u2212 for all n simultaneously, for all n Lemma 6. ([2], Theorem 2)."}], "references": [{"title": "Active and passive learning of linear separators under log-concave distributions", "author": ["M.-F. Balcan", "P.M. Long"], "venue": "In COLT,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2013}, {"title": "Agnostic active learning", "author": ["Maria-Florina Balcan", "Alina Beygelzimer", "John Langford"], "venue": "In Proceedings of the 23rd international conference on Machine learning,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2006}, {"title": "Robust interactive learning", "author": ["Maria-Florina Balcan", "Steve Hanneke"], "venue": "In Proceedings of The 25th Conference on Learning Theory,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2012}, {"title": "Agnostic active learning without constraints", "author": ["A. Beygelzimer", "D. Hsu", "J. Langford", "T. Zhang"], "venue": "In NIPS,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2010}, {"title": "Search improves label for active learning", "author": ["Alina Beygelzimer", "Daniel Hsu", "John Langford", "Chicheng Zhang"], "venue": "arXiv preprint arXiv:1602.07265,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2016}, {"title": "Minimax bounds for active learning", "author": ["Rui M. Castro", "Robert D. Nowak"], "venue": "IEEE Transactions on Information Theory,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2008}, {"title": "Sequential information maximization: When is greedy near-optimal", "author": ["Yuxin Chen", "S Hamed Hassani", "Amin Karbasi", "Andreas Krause"], "venue": "In Proceedings of The 28th Conference on Learning Theory,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2015}, {"title": "Improving generalization with active learning", "author": ["D.A. Cohn", "L.E. Atlas", "R.E. Ladner"], "venue": "Machine Learning,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 1994}, {"title": "Coarse sample complexity bounds for active learning", "author": ["S. Dasgupta"], "venue": "In NIPS,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2005}, {"title": "A general agnostic active learning algorithm", "author": ["S. Dasgupta", "D. Hsu", "C. Monteleoni"], "venue": "In NIPS,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2007}, {"title": "I don\u2019t know the label: Active learning with blind knowledge", "author": ["Meng Fang", "Xingquan Zhu"], "venue": "In Pattern Recognition (ICPR),", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2012}, {"title": "Teaching dimension and the complexity of active learning", "author": ["Steve Hanneke"], "venue": "In Learning Theory,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2007}, {"title": "Generalized teaching dimensions and the query complexity of learning", "author": ["Tibor Heged\u0171s"], "venue": "In Proceedings of the eighth annual conference on Computational learning theory,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 1995}, {"title": "Active learning in the non-realizable case", "author": ["M. K\u00e4\u00e4ri\u00e4inen"], "venue": "In ALT,", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2006}, {"title": "Active learning and discovery of object categories in the presence of unnameable instances", "author": ["Christoph Kading", "Alexander Freytag", "Erik Rodner", "Paul Bodesheim", "Joachim Denzler"], "venue": "In Computer Vision and Pattern Recognition (CVPR),", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2015}, {"title": "Some equivalent forms of bernoulli\u2019s inequality: A survey", "author": ["Yuan-Chuan Li", "Cheh-Chih Yeh"], "venue": "Applied Mathematics,", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2013}, {"title": "Plug-in approach to active learning", "author": ["Stanislav Minsker"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2012}, {"title": "Bayesian active learning with nonpersistent noise", "author": ["Mohammad Naghshvar", "Tara Javidi", "Kamalika Chaudhuri"], "venue": "IEEE Transactions on Information Theory,", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2015}, {"title": "The geometry of generalized binary search", "author": ["R.D. Nowak"], "venue": "IEEE Transactions on Information Theory,", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2011}, {"title": "Lower bounds for passive and active learning", "author": ["Maxim Raginsky", "Alexander Rakhlin"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "20", "shortCiteRegEx": "20", "year": 2011}, {"title": "Sequential nonparametric testing with the law of the iterated logarithm", "author": ["Aaditya Ramdas", "Akshay Balsubramani"], "venue": "In Proceedings of the Conference on Uncertainty in Artificial Intelligence,", "citeRegEx": "21", "shortCiteRegEx": "21", "year": 2016}, {"title": "Optimal aggregation of classifiers in statistical learning", "author": ["A.B. Tsybakov"], "venue": "Annals of Statistics,", "citeRegEx": "22", "shortCiteRegEx": "22", "year": 2004}, {"title": "Learning from weak teachers", "author": ["Ruth Urner", "Shai Ben-david", "Ohad Shamir"], "venue": "In International Conference on Artificial Intelligence and Statistics,", "citeRegEx": "23", "shortCiteRegEx": "23", "year": 2012}, {"title": "Active learning from noisy and abstention feedback", "author": ["Songbai Yan", "Kamalika Chaudhuri", "Tara Javidi"], "venue": "In Communication, Control, and Computing (Allerton),", "citeRegEx": "24", "shortCiteRegEx": "24", "year": 2015}, {"title": "Beyond disagreement-based agnostic active learning", "author": ["Chicheng Zhang", "Kamalika Chaudhuri"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "25", "shortCiteRegEx": "25", "year": 2014}], "referenceMentions": [{"referenceID": 1, "context": "Previous theoretical work on active learning has mostly focused on the above basic setting [2, 4, 7, 10, 25] and has developed algorithms under a number of different models of label noise.", "startOffset": 91, "endOffset": 108}, {"referenceID": 3, "context": "Previous theoretical work on active learning has mostly focused on the above basic setting [2, 4, 7, 10, 25] and has developed algorithms under a number of different models of label noise.", "startOffset": 91, "endOffset": 108}, {"referenceID": 6, "context": "Previous theoretical work on active learning has mostly focused on the above basic setting [2, 4, 7, 10, 25] and has developed algorithms under a number of different models of label noise.", "startOffset": 91, "endOffset": 108}, {"referenceID": 9, "context": "Previous theoretical work on active learning has mostly focused on the above basic setting [2, 4, 7, 10, 25] and has developed algorithms under a number of different models of label noise.", "startOffset": 91, "endOffset": 108}, {"referenceID": 24, "context": "Previous theoretical work on active learning has mostly focused on the above basic setting [2, 4, 7, 10, 25] and has developed algorithms under a number of different models of label noise.", "startOffset": 91, "endOffset": 108}, {"referenceID": 2, "context": "A handful of exceptions include [3] which allows class conditional queries, [5] which allows requesting counterexamples to current version spaces, and [23, 26] where the learner has access to a strong labeler and one or more weak labelers.", "startOffset": 32, "endOffset": 35}, {"referenceID": 4, "context": "A handful of exceptions include [3] which allows class conditional queries, [5] which allows requesting counterexamples to current version spaces, and [23, 26] where the learner has access to a strong labeler and one or more weak labelers.", "startOffset": 76, "endOffset": 79}, {"referenceID": 22, "context": "A handful of exceptions include [3] which allows class conditional queries, [5] which allows requesting counterexamples to current version spaces, and [23, 26] where the learner has access to a strong labeler and one or more weak labelers.", "startOffset": 151, "endOffset": 159}, {"referenceID": 10, "context": "This scenario arises naturally in difficult labeling tasks and has been considered in computer vision by [11, 15].", "startOffset": 105, "endOffset": 113}, {"referenceID": 14, "context": "This scenario arises naturally in difficult labeling tasks and has been considered in computer vision by [11, 15].", "startOffset": 105, "endOffset": 113}, {"referenceID": 23, "context": "The setting of active learning with an abstaining noisy labeler was first considered by [24], who looked at learning binary threshold classifiers based on queries to an labeler whose abstention rate is higher closer to the decision boundary.", "startOffset": 88, "endOffset": 92}, {"referenceID": 23, "context": "Under slightly stronger conditions as in [24], our algorithm has the same query complexity.", "startOffset": 41, "endOffset": 45}, {"referenceID": 23, "context": "An important property of our algorithm is that the improvement of query complexity is achieved in a completely adaptive manner; unlike previous work [24], our algorithm needs no information whatsoever on the abstention rates or rates of label noise.", "startOffset": 149, "endOffset": 153}, {"referenceID": 5, "context": "Thus our result also strengthens existing results on active learning from (non-abstaining) noisy labelers by providing an adaptive algorithm that achieves that same performance as [6] without knowledge of noise parameters.", "startOffset": 180, "endOffset": 183}, {"referenceID": 23, "context": "Our lower bounds generalize the lower bounds in [24], and shows that our upper bounds are nearly optimal.", "startOffset": 48, "endOffset": 52}, {"referenceID": 5, "context": "Theoretical work on this topic largely falls under two categories \u2014 the membership query model [6, 13, 18, 19], where the learner can request label of any example in the instance space, and the PAC model, where the learner is given a large set of unlabeled examples from an underlying unlabeled data distribution, and can request labels of a subset of these examples.", "startOffset": 95, "endOffset": 110}, {"referenceID": 12, "context": "Theoretical work on this topic largely falls under two categories \u2014 the membership query model [6, 13, 18, 19], where the learner can request label of any example in the instance space, and the PAC model, where the learner is given a large set of unlabeled examples from an underlying unlabeled data distribution, and can request labels of a subset of these examples.", "startOffset": 95, "endOffset": 110}, {"referenceID": 17, "context": "Theoretical work on this topic largely falls under two categories \u2014 the membership query model [6, 13, 18, 19], where the learner can request label of any example in the instance space, and the PAC model, where the learner is given a large set of unlabeled examples from an underlying unlabeled data distribution, and can request labels of a subset of these examples.", "startOffset": 95, "endOffset": 110}, {"referenceID": 18, "context": "Theoretical work on this topic largely falls under two categories \u2014 the membership query model [6, 13, 18, 19], where the learner can request label of any example in the instance space, and the PAC model, where the learner is given a large set of unlabeled examples from an underlying unlabeled data distribution, and can request labels of a subset of these examples.", "startOffset": 95, "endOffset": 110}, {"referenceID": 23, "context": "Our work and also that of [24] builds on the membership query model.", "startOffset": 26, "endOffset": 30}, {"referenceID": 7, "context": "The problem is relatively easy when the labeler always provides the ground truth labels \u2013 see [8, 9, 12] for work in this setting in the PAC model, and [13] for the membership query model.", "startOffset": 94, "endOffset": 104}, {"referenceID": 8, "context": "The problem is relatively easy when the labeler always provides the ground truth labels \u2013 see [8, 9, 12] for work in this setting in the PAC model, and [13] for the membership query model.", "startOffset": 94, "endOffset": 104}, {"referenceID": 11, "context": "The problem is relatively easy when the labeler always provides the ground truth labels \u2013 see [8, 9, 12] for work in this setting in the PAC model, and [13] for the membership query model.", "startOffset": 94, "endOffset": 104}, {"referenceID": 12, "context": "The problem is relatively easy when the labeler always provides the ground truth labels \u2013 see [8, 9, 12] for work in this setting in the PAC model, and [13] for the membership query model.", "startOffset": 152, "endOffset": 156}, {"referenceID": 13, "context": "[14] shows how to address this kind of noise in the PAC model by repeatedly querying an example until the learner is confident of its label; [18, 19] provide more sophisticated algorithms with better query complexities in the membership query model.", "startOffset": 0, "endOffset": 4}, {"referenceID": 17, "context": "[14] shows how to address this kind of noise in the PAC model by repeatedly querying an example until the learner is confident of its label; [18, 19] provide more sophisticated algorithms with better query complexities in the membership query model.", "startOffset": 141, "endOffset": 149}, {"referenceID": 18, "context": "[14] shows how to address this kind of noise in the PAC model by repeatedly querying an example until the learner is confident of its label; [18, 19] provide more sophisticated algorithms with better query complexities in the membership query model.", "startOffset": 141, "endOffset": 149}, {"referenceID": 5, "context": "A second setting is when the noise rate increases closer to the decision boundary; this setting has been studied under the membership query model by [6] and in the PAC model by [10, 4, 25].", "startOffset": 149, "endOffset": 152}, {"referenceID": 9, "context": "A second setting is when the noise rate increases closer to the decision boundary; this setting has been studied under the membership query model by [6] and in the PAC model by [10, 4, 25].", "startOffset": 177, "endOffset": 188}, {"referenceID": 3, "context": "A second setting is when the noise rate increases closer to the decision boundary; this setting has been studied under the membership query model by [6] and in the PAC model by [10, 4, 25].", "startOffset": 177, "endOffset": 188}, {"referenceID": 24, "context": "A second setting is when the noise rate increases closer to the decision boundary; this setting has been studied under the membership query model by [6] and in the PAC model by [10, 4, 25].", "startOffset": 177, "endOffset": 188}, {"referenceID": 0, "context": "Active learning is known to be particularly difficult in this setting; however, algorithms and associated label complexity bounds have been provided by [1, 2, 4, 10, 12, 25] among others.", "startOffset": 152, "endOffset": 173}, {"referenceID": 1, "context": "Active learning is known to be particularly difficult in this setting; however, algorithms and associated label complexity bounds have been provided by [1, 2, 4, 10, 12, 25] among others.", "startOffset": 152, "endOffset": 173}, {"referenceID": 3, "context": "Active learning is known to be particularly difficult in this setting; however, algorithms and associated label complexity bounds have been provided by [1, 2, 4, 10, 12, 25] among others.", "startOffset": 152, "endOffset": 173}, {"referenceID": 9, "context": "Active learning is known to be particularly difficult in this setting; however, algorithms and associated label complexity bounds have been provided by [1, 2, 4, 10, 12, 25] among others.", "startOffset": 152, "endOffset": 173}, {"referenceID": 11, "context": "Active learning is known to be particularly difficult in this setting; however, algorithms and associated label complexity bounds have been provided by [1, 2, 4, 10, 12, 25] among others.", "startOffset": 152, "endOffset": 173}, {"referenceID": 24, "context": "Active learning is known to be particularly difficult in this setting; however, algorithms and associated label complexity bounds have been provided by [1, 2, 4, 10, 12, 25] among others.", "startOffset": 152, "endOffset": 173}, {"referenceID": 5, "context": "A setting similar to ours was considered by [6, 24].", "startOffset": 44, "endOffset": 51}, {"referenceID": 23, "context": "A setting similar to ours was considered by [6, 24].", "startOffset": 44, "endOffset": 51}, {"referenceID": 5, "context": "[6] considers a non-abstaining labeler, and provides a near-optimal binary search style active learning algorithm; however, their algorithm is non-adaptive.", "startOffset": 0, "endOffset": 3}, {"referenceID": 23, "context": "[24] gives a nearly matching lower and upper query complexity bounds for active learning with abstention feedback, but they only give a nonadaptive algorithm for learning one dimensional thresholds, and only study the situation where the 2", "startOffset": 0, "endOffset": 4}, {"referenceID": 23, "context": "Besides [24] , [11, 15] study active learning with abstention feedback in computer vision applications.", "startOffset": 8, "endOffset": 12}, {"referenceID": 10, "context": "Besides [24] , [11, 15] study active learning with abstention feedback in computer vision applications.", "startOffset": 15, "endOffset": 23}, {"referenceID": 14, "context": "Besides [24] , [11, 15] study active learning with abstention feedback in computer vision applications.", "startOffset": 15, "endOffset": 23}, {"referenceID": 0, "context": "A function g : [0, 1]d\u22121 \u2192 R is (K, \u03b3)-H\u00f6lder smooth, if it is continuously differentiable up to \u230a\u03b3\u230b-th order, and for any x,y \u2208 [0, 1]d\u22121, \u2223", "startOffset": 15, "endOffset": 21}, {"referenceID": 0, "context": "A function g : [0, 1]d\u22121 \u2192 R is (K, \u03b3)-H\u00f6lder smooth, if it is continuously differentiable up to \u230a\u03b3\u230b-th order, and for any x,y \u2208 [0, 1]d\u22121, \u2223", "startOffset": 129, "endOffset": 135}, {"referenceID": 0, "context": "We are given an instance space X = [0, 1] and a label space L = {0, 1}.", "startOffset": 35, "endOffset": 41}, {"referenceID": 5, "context": "We consider a non-parametric setting as in [6, 17] where the hypothesis space is the smooth boundary fragment class H = {hg(x) = 1 [xd > g(x\u0303)] | g : [0, 1]d\u22121 \u2192 [0, 1] is (K, \u03b3)-H\u00f6lder smooth}.", "startOffset": 43, "endOffset": 50}, {"referenceID": 16, "context": "We consider a non-parametric setting as in [6, 17] where the hypothesis space is the smooth boundary fragment class H = {hg(x) = 1 [xd > g(x\u0303)] | g : [0, 1]d\u22121 \u2192 [0, 1] is (K, \u03b3)-H\u00f6lder smooth}.", "startOffset": 43, "endOffset": 50}, {"referenceID": 0, "context": "We consider a non-parametric setting as in [6, 17] where the hypothesis space is the smooth boundary fragment class H = {hg(x) = 1 [xd > g(x\u0303)] | g : [0, 1]d\u22121 \u2192 [0, 1] is (K, \u03b3)-H\u00f6lder smooth}.", "startOffset": 150, "endOffset": 156}, {"referenceID": 0, "context": "We consider a non-parametric setting as in [6, 17] where the hypothesis space is the smooth boundary fragment class H = {hg(x) = 1 [xd > g(x\u0303)] | g : [0, 1]d\u22121 \u2192 [0, 1] is (K, \u03b3)-H\u00f6lder smooth}.", "startOffset": 162, "endOffset": 168}, {"referenceID": 0, "context": "When d = 1, H reduces to the space of threshold functions {h\u03b8(x) = 1 [x > \u03b8] : \u03b8 \u2208 [0, 1]}.", "startOffset": 83, "endOffset": 89}, {"referenceID": 0, "context": "The performance of a classifier h(x) = 1 [xd > g(x\u0303)] is evaluated by the L distance between the decision boundaries \u2016g \u2212 g\u2217\u2016 = \u0301 [0,1]d\u22121 |g(x\u0303)\u2212 g\u2217(x\u0303)| dx\u0303.", "startOffset": 130, "endOffset": 135}, {"referenceID": 0, "context": "For each query x \u2208 [0, 1], the labeler L will return y \u2208 Y = {0, 1,\u22a5} (\u22a5 means that the labeler abstains from providing a 0/1 label) according to some distribution PL(Y = y | X = x).", "startOffset": 19, "endOffset": 25}, {"referenceID": 0, "context": "In particular, we want to find an algorithm with low query complexity \u039b(\u01eb, \u03b4,A, L, g\u2217), which is defined as the minimum number of queries that Algorithm A, acting on samples with ground truth g\u2217, should make to a labeler L to ensure that the output classifier hg(x) = 1 [xd > g(x\u0303)] has the property \u2016g \u2212 g\u2217\u2016 = \u0301 [0,1]d\u22121 |g(x\u0303)\u2212 g\u2217(x\u0303)| dx\u0303 \u2264 \u01eb with probability at least 1\u2212 \u03b4 over the responses of L.", "startOffset": 313, "endOffset": 318}, {"referenceID": 0, "context": "The response distribution of the labeler P (Y | X) satisfies: \u2022 (abstention) For any x\u0303 \u2208 [0, 1]d\u22121, xd, xd \u2208 [0, 1], if |xd \u2212 g\u2217(x\u0303)| \u2265 |xd \u2212 g\u2217(x\u0303)| then P (\u22a5| (x\u0303, xd)) \u2264 P (\u22a5| (x\u0303, xd)); \u2022 (noise) For any x \u2208 [0, 1], P (Y 6= 1 [xd > g\u2217(x\u0303)] | x, Y 6=\u22a5) \u2264 1 2 .", "startOffset": 90, "endOffset": 96}, {"referenceID": 0, "context": "The response distribution of the labeler P (Y | X) satisfies: \u2022 (abstention) For any x\u0303 \u2208 [0, 1]d\u22121, xd, xd \u2208 [0, 1], if |xd \u2212 g\u2217(x\u0303)| \u2265 |xd \u2212 g\u2217(x\u0303)| then P (\u22a5| (x\u0303, xd)) \u2264 P (\u22a5| (x\u0303, xd)); \u2022 (noise) For any x \u2208 [0, 1], P (Y 6= 1 [xd > g\u2217(x\u0303)] | x, Y 6=\u22a5) \u2264 1 2 .", "startOffset": 110, "endOffset": 116}, {"referenceID": 0, "context": "The response distribution of the labeler P (Y | X) satisfies: \u2022 (abstention) For any x\u0303 \u2208 [0, 1]d\u22121, xd, xd \u2208 [0, 1], if |xd \u2212 g\u2217(x\u0303)| \u2265 |xd \u2212 g\u2217(x\u0303)| then P (\u22a5| (x\u0303, xd)) \u2264 P (\u22a5| (x\u0303, xd)); \u2022 (noise) For any x \u2208 [0, 1], P (Y 6= 1 [xd > g\u2217(x\u0303)] | x, Y 6=\u22a5) \u2264 1 2 .", "startOffset": 213, "endOffset": 219}, {"referenceID": 0, "context": "Let C, \u03b2 be non-negative constants, and f : [0, 1] \u2192 [0, 1] be a nondecreasing function.", "startOffset": 44, "endOffset": 50}, {"referenceID": 0, "context": "Let C, \u03b2 be non-negative constants, and f : [0, 1] \u2192 [0, 1] be a nondecreasing function.", "startOffset": 53, "endOffset": 59}, {"referenceID": 21, "context": "The condition on the noise satisfies the popular Tsybakov noise condition [22].", "startOffset": 74, "endOffset": 78}, {"referenceID": 0, "context": "Let f : [0, 1] \u2192 [0, 1] be a nondecreasing function such that \u22030 < c < 1, \u22000 < a \u2264 1 \u22000 \u2264 b \u2264 23a, f(b) f(a) \u2264 1\u2212 c.", "startOffset": 8, "endOffset": 14}, {"referenceID": 0, "context": "Let f : [0, 1] \u2192 [0, 1] be a nondecreasing function such that \u22030 < c < 1, \u22000 < a \u2264 1 \u22000 \u2264 b \u2264 23a, f(b) f(a) \u2264 1\u2212 c.", "startOffset": 17, "endOffset": 23}, {"referenceID": 0, "context": "When d = 1, the decision boundary g\u2217 becomes a point in [0, 1], and the corresponding classifier is a threshold function over [0,1].", "startOffset": 56, "endOffset": 62}, {"referenceID": 0, "context": "When d = 1, the decision boundary g\u2217 becomes a point in [0, 1], and the corresponding classifier is a threshold function over [0,1].", "startOffset": 126, "endOffset": 131}, {"referenceID": 0, "context": "In other words the hypothesis space becomes H = {f\u03b8(x) = 1 [x > \u03b8] : \u03b8 \u2208 [0, 1]}).", "startOffset": 73, "endOffset": 79}, {"referenceID": 0, "context": "We denote the ground truth decision boundary by \u03b8\u2217 \u2208 [0, 1].", "startOffset": 53, "endOffset": 59}, {"referenceID": 0, "context": "We want to find a \u03b8\u0302 \u2208 [0, 1] such that |\u03b8\u0302 \u2212 \u03b8\u2217| is small while making as few queries as possible.", "startOffset": 23, "endOffset": 29}, {"referenceID": 0, "context": "Algorithm 1 The active learning algorithm for learning thresholds 1: Input: \u03b4, \u01eb 2: [L0, R0] \u2190 [0, 1] 3: for k = 0, 1, 2, .", "startOffset": 95, "endOffset": 101}, {"referenceID": 23, "context": "The lower bound in [24] can be easily generalized to Condition 2: Theorem 4.", "startOffset": 19, "endOffset": 23}, {"referenceID": 23, "context": "([24]) There is a universal constant \u03b40 \u2208 (0, 1) and a labeler L satisfying Conditions 1 and 2, such that for any active learning algorithm A, there is a \u03b8\u2217 \u2208 [0, 1], such that for small enough \u01eb, \u039b(\u01eb, \u03b40,A, L, \u03b8\u2217) \u2265 \u03a9 ( 1 f(\u01eb)\u01eb \u22122\u03b2 )", "startOffset": 1, "endOffset": 5}, {"referenceID": 0, "context": "([24]) There is a universal constant \u03b40 \u2208 (0, 1) and a labeler L satisfying Conditions 1 and 2, such that for any active learning algorithm A, there is a \u03b8\u2217 \u2208 [0, 1], such that for small enough \u01eb, \u039b(\u01eb, \u03b40,A, L, \u03b8\u2217) \u2265 \u03a9 ( 1 f(\u01eb)\u01eb \u22122\u03b2 )", "startOffset": 159, "endOffset": 165}, {"referenceID": 0, "context": "There is a universal constant \u03b40 \u2208 (0, 1) and a labeler L satisfying Conditions 1, 2, and 3 with f(x) = C\u2032x\u03b1 (C\u2032 > 0 and 0 < \u03b1 \u2264 2 are constants), such that for any active learning algorithm A, there is a \u03b8\u2217 \u2208 [0, 1], such that for small enough \u01eb, \u039b(\u01eb, \u03b40,A, L, \u03b8\u2217) \u2265 \u03a9 (\u01eb\u2212\u03b1).", "startOffset": 210, "endOffset": 216}, {"referenceID": 5, "context": "4 The multidimensional case We follow [6] to generalize the results from one-dimensional thresholds to the d-dimensional (d > 1) smooth boundary fragment class \u03a3(K, \u03b3).", "startOffset": 38, "endOffset": 41}, {"referenceID": 0, "context": "2 Algorithm and Analysis Recall the decision boundary of the smooth boundary fragment class can be seen as the epigraph of a smooth function [0, 1]d\u22121 \u2192 [0, 1].", "startOffset": 141, "endOffset": 147}, {"referenceID": 0, "context": "2 Algorithm and Analysis Recall the decision boundary of the smooth boundary fragment class can be seen as the epigraph of a smooth function [0, 1]d\u22121 \u2192 [0, 1].", "startOffset": 153, "endOffset": 159}, {"referenceID": 0, "context": "References [1] M.", "startOffset": 11, "endOffset": 14}, {"referenceID": 1, "context": "[2] Maria-Florina Balcan, Alina Beygelzimer, and John Langford.", "startOffset": 0, "endOffset": 3}, {"referenceID": 2, "context": "[3] Maria-Florina Balcan and Steve Hanneke.", "startOffset": 0, "endOffset": 3}, {"referenceID": 3, "context": "[4] A.", "startOffset": 0, "endOffset": 3}, {"referenceID": 4, "context": "[5] Alina Beygelzimer, Daniel Hsu, John Langford, and Chicheng Zhang.", "startOffset": 0, "endOffset": 3}, {"referenceID": 5, "context": "[6] Rui M.", "startOffset": 0, "endOffset": 3}, {"referenceID": 6, "context": "[7] Yuxin Chen, S Hamed Hassani, Amin Karbasi, and Andreas Krause.", "startOffset": 0, "endOffset": 3}, {"referenceID": 7, "context": "[8] D.", "startOffset": 0, "endOffset": 3}, {"referenceID": 8, "context": "[9] S.", "startOffset": 0, "endOffset": 3}, {"referenceID": 9, "context": "[10] S.", "startOffset": 0, "endOffset": 4}, {"referenceID": 10, "context": "[11] Meng Fang and Xingquan Zhu.", "startOffset": 0, "endOffset": 4}, {"referenceID": 11, "context": "[12] Steve Hanneke.", "startOffset": 0, "endOffset": 4}, {"referenceID": 12, "context": "[13] Tibor Heged\u0171s.", "startOffset": 0, "endOffset": 4}, {"referenceID": 13, "context": "[14] M.", "startOffset": 0, "endOffset": 4}, {"referenceID": 14, "context": "[15] Christoph Kading, Alexander Freytag, Erik Rodner, Paul Bodesheim, and Joachim Denzler.", "startOffset": 0, "endOffset": 4}, {"referenceID": 15, "context": "[16] Yuan-Chuan Li and Cheh-Chih Yeh.", "startOffset": 0, "endOffset": 4}, {"referenceID": 16, "context": "[17] Stanislav Minsker.", "startOffset": 0, "endOffset": 4}, {"referenceID": 17, "context": "[18] Mohammad Naghshvar, Tara Javidi, and Kamalika Chaudhuri.", "startOffset": 0, "endOffset": 4}, {"referenceID": 18, "context": "[19] R.", "startOffset": 0, "endOffset": 4}, {"referenceID": 19, "context": "[20] Maxim Raginsky and Alexander Rakhlin.", "startOffset": 0, "endOffset": 4}, {"referenceID": 20, "context": "[21] Aaditya Ramdas and Akshay Balsubramani.", "startOffset": 0, "endOffset": 4}, {"referenceID": 21, "context": "[22] A.", "startOffset": 0, "endOffset": 4}, {"referenceID": 22, "context": "[23] Ruth Urner, Shai Ben-david, and Ohad Shamir.", "startOffset": 0, "endOffset": 4}, {"referenceID": 23, "context": "[24] Songbai Yan, Kamalika Chaudhuri, and Tara Javidi.", "startOffset": 0, "endOffset": 4}, {"referenceID": 24, "context": "[25] Chicheng Zhang and Kamalika Chaudhuri.", "startOffset": 0, "endOffset": 4}, {"referenceID": 12, "context": "Let \u03b4 \u2208 [0, 13 ], N \u2265 \u03be \u01eb2 ln 1 \u03b4 [ln ln]+ 1 \u01eb (\u03be is an absolute constant specified in the proof).", "startOffset": 8, "endOffset": 16}, {"referenceID": 0, "context": ", log 1 2\u01eb \u2212 1, define Qk = { (p, q) : p, q \u2208 Q \u2229 [0, 1] and q \u2212 p = ( 3 4 k }", "startOffset": 50, "endOffset": 56}, {"referenceID": 5, "context": "= O ( M\u2212\u03b3\u2212d+1 ) The second equality follows from Lemma 3 of [6] that |gq(x\u0303)\u2212 g\u2217(x\u0303)| = O (M\u2212\u03b3) since g\u2217 is \u03b3-H\u00f6lder smooth.", "startOffset": 60, "endOffset": 63}, {"referenceID": 0, "context": "We take \u0398 to be [0, 1], and d(\u03b81, \u03b82) = |\u03b81 \u2212 \u03b82| in Lemma 5.", "startOffset": 16, "endOffset": 22}, {"referenceID": 0, "context": "\u2264n max x\u2208[0,1] dKL ( PLk(Y | x) \u2016 P\u0304L(Y | x) )", "startOffset": 9, "endOffset": 14}, {"referenceID": 0, "context": "For any k \u2208 {1, 2, 3}, x \u2208 [0, 1], P\u0304L(\u00b7 | x) \u2265 PL0(\u00b7 | x) + PLk(\u00b7 | x) 4 (3) For any k \u2208 {0, 1, 2, 3}, x \u2208 [0, 1], y \u2208 {1,\u22121,\u22a5} 18", "startOffset": 27, "endOffset": 33}, {"referenceID": 0, "context": "For any k \u2208 {1, 2, 3}, x \u2208 [0, 1], P\u0304L(\u00b7 | x) \u2265 PL0(\u00b7 | x) + PLk(\u00b7 | x) 4 (3) For any k \u2208 {0, 1, 2, 3}, x \u2208 [0, 1], y \u2208 {1,\u22121,\u22a5} 18", "startOffset": 108, "endOffset": 114}, {"referenceID": 5, "context": "We first construct {P\u03b8 : \u03b8 \u2208 \u0398} using a similar idea with [6], and then use Lemma 12 to select a subset \u0398\u0303 \u2282 \u0398 to apply Lemma 5.", "startOffset": 58, "endOffset": 61}, {"referenceID": 0, "context": "\u2264n max x\u2208[0,1]d dKL ( P L \u03c9 (i) (Y | x) \u2016 P L \u03c9 (j) (Y | x) )", "startOffset": 9, "endOffset": 14}, {"referenceID": 0, "context": "=n max x\u2208[0,1]d P L \u03c9 (i) (Y 6=\u22a5| x)dKL ( P L \u03c9 (i) (Y | x, Y 6=\u22a5) \u2016 P L \u03c9 (j) (Y | x, Y 6=\u22a5) )", "startOffset": 9, "endOffset": 14}, {"referenceID": 0, "context": "Therefore, dKL ( P i \u2016 P j ) \u2264 nf(A) max x\u2208[0,1]d dKL ( P L \u03c9 (i) (Y | x, Y 6=\u22a5) \u2016 P L \u03c9 (j) (Y | x, Y 6=\u22a5) )", "startOffset": 43, "endOffset": 48}, {"referenceID": 0, "context": "Apply Lemma 10 to P L \u03c9 (i) (Y | x, Y 6=\u22a5) and P L \u03c9 (i) (Y | x, Y 6=\u22a5), and noting they are bounded above by a constant, we have max x\u2208[0,1]d dKL ( P L \u03c9 (i) (Y | x, Y 6=\u22a5) \u2016 P L \u03c9 (j) (Y | x, Y 6=\u22a5) )", "startOffset": 136, "endOffset": 141}, {"referenceID": 0, "context": "By the construction of g, for any x \u2208 [0, 1], any \u03c9 \u2208 \u03a9, PL\u03c9 (\u00b7 | x) equals either P+(\u00b7 | x) or P\u2212(\u00b7 | x).", "startOffset": 38, "endOffset": 44}, {"referenceID": 0, "context": "By the well-balanced property, for any x \u2208 [0, 1], P\u0304L(\u00b7 | x) is between 1 24P+(\u00b7 | x) + 23 24P\u2212(\u00b7 | x) and 3 24P+(\u00b7 | x) + 21 24P\u2212(\u00b7 | x).", "startOffset": 43, "endOffset": 49}, {"referenceID": 0, "context": "For any 0 < i \u2264 M , dKL ( P i \u2016 P\u0304 0 ) \u2264 nmax x\u2208[0,1]d dKL ( PLi(Y | x) \u2016 P\u0304L(Y | x) ) .", "startOffset": 48, "endOffset": 53}, {"referenceID": 0, "context": "For any x \u2208 [0, 1], dKL ( PLi(Y | x) \u2016 P\u0304L(Y | x) )", "startOffset": 12, "endOffset": 18}, {"referenceID": 20, "context": "We need following two results from [21] Lemma 6.", "startOffset": 35, "endOffset": 39}, {"referenceID": 20, "context": "([21], Theorem 2) Take any 0 < \u03b4 < 1.", "startOffset": 1, "endOffset": 5}, {"referenceID": 20, "context": "([21], Lemma 3) Take any 0 < \u03b4 < 1.", "startOffset": 1, "endOffset": 5}, {"referenceID": 0, "context": "If x \u2208 [0, 1], \u01eb \u2264 min { ( 1 2 1/\u03b2 , ( 4 5 1/\u03b1 , 1 4 }", "startOffset": 7, "endOffset": 13}, {"referenceID": 19, "context": "([20], Lemma 4) For sufficiently large d > 0, there is a subset M \u2282 {0, 1} with following properties: (i) |M | \u2265 2; (ii) \u2016v \u2212 v\u20160 > d 12 for any two distinct v, v\u2032 \u2208 M ; (iii) for any i = 1, .", "startOffset": 1, "endOffset": 5}, {"referenceID": 15, "context": "One proof can be found in [16].", "startOffset": 26, "endOffset": 30}], "year": 2016, "abstractText": "We study active learning where the labeler can not only return incorrect labels but also abstain from labeling. We consider different noise and abstention conditions of the labeler. We propose an algorithm which utilizes abstention responses, and analyze its statistical consistency and query complexity under fairly natural assumptions on the noise and abstention rate of the labeler. This algorithm is adaptive in a sense that it can automatically request less queries with a more informed or less noisy labeler. We couple our algorithm with lower bounds to show that under some technical conditions, it achieves nearly optimal query complexity.", "creator": "gnuplot 4.6 patchlevel 4"}}}