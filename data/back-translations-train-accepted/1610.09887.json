{"id": "1610.09887", "review": {"conference": "ICML", "VERSION": "v1", "DATE_OF_SUBMISSION": "31-Oct-2016", "title": "Depth-Width Tradeoffs in Approximating Natural Functions with Neural Networks", "abstract": "We provide a depth-based separation result for feed-forward ReLU neural networks, showing that a wide family of non-linear, twice-differentiable functions on $[0,1]^d$, which can be approximated to accuracy $\\epsilon$ by ReLU networks of depth and width $\\mathcal{O}(\\text{poly}(\\log(1/\\epsilon)))$, cannot be approximated to similar accuracy by constant-depth ReLU networks, unless their width is at least $\\Omega(1/\\epsilon)$.", "histories": [["v1", "Mon, 31 Oct 2016 12:08:46 GMT  (16kb)", "http://arxiv.org/abs/1610.09887v1", null], ["v2", "Thu, 9 Mar 2017 18:07:37 GMT  (75kb,D)", "http://arxiv.org/abs/1610.09887v2", null]], "reviews": [], "SUBJECTS": "cs.LG cs.NE stat.ML", "authors": ["itay safran", "ohad shamir"], "accepted": true, "id": "1610.09887"}, "pdf": {"name": "1610.09887.pdf", "metadata": {"source": "CRF", "title": "Depth Separation in ReLU Networks for Approximating Smooth Non-Linear Functions", "authors": ["Itay Safran"], "emails": ["itay.safran@weizmann.ac.il", "ohad.shamir@weizmann.ac.il"], "sections": [{"heading": null, "text": "ar Xiv: 161 0.09 887v 1 [cs.L G] 31 Oct 2"}, {"heading": "1 Introduction", "text": "This year it will be so far that it will be able to use the mentionlcihsrcsrteeSe rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rfu the rfu the rfu the rfu the r"}, {"heading": "2 Preliminaries", "text": "To formalize our setup, remember that a fully networked artificial neural network calculates a function Rd \u2192 R and is composed of neurons connected according to a directed acyclic graph. Specifically, the neurons can be broken down into layers in which the output of each neuron is connected to all neurons of the following layer and to them alone. We focus on ReLU networks in which each neuron is a function of the form x 7 \u2192 [w x + b] + where w is a weight vector, b is a bias term specific to that neuron, and [z] + = max {0, z} is the ReLU activation function. For a vector b = (b1,., bn) and a matrix W = (w1, w2,.) the bias layer is specific to that neuron and [Wx + b] is an abbreviation of the norm for ([w x + 1]."}, {"heading": "3 Lower Bounds on the Accuracy of Approximating C2 Functions", "text": "We first establish that such a limit can be trivial if we take into account cases in which the function that approximates approximately approximately approximately approximately approximately approximately approximately approximately approximately approximately approximately approximately approximately approximately approximately approximately approximately approximately approximately approximately approximately approximately approximately approximately approximately approximately approximately approximately approximately approximately approximately approximately approximately approximately approximately approximately approximately approximately approximately approximately approximately approximately approximately approximately approximately approximately approximately approximately approximately approximately approximately approximately approximately approximately approximately approximately approximately approximately approximately approximately approximately approximately approximately approximately approximately approximately approximately approximately approximately approximately approximately approximately approximately approximately approximately approximately approximately approximately approximately approximately approximately approximately approximately approximately approximately approximately approximately approximately approximately approximately approximately approximately approximately approximately approximately approximately approximately approximately approximately approximately approximately approximately approximately approximately approximately approximately approximately approximately approximately approximately approximately approximately approximately approximately approximately approximately approximately approximately approximately approximately approximately approximately approximately approximately approximately approximately approximately approximately approximately approximately approximately approximately approximately approximately approximately approximately approximately approximately approximately approximately approximately approximately approximately approximately approximately approximately approximately approximately approximately approximately approximately approximately approximately approximately approximately approximately approximately approximately approximately approximately approximately approximately approximately approximately approximately approximately approximately approximately approximately approximately approximately approximately approximately approximately approximately approximately approximately approximately approximately approximately approximately approximately approximately approximately approximately approximately approximately approximately approximately approximately approximately approximately approximately approximately approximately approximately approximately approximately approximately approximately approximately approximately approximately approximately approximately approximately approximately approximately approximately approximately approximately approximately approximately approximately approximately approximately approximately approximately approximately approximately approximately approximately approximately approximately approximately approximately approximately approximately approximately approximately approximately approximately approximately approximately approximately approximately approximately approximately approximately approximately approximately approximately approximately approximately approximately approximately approximately approximately approximately approximately) approximately approximately approximately approximately approximately approximately approximately approximately approximately approximately approximately approximately approximately approximately approximately approximately approximately approximately approximately approximately approximately approximately approximately approximately approximately approximately approximately approximately approximately approximately approximately approximately approximately approximately approximately approximately approximately approximately approximately approximately approximately approximately approximately approximately approximately approximately approximately approximately approximately approximately approximately approximately approximately approximately approximately approximately approximately approximately approximately approximately approximately approximately approximately approximately approximately approximately approximately approximately approximately approximately approximately approximately approximately approximately approximately approximately approximately approximately approximately approximately approximately approximately approximately approximately approximately approximately approximately approximately approximately approximately approximately approximately approximately approximately approximately approximately approximately approximately approximately approximately approximately approximately approximately approximately approximately approximately approximately approximately approximately approximately approximately approximately approximately approximately approximately approximately approximately approximately approximately approximately approximately approximately approximately approximately approximately approximately approximately approximately approximately approximately approximately approximately approximately approximately approximately approximately approximately approximately approximately approximately approximately approximately approximately approximately approximately approximately approximately approximately approximately approximately approximately approximately approximately approximately approximately approximately approximately approximately approximately approximately approximately approximately approximately approximately approximately approximately approximately approximately approximately approximately approximately approximately approximately approximately approximately approximately approximately approximately approximately approximately approximately approximately approximately approximately approximately approximately approximately approximately approximately approximately approximately approximately approximately approximately approximately approximately approximately approximately approximately approximately approximately approximately approximately approximately approximately approximately approximately to approximately approximately to approximately approximately approximately approximately approximately to approximately to approximately to approximately approximately approximately approximately to approximately approximately to approximately to approximately approximately approximately"}, {"heading": "3.1 Some Technical Tools", "text": "Definition 2. Let us leave Pi to stand for the ith Legendary Polynomial given by Rodrigues' formula: Pi (x) = 12ii! didxi [(x2 \u2212 1) i].Since we are interested in approximations in small intervals where the approximate function is linear, we use the variable change x = 2t \u2212 2% a \u2212 1 to obtain an orthogonal family {P \u0447i}. Since we are interested in approximations in small intervals where the approximate function is linear, we use the variable change x = 2t \u2212 2% a \u2212 1 to obtain an orthogonal family {P \u044bi}."}, {"heading": "3.2 One-dimensional Lower Bounds", "text": "We begin by pointing out two useful lemmas; the first allows us to calculate the error of a linear approach to a one-dimensional function (1), and the second allows us to infer the total approximation, from the lower limits we have at small intervals where the approximate function is more linear. (2) Then the error of the optimal linear approach to the f-dimensional approximation of f is fulfilled at the interval level [a, a + 2]. (2) The best linear approach to the L2 = 2a 2a 2i 2i 2i. (4) Proof. A standard result of the Legendre approximation is that each function f \u2212 a +. (a), the best linear approximation (w.r.t. of the L2 norm) is given by Pf = a 0P 1 (x)."}, {"heading": "3.3 Multi-dimensional Lower Bounds", "text": "We now move to the generalization of the boundaries in the previous subsection on the general dimension d. & \u2212 u > u > This allows us to follow the detection of Thm. \u2212 \u2212 u analogous to the proof of Thm. \u2212 u \u2212 5. we identify a neighborhood of f in which the restriction of f to a line in a certain direction is nonlinear. We then integrate along all lines in this direction and use the result of Thm. \u2212 5. Before we can prove the theory, we must state that there is in fact a set that has a strictly positive unit of measurement in which f exhibits a strong curvature along a certain direction. If we assume that f is not piecemeal linear; namely, we have a certain x0, 1, so that H (f) (x0) 6 = 0. Since H (f) is continuous, we have the function hv (x) = v H (x) v) v) v, and there is a direction v Rd in which has no great effect."}, {"heading": "4 Efficiently Approximating Functions with Small Representations", "text": "In this section we show that it is possible to define a broad family of functions using ReLU neural networks, where the error is reduced exponentially with depth. \u2212 k The key result to show this is this: The multiplication of two (limited) numbers can be approximated by a neural network, with an error decaying exponentially with depth: [\u2212 M] 2 \u2192 R, f (x, y) = x \u00b7 y and let > 0 (bounded size) numbers can be approximated by a neural network, with error decaying exponentially with depth: Theorem 6. Let f, M] 2 \u2192 R, f (x, y) = x \u00b7 y and let > 0 be arbitrary. Then there is a ReLU neural network g of width 4 log (M)."}, {"heading": "5 The Formal Separation Result", "text": "With the previous results in hand, we are finally ready to present our main result, which formally shows how depth can be exponentially more valuable than width as a function of target accuracy: sequence 3. Let us suppose f-C2-Ft (B), M-Ft (B), where t (B) = O (P) (P) and M (B) = O (P) (P). Then let us approach the accuracy, where c, p are a constant and a polynomial, where a fixed depth ReLU network requires at least width, whereas there is a ReLU network with depth and width of no more than p (P), which approaches f the accuracy in the infinity standard, where c, p are a constant and a polynomial, respectively, depend exclusively on f. The proof for corollar3 follows directly from corollar2 and thm 7. While the lower limit is simple, for the upper limit it must be noted that a constant and a polynomial are both a polynomial and a polynomial."}], "references": [{"title": "On the expressive power of deep learning: A tensor analysis", "author": ["Nadav Cohen", "Or Sharir", "Amnon Shashua"], "venue": "In 29th Annual Conference on Learning Theory,", "citeRegEx": "Cohen et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Cohen et al\\.", "year": 2016}, {"title": "Approximation by superpositions of a sigmoidal function", "author": ["George Cybenko"], "venue": "Mathematics of control, signals and systems,", "citeRegEx": "Cybenko.,? \\Q1989\\E", "shortCiteRegEx": "Cybenko.", "year": 1989}, {"title": "The power of depth for feedforward neural networks", "author": ["Ronen Eldan", "Ohad Shamir"], "venue": "In 29th Annual Conference on Learning Theory,", "citeRegEx": "Eldan and Shamir.,? \\Q2016\\E", "shortCiteRegEx": "Eldan and Shamir.", "year": 2016}, {"title": "Deep residual learning for image recognition", "author": ["Kaiming He", "Xiangyu Zhang", "Shaoqing Ren", "Jian Sun"], "venue": "CoRR, abs/1512.03385,", "citeRegEx": "He et al\\.,? \\Q2015\\E", "shortCiteRegEx": "He et al\\.", "year": 2015}, {"title": "benefits of depth in neural networks", "author": ["Matus Telgarsky"], "venue": "In 29th Annual Conference on Learning Theory,", "citeRegEx": "Telgarsky.,? \\Q2016\\E", "shortCiteRegEx": "Telgarsky.", "year": 2016}], "referenceMentions": [{"referenceID": 1, "context": "It is well-known that networks of depth 2 can already approximate any continuous target function on the boolean hypercube [0, 1] to arbitrary accuracy, albeit at the cost of width exponential in the dimension d [Cybenko, 1989].", "startOffset": 211, "endOffset": 226}, {"referenceID": 3, "context": "Indeed, recent empirical evidence suggests that even at large depths, deeper networks can offer benefits over shallower networks [He et al., 2015].", "startOffset": 129, "endOffset": 146}, {"referenceID": 2, "context": "In Eldan and Shamir [2016], the authors show that depth in neural networks can be exponentially valuable even if increased only by 1, by proving the existence of a family of radial functions which are expressible using a depth 3 feed-forward neural network of size polynomial in the dimension d, whereas a network of depth 2 cannot approximate these functions to more than constant accuracy (even if the activation function is arbitrary), unless its width is exponential in d.", "startOffset": 3, "endOffset": 27}, {"referenceID": 2, "context": "In Eldan and Shamir [2016], the authors show that depth in neural networks can be exponentially valuable even if increased only by 1, by proving the existence of a family of radial functions which are expressible using a depth 3 feed-forward neural network of size polynomial in the dimension d, whereas a network of depth 2 cannot approximate these functions to more than constant accuracy (even if the activation function is arbitrary), unless its width is exponential in d. However, it is not clear whether their proof techniques extend to deeper networks. In contrast, our results apply to potentially deeper networks, and the separation is in terms of the required accuracy \u01eb, rather than some fixed depths. The work perhaps most similar to ours is Telgarsky [2016]. In that work, the author proves a separation result between networks of depth k and depth o (", "startOffset": 3, "endOffset": 771}, {"referenceID": 0, "context": "Finally, in Cohen et al. [2016], the authors establish a depth separation result for networks with a certain tensor structure.", "startOffset": 12, "endOffset": 32}, {"referenceID": 4, "context": "To translate this result to the context of ReLU neural networks, we use the result in Telgarsky [2016, Lemma 3.2], of which the following is an immediate corollary. Corollary 1. Let N d m,l denote the family of ReLU neural networks receiving input of dimension d and having depth l and maximal width m. Then N d m,l \u2286 G d (2m) . The following corollary bears great resemblance to the lower bound provided in Telgarsky [2016]. Albeit lower bounding the accuracy to which a ReLU network of a given size can approximate a certain function, rather than indicating what is a lower bound on the minimal size required for achieving non-constant approximation error.", "startOffset": 86, "endOffset": 425}, {"referenceID": 4, "context": "In Telgarsky [2016], the author demonstrates how the composition of the function \u03c6 (x) = [2x]+ \u2212 [4x\u2212 2]+ with itself i times, \u03c6i, yields a highly oscillatory triangle wave function in the domain [0, 1].", "startOffset": 3, "endOffset": 20}], "year": 2017, "abstractText": "We provide a depth-based separation result for feed-forward ReLU neural networks, showing that a wide family of non-linear, twice-differentiable functions on [0, 1]d, which can be approximated to accuracy \u01eb by ReLU networks of depth and width O(poly (log(1/\u01eb))), cannot be approximated to similar accuracy by constant-depth ReLU networks, unless their width is at least \u03a9(1/\u01eb).", "creator": "LaTeX with hyperref package"}}}