{"id": "1610.09893", "review": {"conference": "NIPS", "VERSION": "v1", "DATE_OF_SUBMISSION": "31-Oct-2016", "title": "LightRNN: Memory and Computation-Efficient Recurrent Neural Networks", "abstract": "Recurrent neural networks (RNNs) have achieved state-of-the-art performances in many natural language processing tasks, such as language modeling and machine translation. However, when the vocabulary is large, the RNN model will become very big (e.g., possibly beyond the memory capacity of a GPU device) and its training will become very inefficient. In this work, we propose a novel technique to tackle this challenge. The key idea is to use 2-Component (2C) shared embedding for word representations. We allocate every word in the vocabulary into a table, each row of which is associated with a vector, and each column associated with another vector. Depending on its position in the table, a word is jointly represented by two components: a row vector and a column vector. Since the words in the same row share the row vector and the words in the same column share the column vector, we only need $2 \\sqrt{|V|}$ vectors to represent a vocabulary of $|V|$ unique words, which are far less than the $|V|$ vectors required by existing approaches. Based on the 2-Component shared embedding, we design a new RNN algorithm and evaluate it using the language modeling task on several benchmark datasets. The results show that our algorithm significantly reduces the model size and speeds up the training process, without sacrifice of accuracy (it achieves similar, if not better, perplexity as compared to state-of-the-art language models). Remarkably, on the One-Billion-Word benchmark Dataset, our algorithm achieves comparable perplexity to previous language models, whilst reducing the model size by a factor of 40-100, and speeding up the training process by a factor of 2. We name our proposed algorithm \\emph{LightRNN} to reflect its very small model size and very high training speed.", "histories": [["v1", "Mon, 31 Oct 2016 12:24:13 GMT  (574kb,D)", "http://arxiv.org/abs/1610.09893v1", "NIPS 2016"]], "COMMENTS": "NIPS 2016", "reviews": [], "SUBJECTS": "cs.CL cs.LG", "authors": ["xiang li", "tao qin", "jian yang", "xiaolin hu", "tie-yan liu"], "accepted": true, "id": "1610.09893"}, "pdf": {"name": "1610.09893.pdf", "metadata": {"source": "CRF", "title": "LightRNN: Memory and Computation-Efficient Recurrent Neural Networks", "authors": ["Xiang Li", "Tao Qin", "Jian Yang", "Tie-Yan Liu"], "emails": ["implusdream@gmail.com", "csjyang@njust.edu.cn", "tie-yan.liu}@microsoft.com"], "sections": [{"heading": null, "text": "The results show that our algorithm significantly reduces the model size and accelerates the training process without sacrificing accuracy (it achieves a similar if not better perplexity compared to modern speech models). Remarkably, our algorithm on the One Billion Word Benchmark dataset achieves a similar perplexity as previous speech models, while reducing the model size by a factor of 40-100 and the training process by a factor of 2. We name our proposed algorithm LightRNN to reflect its very small model size and very high training speed."}, {"heading": "1 Introduction", "text": "It is the only way in which we are in the manner and manner, in which we are in the manner and manner, in which we are in the manner and manner, in which we are in the manner and manner, in which we are in the manner and manner, in which we are in the manner and manner, in the language, in the language, in the language, in the language, in the language, in the language, in the language, in the language, in the language, in the language, in the language, in the language, in the language, in the language, in the language, in the language, in the language, in the language, in the language, in the language, in the language, in the language, in the language, in the language, in the language, in the language, in the language, in the language, in the language, in the language, in the language, in the language, in the language, in the language, in the language, in the language, in the language, in the language, in the language, in the language, in the language, in the language, in the language, in the language, in the language of the language, in the language, in the language, in the language, in the language of the language, in the language, in the language, in the language of the language, in the language, in the language, in the language of the language, in the language, in the language of the language, in the language, in the language, in the language of the language, in the language, in the language, in the language of the language, in the language, in the language, in the language of the language, in the language, in the language of the language, in the language, in the language of the language, in the language of the language, in the language of the language, in the language of the language, in the language, in the language of the language, in the language of the language, in the language of the language, in the language of the language, in the language of the language, in the language of the language, in the language of the language, in the language of the language, in the language of the language, in the language of the language, in the language, in the language of the language, in the language, in the language of the language, in the language of the language, in the language of the language, in the language of"}, {"heading": "2 Related work", "text": "In the literature of deep learning there are several papers that try to solve the problem caused by the large vocabulary of the text corpus. Some papers focus on reducing the computational complexity of the Softmax operation on the output bedding matrix. However, in [16, 17] a binary tree is used to represent a hierarchical clustering in the vocabulary. Each leaf node of the tree is associated with a word, and each word has a unique path from the root to the leaf in which it is located. In this way, one can replace the probability of the next word."}, {"heading": "3 LightRNN", "text": "In this section we present our proposed LightRNN algorithm."}, {"heading": "3.1 RNN Model with 2-Component Shared Embedding", "text": "It is the same case for the output word embedding.Figure 2: LightRNN (left) vs. RNN (right).With the 2-component model we can use the output word embedding.Figure 2: LightRNN (left) vs. RNN (right).It is the same case for the output word embedding.Figure 2: LightRNN (left) vs. RNN (left) vs. RNN (left) vs. RNN (right)."}, {"heading": "3.2 Bootstrap for Word Allocation", "text": "There remains a problem with how to create this table, i.e. how to embed the words in the corresponding columns and rows: (1) In this subsection we will include the words in the table (2). (2) Train the input / output embedding vectors in LightRNN based on the given allocation until a stop criterion for speech modeling is met, otherwise we proceed to the next steps. (3) Fixing the embedding vectors in the previous step, refine the allocation in the table to minimize the loss of speech modeling."}, {"heading": "4 Experiments", "text": "To test LightRNN, we conducted a series of experiments on the task of voice modeling."}, {"heading": "4.1 Settings", "text": "We use Perplexity (PPL) as a measure of the performance of a speech modeling algorithm (the lower, the better), defined as PPL = exp (NLLT), where T is the number of tokens in the test set. We used all the linguistic corpora from the 2013 ACL Workshop Morphological Language Datasets (ACLW) [4] and the One Billion Word Benchmark Dataset (BillionW) [5] in our experiments. The detailed information of these public datasets is listed in Table 1: Statistics of Datasets # Token Vocabulary Size ACLW-Spanish 56M 152K ACLW-French 57M 137K ACLW-English 20M 60K ACLW-Czech 17M 206K ACLW-German 51M 339K ACLW-Russian 25M 497K / 793KGB."}, {"heading": "4.2 Results and Discussions", "text": "This year, as never before in the history of a country in which it is a country in which it is a country in which it is a country in which it is a country, a country in which it is a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country"}, {"heading": "5 Conclusion and future work", "text": "In this work, we have proposed a novel algorithm, LightRNN, for natural language processing tasks. By using 2-component shared embedding for text representation, LightRNN achieves high efficiency in terms of both model size and runtime, especially for text corpora with large vocabulary. There are many directions to explore in the future. First, we plan to apply LightRNN to even larger corporations, such as the ClueWeb dataset, for which conventional RNN models cannot fit into a modern GPU. Second, we will apply LightRNN to other NLP tasks such as machine translation and answering questions. Third, we will explore k-component shared embedding (k > 2) and examine the role of k in the trade-off between efficiency and effectiveness. Fourth, we will clean our codes and release them soon by CNTK [27]."}, {"heading": "Acknowledgments", "text": "The authors thank the anonymous reviewers for their critical and constructive comments and suggestions. This work was partially supported by the National Science Fund of China under grant numbers 91420201, 61472187, 61502235, 61233011 and 61373063, the key project of the Chinese Ministry of Education under grant numbers 313030, the 973 program no. 2014CB349303 and the program for Changjiang scholars and innovative research teams at the university. We would also like to thank Professor Xiaolin Hu from the Department of Computer Science and Technology of the Tsinghua National Laboratory for Information Science and Technology (TNList) for many wonderful pieces of advice."}], "references": [{"title": "Optimizing performance of recurrent neural networks on gpus", "author": ["Jeremy Appleyard", "Tomas Kocisky", "Phil Blunsom"], "venue": "arXiv preprint arXiv:1604.01946,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2016}, {"title": "Quick training of probabilistic neural nets by importance sampling", "author": ["Yoshua Bengio", "Jean-S\u00e9bastien Sen\u00e9cal"], "venue": "In AISTATS,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2003}, {"title": "Compositional morphology for word representations and language modelling", "author": ["Jan A Botha", "Phil Blunsom"], "venue": "arXiv preprint arXiv:1405.4273,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2014}, {"title": "One billion word benchmark for measuring progress in statistical language modeling", "author": ["Ciprian Chelba", "Tomas Mikolov", "Mike Schuster", "Qi Ge", "Thorsten Brants", "Phillipp Koehn", "Tony Robinson"], "venue": "arXiv preprint arXiv:1312.3005,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2013}, {"title": "Strategies for training large vocabulary neural language models", "author": ["Welin Chen", "David Grangier", "Michael Auli"], "venue": "arXiv preprint arXiv:1512.04906,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2015}, {"title": "Empirical evaluation of gated recurrent neural networks on sequence modeling", "author": ["Junyoung Chung", "Caglar Gulcehre", "KyungHyun Cho", "Yoshua Bengio"], "venue": "arXiv preprint arXiv:1412.3555,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2014}, {"title": "Learning to forget: Continual prediction with lstm", "author": ["Felix A Gers", "J\u00fcrgen Schmidhuber", "Fred Cummins"], "venue": "Neural computation,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2000}, {"title": "Classes for fast maximum entropy training", "author": ["Joshua Goodman"], "venue": "In Acoustics, Speech, and Signal Processing,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2001}, {"title": "Generating sequences with recurrent neural networks", "author": ["Alex Graves"], "venue": "arXiv preprint arXiv:1308.0850,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2013}, {"title": "Long short-term memory", "author": ["Sepp Hochreiter", "J\u00fcrgen Schmidhuber"], "venue": "Neural computation,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 1997}, {"title": "Blackout: Speeding up recurrent neural network language models with very large vocabularies", "author": ["Shihao Ji", "SVN Vishwanathan", "Nadathur Satish", "Michael J Anderson", "Pradeep Dubey"], "venue": "arXiv preprint arXiv:1511.06909,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2015}, {"title": "Character-aware neural language models", "author": ["Yoon Kim", "Yacine Jernite", "David Sontag", "Alexander M Rush"], "venue": "arXiv preprint arXiv:1508.06615,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2015}, {"title": "Recurrent neural network based language model", "author": ["Tomas Mikolov", "Martin Karafi\u00e1t", "Lukas Burget", "Jan Cernock\u1ef3", "Sanjeev Khudanpur"], "venue": "In INTERSPEECH,", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2010}, {"title": "Extensions of recurrent neural network language model", "author": ["Tom\u00e1\u0161 Mikolov", "Stefan Kombrink", "Luk\u00e1\u0161 Burget", "Jan Honza \u010cernock\u1ef3", "Sanjeev Khudanpur"], "venue": "In Acoustics, Speech and Signal Processing (ICASSP),", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2011}, {"title": "A scalable hierarchical distributed language model", "author": ["Andriy Mnih", "Geoffrey E Hinton"], "venue": "In Advances in neural information processing systems,", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2009}, {"title": "Hierarchical probabilistic neural network language model", "author": ["Frederic Morin", "Yoshua Bengio"], "venue": "In Aistats,", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2005}, {"title": "Combinatorial optimization: algorithms and complexity", "author": ["Christos H Papadimitriou", "Kenneth Steiglitz"], "venue": "Courier Corporation,", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 1982}, {"title": "Building a 70 billion word corpus of english from clueweb", "author": ["Jan Pomik\u00e1lek", "Milos Jakub\u00edcek", "Pavel Rychl\u1ef3"], "venue": "In LREC,", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2012}, {"title": "Linear time 1/2-approximation algorithm for maximum weighted matching in general graphs", "author": ["Robert Preis"], "venue": "In STACS", "citeRegEx": "20", "shortCiteRegEx": "20", "year": 1999}, {"title": "Long short-term memory based recurrent neural network architectures for large vocabulary speech recognition", "author": ["Ha\u015fim Sak", "Andrew Senior", "Fran\u00e7oise Beaufays"], "venue": "arXiv preprint arXiv:1402.1128,", "citeRegEx": "21", "shortCiteRegEx": "21", "year": 2014}, {"title": "Lstm neural networks for language modeling", "author": ["Martin Sundermeyer", "Ralf Schl\u00fcter", "Hermann Ney"], "venue": "In INTERSPEECH,", "citeRegEx": "22", "shortCiteRegEx": "22", "year": 2012}, {"title": "Sequence to sequence learning with neural networks", "author": ["Ilya Sutskever", "Oriol Vinyals", "Quoc V Le"], "venue": "In Advances in neural information processing systems,", "citeRegEx": "23", "shortCiteRegEx": "23", "year": 2014}, {"title": "Document modeling with gated recurrent neural network for sentiment classification", "author": ["Duyu Tang", "Bing Qin", "Ting Liu"], "venue": "In Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing,", "citeRegEx": "24", "shortCiteRegEx": "24", "year": 2015}, {"title": "Backpropagation through time: what it does and how to do it", "author": ["Paul J Werbos"], "venue": "Proceedings of the IEEE,", "citeRegEx": "25", "shortCiteRegEx": "25", "year": 1990}, {"title": "An introduction to computational networks and the computational network toolkit", "author": ["Dong Yu", "Adam Eversole", "Mike Seltzer", "Kaisheng Yao", "Zhiheng Huang", "Brian Guenter", "Oleksii Kuchaiev", "Yu Zhang", "Frank Seide", "Huaming Wang"], "venue": "Technical report,", "citeRegEx": "27", "shortCiteRegEx": "27", "year": 2014}, {"title": "Recurrent neural network regularization", "author": ["Wojciech Zaremba", "Ilya Sutskever", "Oriol Vinyals"], "venue": "arXiv preprint arXiv:1409.2329,", "citeRegEx": "28", "shortCiteRegEx": "28", "year": 2014}], "referenceMentions": [{"referenceID": 12, "context": "Recently recurrent neural networks (RNNs) have been used in many natural language processing (NLP) tasks, such as language modeling [14], machine translation [23], sentiment analysis [24], and question answering [26].", "startOffset": 132, "endOffset": 136}, {"referenceID": 21, "context": "Recently recurrent neural networks (RNNs) have been used in many natural language processing (NLP) tasks, such as language modeling [14], machine translation [23], sentiment analysis [24], and question answering [26].", "startOffset": 158, "endOffset": 162}, {"referenceID": 22, "context": "Recently recurrent neural networks (RNNs) have been used in many natural language processing (NLP) tasks, such as language modeling [14], machine translation [23], sentiment analysis [24], and question answering [26].", "startOffset": 183, "endOffset": 187}, {"referenceID": 6, "context": "A popular RNN architecture is long short-term memory (LSTM) [8, 11, 22], which can model long-term dependence and resolve the gradient-vanishing problem by using memory cells and gating functions.", "startOffset": 60, "endOffset": 71}, {"referenceID": 9, "context": "A popular RNN architecture is long short-term memory (LSTM) [8, 11, 22], which can model long-term dependence and resolve the gradient-vanishing problem by using memory cells and gating functions.", "startOffset": 60, "endOffset": 71}, {"referenceID": 20, "context": "A popular RNN architecture is long short-term memory (LSTM) [8, 11, 22], which can model long-term dependence and resolve the gradient-vanishing problem by using memory cells and gating functions.", "startOffset": 60, "endOffset": 71}, {"referenceID": 17, "context": "Take the ClueWeb dataset [19] as an example, whose vocabulary contains over 10M words.", "startOffset": 25, "endOffset": 29}, {"referenceID": 0, "context": "Further considering the output-embedding matrix and those weights between hidden layers, the RNN model will be larger than 80GB, which is far beyond the capability of the best GPU devices on the market [2].", "startOffset": 202, "endOffset": 205}, {"referenceID": 14, "context": "In [16, 17], a binary tree is used to represent a hierarchical clustering of words in the vocabulary.", "startOffset": 3, "endOffset": 11}, {"referenceID": 15, "context": "In [16, 17], a binary tree is used to represent a hierarchical clustering of words in the vocabulary.", "startOffset": 3, "endOffset": 11}, {"referenceID": 7, "context": "In [9, 15], the words in the vocabulary are organized into a tree with two layers: the root node has roughly \u221a |V | intermediate nodes, each of which also has roughly \u221a |V | leaf nodes.", "startOffset": 3, "endOffset": 10}, {"referenceID": 13, "context": "In [9, 15], the words in the vocabulary are organized into a tree with two layers: the root node has roughly \u221a |V | intermediate nodes, each of which also has roughly \u221a |V | leaf nodes.", "startOffset": 3, "endOffset": 10}, {"referenceID": 1, "context": "on sampling-based approximations intend to select randomly or heuristically a small subset of the output layer and estimate the gradient only from those samples, such as importance sampling [3] and BlackOut [12].", "startOffset": 190, "endOffset": 193}, {"referenceID": 10, "context": "on sampling-based approximations intend to select randomly or heuristically a small subset of the output layer and estimate the gradient only from those samples, such as importance sampling [3] and BlackOut [12].", "startOffset": 207, "endOffset": 211}, {"referenceID": 4, "context": "Techniques [6, 21] like differentiated softmax and recurrent projection are employed to reduce the size of the output-embedding matrix.", "startOffset": 11, "endOffset": 18}, {"referenceID": 19, "context": "Techniques [6, 21] like differentiated softmax and recurrent projection are employed to reduce the size of the output-embedding matrix.", "startOffset": 11, "endOffset": 18}, {"referenceID": 11, "context": "Character-level convolutional filters are used to shrink the size of the inputembedding matrix in [13].", "startOffset": 98, "endOffset": 102}, {"referenceID": 20, "context": "The cell of hidden state h can be implemented by a LSTM [22] or a gated recurrent unit (GRU) [7], and our idea works with any kind of recurrent unit.", "startOffset": 56, "endOffset": 60}, {"referenceID": 5, "context": "The cell of hidden state h can be implemented by a LSTM [22] or a gated recurrent unit (GRU) [7], and our idea works with any kind of recurrent unit.", "startOffset": 93, "endOffset": 96}, {"referenceID": 16, "context": "By defining a weighted bipartite graph G = (V, E) with V = (V, Sr\u00d7Sc), in which the weight of the edge in E connecting a node w \u2208 V and node (i, j) \u2208 Sr \u00d7Sc is l(w, i, j), we will see that the above optimization problem is equivalent to a standard minimum weight perfect matching problem [18] on graph G.", "startOffset": 288, "endOffset": 292}, {"referenceID": 18, "context": "Since the computational complexity of MCMF is O(|V |), which is still costly for a large vocabulary, we alternatively leverage a linear time (with respect to |E|) 1 2 -approximation algorithm [20] in our experiments whose computational complexity is O(|V | ).", "startOffset": 192, "endOffset": 196}, {"referenceID": 2, "context": "We used all the linguistic corpora from 2013 ACL Workshop Morphological Language Datasets (ACLW) [4] and the One-Billion-Word Benchmark Dataset (BillionW) [5] in our experiments.", "startOffset": 97, "endOffset": 100}, {"referenceID": 3, "context": "We used all the linguistic corpora from 2013 ACL Workshop Morphological Language Datasets (ACLW) [4] and the One-Billion-Word Benchmark Dataset (BillionW) [5] in our experiments.", "startOffset": 155, "endOffset": 158}, {"referenceID": 2, "context": "BillionW 799M 793K For the ACLW datasets, we kept all the training/validation/test sets exactly the same as those in [4, 13] by using their processed data 1.", "startOffset": 117, "endOffset": 124}, {"referenceID": 11, "context": "BillionW 799M 793K For the ACLW datasets, we kept all the training/validation/test sets exactly the same as those in [4, 13] by using their processed data 1.", "startOffset": 117, "endOffset": 124}, {"referenceID": 3, "context": "For the BillionW dataset, since the data2 are unprocessed, we processed the data according to the standard procedure as listed in [5]: We discarded all words with count below 3 and padded the sentence boundary markers <S>,<\\S>.", "startOffset": 130, "endOffset": 133}, {"referenceID": 3, "context": "Meanwhile, the partition of training/validation/test sets on BillionW was the same with public settings in [5] for fair comparisons.", "startOffset": 107, "endOffset": 110}, {"referenceID": 8, "context": "We trained LSTM-based LightRNN using stochastic gradient descent with truncated backpropagation through time [10, 25].", "startOffset": 109, "endOffset": 117}, {"referenceID": 23, "context": "We trained LSTM-based LightRNN using stochastic gradient descent with truncated backpropagation through time [10, 25].", "startOffset": 109, "endOffset": 117}, {"referenceID": 25, "context": "5 [28].", "startOffset": 2, "endOffset": 6}, {"referenceID": 11, "context": "For the ACLW datasets, we mainly compared LightRNN with two state-of-the-art LSTM RNN algorithms in [13]: one utilizes hierarchical softmax for word prediction (denoted as HSM), and the other one utilizes hierarchical softmax as well as character-level convolutional filters for input embedding (denoted as C-HSM).", "startOffset": 100, "endOffset": 104}, {"referenceID": 11, "context": "Note that 200 is exactly the word embedding size of HSM and C-HSM models used in [13].", "startOffset": 81, "endOffset": 85}, {"referenceID": 11, "context": "ACLW Method Runtime(hours) Reallocation/Training C-HSM[13] 168 \u2013 LightRNN 82 0.", "startOffset": 54, "endOffset": 58}, {"referenceID": 4, "context": "19% BillionW Method Runtime(hours) Reallocation/Training HSM[6] 168 \u2013 LightRNN 70 2.", "startOffset": 60, "endOffset": 63}, {"referenceID": 3, "context": "Method PPL #param KN[5] 68 2G HSM[6] 85 1.", "startOffset": 20, "endOffset": 23}, {"referenceID": 4, "context": "Method PPL #param KN[5] 68 2G HSM[6] 85 1.", "startOffset": 33, "endOffset": 36}, {"referenceID": 10, "context": "6G B-RNN[12] 68 4.", "startOffset": 8, "endOffset": 12}, {"referenceID": 4, "context": "1G LightRNN 66 41M KN + HSM[6] 56 \u2013 KN + B-RNN[12] 47 \u2013 KN + LightRNN 43 \u2013", "startOffset": 27, "endOffset": 30}, {"referenceID": 10, "context": "1G LightRNN 66 41M KN + HSM[6] 56 \u2013 KN + B-RNN[12] 47 \u2013 KN + LightRNN 43 \u2013", "startOffset": 46, "endOffset": 50}, {"referenceID": 10, "context": "For the BillionW dataset, we mainly compared with BlackOut for RNN [12] (B-RNN) which achieves the state-of-the-art result by interpolating with KN (Kneser-Ney) 5-gram.", "startOffset": 67, "endOffset": 71}, {"referenceID": 4, "context": "In addition, we compared with the HSM result reported in [6], which used 1024 dimensions for word embedding, but still has 40x more parameters than our model.", "startOffset": 57, "endOffset": 60}, {"referenceID": 2, "context": "PPL on ACLW test Method Spanish/#P French/#P English/#P Czech/#P German/#P Russian/#P KN[4] 219/\u2013 243/\u2013 291/\u2013 862/\u2013 463/\u2013 390/\u2013", "startOffset": 88, "endOffset": 91}, {"referenceID": 11, "context": "HSM[13] 186/61M 202/56M 236/25M 701/83M 347/137M 353/200M C-HSM[13] 169/48M 190/44M 216/20M 578/64M 305/104M 313/152M LightRNN 157/18M 176/17M 191/17M 558/18M 281/18M 288/19M", "startOffset": 3, "endOffset": 7}, {"referenceID": 11, "context": "HSM[13] 186/61M 202/56M 236/25M 701/83M 347/137M 353/200M C-HSM[13] 169/48M 190/44M 216/20M 578/64M 305/104M 313/152M LightRNN 157/18M 176/17M 191/17M 558/18M 281/18M 288/19M", "startOffset": 63, "endOffset": 67}, {"referenceID": 24, "context": "Fourth, we are cleaning our codes and will release them soon through CNTK [27].", "startOffset": 74, "endOffset": 78}], "year": 2016, "abstractText": "Recurrent neural networks (RNNs) have achieved state-of-the-art performances in many natural language processing tasks, such as language modeling and machine translation. However, when the vocabulary is large, the RNN model will become very big (e.g., possibly beyond the memory capacity of a GPU device) and its training will become very inefficient. In this work, we propose a novel technique to tackle this challenge. The key idea is to use 2-Component (2C) shared embedding for word representations. We allocate every word in the vocabulary into a table, each row of which is associated with a vector, and each column associated with another vector. Depending on its position in the table, a word is jointly represented by two components: a row vector and a column vector. Since the words in the same row share the row vector and the words in the same column share the column vector, we only need 2 \u221a |V | vectors to represent a vocabulary of |V | unique words, which are far less than the |V | vectors required by existing approaches. Based on the 2-Component shared embedding, we design a new RNN algorithm and evaluate it using the language modeling task on several benchmark datasets. The results show that our algorithm significantly reduces the model size and speeds up the training process, without sacrifice of accuracy (it achieves similar, if not better, perplexity as compared to state-of-the-art language models). Remarkably, on the One-Billion-Word benchmark Dataset, our algorithm achieves comparable perplexity to previous language models, whilst reducing the model size by a factor of 40-100, and speeding up the training process by a factor of 2. We name our proposed algorithm LightRNN to reflect its very small model size and very high training speed.", "creator": "LaTeX with hyperref package"}}}