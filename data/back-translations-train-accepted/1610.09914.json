{"id": "1610.09914", "review": {"conference": "EMNLP", "VERSION": "v1", "DATE_OF_SUBMISSION": "31-Oct-2016", "title": "Named Entity Recognition for Novel Types by Transfer Learning", "abstract": "In named entity recognition, we often don't have a large in-domain training corpus or a knowledge base with adequate coverage to train a model directly. In this paper, we propose a method where, given training data in a related domain with similar (but not identical) named entity (NE) types and a small amount of in-domain training data, we use transfer learning to learn a domain-specific NE model. That is, the novelty in the task setup is that we assume not just domain mismatch, but also label mismatch.", "histories": [["v1", "Mon, 31 Oct 2016 13:36:35 GMT  (53kb,D)", "http://arxiv.org/abs/1610.09914v1", null]], "reviews": [], "SUBJECTS": "cs.CL", "authors": ["lizhen qu", "gabriela ferraro", "liyuan zhou", "weiwei hou", "timothy baldwin"], "accepted": true, "id": "1610.09914"}, "pdf": {"name": "1610.09914.pdf", "metadata": {"source": "CRF", "title": "Named Entity Recognition for Novel Types by Transfer Learning", "authors": ["Lizhen Qu", "Gabriela Ferraro", "Liyuan Zhou", "Weiwei Hou", "Timothy Baldwin"], "emails": ["lizhen.qu@data61.csiro.au", "gabriela.ferraro@data61.csiro.au", "joe.zhou@data61.csiro.au", "houvivid2013@gmail.com,", "tb@ldwin.net"], "sections": [{"heading": "1 Introduction", "text": "There are two main approaches to detecting such structures: (i) constructing sequencer models such as the condition for detecting such structures, (i) detecting such structures, (i) detecting such structures, (i) detecting such structures, (i) detecting such structures, (i) detecting such structures, (i) detecting such structures, (i) detecting such structures, (i) (i), (i) \"i\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s."}, {"heading": "2 Related work", "text": "The main scenario in which transfer learning has been applied to the NERS is domain customization (Arnold et al., 2008; Maynard et al., 2001; Chiticariu et al., 2010), assuming that the Y label set is the same for both the source and target audiences, and that only the domain varies. However, in our case, both the domain and the label set differ between the datasets. Similar to our work, Kim et al. (2015) uses transfer learning to deal with different label distributions in the NERS datasets, using Canonical Correlation Analysis (CCA) to induce label representations and reduce the problem to domain customization, which supports two different label mappings: (i) to a coarse label label set determined by clusters of vector representations of the NE types combined with mention level predictions to form the best prediction of the target domain."}, {"heading": "3 Transfer Learning for NER", "text": "Our proposed TransInit approach consists of three steps: (1) we build a linear chain CRF functions. (2) We learn the correlation between source types NE types and targets NE types when we use a two-tiered neural network to build a neural network for target groups NE types. (3) We use the neural network to build a sequence of words and their labels. (3) We let (x, y) be a sequence of words and their labels. A linear chain CRF takes the form: 1Z L = 1 exp (Wff, x) + W gg (yl \u2212 1), where f (yl \u2212 1), where f (yl, x) is a function that depends only on x, and the function g (yl \u2212 1, yl)."}, {"heading": "4 Experimental Setup", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "4.1 Datasets", "text": "We use CADEC (Karimi et al., 2015) and I2B2 (Ben Abacha and Zweigenbaum, 2011) as target corpora with the standard training and test splits. We have 10% of each training set ready as development set. As source corpora we use CoNLL (Tjong Kim Sang and De Meulder, 2003) and BBN (Weischedel and Brunstein, 2005). To test the effects of the training data size of the target domain on the results, we divided the training set of CADEC and I2B2 into 10 partitions based on a protocol scale and created 10 successively larger training sets by merging these partitions from smallest to largest (with the final merger leading to the full training set)."}, {"heading": "4.2 Baselines", "text": "We compare our methods with the following two in-domain baselines, a cross-domain data-based method and three cross-domain transfer-based benchmark methods.BOW: an in-domain linear chain CRF with handmade features, by Qu et al. (2015).Embed: an in-domain linear chain CRF with handmade features and pre-trained word embeddings, by Qu et al. (2015).LabelEmbed: Take the labels in the source and target domains and determine the alignment based on the similarity between the pre-taught embeddings for each label. CCA: the method by Kim et al. (2015), which generates a one-to-one mapping between source and target NE classes using CCA and k-NN (see Section 2). TransCRCRCRF: A three-layer deep CRF: The bottom layer is a linearity between the source and target NE classes using CCA and k-NN."}, {"heading": "4.3 Experimental Results", "text": "Figure 1 shows the macro-averaged F1 of novel types between our method TransInit and the three baselines on all target corpora. Evaluation results on CADEC with BBN as source corpus are not reported here, because BBN contains all types of CADEC. From the figure we can see that TransInit outperforms all other methods with a wide margin on I2B2. If CoNLL is taken as source corpus, although there are no NE types with I2B2, several target types of source types are DOCTOR and PATIENT w.nn PERSON, and HOS-PITAL w.r.t To verify whether the semantic types of source types are source types of source types of source types such as DOCTOCTOCTOCTOR and PATIENT types."}, {"heading": "18 54 125 268 553 1123 4543 18222", "text": "Most values are either highly positive or negative, which is challenging for online learning algorithms, because these hidden units are unnormalised probabilities generated by the source domain classifier. Therefore, removing the hidden nonlinear layer results in dramatic performance improvement. In addition, Figure 2 shows that further performance improvement is achieved by converting the dual-layer architecture into a linear chain CRF. And updating the hidden layers results in up to 27% higher F1 values than not updating in the second step of TransInit, suggesting that neural networks need to update subordinate features to overcome the problem of covariate shift."}, {"heading": "5 Conclusion", "text": "We have proposed TransInit, a transfer learning method that supports the training of NER models across datasets where there are mismatches in the domain and possibly in the label set. Our method has shown that it achieves an improvement of up to 160% in Formula One over competing baselines based on a handful of instances in the domain."}, {"heading": "Acknowledgments", "text": "This research was supported by NICTA, funded by the Australian Government through the Department of Communications and the Australian Research Council through the ICT Centre of Excellence Program."}], "references": [{"title": "Combining minimally-supervised methods for arabic named entity recognition", "author": ["Maha Althobaiti", "Udo Kruschwitz", "Massimo Poesio."], "venue": "Transactions of the Association for Computational Linguistics, 3:243\u2013255.", "citeRegEx": "Althobaiti et al\\.,? 2015", "shortCiteRegEx": "Althobaiti et al\\.", "year": 2015}, {"title": "Domain adaption of named entity recognition to support credit risk assessment", "author": ["Julio Cesar Salinas Alvarado", "Karin Verspoor", "Timothy Baldwin."], "venue": "Australasian Language Technology Association Workshop 2015.", "citeRegEx": "Alvarado et al\\.,? 2015", "shortCiteRegEx": "Alvarado et al\\.", "year": 2015}, {"title": "Exploiting feature hierarchy for transfer learning in named entity recognition", "author": ["Andrew Arnold", "Ramesh Nallapati", "W. William Cohen."], "venue": "Proceedings of ACL-08: HLT, pages 245\u2013253.", "citeRegEx": "Arnold et al\\.,? 2008", "shortCiteRegEx": "Arnold et al\\.", "year": 2008}, {"title": "Medical entity recognition: A comparison of semantic and statistical methods", "author": ["Asma Ben Abacha", "Pierre Zweigenbaum."], "venue": "Proceedings of BioNLP 2011 Workshop, pages 56\u201364.", "citeRegEx": "Abacha and Zweigenbaum.,? 2011", "shortCiteRegEx": "Abacha and Zweigenbaum.", "year": 2011}, {"title": "Domain adaptation of rule-based annotators for namedentity recognition tasks", "author": ["Laura Chiticariu", "Rajasekar Krishnamurthy", "Yunyao Li", "Frederick Reiss", "Shivakumar Vaithyanathan."], "venue": "Proceedings of the 2010 Conference on Empirical Methods in Natural Language", "citeRegEx": "Chiticariu et al\\.,? 2010", "shortCiteRegEx": "Chiticariu et al\\.", "year": 2010}, {"title": "Natural language processing (almost) from scratch", "author": ["Ronan Collobert", "Jason Weston", "L\u00e9on Bottou", "Michael Karlen", "Koray Kavukcuoglu", "Pavel Kuksa."], "venue": "Journal of Machine Learning Research, 12:2493\u20132537.", "citeRegEx": "Collobert et al\\.,? 2011", "shortCiteRegEx": "Collobert et al\\.", "year": 2011}, {"title": "A hybrid neural model for type classification of entity mentions", "author": ["Li Dong", "Furu Wei", "Hong Tan", "Sun", "Ming Zhou", "Ke Xu."], "venue": "Twenty-Fourth International Joint Conference on Artificial Intelligence (IJCAI), pages 1243\u20131249.", "citeRegEx": "Dong et al\\.,? 2015", "shortCiteRegEx": "Dong et al\\.", "year": 2015}, {"title": "Incorporating non-local information into information extraction systems by Gibbs sampling", "author": ["Jenny Rose Finkel", "Trond Grenager", "Christopher Manning."], "venue": "Proceedings of the 43rd Annual Meeting on Association for Computational Linguistics, pages 363\u2013370.", "citeRegEx": "Finkel et al\\.,? 2005", "shortCiteRegEx": "Finkel et al\\.", "year": 2005}, {"title": "Understanding the difficulty of training deep feedforward neural networks", "author": ["Xavier Glorot", "Yoshua Bengio."], "venue": "Proceedings of the Thirteenth International Conference on Artificial Intelligence and Statistics (AISTATS 2010), pages 249\u2013256.", "citeRegEx": "Glorot and Bengio.,? 2010", "shortCiteRegEx": "Glorot and Bengio.", "year": 2010}, {"title": "Cadec: A corpus of adverse drug event annotations", "author": ["Sarvnaz Karimi", "Alejandro Metke-Jimenez", "Madonna Kemp", "Chen Wang."], "venue": "Journal of Biomedical Informatics, 55:73\u201381.", "citeRegEx": "Karimi et al\\.,? 2015", "shortCiteRegEx": "Karimi et al\\.", "year": 2015}, {"title": "New transfer learning techniques for disparate label sets", "author": ["Young-Bum Kim", "Karl Stratos", "Ruhi Sarikaya", "Minwoo Jeong."], "venue": "Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference", "citeRegEx": "Kim et al\\.,? 2015", "shortCiteRegEx": "Kim et al\\.", "year": 2015}, {"title": "Conditional random fields: Probabilistic models for segmenting and labeling sequence data", "author": ["John Lafferty", "Andrew McCallum", "Fernando Pereira."], "venue": "Proceedings of the 18th International Conference on Machine Learning, pages 282\u2013289.", "citeRegEx": "Lafferty et al\\.,? 2001", "shortCiteRegEx": "Lafferty et al\\.", "year": 2001}, {"title": "Fine-grained named entity recognition and relation extraction for question answering", "author": ["Changki Lee", "Yi-Gyu Hwang", "Myung-Gil Jang."], "venue": "Proceedings of the 30th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval,", "citeRegEx": "Lee et al\\.,? 2007", "shortCiteRegEx": "Lee et al\\.", "year": 2007}, {"title": "Fine-grained entity recognition", "author": ["Xiao Ling", "Daniel S. Weld."], "venue": "Proceedings of the 26th AAAI Conference on Artificial Intelligence.", "citeRegEx": "Ling and Weld.,? 2012", "shortCiteRegEx": "Ling and Weld.", "year": 2012}, {"title": "Named entity recognition from diverse text types", "author": ["Diana Maynard", "Valentin Tablan", "Cristian Ursu", "Hamish Cunningham", "Yorick Wilks."], "venue": "Recent Advances in Natural Language Processing 2001 Conference.", "citeRegEx": "Maynard et al\\.,? 2001", "shortCiteRegEx": "Maynard et al\\.", "year": 2001}, {"title": "DBpedia spotlight: shedding light on the web of documents", "author": ["Pablo N Mendes", "Max Jakob", "Andr\u00e9s Garc\u0131\u0301a-Silva", "Christian Bizer"], "venue": "In Proceedings of the 7th International Conference on Semantic Systems,", "citeRegEx": "Mendes et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Mendes et al\\.", "year": 2011}, {"title": "Big data small data, in domain out-of domain, known word unknown word: The impact of word representations on sequence labelling tasks", "author": ["Lizhen Qu", "Gabriela Ferraro", "Liyuan Zhou", "Weiwei Hou", "Nathan Schneider", "Timothy Baldwin."], "venue": "Proceedings of the", "citeRegEx": "Qu et al\\.,? 2015", "shortCiteRegEx": "Qu et al\\.", "year": 2015}, {"title": "NERD: a framework for unifying named entity recognition and disambiguation extraction tools", "author": ["Giuseppe Rizzo", "Rapha\u00ebl Troncy."], "venue": "Proceedings of the Demonstrations at the 13th Conference of the European Chapter of the Association for Computational", "citeRegEx": "Rizzo and Troncy.,? 2012", "shortCiteRegEx": "Rizzo and Troncy.", "year": 2012}, {"title": "Covariate shift adaptation by importance weighted cross validation", "author": ["Masashi Sugiyama", "Matthias Krauledat", "Klaus-Robert M\u00fcller."], "venue": "Journal of Machine Learning Research, 8:985\u20131005.", "citeRegEx": "Sugiyama et al\\.,? 2007", "shortCiteRegEx": "Sugiyama et al\\.", "year": 2007}, {"title": "Composition of conditional random fields for transfer learning", "author": ["Charles Sutton", "Andrew McCallum."], "venue": "Proceedings of the Conference on Human Language Technology and Empirical Methods in Natural Language Processing, HLT \u201905, pages 748\u2013754.", "citeRegEx": "Sutton and McCallum.,? 2005", "shortCiteRegEx": "Sutton and McCallum.", "year": 2005}, {"title": "Introduction to the CoNLL-2003 shared task: Languageindependent named entity recognition", "author": ["Erik F. Tjong Kim Sang", "Fien De Meulder."], "venue": "Proceedings of CoNLL-2003, pages 142\u2013147.", "citeRegEx": "Sang and Meulder.,? 2003", "shortCiteRegEx": "Sang and Meulder.", "year": 2003}, {"title": "Word representations: a simple and general method for semi-supervised learning", "author": ["Joseph Turian", "Lev Ratinov", "Yoshua Bengio."], "venue": "Proceedings of the 48th annual meeting of the association for computational linguistics, pages 384\u2013394.", "citeRegEx": "Turian et al\\.,? 2010", "shortCiteRegEx": "Turian et al\\.", "year": 2010}, {"title": "BBN pronoun coreference and entity type corpus", "author": ["Ralph Weischedel", "Ada Brunstein."], "venue": "Linguistic Data Consortium.", "citeRegEx": "Weischedel and Brunstein.,? 2005", "shortCiteRegEx": "Weischedel and Brunstein.", "year": 2005}, {"title": "Corpus-level fine-grained entity typing using contextual information", "author": ["Yadollah Yaghoobzadeh", "Hinrich Sch\u00fctze."], "venue": "Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing (EMNLP 2015), pages 715\u2013725.", "citeRegEx": "Yaghoobzadeh and Sch\u00fctze.,? 2015", "shortCiteRegEx": "Yaghoobzadeh and Sch\u00fctze.", "year": 2015}, {"title": "HYENAlive: Fine-grained online entity type classification from natural-language text", "author": ["Mohamed Amir Yosef", "Sandro Bauer", "Johannes Hoffart", "Marc Spaniol", "Gerhard Weikum."], "venue": "Proceedings of the 51st Annual Meeting of the Association for Computational Lin-", "citeRegEx": "Yosef et al\\.,? 2013", "shortCiteRegEx": "Yosef et al\\.", "year": 2013}, {"title": "How transferable are features in deep neural networks", "author": ["Jason Yosinski", "Jeff Clune", "Yoshua Bengio", "Hod Lipson"], "venue": "In Advances in Neural Information Processing Systems", "citeRegEx": "Yosinski et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Yosinski et al\\.", "year": 2014}], "referenceMentions": [{"referenceID": 11, "context": "There are two main approaches to named entity recognition (NER): (i) build sequence labelling models such as conditional random fields (CRFs) (Lafferty et al., 2001) on a large manually-labelled training cor-", "startOffset": 142, "endOffset": 165}, {"referenceID": 7, "context": "pus (Finkel et al., 2005); and (ii) exploit knowledge bases to recognise mentions of entities in text (Rizzo and Troncy, 2012; Mendes et al.", "startOffset": 4, "endOffset": 25}, {"referenceID": 17, "context": ", 2005); and (ii) exploit knowledge bases to recognise mentions of entities in text (Rizzo and Troncy, 2012; Mendes et al., 2011).", "startOffset": 84, "endOffset": 129}, {"referenceID": 15, "context": ", 2005); and (ii) exploit knowledge bases to recognise mentions of entities in text (Rizzo and Troncy, 2012; Mendes et al., 2011).", "startOffset": 84, "endOffset": 129}, {"referenceID": 21, "context": "Handcrafted features play a key role in supervised NER models (Turian et al., 2010), but if we have only limited training amounts of training data, we will be hampered in our ability to reliably learn feature weights.", "startOffset": 62, "endOffset": 83}, {"referenceID": 1, "context": "Alvarado et al. (2015) show that even if the NE label set is identical across domains, large discrepancies in the label distribution can lead to poor performance.", "startOffset": 0, "endOffset": 23}, {"referenceID": 2, "context": "The main scenario where transfer learning has been applied to NER is domain adaptation (Arnold et al., 2008; Maynard et al., 2001; Chiticariu et al., 2010), where it is assumed that the label set Y is the same for both the source and target corpora, and only the domain varies.", "startOffset": 87, "endOffset": 155}, {"referenceID": 14, "context": "The main scenario where transfer learning has been applied to NER is domain adaptation (Arnold et al., 2008; Maynard et al., 2001; Chiticariu et al., 2010), where it is assumed that the label set Y is the same for both the source and target corpora, and only the domain varies.", "startOffset": 87, "endOffset": 155}, {"referenceID": 4, "context": "The main scenario where transfer learning has been applied to NER is domain adaptation (Arnold et al., 2008; Maynard et al., 2001; Chiticariu et al., 2010), where it is assumed that the label set Y is the same for both the source and target corpora, and only the domain varies.", "startOffset": 87, "endOffset": 155}, {"referenceID": 10, "context": "Similar to our work, Kim et al. (2015) use transfer learning to deal with NER data sets with different label distributions.", "startOffset": 21, "endOffset": 39}, {"referenceID": 25, "context": "Similar conclusions were reached by Yosinski et al. (2014), who investigated", "startOffset": 36, "endOffset": 59}, {"referenceID": 19, "context": "Sutton and McCallum (2005) investigated how the target task affects the source task, and demonstrated that decoding for transfer is better than no transfer, and joint", "startOffset": 0, "endOffset": 27}, {"referenceID": 13, "context": "Another way of dealing with a lack of annotated NER data is to use distant supervision by exploiting knowledge bases to recognise mentions of entities (Ling and Weld, 2012; Dong et al., 2015; Yosef et al., 2013; Althobaiti et al., 2015; Yaghoobzadeh and Sch\u00fctze, 2015).", "startOffset": 151, "endOffset": 268}, {"referenceID": 6, "context": "Another way of dealing with a lack of annotated NER data is to use distant supervision by exploiting knowledge bases to recognise mentions of entities (Ling and Weld, 2012; Dong et al., 2015; Yosef et al., 2013; Althobaiti et al., 2015; Yaghoobzadeh and Sch\u00fctze, 2015).", "startOffset": 151, "endOffset": 268}, {"referenceID": 24, "context": "Another way of dealing with a lack of annotated NER data is to use distant supervision by exploiting knowledge bases to recognise mentions of entities (Ling and Weld, 2012; Dong et al., 2015; Yosef et al., 2013; Althobaiti et al., 2015; Yaghoobzadeh and Sch\u00fctze, 2015).", "startOffset": 151, "endOffset": 268}, {"referenceID": 0, "context": "Another way of dealing with a lack of annotated NER data is to use distant supervision by exploiting knowledge bases to recognise mentions of entities (Ling and Weld, 2012; Dong et al., 2015; Yosef et al., 2013; Althobaiti et al., 2015; Yaghoobzadeh and Sch\u00fctze, 2015).", "startOffset": 151, "endOffset": 268}, {"referenceID": 23, "context": "Another way of dealing with a lack of annotated NER data is to use distant supervision by exploiting knowledge bases to recognise mentions of entities (Ling and Weld, 2012; Dong et al., 2015; Yosef et al., 2013; Althobaiti et al., 2015; Yaghoobzadeh and Sch\u00fctze, 2015).", "startOffset": 151, "endOffset": 268}, {"referenceID": 13, "context": "Having a fine-grained entity typology has been shown to improve other tasks such as relation extraction (Ling and Weld, 2012) and question answering (Lee et al.", "startOffset": 104, "endOffset": 125}, {"referenceID": 12, "context": "Having a fine-grained entity typology has been shown to improve other tasks such as relation extraction (Ling and Weld, 2012) and question answering (Lee et al., 2007).", "startOffset": 149, "endOffset": 167}, {"referenceID": 8, "context": "An activation function is saturated if its input values are its max/min values (Glorot and Bengio, 2010).", "startOffset": 79, "endOffset": 104}, {"referenceID": 18, "context": "However, in our experiments, we find that parameter update is necessary for the bottom linear layer because of covariate shift (Sugiyama et al., 2007), which is caused by discrepancy in the distribution between the source and target domains.", "startOffset": 127, "endOffset": 150}, {"referenceID": 17, "context": "Therefore, we apply AdaGrad (Rizzo and Troncy, 2012) with early stopping based on development data, so that the knowledge of the source domain is preserved as much as possible.", "startOffset": 28, "endOffset": 52}, {"referenceID": 9, "context": "We use CADEC (Karimi et al., 2015) and I2B2 (Ben Abacha and Zweigenbaum, 2011) as target corpora with the standard training and test splits.", "startOffset": 13, "endOffset": 34}, {"referenceID": 22, "context": "As source corpora, we adopt CoNLL (Tjong Kim Sang and De Meulder, 2003) and BBN (Weischedel and Brunstein, 2005).", "startOffset": 80, "endOffset": 112}, {"referenceID": 16, "context": "BOW: an in-domain linear-chain CRF with handcrafted features, from Qu et al. (2015).", "startOffset": 67, "endOffset": 84}, {"referenceID": 16, "context": "Embed: an in-domain linear-chain CRF with handcrafted features and pre-trained word embeddings, from Qu et al. (2015).", "startOffset": 101, "endOffset": 118}, {"referenceID": 10, "context": "CCA: the method of Kim et al. (2015), where a one-to-one mapping is generated between source and target NE classes using CCA and k-NN (see Section 2).", "startOffset": 19, "endOffset": 37}, {"referenceID": 5, "context": "The middle layer is a hard tanh function (Collobert et al., 2011).", "startOffset": 41, "endOffset": 65}], "year": 2016, "abstractText": "In named entity recognition, we often don\u2019t have a large in-domain training corpus or a knowledge base with adequate coverage to train a model directly. In this paper, we propose a method where, given training data in a related domain with similar (but not identical) named entity (NE) types and a small amount of in-domain training data, we use transfer learning to learn a domain-specific NE model. That is, the novelty in the task setup is that we assume not just domain mismatch, but also label mismatch.", "creator": "TeX"}}}