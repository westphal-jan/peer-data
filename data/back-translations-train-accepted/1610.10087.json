{"id": "1610.10087", "review": {"conference": "NIPS", "VERSION": "v1", "DATE_OF_SUBMISSION": "31-Oct-2016", "title": "Tensor Switching Networks", "abstract": "We present a novel neural network algorithm, the Tensor Switching (TS) network, which generalizes the Rectified Linear Unit (ReLU) nonlinearity to tensor-valued hidden units. The TS network copies its entire input vector to different locations in an expanded representation, with the location determined by its hidden unit activity. In this way, even a simple linear readout from the TS representation can implement a highly expressive deep-network-like function. The TS network hence avoids the vanishing gradient problem by construction, at the cost of larger representation size. We develop several methods to train the TS network, including equivalent kernels for infinitely wide and deep TS networks, a one-pass linear learning algorithm, and two backpropagation-inspired representation learning algorithms. Our experimental results demonstrate that the TS network is indeed more expressive and consistently learns faster than standard ReLU networks.", "histories": [["v1", "Mon, 31 Oct 2016 19:44:50 GMT  (415kb,D)", "http://arxiv.org/abs/1610.10087v1", null]], "reviews": [], "SUBJECTS": "cs.NE cs.LG stat.ML", "authors": ["chuan-yung tsai", "andrew m saxe", "david d cox"], "accepted": true, "id": "1610.10087"}, "pdf": {"name": "1610.10087.pdf", "metadata": {"source": "CRF", "title": "Tensor Switching Networks", "authors": ["Chuan-Yung Tsai", "Andrew Saxe", "David Cox"], "emails": ["chuanyungtsai@fas.harvard.edu", "asaxe@fas.harvard.edu", "davidcox@fas.harvard.edu"], "sections": [{"heading": "1 Introduction", "text": "In this thesis, we are developing a novel deep learning algorithm, the Tensor Switching (TS) network, which generalizes the ReLU so that each hidden unit mediates a tensor rather than a scalar, resulting in a more expressive model. Like the ReLU network, it is a linear function of its input due to the activation pattern of its hidden units. By separating the decision to activate from the analysis that is performed, even a linear classifier can fall back on all layers of the TS network, while implementing a deep network-like function while avoiding the disappearing gradient problem [5] that can significantly slow down learning in deep networks."}, {"heading": "2 Tensor Switching Networks", "text": "In the following, we first construct the definition of flat (single-hidden-layer) TS networks, then generalize the definition to deep TS networks, and finally describe their qualitative properties. For the sake of simplicity, we only show fully connected architectures using ReLU nonlinearity. But other popular nonlinearities such as Max Pooling and Maxout [16] are also supported in addition to ReLU in both fully connected and revolutionary architectures."}, {"heading": "2.1 Shallow TS Networks", "text": "The TS-ReLU network is a generalization of the standard ReLU networks, which allows each hidden unit to transmit a complete tensor of activity (see Figure 1). To describe it, we build from the standard ReLU network. Consider a ReLU layer with weight matrix W1, Rn1, Rn0, which responds to an input vector X0, Rn0. The resulting hidden activity X1, Rn1 of this layer is X1 = max (0n1, W1X0) = H (W1X0), where H is the heaviside step function and denotes an elementary product. The most located equation separates the decision of each hidden unit represented by the term H (W1X0) from the information (i.e. result of the analysis) it is transmitted when it is active, tenar the product W1X."}, {"heading": "2.2 Deep TS Networks", "text": "Define a nonlinear expansion operation as X-W = H (WX) X and linear contraction operation as Z-W = (Z-W) \u00b7 1n, so that (1) a deep SS-ReLU network with L layers \u00b7 Wl) \u00b7 1nl-1 = Xl-1-Wl for a given layer lwith Xl-Rnl and Wl-Rnl-nl \u2212 1. A deep SS-ReLU network with L layers can then be expressed as a sequence of alternating expansion and contraction steps, XL = X0-W1 W1 \u00b7 \u00b7 \u00b7 \u00b7 WL-WL WL. (2) To obtain the deep TS-ReLU network, we define the ternary expansion operation Z-X-W = H (WX)."}, {"heading": "2.3 Properties", "text": "The TS network decouples the decision of a hidden activation unit (as encrypted by the activation weights) from the analysis performed at the input when the unit is active (as encrypted by the analytical weights).This distinguishing feature leads to the following 3 characteristics. Cross-layer analysis. Since the TS representation preserves the layered structure of a deep network and provides direct access to the entire input (parcelled by the activated hidden units), a simple linear reading can effectively reach back across the input layers, implicitly learning analytical weights for all layers simultaneously in the center. Therefore, it avoids the disappearing gradient problem by design. 2Error-correcting analysis are separated, a careful selection of analytical weights can reveal a certain inaccuracy in the choice of the network, e.g. from loud or even random activation weights."}, {"heading": "3 Equivalent Kernels", "text": "In this section, we derive equivalent kernels for TS-ReLU networks with arbitrary depth and an infinite number of hidden units at each level, with the aim of providing theoretical insights into how TS-ReLU analytically differs from SS-ReLU. These kernels represent the extremes of infinite (but unlearned) properties, and in SVM could only change the way the input data sizes from small to medium size.2It is similar in spirit to models with skip connections to the outputs, although they are not exactly reducible. 3Therefore, TS networks are also universal functional approximations [19].Consider a single-hidden-layer TS-ReLU network with n1 hidden units, in which each element of the activation matrix W1 can be reduced."}, {"heading": "4 Learning Algorithms", "text": "In the following we present 3 learning algorithms, which are suitable for different scenarios: The one-pass ridge regression in seconds 4.1 learns only the linear reading (i.e. analysis weights WZ), whereby the hidden layer representations (i.e. activation weights {Wl}) remain random, so it is convex and precisely solvable; the reverse reverse reverse propagation in seconds 4.2 learns both analysis weights and activation weights; the linear rotation compression in seconds 4.3 also learns both weights, but the activation weights in an indirect way."}, {"heading": "4.1 Linear Readout Learning via One-pass Ridge Regression", "text": "In this diagram we use the intuition that precision is less important when deciding on a hidden unit than carefully tuned analysis weights, which can compensate for partially poorly tuned activation weights. We draw and fix the activation weights randomly {Wl} and then solve the weights WZ by means of burr regression, which can occur in a single pass through the dataset. First, each data point p = 1,.. P is expanded into its tensor representation ZpL and then accumulated in the correlation matrices CZZ = \u2211 p vec (Z p L) vec (Z p L) vec (Z p L) vec (Z p L) and CyZ = p vec (ZpL) cumulative. Once all data points are processed, the analysis weights as WZ = CyZ = CyZ (CZZ + \u03bbI) \u2212 1 are accumulated, whereby a L2 regulation parameter is undefined in the analytical parameter.In contrast to an effective setting of the SS-only in this network, the Z is a definitive parameter."}, {"heading": "4.2 Representation Learning via Inverted Backpropagation", "text": "Comb regression learning uses accidental activation weights and learns only analytical weights. Here we provide a \"gradient-based\" procedure to learn both weights. Learning of analytical weights (i.e. the last linear layer) VT \u00b7 \u00b7 simply requires activation of LTS \u00b7 VT, which is generally easy to calculate. Since activation weights Vl in TS-network however only within the heaviside stepping function H with zero (or indefinite) \u00a5jpjpjpjpjpjpjpjpjpjpjpjpjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjj"}, {"heading": "4.3 Indirect Representation Learning via Linear Rotation-Compression", "text": "Although inverted back-propagation learning manages memory and time complexities better than one-pass-first regression, the exponential growth in the representation of a TS network severely limits its potential for use in newer deep-learning architectures, where network width and depth can easily exceed, say, a thousand. In addition, the success of newer deep-learning architectures also depends heavily on the acceleration provided by highly optimized GPU-enabled libraries, where the operations of the previous tutorials are largely unsupported. To address these two concerns, we offer a standard back-propagation-compatible learning algorithm in which we no longer hold separate X and Z variables. Instead, we define Xl = W-1 vec, which flattens out extended representation and linear projects that target it against W-L compression directly."}, {"heading": "5 Experimental Results", "text": "In recent years, it has been shown that most of them are people who are not able to survive themselves, and that they are not able to survive themselves, \"he said.\" I don't think they are able to survive themselves. \"But he also stressed,\" I don't think they are able to survive me. \"He emphasized,\" I don't think they are able to survive me. \"He added,\" I don't think I am able to survive myself, and I don't think I am able to survive myself. \""}, {"heading": "6 Discussion", "text": "Why do TS networks learn fast? In general, the TS network deflects the problem of the disappearing gradient by skipping the long chain of linear contractions against the analytical weights (i.e. the auxiliary path in Fig. 3). Its linear reading has direct access to the full input vector, which is switched to different parts of the highly expressive advanced representation, directly accelerating learning. In addition, a well-flowing gradient gives benefits beyond the TS layers - for example, SS layers placed in front of TS layers also learn faster as TS layers \"self-organize,\" allowing useful error signals to flow more quickly to the lower layers."}, {"heading": "Acknowledgments", "text": "We thank James Fitzgerald, Mien \"Brabeeba\" Wang, Scott Linderman and Yu Hu for the fruitful discussions and anonymous reviewers for their valuable comments. This work was supported by the NSF (IIS 1409097), IARPA (Contract D16PC00002) and the Swartz Foundation. This is a crucial aspect of downward dynamics in layer structures that behave like a chain - the weakest link must change first [23, 24]."}], "references": [{"title": "Deep learning", "author": ["Y. LeCun", "Y. Bengio", "G. Hinton"], "venue": "Nature, 2015.", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2015}, {"title": "Deep learning in neural networks: An overview", "author": ["J. Schmidhuber"], "venue": "Neural Networks, 2015.", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2015}, {"title": "Digital selection and analogue amplification coexist in a cortex-inspired silicon circuit", "author": ["R. Hahnloser", "R. Sarpeshkar", "M. Mahowald", "R. Douglas", "S. Seung"], "venue": "Nature, 2000.", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2000}, {"title": "Rectified Linear Units Improve Restricted Boltzmann Machines", "author": ["V. Nair", "G. Hinton"], "venue": "ICML, 2010.", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2010}, {"title": "Gradient Flow in Recurrent Nets: the Difficulty of Learning Long-Term Dependencies", "author": ["S. Hochreiter", "Y. Bengio", "P. Frasconi", "J. Schmidhuber"], "venue": "A Field Guide to Dynamical Recurrent Networks, 2001.", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2001}, {"title": "A Spike and Slab Restricted Boltzmann Machine", "author": ["A. Courville", "J. Bergstra", "Y. Bengio"], "venue": "AISTATS, 2011.", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2011}, {"title": "Zero-bias autoencoders and the benefits of co-adapting features", "author": ["K. Konda", "R. Memisevic", "D. Krueger"], "venue": "ICLR, 2015.", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2015}, {"title": "Training Very Deep Networks", "author": ["R. Srivastava", "K. Greff", "J. Schmidhuber"], "venue": "NIPS, 2015.", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2015}, {"title": "Identity Mappings in Deep Residual Networks", "author": ["K. He", "X. Zhang", "S. Ren", "J. Sun"], "venue": "ECCV, 2016.", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2016}, {"title": "Beating the Perils of Non-Convexity: Guaranteed Training of Neural Networks using Tensor Methods", "author": ["M. Janzamin", "H. Sedghi", "A. Anandkumar"], "venue": "arXiv, 2015.", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2015}, {"title": "Deep Convex Net: A Scalable Architecture for Speech Pattern Classification", "author": ["L. Deng", "D. Yu"], "venue": "Interspeech, 2011.", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2011}, {"title": "Input-Convex Deep Networks", "author": ["B. Amos", "Z. Kolter"], "venue": "ICLR Workshop, 2015.", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2015}, {"title": "Convex Deep Learning via Normalized Kernels", "author": ["\u00d6. Aslan", "X. Zhang", "D. Schuurmans"], "venue": "NIPS, 2014.", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2014}, {"title": "Analysis of Deep Neural Networks with the Extended Data Jacobian Matrix", "author": ["S. Wang", "A. Mohamed", "R. Caruana", "J. Bilmes", "M. Plilipose", "M. Richardson", "K. Geras", "G. Urban", "O. Aslan"], "venue": "ICML, 2016.", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2016}, {"title": "Very Deep Convolutional Networks for Large-Scale Image Recognition", "author": ["K. Simonyan", "A. Zisserman"], "venue": "ICLR, 2015.", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2015}, {"title": "Maxout Networks", "author": ["I. Goodfellow", "D. Warde-Farley", "M. Mirza", "A. Courville", "Y. Bengio"], "venue": "ICML, 2013.", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2013}, {"title": "Going Deeper with Convolutions", "author": ["C. Szegedy", "W. Liu", "Y. Jia", "P. Sermanet", "S. Reed", "D. Anguelov", "D. Erhan", "V. Vanhoucke", "A. Rabinovich"], "venue": "CVPR, 2015.", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2015}, {"title": "Densely Connected Convolutional Networks", "author": ["G. Huang", "Z. Liu", "K. Weinberger"], "venue": "arXiv, 2016.", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2016}, {"title": "Neural network with unbounded activation functions is universal approximator", "author": ["S. Sonoda", "N. Murata"], "venue": "Applied and Computational Harmonic Analysis, 2015.", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2015}, {"title": "Large-Margin Classification in Infinite Neural Networks", "author": ["Y. Cho", "L. Saul"], "venue": "Neural Computation, 2010.", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2010}, {"title": "Avoiding pathologies in very deep networks", "author": ["D. Duvenaud", "O. Rippel", "R. Adams", "Z. Ghahramani"], "venue": "AISTATS, 2014.", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2014}, {"title": "Deep Scattering Spectrum", "author": ["J. And\u00e9n", "S. Mallat"], "venue": "IEEE T-SP, 2014.", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2014}, {"title": "Exact solutions to the nonlinear dynamics of learning in deep linear neural networks", "author": ["A. Saxe", "J. McClelland", "S. Ganguli"], "venue": "ICLR, 2014.", "citeRegEx": "23", "shortCiteRegEx": null, "year": 2014}, {"title": "A deep learning theory of perceptual learning dynamics", "author": ["A. Saxe"], "venue": "COSYNE, 2015.", "citeRegEx": "24", "shortCiteRegEx": null, "year": 2015}, {"title": "Distributional Smoothing with Virtual Adversarial Training", "author": ["T. Miyato", "S. Maeda", "M. Koyama", "K. Nakae", "S. Ishii"], "venue": "ICLR, 2016.", "citeRegEx": "25", "shortCiteRegEx": null, "year": 2016}, {"title": "Differential Geometric Regularization for Supervised Learning of Classifiers", "author": ["Q. Bai", "S. Rosenberg", "Z. Wu", "S. Sclaroff"], "venue": "ICML, 2016.", "citeRegEx": "26", "shortCiteRegEx": null, "year": 2016}], "referenceMentions": [{"referenceID": 0, "context": "Deep networks [1, 2] continue to post impressive successes in a wide range of tasks, and the Rectified Linear Unit (ReLU) [3, 4] is arguably the most used simple nonlinearity.", "startOffset": 14, "endOffset": 20}, {"referenceID": 1, "context": "Deep networks [1, 2] continue to post impressive successes in a wide range of tasks, and the Rectified Linear Unit (ReLU) [3, 4] is arguably the most used simple nonlinearity.", "startOffset": 14, "endOffset": 20}, {"referenceID": 2, "context": "Deep networks [1, 2] continue to post impressive successes in a wide range of tasks, and the Rectified Linear Unit (ReLU) [3, 4] is arguably the most used simple nonlinearity.", "startOffset": 122, "endOffset": 128}, {"referenceID": 3, "context": "Deep networks [1, 2] continue to post impressive successes in a wide range of tasks, and the Rectified Linear Unit (ReLU) [3, 4] is arguably the most used simple nonlinearity.", "startOffset": 122, "endOffset": 128}, {"referenceID": 4, "context": "By separating the decision to activate from the analysis performed when active, even a linear classifier can reach back across all layers to the input of the TS network, implementing a deep-network-like function while avoiding the vanishing gradient problem [5], which can otherwise significantly slow down learning in deep networks.", "startOffset": 258, "endOffset": 261}, {"referenceID": 5, "context": "With respect to improving the nonlinearities, the idea of severing activation and analysis weights (or having multiple sets of weights) in each hidden layer has been studied in [6, 7, 8].", "startOffset": 177, "endOffset": 186}, {"referenceID": 6, "context": "With respect to improving the nonlinearities, the idea of severing activation and analysis weights (or having multiple sets of weights) in each hidden layer has been studied in [6, 7, 8].", "startOffset": 177, "endOffset": 186}, {"referenceID": 7, "context": "With respect to improving the nonlinearities, the idea of severing activation and analysis weights (or having multiple sets of weights) in each hidden layer has been studied in [6, 7, 8].", "startOffset": 177, "endOffset": 186}, {"referenceID": 8, "context": "Reordering activation and analysis is proposed by [9].", "startOffset": 50, "endOffset": 53}, {"referenceID": 9, "context": "On tackling the vanishing gradient problem, tensor methods are used by [10] to train single-hidden-layer networks.", "startOffset": 71, "endOffset": 75}, {"referenceID": 10, "context": "Convex learning and inference in various deep architectures can be found in [11, 12, 13] too.", "startOffset": 76, "endOffset": 88}, {"referenceID": 11, "context": "Convex learning and inference in various deep architectures can be found in [11, 12, 13] too.", "startOffset": 76, "endOffset": 88}, {"referenceID": 12, "context": "Convex learning and inference in various deep architectures can be found in [11, 12, 13] too.", "startOffset": 76, "endOffset": 88}, {"referenceID": 13, "context": "Finally, conditional linearity of deep ReLU networks is also used by [14], mainly to analyze their performance.", "startOffset": 69, "endOffset": 73}, {"referenceID": 14, "context": "Instead, it is a cross-layer generalization of these concepts, which can be applied with most of the recent deep learning architectures [15, 9], not only to increase their expressiveness, but also to help avoiding the vanishing gradient problem (see Sec.", "startOffset": 136, "endOffset": 143}, {"referenceID": 8, "context": "Instead, it is a cross-layer generalization of these concepts, which can be applied with most of the recent deep learning architectures [15, 9], not only to increase their expressiveness, but also to help avoiding the vanishing gradient problem (see Sec.", "startOffset": 136, "endOffset": 143}, {"referenceID": 1, "context": "[2 3]", "startOffset": 0, "endOffset": 5}, {"referenceID": 2, "context": "[2 3]", "startOffset": 0, "endOffset": 5}, {"referenceID": 1, "context": "[2 3] Tensor Switching ReLU Z1 W1 1", "startOffset": 0, "endOffset": 5}, {"referenceID": 2, "context": "[2 3] Tensor Switching ReLU Z1 W1 1", "startOffset": 0, "endOffset": 5}, {"referenceID": 15, "context": "max pooling and maxout [16], in addition to ReLU, are also supported in both fully-connected and convolutional architectures.", "startOffset": 23, "endOffset": 27}, {"referenceID": 16, "context": "It is in spirit similar to models with skip connections to the output [17, 18], although not exactly reducible.", "startOffset": 70, "endOffset": 78}, {"referenceID": 17, "context": "It is in spirit similar to models with skip connections to the output [17, 18], although not exactly reducible.", "startOffset": 70, "endOffset": 78}, {"referenceID": 18, "context": "Therefore TS networks are also universal function approximators [19].", "startOffset": 64, "endOffset": 68}, {"referenceID": 19, "context": "Figure 2 compares (5) against the linear kernel and the single-hidden-layer infinite-width random SS-ReLU kernel (4) from [20] (see Linear, TS L = 1 and SS L = 1).", "startOffset": 122, "endOffset": 126}, {"referenceID": 19, "context": "Based upon the recursive formulation from [20], first we define the zeroth-layer kernel k\u2022 0 (x,y) = x Ty and the generalized angle \u03b8\u2022 l = cos \u22121 (k\u2022 l (x,y)/\u221ak\u2022 l (x,x) k\u2022 l (y,y)), where \u2022 denotes SS or TS.", "startOffset": 42, "endOffset": 46}, {"referenceID": 20, "context": "Interestingly, a similar kernel is also observed by [21] for models with explicit skip connections.", "startOffset": 52, "endOffset": 56}, {"referenceID": 19, "context": "We write (4) and k l differently from [20] for cleaner comparisons against TS-ReLU kernels.", "startOffset": 38, "endOffset": 42}, {"referenceID": 19, "context": "However they are numerically unstable expressions and are not used in our experiments to replace the original ones in [20].", "startOffset": 118, "endOffset": 122}, {"referenceID": 21, "context": "SVMs using SS-ReLU and TS-ReLU kernels are implemented in Matlab based on libsvm-compact [22].", "startOffset": 89, "endOffset": 93}, {"referenceID": 14, "context": "CNN) architectures, we adopt VGG-style [15] convolutional layers with 3 standard SS convolution-max pooling blocks,9 where each block can have up to three 3 \u00d7 3 convolutions, plus 1 to 3 fully-connected SS or TS layers of fixed width 256.", "startOffset": 39, "endOffset": 43}, {"referenceID": 24, "context": "To remedy this, we may need to consider other types of regularization for WZ (instead of L2) or other smoothing techniques [25, 26].", "startOffset": 123, "endOffset": 131}, {"referenceID": 25, "context": "To remedy this, we may need to consider other types of regularization for WZ (instead of L2) or other smoothing techniques [25, 26].", "startOffset": 123, "endOffset": 131}, {"referenceID": 8, "context": "With improved scalability, we also plan to further verify the TS nonlinearity\u2019s efficiency in state-of-the-art architectures [27, 9, 18], which are still computationally prohibitive with our current implementation.", "startOffset": 125, "endOffset": 136}, {"referenceID": 17, "context": "With improved scalability, we also plan to further verify the TS nonlinearity\u2019s efficiency in state-of-the-art architectures [27, 9, 18], which are still computationally prohibitive with our current implementation.", "startOffset": 125, "endOffset": 136}, {"referenceID": 22, "context": "This is a crucial aspect of gradient descent dynamics in layered structures, which behave like a chain\u2014the weakest link must change first [23, 24].", "startOffset": 138, "endOffset": 146}, {"referenceID": 23, "context": "This is a crucial aspect of gradient descent dynamics in layered structures, which behave like a chain\u2014the weakest link must change first [23, 24].", "startOffset": 138, "endOffset": 146}], "year": 2016, "abstractText": "We present a novel neural network algorithm, the Tensor Switching (TS) network, which generalizes the Rectified Linear Unit (ReLU) nonlinearity to tensor-valued hidden units. The TS network copies its entire input vector to different locations in an expanded representation, with the location determined by its hidden unit activity. In this way, even a simple linear readout from the TS representation can implement a highly expressive deep-network-like function. The TS network hence avoids the vanishing gradient problem by construction, at the cost of larger representation size. We develop several methods to train the TS network, including equivalent kernels for infinitely wide and deep TS networks, a one-pass linear learning algorithm, and two backpropagation-inspired representation learning algorithms. Our experimental results demonstrate that the TS network is indeed more expressive and consistently learns faster than standard ReLU networks.", "creator": "LaTeX with hyperref package"}}}