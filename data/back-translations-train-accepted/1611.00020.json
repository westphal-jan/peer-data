{"id": "1611.00020", "review": {"conference": "ACL", "VERSION": "v1", "DATE_OF_SUBMISSION": "31-Oct-2016", "title": "Neural Symbolic Machines: Learning Semantic Parsers on Freebase with Weak Supervision", "abstract": "Extending the success of deep neural networks to natural language understanding and symbolic reasoning requires complex operations and external memory. Recent neural program induction approaches have attempted to address this problem, but are typically limited to differentiable memory, and consequently cannot scale beyond small synthetic tasks. In this work, we propose the Manager-Programmer-Computer framework, which integrates neural networks with non-differentiable memory to support abstract, scalable and precise operations through a friendly neural computer interface. Specifically, we introduce a Neural Symbolic Machine, which contains a sequence-to-sequence neural \"programmer\", and a non-differentiable \"computer\" that is a Lisp interpreter with code assist. To successfully apply REINFORCE for training, we augment it with approximate gold programs found by an iterative maximum likelihood training process. NSM is able to learn a semantic parser from weak supervision over a large knowledge base. It achieves new state-of-the-art performance on WebQuestionsSP, a challenging semantic parsing dataset, with weak supervision. Compared to previous approaches, NSM is end-to-end, therefore does not rely on feature engineering or domain specific knowledge.", "histories": [["v1", "Mon, 31 Oct 2016 20:07:23 GMT  (248kb,D)", "http://arxiv.org/abs/1611.00020v1", null], ["v2", "Wed, 2 Nov 2016 05:25:19 GMT  (378kb,D)", "http://arxiv.org/abs/1611.00020v2", "Fixed Latex problem for references"], ["v3", "Thu, 3 Nov 2016 16:24:24 GMT  (246kb,D)", "http://arxiv.org/abs/1611.00020v3", "Fix the Latex compilation problem. In case the problem still exists, we also hosted the PDF version on another link:this https URL"], ["v4", "Sun, 23 Apr 2017 07:16:13 GMT  (400kb,D)", "http://arxiv.org/abs/1611.00020v4", "ACL 2017 camera ready version"]], "reviews": [], "SUBJECTS": "cs.CL cs.AI cs.LG", "authors": ["chen liang", "jonathan berant", "quoc le", "kenneth d forbus", "ni lao"], "accepted": true, "id": "1611.00020"}, "pdf": {"name": "1611.00020.pdf", "metadata": {"source": "CRF", "title": "Neural Symbolic Machines: Learning Semantic Parsers on Freebase with Weak Supervision", "authors": ["Chen Liang", "Jonathan Berant", "Quoc Le", "Kenneth D. Forbus", "Ni Lao"], "emails": ["chenliang2013@u.northwestern.edu", "forbus@u.northwestern.edu", "joberant@cs.tau.ac.il", "qvl@google.com", "nlao@google.com"], "sections": [{"heading": "1 Introduction", "text": "In fact, most of them will be able to play by the rules they had in the past, and they will be able to play by the rules they have established in the past."}, {"heading": "2 Neural Symbolic Machines", "text": "Within the MPC framework, we introduce the Neural Symbolic Machine (NSM) and apply it to semantic parsing. First, we introduce an undifferentiated Lisp interpreter (\"computer\") that executes programs against a large knowledge base and provides code wizards. Then, we describe a sequenceto-sequence neural network model (\"programmer\") that is augmented with a key-variable memory to store and reuse intermediate results for compositivity. Finally, we discuss how to successfully apply REINFORCE to training by adding approximate gold programs that are found with maximum probability through an iterative training process. Before delving into detail, we first define the semantic parsing task: In the face of a knowledge base (KB) K and a question q = (w1, w2,..., wk) we create a program or a logical form that is generated when executed against the correct K."}, {"heading": "2.1 \"Computer\": Lisp interpreter with code assist", "text": "Operations learned through current neural network models with differentiated memories, such as addition or sorting, do not generalize perfectly to inputs that are larger than previously observed. In contrast, operations implemented in ordinary programming languages are abstract, scalable and precise, because no matter how large the input is or whether it was seen or not, it is processed accurately. On the basis of this observation, we implement all operations required for semantic parsing with ordinary, undifferentiated memories, and allow the \"programmer\" to use it with a high general programming language. We take a Lisp interpreter with predefined functions listed in 1 as \"computer.\" \"The programs that can be executed by it are equivalent to the limited subset of calculations in [?], but easier for a sequence-to-model language.\""}, {"heading": "2.2 \"Programmer\": key-variable memory augmented Seq2Seq model", "text": "That's why it's come to this point, it's come to this point, it's never come to this point before. '"}, {"heading": "2.3 Training NSM with Weak Supervision", "text": "Since the Neural Symbolic Machine uses non-differentiable memory, for which traceability cannot be applied, we use REINFORCE [?] for effective training. If the reward signal is very sparse and the search space is large, it is a common practice to use supervised pre-training, but it requires full monitoring. In this work we develop a method to augment REINFORCE by using only weak supervision.REINFORCE Training We can formulate NSM training as an intensified learning problem: given a query q, the state, the action and reward in each time. t {0, 1,..., T} are (st, at, rt).Since the environment is defined by the question q and the action sequence: st = (q, a0: t \u2212 1), where a0: t \u2212 1 = (a0,...) is the history of actions at the time."}, {"heading": "3 Experiments and analysis", "text": "Here we show that NSM is able to learn a semantic parser from weak supervision of a large knowledge base. For evaluation we chose WEBQUESTIONSSP, a challenging semantic parsing dataset with strong baselines. Experiments show that NSM achieves new excellence on WEBQUESTIONSSP with weak supervision. Code support, advanced REINFORCE, curriculum learning and reduction of overfitting are the essential elements of this result, which we will describe in detail."}, {"heading": "3.1 Semantic Parsing and The WEBQUESTIONSSP Dataset", "text": "Modern semantic parsers [?], which map natural language expressions to executable logical forms, have been successfully trained through large knowledge bases of weak oversight [??], but require substantial feature engineering. Recent attempts to train an end-to-end neural network for semantic parsing [??] have either used strong oversight (complete logical forms) or synthetic data packets. We evaluate our NSM model for the task of semantic parsing. Specifically, we used the challenging semantic parsing dataset WEBQUESTIONSSP [?], which consists of 3,098 question-answer pairs for training and 1,639 for testing. These questions were collected using the Google Suggest API and the answers were originally obtained [?] using Amazon Mechanical Turk and updated by annotators familiar with the design of the freebase [?]."}, {"heading": "3.2 Model Details", "text": "The embedding of functions and special tokens (e.g. \"UNK,\" \"GO\") is randomly initialized by an abbreviated normal distribution with mean = 0,0 and stddev = 0,1. All weight matrices are initialized with a uniform distribution in [\u2212 \u221a 3 d, \u221a 3 d], where d is the input dimension. For pre-formed word embedding, we used the 300-dimensional GloVe word embedding, which is configured on 840B common creep corpus [?]. On the encoder side, we added a projection matrix to transform the pre-formed embedding into the 50-dimensional dimension. On the decoder side, we used the same GloVe word embedding to construct the relationships from words in the freebase ID of a relationship."}, {"heading": "3.3 Training Details", "text": "We use Adam Optimizer [?] with an initial learning rate of 0.001 for optimization. In our experiment, this process usually occurs after a few (5-8) iterations. Inspired by the staged generation process in [?], curriculum learning includes two steps. We first perform the iterative ML training procedure for 10 iterations, with programs limited to using only the \"Hop\" function and the maximum number of expressions. Then, we perform the iterative training procedure again, but use both \"Hop\" and \"Equal\" and the maximum number of expressions is 3. However, the relationships used by the \"Hop\" function are limited to those that occurred at best 0: T (q)."}, {"heading": "3.4 Results and Discussion", "text": "We evaluate the performance using the official WEBQUONSSP evaluation script. Since the answer to a question may contain several entities or values, it is calculated based on the results for each question."}, {"heading": "4 Related work", "text": "Among the deep learning models for program induction, Reinforcement Learning Neural Turing Machines (RL-NTMs) [?] are most similar to Neural Symbolic Machines, since an undifferentiated machine is controlled by a sequence model to perform a particular task. Therefore, both rely on REINFORCE for effective training. The main difference between the two is the abstraction level of the programming language. RL-NTM has lower-level operations such as memory address manipulation and byte read / write, while NSM uses a higher programming language over a large knowledge base that includes operations such as following certain relationships of certain entities, or sorting a list of entities based on a property that is more suitable for depicting semantics. We formulate NSM training as an example of reinforcement learning [?], where the task is episodic, rewards are provided in the last step, and the environment consists of the weak oversight of the machine."}, {"heading": "5 Conclusion", "text": "In this paper, we propose the Manager Programmer Computer Framework for Neural Program Induction. It integrates neural networks with non-differentiable memory to support abstract, scalable, and precise operations through a user-friendly neural computer interface. Within this framework, we introduce the Neural Symbolic Machine, which integrates a sequence-to-sequence neural \"programmer\" and a Lisp interpreter with code support. As the interpreter is non-differentiable, we apply reinforcement learning and use approximate gold programs found with maximum probability through the iterative training process to enhance REINFORCE training. NSM achieves new state-of-the-art results on a challenging semantic parsing dataset WEBQUESTIONSSP with weak oversight. Compared to previous approaches, it is end-to-end and therefore does not require feature or domain specific knowledge."}, {"heading": "Acknowledgements", "text": "We thank Arvind Neelakantan, Mohammad Norouzi, Tom Kwiatkowski, Eugene Brevdo, Lukasz Kaizer, Thomas Strohmann, Yonghui Wu, Zhifeng Chen and Alexandre Lacoste for talks and help."}], "references": [], "referenceMentions": [], "year": 2016, "abstractText": "Extending the success of deep neural networks to natural language understanding and symbolic reasoning requires complex operations and external memory. Recent neural program induction approaches have attempted to address this problem, but are typically limited to differentiable memory, and consequently cannot scale beyond small synthetic tasks. In this work, we propose the Manager-ProgrammerComputer framework, which integrates neural networks with non-differentiable memory to support abstract, scalable and precise operations through a friendly neural computer interface. Specifically, we introduce a Neural Symbolic Machine, which contains a sequence-to-sequence neural \"programmer\", and a nondifferentiable \"computer\" that is a Lisp interpreter with code assist. To successfully apply REINFORCE for training, we augment it with approximate gold programs found by an iterative maximum likelihood training process. NSM is able to learn a semantic parser from weak supervision over a large knowledge base. It achieves new state-of-the-art performance on WEBQUESTIONSSP, a challenging semantic parsing dataset, with weak supervision. Compared to previous approaches, NSM is end-to-end, therefore does not rely on feature engineering or domain specific knowledge.", "creator": "LaTeX with hyperref package"}}}