{"id": "1611.00035", "review": {"conference": "NIPS", "VERSION": "v1", "DATE_OF_SUBMISSION": "31-Oct-2016", "title": "Full-Capacity Unitary Recurrent Neural Networks", "abstract": "Recurrent neural networks are powerful models for processing sequential data, but they are generally plagued by vanishing and exploding gradient problems. Unitary recurrent neural networks (uRNNs), which use unitary recurrence matrices, have recently been proposed as a means to avoid these issues. However, in previous experiments, the recurrence matrices were restricted to be a product of parameterized unitary matrices, and an open question remains: when does such a parameterization fail to represent all unitary matrices, and how does this restricted representational capacity limit what can be learned? To address this question, we propose full-capacity uRNNs that optimize their recurrence matrix over all unitary matrices, leading to significantly improved performance over uRNNs that use a restricted-capacity recurrence matrix. Our contribution consists of two main components. First, we provide a theoretical argument to determine if a unitary parameterization has restricted capacity. Using this argument, we show that a recently proposed unitary parameterization has restricted capacity for hidden state dimension greater than 7. Second, we show how a complete, full-capacity unitary recurrence matrix can be optimized over the differentiable manifold of unitary matrices. The resulting multiplicative gradient step is very simple and does not require gradient clipping or learning rate adaptation. We confirm the utility of our claims by empirically evaluating our new full-capacity uRNNs on both synthetic and natural data, achieving superior performance compared to both LSTMs and the original restricted-capacity uRNNs.", "histories": [["v1", "Mon, 31 Oct 2016 20:43:21 GMT  (804kb,D)", "http://arxiv.org/abs/1611.00035v1", "9 pages, to appear in NIPS"]], "COMMENTS": "9 pages, to appear in NIPS", "reviews": [], "SUBJECTS": "stat.ML cs.LG cs.NE", "authors": ["scott wisdom", "thomas powers", "john r hershey", "jonathan le roux", "les e atlas"], "accepted": true, "id": "1611.00035"}, "pdf": {"name": "1611.00035.pdf", "metadata": {"source": "CRF", "title": "Full-Capacity Unitary Recurrent Neural Networks", "authors": ["Scott Wisdom", "Thomas Powers", "John R. Hershey", "Jonathan Le Roux"], "emails": ["atlas}@uw.edu", "leroux}@merl.com"], "sections": [{"heading": "1 Introduction", "text": "This year, it has come to the point that it has only happened once before."}, {"heading": "2 Unitary recurrent neural networks", "text": "The uRNN proposed by Arjovsky et al. [10] consists of the following nonlinear dynamic system, the real or complex value inputs of the dimension M, complex value hidden states of the dimension N and real or complex value outputs of the dimension L: ht = \u03c3b (Wht \u2212 1 + Vxt) yt = Uht + c, (1) where yt = Re {Uht + c} if the outputs yt are real value. Elementary nonlinearity is [\u03c3b (z)] i = (zi | + bi) zi |, if | zi | bi > 0, otherwise. (2) Note that this nonlinearity consists of a soft threshold of the order of magnitude using the bias vector b."}, {"heading": "3 Estimating the representation capacity of structured unitary matrices", "text": "In this case, it is as if it were a real case in which there had been a conspiracy between the two parties. (...) In this case, it is as if it were a conspiracy. (...) It is as if it were a conspiracy. (...) It is as if it were a conspiracy. (...) It is as if it were a conspiracy. (...) It is as if it were a conspiracy. (...) It is as if it were a conspiracy. (...) It is as if it were a conspiracy. (...) It is as if it were a conspiracy. (...) It is as if it were a conspiracy. (...) It is as if it were a conspiracy. (...) It is a conspiracy. (...) (...). (...). (...). (...). (...). (...). (...). (...)."}, {"heading": "4 Optimizing full-capacity unitary matrices on the Stiefel manifold", "text": "In this section we will show how to get around the limitations of the limited capacity and directly optimize a complete uniform matrix. We will consider the Boot Multiplicity of all N \u00b7 N matrices whose columns are N-ormal vectors in CN [14]. Mathematically, the Boot Multiplicity is defined as VN (CN) = {W-CN \u00b7 N: WHW = IN \u00b7 N}. (4) For each W-VN (CN), any matrix Z in the tangent space TWVN (CN) of the Boot Multiplicity fulfills ZHW \u2212 WHZ = 0 [14]. The Boot Multiplicity becomes a Belt Multiplicity if its tangent space is equipped with an inner product. Tagare [14] suggests using the canonical inner product, which is of < Z1, Z2 > c = tr (ZH1 \u2212 12) function ZWH-Zayk = Zayk."}, {"heading": "5 Experiments", "text": "All models are implemented in Theano [16], based on the implementation of capacity-limited uRNNs by [10], available at https: / / github.com / amarshah / complex _ RNN. All the code for replicating our results is available at https: / / github.com / stwisdom / urnn. All models use RMSprop [15] for optimization, except that full uRNNs optimize their recurrence matrices at a fixed learning rate using the update step (6) and the optional RMSprop-style gradient normalization."}, {"heading": "5.1 Synthetic data", "text": "First, we compare the performance of limited capacity uRNNs and LSTMs with synthetic data for two tasks: The first task is the identification of synthetic systems, where an uRNN must learn the dynamics of a target uRNN, giving only samples of the inputs and outputs of the target uRNN; the second task is the problem of copy memory, where the network must retrieve a data sequence after a long period of time."}, {"heading": "5.1.1 System identification", "text": "For the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the"}, {"heading": "5.1.2 Copy memory problem", "text": "The experimental setup follows the copy memory problem of [10], which itself is based on the experiment of [6]. We consider alternative hidden state dimensions and extend the sequence lengths to T = 1000 and T = 2000, which are longer than the maximum length of T = 750, which was taken into account in earlier writings. In this task, the data is a vector of length T + 20 and consists of elements of 10 categories. The vector begins with a sequence of 10 symbols, uniformly ranging from categories 1 to 8. The next T \u2212 1 elements of the vector are the ninth \"empty\" category, followed by an element of the tenth category, the \"delimiter.\" The remaining ten elements are \"empty.\" The task is to output T + 10 empty characters, followed by the sequence of the beginning of the vector. We use average cross entropy as the ninth \"empty\" category, followed by an element of the tenth category, the \"delimiter.\""}, {"heading": "5.2 Speech data", "text": "This year it has come to the point that it will only be a matter of time before it will happen, until it does."}, {"heading": "5.3 Pixel-by-pixel MNIST", "text": "As another challenging long-term storage task with natural data, we test the performance of LSTMs and uRNNs on pixel-by-pixel MNIST and permutated pixel-by-pixel MNIST, first proposed by [5] and used by [10] to test limited capacity uRNNNs. For permutated pixel-by-pixel MNIST, the pixels are mixed, creating some non-local dependencies between pixels in an image. Since the MNIST images are 28 x 28 pixels, pixel-by-pixel sequences result in a length of T = 784 elements. We use 5000 of the 60000 training examples as a validation set to perform early stops with a patience of 5. The loss function is cross-entropy. Weights with the best validation loss are used to process the weights."}, {"heading": "6 Conclusion", "text": "Uniform recurring matrices are proving to be an effective means of solving disappearing and exploding gradient problems. We provided a theoretical argument for quantifying the capacity of limited unified matrices. We also described a method for directly optimizing a unified full-capacity matrix by limiting the gradient to the differentiable multiplicity of unified matrices. The effects of limiting the capacity of the unified weight matrix were tested on system identification and storage tasks where unified recursive full-capacity neural networks (uRNNs) exceeded the limited capacity of uRNs from [10] as well as LSTMs. The fully capacitive uRNNNs also exceeded the limited capacity of uRNNNs from the United States for the protocol size STFT prediction of natural speech signals, and the classification of permutated pixel-for-pixel-pixel-numerical numbers from atom numbers from both STFT and STFT for both STFT-restricted elements."}], "references": [{"title": "Learning long-term dependencies with gradient descent is difficult", "author": ["Y. Bengio", "P. Simard", "P. Frasconi"], "venue": "IEEE Transactions on Neural Networks,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 1994}, {"title": "Gradient flow in recurrent nets: the difficulty of learning long-term dependencies", "author": ["S. Hochreiter", "Y. Bengio", "P. Frasconi", "J. Schmidhuber"], "venue": null, "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2001}, {"title": "On the difficulty of training", "author": ["R. Pascanu", "T. Mikolov", "Y. Bengio"], "venue": "Recurrent Neural Networks", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2012}, {"title": "Exact solutions to the nonlinear dynamics of learning in deep linear neural networks", "author": ["A.M. Saxe", "J.L. McClelland", "S. Ganguli"], "venue": null, "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2013}, {"title": "A simple way to initialize recurrent networks of rectified linear units", "author": ["Q.V. Le", "N. Jaitly", "G.E. Hinton"], "venue": null, "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2015}, {"title": "Long short-term memory", "author": ["S. Hochreiter", "J. Schmidhuber"], "venue": "Neural computation,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 1997}, {"title": "On the properties of neural machine translation: Encoder-decoder approaches", "author": ["K. Cho", "B. van Merri\u00ebnboer", "D. Bahdanau", "Y. Bengio"], "venue": null, "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2014}, {"title": "Deep residual learning for image recognition", "author": ["K. He", "X. Zhang", "S. Ren", "J. Sun"], "venue": null, "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2015}, {"title": "Recurrent models of visual attention", "author": ["V. Mnih", "N. Heess", "A. Graves", "K. Kavukcuoglu"], "venue": "In Advances in Neural Information Processing Systems (NIPS),", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2014}, {"title": "Unitary Evolution Recurrent Neural Networks", "author": ["M. Arjovsky", "A. Shah", "Y. Bengio"], "venue": "In International Conference on Machine Learning", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2016}, {"title": "Unitary triangularization of a nonsymmetric matrix", "author": ["A.S. Householder"], "venue": "Journal of the ACM,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 1958}, {"title": "Lie groups, physics, and geometry: an introduction for physicists, engineers and chemists", "author": ["R. Gilmore"], "venue": null, "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2008}, {"title": "The measure of the critical values of differentiable maps", "author": ["A. Sard"], "venue": "Bulletin of the American Mathematical Society,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 1942}, {"title": "Notes on optimization on Stiefel manifolds", "author": ["H.D. Tagare"], "venue": "Technical report, Yale University,", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2011}, {"title": "Lecture 6.5\u2014RmsProp: Divide the gradient by a running average of its recent magnitude, 2012. COURSERA: Neural Networks for Machine Learning", "author": ["T. Tieleman", "G. Hinton"], "venue": null, "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2012}, {"title": "DARPA TIMIT acoustic-phonetic continous speech corpus", "author": ["J.S. Garofolo", "L.F. Lamel", "W.M. Fisher", "J.G. Fiscus", "D.S. Pallett"], "venue": "Technical Report NISTIR 4930, National Institute of Standards and Technology,", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 1993}, {"title": "Heterogeneous acoustic measurements and multiple classifiers for speech recognition", "author": ["A.K. Halberstadt"], "venue": "PhD thesis, Massachusetts Institute of Technology,", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 1998}, {"title": "VOICEBOX: Speech processing toolbox for MATLAB", "author": ["M. Brookes"], "venue": null, "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2002}, {"title": "An algorithm for intelligibility prediction of timefrequency weighted noisy speech", "author": ["C. Taal", "R. Hendriks", "R. Heusdens", "J. Jensen"], "venue": "IEEE Trans. on Audio, Speech, and Language Processing,", "citeRegEx": "20", "shortCiteRegEx": "20", "year": 2011}, {"title": "Perceptual evaluation of speech quality (PESQ)-a new method for speech quality assessment of telephone networks and codecs", "author": ["A. Rix", "J. Beerends", "M. Hollier", "A. Hekstra"], "venue": "In Proc. ICASSP,", "citeRegEx": "21", "shortCiteRegEx": "21", "year": 2001}, {"title": "Perceptual evaluation of speech quality (PESQ): An objective method for end-to-end speech quality assessment of narrow-band telephone networks and speech", "author": ["ITU-T P"], "venue": null, "citeRegEx": "22", "shortCiteRegEx": "22", "year": 2000}, {"title": "Speech Enhancement: Theory and Practice", "author": ["P.C. Loizou"], "venue": null, "citeRegEx": "23", "shortCiteRegEx": "23", "year": 2007}], "referenceMentions": [{"referenceID": 0, "context": "A primary difficulty in training using gradient-based methods has been the so-called vanishing or exploding gradient problem, in which the instability of the gradients over multiple layers can impede learning [1, 2].", "startOffset": 209, "endOffset": 215}, {"referenceID": 1, "context": "A primary difficulty in training using gradient-based methods has been the so-called vanishing or exploding gradient problem, in which the instability of the gradients over multiple layers can impede learning [1, 2].", "startOffset": 209, "endOffset": 215}, {"referenceID": 2, "context": "This problem has been addressed in the past by various means, including gradient clipping [3], using orthogonal matrices for initialization of the recurrence matrix [4, 5], or by using pioneering architectures such as long short-term memory (LSTM) recurrent networks [6] or gated recurrent units [7].", "startOffset": 90, "endOffset": 93}, {"referenceID": 3, "context": "This problem has been addressed in the past by various means, including gradient clipping [3], using orthogonal matrices for initialization of the recurrence matrix [4, 5], or by using pioneering architectures such as long short-term memory (LSTM) recurrent networks [6] or gated recurrent units [7].", "startOffset": 165, "endOffset": 171}, {"referenceID": 4, "context": "This problem has been addressed in the past by various means, including gradient clipping [3], using orthogonal matrices for initialization of the recurrence matrix [4, 5], or by using pioneering architectures such as long short-term memory (LSTM) recurrent networks [6] or gated recurrent units [7].", "startOffset": 165, "endOffset": 171}, {"referenceID": 5, "context": "This problem has been addressed in the past by various means, including gradient clipping [3], using orthogonal matrices for initialization of the recurrence matrix [4, 5], or by using pioneering architectures such as long short-term memory (LSTM) recurrent networks [6] or gated recurrent units [7].", "startOffset": 267, "endOffset": 270}, {"referenceID": 6, "context": "This problem has been addressed in the past by various means, including gradient clipping [3], using orthogonal matrices for initialization of the recurrence matrix [4, 5], or by using pioneering architectures such as long short-term memory (LSTM) recurrent networks [6] or gated recurrent units [7].", "startOffset": 296, "endOffset": 299}, {"referenceID": 7, "context": "Recently, several innovative architectures have been introduced to improve information flow in a network: residual networks, which directly pass information from previous layers up in a feed-forward network [8], and attention networks, which allow a recurrent network to access past activations [9].", "startOffset": 207, "endOffset": 210}, {"referenceID": 8, "context": "Recently, several innovative architectures have been introduced to improve information flow in a network: residual networks, which directly pass information from previous layers up in a feed-forward network [8], and attention networks, which allow a recurrent network to access past activations [9].", "startOffset": 295, "endOffset": 298}, {"referenceID": 9, "context": "The idea of using a unitary recurrent weight matrix was introduced so that the gradients are inherently stable and do not vanish or explode [10].", "startOffset": 140, "endOffset": 144}, {"referenceID": 9, "context": "Thus, we can show that a previously proposed parameterization [10] cannot represent all unitary matrices larger than 7\u00d7 7.", "startOffset": 62, "endOffset": 66}, {"referenceID": 5, "context": "Our proposed full-capacity uRNNs generally achieve equivalent or superior performance on synthetic and natural data compared to both LSTMs [6] and the original restrictedcapacity uRNNs [10].", "startOffset": 139, "endOffset": 142}, {"referenceID": 9, "context": "Our proposed full-capacity uRNNs generally achieve equivalent or superior performance on synthetic and natural data compared to both LSTMs [6] and the original restrictedcapacity uRNNs [10].", "startOffset": 185, "endOffset": 189}, {"referenceID": 9, "context": "[10] consists of the following nonlinear dynamical system that has real- or complex-valued inputs xt of dimension M , complex-valued hidden states ht of dimension N , and real- or complex-valued outputs yt of dimension L:", "startOffset": 0, "endOffset": 4}, {"referenceID": 9, "context": "[10] propose the following parameterization of the unitary matrix W:", "startOffset": 0, "endOffset": 4}, {"referenceID": 10, "context": "where D are diagonal unitary matrices, R are Householder reflection matrices [11], F is a discrete Fourier transform (DFT) matrix, and P is a permutation matrix.", "startOffset": 77, "endOffset": 81}, {"referenceID": 12, "context": "Then, if P < N, Sard\u2019s theorem [13] implies that the image g(P) of g is of measure zero in U(N), and in particular g is not onto.", "startOffset": 31, "endOffset": 35}, {"referenceID": 13, "context": "We consider the Stiefel manifold of all N \u00d7N complex-valued matrices whose columns are N orthonormal vectors in C [14].", "startOffset": 114, "endOffset": 118}, {"referenceID": 13, "context": "For any W \u2208 VN (C ), any matrix Z in the tangent space TWVN (C ) of the Stiefel manifold satisfies ZW \u2212WZ = 0 [14].", "startOffset": 110, "endOffset": 114}, {"referenceID": 13, "context": "Tagare [14] suggests using the canonical inner product, given by", "startOffset": 7, "endOffset": 11}, {"referenceID": 13, "context": "matrix and G with Gi,j = \u03b4f \u03b4Wi,j is the usual gradient of the loss function f with respect to the matrix W [14].", "startOffset": 108, "endOffset": 112}, {"referenceID": 13, "context": "Using these facts, Tagare [14] suggests a descent curve along the Stiefel manifold at training iteration k given by the matrix product of the Cayley transformation of A with the current solution W: Y(\u03bb) = (", "startOffset": 26, "endOffset": 30}, {"referenceID": 13, "context": "Tagare [14] suggests an Armijo-Wolfe search along the curve to adapt \u03bb, but such a procedure would be expensive for neural network optimization since it requires multiple evaluations of the forward model and gradients.", "startOffset": 7, "endOffset": 11}, {"referenceID": 14, "context": "Also, RMSprop-style scaling of the gradient G by a running average of the previous gradients\u2019 norms [15] before applying the multiplicative step (6) can improve convergence.", "startOffset": 100, "endOffset": 104}, {"referenceID": 9, "context": "All models are implemented in Theano [16], based on the implementation of restricted-capacity uRNNs by [10], available from https://github.", "startOffset": 103, "endOffset": 107}, {"referenceID": 14, "context": "All models use RMSprop [15] for optimization, except that full-capacity uRNNs optimize their recurrence matrices with a fixed learning rate using the update step (6) and optional RMSprop-style gradient normalization.", "startOffset": 23, "endOffset": 27}, {"referenceID": 9, "context": "The experimental setup follows the copy memory problem from [10], which itself was based on the experiment from [6].", "startOffset": 60, "endOffset": 64}, {"referenceID": 5, "context": "The experimental setup follows the copy memory problem from [10], which itself was based on the experiment from [6].", "startOffset": 112, "endOffset": 115}, {"referenceID": 15, "context": "We use the TIMIT dataset [17].", "startOffset": 25, "endOffset": 29}, {"referenceID": 16, "context": "According to common practice [18], we use a training set with 3690 utterances from 462 speakers, a validation set of 400 utterances, an evaluation set of 192 utterances.", "startOffset": 29, "endOffset": 33}, {"referenceID": 14, "context": "For the LSTM and restricted-capacity uRNNs, we use RMSprop [15] with a learning rate of 0.", "startOffset": 59, "endOffset": 63}, {"referenceID": 17, "context": "SegSNR, computed using [19], uses a voice activity detector to avoid measuring SNR in silent frames.", "startOffset": 23, "endOffset": 27}, {"referenceID": 18, "context": "STOI is designed to correlate well with human intelligibility of speech, and takes on values between 0 and 1, with a higher score indicating higher intelligibility [20].", "startOffset": 164, "endOffset": 168}, {"referenceID": 19, "context": "PESQ is the ITU-T standard for telephone voice quality testing [21, 22], and is a popular perceptual quality metric for speech enhancement [23].", "startOffset": 63, "endOffset": 71}, {"referenceID": 20, "context": "PESQ is the ITU-T standard for telephone voice quality testing [21, 22], and is a popular perceptual quality metric for speech enhancement [23].", "startOffset": 63, "endOffset": 71}, {"referenceID": 21, "context": "PESQ is the ITU-T standard for telephone voice quality testing [21, 22], and is a popular perceptual quality metric for speech enhancement [23].", "startOffset": 139, "endOffset": 143}, {"referenceID": 4, "context": "As another challenging long-term memory task with natural data, we test the performance of LSTMs and uRNNs on pixel-by-pixel MNIST and permuted pixel-by-pixel MNIST, first proposed by [5] and used by [10] to test restricted-capacity uRNNs.", "startOffset": 184, "endOffset": 187}, {"referenceID": 9, "context": "As another challenging long-term memory task with natural data, we test the performance of LSTMs and uRNNs on pixel-by-pixel MNIST and permuted pixel-by-pixel MNIST, first proposed by [5] and used by [10] to test restricted-capacity uRNNs.", "startOffset": 200, "endOffset": 204}, {"referenceID": 9, "context": "The effect of restricting the capacity of the unitary weight matrix was tested on system identification and memory tasks, in which full-capacity unitary recurrent neural networks (uRNNs) outperformed restrictedcapacity uRNNs from [10] as well as LSTMs.", "startOffset": 230, "endOffset": 234}], "year": 2016, "abstractText": null, "creator": "LaTeX with hyperref package"}}}