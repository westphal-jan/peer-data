{"id": "1611.00175", "review": {"conference": "NIPS", "VERSION": "v1", "DATE_OF_SUBMISSION": "1-Nov-2016", "title": "Robust Spectral Inference for Joint Stochastic Matrix Factorization", "abstract": "Spectral inference provides fast algorithms and provable optimality for latent topic analysis. But for real data these algorithms require additional ad-hoc heuristics, and even then often produce unusable results. We explain this poor performance by casting the problem of topic inference in the framework of Joint Stochastic Matrix Factorization (JSMF) and showing that previous methods violate the theoretical conditions necessary for a good solution to exist. We then propose a novel rectification method that learns high quality topics and their interactions even on small, noisy data. This method achieves results comparable to probabilistic techniques in several domains while maintaining scalability and provable optimality.", "histories": [["v1", "Tue, 1 Nov 2016 10:06:57 GMT  (507kb,D)", "http://arxiv.org/abs/1611.00175v1", null]], "reviews": [], "SUBJECTS": "cs.LG cs.AI", "authors": ["moontae lee", "david bindel", "david m mimno"], "accepted": true, "id": "1611.00175"}, "pdf": {"name": "1611.00175.pdf", "metadata": {"source": "CRF", "title": "Robust Spectral Inference for Joint Stochastic Matrix Factorization", "authors": ["Moontae Lee", "David Bindel"], "emails": ["moontae@cs.cornell.edu", "bindel@cs.cornell.edu", "mimno@cornell.edu"], "sections": [{"heading": null, "text": "Spectral inference provides fast algorithms and demonstrable optimism for latent topic analysis, but for real data, these algorithms require additional ad hoc heuristics, and even then often lead to useless results. We explain this poor performance by looking at the issue of the Joint Stochastic Matrix Factorization (JSMF) theme conference and showing that previous methods violate the theoretical conditions necessary for a good solution. Subsequently, we propose a novel rectification method that learns high-quality topics and their interactions even on small, noisy data, yielding results comparable to probable techniques in multiple domains while maintaining scalability and demonstrable optimism."}, {"heading": "1 Introduction", "text": "This year it has come to the point that it has never come as far as this year."}, {"heading": "2 Requirements for Factorization", "text": "In this section, we examine the probable and statistical structures of JSMF and then define the geometric structures of co-occurrence matrices required for successful factorization. C-RN \u00b7 N is a common stochastic matrix constructed from M training examples, each of which contains a subset of N objects. We want to find K N latent clusters by dividing the event space of our model into a column stochastic matrix B-RN \u00b7 K and a common stochastic matrix A-RK \u00b7 K. Figure 2 shows the event space of our model. Distribution A across pairs of clusters is initially generated by a stochastic process with a hyperparameter. If the m-th training example contains a total of nm objects, our model assumes that the example consists of all possible nm objects (nm \u2212 1) pairs of objects."}, {"heading": "3 Rectified Anchor Words Algorithm", "text": "In this section, we describe how we estimate the co-occurrence matrix C from the training data, and how we can reform it so that there is a termination of the nodes. (D) The nodes of the nodes of the nodes of the nodes of the nodes of the nodes and the nodes of the nodes of the nodes are not trivial, while 1M M = 1Wm \u2192 E [Wm] asM \u2192 asM \u2192 \"s\" s from the Central Limit Theorem.Generating co-occurrence C. Let Hm be the vector of object counts for the m-th training example, and let pm = BWm whereWm whereWm is the document of the latent topic distribution. ThenHm is believed to be a sample from a multinomial distributionHm."}, {"heading": "4 Experimental Results", "text": "We have also prepared two non-textual item selection datasets: user reviews from the Movielens 10M dataset, 9 and music playlists from the full Yelp dataset.10 We perform similar vocabulary curations, except for frequent stop object elimination."}, {"heading": "5 Analysis of Algorithm", "text": "Why is this problem not solved? Before correction, diagonals of the empirical C matrix is it consistent in terms of the performance of information systems. Why is it so consistent? Before correction, diagonals of the empirical C matrix is it so far from correctness? Because the corresponding lines in the C matrix are very sparse and inaccurate, and these lines are likely to be selected by the pipeline. Since rare objects are likely to be anchors, the matrix CSS is likely to be highly diagonally dominant and provides an uninformative picture of thematic correlations. These problems are exacerbated when K is relatively small to the effective rank of the C matrix, so that a poor anchor presupposes a better choice; and when the number of documents M is small, in which case the empirical C is relatively sparse and heavily influenced by noise."}, {"heading": "6 Related and Future Work", "text": "JSMF is a specific structure-preserving non-negative matrix factorization (NMF) that performs spectral inferences. [19, 20] uses a similar separable structure for NMF problems. In order to tackle hyper-spectral separation problems, [21, 22] we assume pure pixels, a separability equivalent in computer vision. In the more general NMF without such structures, RESCAL [23] investigates tensory expansion of similar themes and SymNMF [24] that fall below BBT and not BABT. For topic modeling, RESCAL [25] performs spectral conclusions on tensors of the third moment or assumes that themes are uncorrelated. Since the core of our algorithm is to reactivate the input event matrix, it can be combined with several more recent developments. [16] one proposes two regulation methods for restoring multiple dimensions of a non-linear space]."}, {"heading": "Acknowledgments", "text": "We thank Adrian Lewis for valuable discussions about AP convergence. 13A setM is prox-regular when PM is unique locally."}], "references": [{"title": "You are who you know: Inferring user profiles in Online Social Networks", "author": ["Alan Mislove", "Bimal Viswanath", "Krishna P. Gummadi", "Peter Druschel"], "venue": "In Proceedings of the 3rd ACM International Conference of Web Search and Data Mining (WSDM\u201910),", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2010}, {"title": "Playlist prediction via metric embedding", "author": ["Shuo Chen", "J. Moore", "D. Turnbull", "T. Joachims"], "venue": "In ACM SIGKDD Conference on Knowledge Discovery and Data Mining (KDD),", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2012}, {"title": "GloVe: Global vectors for word representation", "author": ["Jeffrey Pennington", "Richard Socher", "Christopher D. Manning"], "venue": "In EMNLP,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2014}, {"title": "Neural word embedding as implicit matrix factorization", "author": ["Omer Levy", "Yoav Goldberg"], "venue": "In NIPS,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2014}, {"title": "Learning topic models \u2013 going beyond SVD", "author": ["S. Arora", "R. Ge", "A. Moitra"], "venue": "In FOCS,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2012}, {"title": "A practical algorithm for topic modeling with provable guarantees", "author": ["Sanjeev Arora", "Rong Ge", "Yonatan Halpern", "David Mimno", "Ankur Moitra", "David Sontag", "Yichen Wu", "Michael Zhu"], "venue": "In ICML,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2013}, {"title": "Probabilistic latent semantic analysis", "author": ["T. Hofmann"], "venue": "In UAI, pages 289\u2013296,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 1999}, {"title": "Latent Dirichlet allocation", "author": ["D. Blei", "A. Ng", "M. Jordan"], "venue": "Journal of Machine Learning Research, pages 993\u20131022,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2003}, {"title": "A method for finding projections onto the intersection of convex sets in Hilbert spaces. In Advances in Order Restricted Statistical Inference, volume 37 of Lecture Notes in Statistics, pages 28\u201347", "author": ["JamesP. Boyle", "RichardL. Dykstra"], "venue": null, "citeRegEx": "9", "shortCiteRegEx": "9", "year": 1986}, {"title": "Local linear convergence for alternating and averaged nonconvex projections", "author": ["Adrian S. Lewis", "D.R. Luke", "Jrme Malick"], "venue": "Foundations of Computational Mathematics,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2009}, {"title": "Divide-and-conquer learning by anchoring a conical hull", "author": ["Tianyi Zhou", "Jeff A Bilmes", "Carlos Guestrin"], "venue": "In Advances in Neural Information Processing Systems", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2014}, {"title": "Finding scientific topics", "author": ["T.L. Griffiths", "M. Steyvers"], "venue": "Proceedings of the National Academy of Sciences,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2004}, {"title": "Low-dimensional embeddings for interpretable anchor-based topic inference", "author": ["Moontae Lee", "David Mimno"], "venue": "In Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP),", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2014}, {"title": "Subset selection algorithms: Randomized vs. deterministic", "author": ["Mary E Broadbent", "Martin Brown", "Kevin Penner", "I Ipsen", "R Rehman"], "venue": "SIAM Undergraduate Research Online,", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2010}, {"title": "A correlated topic model of science", "author": ["D. Blei", "J. Lafferty"], "venue": "Annals of Applied Statistics,", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2007}, {"title": "Anchors regularized: Adding robustness and extensibility to scalable topic-modeling algorithms", "author": ["Thang Nguyen", "Yuening Hu", "Jordan Boyd-Graber"], "venue": "In Association for Computational Linguistics,", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2014}, {"title": "Optimizing semantic coherence in topic models", "author": ["David Mimno", "Hanna Wallach", "Edmund Talley", "Miriam Leenders", "Andrew McCallum"], "venue": "In EMNLP,", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2011}, {"title": "Prox-regularity of spectral functions and spectral sets", "author": ["A. Daniilidis", "A.S. Lewis", "J. Malick", "H. Sendov"], "venue": "Journal of Convex Analysis,", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2008}, {"title": "Yes we can: simplex volume maximization for descriptive web-scale matrix factorization", "author": ["Christian Thurau", "Kristian Kersting", "Christian Bauckhage"], "venue": "In CIKM\u201910,", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2010}, {"title": "Fast conical hull algorithms for nearseparable non-negative matrix factorization", "author": ["Abhishek Kumar", "Vikas Sindhwani", "Prabhanjan Kambadur"], "venue": null, "citeRegEx": "20", "shortCiteRegEx": "20", "year": 2012}, {"title": "Vertex component analysis: A fast algorithm to unmix hyperspectral data", "author": ["Jos M.P. Nascimento", "Student Member", "Jos M. Bioucas Dias"], "venue": "IEEE Transactions on Geoscience and Remote Sensing,", "citeRegEx": "21", "shortCiteRegEx": "21", "year": 2005}, {"title": "N-FindR method versus independent component analysis for lithological identification in hyperspectral imagery", "author": ["C\u00e9cile Gomez", "H. Le Borgne", "Pascal Allemand", "Christophe Delacourt", "Patrick Ledru"], "venue": "International Journal of Remote Sensing,", "citeRegEx": "22", "shortCiteRegEx": "22", "year": 2007}, {"title": "A three-way model for collective learning on multi-relational data", "author": ["Maximilian Nickel", "Volker Tresp", "Hans-Peter Kriegel"], "venue": "In Proceedings of the 28th International Conference on Machine Learning (ICML- 11),", "citeRegEx": "23", "shortCiteRegEx": "23", "year": 2011}, {"title": "Symmetric nonnegative matrix factorization for graph clustering", "author": ["Da Kuang", "Haesun Park", "Chris H.Q. Ding"], "venue": "In SDM. SIAM / Omnipress,", "citeRegEx": "24", "shortCiteRegEx": "24", "year": 2012}, {"title": "A spectral algorithm for latent Dirichlet allocation", "author": ["Anima Anandkumar", "Dean P. Foster", "Daniel Hsu", "Sham Kakade", "Yi-Kai Liu"], "venue": "In Advances in Neural Information Processing Systems 25: 26th Annual Conference on Neural Information Processing Systems", "citeRegEx": "25", "shortCiteRegEx": "25", "year": 2012}], "referenceMentions": [{"referenceID": 0, "context": "Communities can be discovered from friendships [1], song genres can be identified from co-occurrence in playlists [2], and neural word embeddings are factorizations of pairwise cooccurrence information [3, 4].", "startOffset": 47, "endOffset": 50}, {"referenceID": 1, "context": "Communities can be discovered from friendships [1], song genres can be identified from co-occurrence in playlists [2], and neural word embeddings are factorizations of pairwise cooccurrence information [3, 4].", "startOffset": 114, "endOffset": 117}, {"referenceID": 2, "context": "Communities can be discovered from friendships [1], song genres can be identified from co-occurrence in playlists [2], and neural word embeddings are factorizations of pairwise cooccurrence information [3, 4].", "startOffset": 202, "endOffset": 208}, {"referenceID": 3, "context": "Communities can be discovered from friendships [1], song genres can be identified from co-occurrence in playlists [2], and neural word embeddings are factorizations of pairwise cooccurrence information [3, 4].", "startOffset": 202, "endOffset": 208}, {"referenceID": 4, "context": "Recent Anchor Word algorithms [5, 6] perform spectral inference on co-occurrence statistics for inferring topic models [7, 8].", "startOffset": 30, "endOffset": 36}, {"referenceID": 5, "context": "Recent Anchor Word algorithms [5, 6] perform spectral inference on co-occurrence statistics for inferring topic models [7, 8].", "startOffset": 30, "endOffset": 36}, {"referenceID": 6, "context": "Recent Anchor Word algorithms [5, 6] perform spectral inference on co-occurrence statistics for inferring topic models [7, 8].", "startOffset": 119, "endOffset": 125}, {"referenceID": 7, "context": "Recent Anchor Word algorithms [5, 6] perform spectral inference on co-occurrence statistics for inferring topic models [7, 8].", "startOffset": 119, "endOffset": 125}, {"referenceID": 5, "context": "Figure 1: 2D visualizations show the low-quality convex hull found by Anchor Words [6] (left) and a better convex hull (middle) found by discovering anchor words on a rectified space (right).", "startOffset": 83, "endOffset": 86}, {"referenceID": 4, "context": "Domain Object Cluster Basis Document Word Topic Anchor Word Image Pixel Segment Pure Pixel Network User Community Representative Legislature Member Party/Group Partisan Playlist Song Genre Signature Song Anchor Word algorithms [5, 6] solve JSMF problems using a separability assumption: each topic contains at least one \u201canchor\u201d word that has non-negligible probability exclusively in that topic.", "startOffset": 227, "endOffset": 233}, {"referenceID": 5, "context": "Domain Object Cluster Basis Document Word Topic Anchor Word Image Pixel Segment Pure Pixel Network User Community Representative Legislature Member Party/Group Partisan Playlist Song Genre Signature Song Anchor Word algorithms [5, 6] solve JSMF problems using a separability assumption: each topic contains at least one \u201canchor\u201d word that has non-negligible probability exclusively in that topic.", "startOffset": 227, "endOffset": 233}, {"referenceID": 4, "context": "The initial algorithm [5] is theoretically sound but unable to produce column-stochastic word-topic matrix B due to unstable matrix inversions.", "startOffset": 22, "endOffset": 25}, {"referenceID": 5, "context": "A subsequent algorithm [6] fixes negative entries in B, but still produces large negative entries in the estimated topic-topic matrix A.", "startOffset": 23, "endOffset": 26}, {"referenceID": 4, "context": "Following [5, 6], our model views B as a set of parameters rather than random variables.", "startOffset": 10, "endOffset": 16}, {"referenceID": 5, "context": "Following [5, 6], our model views B as a set of parameters rather than random variables.", "startOffset": 10, "endOffset": 16}, {"referenceID": 4, "context": "2 in [5] showed that4 AM \u2212\u2192 A\u2217 as M \u2212\u2192\u221e.", "startOffset": 5, "endOffset": 8}, {"referenceID": 4, "context": "The Anchor Word algorithms in [5, 6] consider neither double non-negativity of cluster interactions nor its implication on co-occurrence statistics.", "startOffset": 30, "endOffset": 36}, {"referenceID": 5, "context": "The Anchor Word algorithms in [5, 6] consider neither double non-negativity of cluster interactions nor its implication on co-occurrence statistics.", "startOffset": 30, "endOffset": 36}, {"referenceID": 5, "context": "As in [6], we generate the co-occurrence for the m-th example by", "startOffset": 6, "endOffset": 9}, {"referenceID": 8, "context": "Alternating projection methods like Dykstra\u2019s algorithm [9] allow us to project onto an intersection of finitely many convex sets using projections onto each individual set in turn.", "startOffset": 56, "endOffset": 59}, {"referenceID": 9, "context": "However, [10] show that alternating projection with a non-convex set still works under certain conditions, guaranteeing a local convergence.", "startOffset": 9, "endOffset": 13}, {"referenceID": 5, "context": "[6] use the GramSchmidt process to select anchors, which computes pivoted QR decomposition, but did not utilize the sparsity of C.", "startOffset": 0, "endOffset": 3}, {"referenceID": 5, "context": "After finding the set of basis objects S, we can infer each entry of B by Bayes\u2019 rule as in [6].", "startOffset": 92, "endOffset": 95}, {"referenceID": 10, "context": "To effectively use random projections, it is necessary to either find proper dimensions based on multiple trials or perform low-dimensional random projection multiple times [11] and merge the resulting anchors.", "startOffset": 173, "endOffset": 177}, {"referenceID": 5, "context": "We use an exponentiated gradient algorithm to solve the problem similar to [6].", "startOffset": 75, "endOffset": 78}, {"referenceID": 5, "context": "Figure 3: The algorithm of [6] (first panel) produces negative cluster co-occurrence probabilities.", "startOffset": 27, "endOffset": 30}, {"referenceID": 4, "context": "A probabilistic reconstruction alone (this paper & [5], second panel) removes negative entries but has no offdiagonals and does not sum to one.", "startOffset": 51, "endOffset": 54}, {"referenceID": 5, "context": "[6] recovered A by minimizing \u2016C \u2212 BAB \u2016F ; but the inferred A generally has many negative entries, failing to model the probabilistic interaction between topics.", "startOffset": 0, "endOffset": 3}, {"referenceID": 5, "context": "Our Rectified Anchor Words algorithm with alternating projection fixes many problems in the baseline Anchor Words algorithm [6] while matching the performance of Gibbs sampling [12] and maintaining spectral inference\u2019s determinism and independence from corpus size.", "startOffset": 124, "endOffset": 127}, {"referenceID": 11, "context": "Our Rectified Anchor Words algorithm with alternating projection fixes many problems in the baseline Anchor Words algorithm [6] while matching the performance of Gibbs sampling [12] and maintaining spectral inference\u2019s determinism and independence from corpus size.", "startOffset": 177, "endOffset": 181}, {"referenceID": 4, "context": "We later realized that essentially same approach was previously tried in [5], but it was not able to generate a valid topic-topic matrix as shown in the middle panel of Figure 3.", "startOffset": 73, "endOffset": 76}, {"referenceID": 5, "context": "Although [6] report comparable results to probabilistic algorithms for LDA, the algorithm fails under many circumstances.", "startOffset": 9, "endOffset": 12}, {"referenceID": 12, "context": "2013 (Baseline) neuron layer hidden recognition signal cell noise neuron layer hidden cell signal representation noise neuron layer cell hidden signal noise dynamic neuron layer cell hidden control signal noise neuron layer hidden cell signal recognition noise This paper (AP) neuron circuit cell synaptic signal layer activity control action dynamic optimal policy controller reinforcement recognition layer hidden word speech image net cell field visual direction image motion object orientation gaussian noise hidden approximation matrix bound examples Probabilistic LDA (Gibbs) neuron cell visual signal response field activity control action policy optimal reinforcement dynamic robot recognition image object feature word speech features hidden net layer dynamic neuron recurrent noise gaussian approximation matrix bound component variables Similar to [13], we visualize the five anchor words in the cooccurrence space after 2D PCA of C.", "startOffset": 859, "endOffset": 863}, {"referenceID": 5, "context": "The Baseline algorithm [6] repeats Pulp Fiction and Silence of the Lambs 15 times.", "startOffset": 23, "endOffset": 26}, {"referenceID": 5, "context": "Lines correspond to four methods: \u25e6 Baseline for the algorithm in the previous work [6] without any rectification,4 DC for Diagonal Completion, AP for Alternating Projection, and Gibbs for Gibbs sampling.", "startOffset": 84, "endOffset": 87}, {"referenceID": 13, "context": "Although we expect error to decrease as we increase the number of clusters K, reducing recovery error for a fixed K by choosing better anchors is extremely difficult: no other subset selection algorithm [14] decreased error by more than 0.", "startOffset": 203, "endOffset": 207}, {"referenceID": 14, "context": "11 We expect nontrivial interactions between clusters, even when we do not explicitly model them as in [15].", "startOffset": 103, "endOffset": 107}, {"referenceID": 12, "context": "We do not report held-out probability because we find that relative results are determined by user-defined smoothing parameters [13, 16].", "startOffset": 128, "endOffset": 136}, {"referenceID": 15, "context": "We do not report held-out probability because we find that relative results are determined by user-defined smoothing parameters [13, 16].", "startOffset": 128, "endOffset": 136}, {"referenceID": 16, "context": "While this metric correlates with human evaluation of clusters [17] \u201cworse\u201d coherence can actually be better because the metric does not penalize repetition [13].", "startOffset": 63, "endOffset": 67}, {"referenceID": 12, "context": "While this metric correlates with human evaluation of clusters [17] \u201cworse\u201d coherence can actually be better because the metric does not penalize repetition [13].", "startOffset": 157, "endOffset": 161}, {"referenceID": 5, "context": "In semi-synthetic experiments [6] AP matches Gibbs sampling and outperforms the Baseline, but the discrepancies in topic quality metrics are smaller than in the real experiments (see Appendix).", "startOffset": 30, "endOffset": 33}, {"referenceID": 15, "context": "To mitigate this issue, [16] run exhaustive grid search to find document frequency cutoffs to get informative anchors.", "startOffset": 24, "endOffset": 28}, {"referenceID": 9, "context": ") Why does AP converge? AP enjoys local linear convergence [10] if 1) the initial C is near the convergence point C \u2032, 2) PSDNK is super-regular at C \u2032, and 3) strong regularity holds at C \u2032.", "startOffset": 59, "endOffset": 63}, {"referenceID": 9, "context": ") Checking the third condition a priori is challenging, but we expect noise in the empirical C to prevent an irregular solution, following the argument of Numerical Example 9 in [10].", "startOffset": 178, "endOffset": 182}, {"referenceID": 5, "context": "Because approximation factors of [6] are all computed based on how far C and its co-occurrence shape could be distant from C\u2217\u2019s, all provable guarantees of [6] hold better with our rectified C \u2032.", "startOffset": 33, "endOffset": 36}, {"referenceID": 5, "context": "Because approximation factors of [6] are all computed based on how far C and its co-occurrence shape could be distant from C\u2217\u2019s, all provable guarantees of [6] hold better with our rectified C \u2032.", "startOffset": 156, "endOffset": 159}, {"referenceID": 18, "context": "[19, 20] exploit a similar separable structure for NMF problmes.", "startOffset": 0, "endOffset": 8}, {"referenceID": 19, "context": "[19, 20] exploit a similar separable structure for NMF problmes.", "startOffset": 0, "endOffset": 8}, {"referenceID": 20, "context": "To tackle hyperspectral unmixing problems, [21, 22] assume pure pixels, a separability-equivalent in computer vision.", "startOffset": 43, "endOffset": 51}, {"referenceID": 21, "context": "To tackle hyperspectral unmixing problems, [21, 22] assume pure pixels, a separability-equivalent in computer vision.", "startOffset": 43, "endOffset": 51}, {"referenceID": 22, "context": "In more general NMF without such structures, RESCAL [23] studies tensorial extension of similar factorization and SymNMF [24] infers BB rather than BAB .", "startOffset": 52, "endOffset": 56}, {"referenceID": 23, "context": "In more general NMF without such structures, RESCAL [23] studies tensorial extension of similar factorization and SymNMF [24] infers BB rather than BAB .", "startOffset": 121, "endOffset": 125}, {"referenceID": 24, "context": "For topic modeling, [25] performs spectral inference on third moment tensor assuming topics are uncorrelated.", "startOffset": 20, "endOffset": 24}, {"referenceID": 15, "context": "[16] proposes two regularization methods for recovering better B.", "startOffset": 0, "endOffset": 4}, {"referenceID": 12, "context": "[13] nonlinearly projects co-occurrence to low-dimensional space via t-SNE and achieves better anchors by finding the exact anchors in that space.", "startOffset": 0, "endOffset": 4}, {"referenceID": 10, "context": "[11] performs multiple random projections to low-dimensional spaces and recovers approximate anchors efficiently by divide-and-conquer strategy.", "startOffset": 0, "endOffset": 4}], "year": 2016, "abstractText": "Spectral inference provides fast algorithms and provable optimality for latent topic analysis. But for real data these algorithms require additional ad-hoc heuristics, and even then often produce unusable results. We explain this poor performance by casting the problem of topic inference in the framework of Joint Stochastic Matrix Factorization (JSMF) and showing that previous methods violate the theoretical conditions necessary for a good solution to exist. We then propose a novel rectification method that learns high quality topics and their interactions even on small, noisy data. This method achieves results comparable to probabilistic techniques in several domains while maintaining scalability and provable optimality.", "creator": "LaTeX with hyperref package"}}}