{"id": "1611.00179", "review": {"conference": "NIPS", "VERSION": "v1", "DATE_OF_SUBMISSION": "1-Nov-2016", "title": "Dual Learning for Machine Translation", "abstract": "While neural machine translation (NMT) is making good progress in the past two years, tens of millions of bilingual sentence pairs are needed for its training. However, human labeling is very costly. To tackle this training data bottleneck, we develop a dual-learning mechanism, which can enable an NMT system to automatically learn from unlabeled data through a dual-learning game. This mechanism is inspired by the following observation: any machine translation task has a dual task, e.g., English-to-French translation (primal) versus French-to-English translation (dual); the primal and dual tasks can form a closed loop, and generate informative feedback signals to train the translation models, even if without the involvement of a human labeler. In the dual-learning mechanism, we use one agent to represent the model for the primal task and the other agent to represent the model for the dual task, then ask them to teach each other through a reinforcement learning process. Based on the feedback signals generated during this process (e.g., the language-model likelihood of the output of a model, and the reconstruction error of the original sentence after the primal and dual translations), we can iteratively update the two models until convergence (e.g., using the policy gradient methods). We call the corresponding approach to neural machine translation \\emph{dual-NMT}. Experiments show that dual-NMT works very well on English$\\leftrightarrow$French translation; especially, by learning from monolingual data (with 10% bilingual data for warm start), it achieves a comparable accuracy to NMT trained from the full bilingual data for the French-to-English translation task.", "histories": [["v1", "Tue, 1 Nov 2016 10:38:29 GMT  (46kb,D)", "http://arxiv.org/abs/1611.00179v1", null]], "reviews": [], "SUBJECTS": "cs.CL", "authors": ["di he", "yingce xia", "tao qin", "liwei wang 0001", "nenghai yu", "tie-yan liu", "wei-ying ma"], "accepted": true, "id": "1611.00179"}, "pdf": {"name": "1611.00179.pdf", "metadata": {"source": "CRF", "title": "Dual Learning for Machine Translation", "authors": ["Yingce Xia", "Di He", "Tao Qin", "Liwei Wang", "Nenghai Yu", "Tie-Yan Liu", "Wei-Ying Ma"], "emails": ["yingce.xia@gmail.com;", "ynh@ustc.edu.cn", "dih@cis.pku.edu.cn;", "wanglw@cis.pku.edu.cn;", "taoqin@microsoft.com", "tie-yan.liu@microsoft.com", "wyma@microsoft.com"], "sections": [{"heading": "1 Introduction", "text": "This year is the highest in the history of the country."}, {"heading": "2 Background: Neural Machine Translation", "text": "In principle, our dual learning framework can be applied to both phrase-based statistical machine translation and neural machine translation. (In this work, we focus on the latter, i.e. Neural Machine Translation (NMT), because of its simplicity as an end-to-end system without suffering from human manufacturing [6]. Neural machine translation systems are typically implemented using a recursive neural network (RNN) based on encoder decoder frames. Such a framework learns a probabilistic mapping P (y | x) from a source-language sentence x = {x1, x2,..., xTx} to a target-language sentence y = {y1, yTy}, in which xi and yt are the i-th and t-th words for sentences x and y."}, {"heading": "3 Dual Learning for Neural Machine Translation", "text": "In this section, we present the dual learning mechanism for neural machine translation. Note that MT can (always) happen in two directions, we first design a two-agent game with a forward translation step and a backward translation step that can provide qualitative feedback to the dual translation models, which even include sentences from language A and B. Please note that these two models are not necessarily interconnected, and that they even have a topical relationship to each other. Translation of two (weak) translation models that contain sentences from language A and B. Please note that these two models are not necessarily aligned, and that they each have two (weak) translation models that can translate sentences from A and Visa. Our goal is to improve the accuracy of the two models by using the parallel corpora."}, {"heading": "4 Experiments", "text": "We conducted a series of experiments to test the proposed dual learning mechanism for neural machine translation."}, {"heading": "4.1 Settings", "text": "We compared our dual NMT approach with two baselines: the standard neural machine translation [1] (NMT for a short time), and a current NMT-based method [12], which covers the pseudo-bilingual pairs of sentences from the monolingualism theory to the basic concepts of the Reformation from the Reformation theory to the Reformation practice from the Reformation theory to the practice from the Reformation theory to the practice from the Reformation theory to the Reformation theory from the Reformation theory to the Reformation theory from the Reformation theory to the Reformation practice from the Reformation theory to the Reformation theory from the Reformation theory to the Reformation theory from the Reformation theory from the Reformation theory to the Reformation theory from the Reformation theory and practice."}, {"heading": "4.2 Results and Analysis", "text": "This year it has come to the point that it has never come as far as this year."}, {"heading": "5 Discussions", "text": "In this section, we discuss the possible enhancements to our proposed dual learning mechanism and list several future machine translation work. First, although we have focused on machine translation in this work, the basic idea of dual learning is generally applicable: as long as two tasks are available in dual form, we can use the dual learning mechanism to simultaneously learn both tasks from unlabelled data using reinforcement learning algorithms. In fact, many AI tasks are of course dual form, such as speech recognition versus text generation, question versus question (e.g. Jeopardy!), search (matching queries to documents) versus keyword extraction (extracting keywords / queries to documents), etc. It would be very interesting to develop and test dual learning algorithms for more dual tasks than question. Second, although we have focused on dual learning processes on two tasks, our technology is not limited to two tasks."}], "references": [{"title": "Neural machine translation by jointly learning to align and translate", "author": ["D. Bahdanau", "K. Cho", "Y. Bengio"], "venue": "ICLR", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2015}, {"title": "Large language models in machine translation", "author": ["T. Brants", "A.C. Popat", "P. Xu", "F.J. Och", "J. Dean"], "venue": "In Proceedings of the Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning. Citeseer", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2007}, {"title": "Agreement-based joint training for bidirectional attention-based neural machine translation", "author": ["Y. Cheng", "S. Shen", "Z. He", "W. He", "H. Wu", "M. Sun", "Y. Liu"], "venue": "IJCAI", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2016}, {"title": "Learning phrase representations using rnn encoder\u2013decoder for statistical machine translation", "author": ["K. Cho", "B. van Merrienboer", "C. Gulcehre", "D. Bahdanau", "F. Bougares", "H. Schwenk", "Y. Bengio"], "venue": "In Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP),", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2014}, {"title": "On using monolingual corpora in neural machine translation", "author": ["C. Gulcehre", "O. Firat", "K. Xu", "K. Cho", "L. Barrault", "H.-C. Lin", "F. Bougares", "H. Schwenk", "Y. Bengio"], "venue": "arXiv preprint arXiv:1503.03535", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2015}, {"title": "On using very large target vocabulary for neural machine translation", "author": ["S. Jean", "K. Cho", "R. Memisevic", "Y. Bengio"], "venue": "In Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing (Volume 1: Long Papers),", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2015}, {"title": "Statistical phrase-based translation", "author": ["P. Koehn", "F.J. Och", "D. Marcu"], "venue": "Proceedings of the 2003 Conference of the North American Chapter of the Association for Computational Linguistics on Human Language Technology-Volume 1, pages 48\u201354. Association for Computational Linguistics", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2003}, {"title": "Recurrent neural network based language model", "author": ["T. Mikolov", "M. Karafi\u00e1t", "L. Burget", "J. Cernock\u1ef3", "S. Khudanpur"], "venue": "INTERSPEECH, volume 2, page 3", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2010}, {"title": "Bleu: a method for automatic evaluation of machine translation", "author": ["K. Papineni", "S. Roukos", "T. Ward", "W.-J. Zhu"], "venue": "Proceedings of the 40th annual meeting on association for computational linguistics, pages 311\u2013318. Association for Computational Linguistics", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2002}, {"title": "Sequence level training with recurrent neural networks", "author": ["M. Ranzato", "S. Chopra", "M. Auli", "W. Zaremba"], "venue": "arXiv preprint arXiv:1511.06732", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2015}, {"title": "A neural attention model for abstractive sentence summarization", "author": ["A.M. Rush", "S. Chopra", "J. Weston"], "venue": "In Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2015}, {"title": "Improving neural machine translation models with monolingual data", "author": ["R. Sennrich", "B. Haddow", "A. Birch"], "venue": "ACL", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2016}, {"title": "Minimum risk training for neural machine translation", "author": ["S. Shen", "Y. Cheng", "Z. He", "W. He", "H. Wu", "M. Sun", "Y. Liu"], "venue": "ACL", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2016}, {"title": "Sequence to sequence learning with neural networks", "author": ["I. Sutskever", "O. Vinyals", "Q.V. Le"], "venue": "Advances in neural information processing systems, pages 3104\u20133112", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2014}, {"title": "et al", "author": ["R.S. Sutton", "D.A. McAllester", "S.P. Singh", "Y. Mansour"], "venue": "Policy gradient methods for reinforcement learning with function approximation. In NIPS, volume 99, pages 1057\u20131063", "citeRegEx": "15", "shortCiteRegEx": null, "year": 1999}, {"title": "Semi-supervised model adaptation for statistical machine translation", "author": ["N. Ueffing", "G. Haffari", "A. Sarkar"], "venue": "Machine Translation Journal", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2008}, {"title": "Adadelta: an adaptive learning rate method", "author": ["M.D. Zeiler"], "venue": "arXiv preprint arXiv:1212.5701", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2012}], "referenceMentions": [{"referenceID": 6, "context": "State-of-the-art machine translation (MT) systems, including both the phrase-based statistical translation approaches [7, 4, 14] and the recently emerged neural networks based translation approaches [1, 6], heavily rely on aligned parallel training corpora.", "startOffset": 118, "endOffset": 128}, {"referenceID": 3, "context": "State-of-the-art machine translation (MT) systems, including both the phrase-based statistical translation approaches [7, 4, 14] and the recently emerged neural networks based translation approaches [1, 6], heavily rely on aligned parallel training corpora.", "startOffset": 118, "endOffset": 128}, {"referenceID": 13, "context": "State-of-the-art machine translation (MT) systems, including both the phrase-based statistical translation approaches [7, 4, 14] and the recently emerged neural networks based translation approaches [1, 6], heavily rely on aligned parallel training corpora.", "startOffset": 118, "endOffset": 128}, {"referenceID": 0, "context": "State-of-the-art machine translation (MT) systems, including both the phrase-based statistical translation approaches [7, 4, 14] and the recently emerged neural networks based translation approaches [1, 6], heavily rely on aligned parallel training corpora.", "startOffset": 199, "endOffset": 205}, {"referenceID": 5, "context": "State-of-the-art machine translation (MT) systems, including both the phrase-based statistical translation approaches [7, 4, 14] and the recently emerged neural networks based translation approaches [1, 6], heavily rely on aligned parallel training corpora.", "startOffset": 199, "endOffset": 205}, {"referenceID": 1, "context": "In the first category [2, 5], monolingual corpora in the target language are used to train a language model, which is then integrated with the MT models trained from parallel bilingual corpora to improve the translation quality.", "startOffset": 22, "endOffset": 28}, {"referenceID": 4, "context": "In the first category [2, 5], monolingual corpora in the target language are used to train a language model, which is then integrated with the MT models trained from parallel bilingual corpora to improve the translation quality.", "startOffset": 22, "endOffset": 28}, {"referenceID": 15, "context": "In the second category [16, 12], pseudo bilingual sentence pairs are generated from monolingual data by using the translation model trained from aligned parallel corpora, and then these pseudo bilingual sentence \u2217The first two authors contributed equally to this work.", "startOffset": 23, "endOffset": 31}, {"referenceID": 11, "context": "In the second category [16, 12], pseudo bilingual sentence pairs are generated from monolingual data by using the translation model trained from aligned parallel corpora, and then these pseudo bilingual sentence \u2217The first two authors contributed equally to this work.", "startOffset": 23, "endOffset": 31}, {"referenceID": 5, "context": ", neural machine translation (NMT), due to its simplicity as an end-to-end system, without suffering from human crafted engineering [6].", "startOffset": 132, "endOffset": 135}, {"referenceID": 13, "context": "in which hi is the hidden state at time i, and function f is the recurrent unit such as Long Short-Term Memory (LSTM) unit [14] or Gated Recurrent Unit (GRU) [4].", "startOffset": 123, "endOffset": 127}, {"referenceID": 3, "context": "in which hi is the hidden state at time i, and function f is the recurrent unit such as Long Short-Term Memory (LSTM) unit [14] or Gated Recurrent Unit (GRU) [4].", "startOffset": 158, "endOffset": 161}, {"referenceID": 3, "context": "ct can be a \u2018global\u2019 signal summarizing sentence x [4, 14], e.", "startOffset": 51, "endOffset": 58}, {"referenceID": 13, "context": "ct can be a \u2018global\u2019 signal summarizing sentence x [4, 14], e.", "startOffset": 51, "endOffset": 58}, {"referenceID": 0, "context": ", c1 = \u00b7 \u00b7 \u00b7 = cTy = hTx , or \u2018local\u2019 signal implemented by an attention mechanism [1], e.", "startOffset": 83, "endOffset": 86}, {"referenceID": 14, "context": "As the reward of the game can be considered as a function of s, smid and translation models \u0398AB and \u0398BA, we can optimize the parameters in the translation models through policy gradient methods for reward maximization, as widely used in reinforcement learning [15].", "startOffset": 260, "endOffset": 264}, {"referenceID": 14, "context": "According to the policy gradient theorem [15], it is easy to verify that \u2207\u0398BAE[r] = E[(1\u2212 \u03b1)\u2207\u0398BA logP (s|smid; \u0398BA)] (6)", "startOffset": 41, "endOffset": 45}, {"referenceID": 9, "context": "Considering that random sampling brings very large variance and sometimes unreasonable results in machine translation [10, 14, 11], we use beam search [14] to obtain more meaningful results (more", "startOffset": 118, "endOffset": 130}, {"referenceID": 13, "context": "Considering that random sampling brings very large variance and sometimes unreasonable results in machine translation [10, 14, 11], we use beam search [14] to obtain more meaningful results (more", "startOffset": 118, "endOffset": 130}, {"referenceID": 10, "context": "Considering that random sampling brings very large variance and sometimes unreasonable results in machine translation [10, 14, 11], we use beam search [14] to obtain more meaningful results (more", "startOffset": 118, "endOffset": 130}, {"referenceID": 13, "context": "Considering that random sampling brings very large variance and sometimes unreasonable results in machine translation [10, 14, 11], we use beam search [14] to obtain more meaningful results (more", "startOffset": 151, "endOffset": 155}, {"referenceID": 0, "context": "We compared our dual-NMT approach with two baselines: the standard neural machine translation [1] (NMT for short), and a recent NMT-based method [12] which generates pseudo bilingual sentence pairs from monolingual corpora to assist training (pseudo-NMT for short).", "startOffset": 94, "endOffset": 97}, {"referenceID": 11, "context": "We compared our dual-NMT approach with two baselines: the standard neural machine translation [1] (NMT for short), and a recent NMT-based method [12] which generates pseudo bilingual sentence pairs from monolingual corpora to assist training (pseudo-NMT for short).", "startOffset": 145, "endOffset": 149}, {"referenceID": 0, "context": "In detail, we used the same bilingual corpora from WMT\u201914 as used in [1, 6], which contains 12M sentence pairs extracting from five datasets: Europarl v7, Common Crawl corpus, UN corpus, News Commentary, and 10French-English corpus.", "startOffset": 69, "endOffset": 75}, {"referenceID": 5, "context": "In detail, we used the same bilingual corpora from WMT\u201914 as used in [1, 6], which contains 12M sentence pairs extracting from five datasets: Europarl v7, Common Crawl corpus, UN corpus, News Commentary, and 10French-English corpus.", "startOffset": 69, "endOffset": 75}, {"referenceID": 0, "context": "We used the GRU networks and followed the practice in [1] to set experimental parameters.", "startOffset": 54, "endOffset": 57}, {"referenceID": 0, "context": "For the baseline NMT model, we exactly followed the settings reported in [1].", "startOffset": 73, "endOffset": 76}, {"referenceID": 11, "context": "For the baseline pseudo-NMT [12], we used the trained NMT model to generate pseudo bilingual sentence pairs from monolingual data, removed the sentences with more than 50 words, merged the generated data with the original parallel training data, and then trained the model for testing.", "startOffset": 28, "endOffset": 32}, {"referenceID": 16, "context": "Each of the baseline models was trained with AdaDelta [17] on K40m GPU until their performances stopped to improve on the validation set.", "startOffset": 54, "endOffset": 58}, {"referenceID": 7, "context": "We trained an RNN based language model [8] for each language using its corresponding monolingual corpus.", "startOffset": 39, "endOffset": 42}, {"referenceID": 8, "context": "We used the BLEU score [9] as the evaluation metric, which are computed by the multi-bleu.", "startOffset": 23, "endOffset": 26}, {"referenceID": 13, "context": "Following the common practice, during testing we used beam search [14] with beam size of 12 for all the algorithms as in many previous works.", "startOffset": 66, "endOffset": 70}, {"referenceID": 5, "context": "System BLEU RNNSearch[6] 29.", "startOffset": 21, "endOffset": 24}, {"referenceID": 5, "context": "08 RNNSearch-LV with 500k source/target words[6] 32.", "startOffset": 45, "endOffset": 48}, {"referenceID": 12, "context": "11 MRT [13] 31.", "startOffset": 7, "endOffset": 11}, {"referenceID": 5, "context": "We use the method proposed by [6] to deal with unknown words.", "startOffset": 30, "endOffset": 33}], "year": 2016, "abstractText": "While neural machine translation (NMT) is making good progress in the past two years, tens of millions of bilingual sentence pairs are needed for its training. However, human labeling is very costly. To tackle this training data bottleneck, we develop a dual-learning mechanism, which can enable an NMT system to automatically learn from unlabeled data through a dual-learning game. This mechanism is inspired by the following observation: any machine translation task has a dual task, e.g., English-to-French translation (primal) versus French-to-English translation (dual); the primal and dual tasks can form a closed loop, and generate informative feedback signals to train the translation models, even if without the involvement of a human labeler. In the dual-learning mechanism, we use one agent to represent the model for the primal task and the other agent to represent the model for the dual task, then ask them to teach each other through a reinforcement learning process. Based on the feedback signals generated during this process (e.g., the languagemodel likelihood of the output of a model, and the reconstruction error of the original sentence after the primal and dual translations), we can iteratively update the two models until convergence (e.g., using the policy gradient methods). We call the corresponding approach to neural machine translation dual-NMT. Experiments show that dual-NMT works very well on English\u2194French translation; especially, by learning from monolingual data (with 10% bilingual data for warm start), it achieves a comparable accuracy to NMT trained from the full bilingual data for the French-to-English translation task.", "creator": "LaTeX with hyperref package"}}}