{"id": "1611.00336", "review": {"conference": "NIPS", "VERSION": "v1", "DATE_OF_SUBMISSION": "1-Nov-2016", "title": "Stochastic Variational Deep Kernel Learning", "abstract": "Deep kernel learning combines the non-parametric flexibility of kernel methods with the inductive biases of deep learning architectures. We propose a novel deep kernel learning model and stochastic variational inference procedure which generalizes deep kernel learning approaches to enable classification, multi-task learning, additive covariance structures, and stochastic gradient training. Specifically, we apply additive base kernels to subsets of output features from deep neural architectures, and jointly learn the parameters of the base kernels and deep network through a Gaussian process marginal likelihood objective. Within this framework, we derive an efficient form of stochastic variational inference which leverages local kernel interpolation, inducing points, and structure exploiting algebra. We show improved performance over stand alone deep networks, SVMs, and state of the art scalable Gaussian processes on several classification benchmarks, including an airline delay dataset containing 6 million training points, CIFAR, and ImageNet.", "histories": [["v1", "Tue, 1 Nov 2016 19:04:47 GMT  (223kb,D)", "http://arxiv.org/abs/1611.00336v1", "13 pages, 6 tables, 3 figures. Appearing in NIPS 2016"], ["v2", "Wed, 2 Nov 2016 18:06:16 GMT  (223kb,D)", "http://arxiv.org/abs/1611.00336v2", "13 pages, 6 tables, 3 figures. Appearing in NIPS 2016"]], "COMMENTS": "13 pages, 6 tables, 3 figures. Appearing in NIPS 2016", "reviews": [], "SUBJECTS": "stat.ML cs.LG stat.ME", "authors": ["andrew gordon wilson", "zhiting hu", "ruslan salakhutdinov", "eric p xing"], "accepted": true, "id": "1611.00336"}, "pdf": {"name": "1611.00336.pdf", "metadata": {"source": "CRF", "title": "Stochastic Variational Deep Kernel Learning", "authors": ["Andrew Gordon Wilson", "Zhiting Hu", "Eric P. Xing"], "emails": [], "sections": [{"heading": "1 Introduction", "text": "This year it is so far that it only takes a few days to get there, to get to the point where an agreement can be reached."}, {"heading": "2 Background", "text": "The fact is that you will be able to move to another world, where you have to move to another world, where you will be able to move to another world, where you will be able to move to another world, where you will be able to create another world, where you will be able to explore another world, where you will be able to explore another world, where you will be able to explore another world, where you will be able to explore another world, where you will be able to create a new world, where you will be able to create a new world."}, {"heading": "3 Deep Kernel Learning for Multi-task Classification", "text": "We propose a new deep kernel learning approach to take into account the classification and the non-Gaussian probabilities. We propose to build a probabilistic deep network as follows: 1) a deep nonlinear transformation h (x, w), parameterized by weights w, is applied to the observed input variable x to produce Q characteristics at the last layer L, h (L) 1,.., h (L) Q; 2) J Gaussian processes, with base nuclei k1,., kJ, are applied to subsets of these characteristics that correspond to an additive GP model. Therefore, the base kernels can react to relatively low dimensional inputs in which local kernel interpolation and learning biases are based like similarities based on Euclidean distance."}, {"heading": "4 Structure Exploiting Stochastic Variational Inference", "text": "It is not only the manner in which we move in the world, but also the manner in which we move in the world, in which we move in the world, in which we move in the world, in which we move in the world, in which we move in the world, in which we move in the world, in which we move in the world, in which we move in the world, in which we move in the world, in the world, in which we move in the world, in the world, in the world, in the world, in the world, in the world, in the world, in the world, in the world, in the world, in the world, in the world, in the world, in the world, in the world, in the world, in the world, in the world, in the world, in the world, in the world, in the world, in the world, in the world, in the world, in the world, in the world, in the world, in the world, in the world, in the world, in the world, in the world, in the world, in the world, in the world, in the world, in the world, in the world, in the world, in the world, in the world, in the world, in the world, in the world, in the world, in the world, in the world, in the world, in the world, in the world, in the world, in the world, in the world, in the world, in the world, in the world, in the world, in the world, in the world, in the world, in the world, in the world, in the world, in the world, in the world, in the world, in the world, in the world, in the world, in the world, in the world, in the world, in the world, in the world, in the world, in the world, in the world, in the world, in the world, in the world, in the world, in the world, in the world, in the world, in the world, in the world, in the world, in the world, in the world, in the world, in the world, in which we, in the world, in the world, in the world, in which we, in the world, in the world, in the world, in the world, in the world, in the world,"}, {"heading": "5 Experiments", "text": "We evaluate our proposed approach, stochastic variational deep kernel learning (SV-DKL), on a wide range of classification problems, including an airline delay task with over 5.9 million data points (section 5.1), a large and diverse collection of classification problems from the UCI repository (section 5.2), and image classification benchmarks (section 5.3). Empirical results demonstrate the practical importance of our approach, which provides consistent improvements over stand-alone DNNs, while simultaneously providing GP representation and dramatic improvements in speed and accuracy over modern state-of-the-art GP models. We use classification accuracy when comparing to DNNs because it is a standard for evaluating classification benchmarks with DNNs. However, we also calculate the negative log probability values (NLP), which show similar trends. All experiments were performed on a Linux machine with eight 4.0 GHz CPU-es, Corone Tesla GPU 40PU, and our 32PU networks."}, {"heading": "5.1 Airline Delays", "text": "This year it has come to the point that it has never come as far as this year."}, {"heading": "5.2 UCI Classification Tasks", "text": "The second evaluation of our proposed algorithm (SV-DKL) is based on a number of commonly used UCI classification tasks of different sizes and properties. Table 2 lists the classification accuracy of SVM, DNN, DNN + (a standalone DNN with an additional Q \u00b7 c fully connected hidden layer with Q, c as defined in Figure 1), DNN + GP (a GP trained on the topmost features of a trained DNN without the additional hidden layer), and SV-DKL (the same architecture as DNN). Pure DNN, which effectively learns important features from raw data, offers significantly higher accuracy than SVM, the most commonly used core method for classification problems. We see that the additional layer in DNN + GP can sometimes affect performance. In contrast, the non-parametric flexibility of DNN + GP continuously improves on DNN. And SV-DKL improves through the formation of a DNN target, in particular through a consistent DNN-GP improvement in probability."}, {"heading": "5.3 Image Classification", "text": "Next, we evaluate the proposed scalable SV-DKL method for the efficient handling of high-dimensional highly structured net data. We used a minibatch size of 5,000 for stochastic gradient training of SV-DKL. Table 3 compares SV-DKL with the most recent scalable GP classifiers. In addition to KLSP-GP, we also collected the results of the MC-GP [9], which uses a hybrid Monte Carlo sampler to address non-conjugated probabilities, SAVI-GP [7], which approaches a universal Gaussian sampler, and the distributed GP-latent variable model (referred to as D-GPLVM) [10]. We see that SV-DKL improves the respective benchmark tasks by a large distance across all of the scalable GP methods mentioned above, as well as the distributed GP-latent variable model (referred to as D-GPLVM)."}, {"heading": "5.3.1 Interpretation", "text": "In Figure 3 (a), we examine the deep cores we have learned on the MNIST dataset by randomly selecting 4 classes and visualizing the covariance matrices of the respective dimensions; the covariance matrices are evaluated on the basis of the test inputs sorted by the names of the input images; we see that the deep core on each dimension effectively detects the correlations between the images within the corresponding class. In c = 2, for example, the data points between 2k-3k (i.e. images of the number 2) are strongly correlated with each other and have little correlation with the rest of the images; in addition, we can clearly observe that the remaining data points also form several \"blocks\" instead of being clustered without structure, confirming that the DKL method and the additive GPs capture the correlations across different dimensions. In order to further study the learned dependencies between the output classes and the additive GPs, they form a gradient in addition to the high weighting of the 3."}, {"heading": "6 Discussion", "text": "We have introduced a scalable Gaussian process model that harnesses deep learning, stochastic variation conclusions, structure using algebra, and additive covariance structures, and the resulting deep core learning approach, SV-DKL, enables classifications and non-Gaussian probabilities, multi-task learning, and mini-batch training. SV-DKL achieves superior performance over alternative scalable GP models and standalone deep networks on many important benchmarks. Several fundamental issues arise from exposure: (1) core methods and deep learning approaches are complementary, and we can combine the advantages of each approach; (2) expressive core functions are particularly valuable on large data sets; (3) by looking at neural networks through the lens of metric learning, deep learning approaches become more interpretable. Deep learning is able to achieve good predictable accuracy by providing automated learning structures that would be difficult for engineers to interpret."}, {"heading": "A Negative Log Probability (NLP) Results", "text": "Tables 4, 5 and 6 show the negative log probability values for different tasks. In general, we observed similar trends as for classification accuracy results."}, {"heading": "B Stochastic Variational Inference for Deep Kernel Learning Classification", "text": "Recall the SV-DKL classification mapping, we can get latent function from the samples of we (u)."}], "references": [], "referenceMentions": [], "year": 2016, "abstractText": "Deep kernel learning combines the non-parametric flexibility of kernel methods<lb>with the inductive biases of deep learning architectures. We propose a novel deep<lb>kernel learning model and stochastic variational inference procedure which gener-<lb>alizes deep kernel learning approaches to enable classification, multi-task learning,<lb>additive covariance structures, and stochastic gradient training. Specifically, we<lb>apply additive base kernels to subsets of output features from deep neural archi-<lb>tectures, and jointly learn the parameters of the base kernels and deep network<lb>through a Gaussian process marginal likelihood objective. Within this framework,<lb>we derive an efficient form of stochastic variational inference which leverages local<lb>kernel interpolation, inducing points, and structure exploiting algebra. We show<lb>improved performance over stand alone deep networks, SVMs, and state of the<lb>art scalable Gaussian processes on several classification benchmarks, including an<lb>airline delay dataset containing 6 million training points, CIFAR, and ImageNet.", "creator": "LaTeX with hyperref package"}}}