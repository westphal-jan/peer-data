{"id": "1611.00429", "review": {"conference": "ICML", "VERSION": "v1", "DATE_OF_SUBMISSION": "2-Nov-2016", "title": "Distributed Mean Estimation with Limited Communication", "abstract": "Motivated by the need for distributed optimization algorithms with low communication cost, we study communication efficient algorithms to perform distributed mean estimation. We study scenarios in which each client sends one bit per dimension. We first show that for $d$ dimensional data with $n$ clients, a naive stochastic rounding approach yields a mean squared error $\\Theta(d/n)$. We then show by applying a structured random rotation of the data (an $\\mathcal{O}(d \\log d)$ algorithm), the error can be reduced to $\\mathcal{O}((\\log d)/n)$. The algorithms and the analysis make no distributional assumptions on the data.", "histories": [["v1", "Wed, 2 Nov 2016 00:16:18 GMT  (12kb)", "http://arxiv.org/abs/1611.00429v1", null], ["v2", "Mon, 27 Feb 2017 17:37:14 GMT  (77kb)", "http://arxiv.org/abs/1611.00429v2", null], ["v3", "Mon, 25 Sep 2017 15:10:54 GMT  (77kb)", "http://arxiv.org/abs/1611.00429v3", null]], "reviews": [], "SUBJECTS": "cs.LG", "authors": ["ananda theertha suresh", "felix x yu", "sanjiv kumar", "h brendan mcmahan"], "accepted": true, "id": "1611.00429"}, "pdf": {"name": "1611.00429.pdf", "metadata": {"source": "CRF", "title": "Distributed Mean Estimation with Limited Communication", "authors": ["Ananda Theertha Suresh", "Felix X. Yu", "H. Brendan McMahan", "Sanjiv Kumar"], "emails": ["sanjivk}@google.com"], "sections": [{"heading": null, "text": "ar Xiv: 161 1,00 429v 1 [cs.L G] 2N ov2 01"}, {"heading": "1 Introduction", "text": "In a typical scenario of synchronized distributed learning, each client receives a copy of a global model. Clients then update the model independently based on their local data. The updates of the model (usually in the form of a history) are then sent to a server where they are averaged and applied to update the global model. A critical step in these algorithms is the estimation of the mean value of the updates. Motivated by these applications, we investigate the problem of distributed mean estimation with a limited communication budget. Formally, n vectors Xn def = X1, X2. A critical step in these algorithms is the estimation of the mean value of the distributed mean estimate by these applications. We investigate the problem of distributed mean estimation with a limited communication budget. Formally, n vectors given Xn def = X1, X2. In each protocol Xp, each client transmits a function of Xi say f (Xi) and a central server estimates the mean value of the vectors."}, {"heading": "2 Stochastic Binary Quantization: A Naive Approach", "text": "We propose stochastic binary quantification as follows: B = maximum quantification (1) = maximum quantification (2) = maximum quantification (2) = maximum quantification (2) = maximum quantification Xi (2) = maximum quantification (2) = maximum quantification (2) = maximum quantification (2) = maximum quantification (2) = maximum quantification (2) = maximum quantification (2) = quantification (2) = quantification (2) = quantification (2) = maximum quantification (2) = maximum quantification (2) = quantification (2) = quantification (2) = quantification (2) = quantification (2) = quantification (2) = quantification (2) = quantification (2) = quantification (2) = (quantification) = (2) = (2) = (2) = (2) = (2) = quantification (2) = (2) = (2) = (Xi) = quantification (2) = (2) = (2) = quantification = (2) = (2) = (2) = (2) = (2) = (2) = (Xi) = (Xi) = (Xi) = (Xi) = (Xi) = (Xi) = (Xi) = (Xi) = (Xi) = (Xi) = (Xi) = (Xi) = (Xi) = (Xi = (Xi) = (Xi) = (Xi) = (Xi) = (Xi = (Xi) = (Xi) = (Xi) = (Xi) = (Xi) = (Xi = (Xi) = (Xi) = (Xi) = (Xi = (Xi) = (Xi) = (Xi = (Xi) = (Xi = (Xi) = (Xi = (Xi) = (Xi) = (Xi) = (Xi = (Xi) = (Xi) = (Xi = (Xi) = (Xi) = (Xi = (Xi) = (Xi) = (Xi) = (Xi = (Xi) = Xi = (Xi) = Xi = (Xi) = Xi = (Xi) = Xi = (Xi = Xi = Xi = Xi = Xi = (Xi = (Xi) = Xi = Xi = Xi = Xi = Xi = Xi = Xi = Xi = Xi = Xi = (Xi"}, {"heading": "3 Stochastic Rotated Binary Quantization", "text": "The motivation arises from the fact that the mean quadratic error is small if the two quantity-quantity-quantity-quantity-quantity-quantity-quantity-quantity-quantity-quantity-quantity-quantity-quantity-quantity-quantity-quantity-quantity-quantity-quantity-quantity-quantity-quantity-quantity-quantity-quantity-quantity-quantity-quantity-quantity-quantity-quantity-quantity-quantity-quantity-quantity-quantity-quantity-quantity-quantity-quantity-quantity-quantity-quantity-quantity-quantity-quantity-quantity-quantity-quantity-quantity-quantity-quantity-quantity-quantity-quantity-quantity-quantity-quantity-quantity-quantity-quantity-quantity-quantity-quantity-quantity-quantity-quantitatively-quantitatively-quantitatively-quantitatively-quantitatively-quantitatively-quantitatively-quantitatively-quantitatively-quantitatively-quantitatively-quantitatively-quantitatively-quantitatively-quantitatively-quantitatively-quantitatively-quantitatively-quantitatively-quantitatively-quantitatively-quantitatively-quantitatively-quantitatively-quantitatively-quantitatively-quantitatively-quantitatively-quantitatively-quantitatively-quantitatively-quantitatively-quantitatively-quantitatively-quantitatively-quantitatively-quantitatively-quantitatively-quantitatively-quantitatively-quantitatively-quantitatively-quantitatively-quantitatively-quantitatively-quantitatively-quantitatively-quantitatively-quantitatively-quantitatively-quantitatively-quantitatively-quantitatively-quantitatively-quantitatively-quantitatively-quantitatively-quantitatively-quantitatively-quantitatively-quantitatively-quantitatively-quantitatively-quantitatively-quantitatively-quantitatively-quantitatively-quantitatively-quantitatively-quantitatively-quantitatively-quantitatively-quantitatively-"}, {"heading": "4 The effect of sampling", "text": "It can be shown that the above protocols can be combined by clients or coordination samplers to achieve trade flows between the mean square error and the communication costs. \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p p p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 \u2212 p \u2212 p \u2212 \u2212 \u2212 \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 \u2212 \u2212 \u2212 p \u2212 \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 \u2212 p \u2212 p \u2212 \u2212 p \u2212"}, {"heading": "5 Conclusion", "text": "We have studied the distributed mean estimation, in which each client sends one bit for each dimension of the data. We have shown that a naive stochastic quantization algorithm achieves errors in accuracy (d / n) and an improved stochastic rotating quantization algorithm achieves errors in accuracy (log d) / n)."}, {"heading": "Acknowledgments", "text": "We thank Jayadev Acharya, Keith Bonawitz, Jakub Konecny, Dan Holtmann-Rice and Tengyu Ma for helpful comments and discussions."}, {"heading": "27: Annual Conference on Neural Information Processing Systems 2014, December 8-13 2014,", "text": "Montreal, Quebec, Canada, pp. 2726-2734, 2014. [7] Jakub Konec, H Brendan McMahan, Felix X Yu, Peter Richta \u0301 rik, Ananda Theertha Suresh, and Dave Bacon. Federated learning: Strategies for improve communication efficiency. arXiv preprint arXiv: 1610,05492, 2016. [8] H. Brendan McMahan, Eider Moore, Daniel Ramage, and Blaise Aguera y Arcas. Federated learning of deep networks using model averaging. arXiv: 1602.05629, 2016. [9] Felix X Yu, Ananda Theertha Suresh, Krzysztof Choromanski, Daniel Holtmann-Rice, and Sanjiv Kumar. Orthogonal random features. In NIPS, 2016."}], "references": [{"title": "Fast dimension reduction using rademacher series on dual BCH codes", "author": ["Nir Ailon", "Edo Liberty"], "venue": "Discrete & Computational Geometry,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2009}, {"title": "Communication lower bounds for statistical estimation problems via a distributed data processing inequality", "author": ["Mark Braverman", "Ankit Garg", "Tengyu Ma", "Huy L. Nguyen", "David P. Woodruff"], "venue": "In Proceedings of the 48th Annual ACM SIGACT Symposium on Theory of Computing,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2016}, {"title": "An elementary proof of a theorem of Johnson and Lindenstrauss", "author": ["Sanjoy Dasgupta", "Anupam Gupta"], "venue": "Random Structures & Algorithms,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2003}, {"title": "Large scale distributed deep networks", "author": ["Jeffrey Dean", "Greg Corrado", "Rajat Monga", "Kai Chen", "Matthieu Devin", "Mark Mao", "Andrew Senior", "Paul Tucker", "Ke Yang", "Quoc V Le"], "venue": "In NIPS,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2012}, {"title": "Informationtheoretic lower bounds for distributed statistical estimation with communication", "author": ["John C. Duchi", "Michael I. Jordan", "Martin J. Wainwright", "Yuchen Zhang"], "venue": "constraints. CoRR,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2014}, {"title": "On communication cost of distributed statistical estimation and dimensionality", "author": ["Ankit Garg", "Tengyu Ma", "Huy L. Nguyen"], "venue": "In Advances in Neural Information Processing Systems 27: Annual Conference on Neural Information Processing Systems", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2014}, {"title": "Federated learning: Strategies for improving communication efficiency", "author": ["Jakub Kone\u010dn\u1ef3", "H Brendan McMahan", "Felix X Yu", "Peter Richt\u00e1rik", "Ananda Theertha Suresh", "Dave Bacon"], "venue": "arXiv preprint arXiv:1610.05492,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2016}, {"title": "Federated learning of deep networks using model averaging", "author": ["H. Brendan McMahan", "Eider Moore", "Daniel Ramage", "Blaise Aguera y Arcas"], "venue": null, "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2016}, {"title": "Orthogonal random features", "author": ["Felix X Yu", "Ananda Theertha Suresh", "Krzysztof Choromanski", "Daniel Holtmann-Rice", "Sanjiv Kumar"], "venue": "In NIPS,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2016}], "referenceMentions": [{"referenceID": 7, "context": "1 Introduction Distributed learning algorithms are widely used in training large-scale neural networks [8, 4].", "startOffset": 103, "endOffset": 109}, {"referenceID": 3, "context": "1 Introduction Distributed learning algorithms are widely used in training large-scale neural networks [8, 4].", "startOffset": 103, "endOffset": 109}, {"referenceID": 4, "context": "Unlike previous works [5, 6, 2], where the objective is to estimate the mean of the underlying statistical model, we do not make distribution assumptions on the way data is generated, hence our algorithms are particularly useful in practical settings.", "startOffset": 22, "endOffset": 31}, {"referenceID": 5, "context": "Unlike previous works [5, 6, 2], where the objective is to estimate the mean of the underlying statistical model, we do not make distribution assumptions on the way data is generated, hence our algorithms are particularly useful in practical settings.", "startOffset": 22, "endOffset": 31}, {"referenceID": 1, "context": "Unlike previous works [5, 6, 2], where the objective is to estimate the mean of the underlying statistical model, we do not make distribution assumptions on the way data is generated, hence our algorithms are particularly useful in practical settings.", "startOffset": 22, "endOffset": 31}, {"referenceID": 6, "context": "In many practical scenarios d is much larger than n and the above error is prohibitive [7].", "startOffset": 87, "endOffset": 90}, {"referenceID": 6, "context": "We note that in practice, each dimension of Xi is often stored as 32 bit or 64 bit float [7], and r should be set as either 32 or 64.", "startOffset": 89, "endOffset": 92}, {"referenceID": 6, "context": "For example, in the application of neural networks [7], d can be on the order of millions, yet the number of clients is much lower than that.", "startOffset": 51, "endOffset": 54}, {"referenceID": 2, "context": "[3].", "startOffset": 0, "endOffset": 3}, {"referenceID": 0, "context": "Motivated by recent works in structured matrices [1, 9], we propose to use R = HD, where H is a Walsh-Hadamard matrix and D is a diagonal matrix with i.", "startOffset": 49, "endOffset": 55}, {"referenceID": 8, "context": "Motivated by recent works in structured matrices [1, 9], we propose to use R = HD, where H is a Walsh-Hadamard matrix and D is a diagonal matrix with i.", "startOffset": 49, "endOffset": 55}, {"referenceID": 0, "context": "The lemma is similar to that of results in [1] and we give the proof for completeness.", "startOffset": 43, "endOffset": 46}], "year": 2016, "abstractText": "Abstract Motivated by the need for distributed optimization algorithms with low communication cost, we study communication efficient algorithms to perform distributed mean estimation. We study scenarios in which each client sends one bit per dimension. We first show that for d dimensional data with n clients, a naive stochastic rounding approach yields a mean squared error \u0398(d/n). We then show by applying a structured random rotation of the data (an O(d log d) algorithm), the error can be reduced to O((log d)/n). The algorithms and the analysis make no distributional assumptions on the data.", "creator": "LaTeX with hyperref package"}}}