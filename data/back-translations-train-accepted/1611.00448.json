{"id": "1611.00448", "review": {"conference": "NIPS", "VERSION": "v1", "DATE_OF_SUBMISSION": "2-Nov-2016", "title": "Natural-Parameter Networks: A Class of Probabilistic Neural Networks", "abstract": "Neural networks (NN) have achieved state-of-the-art performance in various applications. Unfortunately in applications where training data is insufficient, they are often prone to overfitting. One effective way to alleviate this problem is to exploit the Bayesian approach by using Bayesian neural networks (BNN). Another shortcoming of NN is the lack of flexibility to customize different distributions for the weights and neurons according to the data, as is often done in probabilistic graphical models. To address these problems, we propose a class of probabilistic neural networks, dubbed natural-parameter networks (NPN), as a novel and lightweight Bayesian treatment of NN. NPN allows the usage of arbitrary exponential-family distributions to model the weights and neurons. Different from traditional NN and BNN, NPN takes distributions as input and goes through layers of transformation before producing distributions to match the target output distributions. As a Bayesian treatment, efficient backpropagation (BP) is performed to learn the natural parameters for the distributions over both the weights and neurons. The output distributions of each layer, as byproducts, may be used as second-order representations for the associated tasks such as link prediction. Experiments on real-world datasets show that NPN can achieve state-of-the-art performance.", "histories": [["v1", "Wed, 2 Nov 2016 02:32:05 GMT  (764kb,D)", "http://arxiv.org/abs/1611.00448v1", "To appear at NIPS 2016"]], "COMMENTS": "To appear at NIPS 2016", "reviews": [], "SUBJECTS": "cs.LG cs.AI cs.CL cs.CV stat.ML", "authors": ["hao wang 0014", "xingjian shi", "dit-yan yeung"], "accepted": true, "id": "1611.00448"}, "pdf": {"name": "1611.00448.pdf", "metadata": {"source": "CRF", "title": "Natural-Parameter Networks: A Class of Probabilistic Neural Networks", "authors": ["Hao Wang", "Xingjian Shi", "Dit-Yan Yeung"], "emails": ["hwangaz@cse.ust.hk", "xshiab@cse.ust.hk", "dyyeung@cse.ust.hk"], "sections": [{"heading": "1 Introduction", "text": "This year it is more than ever before."}, {"heading": "2 Natural-Parameter Networks", "text": "The exponential family refers to an important class of distributions with useful algebraic properties. Distributions in the exponential family are in the form p (x | \u03b7) = h (x) g (\u03b7) exp {\u03b7Tu (x)}, where x is the random variable, \u03b7 denotes the natural parameters, u (x) is a vector of sufficient statistics and g (\u03b7) is the normalizer. Motivated by this observation, only the natural parameters must be learned in the NPN to model the distributions over weights and neurons. Consider an NPN that takes a vector random distribution (\u2212 c2d, \u2212 1 2d). Motivated by this observation, only the natural parameters must be learned in the NPN to model the distributions over weights and neurons. Consider an NPN that takes a vector random distribution (e.g. a multivariate Negaussian distribution) as a natural starting parameter, and then we can only run through a natural distribution parameter."}, {"heading": "2.1 Notation and Conventions", "text": "We use bold uppercase letters such as W to denote matrices, and bold lowercase letters such as b to denote vectors. Similarly, a bold number (e.g. 1 or 0) stands for a row vector or matrix with identical entries. In NPN, o (l) is used to denote the values of neurons in layer l before nonlinear transformation, and a (l) stands for the values after nonlinear transformation. As mentioned above, NPN attempts to learn distributions through variables, not through variables themselves. Therefore, we use letters without the subscripts c, d, m, and s (e.g. o (l) and a (l)) to denote \"random variables\" with appropriate distributions. The subscripts c and d are used to denote natural parameter pairs such as Wc and Wd. Similarly, the subscripts and mean pairs represent variances."}, {"heading": "2.2 Linear Transformation in NPN", "text": "Here we first introduce the linear form of a general NPN (l) + b (l), assuming for the sake of simplicity b (b) distributions with two natural parameters (e.g. gamma distributions, beta distributions and Gaussian distributions). Specifically, we have (f) distributions on the weight matrices, p (W (l), ij \u2212 W (l) d) = i, j (W (l) ij (W (l) ij | W (l) c, ij, W (l) d, c (l) c, ij, W (l) d, ij) d, ij) d) the corresponding natural parameters. For b (l), o (l) and a (l) we assume similar factored distributions. In a traditional NN, the linear transformation o (l) follows o (l)."}, {"heading": "2.3 Nonlinear Transformation in NPN", "text": "After we have defined the linear transformed distribution via o (l) defined by the natural parameters o (l) c and o (l) d, an elementary non-linear transformation v (\u00b7 \u2212 g) (with a well-defined inverse function v \u2212 1 (\u00b7)), the resulting activation distribution is imposed on us. The resulting activation distribution is pa (a) (l)) = po (v \u2212 1 (l)) = po (l) = po (l))) x (l) may not be an exponential family distribution, but we can define it with one, p) and p (l) of the factored distribution via o (l) d) by comparing the first two moments. Once the averages a (l) m and the variance a (l) s of the pa (l) distribution are obtained, we can calculate the corresponding natural parameters with f \u2212 1 (\u00b7)."}, {"heading": "2.4 Deep Nonlinear NPN", "text": "Of course, the layers of nonlinear NPN are able to form a deep NPN as represented in algorithms."}, {"heading": "3 Variants of NPN", "text": "In this section, we present three variants of NPN with different properties to demonstrate the flexibility and effectiveness of NPN. Note that in practice, we use a transformed version of the natural parameters, which are here referred to as vicarious natural parameters, rather than the original one to calculate efficiency. For example, for gamma distributions, we use the vicarious natural parameters (c \u2212 1, \u2212 d) instead of the natural parameters (c, d) during the calculation."}, {"heading": "3.1 Gamma NPN", "text": "The gamma distribution supported by positive values is an important member of the exponential family. The corresponding probability density function is p (x | c, d) = p (c) \u2212 1dcxc \u2212 1 exp (\u2212 dx) with (c \u2212 1, \u2212 d) as its natural parameters (we use (c, d) as natural parameters. If we assume a gamma distribution for W (l), b (l), o (l) and a (l), an AU formed by NPN becomes a deep and non-linear version of the non-negative matrix factorization [14]. To see this, note that this AU with activation v (x) = x and zero biases b (l) is so equivalent to the search for a factorization of the matrix X that X = H-L l = L2 W (l), where H denotes the middle-layer neurons and W (l) as non-negative entries of gamma distributions."}, {"heading": "3.2 Gaussian NPN", "text": "Unlike the gamma distribution, which has only support for positive values, the Gaussian distribution (also an exponential family distribution (2 x) can describe real random variables, making it a natural choice for NPN. We refer to this NPN variant with Gaussian distributions both over weights and over neurons as Gaussian NPN. Details of Algorithm 2 for Gaussian NPN are as follows: Linear transformation: In addition to support for real values, another property of Gaussian distributions is that the mean and the variance can be used as proxy natural parameters, resulting in an identity mapping function f (c, d) = (c, d) that intersects the calculation costs. We can use this function to (W (l) m, W (l) s) s = f (l) s (w), W (l) s (d), b () l)."}, {"heading": "3.3 Poisson NPN", "text": "The Poisson distribution, as another member of the exponential family, is often used to model the number of words, topics, or super-topics in documents. Therefore, it is natural for text modeling to assume Poisson distributions for neurons in NPN. Interestingly, this design of Poisson NPN can be considered a neural analog of some Poisson factor analysis models [26]. Besides the nonlinear transformation in closed form, another challenge of Poisson NPN is to map the pair (o (l) m, o (l) s) to the single parameter o (l) c of the Poisson distributions. According to the central limit theorem, we have o (l) c = 14 (2o (l) m \u2212 1 + \u221a (2o (l) m \u2212 1) 2 + 8o (l) s) (see section C and F. 3 of the supplementary material for evidence, substantiations, and detailed derivatives of Poisson N)."}, {"heading": "4 Experiments", "text": "In this section, we examine variants of NPN and other state-of-the-art methods on four real data sets. We use Matlab (with GPU) to implement NPN, AE variants, and the \"vanilla\" NN that is trained with the drop-out SGD (Drop-out NN). For other baselines, we use the Theano library [2] and MXNet [5]."}, {"heading": "4.1 Toy Regression Task", "text": "To gain some insight into the NPN, we start with a toy 1d regression task, so that the predicted mean and variance can be visualized. Following [1], we generate 20 points in one dimension from a uniform distribution at the interval [\u2212 4, 4]. Target output is queried from the function y = x3 + n, where n \u0445 N (0, 9). We adjust the data using Gaussian NPN, BDK, and PBP (see the supplementary material for detailed hyperparameters). Figure 1 shows the predicted mean and variance of NPN, BDK, and PBP together with the mean provided by the failing NN (for larger versions of the numbers, please refer to the end of the supplementary materials). As we can see, the variance of PBP, BDK, and NPN differs from each other as x is further away from the training data. Both NPse and BDK'xx points are accurate enough to obtain distributions within the other regions."}, {"heading": "4.2 MNIST Classification", "text": "We train the models with 50,000 images and use 10,000 images for validation. Networks with a structure of 784-800-800-10 are used for all methods, as 800 works best for the relegated NN (referred to as Dropout1 in Table 2) and BDK (BDK with a structure of 784-400-400-10 has a 1.41% error rate). We also try to find the relegated NN with twice the number of hidden neurons (Dropout2 in Table 2) for a fair comparison. For BBB, we directly quote their results from [4]. We implement BDK and NPN with the same hyperparameters as in [1] whenever possible. Gaussian priors are used for NPN (see the supplementary material for detailed hyperparameters)."}, {"heading": "4.3 Second-Order Representation Learning", "text": "This year, more than ever before in the history of the country in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is not a country, but in which it is a country, a country, a city and a country."}, {"heading": "5 Conclusion", "text": "NPN considers weights and neurons as arbitrary exponential family distributions, not just dot estimates or factorized Gaussian distributions. This flexibility enables richer descriptions of hierarchical relationships between latent variables and adds a further degree of freedom to adjust NN for different data types. Efficient random sampling, backpropagation-compatible algorithms are designed for learning NPN. Experiments show that NPN is state-of-the-art in classification, regression, and representation learning tasks. As possible extensions of NPN, it would be interesting to combine NPN with arbitrary PGM models to form fully Bayesian deep learning models [24, 25] which even allows richer descriptions of relationships between latent variables."}, {"heading": "A Proof of Theorem 2", "text": "Theoretically we assume an exponential family distribution po (x) p (\u00b2) p (2) p (2) p (2) p (2) p (2) p (2) p (2) p (2) p (2) p (2) p (2) p (2) p (2) p (2) p (2) p (2) p (2) p (2) p (2) p (2) p (2) p (2) p (2) p (2) p (2) p (2) p (2) p (2) p (2) p) p (2) p) p (2) p (2) p (2) p) p (2)."}, {"heading": "B Exponential-Family Distributions and Activation Functions", "text": "In this section, we provide a list of exponential family distributions with corresponding activation functions q, which could lead to narrowed expressions of the first two moments of v (x), namelyE (v (x)) and E (v) 2. In theorem 2, we only use the activation function v (x) = r (1 \u2212 exp (\u2212 \u03c4x)) for the gamma NPN and the Poisson NPN. Figure 4 (left) records this function with different order of magnitude when r = 1. As we can see, this function has a similar shape with the positive half of v (x) = tanh (the negative part is irrelevant because both the gamma distribution and the Poisson distribution have support."}, {"heading": "B.1 Gamma Distributions", "text": "For gamma distributions with (v (x) = r (oc) oc (oc) oc (oc) oc (oc) oc) oc (oc) oc (oc) oc (oc) oc (oc) oc) oc) oc (oc) oc (oc) oc (oc) oc (oc) oc) oc) oc (oc) oc (oc) oc) oc (oc) oc) oc) oc (oc) oc) oc (oc) oc) oc (oc) oc) oc) oc (oc) oc) oc (oc) oc) oc) oc (oc) oc) oc) oc) oc (oc) oc) oc (oc) oc) oc (oc) oc) oc) oc) oc (oc) oc) oc) oc) oc (oc) oc) oc) oc) oc) oc (oc) oc) oc) oc (oc) oc) oc) oc) oc (oc) oc) oc) oc) oc (oc) oc) oc) oc) oc (oc) oc) oc) oc (oc) oc) oc) oc (oc) oc) oc) oc (oc) oc) oc) oc (oc) oc) oc (oc) oc) oc) oc."}, {"heading": "B.2 Poisson Distributions", "text": "For Poisson distributions with v (\u2212 \u2212 f \u2212 f \u2212 f \u2212 f \u2212 f \u2212 f \u2212 f \u2212 f \u2212 f \u2212 f = f = f (\u2212 f = f = f = f = f = f = f (\u2212 f)), using the Taylor expansion of exp (\u2212 f) f (exp (\u2212 f) f = f (\u2212 f) x!, we have x = f (\u2212 f) x! exp (\u2212 f) x! exp (\u2212 f) x! exp (\u2212 f) x! exp (\u2212 f) x! exp (\u2212 f) x! exp (\u2212 f) x! exp (\u2212 f) x! exp (\u2212 f) x! exp (\u2212 f) x! x = f (\u2212 f) x! exp (\u2212 f) x! exp (\u2212 f) x! exp (\u2212 f (\u2212 f) exp (\u2212 f) x!) x = f (\u2212 f) x!"}, {"heading": "B.3 Gaussian Distributions", "text": "In this section we provide a detailed derivation of (am, as) for Gaussian distributions."}, {"heading": "B.3.1 Sigmoid Activation", "text": "We start with the proof of the following theorem: Theorem 3. If we consider a univariate Gaussian DistributionN (x | \u00b5, \u03c32) and the probit function p (x) = p (0, 1) p (for all constants a and b the following equation applies: p (x + b) n (x | \u00b5, \u03c32) dx = p (2 + b) (1 + 2) p (1 + 2) p (7) p (7) p (7) p (7) proof. If we make the variable change x = p, we have I = p (x + b) n (x | p) p (2) p (2) p (2) p (2) p (2) p (2) p (p), p (2) p (p), p (2) p (p), p (2) p (p), p (p), p (2) p (p), p (2) p (p), p (2) p (p), p (2) p (p)."}, {"heading": "B.3.2 Hyperbolic Tangent Activation", "text": "If the tanh \u00b2 \u00b2 activation v (x) = tanh \u00b2 (x) is used, because tanh (x) = 2\u03c3 (2x) \u2212 1, then we have \u00b2 N (o | oc, diag (od)) \u0445 (2\u03c3 (2o) \u2212 1) do = 2 \u00b2 N (o | oc, diag (od)) \u043c (2o) do \u2212 1 \u2248 2 \u00b2 N (o | oc, diag (od)) \u0445 (2\u0445 o) do \u2212 1 = 2\u0438 (2\u0445 o) do \u2212 1 = 2\u0445 oc (1 + 4\u0445 2od) 1 2) \u2212 1 (oc (0.25 + economi2od) 1) \u2212 1, whereas the equation (10) \u00b2 \u00b2 (2) \u00b2 (2) \u00b2 (2) \u00b2 (2) \u00b2 (2) \u00b2 (2) \u00b2 (2) \u00b2 (2) \u00b2 (2) \u00b2 (\u00b2) \u00b2 (o | oc, diag (od) 2 = 2 (2) \u00b2 (2) \u00b2 (2) \u00b2 (2) \u00b2 (2) \u00b2 (2) \u00b2 (2) \u00b2 (2) \u00b2 (2) \u00b2 (2) \u00b2 (2) \u00b2 (2) (2) \u00b2 (2) \u00b2 (2) (2) \u00b2 (2) (2) \u00b2 (2) (2) \u00b2 (2) (2) \u00b2 (2) (2) \u00b2 (2) (2) \u00b2 (2) (2) (2) (2) (2) (2) (2) (2) (2) (2) (2) (2) (2) (2) (2) (2) (2) (2) (2) (2) (2) (2) (2) (2) (2) (2) (2) (2) (2) (2) (2) (2) (2) (2) (2) (2) (2) (2) (2) (2) (2) (2) (2) (2) (2) (2) (2) (2) (2) (2) (2) (2) (2) (2) (2) (2) (2) (2) (2) (2)."}, {"heading": "B.3.3 ReLU Activation", "text": "When ReLU activation v (x) = max (0, x) is used, we can apply techniques in [6] to obtain the first two moments of z = max (z1, z2), where z1 \u0445 N (\u00b51, \u03c321) and z2 \u0445 N (\u00b52, \u03c322). Specifically, we can E (z) = \u00b51\u03a6 (\u03b3) + \u00b52\u03a6 (\u2212 \u03b3) + \u03bd\u03c6 (\u03b3) E (z2) = (\u00b521 + \u03c3 2 1) (\u03b3) + (2 + 2) \u03a6 (\u2212 \u03b3) + (\u00b51 + \u00b52) \u03bd\u03c6 (x) \u2212 aos, \u03c6 (x | 1) \u2212 dhabi, \u03c6 (x | 1), \u03bd = N (0, 1), \u03bd = N (0, 1), ctu21 + \u03b3 2, and \u03b3 = \u00b51 \u2212 \u00b52. If N (\u00b51, \u03c321) \u2212 \u2212 \u2212 \u2212 \u2212 aos, \u03c6 (x) = N (TEST-TEST-TEST, A-TEST-TICTICTICTICTICTICTICTICTICTICTICTICTICTICTICTICTICTICTICTICS (TICTICTICTICTICTICTICTICTICTICTICTICTICTICTICTICTICS), INICTICTICTICTICTICTICTICTICTICTICTICTICTICTICTICTICTICTICTICS (TICTICTICTICTICTICTICTICTICTICTICTICTICTICTICTICTICTICS))"}, {"heading": "C Mapping Function for Poisson Distributions", "text": "Since the mapping function includes Gaussian approximation to a Poisson distribution, we start by demonstrating the connection between Gaussian distributions and Poisson distributions (1 = 1 = 1 = 1 = 1 = 1 = 2 = 2 = 2 = 2 = 2 = 2 = 2 = 2 = 2 = 2 = 2 = 2 = 2 = 2 = 2 = 2 = 2 = 2 = 2 = 2 = 2 = 2 = 2 = 2 = 2 = 2 = 2 = 2 = 2 = 2 = 2 = 2 = 2 = 2 = 2 = 2 = 2 = 2 = 2 = 2 = 2 = 2 = 2 = 2 = 2 = 2 = 2 = 2 = 2 = 2 = 2 = 2 = 2 = 2 = 2 = 2 = 2 = 2 = 2 = 2 = 2 = 2 = 2 = 2 = 2 = 2 = 2 = 2 = 2 = 2 = 2 = 2 = 2 = 2 = 2 = 2 = 2 = 2 = 2 = 2 = 2 = 2 = 2 = 2 = 2 = 2 = 2 = 2 = 2 = 2 = 2 = 2 = 2 = 2 = 2 = 2 = 2 = 2 = 2 = 2 = 2 = 2 = 2 = 2 = 2 = 2 = 2 = 2 = 2 = 2 = 2 = 2 = 2 = 2 = 2 = 2 = 2 = 2 = 2 = 2 = 2 = 2 = 2 = 2 = 2 = 2 = 2 = 2 = 2 = 2 = 2 = 2 = 2 = 2 = 2 = 2 = 2 = 2 = 2 = 2 = 2 = 2 = 2 = 2 = 2 = 2 = 2 = 2 = 2 = 2 = 2 = 2 = 2 = 2 = 2 = 2 = 2 = 2 = 2 = 2 = 2 = 2 = 2 = 2 = 2 = 2 = 2 = 2 = 2 = 2 = 2 = 2 = 2 = 2 = 2 = 2 = 2 = 2 = 2 = 2 = 2 = 2 = 2 = 2 = 2 = 2 = 2 = 2 = 2 = 2 = 2 = 2 = 2 = 2 = 2 = 2 = 2 = 2 = 2 = 2 = 2 = 2 = 2 = 2 = 2 = 2 = 2 = 2 = 2 = 2 = 2 = 2 = 2 = 2 = 2 = 2 = 2 = 2 = 2 = 2 = 2 = 2 = 2 = 2 = 2 = 2 = 2 = 2 = 2 ="}, {"heading": "D AUC for Link Prediction and Different Data Splitting", "text": "In this section, we show the AUC for different models for the task of link prediction. As we can see in Table 5 above, the result in AUC matches that in the ranking of links (as shown in Table 3 of the essay). NPN is able to achieve a much higher AUC than SAE, SDAE and UAE. Among the different variants of the NPN, Gaussian NPN appears to perform best in datasets with fewer words such as Citeulike-t (18.8 words per document). Poisson NPN, as the more natural choice for modeling text, achieves the best performance in datasets with more words (Citeulike-a with 66.6 words per document and arXiv with 88.8 words per document). To perform link prediction, we also try to divide the data in a different way and compare the performance of different models."}, {"heading": "E Hyperparameters and Preprocessing", "text": "In this section we will give details of the hyperparameters and pre-processing of the experiments mentioned in the paper."}, {"heading": "E.1 Toy Regression Task", "text": "For the toy 1d regression task, we use networks with a hidden layer containing 100 neurons and ReLU activation as in [1, 10]. For Gaussian NPN, we use KL divergence loss and isotropic Gaussian priors with precision 10 \u2212 4. The same priors are also used in other experiments."}, {"heading": "E.2 MNIST Classification", "text": "For pre-processing, according to [4, 1] the pixel values are normalized to the range [0, 1]. For the NPN variants, we use these hyperparameters: minibatch size 128, number of eras 2000 (the same as BDK). For the learning rate, AdaDelta is used. As NPN is dropout compatible, we can use dropouts (almost at no additional cost) for effective regulation. Training and testing dropouts PN are similar to those of vanilla dropouts."}, {"heading": "E.3 Second-Order Representation Learning", "text": "Although theoretically no pre-processing of the Poisson NPN is required, since the Poisson distributions naturally determine the number of words of the model, in practice we find that normalization of the BOW vectors increases both stability during training and predictability; for simplicity, the Poisson NPN sets r to 1 and \u03c4 = 0.1 (these two hyperparameters can be set to further improve performance); Gaussian NPN uses sigmoid activation; the other hyperparameters of the NPN are the same as in the MNIST experiments."}, {"heading": "F Details on Variants of NPN", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "F.1 Gamma NPN", "text": "In gamma NPN the parameters W (l) c (l) c (l) d (b) b (b) b (l) c and b (l) d can be learned according to algorithm 2. Specifically, we will do the error E taking into account the input a (0) m = xAlgorithm 2 (Deep Nonlinear NPN1: Input: The data {(xi, yi)} Ni = 1, number of iterations T, learning rate, number of layers L. 2: for t = 1: T do 3: for l = 1: L do 4: Compute (o (l) m, o (l) s) off (l \u2212 1) m, a (l \u2212 1) s, number of layers L. 2: for t = 1: T do 3: for l = 1: for l: for l = 1: L do 4: Compute (o) s: Compute (o).5: Compute (l) s (a (l) m, a).we: for l = 1: we."}, {"heading": "F.2 Gaussian NPN", "text": "For details on Bayesian nonlinear transformation, please refer to Section B.3. For the KL divergence between the learned distribution and the previous distribution of weights, we can calculate itas: KL (N (x | c, d) \u0445 N (x | 0, \u03bb \u2212 1s)) = 12 (\u03bbs + \u03bbsc2 \u2212 1 \u2212 log \u03bbs \u2212 log d), (16) As we can see, the term \u2212 12 log d will help prevent the learned variance d from collapsing to 0 (in practice, we can use 12\u03bbd (d \u2212 h) 2, where \u03bbd and h are hyperparameters to approximate this term for better numerical stability), and the term 12c2 corresponds to the L2 regularization. Similar to BDK, we can use a mixture of Gausses as the previous distribution."}, {"heading": "F.3 Poisson NPN", "text": "The Poisson distribution, as another member of the exponential family, is often used to model the number of events (e.g. number of events or number of words in a document). Unlike the previous distributions, it has support for nonnegative integers. The Poisson distribution takes the form p (x | c) = cx exp (\u2212 c) x (with a single natural parameter log c (we use c as the substitute natural parameter).It is this only natural parameter that makes learning a Poisson NPN trickier. For text modeling, the adoption of Poisson distributions for neurons is natural because it can model the number of words and topics (even super topics) in documents. Here, we assume a factorized Poisson distribution p (o)."}, {"heading": "G Derivation of Gradients", "text": "In this section, we list the gradients used in backpropagation to update the NPN parameters."}, {"heading": "G.1 Gamma NPN", "text": "In the following we go from an activation function of v \u2212 l \u2212 l = r (1 \u2212 l) a (1 \u2212 l) a (1 \u2212 l) l (1 \u2212 l) l (l) l (l) l (l) l (l) l (l) l (l) l (l) l (l) l (l) l (l) l (l) l (l) l (l) l (l) l (l) l (l) l) l (l) l (l) l (l) l (l) l (l) l (l) l (l) l (l) l (l) l) l (l) l (l) l (l) l (l) l (l) l (l) l (l) l (l) l (l) l (l) l (l) l (l) l (l) l (l) l (l) l (l) l (l (l) l (l) l (l) l (l (l) l (l) l (l) l (l) l (l) l (l (l) l (l) l (l) l (l (l) l (l) l (l) l (l (l) l (l) l (l) l (l (l) l (l) l (l (l) l (l) l (l (l (l) l (l (l) l (l) l (l (l (l) l (l (l) l (l (l) l) l (l (l (l) l) l (l (l) l (l (l) l (l (l) l) l (l (l) l (l (l (l) l (l (l) l (l (l) l (l (l (l) l (l) l (l (l (l) l) l (l) l (l (l) l (l (l) l (l) l (l (l (l (l (l (l) l) l (l (l) l) l (l (l (l (l) l (l) l (l) l) l (l (l (l) l (l) l (l) l (l (l"}, {"heading": "G.2 Gaussian NPN", "text": "(v) v (v) v (v) v (v) v (v) v (v) v (v) v (v) v (v) v (v) v (v) v (v) v (v) v (v) v (v) v (v) v (v) v (v) v (v) v (v) v (v) v (v) v (v) v (v) v (v) v (v) v (v) v (v) v (v) v (v) v (v) v (v) v (v) v (v) v (v) v (v) v (v) v (v) v (v) v v v (v) v (v) v (v) v (v) v (v) v (v) v (v) v (v) v v (v) v v (v) v (v) v (v) v (v) v (v) v (v) v (v) v (v) v (v) v (v) v (v) v (v) v (v) v (v) v (v) v (v) v (v) v (v) v (v) v (v) v (v) v) v (v) v (v) v (v) v (v) v (v) v (v) v (v) v (v) v (v) v (v) v (v) v (v) v (v) v v (v) v (v) v (v) v (v (v) v (v) v (v (v) v (v) v (v (v) v (v) v (v (v) v (v) v (v (v) v (v) v (v v (v) v (v) v v (v) v v (v (v) v (v) v (v (v) v (v) v v v (v) v v (v (v) v v (v) v v (v) v (v v v (v) v (v) v (v) v v v (v (v) v v v (v) v v v v (v) v (v (v) v v v v (v) v (v v"}, {"heading": "G.3 Poisson NPN", "text": "Following we proceed from the activation function v (x \u2212 l \u2212 l = r (1 \u2212 exp (\u03c4x) l = l = l (v = l) l (v = l) l (v = l) l (v = v = l) l (v = l) l (v = l) l (v = l) l (v = l) l (v = l) l (v = l) l (v = l) l (v = l) l (v = l) l (v = l) l (v (v) l (v) l) l (v) l (v) l (v) l (v) l (v) l (v) l (v) l (v) l \u2212 l) l (v) l (v) l \u2212 l) l (v) l (v \u2212 l) l) l (v \u2212 l) l) l (v c c c c c c (c c c c c) v (v (v) l (l) (l) (l) (l) (v \u2212 l) l (l) l (v \u2212 l) l) l (v \u2212 l) l (v \u2212 l) l) l (v \u2212 l) l (v \u2212 l) l) l \u2212 l (v \u2212 l) l \u2212 l \u2212 l \u2212 l \u2212 l \u2212 l \u2212 l \u2212 l \u2212 l \u2212 l \u2212 l \u2212 l \u2212 l \u2212 l \u2212 l \u2212 l \u2212 l \u2212 l \u2212 l \u2212 l \u2212 l \u2212 l \u2212 l \u2212 l \u2212 l \u2212 l \u2212 l \u2212 l \u2212 l \u2212 l \u2212 l \u2212 l \u2212 l \u2212 l \u2212 l \u2212 l \u2212 l \u2212 l \u2212 l \u2212 l \u2212 l \u2212 l \u2212 l \u2212 l \u2212 l \u2212 l \u2212 l \u2212 l \u2212 l \u2212 l \u2212 l \u2212 l \u2212 l \u2212 l \u2212 l \u2212 l \u2212 l \u2212 l \u2212 l \u2212 l \u2212 l \u2212 l \u2212 l \u2212 l \u2212 l \u2212 l \u2212 l \u2212 l \u2212 l \u2212 l \u2212 l \u2212 l \u2212 l \u2212 l \u2212 l \u2212 l \u2212 l \u2212 l \u2212 l \u2212 l \u2212 l \u2212 l \u2212 l \u2212 l \u2212 l \u2212 l \u2212 l \u2212 l \u2212 l \u2212 l \u2212 l \u2212 l \u2212 l \u2212 l \u2212 l \u2212 l \u2212 l \u2212 l \u2212 l \u2212 l \u2212 l \u2212 l \u2212 l \u2212 l \u2212 l \u2212 l \u2212 l \u2212 l \u2212 l \u2212 l \u2212 l \u2212 l \u2212 l"}], "references": [{"title": "Bayesian dark knowledge", "author": ["A.K. Balan", "V. Rathod", "K.P. Murphy", "M. Welling"], "venue": "NIPS", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2015}, {"title": "Theano: new features and speed improvements", "author": ["F. Bastien", "P. Lamblin", "R. Pascanu", "J. Bergstra", "I.J. Goodfellow", "A. Bergeron", "N. Bouchard", "Y. Bengio"], "venue": "Deep Learning and Unsupervised Feature Learning NIPS 2012 Workshop", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2012}, {"title": "Pattern Recognition and Machine Learning", "author": ["C.M. Bishop"], "venue": "Springer-Verlag New York, Inc., Secaucus, NJ, USA", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2006}, {"title": "Weight uncertainty in neural network", "author": ["C. Blundell", "J. Cornebise", "K. Kavukcuoglu", "D. Wierstra"], "venue": "ICML", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2015}, {"title": "Mxnet: A flexible and efficient machine learning library for heterogeneous distributed systems", "author": ["T. Chen", "M. Li", "Y. Li", "M. Lin", "N. Wang", "M. Wang", "T. Xiao", "B. Xu", "C. Zhang", "Z. Zhang"], "venue": "CoRR, abs/1512.01274", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2015}, {"title": "The greatest of a finite set of random variables", "author": ["C.E. Clark"], "venue": "Operations Research, 9(2):145\u2013162", "citeRegEx": "6", "shortCiteRegEx": null, "year": 1961}, {"title": "Deep Learning", "author": ["I. Goodfellow", "Y. Bengio", "A. Courville"], "venue": "Book in preparation for MIT Press", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2016}, {"title": "Practical variational inference for neural networks", "author": ["A. Graves"], "venue": "NIPS", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2011}, {"title": "Deep poisson factor modeling", "author": ["R. Henao", "Z. Gan", "J. Lu", "L. Carin"], "venue": "NIPS", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2015}, {"title": "Probabilistic backpropagation for scalable learning of Bayesian neural networks", "author": ["J.M. Hern\u00e1ndez-Lobato", "R. Adams"], "venue": "ICML", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2015}, {"title": "Keeping the neural networks simple by minimizing the description length of the weights", "author": ["G.E. Hinton", "D. Van Camp"], "venue": "COLT", "citeRegEx": "11", "shortCiteRegEx": null, "year": 1993}, {"title": "Deep visual-semantic alignments for generating image descriptions", "author": ["A. Karpathy", "F. Li"], "venue": "CVPR", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2015}, {"title": "Auto-encoding variational Bayes", "author": ["D.P. Kingma", "M. Welling"], "venue": "CoRR, abs/1312.6114", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2013}, {"title": "Algorithms for non-negative matrix factorization", "author": ["D.D. Lee", "H.S. Seung"], "venue": "NIPS", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2001}, {"title": "Krevl. SNAP Datasets: Stanford large network dataset collection", "author": ["A.J. Leskovec"], "venue": "http://snap. stanford.edu/data,", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2014}, {"title": "A practical Bayesian framework for backprop networks", "author": ["J. MacKay David"], "venue": "Neural computation", "citeRegEx": "16", "shortCiteRegEx": null, "year": 1992}, {"title": "Learning stochastic feedforward networks", "author": ["R.M. Neal"], "venue": "Department of Computer Science, University of Toronto", "citeRegEx": "17", "shortCiteRegEx": null, "year": 1990}, {"title": "Bayesian learning for neural networks", "author": ["R.M. Neal"], "venue": "PhD thesis, University of Toronto", "citeRegEx": "18", "shortCiteRegEx": null, "year": 1995}, {"title": "Deep exponential families", "author": ["R. Ranganath", "L. Tang", "L. Charlin", "D.M. Blei"], "venue": "AISTATS", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2015}, {"title": "Semantic hashing", "author": ["R. Salakhutdinov", "G.E. Hinton"], "venue": "Int. J. Approx. Reasoning, 50(7):969\u2013978", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2009}, {"title": "Stacked denoising autoencoders: Learning useful representations in a deep network with a local denoising criterion", "author": ["P. Vincent", "H. Larochelle", "I. Lajoie", "Y. Bengio", "P.-A. Manzagol"], "venue": "JMLR, 11:3371\u20133408", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2010}, {"title": "Collaborative topic modeling for recommending scientific articles", "author": ["C. Wang", "D.M. Blei"], "venue": "KDD", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2011}, {"title": "Collaborative topic regression with social regularization for tag recommendation", "author": ["H. Wang", "B. Chen", "W.-J. Li"], "venue": "IJCAI", "citeRegEx": "23", "shortCiteRegEx": null, "year": 2013}, {"title": "Collaborative deep learning for recommender systems", "author": ["H. Wang", "N. Wang", "D. Yeung"], "venue": "KDD", "citeRegEx": "24", "shortCiteRegEx": null, "year": 2015}, {"title": "Towards Bayesian deep learning: A framework and some existing methods", "author": ["H. Wang", "D. Yeung"], "venue": "TKDE", "citeRegEx": "25", "shortCiteRegEx": null, "year": 2016}, {"title": "Beta-negative binomial process and poisson factor analysis", "author": ["M. Zhou", "L. Hannah", "D.B. Dunson", "L. Carin"], "venue": "AISTATS", "citeRegEx": "26", "shortCiteRegEx": null, "year": 2012}], "referenceMentions": [{"referenceID": 11, "context": "Recently neural networks (NN) have achieved state-of-the-art performance in various applications ranging from computer vision [12] to natural language processing [20].", "startOffset": 126, "endOffset": 130}, {"referenceID": 19, "context": "Recently neural networks (NN) have achieved state-of-the-art performance in various applications ranging from computer vision [12] to natural language processing [20].", "startOffset": 162, "endOffset": 166}, {"referenceID": 15, "context": "Early BNN works include methods based on Laplace approximation [16], variational inference (VI) [11], and Monte Carlo sampling [18], but they have not been widely adopted due to their lack of scalability.", "startOffset": 63, "endOffset": 67}, {"referenceID": 10, "context": "Early BNN works include methods based on Laplace approximation [16], variational inference (VI) [11], and Monte Carlo sampling [18], but they have not been widely adopted due to their lack of scalability.", "startOffset": 96, "endOffset": 100}, {"referenceID": 17, "context": "Early BNN works include methods based on Laplace approximation [16], variational inference (VI) [11], and Monte Carlo sampling [18], but they have not been widely adopted due to their lack of scalability.", "startOffset": 127, "endOffset": 131}, {"referenceID": 7, "context": "[8] proposed a method based on VI in which a Monte Carlo estimate of a lower bound on the marginal likelihood is used to infer the weights.", "startOffset": 0, "endOffset": 3}, {"referenceID": 9, "context": "Recently, [10] used an online version of expectation propagation (EP), called \u2018probabilistic back propagation\u2019 (PBP), for the Bayesian learning of NN, and [4] proposed \u2018Bayes by Backprop\u2019 (BBB), which can be viewed as an extension of [8] based on the \u2018reparameterization trick\u2019 [13].", "startOffset": 10, "endOffset": 14}, {"referenceID": 3, "context": "Recently, [10] used an online version of expectation propagation (EP), called \u2018probabilistic back propagation\u2019 (PBP), for the Bayesian learning of NN, and [4] proposed \u2018Bayes by Backprop\u2019 (BBB), which can be viewed as an extension of [8] based on the \u2018reparameterization trick\u2019 [13].", "startOffset": 155, "endOffset": 158}, {"referenceID": 7, "context": "Recently, [10] used an online version of expectation propagation (EP), called \u2018probabilistic back propagation\u2019 (PBP), for the Bayesian learning of NN, and [4] proposed \u2018Bayes by Backprop\u2019 (BBB), which can be viewed as an extension of [8] based on the \u2018reparameterization trick\u2019 [13].", "startOffset": 234, "endOffset": 237}, {"referenceID": 12, "context": "Recently, [10] used an online version of expectation propagation (EP), called \u2018probabilistic back propagation\u2019 (PBP), for the Bayesian learning of NN, and [4] proposed \u2018Bayes by Backprop\u2019 (BBB), which can be viewed as an extension of [8] based on the \u2018reparameterization trick\u2019 [13].", "startOffset": 278, "endOffset": 282}, {"referenceID": 0, "context": "More recently, an interesting Bayesian treatment called \u2018Bayesian dark knowledge\u2019 (BDK) was designed to approximate a teacher network with a simpler student network based on stochastic gradient Langevin dynamics (SGLD) [1].", "startOffset": 219, "endOffset": 222}, {"referenceID": 7, "context": "Although these recent methods are more practical than earlier ones, several outstanding problems remain to be addressed: (1) most of these methods require sampling either at training time [8, 4, 1] or at test time [4], incurring much higher cost than a \u2018vanilla\u2019 NN; (2) as mentioned in [1], methods", "startOffset": 188, "endOffset": 197}, {"referenceID": 3, "context": "Although these recent methods are more practical than earlier ones, several outstanding problems remain to be addressed: (1) most of these methods require sampling either at training time [8, 4, 1] or at test time [4], incurring much higher cost than a \u2018vanilla\u2019 NN; (2) as mentioned in [1], methods", "startOffset": 188, "endOffset": 197}, {"referenceID": 0, "context": "Although these recent methods are more practical than earlier ones, several outstanding problems remain to be addressed: (1) most of these methods require sampling either at training time [8, 4, 1] or at test time [4], incurring much higher cost than a \u2018vanilla\u2019 NN; (2) as mentioned in [1], methods", "startOffset": 188, "endOffset": 197}, {"referenceID": 3, "context": "Although these recent methods are more practical than earlier ones, several outstanding problems remain to be addressed: (1) most of these methods require sampling either at training time [8, 4, 1] or at test time [4], incurring much higher cost than a \u2018vanilla\u2019 NN; (2) as mentioned in [1], methods", "startOffset": 214, "endOffset": 217}, {"referenceID": 0, "context": "Although these recent methods are more practical than earlier ones, several outstanding problems remain to be addressed: (1) most of these methods require sampling either at training time [8, 4, 1] or at test time [4], incurring much higher cost than a \u2018vanilla\u2019 NN; (2) as mentioned in [1], methods", "startOffset": 287, "endOffset": 290}, {"referenceID": 20, "context": "Input distributions go through layers of linear and nonlinear transformation deterministically before producing distributions to match the target output distributions (previous work [21] shows that providing distributions as input by corrupting the data with noise plays the role of regularization).", "startOffset": 182, "endOffset": 186}, {"referenceID": 2, "context": "Thanks to the properties of the exponential family [3, 19], distributions in NPN are defined by the corresponding natural parameters which can be learned efficiently by backpropagation.", "startOffset": 51, "endOffset": 58}, {"referenceID": 18, "context": "Thanks to the properties of the exponential family [3, 19], distributions in NPN are defined by the corresponding natural parameters which can be learned efficiently by backpropagation.", "startOffset": 51, "endOffset": 58}, {"referenceID": 3, "context": "Unlike [4, 1], NPN explicitly propagates the estimates of uncertainty back and forth in deep networks.", "startOffset": 7, "endOffset": 13}, {"referenceID": 0, "context": "Unlike [4, 1], NPN explicitly propagates the estimates of uncertainty back and forth in deep networks.", "startOffset": 7, "endOffset": 13}, {"referenceID": 0, "context": "Table 1: Activation Functions for Exponential-Family Distributions Distribution Probability Density Function Activation Function Support Beta Distribution p(x) = \u0393(c+d) \u0393(c)\u0393(d) xc\u22121(1\u2212 x)d\u22121 qx , \u03c4 \u2208 (0, 1) [0, 1] Rayleigh Distribution p(x) = x \u03c32 exp{\u2212 x2 2\u03c32 } r \u2212 q exp{\u2212\u03c4x} (0,+\u221e) Gamma Distribution p(x) = 1 \u0393(c) dcxc\u22121 exp{\u2212dx} r \u2212 q exp{\u2212\u03c4x} (0,+\u221e) Poisson Distribution p(x) = c x exp{\u2212c} x! r \u2212 q exp{\u2212\u03c4x} Nonnegative interger Gaussian Distribution p(x) = (2\u03c0\u03c32)\u2212 1 2 exp{\u2212 1 2\u03c32 (x\u2212 \u03bc)} ReLU, tanh, and sigmoid (\u2212\u221e,+\u221e) and \u222b po(x|\u03b7)v(x)dx, can be expressed in closed form.", "startOffset": 208, "endOffset": 214}, {"referenceID": 13, "context": "Using gamma distributions for both the weights and neurons in NPN leads to a deep and nonlinear version of nonnegative matrix factorization [14] while an NPN with the Bernoulli distribution and sigmoid activation resembles a Bayesian treatment of sigmoid belief networks [17].", "startOffset": 140, "endOffset": 144}, {"referenceID": 16, "context": "Using gamma distributions for both the weights and neurons in NPN leads to a deep and nonlinear version of nonnegative matrix factorization [14] while an NPN with the Bernoulli distribution and sigmoid activation resembles a Bayesian treatment of sigmoid belief networks [17].", "startOffset": 271, "endOffset": 275}, {"referenceID": 25, "context": "If Poisson distributions are chosen for the neurons, NPN becomes a neural analogue of deep Poisson factor analysis [26, 9].", "startOffset": 115, "endOffset": 122}, {"referenceID": 8, "context": "If Poisson distributions are chosen for the neurons, NPN becomes a neural analogue of deep Poisson factor analysis [26, 9].", "startOffset": 115, "endOffset": 122}, {"referenceID": 13, "context": "If we assume gamma distributions for W, b, o, and a, an AE formed by NPN becomes a deep and nonlinear version of nonnegative matrix factorization [14].", "startOffset": 146, "endOffset": 150}, {"referenceID": 5, "context": "If the ReLU activation v(x) = max(0, x) is used, we can use the techniques in [6] to obtain the first two moments of max(z1, z2) where z1 and z2 are Gaussian random variables.", "startOffset": 78, "endOffset": 81}, {"referenceID": 25, "context": "Interestingly, this design of Poisson NPN can be seen as a neural analogue of some Poisson factor analysis models [26].", "startOffset": 114, "endOffset": 118}, {"referenceID": 1, "context": "For other baselines, we use the Theano library [2] and MXNet [5].", "startOffset": 47, "endOffset": 50}, {"referenceID": 4, "context": "For other baselines, we use the Theano library [2] and MXNet [5].", "startOffset": 61, "endOffset": 64}, {"referenceID": 0, "context": "Following [1], we generate 20 points in one dimension from a uniform distribution in the interval [\u22124, 4].", "startOffset": 10, "endOffset": 13}, {"referenceID": 3, "context": "For BBB, we directly quote their results from [4].", "startOffset": 46, "endOffset": 49}, {"referenceID": 0, "context": "We implement BDK and NPN using the same hyperparameters as in [1] whenever possible.", "startOffset": 62, "endOffset": 65}, {"referenceID": 0, "context": "As shown in Table 2, BDK and BBB achieve comparable performance with dropout NN (similar to [1], PBP is not included in the comparison since it supports regression only), and gamma NPN slightly outperforms dropout NN.", "startOffset": 92, "endOffset": 95}, {"referenceID": 21, "context": "The first two datasets are from [22, 23], collected separately from CiteULike in different ways to mimic different real-world settings.", "startOffset": 32, "endOffset": 40}, {"referenceID": 22, "context": "The first two datasets are from [22, 23], collected separately from CiteULike in different ways to mimic different real-world settings.", "startOffset": 32, "endOffset": 40}, {"referenceID": 14, "context": "The third one is from arXiv as one of the SNAP datasets [15].", "startOffset": 56, "endOffset": 60}, {"referenceID": 2, "context": "The task is to perform unsupervised representation learning before feeding the extracted representations (middle-layer neurons) into a Bayesian LR algorithm [3].", "startOffset": 157, "endOffset": 160}, {"referenceID": 6, "context": "We use the stacked autoencoder (SAE) [7], stacked denoising autoencoder (SDAE) [21], variational autoencoder (VAE) [13] as baselines (hyperparameters like weight decay and dropout rate are chosen by cross validation).", "startOffset": 37, "endOffset": 40}, {"referenceID": 20, "context": "We use the stacked autoencoder (SAE) [7], stacked denoising autoencoder (SDAE) [21], variational autoencoder (VAE) [13] as baselines (hyperparameters like weight decay and dropout rate are chosen by cross validation).", "startOffset": 79, "endOffset": 83}, {"referenceID": 12, "context": "We use the stacked autoencoder (SAE) [7], stacked denoising autoencoder (SDAE) [21], variational autoencoder (VAE) [13] as baselines (hyperparameters like weight decay and dropout rate are chosen by cross validation).", "startOffset": 115, "endOffset": 119}, {"referenceID": 23, "context": "As possible extensions of NPN, it would be interesting to connect NPN to arbitrary PGM to form fully Bayesian deep learning models [24, 25], allowing even richer descriptions of relationships among latent variables.", "startOffset": 131, "endOffset": 139}, {"referenceID": 24, "context": "As possible extensions of NPN, it would be interesting to connect NPN to arbitrary PGM to form fully Bayesian deep learning models [24, 25], allowing even richer descriptions of relationships among latent variables.", "startOffset": 131, "endOffset": 139}, {"referenceID": 0, "context": "Since we expect the nonlinearly transformed distribution to be another beta distribution, the domain of the function should be (0, 1) and the field should be [0, 1].", "startOffset": 158, "endOffset": 164}, {"referenceID": 5, "context": "3 ReLU Activation If the ReLU activation v(x) = max(0, x) is used, we can use the techniques in [6] to obtain the first two moments of z = max(z1, z2) where z1 \u223c N (\u03bc1, \u03c3 1) and z2 \u223c N (\u03bc2, \u03c3 2).", "startOffset": 96, "endOffset": 99}, {"referenceID": 0, "context": "For the toy 1d regression task, we use networks with one hidden layer containing 100 neurons and ReLU activation, as in [1, 10].", "startOffset": 120, "endOffset": 127}, {"referenceID": 9, "context": "For the toy 1d regression task, we use networks with one hidden layer containing 100 neurons and ReLU activation, as in [1, 10].", "startOffset": 120, "endOffset": 127}, {"referenceID": 3, "context": "For preprocessing, following [4, 1], pixel values are normalized to the range [0, 1].", "startOffset": 29, "endOffset": 35}, {"referenceID": 0, "context": "For preprocessing, following [4, 1], pixel values are normalized to the range [0, 1].", "startOffset": 29, "endOffset": 35}, {"referenceID": 0, "context": "For preprocessing, following [4, 1], pixel values are normalized to the range [0, 1].", "startOffset": 78, "endOffset": 84}, {"referenceID": 0, "context": "For all models, we preprocess the BOW vectors by normalizing them into the range [0, 1].", "startOffset": 81, "endOffset": 87}, {"referenceID": 25, "context": "Interestingly, this design of Poisson NPN can be seen as a neural analogue of some Poisson factor analysis models [26].", "startOffset": 114, "endOffset": 118}], "year": 2016, "abstractText": "Neural networks (NN) have achieved state-of-the-art performance in various applications. Unfortunately in applications where training data is insufficient, they are often prone to overfitting. One effective way to alleviate this problem is to exploit the Bayesian approach by using Bayesian neural networks (BNN). Another shortcoming of NN is the lack of flexibility to customize different distributions for the weights and neurons according to the data, as is often done in probabilistic graphical models. To address these problems, we propose a class of probabilistic neural networks, dubbed natural-parameter networks (NPN), as a novel and lightweight Bayesian treatment of NN. NPN allows the usage of arbitrary exponential-family distributions to model the weights and neurons. Different from traditional NN and BNN, NPN takes distributions as input and goes through layers of transformation before producing distributions to match the target output distributions. As a Bayesian treatment, efficient backpropagation (BP) is performed to learn the natural parameters for the distributions over both the weights and neurons. The output distributions of each layer, as byproducts, may be used as second-order representations for the associated tasks such as link prediction. Experiments on real-world datasets show that NPN can achieve state-of-the-art performance.", "creator": "LaTeX with hyperref package"}}}