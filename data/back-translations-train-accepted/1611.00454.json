{"id": "1611.00454", "review": {"conference": "NIPS", "VERSION": "v1", "DATE_OF_SUBMISSION": "2-Nov-2016", "title": "Collaborative Recurrent Autoencoder: Recommend while Learning to Fill in the Blanks", "abstract": "Hybrid methods that utilize both content and rating information are commonly used in many recommender systems. However, most of them use either handcrafted features or the bag-of-words representation as a surrogate for the content information but they are neither effective nor natural enough. To address this problem, we develop a collaborative recurrent autoencoder (CRAE) which is a denoising recurrent autoencoder (DRAE) that models the generation of content sequences in the collaborative filtering (CF) setting. The model generalizes recent advances in recurrent deep learning from i.i.d. input to non-i.i.d. (CF-based) input and provides a new denoising scheme along with a novel learnable pooling scheme for the recurrent autoencoder. To do this, we first develop a hierarchical Bayesian model for the DRAE and then generalize it to the CF setting. The synergy between denoising and CF enables CRAE to make accurate recommendations while learning to fill in the blanks in sequences. Experiments on real-world datasets from different domains (CiteULike and Netflix) show that, by jointly modeling the order-aware generation of sequences for the content information and performing CF for the ratings, CRAE is able to significantly outperform the state of the art on both the recommendation task based on ratings and the sequence generation task based on content information.", "histories": [["v1", "Wed, 2 Nov 2016 02:49:44 GMT  (1258kb,D)", "http://arxiv.org/abs/1611.00454v1", "To appear at NIPS 2016"]], "COMMENTS": "To appear at NIPS 2016", "reviews": [], "SUBJECTS": "cs.LG cs.AI cs.CL cs.CV stat.ML", "authors": ["hao wang 0014", "xingjian shi", "dit-yan yeung"], "accepted": true, "id": "1611.00454"}, "pdf": {"name": "1611.00454.pdf", "metadata": {"source": "CRF", "title": "Collaborative Recurrent Autoencoder: Recommend while Learning to Fill in the Blanks", "authors": ["Hao Wang", "Xingjian Shi", "Dit-Yan Yeung"], "emails": ["hwangaz@cse.ust.hk", "xshiab@cse.ust.hk", "dyyeung@cse.ust.hk"], "sections": [{"heading": "1 Introduction", "text": "In fact, it is. (...) It is. (...) It is. (...) It is. (...) It is. (...) It is. (...) It is. (...) It is. (...) It is. (...) It is. (...) It is. (...) It is. (...) It is. (...) It is. (...) It is. (...) It is. (...) It is. (...) It is. (...) It is. (...) It is. (...) It is. (...) It is. (...) It is. (...) It is. (...) It is. (...) It is. (...) It is. (...) It is. (...) It is. (...) It is. (...) It is. (...) It is. It is. (...) It is. It is. (...) It is. It is. (...) It is. It is. (...) It is. It is. It is. (...) It is. It is. (...) It is. It is. It is. (...) It is. It is. It is. (... It is. It is. It is. (...) It is. It is. It is. It is. (... It is. It is. It is. It is. (...) It is. It is. (... It is. It is. It is. It is. It is. It is. It is. (...). It is. It is. It is."}, {"heading": "2 Problem Statement and Notation", "text": "Similar to [21], the recommendation task considered in this paper takes implicit feedback [9] as training and test data. There are J items (e.g. articles or movies) in the dataset. For item j, there is a corresponding sequence consisting of Tj words, where the vector e (j) t indicates the t-th word by using the 1-of-S representation, i.e. a vector of length S with the value 1 in only one element that corresponds to the word and 0 in all other elements. Here, S is the word size of the dataset. We define an I-by-J binary rating matrix R = [Rij] I \u00d7 J in which I specify the number of users. For example, in the CiteULike dataset, Rij = 1 if the user has i article j in his or her personal library, and Rij = 0 otherwise."}, {"heading": "3 Collaborative Recurrent Autoencoder", "text": "In this section, we will first propose a generalization of the RNN called robust recurrent networks (RRN), followed by the introduction of two key concepts, wildcard denoising and beta pooling, into our model. Then, the generative process of CRAE will be presented to show how to generalize the RRN as a hierarchical Bayesian model from an i.i.d. setting to a CF (non-i.i.d.) setting."}, {"heading": "3.1 Robust Recurrent Networks", "text": "One problem with RNN models such as Long-Term Short-Term Storage Networks (LSTM) is that the calculation is deterministic without taking noise into account, which means that it is not robust, especially with insufficient training data. To address this robustness problem, we propose RRN as a kind of noisy gated RNN. In RRN, the gates and other latent variables are designed to incorporate noise, making the model more robust. Unlike [4, 5], noise in RRN is propagated back and forth directly in the network, without the need to use separate neural networks to assume the distributions of latent variables. It is much more efficient and easier to implement."}, {"heading": "3.2 Wildcard Denoising", "text": "Since input and output are identical here, as opposed to [19, 3], where input comes from the source language and output from the target language, this naive RRN autocoder may suffer from severe overadjustment even after taking noise into account and reversing the order (we note that reversing the order in the decoder [19] does not improve recommendation performance). A natural way to deal with this is to borrow ideas from the denosizing autocoder [20] by randomly dropping some of the words in the encoder. Unfortunately, dropping words directly can mislead to learning the transition between words. For example, if we drop the word \"is\" in the sentence \"is a good idea,\" the encoder will incorrectly learn the sub-sequence \"this a,\" which never occurs in a grammatically correct sentence."}, {"heading": "3.3 Beta-Pooling", "text": "To facilitate the factorization of the evaluation matrix, we need to bundle the sequence of vectors into a single fixed-length vector 2KW before it is further encoded into a K-dimensional vector. A natural way is to use a weighted average of vectors. In other words, we need a weight vector of different lengths for our pooling scheme. To address this problem, we propose to use a sequence of 8 vectors with 8 entries, while a sequence of 50 vectors needs one with 50 entries. In other words, we need a weight vector of different lengths for our pooling scheme. To address this problem, we propose to use a beta distribution. If six vectors are to be combined into a single vector (using the weighted average), we can use the range wp in the range (p \u2212 16, p 6) of the pooling axis of probability b = function b = weight distribution as a bed-y (always)."}, {"heading": "3.4 CRAE as a Hierarchical Bayesian Model", "text": "If you follow the notation in section 2 and the use of DRAE in section 3.2 = KW + T2j (1.), you get the generative process of CRAE (\u03b2) (Note: t indexes words or time steps, j index sentences or documents, and Tj is the number of words in document j): Encoding (t = 1, 2,., Tj: Generate x \u2032 (j) t \u2212 1, a (j) t \u2212 1, and s (j) t according to the equation (1) - (2).Compression and decompression (t = Tj + 1,.,.,., Equotes N (W1 (h) Tj (j) Tj) Tj) + b1, \u03bb \u2212 1 s IK) t (h), (h).Compression and decompression (t = Tj + 1,.,.,."}, {"heading": "3.5 Learning", "text": "According to the aforementioned CRAE model, all parameters such as h (j) t and vj can be treated as random variables, so a complete Bayesian treatment can be used as methods based on variable approximation. However, due to the extreme nonlinearity and CF setting, this type of treatment is not trivial. Furthermore, with CDL [24] and CTR [21] as our primary baseline, it is fairer to use a maximum of one posteriori (MAP) estimate, which is CDL and CTR do.End-to-end joint learning: maximizing the posterior probability is equivalent to maximizing the common log probability of {ui}, {vj}, W +, {vj}, {vj}, {e (j) t}, {e (j) t}, {e (j) t}, {e (j) t}, {h (j) t}, {h (j) t, and r ()."}, {"heading": "4 Experiments", "text": "In this section we report on some experiments with real data sets from different domains to evaluate the possibilities of recommendation and automatic generation of missing sequences."}, {"heading": "4.1 Datasets", "text": "We use two sets of data from different areas of the real world. CiteULike comes from [21] with 5,551 users and 16,980 articles (articles with text). Netflix consists of 407,261 users, 9,228 movies and 15,348,808 reviews after users with less than 3 positive reviews have been removed (following [24] reviews greater than 3 are considered positive ratings)."}, {"heading": "4.2 Evaluation Schemes", "text": "Recommendation: For the recommendation task, similar to [22, 24], the P-phrases associated with each user are randomly selected to form the training set, and the rest is used as a test set. We evaluate the models if the ratings are of different density (P-phrases {1, 2,.., 5}). For each value of P, we repeat the evaluation five times with different training sets and report on average performance. Following [21, 22], we use the retrieval as a measure of performance, since the evaluations take the form of implicit feedback [9, 12]. Specifically, a zero entry may be due to the fact that the user is not interested in the article or that the user is not aware of its existence. Thus, precision is not an appropriate measure of performance. We sort the predicted ratings of the candidate items and recommend the Top M items for the target user. The retrieval @ M item for each user is then liked as follows the Items = M @."}, {"heading": "4.3 Baselines and Experimental Settings", "text": "The models for comparison are listed as follows: \u2022 CMF: Collective Matrix Factorization [17] is a model that incorporates different sources of information by factorizing multiple matrices at the same time. \u2022 SVDFeature: SVDFeature [2] is a model for function-based collaborative filtering. In this essay, we use the bag of words as raw features to feed into SVDFeature. \u2022 DeepMusic: DeepMusic [10] is a leading model for the music recommendation mentioned in Section 1. \u2022 CDL: Collaborative Deep Learning (CDL) [24] is proposed as a basis. \u2022 CTR: Collaborative Topic Regression [21] is a model that performs theme modeling and collaborative filtering simultaneously, as mentioned in the previous section. \u2022 CDL: Collaborative Deep Learning (CDL) [24] is proposed as a probable delivery model for the shared learning of an Autocollaborative SRAC20 [SRACE] and Record-DAB [24]."}, {"heading": "4.4 Quantitative Comparison", "text": "As we can see, CTR outperforms all other baselines except CDL. Note that in both datasets, DeepMusic suffers greatly from overmatch when the rating matrix is extremely sparse (P = 1) and achieves comparable performance with CTR when the rating matrix is dense (P = 5). CDL as the strongest baseline consistently outperforms other baselines. By collectively performing proper content generation (sequences) and collaborative filtering, CRAE is able to outperform all baselines with a 0.7% lead, 1.9% (a relative increase of 2.0%) in CiteULike and 3.5% compared to 6.0% (a relative increase of 5.7%) in Netflix."}, {"heading": "5 Conclusions and Future Work", "text": "To the best of our knowledge, CRAE is the first model to bridge the gap between RNN and CF. Extensive experiments show that CRAE can significantly outperform the most advanced methods for both recommendation and sequence generation tasks. Due to its Bayesian nature, CRAE can easily be generalized to seamlessly integrate auxiliary information (e.g. the citations network for CiteULike and the co-director network for Netflix) to further enhance accuracy. In addition, several Bayesian recurring layers can be stacked together to enhance its visual power. In addition to making recommendations and guessing sequences on the fly, the wildcard automated repetition problem also has the potential to recover ancient documents."}, {"heading": "A Learning Beta-Pooling", "text": "The cumulative distribution function of the beta distribution F (x; a, b) = B (x; a, b) B (a, b) = Ix (a, b) = Ix (a, b), where B (x; a, b) = Ix (a, b) = Ix (a, b), where B (b) is the gamma function and Ix (a, b) is the incomplete beta function and the denominator B (a, b) = Ix (a)."}, {"heading": "B Qualitative Comparison", "text": "To get a better insight into CRAE, we train CRAE and CDL in the sparsest environment (P = 1) using the CiteULike dataset and use it to recommend articles for two sample users. The corresponding articles for target users in the training set and the top 10 recommended articles are presented in Table 4. Note that in the sparsest setting, the recommendation task is extremely difficult as there is only one article for each user in the training set.As we can see, CRAE successfully identified user I as a researcher working on information gathering with an interest in user modeling. As a result, CRAE achieves a high level of 60% accuracy by focusing its recommendations on articles on information gathering, user modeling and relevance feedback. On the other hand, the topics of the articles recommended by CDL are covered from visual tracking (Article 4) to bioinformatics (Article 3) and programming language (Article 8)."}, {"heading": "C Motivation of Beta-Pooling", "text": "The function fa, b (h (j) t; s (j) t = n) t) is to bundle the initial states h (j) t and the cell states s (j) t of the 2Tj steps (a 2KW -by-2Tj matrix) into a single vector of length 2KW. If we combine the cumulative distribution function of the beta distribution as F (x; a, b), p = (h (j) t; s (j) t = 1,., and p) t = (j) t = (j) t = (j) t = (j) t + 1; s (j) t + 1) for t = Tj + 1,., 2Tj, then we have havefa, b (h (j) t; s (j) t) t; 2 Tj) t = 2 (F (t 2Tj, a, b), b)."}, {"heading": "D Experiments on Beta-Pooling and Wildcard Denoising", "text": "As mentioned in the paper, beta pooling is able to integrate a sequence of 2Tj vectors into a single vector of the same size. Note that Tj can vary here for different j vectors. Hyper parameters a and b control the behavior of beta pooling. If a = b = 1, beta pooling is equivalent to the current pooling, the shape of the 2Tj vectors is shown. In an extreme case, one and b can be set so that the pooling result is equal to one of the 2Tj vectors (e.g. the Tj-th vector). Figure 4 shows the shape of the beta distribution for different a and b. Table 5 shows the corresponding memory of the distributions in Cite."}, {"heading": "E Hyperparameters", "text": "E.1 Hyperparameter SettingsThe vocabulary size S (with the word < wildcard > included) is 15,050 and 17,949 respectively for CiteULike and Netflix. For CMF and SVDFeature, optimal regulation hyperparameters are set for different P. The learning rate is set to 0.005 for SVDFeature. For DeepMusic, we find that the best performance is achieved with a CNN with two revolutionary layers. For CTR, we find that it can achieve good prediction performance if \u03bbu = 0.1, \u03bbv = 10 and K = 50. For CDL, we use hyperparameters similar to those in [24]. The denosis rate is set to 0.3. The dropout rate, \u03bbu, \u03bbv and \u0441n are set with the validation sets."}, {"heading": "F Robust Nonlinearity on Distributions", "text": "Unlike [24], nonlinear transformation after addition of noise is carried out with precision and accuracy. (In this case, the input of nonlinear transformation is a distribution and not a deterministic value, which makes nonlinearity more robust than in [24] and would lead to more efficient and direct learning algorithms than CDL. Consider a univariate Gaussian distribution N (x | \u00b5, \u03bb \u2212 1s) and the sigmoid function \u03c3 (x) = 1 + exp (\u2212 x). Expectation: E (x) = N (x | \u00b5, \u03bb \u2212 1s). (x) Equal distribution N (x, \u03bb \u2212 1s)."}, {"heading": "G Datasets", "text": "We use two sets of data from different areas of the real world, one from CiteULike 1 and the other from Netflix. The first set of data, CiteULike, comes from [21] with 5,551 users and 16,980 articles (articles). The titles of the articles are used as content information (word sequences) in our model. The second set, Netflix, consists of both the users \"movie ratings and the plots (content information) of the movies. After removing users with less than 3 positive ratings (following [24] ratings greater than 3 are considered positive ratings) and movies without plots, we get 407,261 users, 9,228 movies and 15,348,808 ratings in the final data set."}], "references": [{"title": "Deep learning. Book in preparation for", "author": ["Y. Bengio", "I.J. Goodfellow", "A. Courville"], "venue": null, "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2015}, {"title": "SVDFeature: a toolkit for feature-based collaborative filtering", "author": ["T. Chen", "W. Zhang", "Q. Lu", "K. Chen", "Z. Zheng", "Y. Yu"], "venue": null, "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2012}, {"title": "Learning phrase representations using RNN encoder-decoder for statistical machine translation", "author": ["K. Cho", "B. van Merrienboer", "C. Gulcehre", "D. Bahdanau", "F. Bougares", "H. Schwenk", "Y. Bengio"], "venue": "In EMNLP,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2014}, {"title": "A recurrent latent variable model for sequential data", "author": ["J. Chung", "K. Kastner", "L. Dinh", "K. Goel", "A.C. Courville", "Y. Bengio"], "venue": "In NIPS,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2015}, {"title": "Variational recurrent auto-encoders", "author": ["O. Fabius", "J.R. van Amersfoort"], "venue": "arXiv preprint arXiv:1412.6581,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2014}, {"title": "A non-iid framework for collaborative filtering with restricted Boltzmann machines", "author": ["K. Georgiev", "P. Nakov"], "venue": "In ICML,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2013}, {"title": "Session-based recommendations with recurrent neural networks", "author": ["B. Hidasi", "A. Karatzoglou", "L. Baltrunas", "D. Tikk"], "venue": "arXiv preprint arXiv:1511.06939,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2015}, {"title": "Long short-term memory", "author": ["S. Hochreiter", "J. Schmidhuber"], "venue": "Neural Computation,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 1997}, {"title": "Collaborative filtering for implicit feedback datasets", "author": ["Y. Hu", "Y. Koren", "C. Volinsky"], "venue": "In ICDM,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2008}, {"title": "Deep content-based music recommendation", "author": ["A.V.D. Oord", "S. Dieleman", "B. Schrauwen"], "venue": "In NIPS,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2013}, {"title": "BLEU: a method for automatic evaluation of machine translation", "author": ["K. Papineni", "S. Roukos", "T. Ward", "W.-J. Zhu"], "venue": "In ACL,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2002}, {"title": "BPR: Bayesian personalized ranking from implicit feedback", "author": ["S. Rendle", "C. Freudenthaler", "Z. Gantner", "L. Schmidt-Thieme"], "venue": "In UAI,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2009}, {"title": "Introduction to Recommender Systems Handbook", "author": ["F. Ricci", "L. Rokach", "B. Shapira"], "venue": null, "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2011}, {"title": "Low-rank matrix factorization for deep neural network training with high-dimensional output targets", "author": ["T.N. Sainath", "B. Kingsbury", "V. Sindhwani", "E. Arisoy", "B. Ramabhadran"], "venue": "In ICASSP,", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2013}, {"title": "Probabilistic matrix factorization", "author": ["R. Salakhutdinov", "A. Mnih"], "venue": "In NIPS,", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2007}, {"title": "Restricted Boltzmann machines for collaborative filtering", "author": ["R. Salakhutdinov", "A. Mnih", "G.E. Hinton"], "venue": "In ICML,", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2007}, {"title": "Relational learning via collective matrix factorization", "author": ["A.P. Singh", "G.J. Gordon"], "venue": "In KDD,", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2008}, {"title": "Unsupervised learning of video representations using lstms", "author": ["N. Srivastava", "E. Mansimov", "R. Salakhutdinov"], "venue": "In ICML,", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2015}, {"title": "Sequence to sequence learning with neural networks", "author": ["I. Sutskever", "O. Vinyals", "Q.V. Le"], "venue": "In NIPS,", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2014}, {"title": "Stacked denoising autoencoders: Learning useful representations in a deep network with a local denoising criterion", "author": ["P. Vincent", "H. Larochelle", "I. Lajoie", "Y. Bengio", "P.-A. Manzagol"], "venue": null, "citeRegEx": "20", "shortCiteRegEx": "20", "year": 2010}, {"title": "Collaborative topic modeling for recommending scientific articles", "author": ["C. Wang", "D.M. Blei"], "venue": "In KDD,", "citeRegEx": "21", "shortCiteRegEx": "21", "year": 2011}, {"title": "Collaborative topic regression with social regularization for tag recommendation", "author": ["H. Wang", "B. Chen", "W.-J. Li"], "venue": "In IJCAI,", "citeRegEx": "22", "shortCiteRegEx": "22", "year": 2013}, {"title": "Relational stacked denoising autoencoder for tag recommendation", "author": ["H. Wang", "X. Shi", "D. Yeung"], "venue": "In AAAI,", "citeRegEx": "23", "shortCiteRegEx": "23", "year": 2015}, {"title": "Collaborative deep learning for recommender systems", "author": ["H. Wang", "N. Wang", "D. Yeung"], "venue": "In KDD,", "citeRegEx": "24", "shortCiteRegEx": "24", "year": 2015}, {"title": "Towards Bayesian deep learning: A framework and some existing methods", "author": ["H. Wang", "D. Yeung"], "venue": null, "citeRegEx": "25", "shortCiteRegEx": "25", "year": 2016}, {"title": "Improving content-based and hybrid music recommendation using deep learning", "author": ["X. Wang", "Y. Wang"], "venue": "In ACM MM,", "citeRegEx": "26", "shortCiteRegEx": "26", "year": 2014}], "referenceMentions": [{"referenceID": 12, "context": "Existing methods for recommender systems can be roughly categorized into three classes [13]: content-based methods that use the user profiles or product descriptions only, collaborative filtering (CF) based methods that use the ratings only, and hybrid methods that make use of both.", "startOffset": 87, "endOffset": 91}, {"referenceID": 20, "context": "Among the hybrid methods, collaborative topic regression (CTR) [21] was proposed to integrate a topic model and probabilistic matrix factorization (PMF) [15].", "startOffset": 63, "endOffset": 67}, {"referenceID": 14, "context": "Among the hybrid methods, collaborative topic regression (CTR) [21] was proposed to integrate a topic model and probabilistic matrix factorization (PMF) [15].", "startOffset": 153, "endOffset": 157}, {"referenceID": 7, "context": "A more natural and adaptive way of modeling text sequences would be to use gated recurrent neural network (RNN) models [8, 3, 19].", "startOffset": 119, "endOffset": 129}, {"referenceID": 2, "context": "A more natural and adaptive way of modeling text sequences would be to use gated recurrent neural network (RNN) models [8, 3, 19].", "startOffset": 119, "endOffset": 129}, {"referenceID": 18, "context": "A more natural and adaptive way of modeling text sequences would be to use gated recurrent neural network (RNN) models [8, 3, 19].", "startOffset": 119, "endOffset": 129}, {"referenceID": 15, "context": "[16, 6, 7] use restricted Boltzmann machines and RNN instead of the conventional matrix factorization (MF) formulation to perform CF.", "startOffset": 0, "endOffset": 10}, {"referenceID": 5, "context": "[16, 6, 7] use restricted Boltzmann machines and RNN instead of the conventional matrix factorization (MF) formulation to perform CF.", "startOffset": 0, "endOffset": 10}, {"referenceID": 6, "context": "[16, 6, 7] use restricted Boltzmann machines and RNN instead of the conventional matrix factorization (MF) formulation to perform CF.", "startOffset": 0, "endOffset": 10}, {"referenceID": 13, "context": "[14] uses low-rank MF in the last weight layer of a deep network to reduce the number of parameters, but it is for classification instead of recommendation tasks.", "startOffset": 0, "endOffset": 4}, {"referenceID": 9, "context": "There have also been nice explorations on music recommendation [10, 26] in which a CNN or deep belief network (DBN) is directly used for content-based recommendation.", "startOffset": 63, "endOffset": 71}, {"referenceID": 25, "context": "There have also been nice explorations on music recommendation [10, 26] in which a CNN or deep belief network (DBN) is directly used for content-based recommendation.", "startOffset": 63, "endOffset": 71}, {"referenceID": 23, "context": "Very recently, collaborative deep learning (CDL) [24] is proposed as a probabilistic model for joint learning of a probabilistic stacked denoising autoencoder (SDAE) [20] and collaborative filtering.", "startOffset": 49, "endOffset": 53}, {"referenceID": 19, "context": "Very recently, collaborative deep learning (CDL) [24] is proposed as a probabilistic model for joint learning of a probabilistic stacked denoising autoencoder (SDAE) [20] and collaborative filtering.", "startOffset": 166, "endOffset": 170}, {"referenceID": 20, "context": "Similar to [21], the recommendation task considered in this paper takes implicit feedback [9] as the training and test data.", "startOffset": 11, "endOffset": 15}, {"referenceID": 8, "context": "Similar to [21], the recommendation task considered in this paper takes implicit feedback [9] as the training and test data.", "startOffset": 90, "endOffset": 93}, {"referenceID": 3, "context": "Note that unlike [4, 5], the noise in RRN is directly propagated back and forth in the network, without the need for using separate neural networks to approximate the distributions of the latent variables.", "startOffset": 17, "endOffset": 23}, {"referenceID": 4, "context": "Note that unlike [4, 5], the noise in RRN is directly propagated back and forth in the network, without the need for using separate neural networks to approximate the distributions of the latent variables.", "startOffset": 17, "endOffset": 23}, {"referenceID": 0, "context": "Note that RRN can be seen as a generalized and Bayesian version of LSTM [1].", "startOffset": 72, "endOffset": 75}, {"referenceID": 18, "context": "Similar to [19, 3], two RRNs can be concatenated to form an encoder-decoder architecture.", "startOffset": 11, "endOffset": 18}, {"referenceID": 2, "context": "Similar to [19, 3], two RRNs can be concatenated to form an encoder-decoder architecture.", "startOffset": 11, "endOffset": 18}, {"referenceID": 18, "context": "Since the input and output are identical here, unlike [19, 3] where the input is from the source language and the output is from the target language, this naive RRN autoencoder can suffer from serious overfitting, even after taking noise into account and reversing sequence order (we find that", "startOffset": 54, "endOffset": 61}, {"referenceID": 2, "context": "Since the input and output are identical here, unlike [19, 3] where the input is from the source language and the output is from the target language, this naive RRN autoencoder can suffer from serious overfitting, even after taking noise into account and reversing sequence order (we find that", "startOffset": 54, "endOffset": 61}, {"referenceID": 18, "context": "reversing sequence order in the decoder [19] does not improve the recommendation performance).", "startOffset": 40, "endOffset": 44}, {"referenceID": 19, "context": "One natural way of handling it is to borrow ideas from the denoising autoencoder [20] by randomly dropping some of the words in the encoder.", "startOffset": 81, "endOffset": 85}, {"referenceID": 24, "context": "From the generative process, we can see that both CRAE and CDL are Bayesian deep learning (BDL) models (as described in [25]) with a perception component (DRAE in CRAE) and a task-specific component.", "startOffset": 120, "endOffset": 124}, {"referenceID": 23, "context": "Besides, with CDL [24] and CTR [21] as our primary baselines, it would be fairer to use maximum a posteriori (MAP) estimates, which is what CDL and CTR do.", "startOffset": 18, "endOffset": 22}, {"referenceID": 20, "context": "Besides, with CDL [24] and CTR [21] as our primary baselines, it would be fairer to use maximum a posteriori (MAP) estimates, which is what CDL and CTR do.", "startOffset": 31, "endOffset": 35}, {"referenceID": 23, "context": "Robust nonlinearity on distributions: Different from [24, 23], nonlinear transformation is performed after adding the noise with precision \u03bbs (e.", "startOffset": 53, "endOffset": 61}, {"referenceID": 22, "context": "Robust nonlinearity on distributions: Different from [24, 23], nonlinear transformation is performed after adding the noise with precision \u03bbs (e.", "startOffset": 53, "endOffset": 61}, {"referenceID": 23, "context": "In this case, the input of the nonlinear transformation is a distribution rather than a deterministic value, making the nonlinearity more robust than in [24, 23] and leading to more efficient and direct learning algorithms than CDL.", "startOffset": 153, "endOffset": 161}, {"referenceID": 22, "context": "In this case, the input of the nonlinear transformation is a distribution rather than a deterministic value, making the nonlinearity more robust than in [24, 23] and leading to more efficient and direct learning algorithms than CDL.", "startOffset": 153, "endOffset": 161}, {"referenceID": 23, "context": "This way the feedforward computation of DRAE would be seamlessly chained together, leading to more efficient learning algorithms than the layer-wise algorithms in [24, 23] (see Section F of the supplementary materials for more details).", "startOffset": 163, "endOffset": 171}, {"referenceID": 22, "context": "This way the feedforward computation of DRAE would be seamlessly chained together, leading to more efficient learning algorithms than the layer-wise algorithms in [24, 23] (see Section F of the supplementary materials for more details).", "startOffset": 163, "endOffset": 171}, {"referenceID": 20, "context": "CiteULike is from [21] with 5,551 users and 16,980 items (articles with text).", "startOffset": 18, "endOffset": 22}, {"referenceID": 23, "context": "Netflix consists of 407,261 users, 9,228 movies, and 15,348,808 ratings after removing users with less than 3 positive ratings (following [24], ratings larger than 3 are regarded as positive ratings).", "startOffset": 138, "endOffset": 142}, {"referenceID": 21, "context": "Recommendation: For the recommendation task, similar to [22, 24], P items associated with each user are randomly selected to form the training set and the rest is used as the test set.", "startOffset": 56, "endOffset": 64}, {"referenceID": 23, "context": "Recommendation: For the recommendation task, similar to [22, 24], P items associated with each user are randomly selected to form the training set and the rest is used as the test set.", "startOffset": 56, "endOffset": 64}, {"referenceID": 20, "context": "Following [21, 22], we use recall as the performance measure since the ratings are in the form of implicit feedback [9, 12].", "startOffset": 10, "endOffset": 18}, {"referenceID": 21, "context": "Following [21, 22], we use recall as the performance measure since the ratings are in the form of implicit feedback [9, 12].", "startOffset": 10, "endOffset": 18}, {"referenceID": 8, "context": "Following [21, 22], we use recall as the performance measure since the ratings are in the form of implicit feedback [9, 12].", "startOffset": 116, "endOffset": 123}, {"referenceID": 11, "context": "Following [21, 22], we use recall as the performance measure since the ratings are in the form of implicit feedback [9, 12].", "startOffset": 116, "endOffset": 123}, {"referenceID": 9, "context": "Exactly the same as [10], the cutoff point is set at 500 for each user.", "startOffset": 20, "endOffset": 24}, {"referenceID": 10, "context": "The BLEU score [11] is used to evaluate the quality of generation.", "startOffset": 15, "endOffset": 19}, {"referenceID": 18, "context": "For this reason, this task is impossible for existing machine translation models like [19, 3].", "startOffset": 86, "endOffset": 93}, {"referenceID": 2, "context": "For this reason, this task is impossible for existing machine translation models like [19, 3].", "startOffset": 86, "endOffset": 93}, {"referenceID": 16, "context": "The models for comparison are listed as follows: \u2022 CMF: Collective Matrix Factorization [17] is a model incorporating different sources of information by simultaneously factorizing multiple matrices.", "startOffset": 88, "endOffset": 92}, {"referenceID": 1, "context": "\u2022 SVDFeature: SVDFeature [2] is a model for feature-based collaborative filtering.", "startOffset": 25, "endOffset": 28}, {"referenceID": 9, "context": "\u2022 DeepMusic: DeepMusic [10] is a feedforward model for music recommendation mentioned in Section 1.", "startOffset": 23, "endOffset": 27}, {"referenceID": 20, "context": "\u2022 CTR: Collaborative Topic Regression [21] is a model performing topic modeling and collaborative filtering simultaneously as mentioned in the previous section.", "startOffset": 38, "endOffset": 42}, {"referenceID": 23, "context": "\u2022 CDL: Collaborative Deep Learning (CDL) [24] is proposed as a probabilistic feedforward model for joint learning of a probabilistic SDAE [20] and CF.", "startOffset": 41, "endOffset": 45}, {"referenceID": 19, "context": "\u2022 CDL: Collaborative Deep Learning (CDL) [24] is proposed as a probabilistic feedforward model for joint learning of a probabilistic SDAE [20] and CF.", "startOffset": 138, "endOffset": 142}, {"referenceID": 18, "context": "2, this task is impossible for existing machine translation models like [19, 3] due to the lack of source sequences.", "startOffset": 72, "endOffset": 79}, {"referenceID": 2, "context": "2, this task is impossible for existing machine translation models like [19, 3] due to the lack of source sequences.", "startOffset": 72, "endOffset": 79}, {"referenceID": 19, "context": "71% while the number for CRAE with conventional denoising [20] (dropping words completely) is 10.", "startOffset": 58, "endOffset": 62}, {"referenceID": 18, "context": "Note that DRAE is a much more general model than RNN autoencoders like [19, 18].", "startOffset": 71, "endOffset": 79}, {"referenceID": 17, "context": "Note that DRAE is a much more general model than RNN autoencoders like [19, 18].", "startOffset": 71, "endOffset": 79}, {"referenceID": 18, "context": "We also try reversing the order of each sequence in the decoder RNN as in [19, 18], but the performance only changes slightly.", "startOffset": 74, "endOffset": 82}, {"referenceID": 17, "context": "We also try reversing the order of each sequence in the decoder RNN as in [19, 18], but the performance only changes slightly.", "startOffset": 74, "endOffset": 82}, {"referenceID": 23, "context": "For CDL, we use similar hyperparameters as mentioned in [24].", "startOffset": 56, "endOffset": 60}, {"referenceID": 23, "context": "Different from [24], nonlinear transformation is performed after adding the noise with precision \u03bbs.", "startOffset": 15, "endOffset": 19}, {"referenceID": 23, "context": "In this case, the input of the nonlinear transformation is a distribution rather than a deterministic value, making the nonlinearity more robust than in [24] and leading to more efficient and direct learning algorithms than CDL.", "startOffset": 153, "endOffset": 157}, {"referenceID": 23, "context": "This way the feedforward computation of DRAE would be seamlessly chained together, leading to more efficient learning algorithms than the layer-wise algorithms in [24].", "startOffset": 163, "endOffset": 167}, {"referenceID": 20, "context": "The first dataset, CiteULike, is from [21] with 5,551 users and 16,980 items (articles).", "startOffset": 38, "endOffset": 42}, {"referenceID": 23, "context": "After removing users with less than 3 positive ratings (following [24], ratings larger than 3 are regarded as positive ratings) and movies without plots, we get 407,261 users, 9,228 movies, and 15,348,808 ratings in the final dataset.", "startOffset": 66, "endOffset": 70}], "year": 2016, "abstractText": "Hybrid methods that utilize both content and rating information are commonly used in many recommender systems. However, most of them use either handcrafted features or the bag-of-words representation as a surrogate for the content information but they are neither effective nor natural enough. To address this problem, we develop a collaborative recurrent autoencoder (CRAE) which is a denoising recurrent autoencoder (DRAE) that models the generation of content sequences in the collaborative filtering (CF) setting. The model generalizes recent advances in recurrent deep learning from i.i.d. input to non-i.i.d. (CF-based) input and provides a new denoising scheme along with a novel learnable pooling scheme for the recurrent autoencoder. To do this, we first develop a hierarchical Bayesian model for the DRAE and then generalize it to the CF setting. The synergy between denoising and CF enables CRAE to make accurate recommendations while learning to fill in the blanks in sequences. Experiments on real-world datasets from different domains (CiteULike and Netflix) show that, by jointly modeling the order-aware generation of sequences for the content information and performing CF for the ratings, CRAE is able to significantly outperform the state of the art on both the recommendation task based on ratings and the sequence generation task based on content information.", "creator": "LaTeX with hyperref package"}}}