{"id": "1611.00801", "review": {"conference": "acl", "VERSION": "v1", "DATE_OF_SUBMISSION": "2-Nov-2016", "title": "A FOFE-based Local Detection Approach for Named Entity Recognition and Mention Detection", "abstract": "In this paper, we study a novel approach for named entity recognition (NER) and mention detection in natural language processing. Instead of treating NER as a sequence labelling problem, we propose a new local detection approach, which rely on the recent fixed-size ordinally forgetting encoding (FOFE) method to fully encode each sentence fragment and its left/right contexts into a fixed-size representation. Afterwards, a simple feedforward neural network is used to reject or predict entity label for each individual fragment. The proposed method has been evaluated in several popular NER and mention detection tasks, including the CoNLL 2003 NER task and TAC-KBP2015 and TAC-KBP2016 Tri-lingual Entity Discovery and Linking (EDL) tasks. Our methods have yielded pretty strong performance in all of these examined tasks. This local detection approach has shown many advantages over the traditional sequence labelling methods.", "histories": [["v1", "Wed, 2 Nov 2016 20:52:46 GMT  (170kb,D)", "http://arxiv.org/abs/1611.00801v1", null]], "reviews": [], "SUBJECTS": "cs.CL", "authors": ["mingbin xu", "hui jiang"], "accepted": true, "id": "1611.00801"}, "pdf": {"name": "1611.00801.pdf", "metadata": {"source": "CRF", "title": "A FOFE-based Local Detection Approach for Named Entity Recognition and Mention Detection", "authors": ["Mingbin Xu", "Hui Jiang"], "emails": ["xmb@cse.yorku.ca", "hj@cse.yorku.ca"], "sections": [{"heading": "1 Introduction", "text": "Natural language processing (NLP) has been extensively researched for many decades. Conventional NLP techniques include rule-based symbolic approaches that are well over 20 years old, and newer statistical approaches based on feature engineering and relatively simple statistical models, such as the condition that in the last few years we have had a fundamental problem based on deep learning in many other areas of application, from speech recognition to image classification. These approaches are attracting more and more attention in the NLP community. Among many different NLP problems, we are interested in a fundamental problem in the NLP, namely entity recognition (NER), and mention the recognition that recognition is a very demanding task in the NLP community."}, {"heading": "2 Related Work", "text": "The success of word embedding (Mikolov et al., 2013) encourages researchers to focus on machine-learning representation rather than heavy feature engineering in NLP. The use of word embedding as a typical feature representation for words, NNs are competing with traditional approaches in NLP. Many NLP tasks, such as NER, Chunking, and Part-of-Speech (POS) tagging can be formulated as sequence tagging tasks. In (Collobert et al., 2011), deep revolutionary neural networks (CNN) and conditional random fields (CRF) are used to give NLers labels at a sentence level where they still use many handcrafted features to improve performance, such as capitalization features that explicitly focus on first-letter capital, non-initial capital, and so on."}, {"heading": "3 Preliminary", "text": "In this section, we will briefly discuss some of the background techniques that are important to our proposed NER and recognition approach."}, {"heading": "3.1 Deep Feedforward Neural Networks", "text": "It is generally known that a forward-facing neural network is a universal approximator under certain conditions (Hornik, 1991). A forward-facing neural network is a weighted graph with a layered architecture. Each layer consists of several nodes, including a bias node, the value of which is always 1. Successive layers are completely connected to each other. Nodes in each layer take as input the values of the nodes in the previous layer and calculate a function of these values by the connection weights as output. Formally, leave xn, j denote the value of the j-th node in the n-th layer and Wni, j denote the weight of the connection from xn, i to xn + 1, j. Thenzn + 1, j = i Wni, jxn, i (1) xn + 1, j) (2), where the activation function is generally chosen as a sigmoid."}, {"heading": "3.2 Fixed-size Ordinally Forgetting Encoding", "text": "However, it requires the use of fixed-size input and lacks the ability to capture long-term dependencies in sequences. Since most NLP problems are associated with variable 0.5x word sequences, RNNs / LSTMs are more arbitrary in dealing with these problems than conventional NNs. The simple encryption method, called Fixed-size ForgettingEncoding (FOFE), originally proposed in (Zhang et al., 2015), clearly exceeds the limitations of DNNs because it can encrypt a variable word sequence into a specified word sequence without losing information."}, {"heading": "3.3 Character-level Models in NLP", "text": "Recently, as shown in (Kim et al., 2015), it could be beneficial for model morphology at the character level, as it offers some additional advantages when dealing with unknown or vocabulary (OOVs) words in a language.The above FOFE method can easily be extended to the string in NLP. Any word, phrase, or fragment in question can be considered as a string. In this way, we can apply the same FOFE method to encode the string. (This always results in a well-defined representation that is irrelevant to the number of characters in question.) For example, a word fragment of \"iFLYTEK\" can be considered as a sequence of seven characters: \"i,\" \"F,\" \"Y,\" \"T,\" \"E,\" \"K.\" The FOFE codes of the letter kith are always fixed and can be forwarded directly to a network."}, {"heading": "4 FOFE-based Local Detection for NER", "text": "As described above, our FOFENER-based local recognition approach to NER, hereinafter referred to as FOFE-NER, is motivated by the way in which humans actually infer whether a word segment in the text is an entity or a mention, although the entity types of the other entities in the same sentence are not mandatory. Specifically, the dependence between adjacent entities is relatively weak in NER problems. Whether a fragment is an entity or not and what class it may belong to largely depends on the internal structure of the fragment itself, as well as the left and right context in which it appears. To a large extent, the meaning and spelling of the underlying fragment are meaningful in distinguishing named entities from the rest of the text. Contexts play a very important role in NER or mention recognition when it encompasses multimeaningful words / phrases or words (OV) (OV) words (NeV) that lead to certain entities (entities) as entities up to a certain length (E)."}, {"heading": "4.1 Word-level Features", "text": "FOFE-NER generates multiple word-level characteristics for each fragment hypothesis and its left and right contexts as follows: \u2022 Sack-of-Word vector of the fragment. For the example in Figure 1, it is a bag-of-word vector of 'Toronto', 'Maple' and 'Leafs'. \u2022 FOFE code for the left context including the fragment. In Figure 1, it is the FOFE code of the word sequence of... \"Puck from Space for the Toronto Maple Leafs.\" \u2022 FOFE code for the left context excluding the fragment. In Figure 1, it is the FOFE code of the word sequence of... \"Puck from Space for the.\" \u2022 FOFE code for the right context including the fragment. In Figure 1, it is the FOFE code for the left context excluding the fragment."}, {"heading": "4.2 Character-level Features", "text": "In addition to the word-level characteristics described above, we are also expanding character-level characteristics for the underlying segment hypothesis to further model its morphological structure. In the example in Figure 1, the current fragment, Toronto Maple Leafs, is treated as a sequence of uppercase and lowercase letters, i.e. \"{'T,\"' o,...,'s', \"we then add the following character-level characteristics for this fragment: \u2022 Left-to-right FOFE code of the string of the underlying fragment. This is the FOFE code of the sequence,\"'s,' f,..., 'f,','s. \"\u2022 Right-to-left FOFE code of the string of the underlying fragment. This is the FOFE code of the sequence,\" s', 'f'..., 'o', ',', '."}, {"heading": "5 Training and Decoding Algorithm", "text": "In fact, it is so that most of us are able to abide by the rules that we have set ourselves to change the world, \"he said in an interview with the\" New York Times, \"in which he deals with the\" New York Times, \"the\" New York Times, \"the\" New York Times, \"the\" New York Times, \"the\" New York Times, \"the\" New York Times, \"the\" New York Times, \"the\" New York Times, \"the\" New York Times, \"the\" New York Times, \"the\" New York Times, \"the\" New York Times, \"the\" New York Times, \"the\" New York Times, \"the\" the \"New York Times,\" the \"the\" New York Times, \"the\" the \"the\" New York, \"the\" the \"the\" New York Times, \"the\" the \"the\" New York, \"the\" the \"New York Times,\" the \"the\" the \"New York Times,\" the \"the\" the \"New York Times,\" the \"the\" the \"the\" New York Times, \"the\" the \"the\" the \"the\" New York, \"the\" the \"the\" the \"the\" the \"New York Times, the\" the \"the\" the \"the\" the \"the\" the \"New York, the\" the \"the\" the \"the\" the \"the\" the \"New York, the\" the \"the\" the \"the\" the \"the\" the \"the\" the \"New York, the\" the \"the\" the \"the\" the \""}, {"heading": "6 Experiments", "text": "In this section, we will evaluate the effectiveness of our proposed methods in several popular NER tasks and mention detection tasks, including task CoNLL 2003 NER and tasks TAC-KBP2015 and TAC-KBP2016 Tri-lingual Entity Discovery and Linking (EDL) 1."}, {"heading": "6.1 CoNLL 2003 NER task", "text": "The CoNLL-2003 dataset (Sang and Meulder, 2003) consists of message wire from the Reuters RCV1 corpus marked with four types of designated entities: Location (LOC), Organization (ORG), Person (PER), and Miscellaneous (MISC).We have examined the performance of our method on the CoNLL-2003 dataset using various combinations of FOFE characteristics (both word and character level).The detailed comparison results are shown in Table 1. In Table 2, we compared our best performance with some powerful neural network systems for this task. As we can see from Table 2, our system provides very strong performance in this task (90.71 in the F1 score), exceeding most neural network models reported on this dataset. More importantly, we have not used any handcrafted features in our systems, and all features used in this task are automatically derived from either the FOE (word level or FOE)."}, {"heading": "6.2 KBP2015 EDL Task", "text": "Faced with a collection of documents in three languages (English, Chinese and Spanish) as input, the trilingual EDL task KBP2015 (Ji et al., 2015) requires the automatic identification of entity mentions from a collection of text documents in several languages (English, Chinese and Spanish) and their classification into one of the following predefined five types: Person (PER), Geopolitical Unit (GPE), Organization (ORG), Location (LOC) and Facility (FAC) and their association with an existing English Knowledge Base (KB), and1We have made our codes available to readers at https: / / github.com / xmb-cipher / fofe-ner to reflect the results in this paper."}, {"heading": "6.3 KBP2016 EDL task", "text": "In KBP2016, the trilingual EDL task is expanded to recognize nominal mentions of all 5 entity types for all three languages. In our experiments, we treat nominal mentions types only as a few additional entity types and recognize them together with named entities in a single model. We evaluated our proposed FOFE-based local detection method for entity discovery in the KBP2015 dataset and used this method to participate in the official trilingual EDL evaluation of KBP2016. Below, we report on the performance of our method in these KBP EDL tasks."}, {"heading": "6.4 Training Data", "text": "For the trilingual EDL task KBP2015, we use the following data sets as training data to learn the NEW and mention detection models. \u2022 Training and evaluation data in KBP2015: In last year's competition, 335 English documents, 313 Chinese documents and 296 Spanish documents were commented on for training and evaluation, for a total of 944 documents. In this data set, all five named mentions (PER, ORG, GPE, LOC, FAC) and only one nominal mention (PER) are labelled. In KBP2016, the nominal mention has been extended to all 5 classes of named entities. \u2022 Machine-labelled Wikipedia: When terms or names are mentioned for the first time in a Wikipedia article, they are often hyperlinked to the relevant Wikipedia page, clearly highlighting the possible named entities with well-defined boundaries in the text."}, {"heading": "6.5 Data Preprocessing", "text": "The data from KBP2015 and KBP2016 is in XML format. Our pre-processing tools only extract text surrounded by two adjacent XML tags for later stages, because XML tags are usually metadata and irrelevant to our task. Values of all author attributes are extracted from all post tags directly labeled as PER. Extracted text is sent to the Stanford CoreNLP toolkit for sentence splitting and tokenization. Any words that contain digits are mapped to several predefined tokens, such as < number >, < date >, using some regular expression matches."}, {"heading": "6.6 Hyperparameter optimization", "text": "We perform grid searches on several hyperparameters, including initial learning, mini-batch size, initial dropout, number of layers, size of hidden layer, number of epochs, on the endured validation set. Each hyperparameter typically has 3 to 5 options during the grid search. Here we summarize the set of hyperparameters used in our experiments: \u2022 KBP series: i) Number of epochs: we usually execute 256 epochs when the iFLYTEK data is not used in the training. Otherwise, we execute only 64 epochs. ii) Learning rate: It is initially set to 0.128 and is gradually reduced by multiplying a number at the end of each epoch so that it reaches 1 / 16 of the initial value at the end of the entire training process."}, {"heading": "6.7 Effect of various training data", "text": "In our first series of experiments, we examine the impact of using different training data sets on final entity recognition performance. Different training runs are performed using different combinations of the above data sources. In Table 4, we have summarized the official English entity recognition results of three systems that we submitted to the EDL1 evaluation of KBP2016. The first system that uses only the KBP2015 data to train the model reached a value of 0.693 inF1 in the official English KBP2016 evaluation data. After adding the weakly marked data, WIKI, we can see that the entity recognition performance is 0.707 in Formula 1. Finally, we can see that it delivers the best performance by using the KBP2015 data and the iFLYTEK internal data sets to train our models, giving 0.731 in Formula 1."}, {"heading": "6.8 The official performance in KBP2016 EDL evaluation", "text": "After correcting some system errors, we used both the KBP2015 data and the iFLYTEK data to retrain our models for three languages, and finally subjected three systems to the final EDL2 evaluation of KBP2016. Here, the official results of two systems are summarized in Table 5. In our systems, we treat all nominal mentions as special types of designated units, and both designated and nominal units are recognized on the basis of a model. Here, we have broken down system performance by different languages and categories of units (named or nominal). In RUN1, we submitted our best NER system and in the trilingual EDL evaluation system of KBP2016, we achieved approximately 0.718 in F1 value. This is a very strong performance among all participating teams of KBP2016. In RUN3, we submitted system merger results by combining our results with the best results from another participating KBP2016 systems, using a high recall rate of 754, as we have achieved both of these systems very well."}, {"heading": "7 Conclusion", "text": "In this paper, we have proposed a new approach based on local recognition, based on the most recent Fixed Size Ordinally Forgotten Encoding (FOFE) method to fully encode each fragment and its left-right contexts into a fixed size representation, and then use a simple feedback neural network to reject or predict entity markers for each fragment. The proposed method has been evaluated in several popular NER and recognition tasks, including the CoNLL 2003 NER task and TAC-KBP2015 and TAC-KBP2016 Tri-lingual Entity Discovery and Linking (EDL) tasks. Our methods have performed fairly well in all of these investigated tasks. Obviously, this FOFE-based local recognition approach can easily be extended to many other NLP tasks, such as chunking, POS tagging, entity linking, and setic parsing."}], "references": [], "referenceMentions": [], "year": 2016, "abstractText": "In this paper, we study a novel approach for named entity recognition (NER) and mention detection in natural language processing. Instead of treating NER as a sequence labelling problem, we propose a new local detection approach, which rely on the recent fixed-size ordinally forgetting encoding (FOFE) method to fully encode each sentence fragment and its left/right contexts into a fixed-size representation. Afterwards, a simple feedforward neural network is used to reject or predict entity label for each individual fragment. The proposed method has been evaluated in several popular NER and mention detection tasks, including the CoNLL 2003 NER task and TAC-KBP2015 and TAC-KBP2016 Trilingual Entity Discovery and Linking (EDL) tasks. Our methods have yielded pretty strong performance in all of these examined tasks. This local detection approach has shown many advantages over the traditional sequence labelling methods.", "creator": "LaTeX with hyperref package"}}}