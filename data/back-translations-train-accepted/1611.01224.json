{"id": "1611.01224", "review": {"conference": "iclr", "VERSION": "v1", "DATE_OF_SUBMISSION": "3-Nov-2016", "title": "Sample Efficient Actor-Critic with Experience Replay", "abstract": "This paper presents an actor-critic deep reinforcement learning agent with experience replay that is stable, sample efficient, and performs remarkably well on challenging environments, including the discrete 57-game Atari domain and several continuous control problems. To achieve this, the paper introduces several innovations, including truncated importance sampling with bias correction, stochastic dueling network architectures, and a new trust region policy optimization method.", "histories": [["v1", "Thu, 3 Nov 2016 23:21:32 GMT  (1409kb,D)", "http://arxiv.org/abs/1611.01224v1", "20 pages. Prepared for ICLR 2017"], ["v2", "Mon, 10 Jul 2017 14:38:10 GMT  (2708kb,D)", "http://arxiv.org/abs/1611.01224v2", "20 pages. Prepared for ICLR 2017"]], "COMMENTS": "20 pages. Prepared for ICLR 2017", "reviews": [], "SUBJECTS": "cs.LG", "authors": ["ziyu wang", "victor bapst", "nicolas heess", "volodymyr mnih", "remi munos", "koray kavukcuoglu", "nando de freitas"], "accepted": true, "id": "1611.01224"}, "pdf": {"name": "1611.01224.pdf", "metadata": {"source": "CRF", "title": null, "authors": ["Ziyu Wang", "Victor Bapst"], "emails": ["ziyu@google.com", "vbapst@google.com", "heess@google.com", "vmnih@google.com", "Munos@google.com", "korayk@google.com", "nandodefreitas@google.com"], "sections": [{"heading": "1 INTRODUCTION", "text": "In fact, most of the people who are able to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move"}, {"heading": "2 BACKGROUND AND PROBLEM SETUP", "text": "Consider an agent interacting with his environment through discrete time steps. In time step t, the agent observes the nx-dimensional state vectors xt and X-Rnx, selects an action based on a policy \u03c0 (a | xt), and observes a reward signal produced by the environment. We become discrete actions based on sections 1, 2,.., Na} in sections 3 and 4, and continuous actions based on sections A-Rna in section 5. The agent's goal is to maximize the discounted return Rt = 3,. The discount factor \u03b3 + i in expectation. [0, 1) Trades-off the meaning of immediate and future rewards. For an agent following the policy, we use the standard definitions of state-acting and state-value functions only: Q\u03c0 (xt, at) = Excel + 1: Er + 1, based on expectations."}, {"heading": "3 DISCRETE ACTOR CRITIC WITH EXPERIENCE REPLAY", "text": "It may be an obvious strategy to improve the sampling efficiency of stakeholders, but managing the variance and stability of external policy assessors is notoriously difficult, and in our context it works as follows: provided we consider an approach to external policy assessments (Meuleau et al., 2000; Jie & Abbeel, 2010; Levine & Koltun, 2013), provided we draw an approach from our memory of experience, the importance of weighted policy areas is then measured by: g imp = (k) imp = 0 imp = 0 imp = 0 imp = 0 imp = 0 imp = 0 imp = 0 imp = 0 imp = 0 imp = 0 imp = 0 imp = i)."}, {"heading": "3.1 MULTI-STEP ESTIMATION OF THE STATE-ACTION VALUE FUNCTION", "text": "(We also experimented with the corresponding tree backup method used by Precup et al. (2000), but found that retrace performs better in practice.) Given a behavior-generated approach \u00b5, the retrace estimator can be expressed recursively as following1: Qret (xt, at) = rt + predictive bias + 1 [Q ret (xt + 1, at + 1)] + \u03b3V (xt + 1), (5) where it is the truncated weight, \u03c1 t = min {c, \u03c1t} with bias (at | xt). Q (xt + 1, at + 1) is the current estimate of Q\u03c0, and V (x) = Ea quantitative prediction of policy v (x, a). Retrace is an off-policy, yield-based algorithm that exhibits slight variations and proves to be a convergent prediction."}, {"heading": "3.2 IMPORTANCE WEIGHT TRUNCATION WITH BIAS CORRECTION", "text": "The marginal weights of meaning in Equation (4) can become large and thus cause instability (>). To arm ourselves against high variance, we propose to shorten the weights of importance and introduce a correction term for the following decomposition of Gmarg: Gmarg = Extat (a) \u2212 c (at | xt) Q\u03c0 (a) = Ext [Eat [bit, at] = Exit [bit] - Correction term (at | xt) Q\u03c0 (xt, at) + Ea. An alternative to rerace here is Q (c) with extra-political corrections (Harutyunyan et al., 2016), which we discuss in Appendix B., where we discuss more details."}, {"heading": "3.3 EFFICIENT TRUST REGION POLICY OPTIMIZATION", "text": "The policy updates of past strategies often show a high variance. To ensure stability, we must limit the per-step changes to the strategies. (...) Simply using smaller learning rates is insufficient as they cannot protect against the occasional major updates while maintaining a desired learning speed. (...) Despite the effectiveness of their TRPO method, it requires repeated calculations of Fisher vector products for each update. This can prove prohibitively expensive in large domains. (...) In this section, we present a new trust policy optimization method that addresses major problems. Instead of restricting the updated policy to be close to current policy (as in the TRPO), we propose maintaining an average political network that represents the average of past strategies and does not deviate from it far."}, {"heading": "4 RESULTS ON ATARI", "text": "We are using the learning environment of Bellemare et al. (2013) to perform a comprehensive evaluation, using a single algorithm and a network architecture, with fixed hyperparameters, to learn how to play 57 Atari games that contain only raw pixel observations and game rewards, a task that is very challenging because the variety of games and the high-dimensional pixel level have observations, and our experimental setup uses 16 actor-learner threads running on a single machine without GPUs. We adopt the same input pre-processing and network architecture as Mnih et al. (2015) Specifically, the network consists of a convolutional layer with 32 8 \u00d7 8 filters, followed by another convolutional layer with 64 4 \u00d7 4 filters, followed by a final convolutional layer with 64 3 \u00d7 3 filters, followed by a final convolutional layer with 64 3 \u00d7 3 filters, followed by a full layer of 512."}, {"heading": "5 CONTINUOUS ACTOR CRITIC WITH EXPERIENCE REPLAY", "text": "In order to extend ACER to continuous scope of action, we have to overcome some important challenges. In particular, retreat requires estimates of Q and V, but we can no longer easily integrate using Q to derive V. A solution to this problem and changes necessary for updating the trust regions will follow in this section."}, {"heading": "5.1 POLICY EVALUATION", "text": "We could use the weight profile to calculate the V\u03b8v given by Wang et al. (2016), but this estimator exhibits a high variance. We propose a new architecture that we call Stochastic Duel Networks (SDN), inspired by the duel networks of Wang et al. (2016), which is designed to estimate both V \u03c0 and Q\u03c0 off-policy, while maintaining the consistency between the two estimates. At any time, an SDN issues a stochastic estimation Q-\u03b8v of Q-\u03c0 and a deterministic estimation Vendv of V-\u03c0, such an estimation VendV-\u03c0 (xt) off-policy while maintaining the consistency between Vendv (xt) + A\u03b8v (xt) + A\u03b8v (xt) (pp-p-p p p p-p p p p-p p p p-p p p-p p p-p p-p p-p-p p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p"}, {"heading": "5.2 TRUST REGION UPDATING", "text": "In order to adopt the trustee region update scheme (Section 3.3) in the area of continuous control, one only needs to change the distribution f and the gradient g for continuous action spaces. For distribution f, we choose Gaussian distributions with fixed diagonal covariance and medium covariance (x). To derive g for continuous action spaces, we consider the ACER policy gradient for the stochastic duel network, but with respect to \u03c6: gacert = Ext [Eat [Eat] t (xt) log f (at) (at)) (Qopc (xt) (xt, at) \u2212 V\u03b8v (xt)] + E a (a) \u2212 c \u00b2 t (a) + (Q \u00b2 t (xt, a) \u2212 v (xt) log f (xt)."}, {"heading": "6 RESULTS ON MUJOCO", "text": "In fact, most of us are able to adhere to the rules that they have imposed on themselves that they have taken care of. \"We must adhere to the rules,\" he says, \"but we must adhere to the rules.\" (\"We must adhere to the rules.\") \"We must adhere to the rules.\" (\"We must adhere to the rules.\") \"We must adhere to the rules.\" (\"We must adhere to the rules.\") \"(\" We must adhere to the rules. \")\" (\"We must adhere to the rules.\") \"(\" We must adhere to the rules. \"(\" We must adhere to the rules. \")\" (\"We must adhere to the rules.\") \"(\" We must adhere to the rules. \")\" (\"We must adhere to the rules.\" (\"We must.\") \"(\" We must. \"(\" We must adhere to the rules. \")\" (\"We must.\" (\"We must adhere to the rules.\") \"(\" We must adhere to the rules. \")\" (\"We must adhere to the rules.\" (\"We must.\") \"(\" We must. \"(\" We must adhere to the rules. \")\" (We must adhere to the rules. \")\" (We must. \"(We must adhere to the rules.\") \"(We must adhere to the rules.\" (We must. \")\" (We must adhere to the rules. \"(We must.\") \"(We must.\""}, {"heading": "7 THEORETICAL ANALYSIS", "text": "For this reason, and given the central role that Retrace plays in ACER, it is worth shedding more light on this technique. In this section, we will prove that Retrace can be interpreted as applying the tricks of weight loss and bias correction advanced in this paper. (17) If we apply the trick of weight loss and bias correction to the above equation, we can apply the following equation: Q\u03c0 (xt, at) = Ext + 1at + 1 [rt + \u03b3t + 1Q\u03c0 (xt + 1, at + 1) operator. (17) If we apply the trick of weight reduction and bias correction to the above equation, we get Q\u03c0 (xt, at) = Ext + 1at + 1 [rt + quared]."}, {"heading": "8 CONCLUDING REMARKS", "text": "This approach integrates several recent advances in RL in a principled way. In addition, it integrates three innovations developed in this paper: Scanning Meaning with Bias Correction, stochastic duel networks, and an efficient method for optimizing policies in trust regions. We showed that the method not only complies with the best known methods on Atari, but also outperforms popular techniques in several continuous control problems. The method for optimizing trust regions developed in this paper performs remarkably well in continuous areas, and may prove very useful in other deep learning areas where it is difficult to stabilize the training process."}, {"heading": "ACKNOWLEDGMENTS", "text": "We are very grateful to Marc Bellemare, Jascha Sohl-Dickstein and Se'bastien Racaniere for their proofreading and valuable suggestions."}, {"heading": "A ACER PSEUDO-CODE FOR DISCRETE ACTIONS", "text": "Algorithm 1 ACER for discrete actions (master algorithm) / / Assume global shared parameter vectors. / / Assume ratio of replay. / / Assume ratio of replay. / / Acceleration 2 ACER for discrete actions Reset gradients dhabi (0, \u00b7 \u00b7 \u00b7, n) doCall ACER off-policy, algorithm 2. end foruntil Max iteration or time reached.Algorithm 2 ACER for discrete actions Reset gradients dhabi (0, \u00b7 \u00b7, \u00b7, n} doCall ACER off-policy, algorithm 2. end until forMax iteration or time reached.Algorithm 2 ACER for discrete actions Reset gradiments dhabi (0), \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7, \u00b7 d\u03b8v (0, \u00b7, \u00b7) doCall ACER off parameter."}, {"heading": "C RETRACE AS TRUNCATED IMPORTANCE SAMPLING WITH BIAS CORRECTION", "text": "For the purpose of the evidence, we assume that our surroundings are a Markov decisional space = a Markov decisional space. (b) (X, A, P, r) We restrict X to a finite state space. (b) (X, A, X) (X, A) (X, A) (X, A) (X, A) (RMAX, RMAX)) define a reward function. Finally, A (0, 1) is the discount factor. (b) First, we show that B is a contraction operator. (BQ) (x, a). (c) c c. (c) c. (c) c c c c. c. c. c. c. c. c. c. c. c. c. c. c. c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c"}, {"heading": "E CONTINUOUS CONTROL EXPERIMENTS", "text": "It is not only the way in which people in the USA and in the USA, but also the way and manner in which they live and work in the USA, and the way in which they live and work in the USA, and the way in which they live and work in the USA, and the way in which they live and work in the USA, and the way in which they live and work in the USA, and the way in which they live and work in the USA, and the way in which they live and work in the USA, and how they live and work in the USA, and how they live in the USA, in the USA, in the USA, in the USA and in the USA, in the USA, in the USA and in the USA, in the USA and in the USA, in the USA and in the USA, in the USA and in the USA, in the USA and in the USA, in the USA, in the USA, in the USA, in the USA, in the USA and in the USA, in the USA, in the USA and in the USA, in the USA, in the USA and in the USA, in the USA, in the USA and in the USA, in the USA, in the USA and in the USA, in the USA, in the USA, in the USA and in the USA, in the USA, in the USA and in the USA, in the USA, in the USA, in the USA and in the USA, in the USA, in the USA, in the USA and in the USA, in the USA, in the USA, in the USA and in the USA, in the USA, in the USA and in the USA, in the USA, in the USA, in the USA and in the USA, in the USA, in the USA and in the USA, in the USA and in the USA, in the USA, in the USA and in the USA, in the USA, in the USA and in the USA, in the USA, in the USA, in the USA and in the USA, in the USA, in the USA and in the USA, in the USA, in the USA and in the USA, in the USA and in the USA, in the USA and in the USA, and in the USA, in the USA, in the USA and in the USA, in the USA and in the USA, in the USA and in the USA, and in the USA, in the USA and in the USA"}], "references": [], "referenceMentions": [], "year": 2016, "abstractText": "<lb>This paper presents an actor-critic deep reinforcement learning agent with experience replay<lb>that is stable, sample efficient, and performs remarkably well on challenging environments,<lb>including the discrete 57-game Atari domain and several continuous control problems.<lb>To achieve this, the paper introduces several innovations, including truncated importance<lb>sampling with bias correction, stochastic dueling network architectures, and a new trust<lb>region policy optimization method.", "creator": "LaTeX with hyperref package"}}}