{"id": "1611.01276", "review": {"conference": "NIPS", "VERSION": "v1", "DATE_OF_SUBMISSION": "4-Nov-2016", "title": "A Communication-Efficient Parallel Algorithm for Decision Tree", "abstract": "Decision tree (and its extensions such as Gradient Boosting Decision Trees and Random Forest) is a widely used machine learning algorithm, due to its practical effectiveness and model interpretability. With the emergence of big data, there is an increasing need to parallelize the training process of decision tree. However, most existing attempts along this line suffer from high communication costs. In this paper, we propose a new algorithm, called \\emph{Parallel Voting Decision Tree (PV-Tree)}, to tackle this challenge. After partitioning the training data onto a number of (e.g., $M$) machines, this algorithm performs both local voting and global voting in each iteration. For local voting, the top-$k$ attributes are selected from each machine according to its local data. Then, globally top-$2k$ attributes are determined by a majority voting among these local candidates. Finally, the full-grained histograms of the globally top-$2k$ attributes are collected from local machines in order to identify the best (most informative) attribute and its split point. PV-Tree can achieve a very low communication cost (independent of the total number of attributes) and thus can scale out very well. Furthermore, theoretical analysis shows that this algorithm can learn a near optimal decision tree, since it can find the best attribute with a large probability. Our experiments on real-world datasets show that PV-Tree significantly outperforms the existing parallel decision tree algorithms in the trade-off between accuracy and efficiency.", "histories": [["v1", "Fri, 4 Nov 2016 07:09:03 GMT  (250kb,D)", "http://arxiv.org/abs/1611.01276v1", null]], "reviews": [], "SUBJECTS": "cs.LG", "authors": ["qi meng", "guolin ke", "taifeng wang", "wei chen", "qiwei ye", "zhiming ma", "tie-yan liu"], "accepted": true, "id": "1611.01276"}, "pdf": {"name": "1611.01276.pdf", "metadata": {"source": "CRF", "title": "A Communication-Efficient Parallel Algorithm for Decision Tree", "authors": ["Qi Meng", "Guolin Ke", "Taifeng Wang", "Wei Chen", "Qiwei Ye", "Zhi-Ming Ma", "Tie-Yan Liu"], "emails": ["qimeng13@pku.edu.cn;", "tie-yan.liu}@microsoft.com;", "mazm@amt.ac.cn"], "sections": [{"heading": "1 Introduction", "text": "In fact, most people are able to decide for themselves what they want and what they want."}, {"heading": "2 Decision Tree", "text": "Let us assume the training datasets Dn = {xi, j, yi); i = 1, \u00b7 \u00b7 n, j = 1, \u00b7 # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # #"}, {"heading": "3 PV-Tree", "text": "This year it is more than ever before."}, {"heading": "4 Theoretical Analysis", "text": "In this section, we perform theoretical analyses of proposed PV tree algorithms and each has a fixed data base. Specifically, we prove that PV tree can select the best (most informative) attribute in a high probability, for both types of classification and regression. To better represent the theorem, we first present some notations4 in the classification, which we call the most informative attributes. (Then, we will list most informative attributes as attributes that we identify as the most informative attributes. (1) Then, we denote l (j) = | IG (1) \u2212 IG (2)}} to indicate the distance between the largest and the largest IG. In regression, l (j) (k) is defined in the way except to replace IG with VG.Theorem, we have M local machines, and each has a fixed data base."}, {"heading": "5 Experiments", "text": "In this section we report on the experimental comparisons between PV tree and baseline algorithms. We used two sets of data, one for learning to rank (LTR) and the other for ad-click prediction (CTR) 8 (see Table 1 for details). For LTR, we extracted about 1200 numerical attributes per data sample, and5In fact, the global matching size can be \u03b2k with \u03b2 > 1. Then the sufficient condition for regression holds in the same way, with the IG having the most informative attributes. 6Please note that the use of more machines will reduce the local computing time, so that the optimal value of the machine number may be greater in terms of speed up.7The theorem for regression holds with VG. 8We use private data in LTR experiments and data of KDD Cup 2012 track 2 in CTR experiment.used NDCG [Burges] as the rating scale."}, {"heading": "5.1 Comparison with Other Parallel Decision Trees", "text": "For comparison with the aforementioned hsci-tcnlhSrcnlhSrc\u00fc\u00fc\u00fce hta hics in the eeisrcnlhsrtee\u00fccnlhsrteeee\u00fccnlrcehncni nlrrlrlrrrrceheaeaeaeVnlrrrln nlrrrrrrlrrrrteeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeegn nlrrrf\u00fc-tcehnrrrrrrrrrteeeeeeeeeeeegrrreeeeeeeeeeegn the nrrrreeeeeeeeeeeeeeeeeeeegn nrreeeeeeeeeeeeeeeegn nrrrrreeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeegn nrrrrrrreeeeeeeeeeeeeegn ngn nrrreeeeeeeeegn nreeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeVnlrrrrrrrrreeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeVrreeeeeeeeeeeeeeeeeeeeeeeeee"}, {"heading": "5.2 Tradeoff between Speed-up and Accuracy in PV-Tree", "text": "In the previous section, we showed that the PV tree is more efficient than other algorithms. At this point, we delve deep into the PV tree to see how its key parameters affect the trade-off between efficiency and accuracy. According to Theorem 4.1, the following two parameters are crucial for the PV tree: the number of machines M and the size of the selector."}, {"heading": "5.2.1 On Different Numbers of Machines", "text": "When more machines join the distributed training process, the data throughput increases, but the amortized training data on each machine gets smaller. If the data size on each machine gets too small, according to our theorem, there is no guarantee for the accuracy of the tuning process. Therefore, it is important to adjust the number of machines accordingly. To gain more insight into this, we conducted some additional experiments, the results of which are shown in Figure 2a and 2b. From these figures we can see that in LTR, when the number of machines increases from 2 to 8, the training process is significantly accelerated. However, when the number increases to 16, the convergence speed is even lower than when using 8 machines. Similar results can be observed with CTR. These observations are consistent with our theoretical findings. Please note that PV-Tree is designed for the big data scenario. Only when the total training data is huge (and therefore the distribution of training data on each local machine with the total training set is comparable to the number of machines), we should select a reasonable training time from V."}, {"heading": "5.2.2 On Different Sizes of Voting", "text": "Intuitively, a larger k increases the likelihood of finding the world's best attribute from the local candidates, but it also means higher communication costs. According to our theorem, the choice of k should depend on the size of the local training data. If the size of the local training data is large, the local best attributes will resemble the global best. In this case, it is safe to choose a small value of k. Otherwise, we should choose a relatively larger k. To gain more insight into this, we conducted some experiments, the results of which are shown in Table 4, where M refers to the number of machines. From the table, we have the following observations. Firstly, in both cases, in order to achieve good accuracy, one does not have to choose a large k. If k is \u2264 40, the accuracy is very good. Secondly, we note that for cases where a small number of machines is used, e.g. a larger machine can be set to an even smaller value to guarantee greater accuracy if the training size is k."}, {"heading": "5.3 Comparison with Other Parallel GBDT Algorithms", "text": "While we mainly focus on how to parallelise the decision tree construction process within GBDT in the previous subsections, GBDT could also be paralleled in other ways. For example, in [Yu and Skillicorn (2001); Svore and Burges (2011)], each machine learns its own decision tree separately without communication, and then these decision trees are aggregated using winner-takes-all or output ensembles. Although these works are not the focus of our work, it is still interesting to compare them with them. To this end, we have implemented both the algorithms proposed in [Yu and Skillicorn (2001)] and [Svore and Burges (2011)]. To make it easier, we refer to them as Svore and Yu, respectively. Their performance is shown in Figure 3a and 3b. From the numbers we can see that PV-Tree has parallels with both Svore and Yu: Although these two algorithms converge with similar velocity to Pree-TV."}, {"heading": "6 Conclusions", "text": "In this paper, we have proposed a novel parallel algorithm for the decision tree called the Parallel Voting Decision Tree (PV-Tree), which can achieve high accuracy at very low communication costs. Experiments on both ranking and ad click prediction suggest that PV-Tree has an advantage over a number of baselines algorithms. In terms of future work, we plan to generalize the idea of PV-Tree to parallel other machine learning algorithms. In addition, we will develop an open source PV-Tree algorithm that will benefit more researchers and practitioners."}, {"heading": "7 Appendices", "text": "First, we review the definitions of information gain in the classification and variance gain in the regression.Definition 2.1 [Friedman, Hastie and Tibshirani (2001), Quinlan (1986)] In the classification, the information gain (IG) for the attribute Xj [w1, w2] at node O is considered the entropy reduction of output Y after the attribute Xj at w, i.e. IGj (w; O) = Hj \u2212 (Hlj (w) + Hrj (w)))) = P (w1 \u2264 Xj w2) H (Y | w1 \u2264 Xj w2) \u2212 P (w1 \u2264 Xj \u2264 w2) \u2212 w1 < w) H (Y | w1 \u2264 Y < w) < w; w (w) &ltj &ltw; Y &ltw; Y (w; Y) (w; 2) &ltw < Y (w; Y) &ltw; Y (w) (w)."}, {"heading": "7.1 Theorem 4.1 and its Proof for classification and regression", "text": "Theorem 4.1: In classification, we assume that we have local machines, and each of them has n training data. PV tree on any tree node with local choice size k and global majority choice size 2k will select the most informative attribute with a probability of at least 1 (m = [M / 2 + 1] CmM 1 \u2212 d = k + 1 (j) (j) (n, k) m + 1 (j) (j) (n, k) m + 1 (j) (k, k) M \u2212 m, where (j) (j) (j) (n) (n) (n) (n) (j) (j), (j), (j), (j), (j) and c (j).Proof for classification: We introduce some notations. We use subscripts n to denote the corresponding empirical statistics based on empirical distribution."}, {"heading": "7.2 Theorem 4.2 and its proof", "text": "Theorem 4.2: We designate quantified histogram with b containers of the underlying distribution P as P b, that of the empirical distribution Pn as P bn, the information gain of Xj, calculated after the distribution Pb and P bn as IGbj or IG b n, j and fj (b), IGj \u2212 IGbj |. Then we have for \u2264 minj = 1, \u00b7 \u00b7 \u00b7, d fj (b), with a probability of at least \u03b4j (n, fj (b) \u2212)))))))) | IGbn, j \u2212 IGj | >. Proof: First, we have | IGbn, j \u2212 IGj | = | IGbn, j \u2212 IGbj + IGbj \u2212 IGj | \u2265 | IGbn, j \u2212 IGbj | \u2212 f (b) | |. Second, if n is large enough, we have | f (b) | \u2212 IGbn, j \u2212 IGbj + IGbj \u2212 IGbj | > with probability (n, j \u00b7 b) \u00b7 f (f) (f)."}], "references": [{"title": "Parallel classification for data mining in a shared-memory multiprocessor system", "author": ["R. Agrawal", "C.-T. Ho", "M.J. Zaki"], "venue": "US Patent 6,230,151.", "citeRegEx": "Agrawal et al\\.,? 2001", "shortCiteRegEx": "Agrawal et al\\.", "year": 2001}, {"title": "Confidence sets for split points in decision trees. The Annals of Statistics 35(2):543\u2013574", "author": ["M. Banerjee", "I. W McKeague"], "venue": null, "citeRegEx": "Banerjee and McKeague,? \\Q2007\\E", "shortCiteRegEx": "Banerjee and McKeague", "year": 2007}, {"title": "A streaming parallel decision tree algorithm", "author": ["Y. Ben-Haim", "E. Tom-Tov"], "venue": "The Journal of Machine Learning Research 11:849\u2013872.", "citeRegEx": "Ben.Haim and Tom.Tov,? 2010", "shortCiteRegEx": "Ben.Haim and Tom.Tov", "year": 2010}, {"title": "Classification and regression trees", "author": ["L. Breiman", "J. Friedman", "C.J. Stone", "R.A. Olshen"], "venue": "CRC press.", "citeRegEx": "Breiman et al\\.,? 1984", "shortCiteRegEx": "Breiman et al\\.", "year": 1984}, {"title": "Random forests", "author": ["L. Breiman"], "venue": "Machine learning, volume 45, 5\u201332. Springer.", "citeRegEx": "Breiman,? 2001", "shortCiteRegEx": "Breiman", "year": 2001}, {"title": "From ranknet to lambdarank to lambdamart: An overview", "author": ["C.J. Burges"], "venue": "Learning, volume 11, 23\u2013581.", "citeRegEx": "Burges,? 2010", "shortCiteRegEx": "Burges", "year": 2010}, {"title": "The elements of statistical learning, volume 1", "author": ["J. Friedman", "T. Hastie", "R. Tibshirani"], "venue": "Springer series in statistics Springer, Berlin.", "citeRegEx": "Friedman et al\\.,? 2001", "shortCiteRegEx": "Friedman et al\\.", "year": 2001}, {"title": "Greedy function approximation: a gradient boosting machine", "author": ["J.H. Friedman"], "venue": "Annals of statistics, 1189\u20131232. JSTOR.", "citeRegEx": "Friedman,? 2001", "shortCiteRegEx": "Friedman", "year": 2001}, {"title": "Boat\u2014optimistic decision tree construction", "author": ["J. Gehrke", "V. Ganti", "R. Ramakrishnan", "W.-Y. Loh"], "venue": "ACM SIGMOD Record, volume 28, 169\u2013180. ACM.", "citeRegEx": "Gehrke et al\\.,? 1999", "shortCiteRegEx": "Gehrke et al\\.", "year": 1999}, {"title": "Ensemble of collaborative filtering and feature engineered models for click through rate prediction", "author": ["M. Jahrer", "A. Toscher", "J. Lee", "J. Deng", "H. Zhang", "J. Spoelstra"], "venue": "KDDCup Workshop.", "citeRegEx": "Jahrer et al\\.,? 2012", "shortCiteRegEx": "Jahrer et al\\.", "year": 2012}, {"title": "Communication and memory efficient parallel decision tree construction", "author": ["R. Jin", "G. Agrawal"], "venue": "SDM, 119\u2013129. SIAM.", "citeRegEx": "Jin and Agrawal,? 2003", "shortCiteRegEx": "Jin and Agrawal", "year": 2003}, {"title": "Scalparc: A new scalable and efficient parallel classification algorithm for mining large datasets", "author": ["M.V. Joshi", "G. Karypis", "V. Kumar"], "venue": "Parallel processing symposium, 1998. IPPS/SPDP 1998, 573\u2013579. IEEE.", "citeRegEx": "Joshi et al\\.,? 1998", "shortCiteRegEx": "Joshi et al\\.", "year": 1998}, {"title": "Decision trees on parallel processors", "author": ["R. Kufrin"], "venue": "Machine Intelligence and Pattern Recognition, volume 20, 279\u2013306. Elsevier.", "citeRegEx": "Kufrin,? 1997", "shortCiteRegEx": "Kufrin", "year": 1997}, {"title": "Sliq: A fast scalable classifier for data mining", "author": ["M. Mehta", "R. Agrawal", "J. Rissanen"], "venue": "Advances in Database Technology\u2014EDBT\u201996. Springer. 18\u201332.", "citeRegEx": "Mehta et al\\.,? 1996", "shortCiteRegEx": "Mehta et al\\.", "year": 1996}, {"title": "Planet: massively parallel learning of tree ensembles with mapreduce", "author": ["B. Panda", "J.S. Herbach", "S. Basu", "R.J. Bayardo"], "venue": "Proceedings of the VLDB Endowment, volume 2, 1426\u20131437. VLDB Endowment.", "citeRegEx": "Panda et al\\.,? 2009", "shortCiteRegEx": "Panda et al\\.", "year": 2009}, {"title": "A coarse grained parallel induction heuristic", "author": ["R.A. Pearson"], "venue": "University College, University of New South Wales, Department of Computer Science, Australian Defence Force Academy.", "citeRegEx": "Pearson,? 1993", "shortCiteRegEx": "Pearson", "year": 1993}, {"title": "Induction of decision trees", "author": ["J.R. Quinlan"], "venue": "Machine learning, volume 1, 81\u2013106. Springer.", "citeRegEx": "Quinlan,? 1986", "shortCiteRegEx": "Quinlan", "year": 1986}, {"title": "Clouds: A decision tree classifier for large datasets", "author": ["S. Ranka", "V. Singh"], "venue": "Knowledge discovery and data mining, 2\u20138.", "citeRegEx": "Ranka and Singh,? 1998", "shortCiteRegEx": "Ranka and Singh", "year": 1998}, {"title": "A survey of decision tree classifier methodology", "author": ["S.R. Safavian", "D. Landgrebe"], "venue": "IEEE transactions on systems, man, and cybernetics 21(3):660\u2013674.", "citeRegEx": "Safavian and Landgrebe,? 1991", "shortCiteRegEx": "Safavian and Landgrebe", "year": 1991}, {"title": "Sprint: A scalable parallel classi er for data mining", "author": ["J. Shafer", "R. Agrawal", "M. Mehta"], "venue": "Proc. 1996 Int. Conf. Very Large Data Bases, 544\u2013555. Citeseer.", "citeRegEx": "Shafer et al\\.,? 1996", "shortCiteRegEx": "Shafer et al\\.", "year": 1996}, {"title": "Large-scale learning to rank using boosted decision trees", "author": ["K.M. Svore", "C. Burges"], "venue": "Scaling Up Machine Learning: Parallel and Distributed Approaches 2.", "citeRegEx": "Svore and Burges,? 2011", "shortCiteRegEx": "Svore and Burges", "year": 2011}, {"title": "Parallel boosted regression trees for web search ranking", "author": ["S. Tyree", "K.Q. Weinberger", "K. Agrawal", "J. Paykin"], "venue": "Proceedings of the 20th international conference on World wide web, 387\u2013396. ACM.", "citeRegEx": "Tyree et al\\.,? 2011", "shortCiteRegEx": "Tyree et al\\.", "year": 2011}, {"title": "Parallelizing boosting and bagging", "author": ["C. Yu", "D. Skillicorn"], "venue": "Queen\u2019s University, Kingston, Canada, Tech. Rep.", "citeRegEx": "Yu and Skillicorn,? 2001", "shortCiteRegEx": "Yu and Skillicorn", "year": 2001}, {"title": "Ensemble methods: foundations and algorithms", "author": ["Zhou", "Z.-H."], "venue": "CRC Press.", "citeRegEx": "Zhou and Z..H.,? 2012", "shortCiteRegEx": "Zhou and Z..H.", "year": 2012}, {"title": "In classification, the information gain (IG) for attribute Xj \u2208 [w1, w2] at node O, is defined as the entropy reduction of the output Y after splitting node O by attribute", "author": ["Friedman", "Hastie", "Tibshirani"], "venue": null, "citeRegEx": "Friedman et al\\.,? \\Q1986\\E", "shortCiteRegEx": "Friedman et al\\.", "year": 1986}], "referenceMentions": [{"referenceID": 10, "context": "Decision tree [Quinlan (1986)] is a widely used machine learning algorithm, since it is practically effective and the rules it learns are simple and interpretable.", "startOffset": 15, "endOffset": 30}, {"referenceID": 4, "context": "Based on decision tree, people have developed other algorithms such as Random Forest (RF) [Breiman (2001)] and Gradient Boosting Decision Trees (GBDT) [Friedman (2001)], which have demonstrated very promising performances in various learning tasks [Burges (2010)].", "startOffset": 91, "endOffset": 106}, {"referenceID": 4, "context": "Based on decision tree, people have developed other algorithms such as Random Forest (RF) [Breiman (2001)] and Gradient Boosting Decision Trees (GBDT) [Friedman (2001)], which have demonstrated very promising performances in various learning tasks [Burges (2010)].", "startOffset": 91, "endOffset": 168}, {"referenceID": 4, "context": "Based on decision tree, people have developed other algorithms such as Random Forest (RF) [Breiman (2001)] and Gradient Boosting Decision Trees (GBDT) [Friedman (2001)], which have demonstrated very promising performances in various learning tasks [Burges (2010)].", "startOffset": 91, "endOffset": 263}, {"referenceID": 4, "context": "Based on decision tree, people have developed other algorithms such as Random Forest (RF) [Breiman (2001)] and Gradient Boosting Decision Trees (GBDT) [Friedman (2001)], which have demonstrated very promising performances in various learning tasks [Burges (2010)]. In recent years, with the emergence of very big training data (which cannot be held in one single machine), there has been an increasing need of parallelizing the training process of decision tree. To this end, there have been two major categories of attempts: 2. \u2217Denotes equal contribution. This work was done when the first author was visiting Microsoft Research Asia. There is another category of works that parallelize the tasks of sub-tree training once a node is split [Pearson (1993)], which require the training data to be moved from machine to machine for many times and are thus inefficient.", "startOffset": 91, "endOffset": 757}, {"referenceID": 4, "context": "Based on decision tree, people have developed other algorithms such as Random Forest (RF) [Breiman (2001)] and Gradient Boosting Decision Trees (GBDT) [Friedman (2001)], which have demonstrated very promising performances in various learning tasks [Burges (2010)]. In recent years, with the emergence of very big training data (which cannot be held in one single machine), there has been an increasing need of parallelizing the training process of decision tree. To this end, there have been two major categories of attempts: 2. \u2217Denotes equal contribution. This work was done when the first author was visiting Microsoft Research Asia. There is another category of works that parallelize the tasks of sub-tree training once a node is split [Pearson (1993)], which require the training data to be moved from machine to machine for many times and are thus inefficient. Moreover, there are also some other works accelerating decision tree construction by using presorting [Mehta, Agrawal, and Rissanen (1996), Shafer, Agrawal, and Mehta (1996) Joshi, Karypis, and Kumar (1998)] and binning [Ranka and Singh (1998), Gehrke et al.", "startOffset": 91, "endOffset": 1007}, {"referenceID": 4, "context": "Based on decision tree, people have developed other algorithms such as Random Forest (RF) [Breiman (2001)] and Gradient Boosting Decision Trees (GBDT) [Friedman (2001)], which have demonstrated very promising performances in various learning tasks [Burges (2010)]. In recent years, with the emergence of very big training data (which cannot be held in one single machine), there has been an increasing need of parallelizing the training process of decision tree. To this end, there have been two major categories of attempts: 2. \u2217Denotes equal contribution. This work was done when the first author was visiting Microsoft Research Asia. There is another category of works that parallelize the tasks of sub-tree training once a node is split [Pearson (1993)], which require the training data to be moved from machine to machine for many times and are thus inefficient. Moreover, there are also some other works accelerating decision tree construction by using presorting [Mehta, Agrawal, and Rissanen (1996), Shafer, Agrawal, and Mehta (1996) Joshi, Karypis, and Kumar (1998)] and binning [Ranka and Singh (1998), Gehrke et al.", "startOffset": 91, "endOffset": 1042}, {"referenceID": 4, "context": "Based on decision tree, people have developed other algorithms such as Random Forest (RF) [Breiman (2001)] and Gradient Boosting Decision Trees (GBDT) [Friedman (2001)], which have demonstrated very promising performances in various learning tasks [Burges (2010)]. In recent years, with the emergence of very big training data (which cannot be held in one single machine), there has been an increasing need of parallelizing the training process of decision tree. To this end, there have been two major categories of attempts: 2. \u2217Denotes equal contribution. This work was done when the first author was visiting Microsoft Research Asia. There is another category of works that parallelize the tasks of sub-tree training once a node is split [Pearson (1993)], which require the training data to be moved from machine to machine for many times and are thus inefficient. Moreover, there are also some other works accelerating decision tree construction by using presorting [Mehta, Agrawal, and Rissanen (1996), Shafer, Agrawal, and Mehta (1996) Joshi, Karypis, and Kumar (1998)] and binning [Ranka and Singh (1998), Gehrke et al.", "startOffset": 91, "endOffset": 1075}, {"referenceID": 4, "context": "Based on decision tree, people have developed other algorithms such as Random Forest (RF) [Breiman (2001)] and Gradient Boosting Decision Trees (GBDT) [Friedman (2001)], which have demonstrated very promising performances in various learning tasks [Burges (2010)]. In recent years, with the emergence of very big training data (which cannot be held in one single machine), there has been an increasing need of parallelizing the training process of decision tree. To this end, there have been two major categories of attempts: 2. \u2217Denotes equal contribution. This work was done when the first author was visiting Microsoft Research Asia. There is another category of works that parallelize the tasks of sub-tree training once a node is split [Pearson (1993)], which require the training data to be moved from machine to machine for many times and are thus inefficient. Moreover, there are also some other works accelerating decision tree construction by using presorting [Mehta, Agrawal, and Rissanen (1996), Shafer, Agrawal, and Mehta (1996) Joshi, Karypis, and Kumar (1998)] and binning [Ranka and Singh (1998), Gehrke et al.", "startOffset": 91, "endOffset": 1112}, {"referenceID": 4, "context": "Based on decision tree, people have developed other algorithms such as Random Forest (RF) [Breiman (2001)] and Gradient Boosting Decision Trees (GBDT) [Friedman (2001)], which have demonstrated very promising performances in various learning tasks [Burges (2010)]. In recent years, with the emergence of very big training data (which cannot be held in one single machine), there has been an increasing need of parallelizing the training process of decision tree. To this end, there have been two major categories of attempts: 2. \u2217Denotes equal contribution. This work was done when the first author was visiting Microsoft Research Asia. There is another category of works that parallelize the tasks of sub-tree training once a node is split [Pearson (1993)], which require the training data to be moved from machine to machine for many times and are thus inefficient. Moreover, there are also some other works accelerating decision tree construction by using presorting [Mehta, Agrawal, and Rissanen (1996), Shafer, Agrawal, and Mehta (1996) Joshi, Karypis, and Kumar (1998)] and binning [Ranka and Singh (1998), Gehrke et al. (1999), Jin and Agrawal (2003)], or employing a", "startOffset": 91, "endOffset": 1134}, {"referenceID": 4, "context": "Based on decision tree, people have developed other algorithms such as Random Forest (RF) [Breiman (2001)] and Gradient Boosting Decision Trees (GBDT) [Friedman (2001)], which have demonstrated very promising performances in various learning tasks [Burges (2010)]. In recent years, with the emergence of very big training data (which cannot be held in one single machine), there has been an increasing need of parallelizing the training process of decision tree. To this end, there have been two major categories of attempts: 2. \u2217Denotes equal contribution. This work was done when the first author was visiting Microsoft Research Asia. There is another category of works that parallelize the tasks of sub-tree training once a node is split [Pearson (1993)], which require the training data to be moved from machine to machine for many times and are thus inefficient. Moreover, there are also some other works accelerating decision tree construction by using presorting [Mehta, Agrawal, and Rissanen (1996), Shafer, Agrawal, and Mehta (1996) Joshi, Karypis, and Kumar (1998)] and binning [Ranka and Singh (1998), Gehrke et al. (1999), Jin and Agrawal (2003)], or employing a", "startOffset": 91, "endOffset": 1158}, {"referenceID": 4, "context": "Attribute-parallel: Training data are vertically partitioned according to the attributes and allocated to different machines, and then in each iteration, the machines work on non-overlapping sets of attributes in parallel in order to find the best attribute and its split point (suppose this best attribute locates at the i-th machine) [Shafer, Agrawal, and Mehta (1996), Joshi, Karypis, and Kumar (1998), Svore and Burges (2011)].", "startOffset": 416, "endOffset": 430}, {"referenceID": 4, "context": "Attribute-parallel: Training data are vertically partitioned according to the attributes and allocated to different machines, and then in each iteration, the machines work on non-overlapping sets of attributes in parallel in order to find the best attribute and its split point (suppose this best attribute locates at the i-th machine) [Shafer, Agrawal, and Mehta (1996), Joshi, Karypis, and Kumar (1998), Svore and Burges (2011)]. This process is communicationally very efficient. However, after that, the re-partition of the data on other machines than the i-th machine will induce very high communication costs (proportional to the number of data samples). This is because those machines have no information about the best attribute at all, and in order to fulfill the re-partitioning, they must retrieve the partition information of every data sample from the i-th machine. Furthermore, as each worker still has full sample set, the partition process is not parallelized, which slows down the algorithm. Data-parallel: Training data are horizontally partitioned according to the samples and allocated to different machines. Then the machines communicate with each other the local histograms of all attributes (according to their own data samples) in order to obtain the global attribute distributions and identify the best attribute and split point [Kufrin (1997), Panda et al.", "startOffset": 416, "endOffset": 1368}, {"referenceID": 4, "context": "Attribute-parallel: Training data are vertically partitioned according to the attributes and allocated to different machines, and then in each iteration, the machines work on non-overlapping sets of attributes in parallel in order to find the best attribute and its split point (suppose this best attribute locates at the i-th machine) [Shafer, Agrawal, and Mehta (1996), Joshi, Karypis, and Kumar (1998), Svore and Burges (2011)]. This process is communicationally very efficient. However, after that, the re-partition of the data on other machines than the i-th machine will induce very high communication costs (proportional to the number of data samples). This is because those machines have no information about the best attribute at all, and in order to fulfill the re-partitioning, they must retrieve the partition information of every data sample from the i-th machine. Furthermore, as each worker still has full sample set, the partition process is not parallelized, which slows down the algorithm. Data-parallel: Training data are horizontally partitioned according to the samples and allocated to different machines. Then the machines communicate with each other the local histograms of all attributes (according to their own data samples) in order to obtain the global attribute distributions and identify the best attribute and split point [Kufrin (1997), Panda et al. (2009)].", "startOffset": 416, "endOffset": 1389}, {"referenceID": 2, "context": "To reduce the cost, in [Ben-Haim and Tom-Tov (2010), Tyree et al.", "startOffset": 24, "endOffset": 52}, {"referenceID": 2, "context": "To reduce the cost, in [Ben-Haim and Tom-Tov (2010), Tyree et al. (2011), Jin and Agrawal (2003)], it was proposed to exchange quantized histograms between machines when estimating the global attribute distributions.", "startOffset": 24, "endOffset": 73}, {"referenceID": 2, "context": "To reduce the cost, in [Ben-Haim and Tom-Tov (2010), Tyree et al. (2011), Jin and Agrawal (2003)], it was proposed to exchange quantized histograms between machines when estimating the global attribute distributions.", "startOffset": 24, "endOffset": 97}, {"referenceID": 12, "context": "The goal is to learn a regression or shared-memory-processors approach [Kufrin (1997) Agrawal, Ho, and Zaki (2001)].", "startOffset": 72, "endOffset": 86}, {"referenceID": 12, "context": "The goal is to learn a regression or shared-memory-processors approach [Kufrin (1997) Agrawal, Ho, and Zaki (2001)].", "startOffset": 72, "endOffset": 115}, {"referenceID": 14, "context": "Decision tree [Quinlan (1986); Safavian and Landgrebe (1991)] is a widely used model for both regression [Breiman et al.", "startOffset": 15, "endOffset": 30}, {"referenceID": 14, "context": "Decision tree [Quinlan (1986); Safavian and Landgrebe (1991)] is a widely used model for both regression [Breiman et al.", "startOffset": 15, "endOffset": 61}, {"referenceID": 3, "context": "Decision tree [Quinlan (1986); Safavian and Landgrebe (1991)] is a widely used model for both regression [Breiman et al. (1984)] and classification [Safavian and Landgrebe (1991)].", "startOffset": 106, "endOffset": 128}, {"referenceID": 3, "context": "Decision tree [Quinlan (1986); Safavian and Landgrebe (1991)] is a widely used model for both regression [Breiman et al. (1984)] and classification [Safavian and Landgrebe (1991)].", "startOffset": 106, "endOffset": 179}, {"referenceID": 7, "context": "1 [Friedman, Hastie, and Tibshirani (2001),Quinlan (1986)] In classification, the information gain (IG) for attribute Xj \u2208 [w1, w2] at node O, is defined as the entropy reduction of the output Y after splitting node O by attribute Xj at w, i.", "startOffset": 3, "endOffset": 43}, {"referenceID": 7, "context": "1 [Friedman, Hastie, and Tibshirani (2001),Quinlan (1986)] In classification, the information gain (IG) for attribute Xj \u2208 [w1, w2] at node O, is defined as the entropy reduction of the output Y after splitting node O by attribute Xj at w, i.", "startOffset": 3, "endOffset": 58}, {"referenceID": 2, "context": "PV-Tree is a data-parallel algorithm, which also partitions the training data onto M machines just like in [Ben-Haim and Tom-Tov (2010),Tyree et al.", "startOffset": 108, "endOffset": 136}, {"referenceID": 2, "context": "PV-Tree is a data-parallel algorithm, which also partitions the training data onto M machines just like in [Ben-Haim and Tom-Tov (2010),Tyree et al. (2011)].", "startOffset": 108, "endOffset": 156}, {"referenceID": 2, "context": "PV-Tree is a data-parallel algorithm, which also partitions the training data onto M machines just like in [Ben-Haim and Tom-Tov (2010),Tyree et al. (2011)]. However, its design principal is very different. In [Ben-Haim and Tom-Tov (2010),Tyree et al.", "startOffset": 108, "endOffset": 239}, {"referenceID": 2, "context": "PV-Tree is a data-parallel algorithm, which also partitions the training data onto M machines just like in [Ben-Haim and Tom-Tov (2010),Tyree et al. (2011)]. However, its design principal is very different. In [Ben-Haim and Tom-Tov (2010),Tyree et al. (2011)], one does not trust the local information about the attributes in each machine, and decides the best attribute and split point only based on the aggregated global histograms of the attributes.", "startOffset": 108, "endOffset": 259}, {"referenceID": 5, "context": "used NDCG [Burges (2010)] as the evaluation measure.", "startOffset": 11, "endOffset": 25}, {"referenceID": 5, "context": "used NDCG [Burges (2010)] as the evaluation measure. For CTR, we extracted about 800 numerical attributes [Jahrer et al. (2012)], and used AUC as the evaluation measure.", "startOffset": 11, "endOffset": 128}, {"referenceID": 2, "context": "In addition, we implemented a data-parallel algorithm according to [Ben-Haim and Tom-Tov (2010); Tyree et al.", "startOffset": 68, "endOffset": 96}, {"referenceID": 2, "context": "In addition, we implemented a data-parallel algorithm according to [Ben-Haim and Tom-Tov (2010); Tyree et al. (2011)], which can communicate both full-grained histograms and quantized histograms.", "startOffset": 68, "endOffset": 117}, {"referenceID": 20, "context": "For example, in [Yu and Skillicorn (2001); Svore and Burges (2011)], each machine learns its own decision tree separately without communication.", "startOffset": 17, "endOffset": 42}, {"referenceID": 5, "context": "For example, in [Yu and Skillicorn (2001); Svore and Burges (2011)], each machine learns its own decision tree separately without communication.", "startOffset": 53, "endOffset": 67}, {"referenceID": 5, "context": "For example, in [Yu and Skillicorn (2001); Svore and Burges (2011)], each machine learns its own decision tree separately without communication. After that, these decision trees are aggregated by means of winner-takes-all or output ensemble. Although these works are not the focus of our paper, it is still interesting to compare with them. For this purpose, we implemented both the algorithms proposed in [Yu and Skillicorn (2001)] and [Svore and Burges (2011)].", "startOffset": 53, "endOffset": 432}, {"referenceID": 5, "context": "For example, in [Yu and Skillicorn (2001); Svore and Burges (2011)], each machine learns its own decision tree separately without communication. After that, these decision trees are aggregated by means of winner-takes-all or output ensemble. Although these works are not the focus of our paper, it is still interesting to compare with them. For this purpose, we implemented both the algorithms proposed in [Yu and Skillicorn (2001)] and [Svore and Burges (2011)].", "startOffset": 53, "endOffset": 462}], "year": 2016, "abstractText": "Decision tree (and its extensions such as Gradient Boosting Decision Trees and Random Forest) is a widely used machine learning algorithm, due to its practical effectiveness and model interpretability. With the emergence of big data, there is an increasing need to parallelize the training process of decision tree. However, most existing attempts along this line suffer from high communication costs. In this paper, we propose a new algorithm, called Parallel Voting Decision Tree (PV-Tree), to tackle this challenge. After partitioning the training data onto a number of (e.g., M ) machines, this algorithm performs both local voting and global voting in each iteration. For local voting, the top-k attributes are selected from each machine according to its local data. Then, globally top-2k attributes are determined by a majority voting among these local candidates. Finally, the full-grained histograms of the globally top-2k attributes are collected from local machines in order to identify the best (most informative) attribute and its split point. PV-Tree can achieve a very low communication cost (independent of the total number of attributes) and thus can scale out very well. Furthermore, theoretical analysis shows that this algorithm can learn a near optimal decision tree, since it can find the best attribute with a large probability. Our experiments on real-world datasets show that PV-Tree significantly outperforms the existing parallel decision tree algorithms in the trade-off between accuracy and efficiency.", "creator": "LaTeX with hyperref package"}}}