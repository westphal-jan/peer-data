{"id": "1611.01368", "review": {"conference": "acl", "VERSION": "v1", "DATE_OF_SUBMISSION": "4-Nov-2016", "title": "Assessing the Ability of LSTMs to Learn Syntax-Sensitive Dependencies", "abstract": "The success of long short-term memory (LSTM) neural networks in language processing is typically attributed to their ability to capture long-distance statistical regularities. Linguistic regularities are often sensitive to syntactic structure; can such dependencies be captured by LSTMs, which do not have explicit structural representations? We begin addressing this question using number agreement in English subject-verb dependencies. We probe the architecture's grammatical competence both using training objectives with an explicit grammatical target (number prediction, grammaticality judgments) and using language models. In the strongly supervised settings, the LSTM achieved very high overall accuracy (less than 1% errors), but errors increased when sequential and structural information conflicted. The frequency of such errors rose sharply in the language-modeling setting. We conclude that LSTMs can capture a non-trivial amount of grammatical structure given targeted supervision, but stronger architectures may be required to further reduce errors; furthermore, the language modeling signal is insufficient for capturing syntax-sensitive dependencies, and should be supplemented with more direct supervision if such dependencies need to be captured.", "histories": [["v1", "Fri, 4 Nov 2016 13:36:32 GMT  (777kb,D)", "http://arxiv.org/abs/1611.01368v1", "15 pages; to appear in Transactions of the Association for Computational Linguistics"]], "COMMENTS": "15 pages; to appear in Transactions of the Association for Computational Linguistics", "reviews": [], "SUBJECTS": "cs.CL", "authors": ["tal linzen", "emmanuel dupoux", "yoav goldberg"], "accepted": true, "id": "1611.01368"}, "pdf": {"name": "1611.01368.pdf", "metadata": {"source": "CRF", "title": "Assessing the Ability of LSTMs to Learn Syntax-Sensitive Dependencies", "authors": ["Tal Linzen", "Emmanuel Dupoux"], "emails": ["emmanuel.dupoux}@ens.fr", "yoav.goldberg@gmail.com"], "sections": [{"heading": null, "text": "The success of long-term short-term memory (LSTM) neural networks in language processing is typically attributed to their ability to detect statistical regularities over long distances. Linguistic regularities are often sensitive to syntactic structures; can such dependencies on LSTMs that do not have explicit structural representations be detected? We begin to answer this question using numerical agreements in English subject-verb dependencies. We investigate the grammatical competence of the architecture using both training objectives with an explicit grammatical target (number prediction, grammar assessments) and language models. In tightly controlled environments, LSTM achieved a very high overall accuracy (less than 1% error), but the errors increased when sequential and structural information clashed, and the frequency of such errors increased sharply in language modeling settings. We conclude that LSTM's failure to comprehend a larger set of architectural structures may not be necessary, but more severe."}, {"heading": "1 Introduction", "text": "In fact, most of them are able to survive on their own."}, {"heading": "2 Background: Subject-Verb Agreement as Evidence for Syntactic Structure", "text": "This year it has come to the point that it will only be a matter of time before it will happen, until it does."}, {"heading": "3 The Number Prediction Task", "text": "To what extent can a sequence model learn to be sensitive to the hierarchical structure of natural language? To study this question, we propose the number prediction task. In this task, the model provides the sentence up to, but without a present verb, e.g.: (8) The keys to cabinetIt must then guess the number of the following verb (a binary choice, either PLURAL or SINGULAR). We examine variations on this task in Section 5. To perform this task well, the model must encrypt the concepts of syntactic number and syntactic subjectivity: it must be learned that some words are singular and others plural, and be able to identify the correct subject. As we are illus-trated in Section 2, we must correctly identify the subject that corresponds to a particular verb."}, {"heading": "4 Number Prediction Results", "text": "This year, it has come to the point that it will only be once before there is such a process, in which there is such a process."}, {"heading": "5 Alternative Training Objectives", "text": "This year is the highest in the history of the country."}, {"heading": "6 Additional Experiments", "text": "We repeated the number prediction experiment with a simple recursive network (SRN) (Elman, 1990), with the same number of hidden units. SRN's performance was worse than that of LSTM, but the average performance of a given13 A technical exception was that we did not replace the low frequency words with their part of the speech, because the Google LM is a large vocabulary language model and does not have parts of the language as part of its vocabulary. The number of match persons does not indicate a qualitative difference between cell types: the SRN makes about twice as many errors as the LSTM overall (Figure 4d). Training only on difficult dependencies: only a small percentage of the dependencies in the corpus had match persons (Figure 2e). Would the network be better if the dependencies were repeated with this training?"}, {"heading": "7 Error Analysis", "text": "Most of the nouns in English are singular: in our corpus, the fraction of singular subjects is 68%. Match attraction errors in humans are much more common when the attractor is plural than when it is singular (Bock and Miller, 1991). The difference is statistically significant, but the asymmetry is much less pronounced than in humans. Interestingly, the SRN version of the model shows a large asymmetry, especially as the number of attractors increases as singular attractors. The difference is statistically significant, but the asymmetry is much less pronounced than in humans."}, {"heading": "8 Related Work", "text": "The majority of NLP work on neural networks evaluates them based on their performance in a task such as language modeling or machine translation (Sundermeyer et al., 2012; Bahdanau et al., 2015). These assessment approaches, on average, cover many different syntactical constructions, making it difficult to isolate the syntactic capabilities of the network.Other studies have tested the ability of RNNs to learn simple, recurring networks related to that language. (Gers et al., 1999; Rodriguez et al., 2001) These results were recently replicated and expanded by Joulin and Mikolov."}, {"heading": "9 Discussion and Future Work", "text": "Neural network architectures are typically evaluated based on random samples of naturally occurring sentences, e.g. by using perplexity about pre-recorded data in speech modeling. Since the majority of sentences in natural language are grammatically simple, models can achieve high overall accuracy by using erroneous heuristics that fail in harder cases, making it difficult to distinguish simple but robust sequence models from more expressive architectures (Socher, 2014; Grefenstette et al., 2015; Joulin et al., 2015). Our work suggests an alternative strategy - the evaluation of naturally occurring sentences that are sampled based on their grammatical complexity - that can provide more differentiated tests of language models (Rimell et al., 2009; Bender et al., 2011). This approach can be extended to the educational level: neural networks can be encouraged to develop more complex generalizations by exaggerating generalizations."}, {"heading": "10 Conclusion", "text": "LSTMs are sequence models; they do not have built-in hierarchical representations. We have studied how well they can learn subject-verb match, a phenomenon that depends crucially on the hierarchical syntactic structure. In explicit monitoring, LSTMs were able to perform the task of verb number match in most cases, although their error rate increased for particularly difficult sentences. We conclude that LSTMs can learn to approximate structure-sensitive dependencies relatively well under explicit supervision, but may be necessary to have more meaningful architectures to eliminate errors overall. Finally, our results provide evidence that the language modeling target alone is not sufficient to learn structure-sensitive dependencies, and suggest that a common training goal can be used to complement language models in tasks for which syntax-sensitive dependencies are important."}, {"heading": "Acknowledgments", "text": "We thank Marco Baroni, Grzegorz Chrupa\u0142a, Alexander Clark, Sol Lago, Paul Smolensky, Benjamin Spector and Roberto Zamparelli for their comments and discussions, which were supported by the European Research Council (ERC-2011-AdG 295810 BOOTPHON), the Agence Nationale pour la Recherche (ANR-10-IDEX-0001-02 PSL and ANR-10-LABX-0087 IEC) and the Israeli Science Foundation (grant number 1555 / 15)."}], "references": [{"title": "The emergence of grammaticality in connectionist networks", "author": ["Joseph Allen", "Mark S. Seidenberg."], "venue": "Brian MacWhinney, editor, Emergentist approaches to language: Proceedings of the 28th Carnegie symposium on cognition, pages 115\u2013151. Mahwah, NJ:", "citeRegEx": "Allen and Seidenberg.,? 1999", "shortCiteRegEx": "Allen and Seidenberg.", "year": 1999}, {"title": "Neural machine translation by jointly learning to align and translate", "author": ["Dzmitry Bahdanau", "Kyunghyun Cho", "Yoshua Bengio."], "venue": "International Conference for Learning Representations.", "citeRegEx": "Bahdanau et al\\.,? 2015", "shortCiteRegEx": "Bahdanau et al\\.", "year": 2015}, {"title": "Parser evaluation over local and non-local deep dependencies in a large corpus", "author": ["Emily M. Bender", "Dan Flickinger", "Stephan Oepen", "Yi Zhang."], "venue": "Proceedings of EMNLP, pages 397\u2013408.", "citeRegEx": "Bender et al\\.,? 2011", "shortCiteRegEx": "Bender et al\\.", "year": 2011}, {"title": "Broken agreement", "author": ["Kathryn Bock", "Carol A. Miller."], "venue": "Cognitive Psychology, 23(1):45\u201393.", "citeRegEx": "Bock and Miller.,? 1991", "shortCiteRegEx": "Bock and Miller.", "year": 1991}, {"title": "The \u201cno negative evidence\u201d problem: How do children avoid constructing an overly general grammar? In John A", "author": ["Melissa Bowerman."], "venue": "Hawkins, editor, Explaining language universals, pages 73\u2013101. Oxford: Basil Blackwell.", "citeRegEx": "Bowerman.,? 1988", "shortCiteRegEx": "Bowerman.", "year": 1988}, {"title": "Tree-structured composition in neural networks without tree-structured architectures", "author": ["Samuel R. Bowman", "Christopher D. Manning", "Christopher Potts."], "venue": "Proceedings of the NIPS Workshop on Cognitive Computation: Integrating Neural and Symbolic", "citeRegEx": "Bowman et al\\.,? 2015", "shortCiteRegEx": "Bowman et al\\.", "year": 2015}, {"title": "On the implicit acquisition of a context-free grammar by a simple recurrent neural network", "author": ["Bo Cartling."], "venue": "Neurocomputing, 71(7):1527\u20131537.", "citeRegEx": "Cartling.,? 2008", "shortCiteRegEx": "Cartling.", "year": 2008}, {"title": "Multitask learning", "author": ["Rich Caruana."], "venue": "Sebastian Thrun and Lorien Pratt, editors, Learning to learn, pages 95\u2013133. Boston: Kluwer.", "citeRegEx": "Caruana.,? 1998", "shortCiteRegEx": "Caruana.", "year": 1998}, {"title": "One billion word benchmark for measuring progress in statistical language modeling", "author": ["Ciprian Chelba", "Tomas Mikolov", "Mike Schuster", "Qi Ge", "Thorsten Brants", "Phillipp Koehn", "Tony Robinson."], "venue": "arXiv preprint arXiv:1312.3005.", "citeRegEx": "Chelba et al\\.,? 2013", "shortCiteRegEx": "Chelba et al\\.", "year": 2013}, {"title": "Learning phrase representations using RNN encoder\u2013decoder for statistical machine translation", "author": ["Kyunghyun Cho", "Bart van Merrienboer", "Caglar Gulcehre", "Dzmitry Bahdanau", "Fethi Bougares", "Holger Schwenk", "Yoshua Bengio."], "venue": "Proceedings of EMNLP, pages", "citeRegEx": "Cho et al\\.,? 2014", "shortCiteRegEx": "Cho et al\\.", "year": 2014}, {"title": "Aspects of the Theory of Syntax", "author": ["Noam Chomsky."], "venue": "Cambridge, MA: MIT press.", "citeRegEx": "Chomsky.,? 1965", "shortCiteRegEx": "Chomsky.", "year": 1965}, {"title": "Statistical representation of grammaticality judgements: The limits of n-gram models", "author": ["Alexander Clark", "Gianluca Giorgolo", "Shalom Lappin."], "venue": "Proceedings of the Fourth Annual Workshop on Cognitive Modeling and Computational Linguistics (CMCL), pages", "citeRegEx": "Clark et al\\.,? 2013", "shortCiteRegEx": "Clark et al\\.", "year": 2013}, {"title": "Recurrent neural network grammars", "author": ["Chris Dyer", "Adhiguna Kuncoro", "Miguel Ballesteros", "A. Noah Smith."], "venue": "Proceedings of NAACL/HLT, pages 199\u2013209.", "citeRegEx": "Dyer et al\\.,? 2016", "shortCiteRegEx": "Dyer et al\\.", "year": 2016}, {"title": "Making syntax of sense: Number agreement in sentence production", "author": ["Kathleen M. Eberhard", "J. Cooper Cutting", "Kathryn Bock."], "venue": "Psychological Review, 112(3):531\u2013559.", "citeRegEx": "Eberhard et al\\.,? 2005", "shortCiteRegEx": "Eberhard et al\\.", "year": 2005}, {"title": "Finding structure in time", "author": ["Jeffrey L. Elman."], "venue": "Cognitive Science, 14(2):179\u2013211.", "citeRegEx": "Elman.,? 1990", "shortCiteRegEx": "Elman.", "year": 1990}, {"title": "Distributed representations, simple recurrent networks, and grammatical structure", "author": ["Jeffrey L. Elman."], "venue": "Machine Learning, 7(2-3):195\u2013225.", "citeRegEx": "Elman.,? 1991", "shortCiteRegEx": "Elman.", "year": 1991}, {"title": "Learning and development in neural networks: The importance of starting small", "author": ["Jeffrey L. Elman."], "venue": "Cognition, 48(1):71\u201399.", "citeRegEx": "Elman.,? 1993", "shortCiteRegEx": "Elman.", "year": 1993}, {"title": "Structures, not strings: Linguistics as part of the cognitive sciences", "author": ["Martin B.H. Everaert", "Marinus A.C. Huybregts", "Noam Chomsky", "Robert C. Berwick", "Johan J. Bolhuis."], "venue": "Trends in Cognitive Sciences, 19(12):729\u2013743.", "citeRegEx": "Everaert et al\\.,? 2015", "shortCiteRegEx": "Everaert et al\\.", "year": 2015}, {"title": "The acquisition of anaphora by simple recurrent networks", "author": ["Robert Frank", "Donald Mathis", "William Badecker."], "venue": "Language Acquisition, 20(3):181\u2013227.", "citeRegEx": "Frank et al\\.,? 2013", "shortCiteRegEx": "Frank et al\\.", "year": 2013}, {"title": "LSTM recurrent networks learn simple context-free and contextsensitive languages", "author": ["Felix Gers", "J\u00fcrgen Schmidhuber."], "venue": "IEEE Transactions on Neural Networks, 12(6):1333\u20131340.", "citeRegEx": "Gers and Schmidhuber.,? 2001", "shortCiteRegEx": "Gers and Schmidhuber.", "year": 2001}, {"title": "Negative and positive polarity items: Variation, licensing, and compositionality", "author": ["Anastasia Giannakidou."], "venue": "Claudia Maienborn, Klaus von Heusinger, and Paul Portner, editors, Semantics: An international handbook of natural language meaning. Berlin: Mouton de", "citeRegEx": "Giannakidou.,? 2011", "shortCiteRegEx": "Giannakidou.", "year": 2011}, {"title": "A dynamic oracle for arc-eager dependency parsing", "author": ["Yoav Goldberg", "Joakim Nivre."], "venue": "Proceedings of COLING 2012, pages 959\u2013976.", "citeRegEx": "Goldberg and Nivre.,? 2012", "shortCiteRegEx": "Goldberg and Nivre.", "year": 2012}, {"title": "Learning to transduce with unbounded memory", "author": ["Edward Grefenstette", "Karl Moritz Hermann", "Mustafa Suleyman", "Phil Blunsom."], "venue": "Advances in Neural Information Processing Systems, pages 1828\u20131836.", "citeRegEx": "Grefenstette et al\\.,? 2015", "shortCiteRegEx": "Grefenstette et al\\.", "year": 2015}, {"title": "Long short-term memory", "author": ["Sepp Hochreiter", "J\u00fcrgen Schmidhuber."], "venue": "Neural Computation, 9(8):1735\u2013 1780.", "citeRegEx": "Hochreiter and Schmidhuber.,? 1997", "shortCiteRegEx": "Hochreiter and Schmidhuber.", "year": 1997}, {"title": "The Cambridge Grammar of the English Language", "author": ["Rodney Huddleston", "Geoffrey K. Pullum."], "venue": "Cambridge University Press, Cambridge.", "citeRegEx": "Huddleston and Pullum.,? 2002", "shortCiteRegEx": "Huddleston and Pullum.", "year": 2002}, {"title": "Inferring algorithmic patterns with stack-augmented recurrent nets", "author": ["Armand Joulin", "Tomas Mikolov."], "venue": "Advances in Neural Information Processing Systems, pages 190\u2013198.", "citeRegEx": "Joulin and Mikolov.,? 2015", "shortCiteRegEx": "Joulin and Mikolov.", "year": 2015}, {"title": "Exploring the limits of language modeling", "author": ["Rafal Jozefowicz", "Oriol Vinyals", "Mike Schuster", "Noam Shazeer", "Yonghui Wu."], "venue": "arXiv preprint arXiv:1602.02410.", "citeRegEx": "Jozefowicz et al\\.,? 2016", "shortCiteRegEx": "Jozefowicz et al\\.", "year": 2016}, {"title": "Representation of linguistic form and function in recurrent neural networks", "author": ["\u00c1kos K\u00e1d\u00e1r", "Grzegorz Chrupa\u0142a", "Afra Alishahi."], "venue": "arXiv preprint arXiv:1602.08952.", "citeRegEx": "K\u00e1d\u00e1r et al\\.,? 2016", "shortCiteRegEx": "K\u00e1d\u00e1r et al\\.", "year": 2016}, {"title": "Visualizing and understanding recurrent networks", "author": ["Andrej Karpathy", "Justin Johnson", "Fei-Fei Li"], "venue": null, "citeRegEx": "Karpathy et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Karpathy et al\\.", "year": 2016}, {"title": "Adam: A method for stochastic optimization", "author": ["Diederik Kingma", "Jimmy Ba."], "venue": "International Conference for Learning Representations.", "citeRegEx": "Kingma and Ba.,? 2015", "shortCiteRegEx": "Kingma and Ba.", "year": 2015}, {"title": "Simple and accurate dependency parsing using bidirectional lstm feature representations", "author": ["Eliyahu Kiperwasser", "Yoav Goldberg."], "venue": "Transactions of the Association of Computational Linguistics, 4:313\u2013327.", "citeRegEx": "Kiperwasser and Goldberg.,? 2016", "shortCiteRegEx": "Kiperwasser and Goldberg.", "year": 2016}, {"title": "Unsupervised prediction of acceptability judgements", "author": ["Jey Han Lau", "Alexander Clark", "Shalom Lappin."], "venue": "Proceedings of ACL/IJCNLP, pages 1618\u20131628.", "citeRegEx": "Lau et al\\.,? 2015", "shortCiteRegEx": "Lau et al\\.", "year": 2015}, {"title": "Can recurrent neural networks learn natural language grammars", "author": ["Steve Lawrence", "Lee C. Giles", "Santliway Fong"], "venue": "In IEEE International Conference on Neural Networks,", "citeRegEx": "Lawrence et al\\.,? \\Q1996\\E", "shortCiteRegEx": "Lawrence et al\\.", "year": 1996}, {"title": "A theory of lexical access in speech production", "author": ["Willem J.M. Levelt", "Ardi Roelofs", "Antje S. Meyer."], "venue": "Behavioral and Brain Sciences, 22(1):1\u201375.", "citeRegEx": "Levelt et al\\.,? 1999", "shortCiteRegEx": "Levelt et al\\.", "year": 1999}, {"title": "Visualizing and understanding neural models in NLP", "author": ["Jiwei Li", "Xinlei Chen", "Eduard H. Hovy", "Dan Jurafsky."], "venue": "Proceedings of NAACL-HLT 2016, pages 681\u2013691.", "citeRegEx": "Li et al\\.,? 2016", "shortCiteRegEx": "Li et al\\.", "year": 2016}, {"title": "Last-conjunct agreement in Slovenian", "author": ["Franc Maru\u0161i\u010d", "Andrew Nevins", "Amanda Saksida."], "venue": "Annual Workshop on Formal Approaches to Slavic Linguistics, pages 210\u2013227.", "citeRegEx": "Maru\u0161i\u010d et al\\.,? 2007", "shortCiteRegEx": "Maru\u0161i\u010d et al\\.", "year": 2007}, {"title": "Recurrent neural network based language model", "author": ["Tomas Mikolov", "Martin Karafi\u00e1t", "Lukas Burget", "Jan Cernock\u1ef3", "Sanjeev Khudanpur."], "venue": "INTERSPEECH, pages 1045\u20131048.", "citeRegEx": "Mikolov et al\\.,? 2010", "shortCiteRegEx": "Mikolov et al\\.", "year": 2010}, {"title": "Subject\u2013verb agreement processes in comprehension", "author": ["Janet L. Nicol", "Kenneth I. Forster", "Csaba Veres."], "venue": "Journal of Memory and Language, 36(4):569\u2013587.", "citeRegEx": "Nicol et al\\.,? 1997", "shortCiteRegEx": "Nicol et al\\.", "year": 1997}, {"title": "Evaluation of dependency parsers on unbounded dependencies", "author": ["Joakim Nivre", "Laura Rimell", "Ryan McDonald", "Carlos Gomez-Rodriguez."], "venue": "Proceedings of the 23rd International Conference on Computational Linguistics, pages 833\u2013841. Association for Computa-", "citeRegEx": "Nivre et al\\.,? 2010", "shortCiteRegEx": "Nivre et al\\.", "year": 2010}, {"title": "Unbounded dependency recovery for parser evaluation", "author": ["Laura Rimell", "Stephen Clark", "Mark Steedman."], "venue": "Proceedings of EMNLP, pages 813\u2013821.", "citeRegEx": "Rimell et al\\.,? 2009", "shortCiteRegEx": "Rimell et al\\.", "year": 2009}, {"title": "A recurrent neural network that learns to count", "author": ["Paul Rodriguez", "Janet Wiles", "Jeffrey L. Elman."], "venue": "Connection Science, 11(1):5\u201340.", "citeRegEx": "Rodriguez et al\\.,? 1999", "shortCiteRegEx": "Rodriguez et al\\.", "year": 1999}, {"title": "Simple recurrent networks learn context-free and context-sensitive languages by counting", "author": ["Paul Rodriguez."], "venue": "Neural Computation, 13(9):2093\u20132118.", "citeRegEx": "Rodriguez.,? 2001", "shortCiteRegEx": "Rodriguez.", "year": 2001}, {"title": "Language acquisition in the absence of explicit negative evidence: How important is starting small", "author": ["Douglas L.T. Rohde", "David C. Plaut"], "venue": null, "citeRegEx": "Rohde and Plaut.,? \\Q1999\\E", "shortCiteRegEx": "Rohde and Plaut.", "year": 1999}, {"title": "Constraints on variables in syntax", "author": ["John Robert Ross."], "venue": "Ph.D. thesis, MIT.", "citeRegEx": "Ross.,? 1967", "shortCiteRegEx": "Ross.", "year": 1967}, {"title": "The empirical base of linguistics: Grammaticality judgments and linguistic methodology", "author": ["Carson T. Sch\u00fctze."], "venue": "Chicago, IL: University of Chicago Press.", "citeRegEx": "Sch\u00fctze.,? 1996", "shortCiteRegEx": "Sch\u00fctze.", "year": 1996}, {"title": "Recursive Deep Learning for Natural Language Processing and Computer Vision", "author": ["Richard Socher."], "venue": "Ph.D. thesis, Stanford University.", "citeRegEx": "Socher.,? 2014", "shortCiteRegEx": "Socher.", "year": 2014}, {"title": "On the interpretation of the number attraction effect: Response time evidence", "author": ["Adrian Staub."], "venue": "Journal of Memory and Language, 60(2):308\u2013327.", "citeRegEx": "Staub.,? 2009", "shortCiteRegEx": "Staub.", "year": 2009}, {"title": "LSTM neural networks for language modeling", "author": ["Martin Sundermeyer", "Ralf Schl\u00fcter", "Hermann Ney."], "venue": "INTERSPEECH.", "citeRegEx": "Sundermeyer et al\\.,? 2012", "shortCiteRegEx": "Sundermeyer et al\\.", "year": 2012}, {"title": "The time-course of feature interference in agreement comprehension: Multiple mechanisms and asymmetrical attraction", "author": ["Darren Tanner", "Janet Nicol", "Laurel Brehm."], "venue": "Journal of Memory and Language, 76:195\u2013 215.", "citeRegEx": "Tanner et al\\.,? 2014", "shortCiteRegEx": "Tanner et al\\.", "year": 2014}, {"title": "Grammar as a foreign language", "author": ["Oriol Vinyals", "\u0141ukasz Kaiser", "Terry Koo", "Slav Petrov", "Ilya Sutskever", "Geoffrey Hinton."], "venue": "Advances in Neural Information Processing Systems, pages 2755\u20132763.", "citeRegEx": "Vinyals et al\\.,? 2015", "shortCiteRegEx": "Vinyals et al\\.", "year": 2015}, {"title": "Agreement with nearest always bad? http://itre.cis.upenn.edu/ \u0303myl/ languagelog/archives/001846.html", "author": ["Arnold Zwicky"], "venue": null, "citeRegEx": "Zwicky.,? \\Q2005\\E", "shortCiteRegEx": "Zwicky.", "year": 2005}], "referenceMentions": [{"referenceID": 14, "context": "1 Introduction Recurrent neural networks (RNNs) are highly effective models of sequential data (Elman, 1990).", "startOffset": 95, "endOffset": 108}, {"referenceID": 23, "context": "The rapid adoption of RNNs in NLP systems in recent years, in particular of RNNs with gating mechanisms such as long short-term memory (LSTM) units (Hochreiter and Schmidhuber, 1997) or gated recurrent units (GRU) (Cho et al.", "startOffset": 148, "endOffset": 182}, {"referenceID": 9, "context": "The rapid adoption of RNNs in NLP systems in recent years, in particular of RNNs with gating mechanisms such as long short-term memory (LSTM) units (Hochreiter and Schmidhuber, 1997) or gated recurrent units (GRU) (Cho et al., 2014), has led to significant gains in language modeling (Mikolov et al.", "startOffset": 214, "endOffset": 232}, {"referenceID": 36, "context": ", 2014), has led to significant gains in language modeling (Mikolov et al., 2010; Sundermeyer et al., 2012), parsing (Vinyals et al.", "startOffset": 59, "endOffset": 107}, {"referenceID": 47, "context": ", 2014), has led to significant gains in language modeling (Mikolov et al., 2010; Sundermeyer et al., 2012), parsing (Vinyals et al.", "startOffset": 59, "endOffset": 107}, {"referenceID": 49, "context": ", 2012), parsing (Vinyals et al., 2015; Kiperwasser and Goldberg, 2016; Dyer et al., 2016), machine translation (Bahdanau et al.", "startOffset": 17, "endOffset": 90}, {"referenceID": 30, "context": ", 2012), parsing (Vinyals et al., 2015; Kiperwasser and Goldberg, 2016; Dyer et al., 2016), machine translation (Bahdanau et al.", "startOffset": 17, "endOffset": 90}, {"referenceID": 12, "context": ", 2012), parsing (Vinyals et al., 2015; Kiperwasser and Goldberg, 2016; Dyer et al., 2016), machine translation (Bahdanau et al.", "startOffset": 17, "endOffset": 90}, {"referenceID": 1, "context": ", 2016), machine translation (Bahdanau et al., 2015) and other tasks.", "startOffset": 29, "endOffset": 52}, {"referenceID": 10, "context": "Other dependencies, however, are sensitive to the syntactic structure of the sentence (Chomsky, 1965; Everaert et al., 2015).", "startOffset": 86, "endOffset": 124}, {"referenceID": 17, "context": "Other dependencies, however, are sensitive to the syntactic structure of the sentence (Chomsky, 1965; Everaert et al., 2015).", "startOffset": 86, "endOffset": 124}, {"referenceID": 19, "context": "To what extent can RNNs learn to model such phenomena based only on sequential cues? Previous research has shown that RNNs (in particular LSTMs) can learn artificial context-free languages (Gers and Schmidhuber, 2001) as well as nesting and In this work we use the term RNN to refer to the entire class of sequential recurrent neural networks.", "startOffset": 189, "endOffset": 217}, {"referenceID": 1, "context": ", 2016), machine translation (Bahdanau et al., 2015) and other tasks. The effectiveness of RNNs1 is attributed to their ability to capture statistical contingencies that may span an arbitrary number of words. The word France, for example, is more likely to occur somewhere in a sentence that begins with Paris than in a sentence that begins with Penguins. The fact that an arbitrary number of words can intervene between the mutually predictive words implies that they cannot be captured by models with a fixed window such as n-gram models, but can in principle be captured by RNNs, which do not have an architecturally fixed limit on dependency length. RNNs are sequence models: they do not explicitly incorporate syntactic structure. Indeed, many word co-occurrence statistics can be captured by treating the sentence as an unstructured list of words (ParisFrance); it is therefore unsurprising that RNNs can learn them well. Other dependencies, however, are sensitive to the syntactic structure of the sentence (Chomsky, 1965; Everaert et al., 2015). To what extent can RNNs learn to model such phenomena based only on sequential cues? Previous research has shown that RNNs (in particular LSTMs) can learn artificial context-free languages (Gers and Schmidhuber, 2001) as well as nesting and In this work we use the term RNN to refer to the entire class of sequential recurrent neural networks. Instances of the class include long short-term memory networks (LSTM) and the Simple Recurrent Network (SRN) due to Elman (1990). ar X iv :1 61 1.", "startOffset": 30, "endOffset": 1527}, {"referenceID": 28, "context": "indentation in a programming language (Karpathy et al., 2016).", "startOffset": 38, "endOffset": 61}, {"referenceID": 26, "context": "Even a state-ofthe-art large-scale language model (Jozefowicz et al., 2016) was highly sensitive to recent but structurally irrelevant nouns, making more than five times as many mistakes as the number prediction model on these harder cases.", "startOffset": 50, "endOffset": 75}, {"referenceID": 14, "context": "These results suggest that explicit supervision is necessary for learning the agreement dependency using this architecture, limiting its plausibility as a model of child language acquisition (Elman, 1990).", "startOffset": 191, "endOffset": 204}, {"referenceID": 3, "context": "A more fundamental challenge that the dependency poses for structure-insensitive models is the possibility of agreement attraction errors (Bock and Miller, 1991).", "startOffset": 138, "endOffset": 161}, {"referenceID": 17, "context": "Given the difficulty in identifying the subject from the linear sequence of the sentence, dependencies such as subject-verb agreement serve as an argument for structured syntactic representations in humans (Everaert et al., 2015); they may challenge models such as RNNs that do not have pre-wired syntactic representations.", "startOffset": 206, "endOffset": 229}, {"referenceID": 29, "context": "The network was optimized using Adam (Kingma and Ba, 2015) and early stopping based on validation set error.", "startOffset": 37, "endOffset": 58}, {"referenceID": 21, "context": "Agreement attractors: We next examine how the model\u2019s error rate was affected by nouns that intervened between the subject and the verb in the linear These properties of the dependencies were identified by parsing the test sentences using the parser described in Goldberg and Nivre (2012).", "startOffset": 263, "endOffset": 289}, {"referenceID": 33, "context": ", write), he or she needs to decide whether its form will be write or writes (Levelt et al., 1999; Staub, 2009).", "startOffset": 77, "endOffset": 111}, {"referenceID": 46, "context": ", write), he or she needs to decide whether its form will be write or writes (Levelt et al., 1999; Staub, 2009).", "startOffset": 77, "endOffset": 111}, {"referenceID": 44, "context": "This task is modeled after a common human data collection technique in linguistics (Sch\u00fctze, 1996), although our training regime is of course very different to the training that humans are exposed to: humans rarely receive ungrammatical sentences labeled as such (Bowerman, 1988).", "startOffset": 83, "endOffset": 98}, {"referenceID": 4, "context": "This task is modeled after a common human data collection technique in linguistics (Sch\u00fctze, 1996), although our training regime is of course very different to the training that humans are exposed to: humans rarely receive ungrammatical sentences labeled as such (Bowerman, 1988).", "startOffset": 263, "endOffset": 279}, {"referenceID": 14, "context": "Language modeling (LM): Finally, we experimented with a word prediction objective, in which the model did not receive any grammatically relevant supervision (Elman, 1990; Elman, 1991).", "startOffset": 157, "endOffset": 183}, {"referenceID": 15, "context": "Language modeling (LM): Finally, we experimented with a word prediction objective, in which the model did not receive any grammatically relevant supervision (Elman, 1990; Elman, 1991).", "startOffset": 157, "endOffset": 183}, {"referenceID": 24, "context": "with collective nouns such as group, which are compatible with both singular and plural verbs in some dialects of English (Huddleston and Pullum, 2002); those cases appear to be rare.", "startOffset": 122, "endOffset": 151}, {"referenceID": 26, "context": "Figure 4: Alternative tasks and additional experiments: (a) overall error rate across tasks (note that the y-axis ends in 10%); (b) effect of count of attractors in homogeneous dependencies across training objectives; (c) comparison of the Google LM (Jozefowicz et al., 2016) to our LM and one of our supervised verb inflection systems, on a sample of sentences; (d) number prediction: effect of count of attractors using SRNs with standard training or LSTM with targeted training; (e) number prediction: difference in error rate between singular and plural subjects across RNN cell types.", "startOffset": 250, "endOffset": 275}, {"referenceID": 26, "context": "Would the performance gap between the LM and the explicitly supervised models close if we increased the capacity of the LM? We address this question using a very large publicly available LM (Jozefowicz et al., 2016), which we refer to as the Google LM.", "startOffset": 190, "endOffset": 215}, {"referenceID": 8, "context": "12 The Google LM represent the current state-of-the-art in language modeling: it is trained on a billion-word corpus (Chelba et al., 2013), with a vocabulary of 800,000 words.", "startOffset": 117, "endOffset": 138}, {"referenceID": 14, "context": "6 Additional Experiments Comparison to simple recurrent networks: How much of the success of the network is due to the LSTM cells? We repeated the number prediction experiment with a simple recurrent network (SRN) (Elman, 1990), with the same number of hidden units.", "startOffset": 214, "endOffset": 227}, {"referenceID": 16, "context": "While tentative at this point, these results suggest that oversampling difficult training cases may be beneficial; a curriculum progressing from easier to harder dependencies (Elman, 1993) may provide additional gains.", "startOffset": 175, "endOffset": 188}, {"referenceID": 3, "context": "Agreement attraction errors in humans are much more common when the attractor is plural than when it is singular (Bock and Miller, 1991; Eberhard et al., 2005).", "startOffset": 113, "endOffset": 159}, {"referenceID": 13, "context": "Agreement attraction errors in humans are much more common when the attractor is plural than when it is singular (Bock and Miller, 1991; Eberhard et al., 2005).", "startOffset": 113, "endOffset": 159}, {"referenceID": 47, "context": "8 Related Work The majority of NLP work on neural networks evaluates them on their performance in a task such as language modeling or machine translation (Sundermeyer et al., 2012; Bahdanau et al., 2015).", "startOffset": 154, "endOffset": 203}, {"referenceID": 1, "context": "8 Related Work The majority of NLP work on neural networks evaluates them on their performance in a task such as language modeling or machine translation (Sundermeyer et al., 2012; Bahdanau et al., 2015).", "startOffset": 154, "endOffset": 203}, {"referenceID": 40, "context": "Simple recurrent networks struggled with this language (Rodriguez et al., 1999; Rodriguez, 2001).", "startOffset": 55, "endOffset": 96}, {"referenceID": 41, "context": "Simple recurrent networks struggled with this language (Rodriguez et al., 1999; Rodriguez, 2001).", "startOffset": 55, "endOffset": 96}, {"referenceID": 16, "context": "Elman (1991) tested an SRN on a miniature language that simulated English relative clauses, and found that the network was only able to learn the language under highly specific circumstances (Elman, 1993), though later work has called some of his conclusions into question (Rohde and Plaut, 1999; Cartling, 2008).", "startOffset": 191, "endOffset": 204}, {"referenceID": 42, "context": "Elman (1991) tested an SRN on a miniature language that simulated English relative clauses, and found that the network was only able to learn the language under highly specific circumstances (Elman, 1993), though later work has called some of his conclusions into question (Rohde and Plaut, 1999; Cartling, 2008).", "startOffset": 273, "endOffset": 312}, {"referenceID": 6, "context": "Elman (1991) tested an SRN on a miniature language that simulated English relative clauses, and found that the network was only able to learn the language under highly specific circumstances (Elman, 1993), though later work has called some of his conclusions into question (Rohde and Plaut, 1999; Cartling, 2008).", "startOffset": 273, "endOffset": 312}, {"referenceID": 0, "context": ", 2012; Bahdanau et al., 2015). These evaluation setups average over many different syntactic constructions, making it difficult to isolate the network\u2019s syntactic capabilities. Other studies have tested the capabilities of RNNs to learn simple artificial languages. Gers and Schmidhuber (2001) showed that LSTMs can learn the context-free language anbn, generalizing to ns as high as 1000 even when trained only on n \u2208 {1, .", "startOffset": 8, "endOffset": 295}, {"referenceID": 0, "context": ", 2012; Bahdanau et al., 2015). These evaluation setups average over many different syntactic constructions, making it difficult to isolate the network\u2019s syntactic capabilities. Other studies have tested the capabilities of RNNs to learn simple artificial languages. Gers and Schmidhuber (2001) showed that LSTMs can learn the context-free language anbn, generalizing to ns as high as 1000 even when trained only on n \u2208 {1, . . . , 10}. Simple recurrent networks struggled with this language (Rodriguez et al., 1999; Rodriguez, 2001). These results have been recently replicated and extended by Joulin and Mikolov (2015). Elman (1991) tested an SRN on a miniature language that simulated English relative clauses, and found that the network was only able to learn the language under highly specific circumstances (Elman, 1993), though later work has called some of his conclusions into question (Rohde and Plaut, 1999; Cartling, 2008).", "startOffset": 8, "endOffset": 621}, {"referenceID": 0, "context": ", 2012; Bahdanau et al., 2015). These evaluation setups average over many different syntactic constructions, making it difficult to isolate the network\u2019s syntactic capabilities. Other studies have tested the capabilities of RNNs to learn simple artificial languages. Gers and Schmidhuber (2001) showed that LSTMs can learn the context-free language anbn, generalizing to ns as high as 1000 even when trained only on n \u2208 {1, . . . , 10}. Simple recurrent networks struggled with this language (Rodriguez et al., 1999; Rodriguez, 2001). These results have been recently replicated and extended by Joulin and Mikolov (2015). Elman (1991) tested an SRN on a miniature language that simulated English relative clauses, and found that the network was only able to learn the language under highly specific circumstances (Elman, 1993), though later work has called some of his conclusions into question (Rohde and Plaut, 1999; Cartling, 2008).", "startOffset": 8, "endOffset": 635}, {"referenceID": 0, "context": ", 2012; Bahdanau et al., 2015). These evaluation setups average over many different syntactic constructions, making it difficult to isolate the network\u2019s syntactic capabilities. Other studies have tested the capabilities of RNNs to learn simple artificial languages. Gers and Schmidhuber (2001) showed that LSTMs can learn the context-free language anbn, generalizing to ns as high as 1000 even when trained only on n \u2208 {1, . . . , 10}. Simple recurrent networks struggled with this language (Rodriguez et al., 1999; Rodriguez, 2001). These results have been recently replicated and extended by Joulin and Mikolov (2015). Elman (1991) tested an SRN on a miniature language that simulated English relative clauses, and found that the network was only able to learn the language under highly specific circumstances (Elman, 1993), though later work has called some of his conclusions into question (Rohde and Plaut, 1999; Cartling, 2008). Frank et al. (2013) studied the acquisition of anaphora coreference by SRNs, again in a miniature language.", "startOffset": 8, "endOffset": 956}, {"referenceID": 0, "context": ", 2012; Bahdanau et al., 2015). These evaluation setups average over many different syntactic constructions, making it difficult to isolate the network\u2019s syntactic capabilities. Other studies have tested the capabilities of RNNs to learn simple artificial languages. Gers and Schmidhuber (2001) showed that LSTMs can learn the context-free language anbn, generalizing to ns as high as 1000 even when trained only on n \u2208 {1, . . . , 10}. Simple recurrent networks struggled with this language (Rodriguez et al., 1999; Rodriguez, 2001). These results have been recently replicated and extended by Joulin and Mikolov (2015). Elman (1991) tested an SRN on a miniature language that simulated English relative clauses, and found that the network was only able to learn the language under highly specific circumstances (Elman, 1993), though later work has called some of his conclusions into question (Rohde and Plaut, 1999; Cartling, 2008). Frank et al. (2013) studied the acquisition of anaphora coreference by SRNs, again in a miniature language. Recently, Bowman et al. (2015) tested the ability of LSTMs to learn an artificial language based on propositional logic.", "startOffset": 8, "endOffset": 1075}, {"referenceID": 0, "context": ", 2012; Bahdanau et al., 2015). These evaluation setups average over many different syntactic constructions, making it difficult to isolate the network\u2019s syntactic capabilities. Other studies have tested the capabilities of RNNs to learn simple artificial languages. Gers and Schmidhuber (2001) showed that LSTMs can learn the context-free language anbn, generalizing to ns as high as 1000 even when trained only on n \u2208 {1, . . . , 10}. Simple recurrent networks struggled with this language (Rodriguez et al., 1999; Rodriguez, 2001). These results have been recently replicated and extended by Joulin and Mikolov (2015). Elman (1991) tested an SRN on a miniature language that simulated English relative clauses, and found that the network was only able to learn the language under highly specific circumstances (Elman, 1993), though later work has called some of his conclusions into question (Rohde and Plaut, 1999; Cartling, 2008). Frank et al. (2013) studied the acquisition of anaphora coreference by SRNs, again in a miniature language. Recently, Bowman et al. (2015) tested the ability of LSTMs to learn an artificial language based on propositional logic. As in our study, the performance of the network degraded as the complexity of the test sentences increased. Karpathy et al. (2016) present analyses and visualization methods for character-level RNNs.", "startOffset": 8, "endOffset": 1296}, {"referenceID": 0, "context": ", 2012; Bahdanau et al., 2015). These evaluation setups average over many different syntactic constructions, making it difficult to isolate the network\u2019s syntactic capabilities. Other studies have tested the capabilities of RNNs to learn simple artificial languages. Gers and Schmidhuber (2001) showed that LSTMs can learn the context-free language anbn, generalizing to ns as high as 1000 even when trained only on n \u2208 {1, . . . , 10}. Simple recurrent networks struggled with this language (Rodriguez et al., 1999; Rodriguez, 2001). These results have been recently replicated and extended by Joulin and Mikolov (2015). Elman (1991) tested an SRN on a miniature language that simulated English relative clauses, and found that the network was only able to learn the language under highly specific circumstances (Elman, 1993), though later work has called some of his conclusions into question (Rohde and Plaut, 1999; Cartling, 2008). Frank et al. (2013) studied the acquisition of anaphora coreference by SRNs, again in a miniature language. Recently, Bowman et al. (2015) tested the ability of LSTMs to learn an artificial language based on propositional logic. As in our study, the performance of the network degraded as the complexity of the test sentences increased. Karpathy et al. (2016) present analyses and visualization methods for character-level RNNs. K\u00e1d\u00e1r et al. (2016) and Li et al.", "startOffset": 8, "endOffset": 1385}, {"referenceID": 0, "context": ", 2012; Bahdanau et al., 2015). These evaluation setups average over many different syntactic constructions, making it difficult to isolate the network\u2019s syntactic capabilities. Other studies have tested the capabilities of RNNs to learn simple artificial languages. Gers and Schmidhuber (2001) showed that LSTMs can learn the context-free language anbn, generalizing to ns as high as 1000 even when trained only on n \u2208 {1, . . . , 10}. Simple recurrent networks struggled with this language (Rodriguez et al., 1999; Rodriguez, 2001). These results have been recently replicated and extended by Joulin and Mikolov (2015). Elman (1991) tested an SRN on a miniature language that simulated English relative clauses, and found that the network was only able to learn the language under highly specific circumstances (Elman, 1993), though later work has called some of his conclusions into question (Rohde and Plaut, 1999; Cartling, 2008). Frank et al. (2013) studied the acquisition of anaphora coreference by SRNs, again in a miniature language. Recently, Bowman et al. (2015) tested the ability of LSTMs to learn an artificial language based on propositional logic. As in our study, the performance of the network degraded as the complexity of the test sentences increased. Karpathy et al. (2016) present analyses and visualization methods for character-level RNNs. K\u00e1d\u00e1r et al. (2016) and Li et al. (2016) suggest visualization techniques for word-level RNNs trained to perform tasks that aren\u2019t explicitly syntactic (image captioning and sentiment analysis).", "startOffset": 8, "endOffset": 1406}, {"referenceID": 0, "context": "Early work that used neural networks to model grammaticality judgments includes Allen and Seidenberg (1999) and Lawrence et al.", "startOffset": 80, "endOffset": 108}, {"referenceID": 0, "context": "Early work that used neural networks to model grammaticality judgments includes Allen and Seidenberg (1999) and Lawrence et al. (1996). More recently, the connection between grammaticality judg-", "startOffset": 80, "endOffset": 135}, {"referenceID": 39, "context": "Finally, arguments for evaluating NLP models on a strategically sampled set of dependency types rather than a random sample of sentences have been made in the parsing literature (Rimell et al., 2009; Nivre et al., 2010; Bender et al., 2011).", "startOffset": 178, "endOffset": 240}, {"referenceID": 38, "context": "Finally, arguments for evaluating NLP models on a strategically sampled set of dependency types rather than a random sample of sentences have been made in the parsing literature (Rimell et al., 2009; Nivre et al., 2010; Bender et al., 2011).", "startOffset": 178, "endOffset": 240}, {"referenceID": 2, "context": "Finally, arguments for evaluating NLP models on a strategically sampled set of dependency types rather than a random sample of sentences have been made in the parsing literature (Rimell et al., 2009; Nivre et al., 2010; Bender et al., 2011).", "startOffset": 178, "endOffset": 240}, {"referenceID": 10, "context": "ments and the probabilities assigned by a language model was explored by Clark et al. (2013) and Lau et al.", "startOffset": 73, "endOffset": 93}, {"referenceID": 10, "context": "ments and the probabilities assigned by a language model was explored by Clark et al. (2013) and Lau et al. (2015). Finally, arguments for evaluating NLP models on a strategically sampled set of dependency types rather than a random sample of sentences have been made in the parsing literature (Rimell et al.", "startOffset": 73, "endOffset": 115}, {"referenceID": 45, "context": "This makes it difficult to distinguish simple but robust sequence models from more expressive architectures (Socher, 2014; Grefenstette et al., 2015; Joulin and Mikolov, 2015).", "startOffset": 108, "endOffset": 175}, {"referenceID": 22, "context": "This makes it difficult to distinguish simple but robust sequence models from more expressive architectures (Socher, 2014; Grefenstette et al., 2015; Joulin and Mikolov, 2015).", "startOffset": 108, "endOffset": 175}, {"referenceID": 25, "context": "This makes it difficult to distinguish simple but robust sequence models from more expressive architectures (Socher, 2014; Grefenstette et al., 2015; Joulin and Mikolov, 2015).", "startOffset": 108, "endOffset": 175}, {"referenceID": 39, "context": "Our work suggests an alternative strategy\u2014evaluation on naturally occurring sentences that are sampled based on their grammatical complexity\u2014which can provide more nuanced tests of language models (Rimell et al., 2009; Bender et al., 2011).", "startOffset": 197, "endOffset": 239}, {"referenceID": 2, "context": "Our work suggests an alternative strategy\u2014evaluation on naturally occurring sentences that are sampled based on their grammatical complexity\u2014which can provide more nuanced tests of language models (Rimell et al., 2009; Bender et al., 2011).", "startOffset": 197, "endOffset": 239}, {"referenceID": 7, "context": "As such, neural models used in NLP applications may benefit from grammatically sophisticated sentence representations developed in a multitask learning setup (Caruana, 1998), where the model is trained concurrently on the task of interest and on one of the tasks we proposed in this paper.", "startOffset": 158, "endOffset": 173}, {"referenceID": 20, "context": "The distribution of negative polarity items is highly sensitive to semantic factors (Giannakidou, 2011).", "startOffset": 84, "endOffset": 103}, {"referenceID": 43, "context": "Restrictions on unbounded dependencies (Ross, 1967) may require richer syntactic representations than those required for subject-verb dependencies.", "startOffset": 39, "endOffset": 51}, {"referenceID": 3, "context": "Humans occasionally make agreement attraction mistakes during language production (Bock and Miller, 1991) and comprehension (Nicol et al.", "startOffset": 82, "endOffset": 105}, {"referenceID": 37, "context": "Humans occasionally make agreement attraction mistakes during language production (Bock and Miller, 1991) and comprehension (Nicol et al., 1997).", "startOffset": 124, "endOffset": 144}, {"referenceID": 48, "context": "These errors persist in human acceptability judgments (Tanner et al., 2014), which parallel our grammaticality judgment task.", "startOffset": 54, "endOffset": 75}, {"referenceID": 35, "context": "Cases of grammatical agreement with the nearest rather than structurally relevant constituent have been documented in languages such as Slovenian (Maru\u0161i\u010d et al., 2007), and have even been argued to be occasionally grammatical in English (Zwicky, 2005).", "startOffset": 146, "endOffset": 168}, {"referenceID": 50, "context": ", 2007), and have even been argued to be occasionally grammatical in English (Zwicky, 2005).", "startOffset": 77, "endOffset": 91}], "year": 2016, "abstractText": "The success of long short-term memory (LSTM) neural networks in language processing is typically attributed to their ability to capture long-distance statistical regularities. Linguistic regularities are often sensitive to syntactic structure; can such dependencies be captured by LSTMs, which do not have explicit structural representations? We begin addressing this question using number agreement in English subject-verb dependencies. We probe the architecture\u2019s grammatical competence both using training objectives with an explicit grammatical target (number prediction, grammaticality judgments) and using language models. In the strongly supervised settings, the LSTM achieved very high overall accuracy (less than 1% errors), but errors increased when sequential and structural information conflicted. The frequency of such errors rose sharply in the language-modeling setting. We conclude that LSTMs can capture a non-trivial amount of grammatical structure given targeted supervision, but stronger architectures may be required to further reduce errors; furthermore, the language modeling signal is insufficient for capturing syntax-sensitive dependencies, and should be supplemented with more direct supervision if such dependencies need to be captured.", "creator": "LaTeX with hyperref package"}}}