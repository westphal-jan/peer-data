{"id": "1611.01423", "review": {"conference": "ICML", "VERSION": "v1", "DATE_OF_SUBMISSION": "4-Nov-2016", "title": "Learning Continuous Semantic Representations of Symbolic Expressions", "abstract": "The question of how procedural knowledge is represented and inferred is a fundamental problem in machine learning and artificial intelligence. Recent work on program induction has proposed neural architectures, based on abstractions like stacks, Turing machines, and interpreters, that operate on abstract computational machines or on execution traces. But the recursive abstraction that is central to procedural knowledge is perhaps most naturally represented by symbolic representations that have syntactic structure, such as logical expressions and source code. Combining abstract, symbolic reasoning with continuous neural reasoning is a grand challenge of representation learning. As a step in this direction, we propose a new architecture, called neural equivalence networks, for the problem of learning continuous semantic representations of mathematical and logical expressions. These networks are trained to represent semantic equivalence, even of expressions that are syntactically very different. The challenge is that semantic representations must be computed in a syntax-directed manner, because semantics is compositional, but at the same time, small changes in syntax can lead to very large changes in semantics, which can be difficult for continuous neural architectures. We perform an exhaustive evaluation on the task of checking equivalence on a highly diverse class of symbolic algebraic and boolean expression types, showing that our model significantly outperforms existing architectures.", "histories": [["v1", "Fri, 4 Nov 2016 15:30:43 GMT  (1604kb,D)", "https://arxiv.org/abs/1611.01423v1", null], ["v2", "Sat, 10 Jun 2017 19:18:55 GMT  (3178kb,D)", "http://arxiv.org/abs/1611.01423v2", "Accepted to ICML 2017"]], "reviews": [], "SUBJECTS": "cs.LG cs.AI", "authors": ["miltiadis allamanis", "pankajan chanthirasegaran", "pushmeet kohli", "charles a sutton"], "accepted": true, "id": "1611.01423"}, "pdf": {"name": "1611.01423.pdf", "metadata": {"source": "META", "title": "Learning Continuous Semantic Representations of Symbolic Expressions", "authors": ["Miltiadis Allamanis", "Pankajan Chanthirasegaran", "Pushmeet Kohli", "Charles Sutton"], "emails": ["<t-mialla@microsoft.com>."], "sections": [{"heading": "1. Introduction", "text": "This year it is so far that it will only be a matter of time before an agreement is reached."}, {"heading": "2. Model", "text": "In this work, we are interested in semantic, compositional representations of mathematical expressions, which we call SEMVECs, and in learning to generate identical representations of expressions that are semantically equivalent, i.e. they belong to the same class of equivalence. Equivalence is a stronger property than similarity, which was the focus of previous work in neural network learning (Chopra et al., 2005), since equivalence is additionally a transitive relationship ship.Problem Hardness. Finding the equivalence of arbitrary symbolic expressions is a NP-hard problem or worse. If we focus, for example, on Boolean expressions, we reduce an expression to the representation of the false equivalence class to prove their dissatisfaction - an NPcomplete problem. Of course, we do not expect to deal with a NP-complete problem with neural networks."}, {"heading": "2.1. Neural Equivalence Networks", "text": "This year, it is only a matter of time before agreement is reached."}, {"heading": "2.2. Training", "text": "We train EQNETs from a data set of expressions whose semantic equivalence is known. (Given a training set T = {T1... TN} of expression trees, we assume that the training set will be in equivalence classes E = {e1... eJ}. We use a monitored target similar to the classification; the difference between classification and our setting is that while standard classification problems take into account a fixed set of class names, the number of equivalence classes in the training set will vary with N. Given an expression tree T belonging to equivalence class E, we calculate the probability class P (ei | T) = exp (TREENN) > qei + bi) jexp (TREENN (T) > qej + bj), where Qei are model parameters that we can interpret as representations of each equivalence class."}, {"heading": "3. Evaluation", "text": "We generate datasets of expressions grouped into equivalence classes to determine their order of value. We generate datasets from the BOOL domain. The datasets from the BOOL domain contain Boolean expressions and the knowledge associated with them. The datasets from the BOOL domain contain two different types of expressions that relate to a single expression. When we define equivalence, we interpret different variables that refer to different entities in the domain, such as the polynomial C, which combines two expressions, and a non-uniform operator applied to a single expression."}, {"heading": "3.1. Quantitative Evaluation", "text": "This year it is as far as never before in the history of the Federal Republic of Germany."}, {"heading": "3.2. Qualitative Evaluation", "text": "(b) c (b) c (b) c (b) c (b) c (c) c (c) c (c) c (b) c (b) c (b) c (c) c (c) c (c) c (c) c) c (c) c (c) c (c) c (c) c (c) c (c) c (c) c) c (c) c) c (c) c (b) c (c) c (c) c) c (c) c (c) c (c) c) c (c) c (c) c) c (c) c (c) c (c) c (c) c (c) c (c) c (c) c (c) c) c c c (c) c c c (c) c) c (c) c (c) c) c (c) c (c) c) c (c) c (c) c (c) c (c) c (c) c) c (c) c (c) c) c (c) c) c (c) c (c) c) c (c) c (c) c) c (c) c (c) c) c (c) c (c) c) c (c) c (c) c (c) c (c) c) c (c) c (c) c (c) c) c (c) c (c) c (c) c) c (c) c (c) c (c) c) c (c) c (c) c (c) c (c) c) c (c) c (c) c (c) c (c) c) c (c (c) c (c) c (c) c (c) c) c (c) c (c) c) c (c) c (c) c (c) c) c (c) c (c) c) c (c) c (c) c) c (c) c (c) c (c) c (c) c (c) c) c (c) c (c) c (c) c) c (c) c) c) c (c) c (c (c) c) c (c) c (c) c"}, {"heading": "4. Related Work", "text": "In this context, it is also necessary to ask to what extent the implementation of strategies into practice is actually a strategy that has proven itself in practice. Thus, for example, it is possible to reconcile the individual strategies and strategies of the individual strategies and strategies of the different strategies and strategies of the different strategies and strategies of the different strategies and strategies in order to reconcile the respective strategies and strategies of the individual strategies and strategies of the different strategies and strategies of the different strategies and strategies of the different strategies and strategies of the different strategies and strategies of the different strategies and strategies of the different strategies and strategies. In this context, it is also important that the individual strategies and strategies of the individual strategies and strategies of the different strategies and strategies of the different strategies and strategies of the different strategies and strategies of the different strategies and strategies of the different strategies and strategies of the different strategies and strategies of the different strategies and strategies of the different strategies and strategies of the different strategies and strategies of the different strategies and strategies of the different strategies of the different strategies and strategies of the different strategies and strategies of the different strategies of the different strategies and strategies of the different strategies of the different strategies and strategies of the different strategies of the different strategies and strategies of the different strategies and strategies of the different strategies of the different strategies and strategies of the different strategies and strategies of the different strategies of the different strategies and strategies of the different strategies of the different strategies and strategies of the different strategies of the different strategies and strategies of the different strategies and strategies of the different strategies of the different strategies and strategies of the different strategies of the different strategies and strategies of the different strategies of the different strategies and strategies of the different strategies of the different strategies and strategies of the different strategies of the different strategies and strategies of the different strategies of the different strategies and strategies of the different strategies and strategies of the different strategies and strategies of the different strategies of the different strategies of the different strategies and strategies of the different strategies and strategies of the different strategies of the different strategies and strategies of the different strategies of the different strategies and strategies of the different strategies of the different strategies and strategies of the different strategies of the different strategies and strategies of the different strategies and strategies of the different strategies of the different strategies and strategies of the different strategies of the different strategies and strategies of the different strategies and strategies of the different strategies of the different strategies of the different strategies and strategies of the different strategies of the different strategies of the different strategies and strategies of the different strategies of the different strategies of"}, {"heading": "5. Discussion & Conclusions", "text": "In this paper, we presented EQNETs, a first step in learning continuous semantic representations (SEMVECs) of procedural knowledge. SEMVECs have the potential to bridge continuous representations with symbolic representations, which is useful for multiple applications in artificial intelligence, machine learning, and programming languages.We show that EQNETs work significantly better than modern alternatives, but further improvements are needed, especially for a more robust training of compositional models. In addition, we have an exponential explosion of the semantic space to be represented even for relatively small symbolic expressions. Fixed SEMVECs, as used in EQNET, ultimately limit the available capacity to represent procedural knowledge. In order to present more complex procedures in the future, varying sizes of representations seem to be required."}, {"heading": "Acknowledgments", "text": "This work was supported by Microsoft Research through its PhD Scholarship Program and the Engineering and Physical Sciences Research Council [grant number EP / K024043 / 1] and we thank the Data Science EPSRC Centre for Doctoral Training at the University of Edinburgh for providing additional computing resources."}, {"heading": "A. Synthetic Expression Datasets", "text": "Table 3 and Table 4 are sample expressions within an equivalence class for the two dataset types we are considering."}, {"heading": "B. Detailed Evaluation", "text": "Figure 6 shows the detailed evaluation of our k-NN metric for each dataset. Figure 7 shows the detailed evaluation when using models trained on simpler datasets but tested on more complex ones, and essentially evaluates the learned composition of the models. Figure 9 shows how performance varies between datasets due to their properties. As expected, performance deteriorates with increasing number of variables (Figure 9a) and expressions with more complex operators tend to perform worse (Figure 9b). Results for UNSEENEQCLASS look very similar and are not plotted here."}, {"heading": "C. Model Hyperparameters", "text": "The optimized hyperparameters are listed in Table 5. All hyperparameters were optimized with the Spearmint (Snoek et al., 2012) Bayesian optimization package. The same range was used for all common hyperparameters."}], "references": [{"title": "DeepMath \u2013 Deep sequence models for premise selection", "author": ["Alemi", "Alex A", "Chollet", "Francois", "Irving", "Geoffrey", "Szegedy", "Christian", "Urban", "Josef"], "venue": "arXiv preprint arXiv:1606.04442,", "citeRegEx": "Alemi et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Alemi et al\\.", "year": 2016}, {"title": "A convolutional attention network for extreme summarization of source code", "author": ["Allamanis", "Miltiadis", "Peng", "Hao", "Sutton", "Charles"], "venue": "In ICML,", "citeRegEx": "Allamanis et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Allamanis et al\\.", "year": 2016}, {"title": "Neural machine translation by jointly learning to align and translate", "author": ["Bahdanau", "Dzmitry", "Cho", "Kyunghyun", "Bengio", "Yoshua"], "venue": "In ICLR,", "citeRegEx": "Bahdanau et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Bahdanau et al\\.", "year": 2015}, {"title": "Learning a similarity metric discriminatively, with application to face verification", "author": ["Chopra", "Sumit", "Hadsell", "Raia", "LeCun", "Yann"], "venue": "In CVPR,", "citeRegEx": "Chopra et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Chopra et al\\.", "year": 2005}, {"title": "Transition-based dependency parsing with stack long short-term memory", "author": ["Dyer", "Chris", "Ballesteros", "Miguel", "Ling", "Wang", "Matthews", "Austin", "Smith", "Noah A"], "venue": "In ACL,", "citeRegEx": "Dyer et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Dyer et al\\.", "year": 2015}, {"title": "Learning to transduce with unbounded memory", "author": ["Grefenstette", "Edward", "Hermann", "Karl Moritz", "Suleyman", "Mustafa", "Blunsom", "Phil"], "venue": "In NIPS,", "citeRegEx": "Grefenstette et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Grefenstette et al\\.", "year": 2015}, {"title": "A neural compiler", "author": ["Gruau", "Fr\u00e9d\u00e9ric", "Ratajszczak", "Jean-Yves", "Wiber", "Gilles"], "venue": "Theoretical Computer Science,", "citeRegEx": "Gruau et al\\.,? \\Q1995\\E", "shortCiteRegEx": "Gruau et al\\.", "year": 1995}, {"title": "Inferring algorithmic patterns with stack-augmented recurrent nets", "author": ["Joulin", "Armand", "Mikolov", "Tomas"], "venue": "In NIPS,", "citeRegEx": "Joulin et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Joulin et al\\.", "year": 2015}, {"title": "Neural GPUs learn algorithms", "author": ["Kaiser", "\u0141ukasz", "Sutskever", "Ilya"], "venue": "In ICLR,", "citeRegEx": "Kaiser et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Kaiser et al\\.", "year": 2016}, {"title": "Deep network guided proof search", "author": ["Loos", "Sarah", "Irving", "Geoffrey", "Szegedy", "Christian", "Kaliszyk", "Cezary"], "venue": "arXiv preprint arXiv:1701.06972,", "citeRegEx": "Loos et al\\.,? \\Q2017\\E", "shortCiteRegEx": "Loos et al\\.", "year": 2017}, {"title": "Convolutional neural networks over tree structures for programming language processing", "author": ["Mou", "Lili", "Li", "Ge", "Zhang", "Lu", "Wang", "Tao", "Jin", "Zhi"], "venue": "In AAAI,", "citeRegEx": "Mou et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Mou et al\\.", "year": 2016}, {"title": "Neural programmer: Inducing latent programs with gradient descent", "author": ["Neelakantan", "Arvind", "Le", "Quoc V", "Sutskever", "Ilya"], "venue": "In ICLR,", "citeRegEx": "Neelakantan et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Neelakantan et al\\.", "year": 2015}, {"title": "Symbolic processing in neural networks", "author": ["Neto", "Jo\u00e3o Pedro", "Siegelmann", "Hava T", "Costa", "J F\u00e9lix"], "venue": "Journal of the Brazilian Computer Society,", "citeRegEx": "Neto et al\\.,? \\Q2003\\E", "shortCiteRegEx": "Neto et al\\.", "year": 2003}, {"title": "Learning program embeddings to propagate feedback on student code", "author": ["Piech", "Chris", "Huang", "Jonathan", "Nguyen", "Andy", "Phulsuksombati", "Mike", "Sahami", "Mehran", "Guibas", "Leonidas J"], "venue": "In ICML,", "citeRegEx": "Piech et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Piech et al\\.", "year": 2015}, {"title": "Weight normalization: A simple reparameterization to accelerate training of deep neural networks", "author": ["Salimans", "Tim", "Kingma", "Diederik P"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Salimans et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Salimans et al\\.", "year": 2016}, {"title": "Neural programming language", "author": ["Siegelmann", "Hava T"], "venue": "In Proceedings of the 12th National Conference on Artificial Intelligence,", "citeRegEx": "Siegelmann and T.,? \\Q1994\\E", "shortCiteRegEx": "Siegelmann and T.", "year": 1994}, {"title": "Practical Bayesian optimization of machine learning algorithms", "author": ["Snoek", "Jasper", "Larochelle", "Hugo", "Adams", "Ryan P"], "venue": "In NIPS,", "citeRegEx": "Snoek et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Snoek et al\\.", "year": 2012}, {"title": "Semi-supervised recursive autoencoders for predicting sentiment distributions", "author": ["Socher", "Richard", "Pennington", "Jeffrey", "Huang", "Eric H", "Ng", "Andrew Y", "Manning", "Christopher D"], "venue": "In EMNLP,", "citeRegEx": "Socher et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Socher et al\\.", "year": 2011}, {"title": "Semantic compositionality through recursive matrix-vector spaces", "author": ["Socher", "Richard", "Huval", "Brody", "Manning", "Christopher D", "Ng", "Andrew Y"], "venue": "In EMNLP,", "citeRegEx": "Socher et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Socher et al\\.", "year": 2012}, {"title": "Recursive deep models for semantic compositionality over a sentiment treebank", "author": ["Socher", "Richard", "Perelygin", "Alex", "Wu", "Jean Y", "Chuang", "Jason", "Manning", "Christopher D", "Ng", "Andrew Y", "Potts", "Christopher"], "venue": "In EMNLP,", "citeRegEx": "Socher et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Socher et al\\.", "year": 2013}, {"title": "Learning to discover efficient mathematical identities", "author": ["Zaremba", "Wojciech", "Kurach", "Karol", "Fergus", "Rob"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Zaremba et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Zaremba et al\\.", "year": 2014}], "referenceMentions": [{"referenceID": 0, "context": "However, apart from some notable exceptions (Alemi et al., 2016; Loos et al., 2017; Zaremba et al., 2014), this area has received relatively little attention in machine learning.", "startOffset": 44, "endOffset": 105}, {"referenceID": 9, "context": "However, apart from some notable exceptions (Alemi et al., 2016; Loos et al., 2017; Zaremba et al., 2014), this area has received relatively little attention in machine learning.", "startOffset": 44, "endOffset": 105}, {"referenceID": 20, "context": "However, apart from some notable exceptions (Alemi et al., 2016; Loos et al., 2017; Zaremba et al., 2014), this area has received relatively little attention in machine learning.", "startOffset": 44, "endOffset": 105}, {"referenceID": 18, "context": "They use recursive neural networks (TREENN)1 (Socher et al., 2012) for modeling homogenous, single-variable polynomial expressions.", "startOffset": 45, "endOffset": 66}, {"referenceID": 17, "context": "Our work in similar in spirit to the work of Zaremba et al. (2014), who focus on learning expression representations to aid the search for computationally efficient identities.", "startOffset": 45, "endOffset": 67}, {"referenceID": 3, "context": "Equivalence is a stronger property than similarity, which has been the focus of previous work in neural network learning (Chopra et al., 2005), since equivalence is additionally a transitive relationship.", "startOffset": 121, "endOffset": 142}, {"referenceID": 18, "context": "To allow our representations to be compositional, we employ the general framework of recursive neural networks (TREENN) (Socher et al., 2012; 2013), in our case operating on tree structures of the syntactic parse of a formula.", "startOffset": 120, "endOffset": 147}, {"referenceID": 19, "context": "Traditional TREENNs (Socher et al., 2013) define LOOKUPLEAFEMBEDDING as a simple lookup operation within a matrix of embeddings and COMBINE as a single-layer neural network.", "startOffset": 20, "endOffset": 41}, {"referenceID": 19, "context": "Traditional TREENNs (Socher et al., 2013) use a single-layer neural network at each tree node.", "startOffset": 20, "endOffset": 41}, {"referenceID": 17, "context": "Although our SUBEXPAE may seem similar to the recursive autoencoders of Socher et al. (2011), it differs in two major ways.", "startOffset": 72, "endOffset": 93}, {"referenceID": 3, "context": "Instead of the supervised objective that we propose, an alternative option for training EQNET would be a Siamese objective (Chopra et al., 2005) that learns about similarities (rather than equivalence) between expressions.", "startOffset": 123, "endOffset": 144}, {"referenceID": 20, "context": "For the polynomial domain, we also generated ONEV-POLY datasets, which are polynomials over a single variable, since they are similar to the setting considered by Zaremba et al. (2014) \u2014 although ONEV-POLY is still a little more general because it is not restricted to homogeneous polynomials.", "startOffset": 163, "endOffset": 185}, {"referenceID": 2, "context": "GRU refers to the token-level gated recurrent unit encoder of Bahdanau et al. (2015) that encodes the token-sequence of an expression into a distributed representation.", "startOffset": 62, "endOffset": 85}, {"referenceID": 2, "context": "GRU refers to the token-level gated recurrent unit encoder of Bahdanau et al. (2015) that encodes the token-sequence of an expression into a distributed representation. Stack-augmented RNN refers to the work of Joulin & Mikolov (2015) which was used to learn algorithmic patterns and uses a stack as a memory and operates on the expression tokens.", "startOffset": 62, "endOffset": 235}, {"referenceID": 2, "context": "GRU refers to the token-level gated recurrent unit encoder of Bahdanau et al. (2015) that encodes the token-sequence of an expression into a distributed representation. Stack-augmented RNN refers to the work of Joulin & Mikolov (2015) which was used to learn algorithmic patterns and uses a stack as a memory and operates on the expression tokens. We also include two recursive neural networks (TREENN). The 1layer TREENN which is the original TREENN also used by Zaremba et al. (2014). We also include a 2-layer TREENN, where COMBINE is a classic two-layer MLP without residual connections.", "startOffset": 62, "endOffset": 486}, {"referenceID": 16, "context": "We tune the hyperparameters of all models using Bayesian optimization (Snoek et al., 2012) on a boolean dataset with 5 variables and maximum tree size of 7 (not shown in Table 1) using the average k-NN (k = 1, .", "startOffset": 70, "endOffset": 90}, {"referenceID": 20, "context": "Although Zaremba et al. (2014) consider a different problem to us, they use data similar to the ONEV-POLY datasets with a traditional TREENN architecture.", "startOffset": 9, "endOffset": 31}, {"referenceID": 17, "context": "TREENN refers to Socher et al. (2012).", "startOffset": 17, "endOffset": 38}, {"referenceID": 18, "context": "Recursive neural networks (TREENN) (Socher et al., 2012; 2013) have been successfully used in NLP with multiple applications.", "startOffset": 35, "endOffset": 62}, {"referenceID": 17, "context": "EQNET\u2019s SUBEXPAE may resemble recursive autoencoders (Socher et al., 2011) but differs in form and function, encoding the whole parent-children tuple to force a clustering behavior.", "startOffset": 53, "endOffset": 74}, {"referenceID": 14, "context": "Recursive neural networks (TREENN) (Socher et al., 2012; 2013) have been successfully used in NLP with multiple applications. Socher et al. (2012) show that TREENNs can learn to compute the values of some simple propositional statements.", "startOffset": 36, "endOffset": 147}, {"referenceID": 9, "context": "Mou et al. (2016) design tree convolutional networks to classify code into student submission tasks.", "startOffset": 0, "endOffset": 18}, {"referenceID": 9, "context": "Mou et al. (2016) design tree convolutional networks to classify code into student submission tasks. Although they learn representations of the student tasks, these representations capture task-specific syntactic features rather than code semantics. Piech et al. (2015) also learn distributed matrix representations of student code submissions.", "startOffset": 0, "endOffset": 270}, {"referenceID": 1, "context": "Allamanis et al. (2016) learn variable-sized representations of source code snippets to summarize them with a short function-like name but aim learn summarization features in code rather than representations of symbolic expression equivalence.", "startOffset": 0, "endOffset": 24}, {"referenceID": 6, "context": "Our work is also related to recent work on neural network architectures that learn controllers/programs (Gruau et al., 1995; Graves et al., 2014; Joulin & Mikolov, 2015; Grefenstette et al., 2015; Dyer et al., 2015; Reed & de Freitas, 2016; Neelakantan et al., 2015; Kaiser & Sutskever, 2016).", "startOffset": 104, "endOffset": 292}, {"referenceID": 5, "context": "Our work is also related to recent work on neural network architectures that learn controllers/programs (Gruau et al., 1995; Graves et al., 2014; Joulin & Mikolov, 2015; Grefenstette et al., 2015; Dyer et al., 2015; Reed & de Freitas, 2016; Neelakantan et al., 2015; Kaiser & Sutskever, 2016).", "startOffset": 104, "endOffset": 292}, {"referenceID": 4, "context": "Our work is also related to recent work on neural network architectures that learn controllers/programs (Gruau et al., 1995; Graves et al., 2014; Joulin & Mikolov, 2015; Grefenstette et al., 2015; Dyer et al., 2015; Reed & de Freitas, 2016; Neelakantan et al., 2015; Kaiser & Sutskever, 2016).", "startOffset": 104, "endOffset": 292}, {"referenceID": 11, "context": "Our work is also related to recent work on neural network architectures that learn controllers/programs (Gruau et al., 1995; Graves et al., 2014; Joulin & Mikolov, 2015; Grefenstette et al., 2015; Dyer et al., 2015; Reed & de Freitas, 2016; Neelakantan et al., 2015; Kaiser & Sutskever, 2016).", "startOffset": 104, "endOffset": 292}, {"referenceID": 15, "context": "More closely related is the work of Zaremba et al. (2014) who use a TREENN to guide the search for more efficient mathematical identities, limited to homogeneous singlevariable polynomial expressions.", "startOffset": 36, "endOffset": 58}, {"referenceID": 0, "context": "Alemi et al. (2016) use RNNs and convolutional neural networks to detect features within mathematical expressions to speed the search for premise selection in automated theorem proving but do not explicitly account for semantic equivalence.", "startOffset": 0, "endOffset": 20}], "year": 2017, "abstractText": "Combining abstract, symbolic reasoning with continuous neural reasoning is a grand challenge of representation learning. As a step in this direction, we propose a new architecture, called neural equivalence networks, for the problem of learning continuous semantic representations of algebraic and logical expressions. These networks are trained to represent semantic equivalence, even of expressions that are syntactically very different. The challenge is that semantic representations must be computed in a syntax-directed manner, because semantics is compositional, but at the same time, small changes in syntax can lead to very large changes in semantics, which can be difficult for continuous neural architectures. We perform an exhaustive evaluation on the task of checking equivalence on a highly diverse class of symbolic algebraic and boolean expression types, showing that our model significantly outperforms existing architectures.", "creator": "LaTeX with hyperref package"}}}