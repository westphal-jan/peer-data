{"id": "1611.01427", "review": {"conference": "iclr", "VERSION": "v1", "DATE_OF_SUBMISSION": "4-Nov-2016", "title": "Sparsely-Connected Neural Networks: Towards Efficient VLSI Implementation of Deep Neural Networks", "abstract": "Recently deep neural networks have received considerable attention due to their ability to extract and represent high-level abstractions in data sets. Deep neural networks such as fully-connected and convolutional neural networks have shown excellent performance on a wide range of recognition and classification tasks. However, their hardware implementations currently suffer from large silicon area and high power consumption due to the their high degree of complexity. The power/energy consumption of neural networks is dominated by memory accesses, the majority of which occur in fully-connected networks. In fact, they contain most of the deep neural network parameters. In this paper, we propose sparsely-connected networks, by showing that the number of connections in fully-connected networks can be reduced by up to 90% while improving the accuracy performance on three popular datasets (MNIST, CIFAR10 and SVHN). We then propose an efficient hardware architecture based on linear-feedback shift registers to reduce the memory requirements of the proposed sparsely-connected networks. The proposed architecture can save up to 90% of memory compared to the conventional implementations of fully-connected neural networks. Moreover, implementation results show up to 84% reduction in the energy consumption of a single neuron of the proposed sparsely-connected networks compared to a single neuron of fully-connected neural networks.", "histories": [["v1", "Fri, 4 Nov 2016 15:47:32 GMT  (879kb,D)", "http://arxiv.org/abs/1611.01427v1", "13 pages, 3 figures"], ["v2", "Fri, 17 Mar 2017 15:52:44 GMT  (860kb,D)", "http://arxiv.org/abs/1611.01427v2", "14 pages, 3 figures"], ["v3", "Thu, 30 Mar 2017 19:51:47 GMT  (1775kb,D)", "http://arxiv.org/abs/1611.01427v3", "Published as a conference paper at ICLR 2017"]], "COMMENTS": "13 pages, 3 figures", "reviews": [], "SUBJECTS": "cs.NE cs.LG", "authors": ["arash ardakani", "carlo condo", "warren j gross"], "accepted": true, "id": "1611.01427"}, "pdf": {"name": "1611.01427.pdf", "metadata": {"source": "CRF", "title": "SPARSELY-CONNECTED NEURAL NETWORKS: TO-", "authors": ["Arash Ardakani", "Carlo Condo", "Warren J. Gross"], "emails": ["arash.ardakani@mail.mcgill.ca,", "carlo.condo@mail.mcgill.ca,", "warren.gross@mcgill.ca"], "sections": [{"heading": "1 INTRODUCTION", "text": "In recent years, it has been shown that these two are a very complex constellation, in which people are able to identify themselves in their entirety. (...) In recent years, it has been shown that people are able to identify themselves in their entirety. (...) In recent years, it has been shown that the constellation in its entirety is very different. (...) In recent years, the constellation in its entirety has changed. (...) The constellation in its entirety has changed. (...) The constellation in its entirety has changed. (...) The constellation in its entirety has changed. \"(...)\" The constellation in its entirety has changed. \"..................................................\" \"The constellation.\" The constellation. \"The constellation.\" The constellation. \"The constellation.\" The constellation. \"The constellation in its entirety has changed.\" The constellation in its entirety has changed. \"(...) The constellation in its entirety has changed.\" The constellation in its entirety has changed. \"(...)\" The constellation in its entirety has changed. \""}, {"heading": "2 PRELIMINARIES", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "2.1 DEEP NEURAL NETWORKS", "text": "They are used in many current image and voice applications to perform complex tasks such as recognition or classification. DNNs are trained through an initial phase known as the learning phase, which uses data to prepare the DNN for the task that will follow in the inference phase. Two subcategories of DNNs that are widely used in recognition and recognition tasks are Convolutionary Neural Networks (CNNs) and RNNNNs (Han et al. (2016). Due to the reuse of parameters in conditional layers, they are well studied and can be efficiently implemented with customized hardware platforms (Chen et al. (2016); Shafiee et al. (2016a); Chen et al. (2016)). Parameter reuse in conditional layers that are widely used in RNNNNs, such as long-term memory and as part of CNNs, largely requires a controlled number of algorithms."}, {"heading": "2.2 TOWARDS HARDWARE IMPLEMENTATION OF DNNS", "text": "Most of these parameters (more than 96%) are located in fully connected layers, which are widely used in classification algorithms. Despite their very good classification performance, they require large amounts of memory to store the numerous parameters, and most of these parameters are found in fully connected layers. (Han et al) It has been shown that the total energy of DNNs is dominated by the storage activities required to store the numerous parameters."}, {"heading": "3 SPARSELY-CONNECTED NEURAL NETWORKS", "text": "(1), (2), (2), (2), (2), (2), (2), (2), (2), (2), (2), (2), (2), (2), (2), (2), (2), (2), (2), (2), (2), (2), (2), (2), (2), (2), (2), (2), (2), (2), (2), (2), (2), (2), (2), (2), (2), (2), (2), (2), (2), (2), (2), (2), (2), (2), (2), (2), (2), (2), (2), (2), (2), (2), (2), (2), (2), (2), (1), (2), (2), (2), (2), (2), (2), (2), (2), (2), (2), (2), (2), (2), (2), (2), (2), (2), (2), (2), (2), (2), (2), (2), (2), (2), (2), (2), (2), (2), (2), (2), (2), (2), (, (2), (, (2), (, (2), (2), (2), (2), (), (, (2), (, (2), (, (, (), (), (2), (, (, (), (, (2), (, (), (2), (, (, (, (, (), (), (), (, (, (), (, (, (2), (, (), (), (, (), (, (, (), (, (,.), (,"}, {"heading": "4 EXPERIMENTAL RESULTS", "text": "We validated the effectiveness of the proposed sparsely networked network and its training algorithm using three data sets: MNIST (LeCun & Cortes (2010)), CIFAR10 (Krizhevsky (2009)) and SVHN (Netzer et al. (2011)) using the Theano library (Team (2016)) in Python."}, {"heading": "4.1 EXPERIMENTAL RESULTS ON MNIST", "text": "In this context, it should be noted that this is a very complex matter."}, {"heading": "4.2 EXPERIMENTAL RESULTS ON CIFAR10", "text": "The CIFAR10 data set consists of a total of 60,000 32 x 32 RGB images. Similar to MNIST, we divide the images into 40,000, 10,000 and 10,000 training, validation and test data sets, respectively. As our model, we use a wavy network of {128-128-256-256-512-512} channels for six wavy / pool layers and two wavy layers, each with 1024 nodes, followed by a classification layer. This architecture is inspired by VGNet (Simonyan & Zisserman (2014) and was also used in (Courbariaux et al. (2015))."}, {"heading": "4.3 EXPERIMENTAL RESULTS ON SVHN", "text": "The SVHN dataset contains 32 x 32 RGB images (600 000 images for training and about 26 000 images for testing) of house numbers and 6 000 images are separated from the training part for validation. Similar to the CIFAR10 case, we use a Convolutionary Network consisting of {128-128-128-256-256-512-512} channels for six folding / pooling layers and two 1024 fully connected layers followed by a classification layer. Hinge loss is used as a cost function with batch normalization and batch size of 50. Table 4 summarizes the accuracy performance of using the proposed sparsely connected network in the Convolutionary Network model compared to the hardware-friendly binarized and ternarized models. Despite the few parameters offered by the proposed sparsely connected network, it also provides the most modern results in terms of accuracy."}, {"heading": "4.4 COMPARISON WITH THE STATE OF THE ART", "text": "The proposed sparsely networked network was compared with other networks in the literature in terms of the misclassification rate in Table 5. In Section 4.1 to 4.3, we used the binarization / ternarization algorithm to train our models in the learning phase while using precise floating-point weights during the test run (i.e. in the inference phase).The first part of Table 5 applies the same technique, while in the second part, we use binarized / ternarized weights also during the test run. Therefore, we use a deterministic method introduced in (Courbariaux et al. (2015)) to perform the test run with binarized / ternarized weights. Weights are obtained as follows: Wb = {1 if W \u2265 0 -1 otherwise, Wt = 1 if W \u2265 1 3 0 otherwise-1, if W \u2264 -1 3, if W \u2264 -3, where Wb and Wt binarized and modularized weights are similar in mathematical power."}, {"heading": "5 VLSI IMPLEMENTATION OF SPARSELY-CONNECTED NEURAL NETWORKS", "text": "This year it is as far as never before in the history of the Federal Republic of Germany."}, {"heading": "6 CONCLUSION", "text": "DNNs are capable of solving complex tasks: their ability to do so depends on the number of neurons and their connections. Fully connected layers in DNNs contain more than 96% of the total neural network parameters, prompting developers to use off-chip memory that has a limited bandwidth and consumes large amounts of energy. In this paper, we proposed sparsely connected networks and their training algorithm to significantly reduce the storage requirements of DNNs. The sparseness level of the proposed network can be matched by an SNG implemented using an LFSR unit and a comparator. We used the proposed sparsely connected network instead of fully connected networks in a VGG-like network to three commonly used datasets: We achieved better results with up to 90% fewer connections than the state of the art. In addition, our simulation results confirm that the proposed network can be used as a regulator rather than a single neural network, so that we can completely avoid an overarmament of a single architecture with 65% less energy."}], "references": [{"title": "TrueNorth: design and tool flow of a 65 mW 1 million neuron programmable neurosynaptic chip", "author": ["F. Akopyan", "J. Sawada", "A. Cassidy", "R. Alvarez-Icaza", "J. Arthur", "P. Merolla", "N. Imam", "Y. Nakamura", "P. Datta", "G.J. Nam", "B. Taba", "M. Beakes", "B. Brezzo", "J.B. Kuang", "R. Manohar", "W.P. Risk", "B. Jackson", "D.S. Modha"], "venue": "IEEE Transactions on Computer-Aided Design of Integrated Circuits and Systems,", "citeRegEx": "Akopyan et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Akopyan et al\\.", "year": 2015}, {"title": "Structured pruning of deep convolutional neural networks", "author": ["Sajid Anwar", "Kyuyeon Hwang", "Wonyong Sung"], "venue": "CoRR, abs/1512.08571,", "citeRegEx": "Anwar et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Anwar et al\\.", "year": 2015}, {"title": "VLSI implementation of deep neural network using integral stochastic computing", "author": ["Arash Ardakani", "Fran\u00e7ois Leduc-Primeau", "Naoya Onizawa", "Takahiro Hanyu", "Warren J. Gross"], "venue": "CoRR, abs/1509.08972,", "citeRegEx": "Ardakani et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Ardakani et al\\.", "year": 2015}, {"title": "Subdominant dense clusters allow for simple learning and high computational performance in neural networks with discrete synapses", "author": ["Carlo Baldassi", "Alessandro Ingrosso", "Carlo Lucibello", "Luca Saglietti", "Riccardo Zecchina"], "venue": null, "citeRegEx": "Baldassi et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Baldassi et al\\.", "year": 2015}, {"title": "Origami: a convolutional network accelerator", "author": ["Lukas Cavigelli", "David Gschwend", "Christoph Mayer", "Samuel Willi", "Beat Muheim", "Luca Benini"], "venue": "CoRR, abs/1512.04295,", "citeRegEx": "Cavigelli et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Cavigelli et al\\.", "year": 2015}, {"title": "Eyeriss: a spatial architecture for energy-efficient dataflow for convolutional neural networks", "author": ["Yu-Hsin Chen", "Joel Emer", "Vivienne Sze"], "venue": "In Proceedings of the 43rd International Symposium on Computer Architecture,", "citeRegEx": "Chen et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Chen et al\\.", "year": 2016}, {"title": "Eyeriss: An Energy-Efficient Reconfigurable Accelerator for Deep Convolutional Neural Networks", "author": ["Yu-Hsin Chen", "Tushar Krishna", "Joel Emer", "Vivienne Sze"], "venue": "In IEEE International Solid-State Circuits Conference,", "citeRegEx": "Chen et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Chen et al\\.", "year": 2016}, {"title": "Training binary multilayer neural networks for image classification using expectation backpropagation", "author": ["Zhiyong Cheng", "Daniel Soudry", "Zexi Mao", "Zhen-zhong Lan"], "venue": "CoRR, abs/1503.03562,", "citeRegEx": "Cheng et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Cheng et al\\.", "year": 2015}, {"title": "BinaryNet: Training deep neural networks with weights and activations constrained to +1 or -1", "author": ["Matthieu Courbariaux", "Yoshua Bengio"], "venue": null, "citeRegEx": "Courbariaux and Bengio.,? \\Q2016\\E", "shortCiteRegEx": "Courbariaux and Bengio.", "year": 2016}, {"title": "BinaryConnect: Training deep neural networks with binary weights during propagations", "author": ["Matthieu Courbariaux", "Yoshua Bengio", "Jean-Pierre David"], "venue": "CoRR, abs/1511.00363,", "citeRegEx": "Courbariaux et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Courbariaux et al\\.", "year": 2015}, {"title": "Stochastic Computing Systems, pp. 37\u2013172", "author": ["B.R. Gaines"], "venue": null, "citeRegEx": "Gaines.,? \\Q1969\\E", "shortCiteRegEx": "Gaines.", "year": 1969}, {"title": "Deep learning. Book in preparation for MIT Press, 2016", "author": ["Ian Goodfellow", "Yoshua Bengio", "Aaron Courville"], "venue": "URL http://www.deeplearningbook.org", "citeRegEx": "Goodfellow et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Goodfellow et al\\.", "year": 2016}, {"title": "EIE: Efficient inference engine on compressed deep neural network", "author": ["S. Han", "X. Liu", "H. Mao", "J. Pu", "A. Pedram", "M.A. Horowitz", "W.J. Dally"], "venue": "In 2016 ACM/IEEE 43rd Annual International Symposium on Computer Architecture (ISCA),", "citeRegEx": "Han et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Han et al\\.", "year": 2016}, {"title": "Deep compression: compressing deep neural network with pruning, trained quantization and huffman", "author": ["Song Han", "Huizi Mao", "William J. Dally"], "venue": "coding. CoRR,", "citeRegEx": "Han et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Han et al\\.", "year": 2015}, {"title": "computing\u2019s energy problem (and what we can do about it)", "author": ["Mark Horowitz"], "venue": "IEEE International Solid-State Circuits Conference Digest of Technical Papers (ISSCC),", "citeRegEx": "Horowitz.,? \\Q2014\\E", "shortCiteRegEx": "Horowitz.", "year": 2014}, {"title": "Fixed-point feedforward deep neural network design using weights -1, 0, and 1", "author": ["K. Hwang", "W. Sung"], "venue": "In 2014 IEEE Workshop on Signal Processing Systems (SiPS),", "citeRegEx": "Hwang and Sung.,? \\Q2014\\E", "shortCiteRegEx": "Hwang and Sung.", "year": 2014}, {"title": "Batch normalization: accelerating deep network training by reducing internal covariate", "author": ["Sergey Ioffe", "Christian Szegedy"], "venue": "shift. CoRR,", "citeRegEx": "Ioffe and Szegedy.,? \\Q2015\\E", "shortCiteRegEx": "Ioffe and Szegedy.", "year": 2015}, {"title": "Learning multiple layers of features from tiny images", "author": ["Alex Krizhevsky"], "venue": "Technical report,", "citeRegEx": "Krizhevsky.,? \\Q2009\\E", "shortCiteRegEx": "Krizhevsky.", "year": 2009}, {"title": "Imagenet classification with deep convolutional neural networks", "author": ["Alex Krizhevsky", "Ilya Sutskever", "Geoffrey E. Hinton"], "venue": "Advances in Neural Information Processing Systems", "citeRegEx": "Krizhevsky et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Krizhevsky et al\\.", "year": 2012}, {"title": "Backpropagation applied to handwritten zip code recognition", "author": ["Y. LeCun", "B. Boser", "J.S. Denker", "D. Henderson", "R.E. Howard", "W. Hubbard", "L.D. Jackel"], "venue": "Neural Comput.,", "citeRegEx": "LeCun et al\\.,? \\Q1989\\E", "shortCiteRegEx": "LeCun et al\\.", "year": 1989}, {"title": "URL http://yann", "author": ["Yann LeCun", "Corinna Cortes. MNIST handwritten digit database."], "venue": "lecun.com/exdb/mnist/.", "citeRegEx": "LeCun and database.,? 2010", "shortCiteRegEx": "LeCun and database.", "year": 2010}, {"title": "Gradient-based learning applied to document recognition", "author": ["Yann Lecun", "Lon Bottou", "Yoshua Bengio", "Patrick Haffner"], "venue": "In Proceedings of the IEEE,", "citeRegEx": "Lecun et al\\.,? \\Q1998\\E", "shortCiteRegEx": "Lecun et al\\.", "year": 1998}, {"title": "Generalizing pooling functions in convolutional neural networks: mixed", "author": ["Chen-Yu Lee", "Patrick W. Gallagher", "Zhuowen Tu"], "venue": "gated, and tree. CoRR,", "citeRegEx": "Lee et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Lee et al\\.", "year": 2015}, {"title": "Neural networks with few multiplications", "author": ["Zhouhan Lin", "Matthieu Courbariaux", "Roland Memisevic", "Yoshua Bengio"], "venue": "CoRR, abs/1510.03009,", "citeRegEx": "Lin et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Lin et al\\.", "year": 2015}, {"title": "Fpga implementation of an image recognition system based on tiny neural networks and on-line reconfiguration", "author": ["F. Moreno", "J. Alarcon", "R. Salvador", "T. Riesgo"], "venue": "In Industrial Electronics,", "citeRegEx": "Moreno et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Moreno et al\\.", "year": 2008}, {"title": "Rectified linear units improve restricted boltzmann machines", "author": ["Vinod Nair", "Geoffrey E. Hinton"], "venue": "In Johannes Frnkranz and Thorsten Joachims (eds.), Proceedings of the 27th International Conference on Machine Learning", "citeRegEx": "Nair and Hinton.,? \\Q2010\\E", "shortCiteRegEx": "Nair and Hinton.", "year": 2010}, {"title": "Reading digits in natural images with unsupervised feature learning", "author": ["Yuval Netzer", "Tao Wang", "Adam Coates", "Alessandro Bissacco", "Bo Wu", "Andrew Y. Ng"], "venue": "In NIPS Workshop on Deep Learning and Unsupervised Feature Learning", "citeRegEx": "Netzer et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Netzer et al\\.", "year": 2011}, {"title": "ISAAC: a convolutional neural network accelerator with in-situ analog arithmetic in crossbars", "author": ["A. Shafiee", "A. Nag", "N. Muralimanohar", "R. Balasubramonian", "J.P. Strachan", "M. Hu", "R.S. Williams", "V. Srikumar"], "venue": "In 2016 ACM/IEEE 43rd Annual International Symposium on Computer Architecture (ISCA),", "citeRegEx": "Shafiee et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Shafiee et al\\.", "year": 2016}, {"title": "StochasticNet: forming deep neural networks via stochastic connectivity", "author": ["M.J. Shafiee", "P. Siva", "A. Wong"], "venue": "IEEE Access,", "citeRegEx": "Shafiee et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Shafiee et al\\.", "year": 2016}, {"title": "Very deep convolutional networks for large-scale image recognition", "author": ["Karen Simonyan", "Andrew Zisserman"], "venue": "CoRR, abs/1409.1556,", "citeRegEx": "Simonyan and Zisserman.,? \\Q2014\\E", "shortCiteRegEx": "Simonyan and Zisserman.", "year": 2014}, {"title": "Stochastic computing can improve upon digital spiking neural networks", "author": ["Sean C. Smithson", "Kaushik Boga", "Arash Ardakani", "Brett H. Meyer", "Warren J. Gross"], "venue": "In 2016 IEEE Workshop on Signal Processing Systems (SiPS),", "citeRegEx": "Smithson et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Smithson et al\\.", "year": 2016}, {"title": "Going deeper with convolutions", "author": ["C. Szegedy", "Wei Liu", "Yangqing Jia", "P. Sermanet", "S. Reed", "D. Anguelov", "D. Erhan", "V. Vanhoucke", "A. Rabinovich"], "venue": "In 2015 IEEE Conference on Computer Vision and Pattern Recognition (CVPR),", "citeRegEx": "Szegedy et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Szegedy et al\\.", "year": 2015}, {"title": "Deep learning using support vector machines", "author": ["Yichuan Tang"], "venue": "CoRR, abs/1306.0239,", "citeRegEx": "Tang.,? \\Q2013\\E", "shortCiteRegEx": "Tang.", "year": 2013}, {"title": "Visualizing and understanding convolutional networks", "author": ["Matthew D. Zeiler", "Rob Fergus"], "venue": "CoRR, abs/1311.2901,", "citeRegEx": "Zeiler and Fergus.,? \\Q2013\\E", "shortCiteRegEx": "Zeiler and Fergus.", "year": 2013}], "referenceMentions": [{"referenceID": 15, "context": "Deep neural networks (DNNs) have shown remarkable performance in extracting and representing high-level abstractions in complex data (Lecun et al. (2015)).", "startOffset": 134, "endOffset": 154}, {"referenceID": 14, "context": "DNNs rely on multiple layers of interconnected neurons and parameters to solve complex tasks, such as image recognition and classification (Krizhevsky et al. (2012)).", "startOffset": 140, "endOffset": 165}, {"referenceID": 11, "context": "Therefore, research efforts have been conducted towards more efficient implementations of DNNs (Han et al. (2016)).", "startOffset": 96, "endOffset": 114}, {"referenceID": 11, "context": "Therefore, research efforts have been conducted towards more efficient implementations of DNNs (Han et al. (2016)). In the past few years, the parallel nature of DNNs has led to the use of graphical processing units (GPUs) to execute neural networks tasks (Han et al. (2015)).", "startOffset": 96, "endOffset": 275}, {"referenceID": 4, "context": "However, their large latency and power consumption have pushed researchers towards application-specific integrated circuits (ASICs) for hardware implementations (Cavigelli et al. (2015)).", "startOffset": 162, "endOffset": 186}, {"referenceID": 4, "context": "However, their large latency and power consumption have pushed researchers towards application-specific integrated circuits (ASICs) for hardware implementations (Cavigelli et al. (2015)). For instance, in (Han et al. (2016)), it was shown that a DNN implemented with customized hardware can accelerate the classification task by 189\u00d7 and 13\u00d7, while saving 24,000\u00d7 and 3,400\u00d7 energy compared to CPU (Intel i7-5930k) and GPU (GeForce TITAN X), respectively.", "startOffset": 162, "endOffset": 224}, {"referenceID": 4, "context": "However, their large latency and power consumption have pushed researchers towards application-specific integrated circuits (ASICs) for hardware implementations (Cavigelli et al. (2015)). For instance, in (Han et al. (2016)), it was shown that a DNN implemented with customized hardware can accelerate the classification task by 189\u00d7 and 13\u00d7, while saving 24,000\u00d7 and 3,400\u00d7 energy compared to CPU (Intel i7-5930k) and GPU (GeForce TITAN X), respectively. Convolutional layers in DNNs are used to extract high level abstractions and features of data. In such layers, the connectivity between neurons follows a pattern inspired by the organization of the animal visual cortex. It was shown that the computation in the visual cortex can mathematically be described by a convolution operation (LeCun et al. (1989)).", "startOffset": 162, "endOffset": 811}, {"referenceID": 11, "context": "In (Courbariaux & Bengio (2016); Horowitz (2014); Han et al.", "startOffset": 33, "endOffset": 49}, {"referenceID": 11, "context": "In (Courbariaux & Bengio (2016); Horowitz (2014); Han et al. (2016)), it was shown that the power/energy consumption of DNNs is dominated by memory accesses.", "startOffset": 50, "endOffset": 68}, {"referenceID": 11, "context": "In (Courbariaux & Bengio (2016); Horowitz (2014); Han et al. (2016)), it was shown that the power/energy consumption of DNNs is dominated by memory accesses. Fully-connected layers, which are widely used in recurrent neural networks (RNNs) and adopted in many state-of-the-art neural network architectures (Krizhevsky et al. (2012); Simonyan & Zisserman (2014); Zeiler & Fergus (2013); Szegedy et al.", "startOffset": 50, "endOffset": 332}, {"referenceID": 11, "context": "In (Courbariaux & Bengio (2016); Horowitz (2014); Han et al. (2016)), it was shown that the power/energy consumption of DNNs is dominated by memory accesses. Fully-connected layers, which are widely used in recurrent neural networks (RNNs) and adopted in many state-of-the-art neural network architectures (Krizhevsky et al. (2012); Simonyan & Zisserman (2014); Zeiler & Fergus (2013); Szegedy et al.", "startOffset": 50, "endOffset": 361}, {"referenceID": 11, "context": "In (Courbariaux & Bengio (2016); Horowitz (2014); Han et al. (2016)), it was shown that the power/energy consumption of DNNs is dominated by memory accesses. Fully-connected layers, which are widely used in recurrent neural networks (RNNs) and adopted in many state-of-the-art neural network architectures (Krizhevsky et al. (2012); Simonyan & Zisserman (2014); Zeiler & Fergus (2013); Szegedy et al.", "startOffset": 50, "endOffset": 385}, {"referenceID": 11, "context": "In (Courbariaux & Bengio (2016); Horowitz (2014); Han et al. (2016)), it was shown that the power/energy consumption of DNNs is dominated by memory accesses. Fully-connected layers, which are widely used in recurrent neural networks (RNNs) and adopted in many state-of-the-art neural network architectures (Krizhevsky et al. (2012); Simonyan & Zisserman (2014); Zeiler & Fergus (2013); Szegedy et al. (2015); Lecun et al.", "startOffset": 50, "endOffset": 408}, {"referenceID": 11, "context": "In (Courbariaux & Bengio (2016); Horowitz (2014); Han et al. (2016)), it was shown that the power/energy consumption of DNNs is dominated by memory accesses. Fully-connected layers, which are widely used in recurrent neural networks (RNNs) and adopted in many state-of-the-art neural network architectures (Krizhevsky et al. (2012); Simonyan & Zisserman (2014); Zeiler & Fergus (2013); Szegedy et al. (2015); Lecun et al. (1998)), independently or as a part of convolutional neural networks, contain most of the weights of a DNN.", "startOffset": 50, "endOffset": 429}, {"referenceID": 11, "context": "In (Courbariaux & Bengio (2016); Horowitz (2014); Han et al. (2016)), it was shown that the power/energy consumption of DNNs is dominated by memory accesses. Fully-connected layers, which are widely used in recurrent neural networks (RNNs) and adopted in many state-of-the-art neural network architectures (Krizhevsky et al. (2012); Simonyan & Zisserman (2014); Zeiler & Fergus (2013); Szegedy et al. (2015); Lecun et al. (1998)), independently or as a part of convolutional neural networks, contain most of the weights of a DNN. For instance, the first fully-connected layer of VGGNet (Simonyan & Zisserman (2014)), which is composed of 13 convolution layers and three fully-connected layers, contains 100M weights out of a total of 140M.", "startOffset": 50, "endOffset": 615}, {"referenceID": 11, "context": "In (Courbariaux & Bengio (2016); Horowitz (2014); Han et al. (2016)), it was shown that the power/energy consumption of DNNs is dominated by memory accesses. Fully-connected layers, which are widely used in recurrent neural networks (RNNs) and adopted in many state-of-the-art neural network architectures (Krizhevsky et al. (2012); Simonyan & Zisserman (2014); Zeiler & Fergus (2013); Szegedy et al. (2015); Lecun et al. (1998)), independently or as a part of convolutional neural networks, contain most of the weights of a DNN. For instance, the first fully-connected layer of VGGNet (Simonyan & Zisserman (2014)), which is composed of 13 convolution layers and three fully-connected layers, contains 100M weights out of a total of 140M. Such large storage requirements in fully-connected networks result in copious power/energy consumption. To overcome the aforementioned issue, a pruning technique was first introduced in (Han et al. (2015)) to reduce the memory required by DNN architectures for mobile applications.", "startOffset": 50, "endOffset": 945}, {"referenceID": 11, "context": "In (Courbariaux & Bengio (2016); Horowitz (2014); Han et al. (2016)), it was shown that the power/energy consumption of DNNs is dominated by memory accesses. Fully-connected layers, which are widely used in recurrent neural networks (RNNs) and adopted in many state-of-the-art neural network architectures (Krizhevsky et al. (2012); Simonyan & Zisserman (2014); Zeiler & Fergus (2013); Szegedy et al. (2015); Lecun et al. (1998)), independently or as a part of convolutional neural networks, contain most of the weights of a DNN. For instance, the first fully-connected layer of VGGNet (Simonyan & Zisserman (2014)), which is composed of 13 convolution layers and three fully-connected layers, contains 100M weights out of a total of 140M. Such large storage requirements in fully-connected networks result in copious power/energy consumption. To overcome the aforementioned issue, a pruning technique was first introduced in (Han et al. (2015)) to reduce the memory required by DNN architectures for mobile applications. However, it makes use of an additional training stage, while information addresses identifying the pruned connections still need to be stored in a memory. More recently, several works have focused on the binarization and ternarization of the weights of DNNs (Courbariaux & Bengio (2016); Courbariaux et al.", "startOffset": 50, "endOffset": 1309}, {"referenceID": 9, "context": "More recently, several works have focused on the binarization and ternarization of the weights of DNNs (Courbariaux & Bengio (2016); Courbariaux et al. (2015); Lin et al.", "startOffset": 133, "endOffset": 159}, {"referenceID": 9, "context": "More recently, several works have focused on the binarization and ternarization of the weights of DNNs (Courbariaux & Bengio (2016); Courbariaux et al. (2015); Lin et al. (2015); Kim & Smaragdis (2016)).", "startOffset": 133, "endOffset": 178}, {"referenceID": 9, "context": "More recently, several works have focused on the binarization and ternarization of the weights of DNNs (Courbariaux & Bengio (2016); Courbariaux et al. (2015); Lin et al. (2015); Kim & Smaragdis (2016)).", "startOffset": 133, "endOffset": 202}, {"referenceID": 9, "context": "More recently, several works have focused on the binarization and ternarization of the weights of DNNs (Courbariaux & Bengio (2016); Courbariaux et al. (2015); Lin et al. (2015); Kim & Smaragdis (2016)). While these approaches reduce weight quantization and thus the memory width, the number of weights is unchanged. In (Shafiee et al. (2016b)), an alternative deep network connectivity named StochasticNet and inspired from the brain synaptic connection between neurons was explored on low-power CPUs.", "startOffset": 133, "endOffset": 344}, {"referenceID": 9, "context": "Two subcategories of DNNs which are widely used in detection and recognition tasks are convolutional neural networks (CNNs) and RNNs (Han et al. (2016)).", "startOffset": 134, "endOffset": 152}, {"referenceID": 5, "context": "Due to parameter reuse in convolutional layers, they are well-studied and can be efficiently implemented with customized hardware platforms (Chen et al. (2016); Shafiee et al.", "startOffset": 141, "endOffset": 160}, {"referenceID": 5, "context": "Due to parameter reuse in convolutional layers, they are well-studied and can be efficiently implemented with customized hardware platforms (Chen et al. (2016); Shafiee et al. (2016a); Chen et al.", "startOffset": 141, "endOffset": 184}, {"referenceID": 5, "context": "Due to parameter reuse in convolutional layers, they are well-studied and can be efficiently implemented with customized hardware platforms (Chen et al. (2016); Shafiee et al. (2016a); Chen et al. (2016)).", "startOffset": 141, "endOffset": 204}, {"referenceID": 5, "context": "Due to parameter reuse in convolutional layers, they are well-studied and can be efficiently implemented with customized hardware platforms (Chen et al. (2016); Shafiee et al. (2016a); Chen et al. (2016)). On the other hand, fully-connected layers, which are widely used in RNNs like long short-term memories and as a part of CNNs, require a large number of parameters to be stored in memories. DNNs are mostly trained by the backpropagation algorithm in conjunction with stochastic gradient descent (SGD) optimization method (Goodfellow et al. (2016)).", "startOffset": 141, "endOffset": 552}, {"referenceID": 5, "context": "Due to parameter reuse in convolutional layers, they are well-studied and can be efficiently implemented with customized hardware platforms (Chen et al. (2016); Shafiee et al. (2016a); Chen et al. (2016)). On the other hand, fully-connected layers, which are widely used in RNNs like long short-term memories and as a part of CNNs, require a large number of parameters to be stored in memories. DNNs are mostly trained by the backpropagation algorithm in conjunction with stochastic gradient descent (SGD) optimization method (Goodfellow et al. (2016)). This algorithm computes the gradient of a cost function C with respect to all the weights in all the layers. A common choice for the cost function is using the modified hinge loss introduced in (Tang (2013)).", "startOffset": 141, "endOffset": 761}, {"referenceID": 5, "context": "Due to parameter reuse in convolutional layers, they are well-studied and can be efficiently implemented with customized hardware platforms (Chen et al. (2016); Shafiee et al. (2016a); Chen et al. (2016)). On the other hand, fully-connected layers, which are widely used in RNNs like long short-term memories and as a part of CNNs, require a large number of parameters to be stored in memories. DNNs are mostly trained by the backpropagation algorithm in conjunction with stochastic gradient descent (SGD) optimization method (Goodfellow et al. (2016)). This algorithm computes the gradient of a cost function C with respect to all the weights in all the layers. A common choice for the cost function is using the modified hinge loss introduced in (Tang (2013)). The obtained errors are then backward propagated through the layers to update the weights in an attempt to minimize the cost function. Instead of using a whole dataset to update parameters, data are first divided in mini-batches and parameters are updated using each mini-batch several times to speed up the convergence of the training algorithm. The weight updating speed is controlled by a learning rate \u03b7. Batch normalization is also commonly used to regularize each mini-batch of data (Ioffe & Szegedy (2015)): it speeds up the training process by allowing the use of a bigger \u03b7.", "startOffset": 141, "endOffset": 1276}, {"referenceID": 12, "context": "AlexNet (Krizhevsky et al. (2012)) and VGGNet (Simonyan & Zisserman (2014)) are two models comprising convolutional layers followed by some fully-connected layers, which are widely used in classification algorithms.", "startOffset": 9, "endOffset": 34}, {"referenceID": 12, "context": "AlexNet (Krizhevsky et al. (2012)) and VGGNet (Simonyan & Zisserman (2014)) are two models comprising convolutional layers followed by some fully-connected layers, which are widely used in classification algorithms.", "startOffset": 9, "endOffset": 75}, {"referenceID": 9, "context": "In (Han et al. (2016)), it was shown that the total energy of DNNs is dominated by the required memory accesses.", "startOffset": 4, "endOffset": 22}, {"referenceID": 0, "context": "In (Akopyan et al. (2015)), the spiking neural network based on stochastic computing (Smithson et al.", "startOffset": 4, "endOffset": 26}, {"referenceID": 0, "context": "In (Akopyan et al. (2015)), the spiking neural network based on stochastic computing (Smithson et al. (2016)) was introduced, where 1-bit calculations are performed throughout the whole architecture.", "startOffset": 4, "endOffset": 109}, {"referenceID": 0, "context": "In (Akopyan et al. (2015)), the spiking neural network based on stochastic computing (Smithson et al. (2016)) was introduced, where 1-bit calculations are performed throughout the whole architecture. In (Ardakani et al. (2015)), integral stochastic computing was used to reduce the computation latency, showing that stochastic computing can consume less energy than conventional binary radix implementations.", "startOffset": 4, "endOffset": 227}, {"referenceID": 0, "context": "In (Akopyan et al. (2015)), the spiking neural network based on stochastic computing (Smithson et al. (2016)) was introduced, where 1-bit calculations are performed throughout the whole architecture. In (Ardakani et al. (2015)), integral stochastic computing was used to reduce the computation latency, showing that stochastic computing can consume less energy than conventional binary radix implementations. However, both works do not manage to reduce the DNN memory requirements. Network pruning, compression and weight sharing have been proposed in (Han et al. (2016)), together with weight matrix sparsification and compression.", "startOffset": 4, "endOffset": 571}, {"referenceID": 0, "context": "In (Akopyan et al. (2015)), the spiking neural network based on stochastic computing (Smithson et al. (2016)) was introduced, where 1-bit calculations are performed throughout the whole architecture. In (Ardakani et al. (2015)), integral stochastic computing was used to reduce the computation latency, showing that stochastic computing can consume less energy than conventional binary radix implementations. However, both works do not manage to reduce the DNN memory requirements. Network pruning, compression and weight sharing have been proposed in (Han et al. (2016)), together with weight matrix sparsification and compression. However, additional indexes denoting the pruned connections are required to be stored along with the compressed weight matrices. In (Han et al. (2015)), it was shown that the number of indexes are almost the same as the number of non-zero elements of weight matrices, thus increasing the word length of the required memories.", "startOffset": 4, "endOffset": 784}, {"referenceID": 0, "context": "In (Akopyan et al. (2015)), the spiking neural network based on stochastic computing (Smithson et al. (2016)) was introduced, where 1-bit calculations are performed throughout the whole architecture. In (Ardakani et al. (2015)), integral stochastic computing was used to reduce the computation latency, showing that stochastic computing can consume less energy than conventional binary radix implementations. However, both works do not manage to reduce the DNN memory requirements. Network pruning, compression and weight sharing have been proposed in (Han et al. (2016)), together with weight matrix sparsification and compression. However, additional indexes denoting the pruned connections are required to be stored along with the compressed weight matrices. In (Han et al. (2015)), it was shown that the number of indexes are almost the same as the number of non-zero elements of weight matrices, thus increasing the word length of the required memories. Moreover, the encoding and compression techniques require inverse computations to obtain decoded and decompressed weights, and introduce additional hardware complexity for hardware implementation compared to the conventional computational architectures. Other pruning techniques presented in literature such as (Anwar et al. (2015)) try to reduce the memory required to store the pruned locations by introducing a structured sparsity in DNNs.", "startOffset": 4, "endOffset": 1291}, {"referenceID": 10, "context": "(3) We propose the use of LFSRs to form each column of M , similar to the approach used in stochastic computing to generate a binary stream (Gaines (1969)).", "startOffset": 141, "endOffset": 155}, {"referenceID": 17, "context": "We have validated the effectiveness of the proposed sparsely-connected network and its training algorithm on three datasets: MNIST (LeCun & Cortes (2010)), CIFAR10 (Krizhevsky (2009)) and SVHN (Netzer et al.", "startOffset": 165, "endOffset": 183}, {"referenceID": 17, "context": "We have validated the effectiveness of the proposed sparsely-connected network and its training algorithm on three datasets: MNIST (LeCun & Cortes (2010)), CIFAR10 (Krizhevsky (2009)) and SVHN (Netzer et al. (2011)) using the Theano library (Team (2016)) in Python.", "startOffset": 165, "endOffset": 215}, {"referenceID": 17, "context": "We have validated the effectiveness of the proposed sparsely-connected network and its training algorithm on three datasets: MNIST (LeCun & Cortes (2010)), CIFAR10 (Krizhevsky (2009)) and SVHN (Netzer et al. (2011)) using the Theano library (Team (2016)) in Python.", "startOffset": 165, "endOffset": 254}, {"referenceID": 9, "context": "Recently, BinaryConnect and TernaryConnect neural networks have outperformed the state-of-theart on different datasets (Courbariaux et al. (2015); Lin et al.", "startOffset": 120, "endOffset": 146}, {"referenceID": 9, "context": "Recently, BinaryConnect and TernaryConnect neural networks have outperformed the state-of-theart on different datasets (Courbariaux et al. (2015); Lin et al. (2015)).", "startOffset": 120, "endOffset": 165}, {"referenceID": 9, "context": "Recently, BinaryConnect and TernaryConnect neural networks have outperformed the state-of-theart on different datasets (Courbariaux et al. (2015); Lin et al. (2015)). In BinaryConnect, weights are represented with either -1 or 1, whereas they can be -1, 0 or 1 in TernaryConnect. These networks have emerged to facilitate hardware implementations of neural networks by reducing the memory requirements and removing multiplications. We applied our training method to BinaryConnect and TernaryConnect training algorithms: the obtained results are provided in Table 2. The source Python codes used for comparison are the same used in (Courbariaux et al. (2015); Lin et al.", "startOffset": 120, "endOffset": 658}, {"referenceID": 9, "context": "Recently, BinaryConnect and TernaryConnect neural networks have outperformed the state-of-theart on different datasets (Courbariaux et al. (2015); Lin et al. (2015)). In BinaryConnect, weights are represented with either -1 or 1, whereas they can be -1, 0 or 1 in TernaryConnect. These networks have emerged to facilitate hardware implementations of neural networks by reducing the memory requirements and removing multiplications. We applied our training method to BinaryConnect and TernaryConnect training algorithms: the obtained results are provided in Table 2. The source Python codes used for comparison are the same used in (Courbariaux et al. (2015); Lin et al. (2015)), available online (Lin et al.", "startOffset": 120, "endOffset": 677}, {"referenceID": 9, "context": "Recently, BinaryConnect and TernaryConnect neural networks have outperformed the state-of-theart on different datasets (Courbariaux et al. (2015); Lin et al. (2015)). In BinaryConnect, weights are represented with either -1 or 1, whereas they can be -1, 0 or 1 in TernaryConnect. These networks have emerged to facilitate hardware implementations of neural networks by reducing the memory requirements and removing multiplications. We applied our training method to BinaryConnect and TernaryConnect training algorithms: the obtained results are provided in Table 2. The source Python codes used for comparison are the same used in (Courbariaux et al. (2015); Lin et al. (2015)), available online (Lin et al. (2015)).", "startOffset": 120, "endOffset": 715}, {"referenceID": 9, "context": "Recently, BinaryConnect and TernaryConnect neural networks have outperformed the state-of-theart on different datasets (Courbariaux et al. (2015); Lin et al. (2015)). In BinaryConnect, weights are represented with either -1 or 1, whereas they can be -1, 0 or 1 in TernaryConnect. These networks have emerged to facilitate hardware implementations of neural networks by reducing the memory requirements and removing multiplications. We applied our training method to BinaryConnect and TernaryConnect training algorithms: the obtained results are provided in Table 2. The source Python codes used for comparison are the same used in (Courbariaux et al. (2015); Lin et al. (2015)), available online (Lin et al. (2015)). The simulation results show that up to 70% and 80% of connections can be dropped by the proposed method from BinaryConnfewer connectionect and TernaryConnect networks without any compromise in performance, respectively. Moreover, the binarized and ternarized sparsely-connected 50% improve the accuracy compared to the conventional binarized and ternarized fully-connected networks. It is worth specifying that we only used the binarized/ternarized algorithm during the learning phase, and we used single-precision floating-point weights during the test run in Section 4, similar to the approach used in (Lin et al. (2015)).", "startOffset": 120, "endOffset": 1340}, {"referenceID": 9, "context": "33 2913290 BinaryConnecta (Courbariaux et al. (2015)) 1.", "startOffset": 27, "endOffset": 53}, {"referenceID": 9, "context": "33 2913290 BinaryConnecta (Courbariaux et al. (2015)) 1.23 2913290 TernaryConnectb (Lin et al. (2015)) 1.", "startOffset": 27, "endOffset": 102}, {"referenceID": 9, "context": "05 5523184 BinaryConnect a (Courbariaux et al. (2015)) 9.", "startOffset": 28, "endOffset": 54}, {"referenceID": 9, "context": "05 5523184 BinaryConnect a (Courbariaux et al. (2015)) 9.91 14025866 TernaryConnect b (Lin et al. (2015)) 9.", "startOffset": 28, "endOffset": 105}, {"referenceID": 9, "context": "This architecture is inspired by VGGNet (Simonyan & Zisserman (2014)) and was also used in (Courbariaux et al. (2015)).", "startOffset": 92, "endOffset": 118}, {"referenceID": 9, "context": "734615 14025866 BinaryConnect a (Courbariaux et al. (2015)) 2.", "startOffset": 33, "endOffset": 59}, {"referenceID": 9, "context": "734615 14025866 BinaryConnect a (Courbariaux et al. (2015)) 2.134615 14025866 TernaryConnect b (Lin et al. (2015)) 2.", "startOffset": 33, "endOffset": 114}, {"referenceID": 9, "context": "We thus exploit a deterministic method introduced in (Courbariaux et al. (2015)) to perform the test run using binarized/ternarized weights.", "startOffset": 54, "endOffset": 80}, {"referenceID": 9, "context": "Similar conclusions were also obtained in (Courbariaux et al. (2015)).", "startOffset": 43, "endOffset": 69}, {"referenceID": 3, "context": "40% (Baldassi et al. (2015)) 1.", "startOffset": 5, "endOffset": 28}, {"referenceID": 3, "context": "40% (Baldassi et al. (2015)) 1.35% \u2013 \u2013 BinaryConnect (Courbariaux et al. (2015)) 1.", "startOffset": 5, "endOffset": 80}, {"referenceID": 3, "context": "40% (Baldassi et al. (2015)) 1.35% \u2013 \u2013 BinaryConnect (Courbariaux et al. (2015)) 1.29% 2.30% 9.90% EBP (Cheng et al. (2015)) 2.", "startOffset": 5, "endOffset": 124}, {"referenceID": 3, "context": "40% (Baldassi et al. (2015)) 1.35% \u2013 \u2013 BinaryConnect (Courbariaux et al. (2015)) 1.29% 2.30% 9.90% EBP (Cheng et al. (2015)) 2.2% \u2013 \u2013 Bitwise DNNs (Kim & Smaragdis (2016)) 1.", "startOffset": 5, "endOffset": 171}, {"referenceID": 3, "context": "40% (Baldassi et al. (2015)) 1.35% \u2013 \u2013 BinaryConnect (Courbariaux et al. (2015)) 1.29% 2.30% 9.90% EBP (Cheng et al. (2015)) 2.2% \u2013 \u2013 Bitwise DNNs (Kim & Smaragdis (2016)) 1.33% \u2013 \u2013 (Hwang & Sung (2014)) 1.", "startOffset": 5, "endOffset": 203}, {"referenceID": 3, "context": "40% (Baldassi et al. (2015)) 1.35% \u2013 \u2013 BinaryConnect (Courbariaux et al. (2015)) 1.29% 2.30% 9.90% EBP (Cheng et al. (2015)) 2.2% \u2013 \u2013 Bitwise DNNs (Kim & Smaragdis (2016)) 1.33% \u2013 \u2013 (Hwang & Sung (2014)) 1.45% \u2013 \u2013 Sparsely-Connected + BinaryConnect 1.08% 2.053846% 8.66% Sparsely-Connected + TernaryConnect 0.98% 1.992308% 8.24% Method Single-Precision Floating-Point Weights During Test Run TernaryConnect (Lin et al. (2015)) 1.", "startOffset": 5, "endOffset": 426}, {"referenceID": 3, "context": "40% (Baldassi et al. (2015)) 1.35% \u2013 \u2013 BinaryConnect (Courbariaux et al. (2015)) 1.29% 2.30% 9.90% EBP (Cheng et al. (2015)) 2.2% \u2013 \u2013 Bitwise DNNs (Kim & Smaragdis (2016)) 1.33% \u2013 \u2013 (Hwang & Sung (2014)) 1.45% \u2013 \u2013 Sparsely-Connected + BinaryConnect 1.08% 2.053846% 8.66% Sparsely-Connected + TernaryConnect 0.98% 1.992308% 8.24% Method Single-Precision Floating-Point Weights During Test Run TernaryConnect (Lin et al. (2015)) 1.15% 2.42% 12.01% Maxout Networks (Goodfellow et al. (2013)) 0.", "startOffset": 5, "endOffset": 488}, {"referenceID": 3, "context": "40% (Baldassi et al. (2015)) 1.35% \u2013 \u2013 BinaryConnect (Courbariaux et al. (2015)) 1.29% 2.30% 9.90% EBP (Cheng et al. (2015)) 2.2% \u2013 \u2013 Bitwise DNNs (Kim & Smaragdis (2016)) 1.33% \u2013 \u2013 (Hwang & Sung (2014)) 1.45% \u2013 \u2013 Sparsely-Connected + BinaryConnect 1.08% 2.053846% 8.66% Sparsely-Connected + TernaryConnect 0.98% 1.992308% 8.24% Method Single-Precision Floating-Point Weights During Test Run TernaryConnect (Lin et al. (2015)) 1.15% 2.42% 12.01% Maxout Networks (Goodfellow et al. (2013)) 0.94% 2.47% 11.68% Network in Network (Lin et al. (2013)) \u2013 2.", "startOffset": 5, "endOffset": 546}, {"referenceID": 3, "context": "40% (Baldassi et al. (2015)) 1.35% \u2013 \u2013 BinaryConnect (Courbariaux et al. (2015)) 1.29% 2.30% 9.90% EBP (Cheng et al. (2015)) 2.2% \u2013 \u2013 Bitwise DNNs (Kim & Smaragdis (2016)) 1.33% \u2013 \u2013 (Hwang & Sung (2014)) 1.45% \u2013 \u2013 Sparsely-Connected + BinaryConnect 1.08% 2.053846% 8.66% Sparsely-Connected + TernaryConnect 0.98% 1.992308% 8.24% Method Single-Precision Floating-Point Weights During Test Run TernaryConnect (Lin et al. (2015)) 1.15% 2.42% 12.01% Maxout Networks (Goodfellow et al. (2013)) 0.94% 2.47% 11.68% Network in Network (Lin et al. (2013)) \u2013 2.35% 10.41% Gated pooling (Lee et al. (2015)) \u2013 1.", "startOffset": 5, "endOffset": 595}, {"referenceID": 24, "context": "In this approach, each neuron performs its computations serially, and a certain number of neurons are instantiated in parallel (Moreno et al. (2008)).", "startOffset": 128, "endOffset": 149}], "year": 2017, "abstractText": "Recently deep neural networks have received considerable attention due to their ability to extract and represent high-level abstractions in data sets. Deep neural networks such as fully-connected and convolutional neural networks have shown excellent performance on a wide range of recognition and classification tasks. However, their hardware implementations currently suffer from large silicon area and high power consumption due to the their high degree of complexity. The power/energy consumption of neural networks is dominated by memory accesses, the majority of which occur in fully-connected networks. In fact, they contain most of the deep neural network parameters. In this paper, we propose sparsely-connected networks, by showing that the number of connections in fullyconnected networks can be reduced by up to 90% while improving the accuracy performance on three popular datasets (MNIST, CIFAR10 and SVHN). We then propose an efficient hardware architecture based on linear-feedback shift registers to reduce the memory requirements of the proposed sparsely-connected networks. The proposed architecture can save up to 90% of memory compared to the conventional implementations of fully-connected neural networks. Moreover, implementation results show up to 84% reduction in the energy consumption of a single neuron of the proposed sparsely-connected networks compared to a single neuron of fully-connected neural networks.", "creator": "LaTeX with hyperref package"}}}