{"id": "1611.01487", "review": {"conference": "ACL", "VERSION": "v1", "DATE_OF_SUBMISSION": "4-Nov-2016", "title": "Morphological Inflection Generation with Hard Monotonic Attention", "abstract": "We present a supervised sequence to sequence transduction model with a hard attention mechanism which combines the more traditional statistical alignment methods with the power of recurrent neural networks. We evaluate the model on the task of morphological inflection generation and show that it provides state of the art results in various setups compared to the previous neural and non-neural approaches. Eventually we present an analysis of the learned representations for both hard and soft attention models, shedding light on the features such models extract in order to solve the task.", "histories": [["v1", "Fri, 4 Nov 2016 18:42:47 GMT  (2156kb,D)", "http://arxiv.org/abs/1611.01487v1", "Under review as a conference paper at ICLR 2017"], ["v2", "Thu, 8 Dec 2016 12:33:44 GMT  (2145kb,D)", "http://arxiv.org/abs/1611.01487v2", "Under review as a conference paper at ICLR 2017"], ["v3", "Tue, 11 Apr 2017 08:51:27 GMT  (591kb,D)", "http://arxiv.org/abs/1611.01487v3", "Accepted as a long paper in ACL 2017"]], "COMMENTS": "Under review as a conference paper at ICLR 2017", "reviews": [], "SUBJECTS": "cs.CL", "authors": ["roee aharoni", "yoav goldberg"], "accepted": true, "id": "1611.01487"}, "pdf": {"name": "1611.01487.pdf", "metadata": {"source": "CRF", "title": "HARD MONOTONIC ATTENTION", "authors": ["Roee Aharoni"], "emails": ["roee.aharoni@gmail.com", "yoav.goldberg@gmail.com"], "sections": [{"heading": "1 INTRODUCTION", "text": "The neural sequence for sequence transduction has become a prominent approach for natural language processing tasks such as morphological agility processes, which do not always require optimal training (Faruqui et al., 2016) and automatic summary (Rush et al., 2015). A common way to improve the vanilla encoder decoder framework for sequence tasks is the (soft) attention mechanism (Bahdanau et al., 2014), which allows the decoder to participate in certain elements in the encrypted sequence, the problems in coding very long sequences into a single vector. It has also been shown that the attention mechanism effectively learns alignment between input and initial sequences, which are naturally found in the data, and practically learns to participate in the relevant elements of input."}, {"heading": "2 THE HARD ATTENTION MODEL", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "2.1 MOTIVATION", "text": "Imagine a machine that has only readable random access to the encoding of the input sequence, and a single pointer that determines the current read location. We can then model sequence transmission as a series of write operations and pointer motion operations. In the case where alignment between sequences is monotonous, the pointer movement can be controlled by a single \"one step forward\" operation (step) that we add to the output vocabulary. We implement this behavior using a neural encoder decoder network, with a control mechanism that determines at each step of the decoder whether it is time to predict an output symbol or direct the attention pointer to the next element of the encoded input."}, {"heading": "2.2 MODEL DEFINITION", "text": "In the prediction time, we look for the output sequence y1: m \u03a3 \u043d y, for which: y1: m = arg max y \u2032 p (y \u2032 | x, f) (1) Where: x \u03a3 \u0445 x is the input sequence and: f = {f1,..., fm} is a series of features that affect the transmission task (for example, in the task of inflection generation, these would be the desired morphosyntactic features of the output sequence). Since we want to force a monotonous alignment between input and output with our model, we look instead for a sequence of actions: s1: q ds, where: \u0441s = \u0448y-y-y-step}. This sequence is the step / write action sequence required to go from x1: n to y1: m, according to the monotonic alignment between them. In this case, we define: s1: q = arg max s \u00b2 p (\u2032 s), then we can describe an xf: n \u00b2 (s)"}, {"heading": "2.3 NETWORK ARCHITECTURE", "text": "Notation We use bold letters for vectors and matrices. We treat LSTM as a parameterized function LSTM\u03b8 (x1... xn), which creates a sequence of input vectors x1... xn to an output vector hn.Encoder For each element in the input sequence: x1: n = x1... xn, we take the corresponding embedding: ex1... exn, where: exi... R2H, where each vector xi = [LSTMforward (x2,... xi), LSTM encoder (Graves & Schmidhuber, 2005), which results in a sequence of vectors: x1: n = x0... xn... R2H, where each vector xi = [LSTMforward (x2,... xi), LSTMbackward (xn, xn \u2212 1... xi).Decoder Once the input sequence is encoded, we feed the decoder NN."}, {"heading": "2.4 TRAINING THE MODEL", "text": "For each example: (x1: n, y1: m, f) in the training data, we should generate a sequence of step and write actions s1: q that must be predicted by the decoder, depending on the alignment between the input and output - the network must participate in all input elements aligned to an output element before writing. Instead, while there is a trend in neural networks to train alignment and decoding together (Bahdanau et al., 2014; Yu et al., 2016), we show that it is worth decoupling these steps and learning the hard alignment before the hand by using them to guide the training of the encoder decoder network and to enable the use of correct alignment for the attention mechanism from the beginning of the training process. To this end, we first perform a sign-level alignment process to align the alignment processes to the training data."}, {"heading": "3 EXPERIMENTS", "text": "We conduct extensive experiments with three previously studied morphological diffraction data sets to evaluate our hard attention model in different environments. In all experiments, we report on the results of the most powerful neural and non-neural baselines previously published on these data sets to our knowledge. Implementation details for the models can be found in the supplementary material of this thesis. The source code for the models is available on github.2."}, {"heading": "3.1 CELEX", "text": "To determine whether our model is fit for purpose, we first evaluate it using a very small data set compiled by Dreyer et al. (2008) from the CELEX database (Baayen et al., 1993), which contains only 500 training examples for each of the four types of inflection: 13SIA \u2192 13SKE, 2PIE \u2192 13PKE, 2PKE \u2192 z and rP \u2192 pA, which we call 13SIA, 2PIE, 2PKE and rP. [3] We compare our model with three competitive baselines that have reported results on this data set: the Can & Protect morphological encoder decoder (MED) (2016b), which is based on the soft attention model by Bahdanau et al. (2014), the Can & Protect morphological encoder decoder (MED) (2016b)."}, {"heading": "3.2 WIKTIONARY", "text": "In order to neutralize the negative effects of very small training sets on the performance of the various learning approaches, we evaluate our model using the data set created by Durrett & DeNero (2013), which contains up to 360k training examples per language. It was created by extracting Finnish, German and Spanish diffraction tables from Wiktionary, which are used to evaluate their system based on string alignments and a semi-CRF sequence classifier with linguistically inspired characteristics. We also used the extension of Nicolai et al. (2015) to include French and Dutch diffraction tables as well. Their1https: / github.com / ryancotterural / sigmorphontime 2https: / github.com / roeeaharoni / morphological-reinflection 3The acronyms stand for: 13SIA = 1st / 3rd person, singular, finite, past; 13SKE = 1st indeplural / 3rd indeural = indeural, 3rd person, present = 13IA-present = 13IA-present; each present-system = 13lIA-present = each; 13SKE = each indefinite, 3rd indeural = 13IA = present, 2impulsive, 2IA = present-present; 13IA-present = each."}, {"heading": "3.3 SIGMORPHON", "text": "In fact, it is as if most people are able to survive themselves by going in search of their own identity. (...) In fact, it is as if most of them are able to identify themselves. (...) It is as if they are able to put themselves in the centre. (...) It is as if they were able to put themselves in the centre. (...) It is as if they were able to put themselves in the centre. (...) It is as if they were able to put themselves in the centre. (...) It is as if they were able to put themselves in the centre. (...) It is as if they put themselves in the centre. (...) It is as if they put themselves in the centre. (...) It is as if they put themselves in the centre. (...) It is as if they put themselves in the centre. (...) It is as if they put themselves in the centre. (...) It is as if they put themselves in the centre."}, {"heading": "5 ANALYSIS", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "5.1 COMPARISON OF LEARNED ALIGNMENTS", "text": "To see if the alignments that our model predicts match the monotonous alignment structure contained in the data, and whether they are more suitable for the task than the alignments found by the soft attention model, we examined alignment predictions of the two models from the CELEX dataset shown in Figure 3. First, we note that the alignments found by the soft attention model are also monotonous and encourage our modeling approach to the task. In addition, we note how our model learns to deal with morphological phenomena such as deletion, as can be seen in the right part of Figure 3 by showing the alignments for the put: put \u2192 insert. This alignment requires that the model deletes the fourth character of the input sequence by reading and writing each character until it reaches the t-sign that is deleted by performing two consecutive step operations, i.e. one must shift the morphological alignment to the corresponding one in this example by displacing the other two phological transformations."}, {"heading": "5.2 LEARNED REPRESENTATIONS ANALYSIS", "text": "When we witness the success of the hard and soft attention models in the sequence transduction sequence, the following questions arise: How do the models manage to learn monotonous alignments? Perhaps the network is learning to encode the sequential position as part of its encoding of an input element? In an attempt to answer these questions, we performed the following analysis: We performed 500 continuous character representations in the context of each model in which each representation is a vector in R200, which is the output of the model's Bi-LSTM encoder. Each vector of this form carries information of a particular character with its context. We perform dimension reductions to reduce these two sets of vectors, each in: R500 x 200 in: R500 x 2 using SVD. We then draw the 2D character in context representations and color them in two ways. First, we color them by the character they represent, with each character having different colors in the alphabet (4)."}, {"heading": "6 RELATED WORK", "text": "Previous approaches to automatic diffraction generation typically used manually constructed finite state transducers (Koskenniemi, 1983; Kaplan & Kay, 1994) that required expert knowledge or machine learning methods for string transduction (Yarowsky & Wicentowski, 2000; Dreyer & Eisner, 2011; Durrett & DeNero, 2013; Hulden et al., 2014; Ahlberg et al., 2015; Nicolai et al., 2015). While these studies achieved high accuracy, they also assumed specific assumptions about the number of possible morphological processes that generate the injection, and required feature engineering via input. More recently, Faruqui et al. (2016) used encoder decoders to generate neural networks for infections. The general idea is to use an encoder decoder network via the characters that convert the input lemma into a vector and decode it."}, {"heading": "7 CONCLUSION", "text": "We presented the hard attention model for learning sequences of monotonous alignment sequences and evaluated it against the task of morphological diffraction generation. The model uses an explicit alignment model learned independently of migratory time, which is used to teach a neural network to perform both alignment and transduction while decrypting it using a hard attention mechanism. We demonstrated that our model performs better or better than more complex soft attention models on well-studied morphological diffraction data sets, represents a new state of the art on the CELEX dataset and the Wiktionary dataset, and surpasses the best system in the task of SIGMORPHON2016 diffraction generation. Future work could include experimenting with various external alignment methods or applying the model to other tasks that require a monotonic alignment and transduction approach such as abstract summary or transcription."}, {"heading": "ACKNOWLEDGMENTS", "text": "This work was supported by the Intel Collaborative Research Institute for Computational Intelligence (ICRI-CI)."}], "references": [{"title": "Paradigm classification in supervised learning of morphology", "author": ["Malin Ahlberg", "Markus Forsberg", "Mans Hulden"], "venue": "The Association for Computational Linguistics,", "citeRegEx": "Ahlberg et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Ahlberg et al\\.", "year": 2015}, {"title": "The {CELEX} lexical data base on {CDROM", "author": ["R Harald Baayen", "Richard Piepenbrock", "Rijn van H"], "venue": null, "citeRegEx": "Baayen et al\\.,? \\Q1993\\E", "shortCiteRegEx": "Baayen et al\\.", "year": 1993}, {"title": "Neural machine translation by jointly learning to align and translate", "author": ["Dzmitry Bahdanau", "Kyunghyun Cho", "Yoshua Bengio"], "venue": "CoRR, abs/1409.0473,", "citeRegEx": "Bahdanau et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Bahdanau et al\\.", "year": 2014}, {"title": "Translating into morphologically rich languages with synthetic phrases", "author": ["Victor Chahuneau", "Eva Schlinger", "Noah A. Smith", "Chris Dyer"], "venue": "In EMNLP, pp. 1677\u20131687", "citeRegEx": "Chahuneau et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Chahuneau et al\\.", "year": 2013}, {"title": "The SIGMORPHON 2016 shared task\u2014morphological reinflection", "author": ["Ryan Cotterell", "Christo Kirov", "John Sylak-Glassman", "David Yarowsky", "Jason Eisner", "Mans Hulden"], "venue": "In Proceedings of the 2016 Meeting of SIGMORPHON, Berlin,", "citeRegEx": "Cotterell et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Cotterell et al\\.", "year": 2016}, {"title": "Discovering morphological paradigms from plain text using a dirichlet process mixture model", "author": ["Markus Dreyer", "Jason Eisner"], "venue": "In EMNLP,", "citeRegEx": "Dreyer and Eisner.,? \\Q2011\\E", "shortCiteRegEx": "Dreyer and Eisner.", "year": 2011}, {"title": "Latent-variable modeling of string transductions with finite-state methods", "author": ["Markus Dreyer", "Jason R Smith", "Jason Eisner"], "venue": "In Proceedings of the conference on empirical methods in natural language processing,", "citeRegEx": "Dreyer et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Dreyer et al\\.", "year": 2008}, {"title": "Supervised learning of complete morphological paradigms", "author": ["Greg Durrett", "John DeNero"], "venue": "In Proceedings of the 2013 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies,", "citeRegEx": "Durrett and DeNero.,? \\Q2013\\E", "shortCiteRegEx": "Durrett and DeNero.", "year": 2013}, {"title": "Parameter estimation for probabilistic finite-state transducers", "author": ["Jason Eisner"], "venue": "In Proceedings of the 40th annual meeting on Association for Computational Linguistics,", "citeRegEx": "Eisner.,? \\Q2002\\E", "shortCiteRegEx": "Eisner.", "year": 2002}, {"title": "Morphological inflection generation using character sequence to sequence learning", "author": ["Manaal Faruqui", "Yulia Tsvetkov", "Graham Neubig", "Chris Dyer"], "venue": "In NAACL HLT", "citeRegEx": "Faruqui et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Faruqui et al\\.", "year": 2016}, {"title": "Factored neural machine translation", "author": ["Mercedes Garc\u0131\u0301a-Mart\u0131\u0301nez", "Lo\u0131\u0308c Barrault", "Fethi Bougares"], "venue": "arXiv preprint arXiv:1609.04621,", "citeRegEx": "Garc\u0131\u0301a.Mart\u0131\u0301nez et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Garc\u0131\u0301a.Mart\u0131\u0301nez et al\\.", "year": 2016}, {"title": "Framewise phoneme classification with bidirectional LSTM and other neural network architectures", "author": ["A. Graves", "J. Schmidhuber"], "venue": "Neural Networks,", "citeRegEx": "Graves and Schmidhuber.,? \\Q2005\\E", "shortCiteRegEx": "Graves and Schmidhuber.", "year": 2005}, {"title": "Semi-supervised learning of morphological paradigms and lexicons", "author": ["Mans Hulden", "Markus Forsberg", "Malin Ahlberg"], "venue": "In Gosse Bouma and Yannick Parmentier 0001 (eds.), EACL,", "citeRegEx": "Hulden et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Hulden et al\\.", "year": 2014}, {"title": "Single-model encoder-decoder with explicit morphological representation for reinflection", "author": ["Katharina Kann", "Hinrich Sch\u00fctze"], "venue": null, "citeRegEx": "Kann and Sch\u00fctze.,? \\Q2016\\E", "shortCiteRegEx": "Kann and Sch\u00fctze.", "year": 2016}, {"title": "Med: The lmu system for the sigmorphon 2016 shared task on morphological reinflection", "author": ["Katharina Kann", "Hinrich Sch\u00fctze"], "venue": null, "citeRegEx": "Kann and Sch\u00fctze.,? \\Q2016\\E", "shortCiteRegEx": "Kann and Sch\u00fctze.", "year": 2016}, {"title": "Regular models of phonological rule systems", "author": ["Ronald M. Kaplan", "Martin Kay"], "venue": "Computational Linguistics,", "citeRegEx": "Kaplan and Kay.,? \\Q1994\\E", "shortCiteRegEx": "Kaplan and Kay.", "year": 1994}, {"title": "Adam: A method for stochastic optimization", "author": ["Diederik Kingma", "Jimmy Ba"], "venue": null, "citeRegEx": "Kingma and Ba.,? \\Q2014\\E", "shortCiteRegEx": "Kingma and Ba.", "year": 2014}, {"title": "Two-level morphology: A general computational model of word-form recognition and production", "author": ["Kimmo Koskenniemi"], "venue": "Technical Report Publication No. 11,", "citeRegEx": "Koskenniemi.,? \\Q1983\\E", "shortCiteRegEx": "Koskenniemi.", "year": 1983}, {"title": "Effective approaches to attentionbased neural machine translation", "author": ["Minh-Thang Luong", "Hieu Pham", "Christopher D Manning"], "venue": "arXiv preprint arXiv:1508.04025,", "citeRegEx": "Luong et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Luong et al\\.", "year": 2015}, {"title": "A rational design for a weighted finite-state transducer library", "author": ["Mehryar Mohri", "Fernando Pereira", "Michael Riley"], "venue": "In International Workshop on Implementing Automata,", "citeRegEx": "Mohri et al\\.,? \\Q1997\\E", "shortCiteRegEx": "Mohri et al\\.", "year": 1997}, {"title": "Inflection generation as discriminative string transduction", "author": ["Garrett Nicolai", "Colin Cherry", "Grzegorz Kondrak"], "venue": "In Proceedings of the 2015 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies,", "citeRegEx": "Nicolai et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Nicolai et al\\.", "year": 2015}, {"title": "Weighting finite-state transductions with neural context", "author": ["Pushpendre Rastogi", "Ryan Cotterell", "Jason Eisner"], "venue": "In Proc. of NAACL,", "citeRegEx": "Rastogi et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Rastogi et al\\.", "year": 2016}, {"title": "A neural attention model for abstractive sentence summarization", "author": ["Alexander M Rush", "Sumit Chopra", "Jason Weston"], "venue": "arXiv preprint arXiv:1509.00685,", "citeRegEx": "Rush et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Rush et al\\.", "year": 2015}, {"title": "Noise-aware character alignment for bootstrapping statistical machine transliteration from bilingual corpora", "author": ["Katsuhito Sudoh", "Shinsuke Mori", "Masaaki Nagata"], "venue": "In EMNLP,", "citeRegEx": "Sudoh et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Sudoh et al\\.", "year": 2013}, {"title": "Minimally supervised morphological analysis by multimodal alignment", "author": ["David Yarowsky", "Richard Wicentowski"], "venue": "In ACL. The Association for Computational Linguistics,", "citeRegEx": "Yarowsky and Wicentowski.,? \\Q2000\\E", "shortCiteRegEx": "Yarowsky and Wicentowski.", "year": 2000}, {"title": "Online segment to segment neural transduction", "author": ["Lei Yu", "Jan Buys", "Phil Blunsom"], "venue": "arXiv preprint arXiv:1609.08194,", "citeRegEx": "Yu et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Yu et al\\.", "year": 2016}], "referenceMentions": [{"referenceID": 9, "context": "Neural sequence to sequence transduction became a prominent approach to natural language processing tasks like morphological inflection generation (Faruqui et al., 2016) and automatic summarization (Rush et al.", "startOffset": 147, "endOffset": 169}, {"referenceID": 22, "context": ", 2016) and automatic summarization (Rush et al., 2015) among others.", "startOffset": 36, "endOffset": 55}, {"referenceID": 2, "context": "A common way to improve the vanilla encoder-decoder framework for sequence to sequence tasks is the (soft) attention mechanism (Bahdanau et al., 2014) which enables the decoder to attend at specific elements in the encoded sequence, overcoming the issues in encoding very long sequences to a single vector.", "startOffset": 127, "endOffset": 150}, {"referenceID": 17, "context": "The more traditional approaches to monotonic sequence transduction were hand engineered finite state transducers (FST) (Koskenniemi, 1983; Kaplan & Kay, 1994) which relied on expert knowledge, or weighted finite state transducers (Mohri et al.", "startOffset": 119, "endOffset": 158}, {"referenceID": 19, "context": "The more traditional approaches to monotonic sequence transduction were hand engineered finite state transducers (FST) (Koskenniemi, 1983; Kaplan & Kay, 1994) which relied on expert knowledge, or weighted finite state transducers (Mohri et al., 1997; Eisner, 2002) which combined expert knowledge with data-driven parameter tuning.", "startOffset": 230, "endOffset": 264}, {"referenceID": 8, "context": "The more traditional approaches to monotonic sequence transduction were hand engineered finite state transducers (FST) (Koskenniemi, 1983; Kaplan & Kay, 1994) which relied on expert knowledge, or weighted finite state transducers (Mohri et al., 1997; Eisner, 2002) which combined expert knowledge with data-driven parameter tuning.", "startOffset": 230, "endOffset": 264}, {"referenceID": 21, "context": "While the FST approaches may work well even on small datasets due to their engineered structure, it may be cumbersome to use them while conditioning on the entire output history as it requires a very large set of states, resulting in a model conditioning only on the last predicted output symbol (Rastogi et al., 2016).", "startOffset": 296, "endOffset": 318}, {"referenceID": 3, "context": "Several studies showed that inflection generation is beneficial for phrase-based machine translation (Chahuneau et al., 2013) and more recently for neural machine translation (Garc\u0131\u0301a-Mart\u0131\u0301nez et al.", "startOffset": 101, "endOffset": 125}, {"referenceID": 10, "context": ", 2013) and more recently for neural machine translation (Garc\u0131\u0301a-Mart\u0131\u0301nez et al., 2016).", "startOffset": 57, "endOffset": 89}, {"referenceID": 6, "context": "We show that while our model is better or on-par with the previous state-of-the-art on the task, it is also performing significantly better for very small training sets, being the first neural model to surpass the performance of a weighted FST model with latent variables specifically tailored for it (Dreyer et al., 2008).", "startOffset": 301, "endOffset": 322}, {"referenceID": 2, "context": "While there is a trend in neural networks to jointly train the alignment and the decoding (Bahdanau et al., 2014; Yu et al., 2016), we instead show that in our case it is worthwhile to decouple these stages and learn the hard alignment before hand, using it to guide the training of the encoder-decoder network and enabling the use of correct alignments for the attention mechanism from the beginning of the training process.", "startOffset": 90, "endOffset": 130}, {"referenceID": 25, "context": "While there is a trend in neural networks to jointly train the alignment and the decoding (Bahdanau et al., 2014; Yu et al., 2016), we instead show that in our case it is worthwhile to decouple these stages and learn the hard alignment before hand, using it to guide the training of the encoder-decoder network and enabling the use of correct alignments for the attention mechanism from the beginning of the training process.", "startOffset": 90, "endOffset": 130}, {"referenceID": 2, "context": "While there is a trend in neural networks to jointly train the alignment and the decoding (Bahdanau et al., 2014; Yu et al., 2016), we instead show that in our case it is worthwhile to decouple these stages and learn the hard alignment before hand, using it to guide the training of the encoder-decoder network and enabling the use of correct alignments for the attention mechanism from the beginning of the training process. For that purpose, we first run a character level alignment process on the training data. We use the character alignment model of Sudoh et al. (2013) which is based on a Chinese Restaurant Process which weights single alignments (characterto-character) in proportion to how many times such an alignment has been seen elsewhere out of all possible alignments.", "startOffset": 91, "endOffset": 575}, {"referenceID": 1, "context": "(2008) from the CELEX database (Baayen et al., 1993).", "startOffset": 31, "endOffset": 52}, {"referenceID": 4, "context": "For this purpose we report exact match accuracy on the German inflection generation dataset compiled by Dreyer et al. (2008) from the CELEX database (Baayen et al.", "startOffset": 104, "endOffset": 125}, {"referenceID": 1, "context": "(2008) from the CELEX database (Baayen et al., 1993). The dataset includes only 500 training examples for each of the four inflection types: 13SIA\u219213SKE, 2PIE\u219213PKE, 2PKE\u2192z, and rP\u2192pA which we refer to as 13SIA, 2PIE, 2PKE and rP, respectively.3 We compare our model to three competitive baselines that reported results on this dataset: the Morphological Encoder-Decoder (MED) of Kann & Sch\u00fctze (2016b), which is based on the soft-attention model of Bahdanau et al.", "startOffset": 32, "endOffset": 403}, {"referenceID": 1, "context": "(2008) from the CELEX database (Baayen et al., 1993). The dataset includes only 500 training examples for each of the four inflection types: 13SIA\u219213SKE, 2PIE\u219213PKE, 2PKE\u2192z, and rP\u2192pA which we refer to as 13SIA, 2PIE, 2PKE and rP, respectively.3 We compare our model to three competitive baselines that reported results on this dataset: the Morphological Encoder-Decoder (MED) of Kann & Sch\u00fctze (2016b), which is based on the soft-attention model of Bahdanau et al. (2014), the neural-weighted FST of Rastogi et al.", "startOffset": 32, "endOffset": 473}, {"referenceID": 1, "context": "(2008) from the CELEX database (Baayen et al., 1993). The dataset includes only 500 training examples for each of the four inflection types: 13SIA\u219213SKE, 2PIE\u219213PKE, 2PKE\u2192z, and rP\u2192pA which we refer to as 13SIA, 2PIE, 2PKE and rP, respectively.3 We compare our model to three competitive baselines that reported results on this dataset: the Morphological Encoder-Decoder (MED) of Kann & Sch\u00fctze (2016b), which is based on the soft-attention model of Bahdanau et al. (2014), the neural-weighted FST of Rastogi et al. (2016) which uses stacked bi-directional LSTM\u2019s to weigh its arcs (WFST), and the model of Dreyer et al.", "startOffset": 32, "endOffset": 523}, {"referenceID": 1, "context": "(2008) from the CELEX database (Baayen et al., 1993). The dataset includes only 500 training examples for each of the four inflection types: 13SIA\u219213SKE, 2PIE\u219213PKE, 2PKE\u2192z, and rP\u2192pA which we refer to as 13SIA, 2PIE, 2PKE and rP, respectively.3 We compare our model to three competitive baselines that reported results on this dataset: the Morphological Encoder-Decoder (MED) of Kann & Sch\u00fctze (2016b), which is based on the soft-attention model of Bahdanau et al. (2014), the neural-weighted FST of Rastogi et al. (2016) which uses stacked bi-directional LSTM\u2019s to weigh its arcs (WFST), and the model of Dreyer et al. (2008) which uses a weighted FST with latent-variables structured particularly for morphological string transduction tasks (LAT).", "startOffset": 32, "endOffset": 628}, {"referenceID": 1, "context": "(2008) from the CELEX database (Baayen et al., 1993). The dataset includes only 500 training examples for each of the four inflection types: 13SIA\u219213SKE, 2PIE\u219213PKE, 2PKE\u2192z, and rP\u2192pA which we refer to as 13SIA, 2PIE, 2PKE and rP, respectively.3 We compare our model to three competitive baselines that reported results on this dataset: the Morphological Encoder-Decoder (MED) of Kann & Sch\u00fctze (2016b), which is based on the soft-attention model of Bahdanau et al. (2014), the neural-weighted FST of Rastogi et al. (2016) which uses stacked bi-directional LSTM\u2019s to weigh its arcs (WFST), and the model of Dreyer et al. (2008) which uses a weighted FST with latent-variables structured particularly for morphological string transduction tasks (LAT). Following previous reports on this dataset, we use the same data splits as Dreyer et al. (2008), dividing the data for each inflection type into five folds, each consisting of 500 training, 1000 development and 1000 test examples.", "startOffset": 32, "endOffset": 847}, {"referenceID": 20, "context": "We also used the expansion made by Nicolai et al. (2015) to include French and Dutch inflections as well.", "startOffset": 35, "endOffset": 57}, {"referenceID": 9, "context": "In addition to those systems we also compare to the results of the recent neural approaches of Faruqui et al. (2016), which did not use an attention mechanism, and Yu et al.", "startOffset": 95, "endOffset": 117}, {"referenceID": 9, "context": "In addition to those systems we also compare to the results of the recent neural approaches of Faruqui et al. (2016), which did not use an attention mechanism, and Yu et al. (2016), which coupled the alignment and transduction tasks, requiring a beam search decoding procedure.", "startOffset": 95, "endOffset": 181}, {"referenceID": 4, "context": "As different languages show different morphological phenomena, we also experiment with how our model copes with this variety using the morphological inflection dataset from the SIGMORPHON2016 shared task (Cotterell et al., 2016).", "startOffset": 204, "endOffset": 228}, {"referenceID": 4, "context": "As different languages show different morphological phenomena, we also experiment with how our model copes with this variety using the morphological inflection dataset from the SIGMORPHON2016 shared task (Cotterell et al., 2016). Here the training data consists of ten languages, with five morphological system types (detailed in Table 3): Russian (RU), German (DE), Spanish (ES), Georgian (GE), Finnish (FI), Turkish (TU), Arabic (AR), Navajo (NA), Hungarian (HU) and Maltese (MA) with roughly 12,800 training and 1600 development examples per language. We compare our model to two soft attention baselines on this dataset: MED (Kann & Sch\u00fctze, 2016a), which was the best participating system in the shared task, and our implementation of the global (soft) attention model presented by Luong et al. (2015).", "startOffset": 205, "endOffset": 807}, {"referenceID": 20, "context": "On the low resource setting (CELEX), our model significantly outperforms both the recent neural models of Kann & Sch\u00fctze (2016b) and Rastogi et al. (2016) and the morphologically aware latent variable model of Dreyer et al.", "startOffset": 133, "endOffset": 155}, {"referenceID": 6, "context": "(2016) and the morphologically aware latent variable model of Dreyer et al. (2008), as detailed in Table 1.", "startOffset": 62, "endOffset": 83}, {"referenceID": 9, "context": "This shows the robustness of our model also with large amounts of training examples, and the advantage the hard attention mechanism provides over the encoder-decoder approach of Faruqui et al. (2016) which does not employ an attention mechanism.", "startOffset": 178, "endOffset": 200}, {"referenceID": 25, "context": "more accurate than the model of Yu et al. (2016), showing the advantage in using independently learned alignments to guide the network from the beginning of the training process.", "startOffset": 32, "endOffset": 49}, {"referenceID": 4, "context": "We explain this by looking at the languages from a linguistic typology point of view, as detailed in Cotterell et al. (2016). Since Russian, German and Spanish employ a suffixing morphology with internal stem changes, they are more suitable for monotonic alignment as the transformations they need to model are the addition of suffixes and changing characters in the stem.", "startOffset": 101, "endOffset": 125}, {"referenceID": 17, "context": "Previous approaches to automatic inflection generation usually make use of manually constructed Finite State Transducers (Koskenniemi, 1983; Kaplan & Kay, 1994), which require expert knowledge, or machine learning methods for string transduction (Yarowsky & Wicentowski, 2000; Dreyer & Eisner, 2011; Durrett & DeNero, 2013; Hulden et al.", "startOffset": 121, "endOffset": 160}, {"referenceID": 12, "context": "Previous approaches to automatic inflection generation usually make use of manually constructed Finite State Transducers (Koskenniemi, 1983; Kaplan & Kay, 1994), which require expert knowledge, or machine learning methods for string transduction (Yarowsky & Wicentowski, 2000; Dreyer & Eisner, 2011; Durrett & DeNero, 2013; Hulden et al., 2014; Ahlberg et al., 2015; Nicolai et al., 2015).", "startOffset": 246, "endOffset": 388}, {"referenceID": 0, "context": "Previous approaches to automatic inflection generation usually make use of manually constructed Finite State Transducers (Koskenniemi, 1983; Kaplan & Kay, 1994), which require expert knowledge, or machine learning methods for string transduction (Yarowsky & Wicentowski, 2000; Dreyer & Eisner, 2011; Durrett & DeNero, 2013; Hulden et al., 2014; Ahlberg et al., 2015; Nicolai et al., 2015).", "startOffset": 246, "endOffset": 388}, {"referenceID": 20, "context": "Previous approaches to automatic inflection generation usually make use of manually constructed Finite State Transducers (Koskenniemi, 1983; Kaplan & Kay, 1994), which require expert knowledge, or machine learning methods for string transduction (Yarowsky & Wicentowski, 2000; Dreyer & Eisner, 2011; Durrett & DeNero, 2013; Hulden et al., 2014; Ahlberg et al., 2015; Nicolai et al., 2015).", "startOffset": 246, "endOffset": 388}, {"referenceID": 2, "context": "Kann & Sch\u00fctze (2016b;a) explored the soft attention model proposed for machine translation (Bahdanau et al., 2014).", "startOffset": 92, "endOffset": 115}, {"referenceID": 4, "context": "This work also gave the best results in the SIGMORPHON 2016 shared task (Cotterell et al., 2016).", "startOffset": 72, "endOffset": 96}, {"referenceID": 21, "context": "Another notable contribution is the work on weighting finite state transducers with neural context (Rastogi et al., 2016).", "startOffset": 99, "endOffset": 121}, {"referenceID": 0, "context": ", 2014; Ahlberg et al., 2015; Nicolai et al., 2015). While these studies achieved high accuracies, they also make specific assumptions about the set of possible morphological processes that create the inflection, and require feature engineering over the input. More recently, Faruqui et al. (2016) used encoder-decoder neural networks for inflection generation.", "startOffset": 8, "endOffset": 298}, {"referenceID": 0, "context": ", 2014; Ahlberg et al., 2015; Nicolai et al., 2015). While these studies achieved high accuracies, they also make specific assumptions about the set of possible morphological processes that create the inflection, and require feature engineering over the input. More recently, Faruqui et al. (2016) used encoder-decoder neural networks for inflection generation. The general idea is to use an encoder-decoder network over characters, that encodes the input lemma into a vector and decodes it one character at a time into the inflected surface word. Kann & Sch\u00fctze (2016b;a) explored the soft attention model proposed for machine translation (Bahdanau et al., 2014). This work also gave the best results in the SIGMORPHON 2016 shared task (Cotterell et al., 2016). Another notable contribution is the work on weighting finite state transducers with neural context (Rastogi et al., 2016). There, the arcs of an FST are scored by optimizing a global loss function over all the possible paths in the state graph while modeling contextual features with Bi-Directional LSTM\u2019s. Another recent work by Yu et al. (2016) introduced an online neural sequence to sequence model that learns to alternate between encoding and decoding segments of the input as it is read.", "startOffset": 8, "endOffset": 1110}], "year": 2017, "abstractText": "We present a supervised sequence to sequence transduction model with a hard attention mechanism which combines the more traditional statistical alignment methods with the power of recurrent neural networks. We evaluate the model on the task of morphological inflection generation and show that it provides state of the art results in various setups compared to the previous neural and non-neural approaches. Eventually we present an analysis of the learned representations for both hard and soft attention models, shedding light on the features such models extract in order to solve the task.", "creator": "LaTeX with hyperref package"}}}