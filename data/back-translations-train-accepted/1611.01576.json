{"id": "1611.01576", "review": {"conference": "iclr", "VERSION": "v1", "DATE_OF_SUBMISSION": "5-Nov-2016", "title": "Quasi-Recurrent Neural Networks", "abstract": "Recurrent neural networks are a powerful tool for modeling sequential data, but the dependence of each timestep's computation on the previous timestep's output limits parallelism and makes RNNs unwieldy for very long sequences. We introduce quasi-recurrent neural networks (QRNNs), an approach to neural sequence modeling that alternates convolutional layers, which apply in parallel across timesteps, and a minimalist recurrent pooling function that applies in parallel across channels. Despite lacking trainable recurrent layers, stacked QRNNs have better predictive accuracy than stacked LSTMs of the same hidden size. Due to their increased parallelism, they are up to 16 times faster at train and test time. Experiments on language modeling, sentiment classification, and character-level neural machine translation demonstrate these advantages and underline the viability of QRNNs as a basic building block for a variety of sequence tasks.", "histories": [["v1", "Sat, 5 Nov 2016 00:31:25 GMT  (353kb,D)", "http://arxiv.org/abs/1611.01576v1", "Submitted to conference track at ICLR 2017"], ["v2", "Mon, 21 Nov 2016 20:52:34 GMT  (353kb,D)", "http://arxiv.org/abs/1611.01576v2", "Submitted to conference track at ICLR 2017"]], "COMMENTS": "Submitted to conference track at ICLR 2017", "reviews": [], "SUBJECTS": "cs.NE cs.AI cs.CL cs.LG", "authors": ["james bradbury", "stephen merity", "caiming xiong", "richard socher"], "accepted": true, "id": "1611.01576"}, "pdf": {"name": "1611.01576.pdf", "metadata": {"source": "CRF", "title": "QUASI-RECURRENT NEURAL NETWORKS", "authors": ["James Bradbury", "Stephen Merity", "Caiming Xiong"], "emails": ["james.bradbury@salesforce.com", "smerity@salesforce.com", "cxiong@salesforce.com", "rsocher@salesforce.com"], "sections": [{"heading": "1 INTRODUCTION", "text": "Recurrent neural networks (RNNs), including gated variants such as long-term short-term memory (LSTM) (Hochreiter & Schmidhuber, 1997), have become the standard model architecture for deep learning approaches to sequence modeling. RNNs repeatedly apply a function with supporting parameters to a hidden state. Recurring layers can also be stacked to increase network depth, representation power, and often accuracy. RNN applications in the natural language domain range from sentence classification (Wang et al., 2015) to word and character level speech modeling (Zaremba et al., 2014)."}, {"heading": "2 MODEL", "text": "There are only two types of subcomponents that allow access to all information. (We have only two types of subcomponents that allow access to all information.) There are only two types of subcomponents that allow access to all information. (We have only two types of subcomponents that allow access to all information.) There are only two types of subcomponents. (We have only two types of subcomponents.) There are only two types of subcomponents that relate to each component. (We have an input sequence X-RT-n of T n-dimensional vectors x1..... xT, the Convolutionary subcomponent of a QRNN performs the constellation with a bank of m-filters that produces a sequence of Z-RRT.) To be useful for tasks that involve predicting the next tokens, the filters do not need to be allowed."}, {"heading": "2.1 VARIANTS", "text": "In fact, it is such that it concerns a way and manner in which people move in the most diverse life worlds of the world, which are reflected in the most diverse life worlds, which are reflected in the most diverse life worlds: in art, in art, in culture, in culture, in culture, in culture, in culture, in culture, in society, in society, in society, in society, in society, in society, in society, in society, in society, in society, in society, in society, in society, in society, in society, in society, in society, in society, in society, in society, in society, in society, in society, in society, in society, in society, in society, in society, in society, in society, in society, in society, in society, in society, in society, in society, in society, in society, in society, in society, in society, in society, in society, in society, in society, in society, in society, in society, in society, in society, in society, in society, in society, in society, in society, in society, in society, in society, in society, in society, in society, in society, in society, in society, in society, in society, in society, in society, in society, in society, in society, in society, in society, in society, in society, in society, in society, in society, in society, in society, in society, in society, in society, in society, in society, in society, in society, in society, in society, in society, in society, in society, in society, in society, in society, in society, in society, in society, in society, in society, in society, in society, in society, in society, in society, in society, in society, in society, in society, in society, in society, in society, in society, in society, in society, in society, in society, in society, in society, in society, in, in society, in society, in society, in society, in society, in society, in society, in society, in society, in society, in society, in the society, in society, in society, in society, in society, in the society, in society, in the world, in the world, in the world, in the world"}, {"heading": "Model Time / Epoch (s) Test Acc (%)", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "3 EXPERIMENTS", "text": "We evaluate the performance of QRNN against three different tasks of natural language: document-level sentiment classification, language modeling, and character-based neural machine translation. Our QRNN models outperform LSTM-based models of the same hidden size in all three tasks and dramatically improve computing speed. Experiments were implemented in Chainer (Tokui et al.)."}, {"heading": "3.1 SENTIMENT CLASSIFICATION", "text": "The data set consists of a balanced sample of 25,000 positive and 25,000 negative ratings, divided into equal-sized tensile and test sets, with an average document length of 231 words (Wang & Manning, 2012).We compare only with other results that do not use additional, unlabeled data (thus excluding, for example, the processing of Miyato et al. (2016).Our best performance on a held development set was achieved with a four-layer, tightly connected QRNN with 256 units per layer and word vectors initialized with 300-dimensional GloVe embedding (Pennington et al., 2014).The failure rate of 0.3 was applied between layers, and we used the L2 regularization of 4 x 10 \u2212 6. The optimization was based on minibatches of 24 examples with RMSprop (Tieleman et al., Note that Q2 layers with a length of 0.0001 = the 0.0001 layer and 0.0001 = the LN-0.0001 layer)."}, {"heading": "3.2 LANGUAGE MODELING", "text": "This year, the number of new entries in the United States is many times higher than the number of new entries in the United States."}, {"heading": "Model Parameters Validation Test", "text": "(2016), which had variation-based dropout layers of 0.2 that were applied repeatedly, and the most powerful variant also used Monte Carlo (MC) dropout layers averaged at a test time of 1000 different masks, which makes execution mathematically expensive. In training on the PTB dataset with an NVIDIA K40 GPU, we found that the QRNN is significantly faster than a standard LSTM, even when compared to the optimized CuDNN-LSTM. Figure 4 shows a breakdown of the time needed for Chainer's standard LSTM, CuDNN-LSTM, and QRNN to perform full forward and backward transmission to a single stack during training of the RNN-LSTM on the PTB."}, {"heading": "3.3 CHARACTER-LEVEL NEURAL MACHINE TRANSLATION", "text": "We evaluate the sequence-to-sequence QRNN architecture described in 2.1 using a sophisticated neural machine translation task, the IWSLT German-English spoken-domain translation, using complete character-level segmentation. This data set consists of 209,772 sentence pairs of parallel training data from transcribed TED and TEDx presentations with an average sentence length of 103 characters for German and 93 characters for English. We remove training sets with more than 300 characters in English or German and use a unified vocabulary of 187 Unicode code points. Our best performance on a development set (TED.tst2013) was achieved with a four-layer QRNN encoder decoder with 320 units per shift, no exposure or L2 regulation, and gradient recalculation to a maximum order of 5 points. Entries were delivered to the encoder layer."}, {"heading": "Model Train Time BLEU (TED.tst2014)", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "4 RELATED WORK", "text": "In this context, it is also worth mentioning that the motivations and constraints described in this work are different from the concepts of \"learnware\" and \"firmware\" discussed in the discussion of \"convolution-like and pooling-like subcomponents.\" Since the use of a fully networked layer for recursive connections violates the limitations of \"strong typing,\" all strongly typed RNN architectures (including the T-RNN, T-LSTM, and T-LSTRNN models) are synonymous."}, {"heading": "5 CONCLUSION", "text": "Intuitively, many aspects of the semantics of long sequences are context-invariant and can be calculated in parallel (e.g., in a revolutionary manner), but some aspects require a long-term context and need to be calculated repeatedly. Many existing neural network architectures either do not exploit the contextual information or do not exploit the parallelism. QRNs use both parallelism and context, and have advantages of both revolutionary and recursive neural networks. QRNNs have better predictive accuracy than LSTM-based models of the same hidden size, although they use fewer parameters and run much faster. Our experiments show that the benefits of speed and accuracy remain consistent across tasks and both at the word and character levels. Extensions to CNNs and RNNNNs are often directly applicable to the QRNN, while the hidden states of the model are easier to interpret than those of other recursive architectures, maintaining their independence over time frames."}, {"heading": "APPENDIX", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "BEAM SEARCH RANKING CRITERION", "text": "The modified log probability ranking criterion that we used in the bar search for translation experiments is: log (Pcand) = T + \u03b1T.... Ttrg + \u03b1Ttrg T \u2211 i = 1 log (p (wi | w1... wi \u2212 1))), (9) where \u03b1 is a length normalization parameter (Wu et al., 2016), wi is the ith output mark and Ttrg is a \"target length\" corresponding to the source sentence length plus five characters. At \u03b1 = 0, this is reduced to the usual bar search with probabilities: log (Pcand) = T \u2211 i = 1 log (p (wi | w1. wi \u2212 1)))), (10) and for \u03b1 = 1, the bar search with probabilities normalized by length (up to the target length): log (Pcand) \u0445 1T \u0445 i = 1 log (p (wi | w1. wi) \u2212 1) can be applied to this need (11)."}], "references": [{"title": "Neural machine translation by jointly learning to align and translate", "author": ["Dzmitry Bahdanau", "Kyunghyun Cho", "Yoshua Bengio"], "venue": "In ICLR,", "citeRegEx": "Bahdanau et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Bahdanau et al\\.", "year": 2015}, {"title": "Strongly-typed recurrent neural networks", "author": ["David Balduzzi", "Muhammad Ghifary"], "venue": "In ICML,", "citeRegEx": "Balduzzi and Ghifary.,? \\Q2016\\E", "shortCiteRegEx": "Balduzzi and Ghifary.", "year": 2016}, {"title": "In Proceedings of the First Conference on Machine Translation, Berlin, Germany", "author": ["James Bradbury", "Richard Socher. MetaMind neural machine translation system for WMT"], "venue": "Association for Computational Linguistics, 2016.", "citeRegEx": "Bradbury and WMT,? 2016", "shortCiteRegEx": "Bradbury and WMT", "year": 2016}, {"title": "A theoretically grounded application of dropout in recurrent neural networks", "author": ["Yarin Gal", "Zoubin Ghahramani"], "venue": "In NIPS,", "citeRegEx": "Gal and Ghahramani.,? \\Q2016\\E", "shortCiteRegEx": "Gal and Ghahramani.", "year": 2016}, {"title": "Long short-term memory", "author": ["Sepp Hochreiter", "J\u00fcrgen Schmidhuber"], "venue": "Neural Computation,", "citeRegEx": "Hochreiter and Schmidhuber.,? \\Q1997\\E", "shortCiteRegEx": "Hochreiter and Schmidhuber.", "year": 1997}, {"title": "Densely connected convolutional networks", "author": ["Gao Huang", "Zhuang Liu", "Kilian Q Weinberger"], "venue": "arXiv preprint arXiv:1608.06993,", "citeRegEx": "Huang et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Huang et al\\.", "year": 2016}, {"title": "Effective use of word order for text categorization with convolutional neural networks", "author": ["Rie Johnson", "Tong Zhang"], "venue": "arXiv preprint arXiv:1412.1058,", "citeRegEx": "Johnson and Zhang.,? \\Q2014\\E", "shortCiteRegEx": "Johnson and Zhang.", "year": 2014}, {"title": "Neural machine translation in linear time", "author": ["Nal Kalchbrenner", "Lasse Espeholt", "Karen Simonyan", "Aaron van den Oord", "Alex Graves", "Koray Kavukcuoglu"], "venue": "arXiv preprint arXiv:1610.10099,", "citeRegEx": "Kalchbrenner et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Kalchbrenner et al\\.", "year": 2016}, {"title": "Character-aware neural language models", "author": ["Yoon Kim", "Yacine Jernite", "David Sontag", "Alexander M. Rush"], "venue": "arXiv preprint arXiv:1508.06615,", "citeRegEx": "Kim et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Kim et al\\.", "year": 2016}, {"title": "Adam: A method for stochastic optimization", "author": ["Diederik Kingma", "Jimmy Ba"], "venue": "arXiv preprint arXiv:1412.6980,", "citeRegEx": "Kingma and Ba.,? \\Q2014\\E", "shortCiteRegEx": "Kingma and Ba.", "year": 2014}, {"title": "ImageNet classification with deep convolutional neural networks", "author": ["Alex Krizhevsky", "Ilya Sutskever", "Geoffrey E Hinton"], "venue": "In NIPS,", "citeRegEx": "Krizhevsky et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Krizhevsky et al\\.", "year": 2012}, {"title": "Zoneout: Regularizing RNNs by Randomly Preserving Hidden Activations", "author": ["David Krueger", "Tegan Maharaj", "J\u00e1nos Kram\u00e1r", "Mohammad Pezeshki", "Nicolas Ballas", "Nan Rosemary Ke", "Anirudh Goyal", "Yoshua Bengio", "Hugo Larochelle", "Aaron Courville"], "venue": "arXiv preprint arXiv:1606.01305,", "citeRegEx": "Krueger et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Krueger et al\\.", "year": 2016}, {"title": "Ask me anything: Dynamic memory networks for natural language processing", "author": ["Ankit Kumar", "Ozan Irsoy", "Peter Ondruska", "Mohit Iyyer", "James Bradbury", "Ishaan Gulrajani", "Victor Zhong", "Romain Paulus", "Richard Socher"], "venue": null, "citeRegEx": "Kumar et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Kumar et al\\.", "year": 2016}, {"title": "Fully character-level neural machine translation without explicit segmentation", "author": ["Jason Lee", "Kyunghyun Cho", "Thomas Hofmann"], "venue": "arXiv preprint arXiv:1610.03017,", "citeRegEx": "Lee et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Lee et al\\.", "year": 2016}, {"title": "A way out of the odyssey: Analyzing and combining recent insights for LSTMs", "author": ["Shayne Longpre", "Sabeek Pradhan", "Caiming Xiong", "Richard Socher"], "venue": null, "citeRegEx": "Longpre et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Longpre et al\\.", "year": 2016}, {"title": "Effective approaches to attention-based neural machine translation", "author": ["M.T. Luong", "H. Pham", "C.D. Manning"], "venue": "In EMNLP,", "citeRegEx": "Luong et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Luong et al\\.", "year": 2015}, {"title": "Multi-dimensional sentiment analysis with learned representations", "author": ["Andrew L Maas", "Andrew Y Ng", "Christopher Potts"], "venue": "Technical report,", "citeRegEx": "Maas et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Maas et al\\.", "year": 2011}, {"title": "Pointer sentinel mixture models", "author": ["Stephen Merity", "Caiming Xiong", "James Bradbury", "Richard Socher"], "venue": "arXiv preprint arXiv:1609.07843,", "citeRegEx": "Merity et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Merity et al\\.", "year": 2016}, {"title": "Ensemble of generative and discriminative techniques for sentiment analysis of movie reviews", "author": ["Gr\u00e9goire Mesnil", "Tomas Mikolov", "Marc\u2019Aurelio Ranzato", "Yoshua Bengio"], "venue": "arXiv preprint arXiv:1412.5335,", "citeRegEx": "Mesnil et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Mesnil et al\\.", "year": 2014}, {"title": "Recurrent neural network based language model", "author": ["Tomas Mikolov", "Martin Karafi\u00e1t", "Luk\u00e1s Burget", "Jan Cernock\u00fd", "Sanjeev Khudanpur"], "venue": "In INTERSPEECH,", "citeRegEx": "Mikolov et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Mikolov et al\\.", "year": 2010}, {"title": "Virtual adversarial training for semi-supervised text classification", "author": ["Takeru Miyato", "Andrew M Dai", "Ian Goodfellow"], "venue": "arXiv preprint arXiv:1605.07725,", "citeRegEx": "Miyato et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Miyato et al\\.", "year": 2016}, {"title": "GloVe: Global vectors for word representation", "author": ["Jeffrey Pennington", "Richard Socher", "Christopher D Manning"], "venue": "In EMNLP,", "citeRegEx": "Pennington et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Pennington et al\\.", "year": 2014}, {"title": "Sequence level training with recurrent neural networks", "author": ["Marc\u2019Aurelio Ranzato", "Sumit Chopra", "Michael Auli", "Wojciech Zaremba"], "venue": "In ICLR,", "citeRegEx": "Ranzato et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Ranzato et al\\.", "year": 2016}, {"title": "Lecture 6.5-rmsprop: Divide the gradient by a running average of its recent magnitude", "author": ["Tijmen Tieleman", "Geoffrey Hinton"], "venue": "COURSERA: Neural Networks for Machine Learning,", "citeRegEx": "Tieleman and Hinton.,? \\Q2012\\E", "shortCiteRegEx": "Tieleman and Hinton.", "year": 2012}, {"title": "Pixel recurrent neural networks", "author": ["Aaron van den Oord", "Nal Kalchbrenner", "Koray Kavukcuoglu"], "venue": "arXiv preprint arXiv:1601.06759,", "citeRegEx": "Oord et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Oord et al\\.", "year": 2016}, {"title": "Baselines and bigrams: Simple, good sentiment and topic classification", "author": ["Sida Wang", "Christopher D Manning"], "venue": "In ACL,", "citeRegEx": "Wang and Manning.,? \\Q2012\\E", "shortCiteRegEx": "Wang and Manning.", "year": 2012}, {"title": "Predicting polarities of tweets by composing word embeddings with long short-term memory", "author": ["Xin Wang", "Yuanchao Liu", "Chengjie Sun", "Baoxun Wang", "Xiaolong Wang"], "venue": "In ACL,", "citeRegEx": "Wang et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Wang et al\\.", "year": 2015}, {"title": "Sequence-to-sequence learning as beam-search optimization", "author": ["Sam Wiseman", "Alexander M Rush"], "venue": "arXiv preprint arXiv:1606.02960,", "citeRegEx": "Wiseman and Rush.,? \\Q2016\\E", "shortCiteRegEx": "Wiseman and Rush.", "year": 2016}, {"title": "Google\u2019s neural machine translation system: Bridging the gap between human and machine translation", "author": ["Yonghui Wu", "Mike Schuster", "Zhifeng Chen", "Quoc V Le", "Mohammad Norouzi", "Wolfgang Macherey", "Maxim Krikun", "Yuan Cao", "Qin Gao", "Klaus Macherey"], "venue": "arXiv preprint arXiv:1609.08144,", "citeRegEx": "Wu et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Wu et al\\.", "year": 2016}, {"title": "Efficient character-level document classification by combining convolution and recurrent layers", "author": ["Yijun Xiao", "Kyunghyun Cho"], "venue": "arXiv preprint arXiv:1602.00367,", "citeRegEx": "Xiao and Cho.,? \\Q2016\\E", "shortCiteRegEx": "Xiao and Cho.", "year": 2016}, {"title": "Dynamic memory networks for visual and textual question answering", "author": ["Caiming Xiong", "Stephen Merity", "Richard Socher"], "venue": "In ICML,", "citeRegEx": "Xiong et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Xiong et al\\.", "year": 2016}, {"title": "Recurrent neural network regularization", "author": ["Wojciech Zaremba", "Ilya Sutskever", "Oriol Vinyals"], "venue": "arXiv preprint arXiv:1409.2329,", "citeRegEx": "Zaremba et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Zaremba et al\\.", "year": 2014}, {"title": "Character-level convolutional networks for text classification", "author": ["Xiang Zhang", "Junbo Zhao", "Yann LeCun"], "venue": "In NIPS,", "citeRegEx": "Zhang et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Zhang et al\\.", "year": 2015}, {"title": "A C-LSTM neural network for text classification", "author": ["Chunting Zhou", "Chonglin Sun", "Zhiyuan Liu", "Francis Lau"], "venue": "arXiv preprint arXiv:1511.08630,", "citeRegEx": "Zhou et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Zhou et al\\.", "year": 2015}], "referenceMentions": [{"referenceID": 26, "context": "RNN applications in the natural language domain range from sentence classification (Wang et al., 2015) to word- and character-level language modeling (Zaremba et al.", "startOffset": 83, "endOffset": 102}, {"referenceID": 31, "context": ", 2015) to word- and character-level language modeling (Zaremba et al., 2014).", "startOffset": 55, "endOffset": 77}, {"referenceID": 0, "context": "RNNs are also commonly the basic building block for more complex models for tasks such as machine translation (Bahdanau et al., 2015; Luong et al., 2015; Bradbury & Socher, 2016) or question answering (Kumar et al.", "startOffset": 110, "endOffset": 178}, {"referenceID": 15, "context": "RNNs are also commonly the basic building block for more complex models for tasks such as machine translation (Bahdanau et al., 2015; Luong et al., 2015; Bradbury & Socher, 2016) or question answering (Kumar et al.", "startOffset": 110, "endOffset": 178}, {"referenceID": 12, "context": ", 2015; Bradbury & Socher, 2016) or question answering (Kumar et al., 2016; Xiong et al., 2016).", "startOffset": 55, "endOffset": 95}, {"referenceID": 30, "context": ", 2015; Bradbury & Socher, 2016) or question answering (Kumar et al., 2016; Xiong et al., 2016).", "startOffset": 55, "endOffset": 95}, {"referenceID": 10, "context": "Convolutional neural networks (CNNs) (Krizhevsky et al., 2012), though more popular on tasks involving image data, have also been applied to sequence encoding tasks (Zhang et al.", "startOffset": 37, "endOffset": 62}, {"referenceID": 32, "context": ", 2012), though more popular on tasks involving image data, have also been applied to sequence encoding tasks (Zhang et al., 2015).", "startOffset": 110, "endOffset": 130}, {"referenceID": 13, "context": "Convolutional models for sequence processing have been more successful when combined with RNN layers in a hybrid architecture (Lee et al., 2016), because traditional max- and average-pooling approaches to combining convolutional features across timesteps assume time invariance and hence cannot make full use of large-scale sequence order information.", "startOffset": 126, "endOffset": 144}, {"referenceID": 11, "context": "The need for an effective regularization method for LSTMs, and dropout\u2019s relative lack of efficacy when applied to recurrent connections, led to the development of recurrent dropout schemes, including variational inference\u2013based dropout (Gal & Ghahramani, 2016) and zoneout (Krueger et al., 2016).", "startOffset": 274, "endOffset": 296}, {"referenceID": 5, "context": "For sequence classification tasks, we found it helpful to use skip-connections between every QRNN layer, a technique termed \u201cdense convolution\u201d by Huang et al. (2016). Where traditional feed-forward or convolutional networks have connections only between subsequent layers, a \u201cDenseNet\u201d with L layers has feed-forward or convolutional connections between every pair of layers, for a total ofL(L\u22121).", "startOffset": 147, "endOffset": 167}, {"referenceID": 0, "context": "Encoder\u2013decoder models which operate on long sequences are made significantly more powerful with the addition of soft attention (Bahdanau et al., 2015), which removes the need for the entire input representation to fit into a fixed-length encoding vector.", "startOffset": 128, "endOffset": 151}, {"referenceID": 18, "context": "3 Ensemble of RNNs and NB-SVM (Mesnil et al., 2014) \u2212 92.", "startOffset": 30, "endOffset": 51}, {"referenceID": 14, "context": "6 2-layer LSTM (Longpre et al., 2016) \u2212 87.", "startOffset": 15, "endOffset": 37}, {"referenceID": 14, "context": "6 Residual 2-layer bi-LSTM (Longpre et al., 2016) \u2212 90.", "startOffset": 27, "endOffset": 49}, {"referenceID": 16, "context": "We evaluate the QRNN architecture on a popular document-level sentiment classification benchmark, the IMDb movie review dataset (Maas et al., 2011).", "startOffset": 128, "endOffset": 147}, {"referenceID": 21, "context": "Our best performance on a held-out development set was achieved using a four-layer denselyconnected QRNN with 256 units per layer and word vectors initialized using 300-dimensional cased GloVe embeddings (Pennington et al., 2014).", "startOffset": 204, "endOffset": 229}, {"referenceID": 16, "context": "We evaluate the QRNN architecture on a popular document-level sentiment classification benchmark, the IMDb movie review dataset (Maas et al., 2011). The dataset consists of a balanced sample of 25,000 positive and 25,000 negative reviews, divided into equal-size train and test sets, with an average document length of 231 words (Wang & Manning, 2012). We compare only to other results that do not make use of additional unlabeled data (thus excluding e.g., Miyato et al. (2016)).", "startOffset": 129, "endOffset": 479}, {"referenceID": 31, "context": "While the \u201cmedium\u201d models used in other work (Zaremba et al., 2014; Gal & Ghahramani, 2016) consist of 650 units in", "startOffset": 45, "endOffset": 91}, {"referenceID": 30, "context": "We replicate the language modeling experiment of Zaremba et al. (2014) and Gal & Ghahramani (2016) to benchmark the QRNN architecture for natural language sequence prediction.", "startOffset": 49, "endOffset": 71}, {"referenceID": 30, "context": "We replicate the language modeling experiment of Zaremba et al. (2014) and Gal & Ghahramani (2016) to benchmark the QRNN architecture for natural language sequence prediction.", "startOffset": 49, "endOffset": 99}, {"referenceID": 19, "context": "The experiment uses a standard preprocessed version of the Penn Treebank (PTB) by Mikolov et al. (2010). We implemented a gated QRNN model with medium hidden size: 2 layers with 640 units in each layer.", "startOffset": 82, "endOffset": 104}, {"referenceID": 31, "context": "The experimental settings largely followed the \u201cmedium\u201d setup of Zaremba et al. (2014). Optimization was performed by stochastic gradient descent (SGD) without momentum.", "startOffset": 65, "endOffset": 87}, {"referenceID": 31, "context": "The experimental settings largely followed the \u201cmedium\u201d setup of Zaremba et al. (2014). Optimization was performed by stochastic gradient descent (SGD) without momentum. The learning rate was set at 1 for six epochs, then decayed by 0.95 for each subsequent epoch, for a total of 72 epochs. We additionally used L regularization of 2 \u00d7 10\u22124 and rescaled gradients with norm above 10. Zoneout was applied by performing dropout with ratio 0.1 on the forget gates of the QRNN, without rescaling the output of the dropout function. Batches consist of 20 examples, each 105 timesteps. Comparing our results on the gated QRNN with zoneout to the results of LSTMs with both ordinary and variational dropout in Table 2, we see that the QRNN is highly competitive. The QRNN without zoneout strongly outperforms both our medium LSTM and the medium LSTM of Zaremba et al. (2014) which do not use recurrent dropout and is even competitive with variational LSTMs.", "startOffset": 65, "endOffset": 868}, {"referenceID": 31, "context": "Model Parameters Validation Test LSTM (medium) (Zaremba et al., 2014) 20M 86.", "startOffset": 47, "endOffset": 69}, {"referenceID": 8, "context": "7 LSTM with CharCNN embeddings (Kim et al., 2016) 19M \u2212 78.", "startOffset": 31, "endOffset": 49}, {"referenceID": 17, "context": "9 Zoneout + Variational LSTM (medium) (Merity et al., 2016) 20M 84.", "startOffset": 38, "endOffset": 59}, {"referenceID": 22, "context": "tst2014) Word-level LSTM w/attn (Ranzato et al., 2016) \u2212 20.", "startOffset": 32, "endOffset": 54}, {"referenceID": 7, "context": "The QRNN encoder\u2013decoder model shares the favorable parallelism and path-length properties exhibited by the ByteNet (Kalchbrenner et al., 2016), an architecture for character-level machine translation based on residual convolutions over binary trees.", "startOffset": 116, "endOffset": 143}, {"referenceID": 31, "context": "Zhou et al. (2015) apply CNNs at the word level to generate n-gram features used by an LSTM for text classification.", "startOffset": 0, "endOffset": 19}, {"referenceID": 31, "context": "Zhou et al. (2015) apply CNNs at the word level to generate n-gram features used by an LSTM for text classification. Xiao & Cho (2016) also tackle text classification by applying convolutions at the character level, with a stride to reduce sequence length, then feeding these features into a bidirectional LSTM.", "startOffset": 0, "endOffset": 135}, {"referenceID": 12, "context": "A similar approach was taken by Lee et al. (2016) for character-level machine translation.", "startOffset": 32, "endOffset": 50}], "year": 2016, "abstractText": "Recurrent neural networks are a powerful tool for modeling sequential data, but the dependence of each timestep\u2019s computation on the previous timestep\u2019s output limits parallelism and makes RNNs unwieldy for very long sequences. We introduce quasi-recurrent neural networks (QRNNs), an approach to neural sequence modeling that alternates convolutional layers, which apply in parallel across timesteps, and a minimalist recurrent pooling function that applies in parallel across channels. Despite lacking trainable recurrent layers, stacked QRNNs have better predictive accuracy than stacked LSTMs of the same hidden size. Due to their increased parallelism, they are up to 16 times faster at train and test time. Experiments on language modeling, sentiment classification, and character-level neural machine translation demonstrate these advantages and underline the viability of QRNNs as a basic building block for a variety of sequence tasks.", "creator": "LaTeX with hyperref package"}}}