{"id": "1611.01578", "review": {"conference": "iclr", "VERSION": "v1", "DATE_OF_SUBMISSION": "5-Nov-2016", "title": "Neural Architecture Search with Reinforcement Learning", "abstract": "Neural networks are powerful and flexible models that work well for many difficult learning tasks in image, speech and natural language understanding. Despite their success, neural networks are still hard to design. In this paper, we use a recurrent network to generate the model descriptions of neural networks and train this RNN with reinforcement learning to maximize the expected accuracy of the generated architectures on a validation set. On the CIFAR-10 dataset, our method, starting from scratch, can design a novel network architecture that rivals the best human-invented architecture in terms of test set accuracy. Our CIFAR-10 model achieves a test error rate of 3.84, which is only 0.1 percent worse and 1.2x faster than the current state-of-the-art model. On the Penn Treebank dataset, our model can compose a novel recurrent cell that outperforms the widely-used LSTM cell, and other state-of-the-art baselines. Our cell achieves a test set perplexity of 62.4 on the Penn Treebank, which is 3.6 perplexity better than the previous state-of-the-art.", "histories": [["v1", "Sat, 5 Nov 2016 00:41:37 GMT  (583kb,D)", "http://arxiv.org/abs/1611.01578v1", null], ["v2", "Wed, 15 Feb 2017 05:28:05 GMT  (585kb,D)", "http://arxiv.org/abs/1611.01578v2", null]], "reviews": [], "SUBJECTS": "cs.LG cs.AI cs.NE", "authors": ["barret zoph", "quoc v le"], "accepted": true, "id": "1611.01578"}, "pdf": {"name": "1611.01578.pdf", "metadata": {"source": "CRF", "title": null, "authors": ["Barret Zoph", "Quoc V. Le"], "emails": ["barretzoph@google.com", "qvl@google.com"], "sections": [{"heading": "1 INTRODUCTION", "text": "In recent years, we have seen great success with deep neural networks in many challenging applications, such as speech recognition (Hinton et al., 2012), image recognition (LeCun et al., 1998; Krizhevsky et al., 2012) and machine translation (Sutskever et al., 2014; Bahdanau et al., 2015; Wu et al., 2016). Along with this success is a paradigm shift from feature design to architectural design, i.e. from SIFT (Lowe, 1999) and HOG (Dalal & Triggs, 2005), to AlexNet (Krizhevsky et al., 2012), VGGNet (Simonyan & Zisserman, 2014), GoogleNet et al., 2015), and ResNet (He et al., 2016a). Although it has become easier, architectural design still requires a lot of expertise and time."}, {"heading": "2 RELATED WORK", "text": "Hyperparameter optimization is an important research topic in machine learning and is therefore widely used in practice (Bergstra et al., 2011; Bergstra & Bengio, 2012; Snoek et al., 2012; 2015). Despite their success, these methods are still limited because they only seek models from a fixed-length room. In other words, it is difficult to require them to generate a variable-length configuration that specifies the structure and connectivity of a network. In practice, these methods often work better when equipped with a good initial model (Bergstra & Bengio, 2012; Snoek et al., 2012; 2015). Modern neuro-evolution algorithms, e.g. Wierstra et al. (2005); Floreano et al. (2008); Stanley et al. (2009), are much more flexible to compose novel models, but they are generally less practical."}, {"heading": "3 METHODS", "text": "In the following section, we will first describe a simple method of using a recursive network to generate Convolutionary Architectures. We will show how the recursive network can be trained using a policy gradient method to maximize the expected accuracy of the captured architectures. We will present several improvements to our core approach, such as the creation of skip connections to increase model complexity and the use of a parameter server approach to speed up the training. In the last part of the section, we will focus on generating recurrent architectures, which is another important contribution of our work."}, {"heading": "3.1 GENERATING MODEL DESCRIPTIONS WITH A CONTROLLER RECURRENT NEURAL NETWORK", "text": "In Neural Architecture Search, we use a controller to generate architectural hyperparameters of neural networks. To be flexible, the controller is implemented as a recursive neural network. Suppose we want to predict feedback-forward neural networks with only conventional layers, we can use the controller to generate their hyperparameters as a sequence of tokens: in our experiments, the process of creating an architecture stops when the number of layers exceeds a certain value. This value follows a schedule in which we increase it during the training. Once the controller RNN has completed the generation of an architecture, a neural network with that architecture is built and trained. At convergence, the accuracy of the network is recorded on a pre-set validation set. The parameters of the controller RNN, successc, are then optimized to better influence the expected validation accuracy of the proposed architectures, so that we can better describe the parameters of the RNC, to maximize the parameters of the proposed architectures."}, {"heading": "3.2 TRAINING WITH REINFORCE", "text": "The list of tokens that the controller predicts can be considered a list of actions a1: T to design an architecture for a children's network. To find the optimal architecture, we ask our controller to maximize its expected reward, represented by J (\u03b8c) = EP (a1: T; \u03b8c) [R] Since the reward signalR is not differentiable, we must use a political graduation method that is updated iteratively. In this thesis, we use the REINFORCE rule off (a1: T; \u03b8c) = 1 EP (a1: T).3 The control strategies will be from A to Z (a2: A).4"}, {"heading": "3.3 INCREASE ARCHITECTURE COMPLEXITY WITH SKIP CONNECTIONS AND OTHER LAYER TYPES", "text": "This year, we will be able to put ourselves at the top, \"he said.\" We have to put ourselves at the top, \"he said.\" We have to put ourselves at the top, \"he said.\" We have to put ourselves at the top, \"he said.\" We have to put ourselves at the top. \""}, {"heading": "3.4 GENERATE RECURRENT CELL ARCHITECTURES", "text": "This year we have the opportunity to put ourselves at the top of the list, \"he said in an interview with the German Press Agency.\" We have the opportunity to put ourselves at the top, \"he said.\" But we didn't make it. \""}, {"heading": "4 EXPERIMENTS AND RESULTS", "text": "We apply our method to an image classification task with CIFAR-10 and a speech modeling task with Penn Treebank, two of the most frequently benchmarked data sets in deep learning. In CIFAR-10, our goal is to find a good Convolutionary Architecture, while in Penn Treebank, our goal is to find a good recurring cell. We have a separate validation data set for each data set to calculate the reward signal. Reported performance of the test set is only calculated once for the network that achieves the best result on the validation data set we have endured. Further details of our experimental procedures and results are as follows."}, {"heading": "4.1 LEARNING CONVOLUTIONAL ARCHITECTURES FOR CIFAR-10", "text": "This year is the highest in the history of the country."}, {"heading": "4.2 LEARNING RECURRENT CELLS FOR PENN TREEBANK", "text": "In fact, it is such that most of us will be able to put ourselves in a different world, in which they are able to move around, and in which they will be able to move into another world, in which they are able to move, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they, in which they live, in which they live, in which they live, in which they live, in which they live, in which they are able to move, in which they live, in which they live in which they live in which they live, in which they live in which they live, in which they live in which they live in which they live, in which they live in which they live, in which they live in which they live, in which they live in which they live, in which they live in which they live in which they live, in which they live in which they live, in which they live in which they live, in which they live in which they live, in which they live in which they live, in which they live in which they live, in which they live in which they live, in which they live in which they live, in which they live, in which they live in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they are able"}, {"heading": "5 CONCLUSION", "text": "In this article, we present Neural Architecture Search, an idea for using a recursive neural network to create neural network architectures. By using a recursive network as a controller, our method is flexible so that it can search the architectural space with variable length. Our method has strong empirical performance at very demanding benchmarks and represents a new direction of research for automatically finding good neural network architectures. Code for executing the models found by the controller on CIFAR-10 and PTB is published at https: / / github.com / tensorflow / models."}, {"heading": "ACKNOWLEDGMENTS", "text": "We thank Greg Corrado, Jeff Dean, David Ha, Lukasz Kaiser and the Google Brain team for their help with the project."}, {"heading": "A APPENDIX", "text": "It is not the first time that the EU Commission has taken such a step."}], "references": [{"title": "Learning to compose neural networks for question answering", "author": ["Jacob Andreas", "Marcus Rohrbach", "Trevor Darrell", "Dan Klein"], "venue": "In NAACL,", "citeRegEx": "Andreas et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Andreas et al\\.", "year": 2016}, {"title": "Learning to learn by gradient descent by gradient descent", "author": ["Marcin Andrychowicz", "Misha Denil", "Sergio Gomez", "Matthew W Hoffman", "David Pfau", "Tom Schaul", "Nando de Freitas"], "venue": "arXiv preprint arXiv:1606.04474,", "citeRegEx": "Andrychowicz et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Andrychowicz et al\\.", "year": 2016}, {"title": "Neural machine translation by jointly learning to align and translate", "author": ["Dzmitry Bahdanau", "Kyunghyun Cho", "Yoshua Bengio"], "venue": "In ICLR,", "citeRegEx": "Bahdanau et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Bahdanau et al\\.", "year": 2015}, {"title": "Random search for hyper-parameter optimization", "author": ["James Bergstra", "Yoshua Bengio"], "venue": null, "citeRegEx": "Bergstra and Bengio.,? \\Q2012\\E", "shortCiteRegEx": "Bergstra and Bengio.", "year": 2012}, {"title": "Algorithms for hyper-parameter optimization", "author": ["James Bergstra", "R\u00e9mi Bardenet", "Yoshua Bengio", "Bal\u00e1zs K\u00e9gl"], "venue": "In NIPS,", "citeRegEx": "Bergstra et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Bergstra et al\\.", "year": 2011}, {"title": "The inference of regular LISP programs from examples", "author": ["Alan W. Biermann"], "venue": "IEEE transactions on Systems, Man, and Cybernetics,", "citeRegEx": "Biermann.,? \\Q1978\\E", "shortCiteRegEx": "Biermann.", "year": 1978}, {"title": "Language modeling with sum-product networks", "author": ["Wei-Chen Cheng", "Stanley Kok", "Hoai Vu Pham", "Hai Leong Chieu", "Kian Ming Adam Chai"], "venue": "In INTERSPEECH,", "citeRegEx": "Cheng et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Cheng et al\\.", "year": 2014}, {"title": "Histograms of oriented gradients for human detection", "author": ["Navneet Dalal", "Bill Triggs"], "venue": "In CVPR,", "citeRegEx": "Dalal and Triggs.,? \\Q2005\\E", "shortCiteRegEx": "Dalal and Triggs.", "year": 2005}, {"title": "Large scale distributed deep networks", "author": ["Jeffrey Dean", "Greg Corrado", "Rajat Monga", "Kai Chen", "Matthieu Devin", "Mark Mao", "Andrew Senior", "Paul Tucker", "Ke Yang", "Quoc V. Le"], "venue": "In NIPS,", "citeRegEx": "Dean et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Dean et al\\.", "year": 2012}, {"title": "Neuroevolution: from architectures to learning", "author": ["Dario Floreano", "Peter D\u00fcrr", "Claudio Mattiussi"], "venue": "Evolutionary Intelligence,", "citeRegEx": "Floreano et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Floreano et al\\.", "year": 2008}, {"title": "A theoretically grounded application of dropout in recurrent neural networks", "author": ["Yarin Gal"], "venue": "arXiv preprint arXiv:1512.05287,", "citeRegEx": "Gal.,? \\Q2015\\E", "shortCiteRegEx": "Gal.", "year": 2015}, {"title": "Deep residual learning for image recognition", "author": ["Kaiming He", "Xiangyu Zhang", "Shaoqing Ren", "Jian Sun"], "venue": "In CVPR,", "citeRegEx": "He et al\\.,? \\Q2016\\E", "shortCiteRegEx": "He et al\\.", "year": 2016}, {"title": "Identity mappings in deep residual networks", "author": ["Kaiming He", "Xiangyu Zhang", "Shaoqing Ren", "Jian Sun"], "venue": "arXiv preprint arXiv:1603.05027,", "citeRegEx": "He et al\\.,? \\Q2016\\E", "shortCiteRegEx": "He et al\\.", "year": 2016}, {"title": "Deep neural networks for acoustic modeling in speech recognition: The shared views of four research groups", "author": ["Geoffrey Hinton", "Li Deng", "Dong Yu", "George E. Dahl", "Abdel-rahman Mohamed", "Navdeep Jaitly", "Andrew Senior", "Vincent Vanhoucke", "Patrick Nguyen", "Tara N. Sainath"], "venue": "IEEE Signal Processing Magazine,", "citeRegEx": "Hinton et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Hinton et al\\.", "year": 2012}, {"title": "Long short-term memory", "author": ["Sepp Hochreiter", "Juergen Schmidhuber"], "venue": "Neural Computation,", "citeRegEx": "Hochreiter and Schmidhuber.,? \\Q1997\\E", "shortCiteRegEx": "Hochreiter and Schmidhuber.", "year": 1997}, {"title": "Densely connected convolutional networks", "author": ["Gao Huang", "Zhuang Liu", "Kilian Q. Weinberger"], "venue": "arXiv preprint arXiv:1608.06993,", "citeRegEx": "Huang et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Huang et al\\.", "year": 2016}, {"title": "Deep networks with stochastic depth", "author": ["Gao Huang", "Yu Sun", "Zhuang Liu", "Daniel Sedra", "Kilian Weinberger"], "venue": "arXiv preprint arXiv:1603.09382,", "citeRegEx": "Huang et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Huang et al\\.", "year": 2016}, {"title": "Batch normalization: Accelerating deep network training by reducing internal covariate shift", "author": ["Sergey Ioffe", "Christian Szegedy"], "venue": "In ICML,", "citeRegEx": "Ioffe and Szegedy.,? \\Q2015\\E", "shortCiteRegEx": "Ioffe and Szegedy.", "year": 2015}, {"title": "What is the best multi-stage architecture for object recognition", "author": ["Kevin Jarrett", "Koray Kavukcuoglu", "Yann Lecun"], "venue": "In ICCV,", "citeRegEx": "Jarrett et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Jarrett et al\\.", "year": 2009}, {"title": "An empirical exploration of recurrent network architectures", "author": ["Rafal Jozefowicz", "Wojciech Zaremba", "Ilya Sutskever"], "venue": "In ICML,", "citeRegEx": "Jozefowicz et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Jozefowicz et al\\.", "year": 2015}, {"title": "Character-aware neural language models", "author": ["Yoon Kim", "Yacine Jernite", "David Sontag", "Alexander M. Rush"], "venue": "arXiv preprint arXiv:1508.06615,", "citeRegEx": "Kim et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Kim et al\\.", "year": 2015}, {"title": "Adam: A method for stochastic optimization", "author": ["Diederik P. Kingma", "Jimmy Ba"], "venue": "In ICLR,", "citeRegEx": "Kingma and Ba.,? \\Q2015\\E", "shortCiteRegEx": "Kingma and Ba.", "year": 2015}, {"title": "Imagenet classification with deep convolutional neural networks", "author": ["Alex Krizhevsky", "Ilya Sutskever", "Geoffrey E. Hinton"], "venue": "In NIPS,", "citeRegEx": "Krizhevsky et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Krizhevsky et al\\.", "year": 2012}, {"title": "Human-level concept learning through probabilistic program induction", "author": ["Brenden M. Lake", "Ruslan Salakhutdinov", "Joshua B. Tenenbaum"], "venue": null, "citeRegEx": "Lake et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Lake et al\\.", "year": 2015}, {"title": "Fractalnet: Ultra-deep neural networks without residuals", "author": ["Gustav Larsson", "Michael Maire", "Gregory Shakhnarovich"], "venue": "arXiv preprint arXiv:1605.07648,", "citeRegEx": "Larsson et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Larsson et al\\.", "year": 2016}, {"title": "Gradient-based learning applied to document recognition", "author": ["Yann LeCun", "L\u00e9on Bottou", "Yoshua Bengio", "Patrick Haffner"], "venue": "Proceedings of the IEEE,", "citeRegEx": "LeCun et al\\.,? \\Q1998\\E", "shortCiteRegEx": "LeCun et al\\.", "year": 1998}, {"title": "Learning to optimize", "author": ["Ke Li", "Jitendra Malik"], "venue": "arXiv preprint arXiv:1606.01885,", "citeRegEx": "Li and Malik.,? \\Q2016\\E", "shortCiteRegEx": "Li and Malik.", "year": 2016}, {"title": "Learning programs: A hierarchical Bayesian approach", "author": ["Percy Liang", "Michael I. Jordan", "Dan Klein"], "venue": "In ICML,", "citeRegEx": "Liang et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Liang et al\\.", "year": 2010}, {"title": "Object recognition from local scale-invariant features", "author": ["David G. Lowe"], "venue": "In CVPR,", "citeRegEx": "Lowe.,? \\Q1999\\E", "shortCiteRegEx": "Lowe.", "year": 1999}, {"title": "Pointer sentinel mixture models", "author": ["Stephen Merity", "Caiming Xiong", "James Bradbury", "Richard Socher"], "venue": "arXiv preprint arXiv:1609.07843,", "citeRegEx": "Merity et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Merity et al\\.", "year": 2016}, {"title": "Context dependent recurrent neural network language model", "author": ["Tomas Mikolov", "Geoffrey Zweig"], "venue": "In SLT,", "citeRegEx": "Mikolov and Zweig.,? \\Q2012\\E", "shortCiteRegEx": "Mikolov and Zweig.", "year": 2012}, {"title": "Rectified linear units improve restricted Boltzmann machines", "author": ["Vinod Nair", "Geoffrey E. Hinton"], "venue": "In ICML,", "citeRegEx": "Nair and Hinton.,? \\Q2010\\E", "shortCiteRegEx": "Nair and Hinton.", "year": 2010}, {"title": "Neural programmer: Inducing latent programs with gradient descent", "author": ["Arvind Neelakantan", "Quoc V. Le", "Ilya Sutskever"], "venue": "In ICLR,", "citeRegEx": "Neelakantan et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Neelakantan et al\\.", "year": 2015}, {"title": "How to construct deep recurrent neural networks", "author": ["Razvan Pascanu", "Caglar Gulcehre", "Kyunghyun Cho", "Yoshua Bengio"], "venue": "arXiv preprint arXiv:1312.6026,", "citeRegEx": "Pascanu et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Pascanu et al\\.", "year": 2013}, {"title": "Using the output embedding to improve language models", "author": ["Ofir Press", "Lior Wolf"], "venue": "arXiv preprint arXiv:1608.05859,", "citeRegEx": "Press and Wolf.,? \\Q2016\\E", "shortCiteRegEx": "Press and Wolf.", "year": 2016}, {"title": "Sequence level training with recurrent neural networks", "author": ["Marc\u2019Aurelio Ranzato", "Sumit Chopra", "Michael Auli", "Wojciech Zaremba"], "venue": "arXiv preprint arXiv:1511.06732,", "citeRegEx": "Ranzato et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Ranzato et al\\.", "year": 2015}, {"title": "Neural programmer-interpreters", "author": ["Scott Reed", "Nando de Freitas"], "venue": "In ICLR,", "citeRegEx": "Reed and Freitas.,? \\Q2015\\E", "shortCiteRegEx": "Reed and Freitas.", "year": 2015}, {"title": "Minimum risk training for neural machine translation", "author": ["Shiqi Shen", "Yong Cheng", "Zhongjun He", "Wei He", "Hua Wu", "Maosong Sun", "Yang Liu"], "venue": null, "citeRegEx": "Shen et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Shen et al\\.", "year": 2016}, {"title": "Very deep convolutional networks for large-scale image recognition", "author": ["Karen Simonyan", "Andrew Zisserman"], "venue": "arXiv preprint arXiv:1409.1556,", "citeRegEx": "Simonyan and Zisserman.,? \\Q2014\\E", "shortCiteRegEx": "Simonyan and Zisserman.", "year": 2014}, {"title": "Practical Bayesian optimization of machine learning algorithms", "author": ["Jasper Snoek", "Hugo Larochelle", "Ryan P. Adams"], "venue": "In NIPS,", "citeRegEx": "Snoek et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Snoek et al\\.", "year": 2012}, {"title": "Scalable bayesian optimization using deep neural networks", "author": ["Jasper Snoek", "Oren Rippel", "Kevin Swersky", "Ryan Kiros", "Nadathur Satish", "Narayanan Sundaram", "Mostofa Patwary", "Mostofa Ali", "Ryan P. Adams"], "venue": "In ICML,", "citeRegEx": "Snoek et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Snoek et al\\.", "year": 2015}, {"title": "Striving for simplicity: The all convolutional net", "author": ["Jost Tobias Springenberg", "Alexey Dosovitskiy", "Thomas Brox", "Martin Riedmiller"], "venue": "arXiv preprint arXiv:1412.6806,", "citeRegEx": "Springenberg et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Springenberg et al\\.", "year": 2014}, {"title": "A hypercube-based encoding for evolving large-scale neural networks", "author": ["Kenneth O. Stanley", "David B. D\u2019Ambrosio", "Jason Gauci"], "venue": "Artificial Life,", "citeRegEx": "Stanley et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Stanley et al\\.", "year": 2009}, {"title": "A methodology for LISP program construction from examples", "author": ["Phillip D. Summers"], "venue": "Journal of the ACM,", "citeRegEx": "Summers.,? \\Q1977\\E", "shortCiteRegEx": "Summers.", "year": 1977}, {"title": "On the importance of initialization and momentum in deep learning", "author": ["Ilya Sutskever", "James Martens", "George Dahl", "Geoffrey Hinton"], "venue": "In ICML,", "citeRegEx": "Sutskever et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Sutskever et al\\.", "year": 2013}, {"title": "Sequence to sequence learning with neural networks", "author": ["Ilya Sutskever", "Oriol Vinyals", "Quoc V. Le"], "venue": "In NIPS,", "citeRegEx": "Sutskever et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Sutskever et al\\.", "year": 2014}, {"title": "Going deeper with convolutions", "author": ["Christian Szegedy", "Wei Liu", "Yangqing Jia", "Pierre Sermanet", "Scott Reed", "Dragomir Anguelov", "Dumitru Erhan", "Vincent Vanhoucke", "Andrew Rabinovich"], "venue": "In CVPR,", "citeRegEx": "Szegedy et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Szegedy et al\\.", "year": 2015}, {"title": "Learning to learn", "author": ["Sebastian Thrun", "Lorien Pratt"], "venue": "Springer Science & Business Media,", "citeRegEx": "Thrun and Pratt.,? \\Q2012\\E", "shortCiteRegEx": "Thrun and Pratt.", "year": 2012}, {"title": "Modeling systems with internal state using evolino", "author": ["Daan Wierstra", "Faustino J Gomez", "J\u00fcrgen Schmidhuber"], "venue": "In GECCO,", "citeRegEx": "Wierstra et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Wierstra et al\\.", "year": 2005}, {"title": "Simple statistical gradient-following algorithms for connectionist reinforcement learning", "author": ["Ronald J. Williams"], "venue": "In Machine Learning,", "citeRegEx": "Williams.,? \\Q1992\\E", "shortCiteRegEx": "Williams.", "year": 1992}, {"title": "Google\u2019s neural machine translation system: Bridging the gap between human and machine translation", "author": ["Yonghui Wu", "Mike Schuster", "Zhifeng Chen", "Quoc V. Le", "Mohammad Norouzi"], "venue": "arXiv preprint arXiv:1609.08144,", "citeRegEx": "Wu et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Wu et al\\.", "year": 2016}, {"title": "Wide residual networks", "author": ["Sergey Zagoruyko", "Nikos Komodakis"], "venue": "In BMVC,", "citeRegEx": "Zagoruyko and Komodakis.,? \\Q2016\\E", "shortCiteRegEx": "Zagoruyko and Komodakis.", "year": 2016}, {"title": "Recurrent neural network regularization", "author": ["Wojciech Zaremba", "Ilya Sutskever", "Oriol Vinyals"], "venue": "arXiv preprint arXiv:1409.2329,", "citeRegEx": "Zaremba et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Zaremba et al\\.", "year": 2014}, {"title": "Recurrent highway networks", "author": ["Julian Georg Zilly", "Rupesh Kumar Srivastava", "Jan Koutn\u0131\u0301k", "J\u00fcrgen Schmidhuber"], "venue": "arXiv preprint arXiv:1607.03474,", "citeRegEx": "Zilly et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Zilly et al\\.", "year": 2016}], "referenceMentions": [{"referenceID": 13, "context": "The last few years have seen much success of deep neural networks in many challenging applications, such as speech recognition (Hinton et al., 2012), image recognition (LeCun et al.", "startOffset": 127, "endOffset": 148}, {"referenceID": 25, "context": ", 2012), image recognition (LeCun et al., 1998; Krizhevsky et al., 2012) and machine translation (Sutskever et al.", "startOffset": 27, "endOffset": 72}, {"referenceID": 22, "context": ", 2012), image recognition (LeCun et al., 1998; Krizhevsky et al., 2012) and machine translation (Sutskever et al.", "startOffset": 27, "endOffset": 72}, {"referenceID": 45, "context": ", 2012) and machine translation (Sutskever et al., 2014; Bahdanau et al., 2015; Wu et al., 2016).", "startOffset": 32, "endOffset": 96}, {"referenceID": 2, "context": ", 2012) and machine translation (Sutskever et al., 2014; Bahdanau et al., 2015; Wu et al., 2016).", "startOffset": 32, "endOffset": 96}, {"referenceID": 50, "context": ", 2012) and machine translation (Sutskever et al., 2014; Bahdanau et al., 2015; Wu et al., 2016).", "startOffset": 32, "endOffset": 96}, {"referenceID": 28, "context": ", from SIFT (Lowe, 1999), and HOG (Dalal & Triggs, 2005), to AlexNet (Krizhevsky et al.", "startOffset": 12, "endOffset": 24}, {"referenceID": 22, "context": ", from SIFT (Lowe, 1999), and HOG (Dalal & Triggs, 2005), to AlexNet (Krizhevsky et al., 2012), VGGNet (Simonyan & Zisserman, 2014), GoogleNet (Szegedy et al.", "startOffset": 69, "endOffset": 94}, {"referenceID": 46, "context": ", 2012), VGGNet (Simonyan & Zisserman, 2014), GoogleNet (Szegedy et al., 2015), and ResNet (He et al.", "startOffset": 56, "endOffset": 78}, {"referenceID": 4, "context": "Hyperparameter optimization is an important research topic in machine learning, and is widely used in practice (Bergstra et al., 2011; Bergstra & Bengio, 2012; Snoek et al., 2012; 2015).", "startOffset": 111, "endOffset": 185}, {"referenceID": 39, "context": "Hyperparameter optimization is an important research topic in machine learning, and is widely used in practice (Bergstra et al., 2011; Bergstra & Bengio, 2012; Snoek et al., 2012; 2015).", "startOffset": 111, "endOffset": 185}, {"referenceID": 39, "context": "In practice, these methods often work better if they are supplied with a good initial model (Bergstra & Bengio, 2012; Snoek et al., 2012; 2015).", "startOffset": 92, "endOffset": 143}, {"referenceID": 43, "context": "Neural Architecture Search has some parallels to program synthesis and inductive programming, the idea of searching a program from examples (Summers, 1977; Biermann, 1978).", "startOffset": 140, "endOffset": 171}, {"referenceID": 5, "context": "Neural Architecture Search has some parallels to program synthesis and inductive programming, the idea of searching a program from examples (Summers, 1977; Biermann, 1978).", "startOffset": 140, "endOffset": 171}, {"referenceID": 27, "context": "In machine learning, probabilistic program induction has been used successfully in many settings, such as learning to solve simple Q&A (Liang et al., 2010; Neelakantan et al., 2015; Andreas et al., 2016), sort a list of numbers (Reed & de Freitas, 2015), and learning with very few examples (Lake et al.", "startOffset": 135, "endOffset": 203}, {"referenceID": 32, "context": "In machine learning, probabilistic program induction has been used successfully in many settings, such as learning to solve simple Q&A (Liang et al., 2010; Neelakantan et al., 2015; Andreas et al., 2016), sort a list of numbers (Reed & de Freitas, 2015), and learning with very few examples (Lake et al.", "startOffset": 135, "endOffset": 203}, {"referenceID": 0, "context": "In machine learning, probabilistic program induction has been used successfully in many settings, such as learning to solve simple Q&A (Liang et al., 2010; Neelakantan et al., 2015; Andreas et al., 2016), sort a list of numbers (Reed & de Freitas, 2015), and learning with very few examples (Lake et al.", "startOffset": 135, "endOffset": 203}, {"referenceID": 23, "context": ", 2016), sort a list of numbers (Reed & de Freitas, 2015), and learning with very few examples (Lake et al., 2015).", "startOffset": 95, "endOffset": 114}, {"referenceID": 45, "context": "This idea is borrowed from the decoder in end-to-end sequence to sequence learning (Sutskever et al., 2014).", "startOffset": 83, "endOffset": 107}, {"referenceID": 37, "context": "It is therefore similar to the work on BLEU optimization in Neural Machine Translation (Shen et al., 2016; Ranzato et al., 2015).", "startOffset": 87, "endOffset": 128}, {"referenceID": 35, "context": "It is therefore similar to the work on BLEU optimization in Neural Machine Translation (Shen et al., 2016; Ranzato et al., 2015).", "startOffset": 87, "endOffset": 128}, {"referenceID": 1, "context": "More closely related is the idea of using a neural network to learn the gradient descent updates for another network (Andrychowicz et al., 2016) and the idea of using reinforcement learning to find update policies for another network (Li & Malik, 2016).", "startOffset": 117, "endOffset": 144}, {"referenceID": 2, "context": "Hyperparameter optimization is an important research topic in machine learning, and is widely used in practice (Bergstra et al., 2011; Bergstra & Bengio, 2012; Snoek et al., 2012; 2015). Despite their success, these methods are still limited in that they only search models from a fixed-length space. In other words, it is difficult to ask them to generate a variable-length configuration that specifies the structure and connectivity of a network. In practice, these methods often work better if they are supplied with a good initial model (Bergstra & Bengio, 2012; Snoek et al., 2012; 2015). Modern neuro-evolution algorithms, e.g., Wierstra et al. (2005); Floreano et al.", "startOffset": 112, "endOffset": 658}, {"referenceID": 2, "context": "Hyperparameter optimization is an important research topic in machine learning, and is widely used in practice (Bergstra et al., 2011; Bergstra & Bengio, 2012; Snoek et al., 2012; 2015). Despite their success, these methods are still limited in that they only search models from a fixed-length space. In other words, it is difficult to ask them to generate a variable-length configuration that specifies the structure and connectivity of a network. In practice, these methods often work better if they are supplied with a good initial model (Bergstra & Bengio, 2012; Snoek et al., 2012; 2015). Modern neuro-evolution algorithms, e.g., Wierstra et al. (2005); Floreano et al. (2008); Stanley et al.", "startOffset": 112, "endOffset": 682}, {"referenceID": 2, "context": "Hyperparameter optimization is an important research topic in machine learning, and is widely used in practice (Bergstra et al., 2011; Bergstra & Bengio, 2012; Snoek et al., 2012; 2015). Despite their success, these methods are still limited in that they only search models from a fixed-length space. In other words, it is difficult to ask them to generate a variable-length configuration that specifies the structure and connectivity of a network. In practice, these methods often work better if they are supplied with a good initial model (Bergstra & Bengio, 2012; Snoek et al., 2012; 2015). Modern neuro-evolution algorithms, e.g., Wierstra et al. (2005); Floreano et al. (2008); Stanley et al. (2009), on the other hand, are much more flexible for composing novel models, yet they are usually less practical at a large scale.", "startOffset": 112, "endOffset": 705}, {"referenceID": 49, "context": "In this work, we use the REINFORCE rule from (Williams, 1992):", "startOffset": 45, "endOffset": 61}, {"referenceID": 8, "context": "As training a child network can take hours, we use distributed training and asynchronous parameter updates in order to speed up the learning process of the controller (Dean et al., 2012).", "startOffset": 167, "endOffset": 186}, {"referenceID": 46, "context": "1, the search space does not have skip connections, or branching layers used in modern architectures such as GoogleNet (Szegedy et al., 2015), and Residual Net (He et al.", "startOffset": 119, "endOffset": 141}, {"referenceID": 32, "context": "To enable the controller to predict such connections, we use a set-selection type attention (Neelakantan et al., 2015) which was built upon the attention mechanism (Bahdanau et al.", "startOffset": 92, "endOffset": 118}, {"referenceID": 2, "context": ", 2015) which was built upon the attention mechanism (Bahdanau et al., 2015; Vinyals et al., 2015).", "startOffset": 53, "endOffset": 98}, {"referenceID": 18, "context": "Additionally, it is also possible to predict pooling, local contrast normalization (Jarrett et al., 2009; Krizhevsky et al., 2012), and batchnorm (Ioffe & Szegedy, 2015) in the architectures.", "startOffset": 83, "endOffset": 130}, {"referenceID": 22, "context": "Additionally, it is also possible to predict pooling, local contrast normalization (Jarrett et al., 2009; Krizhevsky et al., 2012), and batchnorm (Ioffe & Szegedy, 2015) in the architectures.", "startOffset": 83, "endOffset": 130}, {"referenceID": 44, "context": "9 and used Nesterov Momentum (Sutskever et al., 2013).", "startOffset": 29, "endOffset": 53}, {"referenceID": 41, "context": "81 All-CNN (Springenberg et al., 2014) 7.", "startOffset": 11, "endOffset": 38}, {"referenceID": 40, "context": "72 Scalable Bayesian Optimization (Snoek et al., 2015) 6.", "startOffset": 34, "endOffset": 54}, {"referenceID": 24, "context": "37 FractalNet (Larsson et al., 2016) 21 38.", "startOffset": 14, "endOffset": 36}, {"referenceID": 11, "context": "60 ResNet (He et al., 2016a) 110 1.7M 6.61 ResNet (reported by Huang et al. (2016b)) 110 1.", "startOffset": 11, "endOffset": 84}, {"referenceID": 11, "context": "60 ResNet (He et al., 2016a) 110 1.7M 6.61 ResNet (reported by Huang et al. (2016b)) 110 1.7M 6.41 ResNet with Stochastic Depth (Huang et al., 2016b) 110 1.7M 5.23 1202 10.2M 4.91 Wide ResNet (Zagoruyko & Komodakis, 2016) 16 11.0M 4.81 28 36.5M 4.17 ResNet (pre-activation) (He et al., 2016b) 164 1.7M 5.46 1001 10.2M 4.62 DenseNet (L = 40, k = 12) Huang et al. (2016a) 40 1.", "startOffset": 11, "endOffset": 370}, {"referenceID": 11, "context": "60 ResNet (He et al., 2016a) 110 1.7M 6.61 ResNet (reported by Huang et al. (2016b)) 110 1.7M 6.41 ResNet with Stochastic Depth (Huang et al., 2016b) 110 1.7M 5.23 1202 10.2M 4.91 Wide ResNet (Zagoruyko & Komodakis, 2016) 16 11.0M 4.81 28 36.5M 4.17 ResNet (pre-activation) (He et al., 2016b) 164 1.7M 5.46 1001 10.2M 4.62 DenseNet (L = 40, k = 12) Huang et al. (2016a) 40 1.0M 5.24 DenseNet(L = 100, k = 12) Huang et al. (2016a) 100 7.", "startOffset": 11, "endOffset": 430}, {"referenceID": 11, "context": "60 ResNet (He et al., 2016a) 110 1.7M 6.61 ResNet (reported by Huang et al. (2016b)) 110 1.7M 6.41 ResNet with Stochastic Depth (Huang et al., 2016b) 110 1.7M 5.23 1202 10.2M 4.91 Wide ResNet (Zagoruyko & Komodakis, 2016) 16 11.0M 4.81 28 36.5M 4.17 ResNet (pre-activation) (He et al., 2016b) 164 1.7M 5.46 1001 10.2M 4.62 DenseNet (L = 40, k = 12) Huang et al. (2016a) 40 1.0M 5.24 DenseNet(L = 100, k = 12) Huang et al. (2016a) 100 7.0M 4.10 DenseNet (L = 100, k = 24) Huang et al. (2016a) 100 27.", "startOffset": 11, "endOffset": 492}, {"referenceID": 52, "context": "On this task, LSTM architectures tend to excel (Zaremba et al., 2014; Gal, 2015), and improving them is difficult (Jozefowicz et al.", "startOffset": 47, "endOffset": 80}, {"referenceID": 10, "context": "On this task, LSTM architectures tend to excel (Zaremba et al., 2014; Gal, 2015), and improving them is difficult (Jozefowicz et al.", "startOffset": 47, "endOffset": 80}, {"referenceID": 19, "context": ", 2014; Gal, 2015), and improving them is difficult (Jozefowicz et al., 2015).", "startOffset": 52, "endOffset": 77}, {"referenceID": 52, "context": "First, we make use of the embedding dropout and recurrent dropout techniques proposed in (Zaremba et al., 2014) and (Gal, 2015).", "startOffset": 89, "endOffset": 111}, {"referenceID": 10, "context": ", 2014) and (Gal, 2015).", "startOffset": 12, "endOffset": 23}, {"referenceID": 10, "context": ", 2014; Gal, 2015), and improving them is difficult (Jozefowicz et al., 2015). As PTB is a small dataset, regularization methods are needed to avoid overfitting. First, we make use of the embedding dropout and recurrent dropout techniques proposed in (Zaremba et al., 2014) and (Gal, 2015). We also try to combine them with the shared Input/Output embedding method by Press & Wolf (2016). Results with this method are marked with \u201dshared embeddings.", "startOffset": 8, "endOffset": 388}, {"referenceID": 52, "context": "Every child model has two layers, with the number of hidden units adjusted so that total number of learnable parameters approximately match the \u201dmedium\u201d baselines (Zaremba et al., 2014; Gal, 2015).", "startOffset": 163, "endOffset": 196}, {"referenceID": 10, "context": "Every child model has two layers, with the number of hidden units adjusted so that total number of learnable parameters approximately match the \u201dmedium\u201d baselines (Zaremba et al., 2014; Gal, 2015).", "startOffset": 163, "endOffset": 196}, {"referenceID": 53, "context": "Not only is our cell is better, the model that achieves 64 perplexity is also more than two times faster because the previous best network requires running a cell 10 times per time step (Zilly et al., 2016).", "startOffset": 186, "endOffset": 206}, {"referenceID": 29, "context": "0 Pascanu et al. (2013) - Deep RNN 6M 107.", "startOffset": 2, "endOffset": 24}, {"referenceID": 6, "context": "5 Cheng et al. (2014) - Sum-Prod Net 5M\u2021 100.", "startOffset": 2, "endOffset": 22}, {"referenceID": 6, "context": "5 Cheng et al. (2014) - Sum-Prod Net 5M\u2021 100.0 Zaremba et al. (2014) - LSTM (medium) 20M 82.", "startOffset": 2, "endOffset": 69}, {"referenceID": 6, "context": "5 Cheng et al. (2014) - Sum-Prod Net 5M\u2021 100.0 Zaremba et al. (2014) - LSTM (medium) 20M 82.7 Zaremba et al. (2014) - LSTM (large) 66M 78.", "startOffset": 2, "endOffset": 116}, {"referenceID": 6, "context": "5 Cheng et al. (2014) - Sum-Prod Net 5M\u2021 100.0 Zaremba et al. (2014) - LSTM (medium) 20M 82.7 Zaremba et al. (2014) - LSTM (large) 66M 78.4 Gal (2015) - Variational LSTM (medium, untied) 20M 79.", "startOffset": 2, "endOffset": 151}, {"referenceID": 6, "context": "5 Cheng et al. (2014) - Sum-Prod Net 5M\u2021 100.0 Zaremba et al. (2014) - LSTM (medium) 20M 82.7 Zaremba et al. (2014) - LSTM (large) 66M 78.4 Gal (2015) - Variational LSTM (medium, untied) 20M 79.7 Gal (2015) - Variational LSTM (medium, untied, MC) 20M 78.", "startOffset": 2, "endOffset": 207}, {"referenceID": 6, "context": "5 Cheng et al. (2014) - Sum-Prod Net 5M\u2021 100.0 Zaremba et al. (2014) - LSTM (medium) 20M 82.7 Zaremba et al. (2014) - LSTM (large) 66M 78.4 Gal (2015) - Variational LSTM (medium, untied) 20M 79.7 Gal (2015) - Variational LSTM (medium, untied, MC) 20M 78.6 Gal (2015) - Variational LSTM (large, untied) 66M 75.", "startOffset": 2, "endOffset": 267}, {"referenceID": 6, "context": "5 Cheng et al. (2014) - Sum-Prod Net 5M\u2021 100.0 Zaremba et al. (2014) - LSTM (medium) 20M 82.7 Zaremba et al. (2014) - LSTM (large) 66M 78.4 Gal (2015) - Variational LSTM (medium, untied) 20M 79.7 Gal (2015) - Variational LSTM (medium, untied, MC) 20M 78.6 Gal (2015) - Variational LSTM (large, untied) 66M 75.2 Gal (2015) - Variational LSTM (large, untied, MC) 66M 73.", "startOffset": 2, "endOffset": 322}, {"referenceID": 6, "context": "5 Cheng et al. (2014) - Sum-Prod Net 5M\u2021 100.0 Zaremba et al. (2014) - LSTM (medium) 20M 82.7 Zaremba et al. (2014) - LSTM (large) 66M 78.4 Gal (2015) - Variational LSTM (medium, untied) 20M 79.7 Gal (2015) - Variational LSTM (medium, untied, MC) 20M 78.6 Gal (2015) - Variational LSTM (large, untied) 66M 75.2 Gal (2015) - Variational LSTM (large, untied, MC) 66M 73.4 Kim et al. (2015) - CharCNN 19M 78.", "startOffset": 2, "endOffset": 388}, {"referenceID": 6, "context": "5 Cheng et al. (2014) - Sum-Prod Net 5M\u2021 100.0 Zaremba et al. (2014) - LSTM (medium) 20M 82.7 Zaremba et al. (2014) - LSTM (large) 66M 78.4 Gal (2015) - Variational LSTM (medium, untied) 20M 79.7 Gal (2015) - Variational LSTM (medium, untied, MC) 20M 78.6 Gal (2015) - Variational LSTM (large, untied) 66M 75.2 Gal (2015) - Variational LSTM (large, untied, MC) 66M 73.4 Kim et al. (2015) - CharCNN 19M 78.9 Press & Wolf (2016) - Variational LSTM, shared embeddings 24M 73.", "startOffset": 2, "endOffset": 427}, {"referenceID": 6, "context": "5 Cheng et al. (2014) - Sum-Prod Net 5M\u2021 100.0 Zaremba et al. (2014) - LSTM (medium) 20M 82.7 Zaremba et al. (2014) - LSTM (large) 66M 78.4 Gal (2015) - Variational LSTM (medium, untied) 20M 79.7 Gal (2015) - Variational LSTM (medium, untied, MC) 20M 78.6 Gal (2015) - Variational LSTM (large, untied) 66M 75.2 Gal (2015) - Variational LSTM (large, untied, MC) 66M 73.4 Kim et al. (2015) - CharCNN 19M 78.9 Press & Wolf (2016) - Variational LSTM, shared embeddings 24M 73.2 Merity et al. (2016) - Zoneout + Variational LSTM (medium) 20M 80.", "startOffset": 2, "endOffset": 495}, {"referenceID": 6, "context": "5 Cheng et al. (2014) - Sum-Prod Net 5M\u2021 100.0 Zaremba et al. (2014) - LSTM (medium) 20M 82.7 Zaremba et al. (2014) - LSTM (large) 66M 78.4 Gal (2015) - Variational LSTM (medium, untied) 20M 79.7 Gal (2015) - Variational LSTM (medium, untied, MC) 20M 78.6 Gal (2015) - Variational LSTM (large, untied) 66M 75.2 Gal (2015) - Variational LSTM (large, untied, MC) 66M 73.4 Kim et al. (2015) - CharCNN 19M 78.9 Press & Wolf (2016) - Variational LSTM, shared embeddings 24M 73.2 Merity et al. (2016) - Zoneout + Variational LSTM (medium) 20M 80.6 Merity et al. (2016) - Pointer Sentinel-LSTM (medium) 21M 70.", "startOffset": 2, "endOffset": 563}, {"referenceID": 6, "context": "5 Cheng et al. (2014) - Sum-Prod Net 5M\u2021 100.0 Zaremba et al. (2014) - LSTM (medium) 20M 82.7 Zaremba et al. (2014) - LSTM (large) 66M 78.4 Gal (2015) - Variational LSTM (medium, untied) 20M 79.7 Gal (2015) - Variational LSTM (medium, untied, MC) 20M 78.6 Gal (2015) - Variational LSTM (large, untied) 66M 75.2 Gal (2015) - Variational LSTM (large, untied, MC) 66M 73.4 Kim et al. (2015) - CharCNN 19M 78.9 Press & Wolf (2016) - Variational LSTM, shared embeddings 24M 73.2 Merity et al. (2016) - Zoneout + Variational LSTM (medium) 20M 80.6 Merity et al. (2016) - Pointer Sentinel-LSTM (medium) 21M 70.9 Zilly et al. (2016) - Variational RHN, shared embeddings 24M 66.", "startOffset": 2, "endOffset": 625}, {"referenceID": 29, "context": "Parameter numbers with \u2021 are estimates with reference to Merity et al. (2016).", "startOffset": 57, "endOffset": 78}, {"referenceID": 10, "context": ", 2016), but use variational dropout by Gal (2015). We also run our own LSTM with our setup to get a fair LSTM baseline.", "startOffset": 40, "endOffset": 51}], "year": 2016, "abstractText": "Neural networks are powerful and flexible models that work well for many difficult learning tasks in image, speech and natural language understanding. Despite their success, neural networks are still hard to design. In this paper, we use a recurrent network to generate the model descriptions of neural networks and train this RNN with reinforcement learning to maximize the expected accuracy of the generated architectures on a validation set. On the CIFAR-10 dataset, our method, starting from scratch, can design a novel network architecture that rivals the best human-invented architecture in terms of test set accuracy. Our CIFAR-10 model achieves a test error rate of 3.84, which is only 0.1 percent worse and 1.2x faster than the current state-of-the-art model. On the Penn Treebank dataset, our model can compose a novel recurrent cell that outperforms the widely-used LSTM cell, and other state-of-the-art baselines. Our cell achieves a test set perplexity of 62.4 on the Penn Treebank, which is 3.6 perplexity better than the previous state-ofthe-art.", "creator": "LaTeX with hyperref package"}}}