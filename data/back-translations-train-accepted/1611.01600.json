{"id": "1611.01600", "review": {"conference": "iclr", "VERSION": "v1", "DATE_OF_SUBMISSION": "5-Nov-2016", "title": "Loss-aware Binarization of Deep Networks", "abstract": "Deep neural network models, though very powerful and highly successful, are computationally expensive in terms of space and time. Recently, there have been a number of attempts on binarizing the network weights and activations. This greatly reduces the network size, and replaces the underlying multiplications to additions or even XNOR bit operations. However, existing binarization schemes are based on simple matrix approximation and ignore the effect of binarization on the loss. In this paper, we propose a proximal Newton algorithm with diagonal Hessian approximation that directly minimizes the loss w.r.t. the binarized weights. The underlying proximal step has an efficient closed-form solution, and the second-order information can be efficiently obtained from the second moments already computed by the Adam optimizer. Experiments on both feedforward and recurrent networks show that the proposed loss-aware binarization algorithm outperforms existing binarization schemes, and is also more robust for wide and deep networks.", "histories": [["v1", "Sat, 5 Nov 2016 04:23:42 GMT  (395kb,D)", "https://arxiv.org/abs/1611.01600v1", null], ["v2", "Fri, 3 Mar 2017 02:49:19 GMT  (837kb,D)", "http://arxiv.org/abs/1611.01600v2", null]], "reviews": [], "SUBJECTS": "cs.NE cs.LG", "authors": ["lu hou", "quanming yao", "james t kwok"], "accepted": true, "id": "1611.01600"}, "pdf": {"name": "1611.01600.pdf", "metadata": {"source": "CRF", "title": null, "authors": ["DEEP NETWORKS", "Lu Hou", "Quanming Yao", "James T. Kwok"], "emails": ["lhouab@cse.ust.hk", "qyaoaa@cse.ust.hk", "jamesk@cse.ust.hk"], "sections": [{"heading": "1 INTRODUCTION", "text": "Recently, neural networks have evolved into quantization (Gong et al., 2014), but in various tasks such as speech recognition, visual object recognition, and image classification (LeCun et al., 2015), this is not the case in 2016 (LeCun et al., 2015). Although the high number of network weights results in spatial and temporal inefficiency in both training and storage, the popular method AlexNet, VGG-16, and Resnet-18 each requires hundreds of megabytes to store and billions of high-precision classification operations, limiting their use in embedded systems, smartphones, and other portable devices that are now widely used. To alleviate this problem, a number of approaches have recently been proposed, an attempt to first store and then compress a neural one (Han et al., 2016). Instead of this two-step approach, it is more desirable to compress and train the network."}, {"heading": "2 RELATED WORK", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "2.1 WEIGHT BINARIZATION IN DEEP NETWORKS", "text": "In a forward-facing neural network with L layers, the weight matrix (or tensor in the case of a revolutionary layer) at the layer l Wl. We combine the (fully precise) weights from all layers as w = [w > 1, w > 2,..., w > L] >, where wl = vec (Wl) is. Similarly, the binarized weights are used as w = [w > 1, w > 2,.., w > L] >. Since it is important to use the full precision weights for updates (Courbariaux et al al al al al al al al al al al al al al al al al al al), the binarized weights are typically used only during forward and backward propagation, but not for parameter updates. In tth iteration, the (fully precise) weight matrix at the layer wtl is updated."}, {"heading": "2.2 PROXIMAL NEWTON ALGORITHM", "text": "The proximal Newton algorithm (Lee et al., 2014) has been popularly proposed to solve composite optimization problems of the form x f (x) + g (x), 1A stochastic binarization scheme (Courbariaux et al., 2015), but it is much more expensive than (1) and therefore not factored.Where f is convex and smooth and g is convex but possibly not smooth.With iteration t, it generates the next iteration asxt + 1 = argmin x \u00b2 f (xt) > (x \u2212 xt) + (x \u2212 xt) > H (x \u2212 xt) + g (x), where H is an approximate Hessian matrix from f to x.Using second-order information, the proximal Newton algorithm converges faster than the proximal gradient algorithm (Lee et al., 2014), assuming that the Rangotvey and the difference between Yuil is extended (2002)."}, {"heading": "3 LOSS-AWARE BINARIZATION", "text": "As can be seen, the existing methods of weight binarization (Courbariaux et al., 2015; Rastegari et al., 2016) simply find the closest binary approximation of w and ignore its effects on loss. In this paper, we look at loss directly during binarization. As in (Rastegari et al., 2016), we also binarize the weight wl in each layer as w-l = \u03b1lbl, where \u03b1l > 0 and bl are binary. In the following, we proceed from the following assumptions:. \"(A1)\" must be continuously differentiated with Lipschitz continuous gradient, i.e. there is \u03b2 > 0, so that the boundary between \"(u) \u2212 (v) \u2212 2 \u2264 \u03b2 \u2212 v \u2212 2 for each u, v; (A2)\" is limited from below."}, {"heading": "3.1 BINARIZATION USING PROXIMAL NEWTON ALGORITHM", "text": "We formulate the problem: \"It is not the way it is.\" \"It is the way it is.\" \"It is the way it is.\" \"It is the way it is.\" \"It is the way it is.\" \"It is the way it is.\" \"It is the way it is.\" \"It is the way it is.\" \"It is the way it is.\" \"\" It is the way it is. \"\" It is the way it is. \"\" It is the way it is. \"\" \"It is the way it is.\" \"It is the way it is.\" \"\" \"It is the way it is.\" \"\" \"It is the way it is.\" \"\" \"It is the way it is.\" \"\" \"It is the way it is.\" \"\" \"\" It is the way it is. \"\" \"..\" \"\" \"..\" \"\" \"\".. \"\" \"\".. \"\" \"\" \"..\" \".\" \"\" \"\".. \"\". \"\" \"..\" \"\" \"..\" \".\" \"\". \"\".. \"\" \".\" \".\" \"..\" \"\". \".\".. \"\". \".\". \"\". \".\". \".\". \"..\". \"\". \".\". \".\". \".\".. \".\". \"\". \"..\". \"\". \".\". \".\". \"..\". \".\" \".\". \".\" \"..\" \".\". \".\".. \".\" \".\". \".\". \".\". \".\" \"..\" \".\" \".\".. \".\". \".\" \".\". \".\". \".\". \"..\". \".\". \".\". \".\". \".\". \".\".. \".\". \".\". \"\". \".\". \".\". \"\". \".\". \".\". \".\". \".\" \".\". \".\". \".\". \".\". \".\".. \".\". \".\". \".\". \".\". \"..\". \".\". \".\". \".\". \""}, {"heading": "3.2 EXTENSION TO RECURRENT NEURAL NETWORKS", "text": "The proposed method is easy to recur (Pascanu et al., 2013).Let xl and hl use the input-to-hidden weight Wx and hidden weight Wh., since the weights are divided in a recurrent network, we only have to binarize Wx and Wh in each forward propagated Wx and hidden weight Wh., since the weights are divided in a recurrent network, we only take the form of recurrent Wx and Wh., one can also binarize the activations (of the inputs and hidden states) as in the previous section.In deep networks, the backpropagated gradient takes the form of a product of Jacobian matrices."}, {"heading": "4 EXPERIMENTS", "text": "In this section, we will conduct experiments on the proposed binarization scheme with both feedback networks (Sections 4.1 and 4.2) and recurrent neural networks (Sections 4.3 and 4.4)."}, {"heading": "4.1 FEEDFORWARD NEURAL NETWORKS", "text": "In fact, most of them are able to move to another world, to move to another world, to move to another world, to move to another world, to move to another world, to move to another world, to move to another world, to move to another world, to move to another world, to move to another world, to move to another world, to move to another world, to move to another world, to move to another world, to move to another world, to move to another world, to move to another world, to move to another world, to another world, to another world, to move to another world, to another world, to move to another world."}, {"heading": "4.2 VARYING THE NUMBER OF FILTERS IN CNN", "text": "As in Zhou et al. (2016), we examine the sensitivity to network width by varying the number of filters K in the SVHN dataset. As in Section 4.1, we use the model (2 \u00d7 KC3) \u2212 MP2 \u2212 (2 \u00d7 2KC3) \u2212 MP2 \u2212 (2 \u00d7 4KC3) \u2212 MP2 \u2212 (2 \u00d7 1024FC) \u2212 10SVM. The results are shown in Table 2. Here, too, the proposed LAB performs best. In addition, as the number of filters increases, the degradation due to binarization decreases, suggesting that higher-performance models (e.g. CNN with more filters, standard feedback networks with more hidden units) are less susceptible to performance degradation due to binarization. We suspect that this is due to the fact that large networks often have greater than needed capacities and are therefore less affected by the limited expressiveness of binary weights."}, {"heading": "4.3 RECURRENT NEURAL NETWORKS", "text": "In this section, we conduct experiments with popular short-term memory (LSTM) (Hochreiter & Schmidhuber, 1997); performance is evaluated in the context of character-level speech modeling; the LSTM takes as input a sequence of characters and predicts the next character in each time step; the training goal is cross-entropy loss across all target sequences; following Karpathy et al. (2016), we use two sets of data (with the same training / validation / test-set splitting): (i) Leo Tolstoy's War and Peace, which consists of 3258246 characters almost exclusively in English text, with minimal marking and a vocabulary size of 87; and (ii) the source code of the Linux kernel, which consists of 6206996 characters and a vocabulary size of 101. We use a single-layer LSTM with 512 cells; the maximum number of epochs is 200, and the number of time steps is 100."}, {"heading": "4.4 VARYING THE NUMBER OF TIME STEPS IN LSTM", "text": "In this experiment, we will examine the sensitivity of the binarization schemes with different numbers of unrolled time steps (TS) in the LSTM. The results will be shown in Table 4. Here, too, the proposed LAB performs best. At TS = 10, the LSTM is relatively flat, and all binarization schemes perform similarly to the complete precision network. At TS \u2265 50, BinaryConnect fails, while BWN and the proposed LAB perform better (as discussed in Section 3.2). Figure 2 shows the distribution of hidden to hidden weight gradients for TS = 10 and 100. As can be seen, all models exhibit similar gradient distributions at TS = 10, but the gradient values in BinaryConnect are much higher than those of the other lower network algorithms (TS = 100).Table 4 shows that with increasing time step, all but BinaryConnect perform better."}, {"heading": "5 CONCLUSION", "text": "In this paper, we propose a binarization algorithm that directly considers its impact on loss during binarization. Binarized weights are determined using a proximal Newton algorithm with diagonal Hessian approximation, the proximal step has an efficient closed-loop solution, and the secondary information in Hessian can easily be obtained from the Adam Optimizer. Experiments show that the proposed algorithm outperforms existing binarization schemes, performs similarly to the original full-precision network, and is robust for wide and deep networks."}, {"heading": "ACKNOWLEDGMENTS", "text": "We thank Yongqi Zhang for helping with the experiments and the developers of Theano (Theano Development Team, 2016), Pylearn2 (Goodfellow et al., 2013) and Lasagne. We also thank NVIDIA for supporting the Titan X GPU."}, {"heading": "A PROOF OF PROPOSITION 3.1", "text": "(w = 1) > (w = 1) > (w = 1) > (w = 1) > (w = 1) > (w = 1) = 12 L = 1 (p = 1) > (w = 1 l) > (w = 1 l) > (w = 1 l))) 2 + c1 = 12 L = 1 (l = 1 (l = 1 l))))) 2 + c1 = 12 L = 1 (l = 1 (l = 1 l) > (l = 12 L = 1 (l = 1 l))) 2 + c1 = 12 L = 1 (l = 1 (l = 1 l = 1 l) > (l = 1 (l = 0, l = 1)) 2 + c1, where c1 = 12 (l = 1 l = 1 l) > (l = 1 l = 1 l)."}, {"heading": "B PROOF OF THEOREM 3.1", "text": "Leave \u03b1 = [\u03b1t1. \u2212 t \u2212 t] > and label the target in (3) with F (w \u2212 t, \u03b1). Since w \u2212 t is the minimizer in (6), we have \"(w \u2212 t \u2212 1) +\" (w \u2212 t \u2212 1) (w \u2212 t \u2212 1). (9) From assumption A1 we have \"(w \u2212 t) \u2264\" (w \u2212 t \u2212 1) > Dt \u2212 1) > (w \u2212 t \u2212 1) + \u03b2 2 (w \u2212 t \u2212 1). (9) Under assumption A1 we get \"(w \u2212 t \u2212 1) +\" (w \u2212 t \u2212 1) > (w \u2212 t \u2212 1) + \u03b2 2 (w \u2212 t \u2212 1). (10) Using (9) and (10) we get \"(w \u2212 t \u2212 1) > (w \u2212 t \u2212 t \u2212 t \u2212 1)."}, {"heading": "C PROOF OF PROPOSITION 3.2", "text": "The singular values of W should be \u03bb1 (W) \u2265 \u03bb2 (W) \u2265 \u00b7 \u2265 \u03bbm (W).\u03bb21 (W) \u2265 1m m \u2211 i = 1 \u03bb2i (W) = 1 m \u00b2 W \u00b2 2F = 1 m \u00b2 mn = n.Also \u03bb1 (W) \u2265 \u221a n."}], "references": [{"title": "BinaryConnect: Training deep neural networks with binary weights during propagations", "author": ["M. Courbariaux", "Y. Bengio", "J.P. David"], "venue": "In NIPS,", "citeRegEx": "Courbariaux et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Courbariaux et al\\.", "year": 2015}, {"title": "Equilibrated adaptive learning rates for non-convex optimization", "author": ["Y. Dauphin", "H. de Vries", "Y. Bengio"], "venue": "In NIPS, pp", "citeRegEx": "Dauphin et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Dauphin et al\\.", "year": 2015}, {"title": "RMSprop and equilibrated adaptive learning rates for non-convex optimization", "author": ["Y. Dauphin", "H. de Vries", "J. Chung", "Y. Bengio"], "venue": "Technical Report arXiv:1502.04390,", "citeRegEx": "Dauphin et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Dauphin et al\\.", "year": 2015}, {"title": "Adaptive subgradient methods for online learning and stochastic optimization", "author": ["J. Duchi", "E. Hazan", "Y. Singer"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Duchi et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Duchi et al\\.", "year": 2011}, {"title": "Understanding the difficulty of training deep feedforward neural networks", "author": ["X. Glorot", "Y. Bengio"], "venue": "In AISTAT, pp", "citeRegEx": "Glorot and Bengio.,? \\Q2010\\E", "shortCiteRegEx": "Glorot and Bengio.", "year": 2010}, {"title": "Compressing deep convolutional networks using vector quantization", "author": ["Y. Gong", "L. Liu", "M. Yang", "L. Bourdev"], "venue": "Technical Report arXiv:1412.6115,", "citeRegEx": "Gong et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Gong et al\\.", "year": 2014}, {"title": "Pylearn2: a machine learning research", "author": ["I.J. Goodfellow", "D. Warde-Farley", "P. Lamblin", "V. Dumoulin", "M. Mirza", "R. Pascanu", "J. Bergstra", "F. Bastien", "Y. Bengio"], "venue": "library. arXiv preprint arXiv:1308.4214,", "citeRegEx": "Goodfellow et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Goodfellow et al\\.", "year": 2013}, {"title": "Deep compression: Compressing deep neural network with pruning, trained quantization and Huffman coding", "author": ["S. Han", "H. Mao", "W.J. Dally"], "venue": "In ICLR,", "citeRegEx": "Han et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Han et al\\.", "year": 2016}, {"title": "Long short-term memory", "author": ["S. Hochreiter", "J. Schmidhuber"], "venue": "Neural Computation, pp", "citeRegEx": "Hochreiter and Schmidhuber.,? \\Q1997\\E", "shortCiteRegEx": "Hochreiter and Schmidhuber.", "year": 1997}, {"title": "Visualizing and understanding recurrent networks", "author": ["A. Karpathy", "J. Johnson", "F.-F. Li"], "venue": "In ICLR,", "citeRegEx": "Karpathy et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Karpathy et al\\.", "year": 2016}, {"title": "Compression of deep convolutional neural networks for fast and low power mobile applications", "author": ["Y.-D. Kim", "E. Park", "S. Yoo", "T. Choi", "L. Yang", "D. Shin"], "venue": "In ICLR,", "citeRegEx": "Kim et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Kim et al\\.", "year": 2016}, {"title": "Adam: A method for stochastic optimization", "author": ["D. Kingma", "J. Ba"], "venue": "In ICLR,", "citeRegEx": "Kingma and Ba.,? \\Q2015\\E", "shortCiteRegEx": "Kingma and Ba.", "year": 2015}, {"title": "Proximal Newton-type methods for minimizing composite functions", "author": ["J.D. Lee", "Y. Sun", "M.A. Saunders"], "venue": "SIAM Journal on Optimization,", "citeRegEx": "Lee et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Lee et al\\.", "year": 2014}, {"title": "Ternary weight networks", "author": ["F. Li", "B. Liu"], "venue": "Technical Report", "citeRegEx": "Li and Liu.,? \\Q2016\\E", "shortCiteRegEx": "Li and Liu.", "year": 2016}, {"title": "Neural networks with few multiplications", "author": ["Z. Lin", "M. Courbariaux", "R. Memisevic", "Y. Bengio"], "venue": "In ICLR,", "citeRegEx": "Lin et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Lin et al\\.", "year": 2016}, {"title": "Training deep and recurrent networks with Hessian-free optimization", "author": ["J. Martens", "I. Sutskever"], "venue": "In Neural Networks: Tricks of the trade,", "citeRegEx": "Martens and Sutskever.,? \\Q2012\\E", "shortCiteRegEx": "Martens and Sutskever.", "year": 2012}, {"title": "Tensorizing neural networks", "author": ["A. Novikov", "D. Podoprikhin", "A. Osokin", "D.P. Vetrov"], "venue": "In NIPS, pp", "citeRegEx": "Novikov et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Novikov et al\\.", "year": 2015}, {"title": "Revisiting natural gradient for deep networks", "author": ["R. Pascanu", "Y. Bengio"], "venue": "In ICLR,", "citeRegEx": "Pascanu and Bengio.,? \\Q2014\\E", "shortCiteRegEx": "Pascanu and Bengio.", "year": 2014}, {"title": "On the difficulty of training recurrent neural networks", "author": ["R. Pascanu", "T. Mikolov", "Y. Bengio"], "venue": "In ICLR,", "citeRegEx": "Pascanu et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Pascanu et al\\.", "year": 2013}, {"title": "DC proximal Newton for nonconvex optimization problems", "author": ["A. Rakotomamonjy", "R. Flamary", "G. Gasso"], "venue": "IEEE Transactions on Neural Networks and Learning Systems,", "citeRegEx": "Rakotomamonjy et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Rakotomamonjy et al\\.", "year": 2016}, {"title": "XNOR-Net: ImageNet classification using binary convolutional neural networks", "author": ["M. Rastegari", "V. Ordonez", "J. Redmon", "A. Farhadi"], "venue": "In ECCV,", "citeRegEx": "Rastegari et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Rastegari et al\\.", "year": 2016}, {"title": "Lecture 6.5-rmsprop: Divide the gradient by a running average of its recent magnitude", "author": ["T. Tieleman", "G. Hinton"], "venue": null, "citeRegEx": "Tieleman and Hinton.,? \\Q2012\\E", "shortCiteRegEx": "Tieleman and Hinton.", "year": 2012}, {"title": "The concave-convex procedure (CCCP)", "author": ["A.L. Yuille", "A. Rangarajan"], "venue": "NIPS, 2:1033\u20131040,", "citeRegEx": "Yuille and Rangarajan.,? \\Q2002\\E", "shortCiteRegEx": "Yuille and Rangarajan.", "year": 2002}, {"title": "ADADELTA: An adaptive learning rate method", "author": ["M.D. Zeiler"], "venue": "Technical Report arXiv:1212.5701,", "citeRegEx": "Zeiler.,? \\Q2012\\E", "shortCiteRegEx": "Zeiler.", "year": 2012}, {"title": "DoReFa-Net: Training low bitwidth convolutional neural networks with low bitwidth gradients", "author": ["S. Zhou", "Z. Ni", "X. Zhou", "H. Wen", "Y. Wu", "Y. Zou"], "venue": "Technical Report", "citeRegEx": "Zhou et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Zhou et al\\.", "year": 2016}], "referenceMentions": [{"referenceID": 7, "context": "One attempt first trains a neural network and then compresses it (Han et al., 2016; Kim et al., 2016).", "startOffset": 65, "endOffset": 101}, {"referenceID": 10, "context": "One attempt first trains a neural network and then compresses it (Han et al., 2016; Kim et al., 2016).", "startOffset": 65, "endOffset": 101}, {"referenceID": 16, "context": "Example approaches include tensorizing (Novikov et al., 2015), parameter quantization (Gong et al.", "startOffset": 39, "endOffset": 61}, {"referenceID": 5, "context": ", 2015), parameter quantization (Gong et al., 2014), and binarization (Courbariaux et al.", "startOffset": 32, "endOffset": 51}, {"referenceID": 0, "context": ", 2014), and binarization (Courbariaux et al., 2015; Hubara et al., 2016; Rastegari et al., 2016).", "startOffset": 26, "endOffset": 97}, {"referenceID": 20, "context": ", 2014), and binarization (Courbariaux et al., 2015; Hubara et al., 2016; Rastegari et al., 2016).", "startOffset": 26, "endOffset": 97}, {"referenceID": 14, "context": "Instead of binarization, ternary-connect quantizes each weight to {\u22121, 0, 1} (Lin et al., 2016).", "startOffset": 77, "endOffset": 95}, {"referenceID": 24, "context": "Similarly, the ternary weight network (Li & Liu, 2016) and DoReFa-net (Zhou et al., 2016) quantize weights to three levels or more.", "startOffset": 70, "endOffset": 89}, {"referenceID": 0, "context": ", 2014), and binarization (Courbariaux et al., 2015; Hubara et al., 2016; Rastegari et al., 2016). In particular, binarization only requires one bit for each weight value. This can significantly reduce storage, and also eliminates most multiplications during the forward pass. Courbariaux et al. (2015) pioneered neural network binarization with the BinaryConnect algorithm, which achieves state-of-the-art results on many classification tasks.", "startOffset": 27, "endOffset": 303}, {"referenceID": 0, "context": ", 2014), and binarization (Courbariaux et al., 2015; Hubara et al., 2016; Rastegari et al., 2016). In particular, binarization only requires one bit for each weight value. This can significantly reduce storage, and also eliminates most multiplications during the forward pass. Courbariaux et al. (2015) pioneered neural network binarization with the BinaryConnect algorithm, which achieves state-of-the-art results on many classification tasks. Besides binarizing the weights, Hubara et al. (2016) further binarized the activations.", "startOffset": 27, "endOffset": 498}, {"referenceID": 0, "context": ", 2014), and binarization (Courbariaux et al., 2015; Hubara et al., 2016; Rastegari et al., 2016). In particular, binarization only requires one bit for each weight value. This can significantly reduce storage, and also eliminates most multiplications during the forward pass. Courbariaux et al. (2015) pioneered neural network binarization with the BinaryConnect algorithm, which achieves state-of-the-art results on many classification tasks. Besides binarizing the weights, Hubara et al. (2016) further binarized the activations. Rastegari et al. (2016) also learned to scale the binarized weights, and obtained better results.", "startOffset": 27, "endOffset": 557}, {"referenceID": 3, "context": "Another approach uses element-wise adaptive learning rate, as in Adagrad (Duchi et al., 2011), Adadelta (Zeiler, 2012), RMSprop (Tieleman & Hinton, 2012), and Adam Kingma & Ba (2015).", "startOffset": 73, "endOffset": 93}, {"referenceID": 23, "context": ", 2011), Adadelta (Zeiler, 2012), RMSprop (Tieleman & Hinton, 2012), and Adam Kingma & Ba (2015).", "startOffset": 18, "endOffset": 32}, {"referenceID": 12, "context": "We formulate this as an optimization problem using the proximal Newton algorithm (Lee et al., 2014) with a diagonal Hessian.", "startOffset": 81, "endOffset": 99}, {"referenceID": 0, "context": "The proposed method also reduces to BinaryConnect (Courbariaux et al., 2015) and the Binary-Weight-Network (Hubara et al.", "startOffset": 50, "endOffset": 76}, {"referenceID": 2, "context": "Another approach uses element-wise adaptive learning rate, as in Adagrad (Duchi et al., 2011), Adadelta (Zeiler, 2012), RMSprop (Tieleman & Hinton, 2012), and Adam Kingma & Ba (2015). This can also be considered as preconditioning that rescales the gradient so that all dimensions have similar curvatures.", "startOffset": 74, "endOffset": 183}, {"referenceID": 0, "context": "As it is essential to use full-precision weights during updates (Courbariaux et al., 2015), typically binarized weights are only used during the forward and backward propagations, but not on parameter update.", "startOffset": 64, "endOffset": 90}, {"referenceID": 0, "context": "The two most popular binarization schemes are BinaryConnect (Courbariaux et al., 2015) and Binary-Weight-Network (BWN) (Rastegari et al.", "startOffset": 60, "endOffset": 86}, {"referenceID": 20, "context": ", 2015) and Binary-Weight-Network (BWN) (Rastegari et al., 2016).", "startOffset": 40, "endOffset": 64}, {"referenceID": 12, "context": "The proximal Newton algorithm (Lee et al., 2014) has been popularly used for solving composite optimization problems of the form min x f(x) + g(x),", "startOffset": 30, "endOffset": 48}, {"referenceID": 0, "context": "A stochastic binarization scheme is also proposed in (Courbariaux et al., 2015).", "startOffset": 53, "endOffset": 79}, {"referenceID": 12, "context": "With the use of second-order information, the proximal Newton algorithm converges faster than the proximal gradient algorithm (Lee et al., 2014).", "startOffset": 126, "endOffset": 144}, {"referenceID": 19, "context": "Recently, by assuming that f and g have difference-of-convex decompositions (Yuille & Rangarajan, 2002), the proximal Newton algorithm is also extended to the case where g is nonconvex (Rakotomamonjy et al., 2016).", "startOffset": 185, "endOffset": 213}, {"referenceID": 0, "context": "As can be seen, existing weight binarization methods (Courbariaux et al., 2015; Rastegari et al., 2016) simply find the closest binary approximation of w, and ignore its effects to the loss.", "startOffset": 53, "endOffset": 103}, {"referenceID": 20, "context": "As can be seen, existing weight binarization methods (Courbariaux et al., 2015; Rastegari et al., 2016) simply find the closest binary approximation of w, and ignore its effects to the loss.", "startOffset": 53, "endOffset": 103}, {"referenceID": 20, "context": "As in (Rastegari et al., 2016), we also binarize the weight wl in each layer as \u0175l = \u03b1lbl, where \u03b1l > 0 and bl is binary.", "startOffset": 6, "endOffset": 30}, {"referenceID": 12, "context": "Hence, convergence analysis of the proximal Newton algorithm in (Lee et al., 2014), which is only for convex problems, cannot be applied.", "startOffset": 64, "endOffset": 82}, {"referenceID": 12, "context": "Hence, convergence analysis of the proximal Newton algorithm in (Lee et al., 2014), which is only for convex problems, cannot be applied. Recently, Rakotomamonjy et al. (2016) proposed a nonconvex proximal Newton extension.", "startOffset": 65, "endOffset": 176}, {"referenceID": 12, "context": "In composite optimization, it is known that the proximal Newton method is more efficient than the proximal gradient algorithm (Lee et al., 2014; Rakotomamonjy et al., 2016).", "startOffset": 126, "endOffset": 172}, {"referenceID": 19, "context": "In composite optimization, it is known that the proximal Newton method is more efficient than the proximal gradient algorithm (Lee et al., 2014; Rakotomamonjy et al., 2016).", "startOffset": 126, "endOffset": 172}, {"referenceID": 0, "context": "As discussed in (Courbariaux et al., 2015), it is important to keep a full-precision weight during training.", "startOffset": 16, "endOffset": 42}, {"referenceID": 18, "context": "A typical recurrent neural network has a recurrence of the form hl = Wxxl +Wh\u03c3(hl\u22121) + b (equivalent to the more widely known hl = \u03c3(Wxxl+Whhl\u22121+b) (Pascanu et al., 2013) ).", "startOffset": 148, "endOffset": 170}, {"referenceID": 18, "context": "In deep networks, the backpropagated gradient takes the form of a product of Jacobian matrices (Pascanu et al., 2013).", "startOffset": 95, "endOffset": 117}, {"referenceID": 18, "context": "The necessary condition for exploding gradients is that the largest singular value \u03bb1(Wh) of Wh is larger than some given constant (Pascanu et al., 2013).", "startOffset": 131, "endOffset": 153}, {"referenceID": 20, "context": ", 2016), the weight-andactivation binarized counterpart of BinaryConnect; (ii) XNOR-Network (XNOR) (Rastegari et al., 2016), the counterpart of BWN; (iii) LAB2, the counterpart of the proposed method, which binarizes weights using proximal Newton method and binarizes activations using a simple sign function.", "startOffset": 99, "endOffset": 123}, {"referenceID": 0, "context": "The setup is similar to that in Courbariaux et al. (2015). We do not perform data augmentation or unsupervised pretraining.", "startOffset": 32, "endOffset": 58}, {"referenceID": 0, "context": "Since binarization is a form of regularization (Courbariaux et al., 2015), we do not use other regularization methods (like Dropout).", "startOffset": 47, "endOffset": 73}, {"referenceID": 24, "context": "As in Zhou et al. (2016), we study sensitivity to network width by varying the number of filters K on the SVHN data set.", "startOffset": 6, "endOffset": 25}, {"referenceID": 9, "context": "Following Karpathy et al. (2016), we use two data sets (with the same training/validation/test set splitting): (i) Leo Tolstoy\u2019s War and Peace, which consists of 3258246 characters of almost entirely English text with minimal markup and has a vocabulary size of 87; and (ii) the source code of the Linux Kernel, which consists of 6206996 characters and has a vocabulary size of 101.", "startOffset": 10, "endOffset": 33}, {"referenceID": 6, "context": "We thank Yongqi Zhang for helping with the experiments, and developers of Theano (Theano Development Team, 2016), Pylearn2 (Goodfellow et al., 2013) and Lasagne.", "startOffset": 123, "endOffset": 148}], "year": 2017, "abstractText": "Deep neural network models, though very powerful and highly successful, are computationally expensive in terms of space and time. Recently, there have been a number of attempts on binarizing the network weights and activations. This greatly reduces the network size, and replaces the underlying multiplications to additions or even XNOR bit operations. However, existing binarization schemes are based on simple matrix approximations and ignore the effect of binarization on the loss. In this paper, we propose a proximal Newton algorithm with diagonal Hessian approximation that directly minimizes the loss w.r.t. the binarized weights. The underlying proximal step has an efficient closed-form solution, and the second-order information can be efficiently obtained from the second moments already computed by the Adam optimizer. Experiments on both feedforward and recurrent networks show that the proposed loss-aware binarization algorithm outperforms existing binarization schemes, and is also more robust for wide and deep networks.", "creator": "LaTeX with hyperref package"}}}