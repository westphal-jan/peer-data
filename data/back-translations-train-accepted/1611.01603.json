{"id": "1611.01603", "review": {"conference": "iclr", "VERSION": "v1", "DATE_OF_SUBMISSION": "5-Nov-2016", "title": "Bidirectional Attention Flow for Machine Comprehension", "abstract": "Machine Comprehension (MC), answering questions about a given context, re-quires modeling complex interactions between the context and the query. Recently, attention mechanisms have been successfully extended to MC. Typically these mechanisms use attention to summarize the query and context into a single vectors, couple attentions temporally, and often form a unidirectional attention. In this paper we introduce the Bidirectional Attention Flow (BIDAF) Model, a multi-stage hierarchical process that represents the context at different levels of granularity and uses a bi-directional attention flow mechanism to achieve a query-aware context representation without early summarization. Our experimental evaluations show that our model achieves the state-of-the-art results in Stanford QA(SQuAD) and CNN/DailyMail Cloze Test datasets.", "histories": [["v1", "Sat, 5 Nov 2016 04:49:00 GMT  (1009kb,D)", "http://arxiv.org/abs/1611.01603v1", null], ["v2", "Mon, 21 Nov 2016 21:34:22 GMT  (1010kb,D)", "http://arxiv.org/abs/1611.01603v2", null], ["v3", "Tue, 29 Nov 2016 18:46:50 GMT  (1011kb,D)", "http://arxiv.org/abs/1611.01603v3", null], ["v4", "Tue, 7 Feb 2017 22:01:42 GMT  (1018kb,D)", "http://arxiv.org/abs/1611.01603v4", "Published as a conference paper at ICLR 2017"], ["v5", "Fri, 24 Feb 2017 19:57:53 GMT  (1017kb,D)", "http://arxiv.org/abs/1611.01603v5", "Published as a conference paper at ICLR 2017"]], "reviews": [], "SUBJECTS": "cs.CL", "authors": ["minjoon seo", "aniruddha kembhavi", "ali farhadi", "hannaneh hajishirzi"], "accepted": true, "id": "1611.01603"}, "pdf": {"name": "1611.01603.pdf", "metadata": {"source": "CRF", "title": null, "authors": ["MACHINE COMPREHENSION", "Minjoon Seo", "Aniruddha Kembhavi", "Ali Farhadi"], "emails": ["minjoon@cs.washington.edu,", "ali@cs.washington.edu,", "hannaneh@cs.washington.edu,", "anik@allenai.org"], "sections": [{"heading": "1 INTRODUCTION", "text": "In recent years, the number of those able to increase significantly has multiplied, both in the US and in Europe."}, {"heading": "2 MODEL", "text": "It is a question of whether and in what form people in the United States, in Europe and throughout the world will be able to surpass themselves, and it is a question of whether and how they will be able to surpass themselves. (...) It is a question of whether and how they can surpass themselves. (...) It is a question of whether and how they can surpass themselves. (...) It is a question of whether and how they must surpass themselves. (...) It is a question of whether and how they can surpass themselves. (...) It is a question of whether and how they must surpass themselves. (...) It is a question of whether they must surpass themselves. (...) It is a question of whether they must surpass themselves and whether they must surpass themselves. (...) It is a question of whether they must surpass themselves. (...) It is a question. (...) It is a question of whether they must surpass themselves. (...) It is a question. (...) It is a question of whether they must surpass themselves. (...) It is a question of whether they must surpass themselves. (... \"(...) It is a question."}, {"heading": "3 RELATED WORK", "text": "The first group (largely inspired by Bahdanau et al. (2015) uses a dynamic attention mechanism in which attention is dynamically directed to the query and context as well as attention; the first group (largely inspired by Bahdanau et al.) uses a dynamic attention mechanism in which attention is dynamically directed to the query and fixed attention."}, {"heading": "4 QUESTION ANSWERING EXPERIMENTS", "text": "In this section we evaluate our model for answering the question we have asked ourselves, with the recently published SQuAD study (Rajpurkar et al., 2016), which has attracted a lot of attention in order to answer more than 100,000 questions. In the next section we evaluate our model in relation to human understanding. The answer to this question is: \"What is this man?,\" \"What is this man?,\" \"What is this man?,\" \"What is this man?,\" \"What is this man?,\" \"\" What is man?, \"\" \"What is\" man?, \"\" \"What is man?,\" \"\" What is man?, \"\" What is man?, \"\" What is man?, \"\" What is man?, \"What is man?,\" What is man?, \"What is man?,\" What is man?, \"What is man?,\" What is man?, \"What is man?,\" What is man?, \"What is man?,\" What is man?, \"What is man?,\" What is man?, \"What is man?,\" What is man?, \"What is man?,\" What is man?, \"what is man?,\" what is man?, \"what is man?,\" what is man?, \"what is man?,\" what is man?, \"what is man?,\" what is man?, \"what is man?,\" what is man?, \"what is man?,\" what is man?, \"what is man?,\" what is man?, \"what is man?,\" what is man?, \"what is man?,\" what is man?"}, {"heading": "5 CLOZE EXPERIMENTS", "text": "In a Cloze test, the reader is asked to fill in words that have been removed from a passage to measure their ability to understand text. Hermann et al. (2015) recently compiled a mas-sive Cloze-style comprehension dataset that consists of 300k / 4k / 3k and 879k / 65k (train / development / test) examples from CNN and DailyMail news articles. Each example has a news article and an incomplete sentence extracted from the article's summary. To distinguish this task from voice modeling and force us to refer to the article to predict the correct missing word, the missing word is always a named entity, anonymized with random IDs. Model Details The model architecture used for this task is very similar to the one previously used with only a few small changes to adapt it to the task."}, {"heading": "6 CONCLUSION", "text": "In this article, we introduce BIDAF, a multi-step hierarchical process that presents context at different levels of granularity and uses a bidirectional attention-flow mechanism to achieve query-conscious context representation without early summary. Experimental evaluations show that our model achieves the current results in Stanford QA (SQuAD) and CNN / DailyMail Cloze test datasets. Ablation analyses show the importance of each component in our model. Analysis and visualization show that our model learns a suitable representation for MC and is able to answer complex questions by taking care of the correct places in the given paragraph."}, {"heading": "A ERROR ANALYSIS", "text": "The following table summarizes the error modes of BIDAF and shows examples for each error category."}], "references": [{"title": "Vqa: Visual question answering", "author": ["Stanislaw Antol", "Aishwarya Agrawal", "Jiasen Lu", "Margaret Mitchell", "Dhruv Batra", "C Lawrence Zitnick", "Devi Parikh"], "venue": "In ICCV,", "citeRegEx": "Antol et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Antol et al\\.", "year": 2015}, {"title": "Neural machine translation by jointly learning to align and translate", "author": ["Dzmitry Bahdanau", "Kyunghyun Cho", "Yoshua Bengio"], "venue": null, "citeRegEx": "Bahdanau et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Bahdanau et al\\.", "year": 2015}, {"title": "A thorough examination of the cnn/daily mail reading comprehension", "author": ["Danqi Chen", "Jason Bolton", "Christopher D. Manning"], "venue": null, "citeRegEx": "Chen et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Chen et al\\.", "year": 2016}, {"title": "Attention-overattention neural networks for reading comprehension", "author": ["Yiming Cui", "Zhipeng Chen", "Si Wei", "Shijin Wang", "Ting Liu", "Guoping Hu"], "venue": "arXiv preprint arXiv:1607.04423,", "citeRegEx": "Cui et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Cui et al\\.", "year": 2016}, {"title": "Gated-attention readers for text comprehension", "author": ["Bhuwan Dhingra", "Hanxiao Liu", "William W Cohen", "Ruslan Salakhutdinov"], "venue": "arXiv preprint arXiv:1606.01549,", "citeRegEx": "Dhingra et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Dhingra et al\\.", "year": 2016}, {"title": "Multimodal compact bilinear pooling for visual question answering and visual grounding", "author": ["Akira Fukui", "Dong Huk Park", "Daylen Yang", "Anna Rohrbach", "Trevor Darrell", "Marcus Rohrbach"], "venue": null, "citeRegEx": "Fukui et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Fukui et al\\.", "year": 2016}, {"title": "Teaching machines to read and comprehend", "author": ["Karl Moritz Hermann", "Tom\u00e1s Kocisk\u00fd", "Edward Grefenstette", "Lasse Espeholt", "Will Kay", "Mustafa Suleyman", "Phil Blunsom"], "venue": "In NIPS,", "citeRegEx": "Hermann et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Hermann et al\\.", "year": 2015}, {"title": "The goldilocks principle: Reading children\u2019s books with explicit memory representations", "author": ["Felix Hill", "Antoine Bordes", "Sumit Chopra", "Jason Weston"], "venue": "In ICLR,", "citeRegEx": "Hill et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Hill et al\\.", "year": 2016}, {"title": "Long short-term memory", "author": ["Sepp Hochreiter", "Jurgen Schmidhuber"], "venue": "Neural Computation,", "citeRegEx": "Hochreiter and Schmidhuber.,? \\Q1997\\E", "shortCiteRegEx": "Hochreiter and Schmidhuber.", "year": 1997}, {"title": "Text understanding with the attention sum reader network", "author": ["Rudolf Kadlec", "Martin Schmid", "Ondrej Bajgar", "Jan Kleindienst"], "venue": "In ACL,", "citeRegEx": "Kadlec et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Kadlec et al\\.", "year": 2016}, {"title": "Convolutional neural networks for sentence classification", "author": ["Yoon Kim"], "venue": "In EMNLP,", "citeRegEx": "Kim.,? \\Q2014\\E", "shortCiteRegEx": "Kim.", "year": 2014}, {"title": "Dynamic entity representation with max-pooling improves machine reading", "author": ["Sosuke Kobayashi", "Ran Tian", "Naoaki Okazaki", "Kentaro Inui"], "venue": "In NAACL-HLT,", "citeRegEx": "Kobayashi et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Kobayashi et al\\.", "year": 2016}, {"title": "Hierarchical question-image co-attention for visual question answering", "author": ["Jiasen Lu", "Jianwei Yang", "Dhruv Batra", "Devi Parikh"], "venue": "In NIPS,", "citeRegEx": "Lu et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Lu et al\\.", "year": 2016}, {"title": "Ask your neurons: A neural-based approach to answering questions about images", "author": ["Mateusz Malinowski", "Marcus Rohrbach", "Mario Fritz"], "venue": "In ICCV,", "citeRegEx": "Malinowski et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Malinowski et al\\.", "year": 2015}, {"title": "Glove: Global vectors for word representation", "author": ["Jeffrey Pennington", "Richard Socher", "Christopher D Manning"], "venue": "In EMNLP,", "citeRegEx": "Pennington et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Pennington et al\\.", "year": 2014}, {"title": "Squad: 100,000+ questions for machine comprehension of text", "author": ["Pranav Rajpurkar", "Jian Zhang", "Konstantin Lopyrev", "Percy Liang"], "venue": "In EMNLP,", "citeRegEx": "Rajpurkar et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Rajpurkar et al\\.", "year": 2016}, {"title": "Mctest: A challenge dataset for the open-domain machine comprehension of text", "author": ["Matthew Richardson", "Christopher JC Burges", "Erin Renshaw"], "venue": "In EMNLP,", "citeRegEx": "Richardson et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Richardson et al\\.", "year": 2013}, {"title": "Reasonet: Learning to stop reading in machine comprehension", "author": ["Yelong Shen", "Po-Sen Huang", "Jianfeng Gao", "Weizhu Chen"], "venue": "arXiv preprint arXiv:1609.05284,", "citeRegEx": "Shen et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Shen et al\\.", "year": 2016}, {"title": "Iterative alternating neural attention for machine reading", "author": ["Alessandro Sordoni", "Phillip Bachman", "Yoshua Bengio"], "venue": "arXiv preprint arXiv:1606.02245,", "citeRegEx": "Sordoni et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Sordoni et al\\.", "year": 2016}, {"title": "Dropout: a simple way to prevent neural networks from overfitting", "author": ["Nitish Srivastava", "Geoffrey E. Hinton", "Alex Krizhevsky", "Ilya Sutskever", "Ruslan Salakhutdinov"], "venue": null, "citeRegEx": "Srivastava et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Srivastava et al\\.", "year": 2014}, {"title": "Natural language comprehension with the epireader", "author": ["Adam Trischler", "Zheng Ye", "Xingdi Yuan", "Kaheer Suleman"], "venue": "In EMNLP,", "citeRegEx": "Trischler et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Trischler et al\\.", "year": 2016}, {"title": "Machine comprehension using match-lstm and answer pointer", "author": ["Shuohang Wang", "Jing Jiang"], "venue": "arXiv preprint arXiv:1608.07905,", "citeRegEx": "Wang and Jiang.,? \\Q2016\\E", "shortCiteRegEx": "Wang and Jiang.", "year": 2016}, {"title": "Dynamic memory networks for visual and textual question answering", "author": ["Caiming Xiong", "Stephen Merity", "Richard Socher"], "venue": "In ICML,", "citeRegEx": "Xiong et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Xiong et al\\.", "year": 2016}, {"title": "Ask, attend and answer: Exploring question-guided spatial attention for visual question answering", "author": ["Huijuan Xu", "Kate Saenko"], "venue": "In ECCV,", "citeRegEx": "Xu and Saenko.,? \\Q2016\\E", "shortCiteRegEx": "Xu and Saenko.", "year": 2016}, {"title": "Stacked attention networks for image question answering", "author": ["Zichao Yang", "Xiaodong He", "Jianfeng Gao", "Li Deng", "Alex Smola"], "venue": "arXiv preprint arXiv:1511.02274,", "citeRegEx": "Yang et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Yang et al\\.", "year": 2015}, {"title": "End-to-end reading comprehension with dynamic answer chunk ranking", "author": ["Yang Yu", "Wei Zhang", "Kazi Hasan", "Mo Yu", "Bing Xiang", "Bowen Zhou"], "venue": "arXiv preprint arXiv:1610.09996,", "citeRegEx": "Yu et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Yu et al\\.", "year": 2016}, {"title": "Adadelta: an adaptive learning rate method", "author": ["Matthew D Zeiler"], "venue": "arXiv preprint arXiv:1212.5701,", "citeRegEx": "Zeiler.,? \\Q2012\\E", "shortCiteRegEx": "Zeiler.", "year": 2012}, {"title": "Visual7w: Grounded question answering in images", "author": ["Yuke Zhu", "Oliver Groth", "Michael S. Bernstein", "Li Fei-Fei"], "venue": null, "citeRegEx": "Zhu et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Zhu et al\\.", "year": 2016}], "referenceMentions": [{"referenceID": 15, "context": "The two significant contributors towards this progress are the advent of deep neural architectures and the availability of massive datasets on the order of 100,000 questions (Rajpurkar et al., 2016; Hermann et al., 2015).", "startOffset": 174, "endOffset": 220}, {"referenceID": 6, "context": "The two significant contributors towards this progress are the advent of deep neural architectures and the availability of massive datasets on the order of 100,000 questions (Rajpurkar et al., 2016; Hermann et al., 2015).", "startOffset": 174, "endOffset": 220}, {"referenceID": 1, "context": ", in machine translation (Bahdanau et al., 2015)) have led to a significant improvement in their performance.", "startOffset": 25, "endOffset": 48}, {"referenceID": 0, "context": ", in machine translation (Bahdanau et al., 2015)) have led to a significant improvement in their performance. Attention mechanisms have also been successfully extended to machine comprehension (attention between the query and the context paragraph) and visual question answering (attention between the query and image). These mechanisms (e.g., Weston et al. (2015); Antol et al.", "startOffset": 26, "endOffset": 365}, {"referenceID": 0, "context": "(2015); Antol et al. (2015); Xiong et al.", "startOffset": 8, "endOffset": 28}, {"referenceID": 0, "context": "(2015); Antol et al. (2015); Xiong et al. (2016)) typically have one or more of the following characteristics.", "startOffset": 8, "endOffset": 49}, {"referenceID": 15, "context": "Finally, we provide an in depth ablation study of our model on the SQuAD development set, visualize the intermediate feature spaces in our model, analyse its performance as compared to a more traditional language model for machine comprehension (Rajpurkar et al., 2016) and present an analysis of the questions answered incorrectly by our model.", "startOffset": 245, "endOffset": 269}, {"referenceID": 10, "context": "Character Embedding Layer embeds each word into a vector space using characrter level CNNs (Kim (2014)) 2.", "startOffset": 92, "endOffset": 103}, {"referenceID": 10, "context": "Following Kim (2014), we obtain the characterlevel embedding of each word using Convolutional Neural Networks (CNN).", "startOffset": 10, "endOffset": 21}, {"referenceID": 14, "context": "We use pre-trained word vectors, GloVe (Pennington et al., 2014), to obtain the fixed word embedding of each word.", "startOffset": 39, "endOffset": 64}, {"referenceID": 16, "context": "Early datasets such as MCTest (Richardson et al., 2013) were too small to train end to end neural models.", "startOffset": 30, "endOffset": 55}, {"referenceID": 6, "context": "Massive cloze datasets (CNN & DailyMail by Hermann et al. (2015) and Childrens Book Test by Hill et al.", "startOffset": 43, "endOffset": 65}, {"referenceID": 6, "context": "Massive cloze datasets (CNN & DailyMail by Hermann et al. (2015) and Childrens Book Test by Hill et al. (2016)), enabled the application of deep neural architectures to this task.", "startOffset": 43, "endOffset": 111}, {"referenceID": 6, "context": "Massive cloze datasets (CNN & DailyMail by Hermann et al. (2015) and Childrens Book Test by Hill et al. (2016)), enabled the application of deep neural architectures to this task. More recently, Rajpurkar et al. (2016) released the Stanford Question Answering (SQuAD) dataset with over 100,000 questions.", "startOffset": 43, "endOffset": 219}, {"referenceID": 1, "context": "The first group (largely inspired by Bahdanau et al. (2015)) uses a dynamic attention mechanism, in which the attention weights are updated dynamically given the query and the context as well as the previous attention.", "startOffset": 37, "endOffset": 60}, {"referenceID": 1, "context": "The first group (largely inspired by Bahdanau et al. (2015)) uses a dynamic attention mechanism, in which the attention weights are updated dynamically given the query and the context as well as the previous attention. Hermann et al. (2015) argue that the dynamic attention model performs better than using a single fixed query vector to attend on context words on CNN & DailyMail datasets.", "startOffset": 37, "endOffset": 241}, {"referenceID": 1, "context": "The first group (largely inspired by Bahdanau et al. (2015)) uses a dynamic attention mechanism, in which the attention weights are updated dynamically given the query and the context as well as the previous attention. Hermann et al. (2015) argue that the dynamic attention model performs better than using a single fixed query vector to attend on context words on CNN & DailyMail datasets. Chen et al. (2016) show that simply using bilinear term for computing the attention weights in the same model drastically improves the accuracy.", "startOffset": 37, "endOffset": 410}, {"referenceID": 1, "context": "The first group (largely inspired by Bahdanau et al. (2015)) uses a dynamic attention mechanism, in which the attention weights are updated dynamically given the query and the context as well as the previous attention. Hermann et al. (2015) argue that the dynamic attention model performs better than using a single fixed query vector to attend on context words on CNN & DailyMail datasets. Chen et al. (2016) show that simply using bilinear term for computing the attention weights in the same model drastically improves the accuracy. Wang & Jiang (2016) reverse the direction of the attention (attending on query words as the context RNN progresses) for SQuAD.", "startOffset": 37, "endOffset": 556}, {"referenceID": 3, "context": "Attention-over-attention model (Cui et al., 2016) uses a 2D similarity matrix between the query and context words (similar to Equation 1) to compute the weighted average of query-to-context attention.", "startOffset": 31, "endOffset": 49}, {"referenceID": 8, "context": ", Kadlec et al. (2016)).", "startOffset": 2, "endOffset": 23}, {"referenceID": 0, "context": "Early works on visual question answering (VQA) involved encoding the question using an RNN, encoding the image using a CNN and combining them to answer the question (Antol et al., 2015; Malinowski et al., 2015).", "startOffset": 165, "endOffset": 210}, {"referenceID": 13, "context": "Early works on visual question answering (VQA) involved encoding the question using an RNN, encoding the image using a CNN and combining them to answer the question (Antol et al., 2015; Malinowski et al., 2015).", "startOffset": 165, "endOffset": 210}, {"referenceID": 27, "context": "At the coarse level of granularity, the question attends to different patches in the image (Zhu et al., 2016; Xiong et al., 2016).", "startOffset": 91, "endOffset": 129}, {"referenceID": 22, "context": "At the coarse level of granularity, the question attends to different patches in the image (Zhu et al., 2016; Xiong et al., 2016).", "startOffset": 91, "endOffset": 129}, {"referenceID": 24, "context": "A hybrid approach is to combine questions representations at multiple levels of granularity (unigrams, bigrams, trigrams) (Yang et al., 2015).", "startOffset": 122, "endOffset": 141}, {"referenceID": 5, "context": "Several approaches to constructing the attention matrix have been used including element-wise product, element-wise sum, concatenation and Multimodal Compact Bilinear Pooling (Fukui et al., 2016).", "startOffset": 175, "endOffset": 195}, {"referenceID": 13, "context": ", Sordoni et al. (2016); Dhingra et al.", "startOffset": 2, "endOffset": 24}, {"referenceID": 3, "context": "(2016); Dhingra et al. (2016)).", "startOffset": 8, "endOffset": 30}, {"referenceID": 3, "context": "(2016); Dhingra et al. (2016)). Shen et al. (2016) combine Memory Networks with Reinforcement Learning in order to dynamically control the number of hops.", "startOffset": 8, "endOffset": 51}, {"referenceID": 15, "context": "In this section, we evaluate our model on the task of question answering using the recently released SQuAD (Rajpurkar et al., 2016), which has gained a lot of attention over a few short months.", "startOffset": 107, "endOffset": 131}, {"referenceID": 26, "context": "We use the AdaDelta (Zeiler, 2012) optimizer, using a minibatch size of 60 and an initial learning rate of 0.", "startOffset": 20, "endOffset": 34}, {"referenceID": 19, "context": "A dropout (Srivastava et al., 2014) rate", "startOffset": 10, "endOffset": 35}, {"referenceID": 15, "context": "Table 1: (1a) The performance of our model BIDAF\u2217 and competing approaches (Rajpurkar et al., 2016), (Wang & Jiang, 2016) and (Yu et al.", "startOffset": 75, "endOffset": 99}, {"referenceID": 25, "context": ", 2016), (Wang & Jiang, 2016) and (Yu et al., 2016) on the SQuAD test set.", "startOffset": 34, "endOffset": 51}, {"referenceID": 1, "context": "To evaluate the attention flow, we study a dynamic attention model, where the attention is dynamically computed within the modeling layer\u2019s LSTM, following previous work (Bahdanau et al., 2015; Wang & Jiang, 2016).", "startOffset": 170, "endOffset": 213}, {"referenceID": 15, "context": "(b) Venn diagram of the questions answered correctly by our model and the more traditional baseline (Rajpurkar et al., 2016).", "startOffset": 100, "endOffset": 124}, {"referenceID": 15, "context": "Discussion We analysed the performance of our our model with a more traditional language feature based baseline (Rajpurkar et al., 2016).", "startOffset": 112, "endOffset": 136}, {"referenceID": 6, "context": "We also evaluate our model on the task of Cloze-style reading comprehension using the CNN and Daily Mail datasets (Hermann et al., 2015).", "startOffset": 114, "endOffset": 136}, {"referenceID": 6, "context": "We also evaluate our model on the task of Cloze-style reading comprehension using the CNN and Daily Mail datasets (Hermann et al., 2015). Dataset In a Cloze test, the reader is asked to fill in words that have been removed from a passage, for measuring one\u2019s ability to comprehend text. Hermann et al. (2015) have recently compiled a mas-", "startOffset": 115, "endOffset": 309}, {"referenceID": 7, "context": "Inspired by the window-based method (Hill et al., 2016), in order to speed up the training process and reduce GPU memory usage, we only consider words within 7 word-distance of any entity, which we found to be sufficient in most cases.", "startOffset": 36, "endOffset": 55}, {"referenceID": 8, "context": "To address this , we follow a similar strategy from Kadlec et al. (2016). After we obtain p during training, we sum all probability values of entities in the context that correspond to the correct answer.", "startOffset": 52, "endOffset": 73}, {"referenceID": 4, "context": "The next best approaches for the single model and ensemble runs, GAReader (Dhingra et al., 2016) and ReasoNet (Shen et al.", "startOffset": 74, "endOffset": 96}, {"referenceID": 17, "context": ", 2016) and ReasoNet (Shen et al., 2016), both compute attention dynamically over several hops of a recurrent layer.", "startOffset": 21, "endOffset": 40}, {"referenceID": 6, "context": "CNN DailyMail val test val test Attentive Reader (Hermann et al., 2015) 61.", "startOffset": 49, "endOffset": 71}, {"referenceID": 7, "context": "0 MemNN (Hill et al., 2016) 63.", "startOffset": 8, "endOffset": 27}, {"referenceID": 9, "context": "8 - AS Reader (Kadlec et al., 2016) 68.", "startOffset": 14, "endOffset": 35}, {"referenceID": 2, "context": "9 Stanford AR (Chen et al., 2016) 68.", "startOffset": 14, "endOffset": 33}, {"referenceID": 11, "context": "9 DER Network (Kobayashi et al., 2016) 71.", "startOffset": 14, "endOffset": 38}, {"referenceID": 18, "context": "9 - Iterative Attention (Sordoni et al., 2016) 72.", "startOffset": 24, "endOffset": 46}, {"referenceID": 20, "context": "3 - EpiReader (Trischler et al., 2016) 73.", "startOffset": 14, "endOffset": 38}, {"referenceID": 4, "context": "0 - GAReader (Dhingra et al., 2016) 73.", "startOffset": 13, "endOffset": 35}, {"referenceID": 3, "context": "7 AoA Reader (Cui et al., 2016) 73.", "startOffset": 13, "endOffset": 31}, {"referenceID": 17, "context": "4 - ReasoNet (Shen et al., 2016) 72.", "startOffset": 13, "endOffset": 32}, {"referenceID": 7, "context": "6 MemNN\u2217 (Hill et al., 2016) 66.", "startOffset": 9, "endOffset": 28}, {"referenceID": 9, "context": "4 - ASReader\u2217 (Kadlec et al., 2016) 73.", "startOffset": 14, "endOffset": 35}, {"referenceID": 18, "context": "7 Iterative Attention\u2217 (Sordoni et al., 2016) 74.", "startOffset": 23, "endOffset": 45}, {"referenceID": 3, "context": "7 - GA Reader\u2217 (Cui et al., 2016) 76.", "startOffset": 15, "endOffset": 33}], "year": 2016, "abstractText": "Machine Comprehension (MC), answering questions about a given context, requires modeling complex interactions between the context and the query. Recently, attention mechanisms have been successfully extended to MC. Typically these mechanisms use attention to summarize the query and context into a single vector, couple attentions temporally, and often form a uni-directional attention. In this paper we introduce the Bi-Directional Attention Flow (BIDAF) network, a multi-stage hierarchical process that represents the context at different levels of granularity and uses a bi-directional attention flow mechanism to achieve a query-aware context representation without early summarization. Our experimental evaluations show that our model achieves the state-of-the-art results in Stanford QA (SQuAD) and CNN/DailyMail Cloze Test datasets.", "creator": "LaTeX with hyperref package"}}}