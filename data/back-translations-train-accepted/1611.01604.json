{"id": "1611.01604", "review": {"conference": "iclr", "VERSION": "v1", "DATE_OF_SUBMISSION": "5-Nov-2016", "title": "Dynamic Coattention Networks For Question Answering", "abstract": "Several deep learning models have been proposed for question answering. However, due to their single-pass nature, they have no way to recover from local maxima corresponding to incorrect answers. To address this problem, we introduce the Dynamic Coattention Network (DCN) for question answering. The DCN first fuses co-dependent representations of the question and the document in order to focus on relevant parts of both. Then a dynamic pointing decoder iterates over potential answer spans. This iterative procedure enables the model to recover from initial local maxima corresponding to incorrect answers. On the Stanford question answering dataset, a single DCN model improves the previous state of the art from 71.0% F1 to 75.9%, while a DCN ensemble obtains 80.4% F1.", "histories": [["v1", "Sat, 5 Nov 2016 04:53:40 GMT  (802kb,D)", "http://arxiv.org/abs/1611.01604v1", "13 pages, 7 figures"], ["v2", "Thu, 17 Nov 2016 19:58:22 GMT  (803kb,D)", "http://arxiv.org/abs/1611.01604v2", "13 pages, 7 figures"], ["v3", "Mon, 13 Feb 2017 23:00:32 GMT  (803kb,D)", "http://arxiv.org/abs/1611.01604v3", "14 pages, 7 figures, International Conference on Learning Representations 2017"]], "COMMENTS": "13 pages, 7 figures", "reviews": [], "SUBJECTS": "cs.CL cs.AI", "authors": ["caiming xiong", "victor zhong", "richard socher"], "accepted": true, "id": "1611.01604"}, "pdf": {"name": "1611.01604.pdf", "metadata": {"source": "CRF", "title": null, "authors": ["QUESTION ANSWERING", "Caiming Xiong", "Victor Zhong", "Richard Socher"], "emails": ["rsocher}@salesforce.com"], "sections": [{"heading": null, "text": "To solve this problem, we introduce the Dynamic Coattention Network (DCN) for answering questions. DCN first merges codependent representations of the question and the document to focus on relevant parts of both. Afterwards, a dynamic pointer decoder iterates over potential response spans. This iterative procedure allows the model to recover from initial local maxima that match the wrong answers. In answering the Stanford question, a single DCN model improves the prior art from 71.0% F1 to 75.9%, while a DCN ensemble reaches 80.4% F1."}, {"heading": "1 INTRODUCTION", "text": "Answering questions (QA) is a critical task in processing natural language that requires both an understanding of natural language and global knowledge. Previous QA datasets tended to be of high quality due to human annotations, but small in size (Berant et al., 2014; Richardson et al., 2013), so they did not provide training-intensive, expressive models such as deep neural networks. To solve this problem, researchers have developed large-scale datasets using more semi-automatic techniques (Hermann et al., 2015; Hill et al., 2015). Compared to their smaller, hand-annotated counterparts, these QA datasets allow for the formation of more meaningful models. However, they have been shown to differ from natural, human-annotated datasets in the types of reasoning needed to answer the questions (Chen et al., 2016)."}, {"heading": "2 DYNAMIC COATTENTION NETWORKS", "text": "Figure 1 illustrates an overview of the DCN. We first describe the encoders for the document and the question, followed by the coattention mechanism and the dynamic decoder that generates the response span."}, {"heading": "2.1 DOCUMENT AND QUESTION ENCODER", "text": "Let us specify (xQ1, x Q 2,.., x Q n) the order of the word vectors corresponding to the words in the question, and (xD1, x D 2,.., x D m) the same for the words in the document. Using an LSTM (Hochreiter & Schmidhuber, 1997) we encode the document as: dt = LSTMenc (dt \u2212 1, x D t). We define the document encoding the matrix for the encoding as D = [d1...."}, {"heading": "2.2 COATTENTION ENCODER", "text": "We propose a co-attention mechanism that deals with the question and the document at the same time, similar to (Lu et al., 2016) and finally merges both contexts of attention. Figure 2 provides an illustration of the co-attention key. We first calculate the affinity matrix, which contains affinity values that correspond to all word pairs and question words: L = D > Q * R (m + 1) \u00b7 (n + 1). The affinity matrix is normalized in series to generate the attention weights AQ for each word in the question, and in columns to generate the attention weights AD for each word in the document: AQ = softmax (n + 1) and AD = softmax (L >)."}, {"heading": "2.3 DYNAMIC POINTING DECODER", "text": "Due to the nature of the SQuAD, an intuitive method of generating the response span, predicting the start and end points of the span (Wang & Jiang, 2016), however, there may be several intuitive response ranges within the document, each corresponding to a local maxim. We propose an iterative method to select a response span similar to a state machine whose state is maintained by an LSTM-based sequential model. During each iteration, the decoder updates its state taking into account the coding of current estimates of start and end positions and is produced."}, {"heading": "3 RELATED WORK", "text": "Statistical QA Traditional approaches to answering questions typically involve rule-based algorithms or linear classifiers for answering 2016-style questions. Richardson et al. (2013) proposed two basic lines, one that uses simple lexical features such as a sliding window to assign word distances between words in the question and in the document, and another that uses word distances between words in the question and in the document. Berant et al. (2014) proposed an alternative approach that first learns a structured representation of entities and relationships in the document in the form of a knowledge base, then converts the question into a structured query that matches the content of the knowledge base. Wang & McAllester (2015) described a statistical model that uses semantic features and syntactic features as part of language tags and dependency spares."}, {"heading": "4 EXPERIMENTS", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "4.1 IMPLEMENTATION DETAILS", "text": "To pre-process the corpus, we use the tokenizer from Stanford CoreNLP (Manning et al., 2014). We limit the vocabulary to words present in the common crawl corpus, and set embeddings for words outside the vocabulary to zero. Empirically, we found that embedding consistently resulted in overhauling and below-average performance, and therefore report only results with fixed word embeddings. We use a maximum sequence length of 600 during training and a hidden state size of 200 for all recurring units, maxout layers, and linear layers. For the dynamic decoder, we set the maximum number of iterations to 4 and use a maxout pool of 32. We use dropout to regulate our network (Srivava, 2014, and Sstava models)."}, {"heading": "4.2 RESULTS", "text": "In fact, in the USA, in the USA, in the USA, in the USA, in the USA, in Europe, in the USA, in Europe, in the USA, in the USA, in the USA, in Europe, in the USA, in the USA, in the USA, in Europe, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA"}, {"heading": "5 CONCLUSION", "text": "The DCN consists of a coattention encoder that learns codependent representations of the question and the document, and a dynamic decoder that iteratively estimates the response span. We showed that the iterative nature of the model allows it to recover from initial local maxima that correspond to incorrect predictions. On the SQuAD dataset, the DCN reaches the state of the art at 75.9% F1 with a single model and 80.4% F1 with an overall ensemble."}, {"heading": "ACKNOWLEDGMENTS", "text": "We thank Kazuma Hashimoto for his help and insights."}, {"heading": "A APPENDIX", "text": "A.1 SAMPLE OF CORRECT QUAD REGULATIONS BY THE DYNAMIC COAT NETWORK"}, {"heading": "HOW DID THE MONGOLS ACQUIRE CHINESE PRINTING TECHNOLOGY?", "text": "ID: 572882242ca10214002da420Mongolian rulers promoted the yuan printing industry. Chinese printing technology was transferred to the Mongols through the Kingdom of Qocho and Tibetan middlemen. Some yuan documents, such as Wang Zhen's Nong Shu, were printed with movable pottery, a technology invented in the 12th century. However, most published works were still produced using traditional block printing techniques. The publication of a Taoist text called Tregene Khatun's Wife is one of the first printed products sponsored by the Mongols. In 1273, the Mongols founded the Imperial Library Directorate, a printing house sponsored by the government. The Yuan government established printing centers throughout China. Local schools and government agencies were financed to support the publication of books. Fundamentals were predicted by the Kingdom of Qocho and the Tibetan middlemen: the Kingdom of Qo and the Middle Men."}, {"heading": "WHO APPOINTS ELDERS?", "text": "ID 5730d473b7151e1900c0155bElders are called by God, confirmed by the Church and ordained by a Bishop to a ministry of the Word, Sacraments, Order and Service within the Church. They may be called to the local Church or to other valid extended offices of the Church. Elders are given the power to preach the Word of God, administer the sacraments of the Church, provide care and counsel, and arrange the life of the Church for service and mission. Elders may also be appointed as District Rectors, and they are eligible for election to the episcopate. Elders serve as provisional elders for a term of 23 years prior to ordination. Fundamentals Bishop, the local Church predicts a Bishop."}, {"heading": "AN ALGORITHM FOR X WHICH REDUCES TO C WOULD ALLOW US TO DO WHAT?", "text": "ID 56e1ce08e3433e14004231a6This motivates the concept that a problem is difficult for a complexity class. A problem X is difficult for a group of problems C, if every problem in C can be reduced to X. Therefore, no problem in C is more difficult than X, because an algorithm for X allows us to solve every problem in C. Of course, the idea of hard problems depends on the type of reduction used. Polynomial time reductions are often used for complexity classes greater than P."}, {"heading": "HOW MANY GENERAL QUESTIONS ARE AVAILABLE TO OPPOSITION LEADERS?", "text": "ID 572fd7b8947a6a140053cd3eParliamentary time is also earmarked for Question Time in the Debate Chamber. A \"General Question Time\" will take place on Thursdays between 11.40 and 12.00, during which Members can put questions to any member of the Scottish Government. At 2.30 p.m., a 40-minute thematic \"Question Time\" will take place, during which Members will be able to ask questions to Ministers in Ministries selected for questioning on that day, such as Health and Justice or Education and Transport. Between 12.00 and 12.30 p.m. on Thursdays when Parliament is in session, the First Minister's Question Time will take place, giving Members the opportunity to question the First Minister directly on issues within their jurisdiction. Opposition leaders will put a general question to the First Minister and subsequently supplementary questions. Such a practice will allow the questioner to receive an \"introduction,\" who will then use their supplementary question to ask the First Minister. The four general questions available to the Leader of the Opposition are:"}, {"heading": "WHAT ARE SOME OF THE ACCEPTED GENERAL PRINCIPLES OF EUROPEAN UNION LAW?", "text": "ID 5726a00cf1498d1400e8e551The principles of European Union law are legislation developed by the European Court of Justice and represent unwritten rules that are not expressly provided for in the Treaties but influence the interpretation and application of European Union law. In formulating these principles, the courts have relied on a variety of sources, including: public international law and legal principles and principles present in the legal systems of the EU Member States and in the case law of the European Court of Human Rights. Recognised general principles of EU law include fundamental rights (see Human Rights), proportionality, legal certainty, equality before the law and subsidiarity. Fundamental rights (see Human Rights), proportionality, legal certainty, equality before the law and subsidiarity Predict fundamental rights (see Human Rights), proportionality, legal certainty, equality before the law and subsidiarity"}, {"heading": "WHY WAS TESLA RETURNED TO GOSPIC?", "text": "ID 56dfaa047aa994140058dfbdOn March 24, 1879, Tesla was returned to Gospi under police protection because he did not have a residence permit. On April 17, 1879, Milutin Tesla died at the age of 60 after an unspecified illness (although some sources say he died of a stroke). This year, Tesla taught a large class of students at his old school, the Higher Real Gymnasium, in Gospi. Among the basic truths that do not have a residence permit are predictions that do not have a residence permit. 2 SAMPLES OF INCORRECT SQUAD PREDICTIONS BY DYNAMIC COATTENTION NETWORKWHAT IS ONE SUPLEMENTARY PRODESS OF EUROPEAN UNION LAW? ID 5725c3a9ec44d21400f506European Union law is used by European Union Mechanism Case of EUPROPLAW."}, {"heading": "WHO DESIGNED THE ILLUMINATION SYSTEMS THAT TESLA ELECTRIC LIGHT &", "text": "MANUFACTURING INSTALLED? ID 56e0d6cf231d4119001ac424After Tesla left Edison's company in 1886, it partnered with two businessmen, Robert Lane and Benjamin Vail, who agreed to fund an electric lighting company on behalf of Tesla, Tesla Electric Light & Manufacturing. Tesla installed electric arc lighting systems designed by Tesla and also had designs for dynamic machinery, the first patents granted to Tesla in the USA.Basics TeslaPredictions Robert Lane and Benjamin VailComment The model produces a false prediction that matches people who funded Tesla, rather than Tesla who actually designed the lighting system.Empirically, we find that most errors made by the model have the right type (e.g. entity type), although types are not taken into account as prior knowledge of the model. In this case, the right person has the wrong reaction."}, {"heading": "CYDIPPID ARE TYPICALLY WHAT SHAPE?", "text": "ID 57265746dd62a815002e821aCydippid ctenophores have bodies that are more or less rounded, sometimes almost spherical and sometimes cylindrical or ovoid; the common coastal gooseberry, Pleurobrachia, sometimes has an ovoid body with its mouth at the narrow end, although some individuals are more evenly rounded; from opposite sides of the body, a pair of long, slender tentacles, each housed in a sheath into which it can be withdrawn; some species of cydippids have bodies that are flattened to varying degrees so that they are wider in the plane of the tentacles. Grounded truths are more or less rounded, ovoid predictions spherical Comment Although the error is subtle, the prediction is incorrect. The statement \"are more or less rounded, sometimes almost spherical\" indicates that the typicality \"is more or more common than\" an intuitive \"response to this.\""}], "references": [{"title": "Vqa: Visual question answering", "author": ["Stanislaw Antol", "Aishwarya Agrawal", "Jiasen Lu", "Margaret Mitchell", "Dhruv Batra", "C Lawrence Zitnick", "Devi Parikh"], "venue": "In Proceedings of the IEEE International Conference on Computer Vision, pp", "citeRegEx": "Antol et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Antol et al\\.", "year": 2015}, {"title": "Modeling biological processes for reading comprehension", "author": ["Jonathan Berant", "Vivek Srikumar", "Pei-Chun Chen", "Abby Vander Linden", "Brittany Harding", "Brad Huang", "Peter Clark", "Christopher D Manning"], "venue": "In EMNLP,", "citeRegEx": "Berant et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Berant et al\\.", "year": 2014}, {"title": "On the statistical analysis of dirty pictures", "author": ["Julian Besag"], "venue": "Journal of the Royal Statistical Society. Series B (Methodological), pp", "citeRegEx": "Besag.,? \\Q1986\\E", "shortCiteRegEx": "Besag.", "year": 1986}, {"title": "A thorough examination of the cnn/daily mail reading comprehension", "author": ["Danqi Chen", "Jason Bolton", "Christopher D Manning"], "venue": "task. arXiv preprint arXiv:1606.02858,", "citeRegEx": "Chen et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Chen et al\\.", "year": 2016}, {"title": "Attention-overattention neural networks for reading comprehension", "author": ["Yiming Cui", "Zhipeng Chen", "Si Wei", "Shijin Wang", "Ting Liu", "Guoping Hu"], "venue": "arXiv preprint arXiv:1607.04423,", "citeRegEx": "Cui et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Cui et al\\.", "year": 2016}, {"title": "Teaching machines to read and comprehend", "author": ["Karl Moritz Hermann", "Tomas Kocisky", "Edward Grefenstette", "Lasse Espeholt", "Will Kay", "Mustafa Suleyman", "Phil Blunsom"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Hermann et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Hermann et al\\.", "year": 2015}, {"title": "The goldilocks principle: Reading children\u2019s books with explicit memory representations", "author": ["Felix Hill", "Antoine Bordes", "Sumit Chopra", "Jason Weston"], "venue": "arXiv preprint arXiv:1511.02301,", "citeRegEx": "Hill et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Hill et al\\.", "year": 2015}, {"title": "Long short-term memory", "author": ["Sepp Hochreiter", "J\u00fcrgen Schmidhuber"], "venue": "Neural computation,", "citeRegEx": "Hochreiter and Schmidhuber.,? \\Q1997\\E", "shortCiteRegEx": "Hochreiter and Schmidhuber.", "year": 1997}, {"title": "Text understanding with the attention sum reader network", "author": ["Rudolf Kadlec", "Martin Schmid", "Ondrej Bajgar", "Jan Kleindienst"], "venue": "arXiv preprint arXiv:1603.01547,", "citeRegEx": "Kadlec et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Kadlec et al\\.", "year": 2016}, {"title": "Adam: A method for stochastic optimization", "author": ["Diederik Kingma", "Jimmy Ba"], "venue": "arXiv preprint arXiv:1412.6980,", "citeRegEx": "Kingma and Ba.,? \\Q2014\\E", "shortCiteRegEx": "Kingma and Ba.", "year": 2014}, {"title": "Hierarchical question-image co-attention for visual question answering", "author": ["Jiasen Lu", "Jianwei Yang", "Dhruv Batra", "Devi Parikh"], "venue": "arXiv preprint arXiv:1606.00061,", "citeRegEx": "Lu et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Lu et al\\.", "year": 2016}, {"title": "Effective approaches to attentionbased neural machine translation", "author": ["Minh-Thang Luong", "Hieu Pham", "Christopher D Manning"], "venue": "arXiv preprint arXiv:1508.04025,", "citeRegEx": "Luong et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Luong et al\\.", "year": 2015}, {"title": "The stanford corenlp natural language processing toolkit", "author": ["Christopher D Manning", "Mihai Surdeanu", "John Bauer", "Jenny Rose Finkel", "Steven Bethard", "David McClosky"], "venue": "In ACL (System Demonstrations),", "citeRegEx": "Manning et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Manning et al\\.", "year": 2014}, {"title": "Pointer sentinel mixture models", "author": ["Stephen Merity", "Caiming Xiong", "James Bradbury", "Richard Socher"], "venue": "arXiv preprint arXiv:1609.07843,", "citeRegEx": "Merity et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Merity et al\\.", "year": 2016}, {"title": "Glove: Global vectors for word representation", "author": ["Jeffrey Pennington", "Richard Socher", "Christopher D Manning"], "venue": "In EMNLP,", "citeRegEx": "Pennington et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Pennington et al\\.", "year": 2014}, {"title": "Squad: 100,000+ questions for machine comprehension of text", "author": ["Pranav Rajpurkar", "Jian Zhang", "Konstantin Lopyrev", "Percy Liang"], "venue": "arXiv preprint arXiv:1606.05250,", "citeRegEx": "Rajpurkar et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Rajpurkar et al\\.", "year": 2016}, {"title": "Mctest: A challenge dataset for the open-domain machine comprehension of text", "author": ["Matthew Richardson", "Christopher JC Burges", "Erin Renshaw"], "venue": "In EMNLP,", "citeRegEx": "Richardson et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Richardson et al\\.", "year": 2013}, {"title": "Iterative alternating neural attention for machine reading", "author": ["Alessandro Sordoni", "Phillip Bachman", "Yoshua Bengio"], "venue": "arXiv preprint arXiv:1606.02245,", "citeRegEx": "Sordoni et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Sordoni et al\\.", "year": 2016}, {"title": "Dropout: a simple way to prevent neural networks from overfitting", "author": ["Nitish Srivastava", "Geoffrey E Hinton", "Alex Krizhevsky", "Ilya Sutskever", "Ruslan Salakhutdinov"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Srivastava et al\\.,? \\Q1929\\E", "shortCiteRegEx": "Srivastava et al\\.", "year": 1929}, {"title": "Chainer: a next-generation open source framework for deep learning", "author": ["Seiya Tokui", "Kenta Oono", "Shohei Hido", "Justin Clayton"], "venue": "In Proceedings of Workshop on Machine Learning Systems (LearningSys) in The Twenty-ninth Annual Conference on Neural Information Processing Systems (NIPS),", "citeRegEx": "Tokui et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Tokui et al\\.", "year": 2015}, {"title": "Machine comprehension with syntax, frames, and semantics", "author": ["Hai Wang"], "venue": "Volume 2: Short Papers,", "citeRegEx": "Wang and McAllester.,? \\Q2015\\E", "shortCiteRegEx": "Wang and McAllester.", "year": 2015}, {"title": "Learning natural language inference with lstm", "author": ["Shuohang Wang", "Jing Jiang"], "venue": "arXiv preprint arXiv:1512.08849,", "citeRegEx": "Wang and Jiang.,? \\Q2015\\E", "shortCiteRegEx": "Wang and Jiang.", "year": 2015}, {"title": "Machine comprehension using match-lstm and answer pointer", "author": ["Shuohang Wang", "Jing Jiang"], "venue": "arXiv preprint arXiv:1608.07905,", "citeRegEx": "Wang and Jiang.,? \\Q2016\\E", "shortCiteRegEx": "Wang and Jiang.", "year": 2016}, {"title": "End-to-End Reading Comprehension with Dynamic Answer Chunk Ranking", "author": ["Y. Yu", "W. Zhang", "K. Hasan", "M. Yu", "B. Xiang", "B. Zhou"], "venue": null, "citeRegEx": "Yu et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Yu et al\\.", "year": 2016}, {"title": "End-to-end answer chunk extraction and ranking for reading comprehension", "author": ["Yang Yu", "Wei Zhang", "Kazi Hasan", "Mo Yu", "Bing Xiang", "Bowen Zhou"], "venue": "arXiv preprint arXiv:1610.09996v2,", "citeRegEx": "Yu et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Yu et al\\.", "year": 2016}], "referenceMentions": [{"referenceID": 1, "context": "Previous QA datasets tend to be high in quality due to human annotation, but small in size (Berant et al., 2014; Richardson et al., 2013).", "startOffset": 91, "endOffset": 137}, {"referenceID": 16, "context": "Previous QA datasets tend to be high in quality due to human annotation, but small in size (Berant et al., 2014; Richardson et al., 2013).", "startOffset": 91, "endOffset": 137}, {"referenceID": 5, "context": "To address this problem, researchers have developed large-scale datasets through semi-automated techniques (Hermann et al., 2015; Hill et al., 2015).", "startOffset": 107, "endOffset": 148}, {"referenceID": 6, "context": "To address this problem, researchers have developed large-scale datasets through semi-automated techniques (Hermann et al., 2015; Hill et al., 2015).", "startOffset": 107, "endOffset": 148}, {"referenceID": 3, "context": "However, it has been shown that they differ from more natural, human annotated datasets in the types of reasoning required to answer the questions (Chen et al., 2016).", "startOffset": 147, "endOffset": 166}, {"referenceID": 23, "context": "0% (Yu et al., 2016).", "startOffset": 3, "endOffset": 20}, {"referenceID": 1, "context": "Previous QA datasets tend to be high in quality due to human annotation, but small in size (Berant et al., 2014; Richardson et al., 2013). Hence, they did not allow for training data-intensive, expressive models such as deep neural networks. To address this problem, researchers have developed large-scale datasets through semi-automated techniques (Hermann et al., 2015; Hill et al., 2015). Compared to their smaller, hand-annotated counterparts, these QA datasets allow the training of more expressive models. However, it has been shown that they differ from more natural, human annotated datasets in the types of reasoning required to answer the questions (Chen et al., 2016). Recently, Rajpurkar et al. (2016) released the Stanford Question Answering dataset (SQuAD), which is orders of magnitude larger than all previous hand-annotated datasets and has a variety of qualities that culminate in a natural QA task.", "startOffset": 92, "endOffset": 714}, {"referenceID": 1, "context": "Previous QA datasets tend to be high in quality due to human annotation, but small in size (Berant et al., 2014; Richardson et al., 2013). Hence, they did not allow for training data-intensive, expressive models such as deep neural networks. To address this problem, researchers have developed large-scale datasets through semi-automated techniques (Hermann et al., 2015; Hill et al., 2015). Compared to their smaller, hand-annotated counterparts, these QA datasets allow the training of more expressive models. However, it has been shown that they differ from more natural, human annotated datasets in the types of reasoning required to answer the questions (Chen et al., 2016). Recently, Rajpurkar et al. (2016) released the Stanford Question Answering dataset (SQuAD), which is orders of magnitude larger than all previous hand-annotated datasets and has a variety of qualities that culminate in a natural QA task. SQuAD has the desirable quality that answers are spans in a reference document. This constrains answers to the space of all possible spans. However, Rajpurkar et al. (2016) show that the dataset retains a diverse set of answers and requires different forms of logical reasoning, including multi-sentence reasoning.", "startOffset": 92, "endOffset": 1091}, {"referenceID": 13, "context": "We also add a sentinel vector d\u2205 (Merity et al., 2016), which we later show allows the model to not attend to any particular word in the input.", "startOffset": 33, "endOffset": 54}, {"referenceID": 10, "context": "We propose a coattention mechanism that attends to the question and document simultaneously, similar to (Lu et al., 2016), and finally fuses both attention contexts.", "startOffset": 104, "endOffset": 121}, {"referenceID": 4, "context": "Similar to Cui et al. (2016), we also compute the summaries CA of the previous attention contexts in light of each word of the document.", "startOffset": 11, "endOffset": 29}, {"referenceID": 0, "context": "(2016) proposed a hierarchical co-attention model for visual question answering, which achieved state of the art result on the COCO-VQA dataset (Antol et al., 2015).", "startOffset": 144, "endOffset": 164}, {"referenceID": 10, "context": "In (Lu et al., 2016), the co-attention mechanism computes a conditional representation of the image given the question, as well as a conditional representation of the question given the image.", "startOffset": 3, "endOffset": 20}, {"referenceID": 8, "context": "Richardson et al. (2013) proposed two baselines, one that uses simple lexical features such as a sliding window to match bags of words, and another that uses word-distances between words in the question and in the document.", "startOffset": 0, "endOffset": 25}, {"referenceID": 0, "context": "Berant et al. (2014) proposed an alternative approach in which one first learns a structured representation of the entities and relations in the document in the form of a knowledge base, then converts the question to a structured query with which to match the content of the knowledge base.", "startOffset": 0, "endOffset": 21}, {"referenceID": 0, "context": "Berant et al. (2014) proposed an alternative approach in which one first learns a structured representation of the entities and relations in the document in the form of a knowledge base, then converts the question to a structured query with which to match the content of the knowledge base. Wang & McAllester (2015) described a statistical model using frame semantic features as well as syntactic features such as part of speech tags and dependency parses.", "startOffset": 0, "endOffset": 316}, {"referenceID": 0, "context": "Berant et al. (2014) proposed an alternative approach in which one first learns a structured representation of the entities and relations in the document in the form of a knowledge base, then converts the question to a structured query with which to match the content of the knowledge base. Wang & McAllester (2015) described a statistical model using frame semantic features as well as syntactic features such as part of speech tags and dependency parses. Chen et al. (2016) proposed a competitive statistical baseline using a variety of carefully crafted lexical, syntactic, and word order features.", "startOffset": 0, "endOffset": 476}, {"referenceID": 0, "context": "Berant et al. (2014) proposed an alternative approach in which one first learns a structured representation of the entities and relations in the document in the form of a knowledge base, then converts the question to a structured query with which to match the content of the knowledge base. Wang & McAllester (2015) described a statistical model using frame semantic features as well as syntactic features such as part of speech tags and dependency parses. Chen et al. (2016) proposed a competitive statistical baseline using a variety of carefully crafted lexical, syntactic, and word order features. Neural QA Neural attention models have been widely applied for machine comprehension or question-answering in NLP. Hermann et al. (2015) proposed an AttentiveReader model with the release of the CNN/Daily Mail cloze-style question answering dataset.", "startOffset": 0, "endOffset": 739}, {"referenceID": 0, "context": "Berant et al. (2014) proposed an alternative approach in which one first learns a structured representation of the entities and relations in the document in the form of a knowledge base, then converts the question to a structured query with which to match the content of the knowledge base. Wang & McAllester (2015) described a statistical model using frame semantic features as well as syntactic features such as part of speech tags and dependency parses. Chen et al. (2016) proposed a competitive statistical baseline using a variety of carefully crafted lexical, syntactic, and word order features. Neural QA Neural attention models have been widely applied for machine comprehension or question-answering in NLP. Hermann et al. (2015) proposed an AttentiveReader model with the release of the CNN/Daily Mail cloze-style question answering dataset. Hill et al. (2015) released another dataset steming from the children\u2019s book and proposed a window-based memory network.", "startOffset": 0, "endOffset": 871}, {"referenceID": 0, "context": "Berant et al. (2014) proposed an alternative approach in which one first learns a structured representation of the entities and relations in the document in the form of a knowledge base, then converts the question to a structured query with which to match the content of the knowledge base. Wang & McAllester (2015) described a statistical model using frame semantic features as well as syntactic features such as part of speech tags and dependency parses. Chen et al. (2016) proposed a competitive statistical baseline using a variety of carefully crafted lexical, syntactic, and word order features. Neural QA Neural attention models have been widely applied for machine comprehension or question-answering in NLP. Hermann et al. (2015) proposed an AttentiveReader model with the release of the CNN/Daily Mail cloze-style question answering dataset. Hill et al. (2015) released another dataset steming from the children\u2019s book and proposed a window-based memory network. Kadlec et al. (2016) presented a pointer-style attention mechanism but performs only one attention step.", "startOffset": 0, "endOffset": 994}, {"referenceID": 0, "context": "Berant et al. (2014) proposed an alternative approach in which one first learns a structured representation of the entities and relations in the document in the form of a knowledge base, then converts the question to a structured query with which to match the content of the knowledge base. Wang & McAllester (2015) described a statistical model using frame semantic features as well as syntactic features such as part of speech tags and dependency parses. Chen et al. (2016) proposed a competitive statistical baseline using a variety of carefully crafted lexical, syntactic, and word order features. Neural QA Neural attention models have been widely applied for machine comprehension or question-answering in NLP. Hermann et al. (2015) proposed an AttentiveReader model with the release of the CNN/Daily Mail cloze-style question answering dataset. Hill et al. (2015) released another dataset steming from the children\u2019s book and proposed a window-based memory network. Kadlec et al. (2016) presented a pointer-style attention mechanism but performs only one attention step. Sordoni et al. (2016) introduced an iterative neural attention model and applied it to cloze-style machine comprehension tasks.", "startOffset": 0, "endOffset": 1100}, {"referenceID": 0, "context": "Berant et al. (2014) proposed an alternative approach in which one first learns a structured representation of the entities and relations in the document in the form of a knowledge base, then converts the question to a structured query with which to match the content of the knowledge base. Wang & McAllester (2015) described a statistical model using frame semantic features as well as syntactic features such as part of speech tags and dependency parses. Chen et al. (2016) proposed a competitive statistical baseline using a variety of carefully crafted lexical, syntactic, and word order features. Neural QA Neural attention models have been widely applied for machine comprehension or question-answering in NLP. Hermann et al. (2015) proposed an AttentiveReader model with the release of the CNN/Daily Mail cloze-style question answering dataset. Hill et al. (2015) released another dataset steming from the children\u2019s book and proposed a window-based memory network. Kadlec et al. (2016) presented a pointer-style attention mechanism but performs only one attention step. Sordoni et al. (2016) introduced an iterative neural attention model and applied it to cloze-style machine comprehension tasks. Recently, Rajpurkar et al. (2016) released the SQuAD dataset.", "startOffset": 0, "endOffset": 1240}, {"referenceID": 0, "context": "Berant et al. (2014) proposed an alternative approach in which one first learns a structured representation of the entities and relations in the document in the form of a knowledge base, then converts the question to a structured query with which to match the content of the knowledge base. Wang & McAllester (2015) described a statistical model using frame semantic features as well as syntactic features such as part of speech tags and dependency parses. Chen et al. (2016) proposed a competitive statistical baseline using a variety of carefully crafted lexical, syntactic, and word order features. Neural QA Neural attention models have been widely applied for machine comprehension or question-answering in NLP. Hermann et al. (2015) proposed an AttentiveReader model with the release of the CNN/Daily Mail cloze-style question answering dataset. Hill et al. (2015) released another dataset steming from the children\u2019s book and proposed a window-based memory network. Kadlec et al. (2016) presented a pointer-style attention mechanism but performs only one attention step. Sordoni et al. (2016) introduced an iterative neural attention model and applied it to cloze-style machine comprehension tasks. Recently, Rajpurkar et al. (2016) released the SQuAD dataset. Different from cloze-style queries, answers include non-entities and longer phrases, and questions are more realistic. For SQuAD, Wang & Jiang (2016) proposed an end-to-end neural network model that consists of a Match-LSTM encoder, originally introduced in Wang & Jiang (2015), and a pointer network decoder (Vinyals et al.", "startOffset": 0, "endOffset": 1418}, {"referenceID": 0, "context": "Berant et al. (2014) proposed an alternative approach in which one first learns a structured representation of the entities and relations in the document in the form of a knowledge base, then converts the question to a structured query with which to match the content of the knowledge base. Wang & McAllester (2015) described a statistical model using frame semantic features as well as syntactic features such as part of speech tags and dependency parses. Chen et al. (2016) proposed a competitive statistical baseline using a variety of carefully crafted lexical, syntactic, and word order features. Neural QA Neural attention models have been widely applied for machine comprehension or question-answering in NLP. Hermann et al. (2015) proposed an AttentiveReader model with the release of the CNN/Daily Mail cloze-style question answering dataset. Hill et al. (2015) released another dataset steming from the children\u2019s book and proposed a window-based memory network. Kadlec et al. (2016) presented a pointer-style attention mechanism but performs only one attention step. Sordoni et al. (2016) introduced an iterative neural attention model and applied it to cloze-style machine comprehension tasks. Recently, Rajpurkar et al. (2016) released the SQuAD dataset. Different from cloze-style queries, answers include non-entities and longer phrases, and questions are more realistic. For SQuAD, Wang & Jiang (2016) proposed an end-to-end neural network model that consists of a Match-LSTM encoder, originally introduced in Wang & Jiang (2015), and a pointer network decoder (Vinyals et al.", "startOffset": 0, "endOffset": 1546}, {"referenceID": 0, "context": "Berant et al. (2014) proposed an alternative approach in which one first learns a structured representation of the entities and relations in the document in the form of a knowledge base, then converts the question to a structured query with which to match the content of the knowledge base. Wang & McAllester (2015) described a statistical model using frame semantic features as well as syntactic features such as part of speech tags and dependency parses. Chen et al. (2016) proposed a competitive statistical baseline using a variety of carefully crafted lexical, syntactic, and word order features. Neural QA Neural attention models have been widely applied for machine comprehension or question-answering in NLP. Hermann et al. (2015) proposed an AttentiveReader model with the release of the CNN/Daily Mail cloze-style question answering dataset. Hill et al. (2015) released another dataset steming from the children\u2019s book and proposed a window-based memory network. Kadlec et al. (2016) presented a pointer-style attention mechanism but performs only one attention step. Sordoni et al. (2016) introduced an iterative neural attention model and applied it to cloze-style machine comprehension tasks. Recently, Rajpurkar et al. (2016) released the SQuAD dataset. Different from cloze-style queries, answers include non-entities and longer phrases, and questions are more realistic. For SQuAD, Wang & Jiang (2016) proposed an end-to-end neural network model that consists of a Match-LSTM encoder, originally introduced in Wang & Jiang (2015), and a pointer network decoder (Vinyals et al., 2015); Yu et al. (2016) introduced a dynamic chunk reader, a neural reading comprehension model that extracts a set of answer candidates of variable lengths from the document and ranks them to answer the question.", "startOffset": 0, "endOffset": 1618}, {"referenceID": 0, "context": "Berant et al. (2014) proposed an alternative approach in which one first learns a structured representation of the entities and relations in the document in the form of a knowledge base, then converts the question to a structured query with which to match the content of the knowledge base. Wang & McAllester (2015) described a statistical model using frame semantic features as well as syntactic features such as part of speech tags and dependency parses. Chen et al. (2016) proposed a competitive statistical baseline using a variety of carefully crafted lexical, syntactic, and word order features. Neural QA Neural attention models have been widely applied for machine comprehension or question-answering in NLP. Hermann et al. (2015) proposed an AttentiveReader model with the release of the CNN/Daily Mail cloze-style question answering dataset. Hill et al. (2015) released another dataset steming from the children\u2019s book and proposed a window-based memory network. Kadlec et al. (2016) presented a pointer-style attention mechanism but performs only one attention step. Sordoni et al. (2016) introduced an iterative neural attention model and applied it to cloze-style machine comprehension tasks. Recently, Rajpurkar et al. (2016) released the SQuAD dataset. Different from cloze-style queries, answers include non-entities and longer phrases, and questions are more realistic. For SQuAD, Wang & Jiang (2016) proposed an end-to-end neural network model that consists of a Match-LSTM encoder, originally introduced in Wang & Jiang (2015), and a pointer network decoder (Vinyals et al., 2015); Yu et al. (2016) introduced a dynamic chunk reader, a neural reading comprehension model that extracts a set of answer candidates of variable lengths from the document and ranks them to answer the question. Lu et al. (2016) proposed a hierarchical co-attention model for visual question answering, which achieved state of the art result on the COCO-VQA dataset (Antol et al.", "startOffset": 0, "endOffset": 1825}, {"referenceID": 2, "context": "end positions of the answer span in a single pass (Wang & Jiang, 2016), we iteratively update the start and end positions in a similar fashion to the Iterative Conditional Modes algorithm (Besag, 1986).", "startOffset": 188, "endOffset": 201}, {"referenceID": 12, "context": "To preprocess the corpus, we use the tokenizer from Stanford CoreNLP (Manning et al., 2014).", "startOffset": 69, "endOffset": 91}, {"referenceID": 14, "context": "We use as GloVe word vectors pretrained on the 840B Common Crawl corpus (Pennington et al., 2014).", "startOffset": 72, "endOffset": 97}, {"referenceID": 19, "context": "All models are implemented and trained with Chainer (Tokui et al., 2015).", "startOffset": 52, "endOffset": 72}, {"referenceID": 23, "context": "3 Dynamic Chunk Reader (Yu et al., 2016) 62.", "startOffset": 23, "endOffset": 40}, {"referenceID": 15, "context": "3 Baseline (Rajpurkar et al., 2016) 40.", "startOffset": 11, "endOffset": 35}, {"referenceID": 15, "context": "0 Human (Rajpurkar et al., 2016) 81.", "startOffset": 8, "endOffset": 32}, {"referenceID": 11, "context": "Intuitively, we expect the model performance to deteriorate with longer examples, as is the case with neural machine translation (Luong et al., 2015).", "startOffset": 129, "endOffset": 149}, {"referenceID": 23, "context": "In Figure 7, we note that the mean F1 of DCN exceeds those of previous systems (Wang & Jiang, 2016; Yu et al., 2016) across all question types.", "startOffset": 79, "endOffset": 116}], "year": 2016, "abstractText": "Several deep learning models have been proposed for question answering. However, due to their single-pass nature, they have no way to recover from local maxima corresponding to incorrect answers. To address this problem, we introduce the Dynamic Coattention Network (DCN) for question answering. The DCN first fuses co-dependent representations of the question and the document in order to focus on relevant parts of both. Then a dynamic pointing decoder iterates over potential answer spans. This iterative procedure enables the model to recover from initial local maxima corresponding to incorrect answers. On the Stanford question answering dataset, a single DCN model improves the previous state of the art from 71.0% F1 to 75.9%, while a DCN ensemble obtains 80.4% F1.", "creator": "LaTeX with hyperref package"}}}