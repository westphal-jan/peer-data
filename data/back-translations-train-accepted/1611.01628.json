{"id": "1611.01628", "review": {"conference": "acl", "VERSION": "v1", "DATE_OF_SUBMISSION": "5-Nov-2016", "title": "Reference-Aware Language Models", "abstract": "We propose a general class of language models that treat reference as an explicit stochastic latent variable. This architecture allows models to create mentions of entities and their attributes by accessing external databases (required by, e.g., dialogue generation and recipe generation) and internal state (required by, e.g. language models which are aware of coreference). This facilitates the incorporation of information that can be accessed in predictable locations in databases or discourse context, even when the targets of the reference may be rare words. Experiments on three tasks shows our model variants based on deterministic attention.", "histories": [["v1", "Sat, 5 Nov 2016 10:55:37 GMT  (314kb,D)", "http://arxiv.org/abs/1611.01628v1", "iclr submission"], ["v2", "Fri, 11 Nov 2016 22:51:28 GMT  (313kb,D)", "http://arxiv.org/abs/1611.01628v2", "iclr submission"], ["v3", "Tue, 7 Feb 2017 20:33:12 GMT  (311kb,D)", "http://arxiv.org/abs/1611.01628v3", "9 pages"], ["v4", "Tue, 8 Aug 2017 17:05:33 GMT  (967kb,D)", "http://arxiv.org/abs/1611.01628v4", "emnlp camera ready"], ["v5", "Wed, 9 Aug 2017 00:39:51 GMT  (967kb,D)", "http://arxiv.org/abs/1611.01628v5", "emnlp camera ready"]], "COMMENTS": "iclr submission", "reviews": [], "SUBJECTS": "cs.CL", "authors": ["zichao yang", "phil blunsom", "chris dyer", "wang ling"], "accepted": true, "id": "1611.01628"}, "pdf": {"name": "1611.01628.pdf", "metadata": {"source": "CRF", "title": "REFERENCE-AWARE LANGUAGE MODELS", "authors": ["Zichao Yang", "Phil Blunsom", "Chris Dyer", "Wang Ling"], "emails": ["zichaoy@cs.cmu.edu,", "pblunsom@google.com", "cdyer@google.com", "lingwang@google.com"], "sections": [{"heading": "1 INTRODUCTION", "text": "In fact, most of them are able to survive by themselves if they do not feel able to do so."}, {"heading": "2 REFERENCE-AWARE LANGUAGE MODELS", "text": "Here we propose a general framework for reference-conscious language models. We refer to each document as a series of characters x1,.., xL, where L is the number of characters in the document. Our goal is to determine the probabilities p (xi | ci) for each word in the document on the basis of its previous context ci = x1,..., xi \u2212 1. Unlike traditional neural language models, we introduce a variable zi at each position, which determines the decision from which source xi is generated. The conditional number is then likely to result from the marginalization of this variable: p (xi | ci) = p (xi | zi, ci) p (zi | ci). (1) In dialog modelling and recipe generation zi is simply taken on the basis of values in {0, 1}. Where zi = 1 means xi is generated as a reference, either to a database entry or to an element in a list. However, zi can also be defined as a distribution over previous units, so that the model is usually not focused on the probability of the word."}, {"heading": "2.1 DIALOGUE MODEL WITH DATABASE SUPPORT", "text": "We first apply our model to task-oriented dialog systems in the area of restaurant recommendations and work on the data of the second Dialogue State Tracking Challenge (DSTC2) (Henderson et al., 2014). Table 1 is a sample dialog from this data set. From this example, we can observe that users receive recommendations from restaurants based on queries indicating the area, price and food type of the restaurant. We can support the decisions of the system by including a mechanism that allows the model to query the database that allows the model to find restaurants that satisfy the queries of users. Thus, we searched TripAdvisor for restaurants in the Cambridge area where the dialog data set was collected. We then remove restaurants that do not appear in the data set and create a database of 109 entries with restaurants and their attributes (e.g. type of food). A sample of our database is shown in Table. 2. We can find that each restaurant contains 6 attributes referred to in the dialogues in general."}, {"heading": "2.1.1 DIALOGUE MODEL", "text": "In this sense, it is important that we are able to abide by the rules that we have imposed on ourselves, and that we must abide by the rules that we must abide by in order to abide by them. \"And further:\" We must abide by the rules. \"And further:\" We must abide by the rules that we abide by. \"And further:\" We must abide by the rules. \"And further:\" We must abide by the rules. \"And further:\" We must abide by the rules. \"And further:\" We must abide by the rules that we abide by. \"And further:\" We must abide by the rules. \"We must abide by the rules.\" We must abide by the rules. \"We must abide by the rules.\" We must abide by the rules. \"We must abide by the rules.\" We must abide by the rules. \"We must abide by the rules.\" We must abide by the rules. \"We must abide by the rules.\" We must abide by the rules. \""}, {"heading": "2.2 INCORPORATING TABLE ATTENTION", "text": "We now expand the attention model so that attention can be calculated via a table, so that the model q can condition the generation via a database. We designate a table with R rows and C columns as {fr, c}, r-th Attribute.Table Encoding: To encode the table, we build an attribute vector gc for each fr cell, c of the table, we associate it with the corresponding attribute gc and then feed it through a single-layer MLP as follows: gc = Wesc = tanh (W [Wefr, c, gc].Table attribute fr, c of the table, we associate it with the corresponding attribute gc and feed it with a single-layer MLP as follows."}, {"heading": "2.2.1 INCORPORATING TABLE POINTER NETWORKS", "text": "Indeed it is so that the pseudo-pseudo-pseudo-pseudo-pseudo-pseudo-pseudo-pseudo-pseudo-pseudo-pseudo-pseudo-pseudo-pseudo-pseudo-pseudo-pseudo-pseudo-pseudo-pseudo-pseudo-pseudo-pseudo-pseudo-pseudo-pseudo-pseudo-pseudo-pseudo-pseudo-pseudo-pseudo-pseudo-pseudo-pseudo-pseudo-pseudo-pseudo-pseudo-pseudo-pseudo-pseudo-pseudo-pseudo-pseudo-pseudo-pseudo-pseudo-pseudo-pseudo-pseudo-pseudo-pseudo-pseudo-pseudo-pseudo-pseudo-pseudo-pseudo-pseudo-pseudo-pseudo-pseudo-pseudo-pseudo-pseudo-pseudo-pseudo-pseudo-pseudo-pseudo-pseudo-pseudo-pseudo-pseudo-pseudo-pseudo-pseudo-pseudo-pseudo-pseudo-pseudo-pseudo-pseudo-pseudo-pseudo-pseudo-pseudo-pseudo-pseudo-pseudo-pseudo-pseudo-pseudo-pseudo-pseudo-pseudo-pseudo-pseudo-pseudo-pseudo-pseudo-pseudo-pseudo-pseudo-pseudo-pseudo-pseudo-pseudo-pseudo-pseudo-pseudo-pseudo-pseudo-pseudo-pseudo-pseudo-pseudo-pseudo-pseudo-pseudo-pseudo-pseudo-pseudo-pseudo-pseudo-"}, {"heading": "2.4 COREFERENCE BASED LANGUAGE MODEL", "text": "Finally, we build a language model that uses coreference links to refer to previous words. Before creating a word, we first decide whether it is an entity. If so, we decide which entity this mention belongs to, then we create the word based on that entity. Let's call the document X = {xi} Li = 1, and the entities are E = {ei} Ni = 1, each entity has Mi \u2212 1, so {xmij} Mi j = 1 refer to the same entity. We use an LSTM to model the document, the hidden state of each entity is hi = LSTM (Wexi, hi \u2212 1). We use a sentence er = {he0, he1..., heM} to keep track of the entity where Hej of the entity is."}, {"heading": "3 EXPERIMENTS", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "3.1 DATA SETS", "text": "For the task of dialog modeling, we use the DSTC2 dataset. For recipe creation, we used a crawl of all recipes from www.allrecipes.com, and for the Coref LM, we use the Xinhua News part of the Gigaword v5 corpus. Details are given in Appendix A."}, {"heading": "3.2 MODEL TRAINING AND EVALUATION", "text": "We train all models with simple stochastic gradient drop-off with clipping. We use a single-layer LSTM for all RNN components. Hyper parameters are selected by means of a raster search based on the validation set. We use drop-out after input embedding and LSTM output. The learning rate is selected from [0,1, 0,2, 0,5, 1], the maximum gradient standard is selected from [1, 2, 5, 10] and the fall ratio is [0,2, 0,3, 0,5]. The lot size and LSTM dimension size differ slightly for different tasks so that the model fits into memory. The number of epochs to train is different for each task and we lower the learning rate after reaching a certain number of epochs. We report perplexity per word for all tasks, in particular we report perplexity of all words, words that can be generated from reference and non-reference words."}, {"heading": "3.3 RESULTS AND ANALYSIS", "text": "The results for dialog, recipe generation, and Coref language model are shown in Table 4, 5, and 6, respectively. We can see from Table 4 that models based on the table generally perform better at predicting table tokens; the table pointer has the lowest perplexity for tokens in the table; because the table token rarely appears in the dialog, the general perplexity does not differ much, and the non-table tokens are similar; the attention mechanism above the table improves the perplexity of table tokens compared to the basic seq2seq model, but not as well as the direct reference to cells in the table; as expected, the use of sentence pointers without sentence attention improves significantly; surprisingly, the latent table pointer performs much worse than the table pointer; we also measure the perplexity of table tokens that appear only in the test sentence."}, {"heading": "4 RELATED WORK", "text": "Recently, there has been great progress in modeling languages based on neural networks, including language modeling, which was invested in our database in 2016 (Mikolov et al., 2010; Jozefowicz et al., 2016), machine translation (Sutskeveret al., 2014; Bahdanau et al., 2014), answering questions (Hermann et al., 2015), etc. Based on the success of seq2seq models, neural networks are used in modeling Chit Chat dialogues (Li et al., 2016; Vinyals & Le, 2015; Sordoni et al., 2015; Serban et al., 2016; Shang et al., 2015) and task-oriented dialogue (Wen et al., 2015; Bordes & Weston, 2016; Wen et al., 2016). Most of the Chit Chat Chat dialog models are simply based on seq2seq models."}, {"heading": "5 CONCLUSION", "text": "Our model can also learn the decision by treating it as a latent variable. We demonstrate through three tasks, tabular dialog modeling, recipe generation, and Coref-based LM that our model performs better than a task-oriented model that does not explicitly take this decision into account. There are several directions to explore further within our framework. The current evaluation method is based on helplessness and BLEU. In task-oriented dialogs, we can also try out human evaluation to see if the model can accurately answer users \"questions. It is also interesting to use reinforcement to learn the actions at each step."}, {"heading": "A DATASETS AND PREPROCESSING", "text": "There are about 3200 dialogs in total. As this is a small record, we use 5-fold cross-validation and report the average result across the 5 partitions. There may be several tokens in each table cell, for example in Table.2, the name, address, zip code and phone number have multiple tokens. We replace the tokens in each cell with NAME j, POSTCODE j, PHONE j, PHONE j, we replace them with a special token. For the name, zip code and phone number of the j-th series, we replace the tokens in each cell with NAME j, POSTCODE j, PHONE j, PHONE j, we replace them with an empty token EMPTY. We do a string match in the transcription and replace the corresponding tokens in transcripts."}], "references": [{"title": "Neural machine translation by jointly learning to align and translate", "author": ["Dzmitry Bahdanau", "Kyunghyun Cho", "Yoshua Bengio"], "venue": "CoRR, abs/1409.0473,", "citeRegEx": "Bahdanau et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Bahdanau et al\\.", "year": 2014}, {"title": "Learning end-to-end goal-oriented dialog", "author": ["Antoine Bordes", "Jason Weston"], "venue": "arXiv preprint arXiv:1605.07683,", "citeRegEx": "Bordes and Weston.,? \\Q2016\\E", "shortCiteRegEx": "Bordes and Weston.", "year": 2016}, {"title": "Incorporating copying mechanism in sequence-to-sequence learning", "author": ["Jiatao Gu", "Zhengdong Lu", "Hang Li", "Victor O.K. Li"], "venue": "CoRR, abs/1603.06393,", "citeRegEx": "Gu et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Gu et al\\.", "year": 2016}, {"title": "Coreference resolution in a modular, entity-centered model", "author": ["Aria Haghighi", "Dan Klein"], "venue": "In Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the Association for Computational Linguistics,", "citeRegEx": "Haghighi and Klein.,? \\Q2010\\E", "shortCiteRegEx": "Haghighi and Klein.", "year": 2010}, {"title": "Dialog state tracking challenge 2 ", "author": ["Matthew Henderson", "Blaise Thomson", "Jason Williams"], "venue": null, "citeRegEx": "Henderson et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Henderson et al\\.", "year": 2014}, {"title": "Teaching machines to read and comprehend", "author": ["Karl Moritz Hermann", "Tomas Kocisky", "Edward Grefenstette", "Lasse Espeholt", "Will Kay", "Mustafa Suleyman", "Phil Blunsom"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Hermann et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Hermann et al\\.", "year": 2015}, {"title": "Long short-term memory", "author": ["Sepp Hochreiter", "J\u00fcrgen Schmidhuber"], "venue": "Neural Comput.,", "citeRegEx": "Hochreiter and Schmidhuber.,? \\Q1997\\E", "shortCiteRegEx": "Hochreiter and Schmidhuber.", "year": 1997}, {"title": "Document context language models", "author": ["Yangfeng Ji", "Trevor Cohn", "Lingpeng Kong", "Chris Dyer", "Jacob Eisenstein"], "venue": "arXiv preprint arXiv:1511.03962,", "citeRegEx": "Ji et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Ji et al\\.", "year": 2015}, {"title": "Exploring the limits of language modeling", "author": ["Rafal Jozefowicz", "Oriol Vinyals", "Mike Schuster", "Noam Shazeer", "Yonghui Wu"], "venue": "arXiv preprint arXiv:1602.02410,", "citeRegEx": "Jozefowicz et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Jozefowicz et al\\.", "year": 2016}, {"title": "Globally coherent text generation with neural checklist models", "author": ["Chlo\u00e9 Kiddon", "Luke Zettlemoyer", "Yejin Choi"], "venue": "In Proc. EMNLP,", "citeRegEx": "Kiddon et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Kiddon et al\\.", "year": 2016}, {"title": "Deep reinforcement learning for dialogue generation", "author": ["Jiwei Li", "Will Monroe", "Alan Ritter", "Michel Galley", "Jianfeng Gao", "Dan Jurafsky"], "venue": "In Proc. EMNLP,", "citeRegEx": "Li et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Li et al\\.", "year": 2016}, {"title": "Latent predictor networks for code generation", "author": ["Wang Ling", "Edward Grefenstette", "Karl Moritz Hermann", "Tom\u00e1\u0161 Ko\u010disk\u00fd", "Andrew Senior", "Fumin Wang", "Phil Blunsom"], "venue": "In Proc. ACL,", "citeRegEx": "Ling et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Ling et al\\.", "year": 2016}, {"title": "Pointer sentinel mixture models", "author": ["Stephen Merity", "Caiming Xiong", "James Bradbury", "Richard Socher"], "venue": "arXiv preprint arXiv:1609.07843,", "citeRegEx": "Merity et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Merity et al\\.", "year": 2016}, {"title": "Recurrent neural network based language model", "author": ["Tomas Mikolov", "Martin Karafi\u00e1t", "Lukas Burget", "Jan Cernock\u1ef3", "Sanjeev Khudanpur"], "venue": "In Interspeech,", "citeRegEx": "Mikolov et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Mikolov et al\\.", "year": 2010}, {"title": "English gigaword fifth edition, linguistic data consortium", "author": ["Robert Parker", "David Graff", "Junbo Kong", "Ke Chen", "Kazuaki Maeda"], "venue": "Technical report,", "citeRegEx": "Parker et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Parker et al\\.", "year": 2011}, {"title": "Building end-to-end dialogue systems using generative hierarchical neural network models", "author": ["Iulian V Serban", "Alessandro Sordoni", "Yoshua Bengio", "Aaron Courville", "Joelle Pineau"], "venue": "In Proceedings of the 30th AAAI Conference on Artificial Intelligence", "citeRegEx": "Serban et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Serban et al\\.", "year": 2016}, {"title": "Neural responding machine for short-text conversation", "author": ["Lifeng Shang", "Zhengdong Lu", "Hang Li"], "venue": "arXiv preprint arXiv:1503.02364,", "citeRegEx": "Shang et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Shang et al\\.", "year": 2015}, {"title": "A neural network approach to context-sensitive generation of conversational responses", "author": ["Alessandro Sordoni", "Michel Galley", "Michael Auli", "Chris Brockett", "Yangfeng Ji", "Meg Mitchell", "JianYun Nie", "Jianfeng Gao", "Bill Dolan"], "venue": "In Proc. NAACL,", "citeRegEx": "Sordoni et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Sordoni et al\\.", "year": 2015}, {"title": "Sequence to sequence learning with neural networks. In Advances in neural information processing", "author": ["Ilya Sutskever", "Oriol Vinyals", "Quoc V Le"], "venue": null, "citeRegEx": "Sutskever et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Sutskever et al\\.", "year": 2014}, {"title": "A neural conversational model", "author": ["Oriol Vinyals", "Quoc V. Le"], "venue": "In Proc. ICML Deep Learning Workshop,", "citeRegEx": "Vinyals and Le.,? \\Q2015\\E", "shortCiteRegEx": "Vinyals and Le.", "year": 2015}, {"title": "Larger-context language modelling", "author": ["Tian Wang", "Kyunghyun Cho"], "venue": "arXiv preprint arXiv:1511.03729,", "citeRegEx": "Wang and Cho.,? \\Q2015\\E", "shortCiteRegEx": "Wang and Cho.", "year": 2015}, {"title": "Semantically conditioned LSTM-based natural language generation for spoken dialogue systems", "author": ["Tsung-Hsien Wen", "Milica Gasic", "Nikola Mrksic", "Pei-hao Su", "David Vandyke", "Steve J. Young"], "venue": "In Proc. EMNLP,", "citeRegEx": "Wen et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Wen et al\\.", "year": 2015}, {"title": "A network-based end-to-end trainable task-oriented dialogue system", "author": ["Tsung-Hsien Wen", "Milica Gasic", "Nikola Mrksic", "Lina M Rojas-Barahona", "Pei-Hao Su", "Stefan Ultes", "David Vandyke", "Steve Young"], "venue": "arXiv preprint arXiv:1604.04562,", "citeRegEx": "Wen et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Wen et al\\.", "year": 2016}, {"title": "End-to-end lstm-based dialog control optimized with supervised and reinforcement learning", "author": ["Jason D Williams", "Geoffrey Zweig"], "venue": "arXiv preprint arXiv:1606.01269,", "citeRegEx": "Williams and Zweig.,? \\Q2016\\E", "shortCiteRegEx": "Williams and Zweig.", "year": 2016}, {"title": "Learning global features for coreference resolution", "author": ["Sam Wiseman", "Alexander M Rush", "Stuart M Shieber"], "venue": "arXiv preprint arXiv:1604.03035,", "citeRegEx": "Wiseman et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Wiseman et al\\.", "year": 2016}, {"title": "Pomdp-based statistical spoken dialog systems: A review", "author": ["Steve Young", "Milica Ga\u0161i\u0107", "Blaise Thomson", "Jason D Williams"], "venue": "Proceedings of the IEEE,", "citeRegEx": "Young et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Young et al\\.", "year": 2013}], "referenceMentions": [{"referenceID": 25, "context": "One example is in task oriented dialogue where access to a database is necessary to answer a user\u2019s query (Young et al., 2013; Li et al., 2016; Vinyals & Le, 2015; Wen et al., 2015; Sordoni et al., 2015; Serban et al., 2016; Bordes & Weston, 2016; Williams & Zweig, 2016; Shang et al., 2015; Wen et al., 2016).", "startOffset": 106, "endOffset": 309}, {"referenceID": 10, "context": "One example is in task oriented dialogue where access to a database is necessary to answer a user\u2019s query (Young et al., 2013; Li et al., 2016; Vinyals & Le, 2015; Wen et al., 2015; Sordoni et al., 2015; Serban et al., 2016; Bordes & Weston, 2016; Williams & Zweig, 2016; Shang et al., 2015; Wen et al., 2016).", "startOffset": 106, "endOffset": 309}, {"referenceID": 21, "context": "One example is in task oriented dialogue where access to a database is necessary to answer a user\u2019s query (Young et al., 2013; Li et al., 2016; Vinyals & Le, 2015; Wen et al., 2015; Sordoni et al., 2015; Serban et al., 2016; Bordes & Weston, 2016; Williams & Zweig, 2016; Shang et al., 2015; Wen et al., 2016).", "startOffset": 106, "endOffset": 309}, {"referenceID": 17, "context": "One example is in task oriented dialogue where access to a database is necessary to answer a user\u2019s query (Young et al., 2013; Li et al., 2016; Vinyals & Le, 2015; Wen et al., 2015; Sordoni et al., 2015; Serban et al., 2016; Bordes & Weston, 2016; Williams & Zweig, 2016; Shang et al., 2015; Wen et al., 2016).", "startOffset": 106, "endOffset": 309}, {"referenceID": 15, "context": "One example is in task oriented dialogue where access to a database is necessary to answer a user\u2019s query (Young et al., 2013; Li et al., 2016; Vinyals & Le, 2015; Wen et al., 2015; Sordoni et al., 2015; Serban et al., 2016; Bordes & Weston, 2016; Williams & Zweig, 2016; Shang et al., 2015; Wen et al., 2016).", "startOffset": 106, "endOffset": 309}, {"referenceID": 16, "context": "One example is in task oriented dialogue where access to a database is necessary to answer a user\u2019s query (Young et al., 2013; Li et al., 2016; Vinyals & Le, 2015; Wen et al., 2015; Sordoni et al., 2015; Serban et al., 2016; Bordes & Weston, 2016; Williams & Zweig, 2016; Shang et al., 2015; Wen et al., 2016).", "startOffset": 106, "endOffset": 309}, {"referenceID": 22, "context": "One example is in task oriented dialogue where access to a database is necessary to answer a user\u2019s query (Young et al., 2013; Li et al., 2016; Vinyals & Le, 2015; Wen et al., 2015; Sordoni et al., 2015; Serban et al., 2016; Bordes & Weston, 2016; Williams & Zweig, 2016; Shang et al., 2015; Wen et al., 2016).", "startOffset": 106, "endOffset": 309}, {"referenceID": 9, "context": "Secondly, many models need to refer to a list of items (Kiddon et al., 2016; Wen et al., 2015).", "startOffset": 55, "endOffset": 94}, {"referenceID": 21, "context": "Secondly, many models need to refer to a list of items (Kiddon et al., 2016; Wen et al., 2015).", "startOffset": 55, "endOffset": 94}, {"referenceID": 9, "context": "In the task of recipe generation from a list of ingredients (Kiddon et al., 2016), the generation of the recipe will frequently reference these items.", "startOffset": 60, "endOffset": 81}, {"referenceID": 13, "context": "Finally, we address references within a document (Mikolov et al., 2010; Ji et al., 2015; Wang & Cho, 2015), as the generation of words will ofter refer to previously generated words.", "startOffset": 49, "endOffset": 106}, {"referenceID": 7, "context": "Finally, we address references within a document (Mikolov et al., 2010; Ji et al., 2015; Wang & Cho, 2015), as the generation of words will ofter refer to previously generated words.", "startOffset": 49, "endOffset": 106}, {"referenceID": 0, "context": "Selecting an entity in context is similar to familiar models of attention (Bahdanau et al., 2014), but rather than being a deterministic function that reweights representations of elements in the context, it is treated as a distribution over contextual elements which are stochastically selected and then copied or, if the task warrants it, transformed (e.", "startOffset": 74, "endOffset": 97}, {"referenceID": 4, "context": "1 DIALOGUE MODEL WITH DATABASE SUPPORT We first apply our model on task-oriented dialogue systems in the domain of restaurant recommendations, and work on the data set from the second Dialogue State Tracking Challenge (DSTC2) (Henderson et al., 2014).", "startOffset": 226, "endOffset": 250}, {"referenceID": 15, "context": "Figure 2: Hierarchical RNN Seq2Seq model We build a model based on the hierarchical RNN model described in (Serban et al., 2016), as in dialogues, the generation of the response is not only dependent on the previous sentence, but on all sentences leading to the response.", "startOffset": 107, "endOffset": 128}, {"referenceID": 0, "context": "A full description of this operation is described in (Bahdanau et al., 2014).", "startOffset": 53, "endOffset": 76}, {"referenceID": 24, "context": "Figure 5: Coreference based language model, example taken from Wiseman et al. (2016). Word generation: At each time step before generating the next word, we predict whether the word is an entity mention: p(vi|hi\u22121, h) = ATTN(h, hi\u22121), di = \u2211", "startOffset": 63, "endOffset": 85}, {"referenceID": 13, "context": "Recently, there has been great progresses in modeling languages based on neural network, including language modeling (Mikolov et al., 2010; Jozefowicz et al., 2016), machine translation (Sutskever", "startOffset": 117, "endOffset": 164}, {"referenceID": 8, "context": "Recently, there has been great progresses in modeling languages based on neural network, including language modeling (Mikolov et al., 2010; Jozefowicz et al., 2016), machine translation (Sutskever", "startOffset": 117, "endOffset": 164}, {"referenceID": 5, "context": ", 2014), question answering (Hermann et al., 2015) etc.", "startOffset": 28, "endOffset": 50}, {"referenceID": 10, "context": "Based on the success of seq2seq models, neural networks are applied in modeling chit-chat dialogue (Li et al., 2016; Vinyals & Le, 2015; Sordoni et al., 2015; Serban et al., 2016; Shang et al., 2015) and task oriented dialogue (Wen et al.", "startOffset": 99, "endOffset": 199}, {"referenceID": 17, "context": "Based on the success of seq2seq models, neural networks are applied in modeling chit-chat dialogue (Li et al., 2016; Vinyals & Le, 2015; Sordoni et al., 2015; Serban et al., 2016; Shang et al., 2015) and task oriented dialogue (Wen et al.", "startOffset": 99, "endOffset": 199}, {"referenceID": 15, "context": "Based on the success of seq2seq models, neural networks are applied in modeling chit-chat dialogue (Li et al., 2016; Vinyals & Le, 2015; Sordoni et al., 2015; Serban et al., 2016; Shang et al., 2015) and task oriented dialogue (Wen et al.", "startOffset": 99, "endOffset": 199}, {"referenceID": 16, "context": "Based on the success of seq2seq models, neural networks are applied in modeling chit-chat dialogue (Li et al., 2016; Vinyals & Le, 2015; Sordoni et al., 2015; Serban et al., 2016; Shang et al., 2015) and task oriented dialogue (Wen et al.", "startOffset": 99, "endOffset": 199}, {"referenceID": 21, "context": ", 2015) and task oriented dialogue (Wen et al., 2015; Bordes & Weston, 2016; Williams & Zweig, 2016; Wen et al., 2016).", "startOffset": 35, "endOffset": 118}, {"referenceID": 22, "context": ", 2015) and task oriented dialogue (Wen et al., 2015; Bordes & Weston, 2016; Williams & Zweig, 2016; Wen et al., 2016).", "startOffset": 35, "endOffset": 118}, {"referenceID": 9, "context": "Previous work on recipe generation from ingredients was proposed in (Kiddon et al., 2016).", "startOffset": 68, "endOffset": 89}, {"referenceID": 13, "context": "Context dependent language models (Mikolov et al., 2010; Ji et al., 2015; Wang & Cho, 2015) are proposed to capture long term dependency of text.", "startOffset": 34, "endOffset": 91}, {"referenceID": 7, "context": "Context dependent language models (Mikolov et al., 2010; Ji et al., 2015; Wang & Cho, 2015) are proposed to capture long term dependency of text.", "startOffset": 34, "endOffset": 91}, {"referenceID": 24, "context": "There are also lots of works on coreference resolution (Haghighi & Klein, 2010; Wiseman et al., 2016).", "startOffset": 55, "endOffset": 101}, {"referenceID": 2, "context": "Much effort has been invested in embedding a copying mechanism for neural models (G\u00fcl\u00e7ehre et al., 2016; Gu et al., 2016; Ling et al., 2016).", "startOffset": 81, "endOffset": 140}, {"referenceID": 11, "context": "Much effort has been invested in embedding a copying mechanism for neural models (G\u00fcl\u00e7ehre et al., 2016; Gu et al., 2016; Ling et al., 2016).", "startOffset": 81, "endOffset": 140}, {"referenceID": 12, "context": "Our models are similar to models proposed in (Ahn et al., 2016; Merity et al., 2016), where the generation of each word can be conditioned on a particular entry in a knowledge lists and previous words.", "startOffset": 45, "endOffset": 84}], "year": 2017, "abstractText": "We propose a general class of language models that treat reference as an explicit stochastic latent variable. This architecture allows models to create mentions of entities and their attributes by accessing external databases (required by, e.g., dialogue generation and recipe generation) and internal state (required by, e.g. language models which are aware of coreference). This facilitates the incorporation of information that can be accessed in predictable locations in databases or discourse context, even when the targets of the reference may be rare words. Experiments on three tasks show our model variants outperform models based on deterministic attention.", "creator": "LaTeX with hyperref package"}}}