{"id": "1611.01724", "review": {"conference": "iclr", "VERSION": "v1", "DATE_OF_SUBMISSION": "6-Nov-2016", "title": "Words or Characters? Fine-grained Gating for Reading Comprehension", "abstract": "Previous work combines word-level and character-level representations using concatenation or scalar weighting, which is suboptimal for high-level tasks like reading comprehension. We present a fine-grained gating mechanism to dynamically combine word-level and character-level representations based on properties of the words. We also extend the idea of fine-grained gating to modeling the interaction between questions and paragraphs for reading comprehension. Experiments show that our approach can improve the performance on reading comprehension tasks, achieving new state-of-the-art results on the Children's Book Test dataset. To demonstrate the generality of our gating mechanism, we also show improved results on a social media tag prediction task.", "histories": [["v1", "Sun, 6 Nov 2016 03:17:42 GMT  (2851kb,D)", "http://arxiv.org/abs/1611.01724v1", null], ["v2", "Mon, 11 Sep 2017 21:00:30 GMT  (2852kb,D)", "http://arxiv.org/abs/1611.01724v2", "Accepted as a conference paper at ICLR 2017"]], "reviews": [], "SUBJECTS": "cs.CL cs.LG", "authors": ["zhilin yang", "bhuwan dhingra", "ye yuan", "junjie hu", "william w cohen", "ruslan salakhutdinov"], "accepted": true, "id": "1611.01724"}, "pdf": {"name": "1611.01724.pdf", "metadata": {"source": "CRF", "title": null, "authors": ["READING COMPREHENSION", "Zhilin Yang", "Bhuwan Dhingra", "Ye Yuan", "Junjie Hu", "William W. Cohen", "Ruslan Salakhutdinov"], "emails": ["zhiliny@cs.cmu.edu", "wcohen@cs.cmu.edu", "rsalakhu@cs.cmu.edu"], "sections": [{"heading": "1 INTRODUCTION", "text": "This year, it has come to the point where you feel you are able to live in a country where most people are able to live in a country where they are able, where they are able to move, and where they are able to move, where they are able to move, where they are able to move, where they are able to move, where they are able to move, where they are able to move, where they are able to move."}, {"heading": "2 RELATED WORK", "text": "Hybrid word-sign models have been proposed to demonstrate benefits at both word and character levels. Ling et al. (2015) introduce a compositional sign-to-word model (C2W) based on bidirectional LSTMs. Kim et al. (2016) describe a model that uses a revolutionary neural network (CNN) and a highway network of signs for speech modeling. Miyamoto & Cho (2016) use a gate to find the optimal mix of character level and word-level input in an adaptive way. Yang et al. (2016a) use deeply gated recurrent units at both sign and word levels to encode morphology and context information. Gating mechanisms are commonly used in sequence modeling. Long-term short-term memory (LSTM) networks (Hochreiter & Schmidhuber, 1997) are designed to blend in with disappearing grades."}, {"heading": "3 FINE-GRAINED GATING", "text": "In this section, we will describe our fine-grained gating approach in the context of reading comprehension. First, we will present the settings for reading comprehension tasks and a general architecture of neural networks, and then we will describe our approaches to word-character gating and document query gating."}, {"heading": "3.1 READING COMPREHENSION SETTING", "text": "The reading comprehension task contains a document P = (p1, p2, \u00b7 \u00b7, pM) and a query Q = (q1, q2, \u00b7 \u00b7, qN), where M and N are the lengths of the document or query respectively. Each token pi is called (wi, Ci), where wi is the most uniform encoding of the token in the vocabulary, and Ci is a matrix with each line representing a uniform encoding of a character. Each token in the query qj is similarly defined. We use i as a subscript for documents and j for queries. The output of the problem is an answer a, which can be either an index or a range of indices in the documentation. Now, we describe a general architecture used in this work, which is a generalization of the gated attention reader (Dhingra et al., 2016a)."}, {"heading": "3.2 WORD-CHARACTER FINE-GRAINED GATING", "text": "Given a one-hot encoding wi and a character sequence Ci, we will now describe how to calculate the vector representation hi = f (wi, Ci) for the token. In the rest of the section, we will simply drop the i subscript for the notation. We will first apply an RNN to C and assume the hidden state in the last step c as the representation at the drawing level (Yang et al., 2016a). Let E refer to the representation of the token as an embed table. We will perform a matrix vector multiplication Ew to obtain a word-level representation, assuming that c and Ew have the same length de in this work. Previous methods defined f with the word representation Ew (Collobert et al., 2011), the representation at the character level c (Ling et al., 2015), or the concatenation [Ew; c] (Yang et al, 2016a)."}, {"heading": "3.3 DOCUMENT-QUERY FINE-GRAINED GATING", "text": "Considering the hidden states Pk and Qk, we now describe how to calculate a representation Hk that encodes the interactions between the document and the query. In this section, we drop the high criterion k (the number of layers) for the simplicity of the notation. Drop pi the i-th row of P and qj the j-row of Q. I.e., drop the lengths of pi and qj. Attention-over-attention (AoA) (Cui et al., 2016) defines a point product between each pair of tokens in the document and the query, i.e. pTi qj the lengths of pi and qj, followed by rows and columns of softmax nonlinearity. AoA imposes pair interactions between the document and the query, but the use of a point product between the document and the query is potentially not meaningful enough and heavy enough to generalize on multi-layer networks."}, {"heading": "4 EXPERIMENTS", "text": "We will first present experimental results on the Twitter dataset, where we can rule out the impact of different decisions of network architectures to demonstrate the effectiveness of our fine-grained word character gating approach. Later, we will demonstrate experiments with more sophisticated data sets for reading comprehension, further demonstrating that our approach can also be used to improve performance in demanding NLP tasks."}, {"heading": "4.1 EVALUATING WORD-CHARACTER GATING ON TWITTER", "text": "We evaluate the effectiveness of our word-sign dating mechanism on a social media tag prediction task. We use the Twitter dataset and follow the experimental settings in Dhingra et al. (2016b). We also use the same network architecture on the token representations, which is an LSTM layer, followed by a Softmax classification layer (Dhingra et al., 2016b). The Twitter dataset consists of English tweets with at least one Twitter hashtag. Hashtags and HTML tags have been removed from the body of the tweet, and usernames and URLs are replaced by special tokens. The dataset contains 2 million tweets for training, 10K for validation, and 50K for testing, with a total of 2,039 unique hashtags. The task is to predict the hashtags of each tweet (hashtags) and compare the different methods."}, {"heading": "4.2 PERFORMANCE ON READING COMPREHENSION", "text": "After studying the effectiveness of the fine-grained gating mechanism of word characters in the Twitter dataset, we now move on to a more challenging task: reading comprehension. In this section, we will experiment with two datasets, the Children's Book Test dataset (Hill et al., 2016) and the SQuAD dataset (Rajpurkar et al., 2016)."}, {"heading": "4.2.1 CHILDREN\u2019S BOOK TEST", "text": "The total dataset includes 669,343 questions for training, 8,000 for validation and 10,000 for the test. We are following the procedure in Dhingra et al. (2016a) closely and gradually adding different components to see the changes in performance. For the fine-grained gating approach, we are using the same hyperparameters as in Dhingra et al. (2016a), except that we are using a character level GRU of 100 units to have the same size as the word search table. In addition to the different ways to combine word and character representations, we are also comparing two different types of document and query integration: GA refers to the gated attention reader (Dhingra et al., 2016a) and FG refers to our fine-grained gating method, which is described in Section 3.3.3. The results are presented in congruent Table 2. We report the results of the common denominations (CN) and name the NG categories that are used in the CBT and the final results of the CBT method."}, {"heading": "4.2.2 SQUAD", "text": "The Stanford Question Answering Dataset (SQuAD) is a recent reading comprehension data set (Rajpurkar et al., 2016), containing 23,215 paragraphs from 536 Wikipedia articles. Unlike other reading comprehension data sets such as CBT, the answers are a span of text rather than a single word. The data set is divided into a training set (80%, 87,636 question-answer pairs), a development set (10%, 10,600 question-answer pairs) and a test set that is not released.We report on our results in Table 3. \"Exact Match\" calculates the ratio of questions that are correctly answered by a strict string comparison, and the F1 score is calculated at the token level. We can find that both a fine-grained word-character rating and a fine-grained document query rating can significantly improve performance, leading to state-of-the-art results among the published work."}, {"heading": "4.3 VISUALIZATION AND ANALYSIS", "text": "The results are described in Figure 3. We see that designated units such as \"organization\" and noun phrases (with the keywords \"NNP\" or \"NNPS\") tend to use representations at the character level, which is in line with human intuition, as these characters are usually rare or have rich morphologies. Also, DOCLEN-4, WH adverb (\"WRB\") and conjunction characters (\"IN\" and \"CC\") tend to use representations at the word level because they are common. We also try random text spreads from the SQuAD dataset and visualize the average goal values in Figure 4. The results are consistent with our observations in Figure 3. Rare tokens, noun phrases and designated units tend to use representations at the character level, while others tend to use word values in Figure 4 to justify these values as well with the lowest table."}, {"heading": "5 CONCLUSIONS", "text": "We present a fine-grained gating mechanism that dynamically combines word and character level representations based on word properties. Experiments with the Twitter tag prediction dataset show that fine-grained gating is much more powerful than scalar gating and concatenation. Our method also improves reading comprehension and achieves new, state-of-the-art results on CBT. In our future work, we plan to use the fine-grained gating mechanism to combine other representation levels such as phrases and sentences. It will also be fascinating to integrate NER and POS networks and learn token representation in an end-to-end manner."}, {"heading": "ACKNOWLEDGMENTS", "text": "This work is funded by Disney, the ADeLAIDE grant FA8750-16C-0130-001 and the ONR grant N000141310721."}], "references": [{"title": "Neural machine translation by jointly learning to align and translate", "author": ["Dzmitry Bahdanau", "Kyunghyun Cho", "Yoshua Bengio"], "venue": "In ICLR,", "citeRegEx": "Bahdanau et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Bahdanau et al\\.", "year": 2015}, {"title": "A thorough examination of the cnn/daily mail reading comprehension", "author": ["Danqi Chen", "Jason Bolton", "Christopher D Manning"], "venue": null, "citeRegEx": "Chen et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Chen et al\\.", "year": 2016}, {"title": "On the properties of neural machine translation: Encoder-decoder approaches", "author": ["Kyunghyun Cho", "Bart Van Merri\u00ebnboer", "Dzmitry Bahdanau", "Yoshua Bengio"], "venue": "In Eighth Workshop on Syntax, Semantics and Structure in Statistical Translation,", "citeRegEx": "Cho et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Cho et al\\.", "year": 2014}, {"title": "Natural language processing (almost) from scratch", "author": ["Ronan Collobert", "Jason Weston", "L\u00e9on Bottou", "Michael Karlen", "Koray Kavukcuoglu", "Pavel Kuksa"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Collobert et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Collobert et al\\.", "year": 2011}, {"title": "Attention-over-attention neural networks for reading comprehension", "author": ["Yiming Cui", "Zhipeng Chen", "Si Wei", "Shijin Wang", "Ting Liu", "Guoping Hu"], "venue": "arXiv preprint arXiv:1607.04423,", "citeRegEx": "Cui et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Cui et al\\.", "year": 2016}, {"title": "Gated-attention readers for text comprehension", "author": ["Bhuwan Dhingra", "Hanxiao Liu", "Zhilin Yang", "William W Cohen", "Ruslan Salakhutdinov"], "venue": "arXiv preprint arXiv:1606.01549,", "citeRegEx": "Dhingra et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Dhingra et al\\.", "year": 2016}, {"title": "Tweet2vec: Character-based distributed representations for social media", "author": ["Bhuwan Dhingra", "Zhong Zhou", "Dylan Fitzpatrick", "Michael Muehl", "William W Cohen"], "venue": "In ACL,", "citeRegEx": "Dhingra et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Dhingra et al\\.", "year": 2016}, {"title": "Teaching machines to read and comprehend", "author": ["Karl Moritz Hermann", "Tomas Kocisky", "Edward Grefenstette", "Lasse Espeholt", "Will Kay", "Mustafa Suleyman", "Phil Blunsom"], "venue": "In NIPS,", "citeRegEx": "Hermann et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Hermann et al\\.", "year": 2015}, {"title": "The goldilocks principle: Reading children\u2019s books with explicit memory representations", "author": ["Felix Hill", "Antoine Bordes", "Sumit Chopra", "Jason Weston"], "venue": "In ICLR,", "citeRegEx": "Hill et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Hill et al\\.", "year": 2016}, {"title": "Long short-term memory", "author": ["Sepp Hochreiter", "J\u00fcrgen Schmidhuber"], "venue": "Neural computation,", "citeRegEx": "Hochreiter and Schmidhuber.,? \\Q1997\\E", "shortCiteRegEx": "Hochreiter and Schmidhuber.", "year": 1997}, {"title": "Text understanding with the attention sum reader network", "author": ["Rudolf Kadlec", "Martin Schmid", "Ondrej Bajgar", "Jan Kleindienst"], "venue": "In ACL,", "citeRegEx": "Kadlec et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Kadlec et al\\.", "year": 2016}, {"title": "Character-aware neural language models", "author": ["Yoon Kim", "Yacine Jernite", "David Sontag", "Alexander M Rush"], "venue": "In AAAI,", "citeRegEx": "Kim et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Kim et al\\.", "year": 2016}, {"title": "Finding function in form: Compositional character models for open vocabulary word representation", "author": ["Wang Ling", "Tiago Lu\u0131\u0301s", "Lu\u0131\u0301s Marujo", "Ram\u00f3n Fernandez Astudillo", "Silvio Amir", "Chris Dyer", "Alan W Black", "Isabel Trancoso"], "venue": "In EMNLP,", "citeRegEx": "Ling et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Ling et al\\.", "year": 2015}, {"title": "Achieving open vocabulary neural machine translation with hybrid word-character models", "author": ["Minh-Thang Luong", "Christopher D Manning"], "venue": "In ACL,", "citeRegEx": "Luong and Manning.,? \\Q2016\\E", "shortCiteRegEx": "Luong and Manning.", "year": 2016}, {"title": "Gated word-character recurrent language model", "author": ["Yasumasa Miyamoto", "Kyunghyun Cho"], "venue": "In EMNLP,", "citeRegEx": "Miyamoto and Cho.,? \\Q2016\\E", "shortCiteRegEx": "Miyamoto and Cho.", "year": 2016}, {"title": "Neural semantic encoders", "author": ["Tsendsuren Munkhdalai", "Hong Yu"], "venue": "arXiv preprint arXiv:1607.04315,", "citeRegEx": "Munkhdalai and Yu.,? \\Q2016\\E", "shortCiteRegEx": "Munkhdalai and Yu.", "year": 2016}, {"title": "Squad: 100,000+ questions for machine comprehension of text", "author": ["Pranav Rajpurkar", "Jian Zhang", "Konstantin Lopyrev", "Percy Liang"], "venue": "In EMNLP,", "citeRegEx": "Rajpurkar et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Rajpurkar et al\\.", "year": 2016}, {"title": "Iterative alternating neural attention for machine reading", "author": ["Alessandro Sordoni", "Phillip Bachman", "Yoshua Bengio"], "venue": "arXiv preprint arXiv:1606.02245,", "citeRegEx": "Sordoni et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Sordoni et al\\.", "year": 2016}, {"title": "Natural language comprehension with the epireader", "author": ["Adam Trischler", "Zheng Ye", "Xingdi Yuan", "Kaheer Suleman"], "venue": "In EMNLP,", "citeRegEx": "Trischler et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Trischler et al\\.", "year": 2016}, {"title": "Machine comprehension using match-lstm and answer pointer", "author": ["Shuohang Wang", "Jing Jiang"], "venue": "arXiv preprint arXiv:1608.07905,", "citeRegEx": "Wang and Jiang.,? \\Q2016\\E", "shortCiteRegEx": "Wang and Jiang.", "year": 2016}, {"title": "On multiplicative integration with recurrent neural networks", "author": ["Yuhuai Wu", "Saizheng Zhang", "Ying Zhang", "Yoshua Bengio", "Ruslan Salakhutdinov"], "venue": "In NIPS,", "citeRegEx": "Wu et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Wu et al\\.", "year": 2016}, {"title": "Learning multi-relational semantics using neural-embedding models", "author": ["Bishan Yang", "Wen-tau Yih", "Xiaodong He", "Jianfeng Gao", "Li Deng"], "venue": "In NIPS 2014 workshop on Learning Semantics,", "citeRegEx": "Yang et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Yang et al\\.", "year": 2014}, {"title": "Multi-task cross-lingual sequence tagging from scratch", "author": ["Zhilin Yang", "Ruslan Salakhutdinov", "William Cohen"], "venue": "arXiv preprint arXiv:1603.06270,", "citeRegEx": "Yang et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Yang et al\\.", "year": 2016}, {"title": "Review networks for caption generation", "author": ["Zhilin Yang", "Ye Yuan", "Yuexin Wu", "Ruslan Salakhutdinov", "William W Cohen"], "venue": "In NIPS,", "citeRegEx": "Yang et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Yang et al\\.", "year": 2016}, {"title": "End-to-end answer chunk extraction and ranking for reading comprehension", "author": ["Yang Yu", "Wei Zhang", "Kazi Hasan", "Mo Yu", "Bing Xiang", "Bowen Zhou"], "venue": "arXiv preprint arXiv:1610.09996,", "citeRegEx": "Yu et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Yu et al\\.", "year": 2016}], "referenceMentions": [{"referenceID": 12, "context": "Word-level representations are good at memorizing the semantics of the tokens while character-level representations are more suitable for modeling sub-word morphologies (Ling et al., 2015; Yang et al., 2016a).", "startOffset": 169, "endOffset": 208}, {"referenceID": 12, "context": "Word-level representations are good at memorizing the semantics of the tokens while character-level representations are more suitable for modeling sub-word morphologies (Ling et al., 2015; Yang et al., 2016a). For example, considering \u201ccat\u201d and \u201ccats\u201d, word-level representations can only learn the similarities between the two tokens by training on a large amount of training data, while character-level representations, by design, can easily capture the similarities. Character-level representations are also used to alleviate the difficulties of modeling out-of-vocabulary (OOV) tokens (Luong & Manning, 2016). Hybrid word-character models have been proposed to leverage the advantages of both word-level and character-level representations. The most commonly used method is to concatenate these two representations (Yang et al., 2016a). However, concatenating word-level and character-level representations is technically problematic. For frequent tokens, the word-level representations are usually accurately estimated during the training process, and thus introducing character-level representations can potentially bias the entire representations. For infrequent tokens, the estimation of wordlevel representations have high variance, which will have negative effects when combined with the character-level representations. To address this issue, recently Miyamoto & Cho (2016) introduced a scalar gate conditioned on the word-level representations to control the ratio of the two representations.", "startOffset": 170, "endOffset": 1385}, {"referenceID": 7, "context": "Previous work has shown the importance of modeling interactions between document and query tokens by introducing various attention architectures for the task (Hermann et al., 2015; Kadlec et al., 2016).", "startOffset": 158, "endOffset": 201}, {"referenceID": 10, "context": "Previous work has shown the importance of modeling interactions between document and query tokens by introducing various attention architectures for the task (Hermann et al., 2015; Kadlec et al., 2016).", "startOffset": 158, "endOffset": 201}, {"referenceID": 0, "context": "The gating mechanism can also be viewed as a form of attention mechanism (Bahdanau et al., 2015; Yang et al., 2016b) over two inputs.", "startOffset": 73, "endOffset": 116}, {"referenceID": 8, "context": "A variety of models have been proposed to extract answers from given text (Hill et al., 2016; Kadlec et al., 2016; Trischler et al., 2016; Chen et al., 2016; Sordoni et al., 2016; Cui et al., 2016).", "startOffset": 74, "endOffset": 197}, {"referenceID": 10, "context": "A variety of models have been proposed to extract answers from given text (Hill et al., 2016; Kadlec et al., 2016; Trischler et al., 2016; Chen et al., 2016; Sordoni et al., 2016; Cui et al., 2016).", "startOffset": 74, "endOffset": 197}, {"referenceID": 18, "context": "A variety of models have been proposed to extract answers from given text (Hill et al., 2016; Kadlec et al., 2016; Trischler et al., 2016; Chen et al., 2016; Sordoni et al., 2016; Cui et al., 2016).", "startOffset": 74, "endOffset": 197}, {"referenceID": 1, "context": "A variety of models have been proposed to extract answers from given text (Hill et al., 2016; Kadlec et al., 2016; Trischler et al., 2016; Chen et al., 2016; Sordoni et al., 2016; Cui et al., 2016).", "startOffset": 74, "endOffset": 197}, {"referenceID": 17, "context": "A variety of models have been proposed to extract answers from given text (Hill et al., 2016; Kadlec et al., 2016; Trischler et al., 2016; Chen et al., 2016; Sordoni et al., 2016; Cui et al., 2016).", "startOffset": 74, "endOffset": 197}, {"referenceID": 4, "context": "A variety of models have been proposed to extract answers from given text (Hill et al., 2016; Kadlec et al., 2016; Trischler et al., 2016; Chen et al., 2016; Sordoni et al., 2016; Cui et al., 2016).", "startOffset": 74, "endOffset": 197}, {"referenceID": 3, "context": "Ling et al. (2015) introduce a compositional character to word (C2W) model based on bidirectional LSTMs.", "startOffset": 0, "endOffset": 19}, {"referenceID": 3, "context": "Kim et al. (2016) describe a model that employs a convolutional neural network (CNN) and a highway network over characters for language modeling.", "startOffset": 0, "endOffset": 18}, {"referenceID": 3, "context": "Kim et al. (2016) describe a model that employs a convolutional neural network (CNN) and a highway network over characters for language modeling. Miyamoto & Cho (2016) use a gate to adaptively find the optimal mixture of the character-level and word-level inputs.", "startOffset": 0, "endOffset": 168}, {"referenceID": 3, "context": "Kim et al. (2016) describe a model that employs a convolutional neural network (CNN) and a highway network over characters for language modeling. Miyamoto & Cho (2016) use a gate to adaptively find the optimal mixture of the character-level and word-level inputs. Yang et al. (2016a) employ deep gated recurrent units on both character and word levels to encode morphology and context information.", "startOffset": 0, "endOffset": 284}, {"referenceID": 0, "context": "Similar to LSTM, Gated Recurrent Unit (GRU) was proposed by Cho et al. (2014), which also uses gating units to modulate the flow of information.", "startOffset": 60, "endOffset": 78}, {"referenceID": 0, "context": "The gating mechanism can also be viewed as a form of attention mechanism (Bahdanau et al., 2015; Yang et al., 2016b) over two inputs. Similar to the idea of gating, multiplicative integration has also been shown to provide a benefit in various settings. Yang et al. (2014) find that multiplicative operations are superior to additive operations in modeling relations.", "startOffset": 74, "endOffset": 273}, {"referenceID": 0, "context": "The gating mechanism can also be viewed as a form of attention mechanism (Bahdanau et al., 2015; Yang et al., 2016b) over two inputs. Similar to the idea of gating, multiplicative integration has also been shown to provide a benefit in various settings. Yang et al. (2014) find that multiplicative operations are superior to additive operations in modeling relations. Wu et al. (2016) propose to use Hadamard product to replace sum operation in recurrent networks, which gives a significant performance boost over existing RNN models.", "startOffset": 74, "endOffset": 385}, {"referenceID": 0, "context": "The gating mechanism can also be viewed as a form of attention mechanism (Bahdanau et al., 2015; Yang et al., 2016b) over two inputs. Similar to the idea of gating, multiplicative integration has also been shown to provide a benefit in various settings. Yang et al. (2014) find that multiplicative operations are superior to additive operations in modeling relations. Wu et al. (2016) propose to use Hadamard product to replace sum operation in recurrent networks, which gives a significant performance boost over existing RNN models. Dhingra et al. (2016a) use a multiplicative gating mechanism to achieve state-of-the-art results on question answering benchmarks.", "startOffset": 74, "endOffset": 558}, {"referenceID": 0, "context": "The gating mechanism can also be viewed as a form of attention mechanism (Bahdanau et al., 2015; Yang et al., 2016b) over two inputs. Similar to the idea of gating, multiplicative integration has also been shown to provide a benefit in various settings. Yang et al. (2014) find that multiplicative operations are superior to additive operations in modeling relations. Wu et al. (2016) propose to use Hadamard product to replace sum operation in recurrent networks, which gives a significant performance boost over existing RNN models. Dhingra et al. (2016a) use a multiplicative gating mechanism to achieve state-of-the-art results on question answering benchmarks. Reading comprehension is a challenging task for machines. A variety of models have been proposed to extract answers from given text (Hill et al., 2016; Kadlec et al., 2016; Trischler et al., 2016; Chen et al., 2016; Sordoni et al., 2016; Cui et al., 2016). Yu et al. (2016) propose a dynamic chunk reader to extract and rank a set of answer candidates from a given document to answer questions.", "startOffset": 74, "endOffset": 940}, {"referenceID": 0, "context": "The gating mechanism can also be viewed as a form of attention mechanism (Bahdanau et al., 2015; Yang et al., 2016b) over two inputs. Similar to the idea of gating, multiplicative integration has also been shown to provide a benefit in various settings. Yang et al. (2014) find that multiplicative operations are superior to additive operations in modeling relations. Wu et al. (2016) propose to use Hadamard product to replace sum operation in recurrent networks, which gives a significant performance boost over existing RNN models. Dhingra et al. (2016a) use a multiplicative gating mechanism to achieve state-of-the-art results on question answering benchmarks. Reading comprehension is a challenging task for machines. A variety of models have been proposed to extract answers from given text (Hill et al., 2016; Kadlec et al., 2016; Trischler et al., 2016; Chen et al., 2016; Sordoni et al., 2016; Cui et al., 2016). Yu et al. (2016) propose a dynamic chunk reader to extract and rank a set of answer candidates from a given document to answer questions. Wang & Jiang (2016) introduce an end-to-end neural architecture which incorporates match-LSTM and pointer networks (Vinyals et al.", "startOffset": 74, "endOffset": 1081}, {"referenceID": 3, "context": "Previous methods defined f using the word-level representation Ew (Collobert et al., 2011), the character-level representation c (Ling et al.", "startOffset": 66, "endOffset": 90}, {"referenceID": 12, "context": ", 2011), the character-level representation c (Ling et al., 2015), or the concatenation [Ew; c] (Yang et al.", "startOffset": 46, "endOffset": 65}, {"referenceID": 3, "context": "Previous methods defined f using the word-level representation Ew (Collobert et al., 2011), the character-level representation c (Ling et al., 2015), or the concatenation [Ew; c] (Yang et al., 2016a). Unlike these methods, we propose to use a gate to dynamically choose between the word-level and character-level representations based on the properties of the token. Let v denote a feature vector that encodes these properties. In this work, we use the concatenation of named entity tags, partof-speech tags, binned document frequency vectors, and the word-level representations to form the feature vector v. Let dv denote the length of v. The gate is computed as follows: g = \u03c3(Wgv + bg) where Wg and bg are the model parameters with shapes de\u00d7 dv and de, and \u03c3 denotes an elementwise sigmoid function. The final representation is computed using a fine-grained gating mechanism, h = f(c,w) = g c+ (1\u2212 g) (Ew) where denotes element-wise product between two vectors. An illustration of our fine-grained gating mechanism is shown in Figure 1. Intuitively speaking, when the gate g has high values, more information flows from the character-level representation to the final representation; when the gate g has low values, the final representation is dominated by the word-level representation. Though Miyamoto & Cho (2016) also use a gate to choose between word-level and character-level representations, our method is different in two ways.", "startOffset": 67, "endOffset": 1321}, {"referenceID": 4, "context": "Attention-over-attention (AoA) (Cui et al., 2016) defines a dot product between each pair of tokens in the document and the query, i.", "startOffset": 31, "endOffset": 49}, {"referenceID": 5, "context": "We use the Twitter dataset and follow the experimental settings in Dhingra et al. (2016b). We also use the same network architecture upon the token representations, which is an LSTM layer followed by a softmax classification layer (Dhingra et al.", "startOffset": 67, "endOffset": 90}, {"referenceID": 5, "context": "We use the Twitter dataset and follow the experimental settings in Dhingra et al. (2016b). We also use the same network architecture upon the token representations, which is an LSTM layer followed by a softmax classification layer (Dhingra et al., 2016b). The Twitter dataset consists of English tweets with at least one hashtag from Twitter. Hashtags and HTML tags have been removed from the body of the tweet, and user names and URLs are replaced with special tokens. The dataset contains 2 million tweets for training, 10K for validation and 50K for testing, with a total of 2,039 distinct hashtags. The task is to predict the hashtags of each tweet. We compare several different methods as follows. Word char concat uses the concatenation of word-level and character-level representations as in Yang et al. (2016a); word char feat concat concatenates the word-level and character-level representations along with the features described in Section 3.", "startOffset": 67, "endOffset": 819}, {"referenceID": 5, "context": "We use the Twitter dataset and follow the experimental settings in Dhingra et al. (2016b). We also use the same network architecture upon the token representations, which is an LSTM layer followed by a softmax classification layer (Dhingra et al., 2016b). The Twitter dataset consists of English tweets with at least one hashtag from Twitter. Hashtags and HTML tags have been removed from the body of the tweet, and user names and URLs are replaced with special tokens. The dataset contains 2 million tweets for training, 10K for validation and 50K for testing, with a total of 2,039 distinct hashtags. The task is to predict the hashtags of each tweet. We compare several different methods as follows. Word char concat uses the concatenation of word-level and character-level representations as in Yang et al. (2016a); word char feat concat concatenates the word-level and character-level representations along with the features described in Section 3.2; scalar gate uses a scalar gate similar to Miyamoto & Cho (2016) but is conditioned on the features; fine-grained gate is our method described in Section 3.", "startOffset": 67, "endOffset": 1020}, {"referenceID": 5, "context": "The \u201cGA word char concat\u201d results are extracted from Dhingra et al. (2016a). Our results on fine-grained gating are based on a single model.", "startOffset": 53, "endOffset": 76}, {"referenceID": 4, "context": "697 Cui et al. (2016) 0.", "startOffset": 4, "endOffset": 22}, {"referenceID": 4, "context": "697 Cui et al. (2016) 0.722 0.694 0.778 0.720 Munkhdalai & Yu (2016) 0.", "startOffset": 4, "endOffset": 69}, {"referenceID": 8, "context": "In this section, we experiment with two datasets, the Children\u2019s Book Test dataset (Hill et al., 2016) and the SQuAD dataset (Rajpurkar et al.", "startOffset": 83, "endOffset": 102}, {"referenceID": 16, "context": ", 2016) and the SQuAD dataset (Rajpurkar et al., 2016).", "startOffset": 30, "endOffset": 54}, {"referenceID": 5, "context": "We closely follow the setting in Dhingra et al. (2016a) and incrementally add different components to see the changes in performance.", "startOffset": 33, "endOffset": 56}, {"referenceID": 5, "context": "We closely follow the setting in Dhingra et al. (2016a) and incrementally add different components to see the changes in performance. For the fine-grained gating approach, we use the same hyper-parameters as in Dhingra et al. (2016a) except that we use a character-level GRU with 100 units to be of the same size as the word lookup table.", "startOffset": 33, "endOffset": 234}, {"referenceID": 16, "context": "The Stanford Question Answering Dataset (SQuAD) is a reading comprehension dataset collected recently (Rajpurkar et al., 2016).", "startOffset": 102, "endOffset": 126}], "year": 2016, "abstractText": "Previous work combines word-level and character-level representations using concatenation or scalar weighting, which is suboptimal for high-level tasks like reading comprehension. We present a fine-grained gating mechanism to dynamically combine word-level and character-level representations based on properties of the words. We also extend the idea of fine-grained gating to modeling the interaction between questions and paragraphs for reading comprehension. Experiments show that our approach can improve the performance on reading comprehension tasks, achieving new state-of-the-art results on the Children\u2019s Book Test dataset. To demonstrate the generality of our gating mechanism, we also show improved results on a social media tag prediction task.", "creator": "LaTeX with hyperref package"}}}