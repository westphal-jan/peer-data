{"id": "1611.01734", "review": {"conference": "iclr", "VERSION": "v1", "DATE_OF_SUBMISSION": "6-Nov-2016", "title": "Deep Biaffine Attention for Neural Dependency Parsing", "abstract": "While deep learning parsing approaches have proven very successful at finding the structure of sentences, most neural dependency parsers use neural networks only for feature extraction, and then use those features in traditional parsing algorithms. In contrast, this paper builds off recent work using general-purpose neural network components, training an attention mechanism over an LSTM to attend to the head of the phrase. We get state-of-the-art results for standard dependency parsing benchmarks, achieving 95.44% UAS and 93.76% LAS on the PTB dataset, 0.8% and 1.0% improvement, respectively, over Andor et al. (2016). In addition to proposing a new parsing architecture using dimensionality reduction and biaffine interactions, we examine simple hyperparameter choices that had a profound influence on the model's performance, such as reducing the value of beta2 in the Adam optimization algorithm.", "histories": [["v1", "Sun, 6 Nov 2016 07:26:38 GMT  (22kb)", "http://arxiv.org/abs/1611.01734v1", null], ["v2", "Tue, 22 Nov 2016 02:01:39 GMT  (22kb)", "http://arxiv.org/abs/1611.01734v2", "Under review for ICLR 2017; fixed typos and clarified prediction process"], ["v3", "Fri, 10 Mar 2017 04:37:03 GMT  (19kb)", "http://arxiv.org/abs/1611.01734v3", "Accepted to ICLR 2017; updated with new results and comparison to more recent models, including current state-of-the-art"]], "reviews": [], "SUBJECTS": "cs.CL cs.NE", "authors": ["timothy dozat", "christopher d manning"], "accepted": true, "id": "1611.01734"}, "pdf": {"name": "1611.01734.pdf", "metadata": {"source": "CRF", "title": null, "authors": [], "emails": ["tdozat@stanford.edu", "manning@stanford.edu"], "sections": [{"heading": null, "text": "ar Xiv: 161 1.01 734v 1 [cs.C L] 6N ov2 016 Revised as conference contribution at ICLR 2017"}, {"heading": "1 INTRODUCTION", "text": "In recent years, it has become clear that the number of people who are able, are able, are able, are able to move, are able to move, and that they are able, are able, are able, are able to feel able, are able to feel able, feel able, feel able, feel able, feel able, feel able, feel able, feel able, feel able, feel able, feel able, feel able, feel able, feel able, feel able, feel able, feel able, feel able and act."}, {"heading": "2 BACKGROUND AND RELATED WORK", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "2.1 TRANSITION-BASED NEURAL PARSING", "text": "Transition-based parsers - such as shift-reduce parsers - parse sentences from left to right, maintaining a \"buffer\" of words that have not yet been analyzed, and a \"stack\" of words whose heads have not been seen or whose dependencies have not all been fully analyzed. At each step, transition-based parsers can manipulate the stack and buffer, and assign arcs from one word to another. You can then train each multi-level machine learning classifier for properties that have been extracted from the stack, buffer, and previous arc actions to predict the next action. Here, we gather the contributions of models that use neural networks to make these transition activities.Chen & Manning (2014) make the first successful attempt to integrate deep learning into a dependency parser (henceforth the CM parser). Their approach involves using a feedback-forward network classifier to make parsing actions; parsing behavior that requires parsing behavior in each network."}, {"heading": "2.2 GRAPH-BASED NEURAL PARSING", "text": "This year it has come to the point where it will be able to retaliate, \"he said in an interview with the\" Welt am Sonntag. \""}, {"heading": "3 PROPOSED MODEL", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "3.1 DEEP BIAFFINE PARSING", "text": "This year, it has reached the stage where it will be able to take the lead."}, {"heading": "3.2 DEEP BIAFFINE CLASSIFICATION", "text": "To predict the labels, we use a parallel mechanism for deep biaffin attention. We want the label that predicts the model for a particular word to be conditioned on the head of that word (e.g., we want a word like \"fast\" to be classified as an adverbial modifier if it depends on a verb, but not if it depends on a noun). So, again, we use MLPs to transform the recursive state of the current word ri and its gold or predicted head yi as a recurring state ryi, but this time we let the model predict a series of results - one for each possible label - by allowing the biaffin transformation to map the two vectors into a third vector, rather than into a scalar. This can then also be trained under a hinged loss or transverse entropy loss object, and we use the arrangement of results - one for each possible label - to allow an array of two vectors - to transform into a third one."}, {"heading": "3.3 PRACTICAL CONSIDERATIONS", "text": "A notable advantage of bilinear or biaffine attention over traditional attention is that less memory is required. Traditional attention explicitly requires the calculation of a size d hidden state for each pair of words ordered in the length n sentence; the resulting tensorH is (n \u00d7 n \u00d7 d) -dimensional. However, bilinear attention can be calculated more memory-efficiently; since matrix multiplications are used, it never explicitly calculates a full (n \u00d7 n \u00b7 d) hidden state. Consequently, the calculation of traditional attention requires O (dn2) memory, whereas the calculation of bilinear attention requires only O (dn + n2) memory. Similarly, the memory complexity of our approach to labeling classification O (dnc + n2c) - where c is the number of classes to be predicted - while the complexity of the traditional attention approach O (dn2 + cn) requires memory. Similarly, as long as the number of sequences in the classification is significantly smaller than the number of the GB, the number of labels in the assignment may be less effective."}, {"heading": "4 EXPERIMENTS AND RESULTS", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "4.1 HYPERPARAMETER SELECTION", "text": "In this section, we will delve into how the various hyperparameters affect parsing performance, using the Penn Treebank Train and validation splits that are converted to Stanford Dependencies with version 3.5.0 of the Stanford Dependencies Converter. Unless otherwise stated, our model uses: 100-dimensional word and tag embedding with word vectors initialized to GloVe (Pennington et al., 2014), trained on Wikipedia and Gigaword, and a 15-percent chance of dropping tag embedding; 4-layer BiLSTMs with 300-dimensional left and right LSTMs using the form of recurrent exposure errors proposed by Gal & Ghahramani (2015), with a 75 percent probability between timesteps and a 67 percent probability between layers; a 1-layer 100-dimensional MLSTM layer with <"}, {"heading": "4.1.1 DEPTH", "text": "First, we examine how the deepening of the network affects performance, since all other models discussed here use two-layer neural networks (except Cheng et al. (2016), which use single-layer networks). We test with two or four BiLSTM layers and zero, one or two MLP layers after the last BiLSTM. What we find is that the deepening of the network improves the performance to some extent - if the BiLSTM is flat and an MLP is added, the performance does not significantly improve, presumably because the reduction of dimensionality limits its representativeness too much - but if the BiLSTM is deeper and can learn more abstract features, the dimensionality reduction helps the model to successfully avoid an overhauling. However, the use of a deeper MLP actually impedes the performance. The training accuracy of the two- and four-layer LSTM models with two-layer MPs are more streamlined, however, indicating that the gain of the MLP does not seem significant."}, {"heading": "4.1.2 ATTENTION MECHANISM", "text": "Next, we compare three attention-based scoring mechanisms - since our model uses a very different hyperparameter configuration than Kiperwasser & Goldberg's - and implement the chain-based attention mechanism in addition to our deep biaffin. However, the biaffin layer of our parser O (d2) parameters and the last layer of the chain-based layer have only O (d). To ensure that the additional parameters in our model do not affect the result, we also train a special case of our model with a two-dimensional layer of O (d) parameters in which U (arc) and each layer U (rel), i, \u00b7 is diagonal."}, {"heading": "4.1.3 RECURRENT CELL", "text": "We also tested how the choice of LSTM or GRU affects performance. However, we found that GRUs were unable to train with recurring drop-outs, with the loss exploding after a few iterations of training even at a lower learning rate. Greff et al. (2015) suggested modifying the formulation of LSTMs to make them stronger through a coupled entry gate (CifLSTM), but retaining the vanilla LSTM's exit gate. Therefore, we slightly modified the formulation to remove one of the tanh nonlinearities - which is not necessary when an update gate (18-19) is used."}, {"heading": "4.1.4 OPTIMIZATION ALGORITHM", "text": "We opt for optimization with Adam (Kingma & Ba, 2014), which maintains a moving average of the L2 standard of gradient for each parameter throughout the training and divides the gradient for each parameter by that moving average, ensuring that the magnitude of the gradient is on average close to 1. However, we note that the value recommended by Kingma & Ba for \u03b22 - which controls the decay rate for that moving average - is too high. If this value is very large, the magnitude of the current update is strongly influenced by the greater magnitude of the gradient, which is very far in the past, with the effect that the optimizer cannot quickly adjust to the recent changes in the model."}, {"heading": "4.1.5 TAG DROPOUT", "text": "All models under consideration here - including our own - use POS tags as input. Although we find POS tag training very helpful for final performance, we want to make sure that our model does not match specific sequences of POS tags, and we want to make sure that it remains robust against upstream tagging errors. So to prevent our model from being too reliant on POS tags, we drop it randomly 15% of the time and find that it significantly improves performance."}, {"heading": "4.2 MODEL COMPARISON", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "4.2.1 DATASETS", "text": "In this thesis, we show test results for the proposed model on three sets of data coming from two sources: the English Penn Treebank, which was automatically converted from constituency trees to Stanford dependencies, using both version 3.3.0 and version 3.5.0 of the Stanford Dependency Converter (PTB-SD 3.3.0 and PTB-SD 3.5.0); and the Chinese Penn Treebank version 5.1 (CTB 5.1), which was automatically converted from constituency trees to the CoNLL 2007 dependency format with Penn2Malt. PTB-SD3.0 and CTB 5.1 are sets of data used by default in other dependency analyses over the past four years, and PTB-SD 3.5.0 is an updated version of PTB-SD 3.0. As usual, we omit the score from the evaluation and use the predicted POS tags for the English PTB Gold dataset - generated from Stanford's 2003 bibliography, the PTB tags - and the PTB TD literature section - we have predicted the PTB tags."}, {"heading": "4.2.2 RESULTS", "text": "Here we compare our model with a number of others in the literature: the models by Dyer et al. (2015) and Ballesteros et al. (2016), which extend the CM parser to include a beam search and a globally normalized CRF lens function; and Kiperwasser & Goldberg (2016) and Cheng et al. (2016), which, like this paper, use BiLSTMs to generate feature embeddings used for an attention-based parser. What we see is that our implementation, with the exception of LAS on CTB 5.1, reaches the state of the art by a fairly considerable distance. It is also worth pointing out that our parser performs better on PTB-SD 3.5.0 and 3.3.0, while it is possible that this is partly because we have matched tags to 3.5.0."}, {"heading": "5 CONCLUSION", "text": "In this paper, we proposed using a modified version of bilinear attention in a neural network dependency parser, and demonstrated that our approach far exceeded the state of the art. We also discussed in detail some of the hyperparameter selections that make a crucial difference in final performance: we found that deeper networks with four LSTM layers outperform flatter networks with two LSTM layers when using our deep biaffin attention mechanism; we found that GRU cells have significant difficulties in training with suspensions, but LSTMs (vanilla or with a coupled input-forget gate) have no problems; we argued that the default settings for the Adam optimizer should be adjusted; and we demonstrated that dropouts are effective."}], "references": [{"title": "TensorFlow: Large-scale machine learning on heterogeneous systems", "author": ["cent Vanhoucke", "Vijay Vasudevan", "Fernanda Vi\u00e9gas", "Oriol Vinyals", "Pete Warden", "Martin Wattenberg", "Martin Wicke", "Yuan Yu", "Xiaoqiang Zheng"], "venue": null, "citeRegEx": "Vanhoucke et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Vanhoucke et al\\.", "year": 2015}, {"title": "Globally normalized transitionbased neural networks", "author": ["Daniel Andor", "Chris Alberti", "David Weiss", "Aliaksei Severyn", "Alessandro Presta", "Kuzman Ganchev", "Slav Petrov", "Michael Collins"], "venue": "In Association for Computational Linguistics,", "citeRegEx": "Andor et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Andor et al\\.", "year": 2016}, {"title": "Neural machine translation by jointly learning to align and translate", "author": ["Dzmitry Bahdanau", "Kyunghyun Cho", "Yoshua Bengio"], "venue": "International Conference on Learning Representations,", "citeRegEx": "Bahdanau et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Bahdanau et al\\.", "year": 2014}, {"title": "Training with exploration improves a greedy stack-LSTM parser", "author": ["Miguel Ballesteros", "Yoav Goldberg", "Chris Dyer", "Noah A Smith"], "venue": "Proceedings of the conference on empirical methods in natural language processing,", "citeRegEx": "Ballesteros et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Ballesteros et al\\.", "year": 2016}, {"title": "A fast and accurate dependency parser using neural networks", "author": ["Danqi Chen", "Christopher D Manning"], "venue": "In Proceedings of the conference on empirical methods in natural language processing,", "citeRegEx": "Chen and Manning.,? \\Q2014\\E", "shortCiteRegEx": "Chen and Manning.", "year": 2014}, {"title": "Bi-directional attention with agreement for dependency parsing", "author": ["Hao Cheng", "Hao Fang", "Xiaodong He", "Jianfeng Gao", "Li Deng"], "venue": null, "citeRegEx": "Cheng et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Cheng et al\\.", "year": 2016}, {"title": "Fast and accurate deep network learning by exponential linear units (elus)", "author": ["Djork-Arn\u00e9 Clevert", "Thomas Unterthiner", "Sepp Hochreiter"], "venue": "International Conference on Learning Representations,", "citeRegEx": "Clevert et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Clevert et al\\.", "year": 2015}, {"title": "Transitionbased dependency parsing with stack long short-term memory", "author": ["Chris Dyer", "Miguel Ballesteros", "Wang Ling", "Austin Matthews", "Noah A Smith"], "venue": "Proceedings of the conference on empirical methods in natural language processing,", "citeRegEx": "Dyer et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Dyer et al\\.", "year": 2015}, {"title": "Dropout as a bayesian approximation: Representing model uncertainty in deep learning", "author": ["Yarin Gal", "Zoubin Ghahramani"], "venue": "International Conference on Machine Learning,", "citeRegEx": "Gal and Ghahramani.,? \\Q2015\\E", "shortCiteRegEx": "Gal and Ghahramani.", "year": 2015}, {"title": "LSTM: A search space odyssey", "author": ["Klaus Greff", "Rupesh Kumar Srivastava", "Jan Koutn\u0131\u0301k", "Bas R Steunebrink", "J\u00fcrgen Schmidhuber"], "venue": "IEEE Transactions on Neural Networks and Learning Systems,", "citeRegEx": "Greff et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Greff et al\\.", "year": 2015}, {"title": "Adam: A method for stochastic optimization", "author": ["Diederik Kingma", "Jimmy Ba"], "venue": "International Conference on Learning Representations,", "citeRegEx": "Kingma and Ba.,? \\Q2014\\E", "shortCiteRegEx": "Kingma and Ba.", "year": 2014}, {"title": "Simple and accurate dependency parsing using bidirectional LSTM feature representations", "author": ["Eliyahu Kiperwasser", "Yoav Goldberg"], "venue": "Transactions of the Association for Computational Linguistics,", "citeRegEx": "Kiperwasser and Goldberg.,? \\Q2016\\E", "shortCiteRegEx": "Kiperwasser and Goldberg.", "year": 2016}, {"title": "Zoneout: Regularizing rnns by randomly preserving hidden activations", "author": ["David Krueger", "Tegan Maharaj", "J\u00e1nos Kram\u00e1r", "Mohammad Pezeshki", "Nicolas Ballas", "Nan Rosemary Ke", "Anirudh Goyal", "Yoshua Bengio", "Hugo Larochelle", "Aaron Courville"], "venue": "arXiv preprint arXiv:1606.01305,", "citeRegEx": "Krueger et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Krueger et al\\.", "year": 2016}, {"title": "Effective approaches to attentionbased neural machine translation", "author": ["Minh-Thang Luong", "Hieu Pham", "Christopher D Manning"], "venue": "Empirical Methods in Natural Language Processing,", "citeRegEx": "Luong et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Luong et al\\.", "year": 2015}, {"title": "Efficient estimation of word representations in vector space", "author": ["Tomas Mikolov", "Kai Chen", "Greg Corrado", "Jeffrey Dean"], "venue": "International Conference on Learning Representations,", "citeRegEx": "Mikolov et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Mikolov et al\\.", "year": 2013}, {"title": "Dependency parsing features for semantic parsing", "author": ["Will Monroe", "Yushi Wang"], "venue": null, "citeRegEx": "Monroe and Wang.,? \\Q2014\\E", "shortCiteRegEx": "Monroe and Wang.", "year": 2014}, {"title": "Glove: Global vectors for word representation", "author": ["Jeffrey Pennington", "Richard Socher", "Christopher D Manning"], "venue": "Proceedings of the Empiricial Methods in Natural Language Processing (EMNLP 2014),", "citeRegEx": "Pennington et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Pennington et al\\.", "year": 2014}, {"title": "Grounded compositional semantics for finding and describing images with sentences", "author": ["Richard Socher", "Andrej Karpathy", "Quoc V Le", "Christopher D Manning", "Andrew Y Ng"], "venue": "Transactions of the Association for Computational Linguistics,", "citeRegEx": "Socher et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Socher et al\\.", "year": 2014}, {"title": "Dropout: a simple way to prevent neural networks from overfitting", "author": ["Nitish Srivastava", "Geoffrey E Hinton", "Alex Krizhevsky", "Ilya Sutskever", "Ruslan Salakhutdinov"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Srivastava et al\\.,? \\Q1929\\E", "shortCiteRegEx": "Srivastava et al\\.", "year": 1929}, {"title": "Feature-rich part-ofspeech tagging with a cyclic dependency network", "author": ["Kristina Toutanova", "Dan Klein", "Christopher D Manning", "Yoram Singer"], "venue": "In Proceedings of the 2003 Conference of the North American Chapter of the Association for Computational Linguistics on Human Language Technology-Volume", "citeRegEx": "Toutanova et al\\.,? \\Q2003\\E", "shortCiteRegEx": "Toutanova et al\\.", "year": 2003}, {"title": "Grammar as a foreign language", "author": ["Oriol Vinyals", "\u0141ukasz Kaiser", "Terry Koo", "Slav Petrov", "Ilya Sutskever", "Geoffrey Hinton"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Vinyals et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Vinyals et al\\.", "year": 2015}, {"title": "Structured training for neural network transition-based parsing", "author": ["David Weiss", "Chris Alberti", "Michael Collins", "Slav Petrov"], "venue": "Annual Meeting of the Association for Computational Linguistics,", "citeRegEx": "Weiss et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Weiss et al\\.", "year": 2015}], "referenceMentions": [{"referenceID": 1, "context": "0% improvement, respectively, over Andor et al. (2016). In addition to proposing a new parsing architecture using dimensionality reduction and biaffine interactions, we examine simple hyperparameter choices that had a profound influence on the model\u2019s performance, such as reducing the value of \u03b22 in the Adam optimization algorithm.", "startOffset": 35, "endOffset": 55}, {"referenceID": 17, "context": "Dependency parsers\u2014which annotate sentences in a way designed to be easy for humans and computers alike to understand\u2014have been found to be extremely useful for a sizable number of NLP tasks, especially those involving natural language understanding in some way, such as semantic parsing (Monroe & Wang, 2014) and retrieving images based on a textual description (Socher et al., 2014).", "startOffset": 363, "endOffset": 384}, {"referenceID": 16, "context": "Dependency parsers\u2014which annotate sentences in a way designed to be easy for humans and computers alike to understand\u2014have been found to be extremely useful for a sizable number of NLP tasks, especially those involving natural language understanding in some way, such as semantic parsing (Monroe & Wang, 2014) and retrieving images based on a textual description (Socher et al., 2014). However, frequent incorrect parses can severely inhibit final performance, even completely road-blocking lines of research; for this reason, improving the quality of dependency parsers is needed for the improvement and success of these downstream tasks. In recent years, using deep learning in dependency parsers has gained a lot of attention due to the uncanny ability of neural networks to find real statistical patterns from data. The usual approach involves essentially training a neural network feature extractor and using the neural features in place of handcrafted ones to take discrete actions in a traditional parsing algorithm. Until recently, little to no research had successfully built dependency parsers that only used components that have been used successfully in a wide variety of neural models (although Vinyals et al. (2015) build such a sequence-to-sequence constituency tree parser).", "startOffset": 364, "endOffset": 1230}, {"referenceID": 13, "context": "Our model substitutes the concatenation-based attention mechanism they use with a variant of the bilinear attention proposed in the neural machine translation literature by Luong et al. (2015), augmenting it with additional MLP layers and making it parallel to traditional classification over a fixed number of classes.", "startOffset": 173, "endOffset": 193}, {"referenceID": 5, "context": "Dyer et al. (2015) and Ballesteros et al.", "startOffset": 0, "endOffset": 19}, {"referenceID": 2, "context": "(2015) and Ballesteros et al. (2016) replace the input to the feedforward network\u2014which in the CM parser is a concatenation of embeddings\u2014with the output of LSTMs over the stack, buffer, and previous actions.", "startOffset": 11, "endOffset": 37}, {"referenceID": 2, "context": "(2015) and Ballesteros et al. (2016) replace the input to the feedforward network\u2014which in the CM parser is a concatenation of embeddings\u2014with the output of LSTMs over the stack, buffer, and previous actions. Weiss et al. (2015) and Andor et al.", "startOffset": 11, "endOffset": 229}, {"referenceID": 1, "context": "(2015) and Andor et al. (2016) achieve state of the art performance by instead augmenting it with a beam search and a CRF loss so that the model can avoid committing to partial parses that later evidence might reveal to be incorrect.", "startOffset": 11, "endOffset": 31}, {"referenceID": 2, "context": "Kiperwasser & Goldberg (2016) take a graph-based approach to neural dependency parsing that is in many ways reminsicent of attention in neural machine translation as described by Bahdanau et al. (2014). In the attention model of Bahdanau et al.", "startOffset": 179, "endOffset": 202}, {"referenceID": 2, "context": "The traditional attention mechanism of Bahdanau et al. (2014) is not the only one that has been proposed in the literature; Luong et al.", "startOffset": 39, "endOffset": 62}, {"referenceID": 2, "context": "The traditional attention mechanism of Bahdanau et al. (2014) is not the only one that has been proposed in the literature; Luong et al. (2015) argue for substituting the MLP in the attention mechanism with a single bilinear transformation, mapping the target recurrent output vector r i and the source recurrent output vector r j to a score for the alignment: sij = r \u22a4(target) i Ur (source) j (7) The straightforward application of this to dependency parsing would be to define the score of a potential dependency arc as a bilinear map between the dependent and the potential head.", "startOffset": 39, "endOffset": 144}, {"referenceID": 16, "context": "When not otherwise specified, our model uses: 100dimensional word and tag embeddings with word vectors initialized to GloVe (Pennington et al., 2014) trained on Wikipedia and Gigaword and an 15% chance of dropping tag embeddings; 4layer BiLSTMs with 300-dimensional left and right LSTMs, using the form of recurrent dropout suggested by Gal & Ghahramani (2015) with a 75% keep probability between timesteps and a 67% keep probability between layers; a 1-layer 100 dimensional MLP layer with the elu function (Clevert et al.", "startOffset": 124, "endOffset": 149}, {"referenceID": 6, "context": ", 2014) trained on Wikipedia and Gigaword and an 15% chance of dropping tag embeddings; 4layer BiLSTMs with 300-dimensional left and right LSTMs, using the form of recurrent dropout suggested by Gal & Ghahramani (2015) with a 75% keep probability between timesteps and a 67% keep probability between layers; a 1-layer 100 dimensional MLP layer with the elu function (Clevert et al., 2015), also with a 67% keep probability; the Adam optimizer (Kingma & Ba, Using a different framework or implementation could yield different results", "startOffset": 366, "endOffset": 388}, {"referenceID": 15, "context": "When not otherwise specified, our model uses: 100dimensional word and tag embeddings with word vectors initialized to GloVe (Pennington et al., 2014) trained on Wikipedia and Gigaword and an 15% chance of dropping tag embeddings; 4layer BiLSTMs with 300-dimensional left and right LSTMs, using the form of recurrent dropout suggested by Gal & Ghahramani (2015) with a 75% keep probability between timesteps and a 67% keep probability between layers; a 1-layer 100 dimensional MLP layer with the elu function (Clevert et al.", "startOffset": 125, "endOffset": 361}, {"referenceID": 5, "context": "First we examine how making the network deeper affects performance, since all other models discussed hereuse two-layer neural networks (except Cheng et al. (2016), who use one-layer networks).", "startOffset": 143, "endOffset": 163}, {"referenceID": 9, "context": "Greff et al. (2015) suggest modifying the formulation of LSTMs to make them more GRU-like by using a coupled input-forget gate (CifLSTM), but retaining the output gate of the vanilla LSTM.", "startOffset": 0, "endOffset": 20}, {"referenceID": 12, "context": "One recently proposed alternative to dropout, zoneout (Krueger et al., 2016), would address this issue with using dropout in GRUs\u2013however, we leave experimenting with this for future work.", "startOffset": 54, "endOffset": 76}, {"referenceID": 4, "context": "1 Model UAS LAS UAS LAS Dyer et al. (2015) 93.", "startOffset": 24, "endOffset": 43}, {"referenceID": 2, "context": "5 Ballesteros et al. (2016) 93.", "startOffset": 2, "endOffset": 28}, {"referenceID": 2, "context": "5 Ballesteros et al. (2016) 93.56 91.42 87.65 86.21 Kiperwasser & Goldberg (2016) 93.", "startOffset": 2, "endOffset": 82}, {"referenceID": 2, "context": "5 Ballesteros et al. (2016) 93.56 91.42 87.65 86.21 Kiperwasser & Goldberg (2016) 93.9 91.9 87.6 86.1 Cheng et al. (2016) 94.", "startOffset": 2, "endOffset": 122}, {"referenceID": 2, "context": "5 Ballesteros et al. (2016) 93.56 91.42 87.65 86.21 Kiperwasser & Goldberg (2016) 93.9 91.9 87.6 86.1 Cheng et al. (2016) 94.10 91.49 88.1 85.7 Weiss et al. (2015) 94.", "startOffset": 2, "endOffset": 164}, {"referenceID": 1, "context": "41 - Andor et al. (2016) 94.", "startOffset": 5, "endOffset": 25}, {"referenceID": 19, "context": "As is standard, we omit puncuation from evaluation and use predicted POS tags for the English PTB dataset\u2014generated from the Stanford POS tagger (Toutanova et al., 2003)\u2014and gold POS tags for the Chinese PTB dataset.", "startOffset": 145, "endOffset": 169}, {"referenceID": 14, "context": "Word embeddings for Chinese were generated using Word2Vec (Mikolov et al., 2013) on Chinese Wikipedia.", "startOffset": 58, "endOffset": 80}, {"referenceID": 4, "context": "Here, we compare our model to a number of others in the literature: the models of Dyer et al. (2015) and Ballesteros et al.", "startOffset": 82, "endOffset": 101}, {"referenceID": 2, "context": "(2015) and Ballesteros et al. (2016), which use LSTMs to generate features for a transition-based parser; the models of Weiss et al.", "startOffset": 11, "endOffset": 37}, {"referenceID": 2, "context": "(2015) and Ballesteros et al. (2016), which use LSTMs to generate features for a transition-based parser; the models of Weiss et al. (2015) and Andor et al.", "startOffset": 11, "endOffset": 140}, {"referenceID": 1, "context": "(2015) and Andor et al. (2016), which augment the CM parser with a beam search and a globally normalized CRF objective function; and Kiperwasser & Goldberg (2016) and Cheng et al.", "startOffset": 11, "endOffset": 31}, {"referenceID": 1, "context": "(2015) and Andor et al. (2016), which augment the CM parser with a beam search and a globally normalized CRF objective function; and Kiperwasser & Goldberg (2016) and Cheng et al.", "startOffset": 11, "endOffset": 163}, {"referenceID": 1, "context": "(2015) and Andor et al. (2016), which augment the CM parser with a beam search and a globally normalized CRF objective function; and Kiperwasser & Goldberg (2016) and Cheng et al. (2016), which like this work uses BiLSTMs to generate feature embeddings used for an attention-based parser.", "startOffset": 11, "endOffset": 187}], "year": 2016, "abstractText": "While deep learning parsing approaches have proven very successful at finding the structure of sentences, most neural dependency parsers use neural networks only for feature extraction, and then use those features in traditional parsing algorithms. In contrast, this paper builds off recent work using general-purpose neural network components, training an attention mechanism over an LSTM to attend to the head of the phrase. We get state-of-the-art results for standard dependency parsing benchmarks, achieving 95.44% UAS and 93.76% LAS on the PTB dataset, 0.8% and 1.0% improvement, respectively, over Andor et al. (2016). In addition to proposing a new parsing architecture using dimensionality reduction and biaffine interactions, we examine simple hyperparameter choices that had a profound influence on the model\u2019s performance, such as reducing the value of \u03b22 in the Adam optimization algorithm.", "creator": "LaTeX with hyperref package"}}}