{"id": "1611.01747", "review": {"conference": "iclr", "VERSION": "v1", "DATE_OF_SUBMISSION": "6-Nov-2016", "title": "A Compare-Aggregate Model for Matching Text Sequences", "abstract": "Many NLP tasks including machine comprehension, answer selection and text entailment require the comparison between sequences. Matching the important units between sequences is a key to solve these problems. In this paper, we present a general \"compare-aggregate\" framework that performs word-level matching followed by aggregation using Convolutional Neural Networks. We particularly focus on the different comparison functions we can use to match two vectors. We use four different datasets to evaluate the model. We find that some simple comparison functions based on element-wise operations can work better than standard neural network and neural tensor network.", "histories": [["v1", "Sun, 6 Nov 2016 09:50:24 GMT  (374kb,D)", "http://arxiv.org/abs/1611.01747v1", "11 pages, 2 figures"]], "COMMENTS": "11 pages, 2 figures", "reviews": [], "SUBJECTS": "cs.CL cs.AI", "authors": ["shuohang wang", "jing jiang"], "accepted": true, "id": "1611.01747"}, "pdf": {"name": "1611.01747.pdf", "metadata": {"source": "CRF", "title": "A COMPARE-AGGREGATE MODEL FOR MATCHING TEXT SEQUENCES", "authors": ["Shuohang Wang", "Jing Jiang"], "emails": ["shwang.2014@phdis.smu.edu.sg", "jingjiang@smu.edu.sg"], "sections": [{"heading": "1 INTRODUCTION", "text": "It involves two or more sequences to make a decision. For example, in textual processing, one has to determine whether a mortgage rate can be derived from a premise set (Bowman et al., 2015). Indeed, a question has to be compared against it to find the right answer (Richardson et al., 2013; Tapaswi et al., 2016). In the second example, there are two sequencers that compare problems with each other. A passage, a question, and four candidate answers are given. We can see that we have to find the right answer to match the question against the passage and identify the last sentence that has the response behavior. In the second example, we have to find answers and a set of candidate answers that best matches the question. Given the fundamental importance of comparing two sequences of the text to assess their semantic similarity or relativization."}, {"heading": "2 METHOD", "text": "In this section, we propose a general model that follows the \"compare-aggregate\" framework for the concordance of two sequences. This general model can be applied to different tasks. We will focus our discussion on six different comparison functions that can be inserted into this general \"compare-aggregate\" model. In particular, we assume that two comparison functions based on elementary operations, SUB and MULT, are a good middle ground between highly flexible functions that use standard neural network models and highly restrictive functions based on cosmic similarity and / or Euclidean1https: / / github.com / shuohangwang / SeqMatchSeqdistance. As we will show in the experiment section, these comparison functions, which are based on elementary operations, can actually work very well on a number of sequence concordance problems."}, {"heading": "2.1 PROBLEM DEFINITION AND MODEL OVERVIEW", "text": "We assume that there are two sequences that must be assigned to Q \u00d7 maps A. We use two matrices Q-Rd-Q and A-Rd-A to represent the word embedding of the two sequences, where Q and A are the lengths of the two sequences, and d is the dimensionality of the word embedding. In other words, each column vector of Q or A is an embedding vector that represents a single word. Given a pair of Q and A, the goal is to predict a caption y. For example, in textual embedding, Q can represent a premise and a hypothesis, and y indicates whether Q contains an embedding sequence A or contradicts A. When answering the question, Q can be a question and a candidate answer, and y indicates whether A is the correct answer to Q.We treat the problem as an overarching learning task. We assume that A will play form given by Y in a training series."}, {"heading": "2.2 PREPROCESSING AND ATTENTION", "text": "Our pre-processing layer uses a recursive neural network to process the two sequences. We use a modified version of LSTM / GRU, in which we keep only the input gates to remember meaningful words: Q = \u03c3 (WiQ + bi'eQ) tanh (WuQ + bu'eQ), A = \u03c3 (WiA + bi'eA) tanh (WuA + bu'eA), (1) where elementary multiplication takes place, and Wi, Wu'Rl'd and bi, bu'Rl are parameters to be learned. The outer product (\u00b7 eX) creates a matrix or a row vector by repeating the vector or scalar on the left X times. The attention layer is composed of the resulting questions and A as follows: G = softmax ((((((WgQ + bg'eQ) TA)), H = G, where the parameters are \u00d7 2, bg 'and Rl')."}, {"heading": "2.3 COMPARISON", "text": "iiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiii"}, {"heading": "2.4 AGGREGATION", "text": "After applying the comparison function to each pair of aj and hj to obtain a set of vectors tj, we finally aggregate these vectors using a single-layer CNN (Kim, 2014): r = CNN ([t1,..., tA]). (9) r-Rnl is then used for the final classification, where n is the number of windows in CNN."}, {"heading": "3 EXPERIMENTS", "text": "In this section, we evaluate our model using four different datasets representing different tasks: the first three datasets are for answering questions, the last one is for word processing; the statistics of the four datasets are presented in Table 2; we will first present the task settings and the way we adapt the \"compare aggregate\" structure to each task; then we will show the baselines for the different datasets; and finally, we will discuss the experimental results shown in Table 3."}, {"heading": "3.1 TASK-SPECIFIC MODEL STRUCTURES", "text": "In all of these tasks, we use the matrix Q-Rd-Q to represent the question or premise and the matrix Ak-Rd-Ak (k-Rd-K) to represent the kest answer or hypothesis. For the machine understanding task MovieQA (Tapaswi et al., 2016), there is also a matrix P-Rd-P that represents the plot of a movie. Here, Q is the length of the question or premise, Ak the length of the kth answer, and P the length of the plot. For the SNLI (Bowman et al., 2015) dataset, the task is text sequence that identifies the relationship (origin, contradiction, or neutrality) between a premise and a hypoxation theorem. Here, K = 1, and there are exactly two sequences that we agree on. The actual model structure is what we have described above."}, {"heading": "3.2 BASELINES", "text": "We did not re-implement these models, but simply used the reported performance for comparison purposes. \u2022 SNLI: \u2022 W-by-W Attention: The model of Rockta \ufffd schel et al. (2015), which first introduced attention mechanism into text inclusion. \u2022 Match-LSTM: The model of Wang & Jiang (2016b), which links the matching words as inputs of an LSTM. \u2022 LSTMN: Long-term memory networks proposed by Cheng et al. (2016). \u2022 Decomp Attention: The other \"compare-aggregate\" model by Parikh et al. (2016). \u2022 EBIM + TreeLSTM: The state-of-the-art model proposed by Chen et al. (2016)."}, {"heading": "3.3 ANALYSIS OF RESULTS", "text": "We use accuracy as a benchmark for the MovieQA, InsuranceQA, and SNLI datasets, because there is only one correct answer or label for each instance. WikiQA can have several correct answers, so the benchmarks we use are Mean Average Precision (MAP) and Mean Reciprocal Rank (MRR), and we derive the following from the results. (1) Overall, we can conclude that our overall \"Compareaggregate\" structure performs best on MovieQA, InsuranceQA, WikiQA datasets, and highly competitive SNLI datasets. (2) The SUBMULT + NN comparison method is generally the best. (3) Some simple comparison functions can perform better than neural networks or network comparison functions. For example, QA can make the comparison function easiest, and COBMULT the comparison function cannot perform best on viewers."}, {"heading": "3.4 FURTHER ANALYSES", "text": "To further explain how our model works, we illustrate the maximum values in each dimension of the wave layer. We use two examples shown in Table 1 from the MovieQA and InsuranceQA datasets, respectively. In Figure 2, we can see that the plotwords that also appear in the question or answer will attract more attention from CNN. We assume that if the nearby words in the figure match both the words in question and the words in an answer, this answer will more likely be the correct one. Likewise, the lower one in Figure 2 shows that CNN will focus more on the matching word representations. If the words in an answer continuously match the words in the question, this answer is more likely to be the correct one."}, {"heading": "4 RELATED WORK", "text": "Siamense network: These types of models use the same structure as RNN or CNN to generate the representations for the sequences separately and then use them for classification, using cosine similarities (Feng et al., 2015; Yang et al., 2015), elementary operation (Tai et al., 2015; Mou et al., 2016), or neural network-based combination Bowman et al. (2015) for sequence comparison. Attentive network: Soft-attention mechanism (Bahdanau et al., 2014) was widely used for sequence matching in machine understanding (Hermann et al., 2015), text sequencing (Rockta \ufffd schel et al., 2015), and answering questions (Tan et al., 2016). Instead of using the final state of RNN to represent a sequence, these studies use the weighted sum of all states for sequence representation."}, {"heading": "5 CONCLUSIONS", "text": "In this paper, we systematically analyzed the effectiveness of a \"compare-aggregate\" model on four different sets of data representing different tasks. In addition, we compared and tested different types of word-level comparison functions and found that some elementary comparison functions can outperform others. According to our test results, many different tasks can share the same \"compare-aggregate\" structure. In future work, we would like to test their effectiveness in the field of multi-task learning."}, {"heading": "A APPENDIX", "text": "We use ADAMAX (Kingma & Ba, 2015) with coefficients \u03b21 = 0.9 and \u03b22 = 0.999 to optimize the model. Batch size is set at 30 and the learning rate is 0.002. We do not use L2 regulation. The hyperparameter we have set is the dropout on the embedding layer. For WikiQA, which is a relatively small data set, we also adjust the learning rate and the batch size. For the revolutionary window sizes of MovieQA, InsuranceQA, WikiQA and SNLI, we use [1,3,5], [1,2,3], [1,2,3,4,5] and [1,2,3,4,5], each 4.5."}], "references": [{"title": "Neural machine translation by jointly learning to align and translate", "author": ["Dzmitry Bahdanau", "Kyunghyun Cho", "Yoshua Bengio"], "venue": "In Proceedings of the International Conference on Learning Representations,", "citeRegEx": "Bahdanau et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Bahdanau et al\\.", "year": 2014}, {"title": "Learning concept importance using a weighted dependence model", "author": ["Michael Bendersky", "Donald Metzler", "W Bruce Croft"], "venue": "In Proceedings of the third ACM international conference on Web search and data mining,", "citeRegEx": "Bendersky et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Bendersky et al\\.", "year": 2010}, {"title": "A large annotated corpus for learning natural language inference", "author": ["Samuel R Bowman", "Gabor Angeli", "Christopher Potts", "Christopher D Manning"], "venue": "In Proceedings of the Conference on Empirical Methods in Natural Language Processing,", "citeRegEx": "Bowman et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Bowman et al\\.", "year": 2015}, {"title": "Enhancing and combining sequential and tree lstm for natural language inference", "author": ["Qian Chen", "Xiaodan Zhu", "Zhenhua Ling", "Si Wei", "Hui Jiang"], "venue": "arXiv preprint arXiv:1609.06038,", "citeRegEx": "Chen et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Chen et al\\.", "year": 2016}, {"title": "Long short-term memory-networks for machine reading", "author": ["Jianpeng Cheng", "Li Dong", "Mirella Lapata"], "venue": "In Proceedings of the Conference on Empirical Methods in Natural Language Processing,", "citeRegEx": "Cheng et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Cheng et al\\.", "year": 2016}, {"title": "Applying deep learning to answer selection: A study and an open task", "author": ["Minwei Feng", "Bing Xiang", "Michael R Glass", "Lidan Wang", "Bowen Zhou"], "venue": "In 2015 IEEE Workshop on Automatic Speech Recognition and Understanding (ASRU),", "citeRegEx": "Feng et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Feng et al\\.", "year": 2015}, {"title": "Pairwise word interaction modeling with deep neural networks for semantic similarity measurement", "author": ["Hua He", "Jimmy Lin"], "venue": "In Proceedings of the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies,", "citeRegEx": "He and Lin.,? \\Q2016\\E", "shortCiteRegEx": "He and Lin.", "year": 2016}, {"title": "Teaching machines to read and comprehend", "author": ["Karl Moritz Hermann", "Tomas Kocisky", "Edward Grefenstette", "Lasse Espeholt", "Will Kay", "Mustafa Suleyman", "Phil Blunsom"], "venue": "In Proceedings of the Conference on Advances in Neural Information Processing Systems,", "citeRegEx": "Hermann et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Hermann et al\\.", "year": 2015}, {"title": "The Goldilocks principle: Reading children\u2019s books with explicit memory representations", "author": ["Felix Hill", "Antoine Bordes", "Sumit Chopra", "Jason Weston"], "venue": "In Proceedings of the International Conference on Learning Representations,", "citeRegEx": "Hill et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Hill et al\\.", "year": 2016}, {"title": "Convolutional neural network architectures for matching natural language sentences", "author": ["Baotian Hu", "Zhengdong Lu", "Hang Li", "Qingcai Chen"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Hu et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Hu et al\\.", "year": 2014}, {"title": "Convolutional neural networks for sentence classification", "author": ["Yoon Kim"], "venue": "In Proceedings of the Conference on Empirical Methods in Natural Language Processing,", "citeRegEx": "Kim.,? \\Q2014\\E", "shortCiteRegEx": "Kim.", "year": 2014}, {"title": "Adam: A method for stochastic optimization", "author": ["Diederik Kingma", "Jimmy Ba"], "venue": "In Proceedings of the International Conference on Learning Representations,", "citeRegEx": "Kingma and Ba.,? \\Q2015\\E", "shortCiteRegEx": "Kingma and Ba.", "year": 2015}, {"title": "Natural language inference by tree-based convolution and heuristic matching", "author": ["Lili Mou", "Rui Men", "Ge Li", "Yan Xu", "Lu Zhang", "Rui Yan", "Zhi Jin"], "venue": "In Proceedings of the Conference on Empirical Methods in Natural Language Processing,", "citeRegEx": "Mou et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Mou et al\\.", "year": 2016}, {"title": "A decomposable attention model for natural language inference", "author": ["Ankur P Parikh", "Oscar T\u00e4ckstr\u00f6m", "Dipanjan Das", "Jakob Uszkoreit"], "venue": "In Proceedings of the Conference on Empirical Methods in Natural Language Processing,", "citeRegEx": "Parikh et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Parikh et al\\.", "year": 2016}, {"title": "GloVe: Global vectors for word representation", "author": ["Jeffrey Pennington", "Richard Socher", "Christopher D Manning"], "venue": "In Proceedings of the Conference on Empirical Methods in Natural Language Processing,", "citeRegEx": "Pennington et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Pennington et al\\.", "year": 2014}, {"title": "MCTest: A challenge dataset for the open-domain machine comprehension of text", "author": ["Matthew Richardson", "Christopher JC Burges", "Erin Renshaw"], "venue": "In Proceedings of the Conference on Empirical Methods in Natural Language Processing,", "citeRegEx": "Richardson et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Richardson et al\\.", "year": 2013}, {"title": "Reasoning about entailment with neural attention", "author": ["Tim Rockt\u00e4schel", "Edward Grefenstette", "Karl Moritz Hermann", "Tom\u00e1\u0161 Ko\u010disk\u1ef3", "Phil Blunsom"], "venue": "In Proceedings of the International Conference on Learning Representations,", "citeRegEx": "Rockt\u00e4schel et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Rockt\u00e4schel et al\\.", "year": 2015}, {"title": "Recursive deep models for semantic compositionality over a sentiment treebank", "author": ["Richard Socher", "Alex Perelygin", "Jean Y Wu", "Jason Chuang", "Christopher D Manning", "Andrew Y Ng", "Christopher Potts"], "venue": "In Proceedings of the conference on empirical methods in natural language processing,", "citeRegEx": "Socher et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Socher et al\\.", "year": 2013}, {"title": "Improved semantic representations from tree-structured long short-term memory networks", "author": ["Kai Sheng Tai", "Richard Socher", "Christopher D Manning"], "venue": "In Proceedings of the Conference on Association for Computational Linguistics,", "citeRegEx": "Tai et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Tai et al\\.", "year": 2015}, {"title": "Improved representation learning for question answer matching", "author": ["Ming Tan", "Cicero dos Santos", "Bing Xiang", "Bowen Zhou"], "venue": "In Proceedings of the Conference on Association for Computational Linguistics,", "citeRegEx": "Tan et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Tan et al\\.", "year": 2016}, {"title": "MovieQA: Understanding stories in movies through question-answering", "author": ["Makarand Tapaswi", "Yukun Zhu", "Rainer Stiefelhagen", "Antonio Torralba", "Raquel Urtasun", "Sanja Fidler"], "venue": "In Proceedings of IEEE Conference on Computer Vision and Pattern Recognition,", "citeRegEx": "Tapaswi et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Tapaswi et al\\.", "year": 2016}, {"title": "A parallel-hierarchical model for machine comprehension on sparse data", "author": ["Adam Trischler", "Zheng Ye", "Xingdi Yuan", "Jing He", "Phillip Bachman", "Kaheer Suleman"], "venue": "In Proceedings of the Conference on Association for Computational Linguistics,", "citeRegEx": "Trischler et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Trischler et al\\.", "year": 2016}, {"title": "Inner attention based recurrent neural networks for answer selection", "author": ["Bingning Wang", "Kang Liu", "Jun Zhao"], "venue": "In Proceedings of the Conference on Association for Computational Linguistics,", "citeRegEx": "Wang et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Wang et al\\.", "year": 2016}, {"title": "Machine comprehension using match-lstm and answer pointer", "author": ["Shuohang Wang", "Jing Jiang"], "venue": "arXiv preprint arXiv:1608.07905,", "citeRegEx": "Wang and Jiang.,? \\Q2016\\E", "shortCiteRegEx": "Wang and Jiang.", "year": 2016}, {"title": "Learning natural language inference with LSTM", "author": ["Shuohang Wang", "Jing Jiang"], "venue": "In Proceedings of the Conference on the North American Chapter of the Association for Computational Linguistics,", "citeRegEx": "Wang and Jiang.,? \\Q2016\\E", "shortCiteRegEx": "Wang and Jiang.", "year": 2016}, {"title": "Wikiqa: A challenge dataset for open-domain question answering", "author": ["Yi Yang", "Wen-tau Yih", "Christopher Meek"], "venue": "In Proceedings of the Conference on Empirical Methods in Natural Language Processing,", "citeRegEx": "Yang et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Yang et al\\.", "year": 2015}, {"title": "Abcnn: Attention-based convolutional neural network for modeling sentence pairs", "author": ["Wenpeng Yin", "Hinrich Sch\u00fctze", "Bing Xiang", "Bowen Zhou"], "venue": "arXiv preprint arXiv:1512.05193,", "citeRegEx": "Yin et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Yin et al\\.", "year": 2015}], "referenceMentions": [{"referenceID": 2, "context": "For example, in textual entailment, one needs to determine whether a hypothesis sentence can be inferred from a premise sentence (Bowman et al., 2015).", "startOffset": 129, "endOffset": 150}, {"referenceID": 15, "context": "In machine comprehension, given a passage, a question needs to be matched against it in order to find the correct answer (Richardson et al., 2013; Tapaswi et al., 2016).", "startOffset": 121, "endOffset": 168}, {"referenceID": 20, "context": "In machine comprehension, given a passage, a question needs to be matched against it in order to find the correct answer (Richardson et al., 2013; Tapaswi et al., 2016).", "startOffset": 121, "endOffset": 168}, {"referenceID": 2, "context": "To match two sequences, a straightforward approach is to encode each sequence as a vector and then to combine the two vectors to make a decision (Bowman et al., 2015; Feng et al., 2015).", "startOffset": 145, "endOffset": 185}, {"referenceID": 5, "context": "To match two sequences, a straightforward approach is to encode each sequence as a vector and then to combine the two vectors to make a decision (Bowman et al., 2015; Feng et al., 2015).", "startOffset": 145, "endOffset": 185}, {"referenceID": 7, "context": "However, it has been found that using a single vector to encode an entire sequence is not sufficient to capture all the important information from the sequence, and therefore advanced techniques such as attention mechanisms and memory networks have been applied to sequence matching problems (Hermann et al., 2015; Hill et al., 2016; Rockt\u00e4schel et al., 2015).", "startOffset": 292, "endOffset": 359}, {"referenceID": 8, "context": "However, it has been found that using a single vector to encode an entire sequence is not sufficient to capture all the important information from the sequence, and therefore advanced techniques such as attention mechanisms and memory networks have been applied to sequence matching problems (Hermann et al., 2015; Hill et al., 2016; Rockt\u00e4schel et al., 2015).", "startOffset": 292, "endOffset": 359}, {"referenceID": 16, "context": "However, it has been found that using a single vector to encode an entire sequence is not sufficient to capture all the important information from the sequence, and therefore advanced techniques such as attention mechanisms and memory networks have been applied to sequence matching problems (Hermann et al., 2015; Hill et al., 2016; Rockt\u00e4schel et al., 2015).", "startOffset": 292, "endOffset": 359}, {"referenceID": 13, "context": "A common trait of a number of these recent studies on sequence matching problems is the use of a \u201ccompare-aggregate\u201d framework (Wang & Jiang, 2016b; He & Lin, 2016; Parikh et al., 2016).", "startOffset": 127, "endOffset": 185}, {"referenceID": 2, "context": "For example, in textual entailment, one needs to determine whether a hypothesis sentence can be inferred from a premise sentence (Bowman et al., 2015). In machine comprehension, given a passage, a question needs to be matched against it in order to find the correct answer (Richardson et al., 2013; Tapaswi et al., 2016). Table 1 gives two example sequence matching problems. In the first example, a passage, a question and four candidate answers are given. We can see that to get the correct answer, we need to match the question against the passage and identify the last sentence to be the answer-bearing sentence. In the second example, given a question and a set of candidate answers, we need to find the answer that best matches the question. Because of the fundamental importance of comparing two sequences of text to judge their semantic similarity or relatedness, sequence matching has been well studied in natural language processing. With recent advances of neural network models in natural language processing, a standard practice for sequence modeling now is to encode a sequence of text as an embedding vector using models such as RNN and CNN. To match two sequences, a straightforward approach is to encode each sequence as a vector and then to combine the two vectors to make a decision (Bowman et al., 2015; Feng et al., 2015). However, it has been found that using a single vector to encode an entire sequence is not sufficient to capture all the important information from the sequence, and therefore advanced techniques such as attention mechanisms and memory networks have been applied to sequence matching problems (Hermann et al., 2015; Hill et al., 2016; Rockt\u00e4schel et al., 2015). A common trait of a number of these recent studies on sequence matching problems is the use of a \u201ccompare-aggregate\u201d framework (Wang & Jiang, 2016b; He & Lin, 2016; Parikh et al., 2016). In such a framework, comparison of two sequences is not done by comparing two vectors each representing an entire sequence. Instead, these models first compare vector representations of smaller units such as words from these sequences and then aggregate these comparison results to make the final decision. For example, the match-LSTM model proposed by Wang & Jiang (2016b) for textual entailment first compares each word in the hypothesis with an attention-weighted version of the premise.", "startOffset": 130, "endOffset": 2266}, {"referenceID": 2, "context": "For example, in textual entailment, one needs to determine whether a hypothesis sentence can be inferred from a premise sentence (Bowman et al., 2015). In machine comprehension, given a passage, a question needs to be matched against it in order to find the correct answer (Richardson et al., 2013; Tapaswi et al., 2016). Table 1 gives two example sequence matching problems. In the first example, a passage, a question and four candidate answers are given. We can see that to get the correct answer, we need to match the question against the passage and identify the last sentence to be the answer-bearing sentence. In the second example, given a question and a set of candidate answers, we need to find the answer that best matches the question. Because of the fundamental importance of comparing two sequences of text to judge their semantic similarity or relatedness, sequence matching has been well studied in natural language processing. With recent advances of neural network models in natural language processing, a standard practice for sequence modeling now is to encode a sequence of text as an embedding vector using models such as RNN and CNN. To match two sequences, a straightforward approach is to encode each sequence as a vector and then to combine the two vectors to make a decision (Bowman et al., 2015; Feng et al., 2015). However, it has been found that using a single vector to encode an entire sequence is not sufficient to capture all the important information from the sequence, and therefore advanced techniques such as attention mechanisms and memory networks have been applied to sequence matching problems (Hermann et al., 2015; Hill et al., 2016; Rockt\u00e4schel et al., 2015). A common trait of a number of these recent studies on sequence matching problems is the use of a \u201ccompare-aggregate\u201d framework (Wang & Jiang, 2016b; He & Lin, 2016; Parikh et al., 2016). In such a framework, comparison of two sequences is not done by comparing two vectors each representing an entire sequence. Instead, these models first compare vector representations of smaller units such as words from these sequences and then aggregate these comparison results to make the final decision. For example, the match-LSTM model proposed by Wang & Jiang (2016b) for textual entailment first compares each word in the hypothesis with an attention-weighted version of the premise. The comparison results are then aggregated through an LSTM. He & Lin (2016) proposed a pairwise word interaction model that first takes each pair of words from two sequences and applies a comparison unit on the two words.", "startOffset": 130, "endOffset": 2459}, {"referenceID": 2, "context": "For example, in textual entailment, one needs to determine whether a hypothesis sentence can be inferred from a premise sentence (Bowman et al., 2015). In machine comprehension, given a passage, a question needs to be matched against it in order to find the correct answer (Richardson et al., 2013; Tapaswi et al., 2016). Table 1 gives two example sequence matching problems. In the first example, a passage, a question and four candidate answers are given. We can see that to get the correct answer, we need to match the question against the passage and identify the last sentence to be the answer-bearing sentence. In the second example, given a question and a set of candidate answers, we need to find the answer that best matches the question. Because of the fundamental importance of comparing two sequences of text to judge their semantic similarity or relatedness, sequence matching has been well studied in natural language processing. With recent advances of neural network models in natural language processing, a standard practice for sequence modeling now is to encode a sequence of text as an embedding vector using models such as RNN and CNN. To match two sequences, a straightforward approach is to encode each sequence as a vector and then to combine the two vectors to make a decision (Bowman et al., 2015; Feng et al., 2015). However, it has been found that using a single vector to encode an entire sequence is not sufficient to capture all the important information from the sequence, and therefore advanced techniques such as attention mechanisms and memory networks have been applied to sequence matching problems (Hermann et al., 2015; Hill et al., 2016; Rockt\u00e4schel et al., 2015). A common trait of a number of these recent studies on sequence matching problems is the use of a \u201ccompare-aggregate\u201d framework (Wang & Jiang, 2016b; He & Lin, 2016; Parikh et al., 2016). In such a framework, comparison of two sequences is not done by comparing two vectors each representing an entire sequence. Instead, these models first compare vector representations of smaller units such as words from these sequences and then aggregate these comparison results to make the final decision. For example, the match-LSTM model proposed by Wang & Jiang (2016b) for textual entailment first compares each word in the hypothesis with an attention-weighted version of the premise. The comparison results are then aggregated through an LSTM. He & Lin (2016) proposed a pairwise word interaction model that first takes each pair of words from two sequences and applies a comparison unit on the two words. It then combines the results of these word interactions using a similarity focus layer followed by a multi-layer CNN. Parikh et al. (2016) proposed a decomposable attention model for textual entailment, in which words from each sequence are compared with an", "startOffset": 130, "endOffset": 2744}, {"referenceID": 9, "context": "Usually a standard feedforward network is used (Hu et al., 2014; Wang & Jiang, 2016b) to combine two vectors representing two units that need to be compared, e.", "startOffset": 47, "endOffset": 85}, {"referenceID": 9, "context": "Usually a standard feedforward network is used (Hu et al., 2014; Wang & Jiang, 2016b) to combine two vectors representing two units that need to be compared, e.g., two words. However, based on the nature of these sequence matching problems, we essentially need to measure how semantically similar the two sequences are. Presumably, this property of these sequence matching problems should guide us in choosing more appropriate comparison functions. Indeed He & Lin (2016) used cosine similarity, Euclidean distance and dot product to define the comparison function, which seem to be better justifiable.", "startOffset": 48, "endOffset": 472}, {"referenceID": 13, "context": "Although this model follows more or less the same framework as the model proposed by Parikh et al. (2016), our work has some notable differences.", "startOffset": 85, "endOffset": 106}, {"referenceID": 13, "context": "Although this model follows more or less the same framework as the model proposed by Parikh et al. (2016), our work has some notable differences. First, we will pay much attention to the comparison function f and compare a number of options, including a some uncommon ones based on elementwise operations. Second, we apply our model to four different datasets representing four different tasks to evaluate its general effectiveness for sequence matching problems. There are also some other differences from the work by Parikh et al. (2016). For example, we use a CNN layer instead of summation and concatenation for aggregation.", "startOffset": 85, "endOffset": 540}, {"referenceID": 17, "context": "Alternatively, another natural choice is a neural tensor network (Socher et al., 2013) as follows:", "startOffset": 65, "endOffset": 86}, {"referenceID": 18, "context": "These functions have been used previously by Tai et al. (2015). SUBTRACTION (SUB): tj = f(aj ,hj) = (aj \u2212 hj) (aj \u2212 hj), (6) MULTIPLICATION (MULT): tj = f(aj ,hj) = aj hj .", "startOffset": 45, "endOffset": 63}, {"referenceID": 10, "context": "After we apply the comparison function to each pair of aj and hj to obtain a series of vectors tj , finally we aggregate these vectors using a one-layer CNN (Kim, 2014):", "startOffset": 157, "endOffset": 168}, {"referenceID": 20, "context": "For the machine comprehension task MovieQA (Tapaswi et al., 2016), there is also a matrix P \u2208 Rd\u00d7P that represents the plot of a movie.", "startOffset": 43, "endOffset": 65}, {"referenceID": 2, "context": "For the SNLI (Bowman et al., 2015) dataset, the task is text entailment, which identifies the relationship (entailment, contradiction or neutral) between a premise sentence and a hypothesis sentence.", "startOffset": 13, "endOffset": 34}, {"referenceID": 5, "context": "For the InsuranceQA (Feng et al., 2015) dataset, the task is an answer selection task which needs to select the correct answer for a question from a candidate pool.", "startOffset": 20, "endOffset": 39}, {"referenceID": 25, "context": "For the WikiQA (Yang et al., 2015) datasets, we need to rank the candidate answers according to a question.", "startOffset": 15, "endOffset": 34}, {"referenceID": 11, "context": "SNLI: \u2022W-by-W Attention: The model by Rockt\u00e4schel et al. (2015), who first introduced attention mechanism into text entailment.", "startOffset": 38, "endOffset": 64}, {"referenceID": 11, "context": "SNLI: \u2022W-by-W Attention: The model by Rockt\u00e4schel et al. (2015), who first introduced attention mechanism into text entailment. \u2022 match-LSTM: The model by Wang & Jiang (2016b), which concatenates the matched words as the inputs of an LSTM.", "startOffset": 38, "endOffset": 176}, {"referenceID": 2, "context": "\u2022 LSTMN: Long short-term memorynetworks proposed by Cheng et al. (2016). \u2022 Decomp Attention: Another \u201ccompare-aggregate\u201d model proposed by Parikh et al.", "startOffset": 52, "endOffset": 72}, {"referenceID": 2, "context": "\u2022 LSTMN: Long short-term memorynetworks proposed by Cheng et al. (2016). \u2022 Decomp Attention: Another \u201ccompare-aggregate\u201d model proposed by Parikh et al. (2016). \u2022 EBIM+TreeLSTM: The state-of-the-art model proposed by Chen et al.", "startOffset": 52, "endOffset": 160}, {"referenceID": 2, "context": "\u2022 EBIM+TreeLSTM: The state-of-the-art model proposed by Chen et al. (2016) on the SNLI dataset.", "startOffset": 56, "endOffset": 75}, {"referenceID": 1, "context": "InsuranceQA: \u2022 IR model: This model by Bendersky et al. (2010) learns the concept information to help rank the candidates.", "startOffset": 39, "endOffset": 63}, {"referenceID": 1, "context": "InsuranceQA: \u2022 IR model: This model by Bendersky et al. (2010) learns the concept information to help rank the candidates. \u2022 CNN with GESD: This model by Feng et al. (2015) uses Euclidean distance and dot product between sequence representations built through convolutional neural networks to select the answer.", "startOffset": 39, "endOffset": 173}, {"referenceID": 1, "context": "InsuranceQA: \u2022 IR model: This model by Bendersky et al. (2010) learns the concept information to help rank the candidates. \u2022 CNN with GESD: This model by Feng et al. (2015) uses Euclidean distance and dot product between sequence representations built through convolutional neural networks to select the answer. \u2022 Attentive LSTM: Tan et al. (2016) used soft-attention mechanism to select the most important information from the candidates according to the representation of the questions.", "startOffset": 39, "endOffset": 348}, {"referenceID": 1, "context": "InsuranceQA: \u2022 IR model: This model by Bendersky et al. (2010) learns the concept information to help rank the candidates. \u2022 CNN with GESD: This model by Feng et al. (2015) uses Euclidean distance and dot product between sequence representations built through convolutional neural networks to select the answer. \u2022 Attentive LSTM: Tan et al. (2016) used soft-attention mechanism to select the most important information from the candidates according to the representation of the questions. \u2022 IARNN-Occam: This model by Wang et al. (2016) adds regularization on the attention weights.", "startOffset": 39, "endOffset": 537}, {"referenceID": 1, "context": "InsuranceQA: \u2022 IR model: This model by Bendersky et al. (2010) learns the concept information to help rank the candidates. \u2022 CNN with GESD: This model by Feng et al. (2015) uses Euclidean distance and dot product between sequence representations built through convolutional neural networks to select the answer. \u2022 Attentive LSTM: Tan et al. (2016) used soft-attention mechanism to select the most important information from the candidates according to the representation of the questions. \u2022 IARNN-Occam: This model by Wang et al. (2016) adds regularization on the attention weights. \u2022 IARNN-Gate: This model by Wang et al. (2016) uses the representation of the question to build the GRU gates for each candidate answer.", "startOffset": 39, "endOffset": 630}, {"referenceID": 1, "context": "InsuranceQA: \u2022 IR model: This model by Bendersky et al. (2010) learns the concept information to help rank the candidates. \u2022 CNN with GESD: This model by Feng et al. (2015) uses Euclidean distance and dot product between sequence representations built through convolutional neural networks to select the answer. \u2022 Attentive LSTM: Tan et al. (2016) used soft-attention mechanism to select the most important information from the candidates according to the representation of the questions. \u2022 IARNN-Occam: This model by Wang et al. (2016) adds regularization on the attention weights. \u2022 IARNN-Gate: This model by Wang et al. (2016) uses the representation of the question to build the GRU gates for each candidate answer. WikiQA: \u2022 IARNN-Occam and IARNN-Gate as introduced before. \u2022 CNN-Cnt: This model by Yang et al. (2015) combines sentence representations built by a convolutional neural network with logistic regression.", "startOffset": 39, "endOffset": 823}, {"referenceID": 1, "context": "InsuranceQA: \u2022 IR model: This model by Bendersky et al. (2010) learns the concept information to help rank the candidates. \u2022 CNN with GESD: This model by Feng et al. (2015) uses Euclidean distance and dot product between sequence representations built through convolutional neural networks to select the answer. \u2022 Attentive LSTM: Tan et al. (2016) used soft-attention mechanism to select the most important information from the candidates according to the representation of the questions. \u2022 IARNN-Occam: This model by Wang et al. (2016) adds regularization on the attention weights. \u2022 IARNN-Gate: This model by Wang et al. (2016) uses the representation of the question to build the GRU gates for each candidate answer. WikiQA: \u2022 IARNN-Occam and IARNN-Gate as introduced before. \u2022 CNN-Cnt: This model by Yang et al. (2015) combines sentence representations built by a convolutional neural network with logistic regression. \u2022 ABCNN: This model is Attention-Based Convolutional Neural Network proposed by Yin et al. (2015). \u2022 CubeCNN proposed by He & Lin (2016) builds a CNN on all pairs of word similarity.", "startOffset": 39, "endOffset": 1021}, {"referenceID": 1, "context": "InsuranceQA: \u2022 IR model: This model by Bendersky et al. (2010) learns the concept information to help rank the candidates. \u2022 CNN with GESD: This model by Feng et al. (2015) uses Euclidean distance and dot product between sequence representations built through convolutional neural networks to select the answer. \u2022 Attentive LSTM: Tan et al. (2016) used soft-attention mechanism to select the most important information from the candidates according to the representation of the questions. \u2022 IARNN-Occam: This model by Wang et al. (2016) adds regularization on the attention weights. \u2022 IARNN-Gate: This model by Wang et al. (2016) uses the representation of the question to build the GRU gates for each candidate answer. WikiQA: \u2022 IARNN-Occam and IARNN-Gate as introduced before. \u2022 CNN-Cnt: This model by Yang et al. (2015) combines sentence representations built by a convolutional neural network with logistic regression. \u2022 ABCNN: This model is Attention-Based Convolutional Neural Network proposed by Yin et al. (2015). \u2022 CubeCNN proposed by He & Lin (2016) builds a CNN on all pairs of word similarity.", "startOffset": 39, "endOffset": 1060}, {"referenceID": 1, "context": "InsuranceQA: \u2022 IR model: This model by Bendersky et al. (2010) learns the concept information to help rank the candidates. \u2022 CNN with GESD: This model by Feng et al. (2015) uses Euclidean distance and dot product between sequence representations built through convolutional neural networks to select the answer. \u2022 Attentive LSTM: Tan et al. (2016) used soft-attention mechanism to select the most important information from the candidates according to the representation of the questions. \u2022 IARNN-Occam: This model by Wang et al. (2016) adds regularization on the attention weights. \u2022 IARNN-Gate: This model by Wang et al. (2016) uses the representation of the question to build the GRU gates for each candidate answer. WikiQA: \u2022 IARNN-Occam and IARNN-Gate as introduced before. \u2022 CNN-Cnt: This model by Yang et al. (2015) combines sentence representations built by a convolutional neural network with logistic regression. \u2022 ABCNN: This model is Attention-Based Convolutional Neural Network proposed by Yin et al. (2015). \u2022 CubeCNN proposed by He & Lin (2016) builds a CNN on all pairs of word similarity. MovieQA: All the baselines we consider come from Tapaswi et al. (2016)\u2019s work: \u2022 Cosine Word2Vec: A sliding window is used to select the answer according to the similarities computed through Word2Vec between the sentences in plot and the question/answer.", "startOffset": 39, "endOffset": 1177}, {"referenceID": 5, "context": "Then cosine similarity (Feng et al., 2015; Yang et al., 2015), element-wise operation (Tai et al.", "startOffset": 23, "endOffset": 61}, {"referenceID": 25, "context": "Then cosine similarity (Feng et al., 2015; Yang et al., 2015), element-wise operation (Tai et al.", "startOffset": 23, "endOffset": 61}, {"referenceID": 18, "context": ", 2015), element-wise operation (Tai et al., 2015; Mou et al., 2016) or neural network-based combination Bowman et al.", "startOffset": 32, "endOffset": 68}, {"referenceID": 12, "context": ", 2015), element-wise operation (Tai et al., 2015; Mou et al., 2016) or neural network-based combination Bowman et al.", "startOffset": 32, "endOffset": 68}, {"referenceID": 0, "context": "Attentive network: Soft-attention mechanism (Bahdanau et al., 2014) has been widely used for sequence matching in machine comprehension (Hermann et al.", "startOffset": 44, "endOffset": 67}, {"referenceID": 7, "context": ", 2014) has been widely used for sequence matching in machine comprehension (Hermann et al., 2015), text entailment (Rockt\u00e4schel et al.", "startOffset": 76, "endOffset": 98}, {"referenceID": 16, "context": ", 2015), text entailment (Rockt\u00e4schel et al., 2015) and question answering (Tan et al.", "startOffset": 25, "endOffset": 51}, {"referenceID": 19, "context": ", 2015) and question answering (Tan et al., 2016).", "startOffset": 31, "endOffset": 49}, {"referenceID": 13, "context": "Compare-Aggregate network: This kind of framework is to perform the word level matching (Wang & Jiang, 2016a; Parikh et al., 2016; He & Lin, 2016; Trischler et al., 2016).", "startOffset": 88, "endOffset": 170}, {"referenceID": 21, "context": "Compare-Aggregate network: This kind of framework is to perform the word level matching (Wang & Jiang, 2016a; Parikh et al., 2016; He & Lin, 2016; Trischler et al., 2016).", "startOffset": 88, "endOffset": 170}, {"referenceID": 1, "context": ", 2016) or neural network-based combination Bowman et al. (2015) are used for sequence matching.", "startOffset": 44, "endOffset": 65}], "year": 2016, "abstractText": "Many NLP tasks including machine comprehension, answer selection and text entailment require the comparison between sequences. Matching the important units between sequences is a key to solve these problems. In this paper, we present a general \u201ccompare-aggregate\u201d framework that performs word-level matching followed by aggregation using Convolutional Neural Networks. We particularly focus on the different comparison functions we can use to match two vectors. We use four different datasets to evaluate the model. We find that some simple comparison functions based on element-wise operations can work better than standard neural network and neural tensor network.", "creator": "LaTeX with hyperref package"}}}