{"id": "1611.01779", "review": {"conference": "iclr", "VERSION": "v1", "DATE_OF_SUBMISSION": "6-Nov-2016", "title": "Learning to Act by Predicting the Future", "abstract": "We present an approach to sensorimotor control in immersive environments. Our approach utilizes a high-dimensional sensory stream and a lower-dimensional measurement stream. The cotemporal structure of these streams provides a rich supervisory signal, which enables training a sensorimotor control model by interacting with the environment. The model is trained using supervised learning techniques, but without extraneous supervision. It learns to act based on raw sensory input from a complex three-dimensional environment. The presented formulation allows changing the agent's goal at test time. We conduct extensive experiments in three-dimensional simulations based on the classical first-person game Doom. The results demonstrate that the presented approach outperforms sophisticated prior formulations, particularly on challenging tasks. The results also show that trained models successfully generalize across environments and goals. A model trained using the presented approach won the Full Deathmatch track of the Visual Doom AI Competition, which was held in previously unseen environments.", "histories": [["v1", "Sun, 6 Nov 2016 13:45:00 GMT  (3500kb,D)", "http://arxiv.org/abs/1611.01779v1", "ICLR-2017 submission"], ["v2", "Tue, 14 Feb 2017 19:47:46 GMT  (1100kb,D)", "http://arxiv.org/abs/1611.01779v2", "Published as a conference paper at ICLR 2017"]], "COMMENTS": "ICLR-2017 submission", "reviews": [], "SUBJECTS": "cs.LG cs.AI cs.CV", "authors": ["alexey dosovitskiy", "vladlen koltun"], "accepted": true, "id": "1611.01779"}, "pdf": {"name": "1611.01779.pdf", "metadata": {"source": "CRF", "title": null, "authors": ["Alexey Dosovitskiy", "Vladlen Koltun"], "emails": [], "sections": [{"heading": null, "text": "We present an approach to sensorimotor control in immersive environments. Our approach utilizes a high-dimensional sensory current and a low-dimensional measurement current. The simultaneous structure of these currents provides a rich monitoring signal that enables a sensorimotor control model to be trained by interacting with the environment. The model is trained using monitored learning techniques, but without external monitoring. It learns to operate from a complex three-dimensional environment based on raw sensory input. The presented formulation allows the target of the drug to be changed during the test period.We conduct extensive experiments in three-dimensional simulations based on the classic first-person game Doom. The results show that the presented approach outperforms challenging previous formulations, especially for demanding tasks. The results also show that trained models successfully generalize across environments and objectives. A model trained with the presented approach won the Full Deathoom Track of the Competition Dsual I."}, {"heading": "1 INTRODUCTION", "text": "In this sense, it is important that people who are able to understand things and understand what they do. (...) It is important that people are able to understand and understand things. (...) It is important that people are able to understand and understand things. (...) It is important that people are able to understand and understand things. (...) It is important that people are able to understand things. (...) It is important that people are able to understand and understand things. (...) It is important that people are able to understand and understand things. (...) It is important that people are able to understand things. (...) (...) (...) () () () () () () () () () () () ()) () () () () ()) () ()) () () () ()) () () () () ()) () () () () ()) () () () () () () ()) () () () () () ()) () () () () () () () ()) () () () ()) () () () ()) () () () ()) () ()) () () () ()) () () ()) () () () () () ()) () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () (() () () () (() (() () () (() () (() () () (() () ((() () (() (() () () () (() () () (() () ((((() () (() () () (() (() () ((() () () (() () () ((()"}, {"heading": "2 BACKGROUND", "text": "The perspective of supervised learning (SL) on acting through environmental interaction goes back decades. Jordan & Rumelhart (1992) analyze this approach, review early work, and argue that the choice of SL versus RL should be guided by the characteristics of the environment. Their analysis suggests that RL can be more efficient when the environment provides only a sparse scalar reward signal, whereas SL can be beneficial when time-condensed multidimensional feedback is available. Sutton (1988) analyzes time difference (TD) learning and argues that SL is preferable for predictive problems where the correctness of the prediction reveals many steps after the prediction is made. Sutton's influential analysis assumes a sparse scalar reward. TD and political gradient methods have dominated learning since then (Kober et al., 2013; Mnih et al.)."}, {"heading": "3 MODEL", "text": "We assume that the observations have the following structure: ot = < st, mt >, where st is a raw sensory input and mt is a series of measurements. In our experiments, st is an image: the agent's view of his three-dimensional environment. More generally, st can include input from several sensory modalities. mt measurements can represent the attitude, supply level and structural integrity in a physical system, or health, ammunition and score in a computer game. The distinction between sensory input and mt measurements is artificial: both st and mt represent sensory input in various forms. In our model, the measurement vector mt is distinguished from other sensations in two respects."}, {"heading": "3.1 TRAINING", "text": "The predictor P is trained on the basis of the experience gained by the agent. No human trajectories are used. On the basis of a random strategy, the predictor P begins to interact with its environment. This interaction takes place via episodes that last for a certain number of time steps or until a terminal event. Let's consider a series of experiences gained by the agent, resulting in a series of training examples: D = {< oi, ai, gi, fi > Ni = 1. Here < oi, ai, gi > is the input and fi is the output of example i. The predictor is trained on the basis of a regression loss: L (\u03b8) = N \u00b2 P (oi, ai, gi;) \u2212 fi \u00b2 2. (4) A classification loss can be used to predict categorical measurements, but this was not necessary in our experiments. As the predictor gains new experience, the training setD and the random predictor, which is used by the agent, have a random one."}, {"heading": "3.2 ARCHITECTURE", "text": "The network architecture we use is shown in Figure 1. (The network has three input modules: a perception module S (s), a measurement module M (m) and a target module G (g). In our experiments, s is an image and the perception module S is implemented as a revolutionary network. The measurement and target modules are fully interconnected networks. (5) The results of the three input modules are interlinked and form the common input representation used for subsequent processing: j = J (s, m, g) = < S (s), M (m), G (g) >. Future measurements are predicted on the basis of this input representation. The network predicts future measurements for all measures at once."}, {"heading": "4 EXPERIMENTS", "text": "We evaluate the presented approach in immersive three-dimensional simulations based on the classic game Doom. In these simulations, the agent has a personal view of the environment and must act on the same visual information that is shown to human players in the game. To connect to the game engine, we use the ViZDoom platform developed by Kempka et al. (2016). One of the advantages of this platform is that it runs the simulation at thousands of frames per second on a single CPU core, enabling training models to perform tens of millions of simulation steps in a single day.We compare the presented approach with modern deep RL methods in four scenarios with increasing difficulty, examine generalization across environments and objectives, and evaluate the importance of various aspects of the model."}, {"heading": "4.1 SETUP", "text": "In fact, most of them are able to play by the rules that they have set themselves, and they are able to play by the rules that they have set by the rules."}, {"heading": "4.2 RESULTS", "text": "This year, it is only a matter of time before an agreement is reached."}, {"heading": "5 DISCUSSION", "text": "We presented an approach to sensorimotor control in immersive environments. Our approach is simple and shows that supervised learning techniques can be adapted to learning to operate in complex and dynamic three-dimensional environments using raw sensory input and intrinsic measurements; the model trains on raw experiences by interacting with the environment without external monitoring; natural monitoring is provided by the spatial structure of sensory and measurement streams; our experiments have shown that this simple approach outperforms sophisticated formulations for deepening learning tasks in immersive environments; experiments have also shown that the use of multivariate measurements offers a significant advantage over traditional scalar rewards; and that the trained model can effectively adapt to new goals; the work presented can be expanded in several ways that are important to broaden the range of behaviors that can be learned; and first, the test framework presented is purely reactive, with no explicit effect on the current model."}, {"heading": "A SCENARIOS", "text": "In fact, it is as if most of them will be able to abide by the rules that they have imposed on themselves. (...) It is as if they were able to break the rules of the market. (...) It is as if they were able to abide by the rules of the market. (...) It is as if they were not abiding by the rules of the market. (...) It is as if they were not abiding by the rules of the market. (...) It is as if they were not abiding by the rules of the market. (...) It is as if they were not abiding by the rules of the market. (...) It is as if they were not abiding by the rules of the market. (...) It is as if they were not abiding by the rules of the market. (...) It is as if they were not abiding by the rules of the market. (...)"}, {"heading": "C BASELINES", "text": "We compared our approach with three previous methods: DQN (Mnih et al., 2015), DSR (Kulkarni et al., 2016b) and A3C (Mnih et al., 2016). We used the implementations of the authors of DQN (https: / / github.com / kuz / DeepMind-Atari-Deep-Q-Learner) and DSR (https: / / github.com / Ardavans / DSR) and an independent implementation of A3C (https: / / github.com / kuz / DeepMind-Atari-Deep-Q-Learner). For scenarios D1 and D2, we used the change in health as a reward. For D3 and D4, we used a linear combination of changes in the three measurements with the same coefficients as for the presented approach: (0.5, 0.5, 1). For DQN, we tested three learning rates \u2212 the standard values (0.00025 and two alternatives between 000.0005 and 0.0000.0002 and 0.00.00.00.02 respectively)."}], "references": [{"title": "Motor development", "author": ["Karen E. Adolph", "Sarah E. Berger"], "venue": "In Handbook of Child Psychology,", "citeRegEx": "Adolph and Berger.,? \\Q2006\\E", "shortCiteRegEx": "Adolph and Berger.", "year": 2006}, {"title": "Recent advances in hierarchical reinforcement learning", "author": ["Andrew G. Barto", "Sridhar Mahadevan"], "venue": "Discrete Event Dynamic Systems,", "citeRegEx": "Barto and Mahadevan.,? \\Q2003\\E", "shortCiteRegEx": "Barto and Mahadevan.", "year": 2003}, {"title": "A counterexample to temporal differences learning", "author": ["Dimitri P. Bertsekas"], "venue": "Neural Computation,", "citeRegEx": "Bertsekas.,? \\Q1995\\E", "shortCiteRegEx": "Bertsekas.", "year": 1995}, {"title": "Pathologies of temporal difference methods in approximate dynamic programming", "author": ["Dimitri P. Bertsekas"], "venue": "In IEEE Conference on Decision and Control,", "citeRegEx": "Bertsekas.,? \\Q2010\\E", "shortCiteRegEx": "Bertsekas.", "year": 2010}, {"title": "Model-free episodic control", "author": ["Charles Blundell", "Benigno Uria", "Alexander Pritzel", "Yazhe Li", "Avraham Ruderman", "Joel Z. Leibo", "Jack Rae", "Daan Wierstra", "Demis Hassabis"], "venue": null, "citeRegEx": "Blundell et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Blundell et al\\.", "year": 2016}, {"title": "Learning parameterized skills", "author": ["Bruno Castro da Silva", "George Konidaris", "Andrew G. Barto"], "venue": "In ICML,", "citeRegEx": "Silva et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Silva et al\\.", "year": 2012}, {"title": "Multi-task policy search for robotics", "author": ["Marc Peter Deisenroth", "Peter Englert", "Jan Peters", "Dieter Fox"], "venue": "In ICRA,", "citeRegEx": "Deisenroth et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Deisenroth et al\\.", "year": 2014}, {"title": "Learning rates for Q-learning", "author": ["Eyal Even-Dar", "Yishay Mansour"], "venue": null, "citeRegEx": "Even.Dar and Mansour.,? \\Q2003\\E", "shortCiteRegEx": "Even.Dar and Mansour.", "year": 2003}, {"title": "Unsupervised learning for physical interaction through video prediction", "author": ["Chelsea Finn", "Ian J. Goodfellow", "Sergey Levine"], "venue": "In NIPS,", "citeRegEx": "Finn et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Finn et al\\.", "year": 2016}, {"title": "Multi-criteria reinforcement learning", "author": ["Zolt\u00e1n G\u00e1bor", "Zsolt Kalm\u00e1r", "Csaba Szepesv\u00e1ri"], "venue": "In ICML,", "citeRegEx": "G\u00e1bor et al\\.,? \\Q1998\\E", "shortCiteRegEx": "G\u00e1bor et al\\.", "year": 1998}, {"title": "Delving deep into rectifiers: Surpassing humanlevel performance on ImageNet classification", "author": ["Kaiming He", "Xiangyu Zhang", "Shaoqing Ren", "Jian Sun"], "venue": "In ICCV,", "citeRegEx": "He et al\\.,? \\Q2015\\E", "shortCiteRegEx": "He et al\\.", "year": 2015}, {"title": "Forward models: Supervised learning with a distal teacher", "author": ["Michael I. Jordan", "David E. Rumelhart"], "venue": "Cognitive Science,", "citeRegEx": "Jordan and Rumelhart.,? \\Q1992\\E", "shortCiteRegEx": "Jordan and Rumelhart.", "year": 1992}, {"title": "ViZDoom: A Doom-based AI research platform for visual reinforcement learning", "author": ["Micha\u0142 Kempka", "Marek Wydmuch", "Grzegorz Runc", "Jakub Toczek", "Wojciech Ja\u015bkowski"], "venue": "In IEEE Conference on Computational Intelligence and Games,", "citeRegEx": "Kempka et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Kempka et al\\.", "year": 2016}, {"title": "Adam: A method for stochastic optimization", "author": ["Diederik P. Kingma", "Jimmy Ba"], "venue": "In ICLR,", "citeRegEx": "Kingma and Ba.,? \\Q2015\\E", "shortCiteRegEx": "Kingma and Ba.", "year": 2015}, {"title": "Reinforcement learning to adjust parametrized motor primitives to new situations", "author": ["Jens Kober", "Andreas Wilhelm", "Erhan Oztop", "Jan Peters"], "venue": "Autonomous Robots,", "citeRegEx": "Kober et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Kober et al\\.", "year": 2012}, {"title": "Reinforcement learning in robotics: A survey", "author": ["Jens Kober", "J. Andrew Bagnell", "Jan Peters"], "venue": "IJRR, 32(11),", "citeRegEx": "Kober et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Kober et al\\.", "year": 2013}, {"title": "Transfer in reinforcement learning via shared features", "author": ["George Konidaris", "Ilya Scheidwasser", "Andrew G. Barto"], "venue": null, "citeRegEx": "Konidaris et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Konidaris et al\\.", "year": 2012}, {"title": "Hierarchical deep reinforcement learning: Integrating temporal abstraction and intrinsic motivation", "author": ["Tejas D. Kulkarni", "Karthik Narasimhan", "Ardavan Saeedi", "Joshua B. Tenenbaum"], "venue": "In NIPS,", "citeRegEx": "Kulkarni et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Kulkarni et al\\.", "year": 2016}, {"title": "Deep successor reinforcement learning", "author": ["Tejas D. Kulkarni", "Ardavan Saeedi", "Simanta Gautam", "Samuel J. Gershman"], "venue": null, "citeRegEx": "Kulkarni et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Kulkarni et al\\.", "year": 2016}, {"title": "Masters of Doom: How Two Guys Created an Empire and Transformed Pop Culture", "author": ["David Kushner"], "venue": "Random House,", "citeRegEx": "Kushner.,? \\Q2003\\E", "shortCiteRegEx": "Kushner.", "year": 2003}, {"title": "Building machines that learn and think like people", "author": ["Brenden M. Lake", "Tomer D. Ullman", "Joshua B. Tenenbaum", "Samuel J. Gershman"], "venue": null, "citeRegEx": "Lake et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Lake et al\\.", "year": 2016}, {"title": "Playing FPS games with deep reinforcement learning", "author": ["Guillaume Lample", "Devendra Singh Chaplot"], "venue": null, "citeRegEx": "Lample and Chaplot.,? \\Q2016\\E", "shortCiteRegEx": "Lample and Chaplot.", "year": 2016}, {"title": "Off-road obstacle avoidance through end-toend learning", "author": ["Yann LeCun", "Urs Muller", "Jan Ben", "Eric Cosatto", "Beat Flepp"], "venue": "In NIPS,", "citeRegEx": "LeCun et al\\.,? \\Q2005\\E", "shortCiteRegEx": "LeCun et al\\.", "year": 2005}, {"title": "Guided policy search", "author": ["Sergey Levine", "Vladlen Koltun"], "venue": "In ICML,", "citeRegEx": "Levine and Koltun.,? \\Q2013\\E", "shortCiteRegEx": "Levine and Koltun.", "year": 2013}, {"title": "Learning hand-eye coordination for robotic grasping with deep learning and large-scale data collection", "author": ["Sergey Levine", "Peter Pastor", "Alex Krizhevsky", "Deirdre Quillen"], "venue": "ISER,", "citeRegEx": "Levine et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Levine et al\\.", "year": 2016}, {"title": "Continuous control with deep reinforcement learning", "author": ["Timothy P. Lillicrap", "Jonathan J. Hunt", "Alexander Pritzel", "Nicolas Heess", "Tom Erez", "Yuval Tassa", "David Silver", "Daan Wierstra"], "venue": "In ICLR,", "citeRegEx": "Lillicrap et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Lillicrap et al\\.", "year": 2016}, {"title": "Predictive representations of state", "author": ["Michael L. Littman", "Richard S. Sutton", "Satinder P. Singh"], "venue": "In NIPS,", "citeRegEx": "Littman et al\\.,? \\Q2001\\E", "shortCiteRegEx": "Littman et al\\.", "year": 2001}, {"title": "Deep multi-scale video prediction beyond mean square error", "author": ["Micha\u00ebl Mathieu", "Camille Couprie", "Yann LeCun"], "venue": "In ICLR,", "citeRegEx": "Mathieu et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Mathieu et al\\.", "year": 2016}, {"title": "Human-level control through deep reinforcement learning", "author": ["Volodymyr Mnih", "Koray Kavukcuoglu", "David Silver", "Andrei A. Rusu", "Joel Veness", "Marc G. Bellemare", "Alex Graves", "Martin Riedmiller", "Andreas K. Fidjeland", "Georg Ostrovski", "Stig Petersen", "Charles Beattie", "Amir Sadik"], "venue": null, "citeRegEx": "Mnih et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Mnih et al\\.", "year": 2015}, {"title": "Asynchronous methods for deep reinforcement learning", "author": ["Volodymyr Mnih", "Adri\u00e0 Puigdom\u00e8nech Badia", "Mehdi Mirza", "Alex Graves", "Timothy P. Lillicrap", "Tim Harley", "David Silver", "Koray Kavukcuoglu"], "venue": null, "citeRegEx": "Mnih et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Mnih et al\\.", "year": 2016}, {"title": "Machine Learning: A Probabilistic Perspective", "author": ["Kevin P. Murphy"], "venue": null, "citeRegEx": "Murphy.,? \\Q2012\\E", "shortCiteRegEx": "Murphy.", "year": 2012}, {"title": "Action-conditional video prediction using deep networks in Atari games", "author": ["Junhyuk Oh", "Xiaoxiao Guo", "Honglak Lee", "Richard L. Lewis", "Satinder P. Singh"], "venue": "In NIPS,", "citeRegEx": "Oh et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Oh et al\\.", "year": 2015}, {"title": "Control of memory, active perception, and action in Minecraft", "author": ["Junhyuk Oh", "Valliappa Chockalingam", "Satinder P. Singh", "Honglak Lee"], "venue": null, "citeRegEx": "Oh et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Oh et al\\.", "year": 2016}, {"title": "A survey of multi-objective sequential decision-making", "author": ["Diederik M. Roijers", "Peter Vamplew", "Shimon Whiteson", "Richard Dazeley"], "venue": null, "citeRegEx": "Roijers et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Roijers et al\\.", "year": 2013}, {"title": "Learning monocular reactive UAV control in cluttered natural environments", "author": ["St\u00e9phane Ross", "Narek Melik-Barkhudarov", "Kumar Shaurya Shankar", "Andreas Wendel", "Debadeepta Dey", "J. Andrew Bagnell", "Martial Hebert"], "venue": "In ICRA,", "citeRegEx": "Ross et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Ross et al\\.", "year": 2013}, {"title": "Universal value function approximators", "author": ["Tom Schaul", "Daniel Horgan", "Karol Gregor", "David Silver"], "venue": "In ICML,", "citeRegEx": "Schaul et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Schaul et al\\.", "year": 2015}, {"title": "Mastering the game of Go with deep neural networks and tree", "author": ["David Silver", "Aja Huang", "Chris J. Maddison", "Arthur Guez", "Laurent Sifre", "George van den Driessche", "Julian Schrittwieser", "Ioannis Antonoglou", "Veda Panneershelvam", "Marc Lanctot", "Sander Dieleman", "Dominik Grewe"], "venue": "search. Nature,", "citeRegEx": "Silver et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Silver et al\\.", "year": 2016}, {"title": "Reinforcement learning with replacing eligibility traces", "author": ["Satinder P. Singh", "Richard S. Sutton"], "venue": "Machine Learning,", "citeRegEx": "Singh and Sutton.,? \\Q1996\\E", "shortCiteRegEx": "Singh and Sutton.", "year": 1996}, {"title": "Learning predictive state representations", "author": ["Satinder P. Singh", "Michael L. Littman", "Nicholas K. Jong", "David Pardoe", "Peter Stone"], "venue": "In ICML,", "citeRegEx": "Singh et al\\.,? \\Q2003\\E", "shortCiteRegEx": "Singh et al\\.", "year": 2003}, {"title": "Learning to predict by the methods of temporal differences", "author": ["Richard S. Sutton"], "venue": "Machine Learning,", "citeRegEx": "Sutton.,? \\Q1988\\E", "shortCiteRegEx": "Sutton.", "year": 1988}, {"title": "Generalization in reinforcement learning: Successful examples using sparse coarse coding", "author": ["Richard S. Sutton"], "venue": "In NIPS,", "citeRegEx": "Sutton.,? \\Q1995\\E", "shortCiteRegEx": "Sutton.", "year": 1995}, {"title": "Reinforcement Learning: An Introduction", "author": ["Richard S. Sutton", "Andrew G. Barto"], "venue": "MIT Press,", "citeRegEx": "Sutton and Barto.,? \\Q2017\\E", "shortCiteRegEx": "Sutton and Barto.", "year": 2017}, {"title": "Horde: a scalable real-time architecture for learning knowledge from unsupervised sensorimotor interaction", "author": ["Richard S. Sutton", "Joseph Modayil", "Michael Delp", "Thomas Degris", "Patrick M. Pilarski", "Adam White", "Doina Precup"], "venue": "In AAMAS,", "citeRegEx": "Sutton et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Sutton et al\\.", "year": 2011}, {"title": "A unified analysis of value-function-based reinforcement learning algorithms", "author": ["Csaba Szepesv\u00e1ri", "Michael L. Littman"], "venue": "Neural Computation,", "citeRegEx": "Szepesv\u00e1ri and Littman.,? \\Q1999\\E", "shortCiteRegEx": "Szepesv\u00e1ri and Littman.", "year": 1999}, {"title": "TD-gammon, a self-teaching backgammon program, achieves master-level play", "author": ["Gerald Tesauro"], "venue": "Neural Computation,", "citeRegEx": "Tesauro.,? \\Q1994\\E", "shortCiteRegEx": "Tesauro.", "year": 1994}, {"title": "On the convergence of optimistic policy iteration", "author": ["John N. Tsitsiklis"], "venue": null, "citeRegEx": "Tsitsiklis.,? \\Q2002\\E", "shortCiteRegEx": "Tsitsiklis.", "year": 2002}, {"title": "WaveNet: A generative model for raw audio", "author": ["A\u00e4ron van den Oord", "Sander Dieleman", "Heiga Zen", "Karen Simonyan", "Oriol Vinyals", "Alex Graves", "Nal Kalchbrenner", "Andrew W. Senior", "Koray Kavukcuoglu"], "venue": null, "citeRegEx": "Oord et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Oord et al\\.", "year": 2016}, {"title": "Dueling network architectures for deep reinforcement learning", "author": ["Ziyu Wang", "Tom Schaul", "Matteo Hessel", "Hado van Hasselt", "Marc Lanctot", "Nando de Freitas"], "venue": null, "citeRegEx": "Wang et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Wang et al\\.", "year": 2016}, {"title": "The shallow network follows the architecture", "author": [], "venue": null, "citeRegEx": "B3.,? \\Q2015\\E", "shortCiteRegEx": "B3.", "year": 2015}], "referenceMentions": [{"referenceID": 30, "context": "In this view, supervised learning is concerned with learning input-output mappings, unsupervised learning aims to find hidden structure in data, and reinforcement learning deals with goal-directed behavior (Murphy, 2012).", "startOffset": 206, "endOffset": 220}, {"referenceID": 28, "context": "While reinforcement learning (RL) has achieved significant progress (Mnih et al., 2015), key challenges remain.", "startOffset": 68, "endOffset": 87}, {"referenceID": 20, "context": "Another is the acquisition of general skills that can be flexibly deployed to accomplish a multitude of dynamically specified goals (Lake et al., 2016).", "startOffset": 132, "endOffset": 151}, {"referenceID": 44, "context": "While a sparse scalar reward may be the only feedback available in a board game (Tesauro, 1994; Silver et al., 2016), a multidimensional stream of sensations is a more appropriate model for an organism that is learning to function in an immersive environment (Adolph & Berger, 2006).", "startOffset": 80, "endOffset": 116}, {"referenceID": 36, "context": "While a sparse scalar reward may be the only feedback available in a board game (Tesauro, 1994; Silver et al., 2016), a multidimensional stream of sensations is a more appropriate model for an organism that is learning to function in an immersive environment (Adolph & Berger, 2006).", "startOffset": 80, "endOffset": 116}, {"referenceID": 19, "context": "We use the classical first-person game Doom, which introduced immersive three-dimensional games to popular culture (Kushner, 2003).", "startOffset": 115, "endOffset": 130}, {"referenceID": 15, "context": "TD and policy gradient methods have since come to dominate the study of sensorimotor learning (Kober et al., 2013; Mnih et al., 2015; Sutton & Barto, 2017).", "startOffset": 94, "endOffset": 155}, {"referenceID": 28, "context": "TD and policy gradient methods have since come to dominate the study of sensorimotor learning (Kober et al., 2013; Mnih et al., 2015; Sutton & Barto, 2017).", "startOffset": 94, "endOffset": 155}, {"referenceID": 22, "context": "While the use of SL is natural in imitation learning (LeCun et al., 2005; Ross et al., 2013) or in conjunction with model-based RL (Levine & Koltun, 2013), the formulation of sensorimotor learning from raw experience as supervised learning is rare (Levine et al.", "startOffset": 53, "endOffset": 92}, {"referenceID": 34, "context": "While the use of SL is natural in imitation learning (LeCun et al., 2005; Ross et al., 2013) or in conjunction with model-based RL (Levine & Koltun, 2013), the formulation of sensorimotor learning from raw experience as supervised learning is rare (Levine et al.", "startOffset": 53, "endOffset": 92}, {"referenceID": 24, "context": ", 2013) or in conjunction with model-based RL (Levine & Koltun, 2013), the formulation of sensorimotor learning from raw experience as supervised learning is rare (Levine et al., 2016).", "startOffset": 163, "endOffset": 184}, {"referenceID": 2, "context": "The convergence of such methods was analyzed early on and they were seen as theoretically advantageous, particularly when function approximators are used (Bertsekas, 1995; Sutton, 1995; Singh & Sutton, 1996).", "startOffset": 154, "endOffset": 207}, {"referenceID": 40, "context": "The convergence of such methods was analyzed early on and they were seen as theoretically advantageous, particularly when function approximators are used (Bertsekas, 1995; Sutton, 1995; Singh & Sutton, 1996).", "startOffset": 154, "endOffset": 207}, {"referenceID": 40, "context": "The choice of TD learning over Monte Carlo methods was argued on practical grounds, based on empirical performance on canonical examples (Sutton, 1995).", "startOffset": 137, "endOffset": 151}, {"referenceID": 45, "context": "While the understanding of the convergence of both types of methods has since improved (Szepesv\u00e1ri & Littman, 1999; Tsitsiklis, 2002; Even-Dar & Mansour, 2003), the argument for TD versus Monte Carlo is to this day empirical (Sutton & Barto, 2017).", "startOffset": 87, "endOffset": 159}, {"referenceID": 3, "context": "Sharp negative examples exist (Bertsekas, 2010).", "startOffset": 30, "endOffset": 47}, {"referenceID": 30, "context": "Sutton (1988) analyzed temporal-difference (TD) learning and argued that it is preferable to SL for prediction problems in which the correctness of the prediction is revealed many steps after the prediction is made.", "startOffset": 0, "endOffset": 14}, {"referenceID": 9, "context": "Vector-valued feedback has been considered in the context of multi-objective decision-making (G\u00e1bor et al., 1998; Roijers et al., 2013).", "startOffset": 93, "endOffset": 135}, {"referenceID": 33, "context": "Vector-valued feedback has been considered in the context of multi-objective decision-making (G\u00e1bor et al., 1998; Roijers et al., 2013).", "startOffset": 93, "endOffset": 135}, {"referenceID": 14, "context": "Parameterized goals have been studied in the context of continuous motor skills such as throwing darts at a target (da Silva et al., 2012; Kober et al., 2012; Deisenroth et al., 2014).", "startOffset": 115, "endOffset": 183}, {"referenceID": 6, "context": "Parameterized goals have been studied in the context of continuous motor skills such as throwing darts at a target (da Silva et al., 2012; Kober et al., 2012; Deisenroth et al., 2014).", "startOffset": 115, "endOffset": 183}, {"referenceID": 27, "context": "Prediction of full sensory input in realistic three-dimensional environments remains an open challenge, although significant progress is being made (Mathieu et al., 2016; Finn et al., 2016; Kalchbrenner et al., 2016).", "startOffset": 148, "endOffset": 216}, {"referenceID": 8, "context": "Prediction of full sensory input in realistic three-dimensional environments remains an open challenge, although significant progress is being made (Mathieu et al., 2016; Finn et al., 2016; Kalchbrenner et al., 2016).", "startOffset": 148, "endOffset": 216}, {"referenceID": 5, "context": "Vector-valued feedback has been considered in the context of multi-objective decision-making (G\u00e1bor et al., 1998; Roijers et al., 2013). Transfer across related tasks has been analyzed by Konidaris et al. (2012). Parameterized goals have been studied in the context of continuous motor skills such as throwing darts at a target (da Silva et al.", "startOffset": 94, "endOffset": 212}, {"referenceID": 4, "context": "Parameterized goals have been studied in the context of continuous motor skills such as throwing darts at a target (da Silva et al., 2012; Kober et al., 2012; Deisenroth et al., 2014). A general framework for sharing value function approximators across both states and goals has been described by Schaul et al. (2015). Our work is related, but develops a new model and shows that it achieves state-of-the-art performance in complex and dynamic three-dimensional environments.", "startOffset": 119, "endOffset": 318}, {"referenceID": 4, "context": "Parameterized goals have been studied in the context of continuous motor skills such as throwing darts at a target (da Silva et al., 2012; Kober et al., 2012; Deisenroth et al., 2014). A general framework for sharing value function approximators across both states and goals has been described by Schaul et al. (2015). Our work is related, but develops a new model and shows that it achieves state-of-the-art performance in complex and dynamic three-dimensional environments. Learning to act in simulated environments has been the focus of significant attention following the successful application of deep RL to Atari games by Mnih et al. (2015). A number of recent efforts applied related ideas to three-dimensional environments.", "startOffset": 119, "endOffset": 647}, {"referenceID": 4, "context": "Parameterized goals have been studied in the context of continuous motor skills such as throwing darts at a target (da Silva et al., 2012; Kober et al., 2012; Deisenroth et al., 2014). A general framework for sharing value function approximators across both states and goals has been described by Schaul et al. (2015). Our work is related, but develops a new model and shows that it achieves state-of-the-art performance in complex and dynamic three-dimensional environments. Learning to act in simulated environments has been the focus of significant attention following the successful application of deep RL to Atari games by Mnih et al. (2015). A number of recent efforts applied related ideas to three-dimensional environments. Lillicrap et al. (2016) considered continuous and high-dimensional action spaces and learned control policies in the TORCS simulator.", "startOffset": 119, "endOffset": 756}, {"referenceID": 4, "context": "Parameterized goals have been studied in the context of continuous motor skills such as throwing darts at a target (da Silva et al., 2012; Kober et al., 2012; Deisenroth et al., 2014). A general framework for sharing value function approximators across both states and goals has been described by Schaul et al. (2015). Our work is related, but develops a new model and shows that it achieves state-of-the-art performance in complex and dynamic three-dimensional environments. Learning to act in simulated environments has been the focus of significant attention following the successful application of deep RL to Atari games by Mnih et al. (2015). A number of recent efforts applied related ideas to three-dimensional environments. Lillicrap et al. (2016) considered continuous and high-dimensional action spaces and learned control policies in the TORCS simulator. Mnih et al. (2016) described asynchronous variants of deep RL methods and demonstrated navigation in a three-dimensional labyrinth.", "startOffset": 119, "endOffset": 885}, {"referenceID": 4, "context": "Parameterized goals have been studied in the context of continuous motor skills such as throwing darts at a target (da Silva et al., 2012; Kober et al., 2012; Deisenroth et al., 2014). A general framework for sharing value function approximators across both states and goals has been described by Schaul et al. (2015). Our work is related, but develops a new model and shows that it achieves state-of-the-art performance in complex and dynamic three-dimensional environments. Learning to act in simulated environments has been the focus of significant attention following the successful application of deep RL to Atari games by Mnih et al. (2015). A number of recent efforts applied related ideas to three-dimensional environments. Lillicrap et al. (2016) considered continuous and high-dimensional action spaces and learned control policies in the TORCS simulator. Mnih et al. (2016) described asynchronous variants of deep RL methods and demonstrated navigation in a three-dimensional labyrinth. Oh et al. (2016) augmented deep Q-networks with external memory and evaluated their performance on a set of tasks in Minecraft.", "startOffset": 119, "endOffset": 1015}, {"referenceID": 4, "context": "Parameterized goals have been studied in the context of continuous motor skills such as throwing darts at a target (da Silva et al., 2012; Kober et al., 2012; Deisenroth et al., 2014). A general framework for sharing value function approximators across both states and goals has been described by Schaul et al. (2015). Our work is related, but develops a new model and shows that it achieves state-of-the-art performance in complex and dynamic three-dimensional environments. Learning to act in simulated environments has been the focus of significant attention following the successful application of deep RL to Atari games by Mnih et al. (2015). A number of recent efforts applied related ideas to three-dimensional environments. Lillicrap et al. (2016) considered continuous and high-dimensional action spaces and learned control policies in the TORCS simulator. Mnih et al. (2016) described asynchronous variants of deep RL methods and demonstrated navigation in a three-dimensional labyrinth. Oh et al. (2016) augmented deep Q-networks with external memory and evaluated their performance on a set of tasks in Minecraft. In a recent technical report, Kulkarni et al. (2016b) proposed end-to-end training of successor representations and demonstrated navigation in the Doom game engine.", "startOffset": 119, "endOffset": 1180}, {"referenceID": 4, "context": "In another recent report, Blundell et al. (2016) considered a nonparametric approach to control and conducted experiments in a three-dimensional labyrinth.", "startOffset": 26, "endOffset": 49}, {"referenceID": 4, "context": "In another recent report, Blundell et al. (2016) considered a nonparametric approach to control and conducted experiments in a three-dimensional labyrinth. We compare to state-of-the-art deep RL methods in Section 4 and demonstrate that the presented approach performs favorably. Prediction of future states in dynamical systems was considered by Littman et al. (2001) and Singh et al.", "startOffset": 26, "endOffset": 369}, {"referenceID": 4, "context": "In another recent report, Blundell et al. (2016) considered a nonparametric approach to control and conducted experiments in a three-dimensional labyrinth. We compare to state-of-the-art deep RL methods in Section 4 and demonstrate that the presented approach performs favorably. Prediction of future states in dynamical systems was considered by Littman et al. (2001) and Singh et al. (2003). Predictive representations in the form of generalized value functions were advocated by Sutton et al.", "startOffset": 26, "endOffset": 393}, {"referenceID": 4, "context": "In another recent report, Blundell et al. (2016) considered a nonparametric approach to control and conducted experiments in a three-dimensional labyrinth. We compare to state-of-the-art deep RL methods in Section 4 and demonstrate that the presented approach performs favorably. Prediction of future states in dynamical systems was considered by Littman et al. (2001) and Singh et al. (2003). Predictive representations in the form of generalized value functions were advocated by Sutton et al. (2011). More recently, Oh et al.", "startOffset": 26, "endOffset": 503}, {"referenceID": 4, "context": "In another recent report, Blundell et al. (2016) considered a nonparametric approach to control and conducted experiments in a three-dimensional labyrinth. We compare to state-of-the-art deep RL methods in Section 4 and demonstrate that the presented approach performs favorably. Prediction of future states in dynamical systems was considered by Littman et al. (2001) and Singh et al. (2003). Predictive representations in the form of generalized value functions were advocated by Sutton et al. (2011). More recently, Oh et al. (2015) learned to predict future frames in Atari games.", "startOffset": 26, "endOffset": 536}, {"referenceID": 46, "context": "(2016) and van den Oord et al. (2016) for impressive recent progress).", "startOffset": 19, "endOffset": 38}, {"referenceID": 47, "context": "To this end, we build on the ideas of Wang et al. (2016) and split the prediction module into two streams: an expectation stream E(j) and an action stream A(j).", "startOffset": 38, "endOffset": 57}, {"referenceID": 12, "context": "To interface with the game engine, we use the ViZDoom platform developed by Kempka et al. (2016). One of the advantages of this platform is that it allows running the simulation at thousands of frames per second on a single CPU core, which enables training models on tens of millions of simulation steps in a single day.", "startOffset": 76, "endOffset": 97}, {"referenceID": 28, "context": "The shallow version was configured to be as close as possible to the DQN model of Mnih et al. (2015), to ensure a fair comparison.", "startOffset": 82, "endOffset": 101}, {"referenceID": 28, "context": "We have compared the presented approach to three deep RL methods: DQN (Mnih et al., 2015), A3C (Mnih et al.", "startOffset": 70, "endOffset": 89}, {"referenceID": 29, "context": ", 2015), A3C (Mnih et al., 2016), and DSR (Kulkarni et al.", "startOffset": 13, "endOffset": 32}, {"referenceID": 17, "context": "to significantly exceed the number of training steps reported in the experiments of Kulkarni et al. (2016b), but not sufficient to approach the number of steps afforded by the other approaches.", "startOffset": 84, "endOffset": 108}, {"referenceID": 28, "context": "In the more complex Navigation scenario, a significant gap opens up between DQN and A3C; this is consistent with the experiments of Mnih et al. (2016). DFP achieves the best performance in this scenario, with a 5 percentage point advantage during testing.", "startOffset": 132, "endOffset": 151}, {"referenceID": 32, "context": "Recent work has explored memory-based models (Oh et al., 2016) and integrating such ideas with the presented approach may yield substantial advances.", "startOffset": 45, "endOffset": 62}, {"referenceID": 25, "context": "Third, the presented model was developed for discrete action spaces; applying the presented ideas to continuous action spaces would be interesting (Lillicrap et al., 2016).", "startOffset": 147, "endOffset": 171}, {"referenceID": 27, "context": "Finally, predicting features learned directly from rich sensory input can blur the distinction between sensory and measurement streams (Mathieu et al., 2016; Finn et al., 2016; Kalchbrenner et al., 2016).", "startOffset": 135, "endOffset": 203}, {"referenceID": 8, "context": "Finally, predicting features learned directly from rich sensory input can blur the distinction between sensory and measurement streams (Mathieu et al., 2016; Finn et al., 2016; Kalchbrenner et al., 2016).", "startOffset": 135, "endOffset": 203}], "year": 2016, "abstractText": "We present an approach to sensorimotor control in immersive environments. Our approach utilizes a high-dimensional sensory stream and a lower-dimensional measurement stream. The cotemporal structure of these streams provides a rich supervisory signal, which enables training a sensorimotor control model by interacting with the environment. The model is trained using supervised learning techniques, but without extraneous supervision. It learns to act based on raw sensory input from a complex three-dimensional environment. The presented formulation allows changing the agent\u2019s goal at test time. We conduct extensive experiments in three-dimensional simulations based on the classical first-person game Doom. The results demonstrate that the presented approach outperforms sophisticated prior formulations, particularly on challenging tasks. The results also show that trained models successfully generalize across environments and goals. A model trained using the presented approach won the Full Deathmatch track of the Visual Doom AI Competition, which was held in previously unseen environments.", "creator": "LaTeX with hyperref package"}}}