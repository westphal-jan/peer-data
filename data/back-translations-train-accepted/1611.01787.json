{"id": "1611.01787", "review": {"conference": "iclr", "VERSION": "v1", "DATE_OF_SUBMISSION": "6-Nov-2016", "title": "Learning to superoptimize programs", "abstract": "Code super-optimization is the task of transforming any given program to a more efficient version while preserving its input-output behaviour. In some sense, it is similar to the paraphrase problem from natural language processing where the intention is to change the syntax of an utterance without changing its semantics. Code-optimization has been the subject of years of research that has resulted in the development of rule-based transformation strategies that are used by compilers. More recently, however, a class of stochastic search based methods have been shown to outperform these strategies. This approach involves repeated sampling of modifications to the program from a proposal distribution, which are accepted or rejected based on whether they preserve correctness, and the improvement they achieve. These methods, however, neither learn from past behaviour nor do they try to leverage the semantics of the program under consideration. Motivated by this observation, we present a novel learning based approach for code super-optimization. Intuitively, our method works by learning the proposal distribution using unbiased estimators of the gradient of the expected improvement. Experiments on benchmarks comprising of automatically generated as well as existing (\"Hacker's Delight\") programs show that the proposed method is able to significantly outperform state of the art approaches for code super-optimization.", "histories": [["v1", "Sun, 6 Nov 2016 14:35:38 GMT  (549kb,D)", "http://arxiv.org/abs/1611.01787v1", "Submitted to ICLR 2017"], ["v2", "Thu, 23 Feb 2017 14:59:49 GMT  (792kb,D)", "http://arxiv.org/abs/1611.01787v2", "Accepted to ICLR 2017"], ["v3", "Wed, 28 Jun 2017 15:04:46 GMT  (792kb,D)", "http://arxiv.org/abs/1611.01787v3", "Accepted to ICLR 2017"]], "COMMENTS": "Submitted to ICLR 2017", "reviews": [], "SUBJECTS": "cs.LG", "authors": ["rudy bunel", "alban desmaison", "m pawan kumar", "philip h s torr", "pushmeet kohli"], "accepted": true, "id": "1611.01787"}, "pdf": {"name": "1611.01787.pdf", "metadata": {"source": "CRF", "title": "Learning to superoptimize programs", "authors": ["Rudy Bunel", "Alban Desmaison", "M. Pawan Kumar"], "emails": ["rudy@robots.ox.ac.uk,", "alban@robots.ox.ac.uk,", "pawan@robots.ox.ac.uk,", "philip.torr@eng.ox.ac.uk", "pkohli@microsoft.com"], "sections": [{"heading": "1 Introduction", "text": "Considering the importance of computing for human society, it is not surprising that a very large part of the research has gone into studying the syntax and semantics of programs and programming languages. Code super optimization is an extremely important issue in this context. However, given a program or a snippet of the source code, super optimization is the task of transforming it into a version that has the same input-output behavior, but can be executed more efficiently on a target computing architecture. In a sense, it is the natural analogy of the problem of natural language processing, in which we want to change the syntax without changing the semantics. Decades of research have been conducted on the problem of code optimization, resulting in the development of rules-based transformation strategies that are used in compilers to allow them to carry out code optimizations. While modern compilers implement a large number of circumscription board rules and are able to achieve impressive speed-ups, they fail to provide a guarantee for further improvements."}, {"heading": "2 Related Works", "text": "The earliest approaches to search results were found in the search for a solution."}, {"heading": "3 Learning Stochastic Super-optimization", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "3.1 Stochastic search as a program optimization procedure", "text": "Stostocker (Schkufza et al., 2013) performs black box optimization of a cost function on the space of the programs, represented as a set of statements. Each statement consists of an opcode that indicates what to execute, and some operands that indicate the corresponding registers. Each given input program T defines a cost function. For a candidate called Rewrite, the goal is to optimize the following cost function: Cost (R, T) = \u03c9e \u00b7 eq (R, T) + \u03c9p \u00b7 perf (R) (1) (The term eq (R; T) measures how well the results of the rewrite correspond to the results of the reference program. This can be achieved either precisely by executing a symbolic validator or approximately by executing test cases. The term perf (R) is a measure of the efficiency of the program. In this paper, we consider runtime as a measure of this efficiency."}, {"heading": "3.2 Learning to search", "text": "We now describe our approach to improving stochastic search by learning the distribution of offers. We begin our description by defining the learning objective (Section 3.2.1), followed by parameterizing the distribution of offers (Section 3.2.2) and finally the reinforced learning framework for assessing the parameters of the distribution of offers (Section 3.2.3)."}, {"heading": "3.2.1 Objective function", "text": "Our goal is to optimize the cost function defined in Equation (1). Given a fixed computational budget of T iterations for program super optimization, we want to take steps that lead us to the lowest possible cost. Since different programs have different runtimes and therefore different associated costs, we must perform a normalization. As a normalized loss function, we use the ratio between the best possible rewrite and the cost of the original non-optimized program R0. Formally, the loss for a series of rewrites {Rt} t = 0.. T is defined as follows: r ({Rt} t = 0.. T) = (mint = 0.. T-costs (Rt, T) (R0, T))). (5) Remember that our goal is to learn a supply distribution. Since our optimization process is stochastic, we must consider the expected costs as our loss. This expected loss is a function of the parameters of our parameters of our parametric proposal distribution qm: L (mean) {t = 1} (t) (1)."}, {"heading": "3.2.2 Parameterization of the Move Proposal Distribution", "text": "The proposal distribution (3) originally used in (Schkufza et al., 2013) takes the form of a hierarchical model, first sampling the type of move based on a probability distribution, then taking further samples to determine, for example, the position in the programs concerned, the new operands or opcodes to be used, and which of these probability distributions will be sampled depending on the type of train sampled first. In this paper, we propose to learn these probability distributions in order to maximize the probability of achieving the best programs. Our chosen parameterization of q is to determine the hierarchical structure of the original work of Schkufza et al. (2013) as a detailed element distribution program for determining the respective probability variable."}, {"heading": "3.2.3 Learning the Proposal Distribution", "text": "To determine the distribution parameters of our proposal, we use the stochastic distribution calculation of our loss function (6). We obtain the derivatives of the first order in relation to our distribution parameters of the proposal using the estimator REINFORCE (Williams, 1992), also known as the probability calculator (Glynn, 1990) or the estimator of the score function (Fu, 2006). This estimator is based on a rewriting of the distribution of expectation in relation to a probability distribution calculation (Glynn, 1990). The estimator of REINFORCE reads: \"Experience values x (x) = empirical values x (x) - Proposals of distribution x (x) - Proposals of distribution x (x) - Proposals of distribution - Proposals of distribution (x) - Proposals of distribution (x) - Proposals of distribution - (x) (Proposals of distribution - (x) (Proposals of distribution - (x) (Proposals of distribution - (x) - Proposals of distribution - (x) (Proposals of distribution - (x);"}, {"heading": "4 Experiments", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "4.1 Setup", "text": "We orchestrated the implementation of the Metropolis algorithm to enable sampling from the parameterized distribution of supply and to follow the tracks through stochastic graphics. The first is based on the joy of hackers (Warren, 2002) Corpus, a collection of twenty-five bit manipulation programs used as a benchmark in program synthesis (Gulwani et al., 2011; Schkufza et al., 2013) Corpus, a collection of twenty-five bit manipulation programs used as a benchmark for program synthesis."}, {"heading": "4.2 Existing Programs", "text": "In order to have a larger body than the twenty-five programs originally included in Hacker's Delight, we generate different starting points for each optimization, which is accomplished by running Stoke with a cost function in which \u03c9p = 0 in (1) and only keeping the correct programs. Duplicate programs are filtered out, allowing us to create a larger set of data from which we can learn. Examples of these programs at different optimization levels can be found in Appendix C. We divide this extended set of Hacker's Delight data into two sets. All programs that match even tasks are assigned to the first set we use for training. Programs that match odd tasks are kept for a separate evaluation to evaluate the generalization of our learned offer distribution. The optimization process is visible in Figure 2, which shows a significant reduction in training losses and test losses in both models."}, {"heading": "4.3 Automatically Generated Programs", "text": "Although previous experiments have shown promising results for a number of interesting programs, the limited variety of programs may have made the task too easy, as demonstrated by the good performance of a blind model. Despite the data magnification, only 25 different tasks were present, with all variations of the same program task having the same optimum. To evaluate our performance on a more difficult problem, we automatically synthesize a larger set of programs. Our methods for doing this are to run Stoke repeatedly with a constant cost function for a large number of iterations, resulting in a completely random process, as each proposed program will have the same cost, resulting in a 50% chance of acceptance. We generate 600 of these programs, 300 of which we use as a training set for the optimizer to learn over 300, which we keep as a test set. The performance achieved on this more complex dataset is shown in Figure 4 and Table 2."}, {"heading": "5 Conclusion", "text": "As part of this work, we have formulated the problem of optimizing the performance of a stochastic super-optimizer as a machine learning problem. We have shown that learning how to distribute an MCMC sampler request was feasible and has led to faster and higher quality improvements. It is interesting to compare our method with the synthetic approaches that have recently emerged in the deep learning community (Graves et al., 2014), which aim to learn algorithms directly using differentiated representations of programs. We find that the stochastic search-based approach offers a significant advantage over these approaches, as the resulting program can be operated independently of the neural network used to discover them. Several improvements are possible over the methods presented. In mature areas such as Computer Vision, representations of objects of interest have been extensively studied and as a result, the information from each sample can be captured. In areas of programs, the achievement of informational tasks remains a larger part of the pre-presented structure, whereas our more informative approach to the objectives often remains a challenge."}, {"heading": "A Structure of the proposal distribution", "text": "The scanning process of a move is a hierarchy of scanning steps. It is easiest to represent as a generative model for the program transformations. Depending on the type of move being scanned, different scanning steps must be performed. For a given move, all probabilities are scanned independently, so the probability of proposing the move is the product of the probability of selecting each of the scanning steps. The generative model is defined in Figure 5 and is parameterized by the parameters of each specific probability distribution from which it scans. In the standard Stoke version, uniform probabilities are used across all these elementary distributions."}, {"heading": "B Hacker\u2019s Delight Tasks", "text": "The 25 tasks of Hacker Joy Warren (2002) data sets are the following: 1. Turn off the right-hand bit 2. Test whether an unsigned integer of the form 2 (n \u2212 1) is 3. Isolate the far-right bit 4. Form a mask that identifies the far-right bit and the zeros behind it 5. Test whether the number of leading zeros of two words is the same bit 6. Test whether the number of leading zeros of a word is strictly smaller than that of another work 12. Test whether the number of leading zeros of a word is smaller than that of another work 13. Character function 14. Test whether the number of leading zeros of two words is the same 11. Test whether the number of leading zeros of a word is strictly smaller than that of another work 12. Test whether the number of leading zeros of a word is smaller than that of another work 13. Test function 14. Test whether the average number of two integers of a word is equal to 16."}, {"heading": "C Examples of Hacker\u2019s delight optimisation", "text": "The first task of Hacker's Delight-Corpus is to turn off the most right-hand register. When compiling the code in Listing 6a, llvm generates the code shown in Listing 6b. A typical example of an equivalent version of the same program obtained through data augmentation can be seen in Listing 6c. Listing 6d contains the optimal version of this program. Note that such an optimization is already possible with the Schkufza et al. (2013) collision system."}], "references": [{"title": "Learning to learn by gradient descent by gradient descent", "author": ["Marcin Andrychowicz", "Misha Denil", "Sergio Gomez", "Matthew W Hoffman", "David Pfau", "Tom Schaul", "Nando de Freitas"], "venue": null, "citeRegEx": "Andrychowicz et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Andrychowicz et al\\.", "year": 2016}, {"title": "Adaptive neural compilation", "author": ["Rudy Bunel", "Alban Desmaison", "Pushmeet Kohli", "Philip HS Torr", "M Pawan Kumar"], "venue": "In NIPS", "citeRegEx": "Bunel et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Bunel et al\\.", "year": 2016}, {"title": "Torch7: A matlab-like environment for machine learning", "author": ["Ronan Collobert", "Koray Kavukcuoglu", "Cl\u00e9ment Farabet"], "venue": "In NIPS,", "citeRegEx": "Collobert et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Collobert et al\\.", "year": 2011}, {"title": "Hc-search: A learning framework for search-based structured prediction", "author": ["Janardhan Rao Doppa", "Alan Fern", "Prasad Tadepalli"], "venue": "JAIR,", "citeRegEx": "Doppa et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Doppa et al\\.", "year": 2014}, {"title": "Gradient estimation", "author": ["Michael C. Fu"], "venue": "Handbooks in Operations Research and Management Science", "citeRegEx": "Fu.,? \\Q2006\\E", "shortCiteRegEx": "Fu.", "year": 2006}, {"title": "Likelihood ratio gradient estimation for stochastic systems", "author": ["Peter W Glynn"], "venue": "Communications of the ACM,", "citeRegEx": "Glynn.,? \\Q1990\\E", "shortCiteRegEx": "Glynn.", "year": 1990}, {"title": "Eliminating branches using a superoptimizer and the GNU C compiler", "author": ["Torbj\u00f6rn Granlund", "Richard Kenner"], "venue": "ACM SIGPLAN Notices,", "citeRegEx": "Granlund and Kenner.,? \\Q1992\\E", "shortCiteRegEx": "Granlund and Kenner.", "year": 1992}, {"title": "Synthesis of loop-free programs", "author": ["Sumit Gulwani", "Susmit Jha", "Ashish Tiwari", "Ramarathnam Venkatesan"], "venue": "In PLDI,", "citeRegEx": "Gulwani et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Gulwani et al\\.", "year": 2011}, {"title": "The informed sampler: A discriminative approach to bayesian inference in generative computer vision models", "author": ["Varun Jampani", "Sebastian Nowozin", "Matthew Loper", "Peter V Gehler"], "venue": "Computer Vision and Image Understanding,", "citeRegEx": "Jampani et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Jampani et al\\.", "year": 2015}, {"title": "Oracle-guided component-based program synthesis", "author": ["Susmit Jha", "Sumit Gulwani", "Sanjit A Seshia", "Ashish Tiwari"], "venue": "In International Conference on Software Engineering,", "citeRegEx": "Jha et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Jha et al\\.", "year": 2010}, {"title": "Adam: A method for stochastic optimization", "author": ["Diederik Kingma", "Jimmy Ba"], "venue": "In ICLR,", "citeRegEx": "Kingma and Ba.,? \\Q2015\\E", "shortCiteRegEx": "Kingma and Ba.", "year": 2015}, {"title": "Picture: A probabilistic programming language for scene perception", "author": ["Tejas D Kulkarni", "Pushmeet Kohli", "Joshua B Tenenbaum", "Vikash Mansinghka"], "venue": "In CVPR,", "citeRegEx": "Kulkarni et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Kulkarni et al\\.", "year": 2015}, {"title": "Learning to optimize", "author": ["Ke Li", "Jitendra Malik"], "venue": null, "citeRegEx": "Li and Malik.,? \\Q2016\\E", "shortCiteRegEx": "Li and Malik.", "year": 2016}, {"title": "Superoptimizer: A look at the smallest program", "author": ["Henry Massalin"], "venue": "In ACM SIGPLAN Notices,", "citeRegEx": "Massalin.,? \\Q1987\\E", "shortCiteRegEx": "Massalin.", "year": 1987}, {"title": "Equation of state calculations by fast computing machines", "author": ["Nicholas Metropolis", "Arianna W Rosenbluth", "Marshall N Rosenbluth", "Augusta H Teller", "Edward Teller"], "venue": "The journal of chemical physics,", "citeRegEx": "Metropolis et al\\.,? \\Q1953\\E", "shortCiteRegEx": "Metropolis et al\\.", "year": 1953}, {"title": "Inference networks for sequential Monte Carlo in graphical models", "author": ["Brookes Paige", "Frank Wood"], "venue": "In ICML,", "citeRegEx": "Paige and Wood.,? \\Q2016\\E", "shortCiteRegEx": "Paige and Wood.", "year": 2016}, {"title": "Gradient estimation using stochastic computation graphs", "author": ["John Schulman", "Nicolas Heess", "Theophane Weber", "Pieter Abbeel"], "venue": "In NIPS,", "citeRegEx": "Schulman et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Schulman et al\\.", "year": 2015}, {"title": "Simple statistical gradient-following algorithms for connectionist reinforcement learning", "author": ["Ronald J Williams"], "venue": "Machine learning,", "citeRegEx": "Williams.,? \\Q1992\\E", "shortCiteRegEx": "Williams.", "year": 1992}, {"title": "Learning to discover efficient mathematical identities", "author": ["Wojciech Zaremba", "Karol Kurach", "Rob Fergus"], "venue": "In NIPS", "citeRegEx": "Zaremba et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Zaremba et al\\.", "year": 2014}, {"title": "Integrating bottom-up/top-down for object recognition by data driven markov chain monte carlo", "author": ["Song-Chun Zhu", "Rong Zhang", "Zhuowen Tu"], "venue": "In CVPR,", "citeRegEx": "Zhu et al\\.,? \\Q2000\\E", "shortCiteRegEx": "Zhu et al\\.", "year": 2000}, {"title": "Delight Tasks The 25 tasks of the Hacker\u2019s delight Warren (2002) datasets", "author": ["B Hacker\u2019s"], "venue": null, "citeRegEx": "Hacker.s,? \\Q2002\\E", "shortCiteRegEx": "Hacker.s", "year": 2002}], "referenceMentions": [{"referenceID": 17, "context": "Using training data, which consists of a set of input programs, the parameters are learnt via the REINFORCE algorithm (Williams, 1992).", "startOffset": 118, "endOffset": 134}, {"referenceID": 13, "context": "By sequentially enumerating all programs in increasing length orders (Granlund & Kenner, 1992; Massalin, 1987), the shortest program meeting the specification is guaranteed to be found.", "startOffset": 69, "endOffset": 110}, {"referenceID": 13, "context": "The longest reported synthesized program was 12 instructions long, on a restricted instruction set (Massalin, 1987).", "startOffset": 99, "endOffset": 115}, {"referenceID": 18, "context": "Neural Computing Similar work was done in the restricted case of finding efficient implementation of computation of value of degree k polynomials (Zaremba et al., 2014).", "startOffset": 146, "endOffset": 168}, {"referenceID": 1, "context": "Bunel et al. (2016) attempted this by simulating program execution using Recurrent Neural Networks.", "startOffset": 0, "endOffset": 20}, {"referenceID": 8, "context": "Similar approaches were successfully employed in computer vision problems where data driven proposals allowed to make inference feasible (Jampani et al., 2015; Kulkarni et al., 2015; Zhu et al., 2000).", "startOffset": 137, "endOffset": 200}, {"referenceID": 11, "context": "Similar approaches were successfully employed in computer vision problems where data driven proposals allowed to make inference feasible (Jampani et al., 2015; Kulkarni et al., 2015; Zhu et al., 2000).", "startOffset": 137, "endOffset": 200}, {"referenceID": 19, "context": "Similar approaches were successfully employed in computer vision problems where data driven proposals allowed to make inference feasible (Jampani et al., 2015; Kulkarni et al., 2015; Zhu et al., 2000).", "startOffset": 137, "endOffset": 200}, {"referenceID": 2, "context": "Doppa et al. (2014) proposed imitation learning based methods to deal with structured output spaces, in a \u201cLearning to search\u201d framework.", "startOffset": 0, "endOffset": 20}, {"referenceID": 2, "context": "Doppa et al. (2014) proposed imitation learning based methods to deal with structured output spaces, in a \u201cLearning to search\u201d framework. While this is similar in spirit to stochastic search, our setting differs in the crucial aspect of having a valid cost function instead of searching for one. More relevant is the recent literature on learning to optimize. Li & Malik (2016) and Andrychowicz et al.", "startOffset": 0, "endOffset": 378}, {"referenceID": 0, "context": "Li & Malik (2016) and Andrychowicz et al. (2016) learn how to improve on first-order gradient descent algorithms, making use of neural networks.", "startOffset": 22, "endOffset": 49}, {"referenceID": 0, "context": "Li & Malik (2016) and Andrychowicz et al. (2016) learn how to improve on first-order gradient descent algorithms, making use of neural networks. Our work is similar, as we aim to improve the optimization process. However, as opposed to the gradient descent that they learn on a continuous unconstrained space, our initial algorithm is an MCMC sampler on a discrete domain. Similarly, training a proposal distribution parameterized by a Neural Network was also proposed by Paige & Wood (2016) to accelerate inference in graphical models.", "startOffset": 22, "endOffset": 492}, {"referenceID": 14, "context": "To find the optimum of this cost function, Stoke runs an MCMC sampler using the Metropolis (Metropolis et al., 1953) algorithm.", "startOffset": 91, "endOffset": 116}, {"referenceID": 17, "context": "We obtain the first order derivatives with regards to our proposal distribution parameters using the REINFORCE (Williams, 1992) estimator, also known as the likelihood ratio estimator (Glynn, 1990) or the score function estimator (Fu, 2006).", "startOffset": 111, "endOffset": 127}, {"referenceID": 5, "context": "We obtain the first order derivatives with regards to our proposal distribution parameters using the REINFORCE (Williams, 1992) estimator, also known as the likelihood ratio estimator (Glynn, 1990) or the score function estimator (Fu, 2006).", "startOffset": 184, "endOffset": 197}, {"referenceID": 4, "context": "We obtain the first order derivatives with regards to our proposal distribution parameters using the REINFORCE (Williams, 1992) estimator, also known as the likelihood ratio estimator (Glynn, 1990) or the score function estimator (Fu, 2006).", "startOffset": 230, "endOffset": 240}, {"referenceID": 16, "context": "A helpful way to derive the gradients is to consider the execution traces of the search procedure under the formalism of stochastic computation graphs (Schulman et al., 2015).", "startOffset": 151, "endOffset": 174}, {"referenceID": 2, "context": "Using those traces, we can compute the estimator of our gradients, implemented using the Torch framework (Collobert et al., 2011).", "startOffset": 105, "endOffset": 129}, {"referenceID": 7, "context": "The first is based on the Hacker\u2019s delight (Warren, 2002) corpus, a collection of twenty five bitmanipulation programs, used as benchmark in program synthesis (Gulwani et al., 2011; Jha et al., 2010; Schkufza et al., 2013).", "startOffset": 159, "endOffset": 222}, {"referenceID": 9, "context": "The first is based on the Hacker\u2019s delight (Warren, 2002) corpus, a collection of twenty five bitmanipulation programs, used as benchmark in program synthesis (Gulwani et al., 2011; Jha et al., 2010; Schkufza et al., 2013).", "startOffset": 159, "endOffset": 222}], "year": 2016, "abstractText": "Code super-optimization is the task of transforming any given program to a more efficient version while preserving its input-output behaviour. In some sense, it is similar to the paraphrase problem from natural language processing where the intention is to change the syntax of an utterance without changing its semantics. Code-optimization has been the subject of years of research that has resulted in the development of rule-based transformation strategies that are used by compilers. More recently, however, a class of stochastic search based methods have been shown to outperform these strategies. This approach involves repeated sampling of modifications to the program from a proposal distribution, which are accepted or rejected based on whether they preserve correctness and the improvement they achieve. These methods, however, neither learn from past behaviour nor do they try to leverage the semantics of the program under consideration. Motivated by this observation, we present a novel learning based approach for code super-optimization. Intuitively, our method works by learning the proposal distribution using unbiased estimators of the gradient of the expected improvement. Experiments on benchmarks comprising of automatically generated as well as existing (\u201cHacker\u2019s Delight\u201d) programs show that the proposed method is able to significantly outperform state of the art approaches for code super-optimization.", "creator": "LaTeX with hyperref package"}}}