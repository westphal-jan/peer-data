{"id": "1611.01796", "review": {"conference": "ICML", "VERSION": "v1", "DATE_OF_SUBMISSION": "6-Nov-2016", "title": "Modular Multitask Reinforcement Learning with Policy Sketches", "abstract": "We describe a framework for multitask deep reinforcement learning guided by policy sketches. Sketches annotate each task with a sequence of named subtasks, providing high-level structural relationships among tasks, but not providing the detailed guidance required by previous work on learning policy abstractions for RL (e.g. intermediate rewards, subtask completion signals, or intrinsic motivations). Our approach associates every subtask with its own modular subpolicy, and jointly optimizes over full task-specific policies by tying parameters across shared subpolicies. This optimization is accomplished via a simple decoupled actor-critic training objective that facilitates learning common behaviors from dissimilar reward functions. We evaluate the effectiveness of our approach on a maze navigation game and a 2-D Minecraft-inspired crafting game. Both games feature extremely sparse rewards that can be obtained only after completing a number of high-level subgoals (e.g. escaping from a sequence of locked rooms or collecting and combining various ingredients in the proper order). Experiments illustrate two main advantages of our approach. First, we outperform standard baselines that learn task-specific or shared monolithic policies. Second, our method naturally induces a library of primitive behaviors that can be recombined to rapidly acquire policies for new tasks.", "histories": [["v1", "Sun, 6 Nov 2016 15:36:56 GMT  (896kb,D)", "http://arxiv.org/abs/1611.01796v1", "Submitted to ICLR"], ["v2", "Sat, 17 Jun 2017 01:49:12 GMT  (3167kb,D)", "http://arxiv.org/abs/1611.01796v2", "To appear at ICML 2017"]], "COMMENTS": "Submitted to ICLR", "reviews": [], "SUBJECTS": "cs.LG cs.NE", "authors": ["jacob andreas", "dan klein", "sergey levine"], "accepted": true, "id": "1611.01796"}, "pdf": {"name": "1611.01796.pdf", "metadata": {"source": "CRF", "title": null, "authors": ["Jacob Andreas", "Dan Klein", "Sergey Levine"], "emails": ["jda@eecs.berkeley.edu", "klein@eecs.berkeley.edu", "svlevine@eecs.berkeley.edu"], "sections": [{"heading": null, "text": "In fact, most of them will be able to play by the rules they have imposed on themselves."}, {"heading": "2 RELATED WORK", "text": "In fact, most of them are able to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move and to move."}, {"heading": "3 LEARNING MODULAR POLICIES", "text": "We consider a multitask amplification problem originating from a family of infinite horizons as discounted Markov decision-making processes in a common environment. This environment is specified by a tuple (S, A, P, \u03b3), where S has a series of states, A a series of low-level actions, P: S \u00b7 A \u00b7 S \u2192 R has a distribution of transition probability and \u03b3 a discount factor. Each task is then specified by a pair (R\u03c4, \u03c1\u043e), where R\u03c4: S \u2192 R is a task-specific reward function and vice versa: S \u2192 R is an initial distribution across states. In addition to the components of a standard multitask RL problem, we assume that tasks are annotated by sketches of a given policy, each consisting of a sequence (B1 = i-jR = i-jR (sj)."}, {"heading": "3.1 MODEL", "text": "We take advantage of the structural information that has a corresponding subpolitical function by constructing for each symbol. (At any time frame, a subpolitical strategy can choose either a low-level action a or a special STOP action.) We refer to the expanded state space as a neural network, whose input is a representation of the current state and whose output is a distribution via A +. While all the action spaces in our experiments are discrete, it is easy to parameterise this last level, rather than a mixed distribution via an underlying action space and the STOP action. These subpolitical strategies can be seen as options of the kind described by Sutton et al. (1999), with the key distinction that they have no initiative setics, but are representable everywhere."}, {"heading": "3.3 CURRICULUM LEARNING", "text": "For complex tasks, such as the one shown in Figure 3b, it is difficult for the agent to detect any states with positive rewards until many subpolitical behaviors have already been learned. So it is a better use of the learner's time to focus on \"simple\" tasks, in which many rollouts lead to high rewards, from which appropriate subpolitical behavior can be derived. However, there is a fundamental trade-off involved here: If the learner spends too much time on simple tasks before becoming aware of the existence of more difficult tasks, excessive and subpolitical measures may occur that no longer generalize or exhibit the desired structural characteristics. To avoid both of these problems, we use a curriculum learning scheme (Bengio et al., 2009) that allows the model to be smoothly expanded from simple tasks to more difficult ones, while avoiding overadjustment. Originally, the model is presented with tasks associated with short sketches, once all of these tasks are achieved, the reward limit is increased for average."}, {"heading": "4 EXPERIMENTS", "text": "As described in the introduction, we evaluate the performance of our approach in two environments: a maze navigation game and a craft game. Both games include non-trivial low-level controls: Agents must learn to avoid obstacles and interact with different types of objects, but the environments also have a hierarchical structure: rewards are only accessible when the agent completes two to five high-level actions in the appropriate order. In all our experiments, we implement each partial policy as a multi-layer perceptron with ReLU nonlinearity and a hidden layer of 128 hidden units, and each critic as a linear function of the current state. Each subpolicy network receives as input a set of characteristics describing the current state of the environment, and outputs a distribution of actions. The agent acts in each timeframe by sampling samples from this distribution. The values specified in line 8 and 9 of the algorithm are set to a learning scale of 0.001 by RP 1 (Gradient Steps 2012)."}, {"heading": "4.1 ENVIRONMENTS", "text": "The labyrinth environment (Figure 3a) largely corresponds to the \"world of light\" described by Konidaris & Barto (2007). For our experiments, each task corresponds to a target space (always in the same position relative to the agent's starting position) that the agent must reach by navigating through a sequence of spaces in between. On each side of his body, the agent has a sensor that indicates the distance to keys, locked doors and open doors in the appropriate direction. Sketches define a specific sequence of instructions that the agent must traverse between the rooms in order to reach the target. Labyrinths are scanned with random sizes and random decisions about whether to connect rooms with open doors, locked doors and no doors. The sketch always corresponds to a usable crossing of the agent to reach the target."}, {"heading": "4.2 MULTITASK LEARNING", "text": "The primary experimental question in this paper is whether the additional structure provided by policy sketches is sufficient on its own to enable rapid learning of coupled strategies across tasks. To evaluate this, we compare our modular approach with two baselines: one that learns an independent policy for each task, and one that learns a common policy across all tasks. To evaluate the independent model, task-specific strategies are represented by networks with the same structure as the modular subpolicies. The common model requires both these environmental features and a feature vector that encodes the complete sketch. Both baselines perform best when they are trained with the same curriculum described in Section 3.3.3. Learning curves for both baselines and the modular model are shown in Figure 4. It can be seen that our approach performs significantly better in both the labyrinth area and in the crafting area than the baselines do in the fast-induced strategies: it significantly induced with a higher average."}, {"heading": "4.3 ABLATIONS", "text": "In addition to the general modular parameter coupling structure generated by our sketches, the key components of our training process are the decoupled critic and the curriculum. Our next experiments will investigate to what extent these are necessary for good performance. To evaluate the critic, we will consider three ablations: (1) the elimination of the model's dependence on the environmental state, with the baseline being a single scalar per task; (2) the elimination of the model's dependence on the task, with the baseline being a conventional general estimator of the benefit; and (3) the elimination of both, with the baseline being a single scalar, as in a vanilla-politgradient approach. Results are shown in Figure 5a. Introducing both the state and task dependence into the baseline leads to faster convergence of the model: The constant baseline approach achieves less than half of the total performance of the full 3 million critics after each episode."}, {"heading": "4.4 ZERO-SHOT AND ADAPTATION LEARNING", "text": "The model MT 0-S Ad.Independent.44 - < 1 Joint.49 < 1 - Modular.89.77.76edly executes this policy (without learning) to obtain an estimate of its effectiveness. For adaptation experiments, we consider ordinary reinforcement learning about B rather than A and implement the high-level learner using the same agent architecture as described in Section 3.1. Note that the independent baseline cannot be applied to the zero-shot assessment, while the common baseline cannot be applied to the adaptation baseline (because it depends on predefined sketch characteristics).The results are shown in Table 1. The tasks held are sufficiently challenging that the baselines can no longer receive negligible reward, while the modular model performs comparatively well."}, {"heading": "5 CONCLUSIONS", "text": "By linking each symbol that appears in a sketch with a modular neural sub-policy, we have shown that it is possible to build agents that share behavior between tasks to achieve success on tasks with sparse and delayed rewards. This process leads to an inventory of reusable and interpretable sub-policies that can be used for zero-shot generalization when more sketches are available and hierarchical reinforcement when they are not. Our work suggests that these sketches, which are easy to create and do not require a foundation in the environment, provide an effective framework for learning hierarchical strategies from minimal monitoring. We have published our code at http: / / github.com / jacobandreas / psketch."}, {"heading": "ACKNOWLEDGMENTS", "text": "YES is supported by a Facebook Graduate Fellowship."}, {"heading": "A TASKS AND SKETCHES", "text": "The full list of tasks, sketches and symbols is listed below. Tasks marked with an asterisk are offered for the generalization experiments described in section 4.4, but in the multitasking experiments in sections 4.2 and 4.3.Target sketchesLabyrinth environment Target left Tor2 left Tor3 left Tor3 right down Tor3 right Tor4 left Tor5 right Tor6 right up Tor7 right up Tor7 bottom right Tor8 left down Tor9 right down Tor10 left up right crafting environment make boards wood use tool shed make baton make wood use tool sheds use tool make cloth make grass use factory make rope use tool use tool bridge use iron use wood use factory use tool sheds use wood use tool shed use grass use tool sheds use tool bench use hatchet use wood use tool shed use tool sheds get used wood use tool bench use tool sheds use tool use tool sheds get get get get get get get get get get used get used get get used get used get get get used get used gold tool bank"}], "references": [{"title": "Learning to compose neural networks for question answering", "author": ["Jacob Andreas", "Marcus Rohrbach", "Trevor Darrell", "Dan Klein"], "venue": "In Proceedings of the Annual Meeting of the North American Chapter of the Association for Computational Linguistics,", "citeRegEx": "Andreas et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Andreas et al\\.", "year": 2016}, {"title": "The option-critic architecture", "author": ["Pierre-Luc Bacon", "Doina Precup"], "venue": "In NIPS Deep Reinforcement Learning Workshop,", "citeRegEx": "Bacon and Precup.,? \\Q2015\\E", "shortCiteRegEx": "Bacon and Precup.", "year": 2015}, {"title": "Hierarchical reinforcement learning based on subgoal discovery and subpolicy specialization", "author": ["Bram Bakker", "J\u00fcrgen Schmidhuber"], "venue": "In Proc. of the 8-th Conf. on Intelligent Autonomous Systems,", "citeRegEx": "Bakker and Schmidhuber.,? \\Q2004\\E", "shortCiteRegEx": "Bakker and Schmidhuber.", "year": 2004}, {"title": "Reinforcement learning for mapping instructions to actions", "author": ["S.R.K. Branavan", "Harr Chen", "Luke S. Zettlemoyer", "Regina Barzilay"], "venue": "In Proceedings of the Annual Meeting of the Association for Computational Linguistics,", "citeRegEx": "Branavan et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Branavan et al\\.", "year": 2009}, {"title": "Hierarchical relative entropy policy search", "author": ["Christian Daniel", "Gerhard Neumann", "Jan Peters"], "venue": "In Proceedings of the International Conference on Artificial Intelligence and Statistics,", "citeRegEx": "Daniel et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Daniel et al\\.", "year": 2012}, {"title": "Learning modular neural network policies for multi-task and multi-robot transfer", "author": ["Coline Devin", "Abhishek Gupta", "Trevor Darrell", "Pieter Abbeel", "Sergey Levine"], "venue": "arXiv preprint arXiv:1609.07088,", "citeRegEx": "Devin et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Devin et al\\.", "year": 2016}, {"title": "Variance reduction techniques for gradient estimates in reinforcement learning", "author": ["Evan Greensmith", "Peter L Bartlett", "Jonathan Baxter"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Greensmith et al\\.,? \\Q2004\\E", "shortCiteRegEx": "Greensmith et al\\.", "year": 2004}, {"title": "Using motion primitives in probabilistic sample-based planning for humanoid robots", "author": ["Kris Hauser", "Timothy Bretl", "Kensuke Harada", "Jean-Claude Latombe"], "venue": "In Algorithmic foundation of robotics,", "citeRegEx": "Hauser et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Hauser et al\\.", "year": 2008}, {"title": "Discovering hierarchy in reinforcement learning with HEXQ", "author": ["Bernhard Hengst"], "venue": "In ICML,", "citeRegEx": "Hengst.,? \\Q2002\\E", "shortCiteRegEx": "Hengst.", "year": 2002}, {"title": "A neural network for factoid question answering over paragraphs", "author": ["Mohit Iyyer", "Jordan Boyd-Graber", "Leonardo Claudino", "Richard Socher", "Hal Daum\u00e9 III"], "venue": "In Proceedings of the Conference on Empirical Methods in Natural Language Processing,", "citeRegEx": "Iyyer et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Iyyer et al\\.", "year": 2014}, {"title": "Near-optimal reinforcement learning in polynomial time", "author": ["Michael Kearns", "Satinder Singh"], "venue": "Machine Learning,", "citeRegEx": "Kearns and Singh.,? \\Q2002\\E", "shortCiteRegEx": "Kearns and Singh.", "year": 2002}, {"title": "Building portable options: Skill transfer in reinforcement learning", "author": ["George Konidaris", "Andrew G Barto"], "venue": "In IJCAI,", "citeRegEx": "Konidaris and Barto.,? \\Q2007\\E", "shortCiteRegEx": "Konidaris and Barto.", "year": 2007}, {"title": "Robot learning from demonstration by constructing skill trees", "author": ["George Konidaris", "Scott Kuindersma", "Roderic Grupen", "Andrew Barto"], "venue": "The International Journal of Robotics Research,", "citeRegEx": "Konidaris et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Konidaris et al\\.", "year": 2011}, {"title": "Hierarchical deep reinforcement learning: Integrating temporal abstraction and intrinsic motivation", "author": ["Tejas D Kulkarni", "Karthik R Narasimhan", "Ardavan Saeedi", "Joshua B Tenenbaum"], "venue": "arXiv preprint arXiv:1604.06057,", "citeRegEx": "Kulkarni et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Kulkarni et al\\.", "year": 2016}, {"title": "Grounding english commands to reward functions", "author": ["J MacGlashan", "M Babes-Vroman", "M desJardins", "M Littman", "S Muresan", "S Squire", "S Tellex", "D Arumugam", "L Yang"], "venue": "In Robotics: Science and Systems,", "citeRegEx": "MacGlashan et al\\.,? \\Q2015\\E", "shortCiteRegEx": "MacGlashan et al\\.", "year": 2015}, {"title": "Q-cutdynamic discovery of sub-goals in reinforcement learning", "author": ["Ishai Menache", "Shie Mannor", "Nahum Shimkin"], "venue": "In European Conference on Machine Learning,", "citeRegEx": "Menache et al\\.,? \\Q2002\\E", "shortCiteRegEx": "Menache et al\\.", "year": 2002}, {"title": "Neural programmer: Inducing latent programs with gradient descent", "author": ["Arvind Neelakantan", "Quoc V Le", "Ilya Sutskever"], "venue": "arXiv preprint arXiv:1511.04834,", "citeRegEx": "Neelakantan et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Neelakantan et al\\.", "year": 2015}, {"title": "Temporal abstraction in reinforcement learning", "author": ["Doina Precup"], "venue": "PhD thesis,", "citeRegEx": "Precup.,? \\Q2000\\E", "shortCiteRegEx": "Precup.", "year": 2000}, {"title": "Neural programmer-interpreters", "author": ["Scott Reed", "Nando de Freitas"], "venue": "Proceedings of the International Conference on Learning Representations,", "citeRegEx": "Reed and Freitas.,? \\Q2015\\E", "shortCiteRegEx": "Reed and Freitas.", "year": 2015}, {"title": "Highdimensional continuous control using generalized advantage estimation", "author": ["John Schulman", "Philipp Moritz", "Sergey Levine", "Michael Jordan", "Pieter Abbeel"], "venue": "arXiv preprint arXiv:1506.02438,", "citeRegEx": "Schulman et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Schulman et al\\.", "year": 2015}, {"title": "Semantic compositionality through recursive matrix-vector spaces", "author": ["Richard Socher", "Brody Huval", "Christopher Manning", "Andrew Ng"], "venue": "In Proceedings of the Conference on Empirical Methods in Natural Language Processing,", "citeRegEx": "Socher et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Socher et al\\.", "year": 2012}, {"title": "Learning options in reinforcement learning", "author": ["Martin Stolle", "Doina Precup"], "venue": "In International Symposium on Abstraction, Reformulation, and Approximation,", "citeRegEx": "Stolle and Precup.,? \\Q2002\\E", "shortCiteRegEx": "Stolle and Precup.", "year": 2002}, {"title": "Between MDPs and semi-MDPs: A framework for temporal abstraction in reinforcement learning", "author": ["Richard S Sutton", "Doina Precup", "Satinder Singh"], "venue": "Artificial intelligence,", "citeRegEx": "Sutton et al\\.,? \\Q1999\\E", "shortCiteRegEx": "Sutton et al\\.", "year": 1999}, {"title": "Understanding natural language commands for robotic navigation and mobile manipulation", "author": ["Stefanie Tellex", "Thomas Kollar", "Steven Dickerson", "Matthew R. Walter", "Ashis Gopal Banerjee", "Seth Teller", "Nicholas Roy"], "venue": "Proceedings of the National Conference on Artificial Intelligence,", "citeRegEx": "Tellex et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Tellex et al\\.", "year": 2011}, {"title": "Strategic attentive writer for learning macro-actions", "author": ["Alexander Vezhnevets", "Volodymyr Mnih", "John Agapiou", "Simon Osindero", "Alex Graves", "Oriol Vinyals", "Koray Kavukcuoglu"], "venue": "arXiv preprint arXiv:1606.04695,", "citeRegEx": "Vezhnevets et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Vezhnevets et al\\.", "year": 2016}, {"title": "Simple statistical gradient-following algorithms for connectionist reinforcement learning", "author": ["Ronald J Williams"], "venue": "Machine learning,", "citeRegEx": "Williams.,? \\Q1992\\E", "shortCiteRegEx": "Williams.", "year": 1992}], "referenceMentions": [{"referenceID": 22, "context": "In these approaches, a high-level controller learns a policy over high-level actions\u2014known variously as options (Sutton et al., 1999), skills", "startOffset": 112, "endOffset": 133}, {"referenceID": 7, "context": "(Konidaris & Barto, 2007), or primitives (Hauser et al., 2008)\u2014which are themselves implemented as policies over low-level actions in the environment.", "startOffset": 41, "endOffset": 62}, {"referenceID": 8, "context": "(2012)) investigates learning hierarchical policies without any supervision, such hierarchies are empirically difficult to learn directly from unconstrained interaction (Hengst, 2002).", "startOffset": 169, "endOffset": 183}, {"referenceID": 4, "context": "Daniel et al. (2012)) investigates learning hierarchical policies without any supervision, such hierarchies are empirically difficult to learn directly from unconstrained interaction (Hengst, 2002).", "startOffset": 0, "endOffset": 21}, {"referenceID": 0, "context": "The present work may be viewed as an extension of recent approaches for learning compositional deep architectures from structured program descriptors (Andreas et al., 2016; Reed & de Freitas, 2015).", "startOffset": 150, "endOffset": 197}, {"referenceID": 13, "context": "to encourage exploration (Kearns & Singh, 2002) or completion of predefined subtasks (Kulkarni et al., 2016).", "startOffset": 85, "endOffset": 108}, {"referenceID": 12, "context": "An alternative family of approaches employs either posthoc analysis of already-learned policies to extract reusable sub-components (Stolle & Precup, 2002; Konidaris et al., 2011).", "startOffset": 131, "endOffset": 178}, {"referenceID": 17, "context": "As detailed in Section 3, our subpolicies may be viewed as a relaxation of the options framework first described by Sutton et al. (1999). A large body of work describes techniques for learning options and related abstract actions, in both single- and multitask settings.", "startOffset": 116, "endOffset": 137}, {"referenceID": 11, "context": "An alternative family of approaches employs either posthoc analysis of already-learned policies to extract reusable sub-components (Stolle & Precup, 2002; Konidaris et al., 2011). Techniques for learning options with less guidance than the present work include Bacon & Precup (2015) and Vezhnevets et al.", "startOffset": 155, "endOffset": 283}, {"referenceID": 11, "context": "An alternative family of approaches employs either posthoc analysis of already-learned policies to extract reusable sub-components (Stolle & Precup, 2002; Konidaris et al., 2011). Techniques for learning options with less guidance than the present work include Bacon & Precup (2015) and Vezhnevets et al. (2016), and other general hierarchical policy learners include Daniel et al.", "startOffset": 155, "endOffset": 312}, {"referenceID": 4, "context": "(2016), and other general hierarchical policy learners include Daniel et al. (2012), Bakker & Schmidhuber (2004) and Menache et al.", "startOffset": 63, "endOffset": 84}, {"referenceID": 4, "context": "(2016), and other general hierarchical policy learners include Daniel et al. (2012), Bakker & Schmidhuber (2004) and Menache et al.", "startOffset": 63, "endOffset": 113}, {"referenceID": 4, "context": "(2016), and other general hierarchical policy learners include Daniel et al. (2012), Bakker & Schmidhuber (2004) and Menache et al. (2002).", "startOffset": 63, "endOffset": 139}, {"referenceID": 9, "context": "Such models have been previously used for tasks involving question answering (Iyyer et al., 2014; Andreas et al., 2016) and relational reasoning (Socher et al.", "startOffset": 77, "endOffset": 119}, {"referenceID": 0, "context": "Such models have been previously used for tasks involving question answering (Iyyer et al., 2014; Andreas et al., 2016) and relational reasoning (Socher et al.", "startOffset": 77, "endOffset": 119}, {"referenceID": 20, "context": ", 2016) and relational reasoning (Socher et al., 2012), and more recently for multi-task, multi-robot transfer problems (Devin et al.", "startOffset": 33, "endOffset": 54}, {"referenceID": 5, "context": ", 2012), and more recently for multi-task, multi-robot transfer problems (Devin et al., 2016).", "startOffset": 73, "endOffset": 93}, {"referenceID": 16, "context": "Another closely related family of models includes neural programmers (Neelakantan et al., 2015) and programmer\u2013interpreters (Reed & de Freitas, 2015), which generate discrete computational structures but require supervision in the form of output actions or full execution traces.", "startOffset": 69, "endOffset": 95}, {"referenceID": 3, "context": "While existing approaches learn mappings from instructions to actions (Branavan et al., 2009), reward functions (MacGlashan et al.", "startOffset": 70, "endOffset": 93}, {"referenceID": 14, "context": ", 2009), reward functions (MacGlashan et al., 2015), and planning constraints (Tellex et al.", "startOffset": 26, "endOffset": 51}, {"referenceID": 23, "context": ", 2015), and planning constraints (Tellex et al., 2011), we are not aware of past work that maps descriptions of behavior directly into hierarchical policies.", "startOffset": 34, "endOffset": 55}, {"referenceID": 22, "context": "These subpolicies may be viewed as options of the kind described by Sutton et al. (1999), with the key distinction that they have no initiation semantics, but are instead invokable everywhere, and have no explicit representation as a function from an initial state to a distribution over final states (instead implicitly using the STOP action to terminate).", "startOffset": 68, "endOffset": 89}, {"referenceID": 25, "context": "In a standard policy gradient approach, with a single policy \u03c0 with parameters \u03b8, we compute gradient steps of the form (Williams, 1992):", "startOffset": 120, "endOffset": 136}, {"referenceID": 19, "context": "Recalling our previous definition of qi as the empirical return starting from si, this form of the gradient corresponds to a generalized advantage estimator (Schulman et al., 2015) with \u03bb = 1.", "startOffset": 157, "endOffset": 180}, {"referenceID": 6, "context": "Here c achieves close to the optimal variance (Greensmith et al., 2004) when it is set exactly equal to the state-value function V\u03c0(si) = E\u03c0qi for the target policy \u03c0 starting in state si.", "startOffset": 46, "endOffset": 71}, {"referenceID": 6, "context": "(This follows immediately by applying the corresponding argument in Greensmith et al. (2004) individually to each term in the sum over \u03c4 in Equation 2.", "startOffset": 68, "endOffset": 93}], "year": 2016, "abstractText": "We describe a framework for multitask deep reinforcement learning guided by policy sketches. Sketches annotate each task with a sequence of named subtasks, providing high-level structural relationships among tasks, but not providing the detailed guidance required by previous work on learning policy abstractions for RL (e.g. intermediate rewards, subtask completion signals, or intrinsic motivations). Our approach associates every subtask with its own modular subpolicy, and jointly optimizes over full task-specific policies by tying parameters across shared subpolicies. This optimization is accomplished via a simple decoupled actor\u2013critic training objective that facilitates learning common behaviors from dissimilar reward functions. We evaluate the effectiveness of our approach on a maze navigation game and a 2-D Minecraft-inspired crafting game. Both games feature extremely sparse rewards that can be obtained only after completing a number of high-level subgoals (e.g. escaping from a sequence of locked rooms or collecting and combining various ingredients in the proper order). Experiments illustrate two main advantages of our approach. First, we outperform standard baselines that learn task-specific or shared monolithic policies. Second, our method naturally induces a library of primitive behaviors that can be recombined to rapidly acquire policies for new tasks.", "creator": "LaTeX with hyperref package"}}}