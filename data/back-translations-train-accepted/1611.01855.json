{"id": "1611.01855", "review": {"conference": "iclr", "VERSION": "v1", "DATE_OF_SUBMISSION": "6-Nov-2016", "title": "Neuro-Symbolic Program Synthesis", "abstract": "Recent years have seen the proposal of a number of neural architectures for the problem of Program Induction. Given a set of input-output examples, these architectures are able to learn mappings that generalize to new test inputs. While achieving impressive results, these approaches have a number of important limitations: (a) they are computationally expensive and hard to train, (b) a model has to be trained for each task (program) separately, and (c) it is hard to interpret or verify the correctness of the learnt mapping (as it is defined by a neural network). In this paper, we propose a novel technique, Neuro-Symbolic Program Synthesis, to overcome the above-mentioned problems. Once trained, our approach can automatically construct computer programs in a domain-specific language that are consistent with a set of input-output examples provided at test time. Our method is based on two novel neural modules. The first module, called the cross correlation I/O network, given a set of input-output examples, produces a continuous representation of the set of I/O examples. The second module, the Recursive-Reverse-Recursive Neural Network (R3NN), given the continuous representation of the examples, synthesizes a program by incrementally expanding partial programs. We demonstrate the effectiveness of our approach by applying it to the rich and complex domain of regular expression based string transformations. Experiments show that the R3NN model is not only able to construct programs from new input-output examples, but it is also able to construct new programs for tasks that it had never observed before during training.", "histories": [["v1", "Sun, 6 Nov 2016 22:23:56 GMT  (1189kb,D)", "http://arxiv.org/abs/1611.01855v1", null]], "reviews": [], "SUBJECTS": "cs.AI cs.PL", "authors": ["emilio parisotto", "abdel-rahman mohamed", "rishabh singh", "lihong li", "dengyong zhou", "pushmeet kohli"], "accepted": true, "id": "1611.01855"}, "pdf": {"name": "1611.01855.pdf", "metadata": {"source": "CRF", "title": "NEURO-SYMBOLIC PROGRAM SYNTHESIS", "authors": ["Emilio Parisotto", "Abdel-rahman Mohamed", "Rishabh Singh", "Lihong Li", "Dengyong Zhou", "Pushmeet Kohli"], "emails": ["eparisot@andrew.cmu.edu", "asamir@microsoft.com", "risin@microsoft.com", "lihongli@microsoft.com", "denzho@microsoft.com", "pkohli@microsoft.com"], "sections": [{"heading": "1 INTRODUCTION", "text": "In fact, it is so that it will be able to reactivate the erroneous orders in order to reactivate them."}, {"heading": "2 PROBLEM DEFINITION", "text": "In this section, we will formally define the DSL-based synthesis problem that we will consider in this paper. If we use a DSL constant, we will automatically construct a synthesis algorithm A that provides a set of input-output examples, {(i1, o1), \u00b7 \u00b7, (in, on)}, A will supply a program P-L that corresponds to the input-output examples, i.e., the syntax and semantics of DSL for string transformations is shown in Figure 1 (b) and Figure 7, respectively. DSL corresponds to a large subset of FlashFill DSL (ij) = oj. (1) The syntax and semantics of DSL for string transformations is shown in Figure 1 (b)."}, {"heading": "3 OVERVIEW OF OUR APPROACH", "text": "We will now present an overview of our approach. Faced with a DSL L, we will learn a generative model of programs in DSL that is conditioned on input-output examples to efficiently search for consistent programs. Figure 2 shows the workflow of our system, which is trained end-to-end with a large set of programs in DSL along with their corresponding input-output examples. To generate a large training set, we can use standardized sample programs from DSL to calculate well-trained input strings that meet the preconditions of the programs, and the corresponding output strings are obtained by executing the programs on the input strings. A DSL can be obtained as context-free grammar with a set of non-terminals with appropriate expansion rules that meet the preconditions of the programs."}, {"heading": "4 TREE-STRUCTURED GENERATION MODEL", "text": "A PPT has two types of nodes: leaf nodes (symbol nodes) and inner non-leaf nodes (rule nodes). A leaf node represents a symbol, whether non-terminal or terminal. An inner non-leaf node represents a certain production rule of the DSL, where the number of children of the non-leaf node corresponds to the uniformity of the rule it represents. A PPT is called a program tree (PT) whenever all leaves of the tree are terminal symbols. Such a tree represents a completed program under the DSL and can be executed. We define an expansion as the valid application of a specific production rule (e \u2192 e op2 e) to a certain non-terminal sheet within a PPT (leaf with symbol e). We refer to the specific production rule that an expansion is derived from the expansion type."}, {"heading": "4.1 RECURSIVE-REVERSE-RECURSIVE NEURAL NETWORK", "text": "For each valid expansion, there are two components: first, the production rule that is used, and second, the position of the expanded leaf node relative to each other node in the tree. To accommodate the first component, a separate distributed representation is maintained for each production rule; second component is handled with an architecture in which forward propagation resembles belief in trees, allowing an idea of the global tree state at each node within the tree; a given probability of expansion is then considered proportional to the inner product between the production rule representation and the global tree representation; and we will describe the design of this architecture in more detail. R3NN has the following parameters for the grammar described by a DSL (see Figure 3)."}, {"heading": "4.1.1 GLOBAL TREE INFORMATION AT THE LEAVES", "text": "To calculate the probability distribution over set E, the R3NN first calculates a distributed representation \u03c6 (S (l)) for each leaf node that contains global tree information. To achieve this, we retrieve its distributed representation \u03c6 (S (l)) for each leaf node in the tree. We now make a standard recursive representation from bottom to top, RHS \u2192 LHS pass the network by going up the tree and applying fR (n) for each non-leaf node n-N to its RHS node representations (see Figure 3 (a)). These networks fR (n) produce a node representation that is entered into the trunk rule network and so on until we reach the root node. Once at the root node, we effectively have a fixed dimensionality global tree representation (root) global tree representation (root) c for the start symbol c. The problem is that this representation has lost any representation of tree position."}, {"heading": "4.1.2 EXPANSION PROBABILITIES", "text": "In view of the global leaf representations (l), we can now easily determine values for each e-mail. E.r is the expansion type for expansion (production rule r that e applies) and let e.l be the leaf regulation node l to which e.r. is applied. The probability of expansion e is simply the exponentialized normalized sum of all results: \u03c0 (e) = eze \u2211 e. \"E. e.\" An additional improvement that has proved helpful was the addition of a bidirectional LSTM to process global leaf pulations immediately before the results are calculated. The hidden LSTM states are then used in the score calculation and not the leaves themselves. This primarily serves to reduce the minimum length that information must spread between the nodes in the tree."}, {"heading": "5 CONDITIONING WITH INPUT/OUTPUT EXAMPLES", "text": "Now that we have defined a generation process using tree-structured programs, we need a way to condition this generation process based on a set of input / output examples. The set of input / output examples provides an almost complete specification for the desired output program, so good encoding of the examples is critical to the success of our program generator. This sample encoding must be largely domain specific, since different DSLs have different inputs (some can be integers, others via strings, etc.), so in our case we are using an encoding adapted to the input / output strings that our DSL works with. We are also investigating different ways of conditioning program searches based on the learned sample input / output encodings."}, {"heading": "5.1 ENCODING INPUT/OUTPUT EXAMPLES", "text": "There are two types of information that string manipulation programs need to extract from input examples: 1) constant strings such as \"@ domain.com\" or. \"\" that occur in all output examples; 2) partial string indices in input where the index could be further defined by a regular expression. These indices determine which parts of the input are also included in the output. To simplify DSL, we assume that there is a fixed finite universe of possible constant strings that could occur in programs. Therefore, we focus on extracting the second type of information that displays substrings. In earlier handcrafted systems such as FlashFill, this information was extracted from the input strings by executing the longest common substring algorithm, a dynamic programming algorithm that efficiently finds matching substrings in string pairs. To extract substrings, FlashFill generates a pair of strings from each input to generate a substring string line."}, {"heading": "5.1.1 BASELINE LSTM ENCODER", "text": "Our first I / O encoding network involves running two separate deep bidirectional LSTM networks to process input and output strings in each sample pair. For each pair, it then concatenates the uppermost hidden representation in each step to create a 4HT feature vector per I / O pair, with T being the maximum string length for each input or output string, and H the uppermost hidden LSTM dimension. We then link the encoding vectors across all I / O pairs to obtain a vector representation of the entire I / O set. This encoding is conceptually simple and has very little prior knowledge of what operations are performed over the strings, i.e. substrings, constants, etc., which could make it difficult to determine substring indices, especially those based on regular expressions."}, {"heading": "5.1.2 CROSS CORRELATION ENCODER", "text": "To help the model find input substrings that will be copied to the output, we have designed a novel I / O sample pair to calculate the cross-correlation between each input and output sample representation. We used the two output tensors of the LSTM encoder (discussed above) as inputs to this encoder. For each sample pair, we first push the output feature block over the input feature block and calculate the dot product between the respective position representation. Then, we sum up all overlapping time steps. Properties of all pairs are then linked to form a 2-dimensional (T \u2212 1) dimensional alignment encoding for all sample pairs. There are 2-dimensional (T \u2212 1) possible alignments overall between input and output feature blocks. We have also designed the following variants of this encoder. Diffused Cross Correlation of the Encorelation of the Encoding: This is the Encorelation of the Encoding Encoding: This is the Encoded Encoding of the Encoding of the Encoding."}, {"heading": "5.2 CONDITIONING PROGRAM SEARCH ON EXAMPLE ENCODINGS", "text": "Once the I / O sample encodings have been calculated, we can use them to perform the conditional generation of the program tree using the R3NN model. There are a number of ways in which the PPT generation model can be conditioned using the I / O sample encodings, depending on where the I / O sample information is inserted into the R3NN model. We examined three places to insert sample encodings: 1) Pre-conditioning: where sample encodings are linked to the encoding of each tree leaf and then passed on to a conditioning network before being passed on recursively from bottom to top over the program tree. The conditioning network can be either a multi-layered forward network or a bi-directional LSTM network running over tree leaves. After executing an LSTM over tree leaves, the model allows you to learn more about the position of each leaf cursive back to the top, before each cursive back to the top:"}, {"heading": "6 EXPERIMENTS", "text": "To list all possible programs under DSL up to a certain number of instructions, which are then divided into training, validation, and test sets. To have a comprehensible number of programs, we limit the maximum number of instructions for programs to 13. Length 13 programs are important for this specific DSL, since all larger programs can be written as compositions of subroutines with a length of no more than 13. Therefore, the semantics of length 13 programs form the \"atoms\" of this particular DSL. When testing our model, there are two different categories of generalization: the first is the input / output generalization, where we get a new set of input / output examples, and a program with a specific tree that we saw during the training. This represents the ability of the model to apply to new data. The second category is the generalization, where we get both a previously unknown program and an input / output output example, as input-inputs we do not have the input-input results sufficiently prepared."}, {"heading": "6.1 SETUP AND HYPERPARAMETERS SETTINGS", "text": "For the training of the R3NN, two hyperparameters that were critical to stabilizing the training were the use of hyperbolic tangent activation functions in both R3NN and cross-correlation I / O encoders, and the use of minibatches of length 8. Due to the difficulty of batching tree-based neural networks and time constraints, we were limited to 8 samples per batch, but some preliminary experiments showed that increasing the batch size further improved performance. Additionally, program tree generation is conditioned to a set of 10 input-output string pairs for all results. For each latent function and a series of input-output examples that we test, we report whether we were successful after sampling 100 functions from the model and testing all 100 to see if any of these functions correspond to latent function."}, {"heading": "6.2 EXAMPLE ENCODING", "text": "In this section, we evaluate the effect of several different sample input / output encoders. To control the effect of the tree model, all the results here used an R3NN with fixed hyperparameters to generate the program tree. Table 1 shows the performance of several of these sample input / output encoders. We see that the summed cross-correlation encoder did not perform well, which may be because the sum destroys position information that could be useful for determining certain sub-string indices. LSTM sum and advanced diffuse cross-correlation models performed best. Surprisingly, the LSTM encoder was able to find almost 88% of all programs without explicitly having prior knowledge built into the architecture. We use 100 samples to evaluate the traction and test sets. Training performance is sometimes slightly lower because there are nearly 5 million training programs, but we only look at less than 2 million of these programs during the training."}, {"heading": "6.3 IO2SEQ", "text": "In this section, we motivate the use of the R3NN model by testing whether a simpler model can also be used to generate programs. the io2seq model is an LSTM whose initial hidden states and cell states are a function of the input / output encoding vector. the io2seq model then generates a linear tree of a program symbol for symbol. an example of what a linear program tree looks like is (S (e (f (ConstStr \"@) ConstStr) f) e) S, which represents the program tree that returns the constant string\" @. \"Predicting a linear tree by means of an LSTM was also done in the context of the consensus (Vinyals et al., 2015). For the io2seq model, we used the LSTM sum correlation I / O conditioning model.The results in table 2 show that the 2b 2b 2b 2b 2b 2b 2b 2b 2b 2b 2b 2b 2b 2b 2b 2b 2b 2b 2b 2b 2b 2b 2b 2b 2b 2b 2b 2b 2b 2b 2b 2b 2b 2b 2b 2b 2b 2b 2b 2b 2b 2b 2b 2b 2b 2b 2b 2b 2b 2b 2b 2b 2b 2b 2b 2b 2b 2b 2b 2b 2b 2b 2b 2b 2b 2b 2b 2b 2b 2b 2b 2b 2b 2b 2b 2b 2b 2b 2b 2b 2b 2b 2b 2b 2b 2b 2b 2b 2b 2b 2b 2b 2b 2b 2b 2b 2b 2b 2b 2b 2b 2b 2b 2b 2b 2b 2b 2b 2b 2b 2b 2b 2b 2b 2b 2b 2b 2b 2b 2b 2b 2b 2b 2b 2b 2b 2b 2b 2b 2b 2b 2b 2b 2b 2b 2b 2b 2b 2b 2b 2b 2b 2b 2b 2b 2b 2b 2b 2b 2b 2b 2"}, {"heading": "6.4 EFFECT OF BACKTRACKING SEARCH", "text": "For the best R3NN model we trained, we also investigated the effect that a different number of samples per latent function had on performance; the results are shown in Table 3. Increasing the performance of the model as the sample size increases indicates that the model has an idea of what kind of program a particular I / O pair fulfills, but it may not be so sure about the details, such as which regex to use, and so on. For 300 samples, the model approaches perfect accuracy on the test sets."}, {"heading": "6.5 FLASHFILL BENCHMARKS", "text": "In fact, it is the case that most people who live in the United States also live in the United States. (...) It is not the case that they want to live in the United States, that they want to live in Europe. (...) It is not the case that they want to live in Europe. (...) It is not the case that they want to live in Europe. (...) It is the case that they want to live in Europe. (...) It is not the case that they want to live in Europe. (...) It is the case that they want to live in Europe. (...) It is the case that they want to live in Europe. (...) It is the case that they want to live in Europe. (...) It is the case that they want to live in Europe. (...) It is the case that they want to live in Europe. (...) It is the case that they want to live. (...) It is the case. (...) It is the case. (...) It is the case. (...) It is the case. (...) It is the case. (... It is the case. (...) It is the case. (... It is the case. (...) It is the case. (it is the case. (...) It is the case. (it is the case. It is the case. (...) it is. It is. (it is the case. (it is. It is.) It is. It is. (it is."}, {"heading": "7 RELATED WORK", "text": "We have seen a renewed interest in program induction and synthesis in recent years; in the machine learning community, a number of promising neural architectures have been proposed to perform program induction in 2016, using architectures inspired by computational modules (Turing Machines, RAM) (Graves et al., 2014; Kurach et al., 2015; Reed & de Freitas, 2015; Neelakantan et al., 2015) or common data structures such as stacks used in many algorithms (Joulin & Mikolov, 2015), which represent the atomic operations of the network in a differentiated form that allows efficient end-to-end training of a neural controller. However, unlike our approach, which learns comprehensible complete programs, many of these approaches only learn program behavior (i.e. they produce desired outputs on new input data)."}, {"heading": "8 CONCLUSION", "text": "We have proposed a novel technique called Neuro-Symbolic Program Synthesis, which is able to gradually construct a program based on given input-output examples. To do this, a new neural architecture called Recursive-Reverse-Recursive Neural Network is used to encode and expand a partial program tree into a complete program tree. Its effectiveness in exemplary program synthesis is demonstrated even if the program was not seen during the training. These promising results open up a number of interesting directions for future research. Here, for example, we have chosen an approach of supervised learning, assuming the availability of target programs during the training. In some scenarios, we may only have access to an oracle that returns the desired output from an input. In this case, amplification learning is a promising framework for program synthesis."}, {"heading": "A DOMAIN-SPECIFIC LANGUAGE FOR STRING TRANSFORMATIONS", "text": "The semantics of a ConstPos (k) expression is to link the results of the recursive evaluation of the constituent substring expressions fi. The semantics of ConstStr (s) is simply to return the constant string s. The semantics of a substring expression is to first evaluate the two position logics pl and pr to p1 and p2, respectively, and then return the substring corresponding to v [p1.. p2]. We designate s [i.. j] as a substring of the string s beginning with index i (inclusive) and ending with index j (exclusive) and len (s) as length. The semantics of ConstPos (k) as the return value k if k > 0 or len + k (if k < 0) if lt; the semantics of the position logic (r, k, start) as the return value of the kth match in r at the beginning v (if k > or end)."}], "references": [{"title": "Syntax-guided synthesis", "author": ["Alur", "Rajeev", "Bod\u0131\u0301k", "Rastislav", "Dallal", "Eric", "Fisman", "Dana", "Garg", "Pranav", "Juniwal", "Garvit", "KressGazit", "Hadas", "P. Madhusudan", "Martin", "Milo M. K", "Raghothaman", "Mukund", "Saha", "Shamwaditya", "Seshia", "Sanjit A", "Singh", "Rishabh", "Solar-Lezama", "Armando", "Torlak", "Emina", "Udupa", "Abhishek"], "venue": "In Dependable Software Systems Engineering,", "citeRegEx": "Alur et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Alur et al\\.", "year": 2015}, {"title": "PHOG: probabilistic model for code", "author": ["Bielik", "Pavol", "Raychev", "Veselin", "Vechev", "Martin T"], "venue": "In ICML,", "citeRegEx": "Bielik et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Bielik et al\\.", "year": 2016}, {"title": "The inference of regular lisp programs from examples", "author": ["Biermann", "Alan W"], "venue": "IEEE transactions on Systems, Man, and Cybernetics,", "citeRegEx": "Biermann and W.,? \\Q1978\\E", "shortCiteRegEx": "Biermann and W.", "year": 1978}, {"title": "Adaptive neural compilation", "author": ["Bunel", "Rudy", "Desmaison", "Alban", "Kohli", "Pushmeet", "Torr", "Philip H. S", "Kumar", "M. Pawan"], "venue": "CoRR, abs/1605.07969,", "citeRegEx": "Bunel et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Bunel et al\\.", "year": 2016}, {"title": "Terpret: A probabilistic programming language for program induction", "author": ["Gaunt", "Alexander L", "Brockschmidt", "Marc", "Singh", "Rishabh", "Kushman", "Nate", "Kohli", "Pushmeet", "Taylor", "Jonathan", "Tarlow", "Daniel"], "venue": "arXiv preprint arXiv:1608.04428,", "citeRegEx": "Gaunt et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Gaunt et al\\.", "year": 2016}, {"title": "Neural turing machines", "author": ["Graves", "Alex", "Wayne", "Greg", "Danihelka", "Ivo"], "venue": "arXiv preprint arXiv:1410.5401,", "citeRegEx": "Graves et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Graves et al\\.", "year": 2014}, {"title": "Automating string processing in spreadsheets using input-output examples", "author": ["Gulwani", "Sumit"], "venue": "In POPL, pp", "citeRegEx": "Gulwani and Sumit.,? \\Q2011\\E", "shortCiteRegEx": "Gulwani and Sumit.", "year": 2011}, {"title": "Synthesis of loop-free programs", "author": ["Gulwani", "Sumit", "Jha", "Susmit", "Tiwari", "Ashish", "Venkatesan", "Ramarathnam"], "venue": "In PLDI, pp", "citeRegEx": "Gulwani et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Gulwani et al\\.", "year": 2011}, {"title": "Spreadsheet data manipulation using examples", "author": ["Gulwani", "Sumit", "Harris", "William", "Singh", "Rishabh"], "venue": "Communications of the ACM,", "citeRegEx": "Gulwani et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Gulwani et al\\.", "year": 2012}, {"title": "Bidirectional recursive neural networks for token-level labeling with structure", "author": ["Irsoy", "Orzan", "Cardie", "Claire"], "venue": "In NIPS Deep Learning Workshop,", "citeRegEx": "Irsoy et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Irsoy et al\\.", "year": 2013}, {"title": "Inferring algorithmic patterns with stack-augmented recurrent nets", "author": ["Joulin", "Armand", "Mikolov", "Tomas"], "venue": "In NIPS, pp", "citeRegEx": "Joulin et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Joulin et al\\.", "year": 2015}, {"title": "Neural random-access machines", "author": ["Kurach", "Karol", "Andrychowicz", "Marcin", "Sutskever", "Ilya"], "venue": "arXiv preprint arXiv:1511.06392,", "citeRegEx": "Kurach et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Kurach et al\\.", "year": 2015}, {"title": "The inside-outside recursive neural network model for dependency parsing", "author": ["Le", "Phong", "Zuidema", "Willem"], "venue": "In EMNLP, pp", "citeRegEx": "Le et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Le et al\\.", "year": 2014}, {"title": "Learning programs: A hierarchical Bayesian approach", "author": ["Liang", "Percy", "Jordan", "Michael I", "Klein", "Dan"], "venue": "In ICML, pp", "citeRegEx": "Liang et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Liang et al\\.", "year": 2010}, {"title": "Structured generative models of natural source code", "author": ["Maddison", "Chris J", "Tarlow", "Daniel"], "venue": "In ICML, pp", "citeRegEx": "Maddison et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Maddison et al\\.", "year": 2014}, {"title": "A machine learning framework for programming by example", "author": ["Menon", "Aditya Krishna", "Tamuz", "Omer", "Gulwani", "Sumit", "Lampson", "Butler W", "Kalai", "Adam"], "venue": "In ICML, pp", "citeRegEx": "Menon et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Menon et al\\.", "year": 2013}, {"title": "Neural programmer: Inducing latent programs with gradient descent", "author": ["Neelakantan", "Arvind", "Le", "Quoc V", "Sutskever", "Ilya"], "venue": "arXiv preprint arXiv:1511.04834,", "citeRegEx": "Neelakantan et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Neelakantan et al\\.", "year": 2015}, {"title": "Global belief recursive neural networks", "author": ["Paulus", "Romain", "Socher", "Richard", "Manning", "Christopher D"], "venue": null, "citeRegEx": "Paulus et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Paulus et al\\.", "year": 2014}, {"title": "Predicting program properties from \u201dbig code", "author": ["Raychev", "Veselin", "Vechev", "Martin T", "Krause", "Andreas"], "venue": "In POPL, pp", "citeRegEx": "Raychev et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Raychev et al\\.", "year": 2015}, {"title": "Programming with a differentiable forth interpreter", "author": ["Riedel", "Sebastian", "Bosnjak", "Matko", "Rockt\u00e4schel", "Tim"], "venue": "CoRR, abs/1605.06640,", "citeRegEx": "Riedel et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Riedel et al\\.", "year": 2016}, {"title": "Stochastic superoptimization", "author": ["Schkufza", "Eric", "Sharma", "Rahul", "Aiken", "Alex"], "venue": "In ASPLOS, pp", "citeRegEx": "Schkufza et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Schkufza et al\\.", "year": 2013}, {"title": "Synthesizing data structure manipulations from storyboards", "author": ["Singh", "Rishabh", "Solar-Lezama", "Armando"], "venue": "In SIGSOFT FSE, pp", "citeRegEx": "Singh et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Singh et al\\.", "year": 2011}, {"title": "Automated feedback generation for introductory programming assignments", "author": ["Singh", "Rishabh", "Gulwani", "Sumit", "Solar-Lezama", "Armando"], "venue": "In PLDI, pp", "citeRegEx": "Singh et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Singh et al\\.", "year": 2013}, {"title": "Program Synthesis By Sketching", "author": ["Solar-Lezama", "Armando"], "venue": "PhD thesis, EECS Dept., UC Berkeley,", "citeRegEx": "Solar.Lezama and Armando.,? \\Q2008\\E", "shortCiteRegEx": "Solar.Lezama and Armando.", "year": 2008}, {"title": "Programming by sketching for bit-streaming programs", "author": ["Solar-Lezama", "Armando", "Rabbah", "Rodric", "Bodik", "Rastislav", "Ebcioglu", "Kemal"], "venue": "In PLDI,", "citeRegEx": "Solar.Lezama et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Solar.Lezama et al\\.", "year": 2005}, {"title": "A methodology for lisp program construction from examples", "author": ["Summers", "Phillip D"], "venue": "Journal of the ACM (JACM),", "citeRegEx": "Summers and D.,? \\Q1977\\E", "shortCiteRegEx": "Summers and D.", "year": 1977}, {"title": "TRANSIT: specifying protocols with concolic snippets", "author": ["Udupa", "Abhishek", "Raghavan", "Arun", "Deshmukh", "Jyotirmoy V", "Mador-Haim", "Sela", "Martin", "Milo M. K", "Alur", "Rajeev"], "venue": "In PLDI, pp", "citeRegEx": "Udupa et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Udupa et al\\.", "year": 2013}, {"title": "Grammar as a foreign language", "author": ["Vinyals", "Oriol", "Kaiser", "Lukasz", "Koo", "Terry", "Petrov", "Slav", "Sutskever", "Ilya", "Hinton", "Geoffrey"], "venue": "In ICLR,", "citeRegEx": "Vinyals et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Vinyals et al\\.", "year": 2015}], "referenceMentions": [{"referenceID": 5, "context": "Many of these models are inspired from computation modules (CPU, RAM, GPU) (Graves et al., 2014; Kurach et al., 2015; Reed & de Freitas, 2015; Neelakantan et al., 2015) or common data structures used in many algorithms (stack) (Joulin & Mikolov, 2015).", "startOffset": 75, "endOffset": 168}, {"referenceID": 11, "context": "Many of these models are inspired from computation modules (CPU, RAM, GPU) (Graves et al., 2014; Kurach et al., 2015; Reed & de Freitas, 2015; Neelakantan et al., 2015) or common data structures used in many algorithms (stack) (Joulin & Mikolov, 2015).", "startOffset": 75, "endOffset": 168}, {"referenceID": 16, "context": "Many of these models are inspired from computation modules (CPU, RAM, GPU) (Graves et al., 2014; Kurach et al., 2015; Reed & de Freitas, 2015; Neelakantan et al., 2015) or common data structures used in many algorithms (stack) (Joulin & Mikolov, 2015).", "startOffset": 75, "endOffset": 168}, {"referenceID": 11, "context": "While some recently proposed methods (Kurach et al., 2015; Gaunt et al., 2016; Riedel et al., 2016; Bunel et al., 2016) do learn interpretable programs, they still need to learn a separate neural network model for each individual task.", "startOffset": 37, "endOffset": 119}, {"referenceID": 4, "context": "While some recently proposed methods (Kurach et al., 2015; Gaunt et al., 2016; Riedel et al., 2016; Bunel et al., 2016) do learn interpretable programs, they still need to learn a separate neural network model for each individual task.", "startOffset": 37, "endOffset": 119}, {"referenceID": 19, "context": "While some recently proposed methods (Kurach et al., 2015; Gaunt et al., 2016; Riedel et al., 2016; Bunel et al., 2016) do learn interpretable programs, they still need to learn a separate neural network model for each individual task.", "startOffset": 37, "endOffset": 119}, {"referenceID": 3, "context": "While some recently proposed methods (Kurach et al., 2015; Gaunt et al., 2016; Riedel et al., 2016; Bunel et al., 2016) do learn interpretable programs, they still need to learn a separate neural network model for each individual task.", "startOffset": 37, "endOffset": 119}, {"referenceID": 24, "context": "the back of the development of methods for learning programs in various domains, ranging from low-level bit manipulation code (Solar-Lezama et al., 2005) to data structure manipulations (Singh & Solar-Lezama, 2011) and regular expression based string transformations (Gulwani, 2011).", "startOffset": 126, "endOffset": 153}, {"referenceID": 26, "context": "Several search techniques including enumerative (Udupa et al., 2013), stochastic (Schkufza et al.", "startOffset": 48, "endOffset": 68}, {"referenceID": 20, "context": ", 2013), stochastic (Schkufza et al., 2013), constraint-based (Solar-Lezama, 2008), and version-space algebra based algorithms (Gulwani et al.", "startOffset": 20, "endOffset": 43}, {"referenceID": 8, "context": ", 2013), constraint-based (Solar-Lezama, 2008), and version-space algebra based algorithms (Gulwani et al., 2012) have been developed to search over the space of programs in the DSL, which support different kinds of specifications (examples, partial programs, natural language etc.", "startOffset": 91, "endOffset": 113}, {"referenceID": 8, "context": "We demonstrate the efficacy of our method by applying it to the rich and complex domain of regularexpression-based syntactic string transformations, using a DSL based on the one used by FlashFill (Gulwani, 2011; Gulwani et al., 2012), a Programming-By-Example (PBE) system in Microsoft Excel 2013.", "startOffset": 196, "endOffset": 233}, {"referenceID": 17, "context": "The R3NN can be seen as an extension and combination of several previous tree-based models, which were mainly developed in the context of natural language processing (Le & Zuidema, 2014; Paulus et al., 2014; Irsoy & Cardie, 2013).", "startOffset": 166, "endOffset": 229}, {"referenceID": 27, "context": "Predicting a linearized tree using an LSTM was also done in the context of parsing (Vinyals et al., 2015).", "startOffset": 83, "endOffset": 105}, {"referenceID": 0, "context": "Since there are more than 2 million programs in the DSL of length 11 itself, the enumerative techniques with uniform search do not scale well (Alur et al., 2015).", "startOffset": 142, "endOffset": 161}, {"referenceID": 5, "context": "These methods have employed architectures inspired from computation modules (Turing Machines, RAM) (Graves et al., 2014; Kurach et al., 2015; Reed & de Freitas, 2015; Neelakantan et al., 2015) or common data structures such as stacks used in many algorithms (Joulin & Mikolov, 2015).", "startOffset": 99, "endOffset": 192}, {"referenceID": 11, "context": "These methods have employed architectures inspired from computation modules (Turing Machines, RAM) (Graves et al., 2014; Kurach et al., 2015; Reed & de Freitas, 2015; Neelakantan et al., 2015) or common data structures such as stacks used in many algorithms (Joulin & Mikolov, 2015).", "startOffset": 99, "endOffset": 192}, {"referenceID": 16, "context": "These methods have employed architectures inspired from computation modules (Turing Machines, RAM) (Graves et al., 2014; Kurach et al., 2015; Reed & de Freitas, 2015; Neelakantan et al., 2015) or common data structures such as stacks used in many algorithms (Joulin & Mikolov, 2015).", "startOffset": 99, "endOffset": 192}, {"referenceID": 11, "context": "Some recently proposed methods (Kurach et al., 2015; Gaunt et al., 2016; Riedel et al., 2016; Bunel et al., 2016) do learn interpretable programs but these techniques require learning a separate neural network model for each individual task, which is undesirable in many synthesis settings where we would like to learn programs in real-time for a large number of tasks.", "startOffset": 31, "endOffset": 113}, {"referenceID": 4, "context": "Some recently proposed methods (Kurach et al., 2015; Gaunt et al., 2016; Riedel et al., 2016; Bunel et al., 2016) do learn interpretable programs but these techniques require learning a separate neural network model for each individual task, which is undesirable in many synthesis settings where we would like to learn programs in real-time for a large number of tasks.", "startOffset": 31, "endOffset": 113}, {"referenceID": 19, "context": "Some recently proposed methods (Kurach et al., 2015; Gaunt et al., 2016; Riedel et al., 2016; Bunel et al., 2016) do learn interpretable programs but these techniques require learning a separate neural network model for each individual task, which is undesirable in many synthesis settings where we would like to learn programs in real-time for a large number of tasks.", "startOffset": 31, "endOffset": 113}, {"referenceID": 3, "context": "Some recently proposed methods (Kurach et al., 2015; Gaunt et al., 2016; Riedel et al., 2016; Bunel et al., 2016) do learn interpretable programs but these techniques require learning a separate neural network model for each individual task, which is undesirable in many synthesis settings where we would like to learn programs in real-time for a large number of tasks.", "startOffset": 31, "endOffset": 113}, {"referenceID": 0, "context": "The DSL-based program synthesis approach has also seen a renewed interest recently (Alur et al., 2015).", "startOffset": 83, "endOffset": 102}, {"referenceID": 24, "context": "It has been used for many applications including synthesizing low-level bitvector implementations (Solar-Lezama et al., 2005), Excel macros for data manipulation (Gulwani, 2011; Gulwani et al.", "startOffset": 98, "endOffset": 125}, {"referenceID": 8, "context": ", 2005), Excel macros for data manipulation (Gulwani, 2011; Gulwani et al., 2012), superoptimization by finding smaller equivalent loop bodies (Schkufza et al.", "startOffset": 44, "endOffset": 81}, {"referenceID": 20, "context": ", 2012), superoptimization by finding smaller equivalent loop bodies (Schkufza et al., 2013), protocol synthesis from scenarios (Udupa et al.", "startOffset": 69, "endOffset": 92}, {"referenceID": 26, "context": ", 2013), protocol synthesis from scenarios (Udupa et al., 2013), synthesis of loop-free programs (Gulwani et al.", "startOffset": 43, "endOffset": 63}, {"referenceID": 7, "context": ", 2013), synthesis of loop-free programs (Gulwani et al., 2011), and automated feedback generation for programming assignments (Singh et al.", "startOffset": 41, "endOffset": 63}, {"referenceID": 22, "context": ", 2011), and automated feedback generation for programming assignments (Singh et al., 2013).", "startOffset": 71, "endOffset": 91}, {"referenceID": 15, "context": "There is some work on guiding the program search using learnt clues that suggest likely DSL expansions, but the clues are learnt over hand-coded textual features of examples (Menon et al., 2013).", "startOffset": 174, "endOffset": 194}, {"referenceID": 2, "context": ", 2016; Bunel et al., 2016) do learn interpretable programs but these techniques require learning a separate neural network model for each individual task, which is undesirable in many synthesis settings where we would like to learn programs in real-time for a large number of tasks. Liang et al. (2010) restrict the problem space with a probabilistic context-free grammar and introduce a new representation of programs based on combinatory logic, which allows for sharing sub-programs across multiple tasks.", "startOffset": 8, "endOffset": 304}, {"referenceID": 18, "context": "There is also a recent line of work on learning probabilistic models of code from a large number of code repositories (big code) (Raychev et al., 2015; Bielik et al., 2016; Hindle et al., 2016), which are then used for applications such as auto-completion of partial programs, inference of variable and method names, program repair, etc.", "startOffset": 129, "endOffset": 193}, {"referenceID": 1, "context": "There is also a recent line of work on learning probabilistic models of code from a large number of code repositories (big code) (Raychev et al., 2015; Bielik et al., 2016; Hindle et al., 2016), which are then used for applications such as auto-completion of partial programs, inference of variable and method names, program repair, etc.", "startOffset": 129, "endOffset": 193}, {"referenceID": 17, "context": "The R3NN model employed in our work is related to several tree and graph structured neural networks present in the NLP literature (Le & Zuidema, 2014; Paulus et al., 2014; Irsoy & Cardie, 2013).", "startOffset": 130, "endOffset": 193}], "year": 2016, "abstractText": "Recent years have seen the proposal of a number of neural architectures for the problem of Program Induction. Given a set of input-output examples, these architectures are able to learn mappings that generalize to new test inputs. While achieving impressive results, these approaches have a number of important limitations: (a) they are computationally expensive and hard to train, (b) a model has to be trained for each task (program) separately, and (c) it is hard to interpret or verify the correctness of the learnt mapping (as it is defined by a neural network). In this paper, we propose a novel technique, Neuro-Symbolic Program Synthesis, to overcome the above-mentioned problems. Once trained, our approach can automatically construct computer programs in a domain-specific language that are consistent with a set of input-output examples provided at test time. Our method is based on two novel neural modules. The first module, called the cross correlation I/O network, given a set of input-output examples, produces a continuous representation of the set of I/O examples. The second module, the RecursiveReverse-Recursive Neural Network (R3NN), given the continuous representation of the examples, synthesizes a program by incrementally expanding partial programs. We demonstrate the effectiveness of our approach by applying it to the rich and complex domain of regular expression based string transformations. Experiments show that the R3NN model is not only able to construct programs from new input-output examples, but it is also able to construct new programs for tasks that it had never observed before during training.", "creator": "LaTeX with hyperref package"}}}