{"id": "1611.01867", "review": {"conference": "NIPS", "VERSION": "v1", "DATE_OF_SUBMISSION": "7-Nov-2016", "title": "Latent Attention For If-Then Program Synthesis", "abstract": "Automatic translation from natural language descriptions into programs is a longstanding challenging problem. In this work, we consider a simple yet important sub-problem: translation from textual descriptions to If-Then programs. We devise a novel neural network architecture for this task which we train end-to-end. Specifically, we introduce Latent Attention, which computes multiplicative weights for the words in the description in a two-stage process with the goal of better leveraging the natural language structures that indicate the relevant parts for predicting program elements. Our architecture reduces the error rate by 28.57% compared to prior art. We also propose a one-shot learning scenario of If-Then program synthesis and simulate it with our existing dataset. We demonstrate a variation on the training procedure for this scenario that outperforms the original procedure, significantly closing the gap to the model trained with all data.", "histories": [["v1", "Mon, 7 Nov 2016 00:56:19 GMT  (1609kb,D)", "http://arxiv.org/abs/1611.01867v1", "Accepted by NIPS 2016"]], "COMMENTS": "Accepted by NIPS 2016", "reviews": [], "SUBJECTS": "cs.CL", "authors": ["chang liu", "xinyun chen", "eui chul richard shin", "mingcheng chen", "dawn xiaodong song"], "accepted": true, "id": "1611.01867"}, "pdf": {"name": "1611.01867.pdf", "metadata": {"source": "CRF", "title": "Latent Attention For If-Then Program Synthesis", "authors": ["Xinyun Chen", "Chang Liu", "Richard Shin", "Dawn Song", "Mingcheng Chen"], "emails": [], "sections": [{"heading": "1 Introduction", "text": "Over the past decade, there has been an increasing number of attempts to address this problem by both the natural language processing community and the programming language community. In this paper, we focus on a simple but important subset of programs that contain only an if-then explanation. An if-then program, also called a recipe, specifies a trigger and an action function that represents a program that takes action when the trigger condition is met. On websites like IFTTT.com, a user often provides a natural language description of the functionality of the recipe, as well as the latest work [16, 3, 7] examines the problem of the automatic synthesis of if-then programs from their descriptions. In particular, LSTM-based sequence-to-sequence approaches [7] and an approach of neural network appropriation and logistical regression [3] have been proposed to deal with this problem."}, {"heading": "2 If-Then Program Synthesis", "text": "In this paper, we look at an important class of simple programs called IfThen \"Recipes\" (or short recipes), which are very small programs for event-oriented automation of tasks. Specifically, a recipe consists of a trigger and an action, suggesting that the action is executed when the trigger is met. The simplicity of if-then recipes makes it a great tool for users who don't know how to code. Even non-technical users can specify their goals with recipes, rather than writing code in a more complete programming language. A number of websites have adopted the programming paradigm and are hugely successful with tens of thousands of personal recipes that have been created, including IFTTT.com and Zapier.com. In this paper, we focus on data cracked by IFTTT.com."}, {"heading": "3 Related Work", "text": "Recently, there has been an increasing interest in generating executable code. Existing work has examined how to generate domain-specific code, such as regular expressions [12], code for parsing input documents [14], database queries [22, 4], commands to robots [10], operating systems [5], smartphone automation [13], and tables [8]. A recent experiment is considering translating a mixed natural language and structured specification into programming code [15]. Most of these approaches are based on semantic parsing [19, 9, 1, 16]. In particular [16] introduces the problem of translating IFTTT descriptions into executable code and provides a semantic parsing-based approach. Two recent studies have examined approaches that use sequence-to-sequence model [7] and the interaction of a neural network and a logistic regression model [3] to deal with this problem, and a better performance [16]."}, {"heading": "4 Latent Attention Model", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "4.1 Motivation", "text": "In order to translate a description of the natural language into a program, we would like to find the words in the description that are most relevant for predicting the desired labels (triggers / action channels / functions). To capture this information, we can adjust the attention mechanism [2, 17] - first calculate the weight of each token's importance in the sentence and then output a weighted sum of the embeddings of those tokens. However, our intuition suggests that the weight of each token depends not only on the token itself, but also on the overall sentence structure. For example, in post photos in your Dropbox folder on Instagram, \"Dropbox\" determines the trigger, although in the previous example, which contains almost the same set of tokens, \"Instagram\" should play a certain role. In this example, the prepositions like \"indicate the trigger that attention is directed to this channel to determine the mean weight.\""}, {"heading": "4.2 The network", "text": "The latent attention layer is shown in Figure 1. We follow the convention of using lowercase letters to specify column vectors and uppercase letters for matrices. Our model takes as input a sequence of symbols x1,..., xJ, each of which comes from a dictionary of N-words. We denote X = [x1,..., xJ]. Here, J is the maximum length of a description. We illustrate each layer of the mesh using the latent attention layer. We assume that each symbol xi is encoded as a uniform vector of N-dimensions. We can embed the input sequence X in a d-dimensional embedding sequence using E = embedding size 1 (X). We will discuss various embedding methods in Section 4.3. Here, E is of size d \u00b7 J. The latent attention layer is calculated as the default soft max on Figure E."}, {"heading": "4.3 Details", "text": "We will consider two embedding methods for displaying words in vector space. The first is a simple word embedding, i.e., Embed\u03b8 (X) = \u03b8X, where \u03b8 is a d \u00b7 N matrix and the lines of X are a hot vector over the vocabulary of size N. We will point out that this simple method is effective enough to surpass some recent results [16, 7]. The other approach is to take the word embedding, guide it through a bi-directional LSTM (BDLSTM) [21], and then concatenate two LSTMs \"outputs at any time as embedding. We can consider the context around a token and thus include the beds, we should get the individual information from the embedding method."}, {"heading": "5 If-Then Program Synthesis Task Evaluation", "text": "This year it is as far as never before in the history of the city, where it is as far as never before that it is a place where it is a place, it is a place."}, {"heading": "6 One-Shot Learning", "text": "We look at the scenario when websites like IFTTT.com release new channels and features. In such a scenario, there will be very few recipes that use the newly available channels and features for a certain period of time; however, we want to enable the synthesis of if-then programs with these new features. The rarity of such recipes in the training set presents a challenge similar to the unique learning situation. In this scenario, we want to use the large amount of recipes for existing functions and the goal is to achieve good predictive accuracy for the new features without significantly affecting the overall accuracy."}, {"heading": "6.1 Datasets to simulate one-shot learning", "text": "To simulate this scenario with our existing dataset, we build two one-shot variants of it as follows: First, we divide the set of trigger functions into two sets based on their frequency; the Top100 set contains the 100 most commonly used trigger functions, while the Non-Top100 set contains the remainder. With a set of trigger functions S, we can create a distorted training set that contains all recipes with functions in S, and 10 randomly selected recipes for each function not in S. We refer to these two training sets that were created on S as (S, S), and refer to functions in S as majority functions and functions in S as minority functions. In our experiments, we construct two new training sets by selecting S as Top100 set and Non-Top100 set, respectively. We refer to these two training sets as SkewTop100 and SkewTop100. The motivation for creating these data sets is to mimic two different scenarios."}, {"heading": "6.2 Training", "text": "We evaluate three training methods as follows, the last of which is specifically designed for attention mechanisms. In all methods, the training data is either SkewTop100 or SkewNonTop100. Standard training. We do not modify the training process. We first do standard training. As the data is heavily distorted, the model can behave badly in terms of minority functions. From a training set (S, S), we create a newly balanced dataset by randomly selecting 10 recipes for each function in S and all recipes with functions in S. Therefore, the number of recipes using each function is similar in this newly balanced dataset. We recommend training on the basis of this newly balanced training dataset in the second step. We still do standard training first and then create the newly balanced dataset in a manner similar to the ones used in na\u00efve two-step training. However, in the second step, instead of the entire network, we keep the attention parameters fixed and train a small part of the second step, we only add the remaining one to the remaining one."}, {"heading": "6.3 Results", "text": "For reference, the best single BDLSTM + LA model can achieve an accuracy of 89.38% for the trigger function: 91.11% for the Top100 functions and 85.12% for the non-Top100 functions. We observe that \u2022 When using two-step training, both general accuracy and minority function accuracy are generally better than when using standard training and a two-step training. \u2022 Latent attention exceeds standard attention when using the same training method. \u2022 The best latent attention model (Dict + LA) for two-step training can achieve 82.71% and 64.84% accuracy for trigger function on the gold test set when exercising on the SkewTop100. \u2022 The best latent Attention Model (Dict + LA) for two-step training can achieve 82.71% and 64.84% accuracy for trigger function on the gold test set."}, {"heading": "7 Empirical Analysis of Latent Attention", "text": "We show some correctly classified and incorrectly classified examples in Figure 5 together with their attention weights. The weights are calculated using a Dict + LA model. We choose Dict + LA instead of BDLSTM + LA because the BDLSTM embedding of each token does not just correspond to the token itself - it will contain the information passed on by previous and subsequent tokens in order. Therefore, the attention of BDLSTM + LA is not as easy to interpret as Dict + LA. The latent weights are used to predict the action functions. In correctly classified examples, we observe that the latent weights are assigned to the prepositions that determine which parts of the set are associated with the trigger or action. An interesting example is (b), where a high latent weight is assigned to predict the action functions."}, {"heading": "A BDLSTM and attention model details", "text": "A.1 BDLSTM embeddingRecurrent neural networks are popular for natural language processing tasks because of their suitability for processing sequential data, Wxh and Whh are trained parameter matrices respectively of size m \u00b7 n and n \u00b7 n, and bh Rm is used as a bias. Long Short-Term Memory (LSTM) is an RNN variant which is better suited for learning long term dependencies. Although several versions of it have been described in the literature, we use the version in Zaremba et al. [21] and borrow their notation here: ifog = \u03c3tanhT2n, 4n (xtht \u2212 1) ct = f ct \u2212 i g ht = o tanh (ct).We is the sigd.J, the function and the memory, which is only overview: ifog = \u03c3tanhT2n, 4n (xtht \u2212 1) ct = f ct \u2212 1 (ct)."}, {"heading": "B Predicting Arguments", "text": "We offer a frequency-based method of predicting the function arguments as a baseline and show that these can dramatically exceed existing approaches when combined with our more powerful prediction of the function name. In particular, for each description we first predict the (trigger and action) functions ft, fa. For each function f, for each argument a, and for each possible argument value v we calculate the frequency with which f's argument a takes the value v. We designate this frequency as Pr (v | f, a). Our prediction is made by calculating argmaxvPr (v | f, a). Note that the prediction is entirely based on the predicted function f, without using any information from the description. We have found that for a given function some arguments do not occur in all recipes that use this function. In this case we give the value a special mark, < MISSING >; this is different from case 7, which does not have an argument as shown in the figure 7."}, {"heading": "C Data statistics and numerical results", "text": "In this section, we provide concrete data statistics and results. Statistics for the IFTTT data set that we evaluated are presented in Table 1. Numerical values corresponding to Figures 2, 3 and 7 are presented in Table 2. Statistics for the data used in one-shot learning are presented in Table 3. Numerical results corresponding to Figures 4a and 4b are presented in Table 4."}], "references": [], "referenceMentions": [], "year": 2016, "abstractText": "Automatic translation from natural language descriptions into programs is a long-<lb>standing challenging problem. In this work, we consider a simple yet impor-<lb>tant sub-problem: translation from textual descriptions to If-Then programs. We<lb>devise a novel neural network architecture for this task which we train end-to-<lb>end. Specifically, we introduce Latent Attention, which computes multiplicative<lb>weights for the words in the description in a two-stage process with the goal of<lb>better leveraging the natural language structures that indicate the relevant parts for<lb>predicting program elements. Our architecture reduces the error rate by 28.57%<lb>compared to prior art [3]. We also propose a one-shot learning scenario of If-Then<lb>program synthesis and simulate it with our existing dataset. We demonstrate a<lb>variation on the training procedure for this scenario that outperforms the original<lb>procedure, significantly closing the gap to the model trained with all data.", "creator": "LaTeX with hyperref package"}}}