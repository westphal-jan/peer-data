{"id": "1611.01874", "review": {"conference": "AAAI", "VERSION": "v1", "DATE_OF_SUBMISSION": "7-Nov-2016", "title": "Neural Machine Translation with Reconstruction", "abstract": "Although end-to-end Neural Machine Translation (NMT) has achieved remarkable progress in the past two years, it suffers from a major drawback: translations generated by NMT systems often lack of adequacy. It has been widely observed that NMT tends to repeatedly translate some source words while mistakenly ignoring other words. To alleviate this problem, we propose a novel encoder-decoder-reconstructor framework for NMT. The reconstructor, incorporated into the NMT model, manages to reconstruct the input source sentence from the hidden layer of the output target sentence, to ensure that the information in the source side is transformed to the target side as much as possible. Experiments show that the proposed framework significantly improves the adequacy of NMT output and achieves superior translation result over state-of-the-art NMT and statistical MT systems.", "histories": [["v1", "Mon, 7 Nov 2016 02:03:55 GMT  (1421kb,D)", "http://arxiv.org/abs/1611.01874v1", null], ["v2", "Mon, 21 Nov 2016 09:47:22 GMT  (1321kb,D)", "http://arxiv.org/abs/1611.01874v2", "Accepted by AAAI 2017"]], "reviews": [], "SUBJECTS": "cs.CL", "authors": ["zhaopeng tu", "yang liu", "lifeng shang", "xiaohua liu", "hang li"], "accepted": true, "id": "1611.01874"}, "pdf": {"name": "1611.01874.pdf", "metadata": {"source": "META", "title": "Neural Machine Translation with Reconstruction", "authors": ["Zhaopeng Tu", "Yang Liu", "Lifeng Shang", "Xiaohua Liu", "Hang Li"], "emails": ["tu.zhaopeng@huawei.com", "shang.lifeng@huawei.com", "liuxiaohua3@huawei.com", "hangli.hl@huawei.com", "liuyang2011@tsinghua.edu.cn"], "sections": [{"heading": "Introduction", "text": "This year it has come to the point that it will be able to retaliate, \"he said in an interview with the German Press Agency.\" We have never waited so long to be able to retaliate, \"he said."}, {"heading": "Background", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "Encoder-Decoder based NMT", "text": "Given a source sentence x = x1,.. xj,. xJ and a target sentence y = y1,.. yi,.. yI, end-to-end NMT, the translation probability is directly modelled word by word: P (y | x) = 1 P (yi | y < i, x; \u03b8) (1), where \u03b8 is the model parameter and y < i = y1,.. yi \u2212 1 is partial translation. The prediction of the i-th target is generally made in an encoder decoder frame: P (yi | y < i, x;.)."}, {"heading": "Beam Likelihood + Normalization", "text": "It is important to introduce an auxiliary target to measure the adequacy of the translation, which complements the probability."}, {"heading": "Reconstruction in Auto-Encoder", "text": "Reconstruction is a standard concept in the auto encoder that is normally implemented through a feed-forward network (Bourlard and Kamp 1988; Vincent et al. 2010; Socher et al. 2011).The model consists of a coding function to calculate a representation from an input and a decoding function to reconstruct the input from the input.The parameters accompanying the two functions are trained to maximize the reconstruction score, which measures the similarity between the original input and the reconstructed input.The basic idea of our approach is to reconstruct the source set from the latent representations of the decoder and use the reconstruction score as the appropriate yardstick. Similar to the auto encoder, our approach also learns that the target sentence cannot be represented as latent on the target page."}, {"heading": "Approach", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "Architecture", "text": "Specifically, we are based on an approach that is generally applicable to all other types of NMT architectures, such as the sequence-to-sequence model (Sutskever et al. 2014). The model architecture shown in Figure 2 consists of two components: \u2022 The standard encoder decoder reads the input sentence and outputs its translation together with the sequence-to-sequence model (Sutskever et al. 2014). \u2022 The added reconstructor reads the hidden state sequence from the decoder and outputs a score that accurately reconstructs the input set that we will describe below. Reconstructor As shown in Figure 2, the reconstructor reconstructs the hidden state sequence from the decoder."}, {"heading": "Training", "text": "Formally, we train both the encoder decoder P (y | x; \u03b8) and the reconstructor R (x | s; \u03b3) on the basis of a series of training examples {[xn, yn]} Nn = 1, where s is the state sequence in the decoder after the generation of y, \u03b8 and \u03b3 are model parameters in the encoder decoder or reconstructor respectively. The new training objective is: J (\u03b8, \u03b3) = argmax \u03b8, \u03b3 N-N = 1 {logP (yn | xn; \u03b8) probability + \u03bb logR (xn | sn; \u03b3) reconstruction} (7), where \u03bb is a hyperparameter that balances the preference between probability and reconstruction. Note that the goal consists of two parts: probability measures the translation flow and reconstruction measures the translation adequacy. It is clear that the combined goal is more compatible with the goal of improving the overall quality and reconstruction."}, {"heading": "Testing", "text": "Once a model is trained, we use a beam search to find a translation that approximates both the probability and the reconstruction value. As shown in Figure 3, when using an input sentence, a two-phase scheme is used: 1 The standard encoder decoder generates a series of translation candidates, each of which produces a triple of a translation candidate, its associated hidden layer on the target page, and its probability value P.2 For each translation candidate, the reconstructor reads its associated hidden layer on the target page and issues an auxiliary reconstruction score R. Linear interpolation of the probability P and reconstruction score R generate an overall score that is used to select the final translation.1In the test, reconstruction works as anchor technique to select a better translation from the k-best candidates generated by the decoder.Experiments"}, {"heading": "Setup", "text": "We conduct experiments with Chinese-English translation. The training data set consists of 1.25M set pairs extracted from LDC corpora, with 27.9M Chinese words and 34.5m English words, respectively. We choose the data set NIST 2002 (MT02) as validation set and the data sets NIST 2005 (MT05), 2006 (MT06) and 2008 (MT08) as test sets. We compare our method with state-of-the-art SMT and NMT models: \u2022 MOSES (Koehn et al. 2007): a source-based translation system with standard configuration and signal test (Collins et al. 2005) for statistical significance tests. We compare our method with state-of-the-art SMT and NMT models: \u2022 MOSES et al. 2007: an open source translation system based on phrases with standard configuration and signal test (Collins et al. 2005) for statistical significance tests. We compare our method with state-of-the-art SMT and NMT models: \u2022 MOSES et al. 2007: an open source translation system with standard configuration and signal test (Collins et al. 2005) for statistical significance tests. We compare our method with state-of the latest SMT and NMT models: \u2022 MOSES et al. 2007: an open source translation system with standard configuration and signal test (Collins et al. 2005) for statistical significance tests."}, {"heading": "Correlation between Reconstruction and Adequacy", "text": "In the first experiment, we examine the validity of our assumption that the reconstruction value correlates well with the adequacy of the translation, which is the underlying assumption of the approach. We perform a subjective evaluation: Two human evaluators are asked to evaluate the translations of 200 random source sentences taken from the test sentences. We1Interpolation weight \u03bb in the test is the same as in training. Calculate Pearson correlation between the reconstruction values and the corresponding appropriateness and fluid values in the samples, as in Table 2. Two evaluators provide similar results: The reconstruction value is more related to the adequacy of the translation than to the fluid."}, {"heading": "Effect of Reconstruction on Translation", "text": "In this experiment, we study the influence of reconstruction on translation performance over time, as measured in BLEU values on the validation set. In reconstruction, we use the reconstructor to stochastically generate one source sentence for each translation 2 and calculate the BLEU value of the reconstructed input with reference to the original input. Generally, as shown in Figure 4, the BLEU value of the translation increases with the improvement of the reconstruction over time. Translation performance peaks at 110K iteration when the model reaches an equilibrium between probability and reconstruction value. Therefore, we use the trained model in the 110K iteration in the following experiments."}, {"heading": "Effect of Reconstruction in Large Decoding Space", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "Beam Likelihood +Reconstruction", "text": "To answer this question, we examine the effects of reconstruction on different beam sizes k, as shown in Table 3. Our approach can actually solve the problem: Increasing the decoding space generally leads to an improvement in the BLEU score. We attribute this to the ability of the combined target to measure both the fluctuation and adequacy of translation candidates. There is a significant gap between k = 10 and k = 100. However, maintaining the increase in k does not lead to significant improvements in translation accuracy, but significantly reduces decoding efficiency. Therefore, in the following experiments, we set the maximum value of k to 100 and use the normalized probability for k = 100 when we do not use reconstruction in the test."}, {"heading": "Main Results", "text": "Table 4 shows the translation performance of the test sets measured in the BLEU score. RNNSEARCH performs significantly better on average than Moses by 2.2 BLEU points, indicating that it is a strong NMT base system. This is mainly due to the introduction of two advanced techniques. Increasing the beam size leads to a reduction in the translation performance on the test sets, which is consistent with the result on the validation set. In the analysis below, we compare our methods with \"RNSEARCH (beam = 10),\" as it performs best in the base systems. First, the introduction of reconstruction significantly improves performance over the base system by 1.1 BLEU points with the beam size k = 10. Above all, we achieve a further improvement of 1.2 BLEU points in expanding the deciphering space. Secondly, our approach also consistently improves the quality (with regard to the Oracle score, see last column) of the best translation candidates confirms that our combined translation system is the best one of the various translation candidates."}, {"heading": "Analysis", "text": "We conduct extensive analysis to better understand our model in terms of the contribution of reconstruction from training and testing, to alleviate typical translation problems and to improve the ability to process long sentences. Contribution analysis The contribution of reconstruction consists of two parts: (1) enabling parameter training to generate better translation candidates and (2) enabling better recruitment of generated candidates in the test. Table 5 lists the improvements from the two sources of contribution. If reconstruction is applied only in training, it improves translation performance by generating fluent and adequate translation candidates. In addition, reconstruction-based ranking further improves performance. The improvements are more significant when the decoding spaces increase."}, {"heading": "Model Under-Tran. Over-Tran.", "text": "Problem Analysis We then perform a subjective evaluation to examine the benefit of including the reconstruction using randomly selected 200 sentences. Table 6 shows the results of subjective evaluation of the translation. RNSEARCH suffers from serious problems of under- or translation, which is in line with the results of other work (Tu et al. 2016b). Including the reconstruction significantly alleviates these problems and reduces 11.0% or 38.5% of under- or translation errors, respectively. The main reason for this is that both the under- and translation result in lower reconstruction values and are therefore penalized by the reconstruction target. As a result, the respective candidate is less likely to be selected as the final translation. Length Analysis According to Bahdanau et al. (2015) we group sentences of similar length and translation and calculate the BLEU score for each group, as shown in Figure 5."}, {"heading": "Comparison with Previous Work", "text": "We implement the methods of Tu et al. (2016b; 2016a) on RNNSEARCH. For the coverage mechanism (Tu et al. 2016b), we use the neural network coverage, and the coverage dimension is 100. For the context gates (Tu et al. 2016a), we apply them on both the source and the target side. Table 7 lists the comparison results. Coverage mechanisms and context types significantly improve translation performance individually, and their combination achieves further improvement. This is in line with the results in (Tu et al. 2016b; 2016a). Our model consistently improves translation performance when further combined with the models."}, {"heading": "Related Work", "text": "Our work is inspired by research to improve NMT by: Enhancing Translation Adequacy Recently, several papers have shown that NMT prefers fluent but insufficient translations (Tu et al. 2016b; 2016a). Although all work is aimed at improving the adequacy of NMT, our approach is complementary: the above work is still below the standard encoder decoder framework, while we propose a novel encoder-reconstructor framework. Experiments show that combining these models can further improve translation performance. Improving the NMT models uses a simple beam search algorithm to generate the translation word by word. Several researchers use word candidates with additional features, such as language model probability (Gulcehre et al al al. 2015) and SMT features (He et al. 2016; Stahlberg et al. 2016) to realize the translation models."}, {"heading": "Conclusion", "text": "We propose a novel encoder-decoder-reconstructor framework for NMT in which the newly added reconstructor introduces an auxiliary score to measure the adequacy of translation candidates. The advantage of the proposed approach is twofold: firstly, it improves parameter training to produce better translation candidates; secondly, it consistently improves translation performance when the decoding space increases while traditional NMT does not. Experimental results show that these two benefits can actually help our approach consistently improve translation performance.There is still a significant gap between the de facto translation and the oracle of the k-best translation candidates, especially when the decoding space increases."}], "references": [{"title": "Neural machine translation by jointly learning to align and translate", "author": ["D. Bahdanau", "K. Cho", "Y. Bengio"], "venue": "ICLR 2015.", "citeRegEx": "Bahdanau et al\\.,? 2015", "shortCiteRegEx": "Bahdanau et al\\.", "year": 2015}, {"title": "Neural versus Phrase-Based Machine Translation Quality: a Case Study", "author": ["L. Bentivogli", "A. Bisazza", "M. Cettolo", "M. Federico"], "venue": "EMNLP 2016.", "citeRegEx": "Bentivogli et al\\.,? 2016", "shortCiteRegEx": "Bentivogli et al\\.", "year": 2016}, {"title": "Auto-association by multilayer perceptrons and singular value decomposition", "author": ["H. Bourlard", "Y. Kamp"], "venue": "Biological Cybernetics 59(4-5):291\u2013294.", "citeRegEx": "Bourlard and Kamp,? 1988", "shortCiteRegEx": "Bourlard and Kamp", "year": 1988}, {"title": "The mathematics of statistical machine translation: Parameter estimation", "author": ["P.E. Brown", "S.A.D. Pietra", "V.J.D. Pietra", "R.L. Mercer"], "venue": "Computational Linguistics 19(2):263\u2013311.", "citeRegEx": "Brown et al\\.,? 1993", "shortCiteRegEx": "Brown et al\\.", "year": 1993}, {"title": "Agreement-based joint training for bidirectional attention-based neural machine translation", "author": ["Y. Cheng", "S. Shen", "Z. He", "W. He", "H. Wu", "M. Sun", "Y. Liu"], "venue": "IJCAI 2016.", "citeRegEx": "Cheng et al\\.,? 2016a", "shortCiteRegEx": "Cheng et al\\.", "year": 2016}, {"title": "Semi-Supervised Learning for Neural Machine Translation", "author": ["Y. Cheng", "W. Xu", "Z. He", "W. He", "H. Wu", "M. Sun", "Y. Liu"], "venue": "ACL 2016.", "citeRegEx": "Cheng et al\\.,? 2016b", "shortCiteRegEx": "Cheng et al\\.", "year": 2016}, {"title": "Learning phrase representations using rnn encoder-decoder for statistical machine translation", "author": ["K. Cho", "B. van Merrienboer", "C. Gulcehre", "F. Bougares", "H. Schwenk", "Y. Bengio"], "venue": null, "citeRegEx": "Cho et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Cho et al\\.", "year": 2014}, {"title": "Clause restructuring for statistical machine translation", "author": ["M. Collins", "P. Koehn", "I. Ku\u010derov\u00e1"], "venue": "ACL 2005.", "citeRegEx": "Collins et al\\.,? 2005", "shortCiteRegEx": "Collins et al\\.", "year": 2005}, {"title": "On Using Monolingual Corpora in Neural Machine Translation", "author": ["C. Gulcehre", "O. Firat", "K. Xu", "K. Cho", "L. Barrault", "H.-C. Lin", "F. Bougares", "H. Schwenk", "Y. Bengio"], "venue": "arXiv.", "citeRegEx": "Gulcehre et al\\.,? 2015", "shortCiteRegEx": "Gulcehre et al\\.", "year": 2015}, {"title": "Improved neural machine translation with smt features", "author": ["W. He", "Z. He", "H. Wu", "H. Wang"], "venue": "AAAI 2016.", "citeRegEx": "He et al\\.,? 2016", "shortCiteRegEx": "He et al\\.", "year": 2016}, {"title": "Improving neural networks by preventing co-adaptation of feature detectors", "author": ["G.E. Hinton", "N. Srivastava", "A. Krizhevsky", "I. Sutskever", "R.R. Salakhutdinov"], "venue": "arXiv:1207.0580.", "citeRegEx": "Hinton et al\\.,? 2012", "shortCiteRegEx": "Hinton et al\\.", "year": 2012}, {"title": "Long short-term memory", "author": ["S. Hochreiter", "J. Schmidhuber"], "venue": "Neural Computation.", "citeRegEx": "Hochreiter and Schmidhuber,? 1997", "shortCiteRegEx": "Hochreiter and Schmidhuber", "year": 1997}, {"title": "Recurrent continuous translation models", "author": ["N. Kalchbrenner", "P. Blunsom"], "venue": "EMNLP 2013.", "citeRegEx": "Kalchbrenner and Blunsom,? 2013", "shortCiteRegEx": "Kalchbrenner and Blunsom", "year": 2013}, {"title": "Statistical phrasebased translation", "author": ["P. Koehn", "F.J. Och", "D. Marcu"], "venue": "NAACL 2003.", "citeRegEx": "Koehn et al\\.,? 2003", "shortCiteRegEx": "Koehn et al\\.", "year": 2003}, {"title": "Moses: open source toolkit for statistical machine translation", "author": ["P. Koehn", "H. Hoang", "A. Birch", "C. Callison-Burch", "M. Federico", "N. Bertoldi", "B. Cowan", "W. Shen", "C. Moran", "R. Zens", "C. Dyer", "O. Bojar", "A. Constantin", "E. Herbst"], "venue": "ACL 2007.", "citeRegEx": "Koehn et al\\.,? 2007", "shortCiteRegEx": "Koehn et al\\.", "year": 2007}, {"title": "Mutual information and diverse decoding improve neural machine translation", "author": ["J. Li", "D. Jurafsky"], "venue": "NAACL 2016.", "citeRegEx": "Li and Jurafsky,? 2016", "shortCiteRegEx": "Li and Jurafsky", "year": 2016}, {"title": "Alignment by agreement", "author": ["P. Liang", "B. Taskar", "D. Klein"], "venue": "NAACL 2006. Luong, M.-T., and Manning, C. D. 2015. Stanford neural machine translation systems for spoken language domains. In IWSLT 2015.", "citeRegEx": "Liang et al\\.,? 2006", "shortCiteRegEx": "Liang et al\\.", "year": 2006}, {"title": "Effective approaches to attention-based neural machine translation", "author": ["M.-T. Luong", "H. Pham", "C.D. Manning"], "venue": "EMNLP 2015.", "citeRegEx": "Luong et al\\.,? 2015", "shortCiteRegEx": "Luong et al\\.", "year": 2015}, {"title": "Bleu: a method for automatic evaluation of machine translation", "author": ["K. Papineni", "S. Roukos", "T. Ward", "W.-J. Zhu"], "venue": "ACL 2002.", "citeRegEx": "Papineni et al\\.,? 2002", "shortCiteRegEx": "Papineni et al\\.", "year": 2002}, {"title": "Minimum Risk Training for Neural Machine Translation", "author": ["S. Shen", "Y. Cheng", "Z. He", "W. He", "H. Wu", "M. Sun", "Y. Liu"], "venue": "ACL 2016.", "citeRegEx": "Shen et al\\.,? 2016", "shortCiteRegEx": "Shen et al\\.", "year": 2016}, {"title": "Semi-supervised recursive autoencoders for predicting sentiment distributions", "author": ["R. Socher", "J. Pennington", "E.H. Huang", "A.Y. Ng", "C.D. Manning"], "venue": "EMNLP 2011.", "citeRegEx": "Socher et al\\.,? 2011", "shortCiteRegEx": "Socher et al\\.", "year": 2011}, {"title": "Syntactically Guided Neural Machine Translation", "author": ["F. Stahlberg", "E. Hasler", "A. Waite", "B. Byrne"], "venue": "arxiv.org.", "citeRegEx": "Stahlberg et al\\.,? 2016", "shortCiteRegEx": "Stahlberg et al\\.", "year": 2016}, {"title": "Sequence to sequence learning with neural networks", "author": ["I. Sutskever", "O. Vinyals", "Q.V. Le"], "venue": "NIPS 2014.", "citeRegEx": "Sutskever et al\\.,? 2014", "shortCiteRegEx": "Sutskever et al\\.", "year": 2014}, {"title": "Context Gates for Neural Machine Translation", "author": ["Z. Tu", "Y. Liu", "Z. Lu", "X. Liu", "H. Li"], "venue": "arXiv.", "citeRegEx": "Tu et al\\.,? 2016a", "shortCiteRegEx": "Tu et al\\.", "year": 2016}, {"title": "Modeling Coverage for Neural Machine Translation", "author": ["Z. Tu", "Z. Lu", "Y. Liu", "X. Liu", "H. Li"], "venue": "ACL 2016.", "citeRegEx": "Tu et al\\.,? 2016b", "shortCiteRegEx": "Tu et al\\.", "year": 2016}, {"title": "Stacked denoising autoencoders: Learning useful representations in a deep network with a local denoising criterion", "author": ["P. Vincent", "H. Larochelle", "I. Lajoie", "Y. Bengio", "P.-A. Manzagol"], "venue": "Journal of Machine Learning Research 11(Dec):3371\u20133408.", "citeRegEx": "Vincent et al\\.,? 2010", "shortCiteRegEx": "Vincent et al\\.", "year": 2010}, {"title": "ADADELTA: an adaptive learning rate method", "author": ["M.D. Zeiler"], "venue": "arXiv.", "citeRegEx": "Zeiler,? 2012", "shortCiteRegEx": "Zeiler", "year": 2012}], "referenceMentions": [{"referenceID": 12, "context": "Past several years have observed a significant progress in Neural Machine Translation (NMT) (Kalchbrenner and Blunsom 2013; Cho et al. 2014; Sutskever et al. 2014; Bahdanau et al. 2015).", "startOffset": 92, "endOffset": 185}, {"referenceID": 6, "context": "Past several years have observed a significant progress in Neural Machine Translation (NMT) (Kalchbrenner and Blunsom 2013; Cho et al. 2014; Sutskever et al. 2014; Bahdanau et al. 2015).", "startOffset": 92, "endOffset": 185}, {"referenceID": 22, "context": "Past several years have observed a significant progress in Neural Machine Translation (NMT) (Kalchbrenner and Blunsom 2013; Cho et al. 2014; Sutskever et al. 2014; Bahdanau et al. 2015).", "startOffset": 92, "endOffset": 185}, {"referenceID": 0, "context": "Past several years have observed a significant progress in Neural Machine Translation (NMT) (Kalchbrenner and Blunsom 2013; Cho et al. 2014; Sutskever et al. 2014; Bahdanau et al. 2015).", "startOffset": 92, "endOffset": 185}, {"referenceID": 1, "context": "Particularly, NMT has significantly enhanced the performance of translation between a language pair involving rich morphology prediction and/or significant word reordering (Luong and Manning 2015; Bentivogli et al. 2016).", "startOffset": 172, "endOffset": 220}, {"referenceID": 11, "context": "Long Short-Term Memory (Hochreiter and Schmidhuber 1997) enables NMT to conduct longdistance reordering, which is a significant challenge for Statistical Machine Translation (SMT) (Brown et al.", "startOffset": 23, "endOffset": 56}, {"referenceID": 3, "context": "Long Short-Term Memory (Hochreiter and Schmidhuber 1997) enables NMT to conduct longdistance reordering, which is a significant challenge for Statistical Machine Translation (SMT) (Brown et al. 1993; Koehn et al. 2003).", "startOffset": 180, "endOffset": 218}, {"referenceID": 13, "context": "Long Short-Term Memory (Hochreiter and Schmidhuber 1997) enables NMT to conduct longdistance reordering, which is a significant challenge for Statistical Machine Translation (SMT) (Brown et al. 1993; Koehn et al. 2003).", "startOffset": 180, "endOffset": 218}, {"referenceID": 24, "context": "repeatedly selects some parts of the source sentence while ignoring other parts, which leads to over-translation and under-translation (Tu et al. 2016b).", "startOffset": 135, "endOffset": 152}, {"referenceID": 15, "context": "The main reason is that likelihood only captures unidirectional dependency from source to target, which does not correlate well with translation adequacy (Li and Jurafsky 2016; Shen et al. 2016).", "startOffset": 154, "endOffset": 194}, {"referenceID": 19, "context": "The main reason is that likelihood only captures unidirectional dependency from source to target, which does not correlate well with translation adequacy (Li and Jurafsky 2016; Shen et al. 2016).", "startOffset": 154, "endOffset": 194}, {"referenceID": 22, "context": "Please refer to (Sutskever et al. 2014; Bahdanau et al. 2015) for more details.", "startOffset": 16, "endOffset": 61}, {"referenceID": 0, "context": "Please refer to (Sutskever et al. 2014; Bahdanau et al. 2015) for more details.", "startOffset": 16, "endOffset": 61}, {"referenceID": 2, "context": "Reconstruction in Auto-Encoder Reconstruction is a standard concept in auto-encoder, which is usually realized by a feed forward network (Bourlard and Kamp 1988; Vincent et al. 2010; Socher et al. 2011).", "startOffset": 137, "endOffset": 202}, {"referenceID": 25, "context": "Reconstruction in Auto-Encoder Reconstruction is a standard concept in auto-encoder, which is usually realized by a feed forward network (Bourlard and Kamp 1988; Vincent et al. 2010; Socher et al. 2011).", "startOffset": 137, "endOffset": 202}, {"referenceID": 20, "context": "Reconstruction in Auto-Encoder Reconstruction is a standard concept in auto-encoder, which is usually realized by a feed forward network (Bourlard and Kamp 1988; Vincent et al. 2010; Socher et al. 2011).", "startOffset": 137, "endOffset": 202}, {"referenceID": 0, "context": "More specifically, we base our approach on top of attention-based NMT (Bahdanau et al. 2015; Luong et al. 2015), which will be used as baseline in the experiments later.", "startOffset": 70, "endOffset": 111}, {"referenceID": 17, "context": "More specifically, we base our approach on top of attention-based NMT (Bahdanau et al. 2015; Luong et al. 2015), which will be used as baseline in the experiments later.", "startOffset": 70, "endOffset": 111}, {"referenceID": 22, "context": "We note that the proposed approach is generally applicable to any other type of NMT architectures, such as the sequence-to-sequence model (Sutskever et al. 2014).", "startOffset": 138, "endOffset": 161}, {"referenceID": 18, "context": "We use the case-insensitive 4-gram NIST BLEU score (Papineni et al. 2002) as evaluation metric, and signtest (Collins et al.", "startOffset": 51, "endOffset": 73}, {"referenceID": 7, "context": "2002) as evaluation metric, and signtest (Collins et al. 2005) for statistical significance test.", "startOffset": 41, "endOffset": 62}, {"referenceID": 14, "context": "\u2022 MOSES (Koehn et al. 2007): an open source phrasebased translation system with default configuration and a 4-gram language model trained on the target portion of training data.", "startOffset": 8, "endOffset": 27}, {"referenceID": 10, "context": "\u2022 RNNSEARCH: our re-implemented attention-based NMT system, which incorporates dropout (Hinton et al. 2012) on the output layer and improves the attention model by feeding the lastly generated word.", "startOffset": 87, "endOffset": 107}, {"referenceID": 26, "context": "We train for 15 epochs using Adadelta (Zeiler 2012).", "startOffset": 38, "endOffset": 51}, {"referenceID": 24, "context": "RNNSEARCH suffers from serious under-translation and over-translation problems, which is consistent with the finding in other work (Tu et al. 2016b).", "startOffset": 131, "endOffset": 148}, {"referenceID": 1, "context": "Specifically, RNNSEARCH outperforms Moses on all sentence segments, while its performance degrades faster than its competitors, which is consistent with the finding in (Bentivogli et al. 2016).", "startOffset": 168, "endOffset": 192}, {"referenceID": 0, "context": "Length Analysis Following Bahdanau et al. (2015), we group sentences of similar lengths together and compute the BLEU score for each group, as shown in Figure 5.", "startOffset": 26, "endOffset": 49}, {"referenceID": 24, "context": "mainly due to that RNNSEARCH seriously suffers from inadequate translations on long sentences (Tu et al. 2016b).", "startOffset": 94, "endOffset": 111}, {"referenceID": 24, "context": "\u201d denotes coverage mechanism to keep track of the attention history (Tu et al. 2016b), and \u201cCtx.", "startOffset": 68, "endOffset": 85}, {"referenceID": 23, "context": "\u201d denotes context gate to dynamically control the ratios at which source and target contexts contribute to the generation of target words (Tu et al. 2016a).", "startOffset": 138, "endOffset": 155}, {"referenceID": 24, "context": "For the coverage mechanism (Tu et al. 2016b), we use the neural network based coverage, and the coverage dimension is 100.", "startOffset": 27, "endOffset": 44}, {"referenceID": 23, "context": "For the context gates (Tu et al. 2016a), we apply them on both source and target sides.", "startOffset": 22, "endOffset": 39}, {"referenceID": 24, "context": "This is consistent with the results in (Tu et al. 2016b; 2016a).", "startOffset": 39, "endOffset": 63}, {"referenceID": 24, "context": "Enhancing Translation Adequacy Recently, several work shows that NMT favors fluent but inadequate translations (Tu et al. 2016b; 2016a).", "startOffset": 111, "endOffset": 135}, {"referenceID": 8, "context": "Several researchers rescore word candidates with additional features, such as language model probability (Gulcehre et al. 2015) and SMT features (He et al.", "startOffset": 105, "endOffset": 127}, {"referenceID": 9, "context": "2015) and SMT features (He et al. 2016; Stahlberg et al. 2016).", "startOffset": 23, "endOffset": 62}, {"referenceID": 21, "context": "2015) and SMT features (He et al. 2016; Stahlberg et al. 2016).", "startOffset": 23, "endOffset": 62}, {"referenceID": 8, "context": "Several researchers rescore word candidates with additional features, such as language model probability (Gulcehre et al. 2015) and SMT features (He et al. 2016; Stahlberg et al. 2016). In contrast, Li and Jurafsky (2016) rescore translation candidates on sentence-level with the mutual information between source and target sides.", "startOffset": 106, "endOffset": 222}, {"referenceID": 16, "context": "It has been shown that combination of two directional models outperforms each model alone (Liang et al. 2006; Cheng et al. 2016a; Cheng et al. 2016b).", "startOffset": 90, "endOffset": 149}, {"referenceID": 4, "context": "It has been shown that combination of two directional models outperforms each model alone (Liang et al. 2006; Cheng et al. 2016a; Cheng et al. 2016b).", "startOffset": 90, "endOffset": 149}, {"referenceID": 5, "context": "It has been shown that combination of two directional models outperforms each model alone (Liang et al. 2006; Cheng et al. 2016a; Cheng et al. 2016b).", "startOffset": 90, "endOffset": 149}, {"referenceID": 4, "context": "2006; Cheng et al. 2016a; Cheng et al. 2016b). Among them, Cheng et al. (2016b) reconstruct the monolingual corpora with two separate source-to-target and target-to-source NMT models.", "startOffset": 6, "endOffset": 80}, {"referenceID": 4, "context": "2006; Cheng et al. 2016a; Cheng et al. 2016b). Among them, Cheng et al. (2016b) reconstruct the monolingual corpora with two separate source-to-target and target-to-source NMT models. Closely related to Cheng et al. (2016b), our approach aims at enhancing adequacy of unidirectional (i.", "startOffset": 6, "endOffset": 224}], "year": 2016, "abstractText": "Although end-to-end Neural Machine Translation (NMT) has achieved remarkable progress in the past two years, it suffers from a major drawback: translations generated by NMT systems often lack of adequacy. It has been widely observed that NMT tends to repeatedly translate some source words while mistakenly ignoring other words. To alleviate this problem, we propose a novel encoder-decoder-reconstructor framework for NMT. The reconstructor, incorporated into the NMT model, manages to reconstruct the input source sentence from the hidden layer of the output target sentence, to ensure that the information in the source side is transformed to the target side as much as possible. Experiments show that the proposed framework significantly improves the adequacy of NMT output and achieves superior translation result over state-of-theart NMT and statistical MT systems.", "creator": "TeX"}}}