{"id": "1611.01929", "review": {"conference": "ICML", "VERSION": "v1", "DATE_OF_SUBMISSION": "7-Nov-2016", "title": "Averaged-DQN: Variance Reduction and Stabilization for Deep Reinforcement Learning", "abstract": "The commonly used Q-learning algorithm combined with function approximation induces systematic overestimations of state-action values. These systematic errors might cause instability, poor performance and sometimes divergence of learning. In this work, we present the \\textsc{Averaged Target DQN} (ADQN) algorithm, an adaptation to the DQN class of algorithms which uses a weighted average over past learned networks to reduce generalization noise variance. As a consequence, this leads to reduced overestimations, more stable learning process and improved performance. Additionally, we analyze ADQN variance reduction along trajectories and demonstrate the performance of ADQN on a toy Gridworld problem, as well as on several of the Atari 2600 games from the Arcade Learning Environment.", "histories": [["v1", "Mon, 7 Nov 2016 08:12:53 GMT  (607kb,D)", "http://arxiv.org/abs/1611.01929v1", null], ["v2", "Tue, 8 Nov 2016 08:40:02 GMT  (606kb,D)", "http://arxiv.org/abs/1611.01929v2", null], ["v3", "Wed, 8 Mar 2017 13:50:38 GMT  (4177kb,D)", "http://arxiv.org/abs/1611.01929v3", null], ["v4", "Fri, 10 Mar 2017 09:52:52 GMT  (2087kb,D)", "http://arxiv.org/abs/1611.01929v4", null]], "reviews": [], "SUBJECTS": "cs.AI cs.LG stat.ML", "authors": ["oron anschel", "nir baram", "nahum shimkin"], "accepted": true, "id": "1611.01929"}, "pdf": {"name": "1611.01929.pdf", "metadata": {"source": "CRF", "title": "Deep Reinforcement Learning with Averaged Target DQN", "authors": ["Oron Anschel", "Nir Baram", "Nahum Shimkin"], "emails": ["{oronanschel@campus,", "nirb@campus,", "shimkin@ee}.technion.ac.il"], "sections": [{"heading": "1 Introduction", "text": "In fact, the fact is that most of them will be able to move around without being able to achieve their goals."}, {"heading": "2 Related Work", "text": "Thrun and Schwartz (1993) is an early theoretical paper analyzing the effects of the combination of function approximation and RL techniques. This paper models the generalization noise caused by the use of approximator as a zero-medium independent random variable and shows that systematic overestimation errors may occur due to the use of the max operator in the standard Q-Learning update rule. Double Q-Learning (Hasselt, 2010) was proposed as a solution to this problem. Max operator was divided into selection and evaluation parts that replace the positive bias of the original predictor with a negative one. Later, the Double-DQN (DQN) algorithm was developed, which provided a DQN implementation of this algorithm in Van Hasselt et al. (2015). Since the publication of the DQN thesis, many variants and modifications of the standard algorithm have been developed."}, {"heading": "3 Deep Q Networks (DQN)", "text": "In classical Q-Learning (Watkins and Dayan (1992), the update rule is given by Qt + 1 (s, a) (1 \u2212 \u0445) Qt (s, a) + \u03b1 (r, a) + \u03b3max a \"Qt (s, a) (1), where s\" is the resulting state after the application of action a in the current state s, \u03b3 is the discount factor of the MDP and \u03b1 is a learning rate that varies over time. Assuming, mildly, that the Q-Learning algorithm agrees with the optimal action value function as long as each state action pair is updated infinitely, the update of the Q function estimate can be performed online or offline. The above Q-Learning Update rule can be implemented directly into a neural network. As no direct assignment of Q-values can be performed, Qycent is a computational task."}, {"heading": "4 Averaged Target DQN (ADQN)", "text": "Essentially, the DQN algorithm can be regarded as a special case of the ADQN algorithm with a buffer of size One. The only algorithmic difference to the DQN algorithm is the different choice of the target value as in Eq.5. The loss function (Eq.6) remains the same as in Eq.1. Likewise, the parameter update equation (Eq.7): yAt = Es \u2032 B [r, a) + quadratic maxa \u2032 K \u2212 1 square k = 0 wkQ (s \u2032, a \u2032; quadratic equation (bt / percc \u2212 k) and the parameter update (Eq.7): yAt = Es \u00b7 quadratic B \u2212 kquadratic deviation k = 0 wkQ (s, a \u2032 quadratic deviation) and quadratic deviation (square / quadratic deviation)."}, {"heading": "5 Estimation Errors in RL", "text": "Thrun and Schwartz (1993) provided a first theoretical analysis on combing Q-learning and function approximations. In this section we will discuss their results and propose further analysis of variance propagation along trajectories."}, {"heading": "5.1 Overestimation", "text": "Thrun and Schwartz (1993) considered an additive zero-mean random variable, a generalization error model for the Q function. That is, Q (s), a) = Qtarget (s) + Z, \"a\" in Equation 2. Due to the max operator in the equation, an overestimate of up to \u03b3 n \u2212 1n + 1 can be observed in each target action value y Q t (s, a) on average (where Zs, a is evenly distributed in an interval and n is the number of actions applicable in state s). Systematic errors occur due to the nature of the Q learning update equations, where along the paths of current greedy policies each state can be overestimated up to \u0433n \u2212 1n + 1 plus the bias of the next state in the orbit. Hasselt (2010) argued that even in a tabular setting environmental noise can lead to overestimates and recommended that the double Q function has a major impact on the Q function."}, {"heading": "5.2 Variance Reduction due to Averaging", "text": "In the following illustrative example, we analyze the noise variance propagation along paths Q = Q = Q networks after the use of QQN, DQN and an Ent technique, which will be introduced later in this section. We model the noise in the learned Q function, in which the indexes of points in time are copied from the learned network to the target buffer (DQN). We model the noise in the learned Q function, in Eq. 2 or 5 as Q (s, \"a,\" a, \"a\"), in which the parameters from the learned network are copied into the target buffer (s, \"a,\" a, \"a,\" a, \"a,\" a, \"a,\" a, \"a,\" b, \"b...,\" b, \"b...,\" b, \"b...,\" b, \"b...\", \"b,\" b... \",\" b, \"...\", \"b,\" b, \"b,\" b, \"...\", \"b."}, {"heading": "6 Experiments", "text": "We use two different environments to evaluate the performance of the ADQN algorithm and some of the assumptions made in previous sections. First, we show experiments on a toy problem from Gridworld where the optimal value function can be calculated precisely using a tabular representation of the state space. Second, we demonstrate and compare the performance with the Arcade Learning Environment (Bellemare et al. (2013)), the most commonly used benchmark environment for RL algorithms."}, {"heading": "6.1 Gridworld", "text": "In this experiment on the problem of the 2D network world, the state space contains pairs of points from a 2D discrete network Q = bias (S = {(x, y)} x, y-0,..., N). The algorithm interacts with the MDP through raw pixel characteristics with a 2D uniform feature card \u03c6 (st): = (1 {(x, y) = st} x, y-0,..., N. There are four actions that follow each other in each compass direction, a reward of r = 1 if st = (N, N) (otherwise the reward is zero) and a discount factor of \u03b3 = 0.9. Figure 4 shows the mean value estimate for all states for the ADQN algorithm with different size of the target network buffer. In addition, we have plotted a baseline of the true optimal mean V \u0445. On the left diagram we can see a negative effect of the slowdown of the ADQ1 networks in state values, which has been more recent and recent."}, {"heading": "6.2 Arcade Learning Environment", "text": "We evaluated the ADQN algorithm on several Atari games on the Arcade Learning Environment (Bellemare et al. (2013). The Arcade Learning Environment is the most commonly used benchmark for evaluating DQN variants and adjustments, and this is due to the variety of games that need to be solved with a single set of architecture and hyperparameters. In addition, we have a moderate calculation and memory architecture for ADQN = 10 target networks and reward signals. We followed the setup of the hyperparameters and network architecture as in (Mnih et al. (2013)."}, {"heading": "7 Conclusions and Future Directions", "text": "In this paper, we have presented the ADQN algorithm, an efficient method of reducing variance for the DQN family. We have analyzed and empirically demonstrated why an average-time approach to reducing variance is more advantageous than a direct average approach such as the aforementioned ensemble DQN. Using the Arcade Learning Environment, we have shown how the learning process is stabilized along with an improvement in performance, and how the peak phenomena that occur in both DQN and DQN in some games will no longer occur. In future work, it might be interesting to see how \u03b1, the ADQN learning rate parameter can be learned depending on the states or characteristics, and to what extent the combination of the proposed average scheme with other techniques will prove advantageous. In addition, as our analysis of variance predicts, the ADQN technique is likely to be advantageous in estimation schemes, and further investigation of the topic is required. Finally, the average is only a method of calculating the majority that could be learned."}, {"heading": "B Trajectory Calculations for Section 5.2", "text": "We use the following notation for the discrete Fourier transformation (DFT) in our calculations: For a vector x, we refer to the discrete rectangle function as uK, the uKn = 1 {n (0,.., K \u2212 1)} and its DFT as UDFT, K. Since Zt is assumed to be a zero-mean and independent random variable, it is easy to check whether E [(\u2211 K \u2212 1kL = 0,..., k2 = 0 Zk1 \u2212 kL) 2] = \u03c32z | u1 \u0445 u2."}], "references": [{"title": "The arcade learning environment: An evaluation platform for general agents", "author": ["M.G. Bellemare", "Y. Naddaf", "J. Veness", "M. Bowling"], "venue": "Journal of Artificial Intelligence Research, 47:253\u2013279.", "citeRegEx": "Bellemare et al\\.,? 2013", "shortCiteRegEx": "Bellemare et al\\.", "year": 2013}, {"title": "Unifying count-based exploration and intrinsic motivation", "author": ["M.G. Bellemare", "S. Srinivasan", "G. Ostrovski", "T. Schaul", "D. Saxton", "R. Munos"], "venue": "arXiv preprint arXiv:1606.01868.", "citeRegEx": "Bellemare et al\\.,? 2016", "shortCiteRegEx": "Bellemare et al\\.", "year": 2016}, {"title": "Generalization in reinforcement learning: Safely approximating the value function", "author": ["J. Boyan", "A.W. Moore"], "venue": "Advances in neural information processing systems, pages 369\u2013376.", "citeRegEx": "Boyan and Moore,? 1995", "shortCiteRegEx": "Boyan and Moore", "year": 1995}, {"title": "Double Q-learning", "author": ["H.V. Hasselt"], "venue": "Lafferty, J. D., Williams, C. K. I., Shawe-Taylor, J., Zemel, R. S., and Culotta, A., editors, Advances in Neural Information Processing Systems 23, pages 2613\u20132621. Curran Associates, Inc.", "citeRegEx": "Hasselt,? 2010", "shortCiteRegEx": "Hasselt", "year": 2010}, {"title": "Adam: A method for stochastic optimization", "author": ["D.P. Kingma", "J. Ba"], "venue": "arXiv preprint arXiv: 1412.6980.", "citeRegEx": "Kingma and Ba,? 2014", "shortCiteRegEx": "Kingma and Ba", "year": 2014}, {"title": "Asynchronous methods for deep reinforcement learning", "author": ["V. Mnih", "A.P. Badia", "M. Mirza", "A. Graves", "T.P. Lillicrap", "T. Harley", "D. Silver", "K. Kavukcuoglu"], "venue": "arXiv preprint arXiv:1602.01783.", "citeRegEx": "Mnih et al\\.,? 2016", "shortCiteRegEx": "Mnih et al\\.", "year": 2016}, {"title": "Playing atari with deep reinforcement learning", "author": ["V. Mnih", "K. Kavukcuoglu", "D. Silver", "A. Graves", "I. Antonoglou", "D. Wierstra", "M. Riedmiller"], "venue": "arXiv preprint arXiv:1312.5602.", "citeRegEx": "Mnih et al\\.,? 2013", "shortCiteRegEx": "Mnih et al\\.", "year": 2013}, {"title": "Deep exploration via bootstrapped dqn", "author": ["I. Osband", "C. Blundell", "A. Pritzel", "B. Van Roy"], "venue": "arXiv preprint arXiv:1602.04621.", "citeRegEx": "Osband et al\\.,? 2016", "shortCiteRegEx": "Osband et al\\.", "year": 2016}, {"title": "Neural fitted q iteration\u2013first experiences with a data efficient neural reinforcement learning method", "author": ["M. Riedmiller"], "venue": "European Conference on Machine Learning, pages 317\u2013328. Springer.", "citeRegEx": "Riedmiller,? 2005", "shortCiteRegEx": "Riedmiller", "year": 2005}, {"title": "Prioritized experience replay", "author": ["T. Schaul", "J. Quan", "I. Antonoglou", "D. Silver"], "venue": "arXiv preprint arXiv:1511.05952.", "citeRegEx": "Schaul et al\\.,? 2015", "shortCiteRegEx": "Schaul et al\\.", "year": 2015}, {"title": "Reinforcement Learning: An Introduction, volume 1", "author": ["R.S. Sutton", "A.G. Barto"], "venue": "MIT Press Cambridge.", "citeRegEx": "Sutton and Barto,? 1998", "shortCiteRegEx": "Sutton and Barto", "year": 1998}, {"title": "Practical issues in temporal difference learning", "author": ["G. Tesauro"], "venue": "Reinforcement Learning, pages 33\u201353. Springer.", "citeRegEx": "Tesauro,? 1992", "shortCiteRegEx": "Tesauro", "year": 1992}, {"title": "Issues in using function approximation for reinforcement learning", "author": ["S. Thrun", "A. Schwartz"], "venue": "Proceedings of the 1993 Connectionist Models Summer School Hillsdale, NJ. Lawrence Erlbaum.", "citeRegEx": "Thrun and Schwartz,? 1993", "shortCiteRegEx": "Thrun and Schwartz", "year": 1993}, {"title": "Deep reinforcement learning with double Q-learning", "author": ["H. Van Hasselt", "A. Guez", "D. Silver"], "venue": "arXiv preprint arXiv: 1509.06461.", "citeRegEx": "Hasselt et al\\.,? 2015", "shortCiteRegEx": "Hasselt et al\\.", "year": 2015}, {"title": "Dueling network architectures for deep reinforcement learning", "author": ["Z. Wang", "N. de Freitas", "M. Lanctot"], "venue": "arXiv preprint arXiv:", "citeRegEx": "Wang et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Wang et al\\.", "year": 2015}, {"title": "Q-learning", "author": ["C.J. Watkins", "P. Dayan"], "venue": "Machine Learning, 8(3-4):279\u2013292.", "citeRegEx": "Watkins and Dayan,? 1992", "shortCiteRegEx": "Watkins and Dayan", "year": 1992}], "referenceMentions": [{"referenceID": 10, "context": "1 Introduction In Reinforcement Learning (RL) an agent seeks to find an optimal policy for a sequential decision problem (Sutton and Barto, 1998).", "startOffset": 121, "endOffset": 145}, {"referenceID": 15, "context": "One of the most popular algorithm for RL problems is the Q-learning algorithm (Watkins and Dayan, 1992).", "startOffset": 78, "endOffset": 103}, {"referenceID": 11, "context": "One of the most impressive early works, bringing together RL and neural networks presented a worldclass backgammon agent (Tesauro, 1992).", "startOffset": 121, "endOffset": 136}, {"referenceID": 2, "context": "Nevertheless, combined directly, divergence was shown even on simple problems (Boyan and Moore, 1995), and until recent breakthroughs the progress in the field was slow.", "startOffset": 78, "endOffset": 101}, {"referenceID": 6, "context": "Although DQN proved to be successful in most of the benchmark problems (Mnih et al., 2013), in some of them it seems to fail.", "startOffset": 71, "endOffset": 90}, {"referenceID": 2, "context": "Nevertheless, combined directly, divergence was shown even on simple problems (Boyan and Moore, 1995), and until recent breakthroughs the progress in the field was slow. The DQN algorithm (Mnih et al. (2013)) combines Q-learning with the recent progress in neural network techniques.", "startOffset": 79, "endOffset": 208}, {"referenceID": 1, "context": "exploration techniques for high-dimensional state space to remedy this problem (Bellemare et al., 2016; Osband et al., 2016).", "startOffset": 79, "endOffset": 124}, {"referenceID": 7, "context": "exploration techniques for high-dimensional state space to remedy this problem (Bellemare et al., 2016; Osband et al., 2016).", "startOffset": 79, "endOffset": 124}, {"referenceID": 0, "context": "Section 6 provides an empirical evaluation of the ADQN algorithm both in a toy problem and in several of the Atari 2600 games from the Arcade Learning Environment (Bellemare et al., 2013).", "startOffset": 163, "endOffset": 187}, {"referenceID": 3, "context": "Double Q-learning (Hasselt, 2010) was suggested as a solution for this problem.", "startOffset": 18, "endOffset": 33}, {"referenceID": 7, "context": "Another recent work, the Bootstrapped DQN algorithm (Osband et al., 2016) implemented Deep Exploration with the use of several Q-networks learning in parallel.", "startOffset": 52, "endOffset": 73}, {"referenceID": 3, "context": "Double Q-learning (Hasselt, 2010) was suggested as a solution for this problem. The max operator was divided into selection and evaluation parts, which replaced the positive bias of the original predictor with a negative one. Later the Double-DQN (DDQN) algorithm that was introduced in Van Hasselt et al. (2015) provided a DQN implementation of this algorithm.", "startOffset": 19, "endOffset": 313}, {"referenceID": 3, "context": "Double Q-learning (Hasselt, 2010) was suggested as a solution for this problem. The max operator was divided into selection and evaluation parts, which replaced the positive bias of the original predictor with a negative one. Later the Double-DQN (DDQN) algorithm that was introduced in Van Hasselt et al. (2015) provided a DQN implementation of this algorithm. Since the publication of the DQN work, many variants and modification of the standard algorithm have been developed. Schaul et al. (2015) showed improved results by employing smart policies for sampling from the experience buffer; Wang et al.", "startOffset": 19, "endOffset": 500}, {"referenceID": 3, "context": "Double Q-learning (Hasselt, 2010) was suggested as a solution for this problem. The max operator was divided into selection and evaluation parts, which replaced the positive bias of the original predictor with a negative one. Later the Double-DQN (DDQN) algorithm that was introduced in Van Hasselt et al. (2015) provided a DQN implementation of this algorithm. Since the publication of the DQN work, many variants and modification of the standard algorithm have been developed. Schaul et al. (2015) showed improved results by employing smart policies for sampling from the experience buffer; Wang et al. (2015) introduced a modified Q-network architecture; Mnih et al.", "startOffset": 19, "endOffset": 612}, {"referenceID": 3, "context": "Double Q-learning (Hasselt, 2010) was suggested as a solution for this problem. The max operator was divided into selection and evaluation parts, which replaced the positive bias of the original predictor with a negative one. Later the Double-DQN (DDQN) algorithm that was introduced in Van Hasselt et al. (2015) provided a DQN implementation of this algorithm. Since the publication of the DQN work, many variants and modification of the standard algorithm have been developed. Schaul et al. (2015) showed improved results by employing smart policies for sampling from the experience buffer; Wang et al. (2015) introduced a modified Q-network architecture; Mnih et al. (2016) implemented asynchronous version of the DQN which does not require the use of an experience-buffer and showed impressive performance.", "startOffset": 19, "endOffset": 677}, {"referenceID": 15, "context": "In classical Q-learning (Watkins and Dayan (1992)) the update rule is given by Qt+1(s, a)\u2190 (1\u2212 \u03b1)Qt(s, a) + \u03b1(r(s, a) + \u03b3max a\u2032 Qt(s \u2032, a\u2032)) (1) where s\u2032 is the resulting state after applying action a in the current state s, \u03b3 is the MDP\u2019s discount factor and \u03b1 is a time varying learning rate.", "startOffset": 25, "endOffset": 50}, {"referenceID": 8, "context": "must be introduced (Riedmiller, 2005).", "startOffset": 19, "endOffset": 37}, {"referenceID": 6, "context": "The DQN algorithm (Mnih et al., 2013) introduced two important modifications for the algorithm above.", "startOffset": 18, "endOffset": 37}, {"referenceID": 12, "context": "Thrun and Schwartz (1993) provided a first theoretical analysis for combing Q-learning and function approximations.", "startOffset": 0, "endOffset": 26}, {"referenceID": 11, "context": "1 Overestimation Thrun and Schwartz (1993) considered an additive zero-mean random variable, a generalization error model for the Q-function.", "startOffset": 17, "endOffset": 43}, {"referenceID": 3, "context": "Hasselt (2010) argued that even in a tabular setting environment noise can cause overestimations and suggested the Double Q-learning algorithm as a solution.", "startOffset": 0, "endOffset": 15}, {"referenceID": 0, "context": "Second, we will demonstrate and compare performance using the Arcade Learning Environment (Bellemare et al. (2013)), which is the most commonly used benchmark environment for RL algorithms.", "startOffset": 91, "endOffset": 115}, {"referenceID": 4, "context": "For the experiments, we have used the ADAM optimizer (Kingma and Ba (2014)), updated the target network parameters each \u03c4 = 300 iterations, and \u03b1 = 0.", "startOffset": 54, "endOffset": 75}, {"referenceID": 0, "context": "2 Arcade Learning Environment We have evaluated the ADQN algorithm on several Atari games on the Arcade Learning Environment (Bellemare et al. (2013)).", "startOffset": 126, "endOffset": 150}, {"referenceID": 5, "context": "We followed the setup of hyper-parameters and network architecture as in (Mnih et al. (2013)).", "startOffset": 74, "endOffset": 93}], "year": 2016, "abstractText": "The commonly used Q-learning algorithm combined with function approximation induces systematic overestimations of state-action values. These systematic errors might cause instability, poor performance and sometimes divergence of learning. In this work, we present the AVERAGED TARGET DQN (ADQN) algorithm, an adaptation to the DQN class of algorithms which uses a weighted average over past learned networks to reduce generalization noise variance. As a consequence, this leads to reduced overestimations, more stable learning process and improved performance. Additionally, we analyze ADQN variance reduction along trajectories and demonstrate the performance of ADQN on a toy Gridworld problem, as well as on several of the Atari 2600 games from the Arcade Learning Environment.", "creator": "LaTeX with hyperref package"}}}