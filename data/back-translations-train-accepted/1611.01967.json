{"id": "1611.01967", "review": {"conference": "iclr", "VERSION": "v1", "DATE_OF_SUBMISSION": "7-Nov-2016", "title": "Regularizing CNNs with Locally Constrained Decorrelations", "abstract": "Regularization is key for deep learning since it allows training more complex models while keeping lower levels of overfitting. However, the most prevalent regularizations do not leverage all the capacity of the models since they rely on reducing the effective number of parameters. Feature decorrelation is an alternative for using the full capacity of the models but the overfitting reduction margins are too narrow given the overhead it introduces. In this paper, we show that regularizing negatively correlated features is an obstacle for effective decorrelation and present OrthoReg, a novel regularization technique that locally enforces feature orthogonality. As a result, imposing locality constraints in feature decorrelation removes interferences between negatively correlated features, allowing the regularizer to reach higher decorrelation bounds, and reducing the overfitting more effectively. In particular, we show that the models regularized with OrthoReg have higher accuracy bounds even when batch normalization and dropout present. Moreover, since our regularization is directly performed on the weights, it is especially suitable for fully convolutional neural networks, where the weight space is constant compared to the feature map space. As a result, we are able to reduce the overfitting of state-of-the-art CNNs on CIFAR-10, CIFAR-100, and SVHN.", "histories": [["v1", "Mon, 7 Nov 2016 10:15:40 GMT  (794kb,D)", "http://arxiv.org/abs/1611.01967v1", "Submitted to ICLR2017 conference"], ["v2", "Wed, 15 Mar 2017 08:18:28 GMT  (1024kb,D)", "http://arxiv.org/abs/1611.01967v2", "Accepted at ICLR2017"]], "COMMENTS": "Submitted to ICLR2017 conference", "reviews": [], "SUBJECTS": "cs.LG cs.NE", "authors": ["pau rodr\\'iguez", "jordi gonz\\`alez", "guillem cucurull", "josep m gonfaus", "xavier roca"], "accepted": true, "id": "1611.01967"}, "pdf": {"name": "1611.01967.pdf", "metadata": {"source": "CRF", "title": "REGULARIZING CNNS WITH LOCALLY CONSTRAINED DECORRELATIONS", "authors": ["Pau Rodr\u0131\u0301guez", "Jordi Gonz\u00e0lez", "Guillem Cucurull", "Josep M. Gonfaus", "Xavier Roca"], "emails": [], "sections": [{"heading": "1 INTRODUCTION", "text": "In fact, most of them are able to survive themselves if they do not play by the rules. (...) Most of them are able to survive themselves. (...) Most of them are not able to survive themselves. (...) Most of them are not able to survive themselves. (...) Most of them are able to survive themselves. (...) Most of them are able to survive themselves. (...) Most of them are able to survive themselves. (...) Most of them are not able to survive themselves. (...) Most of them are able to survive themselves. (...) Most of them are able to survive themselves. (...) Most of them are able to survive themselves. (...) Most of them are able to survive themselves. (...) Most of them are able to survive themselves. (...)"}, {"heading": "2 DEALING WITH WEIGHT REDUNDANCIES", "text": "Deep Neural Networks (DNN) are very meaningful models that can normally have millions of parameters, but with limited data they tend to overlap; there are a plethora of techniques to deal with this problem, from L1 and L2 regularizations (Nowlan & Hinton (1992), Early-Stop, Dropout, or DropConnect. Models that exhibit a high degree of overmatch usually have a lot of redundancy in their properties, with similar patterns detected with slight differences that usually correspond to noise in the training data. A particular case where this is obvious is in AlexNet (Krizhevsky et al. (2012), which has very similar conversion filters and even \"dead\" as noted by Zeiler & Fergus (2014). Indeed, if a set of parameters, I, j linking inputs I = {i1, i2,"}, {"heading": "2.1 ORTHOGONAL WEIGHT REGULARIZATION", "text": "In this section, the orthogonal weight regulation is introduced = 1 = j = j = j = j = j = j = j = j = j = j = j = j = f = f = f = f = f = f = f = f = f = f = f = f = f = f = f = f = f = f = f = f = f = f = f = f = f = f = f = f = f = f = f = f = f = f = f = f = f = f = f = f = f = f = f = f = f = f = f = f = f = f = f = f = f = f = f = f = f = f = f = f = f = f = f = f = f = f = f = f = f = f = f = f = f = f = f = f = f = f = f = f = f = f = f = f = f = f = f = f = f = f = f = f = f = f = f = f = f = f = f = f = f = f = f = f = f = f = f = f = f = f = f = f = f = f = f = f = f = f = f = f = f = f = f = f = f = f = f = f = f = f = f = f = f = f = f = f = f = f = f = f = f = f = f = f = f = f = f = f = f = f = f = f = f = f = f = f = f = f = f = f = f = f = f = f = f = f = f = f = f = f = f = f = f = f = f = f = f = f = f = f = f = f = f = f = f = f = f = f = f = f = f = f = f = f = f = f = f = f = f = f = f = f = f = f = f = f = f = f = f = f = f = f = f = f = f = f = f = f = f f = f = f = f = f = f = f = f = f = f = f = f = f = f = f = f = f"}, {"heading": "2.2 NEGATIVE CORRELATIONS", "text": "Note that the presented algorithm, based on cosmic similarity, has any kind of correlation between all pairs of trait detectors (i.e. the positive and negative correlations, see Figure 1a. Negative correlations, however, are related to inhibitory relationships, competitive learning, and self-organization. In fact, there is evidence that negative correlations can help a neural population increase the signal-to-noise ratio (Chelaru & Dragoi (2016)) in V1. To find out the benefits of maintaining negative correlations, we suggest using an exponential approach to attract the gradients for angles greater than 2 (orthogonal)."}, {"heading": "3 EXPERIMENTS", "text": "In this section, we provide a series of experiments demonstrating that (i) training with the proposed regulation increases the performance of na\u00efve unregulated models, (ii) negatively correlated features are useful, and (iii) the proposed regulation improves the performance of modern models."}, {"heading": "3.1 VERIFICATION EXPERIMENTS", "text": "As a health test, we first train a three-layer multi-layer perceptron (MLP) with ReLU nonlinearities on the MNIST dataset (LeCun et al. (1998)).Our code is based on the train-a-digit classifier example included in Torch / Demos1, which uses an upsampled version of the dataset (32 x 32).The only pre-processing that is applied to the data is global standardization. The model is trained with SGD and a batch size of 200 over 200 epochs. Neither dynamics nor weight decay has been applied. By default, the order of weights of these experiments is restored after each regulation step to prove the regulation only its angle.Sensitivity to hyperparameters. We train a three-layer MLP with 1024 hidden units and different index and index values to check how they affect the model's performance."}, {"heading": "3.2 REGULARIZATION ON CIFAR-10 AND CIFAR-100", "text": "We show that the proposed OrthoReg can help improve the performance of modern models such as deep residual meshes (He et al. (2015a). To show that the regulation is suitable for deep CNNs, we successfully regulate a 110-layer ResNet3 on CIFAR-10, reducing the error from 6.55% to 6.29% without data augmentation. To compare with the state-of-the-art, we train a wide residual network (Zagoruyko & Komodakis (May 2016) on Cifar-10 and Cifar-100) based on a torch implementation of the 28-layer and 10-layer wide deep residual model, for which the average error rate on CIFAR-10 is 4.50% and 20.04% on CIFAR-100 after 200 epochs."}, {"heading": "3.3 REGULARIZATION ON SVHN", "text": "For SVHN, we follow the procedure described in Zagoruyko & Komodakis (May 2016) by training a wide residual network of depth = 28, width = 4 and dropout.The results are shown in Table 4. As you can see, we are reducing the error rate from 1.64% to 1.54%, which is the lowest recorded in this dataset to the best of our knowledge. 3https: / / github.com / gcr / facch-residual-networks 4https: / / github.com / szagoruyko / wide-residual-networks"}, {"heading": "4 DISCUSSION", "text": "Regulation by Feature Decoration can reduce overmatching of neural networks even in the presence of a different type of regulation, especially when the number of feature decoration detectors exceeds the input dimensionality, their decoration capacity is limited due to the effects of negatively correlated features. We have shown that imposed localization restrictions in feature decoration eliminate interference between negatively correlated features, allow regulators to reach higher decoration limits and more effectively reduce overmatching. In particular, we show that models regulated by restricted regulation exhibit lower overfits even when batch normalization and dropout are present. As our regulation is applied directly to weights, it is particularly suitable for fully convolutionary neural networks where the weight space is constant compared to the feature map space. As a result, we are able to reduce overfitting of 110-Nets reslayer and SVNNCIAR-F10 to HIAR and HIAR-F10."}, {"heading": "ACKNOWLEDGEMENTS", "text": "This work was supported by the Spanish project TIN2015-65464-R (MINECO / FEDER), the FI B 01163 of 2016 and the COST action IC1307 iV & L Net (European Network on Integrating Vision and Language), supported by COST (European Cooperation in Science and Technology)."}], "references": [{"title": "Incoherent training of deep neural networks to de-correlate bottleneck features for speech recognition", "author": ["Yebo Bao", "Hui Jiang", "Lirong Dai", "Cong Liu"], "venue": "IEEE ICASSP,", "citeRegEx": "Bao et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Bao et al\\.", "year": 2013}, {"title": "Slow, decorrelated features for pretraining complex cell-like networks", "author": ["Yoshua Bengio", "James S Bergstra"], "venue": "In NIPS, pp", "citeRegEx": "Bengio and Bergstra.,? \\Q2009\\E", "shortCiteRegEx": "Bengio and Bergstra.", "year": 2009}, {"title": "Negative correlations in visual cortical networks", "author": ["Mircea I Chelaru", "Valentin Dragoi"], "venue": "Cerebral Cortex,", "citeRegEx": "Chelaru and Dragoi.,? \\Q2016\\E", "shortCiteRegEx": "Chelaru and Dragoi.", "year": 2016}, {"title": "Fast and accurate deep network learning by exponential linear units (ELUs)", "author": ["Djork-Arn Clevert", "Thomas Unterthiner", "Sepp Hochreiter"], "venue": null, "citeRegEx": "Clevert et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Clevert et al\\.", "year": 2016}, {"title": "Reducing overfitting in deep networks by decorrelating representations", "author": ["Michael Cogswell", "Faruk Ahmed", "Ross Girshick", "Larry Zitnick", "Dhruv Batra"], "venue": null, "citeRegEx": "Cogswell et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Cogswell et al\\.", "year": 2016}, {"title": "Understanding the difficulty of training deep feedforward neural networks", "author": ["Xavier Glorot", "Yoshua Bengio"], "venue": "In AISTATS,", "citeRegEx": "Glorot and Bengio.,? \\Q2010\\E", "shortCiteRegEx": "Glorot and Bengio.", "year": 2010}, {"title": "Fractional max-pooling", "author": ["Benjamin Graham"], "venue": "arXiv preprint arXiv:1412.6071,", "citeRegEx": "Graham.,? \\Q2014\\E", "shortCiteRegEx": "Graham.", "year": 2014}, {"title": "Deep residual learning for image recognition", "author": ["Kaiming He", "Xiangyu Zhang", "Shaoqing Ren", "Jian Sun"], "venue": "arXiv preprint arXiv:1512.03385,", "citeRegEx": "He et al\\.,? \\Q2015\\E", "shortCiteRegEx": "He et al\\.", "year": 2015}, {"title": "Delving deep into rectifiers: Surpassing human-level performance on imagenet classification", "author": ["Kaiming He", "Xiangyu Zhang", "Shaoqing Ren", "Jian Sun"], "venue": "In ICCV,", "citeRegEx": "He et al\\.,? \\Q2015\\E", "shortCiteRegEx": "He et al\\.", "year": 2015}, {"title": "Deep networks with stochastic depth", "author": ["Gao Huang", "Yu Sun", "Zhuang Liu", "Daniel Sedra", "Kilian Weinberger"], "venue": "arXiv preprint arXiv:1603.09382,", "citeRegEx": "Huang et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Huang et al\\.", "year": 2016}, {"title": "Batch normalization: Accelerating deep network training by reducing internal covariate shift", "author": ["Sergey Ioffe", "Christian Szegedy"], "venue": "In ICML, pp", "citeRegEx": "Ioffe and Szegedy.,? \\Q2015\\E", "shortCiteRegEx": "Ioffe and Szegedy.", "year": 2015}, {"title": "ImageNet Classification with Deep Convolutional Neural Networks", "author": ["Alex Krizhevsky", "Ilya Sutskever", "Geoffrey E. Hinton"], "venue": "In NIPS, pp", "citeRegEx": "Krizhevsky et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Krizhevsky et al\\.", "year": 2012}, {"title": "Gradient-based learning applied to document recognition", "author": ["Yann LeCun", "L\u00e9on Bottou", "Yoshua Bengio", "Patrick Haffner"], "venue": "Proceedings of the IEEE,", "citeRegEx": "LeCun et al\\.,? \\Q1998\\E", "shortCiteRegEx": "LeCun et al\\.", "year": 1998}, {"title": "All you need is a good init", "author": ["Dmytro Mishkin", "Jiri Matas"], "venue": null, "citeRegEx": "Mishkin and Matas.,? \\Q2016\\E", "shortCiteRegEx": "Mishkin and Matas.", "year": 2016}, {"title": "Reading digits in natural images with unsupervised feature learning", "author": ["Yuval Netzer", "Tao Wang", "Adam Coates", "Alessandro Bissacco", "Bo Wu", "Andrew Y Ng"], "venue": null, "citeRegEx": "Netzer et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Netzer et al\\.", "year": 2011}, {"title": "Simplifying neural networks by soft weight-sharing", "author": ["Steven J. Nowlan", "Geoffrey E. Hinton"], "venue": "Neural computation,", "citeRegEx": "Nowlan and Hinton.,? \\Q1992\\E", "shortCiteRegEx": "Nowlan and Hinton.", "year": 1992}, {"title": "Imagenet large scale visual recognition challenge", "author": ["Olga Russakovsky", "Jia Deng", "Hao Su", "Jonathan Krause", "Sanjeev Satheesh", "Sean Ma", "Zhiheng Huang", "Andrej Karpathy", "Aditya Khosla", "Michael Bernstein"], "venue": null, "citeRegEx": "Russakovsky et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Russakovsky et al\\.", "year": 2015}, {"title": "Exact solutions to the nonlinear dynamics of learning in deep linear neural networks", "author": ["Andrew M. Saxe", "James L. McClelland", "Surya Ganguli"], "venue": null, "citeRegEx": "Saxe et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Saxe et al\\.", "year": 2013}, {"title": "Striving for simplicity: The all convolutional net", "author": ["Jost T. Springenberg", "Alexey Dosovitskiy", "Thomas Brox", "Martin Riedmiller"], "venue": "In ICLR (workshop track),", "citeRegEx": "Springenberg et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Springenberg et al\\.", "year": 2015}, {"title": "Dropout: A simple way to prevent neural networks from overfitting", "author": ["Nitish Srivastava", "Geoffrey E. Hinton", "Alex Krizhevsky", "Ilya Sutskever", "Ruslan Salakhutdinov"], "venue": null, "citeRegEx": "Srivastava et al\\.,? \\Q1929\\E", "shortCiteRegEx": "Srivastava et al\\.", "year": 1929}, {"title": "Training very deep networks", "author": ["Rupesh K. Srivastava", "Klaus Greff", "J\u00fcrgen Schmidhuber"], "venue": "In NIPS, pp", "citeRegEx": "Srivastava et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Srivastava et al\\.", "year": 2015}, {"title": "Going deeper with convolutions", "author": ["Christian Szegedy", "Wei Liu", "Yangqing Jia", "Pierre Sermanet", "Scott Reed", "Dragomir Anguelov", "Dumitru Erhan", "Vincent Vanhoucke", "Andrew Rabinovich"], "venue": "In CVPR, pp", "citeRegEx": "Szegedy et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Szegedy et al\\.", "year": 2015}, {"title": "Regularization of neural networks using dropconnect", "author": ["Li Wan", "Matthew D Zeiler", "Sixin Zhang", "Yann L Cun", "Rob Fergus"], "venue": "In ICML,", "citeRegEx": "Wan et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Wan et al\\.", "year": 2013}, {"title": "Wide residual networks", "author": ["Sergey Zagoruyko", "Nikos Komodakis"], "venue": "arXiv preprint arXiv:1605.07146,", "citeRegEx": "Zagoruyko and Komodakis.,? \\Q2016\\E", "shortCiteRegEx": "Zagoruyko and Komodakis.", "year": 2016}, {"title": "Visualizing and understanding convolutional networks", "author": ["Matthew D. Zeiler", "Rob Fergus"], "venue": "In ECCV, pp", "citeRegEx": "Zeiler and Fergus.,? \\Q2014\\E", "shortCiteRegEx": "Zeiler and Fergus.", "year": 2014}], "referenceMentions": [{"referenceID": 7, "context": "Neural networks perform really well in numerous tasks even when initialized randomly and trained with Stochastic Gradient Descent (SGD) (see Krizhevsky et al. (2012)).", "startOffset": 141, "endOffset": 166}, {"referenceID": 7, "context": "Neural networks perform really well in numerous tasks even when initialized randomly and trained with Stochastic Gradient Descent (SGD) (see Krizhevsky et al. (2012)). Deeper models, like Googlenet (Szegedy et al. (2015)) and Deep Residual Networks (Szegedy et al.", "startOffset": 141, "endOffset": 221}, {"referenceID": 7, "context": "Neural networks perform really well in numerous tasks even when initialized randomly and trained with Stochastic Gradient Descent (SGD) (see Krizhevsky et al. (2012)). Deeper models, like Googlenet (Szegedy et al. (2015)) and Deep Residual Networks (Szegedy et al. (2015); He et al.", "startOffset": 141, "endOffset": 272}, {"referenceID": 5, "context": "(2015); He et al. (2015a)) are released each year, providing impressive results and even surpassing human performances in well-known datasets such as the Imagenet (Russakovsky et al.", "startOffset": 8, "endOffset": 26}, {"referenceID": 5, "context": "(2015); He et al. (2015a)) are released each year, providing impressive results and even surpassing human performances in well-known datasets such as the Imagenet (Russakovsky et al. (2015)).", "startOffset": 8, "endOffset": 190}, {"referenceID": 5, "context": "(2015); He et al. (2015a)) are released each year, providing impressive results and even surpassing human performances in well-known datasets such as the Imagenet (Russakovsky et al. (2015)). This would not have been possible without the help of regularization and initialization techniques which solve the overfitting and convergence problems that are usually caused by data scarcity and the growth of the architectures. There are two clearly defined regularization strategies in the literature. The first ones consist in reducing the complexity of the model by (i) reducing the effective number of parameters with weight decay (Nowlan & Hinton (1992)), and (ii) randomly dropping activations with Dropout (Srivastava et al.", "startOffset": 8, "endOffset": 653}, {"referenceID": 5, "context": "(2015); He et al. (2015a)) are released each year, providing impressive results and even surpassing human performances in well-known datasets such as the Imagenet (Russakovsky et al. (2015)). This would not have been possible without the help of regularization and initialization techniques which solve the overfitting and convergence problems that are usually caused by data scarcity and the growth of the architectures. There are two clearly defined regularization strategies in the literature. The first ones consist in reducing the complexity of the model by (i) reducing the effective number of parameters with weight decay (Nowlan & Hinton (1992)), and (ii) randomly dropping activations with Dropout (Srivastava et al. (2014)) or dropping weights with DropConnect (Wan et al.", "startOffset": 8, "endOffset": 733}, {"referenceID": 5, "context": "(2015); He et al. (2015a)) are released each year, providing impressive results and even surpassing human performances in well-known datasets such as the Imagenet (Russakovsky et al. (2015)). This would not have been possible without the help of regularization and initialization techniques which solve the overfitting and convergence problems that are usually caused by data scarcity and the growth of the architectures. There are two clearly defined regularization strategies in the literature. The first ones consist in reducing the complexity of the model by (i) reducing the effective number of parameters with weight decay (Nowlan & Hinton (1992)), and (ii) randomly dropping activations with Dropout (Srivastava et al. (2014)) or dropping weights with DropConnect (Wan et al. (2013)) so as to prevent feature co-adaptation.", "startOffset": 8, "endOffset": 790}, {"referenceID": 5, "context": "(2015); He et al. (2015a)) are released each year, providing impressive results and even surpassing human performances in well-known datasets such as the Imagenet (Russakovsky et al. (2015)). This would not have been possible without the help of regularization and initialization techniques which solve the overfitting and convergence problems that are usually caused by data scarcity and the growth of the architectures. There are two clearly defined regularization strategies in the literature. The first ones consist in reducing the complexity of the model by (i) reducing the effective number of parameters with weight decay (Nowlan & Hinton (1992)), and (ii) randomly dropping activations with Dropout (Srivastava et al. (2014)) or dropping weights with DropConnect (Wan et al. (2013)) so as to prevent feature co-adaptation. Due to their nature, although this set of strategies have proved to be very effective, they do not leverage all the capacity of the models they regularize. The second group of regularizations is those which improve the effectiveness and generality of the trained model without reducing their capacity. In this second group, the most relevant approaches decorrelate the weights or feature maps, e.g. Bengio & Bergstra (2009) introduced a new criterion so as to learn slow decorrelated features while pre-training models.", "startOffset": 8, "endOffset": 1255}, {"referenceID": 0, "context": "In the same line Bao et al. (2013) presented \u201dincoherent training\u201d, a regularizer for reducing the decorrelation of the network activations or feature maps in the context of speech recognition.", "startOffset": 17, "endOffset": 35}, {"referenceID": 0, "context": "In the same line Bao et al. (2013) presented \u201dincoherent training\u201d, a regularizer for reducing the decorrelation of the network activations or feature maps in the context of speech recognition. Although regularizations in the second group are promising and have already been used to reduce the overfitting in different tasks, even with the presence of Dropout (as shown by Cogswell et al. (2016)), they are seldom used in the large scale image recognition domain because of the small improvement margins they provide together with the computational overhead they introduce.", "startOffset": 17, "endOffset": 396}, {"referenceID": 4, "context": "Count of the Flops for the models used in this paper: the 3hidden-layer MLP and the 110-layer ResNet we use later in the experiments section when not regularized, using DeCov (Cogswell et al. (2016)) and using OrthoReg.", "startOffset": 176, "endOffset": 199}, {"referenceID": 7, "context": "In the same line initialization strategies such as \u201dXavier\u201d (Glorot & Bengio (2010)) or \u201dHe\u201d (He et al. (2015b)), also keep the same variance at both input and output of the layers in order to preserve propagated signals in deep neural networks.", "startOffset": 94, "endOffset": 112}, {"referenceID": 7, "context": "In the same line initialization strategies such as \u201dXavier\u201d (Glorot & Bengio (2010)) or \u201dHe\u201d (He et al. (2015b)), also keep the same variance at both input and output of the layers in order to preserve propagated signals in deep neural networks. Orthogonal initialization techniques are another family which set the weights in a decorrelated initial state so as to condition the network training to converge into better representations. For instance, Mishkin & Matas (2016) propose to initialize the network with decorrelated features using orthonormal initialization (Saxe et al.", "startOffset": 94, "endOffset": 474}, {"referenceID": 7, "context": "In the same line initialization strategies such as \u201dXavier\u201d (Glorot & Bengio (2010)) or \u201dHe\u201d (He et al. (2015b)), also keep the same variance at both input and output of the layers in order to preserve propagated signals in deep neural networks. Orthogonal initialization techniques are another family which set the weights in a decorrelated initial state so as to condition the network training to converge into better representations. For instance, Mishkin & Matas (2016) propose to initialize the network with decorrelated features using orthonormal initialization (Saxe et al. (2013)) while normalizing the variance of the outputs as well.", "startOffset": 94, "endOffset": 588}, {"referenceID": 7, "context": "In the same line initialization strategies such as \u201dXavier\u201d (Glorot & Bengio (2010)) or \u201dHe\u201d (He et al. (2015b)), also keep the same variance at both input and output of the layers in order to preserve propagated signals in deep neural networks. Orthogonal initialization techniques are another family which set the weights in a decorrelated initial state so as to condition the network training to converge into better representations. For instance, Mishkin & Matas (2016) propose to initialize the network with decorrelated features using orthonormal initialization (Saxe et al. (2013)) while normalizing the variance of the outputs as well. In this work we hypothesize that regularizing negatively correlated features is an obstacle for achieving better results and we introduce OrhoReg, a novel regularization technique that addresses the performance margin issue by only regularizing positively correlated features. Moreover, OrthoReg is computationally efficient since it only regularizes the feature weights, which makes it very suitable for the latest CNN models. We verify our hypothesis through a series of experiments: first using MNIST as a proof of concept, secondly we regularize wide residual networks on Cifar10, Cifar100, and SVHN (Netzer et al. (2011)) achieving the best error rates in the dataset to the best of our knowledge.", "startOffset": 94, "endOffset": 1270}, {"referenceID": 11, "context": "A particular case where this is evident is in AlexNet (Krizhevsky et al. (2012)), which presents very similar convolution filters and even \u201ddead\u201d ones, as it was remarked by Zeiler & Fergus (2014).", "startOffset": 55, "endOffset": 80}, {"referenceID": 11, "context": "A particular case where this is evident is in AlexNet (Krizhevsky et al. (2012)), which presents very similar convolution filters and even \u201ddead\u201d ones, as it was remarked by Zeiler & Fergus (2014). In fact, given a set of parameters \u03b8I,j connecting a set of inputs I = {i1, i2, .", "startOffset": 55, "endOffset": 197}, {"referenceID": 12, "context": "1 VERIFICATION EXPERIMENTS As a sanity check, we first train a three-hidden-layer Multi-Layer Perceptron (MLP) with ReLU non-liniarities on the MNIST dataset (LeCun et al. (1998)).", "startOffset": 159, "endOffset": 179}, {"referenceID": 7, "context": "We show that the proposed OrthoReg can help to improve the performance of state-of-the-art models such as deep residual networks (He et al. (2015a)).", "startOffset": 130, "endOffset": 148}, {"referenceID": 14, "context": "57 YES Highway Network (Srivastava et al. (2015)) 7.", "startOffset": 24, "endOffset": 49}, {"referenceID": 14, "context": "24 YES All-CNN (Springenberg et al. (2015)) 7.", "startOffset": 16, "endOffset": 43}, {"referenceID": 5, "context": "71 NO 110-Layer ResNet (He et al. (2015a)) 6.", "startOffset": 24, "endOffset": 42}, {"referenceID": 3, "context": "4 NO ELU-Network (Clevert et al. (2016)) 6.", "startOffset": 18, "endOffset": 40}, {"referenceID": 3, "context": "4 NO ELU-Network (Clevert et al. (2016)) 6.55 24.28 NO OrthoReg on 110-Layer ResNet* 6.29\u00b1 0.19 28.33\u00b1 0.5 NO LSUV (Mishkin & Matas (2016)) 5.", "startOffset": 18, "endOffset": 139}, {"referenceID": 3, "context": "4 NO ELU-Network (Clevert et al. (2016)) 6.55 24.28 NO OrthoReg on 110-Layer ResNet* 6.29\u00b1 0.19 28.33\u00b1 0.5 NO LSUV (Mishkin & Matas (2016)) 5.84 YES Fract. Max-Pooling (Graham (2014)) 4.", "startOffset": 18, "endOffset": 183}, {"referenceID": 9, "context": "92 Stochastic Depth ResNet (Huang et al. (2016)) 1.", "startOffset": 28, "endOffset": 48}], "year": 2017, "abstractText": "Regularization is key for deep learning since it allows training more complex models while keeping lower levels of overfitting. However, the most prevalent regularizations do not leverage all the capacity of the models since they rely on reducing the effective number of parameters. Feature decorrelation is an alternative for using the full capacity of the models but the overfitting reduction margins are too narrow given the overhead it introduces. In this paper, we show that regularizing negatively correlated features is an obstacle for effective decorrelation and present OrthoReg, a novel regularization technique that locally enforces feature orthogonality. As a result, imposing locality constraints in feature decorrelation removes interferences between negatively correlated features, allowing the regularizer to reach higher decorrelation bounds, and reducing the overfitting more effectively. In particular, we show that the models regularized with OrthoReg have higher accuracy bounds even when batch normalization and dropout present. Moreover, since our regularization is directly performed on the weights, it is especially suitable for fully convolutional neural networks, where the weight space is constant compared to the feature map space. As a result, we are able to reduce the overfitting of state-of-the-art CNNs on CIFAR-10, CIFAR-100, and SVHN.", "creator": "LaTeX with hyperref package"}}}