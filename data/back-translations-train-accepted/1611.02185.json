{"id": "1611.02185", "review": {"conference": "iclr", "VERSION": "v1", "DATE_OF_SUBMISSION": "7-Nov-2016", "title": "Trusting SVM for Piecewise Linear CNNs", "abstract": "We present a novel layerwise optimization algorithm for the learning objective of a large class of convolutional neural networks (CNNs). Specifically, we consider CNNs that employ piecewise linear non-linearities such as the commonly used ReLU and max-pool, and an SVM classifier as the final layer. The key observation of our approach is that the problem corresponding to the parameter estimation of a layer can be formulated as a difference-of-convex (DC) program, which happens to be a latent structured SVM. We optimize the DC program using the concave- convex procedure, which requires us to iteratively solve a structured SVM problem. To this end, we extend the block-coordinate Frank-Wolfe (BCFW) algorithm in three important ways: (i) we include a trust-region for the parameters, which allows us to use the previous parameters as an initialization; (ii) we reduce the memory requirement of BCFW by potentially several orders of magnitude for the dense layers, which enables us to learn a large set of parameters; and (iii) we observe that, empirically, the optimal solution of the structured SVM problem can be obtained efficiently by solving a related, but significantly easier, multi-class SVM problem. Using publicly available data sets, we show that our approach outperforms the state of the art variants of backpropagation, and is also more robust to the hyperparameters of the learning objective.", "histories": [["v1", "Mon, 7 Nov 2016 17:41:20 GMT  (80kb,D)", "https://arxiv.org/abs/1611.02185v1", null], ["v2", "Tue, 8 Nov 2016 23:54:26 GMT  (103kb,D)", "http://arxiv.org/abs/1611.02185v2", null], ["v3", "Sat, 17 Dec 2016 09:19:58 GMT  (216kb,D)", "http://arxiv.org/abs/1611.02185v3", null], ["v4", "Mon, 30 Jan 2017 16:42:22 GMT  (477kb,D)", "http://arxiv.org/abs/1611.02185v4", null], ["v5", "Mon, 6 Mar 2017 16:21:35 GMT  (478kb,D)", "http://arxiv.org/abs/1611.02185v5", null]], "reviews": [], "SUBJECTS": "cs.LG", "authors": ["leonard berrada", "rew zisserman", "m pawan kumar"], "accepted": true, "id": "1611.02185"}, "pdf": {"name": "1611.02185.pdf", "metadata": {"source": "CRF", "title": "TRUSTING SVM FOR PIECEWISE LINEAR CNNS", "authors": ["Leonard Berrada", "Andrew Zisserman", "Pawan Kumar"], "emails": ["lberrada@robots.ox.ac.uk", "az@robots.ox.ac.uk", "pawan@robots.ox.ac.uk"], "sections": [{"heading": "1 INTRODUCTION", "text": "In fact, most people who live and work in the US are able to surpass themselves, \"he told the German Press Agency.\" I don't think I'm able to surpass myself. \"He pointed out that most of them are able to surpass themselves:\" I don't think I'm able to surpass myself. \"He pointed out that most of them are able to surpass themselves,\" but I don't think I'm able to surpass myself. \""}, {"heading": "2 RELATED WORK", "text": "In fact, it is the case that most people are in a position to go into a different world, in which they are able to move, in which they move, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they, in which they live, in which they, in which they live, in which they, in which they, in which they live, in which they, in which they live."}, {"heading": "3 PIECEWISE LINEAR CONVOLUTIONAL NEURAL NETWORKS", "text": "A piecemeal Convolutionary Neural Network (PL-CNN), therefore, consists of a series of revolutionary layers followed by a series of dense layers that provide an accurate representation of an input image. Each layer of the network performs two operations: a linear transformation (i.e., a convolution or a matrix multiplication), followed by a piecemeal linear non-linear operation such as ReLU or Max Pool. The resulting representation of the image is used for classification via SVM. In the rest of this section, we offer a formal description of the PL-CNN.Piecewise Linear Functions. A piecemeal linear (PL) function f (u) is a function of the following form (Melzer, 1986): f (u) = max i (m) {a > i}."}, {"heading": "4 PARAMETER ESTIMATION FOR PL-CNN", "text": "To enable layer-by-layer optimization of PL CNNs, we show that the parameter estimation of a layer can be formulated as a difference-by-convex (DC) program (subsection 4.1), which allows us to apply the concave-convex method that solves a number of convex optimization problems (subsection 4.2). We show that each convex problem is very similar to a structured SVM target that can be addressed by the powerful Frank Wolfe block coordinate algorithm (BCFW). We expand BCFW to improve its initialization, time complexity, and memory requirements, enabling its use in learning PL CNNs (subsection 4.3). For clarity, we provide only sketches of the evidence for those statements that are necessary to understand the work. Detailed evidence of the remaining propositions can be found in the appendix."}, {"heading": "4.1 LAYERWISE OPTIMIZATION AS A DC PROGRAM", "text": "Considering the values of the DC program, which enables efficient optimization through iterative use of the CFW functions, the learning objective (3) is the standard problem of SVM. (3) In other words, it is a convex optimization problem with several efficient solvers (Tsochantaridis et al., 2004; the optimization of the last layer is a simple computer problem. In contrast, the optimization of the parameters of a convex or a convex layer l is not the result of a convex program. In general, this problem can be arbitrarily difficult to solve."}, {"heading": "4.2 CONCAVE-CONVEX PROCEDURE", "text": "This results from the fact that the upper limit of empirical risk has a PL function and therefore the difference of two different PL functions cannot be taken into account (Melzer, 1986). (In addition, the top level of the WL function is also a conventional function of the WL function, which allows us to obtain an approximate solution to the problem (4) using the iterative convex method (CCCP) (Yuille & Rangarajan, 2002).Algorithm 1 describes the most important steps of CCCP. In step 3 we imply the best value of the latent variable correspondence of the respective class yi. This imputation corresponds to the linearization of CCCP. The selected latent variables correspond to a choice of activation of the non-linear ones of the network, and therefore defines a problem of activation of truth."}, {"heading": "4.3 IMPROVING THE BCFW ALGORITHM", "text": "Since the BCFW algorithms have the following effects in their objective function (Parikh Boyd, 2014), they require further enhancements in order to be suitable for the formation of a PL-CNN. Below, we present three such extensions, each of which reduces the initialization, memory requirements and time complexity of the BCFW algorithm to 0. The reason for this initialization is that it is possible to calculate the dual variables corresponding to the 0 primary variables. However, since our algorithms visit each layer of the network several times, it would be desirable to initialize its parameters using its current value. To achieve this, we present a confidence region in the problem (7), or equivalent, a \"2 standardized term in its objective function (Parikh Boyd, 2014)."}, {"heading": "5 EXPERIMENTS", "text": "Our experiments are designed to evaluate the ability of LW-SVM (Layer-Wise SVM, our method) and the SGD baselines to optimize the problem (3). To compare LW-SVM with the most advanced variants of back propagation, we look at training and test accuracy as well as training target value. Unlike Dropout, which effectively learns an ensemble model, we learn a single model using any baseline optimization algorithm. All experiments are performed on a GPU (Nvidia Titan X) and use Theano (Bergstra et al., 2010; Bastien et al., 2012). We compare LW-SVM with Adagrad, Adadelta and Adam. For all data sets, we start with a good solution provided by these solvers, and refine it with LW-SVM. We then check if a longer life of the SGD solver reaches the same level of performance."}, {"heading": "5.1 MNIST DATA SET", "text": "The architecture used in this experiment is illustrated in Figure 1. The number of epochs is set at 200, 100 and 100 for Adagrad, Adadelta and Adam - Adagrad, and we get more epochs than we have observed. Convergence took longer. We then use LWSVM and compare the results in terms of training target, training accuracy and test accuracy. We run the solvers up to 500 epochs to verify that we have not stopped the optimization prematurely. Regulatory hyperparameters \u03bb and the initial learning rate are selected by cross-validation."}, {"heading": "5.2 CIFAR DATA SETS", "text": "The data sets & architectures The CIFAR-10 / 100 data sets consist of 60,000 32 x 32 RGB nature images with 10 / 100 classes each (Krizhevsky, 2009). We split the training sets into 45,000 training samples and 5,000 validation samples in both cases. Images are centered and normalized, and we do not use data amplification. To get a sufficiently strong baseline, we use (i) pre-training with a Softmax loss and a cross-entropy loss, and (ii) batch normalization (BN) layers before each non-linearity test. We have experimentally found that pre-training with a Softmax layer followed by a cross-entropy loss resulted in better behavior and results than using an SVM loss alone. Baselines are trained with batch normalization. Once they are converged, the estimated mean and standard deviations are fixed, as if it becomes a test line, the SVM architecture can become a time line."}, {"heading": "5.3 IMAGENET DATA SET", "text": "For this experiment, we use a VGG-16 network (Configuration D in (Simonyan & Zisserman, 2015)), starting with a pre-trained model that is publicly available online, and matching each of the dense layers and the final SVM layer with the LW-SVM algorithm. This experiment is designed to test the scalability of LW-SVM to large datasets and large networks, rather than comparing it to optimization baselines as before - indeed, for each baseline, with proper convergence taking a very long time, as in previous experiments."}, {"heading": "6 DISCUSSION", "text": "We presented a novel optimization algorithm for a large and useful class of conventional networks, which we call PL-CNNs. Our main observations are that optimizing the parameters of a layer of a PL-CNNs is tantamount to solving a structured SVM problem. Of course, the problem is that it is a pure approach to learning PL-CNNs, with a convex structured SVM objective in any case, which allows us to take advantage of the advances in structured SVM optimization over the past decade."}, {"heading": "ACKNOWLEDGMENTS", "text": "This work was supported by the EPSRC AIMS CDT Grant EP / L015987 / 1, the EPSRC Programme Grant Seebibyte EP / M013774 / 1 and Yougov. Many thanks to A. Desmaison, R. Bunel and D. Bouchacourt for the helpful discussions."}, {"heading": "A PIECEWISE LINEAR FUNCTIONS", "text": "After the definition of (Melzer, 1986) we can write any function as the difference of two point-by-point maxima of linear functions (max.): g (v) = max j (m) - max k (m) - max k (m) - max k (m) - max k (m) - maximum maximum maximum linear functions. Then: f (u) = g (g) = g (u), \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 gn (u) - max k (m) - max k (m) - maximum (m) - maximum (m) - maximum (m) - maximum (m) - (m) - (m) - maximum (m) - (m) - (m) - (m) - (m) - (m) - (m) - (m) - (m - - m) (n - (n - n), maximum k (m - (m) (m - (m) (m) (m - (m - m) (m - (m) (m) (m - (m) (m - - - m) (m - (m) (m) (n - n - n - n) (n (n -), maximum k (m - - m) (m - (m) (m) (m - (m) (m) (m) (m - - m - m - (m) (m) (m - (m) (m - m - - - m) (m) (m - (m) (m - (m) (m - (m - - m) (m - m - m) (m) (m - (m) (m - m) (m - (m - m - m - m - - m - m - m) (m) (m - m) (m - m) (m - m - m - m - (m - m - m) (m (m) (m - m - m - m) (m - m - m - m) (m - m - m - m - m) (m - m - m) (m - m (m) (m - m - m (m - m - m - m - m - m - m - m - m) (m - m) (m - m) (m - m - m) (m -"}, {"heading": "B COMPUTING THE FEATURE VECTORS", "text": "This year, it is so far that it is not even a year ago."}, {"heading": "C EXPERIMENTAL DETAILS", "text": "Hyper parameters The hyper parameters are cross-validated with a search for potencies of 10. In this section \u03b7 will denote the initial learning rate. We denote the Softmax + cross-entropy loss by SCE, while SVM denotes the usual support vector machine losses. It should be noted that the hyper parameters are the same for both CIFAR-10 and CIFAR-100 for any combination of solver and loss. This is useful because the initial learning rate mainly depends on the architecture of the network (and not so much on which images are fed into that network), which is very similar to the experiments with the CIFAR-10 and CIFAR-100 datasets."}, {"heading": "D SVM FORMULATION & DUAL DERIVATION", "text": "We assume that we will get a record of N samples, for each sample i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i"}, {"heading": "E SENSITIVITY OF SGD ALGORITHMS", "text": "Here we discuss some of the weaknesses of the SGD-based algorithms that we have encountered in practice for our learning goal function. These behaviors have been observed in the case of PL-CNNs and generally do not appear in different architectures (in particular, the failure to learn with high regularization disappears with the use of batch normalization layers).E.1 INITIAL LEARNING RATEAs mentioned in the experiment section, the choice of the initial learning rate is critical to the good performance of all Adagrad, Noble Delta and Adam. If the learning rate is too high, the network learns nothing and the training and validation of accuracies remain at random level. If it is too low, the network may need a significantly greater number of eras to convert. E.2 FAILURES TO LEARNRegularization If the regulation rate is too high, it is set to a value of 0.01 or higher."}], "references": [{"title": "Input convex neural networks", "author": ["Brandon Amos", "Lei Xu", "J. Zico Kolter"], "venue": "arXiv preprint arXiv:1609.07152,", "citeRegEx": "Amos et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Amos et al\\.", "year": 2016}, {"title": "Greedy layer-wise training of deep networks", "author": ["Yoshua Bengio", "Pascal Lamblin", "Dan Popovici", "Hugo Larochelle"], "venue": "Conference on Neural Information Processing Systems,", "citeRegEx": "Bengio et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Bengio et al\\.", "year": 2007}, {"title": "Theano: a CPU and GPU math expression compiler", "author": ["James Bergstra", "Olivier Breuleux", "Fr\u00e9d\u00e9ric Bastien", "Pascal Lamblin", "Razvan Pascanu", "Guillaume Desjardins", "Joseph Turian", "David Warde-Farley", "Yoshua Bengio"], "venue": "Python for Scientific Computing Conference (SciPy),", "citeRegEx": "Bergstra et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Bergstra et al\\.", "year": 2010}, {"title": "Adaptive subgradient methods for online learning and stochastic optimization", "author": ["John Duchi", "Elad Hazan", "Yoram Singer"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Duchi et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Duchi et al\\.", "year": 2011}, {"title": "Reliably learning the ReLU in polynomial time", "author": ["Surbhi Goel", "Varun Kanade", "Adam Klivans", "Justin Thaler"], "venue": "arXiv preprint arXiv:1611.10258,", "citeRegEx": "Goel et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Goel et al\\.", "year": 2016}, {"title": "Deep Learning", "author": ["Ian Goodfellow", "Yoshua Bengio", "Aaron Courville"], "venue": null, "citeRegEx": "Goodfellow et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Goodfellow et al\\.", "year": 2016}, {"title": "Improper deep kernels", "author": ["Uri Heinemann", "Roi Livni", "Elad Eban", "Gal Elidan", "Amir Globerson"], "venue": "International Conference on Artificial Intelligence and Statistics,", "citeRegEx": "Heinemann et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Heinemann et al\\.", "year": 2016}, {"title": "A fast learning algorithm for deep belief nets", "author": ["Geoffrey E Hinton", "Simon Osindero", "Yee-Whye Teh"], "venue": "Neural computation,", "citeRegEx": "Hinton et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Hinton et al\\.", "year": 2006}, {"title": "DC programming: overview", "author": ["Reiner Horst", "Nguyen V. Thoai"], "venue": "Journal of Optimization Theory and Applications,", "citeRegEx": "Horst and Thoai.,? \\Q1999\\E", "shortCiteRegEx": "Horst and Thoai.", "year": 1999}, {"title": "Batch normalization: Accelerating deep network training by reducing internal covariate shift", "author": ["Sergey Ioffe", "Christian Szegedy"], "venue": "International Conference on Machine Learning,", "citeRegEx": "Ioffe and Szegedy.,? \\Q2015\\E", "shortCiteRegEx": "Ioffe and Szegedy.", "year": 2015}, {"title": "Cutting-plane training of structural SVMs", "author": ["Thorsten Joachims", "Thomas Finley", "Chun-Nam John Yu"], "venue": "Machine Learning,", "citeRegEx": "Joachims et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Joachims et al\\.", "year": 2009}, {"title": "Adam: A method for stochastic optimization", "author": ["Diederik Kingma", "Jimmy Ba"], "venue": "International Conference on Learning Representations,", "citeRegEx": "Kingma and Ba.,? \\Q2015\\E", "shortCiteRegEx": "Kingma and Ba.", "year": 2015}, {"title": "Learning multiple layers of features from tiny images", "author": ["Alex Krizhevsky"], "venue": "Technical report, University of Toronto,", "citeRegEx": "Krizhevsky.,? \\Q2009\\E", "shortCiteRegEx": "Krizhevsky.", "year": 2009}, {"title": "Block-coordinate FrankWolfe optimization for structural SVMs", "author": ["Simon Lacoste-Julien", "Martin Jaggi", "Mark Schmidt", "Patrick Pletscher"], "venue": "International Conference on Machine Learning,", "citeRegEx": "Lacoste.Julien et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Lacoste.Julien et al\\.", "year": 2013}, {"title": "Difference target propagation", "author": ["Dong-Hyun Lee", "Saizheng Zhang", "Asja Fischer", "Yoshua Bengio"], "venue": "Joint European Conference on Machine Learning and Knowledge Discovery in Databases,", "citeRegEx": "Lee et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Lee et al\\.", "year": 2015}, {"title": "Optimizing neural networks with Kronecker-factored approximate curvature", "author": ["James Martens", "Roger Grosse"], "venue": "International Conference on Machine Learning,", "citeRegEx": "Martens and Grosse.,? \\Q2015\\E", "shortCiteRegEx": "Martens and Grosse.", "year": 2015}, {"title": "Training deep and recurrent networks with hessian-free optimization", "author": ["James Martens", "Ilya Sutskever"], "venue": "Neural Networks: Tricks of the Trade,", "citeRegEx": "Martens and Sutskever.,? \\Q2012\\E", "shortCiteRegEx": "Martens and Sutskever.", "year": 2012}, {"title": "On the expressibility of piecewise-linear continuous functions as the difference of two piecewise-linear convex functions", "author": ["D. Melzer"], "venue": null, "citeRegEx": "Melzer.,? \\Q1986\\E", "shortCiteRegEx": "Melzer.", "year": 1986}, {"title": "Partial linearization based optimization for multi-class SVM", "author": ["Pritish Mohapatra", "Puneet Dokania", "CV Jawahar", "M Pawan Kumar"], "venue": "European Conference on Computer Vision,", "citeRegEx": "Mohapatra et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Mohapatra et al\\.", "year": 2016}, {"title": "Riemannian metrics for neural networks", "author": ["Yann Ollivier"], "venue": "Information and Inference: a Journal of the IMA,", "citeRegEx": "Ollivier.,? \\Q2013\\E", "shortCiteRegEx": "Ollivier.", "year": 2013}, {"title": "Minding the gaps for block Frank-Wolfe optimization of structured SVMs", "author": ["Anton Osokin", "Jean-Baptiste Alayrac", "Isabella Lukasewitz", "Puneet Dokania", "Simon LacosteJulien"], "venue": "Inernational Conference on Machine Learning,", "citeRegEx": "Osokin et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Osokin et al\\.", "year": 2016}, {"title": "Learning representations by backpropagating errors", "author": ["David Rumelhart", "Geoffrey Hinton", "Ronald Williams"], "venue": "Nature,", "citeRegEx": "Rumelhart et al\\.,? \\Q1986\\E", "shortCiteRegEx": "Rumelhart et al\\.", "year": 1986}, {"title": "A multi-plane block-coordinate Frank-Wolfe algorithm for training structural SVMs with a costly max-oracle", "author": ["Neel Shah", "Vladimir Kolmogorov", "Christoph H. Lampert"], "venue": "Conference on Computer Vision and Pattern Recognition,", "citeRegEx": "Shah et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Shah et al\\.", "year": 2015}, {"title": "Pegasos: Primal estimated sub-gradient solver for SVM", "author": ["Shai Shalev-Shwartz", "Yoram Singer", "Nathan Srebro"], "venue": "International Conference on Machine Learning,", "citeRegEx": "Shalev.Shwartz et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Shalev.Shwartz et al\\.", "year": 2009}, {"title": "Very deep convolutional networks for large-scale image recognition", "author": ["Karen Simonyan", "Andrew Zisserman"], "venue": "International Conference on Learning Representations,", "citeRegEx": "Simonyan and Zisserman.,? \\Q2015\\E", "shortCiteRegEx": "Simonyan and Zisserman.", "year": 2015}, {"title": "Dropout: a simple way to prevent neural networks from overfitting", "author": ["Nitish Srivastava", "Geoffrey Hinton", "Alex Krizhevsky", "Ilya Sutskever", "Ruslan Salakhutdinov"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Srivastava et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Srivastava et al\\.", "year": 2014}, {"title": "Training neural networks without gradients: A scalable ADMM approach", "author": ["Gavin Taylor", "Ryan Burmeister", "Zheng Xu", "Bharat Singh", "Ankit Patel", "Tom Goldstein"], "venue": "International Conference on Machine Learning,", "citeRegEx": "Taylor et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Taylor et al\\.", "year": 2016}, {"title": "Support vector machine learning for interdependent and structured output spaces", "author": ["Ioannis Tsochantaridis", "Thomas Hofmann", "Thorsten Joachims", "Yasemin Altun"], "venue": "International Conference on Machine Learning,", "citeRegEx": "Tsochantaridis et al\\.,? \\Q2004\\E", "shortCiteRegEx": "Tsochantaridis et al\\.", "year": 2004}, {"title": "Learning structural SVMs with latent variables", "author": ["Chun-Nam John Yu", "Thorsten Joachims"], "venue": "International Conference on Machine Learning,", "citeRegEx": "Yu and Joachims.,? \\Q2009\\E", "shortCiteRegEx": "Yu and Joachims.", "year": 2009}, {"title": "The concave-convex procedure (CCCP)", "author": ["Alan L. Yuille", "Anand Rangarajan"], "venue": "Conference on Neural Information Processing Systems,", "citeRegEx": "Yuille and Rangarajan.,? \\Q2002\\E", "shortCiteRegEx": "Yuille and Rangarajan.", "year": 2002}, {"title": "ADADELTA: an adaptive learning rate method", "author": ["Matthew Zeiler"], "venue": null, "citeRegEx": "Zeiler.,? \\Q2012\\E", "shortCiteRegEx": "Zeiler.", "year": 2012}, {"title": "Convexified convolutional neural networks", "author": ["Yuchen Zhang", "Percy Liang", "Martin J. Wainwright"], "venue": "arXiv preprint arXiv:1609.01000,", "citeRegEx": "Zhang et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Zhang et al\\.", "year": 2016}], "referenceMentions": [{"referenceID": 21, "context": "The backpropagation algorithm is commonly employed to estimate the parameters of a convolutional neural network (CNN) using a supervised training data set (Rumelhart et al., 1986).", "startOffset": 155, "endOffset": 179}, {"referenceID": 3, "context": "Choosing the learning rate thus remains an open issue, with the state-of-the-art algorithms suggesting adaptive learning rates (Duchi et al., 2011; Zeiler, 2012; Kingma & Ba, 2015).", "startOffset": 127, "endOffset": 180}, {"referenceID": 30, "context": "Choosing the learning rate thus remains an open issue, with the state-of-the-art algorithms suggesting adaptive learning rates (Duchi et al., 2011; Zeiler, 2012; Kingma & Ba, 2015).", "startOffset": 127, "endOffset": 180}, {"referenceID": 25, "context": "In addition, techniques such as batch normalization (Ioffe & Szegedy, 2015) and dropout (Srivastava et al., 2014) have been introduced to respectively reduce the sensitivity to the learning rate and to prevent from overfitting.", "startOffset": 88, "endOffset": 113}, {"referenceID": 13, "context": "To this end, we use the powerful block-coordinate Frank-Wolfe (BCFW) algorithm (Lacoste-Julien et al., 2013), which solves the dual of the convex program iteratively by computing the conditional gradients corresponding to a subset of training samples.", "startOffset": 79, "endOffset": 108}, {"referenceID": 21, "context": "Compared to backpropagation (Rumelhart et al., 1986) or its variants (Duchi et al.", "startOffset": 28, "endOffset": 52}, {"referenceID": 3, "context": ", 1986) or its variants (Duchi et al., 2011; Zeiler, 2012; Kingma & Ba, 2015), our algorithm offers three advantages.", "startOffset": 24, "endOffset": 77}, {"referenceID": 30, "context": ", 1986) or its variants (Duchi et al., 2011; Zeiler, 2012; Kingma & Ba, 2015), our algorithm offers three advantages.", "startOffset": 24, "endOffset": 77}, {"referenceID": 13, "context": "Third, since the only step-size required in our approach comes while solving the SVM dual, we can use the optimal step-size that is computed analytically during each iteration of BCFW (Lacoste-Julien et al., 2013).", "startOffset": 184, "endOffset": 213}, {"referenceID": 7, "context": "While some of the early successful approaches for the optimization of deep neural networks relied on greedy layer-wise training (Hinton et al., 2006; Bengio et al., 2007), most currently used methods are variants of backpropagation (Rumelhart et al.", "startOffset": 128, "endOffset": 170}, {"referenceID": 1, "context": "While some of the early successful approaches for the optimization of deep neural networks relied on greedy layer-wise training (Hinton et al., 2006; Bengio et al., 2007), most currently used methods are variants of backpropagation (Rumelhart et al.", "startOffset": 128, "endOffset": 170}, {"referenceID": 21, "context": ", 2007), most currently used methods are variants of backpropagation (Rumelhart et al., 1986) with adaptive learning rates, as discussed in the introduction.", "startOffset": 69, "endOffset": 93}, {"referenceID": 5, "context": "When applied to the non-convex CNN optimization problem, Adagrad may converge prematurely due to a rapid decrease in the learning rate (Goodfellow et al., 2016).", "startOffset": 135, "endOffset": 160}, {"referenceID": 30, "context": "In order to prevent this behavior, the Adadelta algorithm (Zeiler, 2012) makes the decay of the learning rate slower.", "startOffset": 58, "endOffset": 72}, {"referenceID": 1, "context": ", 2006; Bengio et al., 2007), most currently used methods are variants of backpropagation (Rumelhart et al., 1986) with adaptive learning rates, as discussed in the introduction. At every iteration, backpropagation performs a forward pass and a backward pass on the network, and updates the parameters of each layer by stochastic or mini-batch gradient descent. This makes the choice of the learning rate critical for efficient optimization. Duchi et al. (2011) have proposed the Adagrad convex solver, which adapts the learning rate for every direction and takes into account past updates.", "startOffset": 8, "endOffset": 462}, {"referenceID": 1, "context": ", 2006; Bengio et al., 2007), most currently used methods are variants of backpropagation (Rumelhart et al., 1986) with adaptive learning rates, as discussed in the introduction. At every iteration, backpropagation performs a forward pass and a backward pass on the network, and updates the parameters of each layer by stochastic or mini-batch gradient descent. This makes the choice of the learning rate critical for efficient optimization. Duchi et al. (2011) have proposed the Adagrad convex solver, which adapts the learning rate for every direction and takes into account past updates. Adagrad changes the learning rate to favor steps in gradient directions that have not been observed frequently in past updates. When applied to the non-convex CNN optimization problem, Adagrad may converge prematurely due to a rapid decrease in the learning rate (Goodfellow et al., 2016). In order to prevent this behavior, the Adadelta algorithm (Zeiler, 2012) makes the decay of the learning rate slower. It is worth noting that this fix is empirical, and to the best of our knowledge, provides no theoretical guarantees. Kingma & Ba (2015) propose a different scheme for the learning rate, called Adam, which uses an online estimation of the first and second moments of the gradients to provide centered and normalized updates.", "startOffset": 8, "endOffset": 1135}, {"referenceID": 1, "context": ", 2006; Bengio et al., 2007), most currently used methods are variants of backpropagation (Rumelhart et al., 1986) with adaptive learning rates, as discussed in the introduction. At every iteration, backpropagation performs a forward pass and a backward pass on the network, and updates the parameters of each layer by stochastic or mini-batch gradient descent. This makes the choice of the learning rate critical for efficient optimization. Duchi et al. (2011) have proposed the Adagrad convex solver, which adapts the learning rate for every direction and takes into account past updates. Adagrad changes the learning rate to favor steps in gradient directions that have not been observed frequently in past updates. When applied to the non-convex CNN optimization problem, Adagrad may converge prematurely due to a rapid decrease in the learning rate (Goodfellow et al., 2016). In order to prevent this behavior, the Adadelta algorithm (Zeiler, 2012) makes the decay of the learning rate slower. It is worth noting that this fix is empirical, and to the best of our knowledge, provides no theoretical guarantees. Kingma & Ba (2015) propose a different scheme for the learning rate, called Adam, which uses an online estimation of the first and second moments of the gradients to provide centered and normalized updates. However all these methods still require the tuning of the initial learning rate to perform well. Second-order and natural gradient optimization methods have also been a subject of attention. The focus in this line of work has been to come up with appropriate approximations to make the updates cheaper. Martens & Sutskever (2012) suggested a Hessian-free second order optimization using finite differences to approximate the Hessian and conjugate gradient to compute the update.", "startOffset": 8, "endOffset": 1653}, {"referenceID": 1, "context": ", 2006; Bengio et al., 2007), most currently used methods are variants of backpropagation (Rumelhart et al., 1986) with adaptive learning rates, as discussed in the introduction. At every iteration, backpropagation performs a forward pass and a backward pass on the network, and updates the parameters of each layer by stochastic or mini-batch gradient descent. This makes the choice of the learning rate critical for efficient optimization. Duchi et al. (2011) have proposed the Adagrad convex solver, which adapts the learning rate for every direction and takes into account past updates. Adagrad changes the learning rate to favor steps in gradient directions that have not been observed frequently in past updates. When applied to the non-convex CNN optimization problem, Adagrad may converge prematurely due to a rapid decrease in the learning rate (Goodfellow et al., 2016). In order to prevent this behavior, the Adadelta algorithm (Zeiler, 2012) makes the decay of the learning rate slower. It is worth noting that this fix is empirical, and to the best of our knowledge, provides no theoretical guarantees. Kingma & Ba (2015) propose a different scheme for the learning rate, called Adam, which uses an online estimation of the first and second moments of the gradients to provide centered and normalized updates. However all these methods still require the tuning of the initial learning rate to perform well. Second-order and natural gradient optimization methods have also been a subject of attention. The focus in this line of work has been to come up with appropriate approximations to make the updates cheaper. Martens & Sutskever (2012) suggested a Hessian-free second order optimization using finite differences to approximate the Hessian and conjugate gradient to compute the update. Martens & Grosse (2015) derive an approximation of the Fisher matrix inverse, which provides a more efficient method for natural gradient descent.", "startOffset": 8, "endOffset": 1826}, {"referenceID": 1, "context": ", 2006; Bengio et al., 2007), most currently used methods are variants of backpropagation (Rumelhart et al., 1986) with adaptive learning rates, as discussed in the introduction. At every iteration, backpropagation performs a forward pass and a backward pass on the network, and updates the parameters of each layer by stochastic or mini-batch gradient descent. This makes the choice of the learning rate critical for efficient optimization. Duchi et al. (2011) have proposed the Adagrad convex solver, which adapts the learning rate for every direction and takes into account past updates. Adagrad changes the learning rate to favor steps in gradient directions that have not been observed frequently in past updates. When applied to the non-convex CNN optimization problem, Adagrad may converge prematurely due to a rapid decrease in the learning rate (Goodfellow et al., 2016). In order to prevent this behavior, the Adadelta algorithm (Zeiler, 2012) makes the decay of the learning rate slower. It is worth noting that this fix is empirical, and to the best of our knowledge, provides no theoretical guarantees. Kingma & Ba (2015) propose a different scheme for the learning rate, called Adam, which uses an online estimation of the first and second moments of the gradients to provide centered and normalized updates. However all these methods still require the tuning of the initial learning rate to perform well. Second-order and natural gradient optimization methods have also been a subject of attention. The focus in this line of work has been to come up with appropriate approximations to make the updates cheaper. Martens & Sutskever (2012) suggested a Hessian-free second order optimization using finite differences to approximate the Hessian and conjugate gradient to compute the update. Martens & Grosse (2015) derive an approximation of the Fisher matrix inverse, which provides a more efficient method for natural gradient descent. Ollivier (2013) explore a set of Riemannian methods based on natural gradient descent and quasi-Newton methods to guarantee reparametrization invariance of the problem.", "startOffset": 8, "endOffset": 1965}, {"referenceID": 1, "context": ", 2006; Bengio et al., 2007), most currently used methods are variants of backpropagation (Rumelhart et al., 1986) with adaptive learning rates, as discussed in the introduction. At every iteration, backpropagation performs a forward pass and a backward pass on the network, and updates the parameters of each layer by stochastic or mini-batch gradient descent. This makes the choice of the learning rate critical for efficient optimization. Duchi et al. (2011) have proposed the Adagrad convex solver, which adapts the learning rate for every direction and takes into account past updates. Adagrad changes the learning rate to favor steps in gradient directions that have not been observed frequently in past updates. When applied to the non-convex CNN optimization problem, Adagrad may converge prematurely due to a rapid decrease in the learning rate (Goodfellow et al., 2016). In order to prevent this behavior, the Adadelta algorithm (Zeiler, 2012) makes the decay of the learning rate slower. It is worth noting that this fix is empirical, and to the best of our knowledge, provides no theoretical guarantees. Kingma & Ba (2015) propose a different scheme for the learning rate, called Adam, which uses an online estimation of the first and second moments of the gradients to provide centered and normalized updates. However all these methods still require the tuning of the initial learning rate to perform well. Second-order and natural gradient optimization methods have also been a subject of attention. The focus in this line of work has been to come up with appropriate approximations to make the updates cheaper. Martens & Sutskever (2012) suggested a Hessian-free second order optimization using finite differences to approximate the Hessian and conjugate gradient to compute the update. Martens & Grosse (2015) derive an approximation of the Fisher matrix inverse, which provides a more efficient method for natural gradient descent. Ollivier (2013) explore a set of Riemannian methods based on natural gradient descent and quasi-Newton methods to guarantee reparametrization invariance of the problem. Desjardins et al. (2015) demonstrate a scaled up natural gradient descent method by training on the ImageNet data set (Russakovsky et al.", "startOffset": 8, "endOffset": 2143}, {"referenceID": 0, "context": "In (Amos et al., 2016), the authors identify convex problems for the inference task, when the neural network is a convex function of some of its inputs.", "startOffset": 3, "endOffset": 22}, {"referenceID": 22, "context": "For example, Taylor et al. (2016) use ADMM for massive distribution of computation in a layer-wise fashion, and in particular their method will yield closed-form updates for any PL-CNN.", "startOffset": 13, "endOffset": 34}, {"referenceID": 11, "context": "Lee et al. (2015) propose to use targets instead of gradients to propagate information through the network, which could help to extend our algorithm.", "startOffset": 0, "endOffset": 18}, {"referenceID": 11, "context": "Lee et al. (2015) propose to use targets instead of gradients to propagate information through the network, which could help to extend our algorithm. Zhang et al. (2016) derive a convex relaxation for the learning objective for a restricted class of CNNs, which also relies on solving an approximate convex problem.", "startOffset": 0, "endOffset": 170}, {"referenceID": 0, "context": "In (Amos et al., 2016), the authors identify convex problems for the inference task, when the neural network is a convex function of some of its inputs. With a more theoretical approach, Goel et al. (2016) propose an algorithm to learn shallow ReLU nets with guarantees of time convergence and generalization error.", "startOffset": 4, "endOffset": 206}, {"referenceID": 0, "context": "In (Amos et al., 2016), the authors identify convex problems for the inference task, when the neural network is a convex function of some of its inputs. With a more theoretical approach, Goel et al. (2016) propose an algorithm to learn shallow ReLU nets with guarantees of time convergence and generalization error. Heinemann et al. (2016) show that a subclass of neural networks can be modeled as an improper kernel, which then reduces the learning problem to a simple SVM with the constructed kernel.", "startOffset": 4, "endOffset": 340}, {"referenceID": 17, "context": "A piecewise linear (PL) function f(u) is a function of the following form (Melzer, 1986): f(u) = max i\u2208[m] {ai u} \u2212max j\u2208[n] {bj u}, (1)", "startOffset": 74, "endOffset": 88}, {"referenceID": 27, "context": "In other words, it is a convex optimization problem with several efficient solvers (Tsochantaridis et al., 2004; Joachims et al., 2009; Shalev-Shwartz et al., 2009), including the BCFW algorithm (Lacoste-Julien et al.", "startOffset": 83, "endOffset": 164}, {"referenceID": 10, "context": "In other words, it is a convex optimization problem with several efficient solvers (Tsochantaridis et al., 2004; Joachims et al., 2009; Shalev-Shwartz et al., 2009), including the BCFW algorithm (Lacoste-Julien et al.", "startOffset": 83, "endOffset": 164}, {"referenceID": 23, "context": "In other words, it is a convex optimization problem with several efficient solvers (Tsochantaridis et al., 2004; Joachims et al., 2009; Shalev-Shwartz et al., 2009), including the BCFW algorithm (Lacoste-Julien et al.", "startOffset": 83, "endOffset": 164}, {"referenceID": 13, "context": ", 2009), including the BCFW algorithm (Lacoste-Julien et al., 2013).", "startOffset": 38, "endOffset": 67}, {"referenceID": 17, "context": "This follows from the fact that the upper bound of the empirical risk is a PL function, and can therefore be expressed as the difference of two convex PL functions (Melzer, 1986).", "startOffset": 164, "endOffset": 178}, {"referenceID": 13, "context": "In order to solve the convex program (7), which corresponds to a structured SVM problem, we make use of the powerful BCFW algorithm (Lacoste-Julien et al., 2013) that solves its dual via conditional gradients.", "startOffset": 132, "endOffset": 161}, {"referenceID": 3, "context": "This is once again in stark contrast to backpropagation, where the estimation of the step-size is still an active area of research (Duchi et al., 2011; Zeiler, 2012; Kingma & Ba, 2015).", "startOffset": 131, "endOffset": 184}, {"referenceID": 30, "context": "This is once again in stark contrast to backpropagation, where the estimation of the step-size is still an active area of research (Duchi et al., 2011; Zeiler, 2012; Kingma & Ba, 2015).", "startOffset": 131, "endOffset": 184}, {"referenceID": 3, "context": "This is once again in stark contrast to backpropagation, where the estimation of the step-size is still an active area of research (Duchi et al., 2011; Zeiler, 2012; Kingma & Ba, 2015). As shown by Lacoste-Julien et al. (2013), given the current estimate of the parameters W , the conditional gradient of the dual of program (7) with respect to a training sample (xi, yi) can be obtained by solving the following problem: (\u0177i, \u0125i) = argmax \u0233\u2208Y,h\u2208H (W )\u03a8(xi, \u0233,h) + \u2206(\u0233, yi).", "startOffset": 132, "endOffset": 227}, {"referenceID": 13, "context": "We refer the interested reader to (Lacoste-Julien et al., 2013) for further details.", "startOffset": 34, "endOffset": 63}, {"referenceID": 2, "context": "All experiments are conducted on a GPU (Nvidia Titan X) and use Theano (Bergstra et al., 2010; Bastien et al., 2012).", "startOffset": 71, "endOffset": 116}, {"referenceID": 12, "context": "Data sets & Architectures The CIFAR-10/100 data sets are comprised of 60,000 RGB natural images of size 32\u00d7 32 with 10/100 classes (Krizhevsky, 2009)).", "startOffset": 131, "endOffset": 149}, {"referenceID": 27, "context": "It is worth noting that other approaches for solving structured SVM problems, such as cuttingplane algorithms (Tsochantaridis et al., 2004; Joachims et al., 2009) and stochastic subgradient descent (Shalev-Shwartz et al.", "startOffset": 110, "endOffset": 162}, {"referenceID": 10, "context": "It is worth noting that other approaches for solving structured SVM problems, such as cuttingplane algorithms (Tsochantaridis et al., 2004; Joachims et al., 2009) and stochastic subgradient descent (Shalev-Shwartz et al.", "startOffset": 110, "endOffset": 162}, {"referenceID": 23, "context": ", 2009) and stochastic subgradient descent (Shalev-Shwartz et al., 2009), also rely on the efficiency of estimating the conditional gradient of the dual.", "startOffset": 43, "endOffset": 72}, {"referenceID": 22, "context": "This includes multi-plane variants of BCFW (Shah et al., 2015; Osokin et al., 2016), as well as generalizations of Frank-Wolfe such as partial linearization (Mohapatra et al.", "startOffset": 43, "endOffset": 83}, {"referenceID": 20, "context": "This includes multi-plane variants of BCFW (Shah et al., 2015; Osokin et al., 2016), as well as generalizations of Frank-Wolfe such as partial linearization (Mohapatra et al.", "startOffset": 43, "endOffset": 83}, {"referenceID": 18, "context": ", 2016), as well as generalizations of Frank-Wolfe such as partial linearization (Mohapatra et al., 2016).", "startOffset": 81, "endOffset": 105}], "year": 2017, "abstractText": "We present a novel layerwise optimization algorithm for the learning objective of Piecewise-Linear Convolutional Neural Networks (PL-CNNs), a large class of convolutional neural networks. Specifically, PL-CNNs employ piecewise linear non-linearities such as the commonly used ReLU and max-pool, and an SVM classifier as the final layer. The key observation of our approach is that the problem corresponding to the parameter estimation of a layer can be formulated as a difference-of-convex (DC) program, which happens to be a latent structured SVM. We optimize the DC program using the concave-convex procedure, which requires us to iteratively solve a structured SVM problem. This allows to design an optimization algorithm with an optimal learning rate that does not require any tuning. Using the MNIST, CIFAR and ImageNet data sets, we show that our approach always improves over the state of the art variants of backpropagation and scales to large data and large network settings.", "creator": "LaTeX with hyperref package"}}}