{"id": "1611.02247", "review": {"conference": "iclr", "VERSION": "v1", "DATE_OF_SUBMISSION": "7-Nov-2016", "title": "Q-Prop: Sample-Efficient Policy Gradient with An Off-Policy Critic", "abstract": "Model-free deep reinforcement learning (RL) methods have been successful in a wide variety of simulated domains. However, a major obstacle facing deep RL in the real world is the high sample complexity of such methods. Unbiased batch policy-gradient methods offer stable learning, but at the cost of high variance, which often requires large batches, while TD-style methods, such as off-policy actor-critic and Q-learning, are more sample-efficient but biased, and often require costly hyperparameter sweeps to stabilize. In this work, we aim to develop methods that combine the stability of unbiased policy gradients with the efficiency of off-policy RL. We present Q-Prop, a policy gradient method that uses a Taylor expansion of the off-policy critic as a control variate. Q-Prop is both sample efficient and stable, and effectively combines the benefits of on-policy and off-policy methods. We analyze the connection between Q-Prop and existing model-free algorithms, and use control variate theory to derive two variants of Q-Prop with conservative and aggressive adaptation. We show that conservative Q-Prop provides substantial gains in sample efficiency over trust region policy optimization (TRPO) with generalized advantage estimation (GAE), and improves stability over deep deterministic policy gradient (DDPG), the state-of-the-art on-policy and off-policy methods, on OpenAI Gym's MuJoCo continuous control environments.", "histories": [["v1", "Mon, 7 Nov 2016 20:09:16 GMT  (778kb,D)", "http://arxiv.org/abs/1611.02247v1", null], ["v2", "Thu, 17 Nov 2016 21:57:19 GMT  (781kb,D)", "http://arxiv.org/abs/1611.02247v2", null], ["v3", "Mon, 27 Feb 2017 21:48:25 GMT  (788kb,D)", "http://arxiv.org/abs/1611.02247v3", "Conference Paper at the International Conference on Learning Representations (ICLR) 2017"]], "reviews": [], "SUBJECTS": "cs.LG", "authors": ["shixiang gu", "timothy lillicrap", "zoubin ghahramani", "richard e turner", "sergey levine"], "accepted": true, "id": "1611.02247"}, "pdf": {"name": "1611.02247.pdf", "metadata": {"source": "CRF", "title": "Q-PROP: SAMPLE-EFFICIENT POLICY GRADIENT WITH AN OFF-POLICY CRITIC", "authors": ["Shixiang Gu", "Timothy Lillicrap", "Zoubin Ghahramani", "Richard E. Turner", "Sergey Levine"], "emails": [], "sections": [{"heading": "1 INTRODUCTION", "text": "In fact, it's as if most people are able to understand themselves and understand what they are doing. (...) It's not as if they are able to understand themselves. (...) It's not as if they are able to identify themselves. (...) It's as if they are able to identify themselves. (...) It's not as if they are able to identify themselves. (...) It's as if they are able to identify themselves. (...) It's as if they are able to identify themselves. (...) It's as if they are able to identify themselves. (...) It's as if they are doing it, as if they are doing it, as if they are doing it. \"(...) It's as if they are doing it, as if they are doing it, as if they are doing it, if they are doing it, if they are doing it, if they are doing it, if they are doing it. (...) It's as if they are doing it, if they are doing it, if they are doing it, if they are doing it, if they are doing it, if they are doing it."}, {"heading": "2 BACKGROUND", "text": "Reinforcement Learning (RL) aims to learn a policy for an agent so that he behaves optimally according to a reward function. In a time step t and the state st, the agent selects an action according to his policy \u03c0 (at | st), the state of the agent and the environment change according to the dynamics p (st + 1 | st, at) to the new state st + 1, the agent receives a reward r (st, at), and the process continues. Let Rt denote a g-discounted cumulative yield of t for an infinite horizon problem, i.e. Rt = \u2211 t \u00b2 = t \u00b2 t \u00b2 t \u00b2 t (st \u00b2, at \u00b2). The goal of reinforcement learning is to maximize the expected yield J (\u03b8) = EEMC [R0] in terms of policy parameters. In this section, we will review several standard techniques for performing this optimization, and in the next section we will discuss our proposed Q-Prop algorithm, which combines the strengths of these RL to achieve an efficient combination."}, {"heading": "2.1 POLICY GRADIENT METHODS", "text": "The standard form known as the REINFORCE algorithm (Williams, 1992) is presented below:.........................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................."}, {"heading": "2.2 OFF-POLICY ACTOR-CRITIC METHODS", "text": "Actor-critic methods (Sutton et al., 1999) include a political evaluation step that uses temporal difference (TD) to adapt a critic-critic Qw to current policies (\u03b8), and a policy improvement step that greedily optimizes policy Qw against the critic estimate. However, a significant efficiency gain is achievable for the critic with non-political TD learning, as in Q-learning and deterministic policy gradient (Sutton, 1990; Silver et al., 2014), which has recently been popularized by field reports on the formation of deep Q networks (Mnih et al., 2015; Lillicrap et al., 2016; Gu et al., 2016b). Deep deterministic policy gradient (DPG et al, 2014; Lillicrap et al al al al al al al al al al al al al al al al al al) properties (Mnih et al., 2015)."}, {"heading": "3 Q-PROP", "text": "In this section we derive the Q-Prop estimator for the policy gradient. The key idea of this estimator comes from the observation of equations 2 and 5 and the finding that the former provides an unbiased, superficial gradient of variance, while the latter provides a deterministic but tendentious gradient. By using the deterministic biased estimator as a particular form of control variant (Ross, 2006; Paisley et al., 2012) for the unbiased policy gradient estimator, we can effectively use both types of gradient information to construct a new estimator that is generally unbiased and in practice has improved sampling efficiency by including non-political samples."}, {"heading": "3.1 Q-PROP ESTIMATOR", "text": "To derive the Q-Prop-Prop-Estimator, we begin with the Q-Q-Expansion of the First Order (Q-Prop-Estimator) (Q-Prop-Estimator) (Q-Prop-Estimator) (Q-Prop-Estimator) (Q-Prop-Estimator) (Q-Prop-Estimator) (Q-Prop-Estimator) (Q-Prop-Estimator) (Q-Prop-Q-Q-Prop-Estimator) (Q-Q-Q-Prop) (Q-Q-Q-Estimator) (Q-Prop) (Q-Prop-Q-Q-Q-Prop-Estimator) (Q-Q-Q-Q-Q-Prop-Estimator) (Q-Prop) (Q-Prop) (Q-Prop) (Q-Prop) (Q-Q-Q-Q-Q-Estimator) (Q-Q-Q-Q-Q-Prop) (Q-Q-Q-Estimator) (Q-Q-Q-Q) (Q-Q-Prop) (Q-Q-Prop) (Q-Prop) (Q-Prop) Estimator (Q-Prop) (Q-Prop) (Q-Prop) (Q-Prop-Prop-Q-Q-Q-Q-Q-Q-Prop-Estimator) (Q-Q-Q-Prop) (Q-Prop) (Q-Q-Q-Q-Prop-Q-Estimator) (Q-Prop) (Q-Prop) (Q-Prop-Q-Q-Q-Q-Q-Prop-Q-Prop-Estimator (Q-Q-Q-Q-Q-Q-Estimator) (Q-Q-Prop) (Q-Prop) (Q-Prop) (Q-Q-Prop) (Q-Q-Prop) (Q-Q-Prop) (Q-Q-Prop) (Q-Q-Q-Q-Prop) (Q-Q-Q-Prop) (Q-Q"}, {"heading": "3.2 CONTROL VARIATE ANALYSIS AND ADAPTIVE Q-PROP", "text": "For the reliable use of Q-Prop, it is crucial to analyze how the estimator's variance (Q = 1) changes before and after the use of the control variate. Following the previous work on the control variant (Ross, 2006; Paisley et al., 2012), we first introduce \"Q\" (st) to Eq. 8, a weighing variable that modulates the strength of the control variate. This additional variable p (st) does not introduce bias into the estimator. (st) \"Q\" (st) to Eq. 8, a weighing variable that modulates the strength of the control variate. (st) This additional variable p. \"(st) does not introduce bias into the estimator. (st)\" A. \"(st)."}, {"heading": "3.3 Q-PROP ALGORITHM", "text": "Pseudo-code for the adaptive Q-Prop algorithm is provided in Algorithm 1. It is a mixture of policy gradients and stakeholder critics. At each iteration, it first executes the stochastic policy to collect policy samples, adds the batch to a replay buffer, introduces a few gradient steps to the critic, calculates A and A, and finally applies a gradient step to policy. In our implementation, the critic Qw is equipped with non-political TD learning methods that use the same techniques as in DDPG (Lillicrap et al., 2016): w = argmin w Est \u03b2 (\u00b7), at \u03b2 (\u00b7 st) + E-Learning using the same techniques as in DDPG (Lillicrap et al., 2016)."}, {"heading": "4 RELATED WORK", "text": "A subtle exception is the compatible feature approximation (Sutton et al., 1999), which can be considered a control variant as explained in Appendix B. Another exception is the doubly robust estimator in contextual bandits (Dud\u0131k et al., 2011), which uses another control variant whose bias cannot be demonstrably corrected. Control variants have not recently been examined in RL, but for approximate conclusions in stochastic models (Paisley et al., 2012), and the most obvious related work in this area is the MuProp algorithm, whose bias cannot be demonstrably corrected."}, {"heading": "5 EXPERIMENTS", "text": "We evaluated Q-Prop and its variants on continuous control environments according to the OpenAI Gym Benchmark (Brockman et al., 2016) using the MuJoCo physics simulator (Todorov et al., 2012), as shown in Figure 1. Algorithms are identified by acronyms, followed by a number indicating batch size, except for DDPG, a former online actor-critic algorithm (Lillicrap et al., 2016). \"c-\" and \"v-\" stand for conservative and aggressive Q-Prop variants, as described in Section 3.2. \"TR-\" stands for confidence policy optimization (Schulman et al., 2015), while \"V-\" stands for a vanilla policy gradient. For example, \"TR-c-Q-Prop-5000\" means convergent Q-Prop variants, as described in Section 3.2."}, {"heading": "5.1 ADAPTIVE Q-PROP", "text": "First, it is useful to determine how reliable each variant of Q-Prop is. In this section, we analyze standard Q-Prop and two adaptive variants, c-Q-Prop and a-Q-Prop, and demonstrate the stability of the method across different batch sizes. Figure 2a shows a comparison of Q-Prop variants with trust-regional updates on the HalfCheetah v1 domain, along with the best-performing TRPO hyperparameters. The results agree with the theory: Conservative Q-Prop achieves much more stable performance than standard and aggressive variants, and all Q-Prop variants significantly exceed TRPO in sample efficiency, e.g. conservative Q-Prop achieves an average reward of 4,000 times less samples than TRPO. Figure 2b shows the performance of conservative Q-Prop over standard and aggressive variants, and all Q-Prop variants significantly exceed TRPO in sample efficiency, and all Q-Prop variants achieve significant stability at very high levels of TRPO."}, {"heading": "5.2 EVALUATION ACROSS ALGORITHMS", "text": "In this section, we evaluate two versions of conservative Q-Prop, v-c-Q-Prop using vanilla policy gradients, and TR-c-Q-Prop using trust region updates against other model-free algorithms on the HalfCheetah v1 domain. Figure 3a shows that c-Q-Prop methods significantly outperform the best TRPO and VPG methods. Even Q-Prop with vanilla policy gradients outperforms TRPO, confirming the significant benefits of reducing variance. DDPG, on the other hand, shows inconsistent performance. With appropriate reward scaling, i.e. \"DPG-r0.1,\" it outperforms other methods, as does the DDPG results from previous work (Duan et al., 2016; Amos et al., 2016). This illustrates the sensitivity of DDPG to hyperparameter settings, while Q-Prop exhibits improved learning stability compared to DDDPG in the next Learning Monitor section."}, {"heading": "5.3 EVALUATION ACROSS DOMAINS", "text": "Finally, we evaluate Q-Prop against TRPO and DDPG in several areas. While fitness environments tend toward locomotion, we expect to achieve similar performance on manipulation tasks as in Lillicrap et al. (2016). Table 1 summarizes the results, including the best achieved average rewards and steps toward convergence. Q-Prop consistently outperforms TRPO in terms of sample complexity and sometimes achieves higher rewards than DDPG in more complex areas. A particularly noteworthy case is Figure 3b, where Q-Prop significantly improves sample efficiency over TRPO in the Humanoid v1 domain, while DDPG cannot find a good solution. Better performance in the more complex areas underscores the importance of stable deep RL algorithms: While costly hyperparameter sweeps may allow even less stable algorithms to perform well in simpler problems, more complex tasks may make such tight regions appear more racial than the hyperparameters."}, {"heading": "6 DISCUSSION AND CONCLUSION", "text": "We introduced Q-Prop, a policy gradient algorithm that combines reliable, consistent, and potentially unbiased policy gradient estimates with a sample-efficient external critique method that acts as a control variant. It offers a large improvement in sample efficiency over modern policy gradient estimation methods such as TRPO and outperforms state-of-the-art stakeholder critique methods for more difficult tasks such as humanoid locomotion. We hope that techniques like this, which combine unbiased gradient estimation with sample-efficient variant reduction by external critics, will ultimately lead to a profound strengthening of learning algorithms that are more stable and efficient, and therefore better suited for application to complex real-world learning tasks."}, {"heading": "ACKNOWLEDGMENTS", "text": "We thank Rocky Duan for answering questions about the rllab code and Yutian Chen and Laurent Dinh for discussing control variants. SG was funded by NSERC and a Google Focused Research Award."}, {"heading": "A Q-PROP ESTIMATOR DERIVATION", "text": "The complete derivative of the Q-Prop estimator is derived from Eq. 14. We use the following property (f (st, at) if f (st, a) is an arbitrary function that is distinguishable in relation to at and f (st). Here, Taylor expansion of the first order is at = a), i.e. f (st, at) = f (st, a) + a (st, a). The derivative appears below the following property J (st, a) = f (st, a). The derivative appears below the f (st, a). The complete derivative of the Q-Prop estimator is derived from Eq."}, {"heading": "B CONNECTION BETWEEN Q-PROP AND COMPATIBLE FEATURE APPROXIMATION", "text": "In this section, we show that Actor Critic with Compatible Feature Approximation is a form of control variable. A critic Qw is compatible (Sutton et al., 1999) if he (1) Qw (st, at) = wT (at), i.e. if he (st, at), and (2) w with objective w = argminw L (w) = argminwEically (at), \u03c0 [(Q, at) \u2212 Qw (st, at) 2], which matches Qw on-Policy Monte Carlo. Condition (2) implies the following identity, namely wL = 2E\u03b5p (at) = logical development (at) (Q (st, at) \u2212 Qw (st, at) \u2212 Qw (st, at) = 0. (15) In compatible feature approximation, Qw is used directly as control variant, not as logical solution (at)."}, {"heading": "C UNIFYING POLICY GRADIENT AND ACTOR-CRITIC", "text": "Q-Prop closely links policy gradients and stakeholder-critic algorithms. To analyze this point, we will write below a generalization of equation 9, introducing two additional variables \u03b1, \u03c1CR:.........................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................."}, {"heading": "D EXPERIMENT DETAILS", "text": "The network architectures are largely based on the reference paper by Duan et al. (2016). For political gradient methods, stochastic politics \u03c0\u03b8 (at | st) = N (\u00b5\u03b8 (st), \u03b8) is a local Gaussian policy with a local, state-dependent mean and a global covariance matrix. (st) is a neural network with 3 hidden layers of size 100-50-25 and tanh nonlinearity in the first two layers, and politics is diagonal. For DPG, politics is deterministic and has the same architecture as \u00b5\u03b8, except that it has an additional tanh layer of size 100-50-25 and tanh nonlinearity in the first two steps. (2016), a variant of linear regression on Monte Carlo returns with a soft-update limitation of 0.00ms. For Q-Prop and DDPG, Qw (s, a) it is etamrized 01 with a network of stecked and steered-Q-Q layers (2016)."}], "references": [{"title": "Input convex neural networks", "author": ["Brandon Amos", "Lei Xu", "J Zico Kolter"], "venue": "arXiv preprint arXiv:1609.07152,", "citeRegEx": "Amos et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Amos et al\\.", "year": 2016}, {"title": "A comparison of direct and model-based reinforcement learning", "author": ["Christopher G Atkeson", "Juan Carlos Santamaria"], "venue": "In In International Conference on Robotics and Automation. Citeseer,", "citeRegEx": "Atkeson and Santamaria.,? \\Q1997\\E", "shortCiteRegEx": "Atkeson and Santamaria.", "year": 1997}, {"title": "Pilco: A model-based and data-efficient approach to policy search", "author": ["Marc Deisenroth", "Carl E Rasmussen"], "venue": "In Proceedings of the 28th International Conference on machine learning", "citeRegEx": "Deisenroth and Rasmussen.,? \\Q2011\\E", "shortCiteRegEx": "Deisenroth and Rasmussen.", "year": 2011}, {"title": "Benchmarking deep reinforcement learning for continuous control", "author": ["Yan Duan", "Xi Chen", "Rein Houthooft", "John Schulman", "Pieter Abbeel"], "venue": "International Conference on Machine Learning (ICML),", "citeRegEx": "Duan et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Duan et al\\.", "year": 2016}, {"title": "Doubly robust policy evaluation and learning", "author": ["Miroslav Dud\u0131\u0301k", "John Langford", "Lihong Li"], "venue": "arXiv preprint arXiv:1103.4601,", "citeRegEx": "Dud\u0131\u0301k et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Dud\u0131\u0301k et al\\.", "year": 2011}, {"title": "Variance reduction techniques for gradient estimates in reinforcement learning", "author": ["Evan Greensmith", "Peter L Bartlett", "Jonathan Baxter"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Greensmith et al\\.,? \\Q2004\\E", "shortCiteRegEx": "Greensmith et al\\.", "year": 2004}, {"title": "Muprop: Unbiased backpropagation for stochastic neural networks", "author": ["Shixiang Gu", "Sergey Levine", "Ilya Sutskever", "Andriy Mnih"], "venue": "International Conference on Learning Representations (ICLR),", "citeRegEx": "Gu et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Gu et al\\.", "year": 2016}, {"title": "Continuous deep q-learning with model-based acceleration", "author": ["Shixiang Gu", "Tim Lillicrap", "Ilya Sutskever", "Sergey Levine"], "venue": "In International Conference on Machine Learning (ICML),", "citeRegEx": "Gu et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Gu et al\\.", "year": 2016}, {"title": "Double q-learning", "author": ["Hado V Hasselt"], "venue": "In Advances in Neural Information Processing Systems, pp. 2613\u20132621,", "citeRegEx": "Hasselt.,? \\Q2010\\E", "shortCiteRegEx": "Hasselt.", "year": 2010}, {"title": "A natural policy gradient", "author": ["Sham Kakade"], "venue": "In NIPS,", "citeRegEx": "Kakade.,? \\Q2001\\E", "shortCiteRegEx": "Kakade.", "year": 2001}, {"title": "Adam: A method for stochastic optimization", "author": ["Diederik Kingma", "Jimmy Ba"], "venue": "arXiv preprint arXiv:1412.6980,", "citeRegEx": "Kingma and Ba.,? \\Q2014\\E", "shortCiteRegEx": "Kingma and Ba.", "year": 2014}, {"title": "Deterministic policy gradient algorithms", "author": ["Guy Lever"], "venue": null, "citeRegEx": "Lever.,? \\Q2014\\E", "shortCiteRegEx": "Lever.", "year": 2014}, {"title": "Guided policy search", "author": ["Sergey Levine", "Vladlen Koltun"], "venue": "In International Conference on Machine Learning (ICML), pp", "citeRegEx": "Levine and Koltun.,? \\Q2013\\E", "shortCiteRegEx": "Levine and Koltun.", "year": 2013}, {"title": "Continuous control with deep reinforcement learning", "author": ["Timothy P Lillicrap", "Jonathan J Hunt", "Alexander Pritzel", "Nicolas Heess", "Tom Erez", "Yuval Tassa", "David Silver", "Daan Wierstra"], "venue": "International Conference on Learning Representations (ICLR),", "citeRegEx": "Lillicrap et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Lillicrap et al\\.", "year": 2016}, {"title": "Neural variational inference and learning in belief networks", "author": ["Andriy Mnih", "Karol Gregor"], "venue": "International Conference on Machine Learning (ICML),", "citeRegEx": "Mnih and Gregor.,? \\Q2014\\E", "shortCiteRegEx": "Mnih and Gregor.", "year": 2014}, {"title": "Human-level control through deep reinforcement learning", "author": ["Volodymyr Mnih", "Koray Kavukcuoglu", "David Silver", "Andrei A Rusu", "Joel Veness", "Marc G Bellemare", "Alex Graves", "Martin Riedmiller", "Andreas K Fidjeland", "Georg Ostrovski"], "venue": "Nature, 518(7540):529\u2013533,", "citeRegEx": "Mnih et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Mnih et al\\.", "year": 2015}, {"title": "Asynchronous methods for deep reinforcement learning", "author": ["Volodymyr Mnih", "Adria Puigdomenech Badia", "Mehdi Mirza", "Alex Graves", "Timothy P Lillicrap", "Tim Harley", "David Silver", "Koray Kavukcuoglu"], "venue": "In International Conference on Machine Learning (ICML),", "citeRegEx": "Mnih et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Mnih et al\\.", "year": 2016}, {"title": "Safe and efficient offpolicy reinforcement learning", "author": ["R\u00e9mi Munos", "Tom Stepleton", "Anna Harutyunyan", "Marc G Bellemare"], "venue": "arXiv preprint arXiv:1606.02647,", "citeRegEx": "Munos et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Munos et al\\.", "year": 2016}, {"title": "Variational bayesian inference with stochastic search", "author": ["John Paisley", "David Blei", "Michael Jordan"], "venue": "International Conference on Machine Learning (ICML),", "citeRegEx": "Paisley et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Paisley et al\\.", "year": 2012}, {"title": "Policy gradient methods for robotics", "author": ["Jan Peters", "Stefan Schaal"], "venue": "In International Conference on Intelligent Robots and Systems (IROS),", "citeRegEx": "Peters and Schaal.,? \\Q2006\\E", "shortCiteRegEx": "Peters and Schaal.", "year": 2006}, {"title": "Relative entropy policy search", "author": ["Jan Peters", "Katharina M\u00fclling", "Yasemin Altun"], "venue": "In AAAI. Atlanta,", "citeRegEx": "Peters et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Peters et al\\.", "year": 2010}, {"title": "Eligibility traces for off-policy policy evaluation", "author": ["Doina Precup"], "venue": "Computer Science Department Faculty Publication Series, pp", "citeRegEx": "Precup.,? \\Q2000\\E", "shortCiteRegEx": "Precup.", "year": 2000}, {"title": "Trust region policy optimization", "author": ["John Schulman", "Sergey Levine", "Pieter Abbeel", "Michael I. Jordan", "Philipp Moritz"], "venue": "In International Conference on Machine Learning (ICML),", "citeRegEx": "Schulman et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Schulman et al\\.", "year": 2015}, {"title": "Highdimensional continuous control using generalized advantage estimation", "author": ["John Schulman", "Philipp Moritz", "Sergey Levine", "Michael Jordan", "Pieter Abbeel"], "venue": "International Conference on Learning Representations (ICLR),", "citeRegEx": "Schulman et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Schulman et al\\.", "year": 2016}, {"title": "Deterministic policy gradient algorithms", "author": ["David Silver", "Guy Lever", "Nicolas Heess", "Thomas Degris", "Daan Wierstra", "Martin Riedmiller"], "venue": "In International Conference on Machine Learning (ICML),", "citeRegEx": "Silver et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Silver et al\\.", "year": 2014}, {"title": "Mastering the game of go with deep neural networks and tree", "author": ["David Silver", "Aja Huang", "Chris J Maddison", "Arthur Guez", "Laurent Sifre", "George Van Den Driessche", "Julian Schrittwieser", "Ioannis Antonoglou", "Veda Panneershelvam", "Marc Lanctot"], "venue": "search. Nature,", "citeRegEx": "Silver et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Silver et al\\.", "year": 2016}, {"title": "Integrated architectures for learning, planning, and reacting based on approximating dynamic programming", "author": ["Richard S Sutton"], "venue": "In International Conference on Machine Learning (ICML),", "citeRegEx": "Sutton.,? \\Q1990\\E", "shortCiteRegEx": "Sutton.", "year": 1990}, {"title": "Policy gradient methods for reinforcement learning with function approximation", "author": ["Richard S Sutton", "David A McAllester", "Satinder P Singh", "Yishay Mansour"], "venue": "In Advances in Neural Information Processing Systems (NIPS),", "citeRegEx": "Sutton et al\\.,? \\Q1999\\E", "shortCiteRegEx": "Sutton et al\\.", "year": 1999}, {"title": "Mujoco: A physics engine for model-based control", "author": ["Emanuel Todorov", "Tom Erez", "Yuval Tassa"], "venue": "In 2012 IEEE/RSJ International Conference on Intelligent Robots and Systems,", "citeRegEx": "Todorov et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Todorov et al\\.", "year": 2012}, {"title": "Simple statistical gradient-following algorithms for connectionist reinforcement learning", "author": ["Ronald J Williams"], "venue": "Machine learning,", "citeRegEx": "Williams.,? \\Q1992\\E", "shortCiteRegEx": "Williams.", "year": 1992}, {"title": "For policy gradient methods, the stochastic policy \u03c0\u03b8 (at |st) = N (\u03bc\u03b8 (st),\u03a3\u03b8 ) is a local Gaussian policy with a local state-dependent mean and a global covariance matrix. \u03bc\u03b8 (st) is a neural network with 3 hidden layers of sizes 100-50-25 and tanh nonlinearities at the first 2 layers", "author": ["Duan"], "venue": null, "citeRegEx": "Duan,? \\Q2016\\E", "shortCiteRegEx": "Duan", "year": 2016}], "referenceMentions": [{"referenceID": 15, "context": "It has recently been extended to utilize large neural network policies and value functions, and has been shown to be successful in solving a range of difficult problems (Mnih et al., 2015; Schulman et al., 2015; Lillicrap et al., 2016; Silver et al., 2016; Gu et al., 2016b; Mnih et al., 2016).", "startOffset": 169, "endOffset": 293}, {"referenceID": 22, "context": "It has recently been extended to utilize large neural network policies and value functions, and has been shown to be successful in solving a range of difficult problems (Mnih et al., 2015; Schulman et al., 2015; Lillicrap et al., 2016; Silver et al., 2016; Gu et al., 2016b; Mnih et al., 2016).", "startOffset": 169, "endOffset": 293}, {"referenceID": 13, "context": "It has recently been extended to utilize large neural network policies and value functions, and has been shown to be successful in solving a range of difficult problems (Mnih et al., 2015; Schulman et al., 2015; Lillicrap et al., 2016; Silver et al., 2016; Gu et al., 2016b; Mnih et al., 2016).", "startOffset": 169, "endOffset": 293}, {"referenceID": 25, "context": "It has recently been extended to utilize large neural network policies and value functions, and has been shown to be successful in solving a range of difficult problems (Mnih et al., 2015; Schulman et al., 2015; Lillicrap et al., 2016; Silver et al., 2016; Gu et al., 2016b; Mnih et al., 2016).", "startOffset": 169, "endOffset": 293}, {"referenceID": 16, "context": "It has recently been extended to utilize large neural network policies and value functions, and has been shown to be successful in solving a range of difficult problems (Mnih et al., 2015; Schulman et al., 2015; Lillicrap et al., 2016; Silver et al., 2016; Gu et al., 2016b; Mnih et al., 2016).", "startOffset": 169, "endOffset": 293}, {"referenceID": 8, "context": "Although a number of recent techniques have sought to alleviate some of these issues (Hasselt, 2010; Mnih et al., 2015; Schulman et al., 2015; 2016), these recent advances still provide only a partial solution to the instability and sample complexity challenges.", "startOffset": 85, "endOffset": 148}, {"referenceID": 15, "context": "Although a number of recent techniques have sought to alleviate some of these issues (Hasselt, 2010; Mnih et al., 2015; Schulman et al., 2015; 2016), these recent advances still provide only a partial solution to the instability and sample complexity challenges.", "startOffset": 85, "endOffset": 148}, {"referenceID": 22, "context": "Although a number of recent techniques have sought to alleviate some of these issues (Hasselt, 2010; Mnih et al., 2015; Schulman et al., 2015; 2016), these recent advances still provide only a partial solution to the instability and sample complexity challenges.", "startOffset": 85, "endOffset": 148}, {"referenceID": 22, "context": "Policy gradient methods (Peters & Schaal, 2006; Schulman et al., 2015) are popular on-policy methods that directly maximize the cumulative future returns with respect to the policy.", "startOffset": 24, "endOffset": 70}, {"referenceID": 9, "context": "To cope with high variance gradient estimates and difficult optimization landscapes, a number of techniques have been proposed, including constraining the change in the policy at each gradient step (Kakade, 2001; Peters et al., 2010) and mixing valuebased back-ups to trade off bias and variance in Monte Carlo return estimates (Schulman et al.", "startOffset": 198, "endOffset": 233}, {"referenceID": 20, "context": "To cope with high variance gradient estimates and difficult optimization landscapes, a number of techniques have been proposed, including constraining the change in the policy at each gradient step (Kakade, 2001; Peters et al., 2010) and mixing valuebased back-ups to trade off bias and variance in Monte Carlo return estimates (Schulman et al.", "startOffset": 198, "endOffset": 233}, {"referenceID": 22, "context": ", 2010) and mixing valuebased back-ups to trade off bias and variance in Monte Carlo return estimates (Schulman et al., 2015).", "startOffset": 102, "endOffset": 125}, {"referenceID": 27, "context": "Off-policy methods, such as Q-learning (Watkins & Dayan, 1992; Sutton et al., 1999; Mnih et al., 2015; Gu et al., 2016b) and off-policy actor-critic methods (Lever, 2014; Lillicrap et al.", "startOffset": 39, "endOffset": 120}, {"referenceID": 15, "context": "Off-policy methods, such as Q-learning (Watkins & Dayan, 1992; Sutton et al., 1999; Mnih et al., 2015; Gu et al., 2016b) and off-policy actor-critic methods (Lever, 2014; Lillicrap et al.", "startOffset": 39, "endOffset": 120}, {"referenceID": 11, "context": ", 2016b) and off-policy actor-critic methods (Lever, 2014; Lillicrap et al., 2016), can instead use all samples, including off-policy samples, by adopting temporal difference learning with experience replay.", "startOffset": 45, "endOffset": 82}, {"referenceID": 13, "context": ", 2016b) and off-policy actor-critic methods (Lever, 2014; Lillicrap et al., 2016), can instead use all samples, including off-policy samples, by adopting temporal difference learning with experience replay.", "startOffset": 45, "endOffset": 82}, {"referenceID": 27, "context": "Unlike prior approaches for off-policy learning, which either introduce bias (Sutton et al., 1999; Silver et al., 2014) or increase variance (Precup, 2000; Levine & Koltun, 2013; Munos et al.", "startOffset": 77, "endOffset": 119}, {"referenceID": 24, "context": "Unlike prior approaches for off-policy learning, which either introduce bias (Sutton et al., 1999; Silver et al., 2014) or increase variance (Precup, 2000; Levine & Koltun, 2013; Munos et al.", "startOffset": 77, "endOffset": 119}, {"referenceID": 21, "context": ", 2014) or increase variance (Precup, 2000; Levine & Koltun, 2013; Munos et al., 2016), Q-Prop can reduce the variance of gradient estimator without adding bias; unlike prior approaches for critic-based variance reduction (Schulman et al.", "startOffset": 29, "endOffset": 86}, {"referenceID": 17, "context": ", 2014) or increase variance (Precup, 2000; Levine & Koltun, 2013; Munos et al., 2016), Q-Prop can reduce the variance of gradient estimator without adding bias; unlike prior approaches for critic-based variance reduction (Schulman et al.", "startOffset": 29, "endOffset": 86}, {"referenceID": 23, "context": ", 2016), Q-Prop can reduce the variance of gradient estimator without adding bias; unlike prior approaches for critic-based variance reduction (Schulman et al., 2016) which fit the value function on-policy, Q-Prop learns the action-value function off-policy.", "startOffset": 143, "endOffset": 166}, {"referenceID": 22, "context": "We show that Q-Prop provides substantial gains in sample efficiency over trust region policy optimization (TRPO) with generalized advantage estimation (GAE) (Schulman et al., 2015; 2016), and improved stability over deep deterministic policy gradient (DDPG) (Lillicrap et al.", "startOffset": 157, "endOffset": 186}, {"referenceID": 13, "context": ", 2015; 2016), and improved stability over deep deterministic policy gradient (DDPG) (Lillicrap et al., 2016) across a repertoire of continuous control tasks.", "startOffset": 85, "endOffset": 109}, {"referenceID": 29, "context": "The standard form, known as the REINFORCE algorithm (Williams, 1992), is shown below:", "startOffset": 52, "endOffset": 68}, {"referenceID": 27, "context": "1\u201cPolicy gradient\u201d technically refers to a more general class of methods (Sutton et al., 1999); however, to make it compatible with references in other work we use it to primarily refer REINFORCE (Williams, 1992).", "startOffset": 73, "endOffset": 94}, {"referenceID": 29, "context": ", 1999); however, to make it compatible with references in other work we use it to primarily refer REINFORCE (Williams, 1992).", "startOffset": 109, "endOffset": 125}, {"referenceID": 21, "context": "Prior attempts use importance sampling to use off-policy trajectories; however, these are known to be difficult scale to high-dimensional action spaces because of rapidly degenerating importance weights (Precup, 2000).", "startOffset": 203, "endOffset": 217}, {"referenceID": 27, "context": "Actor-critic methods (Sutton et al., 1999) include a policy evaluation step, which uses temporal difference (TD) learning to fit a critic Qw for the current policy \u03c0(\u03b8), and a policy improvement step which greedily optimizes the policy \u03c0 against the critic estimate Qw.", "startOffset": 21, "endOffset": 42}, {"referenceID": 26, "context": "Significant gain in sample efficiency is achievable using off-policy TD learning for the critic, as in Q-learning and deterministic policy gradient (Sutton, 1990; Silver et al., 2014), recently popularized by experience replay for training deep Q networks (Mnih et al.", "startOffset": 148, "endOffset": 183}, {"referenceID": 24, "context": "Significant gain in sample efficiency is achievable using off-policy TD learning for the critic, as in Q-learning and deterministic policy gradient (Sutton, 1990; Silver et al., 2014), recently popularized by experience replay for training deep Q networks (Mnih et al.", "startOffset": 148, "endOffset": 183}, {"referenceID": 15, "context": ", 2014), recently popularized by experience replay for training deep Q networks (Mnih et al., 2015; Lillicrap et al., 2016; Gu et al., 2016b).", "startOffset": 80, "endOffset": 141}, {"referenceID": 13, "context": ", 2014), recently popularized by experience replay for training deep Q networks (Mnih et al., 2015; Lillicrap et al., 2016; Gu et al., 2016b).", "startOffset": 80, "endOffset": 141}, {"referenceID": 24, "context": "Deep deterministic policy gradient (DDPG) (Silver et al., 2014; Lillicrap et al., 2016) is an instance of off-policy algorithms which achieves significant results on high-dimensional continuous control tasks.", "startOffset": 42, "endOffset": 87}, {"referenceID": 13, "context": "Deep deterministic policy gradient (DDPG) (Silver et al., 2014; Lillicrap et al., 2016) is an instance of off-policy algorithms which achieves significant results on high-dimensional continuous control tasks.", "startOffset": 42, "endOffset": 87}, {"referenceID": 13, "context": "These properties make DDPG and other analogous off-policy methods significantly more sample-efficient than policy gradient methods (Lillicrap et al., 2016; Gu et al., 2016b; Duan et al., 2016).", "startOffset": 131, "endOffset": 192}, {"referenceID": 3, "context": "These properties make DDPG and other analogous off-policy methods significantly more sample-efficient than policy gradient methods (Lillicrap et al., 2016; Gu et al., 2016b; Duan et al., 2016).", "startOffset": 131, "endOffset": 192}, {"referenceID": 18, "context": "By using the deterministic biased estimator as a particular form of control variate (Ross, 2006; Paisley et al., 2012) for the unbiased policy gradient estimator, we can effectively use both types of gradient information to construct a new estimator that is in general unbiased, and in practice exhibits improved sample efficiency through the inclusion of off-policy samples.", "startOffset": 84, "endOffset": 118}, {"referenceID": 18, "context": "Following the prior work on control variate (Ross, 2006; Paisley et al., 2012), we first introduce \u03b7(st) to Eq.", "startOffset": 44, "endOffset": 78}, {"referenceID": 29, "context": "difficult (Williams, 1992).", "startOffset": 10, "endOffset": 26}, {"referenceID": 13, "context": "In our implementation, the critic Qw is fitted with off-policy TD learning using the same techniques as in DDPG (Lillicrap et al., 2016): w = argmin w Est\u223c\u03c1\u03b2 (\u00b7),at\u223c\u03b2 (\u00b7|st )[(r(st ,at)+ \u03b3E\u03c0 [Q \u2032(st+1,at+1)]\u2212Qw(st ,at))].", "startOffset": 112, "endOffset": 136}, {"referenceID": 23, "context": "(13) V\u03c6 is fitted with the same technique in (Schulman et al., 2016).", "startOffset": 45, "endOffset": 68}, {"referenceID": 23, "context": "Generalized advantage estimation (GAE) (Schulman et al., 2016) is used to estimate \u00c2.", "startOffset": 39, "endOffset": 62}, {"referenceID": 22, "context": "The policy update can be done by any method that utilizes the first-order gradient and possibly the on-policy batch data, which includes trust region policy optimization (TRPO) (Schulman et al., 2015).", "startOffset": 177, "endOffset": 200}, {"referenceID": 29, "context": "Variance reduction in policy gradient methods is a long-standing problem with a large body of prior work (Williams, 1992; Greensmith et al., 2004; Schulman et al., 2016).", "startOffset": 105, "endOffset": 169}, {"referenceID": 5, "context": "Variance reduction in policy gradient methods is a long-standing problem with a large body of prior work (Williams, 1992; Greensmith et al., 2004; Schulman et al., 2016).", "startOffset": 105, "endOffset": 169}, {"referenceID": 23, "context": "Variance reduction in policy gradient methods is a long-standing problem with a large body of prior work (Williams, 1992; Greensmith et al., 2004; Schulman et al., 2016).", "startOffset": 105, "endOffset": 169}, {"referenceID": 27, "context": "A subtle exception is compatible feature approximation (Sutton et al., 1999) which can be viewed as a control variate as explained in Appendix B.", "startOffset": 55, "endOffset": 76}, {"referenceID": 4, "context": "Another exception is doubly robust estimator in contextual bandits (Dud\u0131\u0301k et al., 2011), which uses a different control variate whose bias cannot be tractably corrected.", "startOffset": 67, "endOffset": 88}, {"referenceID": 18, "context": "Control variates were explored recently not in RL but for approximate inference in stochastic models (Paisley et al., 2012), and the closest related work in that domain is the MuProp algorithm (Gu et al.", "startOffset": 101, "endOffset": 123}, {"referenceID": 27, "context": "Prior methods that combine off-policy data with policy gradients either introduce bias (Sutton et al., 1999; Silver et al., 2014) or use importance weighting, which is known to result in degenerate importance weights in high dimensions, resulting in very high variance (Precup, 2000; Levine & Koltun, 2013).", "startOffset": 87, "endOffset": 129}, {"referenceID": 24, "context": "Prior methods that combine off-policy data with policy gradients either introduce bias (Sutton et al., 1999; Silver et al., 2014) or use importance weighting, which is known to result in degenerate importance weights in high dimensions, resulting in very high variance (Precup, 2000; Levine & Koltun, 2013).", "startOffset": 87, "endOffset": 129}, {"referenceID": 21, "context": ", 2014) or use importance weighting, which is known to result in degenerate importance weights in high dimensions, resulting in very high variance (Precup, 2000; Levine & Koltun, 2013).", "startOffset": 147, "endOffset": 184}, {"referenceID": 3, "context": "Figure 1: Illustrations of OpenAI Gym MuJoCo domains (Brockman et al., 2016; Duan et al., 2016): (a) Ant, (b) HalfCheetah, (c) Hopper, (d) Humanoid, (e) Reacher, (f) Swimmer, (g) Walker.", "startOffset": 53, "endOffset": 95}, {"referenceID": 28, "context": ", 2016) using the MuJoCo physics simulator (Todorov et al., 2012) as shown in Figure 1.", "startOffset": 43, "endOffset": 65}, {"referenceID": 13, "context": "size, except for DDPG, which is a prior online actor-critic algorithm (Lillicrap et al., 2016).", "startOffset": 70, "endOffset": 94}, {"referenceID": 22, "context": "\u201cTR-\u201d denotes trust-region policy optimization (Schulman et al., 2015), while \u201cV-\u201d denotes vanilla policy gradient.", "startOffset": 47, "endOffset": 70}, {"referenceID": 23, "context": "\u201cVPG\u201d and \u201cTRPO\u201d are vanilla policy gradient and trust-region policy optimization respectively (Schulman et al., 2016; Duan et al., 2016).", "startOffset": 95, "endOffset": 137}, {"referenceID": 3, "context": "\u201cVPG\u201d and \u201cTRPO\u201d are vanilla policy gradient and trust-region policy optimization respectively (Schulman et al., 2016; Duan et al., 2016).", "startOffset": 95, "endOffset": 137}, {"referenceID": 23, "context": "97) (Schulman et al., 2016).", "startOffset": 4, "endOffset": 27}, {"referenceID": 3, "context": ", 2016; Duan et al., 2016). Unless otherwise stated, all policy gradient methods are implemented with GAE(\u03bb = 0.97) (Schulman et al., 2016). Note that TRPOGAE is currently the state-of-the-art method on most of the OpenAI Gym benchmark tasks, though our experiments show that a well-tuned DDPG implementation sometimes achieves better results. Our algorithm implementations are built on top of the rllab TRPO and DDPG codes from Duan et al. (2016) and will be released upon publication.", "startOffset": 8, "endOffset": 448}, {"referenceID": 3, "context": "as the DDPG results reported in prior work (Duan et al., 2016; Amos et al., 2016).", "startOffset": 43, "endOffset": 81}, {"referenceID": 0, "context": "as the DDPG results reported in prior work (Duan et al., 2016; Amos et al., 2016).", "startOffset": 43, "endOffset": 81}, {"referenceID": 13, "context": "While the gym environments are biased toward locomotion, we expect we can achieve similar performance on manipulation tasks such as those in Lillicrap et al. (2016). Table 1 summarizes the results, including the best attained average rewards and the steps to convergence.", "startOffset": 141, "endOffset": 165}], "year": 2016, "abstractText": "Model-free deep reinforcement learning (RL) methods have been successful in a wide variety of simulated domains. However, a major obstacle facing deep RL in the real world is the high sample complexity of such methods. Unbiased batch policy-gradient methods offer stable learning, but at the cost of high variance, which often requires large batches, while TD-style methods, such as off-policy actor-critic and Q-learning, are more sample-efficient but biased, and often require costly hyperparameter sweeps to stabilize. In this work, we aim to develop methods that combine the stability of unbiased policy gradients with the efficiency of off-policy RL. We present Q-Prop, a policy gradient method that uses a Taylor expansion of the off-policy critic as a control variate. Q-Prop is both sample efficient and stable, and effectively combines the benefits of on-policy and offpolicy methods. We analyze the connection between Q-Prop and existing modelfree algorithms, and use control variate theory to derive two variants of Q-Prop with conservative and aggressive adaptation. We show that conservative Q-Prop provides substantial gains in sample efficiency over trust region policy optimization (TRPO) with generalized advantage estimation (GAE), and improves stability over deep deterministic policy gradient (DDPG), the state-of-the-art on-policy and off-policy methods, on OpenAI Gym\u2019s MuJoCo continuous control environments.", "creator": "LaTeX with hyperref package"}}}