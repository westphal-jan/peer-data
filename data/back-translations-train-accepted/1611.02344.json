{"id": "1611.02344", "review": {"conference": "ACL", "VERSION": "v1", "DATE_OF_SUBMISSION": "7-Nov-2016", "title": "A Convolutional Encoder Model for Neural Machine Translation", "abstract": "The prevalent approach to neural machine translation relies on bi-directional LSTMs to encode the source sentence. In this paper we present a faster and conceptually simpler architecture based on a succession of convolutional layers. This allows to encode the entire source sentence simultaneously compared to recurrent networks for which computation is constrained by temporal dependencies. We achieve a new state-of-the-art on WMT'16 English-Romanian translation and outperform several recently published results on the WMT'15 English-German task. We also achieve almost the same accuracy as a very deep LSTM setup on WMT'14 English-French translation. Our convolutional encoder speeds up CPU decoding by more than two times at the same or higher accuracy as a strong bi-directional LSTM baseline.", "histories": [["v1", "Mon, 7 Nov 2016 23:46:45 GMT  (175kb,D)", "http://arxiv.org/abs/1611.02344v1", "13 pages"], ["v2", "Thu, 17 Nov 2016 01:45:37 GMT  (175kb,D)", "http://arxiv.org/abs/1611.02344v2", "13 pages"], ["v3", "Tue, 25 Jul 2017 01:36:14 GMT  (358kb,D)", "http://arxiv.org/abs/1611.02344v3", "13 pages"]], "COMMENTS": "13 pages", "reviews": [], "SUBJECTS": "cs.CL", "authors": ["jonas gehring", "michael auli", "david grangier", "yann dauphin"], "accepted": true, "id": "1611.02344"}, "pdf": {"name": "1611.02344.pdf", "metadata": {"source": "CRF", "title": "A CONVOLUTIONAL ENCODER MODEL FOR NEURAL MACHINE TRANSLATION", "authors": ["Jonas Gehring", "Michael Auli", "David Grangier", "Yann N. Dauphin"], "emails": [], "sections": [{"heading": "1 INTRODUCTION", "text": "\"It's like we're able to unite,\" he says, \"and it's like we're able to unite, it's like we're able to unite, it's like we're able to unite.\""}, {"heading": "2 RECURRENT NEURAL MACHINE TRANSLATION", "text": "In fact, it is an infinite time, a time when it is a time when most people are able to understand, understand, understand, understand, understand, understand, understand, understand, understand, understand, understand, understand, understand, understand, understand, understand, understand, understand, understand, understand, understand, understand, understand, understand, understand, understand, understand, understand, understand, understand, understand, understand, understand, understand, understand, understand, understand, understand, understand, understand, understand, understand, understand, understand, understand, understand, understand, understand, understand, understand, understand, understand, understand, understand."}, {"heading": "3 NON-RECURRENT ENCODERS", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "3.1 POOLING ENCODER", "text": "A simple baseline for non-recurring encoders is the pooling model described in Ranzato et al. (2015), which only averages the embedding of k consecutive words. Average word embedding does not convey position information, except that the words in the input are reasonably close to each other. As a remedy, we add position embedding to encode the absolute position of each source word within a sentence. Therefore, each source embedding ej contains a position that embeds both lj and the word wj. Position embedding has also been shown to be helpful in memory networks for answering questions and language modeling (Sukhbaatar et al., 2015). Similar to the recurring encoder (\u00a7 2), the attention values aij are calculated from the pooled representations zj, with the conditional input ci being a weighted sum of embedding ej + wk = 1j = lb."}, {"heading": "3.2 CONVOLUTIONAL ENCODER", "text": "A simple extension of the pooling function is to learn the kernel in a Convolutionary Neural Network (CNN).The output of the zj encoder contains information about a context of fixed size depending on the kernel width k, but the desired context width can vary. This can be achieved by stacking several layers of turns, followed by nonlinearity: additional layers increase the total context size, while nonlinearity can modulate the effective size of the context as needed. For example, stacking 5 turns with kernel width k = 3 leads to an input field of 11 words, i.e. each output depends on 11 input words, and the nonlinearity allows the encoder to exhaust the full input field or focus on fewer words.To simplify learning for deep encoders, we add residual connections from the input of each conversion to the output and then apply the linear non-encoding function to the netarize output (which is therefore not put together by He).h output."}, {"heading": "3.3 RELATED WORK", "text": "Kalchbrenner & Blunsom (2013) introduce a revolutionary set encoder, in which a multi-layered CNN creates a fixed embedding for a source sentence or n-gram representation, followed by transposed turns to directly generate a pro-token decoder input, the latter requiring the length of the translation before generation, and both models were evaluated by correcting the output of an existing translation system. Cho et al. (2014a) suggest a gated recursive CNN, which is repeated until a fixed size is reached, but the recurring encoder achieves greater accuracy. Subsequently, the authors improved the model through a soft attention mechanism, but not again using revolutionary encoder models (Bahdanau et al., 2015)."}, {"heading": "4 EXPERIMENTAL SETUP", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "4.1 DATASETS", "text": "We evaluate different encoders and place architectural decisions on a small dataset from the German English machine translation track of IWSLT 2014 (Cettolo et al., 2014) with a setting similar to Ranzato et al. (2015). Unless otherwise specified, we limit training sets to no more than 175 words; test sets are not filtered. This is a higher threshold than other publications, but ensures adequate training of position embeddings for non-recurring encoders; the longitudinal threshold has no significant effect on recurring encoders. Length filtering of results into 167K sets and we test on the concatenation of tst2010, tst2012, tst2010 and dev2010, which consist of 6948 sentence pairs. Our final results are based on three major WMT tasks: WMT '16 English-Romanian."}, {"heading": "4.2 MODEL PARAMETERS", "text": "We use 512 hidden units for both recursive encoders and decoders. All embedding, including the output produced by the decoder before the final linear layer, has 256 dimensions. In the WMT corpora, we find that we can improve the performance of bidirectional LSTM models (BiLSTM) by using 512-dimensional word embeddings. Model weights are initialized by a uniform distribution within [\u2212 0.05, 0.05]. In the case of convective layers, we use a uniform distribution of [\u2212 kd \u2212 0.5, kd \u2212 0.5] where k is the core width (during this work, we use 3) and d is the input size for the first layer and the number of hidden units for the subsequent layers (Collobert et al., 2011b). In CNN-c, we form the input of the larger and the matching of the linear T model to the respective size."}, {"heading": "4.3 OPTIMIZATION", "text": "Recurring models are trained with Adam as they benefit from aggressive optimization. We use a step width of 3.125 \u00b7 10 \u2212 4 and an early cessation based on validation perplexity (Kingma & Ba, 2014). For non-recurring encoders, we achieve the best results with stochastic gradient descent (SGD) and annealing: We use a learning rate of 0.1 and when validation perplexity ceases, we improve the learning rate by an order of magnitude per epoch until it is below 10 \u2212 4. For all models, we use mini-batches of 32 sets for IWSLT '14 and 64 for WMT. We use truncated backpropagation over time to limit the length of the target sequences per mini-batch to 25 words. Gradations are normalized by the mini-batches to 32 sets for IWSLT' 14 and 64 for WMT."}, {"heading": "4.4 EVALUATION", "text": "We report on the accuracy of individual systems by creating several identical models with different random seeds (5 for IWSLT '14, 3 for WMT) and select the one with the best validation perplexity for the final BLEU evaluation. Translations are generated by a bar search and we normalize log-likelihood2Different from the other data sets, we reduce the training data and the evaluation with case-insensitive BLEU. 3We followed the pre-processing of https: / / github.com / rsennrich / wmt16-scripts / blob / master / sample / preprocess.sh and added the backward translated data from http: / / data. statmt.org / rsennrich / wmt16 _ backtranslations / en-ro.scores by sentence length. On IWSLT' 14 we use a beam width of 10 and for WMT models we set to a separate test."}, {"heading": "5 RESULTS", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "5.1 RECURRENT VS. NON-RECURRENT ENCODERS", "text": "We first compare recursive and non-recurrent encoders in terms of conplexity and BLEU on IWSLT '14 with and without position embedding (\u00a7 3.1) and include a phrase-based system (Koehn et al., 2007). Table 1 shows that a single-layer convolutional model with position embedding (Convolutional) can outperform both a unidirectional LSTM encoder (LSTM) and a bidirectional LSTM encoder (BiLSTM). Next, we increase the depth of the convolutional encoder by varying the number of layers in CNN-a and CNN-c between 1 and 10 and obtain best validation rates for CNN-a and three layers for CNN-c. This configuration exceeds BiLSTM by 0.7 BLEU (Deep Convolutional 6 / 3)."}, {"heading": "5.2 EVALUATION ON WMT CORPORA", "text": "It is about the question of whether and how one sees oneself in a position to survive oneself, and about the question of how one can survive oneself if one wants to survive oneself. (...) It is about the question of how one can survive oneself. (...) It is about the question of how one can survive oneself. (...) It is about the question of how one can survive oneself. (...) It is about the question of how one can survive oneself. (...) It is about how one can survive oneself. (...) It is about how one can survive oneself. (...) It is about how one can survive oneself. (...) It is about. (...) It is about outliving oneself. (...). (...) It is about outliving oneself. (...). (...) It is about outliving oneself. (...). (...) It is about outliving oneself. (...). (...) It is about outliving oneself. (...). (...) It is about outliving oneself. (...). (...). (...) It is about outliving oneself. (...). (...). (...) It is about outliving oneself. (...). (...). (...) It is about outliving oneself. (...). (...). (...). (...) It is about outliving oneself. (.... (...). (...) It is about outliving oneself."}, {"heading": "5.3 CONVOLUTIONAL ENCODER ARCHITECTURE DETAILS", "text": "Next we motivate our design of the Convolutional Encoder (\u00a7 3.2). We use the smaller IWSLT '14 German-English setup with no unknown word exchange to allow for a quick experimental reversal. BLEU results are calculated over three training runs with different seeds. Figure 1 shows accuracy for a different number of layers of the two CNNs with and without residual connections. Our first observation is that the calculation of conditional input ci directly via embedding e (line \"without CNN-c\") already works well with 28.3 BLEU with a single CNN-a layer and 29.1 BLEU for CNN-a with 7 layers (Figure 1a). However, the increase in the number of CNN-c layers directly via embedding e (line \"without CNN-c\") does not result in any further improvement. Likewise, increasing the number of layers in CNN-a beyond six does not lead to an increase in the accuracy of visualizations (Figure 1a)."}, {"heading": "5.4 TRAINING AND GENERATION SPEED", "text": "For training, we use the fast CuDNN LSTM implementation for layers without attention and experiment on IWSLT '14 with batch size 32. The single-layer BiLSTM model trains at 4300 target words / second, while the 6 / 3 deep Convolutionary Encoder compares at 5500 words / second on an NVidia Tesla M40 GPU. We observe no shorter general training time as SGD converges slower than Adam we use for BiLSTM models. We measure the generation speed on an Intel Haswell CPU clocked at 2.50 GHz with a single thread for BLAS operations. We use vocabulary selection that neglects generation by up to a factor of ten at no cost to calculate the final production shift (Mi et al al al., 2016; L'Hostis et al al al al al al al al al al al al al al al al al al al."}, {"heading": "6 CONCLUSION", "text": "This approach is more parallelizable than recursive networks and provides a shorter way to capture far-reaching dependencies in the source. We believe it is essential to use source position embedding and various CNNs for calculating attention value and conditional input aggregation. Our experiments show that revolutionary encoders work on an equal footing or better than baselines based on bi-directional LSTM encoders. Compared to other recent work, our deeper Constitutional Encoder performs better (WMT '16 English-Romanian) or approaches the best published results to date obtained with significantly more complex models (WMT' 14 English-French) or derives from improvements that are orthogonal to our work (WMT '15 English-German). Our architecture also leads to large speed improvements of the generation: Translation models using our Convolutionary Encoder can double as fast as strong baselines with better attention on a cursive level (WMT' 15)."}, {"heading": "ACKNOWLEDGMENTS", "text": "We would like to thank Sumit Chopra and Marc'Aurelio Ranzato for the helpful conversations in connection with this work."}, {"heading": "A ALIGNMENT VISUALIZATION", "text": "Figure 3 and Figure 4 show the attention scores for a sample of WMT '15 and WMT' 14. Attention scores in the BiLSTM output are sharp but do not necessarily represent correct alignment. For CNN encoders, the results are less focused, but they point to an approximate source location, such as Figure 3b, where the clause \"over 1,000 people have been held hostage.\" Attention of the encoders is often shifted by a token based on a model, as both are shown in Figure 3b and Figure 4b."}], "references": [{"title": "Neural machine translation by jointly learning to align and translate", "author": ["Dzmitry Bahdanau", "Kyunghyun Cho", "Yoshua Bengio"], "venue": "In Proc. of ICLR,", "citeRegEx": "Bahdanau et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Bahdanau et al\\.", "year": 2015}, {"title": "In Proc", "author": ["James Bradbury", "Richard Socher. MetaMind Neural Machine Translation System for WMT"], "venue": "of WMT, 2016.", "citeRegEx": "Bradbury and WMT,? 2016", "shortCiteRegEx": "Bradbury and WMT", "year": 2016}, {"title": "Report on the 11th IWSLT evaluation campaign", "author": ["Mauro Cettolo", "Jan Niehues", "Sebastian St\u00fcker", "Luisa Bentivogli", "Marcello Federico"], "venue": "In Proc. of IWSLT,", "citeRegEx": "Cettolo et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Cettolo et al\\.", "year": 2014}, {"title": "On the Properties of Neural Machine Translation: Encoder-decoder Approaches", "author": ["Kyunghyun Cho", "Bart Van Merri\u00ebnboer", "Dzmitry Bahdanau", "Yoshua Bengio"], "venue": "In Proc. of SSST,", "citeRegEx": "Cho et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Cho et al\\.", "year": 2014}, {"title": "Learning Phrase Representations using RNN Encoder-Decoder for Statistical Machine Translation", "author": ["Kyunghyun Cho", "Bart Van Merri\u00ebnboer", "Caglar Gulcehre", "Dzmitry Bahdanau", "Fethi Bougares", "Holger Schwenk", "Yoshua Bengio"], "venue": "In Proc. of EMNLP,", "citeRegEx": "Cho et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Cho et al\\.", "year": 2014}, {"title": "A Character-level Decoder without Explicit Segmentation for Neural Machine Translation", "author": ["Junyoung Chung", "Kyunghyun Cho", "Yoshua Bengio"], "venue": "arXiv preprint arXiv:1603.06147,", "citeRegEx": "Chung et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Chung et al\\.", "year": 2016}, {"title": "Torch7: A Matlab-like Environment for Machine Learning", "author": ["Ronan Collobert", "Koray Kavukcuoglu", "Clement Farabet"], "venue": "In BigLearn, NIPS Workshop,", "citeRegEx": "Collobert et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Collobert et al\\.", "year": 2011}, {"title": "A Simple, Fast, and Effective Reparameterization of IBM Model 2", "author": ["Chris Dyer", "Victor Chahuneau", "Noah A Smith"], "venue": "Proc. of ACL,", "citeRegEx": "Dyer et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Dyer et al\\.", "year": 2013}, {"title": "Deep Residual Learning for Image Recognition", "author": ["Kaiming He", "Xiangyu Zhang", "Shaoqing Ren", "Jian Sun"], "venue": "In Proc. of CVPR,", "citeRegEx": "He et al\\.,? \\Q2015\\E", "shortCiteRegEx": "He et al\\.", "year": 2015}, {"title": "Long short-term memory", "author": ["Sepp Hochreiter", "J\u00fcrgen Schmidhuber"], "venue": "Neural computation,", "citeRegEx": "Hochreiter and Schmidhuber.,? \\Q1997\\E", "shortCiteRegEx": "Hochreiter and Schmidhuber.", "year": 1997}, {"title": "Montreal Neural Machine Translation systems for WMT15", "author": ["S\u00e9bastien Jean", "Orhan Firat", "Kyunghyun Cho", "Roland Memisevic", "Yoshua Bengio"], "venue": "In Proc. of WMT,", "citeRegEx": "Jean et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Jean et al\\.", "year": 2015}, {"title": "Is Neural Machine Translation Ready for Deployment? A Case Study on 30 Translation Directions", "author": ["Marcin Junczys-Dowmunt", "Tomasz Dwojak", "Hieu Hoang"], "venue": "arXiv preprint arXiv:1610.01108,", "citeRegEx": "Junczys.Dowmunt et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Junczys.Dowmunt et al\\.", "year": 2016}, {"title": "Recurrent Continuous Translation Models", "author": ["Nal Kalchbrenner", "Phil Blunsom"], "venue": "In Proc. of EMNLP,", "citeRegEx": "Kalchbrenner and Blunsom.,? \\Q2013\\E", "shortCiteRegEx": "Kalchbrenner and Blunsom.", "year": 2013}, {"title": "Adam: A Method for Stochastic Optimization", "author": ["Diederik P. Kingma", "Jimmy Ba"], "venue": "Proc. of ICLR,", "citeRegEx": "Kingma and Ba.,? \\Q2014\\E", "shortCiteRegEx": "Kingma and Ba.", "year": 2014}, {"title": "Moses: Open Source Toolkit for Statistical Machine Translation", "author": ["Philipp Koehn", "Hieu Hoang", "Alexandra Birch", "Chris Callison-Burch", "Marcello Federico", "Nicola Bertoldi", "Brooke Cowan", "Wade Shen", "Christine Moran", "Richard Zens", "Chris Dyer", "Ondej Bojar", "Alexandra Constantin", "Evan Herbst"], "venue": "In Proc. of ACL,", "citeRegEx": "Koehn et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Koehn et al\\.", "year": 2007}, {"title": "Convolutional Encoders for Neural Machine Translation", "author": ["Andrew Lamb", "Michael Xie"], "venue": "https: //cs224d.stanford.edu/reports/LambAndrew.pdf,", "citeRegEx": "Lamb and Xie.,? \\Q2016\\E", "shortCiteRegEx": "Lamb and Xie.", "year": 2016}, {"title": "Vocabulary Selection Strategies for Neural Machine Translation", "author": ["Gurvan L\u2019Hostis", "David Grangier", "Michael Auli"], "venue": "arXiv preprint arXiv:1610.00072,", "citeRegEx": "L.Hostis et al\\.,? \\Q2016\\E", "shortCiteRegEx": "L.Hostis et al\\.", "year": 2016}, {"title": "Effective approaches to attentionbased neural machine translation", "author": ["Minh-Thang Luong", "Hieu Pham", "Christopher D Manning"], "venue": "In Proc. of EMNLP,", "citeRegEx": "Luong et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Luong et al\\.", "year": 2015}, {"title": "Addressing the Rare Word Problem in Neural Machine Translation", "author": ["Minh-Thang Luong", "Ilya Sutskever", "Quoc V Le", "Oriol Vinyals", "Wojciech Zaremba"], "venue": "In Proc. of ACL,", "citeRegEx": "Luong et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Luong et al\\.", "year": 2015}, {"title": "Encoding Source Language with Convolutional Neural Network for Machine Translation", "author": ["Fandong Meng", "Zhengdong Lu", "Mingxuan Wang", "Hang Li", "Wenbin Jiang", "Qun Liu"], "venue": "In Proc. of ACL,", "citeRegEx": "Meng et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Meng et al\\.", "year": 2015}, {"title": "Vocabulary Manipulation for Neural Machine Translation", "author": ["Haitao Mi", "Zhiguo Wang", "Abe Ittycheriah"], "venue": "arXiv preprint arXiv:1605.03209,", "citeRegEx": "Mi et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Mi et al\\.", "year": 2016}, {"title": "On the Difficulty of Training Recurrent Neural Networks", "author": ["Razvan Pascanu", "Tomas Mikolov", "Yoshua Bengio"], "venue": "ICML (3),", "citeRegEx": "Pascanu et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Pascanu et al\\.", "year": 2013}, {"title": "Convolutional Neural Network Language Models", "author": ["Ngoc-Quan Pham", "Germn Kruszewski", "Gemma Boleda"], "venue": "In Proc. of EMNLP,", "citeRegEx": "Pham et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Pham et al\\.", "year": 2016}, {"title": "Sequence level Training with Recurrent Neural Networks", "author": ["Marc\u2019Aurelio Ranzato", "Sumit Chopra", "Michael Auli", "Wojciech Zaremba"], "venue": "In Proc. of ICLR,", "citeRegEx": "Ranzato et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Ranzato et al\\.", "year": 2015}, {"title": "Neural Machine Translation of Rare Words with Subword Units", "author": ["Rico Sennrich", "Barry Haddow", "Alexandra Birch"], "venue": "In Proc. of ACL,", "citeRegEx": "Sennrich et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Sennrich et al\\.", "year": 2016}, {"title": "Edinburgh neural machine translation systems for wmt", "author": ["Rico Sennrich", "Barry Haddow", "Alexandra Birch"], "venue": null, "citeRegEx": "Sennrich et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Sennrich et al\\.", "year": 2016}, {"title": "Dropout: a simple way to prevent Neural Networks from overfitting", "author": ["Nitish Srivastava", "Geoffrey E. Hinton", "Alex Krizhevsky", "Ilya Sutskever", "Ruslan Salakhutdinov"], "venue": null, "citeRegEx": "Srivastava et al\\.,? \\Q1929\\E", "shortCiteRegEx": "Srivastava et al\\.", "year": 1929}, {"title": "End-to-end Memory Networks", "author": ["Sainbayar Sukhbaatar", "Jason Weston", "Rob Fergus", "Arthur Szlam"], "venue": "In Proc. of NIPS, pp", "citeRegEx": "Sukhbaatar et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Sukhbaatar et al\\.", "year": 2015}, {"title": "Sequence to Sequence Learning with Neural Networks", "author": ["Ilya Sutskever", "Oriol Vinyals", "Quoc V Le"], "venue": "In Proc. of NIPS,", "citeRegEx": "Sutskever et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Sutskever et al\\.", "year": 2014}, {"title": "Context-dependent Translation selection using Convolutional Neural Network", "author": ["Zhaopeng Tu", "Baotian Hu", "Zhengdong Lu", "Hang Li"], "venue": "In Proc. of ACL-IJCNLP,", "citeRegEx": "Tu et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Tu et al\\.", "year": 2015}, {"title": "Google\u2019s Neural Machine Translation System: Bridging the Gap between Human and Machine Translation", "author": ["Yonghui Wu", "Mike Schuster", "Zhifeng Chen", "Quoc V Le", "Mohammad Norouzi", "Wolfgang Macherey", "Maxim Krikun", "Yuan Cao", "Qin Gao", "Klaus Macherey"], "venue": "arXiv preprint arXiv:1609.08144,", "citeRegEx": "Wu et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Wu et al\\.", "year": 2016}, {"title": "Neural Machine Translation with Recurrent Attention Modeling", "author": ["Zichao Yang", "Zhiting Hu", "Yuntian Deng", "Chris Dyer", "Alex Smola"], "venue": "arXiv preprint arXiv:1607.05108,", "citeRegEx": "Yang et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Yang et al\\.", "year": 2016}, {"title": "Deep Recurrent Models with FastForward Connections for Neural Machine Translation", "author": ["Jie Zhou", "Ying Cao", "Xuguang Wang", "Peng Li", "Wei Xu"], "venue": "arXiv preprint arXiv:1606.04199,", "citeRegEx": "Zhou et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Zhou et al\\.", "year": 2016}, {"title": "BLEU per sentence length on WMT\u201915 English-German newstest2015. The test set is partitioned into 15 equally-sized buckets according to source sentence length. One characteristic of our convolutional encoder architecture is that the context over which outputs", "author": ["Deep Conv"], "venue": null, "citeRegEx": "Conv.,? \\Q2015\\E", "shortCiteRegEx": "Conv.", "year": 2015}], "referenceMentions": [{"referenceID": 28, "context": "Neural machine translation (NMT) is an end-to-end approach to machine translation (Sutskever et al., 2014).", "startOffset": 82, "endOffset": 106}, {"referenceID": 0, "context": "The most successful approach to date encodes the source sentence with a bi-directional recurrent neural network (RNN) into a variable length representation and then generates the translation left-to-right with another RNN where both components interface via a soft-attention mechanism (Bahdanau et al., 2015; Luong et al., 2015a; Bradbury & Socher, 2016; Sennrich et al., 2016b).", "startOffset": 285, "endOffset": 378}, {"referenceID": 3, "context": "1997) or gated recurrent units (GRU; Cho et al. 2014), often with residual or skip connections (Wu et al.", "startOffset": 31, "endOffset": 53}, {"referenceID": 30, "context": "2014), often with residual or skip connections (Wu et al., 2016; Zhou et al., 2016) to enable stacking of several layers (\u00a72).", "startOffset": 47, "endOffset": 83}, {"referenceID": 32, "context": "2014), often with residual or skip connections (Wu et al., 2016; Zhou et al., 2016) to enable stacking of several layers (\u00a72).", "startOffset": 47, "endOffset": 83}, {"referenceID": 0, "context": "The general architecture of the models in this work follows the encoder-decoder approach with soft attention first introduced in Bahdanau et al. (2015). A source sentence x = (x1, .", "startOffset": 129, "endOffset": 152}, {"referenceID": 0, "context": "In preliminary experiments, we did not find the MLP attention of Bahdanau et al. (2015) to perform significantly better in terms of BLEU nor perplexity.", "startOffset": 65, "endOffset": 88}, {"referenceID": 0, "context": "In preliminary experiments, we did not find the MLP attention of Bahdanau et al. (2015) to perform significantly better in terms of BLEU nor perplexity. However, we found the dot-product attention to be more favorable in terms of training and evaluation speed. We use bi-directional LSTMs to implement recurrent encoders similar to Zhou et al. (2016) which achieved some of the best WMT14 English-French results reported to date.", "startOffset": 65, "endOffset": 351}, {"referenceID": 27, "context": "Position embeddings have also been found helpful in memory networks for question-answering and language modeling (Sukhbaatar et al., 2015).", "startOffset": 113, "endOffset": 138}, {"referenceID": 23, "context": "A simple baseline for non-recurrent encoders is the pooling model described in Ranzato et al. (2015) which simply averages the embeddings of k consecutive words.", "startOffset": 79, "endOffset": 101}, {"referenceID": 23, "context": "We set k to 5 in all experiments as Ranzato et al. (2015).", "startOffset": 36, "endOffset": 58}, {"referenceID": 8, "context": "To ease learning for deep encoders, we add residual connections from the input of each convolution to the output and then apply the non-linear activation function to the output (tanh; He et al., 2015); the non-linearities are therefore not \u2019bypassed\u2019.", "startOffset": 177, "endOffset": 200}, {"referenceID": 0, "context": "In follow-up work, the authors improved the model via a soft-attention mechanism but did not re-consider convolutional encoder models (Bahdanau et al., 2015).", "startOffset": 134, "endOffset": 157}, {"referenceID": 22, "context": "Convolutional architectures have also been successful in language modeling but so far failed to outperform LSTMs (Pham et al., 2016).", "startOffset": 113, "endOffset": 132}, {"referenceID": 2, "context": "Cho et al. (2014a) propose a gated recursive CNN which is repeatedly applied until a fixed-size representation is obtained but the recurrent encoder achieves higher accuracy.", "startOffset": 0, "endOffset": 19}, {"referenceID": 0, "context": "In follow-up work, the authors improved the model via a soft-attention mechanism but did not re-consider convolutional encoder models (Bahdanau et al., 2015). Concurrently to our work, Kalchbrenner et al. (2016) have introduced convolutional translation models without an explicit attention mechanism but their approach does not yet result in state-of-the-art accuracy.", "startOffset": 135, "endOffset": 212}, {"referenceID": 0, "context": "In follow-up work, the authors improved the model via a soft-attention mechanism but did not re-consider convolutional encoder models (Bahdanau et al., 2015). Concurrently to our work, Kalchbrenner et al. (2016) have introduced convolutional translation models without an explicit attention mechanism but their approach does not yet result in state-of-the-art accuracy. Lamb & Xie (2016) also proposed a multi-layer CNN to generate a fixed-size encoder representation but their work lacks quantitative evaluation in terms of BLEU.", "startOffset": 135, "endOffset": 388}, {"referenceID": 0, "context": "In follow-up work, the authors improved the model via a soft-attention mechanism but did not re-consider convolutional encoder models (Bahdanau et al., 2015). Concurrently to our work, Kalchbrenner et al. (2016) have introduced convolutional translation models without an explicit attention mechanism but their approach does not yet result in state-of-the-art accuracy. Lamb & Xie (2016) also proposed a multi-layer CNN to generate a fixed-size encoder representation but their work lacks quantitative evaluation in terms of BLEU. Meng et al. (2015) and Tu et al.", "startOffset": 135, "endOffset": 550}, {"referenceID": 0, "context": "In follow-up work, the authors improved the model via a soft-attention mechanism but did not re-consider convolutional encoder models (Bahdanau et al., 2015). Concurrently to our work, Kalchbrenner et al. (2016) have introduced convolutional translation models without an explicit attention mechanism but their approach does not yet result in state-of-the-art accuracy. Lamb & Xie (2016) also proposed a multi-layer CNN to generate a fixed-size encoder representation but their work lacks quantitative evaluation in terms of BLEU. Meng et al. (2015) and Tu et al. (2015) applied convolutional models to score phrase-pairs of traditional phrase-based and dependency-based translation models.", "startOffset": 135, "endOffset": 571}, {"referenceID": 2, "context": "We evaluate different encoders and ablate architectural choices on a small dataset from the GermanEnglish machine translation track of IWSLT 2014 (Cettolo et al., 2014) with a similar setting to Ranzato et al.", "startOffset": 146, "endOffset": 168}, {"referenceID": 2, "context": "We evaluate different encoders and ablate architectural choices on a small dataset from the GermanEnglish machine translation track of IWSLT 2014 (Cettolo et al., 2014) with a similar setting to Ranzato et al. (2015). Unless otherwise stated, we restrict training sentences to have no more than 175 words; test sentences are not filtered.", "startOffset": 147, "endOffset": 217}, {"referenceID": 14, "context": "9M sentence pairs (Koehn et al., 2007).", "startOffset": 18, "endOffset": 38}, {"referenceID": 23, "context": "We use the same data and pre-processing as Sennrich et al. (2016b) and train on 2.", "startOffset": 43, "endOffset": 67}, {"referenceID": 21, "context": "We re-normalizing the gradients if their norm exceeds 25 (Pascanu et al., 2013).", "startOffset": 57, "endOffset": 79}, {"referenceID": 6, "context": "Gradients of convolutional layers are scaled by sqrt(dim(input))\u22121 similar to Collobert et al. (2011b). We use dropout on the embeddings and decoder outputs hi with a rate of 0.", "startOffset": 78, "endOffset": 103}, {"referenceID": 10, "context": "Prior to scoring the generated translations against the respective references, we perform unknown word replacement based on attention scores (Jean et al., 2015).", "startOffset": 141, "endOffset": 160}, {"referenceID": 7, "context": "Dictionaries were extracted from the aligned training data that was aligned with fast align (Dyer et al., 2013).", "startOffset": 92, "endOffset": 111}, {"referenceID": 14, "context": "1) and include a phrase-based system (Koehn et al., 2007).", "startOffset": 37, "endOffset": 57}, {"referenceID": 24, "context": "WMT\u201916 English-Romanian Encoder Vocabulary BLEU Sennrich et al. (2016b) BiGRU BPE 90K 28.", "startOffset": 48, "endOffset": 72}, {"referenceID": 9, "context": "WMT\u201915 English-German Encoder Vocabulary BLEU Jean et al. (2015) RNNsearch-LV BiGRU 500K 22.", "startOffset": 46, "endOffset": 65}, {"referenceID": 9, "context": "WMT\u201915 English-German Encoder Vocabulary BLEU Jean et al. (2015) RNNsearch-LV BiGRU 500K 22.4 Sennrich et al. (2016a) BPE-J90k BiGRU BPE 90K 22.", "startOffset": 46, "endOffset": 118}, {"referenceID": 5, "context": "8 Chung et al. (2016) BPE-Char BiGRU Char 500 23.", "startOffset": 2, "endOffset": 22}, {"referenceID": 5, "context": "8 Chung et al. (2016) BPE-Char BiGRU Char 500 23.9 Yang et al. (2016) RNNSearch + UNK replace BiLSTM 50K 24.", "startOffset": 2, "endOffset": 70}, {"referenceID": 0, "context": "WMT\u201914 English-French (12M) Encoder Vocabulary BLEU Bahdanau et al. (2015) RNNsearch BiGRU 30K 28.", "startOffset": 52, "endOffset": 75}, {"referenceID": 0, "context": "WMT\u201914 English-French (12M) Encoder Vocabulary BLEU Bahdanau et al. (2015) RNNsearch BiGRU 30K 28.5 Jean et al. (2015) RNNsearch-LV BiGRU 500K 32.", "startOffset": 52, "endOffset": 119}, {"referenceID": 0, "context": "WMT\u201914 English-French (12M) Encoder Vocabulary BLEU Bahdanau et al. (2015) RNNsearch BiGRU 30K 28.5 Jean et al. (2015) RNNsearch-LV BiGRU 500K 32.7 Luong et al. (2015b) Single LSTM 6-layer LSTM 40K 34.", "startOffset": 52, "endOffset": 169}, {"referenceID": 0, "context": "WMT\u201914 English-French (12M) Encoder Vocabulary BLEU Bahdanau et al. (2015) RNNsearch BiGRU 30K 28.5 Jean et al. (2015) RNNsearch-LV BiGRU 500K 32.7 Luong et al. (2015b) Single LSTM 6-layer LSTM 40K 34.8 Zhou et al. (2016) Deep-Att Deep BiLSTM 30K 35.", "startOffset": 52, "endOffset": 222}, {"referenceID": 22, "context": "On WMT\u201916 English-Romanian translation we compare to Sennrich et al. (2016b), the winning single system entry for this language pair.", "startOffset": 53, "endOffset": 77}, {"referenceID": 22, "context": "On WMT\u201916 English-Romanian translation we compare to Sennrich et al. (2016b), the winning single system entry for this language pair. Their model consists of a bi-directional GRU encoder, a GRU decoder and MLP-based attention. They use byte pair encoding (BPE) to achieve open-vocabulary translation and dropout in all components of the neural network to achieve 28.1 BLEU; we use the same pre-processing but no BPE (\u00a74). The results (Table 2) show that our bi-directional LSTM encoder matches their accuracy with smaller layers: Sennrich et al. (2016b) use 500-dimensional embeddings and 1024 unit hidden layers, while we use 512 dimensions for both embeddings and hidden layers.", "startOffset": 53, "endOffset": 554}, {"referenceID": 9, "context": "On WMT\u201915 English to German, we compare to a BiLSTM baseline and prior work: Jean et al. (2015) introduce a large output vocabulary; Sennrich et al.", "startOffset": 77, "endOffset": 96}, {"referenceID": 9, "context": "On WMT\u201915 English to German, we compare to a BiLSTM baseline and prior work: Jean et al. (2015) introduce a large output vocabulary; Sennrich et al. (2016a) use a joint source and target BPE; the decoder of Chung et al.", "startOffset": 77, "endOffset": 157}, {"referenceID": 5, "context": "(2016a) use a joint source and target BPE; the decoder of Chung et al. (2016) operates on the character-level; Yang et al.", "startOffset": 58, "endOffset": 78}, {"referenceID": 5, "context": "(2016a) use a joint source and target BPE; the decoder of Chung et al. (2016) operates on the character-level; Yang et al. (2016) uses LSTMs instead of GRUs and feeds the conditional input to the output layer as well as to the decoder.", "startOffset": 58, "endOffset": 130}, {"referenceID": 5, "context": "(2016a) use a joint source and target BPE; the decoder of Chung et al. (2016) operates on the character-level; Yang et al. (2016) uses LSTMs instead of GRUs and feeds the conditional input to the output layer as well as to the decoder. Our single-layer BiLSTM baseline performs competitively compared to prior work and a two-layer BiLSTM performs about 0.4 BLEU better at 23.6 BLEU. Previous work also used multi-layer setups, e.g., Chung et al. (2016) has two layers both in the encoder and the decoder with 1024 hidden units, and Yang et al.", "startOffset": 58, "endOffset": 453}, {"referenceID": 5, "context": "(2016a) use a joint source and target BPE; the decoder of Chung et al. (2016) operates on the character-level; Yang et al. (2016) uses LSTMs instead of GRUs and feeds the conditional input to the output layer as well as to the decoder. Our single-layer BiLSTM baseline performs competitively compared to prior work and a two-layer BiLSTM performs about 0.4 BLEU better at 23.6 BLEU. Previous work also used multi-layer setups, e.g., Chung et al. (2016) has two layers both in the encoder and the decoder with 1024 hidden units, and Yang et al. (2016) use 1000 hidden units per LSTM.", "startOffset": 58, "endOffset": 551}, {"referenceID": 17, "context": "We also outperform several previous systems, including the very deep encoder-decoder model proposed by Luong et al. (2015a). Our best result is just 0.", "startOffset": 103, "endOffset": 124}, {"referenceID": 17, "context": "We also outperform several previous systems, including the very deep encoder-decoder model proposed by Luong et al. (2015a). Our best result is just 0.2 BLEU below Zhou et al. (2016) who use a very deep LSTM setup with a 9-layer encoder, a 7-layer decoder, shortcut connections and extensive regularization with dropout and L2 regularization.", "startOffset": 103, "endOffset": 183}, {"referenceID": 20, "context": "We use vocabulary selection which can speed up generation by up to a factor of ten at no cost in accuracy via making the time to compute the final output layer negligible (Mi et al., 2016; L\u2019Hostis et al., 2016).", "startOffset": 171, "endOffset": 211}, {"referenceID": 16, "context": "We use vocabulary selection which can speed up generation by up to a factor of ten at no cost in accuracy via making the time to compute the final output layer negligible (Mi et al., 2016; L\u2019Hostis et al., 2016).", "startOffset": 171, "endOffset": 211}, {"referenceID": 15, "context": ", 2016; L\u2019Hostis et al., 2016). This shifts the focus on the efficiency of the encoder and decoder components. On IWSLT\u201914 (Table 3a) the convolutional encoder increases the speed of the overall model by a factor of 1.35 compared to the BiLSTM encoder while improving accuracy by 0.7 BLEU. In this setup both encoders models have the same hidden layer and embedding sizes. On the larger WMT\u201915 English-German task (Table 3b) the convolutional encoder speeds up generation by 2.1 times compared to a two-layer BiLSTM. This corresponds to 231 source words/second with beam size 5. Our best model on this dataset generates 203 words/second but at slightly lower accuracy compared to the full vocabulary setting in Table 2. The recurrent encoder uses larger embeddings than the convolutional encoder which were required for the models to match in accuracy. For comparison, the quantized deep LSTM-based model in Wu et al. (2016) processes 104.", "startOffset": 8, "endOffset": 925}, {"referenceID": 11, "context": "The optimized RNNsearch model and C++ decoder described by Junczys-Dowmunt et al. (2016) translates 265.", "startOffset": 59, "endOffset": 89}, {"referenceID": 30, "context": "The LSTM decoder may benefit from residual connections (Wu et al., 2016) and recurrent attention (Yang et al.", "startOffset": 55, "endOffset": 72}, {"referenceID": 31, "context": ", 2016) and recurrent attention (Yang et al., 2016).", "startOffset": 32, "endOffset": 51}], "year": 2016, "abstractText": "The prevalent approach to neural machine translation relies on bi-directional LSTMs to encode the source sentence. In this paper we present a faster and conceptually simpler architecture based on a succession of convolutional layers. This allows to encode the entire source sentence simultaneously compared to recurrent networks for which computation is constrained by temporal dependencies. We achieve a new state-of-the-art on WMT\u201916 English-Romanian translation and outperform several recently published results on the WMT\u201915 English-German task. We also achieve almost the same accuracy as a very deep LSTM setup on WMT\u201914 English-French translation. Our convolutional encoder speeds up CPU decoding by more than two times at the same or higher accuracy as a strong bidirectional LSTM baseline.", "creator": "LaTeX with hyperref package"}}}