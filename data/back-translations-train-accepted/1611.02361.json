{"id": "1611.02361", "review": {"conference": "HLT-NAACL", "VERSION": "v1", "DATE_OF_SUBMISSION": "8-Nov-2016", "title": "Dependency Sensitive Convolutional Neural Networks for Modeling Sentences and Documents", "abstract": "The goal of sentence and document modeling is to accurately represent the meaning of sentences and documents for various Natural Language Processing tasks. In this work, we present Dependency Sensitive Convolutional Neural Networks (DSCNN) as a general-purpose classification system for both sentences and documents. DSCNN hierarchically builds textual representations by processing pretrained word embeddings via Long Short-Term Memory networks and subsequently extracting features with convolution operators. Compared with existing recursive neural models with tree structures, DSCNN does not rely on parsers and expensive phrase labeling, and thus is not restricted to sentence-level tasks. Moreover, unlike other CNN-based models that analyze sentences locally by sliding windows, our system captures both the dependency information within each sentence and relationships across sentences in the same document. Experiment results demonstrate that our approach is achieving state-of-the-art performance on several tasks, including sentiment analysis, question type classification, and subjectivity classification.", "histories": [["v1", "Tue, 8 Nov 2016 01:48:15 GMT  (192kb,D)", "http://arxiv.org/abs/1611.02361v1", "NAACL2016"]], "COMMENTS": "NAACL2016", "reviews": [], "SUBJECTS": "cs.CL", "authors": ["rui zhang", "honglak lee", "dragomir r radev"], "accepted": true, "id": "1611.02361"}, "pdf": {"name": "1611.02361.pdf", "metadata": {"source": "CRF", "title": "Dependency Sensitive Convolutional Neural Networks for Modeling Sentences and Documents", "authors": ["Rui Zhang", "Honglak Lee", "Dragomir Radev"], "emails": ["ryanzh@umich.edu", "honglak@eecs.umich.edu", "radev@umich.edu"], "sections": [{"heading": "1 Introduction", "text": "The question is how it could have come to such a situation. \"I don't think it could have come to that,\" he said. \"But I don't think it has come to that.\" \"I believe it has.\" \"No.\" \"No.\" \"No.\" \"No.\" \"No.\" \"No.\" \"No.\" \"No.\" \"No.\" \"No.\" \"\" No. \"\" No. \"\" No. \"\" No. \"\" No. \"\" No. \"\" No. \"No.\" \"No.\" No. \"\" No. \"\" No. \"\" No. \"No.\" No. \"\" No. \"\" No. \"No.\" No. \"No.\" No. \"No.\" No. \"No.\" No. \"No.\" No. \"No.\" No. \"No.\" No. \"No.\" No. \"No.\" No. \"No.\" No. \"No.\" No. \"No.\" No. \"No.\" No. \"No.\" No. \"No.\" No. \"No.\" No. \"No.\" No. \"No.\" No. \"No.\" No. \"No.\" No. \"No.\" No. \"No.\" No. \"No.\" No. \"No.\" No. \"No.\" No. \"No.\" No. \"No.\" No. \"No.\" No. \"No.\" No. \"No.\" No. \"No.\" No. \"No.\" No. \"No.\" No. \"No.\" No. \"No.\" No. \"No.\" No. \"No.\" No. \"No.\" No. \"No.\" No. \"No.\" No. \"No.\" No. \"No.\" No. \"No.\" No. \"No.\" No. \"No.\" No. \"No.\""}, {"heading": "2 Related Work", "text": "The success of profound learning architectures for NLP is based primarily on progress in learning distributed word representations in semantic vector space (Bengio et al., 2003; Mikolov et al., 2013; Pennington et al., 2014), with each word being modeled with a newly evaluated vector called word embedding. In this formulation, not only single vectors are used by indexing words into a vocabulary, but also word embedding is learned by projecting words onto a low-dimensional and dense vector space that encodes both semantic and syntactic properties of words. In light of word embedding, various models have been proposed to learn the composition of words to build phrases and sentence representations. Most methods fall into three types: disordered models, sequence models, and Convolutionary Neural Networks models. In disordered models, models, textual representations of words are independent of word order."}, {"heading": "3 Preliminaries", "text": "In this section, we describe two building blocks for our system: First, we discuss Long Short-Term Memory as a powerful network for modeling sequence data, and then formulate folding and maximum overtime pooling operators for feature extraction via sequence input."}, {"heading": "3.1 Long Short-Term Memory", "text": "Recursive Neural Network (RNN) is a class of models for processing input sequences of arbitrary length b = input sequences of high tab (by recursive construction of hidden state vectors ht).At each step t, the hidden state ht is an affine function of the input vector xt at the time t and its previous hidden state ht \u2212 1, followed by a nonlinearity such as the hyperbolic tangent function: ht = tanh (Wxt + Uht \u2212 1 + b) (1), where W, U and b are parameters of the model. However, the traditional RNN suffers from exploding or vanishing gradient problems, where the gradient vectors can grow or decay exponentially as they propagate into earlier time steps. This problem makes it difficult to capture RNN, capture dependencies in a sequence b (Bengio et al., 1994; High tab et al, 1998).To address this long-term sequence problem, they have a long-term STORY relationship with high tab sequences."}, {"heading": "3.2 Convolution and Max-over-time Pooling", "text": "Folding operators have been widely used in object detection (LeCun et al., 1998), phoneme detection (Waibel et al., 1989), sentence modeling and classification (Kalchburner et al., 2014; Kim, 2014), and other traditional NLP tasks (Collobert and Weston, 2008). Considering an input set of length s: [w1, w2,..., ws], folding operators apply a number of filters to extract local features of the set. In this thesis, we use one-dimensional wide folding described in (Kalchburner et al., 2014). Let ht-Rd be the representation of wt \u2212 and F-Rd \u00b7 l as a filter in which l is the window size. One-dimensional wide folding calculates the feature chart c of length (s + l \u2212 1) c = [c1, c2,..., cs + l \u2212 3] (input set)."}, {"heading": "4 Model Architectures", "text": "Despite the fact that CNN is an order-sensitive model, traditional folding operators extract local features from any possible window of words through filters with predefined sizes. Therefore, sentences are effectively processed like a bag of n-grams, and long-distance dependencies can only be captured if we have filters long enough. To capture long-distance dependencies, great efforts have been made recently to build tree-structured models from syntactical parsing information. However, we observe that these methods suffer from three problems: First, they require an external parser and are prone to parsing errors (Iyyer et al., 2015). Also, tree-structured models require heavy supervisions to overcome disappearing gradient problems. For example, in (Socher et al., 2013) input sets can be labeled for each subphrase, and Softmax layers are applied to each internal node."}, {"heading": "4.1 Sentence Modeling", "text": "Let the input of our model be a set of the length s: [w1, w2,..., ws], and c the total number of word embedding versions. Various versions come from pre-formed word vectors such as word2vec (Mikolov et al., 2013) and GloVe (Pennington et al., 2014).The first layer of our model consists of LSTM networks that process multiple versions of word embedding. For each version of word embedding, we construct an LSTM network in which the input xt-Rd is the d-dimensional word embedding vector for wt. As described in the previous section, the LSTM layer generates a hidden state representation ht-Rd at each time step. We collect hidden state representations as the output of the LSTM layers: h (i) = [h (i) olence-maps of wds) 1, h (i) t \u2212]."}, {"heading": "4.2 Document Modeling", "text": "Our model is not limited to sentences; it can be restructured to model documents (Li et al., 2015). Intuition comes from the fact that the input of our model is a document consisting of n subordinates: [s1, s2,..., sn]. Subordinates can be achieved by dividing the document using punctuation (comma, period, question mark and exclamation mark) as separators. We use independent LSTM networks for each subordinate in the same way as the first layer of the sentence model. For each subordinate, we forward the hidden states of the corresponding LSTM network to the average pooling layer. Let's take the first sentence of the document as an example: h (i) s1 = 1len (s1) len (s1) len (s1) l = 1 hierpoj (8) after the sentence where i (layer) is hidden (i)."}, {"heading": "5 Experiments", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "5.1 Datasets", "text": "The dataset consists of 5,331 positive and 5,331 negative reviews, mostly in one sentence. We follow the practice of 10-fold cross-validation to report outcomes.Stanford Sentiment Treebank (SST) is another dataset introduced by Socher et al., 2013. The records are finely labeled (SST-5): {very negative, negative, neutral, positive, very positive} The dataset has been divided into 8,544 training sessions, 1,101 validation and 2,210 test sets. Without neutral sets, SST can also be used in binary mode (SST2), where the split includes 6,920 training sessions, 872 validation, and 1,821 audits.In addition, we apply DSCNN to data type classification tasks on TREC Dataset (Li and Roth, 2002)."}, {"heading": "5.2 Training Details and Implementation", "text": "We use two sets of 300-dimensional, pre-trained embedding, word2vec1 and GloVe2, which form two channels for our network. We use 100 folding filters each for window sizes of 3, 4 and 5. Rectified linear units (ReLU) are selected as a nonlinear function in the folding layer. For regulation, we use the dropout mode (Hinton et al., 2012) before the Softmax layers with a dropout rate of 0.5, and we do not impose l2 limitations on parameters. For regulation, we use the process-based Adadelta optimizer (Zeiler, 2012) to minimize cross-entropy loss between predicted and true distributions, and the training is stopped early when the accuracy of the validation sets begins to decline."}, {"heading": "5.3 Pretraining of LSTM", "text": "We experiment with two variants of parameter initialization of set-level LSTMs: The first variant (DSCNN in Table 1) initializes the weight matrices in LSTMs as random orthogonal matrices. In the second variant (DSCNN pretrain in Table 1) we train sequence autoencoders (Dai and Le, 2015) to read the input sets on the encoder and reconstruct the input on the decoder. We train for each task separately on the basis of the same move / valid / test splits. The pre-trained encoders serve as starting points of LSTM layers for later monitored classification tasks."}, {"heading": "5.4 Results and Discussions", "text": "Table 1 shows the results of DSCNN on different datasets and shows its effectiveness compared to other state-of-the-art methods."}, {"heading": "5.4.1 Sentence Modeling", "text": "In SST-2, DSCNN reports only slightly lower accuracy than MVCNN. In MVCNN, however, the author uses more resources, including five versions of word embedding. For SST-5, DSCNN is only second only to Tree-LSTM, which relies on parsers to create tree-structured neural models.The utility of DSCNN is illustrated by its consistently better results over sequential CNN models, including DCNN-MC. DSCNN's superiority is largely attributed to its ability to maintain long-term dependencies. Figure 3 describes the correlation between dependence length and classification accuracy. While CNN-MC and DSCNN are similar when the sum of dependence arc lengths is below 15, DSCNN gains obvious predictive values."}, {"heading": "5.4.2 Document Modeling", "text": "For document modeling, the result of DSCNN on IMDB compared to other baselines is listed in the last column of Table 1. Documents in IMDB consist of several sentences and are therefore very long: the average length is 241 characters per document and the maximum length is 2526 words (Dai and Le, 2015). As a result, no result is reported with CNN-based models due to forbidden computing time, and most previous work is disordered models, including variations of wordbag functions (Dahl et al., 2012). The main weakness of wordbag models (Maas et al., 2011), Deep Averaging Network (Iyyer et al., 2015) and word representation in combination with wordbag functions (Dahl et al., 2012) prevents these models from covering long-term dependencies. In addition, Paragraph Vector (Le and Mikolov, 2014) and SA-LSTM (Dai et al., 2015) fall that DSR super results are better than Le data processing methods."}, {"heading": "6 Conclusion", "text": "In this paper, we present DSCNN, Dependency Sensitive Convolutional Neural Networks for the purpose of text modeling at both the sentence and document levels. DSCNN captures long-term dependencies between sentences and sentences by processing word vectors through layers of LSTM networks, and extracts traits using revolutionary operators for classification. Experiments show that DSCNN consistently outperforms traditional CNNs and achieves state-of-the-art results based on multiple emotional analyses, question type classifications, and subjectivity classification datasets."}, {"heading": "Acknowledgments", "text": "We thank anonymous reviewers for their constructive comments. This work was supported by an EECS scholarship from the University of Michigan and the NSF CAREER scholarship IIS-1453651."}], "references": [{"title": "Neural machine translation by jointly learning to align and translate", "author": ["Kyunghyun Cho", "Yoshua Bengio"], "venue": "arXiv preprint arXiv:1409.0473", "citeRegEx": "Bahdanau et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Bahdanau et al\\.", "year": 2014}, {"title": "Learning long-term dependencies with gradient descent is difficult", "author": ["Bengio et al.1994] Yoshua Bengio", "Patrice Simard", "Paolo Frasconi"], "venue": "IEEE Transactions on Neural Networks,", "citeRegEx": "Bengio et al\\.,? \\Q1994\\E", "shortCiteRegEx": "Bengio et al\\.", "year": 1994}, {"title": "A neural probabilistic language model", "author": ["Bengio et al.2003] Yoshua Bengio", "R\u00e9jean Ducharme", "Pascal Vincent", "Christian Janvin"], "venue": "The Journal of Machine Learning Research,", "citeRegEx": "Bengio et al\\.,? \\Q2003\\E", "shortCiteRegEx": "Bengio et al\\.", "year": 2003}, {"title": "Guidelines for the clear style constituent to dependency conversion", "author": ["Choi", "Palmer2012] Jinho D Choi", "Martha Palmer"], "venue": "Technical report, Technical Report 01-12,", "citeRegEx": "Choi et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Choi et al\\.", "year": 2012}, {"title": "A unified architecture for natural language processing: Deep neural networks with multitask learning", "author": ["Collobert", "Weston2008] Ronan Collobert", "Jason Weston"], "venue": "In Proceedings of ICML,", "citeRegEx": "Collobert et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Collobert et al\\.", "year": 2008}, {"title": "Training restricted boltzmann machines on word observations", "author": ["Dahl et al.2012] George E Dahl", "Ryan P Adams", "Hugo Larochelle"], "venue": "arXiv preprint arXiv:1202.5695", "citeRegEx": "Dahl et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Dahl et al\\.", "year": 2012}, {"title": "Semi-supervised sequence learning. arXiv preprint arXiv:1511.01432", "author": ["Dai", "Le2015] Andrew M Dai", "Quoc V Le"], "venue": null, "citeRegEx": "Dai et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Dai et al\\.", "year": 2015}, {"title": "Improving neural networks by preventing co-adaptation of feature detectors. arXiv preprint arXiv:1207.0580", "author": ["Nitish Srivastava", "Alex Krizhevsky", "Ilya Sutskever", "Ruslan R Salakhutdinov"], "venue": null, "citeRegEx": "Hinton et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Hinton et al\\.", "year": 2012}, {"title": "Long short-term memory", "author": ["Hochreiter", "Schmidhuber1997] Sepp Hochreiter", "J\u00fcrgen Schmidhuber"], "venue": "Neural computation,", "citeRegEx": "Hochreiter et al\\.,? \\Q1997\\E", "shortCiteRegEx": "Hochreiter et al\\.", "year": 1997}, {"title": "The vanishing gradient problem during learning recurrent neural nets and problem solutions. International Journal of Uncertainty, Fuzziness and Knowledge-Based Systems, 6(02):107\u2013116", "author": ["Sepp Hochreiter"], "venue": null, "citeRegEx": "Hochreiter.,? \\Q1998\\E", "shortCiteRegEx": "Hochreiter.", "year": 1998}, {"title": "Deep recursive neural networks for compositionality in language", "author": ["Irsoy", "Cardie2014] Ozan Irsoy", "Claire Cardie"], "venue": "In Proceedings of NIPS,", "citeRegEx": "Irsoy et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Irsoy et al\\.", "year": 2014}, {"title": "Deep unordered composition rivals syntactic methods for text classification", "author": ["Iyyer et al.2015] Mohit Iyyer", "Varun Manjunatha", "Jordan Boyd-Graber", "Hal Daum\u00e9 III"], "venue": "In Proceedings of ACL-IJCNLP,", "citeRegEx": "Iyyer et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Iyyer et al\\.", "year": 2015}, {"title": "A convolutional neural network for modelling sentences", "author": ["Edward Grefenstette", "Phil Blunsom"], "venue": "arXiv preprint arXiv:1404.2188", "citeRegEx": "Kalchbrenner et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Kalchbrenner et al\\.", "year": 2014}, {"title": "Convolutional neural networks for sentence classification", "author": ["Yoon Kim"], "venue": "In Proceedings of EMNLP,", "citeRegEx": "Kim.,? \\Q2014\\E", "shortCiteRegEx": "Kim.", "year": 2014}, {"title": "A solution to plato\u2019s problem: The latent semantic analysis theory of acquisition, induction, and representation of knowledge", "author": ["Landauer", "Dumais1997] Thomas K Landauer", "Susan T Dumais"], "venue": "Psychological review,", "citeRegEx": "Landauer et al\\.,? \\Q1997\\E", "shortCiteRegEx": "Landauer et al\\.", "year": 1997}, {"title": "Distributed representations of sentences and documents. arXiv preprint arXiv:1405.4053", "author": ["Le", "Mikolov2014] Quoc V Le", "Tomas Mikolov"], "venue": null, "citeRegEx": "Le et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Le et al\\.", "year": 2014}, {"title": "Gradient-based learning applied to document recognition", "author": ["LeCun et al.1998] Yann LeCun", "L\u00e9on Bottou", "Yoshua Bengio", "Patrick Haffner"], "venue": "Proceedings of the IEEE,", "citeRegEx": "LeCun et al\\.,? \\Q1998\\E", "shortCiteRegEx": "LeCun et al\\.", "year": 1998}, {"title": "Learning question classifiers", "author": ["Li", "Roth2002] X. Li", "D. Roth"], "venue": "In COLING,", "citeRegEx": "Li et al\\.,? \\Q2002\\E", "shortCiteRegEx": "Li et al\\.", "year": 2002}, {"title": "A hierarchical neural autoencoder for paragraphs and documents. arXiv preprint arXiv:1506.01057", "author": ["Li et al.2015] Jiwei Li", "Minh-Thang Luong", "Dan Jurafsky"], "venue": null, "citeRegEx": "Li et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Li et al\\.", "year": 2015}, {"title": "Dependency-based convolutional neural networks for sentence embedding", "author": ["Ma et al.2015] Mingbo Ma", "Liang Huang", "Bing Xiang", "Bowen Zhou"], "venue": "In Proceedings of ACL-IJCNLP,", "citeRegEx": "Ma et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Ma et al\\.", "year": 2015}, {"title": "Learning word vectors for sentiment analysis", "author": ["Christopher Potts."], "venue": "Proceedings of ACL-HLT, pages 142\u2013150, Portland, Oregon, USA, June.", "citeRegEx": "Potts.,? 2011", "shortCiteRegEx": "Potts.", "year": 2011}, {"title": "Distributed representations of words and phrases and their compositionality", "author": ["Ilya Sutskever", "Kai Chen", "Greg S Corrado", "Jeff Dean"], "venue": "In Proceedings of NIPS,", "citeRegEx": "Mikolov et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Mikolov et al\\.", "year": 2013}, {"title": "A sentimental education: Sentiment analysis using subjectivity summarization based on minimum cuts", "author": ["Pang", "Lee2004] Bo Pang", "Lillian Lee"], "venue": "In Proceedings of ACL,", "citeRegEx": "Pang et al\\.,? \\Q2004\\E", "shortCiteRegEx": "Pang et al\\.", "year": 2004}, {"title": "Seeing stars: Exploiting class relationships for sentiment categorization with respect to rating scales", "author": ["Pang", "Lee2005] Bo Pang", "Lillian Lee"], "venue": "In Proceedings of ACL,", "citeRegEx": "Pang et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Pang et al\\.", "year": 2005}, {"title": "Glove: Global vectors for word representation", "author": ["Richard Socher", "Christopher D Manning"], "venue": "Proceedings of EMNLP,", "citeRegEx": "Pennington et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Pennington et al\\.", "year": 2014}, {"title": "From symbolic to sub-symbolic information in question classification", "author": ["Silva et al.2011] Joao Silva", "Lu\u0131\u0301sa Coheur", "Ana Cristina Mendes", "Andreas Wichert"], "venue": "Artificial Intelligence Review,", "citeRegEx": "Silva et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Silva et al\\.", "year": 2011}, {"title": "Semantic compositionality through recursive matrixvector spaces", "author": ["Brody Huval", "Christopher D Manning", "Andrew Y Ng"], "venue": "In Proceedings of EMNLP-CoNLL,", "citeRegEx": "Socher et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Socher et al\\.", "year": 2012}, {"title": "Recursive deep models for semantic compositionality over a sentiment treebank", "author": ["Alex Perelygin", "Jean Y Wu", "Jason Chuang", "Christopher D Manning", "Andrew Y Ng", "Christopher Potts"], "venue": "Proceedings of EMNLP,", "citeRegEx": "Socher et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Socher et al\\.", "year": 2013}, {"title": "Improved semantic representations from tree-structured long short-term memory networks. arXiv preprint arXiv:1503.00075", "author": ["Tai et al.2015] Kai Sheng Tai", "Richard Socher", "Christopher D Manning"], "venue": null, "citeRegEx": "Tai et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Tai et al\\.", "year": 2015}, {"title": "Show and tell: A neural image caption generator", "author": ["Alexander Toshev", "Samy Bengio", "Dumitru Erhan"], "venue": "arXiv preprint arXiv:1411.4555", "citeRegEx": "Vinyals et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Vinyals et al\\.", "year": 2014}, {"title": "Phoneme recognition using time-delay", "author": ["Toshiyuki Hanazawa", "Geoffrey Hinton", "Kiyohiro Shikano", "Kevin J Lang"], "venue": null, "citeRegEx": "Waibel et al\\.,? \\Q1989\\E", "shortCiteRegEx": "Waibel et al\\.", "year": 1989}, {"title": "Semantically conditioned lstm-based natural language generation for spoken dialogue systems. arXiv preprint arXiv:1508.01745", "author": ["Wen et al.2015] Tsung-Hsien Wen", "Milica Gasic", "Nikola Mrksic", "Pei-Hao Su", "David Vandyke", "Steve Young"], "venue": null, "citeRegEx": "Wen et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Wen et al\\.", "year": 2015}, {"title": "Multichannel variable-size convolution for sentence classification", "author": ["Yin", "Sch\u00fctze2015] Wenpeng Yin", "Hinrich Sch\u00fctze"], "venue": "In Proceedings of CoNLL,", "citeRegEx": "Yin et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Yin et al\\.", "year": 2015}, {"title": "Adadelta: An adaptive learning rate method", "author": ["Matthew D Zeiler"], "venue": "arXiv preprint arXiv:1212.5701", "citeRegEx": "Zeiler.,? \\Q2012\\E", "shortCiteRegEx": "Zeiler.", "year": 2012}], "referenceMentions": [{"referenceID": 27, "context": "For example, (Socher et al., 2013) uses Recursive Neural Networks to build", "startOffset": 13, "endOffset": 34}, {"referenceID": 12, "context": "CNN based models, as the second category, utilize convolutional filters to extract local features (Kalchbrenner et al., 2014; Kim, 2014) over embedding matrices consisting of pretrained word vectors.", "startOffset": 98, "endOffset": 136}, {"referenceID": 13, "context": "CNN based models, as the second category, utilize convolutional filters to extract local features (Kalchbrenner et al., 2014; Kim, 2014) over embedding matrices consisting of pretrained word vectors.", "startOffset": 98, "endOffset": 136}, {"referenceID": 11, "context": "First, recursive models depend on well-performing parsers, which can be difficult for many languages or noisy domains (Iyyer et al., 2015; Ma et al., 2015).", "startOffset": 118, "endOffset": 155}, {"referenceID": 19, "context": "First, recursive models depend on well-performing parsers, which can be difficult for many languages or noisy domains (Iyyer et al., 2015; Ma et al., 2015).", "startOffset": 118, "endOffset": 155}, {"referenceID": 11, "context": "Besides, since tree-structured neural networks are vulnerable to the vanishing gradient problem (Iyyer et al., 2015), recursive models require heavy labelar X iv :1 61 1.", "startOffset": 96, "endOffset": 116}, {"referenceID": 2, "context": "The success of deep learning architectures for NLP is first based on the progress in learning distributed word representations in semantic vector space (Bengio et al., 2003; Mikolov et al., 2013; Pennington et al., 2014), where each word is modeled with a realvalued vector called a word embedding.", "startOffset": 152, "endOffset": 220}, {"referenceID": 21, "context": "The success of deep learning architectures for NLP is first based on the progress in learning distributed word representations in semantic vector space (Bengio et al., 2003; Mikolov et al., 2013; Pennington et al., 2014), where each word is modeled with a realvalued vector called a word embedding.", "startOffset": 152, "endOffset": 220}, {"referenceID": 24, "context": "The success of deep learning architectures for NLP is first based on the progress in learning distributed word representations in semantic vector space (Bengio et al., 2003; Mikolov et al., 2013; Pennington et al., 2014), where each word is modeled with a realvalued vector called a word embedding.", "startOffset": 152, "endOffset": 220}, {"referenceID": 12, "context": "Besides, a neural-bagof-words model described in (Kalchbrenner et al., 2014) adds an additional hidden layer on top of the", "startOffset": 49, "endOffset": 76}, {"referenceID": 0, "context": "For example, thanks to its ability to capture longdistance dependencies, LSTM has re-emerged as a popular choice for many sequence-modeling tasks, including machine translation (Bahdanau et al., 2014), image caption generation (Vinyals et al.", "startOffset": 177, "endOffset": 200}, {"referenceID": 29, "context": ", 2014), image caption generation (Vinyals et al., 2014), and natural language generation (Wen et al.", "startOffset": 34, "endOffset": 56}, {"referenceID": 31, "context": ", 2014), and natural language generation (Wen et al., 2015).", "startOffset": 41, "endOffset": 59}, {"referenceID": 27, "context": "For example, (Socher et al., 2013) applied Recursive Neural Networks as a variant of the", "startOffset": 13, "endOffset": 34}, {"referenceID": 28, "context": "(Tai et al., 2015) also generalizes LSTM to Tree-LSTM where each LSTM unit combines information from its children units.", "startOffset": 0, "endOffset": 18}, {"referenceID": 12, "context": "For example, DCNN in (Kalchbrenner et al., 2014) constructs hierarchical features of sentences by onedimensional convolution and dynamic k-max pooling.", "startOffset": 21, "endOffset": 48}, {"referenceID": 1, "context": "This problem makes it difficult to train RNN to capture longdistance dependencies in a sequence (Bengio et al., 1994; Hochreiter, 1998).", "startOffset": 96, "endOffset": 135}, {"referenceID": 9, "context": "This problem makes it difficult to train RNN to capture longdistance dependencies in a sequence (Bengio et al., 1994; Hochreiter, 1998).", "startOffset": 96, "endOffset": 135}, {"referenceID": 16, "context": "Convolution operators have been extensively used in object recognition (LeCun et al., 1998), phoneme recognition (Waibel et al.", "startOffset": 71, "endOffset": 91}, {"referenceID": 30, "context": ", 1998), phoneme recognition (Waibel et al., 1989), sentence modeling and classification (Kalchbrenner et al.", "startOffset": 29, "endOffset": 50}, {"referenceID": 12, "context": ", 1989), sentence modeling and classification (Kalchbrenner et al., 2014; Kim, 2014), and other traditional NLP tasks (Col-", "startOffset": 46, "endOffset": 84}, {"referenceID": 13, "context": ", 1989), sentence modeling and classification (Kalchbrenner et al., 2014; Kim, 2014), and other traditional NLP tasks (Col-", "startOffset": 46, "endOffset": 84}, {"referenceID": 12, "context": "In this work, we employ one-dimensional wide convolution described in (Kalchbrenner et al., 2014).", "startOffset": 70, "endOffset": 97}, {"referenceID": 11, "context": "First, they require an external parser and are vulnerable to parsing errors (Iyyer et al., 2015).", "startOffset": 76, "endOffset": 96}, {"referenceID": 27, "context": "For example, in (Socher et al., 2013), input sentences are labeled for each sub-", "startOffset": 16, "endOffset": 37}, {"referenceID": 21, "context": "Different versions come from pre-trained word vectors such as word2vec (Mikolov et al., 2013) and GloVe (Pennington et al.", "startOffset": 71, "endOffset": 93}, {"referenceID": 24, "context": ", 2013) and GloVe (Pennington et al., 2014).", "startOffset": 18, "endOffset": 43}, {"referenceID": 18, "context": "The intuition comes from the fact that as the composition of words builds up the semantic meaning for sentences, the composition of sentences establishes the semantic meaning for documents (Li et al., 2015).", "startOffset": 189, "endOffset": 206}, {"referenceID": 27, "context": "Method MR SST-2 SST-5 TREC SUBJ IMDB SVM (Socher et al., 2013) \u2014 79.", "startOffset": 41, "endOffset": 62}, {"referenceID": 27, "context": "7 \u2014 \u2014 \u2014 NB (Socher et al., 2013) \u2014 81.", "startOffset": 11, "endOffset": 32}, {"referenceID": 25, "context": "2 SVMS (Silva et al., 2011) \u2014 \u2014 \u2014 95.", "startOffset": 7, "endOffset": 27}, {"referenceID": 27, "context": "0 \u2014 \u2014 Standard-RNN (Socher et al., 2013) \u2014 82.", "startOffset": 19, "endOffset": 40}, {"referenceID": 26, "context": "2 \u2014 \u2014 \u2014 MV-RNN (Socher et al., 2012) 79.", "startOffset": 15, "endOffset": 36}, {"referenceID": 27, "context": "4 \u2014 \u2014 \u2014 RNTN (Socher et al., 2013) \u2014 85.", "startOffset": 13, "endOffset": 34}, {"referenceID": 28, "context": "8 \u2014 \u2014 \u2014 Standard-LSTM (Tai et al., 2015) \u2014 86.", "startOffset": 22, "endOffset": 40}, {"referenceID": 28, "context": "8 \u2014 \u2014 \u2014 bi-LSTM (Tai et al., 2015) \u2014 86.", "startOffset": 16, "endOffset": 34}, {"referenceID": 28, "context": "1 \u2014 \u2014 \u2014 Tree-LSTM (Tai et al., 2015) \u2014 88.", "startOffset": 18, "endOffset": 36}, {"referenceID": 12, "context": "8 DCNN (Kalchbrenner et al., 2014) \u2014 86.", "startOffset": 7, "endOffset": 34}, {"referenceID": 13, "context": "0 \u2014 \u2014 CNN-MC (Kim, 2014) 81.", "startOffset": 13, "endOffset": 24}, {"referenceID": 19, "context": "9 \u2014 Dep-CNN (Ma et al., 2015) 81.", "startOffset": 12, "endOffset": 29}, {"referenceID": 12, "context": "4 \u2014 \u2014 Neural-BoW (Kalchbrenner et al., 2014) \u2014 80.", "startOffset": 17, "endOffset": 44}, {"referenceID": 11, "context": "2 \u2014 \u2014 DAN (Iyyer et al., 2015) 80.", "startOffset": 10, "endOffset": 30}, {"referenceID": 5, "context": "6 WRRBM+BoW(bnc) (Dahl et al., 2012) \u2014 \u2014 \u2014 \u2014 \u2014 89.", "startOffset": 17, "endOffset": 36}, {"referenceID": 27, "context": "SVM: Support Vector Machines with unigram features (Socher et al., 2013) NB: Naive Bayes with unigram features(Socher et al.", "startOffset": 51, "endOffset": 72}, {"referenceID": 27, "context": ", 2013) NB: Naive Bayes with unigram features(Socher et al., 2013) NBSVM-bi: Naive Bayes SVM and Multinomial Naive Bayes with bigrams (Wang and Man-", "startOffset": 45, "endOffset": 66}, {"referenceID": 25, "context": "ning, 2012) SVMS : SVM with features including uni-bi-trigrams, POS, parser, and 60 hand-coded rules (Silva et al., 2011) Standard-RNN: Standard Recursive Neural Network (Socher et al.", "startOffset": 101, "endOffset": 121}, {"referenceID": 27, "context": ", 2011) Standard-RNN: Standard Recursive Neural Network (Socher et al., 2013) MV-RNN: Matrix-Vector Recursive Neural Network (Socher et al.", "startOffset": 56, "endOffset": 77}, {"referenceID": 26, "context": ", 2013) MV-RNN: Matrix-Vector Recursive Neural Network (Socher et al., 2012) RNTN:Recursive Neural Tensor Network (Socher et al.", "startOffset": 55, "endOffset": 76}, {"referenceID": 27, "context": ", 2012) RNTN:Recursive Neural Tensor Network (Socher et al., 2013) DRNN: Deep Recursive Neural Network (Irsoy and Cardie, 2014) Standard-LSTM: Standard Long Short-Term Memory Network (Tai et al.", "startOffset": 45, "endOffset": 66}, {"referenceID": 28, "context": ", 2013) DRNN: Deep Recursive Neural Network (Irsoy and Cardie, 2014) Standard-LSTM: Standard Long Short-Term Memory Network (Tai et al., 2015) bi-LSTM: Bidirectional LSTM (Tai et al.", "startOffset": 124, "endOffset": 142}, {"referenceID": 28, "context": ", 2015) bi-LSTM: Bidirectional LSTM (Tai et al., 2015) Tree-LSTM: Tree-Structured LSTM (Tai et al.", "startOffset": 36, "endOffset": 54}, {"referenceID": 28, "context": ", 2015) Tree-LSTM: Tree-Structured LSTM (Tai et al., 2015) SA-LSTM: Sequence Autoencoder LSTM (Dai and Le, 2015).", "startOffset": 40, "endOffset": 58}, {"referenceID": 12, "context": "DCNN: Dynamic Convolutional Neural Network with k-max pooling (Kalchbrenner et al., 2014) CNN-MC: Convolutional Neural Network with static pretrained and fine-tuned pretrained word-embeddings (Kim, 2014) MVCNN: Multichannel Variable-Size Convolution Neural Network (Yin and Sch\u00fctze, 2015) Dep-CNN: Dependency-based Convolutional Neural Network (Ma et al.", "startOffset": 62, "endOffset": 89}, {"referenceID": 13, "context": ", 2014) CNN-MC: Convolutional Neural Network with static pretrained and fine-tuned pretrained word-embeddings (Kim, 2014) MVCNN: Multichannel Variable-Size Convolution Neural Network (Yin and Sch\u00fctze, 2015) Dep-CNN: Dependency-based Convolutional Neural Network (Ma et al.", "startOffset": 110, "endOffset": 121}, {"referenceID": 12, "context": "Neural-BoW : Neural Bag-of-Words Models (Kalchbrenner et al., 2014) DAN: Deep Averaging Network (Iyyer et al.", "startOffset": 40, "endOffset": 67}, {"referenceID": 11, "context": ", 2014) DAN: Deep Averaging Network (Iyyer et al., 2015) Paragraph-Vector: Logistic Regression on Paragraph-Vector (Le and Mikolov, 2014) WRRBM+BoW(bnc): word representation Restricted Boltzmann Machine combined with bag-of-words features (Dahl et al.", "startOffset": 36, "endOffset": 56}, {"referenceID": 5, "context": ", 2015) Paragraph-Vector: Logistic Regression on Paragraph-Vector (Le and Mikolov, 2014) WRRBM+BoW(bnc): word representation Restricted Boltzmann Machine combined with bag-of-words features (Dahl et al., 2012) Full+Unlabeled+BoW(bnc):word vector based model capturing both semantic and sentiment, trained on unlabeled examples, and with bag-of-words features concatenated (Maas et al.", "startOffset": 190, "endOffset": 209}, {"referenceID": 27, "context": "by (Socher et al., 2013).", "startOffset": 3, "endOffset": 24}, {"referenceID": 7, "context": "For regularization, before the softmax layers, we employ Dropout operation (Hinton et al., 2012) with dropout rate 0.", "startOffset": 75, "endOffset": 96}, {"referenceID": 33, "context": "We use the gradientbased optimizer Adadelta (Zeiler, 2012) to minimize cross-entropy loss between the predicted and true distributions, and the training is early stopped when the accuracy on validation set starts to drop.", "startOffset": 44, "endOffset": 58}, {"referenceID": 19, "context": "The finding here agrees with the discussion in DepCNN work (Ma et al., 2015).", "startOffset": 59, "endOffset": 76}, {"referenceID": 11, "context": ", 2011), Deep Averaging Network (Iyyer et al., 2015), and word representation Restricted Boltzmann Machine model combined with bag-of-words features (Dahl et al.", "startOffset": 32, "endOffset": 52}, {"referenceID": 5, "context": ", 2015), and word representation Restricted Boltzmann Machine model combined with bag-of-words features (Dahl et al., 2012).", "startOffset": 104, "endOffset": 123}], "year": 2016, "abstractText": "The goal of sentence and document modeling is to accurately represent the meaning of sentences and documents for various Natural Language Processing tasks. In this work, we present Dependency Sensitive Convolutional Neural Networks (DSCNN) as a generalpurpose classification system for both sentences and documents. DSCNN hierarchically builds textual representations by processing pretrained word embeddings via Long ShortTerm Memory networks and subsequently extracting features with convolution operators. Compared with existing recursive neural models with tree structures, DSCNN does not rely on parsers and expensive phrase labeling, and thus is not restricted to sentencelevel tasks. Moreover, unlike other CNNbased models that analyze sentences locally by sliding windows, our system captures both the dependency information within each sentence and relationships across sentences in the same document. Experiment results demonstrate that our approach is achieving state-ofthe-art performance on several tasks, including sentiment analysis, question type classification, and subjectivity classification.", "creator": "LaTeX with hyperref package"}}}