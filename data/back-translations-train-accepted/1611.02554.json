{"id": "1611.02554", "review": {"conference": "iclr", "VERSION": "v1", "DATE_OF_SUBMISSION": "8-Nov-2016", "title": "The Neural Noisy Channel", "abstract": "We formulate sequence to sequence transduction as a noisy channel decoding problem and use recurrent neural networks to parameterise the source and channel models. Unlike direct models which can suffer from explaining-away effects during training, noisy channel models must produce outputs that explain their inputs, and their component models can be trained with not only paired training samples but also unpaired samples from the marginal output distribution. Using a latent variable to control how much of the conditioning sequence the channel model needs to read in order to generate a subsequent symbol, we obtain a tractable and effective beam search decoder. Experimental results on abstractive sentence summarisation, morphological inflection, and machine translation show that noisy channel models outperform direct models, and that they significantly benefit from increased amounts of unpaired output data that direct models cannot easily use.", "histories": [["v1", "Tue, 8 Nov 2016 15:18:44 GMT  (27kb)", "http://arxiv.org/abs/1611.02554v1", "ICLR 2017 submission"], ["v2", "Mon, 6 Mar 2017 12:37:12 GMT  (27kb)", "http://arxiv.org/abs/1611.02554v2", "ICLR 2017"]], "COMMENTS": "ICLR 2017 submission", "reviews": [], "SUBJECTS": "cs.CL cs.AI cs.NE", "authors": ["lei yu", "phil blunsom", "chris dyer", "edward grefenstette", "tomas kocisky"], "accepted": true, "id": "1611.02554"}, "pdf": {"name": "1611.02554.pdf", "metadata": {"source": "CRF", "title": "THE NEURAL NOISY CHANNEL", "authors": ["Lei Yu", "Phil Blunsom", "Chris Dyer", "Edward Grefenstette"], "emails": ["lei.yu@cs.ox.ac.uk,", "pblunsom@google.com", "cdyer@google.com", "etg@google.com", "tkocisky@google.com"], "sections": [{"heading": null, "text": "ar Xiv: 161 1.02 554v 1 [cs.C L] 8N ov2 016 Revised as conference contribution at ICLR 2017"}, {"heading": "1 INTRODUCTION", "text": "In fact, it is as if most of us are able to outdo ourselves and that they do not. (...) It is not as if they are able to outdo themselves. (...) It is not as if they are doing it. (...) It is not as if they are doing it. (...) It is as if they are doing it. (...) It is not as if they are doing it. (...) It is as if they are doing it. (...) It is as if they are doing it. (...) It is not as if they are doing it. (...) It is as if they are doing it. (...) It is not as if they are doing it. (...) It is not as if they are doing it. (...). (...) It is as if they are doing it. (...) It is not as if they are doing it. (...) It is not as if they are doing it. (...) It is not as if they are doing it. (...) It is not as if they are doing it. (...). (...) It is not as if they are doing it. (...) It is not as if they are doing it. (...). (...) It is not as if they are doing it. \"...\"... \"...\"... \"...\"....... \".......\"....... \".......\"... \"...\"... \"...\"... \"...\"... \"...\"... \"...\". \"(...\"....... \"...\". \".\". \".\" (... \").\". \".\" (... \").\"... \".\". \".\". \"...\". \".\". \"...\". \".\". \".\" (... \").\"). \"It is.\" it. \""}, {"heading": "2 BACKGROUND: SEGMENT TO SEGMENT NEURAL TRANSDUCTION", "text": "Our model is based on the Segment to Segment Neural Transduction Model (SSNT) by Yu et al., 2016. At a high level, the model alternates between encoding a larger part of the input sequence and decoding output marks from the encoded representation. This representation deviates from the representation of Yu et al. to emphasize the incremental construction of the conditioning context made possible by the latent variable."}, {"heading": "2.1 MODEL DESCRIPTION", "text": "The question we have to ask ourselves is whether we are able to read the entire sequence. Since we assume that the input is read only once from left to right, we limit ourselves to being a monotonically increasing alignment variant. (i.e.) The SSNT model is a monotonically increasing alignment variant. (i.e.) The SSNT model is true with probability 1), where zj = i denotes that the output token is generated at position j (yj) when the input is generated by position i. (i.e.) The SSNT model is: p (y | x) = zp (y) p (y, z | x) p (y, z | x) p."}, {"heading": "2.2 INFERENCE ALGORITHMS", "text": "In SSNT, the probability of creating each yj depends only on the alignment of the current starting position (zj), the current starting prefix (y j \u2212 1), the input prefix up to the current alignment (x zj 1). It does not depend on the history of the alignment decisions. Likewise, the alignment decisions at each position are conditionally independent of the history of the alignment decisions. Based on these assumptions of independence, z can be marginalized by means of an O (| x | 2 \u00b7 |) time-dynamic programming algorithm, in which each is inserted into a diagram with the calculation of the following marginal probabilities: \u03b1 (i, j) = p (zj = i, y zj 1) = i \u00b2 = 1\u03b1 (i \u2032, j \u2212 1) p (zj \u2212 1)."}, {"heading": "3 DECODING", "text": "We now turn to the problem of deciphering, that is, the calculation of the given input hypothesis Q = argmax y p (x | y) p (y), using the SSNT model described in the previous section as a channel model and a language model that returns earlier probabilities of the output sequence in the order from left to right, i.e., p (yi | y i \u2212 1). Marginalizing the latent variable during the search is mathematically difficult (Sima'an, 1996), and so we approach the search problem asy = argmax y max y z p (x, z | y) p (y).But even with this simplification, the search problem does not remain trivial. On the one hand, we must search over the space of all possible results with a model that does not allow marcovial assumptions. This is similar to the decoding problem of the standard model seq2seq2seq Transducers."}, {"heading": "3.1 MODEL COMBINATION", "text": "The decoder we have just described uses an auxiliary decryption model. This means that, as a generalization, it is able to decrypt under a lens that is a linear combination of direct model, channel model, language model and a bias for the initial length O xi1, yj 1 = \u03bb1 log p (y j 1 | x i 1) + \u03bb2 log p (x i 1 | y j 1) + \u03bb3 log p (y j 1) + \u03bb4 | y j 1 |. (3) The bias is used to punish the noisy channel model for generating too short (or long) sequences."}, {"heading": "4 EXPERIMENTS", "text": "We evaluate our model based on three tasks for processing natural language, abstract sentence summary, machine translation, and morphological diffraction generation. For each task, we compare the performance of the direct model, the noise channel model, and the interpolation of the two models."}, {"heading": "4.1 ABSTRACTIVE SENTENCE SUMMARISATION", "text": "This year is the highest in the history of the country."}, {"heading": "4.2 MACHINE TRANSLATION", "text": "Next, we evaluate our models using a machine translation task between Chinese and English. We used parallel data with 184k pairs of sentences (from the FBIS corpus, LDC2003E14) and monolingual data with 4.3 million English sentences (selected from the English gigaword). Training data is pre-processed by reducing English sentences, replacing digits with \"#\" characters and replacing tokens that appear less than 5 times with a UNK token, resulting in vocabulary sizes of 30k and 20k for Chinese sentences and English sentences. Models are trained using Adam (Kingma & Ba, 2015) with an initial learning rate of 0.001 for the direct model and channel model, and 0.0001 for the language model. Direct and channel model LSTMs have 512 hidden units and 1 shift, as well as 2 layers of 1024 hidden units per shift for the language model."}, {"heading": "4.3 MORPHOLOGICAL INFLECTION GENERATION", "text": "Morphological diffraction is the task of generating a target (flexed form) word from a source word (base form), as a morphological attribute is given, e.g. number, voltage and person, etc. It is useful to reduce problems of data economy when translating morphologically rich languages. Transformation from the base form to the flexed form is usually to add prefix or suffix, or to do character substitutions.The dataset (Durrett & DeNero, 2013) we use in the experiments is created by Wiktionary, including the infections for German nouns, German verbs, Spanish verbs, Finnish noun and adjectives, and Finnish verbs. We experimented only with German nouns and German verbs is the most difficult task2, and the direct model does not perform as well as other state-of-theart systems on German verbs. The train / dev / 200 split for German nouns is 1664 / It is for German verbs, and 17 / It is for German verbs."}, {"heading": "5 ANALYSIS", "text": "In contrast, the noisy channel model seems to work around this problem. To illustrate in Example 1 (see Appendix B) in Table 5, the direct model ignores the key term \"coping,\" which leads to an incomplete meaning, but the noisy channel model covers it. In Example 4, the direct model does not translate the Chinese word corresponding to \"investigation.\" We also observe that while modern systems can achieve 99% accuracy for Spanish verbs and Finnish verbs, they can only achieve 89% accuracy for German noises. 3http: / / www.statmt.org / wmt16 / translation-task.htmlwhile the direct model copies words from the source set, the noisy channel model prefers to generate paraphrases. For example, an Example 2 copies the direct model while speeding up the output speed. \""}, {"heading": "6 RELATED WORK", "text": "However, the idea of adding language models and monolingual data to machine translation has been explored in previous work. Gu \u00bc lc, Ehre and al. (2015) suggest two strategies to combine a language model with a neural sequence to the sequence model. In flat fusion, candidate results are proposed during the decryption of the sequence to the sequence model (direct model), and these candidates are reordered based on the translation model and the language model calculated by a weighted sum of probabilities of the translation model. In deep fusion, the language model is integrated into the decoder of the sequence model."}, {"heading": "7 CONCLUSION", "text": "We presented and empirically validated a noisy channel transduction model based on component models based on recurrent neural networks, which enables us to estimate the parameters of the source model and the input-output pairs using unpaired outputs to train the channel model. Despite the channel model's ability to condition over long sequences, we are able to maintain a tractable decoding by using a latent segmentation variable that divides the conditioning context into a series of monotonously growing segments. Our experiments show that this model makes excellent use of unpaired training data."}, {"heading": "A ALGORITHM", "text": "It is not only a matter of time, but also a matter of time until agreement is reached. (zA) Wptemp: \"It is a matter of time in which agreement is reached.\" (A) Wptemp: \"It is a matter of time.\" (A) Wptemp: \"It is a matter of time.\" (A) Wptemp: \"It is a matter of time.\" (A) Wptemp: \"It is a matter of time.\" (A) Wptemp: \"It is a matter of time.\" (A) Wptemp: \"It is a matter of time.\" (A) Wptemp: \"It is a matter of time.\" (A) Wptemp: \"It is a matter of time.\" (A) Wptemp: \"It is a question of time.\" (A) It is a question of time. (A) It is a question of time. (A) It is a question of time. (A) It is a question of time. (A) It is a question of time. (A) It is a question of time. (A) It is a question of time."}], "references": [{"title": "Neural machine translation by jointly learning to align and translate", "author": ["Dzmitry Bahdanau", "Kyunghyun Cho", "Yoshua Bengio"], "venue": "In Proc. ICLR,", "citeRegEx": "Bahdanau et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Bahdanau et al\\.", "year": 2015}, {"title": "An improved error model for noisy channel spelling correction", "author": ["Eric Brill", "Robert C. Moore"], "venue": "In Proc. ACL,", "citeRegEx": "Brill and Moore.,? \\Q2000\\E", "shortCiteRegEx": "Brill and Moore.", "year": 2000}, {"title": "The mathematics of statistical machine translation: Parameter estimation", "author": ["Peter F. Brown", "Stephen A. Della Pietra", "Vincent J. Della Pietra", "Robert. L. Mercer"], "venue": "Computational Linguistics,", "citeRegEx": "Brown et al\\.,? \\Q1993\\E", "shortCiteRegEx": "Brown et al\\.", "year": 1993}, {"title": "Semisupervised learning for neural machine translation", "author": ["Yong Cheng", "Wei Xu", "Zhongjun He", "Wei He", "Hua Wu", "Maosong Sun", "Yang Liu"], "venue": "In Proc. ACL,", "citeRegEx": "Cheng et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Cheng et al\\.", "year": 2016}, {"title": "Abstractive sentence summarization with attentive recurrent neural networks", "author": ["Sumit Chopra", "Michael Auli", "Alexander M. Rush"], "venue": "In Proc. NAACL,", "citeRegEx": "Chopra et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Chopra et al\\.", "year": 2016}, {"title": "Supervised learning of complete morphological paradigms", "author": ["Greg Durrett", "John DeNero"], "venue": "In HLT-NAACL,", "citeRegEx": "Durrett and DeNero.,? \\Q2013\\E", "shortCiteRegEx": "Durrett and DeNero.", "year": 2013}, {"title": "A noisy-channel approach to question answering", "author": ["Abdessamad Echihabi", "Daniel Marcu"], "venue": "In Proc. ACL,", "citeRegEx": "Echihabi and Marcu.,? \\Q2003\\E", "shortCiteRegEx": "Echihabi and Marcu.", "year": 2003}, {"title": "Finding structure in time", "author": ["Jeffrey L. Elman"], "venue": "Cognitive science,", "citeRegEx": "Elman.,? \\Q1990\\E", "shortCiteRegEx": "Elman.", "year": 1990}, {"title": "Morphological inflection generation using character sequence to sequence learning", "author": ["Manaal Faruqui", "Yulia Tsvetkov", "Graham Neubig", "Chris Dyer"], "venue": "In HLT-NAACL,", "citeRegEx": "Faruqui et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Faruqui et al\\.", "year": 2016}, {"title": "Don\u2019t until the final verb wait: Reinforcement learning for simultaneous machine translation", "author": ["Alvin Grissom II", "Jordan Boyd-Graber", "He He", "John Morgan", "Hal Daum\u00e9 III"], "venue": "In Empirical Methods in Natural Language Processing,", "citeRegEx": "II et al\\.,? \\Q2014\\E", "shortCiteRegEx": "II et al\\.", "year": 2014}, {"title": "Learning to translate in real-time with neural machine", "author": ["Jiatao Gu", "Graham Neubig", "Kyunghyun Cho", "Victor O.K. Li"], "venue": "translation. CoRR,", "citeRegEx": "Gu et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Gu et al\\.", "year": 2016}, {"title": "On using monolingual corpora in neural machine", "author": ["\u00c7aglar G\u00fcl\u00e7ehre", "Orhan Firat", "Kelvin Xu", "Kyunghyun Cho", "Lo\u0131\u0308c Barrault", "Huei-Chi Lin", "Fethi Bougares", "Holger Schwenk", "Yoshua Bengio"], "venue": "translation. CoRR,", "citeRegEx": "G\u00fcl\u00e7ehre et al\\.,? \\Q2015\\E", "shortCiteRegEx": "G\u00fcl\u00e7ehre et al\\.", "year": 2015}, {"title": "Long short-term memory", "author": ["Sepp Hochreiter", "J\u00fcrgen Schmidhuber"], "venue": "Neural computation,", "citeRegEx": "Hochreiter and Schmidhuber.,? \\Q1997\\E", "shortCiteRegEx": "Hochreiter and Schmidhuber.", "year": 1997}, {"title": "Character-level incremental speech recognition with recurrent neural networks", "author": ["Kyuyeon Hwang", "Wonyong Sung"], "venue": "In Proc. ICASSP,", "citeRegEx": "Hwang and Sung.,? \\Q2016\\E", "shortCiteRegEx": "Hwang and Sung.", "year": 2016}, {"title": "A neural transducer", "author": ["Navdeep Jaitly", "David Sussillo", "Quoc V Le", "Oriol Vinyals", "Ilya Sutskever", "Samy Bengio"], "venue": null, "citeRegEx": "Jaitly et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Jaitly et al\\.", "year": 2016}, {"title": "Statistical Methods for Speech Recognition", "author": ["Frederick Jelinek"], "venue": null, "citeRegEx": "Jelinek.,? \\Q1998\\E", "shortCiteRegEx": "Jelinek.", "year": 1998}, {"title": "Recurrent continuous translation models", "author": ["Nal Kalchbrenner", "Phil Blunsom"], "venue": "In Proc. EMNLP,", "citeRegEx": "Kalchbrenner and Blunsom.,? \\Q2013\\E", "shortCiteRegEx": "Kalchbrenner and Blunsom.", "year": 2013}, {"title": "Adam: A method for stochastic optimization", "author": ["Diederik P. Kingma", "Jimmy Ba"], "venue": "In Proc. ICIR,", "citeRegEx": "Kingma and Ba.,? \\Q2015\\E", "shortCiteRegEx": "Kingma and Ba.", "year": 2015}, {"title": "Conditional structure versus conditional estimation in nlp models", "author": ["Dan Klein", "Christopher D. Manning"], "venue": "In Proc. EMNLP,", "citeRegEx": "Klein and Manning.,? \\Q2001\\E", "shortCiteRegEx": "Klein and Manning.", "year": 2001}, {"title": "Agreement on target-bidirectional neural machine translation", "author": ["Lemao Liu", "Masao Utiyama", "Andrew Finch", "Eiichiro Sumita"], "venue": "In Proc. NAACL,", "citeRegEx": "Liu et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Liu et al\\.", "year": 2016}, {"title": "Language as a latent variable: Discrete generative models for sentence compression", "author": ["Yishu Miao", "Phil Blunsom"], "venue": "In Proc. EMNLP,", "citeRegEx": "Miao and Blunsom.,? \\Q2016\\E", "shortCiteRegEx": "Miao and Blunsom.", "year": 2016}, {"title": "Annotated gigaword", "author": ["Courtney Napoles", "Matthew Gormley", "Benjamin Van Durme"], "venue": "In Proceedings of the Joint Workshop on Automatic Knowledge Base Construction and Web-scale Knowledge Extraction,", "citeRegEx": "Napoles et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Napoles et al\\.", "year": 2012}, {"title": "Inflection generation as discriminative string transduction", "author": ["Garrett Nicolai", "Colin Cherry", "Grzegorz Kondrak"], "venue": "In HLT-NAACL,", "citeRegEx": "Nicolai et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Nicolai et al\\.", "year": 2015}, {"title": "A neural attention model for abstractive sentence summarization", "author": ["Alexander M. Rush", "Sumit Chopra", "Jason Weston"], "venue": "In Proc. EMNLP,", "citeRegEx": "Rush et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Rush et al\\.", "year": 2015}, {"title": "Incremental decoding for phrase-based statistical machine translation", "author": ["Baskaran Sankaran", "Ajeet Grewal", "Anoop Sarkar"], "venue": "In Proc. WMT,", "citeRegEx": "Sankaran et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Sankaran et al\\.", "year": 2010}, {"title": "Improving neural machine translation models with monolingual data", "author": ["Rico Sennrich", "Barry Haddow", "Alexandra Birch"], "venue": "In Proc. ACL,", "citeRegEx": "Sennrich et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Sennrich et al\\.", "year": 2016}, {"title": "A mathematical theory of communication", "author": ["Claude Shannon"], "venue": "Bell System Technical Journal,", "citeRegEx": "Shannon.,? \\Q1948\\E", "shortCiteRegEx": "Shannon.", "year": 1948}, {"title": "Computational complexity of probabilistic disambiguation by means of treegrammars", "author": ["Khalil Sima\u2019an"], "venue": "In Proc. COLING,", "citeRegEx": "Sima.an.,? \\Q1996\\E", "shortCiteRegEx": "Sima.an.", "year": 1996}, {"title": "Sequence to sequence learning with neural networks", "author": ["Ilya Sutskever", "Oriol Vinyals", "Quoc V. Le"], "venue": "In Proc. NIPS,", "citeRegEx": "Sutskever et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Sutskever et al\\.", "year": 2014}, {"title": "A DP-based search using monotone alignments in statistical translation", "author": ["Christoph Tillmann", "Stephan Vogel", "Hermann Ney", "Alex Zubiaga"], "venue": "In Proc. EACL,", "citeRegEx": "Tillmann et al\\.,? \\Q1997\\E", "shortCiteRegEx": "Tillmann et al\\.", "year": 1997}, {"title": "Neural machine translation with reconstruction", "author": ["Zhaopeng Tu", "Yang Liu", "Lifeng Shang", "Xiaohua Liu", "Hang Li"], "venue": null, "citeRegEx": "Tu et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Tu et al\\.", "year": 2016}, {"title": "Online segment to segment neural transduction", "author": ["Lei Yu", "Jan Buys", "Phil Blunsom"], "venue": "In Proc. EMNLP,", "citeRegEx": "Yu et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Yu et al\\.", "year": 2016}], "referenceMentions": [{"referenceID": 28, "context": "Recurrent neural network sequence to sequence models (Kalchbrenner & Blunsom, 2013; Sutskever et al., 2014; Bahdanau et al., 2015) are excellent models of p(output sequence y | input sequence x), provided sufficient input\u2013output (x,y) pairs are available for estimating their parameters.", "startOffset": 53, "endOffset": 130}, {"referenceID": 0, "context": "Recurrent neural network sequence to sequence models (Kalchbrenner & Blunsom, 2013; Sutskever et al., 2014; Bahdanau et al., 2015) are excellent models of p(output sequence y | input sequence x), provided sufficient input\u2013output (x,y) pairs are available for estimating their parameters.", "startOffset": 53, "endOffset": 130}, {"referenceID": 26, "context": "A classic strategy for exploiting both kinds of data is to use Bayes\u2019 rule to rewrite p(y | x) as p(x | y)p(y)/p(x), a factorisation which is called a noisy channel model (Shannon, 1948).", "startOffset": 171, "endOffset": 186}, {"referenceID": 0, "context": "To illustrate, an appealing parameterization would be to use an attentional seq2seq network (Bahdanau et al., 2015) to model Work completed at DeepMind.", "startOffset": 92, "endOffset": 115}, {"referenceID": 31, "context": "In this work, we use a variant of the newly proposed online seq2seq model of Yu et al. (2016) which uses a latent alignment variable to enable its probabilities to factorize in terms of prefixes of both the input and output, making it an appropriate channel model (\u00a72).", "startOffset": 77, "endOffset": 94}, {"referenceID": 31, "context": "1 of Yu et al. (2016) for details.", "startOffset": 5, "endOffset": 22}, {"referenceID": 31, "context": "1 of Yu et al. (2016) for details. In this paper, we use a slightly different objective from the one described in Yu et al. (2016). Rather than marginalizing over the paths that end in any possible input positions \u2211I i=1 \u03b1(i, |y|), we require that the full input be consumed when the final output symbol is generated.", "startOffset": 5, "endOffset": 131}, {"referenceID": 27, "context": "Marginalizing the latent variable during search is computationally hard (Sima\u2019an, 1996), and so we approximate the search problem as", "startOffset": 72, "endOffset": 87}, {"referenceID": 29, "context": "Algorithm 1, in Appendix A, describes the decoding algorithm based on a formulation by Tillmann et al. (1997). The idea is to create a matrix Q of partial hypotheses.", "startOffset": 87, "endOffset": 110}, {"referenceID": 23, "context": "The dataset (Rush et al., 2015) that we use is constructed by pairing the first sentence and", "startOffset": 12, "endOffset": 31}, {"referenceID": 21, "context": "the headline of each article from the annotated Gigaword corpus (Graff et al., 2003; Napoles et al., 2012).", "startOffset": 64, "endOffset": 106}, {"referenceID": 23, "context": "This setup is in line with the previous work on this task (Rush et al., 2015; Chopra et al., 2016; G\u00fcl\u00e7ehre et al., 2016; Yu et al., 2016).", "startOffset": 58, "endOffset": 138}, {"referenceID": 4, "context": "This setup is in line with the previous work on this task (Rush et al., 2015; Chopra et al., 2016; G\u00fcl\u00e7ehre et al., 2016; Yu et al., 2016).", "startOffset": 58, "endOffset": 138}, {"referenceID": 31, "context": "This setup is in line with the previous work on this task (Rush et al., 2015; Chopra et al., 2016; G\u00fcl\u00e7ehre et al., 2016; Yu et al., 2016).", "startOffset": 58, "endOffset": 138}, {"referenceID": 23, "context": "ABS+ (Rush et al., 2015), RAS-LSTM and RAS-Elman (Chopra et al.", "startOffset": 5, "endOffset": 24}, {"referenceID": 4, "context": ", 2015), RAS-LSTM and RAS-Elman (Chopra et al., 2016) are different variations of the attentive models.", "startOffset": 32, "endOffset": 53}, {"referenceID": 18, "context": ", 2003; Napoles et al., 2012). There are 3.8m, 190k and 381k sentence pairs in the training, validation and test sets, respectively. Yu et al. (2016) filtered the dataset by restricting the lengths of the input and output sentences to be no greater than 50 and 25 tokens, respectively.", "startOffset": 8, "endOffset": 150}, {"referenceID": 23, "context": "ABS+ (Rush et al., 2015) is the attentive model with bag-of-words as the encoder.", "startOffset": 5, "endOffset": 24}, {"referenceID": 4, "context": "RAS-LSTM and RAS-Elman (Chopra et al., 2016) are the sequence to sequence models with attention with the RNN cell implemented as LSTMs and an Elman architecture (Elman, 1990), respectively.", "startOffset": 23, "endOffset": 44}, {"referenceID": 7, "context": ", 2016) are the sequence to sequence models with attention with the RNN cell implemented as LSTMs and an Elman architecture (Elman, 1990), respectively.", "startOffset": 124, "endOffset": 137}, {"referenceID": 0, "context": "To set a benchmark, we train the attentional sequence to sequence model (Bahdanau et al., 2015) using the same parallel data.", "startOffset": 72, "endOffset": 95}, {"referenceID": 22, "context": "NCK15 (Nicolai et al., 2015) tackles the task based on the three-stage approach: (1) align the source and target word, (2) extract inflection rules, (3) apply the rule to new examples.", "startOffset": 6, "endOffset": 28}, {"referenceID": 8, "context": "FTND16 (Faruqui et al., 2016) is based on neural sequence to sequence models.", "startOffset": 7, "endOffset": 29}, {"referenceID": 22, "context": "NCK15 (Nicolai et al., 2015) and FTND16 (Faruqui et al.", "startOffset": 6, "endOffset": 28}, {"referenceID": 8, "context": ", 2015) and FTND16 (Faruqui et al., 2016) are previous state-of-the-art on this task, with NCK15 based on feature engineering, and FTND16 based on neural networks.", "startOffset": 19, "endOffset": 41}, {"referenceID": 15, "context": "Noisy channel decompositions have been successfully used in a variety of problems, including speech recognition (Jelinek, 1998), machine translation (Brown et al.", "startOffset": 112, "endOffset": 127}, {"referenceID": 2, "context": "Noisy channel decompositions have been successfully used in a variety of problems, including speech recognition (Jelinek, 1998), machine translation (Brown et al., 1993), spelling correction (Brill & Moore, 2000), and question answering (Echihabi & Marcu, 2003).", "startOffset": 149, "endOffset": 169}, {"referenceID": 10, "context": "Another trend of work that is related to our model is the investigation of making online prediction for machine translation (Gu et al., 2016; Grissom II et al., 2014; Sankaran et al., 2010) and speech recognition (Hwang & Sung, 2016; Jaitly et al.", "startOffset": 124, "endOffset": 189}, {"referenceID": 24, "context": "Another trend of work that is related to our model is the investigation of making online prediction for machine translation (Gu et al., 2016; Grissom II et al., 2014; Sankaran et al., 2010) and speech recognition (Hwang & Sung, 2016; Jaitly et al.", "startOffset": 124, "endOffset": 189}, {"referenceID": 14, "context": ", 2010) and speech recognition (Hwang & Sung, 2016; Jaitly et al., 2016).", "startOffset": 31, "endOffset": 72}, {"referenceID": 2, "context": "Noisy channel decompositions have been successfully used in a variety of problems, including speech recognition (Jelinek, 1998), machine translation (Brown et al., 1993), spelling correction (Brill & Moore, 2000), and question answering (Echihabi & Marcu, 2003). The idea of adding language models and monolingual data in machine translation has been explored in earlier work. G\u00fcl\u00e7ehre et al. (2015) propose two strategies of combining a language model with a neural sequence to sequence model.", "startOffset": 150, "endOffset": 400}, {"referenceID": 2, "context": "Noisy channel decompositions have been successfully used in a variety of problems, including speech recognition (Jelinek, 1998), machine translation (Brown et al., 1993), spelling correction (Brill & Moore, 2000), and question answering (Echihabi & Marcu, 2003). The idea of adding language models and monolingual data in machine translation has been explored in earlier work. G\u00fcl\u00e7ehre et al. (2015) propose two strategies of combining a language model with a neural sequence to sequence model. In shallow fusion, during decoding the sequence to sequence model (direct model) proposes candidate outputs and these candidates are reranked based on the scores calculated by a weighted sum of the probability of the translation model and that of the language model. In deep fusion, the language model is integrated into the decoder of the sequence to sequence model by concatenating their hidden state at each time step. Sennrich et al. (2016) incorporate target language unpaired training data by doing back-translation to create synthetic parallel training data.", "startOffset": 150, "endOffset": 940}, {"referenceID": 2, "context": "Noisy channel decompositions have been successfully used in a variety of problems, including speech recognition (Jelinek, 1998), machine translation (Brown et al., 1993), spelling correction (Brill & Moore, 2000), and question answering (Echihabi & Marcu, 2003). The idea of adding language models and monolingual data in machine translation has been explored in earlier work. G\u00fcl\u00e7ehre et al. (2015) propose two strategies of combining a language model with a neural sequence to sequence model. In shallow fusion, during decoding the sequence to sequence model (direct model) proposes candidate outputs and these candidates are reranked based on the scores calculated by a weighted sum of the probability of the translation model and that of the language model. In deep fusion, the language model is integrated into the decoder of the sequence to sequence model by concatenating their hidden state at each time step. Sennrich et al. (2016) incorporate target language unpaired training data by doing back-translation to create synthetic parallel training data. While this technique is quite effective, its practicality seems limited to problems where the inputs and outputs contain roughly the same information (such as translation). Cheng et al. (2016) leverages the abundant monolingual data by doing multitask learning with an autoencoding objective.", "startOffset": 150, "endOffset": 1254}, {"referenceID": 2, "context": "Noisy channel decompositions have been successfully used in a variety of problems, including speech recognition (Jelinek, 1998), machine translation (Brown et al., 1993), spelling correction (Brill & Moore, 2000), and question answering (Echihabi & Marcu, 2003). The idea of adding language models and monolingual data in machine translation has been explored in earlier work. G\u00fcl\u00e7ehre et al. (2015) propose two strategies of combining a language model with a neural sequence to sequence model. In shallow fusion, during decoding the sequence to sequence model (direct model) proposes candidate outputs and these candidates are reranked based on the scores calculated by a weighted sum of the probability of the translation model and that of the language model. In deep fusion, the language model is integrated into the decoder of the sequence to sequence model by concatenating their hidden state at each time step. Sennrich et al. (2016) incorporate target language unpaired training data by doing back-translation to create synthetic parallel training data. While this technique is quite effective, its practicality seems limited to problems where the inputs and outputs contain roughly the same information (such as translation). Cheng et al. (2016) leverages the abundant monolingual data by doing multitask learning with an autoencoding objective. A number of papers have remarked on the tendency for content to get dropped (or repeated) in translation. Liu et al. (2016) propose translating in both a left-to-right and a left-to-right direction and seeking a consensus.", "startOffset": 150, "endOffset": 1478}, {"referenceID": 2, "context": "Noisy channel decompositions have been successfully used in a variety of problems, including speech recognition (Jelinek, 1998), machine translation (Brown et al., 1993), spelling correction (Brill & Moore, 2000), and question answering (Echihabi & Marcu, 2003). The idea of adding language models and monolingual data in machine translation has been explored in earlier work. G\u00fcl\u00e7ehre et al. (2015) propose two strategies of combining a language model with a neural sequence to sequence model. In shallow fusion, during decoding the sequence to sequence model (direct model) proposes candidate outputs and these candidates are reranked based on the scores calculated by a weighted sum of the probability of the translation model and that of the language model. In deep fusion, the language model is integrated into the decoder of the sequence to sequence model by concatenating their hidden state at each time step. Sennrich et al. (2016) incorporate target language unpaired training data by doing back-translation to create synthetic parallel training data. While this technique is quite effective, its practicality seems limited to problems where the inputs and outputs contain roughly the same information (such as translation). Cheng et al. (2016) leverages the abundant monolingual data by doing multitask learning with an autoencoding objective. A number of papers have remarked on the tendency for content to get dropped (or repeated) in translation. Liu et al. (2016) propose translating in both a left-to-right and a left-to-right direction and seeking a consensus. Tu et al. (2016) propose augmenting a direct model\u2019s decoding objective with a reverse translation model (similar to our channel model except it conditions on the direct model\u2019s output RNN\u2019s hidden states rather than the words); however, that work just reranks complete translation hypotheses rather than developing a model that permits an incremental search.", "startOffset": 150, "endOffset": 1594}], "year": 2016, "abstractText": "We formulate sequence to sequence transduction as a noisy channel decoding problem and use recurrent neural networks to parameterise the source and channel models. Unlike direct models which can suffer from explaining-away effects during training, noisy channel models must produce outputs that explain their inputs, and their component models can be trained with not only paired training samples but also unpaired samples from the marginal output distribution. Using a latent variable to control how much of the conditioning sequence the channel model needs to read in order to generate a subsequent symbol, we obtain a tractable and effective beam search decoder. Experimental results on abstractive sentence summarisation, morphological inflection, and machine translation show that noisy channel models outperform direct models, and that they significantly benefit from increased amounts of unpaired output data that direct models cannot easily use.", "creator": "LaTeX with hyperref package"}}}