{"id": "1611.02683", "review": {"conference": "EMNLP", "VERSION": "v1", "DATE_OF_SUBMISSION": "8-Nov-2016", "title": "Unsupervised Pretraining for Sequence to Sequence Learning", "abstract": "Sequence to sequence models are successful tools for supervised sequence learning tasks, such as machine translation. Despite their success, these models still require much labeled data and it is unclear how to improve them using unlabeled data, which is much less expensive to obtain. In this paper, we present simple changes that lead to a significant improvement in the accuracy of seq2seq models when the labeled set is small. Our method intializes the encoder and decoder of the seq2seq model with the trained weights of two language models, and then all weights are jointly fine-tuned with labeled data. An additional language modeling loss can be used to regularize the model during fine-tuning. We apply this method to low-resource tasks in machine translation and abstractive summarization and find that it significantly improves the subsequent supervised models. Our main finding is that the pretraining accelerates training and improves generalization of seq2seq models, achieving state-of-the-art results on the WMT English$\\rightarrow$German task. Our model obtains an improvement of 1.3 BLEU from the previous best models on both WMT'14 and WMT'15 English$\\rightarrow$German. Our ablation study shows that pretraining helps seq2seq models in different ways depending on the nature of the task: translation benefits from the improved generalization whereas summarization benefits from the improved optimization.", "histories": [["v1", "Tue, 8 Nov 2016 20:42:26 GMT  (89kb,D)", "http://arxiv.org/abs/1611.02683v1", null]], "reviews": [], "SUBJECTS": "cs.CL cs.LG cs.NE", "authors": ["prajit ramachandran", "peter j liu", "quoc v le"], "accepted": true, "id": "1611.02683"}, "pdf": {"name": "1611.02683.pdf", "metadata": {"source": "CRF", "title": null, "authors": ["SEQUENCE LEARNING", "Prajit Ramachandran", "Peter J. Liu", "Quoc V. Le"], "emails": ["prajitram@gmail.com", "peterjliu@google.com", "qvl@google.com"], "sections": [{"heading": "1 INTRODUCTION", "text": "Sequence to sequence (seq2seq) models (Sutskever et al., 2014; Cho et al., 2014; Kalchbrenner & Blunsom, 2013) are extremely effective in a variety of tasks that require mapping between a variable-length input sequence and a variable-length output sequence (Sutskever et al., 2014; Vinyals et al., 2015b; a; Bahdanau et al., 2015; Chan et al., 2015; Bahdanau et al., 2016; Vinyals & Le, 2015; Shang et al., 2015; Nallapati et al., 2016; Wu et al., 2016). The main weakness of sequence models to sequence models and deep networks in general is that they can easily match when the amount of monitored training data matches."}, {"heading": "2 UNSUPERVISED PRETRAINING FOR SEQUENCE TO SEQUENCE LEARNING", "text": "In the following section, we describe our basic, unsupervised preparation process for sequence-to-sequence learning and how to modify sequence-to-sequence learning to effectively use the pre-trained weights. Afterwards, we show several enhancements to improve the basic model."}, {"heading": "2.1 BASIC PROCEDURE", "text": "The basic approach of our approach is to pre-train both the encoder and decoder networks in the sequence for sequencing the framework with language models that can be trained on large amounts of unlabeled text data. This can be seen in Figure 1, where the parameters in the shaded fields are pre-trained. However, in the following we will describe the method in detail using machine translation as an example application, but the method can be applied to all sequence learning tasks. First, two monolingual data sets are collected, one for the source language Dsrc, and one for the target language Dtgt. A language model (LM) is independently trained on each data set, with one LM trained on the source language Lsrc, and one LM trained on the target language Dsrc, and one LM trained on the target language Ltgt."}, {"heading": "2.2 IMPROVING THE MODEL", "text": "The three methods are: a) Monolingual language modeling loss, b) Residual connections and c) Attenuation and overfit on the labeled dataset. To combat this problem, we need additional monolingual language modeling loss: L = Llabel (Dlabel; M) + Lsrc (Dsrc) + \u03b2Ltgt (Dtgt) The additional losses Lsrc and tgt tgt) The three methods are: a) Monolingual language modeling loss: This is the seq2seq model can forget the learned information from language modeling and overfit on the labeled set, we incorporate additional monolingual language modeling loss to optimize M: L = Llabel (Dlabel; M) + \u03b1Lsrc (Dsrc) + \u03b2Ltgt."}, {"heading": "3 RELATED WORK", "text": "In fact, the number of those who are able to play by the rules is very high. (...) Most people who are able to play by the rules are not able to play by the rules. (...) Most people who are able to play by the rules are not able to play by the rules. (...) Most people who are able to play by the rules have to play by the rules. (...) Most people who are able to play by the rules have to play by the rules. (...) Most people who are able to play by the rules. \"(...) Most people who are able to play by the rules.\" (...) Most people who are able to play by the rules. \""}, {"heading": "4 EXPERIMENTS", "text": "In the following section, we apply our approach to two important tasks of seq2seq learning: machine translation and abstract summary. For each task, we compare with the best systems to date. We also conduct ablation experiments to understand the behavior of each component of our method."}, {"heading": "4.1 MACHINE TRANSLATION", "text": "We used the model of the WMT-14 training datasets, which are slightly smaller than the WMT-15 datasets. Since the datasets have some noisy examples, we used a voice recognition system to filter the training examples. Sentences pairs in which one source or another was not English or the other target was not German were discarded, resulting in about 4 million training examples. According to Sennrich et al. (2015b) we use subword units (Sennrich et al., 2015a) with 89500 merge operations that give a vocabulary around 90000. The validation is the linked newstest2012 and newsteste2013, and our test sets are newstestesteste2014 and newsteste2015. The validation evaluation of the validation was with case-sensitive BLEU (Papineni et al., 2002) on tokenized text with multiple evaluation."}, {"heading": "System BLEU", "text": "For this purpose, we trained a model with and without pretraining on random subsets of the English \u2192 German corpus. Both models use the additional LM target. The results are summarized in Figure 4. If 100% of the marked data is used, the gap between the pre-trained and no pretrain model is 2.0 BLEU points. However, this gap grows when less data is available. If you train on 20% of the marked data, there is a gap of 3.8 BLEU points. This shows that the pre-trained models deteriorate less when the marked data set becomes smaller."}, {"heading": "4.2 ABSTRACTIVE SUMMARIZATION", "text": "This year, more than ever before in the history of a country in which it is a country in which it is a country in which it is not a country, but a country in which it is a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country"}, {"heading": "5 CONCLUSION", "text": "We have shown that sequence model pre-training is a simple but effective technique that can help both generalize and optimize. Our scheme includes pre-training two source and target language models and initializing the embedding, first LSTM layers, and softmax of a sequence model to sequence models weighted with the language models. Using an additional voice modeling target during fine-tuning helps regularize the model. In our experiments, we showed that pre-training can improve results for tasks with hundreds of thousands or millions of examples. A key advantage of this technique is that it is flexible and can be applied to a wide variety of tasks, including conversation modeling for chatbots or answering questions."}, {"heading": "ACKNOWLEDGMENTS", "text": "We thank George Dahl, Andrew Dai, Laurent Dinh, Stephan Gouws, Geoffrey Hinton, Rafal Jozefowicz, Pooya Khorrami, Phillip Louis, Ramesh Nallapati, Arvind Neelakantan, Xin Pan, Abi See, Rico Sennrich, Luke Vilnis, Yuan Yu and the Google Brain team for their help with the project."}, {"heading": "Source Document", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "SELECTED SUMMARIZATION OUTPUTS", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "APPENDIX", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "Source Document", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "Source Document", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "Source Document", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "Source", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "SELECTED ENGLISH\u2192GERMAN OUTPUTS", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "Source", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "Source", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "Source", "text": "It is not the first time that the EU Commission has taken such a step."}], "references": [{"title": "Neural machine translation by jointly learning to align and translate", "author": ["Dzmitry Bahdanau", "Kyunghyun Cho", "Yoshua Bengio"], "venue": "In ICLR,", "citeRegEx": "Bahdanau et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Bahdanau et al\\.", "year": 2015}, {"title": "End-to-end attentionbased large vocabulary speech recognition", "author": ["Dzmitry Bahdanau", "Jan Chorowski", "Dmitriy Serdyuk", "Yoshua Bengio"], "venue": "In ICASSP,", "citeRegEx": "Bahdanau et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Bahdanau et al\\.", "year": 2016}, {"title": "Greedy layer-wise training of deep networks", "author": ["Yoshua Bengio", "Pascal Lamblin", "Dan Popovici", "Hugo Larochelle"], "venue": "In NIPS,", "citeRegEx": "Bengio et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Bengio et al\\.", "year": 2007}, {"title": "Findings of the 2015 workshop on statistical machine translation", "author": ["Ond\u0159ej Bojar", "Rajen Chatterjee", "Christian Federmann", "Barry Haddow", "Matthias Huck", "Chris Hokamp", "Philipp Koehn", "Varvara Logacheva", "Christof Monz", "Matteo Negri", "Matt Post", "Carolina Scarton", "Lucia Specia", "Marco Turchi"], "venue": "In Proceedings of the Tenth Workshop on Statistical Machine Translation,", "citeRegEx": "Bojar et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Bojar et al\\.", "year": 2015}, {"title": "Listen, attend and spell", "author": ["William Chan", "Navdeep Jaitly", "Quoc V. Le", "Oriol Vinyals"], "venue": "arXiv preprint arXiv:1508.01211,", "citeRegEx": "Chan et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Chan et al\\.", "year": 2015}, {"title": "Semisupervised learning for neural machine translation", "author": ["Yong Cheng", "Wei Xu", "Zhongjun He", "Wei He", "Hua Wu", "Maosong Sun", "Yang Liu"], "venue": "arXiv preprint arXiv:1606.04596,", "citeRegEx": "Cheng et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Cheng et al\\.", "year": 2016}, {"title": "Learning phrase representations using RNN encoder-decoder for statistical machine translation", "author": ["Kyunghyun Cho", "Bart Van Merri\u00ebnboer", "Caglar Gulcehre", "Dzmitry Bahdanau", "Fethi Bougares", "Holger Schwenk", "Yoshua Bengio"], "venue": "In EMNLP,", "citeRegEx": "Cho et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Cho et al\\.", "year": 2014}, {"title": "Phone recognition with the mean-covariance restricted boltzmann machine", "author": ["George Dahl", "Marc\u2019Aurelio Ranzato", "Abdel rahman Mohamed", "Geoffrey E. Hinton"], "venue": null, "citeRegEx": "Dahl et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Dahl et al\\.", "year": 2010}, {"title": "Semi-supervised sequence learning", "author": ["Andrew M. Dai", "Quoc V. Le"], "venue": "In NIPS", "citeRegEx": "Dai and Le.,? \\Q2015\\E", "shortCiteRegEx": "Dai and Le.", "year": 2015}, {"title": "Why does unsupervised pre-training help deep learning", "author": ["Dumitru Erhan", "Yoshua Bengio", "Aaron Courville", "Pierre-Antoine Manzagol", "Pascal Vincent", "Samy Bengio"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Erhan et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Erhan et al\\.", "year": 2010}, {"title": "Zero-resource translation with multi-lingual neural machine translation", "author": ["Orhan Firat", "Baskaran Sankaran", "Yaser Al-Onaizan", "Fatos T. Yarman-Vural", "Kyunghyun Cho"], "venue": "arXiv preprint arXiv:1606.04164,", "citeRegEx": "Firat et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Firat et al\\.", "year": 2016}, {"title": "On using monolingual corpora in neural machine translation", "author": ["Caglar Gulcehre", "Orhan Firat", "Kelvin Xu", "Kyunghyun Cho", "Loic Barrault", "Huei-Chi Lin", "Fethi Bougares", "Holger Schwenk", "Yoshua Bengio"], "venue": "arXiv preprint arXiv:1503.03535,", "citeRegEx": "Gulcehre et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Gulcehre et al\\.", "year": 2015}, {"title": "Deep residual learning for image recognition", "author": ["Kaiming He", "Xiangyu Zhang", "Shaoqing Ren", "Jian Sun"], "venue": null, "citeRegEx": "He et al\\.,? \\Q2016\\E", "shortCiteRegEx": "He et al\\.", "year": 2016}, {"title": "Teaching machines to read and comprehend", "author": ["Karl Moritz Hermann", "Tomas Kocisky", "Edward Grefenstette", "Lasse Espeholt", "Will Kay", "Mustafa Suleyman", "Phil Blunsom"], "venue": "In NIPS", "citeRegEx": "Hermann et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Hermann et al\\.", "year": 2015}, {"title": "A fast learning algorithm for deep belief nets", "author": ["Geoffrey E. Hinton", "Simon Osindero", "Yee-Whye Teh"], "venue": "Neural Computation,", "citeRegEx": "Hinton et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Hinton et al\\.", "year": 2006}, {"title": "Long short-term memory", "author": ["Sepp Hochreiter", "J\u00fcrgen Schmidhuber"], "venue": "Neural Computation,", "citeRegEx": "Hochreiter and Schmidhuber.,? \\Q1997\\E", "shortCiteRegEx": "Hochreiter and Schmidhuber.", "year": 1997}, {"title": "Montreal neural machine translation systems for WMT\u201915", "author": ["S\u00e9bastien Jean", "Orhan Firat", "Kyunghyun Cho", "Roland Memisevic", "Yoshua Bengio"], "venue": "In Proceedings of the Tenth Workshop on Statistical Machine Translation,", "citeRegEx": "Jean et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Jean et al\\.", "year": 2015}, {"title": "Exploring the limits of language modeling", "author": ["Rafal Jozefowicz", "Oriol Vinyals", "Mike Schuster", "Noam Shazeer", "Yonghui Wu"], "venue": "arXiv preprint arXiv:1602.02410,", "citeRegEx": "Jozefowicz et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Jozefowicz et al\\.", "year": 2016}, {"title": "Recurrent continuous translation models", "author": ["Nal Kalchbrenner", "Phil Blunsom"], "venue": "In EMNLP,", "citeRegEx": "Kalchbrenner and Blunsom.,? \\Q2013\\E", "shortCiteRegEx": "Kalchbrenner and Blunsom.", "year": 2013}, {"title": "Adam: A method for stochastic optimization", "author": ["Diederik Kingma", "Jimmy Ba"], "venue": "In ICLR,", "citeRegEx": "Kingma and Ba.,? \\Q2015\\E", "shortCiteRegEx": "Kingma and Ba.", "year": 2015}, {"title": "Imagenet classification with deep convolutional neural networks", "author": ["Alex Krizhevsky", "Ilya Sutskever", "Geoffrey E. Hinton"], "venue": "In NIPS,", "citeRegEx": "Krizhevsky et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Krizhevsky et al\\.", "year": 2012}, {"title": "WikiReading: A novel large-scale language understanding task over wikipedia", "author": ["Alexandre Lacoste", "Andrew Fandrianto", "Daniel Hewlett", "David Berthelot", "Illia Polosukhin", "Jay Han", "Llion Jones", "Matthew Kelcey"], "venue": null, "citeRegEx": "Lacoste et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Lacoste et al\\.", "year": 2016}, {"title": "ROUGE: a package for automatic evaluation of summaries", "author": ["Chin-Yew Lin"], "venue": "In Proceedings of the Workshop on Text Summarization Branches Out (WAS", "citeRegEx": "Lin.,? \\Q2004\\E", "shortCiteRegEx": "Lin.", "year": 2004}, {"title": "Multi-task sequence to sequence learning", "author": ["Minh-Thang Luong", "Quoc V. Le", "Ilya Sutskever", "Oriol Vinyals", "Lukasz Kaiser"], "venue": "In ICLR,", "citeRegEx": "Luong et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Luong et al\\.", "year": 2015}, {"title": "Effective approaches to attentionbased neural machine translation", "author": ["Minh-Thang Luong", "Hieu Pham", "Christopher D. Manning"], "venue": "In EMNLP,", "citeRegEx": "Luong et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Luong et al\\.", "year": 2015}, {"title": "Distributed representations of words and phrases and their compositionality", "author": ["Tomas Mikolov", "Ilya Sutskever", "Kai Chen", "Greg S. Corrado", "Jeff Dean"], "venue": "In NIPS", "citeRegEx": "Mikolov et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Mikolov et al\\.", "year": 2013}, {"title": "Rectified linear units improve restricted boltzmann machines", "author": ["Vinod Nair", "Geoffrey E. Hinton"], "venue": "In ICML,", "citeRegEx": "Nair and Hinton.,? \\Q2010\\E", "shortCiteRegEx": "Nair and Hinton.", "year": 2010}, {"title": "Sequence-to-sequence RNNs for text summarization", "author": ["Ramesh Nallapati", "Bing Xiang", "Bowen Zhou"], "venue": "arXiv preprint arXiv:1602.06023,", "citeRegEx": "Nallapati et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Nallapati et al\\.", "year": 2016}, {"title": "Annotated gigaword", "author": ["Courtney Napoles", "Matthew Gormley", "Benjamin Van Durme"], "venue": "In Proceedings of the Joint Workshop on Automatic Knowledge Base Construction and Web-scale Knowledge Extraction,", "citeRegEx": "Napoles et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Napoles et al\\.", "year": 2012}, {"title": "An analysis of unsupervised pre-training in light of recent advances", "author": ["Tom Le Paine", "Pooya Khorrami", "Wei Han", "Thomas S. Huang"], "venue": "arXiv preprint arXiv:1412.6597,", "citeRegEx": "Paine et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Paine et al\\.", "year": 2014}, {"title": "BLEU: A method for automatic evaluation of machine translation", "author": ["Kishore Papineni", "Salim Roukos", "Todd Ward", "Wei-Jing Zhu"], "venue": "In ACL,", "citeRegEx": "Papineni et al\\.,? \\Q2002\\E", "shortCiteRegEx": "Papineni et al\\.", "year": 2002}, {"title": "On the difficulty of training recurrent neural networks", "author": ["Razvan Pascanu", "Tomas Mikolov", "Yoshua Bengio"], "venue": null, "citeRegEx": "Pascanu et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Pascanu et al\\.", "year": 2013}, {"title": "Glove: Global vectors for word representation", "author": ["Jeffrey Pennington", "Richard Socher", "Christopher D. Manning"], "venue": "In EMNLP,", "citeRegEx": "Pennington et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Pennington et al\\.", "year": 2014}, {"title": "ImageNet large scale visual recognition challenge", "author": ["Olga Russakovsky", "Jia Deng", "Hao Su", "Jonathan Krause", "Sanjeev Satheesh", "Sean Ma", "Zhiheng Huang", "Andrej Karpathy", "Aditya Khosla", "Michael Bernstein", "Alexander C. Berg", "Li Fei-Fei"], "venue": null, "citeRegEx": "Russakovsky et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Russakovsky et al\\.", "year": 2015}, {"title": "Long short-term memory recurrent neural network architectures for large scale acoustic modeling", "author": ["Hasim Sak", "Andrew W. Senior", "Fran\u00e7oise Beaufays"], "venue": "In INTERSPEECH,", "citeRegEx": "Sak et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Sak et al\\.", "year": 2014}, {"title": "Neural machine translation of rare words with subword units", "author": ["Rico Sennrich", "Barry Haddow", "Alexandra Birch"], "venue": "arXiv preprint arXiv:1508.07909,", "citeRegEx": "Sennrich et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Sennrich et al\\.", "year": 2015}, {"title": "Improving neural machine translation models with monolingual data", "author": ["Rico Sennrich", "Barry Haddow", "Alexandra Birch"], "venue": "arXiv preprint arXiv:1511.06709,", "citeRegEx": "Sennrich et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Sennrich et al\\.", "year": 2015}, {"title": "Neural responding machine for short-text conversation", "author": ["Lifeng Shang", "Zhengdong Lu", "Hang Li"], "venue": "arXiv preprint arXiv:1503.02364,", "citeRegEx": "Shang et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Shang et al\\.", "year": 2015}, {"title": "Sequence to sequence learning with neural networks", "author": ["Ilya Sutskever", "Oriol Vinyals", "Quoc V. Le"], "venue": "In NIPS", "citeRegEx": "Sutskever et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Sutskever et al\\.", "year": 2014}, {"title": "Improving LSTM-based video description with linguistic knowledge mined from text", "author": ["Subhashini Venugopalan", "Lisa Anne Hendricks", "Raymond Mooney", "Kate Saenko"], "venue": "arXiv preprint arXiv:1604.01729,", "citeRegEx": "Venugopalan et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Venugopalan et al\\.", "year": 2016}, {"title": "A neural conversational model", "author": ["Oriol Vinyals", "Quoc V. Le"], "venue": "arXiv preprint arXiv:1506.05869,", "citeRegEx": "Vinyals and Le.,? \\Q2015\\E", "shortCiteRegEx": "Vinyals and Le.", "year": 2015}, {"title": "Grammar as a foreign language", "author": ["Oriol Vinyals", "Lukasz Kaiser", "Terry Koo", "Slav Petrov", "Ilya Sutskever", "Geoffrey E. Hinton"], "venue": "In NIPS", "citeRegEx": "Vinyals et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Vinyals et al\\.", "year": 2015}, {"title": "Show and tell: A neural image caption generator", "author": ["Oriol Vinyals", "Alexander Toshev", "Samy Bengio", "Dumitru Erhan"], "venue": "In CVPR,", "citeRegEx": "Vinyals et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Vinyals et al\\.", "year": 2015}, {"title": "Google\u2019s neural machine translation system: Bridging the gap between human and machine translation", "author": ["Yonghui Wu", "Mike Schuster", "Zhifeng Chen", "Quoc V. Le", "Mohammad Norouzi"], "venue": "arXiv preprint arXiv:1609.08144,", "citeRegEx": "Wu et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Wu et al\\.", "year": 2016}, {"title": "Recurrent neural network regularization", "author": ["Wojciech Zaremba", "Ilya Sutskever", "Oriol Vinyals"], "venue": "arXiv preprint arXiv:1409.2329,", "citeRegEx": "Zaremba et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Zaremba et al\\.", "year": 2014}, {"title": "Exploiting source-side monolingual data in neural machine translation", "author": ["Jiajun Zhang", "Chengqing Zong"], "venue": "In EMNLP,", "citeRegEx": "Zhang and Zong.,? \\Q2016\\E", "shortCiteRegEx": "Zhang and Zong.", "year": 2016}, {"title": "Transfer learning for low-resource neural machine translation", "author": ["Barret Zoph", "Deniz Yuret", "Jonathan May", "Kevin Knight"], "venue": "In EMNLP,", "citeRegEx": "Zoph et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Zoph et al\\.", "year": 2016}], "referenceMentions": [{"referenceID": 38, "context": "Sequence to sequence (seq2seq) models (Sutskever et al., 2014; Cho et al., 2014; Kalchbrenner & Blunsom, 2013) are extremely effective on a variety of tasks that require a mapping between a variable-length input sequence to a variable-length output sequence (Sutskever et al.", "startOffset": 38, "endOffset": 110}, {"referenceID": 6, "context": "Sequence to sequence (seq2seq) models (Sutskever et al., 2014; Cho et al., 2014; Kalchbrenner & Blunsom, 2013) are extremely effective on a variety of tasks that require a mapping between a variable-length input sequence to a variable-length output sequence (Sutskever et al.", "startOffset": 38, "endOffset": 110}, {"referenceID": 38, "context": ", 2014; Kalchbrenner & Blunsom, 2013) are extremely effective on a variety of tasks that require a mapping between a variable-length input sequence to a variable-length output sequence (Sutskever et al., 2014; Vinyals et al., 2015b;a; Bahdanau et al., 2015; Chan et al., 2015; Bahdanau et al., 2016; Vinyals & Le, 2015; Shang et al., 2015; Nallapati et al., 2016; Wu et al., 2016).", "startOffset": 185, "endOffset": 380}, {"referenceID": 0, "context": ", 2014; Kalchbrenner & Blunsom, 2013) are extremely effective on a variety of tasks that require a mapping between a variable-length input sequence to a variable-length output sequence (Sutskever et al., 2014; Vinyals et al., 2015b;a; Bahdanau et al., 2015; Chan et al., 2015; Bahdanau et al., 2016; Vinyals & Le, 2015; Shang et al., 2015; Nallapati et al., 2016; Wu et al., 2016).", "startOffset": 185, "endOffset": 380}, {"referenceID": 4, "context": ", 2014; Kalchbrenner & Blunsom, 2013) are extremely effective on a variety of tasks that require a mapping between a variable-length input sequence to a variable-length output sequence (Sutskever et al., 2014; Vinyals et al., 2015b;a; Bahdanau et al., 2015; Chan et al., 2015; Bahdanau et al., 2016; Vinyals & Le, 2015; Shang et al., 2015; Nallapati et al., 2016; Wu et al., 2016).", "startOffset": 185, "endOffset": 380}, {"referenceID": 1, "context": ", 2014; Kalchbrenner & Blunsom, 2013) are extremely effective on a variety of tasks that require a mapping between a variable-length input sequence to a variable-length output sequence (Sutskever et al., 2014; Vinyals et al., 2015b;a; Bahdanau et al., 2015; Chan et al., 2015; Bahdanau et al., 2016; Vinyals & Le, 2015; Shang et al., 2015; Nallapati et al., 2016; Wu et al., 2016).", "startOffset": 185, "endOffset": 380}, {"referenceID": 37, "context": ", 2014; Kalchbrenner & Blunsom, 2013) are extremely effective on a variety of tasks that require a mapping between a variable-length input sequence to a variable-length output sequence (Sutskever et al., 2014; Vinyals et al., 2015b;a; Bahdanau et al., 2015; Chan et al., 2015; Bahdanau et al., 2016; Vinyals & Le, 2015; Shang et al., 2015; Nallapati et al., 2016; Wu et al., 2016).", "startOffset": 185, "endOffset": 380}, {"referenceID": 27, "context": ", 2014; Kalchbrenner & Blunsom, 2013) are extremely effective on a variety of tasks that require a mapping between a variable-length input sequence to a variable-length output sequence (Sutskever et al., 2014; Vinyals et al., 2015b;a; Bahdanau et al., 2015; Chan et al., 2015; Bahdanau et al., 2016; Vinyals & Le, 2015; Shang et al., 2015; Nallapati et al., 2016; Wu et al., 2016).", "startOffset": 185, "endOffset": 380}, {"referenceID": 43, "context": ", 2014; Kalchbrenner & Blunsom, 2013) are extremely effective on a variety of tasks that require a mapping between a variable-length input sequence to a variable-length output sequence (Sutskever et al., 2014; Vinyals et al., 2015b;a; Bahdanau et al., 2015; Chan et al., 2015; Bahdanau et al., 2016; Vinyals & Le, 2015; Shang et al., 2015; Nallapati et al., 2016; Wu et al., 2016).", "startOffset": 185, "endOffset": 380}, {"referenceID": 17, "context": "However, this requires pretraining a new model for every architecture change, which can be very costly given that a large language model can take on the order of a week or weeks to train (Jozefowicz et al., 2016).", "startOffset": 187, "endOffset": 212}, {"referenceID": 12, "context": "We used a simpler solution by introducing a residual connection (He et al., 2016) from the output of the first LSTM layer directly to the input of the softmax.", "startOffset": 64, "endOffset": 81}, {"referenceID": 11, "context": "In the experiments we show below, we used a gated residual connection with a bias initialized to a large positive as in Gulcehre et al. (2015), but later experimentation showed that a regular residual connection works just as well.", "startOffset": 120, "endOffset": 143}, {"referenceID": 0, "context": "Attention over multiple layers: In all our models, we used an attention mechanism (Bahdanau et al., 2015).", "startOffset": 82, "endOffset": 105}, {"referenceID": 14, "context": "Pretraining was once an essential technique for training deep networks (Hinton et al., 2006; Bengio et al., 2007; Dahl et al., 2010; Erhan et al., 2010).", "startOffset": 71, "endOffset": 152}, {"referenceID": 2, "context": "Pretraining was once an essential technique for training deep networks (Hinton et al., 2006; Bengio et al., 2007; Dahl et al., 2010; Erhan et al., 2010).", "startOffset": 71, "endOffset": 152}, {"referenceID": 7, "context": "Pretraining was once an essential technique for training deep networks (Hinton et al., 2006; Bengio et al., 2007; Dahl et al., 2010; Erhan et al., 2010).", "startOffset": 71, "endOffset": 152}, {"referenceID": 9, "context": "Pretraining was once an essential technique for training deep networks (Hinton et al., 2006; Bengio et al., 2007; Dahl et al., 2010; Erhan et al., 2010).", "startOffset": 71, "endOffset": 152}, {"referenceID": 20, "context": "However, better optimization techniques and big labeled datasets rendered pretraining unnecessary for training large, deep networks (Krizhevsky et al., 2012; Nair & Hinton, 2010; Russakovsky et al., 2015; He et al., 2016).", "startOffset": 132, "endOffset": 221}, {"referenceID": 33, "context": "However, better optimization techniques and big labeled datasets rendered pretraining unnecessary for training large, deep networks (Krizhevsky et al., 2012; Nair & Hinton, 2010; Russakovsky et al., 2015; He et al., 2016).", "startOffset": 132, "endOffset": 221}, {"referenceID": 12, "context": "However, better optimization techniques and big labeled datasets rendered pretraining unnecessary for training large, deep networks (Krizhevsky et al., 2012; Nair & Hinton, 2010; Russakovsky et al., 2015; He et al., 2016).", "startOffset": 132, "endOffset": 221}, {"referenceID": 29, "context": "Some works in computer vision even found that pretraining can hurt when a lot of labeled data is available (Paine et al., 2014).", "startOffset": 107, "endOffset": 127}, {"referenceID": 2, "context": ", 2006; Bengio et al., 2007; Dahl et al., 2010; Erhan et al., 2010). However, better optimization techniques and big labeled datasets rendered pretraining unnecessary for training large, deep networks (Krizhevsky et al., 2012; Nair & Hinton, 2010; Russakovsky et al., 2015; He et al., 2016). Some works in computer vision even found that pretraining can hurt when a lot of labeled data is available (Paine et al., 2014). In our work, we find that pretraining helps sequence to sequence models. Dai & Le (2015) was amongst the rare studies which showed the benefits of pretraining in a semisupervised learning setting.", "startOffset": 8, "endOffset": 510}, {"referenceID": 2, "context": ", 2006; Bengio et al., 2007; Dahl et al., 2010; Erhan et al., 2010). However, better optimization techniques and big labeled datasets rendered pretraining unnecessary for training large, deep networks (Krizhevsky et al., 2012; Nair & Hinton, 2010; Russakovsky et al., 2015; He et al., 2016). Some works in computer vision even found that pretraining can hurt when a lot of labeled data is available (Paine et al., 2014). In our work, we find that pretraining helps sequence to sequence models. Dai & Le (2015) was amongst the rare studies which showed the benefits of pretraining in a semisupervised learning setting. They found that pretraining LSTMs gave the state of the art performance for document classification and that increasing the amount of unlabeled data gave better performance for low resource settings. Zoph et al. (2016) showed that initializing a low resource seq2seq model with a seq2seq model trained on a different, higher resource language pair gave large performance gains.", "startOffset": 8, "endOffset": 837}, {"referenceID": 2, "context": ", 2006; Bengio et al., 2007; Dahl et al., 2010; Erhan et al., 2010). However, better optimization techniques and big labeled datasets rendered pretraining unnecessary for training large, deep networks (Krizhevsky et al., 2012; Nair & Hinton, 2010; Russakovsky et al., 2015; He et al., 2016). Some works in computer vision even found that pretraining can hurt when a lot of labeled data is available (Paine et al., 2014). In our work, we find that pretraining helps sequence to sequence models. Dai & Le (2015) was amongst the rare studies which showed the benefits of pretraining in a semisupervised learning setting. They found that pretraining LSTMs gave the state of the art performance for document classification and that increasing the amount of unlabeled data gave better performance for low resource settings. Zoph et al. (2016) showed that initializing a low resource seq2seq model with a seq2seq model trained on a different, higher resource language pair gave large performance gains. Luong et al. (2015a) found that multi-task learning with an autoencoding objective can improve translation performance.", "startOffset": 8, "endOffset": 1017}, {"referenceID": 2, "context": ", 2006; Bengio et al., 2007; Dahl et al., 2010; Erhan et al., 2010). However, better optimization techniques and big labeled datasets rendered pretraining unnecessary for training large, deep networks (Krizhevsky et al., 2012; Nair & Hinton, 2010; Russakovsky et al., 2015; He et al., 2016). Some works in computer vision even found that pretraining can hurt when a lot of labeled data is available (Paine et al., 2014). In our work, we find that pretraining helps sequence to sequence models. Dai & Le (2015) was amongst the rare studies which showed the benefits of pretraining in a semisupervised learning setting. They found that pretraining LSTMs gave the state of the art performance for document classification and that increasing the amount of unlabeled data gave better performance for low resource settings. Zoph et al. (2016) showed that initializing a low resource seq2seq model with a seq2seq model trained on a different, higher resource language pair gave large performance gains. Luong et al. (2015a) found that multi-task learning with an autoencoding objective can improve translation performance. Lacoste et al. (2016) found it necessary to pretrain a character level seq2seq model due to the long unroll length.", "startOffset": 8, "endOffset": 1138}, {"referenceID": 2, "context": ", 2006; Bengio et al., 2007; Dahl et al., 2010; Erhan et al., 2010). However, better optimization techniques and big labeled datasets rendered pretraining unnecessary for training large, deep networks (Krizhevsky et al., 2012; Nair & Hinton, 2010; Russakovsky et al., 2015; He et al., 2016). Some works in computer vision even found that pretraining can hurt when a lot of labeled data is available (Paine et al., 2014). In our work, we find that pretraining helps sequence to sequence models. Dai & Le (2015) was amongst the rare studies which showed the benefits of pretraining in a semisupervised learning setting. They found that pretraining LSTMs gave the state of the art performance for document classification and that increasing the amount of unlabeled data gave better performance for low resource settings. Zoph et al. (2016) showed that initializing a low resource seq2seq model with a seq2seq model trained on a different, higher resource language pair gave large performance gains. Luong et al. (2015a) found that multi-task learning with an autoencoding objective can improve translation performance. Lacoste et al. (2016) found it necessary to pretrain a character level seq2seq model due to the long unroll length. Zhang & Zong (2016) found it useful to add an additional task of sentence reordering of source-side monolingual data for neural machine translation.", "startOffset": 8, "endOffset": 1252}, {"referenceID": 2, "context": ", 2006; Bengio et al., 2007; Dahl et al., 2010; Erhan et al., 2010). However, better optimization techniques and big labeled datasets rendered pretraining unnecessary for training large, deep networks (Krizhevsky et al., 2012; Nair & Hinton, 2010; Russakovsky et al., 2015; He et al., 2016). Some works in computer vision even found that pretraining can hurt when a lot of labeled data is available (Paine et al., 2014). In our work, we find that pretraining helps sequence to sequence models. Dai & Le (2015) was amongst the rare studies which showed the benefits of pretraining in a semisupervised learning setting. They found that pretraining LSTMs gave the state of the art performance for document classification and that increasing the amount of unlabeled data gave better performance for low resource settings. Zoph et al. (2016) showed that initializing a low resource seq2seq model with a seq2seq model trained on a different, higher resource language pair gave large performance gains. Luong et al. (2015a) found that multi-task learning with an autoencoding objective can improve translation performance. Lacoste et al. (2016) found it necessary to pretrain a character level seq2seq model due to the long unroll length. Zhang & Zong (2016) found it useful to add an additional task of sentence reordering of source-side monolingual data for neural machine translation. Firat et al. (2016) introduced fine-tuning techniques that gave promising results for zero-resource neural machine translation.", "startOffset": 8, "endOffset": 1401}, {"referenceID": 2, "context": ", 2006; Bengio et al., 2007; Dahl et al., 2010; Erhan et al., 2010). However, better optimization techniques and big labeled datasets rendered pretraining unnecessary for training large, deep networks (Krizhevsky et al., 2012; Nair & Hinton, 2010; Russakovsky et al., 2015; He et al., 2016). Some works in computer vision even found that pretraining can hurt when a lot of labeled data is available (Paine et al., 2014). In our work, we find that pretraining helps sequence to sequence models. Dai & Le (2015) was amongst the rare studies which showed the benefits of pretraining in a semisupervised learning setting. They found that pretraining LSTMs gave the state of the art performance for document classification and that increasing the amount of unlabeled data gave better performance for low resource settings. Zoph et al. (2016) showed that initializing a low resource seq2seq model with a seq2seq model trained on a different, higher resource language pair gave large performance gains. Luong et al. (2015a) found that multi-task learning with an autoencoding objective can improve translation performance. Lacoste et al. (2016) found it necessary to pretrain a character level seq2seq model due to the long unroll length. Zhang & Zong (2016) found it useful to add an additional task of sentence reordering of source-side monolingual data for neural machine translation. Firat et al. (2016) introduced fine-tuning techniques that gave promising results for zero-resource neural machine translation. Sennrich et al. (2015b) introduced a data augmentation technique called backtranslation for neural machine translation.", "startOffset": 8, "endOffset": 1533}, {"referenceID": 32, "context": "Using pretrained GloVe (Pennington et al., 2014) vectors for embeddings was more important for their model.", "startOffset": 23, "endOffset": 48}, {"referenceID": 5, "context": "Cheng et al. (2016) proposed using monolingual data for neural machine translation by jointly training a source\u2192target model, target\u2192source model, source autoencoder, and target autoencoder.", "startOffset": 0, "endOffset": 20}, {"referenceID": 5, "context": "Cheng et al. (2016) proposed using monolingual data for neural machine translation by jointly training a source\u2192target model, target\u2192source model, source autoencoder, and target autoencoder. Their method demonstrates strong improvements. However, it is sensitive to how the monolingual corpora are picked, has to do an expensive top-k search every step for the autoencoding objective, and shows no improvement when using both target and source monolingual data over using just target monolingual data. Gulcehre et al. (2015) is the work closest to ours.", "startOffset": 0, "endOffset": 525}, {"referenceID": 5, "context": "Cheng et al. (2016) proposed using monolingual data for neural machine translation by jointly training a source\u2192target model, target\u2192source model, source autoencoder, and target autoencoder. Their method demonstrates strong improvements. However, it is sensitive to how the monolingual corpora are picked, has to do an expensive top-k search every step for the autoencoding objective, and shows no improvement when using both target and source monolingual data over using just target monolingual data. Gulcehre et al. (2015) is the work closest to ours. They combine an LM with an already trained seq2seq model by fine-tuning additional deep output layers. However, their method produces small improvements over the supervised baseline (\u2264 0.5 BLEU). We suspect that their method does not produce significant gains because (i) the models are trained independently of each other and are not fine-tuned (ii) the LM is combined with the seq2seq model after the last layer, wasting the benefit of the low level LM features, and (iii) only using the LM on the decoder side. Venugopalan et al. (2016) addressed (i) but still experienced minor improvements.", "startOffset": 0, "endOffset": 1094}, {"referenceID": 3, "context": "Dataset and Evaluation: For machine translation, we evaluate our method on the WMT English\u2192German task (Bojar et al., 2015).", "startOffset": 103, "endOffset": 123}, {"referenceID": 30, "context": "Evaluation on the validation set was with case-sensitive BLEU (Papineni et al., 2002) on tokenized text using multi-bleu.", "startOffset": 62, "endOffset": 85}, {"referenceID": 3, "context": "Dataset and Evaluation: For machine translation, we evaluate our method on the WMT English\u2192German task (Bojar et al., 2015). We used the WMT 14 training dataset, which is slightly smaller than the WMT 15 dataset. Because the dataset has some noisy examples, we used a language detection system to filter the training examples. Sentences pairs where the either source was not English or the target was not German were thrown away. This resulted in around 4 million training examples. Following Sennrich et al. (2015b), we use subword units (Sennrich et al.", "startOffset": 104, "endOffset": 517}, {"referenceID": 17, "context": "Experimental settings: The language models were trained in the same fashion as (Jozefowicz et al., 2016) We used a 1 layer 4096 dimensional LSTM with the hidden state projected down to 1024 units (Sak et al.", "startOffset": 79, "endOffset": 104}, {"referenceID": 34, "context": ", 2016) We used a 1 layer 4096 dimensional LSTM with the hidden state projected down to 1024 units (Sak et al., 2014) and trained for one week on 32 Tesla K40 GPUs.", "startOffset": 99, "endOffset": 117}, {"referenceID": 31, "context": "0 (Pascanu et al., 2013), and dropout of 0.", "startOffset": 2, "endOffset": 24}, {"referenceID": 44, "context": "2 on non-recurrent connections (Zaremba et al., 2014).", "startOffset": 31, "endOffset": 53}, {"referenceID": 16, "context": "BLEU System ensemble? newstest2014 newstest2015 Supervised Neural MT (Jean et al., 2015) single 22.", "startOffset": 69, "endOffset": 88}, {"referenceID": 3, "context": "Wikipedia is a different domain than the parallel corpus, which consists mainly of translations from the European Parliment and online news articles (Bojar et al., 2015).", "startOffset": 149, "endOffset": 169}, {"referenceID": 13, "context": "Dataset and Evaluation: For a low-resource abstractive summarization task, we use the CNN/Daily Mail corpus from (Hermann et al., 2015).", "startOffset": 113, "endOffset": 135}, {"referenceID": 13, "context": "Dataset and Evaluation: For a low-resource abstractive summarization task, we use the CNN/Daily Mail corpus from (Hermann et al., 2015). Following Nallapati et al. (2016), we modify the data collection scripts to restore the bullet point summaries.", "startOffset": 114, "endOffset": 171}, {"referenceID": 22, "context": "1 We evaluate our system using full length ROUGE (Lin, 2004).", "startOffset": 49, "endOffset": 60}, {"referenceID": 28, "context": "In this setting, we used the English Gigaword corpus (Napoles et al., 2012) as our larger, unlabeled \u201cmonolingual\u201d corpus, although all data used in this task is in English.", "startOffset": 53, "endOffset": 75}, {"referenceID": 26, "context": "To compare against Nallapati et al. (2016), we used the anonymized corpus.", "startOffset": 19, "endOffset": 43}, {"referenceID": 22, "context": "1 We evaluate our system using full length ROUGE (Lin, 2004). For the anonymized corpus in particular, we considered each highlight as a separate sentence following Nallapati et al. (2016). In this setting, we used the English Gigaword corpus (Napoles et al.", "startOffset": 50, "endOffset": 189}, {"referenceID": 17, "context": "The LM is a one-layer LSTM of size 1024 trained in a similar fashion to Jozefowicz et al. (2016). For the seq2seq model, we use the same settings as the machine translation experiments.", "startOffset": 72, "endOffset": 97}, {"referenceID": 25, "context": "Furthermore, they use pretrained word2vec (Mikolov et al., 2013) vectors to initialize their word embeddings.", "startOffset": 42, "endOffset": 64}, {"referenceID": 26, "context": "Our pretrained model is only able to match the previous baseline seq2seq of Nallapati et al. (2016). However, our model is a unidirectional LSTM while they use a bidirectional LSTM.", "startOffset": 76, "endOffset": 100}, {"referenceID": 27, "context": "System ROUGE-1 ROUGE-2 ROUGE-L Seq2seq + pretrained embeddings (Nallapati et al., 2016) 32.", "startOffset": 63, "endOffset": 87}, {"referenceID": 27, "context": "47 + temporal attention (Nallapati et al., 2016) 35.", "startOffset": 24, "endOffset": 48}, {"referenceID": 31, "context": "Randomly initialized encoder embeddings may be difficult to learn because the long unrolling of the encoder leads to vanishing gradient (Pascanu et al., 2013).", "startOffset": 136, "endOffset": 158}, {"referenceID": 27, "context": "Unwanted repetition was also noticed by Nallapati et al. (2016). Table 4 and 5 show the results of the study.", "startOffset": 40, "endOffset": 64}], "year": 2016, "abstractText": "Sequence to sequence models are successful tools for supervised sequence learning tasks, such as machine translation. Despite their success, these models still require much labeled data and it is unclear how to improve them using unlabeled data, which is much less expensive to obtain. In this paper, we present simple changes that lead to a significant improvement in the accuracy of seq2seq models when the labeled set is small. Our method intializes the encoder and decoder of the seq2seq model with the trained weights of two language models, and then all weights are jointly fine-tuned with labeled data. An additional language modeling loss can be used to regularize the model during fine-tuning. We apply this method to low-resource tasks in machine translation and abstractive summarization and find that it significantly improves the subsequent supervised models. Our main finding is that the pretraining accelerates training and improves generalization of seq2seq models, achieving state-of-the-art results on the WMT English\u2192German task. Our model obtains an improvement of 1.3 BLEU from the previous best models on both WMT\u201914 and WMT\u201915 English\u2192German. Our ablation study shows that pretraining helps seq2seq models in different ways depending on the nature of the task: translation benefits from the improved generalization whereas summarization benefits from the improved optimization.", "creator": "LaTeX with hyperref package"}}}