{"id": "1611.02770", "review": {"conference": "iclr", "VERSION": "v1", "DATE_OF_SUBMISSION": "8-Nov-2016", "title": "Delving into Transferable Adversarial Examples and Black-box Attacks", "abstract": "An intriguing property of deep neural networks is the existence of adversarial examples, which can transfer among different architectures. These transferable adversarial examples may severely hinder deep neural network-based applications. Previous works mostly study the transferability using small scale datasets. In this work, we are the first to conduct an extensive study of the transferability over large models and a large scale dataset, and we are also the first to study the transferability of targeted adversarial examples with their target labels. We study both non-targeted and targeted adversarial examples, and show that while transferable non-targeted adversarial examples are easy to find, targeted adversarial examples generated using existing approaches almost never transfer with their target labels. Therefore, we propose novel ensemble-based approaches to generating transferable adversarial examples. Using such approaches, we observe a large proportion of targeted adversarial examples that are able to transfer with their target labels for the first time. We also present some geometric studies to help understanding the transferable adversarial examples. Finally, we show that the adversarial examples generated using ensemble-based approaches can successfully attack Clarifai.com, which is a black-box image classification system.", "histories": [["v1", "Tue, 8 Nov 2016 23:25:00 GMT  (4546kb,D)", "https://arxiv.org/abs/1611.02770v1", null], ["v2", "Mon, 21 Nov 2016 22:28:51 GMT  (5312kb,D)", "http://arxiv.org/abs/1611.02770v2", null], ["v3", "Tue, 7 Feb 2017 14:24:44 GMT  (5328kb,D)", "http://arxiv.org/abs/1611.02770v3", null]], "reviews": [], "SUBJECTS": "cs.LG", "authors": ["yanpei liu", "xinyun chen", "chang liu", "dawn song"], "accepted": true, "id": "1611.02770"}, "pdf": {"name": "1611.02770.pdf", "metadata": {"source": "CRF", "title": null, "authors": ["BLACK-BOX ATTACKS", "Yanpei Liu", "Xinyun Chen", "Chang Liu", "Dawn Song"], "emails": [], "sections": [{"heading": null, "text": "A fascinating feature of deep neural networks is the presence of counter-examples that can be transferred to different architectures, and these transferable counter-examples can severely impede deep neural networks. Previous work has mostly examined the transferability of targeted counter-examples with their target designations. In this work, we are the first to conduct a comprehensive study of transferability to large models and large datasets, and we are also the first to examine the transferability of targeted counter-examples with their target designations. We examine both non-targeted and targeted counter-examples and show that while transferable, non-targeted counter-examples are easy to find, targeted counter-examples generated with existing approaches are almost never transferable with their target designations. Therefore, we propose novel ensemble-based approaches to generate transferable counter-examples. With such approaches, we observe a large proportion of targeted counter-examples that can be transfered with their target designations."}, {"heading": "1 INTRODUCTION", "text": "In fact, it is the case that most of them will be able to move to another world, in which they are able to move to another world, in which they are able to move to another world, in which they are able to move, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they, in which they live, in which they live, in which they, in which they live, in which they live, in which they live, in which they live, in which they, in which they live, in which they live, in which they live, in which they live, in which they, in which they, in which they, in which they live, in which they, in which they, in which they live, in which they, in which they live, in which they are able to"}, {"heading": "2 ADVERSARIAL DEEP LEARNING AND TRANSFERABILITY", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "2.1 THE ADVERSARIAL DEEP LEARNING PROBLEM", "text": "We assume that a classifier f\u03b8 (x) predicts a category (or label). Considering an original image x with the basic truth designation y, the adversarial, profound learning problem is to look for adversarial examples for the classifier f\u03b8 (x). Specifically, we consider two classes of adversarial examples. An untargeted adversarial example x? is an instance that is close to x, where x? should have the same basic truth as x, while f\u03b8 (x?) 6 = y. To make the problem not trivial, we assume f\u03b8 (x) = y without loss of universality. A targeted adversarial example x? is close to x? and fulfils f\u03b8 (x?) = y?, where y? is a target designation specified by the opponent, and y? 6 = y."}, {"heading": "2.2 APPROACHES FOR GENERATING ADVERSARIAL EXAMPLES", "text": "In this paper, we consider three classes of approaches for generating counter-examples: optimization-based approaches, fast gradient approaches, and fast gradient-sign approaches. Each class has non-targeted or target-oriented versions."}, {"heading": "2.2.1 APPROACHES FOR GENERATING NON-TARGETED ADVERSARIAL EXAMPLES", "text": "Formally speaking, an image x with the truth of the ground y = f\u03b8 (x), the search for an untargeted example of the opponent GS can be modeled as a search for an instance x? to fulfill the following constraints: f\u03b8 (x?) 6 = y (1) d (x, x?) \u2264 B (2), where d (\u00b7 \u00b7) is a metric to quantify the distance between an original image and its opponent counterpart, and B, called distortion, is an upper limit at that distance. Without loss of generality, we consider the model f composed of a network that gives the probability for each category x, so that f gives the category with the highest probability. One approach is to approximate the solution to the following optimization problem: argminx?"}, {"heading": "2.2.2 APPROACHES FOR GENERATING TARGETED ADVERSARIAL EXAMPLES", "text": "For the optimization-based approach, we approach the solution by solving the following dual goal: argminx? \u03bbd (x, x?) + '(1y?, J\u03b8 (x?)) (5) In this work, we choose the default cross entropy loss' (u, v) = \u2212 \u2211 i ui log vi.For FGS and FG, we construct contrary examples as follows: x? \u2190 Clip (x \u2212 Bsgn (x '(1y?, J\u03b8 (x))))) (FGS) x? \u2190 Clip (x \u2212 B \u00b2 x' (1y?, J\u03b8 (x)) | | (FG), where the same approach is used."}, {"heading": "2.3 EVALUATION METHODOLOGY", "text": "For the rest of the work, we will focus on examining the transferability between state-of-the-art models trained via ImageNet (Russakovsky et al. (2015). In this section, we will detail the models to be examined, the data set to be evaluated and the measurements to be used. We will examine five networks, ResNet-50, ResNet-101, ResNet-152 (He et al. (2015) 1, GoogLeNet (Szegedy et al. (2014) 2, and VGG-16 (Simonyan & Zisserman (2014) 3. We will retrieve the pre-trained models for each network online. The performance of these models at the ILSVRC 2012 (Russakovsky et al al al. (2015)))) validation set will be presented in the appendix (Table 7). We will select these models to examine the transferability between homogeneous architectures (i.e. ResNet models) and heterogeneous images."}, {"heading": "3 NON-TARGETED ADVERSARIAL EXAMPLES", "text": "In this section we will examine different approaches to creating untargeted enemy images."}, {"heading": "3.1 OPTIMIZATION-BASED APPROACH", "text": "In order to apply the optimization-based approach to a single model, we initialize x? to x and use Adam Optimizer (Kingma & Ba (2014) to optimize goal (3). We find that we can optimize RMSD by adjusting the learning rate of Adam and \u03bb. We find that we can use a small learning rate for each model to generate contrary images with small RMSD, i.e. < 2, with each \u03bb. In fact, we find that Adam Optimizer will look for a contrary example of x when initializing x? with x, even if we set it to 0, i.e. do not limit the distance between x? and x. Therefore, for all experiments that use optimization-based approaches throughout the paper, we set it to 0. Although these contrary examples are based on resaries with small distortions, we can successfully deceive the target model."}, {"heading": "3.2 FAST GRADIENT-BASED APPROACHES", "text": "A good feature of fast gradient-based approaches is that all produced contrary examples are in a 1-D subspace. Therefore, we can easily approximate the minimal distortion in this subspace of transferable contrary examples between two models. In the following, we first check the RMSD to investigate the effectiveness of fast gradient-based approaches. Second, we investigate the transferable minimal distortions of fast gradient-based approaches."}, {"heading": "3.2.1 EFFECTIVENESS AND TRANSFERABILITY OF THE FAST GRADIENT-BASED APPROACHES", "text": "Since the distortion B and the RMSD of the enemy images generated are strongly correlated, we can choose this hyperparameter B to produce hostile images with a given RMSD. In Table 1 Panel B, we generate hostile images with FG, so that the average RMSD is almost identical to those generated with the optimization-based approach. We observe that the diagonal values in the table are all positive, which means that FG cannot completely mislead the models. One possible reason for this is that FG can be seen as an approximation to optimization, but is tailored to speed over accuracy. On the other hand, the values of the non-diagonal cells in the table, which are evaluated to the accuracies of those generated for one model but evaluated on another, are also comparable to or less than their counterparts in the optimization-based approach. This shows that non-targeted hostile examples generated by FG are also transferable."}, {"heading": "3.2.2 ADVERSARIAL IMAGES WITH MINIMAL TRANSFERABLE RMSD", "text": "For a picture x and two models M1, M2, we can approximate the minimum distortion B along one direction \u03b4, so that xB = x + B3 for M1 is counterproductive for both M1 and M2. We refer to the minimally transferable RMSD from M1 to M2 using FG (or FGS) as the RMSD of a transferable opposite example xB with the minimally transferable distortion B using FG (or FGS). The minimally transferable RMSD can illustrate the trade-off between the distortion and transferability of a transferable opposite example xB with the minimally transferable distortion B from M1 to M2 using FG (or FGS). Below, we approach the minimally transferable RMSD transferability of FG and the minimally transferable RSD transferability M1 by searching linear for FG."}, {"heading": "3.3 COMPARISON WITH RANDOM PERTURBATIONS", "text": "The concrete results can be found in the annex and we conclude that the \"transferability\" of this approach is significantly inferior to optimization-based approaches or rapid gradient-based approaches."}, {"heading": "4 TARGETED ADVERSARIAL EXAMPLES", "text": "In this section, we examine the transferability of target enemy images. Table 2 presents the results for the use of optimization-based approaches. We note that (1) the prediction of target enemy images may be consistent with the target markers if they are evaluated using the same model used to generate the enemy examples; but (2) the target enemy images can rarely be predicted with the target markers by another model. We refer to the latter as not transferring the target markers. Even if we increase the distortion, we still do not see improvements in target transfer. Some results can be found in the appendix (Table 17). Even if we calculate the matching rate based on top-5 accuracy, the highest matching rate is only 10%. The results can be found in the appendix (Table 18).We also examine the target enemy images generated by fast gradient-based approaches, and we observe that the markings are not shifted so well to the large (25) results."}, {"heading": "5 ENSEMBLE-BASED APPROACHES", "text": "This year, it has reached the point where it will be able to seek a solution that is capable of finding a solution, that is capable of finding a solution that is capable of finding a solution, and that is capable of finding a solution that is capable of finding a solution, that is capable of finding a solution that is capable of finding a solution, that is capable of finding a solution that is capable of finding a solution, that is capable of finding a solution, that is capable of finding a solution that is capable of finding a solution."}, {"heading": "6 GEOMETRIC PROPERTIES OF DIFFERENT MODELS", "text": "In this section, we will show some geometric properties of the models to try to better understand transferable opposing models. Previous work also attempts to theoretically understand the geometric properties of conflicting examples (Fawzi et al. (2016)) or empirically (Goodfellow et al. (2014). In this work, we will examine large models formed via a large dataset of 1000 labels whose geometric properties have never been studied before. This allows us to make new observations to better understand the models and their opposing amplitudes. The gradient directions of different models in our evaluation are almost orthogonally related to each other. We will investigate whether the contradictory properties of different models are interconnected. We will calculate the cosmic value of the angle between the gradient directions of different models, and the results can be found in the appendix (Table 33). We note that all non-diagonal values are close to 0, indicating that for most images, their models are different in relation to different models."}, {"heading": "7 REAL WORLD EXAMPLE: ADVERSARIAL EXAMPLES FOR CLARIFAI.COM", "text": "In fact, we will be able to achieve a subjective perception in the areas in which we find ourselves. We also have 400 contrary images in the aggregate, in which 200 of them turn out to be admineral examples, and the rest are not admineralized. As if the 200 targeted admineral images, 100 of them are generated as subjective metrics, we are able to realize the optimization on the basis of VGG-16, and the remaining 200 are not admineralizable."}, {"heading": "8 CONCLUSION", "text": "Our results confirm that the transferability of non-target enemy examples is important even for large models and a large dataset. On the other hand, we note that it is difficult to use existing approaches to generate target enemy examples whose target tag can be transmitted. We are developing novel ensemble-based approaches and show that they can generate transferable target enemy examples with a high success rate. Meanwhile, these new approaches perform better than previous work in generating non-target transferable enemy examples. We also show that both non-target and target enemy examples generated with our new approaches can successfully attack Clarifai.com, which is a black box image classification system. In addition, we are investigating some geometric properties to better understand the transferable enemy examples."}, {"heading": "ACKNOWLEDGMENTS", "text": "This material is based in part on work supported by the National Science Foundation under grant number TWC-1409915. Any opinions, findings, conclusions or recommendations expressed in this material are those of the author (s) and do not necessarily reflect the views of the National Science Foundation."}], "references": [], "referenceMentions": [], "year": 2017, "abstractText": "An intriguing property of deep neural networks is the existence of adversarial examples, which can transfer among different architectures. These transferable adversarial examples may severely hinder deep neural network-based applications. Previous works mostly study the transferability using small scale datasets. In this work, we are the first to conduct an extensive study of the transferability over large models and a large scale dataset, and we are also the first to study the transferability of targeted adversarial examples with their target labels. We study both non-targeted and targeted adversarial examples, and show that while transferable non-targeted adversarial examples are easy to find, targeted adversarial examples generated using existing approaches almost never transfer with their target labels. Therefore, we propose novel ensemble-based approaches to generating transferable adversarial examples. Using such approaches, we observe a large proportion of targeted adversarial examples that are able to transfer with their target labels for the first time. We also present some geometric studies to help understanding the transferable adversarial examples. Finally, we show that the adversarial examples generated using ensemble-based approaches can successfully attack Clarifai.com, which is a black-box image classification system.", "creator": "LaTeX with hyperref package"}}}