{"id": "1611.02796", "review": {"conference": "ICML", "VERSION": "v1", "DATE_OF_SUBMISSION": "9-Nov-2016", "title": "Sequence Tutor: Conservative Fine-Tuning of Sequence Generation Models with KL-control", "abstract": "Sequence models can be trained using supervised learning and a next-step prediction objective. This approach, however, suffers from known failure modes. For example, it is notoriously difficult to ensure multi-step generated sequences have coherent global structure. Motivated by the fact that reinforcement learning (RL) can be used to impose arbitrary properties on generated data by choosing appropriate reward functions, in this paper we propose a novel approach for sequence training which combines Maximum Likelihood (ML) and RL training. We refine a sequence predictor by optimizing for some imposed reward functions, while maintaining good predictive properties learned from data. We propose efficient ways to solve this by augmenting deep Q-learning with a cross-entropy reward and deriving novel off-policy methods for RNNs from stochastic optimal control (SOC). We explore the usefulness of our approach in the context of music generation. An LSTM is trained on a large corpus of songs to predict the next note in a musical sequence. This Note-RNN is then refined using RL, where the reward function is a combination of rewards based on rules of music theory, as well as the output of another trained Note-RNN. We show that by combining ML and RL, this RL Tuner method can not only produce more pleasing melodies, but that it can significantly reduce unwanted behaviors and failure modes of the RNN.", "histories": [["v1", "Wed, 9 Nov 2016 01:46:32 GMT  (406kb,D)", "http://arxiv.org/abs/1611.02796v1", null], ["v2", "Thu, 10 Nov 2016 18:54:17 GMT  (406kb,D)", "http://arxiv.org/abs/1611.02796v2", "Update affiliations"], ["v3", "Wed, 7 Dec 2016 14:42:30 GMT  (405kb,D)", "http://arxiv.org/abs/1611.02796v3", "Update acknowledgements"], ["v4", "Thu, 12 Jan 2017 02:18:20 GMT  (409kb,D)", "http://arxiv.org/abs/1611.02796v4", "Update acknowledgements"], ["v5", "Mon, 27 Feb 2017 20:38:06 GMT  (404kb,D)", "http://arxiv.org/abs/1611.02796v5", "Complete rewrite; new results on computational molecular generation"], ["v6", "Sat, 4 Mar 2017 19:38:01 GMT  (405kb,D)", "http://arxiv.org/abs/1611.02796v6", "Add citation for related work"], ["v7", "Thu, 6 Apr 2017 15:02:04 GMT  (405kb,D)", "http://arxiv.org/abs/1611.02796v7", "Add citation for related work"], ["v8", "Thu, 4 May 2017 17:11:45 GMT  (405kb,D)", "http://arxiv.org/abs/1611.02796v8", "Add citation for related work"], ["v9", "Mon, 16 Oct 2017 21:31:31 GMT  (429kb,D)", "http://arxiv.org/abs/1611.02796v9", "Add supplementary material"]], "reviews": [], "SUBJECTS": "cs.LG cs.AI", "authors": ["natasha jaques", "shixiang gu", "dzmitry bahdanau", "jos\u00e9 miguel hern\u00e1ndez-lobato", "richard e turner", "douglas eck"], "accepted": true, "id": "1611.02796"}, "pdf": {"name": "1611.02796.pdf", "metadata": {"source": "CRF", "title": null, "authors": ["Natasha Jaques", "Shixiang Gu", "Richard E. Turner", "Douglas Eck"], "emails": ["jaquesn@mit.edu,", "sg717@cam.ac.uk,", "ret26@cam.ac.uk,", "deck@google.com"], "sections": [{"heading": "1 INTRODUCTION", "text": "In fact, most people who are able to move, to move, to move, to move, to move, to move and to move, to move, to move, to move and to move, to move, to move and to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move."}, {"heading": "2 BACKGROUND", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "2.1 DEEP Q-LEARNING", "text": "In RL, an agent interacts with an environment. Considering the state of the environment at the time t, st, Q \u03b2, the agent takes an action according to his policy \u03c0 (at | st), receives a reward r (st, at) and the transition of the environment to a new state st + 1. The objective of the agent is to maximize the reward through a sequence of actions, with a discount factor of \u03b3 that is applied to future rewards. The optimal deterministic policy \u03c0 is known to satisfy the following Bellman optimality equation, Q (st, at) = r (st, at) + \u03b3Ep (st + 1 | st, at) [maxat + 1 Q (st + 1, at + 1; \u03c0)] (1), where Q\u03c0 (st, at) = E\u03c0 (st, at, t) = t (st, at) = tr (st, at) + \u03b3Ep (st) = Est, at (+ 1)."}, {"heading": "2.2 MUSIC GENERATION WITH LSTM", "text": "Previous work on creating music using deep learning (e.g. (Eck & Schmidhuber, 2002), (Sturm et al., 2016) included training an RNN to learn to predict the next note in a monophonic melody; we refer to this type of model as a note RNN. Often, the note RNN is implemented using a Long ShortTerm Memory (LSTM) network (Gers et al., 2000). LSTMs are networks in which each recurring cell learns to control the storage of information through the use of an input gate, output gate and forgotten gate. The first two gates control whether information can flow in and out of the cell, and the latter control whether the cell contents should be reset or not. Due to these characteristics, LSTMs are better at learning long-term dependencies in the data, forgetting output gate and forgotten gate."}, {"heading": "3 RL TUNER DESIGN", "text": "To accomplish this task, we propose RL Tuner, a novel sequence training method that incorporates RL. We use an LSTM trained on data (the Note RNN) to deliver the initial weights for three networks in RL Tuner (Note RNN): the Q network and the Target Q network in the DQN algorithm, as described in Section 2.1, and a Reward RNN. The Reward RNN is held firmly and used to deliver a portion of the reward value used in the training. To formulate the musical composition as an RL problem, we treat the placement of the next note in the composition as an action. The state of the environment s consists both of the notes placed so far in the composition, and of the internal state of the LSTM cells of both the Q network and the Reward RNN. To calculate the reward, we apply the rules we learned combine with the theory."}, {"heading": "3.1 RELATIONSHIP TO STOCHASTIC OPTIMAL CONTROL", "text": "The technique described in Section 3 is closely related to the previously defined problem of stochastic optimum control (SOC) (Todorov, 2006; Kappen et al., 2012; Rawlik et al., 2012). SOC defines a previous dynamic or policy and derives from it a variant of the control or RL problem as an approximate conclusion in a graphic model. Let log be a path of state and action, p () is a previous dynamic and r () is the reward of the path. Then SOC introduces an additional binary variable b and defines a graphic model as p (grandiose). b) = p (grandiose) p (b) is a previous dynamic where p (er) / c and c is the temperature variable. An approximation to p (grandiose) b = 1) can be derived using the variational method of free energy, and this results in costs with a similar form of the RL problem that is previously defined."}, {"heading": "3.2 MUSIC-THEORY BASED REWARD", "text": "A central question of this work is whether RL can be used to restrict a learner sequence in such a way that the sequences it generates adhere to a desired structure. To test this hypothesis, we developed several rules that we believe to describe more pleasant sounding melodies, taking inspiration from a text on melodic composition (Gauldin, 1995). We do not claim that these qualities are exhaustive, strictly necessary for a good composition, or even particularly interesting. They simply serve the purpose of guiding the model towards traditional compositional structure. It is therefore crucial to maintain knowledge of real songs in the training files. Following the principles set forth on page 42 of Gauldin's book, 1995, we define the reward function rMT (a, s) to encourage compositions to have the following characteristics. All notes should belong to the same key, and the composition should begin with the note ending and key signature."}, {"heading": "4 RELATED WORK", "text": "Generative modelling of music using RNNs as a reward signal has been studied in a variety of contexts, including the generation of Celtic folk music (Sturm et al., 2016), or performing blues improvisation (Eck & Schmidhuber, 2002) Other approaches have explored RNNs with richer expressiveness, latent variables for note-taking, or raw audio synthesis (Boulanger-Lewandowski et al., 2012; Gu et al., 2015; Chung et al.). Recently, impressive performance has been achieved in generating music from raw audio with revolutionary neural networks with receptive fields in different timescales. (Dieleman et al., 2016) Although the application of RNNN to a relatively new area, recent work has attempted to combine the two approaches. MIXER (Mixed Incremental Cross-Entropy Reinforce) (Ranzato et al., 2015) has been used as a BLN signal."}, {"heading": "5 EXPERIMENTS", "text": "To learn the note RNN, we extract monophonic melodies from a corpus of 30,000 MIDI songs. Melodies are quantified at the granularity of a sixteenth note, so that each time step corresponds to a sixteenth of a bar of music. We encode a melody with two special events plus three octaves of notes. The special events are used to introduce rests and notes with longer runtimes, and are encoded as 0 = note out, 1 = no event. Three octaves of pitches, starting with MIDI pitch 48, are then encoded as 2 = C3, 3 = C # 3, 4 = D3, 37 = B5. The sequence {4, 0} encodes an eighth note with pitch D3, followed by an eighth note rest. Since the melodies are monophonic, another note implicitly plays the last note that was played without requiring an explicit note."}, {"heading": "6 RESULTS", "text": "In fact, most of them are able to go in search of a solution that fulfils their purpose. (...) Most of them are able to go in search of a solution. (...) Most of them are able to go in search of a solution. (...) Most of them are able to go in search of a solution. (...) Most of them are able to go in search of a solution. (...) Most of them are able to go in search of a solution. (...) Most of them are able to go in search of a solution. (...) Most of them are able to go in search of a solution. (...) Most of them are able to find a solution for themselves. (...) Most of them are able to find a solution for themselves. (...)"}, {"heading": "7 DISCUSSION AND FUTURE WORK", "text": "We have developed a novel sequence learning system that uses RL rewards to correct characteristics of sequences generated by an RNN while retaining much of the information gained from the supervised training on data. We have proposed and evaluated three alternative techniques to achieve this, and have shown promising results in creating music generation tasks. In addition to the ability to train models to generate fine-sounding melodies, we believe that our approach to using RL to refine RNN models could be promising for a number of applications. For example, it is known that a common error mode of RNNs is to generate the same character repeatedly. In text generation and auto-answering questions, this can take the form of repeated generation of such answers (e.g. \"How are you?\" \u2192 \"How are you doing?...\"). We have shown that with our approach, we can only maintain the undesirable behavior that we have learned from writing the undesired information while maintaining the model."}, {"heading": "ACKNOWLEDGMENTS", "text": "This work was supported by Google Brain, the MIT Media Lab Consortium and the Canadian Natural Sciences and Engineering Research Council (NSERC). We thank Greg Wayne, Sergey Levine and Timothy Lillicrap for helpful discussions on stochastic optimal control."}, {"heading": "8 APPENDIX", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "8.1 OFF-POLICY METHODS DERIVATIONS FOR KL-REGULARIZED REINFORCEMENT LEARNING", "text": "In view of the LL-regulated RL target defined in Eq.9, the value function is given by V (st; \u03c0) = E\u03c0 [\u2211 t \u2032 \u2265 t r (st \u2032, at \u2032) / c \u2212 KL [\u03c0 (\u00b7 | st \u2032) | | p (\u00b7 | st \u2032)] (15)."}, {"heading": "8.1.1 \u03a8-LEARNING", "text": "The following derivative is based on modifications to (Rawlik et al., 2012) and is similar to the derivative (in Fox et al.) (in Fox et al.) (in Fox et al.) (in Fox et al.) (in Fox et al.) (in Ep.) (in Ep.) (in Ep. + 1 r (in st.) / c \u2212 KL (in KL) (18) Value function can be expressed in V (st.; in Ep.) (in st.) (17) = r (st.) / c + log p (in | st.) + Ep (in Ep.) (in st. + 1 | st.) (18) Value function can be expressed in V (st.). (in Bellant.) = Ep. (in st.) + H (in Ep.) = Ep. (in Ep.) (in Ep.) (in. (in Ep.) (in Ep.)"}], "references": [{"title": "An actor-critic algorithm for sequence prediction", "author": ["Bahdanau"], "venue": "arXiv preprint:1607.07086,", "citeRegEx": "Bahdanau,? \\Q2016\\E", "shortCiteRegEx": "Bahdanau", "year": 2016}, {"title": "Modeling temporal dependencies in highdimensional sequences: Application to polyphonic music generation and transcription", "author": ["Boulanger-Lewandowski", "Bengio", "Vincent"], "venue": "arXiv preprint:1206.6392,", "citeRegEx": "Boulanger.Lewandowski et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Boulanger.Lewandowski et al\\.", "year": 2012}, {"title": "Semantics derived automatically from language corpora necessarily contain human biases", "author": ["Caliskan-Islam", "Bryson", "Narayanan"], "venue": "arXiv preprint:1608.07187,", "citeRegEx": "Caliskan.Islam et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Caliskan.Islam et al\\.", "year": 2016}, {"title": "A recurrent latent variable model for sequential data", "author": ["Chung", "Kastner", "Dinh", "Goel", "Courville", "Bengio"], "venue": "In NIPS, pp", "citeRegEx": "Chung et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Chung et al\\.", "year": 2015}, {"title": "Wavenet: A generative model for raw audio", "author": ["Dieleman"], "venue": "arXiv preprint:1609.03499,", "citeRegEx": "Dieleman,? \\Q2016\\E", "shortCiteRegEx": "Dieleman", "year": 2016}, {"title": "Finding temporal structure in music: Blues improvisation with LSTM recurrent networks", "author": ["Eck", "Schmidhuber"], "venue": "In Neural Networks for Signal Processing,", "citeRegEx": "Eck and Schmidhuber.,? \\Q2002\\E", "shortCiteRegEx": "Eck and Schmidhuber.", "year": 2002}, {"title": "Learning to forget: Continual prediction with LSTM", "author": ["Gers", "Schmidhuber", "Cummins"], "venue": "Neural computation,", "citeRegEx": "Gers et al\\.,? \\Q2000\\E", "shortCiteRegEx": "Gers et al\\.", "year": 2000}, {"title": "Generative adversarial nets", "author": ["Goodfellow"], "venue": "In NIPS, pp", "citeRegEx": "Goodfellow,? \\Q2014\\E", "shortCiteRegEx": "Goodfellow", "year": 2014}, {"title": "Framewise phoneme classification with bidirectional LSTM and other neural network architectures", "author": ["Graves", "Schmidhuber"], "venue": "Neural Networks,", "citeRegEx": "Graves and Schmidhuber.,? \\Q2005\\E", "shortCiteRegEx": "Graves and Schmidhuber.", "year": 2005}, {"title": "Neural adaptive sequential monte carlo", "author": ["Gu", "Ghahramani", "Turner"], "venue": "In NIPS, pp. 2629\u20132637,", "citeRegEx": "Gu et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Gu et al\\.", "year": 2015}, {"title": "Continuous Deep Q-Learning with model-based acceleration", "author": ["Gu", "Lillicrap", "Sutskever", "Levine"], "venue": "In ICML,", "citeRegEx": "Gu et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Gu et al\\.", "year": 2016}, {"title": "Deep reinforcement learning with double Q-learning", "author": ["Van Hasselt", "Guez", "Silver"], "venue": "CoRR, abs/1509.06461,", "citeRegEx": "Hasselt et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Hasselt et al\\.", "year": 2015}, {"title": "Optimal control as a graphical model inference problem", "author": ["Kappen", "G\u00f3mez", "Opper"], "venue": "Machine learning,", "citeRegEx": "Kappen et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Kappen et al\\.", "year": 2012}, {"title": "Adam: A method for stochastic optimization", "author": ["Kingma", "Ba"], "venue": "arXiv preprint:1412.6980,", "citeRegEx": "Kingma and Ba.,? \\Q2014\\E", "shortCiteRegEx": "Kingma and Ba.", "year": 2014}, {"title": "Continuous control with deep reinforcement learning", "author": ["Lillicrap"], "venue": null, "citeRegEx": "Lillicrap,? \\Q2016\\E", "shortCiteRegEx": "Lillicrap", "year": 2016}, {"title": "Emotional response to musical repetition", "author": ["Livingstone", "Palmer", "Schubert"], "venue": "Emotion, 12(3):552,", "citeRegEx": "Livingstone et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Livingstone et al\\.", "year": 2012}, {"title": "Recurrent neural network based language model", "author": ["Mikolov"], "venue": "In Interspeech,", "citeRegEx": "Mikolov,? \\Q2010\\E", "shortCiteRegEx": "Mikolov", "year": 2010}, {"title": "Playing atari with deep reinforcement learning", "author": ["Mnih"], "venue": "arXiv preprint:1312.5602,", "citeRegEx": "Mnih,? \\Q2013\\E", "shortCiteRegEx": "Mnih", "year": 2013}, {"title": "Reward augmented maximum likelihood for neural structured prediction", "author": ["Norouzi"], "venue": "arXiv preprint:1609.00150,", "citeRegEx": "Norouzi,? \\Q2016\\E", "shortCiteRegEx": "Norouzi", "year": 2016}, {"title": "Relative entropy policy search", "author": ["Peters", "M\u00fclling", "Altun"], "venue": "In AAAI. Atlanta,", "citeRegEx": "Peters et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Peters et al\\.", "year": 2010}, {"title": "Sequence level training with recurrent neural networks", "author": ["Ranzato", "Chopra", "Auli", "Zaremba"], "venue": "arXiv preprint:1511.06732,", "citeRegEx": "Ranzato et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Ranzato et al\\.", "year": 2015}, {"title": "On stochastic optimal control and reinforcement learning by approximate inference", "author": ["Rawlik", "Toussaint", "Vijayakumar"], "venue": "Proceedings of Robotics: Science and Systems VIII,", "citeRegEx": "Rawlik et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Rawlik et al\\.", "year": 2012}, {"title": "Trust region policy optimization", "author": ["Schulman", "Levine", "Moritz", "Jordan", "Abbeel"], "venue": "In ICML,", "citeRegEx": "Schulman et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Schulman et al\\.", "year": 2015}, {"title": "Music transcription modelling and composition using deep learning", "author": ["Sturm", "Santos", "Ben-Tal", "Korshunova"], "venue": "arXiv preprint:1604.08723,", "citeRegEx": "Sturm et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Sturm et al\\.", "year": 2016}, {"title": "Policy gradient methods for reinforcement learning with function approximation", "author": ["Sutton"], "venue": "In NIPS,", "citeRegEx": "Sutton,? \\Q1999\\E", "shortCiteRegEx": "Sutton", "year": 1999}, {"title": "SeqGAN: Sequence generative adversarial nets with policy gradient", "author": ["Yu", "Zhang", "Wang"], "venue": "arXiv preprint:1609.05473,", "citeRegEx": "Yu et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Yu et al\\.", "year": 2016}], "referenceMentions": [{"referenceID": 16, "context": "Similar to a Character RNN (Mikolov et al., 2010), these Note RNNs can be used to generate novel melodies by initializing them with a short sequence of notes, then repeatedly sampling from the model\u2019s output distribution generated to obtain the next note. While compositions generated in this way have recently garnered attention1, this type of model tends to suffer from common failure modes, such as excessively repeating notes, or producing sequences that lack a consistent theme or structure. Such sequences can appear wandering and random (see Graves (2013) for a text-based example).", "startOffset": 28, "endOffset": 563}, {"referenceID": 11, "context": ", 2013) and Deep Double Q-learning (Hasselt et al., 2015) are used to stablize and improve learning.", "startOffset": 35, "endOffset": 57}, {"referenceID": 23, "context": "(Eck & Schmidhuber, 2002), (Sturm et al., 2016)) has involved training an RNN to learn to predict the next note in a monophonic melody; we call this type of model a Note RNN.", "startOffset": 27, "endOffset": 47}, {"referenceID": 6, "context": "Often, the Note RNN is implemented using a Long ShortTerm Memory (LSTM) network (Gers et al., 2000).", "startOffset": 80, "endOffset": 99}, {"referenceID": 12, "context": "The technique described in Section 3 has a close connection with stochastic optimal control (SOC) (Todorov, 2006; Kappen et al., 2012; Rawlik et al., 2012).", "startOffset": 98, "endOffset": 155}, {"referenceID": 21, "context": "The technique described in Section 3 has a close connection with stochastic optimal control (SOC) (Todorov, 2006; Kappen et al., 2012; Rawlik et al., 2012).", "startOffset": 98, "endOffset": 155}, {"referenceID": 19, "context": "\u03a8-learning (Peters et al., 2010) and G-learning (Fox et al.", "startOffset": 11, "endOffset": 32}, {"referenceID": 10, "context": "While this computation is straight-forward for discrete action domains as here, extensions to continuous action domains require additional considerations such as normalizability of advantage function parametrizations (Gu et al., 2016).", "startOffset": 217, "endOffset": 234}, {"referenceID": 15, "context": "Since repetition has been shown to be key to emotional engagement with music (Livingstone et al., 2012), we also sought to train the model to repeat the same motif within a composition.", "startOffset": 77, "endOffset": 103}, {"referenceID": 23, "context": "Generative modeling of music with RNNs has been explored in a variety of contexts, including generating Celtic folk music (Sturm et al., 2016), or performing Blues improvisation (Eck & Schmidhuber, 2002).", "startOffset": 122, "endOffset": 142}, {"referenceID": 1, "context": "Other approaches have examined RNNs with richer expressivity, latent-variables for notes, or raw audio synthesis (Boulanger-Lewandowski et al., 2012; Gu et al., 2015; Chung et al., 2015).", "startOffset": 113, "endOffset": 186}, {"referenceID": 9, "context": "Other approaches have examined RNNs with richer expressivity, latent-variables for notes, or raw audio synthesis (Boulanger-Lewandowski et al., 2012; Gu et al., 2015; Chung et al., 2015).", "startOffset": 113, "endOffset": 186}, {"referenceID": 3, "context": "Other approaches have examined RNNs with richer expressivity, latent-variables for notes, or raw audio synthesis (Boulanger-Lewandowski et al., 2012; Gu et al., 2015; Chung et al., 2015).", "startOffset": 113, "endOffset": 186}, {"referenceID": 20, "context": "MIXER (Mixed Incremental Cross-Entropy Reinforce) (Ranzato et al., 2015) uses BLEU score as a reward signal to gradually introduce a RL loss to a text translation model.", "startOffset": 50, "endOffset": 72}, {"referenceID": 25, "context": "SeqGAN (Yu et al., 2016) applies RL to an RNN by using a discriminator network \u2014 similar to those used in Generative Adversarial Networks (GANs) (Goodfellow et al.", "startOffset": 7, "endOffset": 24}, {"referenceID": 21, "context": "Our work also relates to stochastic optimal control (SOC), in particular the two off-policy methods, \u03a8-learning (Rawlik et al., 2012) and G-learning (Fox et al.", "startOffset": 112, "endOffset": 133}, {"referenceID": 21, "context": "\u03a8-learning (Rawlik et al., 2012) restricts the prior policy to be the policy at the previous iteration and solves the original RL objective with conservative, KL-regularized policy updates, similar to conservative policy gradient methods (Kakade, 2001; Peters et al.", "startOffset": 11, "endOffset": 32}, {"referenceID": 19, "context": ", 2012) restricts the prior policy to be the policy at the previous iteration and solves the original RL objective with conservative, KL-regularized policy updates, similar to conservative policy gradient methods (Kakade, 2001; Peters et al., 2010; Schulman et al., 2015).", "startOffset": 213, "endOffset": 271}, {"referenceID": 22, "context": ", 2012) restricts the prior policy to be the policy at the previous iteration and solves the original RL objective with conservative, KL-regularized policy updates, similar to conservative policy gradient methods (Kakade, 2001; Peters et al., 2010; Schulman et al., 2015).", "startOffset": 213, "endOffset": 271}, {"referenceID": 2, "context": "Recent research has shown that the word2vec embeddings in popular language models trained on standard corpora consistently contain the same harmful biases with respect to race and gender that are revealed by implicit association tests on humans (Caliskan-Islam et al., 2016).", "startOffset": 245, "endOffset": 274}], "year": 2016, "abstractText": "Sequence models can be trained using supervised learning and a next-step prediction objective. This approach, however, suffers from known failure modes. For example, it is notoriously difficult to ensure multi-step generated sequences have coherent global structure. Motivated by the fact that reinforcement learning (RL) can be used to impose arbitrary properties on generated data by choosing appropriate reward functions, in this paper we propose a novel approach for sequence training which combines Maximum Likelihood (ML) and RL training. We refine a sequence predictor by optimizing for some imposed reward functions, while maintaining good predictive properties learned from data. We propose efficient ways to solve this by augmenting deep Q-learning with a cross-entropy reward and deriving novel off-policy methods for RNNs from stochastic optimal control (SOC). We explore the usefulness of our approach in the context of music generation. An LSTM is trained on a large corpus of songs to predict the next note in a musical sequence. This Note-RNN is then refined using RL, where the reward function is a combination of rewards based on rules of music theory, as well as the output of another trained Note-RNN. We show that by combining ML and RL, this RL Tuner method can not only produce more pleasing melodies, but that it can significantly reduce unwanted behaviors and failure modes of the RNN.", "creator": "LaTeX with hyperref package"}}}