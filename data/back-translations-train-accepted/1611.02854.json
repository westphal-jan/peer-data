{"id": "1611.02854", "review": {"conference": "iclr", "VERSION": "v1", "DATE_OF_SUBMISSION": "9-Nov-2016", "title": "Lie-Access Neural Turing Machines", "abstract": "Recent work has demonstrated the effectiveness of employing explicit external memory structures in conjunction with deep neural models for algorithmic learning (Graves et al. 2014; Weston et al. 2014). These models utilize differentiable versions of traditional discrete memory-access structures (random access, stacks, tapes) to provide the variable-length storage necessary for computational tasks. In this work, we propose an alternative model, Lie-access memory, that is explicitly designed for the neural setting. In this paradigm, memory is accessed using a continuous head in a key-space manifold. The head is moved via Lie group actions, such as shifts or rotations, generated by a controller, and soft memory access is performed by considering the distance to keys associated with each memory. We argue that Lie groups provide a natural generalization of discrete memory structures, such as Turing machines, as they provide inverse and identity operators while maintain differentiability. To experiment with this approach, we implement several simplified Lie-access neural Turing machine (LANTM) with different Lie groups. We find that this approach is able to perform well on a range of algorithmic tasks.", "histories": [["v1", "Wed, 9 Nov 2016 08:51:54 GMT  (870kb,D)", "http://arxiv.org/abs/1611.02854v1", "Submitted to ICLR. Rewrite and improvement ofthis https URL"], ["v2", "Sun, 5 Mar 2017 21:03:22 GMT  (1046kb,D)", "http://arxiv.org/abs/1611.02854v2", "Published at ICLR. Rewrite and improvement ofarXiv:1602.08671"]], "COMMENTS": "Submitted to ICLR. Rewrite and improvement ofthis https URL", "reviews": [], "SUBJECTS": "cs.NE cs.LG", "authors": ["greg yang", "alexander m rush"], "accepted": true, "id": "1611.02854"}, "pdf": {"name": "1611.02854.pdf", "metadata": {"source": "CRF", "title": "Lie-Access Neural Turing Machines", "authors": ["Greg Yang", "Alexander M. Rush"], "emails": [], "sections": [{"heading": "1 Introduction", "text": "In fact, most of them will be able to move in a direction in which they are able to move, in which they are able to move."}, {"heading": "2 Background: Recurrent Neural Networks with Memory", "text": "We focus in particular on the case of an RNN controller of an abstract storage network (BNT). Formally, an RNN is a differentiable function RNN: X \u00b7 H \u2192 H, where X is an arbitrary input space and H is the space of internal states. At the input (x (1),., x (T))), x (1), x (2), x (2), x (2), x (2), x (3), x (3), x (3), x (3), x (3), x (4), x (4), x (4), x (4), x (4), x (4), x (4), x (4), x (4), x (4), x (5), x (5), x (5), x (5)."}, {"heading": "3 Lie Groups for Memory", "text": "s look at the classic discrete Turing machine and the movement of its head over a tape. Specifically, the head can move any number of steps to the left or right between reading operations. Moving a + b steps and then c steps eventually puts the head in the same position as moving a step and then b + c steps - i.e. the head movement is associative. Furthermore, the machine should be able to reverse a head shift, for example in a simulation algorithm for several time points. These movements correspond directly to group actions: the possible head movements should be associative and contain inverse and identical elements. This group affects the set of possible head locations. In a TM, the set of Z-weighted head movements appears to be differentiable."}, {"heading": "4 Lie-Access Neural Turing Machines", "text": "These characteristics motivate us to propose Lie access as an alternative formalism to popular soft versions of neural storage systems, such as probabilistic tapes, which surprisingly do not satisfy reversibility and often do not have identity1. Our Lie access memory will consist of a set of 1The Markov Kernel Convolutional Soft Head Shift Mechanism proposed in Graves et al. (2014) and explained in Section 2. In fact, the authors reported problems where the soft head loses \"sharpness\" over time, which they treated with \"sharpness coefficients.\" In the following points in a varied K., we replace the discrete head with a continuous head, q-K. The head moves based on a series of lie group actions generated by the controller. To read memories, we rely on a distance measure in this space, d: K \u00d7 K \u2192 R value 0.2 Together, these characteristics describe a general class of possible neural function of the memory architecture, or Less is a reserived function of memory architecture."}, {"heading": "4.1 Addressing Procedure", "text": "A (h) \u00b7 q = (\u03b1, \u03b2) + (x, y) = (x + \u03b1, y + \u03b2). \u2022 The rotation group SO (3) acting on sphere S2 = {v, \u03b2 = 1}. Each rotation can be described by its axis and angle \u03b8. An action \u00b7 q is exactly the correct rotation of point q and is given by Rodrigues' rotation formula A (h) \u00b7 q = (\u0443, \u03b8) \u00b7 q = q cos \u03b8 + (k \u00b7 q) sin \u03b8 + k < k, q > (1 \u2212 cos \u03b8)."}, {"heading": "4.2 Reading Memories", "text": "Let us remember that memories are stored in a manifold belt mannian, each with a key, ki, memory vector, vi, q and strength, si. Memories are read using a soft weighting scheme based on their key address. Weighting is defined work, Graves and al. (2016) use a temporal memory link matrix for actions. They also explicitly note that \"operation Lw gently shifts the focus forward to the written places...\" whereas L > w shifts the focus backwards, \"but does not force this as a true reversal. They also do not include identity and note\" self-links are excluded (the diagonal of the link matrix is always 0). \"2 This metric should fulfill a compatibility relationship with the lie group action if points x, y-X are shifted simultaneously by the same lie group action v, their distance should remain the same (a possible mathematical formalization is group X, which should be group X and sub-atie)."}, {"heading": "4.3 Writing Memories", "text": "The writing process is similar to reading. The LANTM maintains a separate writing head q (w), which moves analogously to the reading head, i.e. with the action function a (w) and the updated value q (w). Each time RW is called, a new memory is automatically appended with k = q \"(w). The corresponding v and s are generated by the MLP's v (h), s (h), Rm and s (0, 1], using h as input. After writing, the new memory circuit is \u03a3\": = \u03a3, v (w), v (h), s (h)}. No explicit deletion mechanism is provided, but to delete a memory (k, v, s), the controller can simply write (k, \u2212 v, s)."}, {"heading": "4.4 Interpolations", "text": "In practice, we can combine this relative addressing method with direct random access by output an interpolation. Write t (h) for the interpolation gate (0, 1) and q (h) for our proposed level of random access. For keyspace multiplicities K like Rn, there is a well-defined linear interpolation between two points, so that we can put q \u00b2: = A \u00b7 (t \u00b7 q + (1 \u2212 t) \u00b7 q \u00b2), where we project the implicit dependence on h. For other multiplicities such as the spheres Sn, which have well-behaved projection functions: Rn \u2192 Sn, we can simply project the linear interpolation on the sphere: q \u2032: = A \u00b7 \u03c0 (1 \u2212 t) \u00b7 q \u0432: For other multiplicities such as the spheres Sn, which have well-behaved projection functions: Rn \u2192 Sn \u2192 Sq, we can simply project the linear interpolation on the sphere: q \u2032: = A \u00b7 \u03c0 (1 \u2212 t)."}, {"heading": "5 Experiments", "text": "This year, it is as far as ever in the history of the city, where it is as far as never before in the history of the city."}, {"heading": "6 Conclusion", "text": "This paper presents lie-access memory as an alternative paradigm of neural memory access and examines various implementations of this approach. LANTMs follow similar axioms to discrete Turing machines, while offering differentiation. Experiments show that simple models can learn algorithmic tasks. Internally, of course, these models learn the equivalence of standard data structures such as stack and cyclic lists. In future work, we hope to experiment with more groups and scale these methods to more difficult puzzles."}, {"heading": "A Experimental details", "text": "We obtain our results by performing a grid search using the grid specified in Table A.1 and also the seeds 1 to 3 and taking the best values. We include the standard of LANTM head shifts by 1, while trying to both limit and not limit the angle of rotation in our grid for SLANTM. We initialize the Lie access models to favor Lie access over random access through the interpolation mechanism discussed in Section 4.4. The read mechanism of the RAM model is discussed as in Section 2, and the writing is done by appending new (k, v, s) tuples to the memory. The only addition to this model in RAM / band is that the left and right buttons are now calculated by displaced folding with the read weights: kL: = \u2211 i wi + 1kikR: = \u0445 i wi \u2212 1kRand, these buttons and kR (along with the random selection of the controller output) are available in the controller."}], "references": [{"title": "Neural Turing Machines", "author": ["Alex Graves", "Greg Wayne", "Ivo Danihelka"], "venue": "[cs],", "citeRegEx": "Graves et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Graves et al\\.", "year": 2014}, {"title": "Hybrid computing using a neural network with dynamic external", "author": ["Alex Graves", "Greg Wayne", "Malcolm Reynolds", "Tim Harley", "Ivo Danihelka", "Agnieszka GrabskaBarwi\u0144ska", "Sergio G\u00f3mez Colmenarejo", "Edward Grefenstette", "Tiago Ramalho", "John Agapiou"], "venue": "memory. Nature,", "citeRegEx": "Graves et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Graves et al\\.", "year": 2016}, {"title": "Learning to Transduce with Unbounded Memory", "author": ["Edward Grefenstette", "Karl Moritz Hermann", "Mustafa Suleyman", "Phil Blunsom"], "venue": "[cs],", "citeRegEx": "Grefenstette et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Grefenstette et al\\.", "year": 2015}, {"title": "Long Short-Term Memory", "author": ["Sepp Hochreiter", "Jrgen Schmidhuber"], "venue": "Neural Comput.,", "citeRegEx": "Hochreiter and Schmidhuber.,? \\Q1997\\E", "shortCiteRegEx": "Hochreiter and Schmidhuber.", "year": 1997}, {"title": "Inferring Algorithmic Patterns with Stack-Augmented Recurrent Nets", "author": ["Armand Joulin", "Tomas Mikolov"], "venue": "[cs],", "citeRegEx": "Joulin and Mikolov.,? \\Q2015\\E", "shortCiteRegEx": "Joulin and Mikolov.", "year": 2015}, {"title": "Neural GPUs Learn Algorithms", "author": ["ukasz Kaiser", "Ilya Sutskever"], "venue": "[cs],", "citeRegEx": "Kaiser and Sutskever.,? \\Q2015\\E", "shortCiteRegEx": "Kaiser and Sutskever.", "year": 2015}, {"title": "Grid Long Short-Term Memory", "author": ["Nal Kalchbrenner", "Ivo Danihelka", "Alex Graves"], "venue": "[cs],", "citeRegEx": "Kalchbrenner et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Kalchbrenner et al\\.", "year": 2015}, {"title": "Ask Me Anything: Dynamic Memory Networks for Natural Language Processing", "author": ["Ankit Kumar", "Ozan Irsoy", "Peter Ondruska", "Mohit Iyyer", "James Bradbury", "Ishaan Gulrajani", "Richard Socher"], "venue": "[cs],", "citeRegEx": "Kumar et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Kumar et al\\.", "year": 2015}, {"title": "Neural Random-Access Machines", "author": ["Karol Kurach", "Marcin Andrychowicz", "Ilya Sutskever"], "venue": "[cs],", "citeRegEx": "Kurach et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Kurach et al\\.", "year": 2015}, {"title": "Introduction to Smooth Manifolds", "author": ["John Lee"], "venue": "Number 218 in Graduate Texts in Mathematics. Springer, 2 edition,", "citeRegEx": "Lee.,? \\Q2012\\E", "shortCiteRegEx": "Lee.", "year": 2012}, {"title": "Interpolation in Lie Groups", "author": ["A. Marthinsen"], "venue": "SIAM Journal on Numerical Analysis,", "citeRegEx": "Marthinsen.,? \\Q1999\\E", "shortCiteRegEx": "Marthinsen.", "year": 1999}, {"title": "Key-value memory networks for directly reading", "author": ["Alexander Miller", "Adam Fisch", "Jesse Dodge", "Amir-Hossein Karimi", "Antoine Bordes", "Jason Weston"], "venue": "documents. CoRR,", "citeRegEx": "Miller et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Miller et al\\.", "year": 2016}, {"title": "Interpolation in special orthogonal groups", "author": ["Tatiana Shingel"], "venue": "IMA Journal of Numerical Analysis,", "citeRegEx": "Shingel.,? \\Q2009\\E", "shortCiteRegEx": "Shingel.", "year": 2009}, {"title": "End-To-End Memory Networks", "author": ["Sainbayar Sukhbaatar", "Arthur Szlam", "Jason Weston", "Rob Fergus"], "venue": "[cs],", "citeRegEx": "Sukhbaatar et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Sukhbaatar et al\\.", "year": 2015}, {"title": "Sequence to Sequence Learning with Neural Networks", "author": ["Ilya Sutskever", "Oriol Vinyals", "Quoc V. Le"], "venue": "[cs],", "citeRegEx": "Sutskever et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Sutskever et al\\.", "year": 2014}, {"title": "Backpropagation through time: what it does and how to do it", "author": ["Paul J. Werbos"], "venue": "Proceedings of the IEEE,", "citeRegEx": "Werbos.,? \\Q1990\\E", "shortCiteRegEx": "Werbos.", "year": 1990}, {"title": "Memory Networks. arXiv:1410.3916 [cs, stat", "author": ["Jason Weston", "Sumit Chopra", "Antoine Bordes"], "venue": "URL http://arxiv.org/abs/1410.3916", "citeRegEx": "Weston et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Weston et al\\.", "year": 2014}, {"title": "Reinforcement Learning Neural Turing Machines - Revised", "author": ["Wojciech Zaremba", "Ilya Sutskever"], "venue": "[cs],", "citeRegEx": "Zaremba and Sutskever.,? \\Q2015\\E", "shortCiteRegEx": "Zaremba and Sutskever.", "year": 2015}], "referenceMentions": [{"referenceID": 0, "context": "Recent work has demonstrated the effectiveness of employing explicit external memory structures in conjunction with deep neural models for algorithmic learning (Graves et al., 2014; Weston et al., 2014).", "startOffset": 160, "endOffset": 202}, {"referenceID": 16, "context": "Recent work has demonstrated the effectiveness of employing explicit external memory structures in conjunction with deep neural models for algorithmic learning (Graves et al., 2014; Weston et al., 2014).", "startOffset": 160, "endOffset": 202}, {"referenceID": 0, "context": "Pioneering work on neural Turing machines (NTMs) (Graves et al., 2014; 2016) and memory networks (MemNNs) (Weston et al.", "startOffset": 49, "endOffset": 76}, {"referenceID": 16, "context": ", 2014; 2016) and memory networks (MemNNs) (Weston et al., 2014) has demonstrated that explicit external memories can be effectively utilized within deep neural networks, without sacrificing the end-to-end nature of these models.", "startOffset": 43, "endOffset": 64}, {"referenceID": 16, "context": "Early work has explored tasks such as question answering (Weston et al., 2014; Sukhbaatar et al., 2015; Kumar et al., 2015), algorithm learning (Graves et al.", "startOffset": 57, "endOffset": 123}, {"referenceID": 13, "context": "Early work has explored tasks such as question answering (Weston et al., 2014; Sukhbaatar et al., 2015; Kumar et al., 2015), algorithm learning (Graves et al.", "startOffset": 57, "endOffset": 123}, {"referenceID": 7, "context": "Early work has explored tasks such as question answering (Weston et al., 2014; Sukhbaatar et al., 2015; Kumar et al., 2015), algorithm learning (Graves et al.", "startOffset": 57, "endOffset": 123}, {"referenceID": 0, "context": ", 2015), algorithm learning (Graves et al., 2014; Kalchbrenner et al., 2015; Kaiser & Sutskever, 2015; Kurach et al., 2015; Zaremba & Sutskever, 2015; Grefenstette et al., 2015; Joulin & Mikolov, 2015), machine translation (Kalchbrenner et al.", "startOffset": 28, "endOffset": 201}, {"referenceID": 6, "context": ", 2015), algorithm learning (Graves et al., 2014; Kalchbrenner et al., 2015; Kaiser & Sutskever, 2015; Kurach et al., 2015; Zaremba & Sutskever, 2015; Grefenstette et al., 2015; Joulin & Mikolov, 2015), machine translation (Kalchbrenner et al.", "startOffset": 28, "endOffset": 201}, {"referenceID": 8, "context": ", 2015), algorithm learning (Graves et al., 2014; Kalchbrenner et al., 2015; Kaiser & Sutskever, 2015; Kurach et al., 2015; Zaremba & Sutskever, 2015; Grefenstette et al., 2015; Joulin & Mikolov, 2015), machine translation (Kalchbrenner et al.", "startOffset": 28, "endOffset": 201}, {"referenceID": 2, "context": ", 2015), algorithm learning (Graves et al., 2014; Kalchbrenner et al., 2015; Kaiser & Sutskever, 2015; Kurach et al., 2015; Zaremba & Sutskever, 2015; Grefenstette et al., 2015; Joulin & Mikolov, 2015), machine translation (Kalchbrenner et al.", "startOffset": 28, "endOffset": 201}, {"referenceID": 6, "context": ", 2015; Joulin & Mikolov, 2015), machine translation (Kalchbrenner et al., 2015), and others.", "startOffset": 53, "endOffset": 80}, {"referenceID": 0, "context": "Pioneering work on neural Turing machines (NTMs) (Graves et al., 2014; 2016) and memory networks (MemNNs) (Weston et al., 2014) has demonstrated that explicit external memories can be effectively utilized within deep neural networks, without sacrificing the end-to-end nature of these models. Recently researchers have studied many variations of external neural memories. Early work has explored tasks such as question answering (Weston et al., 2014; Sukhbaatar et al., 2015; Kumar et al., 2015), algorithm learning (Graves et al., 2014; Kalchbrenner et al., 2015; Kaiser & Sutskever, 2015; Kurach et al., 2015; Zaremba & Sutskever, 2015; Grefenstette et al., 2015; Joulin & Mikolov, 2015), machine translation (Kalchbrenner et al., 2015), and others. This work has the potential to extend deep networks beyond the limitations of fixed-length encodings such as standard recurrent neural networks (RNNs). A shared theme in many of these works (and earlier exploration of neural memory) is to re-frame traditional memory access paradigms to be continuous and possibly differentiable to allow for simple backpropagation. In MemNNs, traditional random-access memory is replaced with a ranking approach that finds the most likely memory. In the work of Grefenstette et al. (2015), classical stack-based memory is replaced by a soft differentiable stack data-structure.", "startOffset": 50, "endOffset": 1275}, {"referenceID": 15, "context": "RNNs can be trained end-to-end by backpropagation-through-time (BPTT) (Werbos, 1990).", "startOffset": 70, "endOffset": 84}, {"referenceID": 0, "context": "In models with external memories, the RNN can serve as the controller (Graves et al., 2014; Grefenstette et al., 2015; Zaremba & Sutskever, 2015).", "startOffset": 70, "endOffset": 145}, {"referenceID": 2, "context": "In models with external memories, the RNN can serve as the controller (Graves et al., 2014; Grefenstette et al., 2015; Zaremba & Sutskever, 2015).", "startOffset": 70, "endOffset": 145}, {"referenceID": 11, "context": "This description is similar to the work of Miller et al. (2016). The controller hidden state is used to output a random-access pointer q\u0303(h) that determines a weighting of memory vectors via dot products with the corresponding keys.", "startOffset": 43, "endOffset": 64}, {"referenceID": 0, "context": "Our Lie-access memory will consist of a set of The Markov kernel convolutional soft head shift mechanism proposed in Graves et al. (2014) and explained in section 2 does not in general have inverses.", "startOffset": 117, "endOffset": 138}, {"referenceID": 0, "context": "work, Graves et al. (2016) utilize a temporal memory link matrix for actions.", "startOffset": 6, "endOffset": 27}, {"referenceID": 14, "context": "Our experiments utilize an LSTM controller in a version of the encoder-decoder setup (Sutskever et al., 2014), i.", "startOffset": 85, "endOffset": 109}, {"referenceID": 2, "context": "The tasks copy, reverse, and bigram flip tasks are based on Grefenstette et al. (2015); tasks double and interleaved add are designed in a similar vein.", "startOffset": 60, "endOffset": 87}, {"referenceID": 2, "context": "The tasks copy, reverse, and bigram flip tasks are based on Grefenstette et al. (2015); tasks double and interleaved add are designed in a similar vein. Additionally we also include two harder tasks repeat copy and priority sort. In repeat copy each model must repeat a sequence of length 20, N times. In priority sort, each item of the input sequence is given a priority, and the model must output them in priority order. The models are trained with random 32K examples and tested on 3.2K random examples. Prediction is performed via argmax at each step; training is done by minimizing negative log likelihood with RMSProp. For each task, we train each model with the same number of samples. In particular it is not the case that we train till convergence as was done in e.g. Grefenstette et al. (2015). To evaluate the performance of the models, we compute the fraction of characters correctly predicted and the fraction of all answers completely correctly predicted, respectively called fine/ coarse following Grefenstette et al.", "startOffset": 60, "endOffset": 804}, {"referenceID": 2, "context": "The tasks copy, reverse, and bigram flip tasks are based on Grefenstette et al. (2015); tasks double and interleaved add are designed in a similar vein. Additionally we also include two harder tasks repeat copy and priority sort. In repeat copy each model must repeat a sequence of length 20, N times. In priority sort, each item of the input sequence is given a priority, and the model must output them in priority order. The models are trained with random 32K examples and tested on 3.2K random examples. Prediction is performed via argmax at each step; training is done by minimizing negative log likelihood with RMSProp. For each task, we train each model with the same number of samples. In particular it is not the case that we train till convergence as was done in e.g. Grefenstette et al. (2015). To evaluate the performance of the models, we compute the fraction of characters correctly predicted and the fraction of all answers completely correctly predicted, respectively called fine/ coarse following Grefenstette et al. (2015). We score the models on randomly generated in-sample 1x data and out-of-sample 2x data, with sequence lengths 2k (or repeat number 2N in the case of repeat copy) to test the generalization of the system.", "startOffset": 60, "endOffset": 1040}, {"referenceID": 11, "context": "Interested readers are referred to Shingel (2009) and Marthinsen (1999).", "startOffset": 35, "endOffset": 50}, {"referenceID": 10, "context": "Interested readers are referred to Shingel (2009) and Marthinsen (1999). 5 Note that the read weight calculation of a SLANTM with softmax is essentially the same as the RAM model: For head q\u2032, exp(\u2212d(q\u2032, ki)/T ) = exp(\u2212\u2016q\u2212ki\u2016/T ) = exp(\u2212(2\u22122\u3008q\u2032, ki\u3009)/T ), where the last equality comes from \u2016q\u2032\u2016 = \u2016ki\u2016 = 1.", "startOffset": 54, "endOffset": 72}], "year": 2017, "abstractText": "Recent work has demonstrated the effectiveness of employing explicit external memory structures in conjunction with deep neural models for algorithmic learning (Graves et al., 2014; Weston et al., 2014). These models utilize differentiable versions of traditional discrete memory-access structures (random access, stacks, tapes) to provide the variable-length storage necessary for computational tasks. In this work, we propose an alternative model, Lie-access memory, that is explicitly designed for the neural setting. In this paradigm, memory is accessed using a continuous head in a key-space manifold. The head is moved via Lie group actions, such as shifts or rotations, generated by a controller, and soft memory access is performed by considering the distance to keys associated with each memory. We argue that Lie groups provide a natural generalization of discrete memory structures, such as Turing machines, as they provide inverse and identity operators while maintain differentiability. To experiment with this approach, we implement several simplified Lie-access neural Turing machine (LANTM) with different Lie groups. We find that this approach is able to perform well on a range of algorithmic tasks.", "creator": "LaTeX with hyperref package"}}}