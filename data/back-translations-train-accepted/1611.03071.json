{"id": "1611.03071", "review": {"conference": "ICML", "VERSION": "v1", "DATE_OF_SUBMISSION": "9-Nov-2016", "title": "Fairness in Reinforcement Learning", "abstract": "We initiate the study of fair learning in Markovian settings, where the actions of a learning algorithm may affect its environment and future rewards. Working in the model of reinforcement learning, we define a fairness constraint requiring that an algorithm never prefers one action over another if the long-term (discounted) reward of choosing the latter action is higher.", "histories": [["v1", "Wed, 9 Nov 2016 20:19:45 GMT  (415kb,D)", "http://arxiv.org/abs/1611.03071v1", null], ["v2", "Thu, 17 Nov 2016 17:46:00 GMT  (411kb,D)", "http://arxiv.org/abs/1611.03071v2", null], ["v3", "Wed, 1 Mar 2017 16:35:53 GMT  (409kb,D)", "http://arxiv.org/abs/1611.03071v3", null], ["v4", "Sun, 6 Aug 2017 00:12:49 GMT  (426kb,D)", "http://arxiv.org/abs/1611.03071v4", "The short version of this paper appears in the proceedings of ICML-17"]], "reviews": [], "SUBJECTS": "cs.LG", "authors": ["shahin jabbari", "matthew joseph", "michael kearns", "jamie morgenstern", "aaron roth"], "accepted": true, "id": "1611.03071"}, "pdf": {"name": "1611.03071.pdf", "metadata": {"source": "CRF", "title": "Fair Learning in Markovian Environments", "authors": ["Shahin Jabbari", "Matthew Joseph", "Michael Kearns", "Jamie Morgenstern", "Aaron Roth"], "emails": ["aaroth}@cis.upenn.edu"], "sections": [{"heading": null, "text": "Our first result is negative: despite the fact that fairness is consistent with optimal policy, any learning algorithm that satisfies fairness must go through exponentially many rounds in the number of states to achieve a non-trivial approximation of optimal policy. Our main result is a polynomial age algorithm that is demonstrably fair under an approximate concept of fairness, creating an exponential gap between exact and approximate fairness."}, {"heading": "1 Introduction", "text": "The increasing use of machine learning for automated decision-making has raised concerns about the potential for injustice and discrimination in learning algorithms and models, and in contexts as diverse as hiring [23], lending [4], policing [25], and law enforcement [3], which require only empirical rewards, these concerns are not merely hypothetical [2, 27].This paper initiates the study of fair learning algorithms in Markovian environments, where algorithm decisions can influence the state of the world and future rewards. Preliminary work on fairness in machine learning has focused on short-sighted attitudes where such influence is lacking, such as in i.i.i.d. or non-regrettable models (see e.g. [6, 7, 14]). Short-sighted notions of fairness may be undesirable in the contexts outlined above, in which policies influence actions and rewards."}, {"heading": "1.1 Our Contributions", "text": "The contributions in this paper can be divided into three parts. First, in Section 4, we propose several definitions of fairness for learning in MDPs. The first (formalized in Definition 1) requires that an algorithm most likely never prefers a worse action to a better one, where the quality of an action is measured in terms of its potential long-term discounted reward. We then loosen this definition in two ways. The first loosening, which we call approximate fairness of choice, requires that an algorithm never selects a worse action, the likelihood of which is much higher than better action. The second loosening of fairness, approximate fairness of action, requires that an algorithm never prefers an action of much lower quality than that of a better action. Second, in Section 5, we analyze the consequences of these definitions and offer a lower limit on the time required for fair learning."}, {"heading": "1.2 Our Techniques", "text": "The lower limit follows from the construction of two similar MDPs and the argument that fair and approximate choice fair algorithms must behave identically until they obtain an observation that distinguishes the two from each other, requiring an exponential number of steps in the size of government space with constant probability. We then investigate another loosening of fairness, called approach-action fairness, which allows an algorithm to favor slightly worse actions over slightly better ones. We adapt E3 [19], a well-known algorithm for MDP learning, to satisfy approximate fairness of action. E3 transitions between exploration and exploitation are not sufficiently explored, E3 takes the least-explored actions in that state."}, {"heading": "2 Related Work", "text": "There is a huge literature on enhancing learning, and the most relevant parts focus on the construction of learning algorithms with verifiable performance guarantees. E3 [19] was the first learning algorithm with a polynomial learning rate, and a subsequent literature gave a succession of improved boundaries (see Szita and Szepesva) and references within. One line of work aims to give algorithms for batch classification that achieve concepts of group justice, and in particular statistical parity (see eg.), 15, 17, 18, 24, 29."}, {"heading": "3 Preliminaries", "text": "In this paper, our starting point is to support the distribution of rewards for all."}, {"heading": "4 Notions of Fairness", "text": "We start by giving Joseph et al. [14] \"s definition of learning in a bandit contextual environment: namely, definition 1, when the quantity of quantity (s, a) values are replaced by the expected reward for choosing a particular arm a. [14] Defining the quality of an action that is less likely than a lower one-step reward than a lower one-step reward for selecting a particular arm a (s, a). This definition would require that an algorithm in states never plays with a higher probability than a higher one-step reward than a lower one-step reward.\" This naive translation does not adequately capture the structural differences between the bandit and MDP attitudes: in the bandit environment, the current rewards are independent of past decisions, whereas in the MDPs they are not."}, {"heading": "5 Lower Bounds", "text": "In this section, we show a stark separation between the performance of algorithms to enhance learning with and without fairness. First, we show that neither fair nor approximate choice fair algorithms achieve approximate optimality (see definition 5), unless the number of time steps T is at least approximately equal (kn), exponentially in the size of government space. We then show that any approximate action fair algorithms require a number of time steps T that can achieve at least approximate optimality. We start by demonstrating a lower limit for fair algorithms. Theorem 4: For us < 1 / 4, \u03b3 > 0.5, and < 1 / 8, no fair algorithms can be optimal in T = O (kn) steps. 33We have no optimization for the upper limit constants in the statement of Theorem 4 as well as Theorem 5. Hence, these values are chosen only for convenience 2."}, {"heading": "6 A Provably Fair and Efficient Learning Algorithm", "text": "Theorem 7: Given > 0, \u03b1 > 0, \u03b4 (0, 1 / 2) and \u03b3 [0, 1) as input factors, Fair-E3 is an alpha-action fair algorithm that achieves the -optimality according to T = O (n5T). The runtime of Fair-E3 is polynomial in all MDP parameters except 1 / (1 \u2212 3). Furthermore, Theorem 6 implies that no approximate Fair-action algorithms can have a runtime that is polynomial in 1 / (1 \u2212 3). The rest of this section is structured as follows."}, {"heading": "6.5 Relaxations and Discussion", "text": "During sections 6.3 and 6.4, we assumed that T \u0445, the -mixing time of the optimal policy \u03c0, was known (see assumption 2). Although Fair-E3 uses the knowledge of T \u0445 to decide whether we should pursue the exploration or exploitation policy, Lemma 9 continues even without this assumption. Note that Fair-E3 is parameterized by T \u0445 and for each input T \u0445 runs in time poly (T \u0445). If T \u0445 is unknown, we simply have to execute the new algorithm in enough steps so that the possibly low V \u0445 M values of the visited states in the early stages will still be poly (T \u0445). Similar to the analysis of Fair-E 3, if T \u0445 is known, we must execute the new algorithm for sufficiently many steps so that the possibly low V \u0445 M values of the visited states in the early stages are unsolvable in the approximately optimal V \u0445 M values of the visited states, unsolvable in an insoluble manner, 1 Finally, our work leaves several questions unsolvable in an unsolvable manner, but in an insolvable manner."}, {"heading": "A Observations on Optimality and Fairness", "text": "Observation 1. For each MDP M there is an optimal policy. Then there is an optimal policy. There is an optimal policy. Then there is an optimal policy. Then there is a fair and optimal action. Then there is a fair and optimal action. Let's say that a fair action is not nearly fair. Let's say that a fair action is not nearly fair. Let's say that a fair action is not nearly fair. Let's say that a fair action is not nearly fair. Let's say that an optimal action is fair. Let's say that an optimal action is fair. Let's say that an optimal action is fair. Let's say that an optimal action is fair. Let's say that an optimal action is fair. Let's say that an optimal action is fair."}, {"heading": "B Omitted Proofs", "text": "B.1 Neglected Evidence for Section 3Proof of Lemma 1. Let us specify the distribution of \u03c0 to the states of M after the following \u03c0 steps starting from s. \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 Proof of Lemma 1. Let us specify the distribution of \u03c0 to the states of M (si) = n \u2211 i = 1 (\u00b5\u03c0 (si) \u2212 p (si) V \u03c0M (si) \u2264 \u2212 n \u043c (si). \u2212 Producing (si) \u2212 p (si) \u2212 p (si). The last inequality is due to the following observations: (i) V \u03c0M (si). \u2212 p (si). \u2212 p) As rewards are in [0, 1] and (ii)."}], "references": [{"title": "Auditing black-box models by obscuring features", "author": ["Philip Adler", "Casey Falk", "Sorelle Friedler", "Gabriel Rybeck", "Carlos Scheidegger", "Brandon Smith", "Suresh Venkatasubramanian"], "venue": null, "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2016}, {"title": "The new science of sentencing", "author": ["Anna Barry-Jester", "Ben Casselman", "Dana Goldstein"], "venue": "The Marshall Project, August", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2015}, {"title": "Artificial intolerance. MIT Technology Review, March 28 2016", "author": ["Nanette Byrnes"], "venue": "URL https: //www.technologyreview.com/s/600996/artificial-intolerance/", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2016}, {"title": "Three naive Bayes approaches for discrimination-free classification", "author": ["Toon Calders", "Sicco Verwer"], "venue": "Data Mining and Knowledge Discovery,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2010}, {"title": "Fairness through awareness", "author": ["Cynthia Dwork", "Moritz Hardt", "Toniann Pitassi", "Omer Reingold", "Richard Zemel"], "venue": "In Proceedings of the 3rd Innovations in Theoretical Computer Science,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2012}, {"title": "Certifying and removing disparate impact", "author": ["Michael Feldman", "Sorelle Friedler", "John Moeller", "Carlos Scheidegger", "Suresh Venkatasubramanian"], "venue": "In Proceedings of the 21th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2015}, {"title": "A confidence-based approach for balancing fairness and accuracy", "author": ["Benjamin Fish", "Jeremy Kun", "\u00c1d\u00e1m D\u00e1niel Lelkes"], "venue": "In Proceedings of the 2016 SIAM International Conference on Data Mining,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2016}, {"title": "A methodology for direct and indirect discrimination prevention in data mining", "author": ["Sara Hajian", "Josep Domingo-Ferrer"], "venue": "IEEE Transactions on Knowledge and Data Engineering,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2013}, {"title": "Martinez-Balleste. Rule protection for indirect discrimination prevention in data mining", "author": ["Sara Hajian", "Josep Domingo-Ferrer", "Antoni"], "venue": "In Modeling Decision for Artificial Intelligence,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2011}, {"title": "Equality of opportunity in supervised learning", "author": ["Moritz Hardt", "Eric Price", "Nathan Srebro"], "venue": "In Proceedings of the 30th Annual Conference on Neural Information Processing Systems,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2016}, {"title": "Rawlsian fairness for machine learning", "author": ["Matthew Joseph", "Michael Kearns", "Jamie Morgenstern", "Seth Neel", "Aaron Roth"], "venue": null, "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2016}, {"title": "Fairness in learning: Classic and contextual bandits", "author": ["Matthew Joseph", "Michael Kearns", "Jamie Morgenstern", "Aaron Roth"], "venue": "In Proceedings of the 30th Annual Conference on Neural Information Processing Systems,", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2016}, {"title": "Data preprocessing techniques for classification without discrimination", "author": ["Faisal Kamiran", "Toon Calders"], "venue": "Knowledge and Information Systems,", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2011}, {"title": "Discrimination aware decision tree learning", "author": ["Faisal Kamiran", "Toon Calders", "Mykola Pechenizkiy"], "venue": "In Proceedings of the 10th IEEE International Conference on Data Mining,", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2010}, {"title": "Decision theory for discrimination-aware classification", "author": ["Faisal Kamiran", "Asim Karim", "Xiangliang Zhang"], "venue": "In Proceedings of the 12th IEEE International Conference on Data Mining,", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2012}, {"title": "Fairness-aware classifier with prejudice remover regularizer", "author": ["Toshihiro Kamishima", "Shotaro Akaho", "Hideki Asoh", "Jun Sakuma"], "venue": "In Proceedings of the European Conference on Machine Learning and Knowledge Discovery in Databases,", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2012}, {"title": "Near-optimal reinforcement learning in polynomial time", "author": ["Michael Kearns", "Satinder Singh"], "venue": "Machine Learning,", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2002}, {"title": "Approximate planning in large POMDPs via reusable trajectories", "author": ["Michael Kearns", "Yishay Mansour", "Andrew Ng"], "venue": "In Proceedings of the 13th Annual Conference on Neural Information Processing Systems,", "citeRegEx": "20", "shortCiteRegEx": "20", "year": 2000}, {"title": "Inherent trade-offs in the fair determination of risk", "author": ["Jon Kleinberg", "Sendhil Mullainathan", "Manish Raghavan"], "venue": "scores. CoRR,", "citeRegEx": "21", "shortCiteRegEx": "21", "year": 2016}, {"title": "k-NN as an implementation of situation testing for discrimination discovery and prevention", "author": ["Binh Thanh Luong", "Salvatore Ruggieri", "Franco Turini"], "venue": "In Proceedings of the 17th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining,", "citeRegEx": "22", "shortCiteRegEx": "22", "year": 2011}, {"title": "Can an algorithm hire better than a human?  The New York", "author": ["Clair Miller"], "venue": "Times, June", "citeRegEx": "23", "shortCiteRegEx": "23", "year": 2015}, {"title": "Discrimination-aware data mining", "author": ["Dino Pedreshi", "Salvatore Ruggieri", "Franco Turini"], "venue": "In Proceedings of the 14th ACM SIGKDD international conference on Knowledge discovery and data mining,", "citeRegEx": "24", "shortCiteRegEx": "24", "year": 2008}, {"title": "Predictive policing using machine learning to detect patterns of crime", "author": ["Cynthia Rudin"], "venue": "URL http://www.wired.com/insights/2013/08/ predictive-policing-using-machine-learning-to-detect-patterns-of-crime/", "citeRegEx": "25", "shortCiteRegEx": "25", "year": 2013}, {"title": "Personal Communication, June 2016", "author": ["Satinder Singh"], "venue": null, "citeRegEx": "26", "shortCiteRegEx": "26", "year": 2016}, {"title": "Discrimination in online ad delivery", "author": ["Latanya Sweeney"], "venue": "Communications of the ACM,", "citeRegEx": "27", "shortCiteRegEx": "27", "year": 2013}, {"title": "Model-based reinforcement learning with nearly tight exploration complexity bounds", "author": ["Istv\u2019an Szita", "Csaba Szepesv\u00e1ri"], "venue": "In Proceedings of the 27th International Conference on Machine Learning,", "citeRegEx": "28", "shortCiteRegEx": "28", "year": 2010}], "referenceMentions": [{"referenceID": 20, "context": "In contexts as diverse as hiring [23], lending [4], policing [25], and criminal sentencing [3], mounting empirical evidence suggests these concerns are not merely hypothetical [2, 27].", "startOffset": 33, "endOffset": 37}, {"referenceID": 2, "context": "In contexts as diverse as hiring [23], lending [4], policing [25], and criminal sentencing [3], mounting empirical evidence suggests these concerns are not merely hypothetical [2, 27].", "startOffset": 47, "endOffset": 50}, {"referenceID": 22, "context": "In contexts as diverse as hiring [23], lending [4], policing [25], and criminal sentencing [3], mounting empirical evidence suggests these concerns are not merely hypothetical [2, 27].", "startOffset": 61, "endOffset": 65}, {"referenceID": 1, "context": "In contexts as diverse as hiring [23], lending [4], policing [25], and criminal sentencing [3], mounting empirical evidence suggests these concerns are not merely hypothetical [2, 27].", "startOffset": 91, "endOffset": 94}, {"referenceID": 24, "context": "In contexts as diverse as hiring [23], lending [4], policing [25], and criminal sentencing [3], mounting empirical evidence suggests these concerns are not merely hypothetical [2, 27].", "startOffset": 176, "endOffset": 183}, {"referenceID": 4, "context": "[6, 7, 12, 14]).", "startOffset": 0, "endOffset": 14}, {"referenceID": 5, "context": "[6, 7, 12, 14]).", "startOffset": 0, "endOffset": 14}, {"referenceID": 9, "context": "[6, 7, 12, 14]).", "startOffset": 0, "endOffset": 14}, {"referenceID": 11, "context": "[6, 7, 12, 14]).", "startOffset": 0, "endOffset": 14}, {"referenceID": 11, "context": "[14], who study fairness in the contextual bandit framework.", "startOffset": 0, "endOffset": 4}, {"referenceID": 11, "context": "[14] define fairness with respect to one-step rewards, which is appropriate for the contextual bandit setting where the learner\u2019s actions do not affect future state.", "startOffset": 0, "endOffset": 4}, {"referenceID": 11, "context": "[14] in terms of myopic rewards would be in conflict with the optimal policy in an underlying MDP if it was necessary to make sub-optimal decisions in the short run (e.", "startOffset": 0, "endOffset": 4}, {"referenceID": 11, "context": "[14], our results establish rigorous trade-offs between fairness and performance in reinforcement learning algorithms.", "startOffset": 0, "endOffset": 4}, {"referenceID": 16, "context": "We adapt E3 [19], a wellknown algorithm for MDP learning, to satisfy approximate-action fairness.", "startOffset": 12, "endOffset": 16}, {"referenceID": 16, "context": "E3 [19] was the first learning algorithm with a polynomial learning rate, and a subsequent literature gave a sequence of improved bounds (see Szita and Szepesv\u00e1ri [28] and references within).", "startOffset": 3, "endOffset": 7}, {"referenceID": 25, "context": "E3 [19] was the first learning algorithm with a polynomial learning rate, and a subsequent literature gave a sequence of improved bounds (see Szita and Szepesv\u00e1ri [28] and references within).", "startOffset": 163, "endOffset": 167}, {"referenceID": 7, "context": "[9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 21, 22, 24, 29]).", "startOffset": 0, "endOffset": 55}, {"referenceID": 8, "context": "[9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 21, 22, 24, 29]).", "startOffset": 0, "endOffset": 55}, {"referenceID": 9, "context": "[9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 21, 22, 24, 29]).", "startOffset": 0, "endOffset": 55}, {"referenceID": 10, "context": "[9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 21, 22, 24, 29]).", "startOffset": 0, "endOffset": 55}, {"referenceID": 11, "context": "[9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 21, 22, 24, 29]).", "startOffset": 0, "endOffset": 55}, {"referenceID": 12, "context": "[9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 21, 22, 24, 29]).", "startOffset": 0, "endOffset": 55}, {"referenceID": 13, "context": "[9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 21, 22, 24, 29]).", "startOffset": 0, "endOffset": 55}, {"referenceID": 14, "context": "[9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 21, 22, 24, 29]).", "startOffset": 0, "endOffset": 55}, {"referenceID": 15, "context": "[9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 21, 22, 24, 29]).", "startOffset": 0, "endOffset": 55}, {"referenceID": 18, "context": "[9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 21, 22, 24, 29]).", "startOffset": 0, "endOffset": 55}, {"referenceID": 19, "context": "[9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 21, 22, 24, 29]).", "startOffset": 0, "endOffset": 55}, {"referenceID": 21, "context": "[9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 21, 22, 24, 29]).", "startOffset": 0, "endOffset": 55}, {"referenceID": 0, "context": "[1, 5, 7, 8, 18, 22]).", "startOffset": 0, "endOffset": 20}, {"referenceID": 3, "context": "[1, 5, 7, 8, 18, 22]).", "startOffset": 0, "endOffset": 20}, {"referenceID": 5, "context": "[1, 5, 7, 8, 18, 22]).", "startOffset": 0, "endOffset": 20}, {"referenceID": 6, "context": "[1, 5, 7, 8, 18, 22]).", "startOffset": 0, "endOffset": 20}, {"referenceID": 15, "context": "[1, 5, 7, 8, 18, 22]).", "startOffset": 0, "endOffset": 20}, {"referenceID": 19, "context": "[1, 5, 7, 8, 18, 22]).", "startOffset": 0, "endOffset": 20}, {"referenceID": 4, "context": "[6], it suffers from two problems.", "startOffset": 0, "endOffset": 3}, {"referenceID": 4, "context": "[6] for a catalog of ways in which statistical parity fails as a definition of fairness.", "startOffset": 0, "endOffset": 3}, {"referenceID": 9, "context": "[12] give a definition aiming to capture the idea of equality of opportunity that overcomes many of these limitations of notions of group fairness.", "startOffset": 0, "endOffset": 4}, {"referenceID": 4, "context": "[6] propose and explore the basic properties of a technical definition of individual fairness formalizing the idea that \u201csimilar individuals should be treated similarly\u201d.", "startOffset": 0, "endOffset": 3}, {"referenceID": 11, "context": "[14] and the present work suggest a natural metric for the tasks of regret minimization in a bandit setting and long-term reward maximization in an MDP.", "startOffset": 0, "endOffset": 4}, {"referenceID": 4, "context": "[6] is that the latter operates under the assumption that the metric is known to the algorithm designer, and hence in their setting, the fairness constraint binds only insofar as it is in conflict with the desired outcome of the algorithm designer.", "startOffset": 0, "endOffset": 3}, {"referenceID": 0, "context": "\u2022 PM : SM \u00d7 AM \u00d7 SM \u2192 [0, 1] is a transition probability distribution mapping (s, a, s\u2032) 7\u2192 p where p is the probability of arriving in state s\u2032 after taking action a from state s.", "startOffset": 22, "endOffset": 28}, {"referenceID": 0, "context": "We assume the support of the reward distribution is [0, 1] for all s.", "startOffset": 52, "endOffset": 58}, {"referenceID": 0, "context": "R\u0304M : SM \u2192 [0, 1] denote the mean of reward distribution at state s.", "startOffset": 11, "endOffset": 17}, {"referenceID": 16, "context": "Kearns and Singh [19]).", "startOffset": 17, "endOffset": 21}, {"referenceID": 16, "context": "distribution is over a possibly unbounded region but the mean and the variance of the reward distribution is bounded in all states using standard techniques (see Kearns and Singh [19] for more details).", "startOffset": 179, "endOffset": 183}, {"referenceID": 0, "context": "Furthermore, assuming that the support of the reward distributions is between [0, 1] can be made without loss of generality up to scaling.", "startOffset": 78, "endOffset": 84}, {"referenceID": 16, "context": "-reward mixing time is always linearly bounded by the -mixing time but can be much smaller than the -mixing time in certain cases (see Kearns and Singh [19] for a discussion).", "startOffset": 152, "endOffset": 156}, {"referenceID": 16, "context": "In the discounted rewards setting, the expected discounted reward of any policy after H = log ( (1\u2212 \u03b3)) / log(\u03b3) (see Kearns and Singh [19], Lemma 2) steps approaches the expected asymptotic discounted reward: Lemma 2 (Lemma 2, Kearns and Singh [19]).", "startOffset": 135, "endOffset": 139}, {"referenceID": 16, "context": "In the discounted rewards setting, the expected discounted reward of any policy after H = log ( (1\u2212 \u03b3)) / log(\u03b3) (see Kearns and Singh [19], Lemma 2) steps approaches the expected asymptotic discounted reward: Lemma 2 (Lemma 2, Kearns and Singh [19]).", "startOffset": 245, "endOffset": 249}, {"referenceID": 23, "context": "Lemma 3 (Singh [26]).", "startOffset": 15, "endOffset": 19}, {"referenceID": 11, "context": "[14]\u2019s definition for learning in a contextual bandit setting: namely, Definition 1 when the Q\u2217(s, a) values are replaced with the expected payoff of choosing a given arm a.", "startOffset": 0, "endOffset": 4}, {"referenceID": 11, "context": "[14] define the quality of an action to be QM (s, a, 1), the expected one-step reward for choosing a from s.", "startOffset": 0, "endOffset": 4}, {"referenceID": 17, "context": "[20], connecting the number of random walks taken from s to the accuracy of the empirical V \u2217 M estimates.", "startOffset": 0, "endOffset": 4}, {"referenceID": 17, "context": "[20]).", "startOffset": 0, "endOffset": 4}, {"referenceID": 16, "context": "For the second condition, we adapt the analysis of E3 in which Kearns and Singh [19] show that if each action in a state s is taken m2 times, then the transition probabilities and reward in state s can be estimated accurately (see Section 6.", "startOffset": 80, "endOffset": 84}], "year": 2017, "abstractText": "We initiate the study of fair learning in Markovian settings, where the actions of a learning algorithm may affect its environment and future rewards. Working in the model of reinforcement learning, we define a fairness constraint requiring that an algorithm never prefers one action over another if the long-term (discounted) reward of choosing the latter action is higher. Our first result is negative: despite the fact that fairness is consistent with the optimal policy, any learning algorithm satisfying fairness must take exponentially many rounds in the number of states to achieve non-trivial approximation to the optimal policy. Our main result is a polynomial time algorithm that is provably fair under an approximate notion of fairness, thus establishing an exponential gap between exact and approximate fairness.", "creator": "LaTeX with hyperref package"}}}