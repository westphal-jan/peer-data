{"id": "1611.03231", "review": {"conference": "AAAI", "VERSION": "v1", "DATE_OF_SUBMISSION": "10-Nov-2016", "title": "Policy Search with High-Dimensional Context Variables", "abstract": "Direct contextual policy search methods learn to improve policy parameters and simultaneously generalize these parameters to different context or task variables. However, learning from high-dimensional context variables, such as camera images, is still a prominent problem in many real-world tasks. A naive application of unsupervised dimensionality reduction methods to the context variables, such as principal component analysis, is insufficient as task-relevant input may be ignored. In this paper, we propose a contextual policy search method in the model-based relative entropy stochastic search framework with integrated dimensionality reduction. We learn a model of the reward that is locally quadratic in both the policy parameters and the context variables. Furthermore, we perform supervised linear dimensionality reduction on the context variables by nuclear norm regularization. The experimental results show that the proposed method outperforms naive dimensionality reduction via principal component analysis and a state-of-the-art contextual policy search method.", "histories": [["v1", "Thu, 10 Nov 2016 09:25:12 GMT  (749kb)", "http://arxiv.org/abs/1611.03231v1", null]], "reviews": [], "SUBJECTS": "stat.ML cs.LG", "authors": ["voot tangkaratt", "herke van hoof", "simone parisi", "gerhard neumann", "jan peters 0001", "masashi sugiyama"], "accepted": true, "id": "1611.03231"}, "pdf": {"name": "1611.03231.pdf", "metadata": {"source": "CRF", "title": "Policy Search with High-Dimensional Context Variables", "authors": ["Voot Tangkaratt", "Herke van Hoof", "Simone Parisi", "Gerhard Neumann", "Jan Peters", "Masashi Sugiyama"], "emails": [], "sections": [{"heading": null, "text": "ar Xiv: 161 1.03 231v 1 [stat.ML] 1 0N ov2 01Direct contextual political search methods learn to improve political parameters while generalizing these parameters to different context or task variables at the same time. However, learning from high-dimensional context variables, such as camera images, is still a prominent problem in many real-world tasks. A naive application of methods for unattended dimension reduction to context variables, such as the main component analysis, is not sufficient as task-relevant inputs can be ignored. In this paper, we propose a context-related political search method in the model-based relative entropy stochastic search framework with integrated dimensionality reduction. We learn a model of reward that is conceptual in both the political parameters and the context variable locally quadratic. Furthermore, we demonstrate a linear dimensionality reduction based on policy principles that the contextual variability adjustment is preconceived."}, {"heading": "Introduction", "text": "An autonomous agent often requires different strategies for solving tasks in different contexts. For example, naive problem solving allows the robot to adjust its controller according to the ball position, i.e. context. Direct political search approaches (Baxter and Bartlett, 2000; Rosenstein and Barto, 2001; Deisenroth, Neumann and Peters, 2013) allow the agent to learn a separate policy for each context by trial and error. However, learning optimal strategies for many large contexts, such as in the presence of continuous context variables, is impractical. On the other hand, direct contextual political search approaches (Kober, Oztop, and Peters, 2011; da Silva, Konidaris, and Barto, 2012) require the representation of contexts by real evaluated vectors and are able to learn context-dependent dissemination through the political parameters. Such distribution can become generalized via context values, and therefore the agent is uncontextually adaptable."}, {"heading": "Contextual Policy Search", "text": "In this section we will formulate the problem of direct contextual policy search and briefly discuss existing methods."}, {"heading": "Problem Formulation", "text": "The direct context-related political search is formulated as follows: An agent observes the context variable c-Rdc and extracts a parameter from a search distribution \u03c0 (\u03b8 | c). Subsequently, the agent executes a policy with the parameter \u03b8 and observes a scalar reward calculated by a reward function R (\u03b8, c). The aim is to find a search distribution \u03c0 (\u03b8 | c) that maximizes the expected reward as much as possible; however, it is assumed that the agent can always access the reward value."}, {"heading": "Related Work", "text": "In the basic context-dependent political representation, the agent iteratively collects samples {Nazin, cn, R (\u03b8n, cn)} Nn = 1 using a sampling distribution q (\u03b8 | c). Subsequently, he calculates a new search distribution method \u03c0 (\u03b8 | c) so that the expected reward increases or is maximized. In the literature, various approaches are used to calculate the new search distribution method, e.g. evolutionary strategies (Hansen, Mu Muller, and Koumoutsakos, 2003), expectation maximization algorithms (Kober, Oztop, and Peters, 2011), or information theory approaches (Deisenroth, and Peters, 2013). Most of the existing context-dependent search methods focus on tasks with low context variable variables that relate to highly contextual variable contexts."}, {"heading": "Contextual MORE", "text": "The original MORE (Abdolmaleki et al., 2015) finds a search distribution (without context) that maximizes the expected reward while limiting Kullback Leibler (KL) divergence (Kullback and Leibler, 1951) and entropy (KL and entropy) upwards to control the trade-off between exploration and exploitation. The key finding of MORE is to learn a reward model to efficiently calculate a new search distribution in closed form. Below, we propose our method, called Contextual Model-Based Relative Entropy Stochastic Search (C-MEHR), a direct contextual political search method within the framework of MORE."}, {"heading": "Learning the Search Distribution", "text": "The goal of C-MORE is to find a search distribution that maximizes the expected reward and at the same time exceeds the expected KL divergence between \u03c0 (2001) and q (2001) and falls short of the expected entropy of \u03c0 (2001). Formally, the maximum divergence between \u03c0 (2001) and q (2001) describes the expected entropy (2001) q (2001) p (2001) p (2001) p (2001) p (2001) p (2001) p (2001) p (2001) p (2001) p (2001) p (2001) p (2001) p (2001) p (2001) p) p (2001) p) p (2001) p (p) p (2001) p (2001) p (2001) p (2001) p (2001) p (2001) p (2001) p) p (2005) p) p) p) p) p (2001) p (p) p) p) p (2001) p) p) p) p) p) p) p) p) p) 2001 p) p) p (2001 p) p) p) 2001 p (2001 p) p) p) 2001 c) 2001 p (2001 p (2001 p) p) p (2001 p) p) p (2001 p) p) p) p (2001 p) p) p) p \"p) p) p\" p) p (2001 p) p (2001 p) p) p) p \"p) p) p\" p \"p) p (2001 p) p (2001 p) p) p\" p (2001 p) p) p) p \"p (2001 p) p) p\" p \"p (2001 p) p) p (2001 p (p) p) p\" p) p \"p\" p (2001 p) p) p \"p\" p (2001 p) p (p) p) p \"p (2001 p) p (p) p (p) p\" p (p) p \"p\" p \"p) p\" p \"p\" p) p."}, {"heading": "Dual Function Evaluation via the Quadratic Model", "text": "We assume that the reward function R (conceived, c \u2212 conceived) can be approximated by means of a quadratic model, with all derivatives in the supplementary material.Algorithm 1: C-MORE Input: Parameter and \u03b2, initial distribution (conceived, c) 1 for k = 1,.., K do 2 for n = 1,., N do 3 Context, cn \u00b5 (c) 4 Draw parameters and feel. \u2212 Empirical task with empirical n and receive R (empirical, cn) 6 Learn the quadratic model., N do 3 Observe the context cn \u00b5 (c) 4 Draw parameters empirically (empirically) empirically empirically empirically empirically (empirically, cn) and empirically empirically empirically (empirically, cn)."}, {"heading": "Learning the Quadratic Model", "text": "The performance of C-MORE depends on the accuracy of the quadratic model. For many problems, the reward function R (\u03b8, c) is not quadratic and the quadratic model is not suitable for approximating the entire reward function. However, the reward function is often smooth and can be approximated locally by a quadratic model. Therefore, we approach the reward function locally by learning a new quadratic model for each update.The quadratic model can be learned by regression methods such as comb regression2 (Bischof, 2006). However, comb regression tends to make errors when the context is high-dimensional. Below, we will address this problem by first showing that linear dimensionality reduction on the context variables results in a matrix of low-order to perform a nuclear standardization approach to achieve a low matrix without exposing a matrix."}, {"heading": "Dimensionality Reduction and Low-Rank Representation", "text": "A linear reduction of dimensionality to the contextual variables leads to the following square model: (3) W = W \u00b2 W \u00b2 W \u00b2 W \u00b2 W \u00b2 W \u00b2 W \u00b2 W \u00b2 W \u00b2 W \u00b2 W \u00b2 W \u00b2 W \u00b2 W \u00b2 W \u00b2 W \u00b2 W \u00b2 W \u00b2 W \u00b2 W \u00b2 W \u00b2 W \u00b2 W \u00b2 W \u00b2 2 + r0, (7) W \u00b2 Rdz \u00b2 W \u00b2 W \u00b2 W \u00b2 W \u00b2 W \u00b2 W \u00b2 W \u00b2 W \u00b2 W \u00b2 W \u00b2 W \u00b2 W \u00b2 W \u00b2 W \u00b2 W \u00b2 R \u00b2, (7) W \u00b2 R \u00b2 R \u00b2 R \u00b2 W \u00b2 W \u00b2 W \u00b2 W \u00b2 W \u00b2 W \u00b2 W \u00b2 W \u00b2 W \u00b2 W \u00b2 W \u00b2 W \u00b2 W \u00b2 W \u00b2 W \u00b2 W \u00b2 W \u00b2 W \u00b2 W \u00b2 R \u00b2, (7) W \u00b2 R \u00b2 R \u00b2 R \u00b2 R \u00b2 W \u00b2 R \u00b2 W \u00b2 W \u00b2 W \u00b2 W \u00b2 W \u00b2 W \u00b2 W \u00b2 W \u00b2 R \u00b2 W \u00b2 W \u00b2 W \u00b2 W \u00b2 W \u00b2 W \u00b2 R \u00b2 W \u00b2 R \u00b2 R \u00b2, (7) W \u00b2 R \u00b2 R \u00b2 W \u00b2 R \u00b2 W \u00b2 W \u00b2 W \u00b2 R \u00b2, (7) W \u00b2 R \u00b2, R \u00b2, R \u00b2, R \u00b2, R \u00b2, R \u00b2, R \u00b2, R \u00b2, R \u00b2, R \u00b2, R \u00b2, R \u00b2, R \u00b2, R \u00b2, R \u00b2, R \u00b2, R \u00b2, R \u00b2, R \u00b2, R \u00b2, R \u00b2, R \u00b2, R \u00b2, R \u00b2, R \u00b2, R \u00b2, R \u00b2, R \u00b2, R \u00b2, R \u00b2, R \u00b2, R \u00b2, W \u00b2, R \u00b2, R \u00b2, R \u00b2, R, R \u00b2, R \u00b2, R \u00b2, W \u00b2, R \u00b2, R \u00b2, R \u00b2, R, R \u00b2, R \u00b2, R \u00b2, R \u00b2, R, R, R \u00b2, R \u00b2, R, R, R, R, R, R, R \u00b2, R, R, R, R, R, R, R, R, R, R, R, (7), (7) W \u00b2, R, R \u00b2, R, R, R, R, R, R, R, R, R, R, R, R, R, R, R, R, R, R, R, R, R, R, R,"}, {"heading": "Learning a Low-Rank Matrix with Nuclear Norm Regularization", "text": "It is not as if it were an unforeseen, unforeseen, unforeseen, unforeseen, unforeseen, unforeseen, unforeseen, unforeseen, unforeseen, unforeseen, unforeseen, unforeseen, unforeseen, unforeseen, unforeseen, unforeseen, unforeseen, unforeseen, unforeseen, unforeseen, unforeseen, unforeseen, unforeseen, unforeseen, unforeseen, unforeseen, unforeseen, unforeseen, unforeseen,"}, {"heading": "Experiments", "text": "We evaluate the proposed method based on three problems. We start by examining the behavior of C-MORE in a scenario where we know the true reward model and the true low-dimensional context. Then, we focus our attention on two simulated robot ball-striking tasks. In the first task, a planar robot arm with toy 2-DoF must hit a ball that is placed on a plane. In the second task, a simulated robot arm with 6-DoF must hit a ball that is placed in a three-dimensional space. In both cases, the robots fulfill their task by using raw camera images as context variables. In the latter case, however, we have limited data, and therefore the efficiency of the samples is of primary importance. The evaluation is made on three different versions of C-MORE, according to the learning approach of the model: using only the ridge regression (C-MEHR Ridge), supported by a low-dimensional context variable, which is supported by MEA (MEHR) and sufficiently learned by MEHR (MEHR)."}, {"heading": "Quadratic Cost Function Optimization", "text": "In the first experiment, we want to examine the performance of the algorithms in a context in which we are able to analytically calculate both the reward and the true low-dimensional context. To achieve this goal, we define the following problemsR (\u03b8, c) = \u2212 (| \u03b8 \u2212 T 1c, 2) 2, c = I, T \u2212 12 c, T 1, dc, T 2, Rdc \u00b7 dc, I, Rdc, dc, dc < dc, where I am the identity matrix, I, is a rectangular matrix with ones in their most important diagonal and zeros, otherwise c, is the true low-dimensional context, and T 1 is to agree with the dimension of the true context and parameter for calculating the reward. This context is particularly interesting because only a subset of the observed context affects the reward."}, {"heading": "Ball Hitting with a 2-DoF Robot Arm", "text": "The context observed by the robot (blue and red lines) consists of a virtual green ball and the background of the image. In this task, a simulated planar robotic arm (shown aside) must meet a green virtual ball placed on RGB camera images of the size 32 x 24. The context is defined by the observed pixels, for a total of 2304 context variables. The ball is placed randomly and evenly in the working area of the robot. Noises from an even random distribution in [\u2212 30, 30] are also inserted into the context to simulate different lighting conditions. The robot controls the collective accelerations in each time step through a linear-in-parameter-based controller with Gaussian basic functions, for a total of 32 parameters to be learned. The reward R (\u03b8, c) is the negative cumulative joint acceleration plus the negative distance between the end effector and the ball in the end-time stage."}, {"heading": "Ball Hitting with a 6-DoF Robot Arm", "text": "Similar to the previous task, here a 6-DoF robotic arm must hit a ball placed in a three-dimensional space, as in Figure 3. The context is again defined by the vectorized pixels of 32 x 24 RGB images, for a total of 2304 context variables. Note that Figure 3a shows an image before we take it to the size of 32 x 24. However, unlike the 2-DoF task, the ball is recorded directly by a real camera placed near the physical robot, and it is not generated virtually on the images. In addition, the robot is controlled by dynamic motoric primitives (Ijspeert, Nakanishi and Schaal, 2002) (DMPs), which are non-linear dynamic systems. We use one DMP per connection, with five basic functions per DMP. We also learn the target attractor of the DMPs, for a total of 36 parameters we want to learn. The cumulative R (the accumulation of the ball) is collected as a 3."}, {"heading": "Conclusion", "text": "Learning with high-dimensional context variables is a challenging and prominent problem in machine learning. In this paper, we proposed C-MORE, a novel contextual policy search method with integrated dimensionality reduction. C-MORE learns a reward model that is locally square in terms of policy parameters and context variables. By presenting the model as low, we perform a supervised linear dimensionality reduction. Unlike existing techniques that rely on non-convex formulations, the nuclear standard allows us to learn low representation by solving a convex optimization problem, thereby guaranteeing convergence to a global optimum. The main drawback of the proposed method is that it requires more computing time due to nuclear standardization. Although we did not have serious problems in our experiments, this problem can be mitigated for 2014 by using very large-scale subdimensional tasks such as oluminescence."}, {"heading": "Derivations of C-MORE", "text": "In this section we deduct C-MORE in detail. C-MEHR SolutionsMaxMaxMaxMaxMaxMaxMaxMaxMaxMaxMaxMaxMaxMaxMaxMaxMaxMaxMaxMaxMaxMaxMaxMaxMaxMaxMaxMaxMaxMaxMaxMaxMaxMaxMaxMaxMaxMaxMaxMaxMaxMaxMaxMaxMaxMaxMaxMaxMaxMaxMaxMaxMaxMaxMaxMaxMaxMaxMaxMaxMaxMaxMaxMaxMaxMaxMaxMaxMaxMaxMaxMaxMaxMaxMaxMaxMaxMaxMaxMaxMaxMaxMaxMaxMaxMaxMaxMaxMaxMaxMaxMaxMaxMaxMaxMaxMaxMaxMaxMaxMaxMaxMaxMaxMaxMaxMaxMaxMaxMaxMaxMaxMaxMaxMaxMaxMaxMaxMaxMaxMaxMaxMaxMaxMaxMaxMaxMaxMaxMaxMaxMaxMaxMaxMaxMaxMaxMaxMaxMaxMaxMaxMaxMaxMaxMaxMaxMaxMaxMaxMaxMaxMaxMaxMaxMaxMaxMaxMaxMaxMaxMaxMaxMaxMaxMaxMaxMaxMaxMaxMaxMaxMaxMaxMaxMaxMaxMaxMaxMaxMaxMaxMaxMaxMaxMaxMaxMaxMaxMaxMaxMaxMaxMaxMaxMaxMaxMaxMaxMaxMaxMaxMaxMaxMaxMaxMaxMaxMaxMaxMaxMaxMaxMaxMaxMaxMaxMaxMaxMaxMaxMaxMaxMaxMaxMaxMaxMaxMaxMaxMaxMaxMaxMaxMaxMaxMaxMaxMaxMaxMaxMaxMaxMaxMaxMaxMaxMaxMaxMaxMaxMaxMaxMaxMaxMaxMaxMaxMaxMaxottottottottottottottottottottottottottottottottottottottottottottottottottottottottottottottottottottottottottottottottottottottottottottottottottottottottottottottottottottottottottottottottottottottottottottottottottottottottottottottottottottottottottottottottottottottottottottottottottottottottottottottottottottottottottottottottottottottottottottottottottottottottottottottottottottottottottottottottottottottottottottottottottottottottottottottottottottottottottottottottottottottottottottottottottottottottottottottottottottottottottottottottottottottottottottottottottottottottottottottottottottottottottottottottottottottottottottottottottottottottottottottottottottottottott"}, {"heading": "Evaluating the Dual Function", "text": "Here we show how to calculate the new search distribution in a closed form."}, {"heading": "Experiments Details", "text": "At this point, we present all additional details to the experiments that are not mentioned in the paper."}, {"heading": "Quadratic Cost Function Optimization", "text": "The mean value of the collected Gaussian distribution is initialized in [0, 5] according to the random principle, while the initial covariance is Q = 10, 000I. For the core standard, the regularization parameters are set to \u03bb * = 0.00002 and \u03bb = 0.0000001. For PCAs, the dimensionality candidates are dz * 3, 6, 10, 20}. Figure 6 shows the performance with other dimensionality. The regularization parameter for the \"2 regularization\" is set to \u03bb = 0.00001. The step size for the accelerated proximal gradient (APG) is set to 0.001. We also normalize the gradient so that the Frobenius standard is 1. The maximum iteration of APG is set to K = 300. C-REPS and C-MORE KL divergence is set to 0.9."}, {"heading": "Ball Hitting with a 2-DoF Robot Arm", "text": "We use a linear-in-parameter controller with Gaussian basic functions to control the combined accelerations q \u00b2 of the robot q \u00b2 = [\u03b8 1 \u00b2 (q), \u03b8 \u00b2 (q)], where \u03b8 = [\u03b8 1, \u03b8 2] is the political parameter vector, q \u00b2 R2 is the common angle vector, and \u0432 is the Gaussian basic function vector with 16 Gaussian centers located at {0, \u03c0 2, 3\u03c0 2} \u00b7 {0, \u03c0 2, 3\u03c0 2} for both joints. The number of total angle parameter vectors is 32. The reward R (\u03b8, c) is calculated because the negative cumulative acceleration plus the negative distance between the end effector and the sphere is the final time stage R (\u03b8, c) = \u2212 0.05 40 \u0445t = 1 | q \u00b2 d norm. The reward R (ballx \u2212 2) plus the negative distance between the end effector and the negative time stage is the acceleration of the deactivation."}, {"heading": "Ball Hitting with a 6-DoF Robot Arm", "text": "In this task, the reward function is defined as follows: \"A\" (\u03b8, c) = \"J\" = \"J\" = \"Y\" = \"Y\" = \"K\" = \"D\" = \"D\" = \"D\" (\u2212 eigenance2), if distance > 0.1 cm. \u2212 0.05 \"qt + 10\" exp \"(\u2212 distance2) + 20,\" otherwise, if the distance indicates the minimum distance between ball and end effector along the trajectory. \u2212 APG and PCA setup are identical to the robot experiment \"2-DoF.\" \u2212 Algorithm 2: APG for solving the nuclear standard minimization problem \u2212 input: Parameters \"and maximum accuracy over 50 TP = 0.1,\" TP = 1, \"maximum accuracy over 50 TP = 1,\" in each case \u2212 D = 1, TD = 1 and TP = 1."}], "references": [{"title": "Model-based relative entropy stochastic search", "author": ["A. Abdolmaleki", "R. Lioutikov", "J. Peters", "N. Lau", "L.P. Reis", "G. Neumann"], "venue": "Advances in Neural Information Processing Systems 28, 3537\u20133545.", "citeRegEx": "Abdolmaleki et al\\.,? 2015", "shortCiteRegEx": "Abdolmaleki et al\\.", "year": 2015}, {"title": "Reinforcement learning in POMDP\u2019s via direct gradient ascent", "author": ["J. Baxter", "P.L. Bartlett"], "venue": "The 17th International Conference on Machine Learning, 41\u201348.", "citeRegEx": "Baxter and Bartlett,? 2000", "shortCiteRegEx": "Baxter and Bartlett", "year": 2000}, {"title": "Learning deep architectures for AI", "author": ["Y. Bengio"], "venue": "Foundations and Trends in Machine Learning 2(1):1\u2013 127.", "citeRegEx": "Bengio,? 2009", "shortCiteRegEx": "Bengio", "year": 2009}, {"title": "Pattern Recognition and Machine Learning (Information Science and Statistics)", "author": ["C.M. Bishop"], "venue": "Secaucus, NJ, USA: Springer-Verlag New York, Inc.", "citeRegEx": "Bishop,? 2006", "shortCiteRegEx": "Bishop", "year": 2006}, {"title": "Convex Optimization", "author": ["S. Boyd", "L. Vandenberghe"], "venue": "New York, NY, USA: Cambridge University Press.", "citeRegEx": "Boyd and Vandenberghe,? 2004", "shortCiteRegEx": "Boyd and Vandenberghe", "year": 2004}, {"title": "The power of convex relaxation: near-optimal matrix completion", "author": ["E.J. Cand\u00e8s", "T. Tao"], "venue": "IEEE Transactions on Information Theory 56(5):2053\u20132080.", "citeRegEx": "Cand\u00e8s and Tao,? 2010", "shortCiteRegEx": "Cand\u00e8s and Tao", "year": 2010}, {"title": "Learning parameterized skills", "author": ["B.C. da Silva", "G. Konidaris", "A.G. Barto"], "venue": "In The 29th International Conference on Machine Learning", "citeRegEx": "Silva et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Silva et al\\.", "year": 2012}, {"title": "A survey on policy search for robotics", "author": ["M.P. Deisenroth", "G. Neumann", "J. Peters"], "venue": "Foundations and Trends in Robotics 2(1-2):1\u2013142.", "citeRegEx": "Deisenroth et al\\.,? 2013", "shortCiteRegEx": "Deisenroth et al\\.", "year": 2013}, {"title": "Projection pursuit regression", "author": ["J.H. Friedman", "W. Stuetzle"], "venue": "Journal of the American Statistical Association 76(376):817\u2013823.", "citeRegEx": "Friedman and Stuetzle,? 1981", "shortCiteRegEx": "Friedman and Stuetzle", "year": 1981}, {"title": "Kernel dimension reduction in regression", "author": ["K. Fukumizu", "F.R. Bach", "M.I. Jordan"], "venue": "The Annals of Statistics 37(4):1871\u20131905.", "citeRegEx": "Fukumizu et al\\.,? 2009", "shortCiteRegEx": "Fukumizu et al\\.", "year": 2009}, {"title": "Bayesian supervised dimensionality reduction", "author": ["M. G\u00f6nen"], "venue": "IEEE Transanscation on Cybernetics 43(6):2179\u20132189.", "citeRegEx": "G\u00f6nen,? 2013", "shortCiteRegEx": "G\u00f6nen", "year": 2013}, {"title": "Finding structure with randomness: Probabilistic algorithms for constructing approximate matrix decompositions", "author": ["N. Halko", "P. Martinsson", "J.A. Tropp"], "venue": "SIAM Review 53(2):217\u2013288.", "citeRegEx": "Halko et al\\.,? 2011", "shortCiteRegEx": "Halko et al\\.", "year": 2011}, {"title": "Reducing the time complexity of the derandomized evolution strategy with covariance matrix adaptation (CMA-ES)", "author": ["N. Hansen", "S.D. M\u00fcller", "P. Koumoutsakos"], "venue": "Evolutionary Computation 11(1):1\u201318.", "citeRegEx": "Hansen et al\\.,? 2003", "shortCiteRegEx": "Hansen et al\\.", "year": 2003}, {"title": "Nuclear norm minimization via active subspace selection", "author": ["C. Hsieh", "P.A. Olsen"], "venue": "The 31st International Conference on Machine Learning, 575\u2013 583.", "citeRegEx": "Hsieh and Olsen,? 2014", "shortCiteRegEx": "Hsieh and Olsen", "year": 2014}, {"title": "Learning attractor landscapes for learning motor primitives", "author": ["A.J. Ijspeert", "J. Nakanishi", "S. Schaal"], "venue": "Advances in Neural Information Processing Systems 15, 1523\u20131530.", "citeRegEx": "Ijspeert et al\\.,? 2002", "shortCiteRegEx": "Ijspeert et al\\.", "year": 2002}, {"title": "Principal Component Analysis", "author": ["I.T. Jolliffe"], "venue": "Springer Verlag.", "citeRegEx": "Jolliffe,? 1986", "shortCiteRegEx": "Jolliffe", "year": 1986}, {"title": "Reinforcement learning to adjust robot movements to new situations", "author": ["J. Kober", "E. Oztop", "J. Peters"], "venue": "The 22nd International Joint Conference on Artificial Intelligence, 2650\u20132655.", "citeRegEx": "Kober et al\\.,? 2011", "shortCiteRegEx": "Kober et al\\.", "year": 2011}, {"title": "On information and sufficiency", "author": ["S. Kullback", "R.A. Leibler"], "venue": "The Annals of Mathematical Statistics 22(1):79\u201386.", "citeRegEx": "Kullback and Leibler,? 1951", "shortCiteRegEx": "Kullback and Leibler", "year": 1951}, {"title": "Variational inference for policy search in changing situations", "author": ["G. Neumann"], "venue": "The 28th International Conference on Machine Learning, 817\u2013824.", "citeRegEx": "Neumann,? 2011", "shortCiteRegEx": "Neumann", "year": 2011}, {"title": "Trace norm regularization: Reformulations, algorithms, and multi-task learning", "author": ["T.K. Pong", "P. Tseng", "S. Ji", "J. Ye"], "venue": "SIAM Journal on Optimization 20(6):3465\u20133489.", "citeRegEx": "Pong et al\\.,? 2010", "shortCiteRegEx": "Pong et al\\.", "year": 2010}, {"title": "Guaranteed minimum-rank solutions of linear matrix equations via nuclear norm minimization", "author": ["B. Recht", "M. Fazel", "P.A. Parrilo"], "venue": "SIAM Review 52(3):471\u2013501.", "citeRegEx": "Recht et al\\.,? 2010", "shortCiteRegEx": "Recht et al\\.", "year": 2010}, {"title": "Robot weightlifting by direct policy search", "author": ["M.T. Rosenstein", "A.G. Barto"], "venue": "The 17th International Joint Conference on Artificial Intelligence, 839\u2013846.", "citeRegEx": "Rosenstein and Barto,? 2001", "shortCiteRegEx": "Rosenstein and Barto", "year": 2001}, {"title": "Mastering the game of go with deep neural networks and tree search", "author": ["K.", "T. Graepel", "D. Hassabis"], "venue": "Nature 529:484\u2013503.", "citeRegEx": "K. et al\\.,? 2016", "shortCiteRegEx": "K. et al\\.", "year": 2016}, {"title": "Sufficient dimension reduction via squared-loss mutual information estimation", "author": ["T. Suzuki", "M. Sugiyama"], "venue": "Neural Computation 25:725\u2013758.", "citeRegEx": "Suzuki and Sugiyama,? 2013", "shortCiteRegEx": "Suzuki and Sugiyama", "year": 2013}, {"title": "An accelerated proximal gradient algorithm for nuclear norm regularized least squares problems", "author": ["K. Toh", "S. Yun"], "venue": "International Symposium on Mathematical Programming.", "citeRegEx": "Toh and Yun,? 2009", "shortCiteRegEx": "Toh and Yun", "year": 2009}, {"title": "Locally weighted projection regression: Incremental real time learning in high dimensional space", "author": ["S. Vijayakumar", "S. Schaal"], "venue": "The 17th International Conference on Machine Learning, 1079\u20131086.", "citeRegEx": "Vijayakumar and Schaal,? 2000", "shortCiteRegEx": "Vijayakumar and Schaal", "year": 2000}, {"title": "On the implementation of an interior-point filter line-search algorithm for large-scale nonlinear programming", "author": ["A. W\u00e4chter", "L.T. Biegler"], "venue": "Mathametical Programming 106(1):25\u201357.", "citeRegEx": "W\u00e4chter and Biegler,? 2006", "shortCiteRegEx": "W\u00e4chter and Biegler", "year": 2006}, {"title": "Embed to control: A locally linear latent dynamics model for control from raw images", "author": ["M. Watter", "J.T. Springenberg", "J. Boedecker", "M.A. Riedmiller"], "venue": "Advances in Neural Information Processing Systems 28, 2746\u20132754.", "citeRegEx": "Watter et al\\.,? 2015", "shortCiteRegEx": "Watter et al\\.", "year": 2015}], "referenceMentions": [{"referenceID": 1, "context": "Direct policy search approaches (Baxter and Bartlett, 2000; Rosenstein and Barto, 2001; Deisenroth, Neumann, and Peters, 2013) allow the agent to learn a separate policy for each context through trial and error.", "startOffset": 32, "endOffset": 126}, {"referenceID": 21, "context": "Direct policy search approaches (Baxter and Bartlett, 2000; Rosenstein and Barto, 2001; Deisenroth, Neumann, and Peters, 2013) allow the agent to learn a separate policy for each context through trial and error.", "startOffset": 32, "endOffset": 126}, {"referenceID": 18, "context": "On the other hand, direct contextual policy search approaches (Kober, Oztop, and Peters, 2011; Neumann, 2011; da Silva, Konidaris, and Barto, 2012) represent the contexts by real-valued vectors and are able to learn a context-dependent distribution over the policy parameters.", "startOffset": 62, "endOffset": 147}, {"referenceID": 3, "context": "Learning from high-dimensional variables, in fact, is still an important problem in statistics and machine learning (Bishop, 2006).", "startOffset": 116, "endOffset": 130}, {"referenceID": 0, "context": "To alleviate these issues, Abdolmaleki et al. (2015) recently proposed a stochastic search framework called model-based relative entropy stochastic search (MORE).", "startOffset": 27, "endOffset": 53}, {"referenceID": 15, "context": "A prominent example is principal component analysis (PCA) (Jolliffe, 1986), that does not take the supervisory signal into account and therefore cannot discriminate between relevant and irrelevant latent dimensions.", "startOffset": 58, "endOffset": 74}, {"referenceID": 23, "context": "Moreover, they often involve non-convex optimization and suffer from local optima (Fukumizu, Bach, and Jordan, 2009; Suzuki and Sugiyama, 2013).", "startOffset": 82, "endOffset": 143}, {"referenceID": 2, "context": "In the last years, non-linear dimensionality reduction techniques based on deep learning have gained popularity (Bengio, 2009).", "startOffset": 112, "endOffset": 126}, {"referenceID": 2, "context": "In the last years, non-linear dimensionality reduction techniques based on deep learning have gained popularity (Bengio, 2009). For instance, Watter et al. (2015) proposed a generative deep network to learn lowdimensional representations of images in order to capture information about the system transition dynamics and allow optimal control problems to be solved in lowdimensional spaces.", "startOffset": 113, "endOffset": 163}, {"referenceID": 2, "context": "In the last years, non-linear dimensionality reduction techniques based on deep learning have gained popularity (Bengio, 2009). For instance, Watter et al. (2015) proposed a generative deep network to learn lowdimensional representations of images in order to capture information about the system transition dynamics and allow optimal control problems to be solved in lowdimensional spaces. More recently, Silver et al. (2016) successfully trained a machine to play a high-level game of go using a deep convolutional network.", "startOffset": 113, "endOffset": 427}, {"referenceID": 0, "context": "The original MORE (Abdolmaleki et al., 2015) finds a search distribution (without context) that maximizes the expected reward while upper-bounding the KullbackLeibler (KL) divergence (Kullback and Leibler, 1951) and lower-bounding the entropy.", "startOffset": 18, "endOffset": 44}, {"referenceID": 17, "context": ", 2015) finds a search distribution (without context) that maximizes the expected reward while upper-bounding the KullbackLeibler (KL) divergence (Kullback and Leibler, 1951) and lower-bounding the entropy.", "startOffset": 146, "endOffset": 174}, {"referenceID": 0, "context": "The latter is adaptively changed according to the percentage of the relative difference between the sampling policy\u2019s expected entropy and the minimal entropy, as described by Abdolmaleki et al. (2015), i.", "startOffset": 176, "endOffset": 202}, {"referenceID": 26, "context": "The dual function can be minimized by standard non-linear optimization routines such as IPOPT (W\u00e4chter and Biegler, 2006).", "startOffset": 94, "endOffset": 121}, {"referenceID": 3, "context": "The quadratic model can be learned by regression methods such as ridge regression2 (Bishop, 2006).", "startOffset": 83, "endOffset": 97}, {"referenceID": 15, "context": "Principal component analysis (PCA) (Jolliffe, 1986) is a common method used in statistics and machine learning.", "startOffset": 35, "endOffset": 51}, {"referenceID": 23, "context": "Alternative supervised techniques, such as KDR (Fukumizu, Bach, and Jordan, 2009) and LSDR (Suzuki and Sugiyama, 2013), do not take the regression model, i.", "startOffset": 91, "endOffset": 118}, {"referenceID": 8, "context": "On the contrary, in projection regression (Friedman and Stuetzle, 1981; Vijayakumar and Schaal, 2000) the model parameters and the projection matrix are learned simultaneously.", "startOffset": 42, "endOffset": 101}, {"referenceID": 25, "context": "On the contrary, in projection regression (Friedman and Stuetzle, 1981; Vijayakumar and Schaal, 2000) the model parameters and the projection matrix are learned simultaneously.", "startOffset": 42, "endOffset": 101}, {"referenceID": 10, "context": "In the original MORE, Bayesian dimensionality reduction (G\u00f6nen, 2013) is applied to perform linear supervised dimensionality reduction on \u03b8, i.", "startOffset": 56, "endOffset": 69}, {"referenceID": 4, "context": "Second, a set of negative definite matrices is convex since y(aX + (1 \u2212 a)Y )y < 0 for any negative definite matrices X and Y , 0 \u2264 a \u2264 1, and any vector y (Boyd and Vandenberghe, 2004).", "startOffset": 156, "endOffset": 185}, {"referenceID": 5, "context": "For this reason, the nuclear norm has been a popular surrogate to a low-rank constraint in many applications, such as matrix completion (Cand\u00e8s and Tao, 2010) and multi-task learning (Pong et al.", "startOffset": 136, "endOffset": 158}, {"referenceID": 19, "context": "For this reason, the nuclear norm has been a popular surrogate to a low-rank constraint in many applications, such as matrix completion (Cand\u00e8s and Tao, 2010) and multi-task learning (Pong et al., 2010).", "startOffset": 183, "endOffset": 202}, {"referenceID": 4, "context": "(8) is convex, any convex optimization method can be used (Boyd and Vandenberghe, 2004).", "startOffset": 58, "endOffset": 87}, {"referenceID": 24, "context": "For our experiments, we use the accelerated proximal gradient descend (APG) (Toh and Yun, 2009).", "startOffset": 76, "endOffset": 95}, {"referenceID": 13, "context": "Although we did not encounter severe problems in our experiments, for very large dimensional tasks this issue can be mitigated by using more efficient techniques, such as active subspace selection (Hsieh and Olsen, 2014).", "startOffset": 197, "endOffset": 220}, {"referenceID": 2, "context": "Recently, non-linear techniques based on deep network has been showing impressive performance (Bengio, 2009; Watter et al., 2015).", "startOffset": 94, "endOffset": 129}, {"referenceID": 27, "context": "Recently, non-linear techniques based on deep network has been showing impressive performance (Bengio, 2009; Watter et al., 2015).", "startOffset": 94, "endOffset": 129}], "year": 2016, "abstractText": "Direct contextual policy search methods learn to improve policy parameters and simultaneously generalize these parameters to different context or task variables. However, learning from high-dimensional context variables, such as camera images, is still a prominent problem in many real-world tasks. A naive application of unsupervised dimensionality reduction methods to the context variables, such as principal component analysis, is insufficient as task-relevant input may be ignored. In this paper, we propose a contextual policy search method in the model-based relative entropy stochastic search framework with integrated dimensionality reduction. We learn a model of the reward that is locally quadratic in both the policy parameters and the context variables. Furthermore, we perform supervised linear dimensionality reduction on the context variables by nuclear norm regularization. The experimental results show that the proposed method outperforms naive dimensionality reduction via principal component analysis and a state-ofthe-art contextual policy search method.", "creator": "LaTeX with hyperref package"}}}