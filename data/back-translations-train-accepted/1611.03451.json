{"id": "1611.03451", "review": {"conference": "AAAI", "VERSION": "v1", "DATE_OF_SUBMISSION": "10-Nov-2016", "title": "Importance Sampling with Unequal Support", "abstract": "Importance sampling is often used in machine learning when training and testing data come from different distributions. In this paper we propose a new variant of importance sampling that can reduce the variance of importance sampling-based estimates by orders of magnitude when the supports of the training and testing distributions differ. After motivating and presenting our new importance sampling estimator, we provide a detailed theoretical analysis that characterizes both its bias and variance relative to the ordinary importance sampling estimator (in various settings, which include cases where ordinary importance sampling is biased, while our new estimator is not, and vice versa). We conclude with an example of how our new importance sampling estimator can be used to improve estimates of how well a new treatment policy for diabetes will work for an individual, using only data from when the individual used a previous treatment policy.", "histories": [["v1", "Thu, 10 Nov 2016 19:11:09 GMT  (1654kb,D)", "http://arxiv.org/abs/1611.03451v1", null]], "reviews": [], "SUBJECTS": "cs.LG cs.AI stat.ML", "authors": ["philip s thomas", "emma brunskill"], "accepted": true, "id": "1611.03451"}, "pdf": {"name": "1611.03451.pdf", "metadata": {"source": "CRF", "title": "Importance Sampling with Unequal Support", "authors": ["Philip S. Thomas", "Emma Brunskill"], "emails": [], "sections": [{"heading": "Introduction", "text": "Sampling-sampling-sampling-sampling-sampling-sampling-sampling-sampling-sampling-sampling-sampling-sampling-sampling-sampling-sampling-sampling-sampling-sampling-sampling-sampling-sampling-sampling-sampling-sampling-sampling-sampling-sampling-sampling-sampling-sampling-sampling-sampling-sampling-sampling-sampling-sampling-sampling-sampling-sampling-sampling-sampling-sampling-sampling-sampling-sampling-sampling-sampling-sampling-sampling-sampling-sampling-sampling-sampling-sampling-sampling-sampling-sampling-sampling-sampling-sampling-sampling-sampling-sampling-sampling-sampling-sampling-sampling-sampling-sampling-sampling-sampling-sampling-sampling-sampling-sampling-sampling-sampling-sampling-sampling-sampling-sampling-sampling-sampling-sampling-sampling-sampling-sampling-sampling-sampling-sampling-sampling-for-sampling-sampling-sampling-sampling-sampling-sampling-sampling-sampling-sampling-sampling-sampling-sampling-sampling-sampling-sampling-sampling-sampling-sampling-sampling-sampling-sampling-sampling-sampling-sampling-sampling-sampling-sampling-sampling-sampling-sampling-sampling-sampling-sampling-sampling-sampling-sampling-"}, {"heading": "Problem Setting and Importance Sampling", "text": "Let us leave f and g the probability density functions (PDFs) for two distributions, which we call the target distribution and the sampling distribution (respectively), let us leave h: R \u2192 R is called the evaluation function (in this case only X), let us leave F: = Xi \u2032 R: f (x) 6 = 0), G: = {x) R: g (x) 6 = 0), and H: = (x) R: h), the evaluation variable (s) in expectation (x) 6 = 0), and H: = (x), the evaluation variant (x).L G: 10 Nov 201 6Sampling distributions, and the evaluation function, and the evaluation function. In this paper we will discuss techniques for estimating n > 0 i.d. sampling, Xn: =."}, {"heading": "Importance Sampling with Unequal Support", "text": "We propose a new Importance Sampling Estimator, Importance Sampling with unequal support (ISUS = US for short), which degenerates into the obvious estimator for our illustrative example. Intuitively, we can select from Xn the samples that are outside of F (or more generally outside of any group C that we will define later) to construct a new dataset, X \u2032 n, that has fewer samples. This new dataset can then be considered (# Xi, F) i.i.d. samples from another sample distribution - a distribution with PDF g \u2032 that is simple g \u2032, but is truncated to have only support for F and is normalized to merge into one."}, {"heading": "Theoretical Analysis of US", "text": "We start with two simple theorems that show the relationship between IS and US in place of known values. We start with two simple theorems that replace the relationship between IS and US. The proofs of both theorems are simple but deferred to the supplementary document. First, Theorem 1 shows that if C = G, US = G necessarily degenerates to IS. A case where C = G is, if the support of target distribution and evaluation function are both equal, if we replace the support of sample distribution in the US, i.e. if F = H = G necessarily degenerates to IS. If C = G, then US (Xn) = IS (Xn) = IS (Xn).Theorem 2 shows that if we replace c in the definition of the US by an empirical estimate, c = Xn): = k (Xn) / n, then US and IS are equal. This provides some intuitions for why US tendencies exceed IS, if IS is an estimate of C (irical), but G (irical)."}, {"heading": "Application to Illustrative Example", "text": "Since neither method is always superior, here we look at the application of IS and US to the illustrative example to see when each method works best and by how much. We look at the setting where C = F, but slightly modify the example. Firstly, although the target distribution is always the same, we increasingly allow their support to be scaled. Specifically, however, we define the support of f to be [0, Fmax], where Fmax [1], where Fmax [2], is small, it corresponds to significant differences in support, while large Fmax [if Fmax = 2, C = F = G and so on are equivalent to the two estimators. We also modify h to allow different values of inequality. Specifically, we define h (x) + if x < Fmax / 2 and h (x) = 1 +."}, {"heading": "Application to Diabetes Treatment", "text": "This year, it has come to the point that it has never come as far as it has this year."}, {"heading": "Conclusion and Future Work", "text": "Our analytical and empirical results indicate that the US can significantly outperform the ordinary sampling method, and we provide an a priori calculation to verify the rare cases in which it can do slightly worse. Unlike some other IS estimates designed to reduce variance (such as WIS), the US is unbiased in the face of mild conditions that still allow easy calculation of confidence intervals."}, {"heading": "Supplemental Document", "text": "In this supplementary document we prove the various properties and theorems referred to earlier (in particular those in Table 1). Property 1. If F-H-G then Eg [IS (Xn)] = \u03b8."}, {"heading": "Proof.", "text": "Eg [IS (Xn)] (a) = Eg [f (X) g (X) g (X) h (X)] (x) g (x) g (x) h (x) dx (b) = Xi (x) h (x) dx (X) = Ef (h) (h) (x) = f), where (a) applies because IS (Xn) is the mean of n independent and identically distributed random variables, and (b) applies because (x) G (F), f (x) = 0.We now provide evidence for Theorem 1, which states that if C = G, then US (Xn) = IS (Xn).Proof. In this setting c = Xi g (x) dx = 1 and since each Xi must be within C, k (Xn) = 1c."}, {"heading": "Proof.", "text": "Eg [US (Xn) | k (Xn) > 0] = n (Xn) = n (Xn) = n (Xn) = n (n) (n) (n) (n) (n) (n) (n) (n) (n) (n) (n) (n) (n) (n) (n) (n) (n) (n) (n) (n) (n) (n) (n) (n) (n) (n) (n) (n) (n) (n) (n) (n) (n) (n) (n) (n) (n) (n) (n) (n) (n) (n) (n) (n) (n) (n) (n) (n) (n) (n (n) (n) (n (n) (n) (n (n) (n (n) (n (n) (n) (n (n) (n) (n) (n) (n) (n) (n) (n) (n) (n) (n) (n) (n) (n) (n) (n) (n) (n) (n) (n) (n) (n) (n) (n) (n) (n) (n) (n) (n) (n) (n) (n) (n) (n) (n) (n) (n) (n) (n) (n) (n (n) (n) (n) (n) (n) (n) (n) (n (n) (n) (n) (n) (n) (n) (n (n) (n) (n) (n) (n) (n) (n (n) (n) n (n) (n) (n) (n) (n (n) n (n) (n) (n) (n) n (n (n) (n (n) (n) n) (n (n) (n) (n) n (n (n) n (n) (n) (n) (n) n (n) (n) (n (n) n (n) (n) n (n"}, {"heading": "Proof.", "text": "Eg [US (Xn)] = Pr (k (Xn) > 0) = 1 \u2212 (1 \u2212 c) n Eg [US (Xn) | k (Xn) > 0] = 0] = 0 (1 \u2212 c) n) Before proceeding, call the following property (whose completeness we prove): Property 2. Let X1,.., Xn n n be independent and identically distributed random variables, each with finite mean and variance. Then E (1nn) i = 1Xi) 2 = 1n Var (X1) + E [X1] + (X1] (1Xi] n (1Xi) (1) (1Xi) (1Xi) (1a)."}, {"heading": "Proof.", "text": "Varg (US (Xn) | k (Xn) > 0) = Eg (US (Xn) (US (Xn) > 0) \u2212 Eg (US (Xn) \u2212 Eg (US (Xn) > 0) Eg (US (Xn) = 1) \u2212 Eg (US (Xn) > 0) \u2212 Eg (US (Xn) \u2212 Eg (US (Xn) = 2) \u2212 Eg (US (Xn) \u2212 k (US) \u2212 k (Xn) \u2212 k (US (Xn) = 1) \u2212 k (US) \u2212 n (US (Xn) > 2) \u2212 c (US) \u2212 n (n) \u2212 n (US (Xn) > 2) \u2212 c (1), the elements of which are y1,.,. We also write yi: j to denote ith by jth entries of y, i.e., yi: j: [yi, yi + 1, yj yj \u2212 j]."}, {"heading": "Proof.", "text": "Varg (US (Xn)) = Eg [US (Xn) 2] \u2212 Eg [US (Xn)] 2 (a) = Eg [US (Xn) 2] \u2212 fic (Xn) 2 = (n) (k (Xn) = 0) (n) (n) (k (Xn) =) \u2212 fic (k (Xn) = 0) Eg [US (Xn) 2 | k (n) = (n) (k (Xn) = 0) (n) (US (Xn) 2 | k (Xn) = (n) (n) (k (Xn) = (n) (n) (n) (n) (n) (n) (n) (n) (n) (n) (n) (n) (n) (n), n (n), n (n), n (n), n (n, n, n (n), n (n, n, n, n (n), n (n, n, n, n, n (n), n (n, n, n, n (n), n (n, n, n, n, n (n), n (n, n, n, n, n (n), n (n, n, n, n, n (Xn), n, n (Xn), n (n (Xn), n (Xn), n (Xn (Xn) = (Xn (Xn) = (Xn) = (Xn (Xn), Eg (Xn) = (Xn (Xn) = (Xn) = (n (n) = (n (n), (n (n), (n (n), n (n (Xn) = (n (n), n (n (n), n (n), n (n (n (n), n (n (n), n (n (n), n (n (n), n (n (n), n (n (n), n (n (n), n (n (n (n), n (n (n), n (n), n (n (n), n), n (n"}, {"heading": "Proof.", "text": "Varg (IS (Xn)) (a) = 1 n Varg (IS (X))) = 1n (Eg [IS (X) 2] \u2212 Eg [IS (X)] 2) (b) = 1n (Eg [IS (X) 2] \u2212 n (IS (X) 2) \u2212 (X) 2 | X C] + Pr (X) \u2212 n \u2212 n (IS (X) 2 | X 6 C] n (X) n \u2212 n \u2212 n (IS (X) 2 \u2212 n) \u2212 n (IS (X) 2 (F \u2212 n) \u2212 n (X) (c) \u2212 n (C) \u2212 n) \u2212 n (c) \u2212 n \u2212 n (c) \u2212 n \u2212 n (c) \u2212 n (c) \u2212 n (c) \u2212 n (c) \u2212 n (f \u2212 n) \u2212 n) \u2212 n (c) \u2212 n (c) \u2212 n (c) \u2212 n (c) \u2212 n (c) \u2212 n (c) (c), c) (c) (c), c) (c), (c) (c), c), (x), (c), (c), (c), (c), (is) (x), (c), (c) (c), (c), (x), (x), (x), (x), (x), (x), (x), (x) (x), (x), (x), (x), (x) (x), (x) (x), (x) (x), (x) (x), (x), (x) (x), (x), (x) (x), (x) (x), (x) (x), (x) (x), (x), (x) (x), (x) (x), (x) (x), (x) (x), (x) (x), (x), (x) (x), (x) (x), (x) (x) (c), (c), (x), (x) (x), (x), (x), (x) (x),"}], "references": [{"title": "Model-free intelligent diabetes management using machine learning", "author": ["M. Bastani"], "venue": "Master\u2019s thesis, Department of Computing Science, University of Alberta,", "citeRegEx": "Bastani.,? \\Q2014\\E", "shortCiteRegEx": "Bastani.", "year": 2014}, {"title": "The uva/padova type 1 diabetes simulator new features", "author": ["C. Dalla Man", "F. Micheletto", "D. Lv", "M. Breton", "B. Kovatchev", "C. Cobelli"], "venue": "Journal of diabetes science and technology,", "citeRegEx": "Man et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Man et al\\.", "year": 2014}, {"title": "Doubly robust policy evaluation and learning", "author": ["M. Dud\u0131\u0301k", "J. Langford", "L. Li"], "venue": "In Proceedings of the TwentyEighth International Conference on Machine Learning,", "citeRegEx": "Dud\u0131\u0301k et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Dud\u0131\u0301k et al\\.", "year": 2011}, {"title": "Monte carlo methods for solving multivariable problems", "author": ["J.M. Hammersley"], "venue": "Annals of the New York Academy of Sciences,", "citeRegEx": "Hammersley.,? \\Q1960\\E", "shortCiteRegEx": "Hammersley.", "year": 1960}, {"title": "Monte carlo methods, methuen & co", "author": ["J.M. Hammersley", "D.C. Handscomb"], "venue": null, "citeRegEx": "Hammersley and Handscomb.,? \\Q1964\\E", "shortCiteRegEx": "Hammersley and Handscomb.", "year": 1964}, {"title": "Probability inequalities for sums of bounded random variables", "author": ["W. Hoeffding"], "venue": "Journal of the American Statistical Association,", "citeRegEx": "Hoeffding.,? \\Q1963\\E", "shortCiteRegEx": "Hoeffding.", "year": 1963}, {"title": "Doubly robust off-policy value evaluation for reinforcement learning", "author": ["N. Jiang", "L. Li"], "venue": "In International Conference on Machine Learning,", "citeRegEx": "Jiang and Li.,? \\Q2016\\E", "shortCiteRegEx": "Jiang and Li.", "year": 2016}, {"title": "Use of different Monte Carlo sampling techniques", "author": ["H. Kahn"], "venue": "Technical Report P-766,", "citeRegEx": "Kahn.,? \\Q1955\\E", "shortCiteRegEx": "Kahn.", "year": 1955}, {"title": "Concentration Inequalities and Model Selection", "author": ["P. Massart"], "venue": null, "citeRegEx": "Massart.,? \\Q2007\\E", "shortCiteRegEx": "Massart.", "year": 2007}, {"title": "Simulation and the Monte Carlo method", "author": ["R. Rubinstein"], "venue": null, "citeRegEx": "Rubinstein.,? \\Q1981\\E", "shortCiteRegEx": "Rubinstein.", "year": 1981}, {"title": "Data-efficient off-policy policy evaluation for reinforcement learning", "author": ["P.S. Thomas", "E. Brunskill"], "venue": "In International Conference on Machine Learning,", "citeRegEx": "Thomas and Brunskill.,? \\Q2016\\E", "shortCiteRegEx": "Thomas and Brunskill.", "year": 2016}], "referenceMentions": [{"referenceID": 7, "context": "A classical approach to this problem is to use importance sampling (IS), which reweighs the observed samples to account for the difference between the target and sampling distributions (Kahn, 1955).", "startOffset": 185, "endOffset": 197}, {"referenceID": 9, "context": "By contrast, weighted importance sampling (Rubinstein, 1981) is another variant of importance sampling that can reduce variance, but which introduces bias that makes it incompatible with Hoeffding\u2019s inequality.", "startOffset": 42, "endOffset": 60}, {"referenceID": 3, "context": "A control variate is a constant, t \u2208 R, that is subtracted from each h(Xi) and then added back to the final estimate, as in (1) (Hammersley, 1960; Hammersley and Handscomb, 1964).", "startOffset": 128, "endOffset": 178}, {"referenceID": 4, "context": "A control variate is a constant, t \u2208 R, that is subtracted from each h(Xi) and then added back to the final estimate, as in (1) (Hammersley, 1960; Hammersley and Handscomb, 1964).", "startOffset": 128, "endOffset": 178}, {"referenceID": 2, "context": "That is, by inducing a kind of normalization, a control variate can reduce the variance of estimates without introducing bias\u2014a property that has made the inclusion of control variates a popular topic in some recent works using importance sampling (Dud\u0131\u0301k et al., 2011; Jiang and Li, 2016; Thomas and Brunskill, 2016).", "startOffset": 248, "endOffset": 317}, {"referenceID": 6, "context": "That is, by inducing a kind of normalization, a control variate can reduce the variance of estimates without introducing bias\u2014a property that has made the inclusion of control variates a popular topic in some recent works using importance sampling (Dud\u0131\u0301k et al., 2011; Jiang and Li, 2016; Thomas and Brunskill, 2016).", "startOffset": 248, "endOffset": 317}, {"referenceID": 10, "context": "That is, by inducing a kind of normalization, a control variate can reduce the variance of estimates without introducing bias\u2014a property that has made the inclusion of control variates a popular topic in some recent works using importance sampling (Dud\u0131\u0301k et al., 2011; Jiang and Li, 2016; Thomas and Brunskill, 2016).", "startOffset": 248, "endOffset": 317}, {"referenceID": 8, "context": "Finally, we consider the use of IS and US to create highconfidence upper and lower bounds on \u03b8 using a concentration inequality (Massart, 2007) like Hoeffding\u2019s inequality (Hoeffding, 1963).", "startOffset": 128, "endOffset": 143}, {"referenceID": 5, "context": "Finally, we consider the use of IS and US to create highconfidence upper and lower bounds on \u03b8 using a concentration inequality (Massart, 2007) like Hoeffding\u2019s inequality (Hoeffding, 1963).", "startOffset": 172, "endOffset": 189}, {"referenceID": 0, "context": "If follow-up visits are not an option, recent research has suggested using reinforcement learning algorithms to tune CR and CF (Bastani, 2014).", "startOffset": 127, "endOffset": 142}, {"referenceID": 0, "context": "If follow-up visits are not an option, recent research has suggested using reinforcement learning algorithms to tune CR and CF (Bastani, 2014). Here we focus on a sub-problem of improving CR and CF\u2014using data collected from an initial range of admissible values of CR and CF to predict how well a new range of values for CR and CF would perform. When collecting data, CR and CF are drawn uniformly from an initial admissible range, and then used for one day (which we view as one episode of a Markov decision process). The performance during each day is measured using an objective function similar to the reward function proposed by Bastani (2014), which measures the deviation of blood glucose from optimum levels, with larger penalties for low blood glucose levels.", "startOffset": 128, "endOffset": 649}, {"referenceID": 0, "context": "If follow-up visits are not an option, recent research has suggested using reinforcement learning algorithms to tune CR and CF (Bastani, 2014). Here we focus on a sub-problem of improving CR and CF\u2014using data collected from an initial range of admissible values of CR and CF to predict how well a new range of values for CR and CF would perform. When collecting data, CR and CF are drawn uniformly from an initial admissible range, and then used for one day (which we view as one episode of a Markov decision process). The performance during each day is measured using an objective function similar to the reward function proposed by Bastani (2014), which measures the deviation of blood glucose from optimum levels, with larger penalties for low blood glucose levels. We refer to the measure of how good the outcome was from one day as the return associated with that day, with larger values being better. Using approximately 30 days of data, our goal is to estimate the expected return if a different distribution of CR and CF were to be used. We consider a specific in silico person\u2014a person simulated using a metabolic simulator. We used the subject \u201cAdult#003\u201d in the Type 1 Diabetes Metabolic Simulator (T1DMS) (Dalla Man et al., 2014)\u2014a simulator that has been approved by the US Food and Drug Administration as a substitute for animal trials in pre-clinical testing of treatment policies for type 1 diabetes. During each day, the subject is given three or four meals of randomized sizes at randomized times, similar to the experimental setup proposed by Bastani (2014). As a result of this randomness, and the stochastic nature of the T1DMS model, applying the same values of CR and CF can produce different returns if used for multiple days.", "startOffset": 128, "endOffset": 1577}, {"referenceID": 0, "context": "If follow-up visits are not an option, recent research has suggested using reinforcement learning algorithms to tune CR and CF (Bastani, 2014). Here we focus on a sub-problem of improving CR and CF\u2014using data collected from an initial range of admissible values of CR and CF to predict how well a new range of values for CR and CF would perform. When collecting data, CR and CF are drawn uniformly from an initial admissible range, and then used for one day (which we view as one episode of a Markov decision process). The performance during each day is measured using an objective function similar to the reward function proposed by Bastani (2014), which measures the deviation of blood glucose from optimum levels, with larger penalties for low blood glucose levels. We refer to the measure of how good the outcome was from one day as the return associated with that day, with larger values being better. Using approximately 30 days of data, our goal is to estimate the expected return if a different distribution of CR and CF were to be used. We consider a specific in silico person\u2014a person simulated using a metabolic simulator. We used the subject \u201cAdult#003\u201d in the Type 1 Diabetes Metabolic Simulator (T1DMS) (Dalla Man et al., 2014)\u2014a simulator that has been approved by the US Food and Drug Administration as a substitute for animal trials in pre-clinical testing of treatment policies for type 1 diabetes. During each day, the subject is given three or four meals of randomized sizes at randomized times, similar to the experimental setup proposed by Bastani (2014). As a result of this randomness, and the stochastic nature of the T1DMS model, applying the same values of CR and CF can produce different returns if used for multiple days. After analyzing the performance of many CR and CF pairs, we selected an initial range that results in good performance: CR \u2208 [8.5, 11] and CF \u2208 [10, 15]. Using a large number of samples, we computed an estimate of the expected return if different CR and CF values are used for a single day\u2014this estimate is depicted in Figure 2. As described by Bastani (2014), when the value of CR is set appropriately, performance is robust to changes in CF.", "startOffset": 128, "endOffset": 2111}], "year": 2016, "abstractText": "Importance sampling is often used in machine learning when training and testing data come from different distributions. In this paper we propose a new variant of importance sampling that can reduce the variance of importance samplingbased estimates by orders of magnitude when the supports of the training and testing distributions differ. After motivating and presenting our new importance sampling estimator, we provide a detailed theoretical analysis that characterizes both its bias and variance relative to the ordinary importance sampling estimator (in various settings, which include cases where ordinary importance sampling is biased, while our new estimator is not, and vice versa). We conclude with an example of how our new importance sampling estimator can be used to improve estimates of how well a new treatment policy for diabetes will work for an individual, using only data from when the individual used a previous treatment policy. Introduction A key challenge in artificial intelligence is to estimate the expectation of a random variable. Instances of this problem arise in areas ranging from planning and decision making (e.g., estimating the expected sum of rewards produced by a policy for decision making under uncertainty) to probabilistic inference. Although the estimation of an expected value is straightforward if we can generate many independent and identically distributed (i.i.d.) samples from the relevant probability distribution (which we refer to as the target distribution), we may not have generative access to the target distribution. Instead, we might only have data from a different distribution that we call the sampling distribution. For example, in off-policy evaluation for reinforcement learning, the goal is to estimate the expected sum of rewards that a decision policy will produce, given only data gathered using some other policy. Similarly, in supervised learning, we may wish to predict the performance of a regressor or classifier if it were to be applied to data that comes from a distribution that differs from the distribution of the available data (e.g., we might predict the accuracy of a classifier for hand-written letters given that observed letter frequencies come from English, using a corpus of labeled letters collected from German documents). More precisely, we consider the problem of estimating \u03b8 := E[h(X)], where h is a real-valued function and the expectation is over the random variable X , which is a sample from the target distribution. As input we assume access to n i.i.d. samples from a sampling distribution that is different from the target distribution. A classical approach to this problem is to use importance sampling (IS), which reweighs the observed samples to account for the difference between the target and sampling distributions (Kahn, 1955). Importance sampling produces an unbiased but often highvariance estimate of \u03b8. We introduce importance sampling with unequal support (US)\u2014a simple new importance sampling estimator that can drastically reduce the variance of importance sampling when the supports of the sampling and target distributions differ. This setting with unequal support can occur, for example, in our earlier example where German documents might include symbols like \u00df, that the classifier will not encounter. US essentially performs importance sampling only on the data that falls within the support of the target distribution, and then scales this estimate by a constant that reflects the relative support of the target and sampling distributions. US typically has lower variance than ordinary importance sampling (sometimes by orders of magnitude), and is unbiased in the important setting where at least one sample falls within the support of the target distribution. If no samples do, then none of the available data could have been generated by the target distribution, and so it is unclear what would make for a reasonable estimate. Furthermore, the conditionally unbiased nature of US is sufficient to allow for its use with concentration inequalities like Hoeffding\u2019s inequality to construct confidence bounds on \u03b8. By contrast, weighted importance sampling (Rubinstein, 1981) is another variant of importance sampling that can reduce variance, but which introduces bias that makes it incompatible with Hoeffding\u2019s inequality. Problem Setting and Importance Sampling Let f and g be probability density functions (PDFs) for two distributions that we call the target distribution and sampling distribution, respectively. Let h : R \u2192 R be called the evaluation function. Let \u03b8 := Ef [h(X)], where Ef denotes the expected value given that f is the PDF of the random variable(s) in the expectation (in this case, just X). Let F := {x \u2208 R : f(x) 6= 0}, G := {x \u2208 R : g(x) 6= 0}, and H := {x \u2208 R : h(x) 6= 0} be the supports of the target and ar X iv :1 61 1. 03 45 1v 1 [ cs .L G ] 1 0 N ov 2 01 6 sampling distributions, and the evaluation function, respectively. In this paper we will discuss techniques for estimating \u03b8 given n \u2208 N>0 i.i.d. samples, Xn := {X1, . . . , Xn}, from the sampling distribution, and we focus on the setting where F \u2229H \u2282 G\u2014where the joint support of F and H is a strict subset of the support of G. The importance sampling estimator, IS(Xn) := t+ 1 n n \u2211 i=1 f(Xi) g(Xi) (h(Xi)\u2212 t), (1) is a widely used estimator of \u03b8, where t = 0 (we consider non-zero values of t later). If F \u2229 H \u2286 G, then IS(Xn) is a consistent and unbiased estimator of \u03b8. That is, IS(Xn) a.s. \u2212\u2192 \u03b8 and Eg[IS(Xn)] = \u03b8 (we review this latter result in Property 1 in the supplemental document). A control variate is a constant, t \u2208 R, that is subtracted from each h(Xi) and then added back to the final estimate, as in (1) (Hammersley, 1960; Hammersley and Handscomb, 1964). Although control variates, t(Xi), that depend on the sample, Xi, can be beneficial, for our later purposes we only consider constant control variates. Intuitively, including a constant control variate equates to estimating \u03b8\u2032 := Ef [h\u2032(X)] using importance sampling without a control variate, where h\u2032(x) = h(x) \u2212 t, and then adding t to the resulting estimate to get an estimate of \u03b8. Later we show that the variance of importance sampling increases with \u03b8, and so applying importance sampling to h results in higher variance than applying importance sampling to h\u2032 with t \u2248 \u03b8, since then \u03b8\u2032 \u2248 0. That is, by inducing a kind of normalization, a control variate can reduce the variance of estimates without introducing bias\u2014a property that has made the inclusion of control variates a popular topic in some recent works using importance sampling (Dud\u0131\u0301k et al., 2011; Jiang and Li, 2016; Thomas and Brunskill, 2016). Although later we discuss control variates more, for simplicity our derivations focus on importance sampling estimators without control variates. There are also other extensions of the importance sampling estimator that can reduce variance\u2014notably the weighted importance sampling estimator, which we compare to later, and which can provide large reductions of variance and mean squared error, but which introduces bias. An Illustrative Example In this section we present an example that highlights the peculiar behavior of the IS estimator when F \u2229 H 6= G. Let g(x) = 0.5 if x \u2208 [0, 2] and g(x) = 0 otherwise, and let f(x) = 1 if x \u2208 [0, 1] and f(x) = 0 otherwise. So, F = [0, 1] and G = [0, 2]. Let h(x) = 1 if x \u2208 [0, 1] and h(x) = 0 otherwise, so that H = [0, 1]. Notice that \u03b8 = 1. Since the sampling and target distributions are both uniform, an obvious estimator of \u03b8 (if f and g are known but h is not) would be the average of the points that fall within F . Let (#Xi \u2208 F ) denote the number of samples in Xn that are in F . Formally, the obvious estimator is \u03b8\u0302 := 1 (#Xi \u2208 F ) n \u2211 i=1 1F (Xi)h(Xi), where 1A(x) = 1 if x \u2208 A and 1A(x) = 0 otherwise. Given our knowledge of h, it is straightforward to show that this estimator is equal to 1 if (#Xi \u2208 F ) > 0 and is undefined otherwise\u2014it is exactly correct (has zero bias and variance) as long as at least one sample falls within F . If no samples fall within F , then we have only observed data that will never occur under the target distribution, and so we have no useful information about \u03b8. In this case, we might define our obvious estimator to return an arbitrary value, e.g., zero. Perhaps surprisingly, the importance sampling estimator does not degenerate to this obvious estimator:", "creator": "LaTeX with hyperref package"}}}