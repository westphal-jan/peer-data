{"id": "1611.03949", "review": {"conference": "acl", "VERSION": "v1", "DATE_OF_SUBMISSION": "12-Nov-2016", "title": "Linguistically Regularized LSTMs for Sentiment Classification", "abstract": "Sentiment understanding has been a long-term goal of AI in the past decades. This paper deals with sentence-level sentiment classification. Though a variety of neural network models have been proposed very recently, however, previous models either depend on expensive phrase-level annotation, whose performance drops substantially when trained with only sentence-level annotation; or do not fully employ linguistic resources (e.g., sentiment lexicons, negation words, intensity words), thus not being able to produce linguistically coherent representations. In this paper, we propose simple models trained with sentence-level annotation, but also attempt to generating linguistically coherent representations by employing regularizers that model the linguistic role of sentiment lexicons, negation words, and intensity words. Results show that our models are effective to capture the sentiment shifting effect of sentiment, negation, and intensity words, while still obtain competitive results without sacrificing the models' simplicity.", "histories": [["v1", "Sat, 12 Nov 2016 03:55:10 GMT  (741kb,D)", "http://arxiv.org/abs/1611.03949v1", null], ["v2", "Tue, 25 Apr 2017 18:29:58 GMT  (825kb,D)", "http://arxiv.org/abs/1611.03949v2", null]], "reviews": [], "SUBJECTS": "cs.CL", "authors": ["qiao qian", "minlie huang", "jinhao lei", "xiaoyan zhu"], "accepted": true, "id": "1611.03949"}, "pdf": {"name": "1611.03949.pdf", "metadata": {"source": "CRF", "title": "Linguistically Regularized LSTMs for Sentiment Classification", "authors": ["Qiao Qian", "Minlie Huang", "Xiaoyan Zhu"], "emails": ["qianqiaodecember29@126.com,", "aihuang@tsinghua.edu.cn,", "zxy-dcs@tsinghua.edu.cn"], "sections": [{"heading": "Introduction", "text": "In fact, most of them are able to survive on their own if they do not put themselves in a position to survive on their own."}, {"heading": "Related Work", "text": "The most groundbreaking (perhaps) model may be the recursive autoencoder neural network, which recursively builds the representation of a set of subphrases (Socher et al. 2011; Socher et al. 2013; Dong et al. 2014; Qian et al. 2015). Such recursive models usually depend on the tree structure of the input text and usually require heavy annotations on each subphrase to obtain competitive results. Sequence models do not depend on a particular tree structure, for example, Convolutionary Neural Network (CNN et al) is another type of widely used sensitivity classification models (Kim 2014; Kalchbrenner, Grefenstette, and Blunsom 2014). Similarly used in image processing, CNN Convolution defines operations on a sequence of a text. Long-term memory models are also common for the presentation of learning sets."}, {"heading": "Long Short-term Memory Network", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "Long Short-Term Memory (LSTM)", "text": "In order to deal with the notorious problems of gradient explosion and disappearance in recurrent neural networks (Hochreiter and Schmidhuber 1997), a long-term short-term memory network is proposed by adding an additional memory cell ct-Rd in each step. Hidden states ht-1 and ct-1, as well as the input vector xt, are formally defined as follows: ct, ht = g (LSTM) (ct-1, ht-1, xt) (1) The hidden state ht-Rd denotes the representation of position t and also encodes the preceding context of the position. For further details on LSTM, please refer to the readers (Chung et al. 2014)."}, {"heading": "Bidirectional LSTM", "text": "The bidirectional LSTM (Graves, Jaitly and Mohamed 2013) used two parallel transitions (forward and backward) and concatenated representations of the two LSTMs as representations of each position. The forward and backward LSTMs are each formulated as follows: \u2212 \u2192 c t, \u2212 \u2192 h t = g (LSTM) (\u2212 \u2192 c t \u2212 1, \u2212 h t \u2212 1, xt) (2) \u2190 \u2212 c t, \u2190 \u2212 h t = g (LSTM) (\u2190 c t + 1, \u2190 \u2212 h t + 1, xt) (3), where g (LSTM) is the same as in Equation 1. Parameters in particular are divided into the two LSTMs. The representation of the entire set is [\u2212 \u2192 h n, \u2212 h 1], where n can be considered the length of the set as equivalent."}, {"heading": "Linguistically Regularized LSTM", "text": "The central idea of the paper is to make linguistically coherent predictions in the sentiment classification by regulating the results at adjacent positions of a sentence. For example, in the sentence \"this film is interesting,\" the predicted sentiment distribution should be almost the same for \"this * 1,\" \"this movie *\" and \"this movie is *\" while the predicted sentiment distribution for \"this film is very interesting *\" should be quite different from the predicted positions, since a sentiment word (\"interesting\") will see. More formal is the predicted sentiment distribution (pt, based on Eq. 4) for position t linguistically1The asterisk denotes the current position.regulates with respect to the predicted word (t \u2212 1) or the following (t + 1) regulator positions: We propose a general regulator position and three specific regulator positions, which should be based on the following linguistic observations: non-regulator-regulator-two-each of the two adjacent positions."}, {"heading": "Non-Sentiment Regularizer (NSR)", "text": "This regulator restricts that the mood distributions of adjacent positions should not vary greatly if the additional input word xt is not a mood word, formally as follows: L (NSR) t = max (0, DKL (pt, pt \u2212 1) \u2212 M) (5), where M is a hyperparameter for margin, pt is the predicted distribution at position t whose representation is ht, and DKL (p, q) is a symmetric KL divergence defined as follows: DKL = 12 C \u0445 l = 1 p (l) log q (l) log q (l) log p (l), where p, q distributions are over mood names."}, {"heading": "Sentiment Regularizer (SR)", "text": "The sentiment regulator restricts that the sentiment distributions of adjacent positions should drift accordingly if the input word is a sentiment word. Let's look again at the example of \"this movie is interesting.\" At position t = 4 we see a positive word \"interesting,\" so that the predicted distribution at this position would be more positive than at position t = 3. This is the problem of the sentiment drift. To address the sentiment drift problem, we propose a polarity shift for every sentiment class defined in a lexicon. For example, a sentiment lexicon may have class names such as strong positive, weak positive, weak negative, and strongly negative, and for each class there is a shift in the distribution that is learned by the model. The sentiment regulator determines that if the current word is a sentiment word, the sentiment distribution should be observed in comparison to the previous position, the sentiment distribution (sentiment distribution is followed by the model) (however, the sentiment distribution (class + class = class), class (class), class (class = class), class (Sentipt), Sentipt, class (Class), class (Class), class (Sentipt)"}, {"heading": "Negation Regularizer (NR)", "text": "If the input word xt is a negation word, the negation role should be shifted accordingly. However, the role of negation is more complex than that of emotional words, e.g. the word \"not\" in \"not good\" and \"not bad\" have different roles in the polarity shift. The former changes the polarity to negative, while the latter changes to neutral instead of positive. To respect such complex negation effects, we suggest a transformation matrix Tm \"in\" not good \"and\" not bad \"for each negation word m, and the matrix is learned from the model. The regulator assumes that if the current position is a negation word, the mood distribution of the current position should be close to the next or previous position with the transformation word m. p (NR) t \u2212 1 = soft (Txj) t \u2212 pt = soft (Txj) x \u2212 pt = Dxpt (R + 1 max (R) (R) (N) max (R) (R) p (1)."}, {"heading": "Intensity Regularizer (IR)", "text": "The Intensity Regulator models how intensity words affect the mood assessment of a phrase or sentence. Intensity can change the value of the content of a sentence. The intensity of the sensation of a sentence indicates the strength of the feeling associated with it, which is very important for a fine-grained mood classification or evaluation. The formulation of the intensity effect is exactly the same as with the negation regulator, but of course with different parameters. For each intensity word there is a transformation matrix to favor the different roles of the different intensity amplifiers in mood shifts. We will not repeat the formulas here shortly."}, {"heading": "Applying Linguistic Regularizers to Bidirectional LSTM", "text": "In order to make our model simple and elegant in mathematical form, we do not consider the scope of change of the negation and intensity word, which is quite a challenging problem in the NLP community. However, we can alleviate the problem by using bidirectional LSTM words. However, sometimes the modified words are on the left side of negation and intensity words. To better solve this problem, we use bidirectional LSTM and let the model determine which side of negation and intensity words should be used. Formally, in Bi-LSTM we calculate a changed mood distribution on \u2212 \u2192 p t \u2212 1 of the forward LSTM and also on p \u2212 p of the backward LSTM state. \u2212 t \u2212 t and calculate the minimum distance of the distribution of the current position to the two distributions."}, {"heading": "Discussion", "text": "Unlike previous studies of negation and intensity words, which modulate the linguistic effect of these words by some predefined rules, our models address these factors with mathematical operations, parameterized with changing distribution vectors and transformation matrices. In the Sentiment Regularizer, the sentiment shifting effect is parameterized with a class-specific distribution (but could also be word-specific if with more data). In the negation and intensity regulators, the effect is parameterized with word-specific transformation matrices, which means that different words have different parameters. Since the mechanism by which negation and intensity words shift the expression of feelings is quite complex and highly dependent on individual words, we believe that such a mathematical operation is better suited to address complex linguistic roles of these words. This is a major advantage of our approach compared to other methods."}, {"heading": "Experiment", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "Dataset and Sentiment Lexicon", "text": "To evaluate the proposed models, two sets of data are used: Movie Review (MR) (Pang and Lee 2005), which has two classes as negative, positive, and Stanford Sentiment Treebank (SST) (Socher et al. 2013), which has five classes. For details, we refer readers to the two essays. SST has provided phrase-level annotations for all inner nodes, but we only use sentence-level annotations, as one of our goals is to avoid costly phrase-level annotations.The Sentiment lexicon consists of two parts. The first part comes from MPQA (Wilson, Wiebe, and Hoffmann 2005), which contains 5, 153 sentiment words, each with polarity classification; the second part consists of the sheet nodes of the SST dataset (i.e., all sentiment words), and there are 6, 886 polar words other than neural ones. We combine the two parts and ignore those that can produce a lexical, a lexical, and a lexical, 750-word number of which can be limited."}, {"heading": "Overall Comparison", "text": "The baselines are listed as follows: \u2022 RNN / RNTN Recursive Neural Network over parsingtrees, proposed by (Socher et al. 2011) and Recursive Tensor Neural Network (Socher et al. 2013) employs tensors to model correlations between different dimensions of child-node vectors. \u2022 LSTM / Bi-LSTM Long-short Term Memory (Cho et al. 2014) and the bidirectional variant as previously introduced. \u2022 Tree-LSTM Tree-Structured Long Short-Term Memory (Tai, Socher, and Manning 2015) introduces memory cells and gates into tree-structured neural networks. \u2022 CNN Convolutional Neural Network (Kalchbrenner, Grefenstette, and Blunsom 2014) generates presentation by convolution and pooling operations."}, {"heading": "The Effect of Different Regularizers", "text": "Each time, we remove a regulator and observe how the performance varies. First, we perform this experiment on the whole data set, and then we experiment with partial data sets containing only negative or intensity words.The results of the experiment are presented in Table 4, where we can see that the non-emotion regulator and the mood regulator play a key role2, and the negation regulator and intensity regulator are effective but less important than the two previous regulators. This may be because only 14% of the sets contain negative words in the test datasets and 23% contain intensity words, so we continue to evaluate the models on two subsets, as shown in Table 5.Experiments on the subsets: 1) With linguistic regulators, LR-Bi-LSTM outperforms the Bi-STM remarkably."}, {"heading": "The Effect of the Negation Regularizer", "text": "To further demonstrate the linguistic role of negative words, we compare the predicted sentiment distributions of a phrase pair with or without a negative word. The experimental results of the MR are shown in Fig. 1. Each point denotes a phrase pair (e.g. < interesting, not interesting >), with the x-axis denoting the positive score of the phrase without negators (e.g. interesting) and the y-axis denoting the positive score of the phrase with negators (e.g. not interesting).The curves in the numbers show this function: [1 \u2212 y, y] = softmax (Tnw \u0445 [1 \u2212 x, x]), where [1 \u2212 r, r] is a sentiment distribution on [negative, positive], x is the positive score of the phrase without negators (x-axis) and the negative score of the phrase with negators (y-axis), and Tnw is the transformation matrix for the negative word nw."}, {"heading": "The Effect of the Intensity Regularizer", "text": "In order to further demonstrate the linguistic role of intensity words, we conduct experiments with the SST dataset, as shown in Figure 2. We show the confusion matrix, which shows how mood changes after it has been modified by intensifiers. For example, the number 20 in the first matrix means that there are 20 phrases whose sensation class changes from negative (-) to very negative (-) after it has been modified by an intensity word \"very.\" As the results show, for \"most\" there are 21 / 21 / 13 / 12 phrases whose sensation is shifted from negative to very negative (e.g. most irresponsible image), positive to very positive (e.g. most famous author), neutral to negative (e.g. clearest) and neutral to positive (e.g. narrowest). Similar observations can be found with the word \"very.\" There are also many phrases that maintain their sensation."}, {"heading": "Conclusion and Future Work", "text": "The proposed models address the emotional displacement of sentiment, negation, and intensity words in order to produce linguistically coherent representations. Furthermore, our models are sequence LSTMs that do not depend on a parsing tree structure and do not require expensive phrase-level annotation to achieve competitive results; the results show that our models are capable of addressing the linguistic role of sentiment, negation, and intensity words. To maintain the simplicity of the proposed models, we do not fully consider the scope of modification of negation and intensity words, although we address this problem in part by applying a minimization operator (see Equation 10, Equation 13) and bi-directional LSTM. As a future work, we plan to apply the linguistic regulators to tree LSTM to solve the parsing tree problem, as it is easier to specify the scope of modification."}], "references": [{"title": "Learning phrase representations using rnn encoderdecoder for statistical machine translation", "author": ["Cho"], "venue": "arXiv preprint arXiv:1406.1078", "citeRegEx": "Cho,? \\Q2014\\E", "shortCiteRegEx": "Cho", "year": 2014}, {"title": "Empirical evaluation of gated recurrent neural networks on sequence modeling", "author": ["Chung"], "venue": "arXiv preprint arXiv:1412.3555", "citeRegEx": "Chung,? \\Q2014\\E", "shortCiteRegEx": "Chung", "year": 2014}, {"title": "Adaptive multi-compositionality for recursive neural models with applications to sentiment analysis", "author": ["Dong"], "venue": null, "citeRegEx": "Dong,? \\Q2014\\E", "shortCiteRegEx": "Dong", "year": 2014}, {"title": "Hybrid speech recognition with deep bidirectional lstm", "author": ["Jaitly Graves", "A. Mohamed 2013] Graves", "N. Jaitly", "A.-r. Mohamed"], "venue": "In Automatic Speech Recognition and Understanding (ASRU),", "citeRegEx": "Graves et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Graves et al\\.", "year": 2013}, {"title": "and Schmidhuber", "author": ["S. Hochreiter"], "venue": "J.", "citeRegEx": "Hochreiter and Schmidhuber 1997", "shortCiteRegEx": null, "year": 1997}, {"title": "and Liu", "author": ["M. Hu"], "venue": "B.", "citeRegEx": "Hu and Liu 2004", "shortCiteRegEx": null, "year": 2004}, {"title": "A convolutional neural network for modelling sentences", "author": ["Grefenstette Kalchbrenner", "N. Blunsom 2014] Kalchbrenner", "E. Grefenstette", "P. Blunsom"], "venue": null, "citeRegEx": "Kalchbrenner et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Kalchbrenner et al\\.", "year": 2014}, {"title": "and Inkpen", "author": ["A. Kennedy"], "venue": "D.", "citeRegEx": "Kennedy and Inkpen 2006", "shortCiteRegEx": null, "year": 2006}, {"title": "S", "author": ["S. Kiritchenko", "Mohammad"], "venue": "M.", "citeRegEx": "Kiritchenko and Mohammad 2016", "shortCiteRegEx": null, "year": 2016}, {"title": "Distributional semantic models for affective text analysis", "author": ["Malandrakis"], "venue": "IEEE Transactions on Audio, Speech, and Language Processing", "citeRegEx": "Malandrakis,? \\Q2013\\E", "shortCiteRegEx": "Malandrakis", "year": 2013}, {"title": "and Lee", "author": ["B. Pang"], "venue": "L.", "citeRegEx": "Pang and Lee 2005", "shortCiteRegEx": null, "year": 2005}, {"title": "and Lee", "author": ["B. Pang"], "venue": "L.", "citeRegEx": "Pang and Lee 2008", "shortCiteRegEx": null, "year": 2008}, {"title": "Thumbs up?: sentiment classification using machine learning techniques", "author": ["Lee Pang", "B. Vaithyanathan 2002] Pang", "L. Lee", "S. Vaithyanathan"], "venue": null, "citeRegEx": "Pang et al\\.,? \\Q2002\\E", "shortCiteRegEx": "Pang et al\\.", "year": 2002}, {"title": "and Zaenen", "author": ["L. Polanyi"], "venue": "A.", "citeRegEx": "Polanyi and Zaenen 2006", "shortCiteRegEx": null, "year": 2006}, {"title": "Learning tag embeddings and tag-specific composition functions in recursive neural network", "author": ["Qian"], "venue": "In ACL,", "citeRegEx": "Qian,? \\Q2015\\E", "shortCiteRegEx": "Qian", "year": 2015}, {"title": "C", "author": ["R. Socher", "J. Pennington", "E.H. Huang", "A.Y. Ng", "Manning"], "venue": "D.", "citeRegEx": "Socher et al. 2011", "shortCiteRegEx": null, "year": 2011}, {"title": "A", "author": ["R. Socher", "A. Perelygin", "J.Y. Wu", "J. Chuang", "C.D. Manning", "Ng"], "venue": "Y.; and Potts, C.", "citeRegEx": "Socher et al. 2013", "shortCiteRegEx": null, "year": 2013}, {"title": "Lexicon-based methods for sentiment analysis", "author": ["Taboada"], "venue": "Computational linguistics", "citeRegEx": "Taboada,? \\Q2011\\E", "shortCiteRegEx": "Taboada", "year": 2011}, {"title": "C", "author": ["K.S. Tai", "R. Socher", "Manning"], "venue": "D.", "citeRegEx": "Tai. Socher. and Manning 2015", "shortCiteRegEx": null, "year": 2015}, {"title": "P", "author": ["Turney"], "venue": "D.", "citeRegEx": "Turney 2002", "shortCiteRegEx": null, "year": 2002}, {"title": "Ecnu at semeval-2016 task 7: An enhanced supervised learning method for lexicon sentiment intensity ranking", "author": ["Zhang Wang", "F. Lan 2016] Wang", "Z. Zhang", "M. Lan"], "venue": "Proceedings of SemEval 491\u2013496", "citeRegEx": "Wang et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Wang et al\\.", "year": 2016}, {"title": "A regression approach to affective rating of chinese words from anew", "author": ["Wu Wei", "W.-L. Lin 2011] Wei", "C.-H. Wu", "J.C. Lin"], "venue": "In Affective Computing and Intelligent Interaction", "citeRegEx": "Wei et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Wei et al\\.", "year": 2011}, {"title": "Recognizing contextual polarity in phrase-level sentiment analysis", "author": ["Wiebe Wilson", "T. Hoffmann 2005] Wilson", "J. Wiebe", "P. Hoffmann"], "venue": null, "citeRegEx": "Wilson et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Wilson et al\\.", "year": 2005}, {"title": "An empirical study on the effect of negation words on sentiment", "author": ["Zhu"], "venue": null, "citeRegEx": "Zhu,? \\Q2014\\E", "shortCiteRegEx": "Zhu", "year": 2014}, {"title": "Long short-term memory over recursive structures", "author": ["Sobhani Zhu", "X. Guo 2015] Zhu", "P. Sobhani", "H. Guo"], "venue": null, "citeRegEx": "Zhu et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Zhu et al\\.", "year": 2015}], "referenceMentions": [], "year": 2016, "abstractText": "Sentiment understanding has been a long-term goal of AI in the past decades. This paper deals with sentence-level sentiment classification. Though a variety of neural network models have been proposed very recently, however, previous models either depend on expensive phrase-level annotation, whose performance drops substantially when trained with only sentence-level annotation; or do not fully employ linguistic resources (e.g., sentiment lexicons, negation words, intensity words), thus not being able to produce linguistically coherent representations. In this paper, we propose simple models trained with sentence-level annotation, but also attempt to generating linguistically coherent representations by employing regularizers that model the linguistic role of sentiment lexicons, negation words, and intensity words. Results show that our models are effective to capture the sentiment shifting effect of sentiment, negation, and intensity words, while still obtain competitive results without sacrificing the models\u2019 simplicity.", "creator": "LaTeX with hyperref package"}}}