{"id": "1611.04021", "review": {"conference": "AAAI", "VERSION": "v1", "DATE_OF_SUBMISSION": "12-Nov-2016", "title": "Leveraging Video Descriptions to Learn Video Question Answering", "abstract": "We propose a scalable approach to learn video-based question answering (QA): answer a \"free-form natural language question\" about a video content. Our approach automatically harvests a large number of videos and descriptions freely available online. Then, a large number of candidate QA pairs are automatically generated from descriptions rather than manually annotated. Next, we use these candidate QA pairs to train a number of video-based QA methods extended fromMN (Sukhbaatar et al. 2015), VQA (Antol et al. 2015), SA (Yao et al. 2015), SS (Venugopalan et al. 2015). In order to handle non-perfect candidate QA pairs, we propose a self-paced learning procedure to iteratively identify them and mitigate their effects in training. Finally, we evaluate performance on manually generated video-based QA pairs. The results show that our self-paced learning procedure is effective, and the extended SS model outperforms various baselines.", "histories": [["v1", "Sat, 12 Nov 2016 17:15:57 GMT  (1315kb,D)", "http://arxiv.org/abs/1611.04021v1", "7 pages, 5 figures. Accepted to AAAI 2017"], ["v2", "Mon, 19 Dec 2016 16:07:33 GMT  (2585kb,D)", "http://arxiv.org/abs/1611.04021v2", "7 pages, 5 figures. Accepted to AAAI 2017. Camera-ready version"]], "COMMENTS": "7 pages, 5 figures. Accepted to AAAI 2017", "reviews": [], "SUBJECTS": "cs.CV cs.AI cs.MM", "authors": ["kuo-hao zeng", "tseng-hung chen", "ching-yao chuang", "yuan-hong liao", "juan carlos niebles", "min sun"], "accepted": true, "id": "1611.04021"}, "pdf": {"name": "1611.04021.pdf", "metadata": {"source": "CRF", "title": "Leveraging Video Descriptions to Learn Video Question Answering", "authors": ["Kuo-Hao Zeng", "Tseng-Hung Chen", "Ching-Yao Chuang", "Yuan-Hong Liao", "Juan Carlos Niebles", "Min Sun"], "emails": ["jniebles}@cs.stanford.edu", "{s104061544@m104,", "s102061145@m102,", "s102061137@m102,", "sunmin@ee}.nthu.edu.tw"], "sections": [{"heading": "Introduction", "text": "In fact, most of them are able to decide for themselves what they want and what they want. In fact, most of them are able to decide what they want and what they don't want."}, {"heading": "Related Work", "text": "In fact, it is the case that most people are able to determine for themselves what they want and what they want. In fact, it is the case that most people are able to determine for themselves what they want and what they want. In fact, it is the case that most people are able to determine for themselves what they want and what they do not want. In fact, it is the case that people are able to determine for themselves what they want and what they do not want. In fact, it is the case that people are able to determine for themselves what they want and what they do not want."}, {"heading": "Video Question Answering Dataset", "text": "We describe our new Video Question Answering (VideoQA) dataset. Because we want to collect videos with high-quality descriptions, we create a crawler to retrieve data from an online curated video repository1. Our data collected includes the following types:"}, {"heading": "Harvested Data", "text": "Internet Videos. We have collected 18,100 open domain videos with an average duration of 1.5 minutes (45 seconds average duration). Our videos are typically shot by a handheld camera such as mobile phone, GoPro, etc. Therefore, the video quality and scope of camera movements are quite different from film clips. Descriptions. Originally, each video is associated with a few description sets provided by the video owner. Subsequently, the staff of the video repository curates these sentences by removing abnormal ones. As a result, there are typically 3-5 description sets for each video. A typical description is shown in Fig. 1. It contains detailed descriptions of the scene (e.g. backyard), the actor (e.g. girl), the action (e.g. score) and possibly non-visual information (e.g. practice for their World Cup debut)."}, {"heading": "Questions Generation (QG)", "text": "We use state-of-the-art question generation methodology (Heilman and Smith 2010) to automatically generate candidate-QA pairs (auto-QG) for each set of descriptions, and we expect that some candidate-QA pairs are not perfect. In our Methods section, we will describe our strategy for handling these imperfect QA pairs, generating questions with the answer \"no.\" The status-of-the-art question method (Heilman and Smith 2010) can only answer \"yes / no\" questions with the answer \"yes / no\" questions with the answer \"no,\" we will use existing \"yes / no\" questions to retrieve similar \"yes / no\" questions from other videos. Since the \"yes / no\" questions retrieved are most likely irrelevant / inconsistent with the video content, we assign \"no\" as the answer to these retrieved \"yes / no\" questions."}, {"heading": "Questions and Answers Analysis", "text": "Questions. We categorize the questions into different types based on the words that start the question. Fig.3 (a) is the distribution of question types. Our video QA dataset contains various questions, including 5W1H questions. Since our QA task is based on video content, several questions are aimed at the actions or movements of the characters. In particular, auxiliary verbs such as \"Does,\" \"Did\" and \"Do\" imply that many of our questions are about the most important verbs in the event description. They differ significantly from the image-based QAdatasets (Antol et al. 2015; Ren, Kiros and Zemel 2015; Malinowski and Fritz 2014), which are mainly about objects, colors and numbers. Of all questions, the maximum, minimum, mean, standard deviation and median lengths are 36, 2, 10.8, 5.3, 9. \"In addition, we report more analyses of human-generated QA pairs and the automatic comparison between the A-generated data sets are included in each of the pairs A-Q3 and 2.5)."}, {"heading": "Our Method", "text": "Video QA is the prediction of an answer to a given question q and video observation v. We define a video as a sequence of image observations v = [v1, v2,..] and both answer and question as a set of natural language (i.e. a sequence of words) a = [a1, a2,..] or q = [q1, q2,..]. To achieve video QA, we propose to learn a function a = f (v, q), where v, q are the inputs and a is the output desired. In the face of a loss L (a, f (v, q)), which measures the difference between a truth answer and a predicted answer f (v, q), we can train the function f (\u00b7) using a set of (vi, qi, ai) i2 triplets that are automatically generated from videos and their description sentences. As mentioned in answering video questions to datasets, the videos automatically contain the relevant QA-generated learning pairs, not necessarily due to some pairs of learning."}, {"heading": "Mitigating Effect of Non-perfect Candidate QAs", "text": "We follow our intuition below to design a test for identifying imperfect pairs. Intuitively, the loss L (a, f (v, q) should be small if an answer pair is \"relevant / consistent\" for a training question related to video content. On the other hand, if we maintain the same QA pair but change the video content to a dummy video vD with all zero observations, the loss L (a, f (vD, q) should increase significantly. If we maintain the same QA pair but change the video content to a dummy video vD, the loss L (a, f (vD, q) should be \"irrelevant / inconsistent\" with video content. Our intuition suggests that the loss of an imperfect triplet (vi, qi).i is less sensitive than the loss of a video vD (a, f, q)."}, {"heading": "Extened Methods", "text": "We expand the following methods for our Video QA task. Extended End-to-End Memory Network (MN) (Sukhbaatar et al. 2015). The QA task in MN consists of a series of statements followed by a question whose answer is typically a single word. We change the set of statements into a video - a sequence of images. To capture the temporal relationship between actions in successive images, we first use a bi-directional LSTM to encode the sequence of frame representations. Bidirectional LSTM and MN are trained together in an end-to-end mode. Fig. 4 (a) shows the model visualization similar to that in (Sukhbaatar et al. 2015). Extended VQA et al."}, {"heading": "Experiments and Results", "text": "We evaluate all methods on the video QA dataset described in the video question answering section. In all experiments, we use 14, 100 videos and 151, 263 QA candidate pairs for training, 2, 000 videos and 21, 352 QApairs for validation and 2, 000 videos and 2461 QA pairs for testing."}, {"heading": "Implementation Details", "text": "We remove punctuation and replace digits as < NUMBER >. For answers, we only remove stopwords. We select the top K = 1000 most common responses as possible candidates, which is the same as (Antol et al. 2015). This set of responses covers 81% of training and validation responses. Video data preprocessing. Similar to existing video understanding approaches, we use both appearance and local motion characteristics. For appearance, we extract VGG (Simonyan and Zisserman 2015) for each image. For local motion, we extract C3D (Tran et al. 2015) by dividing a video into a maximum of 45-50 video clips by limiting the GU data pairs by removing the PU data pairs. For local motion, we extract C3D (Tran et al. 2015) for 16 consecutive frames."}, {"heading": "Training details", "text": "We implement and train all advanced methods using TensorFlow (et al. 2015) with a batch size of 100 and select the final model according to the best validation accuracy. Further model-specific training details are described below. E-MN. We use stochastic gradient lineage with an initial learning rate of 0.001, the same learning rate decay and gradient clipping scheme in (Sukhbaatar et al. 2015). Inspired by several memory-based models, we use 500 as the number of memories and the hidden dimension of LSTM. Training runs up to 50 epochs. E-VQA. Except for the total number of iterations, the training settings are all the same as (Antol et al. 2015). Training runs up to 15,000 iterations. E-SA. Except for the optimization algorithm and the total number of epochs, the training settings are all the same as (Yao et al. 2015). We use a KingPoet 1 and Optimizer (Adam B00G)."}, {"heading": "Evaluation Metrics", "text": "Inspired by Image-QA (Malinowski and Fritz 2014; Antol et al. 2015), we rate video QA both on classification accuracy and on WUPS - its relaxed version based on word similarity. Note that our response frame is 1K and the classification accuracy is so strict that it considers \"cat\" to be the wrong class for a basic \"kitten.\" This motivates us to report WUPS where we follow (Malinowski and Fritz 2014) to use 0.0 and 0.9 as thresholds. In addition, we report separately on the performance of \"yes / no\" questions and \"other\" questions. Since \"yes / no\" questions are considered less challenging, we finally give the average accuracy of \"yes / no\" and \"others\" (see Table 1)."}, {"heading": "Results", "text": "We use Skip-Thought (ST) (Kiros et al. 2015) to directly learn the semantic and syntactic properties of the sentence within the recursive neural network. Using the above methods as a representation for questions, we can capture the similarity between the questions. In the face of a test question, we recall the ten closest (with cosinal similarity) training questions and their answers. The final answer is chosen by the majority of the votes in the top 10 answer list. We compare the advanced methods with the retrievable question base in the baseline section of the table. 1. We found that the baseline performs significantly worse than our advanced methods for \"others,\" but on a par with advanced methods for \"yes / no\" questions. Therefore, we suspect that the baseline makes many false positive \"yes / no\" predictions. For \"yes / no\" predictions by the SS, \"which we continue to make true positive + false + negative predictions like Acc \u2020."}, {"heading": "Conclusions", "text": "Our scalable approach has generated an extensive video-based questionnaire dataset (e.g. 18, 100 videos and 175, 076 QA pairs) with minimal human effort. In addition, our advanced models and self-directed learning have proven effective. In the future, we will further increase the size of the video QA dataset and improve the way we work to deal with a larger number of imperfect training examples."}], "references": [{"title": "Deep compositional question answering with neural module", "author": ["Andreas"], "venue": null, "citeRegEx": "Andreas,? \\Q2016\\E", "shortCiteRegEx": "Andreas", "year": 2016}, {"title": "C", "author": ["S. Antol", "A. Agrawal", "J. Lu", "M. Mitchell", "D. Batra", "Zitnick"], "venue": "L.; and Parikh, D.", "citeRegEx": "Antol et al. 2015", "shortCiteRegEx": null, "year": 2015}, {"title": "R", "author": ["J.P. Bigham", "C. Jayant", "H. Ji", "G. Little", "A. Miller", "Miller"], "venue": "C.; Miller, R.; Tatarowicz, A.; White, B.; White, S.; and Yeh, T.", "citeRegEx": "Bigham et al. 2010", "shortCiteRegEx": null, "year": 2010}, {"title": "Are you talking to a machine? dataset and methods for multilingual image question answering", "author": ["Gao"], "venue": null, "citeRegEx": "Gao,? \\Q2015\\E", "shortCiteRegEx": "Gao", "year": 2015}, {"title": "D", "author": ["Gates"], "venue": "M.", "citeRegEx": "Gates 2008", "shortCiteRegEx": null, "year": 2008}, {"title": "Visual turing test for computer vision systems", "author": ["Geman"], "venue": null, "citeRegEx": "Geman,? \\Q2014\\E", "shortCiteRegEx": "Geman", "year": 2014}, {"title": "N", "author": ["M. Heilman", "Smith"], "venue": "A.", "citeRegEx": "Heilman and Smith 2010", "shortCiteRegEx": null, "year": 2010}, {"title": "and Ba", "author": ["D.P. Kingma"], "venue": "J.", "citeRegEx": "Kingma and Ba 2014", "shortCiteRegEx": null, "year": 2014}, {"title": "R", "author": ["R. Kiros", "Y. Zhu", "Salakhutdinov"], "venue": "R.; Zemel, R.; Urtasun, R.; Torralba, A.; and Fidler, S.", "citeRegEx": "Kiros et al. 2015", "shortCiteRegEx": null, "year": 2015}, {"title": "Learning to answer questions from image using convolutional neural network", "author": ["Lu Ma", "L. Li 2016] Ma", "Z. Lu", "H. Li"], "venue": null, "citeRegEx": "Ma et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Ma et al\\.", "year": 2016}, {"title": "and Fritz", "author": ["M. Malinowski"], "venue": "M.", "citeRegEx": "Malinowski and Fritz 2014", "shortCiteRegEx": null, "year": 2014}, {"title": "Ask your neurons: A neural-based approach to answering questions about images", "author": ["Rohrbach Malinowski", "M. Fritz 2015] Malinowski", "M. Rohrbach", "M. Fritz"], "venue": null, "citeRegEx": "Malinowski et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Malinowski et al\\.", "year": 2015}, {"title": "P", "author": ["Noh, H.", "Seo"], "venue": "H.; and Han, B.", "citeRegEx": "Noh. Seo. and Han 2016", "shortCiteRegEx": null, "year": 2016}, {"title": "Exploring models and data for image question answering", "author": ["Kiros Ren", "M. Zemel 2015] Ren", "R. Kiros", "R. Zemel"], "venue": null, "citeRegEx": "Ren et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Ren et al\\.", "year": 2015}, {"title": "Question generation shared task and evaluation challenge v status", "author": ["Rus", "V. Graessar 2009] Rus", "Graessar"], "venue": null, "citeRegEx": "Rus et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Rus et al\\.", "year": 2009}, {"title": "and Lester", "author": ["V. Rus"], "venue": "J.", "citeRegEx": "Rus and Lester 2009", "shortCiteRegEx": null, "year": 2009}, {"title": "and Zisserman", "author": ["K. Simonyan"], "venue": "A.", "citeRegEx": "Simonyan and Zisserman 2015", "shortCiteRegEx": null, "year": 2015}, {"title": "MovieQA: Understanding stories in movies through question-answering", "author": ["Tapaswi"], "venue": null, "citeRegEx": "Tapaswi,? \\Q2016\\E", "shortCiteRegEx": "Tapaswi", "year": 2016}, {"title": "Learning spatiotemporal features with 3d convolutional networks", "author": ["Tran"], "venue": null, "citeRegEx": "Tran,? \\Q2015\\E", "shortCiteRegEx": "Tran", "year": 2015}, {"title": "S", "author": ["K. Tu", "M. Meng", "M.W. Lee", "T.E. Choe", "Zhu"], "venue": "C.", "citeRegEx": "Tu et al. 2014", "shortCiteRegEx": null, "year": 2014}, {"title": "Sequence to sequence - video", "author": ["Venugopalan"], "venue": null, "citeRegEx": "Venugopalan,? \\Q2015\\E", "shortCiteRegEx": "Venugopalan", "year": 2015}, {"title": "Show and tell: A neural image caption generator", "author": ["Vinyals"], "venue": null, "citeRegEx": "Vinyals,? \\Q2015\\E", "shortCiteRegEx": "Vinyals", "year": 2015}, {"title": "Describing videos by exploiting temporal structure", "author": ["Yao"], "venue": null, "citeRegEx": "Yao,? \\Q2015\\E", "shortCiteRegEx": "Yao", "year": 2015}, {"title": "A", "author": ["L. Zhu", "Z. Xu", "Y. Yang", "Hauptmann"], "venue": "G.", "citeRegEx": "Zhu et al. 2015", "shortCiteRegEx": null, "year": 2015}], "referenceMentions": [], "year": 2016, "abstractText": "We propose a scalable approach to learn video-based question answering (QA): answer a \u201cfree-form natural language question\u201d about a video content. Our approach automatically harvests a large number of videos and descriptions freely available online. Then, a large number of candidate QA pairs are automatically generated from descriptions rather than manually annotated. Next, we use these candidate QA pairs to train a number of video-based QA methods extended from MN (Sukhbaatar et al. 2015), VQA (Antol et al. 2015), SA (Yao et al. 2015), SS (Venugopalan et al. 2015). In order to handle non-perfect candidate QA pairs, we propose a self-paced learning procedure to iteratively identify them and mitigate their effects in training. Finally, we evaluate performance on manually generated video-based QA pairs. The results show that our self-paced learning procedure is effective, and the extended SS model outperforms various baselines.", "creator": "LaTeX with hyperref package"}}}