{"id": "1611.04230", "review": {"conference": "AAAI", "VERSION": "v1", "DATE_OF_SUBMISSION": "14-Nov-2016", "title": "SummaRuNNer: A Recurrent Neural Network Based Sequence Model for Extractive Summarization of Documents", "abstract": "We present SummaRuNNer, a Recurrent Neural Network (RNN) based sequence model for extractive summarization of documents and show that it achieves performance better than or comparable to state-of-the-art. Our model has the additional advantage of being very interpretable, since it allows visualization of its predictions broken up by abstract features such as information content, salience and novelty. Another novel contribution of our work is abstractive training of our extractive model that can train on human generated reference summaries alone, eliminating the need for sentence-level extractive labels.", "histories": [["v1", "Mon, 14 Nov 2016 02:44:14 GMT  (76kb,D)", "http://arxiv.org/abs/1611.04230v1", "Published at AAAI 2017, The Thirty-First AAAI Conference on Artificial Intelligence (AAAI-2017)"]], "COMMENTS": "Published at AAAI 2017, The Thirty-First AAAI Conference on Artificial Intelligence (AAAI-2017)", "reviews": [], "SUBJECTS": "cs.CL", "authors": ["ramesh nallapati", "feifei zhai", "bowen zhou"], "accepted": true, "id": "1611.04230"}, "pdf": {"name": "1611.04230.pdf", "metadata": {"source": "CRF", "title": "SummaRuNNer: A Recurrent Neural Network based Sequence Model for Extractive Summarization of Documents", "authors": ["Ramesh Nallapati", "Feifei Zhai", "Bowen Zhou"], "emails": ["zhou@us.ibm.com"], "sections": [{"heading": "Introduction", "text": "Summary techniques are mainly divided into two categories: extractive and abstract. Extractive methods aim to select meaningful snippets, sentences or passages from documents, while abstract summary techniques can generally be classified into greedy approaches (e.g., (Carbonell and Goldstein 1998), graphic approaches (e.g., (Radev and Erkan 2004) and constraint optimization approaches (e.g., (McDonald 2007)).Recently, neural network-based approaches have been popular for extractive summarization. For example, (Kageback et al. 2014) they employed the recursive autocoder (Socher al. 2011) on maritime approaches based on the production of documents."}, {"heading": "SummaRuNNer", "text": "In this paper, we treat Extractive Summary as a sequence classification problem, where each sentence is sequentially picked up in the order of origin and a binary decision (taking into account previous decisions made) in relation to whether or not it should be included in the summary. We use a GRU-based recurrent neural network (Chung et al. 2014) as the basic building block of our sequence classification. (1) The Gru-RNN is a recurrent network with two gates, u called the reset gate and r, and can be described using the following equations: uj + Wuxxj + Wuhj \u2212 1 + bu). (1) Wrxxj \u2212 1 + Wr) h (2) h = tanh = tanh (Whxj + Whh)."}, {"heading": "Extractive Training", "text": "To train our extractive model, we need the basic truth in the form of sentence-level binary labels for each document that represent their affiliation with the summary. However, most abstract corpora contain only human abstract summaries as basic truth. To solve this problem, we use an uncontrolled approach to convert abstract summaries into extractive labels. Our approach is based on the idea that the sentences selected from the document should be the ones that maximize the rouge score in terms of gold collections. Since it is mathematically expensive to find a globally optimal subset of sentences that maximizes the rouge score, we apply a greedy approach in which we add one sentence at a time to the summary so that the rouge score of the current set of selected sentences is maximized in terms of the entire gold collection."}, {"heading": "Abstractive Training", "text": "In this section we propose a new training technology to train SummaRuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuu"}, {"heading": "Experiments and Results", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "Corpora", "text": "For our experiments, we used the CNN / DailyMail corpus, originally constructed by (Hermann et al. 2015) for the task of answering questions based on passages and for the task of summarizing documents, as proposed in (Cheng and Lapata 2016), for the extractive summary and (Nallapati et al. 2016) for the abstract summary. To make a fair comparison with the former, we omitted the CNN subset of the corpus, as they did. To compare with the latter, we used the joint CNN / Daily Mail corpus. In total, we have 196,557 training documents, 12,147 validation documents and 10,396 test documents from the Daily Mail corpus. Taking into account also the CNN subset, we have 286,722 training documents, 13,362 validation documents and 11,480 test documents. On average, there are about 28 sets per document in the training set and an average of 3 sets in the reference documents."}, {"heading": "Evaluation", "text": "In our experiments below, we evaluate the performance of SummaRuNNer using different variants of Rouge Metric 5 calculated with respect to the Gold Summaries. To compare with (Cheng and Lapata 2016) on the Daily Mail corpus, we use the rouge metric used by the authors with limited length and 75 bytes and 275 bytes as reported by them. To compare with (Nallapati et al. 2016) on the CNN / Daily Mail corpus, we use the same complete rouge F1 metric used by the authors. On the DUC 2002 corpus, we use the limited length 4http: / / www-nlpir.nist.gov / projects / duc / guidelines / 2002.html 5http: / / www.berouge.com / Pages / default.aspxRouge remember metric with 75 words. We report the results from BigMarge-1, Rouge-2, and Rouge-samnet, respectively, which match the most common parameters."}, {"heading": "Baselines", "text": "For all data sets, we use the Lead 3 model, which produces only the three leading sets of the document as a baseline. On corpora Daily Mail and DUC 2002, we also report on the performance of LReg, a feature-rich logistics classifier used as the baseline of (Cheng and Lapata 2016); on corpora DUC 2002, we report on several baseline lines such as Integer Linear Programming based approach (Woodsend and Lapata 2010) and graph-based approaches such as TGRAPH (Parveen, Ramsl and Strube 2015) and URANK (Wan 2010), which achieve very high performance on this corpus; and we also compare with state-of-the-art deep learning models (Cheng and Lapata 2016) and (Nallapati et al. 2016)."}, {"heading": "SummaRuNNer Settings", "text": "We used 100-dimensional word2vec (Mikolov et al. 2013) embeddings trained on the CNN / Daily Mail Corpus to speed up the calculation. We limited the vocabulary size to 150K and the maximum number of sentences per document to 100, and the maximum sentence length to 50 words to speed up the calculation. We set the hidden state size of the model to 200. We used a stack size of 64 during training and adadelta (pointer 2012) to train our model. We used gradient cutouts to regulate our model and an early stop criterion based on validation costs. We trained SummaRuNNer both extractively and abstractively. When the model is trained abstractively, we call it SummaRuNNer-abs as a result. At the test date, selecting all sentences with P (y = 1) is not an optimal strategy, as the maximum number of sentences is selected based on the unbalanced number of sentences."}, {"heading": "Results on Daily Mail corpus", "text": "Table 1 shows the performance comparison of SummaRuNNer with the state-of-the-art model of (Cheng and Lapata 2016) and other baselines on the DailyMail corpus using Rouge Recall with a summary limited to 75 bytes. While the abstract-trained SummaRuNNer works on par with the state-of-the-art model, the extractive-trained model significantly improves on its model. In Table 2, we report on the performance of our model in terms of Rouge Recall with 275 bytes of summary length. In this case, our abstract-trained model undercuts the extractive model of (Cheng and Lapata 2016), while the extractive-trained model is statistically indistinguishable from its model. This shows that SummaRuNNer is better at selecting the best sentence for the summary than the ones below. One possible reason that SummaRuNNNer does not consistently surpass the extractive model of (Cheng and Lapata 2016) is that the additional SummaRuNNNer model is better at generating a more dangerous NNer than the training model to produce it."}, {"heading": "Results on CNN/Daily Mail corpus", "text": "The only other work reporting the performance on this dataset is the abstract encoder decoder model from (Nallapati et al. 2016), in which they use the full length F1 as a measurement, as neural abstract approaches can learn when to stop generating words in the summary. To make a fair comparison with their work, we use the same measurement as they do. On this dataset, SummaRuNNer outperforms their model as in Table 3. The superior performance of our model is not entirely surprising, since abstract summary is a much more difficult problem, but the table serves to quantify the current performance gap between extractive and abstract approaches to the summary, and the results also show the difficulty of using formula 1 for extractive summary, since SummaRuNNer, with its three highest sentences with the highest predictive probability, is the summary, has errors on the side of this high-precision memory work and not the precision of the predictive work based on the precision of the 1."}, {"heading": "Results on the Out-of-Domain DUC 2002 corpus", "text": "SummaRuNNer is statistically on a par with the model of (Cheng and Lapata 2016), but both models perform worse than graph-based TGRAPH (Parveen, Ramsl and Strube 2015) and URANK (Wan 2010) algorithms, which are the most advanced models on this corpus. Deep learning monitored models such as SummaRuNNer and those of (Cheng and Lapata 2016) perform very well in the field in which they are trained, but may have domain adaptation problems when tested on a different corpus such as DUC 2002. Graph-based, uncontrolled approaches, on the other hand, can be more resilient to domain variations."}, {"heading": "Qualitative Analysis", "text": "SummaRuNNer is not only a state-of-the-art performer, but also has the added advantage of being very easy to interpret. Clearly separated terms in the classification layer (see Equation 6) allow us to identify various factors responsible for the classification of each set, as illustrated in Figure 2, where we show a representative document from our validation set, along with normalized values from each abstract feature responsible for its final classification. Such visualization is particularly useful for explaining to the end user the decisions made by the system.We also show a few sample documents from the Daily Mail and DUC Corporation highlighting the sets selected by SummaRuNNNer and comparing them with the gold summary in Table 5. These examples qualitatively show that SummaRuNNNer does a reasonably good job of identifying the key points of the document."}, {"heading": "Conclusion", "text": "In this paper, we propose a highly interpretable neural sequence translation through collaborative learning that enables intuitive visualization and shows that it is better than or comparable to the state-of-the-art deep learning model.We also propose a novel abstract training method to eliminate the need for extractive labels in the training period, but this approach is still a few rouge points below our extractive training on most datasets. We plan to further explore the combination of extractive and abstract approaches as part of our future work. A simple approach could be to pre-train the extractive model with abstract training. We also plan to construct a common extractive-abstract model in which the extractive components of our intermedia units are consumed. References [Bahdanau, Cho and Bengio 2014] Bahdanau, D.; Cho, K. and Bengio, Y."}], "references": [{"title": "Neural machine translation by jointly learning to align and translate", "author": ["Cho Bahdanau", "D. Bengio 2014] Bahdanau", "K. Cho", "Y. Bengio"], "venue": "arXiv preprint arXiv:1409.0473", "citeRegEx": "Bahdanau et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Bahdanau et al\\.", "year": 2014}, {"title": "Tgsum: Build tweet guided multidocument summarization dataset. CoRR abs/1511.08417", "author": ["M. Zhou"], "venue": null, "citeRegEx": "Zhou,? \\Q2015\\E", "shortCiteRegEx": "Zhou", "year": 2015}, {"title": "Attsum: Joint learning of focusing and summarization with neural attention", "author": ["Cao"], "venue": "arXiv preprint arXiv:1604.00125", "citeRegEx": "Cao,? \\Q2016\\E", "shortCiteRegEx": "Cao", "year": 2016}, {"title": "and Goldstein", "author": ["J. Carbonell"], "venue": "J.", "citeRegEx": "Carbonell and Goldstein 1998", "shortCiteRegEx": null, "year": 1998}, {"title": "and Lapata", "author": ["J. Cheng"], "venue": "M.", "citeRegEx": "Cheng and Lapata 2016", "shortCiteRegEx": null, "year": 2016}, {"title": "Empirical evaluation of gated recurrent neural networks on sequence modeling", "author": ["Chung"], "venue": "CoRR abs/1412.3555", "citeRegEx": "Chung,? \\Q2014\\E", "shortCiteRegEx": "Chung", "year": 2014}, {"title": "Opinosis: a graph-based approach to abstractive summarization of highly redundant opinions", "author": ["Zhai Ganesan", "K. Han 2010] Ganesan", "C. Zhai", "J. Han"], "venue": "In Proceedings of the 23rd international conference on computational linguistics,", "citeRegEx": "Ganesan et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Ganesan et al\\.", "year": 2010}, {"title": "K", "author": ["Hermann"], "venue": "M.; Kocisk\u00fd, T.; Grefenstette, E.; Espeholt, L.; Kay, W.; Suleyman, M.; and Blunsom, P.", "citeRegEx": "Hermann et al. 2015", "shortCiteRegEx": null, "year": 2015}, {"title": "Extractive summarization using continuous vector space models", "author": ["Kageback"], "venue": null, "citeRegEx": "Kageback,? \\Q2014\\E", "shortCiteRegEx": "Kageback", "year": 2014}, {"title": "G", "author": ["T. Mikolov", "I. Sutskever", "K. Chen", "Corrado"], "venue": "S.; and Dean, J.", "citeRegEx": "Mikolov et al. 2013", "shortCiteRegEx": null, "year": 2013}, {"title": "Abstractive text summarization using sequence-to-sequence rnns and beyond", "author": ["Nallapati"], "venue": "The SIGNLL Conference on Computational Natural Language Learning", "citeRegEx": "Nallapati,? \\Q2016\\E", "shortCiteRegEx": "Nallapati", "year": 2016}, {"title": "Sequence-to-sequence rnns for text summarization", "author": ["Zhou Nallapati", "R. Xiang 2016] Nallapati", "B. Zhou", "B. Xiang"], "venue": "International Conference on Learning Representations,", "citeRegEx": "Nallapati et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Nallapati et al\\.", "year": 2016}, {"title": "Topical coherence for graph-based extractive summarization", "author": ["Ramsl Parveen", "D. Strube 2015] Parveen", "H.M. Ramsl", "M. Strube"], "venue": "In Proceedings of the Conference on Empirical Methods in Natural Language Processing,", "citeRegEx": "Parveen et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Parveen et al\\.", "year": 2015}, {"title": "and Erkan", "author": ["D. Radev"], "venue": "G.", "citeRegEx": "Radev and Erkan 2004", "shortCiteRegEx": null, "year": 2004}, {"title": "A", "author": ["Rush"], "venue": "M.; Chopra, S.; and Weston, J.", "citeRegEx": "Rush. Chopra. and Weston 2015", "shortCiteRegEx": null, "year": 2015}, {"title": "Document summarization using conditional random fields", "author": ["Shen"], "venue": "In Proceedings of IJCAI", "citeRegEx": "Shen,? \\Q2007\\E", "shortCiteRegEx": "Shen", "year": 2007}, {"title": "A", "author": ["R. Socher", "E.H. Huang", "J. Pennin", "C.D. Manning", "Ng"], "venue": "Y.", "citeRegEx": "Socher et al. 2011", "shortCiteRegEx": null, "year": 2011}, {"title": "C", "author": ["K.M. Svore", "L. Vanderwende", "Burges"], "venue": "J.", "citeRegEx": "Svore. Vanderwende. and Burges 2007", "shortCiteRegEx": null, "year": 2007}, {"title": "and Lapata", "author": ["K. Woodsend"], "venue": "M.", "citeRegEx": "Woodsend and Lapata 2010", "shortCiteRegEx": null, "year": 2010}, {"title": "and Pei", "author": ["W. Yin"], "venue": "Y.", "citeRegEx": "Yin and Pei 2015", "shortCiteRegEx": null, "year": 2015}, {"title": "M", "author": ["Zeiler"], "venue": "D.", "citeRegEx": "Zeiler 2012", "shortCiteRegEx": null, "year": 2012}], "referenceMentions": [], "year": 2016, "abstractText": "We present SummaRuNNer, a Recurrent Neural Network (RNN) based sequence model for extractive summarization of documents and show that it achieves performance better than or comparable to state-of-the-art. Our model has the additional advantage of being very interpretable, since it allows visualization of its predictions broken up by abstract features such as information content, salience and novelty. Another novel contribution of our work is abstractive training of our extractive model that can train on human generated reference summaries alone, eliminating the need for sentence-level extractive labels.", "creator": "LaTeX with hyperref package"}}}