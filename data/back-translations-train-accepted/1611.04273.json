{"id": "1611.04273", "review": {"conference": "iclr", "VERSION": "v1", "DATE_OF_SUBMISSION": "14-Nov-2016", "title": "On the Quantitative Analysis of Decoder-Based Generative Models", "abstract": "The past several years have seen remarkable progress in generative models which produce convincing samples of images and other modalities. A shared component of many powerful generative models is a decoder network, a parametric deep neural net that defines a generative distribution. Examples include variational autoencoders, generative adversarial networks, and generative moment matching networks. Unfortunately, it can be difficult to quantify the performance of these models because of the intractability of log-likelihood estimation, and inspecting samples can be misleading. We propose to use Annealed Importance Sampling for evaluating log-likelihoods for decoder-based models and validate its accuracy using bidirectional Monte Carlo. Using this technique, we analyze the performance of decoder-based models, the effectiveness of existing log-likelihood estimators, the degree of overfitting, and the degree to which these models miss important modes of the data distribution.", "histories": [["v1", "Mon, 14 Nov 2016 07:36:22 GMT  (3800kb,D)", "http://arxiv.org/abs/1611.04273v1", null], ["v2", "Tue, 6 Jun 2017 22:36:35 GMT  (2466kb,D)", "http://arxiv.org/abs/1611.04273v2", "Accepted to ICLR2017"]], "reviews": [], "SUBJECTS": "cs.LG", "authors": ["yuhuai wu", "yuri burda", "ruslan salakhutdinov", "roger grosse"], "accepted": true, "id": "1611.04273"}, "pdf": {"name": "1611.04273.pdf", "metadata": {"source": "CRF", "title": "ON THE QUANTITATIVE ANALYSIS OF DECODER- BASED GENERATIVE MODELS", "authors": ["Yuhuai Wu", "Yuri Burda"], "emails": ["ywu@cs.toronto.edu", "yburda@openai.com", "rsalakhu@cs.cmu.edu", "rgrosse@cs.toronto.edu"], "sections": [{"heading": "1 INTRODUCTION", "text": "This year, it has reached the point where it will be able to put itself at the top of the list."}, {"heading": "2 BACKGROUND", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "2.1 DECODER-BASED GENERATIVE MODELS", "text": "Generative modelling often uses a decoder network to define a generative distribution by transforming samples from a simple distribution (e.g. normal) to data diversity. In this paper, we look at three types of decoder-based generative models: Variational Autoencoder (UAE) (Kingma & Welling, 2014), Generative Adversarial Network (GAN) (Goodfellow et al., 2014) and Generative Moment Matching Network (GMMN) (Li & Swersky, 2015; Dziugaite et al., 2015)."}, {"heading": "2.1.1 VARIATIONAL AUTOENCODER", "text": "A Variational Autocoder (VAE) (Kingma & Welling, 2014) is a probabilistic directed graphical model. It is defined by a common distribution over a series of latent random variables z and the observed variables x: p (x, z) = p (x | z) p (z). Prior distribution over the latent random variables, p (z), is usually chosen as the standard Gaussian distribution. The data probability p (x | z) is usually a Gaussian or Bernoulli distribution whose parameters depend on z through a deep neural network known as the decoder network. Furthermore, it uses a harmless inference model called an encoder or detection network that serves as a variable approximation q (z | x) to the rear p (z | x). The decoder network and the encoder networks are jointly trained to reduce the evidence boundary (ELmaxance) BO () (repair): x (z)."}, {"heading": "2.1.2 GENERATIVE ADVERSARIAL NETWORK (GAN)", "text": "A generative adversarial network (GAN) (Goodfellow et al., 2014) is a generative model that is trained by a game between a decoder network and a discriminator network. It defines the generative model by sampling the latent variable z from a simple previous distribution p (z) (e.g. gauss) through the decoder network. The discriminator network D (\u00b7) gives a probability of a given sample originating from the data distribution. Its task is to distinguish samples from the generator distribution of real data. On the other hand, the decoder network tries to produce samples as realistically as possible in order to deceive the discriminator that its results are accepted as real. Competition between the two networks leads to the following minimax problem: min G max D Ex data [logD (x)] + Ez \u00b2 p (z) [log (1 \u2212 D (z)]]. Unlike VAE, the target is not explicitly linked to the log data."}, {"heading": "2.1.3 GENERATIVE MOMENT MATCHING NETWORK (GMMN)", "text": "Generative Moment Matching Networks (GMMNs) (Li & Swersky, 2015; Dziugaite et al., 2015) use Maximum Mean Matching Discrepancy (MMD) as a training target, a moment matching criterion in which kernel embedding techniques are used to avoid unnecessary assumptions about distributions."}, {"heading": "2.2 ANNEALED IMPORTANCE SAMPLING", "text": "Annealed importance sampling (AIS; (Neal, 2001)) is a Monte Carlo algorithm commonly used to estimate (ratios of) normalizing constants. We mainly consider the application of AIS to Bayesian posterior inference via latent variables z in light of some fixed observation x. Given a distribution p (x, z) where the posterior distribution p (z | x) is intractable, we ask how we can obtain the marginal probability p (x). A simple approach is probability weighting, which is a form of weight sample, if the suggested distribution is the previous distribution over z: p (x) = p (x) = sequential p (z) p (z) = Ez-p (z) [p (x | z)]]] (3) However, the previous distribution over z may be drastically different from the actual posterior p (z | x), especially in high dimension. This is likely problematic because the problem is the non-representative phase we get from the previous sample or the orative one."}, {"heading": "2.3 BIDIRECTIONAL MONTE CARLO", "text": "AIS provides an unbiased estimate p (x) of p (x). Generally, however, it is often more meaningful to estimate p (x) in logspace, i.e. log p (x), because the logarithms of an unbiased estimate are a stochastic lower limit of the logarithm (Grosse et al., 2015). Specifically, log p (x) is a stochastic lower limit of the logarithm (x), the E [p (x)]] \u2264 p (x) and Pr (p) > p (x) + b) < e \u2212 b. Grosse et al. (2015) indicates that if AIS is executed in the opposite direction starting from an exact posterior sample, it results in an unbiased estimate of 1 / p (x), which (according to the argument above) can be executed as a biometric upper limit of p (x)."}, {"heading": "3 METHODOLOGY", "text": "For a given generative distribution p (x, z) = p (z) p (x | z) we refer to the Finnish distribution. Since our task is to measure the log probability of test examples, log p (xtest). We will first discuss how to define the generative distribution for decoder-based networks. For UAE, generative distribution is defined in the standard method, where p (z) is a standard normal distribution and p (x | z) is a normal distribution parameterized by mean number of micrometers (z). To be well defined, we follow the same assumption we made in evaluating the use of the kernel density testimator x (Parzen, 1962): we assume a Gaussian observation model with a fixed hyperparameter of variance."}, {"heading": "4 RELATED WORK", "text": "An influential example was the use of AIS to evaluate deep-arbitrary belief networks (Salakhutdinov & Murray, 2008).Although we used the same technique, the problem we are looking at is completely different: First, the model they are looking at is undirected graphic models, whereas decoder-based models are directed graphic models. Second, their model has a well-defined probabilistic density function in terms of energy function, while we need to consider another probabilistic model for one where the probability is poorly defined. Furthermore, we validate our estimates with the help of BDMC.Theis et al. (2016) provide an in-depth analysis of problems that might arise in the evaluation of generative models, pointing out that a model that fails completely in modifying the distribution mode could still achieve a high visual value. Salimane et al. (2016) suggest a quality measurement model that compares the amount of food with the amount of corrosion with that is highly graded."}, {"heading": "5 EXPERIMENTS", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "5.1 DATASETS", "text": "All of our experiments were performed on the basis of the MNIST dataset with handwritten numerical images (LeCun et al., 1998).To ensure consistency with previous work evaluating decoder-based models, most of our experiments used continuous inputs. We dequantified the data according to Uria et al. (2013) by adding a uniform noise of 1256 to the data and recalculating it to [0, 1] D. We used the standard division of MNIST into 60,000 training examples and 10,000 test examples, and used 50,000 images from the training set for training and the remaining 10,000 images for validation. In addition, some of our experiments used the binarized MNIST dataset with a Bernoulli observational model (Salakhutdinov & Murray, 2008)."}, {"heading": "5.2 MODELS", "text": "For most of our experiments, we looked at two decoder architectures: a small one with 10 latent dimensions and a larger one with 50 latent dimensions. We used the Normal Standard Distribution as before the training of all our models. All layers were fully interconnected, and the number of units in each layer was 10-64-256-256-1024-784 for the smaller architecture and 50-1024-1024-1024-784 for the larger one. We trained both architectures using the VAE, GAN and GMMN targets, which resulted in six networks we call VAE-10, VAE-50, etc. In general, the larger architecture performed much better in both training and test sets, but we also analyzed the smaller architecture because it better highlights some of the differences between the training criteria. Additional architectural details are included in Appendix A.1. To allow direct comparison between the training criteria, all models used a spherical Gaussian protocol that is more consistent with the previous GMN observation method."}, {"heading": "5.3 VALIDATION OF LOG-LIKELIHOOD ESTIMATES", "text": "Before analyzing the performance of the trained networks, we must first determine the accuracy of the log likelihood estimators. In this section, we validate the accuracy of our AIS-based estimates using BDMC. We then analyze the error in the KDE and IWAE estimates and highlight some cases where these metrics overlook important phenomena."}, {"heading": "5.3.1 VALIDATION OF AIS", "text": "We used AIS to estimate the log probability for all eligible models. Unless otherwise stated, all AIS estimates using 16 independent chains, 10,000 intermediate distributions of the form in Eqn. 5 and a transition operator consisting of a proposed HMC path with 10 switching steps.1 Following Ranzato et al. (2010), the HMC step size was adjusted to achieve an acceptance rate of 0.65 (as recommended by Neal (2010). For all six models, we evaluated the accuracy of this estimation using BDMC based on data taken from the distribution of the model to 1000 simulated examples. We refer to this gap between the log liquidity estimates considered by Forward-AIS (which indicates a lower limit) and the reverse AIS (which indicates an upper limit) limiting the error of the AIS estimates on simulated data. We refer to this gap as the gap between the BMC model and the actual BMC, which we found to be sufficient for the gap between the two DMC networks."}, {"heading": "5.3.2 HOW ACCURATE IS KERNEL DENSITY ESTIMATION?", "text": "The Core Density Estimate (KDE) (Parzen, 1962) is often used to evaluate decoder-based models (Goodfellow et al., 2014; Li & Swersky, 2015), and a variant has been suggested when discontinuing the evaluation of Boltzmann machines (Bengio et al., 2013). KDE reports often show caution that the HMC implementation of http: / / deeplearning.net / tutorial / deeplearning.pdfKDE is not intended to be applied in high-dimensional spaces and that the results may therefore be inaccurate. Nevertheless, KDE remains the standard protocol for evaluating decoder-based models. We analyzed the accuracy of the KDE estimates by comparing them against AIS. Both estimates are stochastic lower limits of true log probability (see Section 3), so greater values are guaranteed (with high probability) to be more accurate."}, {"heading": "5.3.3 HOW ACCURATE IS THE IWAE BOUND?", "text": "In principle, the UAE probability could be estimated using the UAE objective function (which is a lower limit on the actual log probability), but since the weighted meaning corresponds to the objective function used by the Importance Weighted Autoencoder (IWAE) (Burda et al., 2016), we will demonstrably refer to it as the IWAE limit. In continuous MNIST, the IWAE limit underestimated the actual log probability by at least 33.2 Nats on the training set and 187.4 Nats on the test set. Although this is much more accurate than the IWAE limit, the error is still significant. Interestingly, this result also indicates that the recognition network exceeds the training data. As the UAE and IWAE results are usually reported on a large difference in the HOW determination, we will calculate the initial difference in the AIS determination 1."}, {"heading": "5.4 SCIENTIFIC FINDINGS", "text": "After validating the accuracy of AIS, we now use it to analyze the effectiveness of various training criteria, as well as illuminating phenomena that could not be observed with existing log likelihood estimators or through sample inspection. For all experiments in this section, we used 10,000 intermediate distributions for AIS, 1 million simulated samples for KDE, and 200,000 important samples for the IWAE limit. (These settings resulted in similar computation times for all three estimators.)"}, {"heading": "5.4.1 MODEL LIKELIHOOD COMPARISON", "text": "We evaluated the trained models using AIS and KDE using 1000 test examples for MNIST; the results are presented in Table 2. We note that the larger architectures consistently outperformed the smaller ones on all three training criteria. We also note that the VAEs achieved significantly higher log probabilities for both the 10- and 50-dimensional architectures than GANs or GMMNs. Unsurprisingly, the UAEs achieved a higher probability because they were trained on a probability object, while the GANs and GMMNs were not. However, it is interesting that the difference in log probabilities was so large; for the rest of this section, we try to analyze exactly what causes this big difference. We find that the KDE errors were on the same order of magnitude as the differences between the models, suggesting that they cannot be reliably used to compare the log probabilities."}, {"heading": "5.4.2 MEASURING THE DEGREE OF OVERFITTING", "text": "One question that arises when evaluating decoder-based generative models is whether they remember parts of the training dataset. You can't test this by looking at only model samples. Frequently, the closest neighbors from the training set can be misleading (Theis et al., 2016), and the interpolation in the latent space between different samples can be visually appealing, but does not provide a quantitative measure of the degree of generalization. To analyze the degree of match, Fig. 3 shows training curves for three networks, measured by AIS, KDE, and the IWAE limit. We observed that the training and test protocol probabilities of GAN-50 are nearly identical throughout the training, refuting the hypothesis that it memorized training data. Both GAN-50 and GMMN-50 overmatch less than VAE-50. We also observed two phenomena that could not be measured with existing techniques during the entire training, which refutes the hypothesis that it matches training data."}, {"heading": "5.4.3 HOW APPROPRIATE IS THE OBSERVATION MODEL?", "text": "Appendix B deals with the question of whether the spherical Gaussian observational model fits well and whether the log probability differences could be an artifact of the observational model. We find that all models can be substantially improved taking into account the non-Gaussian difference, but that this effect is not sufficient to explain the gap between the VAEs and the other models."}, {"heading": "5.4.4 ARE THE NETWORKS MISSING MODES?", "text": "It has already been observed that one of the potential failure modes of Boltzmann machines is unable to generate one or more distribution modes or drastically miss the probability mass between modes (Salakhutdinov & Murray, 2008). Here, we analyze these for decoder-based models. First, we ask a coarse-grained version of this question: Have the networks correctly mapped the probability mass between the 10-digit classes, and if not, can this explain the difference in the log probability values? In Fig. 1, we see that the distribution of the digital classes was greatly distorted: from 100 samples it generated 37 images of 1, but only a single second."}, {"heading": "ACKNOWLEDGMENTS", "text": "We thank Yujia Li for providing his original GMMN model and code base and Jimmy Ba for advising on the training of GANs. Ruslan Salakhutdinov is partially supported by Disney and ONR grants N000141310721. We also thank the developers of Lasagne (Battenberg et al., 2014) and Theano (Al-Rfou et al., 2016)."}, {"heading": "A NETWORK ARCHITECTURES/TRAINING", "text": "We have used all the hidden layers in both networks using the tanh activation function, and the output layers used the logistics function. \u2022 The larger model uses an encoder of an architecture 784-4096-1. All the hidden layers in both networks used the tanh activation function, and the output layers used the logistics function. \u2022 The larger model uses an encoder of an architecture 784-1024-100. We add the output layers between each hidden layer of a hidden layer with a smaller layer of a smaller layer. \u2022 We use an encoder of an architecture 784-1024-100."}, {"heading": "C POSTERIOR VISUALIZATION OF DIGIT \u201c2\"", "text": "In this section, we examine the quality of modeling \"2\" of each model. We sample a fixed set of 100 samples of the number \"2\" from training data and compare whether the model captures this mode. We show charts \"2\" for GAN-10, GAN-50, UAE-10 and true data in the following illustrations for illustration. We see that GAN-10 does not capture many cases of number \"2\" in training data! Instead of generating \"2,\" it tries to generate numbers \"1,\" \"7\" \"\" 9, \"\" \"4,\" \"8\" from the reconstruction. GAN-50 performs much better, its reconstruction is all digits digits \"2\" and there is only a certain style difference from the true data. VAE-10 dominates this contest by perfectly reconstructing all samples \"2\" when we directly from the samples \"2\" or \"highlight (if we see the differences between the models)."}], "references": [{"title": "Theano: A python framework for fast computation of mathematical expressions, 2016", "author": ["Rami Al-Rfou", "Guillaume Alain", "Amjad Almahairi"], "venue": null, "citeRegEx": "Al.Rfou et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Al.Rfou et al\\.", "year": 2016}, {"title": "Bounding the test log-likelihood of generative models", "author": ["Y. Bengio", "L. Yao", "K. Cho"], "venue": null, "citeRegEx": "Bengio et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Bengio et al\\.", "year": 2013}, {"title": "A test of relative similarity for model selection in generative models", "author": ["Wacha Bounliphone", "Eugene Belilovsky", "Matthew B. Blaschko", "Ioannis Antonoglou", "Arthur Gretton"], "venue": "In ICLR", "citeRegEx": "Bounliphone et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Bounliphone et al\\.", "year": 2016}, {"title": "Importance weighted autoencoders", "author": ["Yuri Burda", "Roger Grosse", "Ruslan Salakhutdinov"], "venue": "In ICLR,", "citeRegEx": "Burda et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Burda et al\\.", "year": 2016}, {"title": "Deep generative image models using a laplacian pyramid of adversarial networks", "author": ["E. Denton", "S. Chintala", "A. Szlam", "R. Fergus"], "venue": "In NIPS,", "citeRegEx": "Denton et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Denton et al\\.", "year": 2015}, {"title": "Nice: Non-linear independent components estimation", "author": ["Laurent Dinh", "David Krueger", "Yoshua Bengio"], "venue": "arXiv preprint arXiv:1410.8516,", "citeRegEx": "Dinh et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Dinh et al\\.", "year": 2014}, {"title": "Density estimation using real nvp", "author": ["Laurent Dinh", "Jascha Sohl-Dickstein", "Samy Bengio"], "venue": null, "citeRegEx": "Dinh et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Dinh et al\\.", "year": 2016}, {"title": "Training generative neural networks via Maximum Mean Discrepancy optimization", "author": ["Gintare Karolina Dziugaite", "Daniel M. Roy", "Zoubin Ghahramani"], "venue": "In UAI", "citeRegEx": "Dziugaite et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Dziugaite et al\\.", "year": 2015}, {"title": "Generative adversarial nets", "author": ["Ian Goodfellow", "Jean Pouget-Abadie", "Mehdi Mirza", "Bing Xu", "David Warde-Farley", "Sherjil Ozair", "Aaron Courville", "Yoshua Bengio"], "venue": "Advances in Neural Information Processing Systems", "citeRegEx": "Goodfellow et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Goodfellow et al\\.", "year": 2014}, {"title": "Measuring the reliability of MCMC inference with bidirectional Monte Carlo", "author": ["Roger Grosse", "Siddharth Ancha", "Daniel M. Roy"], "venue": "In NIPS,", "citeRegEx": "Grosse et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Grosse et al\\.", "year": 2016}, {"title": "Sandwiching the marginal likelihood using bidirectional monte carlo", "author": ["Roger B. Grosse", "Zoubin Ghahramani", "Ryan P. Adams"], "venue": "arXiv preprint arXiv:1511.02543,", "citeRegEx": "Grosse et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Grosse et al\\.", "year": 2015}, {"title": "Generating images with recurrent adversarial networks", "author": ["Daniel Jiwoong Im", "Chris Dongjoo Kim", "Hui Jiang", "Roland Memisevic"], "venue": "arXiv preprint arXiv:1602.05110,", "citeRegEx": "Im et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Im et al\\.", "year": 2016}, {"title": "Random generation of combinatorial structures from a uniform distribution", "author": ["Mark R. Jerrum", "Leslie G. Valiant", "Vijay V. Vazirani"], "venue": "Theoretical Computer Science,", "citeRegEx": "Jerrum et al\\.,? \\Q1986\\E", "shortCiteRegEx": "Jerrum et al\\.", "year": 1986}, {"title": "Auto-encoding variational bayes", "author": ["Diederik P. Kingma", "Max Welling"], "venue": "In ICLR,", "citeRegEx": "Kingma and Welling.,? \\Q2014\\E", "shortCiteRegEx": "Kingma and Welling.", "year": 2014}, {"title": "Gradient-based learning applied to document recognition", "author": ["Y. LeCun", "L. Bottou", "Y. Bengio", "P. Haffner"], "venue": "Proceedings of the IEEE,", "citeRegEx": "LeCun et al\\.,? \\Q1998\\E", "shortCiteRegEx": "LeCun et al\\.", "year": 1998}, {"title": "Generative moment matching networks", "author": ["Yujia Li", "Kevin Swersky"], "venue": "In In ICML", "citeRegEx": "Li and Swersky.,? \\Q2015\\E", "shortCiteRegEx": "Li and Swersky.", "year": 2015}, {"title": "Annealed importance sampling", "author": ["Radford M. Neal"], "venue": "Statistics and Computing,", "citeRegEx": "Neal.,? \\Q2001\\E", "shortCiteRegEx": "Neal.", "year": 2001}, {"title": "MCMC using Hamiltonian dynamics", "author": ["Radford M. Neal"], "venue": "Handbook of Markov Chain Monte Carlo,", "citeRegEx": "Neal.,? \\Q2010\\E", "shortCiteRegEx": "Neal.", "year": 2010}, {"title": "On estimation of a probability density function and mode", "author": ["Emanuel Parzen"], "venue": "The Annals of Mathematical Statistics,", "citeRegEx": "Parzen.,? \\Q1962\\E", "shortCiteRegEx": "Parzen.", "year": 1962}, {"title": "Unsupervised representation learning with deep convolutional generative adversarial networks", "author": ["Alec Radford", "Luke Metz", "Soumith Chintala"], "venue": "In ICLR,", "citeRegEx": "Radford et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Radford et al\\.", "year": 2016}, {"title": "Factored 3-way restricted Boltzmann machines for modeling natural images", "author": ["Marc\u2019Aurelio Ranzato", "Alex Krizhevsky", "Geoffrey E Hinton"], "venue": "In International Conference on Artificial Intelligence and Statistics (AISTATS),", "citeRegEx": "Ranzato et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Ranzato et al\\.", "year": 2010}, {"title": "Stochastic backpropagation and approximate inference in deep generative models", "author": ["Danilo J. Rezende", "Shakir Mohamed", "Daan Wierstra"], "venue": "Proceedings of the 31st International Conference on Machine Learning", "citeRegEx": "Rezende et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Rezende et al\\.", "year": 2014}, {"title": "On the quantitative analysis of Deep Belief Networks", "author": ["Ruslan Salakhutdinov", "Iain Murray"], "venue": "Proceedings of the 25th Annual International Conference on Machine Learning (ICML", "citeRegEx": "Salakhutdinov and Murray.,? \\Q2008\\E", "shortCiteRegEx": "Salakhutdinov and Murray.", "year": 2008}, {"title": "Improved techniques for training gans", "author": ["Tim Salimans", "Ian Goodfellow", "Wojciech Zaremba", "Vicki Cheung", "Alec Radford", "Xi Chen"], "venue": "In NIPS,", "citeRegEx": "Salimans et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Salimans et al\\.", "year": 2016}, {"title": "A note on the evaluation of generative models", "author": ["Lucas Theis", "A\u00e4ron van den Oord", "Matthias Bethge"], "venue": "In ICLR,", "citeRegEx": "Theis et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Theis et al\\.", "year": 2016}, {"title": "The variational Gaussian process", "author": ["Dustin Tran", "Rajesh Ranganath", "David M. Blei"], "venue": "In ICLR,", "citeRegEx": "Tran et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Tran et al\\.", "year": 2016}, {"title": "RNADE: The real-valued neural autoregressive density-estimator", "author": ["Benigno Uria", "Iain Murray", "Hugo Larochelle"], "venue": "In Advances in Neural Information Processing Systems", "citeRegEx": "Uria et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Uria et al\\.", "year": 2013}, {"title": "Pixel recurrent neural networks", "author": ["A\u00e4ron van den Oord", "Nal Kalchbrenner", "Koray Kavukcuoglu"], "venue": "In ICML,", "citeRegEx": "Oord et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Oord et al\\.", "year": 2016}, {"title": "Scale mixtures of Gaussians and the statistics of natural images", "author": ["Martin J. Wainwright", "Eero P. Simoncelli"], "venue": "In NIPS,", "citeRegEx": "Wainwright and Simoncelli.,? \\Q1999\\E", "shortCiteRegEx": "Wainwright and Simoncelli.", "year": 1999}, {"title": "Generative visual manipulation on the natural image manifold", "author": ["Jun-Yan Zhu", "Philipp Kr\u00e4henb\u00fchl", "Eli Shechtman", "Alexei A. Efros"], "venue": "In Proceedings of European Conference on Computer Vision (ECCV),", "citeRegEx": "Zhu et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Zhu et al\\.", "year": 2016}], "referenceMentions": [{"referenceID": 19, "context": "In recent years, deep generative models have dramatically pushed forward the state-of-the-art in generative modelling by generating convincing samples of images (Radford et al., 2016), achieving state-of-the-art semi-supervised learning results (Salimans et al.", "startOffset": 161, "endOffset": 183}, {"referenceID": 23, "context": ", 2016), achieving state-of-the-art semi-supervised learning results (Salimans et al., 2016), and enabling automatic image manipulation (Zhu et al.", "startOffset": 69, "endOffset": 92}, {"referenceID": 29, "context": ", 2016), and enabling automatic image manipulation (Zhu et al., 2016).", "startOffset": 51, "endOffset": 69}, {"referenceID": 21, "context": "Important examples include variational autoencoders (VAEs) (Kingma & Welling, 2014; Rezende et al., 2014), generative adversarial networks (GANs) (Goodfellow et al.", "startOffset": 59, "endOffset": 105}, {"referenceID": 8, "context": ", 2014), generative adversarial networks (GANs) (Goodfellow et al., 2014), generative moment matching networks (GMMNs) (Li & Swersky, 2015; Dziugaite et al.", "startOffset": 48, "endOffset": 73}, {"referenceID": 7, "context": ", 2014), generative moment matching networks (GMMNs) (Li & Swersky, 2015; Dziugaite et al., 2015), and nonlinear independent components estimation (Dinh et al.", "startOffset": 53, "endOffset": 97}, {"referenceID": 5, "context": ", 2015), and nonlinear independent components estimation (Dinh et al., 2014).", "startOffset": 57, "endOffset": 76}, {"referenceID": 4, "context": "While many decoder-based models are able to produce convincing samples (Denton et al., 2015; Radford et al., 2016), rigorous evaluation remains a challenge.", "startOffset": 71, "endOffset": 114}, {"referenceID": 19, "context": "While many decoder-based models are able to produce convincing samples (Denton et al., 2015; Radford et al., 2016), rigorous evaluation remains a challenge.", "startOffset": 71, "endOffset": 114}, {"referenceID": 24, "context": "Comparing models by inspecting samples is labor-intensive, and potentially misleading (Theis et al., 2016).", "startOffset": 86, "endOffset": 106}, {"referenceID": 2, "context": "While alternative quantitative criteria have been proposed (Bounliphone et al., 2016; Im et al., 2016; Salimans et al., 2016), log-likelihood of held-out test data remains one of the most important measures of a generative model\u2019s performance.", "startOffset": 59, "endOffset": 125}, {"referenceID": 11, "context": "While alternative quantitative criteria have been proposed (Bounliphone et al., 2016; Im et al., 2016; Salimans et al., 2016), log-likelihood of held-out test data remains one of the most important measures of a generative model\u2019s performance.", "startOffset": 59, "endOffset": 125}, {"referenceID": 23, "context": "While alternative quantitative criteria have been proposed (Bounliphone et al., 2016; Im et al., 2016; Salimans et al., 2016), log-likelihood of held-out test data remains one of the most important measures of a generative model\u2019s performance.", "startOffset": 59, "endOffset": 125}, {"referenceID": 5, "context": "Unfortunately, unless the decoder is designed to be reversible (Dinh et al., 2014; 2016), log-likelihood estimation in decoder-based models is typically intractable.", "startOffset": 63, "endOffset": 88}, {"referenceID": 18, "context": "The most widely used estimator of log-likelihood for GANs and GMMNs is the Kernel Density Estimator (KDE) (Parzen, 1962).", "startOffset": 106, "endOffset": 120}, {"referenceID": 24, "context": "Unfortunately, KDE is notoriously inaccurate for estimating likelihood in high dimensions, because it is hard to tile a high-dimensional manifold with spherical Gaussians (Theis et al., 2016).", "startOffset": 171, "endOffset": 191}, {"referenceID": 16, "context": "In this paper, we propose to use annealed importance sampling (AIS; (Neal, 2001)) to estimate log-likelihoods of decoder-based generative models and to obtain approximate posterior samples.", "startOffset": 68, "endOffset": 80}, {"referenceID": 10, "context": "Importantly, we validate this approach using Bidirectional Monte Carlo (BDMC) (Grosse et al., 2015), which provably bounds the log-likelihood estimation error and the KL divergence from the true posterior distribution for data simulated from a model.", "startOffset": 78, "endOffset": 99}, {"referenceID": 8, "context": "(Kingma & Welling, 2014), Generative Adversarial Network (GAN) (Goodfellow et al., 2014), and Generative Moment Matching Network (GMMN) (Li & Swersky, 2015; Dziugaite et al.", "startOffset": 63, "endOffset": 88}, {"referenceID": 7, "context": ", 2014), and Generative Moment Matching Network (GMMN) (Li & Swersky, 2015; Dziugaite et al., 2015).", "startOffset": 55, "endOffset": 99}, {"referenceID": 8, "context": "A generative adversarial network (GAN) (Goodfellow et al., 2014) is a generative model trained by a game between a decoder network and a discriminator network.", "startOffset": 39, "endOffset": 64}, {"referenceID": 7, "context": "Generative moment matching networks (GMMNs) (Li & Swersky, 2015; Dziugaite et al., 2015) adopt maximum mean discrepancy (MMD) as the training objective, a moment matching criterion where kernel mean embedding techniques are used to avoid unnecessary assumptions of the distributions.", "startOffset": 44, "endOffset": 88}, {"referenceID": 16, "context": "Annealed importance sampling (AIS; (Neal, 2001)) is a Monte Carlo algorithm commonly used to estimate (ratios of) normalizing constants.", "startOffset": 35, "endOffset": 47}, {"referenceID": 10, "context": "In general, we note that logarithm of a nonnegative unbiased estimate is a stochastic lower bound of the log estimand (Grosse et al., 2015).", "startOffset": 118, "endOffset": 139}, {"referenceID": 12, "context": "While posterior sampling is just as hard as log-likelihood estimation (Jerrum et al., 1986), in the case of log-likelihood estimation for simulated data, one has available a single exact posterior sample: the parameters and/or latent variables which generated the data.", "startOffset": 70, "endOffset": 91}, {"referenceID": 9, "context": "In general, we note that logarithm of a nonnegative unbiased estimate is a stochastic lower bound of the log estimand (Grosse et al., 2015). In particular, log p\u0302(x) is a stochastic lower bound on log p(x), satisfying E[p\u0302(x)] \u2264 p(x) and Pr(p\u0302(x) > p(x) + b) < e\u2212b. Grosse et al. (2015) pointed out that if AIS is run in reverse starting from an exact posterior sample, it yields an unbiased estimate of 1/p(x), which (by the above argument) can be seen as a stochastic upper bound on log p(x).", "startOffset": 119, "endOffset": 287}, {"referenceID": 9, "context": "In general, we note that logarithm of a nonnegative unbiased estimate is a stochastic lower bound of the log estimand (Grosse et al., 2015). In particular, log p\u0302(x) is a stochastic lower bound on log p(x), satisfying E[p\u0302(x)] \u2264 p(x) and Pr(p\u0302(x) > p(x) + b) < e\u2212b. Grosse et al. (2015) pointed out that if AIS is run in reverse starting from an exact posterior sample, it yields an unbiased estimate of 1/p(x), which (by the above argument) can be seen as a stochastic upper bound on log p(x). The combination of lower and upper bounds from forward and reverse AIS is known as bidirectional Monte Carlo (BDMC). In many cases, the combination of bounds can pinpoint the true value quite precisely. While posterior sampling is just as hard as log-likelihood estimation (Jerrum et al., 1986), in the case of log-likelihood estimation for simulated data, one has available a single exact posterior sample: the parameters and/or latent variables which generated the data. Because this trick is only applicable to simulated data, BDMC is most useful for measuring the accuracy of a log-likelihood estimator on simulated data. Grosse et al. (2016) observed that BDMC can also be used to validate posterior inference algorithms, as the gap between upper and lower bounds is itself a bound on the KL divergence of approximate samples from the true posterior distribution.", "startOffset": 119, "endOffset": 1142}, {"referenceID": 18, "context": "In order for the likelihood to be well-defined, we follow the same assumption made when evaluating using Kernel Density Estimator (Parzen, 1962): we assume a Gaussian observation model with a fixed variance hyperparameter \u03c3.", "startOffset": 130, "endOffset": 144}, {"referenceID": 10, "context": "Because SIS is an unbiased estimator of the likelihood, log p\u0302\u03c3(x) is a stochastic lower bound on log p\u03c3(x) (Grosse et al., 2015).", "startOffset": 108, "endOffset": 129}, {"referenceID": 17, "context": "Since all of our experiments are done using continuous latent space, we use Hamiltonian Monte Carlo (Neal, 2010) as the transition operator for sampling latent samples along annealing.", "startOffset": 100, "endOffset": 112}, {"referenceID": 23, "context": "Theis et al. (2016) give an in-depth analysis of issues that might come up in evaluating generative models.", "startOffset": 0, "endOffset": 20}, {"referenceID": 23, "context": "Salimans et al. (2016) propose an image-quality measure which they find to be highly correlated with human visual judgement.", "startOffset": 0, "endOffset": 23}, {"referenceID": 14, "context": "1 DATASETS All of our experiments were performed on the MNIST dataset of images of handwritten digits (LeCun et al., 1998).", "startOffset": 102, "endOffset": 122}, {"referenceID": 14, "context": "1 DATASETS All of our experiments were performed on the MNIST dataset of images of handwritten digits (LeCun et al., 1998). For consistency with prior work on evaluating decoder-based models, most of our experiments used the continuous inputs. We dequantized the data following Uria et al. (2013), by adding a uniform noise of 1 256 to the data and rescaling it to be in [0, 1] D after dequantization.", "startOffset": 103, "endOffset": 297}, {"referenceID": 16, "context": "1 Following Ranzato et al. (2010), the HMC stepsize was tuned to achieve an acceptance rate of 0.", "startOffset": 12, "endOffset": 34}, {"referenceID": 14, "context": "65 (as recommended by Neal (2010)).", "startOffset": 22, "endOffset": 34}, {"referenceID": 9, "context": "(The BDMC gap is not guaranteed to hold for the real data, although Grosse et al. (2016) found the behavior of AIS to match closely between real and simulated data.", "startOffset": 68, "endOffset": 89}, {"referenceID": 18, "context": "Kernel density estimation (KDE) (Parzen, 1962) is widely used to evaluate decoder-based models (Goodfellow et al.", "startOffset": 32, "endOffset": 46}, {"referenceID": 8, "context": "Kernel density estimation (KDE) (Parzen, 1962) is widely used to evaluate decoder-based models (Goodfellow et al., 2014; Li & Swersky, 2015), and a variant was proposed in the setting of evaluating Boltzmann machines (Bengio et al.", "startOffset": 95, "endOffset": 140}, {"referenceID": 1, "context": ", 2014; Li & Swersky, 2015), and a variant was proposed in the setting of evaluating Boltzmann machines (Bengio et al., 2013).", "startOffset": 104, "endOffset": 125}, {"referenceID": 3, "context": "This is provably more accurate than the VAE bound (Burda et al., 2016).", "startOffset": 50, "endOffset": 70}, {"referenceID": 3, "context": "Because the importance weighted estimate corresponds to the objective function used by the Importance Weighted Autoencoder (IWAE) (Burda et al., 2016), we will refer to it as the IWAE bound.", "startOffset": 130, "endOffset": 150}, {"referenceID": 25, "context": ", it represents about half of the gap between a state-of-the-art permutation-invariant model (Tran et al., 2016) and one which exploits structure (van den Oord et al.", "startOffset": 93, "endOffset": 112}, {"referenceID": 24, "context": "The commonly reported nearest-neighbors from the training set can be misleading (Theis et al., 2016), and interpolation in the latent space between different samples can be visually appealing, but does not provide a quantitative measure of the degree of generalization.", "startOffset": 80, "endOffset": 100}, {"referenceID": 9, "context": "While these samples are approximate, Grosse et al. (2016) point out that the BDMC gap also bounds the KL divergence of approximate samples from the true posterior.", "startOffset": 37, "endOffset": 58}], "year": 2016, "abstractText": "The past several years have seen remarkable progress in generative models which produce convincing samples of images and other modalities. A shared component of many powerful generative models is a decoder network, a parametric deep neural net that defines a generative distribution. Examples include variational autoencoders, generative adversarial networks, and generative moment matching networks. Unfortunately, it can be difficult to quantify the performance of these models because of the intractability of log-likelihood estimation, and inspecting samples can be misleading. We propose to use Annealed Importance Sampling for evaluating log-likelihoods for decoder-based models and validate its accuracy using bidirectional Monte Carlo. Using this technique, we analyze the performance of decoder-based models, the effectiveness of existing log-likelihood estimators, the degree of overfitting, and the degree to which these models miss important modes of the data distribution.", "creator": "LaTeX with hyperref package"}}}