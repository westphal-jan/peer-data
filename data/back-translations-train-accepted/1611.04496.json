{"id": "1611.04496", "review": {"conference": "iclr", "VERSION": "v1", "DATE_OF_SUBMISSION": "14-Nov-2016", "title": "Multi-view Recurrent Neural Acoustic Word Embeddings", "abstract": "Recent work has begun exploring neural acoustic word embeddings--fixed-dimensional vector representations of arbitrary-length speech segments corresponding to words. Such embeddings are applicable to speech retrieval and recognition tasks, where reasoning about whole words may make it possible to avoid ambiguous sub-word representations. The main idea is to map acoustic sequences to fixed-dimensional vectors such that examples of the same word are mapped to similar vectors, while different-word examples are mapped to very different vectors. In this work we take a multi-view approach to learning acoustic word embeddings, in which we jointly learn to embed acoustic sequences and their corresponding character sequences. We use deep bidirectional LSTM embedding models and multi-view contrastive losses. We study the effect of different loss variants, including fixed-margin and cost-sensitive losses. Our acoustic word embeddings improve over previous approaches for the task of word discrimination. We also present results on other tasks that are enabled by the multi-view approach, including cross-view word discrimination and word similarity.", "histories": [["v1", "Mon, 14 Nov 2016 17:47:03 GMT  (900kb)", "https://arxiv.org/abs/1611.04496v1", "In submission to ICLR 2017"], ["v2", "Fri, 10 Mar 2017 23:57:57 GMT  (1464kb,D)", "http://arxiv.org/abs/1611.04496v2", "Appearing in ICLR 2017"]], "COMMENTS": "In submission to ICLR 2017", "reviews": [], "SUBJECTS": "cs.CL", "authors": ["wanjia he", "weiran wang", "karen livescu"], "accepted": true, "id": "1611.04496"}, "pdf": {"name": "1611.04496.pdf", "metadata": {"source": "CRF", "title": "MULTI-VIEW RECURRENT NEURAL ACOUSTIC WORD EMBEDDINGS", "authors": ["Wanjia He", "Weiran Wang"], "emails": ["wanjia@ttic.edu", "weiranwang@ttic.edu", "klivescu@ttic.edu"], "sections": [{"heading": "1 INTRODUCTION", "text": "Word embeddings can be learned using spectral methods (Deerwester et al., 1990) or, more frequently in recent work, via neural networks (Bengio et al., 2003; Mnih & Hinton, 2007; Mikolov et al., 2013; Pennington et al., 2014) Word embeddings can also be embeddings of phrases, sets, or documents (Socher et al., 2014; Kiros et al., 2015; Wieting et al., 2016; Iyyer et al., 2015).Word embeddings can also form embeddings of phrases, sets, or documents (Socher et al., 2015; Emdons et al., 2015; Cordices et al., 2016; Iyyer et al.). Word embeddings is intended to represent the semantics of the corresponding words / sequences."}, {"heading": "2 OUR APPROACH", "text": "In this section, we first present our approach to learning acoustic word embedding in a multiview setting, after briefly looking at related approaches to put ours in context, and then discuss the specific architecture of neural networks we use based on bidirectional long-term short-term memory networks (LSTM networks) (Hochreiter & Schmidhuber, 1997)."}, {"heading": "2.1 MULTI-VIEW LEARNING OF ACOUSTIC WORD EMBEDDINGS", "text": "In fact, it is not the case that it is a matter of a way in which a term is formed that relates to the terminology of the individual terms. (...) It is not as if the terminology of the individual terms in the individual term levels of the individual terms and term styles could be distinguished in the individual term levels. (...) It is not the case that the terminology of the individual term texts in the individual term types of the individual term types of the individual term stamps and term stamps in the individual term stamps and term stamps-term stamps-term-term stamps-term-term-term-type-term-stamp-term-term-term-term-type-term-stamp-term-term-term-term-stamp-term-term-term-type-term-term-stamp-term-term-term-stamp-term-term-term"}, {"heading": "2.2 RECURRENT NEURAL NETWORK ARCHITECTURE", "text": "Since the inputs of both views have a sequential structure, we implement both f and g with recursive neural networks, and in particular long-term short-term memory networks (LSTMs); recursive neural networks are the most advanced models for a range of speech tasks, including speech recognition Graves et al. (2013), and LSTM-based acoustic word embedding has yielded the best results in one of the tasks in our experiments (Settle & Livescu, 2016).As shown in Figure 1, our f and g are generated by multilayered (stacked) bidirectional LSTMs. Inputs can be any acoustic character representations at the frame level and vector representations of the letters in the orthographic input. At each level, two LSTM cells process the input sequence from left to right or from left to left. On intermediate layers, the outputs of the two LSTMs are set to form the last layer, with each input to form the next layer."}, {"heading": "3 RELATED WORK", "text": "We are not aware of any previous work in the field of multi-view learning of acoustic and character-related word embedding, but we have not even begun to deal with other topics. Levin et al. (2013) has proposed an approach for embedding an arbitrarily long word segment of language as a fixed dimensional vector, based on the representation of each word as a vector of dynamic pastime (DTW). One disadvantage of this approach is that while DTW addresses the problem of variable word differentiation, it is also successfully applied to a query-by-example task (Levin et al.)."}, {"heading": "4 EXPERIMENTS AND RESULTS", "text": "The ultimate goal is to achieve improvements in language systems where word differentiation is needed, such as speech recognition and retrieval of examples. However, in order to focus on the content of the embedding itself and to be able to compare a variety of models more quickly, it is desirable that we have some intrinsic measurements of performance. Here, we look at three forms of evaluation, all based on measuring cosmic distances between learned embedding that correspond well to the desired characteristics. In the first task, we are given a pair of acoustic sequences and must decide whether they correspond to the same word or other words. This task has been used in several previous papers on acoustic word embedding that Kamper has embedded. Chung et al. (2016); Settle & Livescu and we are a proxy for query-by-example search."}, {"heading": "4.1 DATA", "text": "We use the same experimental setup and data as in Kamper et al. (2015, 2016); Settle & Livescu (2016); the task and setup were first developed by (Carlin et al., 2011); the data comes from the English conversation corpus of the switchboard (Godfrey et al., 1992); the spoken word segments range in duration from 50 to 200 frames (0.5 - 2 seconds); the train / developer / test splitter contains 9971 / 10966 / 11024 pairs of acoustic segments and character sequences corresponding to 1687 / 3918 / 3390 unique words; when calculating the AP for the developer or test set, all pairs of the set are used, resulting in approximately 60 million word pairs; the input into the acoustic view embedding model is a sequence of 39-dimensional vectors (one per frame) of standard telefrequency receivers (MFCs) and their first and second derivatives each of 26."}, {"heading": "4.2 MODEL DETAILS AND HYPERPARAMETER TUNING", "text": "We experiment with different neural network architectures for each view, varying the number of stacked LSTM layers, the number of hidden units for each layer, and the use of single- or bidirectional LSTM cells. A rough grid search shows that two-layer bidirectional LSTMs with 512 hidden units per layer perform well in the task of acoustic word discrimination, and we record this structure for subsequent experiments (see Appendix A for more details). We use the results of the top-layer LSTMs as learned embedding for each view that is 1024-dimensional when using bidirectional LSTMs. In training, we use exposure sequences at the inputs of the acoustic view and between stacked layers for both views. The architecture is illustrated in Figure 1. For each training example, our contrasting losses require a corresponding negative example."}, {"heading": "4.3 EFFECTS OF DIFFERENT OBJECTIVES", "text": "We presented four contrastive losses (3) - (6) and potential combinations in Section 2.1. We are now examining the impact of these different targets on word discrimination tasks. Table 1 shows the development group AP for acoustic and cross-view word discrimination achieved with the different targets. We matched the targets for the acoustic discrimination task and then used the corresponding convergent models for the cross-view task. Of the simple contrastive targets, Obj0 and Obj2 (which only include cross-view distances) slightly exceed the other two in terms of the acoustic word discrimination task. We note that the most powerful target is the \"symmetrized\" objective Obj0 + Obj2, which significantly exceeds all individual targets (and the combination of the four). Finally, the cost-sensitive target is very competitive while slightly lagging behind the best word discrimination task. We note that a similar objective to our Objy0 + Obyyy2 objective was not used by us in the reactive task 6eval (6v)."}, {"heading": "4.4 WORD SIMILARITY TASKS", "text": "Table 3 gives our results on the tasks of word similarity, i.e. the rank correlation (Spearman's threat) between the embedding distances and the orthographic processing distance (Levenshtein distance between the strings).We measure this correlation for both our acoustic word embedding and our text embedding. In the case of text embedding, of course, we could directly measure the levenshtein distance between the inputs; here we simply measure how much of this information the text embedding can contain. 7Interestingly, the cost-sensitive goal has not brought any significant benefits to the above word discrimination tasks, but it significantly improves the performance of this word similarity measure, as the cost-sensitive loss attempts to improve precisely this relationship between the distances in the embedding space and the orthographic processing distance."}, {"heading": "4.5 VISUALIZATION OF LEARNED EMBEDDINGS", "text": "Figure 3 shows a two-dimensional t-SNE (van der Maaten & Hinton, 2008) visualization of selected acoustic and character sequences from the development set, including some words seen in the development set and some previously unseen words. Previously, the words in this figure were randomly selected from among those that occur at least 15 times in the development set (the invisible words are the only six that occur at least 15 times in the development set).This visualization shows that the acoustic embedding is very close to each other and very close to the text embedding, and that invisible words line up almost as closely as previously seen.While Figure 3 shows the relationship between the multiple acoustic embedding and the text embedding, the words are all very different, so we cannot draw any conclusions about the relationships between words. Figure 4 provides another visualization that this time examines the relationship between a number of closely related words, namely, \"pretty closely\" and \"closely.\""}, {"heading": "5 CONCLUSION", "text": "We have presented an approach to joint learning of acoustic word embeddings and their orthographic equivalents, which results in improved performance of acoustic word embeddings compared to previous approaches, and also has the advantage that the same embeddings can be used for both oral and written query tasks. We have examined a variety of contrasting objectives: those with a fixed margin aimed at separating equal and different word pairs, as well as a cost-sensitive loss aimed at capturing orthographic processing distances. While losses are generally similar for word discrimination tasks, the cost-sensitive loss improves the correlation between embedspacing and orthographic distances. An interesting direction for future work is the direct use of knowledge of phonetic pronunciations in both evaluation and training. Another direction is the expansion of our approach to directly train both word and non-word segments.8"}, {"heading": "ACKNOWLEDGMENTS", "text": "This research was supported by a Google Faculty Award and NSF grant IIS-1321015. Opinions expressed in this work are those of the authors and do not necessarily reflect the views of the funding agency. GPUs donated by NVIDIA Corporation were used for this research. We thank Herman Kamper and Shane Settle for their support with the data and the experimental setup. 9"}, {"heading": "A ADDITIONAL ANALYSIS", "text": "We first examine the impact of network architectures on our embedding models. We learn embedding using objective objects0 and evaluate it using acoustic and interactive word discrimination tasks. The resulting average precision on the development set is shown in Table 4. All models were trained for 1000 epochs, except the single-layer unidirectional models, which converged after 500 epochs. It is clear that bidirectional LSTMs are more successful for these tasks than unidirectional LSTMs, and two layers of LSTMs are much better than a single layer of LSTMs. We did not find a significant further improvement by using more than two layers of LSTMs. In all other experiments, we fix the architecture for each view to 2-layer bidirectional LSTMs. Figure 5 also gives the precision recall curve for our best models, as well as the scatter graph of cosinacoustic spacing between embedding interferences 13."}], "references": [{"title": "Query by example search on speech at mediaeval", "author": ["Xavier Anguera", "Luis Javier Rodriguez-Fuentes", "Igor Sz\u00f6ke", "Andi Buzo", "Florian Metze"], "venue": "In MediaEval,", "citeRegEx": "Anguera et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Anguera et al\\.", "year": 2014}, {"title": "End-to-end ASR-free keyword search from speech", "author": ["Kartik Audhkhasi", "Andrew Rosenberg", "Abhinav Sethy", "Bhuvana Ramabhadran", "Brian Kingsbury"], "venue": "arXiv preprint arXiv:1701.04313,", "citeRegEx": "Audhkhasi et al\\.,? \\Q2017\\E", "shortCiteRegEx": "Audhkhasi et al\\.", "year": 2017}, {"title": "Word embeddings for speech recognition", "author": ["Samy Bengio", "Georg Heigold"], "venue": "In IEEE Int. Conf. Acoustics, Speech and Sig. Proc.,", "citeRegEx": "Bengio and Heigold.,? \\Q2014\\E", "shortCiteRegEx": "Bengio and Heigold.", "year": 2014}, {"title": "A neural probabilistic language model", "author": ["Yoshua Bengio", "R\u00e9jean Ducharme", "Pascal Vincent", "Christian Jauvin"], "venue": "Journal of Machine Learing Research,", "citeRegEx": "Bengio et al\\.,? \\Q2003\\E", "shortCiteRegEx": "Bengio et al\\.", "year": 2003}, {"title": "Signature verification using a siamese time delay neural network", "author": ["Jane Bromley", "Isabelle Guyon", "Yann Lecun", "Eduard S\u00e4ckinger", "Roopak Shah"], "venue": "In Advances in Neural Information Processing Systems (NIPS),", "citeRegEx": "Bromley et al\\.,? \\Q1993\\E", "shortCiteRegEx": "Bromley et al\\.", "year": 1993}, {"title": "Rapid evaluation of speech representations for spoken term discovery", "author": ["Michael A Carlin", "Samuel Thomas", "Aren Jansen", "Hynek Hermansky"], "venue": "In Proc. Interspeech,", "citeRegEx": "Carlin et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Carlin et al\\.", "year": 2011}, {"title": "Query-by-example keyword spotting using long short-term memory networks", "author": ["Guoguo Chen", "Carolina Parada", "Tara N Sainath"], "venue": "In Proc. ICASSP,", "citeRegEx": "Chen et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Chen et al\\.", "year": 2015}, {"title": "Learning a similarity metric discriminatively, with application to face verification", "author": ["Sumit Chopra", "Raia Hadsell", "Yann LeCun"], "venue": "In IEEE Computer Society Conf. Computer Vision and Pattern Recognition,", "citeRegEx": "Chopra et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Chopra et al\\.", "year": 2005}, {"title": "Unsupervised learning of audio segment representations using sequence-to-sequence recurrent neural networks", "author": ["Yu-An Chung", "Chao-Chung Wu", "Chia-Hao Shen", "Hung-Yi Lee"], "venue": "In Proc. Interspeech,", "citeRegEx": "Chung et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Chung et al\\.", "year": 2016}, {"title": "Indexing by latent semantic analysis", "author": ["Scott Deerwester", "Susan T Dumais", "George W Furnas", "Thomas K Landauer", "Richard Harshman"], "venue": "Journal of the American society for information science,", "citeRegEx": "Deerwester et al\\.,? \\Q1990\\E", "shortCiteRegEx": "Deerwester et al\\.", "year": 1990}, {"title": "Placing search in context: The concept revisited", "author": ["Lev Finkelstein", "Evgeniy Gabrilovich", "Yossi Matias", "Ehud Rivlin", "Zach Solan", "Gadi Wolfman", "Eytan Ruppin"], "venue": "In Proceedings of the 10th international conference on World Wide Web,", "citeRegEx": "Finkelstein et al\\.,? \\Q2001\\E", "shortCiteRegEx": "Finkelstein et al\\.", "year": 2001}, {"title": "Results of the 2006 spoken term detection evaluation", "author": ["Jonathan G Fiscus", "Jerome Ajot", "John S Garofolo", "George Doddingtion"], "venue": "In Proc. SIGIR,", "citeRegEx": "Fiscus et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Fiscus et al\\.", "year": 2007}, {"title": "Exemplar-based sparse representations for noise robust automatic speech recognition", "author": ["Jort F Gemmeke", "Tuomas Virtanen", "Antti Hurmalainen"], "venue": "IEEE Transactions on Acoustics, Speech, and Language Processing,", "citeRegEx": "Gemmeke et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Gemmeke et al\\.", "year": 2011}, {"title": "Evaluation of acoustic word embeddings", "author": ["Sahar Ghannay", "Yannick Esteve", "Nathalie Camelin", "Paul Deleglise"], "venue": "In Proc. ACL Workshop on Evaluating Vector-Space Representations for NLP,", "citeRegEx": "Ghannay et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Ghannay et al\\.", "year": 2016}, {"title": "SWITCHBOARD: Telephone speech corpus for research and development", "author": ["John J Godfrey", "Edward C Holliman", "Jane McDaniel"], "venue": "In IEEE Int. Conf. Acoustics, Speech and Sig. Proc.,", "citeRegEx": "Godfrey et al\\.,? \\Q1992\\E", "shortCiteRegEx": "Godfrey et al\\.", "year": 1992}, {"title": "Speech recognition with deep recurrent neural networks", "author": ["Alex Graves", "Abdel rahman Mohamed", "Geoffrey Hinton"], "venue": "In IEEE Int. Conf. Acoustics, Speech and Sig. Proc.,", "citeRegEx": "Graves et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Graves et al\\.", "year": 2013}, {"title": "Dimensionality reduction by learning an invariant mapping", "author": ["Raia Hadsell", "Sumit Chopra", "Yann LeCun"], "venue": "In IEEE Computer Society Conf. Computer Vision and Pattern Recognition,", "citeRegEx": "Hadsell et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Hadsell et al\\.", "year": 2006}, {"title": "Deep multimodal semantic embeddings for speech and images", "author": ["David Harwath", "James Glass"], "venue": "In Proc. IEEE Workshop on Automatic Speech Recognition and Understanding (ASRU),", "citeRegEx": "Harwath and Glass.,? \\Q2015\\E", "shortCiteRegEx": "Harwath and Glass.", "year": 2015}, {"title": "Learning word-like units from joint audio-visual analysis", "author": ["David Harwath", "James R Glass"], "venue": "arXiv preprint arXiv:1701.07481,", "citeRegEx": "Harwath and Glass.,? \\Q2017\\E", "shortCiteRegEx": "Harwath and Glass.", "year": 2017}, {"title": "Unsupervised learning of spoken language with visual context", "author": ["David Harwath", "Antonio Torralba", "James Glass"], "venue": "In Advances in Neural Information Processing Systems (NIPS),", "citeRegEx": "Harwath et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Harwath et al\\.", "year": 2016}, {"title": "Multilingual distributed representations without word alignment", "author": ["Karl Moritz Hermann", "Phil Blunsom"], "venue": "In Int. Conf. Learning Representations,", "citeRegEx": "Hermann and Blunsom.,? \\Q2014\\E", "shortCiteRegEx": "Hermann and Blunsom.", "year": 2014}, {"title": "SimLex-999: Evaluating semantic models with (genuine) similarity estimation", "author": ["Felix Hill", "Roi Reichart", "Anna Korhonen"], "venue": "Computational Linguistics,", "citeRegEx": "Hill et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Hill et al\\.", "year": 2015}, {"title": "Long short-term memory", "author": ["Sepp Hochreiter", "J\u00fcrgen Schmidhuber"], "venue": "Neural Computation,", "citeRegEx": "Hochreiter and Schmidhuber.,? \\Q1997\\E", "shortCiteRegEx": "Hochreiter and Schmidhuber.", "year": 1997}, {"title": "Convolutional neural network architectures for matching natural language sentences", "author": ["Baotian Hu", "Zhengdong Lu", "Hang Li", "Qingcai Chen"], "venue": "In Advances in Neural Information Processing Systems (NIPS),", "citeRegEx": "Hu et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Hu et al\\.", "year": 2014}, {"title": "Deep unordered composition rivals syntactic methods for text classification", "author": ["Mohit Iyyer", "Varun Manjunatha", "Jordan Boyd-Graber", "Hal Daum\u00e9 III"], "venue": "In Proc. Association for Computational Linguistics,", "citeRegEx": "Iyyer et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Iyyer et al\\.", "year": 2015}, {"title": "Unsupervised neural network based feature extraction using weak top-down constraints", "author": ["Herman Kamper", "Micah Elsner", "Aren Jansen", "Sharon J. Goldwater"], "venue": "In IEEE Int. Conf. Acoustics, Speech and Sig. Proc.,", "citeRegEx": "Kamper et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Kamper et al\\.", "year": 2015}, {"title": "Deep convolutional acoustic word embeddings using word-pair side information", "author": ["Herman Kamper", "Weiran Wang", "Karen Livescu"], "venue": "In IEEE Int. Conf. Acoustics, Speech and Sig. Proc.,", "citeRegEx": "Kamper et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Kamper et al\\.", "year": 2016}, {"title": "ADAM: A method for stochastic optimization", "author": ["Diederik Kingma", "Jimmy Ba"], "venue": "In Int. Conf. Learning Representations,", "citeRegEx": "Kingma and Ba.,? \\Q2015\\E", "shortCiteRegEx": "Kingma and Ba.", "year": 2015}, {"title": "Skip-thought vectors", "author": ["Ryan Kiros", "Yukun Zhu", "Ruslan R Salakhutdinov", "Richard Zemel", "Raquel Urtasun", "Antonio Torralba", "Sanja Fidler"], "venue": "In Advances in Neural Information Processing Systems (NIPS),", "citeRegEx": "Kiros et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Kiros et al\\.", "year": 2015}, {"title": "Fixed-dimensional acoustic embeddings of variable-length segments in low-resource settings", "author": ["Keith Levin", "Katharine Henry", "Aren Jansen", "Karen Livescu"], "venue": "In Proc. IEEE Workshop on Automatic Speech Recognition and Understanding (ASRU),", "citeRegEx": "Levin et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Levin et al\\.", "year": 2013}, {"title": "Segmental acoustic indexing for zero resource keyword search", "author": ["Keith Levin", "Aren Jansen", "Benjamin Van Durme"], "venue": "In IEEE Int. Conf. Acoustics, Speech and Sig. Proc.,", "citeRegEx": "Levin et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Levin et al\\.", "year": 2015}, {"title": "Word-level acoustic modeling with convolutional vector regression", "author": ["Andrew L Maas", "Stephen D Miller", "Tyler M O\u2019neil", "Andrew Y Ng", "Patrick Nguyen"], "venue": "In Proc. ICML Workshop on Representation Learning,", "citeRegEx": "Maas et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Maas et al\\.", "year": 2012}, {"title": "Distributed representations of words and phrases and their compositionality", "author": ["Tomas Mikolov", "Ilya Sutskever", "Kai Chen", "Greg S Corrado", "Jeff Dean"], "venue": "In Advances in Neural Information Processing Systems (NIPS),", "citeRegEx": "Mikolov et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Mikolov et al\\.", "year": 2013}, {"title": "Three new graphical models for statistical language modelling", "author": ["Andriy Mnih", "Geoffrey Hinton"], "venue": "In ICML,", "citeRegEx": "Mnih and Hinton.,? \\Q2007\\E", "shortCiteRegEx": "Mnih and Hinton.", "year": 2007}, {"title": "Multimodal deep learning", "author": ["Jiquan Ngiam", "Aditya Khosla", "Mingyu Kim", "Juhan Nam", "Honglak Lee", "Andrew Ng"], "venue": "In ICML, pp", "citeRegEx": "Ngiam et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Ngiam et al\\.", "year": 2011}, {"title": "GloVe: Global vectors for word representation", "author": ["Jeffrey Pennington", "Richard Socher", "Christopher D Manning"], "venue": "In Proc. Conference on Empirical Methods in Natural Language Processing,", "citeRegEx": "Pennington et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Pennington et al\\.", "year": 2014}, {"title": "Discriminative acoustic word embeddings: Recurrent neural network-based approaches", "author": ["Shane Settle", "Karen Livescu"], "venue": "In Proc. IEEE Workshop on Spoken Language Technology (SLT),", "citeRegEx": "Settle and Livescu.,? \\Q2016\\E", "shortCiteRegEx": "Settle and Livescu.", "year": 2016}, {"title": "Grounded compositional semantics for finding and describing images with sentences", "author": ["Richard Socher", "Andrej Karpathy", "Quoc V Le", "Christopher D Manning", "Andrew Y Ng"], "venue": "Transactions of the Association for Computational Linguistics,", "citeRegEx": "Socher et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Socher et al\\.", "year": 2014}, {"title": "Improved multimodal deep learning with variation of information", "author": ["Kihyuk Sohn", "Wenling Shang", "Honglak Lee"], "venue": "In Advances in Neural Information Processing Systems (NIPS),", "citeRegEx": "Sohn et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Sohn et al\\.", "year": 2014}, {"title": "Multimodal learning with deep boltzmann machines", "author": ["Nitish Srivastava", "Ruslan Salakhutdinov"], "venue": "Journal of Machine Learing Research,", "citeRegEx": "Srivastava and Salakhutdinov.,? \\Q2014\\E", "shortCiteRegEx": "Srivastava and Salakhutdinov.", "year": 2014}, {"title": "Phonetics embedding learning with side information", "author": ["Gabriel Synnaeve", "Thomas Schatz", "Emmanuel Dupoux"], "venue": "In Proc. IEEE Workshop on Spoken Language Technology (SLT),", "citeRegEx": "Synnaeve et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Synnaeve et al\\.", "year": 2014}, {"title": "Visualizing data using t-SNE", "author": ["Laurens J.P. van der Maaten", "Geoffrey E. Hinton"], "venue": "Journal of Machine Learing Research,", "citeRegEx": "Maaten and Hinton.,? \\Q2008\\E", "shortCiteRegEx": "Maaten and Hinton.", "year": 2008}, {"title": "Order-embeddings of images and language", "author": ["Ivan Vendrov", "Ryan Kiros", "Sanja Fidler", "Raquel Urtasun"], "venue": "In Int. Conf. Learning Representations,", "citeRegEx": "Vendrov et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Vendrov et al\\.", "year": 2016}, {"title": "On deep multi-view representation learning", "author": ["Weiran Wang", "Raman Arora", "Karen Livescu", "Jeff Bilmes"], "venue": "In ICML, pp", "citeRegEx": "Wang et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Wang et al\\.", "year": 2015}, {"title": "Towards universal paraphrastic sentence embeddings", "author": ["John Wieting", "Mohit Bansal", "Kevin Gimpel", "Karen Livescu"], "venue": "In Int. Conf. Learning Representations,", "citeRegEx": "Wieting et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Wieting et al\\.", "year": 2016}], "referenceMentions": [{"referenceID": 9, "context": "Word embeddings can be learned using spectral methods (Deerwester et al., 1990) or, more commonly in recent work, via neural networks (Bengio et al.", "startOffset": 54, "endOffset": 79}, {"referenceID": 3, "context": ", 1990) or, more commonly in recent work, via neural networks (Bengio et al., 2003; Mnih & Hinton, 2007; Mikolov et al., 2013; Pennington et al., 2014).", "startOffset": 62, "endOffset": 151}, {"referenceID": 32, "context": ", 1990) or, more commonly in recent work, via neural networks (Bengio et al., 2003; Mnih & Hinton, 2007; Mikolov et al., 2013; Pennington et al., 2014).", "startOffset": 62, "endOffset": 151}, {"referenceID": 35, "context": ", 1990) or, more commonly in recent work, via neural networks (Bengio et al., 2003; Mnih & Hinton, 2007; Mikolov et al., 2013; Pennington et al., 2014).", "startOffset": 62, "endOffset": 151}, {"referenceID": 37, "context": "Word embeddings can also be composed to form embeddings of phrases, sentences, or documents (Socher et al., 2014; Kiros et al., 2015; Wieting et al., 2016; Iyyer et al., 2015).", "startOffset": 92, "endOffset": 175}, {"referenceID": 28, "context": "Word embeddings can also be composed to form embeddings of phrases, sentences, or documents (Socher et al., 2014; Kiros et al., 2015; Wieting et al., 2016; Iyyer et al., 2015).", "startOffset": 92, "endOffset": 175}, {"referenceID": 44, "context": "Word embeddings can also be composed to form embeddings of phrases, sentences, or documents (Socher et al., 2014; Kiros et al., 2015; Wieting et al., 2016; Iyyer et al., 2015).", "startOffset": 92, "endOffset": 175}, {"referenceID": 24, "context": "Word embeddings can also be composed to form embeddings of phrases, sentences, or documents (Socher et al., 2014; Kiros et al., 2015; Wieting et al., 2016; Iyyer et al., 2015).", "startOffset": 92, "endOffset": 175}, {"referenceID": 11, "context": "Such embeddings could be useful for tasks like spoken term detection (Fiscus et al., 2007), spoken query-by-example search (Anguera et al.", "startOffset": 69, "endOffset": 90}, {"referenceID": 0, "context": ", 2007), spoken query-by-example search (Anguera et al., 2014), or even speech recognition using a whole-word approach (Gemmeke et al.", "startOffset": 40, "endOffset": 62}, {"referenceID": 12, "context": ", 2014), or even speech recognition using a whole-word approach (Gemmeke et al., 2011; Bengio & Heigold, 2014).", "startOffset": 64, "endOffset": 110}, {"referenceID": 26, "context": "In tasks that involve comparing speech segments to each other, vector embeddings can allow more efficient and more accurate distance computation than sequence-based approaches such as dynamic time warping (Levin et al., 2013, 2015; Kamper et al., 2016; Settle & Livescu, 2016; Chung et al., 2016).", "startOffset": 205, "endOffset": 296}, {"referenceID": 8, "context": "In tasks that involve comparing speech segments to each other, vector embeddings can allow more efficient and more accurate distance computation than sequence-based approaches such as dynamic time warping (Levin et al., 2013, 2015; Kamper et al., 2016; Settle & Livescu, 2016; Chung et al., 2016).", "startOffset": 205, "endOffset": 296}, {"referenceID": 26, "context": "We refer to the objective in (1) as the \u201cclassifier network\u201d objective, which has been used in several prior studies on acoustic word embeddings (Bengio & Heigold, 2014; Kamper et al., 2016; Settle & Livescu, 2016).", "startOffset": 145, "endOffset": 214}, {"referenceID": 4, "context": "An alternative approach, based on Siamese networks (Bromley et al., 1993), uses supervision of the form \u201csegment x is similar to segment x, and is not similar to segment x\u201d, where two segments are considered similar if they have the same word label and dissimilar otherwise.", "startOffset": 51, "endOffset": 73}, {"referenceID": 23, "context": "Models based on Siamese networks have been used for a variety of representation learning problems in NLP (Hu et al., 2014; Wieting et al., 2016), vision (Hadsell et al.", "startOffset": 105, "endOffset": 144}, {"referenceID": 44, "context": "Models based on Siamese networks have been used for a variety of representation learning problems in NLP (Hu et al., 2014; Wieting et al., 2016), vision (Hadsell et al.", "startOffset": 105, "endOffset": 144}, {"referenceID": 16, "context": ", 2016), vision (Hadsell et al., 2006), and speech (Synnaeve et al.", "startOffset": 16, "endOffset": 38}, {"referenceID": 40, "context": ", 2006), and speech (Synnaeve et al., 2014; Kamper et al., 2015) including acoustic word embeddings (Kamper et al.", "startOffset": 20, "endOffset": 64}, {"referenceID": 25, "context": ", 2006), and speech (Synnaeve et al., 2014; Kamper et al., 2015) including acoustic word embeddings (Kamper et al.", "startOffset": 20, "endOffset": 64}, {"referenceID": 26, "context": ", 2015) including acoustic word embeddings (Kamper et al., 2016; Settle & Livescu, 2016).", "startOffset": 43, "endOffset": 88}, {"referenceID": 4, "context": "The term \u201cSiamese\u201d (Bromley et al., 1993; Chopra et al., 2005) refers to the fact that the triplet (x, x, x) share the same embedding network f .", "startOffset": 19, "endOffset": 62}, {"referenceID": 7, "context": "The term \u201cSiamese\u201d (Bromley et al., 1993; Chopra et al., 2005) refers to the fact that the triplet (x, x, x) share the same embedding network f .", "startOffset": 19, "endOffset": 62}, {"referenceID": 34, "context": "While many deep multi-view learning objectives are applicable (Ngiam et al., 2011; Srivastava & Salakhutdinov, 2014; Sohn et al., 2014; Wang et al., 2015), we consider the multi-view contrastive loss objective of (Hermann & Blunsom, 2014), which is simple to optimize and implement and performs well in practice.", "startOffset": 62, "endOffset": 154}, {"referenceID": 38, "context": "While many deep multi-view learning objectives are applicable (Ngiam et al., 2011; Srivastava & Salakhutdinov, 2014; Sohn et al., 2014; Wang et al., 2015), we consider the multi-view contrastive loss objective of (Hermann & Blunsom, 2014), which is simple to optimize and implement and performs well in practice.", "startOffset": 62, "endOffset": 154}, {"referenceID": 43, "context": "While many deep multi-view learning objectives are applicable (Ngiam et al., 2011; Srivastava & Salakhutdinov, 2014; Sohn et al., 2014; Wang et al., 2015), we consider the multi-view contrastive loss objective of (Hermann & Blunsom, 2014), which is simple to optimize and implement and performs well in practice.", "startOffset": 62, "endOffset": 154}, {"referenceID": 15, "context": "Recurrent neural networks are the state-of-the-art models for a number of speech tasks including speech recognition Graves et al. (2013), and LSTM-based acoustic word embeddings have produced the best results on one of the tasks in our experiments (Settle & Livescu, 2016).", "startOffset": 116, "endOffset": 137}, {"referenceID": 30, "context": "This approach produced improved performance on a word discrimination task compared to using raw DTW distances, and was later also applied successfully for a query-by-example task (Levin et al., 2015).", "startOffset": 179, "endOffset": 199}, {"referenceID": 21, "context": "Levin et al. (2013) proposed an approach for embedding an arbitrary-length segment of speech as a fixed-dimensional vector, based on representing each word as a vector of dynamic time warping (DTW) distances to a set of template words.", "startOffset": 0, "endOffset": 20}, {"referenceID": 19, "context": "Kamper et al. (2016) and Settle & Livescu (2016) later improved on Levin et al.", "startOffset": 0, "endOffset": 21}, {"referenceID": 19, "context": "Kamper et al. (2016) and Settle & Livescu (2016) later improved on Levin et al.", "startOffset": 0, "endOffset": 49}, {"referenceID": 19, "context": "Kamper et al. (2016) and Settle & Livescu (2016) later improved on Levin et al.\u2019s word discrimination results using convolutional neural networks (CNNs) and recurrent neural networks (RNNs) trained with either a classification or contrastive loss. Bengio & Heigold (2014) trained convolutional neural network (CNN)-based acoustic word embeddings for rescoring the outputs of a speech recognizer, using a loss combining classification and ranking criteria.", "startOffset": 0, "endOffset": 272}, {"referenceID": 19, "context": "Kamper et al. (2016) and Settle & Livescu (2016) later improved on Levin et al.\u2019s word discrimination results using convolutional neural networks (CNNs) and recurrent neural networks (RNNs) trained with either a classification or contrastive loss. Bengio & Heigold (2014) trained convolutional neural network (CNN)-based acoustic word embeddings for rescoring the outputs of a speech recognizer, using a loss combining classification and ranking criteria. Maas et al. (2012) trained a CNN to predict a semantic word embedding from an acoustic segment, and used the resulting embeddings as features in a segmental word-level speech recognizer.", "startOffset": 0, "endOffset": 475}, {"referenceID": 14, "context": "Harwath and Glass Harwath & Glass (2015); Harwath et al.", "startOffset": 0, "endOffset": 41}, {"referenceID": 14, "context": "Harwath and Glass Harwath & Glass (2015); Harwath et al. (2016); Harwath & Glass (2017) jointly trained CNN embeddings of images and spoken captions, and showed that word-like unit embeddings can be extracted from the speech model.", "startOffset": 0, "endOffset": 64}, {"referenceID": 14, "context": "Harwath and Glass Harwath & Glass (2015); Harwath et al. (2016); Harwath & Glass (2017) jointly trained CNN embeddings of images and spoken captions, and showed that word-like unit embeddings can be extracted from the speech model.", "startOffset": 0, "endOffset": 88}, {"referenceID": 5, "context": "Chen et al. (2015) used long short-term memory (LSTM) networks with a classification loss to embed acoustic words for a simple (single-query) query-by-example search task.", "startOffset": 0, "endOffset": 19}, {"referenceID": 5, "context": "Chen et al. (2015) used long short-term memory (LSTM) networks with a classification loss to embed acoustic words for a simple (single-query) query-by-example search task. Chung et al. (2016) learned acoustic word embeddings based on recurrent neural network (RNN) autoencoders, and found that they improve over DTW for a word discrimination task similar to that of Levin et al.", "startOffset": 0, "endOffset": 192}, {"referenceID": 5, "context": "Chen et al. (2015) used long short-term memory (LSTM) networks with a classification loss to embed acoustic words for a simple (single-query) query-by-example search task. Chung et al. (2016) learned acoustic word embeddings based on recurrent neural network (RNN) autoencoders, and found that they improve over DTW for a word discrimination task similar to that of Levin et al. (2013). Audhkhasi et al.", "startOffset": 0, "endOffset": 386}, {"referenceID": 1, "context": "Audhkhasi et al. (2017) learned autoencoders for acoustic and written words, as well as a model for comparing the two, and applied these to a keyword search task.", "startOffset": 0, "endOffset": 24}, {"referenceID": 29, "context": "Most evaluations have been based on word discrimination \u2013 the task of determining whether two speech segments correspond to the same word or not \u2013 which can be seen as a proxy for query-by-example search (Levin et al., 2013; Kamper et al., 2016; Settle & Livescu, 2016; Chung et al., 2016).", "startOffset": 204, "endOffset": 289}, {"referenceID": 26, "context": "Most evaluations have been based on word discrimination \u2013 the task of determining whether two speech segments correspond to the same word or not \u2013 which can be seen as a proxy for query-by-example search (Levin et al., 2013; Kamper et al., 2016; Settle & Livescu, 2016; Chung et al., 2016).", "startOffset": 204, "endOffset": 289}, {"referenceID": 8, "context": "Most evaluations have been based on word discrimination \u2013 the task of determining whether two speech segments correspond to the same word or not \u2013 which can be seen as a proxy for query-by-example search (Levin et al., 2013; Kamper et al., 2016; Settle & Livescu, 2016; Chung et al., 2016).", "startOffset": 204, "endOffset": 289}, {"referenceID": 8, "context": ", 2016; Settle & Livescu, 2016; Chung et al., 2016). One difference between word discrimination and search/recognition tasks is that in word discrimination the word boundaries are given. However, prior work has been able to apply results from word discrimination Levin et al. (2013) to improve a query-by-example system without known word boundaries Levin et al.", "startOffset": 32, "endOffset": 283}, {"referenceID": 8, "context": ", 2016; Settle & Livescu, 2016; Chung et al., 2016). One difference between word discrimination and search/recognition tasks is that in word discrimination the word boundaries are given. However, prior work has been able to apply results from word discrimination Levin et al. (2013) to improve a query-by-example system without known word boundaries Levin et al. (2015), by simply applying their embeddings to non-word segments as well.", "startOffset": 32, "endOffset": 370}, {"referenceID": 8, "context": ", 2016; Settle & Livescu, 2016; Chung et al., 2016). One difference between word discrimination and search/recognition tasks is that in word discrimination the word boundaries are given. However, prior work has been able to apply results from word discrimination Levin et al. (2013) to improve a query-by-example system without known word boundaries Levin et al. (2015), by simply applying their embeddings to non-word segments as well. The only prior work focused on vector embeddings of character sequences explicitly aimed at representing their acoustic similarity is that of Ghannay et al. (2016), who proposed evaluations based on nearest-neighbor retrieval, phonetic/orthographic similarity measures, and homophone disambiguation.", "startOffset": 32, "endOffset": 601}, {"referenceID": 11, "context": "For example, the standard task of spoken term detection (Fiscus et al., 2007) involves searching for examples of a given text query in spoken documents.", "startOffset": 56, "endOffset": 77}, {"referenceID": 10, "context": "This is analogous to the evaluation of semantic word embeddings via the rank correlation between embedding distances and human similarity judgments (Finkelstein et al., 2001; Hill et al., 2015).", "startOffset": 148, "endOffset": 193}, {"referenceID": 21, "context": "This is analogous to the evaluation of semantic word embeddings via the rank correlation between embedding distances and human similarity judgments (Finkelstein et al., 2001; Hill et al., 2015).", "startOffset": 148, "endOffset": 193}, {"referenceID": 8, "context": "(2015, 2016); Chung et al. (2016); Settle & Livescu (2016) and is a proxy for query-by-example search.", "startOffset": 14, "endOffset": 34}, {"referenceID": 8, "context": "(2015, 2016); Chung et al. (2016); Settle & Livescu (2016) and is a proxy for query-by-example search.", "startOffset": 14, "endOffset": 59}, {"referenceID": 8, "context": "(2015, 2016); Chung et al. (2016); Settle & Livescu (2016) and is a proxy for query-by-example search. For each given spoken word pair, we calculate the cosine distance between their embeddings. If the cosine distance is below a threshold, we output \u201cyes\u201d (same word), otherwise we output \u201cno\u201d (different words). The performance measure is the average precision (AP), which is the area under the precision-recall curve generated by varying the threshold and has a maximum value of 1. In our multi-view setup, we embed not only the acoustic words but also the character sequences. This allows us to use our embeddings also for tasks involving comparisons between written and spoken words. For example, the standard task of spoken term detection (Fiscus et al., 2007) involves searching for examples of a given text query in spoken documents. This task is identical to queryby-example except that the query is given as text. In order to explore the potential of multi-view embeddings for such tasks, we design another proxy task, cross-view word discrimination. Here we are given a pair of inputs, one a written word and one an acoustic word segment, and our task is to determine if the acoustic signal is an example of the written word. The evalution proceeds analogously to the acoustic word discrimination task: We output \u201cyes\u201d if the cosine distance between the embeddings of the written and spoken sequences are below some threshold, and measure performance as the average precision (AP) over all thresholds. Finally, we also would like to obtain a more fine-grained measure of whether the learned embeddings capture our intuitive sense of similarity between words. Being able to capture word similarity may also be useful in building query or recognition systems that fail gracefully and produce humanlike errors. For this purpose we measure the rank correlation between embedding distances and character edit distances. This is analogous to the evaluation of semantic word embeddings via the rank correlation between embedding distances and human similarity judgments (Finkelstein et al., 2001; Hill et al., 2015). In our case, however, we do not use human judgments since the ground-truth edit distances themselves provide a good measure. We refer to this as the word similarity task, and we apply this measure to both pairs of acoustic embeddings and pairs of character sequence embeddings. Similar measures have been proposed by Ghannay et al. (2016) to evaluate acoustic word embeddings, although they considered only near neighbors of each word whereas we consider the correlation across the full range of word pairs.", "startOffset": 14, "endOffset": 2459}, {"referenceID": 5, "context": "The task and setup were first developed by (Carlin et al., 2011).", "startOffset": 43, "endOffset": 64}, {"referenceID": 14, "context": "The data is drawn from the Switchboard English conversational speech corpus (Godfrey et al., 1992).", "startOffset": 76, "endOffset": 98}, {"referenceID": 23, "context": "1 DATA We use the same experimental setup and data as in Kamper et al. (2015, 2016); Settle & Livescu (2016). The task and setup were first developed by (Carlin et al.", "startOffset": 57, "endOffset": 109}, {"referenceID": 42, "context": "We note that a similar objective to our obj + obj was used by Vendrov et al. (2016) for the task of caption-image retrieval, where the authors essentially use all non-paired", "startOffset": 62, "endOffset": 84}, {"referenceID": 26, "context": "Method Test AP Test AP (acoustic) (cross-view) MFCCs + DTW (Kamper et al., 2016) 0.", "startOffset": 59, "endOffset": 80}, {"referenceID": 25, "context": "214 Correspondence autoencoder + DTW (Kamper et al., 2015) 0.", "startOffset": 37, "endOffset": 58}, {"referenceID": 5, "context": "469 Phone posteriors + DTW (Carlin et al., 2011) 0.", "startOffset": 27, "endOffset": 48}, {"referenceID": 26, "context": "497 Siamese CNN (Kamper et al., 2016) 0.", "startOffset": 16, "endOffset": 37}, {"referenceID": 25, "context": "For comparison, we also include acoustic word discrimination results reported previously by Kamper et al. (2016); Settle & Livescu (2016).", "startOffset": 92, "endOffset": 113}, {"referenceID": 25, "context": "For comparison, we also include acoustic word discrimination results reported previously by Kamper et al. (2016); Settle & Livescu (2016). Previous approaches have not addressed the problem of learning embeddings jointly with the text view, so they can not be evaluated on the cross-view task.", "startOffset": 92, "endOffset": 138}], "year": 2017, "abstractText": "Recent work has begun exploring neural acoustic word embeddings\u2014fixeddimensional vector representations of arbitrary-length speech segments corresponding to words. Such embeddings are applicable to speech retrieval and recognition tasks, where reasoning about whole words may make it possible to avoid ambiguous sub-word representations. The main idea is to map acoustic sequences to fixed-dimensional vectors such that examples of the same word are mapped to similar vectors, while different-word examples are mapped to very different vectors. In this work we take a multi-view approach to learning acoustic word embeddings, in which we jointly learn to embed acoustic sequences and their corresponding character sequences. We use deep bidirectional LSTM embedding models and multi-view contrastive losses. We study the effect of different loss variants, including fixed-margin and cost-sensitive losses. Our acoustic word embeddings improve over previous approaches for the task of word discrimination. We also present results on other tasks that are enabled by the multi-view approach, including cross-view word discrimination and word similarity.", "creator": "LaTeX with hyperref package"}}}