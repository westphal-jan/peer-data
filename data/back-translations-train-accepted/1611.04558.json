{"id": "1611.04558", "review": {"conference": "acl", "VERSION": "v1", "DATE_OF_SUBMISSION": "14-Nov-2016", "title": "Google's Multilingual Neural Machine Translation System: Enabling Zero-Shot Translation", "abstract": "We propose a simple, elegant solution to use a single Neural Machine Translation (NMT) model to translate between multiple languages. Our solution requires no change in the model architecture from our base system but instead introduces an artificial token at the beginning of the input sentence to specify the required target language. The rest of the model, which includes encoder, decoder and attention, remains unchanged and is shared across all languages. Using a shared wordpiece vocabulary, our approach enables Multilingual NMT using a single model without any increase in parameters, which is significantly simpler than previous proposals for Multilingual NMT. Our method often improves the translation quality of all involved language pairs, even while keeping the total number of model parameters constant. On the WMT'14 benchmarks, a single multilingual model achieves comparable performance for English$\\rightarrow$French and surpasses state-of-the-art results for English$\\rightarrow$German. Similarly, a single multilingual model surpasses state-of-the-art results for French$\\rightarrow$English and German$\\rightarrow$English on WMT'14 and WMT'15 benchmarks respectively. On production corpora, multilingual models of up to twelve language pairs allow for better translation of many individual pairs. In addition to improving the translation quality of language pairs that the model was trained with, our models can also learn to perform implicit bridging between language pairs never seen explicitly during training, showing that transfer learning and zero-shot translation is possible for neural translation. Finally, we show analyses that hints at a universal interlingua representation in our models and show some interesting examples when mixing languages.", "histories": [["v1", "Mon, 14 Nov 2016 20:24:39 GMT  (2497kb,D)", "http://arxiv.org/abs/1611.04558v1", null], ["v2", "Mon, 21 Aug 2017 20:33:43 GMT  (2500kb,D)", "http://arxiv.org/abs/1611.04558v2", null]], "reviews": [], "SUBJECTS": "cs.CL cs.AI", "authors": ["melvin johnson", "mike schuster", "quoc v le", "maxim krikun", "yonghui wu", "zhifeng chen", "nikhil thorat", "fernanda vi\\'egas", "martin wattenberg", "greg corrado", "macduff hughes", "jeffrey dean"], "accepted": true, "id": "1611.04558"}, "pdf": {"name": "1611.04558.pdf", "metadata": {"source": "CRF", "title": "Google\u2019s Multilingual Neural Machine Translation System: Enabling Zero-Shot Translation", "authors": ["Melvin Johnson", "Mike Schuster", "Quoc V. Le", "Maxim Krikun", "Yonghui Wu", "Zhifeng Chen", "Nikhil Thorat", "Fernanda Vi\u00e9gas", "Martin Wattenberg", "Greg Corrado", "Macduff Hughes", "Jeffrey Dean"], "emails": ["melvinp@google.com", "schuster@google.com", "qvl@google.com", "krikun@google.com", "yonghui@google.com", "zhifengc@google.com", "nsthorat@google.com"], "sections": [{"heading": null, "text": "Our solution does not require a change in the model architecture of our base system, but instead introduces an artificial token at the beginning of the input sentence to specify the required target language. The rest of the model, which includes encoder, decoder and attention, remains unchanged and is shared across all languages. By sharing a common vocabulary, our approach enables multilingual NMT using a single model without parameter increase, which is significantly simpler than previous proposals for multilingual NMT. Our method often improves the translation quality of all language pairs involved, even if the total number of model parameters remains constant. On the 14 benchmarks of WMT, a single multilingual model achieves comparable performance in English \u2192 French and exceeds the state of the art in English \u2192 German. Similarly, a single multilingual model shows results that exceed the state of the art in translation."}, {"heading": "1 Introduction", "text": "We call our system multilingual GNMT because it is an extension of [22, 2, 5]. It is an end-to-end approach to machine translation that has quickly established itself in many large-format environments. Almost all such systems are built for a single language pair - so far, there has been no sufficiently simple and efficient way to handle multiple language pairs using a single model without making significant changes to the basic NMT architecture. In this paper, we present a simple method to translate between multiple languages using a single model, taking advantage of multilingual data to improve NMT for all languages involved. Our method does not require a change in the traditional NMT model architecture. Instead, we add an artificial token to the input sequence to indicate the required target language as described in a single model. - Encoders, decoders, attention, and common piece of vocabulary - remain exactly the same GT as our system is called M24."}, {"heading": "2 Related Work", "text": "Although most practical applications of machine translation are focused on single language pairs because it was simply too difficult to build a single system that reliably translates from and to multiple languages.Although most practical approaches to machine translation [22, 2, 5] represent a promising end-to-end learning approach to machine translation that has rapidly expanded to multilingual machine translation in different ways, the work in [12] of the proposed multilingual education is in a multilingual learning environment. Their model is a basic encoder decoder network for multilingual NMT, in this case without an attention mechanism for multilingual data. To enable the proper use of multilingual data, they expand their model with multiple encoders and decoders, one for each supported source and target language. Similarly in [6], the authors modify an attention-based decoder encoder approach, or NT."}, {"heading": "3 System Architecture for Multilingual Translation", "text": "The multilingual model architecture (see Figure 1) is identical to Google's Neural Machine Translation (GNMT) system [24] (with the optional addition of direct links between encoder and decoder layers, which we used for some of our experiments, see Description of Figure 1), and we refer to this paper for a detailed description. To be able to use multilingual data within a single system, we suggest a simple modification of the input data, which is to insert an artificial token at the beginning of the input sentence to specify the target language into which the model is to be translated. Consider, for example, the following English \u2192 Spanish sentence pair: Hello, how are you? - > Hola como est\u00e1s? It is modified to: < 2es > Hello, how are you? - > Hola como est\u00e1s? to indicate that Spanish is the target language."}, {"heading": "4 Experiments and Results", "text": "In this section, we will apply our suggested method to train multilingual models in multiple different configurations. As we may have models with single or multiple source / target languages, we will test three interesting cases: \u2022 many source languages into one target language (many to one); \u2022 one source language into many target languages (one to many); and \u2022 many source languages into many target languages (many too many); we will also show results and discuss the benefits of merging many (un) related languages into a single model trained on production data; and finally, we will present our results on zero-shot translation, where the model learns to translate between language pairs for which there were no explicit parallel examples in the training data, and show results of experiments where adding additional data further improves zero-shot translation quality."}, {"heading": "4.1 Datasets, Training Protocols and Evaluation Metrics", "text": "We train our models on the WMT '14 English (En) \u2192 French (Fr) dataset, the WMT' 14 English \u2192 German (En) dataset, as well as on some of Google's internal production datasets. On the WMT En \u2192 Fr, the training set contains 36M set pairs. The WMT En \u2192 De training set contains 5M set pairs. In both cases, we use newstest2014 as test sets to compare against previous work [14, 19, 25, 24]. For WMT Fr \u2192 En, we use newstest2014 and newstest2015 as test sets. Despite training on the WMT '14 data, which is slightly smaller than WMT' 15, we test our De-En model on newstest2015, similar to [13] the combination of newstest2012 and newstest2013 is used as development setup."}, {"heading": "4.2 Many to One", "text": "In this section, we will examine multiple source languages and a single target language - the easiest way to combine language pairs. As there is only one target language, no additional source code is required. We will conduct three sets of experiments: \u2022 The first set of experiments is based on the WMT data sets, in which we combine German \u2192 English to train a multilingual model. \u2022 The second set of experiments is based on production data, in which we combine Japanese \u2192 English and English, which are trained independently. We will conduct these experiments once with oversample samples, so that the amount of data per language pair is the same, and once without. \u2022 The second set of experiments is based on production data, in which we combine Japanese \u2192 English and English, with oversample samples. The baselines are two language pair models: Japanese \u2192 English and Korean \u2192 English trained independently of each other. \u2022 The third set of experiments is based on production data, in which we combine Spanish and Portuguese \u2192 English, with oversample samples."}, {"heading": "4.3 One to Many", "text": "In this section, we will examine the application of our method when there is a single source language and multiple target languages. At this point, we must precede the input with an additional symbol to specify the target language. We are conducting three sets of experiments that are almost identical to the previous section, except that the source and target languages have been reversed. Table 2 summarizes the results when we perform translations into multiple target languages. We see that the multilingual models are comparable to baselines and in some cases exceed baselines, but not always. We achieve a large gain of + 0.9 BLEU for English \u2192 Spanish. Unlike the previous results, there are less significant gains in this series of experiments. This may be because the decoder has found it more difficult to translate into multiple target languages that even have different scripts, e.g. Japanese and Korean, which are combined into a single common phrase vocabulary."}, {"heading": "4.4 Many to Many", "text": "In this section, we report on experiments where multiple source languages and multiple target languages are present within a single model - the most difficult constellation. As multiple target languages are specified, the input of the target language tag must be given as above. Results will be shown in Table 3. We see that the multilingual production models with the same model size and vocabulary size as the individual language models are fairly close to baselines. In some cases, they even exceed the base model, while in others they are worse. We believe this effect is due to the fact that we do not go through all the available training data and each language pair sees only a quarter of the data seen by the baselines. In the WMT data sets, we re-examine the effects of oversampling of the smaller language pairs. We notice a similar trend as in the previous section, in which oversampling helps the smaller language pairs at the expense of the larger ones, while overclocking does not have the reverse effect. Although there are some significant quality losses in the total number of languages discussed below these common models in relation to the models."}, {"heading": "4.5 Large Scale Experiments", "text": "This section shows the result of combining 12 production language pairs into a single multilingual model using the same number of parameters as the models of individual language pairs. As described above, the input must be preceded by the target language symbol. We will overestimate the examples from the smaller language pairs to compensate for the data explained above. Results are summarized in Table 4. We find that the multilingual model is relatively close to the best individual models and in some cases even achieves comparable quality. It is noteworthy that a single model with 278M parameters can do what 12 models with a total of 3.33B parameters would have done. The multilingual model also requires one-twelfth of the training time and computing resources for convergence. Another important point is that the individual language pairs, since we train only a little longer than the individual models, can see only one-twelfth of the data compared to their individual language pair models. Again, we find that the comparison for the multilingual model is slightly unfair and we expect the benefits to be better compared to the comparable or greater number of comparable multilingual models."}, {"heading": "4.6 Zero-Shot Translation", "text": "An interesting advantage of our approach is that we can perform zero-shot translation between a language pair for which no explicit training data has been seen. To demonstrate this, we will use two multilingual models - a model that has been trained with examples from two different language pairs, Portuguese \u2192 English \u2192 Spanish (Model 1), and a model that has been trained with examples from four different language pairs, English \u2194 Portuguese and English \u2194 Spanish (Model 2). We show that both models can produce relatively good Portuguese translations without ever having seen Portuguese \u2192 Spanish data during the training. To our knowledge, this is the first demonstration of true multilingual zero-shot translation. As with the previous multilingual models, both models perform comparable or even slightly better translations than the basic models of individual language pairs."}, {"heading": "4.7 Effect of Direct Parallel Data", "text": "In this section, we will examine two ways to use available parallel data to improve the quality of zero-shot translation, namely: \u2022 Incremental training of the multilingual model using additional parallel data for zero-shot directions. \u2022 Training of a new multilingual model in which all available parallel data is equally mixed. However, for our experiments, we use a basic model we call \"zero-shot,\" which is trained on a combined parallel corpus of English \u2194 {Belarusian, Russian, Ukrainian}. We trained a second model on the above-mentioned corpus together with additional Russian \u2194 {Belarusian, Ukrainian} data. We call this model \"From-Scratch.\" Both models support four target languages and are evaluated on our standard test sets. As before, we will trick the data so that all language pairs are represented equally. Finally, we will take the best checkpoint of the \"zero-shot\" model, which represents a \"zero-shot training period\" for a small percentage of the English model."}, {"heading": "5 Visual Analysis", "text": "The results of this work - that training a model across multiple languages can improve performance at the individual language level, and that zero-shot translation can be effective - raise a number of questions about how these tasks are handled within the model, for example: \u2022 Does the network learn a kind of common representation in which sentences with the same meaning are represented in a similar way regardless of the language? \u2022 Does the model work in zero-shot translations in the same way as it treats language pairs on which it was trained? One way to study the representations used by the network is to look at the network activations during translation. A starting point for the study is the amount of attention vectors, i.e. the activations within the layer connecting the encoder and decoder network (see the blue field in the middle of Figure 1)."}, {"heading": "5.1 Evidence for an Interlingua", "text": "For example, the following Figure 2 was created by a multiple-to-many model, which is trained in English \u2194 Japanese and English \u2194 Korean. To visualize the model in action, we started with a small corpus of 74 triples of semantically identical linguistic phrases. That is, each triplicate contained phrases in English, Japanese and Korean with the same underlying meaning. To compile these triplicates, we searched a basic truth database for English sentences paired with both Japanese and Korean translations. Then, we applied the trained model to translate each sentence of each triplicate into the other two possible languages. In performing this process, six new sentences were generated based on each triplicate, for a total of 74 8,000 6 = 444 total translations with 9,978 steps corresponding to the same number of attention vectors. Since attention vectors are represented in the same 3D projector throughout the embodiment process, we used a single point to represent each of the SN2, which is the TORLOW."}, {"heading": "5.2 Partially Separated Representations", "text": "Not all models show such neat semantic clustering. Sometimes, we observed common embedding in some regions of the space that coexist with separate large clusters that contain many attention vectors from just one language pair. For example, Figure 3a shows a t-SNE projection of attention vectors from a model trained in Portuguese \u2192 English (blue) and English (yellow) and a zero-shot translation from Portuguese \u2192 Spanish (red). This projection shows 153 semantically identical triples translated as described above, resulting in 459 total translations. The large red region on the left primarily contains zero-shot Portuguese \u2192 Spanish translations. In other words, for a significant number of sentences, the zero-shot translation has a different embedding than the two trained translation guidelines. On the other hand, some zero-shot translation vectors appear to fall in proximity to inserts in the region, such as in other major languages."}, {"heading": "6 Mixing Languages", "text": "Having a mechanism for translating from a random source language into a single selected target language using additional source code made us think about what happens when languages are mixed on the source or target page. Specifically, we were interested in the following two experiments: 1. Can a multilingual model successfully handle multilingual input (code switching) when this happens in the middle of the sentence? 2. What happens when a multilingual model is triggered not with a single, but with two target-language tokens weighted to add up to one (which corresponds to merging the weighted embeddings of these tokens)? In the following two sections, these experiments will be discussed."}, {"heading": "6.1 Source Language Code-Switching", "text": "In this section, we show how multilingual models deal with switching the source language. Here, we show an example of a multilingual model that has been trained with Japanese, Korean and English data. In many cases, this model produces correct English translations, which show that switching the code can be handled with this model, even though there were no examples of such switching of the code in the training data. Note that the model can effectively handle the various typographic scripts, since the individual characters / phrases are present in our vocabulary. \u2022 Japanese: \"I am a student at the University of Tokyo.\" \u2022 Korean: \"I am a student at the University of Tokyo.\" \u2022 I am a student at the University of Tokyo \"I am a student at the University of Tokyo.\" \u2022 Mixed Japanese / Korean: \"I am a student at the University of Tokyo.\""}, {"heading": "6.2 Weighted Target Language Selection", "text": "In this section, we test what happens when we mix target languages. We take a multilingual model that has been trained with multiple target languages, e.g. English \u2192 {Japanese, Korean}. Instead of feeding the embedding vector for \"< 2ja >\" into the lower level of the LSTM encoder, we feed a linear combination (1 \u2212 w) < 2ja > + w < 2ko >. Sure, for w = 0, the model should produce Japanese, for w = 1 it should produce Korean, but what happens in between? You might expect the model to output some kind of intermediate language (\"Japanese\"), but the results turn out to be less surprising. Most of the time, the output simply switches from one language to another by w = 0.5. In some cases, intermediate values of the model become the languages that are shifted in the middle of the sentence. One possible explanation for this behavior is that the target language model is learned implicitly from the LSTM encoder."}, {"heading": "7 Conclusion", "text": "We present a simple and elegant solution for the multilingual NMT model. We show that we can train multilingual NMT models that can be used to translate between a number of different languages using a single model, sharing all the parameters, which, as a positive side effect, also improves the translation quality of languages with low resources in the mix. We also show that zero-shot translation is possible without explicit bridging with these models, and this is the first time that we know that a form of real machine translation learning works. To explicitly improve zero-shot translation quality, we examine two different ways to add available parallel data and find that small additional amounts are sufficient to achieve satisfactory results. In our largest experiment, we merge 12 language pairs into a single model of the same size as each individual language pair model, achieving only slightly lower translation quality than the baseline capacities of individual language pairs, although the modelling pairing capacity has been reduced."}, {"heading": "Acknowledgements", "text": "We would like to thank the entire Google Brain team and the Google Translate team for their fundamental contributions to this project. In particular, we would like to thank Junyoung Chung for his insights into this topic and Alex Rudnick and Otavio Good for helpful suggestions."}], "references": [{"title": "Tensorflow: A system for large-scale machine learning", "author": ["M. Abadi", "P. Barham", "J. Chen", "Z. Chen", "A. Davis", "J. Dean", "M. Devin", "S. Ghemawat", "G. Irving", "M. Isard", "M. Kudlur", "J. Levenberg", "R. Monga", "S. Moore", "D.G. Murray", "B. Steiner", "P. Tucker", "V. Vasudevan", "P. Warden", "M. Wicke", "Y. Yu", "X. Zheng"], "venue": "Tech. rep., Google Brain,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2016}, {"title": "Neural machine translation by jointly learning to align and translate", "author": ["D. Bahdanau", "K. Cho", "Y. Bengio"], "venue": "In International Conference on Learning Representations", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2015}, {"title": "Does multimodality help human and machine for translation and image captioning", "author": ["O. Caglayan", "W. Aransa", "Y. Wang", "M. Masana", "M. Garc\u00eda-Mart\u00ednez", "F. Bougares", "L. Barrault", "J. van de Weijer"], "venue": "In Proceedings of the First Conference on Machine Translation (Berlin,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2016}, {"title": "Multitask learning", "author": ["R. Caruana"], "venue": "In Learning to learn. Springer,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 1998}, {"title": "Learning phrase representations using RNN encoder-decoder for statistical machine translation", "author": ["K. Cho", "B. van Merrienboer", "\u00c7. G\u00fcl\u00e7ehre", "D. Bahdanau", "F. Bougares", "H. Schwenk", "Y. Bengio"], "venue": null, "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2014}, {"title": "Multi-task learning for multiple language translation", "author": ["D. Dong", "H. Wu", "W. He", "D. Yu", "H. Wang"], "venue": "In Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing of the Asian Federation of Natural Language Processing,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2015}, {"title": "Multi-way, multilingual neural machine translation with a shared attention mechanism", "author": ["O. Firat", "K. Cho", "Y. Bengio"], "venue": "In NAACL HLT", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2016}, {"title": "Zero-resource translation with multi-lingual neural machine translation", "author": ["O. Firat", "B. Sankaran", "Y. Al-Onaizan", "F.T.Y. Vural", "K. Cho"], "venue": null, "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2016}, {"title": "Multilingual language processing from bytes", "author": ["D. Gillick", "C. Brunk", "O. Vinyals", "A. Subramanya"], "venue": "CoRR abs/1512.00103", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2015}, {"title": "Fully character-level neural machine translation without explicit segmentation", "author": ["L. Jason", "K. Cho", "T. Hofmann"], "venue": "arXiv preprint arXiv:1610.03017", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2016}, {"title": "Multi-task sequence to sequence learning", "author": ["Luong", "M.-T", "Q.V. Le", "I. Sutskever", "O. Vinyals", "L. Kaiser"], "venue": "In International Conference on Learning Representations", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2015}, {"title": "Effective approaches to attention-based neural machine translation", "author": ["Luong", "M.-T", "H. Pham", "C.D. Manning"], "venue": "In Conference on Empirical Methods in Natural Language Processing", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2015}, {"title": "Addressing the rare word problem in neural machine translation", "author": ["Luong", "M.-T", "I. Sutskever", "Q.V. Le", "O. Vinyals", "W. Zaremba"], "venue": "In Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2015}, {"title": "Visualizing Data using t-SNE", "author": ["L.V.D. Maaten", "G. Hinton"], "venue": "Journal of Machine Learning Research", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2008}, {"title": "Interlingual machine translation", "author": ["R.H. Richens"], "venue": "The Computer Journal 1,", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 1958}, {"title": "Multilingual speech processing", "author": ["T. Schultz", "K. Kirchhoff"], "venue": null, "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2006}, {"title": "Japanese and Korean voice", "author": ["M. Schuster", "K. Nakajima"], "venue": "IEEE International Conference on Acoustics, Speech and Signal Processing", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2012}, {"title": "On using very large target vocabulary for neural machine translation", "author": ["J. S\u00e9bastien", "C. Kyunghyun", "R. Memisevic", "Y. Bengio"], "venue": "In Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2015}, {"title": "Controlling politeness in neural machine translation via side constraints. In NAACL HLT 2016, The 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies", "author": ["R. Sennrich", "B. Haddow", "A. Birch"], "venue": "San Diego California,", "citeRegEx": "20", "shortCiteRegEx": "20", "year": 2016}, {"title": "Neural machine translation of rare words with subword units", "author": ["R. Sennrich", "B. Haddow", "A. Birch"], "venue": "In Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics", "citeRegEx": "21", "shortCiteRegEx": "21", "year": 2016}, {"title": "Sequence to sequence learning with neural networks", "author": ["I. Sutskever", "O. Vinyals", "Q.V. Le"], "venue": "In Advances in Neural Information Processing Systems", "citeRegEx": "22", "shortCiteRegEx": "22", "year": 2014}, {"title": "Polyglot neural language models: A case study in cross-lingual phonetic representation learning", "author": ["Y. Tsvetkov", "S. Sitaram", "M. Faruqui", "G. Lample", "P. Littell", "D.R. Mortensen", "A.W. Black", "L.S. Levin", "C. Dyer"], "venue": "CoRR abs/1605.03832", "citeRegEx": "23", "shortCiteRegEx": "23", "year": 2016}, {"title": "Deep recurrent models with fast-forward connections for neural machine translation", "author": ["J. Zhou", "Y. Cao", "X. Wang", "P. Li", "W. Xu"], "venue": null, "citeRegEx": "25", "shortCiteRegEx": "25", "year": 2016}, {"title": "Multi-source neural translation. In NAACL HLT 2016, The 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies", "author": ["B. Zoph", "K. Knight"], "venue": "San Diego California,", "citeRegEx": "26", "shortCiteRegEx": "26", "year": 2016}], "referenceMentions": [{"referenceID": 20, "context": "Neural Machine Translation (NMT) [22, 2, 5] is an end-to-end approach to machine translation that has rapidly gained adoption in many large-scale settings [24].", "startOffset": 33, "endOffset": 43}, {"referenceID": 1, "context": "Neural Machine Translation (NMT) [22, 2, 5] is an end-to-end approach to machine translation that has rapidly gained adoption in many large-scale settings [24].", "startOffset": 33, "endOffset": 43}, {"referenceID": 4, "context": "Neural Machine Translation (NMT) [22, 2, 5] is an end-to-end approach to machine translation that has rapidly gained adoption in many large-scale settings [24].", "startOffset": 33, "endOffset": 43}, {"referenceID": 14, "context": "Interlingual translation is a classic method in machine translation [16, 10].", "startOffset": 68, "endOffset": 76}, {"referenceID": 20, "context": "Neural Machine Translation [22, 2, 5] is a promising end-to-end learning approach to machine translation which quickly was extended to multilingual machine translation in various ways.", "startOffset": 27, "endOffset": 37}, {"referenceID": 1, "context": "Neural Machine Translation [22, 2, 5] is a promising end-to-end learning approach to machine translation which quickly was extended to multilingual machine translation in various ways.", "startOffset": 27, "endOffset": 37}, {"referenceID": 4, "context": "Neural Machine Translation [22, 2, 5] is a promising end-to-end learning approach to machine translation which quickly was extended to multilingual machine translation in various ways.", "startOffset": 27, "endOffset": 37}, {"referenceID": 10, "context": "One early attempt is the work in [12] which proposed multilingual training in a multitask learning setting.", "startOffset": 33, "endOffset": 37}, {"referenceID": 5, "context": "Similarly in [6], the authors modify an attention-based encoder-decoder approach to perform multilingual NMT by adding a separate decoder and attention mechanism for each target language.", "startOffset": 13, "endOffset": 16}, {"referenceID": 2, "context": "In [3] the authors incorporate multiple modalities other than text into the encoder-decoder framework.", "startOffset": 3, "endOffset": 6}, {"referenceID": 23, "context": "For instance, in [26] a form of multi-source translation was proposed where the model has multiple different encoders and different attention mechanisms for each source language.", "startOffset": 17, "endOffset": 21}, {"referenceID": 6, "context": "Most closely related to our approach is [7] in which the authors propose multi-way multilingual NMT using a single shared attention mechanism but multiple encoders/decoders for each source/target language.", "startOffset": 40, "endOffset": 43}, {"referenceID": 9, "context": "Recently in [11] a CNN-based character-level encoder was proposed which is shared across multiple source languages.", "startOffset": 12, "endOffset": 16}, {"referenceID": 3, "context": "Our approach is related to the multitask learning framework [4].", "startOffset": 60, "endOffset": 63}, {"referenceID": 15, "context": "In speech recognition, there have been many successful reports of modeling multiple languages using a single model (see [17] for an extensive reference and references therein).", "startOffset": 120, "endOffset": 124}, {"referenceID": 8, "context": "Multilingual language processing has also shown to be successful in domains other than translation [9, 23].", "startOffset": 99, "endOffset": 106}, {"referenceID": 21, "context": "Multilingual language processing has also shown to be successful in domains other than translation [9, 23].", "startOffset": 99, "endOffset": 106}, {"referenceID": 18, "context": "In an approach similar to ours in spirit, but with a very different purpose, the NMT framework has been extended to control the politeness level of the target translation by adding a special token to the source sentence in [20].", "startOffset": 223, "endOffset": 227}, {"referenceID": 7, "context": "Zero-shot translation was the direct goal of [8].", "startOffset": 45, "endOffset": 48}, {"referenceID": 16, "context": "We use a shared wordpiece model [18, 21] across all the source and target data used for training, usually with 32,000 word pieces.", "startOffset": 32, "endOffset": 40}, {"referenceID": 19, "context": "We use a shared wordpiece model [18, 21] across all the source and target data used for training, usually with 32,000 word pieces.", "startOffset": 32, "endOffset": 40}, {"referenceID": 0, "context": "All training is carried out as in our general NMT pipeline as described in [24] and implemented in TensorFlow [1].", "startOffset": 110, "endOffset": 113}, {"referenceID": 12, "context": "In both cases, we use newstest2014 as the test sets to compare against previous work [14, 19, 25, 24].", "startOffset": 85, "endOffset": 101}, {"referenceID": 17, "context": "In both cases, we use newstest2014 as the test sets to compare against previous work [14, 19, 25, 24].", "startOffset": 85, "endOffset": 101}, {"referenceID": 22, "context": "In both cases, we use newstest2014 as the test sets to compare against previous work [14, 19, 25, 24].", "startOffset": 85, "endOffset": 101}, {"referenceID": 11, "context": "Despite training on WMT\u201914 data, which is somewhat smaller than WMT\u201915, we test our De\u2192En model on newstest2015, similar to [13].", "startOffset": 124, "endOffset": 128}, {"referenceID": 20, "context": "We evaluate our models using the standard BLEU score metric and to make our results comparable to [24, 22, 14, 25], we report tokenized BLEU score as computed by the multi-bleu.", "startOffset": 98, "endOffset": 114}, {"referenceID": 12, "context": "We evaluate our models using the standard BLEU score metric and to make our results comparable to [24, 22, 14, 25], we report tokenized BLEU score as computed by the multi-bleu.", "startOffset": 98, "endOffset": 114}, {"referenceID": 22, "context": "We evaluate our models using the standard BLEU score metric and to make our results comparable to [24, 22, 14, 25], we report tokenized BLEU score as computed by the multi-bleu.", "startOffset": 98, "endOffset": 114}, {"referenceID": 13, "context": "Since attention vectors are high-dimensional, we use the TensorFlow Embedding Projector2 to map them into more accessible 3D space via t-SNE [15].", "startOffset": 141, "endOffset": 145}, {"referenceID": 0, "context": "has been embedded at yi \u2208 R1024, we defined a curve \u03b3 : [0, 1]\u2192 R1024 at \u201ccontrol points\u201d of the form i n\u22121 by:", "startOffset": 56, "endOffset": 62}], "year": 2016, "abstractText": "We propose a simple, elegant solution to use a single Neural Machine Translation (NMT) model to translate between multiple languages. Our solution requires no change in the model architecture from our base system but instead introduces an artificial token at the beginning of the input sentence to specify the required target language. The rest of the model, which includes encoder, decoder and attention, remains unchanged and is shared across all languages. Using a shared wordpiece vocabulary, our approach enables Multilingual NMT using a single model without any increase in parameters, which is significantly simpler than previous proposals for Multilingual NMT. Our method often improves the translation quality of all involved language pairs, even while keeping the total number of model parameters constant. On the WMT\u201914 benchmarks, a single multilingual model achieves comparable performance for English\u2192French and surpasses state-of-the-art results for English\u2192German. Similarly, a single multilingual model surpasses state-of-the-art results for French\u2192English and German\u2192English on WMT\u201914 and WMT\u201915 benchmarks respectively. On production corpora, multilingual models of up to twelve language pairs allow for better translation of many individual pairs. In addition to improving the translation quality of language pairs that the model was trained with, our models can also learn to perform implicit bridging between language pairs never seen explicitly during training, showing that transfer learning and zero-shot translation is possible for neural translation. Finally, we show analyses that hints at a universal interlingua representation in our models and show some interesting examples when mixing languages.", "creator": "LaTeX with hyperref package"}}}