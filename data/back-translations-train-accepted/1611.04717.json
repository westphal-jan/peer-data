{"id": "1611.04717", "review": {"conference": "nips", "VERSION": "v1", "DATE_OF_SUBMISSION": "15-Nov-2016", "title": "Exploration: A Study of Count-Based Exploration for Deep Reinforcement Learning", "abstract": "Count-based exploration algorithms are known to perform near-optimally when used in conjunction with tabular reinforcement learning (RL) methods for solving small discrete Markov decision processes (MDPs). It is generally thought that count-based methods cannot be applied in high-dimensional state spaces, since most states will only occur once. Recent deep RL exploration strategies are able to deal with high-dimensional continuous state spaces through complex heuristics, often relying on optimism in the face of uncertainty or intrinsic motivation. In this work, we describe a surprising finding: a simple generalization of the classic count-based approach can reach near state-of-the-art performance on various high-dimensional and/or continuous deep RL benchmarks. States are mapped to hash codes, which allows to count their occurrences with a hash table. These counts are then used to compute a reward bonus according to the classic count-based exploration theory. We find that simple hash functions can achieve surprisingly good results on many challenging tasks. Furthermore, we show that a domain-dependent learned hash code may further improve these results. Detailed analysis reveals important aspects of a good hash function: 1) having appropriate granularity and 2) encoding information relevant to solving the MDP. This exploration strategy achieves near state-of-the-art performance on both continuous control tasks and Atari 2600 games, hence providing a simple yet powerful baseline for solving MDPs that require considerable exploration.", "histories": [["v1", "Tue, 15 Nov 2016 06:42:24 GMT  (2076kb,D)", "https://arxiv.org/abs/1611.04717v1", "16 pages. Under review as a conference paper at ICLR 2017"], ["v2", "Wed, 11 Jan 2017 18:29:16 GMT  (2154kb,D)", "http://arxiv.org/abs/1611.04717v2", "18 pages. Under review as a conference paper at ICLR 2017"]], "COMMENTS": "16 pages. Under review as a conference paper at ICLR 2017", "reviews": [], "SUBJECTS": "cs.AI cs.LG", "authors": ["haoran tang", "rein houthooft", "davis foote", "adam stooke", "xi chen", "yan duan", "john schulman", "filip de turck", "pieter abbeel"], "accepted": true, "id": "1611.04717"}, "pdf": {"name": "1611.04717.pdf", "metadata": {"source": "META", "title": "#Exploration:A Study of Count-Based Explorationfor Deep Reinforcement Learning", "authors": ["Haoran Tang", "Rein Houthooft", "Davis Foote", "Adam Stooke", "Xi Chen", "Yan Duan", "John Schulman", "Filip De Turck", "Pieter Abbeel"], "emails": [], "sections": [{"heading": "1 Introduction", "text": "It is impossible for the agent to act almost optimally until he has sufficiently explored the environment and identified all possibilities for high reward in all scenarios. However, a core challenge in RL is how to balance exploration - actively looking for novel states and measures that could yield high rewards and long-term gains; and exploitation - maximizing short-term rewards using the agent's current knowledge. While there are exploration techniques for finite MDPs that enjoy theoretical guarantees, there are no fully satisfactory techniques for high-dimensional government spaces; therefore, the development of more general and robust exploration techniques is an active area of research.Most of the recent state-of-the-art RL results have been achieved with simple exploration strategies such as uniform sampling (Mnih et al., 2015) and i.d."}, {"heading": "2 Methodology", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "2.1 Notation", "text": "This paper assumes a discounted Markov decision-making process (MDP) with a finite horizon, defined by (S, A, P, r, \u03c10, \u03b3, T), where S is the state space, A is the action space, P is a probability distribution for the transition, r: S \u00b7 A \u2192 R is a reward function, \u03c10 is an initial state distribution, GP (0, 1] is a discount factor, and T is the horizon. The aim of RL is to maximize the expected discounted reward overall: E\u03c0, P [\u2211 T = 0 \u03b3 tr (st, at)] via a policy \u03c0 that outputs a distribution of actions in a given state."}, {"heading": "2.2 Count-Based Exploration via Static Hashing", "text": "Our approach discredits the state space with a hash function \u03c6: S \u2192 Z. An exploration bonus r +: S \u2192 R is added to the reward function, defining asr + (s) = \u03b2 \u221a n (\u03c6 (s))), (1) where \u03b2-R \u2265 0 is the bonus coefficient. Initially, the numbers n (\u00b7) are set to zero for the entire range from \u03c6. For each state encountered in the time t, n (st), the number of rewards (r + r +) increases, while the performance is the sum of rewards without bonuses. Note that our approach is a departure from census-based exploration methods such as MBIE-EB, since we evaluate a state number n (s) instead of a state action number n (s, a). State measures n (s, a) are examined in Appendix A.6, but no significant performance gains through the state census could be observed."}, {"heading": "2.3 Count-Based Exploration via Learned Hashing", "text": "If the MDP states have a complex structure, as is the case with the image observations, their similarity is superimposed directly into the pixel space to capture the semantic similarity that one would wish for. (Si) D (Si) s (Si) s (Si) s (Si) s (Si) s (Si) s (Si) s (Si) s (SI) s (SI) s (SI) s (SI) s (SI) s (SI) s (SI) s (SI) s (SI) s (SI) s (SI) s (SI) s (SI) s (SI) s (SI) s (SI) s (SI) s (SI) s (SI) s (SI) s (SI) s (SI (SI) s (SI) s (SI) s (SI (SI) s (SI) s (SI) s (SI (SI) s (SI) s (SI) s (SI) s (SI) s (SI (SI) s (SI) s (SI) s (SI) s (SI (SI) s (SI) s (SI) s (SI (SI) s (SI) s (SI) s (SI (SI) s (SI) s (SI) s (SI (SI) s (SI) s (SI) s (SI (SI) s (SI) s (SI) s (SI) s (SI) s (SI (SI) s (SI) s (SI) s (SI (SI) s (SI) s (SI) s (SI) s (SI) s (SI) s (SI) s (SI) s (SI (SI) s (SI) s (SI) s (SI) s (SI (SI) s (SI) s (SI) s (SI) s (SI) s (SI) s (SI) s (SI"}, {"heading": "3 Experiments", "text": "The experiments were designed to examine and answer the following research questions: 1. Can number-based exploration by hashing significantly improve performance in various areas? 3. How does the proposed method compare with the current state of research into deep RL? 2. What effect does learned or static state have on overall performance when image observations are used? 3. What factors contribute to good performance, e.g. what is the appropriate level of granularity of the hash function? To answer question 1, we perform the proposed method on low RL benchmarks (rllab and ALE), which have few rewards, and compare them with other state-of-the-art algorithms. Question 2 is answered by trying different image preprocessors in Atari 2600 games. Finally, we examine question 3 in sections 3.3 and 3.4."}, {"heading": "3.1 Continuous Control", "text": "The rllab benchmark (Duan et al., 2016) consists of several control tasks to test deep RL algorithms. We have selected several variants of basic and locomotion tasks that, as shown in Figure 2, use sparse rewards and adopt the experimental setup defined in (Houthooft et al., 2016) - a description can be found in Appendix A.2. These tasks are all extremely difficult to solve with naive exploration strategies, such as adding Gaussian noise to the actions. Figure 3 shows the results of TRPO (baseline), TRPO-SimHash and VIME (Houthooft et al., 2016) to the classic tasks MountainCar and CartPoleSwingup, the locomotion task HalfCheetah and the hierarchical task SwimmerGather. Count-based exploration with hashing is able to achieve the goal in all environments (which does not correspond to a zero return), while SwimmerGIMO and VIMO hierarchical task do not work well."}, {"heading": "3.2 Arcade Learning Environment", "text": "The results of the last four years (2016) are the most important benchmarks for processing image information, which evolve in the way they demonstrate the effectiveness of the proposed exploration strategy, selecting six games while requiring significant exploration. (The TRPO batch size, equivalent to 0.4 M frames).Policy and value functions are neural networks with identical architectures too (Mnih et al., 2016) Although the policy and baseline take into account the previous four frames, the counting of the algorithms only looks at the latest frames.BASS For comparison with the learned methods we apply, we use the code. (2016) Although the policy and baseline consider the previous four frames, the counting of the algorithms only looks at the latest frames.BASS"}, {"heading": "3.3 Granularity", "text": "While our proposed method is able to achieve remarkable results without requiring much tuning, the granularity of the hash function should be chosen wisely. Granularity plays a critical role in count-based exploration, where the hash function should bundle states without generalizing or generalizing. Table 2 summarizes granularity parameters for our hash functions. In Table 3, we summarize the performance of TRPO pixel SimHash among different granularities. We choose Frostbite and Venture, where TRPO pixel SimHash outperforms baseline, and select as a reward bonus coefficient \u03b2 = 0.01 x 256k to maintain average bonus rewards of roughly the same magnitude. k = 16 corresponds to only 65536 distinct hash codes, which is not sufficient to distinguish between semantically different states, and therefore leads to poorer performance."}, {"heading": "3.4 A Case Study of Montezuma\u2019s Revenge", "text": "Montezuma's Revenge is widely known for its extremely meagre rewards and difficult explorations (Bellemare et al., 2016). While our method surprisingly does not surpass Bellemare et al. (2016) in this game, we investigate the reasons for this through various experiments. The experimental process below once again demonstrates the importance of a hash function with the correct granularity and encryption of relevant information for solving the MDP. Our first attempt is to use game RAM states as inputs to policy instead of image observations (details in Appendix A.1), resulting in a score of 2500 with TRPO-BASS-SimHash. Our second attempt is to design a hash function that incorporates domain knowledge using an integer-rated vector consisting of the agent (x, y) location, space number and other useful RAM information than the hash code."}, {"heading": "4 Related Work", "text": "The classical counting-based methods such as MBIE (Strehl & Littman, 2005), MBIE-EB and (Kolter & Ng, 2009) solve an approximate Bellman equation as an inner loop before the agent performs an action (Strehl & Littman, 2008). As such, bonus rewards are immediately propagated throughout the state action space (Schulman et al., 2015; Mnih et al., 2016) methods based on limited speed collected from interaction with environments, with value-based (Mnamih et al., 2015) or political gradient-based (Schulman et al., 2015; Mnih et al., 2016) methods based on limited speed. Furthermore, our proposed method should work with contemporary deep RL algorithms, it differs from the classical counting method in which our method is based on visiting invisible states before the bonus reward strategies can be assigned at the beginning of exploration and uninformed localization."}, {"heading": "5 Conclusions", "text": "This paper demonstrates that generalizing classical counting techniques through hashing is capable of delivering an adequate signal for exploration even in continuous and / or high-dimensional MDPs using functional approximations, resulting in near-state-of-the-art performance across benchmarks. It provides a simple but powerful starting point for solving MDPs that require informed exploration."}, {"heading": "Acknowledgments", "text": "We would like to thank our colleagues at Berkeley and OpenAI for their insightful discussions, which were partially funded by the ONR through a PECASE Prize. Yan Duan was also supported by a Berkeley AI Research lab Fellowship and a Huawei Fellowship. Xi Chen was also supported by a Berkeley AI Research lab Fellowship. We are grateful for the support of NSF through a scholarship IIS-1619362 and ARC through a Laureate Fellowship (FL110100281) and through the ARC Centre of Excellence for Mathematical and Statistical Frontiers. Adam Stooke gratefully accepts funding through a scholarship from the Fannie and John Hertz Foundation. Rein Houthooft is supported by a Ph.D. Fellowship from the Research Foundation - Flanders (FWO)."}, {"heading": "A Appendices", "text": "The other tasks are used by a two-tiered neural network policy in the way it is executed by the two layers of the neural network in each layer. (The other layers are modeled by a fully factored Gaussian distributionN), in which each layer is modeled as network output. (The other layers are used by a fully factored Gaussian code in the manner in which the network outputs are modeled.) The other layers are modeled by the network equipment, while the other is a parameter. (The CartPoleSwingup uses a neural network baseline with a layer of 32 ReLU units, while all other tasks have a linear basic function.) For all tasks we use TRP.1 and a discount factor of 0.99. We select SimeSwingup parameters of 32 ReLU units, while all other tasks make use of a linear basic function."}, {"heading": "128 \u2013 1475 4248 2801 3239 3621 1543 395", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "256 \u2013 2583 4497 4437 7849 3516 2260 374", "text": "The results are summarized in Table 6, where k refers to the length of the binary code for hash, while \u03b2 is the multiplicative coefficient for the reward bonus, as defined in Section 2.2. This table shows that most hyperparameter settings significantly exceed the baseline (\u03b2 = 0). In addition, the final results show a clear pattern in response to changing hyperparameters. Small \u03b2 values lead to insufficient exploration, while large \u03b2 values cause the bonus settings to exceed the true rewards. At a fixed k, the values are roughly concave and reach a peak value of about 0.2. Higher granularity k leads to better performance. It can be concluded that the proposed exploration method is robust against hyperparameter changes compared to the baseline and that the best parameter settings can be derived from a relatively coarse grid search."}, {"heading": "128 1475 / 808 4248 / 4302 2801 / 4802 3239 / 7291 3621 / 4243 1543 / 1941 395 / 362", "text": "It is not the first time that the EU Commission has taken such a step."}], "references": [{"title": "Exploratory gradient boosting for reinforcement learning in complex domains", "author": ["David Abel", "Alekh Agarwal", "Fernando Diaz", "Akshay Krishnamurthy", "Robert E Schapire"], "venue": "arXiv preprint arXiv:1603.04119,", "citeRegEx": "Abel et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Abel et al\\.", "year": 2016}, {"title": "Near-optimal hashing algorithms for approximate nearest neighbor in high dimensions", "author": ["Alexandr Andoni", "Piotr Indyk"], "venue": "In Proceedings of the 47th Annual IEEE Symposium on Foundations of Computer Science (FOCS),", "citeRegEx": "Andoni and Indyk.,? \\Q2006\\E", "shortCiteRegEx": "Andoni and Indyk.", "year": 2006}, {"title": "The arcade learning environment: An evaluation platform for general agents", "author": ["MarcGBellemare", "Yavar Naddaf", "Joel Veness", "andMichael Bowling"], "venue": "Journal of Artificial Intelligence Research, 47:253\u2013279,", "citeRegEx": "MarcGBellemare et al\\.,? \\Q2013\\E", "shortCiteRegEx": "MarcGBellemare et al\\.", "year": 2013}, {"title": "Space/time trade-offs in hash coding with allowable errors", "author": ["Burton H. Bloom"], "venue": "Communications of the ACM,", "citeRegEx": "Bloom.,? \\Q1970\\E", "shortCiteRegEx": "Bloom.", "year": 1970}, {"title": "R-max-a general polynomial time algorithm for near-optimal reinforcement learning", "author": [], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Tennenholtz.,? \\Q2002\\E", "shortCiteRegEx": "Tennenholtz.", "year": 2002}, {"title": "Similarity estimation techniques from rounding algorithms", "author": ["Moses S Charikar"], "venue": "In Proceedings of the 34th Annual ACM Symposium on Theory of Computing (STOC),", "citeRegEx": "Charikar.,? \\Q2002\\E", "shortCiteRegEx": "Charikar.", "year": 2002}, {"title": "An improved data stream summary: the count-min sketch and its applications", "author": ["Graham Cormode", "S Muthukrishnan"], "venue": "Journal of Algorithms,", "citeRegEx": "Cormode and Muthukrishnan.,? \\Q2005\\E", "shortCiteRegEx": "Cormode and Muthukrishnan.", "year": 2005}, {"title": "Histograms of oriented gradients for human detection", "author": ["Navneet Dalal", "Bill Triggs"], "venue": "In Proceedings of the IEEE International Conference on Computer Vision and Pattern Recognition (CVPR),", "citeRegEx": "Dalal and Triggs.,? \\Q2005\\E", "shortCiteRegEx": "Dalal and Triggs.", "year": 2005}, {"title": "Benchmarking deep reinforcement learning for continous control", "author": ["Yan Duan", "Xi Chen", "Rein Houthooft", "John Schulman", "Pieter Abbeel"], "venue": "In Proceedings of the 33rd International Conference on Machine Learning (ICML),", "citeRegEx": "Duan et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Duan et al\\.", "year": 2016}, {"title": "Summary cache: A scalable wide-area web cache sharing protocol", "author": ["Li Fan", "Pei Cao", "Jussara Almeida", "Andrei Z Broder"], "venue": "IEEE/ACM Transactions on Networking,", "citeRegEx": "Fan et al\\.,? \\Q2000\\E", "shortCiteRegEx": "Fan et al\\.", "year": 2000}, {"title": "Bayesian reinforcement learning: A survey", "author": ["Mohammad Ghavamzadeh", "Shie Mannor", "Joelle Pineau", "Aviv Tamar"], "venue": "Foundations and Trends in Machine Learning,", "citeRegEx": "Ghavamzadeh et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Ghavamzadeh et al\\.", "year": 2015}, {"title": "Bayes-adaptive simulation-based search with value function approximation", "author": ["Arthur Guez", "Nicolas Heess", "David Silver", "Peter Dayan"], "venue": "In Advances in Neural Information Processing Systems (Advances in Neural Information Processing Systems (NIPS)),", "citeRegEx": "Guez et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Guez et al\\.", "year": 2014}, {"title": "Deep residual learning for image recognition", "author": ["Kaiming He", "Xiangyu Zhang", "Shaoqing Ren", "Jian Sun"], "venue": null, "citeRegEx": "He et al\\.,? \\Q2015\\E", "shortCiteRegEx": "He et al\\.", "year": 2015}, {"title": "Batch normalization: Accelerating deep network training by reducing internal covariate shift", "author": ["Sergey Ioffe", "Christian Szegedy"], "venue": "In Proceedings of the 32nd International Conference on Machine Learning (ICML),", "citeRegEx": "Ioffe and Szegedy.,? \\Q2015\\E", "shortCiteRegEx": "Ioffe and Szegedy.", "year": 2015}, {"title": "Near-optimal regret bounds for reinforcement learning", "author": ["Thomas Jaksch", "Ronald Ortner", "Peter Auer"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Jaksch et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Jaksch et al\\.", "year": 2010}, {"title": "Near-optimal reinforcement learning in polynomial time.Machine Learning", "author": ["Michael Kearns", "Satinder Singh"], "venue": null, "citeRegEx": "Kearns and Singh.,? \\Q2002\\E", "shortCiteRegEx": "Kearns and Singh.", "year": 2002}, {"title": "Adam: A method for stochastic optimization", "author": ["Diederik Kingma", "Jimmy Ba"], "venue": "In Proceedings of the International Conference on Learning Representations (ICLR),", "citeRegEx": "Kingma and Ba.,? \\Q2015\\E", "shortCiteRegEx": "Kingma and Ba.", "year": 2015}, {"title": "Near-bayesian exploration in polynomial time", "author": ["J Zico Kolter", "Andrew Y Ng"], "venue": "In Proceedings of the 26th International Conference on Machine Learning (ICML),", "citeRegEx": "Kolter and Ng.,? \\Q2009\\E", "shortCiteRegEx": "Kolter and Ng.", "year": 2009}, {"title": "Asymptotically efficient adaptive allocation rules", "author": ["Tze Leung Lai", "Herbert Robbins"], "venue": "Advances in Applied Mathematics,", "citeRegEx": "Lai and Robbins.,? \\Q1985\\E", "shortCiteRegEx": "Lai and Robbins.", "year": 1985}, {"title": "Continuous control with deep reinforcement learning", "author": ["Timothy P Lillicrap", "Jonathan J Hunt", "Alexander Pritzel", "Nicolas Heess", "Tom Erez", "Yuval Tassa", "David Silver", "Daan Wierstra"], "venue": "arXiv preprint arXiv:1509.02971,", "citeRegEx": "Lillicrap et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Lillicrap et al\\.", "year": 2015}, {"title": "Object recognition from local scale-invariant features", "author": ["David G Lowe"], "venue": "In Proceedings of the 7th IEEE International Conference on Computer Vision (ICCV),", "citeRegEx": "Lowe.,? \\Q1999\\E", "shortCiteRegEx": "Lowe.", "year": 1999}, {"title": "Human-level control through deep reinforcement learning", "author": ["Volodymyr Mnih", "Koray Kavukcuoglu", "David Silver", "Andrei A Rusu", "Joel Veness", "Marc G Bellemare", "Alex Graves", "Martin Riedmiller", "Andreas K Fidjeland", "Georg Ostrovski"], "venue": "Nature, 518(7540):529\u2013533,", "citeRegEx": "Mnih et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Mnih et al\\.", "year": 2015}, {"title": "Asynchronous methods for deep reinforcement learning", "author": ["Volodymyr Mnih", "Adria Puigdomenech Badia", "Mehdi Mirza", "Alex Graves", "Timothy P Lillicrap", "Tim Harley", "David Silver", "Koray Kavukcuoglu"], "venue": "arXiv preprint arXiv:1602.01783,", "citeRegEx": "Mnih et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Mnih et al\\.", "year": 2016}, {"title": "Massively parallel methods for deep reinforcement learning", "author": ["Arun Nair", "Praveen Srinivasan", "Sam Blackwell", "Cagdas Alcicek", "Rory Fearon", "Alessandro De Maria", "Vedavyas Panneershelvam", "Mustafa Suleyman", "Charles Beattie", "Stig Petersen"], "venue": "arXiv preprint arXiv:1507.04296,", "citeRegEx": "Nair et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Nair et al\\.", "year": 2015}, {"title": "Generalization and exploration via randomized value functions", "author": ["Ian Osband", "Benjamin Van Roy", "Zheng Wen"], "venue": "In Proceedings of the 33rd International Conference on Machine Learning (ICML),", "citeRegEx": "Osband et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Osband et al\\.", "year": 2016}, {"title": "What is intrinsic motivation? A typology of computational approaches", "author": ["Pierre-Yves Oudeyer", "Frederic Kaplan"], "venue": "Frontiers in Neurorobotics,", "citeRegEx": "Oudeyer and Kaplan.,? \\Q2007\\E", "shortCiteRegEx": "Oudeyer and Kaplan.", "year": 2007}, {"title": "PAC optimal exploration in continuous space Markov decision processes", "author": ["Jason Pazis", "Ronald Parr"], "venue": "In Proceedings of the 27th AAAI Conference on Artificial Intelligence (AAAI),", "citeRegEx": "Pazis and Parr.,? \\Q2013\\E", "shortCiteRegEx": "Pazis and Parr.", "year": 2013}, {"title": "Formal theory of creativity, fun, and intrinsic motivation (1990\u20132010)", "author": ["J\u00fcrgen Schmidhuber"], "venue": "IEEE Transactions on Autonomous Mental Development,", "citeRegEx": "Schmidhuber.,? \\Q2010\\E", "shortCiteRegEx": "Schmidhuber.", "year": 2010}, {"title": "Trust region policy optimization", "author": ["John Schulman", "Sergey Levine", "Philipp Moritz", "Michael I Jordan", "Pieter Abbeel"], "venue": "In Proceedings of the 32nd International Conference on Machine Learning (ICML),", "citeRegEx": "Schulman et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Schulman et al\\.", "year": 2015}, {"title": "Very deep convolutional networks for large-scale image recognition", "author": ["Karen Simonyan", "Andrew Zisserman"], "venue": "arXiv preprint arXiv:1409.1556,", "citeRegEx": "Simonyan and Zisserman.,? \\Q2014\\E", "shortCiteRegEx": "Simonyan and Zisserman.", "year": 2014}, {"title": "Incentivizing exploration in reinforcement learning with deep predictive models", "author": ["Bradly C Stadie", "Sergey Levine", "Pieter Abbeel"], "venue": "arXiv preprint arXiv:1507.00814,", "citeRegEx": "Stadie et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Stadie et al\\.", "year": 2015}, {"title": "A theoretical analysis of model-based interval estimation", "author": ["Alexander L Strehl", "Michael L Littman"], "venue": "In Proceedings of the 21st International Conference on Machine Learning (ICML),", "citeRegEx": "Strehl and Littman.,? \\Q2005\\E", "shortCiteRegEx": "Strehl and Littman.", "year": 2005}, {"title": "An analysis of model-based interval estimation for Markov decision processes", "author": ["Alexander L Strehl", "Michael L Littman"], "venue": "Journal of Computer and System Sciences,", "citeRegEx": "Strehl and Littman.,? \\Q2008\\E", "shortCiteRegEx": "Strehl and Littman.", "year": 2008}, {"title": "Planning to be surprised: Optimal Bayesian exploration in dynamic environments", "author": ["Yi Sun", "Faustino Gomez", "J\u00fcrgen Schmidhuber"], "venue": "In Proceedings of the 4th International Conference on Artificial General Intelligence (AGI),", "citeRegEx": "Sun et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Sun et al\\.", "year": 2011}, {"title": "DAISY: An efficient dense descriptor applied to wide-baseline stereo", "author": ["Engin Tola", "Vincent Lepetit", "Pascal Fua"], "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence,", "citeRegEx": "Tola et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Tola et al\\.", "year": 2010}, {"title": "Pixel recurrent neural networks", "author": ["Aaron van den Oord", "Nal Kalchbrenner", "Koray Kavukcuoglu"], "venue": "In Proceedings of the 33rd International Conference on Machine Learning (ICML),", "citeRegEx": "Oord et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Oord et al\\.", "year": 2016}, {"title": "Learning functions across many orders of magnitudes", "author": ["Hado van Hasselt", "Arthur Guez", "Matteo Hessel", "David Silver"], "venue": "arXiv preprint arXiv:1602.07714,", "citeRegEx": "Hasselt et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Hasselt et al\\.", "year": 2016}, {"title": "Deep reinforcement learning with double Q-learning", "author": ["Hado van Hasselt", "Arthur Guez", "David Silver"], "venue": "In Proceedings of the 30th AAAI Conference on Artificial Intelligence (AAAI),", "citeRegEx": "Hasselt et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Hasselt et al\\.", "year": 2016}, {"title": "Strategic attentive writer for learning macro-actions", "author": ["Alexander Vezhnevets", "Volodymyr Mnih", "John Agapiou", "Simon Osindero", "Alex Graves", "Oriol Vinyals", "Koray Kavukcuoglu"], "venue": "In Advances in Neural Information Processing Systems", "citeRegEx": "Vezhnevets et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Vezhnevets et al\\.", "year": 2016}, {"title": "Dueling network architectures for deep reinforcement learning", "author": ["ZiyuWang", "Nando de Freitas", "andMarc Lanctot"], "venue": "In Proceedings of the 33rd International Conference on Machine Learning (ICML),", "citeRegEx": "ZiyuWang et al\\.,? \\Q2016\\E", "shortCiteRegEx": "ZiyuWang et al\\.", "year": 2016}, {"title": "2016), following the sparse reward adaptation", "author": ["Duan"], "venue": null, "citeRegEx": "Duan,? \\Q2016\\E", "shortCiteRegEx": "Duan", "year": 2016}, {"title": "The tasks have the following state and action dimensions: CartPoleSwingup", "author": ["Houthooft"], "venue": null, "citeRegEx": "Houthooft,? \\Q2016\\E", "shortCiteRegEx": "Houthooft", "year": 2016}], "referenceMentions": [{"referenceID": 21, "context": "Most of the recent state-of-the-art RL results have been obtained using simple exploration strategies such as uniform sampling (Mnih et al., 2015) and i.", "startOffset": 127, "endOffset": 146}, {"referenceID": 28, "context": "/correlated Gaussian noise (Schulman et al., 2015; Lillicrap et al., 2015).", "startOffset": 27, "endOffset": 74}, {"referenceID": 19, "context": "/correlated Gaussian noise (Schulman et al., 2015; Lillicrap et al., 2015).", "startOffset": 27, "endOffset": 74}, {"referenceID": 8, "context": "We select problems from rllab (Duan et al., 2016) and Atari 2600 (Bellemare et al.", "startOffset": 30, "endOffset": 49}, {"referenceID": 23, "context": "(Osband et al., 2016a) led to faster learning in a range of Atari 2600 games by training an ensemble of Q-functions. Intrinsic motivation methods using pseudo-counts achieve state-of-the-art performance on Montezuma\u2019s Revenge, an extremely challenging Atari 2600 game (Bellemare et al., 2016). Variational Information Maximizing Exploration (VIME, Houthooft et al. (2016)) encourages the agent to explore by acquiring information about environment dynamics, and performs well on various robotic locomotion problems with sparse rewards.", "startOffset": 1, "endOffset": 372}, {"referenceID": 23, "context": "(Osband et al., 2016a) led to faster learning in a range of Atari 2600 games by training an ensemble of Q-functions. Intrinsic motivation methods using pseudo-counts achieve state-of-the-art performance on Montezuma\u2019s Revenge, an extremely challenging Atari 2600 game (Bellemare et al., 2016). Variational Information Maximizing Exploration (VIME, Houthooft et al. (2016)) encourages the agent to explore by acquiring information about environment dynamics, and performs well on various robotic locomotion problems with sparse rewards. However, we have not seen a very simple and fast method that can work across different domains. Some of the classic, theoretically-justified exploration methods are based on counting state-action visitations, and turning this count into a bonus reward. In the bandit setting, the well-known UCB algorithm of Lai & Robbins (1985) chooses the action at at time t that maximizes r\u0302 (at ) + \u221a 2 log t n(at ) where r\u0302 (at ) is the estimated reward, and n(at ) is the number of times action at was previously chosen.", "startOffset": 1, "endOffset": 865}, {"referenceID": 23, "context": "(Osband et al., 2016a) led to faster learning in a range of Atari 2600 games by training an ensemble of Q-functions. Intrinsic motivation methods using pseudo-counts achieve state-of-the-art performance on Montezuma\u2019s Revenge, an extremely challenging Atari 2600 game (Bellemare et al., 2016). Variational Information Maximizing Exploration (VIME, Houthooft et al. (2016)) encourages the agent to explore by acquiring information about environment dynamics, and performs well on various robotic locomotion problems with sparse rewards. However, we have not seen a very simple and fast method that can work across different domains. Some of the classic, theoretically-justified exploration methods are based on counting state-action visitations, and turning this count into a bonus reward. In the bandit setting, the well-known UCB algorithm of Lai & Robbins (1985) chooses the action at at time t that maximizes r\u0302 (at ) + \u221a 2 log t n(at ) where r\u0302 (at ) is the estimated reward, and n(at ) is the number of times action at was previously chosen. In the MDP setting, some of the algorithms have similar structure, for example, Model Based Interval Estimation\u2013Exploration Bonus (MBIE-EB) of Strehl & Littman (2008) counts state-action pairs with a table n(s, a) and adding a bonus reward of the form \u03b2 \u221a n(s,a) to encourage exploring less visited pairs.", "startOffset": 1, "endOffset": 1214}, {"referenceID": 23, "context": "(Osband et al., 2016a) led to faster learning in a range of Atari 2600 games by training an ensemble of Q-functions. Intrinsic motivation methods using pseudo-counts achieve state-of-the-art performance on Montezuma\u2019s Revenge, an extremely challenging Atari 2600 game (Bellemare et al., 2016). Variational Information Maximizing Exploration (VIME, Houthooft et al. (2016)) encourages the agent to explore by acquiring information about environment dynamics, and performs well on various robotic locomotion problems with sparse rewards. However, we have not seen a very simple and fast method that can work across different domains. Some of the classic, theoretically-justified exploration methods are based on counting state-action visitations, and turning this count into a bonus reward. In the bandit setting, the well-known UCB algorithm of Lai & Robbins (1985) chooses the action at at time t that maximizes r\u0302 (at ) + \u221a 2 log t n(at ) where r\u0302 (at ) is the estimated reward, and n(at ) is the number of times action at was previously chosen. In the MDP setting, some of the algorithms have similar structure, for example, Model Based Interval Estimation\u2013Exploration Bonus (MBIE-EB) of Strehl & Littman (2008) counts state-action pairs with a table n(s, a) and adding a bonus reward of the form \u03b2 \u221a n(s,a) to encourage exploring less visited pairs. Kolter & Ng (2009) show that the inverse-square-root dependence is optimal.", "startOffset": 1, "endOffset": 1372}, {"referenceID": 5, "context": "A computationally efficient type of LSH is SimHash (Charikar, 2002), which measures similarity by angular distance.", "startOffset": 51, "endOffset": 67}, {"referenceID": 20, "context": "Previous work in computer vision (Lowe, 1999; Dalal & Triggs, 2005; Tola et al., 2010) introduce manually designed feature representations of images that are suitable for semantic tasks including detection and classification.", "startOffset": 33, "endOffset": 86}, {"referenceID": 34, "context": "Previous work in computer vision (Lowe, 1999; Dalal & Triggs, 2005; Tola et al., 2010) introduce manually designed feature representations of images that are suitable for semantic tasks including detection and classification.", "startOffset": 33, "endOffset": 86}, {"referenceID": 12, "context": "More recent methods learn complex features directly from data by training convolutional neural networks (Krizhevsky et al., 2012; Simonyan & Zisserman, 2014; He et al., 2015).", "startOffset": 104, "endOffset": 174}, {"referenceID": 28, "context": "Trust Region Policy Optimization (TRPO, Schulman et al. (2015)) is chosen as the RL algorithm for all experiments, because it can handle both discrete and continuous action spaces, can conveniently ensure stable improvement in the policy performance, and is relatively insensitive to hyperparameter changes.", "startOffset": 40, "endOffset": 63}, {"referenceID": 8, "context": "The rllab benchmark (Duan et al., 2016) consists of various control tasks to test deep RL algorithms.", "startOffset": 20, "endOffset": 39}, {"referenceID": 8, "context": "Figure 2: Illustrations of the rllab tasks used in the continuous control experiments, namely MountainCar, CartPoleSwingup, SimmerGather, and HalfCheetah; taken from (Duan et al., 2016).", "startOffset": 166, "endOffset": 185}, {"referenceID": 22, "context": "Policies and value functions are neural networks with identical architectures to (Mnih et al., 2016).", "startOffset": 81, "endOffset": 100}, {"referenceID": 38, "context": "2 1450 \u2013 3439 \u2013 369 1 While Vezhnevets et al. (2016) reported best score 8108, their evaluation was based on top 5 agents trained with 500M time steps, hence not comparable.", "startOffset": 28, "endOffset": 53}, {"referenceID": 23, "context": ", 2016), Gorila (Nair et al., 2015), and DQN Pop-Art (van Hasselt et al.", "startOffset": 16, "endOffset": 35}, {"referenceID": 21, "context": "In contrast, contemporary deep RL algorithms propagate the bonus signal based on rollouts collected from interacting with environments, with value-based (Mnih et al., 2015) or policy gradient-based (Schulman et al.", "startOffset": 153, "endOffset": 172}, {"referenceID": 28, "context": ", 2015) or policy gradient-based (Schulman et al., 2015; Mnih et al., 2016) methods, at limited speed.", "startOffset": 33, "endOffset": 75}, {"referenceID": 22, "context": ", 2015) or policy gradient-based (Schulman et al., 2015; Mnih et al., 2016) methods, at limited speed.", "startOffset": 33, "endOffset": 75}, {"referenceID": 14, "context": ", R-Max (Brafman & Tennenholtz, 2002), UCRL (Jaksch et al., 2010), and E3 (Kearns & Singh, 2002).", "startOffset": 44, "endOffset": 65}, {"referenceID": 11, "context": "Bayesian RL methods (Kolter & Ng, 2009; Guez et al., 2014; Sun et al., 2011; Ghavamzadeh et al., 2015), which keep track of a distribution over MDPs, are an alternative to optimism-based methods.", "startOffset": 20, "endOffset": 102}, {"referenceID": 33, "context": "Bayesian RL methods (Kolter & Ng, 2009; Guez et al., 2014; Sun et al., 2011; Ghavamzadeh et al., 2015), which keep track of a distribution over MDPs, are an alternative to optimism-based methods.", "startOffset": 20, "endOffset": 102}, {"referenceID": 10, "context": "Bayesian RL methods (Kolter & Ng, 2009; Guez et al., 2014; Sun et al., 2011; Ghavamzadeh et al., 2015), which keep track of a distribution over MDPs, are an alternative to optimism-based methods.", "startOffset": 20, "endOffset": 102}, {"referenceID": 3, "context": "A related line of classical explorationmethods is based on the idea of optimism in the face of uncertainty (Brafman & Tennenholtz, 2002) but not restricted to using counting to implement \u201coptimism\u201d, e.g., R-Max (Brafman & Tennenholtz, 2002), UCRL (Jaksch et al., 2010), and E3 (Kearns & Singh, 2002). These methods, similar to MBIE and MBIE-EB, have theoretical guarantees in tabular settings. Bayesian RL methods (Kolter & Ng, 2009; Guez et al., 2014; Sun et al., 2011; Ghavamzadeh et al., 2015), which keep track of a distribution over MDPs, are an alternative to optimism-based methods. Extensions to continuous state space have been proposed by Pazis & Parr (2013) and Osband et al.", "startOffset": 118, "endOffset": 669}, {"referenceID": 3, "context": "A related line of classical explorationmethods is based on the idea of optimism in the face of uncertainty (Brafman & Tennenholtz, 2002) but not restricted to using counting to implement \u201coptimism\u201d, e.g., R-Max (Brafman & Tennenholtz, 2002), UCRL (Jaksch et al., 2010), and E3 (Kearns & Singh, 2002). These methods, similar to MBIE and MBIE-EB, have theoretical guarantees in tabular settings. Bayesian RL methods (Kolter & Ng, 2009; Guez et al., 2014; Sun et al., 2011; Ghavamzadeh et al., 2015), which keep track of a distribution over MDPs, are an alternative to optimism-based methods. Extensions to continuous state space have been proposed by Pazis & Parr (2013) and Osband et al. (2016b). Another type of exploration is curiosity-based exploration.", "startOffset": 118, "endOffset": 695}, {"referenceID": 3, "context": "A related line of classical explorationmethods is based on the idea of optimism in the face of uncertainty (Brafman & Tennenholtz, 2002) but not restricted to using counting to implement \u201coptimism\u201d, e.g., R-Max (Brafman & Tennenholtz, 2002), UCRL (Jaksch et al., 2010), and E3 (Kearns & Singh, 2002). These methods, similar to MBIE and MBIE-EB, have theoretical guarantees in tabular settings. Bayesian RL methods (Kolter & Ng, 2009; Guez et al., 2014; Sun et al., 2011; Ghavamzadeh et al., 2015), which keep track of a distribution over MDPs, are an alternative to optimism-based methods. Extensions to continuous state space have been proposed by Pazis & Parr (2013) and Osband et al. (2016b). Another type of exploration is curiosity-based exploration. These methods try to capture the agent\u2019s surprise about transition dynamics. As the agent tries to optimize for surprise, it naturally discovers novel states. We refer the reader to Schmidhuber (2010) and Oudeyer & Kaplan (2007) for an extensive review on curiosity and intrinsic rewards.", "startOffset": 118, "endOffset": 957}, {"referenceID": 3, "context": "A related line of classical explorationmethods is based on the idea of optimism in the face of uncertainty (Brafman & Tennenholtz, 2002) but not restricted to using counting to implement \u201coptimism\u201d, e.g., R-Max (Brafman & Tennenholtz, 2002), UCRL (Jaksch et al., 2010), and E3 (Kearns & Singh, 2002). These methods, similar to MBIE and MBIE-EB, have theoretical guarantees in tabular settings. Bayesian RL methods (Kolter & Ng, 2009; Guez et al., 2014; Sun et al., 2011; Ghavamzadeh et al., 2015), which keep track of a distribution over MDPs, are an alternative to optimism-based methods. Extensions to continuous state space have been proposed by Pazis & Parr (2013) and Osband et al. (2016b). Another type of exploration is curiosity-based exploration. These methods try to capture the agent\u2019s surprise about transition dynamics. As the agent tries to optimize for surprise, it naturally discovers novel states. We refer the reader to Schmidhuber (2010) and Oudeyer & Kaplan (2007) for an extensive review on curiosity and intrinsic rewards.", "startOffset": 118, "endOffset": 985}, {"referenceID": 3, "context": "A related line of classical explorationmethods is based on the idea of optimism in the face of uncertainty (Brafman & Tennenholtz, 2002) but not restricted to using counting to implement \u201coptimism\u201d, e.g., R-Max (Brafman & Tennenholtz, 2002), UCRL (Jaksch et al., 2010), and E3 (Kearns & Singh, 2002). These methods, similar to MBIE and MBIE-EB, have theoretical guarantees in tabular settings. Bayesian RL methods (Kolter & Ng, 2009; Guez et al., 2014; Sun et al., 2011; Ghavamzadeh et al., 2015), which keep track of a distribution over MDPs, are an alternative to optimism-based methods. Extensions to continuous state space have been proposed by Pazis & Parr (2013) and Osband et al. (2016b). Another type of exploration is curiosity-based exploration. These methods try to capture the agent\u2019s surprise about transition dynamics. As the agent tries to optimize for surprise, it naturally discovers novel states. We refer the reader to Schmidhuber (2010) and Oudeyer & Kaplan (2007) for an extensive review on curiosity and intrinsic rewards. Several exploration strategies for deep RL have been proposed to handle high-dimensional state space recently. Houthooft et al. (2016) propose VIME, in which information gain is measured in Bayesian neural networks modeling the MDP dynamics, which is used an exploration bonus.", "startOffset": 118, "endOffset": 1180}, {"referenceID": 3, "context": "A related line of classical explorationmethods is based on the idea of optimism in the face of uncertainty (Brafman & Tennenholtz, 2002) but not restricted to using counting to implement \u201coptimism\u201d, e.g., R-Max (Brafman & Tennenholtz, 2002), UCRL (Jaksch et al., 2010), and E3 (Kearns & Singh, 2002). These methods, similar to MBIE and MBIE-EB, have theoretical guarantees in tabular settings. Bayesian RL methods (Kolter & Ng, 2009; Guez et al., 2014; Sun et al., 2011; Ghavamzadeh et al., 2015), which keep track of a distribution over MDPs, are an alternative to optimism-based methods. Extensions to continuous state space have been proposed by Pazis & Parr (2013) and Osband et al. (2016b). Another type of exploration is curiosity-based exploration. These methods try to capture the agent\u2019s surprise about transition dynamics. As the agent tries to optimize for surprise, it naturally discovers novel states. We refer the reader to Schmidhuber (2010) and Oudeyer & Kaplan (2007) for an extensive review on curiosity and intrinsic rewards. Several exploration strategies for deep RL have been proposed to handle high-dimensional state space recently. Houthooft et al. (2016) propose VIME, in which information gain is measured in Bayesian neural networks modeling the MDP dynamics, which is used an exploration bonus. Stadie et al. (2015) propose to use the prediction error of a learned dynamics model as an exploration bonus.", "startOffset": 118, "endOffset": 1344}, {"referenceID": 3, "context": "A related line of classical explorationmethods is based on the idea of optimism in the face of uncertainty (Brafman & Tennenholtz, 2002) but not restricted to using counting to implement \u201coptimism\u201d, e.g., R-Max (Brafman & Tennenholtz, 2002), UCRL (Jaksch et al., 2010), and E3 (Kearns & Singh, 2002). These methods, similar to MBIE and MBIE-EB, have theoretical guarantees in tabular settings. Bayesian RL methods (Kolter & Ng, 2009; Guez et al., 2014; Sun et al., 2011; Ghavamzadeh et al., 2015), which keep track of a distribution over MDPs, are an alternative to optimism-based methods. Extensions to continuous state space have been proposed by Pazis & Parr (2013) and Osband et al. (2016b). Another type of exploration is curiosity-based exploration. These methods try to capture the agent\u2019s surprise about transition dynamics. As the agent tries to optimize for surprise, it naturally discovers novel states. We refer the reader to Schmidhuber (2010) and Oudeyer & Kaplan (2007) for an extensive review on curiosity and intrinsic rewards. Several exploration strategies for deep RL have been proposed to handle high-dimensional state space recently. Houthooft et al. (2016) propose VIME, in which information gain is measured in Bayesian neural networks modeling the MDP dynamics, which is used an exploration bonus. Stadie et al. (2015) propose to use the prediction error of a learned dynamics model as an exploration bonus. Thompson sampling through bootstrapping is proposed by Osband et al. (2016a), using bootstrapped Q-functions.", "startOffset": 118, "endOffset": 1510}, {"referenceID": 3, "context": "A related line of classical explorationmethods is based on the idea of optimism in the face of uncertainty (Brafman & Tennenholtz, 2002) but not restricted to using counting to implement \u201coptimism\u201d, e.g., R-Max (Brafman & Tennenholtz, 2002), UCRL (Jaksch et al., 2010), and E3 (Kearns & Singh, 2002). These methods, similar to MBIE and MBIE-EB, have theoretical guarantees in tabular settings. Bayesian RL methods (Kolter & Ng, 2009; Guez et al., 2014; Sun et al., 2011; Ghavamzadeh et al., 2015), which keep track of a distribution over MDPs, are an alternative to optimism-based methods. Extensions to continuous state space have been proposed by Pazis & Parr (2013) and Osband et al. (2016b). Another type of exploration is curiosity-based exploration. These methods try to capture the agent\u2019s surprise about transition dynamics. As the agent tries to optimize for surprise, it naturally discovers novel states. We refer the reader to Schmidhuber (2010) and Oudeyer & Kaplan (2007) for an extensive review on curiosity and intrinsic rewards. Several exploration strategies for deep RL have been proposed to handle high-dimensional state space recently. Houthooft et al. (2016) propose VIME, in which information gain is measured in Bayesian neural networks modeling the MDP dynamics, which is used an exploration bonus. Stadie et al. (2015) propose to use the prediction error of a learned dynamics model as an exploration bonus. Thompson sampling through bootstrapping is proposed by Osband et al. (2016a), using bootstrapped Q-functions. The most related exploration strategy is proposed by Bellemare et al. (2016), in which an exploration bonus is added inversely proportional to the square root of a pseudo-count quantity.", "startOffset": 118, "endOffset": 1620}, {"referenceID": 0, "context": "Another method similar to hashing is proposed by Abel et al. (2016), which clusters states and counts cluster centers instead of the true states, but this method has yet to be tested on standard exploration benchmark problems.", "startOffset": 49, "endOffset": 68}], "year": 2017, "abstractText": "Count-based exploration algorithms are known to perform near-optimally when used in conjunction with tabular reinforcement learning (RL) methods for solving small discrete Markov decision processes (MDPs). It is generally thought that count-based methods cannot be applied in high-dimensional state spaces, since most states will only occur once. Recent deep RL exploration strategies are able to deal with high-dimensional continuous state spaces through complex heuristics, often relying on optimism in the face of uncertainty or intrinsic motivation. In this work, we describe a surprising finding: a simple generalization of the classic count-based approach can reach near state-of-the-art performance on various highdimensional and/or continuous deep RL benchmarks. States are mapped to hash codes, which allows to count their occurrences with a hash table. These counts are then used to compute a reward bonus according to the classic count-based exploration theory. We find that simple hash functions can achieve surprisingly good results on many challenging tasks. Furthermore, we show that a domain-dependent learned hash code may further improve these results. Detailed analysis reveals important aspects of a good hash function: 1) having appropriate granularity and 2) encoding information relevant to solving the MDP. This exploration strategy achieves near state-of-the-art performance on both continuous control tasks and Atari 2600 games, hence providing a simple yet powerful baseline for solving MDPs that require considerable exploration.", "creator": "LaTeX with hyperref package"}}}