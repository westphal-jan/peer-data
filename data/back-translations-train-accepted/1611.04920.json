{"id": "1611.04920", "review": {"conference": "AAAI", "VERSION": "v1", "DATE_OF_SUBMISSION": "15-Nov-2016", "title": "Unsupervised Learning with Truncated Gaussian Graphical Models", "abstract": "Gaussian graphical models (GGMs) are widely used for statistical modeling, because of ease of inference and the ubiquitous use of the normal distribution in practical approximations. However, they are also known for their limited modeling abilities, due to the Gaussian assumption. In this paper, we introduce a novel variant of GGMs, which relaxes the Gaussian restriction and yet admits efficient inference. Specifically, we impose a bipartite structure on the GGM and govern the hidden variables by truncated normal distributions. The nonlinearity of the model is revealed by its connection to rectified linear unit (ReLU) neural networks. Meanwhile, thanks to the bipartite structure and appealing properties of truncated normals, we are able to train the models efficiently using contrastive divergence. We consider three output constructs, accounting for real-valued, binary and count data. We further extend the model to deep structures and show that deep models can be used for unsupervised pre-training of rectifier neural networks. Extensive experimental results are provided to validate the proposed models and demonstrate their superiority over competing models.", "histories": [["v1", "Tue, 15 Nov 2016 16:26:17 GMT  (590kb,D)", "http://arxiv.org/abs/1611.04920v1", "To appear in AAAI 2017"], ["v2", "Sun, 20 Nov 2016 19:08:51 GMT  (643kb,D)", "http://arxiv.org/abs/1611.04920v2", "To appear in AAAI 2017"]], "COMMENTS": "To appear in AAAI 2017", "reviews": [], "SUBJECTS": "stat.ML cs.LG", "authors": ["qinliang su", "xuejun liao", "chunyuan li", "zhe gan", "lawrence carin"], "accepted": true, "id": "1611.04920"}, "pdf": {"name": "1611.04920.pdf", "metadata": {"source": "CRF", "title": "Unsupervised Learning with Truncated Gaussian Graphical Models", "authors": ["Qinliang Su", "Xuejun Liao", "Chunyuan Li", "Zhe Gan", "Lawrence Carin"], "emails": ["qinliang.su@duke.edu,", "xjliao@duke.edu,", "chunyuan.li@duke.edu,", "zhe.gan@duke.edu,", "lcarin@duke.edu"], "sections": [{"heading": "Introduction", "text": "In fact, most of them will be able to play by the rules they have imposed on themselves."}, {"heading": "Related Work", "text": "The proposed RTGGM is not only a new member of the GGM family, but is also closely related to RBM (Hinton 2002). One of the main differences between the two models is their inherent nonlinearity. In an RTGGM, the visible and hidden variables are associated by smoothed ReLU functions, while they are related by sigmoid functions in an RBM. ReLU is widely used in neural networks and has achieved enormous success due to its simpler training properties (Jarrett et al. 2009). Considering this, there are many efforts to bring ReLU into RBM. For example (Nair and Hinton 2010), it has been proposed to replace binary hidden units by reflected Gaussian approximation. Although a ReLU-like nonlinearity is induced, the proposed model is specified by only two conditional distributions that lack a common distribution description."}, {"heading": "Formulation of Restricted-Truncated Gaussian Graphical Models", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "Basic Model", "text": "The common probability distribution of anRTGGM is defined as an indicator function, E (x, h) is an energy function defined as E (x, h), 12 (xT) diag (a) x + hT diag (d) h \u2212 2xTWh \u2212 2bTx \u2212 2cTh), (2) Z is the partition function, the superscript T stands for matrix transpose, and vice versa, {W, a, d, c} collects all model parameters. The common distribution in (1) can be written as asp (x, h; p) = NT \u2212 j \u2212 -like distribution, and p \u2212 j \u2212 -like distribution."}, {"heading": "Variants", "text": "It is important to maintain the expressiveness of the model. In the basic version of RTGGM, the visible variables are also truncated to obtain symmetry, but this is not necessary. The visible domain can be modified to match the type of data in an application. Below, we present three variants of the basic RTGGM that deal with real, binary and numerical data. In all cases, the expression p (x) is modified in (4), but the expression p (h) remains the same as in (5), and therefore the ReLU nonlinearity is preserved in this case. Real-Valued Data The common distribution is p (x, h) = 1Z e \u2212 E (x, h) I (h) I (h) I (h). The expression of p (x) h (h) changes top (x)."}, {"heading": "Partition Function Estimation", "text": "To evaluate the performance of the model, we have to calculate the division function. By evaluating the two-part structure in an RTGGM as well as the appealing properties of the truncated norms, we can show that we use the collected data (AIS) (Salakhutdinov and Murray 2008; Neal 2001) to estimate them. Here, we focus only on the estimate for the RTGGM with binary data; details for the other types of data are provided in the Supplementary Material. The common distribution of the RTGGM for binary data can be represented asp (x, h). After integrating the hidden variables h, we get p (x) = 1Z e \u2212 12 (x) h \u00b2 2 \u2212 2xT Wh \u2212 2bT x \u2212 2cT h) I (x, 1} n) I (h \u00b2)."}, {"heading": "Experiments", "text": "We report on the experimental results of RTGGM models on various publicly available datasets, including binary, counter, and real value data, and compare them with competing models. For all RTGGM models considered below, we use x (0) and x (25) to get a CD-based gradient estimate, and then use RMSprop (Tieleman and Hinton 2012) to update the model parameters, with the RMSprop delay set to 0.95."}, {"heading": "Binary Data", "text": "The binary versions of MNIST and Caltech 101 Silhouette datasets are being considered. MNIST contains 28 x 28 images of ten handwritten digits, with 60,000 and 10,000 images in the training and test sets, respectively. Caltech 101 Silhouettes is a set of 28 x 28 images for the polygon outlines of objects, of which 6364 are used for training and 2307 for testing (Marlin et al. 2010). Two RTGGGMs, with 100 and 500 hidden nodes, are being trained and tested, but the learning rate is 10 \u2212 4. Log probabilities are estimated at 100,000 inverse temperatures \u03b2k, uniformly divided into [0, 1]. The final estimate is an average of over 100 independent AIS runs.Tables 1 and 2 summarize the average test probabilities on MNIST and Caltech 101 silhouettes. For comparison, the corresponding results of competing models are presented."}, {"heading": "Count Data", "text": "Two publicly available corporas are considered: 20NewsGroups and Reuters Corpus Volume. Both corporas are pre-processed in the same way as in (Hinton and Salakhutdinov 2009). An RTGGM with 50 hidden nodes is trained, calculating the learning rate with the same setting as in the previous experiment. Helplessness is evaluated using the setting used in (Hinton and Salakhutdinov 2009) over 50 heldout documents. For each document, we obtain the test log probability as an average over 100 AIS runs, each with 100,000 inverse temperatures \u03b2k.Table 3 shows the averaged test perplexity per word for the RTGGM. For comparison, we also report on the perplexities of the LDA with 50 and 200 topics, as well as those of the replicated Softmax model (Hinton and Salakhutdinov 2009) with 50 topics. The RSM is a variant of the RBM, which processes data and these RTM results are calculated in a more RTM-like the RTM models."}, {"heading": "Unsupervised Pre-training of ReLU Neural Networks", "text": "As described above, deep RTGGMs can be used to perpetuate multi-layer ReLU neural networks by exploiting the unmarked information. In this task, MNIST and NORB datasets are considered, with MNIST providing the same data as in previous experiments. NORB is a set of images from 6 class tables Table 1: Log probability of test data on MNIST datasets. Model Dim Test log-prob. RBM 500 \u2212 SBN 300-400 \u2212 85.48 DBN 500-2000 \u2212 86.2 RTGGM 100 \u2212 89.3 RTGGM 500 \u2212 99.2 Table 2: Log probability of test data on Caltech 101Silhouettes dataset. Model Dim Test log-prob. RBM 500 \u2212 114.7 RBM 4000 \u2212 107.7 SBN 100-300 \u2212 107.7 SBN 100 \u2212 113.3 RTGGM 500 \u2212 105.1"}, {"heading": "Conclusions", "text": "We have introduced a novel variant of GGM called Restricted Truncated GGM (RTGGM) to improve its reprint data set without pre-train. Pre-Trainrectified RBM RTGGMMNIST 1.43% 1.33% 1.17% NORB 16.88% 16.43% 16.12%. Table 4: Average classification errors achieved by the multi-layered neural network ReLU without pre-training, Pre-Train by the method in (Nair and Hinton 2010) (referred to in the table as \"rectified RBM\") and Pre-Train by the deep RTGGM.tational capabilities while maintaining its beautiful (simple inference) properties. The new model is achieved by truncating the variables of an undirected GGM and imposing a bipartite structure on the truncated GGM."}], "references": [{"title": "T", "author": ["D.H. Ackley", "G.E. Hinton", "Sejnowski"], "venue": "J.", "citeRegEx": "Ackley. Hinton. and Sejnowski 1985", "shortCiteRegEx": null, "year": 1985}, {"title": "and Bengio", "author": ["J. Bornschein"], "venue": "Y.", "citeRegEx": "Bornschein and Bengio 2015", "shortCiteRegEx": null, "year": 2015}, {"title": "Enhanced gradient for training restricted boltzmann machines. Neural computation 25(3):805\u2013831", "author": ["Raiko Cho", "K. Ilin 2013] Cho", "T. Raiko", "A. Ilin"], "venue": null, "citeRegEx": "Cho et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Cho et al\\.", "year": 2013}, {"title": "G", "author": ["B.J. Frey", "Hinton"], "venue": "E.", "citeRegEx": "Frey and Hinton 1999", "shortCiteRegEx": null, "year": 1999}, {"title": "B", "author": ["Frey"], "venue": "J.", "citeRegEx": "Frey 1997", "shortCiteRegEx": null, "year": 1997}, {"title": "Delving deep into rectifiers: Surpassing human-level performance on imagenet classification", "author": ["He"], "venue": "In Proceedings of the IEEE International Conference on Computer Vision,", "citeRegEx": "He,? \\Q2015\\E", "shortCiteRegEx": "He", "year": 2015}, {"title": "R", "author": ["G.E. Hinton", "Salakhutdinov"], "venue": "R.", "citeRegEx": "Hinton and Salakhutdinov 2006", "shortCiteRegEx": null, "year": 2006}, {"title": "R", "author": ["G.E. Hinton", "Salakhutdinov"], "venue": "R.", "citeRegEx": "Hinton and Salakhutdinov 2009", "shortCiteRegEx": null, "year": 2009}, {"title": "G", "author": ["Hinton"], "venue": "E.; Osindero, S.; and Teh, Y.-W.", "citeRegEx": "Hinton. Osindero. and Teh 2006", "shortCiteRegEx": null, "year": 2006}, {"title": "G", "author": ["Hinton"], "venue": "E.", "citeRegEx": "Hinton 2002", "shortCiteRegEx": null, "year": 2002}, {"title": "L", "author": ["J. Honorio", "D. Samaras", "N. Paragios", "R. Goldstein", "Ortiz"], "venue": "E.", "citeRegEx": "Honorio et al. 2009", "shortCiteRegEx": null, "year": 2009}, {"title": "What is the best multi-stage architecture for object recognition", "author": ["Jarrett"], "venue": "In Computer Vision,", "citeRegEx": "Jarrett,? \\Q2009\\E", "shortCiteRegEx": "Jarrett", "year": 2009}, {"title": "N", "author": ["Johnson"], "venue": "L.; Kotz, S.; and Balakrishnan, N.", "citeRegEx": "Johnson. Kotz. and Balakrishnan 1994", "shortCiteRegEx": null, "year": 1994}, {"title": "A", "author": ["Y. Liu", "Willsky"], "venue": "2013. Learning gaussian graphical models with observed or latent fvss. In Advances in Neural Information Processing Systems, 1833\u2013", "citeRegEx": "Liu and Willsky 2013", "shortCiteRegEx": null, "year": 1841}, {"title": "N", "author": ["B.M. Marlin", "K. Swersky", "B. Chen", "Freitas"], "venue": "D.", "citeRegEx": "Marlin et al. 2010", "shortCiteRegEx": null, "year": 2010}, {"title": "Learning latent variable gaussian graphical models", "author": ["Eriksson Meng", "Z. Hero 2014] Meng", "B. Eriksson", "A. Hero"], "venue": null, "citeRegEx": "Meng et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Meng et al\\.", "year": 2014}, {"title": "G", "author": ["V. Nair", "Hinton"], "venue": "E.", "citeRegEx": "Nair and Hinton 2010", "shortCiteRegEx": null, "year": 2010}, {"title": "R", "author": ["Neal"], "venue": "M.", "citeRegEx": "Neal 1992", "shortCiteRegEx": null, "year": 1992}, {"title": "R", "author": ["Neal"], "venue": "M.", "citeRegEx": "Neal 2001", "shortCiteRegEx": null, "year": 2001}, {"title": "J", "author": ["J.H. Oh", "Deasy"], "venue": "O.", "citeRegEx": "Oh and Deasy 2014", "shortCiteRegEx": null, "year": 2014}, {"title": "Stochastic neural networks with monotonic activation functions", "author": ["Ravanbakhsh"], "venue": null, "citeRegEx": "Ravanbakhsh,? \\Q2016\\E", "shortCiteRegEx": "Ravanbakhsh", "year": 2016}, {"title": "C", "author": ["Robert"], "venue": "P.", "citeRegEx": "Robert 1995", "shortCiteRegEx": null, "year": 1995}, {"title": "G", "author": ["R. Salakhutdinov", "Hinton"], "venue": "E.", "citeRegEx": "Salakhutdinov and Hinton 2009", "shortCiteRegEx": null, "year": 2009}, {"title": "and Murray", "author": ["R. Salakhutdinov"], "venue": "I.", "citeRegEx": "Salakhutdinov and Murray 2008", "shortCiteRegEx": null, "year": 2008}, {"title": "Nonlinear statistical learning with truncated gaussian graphical models", "author": ["Su"], "venue": "In Proceedings of the 33st International Conference on Machine Learning (ICML-16)", "citeRegEx": "Su,? \\Q2016\\E", "shortCiteRegEx": "Su", "year": 2016}, {"title": "and Hinton", "author": ["T. Tieleman"], "venue": "G.", "citeRegEx": "Tieleman and Hinton 2012", "shortCiteRegEx": null, "year": 2012}, {"title": "G", "author": ["M. Welling", "M. RosenZvi", "Hinton"], "venue": "E.", "citeRegEx": "Welling. Rosen.Zvi. and Hinton 2004", "shortCiteRegEx": null, "year": 2004}], "referenceMentions": [], "year": 2017, "abstractText": "Gaussian graphical models (GGMs) are widely used for statistical modeling, because of ease of inference and the ubiquitous use of the normal distribution in practical approximations. However, they are also known for their limited modeling abilities, due to the Gaussian assumption. In this paper, we introduce a novel variant of GGMs, which relaxes the Gaussian restriction and yet admits efficient inference. Specifically, we impose a bipartite structure on the GGM and govern the hidden variables by truncated normal distributions. The nonlinearity of the model is revealed by its connection to rectified linear unit (ReLU) neural networks. Meanwhile, thanks to the bipartite structure and appealing properties of truncated normals, we are able to train the models efficiently using contrastive divergence. We consider three output constructs, accounting for real-valued, binary and count data. We further extend the model to deep structures and show that deep models can be used for unsupervised pre-training of rectifier neural networks. Extensive experimental results are provided to validate the proposed models and demonstrate their superiority over competing models.", "creator": "LaTeX with hyperref package"}}}