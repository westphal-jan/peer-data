{"id": "1611.04982", "review": {"conference": "ICML", "VERSION": "v1", "DATE_OF_SUBMISSION": "15-Nov-2016", "title": "Oracle Complexity of Second-Order Methods for Finite-Sum Problems", "abstract": "Finite-sum optimization problems are ubiquitous in machine learning, and are commonly solved using first-order methods which rely on gradient computations. Recently, there has been growing interest in \\emph{second-order} methods, which rely on both gradients and Hessians. In principle, second-order methods can require much fewer iterations than first-order methods, and hold the promise for more efficient algorithms. Although computing and manipulating Hessians is prohibitive for high-dimensional problems in general, the Hessians of individual functions in finite-sum problems can often be efficiently computed, e.g. because they possess a low-rank structure. Can second-order information indeed be used to solve such problems more efficiently? In this paper, we provide evidence that the answer -- perhaps surprisingly -- is negative, at least in terms of worst-case guarantees. However, we also discuss what additional assumptions and algorithmic approaches might potentially circumvent this negative result.", "histories": [["v1", "Tue, 15 Nov 2016 18:41:55 GMT  (24kb)", "http://arxiv.org/abs/1611.04982v1", "23 pages"], ["v2", "Wed, 8 Mar 2017 11:05:59 GMT  (29kb)", "http://arxiv.org/abs/1611.04982v2", "30 pages"]], "COMMENTS": "23 pages", "reviews": [], "SUBJECTS": "math.OC cs.LG stat.ML", "authors": ["yossi arjevani", "ohad shamir"], "accepted": true, "id": "1611.04982"}, "pdf": {"name": "1611.04982.pdf", "metadata": {"source": "CRF", "title": "Oracle Complexity of Second-Order Methods for Finite-Sum Problems", "authors": ["Yossi Arjevani"], "emails": [], "sections": [{"heading": null, "text": "ar Xiv: 161 1.04 982v 1 [mat h.O C] 15 N"}, {"heading": "1 Introduction", "text": "We consider the finite sum problems of formalmin w'WF (w) = 1nn (w) = 1fi (w), (1) where W's is a closed, convex subset of any Euclidean or Hilbert space, any kind of problem is often considered in the complexity of this and other optimization problems, where the optimization algorithms have no objective information about the objective function and receive information from an oracle that provides values and derivatives of the function. [Nemirovsky and Yudin, 1983] The complexity of the algorithm is measured in terms of the number of oracle calls required to optimize the function."}, {"heading": "2 Strongly Convex and Smooth Optimization with a Second-Order Oracle", "text": "Before presenting our main results for finite sum optimization, let us consider the simpler problem of minimizing a single, strongly convex and smooth function F (or equivalent, Eq). (1) If we obtain a result that may be of independent interest, we follow a standard oracle, and assume that the algorithm has no basic information about the objective function F (w) except the strongly convex parameters and the smoothness of the parameter \u00b5. Instead, it has access to an oracle that gives a point w (W), the values and derivatives of F (w) for a first order oracle, or F (w) for a second order oracle."}, {"heading": "3 Second-Order Oracle Complexity Bounds for Finite-Sum Problems", "text": "This year it is so far that it is only a matter of time before it is so far, until it is so far."}, {"heading": "4 Comparison to Existing Approaches", "text": "In fact, it is the case that most of us are able to abide by the rules that they have imposed on themselves. (...) In fact, it is the case that they are able to abide by the rules. (...) In fact, it is the case that they are able to abide by the rules. (...) In fact, it is the case that they are able to abide by the rules. (...) In fact, it is the case that they are able to abide by the rules. \"(...) It is as if they are abiding by the rules.\" (...)"}, {"heading": "5 Summary and Discussion", "text": "This is in contrast to most existing oracle complexity results, which focus on a first order oracle. First, we have formally proven that in the default setting of strongly convex and smooth optimization problems, which show that second order information does not significantly improve the complexity of the oracle, and further assumptions (i.e. Hessian lipschitzness) are actually necessary, we have then outlined our most important lower limits, which show that for second order finite sum problems, the second order oracle, under some reasonable algorithmic assumptions, is the resulting oracle of complexity - again not significantly better than what can be achieved with a first order oracle. Furthermore, this is demonstrated by square functions larger than 2 derivatives of the order."}, {"heading": "Acknowledgments", "text": "This research is supported in part by a Marie Curie CIG Scholarship under the Seventh Research Framework Programme and a scholarship from the Israel Science Foundation 425 / 13."}, {"heading": "A Proofs", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "A.1 Auxiliary Lemmas", "text": "The following Lemma was essentially recorded in Lan [2015], Nesterov [2013], but we provide a proof of completeness: Lemma 1. \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7"}, {"heading": "A.2 Proof of Thm. 1", "text": "The proof is inspired by a technique introduced in Woodworth and Srebro (2016) for the analysis of first-order randomised methods, in which a square function is \"locally flattened\" to render first-order information (gradient) non-informative. We use a similar technique to create second-order information (Hessian), thereby preventing second-order methods from having an advantage over first-order methods. We first choose another dimension d: 2T. We then define another way in which we apply a (deterministic) algorithm and a bound T to the number of oracle calls to construct function F in the following way. We then choose another dimension d: 2T."}, {"heading": "A.3 Proof of Thm. 2", "text": "We will define a random selection of quadratic functions f1,..., fn and demonstrate a lower limit on the expected optimization error of an algorithm (the expectation being both higher than the algorithm and higher than the random functions). Consequently, for each algorithm the same lower limit (only in expectation of the algorithm) applies to a deterministic choice of f1...., fn.There will indeed be two separate constructions, one leading to a lower limit of \u0432 (n) and one leading to a lower limit of \u0432 (\u221a n\u00b5)."}, {"heading": "A.3.1 An \u2126(n) Lower Bound", "text": "Starting from the lower limit, we leave the probability that the probability that the probability that the probability that the probability that the probability that the probability that the probability that the probability that the probability that the probability that the probability that the probability that the probability that the probability that the probability that the probability that the probability that the probability that the probability that the probability that the probability that the probability that the probability that the probability that the probability that the probability that the probability that the probability that the probability that the probability that the probability that the probability that the probability that the probability that the probability that the probability that the probability that the probability that the probability that the probability that the probability that the probability that the probability that the probability that the probability that the probability that the probability that the probability that the probability that the probability that the probability that the probability that the probability that the probability that the probability that the probability that the probability that the probability that the probability that the probability that the probability that the probability that the probability that the probability that the probability that the probability that the probability that the probability that the probability that the probability that the probability that the probability that the probability that the probability that the probability that the probability that the probability that the probability that the probability that the probability that the probability that the probability that the probability that the probability that the probability that the probability that the probability that the probability that the probability that the probability that the probability that the probability that the probability that the probability that the probability that the probability that the probability that the probability that the probability that the probability that the probability that the probability that the probability that the probability that the probability that the probability that the probability that the probability that the probability that the probability that the probability that the probability that the probability that the probability that the probability that the probability that the probability that the probability that the probability that the probability that the probability that the probability that the probability that the probability that the"}], "references": [{"title": "A lower bound for the optimization of finite sums", "author": ["Alekh Agarwal", "Leon Bottou"], "venue": "arXiv preprint arXiv:1410.0723,", "citeRegEx": "Agarwal and Bottou.,? \\Q2014\\E", "shortCiteRegEx": "Agarwal and Bottou.", "year": 2014}, {"title": "Second order stochastic optimization in linear time", "author": ["Naman Agarwal", "Brian Bullins", "Elad Hazan"], "venue": "arXiv preprint arXiv:1602.03943,", "citeRegEx": "Agarwal et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Agarwal et al\\.", "year": 2016}, {"title": "Communication complexity of distributed convex learning and optimization", "author": ["Yossi Arjevani", "Ohad Shamir"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Arjevani and Shamir.,? \\Q2015\\E", "shortCiteRegEx": "Arjevani and Shamir.", "year": 2015}, {"title": "Dimension-free iteration complexity of finite sum optimization problems", "author": ["Yossi Arjevani", "Ohad Shamir"], "venue": "arXiv preprint arXiv:1606.09333,", "citeRegEx": "Arjevani and Shamir.,? \\Q2016\\E", "shortCiteRegEx": "Arjevani and Shamir.", "year": 2016}, {"title": "On the iteration complexity of oblivious first-order optimization algorithms", "author": ["Yossi Arjevani", "Ohad Shamir"], "venue": "arXiv preprint arXiv:1605.03529,", "citeRegEx": "Arjevani and Shamir.,? \\Q2016\\E", "shortCiteRegEx": "Arjevani and Shamir.", "year": 2016}, {"title": "Exact and inexact subsampled newton methods for optimization", "author": ["Raghu Bollapragada", "Richard Byrd", "Jorge Nocedal"], "venue": "arXiv preprint arXiv:1609.08502,", "citeRegEx": "Bollapragada et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Bollapragada et al\\.", "year": 2016}, {"title": "Convergence rates of sub-sampled newton methods", "author": ["Murat A Erdogdu", "Andrea Montanari"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Erdogdu and Montanari.,? \\Q2015\\E", "shortCiteRegEx": "Erdogdu and Montanari.", "year": 2015}, {"title": "An optimal randomized incremental gradient method", "author": ["Guanghui Lan"], "venue": "arXiv preprint arXiv:1507.02000,", "citeRegEx": "Lan.,? \\Q2015\\E", "shortCiteRegEx": "Lan.", "year": 2015}, {"title": "Problem Complexity and Method Efficiency in Optimization", "author": ["A. Nemirovsky", "D. Yudin"], "venue": "WileyInterscience,", "citeRegEx": "Nemirovsky and Yudin.,? \\Q1983\\E", "shortCiteRegEx": "Nemirovsky and Yudin.", "year": 1983}, {"title": "Introductory lectures on convex optimization: A basic course, volume 87", "author": ["Yurii Nesterov"], "venue": "Springer Science & Business Media,", "citeRegEx": "Nesterov.,? \\Q2013\\E", "shortCiteRegEx": "Nesterov.", "year": 2013}, {"title": "Newton sketch: A linear-time optimization algorithm with linearquadratic convergence", "author": ["Mert Pilanci", "Martin J Wainwright"], "venue": "arXiv preprint arXiv:1505.02250,", "citeRegEx": "Pilanci and Wainwright.,? \\Q2015\\E", "shortCiteRegEx": "Pilanci and Wainwright.", "year": 2015}, {"title": "Sub-sampled newton methods i: globally convergent algorithms", "author": ["Farbod Roosta-Khorasani", "Michael W Mahoney"], "venue": "arXiv preprint arXiv:1601.04737,", "citeRegEx": "Roosta.Khorasani and Mahoney.,? \\Q2016\\E", "shortCiteRegEx": "Roosta.Khorasani and Mahoney.", "year": 2016}, {"title": "Sub-sampled newton methods ii: Local convergence rates", "author": ["Farbod Roosta-Khorasani", "Michael W Mahoney"], "venue": "arXiv preprint arXiv:1601.04738,", "citeRegEx": "Roosta.Khorasani and Mahoney.,? \\Q2016\\E", "shortCiteRegEx": "Roosta.Khorasani and Mahoney.", "year": 2016}, {"title": "Tight complexity bounds for optimizing composite objectives", "author": ["Blake Woodworth", "Nathan Srebro"], "venue": "arXiv preprint arXiv:1605.08003,", "citeRegEx": "Woodworth and Srebro.,? \\Q2016\\E", "shortCiteRegEx": "Woodworth and Srebro.", "year": 2016}, {"title": "Sub-sampled newton methods with non-uniform sampling", "author": ["Peng Xu", "Jiyan Yang", "Farbod Roosta-Khorasani", "Christopher R\u00e9", "Michael W Mahoney"], "venue": "arXiv preprint arXiv:1607.00559,", "citeRegEx": "Xu et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Xu et al\\.", "year": 2016}], "referenceMentions": [{"referenceID": 8, "context": "To study the complexity of this and other optimization problems, it is common to consider an oracle model, where the optimization algorithm has no a-priori information about the objective function, and obtains information from an oracle which provides values and derivatives of the function at various domain points [Nemirovsky and Yudin, 1983].", "startOffset": 316, "endOffset": 344}, {"referenceID": 0, "context": "either under algorithmic assumptions or assuming the dimension is sufficiently large2 [Agarwal and Bottou, 2014, Lan, 2015, Woodworth and Srebro, 2016, Arjevani and Shamir, 2016a]. This is matched (up to log factors) by existing approaches, and cannot be improved in general. An alternative to first-order methods are second-order methods, which also utilize Hessian information. A prototypical example is the Newton method, which given a (single) function F , performs iterations of the form wt+1 = wt \u2212 \u03b1t ( \u2207F (w) )\u2207F (w), (2) where \u2207F (w),\u22072F (w) are the gradient and the Hessian of F at w, and \u03b1t is a step size parameter. Secondorder methods can have extremely fast convergence, better than those of first-order methods (i.e. quadratic instead of linear). Moreover, they can be invariant to affine transformations of the objective function, and provably independent of its strong convexity and smoothness parameters (assuming e.g. self-concordance) [Boyd and Vandenberghe, 2004]. A drawback of these methods, however, is that they can be computationally prohibitive. In the context of machine learning, we are often interested in high-dimensional problems (where the dimension d is very large), and the Hessians are d\u00d7 d matrices which in general may not even fit into computer memory. However, for optimization problems as in Eq. (1), the Hessians of individual fi often have a special structure. For example, a very common special case of finite-sum problems in machine learning is empirical risk minimization for linear predictors, where fi(w) = li(\u3008w,xi\u3009), where xi is a training instance and li is some loss function. In that case, assuming li is twice-differentiable, the Hessian has the rank-1 form l\u2032\u2032 i (\u3008w,xi\u3009)xixi . Therefore, the memory and computational effort involved with storing and manipulating the Hessian of this function is merely linear (rather than quadratic) in d. Thus, it is tractable even for high-dimensional problems. Building on this, several recent papers proposed and analyzed second-order methods for finite-sum problems, which utilize Hessians of the individual functions fi (see for instance Erdogdu and Montanari [2015], Agarwal et al.", "startOffset": 87, "endOffset": 2162}, {"referenceID": 0, "context": "either under algorithmic assumptions or assuming the dimension is sufficiently large2 [Agarwal and Bottou, 2014, Lan, 2015, Woodworth and Srebro, 2016, Arjevani and Shamir, 2016a]. This is matched (up to log factors) by existing approaches, and cannot be improved in general. An alternative to first-order methods are second-order methods, which also utilize Hessian information. A prototypical example is the Newton method, which given a (single) function F , performs iterations of the form wt+1 = wt \u2212 \u03b1t ( \u2207F (w) )\u2207F (w), (2) where \u2207F (w),\u22072F (w) are the gradient and the Hessian of F at w, and \u03b1t is a step size parameter. Secondorder methods can have extremely fast convergence, better than those of first-order methods (i.e. quadratic instead of linear). Moreover, they can be invariant to affine transformations of the objective function, and provably independent of its strong convexity and smoothness parameters (assuming e.g. self-concordance) [Boyd and Vandenberghe, 2004]. A drawback of these methods, however, is that they can be computationally prohibitive. In the context of machine learning, we are often interested in high-dimensional problems (where the dimension d is very large), and the Hessians are d\u00d7 d matrices which in general may not even fit into computer memory. However, for optimization problems as in Eq. (1), the Hessians of individual fi often have a special structure. For example, a very common special case of finite-sum problems in machine learning is empirical risk minimization for linear predictors, where fi(w) = li(\u3008w,xi\u3009), where xi is a training instance and li is some loss function. In that case, assuming li is twice-differentiable, the Hessian has the rank-1 form l\u2032\u2032 i (\u3008w,xi\u3009)xixi . Therefore, the memory and computational effort involved with storing and manipulating the Hessian of this function is merely linear (rather than quadratic) in d. Thus, it is tractable even for high-dimensional problems. Building on this, several recent papers proposed and analyzed second-order methods for finite-sum problems, which utilize Hessians of the individual functions fi (see for instance Erdogdu and Montanari [2015], Agarwal et al. [2016], Pilanci and Wainwright [2015], Roosta-Khorasani and Mahoney [2016a,b], Bollapragada et al.", "startOffset": 87, "endOffset": 2185}, {"referenceID": 0, "context": "either under algorithmic assumptions or assuming the dimension is sufficiently large2 [Agarwal and Bottou, 2014, Lan, 2015, Woodworth and Srebro, 2016, Arjevani and Shamir, 2016a]. This is matched (up to log factors) by existing approaches, and cannot be improved in general. An alternative to first-order methods are second-order methods, which also utilize Hessian information. A prototypical example is the Newton method, which given a (single) function F , performs iterations of the form wt+1 = wt \u2212 \u03b1t ( \u2207F (w) )\u2207F (w), (2) where \u2207F (w),\u22072F (w) are the gradient and the Hessian of F at w, and \u03b1t is a step size parameter. Secondorder methods can have extremely fast convergence, better than those of first-order methods (i.e. quadratic instead of linear). Moreover, they can be invariant to affine transformations of the objective function, and provably independent of its strong convexity and smoothness parameters (assuming e.g. self-concordance) [Boyd and Vandenberghe, 2004]. A drawback of these methods, however, is that they can be computationally prohibitive. In the context of machine learning, we are often interested in high-dimensional problems (where the dimension d is very large), and the Hessians are d\u00d7 d matrices which in general may not even fit into computer memory. However, for optimization problems as in Eq. (1), the Hessians of individual fi often have a special structure. For example, a very common special case of finite-sum problems in machine learning is empirical risk minimization for linear predictors, where fi(w) = li(\u3008w,xi\u3009), where xi is a training instance and li is some loss function. In that case, assuming li is twice-differentiable, the Hessian has the rank-1 form l\u2032\u2032 i (\u3008w,xi\u3009)xixi . Therefore, the memory and computational effort involved with storing and manipulating the Hessian of this function is merely linear (rather than quadratic) in d. Thus, it is tractable even for high-dimensional problems. Building on this, several recent papers proposed and analyzed second-order methods for finite-sum problems, which utilize Hessians of the individual functions fi (see for instance Erdogdu and Montanari [2015], Agarwal et al. [2016], Pilanci and Wainwright [2015], Roosta-Khorasani and Mahoney [2016a,b], Bollapragada et al.", "startOffset": 87, "endOffset": 2216}, {"referenceID": 0, "context": "either under algorithmic assumptions or assuming the dimension is sufficiently large2 [Agarwal and Bottou, 2014, Lan, 2015, Woodworth and Srebro, 2016, Arjevani and Shamir, 2016a]. This is matched (up to log factors) by existing approaches, and cannot be improved in general. An alternative to first-order methods are second-order methods, which also utilize Hessian information. A prototypical example is the Newton method, which given a (single) function F , performs iterations of the form wt+1 = wt \u2212 \u03b1t ( \u2207F (w) )\u2207F (w), (2) where \u2207F (w),\u22072F (w) are the gradient and the Hessian of F at w, and \u03b1t is a step size parameter. Secondorder methods can have extremely fast convergence, better than those of first-order methods (i.e. quadratic instead of linear). Moreover, they can be invariant to affine transformations of the objective function, and provably independent of its strong convexity and smoothness parameters (assuming e.g. self-concordance) [Boyd and Vandenberghe, 2004]. A drawback of these methods, however, is that they can be computationally prohibitive. In the context of machine learning, we are often interested in high-dimensional problems (where the dimension d is very large), and the Hessians are d\u00d7 d matrices which in general may not even fit into computer memory. However, for optimization problems as in Eq. (1), the Hessians of individual fi often have a special structure. For example, a very common special case of finite-sum problems in machine learning is empirical risk minimization for linear predictors, where fi(w) = li(\u3008w,xi\u3009), where xi is a training instance and li is some loss function. In that case, assuming li is twice-differentiable, the Hessian has the rank-1 form l\u2032\u2032 i (\u3008w,xi\u3009)xixi . Therefore, the memory and computational effort involved with storing and manipulating the Hessian of this function is merely linear (rather than quadratic) in d. Thus, it is tractable even for high-dimensional problems. Building on this, several recent papers proposed and analyzed second-order methods for finite-sum problems, which utilize Hessians of the individual functions fi (see for instance Erdogdu and Montanari [2015], Agarwal et al. [2016], Pilanci and Wainwright [2015], Roosta-Khorasani and Mahoney [2016a,b], Bollapragada et al. [2016], Xu et al.", "startOffset": 87, "endOffset": 2284}, {"referenceID": 0, "context": "either under algorithmic assumptions or assuming the dimension is sufficiently large2 [Agarwal and Bottou, 2014, Lan, 2015, Woodworth and Srebro, 2016, Arjevani and Shamir, 2016a]. This is matched (up to log factors) by existing approaches, and cannot be improved in general. An alternative to first-order methods are second-order methods, which also utilize Hessian information. A prototypical example is the Newton method, which given a (single) function F , performs iterations of the form wt+1 = wt \u2212 \u03b1t ( \u2207F (w) )\u2207F (w), (2) where \u2207F (w),\u22072F (w) are the gradient and the Hessian of F at w, and \u03b1t is a step size parameter. Secondorder methods can have extremely fast convergence, better than those of first-order methods (i.e. quadratic instead of linear). Moreover, they can be invariant to affine transformations of the objective function, and provably independent of its strong convexity and smoothness parameters (assuming e.g. self-concordance) [Boyd and Vandenberghe, 2004]. A drawback of these methods, however, is that they can be computationally prohibitive. In the context of machine learning, we are often interested in high-dimensional problems (where the dimension d is very large), and the Hessians are d\u00d7 d matrices which in general may not even fit into computer memory. However, for optimization problems as in Eq. (1), the Hessians of individual fi often have a special structure. For example, a very common special case of finite-sum problems in machine learning is empirical risk minimization for linear predictors, where fi(w) = li(\u3008w,xi\u3009), where xi is a training instance and li is some loss function. In that case, assuming li is twice-differentiable, the Hessian has the rank-1 form l\u2032\u2032 i (\u3008w,xi\u3009)xixi . Therefore, the memory and computational effort involved with storing and manipulating the Hessian of this function is merely linear (rather than quadratic) in d. Thus, it is tractable even for high-dimensional problems. Building on this, several recent papers proposed and analyzed second-order methods for finite-sum problems, which utilize Hessians of the individual functions fi (see for instance Erdogdu and Montanari [2015], Agarwal et al. [2016], Pilanci and Wainwright [2015], Roosta-Khorasani and Mahoney [2016a,b], Bollapragada et al. [2016], Xu et al. [2016] and references therein).", "startOffset": 87, "endOffset": 2302}, {"referenceID": 8, "context": "Given a first-order oracle and a strongly convex and smooth objective in sufficiently high dimensions, it is well-known that the worst-case oracle complexity is \u03a9( \u221a \u03bc/\u03bb \u00b7 log(1/\u01eb)) [Nemirovsky and Yudin, 1983].", "startOffset": 182, "endOffset": 210}, {"referenceID": 1, "context": "Agarwal et al. [2016], Xu et al.", "startOffset": 0, "endOffset": 22}, {"referenceID": 1, "context": "Agarwal et al. [2016], Xu et al. [2016]. We note that the limitations arising from oblivious optimization schemes (in a somewhat stronger sense) was also explored in Arjevani and Shamir [2016a,b].", "startOffset": 0, "endOffset": 40}, {"referenceID": 8, "context": "As mentioned earlier, the observation that such first-order oracle bounds can be extended to higherorder oracles is also briefly mentioned (without proof) in Nemirovsky and Yudin [1983, Section 7.2.6]. Also, the theorem considers deterministic algorithms (which includes standard second-order methods, such as the Newton method), but otherwise makes no assumption on the algorithm. Generalizing this result to randomized algorithms should be quite doable, based on the techniques developed in Woodworth and Srebro [2016]. We leave a formal derivation to future work.", "startOffset": 158, "endOffset": 521}, {"referenceID": 5, "context": "Nesterov [2013], Lan [2015], as well as Arjevani and Shamir [2015] in a somewhat different context).", "startOffset": 0, "endOffset": 16}, {"referenceID": 4, "context": "Nesterov [2013], Lan [2015], as well as Arjevani and Shamir [2015] in a somewhat different context).", "startOffset": 17, "endOffset": 28}, {"referenceID": 2, "context": "Nesterov [2013], Lan [2015], as well as Arjevani and Shamir [2015] in a somewhat different context).", "startOffset": 40, "endOffset": 67}, {"referenceID": 10, "context": "A possible exception to this is the Newton sketch algorithm [Pilanci and Wainwright, 2015], which relies on random projections, but on the flip side is computationally expensive.", "startOffset": 60, "endOffset": 90}, {"referenceID": 5, "context": "There have been several rigorous studies of such \u201csubsampled Newton\u201d methods, such as Erdogdu and Montanari [2015], Roosta-Khorasani and Mahoney [2016a,b], Bollapragada et al.", "startOffset": 86, "endOffset": 115}, {"referenceID": 5, "context": "There have been several rigorous studies of such \u201csubsampled Newton\u201d methods, such as Erdogdu and Montanari [2015], Roosta-Khorasani and Mahoney [2016a,b], Bollapragada et al. [2016] and references therein.", "startOffset": 156, "endOffset": 183}, {"referenceID": 5, "context": "There have been several rigorous studies of such \u201csubsampled Newton\u201d methods, such as Erdogdu and Montanari [2015], Roosta-Khorasani and Mahoney [2016a,b], Bollapragada et al. [2016] and references therein. However, our lower bound in Thm. 2 holds for such an approach, since it satisfies both Assumption 1 and 2. As expected, the existing worst-case complexity upper bounds are no better than our lower bound. Xu et al. [2016] recently proposed a subsampled Newton method, together with non-uniform sampling, which assigns more weight to individual functions which are deemed more \u201cimportant\u201d.", "startOffset": 156, "endOffset": 428}, {"referenceID": 5, "context": "There have been several rigorous studies of such \u201csubsampled Newton\u201d methods, such as Erdogdu and Montanari [2015], Roosta-Khorasani and Mahoney [2016a,b], Bollapragada et al. [2016] and references therein. However, our lower bound in Thm. 2 holds for such an approach, since it satisfies both Assumption 1 and 2. As expected, the existing worst-case complexity upper bounds are no better than our lower bound. Xu et al. [2016] recently proposed a subsampled Newton method, together with non-uniform sampling, which assigns more weight to individual functions which are deemed more \u201cimportant\u201d. This is measured via properties of the Hessians of the functions, such as their norms or via leverage scores. This approach breaks Assumption 1, as the sampled indices are now chosen in a way dependent on the individual functions, but since this dependence is only through the Hessians, it still satisfies Assumption 1a. Therefore, our lower bound as stated in Thm. 3 still applies to such a method. A variant of the subsampled Newton approach, studied in Erdogdu and Montanari [2015], uses a lowrank approximation of the sample Hessian (attained by truncated SVD), in lieu of the sample Hessian itself.", "startOffset": 156, "endOffset": 1080}, {"referenceID": 5, "context": "There have been several rigorous studies of such \u201csubsampled Newton\u201d methods, such as Erdogdu and Montanari [2015], Roosta-Khorasani and Mahoney [2016a,b], Bollapragada et al. [2016] and references therein. However, our lower bound in Thm. 2 holds for such an approach, since it satisfies both Assumption 1 and 2. As expected, the existing worst-case complexity upper bounds are no better than our lower bound. Xu et al. [2016] recently proposed a subsampled Newton method, together with non-uniform sampling, which assigns more weight to individual functions which are deemed more \u201cimportant\u201d. This is measured via properties of the Hessians of the functions, such as their norms or via leverage scores. This approach breaks Assumption 1, as the sampled indices are now chosen in a way dependent on the individual functions, but since this dependence is only through the Hessians, it still satisfies Assumption 1a. Therefore, our lower bound as stated in Thm. 3 still applies to such a method. A variant of the subsampled Newton approach, studied in Erdogdu and Montanari [2015], uses a lowrank approximation of the sample Hessian (attained by truncated SVD), in lieu of the sample Hessian itself. However, this still falls in the framework of Assumption 2, and our lower bound still applies. A different approach to approximate the full Hessian is via randomized sketching techniques, which replace the Hessian \u22072F (w) by a low-rank approximation of the form (\u22072F (w))1/2SS\u22a4(\u22072F (w))1/2, where S \u2208 Rd\u00d7m,m \u226a d is a random sketching matrix, and (\u22072F (w))1/2 is the matrix square root of \u22072F (w). This approach forms the basis of the Newton sketch algorithm proposed in Pilanci and Wainwright [2015]. This approach currently escapes our lower bound, since it violates Assumption 2.", "startOffset": 156, "endOffset": 1699}, {"referenceID": 1, "context": "Agarwal et al. [2016] develop another line of stochastic second-order methods, which are based on the observation that the Newton step (\u22072F (w))\u22121\u2207F (w) is the solution of the system of linear equations \u22072F (w)x = \u2207F (w).", "startOffset": 0, "endOffset": 22}, {"referenceID": 1, "context": "That being said, it is important to note that the complexity upper bound attained in Agarwal et al. [2016] for LiSSA-Sample is on the order of \u00d5((n+ \u221a d\u03bc/\u03bb) log(1/\u01eb)) (asymptotically as \u01eb \u2192 0), which is better than our lower bound if d \u226a n.", "startOffset": 85, "endOffset": 107}, {"referenceID": 1, "context": "Based on the fact that only at most d \u2212 1 out of n functions are relevant in the construction, we conjecture that the possible improvement in the oracle complexity of such schemes may amount to replacing dependencies on n with dependencies on d, which is indeed the type of improvement attained (for small enough \u01eb) in Agarwal et al. [2016]. 9", "startOffset": 319, "endOffset": 341}, {"referenceID": 1, "context": "Finally, we note that Agarwal et al. [2016] proposes another algorithm tailored to self-concordant functions, with runtime independent of the smoothness and strong convexity parameters of the problem.", "startOffset": 22, "endOffset": 44}, {"referenceID": 1, "context": "Finally, we note that Agarwal et al. [2016] proposes another algorithm tailored to self-concordant functions, with runtime independent of the smoothness and strong convexity parameters of the problem. However, it requires performing \u2265 1 full Newton steps, so the runtime is prohibitive for large-scale problems (indeed, for quadratics as used in our lower bounds, even a single Newton step suffices to compute an exact solution). 5 Summary and Discussion In this paper, we studied the oracle complexity for optimization problems, assuming availability of a secondorder oracle. This is in contrast to most existing oracle complexity results, which focus on a first-order oracle. First, we formally proved that in the standard setting of strongly-convex and smooth optimization problems, second-order information does not significantly improve the oracle complexity, and further assumptions (i.e. Lipschitzness of the Hessians) are in fact necessary. We then presented our main lower bounds, which show that for finite-sum problems with a second-order oracle, under some reasonable algorithmic assumptions, the resulting oracle complexity is \u2013 again \u2013 not significantly better than what can be obtained using a first-order oracle. Moreover, this is shown using quadratic functions, which have 0 derivatives of order larger than 2. Hence, our lower bounds apply even if we have access to an oracle returning derivatives of order p for all p \u2265 0, and the function is smooth to any order. In Sec. 4, we studied how our framework and lower bounds are applicable to most existing approaches. Although this conclusion may appear very pessimistic, they are actually useful in pinpointing potential assumptions and approaches which may circumvent these lower bounds. In particular: \u2022 Our lower bound for algorithms employing non index-oblivious sampling schemes (Thm. 3) only hold when the dimension d is very large. This leaves open the possibility of better (non index-oblivious) algorithms when d is moderate, as was recently demonstrated in the context of the LiSSA-Sample algorithm of Agarwal et al. [2016] (at least for small enough \u01eb).", "startOffset": 22, "endOffset": 2102}, {"referenceID": 5, "context": "Bollapragada et al. [2016], Xu et al.", "startOffset": 0, "endOffset": 27}, {"referenceID": 5, "context": "Bollapragada et al. [2016], Xu et al. [2016], Roosta-Khorasani and Mahoney [2016a], Pilanci and Wainwright [2015]) show that various algorithms can have an initial super-linear convergence rate, which slows down to linear as one gets closer to the optimum.", "startOffset": 0, "endOffset": 45}, {"referenceID": 5, "context": "Bollapragada et al. [2016], Xu et al. [2016], Roosta-Khorasani and Mahoney [2016a], Pilanci and Wainwright [2015]) show that various algorithms can have an initial super-linear convergence rate, which slows down to linear as one gets closer to the optimum.", "startOffset": 0, "endOffset": 83}, {"referenceID": 5, "context": "Bollapragada et al. [2016], Xu et al. [2016], Roosta-Khorasani and Mahoney [2016a], Pilanci and Wainwright [2015]) show that various algorithms can have an initial super-linear convergence rate, which slows down to linear as one gets closer to the optimum.", "startOffset": 0, "endOffset": 114}, {"referenceID": 7, "context": "1 Auxiliary Lemmas The following lemma was essentially proven in Lan [2015], Nesterov [2013], but we provide a proof for completeness: Lemma 1.", "startOffset": 65, "endOffset": 76}, {"referenceID": 7, "context": "1 Auxiliary Lemmas The following lemma was essentially proven in Lan [2015], Nesterov [2013], but we provide a proof for completeness: Lemma 1.", "startOffset": 65, "endOffset": 93}, {"referenceID": 13, "context": "1 The proof is inspired by a technique introduced in Woodworth and Srebro [2016] for analyzing randomized first-order methods, in which a quadratic function is \u201clocally flattened\u201d in order to make first-order (gradient) information non-informative.", "startOffset": 53, "endOffset": 81}], "year": 2017, "abstractText": "Finite-sum optimization problems are ubiquitous in machine learning, and are commonly solved using first-order methods which rely on gradient computations. Recently, there has been growing interest in second-order methods, which rely on both gradients and Hessians. In principle, second-order methods can require much fewer iterations than first-order methods, and hold the promise for more efficient algorithms. Although computing and manipulating Hessians is prohibitive for high-dimensional problems in general, the Hessians of individual functions in finite-sum problems can often be efficiently computed, e.g. because they possess a low-rank structure. Can second-order information indeed be used to solve such problems more efficiently? In this paper, we provide evidence that the answer \u2013 perhaps surprisingly \u2013 is negative, at least in terms of worst-case guarantees. However, we also discuss what additional assumptions and algorithmic approaches might potentially circumvent this negative result.", "creator": "LaTeX with hyperref package"}}}