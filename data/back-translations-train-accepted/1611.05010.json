{"id": "1611.05010", "review": {"conference": "NIPS", "VERSION": "v1", "DATE_OF_SUBMISSION": "15-Nov-2016", "title": "Anchor-Free Correlated Topic Modeling: Identifiability and Algorithm", "abstract": "In topic modeling, many algorithms that guarantee identifiability of the topics have been developed under the premise that there exist anchor words -- i.e., words that only appear (with positive probability) in one topic. Follow-up work has resorted to three or higher-order statistics of the data corpus to relax the anchor word assumption. Reliable estimates of higher-order statistics are hard to obtain, however, and the identification of topics under those models hinges on uncorrelatedness of the topics, which can be unrealistic. This paper revisits topic modeling based on second-order moments, and proposes an anchor-free topic mining framework. The proposed approach guarantees the identification of the topics under a much milder condition compared to the anchor-word assumption, thereby exhibiting much better robustness in practice. The associated algorithm only involves one eigen-decomposition and a few small linear programs. This makes it easy to implement and scale up to very large problem instances. Experiments using the TDT2 and Reuters-21578 corpus demonstrate that the proposed anchor-free approach exhibits very favorable performance (measured using coherence, similarity count, and clustering accuracy metrics) compared to the prior art.", "histories": [["v1", "Tue, 15 Nov 2016 20:06:40 GMT  (60kb)", "http://arxiv.org/abs/1611.05010v1", null]], "reviews": [], "SUBJECTS": "stat.ML cs.CL cs.IR cs.SI", "authors": ["kejun huang", "xiao fu", "nikos d sidiropoulos"], "accepted": true, "id": "1611.05010"}, "pdf": {"name": "1611.05010.pdf", "metadata": {"source": "CRF", "title": "Anchor-Free Correlated Topic Modeling: Identifiability and Algorithm", "authors": ["Kejun Huang", "Xiao Fu", "Nicholas D. Sidiropoulos"], "emails": ["huang663@umn.edu", "xfu@umn.edu", "nikos@ece.umn.edu"], "sections": [{"heading": null, "text": "ar Xiv: 161 1.05 010v 1 [stat.ML] 1 5"}, {"heading": "1 Introduction", "text": "This year, it will only take one year for an agreement to be reached."}, {"heading": "2 Background", "text": "D D D D D (D D D D D D D (D D D D D) D (D D D D D) D (D D D D) D (D D D D) D (D D D D D) D (D D D D D) D (D D D D D) D (D D D D D) D (D D D D D) D (D D D D D D) D (D D D D D D) D (D D D D) D (D D D D D D D) D (D D D D D D) D (D D D D D D D D) D (D D D D D D D D D) D (D D D D D D D D) D (D D D D D D D D D D D D D D D D D D) D D D D D (D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D) D (D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D (D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D"}, {"heading": "3 Anchor-Free Identifiable Topic Mining", "text": "In this paper we are particularly interested in mining systems from the matrix P due to their robustness and scalability. We will formulate the topic modeling as an optimization problem and show that the word-theme matrix C can be identified under much more relaxed conditions, including the relatively strict anchor word assumption as a special case."}, {"heading": "3.1 Problem Formulation", "text": "We begin with the models P = CECT = CECT = 1: 1. It is the only solution we can imagine. (\"It is the only solution we can imagine.\"). (\"It is the only solution we can imagine.\"). (\"It is the only solution we can imagine.\"). (\"It is the only solution we can imagine.\"). (\"It is the only solution we can imagine.\"). (\"It is the only solution we can imagine.\" \"(\" It is the only solution we can imagine. \"). (\" It is the only solution we can imagine. \"). (\"). (\"It is the only solution we can imagine.\"). (\"It is the only solution we can imagine.\"). (\"It is the only solution we can imagine.\"). (\"It is the only solution we can imagine.\" (\"It is the only solution we can imagine.\"). (\"It is the only solution we can imagine.\"). \"(\" It is the only solution we can imagine. \").\" (\"It is the only solution we can imagine.\"). \"(\" (\"It is the only solution we can imagine.\"). \"(\"). \"(\" (\"It is the only solution we can imagine.\"). \"(\" (\").\" (\"It is the only solution we can imagine.\"). (\"(\"). \"(\" It is the only solution we can imagine. \"(\"). (\"(\"). (\"It is the only solution we can imagine. (\"). (\"). (\""}, {"heading": "3.2 AnchorFree: A Simple and Scalable Algorithm", "text": "The question that arises is whether the term CECT tri-linear and not easily decoupled the problem."}, {"heading": "4 Experiments", "text": "In this area, we are able to contact us in the areas of \"Economy,\" \"Economy,\" \"Economy,\" \"Economy,\" \"Economy,\" \"Economy,\" \"Economy,\" \"Economy,\" \"Economy,\" \"Economy,\" \"Economy,\" \"Economy,\" \"Economy,\" \"Economy,\" \"Economy,\" \"\" Economy, \"\" \"Economy,\" \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\" \",\" \"\", \"\", \"\", \"\" \",\" \"\", \"\" \",\" \"\" \",\" \"\", \"\" \",\" \"\", \"\" \",\" \",\" \",\" \",\", \"\", \"\", \",\", \"\", \",\" \",\", \"\", \"\", \"\", \"\", \"\", \"\", \"\" \",\" \",\" \",\" \"\" \",\" \"\", \"\" \"\", \"\" \",\" \"\" \",\" \"\" \",\" \"\", \",\" \",\" \"\" \",\" \",\" \"\" \",\" \"\", \"\" \"\", \"\" \"\", \"\" \",\" \"\" \",\" \",\" \"\" \",\" \"\" \",\" \"\", \"\" \"\", \"\" \",\" \"\", \"\" \"\", \"\" \"\", \"\" \""}, {"heading": "5 Conclusion", "text": "In this paper, we looked at identifiable, anchoring-free correlated topic modeling; a criterion for topic estimation based on word-word co-occurrence / correlation matrix was proposed and its identification conditions were demonstrated; the proposed approach provides a guarantee of topic identity under considerably milder conditions than the anchor word assumption and thus exhibits better robustness to model incongruity; a simple procedure involving only one self-decomposition and a few small linear programs was proposed to deal with the formulated criterion; experiments with real text corpus data demonstrated the effectiveness of the proposed approach."}, {"heading": "Acknowledgment", "text": "This work is supported in part by the National Science Foundation (NSF) under project numbers NSF-ECCS 1608961 and NSF IIS-1247632 and in part by the Digital Technology Initiative (DTI) Seed Grant, University of Minnesota."}, {"heading": "A Proof of Proposition 1", "text": "Let us designate a workable solution to the problem (3) in the manuscript as (C, E, E, E), and let us designate C, E, for the keyword PMF matrix and the thematic correlation matrix. Note that we can represent any workable solution as C, A, E, A, A, A, 1, where A, RF, F, is an inverted matrix. Considering that rank (P) = F and that adoption 1 is valid, we must rank (C, E) = rank (E, F, for each solution pair (C, E, E, E). Indeed, if the anchor acceptance is valid, then there is a non-singular diagonal submatrix in C, then rank (C,) = F applies, and the same applies to C, since A, since A, is unchangeable."}, {"heading": "B Proof of Lemma 1", "text": "If C is sufficiently scattered, it fulfills Cx = 0, which means that we have Cx \u2265 0 for x-N (C). (10) Equivalent (10) and Equivalent (9) together imply that N (C) K cannot contain a zero space in a second-order cone, which is a contradiction. We now show that every feasible pair of solutions (E-N, C-K) has a full rank. Name the basic PMF matrix as C, and the correlation matrix between topics as E. Assuming 2, the basic truth pair C has full column ranges, and thus E RF \u00b7 F full rank, if rank (P) = F. Now that any feasible solution other than C = E can be written, the basic truth matrix is C = A A E, where we have all pairs, and E \u2212 F has full rank, if rank (P) = F. Now that any feasible solution other than C = E can be written."}, {"heading": "C Proof of Theorem 1", "text": "Let us call the basic truth-word topic PMF matrix C, and the correlation matrix between topics like E. What we observe is their product P = C E C T, and we want to deduce from the observation P = column P what the matrices C and E are. However, the method proposed in this paper is the solution (3), repeated hereminimize E, C \u2212 detE | subject P = CECTCT1 = 1, C \u2265 0.Now, denote an optimal solution of the above as C and E, and theorem 1 asserts that if C s is sufficiently dispersed (cf. assumption 2), then there is a permutation matrix that is such a column C, E column that rank (P) = F f f, and theorem 1 f f f e that the F column is sufficiently dispersed (cf."}, {"heading": "D Synthetic Experiments", "text": "In this section we give the simulation results that restore the word-theme-matrix-C and the thematic correlation-matrix-E in fact in the absence of the anchor words with synthetic data."}, {"heading": "E Complete Results of the Illustrative Example", "text": "The full results of the illustrative example in the manuscript can be found in Tables 6-10. One observation is that FastAnchor, SPA and SNPA give the same anchor words and topics in different order. XRAY returns different anchor words and topics, but the degraded topics are of inferior quality compared to those of FastAnchor, SPA and SNPA. The proposed AnchorFree algorithm returns five clean topics."}], "references": [{"title": "Probabilistic topic models", "author": ["D.M. Blei"], "venue": "Communications of the ACM,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2012}, {"title": "Latent Dirichlet allocation", "author": ["D.M. Blei", "A.Y. Ng", "M.I. Jordan"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2003}, {"title": "Finding scientific topics", "author": ["T.L. Griffiths", "M. Steyvers"], "venue": "Proceedings of the National Academy of Sciences,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2004}, {"title": "Computing a nonnegative matrix factorization\u2013 provably", "author": ["S. Arora", "R. Ge", "R. Kannan", "A. Moitra"], "venue": "In ACM symposium on Theory of Computing,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2012}, {"title": "Factoring nonnegative matrices with linear programs", "author": ["B. Recht", "C. Re", "J. Tropp", "V. Bittorf"], "venue": "In Proc. NIPS", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2012}, {"title": "Fast and robust recursive algorithms for separable nonnegative matrix factorization", "author": ["N. Gillis", "S.A. Vavasis"], "venue": "IEEE Trans. Pattern Anal. Mach. Intell.,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2014}, {"title": "Robustness analysis of hottopixx, a linear programming model for factoring nonnegative matrices", "author": ["N. Gillis"], "venue": "SIAM Journal on Matrix Analysis and Applications,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2013}, {"title": "Fast conical hull algorithms for near-separable non-negative matrix factorization", "author": ["A. Kumar", "V. Sindhwani", "P. Kambadur"], "venue": "In Proc. ICML-12,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2012}, {"title": "Learning topic models\u2013going beyond SVD", "author": ["S. Arora", "R. Ge", "A. Moitra"], "venue": "In Proc. FOCS", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2012}, {"title": "A practical algorithm for topic modeling with provable guarantees", "author": ["S. Arora", "R. Ge", "Y. Halpern", "D. Mimno", "A. Moitra", "D. Sontag", "Y. Wu", "M. Zhu"], "venue": "In Proc. ICML-13,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2013}, {"title": "Successive nonnegative projection algorithm for robust nonnegative blind source separation", "author": ["N. Gillis"], "venue": "SIAM Journal on Imaging Sciences,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2014}, {"title": "When does non-negative matrix factorization give a correct decomposition into parts", "author": ["D. Donoho", "V. Stodden"], "venue": "In Proc. NIPS 2013,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2003}, {"title": "A spectral algorithm for latent Dirichlet allocation", "author": ["A. Anandkumar", "Y.-K. Liu", "D.J. Hsu", "D.P. Foster", "S.M. Kakade"], "venue": "In Proc. NIPS", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2012}, {"title": "Two SVDs suffice: Spectral decompositions for probabilistic topic modeling and latent Dirichlet allocation", "author": ["A. Anandkumar", "S.M. Kakade", "D.P. Foster", "Y.-K. Liu", "D. Hsu"], "venue": "Technical report,", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2012}, {"title": "When are overcomplete topic models identifiable? uniqueness of tensor Tucker decompositions with structured sparsity", "author": ["A. Anandkumar", "D.J. Hsu", "M. Janzamin", "S.M. Kakade"], "venue": "In Proc. NIPS", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2013}, {"title": "Locally consistent concept factorization for document clustering", "author": ["D. Cai", "X. He", "J. Han"], "venue": "IEEE Trans. Knowl. Data Eng.,", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2011}, {"title": "Non-negative matrix factorization revisited: Uniqueness and algorithm for symmetric decomposition", "author": ["K. Huang", "N. Sidiropoulos", "A. Swami"], "venue": "IEEE Trans. Signal Process.,", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2014}, {"title": "Principled neuro-functional connectivity discovery", "author": ["K. Huang", "N.D. Sidiropoulos", "E.E. Papalexakis", "C. Faloutsos", "P.P. Talukdar", "T.M. Mitchell"], "venue": "In Proc. SIAM Conference on Data Mining (SDM),", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2015}, {"title": "Blind separation of quasi-stationary sources: Exploiting convex geometry in covariance domain", "author": ["X. Fu", "W.-K. Ma", "K. Huang", "N.D. Sidiropoulos"], "venue": "IEEE Trans. Signal Process.,", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2015}, {"title": "Convex analysis for non-negative blind source separation with application in imaging", "author": ["W.-K. Ma", "T.-H. Chan", "C.-Y. Chi", "Y. Wang"], "venue": "Convex Optimization in Signal Processing and Communications,", "citeRegEx": "20", "shortCiteRegEx": "20", "year": 2010}], "referenceMentions": [{"referenceID": 0, "context": "Topic mining, or topic modeling, has attracted significant attention in the broader machine learning and data mining community [1].", "startOffset": 127, "endOffset": 130}, {"referenceID": 1, "context": "proposed a Latent Dirichlet Allocation (LDA) model for topic mining [2], where the topics are modeled as probability mass functions (PMFs) over a vocabulary and each document is a mixture of the PMFs.", "startOffset": 68, "endOffset": 71}, {"referenceID": 1, "context": "Under this model, posterior inference-based methods and approximations were proposed [2, 3], but identifiability issues \u2013 i.", "startOffset": 85, "endOffset": 91}, {"referenceID": 2, "context": "Under this model, posterior inference-based methods and approximations were proposed [2, 3], but identifiability issues \u2013 i.", "startOffset": 85, "endOffset": 91}, {"referenceID": 3, "context": "In recent years, considerable effort has been invested in designing identifiable models and estimation criteria as well as polynomial time solvable algorithms for topic modeling [4, 5, 6, 7, 8, 9, 10, 11].", "startOffset": 178, "endOffset": 204}, {"referenceID": 4, "context": "In recent years, considerable effort has been invested in designing identifiable models and estimation criteria as well as polynomial time solvable algorithms for topic modeling [4, 5, 6, 7, 8, 9, 10, 11].", "startOffset": 178, "endOffset": 204}, {"referenceID": 5, "context": "In recent years, considerable effort has been invested in designing identifiable models and estimation criteria as well as polynomial time solvable algorithms for topic modeling [4, 5, 6, 7, 8, 9, 10, 11].", "startOffset": 178, "endOffset": 204}, {"referenceID": 6, "context": "In recent years, considerable effort has been invested in designing identifiable models and estimation criteria as well as polynomial time solvable algorithms for topic modeling [4, 5, 6, 7, 8, 9, 10, 11].", "startOffset": 178, "endOffset": 204}, {"referenceID": 7, "context": "In recent years, considerable effort has been invested in designing identifiable models and estimation criteria as well as polynomial time solvable algorithms for topic modeling [4, 5, 6, 7, 8, 9, 10, 11].", "startOffset": 178, "endOffset": 204}, {"referenceID": 8, "context": "In recent years, considerable effort has been invested in designing identifiable models and estimation criteria as well as polynomial time solvable algorithms for topic modeling [4, 5, 6, 7, 8, 9, 10, 11].", "startOffset": 178, "endOffset": 204}, {"referenceID": 9, "context": "In recent years, considerable effort has been invested in designing identifiable models and estimation criteria as well as polynomial time solvable algorithms for topic modeling [4, 5, 6, 7, 8, 9, 10, 11].", "startOffset": 178, "endOffset": 204}, {"referenceID": 10, "context": "In recent years, considerable effort has been invested in designing identifiable models and estimation criteria as well as polynomial time solvable algorithms for topic modeling [4, 5, 6, 7, 8, 9, 10, 11].", "startOffset": 178, "endOffset": 204}, {"referenceID": 11, "context": "Essentially, these algorithms are based on the so-called separable nonnegative matrix factorization (NMF) model [12].", "startOffset": 112, "endOffset": 116}, {"referenceID": 4, "context": "Based on this assumption, two classes of algorithms are usually employed, namely linear programming based methods [5, 7] and greedy pursuit approaches [11, 6, 8, 10].", "startOffset": 114, "endOffset": 120}, {"referenceID": 6, "context": "Based on this assumption, two classes of algorithms are usually employed, namely linear programming based methods [5, 7] and greedy pursuit approaches [11, 6, 8, 10].", "startOffset": 114, "endOffset": 120}, {"referenceID": 10, "context": "Based on this assumption, two classes of algorithms are usually employed, namely linear programming based methods [5, 7] and greedy pursuit approaches [11, 6, 8, 10].", "startOffset": 151, "endOffset": 165}, {"referenceID": 5, "context": "Based on this assumption, two classes of algorithms are usually employed, namely linear programming based methods [5, 7] and greedy pursuit approaches [11, 6, 8, 10].", "startOffset": 151, "endOffset": 165}, {"referenceID": 7, "context": "Based on this assumption, two classes of algorithms are usually employed, namely linear programming based methods [5, 7] and greedy pursuit approaches [11, 6, 8, 10].", "startOffset": 151, "endOffset": 165}, {"referenceID": 9, "context": "Based on this assumption, two classes of algorithms are usually employed, namely linear programming based methods [5, 7] and greedy pursuit approaches [11, 6, 8, 10].", "startOffset": 151, "endOffset": 165}, {"referenceID": 7, "context": "However, normalization at the factorization stage is usually not desired, since it may destroy the good conditioning of the data matrix brought by pre-processing and amplify noise [8].", "startOffset": 180, "endOffset": 183}, {"referenceID": 8, "context": "[9, 10] works with the pairwise word-word correlation matrix, which has the advantage of suppressing sampling noise and also features better scalability.", "startOffset": 0, "endOffset": 7}, {"referenceID": 9, "context": "[9, 10] works with the pairwise word-word correlation matrix, which has the advantage of suppressing sampling noise and also features better scalability.", "startOffset": 0, "endOffset": 7}, {"referenceID": 8, "context": "However, [9, 10] did not relax the anchor-word assumption or the need for normalization, and did not explore the symmetric structure of the co-occurrence matrix \u2013 i.", "startOffset": 9, "endOffset": 16}, {"referenceID": 9, "context": "However, [9, 10] did not relax the anchor-word assumption or the need for normalization, and did not explore the symmetric structure of the co-occurrence matrix \u2013 i.", "startOffset": 9, "endOffset": 16}, {"referenceID": 8, "context": ", the algorithms in [9, 10] are essentially the same asymmetric separable NMF algorithms as in [4, 6, 8].", "startOffset": 20, "endOffset": 27}, {"referenceID": 9, "context": ", the algorithms in [9, 10] are essentially the same asymmetric separable NMF algorithms as in [4, 6, 8].", "startOffset": 20, "endOffset": 27}, {"referenceID": 3, "context": ", the algorithms in [9, 10] are essentially the same asymmetric separable NMF algorithms as in [4, 6, 8].", "startOffset": 95, "endOffset": 104}, {"referenceID": 5, "context": ", the algorithms in [9, 10] are essentially the same asymmetric separable NMF algorithms as in [4, 6, 8].", "startOffset": 95, "endOffset": 104}, {"referenceID": 7, "context": ", the algorithms in [9, 10] are essentially the same asymmetric separable NMF algorithms as in [4, 6, 8].", "startOffset": 95, "endOffset": 104}, {"referenceID": 12, "context": ", [13, 14, 15] make use of third or higher-order statistics of the data corpus to formulate the topic modeling problem as a tensor factorization problem.", "startOffset": 2, "endOffset": 14}, {"referenceID": 13, "context": ", [13, 14, 15] make use of third or higher-order statistics of the data corpus to formulate the topic modeling problem as a tensor factorization problem.", "startOffset": 2, "endOffset": 14}, {"referenceID": 14, "context": ", [13, 14, 15] make use of third or higher-order statistics of the data corpus to formulate the topic modeling problem as a tensor factorization problem.", "startOffset": 2, "endOffset": 14}, {"referenceID": 12, "context": ", second-order word correlation statistics); and ii) identifiability is guaranteed only when the topics are uncorrelated \u2013 where a super-symmetric parallel factor analysis (PARAFAC) model can be obtained [13, 14].", "startOffset": 204, "endOffset": 212}, {"referenceID": 13, "context": ", second-order word correlation statistics); and ii) identifiability is guaranteed only when the topics are uncorrelated \u2013 where a super-symmetric parallel factor analysis (PARAFAC) model can be obtained [13, 14].", "startOffset": 204, "endOffset": 212}, {"referenceID": 9, "context": "Uncorrelatedness is a restrictive assumption [10].", "startOffset": 45, "endOffset": 49}, {"referenceID": 14, "context": ", sparsity of topic PMFs [15].", "startOffset": 25, "endOffset": 29}, {"referenceID": 8, "context": "In this work, our interest lies in topic mining using word-word correlation matrices like in [9, 10], because of its potential scalability and noise robustness.", "startOffset": 93, "endOffset": 100}, {"referenceID": 9, "context": "In this work, our interest lies in topic mining using word-word correlation matrices like in [9, 10], because of its potential scalability and noise robustness.", "startOffset": 93, "endOffset": 100}, {"referenceID": 1, "context": "where C \u2208 R is the word-topic matrix, whose f -th column C(:, f) represents the probability mass function (PMF) of topic f over a vocabulary of words, and W (f, d) denotes the weight of topic f in document d [2, 13, 10].", "startOffset": 208, "endOffset": 219}, {"referenceID": 12, "context": "where C \u2208 R is the word-topic matrix, whose f -th column C(:, f) represents the probability mass function (PMF) of topic f over a vocabulary of words, and W (f, d) denotes the weight of topic f in document d [2, 13, 10].", "startOffset": 208, "endOffset": 219}, {"referenceID": 9, "context": "where C \u2208 R is the word-topic matrix, whose f -th column C(:, f) represents the probability mass function (PMF) of topic f over a vocabulary of words, and W (f, d) denotes the weight of topic f in document d [2, 13, 10].", "startOffset": 208, "endOffset": 219}, {"referenceID": 15, "context": "a nonnegative matrix factorization (NMF) model \u2013 and many early works tried to use NMF and variants to deal with this problem [16].", "startOffset": 126, "endOffset": 130}, {"referenceID": 16, "context": "However, NMF does not admit a unique solution in general, unless both C and W satisfy some sparsity-related conditions [17].", "startOffset": 119, "endOffset": 123}, {"referenceID": 11, "context": "Such models and algorithms usually rely on an assumption called \u201cseparability\u201d in the NMF literature [12]:", "startOffset": 101, "endOffset": 105}, {"referenceID": 5, "context": "Algorithm 1: Successive Projection Algorithm [6] input : D; F .", "startOffset": 45, "endOffset": 48}, {"referenceID": 5, "context": "The arguably simplest algorithm is the so-called successive projection algorithm (SPA) [6] that is presented in Algorithm 1.", "startOffset": 87, "endOffset": 90}, {"referenceID": 10, "context": "SPA-like algorithms first define a normalized matrix X = D\u03a3 where \u03a3 = Diag(1D ) [11].", "startOffset": 80, "endOffset": 84}, {"referenceID": 7, "context": "The algorithms in [8, 10, 11] can also be considered variants of SPA, with different deflation procedures and pre-/postprocessing.", "startOffset": 18, "endOffset": 29}, {"referenceID": 9, "context": "The algorithms in [8, 10, 11] can also be considered variants of SPA, with different deflation procedures and pre-/postprocessing.", "startOffset": 18, "endOffset": 29}, {"referenceID": 10, "context": "The algorithms in [8, 10, 11] can also be considered variants of SPA, with different deflation procedures and pre-/postprocessing.", "startOffset": 18, "endOffset": 29}, {"referenceID": 7, "context": "In particular, the algorithm in [8] avoids normalization \u2014 for real-word data, normalization at the factorization stage may amplify noise and damage the good conditioning of the data matrix brought by pre-processing, e.", "startOffset": 32, "endOffset": 35}, {"referenceID": 7, "context": ", the tf-idf procedure [8].", "startOffset": 23, "endOffset": 26}, {"referenceID": 6, "context": "To pick out vertices, there are also algorithms using linear programming and sparse optimization [7, 5], but these have serious scalability issues and thus are less appealing.", "startOffset": 97, "endOffset": 103}, {"referenceID": 4, "context": "To pick out vertices, there are also algorithms using linear programming and sparse optimization [7, 5], but these have serious scalability issues and thus are less appealing.", "startOffset": 97, "endOffset": 103}, {"referenceID": 8, "context": "In [9, 10, 14, 15], the authors proposed to use second and higher-order statistics for topic mining.", "startOffset": 3, "endOffset": 18}, {"referenceID": 9, "context": "In [9, 10, 14, 15], the authors proposed to use second and higher-order statistics for topic mining.", "startOffset": 3, "endOffset": 18}, {"referenceID": 13, "context": "In [9, 10, 14, 15], the authors proposed to use second and higher-order statistics for topic mining.", "startOffset": 3, "endOffset": 18}, {"referenceID": 14, "context": "In [9, 10, 14, 15], the authors proposed to use second and higher-order statistics for topic mining.", "startOffset": 3, "endOffset": 18}, {"referenceID": 8, "context": "[9, 10] proposed to work with the following matrix:", "startOffset": 0, "endOffset": 7}, {"referenceID": 9, "context": "[9, 10] proposed to work with the following matrix:", "startOffset": 0, "endOffset": 7}, {"referenceID": 8, "context": "The matrix P is by definition a word-word correlation matrix, but also has a nice interpretation: if D(v, d) denotes the frequency of word v occurring in document d, P (i, j) is the likelihood that term i and j co-occur in a document [9, 10].", "startOffset": 234, "endOffset": 241}, {"referenceID": 9, "context": "The matrix P is by definition a word-word correlation matrix, but also has a nice interpretation: if D(v, d) denotes the frequency of word v occurring in document d, P (i, j) is the likelihood that term i and j co-occur in a document [9, 10].", "startOffset": 234, "endOffset": 241}, {"referenceID": 9, "context": "The algorithm proposed in [10] also makes use of Assumption 1 and is conceptually close to Algorithm 1.", "startOffset": 26, "endOffset": 30}, {"referenceID": 12, "context": "The work in [13, 14, 15] relaxed the anchor-word assumption.", "startOffset": 12, "endOffset": 24}, {"referenceID": 13, "context": "The work in [13, 14, 15] relaxed the anchor-word assumption.", "startOffset": 12, "endOffset": 24}, {"referenceID": 14, "context": "The work in [13, 14, 15] relaxed the anchor-word assumption.", "startOffset": 12, "endOffset": 24}, {"referenceID": 12, "context": "The work in [13, 14] showed that P is a tensor satisfying the parallel factor analysis (PARAFAC) model and thus C is uniquely identifiable, if the topics are uncorrelated, which is a restrictive assumption", "startOffset": 12, "endOffset": 20}, {"referenceID": 13, "context": "The work in [13, 14] showed that P is a tensor satisfying the parallel factor analysis (PARAFAC) model and thus C is uniquely identifiable, if the topics are uncorrelated, which is a restrictive assumption", "startOffset": 12, "endOffset": 20}, {"referenceID": 14, "context": "When the topics are correlated, additional assumptions like sparsity are needed to restore identifiability [15].", "startOffset": 107, "endOffset": 111}, {"referenceID": 7, "context": "However, if the deflation procedure in Algorithm 1 (the update of \u0398) has constraints like in [8, 11], there is a serious complexity issue: solving a constrained least squares problem with FV variables is not an easy task.", "startOffset": 93, "endOffset": 100}, {"referenceID": 10, "context": "However, if the deflation procedure in Algorithm 1 (the update of \u0398) has constraints like in [8, 11], there is a serious complexity issue: solving a constrained least squares problem with FV variables is not an easy task.", "startOffset": 93, "endOffset": 100}, {"referenceID": 7, "context": "Data sparsity is destroyed after the first deflation step, and thus even first-order methods or coordinate descent as in [8, 11] do not really help.", "startOffset": 121, "endOffset": 128}, {"referenceID": 10, "context": "Data sparsity is destroyed after the first deflation step, and thus even first-order methods or coordinate descent as in [8, 11] do not really help.", "startOffset": 121, "endOffset": 128}, {"referenceID": 11, "context": "Note that the dual cone of K is another second-order cone [12], i.", "startOffset": 58, "endOffset": 62}, {"referenceID": 16, "context": "The sufficiently scattered assumption appeared in identifiability proofs of several matrix factorization models [17, 18, 19] with different identification criteria.", "startOffset": 112, "endOffset": 124}, {"referenceID": 17, "context": "The sufficiently scattered assumption appeared in identifiability proofs of several matrix factorization models [17, 18, 19] with different identification criteria.", "startOffset": 112, "endOffset": 124}, {"referenceID": 18, "context": "The sufficiently scattered assumption appeared in identifiability proofs of several matrix factorization models [17, 18, 19] with different identification criteria.", "startOffset": 112, "endOffset": 124}, {"referenceID": 16, "context": "[17] used this condition to show the identifiability of plain NMF, while Fu et al.", "startOffset": 0, "endOffset": 4}, {"referenceID": 18, "context": "[19] related the sufficiently scattered condition to the so-called volume-minimization criterion for blind source separation.", "startOffset": 0, "endOffset": 4}, {"referenceID": 17, "context": "Here, we propose to employ the solver proposed in [18], where the same subproblem (7) was used to solve a dynamical system identification problem.", "startOffset": 50, "endOffset": 54}, {"referenceID": 19, "context": "The idea is to apply the co-factor expansion to deal with the determinant objective function, first proposed in the context of non-negative blind source separation [20]: if we fix all the columns of M except the f th one, detM becomes a linear function with respect to M(:, f), i.", "startOffset": 164, "endOffset": 168}, {"referenceID": 8, "context": "We use the standard tf-idf data as the D matrix, and estimate the correlation matrix using the biased estimator suggested in [9].", "startOffset": 125, "endOffset": 128}, {"referenceID": 5, "context": "Specifically, the successive projection algorithm (SPA) [6], the successive nonnegative projection algorithm (SNPA) [11], the XRAY algorithm [8], and the fast anchor words (FastAnchor) [10] algorithm.", "startOffset": 56, "endOffset": 59}, {"referenceID": 10, "context": "Specifically, the successive projection algorithm (SPA) [6], the successive nonnegative projection algorithm (SNPA) [11], the XRAY algorithm [8], and the fast anchor words (FastAnchor) [10] algorithm.", "startOffset": 116, "endOffset": 120}, {"referenceID": 7, "context": "Specifically, the successive projection algorithm (SPA) [6], the successive nonnegative projection algorithm (SNPA) [11], the XRAY algorithm [8], and the fast anchor words (FastAnchor) [10] algorithm.", "startOffset": 141, "endOffset": 144}, {"referenceID": 9, "context": "Specifically, the successive projection algorithm (SPA) [6], the successive nonnegative projection algorithm (SNPA) [11], the XRAY algorithm [8], and the fast anchor words (FastAnchor) [10] algorithm.", "startOffset": 185, "endOffset": 189}, {"referenceID": 9, "context": "Since we are interested in word-word correlation/co-occurrence based mining, all the algorithms are combined with the framework provided in [10] and the efficient RecoverL2 process is employed for estimating the topics after the anchors are identified.", "startOffset": 140, "endOffset": 144}, {"referenceID": 9, "context": "To alleviate this, we also use the similarity count (SimCount) that was adopted in [10] \u2014 for each topic, the similarity count is obtained simply by adding up the overlapped words of the topics within the leading N words, and a smaller SimCount means the mined topics are more distinguishable.", "startOffset": 83, "endOffset": 87}, {"referenceID": 5, "context": "SPA is the fastest algorithm since it has a recursive update [6].", "startOffset": 61, "endOffset": 64}], "year": 2016, "abstractText": "In topic modeling, many algorithms that guarantee identifiability of the topics have been developed under the premise that there exist anchor words \u2013 i.e., words that only appear (with positive probability) in one topic. Follow-up work has resorted to three or higher-order statistics of the data corpus to relax the anchor word assumption. Reliable estimates of higher-order statistics are hard to obtain, however, and the identification of topics under those models hinges on uncorrelatedness of the topics, which can be unrealistic. This paper revisits topic modeling based on second-order moments, and proposes an anchor-free topic mining framework. The proposed approach guarantees the identification of the topics under a much milder condition compared to the anchor-word assumption, thereby exhibiting much better robustness in practice. The associated algorithm only involves one eigen-decomposition and a few small linear programs. This makes it easy to implement and scale up to very large problem instances. Experiments using the TDT2 and Reuters-21578 corpus demonstrate that the proposed anchor-free approach exhibits very favorable performance (measured using coherence, similarity count, and clustering accuracy metrics) compared to the prior art.", "creator": "LaTeX with hyperref package"}}}