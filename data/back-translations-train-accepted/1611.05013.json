{"id": "1611.05013", "review": {"conference": "iclr", "VERSION": "v1", "DATE_OF_SUBMISSION": "15-Nov-2016", "title": "PixelVAE: A Latent Variable Model for Natural Images", "abstract": "Natural image modeling is a landmark challenge of unsupervised learning. Variational Autoencoders (VAEs) learn a useful latent representation and model global structure well but have difficulty capturing small details. PixelCNN models details very well, but lacks a latent code and is difficult to scale for capturing large structures. We present PixelVAE, a VAE model with an autoregressive decoder based on PixelCNN. Our model requires very few expensive autoregressive layers compared to PixelCNN and learns latent codes that are more compressed than a standard VAE while still capturing most non-trivial structure. Finally, we extend our model to a hierarchy of latent variables at different scales. Our model achieves state-of-the-art performance on binarized MNIST, competitive performance on 64x64 ImageNet, and high-quality samples on the LSUN bedrooms dataset.", "histories": [["v1", "Tue, 15 Nov 2016 20:16:27 GMT  (2749kb,D)", "http://arxiv.org/abs/1611.05013v1", null]], "reviews": [], "SUBJECTS": "cs.LG", "authors": ["ishaan gulrajani", "kundan kumar", "faruk ahmed", "adrien ali taiga", "francesco visin", "david vazquez", "aaron courville"], "accepted": true, "id": "1611.05013"}, "pdf": {"name": "1611.05013.pdf", "metadata": {"source": "CRF", "title": "PIXELVAE: A LATENT VARIABLE MODEL FOR NATURAL IMAGES", "authors": ["Ishaan Gulrajani", "Kundan Kumar", "Faruk Ahmed", "Adrien Ali Taiga", "Francesco Visin", "David Vazquez", "Aaron Courville"], "emails": [], "sections": [{"heading": "1 INTRODUCTION", "text": "Although the recent work has made significant progress (Kingma & Welling, 2014; van den Oord et al., 2016a; b), we are still far from generating compelling, high-resolution nature images. Many recent approaches to this problem are based on an efficient method of performing amortized approximate conclusions in continuous stochastic latent variables: CNN's Variable Autoencoder (VAE) (Kingma & Welling, 2014) collectively forms a generative neural network for performing amortized approximate conclusions from pixel networks. VAEs for images typically use rigid decoders that model output pixels as conditionally independent variables. The resulting model learns useful latent representation of data and effectively formed models of global structure in images, but has difficulty grasping small functions such as textures and sharp edges due to the conditional independence of output pixels."}, {"heading": "2 RELATED WORK", "text": "The Variational Autoencoder (UAE) (Kingma & Welling, 2014) is an elegant framework for training neural networks together for generation and approximate inference by optimizing a variation limit on the likelihood of data logging. However, the use of normalization flows (Rezende & Mohamed, 2015) improves the flexibility of the UAE approximate posterior. Building on this, Kingma et al. (2016) develop an efficient formulation of an autoregressive approximate posterior model using MADE (Germain et al., 2015).In our work, we avoid the need for such flexible inference models by focusing on autoregressive priorities.Another promising recent approach is the Generative Adversarial Networks (GANs) (Goodfellow et al., 2014) approach, which pits a generator network and a discrimination network against each other."}, {"heading": "3 PIXELVAE MODEL", "text": "Like a UAE, our model collectively forms an \"encoder\" inference network that maps an image x to a posterior distribution using latent variables z, and a \"decoder\" generative network that models a distribution over x conditioned to z. Unlike most UAE decoders, which model each dimension of output independently (for example, by modeling the output as Gaussian with diagonal covariance), we use a conditioned pixel CNN structure in the decoder. Unlike most UAE decoders, which model each dimension of output independently (for example, by modeling the output as Gaussian with diagonal covariance), our decoder models use x as a product of each dimension xi conditioned to all preceding dimensions and the latent variable z: p (x | z) = ip (xi | x1, ep.1)."}, {"heading": "3.1 HIERARCHICAL ARCHITECTURE", "text": "The performance of VAEs can be improved by compiling them into a hierarchy of stochastic latent variables, which, in the simplest configuration, has a distribution over the latent variables at the lower level, with generation progressing downwards and upwards through each level (i.e., as in Fig. 3). In revolutionary architectures, the generator is a conditional pixel overview of the latent characteristics at the lower level. As a result, we can typically organize the latent variables into characteristic charts, whose spatial resolution decreases towards the higher levels. Our model can be expanded in the same way. At each level, the generator is a conditional pixel overview of the latent characteristics at the lower level. This allows us to model not only the production distribution on pixels, but also the previous one on each latent characteristic chart."}, {"heading": "4 EXPERIMENTS", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "4.1 MNIST", "text": "We evaluate our model based on the binarized MNIST dataset (Salakhutdinov & Murray, 2008; Lecun et al., 1998) and report on the results in Table 1. We also experiment with a variant of our model where each pixel CNN layer is directly conditioned on a linear transformation of latent variable z (rather than transforming z first through multiple upsampling layers) (as in (van den Oord et al., 2016b) and find that this further improves performance and achieves an NLL upper limit comparable to the current state of the art."}, {"heading": "4.1.1 NUMBER OF PIXELCNN LAYERS", "text": "The masked folding layers in PixelCNN are mathematically expensive because they work at full resolution of the image, and to cover the entire receiver field of the image, PixelCNN typically requires a large number of them. An advantage of our architecture is that we can achieve strong performance with very few PixelCNN layers, which makes training and sampling of our model significantly faster than PixelCNN. To demonstrate this, we compare the performance of our model with PixelCNN as a function of the number of PixelCNN layers (Fig. 4a). We find that our PixelVAE model performs much better than PixelCNN with less than 10 autoregressive layers. This is expected because with a few layers the effective receiver field of the PixelCNN output devices is too small to capture extensive dependencies in the data."}, {"heading": "4.1.2 LATENT VARIABLE INFORMATION CONTENT", "text": "Since the auto-regressive conditional probability function of PixelVAE is meaningful enough to model some properties of the image distribution, it is not forced to consider these properties through its latent variables, as is the case with a standard UAE. Consequently, we can expect PixelVAE to learn latent representations that are invariant for textures, precise positions, and other attributes that are more efficiently modeled by the auto-regressive decoder. To confirm this empirically, we train PixelVAE models with different numbers of auto-regressive layers (and thus different PixelCNN receptive field sizes) and plot the breakdown of the NLL boundary for each of these models into the reconstruction protocol p (x | z) and the KL divergence term DKL (q (x) | x) | p (z) viceptive field sizes), with the cases ranging from the KL to the KL variables in most cases."}, {"heading": "4.2 LSUN BEDROOMS", "text": "To evaluate the performance of our model with more data and complicated image distributions, we conduct experiments on the LSUN bedroom dataset (Yu et al., 2015) using the same pre-processing as in Radford et al. (2015) to remove duplicate images from the dataset. For quantitative experiments, we use a 32 x 32 downsampled version of the dataset and present samples from a model trained on the 64 x 64 version. We train a two-stage PixelVAE with latent variables in 1 x 1 and 8 x 8 spatial resolutions. We note that this is both a two-stage convolutionary VAE with diagonal Gaussian output as well as a single-stage PixelVAE with regard to log probability and sample quality. We also try to replace the pixel CNN layers at a higher level with a diagonal Gaussian decoder, and find that this effectively implies the likelihood of multiple AxelVAE to use."}, {"heading": "4.2.1 FEATURES MODELED AT EACH LAYER", "text": "To see which features are modeled by each of the multiple layers, we take multiple samples while we vary the sampling noise only at a certain level (either pixel-wise output or one of the latent layers) and look at the resulting images visually (Fig. 5). If we vary only the sampling noise distribution at the pixel level (where z1 and z2 are fixed), the samples are almost indistinguishable and differ only by precise positioning and shading details, suggesting that the model uses the pixel-level authoregressive distribution to model only these features. Samples that vary only the noise at the middle level (8 x 8) of latent variables have different objects and colors, but appear to have a similar basic spatial geometry and composition. Finally, samples with different latent variables at the top level exhibit different spatial geometries."}, {"heading": "4.3 64\u00d7 64 IMAGENET", "text": "The generative modeling task 64 \u00d7 64 ImageNet was introduced in (van den Oord et al., 2016a) and involves a density estimate of a difficult, very different image distribution. We trained an archaic PixelUAE model (with an architecture similar to the model in Section 4.2) of similar size to the models in van den Oord et al. (2016a; b) to 64 \u00d7 64 ImageNet in 5 days on 3 NVIDIA GeForce GTX 1080 GPUs. We report on the validation probability in Table 2. Our model achieves a slightly lower log probability than PixelRNN (van den Oord et al., 2016a), but a visual inspection of ImageNet samples from our model (Fig. 6) shows that they are significantly more coherent globally than samples from PixelRNN."}, {"heading": "5 CONCLUSIONS", "text": "In this paper, we introduced a VAE model for natural images with an auto-regressive decoder that performs strongly across a number of datasets. We investigated the properties of our model and showed that it can generate more compressed latent representations than a standard UAE and that it can use less autoregressive layers than PixelCNN. We established a new state of the art for binary MNIST datasets in terms of the probability of 64 \u00d7 64 ImageNet and demonstrated that our model generates high-quality samples on LSUN substrates. PixelVAE's ability to learn compressed representations in latent variables by ignoring the small-scale structure in images is potentially very useful for downstream tasks. It would be interesting to further investigate the capabilities of our model for semi-supervised classification and representation in future work."}, {"heading": "ACKNOWLEDGMENTS", "text": "The authors would like to thank the developers of Theano (Theano Development Team, 2016) and Blocks and Fuel (van Merrie \u00c3 nboer et al., 2015) for the support of the following research funding and computer support agencies: Ubisoft, Nuance Foundation, NSERC, Calcul Quebec, Compute Canada, CIFAR, MEC Project TRA2014-57088-C2-1-R, SGR project 2014- SGR-1506 and TECNIOspring-FP7-ACCI grant."}], "references": [{"title": "Generating sentences from a continuous space. 2016", "author": ["Samuel R Bowman", "Luke Vilnis", "Oriol Vinyals", "Andrew M Dai", "Rafal Jozefowicz", "Samy Bengio"], "venue": null, "citeRegEx": "Bowman et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Bowman et al\\.", "year": 2016}, {"title": "Density estimation using Real NVP", "author": ["Laurent Dinh", "Jascha Sohl-Dickstein", "Samy Bengio"], "venue": null, "citeRegEx": "Dinh et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Dinh et al\\.", "year": 2016}, {"title": "Adversarial feature learning", "author": ["Jeff Donahue", "Philipp Kr\u00e4henb\u00fchl", "Trevor Darrell"], "venue": "CoRR, abs/1605.09782,", "citeRegEx": "Donahue et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Donahue et al\\.", "year": 2016}, {"title": "Made: Masked autoencoder for distribution estimation", "author": ["Matthieu Germain", "Karol Gregor", "Iain Murray", "Hugo Larochelle"], "venue": "CoRR, abs/1502.03509,", "citeRegEx": "Germain et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Germain et al\\.", "year": 2015}, {"title": "Generative adversarial nets", "author": ["Ian Goodfellow", "Jean Pouget-Abadie", "Mehdi Mirza", "Bing Xu", "David Warde-Farley", "Sherjil Ozair", "Aaron Courville", "Yoshua Bengio"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Goodfellow et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Goodfellow et al\\.", "year": 2014}, {"title": "Towards Conceptual Compression", "author": ["Karol Gregor", "Frederic Besse", "Danilo Jimenez Rezende", "Ivo Danihelka", "Daan Wierstra"], "venue": null, "citeRegEx": "Gregor et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Gregor et al\\.", "year": 2016}, {"title": "Auto-encoding variational bayes", "author": ["Diederik P. Kingma", "Max Welling"], "venue": "International Conference on Learning Representations (ICLR),", "citeRegEx": "Kingma and Welling.,? \\Q2014\\E", "shortCiteRegEx": "Kingma and Welling.", "year": 2014}, {"title": "Improving variational inference with inverse autoregressive flow", "author": ["Diederik P. Kingma", "Tim Salimans", "Max Welling"], "venue": null, "citeRegEx": "Kingma et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Kingma et al\\.", "year": 2016}, {"title": "Gradient-based learning applied to document recognition", "author": ["Yann Lecun", "Lon Bottou", "Yoshua Bengio", "Patrick Haffner"], "venue": "In Proceedings of the IEEE,", "citeRegEx": "Lecun et al\\.,? \\Q1998\\E", "shortCiteRegEx": "Lecun et al\\.", "year": 1998}, {"title": "Unsupervised representation learning with deep convolutional generative adversarial networks", "author": ["Alec Radford", "Luke Metz", "Soumith Chintala"], "venue": "CoRR, abs/1511.06434,", "citeRegEx": "Radford et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Radford et al\\.", "year": 2015}, {"title": "Variational inference with normalizing flows", "author": ["Danilo Jimenez Rezende", "Shakir Mohamed"], "venue": "In International Conference on Machine Learning (ICML),", "citeRegEx": "Rezende and Mohamed.,? \\Q2015\\E", "shortCiteRegEx": "Rezende and Mohamed.", "year": 2015}, {"title": "Discrete variational autoencoders", "author": ["Jason Tyler Rolfe"], "venue": "arXiv preprint arXiv:1609.02200,", "citeRegEx": "Rolfe.,? \\Q2016\\E", "shortCiteRegEx": "Rolfe.", "year": 2016}, {"title": "On the quantitative analysis of deep belief networks", "author": ["Ruslan Salakhutdinov", "Iain Murray"], "venue": "Proceedings of the 25th international conference on Machine learning,", "citeRegEx": "Salakhutdinov and Murray.,? \\Q2008\\E", "shortCiteRegEx": "Salakhutdinov and Murray.", "year": 2008}, {"title": "Improved techniques for training", "author": ["Tim Salimans", "Ian J. Goodfellow", "Wojciech Zaremba", "Vicki Cheung", "Alec Radford", "Xi Chen"], "venue": "gans. CoRR,", "citeRegEx": "Salimans et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Salimans et al\\.", "year": 2016}, {"title": "Pixel recurrent neural networks", "author": ["A\u00e4ron van den Oord", "Nal Kalchbrenner", "Koray Kavukcuoglu"], "venue": "In International Conference on Machine Learning (ICML),", "citeRegEx": "Oord et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Oord et al\\.", "year": 2016}, {"title": "Conditional image generation with pixelcnn decoders", "author": ["A\u00e4ron van den Oord", "Nal Kalchbrenner", "Oriol Vinyals", "Lasse Espeholt", "Alex Graves", "Koray Kavukcuoglu"], "venue": "CoRR, abs/1606.05328,", "citeRegEx": "Oord et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Oord et al\\.", "year": 2016}, {"title": "Blocks and fuel: Frameworks for deep learning", "author": ["Bart van Merri\u00ebnboer", "Dzmitry Bahdanau", "Vincent Dumoulin", "Dmitriy Serdyuk", "David WardeFarley", "Jan Chorowski", "Yoshua Bengio"], "venue": "arXiv preprint,", "citeRegEx": "Merri\u00ebnboer et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Merri\u00ebnboer et al\\.", "year": 2015}, {"title": "LSUN: construction of a large-scale image dataset using deep learning with humans", "author": ["Fisher Yu", "Yinda Zhang", "Shuran Song", "Ari Seff", "Jianxiong Xiao"], "venue": "in the loop. CoRR,", "citeRegEx": "Yu et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Yu et al\\.", "year": 2015}], "referenceMentions": [{"referenceID": 3, "context": "(2016) develop an efficient formulation of an autoregressive approximate posterior model using MADE (Germain et al., 2015).", "startOffset": 100, "endOffset": 122}, {"referenceID": 4, "context": "Another promising recent approach is Generative Adversarial Networks (GANs) (Goodfellow et al., 2014), which pit a generator network and a discriminator network against each other.", "startOffset": 76, "endOffset": 101}, {"referenceID": 9, "context": "Recent work has improved training stability (Radford et al., 2015; Salimans et al., 2016) and incorporated inference networks", "startOffset": 44, "endOffset": 89}, {"referenceID": 13, "context": "Recent work has improved training stability (Radford et al., 2015; Salimans et al., 2016) and incorporated inference networks", "startOffset": 44, "endOffset": 89}, {"referenceID": 5, "context": "Based on this, Kingma et al. (2016) develop an efficient formulation of an autoregressive approximate posterior model using MADE (Germain et al.", "startOffset": 15, "endOffset": 36}, {"referenceID": 2, "context": "into the GAN framework (Dumoulin et al., 2016; Donahue et al., 2016).", "startOffset": 23, "endOffset": 68}, {"referenceID": 0, "context": "The idea of using autoregressive conditional likelihoods in VAEs has been explored in the context of sentence modeling in (Bowman et al., 2016), however in that work the use of latent variables fails to improve likelihood over a purely autoregressive model.", "startOffset": 122, "endOffset": 143}, {"referenceID": 5, "context": "Model NLL Test DRAW (Gregor et al., 2016) \u2264 80.", "startOffset": 20, "endOffset": 41}, {"referenceID": 11, "context": "97 Discrete VAE (Rolfe, 2016) = 81.", "startOffset": 16, "endOffset": 29}, {"referenceID": 7, "context": "01 IAF VAE (Kingma et al., 2016) \u2248 79.", "startOffset": 11, "endOffset": 32}, {"referenceID": 14, "context": "\u201cPixelCNN\u201d is the model described in van den Oord et al. (2016a). Our corresponding latent variable model is \u201cPixelVAE\u201d.", "startOffset": 45, "endOffset": 65}, {"referenceID": 14, "context": "\u201cPixelCNN\u201d is the model described in van den Oord et al. (2016a). Our corresponding latent variable model is \u201cPixelVAE\u201d. \u201cGated PixelCNN\u201d and \u201cGated PixelVAE\u201d use the gated activation function in van den Oord et al. (2016b). In \u201cGated PixelVAE without upsampling\u201d, a linear transformation of latent variable conditions the (gated) activation in every PixelCNN layer instead of using upsampling layers.", "startOffset": 45, "endOffset": 224}, {"referenceID": 8, "context": "We evaluate our model on the binarized MNIST dataset (Salakhutdinov & Murray, 2008; Lecun et al., 1998) and report results in Table 1.", "startOffset": 53, "endOffset": 103}, {"referenceID": 17, "context": "To evaluate our model\u2019s performance with more data and complicated image distributions, we perform experiments on the LSUN bedrooms dataset (Yu et al., 2015).", "startOffset": 140, "endOffset": 157}, {"referenceID": 9, "context": "We use the same preprocessing as in Radford et al. (2015) to remove duplicate images in the dataset.", "startOffset": 36, "endOffset": 58}, {"referenceID": 5, "context": "Model NLL Validation (Train) Convolutional DRAW (Gregor et al., 2016) \u2264 4.", "startOffset": 48, "endOffset": 69}, {"referenceID": 1, "context": "04) Real NVP (Dinh et al., 2016) = 4.", "startOffset": 13, "endOffset": 32}], "year": 2016, "abstractText": "Natural image modeling is a landmark challenge of unsupervised learning. Variational Autoencoders (VAEs) learn a useful latent representation and model global structure well but have difficulty capturing small details. PixelCNN models details very well, but lacks a latent code and is difficult to scale for capturing large structures. We present PixelVAE, a VAE model with an autoregressive decoder based on PixelCNN. Our model requires very few expensive autoregressive layers compared to PixelCNN and learns latent codes that are more compressed than a standard VAE while still capturing most non-trivial structure. Finally, we extend our model to a hierarchy of latent variables at different scales. Our model achieves state-of-the-art performance on binarized MNIST, competitive performance on 64 \u00d7 64 ImageNet, and high-quality samples on the LSUN bedrooms dataset.", "creator": "LaTeX with hyperref package"}}}