{"id": "1611.05397", "review": {"conference": "iclr", "VERSION": "v1", "DATE_OF_SUBMISSION": "16-Nov-2016", "title": "Reinforcement Learning with Unsupervised Auxiliary Tasks", "abstract": "Deep reinforcement learning agents have achieved state-of-the-art results by directly maximising cumulative reward. However, environments contain a much wider variety of possible training signals. In this paper, we introduce an agent that also maximises many other pseudo-reward functions simultaneously by reinforcement learning. All of these tasks share a common representation that, like unsupervised learning, continues to develop in the absence of extrinsic rewards. We also introduce a novel mechanism for focusing this representation upon extrinsic rewards, so that learning can rapidly adapt to the most relevant aspects of the actual task. Our agent significantly outperforms the previous state-of-the-art on Atari, averaging 880\\% expert human performance, and a challenging suite of first-person, three-dimensional \\emph{Labyrinth} tasks leading to a mean speedup in learning of 10$\\times$ and averaging 87\\% expert human performance on Labyrinth.", "histories": [["v1", "Wed, 16 Nov 2016 18:21:29 GMT  (8149kb,D)", "http://arxiv.org/abs/1611.05397v1", null]], "reviews": [], "SUBJECTS": "cs.LG cs.NE", "authors": ["max jaderberg", "volodymyr mnih", "wojciech marian czarnecki", "tom schaul", "joel z leibo", "david silver", "koray kavukcuoglu"], "accepted": true, "id": "1611.05397"}, "pdf": {"name": "1611.05397.pdf", "metadata": {"source": "CRF", "title": "REINFORCEMENT LEARNING WITH UNSUPERVISED AUXILIARY TASKS", "authors": ["Max Jaderberg", "Volodymyr Mnih", "Wojciech Marian Czarnecki", "Tom Schaul", "Joel Z Leibo", "David Silver", "Koray Kavukcuoglu"], "emails": ["jaderberg@google.com", "vmnih@google.com", "lejlot@google.com", "schaul@google.com", "jzl@google.com", "davidsilver@google.com", "korayk@google.com"], "sections": [{"heading": null, "text": "In fact, it is such that most of them are able to put themselves in a different world, in which they move, in which they move, in which they move, in which they move, in which they move, in which they move, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they, in which they, in which they live, in which they, in which they live, in which they, in which they, in which they live, in which they, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they, in which they, in which they live, in which they, in which they, in which they, in which they, in which they, in which they, in which they, live, in which they, in which they, in which they, live, in which they, in which they, in which they, in which they, in which they, live, in which they, in which they, live, in which they, in which they, in which they, in fact, in which they, in fact, in fact, are able to put themselves, are able to put themselves, in a different world, in which they, in which they, in which they, in which they, in which they, in which they, in which they, in which they, in which they, in which they, in which they, in"}, {"heading": "1 RELATED WORK", "text": "A variety of reinforcement learning architectures focused on learning temporal abstractions, such as options (Sutton et al., 1999b), typically focusing on developing temporal abstractions that facilitate learning and planning at a high level. In contrast, our agents do not directly use pseudo-reward to maximize the strategies they learn (although this is an interesting direction for future research); instead, they are used exclusively as auxiliary targets for developing a more effective representation. Horde Architecture (Sutton et al., 2011) uses reinforcement learning to identify value functions for a variety of different pseudo-rewards. However, this architecture was not used to learn representational functions; instead, each value function was used separately based on different weights.The UVFA architecture (Schaul et al., 2015a) is a faceted amplification function of reinforcement functions."}, {"heading": "2 BACKGROUND", "text": "We start from the standard learning environment in which an agent interacts with an environment through a number of discrete time steps (Q = Q function).At the time t, the agent receives an observation along with a reward rt and generates an action at. The state st of the agent is a function of his experience up to the time t, st = f (o1, r1, a1,..., ot, rt).The n-step return rate Rt: t + n at the time t is defined as the discounted sum of rewards, Rt: t + n = \u2211 n = i = 1. The value function is the expected return from the state s, V \u03c0 (s).The n-step return rate Rt: p = s, p = s, p value when actions are selected after a political age (a | s).The activation value function Q\u03c0 (s, a) is the expected return from the state s, V \u03c0 (s)."}, {"heading": "3 AUXILIARY TASKS FOR REINFORCEMENT LEARNING", "text": "In this section, we integrate auxiliary tasks into the framework of enhanced learning to promote faster training, more robust learning, and ultimately higher performance for our agents. Section 3.1 introduces the use of auxiliary control tasks, Section 3.2 describes the addition of reward-focused auxiliary tasks, and Section 3.4 describes the full UNREAL Agent combining these auxiliary tasks."}, {"heading": "3.1 AUXILIARY CONTROL TASKS", "text": "The auxiliary control tasks we consider are defined as additional pseudo-reward functions in the environment with which the agent interacts. Q is the principle with which Q interacts. We formally define an auxiliary control task c by a reward function r (c): S \u00b7 A \u2192 R, where S is the space of possible states and A is the space of available measures. The underlying state space S encompasses both the history of the observations and rewards as well as the state of the agent itself, i.e. the activation of the hidden units of the network. In the face of a series of auxiliary control tasks C, leave (c) the policy of the agent for each auxiliary control task and leave the policy of the agent itself, i.e. the activation of the hidden units of the network. The overall goal is to maximize the overall performance across all of these auxiliary control functions."}, {"heading": "3.2 AUXILIARY REWARD TASKS", "text": "In addition to general learning about the dynamics of the environment, an agent must learn to maximize the global reward flow = unfocused reward function. To learn a policy to maximize rewards, an agent needs traits that identify states that lead to high reward and high value. An agent with a good representation of reward states allows learning reward functions and should, in turn, allow easy learning of a policy. However, reward is encountered very sparsely in many interesting environments, which means that it can take a long time to develop traits that signify the beginning of reward functions. We want to eliminate the perceptual spareness of reward functions and reward states to support the training of an agent, but to do so in a way that does not introduce bias into the agent's policy. To do this, we perform the auxiliary task of the reward prediction - or reward prediction - the prediction of some immediate reward function that consists of predicting the reward function."}, {"heading": "3.3 EXPERIENCE REPLAY", "text": "replay-replay-replay-replay-replay-replay-replay-replay-r.replay-replay-replay-r.replay-replay-replay-replay-replay-replay-replay-r.replay-replay-replay-replay-replay-r.replay-replay-replay-replay-replay-replay-replay-replay-replay-r.replay-replay-replay-replay-replay-replay-replay-replay-r.replay-replay-replay-replay-r.replay-replay-replay-replay-replay-replay-replay-replay-replay-replay-r.replay-replay-replay-replay-replay-replay-replay-r.replay-replay-replay-replay-replay-replay-r.replay-replay-replay-replay-replay-replay-replay-replay-r.replay-replay-replay-replay-replay-replay-replay-replay-replay-replay-r.replay-replay-replay-replay-replay-replay-replay-replay-replay-replay-replay-r.replay-replay-replay-replay-replay-replay-replay-replay-replay-replay-replay-replay-replay-r.replay-replay-replay-replay-replay-replay-replay-replay-replay-replay-replay-replay-replay-replay-replay-r.replay-replay-replay-replay-replay-replay-replay-replay-replay-replay-replay-replay-replay-replay-replay-replay-r.replay-replay-replay-replay-replay-replay-replay-replay-replay-replay-replay-replay-replay-replay-replay-replay-replay-replay-replay-re"}, {"heading": "4 EXPERIMENTS", "text": "In this section we give the results of the experiments with the 3D environment Labyrinth in section 4.1 and Atari in section 4.2. In all our experiments we used an A3C CNN-LSTM agent as base and the UNREAL agent with its detached variants added additional outputs and losses to this base agent. The agent is trained in 20 steps and the auxiliary tasks are performed in all 20 environment steps, corresponding to each update of the base agent A3C. The replay buffer stores the most recent 2k observations, actions and rewards of the base agent. In Labyrinth we use the same set of 17 discrete actions for all games and on Atari the action set is game dependent (between 3 and 18 discrete actions). For the full implementation details see section B."}, {"heading": "4.1 LABYRINTH RESULTS", "text": "In fact, the majority of them will be able to be in a position to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to fight, to fight, to fight, to fight, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to fight, to fight, to fight, to fight, to move, to fight, to fight, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to fight, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to fight, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to fight, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move,"}, {"heading": "4.1.1 RESULTS", "text": "This year, it is so far that it is only a matter of time before it is ready, until it is ready."}, {"heading": "4.2 ATARI", "text": "We applied the UNREAL agent and UNREAL without pixel control to 57 Atari games in the area of Arcade Learning Environment (Bellemare et al., 2012). We use the same evaluation protocol as for our labyrinth experiments, in which we evaluate 50 different random hyperparameter settings (learning rate and entropy costs) for each game. Results are shown in the bottom line of Figure 3. The left side shows the average performance curves of the top 3 agents for all three methods, the right half shows sorted average human normalized values for each hyperparameter setting. More detailed learning curves for individual levels can be found in Figure 7. We see that UNREAL exceeds the current state-of-the-art agents, i.e. A3C and prioritized dueling DQN (Wang et al., 2016) at all levels, achieving an average performance of 880% and 250% respectively, and is considerably more robust than UNREAL 3AL."}, {"heading": "5 CONCLUSION", "text": "We have shown how extending a learning agent with additional control and reward predictions can dramatically improve both data efficiency and the robustness of hyperparameter settings. In particular, our proposed UNREAL architecture has more than doubled the previous state-of-the-art results on challenging 3D labyrinth levels, bringing averages to over 87% of human values. The same UNREAL architecture has also significantly improved the learning speed and robustness of A3C compared to 57 Atari games."}, {"heading": "ACKNOWLEDGEMENTS", "text": "We would like to thank Charles Beattie, Julian Schrittwieser, Marcus Wainwright and Stig Petersen for environmental design and development, and Amir Sadik and Sarah York for expert testing of human games. We would also like to thank Joseph Modayil, Andrea Banino, Hubert Soyer, Razvan Pascanu and Raia Hadsell for many helpful conversations."}, {"heading": "A ATARI GAMES", "text": "In fact, it is as if most of us are able to abide by the rules that they have imposed on themselves. (...) It is not as if they were abiding by the rules of the market. (...) It is not as if they were abiding by the rules of the market. (...) It is as if they were not abiding by the rules of the market. (...) It is as if they were not abiding by the rules of the market. (...) It is as if they were not abiding by the rules of the market. (...) It is as if they were not abiding by the rules of the market. (...) It is as if. (...) It is as if. (...) It is. (...) It is as if. (...) The rules are not. (...) It is as if. (...) It is. (... It is as if. (...) The rules are not. (... It is as if. (...) The rules are not. (... It is.) and it is. (... It is."}, {"heading": "C LABYRINTH LEVELS", "text": "It is not the first time that the EU Commission has taken such a step."}], "references": [{"title": "Successor features for transfer in reinforcement learning", "author": ["Andr\u00e9 Barreto", "R\u00e9mi Munos", "Tom Schaul", "David Silver"], "venue": "arXiv preprint arXiv:1606.05312,", "citeRegEx": "Barreto et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Barreto et al\\.", "year": 2016}, {"title": "The arcade learning environment: An evaluation platform for general agents", "author": ["Marc G Bellemare", "Yavar Naddaf", "Joel Veness", "Michael Bowling"], "venue": "Journal of Artificial Intelligence Research,", "citeRegEx": "Bellemare et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Bellemare et al\\.", "year": 2012}, {"title": "Improving generalization for temporal difference learning: The successor representation", "author": ["Peter Dayan"], "venue": "Neural Computation,", "citeRegEx": "Dayan.,? \\Q1993\\E", "shortCiteRegEx": "Dayan.", "year": 1993}, {"title": "Learning to forget: Continual prediction with lstm", "author": ["Felix A Gers", "J\u00fcrgen Schmidhuber", "Fred Cummins"], "venue": "Neural computation,", "citeRegEx": "Gers et al\\.,? \\Q2000\\E", "shortCiteRegEx": "Gers et al\\.", "year": 2000}, {"title": "Vizdoom: A doom-based ai research platform for visual reinforcement learning", "author": ["Micha\u0142 Kempka", "Marek Wydmuch", "Grzegorz Runc", "Jakub Toczek", "Wojciech Ja\u015bkowski"], "venue": null, "citeRegEx": "Kempka et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Kempka et al\\.", "year": 2016}, {"title": "Skill discovery in continuous reinforcement learning domains using skill chaining", "author": ["George Konidaris", "Andre S Barreto"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Konidaris and Barreto.,? \\Q2009\\E", "shortCiteRegEx": "Konidaris and Barreto.", "year": 2009}, {"title": "Deep successor reinforcement learning", "author": ["Tejas D Kulkarni", "Ardavan Saeedi", "Simanta Gautam", "Samuel J Gershman"], "venue": "arXiv preprint arXiv:1606.02396,", "citeRegEx": "Kulkarni et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Kulkarni et al\\.", "year": 2016}, {"title": "Playing FPS games with deep reinforcement learning", "author": ["Guillaume Lample", "Devendra Singh Chaplot"], "venue": null, "citeRegEx": "Lample and Chaplot.,? \\Q2016\\E", "shortCiteRegEx": "Lample and Chaplot.", "year": 2016}, {"title": "Recurrent reinforcement learning: A hybrid approach", "author": ["Xiujun Li", "Lihong Li", "Jianfeng Gao", "Xiaodong He", "Jianshu Chen", "Li Deng", "Ji He"], "venue": "arXiv preprint arXiv:1509.03044,", "citeRegEx": "Li et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Li et al\\.", "year": 2015}, {"title": "Memory approaches to reinforcement learning in non-markovian domains", "author": ["Long-Ji Lin", "Tom M Mitchell"], "venue": "Technical report,", "citeRegEx": "Lin and Mitchell.,? \\Q1992\\E", "shortCiteRegEx": "Lin and Mitchell.", "year": 1992}, {"title": "Learning to navigate in complex environments", "author": ["Piotr Mirowski", "Razvan Pascanu", "Fabio Viola", "Andrea Banino", "Hubert Soyer", "Andy Ballard", "Misha Denil", "Ross Goroshin", "Laurent Sifre", "Koray Kavukcuoglu", "Dharshan Kumaran", "Raia Hadsell"], "venue": null, "citeRegEx": "Mirowski et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Mirowski et al\\.", "year": 2016}, {"title": "Playing atari with deep reinforcement learning", "author": ["Volodymyr Mnih", "Koray Kavukcuoglu", "David Silver", "Alex Graves", "Ioannis Antonoglou", "Daan Wierstra", "Martin Riedmiller"], "venue": "In NIPS Deep Learning Workshop", "citeRegEx": "Mnih et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Mnih et al\\.", "year": 2013}, {"title": "Human-level control through deep reinforcement learning", "author": ["Volodymyr Mnih", "Koray Kavukcuoglu", "David Silver", "Andrei A. Rusu", "Joel Veness", "Marc G. Bellemare", "Alex Graves", "Martin Riedmiller", "Andreas K. Fidjeland", "Georg Ostrovski", "Stig Petersen", "Charles Beattie", "Amir Sadik", "Ioannis Antonoglou", "Helen King", "Dharshan Kumaran", "Daan Wierstra", "Shane Legg", "Demis Hassabis"], "venue": "Nature, 518(7540):529\u2013533,", "citeRegEx": "Mnih et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Mnih et al\\.", "year": 2015}, {"title": "Asynchronous methods for deep reinforcement learning", "author": ["Volodymyr Mnih", "Adri\u00e0 Puigdom\u00e8nech Badia", "Mehdi Mirza", "Alex Graves", "Timothy P. Lillicrap", "Tim Harley", "David Silver", "Koray Kavukcuoglu"], "venue": "In Proceedings of the 33rd International Conference on Machine Learning (ICML),", "citeRegEx": "Mnih et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Mnih et al\\.", "year": 2016}, {"title": "Action-conditional video prediction using deep networks in atari games", "author": ["Junhyuk Oh", "Xiaoxiao Guo", "Honglak Lee", "Richard L Lewis", "Satinder Singh"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Oh et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Oh et al\\.", "year": 2015}, {"title": "Control of memory, active perception, and action in minecraft", "author": ["Junhyuk Oh", "Valliappa Chockalingam", "Satinder Singh", "Honglak Lee"], "venue": "arXiv preprint arXiv:1605.09128,", "citeRegEx": "Oh et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Oh et al\\.", "year": 2016}, {"title": "Incremental multi-step q-learning", "author": ["Jing Peng", "Ronald J Williams"], "venue": "Machine Learning,", "citeRegEx": "Peng and Williams.,? \\Q1996\\E", "shortCiteRegEx": "Peng and Williams.", "year": 1996}, {"title": "The future of memory: remembering", "author": ["Daniel L Schacter", "Donna Rose Addis", "Demis Hassabis", "Victoria C Martin", "R Nathan Spreng", "Karl K Szpunar"], "venue": "imagining, and the brain. Neuron,", "citeRegEx": "Schacter et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Schacter et al\\.", "year": 2012}, {"title": "Universal value function approximators", "author": ["Tom Schaul", "Daniel Horgan", "Karol Gregor", "David Silver"], "venue": "In Proceedings of the 32nd International Conference on Machine Learning", "citeRegEx": "Schaul et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Schaul et al\\.", "year": 2015}, {"title": "Prioritized experience replay", "author": ["Tom Schaul", "John Quan", "Ioannis Antonoglou", "David Silver"], "venue": "arXiv preprint arXiv:1511.05952,", "citeRegEx": "Schaul et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Schaul et al\\.", "year": 2015}, {"title": "Formal theory of creativity, fun, and intrinsic motivation (1990\u20132010)", "author": ["J\u00fcrgen Schmidhuber"], "venue": "IEEE Transactions on Autonomous Mental Development,", "citeRegEx": "Schmidhuber.,? \\Q2010\\E", "shortCiteRegEx": "Schmidhuber.", "year": 2010}, {"title": "Compositional planning using optimal option models", "author": ["David Silver", "Kamil Ciosek"], "venue": "arXiv preprint arXiv:1206.6473,", "citeRegEx": "Silver and Ciosek.,? \\Q2012\\E", "shortCiteRegEx": "Silver and Ciosek.", "year": 2012}, {"title": "Mastering the game of go with deep neural networks and tree", "author": ["David Silver", "Aja Huang", "Chris J Maddison", "Arthur Guez", "Laurent Sifre", "George Van Den Driessche", "Julian Schrittwieser", "Ioannis Antonoglou", "Veda Panneershelvam", "Marc Lanctot"], "venue": "search. Nature,", "citeRegEx": "Silver et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Silver et al\\.", "year": 2016}, {"title": "Policy gradient methods for reinforcement learning with function approximation", "author": ["Richard S Sutton", "David A McAllester", "Satinder P Singh", "Yishay Mansour"], "venue": "In NIPS,", "citeRegEx": "Sutton et al\\.,? \\Q1999\\E", "shortCiteRegEx": "Sutton et al\\.", "year": 1999}, {"title": "Between mdps and semi-mdps: A framework for temporal abstraction in reinforcement learning", "author": ["Richard S Sutton", "Doina Precup", "Satinder Singh"], "venue": "Artificial intelligence,", "citeRegEx": "Sutton et al\\.,? \\Q1999\\E", "shortCiteRegEx": "Sutton et al\\.", "year": 1999}, {"title": "Horde: A scalable real-time architecture for learning knowledge from unsupervised sensorimotor interaction", "author": ["Richard S Sutton", "Joseph Modayil", "Michael Delp", "Thomas Degris", "Patrick M Pilarski", "Adam White", "Doina Precup"], "venue": "In The 10th International Conference on Autonomous Agents and Multiagent Systems-Volume", "citeRegEx": "Sutton et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Sutton et al\\.", "year": 2011}, {"title": "A deep hierarchical approach to lifelong learning in minecraft", "author": ["Chen Tessler", "Shahar Givony", "Tom Zahavy", "Daniel J Mankowitz", "Shie Mannor"], "venue": "arXiv preprint arXiv:1604.07255,", "citeRegEx": "Tessler et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Tessler et al\\.", "year": 2016}, {"title": "Dueling Network Architectures for Deep Reinforcement Learning", "author": ["Z. Wang", "N. de Freitas", "M. Lanctot"], "venue": "In Proceedings of the 33rd International Conference on Machine Learning (ICML),", "citeRegEx": "Wang et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Wang et al\\.", "year": 2016}, {"title": "Learning from delayed rewards", "author": [], "venue": "PhD thesis,", "citeRegEx": "Watkins.,? \\Q1989\\E", "shortCiteRegEx": "Watkins.", "year": 1989}, {"title": "Modelbased reinforcement learning with parametrized physical models and optimism-driven exploration", "author": ["Christopher Xie", "Sachin Patil", "Teodor Mihai Moldovan", "Sergey Levine", "Pieter Abbeel"], "venue": "CoRR, abs/1509.06824,", "citeRegEx": "Xie et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Xie et al\\.", "year": 2015}, {"title": "Graying the black box: Understanding dqns", "author": ["Tom Zahavy", "Nir Ben Zrihem", "Shie Mannor"], "venue": "In Proceedings of the 33rd International Conference on Machine Learning,", "citeRegEx": "Zahavy et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Zahavy et al\\.", "year": 2016}], "referenceMentions": [{"referenceID": 13, "context": "(a) The base agent is a CNN-LSTM agent trained on-policy with the A3C loss (Mnih et al., 2016).", "startOffset": 75, "endOffset": 94}, {"referenceID": 17, "context": "Just as animals dream about positively or negatively rewarding events more frequently (Schacter et al., 2012), our agents preferentially replay sequences containing rewarding events.", "startOffset": 86, "endOffset": 109}, {"referenceID": 13, "context": "This paper brings together the state-of-the-art Asynchronous Advantage Actor-Critic (A3C) framework (Mnih et al., 2016), outlined in Section 2, with auxiliary control tasks and auxiliary reward tasks, defined in sections Section 3.", "startOffset": 100, "endOffset": 119}, {"referenceID": 13, "context": "4) In Section 4 we apply our UNREAL agent to a challenging set of 3D-vision based domains known as the Labyrinth (Mnih et al., 2016), learning solely from the raw RGB pixels of a first-person view.", "startOffset": 113, "endOffset": 132}, {"referenceID": 25, "context": "The Horde architecture (Sutton et al., 2011) also applied reinforcement learning to identify value functions for a multitude of distinct pseudo-rewards.", "startOffset": 23, "endOffset": 44}, {"referenceID": 2, "context": "Similarly, the successor representation (Dayan, 1993; Barreto et al., 2016; Kulkarni et al., 2016) factors a continuous set of expected value functions for a fixed policy, by combining an expectation over features of the state with an embedding of the pseudo-reward function.", "startOffset": 40, "endOffset": 98}, {"referenceID": 0, "context": "Similarly, the successor representation (Dayan, 1993; Barreto et al., 2016; Kulkarni et al., 2016) factors a continuous set of expected value functions for a fixed policy, by combining an expectation over features of the state with an embedding of the pseudo-reward function.", "startOffset": 40, "endOffset": 98}, {"referenceID": 6, "context": "Similarly, the successor representation (Dayan, 1993; Barreto et al., 2016; Kulkarni et al., 2016) factors a continuous set of expected value functions for a fixed policy, by combining an expectation over features of the state with an embedding of the pseudo-reward function.", "startOffset": 40, "endOffset": 98}, {"referenceID": 0, "context": "Successor representations have been used to transfer representations from one pseudo-reward to another (Barreto et al., 2016) or to different scales of reward (Kulkarni et al.", "startOffset": 103, "endOffset": 125}, {"referenceID": 6, "context": ", 2016) or to different scales of reward (Kulkarni et al., 2016).", "startOffset": 41, "endOffset": 64}, {"referenceID": 20, "context": "Another, related line of work involves learning models of the environment (Schmidhuber, 2010; Xie et al., 2015; Oh et al., 2015).", "startOffset": 74, "endOffset": 128}, {"referenceID": 29, "context": "Another, related line of work involves learning models of the environment (Schmidhuber, 2010; Xie et al., 2015; Oh et al., 2015).", "startOffset": 74, "endOffset": 128}, {"referenceID": 14, "context": "Another, related line of work involves learning models of the environment (Schmidhuber, 2010; Xie et al., 2015; Oh et al., 2015).", "startOffset": 74, "endOffset": 128}, {"referenceID": 0, "context": "Similarly, the successor representation (Dayan, 1993; Barreto et al., 2016; Kulkarni et al., 2016) factors a continuous set of expected value functions for a fixed policy, by combining an expectation over features of the state with an embedding of the pseudo-reward function. Successor representations have been used to transfer representations from one pseudo-reward to another (Barreto et al., 2016) or to different scales of reward (Kulkarni et al., 2016). Another, related line of work involves learning models of the environment (Schmidhuber, 2010; Xie et al., 2015; Oh et al., 2015). Although learning environment models as auxiliary tasks could improve RL agents (e.g. Lin & Mitchell (1992); Li et al.", "startOffset": 54, "endOffset": 698}, {"referenceID": 0, "context": "Similarly, the successor representation (Dayan, 1993; Barreto et al., 2016; Kulkarni et al., 2016) factors a continuous set of expected value functions for a fixed policy, by combining an expectation over features of the state with an embedding of the pseudo-reward function. Successor representations have been used to transfer representations from one pseudo-reward to another (Barreto et al., 2016) or to different scales of reward (Kulkarni et al., 2016). Another, related line of work involves learning models of the environment (Schmidhuber, 2010; Xie et al., 2015; Oh et al., 2015). Although learning environment models as auxiliary tasks could improve RL agents (e.g. Lin & Mitchell (1992); Li et al. (2015)), this has not yet been shown to work in rich visual environments.", "startOffset": 54, "endOffset": 716}, {"referenceID": 0, "context": "Similarly, the successor representation (Dayan, 1993; Barreto et al., 2016; Kulkarni et al., 2016) factors a continuous set of expected value functions for a fixed policy, by combining an expectation over features of the state with an embedding of the pseudo-reward function. Successor representations have been used to transfer representations from one pseudo-reward to another (Barreto et al., 2016) or to different scales of reward (Kulkarni et al., 2016). Another, related line of work involves learning models of the environment (Schmidhuber, 2010; Xie et al., 2015; Oh et al., 2015). Although learning environment models as auxiliary tasks could improve RL agents (e.g. Lin & Mitchell (1992); Li et al. (2015)), this has not yet been shown to work in rich visual environments. More recently, auxiliary predictions tasks have been studied in 3D reinforcement learning environments. Lample & Chaplot (2016) showed that predicting internal features of the emulator, such as the presence of an enemy on the screen, is beneficial.", "startOffset": 54, "endOffset": 911}, {"referenceID": 0, "context": "Similarly, the successor representation (Dayan, 1993; Barreto et al., 2016; Kulkarni et al., 2016) factors a continuous set of expected value functions for a fixed policy, by combining an expectation over features of the state with an embedding of the pseudo-reward function. Successor representations have been used to transfer representations from one pseudo-reward to another (Barreto et al., 2016) or to different scales of reward (Kulkarni et al., 2016). Another, related line of work involves learning models of the environment (Schmidhuber, 2010; Xie et al., 2015; Oh et al., 2015). Although learning environment models as auxiliary tasks could improve RL agents (e.g. Lin & Mitchell (1992); Li et al. (2015)), this has not yet been shown to work in rich visual environments. More recently, auxiliary predictions tasks have been studied in 3D reinforcement learning environments. Lample & Chaplot (2016) showed that predicting internal features of the emulator, such as the presence of an enemy on the screen, is beneficial. Mirowski et al. (2016) study auxiliary prediction of depth in the context of navigation.", "startOffset": 54, "endOffset": 1055}, {"referenceID": 28, "context": "Value-based reinforcement learning algorithms, such as Q-learning (Watkins, 1989), or its deep learning instantiations DQN (Mnih et al.", "startOffset": 66, "endOffset": 81}, {"referenceID": 12, "context": "Value-based reinforcement learning algorithms, such as Q-learning (Watkins, 1989), or its deep learning instantiations DQN (Mnih et al., 2015) and asynchronous Q-learning (Mnih et al.", "startOffset": 123, "endOffset": 142}, {"referenceID": 13, "context": ", 2015) and asynchronous Q-learning (Mnih et al., 2016), approximate the action-value function Q(s, a; \u03b8) using parameters \u03b8, and then update parameters to minimise the mean-squared error, for example by optimising an n-step lookahead loss (Peng & Williams, 1996), LQ = E [ (Rt:t+n + \u03b3 n maxa\u2032 Q(s \u2032, a\u2032; \u03b8\u2212)\u2212Q(s, a; \u03b8)) ] ; where \u03b8\u2212 are previous parameters and the optimisation is with respect to \u03b8.", "startOffset": 36, "endOffset": 55}, {"referenceID": 28, "context": "Policy gradient algorithms adjust the policy to maximise the expected reward, L\u03c0 = \u2212Es\u223c\u03c0 [R1:\u221e], using the gradient \u2202Es\u223c\u03c0 [R1:\u221e] \u2202\u03b8 = E [ \u2202 \u2202\u03b8 log \u03c0(a|s)(Q\u03c0(s, a)\u2212 V (s)) ] (Watkins, 1989; Sutton et al., 1999a); in practice the true value functions Q and V \u03c0 are substituted with approximations.", "startOffset": 173, "endOffset": 210}, {"referenceID": 13, "context": "The Asynchronous Advantage Actor-Critic (A3C) algorithm (Mnih et al., 2016) constructs an approximation to both the policy \u03c0(a|s, \u03b8) and the value function V (s, \u03b8) using parameters \u03b8.", "startOffset": 56, "endOffset": 75}, {"referenceID": 11, "context": "Specifically, for each control task c we optimise an n-step Q-learning loss L Q = E [( Rt:t+n + \u03b3 n maxa\u2032 Q (c)(s\u2032, a\u2032, \u03b8\u2212)\u2212Q(c)(s, a, \u03b8) )2] , as described in Mnih et al. (2016).", "startOffset": 160, "endOffset": 179}, {"referenceID": 12, "context": "\u2022 Network features - Since the policy or value networks of an agent learn to extract taskrelevant high-level features of the environment (Mnih et al., 2015; Zahavy et al., 2016; Silver et al., 2016) they can be useful quantities for the agent to learn to control.", "startOffset": 137, "endOffset": 198}, {"referenceID": 30, "context": "\u2022 Network features - Since the policy or value networks of an agent learn to extract taskrelevant high-level features of the environment (Mnih et al., 2015; Zahavy et al., 2016; Silver et al., 2016) they can be useful quantities for the agent to learn to control.", "startOffset": 137, "endOffset": 198}, {"referenceID": 22, "context": "\u2022 Network features - Since the policy or value networks of an agent learn to extract taskrelevant high-level features of the environment (Mnih et al., 2015; Zahavy et al., 2016; Silver et al., 2016) they can be useful quantities for the agent to learn to control.", "startOffset": 137, "endOffset": 198}, {"referenceID": 12, "context": "Experience replay has proven to be an effective mechanism for improving both the data efficiency and stability of deep reinforcement learning algorithms (Mnih et al., 2015).", "startOffset": 153, "endOffset": 172}, {"referenceID": 13, "context": "The primary policy is trained with A3C (Mnih et al., 2016): it learns from parallel streams of experience to gain efficiency and stability; it is updated online using policy gradient methods; and it uses a recurrent neural network to encode the complete history of experience.", "startOffset": 39, "endOffset": 58}, {"referenceID": 4, "context": "platforms for AI research like VizDoom (Kempka et al., 2016) or Minecraft (Tessler et al.", "startOffset": 39, "endOffset": 60}, {"referenceID": 26, "context": ", 2016) or Minecraft (Tessler et al., 2016).", "startOffset": 21, "endOffset": 43}, {"referenceID": 15, "context": "Labyrinth also supports continuous motion unlike the Minecraft platform of (Oh et al., 2016), which is a 3D grid world.", "startOffset": 75, "endOffset": 92}, {"referenceID": 6, "context": "The first baseline was A3C augmented with a pixel reconstruction loss, which has been shown to improve performance on 3D environments (Kulkarni et al., 2016).", "startOffset": 134, "endOffset": 157}, {"referenceID": 1, "context": "We applied the UNREAL agent as well as UNREAL without pixel control to 57 Atari games from the Arcade Learning Environment (Bellemare et al., 2012) domain.", "startOffset": 123, "endOffset": 147}, {"referenceID": 27, "context": "A3C and Prioritized Dueling DQN (Wang et al., 2016), across all levels attaining 880% mean and 250% median performance.", "startOffset": 32, "endOffset": 51}], "year": 2016, "abstractText": "Deep reinforcement learning agents have achieved state-of-the-art results by directly maximising cumulative reward. However, environments contain a much wider variety of possible training signals. In this paper, we introduce an agent that also maximises many other pseudo-reward functions simultaneously by reinforcement learning. All of these tasks share a common representation that, like unsupervised learning, continues to develop in the absence of extrinsic rewards. We also introduce a novel mechanism for focusing this representation upon extrinsic rewards, so that learning can rapidly adapt to the most relevant aspects of the actual task. Our agent significantly outperforms the previous state-of-theart on Atari, averaging 880% expert human performance, and a challenging suite of first-person, three-dimensional Labyrinth tasks leading to a mean speedup in learning of 10\u00d7 and averaging 87% expert human performance on Labyrinth. Natural and artificial agents live in a stream of sensorimotor data. At each time step t, the agent receives observations ot and executes actions at. These actions influence the future course of the sensorimotor stream. In this paper we develop agents that learn to predict and control this stream, by solving a host of reinforcement learning problems, each focusing on a distinct feature of the sensorimotor stream. Our hypothesis is that an agent that can flexibly control its future experiences will also be able to achieve any goal with which it is presented, such as maximising its future rewards. The classic reinforcement learning paradigm focuses on the maximisation of extrinsic reward. However, in many interesting domains, extrinsic rewards are only rarely observed. This raises questions of what and how to learn in their absence. Even if extrinsic rewards are frequent, the sensorimotor stream contains an abundance of other possible learning targets. Traditionally, unsupervised learning attempts to reconstruct these targets, such as the pixels in the current or subsequent frame. It is typically used to accelerate the acquisition of a useful representation. In contrast, our learning objective is to predict and control features of the sensorimotor stream, by treating them as pseudorewards for reinforcement learning. Intuitively, this set of tasks is more closely matched with the agent\u2019s long-term goals, potentially leading to more useful representations. Consider a baby that learns to maximise the cumulative amount of red that it observes. To correctly predict the optimal value, the baby must understand how to increase \u201credness\u201d by various means, including manipulation (bringing a red object closer to the eyes); locomotion (moving in front of a red object); and communication (crying until the parents bring a red object). These behaviours are likely to recur for many other goals that the baby may subsequently encounter. No understanding of these behaviours is required to simply reconstruct the redness of current or subsequent images. Our architecture uses reinforcement learning to approximate both the optimal policy and optimal value function for many different pseudo-rewards. It also makes other auxiliary predictions that serve to focus the agent on important aspects of the task. These include the long-term goal of predicting cumulative extrinsic reward as well as short-term predictions of extrinsic reward. To learn more efficiently, our agents use an experience replay mechanism to provide additional updates \u2217Joint first authors. Ordered alphabetically by first name. 1 ar X iv :1 61 1. 05 39 7v 1 [ cs .L G ] 1 6 N ov 2 01 6", "creator": "LaTeX with hyperref package"}}}