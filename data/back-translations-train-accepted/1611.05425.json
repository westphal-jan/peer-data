{"id": "1611.05425", "review": {"conference": "AAAI", "VERSION": "v1", "DATE_OF_SUBMISSION": "16-Nov-2016", "title": "ProjE: Embedding Projection for Knowledge Graph Completion", "abstract": "With the large volume of new information created every day, determining the validity of information in a knowledge graph and filling in its missing parts are crucial tasks for many researchers and practitioners. To address this challenge, a number of knowledge graph completion methods have been developed using low-dimensional graph embeddings. Although researchers continue to improve these models using an increasingly complex feature space, we show that simple changes in the architecture of the underlying model can outperform state-of-the-art models without the need for complex feature engineering. In this work, we present a shared variable neural network model called ProjE that fills-in missing information in a knowledge graph by learning joint embeddings of the knowledge graph's entities and edges, and through subtle, but important, changes to the standard loss function. In doing so, ProjE has a parameter size that is smaller than 11 out of 15 existing methods while performing $37\\%$ better than the current-best method on standard datasets. We also show, via a new fact checking task, that ProjE is capable of accurately determining the veracity of many declarative statements.", "histories": [["v1", "Wed, 16 Nov 2016 20:09:08 GMT  (1337kb,D)", "http://arxiv.org/abs/1611.05425v1", "14 pages, Accepted to AAAI 2017"]], "COMMENTS": "14 pages, Accepted to AAAI 2017", "reviews": [], "SUBJECTS": "cs.AI stat.ML", "authors": ["baoxu shi", "tim weninger"], "accepted": true, "id": "1611.05425"}, "pdf": {"name": "1611.05425.pdf", "metadata": {"source": "META", "title": "ProjE: Embedding Projection for Knowledge Graph Completion", "authors": ["Baoxu Shi", "Tim Weninger"], "emails": [], "sections": [{"heading": null, "text": "In fact, the fact is that most of them will be able to move to another world, in which they are able, in which they are able to integrate, and in which they are able, in which they are able to change the world, \"he said in an interview with the\" New York Times. \""}, {"heading": "1 Related Work", "text": "These methods typically learn continuous, low-dimensional vector representations (i.e., embedding) for entities WE and relationships WR by minimizing a margin-based pair-by-pair ranking loss [24]. The most common embedding method in this category is TransE [4], which measures the relationships between a translated head and a tail entity on the same low-dimensional level. In contrast to the energy functions of TransE, asE is defined (h, r, t) = the most widespread embedding model in this category is TransE [4], which measures the Ln distance between a translated head and a tail entity. The unstructured model [3] is a specific case of TransE, in which r = 0 is used for all relationship ships.Based on the original idea of treating two entities as one translation of another (about their relationship), several models were introduced to improve the initial relationships."}, {"heading": "2 Methodology", "text": "The present paper considers the KGC problem as a ranking task and optimizes the collective values of the candidate list. As we want to optimize the order of the candidate units together, we have to project the candidate units onto the same embedding vector. For this task, we learn a combination operator that generates a target vector from the input data. Subsequently, the candidate units are projected onto the same target vector, which shows the similarity of the candidate as a scale. In this section, we describe the ProjE architecture, followed by two suggested variants, its loss functions and our choice of the candidate sampling method. In the Experiments section, we show that despite a relatively small parameter space, ProjE outperforms all existing methods. A detailed description of the algorithm can be found in the Supplementary Material."}, {"heading": "2.1 Model Architecture", "text": "The most important findings in the development of ProjE are as follows: In view of two input embeddments, we consider the prediction task to be a ranking problem, where the best placed candidates are the correct entities. To generate this ordered list, we project each of the candidates onto a target vector defined by two input embeddings by a combination operator. Existing models such as Knowledge Vault, HolE, and NTN define specific matrix combination operators that combine entities and / or relationships. In common practice, these matrices are expected to be sparse. As we believe it is unnecessary to have interactions between different feature dimensions at this early stage, we limit our matrices to be diagonal, which are inherently sparse. The combination operator is defined: Dee + Drr + bc, (4) where De and Dr are diagonal matrices that serve as a global unit and relationship weight."}, {"heading": "2.2 Ranking Method and Loss Function", "text": "iSe (iSe) iSe (iSe) iSe (iSe) iSe (iSe) iSe (iSe) iSe (iSe) iSe (iSe) iSe (iSe) iSe (iSe) iSe (iSe) iSe (iSe) iSe) iSe (iSe) iSe (iSe) iSe (iSe) iSe (iSe) iSe (iSe) iSe (iSe) iSe (iSe) iSe (iSe) iSe (iSe) iSe (iSe) iSe (iSe) iSe (iSe) iSe (iSe) iSe (iSe) iSe (iSe) iSe (iSe) iSe (iSe) iSe (iSe) iSe (iSe) iSe (iSe) iSe (iSe) iSe (iSe) iSe (iSe) iSe (iSe) iSe (iSe) iSe (iSe) iSe (iSe) iSe (iSe) iSe (iSe) iSe (iSe) iSe (iSe) iSe (iSe) iSe (iSe) iSe (iSe) iSe (iSe) iSe (iSe) iSe (iSe) iSe (iSe) iSe (iSe) iSe (iSe) iSe (iSe (iSe) iSe) iSe (iSe) iSe (iSe (iSe) iSe (iSe) iSe (iSe) iSe) iSe (iSe) iSe (iSe (iSe (iSe) iSe (iSe) iSe) iSe (iS"}, {"heading": "2.3 Candidate Sampling", "text": "Although ProjE limits the number of additional parameters, the projection process can be costly due to the large number of candidates (i.e. the number of rows in Wc). If we reduce the number of candidate units in the training phase, we could create a smaller working set containing only a subset of the embedding matrix WE. In this sense, we use candidate samples to reduce the number of candidate units. Candidate samples are not a new problem; many recent papers have addressed this problem in an interesting way [16, 26, 13]. We experimented with many possibilities and found that the negative sample used in Word2Vec [26] performs best. For a given unit e, relationship r and a binary label vector y, we calculate the projection with all positive candidates and only a random subset of negative candidates from Py according to the convention of Word2Vec."}, {"heading": "3 Experiments", "text": "The FB15K dataset is a 15,000 entity subset of the Freebase; the Semantic MEDLINE Database (SemMedDB) is a KG derived from all PubMed models; and DBpedia is a KG derived from Wikipedia info boxes [23]. Using DBpedia and SemMedDB, we are also introducing a new fact-checking task for a practical case study on the usefulness of these models. ProjE is implemented in Python with TensorFlow [1]; the code and data are available at https: / / github.com / nddsg / ProjE."}, {"heading": "3.1 Settings", "text": "For both entity prediction and relationship prediction tasks, we use Adam [21] as a stochastic optimizer with standard hyperparameter settings: \u03b21 = 0.9, \u03b22 = 0.999 and = 1e \u2212 8. During the training, we apply an L1 regulator to all parameters in ProjE and a failure layer above the combination operator to prevent overfitting. Hyperparameters in ProjE are the learning rate lr, embedding size k, mini-battery size b, regulatory weight \u03b1, failure probability pd and success probability for negative candidate samples py. We use lr = 0.01, b = 200, \u03b1 = 1e \u2212 5 and pd = 0.5 for both tasks, k = 200, py = 0.5 for the task of predicting entities and k = 100, py = 0.75 for the task of predicting relationships. For all tasks, ProjE was trained for most of the 100 Iterations, all of which can be unified with one E]."}, {"heading": "3.2 Entity and Relationship Prediction", "text": "We evaluated ProjE's performance on entity and relationship prediction tasks with the FB15K dataset according to the experimental settings in TransE [4] and PTransE [24]. For entity prediction, we aim to predict a missing h (or t) for a given candidate. < h, r, t > by ranking all entities in the KG. To create a test set, we have replaced the head or tail entity with all entities in the KG, and rank these replacement entities in the ranking list we use to use the mean and HITS metrics as a measurement measure. Mean Rang measures the average rank of the correct entities / relationships we measure when correct entities / relationships appear within the correct entities / relationships appear within the top k elements."}, {"heading": "3.3 Fact Checking", "text": "Unlike prediction and kinship prediction tasks, which predict randomly sampled triple questions, we use a new fact-checking task that tests the predictive power of different models for real-world issues. We consider the prediction task to be a kind of link prediction problem, because a fact statement < h, r, t > can of course be considered an edge in a KG. We use ProjE _ wlistwise with one small change: instead of directly using the embedding of entities, the input vector of ProjE consists of the predicate paths between the two entities [32]. We learn the embedding of entities by adding an input layer that converts input paths into the embedding of entities."}, {"heading": "4 Conclusions and Future Work", "text": "To summarize, the contributions of the present paper are as follows: 1) we consider the KGC task to be a ranking problem and project candidate units onto a vector that represents a combined embedding of the known parts of an input triple and order the ranking score vector in descending order; 2) we show that by collectively optimizing the ranking score vector based on the list-wise ProjE variation we can significantly improve prediction performance; 3) ProjE only uses directly connected, length-1 paths during training and has a relatively simple 2-layer structure, but surpasses complex models that have richer parameters or features; and 4) unlike other models (e.g. CVSM, TransE, DKRL), the current work does not require pre-trained embedding and has much fewer parameters than related models. Finally, we show that projectors can match existing models to factor checking methods such as N for future work."}, {"heading": "A Appendix", "text": "In this supplement, we present a detailed algorithm description of the proposed ProjE _ wlistwise model in Alg. 1, of which ProjE _ listwise is a special case. Next, two further experiments are shown to determine the training stability and scaling potential of ProjE.A.1 Training ProjE in Alg. 1, we describe the training process of ProjE _ wlistwise. (For a given training Triple-Set S, we first construct the actual training data by randomly corrupting either the head h or the tail, and then generate the corresponding positive and negative candidates of S using candidate sampling when requested. Then, for each mini-batch in the newly generated training data, we calculate the loss and update the corresponding parameters. A.2 Model Stability To evaluate the training stability, we calculate the mean rank, filtered averages, HITS @ 10 and filtered HITS @ 10 over the first 25 training data."}], "references": [{"title": "Tensorflow: Large-scale machine learning on heterogeneous distributed systems", "author": ["M. Abadi", "A. Agarwal", "P. Barham", "E. Brevdo", "Z. Chen", "C. Citro", "G.S. Corrado", "A. Davis", "J. Dean", "M. Devin"], "venue": "arXiv preprint arXiv:1603.04467,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2016}, {"title": "Friends and neighbors on the Web", "author": ["L.A. Adamic", "E. Adar"], "venue": "Social Networks,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2003}, {"title": "Joint Learning of Words and Meaning Representations for Open-Text Semantic Parsing", "author": ["A. Bordes", "X. Glorot", "J. Weston", "Y. Bengio"], "venue": null, "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2012}, {"title": "Translating Embeddings for Modeling Multi-relational Data", "author": ["A. Bordes", "N. Usunier", "A. Garc\u00eda-Dur\u00e1n", "J. Weston", "O. Yakhnenko"], "venue": "In NIPS,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2013}, {"title": "Learning Structured Embeddings of Knowledge Bases", "author": ["A. Bordes", "J. Weston", "R. Collobert", "Y. Bengio"], "venue": null, "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2011}, {"title": "Computational fact checking from knowledge networks", "author": ["G.L. Ciampaglia", "P. Shiralkar", "L.M. Rocha", "J. Bollen", "F. Menczer", "A. Flammini"], "venue": "PLoS ONE,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2015}, {"title": "Large-scale named entity disambiguation based on wikipedia data", "author": ["S. Cucerzan"], "venue": "In EMNLP-CoNLL,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2007}, {"title": "Knowledge vault: a web-scale approach to probabilistic knowledge fusion", "author": ["X. Dong", "E. Gabrilovich", "G. Heitz", "W. Horn", "N. Lao", "K. Murphy", "T. Strohmann", "S. Sun", "W. Zhang"], "venue": "In SIGKDD,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2014}, {"title": "AMIE: association rule mining under incomplete evidence in ontological knowledge bases", "author": ["L.A. Gal\u00e1rraga", "C. Teflioudi", "K. Hose", "F. Suchanek"], "venue": "In WWW,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2013}, {"title": "Composing Relationships with Translations", "author": ["A. Garc\u00eda-Dur\u00e1n", "A. Bordes", "N. Usunier"], "venue": "EMNLP, pages 286\u2013290,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2015}, {"title": "Deep convolutional ranking for multilabel image annotation", "author": ["Y. Gong", "Y. Jia", "T. Leung", "A. Toshev", "S. Ioffe"], "venue": "arXiv preprint arXiv:1312.4894,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2013}, {"title": "Tagprop: Discriminative metric learning in nearest neighbor models for image auto-annotation", "author": ["M. Guillaumin", "T. Mensink", "J. Verbeek", "C. Schmid"], "venue": "In ICCV,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2009}, {"title": "Noise-contrastive estimation: A new estimation principle for unnormalized statistical models", "author": ["M. Gutmann", "A. Hyv\u00e4rinen"], "venue": "In AISTATS,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2010}, {"title": "Evaluating entity linking with wikipedia", "author": ["B. Hachey", "W. Radford", "J. Nothman", "M. Honnibal", "J.R. Curran"], "venue": "AI, 194:130\u2013150,", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2013}, {"title": "Topic-sensitive pagerank", "author": ["T.H. Haveliwala"], "venue": "In WWW, pages 517\u2013526,", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2002}, {"title": "On Using Very Large Target Vocabulary for Neural Machine Translation", "author": ["S. Jean", "K. Cho", "R. Memisevic", "Y. Bengio"], "venue": null, "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2015}, {"title": "SimRank: a measure of structural context similarity", "author": ["G. Jeh", "J. Widom"], "venue": "In KDD,", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2002}, {"title": "A latent factor model for highly multi-relational data", "author": ["R. Jenatton", "N. Le Roux", "A. Bordes", "G. Obozinski"], "venue": null, "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2012}, {"title": "Locally Adaptive Translation for Knowledge Graph Embedding", "author": ["Y. Jia", "Y. Wang", "H. Lin", "X. Jin", "X. Cheng"], "venue": "In AAAI,", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2016}, {"title": "SemMedDB: a PubMed-scale repository of biomedical semantic predications", "author": ["H. Kilicoglu", "D. Shin", "M. Fiszman", "G. Rosemblat", "T.C. Rindflesch"], "venue": null, "citeRegEx": "20", "shortCiteRegEx": "20", "year": 2012}, {"title": "Adam: A method for stochastic optimization", "author": ["D. Kingma", "J. Ba"], "venue": "arXiv preprint arXiv:1412.6980,", "citeRegEx": "21", "shortCiteRegEx": "21", "year": 2014}, {"title": "Relational retrieval using a combination of path-constrained random walks", "author": ["N. Lao", "W.W. Cohen"], "venue": null, "citeRegEx": "22", "shortCiteRegEx": "22", "year": 2010}, {"title": "DBpedia - a large-scale, multilingual knowledge base extracted from Wikipedia", "author": ["J. Lehmann", "R. Isele", "M. Jakob"], "venue": "Semantic Web,", "citeRegEx": "23", "shortCiteRegEx": "23", "year": 2014}, {"title": "Modeling Relation Paths for Representation Learning of Knowledge Bases", "author": ["Y. Lin", "Z. Liu", "M. Sun"], "venue": null, "citeRegEx": "24", "shortCiteRegEx": "24", "year": 2015}, {"title": "Learning Entity and Relation Embeddings for Knowledge Graph Completion", "author": ["Y. Lin", "Z. Liu", "M. Sun", "Y. Liu", "X. Zhu"], "venue": "In AAAI,", "citeRegEx": "25", "shortCiteRegEx": "25", "year": 2015}, {"title": "Distributed representations of words and phrases and their compositionality", "author": ["T. Mikolov", "I. Sutskever", "K. Chen", "G.S. Corrado", "J. Dean"], "venue": "In NIPS,", "citeRegEx": "26", "shortCiteRegEx": "26", "year": 2013}, {"title": "Compositional Vector Space Models for Knowledge Base Inference", "author": ["A. Neelakantan", "B. Roth", "A. McCallum"], "venue": null, "citeRegEx": "27", "shortCiteRegEx": "27", "year": 2015}, {"title": "A review of relational machine learning for knowledge graphs: From multi-relational link prediction to automated knowledge graph construction", "author": ["M. Nickel", "K. Murphy", "V. Tresp", "E. Gabrilovich"], "venue": "arXiv preprint arXiv:1503.00759,", "citeRegEx": "28", "shortCiteRegEx": "28", "year": 2015}, {"title": "Holographic Embeddings of Knowledge Graphs", "author": ["M. Nickel", "L. Rosasco", "P. Tomaso"], "venue": "In AAAI,", "citeRegEx": "29", "shortCiteRegEx": "29", "year": 2016}, {"title": "A Three-Way Model for Collective Learning on Multi-Relational Data", "author": ["M. Nickel", "V. Tresp", "H.-P. Kriegel"], "venue": "In ICML,", "citeRegEx": "30", "shortCiteRegEx": "30", "year": 2011}, {"title": "A representation theory for ranking functions", "author": ["H.H. Pareek", "P.K. Ravikumar"], "venue": "In NIPS,", "citeRegEx": "31", "shortCiteRegEx": "31", "year": 2014}, {"title": "Fact checking in heterogeneous information networks", "author": ["B. Shi", "T. Weninger"], "venue": "In WWW,", "citeRegEx": "32", "shortCiteRegEx": "32", "year": 2016}, {"title": "Reasoning With Neural Tensor Networks for Knowledge Base Completion", "author": ["R. Socher", "D. Chen", "C.D. Manning", "A.Y. Ng"], "venue": "In NIPS,", "citeRegEx": "33", "shortCiteRegEx": "33", "year": 2013}, {"title": "Template-based question answering over rdf data", "author": ["C. Unger", "L. B\u00fchmann", "J. Lehmann", "A.-C. Ngonga Ngomo", "D. Gerber", "P. Cimiano"], "venue": "In WWW,", "citeRegEx": "34", "shortCiteRegEx": "34", "year": 2012}, {"title": "Knowledge Graph Embedding by Translating on Hyperplanes", "author": ["Z. Wang", "J. Zhang", "J. Feng", "Z. Chen"], "venue": null, "citeRegEx": "35", "shortCiteRegEx": "35", "year": 2014}, {"title": "Representation Learning of Knowledge Graphs with Entity Descriptions", "author": ["R. Xie", "Z. Liu", "J. Jia", "H. Luan", "M. Sun"], "venue": null, "citeRegEx": "36", "shortCiteRegEx": "36", "year": 2016}, {"title": "Bipartite network projection and personal recommendation", "author": ["T. Zhou", "J. Ren", "M. Medo", "Y.-C. Zhang"], "venue": "Phys. Rev. E,", "citeRegEx": "37", "shortCiteRegEx": "37", "year": 2007}], "referenceMentions": [{"referenceID": 33, "context": "Knowledge Graphs (KGs) have become a crucial resource for many tasks in machine learning, data mining, and artificial intelligence applications including question answering [34], entity disambiguation [7], named entity linking [14], fact checking [32], and link prediction [28] to name a few.", "startOffset": 173, "endOffset": 177}, {"referenceID": 6, "context": "Knowledge Graphs (KGs) have become a crucial resource for many tasks in machine learning, data mining, and artificial intelligence applications including question answering [34], entity disambiguation [7], named entity linking [14], fact checking [32], and link prediction [28] to name a few.", "startOffset": 201, "endOffset": 204}, {"referenceID": 13, "context": "Knowledge Graphs (KGs) have become a crucial resource for many tasks in machine learning, data mining, and artificial intelligence applications including question answering [34], entity disambiguation [7], named entity linking [14], fact checking [32], and link prediction [28] to name a few.", "startOffset": 227, "endOffset": 231}, {"referenceID": 31, "context": "Knowledge Graphs (KGs) have become a crucial resource for many tasks in machine learning, data mining, and artificial intelligence applications including question answering [34], entity disambiguation [7], named entity linking [14], fact checking [32], and link prediction [28] to name a few.", "startOffset": 247, "endOffset": 251}, {"referenceID": 27, "context": "Knowledge Graphs (KGs) have become a crucial resource for many tasks in machine learning, data mining, and artificial intelligence applications including question answering [34], entity disambiguation [7], named entity linking [14], fact checking [32], and link prediction [28] to name a few.", "startOffset": 273, "endOffset": 277}, {"referenceID": 2, "context": ", Unstructured [3], TransE [4], TransH [35], and TransR [25], use a margin-based pairwise ranking loss function, which measures the score of each possible result as the Ln-distance between h+ r and t.", "startOffset": 15, "endOffset": 18}, {"referenceID": 3, "context": ", Unstructured [3], TransE [4], TransH [35], and TransR [25], use a margin-based pairwise ranking loss function, which measures the score of each possible result as the Ln-distance between h+ r and t.", "startOffset": 27, "endOffset": 30}, {"referenceID": 34, "context": ", Unstructured [3], TransE [4], TransH [35], and TransR [25], use a margin-based pairwise ranking loss function, which measures the score of each possible result as the Ln-distance between h+ r and t.", "startOffset": 39, "endOffset": 43}, {"referenceID": 24, "context": ", Unstructured [3], TransE [4], TransH [35], and TransR [25], use a margin-based pairwise ranking loss function, which measures the score of each possible result as the Ln-distance between h+ r and t.", "startOffset": 56, "endOffset": 60}, {"referenceID": 7, "context": "Instead of simply adding h + r, more expressive combination operators are learned by Knowledge Vault [8] and HolE [29] in order to predict the existence of \u3008h, r, t\u3009 in the KG.", "startOffset": 101, "endOffset": 104}, {"referenceID": 28, "context": "Instead of simply adding h + r, more expressive combination operators are learned by Knowledge Vault [8] and HolE [29] in order to predict the existence of \u3008h, r, t\u3009 in the KG.", "startOffset": 114, "endOffset": 118}, {"referenceID": 32, "context": "Other models, such as the Neural Tensor Network (NTN) [33] and the Compositional Vector Space Model (CVSM) [27], incorporate a multilayer neural network solution into the existing models.", "startOffset": 54, "endOffset": 58}, {"referenceID": 26, "context": "Other models, such as the Neural Tensor Network (NTN) [33] and the Compositional Vector Space Model (CVSM) [27], incorporate a multilayer neural network solution into the existing models.", "startOffset": 107, "endOffset": 111}, {"referenceID": 9, "context": "Unfortunately, due to their extremely large parameter size, these models either (i) do not scale well or (2) consider only a single relationship at a time [10] thereby limiting their usefulness on large, real-world KGs.", "startOffset": 155, "endOffset": 159}, {"referenceID": 23, "context": "PTransE [24] and RTransE [10] employ extended path information from 2 and 3-hop trails over the knowledge graph.", "startOffset": 8, "endOffset": 12}, {"referenceID": 9, "context": "PTransE [24] and RTransE [10] employ extended path information from 2 and 3-hop trails over the knowledge graph.", "startOffset": 25, "endOffset": 29}, {"referenceID": 35, "context": ", they require pre-trained KG embeddings (RTransE, CVSM), pre-selected paths (PTransE, RTransE), or pre-computed content embeddings of each node (DKRL [36]) before their model training can even begin.", "startOffset": 151, "endOffset": 155}, {"referenceID": 23, "context": ", embeddings) for entities W and relationships W by minimizing a margin-based pairwise ranking loss [24].", "startOffset": 100, "endOffset": 104}, {"referenceID": 3, "context": "The most widely used embedding model in this category is TransE [4], which views relationships as translations from a head entity to a tail entity on the same low-dimensional plane.", "startOffset": 64, "endOffset": 67}, {"referenceID": 2, "context": "The Unstructured model [3] is a special case of TransE where r = 0 for all relationships.", "startOffset": 23, "endOffset": 26}, {"referenceID": 34, "context": "For example, the entity translations in TransH [35] are computed on a hyperplane that is perpendicular to the relationship embedding.", "startOffset": 47, "endOffset": 51}, {"referenceID": 24, "context": "In TransR [25] the entities and relationships are embedded on separate planes and then the entity-vectors are translated to the relationship\u2019s plane.", "startOffset": 10, "endOffset": 14}, {"referenceID": 4, "context": "Structured Embedding (SE) [5] creates two translation matrices for each relationship and applies them", "startOffset": 26, "endOffset": 29}, {"referenceID": 7, "context": "Knowledge Vault [8] and HolE [29], on the other hand, focus on learning a new combination operator instead of simply adding two entity embeddings element-wise.", "startOffset": 16, "endOffset": 19}, {"referenceID": 28, "context": "Knowledge Vault [8] and HolE [29], on the other hand, focus on learning a new combination operator instead of simply adding two entity embeddings element-wise.", "startOffset": 29, "endOffset": 33}, {"referenceID": 18, "context": "Unlike aforementioned models that focus on different E(h, r, t), TransA [19] introduces an adaptive local margin approach that determines \u03b3 by a closed set of entity candidates.", "startOffset": 72, "endOffset": 76}, {"referenceID": 29, "context": "Other similar models include RESCAL [30], Semantic Matching Energy (SME) [3], and the Latent Factor Model (LFM) [18].", "startOffset": 36, "endOffset": 40}, {"referenceID": 2, "context": "Other similar models include RESCAL [30], Semantic Matching Energy (SME) [3], and the Latent Factor Model (LFM) [18].", "startOffset": 73, "endOffset": 76}, {"referenceID": 17, "context": "Other similar models include RESCAL [30], Semantic Matching Energy (SME) [3], and the Latent Factor Model (LFM) [18].", "startOffset": 112, "endOffset": 116}, {"referenceID": 32, "context": "The Neural Tensor Network (NTN) model [33] is an exception to the basic energy function in Eq.", "startOffset": 38, "endOffset": 42}, {"referenceID": 26, "context": "For instance, the Compositional Vector Space Model (CVSM) [27] composes a sequence of relationship embeddings into a single path embedding using a Recurrent Neural Network (RNN).", "startOffset": 58, "endOffset": 62}, {"referenceID": 9, "context": "RTransE [10] solves the relationship-specific problem in CVSM by using entity and relationship embeddings learned from TransE.", "startOffset": 8, "endOffset": 12}, {"referenceID": 23, "context": "PTransE [24] is another path-based method that uses path information in its energy function.", "startOffset": 8, "endOffset": 12}, {"referenceID": 36, "context": "Then PTransE uses PCRA [37] to select input paths within a given length constraint.", "startOffset": 23, "endOffset": 27}, {"referenceID": 30, "context": "Typically there are two ways to obtain such an ordering: with either 1) the pointwise method, or 2) the listwise method [31].", "startOffset": 120, "endOffset": 124}, {"referenceID": 11, "context": "Recently, softmax regression loss has achieved good results in multi-label image annotation tasks [12, 11].", "startOffset": 98, "endOffset": 106}, {"referenceID": 10, "context": "Recently, softmax regression loss has achieved good results in multi-label image annotation tasks [12, 11].", "startOffset": 98, "endOffset": 106}, {"referenceID": 15, "context": "Candidate sampling is not a new problem; many recent works have addressed this problem in interesting ways [16, 26, 13].", "startOffset": 107, "endOffset": 119}, {"referenceID": 25, "context": "Candidate sampling is not a new problem; many recent works have addressed this problem in interesting ways [16, 26, 13].", "startOffset": 107, "endOffset": 119}, {"referenceID": 12, "context": "Candidate sampling is not a new problem; many recent works have addressed this problem in interesting ways [16, 26, 13].", "startOffset": 107, "endOffset": 119}, {"referenceID": 25, "context": "We experimented with many choices, and found that the negative sampling used in Word2Vec [26] resulted the best performance.", "startOffset": 89, "endOffset": 93}, {"referenceID": 19, "context": "The FB15K dataset is a 15,000-entity subset of Freebase; the Semantic MEDLINE Database (SemMedDB) is a KG extracted from all of PubMed [20]; and DBpedia is KG extracted from Wikipedia infoboxes [23].", "startOffset": 135, "endOffset": 139}, {"referenceID": 22, "context": "The FB15K dataset is a 15,000-entity subset of Freebase; the Semantic MEDLINE Database (SemMedDB) is a KG extracted from all of PubMed [20]; and DBpedia is KG extracted from Wikipedia infoboxes [23].", "startOffset": 194, "endOffset": 198}, {"referenceID": 0, "context": "ProjE is implemented in Python using TensorFlow [1]; the code and data are available at https://github.", "startOffset": 48, "endOffset": 51}, {"referenceID": 20, "context": "For both entity and relationship prediction tasks, we use Adam [21] as the stochastic optimizer with default hyper-parameter settings: \u03b21 = 0.", "startOffset": 63, "endOffset": 67}, {"referenceID": 3, "context": "For all tasks, ProjE was trained for at most 100 iterations, and all parameters were initialized from a uniform distribution U [\u2212 6 \u221a k , 6 \u221a k ] as suggested by TransE [4].", "startOffset": 169, "endOffset": 172}, {"referenceID": 3, "context": "We evaluated ProjE\u2019s performance on entity and relationship prediction tasks using the FB15K dataset following the experiment settings in TransE [4] and PTransE [24].", "startOffset": 145, "endOffset": 148}, {"referenceID": 23, "context": "We evaluated ProjE\u2019s performance on entity and relationship prediction tasks using the FB15K dataset following the experiment settings in TransE [4] and PTransE [24].", "startOffset": 161, "endOffset": 165}, {"referenceID": 31, "context": "We use ProjE_wlistwise with a small change: rather than using entity embeddings directly, the input vector of ProjE consists of the predicate paths between the two entities [32].", "startOffset": 173, "endOffset": 177}, {"referenceID": 1, "context": "4 show that ProjE outperforms existing fact checking and link prediction models [2, 6, 17, 9, 15, 22] in all but one question type.", "startOffset": 80, "endOffset": 101}, {"referenceID": 5, "context": "4 show that ProjE outperforms existing fact checking and link prediction models [2, 6, 17, 9, 15, 22] in all but one question type.", "startOffset": 80, "endOffset": 101}, {"referenceID": 16, "context": "4 show that ProjE outperforms existing fact checking and link prediction models [2, 6, 17, 9, 15, 22] in all but one question type.", "startOffset": 80, "endOffset": 101}, {"referenceID": 8, "context": "4 show that ProjE outperforms existing fact checking and link prediction models [2, 6, 17, 9, 15, 22] in all but one question type.", "startOffset": 80, "endOffset": 101}, {"referenceID": 14, "context": "4 show that ProjE outperforms existing fact checking and link prediction models [2, 6, 17, 9, 15, 22] in all but one question type.", "startOffset": 80, "endOffset": 101}, {"referenceID": 21, "context": "4 show that ProjE outperforms existing fact checking and link prediction models [2, 6, 17, 9, 15, 22] in all but one question type.", "startOffset": 80, "endOffset": 101}], "year": 2016, "abstractText": "With the large volume of new information created every day, determining the validity of information in a knowledge graph and filling in its missing parts are crucial tasks for many researchers and practitioners. To address this challenge, a number of knowledge graph completion methods have been developed using low-dimensional graph embeddings. Although researchers continue to improve these models using an increasingly complex feature space, we show that simple changes in the architecture of the underlying model can outperform state-of-the-art models without the need for complex feature engineering. In this work, we present a shared variable neural network model called ProjE that fills-in missing information in a knowledge graph by learning joint embeddings of the knowledge graph\u2019s entities and edges, and through subtle, but important, changes to the standard loss function. In doing so, ProjE has a parameter size that is smaller than 11 out of 15 existing methods while performing 37% better than the current-best method on standard datasets. We also show, via a new fact checking task, that ProjE is capable of accurately determining the veracity of many declarative statements. Knowledge Graphs (KGs) have become a crucial resource for many tasks in machine learning, data mining, and artificial intelligence applications including question answering [34], entity disambiguation [7], named entity linking [14], fact checking [32], and link prediction [28] to name a few. In our view, KGs are an example of a heterogeneous information network containing entity-nodes and relationship-edges corresponding to RDF-style triples \u3008h, r, t\u3009 where h represents a head entity, and r is a relationship that connects h to a tail entity t. KGs are widely used for many practical tasks, however, their correctness and completeness are not guaranteed. Therefore, it is necessary to develop knowledge graph completion (KGC) methods to find missing or errant relationships with the goal of improving the general quality of KGs, which, in turn, can be used to improve or create interesting downstream applications. The KGC task can be divided into two non-mutually exclusive sub-tasks: (i) entity prediction and (ii) relationship prediction. The entity prediction task takes a partial triple \u3008h, r, ?\u3009 as input and produces a ranked list of candidate entities as output: Definition 1. (Entity Ranking Problem) Given a Knowledge Graph G = {E,R} and an input triple \u3008h, r, ?\u3009, the entity ranking problem attempts to find the optimal ordered list such that \u2200ej\u2200ei ((ej \u2208 E\u2212 \u2227 ei \u2208 E+)\u2192 ei \u227a ej), where E+ = {e \u2208 {e1, e2, . . . , el}|\u3008h, r, e\u3009 \u2208 G} and E\u2212 = {e \u2208 {el+1, el+2, . . . , e|E|}|\u3008h, r, e\u3009 / \u2208 G}. Distinguishing between head and tail-entities is usually arbitrary, so we can easily substitute \u3008h, r, ?\u3009 for \u3008?, r, t\u3009. The relationship prediction task aims to find a ranked list of relationships that connect a head-entity with a tail-entity, i.e., \u3008h, ?, t\u3009. When discussing the details of the present work, we focus specifically on the entity prediction task; however, it is straightforward to adapt the methodology to the relationship prediction task by changing the input. A number of KGC algorithms have been developed in recent years, and the most successful models all have one thing in common: they use low-dimensional embedding vectors to represent entities and relationships. Many embedding models, e.g., Unstructured [3], TransE [4], TransH [35], and TransR [25], use a margin-based pairwise ranking loss function, which measures the score of each possible result as the Ln-distance between h+ r and t. In these models the loss functions are all the same, so models differ in how they transform the 1 ar X iv :1 61 1. 05 42 5v 1 [ cs .A I] 1 6 N ov 2 01 6 entity embeddings h and t with respect to the relationship embeddings r. Instead of simply adding h + r, more expressive combination operators are learned by Knowledge Vault [8] and HolE [29] in order to predict the existence of \u3008h, r, t\u3009 in the KG. Other models, such as the Neural Tensor Network (NTN) [33] and the Compositional Vector Space Model (CVSM) [27], incorporate a multilayer neural network solution into the existing models. Unfortunately, due to their extremely large parameter size, these models either (i) do not scale well or (2) consider only a single relationship at a time [10] thereby limiting their usefulness on large, real-world KGs. Despite their large model size, the aforementioned methods only use singleton triples, i.e., length-1 paths in the KG. PTransE [24] and RTransE [10] employ extended path information from 2 and 3-hop trails over the knowledge graph. These extended models achieve excellent performance due to the richness of the input data; unfortunately, their model-size grows exponentially as the path-length increases, which further exacerbates the scalability issues associated with the already high number of parameters of the underlying-models. Another curious finding is that some of the existing models are not self-contained models, i.e., they require pre-trained KG embeddings (RTransE, CVSM), pre-selected paths (PTransE, RTransE), or pre-computed content embeddings of each node (DKRL [36]) before their model training can even begin. TransR and TransH are self-contained models, but their experiments only report results using pre-trained TransE embeddings as input. With these considerations in mind, in the present work we rethink some of the basic decisions made by previous models to create a projection embedding model (ProjE) for KGC. ProjE has four parts that distinguish it from the related work: 1. Instead of measuring the distance between input triple \u3008h, r, ?\u3009 and entity candidates on a unified or a relationship-specific plane, we choose to project the entity candidates onto a target vector representing the input data. 2. Unlike existing models that use transformation matrices, we combine the embedding vectors representing the input data into a target vector using a learnable combination operator. This avoids the addition of a large number of transformation matrices by reusing the entity-embeddings. 3. Rather than optimizing the margin-based pairwise ranking loss, we optimize a ranking loss of the list of candidate-entities (or relationships) collectively. We further use candidate sampling to handle very large data sets. 4. Unlike many of the related models that require pre-trained data from prerequisite models or explore expensive multi-hop paths through the knowledge graph, ProjE is a self-contained model over length-1 edges.", "creator": "LaTeX with hyperref package"}}}