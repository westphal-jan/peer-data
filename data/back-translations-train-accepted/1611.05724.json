{"id": "1611.05724", "review": {"conference": "AAAI", "VERSION": "v1", "DATE_OF_SUBMISSION": "17-Nov-2016", "title": "Unimodal Thompson Sampling for Graph-Structured Arms", "abstract": "We study, to the best of our knowledge, the first Bayesian algorithm for unimodal Multi-Armed Bandit (MAB) problems with graph structure. In this setting, each arm corresponds to a node of a graph and each edge provides a relationship, unknown to the learner, between two nodes in terms of expected reward. Furthermore, for any node of the graph there is a path leading to the unique node providing the maximum expected reward, along which the expected reward is monotonically increasing. Previous results on this setting describe the behavior of frequentist MAB algorithms. In our paper, we design a Thompson Sampling-based algorithm whose asymptotic pseudo-regret matches the lower bound for the considered setting. We show that -as it happens in a wide number of scenarios- Bayesian MAB algorithms dramatically outperform frequentist ones. In particular, we provide a thorough experimental evaluation of the performance of our and state-of-the-art algorithms as the properties of the graph vary.", "histories": [["v1", "Thu, 17 Nov 2016 14:59:55 GMT  (805kb,D)", "https://arxiv.org/abs/1611.05724v1", null], ["v2", "Tue, 22 Nov 2016 10:13:02 GMT  (804kb,D)", "http://arxiv.org/abs/1611.05724v2", null]], "reviews": [], "SUBJECTS": "cs.LG stat.ML", "authors": ["stefano paladino", "francesco trov\u00f2", "marcello restelli", "nicola gatti 0001"], "accepted": true, "id": "1611.05724"}, "pdf": {"name": "1611.05724.pdf", "metadata": {"source": "CRF", "title": "Unimodal Thompson Sampling for Graph\u2013Structured Arms", "authors": ["Stefano Paladino", "Marcello Restelli", "Nicola Gatti"], "emails": ["nicola.gatti}@polimi.it"], "sections": [{"heading": "Introduction", "text": "In this context, the objective of an MAB algorithm in each round over a finite horizon is T, the learner selects an action (commonly referred to as an arm) from a finite set and observes only the reward corresponding to the selection he has made. The goal of an MAB algorithm is to adapt to the optimal arm, i.e., the learner selects an action (usually referred to as an arm) while minimizing the loss in the learning process and thus measuring his performance by his expected regret, defined as the difference between the expected reward achieved by an oracle algorithm and that achieved by the considered algorithm. We focus on the so-called unimmodal reward MAB (UMAB) introduced in (Combes and Prouets 2014a), in which each corresponds to a diagram and each edge is associated with a partial relationship."}, {"heading": "Related work", "text": "Some work deals with unimodal reward functions in the field of continuous armed bandits (Jia and Mannor 2011; Combes and Proutiere 2014b; Kleinberg, Slivkins and Upfal 2008). In (Jia and Mannor 2011) it is proposed to develop a successive elimination algorithm called LSE that achieves the regret of O (\u221a T log T). In this case, assumptions about the minimum local decrease and increase of the expected reward are required. Combes and Proutiere (2014b) consider stochastic bandit problems with a continuous group of weapons and where the expected reward is a continuous and unimodal function of the arm. They propose the SP algorithm based on the stochastic pentachotomy method to narrow the search space. Unimodal MABs in metric areas are investigated in the area (Kleinberg, Slivkins and Upfal 2008)."}, {"heading": "Problem Formulation", "text": "The blanks know the nodes and the edges (i.e., they know the graph), but for each edge they do not know what the node of the edge with the largest expected reward is (i.e., they do not know whether the node of the edge with the largest expected reward is (i.e., they do not know what the ordering relationship looks like). In each round t over a time horizon of T: N the learner selects a corresponding reward xi, t. This reward is drawn from an i.e. random variable Xi, t (i.e., we are looking at a stochastic MAB setting)."}, {"heading": "The UTS algorithm", "text": "We describe the UTS algorithm and show that its regret is asymptotically optimal, i.e. asymptotically consistent with the lower limit of theorem 1. The algorithm is an extension of Thompson Sampling (Thompson 1933), which exploits the graph structure and the unimodal property of the UMAB setting. Essentially, the basis of the algorithm is to apply a simple variation of the TS algorithm only to the arms associated with the nodes that make up the neighborhood of the arm with the highest empirical mean reward, the so-called leader."}, {"heading": "The UTS pseudo\u2013code", "text": "The algorithm receives the graph structure G, the time horizon T, and an estimated reward i that we expect. (UTS 1: Input: UMAB Setting G = (V, E), Horizon T, Priors {\u03c0i} Ki = 12: for t {1,.., T} do 3: Compute \u00b5, Ti, t for each i,., K} 4: Find the leader al (t) 5: if Ll (t), t mod | N (t)), 0 then 6: Collect reward xl (t), t 7: else 8: Draw, t from \u03c0i, t from \u03c0i, t for each i (t), t for each i, l (t) 9: Collect reward xit, t = arg maxi \u03b8i., tThe pseudo-code of the UTS algorithm is presented in Algorithm 1. The algorithm receives in input the graph structure G, the expected horizon G, Bayesian and a reward T."}, {"heading": "Finite\u2013time analysis of UTS", "text": "Theoretically, the use of UMAB via the expected reward in OSUB, which is limited in time and which we have most likely ordered, is not possible."}, {"heading": "Experimental Evaluation", "text": "In this section, we compare the empirical performance of the proposed algorithm UTS with the performance of a number of algorithms (URT 2 = URT 2). We examine the performance of the state algorithm OSUB (Combes and Proutiere 2014a) to assess the improvement due to the employment of Bayesian approaches. We also look at the performance of TS (Thompson 1933) to assess the improvement of Bayesian approaches due to the exploitation of the problem structure. For completeness, we also examine the performance of CLUCB (Garivier and Cappe), using a frequency algorithm that is optimal for Bernoulli distributions. Figures of earnings In the face of a policy, we evaluate the average and 95% confidence intervals of the following numbers of merit: \u2022 the pseudo-regret RT (U) as defined in Equation 1; the lower RT (U) the better performance."}, {"heading": "Conclusions and Future Work", "text": "In this paper, we focus on the Unimodal Multi-Armed Bandit problem with the graph structure, in which each arm corresponds to a node of a graph and each edge is associated with a relationship to the expected reward between its arms. To the best of our knowledge, we propose the first Bayesian algorithm for the UMAB setting, called UTS, which is based on the well-known Thompson sampling algorithm. We derive a narrow upper limit for UTS, which asymptotically corresponds to the lower limit for the UMAB setting, providing a non-trivial derivation of the limit. Furthermore, we present a thorough experimental analysis showing that our algorithm outperforms the most modern methods. In the future, we will evaluate the performance of the algorithms included in this work with other classes of graphs, such as Baraba \u00b2 si-Albert and Lattices. Future development of this work could anticipate an analysis of the proposed algorithm in the case of time-varying environments, which will take into account unpleasant time structure, with additional reward structure."}, {"heading": "Appendix A: Proof of Theorem 2", "text": "Theorem 2: Given an UMAB setting G = (A, E), the expected pseudo-repentance of the UTS algorithm (max.) is satisfactory, for each digit (T) > 0: \"RT\" is a constant depending on the number of arms K and the expected rewards {1,.., \"K.\" Proof. First, the regret of the UTS algorithm RT (UTS) can be rewritten by dividing the T rounds into two sentences: those rounds in which the best arm is a leader, i.e. l (t) = \"i,\" and those in which the leader is another arm, i.e., l \"We\" (t) l. \"(t):\" RT (UTS) = i. \""}, {"heading": "Appendix B: Additional Results on p = `", "text": "To investigate why the performance of UTS and the performance of OSUB are asymptotically consistent with the behavior of K = > 0.001 = 0.001, we produce additional experiments with the line graphs described in Combes and Proutiere 2014a. We created line graphs where the minimum expected reward is set to 0.1 and the maximum expected reward varies: Given an edge with the two consecutive nodes i and i + 1, the expected reward of the central arm a8 is set to 0.108, 0.116, and 0.14 when K = 129, the expected reward of the central arm a65 to 0.165, 0.23, and 0.425. The results for T = 107 are set to 0.165, 0.23, and 0.425 in Figure 4 for K = 17, and in Figure 5 for K = 129.We note that the expected reward of the central arm a65 is set to 0.425."}], "references": [{"title": "From bandits to experts: A tale of domination and independence", "author": ["N. Alon", "N. Cesa-Bianchi", "C. Gentile", "Y. Mansour"], "venue": "Advances in Neural Information Processing Systems, 1610\u20131618.", "citeRegEx": "Alon et al\\.,? 2013", "shortCiteRegEx": "Alon et al\\.", "year": 2013}, {"title": "Finitetime analysis of the multiarmed bandit problem", "author": ["P. Auer", "N. Cesa-Bianchi", "P. Fischer"], "venue": "Machine learning 47(2-3):235\u2013256.", "citeRegEx": "Auer et al\\.,? 2002", "shortCiteRegEx": "Auer et al\\.", "year": 2002}, {"title": "Mixing bandits: A recipe for improved cold-start recommendations in a social network", "author": ["S. Caron", "S. Bhagat"], "venue": "Proceedings of the 7th Workshop on Social Network Mining and Analysis, 11. ACM.", "citeRegEx": "Caron and Bhagat,? 2013", "shortCiteRegEx": "Caron and Bhagat", "year": 2013}, {"title": "An empirical evaluation of thompson sampling", "author": ["O. Chapelle", "L. Li"], "venue": "Advances in neural information processing systems, 2249\u20132257.", "citeRegEx": "Chapelle and Li,? 2011", "shortCiteRegEx": "Chapelle and Li", "year": 2011}, {"title": "Unimodal bandits: Regret lower bounds and optimal algorithms", "author": ["R. Combes", "A. Proutiere"], "venue": "ICML, 521\u2013 529.", "citeRegEx": "Combes and Proutiere,? 2014a", "shortCiteRegEx": "Combes and Proutiere", "year": 2014}, {"title": "Unimodal bandits without smoothness", "author": ["R. Combes", "A. Proutiere"], "venue": "arXiv preprint arXiv:1406.7447.", "citeRegEx": "Combes and Proutiere,? 2014b", "shortCiteRegEx": "Combes and Proutiere", "year": 2014}, {"title": "Feedback effects between similarity and social influence in online communities", "author": ["D. Crandall", "D. Cosley", "D. Huttenlocher", "J. Kleinberg", "S. Suri"], "venue": "Proceedings of the 14th ACM SIGKDD international conference on Knowledge discovery and data mining, 160\u2013168. ACM.", "citeRegEx": "Crandall et al\\.,? 2008", "shortCiteRegEx": "Crandall et al\\.", "year": 2008}, {"title": "Strategic bidder behavior in sponsored search auctions", "author": ["B. Edelman", "M. Ostrovsky"], "venue": "Decision support systems 43(1):192\u2013198.", "citeRegEx": "Edelman and Ostrovsky,? 2007", "shortCiteRegEx": "Edelman and Ostrovsky", "year": 2007}, {"title": "On random graphs i", "author": ["P. Erd\u0151s", "A. R\u00e9nyi"], "venue": "Publ. Math. Debrecen 6:290\u2013297.", "citeRegEx": "Erd\u0151s and R\u00e9nyi,? 1959", "shortCiteRegEx": "Erd\u0151s and R\u00e9nyi", "year": 1959}, {"title": "The kl-ucb algorithm for bounded stochastic bandits and beyond", "author": ["A. Garivier", "O. Capp\u00e9"], "venue": "COLT, 359\u2013376.", "citeRegEx": "Garivier and Capp\u00e9,? 2011", "shortCiteRegEx": "Garivier and Capp\u00e9", "year": 2011}, {"title": "Probability inequalities for sums", "author": ["W. Hoeffding"], "venue": null, "citeRegEx": "Hoeffding,? \\Q1963\\E", "shortCiteRegEx": "Hoeffding", "year": 1963}, {"title": "Unimodal bandits", "author": ["Y.Y. Jia", "S. Mannor"], "venue": "Proceedings of the 28th International Conference on Machine Learning (ICML-11), 41\u201348.", "citeRegEx": "Jia and Mannor,? 2011", "shortCiteRegEx": "Jia and Mannor", "year": 2011}, {"title": "Thompson sampling: An asymptotically optimal finite-time analysis", "author": ["E. Kaufmann", "N. Korda", "R. Munos"], "venue": "ALT, volume 7568 of Lecture Notes in Computer Science, 199\u2013213. Springer.", "citeRegEx": "Kaufmann et al\\.,? 2012", "shortCiteRegEx": "Kaufmann et al\\.", "year": 2012}, {"title": "Multi-armed bandits in metric spaces", "author": ["R. Kleinberg", "A. Slivkins", "E. Upfal"], "venue": "Proceedings of the fortieth annual ACM symposium on Theory of computing, 681\u2013690. ACM.", "citeRegEx": "Kleinberg et al\\.,? 2008", "shortCiteRegEx": "Kleinberg et al\\.", "year": 2008}, {"title": "Asymptotically efficient adaptive allocation rules", "author": ["T.L. Lai", "H. Robbins"], "venue": "Advances in applied mathematics 6(1):4\u201322.", "citeRegEx": "Lai and Robbins,? 1985", "shortCiteRegEx": "Lai and Robbins", "year": 1985}, {"title": "From bandits to experts: On the value of side-observations", "author": ["S. Mannor", "O. Shamir"], "venue": "NIPS. 684\u2013692.", "citeRegEx": "Mannor and Shamir,? 2011", "shortCiteRegEx": "Mannor and Shamir", "year": 2011}, {"title": "Birds of a feather: Homophily in social networks", "author": ["M. McPherson", "L. Smith-Lovin", "J.M. Cook"], "venue": "Annual review of sociology 415\u2013444.", "citeRegEx": "McPherson et al\\.,? 2001", "shortCiteRegEx": "McPherson et al\\.", "year": 2001}, {"title": "On the likelihood that one unknown probability exceeds another in view of the evidence of two samples", "author": ["W.R. Thompson"], "venue": "Biometrika 25(3/4):285\u2013294.", "citeRegEx": "Thompson,? 1933", "shortCiteRegEx": "Thompson", "year": 1933}, {"title": "Multi\u2013armed bandit for pricing", "author": ["F. Trov\u00f2", "S. Paladino", "M. Restelli", "N. Gatti"], "venue": "2th European Workshop on Reinforcement Learning (EWRL). https://ewrl. wordpress.com/past-ewrl/ewrl12-2015/.", "citeRegEx": "Trov\u00f2 et al\\.,? 2015", "shortCiteRegEx": "Trov\u00f2 et al\\.", "year": 2015}, {"title": "Spectral bandits for smooth graph functions", "author": ["M. Valko", "R. Munos", "B. Kveton", "T. Kocak"], "venue": "Proceedings of The 31st International Conference on Machine Learning, ICML, 46\u201354.", "citeRegEx": "Valko et al\\.,? 2014", "shortCiteRegEx": "Valko et al\\.", "year": 2014}, {"title": "Thompson sampling for budgeted multi-armed bandits", "author": ["Y. Xia", "H. Li", "T. Qin", "N. Yu", "T.-Y. Liu"], "venue": "TwentyFourth International Joint Conference on Artificial Intelligence.", "citeRegEx": "Xia et al\\.,? 2015", "shortCiteRegEx": "Xia et al\\.", "year": 2015}], "referenceMentions": [{"referenceID": 4, "context": "We focus on the so\u2013called Unimodal MAB (UMAB), introduced in (Combes and Proutiere 2014a), in which each arm corresponds to a node of a graph and each edge is associated with a relationship specifying which node of the edge gives the largest expected reward (providing thus a partial ordering over the arm space).", "startOffset": 61, "endOffset": 89}, {"referenceID": 6, "context": ", neighbor nodes in the graph), therefore interests of people in a social network change smoothly and neighboring nodes in the graph look similar to each other (McPherson, Smith-Lovin, and Cook 2001; Crandall et al. 2008).", "startOffset": 160, "endOffset": 221}, {"referenceID": 11, "context": ", in sequential pricing (Jia and Mannor 2011), bidding in online sponsored search auctions (Edelman and Ostrovsky 2007) and single\u2013peak preferences economics and voting settings (Mas-Collel, Whinston, and Green 1995), are graph\u2013structured problems in which the graph is a line.", "startOffset": 24, "endOffset": 45}, {"referenceID": 7, "context": ", in sequential pricing (Jia and Mannor 2011), bidding in online sponsored search auctions (Edelman and Ostrovsky 2007) and single\u2013peak preferences economics and voting settings (Mas-Collel, Whinston, and Green 1995), are graph\u2013structured problems in which the graph is a line.", "startOffset": 91, "endOffset": 119}, {"referenceID": 11, "context": "are proposed in (Jia and Mannor 2011) and (Combes and Proutiere 2014a).", "startOffset": 16, "endOffset": 37}, {"referenceID": 4, "context": "are proposed in (Jia and Mannor 2011) and (Combes and Proutiere 2014a).", "startOffset": 42, "endOffset": 70}, {"referenceID": 3, "context": ", in bandit problems without structure (Chapelle and Li 2011) and in bandit problems with budget (Xia et al.", "startOffset": 39, "endOffset": 61}, {"referenceID": 20, "context": ", in bandit problems without structure (Chapelle and Li 2011) and in bandit problems with budget (Xia et al. 2015)).", "startOffset": 97, "endOffset": 114}, {"referenceID": 3, "context": "are proposed in (Jia and Mannor 2011) and (Combes and Proutiere 2014a). Jia and Mannor (2011) introduce the GLSE algorithm with a regret of order O( \u221a T log(T )).", "startOffset": 43, "endOffset": 94}, {"referenceID": 3, "context": "are proposed in (Jia and Mannor 2011) and (Combes and Proutiere 2014a). Jia and Mannor (2011) introduce the GLSE algorithm with a regret of order O( \u221a T log(T )). However, GLSE performs better than classical bandit algorithms only when the number of arms is \u0398(T ). Combes and Proutiere (2014a) present the OSUB algorithm\u2014based on KLUCB\u2014achieving asymptotic regret ofO(log(T )) and outperforming GLSE in settings with a few arms.", "startOffset": 43, "endOffset": 294}, {"referenceID": 11, "context": "Some works deal with unimodal reward functions in continuous armed bandit setting (Jia and Mannor 2011; Combes and Proutiere 2014b; Kleinberg, Slivkins, and Upfal 2008).", "startOffset": 82, "endOffset": 168}, {"referenceID": 5, "context": "Some works deal with unimodal reward functions in continuous armed bandit setting (Jia and Mannor 2011; Combes and Proutiere 2014b; Kleinberg, Slivkins, and Upfal 2008).", "startOffset": 82, "endOffset": 168}, {"referenceID": 11, "context": "In (Jia and Mannor 2011) a successive elimination algorithm, called LSE, is proposed achieving regret of O( \u221a T log T ).", "startOffset": 3, "endOffset": 24}, {"referenceID": 19, "context": "An application\u2013dependent solution to the recommendation systems which exploits the similarity of the graph in social network in targeted advertisement has been proposed in (Valko et al. 2014).", "startOffset": 172, "endOffset": 191}, {"referenceID": 2, "context": "Similar information has been considered in (Caron and Bhagat 2013) where the problem of cold\u2013start users (i.", "startOffset": 43, "endOffset": 66}, {"referenceID": 18, "context": "Another type of structure considered in sequential games is the one of monotonicity of the conversion rate in the price (Trov\u00f2 et al. 2015).", "startOffset": 120, "endOffset": 139}, {"referenceID": 0, "context": "In (Alon et al. 2013; Mannor and Shamir 2011), a graph structure of the arm feedback in an adversarial setting is studied.", "startOffset": 3, "endOffset": 45}, {"referenceID": 15, "context": "In (Alon et al. 2013; Mannor and Shamir 2011), a graph structure of the arm feedback in an adversarial setting is studied.", "startOffset": 3, "endOffset": 45}, {"referenceID": 2, "context": "Some works deal with unimodal reward functions in continuous armed bandit setting (Jia and Mannor 2011; Combes and Proutiere 2014b; Kleinberg, Slivkins, and Upfal 2008). In (Jia and Mannor 2011) a successive elimination algorithm, called LSE, is proposed achieving regret of O( \u221a T log T ). In this case, assumptions over the minimum local decrease and increase of the expected reward is required. Combes and Proutiere (2014b) consider stochastic bandit problems with a continuous set of arms and where the expected reward is a continuous and unimodal function of the arm.", "startOffset": 104, "endOffset": 427}, {"referenceID": 4, "context": "shown in (Combes and Proutiere 2014a) that the problem of learning in a UMAB setting presents a lower bound over the regret RT (U) of the following form: Theorem 1.", "startOffset": 9, "endOffset": 37}, {"referenceID": 14, "context": "This result is similar to the one provided in (Lai and Robbins 1985), with the only difference that the summation is restricted to the arms laying in the neighborhood of the optimal arm N(i\u2217) and reduces to it when the optimal arm is connected to all the others (i.", "startOffset": 46, "endOffset": 68}, {"referenceID": 17, "context": "The algorithm is an extension of the Thompson Sampling (Thompson 1933) that exploits the graph structure and the unimodal property of the UMAB setting.", "startOffset": 55, "endOffset": 70}, {"referenceID": 10, "context": "Ri1 can be upper bounded by a constant by relying on conditional probability definition and the Hoeffding inequality (Hoeffding 1963).", "startOffset": 117, "endOffset": 133}, {"referenceID": 4, "context": "We study the performance of the state\u2013 of\u2013the\u2013art algorithm OSUB (Combes and Proutiere 2014a) to evaluate the improvement due to the employment of Bayesian approaches w.", "startOffset": 65, "endOffset": 93}, {"referenceID": 17, "context": "Furthermore, we study the performance of TS (Thompson 1933) to evaluate the improvement in Bayesian approaches due to the exploitation of the problem structure.", "startOffset": 44, "endOffset": 59}, {"referenceID": 9, "context": "For completeness, we study also the performance of KLUCB (Garivier and Capp\u00e9 2011), being a frequentist algorithm that is optimal for Bernoulli distributions.", "startOffset": 57, "endOffset": 82}, {"referenceID": 4, "context": "Line graphs We initially consider the same experimental settings, composed of line graphs, that are studied in (Combes and Proutiere 2014a).", "startOffset": 111, "endOffset": 139}, {"referenceID": 4, "context": "Figure 1: Results for the pseudo\u2013regret Rt(U) in line graphs settings with K = 17 (a) and K = 129 (b) as defined in (Combes and Proutiere 2014a).", "startOffset": 116, "endOffset": 144}, {"referenceID": 8, "context": "Erd\u0151s-R\u00e9nyi graphs To provide a thorough experimental evaluation of the considered algorithms in settings in which the space of arms has a graph structure, we generate graphs using the model proposed by Erd\u0151s and R\u00e9nyi (1959), which allows us to simulate graph structures more complex than a simple line.", "startOffset": 203, "endOffset": 226}, {"referenceID": 4, "context": "In order to investigate the reasons behind such a behavior, we produce an additional experiment with the line graphs of Combes and Proutiere (2014a) except that the maximum expected reward is set to 0.", "startOffset": 120, "endOffset": 149}], "year": 2016, "abstractText": "We study, to the best of our knowledge, the first Bayesian algorithm for unimodal Multi\u2013Armed Bandit (MAB) problems with graph structure. In this setting, each arm corresponds to a node of a graph and each edge provides a relationship, unknown to the learner, between two nodes in terms of expected reward. Furthermore, for any node of the graph there is a path leading to the unique node providing the maximum expected reward, along which the expected reward is monotonically increasing. Previous results on this setting describe the behavior of frequentist MAB algorithms. In our paper, we design a Thompson Sampling\u2013based algorithm whose asymptotic pseudo\u2013regret matches the lower bound for the considered setting. We show that\u2014as it happens in a wide number of scenarios\u2014Bayesian MAB algorithms dramatically outperform frequentist ones. In particular, we provide a thorough experimental evaluation of the performance of our and state\u2013 of\u2013the\u2013art algorithms as the properties of the graph vary. Introduction Multi\u2013Armed Bandit (MAB) algorithms (Auer, CesaBianchi, and Fischer 2002) have been proven to provide effective solutions for a wide range of applications fitting the sequential decisions making scenario. In this framework, at each round over a finite horizon T , the learner selects an action (usually called arm) from a finite set and observes only the reward corresponding to the choice she made. The goal of a MAB algorithm is to converge to the optimal arm, i.e., the one with the highest expected reward, while minimizing the loss incurred in the learning process and, therefore, its performance is measured through its expected regret, defined as the difference between the expected reward achieved by an oracle algorithm always selecting the optimal arm and the one achieved by the considered algorithm. We focus on the so\u2013called Unimodal MAB (UMAB), introduced in (Combes and Proutiere 2014a), in which each arm corresponds to a node of a graph and each edge is associated with a relationship specifying which node of the edge gives the largest expected reward (providing thus a partial ordering over the arm space). Furthermore, from any node there is a path leading to the unique node with the maximum expected reward along which the expected reward is monotonically Copyright c \u00a9 2017, Association for the Advancement of Artificial Intelligence (www.aaai.org). All rights reserved. increasing. While the graph structure may be (not necessarily) known a priori by the UMAB algorithm, the relationship defined over the edges is discovered during the learning. In the present paper, we propose a novel algorithm relying on the Bayesian learning approach for a generic UMAB setting. Models presenting a graph structure have become more and more interesting in last years due to the spread of social networks. Indeed, the relationships among the entities of a social network have a natural graph structure. A practical problem in this scenario is the targeted advertisement problem, whose goal is to discover the part of the network that is interested in a given product. This task is heavily influenced by the graph structure, since in social networks people tend to have similar characteristics to those of their friends (i.e., neighbor nodes in the graph), therefore interests of people in a social network change smoothly and neighboring nodes in the graph look similar to each other (McPherson, Smith-Lovin, and Cook 2001; Crandall et al. 2008). More specifically, an advertiser aims at finding those users that maximize the ad expected revenue (i.e., the product between click probability and value per click), while at the same time reducing the amount of times the advertisement is presented to people not interested in its content. Under the assumption of unimodal expected reward, the learner can move from low expected rewards to high ones just by climbing them in the graph, preventing from the need of a uniform exploration over all the graph nodes. This assumption reduces the complexity in the search for the optimal arm, since the learning algorithm can avoid to pull the arms corresponding to some subset of non\u2013 optimal nodes, reducing thus the regret. Other applications might benefit from this structure, e.g., recommender systems which aims at coupling items with those users are likely to enjoy them. Similarly, the use of the unimodal graph structure might provide more meaningful recommendations without testing all the users in the social network. Finally, notice that unimodal problems with a single variable, e.g., in sequential pricing (Jia and Mannor 2011), bidding in online sponsored search auctions (Edelman and Ostrovsky 2007) and single\u2013peak preferences economics and voting settings (Mas-Collel, Whinston, and Green 1995), are graph\u2013structured problems in which the graph is a line. Frequentist approaches for UMAB with graph structure ar X iv :1 61 1. 05 72 4v 2 [ cs .L G ] 2 2 N ov 2 01 6 are proposed in (Jia and Mannor 2011) and (Combes and Proutiere 2014a). Jia and Mannor (2011) introduce the GLSE algorithm with a regret of order O( \u221a T log(T )). However, GLSE performs better than classical bandit algorithms only when the number of arms is \u0398(T ). Combes and Proutiere (2014a) present the OSUB algorithm\u2014based on KLUCB\u2014achieving asymptotic regret ofO(log(T )) and outperforming GLSE in settings with a few arms. To the best of our knowledge, no Bayesian approach has been proposed for unimodal bandit settings, included the UMAB setting we study. However, it is well known that Bayesian MAB algorithms\u2014the most popular is Thompson Sampling (TS)\u2014usually suffer of same order of regret as the best frequentist one (e.g., in unstructured settings (Kaufmann, Korda, and Munos 2012)), but they outperform the frequentist methods in a wide range of problems (e.g., in bandit problems without structure (Chapelle and Li 2011) and in bandit problems with budget (Xia et al. 2015)). Furthermore, in problems with structure, the classical Thompson Sampling (not exploiting the problem structure) may outperform frequentist algorithms exploiting the problem structure. For this reason, in this paper we explore Bayesian approaches for the UMAB setting. More precisely, we provide the following original contributions: \u2022 we design a novel Bayesian MAB algorithm, called UTS and based on the TS algorithm; \u2022 we derive a tight upper bound over the pseudo\u2013regret for UTS, which asymptotically matches the lower bound for the UMAB setting; \u2022 we describe a wide experimental campaign showing better performance of UTS in applicative scenarios than those of state\u2013of\u2013the\u2013art algorithms, evaluating also how the performance of the algorithms (ours and of the state of the art) varies as the graph structure properties vary. Related work Here, we mention the main works related to ours. Some works deal with unimodal reward functions in continuous armed bandit setting (Jia and Mannor 2011; Combes and Proutiere 2014b; Kleinberg, Slivkins, and Upfal 2008). In (Jia and Mannor 2011) a successive elimination algorithm, called LSE, is proposed achieving regret of O( \u221a T log T ). In this case, assumptions over the minimum local decrease and increase of the expected reward is required. Combes and Proutiere (2014b) consider stochastic bandit problems with a continuous set of arms and where the expected reward is a continuous and unimodal function of the arm. They propose the SP algorithm, based on the stochastic pentachotomy procedure to narrow the search space. Unimodal MABs on metric spaces are studied in (Kleinberg, Slivkins, and Upfal 2008). An application\u2013dependent solution to the recommendation systems which exploits the similarity of the graph in social network in targeted advertisement has been proposed in (Valko et al. 2014). Similar information has been considered in (Caron and Bhagat 2013) where the problem of cold\u2013start users (i.e., new users) is studied. Another type of structure considered in sequential games is the one of monotonicity of the conversion rate in the price (Trov\u00f2 et al. 2015). Interestingly, the assumptions of monotonicity and unimodality are orthogonal, none of them being a special case of the other, therefore the results for monotonic setting cannot be used in unimodal bandits. In (Alon et al. 2013; Mannor and Shamir 2011), a graph structure of the arm feedback in an adversarial setting is studied. More precisely, they assume to have correlation over rewards and not over the expected values of arms. Problem Formulation A learner receives in input a finite undirected graph MAB settingG = (A,E), whose verticesA = {a1, . . . , aK} with K \u2208 N correspond to the arms and an edge (aiaj) \u2208 E exists only if there is a direct partial order relationship between the expected rewards of arms ai and aj . The leaner knows a priori the nodes and the edges (i.e., she knows the graph), but, for each edge, she does not know a priori which is the node of the edge with the largest expected reward (i.e., she does not know the ordering relationship). At each round t over a time horizon of T \u2208 N the learner selects an arm ai and gains the corresponding reward xi,t. This reward is drawn from an i.i.d. random variable Xi,t (i.e., we consider a stochastic MAB setting) characterized by an unknown distribution Di with finite known support \u03a9 \u2282 R (as customary in MAB settings, from now on we consider \u03a9 \u2286 [0, 1]) and by unknown expected value \u03bci := E[Xi,t]. We assume that there is a single optimal arm, i.e., there exists a unique arm ai\u2217 s.t. its expected value \u03bci\u2217 = maxi \u03bci and, for sake of notation, we denote \u03bci\u2217 with \u03bc\u2217. Here we analyze a graph bandit setting with unimodality property, defined as: Definition 1. A graph unimodal MAB (UMAB) settingG = (A,E) is a graph bandit setting G s.t. for each sub\u2013optimal arm ai, i 6= i\u2217 it exists a finite path p = (i1 = i, . . . , im = i\u2217) s.t. \u03bcik < \u03bcik+1 and (aik , aik+1) \u2208 E for each k \u2208 {1, . . . ,m\u2212 1}. This definition assures that if one is able to identify a non\u2013 decreasing path in G of expected rewards, she be able to reach the optimum arm, without getting stuck in local optima. Note that the unimodality property implies that the graph G is connected and therefore we consider only connected graphs from here on. A policy U over a UMAB setting is a procedure able to select at each round t an arm ait by basing on the history ht, i.e., the sequence of past selected arms and past rewards gained. The pseudo\u2013regretRT (U) of a generic policy U over a UMAB setting is defined as: RT (U) := T\u03bc \u2217 \u2212 E [ T \u2211", "creator": "TeX"}}}