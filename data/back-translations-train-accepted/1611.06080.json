{"id": "1611.06080", "review": {"conference": "AAAI", "VERSION": "v1", "DATE_OF_SUBMISSION": "18-Nov-2016", "title": "A Generalized Stochastic Variational Bayesian Hyperparameter Learning Framework for Sparse Spectrum Gaussian Process Regression", "abstract": "While much research effort has been dedicated to scaling up sparse Gaussian process (GP) models based on inducing variables for big data, little attention is afforded to the other less explored class of low-rank GP approximations that exploit the sparse spectral representation of a GP kernel. This paper presents such an effort to advance the state of the art of sparse spectrum GP models to achieve competitive predictive performance for massive datasets. Our generalized framework of stochastic variational Bayesian sparse spectrum GP (sVBSSGP) models addresses their shortcomings by adopting a Bayesian treatment of the spectral frequencies to avoid overfitting, modeling these frequencies jointly in its variational distribution to enable their interaction a posteriori, and exploiting local data for boosting the predictive performance. However, such structural improvements result in a variational lower bound that is intractable to be optimized. To resolve this, we exploit a variational parameterization trick to make it amenable to stochastic optimization. Interestingly, the resulting stochastic gradient has a linearly decomposable structure that can be exploited to refine our stochastic optimization method to incur constant time per iteration while preserving its property of being an unbiased estimator of the exact gradient of the variational lower bound. Empirical evaluation on real-world datasets shows that sVBSSGP outperforms state-of-the-art stochastic implementations of sparse GP models.", "histories": [["v1", "Fri, 18 Nov 2016 14:00:48 GMT  (111kb,D)", "http://arxiv.org/abs/1611.06080v1", "31st AAAI Conference on Artificial Intelligence (AAAI 2017), Extended version with proofs, 11 pages"]], "COMMENTS": "31st AAAI Conference on Artificial Intelligence (AAAI 2017), Extended version with proofs, 11 pages", "reviews": [], "SUBJECTS": "stat.ML cs.LG", "authors": ["quang minh hoang", "trong nghia hoang", "kian hsiang low"], "accepted": true, "id": "1611.06080"}, "pdf": {"name": "1611.06080.pdf", "metadata": {"source": "META", "title": "A Generalized Stochastic Variational Bayesian Hyperparameter Learning Framework for Sparse Spectrum Gaussian Process Regression", "authors": ["Quang Minh Hoang", "Trong Nghia Hoang", "Kian Hsiang Low"], "emails": ["lowkh}@comp.nus.edu.sg,", "nghiaht@nus.edu.sg"], "sections": [{"heading": "1 Introduction", "text": "In fact, it is not so that it is a purely theoretical contemplation as one imagines it, but a purely theoretical contemplation as one imagines it. (...) In fact, it is so that it is a purely theoretical contemplation. (...) It is not so that it is a purely theoretical contemplation. (...) It is not so that it is a purely theoretical contemplation. (...) It is as if it is a purely theoretical contemplation. (...) It is as if it is a purely theoretical contemplation. (...) It is as if it is a purely theoretical contemplation. (...) It is. (...). \"(...) It is.\" (...). \"It is.\" (...). \"It is.\" (...). \"It is.\" (...). \"It is.\" (...). \"It is.\" (...). \"It is.\" (... \"It is.\" (...). \"It is.\" (...). \"It is.\" (... \"It is.\" (...). \"It is. (...).\" It is. (... \"It is. (...).\" It is. (...). (... \"It is. (...). (...). (it is. (...). (it. (...). (it is. (...). (it is. (...). (it is. (...). (it. (it. (it is.). (...). (it. (it is.). (it is. (it. (it is.). (it.). (it is.). (it is. (it. (it. (it. (it.). (it.). (it is.). (it is. (it is.). (it is. (it is. (it.). (it.). (it. (it is. (it is.). (it is. (it. (it.). (it is.). (it is. (it. (it is.). (it.). (it is."}, {"heading": "2 A Generalized Bayesian Sparse Spectrum Gaussian Process Regression Framework", "text": "Let X be a d-dimensional input space, so that each input vector x > x is associated with a latent output fx and a noise output yx, fx +. (Let's) Let's (let's) generate a latent output fx and a noise output yx, fx + by using any finite subset of {fx} x. \"(Then the GP is fully defined by its previous mean E [fx] (i.e., assuming it is 0 for notoriously simplicity) and covariance k (x), cov [fx.\") for all x, x. \"(The latter can be defined by the commonly used exponential distribution). (Let's let the k.\" (x), x. \"(x), x.\" (x. \"(x.\"))."}, {"heading": "3 Variational Inference for Bayesian Sparse Spectrum Gaussian Process Regression", "text": "In this section, a variable approximation q (\u03b1) of the posterior distribution p (\u03b1 | y) is presented, which is achieved by inferring variations, which involves selecting a parameterization for q (\u03b1) (Section 3.1) and optimizing its defining parameters (Section 3.2) to minimize its Kullback Leibler (KL) distance. The optimized q (\u03b1) can then be used as an inexpensive substitute for p (\u03b1 | y) to efficiently derive an approximation to the predictive distribution p (fx \u0445 | y) (4) from the conditional p (fx \u0445 | yk, \u03b1) in Proposition 1."}, {"heading": "3.1 Variational Parameterization", "text": "Specifically, we parameterise \u03b1 = vec (\u03b8, s), Mz + b, where the variation parameters \u03b7, vec (M, b) are independent of (\u03b8, s), and z is replaced by an analytically comprehensible user-specific distribution method (z)'p (z | y), which is straightforward to extract samples from (i.e., z \u00b2 \u00b2 s. We assume that this protocol is analytically differentiable with respect to z and that the affine matrix M is unchangeable, so that z = M \u2212 1 (\u03b1 \u2212 b) exists. Then q (\u03b8, s) can be expressed with respect to the ability of M, and vice versa (z): Lemma 1 The variational distribution method q, s) can be parameterised indirectly with q (z)."}, {"heading": "3.2 Variational Optimization", "text": "To optimize q (\u03b1), we must first show that the log marginal probability protocol p (y) de-1Note that this log (z) means approximating the posterior distribution of z (q) instead of its previous distribution. Therefore, it cannot be used to derive p (\u03b1) by applying affine transformation to z. [Log marginal probability] to a sum of L (q) and DKL (q): Log (y) = L (q) + DKL (q), where L (q), E\u03b1), q (q), q) and L (q) can apply both log marginal probability protocols to z. [Log marginal probability] to z (q), which apply both log marginal probabilities to z."}, {"heading": "4 Stochastic Optimization", "text": "In order to overcome the problem of scalability in assessing the stochastic gradient \u2202 L-X-Y-Y (6) (Section 3.2), we will show in Theorem 1 below that \u2202 log p (y-\u03b1) / \u2202 \u03b7 can be separated into a linear sum of analytically traceable terms, each of which depends on only a small subset of local data. Interestingly, Theorem 1 implies a similar decomposition of \u2202 L-Y-Y-N (X-Y). Consequently, we can derive a new stochastic estimate of the exact gradient \u0445 L-Y-N (6), which can be efficiently and scalably calculated using only one or some randomly sampled subsets of local fixed-size data and still retain the property of being its unbiased estimator (Section 4.1)."}, {"heading": "4.1 Stochastic Gradient Revisited", "text": "In order to derive therefrom any computational, scalable and unbiased stochastic numerals, we rely on our main result, which is based on the erosion of numbers numerals numerals numerals numerals numerals numerals numerals numerals numerals numerals numerals numerals numerals numerals numerals numerals numerals numerals numerals numerals numerals numerals numerals numerals numerals numerals numerals numerals numerals numerals numerals numerals numerals numerals numerals numerals numerals numerals numerals numerals numerals numerals numerals numerals numerals numerals numerals numerals numerals numerals numerals numerals numerals numerals numerals numerals numerals numerals numerals numerals numerals numerals numerals numerals numerals numerals numerals numerals numerals numerals numerals numerals numerals numerals numerals numerals numerals numerals numerals numerals numerals numerals numerals numerals numerals numerals numerals numerals numerals numerals numerals numerals numerals numerals numerals numerals numerals numerals numerals numerals numerals numerals numerals numerals numerals numerals numerals numerals numerals numerals numerals numerals numerals numerals numerals numerals numerals numerals numerals numerals numerals numerers numerals numerals numerals numerals numerals numerals numerals numerals numerals numerers numerals numerers numerers numerers numerers numerers numerers numerers numerers numerers numerers numerers numerers numerers numerers numerers numerers numerers numerers numerers numerers numerers numerers numer"}, {"heading": "4.2 Approximate Predictive Inference", "text": "In the iteration t of the stochastic gradient update, an estimate qt (\u03b1) of the variably optimal approximation q (\u03b1) can be induced on the basis of the current estimate \u03b7t = vid (Mt, bt) of its variation parameters \u03b7 using the parameterization in Lemma 1. Consequently, the predictive mean value can be applied to the law of iterated expectations based on \u00b5 x \u0445 (Mt, bt) [E [fx] [yk, \u03b1] = E\u03b1 \u00b2 qt (\u03b1) [\u00b5x \u00b2 (\u03b1)] = \u03b1 qt (\u03b1) \u00b5x \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 (\u03b1) \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2."}, {"heading": "5 Empirical Studies", "text": "This section empirically evaluates the predictive performance and time efficiency of our sVBSSGP model against three real-world datasets: (a) The AIMPEAK database (Chen et al. 2013) consists of 41800 traffic speed observations (km / h) along 775 urban road sections during the morning peak hours on April 20, 2011. Each observation has a 5- dimensional input vector measuring the length of a road section, the number of lanes, the direction and recording time (i.e. broken down into 54 five-minute time slots) and a corresponding output of traffic speed (km / h); (b) the benchmark AIRLINE data (Hensman, Fusi and Lawrence2013; Hoang, and Low 2015) contains 200,000 pieces of information about commercial flights in 2008."}, {"heading": "6 Conclusion", "text": "This paper describes a novel generalized framework of sVBSSGP regression models that addresses the shortcomings of existing sparse spectrum GP models such as SSGP (La \u0301 zaro-Gredilla et al. 2010) and VSSGP (Gal and Turner 2015) by applying Bayean treatment of spectral frequencies to avoid over-matching, and jointly modelling the spectral frequencies in their variation distribution to enable their interaction a posteriori, and using local data to improve prediction capability while maintaining their scalability to big TC data through stochastic optimization. As a result, empirical evaluation of real datasets (i.e., including the million-sized benchmark AIRLINE dataset) shows that our proposed sVBSSGP regression model compares the existing sparse spectrum Centre FTC Research with real datasets (i.e., including the million-scale benchmark AIRLINE dataset) based on SSTC GP and SSTC regression models such as SSTC GP and SSTC research, SSTC low-spectrum Centre FASP models such as the Singapore GP SSGP and SSGP SSGP models and SSGP models."}, {"heading": "A Proof of Proposition 1", "text": "Da \"fx\" x \u2212 X \u2212 K K \u2212 K (Xk) and \"k\" k \u2212 K (K (Xk), (K), (Xk), (Xk), (Xk) and \"k\" (K), k \u2212 K (K), k (Xk), k (Xk), k (Xk), k (Xk), k (Xk), k (Xk), k (Xk), k (K), k (Xk), k (Xk), k (Xk), k (Xk), k (Xk), k (Xk), k (Xk), k (Xk), k (k), k (K), k (Xk), k (K), k (Xk), k (K), k (Xk), k (k), k (k), k, k, k (k), k, k (k), k, k (k), k, k (k), k, k (k), k, k (k), k (k), k (k), k (k), k (Xk), k (Xk), k (K (Xk), k (Xk), k (Xk), k (Xk), k (Xk (Xk), k (Xk), k (k (Xk), k (Xk), k (k (Xk), k (Xk), k (Xk (Xk), k (Xk (Xk), k (Xk), k (Xk (Xk), k (Xk), k (Xk), k (Xk (Xk), k (k), k (Xk), k (k (k), k (k), k (k), k (k (k), k (k), k (k), k (k), k (k, k, K (Xk), k), k (k, k, k, k, k, K (Xk, k), k, k, k, k, k, k, k, k, k, k, k, k,"}, {"heading": "D Closed-Form Evaluation of \u2202L\u0302/\u2202\u03b7", "text": "To show that this value is analytically traceable, it is sufficient to show that both protocols (q = = q = q = = q = q (q | \u03b1) / XI (q) / VI (\u03b1) / VI (XI) / VI (VI) / VI (VI) / VI) are analytically traceable, with the former being represented in Appendix F. To show the latter, it should be noted that this value of the protocol (q, b) (Section 3.1) implies that this value (q) / VI (VI) / VI (VI) / VI (VI) / VI (VI) / VI (VI) / VI (VI) / VI (VI) / VI (VI) / VI (VI) / VI / VI (VI) / VI (VI) / VI (VI) / VI (VI) / VI (VI) (VI) VI (VI) VI (VI) / VI (VI) VI (VI) / VI (VI) VI (VI) / VI (VI) VI (VI) / VI (VI) (VI) VI (VI) VI (VI / VI) (VI) VI (VI) VI (VI) VI (VI) VI (VI / VI) VI (VI) VI (VI) VI (VI / VI) / VI (VI) / VI (VI) / VI (VI) / VI (VI) / VI (VI) (VI (VI) (VI / VI) (VI) (VI / VI) (VI / VI) (VI (VI) (VI / VI) (VI / VI) (VI (VI) (VI / VI) (VI / VI) (VI / VI) (VI (VI) (VI / VI) / VI (VI (VI / VI) (VI / VI) (VI / VI) (VI (VI / VI) (VI / VI) (VI (VI / VI / VI) (VI / VI / VI) (VI / VI) (VI (VI / VI / VI) (VI / VI / VI) (VI (VI / VI / VI) (VI (VI / VI) (VI / VI) (VI / VI / VI / VI / VI) (VI (VI / VI / VI / VI) (VI / VI) (VI / VI (VI / VI / VI) (VI / VI / VI /"}, {"heading": "E Proof of Lemma 2", "text": "Using a derivative similar to that in Appendix A results in (3), p (s) = N (s), p (y), p (s), p (s), p (s), p (s), p (s), p (s), p (s), p (s), and p (s), with the last equality from Gaussian identity following the affine transformation to marginalization. (19) On the other hand, p (y), p (s) = N (s), p (s), and p (s) can also be expressed in terms such as p (y), p (s), p (s), p (s), and p (s) by applying marginalization: p (y), p (p), p (s), p (s), p (s), p (s), and p (s)."}, {"heading": "F Proof of Theorem 1", "text": "Note that v > v = \u2202 p = 1 v > i vi. If this is included in Lemma 2, a logbook p (y | \u03b1) = \u2212 0.5\u043c \u2212 2n p \u2211 i = 1 v > i \u2212 0.5n log (2\u03c0\u03c32n) is obtained. (21) If we take derivatives relating to \u03b7 on both sides of (21), we obtain a logbook p (y | \u03b1) = \u2212 0.5\u043c \u2212 2n p \u2211 i = 1 \u0432 (v > i vi) = \u2212 0.5 p \u0445 i = 1 ri = p \u2211 i = 1 Fi (\u03b7, \u03b1), where the last two equivalents are from the definitions of ri and Fi (\u03b7, \u03b1) in Theorem 1. This completes our argument. To show that Fi (\u03b7, \u03b1) and hence II (y | III) / III) can be analytically evaluated, it is sufficient to show that FI (v > i vi) is analytically derivable."}, {"heading": "G Generalized Stochastic Gradient", "text": "IV IV IV IV IV IV IV IV IV IV IV IV IV IV IV IV IV IV IV IV IV IV IV IV IV IV IV IV IV IV IV IV IV IV IV IV IV IV IV IV IV IV IV IV IV IV IV IV IV IV IV IV IV IV IV IV IV IV IV IV IV IV IV IV IV IV IV IV IV IV IV IV IV IV IV IV IV IV IV IV IV IV IV IV IV IV IV IV IV IV IV IV IV IV IV IV IV IV IV IV IV IV IV IV IV IV IV IV IV IV IV IV IV IV IV IV IV IV IV IV IV IV IV IV IV IV IV IV IV IV IV IV IV IV IV IV IV IV IV IV IV IV IV IV IV IV IV IV IV IV IV IV IV IV IV IV IV IV IV IV IV IV IV IV IV IV IV IV IV IV IV IV IV IV IV IV IV IV IV IV IV IV IV IV IV IV IV IV IV IV IV IV IV IV IV IV IV IV IV IV IV IV IV IV IV IV IV IV IV IV IV IV IV IV IV IV IV IV IV IV IV IV IV IV IV IV IV IV IV IV IV IV IV IV IV IV IV IV IV IV IV IV IV IV IV IV IV IV IV IV IV IV IV IV IV IV IV IV IV IV IV IV IV IV IV IV IV IV IV IV IV IV IV IV IV IV IV IV IV IV IV IV IV IV IV IV IV IV IV IV IV IV IV IV IV IV IV IV IV IV IV IV IV IV IV IV IV IV IV IV IV IV IV IV IV IV IV IV IV IV IV IV IV IV IV IV IV IV IV IV IV IV IV IV IV IV IV IV IV IV IV IV IV IV IV IV IV IV IV IV IV IV IV IV IV IV IV IV IV IV IV IV IV IV IV IV IV IV IV IV IV IV IV IV IV IV IV IV IV IV IV IV IV IV IV IV IV IV IV IV IV IV IV IV IV IV IV IV IV IV IV IV IV IV IV IV IV IV IV IV IV IV IV IV IV IV IV IV IV IV IV IV IV IV IV IV IV IV IV IV IV IV IV IV IV IV IV IV IV IV IV IV IV IV IV IV IV IV IV IV IV IV IV IV IV IV IV IV IV IV IV IV IV IV IV IV IV IV IV IV IV IV IV IV IV IV IV IV IV IV IV IV IV IV IV IV IV IV IV IV IV IV IV IV IV IV IV IV IV IV IV IV IV IV IV IV IV IV IV IV"}, {"heading": "H Supplementary Experiments", "text": "It is not the first time that the EU Commission has taken such a step."}], "references": [{"title": "Feedback prediction for blogs", "author": ["K. Buza"], "venue": "Spiliopoulou, M.; Schmidt-Thieme, L.; and Janning, R., eds., Data Analysis, Machine Learning and Knowledge Discovery. Springer International Publishing. 145\u2013152.", "citeRegEx": "Buza,? 2014", "shortCiteRegEx": "Buza", "year": 2014}, {"title": "Multi-robot informative path planning for active sensing of environmental phenomena: A tale of two algorithms", "author": ["N. Cao", "K.H. Low", "J.M. Dolan"], "venue": "Proc. AAMAS.", "citeRegEx": "Cao et al\\.,? 2013", "shortCiteRegEx": "Cao et al\\.", "year": 2013}, {"title": "Decentralized data fusion and active sensing with mobile sensors for modeling and predicting spatiotemporal traffic phenomena", "author": ["J. Chen", "K.H. Low", "C.K.-Y. Tan", "A. Oran", "P. Jaillet", "J.M. Dolan", "G.S. Sukhatme"], "venue": "Proc. UAI, 163\u2013173.", "citeRegEx": "Chen et al\\.,? 2012", "shortCiteRegEx": "Chen et al\\.", "year": 2012}, {"title": "Parallel Gaussian process regression with low-rank covariance matrix approximations", "author": ["J. Chen", "N. Cao", "K.H. Low", "R. Ouyang", "C.K.-Y. Tan", "P. Jaillet"], "venue": "Proc. UAI, 152\u2013161.", "citeRegEx": "Chen et al\\.,? 2013", "shortCiteRegEx": "Chen et al\\.", "year": 2013}, {"title": "Gaussian process decentralized data fusion and active sensing for spatiotemporal traffic modeling and prediction in mobilityon-demand systems", "author": ["J. Chen", "K.H. Low", "P. Jaillet", "Y. Yao"], "venue": "IEEE T-ASE 12(3):901\u2013921.", "citeRegEx": "Chen et al\\.,? 2015", "shortCiteRegEx": "Chen et al\\.", "year": 2015}, {"title": "Gaussian process-based decentralized data fusion and active sensing for mobility-on-demand system", "author": ["J. Chen", "K.H. Low", "C.K.-Y. Tan"], "venue": "Proc. RSS.", "citeRegEx": "Chen et al\\.,? 2013", "shortCiteRegEx": "Chen et al\\.", "year": 2013}, {"title": "Cooperative aquatic sensing using the telesupervised adaptive ocean sensor fleet", "author": ["J.M. Dolan", "G. Podnar", "S. Stancliff", "K.H. Low", "A. Elfes", "J. Higinbotham", "J.C. Hosler", "T.A. Moisan", "J. Moisan"], "venue": "Proc. SPIE Conference on Remote Sensing of the Ocean, Sea Ice, and Large Water", "citeRegEx": "Dolan et al\\.,? 2009", "shortCiteRegEx": "Dolan et al\\.", "year": 2009}, {"title": "Improving the Gaussian process sparse spectrum approximation by representing uncertainty in frequency inputs", "author": ["Y. Gal", "R. Turner"], "venue": "Proc. ICML, 655\u2013664.", "citeRegEx": "Gal and Turner,? 2015", "shortCiteRegEx": "Gal and Turner", "year": 2015}, {"title": "Gaussian processes for big data", "author": ["J. Hensman", "N. Fusi", "N.D. Lawrence"], "venue": "Proc. UAI, 282\u2013290.", "citeRegEx": "Hensman et al\\.,? 2013", "shortCiteRegEx": "Hensman et al\\.", "year": 2013}, {"title": "Active learning is planning: Nonmyopic -Bayesoptimal active learning of Gaussian processes", "author": ["T.N. Hoang", "K.H. Low", "P. Jaillet", "M. Kankanhalli"], "venue": "Proc. ECML/PKDD Nectar Track, 494\u2013498.", "citeRegEx": "Hoang et al\\.,? 2014a", "shortCiteRegEx": "Hoang et al\\.", "year": 2014}, {"title": "Nonmyopic -Bayes-Optimal Active Learning of Gaussian Processes", "author": ["T.N. Hoang", "K.H. Low", "P. Jaillet", "M. Kankanhalli"], "venue": "Proc. ICML, 739\u2013747.", "citeRegEx": "Hoang et al\\.,? 2014b", "shortCiteRegEx": "Hoang et al\\.", "year": 2014}, {"title": "A unifying framework of anytime sparse Gaussian process regression models with stochastic variational inference for big data", "author": ["T.N. Hoang", "Q.M. Hoang", "K.H. Low"], "venue": "Proc. ICML, 569\u2013578.", "citeRegEx": "Hoang et al\\.,? 2015", "shortCiteRegEx": "Hoang et al\\.", "year": 2015}, {"title": "A distributed variational inference framework for unifying parallel sparse Gaussian process regression models", "author": ["T.N. Hoang", "Q.M. Hoang", "K.H. Low"], "venue": "Proc. ICML, 382\u2013391.", "citeRegEx": "Hoang et al\\.,? 2016", "shortCiteRegEx": "Hoang et al\\.", "year": 2016}, {"title": "Sparse spectrum Gaussian process regression", "author": ["M. L\u00e1zaro-Gredilla", "J. Qui\u00f1onero-Candela", "C.E. Rasmussen", "A.R. Figueiras-Vidal"], "venue": "JMLR 1865\u20131881.", "citeRegEx": "L\u00e1zaro.Gredilla et al\\.,? 2010", "shortCiteRegEx": "L\u00e1zaro.Gredilla et al\\.", "year": 2010}, {"title": "Gaussian process planning with Lipschitz continuous reward", "author": ["C.K. Ling", "K.H. Low", "P. Jaillet"], "venue": null, "citeRegEx": "Ling et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Ling et al\\.", "year": 2016}, {"title": "Decentralized active robotic exploration and mapping for probabilistic field classification in environmental sensing", "author": ["K.H. Low", "J. Chen", "J.M. Dolan", "S. Chien", "D.R. Thompson"], "venue": "Proc. AAMAS, 105\u2013112.", "citeRegEx": "Low et al\\.,? 2012", "shortCiteRegEx": "Low et al\\.", "year": 2012}, {"title": "Recent advances in scaling up Gaussian process predictive models for large spatiotemporal data", "author": ["K.H. Low", "J. Chen", "T.N. Hoang", "N. Xu", "P. Jaillet"], "venue": "Proc. DyDESS.", "citeRegEx": "Low et al\\.,? 2014a", "shortCiteRegEx": "Low et al\\.", "year": 2014}, {"title": "Generalized online sparse Gaussian processes with application to persistent mobile robot localization", "author": ["K.H. Low", "N. Xu", "J. Chen", "K.K. Lim", "E.B. \u00d6zg\u00fcl"], "venue": "Proc. ECML/PKDD Nectar Track, 499\u2013503.", "citeRegEx": "Low et al\\.,? 2014b", "shortCiteRegEx": "Low et al\\.", "year": 2014}, {"title": "Parallel Gaussian process regression for big data: Low-rank representation meets Markov approximation", "author": ["K.H. Low", "J. Yu", "J. Chen", "P. Jaillet"], "venue": "Proc. AAAI.", "citeRegEx": "Low et al\\.,? 2015", "shortCiteRegEx": "Low et al\\.", "year": 2015}, {"title": "Adaptive multi-robot wide-area exploration and mapping", "author": ["K.H. Low", "J.M. Dolan", "P. Khosla"], "venue": "Proc. AAMAS, 23\u201330.", "citeRegEx": "Low et al\\.,? 2008", "shortCiteRegEx": "Low et al\\.", "year": 2008}, {"title": "Informationtheoretic approach to efficient adaptive path planning for mobile robotic environmental sensing", "author": ["K.H. Low", "J.M. Dolan", "P. Khosla"], "venue": "Proc. ICAPS.", "citeRegEx": "Low et al\\.,? 2009", "shortCiteRegEx": "Low et al\\.", "year": 2009}, {"title": "Active Markov information-theoretic path planning for robotic environmental sensing", "author": ["K.H. Low", "J.M. Dolan", "P. Khosla"], "venue": "Proc. AAMAS, 753\u2013760.", "citeRegEx": "Low et al\\.,? 2011", "shortCiteRegEx": "Low et al\\.", "year": 2011}, {"title": "Multirobot active sensing of non-stationary Gaussian processbased environmental phenomena", "author": ["R. Ouyang", "K.H. Low", "J. Chen", "P. Jaillet"], "venue": "Proc. AAMAS.", "citeRegEx": "Ouyang et al\\.,? 2014", "shortCiteRegEx": "Ouyang et al\\.", "year": 2014}, {"title": "Telesupervised remote surface water quality sensing", "author": ["G. Podnar", "J.M. Dolan", "K.H. Low", "A. Elfes"], "venue": "Proc. IEEE Aerospace Conference.", "citeRegEx": "Podnar et al\\.,? 2010", "shortCiteRegEx": "Podnar et al\\.", "year": 2010}, {"title": "A unifying view of sparse approximate Gaussian process regression", "author": ["J. Qui\u00f1onero-Candela", "C.E. Rasmussen"], "venue": "Journal of Machine Learning Research 6:1939\u20131959.", "citeRegEx": "Qui\u00f1onero.Candela and Rasmussen,? 2005", "shortCiteRegEx": "Qui\u00f1onero.Candela and Rasmussen", "year": 2005}, {"title": "Doubly stochastic variational Bayes for non-conjugate inference", "author": ["M.K. Titsias", "M. L\u00e1zaro-Gredilla"], "venue": "Proc. ICML, 1971\u20131979.", "citeRegEx": "Titsias and L\u00e1zaro.Gredilla,? 2014", "shortCiteRegEx": "Titsias and L\u00e1zaro.Gredilla", "year": 2014}, {"title": "Variational learning of inducing variables in sparse Gaussian processes", "author": ["M.K. Titsias"], "venue": "Proc. AISTATS.", "citeRegEx": "Titsias,? 2009", "shortCiteRegEx": "Titsias", "year": 2009}, {"title": "GP-Localize: Persistent mobile robot localization using online sparse Gaussian process observation model", "author": ["N. Xu", "K.H. Low", "J. Chen", "K.K. Lim", "E.B. \u00d6zg\u00fcl"], "venue": "Proc. AAAI, 2585\u20132592.", "citeRegEx": "Xu et al\\.,? 2014", "shortCiteRegEx": "Xu et al\\.", "year": 2014}, {"title": "Hierarchical Bayesian nonparametric approach to modeling and learning the wisdom of crowds of urban traffic route planning agents", "author": ["J. Yu", "K.H. Low", "A. Oran", "P. Jaillet"], "venue": "Proc. IAT, 478\u2013485.", "citeRegEx": "Yu et al\\.,? 2012", "shortCiteRegEx": "Yu et al\\.", "year": 2012}, {"title": "Near-optimal active learning of multi-output Gaussian processes", "author": ["Y. Zhang", "T.N. Hoang", "K.H. Low", "M. Kankanhalli"], "venue": "Proc. AAAI, 2351\u20132357.", "citeRegEx": "Zhang et al\\.,? 2016", "shortCiteRegEx": "Zhang et al\\.", "year": 2016}], "referenceMentions": [{"referenceID": 24, "context": "To lift this computational curse, a vast literature of sparse GP regression models (Qui\u00f1onero-Candela and Rasmussen 2005; Titsias 2009) have exploited a structural assumption of conditional independence based on the notion of inducing variables for achieving linear time in the data size.", "startOffset": 83, "endOffset": 135}, {"referenceID": 26, "context": "To lift this computational curse, a vast literature of sparse GP regression models (Qui\u00f1onero-Candela and Rasmussen 2005; Titsias 2009) have exploited a structural assumption of conditional independence based on the notion of inducing variables for achieving linear time in the data size.", "startOffset": 83, "endOffset": 135}, {"referenceID": 2, "context": "2016), traffic monitoring (Chen et al. 2012; Chen, Low, and Tan 2013; Chen et al. 2015; Hoang et al. 2014a; 2014b; Low et al. 2014a; 2014b; Ouyang et al. 2014; Xu et al. 2014; Yu et al. 2012)), (a) distributed (Chen et al.", "startOffset": 26, "endOffset": 191}, {"referenceID": 4, "context": "2016), traffic monitoring (Chen et al. 2012; Chen, Low, and Tan 2013; Chen et al. 2015; Hoang et al. 2014a; 2014b; Low et al. 2014a; 2014b; Ouyang et al. 2014; Xu et al. 2014; Yu et al. 2012)), (a) distributed (Chen et al.", "startOffset": 26, "endOffset": 191}, {"referenceID": 9, "context": "2016), traffic monitoring (Chen et al. 2012; Chen, Low, and Tan 2013; Chen et al. 2015; Hoang et al. 2014a; 2014b; Low et al. 2014a; 2014b; Ouyang et al. 2014; Xu et al. 2014; Yu et al. 2012)), (a) distributed (Chen et al.", "startOffset": 26, "endOffset": 191}, {"referenceID": 16, "context": "2016), traffic monitoring (Chen et al. 2012; Chen, Low, and Tan 2013; Chen et al. 2015; Hoang et al. 2014a; 2014b; Low et al. 2014a; 2014b; Ouyang et al. 2014; Xu et al. 2014; Yu et al. 2012)), (a) distributed (Chen et al.", "startOffset": 26, "endOffset": 191}, {"referenceID": 22, "context": "2016), traffic monitoring (Chen et al. 2012; Chen, Low, and Tan 2013; Chen et al. 2015; Hoang et al. 2014a; 2014b; Low et al. 2014a; 2014b; Ouyang et al. 2014; Xu et al. 2014; Yu et al. 2012)), (a) distributed (Chen et al.", "startOffset": 26, "endOffset": 191}, {"referenceID": 27, "context": "2016), traffic monitoring (Chen et al. 2012; Chen, Low, and Tan 2013; Chen et al. 2015; Hoang et al. 2014a; 2014b; Low et al. 2014a; 2014b; Ouyang et al. 2014; Xu et al. 2014; Yu et al. 2012)), (a) distributed (Chen et al.", "startOffset": 26, "endOffset": 191}, {"referenceID": 28, "context": "2016), traffic monitoring (Chen et al. 2012; Chen, Low, and Tan 2013; Chen et al. 2015; Hoang et al. 2014a; 2014b; Low et al. 2014a; 2014b; Ouyang et al. 2014; Xu et al. 2014; Yu et al. 2012)), (a) distributed (Chen et al.", "startOffset": 26, "endOffset": 191}, {"referenceID": 3, "context": "2012)), (a) distributed (Chen et al. 2013; Hoang, Hoang, and Low 2016; Low et al. 2015) and (b) stochastic (Hensman, Fusi, and Lawrence 2013; Hoang, Hoang, and Low 2015) implementations of such models have been developed to, respectively, (a) reduce their time to train with all the data by a factor close to the number of machines and (b) train with a small, randomly sampled subset of data in constant time per iteration of stochastic gradient ascent update and achieve asymptotic convergence to their predictive distributions.", "startOffset": 24, "endOffset": 87}, {"referenceID": 18, "context": "2012)), (a) distributed (Chen et al. 2013; Hoang, Hoang, and Low 2016; Low et al. 2015) and (b) stochastic (Hensman, Fusi, and Lawrence 2013; Hoang, Hoang, and Low 2015) implementations of such models have been developed to, respectively, (a) reduce their time to train with all the data by a factor close to the number of machines and (b) train with a small, randomly sampled subset of data in constant time per iteration of stochastic gradient ascent update and achieve asymptotic convergence to their predictive distributions.", "startOffset": 24, "endOffset": 87}, {"referenceID": 7, "context": "On the other hand, there is a less well-explored, alternative class of low-rank GP approximations that exploit sparsity in the spectral representation of a GP kernel (Gal and Turner 2015; L\u00e1zaro-Gredilla et al. 2010) for gaining time efficiency and have empirically demonstrated competitive predictive performance for datasets of up to tens of thousands in size but, surprisingly, not received as much attention and research effort.", "startOffset": 166, "endOffset": 216}, {"referenceID": 13, "context": "On the other hand, there is a less well-explored, alternative class of low-rank GP approximations that exploit sparsity in the spectral representation of a GP kernel (Gal and Turner 2015; L\u00e1zaro-Gredilla et al. 2010) for gaining time efficiency and have empirically demonstrated competitive predictive performance for datasets of up to tens of thousands in size but, surprisingly, not received as much attention and research effort.", "startOffset": 166, "endOffset": 216}, {"referenceID": 7, "context": "On the other hand, there is a less well-explored, alternative class of low-rank GP approximations that exploit sparsity in the spectral representation of a GP kernel (Gal and Turner 2015; L\u00e1zaro-Gredilla et al. 2010) for gaining time efficiency and have empirically demonstrated competitive predictive performance for datasets of up to tens of thousands in size but, surprisingly, not received as much attention and research effort. In contrast to the above literature, such sparse spectrum GP regression models do not need to introduce an additional set of inducing inputs which is computationally challenging to be jointly optimized, especially with a large number of them that is necessary for accurate predictions. Unfortunately, the sparse spectrum GP (SSGP) model of L\u00e1zaro-Gredilla et al. (2010) does not scale well to massive datasets due to its linear time in the data size and also finds a point estimate of the spectral frequencies of its kernel that risks overfitting.", "startOffset": 167, "endOffset": 803}, {"referenceID": 7, "context": "On the other hand, there is a less well-explored, alternative class of low-rank GP approximations that exploit sparsity in the spectral representation of a GP kernel (Gal and Turner 2015; L\u00e1zaro-Gredilla et al. 2010) for gaining time efficiency and have empirically demonstrated competitive predictive performance for datasets of up to tens of thousands in size but, surprisingly, not received as much attention and research effort. In contrast to the above literature, such sparse spectrum GP regression models do not need to introduce an additional set of inducing inputs which is computationally challenging to be jointly optimized, especially with a large number of them that is necessary for accurate predictions. Unfortunately, the sparse spectrum GP (SSGP) model of L\u00e1zaro-Gredilla et al. (2010) does not scale well to massive datasets due to its linear time in the data size and also finds a point estimate of the spectral frequencies of its kernel that risks overfitting. The recent variational SSGP (VSSGP) model of Gal and Turner (2015) has attempted to address both shortcomings of SSGP with a stochastic implementation and a Bayesian treatment of the spectral frequencies, respectively.", "startOffset": 167, "endOffset": 1048}, {"referenceID": 13, "context": "Such a kernel can be expressed as the Fourier transform of a density function p(r) over the domain of frequency vector r whose coefficients form a set of trigonometric basis functions (L\u00e1zaro-Gredilla et al. 2010): k(x,x\u2032) = Er\u223cp(r)[ \u03c3 scos(2\u03c0r(x\u2212 x\u2032)) ] (1) where p(r) , N ( 0, (4\u03c02\u2206)\u22121 ) .", "startOffset": 184, "endOffset": 213}, {"referenceID": 13, "context": "Such a kernel can be expressed as the Fourier transform of a density function p(r) over the domain of frequency vector r whose coefficients form a set of trigonometric basis functions (L\u00e1zaro-Gredilla et al. 2010): k(x,x\u2032) = Er\u223cp(r)[ \u03c3 scos(2\u03c0r(x\u2212 x\u2032)) ] (1) where p(r) , N ( 0, (4\u03c02\u2206)\u22121 ) . In the same spirit as that of L\u00e1zaro-Gredilla et al. (2010), we approximate the kernel in (1) by its unbiased estimator constructed from m i.", "startOffset": 185, "endOffset": 352}, {"referenceID": 13, "context": "This signifies a key difference between our generalized framework and the sparse spectrum GP (SSGP) model of L\u00e1zaro-Gredilla et al. (2010), the latter of which finds a point estimate of \u03b8 via maximum likelihood estimation that risks overfitting.", "startOffset": 109, "endOffset": 139}, {"referenceID": 7, "context": "Remark 1 The special case of \u03b3 = 1 recovers the degenerate test conditional p(fx\u2217 |\u03b1) = N (\u03c6\u03b8 (x\u2217)s, 0) induced by the variational SSGP (VSSGP) model of Gal and Turner (2015) (see equation 4 and Section 3 therein) which reveals that it imposes a highly restrictive deterministic relationship between fx\u2217 and \u03b1 and also fails to exploit the local data (Xk,yk) (i.", "startOffset": 153, "endOffset": 175}, {"referenceID": 7, "context": "Remark 1 The special case of \u03b3 = 1 recovers the degenerate test conditional p(fx\u2217 |\u03b1) = N (\u03c6\u03b8 (x\u2217)s, 0) induced by the variational SSGP (VSSGP) model of Gal and Turner (2015) (see equation 4 and Section 3 therein) which reveals that it imposes a highly restrictive deterministic relationship between fx\u2217 and \u03b1 and also fails to exploit the local data (Xk,yk) (i.e., due to conditional independence between fx\u2217 and yk given \u03b1) that can potentially improve the predictive performance. Unfortunately, VSSGP cannot be trivially extended to span the entire spectrum since it relies on the induced deterministic relationship between fx\u2217 and\u03b1 to analytically derive its predictive distribution, which does not hold for \u03b3 6= 1. On the other hand, when \u03b3 = 0, the test conditional in Proposition 1 becomes independent of the nuisance variables s and reduces to the predictive distribution p(fx\u2217 |yk,\u03b8) of the SSGP model of L\u00e1zaro-Gredilla et al. (2010) (see equation 7 therein) given its point estimate of the spectral frequencies \u03b8 but restricted to the local data (Xk,yk) corresponding to input subspaceXk.", "startOffset": 153, "endOffset": 944}, {"referenceID": 7, "context": "In contrast, the VSSGP model of Gal and Turner (2015) assumes r1, .", "startOffset": 32, "endOffset": 54}, {"referenceID": 7, "context": "In contrast, the VSSGP model of Gal and Turner (2015) assumes r1, . . . , rm, and s to be statistically independent a posteriori in its variational distribution (see section 4 therein). Relaxing this assumption will cause VSSGP to lose its scalability as its induced variational lower bound will inevitably become intractable. Remark 3 Though the model of Titsias and L\u00e1zaroGredilla (2014) has adopted a similar parameterization but only for the original GP hyperparameters, it incurs cubic time in the data size per iteration of gradient ascent update, as shown in its supplementary materials and experiments.", "startOffset": 32, "endOffset": 390}, {"referenceID": 3, "context": "This section empirically evaluates the predictive performance and time efficiency of our sVBSSGP model on three real-world datasets: (a) The AIMPEAK dataset (Chen et al. 2013) consists of 41800 traffic speed observations (km/h) along 775 urban road segments during the morning peak hours on April 20, 2011.", "startOffset": 157, "endOffset": 175}, {"referenceID": 0, "context": "Each record features a 8-dimensional input vector of the aircraft\u2019s age (year), travel distance (km), the flight\u2019s total airtime, departure time, arrival time (min), and the date given by day of week, day of month, and month, and a corresponding output of the flight\u2019s delay time (min); and (c) the BLOG feedback dataset (Buza 2014) contains 60000 instances of blog posts.", "startOffset": 321, "endOffset": 332}, {"referenceID": 7, "context": "The performance of sVBSSGP is compared against the state-of-the-art VSSGP (Gal and Turner 2015) and stochastic implementations of sparse GP models based on inducing variables such as DTC+ and PIC+ (Hensman, Fusi, and Lawrence 2013; Hoang, Hoang, and Low 2015) (i.", "startOffset": 74, "endOffset": 95}, {"referenceID": 13, "context": "49 Table 1: RMSEs achieved by sVBSSGP, DTC+, PIC+, SSGP (L\u00e1zaro-Gredilla et al. 2010), and VSSGP after final convergence for AIMPEAK and AIRLINE datasets.", "startOffset": 56, "endOffset": 85}, {"referenceID": 13, "context": "This paper describes a novel generalized framework of sVBSSGP regression models that addresses the shortcomings of existing sparse spectrum GP models like SSGP (L\u00e1zaro-Gredilla et al. 2010) and VSSGP (Gal and Turner 2015) by adopting a Bayesian treatment of the spectral frequencies to avoid overfitting, modeling the spectral frequencies jointly in its variational distribution to enable their interaction a posteriori, and exploiting local data for improving the predictive performance while still being able to preserve its scalability to big data through stochastic optimization.", "startOffset": 160, "endOffset": 189}, {"referenceID": 7, "context": "2010) and VSSGP (Gal and Turner 2015) by adopting a Bayesian treatment of the spectral frequencies to avoid overfitting, modeling the spectral frequencies jointly in its variational distribution to enable their interaction a posteriori, and exploiting local data for improving the predictive performance while still being able to preserve its scalability to big data through stochastic optimization.", "startOffset": 16, "endOffset": 37}], "year": 2016, "abstractText": "While much research effort has been dedicated to scaling up sparse Gaussian process (GP) models based on inducing variables for big data, little attention is afforded to the other less explored class of low-rank GP approximations that exploit the sparse spectral representation of a GP kernel. This paper presents such an effort to advance the state of the art of sparse spectrum GP models to achieve competitive predictive performance for massive datasets. Our generalized framework of stochastic variational Bayesian sparse spectrum GP (sVBSSGP) models addresses their shortcomings by adopting a Bayesian treatment of the spectral frequencies to avoid overfitting, modeling these frequencies jointly in its variational distribution to enable their interaction a posteriori, and exploiting local data for boosting the predictive performance. However, such structural improvements result in a variational lower bound that is intractable to be optimized. To resolve this, we exploit a variational parameterization trick to make it amenable to stochastic optimization. Interestingly, the resulting stochastic gradient has a linearly decomposable structure that can be exploited to refine our stochastic optimization method to incur constant time per iteration while preserving its property of being an unbiased estimator of the exact gradient of the variational lower bound. Empirical evaluation on real-world datasets shows that sVBSSGP outperforms stateof-the-art stochastic implementations of sparse GP models.", "creator": "TeX"}}}