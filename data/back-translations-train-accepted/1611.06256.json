{"id": "1611.06256", "review": {"conference": "iclr", "VERSION": "v1", "DATE_OF_SUBMISSION": "18-Nov-2016", "title": "Reinforcement Learning through Asynchronous Advantage Actor-Critic on a GPU", "abstract": "We introduce and analyze the computational aspects of a hybrid CPU/GPU implementation of the Asynchronous Advantage Actor-Critic (A3C) algorithm, currently the state-of-the-art method in reinforcement learning for various gaming tasks. Our analysis concentrates on the critical aspects to leverage the GPU's computational power, including the introduction of a system of queues and a dynamic scheduling strategy, potentially helpful for other asynchronous algorithms as well. We also show the potential for the use of larger DNN models on a GPU. Our TensorFlow implementation achieves a significant speed up compared to our CPU-only implementation, and it will be made publicly available to other researchers.", "histories": [["v1", "Fri, 18 Nov 2016 21:34:47 GMT  (653kb,D)", "http://arxiv.org/abs/1611.06256v1", null], ["v2", "Sat, 3 Dec 2016 04:42:18 GMT  (673kb,D)", "http://arxiv.org/abs/1611.06256v2", null], ["v3", "Thu, 2 Mar 2017 19:12:19 GMT  (2705kb,D)", "http://arxiv.org/abs/1611.06256v3", null]], "reviews": [], "SUBJECTS": "cs.LG", "authors": ["mohammad babaeizadeh", "iuri frosio", "stephen tyree", "jason clemons", "jan kautz"], "accepted": true, "id": "1611.06256"}, "pdf": {"name": "1611.06256.pdf", "metadata": {"source": "CRF", "title": "GA3C: GPU-based A3C for Deep Reinforcement Learning", "authors": ["Mohammad Babaeizadeh", "Iuri Frosio", "Stephen Tyree", "Jason Clemons", "Jan Kautz"], "emails": ["mb2@uiuc.edu", "jkautz}@nvidia.com"], "sections": [{"heading": "1 Introduction", "text": "The need for task-specific features that previously limited the application of Reinforcement Learning (RL) [1], but the introduction of Deep Q-Learning Networks (DQN) [2] revived the use of Deep Neural Networks (DNNs) as functional approximators for value and policy functions, unleashed a rapid series of advances. Notable results include learning video games from raw pixels [3, 4] and demonstrating superhuman performance on ancient board games [5]. Research has produced a variety of effective training formulations and DNN architectures [6, 7], as well as methods for increasing parallelism and reducing computer resources [8, 9]. In particular, Mnih et al. achieve state-of-the-art results on many gaming tasks through the Asynchronous Advantage Actor-Critic (A3C) algorithms."}, {"heading": "2 Hybrid CPU/GPU A3C (GA3C)", "text": "In fact, it is the case that most of them are able to abide by the rules that they have imposed on themselves, and that they are able to abide by the rules that they have imposed on themselves. (...) In fact, it is the case that they are able to abide by the rules. (...) In fact, it is the case that they are able to abide by the rules. (...) In fact, it is not the case that they abide by the rules. (...) In fact, it is the case that they are able to break the rules. \"(...)"}, {"heading": "3 Results and Discussion", "text": "This year, it has reached the point where it will be able to put itself at the top of the group, which is able to put itself at the top of the group."}], "references": [{"title": "Introduction to Reinforcement Learning", "author": ["R.S. Sutton", "A.G. Barto"], "venue": null, "citeRegEx": "1", "shortCiteRegEx": "1", "year": 1998}, {"title": "Human-level control through deep reinforcement learning", "author": ["V. Mnih", "K. Kavukcuoglu", "D. Silver", "A.A. Rusu", "J. Veness", "M.G. Bellemare", "A. Graves", "M. Riedmiller", "A.K. Fidjeland", "G. Ostrovski", "S. Petersen", "C. Beattie", "A. Sadik", "I. Antonoglou", "H. King", "D. Kumaran", "D. Wierstra", "S. Legg", "D. Hassabis"], "venue": "Nature, vol. 518, pp. 529\u2013533, 02 2015. 5", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2015}, {"title": "Unifying Count-Based Exploration and Intrinsic Motivation", "author": ["M.G. Bellemare", "S. Srinivasan", "G. Ostrovski", "T. Schaul", "D. Saxton", "R. Munos"], "venue": "ArXiv e-prints, June 2016.", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2016}, {"title": "Playing FPS Games with Deep Reinforcement Learning", "author": ["G. Lample", "D. Singh Chaplot"], "venue": "ArXiv e-prints, Sept. 2016.", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2016}, {"title": "Mastering the game of go with deep neural networks and tree search", "author": ["D. Silver", "A. Huang", "C.J. Maddison", "A. Guez", "L. Sifre", "G. van den Driessche", "J. Schrittwieser", "I. Antonoglou", "V. Panneershelvam", "M. Lanctot", "S. Dieleman", "D. Grewe", "J. Nham", "N. Kalchbrenner", "I. Sutskever", "T. Lillicrap", "M. Leach", "K. Kavukcuoglu", "T. Graepel", "D. Hassabis"], "venue": "Nature, vol. 529, pp. 484\u2013503, 2016.", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2016}, {"title": "Deep reinforcement learning with double q-learning", "author": ["H. van Hasselt", "A. Guez", "D. Silver"], "venue": "CoRR, vol. abs/1509.06461, 2015.", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2015}, {"title": "Dueling network architectures for deep reinforcement learning", "author": ["Z. Wang", "N. de Freitas", "M. Lanctot"], "venue": "CoRR, vol. abs/1511.06581, 2015.", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2015}, {"title": "Massively parallel methods for deep reinforcement learning", "author": ["A. Nair", "P. Srinivasan", "S. Blackwell", "C. Alcicek", "R. Fearon", "A.D. Maria", "V. Panneershelvam", "M. Suleyman", "C. Beattie", "S. Petersen", "S. Legg", "V. Mnih", "K. Kavukcuoglu", "D. Silver"], "venue": "CoRR, vol. abs/1507.04296, 2015.", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2015}, {"title": "Asynchronous Methods for Deep Reinforcement Learning", "author": ["V. Mnih", "A. Puigdomenech Badia", "M. Mirza", "A. Graves", "T.P. Lillicrap", "T. Harley", "D. Silver", "K. Kavukcuoglu"], "venue": "ArXiv preprint arXiv:1602.01783, 2016.", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2016}, {"title": "Trust region policy optimization", "author": ["J. Schulman", "S. Levine", "P. Moritz", "M.I. Jordan", "P. Abbeel"], "venue": "Proceedings of the 32nd International Conference on Machine Learning (ICML), 2015.", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2015}, {"title": "TensorFlow: Large-scale machine learning on heterogeneous systems", "author": ["M. Abadi", "A. Agarwal", "P. Barham", "E. Brevdo", "Z. Chen", "C. Citro", "G.S. Corrado", "A. Davis", "J. Dean", "M. Devin", "S. Ghemawat", "I. Goodfellow", "A. Harp", "G. Irving", "M. Isard", "Y. Jia", "R. Jozefowicz", "L. Kaiser", "M. Kudlur", "J. Levenberg", "D. Man\u00e9", "R. Monga", "S. Moore", "D. Murray", "C. Olah", "M. Schuster", "J. Shlens", "B. Steiner", "I. Sutskever", "K. Talwar", "P. Tucker", "V. Vanhoucke", "V. Vasudevan", "F. Vi\u00e9gas", "O. Vinyals", "P. Warden", "M. Wattenberg", "M. Wicke", "Y. Yu", "X. Zheng"], "venue": "2015. Software available from tensorflow.org.", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2015}, {"title": "Continuous control with deep reinforcement learning", "author": ["T.P. Lillicrap", "J.J. Hunt", "A. Pritzel", "N. Heess", "T. Erez", "Y. Tassa", "D. Silver", "D. Wierstra"], "venue": "CoRR, vol. abs/1509.02971, 2015. 6", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2015}], "referenceMentions": [{"referenceID": 0, "context": "The need for task-specific features previously limited the application of Reinforcement Learning (RL) [1], but the introduction of Deep Q-Learning Networks (DQN) [2] revived the use of Deep Neural Networks (DNNs) as function approximators for value and policy functions, unleashing a rapid series of advancements.", "startOffset": 102, "endOffset": 105}, {"referenceID": 1, "context": "The need for task-specific features previously limited the application of Reinforcement Learning (RL) [1], but the introduction of Deep Q-Learning Networks (DQN) [2] revived the use of Deep Neural Networks (DNNs) as function approximators for value and policy functions, unleashing a rapid series of advancements.", "startOffset": 162, "endOffset": 165}, {"referenceID": 2, "context": "Remarkable results include learning to play video games from raw pixels [3, 4] and demonstrating super-human performance on ancient board games [5].", "startOffset": 72, "endOffset": 78}, {"referenceID": 3, "context": "Remarkable results include learning to play video games from raw pixels [3, 4] and demonstrating super-human performance on ancient board games [5].", "startOffset": 72, "endOffset": 78}, {"referenceID": 4, "context": "Remarkable results include learning to play video games from raw pixels [3, 4] and demonstrating super-human performance on ancient board games [5].", "startOffset": 144, "endOffset": 147}, {"referenceID": 5, "context": "Research has yielded a variety of effective training formulations and DNN architectures [6, 7], as well as methods to increase parallelism and decrease computational resources [8, 9].", "startOffset": 88, "endOffset": 94}, {"referenceID": 6, "context": "Research has yielded a variety of effective training formulations and DNN architectures [6, 7], as well as methods to increase parallelism and decrease computational resources [8, 9].", "startOffset": 88, "endOffset": 94}, {"referenceID": 7, "context": "Research has yielded a variety of effective training formulations and DNN architectures [6, 7], as well as methods to increase parallelism and decrease computational resources [8, 9].", "startOffset": 176, "endOffset": 182}, {"referenceID": 8, "context": "Research has yielded a variety of effective training formulations and DNN architectures [6, 7], as well as methods to increase parallelism and decrease computational resources [8, 9].", "startOffset": 176, "endOffset": 182}, {"referenceID": 8, "context": "[9] achieve state-of-the-art results on many gaming tasks through the Asynchronous Advantage Actor-Critic (A3C) algorithm.", "startOffset": 0, "endOffset": 3}, {"referenceID": 9, "context": "A3C dispenses with the large replay memory of previous approaches [10, 2] by stabilizing learning with updates from several concurrently playing agents.", "startOffset": 66, "endOffset": 73}, {"referenceID": 1, "context": "A3C dispenses with the large replay memory of previous approaches [10, 2] by stabilizing learning with updates from several concurrently playing agents.", "startOffset": 66, "endOffset": 73}, {"referenceID": 10, "context": "To systematically investigate these issues, we implement2 both CPU and GPU versions of A3C in TensorFlow (TF) [12], optimizing each for efficient system utilization and to match published \u2217corresponding author Our implementation is open-source at https://github.", "startOffset": 110, "endOffset": 114}, {"referenceID": 8, "context": "In Asynchronous Advantage Actor-Critic (A3C) [9], multiple agents play concurrently and optimize a DNN controller using asynchronous gradient descent.", "startOffset": 45, "endOffset": 48}, {"referenceID": 8, "context": "The original implementation of A3C [9] uses 16 agents on a 16 core CPU and takes 1\u2212 4 days to learn how to play an Atari game [11].", "startOffset": 35, "endOffset": 38}, {"referenceID": 8, "context": "Notice that in A3C a model update occurs every time an agent plays TMAX=5 actions [9].", "startOffset": 82, "endOffset": 85}, {"referenceID": 8, "context": "Since each action is repeated four times as in [9], the number of frames per second is 4\u00d7PPS.", "startOffset": 47, "endOffset": 50}, {"referenceID": 11, "context": "This may be particularly critical when exploring real world problems, such as autonomous driving [13].", "startOffset": 97, "endOffset": 101}, {"referenceID": 1, "context": "Atari Game Scores Attributes AMIDAR BOXING CENTIPEDE NAME THIS GAME PACMAN PONG QBERT SEAQUEST UP-DOWN Time PPS\u2217 System Human [2] 1676 10 10322 6796 15375 16 12085 40426 9896 \u2013 \u2013 \u2013 A3C-1 [9] 284 34 3773 5614 3307 11 13752 2300 54525 1 day 352 CPU A3C-4 [9] 264 60 3756 10476 654 6 15149 2355 74706 4 days 352 CPU GA3C 218 92 7386 5643 1978 18 14966 1706 8623 1 day 1080 GPU", "startOffset": 126, "endOffset": 129}, {"referenceID": 8, "context": "Atari Game Scores Attributes AMIDAR BOXING CENTIPEDE NAME THIS GAME PACMAN PONG QBERT SEAQUEST UP-DOWN Time PPS\u2217 System Human [2] 1676 10 10322 6796 15375 16 12085 40426 9896 \u2013 \u2013 \u2013 A3C-1 [9] 284 34 3773 5614 3307 11 13752 2300 54525 1 day 352 CPU A3C-4 [9] 264 60 3756 10476 654 6 15149 2355 74706 4 days 352 CPU GA3C 218 92 7386 5643 1978 18 14966 1706 8623 1 day 1080 GPU", "startOffset": 187, "endOffset": 190}, {"referenceID": 8, "context": "Atari Game Scores Attributes AMIDAR BOXING CENTIPEDE NAME THIS GAME PACMAN PONG QBERT SEAQUEST UP-DOWN Time PPS\u2217 System Human [2] 1676 10 10322 6796 15375 16 12085 40426 9896 \u2013 \u2013 \u2013 A3C-1 [9] 284 34 3773 5614 3307 11 13752 2300 54525 1 day 352 CPU A3C-4 [9] 264 60 3756 10476 654 6 15149 2355 74706 4 days 352 CPU GA3C 218 92 7386 5643 1978 18 14966 1706 8623 1 day 1080 GPU", "startOffset": 253, "endOffset": 256}, {"referenceID": 1, "context": "Table 3: Average scores on a subset of Atari games achieved by: a human player [2]; A3C after one and four days of training on a CPU [9]; and GA3C after one day of training.", "startOffset": 79, "endOffset": 82}, {"referenceID": 8, "context": "Table 3: Average scores on a subset of Atari games achieved by: a human player [2]; A3C after one and four days of training on a CPU [9]; and GA3C after one day of training.", "startOffset": 133, "endOffset": 136}, {"referenceID": 8, "context": "Table 3 compares scores achieved by A3C on the CPU (as reported in [9]) with our TensorFlow implementation GA3C.", "startOffset": 67, "endOffset": 70}], "year": 2017, "abstractText": "We introduce and analyze the computational aspects of a hybrid CPU/GPU implementation of the Asynchronous Advantage Actor-Critic (A3C) algorithm, currently the state-of-the-art method in reinforcement learning for various gaming tasks. Our analysis concentrates on the critical aspects to leverage the GPU\u2019s computational power, including the introduction of a system of queues and a dynamic scheduling strategy, potentially helpful for other asynchronous algorithms as well. We also show the potential for the use of larger DNN models on a GPU. Our TensorFlow implementation achieves a significant speed up compared to our CPU-only implementation, and it will be made publicly available to other researchers.", "creator": "LaTeX with hyperref package"}}}