{"id": "1611.06585", "review": {"conference": "ICML", "VERSION": "v1", "DATE_OF_SUBMISSION": "20-Nov-2016", "title": "Variational Boosting: Iteratively Refining Posterior Approximations", "abstract": "We propose a black-box variational inference method to approximate intractable distributions with an increasingly rich approximating class. Our method, termed variational boosting, iteratively refines an existing variational approximation by solving a sequence of optimization problems, allowing the practitioner to trade computation time for accuracy. We show how to expand the variational approximating class by incorporating additional covariance structure and by introducing new components to form a mixture. We apply variational boosting to synthetic and real statistical models, and show that resulting posterior inferences compare favorably to existing posterior approximation algorithms in both accuracy and efficiency.", "histories": [["v1", "Sun, 20 Nov 2016 20:25:39 GMT  (6398kb,D)", "http://arxiv.org/abs/1611.06585v1", "21 pages, 9 figures"], ["v2", "Sun, 19 Feb 2017 17:30:28 GMT  (6941kb,D)", "http://arxiv.org/abs/1611.06585v2", "25 pages, 9 figures, 2 tables"]], "COMMENTS": "21 pages, 9 figures", "reviews": [], "SUBJECTS": "stat.ML cs.LG stat.ME", "authors": ["andrew c miller", "nicholas j foti", "ryan p adams"], "accepted": true, "id": "1611.06585"}, "pdf": {"name": "1611.06585.pdf", "metadata": {"source": "CRF", "title": "Variational Boosting: Iteratively Refining Posterior Approximations", "authors": ["Andrew C. Miller", "Nicholas Foti", "Ryan P. Adams"], "emails": ["acm@seas.harvard.edu", "nfoti@uw.edu", "rpa@seas.harvard.edu"], "sections": [{"heading": "1 Introduction", "text": "Variational inference (VI) [2, 16, 30] is a family of methods that allow an insoluble target distribution (typically only up to a constant) with an insoluble spare part distribution. VI procedures typically minimize deviation from the target by maximizing an appropriately defined target. Often, the class of insoluble distributions is fixed and excludes the neighborhood around the target distributions that prevents arbitrary approach to the target."}, {"heading": "2 Variational Inference", "text": "Given a target distribution with density (x) + ln C (2) for a random variable x (X) = minor Q = minor Q (x), variational inference approximates \u03c0 (x) with a tractable approximate distribution, 2 q (x; \u03bb), from which we can take samples and derive sample-based estimates of the functions of x. However, variational methods minimize KL divergence, KL (q) divergence, KL divergence, KL divergence, KL divergence, KL divergence, KL divergence, KL divergence, we can derive a tractable target based on the properties of KL divergence, which is often referred to as the evidence of lower limit (ELBO), writtenL (\u03bb) = Eq\u03bb [ln)."}, {"heading": "3 Method: Variational Boosting", "text": "We define our approximate distribution to be a mixture of C simpler component distributions (as long as the components are distributable) We define our approximate distribution to be a mixture of C simpler component distributions (C) (x; \u03bb) = C \u2211 c = 1 \u03c1cqc (x; \u03bbc) s.t. \u03c1c \u2265 0 and \u2211 c \u03c1c = 1 (5) where we have complicated component distributions qc4, mixing component parameters \u03bb = (\u03bb1,.) and mixing proportions parameters f \u00b2, or a composition of immutable maps [27]. Component distributions can be any distributions over x from which we can draw samples using a continuous mapping that depends on \u03bbc (e.g., multivariate Normal Normal Normal [14], or a composition of immutable maps [27]. If subsequent expectations and variances are of interest, we designate mixed subjects4q with mixed components."}, {"heading": "3.1 The re-parameterization trick and mixture distributions", "text": "The Re parameter terisierungstrick is a method for calculating q = lnq expectations of an object for which we only have an unbiased estimator: L (\") = p: p: p: p: p: p: p: p: p: p: p: p: p: p: p: p: p: p: p: p: p: p: p: p: p: p: p: p: p: p: p: p: p: p: p: p: p: p: p: p: p: p: p: p: p: p: p: p: p: p: p: p: p: p: p: p: p: p: p: p: p: p: p: p: p: p: p: p: p: p: p: p: p: p: p: p: p: p: p: p: p: p: p: p: p: p: p: p: p: p: p: p: p: p: p: p: p: p: p: p: p: p: p: p: p: p: p: p: p: p: p: p: p: p: p: p: p: p: p: p: p: p: p: p: p: p: p: p: p: p: p: p: p: p: p: p: p: p: p: p: p: p: p: p: p: p: p: p: p: p: p: p: p: p: p: p: p: p: p: p: p: p: p: p: p: p: p: p: p: p: p: p: p: p: p: p: p: p: p: p: p: p: p: p: p: p: p: p: p: p: p: p: p: p: p: p: p: p: p: p: p: p: p: p: p: p: p: p: p: p: p: p: p: p: p: p: p: p: p: p: p: p: p: p: p: p: p: p: p: p: p: p: p: p: p"}, {"heading": "3.2 Adding Components", "text": "In this section we will present details of the proposed algorithm. We will first describe the process of fitting a single component and then the process of adding an additional component to an existing mixture distribution.The adaptation of the first component The process begins by adapting an approximation to \u03c0 (x) with a distribution consisting of a single component. We do this by maximizing the first ELBO object.L (1) (12) Depending on the shapes of \u03c0 and q1, the optimization of the ELBO can be achieved by various methods.A general method of adapting a continuously evaluated component is the calculation of stochastic, unbiased gradients of L (\u03bb1) and the use of stochastic gradients of C."}, {"heading": "3.3 Structured Multivariate Normal Components", "text": "Although our method can use any distribution of components that can be sampled via a continuous imaging, a sensible choice of component distribution is a multivariate normalq (x; \u03bb) = N (x; \u00b5), \u03a3 (\u03bb)))) (18) = 2 / 2 exp (\u2212 12 (x \u2212 \u00b5)). Specifying the structure of the covariance matrix is a choice that largely depends on the dimensionality of the x (x x-RD) and the correlation structure of the target distribution. A frequent first choice of covariance parameters is a diagonal parametrix."}, {"heading": "3.4 Initializing Components", "text": "The introduction of a new component requires the initialization of component parameters. If our component distributions are mixtures of Gaussians (\"), we found that the optimization process is sensitive to initialization. This section describes an importance-weighting scheme for initialization that yields (empirically) good initial values of the component and mixing parameters. Following this principle, we construct this component by first taking meaning-weighted samples from our existing approximationx (\"), which are underrepresented by the existing approximation q (\"), w (\"). The samples with the largest weights w (\") tell us where regions of the target are explained by our approximation."}, {"heading": "3.5 Related Work", "text": "The use of a mixture model as an approximate distribution of varying mixtures is a well-researched idea. Mixtures of medium field approximations [14] introduced mediocre field-like updates for a mixture approximation, using an entropy-bound and model-specific parameter update. Non-parametric variational inference [8] is a black box variation algorithm that approximates a target distribution using a mixture of equally weighted isotropic norms. Authors use a lower limit for the entropy concept in the ELBO to make the optimization process tractable. Similarly, [28] presents a method for adjusting mixture distributions as an approximation. However, their method is limited to mixture component distributions within the exponential family and a common optimization method. Finally, we note that [10] independently and in parallel, a closely related idea can be used for \"reinforced\" approaches to distributions."}, {"heading": "4 Experiments and Analysis", "text": "To supplement the illustrative synthetic examples, in this section we apply increases in variation to align various intractable posterior distributions resulting from real statistical analyses."}, {"heading": "4.1 Hierarchical Binomial Regression", "text": "We borrow an example from [4] and estimate the binocular success rates (batting7Model and data from the mc-stan case studies, http: / / mc-stan.org / documentation / case-studies / pool-binary-trials.htmlaverages) of baseball players using a hierarchical model. The model describes a latent \"Skill-parameter\" for baseball players - the probability of achieving a hit in a given game. The model of the data is?????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????"}, {"heading": "4.2 Multi-level Poisson GLM", "text": "This model was developed to measure the relative rates of stop-and-frisk events for different ethnicities and in different districts [6], and was used as an illustrative example of multi-stage modeling [7, Chapter 15, Section 1]. The model takes into account a precedent and ethnicity effect to describe the relative rate of stop-and-frisk events."}, {"heading": "5 Discussion and Conclusion", "text": "We have proposed a practical variation deduction method that incorporates new components into the approximation and is applicable to a large number of Bayesian interest models. We have demonstrated the method's ability to learn rich representations of complex background parameters across a moderate number of parameters. We see a few possibilities for future work. First, while it is known that mixtures of Gaussians can approximate smooth distributions with arbitrary precision (with enough components) [5], it remains an open question whether our approach of fixing and iterative addition of components using this sequence of ELBO targets will converge. Existing work has shown that this is the case for the alternative direction of KL divergence, KL (\u03c0 | | q) [19, 24] but it remains for KL (q | Phillips) targets to show that components using this sequence of ELBO objectives would converge better in the ideal BO."}, {"heading": "Acknowledgments", "text": "The authors would like to thank Arjumand Masood, Mike Hughes and Finale Doshi-Velez for their helpful conversations. ACM is supported by the Applied Mathematics Program within the Office of Science Advanced Scientific Computing Research of the U.S. Department of Energy under contract number DE-AC02-05CH11231. NF is supported by a Washington Research Foundation Innovation Postdoctoral Fellowship in Neuroengineering and Data Science. RPA is supported by NSF IIS-1421780 and the Alfred P. Sloan Foundation."}], "references": [{"title": "d  iff  in  m  ar  gi  na  l  s  ds Figure 9: Mean percent change in marginal variances for the Poisson GLM. After rank 5, the average percent change is less than 5% \u2014 this estimate is slightly noisy due to the stochastic optimization procedure", "author": ["C Bishop"], "venue": "Pattern recognition and machine learning,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2006}, {"title": "Variational inference: A review for statisticians", "author": ["David M Blei", "Alp Kucukelbir", "Jon D McAuliffe"], "venue": "arXiv preprint arXiv:1601.00670,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2016}, {"title": "Maximum likelihood from incomplete data via the em algorithm. Journal of the royal statistical society", "author": ["Arthur P Dempster", "Nan M Laird", "Donald B Rubin"], "venue": "Series B (methodological),", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 1977}, {"title": "Data analysis using stein\u2019s estimator and its generalizations", "author": ["Bradley Efron", "Carl Morris"], "venue": "Journal of the American Statistical Association,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 1975}, {"title": "Non-parametric estimation of a multivariate probability density", "author": ["V.A. Epanechnikov"], "venue": "Theory Probab. Appl., 14(1):153\u2013158", "citeRegEx": "5", "shortCiteRegEx": null, "year": 1967}, {"title": "An analysis of the nypd\u2019s stop-and-frisk policy in the context of claims of racial bias", "author": ["Andrew Gelman", "Jeffrey Fagan", "Alex Kiss"], "venue": "Journal of the American Statistical Association,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2007}, {"title": "Data analysis using regression and multilevel/hierarchical models", "author": ["Andrew Gelman", "Jennifer Hill"], "venue": null, "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2006}, {"title": "Nonparametric variational inference", "author": ["Samuel Gershman", "Matt Hoffman", "David M Blei"], "venue": "In Proceedings of the 29th International Conference on Machine Learning", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2012}, {"title": "Matrix Computations", "author": ["G.H. Golub", "C.F. Van Loan"], "venue": "JHU Press", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2013}, {"title": "Boosting variational inference", "author": ["Fangjian Guo", "Xiangyu Wang", "Kai Fan", "Tamara Broderick", "David B. Dunson"], "venue": "[stat.ML],", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2016}, {"title": "Matrix Algebra from a Statistician\u2019s Perspective", "author": ["D.A. Harville"], "venue": "Springer-Verlag", "citeRegEx": "11", "shortCiteRegEx": null, "year": 1997}, {"title": "Stochastic variational inference", "author": ["Matthew D Hoffman", "David M Blei", "Chong Wang", "John William Paisley"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2013}, {"title": "The no-u-turn sampler: adaptively setting path lengths in hamiltonian monte carlo", "author": ["Matthew D Hoffman", "Andrew Gelman"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2014}, {"title": "Improving the mean field approximation via the use of mixture distributions", "author": ["Tommi S Jaakkola", "Michael I Jordan"], "venue": "In Learning in graphical models,", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 1998}, {"title": "Composing graphical models with neural networks for structured representations and fast inference", "author": ["Matthew J. Johnson", "David K. Duvenaud", "Alex B. Wiltschko", "Sandeep R. Datta", "Ryan P. Adams"], "venue": "Arxiv preprint arXiv:1603.06277,", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2016}, {"title": "An introduction to variational methods for graphical models", "author": ["Michael I Jordan", "Zoubin Ghahramani", "Tommi S Jaakkola", "Lawrence K Saul"], "venue": "Machine learning,", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 1999}, {"title": "Adam: A method for stochastic optimization", "author": ["Diederik Kingma", "Jimmy Ba"], "venue": "arXiv preprint arXiv:1412.6980,", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2014}, {"title": "Auto-encoding variational bayes", "author": ["Diederik P Kingma", "Max Welling"], "venue": "arXiv preprint arXiv:1312.6114,", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2013}, {"title": "Mixture density estimation", "author": ["Q.J. Li", "A.R. Barron"], "venue": "Advances in Neural Information Processing Systems", "citeRegEx": "19", "shortCiteRegEx": null, "year": 1999}, {"title": "Auxiliary deep generative models", "author": ["Lars Maal\u00f8e", "Casper Kaae S\u00f8nderby", "S\u00f8ren Kaae S\u00f8nderby", "Ole Winther"], "venue": "arXiv preprint arXiv:1602.05473,", "citeRegEx": "20", "shortCiteRegEx": "20", "year": 2016}, {"title": "Firefly monte carlo: Exact mcmc with subsets of data", "author": ["Dougal Maclaurin", "Ryan P Adams"], "venue": "arXiv preprint arXiv:1403.5693,", "citeRegEx": "21", "shortCiteRegEx": "21", "year": 2014}, {"title": "Autograd: Reverse-mode differentiation of native python", "author": ["Dougal Maclaurin", "David Duvenaud", "Ryan P. Adams"], "venue": "ICML workshop on Automatic Machine Learning,", "citeRegEx": "22", "shortCiteRegEx": "22", "year": 2015}, {"title": "Autograd: Reverse-mode differentiation of native Python", "author": ["Dougal Maclaurin", "David Duvenaud", "Matthew Johnson", "Ryan P. Adams"], "venue": null, "citeRegEx": "23", "shortCiteRegEx": "23", "year": 2015}, {"title": "Risk bounds for mixture density estimation", "author": ["Rakhlin A", "D. Panchenko", "S. Mukherjee"], "venue": "ESAIM: Probability and Statistics,", "citeRegEx": "24", "shortCiteRegEx": "24", "year": 2006}, {"title": "Black box variational inference", "author": ["Rajesh Ranganath", "Sean Gerrish", "David M Blei"], "venue": "In AISTATS,", "citeRegEx": "25", "shortCiteRegEx": "25", "year": 2014}, {"title": "Hierarchical variational models", "author": ["Rajesh Ranganath", "Dustin Tran", "David M Blei"], "venue": "In International Conference on Machine Learning,", "citeRegEx": "26", "shortCiteRegEx": "26", "year": 2016}, {"title": "Variational inference with normalizing flows", "author": ["Danilo Rezende", "Shakir Mohamed"], "venue": "In Proceedings of the 32nd International Conference on Machine Learning", "citeRegEx": "27", "shortCiteRegEx": "27", "year": 2015}, {"title": "Fixed-form variational posterior approximation through stochastic linear regression", "author": ["Tim Salimans", "David A Knowles"], "venue": "Bayesian Analysis,", "citeRegEx": "28", "shortCiteRegEx": "28", "year": 2013}, {"title": "Gaussian covariance and scalable variational inference", "author": ["M.W. Seeger"], "venue": "Proceedings of the 27th International Conference on Machine Learning", "citeRegEx": "29", "shortCiteRegEx": null, "year": 2010}, {"title": "Graphical models, exponential families, and variational inference", "author": ["Martin J Wainwright", "Michael I Jordan"], "venue": "Foundations and Trends R  \u00a9 in Machine Learning,", "citeRegEx": "30", "shortCiteRegEx": "30", "year": 2008}, {"title": "Bayesian learning via stochastic gradient langevin dynamics", "author": ["Max Welling", "Yee W Teh"], "venue": "In Proceedings of the 28th International Conference on Machine Learning", "citeRegEx": "31", "shortCiteRegEx": "31", "year": 2011}], "referenceMentions": [{"referenceID": 1, "context": "Variational inference (VI) [2, 16, 30] is a family of methods designed to approximate an intractable target distribution (typically known only up to a constant) with a tractable surrogate distribution.", "startOffset": 27, "endOffset": 38}, {"referenceID": 15, "context": "Variational inference (VI) [2, 16, 30] is a family of methods designed to approximate an intractable target distribution (typically known only up to a constant) with a tractable surrogate distribution.", "startOffset": 27, "endOffset": 38}, {"referenceID": 29, "context": "Variational inference (VI) [2, 16, 30] is a family of methods designed to approximate an intractable target distribution (typically known only up to a constant) with a tractable surrogate distribution.", "startOffset": 27, "endOffset": 38}, {"referenceID": 29, "context": "Often this mismatch between the variational family and the true posterior manifests as underestimating the posterior variances of the model parameters [30].", "startOffset": 151, "endOffset": 155}, {"referenceID": 17, "context": "Our method builds on black-box variational inference methods using the re-parameterization trick [18, 25, 28], applicable to a broad class of target distributions.", "startOffset": 97, "endOffset": 109}, {"referenceID": 24, "context": "Our method builds on black-box variational inference methods using the re-parameterization trick [18, 25, 28], applicable to a broad class of target distributions.", "startOffset": 97, "endOffset": 109}, {"referenceID": 27, "context": "Our method builds on black-box variational inference methods using the re-parameterization trick [18, 25, 28], applicable to a broad class of target distributions.", "startOffset": 97, "endOffset": 109}, {"referenceID": 0, "context": "Variational methods minimize the KL-divergence, KL(q||\u03c0), between q(\u00b7;\u03bb) and the true \u03c0 as a function of variational parameters \u03bb [1].", "startOffset": 130, "endOffset": 133}, {"referenceID": 20, "context": "With a few exceptions [21, 31], most MCMC algorithms require evaluating a log-likelihood that touches all data at each step in the chain (sometimes many times per step).", "startOffset": 22, "endOffset": 30}, {"referenceID": 30, "context": "With a few exceptions [21, 31], most MCMC algorithms require evaluating a log-likelihood that touches all data at each step in the chain (sometimes many times per step).", "startOffset": 22, "endOffset": 30}, {"referenceID": 13, "context": ", multivariate normals [14], or a composition of invertible maps [27]).", "startOffset": 23, "endOffset": 27}, {"referenceID": 26, "context": ", multivariate normals [14], or a composition of invertible maps [27]).", "startOffset": 65, "endOffset": 69}, {"referenceID": 17, "context": "In order for our method to be applicable to a general class of target distributions, we use black-box variational inference methods and the re-parameterization trick [18, 25, 28] to fit each component and mixture weights.", "startOffset": 166, "endOffset": 178}, {"referenceID": 24, "context": "In order for our method to be applicable to a general class of target distributions, we use black-box variational inference methods and the re-parameterization trick [18, 25, 28] to fit each component and mixture weights.", "startOffset": 166, "endOffset": 178}, {"referenceID": 27, "context": "In order for our method to be applicable to a general class of target distributions, we use black-box variational inference methods and the re-parameterization trick [18, 25, 28] to fit each component and mixture weights.", "startOffset": 166, "endOffset": 178}, {"referenceID": 0, "context": "In this section, the mixing parameter \u03c1C+1 \u2208 [0, 1] mixes between the new component, qC+1(\u00b7;\u03bbC+1) and the existing approximation, q.", "startOffset": 45, "endOffset": 51}, {"referenceID": 29, "context": "Further, the independence restriction can introduce local optima in the variational objective [30].", "startOffset": 94, "endOffset": 98}, {"referenceID": 10, "context": "The matrix determinant lemma [11]", "startOffset": 29, "endOffset": 33}, {"referenceID": 8, "context": "Similarly, the Woodbury matrix identity [9] allows us to represent the inverse of \u03a3 as (CC + I(v))\u22121 = I(\u2212v)\u2212 I(\u2212v)C(Ir + CTI(\u2212v)C)\u22121CTI(\u2212v) (27) which involves the inversion of a smaller, r\u00d7 r matrix, which can be done in O(r) time.", "startOffset": 40, "endOffset": 43}, {"referenceID": 2, "context": "For mixture distributions, an efficient inference procedure is Expectation-Maximization (EM) [3].", "startOffset": 93, "endOffset": 96}, {"referenceID": 0, "context": "We suppress details of the general treatment of EM, and focus on EM for mixture models as presented in [1].", "startOffset": 103, "endOffset": 106}, {"referenceID": 13, "context": "Mixtures of mean field approximations [14] introduced mean fieldlike updates for a mixture approximation using a bound on the entropy term and model-specific parameter updates.", "startOffset": 38, "endOffset": 42}, {"referenceID": 7, "context": "Nonparametric variational inference [8] is a black-box variational inference algorithm that approximates a target distribution with a mixture of equally-weighted isotropic normals.", "startOffset": 36, "endOffset": 39}, {"referenceID": 27, "context": "Similarly, [28] present a method for fitting mixture distributions as an approximation.", "startOffset": 11, "endOffset": 15}, {"referenceID": 9, "context": "Finally, we note that [10] independently and in parallel proposed a closely-related idea for iterative \u201cboosted\u201d construction of variational mixture approximations.", "startOffset": 22, "endOffset": 26}, {"referenceID": 28, "context": "Using a low-rank Gaussian as a variational approximation was explored in [29], using a PCA-like algorithm.", "startOffset": 73, "endOffset": 77}, {"referenceID": 25, "context": "We also note that mixture distributions are a type of hierarchical variational model [26], where the component identity can be thought of as latent variables in our variational distribution.", "startOffset": 85, "endOffset": 89}, {"referenceID": 25, "context": "While in [26] the authors optimize a lower bound on the ELBO to fit general hierarchical variational models, our approach integrates out the discrete latent variables because it is tractable to do so.", "startOffset": 9, "endOffset": 13}, {"referenceID": 3, "context": "7 We borrow an example from [4], and estimate the binomial rates of success (batting 7Model and data from the mc-stan case studies, http://mc-stan.", "startOffset": 28, "endOffset": 31}, {"referenceID": 16, "context": "We use adam [17] for each stochastic optimization problem with default parameters.", "startOffset": 12, "endOffset": 16}, {"referenceID": 22, "context": "In all experiments, we use autograd [23, 22] to obtain automatic gradients with respect to new component parameters.", "startOffset": 36, "endOffset": 44}, {"referenceID": 21, "context": "In all experiments, we use autograd [23, 22] to obtain automatic gradients with respect to new component parameters.", "startOffset": 36, "endOffset": 44}, {"referenceID": 12, "context": "To highlight the fidelity of our method, we compare Variational Boosting to mean field VI and the No-U-Turn Sampler (NUTS) [13].", "startOffset": 123, "endOffset": 127}, {"referenceID": 5, "context": "This model was formulated to measure the relative rates of stop-and-frisk events for different ethnicities and in different precincts [6], and has been used as illustrative example of multi-level modeling [7, Chapter 15, Section 1].", "startOffset": 134, "endOffset": 137}, {"referenceID": 4, "context": "sians can approximate smooth distributions to arbitrary precision (with enough components) [5], it remains an open question if our approach of fixing and iteratively adding components using this sequence of ELBO objectives will converge.", "startOffset": 91, "endOffset": 94}, {"referenceID": 18, "context": "Existing work has shown that this is the case for the alternative direction of KL-divergence, KL(\u03c0||q) [19, 24], but it remains to be shown for KL(q||\u03c0).", "startOffset": 103, "endOffset": 111}, {"referenceID": 23, "context": "Existing work has shown that this is the case for the alternative direction of KL-divergence, KL(\u03c0||q) [19, 24], but it remains to be shown for KL(q||\u03c0).", "startOffset": 103, "endOffset": 111}, {"referenceID": 26, "context": "For instance, compositions of invertible maps have been used to enrich variational families [27], as well as auxiliary variable variational models [20].", "startOffset": 92, "endOffset": 96}, {"referenceID": 19, "context": "For instance, compositions of invertible maps have been used to enrich variational families [27], as well as auxiliary variable variational models [20].", "startOffset": 147, "endOffset": 151}, {"referenceID": 11, "context": "When optimizing parameters of a variational family, it has been shown that the natural gradient can be more robust and lead to better optima [12, 15].", "startOffset": 141, "endOffset": 149}, {"referenceID": 14, "context": "When optimizing parameters of a variational family, it has been shown that the natural gradient can be more robust and lead to better optima [12, 15].", "startOffset": 141, "endOffset": 149}], "year": 2016, "abstractText": "We propose a black-box variational inference method to approximate intractable distributions with an increasingly rich approximating class. Our method, termed variational boosting, iteratively refines an existing variational approximation by solving a sequence of optimization problems, allowing the practitioner to trade computation time for accuracy. We show how to expand the variational approximating class by incorporating additional covariance structure and by introducing new components to form a mixture. We apply variational boosting to synthetic and real statistical models, and show that resulting posterior inferences compare favorably to existing posterior approximation algorithms in both accuracy and efficiency.", "creator": "LaTeX with hyperref package"}}}