{"id": "1611.06652", "review": {"conference": "NIPS", "VERSION": "v1", "DATE_OF_SUBMISSION": "21-Nov-2016", "title": "Scalable Adaptive Stochastic Optimization Using Random Projections", "abstract": "Adaptive stochastic gradient methods such as AdaGrad have gained popularity in particular for training deep neural networks. The most commonly used and studied variant maintains a diagonal matrix approximation to second order information by accumulating past gradients which are used to tune the step size adaptively. In certain situations the full-matrix variant of AdaGrad is expected to attain better performance, however in high dimensions it is computationally impractical. We present Ada-LR and RadaGrad two computationally efficient approximations to full-matrix AdaGrad based on randomized dimensionality reduction. They are able to capture dependencies between features and achieve similar performance to full-matrix AdaGrad but at a much smaller computational cost. We show that the regret of Ada-LR is close to the regret of full-matrix AdaGrad which can have an up-to exponentially smaller dependence on the dimension than the diagonal variant. Empirically, we show that Ada-LR and RadaGrad perform similarly to full-matrix AdaGrad. On the task of training convolutional neural networks as well as recurrent neural networks, RadaGrad achieves faster convergence than diagonal AdaGrad.", "histories": [["v1", "Mon, 21 Nov 2016 05:15:50 GMT  (437kb,D)", "http://arxiv.org/abs/1611.06652v1", "To appear in Advances in Neural Information Processing Systems 29 (NIPS 2016)"]], "COMMENTS": "To appear in Advances in Neural Information Processing Systems 29 (NIPS 2016)", "reviews": [], "SUBJECTS": "stat.ML cs.LG", "authors": ["gabriel krummenacher", "brian mcwilliams", "yannic kilcher", "joachim m buhmann", "nicolai meinshausen"], "accepted": true, "id": "1611.06652"}, "pdf": {"name": "1611.06652.pdf", "metadata": {"source": "CRF", "title": "Scalable Adaptive Stochastic Optimization Using Random Projections", "authors": ["Gabriel Krummenacher", "Brian McWilliams", "Yannic Kilcher", "Joachim M. Buhmann"], "emails": ["gabriel.krummenacher@inf.ethz.ch", "brian@disneyresearch.com", "yannic.kilcher@inf.ethz.ch", "jbuhmann@inf.ethz.ch", "meinshausen@stat.math.ethz.ch"], "sections": [{"heading": "1 Introduction", "text": "Recently, we have a problem with the fact that most of them are able to pay their debts, in the form of debt, debt, debt, debt, debt, debt, debt, debt, debt, debt, debt, debt, debt, debt, debt, debt, debt, debt, debt, debt, debt, debt, debt, debt, debt, debt, debt, debt, debt, debt, debt, debt, debt, debt, debt, debt."}, {"heading": "1.1 Related work", "text": "Motivated by the problem of training deep neural networks, many new adaptive optimization methods for risk minimization have recently been proposed, most of which are first-order methods similar to ADAGRAD, which suggest alternative normalization factors [22, 29, 7]. Several authors suggest efficient stochastic variants of classical second-order methods such as LBFGS [6, 21]. Effective algorithms exist to update the inversion of the Hessian approach by applying the matrix inversion problem or directly updating the Hessian vector product by using the \"double loop\" algorithm, but these are not applicable to ADAGRAD algorithms. In the convex setting of these methods, great theoretical and practical benefit can be demonstrated through first-order methods, but they have yet to be applied comprehensively to the training of deep networks."}, {"heading": "1.2 Problem setting", "text": "The problem considered by [10] is stochastic online optimization, where the goal in each step is to predict a point \u03b2 \u03b2 = > verse \u03b2 = Rp that achieves little regret in relation to a fixed optimal predictor \u03b2opt for a sequence of (convex) functions Ft (\u03b2). After T rounds, the regret can be defined as R (T) = \u2211 T = 1 Ft (\u03b2t) \u2212 \u2211 T = 1 Ft (\u03b2 opt). First, we consider functions Ft of the form Ft (\u03b2): = ft (\u03b2) + \u0432 (\u03b2), which are convex loss or regulation functions. Throughout the process, the vector gt-t gradient (\u03b2t) refers to a specific subgradient of the loss function. Standard first-order methods can update at each step by moving in the opposite direction of gt, according to a step parameter < each algorithm may be different for the AGRAD family instead."}, {"heading": "2 Stochastic optimization in high dimensions", "text": "The question arises as to what we hope to gain by introducing additional computational complexity. To motivate our contribution, we can first point out an analogy of the discussion in Section 3.1 that ADA-LR has the same data and condenses it. We argue that we first check the theoretical properties of the ADAGRAD algorithms that adopt the g1 notation. (Proposition 1) ADAGRAD and ADA-FULL achieve the following regret (Corollaries 6 & 11 from [10] and: RD (T) respectively."}, {"heading": "3.1 Randomized low-rank approximation", "text": "As a first approach to the inverse square root of Gt with a fast randomized single value decomposition (SVD), we describe this approach (SVD) in two stages: First, we calculate an approximate base for the bandwidth of Gt; then we use Q to calculate an approximate base of Gt by building the matrix G = Q > Gt; then, using a structured random projection, we directly calculate a random base for the SVD of Gt. An approximate base Q can be efficiently calculated by building the matrix G = G = GtL using a structured random projection; and then we construct an orthonorthonormal base for the reach of Gt of QR decomposition. Randomized SVD allows us to calculate the square root and pseudo-inverse of the projection Ht by setting H \u2212 1t = V."}, {"heading": "3.3 Practical algorithms", "text": "Here we outline some simple modifications of the RADAGRAD algorithm to improve practical effectiveness. Corrected update. The random projection step therefore retains at most the eigenvalues of Gt. If the assumption of a low effective rank does not apply, important information from the p \u2212 \u03c4 smallest eigenvalues could be discarded. RADAGRAD therefore uses the corrected updated value of Gt + 1 = \u03b2t \u2212 \u03b7V (\u043f1 / 2 + \u03b4I) \u2212 1V > gt \u2212 ig t, calculating the current value on the orthogonal value of the gradient captured by the random projection of Gt. This ensures that important variation in the gradient, which is insufficiently approximated by the random projection, is not completely lost. As a result, the projection of the current gradient on the orthogonal value of Gt is."}, {"heading": "4 Experiments", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "4.1 Low effective rank data", "text": "We compare the performance of our proposed algorithms with the diagonal and full-matrix ADAGRAD variants in the idealized environment in which the data are dense but have a low effective rank. We generate binary classification data with n = 1000 and p = 125. The data is generally sampled from a Gaussian distribution N (\u00b5c, \u03a3), where \u03a3 with rapidly decreasing eigenvalues \u03bbj (\u03a3) = \u03bb0j \u2212 \u03b1 with \u03b1 = 1.3, \u03bb0 = 30. Each of the two classes has a different mean, \u00b5c. For each algorithm, the learning rates are adjusted by cross-validation. Results for 5 epochs are calculated over 5 runs with different permutations of the data set and instantiations of the random projection for ADA-LR and RADAGRAD. For the random projection, we use a sampling factor so that the vaping factor is so far apart that the AGADADNitariates of the highest individual values projected and then the training factor in ADADADD are approximately the same."}, {"heading": "4.2 Non-convex optimization in neural networks", "text": "In fact, most people who stand up for people's rights are not prepared to fight for the rights of others."}, {"heading": "5 Discussion", "text": "We have introduced ADA-LR and RADAGRAD, which approximate the full proximal concept of ADAGRAD using fast, structured random projections. ADA-LR has a similar regret to ADA-FULL, and both methods achieve similar empirical performance at a fraction of the computational cost. Importantly, RADAGRAD can be easily modified to take advantage of standard improvements such as variance reduction, and in particular the use of variance reduction in combination has strong benefits for non-convex optimization in both Constitutional and Recursive Neural Networks. We have observed a marked improvement over widely used techniques such as ADAGRAD and SVRG, whose combination has recently proven to be an excellent choice for non-convex optimization [1]. In addition, we have tried to include exponential forgetfulness models such as RMSPROP and ADAM in the RADAGRAD framework, with these methods deteriorating performance."}, {"heading": "A Computational Complexity", "text": "B RADA-VR: RADAGRAD with variance reduction.Algorithm 3 RADA-VRInput: \u03b7 > 0, \u03b4 \u2265 0, \u03c4, S Number of epochs, m Iterations per epoch, initially \u03b210 1: for s = 1... S do 2: \u00b5 = for ni = 1 fi (\u03b2s0) 3: for t = 1.. m \u2212 1 do 4: Calculation of the VR gradient: gt = ft (\u03b2st) \u2212 ft (\u03b2s0) + \u00b5 5: Project: g-t = 6: G-t = G-t \u2212 1 + gtg-t 7: Qt, Rt-qr _ update (Qt \u2212 1, Rt \u2212 1, gt, g-t) 8: B = G-T-Q 9: U, \u03a3, W = B {SVD} 10: V = WQ-T = 11: \u00df + 1 = \u03b2-V (HQ \u2212 1 / 2 + g-T) 8: B = G-Q: 10: U = \u03b2D = 12-V = B-D = B-D = B-T: 12"}, {"heading": "C Analysis", "text": "The following proof is based on the proof for theorem 7 in [10]. The key difference is that instead of the square root and (pseudo) inversion of the complete matrix Gt: G 1 / 2 t and S \u2020 t we use the approximate square root and inversion based on randomized SVD [16]: S 2 t = (QQ > Gt) 1 / 2 and S 2 = (QQ > Gt) \u2212 1 / 2. Essentially, we use the approximate function T = < x, S \u00b2 tx > or T = < x, H \u00b2 tx > where we set H \u00b2 tx > is the approximate basis for the reach of the matrix Gt [16].We first give the following facts about the relationship between G and G \u00b2 -1 / 2.Lemma 3."}, {"heading": "D Supporting Results", "text": "Theorem 5 (SRFT approximation error (theorem 11.2 in [16]). Definition = \u221a 1 + 7p / \u03c4 \u00b7 \u03c3k + 1 applies with maximum error probability O (k \u2212 1)."}], "references": [], "referenceMentions": [], "year": 2016, "abstractText": "Adaptive stochastic gradient methods such as ADAGRAD have gained popularity in<lb>particular for training deep neural networks. The most commonly used and studied<lb>variant maintains a diagonal matrix approximation to second order information<lb>by accumulating past gradients which are used to tune the step size adaptively. In<lb>certain situations the full-matrix variant of ADAGRAD is expected to attain better<lb>performance, however in high dimensions it is computationally impractical. We<lb>present ADA-LR and RADAGRAD two computationally efficient approximations<lb>to full-matrix ADAGRAD based on randomized dimensionality reduction. They are<lb>able to capture dependencies between features and achieve similar performance to<lb>full-matrix ADAGRAD but at a much smaller computational cost. We show that the<lb>regret of ADA-LR is close to the regret of full-matrix ADAGRAD which can have<lb>an up-to exponentially smaller dependence on the dimension than the diagonal<lb>variant. Empirically, we show that ADA-LR and RADAGRAD perform similarly to<lb>full-matrix ADAGRAD. On the task of training convolutional neural networks as<lb>well as recurrent neural networks, RADAGRAD achieves faster convergence than<lb>diagonal ADAGRAD.", "creator": "LaTeX with hyperref package"}}}