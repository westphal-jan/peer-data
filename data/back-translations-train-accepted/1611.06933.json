{"id": "1611.06933", "review": {"conference": "AAAI", "VERSION": "v1", "DATE_OF_SUBMISSION": "21-Nov-2016", "title": "Unsupervised Learning for Lexicon-Based Classification", "abstract": "In lexicon-based classification, documents are assigned labels by comparing the number of words that appear from two opposed lexicons, such as positive and negative sentiment. Creating such words lists is often easier than labeling instances, and they can be debugged by non-experts if classification performance is unsatisfactory. However, there is little analysis or justification of this classification heuristic. This paper describes a set of assumptions that can be used to derive a probabilistic justification for lexicon-based classification, as well as an analysis of its expected accuracy. One key assumption behind lexicon-based classification is that all words in each lexicon are equally predictive. This is rarely true in practice, which is why lexicon-based approaches are usually outperformed by supervised classifiers that learn distinct weights on each word from labeled instances. This paper shows that it is possible to learn such weights without labeled data, by leveraging co-occurrence statistics across the lexicons. This offers the best of both worlds: light supervision in the form of lexicons, and data-driven classification with higher accuracy than traditional word-counting heuristics.", "histories": [["v1", "Mon, 21 Nov 2016 18:30:17 GMT  (41kb)", "http://arxiv.org/abs/1611.06933v1", "to appear in AAAI 2017"]], "COMMENTS": "to appear in AAAI 2017", "reviews": [], "SUBJECTS": "cs.LG cs.CL stat.ML", "authors": ["jacob eisenstein"], "accepted": true, "id": "1611.06933"}, "pdf": {"name": "1611.06933.pdf", "metadata": {"source": "CRF", "title": "Unsupervised Learning for Lexicon-Based Classification", "authors": ["Jacob Eisenstein"], "emails": [], "sections": [{"heading": null, "text": "ar Xiv: 161 1,06 933v 1 [cs.L G] 21 Nov 2"}, {"heading": "Introduction", "text": "It is. (It is.) It is. (it is.) It is. (it is.) It is. (it is.) It is. (it is.) It is. (it is.) It is. (it is.) It is. (it is.) It is. (it is.) It is. (it is.) It is. (it is.) It is. (it is.) It is. (it is.) It is. (it is.) It is. (it is.) It is. (it is.) It is. (it is.) It is. (it is. (it is.) It is. (...) It is. (it is.) It is. (it is.) It is. (it is. (it is.) It is. (it is.) It is. (it is. (it is.) It is. (it is.) It is. (it. (it is.) It is. (it is. (it.) It is. (it is. (it.) It is. (it is. (it.) It is. (it. (it.) It is. (it.) It is. (it. (it.) It is. (it. (it.) It is. (it. (it.) It is. (it. (it is. (it.) It is. (it. (it.) it. (it is. (it.) It is. (it. (it.) it. (it. (it. (it.) It is. (it. (it.) It is.) it. (it is. (it. (it.) is. (it. (it.) It is. (it is. (it.) It is. (it. (it is.) It is. (it is. (it. (is.) It is. (it. (it.) It is. (it is."}, {"heading": "Lexicon-Based Classification as Na\u0131\u0308ve Bayes", "text": "I will start by showing how the lexicon-based classification can be justified under this premise. (1) We assume that a specific case of classification (1) is probable. (1) Suppose we have a previous probability PY for the label Y, and a probability function PX-Y, where X-y is a random variable corresponding to a vector of the number of words. (2) P (y-x) P (y) P (y) P (y) P (y) P (y) P (y) P (y) P (y) P (y). Assuming that the costs of each type of misclassification error are identical, then the minimum Bayes classification rule is identical, (3) log Pr (Y = 0) + logP (x-Y = 0) log Pr (Y = 1) + logP (x-x), and logP (x-x) moving into the log domain for the simplicity of notation."}, {"heading": "Analysis of Lexicon-Based Classification", "text": "One advantage of deriving a formal basis for the lexiconbased classification is that it is possible to analyze its expected performance. For a label y, let's make a correct prediction of the number of in-lexicon words such as my = \u2211 i-Wy xi, and the number of opposite lexicon words such as m-y = i-W-y xi. lexicon-based classification makes a correct prediction whenever my > m-i-Wy is used for the correct designation y. To evaluate the probability that my > m-y is sufficient to calculate the expectation and variance of the difference of my \u2212 m difference, we can treat this difference as approximately normally distributed and calculate the probability that the difference is positive using the Gaussian cumulative distribution function (CDF). Let's use the convenience of the notation s\u00b5, s\u00b5 i-W0\u00b5i-W1\u00b5i. (14) Recall that we have already made the same assumptions for the two sms."}, {"heading": "Justifying the Word-Appearance Heuristic", "text": "(8) (8) (8) (8) (8) (8) (8) (8) (8) (8) (8) (8) (8) (8) (8) (8) (8) (8) (8)) (8) (8) (8) (8)) (8) (8) (8) (8) (8) (8) (8)) (8) (8) (8) (8) (8) (8) (8) (8) (8) (8) (8) (8) (8) (8) (8) (8) (8) (8) (8) (8) (8) (8) (8) (8) (8) (8) (8) (8) (8) (8) (8) (8) (8) (8) (8) (8) (8) (8) (8) (8) (8) (8) (8) (8) (8) (8) (8) (8) (8) (8) (8) (8) (8) (8) (8) (8) (8) (8) (8) (8) (8) (8) (8) (8) (8) (8) (8) (8) (8) (8) (8) (8) (8) (8) (8) (8) (8) (8) (8) (8) (8) (8) (8) (8) (8) (8) (8) (8) (8) (8) (8) (8) (8) (8) (8) (8) (8) (8) (8) (8) (8) (8) (8) (8) (8) (8) (8) (8) (8) (8) (8) (8) (8) (8) (8) (8) (8) (8) (8) (8) (8) (8) (8) (8) (8) (8) (8 (8) (8) (8) (8) (8) (8) (8) (8 (8) (8) (8) (8) (8"}, {"heading": "Estimating Word Predictiveness", "text": "A crucial simplification of lexicon-based classification is that all words in any lexicon are equally predictable. In reality, words can be more or less predictive than class names for reasons such as ambiguity (e.g., good) and degree (e.g., good versus error-free). By introducing a pro-word predictiveness factor i in (8), we can arrive at a model that is a limited form of Na\u00efve Bayes. (The limitation is that the probability of non-lexicon words being identical across classes is limited.) If the labeled data were available, this model could be estimated from the maximum probability. This section shows how to estimate the model without labeled data by applying the method of implying that the basic probabilities \u00b5i can be estimated directly from an unlabeled word. The challenge is to estimate the parameters for all words in the two lexicon."}, {"heading": "Evaluation", "text": "An empirical evaluation is performed on four datasets in two languages. All datasets involve binary classification problems, and performance is quantified by the Area-under-thecurve (AUC), a measure of classification performance based on unbalanced class distributions. A perfect classifier achieves AUC = 1; in expectation of a random decision gives AUC = 0.5.Datasets The proposed method is based on co-occurrence counts, and is therefore best suited for documents that each contain at least a few sentences. In this sense, the following datasets are used in the evaluation: Amazon English-language product reviews across four do-mains; the proposed method uses 8,000 labeled and other 19677 unlabeled words (Blitzer, Dredze and Pereira 2007).Cornell 2000 English-language film reviews (version 2.0), labeled as positive or negative (Pang and Lee 2004).Corpine CusCfilm Spanish, rated on a scale from one to five."}, {"heading": "Related work", "text": "Turney (2002) uses meaningful mutual information to estimate the \"semantic orientation\" of all vocabulary words interacting with a small seed set, an approach that was later extended to the social media domain by using emoticons as a seed set (Kiritchenko, Zhu and Mohammad 2014).Like the approach proposed here, the basic intuition is to use the coevent statistics to learn weights for individual words; however, PMI is a heuristic topic that is not justified by a probabilistic model of the text classification problem. PMI-based classification undercuts PROBLEX-MULT and PROBLEX-DCM for all four sets of datasets in our evaluation.The method of moments has become an increasingly popular estimator in uncontrolled machine learning, with its application in theme models (Anandkumar al), 2014 sequence models, 2012 sequence models, and Cohen (Ksu) being conditional on only."}, {"heading": "Conclusion", "text": "Dictionary-based classification is a popular heuristic that has not yet been analyzed from the perspective of machine learning. It provides two techniques to improve unattended binary classification: a method-of-moments estimator for word predictivity and a Bayesian adjustment for repeated counting of the same word. The Method-of-Moments estimator delivers significantly better performance than traditional lexicon-based classification, without the need for additional annotation efforts. Future work will consider generalizing to multi-class classification and more ambitiously expanding to multi-word units."}, {"heading": "Supplementary Material: Estimation Details", "text": "This supplement describes the estimation method in detail. The paper uses the method of moments to derive the following optimization problem: min \u03b3 (0), \u03b3 (1) 12 \u2211 i-W0 (ci \u2212 E [ci]) 2 + 12 \u2211 j-W1 (cj \u2212 E [cj]) 2s.t. \u00b5 (0) \u00b7 \u03b3 (0) \u2212 \u00b5 (1) \u00b7 \u03b3 (1) = 0-i-W00 \u2264 (0) i < 1-j [cj] W10 \u2264 \u03b3 (1) j < 1. (40) This problem is biconvex in the parameters \u03b3 (0), \u03b3 (1). We optimize with the method of alternating direction of multipliers (ADMM; Boyd et al. 2011). In the rest of this document x \u00b7 y is used to indicate a point product between x and y, and x is used to indicate an elementary product."}, {"heading": "ADMM for biconvex problems", "text": "In general, we assume that the function F (x, z) is in x and z biconvex and that the constraint G (x, z) = 0 in x and z affin, min x, z F (x, z) (41) s.t.G (x, z) = 0. (42) We can optimize via ADMM through the following updates (Boyd et al 2011, section 9.2), xk + 1 \u2190 argminxF (x, z) + (2) | G (x, z) + (2) | G (x, uk + 1) + uk | | 22 (44) uk + 1 (UK + G (xk + 1, zk + 1). (45) Now we assume that we have a more general optimization problem, min x, z F (x, uk) + (2) + uk (z + 1) + uk + 1) + 1 (UK + G (xk + 1, zk + 1), zk + 1). (45) + (Z) Now we assume that we have a more general optimization problem, z (x, z) + uk (x, uk + 1) + 1) + 1 (Z (Z) + 1 (UK + 1, zk + 1, zk + 1). (45)"}, {"heading": "Application to moment-matching", "text": "In the application to the moment matching estimate we have: x, \u03b3 (0) (50) q = 1 (1) (51) G (x, z), \u00b5 (0) - 2 (0) - (0) - 2 (0) - 3 (52) Cx (0, 1) (53) F (x, z), 12 (0) - 2 (0) - 2 (0) - (0) - 4 (0) - 4 (0) - 4 (0) - 4 (0) - 4 (0) - 4 (0) - 4 (0) - 4 (0) - 4 (0) - 4 (0) - 4 (0) - 4 (0) - 4 (0) - 4 (0) - 4 - (0) - 5 - (5) - 5 - 5 (5) (5) (5 - 5) (5) (5 - (0) (5) (5 - 5) (5) (0 - 5) (5) (5 - (5) (5) (5 - (5) (5) (0 - (5) (5) (5 - (5) - (5) (0 - (5) - (5) (5) (0 - (5) - (5) (0 - (5) - (5) (5) (0 - (5) - (5) (0 - (5) - (5) (5) (5 - (5) (0 - (5) - (5) (5) (5 - (5) (0 - (5) (5) (5) (5 - (5) (5 - (5) (5) (5) (5 - (5) (0 - (5) (5) (5 - (5) (5) (5) (0 - (5) (5 - (5) (5) (5) (5) (5 - (5) (0 - (5) (5) (5) (5) (5) (5 - (5) (5) (5) (5 - (5) (5) (5 - (0 - (5) (0 - (5) (5) (5) (5) (5 - (5) (5) (5 - (5) (5) (5) (5"}, {"heading": "Computer and System Sciences 78(5):1460\u20131480.", "text": "Kiritchenko, S.; Zhu, X.; and Mohammad, S. M. 2014. Sentiment analysis of short informal texts. Journal of Artificial Intelligence Research 50: 723-762.Laver, M., and Garry, J. 2000. Estimating policy positions from political texts. American Journal of Political Science 619-634.Liu, B. 2015. Sentiment Analysis: Mining Opinions, Sentiments, and Emotions. Cambridge University Press.Maas, A. L.; Daly, R. E.; Pham, P. T.; Huang, D.; Ng, A. Y.; and Potts, C. 2011. Learning word Vectors for sentiments Analysis. In Proceedings of the Association for Computational Linguistics (ACL).Madsen, R. E.; Kauchak, D.; and Elkan, C. 2005. Modeling word burstiness using the dirichlet distribution. In Proceedings of the International Conference on Machine Learning, 545-552. ACM."}, {"heading": "In Proceedings of Empirical Methods for Natural Language", "text": "Processing (EMNLP), 79-86. Polanyi, L., and Zaenen, A. 2006. Contextual valence shifters. In Computing attitude and Affect in text: Theory and applications. Springer.Qiu, G.; Liu, B.; Bu, J.; and Chen, C. 2011. Opinion word expansion and target extraction through double propagation. Computational linguistics 37 (1): 9-27.Ribeiro, F. N.; Arau \u0301 jo, M.; Gonc \u0445 alves, P.; Gonc \u0445 alves, M. A.; and Benevenuto, F. 2016. Sentibench-a benchmark comparison of state-of-the-practice sentiment analysis methods. EPJ Data Science 5 (1): 1-29.Settles, B. 2011. Closing the loop: Fast, interactive semi-supervised annotation on features and instances."}], "references": [{"title": "Tensor decompositions for learning latent variable models", "author": ["A. Anandkumar", "R. Ge", "D. Hsu", "S.M. Kakade", "M. Telgarsky"], "venue": "The Journal of Machine Learning Research 15(1):2773\u20132832.", "citeRegEx": "Anandkumar et al\\.,? 2014", "shortCiteRegEx": "Anandkumar et al\\.", "year": 2014}, {"title": "Learning topic models - going beyond SVD", "author": ["S. Arora", "R. Ge", "A. Moitra"], "venue": "FOCS, 1\u201310.", "citeRegEx": "Arora et al\\.,? 2012", "shortCiteRegEx": "Arora et al\\.", "year": 2012}, {"title": "Better documentlevel sentiment analysis from rst discourse parsing", "author": ["P. Bhatia", "Y. Ji", "J. Eisenstein"], "venue": "Proceedings of Empirical Methods for Natural Language Processing (EMNLP).", "citeRegEx": "Bhatia et al\\.,? 2015", "shortCiteRegEx": "Bhatia et al\\.", "year": 2015}, {"title": "Biographies, bollywood, boom-boxes and blenders: Domain adaptation for sentiment classification", "author": ["J. Blitzer", "M. Dredze", "F. Pereira"], "venue": "Proceedings of the Association for Computational Linguistics (ACL), 440\u2013447.", "citeRegEx": "Blitzer et al\\.,? 2007", "shortCiteRegEx": "Blitzer et al\\.", "year": 2007}, {"title": "Distributed optimization and statistical learning via the alternating direction method of multipliers", "author": ["S. Boyd", "N. Parikh", "E. Chu", "B. Peleato", "J. Eckstein"], "venue": "Foundations and Trends R", "citeRegEx": "Boyd et al\\.,? 2011", "shortCiteRegEx": "Boyd et al\\.", "year": 2011}, {"title": "Spectral learning of latent-variable PCFGs: Algorithms and sample complexity", "author": ["S.B. Cohen", "K. Stratos", "M. Collins", "D.P. Foster", "L. Ungar"], "venue": "Journal of Machine Learning Research 15:2399\u20132449.", "citeRegEx": "Cohen et al\\.,? 2014", "shortCiteRegEx": "Cohen et al\\.", "year": 2014}, {"title": "Predicting the semantic orientation of adjectives", "author": ["V. Hatzivassiloglou", "K.R. McKeown"], "venue": "Proceedings of the Association for Computational Linguistics (ACL), 174\u2013181.", "citeRegEx": "Hatzivassiloglou and McKeown,? 1997", "shortCiteRegEx": "Hatzivassiloglou and McKeown", "year": 1997}, {"title": "A spectral algorithm for learning hidden markov models", "author": ["D. Hsu", "S.M. Kakade", "T. Zhang"], "venue": "Journal of Computer and System Sciences 78(5):1460\u20131480.", "citeRegEx": "Hsu et al\\.,? 2012", "shortCiteRegEx": "Hsu et al\\.", "year": 2012}, {"title": "Sentiment analysis of short informal texts", "author": ["S. Kiritchenko", "X. Zhu", "S.M. Mohammad"], "venue": "Journal of Artificial Intelligence Research 50:723\u2013762.", "citeRegEx": "Kiritchenko et al\\.,? 2014", "shortCiteRegEx": "Kiritchenko et al\\.", "year": 2014}, {"title": "Estimating policy positions from political texts", "author": ["M. Laver", "J. Garry"], "venue": "American Journal of Political Science 619\u2013634.", "citeRegEx": "Laver and Garry,? 2000", "shortCiteRegEx": "Laver and Garry", "year": 2000}, {"title": "Sentiment Analysis: Mining Opinions, Sentiments, and Emotions", "author": ["B. Liu"], "venue": "Cambridge University Press.", "citeRegEx": "Liu,? 2015", "shortCiteRegEx": "Liu", "year": 2015}, {"title": "Learning word vectors for sentiment analysis", "author": ["A.L. Maas", "R.E. Daly", "P.T. Pham", "D. Huang", "A.Y. Ng", "C. Potts"], "venue": "Proceedings of the Association for Computational Linguistics (ACL).", "citeRegEx": "Maas et al\\.,? 2011", "shortCiteRegEx": "Maas et al\\.", "year": 2011}, {"title": "Modeling word burstiness using the dirichlet distribution", "author": ["R.E. Madsen", "D. Kauchak", "C. Elkan"], "venue": "Proceedings of the 22nd international conference on Machine learning, 545\u2013552. ACM.", "citeRegEx": "Madsen et al\\.,? 2005", "shortCiteRegEx": "Madsen et al\\.", "year": 2005}, {"title": "Estimating a dirichlet distribution", "author": ["T. Minka"], "venue": null, "citeRegEx": "Minka,? \\Q2012\\E", "shortCiteRegEx": "Minka", "year": 2012}, {"title": "Semantic orientation for polarity classification in spanish reviews. Expert Systems with Applications 40(18):7250\u20137257", "author": ["M.D. Molina-Gonz\u00e1lez", "E. Mart\u0131\u0301nez-C\u00e1mara", "M.-T. Mart\u0131\u0301nValdivia", "J.M. Perea-Ortega"], "venue": null, "citeRegEx": "Molina.Gonz\u00e1lez et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Molina.Gonz\u00e1lez et al\\.", "year": 2013}, {"title": "A sentimental education: Sentiment analysis using subjectivity summarization based on minimum cuts", "author": ["B. Pang", "L. Lee"], "venue": "Proceedings of the Association for Computational Linguistics (ACL), 271\u2013278.", "citeRegEx": "Pang and Lee,? 2004", "shortCiteRegEx": "Pang and Lee", "year": 2004}, {"title": "Opinion mining and sentiment analysis", "author": ["B. Pang", "L. Lee"], "venue": "Foundations and trends in information retrieval 2(1-2):1\u2013135.", "citeRegEx": "Pang and Lee,? 2008", "shortCiteRegEx": "Pang and Lee", "year": 2008}, {"title": "Thumbs up?: sentiment classification using machine learning techniques", "author": ["B. Pang", "L. Lee", "S. Vaithyanathan"], "venue": "Proceedings of Empirical Methods for Natural Language Processing (EMNLP), 79\u201386.", "citeRegEx": "Pang et al\\.,? 2002", "shortCiteRegEx": "Pang et al\\.", "year": 2002}, {"title": "Contextual valence shifters", "author": ["L. Polanyi", "A. Zaenen"], "venue": "Computing attitude and affect in text: Theory and applications. Springer.", "citeRegEx": "Polanyi and Zaenen,? 2006", "shortCiteRegEx": "Polanyi and Zaenen", "year": 2006}, {"title": "Opinion word expansion and target extraction through double propagation", "author": ["G. Qiu", "B. Liu", "J. Bu", "C. Chen"], "venue": "Computational linguistics 37(1):9\u201327.", "citeRegEx": "Qiu et al\\.,? 2011", "shortCiteRegEx": "Qiu et al\\.", "year": 2011}, {"title": "Sentibench-a benchmark comparison of state-of-the-practice sentiment analysis methods", "author": ["F.N. Ribeiro", "M. Ara\u00fajo", "P. Gon\u00e7alves", "M.A. Gon\u00e7alves", "F. Benevenuto"], "venue": "EPJ Data Science 5(1):1\u201329.", "citeRegEx": "Ribeiro et al\\.,? 2016", "shortCiteRegEx": "Ribeiro et al\\.", "year": 2016}, {"title": "Closing the loop: Fast, interactive semi-supervised annotation with queries on features and instances", "author": ["B. Settles"], "venue": "Proceedings of the Conference on Empirical Methods in Natural Language Processing, 1467\u20131478. Association for Computational Linguistics.", "citeRegEx": "Settles,? 2011", "shortCiteRegEx": "Settles", "year": 2011}, {"title": "Discourse level opinion interpretation", "author": ["S. Somasundaran", "J. Wiebe", "J. Ruppenhofer"], "venue": "Proceedings", "citeRegEx": "Somasundaran et al\\.,? 2008", "shortCiteRegEx": "Somasundaran et al\\.", "year": 2008}, {"title": "Lexicon-based methods for sentiment analysis", "author": ["M. Taboada", "J. Brooke", "M. Tofiloski", "K. Voll", "M. Stede"], "venue": "Computational linguistics 37(2):267\u2013307.", "citeRegEx": "Taboada et al\\.,? 2011", "shortCiteRegEx": "Taboada et al\\.", "year": 2011}, {"title": "The psychological meaning of words: LIWC and computerized text analysis methods", "author": ["Y.R. Tausczik", "J.W. Pennebaker"], "venue": "Journal of Language and Social Psychology 29(1):24\u201354.", "citeRegEx": "Tausczik and Pennebaker,? 2010", "shortCiteRegEx": "Tausczik and Pennebaker", "year": 2010}, {"title": "Thumbs up or thumbs down? semantic orientation applied to unsupervised classification of reviews", "author": ["P. Turney"], "venue": "Proceedings of the Association for Computational Linguistics (ACL), 417\u2013424.", "citeRegEx": "Turney,? 2002", "shortCiteRegEx": "Turney", "year": 2002}, {"title": "A syntactic approach for opinion mining on spanish reviews", "author": ["D. Vilares", "M.A. Alonson", "C. G\u00f3mez-Rodr\u0131\u0301guez"], "venue": "Natural Language Engineering", "citeRegEx": "Vilares et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Vilares et al\\.", "year": 2015}, {"title": "Recognizing contextual polarity in phrase-level sentiment analysis", "author": ["T. Wilson", "J. Wiebe", "P. Hoffmann"], "venue": "Proceedings of Empirical Methods for Natural Language Processing (EMNLP), 347\u2013354.", "citeRegEx": "Wilson et al\\.,? 2005", "shortCiteRegEx": "Wilson et al\\.", "year": 2005}], "referenceMentions": [{"referenceID": 23, "context": "Introduction Lexicon-based classification refers to a classification rule in which documents are assigned labels based on the count of words from lexicons associated with each label (Taboada et al. 2011).", "startOffset": 182, "endOffset": 203}, {"referenceID": 16, "context": "Lexicon-based classification is widely used in industry and academia, with applications ranging from sentiment classification and opinion mining (Pang and Lee 2008; Liu 2015) to the psychological and ideological analysis of texts (Laver and Garry 2000; Tausczik and Pennebaker 2010).", "startOffset": 145, "endOffset": 174}, {"referenceID": 10, "context": "Lexicon-based classification is widely used in industry and academia, with applications ranging from sentiment classification and opinion mining (Pang and Lee 2008; Liu 2015) to the psychological and ideological analysis of texts (Laver and Garry 2000; Tausczik and Pennebaker 2010).", "startOffset": 145, "endOffset": 174}, {"referenceID": 9, "context": "Lexicon-based classification is widely used in industry and academia, with applications ranging from sentiment classification and opinion mining (Pang and Lee 2008; Liu 2015) to the psychological and ideological analysis of texts (Laver and Garry 2000; Tausczik and Pennebaker 2010).", "startOffset": 230, "endOffset": 282}, {"referenceID": 24, "context": "Lexicon-based classification is widely used in industry and academia, with applications ranging from sentiment classification and opinion mining (Pang and Lee 2008; Liu 2015) to the psychological and ideological analysis of texts (Laver and Garry 2000; Tausczik and Pennebaker 2010).", "startOffset": 230, "endOffset": 282}, {"referenceID": 21, "context": "The popularity of this approach can be explained by its relative simplicity and ease of use: for domain experts, creating lexicons is intuitive, and, in comparison with labeling instances, it may offer a faster path towards a reasonably accurate classifier (Settles 2011).", "startOffset": 257, "endOffset": 271}, {"referenceID": 10, "context": "Supervised classification systems, which are trained on labeled examples, tend to outperform lexicon-based classifiers, even without accounting for multi-word phenomena (Liu 2015; Pang and Lee 2008).", "startOffset": 169, "endOffset": 198}, {"referenceID": 16, "context": "Supervised classification systems, which are trained on labeled examples, tend to outperform lexicon-based classifiers, even without accounting for multi-word phenomena (Liu 2015; Pang and Lee 2008).", "startOffset": 169, "endOffset": 198}, {"referenceID": 6, "context": "Several researchers have proposed methods for lexicon expansion, automatically growing lexicons from an initial seed set (Hatzivassiloglou and McKeown 1997; Qiu et al. 2011).", "startOffset": 121, "endOffset": 173}, {"referenceID": 19, "context": "Several researchers have proposed methods for lexicon expansion, automatically growing lexicons from an initial seed set (Hatzivassiloglou and McKeown 1997; Qiu et al. 2011).", "startOffset": 121, "endOffset": 173}, {"referenceID": 18, "context": "There is also work on handling multi-word phenomena such as negation (Wilson, Wiebe, and Hoffmann 2005; Polanyi and Zaenen 2006), and discourse (Somasundaran, Wiebe, and Ruppenhofer 2008; Bhatia, Ji, and Eisenstein 2015).", "startOffset": 69, "endOffset": 128}, {"referenceID": 13, "context": "Minka (2012) presents a number of estimators for the concentration parameter \u03c4 from a corpus of text.", "startOffset": 0, "endOffset": 13}, {"referenceID": 4, "context": "(38) This problem can be solved by alternating direction method of multipliers (Boyd et al. 2011).", "startOffset": 79, "endOffset": 97}, {"referenceID": 15, "context": "0), labeled as positive or negative (Pang and Lee 2004).", "startOffset": 36, "endOffset": 55}, {"referenceID": 11, "context": "IMDB 50,000 English-language film reviews (Maas et al. 2011).", "startOffset": 42, "endOffset": 60}, {"referenceID": 10, "context": "The Liu lexicon (Liu 2015) consistently obtained the best performance on all three English-language datasets, so it was made the focus of all subsequent experiments.", "startOffset": 16, "endOffset": 26}, {"referenceID": 14, "context": "For the Spanish data, the ISOL lexicon was used (Molina-Gonz\u00e1lez et al. 2013).", "startOffset": 48, "endOffset": 77}, {"referenceID": 10, "context": "The Liu lexicon (Liu 2015) consistently obtained the best performance on all three English-language datasets, so it was made the focus of all subsequent experiments. Ribeiro et al. (2016) also found that the Liu lexicon is one of the strongest lexicons for review analysis.", "startOffset": 4, "endOffset": 188}, {"referenceID": 25, "context": "Classifiers The evaluation compares the following unsupervised classification strategies: LEXICON basic word counting, as in decision rule (1); LEX-PRESENCE counting word presence rather than frequency, as in decision rule (24); PROBLEX-MULT probabilistic lexicon-based classification, as proposed in this paper, using the multinomial likelihood model; PROBLEX-DCM probabilistic lexicon-based classification, using the Dirichlet Compound Multinomial likelihood to reduce effective counts for repeated words; PMI An alternative approach, discussed in the related work, is to impute document labels from a seed set of words, and then compute \u201csentiment scores\u201d for individual words from pointwise mutual information between the words and imputed labels (Turney 2002).", "startOffset": 751, "endOffset": 764}, {"referenceID": 25, "context": "Classifiers The evaluation compares the following unsupervised classification strategies: LEXICON basic word counting, as in decision rule (1); LEX-PRESENCE counting word presence rather than frequency, as in decision rule (24); PROBLEX-MULT probabilistic lexicon-based classification, as proposed in this paper, using the multinomial likelihood model; PROBLEX-DCM probabilistic lexicon-based classification, using the Dirichlet Compound Multinomial likelihood to reduce effective counts for repeated words; PMI An alternative approach, discussed in the related work, is to impute document labels from a seed set of words, and then compute \u201csentiment scores\u201d for individual words from pointwise mutual information between the words and imputed labels (Turney 2002). The implementation of this method is based on the description from Kiritchenko, Zhu, and Mohammad (2014), using the lexicons as the seed word sets.", "startOffset": 752, "endOffset": 871}, {"referenceID": 0, "context": "The method-of-moments has become an increasingly popular estimator in unsupervised machine learning, with applications in topic models (Anandkumar et al. 2014), sequence models (Hsu, Kakade, and Zhang 2012), and more elaborate linguistic structures (Cohen et al.", "startOffset": 135, "endOffset": 159}, {"referenceID": 5, "context": "2014), sequence models (Hsu, Kakade, and Zhang 2012), and more elaborate linguistic structures (Cohen et al. 2014).", "startOffset": 95, "endOffset": 114}, {"referenceID": 23, "context": "Related work Turney (2002) uses pointwise mutual information to estimate the \u201csemantic orientation\u201d of all vocabulary words from co-occurrence with a small seed set.", "startOffset": 13, "endOffset": 27}, {"referenceID": 4, "context": "We optimize using the alternating direction method of multipliers (ADMM; Boyd et al. 2011).", "startOffset": 66, "endOffset": 90}], "year": 2016, "abstractText": "In lexicon-based classification, documents are assigned labels by comparing the number of words that appear from two opposed lexicons, such as positive and negative sentiment. Creating such words lists is often easier than labeling instances, and they can be debugged by non-experts if classification performance is unsatisfactory. However, there is little analysis or justification of this classification heuristic. This paper describes a set of assumptions that can be used to derive a probabilistic justification for lexicon-based classification, as well as an analysis of its expected accuracy. One key assumption behind lexicon-based classification is that all words in each lexicon are equally predictive. This is rarely true in practice, which is why lexicon-based approaches are usually outperformed by supervised classifiers that learn distinct weights on each word from labeled instances. This paper shows that it is possible to learn such weights without labeled data, by leveraging co-occurrence statistics across the lexicons. This offers the best of both worlds: light supervision in the form of lexicons, and data-driven classification with higher accuracy than traditional word-counting heuristics. Introduction Lexicon-based classification refers to a classification rule in which documents are assigned labels based on the count of words from lexicons associated with each label (Taboada et al. 2011). For example, suppose that we have opposed labels Y \u2208 {0, 1}, and we have associated lexicons W0 and W1. Then for a document with a vector of word counts x, the lexicon-based decision rule is, (1) \u2211", "creator": "dvips(k) 5.991 Copyright 2011 Radical Eye Software"}}}