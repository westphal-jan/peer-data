{"id": "1611.06997", "review": {"conference": "AAAI", "VERSION": "v1", "DATE_OF_SUBMISSION": "21-Nov-2016", "title": "Coherent Dialogue with Attention-Based Language Models", "abstract": "We model coherent conversation continuation via RNN-based dialogue models equipped with a dynamic attention mechanism. Our attention-RNN language model dynamically increases the scope of attention on the history as the conversation continues, as opposed to standard attention (or alignment) models with a fixed input scope in a sequence-to-sequence model. This allows each generated word to be associated with the most relevant words in its corresponding conversation history. We evaluate the model on two popular dialogue datasets, the open-domain MovieTriples dataset and the closed-domain Ubuntu Troubleshoot dataset, and achieve significant improvements over the state-of-the-art and baselines on several metrics, including complementary diversity-based metrics, human evaluation, and qualitative visualizations. We also show that a vanilla RNN with dynamic attention outperforms more complex memory models (e.g., LSTM and GRU) by allowing for flexible, long-distance memory. We promote further coherence via topic modeling-based reranking.", "histories": [["v1", "Mon, 21 Nov 2016 20:25:19 GMT  (61kb,D)", "http://arxiv.org/abs/1611.06997v1", "To appear at AAAI 2017"]], "COMMENTS": "To appear at AAAI 2017", "reviews": [], "SUBJECTS": "cs.CL cs.AI", "authors": ["hongyuan mei", "mohit bansal", "matthew r walter"], "accepted": true, "id": "1611.06997"}, "pdf": {"name": "1611.06997.pdf", "metadata": {"source": "CRF", "title": "Coherent Dialogue with Attention-based Language Models", "authors": ["Hongyuan Mei", "Mohit Bansal", "Matthew R. Walter"], "emails": ["hmei@cs.jhu.edu", "mbansal@cs.unc.edu", "mwalter@ttic.edu"], "sections": [{"heading": "Introduction", "text": "In fact, it is the case that most of them are able to abide by the rules that they have imposed on themselves. (...) Most of them are able to abide by the rules. (...) Most of them are able to abide by the rules. (...) Most of them are able to abide by the rules. (...) Most of them are not able to abide by the rules. (...) Most of them are able to abide by the rules. (...) Most of them are able to abide by the rules. (...) Most of them are not able to abide by themselves. (...) Most of them are able to abide by the rules. (...)"}, {"heading": "Related Work", "text": "The fact is that we are able to manoeuvre ourselves into a situation in which we see ourselves in the situation in which we find ourselves, and in which we find ourselves in a situation in which we find ourselves in, in which we are able to manoeuvre ourselves into a situation in which we are able to put ourselves in a situation in which we are able, in which we are able to put ourselves in a situation in which we are in."}, {"heading": "The Model", "text": "In this section we present a dialog model for attention RNN and compare it with the architectures of basic RNN and sequence-to-sequence models."}, {"heading": "RNN Seq2Seq and Language Models", "text": "The reurent neural networks were used both in sequence-to-sequence models (RNNSeq2Seq = 1) (Sutskever, Vinyals, and Lee 2014) and in language models (RNN-LM, Fig. 1b) (Bengio et al. 2003; Mikolov 2010). We first discuss language models for the dialogue that is the primary focus of our work, then we briefly introduce the sequence-to-sequence model, and finally we discuss the use of attention methods in both models. The RNN-LM models include a set as a sequence of tokens {w0, w1,.., wT} with a sequence-to-sequence function ht = f (ht \u2212 1) and an output (softmax) of functionality P (wt = vj)."}, {"heading": "Attention in RNN-Seq2Seq Models", "text": "There are several ways to integrate the hidden state sequence hS0: M into the RNN decoder. An attention mechanism (fig. 1c) has proved particularly effective for various related tasks in machine translation, caption synthesis and speech comprehension (Mnih et al. 2014; Bahdanau, Cho and Bengio 2015; Xu et al. 2015; Mei, Bansal and Walter 2016a). The attention module enters the hidden state sequence hS0: M of the encoder and the hidden state of the decoder h T l \u2212 1 at each step l \u2212 1 and returns a context vector zl, which is calculated as a weighted average of the hidden states of the encoder hS0: M\u03b2lm = b > tanh (WhTl \u2212 1 + Uh S m) (5a)."}, {"heading": "Attention in RNN-LM", "text": "We develop an Attention RNN Language Model (A-RNNLM), as shown in Figure 1d, and describe how it can be used in the context of dialog modeling, then describe its advantages over the use of attention in sequence models. As with the RNN-LM, the model is first encoded in a sequence of hidden states up to the word t \u2212 1) Given a representation of tokens up to t \u2212 1 {r0, r1,., rt \u2212 1} (which we briefly define), the Attention Module sets the context vector t \u2212 1 as a weighted mean of r0. \u2212 1\u03b2ti > tanh (Wht \u2212 1 + Uri)."}, {"heading": "LDA-based Re-Ranking", "text": "While the trained attention RNN dialogue model creates a natural linguistic continuation of a conversation, while the topic concentration is maintained by symbolic association, some topic supervision at dialogue level can help encourage generations that are more topic-conscious. Such supervision is not universally available, and we use uncontrolled methods to learn latent topics at document level. We use the learned topic model to select the best continuation based on a document collection. We select latent dirichlet allocation (LDA) (Lead, Ng and Jordan 2003; Lead and Lafferty 2009) based on its proven ability to learn distribution over latent topics that result in a collection of documents. This generative model assumes that documents {w0: Tn} Nn = 1 arise from K topics, each of which is defined as distribution over a defined vocabulary of terms."}, {"heading": "Experimental Setup", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "Dataset", "text": "We train and evaluate the models using two large natural language dialogue datasets, MovieTriples (pre-edited by Serban et al. (2016)) and Ubuntu Troubleshoot (pre-edited by Luan, Ji and Ostendorf (2016)). The dialogue within each of these datasets consists of a sequence of utterances (phrases), each of which is a sequence of tokens (words).1 The accompanying supplementary material provides the statistics for these two datasets."}, {"heading": "Evaluation Metrics", "text": "To make a comparison, we closely follow previous work and adopt several standardized (and complementary) evaluation metrics: perplexity (PPL), word error rate (WHO), recall @ N, BLEU, and diversity-based distinct-1. We offer further discussion of the various metrics and their benefits in the supplementary material. On the MovieTriples dataset, we use PPL and WHO, as has happened in previous work; iii) WHO; and iv) WHO @ L (similarly defined). On the Ubuntu dataset, we perform perplexity throughout the dialogue; ii) PL @ L as word-level perplexity during the last utterance of the conversation; iii) WHO @ L (similarly defined). On the PPL dataset, we follow previous work and use @ N. Recall @ N (Manning et al. 2008) a model to evaluate human diversity using the proposed models Li @ N and 2016 (N)."}, {"heading": "Training Details", "text": "For the MovieTriples dataset, we follow the same approach as Serban et al. (2016) and first train on the large Q-A SubTitle dataset (Ameixa et al. 2014), which contains 5.5 M question-and-answer pairs from which we randomly select 20,000 pairs as a hold-out dataset and then match them to the 1Following Luan, Ji and Ostendorf (2016), randomly selecting nine statements as negative examples of the last utterance for each conversation in the Ubuntu troubleshooting for the development set.target MovieTriples dataset. We perform early stop on the hold set according to the PPL score. We train the models for both the MovieTriples and the Ubuntu troubleshoot datasets using Adam (Kingma and Ba 2015) to optimize RNN backpropagation. The accompanying material provides additional training details, including hybrid parameters."}, {"heading": "Results and Analysis", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "Primary Dialogue Modeling Results", "text": "In this section, we compare performance on multiple metrics of our attention-based RNN-LM with RNN baselines and state-of-the-art models on the two benchmark datasets. Table 1 reports on PPL and WHO results on the MovieTriples test set, while Table 2 compares different models on Ubuntu troubleshoot in terms of PPL on the development set and Recall @ N (N = 1 and 2) on the test set (following previous work reports). In the tables, RNN is the plain vanilla RNN language model (RNN-LM) as defined in the model section, and LSTM is an LSTM RNN language model, i.e. an RNN LM with LSTM memory units. ARNN refers to our main model as defined in the NNN-LM proportional model."}, {"heading": "Generation Diversity Results", "text": "Next, we examine the ability of the A-RNN to promote diversity in the generations compared to the vanilla RNN by using the Distinct-1 metric proposed by Li et al. (2016a). Distinct-1 is calculated as the number of different unigrams in the generation scaled by the total number of tokens generated. Table 5 shows that our attention-based RNN language model (A-RNN) produces much more diversity in its generations than the vanilla RNN baseline."}, {"heading": "Topic Coherence Results", "text": "Next, we will examine the ability of the different models to promote topic coherence in the generations in terms of BLEU value. In addition to the RNN and ARNN models, we will consider T-A-RNN, a method that integrates LDA-based topic information into an A-RNN model, according to the approach of Luan, Ji and Ostendorf (2016). We will also evaluate our LDA-based anchor, A-RNN-RR, which is reclassified according to the value S-Sm + (1 \u2212 \u03bb) '(cm | w0: T)', calculating the Log probability '(cm | w0: T) based on a trained A-RNN-M model and validating the weight of the development set."}, {"heading": "Preliminary Human Evaluation", "text": "In addition to several automatic measures, we also report a preliminary human evaluation. For each data set, we manually evaluate the generations of both the A-RNN and the RNN model based on 100 randomly selected examples from the test set. For each example, we randomly mix the two response generations, anonymize the model identity, and ask a human commentator to use the conversation history to decide which answer generation is topically more coherent. As Table 6 shows, the A-RNN model wins significantly more often than the RNN model."}, {"heading": "Qualitative Analysis", "text": "Next, we qualitatively evaluate the effectiveness of our ARNN model by visualizing the attention and results on both datasets. Figure 2 represents attention to a subset of words in the generation for the two datasets. The last line in Fig. 2a and Fig. 2b shows the generated response and we highlight two output words (one left and one right) for two 4Seit Luan, Ji and Ostendorf (2016) no BLEU results or implementations of their models, we cannot compare them with LDA-CONV on BLEU. Instead, we demonstrate the effect of adding the key component of LDA-CONV on the A-RNN. For each highlighted generated word, we visualize the attention weights for words in the conversation history (i.e. words in the preceding phrases and those previously generated in the output file), with darker colors indicating greater attention weights for words in the conversation history (i.e., words previously generated in the output file), with darker shades indicating greater attention weights as the Utility N indicator, as the Rilla model currently indicates."}, {"heading": "Conclusion", "text": "We are investigating how to improve the performance of a recurrent neural network dialogue model through an attention mechanism, and how to promote topic and marker-aware conversation continuity. Our speech RNN model continuously increases the amount of attention during the conversation (which differentiates it from standard fixed-scope attention in sequence-to-sequence models), so that each word generated can be associated with the most related words in the conversation history. We evaluate this simple model using two large dialog datasets (MovieTriples and Ubuntu Troubleshoot) and get the best results to date on multiple dialog metrics (including complementary diversity-based metrics), which perform better than gate-based RNN memory models. We also promote topic concentration by introducing LDA-based ranking, which further improves performance."}, {"heading": "Acknowledgments", "text": "We thank Iulian Serban, Yi Luan and the anonymous reviewers for sharing their records and for their helpful discussion. We thank NVIDIA Corporation for donating GPUs used in this research."}, {"heading": "Appendix", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "LDA-based Re-Ranking Details", "text": "While the trained attention RNN dialogue model creates a natural linguistic continuation of a conversation, while we improve topic concentration through symbolic salinity / association, some dialogue topic supervision can help encourage generations that are more topic-conscious. Such supervision is not universally available, and we use uncontrolled methods to learn latent topics, and adopt the learned topic supervision model to find the best continuation based on topic matching. We choose latent topic dirichlet allocation (LDA) (Lead, Ng and Jordan 2003; Lead and Lafferty 2009) due to its proven ability to learn distribution over latent topics. This generative model assumes that documents arise from multiple topics, each of which is defined as a distribution over a fixed vocabulary of terms. Specifically, we can assume that K topics are associated with a collection of dialogues {w0: Tn} Nn} N1 in the dialogue, each of these terms, and that each one with different one."}, {"heading": "Dataset Details", "text": "We train and evaluate the models on two large natural language dialog datasets, MovieTriples (pre-edited by Serban et al. (2016)) and Ubuntu Troubleshoot (pre-edited by Luan, Ji and Ostendorf (2016)). The dialogue within each of these datasets consists of a sequence of utterances (phrases), each of which is a sequence of tokens (words). Table 7 provides the statistics for these two datasets. [5]"}, {"heading": "Evaluation Metrics", "text": "For the sake of comparison, we closely follow previous work and adopt several standard (and complementary) evaluation metrics (helplessness, word error rate, recall @ N and BLEU, 5Following the approach of Luan, Ji, and Ostendorf (2016), we sampled nine utterances as reasonable examples of the final utterance for each conversation in the Ubuntu Troubleshoot for the development set.diversity-based Distinct-1). On the MovieTriples theme, we use perplexity (PPL) and word error rate (WHO), as it is done in previous work, due to the ability of metrics to adequately quantify the performance of a language model by quantifying its ability to learn the syntactic structure of dialogue. According to Serban et al. (2016), we adopt two versions for each metric: i) PPPL as word-level confusion throughout the dialogue; ii) PPL as explicit-level confusion."}, {"heading": "Training Details", "text": "For the MovieTriples dataset, we follow the same procedure as Serban et al. (2016) and train first on a large Q-A SubTitle dataset (Ameixa et al. 2014) and then on the target MovieTriples dataset. The Q-A SubTitle dataset contains approximately 5.5 million question-answer pairs from which we randomly select 20,000 pairs as a hold-out dataset. We also use a setting of d = 300 on the Ubuntu Troubleshoot dataset. We use Adam (Kingma and Ba 2015) for optimization in RNN hidden states (200, 300, 400, 500) and select d = 300."}, {"heading": "Output Examples", "text": "We evaluate the effectiveness of our A-RNN model qualitatively by visualizing the output on both datasets. The A-RNN generates meaningful and topically coherent responses to the MovieTriples dataset (fig. 6a). In comparison, the vanilla RNN tends to provide generic answers such as \"I don't know.\" On the Ubuntu troubleshoot dataset, the A-RNN either provides promising technical solutions or follows useful questions (fig. 6b)."}], "references": [], "referenceMentions": [], "year": 2016, "abstractText": "We model coherent conversation continuation via RNNbased dialogue models equipped with a dynamic attention mechanism. Our attention-RNN language model dynamically increases the scope of attention on the history as the conversation continues, as opposed to standard attention (or alignment) models with a fixed input scope in a sequence-tosequence model. This allows each generated word to be associated with the most relevant words in its corresponding conversation history. We evaluate the model on two popular dialogue datasets, the open-domain MovieTriples dataset and the closed-domain Ubuntu Troubleshoot dataset, and achieve significant improvements over the state-of-the-art and baselines on several metrics, including complementary diversity-based metrics, human evaluation, and qualitative visualizations. We also show that a vanilla RNN with dynamic attention outperforms more complex memory models (e.g., LSTM and GRU) by allowing for flexible, long-distance memory. We promote further coherence via topic modeling-based reranking.", "creator": "LaTeX with hyperref package"}}}