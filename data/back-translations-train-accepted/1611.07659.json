{"id": "1611.07659", "review": {"conference": "AAAI", "VERSION": "v1", "DATE_OF_SUBMISSION": "23-Nov-2016", "title": "Improving Efficiency of SVM k-Fold Cross-Validation by Alpha Seeding", "abstract": "The k-fold cross-validation is commonly used to evaluate the effectiveness of SVMs with the selected hyper-parameters. It is known that the SVM k-fold cross-validation is expensive, since it requires training k SVMs. However, little work has explored reusing the h-th SVM for training the (h+1)-th SVM for improving the efficiency of k-fold cross-validation. In this paper, we propose three algorithms that reuse the h-th SVM for improving the efficiency of training the (h+1)-th SVM. Our key idea is to efficiently identify the support vectors and to accurately estimate their associated weights (also called alpha values) of the next SVM by using the previous SVM. Our experimental results show that our algorithms are several times faster than the k-fold cross-validation which does not make use of the previously trained SVM. Moreover, our algorithms produce the same results (hence same accuracy) as the k-fold cross-validation which does not make use of the previously trained SVM.", "histories": [["v1", "Wed, 23 Nov 2016 06:48:25 GMT  (49kb,D)", "https://arxiv.org/abs/1611.07659v1", "9 pages, 2 figures, accepted by AAAI-17"], ["v2", "Sat, 4 Feb 2017 17:28:38 GMT  (38kb)", "http://arxiv.org/abs/1611.07659v2", "9 pages, 2 figures, accepted by AAAI-17"]], "COMMENTS": "9 pages, 2 figures, accepted by AAAI-17", "reviews": [], "SUBJECTS": "cs.LG", "authors": ["zeyi wen", "bin li", "kotagiri ramamohanarao", "jian chen", "yawen chen", "rui zhang 0003"], "accepted": true, "id": "1611.07659"}, "pdf": {"name": "1611.07659.pdf", "metadata": {"source": "CRF", "title": "Improving Efficiency of SVM k-fold Cross-validation by Alpha Seeding", "authors": ["Zeyi Wen", "Bin Li", "Kotagiri Ramamohanarao", "Jian Chen", "Yawen Chen", "Rui Zhang"], "emails": ["wenzeyi@gmail.com", "kotagiri}@unimelb.edu.au", "{gitlinux@gmail.com,", "ellachen@scut.edu.cn,", "elfairyhyuk@gmail.com}"], "sections": [{"heading": null, "text": "ar Xiv: 161 1.07 659v 2 [cs.L G] 4F eb2 01SVM. Our key idea is to efficiently identify the support vectors and accurately estimate their associated weights (also called alpha values) of the next SVM by using the previous SVM. Our experimental results show that our algorithms are many times faster than k-fold cross-validation, which does not use the previously trained SVM. Furthermore, our algorithms provide the same results (hence the same accuracy) as k-fold cross-validation, which does not use the previously trained SVM."}, {"heading": "1 Introduction", "text": "This year it has come to the point where it will be able to retaliate, \"he said in an interview with the\" Welt am Sonntag. \""}, {"heading": "2 Preliminaries", "text": "In this case, we are able to find a hyperplane that separates the positive and negative training instances in X with the maximum margin, and now with the minimal error of error on the training instances. To allow a tangible mapping of the training instances to other data spaces, we can express the hyperplane in a dual form (Bennett and Bredensteiner 2000), such as the following quadratic programming problem (Nocedal and Wright 2006)."}, {"heading": "Adjusting Alpha Towards Optimum (ATO)", "text": "ATO aims to gradually reduce the alpha values to their optimal values. It uses the technology developed by Karasuyama and Takeuchi (Karasuyama and Takeuchi 2009) for online SVM training. In the online SVM training, a subsetR of the outdated training instances is removed from the training set, i.e. a subsetR of the outdated training instances is removed in order to obtain the next SVM values. In the ATO algorithm, we first construct a new training dataset X, where X = S = X\\ R. Then we gradually increase the alpha values of the instances in T (i.e. we increase the number of instances to obtain the next SVM values) to (near) their optimal values."}, {"heading": "4 Experimental studies", "text": "We evaluate our proposed algorithms empirically on the basis of five data sets from the LibSVM website (Chang and Lin 2011), all of which were implemented in C + +. Experiments were carried out on a desktop computer running Linux with a 6-core E5-2620 CPU and 128GB of RAM. According to the usual settings, we used the Gaussian kernel function and default is k to 10. Hyperparameters for each data set are identical to the existing studies (Catanzaro, Sundaram and Keutzer 2008; Smirnov, Sprinkhuizen-Kuyper and Nalbantov 2004; Wu and Li 2006). Table 2 provides more details on the data sets. We examine k-fold cross-validation under the setting of the binary classification. Next, we will first show the overall efficiency of our proposed algorithms compared to LibSVM."}, {"heading": "Overall efficiency on different datasets", "text": "The total elapsed time consists of the alpha initialization time and the time for the remainder of the 10-fold cross-validation. The result is shown in Table 1. To fit the table into the page, we do not specify the total elapsed time of ATO, MIR and SIR for each row. But the total elapsed time can easily be calculated by adding the time for the alpha initialization and the time for the rest. Note that the time for \"the rest\" (e.g. the fourth column of Table 1) includes the time for partitioning records into 10 subsets, training (the most significant part) and classification. As we can see from the table, the total elapsed time of MIR and SIR is much smaller than the values spent by LibSVR. In the Madelon dataset, MIR and SIR are about two times and four times faster than LibSVM."}, {"heading": "Effect of varying k", "text": "We varied k from 3 to 100 to investigate the effect of the value of k. Since carrying out this experiment set is very time-consuming, especially at k = 100, we compare SIR (the best of our three algorithms according to the results in Table 1) only with LibSVM. Table 3 shows the results. Since LibSVM was very slow at k = 100 in the MNIST dataset, we only performed the first 30 rounds to estimate the total time. As we can see from the table, SIR consistently outperforms LibSVM. At k = 100, SIR is about 32 times faster than LibSVM in the Madelon dataset. The experiment result for leave-one-out (i.e. k corresponds to the dataset size) cross-validation is similar to k = 100 and is available in Figure 2 in the Supplementary Material."}, {"heading": "5 Related work", "text": "We categorize the related studies into two groups: alpha seeding and online SVM training. The related work on alpha seeding by DeCoste and Wagstaff (DeCoste and Wagstaff 2000) initially introduced the reuse of alpha values in SVM leave-one-out cross-validation. Their method (i.e. the AVG discussed in Supplementary Material) comprises two main steps: (i) the formation of an SVM with the entire data set; (ii) the gradual removal of an instance from the SVM and the even distribution of the associated alpha value across all support vectors. Lee et al. (Lee et al. 2004) proposed a technique (i.e. the TOP discussed in Supplementary Material) to improve the above method. Instead of achieving a uniform distribution of the alpha value across all support vectors, the method distributes the alpha value among all support vectors, the method distributes the vea value among the core instances with the largest support vectors."}, {"heading": "6 conclusion", "text": "In order to improve the efficiency of k-fold cross validation, we have proposed three algorithms that reuse the previously trained SVM to initialize the next SVM, so that the training process for the next SVM reaches the optimal state faster. We have conducted extensive experiments to validate the effectiveness and efficiency of our proposed algorithms. Our experimental results have shown that the best algorithm among the three is SIR. At k = 10, SIR is many times faster than k-fold cross validation in LibSVM, where previously trained SVMs are not used; at k = 100, SIR dramatically outperforms LibSVM (32 times faster than LibSVM in the Madelon dataset). Furthermore, our algorithms provide the same results (hence the same accuracy) as k-fold cross validation in LibSVM doctoral theses."}, {"heading": "Supplementary Material", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "Pseudo-code of our three algorithm", "text": "Here we present the pseudo-code of our three algorithms proposed in the paper. ATO algorithm The complete algorithm of ATS is summarized in algorithm 1. As we can see from algorithm 1, ATO terminates when R is empty, and it could spend a considerable amount of time in the loop, especially if the step size is small. Algorithm 1: Alpha towards optimum (ATO) Input: Sets X and R of instances, \u03b1 associated with instances in X, and a number of new instances. Output: Optimal alpha values for X\\ R and T. 1: Alpha values for T and T."}, {"heading": "2 foreach r \u2208 IR do", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "3 maxV alue \u2190 0, t\u2032 \u2190 \u22121", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "4 foreach t \u2208 IT do", "text": "5 yr = yt \u0445 K (xr, xt) > maxV alue then 6 maxV alue \u2190 K (xr, xt), t \"\u2190 t7 if t\" 6 = \u2212 1 then / * replace xr by xt. \"< p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p. \"p\" p \"p.\" p \"p\" p \"p.\" p. \"p\" p \"p\" p \"p.\" p. \"p.\" p. \"p.\" p. \"p.\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p.\" p. \"p\" p. \"p\" p. \"p\" p \"p\" p \"p.\" p \"p\" p. \"p\" p \"p\" p \"p.\" p \"p\" p. \"p\" p \"p.\" p \"p.\" p \"p\" p \"p.\" p \"p.\" p \"p\" p. \"p\" p \"p\" p \"p.\" p. \"p\" p. \"p\" p. \"p\" p \"p.\" p \"p\" p. \"p\" p. \"p\" p \"p.\" p. \"p\" p \"p.\" p \"p.\" p \"p\" p \"p\" p \"p.\" p \"p.\" p. \"p\" p. \"p\" p \"p.\" p \"p.\" p. \"p\" p. \"p.\" p. \"p.\" p \"p\" p \"p\" p \"p.\" p. \"p.\" p. \"p\" p. \"p.\" p \"p.\" p. \"p\" p. \"p.\" p \"p."}, {"heading": "Efficiency comparison on leave-one-out cross-validation", "text": "Here we examine the efficiency of our proposed algorithms compared to LibSVM and the existing alpha seeding techniques, i.e. AVG and TOP (see section 12), for leave-one-out cross-validation. Similar to the other algorithms, we implemented AVG and TOP in C + +. Since leave-one-out cross-validation is very expensive for the large data sets, we estimated the total time for leave-one-out cross-validation for the three large data sets (i.e. Adult, MNIST and Webdata) for each algorithm. For MNIST and Webdata, we rank the first 30 rounds of leave-one-out cross-validation to estimate the total time for each algorithm; for Adult, we ran the first 100 rounds of leave-one-out cross-validation to estimate the total time for each algorithm, with Heart and Madelon ranking the total leave-one cross-out cross-validation as five times faster than SVR, i.e. the SVR total validation results are slightly faster than SVR."}], "references": [{"title": "GPU acceleration for support vector machines", "author": ["Athanasopoulos"], "venue": "In International Workshop on Image Analysis for Multimedia Interactive Services", "citeRegEx": "Athanasopoulos,? \\Q2011\\E", "shortCiteRegEx": "Athanasopoulos", "year": 2011}, {"title": "A theory of learning with similarity functions. Machine Learning 72(1-2):89\u2013112", "author": ["Blum Balcan", "M.-F. Srebro 2008] Balcan", "A. Blum", "N. Srebro"], "venue": null, "citeRegEx": "Balcan et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Balcan et al\\.", "year": 2008}, {"title": "Duality and geometry in svm classifiers", "author": ["Bennett", "E.J. Bredensteiner"], "venue": "In ICML,", "citeRegEx": "Bennett et al\\.,? \\Q2000\\E", "shortCiteRegEx": "Bennett et al\\.", "year": 2000}, {"title": "Fast support vector machine training and classification on graphics processors", "author": ["Sundaram Catanzaro", "B. Keutzer 2008] Catanzaro", "N. Sundaram", "K. Keutzer"], "venue": null, "citeRegEx": "Catanzaro et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Catanzaro et al\\.", "year": 2008}, {"title": "Incremental and decremental support vector machine learning. Advances in neural information processing", "author": ["Cauwenberghs", "G. Poggio 2001] Cauwenberghs", "T. Poggio"], "venue": null, "citeRegEx": "Cauwenberghs et al\\.,? \\Q2001\\E", "shortCiteRegEx": "Cauwenberghs et al\\.", "year": 2001}, {"title": "LIBSVM: a library for support vector machines", "author": ["Chang", "Lin 2011] Chang", "C.-C", "Lin", "C.-J"], "venue": null, "citeRegEx": "Chang et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Chang et al\\.", "year": 2011}, {"title": "Warm start for parameter selection of linear classifiers", "author": ["Chu"], "venue": "In Proceedings of the 21th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining,", "citeRegEx": "Chu,? \\Q2015\\E", "shortCiteRegEx": "Chu", "year": 2015}, {"title": "Alpha seeding for support vector machines", "author": ["DeCoste", "D. Wagstaff 2000] DeCoste", "K. Wagstaff"], "venue": "In SIGKDD,", "citeRegEx": "DeCoste et al\\.,? \\Q2000\\E", "shortCiteRegEx": "DeCoste et al\\.", "year": 2000}, {"title": "Decomposition methods for", "author": ["Kao"], "venue": null, "citeRegEx": "Kao,? \\Q2004\\E", "shortCiteRegEx": "Kao", "year": 2004}, {"title": "Multiple incremental decremental learning of support vector machines", "author": ["Karasuyama", "M. Takeuchi 2009] Karasuyama", "I. Takeuchi"], "venue": "In Advances in neural information processing systems,", "citeRegEx": "Karasuyama et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Karasuyama et al\\.", "year": 2009}, {"title": "Improvements to platt\u2019s smo algorithm for svm classifier design", "author": ["Keerthi"], "venue": "Neural Computation", "citeRegEx": "Keerthi,? \\Q2001\\E", "shortCiteRegEx": "Keerthi", "year": 2001}, {"title": "Solving least squares problems, volume 161", "author": ["Lawson", "C.L. Hanson 1974] Lawson", "R.J. Hanson"], "venue": null, "citeRegEx": "Lawson et al\\.,? \\Q1974\\E", "shortCiteRegEx": "Lawson et al\\.", "year": 1974}, {"title": "An efficient method for computing leave-one-out error in support vector machines with gaussian kernels", "author": ["Lee"], "venue": "Neural Networks, IEEE Transactions on 15(3):750\u2013757", "citeRegEx": "Lee,? \\Q2004\\E", "shortCiteRegEx": "Lee", "year": 2004}, {"title": "An improved training algorithm for support vector machines", "author": ["Freund Osuna", "E. Girosi 1997] Osuna", "R. Freund", "F. Girosi"], "venue": "In IEEE Workshop on Neural Networks for Signal Processing,", "citeRegEx": "Osuna et al\\.,? \\Q1997\\E", "shortCiteRegEx": "Osuna et al\\.", "year": 1997}, {"title": "Sequential minimal optimization: A fast algorithm for training support vector machines", "author": ["Platt", "J others 1998] Platt"], "venue": null, "citeRegEx": "Platt and Platt,? \\Q1998\\E", "shortCiteRegEx": "Platt and Platt", "year": 1998}, {"title": "Unanimous voting using support vector machines", "author": ["Sprinkhuizen-Kuyper Smirnov", "E. Nalbantov 2004] Smirnov", "I. Sprinkhuizen-Kuyper", "G. Nalbantov"], "venue": "In Belgium-Netherlands Conference on Artificial Intelligence,", "citeRegEx": "Smirnov et al\\.,? \\Q2004\\E", "shortCiteRegEx": "Smirnov et al\\.", "year": 2004}, {"title": "Mascot: fast and highly scalable SVM cross-validation using GPUs and SSDs", "author": ["Wen"], "venue": "In International Conference in Data Mining,", "citeRegEx": "Wen,? \\Q2014\\E", "shortCiteRegEx": "Wen", "year": 2014}, {"title": "Feature selection for classification using transductive support vector machines", "author": ["Wu", "Z. Li 2006] Wu", "C. Li"], "venue": null, "citeRegEx": "Wu et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Wu et al\\.", "year": 2006}], "referenceMentions": [], "year": 2016, "abstractText": "The k-fold cross-validation is commonly used to evaluate the effectiveness of SVMs with the selected hyperparameters. It is known that the SVM k-fold crossvalidation is expensive, since it requires training k SVMs. However, little work has explored reusing the h SVM for training the (h+1) SVM for improving the efficiency of k-fold cross-validation. In this paper, we propose three algorithms that reuse the h SVM for improving the efficiency of training the (h + 1) SVM. Our key idea is to efficiently identify the support vectors and to accurately estimate their associated weights (also called alpha values) of the next SVM by using the previous SVM. Our experimental results show that our algorithms are several times faster than the k-fold cross-validation which does not make use of the previously trained SVM. Moreover, our algorithms produce the same results (hence same accuracy) as the k-fold cross-validation which does not make use of the previously trained SVM.", "creator": "gnuplot 4.6 patchlevel 4"}}}