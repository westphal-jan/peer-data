{"id": "1611.07743", "review": {"conference": "AAAI", "VERSION": "v1", "DATE_OF_SUBMISSION": "23-Nov-2016", "title": "Tunable Sensitivity to Large Errors in Neural Network Training", "abstract": "When humans learn a new concept, they might ignore examples that they cannot make sense of at first, and only later focus on such examples, when they are more useful for learning. We propose incorporating this idea of tunable sensitivity for hard examples in neural network learning, using a new generalization of the cross-entropy gradient step, which can be used in place of the gradient in any gradient-based training method. The generalized gradient is parameterized by a value that controls the sensitivity of the training process to harder training examples. We tested our method on several benchmark datasets. We propose, and corroborate in our experiments, that the optimal level of sensitivity to hard example is positively correlated with the depth of the network. Moreover, the test prediction error obtained by our method is generally lower than that of the vanilla cross-entropy gradient learner. We therefore conclude that tunable sensitivity can be helpful for neural network learning.", "histories": [["v1", "Wed, 23 Nov 2016 11:14:01 GMT  (63kb,D)", "http://arxiv.org/abs/1611.07743v1", "The paper is accepted to the AAAI 2017 conference"]], "COMMENTS": "The paper is accepted to the AAAI 2017 conference", "reviews": [], "SUBJECTS": "stat.ML cs.LG cs.NE", "authors": ["gil keren", "sivan sabato", "bj\u00f6rn w schuller"], "accepted": true, "id": "1611.07743"}, "pdf": {"name": "1611.07743.pdf", "metadata": {"source": "META", "title": "Tunable Sensitivity to Large Errors in Neural Network Training", "authors": ["Gil Keren", "Sivan Sabato", "Bj\u00f6rn Schuller"], "emails": ["gil.keren@uni-passau.de"], "sections": [{"heading": "1 Introduction", "text": "This year, we will be able to go in search of a solution that will enable us, that will enable us to find a solution that will enable us to find a solution, that will enable us to find a solution, that will enable us to find a solution, that will enable us to find a solution, that will enable us to find a solution, that will enable us to find a solution, that will enable us to find a solution, that will enable us to find a solution, that will enable us to find a solution that we are able to find a solution, to find a solution. \""}, {"heading": "1.1 Related Work", "text": "The challenge of selecting the best optimization target for neural network formation is not new. In the past, quadratic loss was typically used with gradient-based learning in neural networks (Rumelhart, Hinton, and Williams 1988), but a number of studies have shown, both theoretically and empirically, that quadratic loss exhibits preferential properties over quadratic loss, such as better learning speed (Levin and Fleisher 1988), better performance (Golik, Doetsch, and Ney 2013), and a more appropriate form of error surface (Glorot and Bengio 2010). Other cost functions have also been considered, for example a novel cost function has been proposed (Silva et al. 2006), but it is not clearly beneficial for cross-entropy. The authors of (Bahdanau et al. 2015) address this question in a different sequence prediction environment."}, {"heading": "2 Setting and Notation", "text": "For each integer n, [n] = {1,.., n}. For a vector v, its i'te coordinate is called v (i). We consider a standardized, multi-layer neural network (Svozil, Kvasnicka, and Pospichal 1997), where the output layer is a softmax layer (Bridle 1990), with n units, each of which represents a class. Let us specify the parameters of the neural network, and let zj (x) specify the value of the output unit j if the network has parameters before the Softmax function is applied. Apply the Softmax function, the probability that the network assigns to class j ispj (x): = e zj / n \u2211 i = 1 ezi. The label predicted by the network, for example, is x (x; bay) = argmaxj [n] pj (x)."}, {"heading": "3 Generalizing the gradient", "text": "Our proposed method allows the sensitivity of the training procedure to be controlled for examples where the network has large errors in prediction, by generalizing the course. An alternative to the same goal would be to use an exponential version of the course of entropy: \"It is an undesirable term, since it is not monotonous in k for a fixed Py, resulting in a lack of relevance for small or large values of k.\" The gradient resulting from the above form is only for the cancellation of the terms l and softmax. Another option would be to consider l = \u2212 log (pky), but this is only a scaled version of the course of the cross-entropy class."}, {"heading": "5.1 A Toy Example", "text": "To further motivate our choice of f; 0; 0; 0; 0; 0; 0; 0; 0; 0; 0; 0; 0; 0; 0; 0; 0; 0; 0; 0; 0; 0; 0; 0; 0; 0; 0; 0; 0; 0; 0; 0; 0; 0; 0; 0; 0; 0; 0; 0; 0; 0; 0; 0; 0; 0; 0; 0; 0; 0; 0; 0; 0; 0; 0; 0; 0; 0; 0; 0; 0; 0; 0; 0; 0; 0; 0; 0; 0; 0; 0; 0; 0; 0; 0; 0; 0; 0; 0; 0; 0; 0; 0; 0; 0; 0; 0; 0; 0; 0; 0; 0; 0; 0; 0; 0; 0; 0; 0; 0; 0; 0; 0; 0; 0; 0; 0; 0; 0; 0; 0; 0; 0; 0; 0; 0; 0; 0; 0; 0; 0; 0; 0; 0; 0; 0; 0; 0; 0; 0; 0; 0; 0; 0; 0; 0; 0; 0; 0; 0; 0; 0; 0; 0; 0; 0; 0; 0; 0; 0; 0; 0; 0; 0; 0; 0; 0; 0; 0; 0; 0; 0; 0; 0; 0; 0; 0; 0; 0; 0; 0; 0; 0; 0; 0; 0; 0; 0; 0; 0; 0; 0; 0; 0; 0; 0; 0; 0; 0; 0; 0; 0; 0; 0; 0; 0; 0; 0; 0; 0; 0; 0; 0; 0; 0; 0; 0; 0; 0; 0; 0; 0; 0; 0; 0; 0; 0; 0; 0; 0; 0; 0; 0; 0; 0; 0; 0; 0; 0; 0; 0; 0; 0; 0; 0; 0; 0; 0; 0; 0; 0; 0; 0; 0; 0; 0; 0; 0; 0; 0; 0; 0"}, {"heading": "5.2 Non-existence of a Cost Function for f", "text": "It is natural to ask if our choice of f in Equation (4), g (\u03b8) is the gradient of another cost function instead of the cross entropy, and the following problem shows that this is not the case. Lemma 1. Let us assume f, as in Equation (4) with k 6 = 1, and g (\u043a) the resulting pseudo-gradient. There is a neural network for which g (\u0443) is not a gradient of any cost function. Proof this is left to the supplementary material. Note that the above problem does not exclude the possibility that a gradient-based algorithm using g instead of the gradient will somehow optimize a cost function."}, {"heading": "6 Experiments", "text": "For our experiments, we used four classification benchmark datasets in the field of computer vision: the MNIST datasets (LeCun et al.), the Street View house numbers (SVHN) (Netzer et al.), and the CIFAR 100 datasets (Krizhevsky and Hinton). A more detailed description of the datasets can be found in Appendix C.1 in the supplementary materials; the neural networks we are experimenting with contain one, three, or five hidden layers of different layer sizes."}, {"heading": "7 Conclusions", "text": "Inspired by an intuition in human cognition, we proposed in this paper a generalization of the gradient of cross-entropy, in which a tunable parameter controls the sensitivity of the training process to hard examples. Our experiments show that, as we expected, the optimal degree of sensitivity to hard examples is positively correlated with the depth of the network. Furthermore, the experiments show that selecting the value of the sensitivity parameter by cross-validation leads to an overall improved predictive error performance on a variety of benchmark datasets. The proposed approach is not limited to upstream neural networks - it can be used in any gradient-based training algorithm and for any network architecture. In future work, we plan to examine this method as a tool for improving training in other architectures, such as in revolutionary networks and reactive neural networks, as well as experiments with different levels of sensitivity to different levels of the precursor."}, {"heading": "Acknowledgments", "text": "This work was supported by the Seventh Framework Programme of the European Communities through ERC Starting Grant No. 338164 (iHEARu). Sivan Sabato was partially supported by the Israel Science Foundation (grant No. 555 / 15)."}, {"heading": "A Proof for Toy Example", "text": "Consider the neural network from the example of the toy in Section 5.1. In this network, there is a classification threshold, so examples above or below it are classified into different classes. We prove that for sufficiently large training \u2212 \u2212 \u2212 \u2212 \u2212 the value of the cross-sectional load is not minimal if the threshold is at 0.Suppose there is a mapping of network parameters (w0 \u2212 w1) that minimizes the cross-sectional load that triggers a threshold at 0. \u2212 The output of the cross-sectional layer is clearly determined by e z0ez1, or equivalent by z0 \u2212 z1 = x (w0 \u2212 w1) + b0 \u2212 b1. Therefore, without loss of generality, we can assume that w1 = b1 = 0. Denote w: = 0, b: = b0. If w = 0 is in the minimization task, then all examples are classified as members of the same class, and in particular the classification threshold is not zero."}, {"heading": "B Proof of Lemma 1", "text": "Yyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyy yyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyy"}, {"heading": "C Additional Experiment details and Results", "text": "In fact, the number of freshmen studying in the US has increased by more than 50% compared to their predecessors in the US."}], "references": [{"title": "Task loss estimation for sequence prediction", "author": ["D. Bahdanau", "D. Serdyuk", "P. Brakel", "N.R. Ke", "J. Chorowski", "A. Courville", "Y. Bengio"], "venue": "arXiv preprint arXiv:1511.06456.", "citeRegEx": "Bahdanau et al\\.,? 2015", "shortCiteRegEx": "Bahdanau et al\\.", "year": 2015}, {"title": "Curriculum learning", "author": ["Y. Bengio", "J. Louradour", "R. Collobert", "J. Weston"], "venue": "Proc. of the 26th annual International Conference on Machine Learning (ICML), 41\u2013", "citeRegEx": "Bengio et al\\.,? 2009", "shortCiteRegEx": "Bengio et al\\.", "year": 2009}, {"title": "Probabilistic interpretation of feedforward classification network outputs, with relationships to statistical pattern recognition", "author": ["J.S. Bridle"], "venue": "Neurocomputing. Springer. 227\u2013236.", "citeRegEx": "Bridle,? 1990", "shortCiteRegEx": "Bridle", "year": 1990}, {"title": "Describing multimedia content using attention-based encoder-decoder networks", "author": ["K. Cho", "A. Courville", "Y. Bengio"], "venue": "IEEE Transactions on Multimedia 17(11):1875\u2013 1886.", "citeRegEx": "Cho et al\\.,? 2015", "shortCiteRegEx": "Cho et al\\.", "year": 2015}, {"title": "Understanding the difficulty of training deep feedforward neural networks", "author": ["X. Glorot", "Y. Bengio"], "venue": "Proc. of International Conference on Artificial Intelligence and Statistics, 249\u2013256.", "citeRegEx": "Glorot and Bengio,? 2010", "shortCiteRegEx": "Glorot and Bengio", "year": 2010}, {"title": "Cross-entropy vs", "author": ["P. Golik", "P. Doetsch", "H. Ney"], "venue": "squared error training: a theoretical and experimental comparison. In Proc. of INTERSPEECH, 1756\u20131760.", "citeRegEx": "Golik et al\\.,? 2013", "shortCiteRegEx": "Golik et al\\.", "year": 2013}, {"title": "Deep neural networks for acoustic modeling in speech recognition: The shared views of four research groups", "author": ["G. Hinton", "L. Deng", "D. Yu", "G.E. Dahl", "A.-r. Mohamed", "N. Jaitly", "A. Senior", "V. Vanhoucke", "P. Nguyen", "T. N Sainath"], "venue": "Signal Processing Magazine,", "citeRegEx": "Hinton et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Hinton et al\\.", "year": 2012}, {"title": "Connectionist learning procedures", "author": ["G.E. Hinton"], "venue": "Artificial intelligence 40(1):185\u2013234.", "citeRegEx": "Hinton,? 1989", "shortCiteRegEx": "Hinton", "year": 1989}, {"title": "Data cleaning for classification using misclassification analysis", "author": ["P. Jeatrakul", "K.W. Wong", "C.C. Fung"], "venue": "Journal of Advanced Computational Intelligence and Intelligent Informatics 14(3):297\u2013302.", "citeRegEx": "Jeatrakul et al\\.,? 2010", "shortCiteRegEx": "Jeatrakul et al\\.", "year": 2010}, {"title": "Convolutional RNN: an enhanced model for extracting features from sequential data", "author": ["G. Keren", "B. Schuller"], "venue": "Proc. of 2016 International Joint Conference on Neural Networks (IJCNN), 3412\u20133419.", "citeRegEx": "Keren and Schuller,? 2016", "shortCiteRegEx": "Keren and Schuller", "year": 2016}, {"title": "Convolutional neural networks with data augmentation for classifying speakers native language", "author": ["G. Keren", "J. Deng", "J. Pohjalainen", "B. Schuller"], "venue": "Proc. of INTERSPEECH, 2393\u20132397.", "citeRegEx": "Keren et al\\.,? 2016", "shortCiteRegEx": "Keren et al\\.", "year": 2016}, {"title": "Adam: A method for stochastic optimization", "author": ["D. Kingma", "J. Ba"], "venue": "International Conference on Learning Representations (ICLR).", "citeRegEx": "Kingma and Ba,? 2015", "shortCiteRegEx": "Kingma and Ba", "year": 2015}, {"title": "Learning multiple layers of features from tiny images", "author": ["A. Krizhevsky", "G. Hinton"], "venue": null, "citeRegEx": "Krizhevsky and Hinton,? \\Q2009\\E", "shortCiteRegEx": "Krizhevsky and Hinton", "year": 2009}, {"title": "Imagenet classification with deep convolutional neural networks", "author": ["A. Krizhevsky", "I. Sutskever", "G.E. Hinton"], "venue": "Proc. of Advances in Neural Information Processing Systems (NIPS), 1097\u20131105.", "citeRegEx": "Krizhevsky et al\\.,? 2012", "shortCiteRegEx": "Krizhevsky et al\\.", "year": 2012}, {"title": "Self-paced learning for latent variable models", "author": ["M.P. Kumar", "B. Packer", "D. Koller"], "venue": "Proc. of Advances in Neural Information Processing Systems (NIPS), 1189\u20131197.", "citeRegEx": "Kumar et al\\.,? 2010", "shortCiteRegEx": "Kumar et al\\.", "year": 2010}, {"title": "Building machines that learn and think like people", "author": ["B.M. Lake", "T.D. Ullman", "J.B. Tenenbaum", "S.J. Gershman"], "venue": "arXiv preprint arXiv:1604.00289.", "citeRegEx": "Lake et al\\.,? 2016", "shortCiteRegEx": "Lake et al\\.", "year": 2016}, {"title": "Gradient-based learning applied to document recognition", "author": ["Y. LeCun", "L. Bottou", "Y. Bengio", "P. Haffner"], "venue": "Proceedings of the IEEE 86(11):2278\u20132324.", "citeRegEx": "LeCun et al\\.,? 1998", "shortCiteRegEx": "LeCun et al\\.", "year": 1998}, {"title": "Accelerated learning in layered neural networks", "author": ["E. Levin", "M. Fleisher"], "venue": "Complex systems 2:625\u2013640.", "citeRegEx": "Levin and Fleisher,? 1988", "shortCiteRegEx": "Levin and Fleisher", "year": 1988}, {"title": "Reading digits in natural images with unsupervised feature learning", "author": ["Y. Netzer", "T. Wang", "A. Coates", "A. Bissacco", "B. Wu", "A.Y. Ng"], "venue": "NIPS workshop on deep learning and unsupervised feature learning. Granada, Spain.", "citeRegEx": "Netzer et al\\.,? 2011", "shortCiteRegEx": "Netzer et al\\.", "year": 2011}, {"title": "On the difficulty of training recurrent neural networks", "author": ["R. Pascanu", "T. Mikolov", "Y. Bengio"], "venue": "Proceedings of the 30th International Conference on Machine Learning (ICML), 1310\u20131318.", "citeRegEx": "Pascanu et al\\.,? 2013", "shortCiteRegEx": "Pascanu et al\\.", "year": 2013}, {"title": "Some methods of speeding up the convergence of iteration methods", "author": ["B.T. Polyak"], "venue": "USSR Computational Mathematics and Mathematical Physics 4(5):1\u201317.", "citeRegEx": "Polyak,? 1964", "shortCiteRegEx": "Polyak", "year": 1964}, {"title": "Learning representations by back-propagating errors", "author": ["D.E. Rumelhart", "G.E. Hinton", "R.J. Williams"], "venue": "Cognitive modeling 5:3.", "citeRegEx": "Rumelhart et al\\.,? 1988", "shortCiteRegEx": "Rumelhart et al\\.", "year": 1988}, {"title": "New developments of the Z-EDM algorithm", "author": ["L.M. Silva", "J.M. De Sa", "L Alexandre"], "venue": "In Intelligent Systems Design and Applications,", "citeRegEx": "Silva et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Silva et al\\.", "year": 2006}, {"title": "Improving classification accuracy by identifying and removing instances that should be misclassified", "author": ["M.R. Smith", "T. Martinez"], "venue": "The 2011 International Joint Conference on Neural Networks (IJCNN), 2690\u20132697.", "citeRegEx": "Smith and Martinez,? 2011", "shortCiteRegEx": "Smith and Martinez", "year": 2011}, {"title": "On the importance of initialization and momentum in deep learning", "author": ["I. Sutskever", "J. Martens", "G. Dahl", "G. Hinton"], "venue": "Proc. of the 30th International Conference on Machine Learning (ICML), 1139\u20131147.", "citeRegEx": "Sutskever et al\\.,? 2013", "shortCiteRegEx": "Sutskever et al\\.", "year": 2013}, {"title": "Sequence to sequence learning with neural networks", "author": ["I. Sutskever", "O. Vinyals", "Q.V. Le"], "venue": "Advances in neural information processing systems, 3104\u20133112.", "citeRegEx": "Sutskever et al\\.,? 2014", "shortCiteRegEx": "Sutskever et al\\.", "year": 2014}, {"title": "Introduction to multi-layer feed-forward neural networks", "author": ["D. Svozil", "V. Kvasnicka", "J. Pospichal"], "venue": "Chemometrics and intelligent laboratory systems 39(1):43\u201362.", "citeRegEx": "Svozil et al\\.,? 1997", "shortCiteRegEx": "Svozil et al\\.", "year": 1997}, {"title": "Going deeper with convolutions", "author": ["C. Szegedy", "W. Liu", "Y. Jia", "P. Sermanet", "S. Reed", "D. Anguelov", "D. Erhan", "V. Vanhoucke", "A. Rabinovich"], "venue": "The IEEE Conference on Computer Vision and Pattern Recognition (CVPR).", "citeRegEx": "Szegedy et al\\.,? 2015", "shortCiteRegEx": "Szegedy et al\\.", "year": 2015}, {"title": "Lecture 6.5-rmsprop: Divide the gradient by a running average of its recent magnitude. COURSERA: Neural Networks for Machine Learning", "author": ["T. Tieleman", "G. Hinton"], "venue": null, "citeRegEx": "Tieleman and Hinton,? \\Q2012\\E", "shortCiteRegEx": "Tieleman and Hinton", "year": 2012}, {"title": "Learning to execute", "author": ["W. Zaremba", "I. Sutskever"], "venue": "arXiv preprint arXiv:1410.4615.", "citeRegEx": "Zaremba and Sutskever,? 2014", "shortCiteRegEx": "Zaremba and Sutskever", "year": 2014}], "referenceMentions": [{"referenceID": 27, "context": "In recent years, neural networks have become empirically successful in a wide range of supervised learning applications, such as computer vision (Krizhevsky, Sutskever, and Hinton 2012; Szegedy et al. 2015), speech recognition (Hinton et al.", "startOffset": 145, "endOffset": 206}, {"referenceID": 6, "context": "2015), speech recognition (Hinton et al. 2012), natural language processing (Sutskever, Vinyals, and Le 2014) and computational paralinguistics (Keren and Schuller 2016; Keren et al.", "startOffset": 26, "endOffset": 46}, {"referenceID": 9, "context": "2012), natural language processing (Sutskever, Vinyals, and Le 2014) and computational paralinguistics (Keren and Schuller 2016; Keren et al. 2016).", "startOffset": 103, "endOffset": 147}, {"referenceID": 10, "context": "2012), natural language processing (Sutskever, Vinyals, and Le 2014) and computational paralinguistics (Keren and Schuller 2016; Keren et al. 2016).", "startOffset": 103, "endOffset": 147}, {"referenceID": 7, "context": "Standard implementations of training feed-forward neural networks for classification are based on gradient-based stochastic optimization, usually optimizing the empirical cross-entropy loss (Hinton 1989).", "startOffset": 190, "endOffset": 203}, {"referenceID": 1, "context": "Intuitions about human cognition have often inspired successful machine learning approaches (Bengio et al. 2009; Cho, Courville, and Bengio 2015; Lake et al. 2016).", "startOffset": 92, "endOffset": 163}, {"referenceID": 15, "context": "Intuitions about human cognition have often inspired successful machine learning approaches (Bengio et al. 2009; Cho, Courville, and Bengio 2015; Lake et al. 2016).", "startOffset": 92, "endOffset": 163}, {"referenceID": 20, "context": "Many such training methods have been proposed, including, to name a few, Momentum (Polyak 1964), RMSProp (Tieleman and Hinton 2012), and Adam (Kingma and Ba 2015).", "startOffset": 82, "endOffset": 95}, {"referenceID": 28, "context": "Many such training methods have been proposed, including, to name a few, Momentum (Polyak 1964), RMSProp (Tieleman and Hinton 2012), and Adam (Kingma and Ba 2015).", "startOffset": 105, "endOffset": 131}, {"referenceID": 11, "context": "Many such training methods have been proposed, including, to name a few, Momentum (Polyak 1964), RMSProp (Tieleman and Hinton 2012), and Adam (Kingma and Ba 2015).", "startOffset": 142, "endOffset": 162}, {"referenceID": 17, "context": "In the past, the quadratic loss was typically used with gradient-based learning in neural networks (Rumelhart, Hinton, and Williams 1988), but a line of studies demonstrated both theoretically and empirically that the cross-entropy loss has preferable properties over the quadratic-loss, such as better learning speed (Levin and Fleisher 1988), better performance (Golik, Doetsch, and Ney 2013) and a more suitable shape of the error surface (Glorot and Bengio 2010).", "startOffset": 318, "endOffset": 343}, {"referenceID": 4, "context": "In the past, the quadratic loss was typically used with gradient-based learning in neural networks (Rumelhart, Hinton, and Williams 1988), but a line of studies demonstrated both theoretically and empirically that the cross-entropy loss has preferable properties over the quadratic-loss, such as better learning speed (Levin and Fleisher 1988), better performance (Golik, Doetsch, and Ney 2013) and a more suitable shape of the error surface (Glorot and Bengio 2010).", "startOffset": 442, "endOffset": 466}, {"referenceID": 22, "context": "For instance, a novel cost function was proposed in (Silva et al. 2006), but it is not clearly advantageous to cross-entropy.", "startOffset": 52, "endOffset": 71}, {"referenceID": 0, "context": "The authors of (Bahdanau et al. 2015) address this question in a different setting of sequence prediction.", "startOffset": 15, "endOffset": 37}, {"referenceID": 23, "context": "In one work (Smith and Martinez 2011), data is preprocessed to detect label noise induced from overlapping classes, and in another work (Jeatrakul, Wong, and Fung 2010) the authors use an auxiliary neural network to detect noisy examples.", "startOffset": 12, "endOffset": 37}, {"referenceID": 1, "context": "The interplay between \u201ceasy\u201d and \u201chard\u201d examples during neural network training has been addressed in the framework of Curriculum Learning (Bengio et al. 2009).", "startOffset": 139, "endOffset": 159}, {"referenceID": 29, "context": "In a more recent work (Zaremba and Sutskever 2014), the authors indeed find that a curriculum in which harder examples are presented in early phases outperforms a curriculum that at first uses only easy examples.", "startOffset": 22, "endOffset": 50}, {"referenceID": 2, "context": "We consider a standard feed-forward multilayer neural network (Svozil, Kvasnicka, and Pospichal 1997), where the output layer is a softmax layer (Bridle 1990), with n units, each representing a class.", "startOffset": 145, "endOffset": 158}, {"referenceID": 16, "context": "For our experiments, we used four classification benchmark datasets from the field of computer vision: The MNIST dataset (LeCun et al. 1998), the Street View House Numbers dataset (SVHN) (Netzer et al.", "startOffset": 121, "endOffset": 140}, {"referenceID": 18, "context": "1998), the Street View House Numbers dataset (SVHN) (Netzer et al. 2011) and the CIFAR-10 and CIFAR-100 datasets (Krizhevsky and Hinton 2009).", "startOffset": 52, "endOffset": 72}, {"referenceID": 12, "context": "2011) and the CIFAR-10 and CIFAR-100 datasets (Krizhevsky and Hinton 2009).", "startOffset": 46, "endOffset": 74}, {"referenceID": 24, "context": "For optimization, we used stochastic gradient descent with momentum (Sutskever et al. 2013) with several values of momentum and a minibatch size of 128 examples.", "startOffset": 68, "endOffset": 91}, {"referenceID": 4, "context": "In the hidden layers, biases were initialized to 0 and for the weights we used the initialization scheme from (Glorot and Bengio 2010).", "startOffset": 110, "endOffset": 134}], "year": 2016, "abstractText": "When humans learn a new concept, they might ignore examples that they cannot make sense of at first, and only later focus on such examples, when they are more useful for learning. We propose incorporating this idea of tunable sensitivity for hard examples in neural network learning, using a new generalization of the cross-entropy gradient step, which can be used in place of the gradient in any gradient-based training method. The generalized gradient is parameterized by a value that controls the sensitivity of the training process to harder training examples. We tested our method on several benchmark datasets. We propose, and corroborate in our experiments, that the optimal level of sensitivity to hard example is positively correlated with the depth of the network. Moreover, the test prediction error obtained by our method is generally lower than that of the vanilla cross-entropy gradient learner. We therefore conclude that tunable sensitivity can be helpful for neural network learning.", "creator": "TeX"}}}