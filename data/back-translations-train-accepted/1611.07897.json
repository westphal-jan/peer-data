{"id": "1611.07897", "review": {"conference": "EMNLP", "VERSION": "v1", "DATE_OF_SUBMISSION": "23-Nov-2016", "title": "Learning Generic Sentence Representations Using Convolutional Neural Networks", "abstract": "We propose a new encoder-decoder approach to learn distributed sentence representations from unlabeled sentences. The word-to-vector representation is used, and convolutional neural networks are employed as sentence encoders, mapping an input sentence into a fixed-length vector. This representation is decoded using long short-term memory recurrent neural networks, considering several tasks, such as reconstructing the input sentence, or predicting the future sentence. We further describe a hierarchical encoder-decoder model to encode a sentence to predict multiple future sentences. By training our models on a large collection of novels, we obtain a highly generic convolutional sentence encoder that performs well in practice. Experimental results on several benchmark datasets, and across a broad range of applications, demonstrate the superiority of the proposed model over competing methods.", "histories": [["v1", "Wed, 23 Nov 2016 17:32:23 GMT  (213kb,D)", "http://arxiv.org/abs/1611.07897v1", null], ["v2", "Wed, 26 Jul 2017 20:48:52 GMT  (275kb,D)", "http://arxiv.org/abs/1611.07897v2", "Accepted by EMNLP 2017"]], "reviews": [], "SUBJECTS": "cs.CL cs.LG", "authors": ["zhe gan", "yunchen pu", "ricardo henao", "chunyuan li", "xiaodong he", "lawrence carin"], "accepted": true, "id": "1611.07897"}, "pdf": {"name": "1611.07897.pdf", "metadata": {"source": "CRF", "title": "Unsupervised Learning of Sentence Representations using Convolutional Neural Networks", "authors": ["Zhe Gan", "Yunchen Pu", "Ricardo Henao", "Chunyuan Li", "Xiaodong He", "Lawrence Carin"], "emails": ["lcarin}@duke.edu", "xiaohe@microsoft.com"], "sections": [{"heading": "1 Introduction", "text": "However, the goal of a model for such a task is to learn a fixed-length sentence that encodes the semantic and syntactic properties of sentences. Most of these models are designed in a way that searches for recurrent neural sentences at the top of the neural network. [1] However, large-format datasets are often difficult to acquire that motivate the need for non-superordinate learning methods in sentences. Furthermore, sentences contain a large amount of linguistic regularity, which makes them particularly suitable for creating superordinate learning models."}, {"heading": "2 Model description", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "2.1 CNN-LSTM model", "text": "Assuming we have a pair of sentences (sx, sy) that depicts the entire sentence (sx), then the first sentence of the second sentence (sx) that depicts the second sentence (sx), abbreviated (sx), abbreviated (sx), abbreviated (sx), abbreviated (sx), abbreviated (sx), abbreviated (sx), abbreviated (sx), abbreviated (sx), abbreviated (sx), abbreviated (sx), abbreviated (sx), abbreviated (sx), abbreviated (sx), \"abbreviated (sx),\" abbreviated (sx), \"(c),\" (c), \"(c),\" abbreviated (c), \"(c),\" (c), \"abbreviated (x),\" (c), \"(x),\" (x), \"(x),\" (c), \"(x),\" (c), \"(abbreviated (c),\" (x), \"(c),\" (c), \"(abbreviated (sx),\" (c), \"(c),\" (abbreviated (sx), \"(c),\" abbreviated (sx), \"(c),\" (c), \"(abbreviated (sx),\" (sx), \"(c),\" abbreviated (sx), \"(c),\" abbreviated (c), \"abbreviated (sx),\" (c), \"abbreviated (c),\" abbreviated (sx), \"(c),\" abbreviated (c), \"abbreviated (sx),\" (c), \"(c), abbreviated (c),\" abbreviated (sx), \"(c),\" abbreviated (c), \"abbreviated (sx),\" (c), \"(c"}, {"heading": "2.2 Hierarchical CNN-LSTM model", "text": "The future scenario described in Section 2.1 looks at the sentence immediately following it as context. < p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p >"}, {"heading": "3 Experiments", "text": "We first provide a qualitative analysis of our CNN encoder and then present experimental results on 8 tasks: 5 classification benchmarks, paraphrase recognition, semantic relativity, and image-sentence ranking. As in [7], we evaluate the capabilities of our encoder as a generic feature extractor. To further demonstrate the benefit of unattended training, we also align our trained sentence encoder with the 5 classification benchmarks. All CNN LSTM models are trained with the BookCorpus [23] dataset, which consists of 70 million records from over 7000 books.We train a total of four models: (i) an auto encoder, (ii) a future predictor, (iii) the composite model, and (iv) the hierarchical model1."}, {"heading": "3.1 Qualitative analysis", "text": "First, we show that the sentence representation learned through our model has a linear structure that allows analog reasoning to be performed using simple vector arithmetic, as in Table 1. It shows that the arithmetic operations on the sentence representations correspond to addition and subtraction at the word level. For example, in the third example, our encoder detects that the difference between sentence B and C is \"you\" and \"he,\" so that the first word in sentence A is replaced by the second (i.e., \"you\" + \"him =\" him \"), leading to sentence D. To further demonstrate the different properties of the CNNLSTM encoder and the LSTM encoder, we train a CNNLSTM encoder and an LSTMLSTM encoder (LSTM encoder and LSTM decoder), and empirically compare their sentence recovery results."}, {"heading": "3.2 Quantitative evaluations", "text": "We will first examine the task of record classification on 5 data sets: MR [26], CR [27], SUBJ [28], MPQA [29] and TREC [30]. A detailed description of the data sets will be provided in the supplementary material collection, whether we simply form a logistic regression model on top of the extracted record characteristics. We will limit ourselves to comparing them to unattended methods that do not require label information for a fair comparison. Results are summarized in Table 3: As can be seen, our CNN encoder provides better results than the Combine Skip model of the [7] on 4 of 5 data sets. We highlight some observations that are better than the future predictor, suggesting that the intra-record information is more important for classification than the interdisciplinary information."}, {"heading": "4 Conclusion", "text": "We presented a new class of CNN-LSTM encoder decoder models for learning sentence representations from unlabeled text. Our trained folding encoder is highly generic and can be an alternative to the skip vectors of [7]. Convincing experimental results on several tasks showed the advantages of our approach. In future work, we aim to use more advanced CNN architectures [10] for sentence encoding. In addition, our proposed models can be used for other applications in natural language. For example, the basic CNN-LSTM model for machine translation can be used [19], while the CNN-LSTM hierarchical model can be expanded to learn document embedding [20]."}, {"heading": "A Additional results", "text": "A.1 Vocabulary Expansion In the first method of vocabulary expansion, a linear mapping is learned between the word2vec embedding space Vw2v and the CNN word embedding space Vcnn by solving a linear regression problem. In Table 6, we show examples of closest adjacent words based on the word embedding we learned after applying the first vocabulary extension method. A.2 Detailed accuracies of the accuracies shown in Figure 2 (left) and Figure 2 (right) can be found in Table 7 and Table 8. A.3 Sentence AvailableTable 9 shows the next neighbors of sentences from a CNN-LSTM autoencoder trained on the BookCorpus dataset. As you can see, our encoder learns to precisely capture the semantic and syntax of sentences."}, {"heading": "B Experimental details", "text": "(i) TREC: This task involves classifying a question into 6 types [30]. (ii) MR: Film ratings with one sentence per review. (iv) CR: Customer ratings for different products. (iii) SUBJ: Subjectivity data where the task is to classify a sentence as subjective or objective [28]. (iv) CR: Customer ratings for different products. (iii) It involves predicting positive / negative ratings [27]. (v) MPQA: Opinion polarity detection Subtask of the MPQA data set [29].B.2 Image sentence ranking x. We use the same method as in [7] for image sentence ranking."}], "references": [{"title": "Long short-term memory", "author": ["S. Hochreiter", "J. Schmidhuber"], "venue": "Neural computation", "citeRegEx": "1", "shortCiteRegEx": null, "year": 1997}, {"title": "A convolutional neural network for modelling sentences", "author": ["N. Kalchbrenner", "E. Grefenstette", "P. Blunsom"], "venue": "ACL", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2014}, {"title": "Convolutional neural networks for sentence classification", "author": ["Y. Kim"], "venue": "EMNLP", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2014}, {"title": "Recursive deep models for semantic compositionality over a sentiment treebank", "author": ["R. Socher", "A. Perelygin", "J. Wu", "J. Chuang", "C. Manning", "A. Ng", "C. Potts"], "venue": "EMNLP", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2013}, {"title": "Distributed representations of sentences and documents", "author": ["Q. Le", "T. Mikolov"], "venue": "ICML", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2014}, {"title": "Distributed representations of words and phrases and their compositionality", "author": ["T. Mikolov", "I. Sutskever", "K. Chen", "G. Corrado", "J. Dean"], "venue": "NIPS", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2013}, {"title": "Skip-thought vectors", "author": ["R. Kiros", "Y. Zhu", "R. Salakhutdinov", "R. Zemel", "R. Urtasun", "A. Torralba", "S. Fidler"], "venue": "NIPS", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2015}, {"title": "Semi-supervised sequence learning", "author": ["A. Dai", "Q. Le"], "venue": "NIPS", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2015}, {"title": "Learning distributed representations of sentences from unlabelled data", "author": ["F. Hill", "K. Cho", "A. Korhonen"], "venue": "NAACL", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2016}, {"title": "Convolutional neural network architectures for matching natural language sentences", "author": ["B. Hu", "Z. Lu", "H. Li", "Q. Chen"], "venue": "NIPS", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2014}, {"title": "Sequence to sequence learning with neural networks", "author": ["I. Sutskever", "O. Vinyals", "Q. Le"], "venue": "NIPS", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2014}, {"title": "Learning phrase representations using rnn encoder-decoder for statistical machine translation", "author": ["K. Cho", "B. Van Merri\u00ebnboer", "C. Gulcehre", "D. Bahdanau", "F. Bougares", "H. Schwenk", "Y. Bengio"], "venue": "EMNLP", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2014}, {"title": "Recurrent neural network based language model", "author": ["T. Mikolov", "M. Karafi\u00e1t", "L. Burget", "J. Cernock\u1ef3", "S. Khudanpur"], "venue": "INTERSPEECH", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2010}, {"title": "Natural language processing (almost) from scratch", "author": ["R. Collobert", "J. Weston", "L. Bottou", "M. Karlen", "K. Kavukcuoglu", "P. Kuksa"], "venue": "JMLR", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2011}, {"title": "Effective use of word order for text categorization with convolutional neural networks", "author": ["R. Johnson", "T. Zhang"], "venue": "NAACL HLT", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2015}, {"title": "Unsupervised learning of video representations using lstms", "author": ["N. Srivastava", "E. Mansimov", "R. Salakhutdinov"], "venue": "ICML", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2015}, {"title": "Deep visual-semantic alignments for generating image descriptions", "author": ["A. Karpathy", "L. Fei-Fei"], "venue": "CVPR", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2015}, {"title": "Recurrent continuous translation models", "author": ["N. Kalchbrenner", "P. Blunsom"], "venue": "EMNLP", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2013}, {"title": "Encoding source language with convolutional neural network for machine translation", "author": ["F. Meng", "Z. Lu", "M. Wang", "H. Li", "W. Jiang", "Q. Liu"], "venue": "ACL", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2015}, {"title": "A hierarchical neural autoencoder for paragraphs and documents", "author": ["J. Li", "M. Luong", "D. Jurafsky"], "venue": "ACL", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2015}, {"title": "and Ji", "author": ["A. Sordoni", "Y. Bengio", "H. Vahabi", "C. Lioma", "J. Grue Simonsen"], "venue": "Nie. A hierarchical recurrent encoder-decoder for generative context-aware query suggestion. In CIKM", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2015}, {"title": "Hierarchical recurrent neural network for document modeling", "author": ["R. Lin", "S. Liu", "M. Yang", "M. Li", "M. Zhou", "S. Li"], "venue": "EMNLP", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2015}, {"title": "Aligning books and movies: Towards story-like visual explanations by watching movies and reading books", "author": ["Y. Zhu", "R. Kiros", "R. Zemel", "R. Salakhutdinov", "R. Urtasun", "A. Torralba", "S. Fidler"], "venue": "ICCV", "citeRegEx": "23", "shortCiteRegEx": null, "year": 2015}, {"title": "Adam: A method for stochastic optimization", "author": ["D. Kingma", "J. Ba"], "venue": "ICLR", "citeRegEx": "24", "shortCiteRegEx": null, "year": 2015}, {"title": "Theano: new features and speed improvements", "author": ["F. Bastien", "P. Lamblin", "R. Pascanu", "J. Bergstra", "I. Goodfellow", "A. Bergeron", "N. Bouchard", "D. Warde-Farley", "Y. Bengio"], "venue": "arXiv:1211.5590", "citeRegEx": "25", "shortCiteRegEx": null, "year": 2012}, {"title": "Seeing stars: Exploiting class relationships for sentiment categorization with respect to rating scales", "author": ["B. Pang", "L. Lee"], "venue": "ACL", "citeRegEx": "26", "shortCiteRegEx": null, "year": 2005}, {"title": "Mining and summarizing customer reviews", "author": ["M. Hu", "B. Liu"], "venue": "SIGKDD", "citeRegEx": "27", "shortCiteRegEx": null, "year": 2004}, {"title": "A sentimental education: Sentiment analysis using subjectivity summarization based on minimum cuts", "author": ["B. Pang", "L. Lee"], "venue": "ACL", "citeRegEx": "28", "shortCiteRegEx": null, "year": 2004}, {"title": "Annotating expressions of opinions and emotions in language", "author": ["J. Wiebe", "T. Wilson", "C. Cardie"], "venue": "Language resources and evaluation", "citeRegEx": "29", "shortCiteRegEx": null, "year": 2005}, {"title": "Learning question classifiers", "author": ["X. Li", "D. Roth"], "venue": "ACL", "citeRegEx": "30", "shortCiteRegEx": null, "year": 2002}, {"title": "Unsupervised construction of large paraphrase corpora: Exploiting massively parallel news sources", "author": ["B. Dolan", "C. Quirk", "C. Brockett"], "venue": "COLING", "citeRegEx": "31", "shortCiteRegEx": null, "year": 2004}, {"title": "Improved semantic representations from tree-structured long short-term memory networks", "author": ["K. Tai", "R. Socher", "C. Manning"], "venue": "ACL", "citeRegEx": "32", "shortCiteRegEx": null, "year": 2015}, {"title": "Microsoft coco: Common objects in context", "author": ["T. Lin", "M. Maire", "S. Belongie", "J. Hays", "P. Perona", "D. Ramanan", "P. Doll\u00e1r", "C.L. Zitnick"], "venue": "ECCV", "citeRegEx": "33", "shortCiteRegEx": null, "year": 2014}, {"title": "Very deep convolutional networks for large-scale image recognition", "author": ["K. Simonyan", "A. Zisserman"], "venue": "ICLR", "citeRegEx": "34", "shortCiteRegEx": null, "year": 2015}], "referenceMentions": [{"referenceID": 0, "context": "Deep learning techniques have shown promising performance on sentence modeling, via recurrent neural networks (RNNs) [1], convolutional neural networks (CNNs) [2, 3], and recursive neural networks [4].", "startOffset": 117, "endOffset": 120}, {"referenceID": 1, "context": "Deep learning techniques have shown promising performance on sentence modeling, via recurrent neural networks (RNNs) [1], convolutional neural networks (CNNs) [2, 3], and recursive neural networks [4].", "startOffset": 159, "endOffset": 165}, {"referenceID": 2, "context": "Deep learning techniques have shown promising performance on sentence modeling, via recurrent neural networks (RNNs) [1], convolutional neural networks (CNNs) [2, 3], and recursive neural networks [4].", "startOffset": 159, "endOffset": 165}, {"referenceID": 3, "context": "Deep learning techniques have shown promising performance on sentence modeling, via recurrent neural networks (RNNs) [1], convolutional neural networks (CNNs) [2, 3], and recursive neural networks [4].", "startOffset": 197, "endOffset": 200}, {"referenceID": 4, "context": "The paragraph-vector model of [5] incorporates a global context vector into the log-linear neural language model [6] to learn the sentence representations; however, at prediction time, one needs to perform gradient descent to compute a new vector.", "startOffset": 30, "endOffset": 33}, {"referenceID": 5, "context": "The paragraph-vector model of [5] incorporates a global context vector into the log-linear neural language model [6] to learn the sentence representations; however, at prediction time, one needs to perform gradient descent to compute a new vector.", "startOffset": 113, "endOffset": 116}, {"referenceID": 6, "context": "The skip-thought model of [7] describes an encoder-decoder model to reconstruct the surrounding sentences of an input sentence, where both the encoder and decoder are modeled as RNNs.", "startOffset": 26, "endOffset": 29}, {"referenceID": 7, "context": "The sequence autoencoder of [8] is a simple variant of [7], in which the decoder is used to reconstruct the input sentence itself.", "startOffset": 28, "endOffset": 31}, {"referenceID": 6, "context": "The sequence autoencoder of [8] is a simple variant of [7], in which the decoder is used to reconstruct the input sentence itself.", "startOffset": 55, "endOffset": 58}, {"referenceID": 8, "context": "Most recently, [9] proposed a sentence-level log-linear bag-of-words (BoW) model, where a BoW representation of an input sentence is used to predict adjacent sentences that are also represented as BoW.", "startOffset": 15, "endOffset": 18}, {"referenceID": 1, "context": "CNNs have recently achieved excellent results in various supervised natural language applications [2, 3, 10].", "startOffset": 98, "endOffset": 108}, {"referenceID": 2, "context": "CNNs have recently achieved excellent results in various supervised natural language applications [2, 3, 10].", "startOffset": 98, "endOffset": 108}, {"referenceID": 9, "context": "CNNs have recently achieved excellent results in various supervised natural language applications [2, 3, 10].", "startOffset": 98, "endOffset": 108}, {"referenceID": 6, "context": "Related to but distinct from the skip-thought model of [7], we propose to use a CNN encoder for unsupervised learning of sentence representations within the framework of encoder-decoder models ar X iv :1 61 1.", "startOffset": 55, "endOffset": 58}, {"referenceID": 10, "context": "proposed by [11, 12].", "startOffset": 12, "endOffset": 20}, {"referenceID": 11, "context": "proposed by [11, 12].", "startOffset": 12, "endOffset": 20}, {"referenceID": 12, "context": "This model abstracts the RNN language model of [13] to the sentence level.", "startOffset": 47, "endOffset": 51}, {"referenceID": 6, "context": "Compared with the LSTM encoders used in [7, 8, 9], a CNN encoder may have the following advantages.", "startOffset": 40, "endOffset": 49}, {"referenceID": 7, "context": "Compared with the LSTM encoders used in [7, 8, 9], a CNN encoder may have the following advantages.", "startOffset": 40, "endOffset": 49}, {"referenceID": 8, "context": "Compared with the LSTM encoders used in [7, 8, 9], a CNN encoder may have the following advantages.", "startOffset": 40, "endOffset": 49}, {"referenceID": 6, "context": "For example, excluding the number of parameters used in the word embeddings, our trained sentence encoder has 3 million parameters, while the skip-thought vector of [7] contains 40 million parameters.", "startOffset": 165, "endOffset": 168}, {"referenceID": 6, "context": "As in [7], we first train our proposed models on a large collection of novels, and then evaluate the CNN sentence encoder as a generic feature extractor for 8 tasks: semantic relatedness, paraphrase detection, image-sentence ranking and 5 standard classification benchmarks.", "startOffset": 6, "endOffset": 9}, {"referenceID": 6, "context": "We show that our trained sentence encoder yields generic representations that perform as well as, or better than those of [7, 8, 9], in all the tasks considered.", "startOffset": 122, "endOffset": 131}, {"referenceID": 7, "context": "We show that our trained sentence encoder yields generic representations that perform as well as, or better than those of [7, 8, 9], in all the tasks considered.", "startOffset": 122, "endOffset": 131}, {"referenceID": 8, "context": "We show that our trained sentence encoder yields generic representations that perform as well as, or better than those of [7, 8, 9], in all the tasks considered.", "startOffset": 122, "endOffset": 131}, {"referenceID": 2, "context": "CNN encoder The CNN architecture in [3, 14] is used for sentence encoding, which consists of a convolution layer and a max-pooling operation over the entire sentence for each feature map.", "startOffset": 36, "endOffset": 43}, {"referenceID": 13, "context": "CNN encoder The CNN architecture in [3, 14] is used for sentence encoding, which consists of a convolution layer and a max-pooling operation over the entire sentence for each feature map.", "startOffset": 36, "endOffset": 43}, {"referenceID": 13, "context": "According to [14], we can induce one feature map c = f(X \u2217Wc + b) \u2208 RT\u2212h+1, where f(\u00b7) is a nonlinear activation function such as the hyperbolic tangent used in our experiments, b \u2208 RT\u2212h+1 is a bias vector, and \u2217 denotes the convolutional operator.", "startOffset": 13, "endOffset": 17}, {"referenceID": 13, "context": "We then apply a max-over-time pooling operation [14] to the feature map and take its maximum value, i.", "startOffset": 48, "endOffset": 52}, {"referenceID": 1, "context": "There exist other CNN architectures in the literature [2, 10, 15].", "startOffset": 54, "endOffset": 65}, {"referenceID": 9, "context": "There exist other CNN architectures in the literature [2, 10, 15].", "startOffset": 54, "endOffset": 65}, {"referenceID": 14, "context": "There exist other CNN architectures in the literature [2, 10, 15].", "startOffset": 54, "endOffset": 65}, {"referenceID": 2, "context": "We adopt the CNN model in [3, 14] due to its simplicity and excellent performance on classification.", "startOffset": 26, "endOffset": 33}, {"referenceID": 13, "context": "We adopt the CNN model in [3, 14] due to its simplicity and excellent performance on classification.", "startOffset": 26, "endOffset": 33}, {"referenceID": 0, "context": "The transition functionH(\u00b7) is implemented with an LSTM [1].", "startOffset": 56, "endOffset": 59}, {"referenceID": 15, "context": "Applications Inspired by [16], we propose three models: (i) an autoencoder, (ii) a future predictor, and (iii) the composite model.", "startOffset": 25, "endOffset": 29}, {"referenceID": 6, "context": "The future predictor (ii) achieves this, effectively capturing the inter-sentence information, which has been shown to be useful to learn the semantics of a sentence [7].", "startOffset": 166, "endOffset": 169}, {"referenceID": 5, "context": "Suppose we have a large pretrained word embedding matrix, such as the publicly available word2vec vectors that have dimensionality 300 and were trained using a continuous bag-of-words architecture [6].", "startOffset": 197, "endOffset": 200}, {"referenceID": 6, "context": "The first method is the same as described in [7].", "startOffset": 45, "endOffset": 48}, {"referenceID": 10, "context": "Related work Our model falls into the category of encoder-decoder models, which have been used for machine translation [11, 12], video analysis [16], image captioning [17], unsupervised sentence embedding [7, 8, 9], among many others.", "startOffset": 119, "endOffset": 127}, {"referenceID": 11, "context": "Related work Our model falls into the category of encoder-decoder models, which have been used for machine translation [11, 12], video analysis [16], image captioning [17], unsupervised sentence embedding [7, 8, 9], among many others.", "startOffset": 119, "endOffset": 127}, {"referenceID": 15, "context": "Related work Our model falls into the category of encoder-decoder models, which have been used for machine translation [11, 12], video analysis [16], image captioning [17], unsupervised sentence embedding [7, 8, 9], among many others.", "startOffset": 144, "endOffset": 148}, {"referenceID": 16, "context": "Related work Our model falls into the category of encoder-decoder models, which have been used for machine translation [11, 12], video analysis [16], image captioning [17], unsupervised sentence embedding [7, 8, 9], among many others.", "startOffset": 167, "endOffset": 171}, {"referenceID": 6, "context": "Related work Our model falls into the category of encoder-decoder models, which have been used for machine translation [11, 12], video analysis [16], image captioning [17], unsupervised sentence embedding [7, 8, 9], among many others.", "startOffset": 205, "endOffset": 214}, {"referenceID": 7, "context": "Related work Our model falls into the category of encoder-decoder models, which have been used for machine translation [11, 12], video analysis [16], image captioning [17], unsupervised sentence embedding [7, 8, 9], among many others.", "startOffset": 205, "endOffset": 214}, {"referenceID": 8, "context": "Related work Our model falls into the category of encoder-decoder models, which have been used for machine translation [11, 12], video analysis [16], image captioning [17], unsupervised sentence embedding [7, 8, 9], among many others.", "startOffset": 205, "endOffset": 214}, {"referenceID": 16, "context": "The combination of CNN and LSTM has been considered in image captioning [17], and in some recent work on machine translation [18, 19].", "startOffset": 72, "endOffset": 76}, {"referenceID": 17, "context": "The combination of CNN and LSTM has been considered in image captioning [17], and in some recent work on machine translation [18, 19].", "startOffset": 125, "endOffset": 133}, {"referenceID": 18, "context": "The combination of CNN and LSTM has been considered in image captioning [17], and in some recent work on machine translation [18, 19].", "startOffset": 125, "endOffset": 133}, {"referenceID": 12, "context": "Inspired by the standard RNN-based language model [13] that uses the current word to predict future words, we propose a hierarchical encoder-decoder model that encodes the current sentence to predict multiple future sentences.", "startOffset": 50, "endOffset": 54}, {"referenceID": 19, "context": "Similar work can be found in [20, 21, 22].", "startOffset": 29, "endOffset": 41}, {"referenceID": 20, "context": "Similar work can be found in [20, 21, 22].", "startOffset": 29, "endOffset": 41}, {"referenceID": 21, "context": "Similar work can be found in [20, 21, 22].", "startOffset": 29, "endOffset": 41}, {"referenceID": 19, "context": "However, [20, 21] uses a LSTM for the sentence encoder, while [22] uses a bag-of-words to represent sentences.", "startOffset": 9, "endOffset": 17}, {"referenceID": 20, "context": "However, [20, 21] uses a LSTM for the sentence encoder, while [22] uses a bag-of-words to represent sentences.", "startOffset": 9, "endOffset": 17}, {"referenceID": 21, "context": "However, [20, 21] uses a LSTM for the sentence encoder, while [22] uses a bag-of-words to represent sentences.", "startOffset": 62, "endOffset": 66}, {"referenceID": 6, "context": "As in [7], we evaluate the capabilities of our encoder as a generic feature extractor.", "startOffset": 6, "endOffset": 9}, {"referenceID": 22, "context": "All the CNN-LSTM models are trained using the BookCorpus dataset [23], which consists of 70 million sentences from over 7000 books.", "startOffset": 65, "endOffset": 69}, {"referenceID": 5, "context": "We initialize the word embeddings using word2vec vectors [6] and consider the two vocabulary expansion methods described in Section 2.", "startOffset": 57, "endOffset": 60}, {"referenceID": 10, "context": "Gradients are clipped if the norm of the parameter vector exceeds 5 [11].", "startOffset": 68, "endOffset": 72}, {"referenceID": 23, "context": "The Adam algorithm [24] with learning rate 2\u00d710\u22124 is utilized for optimization.", "startOffset": 19, "endOffset": 23}, {"referenceID": 24, "context": "All experiments are implemented in Theano [25], using a NVIDIA GeForce GTX TITAN X GPU with 12GB memory.", "startOffset": 42, "endOffset": 46}, {"referenceID": 25, "context": "Classification benchmarks We first study the task of sentence classification on 5 datasets: MR [26], CR [27], SUBJ [28], MPQA [29] and TREC [30].", "startOffset": 95, "endOffset": 99}, {"referenceID": 26, "context": "Classification benchmarks We first study the task of sentence classification on 5 datasets: MR [26], CR [27], SUBJ [28], MPQA [29] and TREC [30].", "startOffset": 104, "endOffset": 108}, {"referenceID": 27, "context": "Classification benchmarks We first study the task of sentence classification on 5 datasets: MR [26], CR [27], SUBJ [28], MPQA [29] and TREC [30].", "startOffset": 115, "endOffset": 119}, {"referenceID": 28, "context": "Classification benchmarks We first study the task of sentence classification on 5 datasets: MR [26], CR [27], SUBJ [28], MPQA [29] and TREC [30].", "startOffset": 126, "endOffset": 130}, {"referenceID": 29, "context": "Classification benchmarks We first study the task of sentence classification on 5 datasets: MR [26], CR [27], SUBJ [28], MPQA [29] and TREC [30].", "startOffset": 140, "endOffset": 144}, {"referenceID": 6, "context": "As can be seen, our CNN encoder provides better results than the combine-skip model of [7] on 4 out of 5 datasets.", "startOffset": 87, "endOffset": 90}, {"referenceID": 6, "context": "We also tried the skip-thought construction [7] of our CNN-LSTM model, but empirically we did not observe better performance than the composite model, hence no results are reported.", "startOffset": 44, "endOffset": 47}, {"referenceID": 8, "context": "ParagraphVec DM [9] 61.", "startOffset": 16, "endOffset": 19}, {"referenceID": 8, "context": "9 SDAE [9] 67.", "startOffset": 7, "endOffset": 10}, {"referenceID": 8, "context": "[9] 74.", "startOffset": 0, "endOffset": 3}, {"referenceID": 8, "context": "7 FastSent [9] 70.", "startOffset": 11, "endOffset": 14}, {"referenceID": 6, "context": "uni-skip [7] 75.", "startOffset": 9, "endOffset": 12}, {"referenceID": 6, "context": "9 bi-skip [7] 73.", "startOffset": 10, "endOffset": 13}, {"referenceID": 6, "context": "2 combine-skip [7] 76.", "startOffset": 15, "endOffset": 18}, {"referenceID": 30, "context": "Paraphrase detection Now we consider paraphrase detection on the MSRP dataset [31].", "startOffset": 78, "endOffset": 82}, {"referenceID": 31, "context": "As in [32], given two sentence representations zx", "startOffset": 6, "endOffset": 10}, {"referenceID": 6, "context": "uni-skip [7] 30.", "startOffset": 9, "endOffset": 12}, {"referenceID": 6, "context": "7 4 bi-skip [7] 32.", "startOffset": 12, "endOffset": 15}, {"referenceID": 6, "context": "2 4 combine-skip [7] 33.", "startOffset": 17, "endOffset": 20}, {"referenceID": 8, "context": "Our best result is better than all the reported results, except the sequential denoising autoencoder (SDAE) used in [9].", "startOffset": 116, "endOffset": 119}, {"referenceID": 32, "context": "We use the Microsoft (MS) COCO dataset [33], which contains 123287 images each with 5 captions.", "startOffset": 39, "endOffset": 43}, {"referenceID": 16, "context": "For development and testing we use the same splits as [17].", "startOffset": 54, "endOffset": 58}, {"referenceID": 33, "context": "We represent images using 4096-dimensional feature vectors from VggNet [34].", "startOffset": 71, "endOffset": 75}, {"referenceID": 6, "context": "The training objective we use is the same pairwise ranking loss as used in [7], which takes the form of max(0, \u03b1 \u2212 f(xn, yn) + f(xn, ym)), where f(\u00b7, \u00b7) is the image-sentence score.", "startOffset": 75, "endOffset": 78}, {"referenceID": 6, "context": "uni-skip [7] 0.", "startOffset": 9, "endOffset": 12}, {"referenceID": 6, "context": "2872 bi-skip [7] 0.", "startOffset": 13, "endOffset": 16}, {"referenceID": 6, "context": "2995 combine-skip [7] 0.", "startOffset": 18, "endOffset": 21}, {"referenceID": 6, "context": "As can be seen, we obtain the same median rank as in [7], indicating that our encoder is as competitive as the skip-thought vectors [7].", "startOffset": 53, "endOffset": 56}, {"referenceID": 6, "context": "As can be seen, we obtain the same median rank as in [7], indicating that our encoder is as competitive as the skip-thought vectors [7].", "startOffset": 132, "endOffset": 135}, {"referenceID": 6, "context": "The performance gain between our encoder and the combine-skip model of [7] on the R@1 score is significant, which shows that the CNN encoder has more discriminative power on retrieving the most correct item than a LSTM encoder.", "startOffset": 71, "endOffset": 74}, {"referenceID": 0, "context": "Given two sentences, our goal is to produce a realvalued score between [1, 5] to indicate how semantically related two sentences are, based on human generated scores.", "startOffset": 71, "endOffset": 77}, {"referenceID": 4, "context": "Given two sentences, our goal is to produce a realvalued score between [1, 5] to indicate how semantically related two sentences are, based on human generated scores.", "startOffset": 71, "endOffset": 77}, {"referenceID": 31, "context": "We follow the method in [32], and use the cross-entropy loss for training.", "startOffset": 24, "endOffset": 28}, {"referenceID": 6, "context": "result is competitive with, though slightly worse than, the combine-skip model of [7].", "startOffset": 82, "endOffset": 85}, {"referenceID": 6, "context": "Our trained convolutional encoder is highly generic, and can be an alternative to the skip-thought vectors of [7].", "startOffset": 110, "endOffset": 113}, {"referenceID": 9, "context": "In future work, we aim to use more advanced CNN architectures [10] for sentence encoding.", "startOffset": 62, "endOffset": 66}, {"referenceID": 18, "context": "For example, the basic CNN-LSTM model can be used for machine translation [19], while the hierarchical CNN-LSTM model can be extended to learn document embeddings [20].", "startOffset": 74, "endOffset": 78}, {"referenceID": 19, "context": "For example, the basic CNN-LSTM model can be used for machine translation [19], while the hierarchical CNN-LSTM model can be extended to learn document embeddings [20].", "startOffset": 163, "endOffset": 167}], "year": 2016, "abstractText": "We propose a new encoder-decoder approach to learn distributed sentence representations from unlabeled sentences. The word-to-vector representation is used, and convolutional neural networks are employed as sentence encoders, mapping an input sentence into a fixed-length vector. This representation is decoded using long short-term memory recurrent neural networks, considering several tasks, such as reconstructing the input sentence, or predicting the future sentence. We further describe a hierarchical encoder-decoder model to encode a sentence to predict multiple future sentences. By training our models on a large collection of novels, we obtain a highly generic convolutional sentence encoder that performs well in practice. Experimental results on several benchmark datasets, and across a broad range of applications, demonstrate the superiority of the proposed model over competing methods.", "creator": "LaTeX with hyperref package"}}}