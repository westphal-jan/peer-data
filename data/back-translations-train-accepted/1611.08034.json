{"id": "1611.08034", "review": {"conference": "ACL", "VERSION": "v1", "DATE_OF_SUBMISSION": "23-Nov-2016", "title": "Scalable Bayesian Learning of Recurrent Neural Networks for Language Modeling", "abstract": "Recurrent neural networks (RNNs) have shown promising performance for language modeling. However, traditional training of RNNs using back-propagation through time often suffers from overfitting. One reason for this is that stochastic optimization (used for large training sets) does not provide good estimates of model uncertainty. This paper leverages recent advances in stochastic gradient Markov Chain Monte Carlo (also appropriate for large training sets) to learn weight uncertainty in RNNs. It yields a principled Bayesian learning algorithm, adding gradient noise during training (enhancing exploration of the model-parameter space) and model averaging when testing. Extensive experiments on various RNN models and across a broad range of applications demonstrate the superiority of the proposed approach over stochastic optimization.", "histories": [["v1", "Wed, 23 Nov 2016 23:40:50 GMT  (1012kb,D)", "http://arxiv.org/abs/1611.08034v1", null], ["v2", "Mon, 24 Apr 2017 15:32:49 GMT  (663kb,D)", "http://arxiv.org/abs/1611.08034v2", "Accepted to ACL 2017"]], "reviews": [], "SUBJECTS": "cs.CL cs.LG", "authors": ["zhe gan", "chunyuan li", "changyou chen", "yunchen pu", "qinliang su", "lawrence carin"], "accepted": true, "id": "1611.08034"}, "pdf": {"name": "1611.08034.pdf", "metadata": {"source": "CRF", "title": "Scalable Bayesian Learning of Recurrent Neural Networks for Language Modeling", "authors": ["Zhe Gan", "Chunyuan Li", "Changyou Chen", "Yunchen Pu", "Qinliang Su", "Lawrence Carin"], "emails": ["lcarin}@duke.edu"], "sections": [{"heading": "1 Introduction", "text": "This year, it has come to the point where it will be able to put itself at the top, \"he said in an interview with the\" Welt am Sonntag. \""}, {"heading": "2 Related Work", "text": "Several scalable Bayesian learning methods have recently been proposed for neural networks, which fall into two broad categories: stochastic variational inference (Graves, 2011; Blundell et al., 2015; Herna \u0301 ndez-Lobato and Adams, 2015) and SG-MCMC methods (Korattikara et al., 2015; Li et al., 2016a). While previous work has focused on forward-looking neural networks, there has been little to no research on the application of dropouts to RNNNs using SGMCMC.Dropout (Hinton et al., 2012; Srivastava et al., 2014) is a commonly used method of regulation for the formation of neural networks. Recently, there has been several papers investigating how to apply dropouts to RNNNNNs (Pachitariu and Sahani, 2013; Bayer et al., 2013; Pham et al., 2014; Zaremal., al., 2014; the ability to dropout; and the ability to dropout."}, {"heading": "3 Recurrent Neural Networks", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "3.1 RNN as Bayesian Predictive Models", "text": "??????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????"}, {"heading": "3.2 RNN Architectures", "text": "The transition function H (\u00b7) can be implemented with a gated activation function, such as Long Short-Term Memory (LSTM) (Hochreiter and Schmidhuber, 1997) or a Gated Recurrent Unit (GRU) (Cho et al., 2014). Both LSTM and GRU have been proposed to address the problem of long-term sequential learning dependencies. Specifically, each LSTM unit has a cell that contains a state at present t, which can be considered a storage unit. Read or write the cell is controlled by sigmoid gates: entrance gate it, forget gate ft, and output gate ot, and output gate ot. Hidden units ht are updated as follows: it = \u03c3 (Wixt + Uiht \u2212 1 + bi), ft = (Wfxt + Ufht \u2212 ft)."}, {"heading": "3.3 Applications", "text": "The proposed Bayesian framework can be applied to any RNN model; we focus on the following basic tasks to demonstrate ideals.Language modeling In word-level speech modeling, input to the network is a sequence of words, and the network is trained to predict the next word in a sequence using a Softmax classifier. Specifically, for a length-T sequence, this means yt = xt + 1 for t = 1..., T \u2212 1. x1 and yT are always set to a special START or END token, respectively. t has a decryption function p (yt | ht) = Softmax (Vht) to calculate word distribution, where V is the decryption weight (the number of lines of V equals the number of words / characters).We are also expanding this basic language model to include other applications: (i) a character-level language model."}, {"heading": "4 Scalable Learning with SG-MCMC", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "4.1 The Pitfall of Stochastic Optimization", "text": "Typically, there is no closed-form solution for (1), and traditional MCMC methods scale poorly for large N. To facilitate computing load, stochastic optimization is often used to find the MAP solution, which is equivalent to minimizing a target of regulated loss function U (\u03b8) that corresponds to a (non-convex) interest model: \u03b8MAP = argminU (\u03b8), U (\u03b8) = \u2212 log p (\u03b8 | D). The expectation in (1) is approximated as: p (Y-X-X, D) = p (Y-X-X, \u03b8MAP). (2) Although this method is simple and effective, it largely loses the benefit of the Bayesian approach because uncertainty about weights is ignored. To be more precise (1), we use SGMCMC."}, {"heading": "4.2 Large-scale Bayesian Learning", "text": "In a Bayesian model, the above-mentioned regulated loss function corresponds to the potential energy defined as a negative log posterior: U (\u03b8), \u2212 log p (\u03b8) \u2212 N-log n = 1 log p (Dn | \u03b8). (3) In optimization, E = \u2212 \u2211 Nn = 1 log p (Dn | \u03b8) is typically referred to as a loss function, and R-log p (\u03b8) as a regulator. Stochastic approximations are often used for large N: U-log p (\u03b8), \u2212 log p (\u03b8) \u2212 NM M m = 1 log p (Dim | \u03b8), (4), where Sm = {i1, \u00b7 \u00b7, iM} is a random subset of the set {1, 2, \u00b7 \u00b7, N}, with M N. The gradient in this mini-batch is referred to as f-log p (Dim | \u03b8), where Sm = {i1, \u00b7, iM} is a random subset {1, \u00b7, N}, {\u00b7,} is a subset of the set."}, {"heading": "4.3 SG-MCMC Algorithms", "text": "SG-MCMC and stochastic optimization are two parallel lines of work, designed for different purposes; their relationship has recently been revealed in the context of deep learning (1). The most basic SG-MCMC algorithms were applied to Langevin dynamics and are referred to as SGLD (Welling and Teh, 2011). To support convergence, an impulse term was introduced into SGHMC (Chen et al., 2014), a \"thermostat\" was developed in SGNHT (Ding et al., 2015) and preconditioners were used in pSGLD (Li et al., 2016a). These SG-MCMC algorithms often share similar properties with their counterparts from the optimization literature such as the SGD, Santa et al., 2016) and RMSprop / Adagrad (Tieleman and Hinchi al., 2011)."}, {"heading": "4.4 Understanding SG-MCMC", "text": "To further understand SG-MCMC, we show its close association with Dropout / DropConnect (Srivastava et al., 2014; Wan et al., 2013), which improves the generalization capability of deep models by randomly adding binary / Gauss noise to local units or global weights. For neural networks with the nonlinear function q (\u00b7) and successive layers h1 and h2, Dropout and DropConnect are referred to as: Dropout method: h2 = 0 q (\u03b8h1), DropConnect: h2 = q (((\u04450) h1), whereby the injected noise mixture \u04450 with drop rate p p or its equivalent Gauss form (Wang and Manning, 2013): Binary noise: Ber (p), Gauss noise: N (1, p1 \u2212 p)."}, {"heading": "5 Experiments", "text": "Hyper parameter setting, model parameter initialization, and model specifications for each dataset are all in the Supplementary Material.We do not perform any data set-specific tuning, except for stopping validation sets early. Empirically, when using the dropout rate, the dropout rate is set to 0.5. All experiments are performed in Theano (Theano Development Team, 2016) using an NVIDIA GeForce GTX TITAN X GPU with 12 GB of memory."}, {"heading": "5.1 Language Modeling", "text": "In fact, the fact is that most of them are able to survive themselves, and that they are able to survive themselves, \"he told the German Press Agency in an interview with\" Welt am Sonntag \":\" I don't think we feel able to change the world. \""}, {"heading": "5.2 Image Caption Generation", "text": "We look at the problem of education protocols, which represent a model of pure networking (er et al., 2016) and are then fed into the RNG to generate the education protocols."}, {"heading": "5.3 Sentence Classification", "text": "We examine the task of record classification on 5 data sets as in (Kiros et al., 2015): TREC (Li and Roth, 2002), MR (Pang and Lee, 2005), SUBJ (Pang and Lee, 2004), CR (Hu and Liu, 2004) and MPQA (Wiebe et al., 2005). A detailed description of the data sets is provided in the Supplementary Material. A single-layer bi-directional LSTM is used with the number of hidden units. Table 5 shows the test errors. 10-fold cross validation is used to evaluate the first 4 data sets."}, {"heading": "6 Conclusion", "text": "We propose a scalable Bayesian learning system that uses SG-MCMC to model weight insecurity in relapsing neural networks, testing the learning system on multiple tasks, including speech models, captions, and sentence classification. Our algorithm outperforms traditional stochastic optimization algorithms, highlighting the importance of learning weight insecurity in relapsing neural networks. Our algorithm requires little additional computational effort in training and multiple forward-passing for model averaging in testing. Future work will include improving test efficiency for the large neural networks by learning a single neural network that approximates the average result of the model (Korattikara et al., 2015)."}, {"heading": "Acknowledgments", "text": "This research was partially supported by ARO, DARPA, DOE, NGA, ONR and NSF."}, {"heading": "A Gated Recurrent Units", "text": "Similar to the LSTM unit, the GRU (Cho et al., 2014) has gating units that modulate the information flow within the unit, but without using a separate memory cell. Specifically, the GRU has two gates: the reset gate rt and the update gate zt. Hidden units ht are updated as follows: rt = \u03c3 (Wrxt + Urht \u2212 1 + br), (9) zt = \u03c3 (Wzxt + Uzht \u2212 1 + bz), (10) h-t = tanh (Whxt + Uh (rt ht \u2212 1) + bh), (11) ht = (1 \u2212 zt) ht \u2212 1 + zt h-t, (12) where \u03c3 (\u00b7) denotes the logistic sigmoid function and represents the element multiplying operator. W {r, z, h} encode weights and U {r, z, h} are recursive weights. b {r, z, h} are terms."}, {"heading": "B Model Details", "text": "B.1 Standard Language ModelingFor an input sequence X = {x1,.., xT} where xt is the input data vector at the time t, we define an output sequence Y = {y1,.., yT} with yt = xt + 1 for t = 1.,., T \u2212 1. x1 and yT are always set to a special START or END token. (13) At any time there is a decryption function p (yt | X) = softmax (Vht) = t = 1 p (yt | x \u2264 t) = T = 1 p (yt | ht). (13) At any time there is a decryption function p (yt | ht) = softmax (Vht) to calculate word distribution, where V is the decoding weights."}, {"heading": "C SGLD Algorithm", "text": "We list the SGLD algorithm below for clarification. Algorithm 2: SGLD Input: Learning rate schedule {\u03b7t} Tt = 1. Initialize: \u03b81 \u0445 N (0, I); for t = 1, 2,..., T do% Estimate gradient from minibatch Sm f \u0435t = \u044b U-T (\u03b8);% Parameter update \u0435t-N (0, \u03b7tI); \u03b8t + 1 \u2190 \u03b8t + \u03b7t2 f-t + \u0442t; end"}, {"heading": "D Experimental Setup", "text": "For RNN training, orthogonal initialization is applied to all recurring matrices (Saxe et al., 2014). Non-recurring weights are initialized by a uniform distribution in [\u2212 0.01, 0.01]. All bias terms are initialized to zero. It is observed that setting a high initial forgetgate bias for LSTMs can yield slightly better results (Le et al., 2015). Therefore, the initial forgetgate bias is set to 3 during the experiments. Word vectors are initialized at random with the publicly available word2vec vectors (Mikolov et al., 2013). These vectors have dimensionality 300 and were trained using a continuous sack-of-words architecture. Words not present in the series of pre-trained words are randomly initialized. Gradients are truncated when the norm of parameter vector 5 is exceeded (Sutever, 2014)."}, {"heading": "E Details of Classification Datasets", "text": "We test SG-MCMC methods using various set classification benchmark data sets. Summary statistics of the data sets are in Table 7. For sets without a standard validation set, we selected 10% of the training data as the validation set. \u2022 TREC: This task involves classifying a question into 6 types (Li and Roth, 2002). \u2022 MR: Film ratings with one set per rating. Classification includes predicting positive / negative ratings (Pang and Lee, 2005). \u2022 SUBJ: Subjectivity data set where the task is to classify a set as subjective or objective (Pang and Lee, 2004). \u2022 CR: Customer ratings of various products. This task is to predict positive / negative ratings (Hu and Liu, 2004). \u2022 MPQA: Sub-task for polarity detection of the MPQA data set (Wiebe et al., 2005)."}], "references": [{"title": "Meteor: An automatic metric for mt evaluation with improved correlation with human judgments", "author": ["Banerjee", "Lavie2005] Satanjeev Banerjee", "Alon Lavie"], "venue": "In ACL workshop", "citeRegEx": "Banerjee et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Banerjee et al\\.", "year": 2005}, {"title": "On fast dropout and its applicability to recurrent networks. arXiv:1311.0701", "author": ["Bayer et al.2013] J. Bayer", "C. Osendorfer", "D. Korhammer", "N. Chen", "S. Urban", "P. van der Smagt"], "venue": null, "citeRegEx": "Bayer et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Bayer et al\\.", "year": 2013}, {"title": "Where to apply dropout in recurrent neural networks for handwriting recognition? In ICDAR", "author": ["Bluche et al.2015] T. Bluche", "C. Kermorvant", "J. Louradour"], "venue": null, "citeRegEx": "Bluche et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Bluche et al\\.", "year": 2015}, {"title": "Weight uncertainty in neural networks", "author": ["Blundell et al.2015] C. Blundell", "J. Cornebise", "K. Kavukcuoglu", "D. Wierstra"], "venue": null, "citeRegEx": "Blundell et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Blundell et al\\.", "year": 2015}, {"title": "Large-scale machine learning with stochastic gradient descent. In COMPSTAT", "author": ["L Bottou"], "venue": null, "citeRegEx": "Bottou.,? \\Q2010\\E", "shortCiteRegEx": "Bottou.", "year": 2010}, {"title": "Stochastic gradient Hamiltonian Monte Carlo", "author": ["T. Chen", "E.B. Fox", "C. Guestrin"], "venue": null, "citeRegEx": "Chen et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Chen et al\\.", "year": 2014}, {"title": "On the convergence of stochastic gradient MCMC algorithms with high-order integrators", "author": ["Chen et al.2015a] C. Chen", "N. Ding", "L. Carin"], "venue": null, "citeRegEx": "Chen et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Chen et al\\.", "year": 2015}, {"title": "2015b. Microsoft coco captions: Data collection and evaluation", "author": ["Chen et al.2015b] Xinlei Chen", "Hao Fang", "Tsung-Yi Lin", "Ramakrishna Vedantam", "Saurabh Gupta", "Piotr Doll\u00e1r", "C Lawrence Zitnick"], "venue": null, "citeRegEx": "Chen et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Chen et al\\.", "year": 2015}, {"title": "Bridging the gap between", "author": ["C. Chen", "D. Carlson", "Z. Gan", "C. Li", "L. Carin"], "venue": null, "citeRegEx": "Chen et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Chen et al\\.", "year": 2016}, {"title": "Learning phrase representations using RNN encoder-decoder for statistical machine translation", "author": ["K. Cho", "B. Van Merri\u00ebnboer", "C. Gulcehre", "D. Bahdanau", "F. Bougares", "H. Schwenk", "Y. Bengio"], "venue": null, "citeRegEx": "Cho et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Cho et al\\.", "year": 2014}, {"title": "Empirical evaluation of gated recurrent neural networks on sequence modeling", "author": ["Chung et al.2014] J. Chung", "C. Gulcehre", "K. Cho", "Y. Bengio"], "venue": null, "citeRegEx": "Chung et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Chung et al\\.", "year": 2014}, {"title": "Bayesian sampling using stochastic gradient thermostats", "author": ["Ding et al.2014] N. Ding", "Y. Fang", "R. Babbush", "C. Chen", "R.D. Skeel", "H. Neven"], "venue": null, "citeRegEx": "Ding et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Ding et al\\.", "year": 2014}, {"title": "Adaptive subgradient methods for online learning and stochastic optimization. JMLR", "author": ["Duchi et al.2011] J. Duchi", "E. Hazan", "Y. Singer"], "venue": null, "citeRegEx": "Duchi et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Duchi et al\\.", "year": 2011}, {"title": "Dropout as a Bayesian approximation: Representing model uncertainty in deep learning", "author": ["Gal", "Ghahramani2016a] Y. Gal", "Z. Ghahramani"], "venue": null, "citeRegEx": "Gal et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Gal et al\\.", "year": 2016}, {"title": "2016b. A theoretically grounded application of dropout in recurrent neural networks", "author": ["Gal", "Ghahramani2016b] Y. Gal", "Z. Ghahramani"], "venue": null, "citeRegEx": "Gal et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Gal et al\\.", "year": 2016}, {"title": "Scalable deep poisson factor analysis for topic modeling", "author": ["Gan et al.2015] Z. Gan", "C. Chen", "R. Henao", "D. Carlson", "L. Carin"], "venue": null, "citeRegEx": "Gan et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Gan et al\\.", "year": 2015}, {"title": "Practical variational inference for neural networks", "author": ["A. Graves"], "venue": "In NIPS", "citeRegEx": "Graves.,? \\Q2011\\E", "shortCiteRegEx": "Graves.", "year": 2011}, {"title": "Deep residual learning for image recognition", "author": ["He et al.2016] Kaiming He", "Xiangyu Zhang", "Shaoqing Ren", "Jian Sun"], "venue": null, "citeRegEx": "He et al\\.,? \\Q2016\\E", "shortCiteRegEx": "He et al\\.", "year": 2016}, {"title": "Probabilistic backpropagation for scalable learning of Bayesian neural networks", "author": ["Hern\u00e1ndez-Lobato", "Adams2015] J.M. Hern\u00e1ndezLobato", "R.P. Adams"], "venue": null, "citeRegEx": "Hern\u00e1ndez.Lobato et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Hern\u00e1ndez.Lobato et al\\.", "year": 2015}, {"title": "Improving neural networks by preventing co-adaptation of feature detectors", "author": ["Hinton et al.2012] G. Hinton", "N. Srivastava", "A. Krizhevsky", "I. Sutskever", "R Salakhutdinov"], "venue": null, "citeRegEx": "Hinton et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Hinton et al\\.", "year": 2012}, {"title": "Long short-term memory", "author": ["Hochreiter", "Schmidhuber1997] S. Hochreiter", "J. Schmidhuber"], "venue": "In Neural computation", "citeRegEx": "Hochreiter et al\\.,? \\Q1997\\E", "shortCiteRegEx": "Hochreiter et al\\.", "year": 1997}, {"title": "Framing image description as a ranking task: Data, models and evaluation metrics. JAIR", "author": ["Hodosh et al.2013] M. Hodosh", "P. Young", "J. Hockenmaier"], "venue": null, "citeRegEx": "Hodosh et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Hodosh et al\\.", "year": 2013}, {"title": "Mining and summarizing customer reviews. SIGKDD", "author": ["Hu", "Liu2004] M. Hu", "B. Liu"], "venue": null, "citeRegEx": "Hu et al\\.,? \\Q2004\\E", "shortCiteRegEx": "Hu et al\\.", "year": 2004}, {"title": "Visualizing and understanding recurrent networks", "author": ["Karpathy et al.2016] A. Karpathy", "J. Johnson", "L. Fei-Fei"], "venue": "In ICLR Workshop", "citeRegEx": "Karpathy et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Karpathy et al\\.", "year": 2016}, {"title": "Adam: A method for stochastic optimization. In ICLR", "author": ["Kingma", "Ba2015] D. Kingma", "J. Ba"], "venue": null, "citeRegEx": "Kingma et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Kingma et al\\.", "year": 2015}, {"title": "Variational dropout and the local reparameterization trick", "author": ["Kingma et al.2015] D. Kingma", "T. Salimans", "M. Welling"], "venue": null, "citeRegEx": "Kingma et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Kingma et al\\.", "year": 2015}, {"title": "Bayesian dark knowledge", "author": ["V. Rathod", "K. Murphy", "M. Welling"], "venue": null, "citeRegEx": "Korattikara et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Korattikara et al\\.", "year": 2015}, {"title": "A simple way to initialize recurrent networks of rectified linear units. arXiv:1504.00941", "author": ["Le et al.2015] Q.V. Le", "N. Jaitly", "G.E. Hinton"], "venue": null, "citeRegEx": "Le et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Le et al\\.", "year": 2015}, {"title": "Preconditioned stochastic gradient Langevin dynamics for deep neural networks", "author": ["Li et al.2016a] C. Li", "C. Chen", "D. Carlson", "L. Carin"], "venue": null, "citeRegEx": "Li et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Li et al\\.", "year": 2016}, {"title": "Learning weight uncertainty with stochastic gradient MCMC for shape classification", "author": ["Li et al.2016b] C. Li", "A. Stevens", "C. Chen", "Y. Pu", "Z. Gan", "L. Carin"], "venue": null, "citeRegEx": "Li et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Li et al\\.", "year": 2016}, {"title": "Rouge: A package for automatic evaluation of summaries", "author": ["Chin-Yew Lin"], "venue": "In ACL workshop", "citeRegEx": "Lin.,? \\Q2004\\E", "shortCiteRegEx": "Lin.", "year": 2004}, {"title": "A practical Bayesian framework for backpropagation networks", "author": ["D.J.C. MacKay"], "venue": null, "citeRegEx": "MacKay.,? \\Q1992\\E", "shortCiteRegEx": "MacKay.", "year": 1992}, {"title": "Building a large annotated corpus of english: The penn treebank", "author": ["M.P. Marcus", "M.A. Marcinkiewicz", "B. Santorini"], "venue": null, "citeRegEx": "Marcus et al\\.,? \\Q1993\\E", "shortCiteRegEx": "Marcus et al\\.", "year": 1993}, {"title": "Recurrent neural network based language model", "author": ["Mikolov et al.2010] T. Mikolov", "M. Karafi\u00e1t", "L. Burget", "J. Cernock\u1ef3", "S. Khudanpur"], "venue": "In INTERSPEECH", "citeRegEx": "Mikolov et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Mikolov et al\\.", "year": 2010}, {"title": "Distributed representations of words and phrases and their compositionality", "author": ["Mikolov et al.2013] T. Mikolov", "I. Sutskever", "K. Chen", "G.S. Corrado", "J. Dean"], "venue": null, "citeRegEx": "Mikolov et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Mikolov et al\\.", "year": 2013}, {"title": "Rnndrop: A novel dropout for rnns in asr. ASRU", "author": ["Moon et al.2015] T. Moon", "H. Choi", "H. Lee", "I. Song"], "venue": null, "citeRegEx": "Moon et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Moon et al\\.", "year": 2015}, {"title": "Bayesian learning for neural networks", "author": ["R.M. Neal"], "venue": "PhD thesis,", "citeRegEx": "Neal.,? \\Q1995\\E", "shortCiteRegEx": "Neal.", "year": 1995}, {"title": "Adding gradient noise improves learning for very deep networks", "author": ["L. Vilnis", "Q. Le", "I. Sutskever", "L. Kaiser", "K. Kurach", "J. Martens"], "venue": "In ICLR workshop", "citeRegEx": "Neelakantan et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Neelakantan et al\\.", "year": 2016}, {"title": "Regularization and nonlinearities for neural language models: when are they needed? arXiv:1301.5650", "author": ["Pachitariu", "Sahani2013] M. Pachitariu", "M. Sahani"], "venue": null, "citeRegEx": "Pachitariu et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Pachitariu et al\\.", "year": 2013}, {"title": "A sentimental education: Sentiment analysis using subjectivity summarization based on minimum cuts", "author": ["Pang", "Lee2004] B. Pang", "L. Lee"], "venue": null, "citeRegEx": "Pang et al\\.,? \\Q2004\\E", "shortCiteRegEx": "Pang et al\\.", "year": 2004}, {"title": "Seeing stars: Exploiting class relationships for sentiment categorization with respect to rating", "author": ["Pang", "Lee2005] B. Pang", "L. Lee"], "venue": null, "citeRegEx": "Pang et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Pang et al\\.", "year": 2005}, {"title": "Bleu: a method for automatic evaluation of machine translation", "author": ["Salim Roukos", "Todd Ward", "Wei-Jing Zhu"], "venue": null, "citeRegEx": "Papineni et al\\.,? \\Q2002\\E", "shortCiteRegEx": "Papineni et al\\.", "year": 2002}, {"title": "On the difficulty of training recurrent neural networks", "author": ["Pascanu et al.2013] R. Pascanu", "T. Mikolov", "Y. Bengio"], "venue": null, "citeRegEx": "Pascanu et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Pascanu et al\\.", "year": 2013}, {"title": "Dropout improves recurrent neural networks for handwriting recognition", "author": ["Pham et al.2014] V. Pham", "T. Bluche", "C. Kermorvant", "J. Louradour"], "venue": "ICFHR", "citeRegEx": "Pham et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Pham et al\\.", "year": 2014}, {"title": "A stochastic approximation method. In The annals of mathematical statistics", "author": ["Robbins", "Monro1951] H. Robbins", "S. Monro"], "venue": null, "citeRegEx": "Robbins et al\\.,? \\Q1951\\E", "shortCiteRegEx": "Robbins et al\\.", "year": 1951}, {"title": "Exact solutions to the nonlinear dynamics of learning in deep linear neural networks. In ICLR", "author": ["Saxe et al.2014] A.M. Saxe", "J.L. McClelland", "S. Ganguli"], "venue": null, "citeRegEx": "Saxe et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Saxe et al\\.", "year": 2014}, {"title": "Recurrent dropout without memory loss", "author": ["A. Severyn", "E. Barth"], "venue": null, "citeRegEx": "Semeniuta et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Semeniuta et al\\.", "year": 2016}, {"title": "Dropout: A simple way to prevent neural networks from overfitting", "author": ["G. Hinton", "A. Krizhevsky", "I. Sutskever", "R. Salakhutdinov"], "venue": null, "citeRegEx": "Srivastava et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Srivastava et al\\.", "year": 2014}, {"title": "Generating text with recurrent neural networks", "author": ["J. Martens", "G.E. Hinton"], "venue": null, "citeRegEx": "Sutskever et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Sutskever et al\\.", "year": 2011}, {"title": "Sequence to sequence learning with neural networks", "author": ["O. Vinyals", "Q.V. Le"], "venue": null, "citeRegEx": "Sutskever et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Sutskever et al\\.", "year": 2014}, {"title": "Consistency and fluctuations for stochastic gradient Langevin dynamics. JMLR", "author": ["Teh et al.2016] Y.W. Teh", "A.H. Thi\u00e9ry", "S.J. Vollmer"], "venue": null, "citeRegEx": "Teh et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Teh et al\\.", "year": 2016}, {"title": "Lecture 6.5-rmsprop: Divide the gradient by a running average of its recent magnitude. Coursera: Neural Networks for Machine Learning", "author": ["Tieleman", "Hinton2012] T. Tieleman", "G. Hinton"], "venue": null, "citeRegEx": "Tieleman et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Tieleman et al\\.", "year": 2012}, {"title": "Visualizing data using tSNE", "author": ["Van der Maaten", "Hinton2008] L. Van der Maaten", "G.E. Hinton"], "venue": null, "citeRegEx": "Maaten et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Maaten et al\\.", "year": 2008}, {"title": "Cider: Consensus-based image description evaluation", "author": ["C Lawrence Zitnick", "Devi Parikh"], "venue": null, "citeRegEx": "Vedantam et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Vedantam et al\\.", "year": 2015}, {"title": "Show and tell: A neural image caption generator", "author": ["Vinyals et al.2015] O. Vinyals", "A. Toshev", "S. Bengio", "D. Erhan"], "venue": null, "citeRegEx": "Vinyals et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Vinyals et al\\.", "year": 2015}, {"title": "Regularization of neural networks using DropConnect", "author": ["Wan et al.2013] L. Wan", "M. Zeiler", "S. Zhang", "Y. LeCun", "R. Fergus"], "venue": null, "citeRegEx": "Wan et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Wan et al\\.", "year": 2013}, {"title": "Fast Dropout training", "author": ["Wang", "Manning2013] S. Wang", "C. Manning"], "venue": "In ICML", "citeRegEx": "Wang et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Wang et al\\.", "year": 2013}, {"title": "Bayesian learning via stochastic gradient Langevin dynamics", "author": ["Welling", "Teh2011] M. Welling", "Y.W. Teh"], "venue": null, "citeRegEx": "Welling et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Welling et al\\.", "year": 2011}, {"title": "Backpropagation through time: what it does and how to do it", "author": ["P. Werbos"], "venue": "In Proceedings of the IEEE", "citeRegEx": "Werbos.,? \\Q1990\\E", "shortCiteRegEx": "Werbos.", "year": 1990}, {"title": "Annotating expressions of opinions and emotions in language. Language resources and evaluation", "author": ["Wiebe et al.2005] J. Wiebe", "T. Wilson", "C. Cardie"], "venue": null, "citeRegEx": "Wiebe et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Wiebe et al\\.", "year": 2005}, {"title": "From image descriptions to visual denotations: New similarity metrics for semantic inference over event descriptions. TACL", "author": ["Young et al.2014] P. Young", "A. Lai", "M. Hodosh", "J. Hockenmaier"], "venue": null, "citeRegEx": "Young et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Young et al\\.", "year": 2014}, {"title": "Recurrent neural network regularization", "author": ["Zaremba et al.2014] W. Zaremba", "I. Sutskever", "O. Vinyals"], "venue": null, "citeRegEx": "Zaremba et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Zaremba et al\\.", "year": 2014}, {"title": "Experimental Setup For RNN training, orthogonal initialization is employed on all recurrent matrices (Saxe et", "author": ["D end"], "venue": null, "citeRegEx": "end,? \\Q2014\\E", "shortCiteRegEx": "end", "year": 2014}], "referenceMentions": [{"referenceID": 33, "context": "Recently, recurrent neural networks (RNNs) have shown promising performance on this task (Mikolov et al., 2010; Sutskever et al., 2011).", "startOffset": 89, "endOffset": 135}, {"referenceID": 48, "context": "Recently, recurrent neural networks (RNNs) have shown promising performance on this task (Mikolov et al., 2010; Sutskever et al., 2011).", "startOffset": 89, "endOffset": 135}, {"referenceID": 58, "context": "RNNs are usually trained via back-propagation through time (Werbos, 1990), using stochastic optimization methods such as stochastic gradient descent (SGD) (Robbins and Monro, 1951); stochastic methods of this type are particularly important for training with large data sets.", "startOffset": 59, "endOffset": 73}, {"referenceID": 3, "context": "The MAP solution is a single point estimate, ignoring weight uncertainty (Blundell et al., 2015; Hern\u00e1ndezLobato and Adams, 2015).", "startOffset": 73, "endOffset": 129}, {"referenceID": 31, "context": "In the neural network literature, Bayesian learning has been proposed as a principled method to impose regularization and incorporate model uncertainty (MacKay, 1992; Neal, 1995), by imposing prior distributions on model parameters.", "startOffset": 152, "endOffset": 178}, {"referenceID": 36, "context": "In the neural network literature, Bayesian learning has been proposed as a principled method to impose regularization and incorporate model uncertainty (MacKay, 1992; Neal, 1995), by imposing prior distributions on model parameters.", "startOffset": 152, "endOffset": 178}, {"referenceID": 36, "context": "Due to the intractability of posterior distributions in neural networks, Hamiltonian Monte Carlo (HMC) (Neal, 1995) has been used to provide sample-based approximations to the true posterior.", "startOffset": 103, "endOffset": 115}, {"referenceID": 5, "context": "This paper seeks to scale up Bayesian learning of RNNs to meet the challenge of the increasing amount of \u201cbig\u201d sequential data in natural language processing, by leveraging recent advances in stochastic gradient Markov Chain Monte Carlo (SG-MCMC) algorithms (Welling and Teh, 2011; Chen et al., 2014; Ding et al., 2014; Li et al., 2016a).", "startOffset": 258, "endOffset": 337}, {"referenceID": 11, "context": "This paper seeks to scale up Bayesian learning of RNNs to meet the challenge of the increasing amount of \u201cbig\u201d sequential data in natural language processing, by leveraging recent advances in stochastic gradient Markov Chain Monte Carlo (SG-MCMC) algorithms (Welling and Teh, 2011; Chen et al., 2014; Ding et al., 2014; Li et al., 2016a).", "startOffset": 258, "endOffset": 337}, {"referenceID": 37, "context": "This procedure was also empirically found effective in (Neelakantan et al., 2016).", "startOffset": 55, "endOffset": 81}, {"referenceID": 50, "context": "(iii) In theory, both asymptotic and non-asymptotic consistency properties of SG-MCMC methods in posterior estimation have been recently established to guarantee convergence (Chen et al., 2015a; Teh et al., 2016).", "startOffset": 174, "endOffset": 212}, {"referenceID": 16, "context": "These come in two broad categories: stochastic variational inference (Graves, 2011; Blundell et al., 2015; Hern\u00e1ndez-Lobato and Adams, 2015) and SG-MCMC methods (Korattikara et al.", "startOffset": 69, "endOffset": 140}, {"referenceID": 3, "context": "These come in two broad categories: stochastic variational inference (Graves, 2011; Blundell et al., 2015; Hern\u00e1ndez-Lobato and Adams, 2015) and SG-MCMC methods (Korattikara et al.", "startOffset": 69, "endOffset": 140}, {"referenceID": 26, "context": ", 2015; Hern\u00e1ndez-Lobato and Adams, 2015) and SG-MCMC methods (Korattikara et al., 2015; Li et al., 2016a).", "startOffset": 62, "endOffset": 106}, {"referenceID": 19, "context": "Dropout (Hinton et al., 2012; Srivastava et al., 2014) is a commonly used regularization method for training neural networks.", "startOffset": 8, "endOffset": 54}, {"referenceID": 47, "context": "Dropout (Hinton et al., 2012; Srivastava et al., 2014) is a commonly used regularization method for training neural networks.", "startOffset": 8, "endOffset": 54}, {"referenceID": 1, "context": "Recently, there has been several works on studying how to apply dropout to RNNs (Pachitariu and Sahani, 2013; Bayer et al., 2013; Pham et al., 2014; Zaremba et al., 2014; Bluche et al., 2015; Moon et al., 2015; Semeniuta et al., 2016; Gal and Ghahramani, 2016b).", "startOffset": 80, "endOffset": 261}, {"referenceID": 43, "context": "Recently, there has been several works on studying how to apply dropout to RNNs (Pachitariu and Sahani, 2013; Bayer et al., 2013; Pham et al., 2014; Zaremba et al., 2014; Bluche et al., 2015; Moon et al., 2015; Semeniuta et al., 2016; Gal and Ghahramani, 2016b).", "startOffset": 80, "endOffset": 261}, {"referenceID": 61, "context": "Recently, there has been several works on studying how to apply dropout to RNNs (Pachitariu and Sahani, 2013; Bayer et al., 2013; Pham et al., 2014; Zaremba et al., 2014; Bluche et al., 2015; Moon et al., 2015; Semeniuta et al., 2016; Gal and Ghahramani, 2016b).", "startOffset": 80, "endOffset": 261}, {"referenceID": 2, "context": "Recently, there has been several works on studying how to apply dropout to RNNs (Pachitariu and Sahani, 2013; Bayer et al., 2013; Pham et al., 2014; Zaremba et al., 2014; Bluche et al., 2015; Moon et al., 2015; Semeniuta et al., 2016; Gal and Ghahramani, 2016b).", "startOffset": 80, "endOffset": 261}, {"referenceID": 35, "context": "Recently, there has been several works on studying how to apply dropout to RNNs (Pachitariu and Sahani, 2013; Bayer et al., 2013; Pham et al., 2014; Zaremba et al., 2014; Bluche et al., 2015; Moon et al., 2015; Semeniuta et al., 2016; Gal and Ghahramani, 2016b).", "startOffset": 80, "endOffset": 261}, {"referenceID": 46, "context": "Recently, there has been several works on studying how to apply dropout to RNNs (Pachitariu and Sahani, 2013; Bayer et al., 2013; Pham et al., 2014; Zaremba et al., 2014; Bluche et al., 2015; Moon et al., 2015; Semeniuta et al., 2016; Gal and Ghahramani, 2016b).", "startOffset": 80, "endOffset": 261}, {"referenceID": 61, "context": "Among them, naive dropout (Zaremba et al., 2014) can impose weight uncertainty only on encoding weights (those that connect input to hidden units) and decoding weights (those that connect hidden units to output), but not the recurrent weights (those that connect consecutive hidden states).", "startOffset": 26, "endOffset": 48}, {"referenceID": 24, "context": "Dropout has been recently shown to be a variational approximation technique in Bayesian learning (Gal and Ghahramani, 2016a; Kingma et al., 2015).", "startOffset": 97, "endOffset": 145}, {"referenceID": 9, "context": "The transition function H(\u00b7) can be implemented with a gated activation function, such as Long Short-Term Memory (LSTM) (Hochreiter and Schmidhuber, 1997) or a Gated Recurrent Unit (GRU) (Cho et al., 2014).", "startOffset": 187, "endOffset": 205}, {"referenceID": 10, "context": "It has been shown that a GRU can achieve similar performance to an LSTM in sequence modeling (Chung et al., 2014).", "startOffset": 93, "endOffset": 113}, {"referenceID": 23, "context": "We also extend this basic language model to consider other applications: (i) a character-level language model can be specified in a similar manner by replacing words with characters (Karpathy et al., 2016).", "startOffset": 182, "endOffset": 205}, {"referenceID": 54, "context": "(ii) Image captioning can be considered as a conditional language modeling problem, in which we learn a generative language model of the caption conditioned on an image (Vinyals et al., 2015).", "startOffset": 169, "endOffset": 191}, {"referenceID": 5, "context": "To help convergence, a momentum term has been introduced in SGHMC (Chen et al., 2014), a \u201cthermostat\u201d has been devised in SGNHT (Ding et al.", "startOffset": 66, "endOffset": 85}, {"referenceID": 11, "context": ", 2014), a \u201cthermostat\u201d has been devised in SGNHT (Ding et al., 2014; Gan et al., 2015) and preconditioners have been employed in pSGLD (Li et al.", "startOffset": 50, "endOffset": 87}, {"referenceID": 15, "context": ", 2014), a \u201cthermostat\u201d has been devised in SGNHT (Ding et al., 2014; Gan et al., 2015) and preconditioners have been employed in pSGLD (Li et al.", "startOffset": 50, "endOffset": 87}, {"referenceID": 8, "context": "These SG-MCMC algorithms often share similar characteristics with their counterpart approaches from the optimization literature such as the momentum SGD, Santa (Chen et al., 2016) and RMSprop/Adagrad (Tieleman and Hinton, 2012; Duchi et al.", "startOffset": 160, "endOffset": 179}, {"referenceID": 12, "context": ", 2016) and RMSprop/Adagrad (Tieleman and Hinton, 2012; Duchi et al., 2011).", "startOffset": 28, "endOffset": 75}, {"referenceID": 4, "context": "SGD is guaranteed to converge to a local minimum under mild conditions (Bottou, 2010).", "startOffset": 71, "endOffset": 85}, {"referenceID": 42, "context": "This is important for RNNs, whose parameter space often exhibits pathological curvature and saddle points (Pascanu et al., 2013), resulting in slow mixing.", "startOffset": 106, "endOffset": 128}, {"referenceID": 12, "context": "There are multiple choices of preconditioners; similar ideas in optimization include Adagrad (Duchi et al., 2011), Adam (Kingma and Ba, 2015) and RMSprop (Tieleman and Hinton, 2012).", "startOffset": 93, "endOffset": 113}, {"referenceID": 47, "context": "To further understand SG-MCMC, we show its close connection to dropout/dropConnect (Srivastava et al., 2014; Wan et al., 2013).", "startOffset": 83, "endOffset": 126}, {"referenceID": 55, "context": "To further understand SG-MCMC, we show its close connection to dropout/dropConnect (Srivastava et al., 2014; Wan et al., 2013).", "startOffset": 83, "endOffset": 126}, {"referenceID": 23, "context": "\u2022 Following (Karpathy et al., 2016), we test character-level language modeling on the War and Peace (WP) novel.", "startOffset": 12, "endOffset": 35}, {"referenceID": 32, "context": "\u2022 The Penn Treebank (PTB) corpus (Marcus et al., 1993) is used for word-level language modeling.", "startOffset": 33, "endOffset": 54}, {"referenceID": 61, "context": "The results reported here do not match (Zaremba et al., 2014) due to the differences of experimental setup.", "startOffset": 39, "endOffset": 61}, {"referenceID": 17, "context": "We next consider the problem of image caption generation, which is a conditional RNN model, where image features are extracted by residual network (He et al., 2016), and then fed into the RNN to generate the caption.", "startOffset": 147, "endOffset": 164}, {"referenceID": 21, "context": "We present results on two benchmark datasets, Flickr8k (Hodosh et al., 2013) and Flickr30k (Young et al.", "startOffset": 55, "endOffset": 76}, {"referenceID": 60, "context": ", 2013) and Flickr30k (Young et al., 2014).", "startOffset": 22, "endOffset": 42}, {"referenceID": 41, "context": "The widely used BLEU (Papineni et al., 2002), METEOR (Banerjee and Lavie, 2005), ROUGEL (Lin, 2004), and CIDEr-D (Vedantam et al.", "startOffset": 21, "endOffset": 44}, {"referenceID": 30, "context": ", 2002), METEOR (Banerjee and Lavie, 2005), ROUGEL (Lin, 2004), and CIDEr-D (Vedantam et al.", "startOffset": 51, "endOffset": 62}, {"referenceID": 53, "context": ", 2002), METEOR (Banerjee and Lavie, 2005), ROUGEL (Lin, 2004), and CIDEr-D (Vedantam et al., 2015) metrics are used to evaluate the performance.", "startOffset": 76, "endOffset": 99}, {"referenceID": 54, "context": "We use the beam search approach (Vinyals et al., 2015) to generate captions, with a beam of size 5.", "startOffset": 32, "endOffset": 54}, {"referenceID": 59, "context": ", 2015): TREC (Li and Roth, 2002), MR (Pang and Lee, 2005), SUBJ (Pang and Lee, 2004), CR (Hu and Liu, 2004) and MPQA (Wiebe et al., 2005).", "startOffset": 118, "endOffset": 138}, {"referenceID": 26, "context": "Future works include improving the testing efficiency for the large-scale RNNs, via learning a single neural network that approximates the model averaging result (Korattikara et al., 2015).", "startOffset": 162, "endOffset": 188}], "year": 2016, "abstractText": "Recurrent neural networks (RNNs) have shown promising performance for language modeling. However, traditional training of RNNs using back-propagation through time often suffers from overfitting. One reason for this is that stochastic optimization (used for large training sets) does not provide good estimates of model uncertainty. This paper leverages recent advances in stochastic gradient Markov Chain Monte Carlo (also appropriate for large training sets) to learn weight uncertainty in RNNs. It yields a principled Bayesian learning algorithm, adding gradient noise during training (enhancing exploration of the model-parameter space) and model averaging when testing. Extensive experiments on various RNN models and across a broad range of applications demonstrate the superiority of the proposed approach over stochastic optimization.", "creator": "LaTeX with hyperref package"}}}