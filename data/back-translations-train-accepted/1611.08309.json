{"id": "1611.08309", "review": {"conference": "AAAI", "VERSION": "v1", "DATE_OF_SUBMISSION": "24-Nov-2016", "title": "On Human Intellect and Machine Failures: Troubleshooting Integrative Machine Learning Systems", "abstract": "We study the problem of troubleshooting machine learning systems that rely on analytical pipelines of distinct components. Understanding and fixing errors that arise in such integrative systems is difficult as failures can occur at multiple points in the execution workflow. Moreover, errors can propagate, become amplified or be suppressed, making blame assignment difficult. We propose a human-in-the-loop methodology which leverages human intellect for troubleshooting system failures. The approach simulates potential component fixes through human computation tasks and measures the expected improvements in the holistic behavior of the system. The method provides guidance to designers about how they can best improve the system. We demonstrate the effectiveness of the approach on an automated image captioning system that has been pressed into real-world use.", "histories": [["v1", "Thu, 24 Nov 2016 21:08:41 GMT  (3785kb,D)", "http://arxiv.org/abs/1611.08309v1", "11 pages, Thirty-First AAAI conference on Artificial Intelligence"]], "COMMENTS": "11 pages, Thirty-First AAAI conference on Artificial Intelligence", "reviews": [], "SUBJECTS": "cs.LG", "authors": ["besmira nushi", "ece kamar", "eric horvitz", "donald kossmann"], "accepted": true, "id": "1611.08309"}, "pdf": {"name": "1611.08309.pdf", "metadata": {"source": "CRF", "title": "On Human Intellect and Machine Failures: Troubleshooting Integrative Machine Learning Systems", "authors": ["Besmira Nushi", "Ece Kamar", "Eric Horvitz", "Donald Kossmann"], "emails": [], "sections": [{"heading": "Introduction", "text": "Advances in machine learning have enabled the design of integrative systems that perform complex tasks through the execution of component analytical pipelines. Despite the widespread adoption of such systems, current applications lack the ability to understand, diagnose and correct their own errors, which as a result reduces user confidence and limits future improvements. Therefore, the problem of understanding and troubleshooting machine learning system errors is of particular interest to the community (Sculley et al. 2015). Our work studies component-based machine learning systems that are individually trained to solve specific problems and the work as a whole to solve a single complex task. We analyze how the specific characteristics of these integrated learning systems, including continuous (non-binary) performance measurements, component-based component design and non-monotonic error propagation make it difficult to assign blame to individual components. These challenges hamper future system improvements, as designers do not have an understanding of how to assign blame individual components."}, {"heading": "Background and Problem Characterization", "text": "We will now define the problem of troubleshooting component-based machine learning systems. Let's start by describing the caption system as a running example, and then proceed to formalize the problem."}, {"heading": "Case Study: An Image Captioning System", "text": "The system (Fang et al. 2015) that we use as a case study automatically generates captions as text descriptions for images. Figure 2 shows the system architecture consisting of three components of machine learning; the first and third components use revolutionary neural networks combined with multiple instance learning (Zhang, Platt and Viola 2005); the second component is a language model with maximum entropy (Berger, Pietra and Pietra 1996); the first component uses an image as input and recognizes a list of words associated with recognition results; the detector recognizes only a limited vocabulary of the 1000 most common words in captions; the speech capsule model. This component is a statistical model that generates probable word sequences as captions based on images generated by the visual image, with most image examinations matching the probability. The five components correspond to the image examiner."}, {"heading": "Problem Characterization", "text": "The system takes a set of data as system input and the individual components work together to produce a final system performance. We assume that the system architecture is provided to the system architecture by system designers by defining the system execution by specifying errors. In our methodology, we only treat a set of system components along with the components input and output data types. Problem definitions of communication dependencies between the components that characterize the input / output exchange. The totality of dependencies defines the execution of system components, but they do not allow the individual components to be compared with each other. Problem definitions of component-based machine learning systems can be decoupled from the answer to the following two questions: Question 1: How does the system fail in identifying and measuring the different types of system failures and their frequency."}, {"heading": "Human-in-the-loop Methodology", "text": "In fact, most of them will be able to play by the rules that they are able to play by."}, {"heading": "Troubleshooting the Image Captioning System", "text": "We now describe the customized crowdsourcing tasks for our case study for both system evaluation and component repair. Table 1 lists all component repairs designed specifically for this case study. Task design is an iterative process in collaboration with system designers so that human repairs can adequately simulate actionable improvements. System evaluation. The system evaluation task is designed to measure different quality metrics associated with captions, as well as overall human satisfaction. The task shows workers a picture-caption-caption pair and asks them to rate the following quality metrics: Accuracy (1-5 Likert Scale), Language (1-5 Likert Scale), Common Sense (0-1) and General Evaluation. For each measure, we have provided a detailed description along with representative examples. However, we intentionally have no worker instruction for general evaluation."}, {"heading": "Experimental Evaluation", "text": "The evaluation of the subtitling system using our methodology is based on an evaluation dataset with 1000 randomly selected images from the MSCOCO validation dataset. All experiments were conducted on Amazon Mechanical Turk. We report on the system quality based on human ratings. An additional analysis using automatic translation results can be found in the appendix."}, {"heading": "Current System State", "text": "To gain a deeper understanding of system performance, we divide the evaluation data into two sets of data: satisfactory and unsatisfactory based on the overall evaluation result from the system evaluation task. We consider each answer in 1-3 to be unsatisfactory, and each other answer in 4-5. All cases where the original picture caption has a majority agreement on being satisfactory belong to the satisfactory dataset. The rest is classified as unsatisfactory. Result: Only 57.8% of the images in the evaluation dataset have a satisfactory caption. Comparison between the satisfactory and unsatisfactory partition shows that the highest discrepancies for accuracy and detail measurements occur, highlighting the correlation of accuracy and detail with general satisfaction."}, {"heading": "Current Component State", "text": "An additional functionality of a human-assisted troubleshooting method is the evaluation of the quality of the existing individual system components. Visual Detector Figure 6 shows the accuracy and retrieval of the Visual Detector for both objects and activities compared to the human-curated lists created by aggregating the majority votes of multiple workers. In the same figure we also show the size of the lists before and after the application of human solutions. Result: The Visual Detector produces longer lists for objects than for activities, but with less precision and callback. Commonwealth fixLanguage fix Both fixes Top1-Eval. 8.0% 22.9% 25.0% Top10-Eval. 8.6% 21.7% 27.1% Top1-Val. 3.0% 15.2% 16.1% Top10-Val. 2.6% 14.2% 14.9%"}, {"heading": "Component Fixes and System Improvement", "text": "Object corrections are more effective than activity corrections for two reasons. First, the precision of the visual detector is originally significantly lower for objects than for activities (0.44No fix commonsense Language All fixes accuracy 3,674 3,698 3,696 3,712 Detail 3,563 3,583 3,590 3,602 Language 4,509 4,575 4,618 4,632 Csense. 0,957 0,973 0,974 0,982 General 3,517 3,546 3,572% Sat. 57.8% 58.5% 59.2% 59.3% Complete fix workflow. Table 7 shows the improvements from each component and the complete fix workflow that applies all components fixes sequentially. Figure 57.8% 58.5% 58.5% 59.2% 59.3% Complete fix workflow."}, {"heading": "Examples of Fix Integration", "text": "Figure 8 (a) is an example of corrections to the visual detector that lead to a satisfactory system output. In this example, workers removed the faulty object kite and added an umbrella that propagated the improvement of the final caption. In the larger dataset, successful multiplications of individual component corrections to the final output are observed for corrections to activities, common sense corrections, and captions to captions. Figure 8 (b) shows an example of corrections that exhibit a limited improvement in the final caption due to the barrier of the language model. In this example, the word horse was present in both the original and the wordlist. However, none of the sentences generated by the language model could represent the situation in the image, as it did not prove probable. This example is not unique, the unsatisfactory dataset contains a few more images of the same kind that are limited by a hard-to-write situation."}, {"heading": "Discussion", "text": "In fact, it is so that most of them are able to survive themselves, and that they are able to survive themselves, \"he said.\" But it is not as if, \"he said.\" But it is as if. \"\" It is as if. \"\" It is as if. \"\" It is as if. \"\" It is as if. \"\" It is as if. \"\" \"It is as if.\" \"\" It is as if. \"\" \".\" \"\" It is as if. \"\" \".\" \"\" \".\" \"\" \".\" \"\" \".\" \"\". \"\" \"\". \"\". \"\". \"\" \"\". \"\" \"\". \"\" \"\". \"\" \"\" \".\" \"\" \".\" \"\" \".\" \"\". \"\". \"\" \".\" \".\" \"\". \".\" \"\". \".\" \"\". \".\" \".\". \"\" \".\". \".\". \".\" \".\". \".\" \".\" \".\". \".\" \"\". \".\" \".\". \".\" \"\". \".\". \".\". \".\". \".\" \".\". \".\". \"\". \".\". \"\" \".\" \".\". \".\". \".\". \"\". \".\". \"\". \"\". \".\" \".\". \"\". \".\" \".\". \".\" \".\". \".\". \"\". \"\". \".\" \".\". \".\" \"\". \".\" \".\". \".\" \".\". \".\". \".\". \".\". \".\" \".\". \".\". \"\". \".\". \"\" \".\". \"\". \"\". \".\" \"\". \"\". \"\" \"\". \".\" \"\". \".\". \"\". \"\". \".\". \".\". \".\". \".\" \"\". \"\" \".\". \".\" \".\". \".\". \".\". \".\". \".\". \".\". \".\". \".\""}, {"heading": "Related Work", "text": "Crowdsourcing's contribution to machine learning is largely limited to the creation of offline data sets for learning (e.g., (Lin et al. 2014; Sheng, Provost, and Ipeirotis 2008), with limited interest in active human participation in the development of machine learning algorithms (Cheng and Bernstein 2015; Zou, Chaudhuri, and Kalai 2015; Chang, Kittur, and Hahn 2016). However, there is limited work to understand and diagnose errors in such systems. To debug a single classifier, researchers have developed techniques for a domain expert to interact with the machine learning process (Chakarov et al. 2016; Kulesza et al. 2010; Attenberg, Ipeirotis and Provost 2011). Our work contributes to this line of literature by examining the diagnosis of component-based systems rather than using individual predictable components."}, {"heading": "Conclusion and Future Work", "text": "The proposed methodology highlights the benefits of deeper integration of crowd input in troubleshooting and improving these integrative systems. Future work policies include exploring models that learn from the log data of our methodology to predict which fixes are most likely to improve system quality for a given input. Such models can enable algorithms to query human fixes during system execution to improve system performance. Finally, there is the possibility to develop generalized troubleshooting pipelines for machine learning systems with reusable crowd-sourced task templates. Such a pipeline would provide valuable insights into system development and create a feedback loop to support continuous improvement."}, {"heading": "Acknowledgements", "text": "This research was initiated during an internship by Besmira Nushi at Microsoft Research and partially supported by the Swiss National Science Foundation. The authors would like to thank Xiadong He, Margaret Mitchell and Larry Zitcnik for their support in understanding and modifying the execution of the captioning system and Paul Koch for his help in setting up the necessary project infrastructure. We would like to thank Dan Bohus for his valuable insights into current problems of integrative systems and Jaime Teevan for helpful discussions on this work."}, {"heading": "A. Experimental evaluation: automatic scores", "text": "The automatic results for the evaluation of caption systems are adjusted by automatic machine translation results, where the automatically translated text is compared with human-made translations. Similarly, the caption compares an automatic caption with the five captions retrieved by crowdsourced workers in the MSCOCO datasets. While this rating is generally cheaper than asking people directly to report their satisfaction, it does not always correlate well with human satisfaction. More specifically, studies show that what people like does not necessarily resemble what people generate (Vedantam, Lawrence Zitnick, and Parikh 2015), these phenomena happen because a picture description can be expressed in many ways by different people. However, the design of good automatic results for caption (and machine learning tasks in general) is an active field of research (Yang et al. 2011; Vedantam, Lawrence Zitnick, and Er Papliiker 2015; Er expose)."}, {"heading": "B. Quality control and methodology cost", "text": "For all crowdsourcing experiments, we have applied the following quality control techniques: Worker training - Providing accurate instructions to crowdsourcing workers is a critical aspect in applying our methodology to new machine learning systems. We have paid particular attention to the task at hand, repeating the design of the user interface by providing several examples of correct solutions and giving workers online feedback on their work quality. Detailed task instructions are necessary for workers to understand the purpose of the system and the role of their solutions. We have also noticed that workers become more engaged when they understand how their work contributes to improving the application presented. Spam detection - Due to the multimodal nature of the case study, the amount of crowdsourcing tasks we use in our methodology is rich and complex both in terms of content (e.g. images, words, sentences) and design."}], "references": [{"title": "F", "author": ["J. Attenberg", "P.G. Ipeirotis", "Provost"], "venue": "J.", "citeRegEx": "Attenberg. Ipeirotis. and Provost 2011", "shortCiteRegEx": null, "year": 2011}, {"title": "and Lavie", "author": ["S. Banerjee"], "venue": "A.", "citeRegEx": "Banerjee and Lavie 2005", "shortCiteRegEx": null, "year": 2005}, {"title": "S", "author": ["A.L. Berger", "V.J.D. Pietra", "Pietra"], "venue": "A. D.", "citeRegEx": "Berger. Pietra. and Pietra 1996", "shortCiteRegEx": null, "year": 1996}, {"title": "and Heckerman", "author": ["J.S. Breese"], "venue": "D.", "citeRegEx": "Breese and Heckerman 1996", "shortCiteRegEx": null, "year": 1996}, {"title": "Debugging machine learning", "author": ["Chakarov"], "venue": null, "citeRegEx": "Chakarov,? \\Q2016\\E", "shortCiteRegEx": "Chakarov", "year": 2016}, {"title": "J", "author": ["Chang"], "venue": "C.; Kittur, A.; and Hahn, N.", "citeRegEx": "Chang. Kittur. and Hahn 2016", "shortCiteRegEx": null, "year": 2016}, {"title": "M", "author": ["J. Cheng", "Bernstein"], "venue": "S.", "citeRegEx": "Cheng and Bernstein 2015", "shortCiteRegEx": null, "year": 2015}, {"title": "J", "author": ["H. Fang", "S. Gupta", "F. Iandola", "R.K. Srivastava", "L. Deng", "P. Doll\u00e1r", "J. Gao", "X. He", "M. Mitchell", "Platt"], "venue": "C.; et al.", "citeRegEx": "Fang et al. 2015", "shortCiteRegEx": null, "year": 2015}, {"title": "Explanatory debugging: Supporting end-user debugging of machine-learned programs", "author": ["Kulesza"], "venue": "In VL/HCC,", "citeRegEx": "Kulesza,? \\Q2010\\E", "shortCiteRegEx": "Kulesza", "year": 2010}, {"title": "2014", "author": ["T.-Y. Lin", "M. Maire", "S. Belongie", "J. Hays", "P. Perona", "D. Ramanan", "P. Doll\u00e1r", "C. L Zitnick"], "venue": "Microsoft coco: Common objects in context. In ECCV", "citeRegEx": "Lin et al. 2014", "shortCiteRegEx": null, "year": 2014}, {"title": "Rouge: A package for automatic evaluation of summaries. In Text summarization branches out", "author": ["Lin", "C.-Y"], "venue": "Proceedings of the ACL-04 workshop,", "citeRegEx": "Lin and C..Y.,? \\Q2004\\E", "shortCiteRegEx": "Lin and C..Y.", "year": 2004}, {"title": "Bleu: a method for automatic evaluation of machine translation", "author": ["Papineni"], "venue": "In ACL,", "citeRegEx": "Papineni,? \\Q2002\\E", "shortCiteRegEx": "Papineni", "year": 2002}, {"title": "Finding the weakest link in person detectors", "author": ["Parikh", "D. Zitnick 2011b] Parikh", "C.L. Zitnick"], "venue": "In CVPR,", "citeRegEx": "Parikh et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Parikh et al\\.", "year": 2011}, {"title": "A", "author": ["K. Patel", "N. Bancroft", "S.M. Drucker", "J. Fogarty", "Ko"], "venue": "J.; and Landay, J.", "citeRegEx": "Patel et al. 2010", "shortCiteRegEx": null, "year": 2010}, {"title": "Hidden technical debt in machine learning systems", "author": ["Sculley"], "venue": null, "citeRegEx": "Sculley,? \\Q2015\\E", "shortCiteRegEx": "Sculley", "year": 2015}, {"title": "P", "author": ["V.S. Sheng", "F. Provost", "Ipeirotis"], "venue": "G.", "citeRegEx": "Sheng. Provost. and Ipeirotis 2008", "shortCiteRegEx": null, "year": 2008}, {"title": "Cider: Consensus-based image description evaluation", "author": ["Lawrence Zitnick Vedantam", "R. Parikh 2015] Vedantam", "C. Lawrence Zitnick", "D. Parikh"], "venue": null, "citeRegEx": "Vedantam et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Vedantam et al\\.", "year": 2015}, {"title": "C", "author": ["Yang, Y.", "Teo"], "venue": "L.; Daum\u00e9 III, H.; and Aloimonos, Y.", "citeRegEx": "Yang et al. 2011", "shortCiteRegEx": null, "year": 2011}, {"title": "J", "author": ["L. Yao", "N. Ballas", "K. Cho", "Smith"], "venue": "R.; and Bengio, Y.", "citeRegEx": "Yao et al. 2015", "shortCiteRegEx": null, "year": 2015}, {"title": "Semantic parsing via staged query graph generation: Question answering with knowledge base", "author": ["Yih"], "venue": null, "citeRegEx": "Yih,? \\Q2015\\E", "shortCiteRegEx": "Yih", "year": 2015}, {"title": "P", "author": ["C. Zhang", "J.C. Platt", "Viola"], "venue": "A.", "citeRegEx": "Zhang. Platt. and Viola 2005", "shortCiteRegEx": null, "year": 2005}, {"title": "A", "author": ["J.Y. Zou", "K. Chaudhuri", "Kalai"], "venue": "T.", "citeRegEx": "Zou. Chaudhuri. and Kalai 2015", "shortCiteRegEx": null, "year": 2015}, {"title": "designing good automatic scores for image captioning (and machine learning tasks in general) is an active research area (Yang et al", "author": ["Vedantam", "Lawrence Zitnick", "Parikh", "Banerjee", "Lavie", "Papineni"], "venue": null, "citeRegEx": "Zitnick. et al\\.,? \\Q2002\\E", "shortCiteRegEx": "Zitnick. et al\\.", "year": 2002}], "referenceMentions": [], "year": 2016, "abstractText": "We study the problem of troubleshooting machine learning systems that rely on analytical pipelines of distinct components. Understanding and fixing errors that arise in such integrative systems is difficult as failures can occur at multiple points in the execution workflow. Moreover, errors can propagate, become amplified or be suppressed, making blame assignment difficult. We propose a human-in-the-loop methodology which leverages human intellect for troubleshooting system failures. The approach simulates potential component fixes through human computation tasks and measures the expected improvements in the holistic behavior of the system. The method provides guidance to designers about how they can best improve the system. We demonstrate the effectiveness of the approach on an automated image captioning system that has been pressed into real-world use.", "creator": "LaTeX with hyperref package"}}}