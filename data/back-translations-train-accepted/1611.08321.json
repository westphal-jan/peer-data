{"id": "1611.08321", "review": {"conference": "NIPS", "VERSION": "v1", "DATE_OF_SUBMISSION": "24-Nov-2016", "title": "Training and Evaluating Multimodal Word Embeddings with Large-scale Web Annotated Images", "abstract": "In this paper, we focus on training and evaluating effective word embeddings with both text and visual information. More specifically, we introduce a large-scale dataset with 300 million sentences describing over 40 million images crawled and downloaded from publicly available Pins (i.e. an image with sentence descriptions uploaded by users) on Pinterest. This dataset is more than 200 times larger than MS COCO, the standard large-scale image dataset with sentence descriptions. In addition, we construct an evaluation dataset to directly assess the effectiveness of word embeddings in terms of finding semantically similar or related words and phrases. The word/phrase pairs in this evaluation dataset are collected from the click data with millions of users in an image search system, thus contain rich semantic relationships. Based on these datasets, we propose and compare several Recurrent Neural Networks (RNNs) based multimodal (text and image) models. Experiments show that our model benefits from incorporating the visual information into the word embeddings, and a weight sharing strategy is crucial for learning such multimodal embeddings. The project page is:", "histories": [["v1", "Thu, 24 Nov 2016 23:15:56 GMT  (1378kb,D)", "http://arxiv.org/abs/1611.08321v1", "Appears in NIPS 2016. The datasets introduced in this work will be gradually released on the project page"]], "COMMENTS": "Appears in NIPS 2016. The datasets introduced in this work will be gradually released on the project page", "reviews": [], "SUBJECTS": "cs.LG cs.CL cs.CV", "authors": ["junhua mao", "jiajing xu", "kevin jing", "alan l yuille"], "accepted": true, "id": "1611.08321"}, "pdf": {"name": "1611.08321.pdf", "metadata": {"source": "CRF", "title": "Training and Evaluating Multimodal Word Embeddings with Large-scale Web Annotated Images", "authors": ["Junhua Mao", "Jiajing Xu", "Yushi Jing", "Alan Yuille"], "emails": ["mjhustc@ucla.edu,", "jiajing@pinterest.com,", "jing@pinterest.com,", "alan.l.yuille@gmail.com"], "sections": [{"heading": "1 Introduction", "text": "This year, it has come to the point that it has never come as far as it has this year."}, {"heading": "2 Related Work", "text": "In fact, most of them are able to survive on their own."}, {"heading": "3 Datasets", "text": "We have constructed two sets of data: one for training our multimodal word embedding (see Section 3.1) and another for evaluating the learned word embedding (see Section 3.2).3.1 Training DatasetTable 1: Scale comparison with other image descriptions benchmarks.Image SentencesFlickr8K [15] 8K 40K Flickr30K [37] 30K 150K IAPR-TC12 [12] 20K 34K MS COCO [22] 200K 1M Im2Text [28] 1M Pinterset40M 300MPinterest is one of the largest repositories of web images. Users often mark images with short descriptions and share the images (and descriptions) with others. Since a given image can be shared and tagged by several, sometimes thousands of users, many images have a very rich set of descriptions, making this data source ideal for training with text and image embedding. The millions of data we initially provide will be available to the public."}, {"heading": "3.2 Evaluation Datasets", "text": "This paper suggests using labeled phrase triplets - each triplet consists of three phrases containing phrases A, phrase B, and phrase C, with A considered to be semantically closer to B than A to C. At test time, we calculate the distance in the word embedding space between A / B and A / C, and consider a test triplet to be positive if d (A, B) < d (A, C). This relative comparative approach was commonly used to evaluate and compare different word embedding models [30]. To generate a large number of phrase triplets, we rely on user click data from Pinterest's image search system. In the end, we construct a large-scale evaluation dataset of 9.8 million triplets (see Section 3.2.1) and its cleaned gold standard version of 10,000 triplets (see Section 3.2.2)."}, {"heading": "3.2.1 The Raw Evaluation Dataset from User Clickthrough Data", "text": "It is very difficult to obtain a large number of semantically similar or related pairs of words and phrases. This is one of the challenges in constructing a large-area word / phrase similarity and relativity of the records. We address this challenge by evaluating the data from the Pinterest search system (i.e. short phrases or words describing the item). Please note that the same annotation can appear in several articles, such as \"Hair tutorial,\" and each item is composed of an image and a series of annotations."}, {"heading": "3.2.2 The Cleaned-up Gold Standard Dataset", "text": "Because the Related Query 10M raw data set is based on user-click information, it contains some noisy triplets (e.g., the positive and the base phrases are not related, or the negative phrases are strongly related to the base phrases).To create a gold standard data set, we are cleaning up the crowdsourcing platform CrowdFlower [1] to remove these inaccurate triples.An example question and choices for the crowdsourced annotators are shown in Figure 3. To help the annotators understand the meaning of the sentences, they can click on the phrases to get Google search results.The annotators must select which phrase has more to do with the base phrase, or whether they are both related or unrelated. To help the annotators understand the meaning of the sentences, they can click on the phrases to get Google search results."}, {"heading": "4 The Multimodal Word Embedding Models", "text": "We propose three RNN-CNN-based models to learn the multimodal word embedding as shown in Figure 4. (All models have two parts in common: a new model and a new model in which we can adjust the images to 224 x 224 and use the 16-layer VGGNet [32] as a visual feature.) The binary activation (i.e. 4096 binary vectors) of the layer before its SoftMax layer are used as image features and are mapped to the same space of the RNN state (Model A) or the word embedding (Model C), depends on the structure of the model, on a fully connected layer and a recited Linear Unit function (ReLU)."}, {"heading": "5 Experiments", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "5.1 Training Details", "text": "We convert the words in all sentences of the Pinterest 40M dataset into lowercase letters, removing all non-phanumeric characters. A start < bos > and an end < eos > are added at the beginning and end of each sentence. We use the stochastic gradient descend method with a mini batch size of 256 sentences and a learning rate of 1.0. The gradient is truncated to 10.0. We train the models until the loss does not decrease on a small validation set of 10,000 images and their descriptions. The models scan the dataset for about five 5 epochs. The presets of gates (i.e. br and bu in Equation 1 and 2) in the GRU layer are initialized to 1.0."}, {"heading": "5.2 Evaluation Details", "text": "We use the trained embedding models to extract embeddings for all words in a phrase and aggregate them by an average summary to obtain the phrase representation. We then check if the cosinal distance between the (base phrase, positive phrase) pair is smaller than the (base phrase, negative phrase) pair. Average accuracy for all triplets in the raw dataset Related Phrases 10M (RP10M) and the gold standard Related Phrases 10K (gold RP10K) is reported."}, {"heading": "5.3 Results on the Gold RP10K and RP10M datasets", "text": "We evaluate and compare our Model A, B, C, their variants, and multiple strong baselines on our RP10M and Gold RP10K datasets, and the results are presented in Table 3. \"Pure Text RNN\" refers to the baseline model without input of the visual features trained on Pinterest40M. It has the same model structure as our Model UM, except that we initialize the hidden state of GRU with a zero vector. \"Model A without weight distribution\" refers to a variant of Model A, in which the weight matrix of the word is embedded, not divided with the weight matrix. \"The weight matrix UM is compared by the sampled SoftMax layer with a zero vector.\" \"Word2Vec-GoogleNews\" refers to the state of the word matrix Uw of the word. \"It is not divided with the weight matrix layer, but with the weight matrix of the soft amplified.\""}, {"heading": "6 Discussion", "text": "We introduce Pinterest 40M, the largest set of images with sentence descriptions to the best of our knowledge, and construct two sets of assessment data (i.e. RP10M and Gold RP10K) for word / phrase similarity and relationship evaluation. Based on these data sets, we propose several CNN RNN-based multimodal models to learn effective word embeddings. Experiments show that visual information significantly supports word embeddings, and our proposed model successfully integrates such information into the learned embeddings. There are many possible extensions to the proposed model and data set. For example, we plan to separate semantically similar or related sentence pairs from the Gold RP10K dataset to better understand the performance of the methods, similar to [3]. We will also forego similarity or similarity values for the pairs."}], "references": [{"title": "A study on similarity and relatedness using distributional and wordnet-based approaches", "author": ["E. Agirre", "E. Alfonseca", "K. Hall", "J. Kravalova", "M. Pa\u015fca", "A. Soroa"], "venue": "NAACL HLT, pages 19\u201327", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2009}, {"title": "Neural probabilistic language models", "author": ["Y. Bengio", "H. Schwenk", "J.-S. Sen\u00e9cal", "F. Morin", "J.-L. Gauvain"], "venue": "Innovations in Machine Learning, pages 137\u2013186. Springer", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2006}, {"title": "Multimodal distributional semantics", "author": ["E. Bruni", "N.-K. Tran", "M. Baroni"], "venue": "JAIR, 49(1-47)", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2014}, {"title": "Learning a recurrent visual representation for image caption generation", "author": ["X. Chen", "C.L. Zitnick"], "venue": "arXiv preprint arXiv:1411.5654", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2014}, {"title": "Learning phrase representations using rnn encoder-decoder for statistical machine translation", "author": ["K. Cho", "B. Van Merri\u00ebnboer", "C. Gulcehre", "D. Bahdanau", "F. Bougares", "H. Schwenk", "Y. Bengio"], "venue": "arXiv preprint arXiv:1406.1078", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2014}, {"title": "On using very large target vocabulary for neural machine translation", "author": ["S.J.K. Cho", "R. Memisevic", "Y. Bengio"], "venue": "ACL", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2015}, {"title": "Long-term recurrent convolutional networks for visual recognition and description", "author": ["J. Donahue", "L. Anne Hendricks", "S. Guadarrama", "M. Rohrbach", "S. Venugopalan", "K. Saenko", "T. Darrell"], "venue": "CVPR", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2015}, {"title": "Finding structure in time", "author": ["J.L. Elman"], "venue": "Cognitive science, 14(2):179\u2013211", "citeRegEx": "10", "shortCiteRegEx": null, "year": 1990}, {"title": "Placing search in context: The concept revisited", "author": ["L. Finkelstein", "E. Gabrilovich", "Y. Matias", "E. Rivlin", "Z. Solan", "G. Wolfman", "E. Ruppin"], "venue": "WWW, pages 406\u2013414. ACM", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2001}, {"title": "The iapr tc-12 benchmark: A new evaluation resource for visual information systems", "author": ["M. Grubinger", "P. Clough", "H. M\u00fcller", "T. Deselaers"], "venue": "International Workshop OntoImage, pages 13\u201323", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2006}, {"title": "Learning abstract concept embeddings from multi-modal data: Since you probably can\u2019t see what i mean", "author": ["F. Hill", "A. Korhonen"], "venue": "EMNLP, pages 255\u2013265. Citeseer", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2014}, {"title": "Simlex-999: Evaluating semantic models with (genuine) similarity estimation", "author": ["F. Hill", "R. Reichart", "A. Korhonen"], "venue": "Computational Linguistics", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2015}, {"title": "Framing image description as a ranking task: Data", "author": ["M. Hodosh", "P. Young", "J. Hockenmaier"], "venue": "models and evaluation metrics. Journal of Artificial Intelligence Research, pages 853\u2013899", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2013}, {"title": "Deep visual-semantic alignments for generating image descriptions", "author": ["A. Karpathy", "L. Fei-Fei"], "venue": "CVPR, pages 3128\u20133137", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2015}, {"title": "Learning image embeddings using convolutional neural networks for improved multi-modal semantics", "author": ["D. Kiela", "L. Bottou"], "venue": "EMNLP, pages 36\u201345. Citeseer", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2014}, {"title": "Unifying visual-semantic embeddings with multimodal neural language models", "author": ["R. Kiros", "R. Salakhutdinov", "R.S. Zemel"], "venue": "arXiv preprint arXiv:1411.2539", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2014}, {"title": "Visual word2vec (vis-w2v): Learning visually grounded word embeddings using abstract scenes", "author": ["S. Kottur", "R. Vedantam", "J.M. Moura", "D. Parikh"], "venue": "arXiv preprint arXiv:1511.07067", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2015}, {"title": "Imagenet classification with deep convolutional neural networks", "author": ["A. Krizhevsky", "I. Sutskever", "G.E. Hinton"], "venue": "NIPS, pages 1097\u20131105", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2012}, {"title": "Combining language and vision with a multimodal skip-gram model", "author": ["A. Lazaridou", "N.T. Pham", "M. Baroni"], "venue": "arXiv preprint arXiv:1501.02598", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2015}, {"title": "Microsoft coco: Common objects in context", "author": ["T.-Y. Lin", "M. Maire", "S. Belongie", "J. Hays", "P. Perona", "D. Ramanan", "P. Doll\u00e1r", "C.L. Zitnick"], "venue": "ECCV, pages 740\u2013755. Springer", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2014}, {"title": "Learning like a child: Fast novel visual concept learning from sentence descriptions of images", "author": ["J. Mao", "X. Wei", "Y. Yang", "J. Wang", "Z. Huang", "A.L. Yuille"], "venue": "ICCV, pages 2533\u20132541", "citeRegEx": "23", "shortCiteRegEx": null, "year": 2015}, {"title": "Deep captioning with multimodal recurrent neural networks (m-rnn)", "author": ["J. Mao", "W. Xu", "Y. Yang", "J. Wang", "Z. Huang", "A. Yuille"], "venue": "ICLR", "citeRegEx": "24", "shortCiteRegEx": null, "year": 2015}, {"title": "Distributed representations of words and phrases and their compositionality", "author": ["T. Mikolov", "I. Sutskever", "K. Chen", "G.S. Corrado", "J. Dean"], "venue": "NIPS, pages 3111\u20133119", "citeRegEx": "25", "shortCiteRegEx": null, "year": 2013}, {"title": "Rectified linear units improve restricted boltzmann machines", "author": ["V. Nair", "G.E. Hinton"], "venue": "Proceedings of the 27th International Conference on Machine Learning (ICML-10), pages 807\u2013814", "citeRegEx": "26", "shortCiteRegEx": null, "year": 2010}, {"title": "The university of south florida free association", "author": ["D.L. Nelson", "C.L. McEvoy", "T.A. Schreiber"], "venue": "rhyme, and word fragment norms. Behavior Research Methods, Instruments, & Computers, 36(3):402\u2013407", "citeRegEx": "27", "shortCiteRegEx": null, "year": 2004}, {"title": "Im2text: Describing images using 1 million captioned photographs", "author": ["V. Ordonez", "G. Kulkarni", "T.L. Berg"], "venue": "NIPS, pages 1143\u20131151", "citeRegEx": "28", "shortCiteRegEx": null, "year": 2011}, {"title": "Glove: Global vectors for word representation", "author": ["J. Pennington", "R. Socher", "C.D. Manning"], "venue": "EMNLP, volume 14, pages 1532\u201343", "citeRegEx": "29", "shortCiteRegEx": null, "year": 2014}, {"title": "Evaluation methods for unsupervised word embeddings", "author": ["T. Schnabel", "I. Labutov", "D. Mimno", "T. Joachims"], "venue": "EMNLP, pages 298\u2013307", "citeRegEx": "30", "shortCiteRegEx": null, "year": 2015}, {"title": "Learning grounded meaning representations with autoencoders", "author": ["C. Silberer", "M. Lapata"], "venue": "ACL, pages 721\u2013732", "citeRegEx": "31", "shortCiteRegEx": null, "year": 2014}, {"title": "Very deep convolutional networks for large-scale image recognition", "author": ["K. Simonyan", "A. Zisserman"], "venue": "ICLR", "citeRegEx": "32", "shortCiteRegEx": null, "year": 2015}, {"title": "Sequence to sequence learning with neural networks", "author": ["I. Sutskever", "O. Vinyals", "Q.V. Le"], "venue": "NIPS, pages 3104\u20133112", "citeRegEx": "33", "shortCiteRegEx": null, "year": 2014}, {"title": "Yfcc100m: The new data in multimedia research", "author": ["B. Thomee", "D.A. Shamma", "G. Friedland", "B. Elizalde", "K. Ni", "D. Poland", "D. Borth", "L.-J. Li"], "venue": "Communications of the ACM, 59(2):64\u201373", "citeRegEx": "34", "shortCiteRegEx": null, "year": 2016}, {"title": "Visualizing data using t-sne", "author": ["L. Van der Maaten", "G. Hinton"], "venue": "JMLR, 9(2579-2605):85,", "citeRegEx": "35", "shortCiteRegEx": "35", "year": 2008}, {"title": "Show and tell: A neural image caption generator", "author": ["O. Vinyals", "A. Toshev", "S. Bengio", "D. Erhan"], "venue": "CVPR, pages 3156\u20133164", "citeRegEx": "36", "shortCiteRegEx": null, "year": 2015}, {"title": "From image descriptions to visual denotations: New similarity metrics for semantic inference over event descriptions", "author": ["P. Young", "A. Lai", "M. Hodosh", "J. Hockenmaier"], "venue": "ACL, pages 479\u2013488", "citeRegEx": "37", "shortCiteRegEx": null, "year": 2014}], "referenceMentions": [{"referenceID": 19, "context": "This dataset is more than 200 times larger than MS COCO [22], the standard large-scale image dataset with sentence descriptions.", "startOffset": 56, "endOffset": 60}, {"referenceID": 10, "context": "[13]), or the visual contraints are only applied to limited number of pre-defined visual concepts (e.", "startOffset": 0, "endOffset": 4}, {"referenceID": 18, "context": "[21]).", "startOffset": 0, "endOffset": 4}, {"referenceID": 19, "context": "the image benchmark with sentences descriptions [22]), our dataset is much larger (40 million images with 300 million sentences compared to 0.", "startOffset": 48, "endOffset": 52}, {"referenceID": 2, "context": "[5, 14, 11]) for word similarity or relatedness contain only less than a thousand word pairs and cannot comprehensively evaluate all the embeddings of the words appearing in the training set.", "startOffset": 0, "endOffset": 11}, {"referenceID": 11, "context": "[5, 14, 11]) for word similarity or relatedness contain only less than a thousand word pairs and cannot comprehensively evaluate all the embeddings of the words appearing in the training set.", "startOffset": 0, "endOffset": 11}, {"referenceID": 8, "context": "[5, 14, 11]) for word similarity or relatedness contain only less than a thousand word pairs and cannot comprehensively evaluate all the embeddings of the words appearing in the training set.", "startOffset": 0, "endOffset": 11}, {"referenceID": 7, "context": "Equipped with these datasets, we propose, train and evaluate several Recurrent Neural Network (RNN [10]) based models with input of both text descriptions and images.", "startOffset": 99, "endOffset": 103}, {"referenceID": 10, "context": "[13, 21]).", "startOffset": 0, "endOffset": 8}, {"referenceID": 18, "context": "[13, 21]).", "startOffset": 0, "endOffset": 8}, {"referenceID": 6, "context": "The best performing model is inspired by recent image captioning models [9, 24, 36], with the additional weight-sharing strategy originally proposed in [23] to learn novel visual concepts.", "startOffset": 72, "endOffset": 83}, {"referenceID": 21, "context": "The best performing model is inspired by recent image captioning models [9, 24, 36], with the additional weight-sharing strategy originally proposed in [23] to learn novel visual concepts.", "startOffset": 72, "endOffset": 83}, {"referenceID": 33, "context": "The best performing model is inspired by recent image captioning models [9, 24, 36], with the additional weight-sharing strategy originally proposed in [23] to learn novel visual concepts.", "startOffset": 72, "endOffset": 83}, {"referenceID": 20, "context": "The best performing model is inspired by recent image captioning models [9, 24, 36], with the additional weight-sharing strategy originally proposed in [23] to learn novel visual concepts.", "startOffset": 152, "endOffset": 156}, {"referenceID": 12, "context": "Image-Sentence Description Datasets The image descriptions datasets, such as Flickr8K [15], Flickr30K [37], IAPR-TC12 [12], and MS COCO [22], greatly facilitated the development of models for language and vision tasks such as image captioning.", "startOffset": 86, "endOffset": 90}, {"referenceID": 34, "context": "Image-Sentence Description Datasets The image descriptions datasets, such as Flickr8K [15], Flickr30K [37], IAPR-TC12 [12], and MS COCO [22], greatly facilitated the development of models for language and vision tasks such as image captioning.", "startOffset": 102, "endOffset": 106}, {"referenceID": 9, "context": "Image-Sentence Description Datasets The image descriptions datasets, such as Flickr8K [15], Flickr30K [37], IAPR-TC12 [12], and MS COCO [22], greatly facilitated the development of models for language and vision tasks such as image captioning.", "startOffset": 118, "endOffset": 122}, {"referenceID": 19, "context": "Image-Sentence Description Datasets The image descriptions datasets, such as Flickr8K [15], Flickr30K [37], IAPR-TC12 [12], and MS COCO [22], greatly facilitated the development of models for language and vision tasks such as image captioning.", "startOffset": 136, "endOffset": 140}, {"referenceID": 25, "context": "The Im2Text dataset proposed in [28] adopts a similar data collection process to ours by using 1 million images with 1 million user annotated captions from Flickr.", "startOffset": 32, "endOffset": 36}, {"referenceID": 31, "context": "Recently, [34] proposed and released the YFCC100M dataset, which is a large-scale multimedia dataset contains metadata of 100 million Flickr images.", "startOffset": 10, "endOffset": 14}, {"referenceID": 8, "context": "Word Similarity-Relatedness Evaluation The standard benchmarks, such as WordSim-353/WSSim [11, 3], MEN [5], and SimLex-999 [14], consist of a couple hundreds of word pairs and their similarity or relatedness scores.", "startOffset": 90, "endOffset": 97}, {"referenceID": 0, "context": "Word Similarity-Relatedness Evaluation The standard benchmarks, such as WordSim-353/WSSim [11, 3], MEN [5], and SimLex-999 [14], consist of a couple hundreds of word pairs and their similarity or relatedness scores.", "startOffset": 90, "endOffset": 97}, {"referenceID": 2, "context": "Word Similarity-Relatedness Evaluation The standard benchmarks, such as WordSim-353/WSSim [11, 3], MEN [5], and SimLex-999 [14], consist of a couple hundreds of word pairs and their similarity or relatedness scores.", "startOffset": 103, "endOffset": 106}, {"referenceID": 11, "context": "Word Similarity-Relatedness Evaluation The standard benchmarks, such as WordSim-353/WSSim [11, 3], MEN [5], and SimLex-999 [14], consist of a couple hundreds of word pairs and their similarity or relatedness scores.", "startOffset": 123, "endOffset": 127}, {"referenceID": 24, "context": "[27, 11]), or by randomly selecting frequent words in large text corpus and manually searching for useful pairs (e.", "startOffset": 0, "endOffset": 8}, {"referenceID": 8, "context": "[27, 11]), or by randomly selecting frequent words in large text corpus and manually searching for useful pairs (e.", "startOffset": 0, "endOffset": 8}, {"referenceID": 2, "context": "[5]).", "startOffset": 0, "endOffset": 3}, {"referenceID": 22, "context": "Another related evaluation is the analogy task proposed in [25].", "startOffset": 59, "endOffset": 63}, {"referenceID": 6, "context": "RNN for Language and Vision Our models are inspired by recent RNN-CNN based image captioning models [9, 24, 36, 16, 6, 18, 23], which can be viewed as a special case of the sequence-tosequence learning framework [33, 7].", "startOffset": 100, "endOffset": 126}, {"referenceID": 21, "context": "RNN for Language and Vision Our models are inspired by recent RNN-CNN based image captioning models [9, 24, 36, 16, 6, 18, 23], which can be viewed as a special case of the sequence-tosequence learning framework [33, 7].", "startOffset": 100, "endOffset": 126}, {"referenceID": 33, "context": "RNN for Language and Vision Our models are inspired by recent RNN-CNN based image captioning models [9, 24, 36, 16, 6, 18, 23], which can be viewed as a special case of the sequence-tosequence learning framework [33, 7].", "startOffset": 100, "endOffset": 126}, {"referenceID": 13, "context": "RNN for Language and Vision Our models are inspired by recent RNN-CNN based image captioning models [9, 24, 36, 16, 6, 18, 23], which can be viewed as a special case of the sequence-tosequence learning framework [33, 7].", "startOffset": 100, "endOffset": 126}, {"referenceID": 3, "context": "RNN for Language and Vision Our models are inspired by recent RNN-CNN based image captioning models [9, 24, 36, 16, 6, 18, 23], which can be viewed as a special case of the sequence-tosequence learning framework [33, 7].", "startOffset": 100, "endOffset": 126}, {"referenceID": 15, "context": "RNN for Language and Vision Our models are inspired by recent RNN-CNN based image captioning models [9, 24, 36, 16, 6, 18, 23], which can be viewed as a special case of the sequence-tosequence learning framework [33, 7].", "startOffset": 100, "endOffset": 126}, {"referenceID": 20, "context": "RNN for Language and Vision Our models are inspired by recent RNN-CNN based image captioning models [9, 24, 36, 16, 6, 18, 23], which can be viewed as a special case of the sequence-tosequence learning framework [33, 7].", "startOffset": 100, "endOffset": 126}, {"referenceID": 30, "context": "RNN for Language and Vision Our models are inspired by recent RNN-CNN based image captioning models [9, 24, 36, 16, 6, 18, 23], which can be viewed as a special case of the sequence-tosequence learning framework [33, 7].", "startOffset": 212, "endOffset": 219}, {"referenceID": 4, "context": "RNN for Language and Vision Our models are inspired by recent RNN-CNN based image captioning models [9, 24, 36, 16, 6, 18, 23], which can be viewed as a special case of the sequence-tosequence learning framework [33, 7].", "startOffset": 212, "endOffset": 219}, {"referenceID": 4, "context": "We adopt Gated Recurrent Units (GRUs [7]), a variation of the simple RNN model.", "startOffset": 37, "endOffset": 40}, {"referenceID": 1, "context": "the continuous bag-of-word model [4]) or to predict the context words given the current word (i.", "startOffset": 33, "endOffset": 36}, {"referenceID": 22, "context": "the skip-gram model [25]).", "startOffset": 20, "endOffset": 24}, {"referenceID": 2, "context": "One type of methods takes a two-step strategy that first extracts text and image features separately and then fuses them together using singular value decomposition [5], stacked autoencoders [31], or even simple concatenation [17].", "startOffset": 165, "endOffset": 168}, {"referenceID": 28, "context": "One type of methods takes a two-step strategy that first extracts text and image features separately and then fuses them together using singular value decomposition [5], stacked autoencoders [31], or even simple concatenation [17].", "startOffset": 191, "endOffset": 195}, {"referenceID": 14, "context": "One type of methods takes a two-step strategy that first extracts text and image features separately and then fuses them together using singular value decomposition [5], stacked autoencoders [31], or even simple concatenation [17].", "startOffset": 226, "endOffset": 230}, {"referenceID": 10, "context": "[13, 21, 19] learn the text and image features jointly by fusing visual or perceptual information in a skip-gram model [25].", "startOffset": 0, "endOffset": 12}, {"referenceID": 18, "context": "[13, 21, 19] learn the text and image features jointly by fusing visual or perceptual information in a skip-gram model [25].", "startOffset": 0, "endOffset": 12}, {"referenceID": 16, "context": "[13, 21, 19] learn the text and image features jointly by fusing visual or perceptual information in a skip-gram model [25].", "startOffset": 0, "endOffset": 12}, {"referenceID": 22, "context": "[13, 21, 19] learn the text and image features jointly by fusing visual or perceptual information in a skip-gram model [25].", "startOffset": 119, "endOffset": 123}, {"referenceID": 18, "context": "[21]) or perception domains (e.", "startOffset": 0, "endOffset": 4}, {"referenceID": 11, "context": "[14]) in the sentences, or focus on abstract scenes (e.", "startOffset": 0, "endOffset": 4}, {"referenceID": 16, "context": "[19]).", "startOffset": 0, "endOffset": 4}, {"referenceID": 12, "context": "Image Sentences Flickr8K [15] 8K 40K Flickr30K [37] 30K 150K IAPR-TC12 [12] 20K 34K MS COCO [22] 200K 1M Im2Text [28] 1M 1M Pinterset40M 40M 300M Pinterest is one of the largest repository of Web images.", "startOffset": 25, "endOffset": 29}, {"referenceID": 34, "context": "Image Sentences Flickr8K [15] 8K 40K Flickr30K [37] 30K 150K IAPR-TC12 [12] 20K 34K MS COCO [22] 200K 1M Im2Text [28] 1M 1M Pinterset40M 40M 300M Pinterest is one of the largest repository of Web images.", "startOffset": 47, "endOffset": 51}, {"referenceID": 9, "context": "Image Sentences Flickr8K [15] 8K 40K Flickr30K [37] 30K 150K IAPR-TC12 [12] 20K 34K MS COCO [22] 200K 1M Im2Text [28] 1M 1M Pinterset40M 40M 300M Pinterest is one of the largest repository of Web images.", "startOffset": 71, "endOffset": 75}, {"referenceID": 19, "context": "Image Sentences Flickr8K [15] 8K 40K Flickr30K [37] 30K 150K IAPR-TC12 [12] 20K 34K MS COCO [22] 200K 1M Im2Text [28] 1M 1M Pinterset40M 40M 300M Pinterest is one of the largest repository of Web images.", "startOffset": 92, "endOffset": 96}, {"referenceID": 25, "context": "Image Sentences Flickr8K [15] 8K 40K Flickr30K [37] 30K 150K IAPR-TC12 [12] 20K 34K MS COCO [22] 200K 1M Im2Text [28] 1M 1M Pinterset40M 40M 300M Pinterest is one of the largest repository of Web images.", "startOffset": 113, "endOffset": 117}, {"referenceID": 27, "context": "This relative comparison approach was commonly used to evaluate and compare different word embedding models [30].", "startOffset": 108, "endOffset": 112}, {"referenceID": 8, "context": "[11, 14]) manually annotated each word pair with an absolute score reflecting how much the words in this pair are semantically related.", "startOffset": 0, "endOffset": 8}, {"referenceID": 11, "context": "[11, 14]) manually annotated each word pair with an absolute score reflecting how much the words in this pair are semantically related.", "startOffset": 0, "endOffset": 8}, {"referenceID": 17, "context": "All of the models have two parts in common: a Convolutional Neural Network (CNN [20]) to extract visual representations and a Recurrent Neural Network (RNN [10]) to model sentences.", "startOffset": 80, "endOffset": 84}, {"referenceID": 7, "context": "All of the models have two parts in common: a Convolutional Neural Network (CNN [20]) to extract visual representations and a Recurrent Neural Network (RNN [10]) to model sentences.", "startOffset": 156, "endOffset": 160}, {"referenceID": 29, "context": "For the CNN part, we resize the images to 224\u00d7 224, and adopt the 16-layer VGGNet [32] as the visual feature extractor.", "startOffset": 82, "endOffset": 86}, {"referenceID": 23, "context": "4096 binary vectors) of the layer before its SoftMax layer are used as the image features and will be mapped to the same space of the state of RNN (Model A, B) or the word embeddings (Model C), depends on the structure of the model, by a fully connected layer and a Rectified Linear Unit function (ReLU [26], ReLU(x) = max(0, x)).", "startOffset": 303, "endOffset": 307}, {"referenceID": 4, "context": "For the RNN part, we use a Gated Recurrent Unit (GRU [7]), an recently very popular RNN structure, with a 512 dimensional state cell.", "startOffset": 53, "endOffset": 56}, {"referenceID": 5, "context": "Because the vocabulary size is very huge, we adopt the sampled SoftMax loss [8] to accelerate the training.", "startOffset": 76, "endOffset": 79}, {"referenceID": 33, "context": "Model A is inspired by the CNN-RNN based image captioning models [36, 23].", "startOffset": 65, "endOffset": 73}, {"referenceID": 20, "context": "Model A is inspired by the CNN-RNN based image captioning models [36, 23].", "startOffset": 65, "endOffset": 73}, {"referenceID": 22, "context": "687 128 Word2Vec-GoogleNews [25] 0.", "startOffset": 28, "endOffset": 32}, {"referenceID": 26, "context": "596 300 GloVe-Twitter [29] 0.", "startOffset": 22, "endOffset": 26}, {"referenceID": 20, "context": "sharing strategy proposed in [23] that was originally used to enhance the models\u2019 ability to learn novel visual concepts.", "startOffset": 29, "endOffset": 33}, {"referenceID": 22, "context": "2 \u201cWord2Vec-GoogleNews\u201d denotes the state-of-the-art off-the-shelf word embedding models of Word2Vec [25] trained on the Google-News data (about 300 billion words).", "startOffset": 101, "endOffset": 105}, {"referenceID": 26, "context": "\u201cGloVe-Twitter\u201d denotes the GloVe model [29] trained on the Twitter data (about 27 billion words).", "startOffset": 40, "endOffset": 44}, {"referenceID": 22, "context": "\u2022 All the models trained on the Pinterest40M dataset performs better than the skip-gram model [25] trained on a much larger dataset of 300 billion words.", "startOffset": 94, "endOffset": 98}, {"referenceID": 0, "context": ", we plan to separate semantically similar or related phrase pairs from the Gold RP10K dataset to better understand the performance of the methods, similar to [3].", "startOffset": 159, "endOffset": 162}, {"referenceID": 2, "context": "[5, 11]).", "startOffset": 0, "endOffset": 7}, {"referenceID": 8, "context": "[5, 11]).", "startOffset": 0, "endOffset": 7}, {"referenceID": 32, "context": "Figure 5: t-SNE [35] visualization of the 500 most frequent words learned by our Model A.", "startOffset": 16, "endOffset": 20}], "year": 2016, "abstractText": "In this paper, we focus on training and evaluating effective word embeddings with both text and visual information. More specifically, we introduce a large-scale dataset with 300 million sentences describing over 40 million images crawled and downloaded from publicly available Pins (i.e. an image with sentence descriptions uploaded by users) on Pinterest [2]. This dataset is more than 200 times larger than MS COCO [22], the standard large-scale image dataset with sentence descriptions. In addition, we construct an evaluation dataset to directly assess the effectiveness of word embeddings in terms of finding semantically similar or related words and phrases. The word/phrase pairs in this evaluation dataset are collected from the click data with millions of users in an image search system, thus contain rich semantic relationships. Based on these datasets, we propose and compare several Recurrent Neural Networks (RNNs) based multimodal (text and image) models. Experiments show that our model benefits from incorporating the visual information into the word embeddings, and a weight sharing strategy is crucial for learning such multimodal embeddings. The project page is: http://www. stat.ucla.edu/~junhua.mao/multimodal_embedding.html1.", "creator": "LaTeX with hyperref package"}}}