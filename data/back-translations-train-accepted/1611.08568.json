{"id": "1611.08568", "review": {"conference": "ICML", "VERSION": "v1", "DATE_OF_SUBMISSION": "25-Nov-2016", "title": "Bottleneck Conditional Density Estimation", "abstract": "We propose a neural network framework for high-dimensional conditional density estimation. The Bottleneck Conditional Density Estimator (BCDE) is a variant of the conditional variational autoencoder (CVAE) that employs layer(s) of stochastic variables as the bottleneck between the input x and target y, where both are high-dimensional. The key to effectively train BCDEs is the hybrid blending of the conditional generative model with a joint generative model that leverages unlabeled data to regularize the conditional model. We show that the BCDE significantly outperforms the CVAE in MNIST quadrant prediction benchmarks in the fully supervised case and establishes new benchmarks in the semi-supervised case.", "histories": [["v1", "Fri, 25 Nov 2016 19:42:53 GMT  (4993kb,D)", "http://arxiv.org/abs/1611.08568v1", null], ["v2", "Mon, 27 Feb 2017 08:28:08 GMT  (6383kb,D)", "http://arxiv.org/abs/1611.08568v2", null], ["v3", "Fri, 30 Jun 2017 12:27:01 GMT  (7196kb,D)", "http://arxiv.org/abs/1611.08568v3", null]], "reviews": [], "SUBJECTS": "stat.ML cs.LG", "authors": ["rui shu", "hung hai bui", "mohammad ghavamzadeh"], "accepted": true, "id": "1611.08568"}, "pdf": {"name": "1611.08568.pdf", "metadata": {"source": "CRF", "title": "Bottleneck Conditional Density Estimation", "authors": ["Rui Shu", "Hung H. Bui", "Mohammad Ghavamzadeh"], "emails": [], "sections": [{"heading": "1 Introduction", "text": "In fact, we are able to assert ourselves, we are able to assert ourselves in the world, we are able to assert ourselves in the world, and we are able to assert ourselves in the world, we are able to assert ourselves in the world."}, {"heading": "2 Related work", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "2.1 Variational Autoencoders", "text": "The Variational Autoencoder (VAE) is a profound generative model for density estimation. It consists of a latent variable z with unit z \u0445 N (0, Ik), which in turn represents a conditionally gaussian observable vector x | z \u0445 N (p), diag (p), where \u00b5 and \u03c32 are non-linear neural networks and represent their parameters. UAE can be regarded as a non-linear generalization of probabilistic PCA [24] and can therefore restore non-linear manifolds in the data. However, the flexibility of VAE makes the posterior inference of the latent variables intractable intractable. This inference question is addressed via a recognition model q\u03c6 (z | x), which serves as amortized variative approximation x x."}, {"heading": "2.2 Conditional VAEs (CVAEs)", "text": "The conditional generative model is similar to the UAE except that the latent variable z and the generating distribution of y | z are both dependent on the input x. Thus, the conditional generative path is a pledge (z | x) = N (z | \u00b5z, \u03b8 (x), diag (\u03c32z, \u03b8 (x))), (3) y \u0445 p\u03b8 (y | x, z) = N (y | \u00b5y, \u03b8 (x, z))), diag (\u03c32y, \u03b8 (x, z)))) or Berne (y | \u00b5y, \u03b8 (x, z))) (4) where it denotes the parameters of the neural networks used in the generative path. CVAE is achieved by maximizing a lower limit of the conditional likelihoodln phabi (y | x)."}, {"heading": "3 Neural bottleneck conditional density estimation", "text": "We provide a high-level overview of our approach (Fig. 1), which consists of a new architecture and a new training method. Our new architecture imposes a bottleneck that leads to a class of conditional density estimators, which we call BCDEs. Unlike BCDE, the generative path prevents x from directly influencing y. Following the conditional training paradigm in [21], conditional / discriminatory training of the BCDE means that the lower limit of conditional probability is maximized. (5) The bottlenecks (y | x) are greater than the Eqs (z, y)."}, {"heading": "3.1 Bottleneck joint density estimation", "text": "In the BJDE, we would like to learn the common distribution of x and y. The bottleneck is introduced in the generative path via the bottleneck variable z, which refers to x and y (Figures 2a to 2c). Thus, the variable lower limit of the common probability (x, y) \u2265 Jxy (\u03b8, y) is paired. (8) To specify the parameters of the BJDE networks and reserves (z, z) for the BCDE parameters, the lower limit of the common probability (X, z) q\u03c6 (z, y). (Z, y) q\u03c6 (z, y). (8) In order to specify the parameters of the BJDE networks and reserve parameters, the BCDE parameters are paired. (For samples in which x or y is not observed, we will have to calculate the variable lower limit of the common task x. Here, the bottleneck plays a decisive role."}, {"heading": "3.2 Factored inference", "text": "When training the BJDE in the semi-monitored system, we introduce a factored q inference procedure that reduces the number of parameters in the recognition model. In the semi-monitored system, the 1-layer BJDE recognition model requires an approximation to three rear levels: p (z | x, y), p (z, y), p (z | x), p (z | z), and p (z | y) p (y | z). The standard approach would be to assign a recognition network to each rear (x, y | z), but this approach does not take advantage of the fact that these rear levels share the same probability functions, i.e. p (x, y | z) = p (x | z) p (y | z) p (z) p (z), the recognition network (z), the recognition model z)."}, {"heading": "3.3 Hybrid training", "text": "While the BJDE can be used directly to estimate the conditional density, it is not expected to provide a reasonable performance. In classification, it has been observed that a generative model trained to estimate the common distribution can provide sub-optimal performance when compared to a model that has been trained in a discriminatory manner [15]. Indeed, both [12] and [14] have included additional discriminatory training in their objective functions in order to successfully perform a semi-supervised classification with deep generative models. In order to solve this problem, a principle-based hybrid mix of common and conditional models was proposed. In other words, if the model is incorrectly specified, we should not expect the optimal common model parameters to match the optimal conditional model parameters. To solve this problem, a principle-based hybrid mix of common and conditional models was proposed. At the core of the model [13], the hybrid blending procedures of the JDP's conditional parameters are adjustable."}, {"heading": "4 Experiments", "text": "We evaluated the performance of our hybrid training method on the permutation invariant MNIST quadrant prediction task [20, 21]. The MNIST quadrant prediction task is a conditional density estimation task in which MNIST digits are partially occluded. The model is given the observed region and is evaluated by its perplexity on the occluded region. The MNIST quadrant prediction task consists of three sub-tasks depending on the degree of partial observability. 1-quadrant prediction: the lower left quadrant is observed. 2-quadrant prediction: the left side is observed. 3-quadrant prediction: the lower right quadrant is not observed. In the full supervised case, the original MNIST training set {x} 50000 i = 1 is converted into our CDE training."}, {"heading": "4.1 Conditional log-likelihood performance", "text": "Tables 2 to 4 show the performance comparisons between CVAE, hybrid BCDE, and baseline variants of Eq. (13): conditional training (C only) and naive pre-training (without Jxy). We also evaluate the performance of the BCDE models when factored inference is applied (Table 1), as well as the performance of the 2-stochastic layer BCDE models. 2-L-BCDE has the generative model p (z1: 2, y) = p\u03b8 (z1 | x) p\u03b8 (z2 | z1) p\u03b8 (y | z2), and its 2-L BJDE models have the generative model p (z1, y) = pumpflaced task p."}, {"heading": "4.2 Conditional latent space walk", "text": "The 2-layer BJD, by construction, learns to use z1 as the common multiplicity for x and y. Hybrid training should ideally preserve z1 as the common multiplicity. To assess whether this is the case, we propose a variant of the MNIST quadrant task, which is called the shift-sensitive upper and lower task. In this task, the goal is to predict the lower half of the MNIST digit (y) when we propose the upper half (x). However, we introduce structural disturbances in both the upper and lower half of the image in our training, validation and test sets by randomly shifting each pair (x, y) horizontally by the same number of pixels (shifts between {\u2212 4, \u2212 3,., 3, 4}. We then train the 2-layer BCDE by shifting either the conditional or hybrid target of the ZCDE using the fully monitored regime."}, {"heading": "4.3 Robustness of representation", "text": "To demonstrate this, we are investigating another variant of the MNIST quadrant task, called the shift-invariant top-bottom task. Therefore, the goal is to learn that predicting y (which is always centered) is invariant to the shifted position of x. To eliminate the benefit of undescribed data, we perform this task only in the fully monitored regime. Table 5 shows that the hybrid training makes the BCDE performance more robust against structural corruption in x. Due to its more compact detection model, factored + hybrid is less susceptible to excessive adjustments, resulting in a smaller performance gap between the corrupt and non-corrupt data."}, {"heading": "5 Conclusion", "text": "We introduced the BCDE, a new framework for high-dimensional conditional density estimation that uses multiple layers of stochastic neural networks as a bottleneck between the input and output data. To train the BCDE, we proposed a new hybrid training method in which the BCDE is regulated in the direction of its common counterpart, the BJDE. The bottleneck implies that only the bottleneck needs to be marginalized if either the input or output is missing during training, allowing the common model to be trained in a semi-supervised manner. The regulating effect between the conditional and the common model in our hybrid training process helps the BCDE to become a conditional model itself, to become more robust and learn from disordered data in a semi-supervised environment."}], "references": [{"title": "Generating Sentences from a Continuous Space", "author": ["S.R. Bowman", "L. Vilnis", "O. Vinyals", "A.M. Dai", "R. Jozefowicz", "S. Bengio"], "venue": "arXiv:1511.06349,", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2015}, {"title": "The Helmholtz Machine", "author": ["P. Dayan", "G. Hinton", "R. Neal", "R. Zemel"], "venue": "Neural computation,", "citeRegEx": "2", "shortCiteRegEx": null, "year": 1995}, {"title": "Density estimation using Real NVP", "author": ["L. Dinh", "J. Sohl-Dickstein", "S. Bengio"], "venue": "arXiv:1605.08803,", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2016}, {"title": "Keras", "author": ["C. Fran\u00e7ois"], "venue": "https://github.com/fchollet/keras,", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2016}, {"title": "Posterior regularization for structured latent variable models", "author": ["K. Ganchev", "J. Graca", "J. Gillenwater", "B. Taskar"], "venue": "JMLR,", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2010}, {"title": "The \u201cwake-sleep\u201d algorithm for unsupervised neural networks", "author": ["G. Hinton", "P. Dayan", "B. Frey", "R. Radford"], "venue": "Science,", "citeRegEx": "6", "shortCiteRegEx": null, "year": 1995}, {"title": "Fast Nonparametric Conditional Density Estimation", "author": ["M.P. Holmes", "A.G. Gray", "C.L. Isbell"], "venue": "arXiv:1206.5278,", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2012}, {"title": "Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift", "author": ["S. Ioffe", "C. Szegedy"], "venue": "arXiv:1502.03167,", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2015}, {"title": "Ladder Variational Autoencoders", "author": ["C. Kaae S\u00f8nderby", "T. Raiko", "L. Maal\u00f8e", "S. Kaae S\u00f8nderby", "O. Winther"], "venue": "arXiv:1602.02282,", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2016}, {"title": "Adam: A Method for Stochastic Optimization", "author": ["D. Kingma", "J. Ba"], "venue": "arXiv:1412.6980,", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2014}, {"title": "Auto-Encoding Variational Bayes", "author": ["D.P. Kingma", "M. Welling"], "venue": "arXiv:1312.6114,", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2013}, {"title": "Semi-Supervised Learning with Deep Generative Models", "author": ["D.P. Kingma", "D.J. Rezende", "S. Mohamed", "M. Welling"], "venue": "arXiv:1406.5298,", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2014}, {"title": "Principled hybrids of generative and discriminative models", "author": ["J. Lasserre", "C. Bishop", "T. Minka"], "venue": "The IEEE Conference on Computer Vision and Pattern Recognition,", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2006}, {"title": "Auxiliary Deep Generative Models", "author": ["L. Maal\u00f8e", "C. Kaae S\u00f8nderby", "S. Kaae S\u00f8nderby", "O. Winther"], "venue": "arXiv:1602.05473,", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2016}, {"title": "On discriminative vs", "author": ["A. Ng", "M. Jordan"], "venue": "generative classifiers: A comparison of logistic regression and naive bayes. Neural Information Processing Systems,", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2002}, {"title": "Unsupervised Representation Learning with Deep Convolutional Generative Adversarial Networks", "author": ["A. Radford", "L. Metz", "S. Chintala"], "venue": "arXiv:1511.06434,", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2015}, {"title": "Hierarchical Variational Models", "author": ["R. Ranganath", "D. Tran", "D.M. Blei"], "venue": "ArXiv e-prints, 1511.02386, Nov.", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2015}, {"title": "Stochastic Backpropagation and Approximate Inference in Deep Generative Models", "author": ["D. Rezende", "S. Mohamed", "D. Wierstra"], "venue": "ArXiv e-prints, 1401.4082, Jan.", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2014}, {"title": "On the quantitative analysis of deep belief networks", "author": ["R. Salakhutdinov", "I. Murray"], "venue": "International Conference on Machine Learning,", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2008}, {"title": "Improved multimodal deep learning with variation of information", "author": ["K. Sohn", "W. Shang", "L. H"], "venue": "Neural Information Processing Systems,", "citeRegEx": "20", "shortCiteRegEx": "20", "year": 2014}, {"title": "Learning structured output representation using deep conditional generative models", "author": ["K. Sohn", "X. Yan", "H. Lee"], "venue": "Neural Information Processing Systems,", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2015}, {"title": "A note on the evaluation of generative models", "author": ["L. Theis", "A. van den Oord", "M. Bethge"], "venue": null, "citeRegEx": "23", "shortCiteRegEx": "23", "year": 2015}, {"title": "Probabilistic Principal Component Analysis", "author": ["M. Tipping", "C. Bishop"], "venue": "J. R. Statist. Soc. B,", "citeRegEx": "24", "shortCiteRegEx": null, "year": 1999}, {"title": "On the Quantitative Analysis of Decoder- Based Generative Models", "author": ["Y. Wu", "Y. Burda", "R. Salakhutdinov", "R. Grosse"], "venue": "arXiv:1611.04273,", "citeRegEx": "25", "shortCiteRegEx": null, "year": 2016}, {"title": "Attribute2Image: Conditional Image Generation from Visual Attributes", "author": ["X. Yan", "J. Yang", "K. Sohn", "H. Lee"], "venue": "arXiv:1512.00570,", "citeRegEx": "26", "shortCiteRegEx": null, "year": 2015}, {"title": "Variational Neural Machine Translation", "author": ["B. Zhang", "D. Xiong", "J. Su", "H. Duan", "M. Zhang"], "venue": "arXiv:1605.07869,", "citeRegEx": "27", "shortCiteRegEx": null, "year": 2016}], "referenceMentions": [{"referenceID": 6, "context": "Classical non-parametric conditional density estimators typically rely directly on local Euclidean distance in the original input and target space [7].", "startOffset": 147, "endOffset": 150}, {"referenceID": 20, "context": "Recent advances in deep generative models have led to new parametric models for high-dimensional CDE tasks, namely the conditional variational autoencoders (CVAE) [21].", "startOffset": 163, "endOffset": 167}, {"referenceID": 20, "context": "CVAEs have been applied to a variety of problems, such as MNIST quadrant prediction, segmentation [21], attribute-based image generation [26], and machine translation [27].", "startOffset": 98, "endOffset": 102}, {"referenceID": 24, "context": "CVAEs have been applied to a variety of problems, such as MNIST quadrant prediction, segmentation [21], attribute-based image generation [26], and machine translation [27].", "startOffset": 137, "endOffset": 141}, {"referenceID": 25, "context": "CVAEs have been applied to a variety of problems, such as MNIST quadrant prediction, segmentation [21], attribute-based image generation [26], and machine translation [27].", "startOffset": 167, "endOffset": 171}, {"referenceID": 10, "context": "We note that while variational methods [11, 18] have been applied to semi-supervised classification (where y is simply a class label) [12, 14], semisupervised CDE (where y is high-dimensional) remains an open problem.", "startOffset": 39, "endOffset": 47}, {"referenceID": 17, "context": "We note that while variational methods [11, 18] have been applied to semi-supervised classification (where y is simply a class label) [12, 14], semisupervised CDE (where y is high-dimensional) remains an open problem.", "startOffset": 39, "endOffset": 47}, {"referenceID": 11, "context": "We note that while variational methods [11, 18] have been applied to semi-supervised classification (where y is simply a class label) [12, 14], semisupervised CDE (where y is high-dimensional) remains an open problem.", "startOffset": 134, "endOffset": 142}, {"referenceID": 13, "context": "We note that while variational methods [11, 18] have been applied to semi-supervised classification (where y is simply a class label) [12, 14], semisupervised CDE (where y is high-dimensional) remains an open problem.", "startOffset": 134, "endOffset": 142}, {"referenceID": 12, "context": "Following [13], we propose a hybrid training framework that regularizes the conditionally-trained BCDE parameters toward the jointly-trained BJDE parameters.", "startOffset": 10, "endOffset": 14}, {"referenceID": 20, "context": "Using our BCDE hybrid training framework, we establish new benchmarks for the MNIST quadrant prediction task [21] in both the fully-supervised and semi-supervised regimes.", "startOffset": 109, "endOffset": 113}, {"referenceID": 22, "context": "The VAE can be seen as a non-linear generalization of the probabilistic PCA [24], and thus can recover non-linear manifolds in the data.", "startOffset": 76, "endOffset": 80}, {"referenceID": 10, "context": "Learning the VAE is done by jointly optimizing the parameters of both the generative and recognition models so as to maximize an objective that resembles an autoencoder regularized reconstruction loss [11]", "startOffset": 201, "endOffset": 205}, {"referenceID": 1, "context": "Second, this term serves as a form of amortized posterior regularization that encourages the posterior p\u03b8(z|x) to be close to an amortized variational family [2, 5, 6].", "startOffset": 158, "endOffset": 167}, {"referenceID": 4, "context": "Second, this term serves as a form of amortized posterior regularization that encourages the posterior p\u03b8(z|x) to be close to an amortized variational family [2, 5, 6].", "startOffset": 158, "endOffset": 167}, {"referenceID": 5, "context": "Second, this term serves as a form of amortized posterior regularization that encourages the posterior p\u03b8(z|x) to be close to an amortized variational family [2, 5, 6].", "startOffset": 158, "endOffset": 167}, {"referenceID": 10, "context": "(1), and the reparameterization trick [11] is used to transform the expectation over z \u223c q\u03c6(z|x) into \u223c N (0, Ik); z = \u03bc\u03c6(x) + diag(\u03c3 \u03c6(x)) which leads to an easily obtained stochastic gradient.", "startOffset": 38, "endOffset": 42}, {"referenceID": 20, "context": "In [21], the authors introduced the conditional version of variational autoencoders.", "startOffset": 3, "endOffset": 7}, {"referenceID": 10, "context": "using the same technique for VAE [11, 18] but with a recognition network q\u03c6(z|x, y) = N ( z|\u03bc\u03c6(x, y), diag ( \u03c3 \u03c6(x, y) )) taking both x and y as input.", "startOffset": 33, "endOffset": 41}, {"referenceID": 17, "context": "using the same technique for VAE [11, 18] but with a recognition network q\u03c6(z|x, y) = N ( z|\u03bc\u03c6(x, y), diag ( \u03c3 \u03c6(x, y) )) taking both x and y as input.", "startOffset": 33, "endOffset": 41}, {"referenceID": 20, "context": "While [21] demonstrated that CVAE can be applied to high-dimensional conditional density estimation, CVAE suffers from two limitations.", "startOffset": 6, "endOffset": 10}, {"referenceID": 20, "context": "Following the conditional training paradigm in [21], conditional/discriminative training of the BCDE means maximizing the lower bound of a conditional likelihood similar to Eq.", "startOffset": 47, "endOffset": 51}, {"referenceID": 8, "context": "When the prior and approximate likelihood are both Gaussians, this is exactly precision-weighted merging of the means and variances [9].", "startOffset": 132, "endOffset": 135}, {"referenceID": 14, "context": "For classification, it has been observed that a generative model trained to estimate the joint distribution may yield sub-optimal performance when compared to a model that was trained discriminatively [15].", "startOffset": 201, "endOffset": 205}, {"referenceID": 11, "context": "Indeed, both [12] and [14] incorporated additional discriminative training into their objective functions in order to successfully perform semi-supervised classification with deep generative models.", "startOffset": 13, "endOffset": 17}, {"referenceID": 13, "context": "Indeed, both [12] and [14] incorporated additional discriminative training into their objective functions in order to successfully perform semi-supervised classification with deep generative models.", "startOffset": 22, "endOffset": 26}, {"referenceID": 12, "context": "The necessity of additional discriminative training is attributable to the joint model being mis-specified [13].", "startOffset": 107, "endOffset": 111}, {"referenceID": 12, "context": "To address this, [13] proposed a principled hybrid blending of the joint and conditional models.", "startOffset": 17, "endOffset": 21}, {"referenceID": 12, "context": "At its core, [13]\u2019s hybrid blending procedure regularizes the parameters \u03b8 of the conditional model toward the parameters \u03b8\u2032 of the joint model by introducing a prior that softly ties \u03b8\u2032 to \u03b8.", "startOffset": 13, "endOffset": 17}, {"referenceID": 12, "context": "However, [13] considers models that only contain generative parameters.", "startOffset": 9, "endOffset": 13}, {"referenceID": 19, "context": "We evaluated the performance of our hybrid training procedure on the permutation-invariant MNIST quadrant prediction task [20, 21].", "startOffset": 122, "endOffset": 130}, {"referenceID": 20, "context": "We evaluated the performance of our hybrid training procedure on the permutation-invariant MNIST quadrant prediction task [20, 21].", "startOffset": 122, "endOffset": 130}, {"referenceID": 18, "context": "The MNIST digits are statically binarized by sampling from the Bernoulli distribution according their pixel values [19].", "startOffset": 115, "endOffset": 119}, {"referenceID": 9, "context": "When training our models, we optimized the various training objectives with Adam [10].", "startOffset": 81, "endOffset": 85}, {"referenceID": 20, "context": "Although our training objective is based on the variational lower bound, our performance scoring metric is the negative conditional log-likelihood score which we approximate using importance sampling with 50 samples [21].", "startOffset": 216, "endOffset": 220}, {"referenceID": 7, "context": "All MLPs are batch-normalized [8] and parameterized with two hidden layers of 500 rectified linear units.", "startOffset": 30, "endOffset": 33}, {"referenceID": 3, "context": "The models were implemented in Python using Theano [22] and Keras [4].", "startOffset": 66, "endOffset": 69}, {"referenceID": 8, "context": "To deal with the multi-layer stochasticity, the BCDE and BJDE are trained using top-down inference [9].", "startOffset": 99, "endOffset": 102}, {"referenceID": 20, "context": "In the fully-supervised regime, hybrid-trained BCDE achieves the best performance, significantly improves upon its conditionally-trained counterpart as well as previously reported result for CVAE [21].", "startOffset": 196, "endOffset": 200}, {"referenceID": 20, "context": "Models nl = 50000 nl = 25000 nl = 10000 nl = 5000 CVAE [21] 63.", "startOffset": 55, "endOffset": 59}, {"referenceID": 20, "context": "Models nl = 50000 nl = 25000 nl = 10000 nl = 5000 CVAE [21] 44.", "startOffset": 55, "endOffset": 59}, {"referenceID": 20, "context": "Models nl = 50000 nl = 25000 nl = 10000 nl = 5000 CVAE [21] 20.", "startOffset": 55, "endOffset": 59}, {"referenceID": 19, "context": "We report the test set negative conditional log-likelihood scores for the MNIST quadrant prediction task [20].", "startOffset": 105, "endOffset": 109}, {"referenceID": 23, "context": "more compact recognition model becomes more apparent; hybrid + factored inference achieves the best performance, out-performing hybrid on the nl = 5000 1-quadrant task by as much as 1 nat\u2014a considerable margin [25].", "startOffset": 210, "endOffset": 214}, {"referenceID": 8, "context": "The 2-layer BCDE generally outperforms 1-layer BCDE due to having a more expressive variational family [9, 17].", "startOffset": 103, "endOffset": 110}, {"referenceID": 16, "context": "The 2-layer BCDE generally outperforms 1-layer BCDE due to having a more expressive variational family [9, 17].", "startOffset": 103, "endOffset": 110}, {"referenceID": 21, "context": "Conditionally generated samples from the hybrid and conditional models in Figure 3 shows that the models with lower perplexity tend to produce high-entropy conditional image generators that spreads the conditional probability mass over the target output space [23].", "startOffset": 260, "endOffset": 264}, {"referenceID": 15, "context": "Latent space interpolation is a popular approach for evaluating the manifold structure of the latent space [16, 3, 1].", "startOffset": 107, "endOffset": 117}, {"referenceID": 2, "context": "Latent space interpolation is a popular approach for evaluating the manifold structure of the latent space [16, 3, 1].", "startOffset": 107, "endOffset": 117}, {"referenceID": 0, "context": "Latent space interpolation is a popular approach for evaluating the manifold structure of the latent space [16, 3, 1].", "startOffset": 107, "endOffset": 117}], "year": 2017, "abstractText": "We propose a neural network framework for high-dimensional conditional density estimation. The Bottleneck Conditional Density Estimator (BCDE) is a variant of the conditional variational autoencoder (CVAE) that employs layer(s) of stochastic variables as the bottleneck between the input x and target y, where both are highdimensional. The key to effectively train BCDEs is the hybrid blending of the conditional generative model with a joint generative model that leverages unlabeled data to regularize the conditional model. We show that the BCDE significantly outperforms the CVAE in MNIST quadrant prediction benchmarks in the fully supervised case and establishes new benchmarks in the semi-supervised case.", "creator": "LaTeX with hyperref package"}}}