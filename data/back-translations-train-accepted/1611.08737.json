{"id": "1611.08737", "review": {"conference": "AAAI", "VERSION": "v1", "DATE_OF_SUBMISSION": "26-Nov-2016", "title": "Structural Correspondence Learning for Cross-Lingual Sentiment Classification with One-to-Many Mappings", "abstract": "Structural correspondence learning (SCL) is an effective method for cross-lingual sentiment classification. This approach uses unlabeled documents along with a word translation oracle to automatically induce task specific, cross-lingual correspondences. It transfers knowledge through identifying important features, i.e., pivot features. For simplicity, however, it assumes that the word translation oracle maps each pivot feature in source language to exactly only one word in target language. This one-to-one mapping between words in different languages is too strict. Also the context is not considered at all. In this paper, we propose a cross-lingual SCL based on distributed representation of words; it can learn meaningful one-to-many mappings for pivot words using large amounts of monolingual data and a small dictionary. We conduct experiments on NLP\\&amp;CC 2013 cross-lingual sentiment analysis dataset, employing English as source language, and Chinese as target language. Our method does not rely on the parallel corpora and the experimental results show that our approach is more competitive than the state-of-the-art methods in cross-lingual sentiment classification.", "histories": [["v1", "Sat, 26 Nov 2016 20:11:00 GMT  (111kb,D)", "http://arxiv.org/abs/1611.08737v1", "To appear in AAAI 2017. arXiv admin note: text overlap witharXiv:1008.0716by other authors"]], "COMMENTS": "To appear in AAAI 2017. arXiv admin note: text overlap witharXiv:1008.0716by other authors", "reviews": [], "SUBJECTS": "cs.LG cs.CL stat.ML", "authors": ["nana li", "shuangfei zhai", "zhongfei zhang", "boying liu"], "accepted": true, "id": "1611.08737"}, "pdf": {"name": "1611.08737.pdf", "metadata": {"source": "CRF", "title": "Structural Correspondence Learning for Cross-lingual Sentiment Classification with One-to-many Mappings", "authors": ["Nana Li", "Shuangfei Zhai", "Zhongfei Zhang", "Boying Liu"], "emails": ["lin@binghamton.edu", "szhai2@binghamton.edu,", "zhongfei@cs.binghamton.edu", "lby@hebut.edu.cn"], "sections": [{"heading": "Introduction", "text": "The task is to predict the sensitivity of a particular document, such as a product overview or a comment essay. Your goal is to develop automatic approaches that classify the polarity in the text as positive, neutral or negative. To achieve a satisfactory classification, most of them need to use their knowledge of time and human efforts."}, {"heading": "Related Work", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "Sentiment Classification", "text": "Sentiment classification is common as a two-class classification problem, positive and negative. Furthermore, training and test data are usually product evaluations. They have been developed since 2000 (Liu 2012) and have become a very active area of research. In general, sentiment classification has been examined mainly on three levels: document level, sentence level and aspect level. In this essay, we focus only on document level. Sentiment level sentiments classification aims to classify an opinion document as an expression of a positive or negative opinion. Approaches are generally based on two types of resources: sentiment lexicon and corporate. Dictionary-based approaches say the sentiment polarities by creating and using sentiment lexicon, while group-based approaches generally treat the sentiment problem classification problem as a machine learning task. Most of the existing approaches focus on extracting different characteristics from text and then applying multicultural learning techniques, while group-based approaches treat the sentiment problem classification problem as a machine learning task."}, {"heading": "Cross-lingual Sentiment Classification", "text": "Traditional CLSC approaches use machine translation systems to bridge the gap between the source language and the target language. Blitzer et al. (Wan 2009) proposed a common approach to address this problem. Labeled English reviews and unlabeled Chinese reviews were then translated separately into labeled Chinese reviews and unlabeled English reviews. Therefore, each review had the two views. Support Vector Factorization Models (SVM) was then applied to learn two classifiers. Finally, the two classifiers were combined into a single classifier. Pan et al. (Pan et al. 2011) they designed a bi-view nonnegative matrix factorization model with machine translation. They learned previously invisible sensation words from the large parallel datasets. Li et al. 2011) studied semi-supervised learning for imbalanced sentiment classification by."}, {"heading": "CL-SCL with One-to-many Mappings", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "Problem Definition", "text": "We have a set of labeled training materials DS = {(xi, yi)} ni = 1 written in the source language S; XS is the source language attribute space, XT is the target language attribute space, and Y is the set of class names. Let X = Xs XT denote the attribute space. For simplicity and without loss of generality, we look at the problem of binary classification, i.e. Y = {+ 1, \u2212 1}. In addition, VS denotes the source vocabulary, VT the target vocabulary, and V = VS VT. In addition to labeled training materials DS, U and DT, U denotes the source language S or the target language T. Let's DU denote DS, U DT, U. The goal is to create a classification for documents written in another language T that can dictate the designations of new, hitherto invisible evaluations in T."}, {"heading": "Pivot Sets Based on CL-SCL with One-to-many Mappings", "text": "We have it in hand to understand the language of the people and to understand what they do. (...) We have it in hand to understand the language of the people. (...) We have it in hand to understand the language of the people. (...) We have it in hand to understand the language of the people. (...) We have it in hand to understand the language of the people. (...) We have it in hand to understand the language of the people. (...) We have it in hand to understand the language of the people. (...) We have it in hand to understand the language of the people. (...) We have it in the language of the language of the people. (...) We have it in the language of the language of the people. (...) We have it in the language of the language. (...) We have it in the language of the people. (...) We have it in the language of the language. (...) We have it in the language of the language. (...) We have it in the language of the language. (...) We have it in the language of the people. (...) We have it in the language of the language. (... We have it in the language of the language.) We have it."}, {"heading": "Framework of the Proposed Method", "text": "First, as described above, we create pivot characters P (| P | = m), which are pairs of words {wS, wT}, where wS is the pivot in the source language and wT is the pivot in the target language. From wS to wT, it is a one-dimensional mapping, learning from bilingual pairs of words that use the distributed representation of words. Second, similar to CL-SCL, we build the connection of unlabeled documents in both the source and target languages and obtain the low-dimensional hypotheses space. For each pivot pl-P, a linear classifier is formed to model the correlations between the pivot {wS, wT} and all other words w / i {wS, wT}. Each linear classifier is characterized by the parameter vector vwl."}, {"heading": "Experiments", "text": "In this section, we evaluate the effectiveness and efficiency of the SCL-OM algorithm proposed in this paper. We use English as the source language and Chinese as the target language for the task of linguistic mood classification."}, {"heading": "Dataset and Preprocessing", "text": "Each category contains 4,000 labeled English reviews as training data, 4,000 Chinese reviews as test data, and over tens of thousands of unlabeled Chinese product reviews. Since training the monolingual language model requires a large amount of text data, we use the unlabeled English reviews from (Prettenhofer and Stein 2011) to learn the representations of English words. See Table 1. Each English or Chinese review contains summary, text, and category; we extract the content of the summary and text, and combine them as a review document d expressed as a feature vector x using a unique word model. In addition, we select these words only as features with frequency greater than 5. We summarize the word size of the datasets in Table 2.The monolingual word x using the unique bag-of-word model."}, {"heading": "Methods", "text": "A Chinese SVM classifier is learned with the translated ratings. The translated test data set is used for the test. \u2022 Train ENG: Using labeled English ratings as training data, an English SVM classifier is learned. Then, the Chinese test reviews from Google Translate are translated into English. \u2022 Basic Co-Training (Co-Train): The co-training method proposed in (Pan et al. 2011) is implemented. It is bidirectional transfer learning. \u2022 CL-SCL: This method is used by Prettenhofer et al. Google Translate to perform the translation, which returns only one word for each pivot feature. Furthermore, we fix the same parameter values as those in our method m = 300, p = 30, k = 120. \u2022 Best is the result in NLP hou & CC 2013: This is the best result reported in NLP-Ram."}, {"heading": "Performance Results", "text": "Remember that SCL-OM has six parameters as input: the number of pivot points m, the dimensionality of the lingual representation k, the minimum support \u03b4 of a pivot point, the word similarity distance threshold \u03c6, the dimensionality of the English word vectors and the dimensionality of the Chinese word vectors. We use fixed values of m = 300, k = 120, \u03b4 = 30 and \u03c6 = 0.1. Regarding the dimensionality of the word vectors trained in the target language, Mikolov et al. (Mikolov, Le and Sutskever 2013) have shown that the dimensionality of the vectors trained in the source language should be several times (approximately 2 to 4 times) greater than that of the vectors trained in the target language for the best performance. Thus, we set the dimensionality of the English word vectors as 200 and that of the Chinese word vectors as the accuracy as 50.Table 3 document the comparisons of the performance between our approach and the competing methods on NCC CLP 2013 0.1."}, {"heading": "Sensitivity Analysis", "text": "In this section, we analyze the sensitivity of the two important parameters while leaving the others unchanged: the number of pivot points m and the dimensionality of the cross-language representation k. Number of pivot points m: Figure 2 shows the influence of the pivot points m on the performance of SCL-OM. The diagrams show that a small number of pivot points can capture a significant proportion of the agreement between S and T. Dimensionality of the cross-language representation k: Figure 3 shows the influence of the dimensionality of the cross-language representation k on the performance of SCLOM. We evaluate SCL-OM when the parameter varies between 50 and 300. As shown in Figure 3, the average accuracy generally moves upwards with increasing k. If k (100, 150) the accuracy reaches the peak value in all three categories, and then the accuracy decreases with the increase in the approach. To gain more insight into the results, we visualize the small portion of the four-dimensional characters in the first column of the numerical L, based on the first column of the four we learn."}, {"heading": "Conclusion", "text": "In this paper, we propose a novel method of structural correspondence learning for the linguistic sentiment classification with one-to-many mappings: this method uses a distributed representation of words to build one-to-many mappings between the pivot characteristics in the source language and those in the target language, without relying on the parallel corpora. This method is evaluated on the basis of the linguistic sentiment analysis data set NLP & CC 2013, using English as the source language and Chinese as the target language. Experimental results show that our approach competes with the most modern methods of linguistic sentiment classification, but our approach ignores polysemy in the one-to-many mappings. In the future, we will explore the method of learning meaning-specific word embedding."}, {"heading": "Acknowledgments", "text": "This work is partially supported by the Tianjin National Natural Science Foundation for Young Scholars (13JCQNJC00200)."}], "references": [{"title": "Multilingual subjectivity analysis using machine translation", "author": ["Banea"], "venue": "In Proceedings of the Conference", "citeRegEx": "Banea,? \\Q2008\\E", "shortCiteRegEx": "Banea", "year": 2008}, {"title": "Biographies, bollywood, boom-boxes and blenders: Domain adaptation for sentiment classification", "author": ["Blitzer"], "venue": "In ACL,", "citeRegEx": "Blitzer,? \\Q2007\\E", "shortCiteRegEx": "Blitzer", "year": 2007}, {"title": "Domain adaptation with structural correspondence learning", "author": ["McDonald Blitzer", "J. Pereira 2006] Blitzer", "R. McDonald", "F. Pereira"], "venue": "In Proceedings of the 2006 conference on empirical methods in natural language processing,", "citeRegEx": "Blitzer et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Blitzer et al\\.", "year": 2006}, {"title": "A mixed model for cross lingual opinion analysis", "author": ["Gui"], "venue": "In Natural Language Processing and Chinese Computing", "citeRegEx": "Gui,? \\Q2013\\E", "shortCiteRegEx": "Gui", "year": 2013}, {"title": "Cross-lingual opinion analysis via negative transfer detection", "author": ["Gui"], "venue": "In ACL", "citeRegEx": "Gui,? \\Q2014\\E", "shortCiteRegEx": "Gui", "year": 2014}, {"title": "Improving transfer learning in cross lingual opinion analysis through negative transfer detection", "author": ["Gui"], "venue": "In International Conference on Knowledge Science, Engineering and Management,", "citeRegEx": "Gui,? \\Q2015\\E", "shortCiteRegEx": "Gui", "year": 2015}, {"title": "and Liu", "author": ["M. Hu"], "venue": "B.", "citeRegEx": "Hu and Liu 2004", "shortCiteRegEx": null, "year": 2004}, {"title": "2011", "author": ["S. Li", "Z. Wang", "G. Zhou", "S.Y. M Lee"], "venue": "Semi-supervised learning for imbalanced sentiment classification. In IJCAI Proceedings-International Joint Conference on Artificial Intelligence, volume 22,", "citeRegEx": "Li et al. 2011", "shortCiteRegEx": null, "year": 1826}, {"title": "Sentiment analysis and opinion mining. Synthesis lectures on human language technologies 5(1):1\u2013167", "author": ["B. Liu"], "venue": null, "citeRegEx": "Liu,? \\Q2012\\E", "shortCiteRegEx": "Liu", "year": 2012}, {"title": "Cross-lingual mixture model for sentiment classification", "author": ["Meng"], "venue": "In Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics: Long Papers-Volume", "citeRegEx": "Meng,? \\Q2012\\E", "shortCiteRegEx": "Meng", "year": 2012}, {"title": "Q", "author": ["Mikolov, T.", "Le"], "venue": "V.; and Sutskever, I.", "citeRegEx": "Mikolov. Le. and Sutskever 2013", "shortCiteRegEx": null, "year": 2013}, {"title": "and Collier", "author": ["T. Mullen"], "venue": "N.", "citeRegEx": "Mullen and Collier 2004", "shortCiteRegEx": null, "year": 2004}, {"title": "Cross-lingual sentiment classification via bi-view non-negative matrix tri-factorization", "author": ["Pan"], "venue": null, "citeRegEx": "Pan,? \\Q2011\\E", "shortCiteRegEx": "Pan", "year": 2011}, {"title": "Thumbs up?: sentiment classification using machine learning techniques", "author": ["Lee Pang", "B. Vaithyanathan 2002] Pang", "L. Lee", "S. Vaithyanathan"], "venue": "In Proceedings of the ACL-02 conference on Empirical methods in natural language processing-Volume", "citeRegEx": "Pang et al\\.,? \\Q2002\\E", "shortCiteRegEx": "Pang et al\\.", "year": 2002}, {"title": "and Stein", "author": ["P. Prettenhofer"], "venue": "B.", "citeRegEx": "Prettenhofer and Stein 2010", "shortCiteRegEx": null, "year": 2010}, {"title": "and Stein", "author": ["P. Prettenhofer"], "venue": "B.", "citeRegEx": "Prettenhofer and Stein 2011", "shortCiteRegEx": null, "year": 2011}, {"title": "P", "author": ["Turney"], "venue": "D.", "citeRegEx": "Turney 2002", "shortCiteRegEx": null, "year": 2002}, {"title": "and Zhang", "author": ["S. Zhai"], "venue": "Z.", "citeRegEx": "Zhai and Zhang 2016", "shortCiteRegEx": null, "year": 2016}, {"title": "Semi-supervised learning on cross-lingual sentiment analysis with space transfer", "author": ["Chao Zhang", "H. Wang 2015] Zhang", "W. Chao", "D. Wang"], "venue": "In Big Data Computing Service and Applications (BigDataService),", "citeRegEx": "Zhang et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Zhang et al\\.", "year": 2015}, {"title": "Learning bilingual sentiment word embeddings for cross-language sentiment classification", "author": ["Zhou"], "venue": null, "citeRegEx": "Zhou,? \\Q2015\\E", "shortCiteRegEx": "Zhou", "year": 2015}, {"title": "Cross-lingual sentiment classification with bilingual document representation learning", "author": ["Wan Zhou", "X. Xiao 2016] Zhou", "X. Wan", "J. Xiao"], "venue": null, "citeRegEx": "Zhou et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Zhou et al\\.", "year": 2016}], "referenceMentions": [], "year": 2016, "abstractText": "Structural correspondence learning (SCL) is an effective method for cross-lingual sentiment classification. This approach uses unlabeled documents along with a word translation oracle to automatically induce task specific, cross-lingual correspondences. It transfers knowledge through identifying important features, i.e., pivot features. For simplicity, however, it assumes that the word translation oracle maps each pivot feature in source language to exactly only one word in target language. This one-to-one mapping between words in different languages is too strict. Also the context is not considered at all. In this paper, we propose a cross-lingual SCL based on distributed representation of words; it can learn meaningful one-to-many mappings for pivot words using large amounts of monolingual data and a small dictionary. We conduct experiments on NLP&CC 2013 cross-lingual sentiment analysis dataset, employing English as source language, and Chinese as target language. Our method does not rely on the parallel corpora and the experimental results show that our approach is more competitive than the state-of-the-art methods in cross-lingual sentiment classification.", "creator": "LaTeX with hyperref package"}}}