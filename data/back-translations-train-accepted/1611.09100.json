{"id": "1611.09100", "review": {"conference": "iclr", "VERSION": "v1", "DATE_OF_SUBMISSION": "28-Nov-2016", "title": "Learning to Compose Words into Sentences with Reinforcement Learning", "abstract": "We use reinforcement learning to learn tree-structured neural networks for computing representations of natural language sentences. In contrast with prior work on tree-structured models in which the trees are either provided as input or predicted using supervision from explicit treebank annotations, the tree structures in this work are optimized to improve performance on a downstream task. Experiments demonstrate the benefit of learning task-specific composition orders, outperforming both sequential encoders and recursive encoders based on treebank annotations. We analyze the induced trees and show that while they discover some linguistically intuitive structures (e.g., noun phrases, simple verb phrases), they are different than conventional English syntactic structures.", "histories": [["v1", "Mon, 28 Nov 2016 12:57:07 GMT  (90kb,D)", "http://arxiv.org/abs/1611.09100v1", null]], "reviews": [], "SUBJECTS": "cs.CL", "authors": ["dani yogatama", "phil blunsom", "chris dyer", "edward grefenstette", "wang ling"], "accepted": true, "id": "1611.09100"}, "pdf": {"name": "1611.09100.pdf", "metadata": {"source": "CRF", "title": null, "authors": ["REINFORCEMENT LEARNING", "Dani Yogatama", "Phil Blunsom", "Chris Dyer", "Edward Grefenstette", "Wang Ling"], "emails": ["dyogatama@google.com", "pblunsom@google.com", "cdyer@google.com", "etg@google.com", "lingwang@google.com"], "sections": [{"heading": "1 INTRODUCTION", "text": "In fact, most of them will be able to play by the rules that they have set themselves, and they will be able to play by the rules that they have set themselves in order to achieve their goals."}, {"heading": "2 MODEL", "text": "Our model consists of two components: a sentence representation model and a reinforcement learning algorithm to learn the tree structure used by the sentence representation model."}, {"heading": "2.1 TREE LSTM", "text": "It is an incomplete sentence which asks whether it is a 'yes', a 'no', a 'no', a 'no', a 'no', a 'no', a 'no', a 'no', a 'no', a 'no', a 'no', a 'no', a 'no', a 'no', a 'no', a 'no', a 'no', a 'no', a 'no', a 'no', a 'no', a 'no', a 'no', a 'no', a 'no', a 'no', a 'no', a 'no', a 'no', a 'no', a 'no'."}, {"heading": "2.2 REINFORCEMENT LEARNING", "text": "In the previous paper (Tai et al., 2015; Bowman et al., 2016), the tree structures that determine the composition of the commands of Tree LSTM models, we are not interested. (D), if the individual training data is a triplet [x, a, y]. (D), if it is observed only during training, a reward is presupposed during the test period. (D), that in this case, the policy is trained to adapt explicit human annotations (i.e., Penn TreeBank annotations), the model learns to optimize representations after structures that follow human intuitions. They found that models that are better both in training and in the test period than models that are observed only during training. Our main idea is to use the methods to optimize the structures that follow human intuitions."}, {"heading": "3 EXPERIMENTS", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "3.1 BASELINES", "text": "The aim of our experiments is to evaluate our hypothesis that we can discover useful task-specific tree structures (composition orders) with reinforcement learning. We compare the following composition methods (the last two are unique in our work): \u2022 Right to left: Words are composed from right to left. \u2022 Left to right: Words are composed from left to right. \u2022 Monitored: Words are composed according to a predefined parse tree of the sentence. \u2022 Bidirectional: A bidirectional right and left-right model where the final sentence embedding is an average of the sentence embedding produced by each of these models. \u2022 Monitored: Words are composed according to a predefined parse tree of the sentence. \u2022 Bidirectional: A bidirectional right and left-right model where the final sentence embedding is an average of the sentence embedding produced by each of these models. \u2022 Monitored: Words are composed according to a predefined parse tree of the sentence."}, {"heading": "3.2 TASKS", "text": "This year it is more than ever before."}, {"heading": "4 DISCUSSION", "text": "In fact, most of us are able to play by the rules we have set ourselves in order to fulfil them."}, {"heading": "5 CONCLUSION", "text": "We presented a reinforcement learning method for learning hierarchical structures of natural language sentences. We demonstrated the usefulness of task-specific compositional order in four tasks: emotional analysis, semantic affinity, natural language reasoning and sentence generation. We analyzed the induced trees qualitatively and quantitatively and showed that they both contain some linguistically intuitive structures (e.g. noun sentences, simple verbs) and differ from conventional syntactic structures in English."}], "references": [{"title": "The meaning factory: Formal semantics for recognizing textual entailment and determining semantic similarity", "author": ["Johannes Bjerva", "Johan Bos", "Rob van der Goot", "Malvina Nissim"], "venue": "In Proc. of SemEval,", "citeRegEx": "Bjerva et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Bjerva et al\\.", "year": 2014}, {"title": "Unsupervised induction of tree substitution grammars for dependency parsing", "author": ["Phil Blunsom", "Trevor Cohn"], "venue": "In Proc. of EMNLP,", "citeRegEx": "Blunsom and Cohn.,? \\Q2010\\E", "shortCiteRegEx": "Blunsom and Cohn.", "year": 2010}, {"title": "A large annotated corpus for learning natural language inference", "author": ["Samuel R. Bowman", "Gabor Angeli", "Christopher Potts", "Christopher D. Manning"], "venue": "In Proc. of EMNLP,", "citeRegEx": "Bowman et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Bowman et al\\.", "year": 2015}, {"title": "A fast unified model for parsing and sentence understanding", "author": ["Samuel R. Bowman", "Jon Gauthier", "Abhinav Rastogi", "Raghav Gupta", "Christopher D. Manning", "Christopher Potts"], "venue": "In Proc. of ACL,", "citeRegEx": "Bowman et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Bowman et al\\.", "year": 2016}, {"title": "Hierarchical phrase-based translation", "author": ["David Chiang"], "venue": "Computational Linguistics,", "citeRegEx": "Chiang.,? \\Q2007\\E", "shortCiteRegEx": "Chiang.", "year": 2007}, {"title": "Learning phrase representations using RNN encoder-decoder for statistical machine translation", "author": ["Kyunghyun Cho", "Bart van Merri\u00ebnboer", "\u00c7aglar G\u00fcl\u00e7ehre", "Dzmitry Bahdanau", "Fethi Bougares", "Holger Schwenk", "Yoshua Bengio"], "venue": "arXiv preprint,", "citeRegEx": "Cho et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Cho et al\\.", "year": 2014}, {"title": "A compositional distributional model of meaning", "author": ["Stephen Clark", "Bob Coecke", "Mehrnoosh Sadrzadeh"], "venue": "In Proc. of the Second Symposium on Quantum Interaction,", "citeRegEx": "Clark et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Clark et al\\.", "year": 2008}, {"title": "Jointly modeling aspects, ratings and sentiments for movie recommendation (JMARS)", "author": ["Qiming Diao", "Minghui Qiu", "Chao-Yuan Wu", "Alexander J. Smola", "Jing Jiang", "Chong Wang"], "venue": "In Proc. of KDD,", "citeRegEx": "Diao et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Diao et al\\.", "year": 2014}, {"title": "Recurrent neural network grammars", "author": ["Chris Dyer", "Adhiguna Kuncoro", "Miguel Ballesteros", "Noah A. Smith"], "venue": "In Proc. of NAACL,", "citeRegEx": "Dyer et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Dyer et al\\.", "year": 2016}, {"title": "Experimental support for a categorical compositional distributional model of meaning", "author": ["Edward Grefenstette", "Mehrnoosh Sadrzadeh"], "venue": "In Proc. of EMNLP,", "citeRegEx": "Grefenstette and Sadrzadeh.,? \\Q2011\\E", "shortCiteRegEx": "Grefenstette and Sadrzadeh.", "year": 2011}, {"title": "Long short-term memory", "author": ["Sepp Hochreiter", "Jurgen Schmidhuber"], "venue": "Neural Computation,", "citeRegEx": "Hochreiter and Schmidhuber.,? \\Q1997\\E", "shortCiteRegEx": "Hochreiter and Schmidhuber.", "year": 1997}, {"title": "UNAL-NLP: Combining soft cardinality features for semantic textual similarity, relatedness and entailment", "author": ["Sergio Jimenez", "George Duenas", "Julia Baquero", "Alexander Gelbukh", "Av Juan Dios Batiz", "Av Mendizabal"], "venue": "In Proc. of SemEval,", "citeRegEx": "Jimenez et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Jimenez et al\\.", "year": 2014}, {"title": "A convolutional neural network for modelling sentences", "author": ["Nal Kalchbrenner", "Edward Grefenstette", "Phil Blunsom"], "venue": "In Prof. of ACL,", "citeRegEx": "Kalchbrenner et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Kalchbrenner et al\\.", "year": 2014}, {"title": "Convolutional neural networks for sentence classification", "author": ["Yoon Kim"], "venue": "In Proc. EMNLP,", "citeRegEx": "Kim.,? \\Q2014\\E", "shortCiteRegEx": "Kim.", "year": 2014}, {"title": "Skip-thought vectors", "author": ["Ryan Kiros", "Yukun Zhu", "Ruslan Salakhutdinov", "Richard S. Zemel", "Antonio Torralba", "Raquel Urtasun", "Sanja Fidler"], "venue": "In Proc. of NIPS,", "citeRegEx": "Kiros et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Kiros et al\\.", "year": 2015}, {"title": "Accurate unlexicalized parsing", "author": ["Dan Klein", "Christopher D. Manning"], "venue": "In Proc. of ACL,", "citeRegEx": "Klein and Manning.,? \\Q2003\\E", "shortCiteRegEx": "Klein and Manning.", "year": 2003}, {"title": "Corpus-based induction of syntactic structure: Models of dependency and constituency", "author": ["Dan Klein", "Christopher D. Manning"], "venue": "In Proc. of ACL,", "citeRegEx": "Klein and Manning.,? \\Q2004\\E", "shortCiteRegEx": "Klein and Manning.", "year": 2004}, {"title": "Illinois-lh: A denotational and distributional approach to semantics", "author": ["Alice Lai", "Julia Hockenmaier"], "venue": "In Proc. of SemEval,", "citeRegEx": "Lai and Hockenmaier.,? \\Q2014\\E", "shortCiteRegEx": "Lai and Hockenmaier.", "year": 2014}, {"title": "Dependency-based convolutional neural networks for sentence embedding", "author": ["Mingbo Ma", "Liang Huang", "Bing Xiang", "Bowen Zhou"], "venue": "In Proc. ACL,", "citeRegEx": "Ma et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Ma et al\\.", "year": 2015}, {"title": "Evaluation of compositional distributional semantic models on full sentences through semantic relatedness and textual entailment", "author": ["Marco Marelli", "Luisa Bentivogli", "Marco Baroni", "Raffaella Bernardi", "Stefano Menini", "Roberto Zamparelli"], "venue": "In Proc. of SemEval,", "citeRegEx": "Marelli et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Marelli et al\\.", "year": 2014}, {"title": "Natural language inference by tree-based convolution and heuristic matching", "author": ["Lili Mou", "Rui Men", "Ge Li", "Yan Xu", "Lu Zhang", "Rui Yan", "Zhi Jin"], "venue": "In Proc. of ACL,", "citeRegEx": "Mou et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Mou et al\\.", "year": 2016}, {"title": "Neural semantic encoders", "author": ["Tsendsuren Munkhdalai", "Hong Yu"], "venue": "arXiv preprint,", "citeRegEx": "Munkhdalai and Yu.,? \\Q2016\\E", "shortCiteRegEx": "Munkhdalai and Yu.", "year": 2016}, {"title": "Neural tree indexers for text understanding", "author": ["Tsendsuren Munkhdalai", "Hong Yu"], "venue": "arXiv preprint,", "citeRegEx": "Munkhdalai and Yu.,? \\Q2016\\E", "shortCiteRegEx": "Munkhdalai and Yu.", "year": 2016}, {"title": "Glove: Global vectors for word representation", "author": ["Jeffrey Pennington", "Richard Socher", "Christopher D. Manning"], "venue": "In Proc. of EMNLP,", "citeRegEx": "Pennington et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Pennington et al\\.", "year": 2014}, {"title": "Language Learnability and Language Development", "author": ["Steven Pinker"], "venue": "Harvard,", "citeRegEx": "Pinker.,? \\Q1984\\E", "shortCiteRegEx": "Pinker.", "year": 1984}, {"title": "Semi-supervised recursive autoencoders for predicting sentiment distributions", "author": ["Richard Socher", "Jeffrey Pennington", "Eric H. Huang", "Andrew Y. Ng", "Christopher D. Manning"], "venue": "In Proc. of EMNLP,", "citeRegEx": "Socher et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Socher et al\\.", "year": 2011}, {"title": "Recursive deep models for semantic compositionality over a sentiment treebank", "author": ["Richard Socher", "Alex Perelygin", "Jean Wu", "Jason Chuang", "Christopher Manning", "Andrew Ng", "Christopher Potts"], "venue": "In Proc. of EMNLP,", "citeRegEx": "Socher et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Socher et al\\.", "year": 2013}, {"title": "Grounded compositional semantics for finding and describing images with sentences", "author": ["Richard Socher", "Andrej Karpathy", "Quoc V Le", "Christopher D Manning", "Andrew Y Ng"], "venue": "Transactions of the Association for Computational Linguistics,", "citeRegEx": "Socher et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Socher et al\\.", "year": 2014}, {"title": "Unsupervised dependency parsing without gold part-of-speech tags", "author": ["Valentin I. Spitkovsky", "Hiyan Alshawi", "Angel X. Chang", "Daniel Jurafsky"], "venue": "In Proc. of EMNLP,", "citeRegEx": "Spitkovsky et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Spitkovsky et al\\.", "year": 2011}, {"title": "Sequence to sequence learning with neural networks", "author": ["Ilya Sutskever", "Oriol Vinyals", "Quoc V. Le"], "venue": "In Proc. NIPS,", "citeRegEx": "Sutskever et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Sutskever et al\\.", "year": 2014}, {"title": "Improved semantic representations from tree-structured long short-term memory networks", "author": ["Kai Sheng Tai", "Richard Socher", "Christopher D. Manning"], "venue": "In Proc. of ACL,", "citeRegEx": "Tai et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Tai et al\\.", "year": 2015}, {"title": "Order-embeddings of images and language", "author": ["Ivan Vendrov", "Ryan Kiros", "Sanja Fidler", "Raquel Urtasun"], "venue": "In Proc. of ICLR,", "citeRegEx": "Vendrov et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Vendrov et al\\.", "year": 2016}, {"title": "Simple statistical gradient-following algorithms for connectionist reinforcement learning", "author": ["Ronald J. Williams"], "venue": "Machine Learning,", "citeRegEx": "Williams.,? \\Q1992\\E", "shortCiteRegEx": "Williams.", "year": 1992}, {"title": "Learning to map sentences to logical form: Structured classification with probabilistic categorial grammars", "author": ["Luke S. Zettlemoyer", "Michael Collins"], "venue": "In Proc. of UAI,", "citeRegEx": "Zettlemoyer and Collins.,? \\Q2005\\E", "shortCiteRegEx": "Zettlemoyer and Collins.", "year": 2005}], "referenceMentions": [{"referenceID": 5, "context": "The first composes words sequentially using a recurrent neural network, treating the RNN\u2019s final hidden state as the representation of the sentence (Cho et al., 2014; Sutskever et al., 2014; Kiros et al., 2015).", "startOffset": 148, "endOffset": 210}, {"referenceID": 29, "context": "The first composes words sequentially using a recurrent neural network, treating the RNN\u2019s final hidden state as the representation of the sentence (Cho et al., 2014; Sutskever et al., 2014; Kiros et al., 2015).", "startOffset": 148, "endOffset": 210}, {"referenceID": 14, "context": "The first composes words sequentially using a recurrent neural network, treating the RNN\u2019s final hidden state as the representation of the sentence (Cho et al., 2014; Sutskever et al., 2014; Kiros et al., 2015).", "startOffset": 148, "endOffset": 210}, {"referenceID": 6, "context": "Prior work on tree-structured models has assumed that trees are either provided together with the input sentences (Clark et al., 2008; Grefenstette & Sadrzadeh, 2011; Socher et al., 2011; 2013; Tai et al., 2015) or that they are predicted based on explicit treebank annotations jointly with the downstream task (Bowman et al.", "startOffset": 114, "endOffset": 211}, {"referenceID": 25, "context": "Prior work on tree-structured models has assumed that trees are either provided together with the input sentences (Clark et al., 2008; Grefenstette & Sadrzadeh, 2011; Socher et al., 2011; 2013; Tai et al., 2015) or that they are predicted based on explicit treebank annotations jointly with the downstream task (Bowman et al.", "startOffset": 114, "endOffset": 211}, {"referenceID": 30, "context": "Prior work on tree-structured models has assumed that trees are either provided together with the input sentences (Clark et al., 2008; Grefenstette & Sadrzadeh, 2011; Socher et al., 2011; 2013; Tai et al., 2015) or that they are predicted based on explicit treebank annotations jointly with the downstream task (Bowman et al.", "startOffset": 114, "endOffset": 211}, {"referenceID": 3, "context": ", 2015) or that they are predicted based on explicit treebank annotations jointly with the downstream task (Bowman et al., 2016; Dyer et al., 2016).", "startOffset": 107, "endOffset": 147}, {"referenceID": 8, "context": ", 2015) or that they are predicted based on explicit treebank annotations jointly with the downstream task (Bowman et al., 2016; Dyer et al., 2016).", "startOffset": 107, "endOffset": 147}, {"referenceID": 18, "context": "The last approach for constructing sentence representations uses convolutional neural networks to produce the representation in a bottom up manner, either with syntactic information (Ma et al., 2015) or without (Kim, 2014; Kalchbrenner et al.", "startOffset": 182, "endOffset": 199}, {"referenceID": 13, "context": ", 2015) or without (Kim, 2014; Kalchbrenner et al., 2014).", "startOffset": 19, "endOffset": 57}, {"referenceID": 12, "context": ", 2015) or without (Kim, 2014; Kalchbrenner et al., 2014).", "startOffset": 19, "endOffset": 57}, {"referenceID": 24, "context": "Since semantic feedback has been proposed as crucial for the acquisition of syntax (Pinker, 1984), our model offers a simpler alternative.", "startOffset": 83, "endOffset": 97}, {"referenceID": 4, "context": "This is in line with prior work showing the value of learning tree structures in statistical machine translation models (Chiang, 2007).", "startOffset": 120, "endOffset": 134}, {"referenceID": 3, "context": "Our sentence representation model follows the Stack-augmented Parser-Interpreter Neural Network (SPINN; Bowman et al., 2016), SPINN is a shift-reduce parser that uses Long Short-Term Memory (LSTM; Hochreiter and Schmidhuber, 1997) as its composition function.", "startOffset": 96, "endOffset": 124}, {"referenceID": 10, "context": ", 2016), SPINN is a shift-reduce parser that uses Long Short-Term Memory (LSTM; Hochreiter and Schmidhuber, 1997) as its composition function.", "startOffset": 73, "endOffset": 113}, {"referenceID": 30, "context": "SPINN uses Tree LSTM (Tai et al., 2015) as the REDUCE composition function, which we follow.", "startOffset": 21, "endOffset": 39}, {"referenceID": 30, "context": "In previous work (Tai et al., 2015; Bowman et al., 2016), the tree structures that guided composition orders of Tree LSTM models are given directly as input (i.", "startOffset": 17, "endOffset": 56}, {"referenceID": 3, "context": "In previous work (Tai et al., 2015; Bowman et al., 2016), the tree structures that guided composition orders of Tree LSTM models are given directly as input (i.", "startOffset": 17, "endOffset": 56}, {"referenceID": 32, "context": "We use REINFORCE (Williams, 1992), which is an instance of a broader class of algorithms called policy gradient methods, to learn WR such that the sequence of actions a = {a1, .", "startOffset": 17, "endOffset": 33}, {"referenceID": 2, "context": ", 2015; Bowman et al., 2016), the tree structures that guided composition orders of Tree LSTM models are given directly as input (i.e., a is observed and provided as an input). Formally, each training data is a triplet {x,a,y}. Tai et al. (2015) consider models where a is also given at test time, whereas Bowman et al.", "startOffset": 8, "endOffset": 246}, {"referenceID": 2, "context": ", 2015; Bowman et al., 2016), the tree structures that guided composition orders of Tree LSTM models are given directly as input (i.e., a is observed and provided as an input). Formally, each training data is a triplet {x,a,y}. Tai et al. (2015) consider models where a is also given at test time, whereas Bowman et al. (2016) explore models where a can be either observed or not at test time.", "startOffset": 8, "endOffset": 327}, {"referenceID": 26, "context": "Stanford Sentiment Treebank We evaluate our model on a sentiment classification task from the Stanford Sentiment Treebank (Socher et al., 2013).", "startOffset": 122, "endOffset": 143}, {"referenceID": 23, "context": "We set the word embedding size to 100 and initialize them with Glove vectors (Pennington et al., 2014)3.", "startOffset": 77, "endOffset": 102}, {"referenceID": 26, "context": "2m RNTN (Socher et al., 2013) 85.", "startOffset": 8, "endOffset": 29}, {"referenceID": 12, "context": "4 DCNN (Kalchbrenner et al., 2014) 86.", "startOffset": 7, "endOffset": 34}, {"referenceID": 13, "context": "8 CNN-random(Kim, 2014) 82.", "startOffset": 12, "endOffset": 23}, {"referenceID": 13, "context": "7 CNN-word2vec (Kim, 2014) 87.", "startOffset": 15, "endOffset": 26}, {"referenceID": 13, "context": "2 CNN-multichannel (Kim, 2014) 88.", "startOffset": 19, "endOffset": 30}, {"referenceID": 30, "context": "8m Left to Right LSTM (Tai et al., 2015) 84.", "startOffset": 22, "endOffset": 40}, {"referenceID": 30, "context": "8m Bidirectional LSTM (Tai et al., 2015) 87.", "startOffset": 22, "endOffset": 40}, {"referenceID": 30, "context": "8m Constituency Tree\u2013LSTM\u2013random (Tai et al., 2015) 82.", "startOffset": 33, "endOffset": 51}, {"referenceID": 30, "context": "8m Constituency Tree\u2013LSTM\u2013GloVe (Tai et al., 2015) 88.", "startOffset": 32, "endOffset": 50}, {"referenceID": 30, "context": "8m Dependency Tree-LSTM (Tai et al., 2015) 85.", "startOffset": 24, "endOffset": 42}, {"referenceID": 19, "context": "Semantic relatedness The second task is to predict the degree of relatedness of two sentences from the Sentences Involving Compositional Knowledge corpus (SICK; Marelli et al., 2014) .", "startOffset": 154, "endOffset": 182}, {"referenceID": 11, "context": "Some of these models (Lai & Hockenmaier, 2014; Jimenez et al., 2014; Bjerva et al., 2014) rely on feature engineering and are designed specifically for this task.", "startOffset": 21, "endOffset": 89}, {"referenceID": 0, "context": "Some of these models (Lai & Hockenmaier, 2014; Jimenez et al., 2014; Bjerva et al., 2014) rely on feature engineering and are designed specifically for this task.", "startOffset": 21, "endOffset": 89}, {"referenceID": 0, "context": ", 2014; Bjerva et al., 2014) rely on feature engineering and are designed specifically for this task. Our Tree LSTM implementation performs competitively with most models in terms of mean squared error. Our best model\u2014semi-supervised syntax\u2014is better than most models except LSTM models of Tai et al. (2015) which were trained with a different objective function.", "startOffset": 8, "endOffset": 308}, {"referenceID": 11, "context": "369 UNAL-NLP(Jimenez et al., 2014) 0.", "startOffset": 12, "endOffset": 34}, {"referenceID": 0, "context": "356 Meaning Factory (Bjerva et al., 2014) 0.", "startOffset": 20, "endOffset": 41}, {"referenceID": 27, "context": "322 DT-RNN (Socher et al., 2014) 0.", "startOffset": 11, "endOffset": 32}, {"referenceID": 30, "context": "382 Mean Vectors (Tai et al., 2015) 0.", "startOffset": 17, "endOffset": 35}, {"referenceID": 30, "context": "456 650k Left to Right LSTM (Tai et al., 2015) 0.", "startOffset": 28, "endOffset": 46}, {"referenceID": 30, "context": "0m Bidirectional LSTM (Tai et al., 2015) 0.", "startOffset": 22, "endOffset": 40}, {"referenceID": 30, "context": "0m Constituency Tree-LSTM (Tai et al., 2015) 0.", "startOffset": 26, "endOffset": 44}, {"referenceID": 30, "context": "0m Dependency Tree-LSTM (Tai et al., 2015) 0.", "startOffset": 24, "endOffset": 42}, {"referenceID": 2, "context": ", recognizing textual entailment) using the Stanford Natural Language Inference corpus (SNLI; Bowman et al., 2015) .", "startOffset": 87, "endOffset": 114}, {"referenceID": 2, "context": ", recognizing textual entailment) using the Stanford Natural Language Inference corpus (SNLI; Bowman et al., 2015) . Natural language inference aims to predict whether two sentences are entailment, contradiction, or neutral, which can be formulated as a three-way classiciation problem. Given a pair of sentences, similar to the previous task, we use Tree LSTM to create sentence representations {s1, s2} \u2208 R for each of the sentences. Following Bowman et al. (2016), we construct our prediction by computing: u = (s2\u2212s1), v = s1 s2, q = ReLU(Wp[u,v, s1, s2]+bp), and p(\u0177 = c | q;wq) \u221d exp(wq,cq+ bq), where Wp \u2208 R,bp \u2208 R,wq \u2208 R, bq \u2208 R are model parameters.", "startOffset": 94, "endOffset": 467}, {"referenceID": 30, "context": "Our experiments with the regularized KL-divergence objective function (Tai et al., 2015) do not result in significant improvements, so we choose to report results with the simpler mean squared error objective function.", "startOffset": 70, "endOffset": 88}, {"referenceID": 2, "context": "Interestingly, the sequential left to right model is better than the supervised recursive model in our experiments, which contradicts results from Bowman et al. (2016) that show 300D-LSTM is worse than 300D-SPINN.", "startOffset": 147, "endOffset": 168}, {"referenceID": 2, "context": "Interestingly, the sequential left to right model is better than the supervised recursive model in our experiments, which contradicts results from Bowman et al. (2016) that show 300D-LSTM is worse than 300D-SPINN. A possible explanation is that our left to right model has identical number of parameters with the supervised model due to the inclusion of the tracking LSTM even in the left to right model (the only difference is in the composition order), whereas the models in Bowman et al. (2016) have different number of parameters.", "startOffset": 147, "endOffset": 498}, {"referenceID": 2, "context": "3m 100D-LSTM (Bowman et al., 2015) 77.", "startOffset": 13, "endOffset": 34}, {"referenceID": 3, "context": "7m 300D-LSTM (Bowman et al., 2016) 80.", "startOffset": 13, "endOffset": 34}, {"referenceID": 3, "context": "5m 300D-SPINN (Bowman et al., 2016) 83.", "startOffset": 14, "endOffset": 35}, {"referenceID": 31, "context": "2m 1024D-GRU (Vendrov et al., 2016) 81.", "startOffset": 13, "endOffset": 35}, {"referenceID": 20, "context": "0m 300D-CNN (Mou et al., 2016) 82.", "startOffset": 12, "endOffset": 30}, {"referenceID": 14, "context": "This is a similar setup to the Skip Thought objective (Kiros et al., 2015), except that we do not generate the previous sentence as well.", "startOffset": 54, "endOffset": 74}, {"referenceID": 7, "context": "We use IMDB movie review corpus (Diao et al., 2014) for this experiment, The corpus consists of 280,593, 33,793, and 34,029 reviews in training, development, and test sets respectively.", "startOffset": 32, "endOffset": 51}], "year": 2016, "abstractText": "We use reinforcement learning to learn tree-structured neural networks for computing representations of natural language sentences. In contrast with prior work on tree-structured models in which the trees are either provided as input or predicted using supervision from explicit treebank annotations, the tree structures in this work are optimized to improve performance on a downstream task. Experiments demonstrate the benefit of learning task-specific composition orders, outperforming both sequential encoders and recursive encoders based on treebank annotations. We analyze the induced trees and show that while they discover some linguistically intuitive structures (e.g., noun phrases, simple verb phrases), they are different than conventional English syntactic structures.", "creator": "LaTeX with hyperref package"}}}