{"id": "1611.09235", "review": {"conference": "AAAI", "VERSION": "v1", "DATE_OF_SUBMISSION": "28-Nov-2016", "title": "Joint Copying and Restricted Generation for Paraphrase", "abstract": "Many natural language generation tasks, such as abstractive summarization and text simplification, are paraphrase-orientated. In these tasks, copying and rewriting are two main writing modes. Most previous sequence-to-sequence (Seq2Seq) models use a single decoder and neglect this fact. In this paper, we develop a novel Seq2Seq model to fuse a copying decoder and a restricted generative decoder. The copying decoder finds the position to be copied based on a typical attention model. The generative decoder produces words limited in the source-specific vocabulary. To combine the two decoders and determine the final output, we develop a predictor to predict the mode of copying or rewriting. This predictor can be guided by the actual writing mode in the training data. We conduct extensive experiments on two different paraphrase datasets. The result shows that our model outperforms the state-of-the-art approaches in terms of both informativeness and language quality.", "histories": [["v1", "Mon, 28 Nov 2016 16:49:37 GMT  (79kb,D)", "http://arxiv.org/abs/1611.09235v1", "7 pages, 1 figure, AAAI-17"]], "COMMENTS": "7 pages, 1 figure, AAAI-17", "reviews": [], "SUBJECTS": "cs.CL cs.IR", "authors": ["ziqiang cao", "chuwei luo", "wenjie li", "sujian li"], "accepted": true, "id": "1611.09235"}, "pdf": {"name": "1611.09235.pdf", "metadata": {"source": "CRF", "title": "Joint Copying and Restricted Generation for Paraphrase", "authors": ["Ziqiang Cao", "Chuwei Luo", "Wenjie Li", "Sujian Li"], "emails": ["cswjli}@comp.polyu.edu.hk", "luochuwei@whu.edu.cn", "lisujian@pku.edu.cn"], "sections": [{"heading": "Introduction", "text": "This year, more than ever before in the history of the country in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is not a country, but in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a city and in which it is a country."}, {"heading": "Background: Seq2Seq Models and Attention Mechanism", "text": "The Seq2Seq models have been successfully applied to a number of natural language generation tasks, such as machine translation (Bahdanau, Cho, and Bengio 2014), response generation (Shang, Lu, and Li 2015), and abstract summary (Rush, Chopra, and Weston 2015).With these models, the source sequence X = [x1, \u00b7 \u00b7, xn] is converted into a fixed-length context vector c, usually by a recursive Neural Network Encoder (RNN), i.e., the source sequence X = [x1, \u00b7, xn] is converted into a recursive context vector c in which the RNN states are, f is the dynamic function, and this summarizes the hidden states, e.g. the choice of the last state.The decoder unfolds the context vector c into the target BenNN state by the dynamics similar to the codist = 1 (followed by yst = 1)."}, {"heading": "Method", "text": "As shown in Figure 1, CoRe is based on the encoder decoder structure. The source sequence is transformed by an RNN encoder into the context representation, which is then read by another RNN decoder to generate the target sequence."}, {"heading": "Encoder", "text": "We follow the work of (Bahdanau, Cho, and Bengio 2014) to build the encoder. Specifically, we use the Gated Recurrent Unit (GRU) as a recursive unit, which often performs much better than the vanilla RNN. Bidirectional RNN is introduced to inform the hidden state manufacturer of the contextual information from both sides. Afterwards, we use the attention mechanism to build the context vector as Equation 2. However, unlike most previous work, we re-use the learned alignments (Equation 5) in the decoders."}, {"heading": "Decoder", "text": "Instead of using canonical RNN decoders such as (Bahdanau, Cho, and Bengio 2014), we are only able to develop two different decoders to simulate copying and rewriting behaviors. (In Figure 5, most keywords from the original document can be interpreted as the copy probability distribution.) Therefore, the output of the copying decoder is as follows: pC (yt < t), X), if yt = xp = xp 0, otherwise (7) Most previous works use the context mechanism as a module to build the context vector."}, {"heading": "Learning", "text": "The cost function in our model is the sum of two parts, i.e. = 1 + 2 (13) The first 1 is the difference between the output {yt} and the actual target sequence {y \u0445 t}. As standard practice, we use the cross entropy (CE) to measure the difference in probability distributions: \u03b51 = \u2212 \u2211 t ln (p (y \u0445 t | y < t, X))) (14) In most existing Seq2Seq models, 1 is the final cost function. In our model, however, we include another cost function 2, which is derived from predicting the spellings. As shown in Equation 10, a process for labeling the binary sequences in our model predicts whether the current target word will be copied or not. 2 measures the performance in this task, \u03b52 = \u2212 (\u0445 t (\u0445 t) ln (1 \u2212 \u0445 t) ln (1 \u2212 \u0441\u0442\u0442\u0430\u0441\u0442\u043e\u043b\u043e\u043b\u043e\u043b\u043e\u0442\u0438\u0442\u0435\u0442\u0435\u0442\u0435\u0442\u0442\u0442\u0442\u0442t) ln (1 \u2212 \u0441\u0442\u043e\u043b\u043e\u043b\u043e\u0442\u043e\u0442\u043e\u0442\u0435\u0442\u0435\u0442\u0435\u0442\u0435\u0442\u0435\u0442\u0435t) ln) (1)) (2) (15))."}, {"heading": "Dataset", "text": "We test our model using the following two paraphrase-oriented tasks: 1. One-sentence summary 2. Text simplification One-sentence summary is the use of a condensed sentence (aka. highlight) to describe the main idea of a document. This task facilitates efficient reading. Text simplification modifies a document in such a way that grammar and vocabulary are greatly simplified while the underlying meaning remains the same. It is able to make the scientific documents easily understandable to outsiders. We build data sets for both tasks based on existing work. One-sentence summary: For this task we need a corpus consisting of < document, highlighting (one-sentence summary) > pairs. We modify an existing corpus used for the task of the passage-based question (Hermann et al. 2015)."}, {"heading": "Implementation", "text": "Based on the validation data set, we set the word embedding dimension to 256 and the hidden state dimension to 512. The initial learning rate is 0.05 and the lot size is 32. Our implementation is based on the standard Seq2Seq model dl4mt3 under the Theano Framework4. We use the popular Fast Align tool (Dyer, Chahuneau and Smith 2013) to create the source-target word alignment table A. The vocabulary of our generative decoder is limited in the top 10 source word alignments as well as in 2000 common words. Although our model is more complex than the attention-grabbing standard model Seq2Seq, it takes only two thirds of the time for training and testing."}, {"heading": "Evaluation Method", "text": "The informativeness is evaluated on the basis of ROUGE5 (Lin 2004), which was regarded as the standard for the automatic summary of evaluation metrics. ROUGE counts the overlapping units such as the n-grams, word strings and word pairs between the candidate text Y and the actual target text T. As usual practice, we take ROUGE-1 and ROUGE-23https: / / github.com / nyu-dl / dl4mt-multi 4http: / / deeplearning.net / software / theano / 5ROUGE-1.5.5 with options: -n 2 -m -u -c 95 -x -r 1000 -f A -p0.5 -t 0.scores as the most important indicators. They measure the similarities between the unigram and the bi-gram. For example, the f-score of ROUGE-2 for the text quality is calculated as follows: ROUGE \u2212 2f - score = 2 x-b-Y min {NY (b)."}, {"heading": "Baselines", "text": "We compare the proposed model CoRe with various typical methods. First, we introduce the standard baseline called \"LEAD.\" It simply selects the \"leading\" words from the source as output. According to the average target length in Table 1, we select the first 20 words for summary and 25 for simplification. We also introduce the modern statistical translation system Moses (Koehn et al. 2007) and the Seq2Seq model ABS (Rush, Chopra and Weston 2015). Moses is the prevailing statistical approach to machine translation. It takes into account parallel data and uses the simultaneous occurrence of words and phrases to close translation correspondences. To make a fair comparison, we also use the alignment tool Fast Align and the language model tool SRILM when implementing Moses. ABS is a Seq2Seq model with the attention mechanism. It is similar to the machine translation proposed in Bahrain."}, {"heading": "Performance", "text": "Consider first the computerization performance. As can be seen, CoRe achieves the highest ROUGE values in both summarizing and simplifying the text. In contrast, the attention-oriented standard Seq2Seq model ABS is slightly inferior to the basic BLEAD model in terms of summary for paraphrase-oriented tasks. Obviously, the introduction of the copying and restricted generation mechanisms is crucial for the paraphrase-oriented tasks. 6http: / / www.speech.sri.com / projects / srilm / Then we check the quality of the generated sentences. According to PPL, the sentences generated by CoRe are most similar to the target language. It is interesting that LEAD extracts the human-written text from the source. Nevertheless, its PPL is significantly higher than CoRe on both sets of data. Although the CoRe words that are actually generated by PQL resemble the characteristics of the target language most closely than we control the target language."}, {"heading": "Case Study", "text": "For example, as the Wikipedia article illustrates, Simple Wikipedia usually adopts the following pattern to describe a community: # NAME is a community. It can be found in # LOCATION.CoRe captures many common colloquial rules, and there are more than 130 cases where CoRe's generating results hit exactly the actual target sentences. Therefore, we focus more on analyzing the summary results than next. In the summary, although most of the target words come from the copy decoder, we find that CoRe tends to select keywords from different parts of the source document. By contrast, the default, attentive Seq2Seq model is often a summary of the summarized source words."}, {"heading": "Related Work", "text": "The Seq2Seq model is an emerging approach, originally proposed by (Kalchburner and Blunsom 2013; Sutskever, Vinyals and Le 2014; Cho et al. 2014) for machine translation. Compared with traditional approaches to machine translation (e.g., (Koehn et al. 2007), Seq2Seq models require less human effort. Later, (Bahdanau, Cho, and Bengio 2014), the attention mechanism that largely promoted the applications of the Seq2Seq models developed. In addition to machine translation, Seq2Seq models achieved the performance of the state in many other tasks such as reaction generation (Shang, Lu, and Li 2015). Some research (Rush, Chopra, and Weston 2015; Hu, Chen, and Zhu 2015) are unable to apply the general Seq2Seq2Seq2Seq model."}, {"heading": "Conclusion and Future Work", "text": "In this paper, we develop a novel Seq2Seq model called CoRe to simulate the two central spellings in paraphrasing, i.e. copying and paraphrasing. CoRe fuses a copy decoder with a limited generative decoder. In order to combine the two decoders and determine the final output, we train a predictor for predicting the spellings. We conduct extensive experiments with two different paraphrase-oriented data sets. The result shows that our model outperforms the most modern approaches in terms of both information power and speech quality. Currently, our model focuses on the production of a single sentence. We plan to extend it to the production of multi-sentence documents.Recognition The work described in this paper has been supported by the Research Grants Council of Hong Kong (PolyU 152094 / 14E), the National Natural Science Foundation of China (61272291, 61672445) and the Hong Kong Polytechnic University (YP6, BjLi B)."}], "references": [{"title": "A convolutional attention network for extreme summarization of source code. arXiv preprint arXiv:1602.03001", "author": ["Peng Allamanis", "M. Sutton 2016] Allamanis", "H. Peng", "C. Sutton"], "venue": null, "citeRegEx": "Allamanis et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Allamanis et al\\.", "year": 2016}, {"title": "Neural machine translation by jointly learning to align and translate", "author": ["Cho Bahdanau", "D. Bengio 2014] Bahdanau", "K. Cho", "Y. Bengio"], "venue": "arXiv preprint arXiv:1409.0473", "citeRegEx": "Bahdanau et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Bahdanau et al\\.", "year": 2014}, {"title": "On the properties of neural machine translation: Encoder-decoder approaches", "author": ["Cho"], "venue": "arXiv preprint arXiv:1409.1259", "citeRegEx": "Cho,? \\Q2014\\E", "shortCiteRegEx": "Cho", "year": 2014}, {"title": "On using very large target vocabulary for neural machine translation", "author": ["Memisevic Cho", "S.J.K. Bengio 2015] Cho", "R. Memisevic", "Y. Bengio"], "venue": null, "citeRegEx": "Cho et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Cho et al\\.", "year": 2015}, {"title": "A simple, fast, and effective reparameterization of ibm model 2. Association for Computational Linguistics", "author": ["Chahuneau Dyer", "C. Smith 2013] Dyer", "V. Chahuneau", "N.A. Smith"], "venue": null, "citeRegEx": "Dyer et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Dyer et al\\.", "year": 2013}, {"title": "Incorporating copying mechanism in sequence-to-sequence learning", "author": ["Gu"], "venue": "arXiv preprint arXiv:1603.06393", "citeRegEx": "Gu,? \\Q2016\\E", "shortCiteRegEx": "Gu", "year": 2016}, {"title": "Teaching machines to read and comprehend", "author": ["Hermann"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Hermann,? \\Q2015\\E", "shortCiteRegEx": "Hermann", "year": 2015}, {"title": "Lcsts: A large scale chinese short text summarization dataset. arXiv preprint arXiv:1506.05865", "author": ["Chen Hu", "B. Zhu 2015] Hu", "Q. Chen", "F. Zhu"], "venue": null, "citeRegEx": "Hu et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Hu et al\\.", "year": 2015}, {"title": "Recurrent continuous translation models", "author": ["Kalchbrenner", "N. Blunsom 2013] Kalchbrenner", "P. Blunsom"], "venue": "In EMNLP,", "citeRegEx": "Kalchbrenner et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Kalchbrenner et al\\.", "year": 2013}, {"title": "Moses: Open source toolkit for statistical machine translation", "author": ["Koehn"], "venue": "In Proceedings of the 45th annual meeting of the ACL on interactive", "citeRegEx": "Koehn,? \\Q2007\\E", "shortCiteRegEx": "Koehn", "year": 2007}, {"title": "Rouge: A package for automatic evaluation of summaries", "author": ["Lin", "C.-Y"], "venue": "In Proceedings of the ACL Workshop,", "citeRegEx": "Lin and C..Y.,? \\Q2004\\E", "shortCiteRegEx": "Lin and C..Y.", "year": 2004}, {"title": "A neural attention model for abstractive sentence summarization", "author": ["Chopra Rush", "A.M. Weston 2015] Rush", "S. Chopra", "J. Weston"], "venue": "In Proceedings of EMNLP,", "citeRegEx": "Rush et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Rush et al\\.", "year": 2015}, {"title": "Neural responding machine for short-text conversation", "author": ["Lu Shang", "L. Li 2015] Shang", "Z. Lu", "H. Li"], "venue": "arXiv preprint arXiv:1503.02364", "citeRegEx": "Shang et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Shang et al\\.", "year": 2015}, {"title": "Sequence to sequence learning with neural networks. In Advances in neural information processing systems, 3104\u20133112", "author": ["Vinyals Sutskever", "I. Le 2014] Sutskever", "O. Vinyals", "Q.V. Le"], "venue": null, "citeRegEx": "Sutskever et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Sutskever et al\\.", "year": 2014}, {"title": "Lecture 6.5-rmsprop: Divide the gradient by a running average of its recent magnitude. COURSERA: Neural Networks for Machine Learning", "author": ["Tieleman", "T. Hinton 2012] Tieleman", "G. Hinton"], "venue": null, "citeRegEx": "Tieleman et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Tieleman et al\\.", "year": 2012}], "referenceMentions": [], "year": 2016, "abstractText": "Many natural language generation tasks, such as abstractive summarization and text simplification, are paraphrase-orientated. In these tasks, copying and rewriting are two main writing modes. Most previous sequence-to-sequence (Seq2Seq) models use a single decoder and neglect this fact. In this paper, we develop a novel Seq2Seq model to fuse a copying decoder and a restricted generative decoder. The copying decoder finds the position to be copied based on a typical attention model. The generative decoder produces words limited in the source-specific vocabulary. To combine the two decoders and determine the final output, we develop a predictor to predict the mode of copying or rewriting. This predictor can be guided by the actual writing mode in the training data. We conduct extensive experiments on two different paraphrase datasets. The result shows that our model outperforms the stateof-the-art approaches in terms of both informativeness and language quality.", "creator": "LaTeX with hyperref package"}}}