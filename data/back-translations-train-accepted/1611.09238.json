{"id": "1611.09238", "review": {"conference": "AAAI", "VERSION": "v1", "DATE_OF_SUBMISSION": "28-Nov-2016", "title": "Improving Multi-Document Summarization via Text Classification", "abstract": "Developed so far, multi-document summarization has reached its bottleneck due to the lack of sufficient training data and diverse categories of documents. Text classification just makes up for these deficiencies. In this paper, we propose a novel summarization system called TCSum, which leverages plentiful text classification data to improve the performance of multi-document summarization. TCSum projects documents onto distributed representations which act as a bridge between text classification and summarization. It also utilizes the classification results to produce summaries of different styles. Extensive experiments on DUC generic multi-document summarization datasets show that, TCSum can achieve the state-of-the-art performance without using any hand-crafted features and has the capability to catch the variations of summary styles with respect to different text categories.", "histories": [["v1", "Mon, 28 Nov 2016 16:53:06 GMT  (161kb,D)", "http://arxiv.org/abs/1611.09238v1", "7 pages, 3 figures, AAAI-17"]], "COMMENTS": "7 pages, 3 figures, AAAI-17", "reviews": [], "SUBJECTS": "cs.CL cs.IR", "authors": ["ziqiang cao", "wenjie li", "sujian li", "furu wei"], "accepted": true, "id": "1611.09238"}, "pdf": {"name": "1611.09238.pdf", "metadata": {"source": "CRF", "title": "Improving Multi-Document Summarization via Text Classification", "authors": ["Ziqiang Cao", "Wenjie Li", "Sujian Li", "Furu Wei"], "emails": ["cswjli}@comp.polyu.edu.hk", "lisujian@pku.edu.cn", "fuwei@microsoft.com"], "sections": [{"heading": "Introduction", "text": "This year, more than ever before in the history of a country in which it is a country in which it is a country in which it is a country in which it is a country, a country in which it is a country, a country, a country, a country, a city, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a"}, {"heading": "Method", "text": "We assume that D belongs to a set of C, i.e., cD [1, | C |], where cD represents the actual category for document D. The text classification model is trained to predict a category for D. For the supervised sentence ranking required for a learning-based summary, each sentence contains a saliency score, which is usually measured in relation to human summaries (hereinafter the reference summaries).The summarization model is designed to learn how to classify sentences in accordance with the actual sentence licensation.In this section, we describe how our summarization system, called TCSum, classifies sentences using text classification (hereinafter the reference summarization).The overall frame of TCSum is illustrated in Fig. 1."}, {"heading": "Text Classification Model", "text": "In this essay, we develop a simple CNN-based classification model. Specifically, we use a CNN to project a sentence s onto its distributed representation v (s), i.e., s CNN \u2212 \u2212 \u2212 \u2212 v (s) (1) A basic CNN contains a folding operation on the top of the word embeddings followed by a pooling operation. Let v (wi).Rk refer to the k-dimensional word embeddings corresponding to the ith word in the sentence. Let v (wi + j) concatenate the word embeddings [v (wi), \u00b7 v (wi + j).m The convolution includes a filter corresponding to the ith word in the sentence."}, {"heading": "Summarization Model", "text": "As already mentioned, the summary model in TCSum shares the same folding and pooling operations with the classification model in generating the document into which v (D) is embedded. Then, TCSum transforms v (D) to match the \"meaning\" of the reference summary, i.e., vS (D) = tanh (W\u03b3 \u00b7 v (D), (6) where the transformed embedding is called summary embedding, and W\u03b3 Rm \u00b7 m is the transformation matrix. Note that we define the same dimension for both the document and the summary embedding. This setting simplifies the process of sentence classification, which will be explained later. We also want the summary embedding to contain the information of summary styles. Inspired by the work of (Dong et al. 2014), we are developing the category-specific transformation matrix matrix matrix matrix-matrix-matrix-prediction category."}, {"heading": "Training", "text": "Therefore, there are three types of weight matrices in our models, i.e. W\u03b1, W\u03b2 and transformation submatrices (W1\u03b3, \u00b7 \u00b7 \u00b7, W | C | \u03bb). Since the text classification data set is much larger than the summation data set, W\u03b1 and W\u03b2 are only learned from the classification data. Nevertheless, the transformation matrices must be trained with the summation data. For text classification, we use cross-entropy as a cost function, i.e., \u03b5C (D) \u2212 C | i = 1 {cD = = i} lnviC (D), (9) where 1 {cD = = i} 1 iff is equal to the actual category i. Under this cost function, the gradient of softmax is similar to a linear function that fixes the training process."}, {"heading": "Experiments", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "Datasets", "text": "Summary The most commonly used assessment corpora for the summary are those published by the Document Understanding Conferences (DUC) and Text Analytics Conferences (TAC2). In this work, we focus on the generic multi-document summary task performed in DUC 2001, 2002 and 2004. The documents are all from the news area and a collection of documents related to the same topic are grouped into a cluster. Each cluster is accompanied by 2 to 4 reference summaries by human experts. Our summary model summarizes the documents into a single document. Table 1 shows the size of the three records and the summary length restriction for each task. DUC records come from a wide range of categories, and we manually categorize the DUC documents into 11 categories, i.e., Biography, Culture, Health, Politics, Law, Society, Sports and International."}, {"heading": "Evaluation Metric for Summarization", "text": "ROUGE measures the quality of the summary by counting overlapping units such as N-grams, word strings, and pairs of words between the candidate summary and the reference summary. In accordance with common practice, we take ROUGE-1 and ROUGE-2 recall scores as the most important parameters for comparison. ROUGE-1 and ROUGE-2 measure the similarities of unigram and bi-gram, respectively. During the training, the actual emphasis of a sentence (equivalent. 10) by ROUGE-2 is also evaluated."}, {"heading": "Model Settings", "text": "For CNN, we are introducing a word embedding set trained on a large English news corpus (1010 characters) using word2vec (Mikolov et al. 2013).The dimension of word embedings is set to 50, as in many previous essays (e.g. (Collobert et al. 2011)).We are also setting the dimension of sentence and document embedings to the dimension of word embedings and the window size h to 2, in order to be consistent with the ROUGE-2 rating. Empirically, we are setting the threshold of the time-wise ranking = 0.1. The initial learning rate is 0.1 and the stack size is 128.A summary is obliged to provide both informative and non-redundant contents.While TCSum focuses on sentence-rank-4ROUGE-1.5.5 with options: -n 2 -m -u -c 95 -x -f A -p 0.5 -t 0. The parameter of the length restriction is 2001 \"4.6C-DU6c-2006b-DU65\" and DU6b-2006b-DU6b-2016b-2016c-2006b."}, {"heading": "Baseline Methods", "text": "We compare TCSum with the best peer systems participating in DUC assessments, called \"peer\" plus their IDs. In addition, we include R2N2 (Cao et al. 2015a) 5, an art-supervised summary model based on neural networks. It uses the recursive neural network to learn the combination of handcrafted features. It is noteworthy that R2N2 still relies heavily on handcrafted features. In contrast, TCSum is completely data driven, i.e., features are all learned automatically. We implement a widely used learning-based summary method that includes support for vector regression (SVR) (Li et al. 2007). It extracts a number of manually compiled features from a set, such as TF (the frequency of a word in the cluster), CF (the frequency of a word in the cluster model wlev), the number of documents that is used (the number of the word in the UMF), and the number of documents (the number of the document in the cluster)."}, {"heading": "Summarization Performance", "text": "The ROUGE values of the models to be compared are shown in Table 2. We draw lines in this table to distinguish the models with and without handmade features. As can be seen, TCSum achieves the highest performance on all three sets of data among models that are entirely dependent on auto-learned features. EmSim's poor performance means that we could not directly use the embeddings gained from text classification to measure the emphasis of sentences for the summary. Note that even NoTC achieves competitive performance with SVR. Therefore, summation models without handmade features are feasible. Meanwhile, SingleT far outperforms the NoTC. It confirms that text classification can actually help find a summation model to learn better document representation. Although TCSum does not always outperform the SingleT model in terms of ROUGEs, we will show in the next section that it generally captures different styles."}, {"heading": "Discussion on Summary Style Learning", "text": "\"We study the ability of TCSum to learn summary styles in two ways. First, we speculate that similar transformation matrices tend to generate summaries with similar styles. Therefore, we calculate the similarity between the transformation matrices (W1\u03b3, W | C | \u03bb). Here, we flatten each matrix into a vector and use cosmic similarity to measure the similarity. The scores of the different transformation matrices are presented in Fig. 3. For the simplicity of the propositions, we show only the results of the three common categories on DUCs, i.e., biography, politics and natural disasters that can be seen, the similarity relationships between these three categories vary, which correspond to the intuition that the big difference of summary styles between these categories exists, we find its transformations on DUCs, i.e., politics and natural disasters."}, {"heading": "Related Work", "text": "Using unsupervised methods, one of the well-known approaches is Maximum Marginal Relevance (MMR) (Carbonell and Goldstein 1998), using a greedy approach to sentence selection and looking at the trade-off between emphasis and redundancy, good results were achieved by reformulating it as an Integrated Linear Program-ming (ILP) problem that was able to find the global optimal solution (McDonald 2007; Gillick and Favre 2009). Graph-based models such as Manifold (Wan and Xiao 2009) played an important role in the extractive summary because it reflected different sentence relationships. In contrast to these unverified methods, there are also many successful learning-based summary techniques. Various classifiers have been studied, including Uditional Random Field (Galley 2006), support for Vector Regression (Li et al. 2007) and Logistic Regression (Li auto, Qi-System, Deep Handled Networks)."}, {"heading": "Conclusion and Future Work", "text": "In this paper, we propose a novel summary system called TCSum that uses text classification to improve summary performance. Extensive experiments with DUC generic summary benchmark datasets show that TCSum delivers the most advanced performance, even without handcrafted features. We also observe that TCSum actually captures variations in summary styles between different text categories. We believe that our model can be used for other summary tasks, including query-focused summary and guided summary. In addition, we plan to have the model distinguish documents in a topic cluster that is better suited to the summary of multiple documents. Recognition The work described in this paper has been supported by the Research Grants Council of Hong Kong (PolyU 152094 / 14E), the National Natural Science Foundation of China (61272291, 61672445), and Hong Kong Polytechnic University (GYLi-B5, Y5-Q5 authors Li)."}], "references": [{"title": "Ranking with recursive neural networks", "author": ["Cao"], "venue": null, "citeRegEx": "Cao,? \\Q2015\\E", "shortCiteRegEx": "Cao", "year": 2015}, {"title": "Learning summary prior representation for extractive summarization", "author": ["Cao"], "venue": "Proceedings of ACL: Short Papers", "citeRegEx": "Cao,? \\Q2015\\E", "shortCiteRegEx": "Cao", "year": 2015}, {"title": "2016a. Tgsum: Build tweet guided multidocument summarization dataset", "author": ["Cao"], "venue": "Proceedings of AAAI", "citeRegEx": "Cao,? \\Q2016\\E", "shortCiteRegEx": "Cao", "year": 2016}, {"title": "2016b. Attsum: Joint learning of focusing and summarization with neural attention", "author": ["Cao"], "venue": "Proceedings of COLING", "citeRegEx": "Cao,? \\Q2016\\E", "shortCiteRegEx": "Cao", "year": 2016}, {"title": "The use of mmr, diversity-based reranking for reordering documents and producing summaries", "author": ["Carbonell", "J. Goldstein 1998] Carbonell", "J. Goldstein"], "venue": "In Proceedings of SIGIR,", "citeRegEx": "Carbonell et al\\.,? \\Q1998\\E", "shortCiteRegEx": "Carbonell et al\\.", "year": 1998}, {"title": "Neural summarization by extracting sentences and words. arXiv preprint arXiv:1603.07252", "author": ["Cheng", "J. Lapata 2016] Cheng", "M. Lapata"], "venue": null, "citeRegEx": "Cheng et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Cheng et al\\.", "year": 2016}, {"title": "Natural language processing (almost) from scratch", "author": ["Collobert"], "venue": "The Journal of Machine Learning Research 12:2493\u20132537", "citeRegEx": "Collobert,? \\Q2011\\E", "shortCiteRegEx": "Collobert", "year": 2011}, {"title": "Adaptive multi-compositionality for recursive neural models with applications to sentiment analysis", "author": ["Dong"], "venue": "Proceedings of AAAI", "citeRegEx": "Dong,? \\Q2014\\E", "shortCiteRegEx": "Dong", "year": 2014}, {"title": "Adaptive subgradient methods for online learning and stochastic optimization", "author": ["Hazan Duchi", "J. Singer 2011] Duchi", "E. Hazan", "Y. Singer"], "venue": "The Journal of Machine Learning Research 12:2121\u20132159", "citeRegEx": "Duchi et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Duchi et al\\.", "year": 2011}, {"title": "Sentence compression by deletion with lstms", "author": ["Filippova"], "venue": "In Proceedings of EMNLP,", "citeRegEx": "Filippova,? \\Q2015\\E", "shortCiteRegEx": "Filippova", "year": 2015}, {"title": "A skip-chain conditional random field for ranking meeting utterances by importance", "author": ["M. Galley"], "venue": "In Proceedings of EMNLP,", "citeRegEx": "Galley,? \\Q2006\\E", "shortCiteRegEx": "Galley", "year": 2006}, {"title": "Deep learning for automatic summary scoring", "author": ["Gotti Genest", "P.-E. Bengio 2011] Genest", "F. Gotti", "Y. Bengio"], "venue": "In Proceedings of the Workshop on Automatic Text Summarization,", "citeRegEx": "Genest et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Genest et al\\.", "year": 2011}, {"title": "A scalable global model for summarization", "author": ["Gillick", "D. Favre 2009] Gillick", "B. Favre"], "venue": "In Proceedings of the Workshop on ILP for NLP,", "citeRegEx": "Gillick et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Gillick et al\\.", "year": 2009}, {"title": "Improving the estimation of word importance for news multi-document summarization", "author": ["Hong", "K. Nenkova 2014] Hong", "A. Nenkova"], "venue": "In Proceedings of EACL", "citeRegEx": "Hong et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Hong et al\\.", "year": 2014}, {"title": "Predicting salient updates for disaster summarization", "author": ["McKeown Kedzie", "C. Diaz 2015] Kedzie", "K. McKeown", "F. Diaz"], "venue": "In Proceedings of ACL,", "citeRegEx": "Kedzie et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Kedzie et al\\.", "year": 2015}, {"title": "Summarization based on embedding distributions", "author": ["Noguchi Kobayashi", "H. Yatsuka 2015] Kobayashi", "M. Noguchi", "T. Yatsuka"], "venue": "In Proceedings of EMNLP,", "citeRegEx": "Kobayashi et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Kobayashi et al\\.", "year": 2015}, {"title": "Recurrent convolutional neural networks for text classification", "author": ["Lai"], "venue": "In Proceedings of AAAI,", "citeRegEx": "Lai,? \\Q2015\\E", "shortCiteRegEx": "Lai", "year": 2015}, {"title": "Multi-document summarization using support vector regression", "author": ["Li"], "venue": "In Proceedings of DUC", "citeRegEx": "Li,? \\Q2007\\E", "shortCiteRegEx": "Li", "year": 2007}, {"title": "Recursive deep models for discourse parsing", "author": ["Li Li", "J. Hovy 2014] Li", "R. Li", "E.H. Hovy"], "venue": "In Proceedings of EMNLP,", "citeRegEx": "Li et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Li et al\\.", "year": 2014}, {"title": "Using supervised bigram-based ilp for extractive summarization", "author": ["Qian Li", "C. Liu 2013] Li", "X. Qian", "Y. Liu"], "venue": "In Proceedings of ACL,", "citeRegEx": "Li et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Li et al\\.", "year": 2013}, {"title": "Rouge: A package for automatic evaluation of summaries", "author": ["Lin", "C.-Y"], "venue": "In Proceedings of the ACL Workshop,", "citeRegEx": "Lin and C..Y.,? \\Q2004\\E", "shortCiteRegEx": "Lin and C..Y.", "year": 2004}, {"title": "Efficient estimation of word representations in vector space. arXiv preprint arXiv:1301.3781", "author": ["Mikolov"], "venue": null, "citeRegEx": "Mikolov,? \\Q2013\\E", "shortCiteRegEx": "Mikolov", "year": 2013}, {"title": "A neural attention model for abstractive sentence summarization", "author": ["Chopra Rush", "A.M. Weston 2015] Rush", "S. Chopra", "J. Weston"], "venue": "In Proceedings of EMNLP,", "citeRegEx": "Rush et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Rush et al\\.", "year": 2015}, {"title": "Graphbased multi-modality learning for topic-focused multidocument summarization", "author": ["Wan", "X. Xiao 2009] Wan", "J. Xiao"], "venue": "In Proceedings of IJCAI,", "citeRegEx": "Wan et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Wan et al\\.", "year": 2009}, {"title": "Multi-document summarization via discriminative summary reranking", "author": ["Wan"], "venue": "arXiv preprint arXiv:1507.02062", "citeRegEx": "Wan,? \\Q2015\\E", "shortCiteRegEx": "Wan", "year": 2015}, {"title": "Optimizing sentence modeling and selection for document summarization", "author": ["Yin", "W. Pei 2015] Yin", "Y. Pei"], "venue": "In Proceedings of IJCAI,", "citeRegEx": "Yin et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Yin et al\\.", "year": 2015}], "referenceMentions": [{"referenceID": 10, "context": "Different classifiers have been explored, including Conditional Random Field (Galley 2006), Support Vector Regression (Li et al.", "startOffset": 77, "endOffset": 90}], "year": 2016, "abstractText": "Developed so far, multi-document summarization has reached its bottleneck due to the lack of sufficient training data and diverse categories of documents. Text classification just makes up for these deficiencies. In this paper, we propose a novel summarization system called TCSum, which leverages plentiful text classification data to improve the performance of multi-document summarization. TCSum projects documents onto distributed representations which act as a bridge between text classification and summarization. It also utilizes the classification results to produce summaries of different styles. Extensive experiments on DUC generic multidocument summarization datasets show that, TCSum can achieve the state-of-the-art performance without using any hand-crafted features and has the capability to catch the variations of summary styles with respect to different text categories.", "creator": "LaTeX with hyperref package"}}}