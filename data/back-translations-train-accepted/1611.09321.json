{"id": "1611.09321", "review": {"conference": "iclr", "VERSION": "v1", "DATE_OF_SUBMISSION": "28-Nov-2016", "title": "Improving Policy Gradient by Exploring Under-appreciated Rewards", "abstract": "This paper presents a novel form of policy gradient for model-free reinforcement learning (RL) with improved exploration properties. Current policy-based methods use entropy regularization to encourage undirected exploration of the reward landscape, which is ineffective in high dimensional spaces with sparse rewards. We propose a more directed exploration strategy that promotes exploration of {\\em under-appreciated reward} regions. An action sequence is considered under-appreciated if its log-probability under the current policy under-estimates its \\mbox{resulting} reward. The proposed exploration strategy is easy to implement, requiring small modifications to an implementation of the REINFORCE algorithm. We evaluate the approach on a set of algorithmic tasks that have long challenged RL methods. Our approach reduces hyper-parameter sensitivity and demonstrates significant improvements over baseline methods. Our algorithm successfully solves a benchmark multi-digit addition task and generalizes to long sequences. This is, to our knowledge, the first time that a pure RL method has solved addition using only reward feedback.", "histories": [["v1", "Mon, 28 Nov 2016 20:15:55 GMT  (391kb,D)", "http://arxiv.org/abs/1611.09321v1", "Under review at ICLR 2017"], ["v2", "Wed, 25 Jan 2017 22:35:03 GMT  (992kb,D)", "http://arxiv.org/abs/1611.09321v2", "Under review at ICLR 2017"], ["v3", "Wed, 15 Mar 2017 22:55:17 GMT  (995kb,D)", "http://arxiv.org/abs/1611.09321v3", "Published as a conference paper at ICLR 2017"]], "COMMENTS": "Under review at ICLR 2017", "reviews": [], "SUBJECTS": "cs.LG cs.AI", "authors": ["ofir nachum", "mohammad norouzi", "dale schuurmans"], "accepted": true, "id": "1611.09321"}, "pdf": {"name": "1611.09321.pdf", "metadata": {"source": "CRF", "title": "IMPROVING POLICY GRADIENT BY EXPLORING UNDER-APPRECIATED REWARDS", "authors": ["Ofir Nachum", "Mohammad Norouzi", "Dale Schuurmans"], "emails": ["ofirnachum@google.com", "mnorouzi@google.com", "schuurmans@google.com"], "sections": [{"heading": "1 INTRODUCTION", "text": "In fact, we are able to manoeuvre ourselves into a situation where we are able, in which we are able to change the world, in which we are able to change the world, and in which we are able to change the world, \"he said in an interview with the New York Times."}, {"heading": "2 NEURAL NETWORKS FOR LEARNING ALGORITHMS", "text": "Although research on the use of neural networks to learn algorithms has seen a recent surge in interest, the problem of program induction from examples has a long history in many areas, including program induction, inductive logic programming (Lavrac & Dzeroski, 1994), relational learning (Kemp et al., 2007), and regular language learning (Angulin, 1987).Instead of providing a comprehensive overview of program induction, we focus on neural networking approaches to algorithmic tasks and emphasize the relative simplicity of our neural network architecture.Most successful applications of neural networks rely on algorithmic tasks based on strong monitoring, where the inputs and outcomes are fully a prioritised.Given a dataset of examples, one learns the network parameters by maximizing the conditional probability of outputs via backpropagation (e.g. Reed & de Freitas (2016); Kaiser & Sutskever (Upskyal)."}, {"heading": "3 LEARNING A POLICY BY MAXIMIZING EXPECTED REWARD", "text": "We begin with the discussion of the most common form of policy gradient, REINFORCE (Williams, 1992), and its entropy-regulated variant (Williams & Peng, 1991).REINFORCE was designed for model-free policy-based learning using neural networks and algorithmic domains (Zaremba & Sutskever, 2015; Graves et al., 2016).As mentioned above, we aim to learn a policy that, in the face of an observed state, estimates a distribution over the next action at, which is designated as such.The environment is initialized with a latent vector that determines the initially observed state s1 = g, and the transition function st + f = f, at, h. In the face of a latent state h, and s1: T,."}, {"heading": "4 UNDER-APPRECIATED REWARD EXPLORATION (UREX)", "text": "In order to explain our novel form of the political gradient, we first point out that the optimal politics \u03c0 \u043d, which globally maximizes the ORL, is in relation to (1) a constant normalization constant, which requires a distribution over the space of action. One can verify this by first recognizing that ORL (h) = - \"objective reward\" (h) = - \"objective reward\" (h) - \"objective reward\" (h) - \"objective reward\" (h) - \"objective reward\" (h) - \"objective reward\" - \"objective\" - \"objective\" - \"objective\" - \"objective\" - \"objective\" - \"objective\" - \"objective\" - \"-\" objective \"-\" objective \"-\" - \"objective\" - \"- objective\" - \"(h) -\" objective \"- objective\" - \"- objective\" (h) - \"objective\" - objective \"(h) -\" objective \"(h) - objective\" objective \"-\" objective \"-\" - objective \"(h) -"}, {"heading": "5 RELATED WORK ON EXPLORATION IN REINFORCEMENT LEARNING", "text": "The most common exploration strategy considered in value-oriented strategies is -greedy Q-learning, in which the agent either takes the best measures at each step according to his current value adjustment or is likely to randomly select measures. Similar to entropy regularization, such an approach applies uncontrolled exploration, but it has recently achieved success in game environments (Mnih et al., 2013; Van Hasselt et al., 2016; Mnih et al., 2016). Prominent approaches to improving exploration over -greedy in value-oriented or model-based RL have focused on reducing uncertainty by prioritizing exploration toward states and measures where the agent knows least. This basic intuition underlies work on counter-and recession methods (Thrun, 1992), exploration methods that represent an optimization policy in 1993 (Uncertainty Policy, 2010)."}, {"heading": "6 SIX ALGORITHMIC TASKS", "text": "We evaluate the effectiveness of the proposed approach based on five algorithmic tasks from the OpenAI Gym (Brockman et al., 2016), as well as a new binary search problem. Each task is summarized below with further details available on the Gym website. 1. Copy: The agent should send a copy of the sequence. The environment has a hidden tape and a hidden sequence. 2. The agent observes the sequence via a pointer to a single character, which can be moved through a series of pointer control actions. Copy: The agent should send a copy of the sequence. Pointer actions are shifted left and right. 2. DuplicatedInput: In the hidden tape, each character is repeated twice. The agent must deduplicate the sequence and emit each other character. Pointer actions will be left and right.3. RepeatCopy: The agent should emit the hidden sequence, then reverse the sequence."}, {"heading": "7 EXPERIMENTS", "text": "We compare our policy gradient method, which uses underestimated reward exploration (UREX), with two key RL baselines: (1) REINFORCE with entropy regularization, known as MENT (Williams & Peng, 1991), where the value \u03c4 determines the degree of regularization."}, {"heading": "7.1 ROBUSTNESS TO HYPER-PARAMETERS", "text": "We found that the proposed UREX method significantly improves the robustness of hyperparameter changes when compared with the MENT method. For our experiments, we perform a careful search for hyperparameters using a set of hyperparameters for both MENT and UREX. \u2022 The learning rate, selected from a set of three possible values, begins with the MENT method and UREX methods five times with different random restarts. \u2022 The maximum L2 standard of gradients beyond which the gradients are truncated. \u2022 This parameter, called c, plays a role in the formation of RNNNs. The value of c is selected by c, 1, 10, 40, 100}. \u2022 The temperature parameters that regulate the degree of exploration for both MENT and the job."}, {"heading": "7.2 RESULTS", "text": "Table 2 shows the number of successful attempts (out of 5 random restarts) and the expected reward values (averaged over 5 attempts) for each RL algorithm given the best hyperparameters. Results from Onestep Q learning are also included in the table. It is clear that UREX outperforms baselines in these tasks. UREX is able to consistently find a suitable algorithm for the more difficult tasks such as reverse and reverse addition, but MENT and Q learning are falling behind. Importantly, for the BinarySearch task, which has many local maxims and requires intelligent exploration, UREX is the only method that can consistently solve it. The Q Learning baseline solves some of the simple tasks, but it does not make much headway on the more difficult tasks. We believe that entropy regulation for political gradients and appetite for Q learning strategies may result in relatively weak exploration strategies with incorrect direction."}, {"heading": "7.3 GENERALIZATION TO LONGER SEQUENCES", "text": "To confirm whether our method is capable of finding the correct algorithm for multi-digit addition, we test its generalization for longer input sequences than during training. We evaluate the trained models for inputs up to 2000 digits long, although the training sequences contain no more than 33 characters. For each length, we test the model using 100 randomly generated inputs and stop if the accuracy drops below 100%. Of the 60 models trained for addition with UREX, we find that 5 models generalize up to 2000 digits without observed errors to numbers. In the best UREX hyperparameters, 2 of the 5 random restarts can be successful generalizations. For more detailed results on generalization performance for 3 different tasks, including copy, duplicate input and reverse addition, see Appendix C. During these evaluations, we most likely take the action from the binary (a | h) rather than performing randomly."}, {"heading": "7.4 IMPLEMENTATION DETAILS", "text": "In all experiments we use Curriculum Learning. The environment starts by providing only small inputs, and moves on to longer sequences as soon as the agent achieves near-maximum reward through a number of steps. For political gradient methods, including MENT and UREX, we only give the agent one reward at the end of the episode, and there is no idea of interim reward. For the value-based baseline, we implement one-step Q-learning methods, as in Mnih et al. (2016) -Alg. 1, using double Q-learning with -greedy exploration. We use the same RNN in our policy-based approaches to estimate the Q values. A network search using Exploration Rate, Exploration Rate, Learning Rate, and Sync Frequency (between online and target network) is performed to find the best hyper-learning parameters with other indicators as opposed to Exploration methods."}, {"heading": "8 CONCLUSION", "text": "Our experimental results show that UREX significantly outperforms other value and policy-based methods, while being more resilient to changes in hyperparameters. By using UREX, we can solve algorithmic tasks such as multi-digit addition that other methods cannot reliably solve even with the best hyperparameters. We are introducing a new algorithmic task based on binary search to encourage more research in this area, especially when it comes to the computational complexity of solutions. Solving these tasks is not only important to develop human intelligence to enable learning algorithms, but also important for generic reinforcement learning, where intelligent and efficient exploration is the key to successful methods."}, {"heading": "9 ACKNOWLEDGMENT", "text": "We thank Irwan Bello, Corey Lynch, George Tucker, Volodymyr Mnih and the Google Brain team for their insightful comments and discussions."}, {"heading": "A OPTIMAL POLICY FOR THE UREX OBJECTIVE", "text": "To derive the form of the optimal policy for the UREX target (11), note that a maximum number of actions is to be taken for each h to enforce this restriction, we introduce a Lagrange multiplier (11) and aim to maximize a maximum number of actions. (a) a (a) a (a) a) a (a) b (a) b (a) x (a) x (a) x (a) x (a) x (a) a (a) a) a (a) x (a) x (a) x (a) a). (14) Since the gradient of the Lagrange multiplier (14) is given by an optimal distribution of averages, an optimal choice is made for the Lagrange multiplier (14) x (a) x (a) x (a) x (c) x x x (a) x x x x x x x x x x x x x (a) x x x x x x (a) x x x x x x (a) x x x x x (a) x x (x x x) x x x x x x x x x (an) x x x (an) x (c c c c) x (c) x x x (c) x x x x (x x x x x x x x x x (x x x x x x x x x x x x x x x x x x x x x x x x x x x x x) x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x (a) x x x x x (x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x (x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x"}, {"heading": "B ROBUSTNESS TO HYPER-PARAMETERS", "text": "Tables 3-8 provide more details on different cells from Table 1. Each table presents the results of the MENT using the best temperature \u03c4 vs. UREX with \u03c4 = 0.1 on a variety of learning rates and clipping values. Each cell is the number of attempts of 5 random restarts that successfully solve the task using a specific \u03b7 and c."}, {"heading": "C GENERALIZATION TO LONGER SEQUENCES", "text": "Table 9 provides a more detailed view of the generalization performance of the trained models on Copy, DuplicatedInput, and ReversedAddition. The tables show how the number of models that can correctly solve the task decreases as the length of input increases."}, {"heading": "D EXAMPLE EXECUTION TRACES", "text": "We provide the traces of two trained agents in the task ReversedAddition (Figure 2) and the task BinarySearch (Table 10)."}], "references": [{"title": "Tensorflow: A system for largescale machine learning", "author": ["Mart\u0131\u0301n Abadi", "Paul Barham", "Jianmin Chen", "Zhifeng Chen", "Andy Davis", "Jeffrey Dean", "Matthieu Devin", "Sanjay Ghemawat", "Geoffrey Irving", "Michael Isard"], "venue": null, "citeRegEx": "Abadi et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Abadi et al\\.", "year": 2016}, {"title": "Learning regular sets form queries and counterexamples", "author": ["Dana Angulin"], "venue": "Information and Computation,", "citeRegEx": "Angulin.,? \\Q1987\\E", "shortCiteRegEx": "Angulin.", "year": 1987}, {"title": "Neural machine translation by jointly learning to align and translate", "author": ["Dzmitry Bahdanau", "Kyunghyun Cho", "Yoshua Bengio"], "venue": null, "citeRegEx": "Bahdanau et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Bahdanau et al\\.", "year": 2015}, {"title": "Unifying count-based exploration and intrinsic motivation", "author": ["Marc G. Bellemare", "Sriram Srinivasan", "Georg Ostrovski", "Tom Schaul", "David Saxton", "R\u00e9mi Munos"], "venue": null, "citeRegEx": "Bellemare et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Bellemare et al\\.", "year": 2016}, {"title": "Some modified matrix eigenvalue problems", "author": ["Gene Golub"], "venue": "SIAM Review,", "citeRegEx": "Golub.,? \\Q1987\\E", "shortCiteRegEx": "Golub.", "year": 1987}, {"title": "Hybrid computing using a neural network with dynamic external memory. Nature, 2016", "author": ["Alex Graves", "Greg Wayne", "Malcolm Reynolds", "Tim Harley", "Ivo Danihelka", "Agnieszka GrabskaBarwinska", "Sergio G. Colmenarejo", "Edward Grefenstette", "Tiago Ramalho", "John Agapiou", "Adria P. Badia", "Karl M. Hermann", "Yori Zwols", "Georg Ostrovski", "Adam Cain", "Helen King", "Christopher Summerfield", "Phil Blunsom", "Koray Kavukcuoglu", "Demis Hassabis"], "venue": null, "citeRegEx": "Graves et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Graves et al\\.", "year": 2016}, {"title": "Long short-term memory", "author": ["Sepp Hochreiter", "J\u00fcrgen Schmidhuber"], "venue": "Neural Comput.,", "citeRegEx": "Hochreiter and Schmidhuber.,? \\Q1997\\E", "shortCiteRegEx": "Hochreiter and Schmidhuber.", "year": 1997}, {"title": "Learning in embedded systems", "author": ["Leslie Pack Kaelbling"], "venue": "MIT press,", "citeRegEx": "Kaelbling.,? \\Q1993\\E", "shortCiteRegEx": "Kaelbling.", "year": 1993}, {"title": "Near-optimal reinforcement learning in polynomial time", "author": ["Michael Kearns", "Satinder Singh"], "venue": "Machine Learning,", "citeRegEx": "Kearns and Singh.,? \\Q2002\\E", "shortCiteRegEx": "Kearns and Singh.", "year": 2002}, {"title": "Learning and using relational theories", "author": ["Charles Kemp", "Noah Goodman", "Joshua Tenebaum"], "venue": null, "citeRegEx": "Kemp et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Kemp et al\\.", "year": 2007}, {"title": "Adam: A method for stochastic optimization", "author": ["Diederik P. Kingma", "Jimmy Ba"], "venue": "ICLR,", "citeRegEx": "Kingma and Ba.,? \\Q2015\\E", "shortCiteRegEx": "Kingma and Ba.", "year": 2015}, {"title": "Inductive Logic Programming: Theory and Methods", "author": ["N. Lavrac", "S. Dzeroski"], "venue": null, "citeRegEx": "Lavrac and Dzeroski.,? \\Q1994\\E", "shortCiteRegEx": "Lavrac and Dzeroski.", "year": 1994}, {"title": "Playing atari with deep reinforcement learning", "author": ["Volodymyr Mnih", "Koray Kavukcuoglu", "David Silver", "Alex Graves", "Ioannis Antonoglou", "Daan Wierstra", "Martin A. Riedmiller"], "venue": null, "citeRegEx": "Mnih et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Mnih et al\\.", "year": 2013}, {"title": "Human-level control through deep reinforcement learning", "author": ["Volodymyr Mnih", "Koray Kavukcuoglu", "David Silver"], "venue": null, "citeRegEx": "Mnih et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Mnih et al\\.", "year": 2015}, {"title": "Asynchronous methods for deep reinforcement learning", "author": ["Volodymyr Mnih", "Adria Puigdomenech Badia", "Mehdi Mirza", "Alex Graves", "Timothy P Lillicrap", "Tim Harley", "David Silver", "Koray Kavukcuoglu"], "venue": null, "citeRegEx": "Mnih et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Mnih et al\\.", "year": 2016}, {"title": "Machine Learning: A Probabilistic Perspective", "author": ["Kevin P. Murphy"], "venue": null, "citeRegEx": "Murphy.,? \\Q2012\\E", "shortCiteRegEx": "Murphy.", "year": 2012}, {"title": "Neural programmer: Inducing latent programs with gradient descent", "author": ["Arvind Neelakantan", "Quoc V. Le", "Ilya Sutskever"], "venue": null, "citeRegEx": "Neelakantan et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Neelakantan et al\\.", "year": 2016}, {"title": "Reward augmented maximum likelihood for neural structured prediction", "author": ["Mohammad Norouzi", "Samy Bengio", "Zhifeng Chen", "Navdeep Jaitly", "Mike Schuster", "Yonghui Wu", "Dale Schuurmans"], "venue": null, "citeRegEx": "Norouzi et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Norouzi et al\\.", "year": 2016}, {"title": "Deep exploration via bootstrapped DQN", "author": ["Ian Osband", "Charles Blundell", "Alexander Pritzel", "Benjamin Van Roy"], "venue": null, "citeRegEx": "Osband et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Osband et al\\.", "year": 2016}, {"title": "Monte Carlo theory, methods and examples", "author": ["Art B. Owen"], "venue": null, "citeRegEx": "Owen.,? \\Q2013\\E", "shortCiteRegEx": "Owen.", "year": 2013}, {"title": "Artificial intelligence: a modern approach, volume 2. Prentice hall", "author": ["Stuart Jonathan Russell", "Peter Norvig", "John F Canny", "Jitendra M Malik", "Douglas D Edwards"], "venue": "Upper Saddle River,", "citeRegEx": "Russell et al\\.,? \\Q2003\\E", "shortCiteRegEx": "Russell et al\\.", "year": 2003}, {"title": "Optimal artificial curiosity, creativity, music, and the fine arts", "author": ["J\u00fcrgen Schmidhuber"], "venue": "Connection Science,", "citeRegEx": "Schmidhuber.,? \\Q2006\\E", "shortCiteRegEx": "Schmidhuber.", "year": 2006}, {"title": "Mastering the game of Go with deep neural networks and tree search", "author": ["David Silver", "Aja Huang"], "venue": null, "citeRegEx": "Silver and Huang,? \\Q2016\\E", "shortCiteRegEx": "Silver and Huang", "year": 2016}, {"title": "Incentivizing exploration in reinforcement learning with deep predictive models", "author": ["Bradly C. Stadie", "Sergey Levine", "Pieter Abbeel"], "venue": null, "citeRegEx": "Stadie et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Stadie et al\\.", "year": 2015}, {"title": "Sequence to sequence learning with neural networks", "author": ["Ilya Sutskever", "Oriol Vinyals", "Quoc V. Le"], "venue": null, "citeRegEx": "Sutskever et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Sutskever et al\\.", "year": 2014}, {"title": "Introduction to Reinforcement Learning", "author": ["Richard S. Sutton", "Andrew G. Barto"], "venue": null, "citeRegEx": "Sutton and Barto.,? \\Q1998\\E", "shortCiteRegEx": "Sutton and Barto.", "year": 1998}, {"title": "Efficient exploration in reinforcement learning", "author": ["Sebastian B Thrun"], "venue": "Technical report,", "citeRegEx": "Thrun.,? \\Q1992\\E", "shortCiteRegEx": "Thrun.", "year": 1992}, {"title": "Adaptive \u03b5-greedy exploration in reinforcement learning based on value differences", "author": ["Michel Tokic"], "venue": null, "citeRegEx": "Tokic.,? \\Q2010\\E", "shortCiteRegEx": "Tokic.", "year": 2010}, {"title": "Deep reinforcement learning with double qlearning", "author": ["Hado Van Hasselt", "Arthur Guez", "David Silver"], "venue": null, "citeRegEx": "Hasselt et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Hasselt et al\\.", "year": 2016}, {"title": "Simple statistical gradient-following algorithms for connectionist reinforcement learning", "author": ["Ronald J. Williams"], "venue": "Machine Learning,", "citeRegEx": "Williams.,? \\Q1992\\E", "shortCiteRegEx": "Williams.", "year": 1992}, {"title": "Function optimization using connectionist reinforcement learning algorithms", "author": ["Ronald J Williams", "Jing Peng"], "venue": "Connection Science,", "citeRegEx": "Williams and Peng.,? \\Q1991\\E", "shortCiteRegEx": "Williams and Peng.", "year": 1991}, {"title": "Reinforcement learning neural turing machines", "author": ["Wojciech Zaremba", "Ilya Sutskever"], "venue": null, "citeRegEx": "Zaremba and Sutskever.,? \\Q2015\\E", "shortCiteRegEx": "Zaremba and Sutskever.", "year": 2015}], "referenceMentions": [{"referenceID": 20, "context": "Although symbolic reasoning has a long history in AI (Russell et al., 2003), only recently have statistical machine learning and neural network approaches begun to make headway in automated algorithm discovery (Reed & de Freitas, 2016; Kaiser & Sutskever, 2016; Neelakantan et al.", "startOffset": 53, "endOffset": 75}, {"referenceID": 16, "context": ", 2003), only recently have statistical machine learning and neural network approaches begun to make headway in automated algorithm discovery (Reed & de Freitas, 2016; Kaiser & Sutskever, 2016; Neelakantan et al., 2016) heading to cross an important milestone on the path to AI.", "startOffset": 142, "endOffset": 219}, {"referenceID": 24, "context": "Nevertheless, most of the recent successes depend on the use of strong supervision to learn a mapping from a set of training inputs to outputs by maximizing a conditional log-likelihood, very much like neural machine translation systems (Sutskever et al., 2014; Bahdanau et al., 2015).", "startOffset": 237, "endOffset": 284}, {"referenceID": 2, "context": "Nevertheless, most of the recent successes depend on the use of strong supervision to learn a mapping from a set of training inputs to outputs by maximizing a conditional log-likelihood, very much like neural machine translation systems (Sutskever et al., 2014; Bahdanau et al., 2015).", "startOffset": 237, "endOffset": 284}, {"referenceID": 13, "context": "Despite the recent excitement around the use of RL to tackle Atari games (Mnih et al., 2015) and Go (Silver et al.", "startOffset": 73, "endOffset": 92}, {"referenceID": 26, "context": "We believe one of the key limitations of the current RL methods, preventing them from making much progress in the sparse reward settings, is the use of undirected exploration strategies (Thrun, 1992), such as -greedy and entropy regularization (Williams & Peng, 1991).", "startOffset": 186, "endOffset": 199}, {"referenceID": 9, "context": "Although research on using neural networks to learn algorithms has had a surge of recent interest, the problem of program induction from examples has a long history in many fields, including program induction, inductive logic programming (Lavrac & Dzeroski, 1994), relational learning (Kemp et al., 2007) and regular language learning (Angulin, 1987).", "startOffset": 285, "endOffset": 304}, {"referenceID": 1, "context": ", 2007) and regular language learning (Angulin, 1987).", "startOffset": 38, "endOffset": 53}, {"referenceID": 5, "context": "Previous work in this area has focused on augmenting a neural network with additional structure and increased capabilities (Zaremba & Sutskever, 2015; Graves et al., 2016).", "startOffset": 123, "endOffset": 171}, {"referenceID": 1, "context": ", 2007) and regular language learning (Angulin, 1987). Rather than presenting a comprehensive survey of program induction here, we focus on neural network approaches to algorithmic tasks and highlight the relative simplicity of our neural network architecture. Most successful applications of neural networks to algorithmic tasks rely on strong supervision, where the inputs and target outputs are completely known a priori. Given a dataset of examples, one learns the network parameters by maximizing the conditional likelihood of the outputs via backpropagation (e.g., Reed & de Freitas (2016); Kaiser & Sutskever (2016); Vinyals et al.", "startOffset": 39, "endOffset": 596}, {"referenceID": 1, "context": ", 2007) and regular language learning (Angulin, 1987). Rather than presenting a comprehensive survey of program induction here, we focus on neural network approaches to algorithmic tasks and highlight the relative simplicity of our neural network architecture. Most successful applications of neural networks to algorithmic tasks rely on strong supervision, where the inputs and target outputs are completely known a priori. Given a dataset of examples, one learns the network parameters by maximizing the conditional likelihood of the outputs via backpropagation (e.g., Reed & de Freitas (2016); Kaiser & Sutskever (2016); Vinyals et al.", "startOffset": 39, "endOffset": 623}, {"referenceID": 1, "context": ", 2007) and regular language learning (Angulin, 1987). Rather than presenting a comprehensive survey of program induction here, we focus on neural network approaches to algorithmic tasks and highlight the relative simplicity of our neural network architecture. Most successful applications of neural networks to algorithmic tasks rely on strong supervision, where the inputs and target outputs are completely known a priori. Given a dataset of examples, one learns the network parameters by maximizing the conditional likelihood of the outputs via backpropagation (e.g., Reed & de Freitas (2016); Kaiser & Sutskever (2016); Vinyals et al. (2015)).", "startOffset": 39, "endOffset": 646}, {"referenceID": 29, "context": "We start by discussing the most common form of policy gradient, REINFORCE (Williams, 1992), and its entropy regularized variant (Williams & Peng, 1991).", "startOffset": 74, "endOffset": 90}, {"referenceID": 5, "context": "REINFORCE has been applied to model-free policy-based learning with neural networks and algorithmic domains (Zaremba & Sutskever, 2015; Graves et al., 2016).", "startOffset": 108, "endOffset": 156}, {"referenceID": 29, "context": "Williams (1992) proposed to compute the stochastic gradient of the expected reward by using Monte Carlo samples.", "startOffset": 0, "endOffset": 16}, {"referenceID": 29, "context": "To combat this tendency, Williams & Peng (1991) augmented the expected reward objective by including a maximum entropy regularizer (\u03c4 > 0) to promote greater exploration.", "startOffset": 25, "endOffset": 48}, {"referenceID": 15, "context": "The KL divergence DKL (\u03c0\u03b8 \u2016 \u03c0\u2217 \u03c4 ) is known to be mode seeking (Murphy, 2012, Section 21.2.2) even with entropy regularization (\u03c4 > 0). Learning a policy by optimizing this direction of the KL is prone to falling into a local optimum resulting in a sub-optimal policy that omits some of the modes of \u03c0\u2217 \u03c4 . Although entropy regularization helps mitigate the issues as confirmed in our experiments, it is not an effective exploration strategy as it is undirected and requires a small regularization coefficient \u03c4 to avoid too much random exploration. Instead, we propose a directed exploration strategy that improves the mean seeking behavior of policy gradient in a principled way. We start by considering the alternate mean seeking direction of the KL divergence, DKL (\u03c0\u2217 \u03c4 \u2016 \u03c0\u03b8). Norouzi et al. (2016) considered this direction of the KL to directly learn a policy by optimizing", "startOffset": 64, "endOffset": 804}, {"referenceID": 19, "context": "This paper proposes to approximate the expectation with respect to \u03c0\u2217 \u03c4 in (7) by using self-normalized importance sampling (Owen, 2013), where the proposal distribution is \u03c0\u03b8 and the reference distribution is \u03c0\u2217 \u03c4 .", "startOffset": 124, "endOffset": 136}, {"referenceID": 17, "context": "(8) Norouzi et al. (2016) argue that in some structured prediction problems when one can draw samples from \u03c0\u2217 \u03c4 , optimizing (7) is more effective than (1), since no sampling from a non-stationary policy \u03c0\u03b8 is required.", "startOffset": 4, "endOffset": 26}, {"referenceID": 12, "context": "Like entropy regularization, such an approach applies undirected exploration, but it has achieved recent success in game playing environments (Mnih et al., 2013; Van Hasselt et al., 2016; Mnih et al., 2016).", "startOffset": 142, "endOffset": 206}, {"referenceID": 14, "context": "Like entropy regularization, such an approach applies undirected exploration, but it has achieved recent success in game playing environments (Mnih et al., 2013; Van Hasselt et al., 2016; Mnih et al., 2016).", "startOffset": 142, "endOffset": 206}, {"referenceID": 26, "context": "This basic intuition underlies work on counter and recency methods (Thrun, 1992), exploration methods based on uncertainty estimates of values (Kaelbling, 1993; Tokic, 2010), methods that prioritize learning environment dynamics (Kearns & Singh, 2002; Stadie et al.", "startOffset": 67, "endOffset": 80}, {"referenceID": 7, "context": "This basic intuition underlies work on counter and recency methods (Thrun, 1992), exploration methods based on uncertainty estimates of values (Kaelbling, 1993; Tokic, 2010), methods that prioritize learning environment dynamics (Kearns & Singh, 2002; Stadie et al.", "startOffset": 143, "endOffset": 173}, {"referenceID": 27, "context": "This basic intuition underlies work on counter and recency methods (Thrun, 1992), exploration methods based on uncertainty estimates of values (Kaelbling, 1993; Tokic, 2010), methods that prioritize learning environment dynamics (Kearns & Singh, 2002; Stadie et al.", "startOffset": 143, "endOffset": 173}, {"referenceID": 23, "context": "This basic intuition underlies work on counter and recency methods (Thrun, 1992), exploration methods based on uncertainty estimates of values (Kaelbling, 1993; Tokic, 2010), methods that prioritize learning environment dynamics (Kearns & Singh, 2002; Stadie et al., 2015), and methods that provide an intrinsic motivation or curiosity bonus for exploring unknown states (Schmidhuber, 2006; Bellemare et al.", "startOffset": 229, "endOffset": 272}, {"referenceID": 21, "context": ", 2015), and methods that provide an intrinsic motivation or curiosity bonus for exploring unknown states (Schmidhuber, 2006; Bellemare et al., 2016).", "startOffset": 106, "endOffset": 149}, {"referenceID": 3, "context": ", 2015), and methods that provide an intrinsic motivation or curiosity bonus for exploring unknown states (Schmidhuber, 2006; Bellemare et al., 2016).", "startOffset": 106, "endOffset": 149}, {"referenceID": 3, "context": ", 2015), and methods that provide an intrinsic motivation or curiosity bonus for exploring unknown states (Schmidhuber, 2006; Bellemare et al., 2016). We relate the concepts of value and policy in RL and propose an exploration strategy based on the discrepancy between the two. In contrast to value-based methods, exploration for policy-based RL methods is often a by-product of the optimization algorithm itself. Since algorithms like REINFORCE and Thompson sampling choose actions according to a stochastic policy, sub-optimal actions are chosen with some non-zero probability. The Q-learning algorithm may also be modified to sample an action from the softmax of the Q values rather than the argmax (Sutton & Barto, 1998). Asynchronous training has also been reported to have an exploration effect on both value- and policy-based methods. Mnih et al. (2016) report that asynchronous training can stabilize training", "startOffset": 126, "endOffset": 861}, {"referenceID": 18, "context": "In the same spirit, Osband et al. (2016) use multiple Q value approximators and sample only one to act for each episode as a way to implicitly incorporate exploration.", "startOffset": 20, "endOffset": 41}, {"referenceID": 12, "context": "For the value-based baseline, we implement one-step Q-learning as described in Mnih et al. (2016)-Alg.", "startOffset": 79, "endOffset": 98}, {"referenceID": 0, "context": "Experiments are conducted using Tensorflow (Abadi et al., 2016).", "startOffset": 43, "endOffset": 63}], "year": 2017, "abstractText": "This paper presents a novel form of policy gradient for model-free reinforcement learning (RL) with improved exploration properties. Current policy-based methods use entropy regularization to encourage undirected exploration of the reward landscape, which is ineffective in high dimensional spaces with sparse rewards. We propose a more directed exploration strategy that promotes exploration of under-appreciated reward regions. An action sequence is considered under-appreciated if its log-probability under the current policy under-estimates its resulting reward. The proposed exploration strategy is easy to implement, requiring small modifications to an implementation of the REINFORCE algorithm. We evaluate the approach on a set of algorithmic tasks that have long challenged RL methods. Our approach reduces hyper-parameter sensitivity and demonstrates significant improvements over baseline methods. Our algorithm successfully solves a benchmark multi-digit addition task and generalizes to long sequences. This is, to our knowledge, the first time that a pure RL method has solved addition using only reward feedback.", "creator": "LaTeX with hyperref package"}}}