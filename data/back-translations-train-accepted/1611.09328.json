{"id": "1611.09328", "review": {"conference": "AAAI", "VERSION": "v1", "DATE_OF_SUBMISSION": "28-Nov-2016", "title": "Accelerated Gradient Temporal Difference Learning", "abstract": "The family of temporal difference (TD) methods span a spectrum from computationally frugal linear methods like TD({\\lambda}) to data efficient least squares methods. Least square methods make the best use of available data directly computing the TD solution and thus do not require tuning a typically highly sensitive learning rate parameter, but require quadratic computation and storage. Recent algorithmic developments have yielded several sub-quadratic methods that use an approximation to the least squares TD solution, but incur bias. In this paper, we propose a new family of accelerated gradient TD (ATD) methods that (1) provide similar data efficiency benefits to least-squares methods, at a fraction of the computation and storage (2) significantly reduce parameter sensitivity compared to linear TD methods, and (3) are asymptotically unbiased. We illustrate these claims with a proof of convergence in expectation and experiments on several benchmark domains and a large-scale industrial energy allocation domain.", "histories": [["v1", "Mon, 28 Nov 2016 20:33:15 GMT  (316kb,D)", "https://arxiv.org/abs/1611.09328v1", "AAAI Conference on Artificial Intelligence (AAAI), 2017"], ["v2", "Thu, 9 Mar 2017 22:36:45 GMT  (1013kb,D)", "http://arxiv.org/abs/1611.09328v2", "AAAI Conference on Artificial Intelligence (AAAI), 2017"]], "COMMENTS": "AAAI Conference on Artificial Intelligence (AAAI), 2017", "reviews": [], "SUBJECTS": "cs.AI cs.LG stat.ML", "authors": ["yangchen pan", "adam m white", "martha white"], "accepted": true, "id": "1611.09328"}, "pdf": {"name": "1611.09328.pdf", "metadata": {"source": "CRF", "title": "Accelerated Gradient Temporal Difference Learning", "authors": ["Yangchen Pan", "Adam White", "Martha White"], "emails": ["yangpan@indiana.edu", "adamw@indiana.edu", "martha@indiana.edu"], "sections": [{"heading": "Introduction", "text": "This year, it is closer than ever before in the history of the country."}, {"heading": "Background and Problem Formulation", "text": "In this work, we focus on the problem of policy evaluation, or the problem of learning a value function in the face of a fixed policy. We model the interaction between an agent and his environment as a Markov decision process (S, A, P, r), where S denotes the set of states, A denotes the set of actions, and P: S \u00b7 A \u00b7 S \u2192 [0, \u221e) encodes the one-step state transition dynamic. At each discrete time step t = 1, 2, 3,... the agent selects an action according to his behavioral policy, and P: S \u00b7 A \u00b7 S \u00b7 S \u00b7 A \u2192 [0, \u221e) and the environment responds to the transition to a new state St + 1 corresponding to P, and emits a scalable reward Rt + 1 def = r (St, At, St + 1).The goal among policy evaluation is the estimation of the value function, v\u03c0: S \u2192 R, as an expected return from any state under any goal policy A \u00b7 S \u00b2."}, {"heading": "Algorithm derivation", "text": "To derive the new algorithm, we must first derive the gradient of the MSPBE (in 2) (in 2), to \u2212 1 2 \u00b0 WMSPBE (w, m) = A > mC \u2212 1E\u00b5 (in 3). (3) Consider a second requirement by calculating the Hessian: H = A > mC \u2212 1A > m (A > C \u2212 1A > C \u2212 1Em). (1). (1). (1). (1). (1). (1). (1). (1). (1). (1). (1). (1). (1). (1). (1). (1). (1). (1). (1). (1). (1). (1). (1). (1). (1). (1). (1). (1). (1). (1). (1). (. (1). (.). (1). (.). (.). (.). (.). (.). (.). (.).). (.). (.).). (.).). (.).). (.).). (.).). (.).). (.).).). (.).).). (.). (.).).).). (.).).). (.).). (.).).). (.).). (.).).). (.).).). (.). (.). (.).).). (.). (.).). (.).).). (.).). (.). (.).). (.).). (.). (.). (.).).). (.). (.).). (.).).). (.).). (.).). (.).).). (.). (.).).). (.).)"}, {"heading": "Convergence of ATD(\u03bb)", "text": "As with previous results for time difference learning algorithms, the first key step is to prove that the expected update converges to the TD fix point. Contrary to previous evidence of convergence in expectation, we do not require the true A value to be full. This generalization is important because, as already shown, A is often classified low, even if the characteristics are linearly independent (Bertsekas 2007; Gehring et al. 2016). Furthermore, ATD should be more effective if A is low and therefore would require a full A classification in order to achieve the typical use cases for ATD. To achieve the main idea, we must first prove the convergence of ATD with weights that give positive semi-definite Am. A general proof for other weights is in the appendix. Assumption 1. A is diagonalizable, i.e. there are invertable Q values with normalized columns (eigenvectors) and diagonal Rd."}, {"heading": "Empirical Results", "text": "The results presented in this section are from over 756 thousand individual experiments conducted in three different areas. Due to space constraints, detailed descriptions of individual areas, error calculations and all other parameter settings are discussed in detail in the appendix. We have included a large number of baselines in our experiments, and other related baselines excluded from our study are also discussed in the appendix."}, {"heading": "1 2 3 4 5 6 7 8 9 101112131415161718", "text": "The aim of this experiment was to examine the performance of ATD in an area where the pre-conditioner matrix is fully evaluated while no ranking truncation is applied. We compared five linear complexity methods (TD (0), TD (\u03bb), real online TD (\u03bb), real online ETD (\u03bb), against LSTD (\u03bb) and ATD, the percentage errors relative to the true value function over the first 1000 steps, averaged over 200 independent runs."}, {"heading": "Convergence proof", "text": "For the more general constellation in which m can also correspond to the fixed point w, we define the ranking order-k-alignment anew. (We say that the ranking order-k-alignment A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-"}, {"heading": "Algorithmic details", "text": "In this section we outline the implemented ATD + 1 t + 1et (\u2212 xt) algorithms. The key decisions are how we approach A + 1 and how we can update the eligibility in order to obtain different variants of the TD. \u2212 rzRead more about this in the Saturday issue of the Passau Neue Presse (Passau edition). \u2212 rzRead more about this in the Tuesday issue of the Passau Neue Presse (Passau edition). \u2212 rzRead more about this in the Tuesday issue of the Passau Neue Presse (Passau Neue Presse edition). \u2212 rzRead more about this in your issue of the Pasauer Neue Presse (Passau Neue Presse edition). \u2212 rzRead more about this in the Saturday issue of the Passau Neue Presse (Passau Neue Presse edition). \u2212 rzRead more about this in the Passau Neue Presse (Passau Neue Presse edition). \u2212 rzRead more about this in the Passau Neue Presse (Passau Neue Presse edition)."}, {"heading": "Algorithms", "text": "The algorithms included in the experiments represent a wide range of stochastic approximation algorithms and matrix-based (subquadratic) algorithms, but there are some related algorithms that we did not want to include; for the sake of completeness, we explain our decision-making here. Some accelerations have been suggested for gradient TD algorithms (Mahadevan et al. 2014; Meyer et al. 2014; Dabney and Thomas 2014). However, they have either shown that they perform poorly in practice (White and White 2016), or that they are based on the application of accelerations outside their intended use (Meyer et al. 2014; Dabney et al. 2014). Dabney and Thomas (2014) have investigated a similar update of ATD, but for control adjustment and with a gradual update of the Fisher information matrix instead of A that is used here. Since they acknowledge that this approach is somewhat adhoc for TD methods, the typical method is not better suited to update and their grades."}, {"heading": "Boyan\u2019s Chain", "text": "This domain was implemented exactly as described in Boyan's paper (Boyan 1999). The task is episodic and the function of the true value is known, so we did not have to calculate any rollouts. Otherwise, the evaluation was performed exactly as described above. We tested the following parameter settings: \u2022 \u03b10: 0.1 \u00b7 2.0j | j = \u2212 12, \u2212 11, \u2212 10,..., 4, 5}, 18 values in total \u2022 n0: 102, 106} \u2022 \u03bb: 0.0, 0.1,..., 0.9, 0.91, 0.93, 0.95, 0.97, 0.99, 1.0}, 16 values in total \u2022 \u03b7: 0.10j | j = \u2212 4, \u2212 3.5, \u2212 3,..., 3.5, 4, 4.5}, 18 values in total. Linear methods (e.g. TD (0), true online ETD (\u03bb)), used zep0, n0, and \u03bb, whereas the LSTD also investigated incremental steps to initialize the fixed step."}, {"heading": "Mountain Car", "text": "Our second group of experiments was conducted on the classic RL benchmark domain Mountain Car. We used the Sutton and Barto (1998) specification of the domain, where the agent's goal is to choose one of three discrete actions (reverse, coast, forward) based on the continuous position and speed of an underpowered car to drive it out of a valley at which time the episode ends. This is an undiscounted task. Each episode starts at the default starting position - randomly at the bottom of the hill - at zero speed. Surveys were selected according to a stochastic bang bang-bang policy, in which the reversal is selected when the speed is negative and used forward when the speed is positive and occasionally a random action is selected - we tested the randomness in the action selection of 0%, 10% and 20%. We used tile coding to convert the continuous state variable into high-dimensional characteristics."}, {"heading": "Energy Allocation", "text": "This domain simulates the control of a storage medium that interacts with a market and a stochastic energy source, whereas the problem was originally modeled as a finite horizon without a discounted task (Salas and Powell 2013), with four state variables in each time step: the amount of energy in the storage facility Rt, the net amount of wind energy Et, the time aggregate demand Dt, and the price of electricity Pt in the spot market. The reward function encodes the revenues generated by the energy strategy of the agent as a real value. The policy to be evaluated was produced by an approximate dynamic programming algorithm from the literature (Salas and Powell 2013). The simulation program is from energy storage datasets II from http: / / castlelab.princeton.edu.We have made several minor modifications to the simulator to allow the compilation of training or test data for policy evaluation."}], "references": [{"title": "SGD-QN: Careful quasi-Newton stochastic gradient descent", "author": ["Bordes"], "venue": "Journal of Machine Learning Research", "citeRegEx": "Bordes,? \\Q2009\\E", "shortCiteRegEx": "Bordes", "year": 2009}, {"title": "J", "author": ["Boyan"], "venue": "A.", "citeRegEx": "Boyan 1999", "shortCiteRegEx": null, "year": 1999}, {"title": "P", "author": ["W. Dabney", "Thomas"], "venue": "S.", "citeRegEx": "Dabney and Thomas 2014", "shortCiteRegEx": null, "year": 2014}, {"title": "Policy evaluation with temporal differences: a survey and comparison", "author": ["Dann"], "venue": "The Journal of Machine Learning Research", "citeRegEx": "Dann,? \\Q2014\\E", "shortCiteRegEx": "Dann", "year": 2014}, {"title": "Incremental Truncated LSTD", "author": ["Gehring"], "venue": "In International Joint Conference on Artificial Intelligence", "citeRegEx": "Gehring,? \\Q2016\\E", "shortCiteRegEx": "Gehring", "year": 2016}, {"title": "and Bowling", "author": ["A. Geramifard"], "venue": "M.", "citeRegEx": "Geramifard and Bowling 2006", "shortCiteRegEx": null, "year": 2006}, {"title": "iLSTD: Eligibility traces and convergence analysis", "author": ["Geramifard"], "venue": "In Advances in Neural Information Processing Systems", "citeRegEx": "Geramifard,? \\Q2007\\E", "shortCiteRegEx": "Geramifard", "year": 2007}, {"title": "O", "author": ["M. Ghavamzadeh", "A. Lazaric", "Maillard"], "venue": "A.; and Munos, R.", "citeRegEx": "Ghavamzadeh et al. 2010", "shortCiteRegEx": null, "year": 2010}, {"title": "and Palhang", "author": ["A. Givchi"], "venue": "M.", "citeRegEx": "Givchi and Palhang 2014", "shortCiteRegEx": null, "year": 2014}, {"title": "P", "author": ["Hansen"], "venue": "C.", "citeRegEx": "Hansen 1990", "shortCiteRegEx": null, "year": 1990}, {"title": "P", "author": ["S. Mahadevan", "B. Liu", "Thomas"], "venue": "S.; Dabney, W.; Giguere, S.; Jacek, N.; Gemp, I.; and 0002, J. L.", "citeRegEx": "Mahadevan et al. 2014", "shortCiteRegEx": null, "year": 2014}, {"title": "Accelerated gradient temporal difference learning algorithms", "author": ["Meyer"], "venue": "In IEEE Symposium on Adaptive Dynamic Programming and Reinforcement Learning", "citeRegEx": "Meyer,? \\Q2014\\E", "shortCiteRegEx": "Meyer", "year": 2014}, {"title": "and Ribeiro", "author": ["A. Mokhtari"], "venue": "A.", "citeRegEx": "Mokhtari and Ribeiro 2014", "shortCiteRegEx": null, "year": 2014}, {"title": "L", "author": ["Prashanth"], "venue": "A.; Korda, N.; and Munos, R.", "citeRegEx": "Prashanth et al. 2013", "shortCiteRegEx": null, "year": 2013}, {"title": "W", "author": ["D.F. Salas", "Powell"], "venue": "B.", "citeRegEx": "Salas and Powell 2013", "shortCiteRegEx": null, "year": 2013}, {"title": "A stochastic quasi-Newton method for online convex optimization", "author": ["Schraudolph"], "venue": "In International Conference on Artificial Intelligence and Statistics", "citeRegEx": "Schraudolph,? \\Q2007\\E", "shortCiteRegEx": "Schraudolph", "year": 2007}, {"title": "Convergence of general nonstationary iterative methods for solving singular linear equations", "author": ["Shi"], "venue": "SIAM Journal on Matrix Analysis and Applications", "citeRegEx": "Shi,? \\Q2011\\E", "shortCiteRegEx": "Shi", "year": 2011}, {"title": "A", "author": ["R. Sutton", "Barto"], "venue": "G.", "citeRegEx": "Sutton and Barto 1998", "shortCiteRegEx": null, "year": 1998}, {"title": "A", "author": ["R. Sutton", "Barto"], "venue": "G.", "citeRegEx": "Sutton and Barto 2016", "shortCiteRegEx": null, "year": 2016}, {"title": "A", "author": ["Sutton, R.S.", "Mahmood"], "venue": "R.; and White, M.", "citeRegEx": "Sutton et al. 2016", "shortCiteRegEx": null, "year": 2016}, {"title": "Off-policy TD (\u03bb) with a true online equivalence", "author": ["van Hasselt"], "venue": "In Conference on Uncertainty in Artificial Intelligence", "citeRegEx": "Hasselt,? \\Q2014\\E", "shortCiteRegEx": "Hasselt", "year": 2014}, {"title": "True online TD(lambda)", "author": ["van Seijen", "H. Sutton 2014] van Seijen", "R. Sutton"], "venue": "In International Conference on Machine Learning", "citeRegEx": "Seijen et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Seijen et al\\.", "year": 2014}, {"title": "True Online Temporal-Difference Learning", "author": ["van Seijen"], "venue": "In Journal of Machine Learning Research", "citeRegEx": "Seijen,? \\Q2016\\E", "shortCiteRegEx": "Seijen", "year": 2016}, {"title": "D", "author": ["M. Wang", "Bertsekas"], "venue": "P.", "citeRegEx": "Wang and Bertsekas 2013", "shortCiteRegEx": null, "year": 2013}, {"title": "and White", "author": ["A.M. White"], "venue": "M.", "citeRegEx": "White and White 2016", "shortCiteRegEx": null, "year": 2016}, {"title": "On convergence of emphatic temporal-difference learning", "author": ["H. Yu"], "venue": "In Annual Conference on Learning Theory", "citeRegEx": "Yu,? \\Q2015\\E", "shortCiteRegEx": "Yu", "year": 2015}, {"title": "There have been some accelerations proposed to gradient TD algorithms (Mahadevan et al", "author": ["Meyer"], "venue": "Dabney and Thomas", "citeRegEx": "Meyer,? \\Q2014\\E", "shortCiteRegEx": "Meyer", "year": 2014}, {"title": "Dabney and Thomas (2014) explored a similar update to ATD, but for the control setting and with an incremental update to the Fisher information matrix rather than A used here", "author": ["Meyer"], "venue": "Dabney and Thomas", "citeRegEx": "Meyer,? \\Q2014\\E", "shortCiteRegEx": "Meyer", "year": 2014}, {"title": "SAGE for GTD similarly indicated little to no gain. Finally, Givchi and Palhang (2014) adapted SGD-QN for TD, and showed some improvements using this diagonal step-size approximation", "author": ["Meyer"], "venue": null, "citeRegEx": "Meyer,? \\Q2014\\E", "shortCiteRegEx": "Meyer", "year": 2014}], "referenceMentions": [], "year": 2017, "abstractText": "The family of temporal difference (TD) methods span a spectrum from computationally frugal linear methods like TD(\u03bb) to data efficient least squares methods. Least square methods make the best use of available data directly computing the TD solution and thus do not require tuning a typically highly sensitive learning rate parameter, but require quadratic computation and storage. Recent algorithmic developments have yielded several sub-quadratic methods that use an approximation to the least squares TD solution, but incur bias. In this paper, we propose a new family of accelerated gradient TD (ATD) methods that (1) provide similar data efficiency benefits to least-squares methods, at a fraction of the computation and storage (2) significantly reduce parameter sensitivity compared to linear TD methods, and (3) are asymptotically unbiased. We illustrate these claims with a proof of convergence in expectation and experiments on several benchmark domains and a large-scale industrial energy allocation domain.", "creator": "LaTeX with hyperref package"}}}