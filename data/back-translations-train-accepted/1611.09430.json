{"id": "1611.09430", "review": {"conference": "iclr", "VERSION": "v1", "DATE_OF_SUBMISSION": "28-Nov-2016", "title": "Emergence of foveal image sampling from learning to attend in visual scenes", "abstract": "We describe a neural attention model with a learnable retinal sampling lattice. The model is trained on a visual search task requiring the classification of an object embedded in a visual scene amidst background distractors using the smallest number of fixations. We explore the tiling properties that emerge in the model's retinal sampling lattice after training. Specifically, we show that this lattice resembles the eccentricity dependent sampling lattice of the primate retina, with a high resolution region in the fovea surrounded by a low resolution periphery. Furthermore, we find conditions where these emergent properties are amplified or eliminated providing clues to their function.", "histories": [["v1", "Mon, 28 Nov 2016 23:39:20 GMT  (794kb,D)", "http://arxiv.org/abs/1611.09430v1", null], ["v2", "Sat, 21 Oct 2017 08:26:16 GMT  (792kb,D)", "http://arxiv.org/abs/1611.09430v2", "Published as a conference paper at ICLR 2017"]], "reviews": [], "SUBJECTS": "cs.NE cs.AI cs.LG", "authors": ["brian cheung", "eric weiss", "bruno olshausen"], "accepted": true, "id": "1611.09430"}, "pdf": {"name": "1611.09430.pdf", "metadata": {"source": "CRF", "title": "EMERGENCE OF FOVEAL IMAGE SAMPLING FROM LEARNING TO ATTEND IN VISUAL SCENES", "authors": ["Brian Cheung", "Eric Weiss", "Bruno Olshausen"], "emails": ["bcheung@berkeley.edu", "eaweiss@berkeley.edu", "baolshausen@berkeley.edu"], "sections": [{"heading": "1 INTRODUCTION", "text": "A striking design feature of the primate retina is the way in which images are scanned spatially by retinal ganglion cells. (Sample spacing and receptive fields are small in the fovea and then increase linear with eccentricity, as) Thus, we have the highest spatial resolution at the center of fixation and lowest resolution at the periphery. The commonly accepted explanation for this eccentric sampling is that it provides us with both high resolution and broad coverage of the visual field with a limited amount of neural resources. Human retina contains 1.5 million ganglion cells whose axons form the only output of the retina."}, {"heading": "2 RETINAL TILING IN NEURAL NETWORKS WITH ATTENTION", "text": "Attention in neural networks can be formulated in terms of a differentiated feedback function that allows the parameters of these models to be trained together with the back-propagation. Most formulations of visual attention via the input image assume a certain structure in the core filters. For example, the most recent attention models by Jaderberg et al. (2015); Mnih et al. (2014); Gregor et al. (2015); Ba et al. (2014) assume that each core filter lies on a rectangular grid. To create a learnable retina sample grid, we loosen this assumption by allowing the cores to tile the image independently."}, {"heading": "2.1 GENERATING A GLIMPSE", "text": "We interpret a cursory glance as a form of routing in which a subset of the visual scene U is scanned to form a smaller view of the output G. Routing is defined by a series of cores k [\u2022] (s), with each kernel i determining which part of the input U [\u2022] contributes to a particular output G [i]. A control variable is used to control routing by adjusting the position and scale of the entire array of cores. In this sense, many attention models can be reformulated into a general equation, which asG [i] = H \u2211 n W \u2211 m U [m, n] k [m, i] (s) (1), with m and n index points representing pixels of U and i output views. Thus, the pixels in the input image U are mapped at a smaller glance G."}, {"heading": "2.2 RETINAL GLIMPSE", "text": "The centers of the individual kernel filters \u00b5 \u0301 [i] are calculated with respect to control variables sc and sz and learnable offset \u00b5 [i]. The control variables indicate the position and zoom of the entire glance. \u00b5 [i] and \u03c3 [i] specify the position and propagation of each of the kernel k [\u2212 \u2212, i]. These parameters are learned during the training with backpropagation. We describe how the control variables are calculated in the next section. Therefore, the cores are specified as follows: \"[i] = (sc \u2212) sz [i]) sz (2)."}, {"heading": "3 RECURRENT NEURAL ARCHITECTURE FOR ATTENTION", "text": "A glance at a given time, Gt, is predicted by a fully connected recursive network frnn ().ht = frnn (Gt, ht \u2212 1) (5) [sc, t; sz, t] = fcontrol (ht) (6) The global center sc, t and zoom sz, t is predicted by the control network fcontrol (), which is parameterized by a fully connected neural network. In this thesis, we examine three variants of the proposed recursive model: \u2022 Fixed Lattice: The core parameters \u00b5 [i] and \u03c3 [i] for each retinal cell are not learnable; the model can only translate the core filters sc, t = fcontrol (ht) and the global zoom is fixed sz, t = 1. \u2022 Translation Only: The core parameters \u00b5 [i] and \u03c3 [i] for each retinal cell are not learnable."}, {"heading": "4 DATASETS AND TASKS", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "4.1 MODIFIED CLUTTERED MNIST DATASET", "text": "Handwritten numbers from the original MNIST dataset LeCun & Cortes (1998) are placed randomly over a 100x100 image with a different amount of distractors (disorder).Distractors are created by placing random segments of non-targeted MNIST digits randomly over the image with the same probability of occurring. In contrast to the crowded MNIST dataset proposed in Mnih et al. (2014), the number of distractors for each image varies randomly between 0 and 20. This prevents the attention model from learning a solution that depends on the number of \"on\" pixels in a particular region. In addition, we create another dataset (dataset 2) with an additional variation factor: the original MNIST digit is randomly placed by a factor of 0.33x to 3.0x. Examples of this dataset are shown in the second row of Figure 4."}, {"heading": "4.2 VISUAL SEARCH TASK", "text": "We define our visual search task as a recognition task in a confusing scene. The recurring attention model that we propose must output the class c of the individual MNIST digit that appears in the image above the prediction network fpredict (). The task loss, L, is specified in Equation 8. To minimize the classification error, we use cross-entropy costs: c-t, n-fpredict (ht, n) (7) L = N-N-T \u2211 t cnlog (c-t, n) (8) Before Training (Initial Layout) After 1 epoch After 10 epochs After 100 epochs Similarly to the visual search experiments conducted in physiological studies, we put pressure on our attention model to perform the visual search as quickly as possible. By applying the task loss at any time, the model is forced to precisely locate the target MNIST digit in as few iterations as possible."}, {"heading": "5 RESULTS", "text": "Figure 5 shows the layouts of learned cores for a translation only at different stages during training. The filters transform smoothly from a uniform grid of cores to an eccentric dependent grid. In addition, the kernel filter distributes their individual centers to create, which cover the entire image. This is useful because the target MNIST digit can appear anywhere in the image with a uniform probability. In addition, the second row shows variable size of digits as an additional factor in the dataset, the translation only shows an even greater variety of variances for the kernel filter. This is visually evident in the first row of Figure 6, and the second row shows a highly dependent relationship between the sampling intervals and standard deviatoin the retinal sampling."}, {"heading": "6 CONCLUSION", "text": "This structure consists of a region of high sharpness in the center surrounded by a wider region of low sharpness. Van Essen & Anderson (1995) postulate that the linear relationship between eccentricity and sampling interval leads to a form of scale invariance in the retina of primates. Our results from the translation-only model with variable-sized digits support this conclusion. Furthermore, we observe that zoom seems to replace the need to learn a region with high sharpness for the visual search task, implying that the region with high sharpness serves a purpose similar to that of a zoomable sampling grid."}, {"heading": "ACKNOWLEDGMENTS", "text": "We would like to thank everyone at the Redwood Center for their helpful discussions and comments, and thank NVIDIA Corporation for their support by donating the Tesla K40 GPUs used for this research."}], "references": [{"title": "Multiple object recognition with visual attention", "author": ["Jimmy Ba", "Volodymyr Mnih", "Koray Kavukcuoglu"], "venue": "arXiv preprint arXiv:1412.7755,", "citeRegEx": "Ba et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Ba et al\\.", "year": 2014}, {"title": "Neural machine translation by jointly learning to align and translate", "author": ["Dzmitry Bahdanau", "Kyunghyun Cho", "Yoshua Bengio"], "venue": "arXiv preprint arXiv:1409.0473,", "citeRegEx": "Bahdanau et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Bahdanau et al\\.", "year": 2014}, {"title": "Theano: new features and speed improvements", "author": ["Fr\u00e9d\u00e9ric Bastien", "Pascal Lamblin", "Razvan Pascanu", "James Bergstra", "Ian Goodfellow", "Arnaud Bergeron", "Nicolas Bouchard", "David Warde-Farley", "Yoshua Bengio"], "venue": "arXiv preprint arXiv:1211.5590,", "citeRegEx": "Bastien et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Bastien et al\\.", "year": 2012}, {"title": "Topography of ganglion cells in human retina", "author": ["Christine A Curcio", "Kimberly A Allen"], "venue": "Journal of Comparative Neurology,", "citeRegEx": "Curcio and Allen.,? \\Q1990\\E", "shortCiteRegEx": "Curcio and Allen.", "year": 1990}, {"title": "Models of overt attention", "author": ["Wilson S Geisler", "Lawrence Cormack"], "venue": "Oxford handbook of eye movements,", "citeRegEx": "Geisler and Cormack.,? \\Q2011\\E", "shortCiteRegEx": "Geisler and Cormack.", "year": 2011}, {"title": "Neural turing machines", "author": ["Alex Graves", "Greg Wayne", "Ivo Danihelka"], "venue": "arXiv preprint arXiv:1410.5401,", "citeRegEx": "Graves et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Graves et al\\.", "year": 2014}, {"title": "Draw: A recurrent neural network for image generation", "author": ["Karol Gregor", "Ivo Danihelka", "Alex Graves", "Daan Wierstra"], "venue": "arXiv preprint arXiv:1502.04623,", "citeRegEx": "Gregor et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Gregor et al\\.", "year": 2015}, {"title": "A saliency-based search mechanism for overt and covert shifts of visual attention", "author": ["Laurent Itti", "Christof Koch"], "venue": "Vision research,", "citeRegEx": "Itti and Koch.,? \\Q2000\\E", "shortCiteRegEx": "Itti and Koch.", "year": 2000}, {"title": "Spatial transformer networks", "author": ["Max Jaderberg", "Karen Simonyan", "Andrew Zisserman"], "venue": "In Advances in Neural Information Processing Systems, pp. 2008\u20132016,", "citeRegEx": "Jaderberg et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Jaderberg et al\\.", "year": 2015}, {"title": "Adam: A method for stochastic optimization", "author": ["Diederik Kingma", "Jimmy Ba"], "venue": "arXiv preprint arXiv:1412.6980,", "citeRegEx": "Kingma and Ba.,? \\Q2014\\E", "shortCiteRegEx": "Kingma and Ba.", "year": 2014}, {"title": "Learning to combine foveal glimpses with a third-order boltzmann machine", "author": ["Hugo Larochelle", "Geoffrey E Hinton"], "venue": "In Advances in neural information processing systems,", "citeRegEx": "Larochelle and Hinton.,? \\Q2010\\E", "shortCiteRegEx": "Larochelle and Hinton.", "year": 2010}, {"title": "The mnist database of handwritten digits", "author": ["Yann LeCun", "Corinna Cortes"], "venue": null, "citeRegEx": "LeCun and Cortes.,? \\Q1998\\E", "shortCiteRegEx": "LeCun and Cortes.", "year": 1998}, {"title": "Recurrent models of visual attention", "author": ["Volodymyr Mnih", "Nicolas Heess", "Alex Graves"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Mnih et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Mnih et al\\.", "year": 2014}, {"title": "Retinal ganglion cells that project to the dorsal lateral geniculate nucleus in the macaque", "author": ["VH Perry", "R Oehler", "A Cowey"], "venue": "monkey. Neuroscience,", "citeRegEx": "Perry et al\\.,? \\Q1984\\E", "shortCiteRegEx": "Perry et al\\.", "year": 1984}, {"title": "Information processing strategies and pathways in the primate visual system", "author": ["David C Van Essen", "Charles H Anderson"], "venue": "An introduction to neural and electronic networks,", "citeRegEx": "Essen and Anderson.,? \\Q1995\\E", "shortCiteRegEx": "Essen and Anderson.", "year": 1995}, {"title": "Show, attend and tell: Neural image caption generation with visual attention", "author": ["Kelvin Xu", "Jimmy Ba", "Ryan Kiros", "Aaron Courville", "Ruslan Salakhutdinov", "Richard Zemel", "Yoshua Bengio"], "venue": "arXiv preprint arXiv:1502.03044,", "citeRegEx": "Xu et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Xu et al\\.", "year": 2015}], "referenceMentions": [{"referenceID": 13, "context": "6(A) and (B) Figure 1: Receptive field size (dendritic field diameter) as a function of eccentricity of Retinal Ganglion Cells from a macaque monkey (taken from Perry et al. (1984)).", "startOffset": 161, "endOffset": 181}, {"referenceID": 1, "context": "While neural attention models have been applied successfully to a variety of engineering applications (Bahdanau et al., 2014; Jaderberg et al., 2015; Xu et al., 2015; Graves et al., 2014), there has been little work in relating the properties of these attention mechanisms back to biological vision.", "startOffset": 102, "endOffset": 187}, {"referenceID": 8, "context": "While neural attention models have been applied successfully to a variety of engineering applications (Bahdanau et al., 2014; Jaderberg et al., 2015; Xu et al., 2015; Graves et al., 2014), there has been little work in relating the properties of these attention mechanisms back to biological vision.", "startOffset": 102, "endOffset": 187}, {"referenceID": 15, "context": "While neural attention models have been applied successfully to a variety of engineering applications (Bahdanau et al., 2014; Jaderberg et al., 2015; Xu et al., 2015; Graves et al., 2014), there has been little work in relating the properties of these attention mechanisms back to biological vision.", "startOffset": 102, "endOffset": 187}, {"referenceID": 5, "context": "While neural attention models have been applied successfully to a variety of engineering applications (Bahdanau et al., 2014; Jaderberg et al., 2015; Xu et al., 2015; Graves et al., 2014), there has been little work in relating the properties of these attention mechanisms back to biological vision.", "startOffset": 102, "endOffset": 187}, {"referenceID": 1, "context": "While neural attention models have been applied successfully to a variety of engineering applications (Bahdanau et al., 2014; Jaderberg et al., 2015; Xu et al., 2015; Graves et al., 2014), there has been little work in relating the properties of these attention mechanisms back to biological vision. An important property which distinguishes neural networks from most other neurobiological models is their ability to learn internal (latent) features directly from data. But existing neural network models specify the input sampling lattice a priori. Larochelle & Hinton (2010) employ an eccentricity dependent sampling lattice mimicking the primate retina, and Mnih et al.", "startOffset": 103, "endOffset": 577}, {"referenceID": 1, "context": "While neural attention models have been applied successfully to a variety of engineering applications (Bahdanau et al., 2014; Jaderberg et al., 2015; Xu et al., 2015; Graves et al., 2014), there has been little work in relating the properties of these attention mechanisms back to biological vision. An important property which distinguishes neural networks from most other neurobiological models is their ability to learn internal (latent) features directly from data. But existing neural network models specify the input sampling lattice a priori. Larochelle & Hinton (2010) employ an eccentricity dependent sampling lattice mimicking the primate retina, and Mnih et al. (2014) utilize a multi scale glimpse window\u2019 that forms a piece-wise approximation of this scheme.", "startOffset": 103, "endOffset": 680}, {"referenceID": 1, "context": "While neural attention models have been applied successfully to a variety of engineering applications (Bahdanau et al., 2014; Jaderberg et al., 2015; Xu et al., 2015; Graves et al., 2014), there has been little work in relating the properties of these attention mechanisms back to biological vision. An important property which distinguishes neural networks from most other neurobiological models is their ability to learn internal (latent) features directly from data. But existing neural network models specify the input sampling lattice a priori. Larochelle & Hinton (2010) employ an eccentricity dependent sampling lattice mimicking the primate retina, and Mnih et al. (2014) utilize a multi scale glimpse window\u2019 that forms a piece-wise approximation of this scheme. While it seems reasonable to think that these design choices contribute to the good performance of these systems, it remains to be seen if this arrangement emerges as the optimal solution. We further extend the learning paradigm of neural networks to the structural features of the glimpse mechanism of an attention model. To explore emergent properties of our learned retinal configurations, we train on artificial datasets where the factors of variation are easily controllable. Despite this departure from biology and natural stimuli, we find our model learns to create an eccentricity dependent layout where a distinct central region of high acuity emerges surrounded by a low acuity periphery. We show that the properties of this layout are highly dependent on the variations present in the task constraints. When we depart from physiology by augmenting our attention model with the ability to spatially rescale or zoom on its input, we find our model learns a more uniform layout which has properties more similar to the glimpse window proposed in Jaderberg et al. (2015); Gregor et al.", "startOffset": 103, "endOffset": 1850}, {"referenceID": 1, "context": "While neural attention models have been applied successfully to a variety of engineering applications (Bahdanau et al., 2014; Jaderberg et al., 2015; Xu et al., 2015; Graves et al., 2014), there has been little work in relating the properties of these attention mechanisms back to biological vision. An important property which distinguishes neural networks from most other neurobiological models is their ability to learn internal (latent) features directly from data. But existing neural network models specify the input sampling lattice a priori. Larochelle & Hinton (2010) employ an eccentricity dependent sampling lattice mimicking the primate retina, and Mnih et al. (2014) utilize a multi scale glimpse window\u2019 that forms a piece-wise approximation of this scheme. While it seems reasonable to think that these design choices contribute to the good performance of these systems, it remains to be seen if this arrangement emerges as the optimal solution. We further extend the learning paradigm of neural networks to the structural features of the glimpse mechanism of an attention model. To explore emergent properties of our learned retinal configurations, we train on artificial datasets where the factors of variation are easily controllable. Despite this departure from biology and natural stimuli, we find our model learns to create an eccentricity dependent layout where a distinct central region of high acuity emerges surrounded by a low acuity periphery. We show that the properties of this layout are highly dependent on the variations present in the task constraints. When we depart from physiology by augmenting our attention model with the ability to spatially rescale or zoom on its input, we find our model learns a more uniform layout which has properties more similar to the glimpse window proposed in Jaderberg et al. (2015); Gregor et al. (2015). These findings help us to understand the task conditions and constraints in which an eccentricity dependent sampling lattice emerges.", "startOffset": 103, "endOffset": 1872}, {"referenceID": 6, "context": "For example, the recent attention models proposed by Jaderberg et al. (2015); Mnih et al.", "startOffset": 53, "endOffset": 77}, {"referenceID": 6, "context": "For example, the recent attention models proposed by Jaderberg et al. (2015); Mnih et al. (2014); Gregor et al.", "startOffset": 53, "endOffset": 97}, {"referenceID": 5, "context": "(2014); Gregor et al. (2015); Ba et al.", "startOffset": 8, "endOffset": 29}, {"referenceID": 0, "context": "(2015); Ba et al. (2014) assume each kernel filter lies on a rectangular grid.", "startOffset": 8, "endOffset": 25}, {"referenceID": 2, "context": "For stochastic gradient descent optimization we use Adam (Kingma & Ba, 2014) and construct our models in Theano (Bastien et al., 2012).", "startOffset": 112, "endOffset": 134}, {"referenceID": 5, "context": "Similar to Gregor et al. (2015), this allows us to train the control network indirectly from signals backpropagated from the task cost.", "startOffset": 11, "endOffset": 32}, {"referenceID": 12, "context": "In contrast to the cluttered MNIST dataset proposed in Mnih et al. (2014), the number of distractors for each image varies randomly from 0 to 20 pieces.", "startOffset": 55, "endOffset": 74}, {"referenceID": 13, "context": "This relationship has also been observed in the primate visual system (Perry et al., 1984; Van Essen & Anderson, 1995).", "startOffset": 70, "endOffset": 118}], "year": 2016, "abstractText": "We describe a neural attention model with a learnable retinal sampling lattice. The model is trained on a visual search task requiring the classification of an object embedded in a visual scene amidst background distractors using the smallest number of fixations. We explore the tiling properties that emerge in the model\u2019s retinal sampling lattice after training. Specifically, we show that this lattice resembles the eccentricity dependent sampling lattice of the primate retina, with a high resolution region in the fovea surrounded by a low resolution periphery. Furthermore, we find conditions where these emergent properties are amplified or eliminated providing clues to their function.", "creator": "LaTeX with hyperref package"}}}