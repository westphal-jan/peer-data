{"id": "1611.09434", "review": {"conference": "ICML", "VERSION": "v1", "DATE_OF_SUBMISSION": "28-Nov-2016", "title": "Input Switched Affine Networks: An RNN Architecture Designed for Interpretability", "abstract": "The computational mechanisms by which nonlinear recurrent neural networks (RNNs) achieve their goals remains an open question. There exist many problem domains where intelligibility of the network model is crucial for deployment. Here we introduce a recurrent architecture composed of input-switched affine transformations, in other words an RNN without any nonlinearity and with one set of weights per input. We show that this architecture achieves near identical performance to traditional architectures on language modeling of Wikipedia text, for the same number of model parameters. It can obtain this performance with the potential for computational speedup compared to existing methods, by precomputing the composed affine transformations corresponding to longer input sequences. As our architecture is affine, we are able to understand the mechanisms by which it functions using linear methods. For example, we show how the network linearly combines contributions from the past to make predictions at the current time step. We show how representations for words can be combined in order to understand how context is transferred across word boundaries. Finally, we demonstrate how the system can be executed and analyzed in arbitrary bases to aid understanding.", "histories": [["v1", "Mon, 28 Nov 2016 23:48:41 GMT  (922kb,D)", "http://arxiv.org/abs/1611.09434v1", "ICLR 2107 submission:this https URL"], ["v2", "Mon, 12 Jun 2017 20:29:48 GMT  (1889kb,D)", "http://arxiv.org/abs/1611.09434v2", "ICLR 2107 submission:this https URL"]], "COMMENTS": "ICLR 2107 submission:this https URL", "reviews": [], "SUBJECTS": "cs.AI cs.CL cs.LG cs.NE", "authors": ["jakob n foerster", "justin gilmer", "jascha sohl-dickstein", "jan chorowski", "david sussillo"], "accepted": true, "id": "1611.09434"}, "pdf": {"name": "1611.09434.pdf", "metadata": {"source": "CRF", "title": "INTELLIGIBLE LANGUAGE MODELING WITH INPUT SWITCHED AFFINE NETWORKS", "authors": ["Jakob N. Foerster", "Justin Gilmer", "Jan Chorowski", "Jascha Sohl-Dickstein", "David Sussillo"], "emails": ["jakob.foerster@cs.ox.ac.uk,", "jan.chorowski@cs.uni.wroc.pl", "sussillo}@google.com"], "sections": [{"heading": "1 INTRODUCTION", "text": "In recent years, we have made remarkable advances in areas such as object recognition (Krizhevsky et al., 2012), language translation (Sutskever et al., 2014), and speech recognition (Graves et al., 2013). However, the success of the deep learning approach has certain applications where the comprehensibility of the system is an essential design requirement. A common example is the need to understand the choices a self-driving vehicle makes in avoiding various obstacles on its way. Another example is the use of neural networking methods for scientific discovery (Mante et al., 2013) Even where intelligibility is not an excessive design requirement, it is fair to say that most users of neural networks would better understand the models they use."}, {"heading": "2 RELATED WORK", "text": "The authors of the study, which deals with the question of how this development came about, agree that they will be able to get to grips with the causes of the crisis.The authors of the study agree that the causes of the crisis are not the causes of the crisis, but the causes of the crisis, which have intensified in recent years.The authors of the study agree that the causes of the crisis are not the causes of the crisis, but the causes of the crisis."}, {"heading": "3 METHODS", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "3.1 MODEL DEFINITION", "text": "Wx and bx each denote a transition matrix and a bias vector for a given input x, the xt symbol is the input at the time t and ht is the hidden state at the time t. Our ISAN model is defined asht = Wxt ht \u2212 1 + bxt. (1) The network also learns an initial hidden state h0. We emphasize the intentional absence of a non-linear activation function."}, {"heading": "3.2 CHARACTER LEVEL LANGUAGE MODELLING WITH RNNS", "text": "The Text8 dataset consists only of the 27 characters \"a\" - \"z\" and \"_\" (spaces). With a string of x1,..., xt, the RNNs are trained to minimize the cross entropy between the true next character and the output prediction. We map from the hidden state ht into a Logit space via an affine map. The probabilities are calculated asp (xt + 1) = softmax (lt) (2) lt = Wro ht + bro (3), where Wro and bro are the readout weights and distortions and according to the Logit vector. In accordance with (Collins et al., 2016), we divide the training data into 80%, 10% for train, test and evaluation set. The network was trained with the same hyperparameter infrastructure that is best executed in the Dynamic Model 2016 (ISAN 1, 27 x)."}, {"heading": "4 RESULTS AND ANALYSIS", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "4.1 ISAN PERFORMANCE ON THE TEXT8 TASK", "text": "The results for Text8 are shown in Figure 1a. For the largest number of parameters, ISAN is almost exactly the same as the performance of all other nonlinear models with the same number of maximum parameters: RNN, IRNN, GRU, LSTM. All analyses use ISAN, which was trained with maximum 1.28e6 parameters (1.58 bPC cross entropy). Examples of text generated from this model are relatively coherent. We show two examples after being primed with \"Annual Reve,\" at inverse temperature of 1.5 and 2.0 respectively: \u2022 \"Annual Revenue and Producer of Telecommunications and the Former Communist Action and Rescue of the New Statehouse of Replicates and Many Practitioners.\" \u2022 \"Annual Revenue Seven Five Three Million to Nine Eight in the Rest of the Country in the United States and South Africa.\" As a preliminary, comparative analysis, we conducted PCA over a large number of sequences for the Vanilla RNN, GRU of varying size, by ISAN and ISAN was represented in Figure 1b."}, {"heading": "4.2 DECOMPOSITION OF CURRENT PREDICTIONS BASED ON PREVIOUS TIME STEPS", "text": "\"Using the linearity of the hidden state dynamics for each sequence of input, we can divide the current latent state ht into posts from different points in time.\" < < < < < < < < < < < < < < < < > > > > > > > > > > > > > >, \"< < < < < < < <,\" \",\" \"> > > > > > > > > >,\" > > >, \",\" \",\" \",\" \",\", \"\", \"\", \"\", \"\", \"\", \",\" \",\" \",\" \",\" \",\" \"\", \"\" \",\" \"\", \"\" \",\" \"\", \"\", \"\" \",\" \",\" \"\", \"\", \"\", \"\" \"\", \"\", \",\" \",\" \",\", \"\", \"\" \",\" \",\", \"\", \"\" \",\", \",\" \",\", \"\" \",\" \",\", \"\", \",\", \"\" \",\", \"\", \",\", \"\" \",\", \",\", \"\" \",\" \",\" \"\", \"\", \",\" \",\", \"\", \",\", \",\", \"\" \"\", \",\" \"\", \",\" \"\" \",\", \"\" \",\", \"\", \",\", \"\" \"\", \",\" \"\", \",\" \",\", \",\", \"\", \",\" \"\" \",\", \",\" \",\" \",\" \",\" \"\", \",\", \"\", \",\" \"\" \",\", \"\", \"\", \",\", \"\", \"\", \"\" \",\" \",\" \"\" \"\" \",\" \"\", \"\", \",\" \"\" \"\", \",\" \",\" \"\" \"\", \",\" \"\" \",\""}, {"heading": "4.3 FROM CHARACTERS TO WORDS", "text": "With the linearity of the hidden state dynamics, we can aggregate all letters belonging to a given word and visualize them as a single contribution to predicting the letters in the next word, allowing us to understand how each preceding word influences the deciphering of the letters of later words.In Figure 5, we show that the words \"the\" and \"annually\" make large contributions to predicting the letters \"r,\" \"n\" in \"turnover,\" as measured by the norms of \"the\" and \"the\" yearly. \""}, {"heading": "4.4 SINGULAR VALUES OF TRANSITION MATRICES", "text": "Since ISAN is composed of switched affine transformations, we can directly analyze the system using the transition matrices. First, the order of magnitude of the singular values for the Wx of each character is shown in Figure 6 a). The singular value spectra show that the spatial and letter markers behave very differently. First, the transition matrix corresponding to the spatial marker has much smaller singular values than the matrices corresponding to the non-spatial markers, which suggests that the spatial marker stimulates forging. Furthermore, all matrices except the spatial marker have an extremely large first singular value. We find that for all non-spatial markers the first correct singular vector has a large inner product with the logit selection vector for predicting a blank mark. This relationship is shown in Figure 6b."}, {"heading": "4.5 CHANGE OF BASIS", "text": "We are free to make a modification of the base on the hidden state, and then run the affine ISAN dynamics in this new base. Note: This modification of the base is not possible for other RNN architectures, since the action of nonlinearity depends on the choice of the bases. In particular, we can construct a \"read base\" that explicitly divides the latent space into a subspace divided by the lines of the read matrix Wro, and its orthogonal complementarity P ro. This representation divides the hidden state dynamics into a 27-dimensional \"read area\" accessed by the read matrix, and a \"computorial\" subspace that includes the remaining 216 \u2212 27 dimensions orthogonally related to the read matrix. We apply this modification of the base to make a complex observation of the hidden offsets."}, {"heading": "5 DISCUSSION", "text": "In this work, we motivated an input-switched affine recurrent network for comprehensibility. We demonstrated that a switch affine architecture achieves the same performance on a voice modeling task as more common RNN architectures, including GRUs and LSTMs, at the same number of maximum parameters. We performed a series of analyses that showed that the simplicity of latent dynamics makes it much easier to understand and interpret the trained RNN."}, {"heading": "5.1 BENEFITS OF AFFINE TRANSITIONS OVER LINEAR", "text": "ISAN uses affine operators to model state transitions associated with each input symbol. Following Equation (1), each transition consists of matrix multiplication and bias vector addition. An important question is whether the distortions are necessary and what the effect of ISAN would be if linear transition operators were used instead of affine operators. Second, we found that affine parameterization of transitions is much easier to train. We have tried to train models only on the basis of linear transitions, but achieve a loss of only 4.1 bits per character, which is equivalent to the performance of an unigrammed character model. Second, affine operators are easier to interpret because they allow a simple visualization of each entry in the final Section 2."}, {"heading": "5.2 COMPUTATIONAL BENEFITS", "text": "As explained in the next two sections, connected affine networks have the potential to be considerably more computational and memory-efficient than standard RNNs for word processing."}, {"heading": "5.2.1 SPARSE PARAMETER ACCESS", "text": "As shown in Figure 1a, the performance of the fixed number of parameters between the ISAN and other recursive networks is almost identical. However, at each step only those parameters are used that are associated with a single input. For K possible input and N parameters, the computational effort per update step is O (N K), a factor of K acceleration compared to non-switched architectures. Likewise, the number of hidden units is O (\u221a N K), a factor of K 1 2 memory improvement for storing the latent state."}, {"heading": "5.2.2 COMPOSITION OF AFFINE UPDATES", "text": "This is possible because the composition of affine transformations is also an affine transformation.This property is used in Section 4.3 to evaluate the linear contributions of words, not characters, which means that the hidden state update, which corresponds to an entire input sequence, can be calculated at identical costs for updating a single character (plus the cost of dictionary searches for the composed transformation).ISAN can therefore achieve very large speeds in input processing, at the expense of increased memory usage, by accumulating large reference tables of the Wx and bx that correspond to the usual input sequences. Of course, practical implementations must include complexities of memory management, batching, etc."}, {"heading": "5.3 FUTURE WORK", "text": "Studying language models at the word level with enormous vocabulary may require additional logic that needs to be scaled up. Adapting this model to continuously evaluating input is another important direction. One approach is to use a tensor factorization similar to that of MRNN (Sutskever et al., 2014); another is to develop a language model that turns on bigrams or trigrams instead of letters or words and targets an average number of affine transformations; training very large linear models has the potential to be extremely fruitful, both because of their improved computer efficiency and because of our ability to better understand and manipulate their behavior."}, {"heading": "6 ACKNOWLEDGEMENTS", "text": "We would like to thank Jasmine Collins for her help and advice and Quoc Le, David Ha and Mohammad Norouzi for helpful conversations."}], "references": [{"title": "Understanding intermediate layers using linear classifier probes", "author": ["Guillaume Alain", "Yoshua Bengio"], "venue": "arXiv preprint arXiv:1610.01644,", "citeRegEx": "Alain and Bengio.,? \\Q2016\\E", "shortCiteRegEx": "Alain and Bengio.", "year": 2016}, {"title": "Learning phrase representations using rnn encoder-decoder for statistical machine translation", "author": ["Kyunghyun Cho", "Bart Van Merri\u00ebnboer", "Caglar Gulcehre", "Dzmitry Bahdanau", "Fethi Bougares", "Holger Schwenk", "Yoshua Bengio"], "venue": "arXiv preprint arXiv:1406.1078,", "citeRegEx": "Cho et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Cho et al\\.", "year": 2014}, {"title": "Capacity and trainability in recurrent neural networks", "author": ["Jasmine Collins", "Jascha Sohl-Dickstein", "David Sussillo"], "venue": "ICLR 2017 submission,", "citeRegEx": "Collins et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Collins et al\\.", "year": 2016}, {"title": "Speech recognition with deep recurrent neural networks", "author": ["Alex Graves", "Abdel-rahman Mohamed", "Geoffrey Hinton"], "venue": "IEEE international conference on acoustics, speech and signal processing,", "citeRegEx": "Graves et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Graves et al\\.", "year": 2013}, {"title": "Lstm: A search space odyssey", "author": ["Klaus Greff", "Rupesh Kumar Srivastava", "Jan Koutn\u00edk", "Bas R Steunebrink", "J\u00fcrgen Schmidhuber"], "venue": "arXiv preprint arXiv:1503.04069,", "citeRegEx": "Greff et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Greff et al\\.", "year": 2015}, {"title": "Long short-term memory", "author": ["Sepp Hochreiter", "J\u00fcrgen Schmidhuber"], "venue": "Neural computation,", "citeRegEx": "Hochreiter and Schmidhuber.,? \\Q1997\\E", "shortCiteRegEx": "Hochreiter and Schmidhuber.", "year": 1997}, {"title": "Observable operator models for discrete stochastic time series", "author": ["Herbert Jaeger"], "venue": "Neural Computation,", "citeRegEx": "Jaeger.,? \\Q2000\\E", "shortCiteRegEx": "Jaeger.", "year": 2000}, {"title": "Visualizing and understanding recurrent networks", "author": ["Andrej Karpathy", "Justin Johnson", "Fei-Fei Li"], "venue": "arXiv preprint arXiv:1506.02078,", "citeRegEx": "Karpathy et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Karpathy et al\\.", "year": 2015}, {"title": "Imagenet classification with deep convolutional neural networks. In Advances in neural information processing", "author": ["Alex Krizhevsky", "Ilya Sutskever", "Geoffrey E Hinton"], "venue": null, "citeRegEx": "Krizhevsky et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Krizhevsky et al\\.", "year": 2012}, {"title": "Building high-level features using large scale unsupervised learning", "author": ["Quoc V Le", "Marc A. Ranzato", "Rajat Monga", "Matthieu Devin", "Kai Chen", "Greg S. Corrado", "J Dean", "Andrew Y Ng"], "venue": "In International Conference on Machine Learning,", "citeRegEx": "Le et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Le et al\\.", "year": 2012}, {"title": "A simple way to initialize recurrent networks of rectified linear units", "author": ["Quoc V Le", "Navdeep Jaitly", "Geoffrey E Hinton"], "venue": "arXiv preprint arXiv:1504.00941,", "citeRegEx": "Le et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Le et al\\.", "year": 2015}, {"title": "Recurrent switching linear dynamical systems", "author": ["Scott W Linderman", "Andrew C Miller", "Ryan P Adams", "David M Blei", "Liam Paninski", "Matthew J Johnson"], "venue": "arXiv preprint arXiv:1610.08466,", "citeRegEx": "Linderman et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Linderman et al\\.", "year": 2016}, {"title": "Large text compression benchmark: About the test data, 2011. URL http://mattmahoney. net/dc/textdata. [Online; accessed 15-November-2016", "author": ["Matt Mahoney"], "venue": null, "citeRegEx": "Mahoney.,? \\Q2016\\E", "shortCiteRegEx": "Mahoney.", "year": 2016}, {"title": "Context-dependent computation by recurrent dynamics in prefrontal cortex", "author": ["Valerio Mante", "David Sussillo", "Krishna V Shenoy", "William T Newsome"], "venue": null, "citeRegEx": "Mante et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Mante et al\\.", "year": 2013}, {"title": "Learning recurrent neural networks with hessian-free optimization", "author": ["James Martens", "Ilya Sutskever"], "venue": "In Proceedings of the 28th International Conference on Machine Learning", "citeRegEx": "Martens and Sutskever.,? \\Q2011\\E", "shortCiteRegEx": "Martens and Sutskever.", "year": 2011}, {"title": "Opening the black box: low-dimensional dynamics in high-dimensional recurrent neural networks", "author": ["David Sussillo", "Omri Barak"], "venue": "Neural computation,", "citeRegEx": "Sussillo and Barak.,? \\Q2013\\E", "shortCiteRegEx": "Sussillo and Barak.", "year": 2013}, {"title": "Generating text with recurrent neural networks", "author": ["Ilya Sutskever", "James Martens", "Geoffrey E Hinton"], "venue": "In Proceedings of the 28th International Conference on Machine Learning", "citeRegEx": "Sutskever et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Sutskever et al\\.", "year": 2011}, {"title": "Sequence to sequence learning with neural networks. In Advances in neural information processing", "author": ["Ilya Sutskever", "Oriol Vinyals", "Quoc V Le"], "venue": null, "citeRegEx": "Sutskever et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Sutskever et al\\.", "year": 2014}], "referenceMentions": [{"referenceID": 8, "context": "Neural networks and the general field of deep learning have made remarkable progress over the last few years in fields such as object recognition (Krizhevsky et al., 2012), language translation (Sutskever et al.", "startOffset": 146, "endOffset": 171}, {"referenceID": 17, "context": ", 2012), language translation (Sutskever et al., 2014), and speech recognition (Graves et al.", "startOffset": 30, "endOffset": 54}, {"referenceID": 3, "context": ", 2014), and speech recognition (Graves et al., 2013).", "startOffset": 32, "endOffset": 53}, {"referenceID": 13, "context": "Another example is the application of neural network methodologies to scientific discovery (Mante et al., 2013).", "startOffset": 91, "endOffset": 111}, {"referenceID": 1, "context": "Surprisingly, we find that this simple architecture performs as well as a vanilla RNN, Gated Recurrent Unit (GRU) (Cho et al., 2014), IRNN (Le et al.", "startOffset": 114, "endOffset": 132}, {"referenceID": 10, "context": ", 2014), IRNN (Le et al., 2015), or Long Short Term Memory (LSTM) (Hochreiter & Schmidhuber, 1997) in this task, despite being a simpler and potentially far more computationally efficient architecture.", "startOffset": 14, "endOffset": 31}, {"referenceID": 7, "context": "Work by the authors of (Karpathy et al., 2015) attempted to use character-based language modeling to begin to understand how the LSTM (Hochreiter & Schmidhuber, 1997) functions.", "startOffset": 23, "endOffset": 46}, {"referenceID": 4, "context": "The authors of (Greff et al., 2015) engaged in a large study to understand the relative importance of the various components of an LSTM.", "startOffset": 15, "endOffset": 35}, {"referenceID": 2, "context": "The authors of (Collins et al., 2016) performed an enormous hyperparameter study to disentangle the effects of capacity and trainability in a number of RNN architectures.", "startOffset": 15, "endOffset": 37}, {"referenceID": 9, "context": "(Le et al., 2012).", "startOffset": 0, "endOffset": 17}, {"referenceID": 11, "context": "A recent example is the switched linear dynamical system in (Linderman et al., 2016).", "startOffset": 60, "endOffset": 84}, {"referenceID": 16, "context": "Finally, multiplicative neural networks (MRNNs) were proposed precisely for character based language modeling in (Sutskever et al., 2011; Martens & Sutskever, 2011).", "startOffset": 113, "endOffset": 164}, {"referenceID": 6, "context": "The Observable Operator Model (OOM) (Jaeger, 2000) is similar to the ISAN in that the OOM updates a latent state using a separate transition matrix for each input symbol and performs probabilistic sequence modeling.", "startOffset": 36, "endOffset": 50}, {"referenceID": 2, "context": "In line with (Collins et al., 2016) we split the training data into 80%, 10%, and 10% for train, test, and evaluation set respectively.", "startOffset": 13, "endOffset": 35}, {"referenceID": 2, "context": "The network was trained with the same hyperparameter tuning infrastructure as in (Collins et al., 2016).", "startOffset": 81, "endOffset": 103}, {"referenceID": 2, "context": "The values reported for all other architectures are taken from (Collins et al., 2016).", "startOffset": 63, "endOffset": 85}, {"referenceID": 17, "context": "One approach is to use a tensor factorization similar to that employed by the MRNN (Sutskever et al., 2014).", "startOffset": 83, "endOffset": 107}], "year": 2017, "abstractText": "The computational mechanisms by which nonlinear recurrent neural networks (RNNs) achieve their goals remains an open question. There exist many problem domains where intelligibility of the network model is crucial for deployment. Here we introduce a recurrent architecture composed of input-switched affine transformations, in other words an RNN without any nonlinearity and with one set of weights per input. We show that this architecture achieves near identical performance to traditional architectures on language modeling of Wikipedia text, for the same number of model parameters. It can obtain this performance with the potential for computational speedup compared to existing methods, by precomputing the composed affine transformations corresponding to longer input sequences. As our architecture is affine, we are able to understand the mechanisms by which it functions using linear methods. For example, we show how the network linearly combines contributions from the past to make predictions at the current time step. We show how representations for words can be combined in order to understand how context is transferred across word boundaries. Finally, we demonstrate how the system can be executed and analyzed in arbitrary bases to aid understanding.", "creator": "LaTeX with hyperref package"}}}