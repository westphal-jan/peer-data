{"id": "1611.09621", "review": {"conference": "AAAI", "VERSION": "v1", "DATE_OF_SUBMISSION": "29-Nov-2016", "title": "Associative Memory Using Dictionary Learning and Expander Decoding", "abstract": "An associative memory is a framework of content-addressable memory that stores a collection of message vectors (or a dataset) over a neural network while enabling a neurally feasible mechanism to recover any message in the dataset from its noisy version. Designing an associative memory requires addressing two main tasks: 1) learning phase: given a dataset, learn a concise representation of the dataset in the form of a graphical model (or a neural network), 2) recall phase: given a noisy version of a message vector from the dataset, output the correct message vector via a neurally feasible algorithm over the network learnt during the learning phase. This paper studies the problem of designing a class of neural associative memories which learns a network representation for a large dataset that ensures correction against a large number of adversarial errors during the recall phase. Specifically, the associative memories designed in this paper can store dataset containing $\\exp(n)$ $n$-length message vectors over a network with $O(n)$ nodes and can tolerate $\\Omega(\\frac{n}{{\\rm polylog} n})$ adversarial errors. This paper carries out this memory design by mapping the learning phase and recall phase to the tasks of dictionary learning with a square dictionary and iterative error correction in an expander code, respectively.", "histories": [["v1", "Tue, 29 Nov 2016 13:27:18 GMT  (47kb,D)", "http://arxiv.org/abs/1611.09621v1", "To appear in AAAI 2017"]], "COMMENTS": "To appear in AAAI 2017", "reviews": [], "SUBJECTS": "stat.ML cs.IT cs.LG math.IT", "authors": ["arya mazumdar", "ankit singh rawat"], "accepted": true, "id": "1611.09621"}, "pdf": {"name": "1611.09621.pdf", "metadata": {"source": "CRF", "title": "Associative Memory using Dictionary Learning and Expander Decoding", "authors": ["Arya Mazumdar", "Ankit Singh Rawat"], "emails": ["arya@cs.umass.edu,", "asrawat@mit.edu"], "sections": [{"heading": "1 Introduction", "text": "Associative reminders aim to address a problem that naturally arises in many information processing systems: faced with a dataset M consisting of n-length vectors, a mechanism has been developed for precisely storing this dataset, so that any future query must be stored in the form of a neural network. (The associative storage solutions to this problem should be universally implementable in order to have two key components: 1) dataset must be stored in the form of a neural network (diagram) and 2) the mechanism for mapping a noisy query to the associated valid vector should be implemented across the network in an iterative neuronically feasible manner, using only local calculations on the nodes of the corresponding network based on the information obtained from its adjacent nodes."}, {"heading": "2 Main results and techniques", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "2.1 Model for datasets", "text": "In our first model, we look at message patterns as vectors over R. In the second model, we comment on neural association stores that store binary message patterns obtained through our approach."}, {"heading": "2.1.1 Dataset over real numbers: the sparse-sub-Gaussian model", "text": "We assume that the message forms a linear subspace defined by sparse linear constraints over R. Let M'Rn denote the set of message vectors (signals) that must be stored in the associative memory. Let B be a m'n matrix that includes the linear constraints that define the message setM. Specifically, we must provide a stochastic model for the matrix B. In this sense, let us consider a random interaction of sparse matrices x = (x1, x2,.., xn). To fully specify the message setM, let us consider another stochastic model for the matrix B. In this sense, let us consider a random interplay of sparse matrices. For each j [n]: = {1, 2,."}, {"heading": "2.1.2 Binary dataset", "text": "Our model of the binary dataset is the same as above, except that 1) M {+ 1, \u2212 1} n and 2) Ri, j via {+ 1, \u2212 1} in (3) is uniform, and the state of (1) must be fulfilled for each x-M."}, {"heading": "2.2 Our main results", "text": "We find that for a datasetM corresponding to the zero space defined by the matrix B, the matrix B can be recovered exactly from the dataset in polynomial time. It is important for us to restore the matrix B exactly. Nevertheless, we claim that it is possible to accurately reconstruct the matrix B generated by the sparse sub-Gaussian method in order to be successful. This matrixB allows error correction during the recovery phase with the help of a neural realisable algorithm. We sum up the parameters achieved by such a reminder."}, {"heading": "3 Proof of Theorem 1", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "3.1 Learning phase of associative memory design", "text": "As discussed in the previous section, the learning phase of associative memory design under the dataset model considered in this essay can be mapped to the problem of dictionary learning with a square dictionary. The same dictionary learning problem with slightly different random code model for the coefficient vector was investigated in [1,3,30]. In Appendix A, we briefly describe this working method together with the results used in this essay. We then use the dictionary learning algorithm used in [1] to learn exactly the matrix B that defines our dataset, and comment on the changes [1] required in analyzing Adamczak to obtain guarantees for the performance of this algorithm."}, {"heading": "3.1.1 Exact recovery of the matrix B", "text": "Using the dictionary learning algorithm of [1], we design the learning phase for an associative memory that stores the message described in Sec. 2.1. The learning phase consists of the following two steps. [1] Given the message vectors from the DatasetM, we first construct a basis for the subspace orthogonal to the dataset SubspaceM = {x: Bx = 0}. The learning phase consists of dim (M) = n \u2212 m.2. Let A-Rm \u00b7 n denote the base obtained in the previous step. As it is a full-fledged matrix, haveA = DB, where D-Rm \u00b7 m is a non-singular matrix. Now, we use the modified ERSpUD learning algorithm [1] with the matrix."}, {"heading": "3.2 Recall phase of associative memory design", "text": "The correctness of the iterative algorithm is based on the observation that the bipartite graph associated with Matrix B, which defines our DatasetM, is a good expander graph. We formalize this expansion property first in the following result. We then present the iterative algorithm and show that it can be proven to tolerate adverse errors. [1] We focus on the sparse Sub-Gaussian model as opposed to the Bernoulli Sub-Gaussian model as the bipartite graph associated with the Matrix B produced by the sparse Sub-Gaussian model."}, {"heading": "3.2.1 Expansion property of the bipartite graph defined by B", "text": "The matrix B, which defines our DatasetM, gives the m \u00b7 n adjacence matrix of the diagram G ', i.e., for the index records of the left and right vertices we have an edge (\", r). (, r) EB is w', r = Br '. It follows from the sparse Sub-Gaussian model (cf. Sec. 2.1), which generates the random matrix B, that each vertice in L has degrees d and each of the d neighbors for a vertice in L, which we randomly select from the set of right vertices R with substitution. The following result states that expansion properties that consider such a graph to be highly probable."}, {"heading": "3.2.2 Iterative decoding algorithm", "text": "Indeed it is so that it is a way in which the people in the real world in the real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real rios"}, {"heading": "4 Proof sketch of Theorem 3: Associative memory storing binary vectors", "text": "Since the graph defined by B \"is still an expander (with edge weights {+ 1, \u2212 1}), we rely on the same expander decoding algorithm for the retrieval phase. We only want to guarantee that the algorithm for learning B is the same as that of Theorem 1. Instead of the random model we considered in Sec. 2.1.2, we consider a random matrix B (n \u2212 \u03b1n log (d log n) / log n) w.h.p, each line of which has independently and uniformly selected d\" unequal \"({+ 1, \u2212 1}) values. This model allows us to directly analyze the number of binary vectors in zero space, while the original model provides the same estimate, but with much longer analysis that we omit the interest of space."}, {"heading": "5 Simulation results", "text": "In fact, it is that we are able to assert ourselves, that we are able, that we will be able, that we will be able, that we will be able, that we will be able, and that we will be able, that we will be able, to put ourselves in a position, to put ourselves in a position, to put ourselves in a position, to put ourselves in a position, to put ourselves in a position, to put ourselves in a position, to put ourselves in the position we are in."}, {"heading": "A The modified ER-SpUD algorithm and proof of Theorem 4", "text": "In this case, it is as if it were an unforeseen, unforeseen, unforeseen, unforeseen, unforeseen, unforeseen, unforeseen, unforeseen, unforeseen, unforeseen, unforeseen, unforeseen, unforeseen, unforeseen, unforeseen, unforeseen, unforeseen, unforeseen, unforeseen, unforeseen, unforeseen, unforeseen, unforeseen, unforeseen, unforeseen, unforeseen, unforeseen, unforeseen, unforeseen, unforeseen, unforeseen, unforeseen,"}], "references": [], "referenceMentions": [], "year": 2016, "abstractText": "<lb>An associative memory is a framework of content-addressable memory that stores a collection of<lb>message vectors (or a dataset) over a neural network while enabling a neurally feasible mechanism to<lb>recover any message in the dataset from its noisy version. Designing an associative memory requires<lb>addressing two main tasks: 1) learning phase: given a dataset, learn a concise representation of the<lb>dataset in the form of a graphical model (or a neural network), 2) recall phase: given a noisy version of<lb>a message vector from the dataset, output the correct message vector via a neurally feasible algorithm<lb>over the network learnt during the learning phase. This paper studies the problem of designing a class<lb>of neural associative memories which learns a network representation for a large dataset that ensures<lb>correction against a large number of adversarial errors during the recall phase. Specifically, the associa-<lb>tive memories designed in this paper can store dataset containing exp(n) n-length message vectors over<lb>a network with O(n) nodes and can tolerate \u03a9( n<lb>polylogn ) adversarial errors. This paper carries out this<lb>memory design by mapping the learning phase and recall phase to the tasks of dictionary learning with<lb>a square dictionary and iterative error correction in an expander code, respectively.", "creator": "LaTeX with hyperref package"}}}