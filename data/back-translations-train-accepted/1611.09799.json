{"id": "1611.09799", "review": {"conference": "AAAI", "VERSION": "v1", "DATE_OF_SUBMISSION": "29-Nov-2016", "title": "Geometry of Compositionality", "abstract": "This paper proposes a simple test for compositionality (i.e., literal usage) of a word or phrase in a context-specific way. The test is computationally simple, relying on no external resources and only uses a set of trained word vectors. Experiments show that the proposed method is competitive with state of the art and displays high accuracy in context-specific compositionality detection of a variety of natural language phenomena (idiomaticity, sarcasm, metaphor) for different datasets in multiple languages. The key insight is to connect compositionality to a curious geometric property of word embeddings, which is of independent interest.", "histories": [["v1", "Tue, 29 Nov 2016 19:23:41 GMT  (1434kb,D)", "http://arxiv.org/abs/1611.09799v1", null]], "reviews": [], "SUBJECTS": "cs.CL", "authors": ["hongyu gong", "suma bhat", "pramod viswanath"], "accepted": true, "id": "1611.09799"}, "pdf": {"name": "1611.09799.pdf", "metadata": {"source": "CRF", "title": "Geometry of Compositionality", "authors": ["Hongyu Gong", "Pramod Viswanath"], "emails": ["hgong6@illinois.edu,", "spbhat2@illinois.edu,", "pramodv@illinois.edu"], "sections": [{"heading": "1 Introduction", "text": "An expression of this kind is the expression of multilingual expressions (MWEs) - phrases with semantic peculiarities that transcend the boundaries of the word. (Sag et al., 2002) Examples of MWEs broadly include the formulation of terms that reflect the terminology of the individual terms. As such, these phrases are idiotic because their meanings cannot be derived from the meaning of their constituent parts, and are therefore designated as non-compositional phrases that are directed against compositional phrases. A particularly exciting aspect of MWEs is their ability to engage with degrees of compositionality that they have in their context. Consider, for example, two contexts in which the phrase bad egg occurs. (1) It is certain that a bad egg does not do good business for those who care for their clientele. (2) I do not know which egg it lays, but uses it!"}, {"heading": "2 Compositionality and the Geometry of Word Embeddings", "text": "Our most important contribution is the discovery of a geometric property of vector embedding of context words (with the exception of function words) within a sentence: They roughly occupy a low-dimensional linear subspace that can be empirically extracted using a standard dimensionality reduction technology: Principal Component Analysis (PCA) (Shlen's) Matrix X. \"We stack the d-dimensional vectors v1,., vn that correspond to the n words in a sentence to form a d \u00b7 n matrix X. PCA finds a d \u00b7 m (m < n) matrix X\" that maximizes data variance with reduced dimension. Here X \"consists of m new vectors, v \u00b2 1, v \u00b2 m.\" Now the original data X \"is represented by fewer vectors of X where vectors and v \u00b2 d.\""}, {"heading": "3 MWE Compositionality Detection", "text": "We evaluate empirically our contextual method of recognizing compositivity, taking into account three specific but very different tasks: a) predicting the compositivity of phrases that, depending on the context, may have either the idiomatic or the literal sense (the focus of this section), b) recognizing sarcasm at the level of a particular word and at the level of a sentence, and c) detecting whether a particular phrase has been used in its metaphorical or literal sense. To highlight the multilingual and linguistic agnostic capabilities of our algorithms, we use standard data sets used in modern studies for each of the tasks, as well as those we have constructed specifically for the experiments. In addition to the data sets available in English, we include data sets in German and Chinese to highlight the multilingual and lingual agnostic capabilities of our algorithms."}, {"heading": "3.1 Experiment I: Phrase Compositionality", "text": "In this part, we evaluate the performance of our algorithm in capturing the semantics of the Chinese context and predicting the composition of phrases we throw as a binary classification task - to determine the composition of the phrase in each context. In relation to the examples in Table 1, the task is to predict that the phrase will be used in its compositional sense firstly and secondly as a non-compositional one. We conduct experiments with different word embeddings (CBOW and MSSG) and different compositional representations for both the phrase and the context (average and PCA). Bi-context dataset: We construct 2 datasets 1 (one for English and the other for Chinese), consisting of a list of phrases and their respective contexts (compositional and non-compositional). The English dataset contains 104 ambiguous phrases extracted from the idiom dictionary (2016 Dictionary, the Chinese dataset) and the Chinese dataset."}, {"heading": "3.2 Experiment II: Lexical Idiomaticity", "text": "Unlike other countries in the world, where people are able to determine for themselves how they want to live and how they want to live, the world is able to determine for itself and understand how they want to live and how they want to live."}, {"heading": "4 Sarcasm Detection", "text": "Sarcasms, also known as irony, are expressions whose actual meaning is very different - and often contrary to their literal meaning - and are examples of non-compositional use (Davidov et al., 2010; Riloff et al., 2013). For example, the word \"nice\" is used sarcastically: \"It is so beautiful that a nice video to rescue an animal can quickly turn comments into political debates and racist attacks.\" The context points to sarcasm; in this example, \"beautiful\" is not consistent with its context words \"debate\" and \"attack.\" These ideas have been used in previous work to create elaborate features (designed based on a large labeled training set) and build a sarcasm recognition system (Ghosh et al., 2015). We evaluate our algorithm for recognizing compositivity directly at this task."}, {"heading": "4.1 Qualitative Test", "text": "We use a subset of the tweets in the dataset of (Ghosh et al., 2015) and study words that are used both literally and sarcastically (e.g. love, as it occurs in our downloaded datasets). We choose six words \"good,\" \"yes,\" \"beautiful\" and \"always,\" which occur with sufficient frequency in our downloaded datasets. Let's take the word \"beautiful\" as an example and consider its appearance in the sentence \"It is so beautiful that a nice video can quickly turn an animal's savings into political debate and racist attacks.\" Our compositionality achieves algorithms (cf. Section 2) is directly applicable: We extract the neighborly content we hold in our hands."}, {"heading": "4.2 Quantitative Test", "text": "This dataset consists of 3,020 annotated comments for a total of 10,401 sentences, and each comment is labeled \"ironic,\" \"don't know,\" and \"unironic.\" An example of an ironic comment is \"It's amazing how Democrats see money. It has to come from somewhere where you idiots and you have signed up to pay the bill. Congratulations.\" The task is to determine whether a given comment is ironic or not. (Wallace et al., 2014) considers the 50,000 most common unigrams and bigrams and uses binary word stumps and punctuations as traits, followed by a linear SVM kernel, grid search for parameter tuning and quintuple cross-validation. (Wallace et al., 2014) Instead, we generate compositionality score traits based on POS tags to allow direct comparison with the state of the art. (Wallace et al, 2014) Algorithm descriptions that we select as the first choice of words (VB)."}, {"heading": "5 Metaphor Detection", "text": "In fact, it is such that most of them will be able to move to another world, in which they are able to integrate themselves, in which they are able to live, in which they are able to move, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live in which they live, in which they live in which they live, in which they live in which they live, in which they live in which they live, in which they live in which they live, in which they live, in which they live in which they live, in which they live, in which they live in which they live, in which they live, in which they live, in which they live in which they live, in which they live, in which they live, in which they live, in which they live in which they live, in which they live, in which they live, in which they live in which they live, in which they live, in which they live in which they live, in which they live, in which they live in which they live, in which they live, in which they live, in which they live in which they live, in which they live, in which they live in which they live, in which they live, in which they live in which they live, in which they live, in which they live in which they live, in which they live in which they live, in which they live, in which they live in which they live, in which they live in which they live, in which they live in which they live, in which they live in which they live, in which they live, in which they live in which they live in which they"}, {"heading": "6 Related Works", "text": "It is a simple but robust system that moves in multiple environments; for example, such a representation is successfully used for sentiment predictions (Faruqui et al., 2015) and in (Kenter and de Rijke, 2015) to study text similarity; average word embedding is also used (Kenter et al., 2016) in conjunction with a neural network architecture to predict the surrounding sentences from the input of sentence embeddings; and computational models of sentential semantics have also shown that they perform reasonably well with average word embeddings (Yu et al., 2014; Gershman and Tenenbaum, 2015 et al., 2016; Wieting et al., 2015). In the compositivity tests of this paper, average representation performs reasonably well, although subspace representation is significantly superior."}, {"heading": "7 Conclusion", "text": "We bring MWEs, sarcasms and metaphors under a common umbrella of composition, followed by a simple, unified framework to study them; this is our central contribution. The proposed method of recognizing word / phrase composition based on local context is simple and provides a clear geometric view. We do not rely on external resources and produce very good results in multiple languages and in a wide variety of environments (metaphors, sarcastic and idiomatic uses); the method naturally scales to handle complications such as invisible phrases and polysemes, and achieves comparable or superior results over the state of the art (which are monitored methods based on sophisticated feature engineering and sometimes use abundant external linguistic resources) on standard datasets."}], "references": [{"title": "Fine-grained analysis of sentence embeddings using auxiliary prediction tasks", "author": ["Yossi Adi", "Einat Kermany", "Yonatan Belinkov", "Ofer Lavi", "Yoav Goldberg."], "venue": "CoRR, abs/1608.04207.", "citeRegEx": "Adi et al\\.,? 2016", "shortCiteRegEx": "Adi et al\\.", "year": 2016}, {"title": "Polyglot: Distributed word representations for multilingual nlp", "author": ["Rami Al-Rfou", "Bryan Perozzi", "Steven Skiena."], "venue": "Proceedings of the Seventeenth Conference on Computational Natural Language Learning, pages 183\u2013192, Sofia, Bulgaria, August. Association", "citeRegEx": "Al.Rfou et al\\.,? 2013", "shortCiteRegEx": "Al.Rfou et al\\.", "year": 2013}, {"title": "Acquiring Phrasal Lexicons from Corpora", "author": ["Colin James Bannard."], "venue": "Ph.D. thesis, University of Edinburgh.", "citeRegEx": "Bannard.,? 2006", "shortCiteRegEx": "Bannard.", "year": 2006}, {"title": "Identifying bilingual multiword expressions for statistical machine translation", "author": ["Dhouha Bouamor", "Nasredine Semmar", "Pierre Zweigenbaum."], "venue": "LREC, pages 674\u2013679.", "citeRegEx": "Bouamor et al\\.,? 2012", "shortCiteRegEx": "Bouamor et al\\.", "year": 2012}, {"title": "Using imageability and topic chaining to locate metaphors in linguistic corpora", "author": ["George Aaron Broadwell", "Umit Boz", "Ignacio Cases", "Tomek Strzalkowski", "Laurie Feldman", "Sarah Taylor", "Samira Shaikh", "Ting Liu", "Kit Cho", "Nick Webb."], "venue": "International", "citeRegEx": "Broadwell et al\\.,? 2013", "shortCiteRegEx": "Broadwell et al\\.", "year": 2013}, {"title": "Available at: http://www", "author": ["ChineseDictionary."], "venue": "chinese-dictionary.org. Accessed:2016-05-", "citeRegEx": "ChineseDictionary.,? 2016", "shortCiteRegEx": "ChineseDictionary.", "year": 2016}, {"title": "The mrc psycholinguistic database", "author": ["Max Coltheart."], "venue": "The Quarterly Journal of Experimental Psychology, 33(4):497\u2013505.", "citeRegEx": "Coltheart.,? 1981", "shortCiteRegEx": "Coltheart.", "year": 1981}, {"title": "Pulling their weight: Exploiting syntactic forms for the automatic identification of idiomatic expressions in context", "author": ["Paul Cook", "Afsaneh Fazly", "Suzanne Stevenson."], "venue": "Proceedings of the workshop on a broader perspective on multiword expressions, pages", "citeRegEx": "Cook et al\\.,? 2007", "shortCiteRegEx": "Cook et al\\.", "year": 2007}, {"title": "Semi-supervised recognition of sarcastic sentences in twitter and amazon", "author": ["Dmitry Davidov", "Oren Tsur", "Ari Rappoport."], "venue": "Proceedings of the fourteenth conference on computational natural language learning, pages 107\u2013116. Association for Computational", "citeRegEx": "Davidov et al\\.,? 2010", "shortCiteRegEx": "Davidov et al\\.", "year": 2010}, {"title": "Probing for semantic evidence of composition by means of simple classification tasks", "author": ["Allyson Ettinger", "Ahmed Elgohary", "Philip Resnik."], "venue": "the Association for Computational Linguistics, page 134.", "citeRegEx": "Ettinger et al\\.,? 2016", "shortCiteRegEx": "Ettinger et al\\.", "year": 2016}, {"title": "Retrofitting word vectors to semantic lexicons", "author": ["Manaal Faruqui", "Jesse Dodge", "Sujay Kumar Jauhar", "Chris Dyer", "Eduard Hovy", "Noah A Smith."], "venue": "Association for Computational Linguistics.", "citeRegEx": "Faruqui et al\\.,? 2015", "shortCiteRegEx": "Faruqui et al\\.", "year": 2015}, {"title": "Distinguishing subtypes of multiword expressions using", "author": ["Afsaneh Fazly", "Suzanne Stevenson"], "venue": null, "citeRegEx": "Fazly and Stevenson.,? \\Q2007\\E", "shortCiteRegEx": "Fazly and Stevenson.", "year": 2007}, {"title": "Phrase similarity in humans and machines", "author": ["Samuel J Gershman", "Joshua B Tenenbaum."], "venue": "Proceedings of the 37th Annual Conference of the Cognitive Science Society. Citeseer.", "citeRegEx": "Gershman and Tenenbaum.,? 2015", "shortCiteRegEx": "Gershman and Tenenbaum.", "year": 2015}, {"title": "Sarcastic or not: Word embeddings to predict the literal or sarcastic meaning of words", "author": ["Debanjan Ghosh", "Weiwei Guo", "Smaranda Muresan."], "venue": "pages 1003\u2013 1012.", "citeRegEx": "Ghosh et al\\.,? 2015", "shortCiteRegEx": "Ghosh et al\\.", "year": 2015}, {"title": "Available at: https://books", "author": ["GoogleBooks."], "venue": "google.com. Accessed: 2016-05-03.", "citeRegEx": "GoogleBooks.,? 2016", "shortCiteRegEx": "GoogleBooks.", "year": 2016}, {"title": "Lstm: A search space odyssey", "author": ["Klaus Greff", "Rupesh Kumar Srivastava", "Jan Koutn\u0131\u0301k", "Bas R Steunebrink", "J\u00fcrgen Schmidhuber"], "venue": "arXiv preprint arXiv:1503.04069", "citeRegEx": "Greff et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Greff et al\\.", "year": 2015}, {"title": "Automatic identification of non-compositional multi-word expressions using latent semantic analysis", "author": ["Graham Katz", "Eugenie Giesbrecht."], "venue": "Proceedings of the Workshop on Multiword Expressions: Identifying and Exploiting Underlying Properties, pages", "citeRegEx": "Katz and Giesbrecht.,? 2006", "shortCiteRegEx": "Katz and Giesbrecht.", "year": 2006}, {"title": "Short text similarity with word embeddings", "author": ["Tom Kenter", "Maarten de Rijke."], "venue": "Proceedings of the 24th ACM International on Conference on Information and Knowledge Management, pages 1411\u20131420. ACM.", "citeRegEx": "Kenter and Rijke.,? 2015", "shortCiteRegEx": "Kenter and Rijke.", "year": 2015}, {"title": "Siamese cbow: Optimizing word embeddings for sentence representations", "author": ["Tom Kenter", "Alexey Borisov", "Maarten de Rijke."], "venue": "Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics, page 941951.", "citeRegEx": "Kenter et al\\.,? 2016", "shortCiteRegEx": "Kenter et al\\.", "year": 2016}, {"title": "Conceptual metaphor in everyday language", "author": ["George Lakoff", "Mark Johnson."], "venue": "The journal of Philosophy, 77(8):453\u2013486.", "citeRegEx": "Lakoff and Johnson.,? 1980", "shortCiteRegEx": "Lakoff and Johnson.", "year": 1980}, {"title": "The perfect solution for detecting sarcasm in tweets", "author": ["CC Liebrecht", "FA Kunneman", "APJ van den Bosch"], "venue": null, "citeRegEx": "Liebrecht et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Liebrecht et al\\.", "year": 2013}, {"title": "Automatic identification of noncompositional phrases", "author": ["Dekang Lin."], "venue": "Proceedings of the 37th annual meeting of the Association for Computational Linguistics on Computational Linguistics, pages 317\u2013 324. Association for Computational Linguistics.", "citeRegEx": "Lin.,? 1999", "shortCiteRegEx": "Lin.", "year": 1999}, {"title": "Collocations", "author": ["Chris Manning", "Hinrich Sch\u00fctze."], "venue": "Foundations of statistical natural language processing, pages 141\u201377.", "citeRegEx": "Manning and Sch\u00fctze.,? 1999", "shortCiteRegEx": "Manning and Sch\u00fctze.", "year": 1999}, {"title": "Cormet: a computational, corpus-based conventional metaphor extraction system", "author": ["Zachary J Mason."], "venue": "Computational linguistics, 30(1):23\u201344.", "citeRegEx": "Mason.,? 2004", "shortCiteRegEx": "Mason.", "year": 2004}, {"title": "Who cares about sarcastic tweets? investigating the impact", "author": ["Diana Maynard", "Mark A Greenwood"], "venue": null, "citeRegEx": "Maynard and Greenwood.,? \\Q2014\\E", "shortCiteRegEx": "Maynard and Greenwood.", "year": 2014}, {"title": "Detecting a continuum of compositionality in phrasal verbs", "author": ["Diana McCarthy", "Bill Keller", "John Carroll."], "venue": "Proceedings of the ACL 2003 workshop on Multiword expressions: analysis, acquisition and treatment-Volume 18, pages 73\u201380. Association for", "citeRegEx": "McCarthy et al\\.,? 2003", "shortCiteRegEx": "McCarthy et al\\.", "year": 2003}, {"title": "Composition in distributional models of semantics", "author": ["Jeff Mitchell", "Mirella Lapata."], "venue": "Cognitive science, 34(8):1388\u20131429.", "citeRegEx": "Mitchell and Lapata.,? 2010", "shortCiteRegEx": "Mitchell and Lapata.", "year": 2010}, {"title": "Geometry of polysemy", "author": ["Jiaqi Mu", "Suma Bhat", "Pramod Viswanath."], "venue": "CoRR, abs/1610.07569.", "citeRegEx": "Mu et al\\.,? 2016", "shortCiteRegEx": "Mu et al\\.", "year": 2016}, {"title": "Efficient nonparametric estimation of multiple embeddings per word in vector space", "author": ["Arvind Neelakantan", "Jeevan Shankar", "Alexandre Passos", "Andrew McCallum."], "venue": "Conference on Empirical Methods in Natural Language Processing.", "citeRegEx": "Neelakantan et al\\.,? 2014", "shortCiteRegEx": "Neelakantan et al\\.", "year": 2014}, {"title": "Glove: Global vectors for word representation", "author": ["Jeffrey Pennington", "Richard Socher", "Christopher D Manning."], "venue": "EMNLP, volume 14, pages 1532\u20131543.", "citeRegEx": "Pennington et al\\.,? 2014", "shortCiteRegEx": "Pennington et al\\.", "year": 2014}, {"title": "An empirical study on compositionality in compound nouns", "author": ["Siva Reddy", "Diana McCarthy", "Suresh Manandhar."], "venue": "IJCNLP, pages 210\u2013218.", "citeRegEx": "Reddy et al\\.,? 2011", "shortCiteRegEx": "Reddy et al\\.", "year": 2011}, {"title": "Sarcasm as contrast between a positive sentiment and negative situation", "author": ["Ellen Riloff", "Ashequl Qadir", "Prafulla Surve", "Lalindra De Silva", "Nathan Gilbert", "Ruihong Huang."], "venue": "EMNLP, volume 13, pages 704\u2013 714.", "citeRegEx": "Riloff et al\\.,? 2013", "shortCiteRegEx": "Riloff et al\\.", "year": 2013}, {"title": "Contextual correlates of synonymy", "author": ["Herbert Rubenstein", "John B Goodenough."], "venue": "Communications of the ACM, 8(10):627\u2013633.", "citeRegEx": "Rubenstein and Goodenough.,? 1965", "shortCiteRegEx": "Rubenstein and Goodenough.", "year": 1965}, {"title": "Multiword expressions: A pain in the neck for nlp", "author": ["Ivan A Sag", "Timothy Baldwin", "Francis Bond", "Ann Copestake", "Dan Flickinger."], "venue": "Computational Linguistics and Intelligent Text Processing, pages 1\u2013", "citeRegEx": "Sag et al\\.,? 2002", "shortCiteRegEx": "Sag et al\\.", "year": 2002}, {"title": "Predicting the compositionality of multiword expressions using translations in multiple languages", "author": ["Bahar Salehi", "Paul Cook."], "venue": "Second Joint Conference on Lexical and Computational Semantics, volume 1, pages 266\u2013275.", "citeRegEx": "Salehi and Cook.,? 2013", "shortCiteRegEx": "Salehi and Cook.", "year": 2013}, {"title": "Detecting non-compositional mwe components using wiktionary", "author": ["Bahar Salehi", "Paul Cook", "Timothy Baldwin."], "venue": "Conference on Empirical Methods in Natural Language Processing, pages 1792\u20131797.", "citeRegEx": "Salehi et al\\.,? 2014a", "shortCiteRegEx": "Salehi et al\\.", "year": 2014}, {"title": "Using distributional similarity of multi-way translations to predict multiword expression compositionality", "author": ["Bahar Salehi", "Paul Cook", "Timothy Baldwin."], "venue": "European Chapter of the Association for Computational Linguistics, pages 472\u2013481.", "citeRegEx": "Salehi et al\\.,? 2014b", "shortCiteRegEx": "Salehi et al\\.", "year": 2014}, {"title": "A word embedding approach to predicting the compositionality of multiword expressions", "author": ["Bahar Salehi", "Paul Cook", "Timothy Baldwin."], "venue": "the North American Chapter of the Association for Computational Linguistics, pages 977\u2013983.", "citeRegEx": "Salehi et al\\.,? 2015", "shortCiteRegEx": "Salehi et al\\.", "year": 2015}, {"title": "Exploring vector space models to predict the compositionality of german noun-noun compounds", "author": ["Sabine Schulte im Walde", "Stefan M\u00fcller", "Stephen Roller."], "venue": "Proceedings of the 2nd Joint Conference on Lexical and Computational Semantics, pages 255\u2013", "citeRegEx": "Walde et al\\.,? 2013", "shortCiteRegEx": "Walde et al\\.", "year": 2013}, {"title": "A tutorial on principal component analysis", "author": ["Jonathon Shlens."], "venue": "CoRR, abs/1404.1100.", "citeRegEx": "Shlens.,? 2014", "shortCiteRegEx": "Shlens.", "year": 2014}, {"title": "Metaphor detection with cross-lingual model transfer", "author": ["Yulia Tsvetkov", "Leonid Boytsov", "Anatole Gershman", "Eric Nyberg", "Chris Dyer"], "venue": null, "citeRegEx": "Tsvetkov et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Tsvetkov et al\\.", "year": 2014}, {"title": "Literal and metaphorical sense identification through concrete and abstract context", "author": ["Peter D Turney", "Yair Neuman", "Dan Assaf", "Yohai Cohen."], "venue": "Proceedings of the 2011 Conference on the Empirical Methods in Natural Language Processing, pages 680\u2013", "citeRegEx": "Turney et al\\.,? 2011", "shortCiteRegEx": "Turney et al\\.", "year": 2011}, {"title": "Humans require context to infer ironic intent (so computers probably do, too)", "author": ["Byron C Wallace", "Laura Kertz Do Kook Choe", "Laura Kertz", "Eugene Charniak."], "venue": "the Association for Computational Linguistics, pages 512\u2013516.", "citeRegEx": "Wallace et al\\.,? 2014", "shortCiteRegEx": "Wallace et al\\.", "year": 2014}, {"title": "Towards universal paraphrastic sentence embeddings", "author": ["John Wieting", "Mohit Bansal", "Kevin Gimpel", "Karen Livescu."], "venue": "arXiv preprint arXiv:1511.08198.", "citeRegEx": "Wieting et al\\.,? 2015", "shortCiteRegEx": "Wieting et al\\.", "year": 2015}, {"title": "Deep learning for answer sentence selection", "author": ["Lei Yu", "Karl Moritz Hermann", "Phil Blunsom", "Stephen Pulman."], "venue": "arXiv preprint arXiv:1412.1632.", "citeRegEx": "Yu et al\\.,? 2014", "shortCiteRegEx": "Yu et al\\.", "year": 2014}], "referenceMentions": [{"referenceID": 33, "context": "One expression type is multiword expressions (MWEs) \u2013 phrases with semantic idiosyncrasies that cross word boundaries (Sag et al., 2002).", "startOffset": 118, "endOffset": 136}, {"referenceID": 3, "context": ", chemin de fer from French to English to be way of iron in place of railway (Bouamor et al., 2012).", "startOffset": 77, "endOffset": 99}, {"referenceID": 37, "context": "Both these are significant improvements over recent works with similar goals: compositionality of MWEs (Salehi et al., 2015), works on sarcasm (Wallace et al.", "startOffset": 103, "endOffset": 124}, {"referenceID": 42, "context": ", 2015), works on sarcasm (Wallace et al., 2014) and metaphor detection (Tsvetkov et al.", "startOffset": 26, "endOffset": 48}, {"referenceID": 40, "context": ", 2014) and metaphor detection (Tsvetkov et al., 2014) (the latter works rely significantly on external linguistic resources and access to labeled training data).", "startOffset": 31, "endOffset": 54}, {"referenceID": 39, "context": "Our main contribution is the discovery of a geometric property of vector embeddings of context words (excluding the function words) within a sentence: they roughly occupy a low dimensional linear subspace which can be empirically extracted via a standard dimensionality reduction technique: principal component analysis (PCA) (Shlens, 2014).", "startOffset": 326, "endOffset": 340}, {"referenceID": 26, "context": "+ vn, adding all component word vectors together, as in (Mitchell and Lapata, 2010) and several works on phrase2vec (Gershman and Tenenbaum, 2015) and sentence2vec (Faruqui et al.", "startOffset": 56, "endOffset": 83}, {"referenceID": 12, "context": "+ vn, adding all component word vectors together, as in (Mitchell and Lapata, 2010) and several works on phrase2vec (Gershman and Tenenbaum, 2015) and sentence2vec (Faruqui et al.", "startOffset": 116, "endOffset": 146}, {"referenceID": 10, "context": "+ vn, adding all component word vectors together, as in (Mitchell and Lapata, 2010) and several works on phrase2vec (Gershman and Tenenbaum, 2015) and sentence2vec (Faruqui et al., 2015; Wieting et al., 2015; Adi et al., 2016).", "startOffset": 164, "endOffset": 226}, {"referenceID": 43, "context": "+ vn, adding all component word vectors together, as in (Mitchell and Lapata, 2010) and several works on phrase2vec (Gershman and Tenenbaum, 2015) and sentence2vec (Faruqui et al., 2015; Wieting et al., 2015; Adi et al., 2016).", "startOffset": 164, "endOffset": 226}, {"referenceID": 0, "context": "+ vn, adding all component word vectors together, as in (Mitchell and Lapata, 2010) and several works on phrase2vec (Gershman and Tenenbaum, 2015) and sentence2vec (Faruqui et al., 2015; Wieting et al., 2015; Adi et al., 2016).", "startOffset": 164, "endOffset": 226}, {"referenceID": 32, "context": "Based on the commonly-used distributional hypothesis that the word or phrase meaning can be inferred from its context (Rubenstein and Goodenough, 1965), we note that the local context (neighboring words) is crucial in deciphering the compositional sense of the word or phrase.", "startOffset": 118, "endOffset": 151}, {"referenceID": 30, "context": "This is in contrast to prior works that use the global context alone (the whole document or corpus), without accounting for the context-dependence of polysemy (Reddy et al., 2011).", "startOffset": 159, "endOffset": 179}, {"referenceID": 27, "context": "At times, the word(s) being tested exhibit polysemous behavior (example: check in blank check) (Mu et al., 2016).", "startOffset": 95, "endOffset": 112}, {"referenceID": 28, "context": "In such cases, it makes sense to consider multiple embeddings for different word senses (we use MSSG representations (Neelakantan et al., 2014)): each word has a single global embedding and two sense embeddings.", "startOffset": 117, "endOffset": 143}, {"referenceID": 1, "context": "The training corpus of embeddings in English, Chinese and German are obtained from polyglot (Al-Rfou et al., 2013).", "startOffset": 92, "endOffset": 114}, {"referenceID": 28, "context": ", 2014), and sense-specific ones using NP-MSSG of MSSG (Neelakantan et al., 2014).", "startOffset": 55, "endOffset": 81}, {"referenceID": 5, "context": "The English dataset contains 104 polysemous phrases which are obtained from the idiom dictionary (TheFreeDictionary, 2016), and the Chinese dataset consists of 64 phrases obtained from (ChineseDictionary, 2016).", "startOffset": 185, "endOffset": 210}, {"referenceID": 14, "context": "Their respective contexts are extracted from the corpus provided by polyglot or electronic resources (GoogleBooks, 2016).", "startOffset": 101, "endOffset": 120}, {"referenceID": 30, "context": "Dataset: The English noun compounds dataset (ENC), has 90 English noun compounds annotated on a continuous [0, 5] scale for the phrase and component-wise compositionality (Reddy et al., 2011); the English verb particle constructions (EVPC) contains 160 English verb-particle compounds, whose componentwise compositionality are annotated on a binary scale (Bannard, 2006).", "startOffset": 171, "endOffset": 191}, {"referenceID": 2, "context": ", 2011); the English verb particle constructions (EVPC) contains 160 English verb-particle compounds, whose componentwise compositionality are annotated on a binary scale (Bannard, 2006).", "startOffset": 171, "endOffset": 186}, {"referenceID": 35, "context": "5 to ENC as in (Salehi et al., 2014a), a threshold of 4 to GNC and use the binary labels of EVPC.", "startOffset": 15, "endOffset": 37}, {"referenceID": 22, "context": "PMI = log P (w1w2) P (w1)P (w2) , where P (\u00b7) is the probability of the unigram or bigram (Manning and Sch\u00fctze, 1999).", "startOffset": 90, "endOffset": 117}, {"referenceID": 9, "context": "While we use PCA, several recent works have shown average word vectors to be robust sentence embeddings (Ettinger et al., 2016; Adi et al., 2016; Wieting et al., 2015) and we measure compositionality by the cosine similarity between the target word vector and the sentence vector.", "startOffset": 104, "endOffset": 167}, {"referenceID": 0, "context": "While we use PCA, several recent works have shown average word vectors to be robust sentence embeddings (Ettinger et al., 2016; Adi et al., 2016; Wieting et al., 2015) and we measure compositionality by the cosine similarity between the target word vector and the sentence vector.", "startOffset": 104, "endOffset": 167}, {"referenceID": 43, "context": "While we use PCA, several recent works have shown average word vectors to be robust sentence embeddings (Ettinger et al., 2016; Adi et al., 2016; Wieting et al., 2015) and we measure compositionality by the cosine similarity between the target word vector and the sentence vector.", "startOffset": 104, "endOffset": 167}, {"referenceID": 35, "context": "We compare with the state-of-the-art of (Salehi et al., 2014a), specifically their methods based on word definitions, synonyms and idiom tags (denoted by ALLDEFS+SYN, ITAG+SYN, ALLDEFS) provided by wikitionary.", "startOffset": 40, "endOffset": 62}, {"referenceID": 35, "context": "The key advantage of our method is its non-reliance on external resources like wikitionary or multilingual translations \u2013 these are heavily relied upon in the state-of-the-art methods (Salehi et al., 2014a; Salehi et al., 2014b).", "startOffset": 184, "endOffset": 228}, {"referenceID": 36, "context": "The key advantage of our method is its non-reliance on external resources like wikitionary or multilingual translations \u2013 these are heavily relied upon in the state-of-the-art methods (Salehi et al., 2014a; Salehi et al., 2014b).", "startOffset": 184, "endOffset": 228}, {"referenceID": 37, "context": "Also, unlike the assumption in (Salehi et al., 2015), we do not require that the test phrases appear in the training corpus.", "startOffset": 31, "endOffset": 52}, {"referenceID": 8, "context": "Sarcasms, also called irony, are expressions whose actual meaning is quite different - and often opposite to - their literal meaning \u2013 and are instances of noncompositional usage (Davidov et al., 2010; Riloff et al., 2013).", "startOffset": 179, "endOffset": 222}, {"referenceID": 31, "context": "Sarcasms, also called irony, are expressions whose actual meaning is quite different - and often opposite to - their literal meaning \u2013 and are instances of noncompositional usage (Davidov et al., 2010; Riloff et al., 2013).", "startOffset": 179, "endOffset": 222}, {"referenceID": 13, "context": "These ideas are used in prior works to create elaborate features (designed based on a large labeled training set) and build a sarcasm detection system (Ghosh et al., 2015).", "startOffset": 151, "endOffset": 171}, {"referenceID": 13, "context": "We use a subset of the tweets in the dataset of (Ghosh et al., 2015) and study words that are used both literally and sarcastically (eg.", "startOffset": 48, "endOffset": 68}, {"referenceID": 42, "context": "A quantitative test is provided via our study on a Reddit irony dataset (Wallace et al., 2014).", "startOffset": 72, "endOffset": 94}, {"referenceID": 42, "context": "(Wallace et al., 2014) considers the 50,000 most frequently occurring unigrams and bigrams, and use binary bag-of-words and punctuations as features followed by a linear kernel SVM, grid-search for parameter tuning and five-fold cross validation.", "startOffset": 0, "endOffset": 22}, {"referenceID": 42, "context": "Instead, we generate compositionality-score features based on POS tags to allow a direct comparison with the state-of-the-art in (Wallace et al., 2014).", "startOffset": 129, "endOffset": 151}, {"referenceID": 42, "context": "These features are then fed into the same supervised learning system as in (Wallace et al., 2014), providing a fair comparison between the two featureselection methods.", "startOffset": 75, "endOffset": 97}, {"referenceID": 40, "context": "Dataset: English datasets comprising of metaphoric and literal uses of two syntactic structures (subject-verb-object (SVO) and adjective-noun (AN) compounds) are provided in (Tsvetkov et al., 2014).", "startOffset": 174, "endOffset": 197}, {"referenceID": 40, "context": "Algorithm Description: The state-of-the-art work (Tsvetkov et al., 2014) uses training-datadriven feature engineering methods while relying on external resources like WordNet and the MRC psycholinguistic database.", "startOffset": 49, "endOffset": 72}, {"referenceID": 40, "context": "These features are then fed into a supervised learning system (random forest), analogous to the one in (Tsvetkov et al., 2014) allowing for a fair comparison of the power of the features extracted.", "startOffset": 103, "endOffset": 126}, {"referenceID": 40, "context": "Detection Results: The experimental results on SVO and AN datasets are detailed in Table 6 where the baseline is provided by the results of (Tsvetkov et al., 2014) (which has access to the MRC psycholinguistic database and the supersense corpus).", "startOffset": 140, "endOffset": 163}, {"referenceID": 10, "context": "For instance, such a representation is successfully used for sentential sentiment prediction (Faruqui et al., 2015) and in (Kenter and de Rijke, 2015) to study text similarity.", "startOffset": 93, "endOffset": 115}, {"referenceID": 18, "context": "Average word embeddings are also used (Kenter et al., 2016) in conjunction with a neural network architecture to predict the surrounding sentences from the input sentence embeddings.", "startOffset": 38, "endOffset": 59}, {"referenceID": 44, "context": "Computational models of sentential semantics have also shown to be robustly handled by average word embeddings (Yu et al., 2014; Gershman and Tenenbaum, 2015; Adi et al., 2016; Wieting et al., 2015).", "startOffset": 111, "endOffset": 198}, {"referenceID": 12, "context": "Computational models of sentential semantics have also shown to be robustly handled by average word embeddings (Yu et al., 2014; Gershman and Tenenbaum, 2015; Adi et al., 2016; Wieting et al., 2015).", "startOffset": 111, "endOffset": 198}, {"referenceID": 0, "context": "Computational models of sentential semantics have also shown to be robustly handled by average word embeddings (Yu et al., 2014; Gershman and Tenenbaum, 2015; Adi et al., 2016; Wieting et al., 2015).", "startOffset": 111, "endOffset": 198}, {"referenceID": 43, "context": "Computational models of sentential semantics have also shown to be robustly handled by average word embeddings (Yu et al., 2014; Gershman and Tenenbaum, 2015; Adi et al., 2016; Wieting et al., 2015).", "startOffset": 111, "endOffset": 198}, {"referenceID": 21, "context": "Early approaches relied on the use of specific lexical and syntactic properties of MWEs (Lin, 1999; McCarthy et al., 2003; Cook et al., 2007; Fazly and Stevenson, 2007).", "startOffset": 88, "endOffset": 168}, {"referenceID": 25, "context": "Early approaches relied on the use of specific lexical and syntactic properties of MWEs (Lin, 1999; McCarthy et al., 2003; Cook et al., 2007; Fazly and Stevenson, 2007).", "startOffset": 88, "endOffset": 168}, {"referenceID": 7, "context": "Early approaches relied on the use of specific lexical and syntactic properties of MWEs (Lin, 1999; McCarthy et al., 2003; Cook et al., 2007; Fazly and Stevenson, 2007).", "startOffset": 88, "endOffset": 168}, {"referenceID": 11, "context": "Early approaches relied on the use of specific lexical and syntactic properties of MWEs (Lin, 1999; McCarthy et al., 2003; Cook et al., 2007; Fazly and Stevenson, 2007).", "startOffset": 88, "endOffset": 168}, {"referenceID": 34, "context": "More recent approaches include multilingual translations (Salehi and Cook, 2013; Salehi et al., 2014b) and using Wikitionary (one approach uses its word definitions, idiom tagging together with word synonyms to classify idiomatic phrases) (Salehi et al.", "startOffset": 57, "endOffset": 102}, {"referenceID": 36, "context": "More recent approaches include multilingual translations (Salehi and Cook, 2013; Salehi et al., 2014b) and using Wikitionary (one approach uses its word definitions, idiom tagging together with word synonyms to classify idiomatic phrases) (Salehi et al.", "startOffset": 57, "endOffset": 102}, {"referenceID": 35, "context": ", 2014b) and using Wikitionary (one approach uses its word definitions, idiom tagging together with word synonyms to classify idiomatic phrases) (Salehi et al., 2014a).", "startOffset": 145, "endOffset": 167}, {"referenceID": 16, "context": "In terms of distributed representation, methods include Latent Semantic Analysis (Katz and Giesbrecht, 2006) and word embeddings which have been extaordinarily successful representations of word semantics, eg.", "startOffset": 81, "endOffset": 108}, {"referenceID": 29, "context": ", word2vec and GloVe (Mikolov et al., 2014; Pennington et al., 2014; Neelakantan et al., 2014).", "startOffset": 21, "endOffset": 94}, {"referenceID": 28, "context": ", word2vec and GloVe (Mikolov et al., 2014; Pennington et al., 2014; Neelakantan et al., 2014).", "startOffset": 21, "endOffset": 94}, {"referenceID": 37, "context": "(Salehi et al., 2015) is a recent work exploring compositionality in conjunction with word embeddings; however, an aspect not considered is that compositionality does not only depend on the phrase but also on its context \u2013 this results in an inability to identify the context-based compositionality of polysemous phrases like bad egg.", "startOffset": 0, "endOffset": 21}, {"referenceID": 24, "context": "Such connections are explored in (Maynard and Greenwood, 2014) via a rule-based method of identifying known sarcastic phrases.", "startOffset": 33, "endOffset": 62}, {"referenceID": 8, "context": "Semi-supervised sarcasm identification algorithms are identified in (Davidov et al., 2010; Riloff et al., 2013; Liebrecht et al., 2013; Maynard and Greenwood, 2014), each using different sets of features (eg.", "startOffset": 68, "endOffset": 164}, {"referenceID": 31, "context": "Semi-supervised sarcasm identification algorithms are identified in (Davidov et al., 2010; Riloff et al., 2013; Liebrecht et al., 2013; Maynard and Greenwood, 2014), each using different sets of features (eg.", "startOffset": 68, "endOffset": 164}, {"referenceID": 20, "context": "Semi-supervised sarcasm identification algorithms are identified in (Davidov et al., 2010; Riloff et al., 2013; Liebrecht et al., 2013; Maynard and Greenwood, 2014), each using different sets of features (eg.", "startOffset": 68, "endOffset": 164}, {"referenceID": 24, "context": "Semi-supervised sarcasm identification algorithms are identified in (Davidov et al., 2010; Riloff et al., 2013; Liebrecht et al., 2013; Maynard and Greenwood, 2014), each using different sets of features (eg.", "startOffset": 68, "endOffset": 164}, {"referenceID": 19, "context": "Metaphor Detection Metaphors offer figurative interpretations and are a key feature of natural language (Lakoff and Johnson, 1980).", "startOffset": 104, "endOffset": 130}, {"referenceID": 23, "context": "(Mason, 2004) considers metaphor expression as a mapping from a source domain to a target domain, and develops a corpus-based system, CorMet, to discover such metaphorical equivalances based on WordNet.", "startOffset": 0, "endOffset": 13}, {"referenceID": 41, "context": "(Turney et al., 2011) hypothesises that metaphorical usage is related to the degreee of contextual abstractness, which they quantify relying on the MRC Psycholinguistic Database Machine Usable Dictionary (MRCPD) (Coltheart, 1981).", "startOffset": 0, "endOffset": 21}, {"referenceID": 6, "context": ", 2011) hypothesises that metaphorical usage is related to the degreee of contextual abstractness, which they quantify relying on the MRC Psycholinguistic Database Machine Usable Dictionary (MRCPD) (Coltheart, 1981).", "startOffset": 198, "endOffset": 215}, {"referenceID": 4, "context": "(Broadwell et al., 2013) proposes a detection method according to lexical imaginability, topic chaining and semantic clustering.", "startOffset": 0, "endOffset": 24}, {"referenceID": 40, "context": "(Tsvetkov et al., 2014) focuses on Subject-VerbObject and Adjective-Noun structures, and use word abstractness and imagineability as well as supersenses as features for metaphor detection.", "startOffset": 0, "endOffset": 23}, {"referenceID": 15, "context": ", LSTM (Greff et al., 2015)) are interesting future avenues of research.", "startOffset": 7, "endOffset": 27}], "year": 2016, "abstractText": "This paper proposes a simple test for compositionality (i.e., literal usage) of a word or phrase in a context-specific way. The test is computationally simple, relying on no external resources and only uses a set of trained word vectors. Experiments show that the proposed method is competitive with state of the art and displays high accuracy in context-specific compositionality detection of a variety of natural language phenomena (idiomaticity, sarcasm, metaphor) for different datasets in multiple languages. The key insight is to connect compositionality to a curious geometric property of word embeddings, which is of independent interest.", "creator": "LaTeX with hyperref package"}}}