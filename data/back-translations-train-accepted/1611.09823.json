{"id": "1611.09823", "review": {"conference": "iclr", "VERSION": "v1", "DATE_OF_SUBMISSION": "29-Nov-2016", "title": "Dialogue Learning With Human-In-The-Loop", "abstract": "An important aspect of developing conversational agents is to give a bot the ability to improve through communicating with humans and to learn from the mistakes that it makes. Most research has focused on learning from fixed training sets of labeled data rather than interacting with a dialogue partner in an online fashion. In this paper we explore this direction in a reinforcement learning setting where the bot improves its question-answering ability from feedback a teacher gives following its generated responses. We build a simulator that tests various aspects of such learning in a synthetic environment, and introduce models that work in this regime. Finally, real experiments with Mechanical Turk validate the approach.", "histories": [["v1", "Tue, 29 Nov 2016 20:16:44 GMT  (360kb,D)", "https://arxiv.org/abs/1611.09823v1", null], ["v2", "Fri, 16 Dec 2016 00:22:53 GMT  (347kb,D)", "http://arxiv.org/abs/1611.09823v2", null], ["v3", "Fri, 13 Jan 2017 21:12:38 GMT  (349kb,D)", "http://arxiv.org/abs/1611.09823v3", null]], "reviews": [], "SUBJECTS": "cs.AI cs.CL", "authors": ["jiwei li", "alexander h miller", "sumit chopra", "marc'aurelio ranzato", "jason weston"], "accepted": true, "id": "1611.09823"}, "pdf": {"name": "1611.09823.pdf", "metadata": {"source": "CRF", "title": "DIALOGUE LEARNING WITH HUMAN-IN-THE-LOOP", "authors": ["Jiwei Li", "Alexander H. Miller", "Sumit Chopra"], "emails": ["jiwel@fb.com", "ahm@fb.com", "spchopra@fb.com", "ranzato@fb.com", "jase@fb.com"], "sections": [{"heading": null, "text": "An important aspect in the development of conversation agents is to give a bot the ability to improve itself by communicating with people and learning from the mistakes it makes. Most research focuses on learning from set training sessions with marked data, rather than interacting online with a dialogue partner. In this post, we explore this direction in a learning environment where the bot improves its ability to answer questions by giving feedback from a teacher based on the answers it generates. We build a simulator that tests various aspects of this learning in a synthetic environment and present models that work in this regime. Finally, real experiments with Mechanical Turk confirm the approach."}, {"heading": "1 INTRODUCTION", "text": "A good interlocutor (who we sometimes refer to as a learner or bot1) should have the ability to learn from a teacher's online feedback: adapt his model when he makes mistakes, and reinforce the model when the teacher's feedback is positive. This is especially important in a situation where the bot is first trained to a fixed amount of synthetic, domain-specific, or pre-built data before publication, but is exposed to a different post-publication environment (e.g. a more diverse natural language use when talking to real people, different distributions, special cases, etc.) Most recent research has focused on training a bot from fixed training sessions with labeled data, but rarely on how the bot can improve through online interaction with people."}, {"heading": "2 RELATED WORK", "text": "This means that we focus not only on learning, but also on what we have learned. \"(Su et al., 2000; Walker et al., 2003; Si et al., 2009)\" what we have learned, \"\" what we have learned, \"\" what we have learned, \"\" what we have learned, \"\" what we have learned, \"\" what we have learned, \"\" what we have learned, \"\" what we have learned, \"\" what we have learned, \"\" what we have learned, \"\" what we have learned, \"\" what we have learned, \"\" what we have learned. \"(Si et al.)\" what we have learned, \"what we have learned,\" \"what we have learned,\" \"what we have learned.\""}, {"heading": "3 DATASET AND TASKS", "text": "We start by describing the data structure we use. In our first experiment we build a simulator as a test bed for learning algorithms. In our second experiment we use Mechanical Turk to give feedback to real human teachers."}, {"heading": "3.1 SIMULATOR", "text": "Following Weston (2016) we use (i) the only supporting fact problem from the bAbI datasets (Weston et al., 2015), which consists of 1000 short stories from a simulated world interspersed with questions; and (ii) the WikiMovies dataset (Weston et al., 2015), which consists of approximately 100k (submitted) questions about 75k units based on questions with answers in the open film database (OMDb). Each dialogue takes place between a teacher scripted by the simulation and a bot. The communication protocol is as follows: (1) the teacher first asks a question from the fixed set of questions that exist in the dataset, (2) the bot answers the question, and finally (3) the teacher gives feedback on the answers to the bot's answers."}, {"heading": "3.2 MECHANICAL TURK EXPERIMENTS", "text": "Finally, we expanded WikiMovies with Mechanical Turk, so that real human teachers give feedback instead of using simulation. Since both the questions and feedback are given in the simulation, they are now both replaced by natural human expressions. Instead of having a series of simulated tasks, we have only one task, and we gave the teachers instructions that they could give feedback as they see fit. The exact instructions given to the Turks can be found in Appendix B. Generally, each independent answer contains feedback such as (i) positive or negative sentences, or (ii) a sentence that contains the answer, or (iii) a clue that resembles the settings defined in the simulator. However, some human responses are not so easily categorized, and the lexical variability is much greater in human responses. Some examples of the collected data can be found in Figure 2."}, {"heading": "4 METHODS", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "4.1 MODEL ARCHITECTURE", "text": "In our experiments, we used variants of the end-to-end memory network (MemN2N) model (Sukhbaatar et al., 2015) as our underlying architecture for learning from the dialog. Input to MemN2N is the last utterance of the dialog history x, as well as a set of memories (context) C = c1, c2,..., cN. Memory C encodes both short-term memory, e.g. dialogue stories between the bot and the teacher, and long-term memories, e.g. the knowledge base facts to which the bot has access. Given the input x and C, the goal is to generate an output / caption. In the first step, the query x is converted into a vector representation u0 by summarizing its constituent word embedding: u0 = Ax. The input x is a bag of words vector and A is the d \u00b7 V word embedding matrix where d reads the dimensions."}, {"heading": "4.2 REINFORCEMENT LEARNING", "text": "In this section, we present the algorithms we have used to train MemN2N in an online way. Our learning setup can be considered a certain form of Reinforcement Learning. Politics is implemented through the MemN2N model. The state is the dialogue story. The action space corresponds to the answers that the MemN2N must choose to answer the teacher's question. In our setting, the policy chooses only one action for each episode. The reward is either 1 (a reward from the teacher if the bot answers correctly) or 0 otherwise. In our experiments, a reward equal to 0 could mean that the answer is wrong or that the positive reward is simply missing."}, {"heading": "4.2.1 REWARD-BASED IMITATION (RBI)", "text": "The simplest algorithm we look at first is the one used in Weston (2016). RBI relies on positive rewards provided by the teacher. He is trained to imitate the correct behavior of the learner, i.e. to learn to predict the right answers (with reward 1) during training and ignore the others. This is done by using a MemN2N that maps a dialogue input to a prediction, i.e. applying the cross entropy criterion to the positively rewarded subset of data. For this to work in an online environment that requires research to find the right answer, we apply a greedy strategy: the learner makes a prediction based on his own model (the most likely answer) with probability 1 - otherwise he selects a random answer with probability. The teacher then gives a reward of + 1 if the answer is correct, otherwise 0."}, {"heading": "4.2.2 REINFORCE", "text": "The second algorithm we use is the REINFORCE algorithm (Williams, 1992), which maximizes the expected cumulative reward of the episode, in our case the expected reward provided by the teacher. Expectation is approximated by scanning a response from the model distribution. Let a denote the response given by the learner, p (a) the probability that the current model assigns to one, r the teacher's reward, and J (\u03b8) the expectation of the reward. We have: B (a) [r \u2212 b] (3), where b is the underlying value estimated using a linear regression model that takes the performance of the memory network as input after the last jump and issues a scalar b that indicates the estimate of the future reward. The base model is trained by minimizing the average square loss between the estimated reward b and the actual reward (R \u2212 b)."}, {"heading": "4.2.3 FORWARD PREDICTION (FP)", "text": "FP (Weston, 2016) deals with the situation where there is no numerical reward for a bot's answer, meaning that there are no + 1 or 0 labels after a student's utterance. Suppose x is the teacher's question and C = c1, c2,..., cN denotes the dialog story as before. In FP, the model will redirect the initial question x and the dialog story C to a vector representation."}, {"heading": "5 EXPERIMENTS", "text": "Experiments are carried out first with our simulator and then with Amazon Mechanical Turk, with real human subjects assuming the role of instruction3."}, {"heading": "5.1 SIMULATOR", "text": "This year, it has reached the stage where it will be able to take the lead."}, {"heading": "5.2 HUMAN FEEDBACK", "text": "Our experimental protocol was as follows: We first trained a MemN2N using supervised (i.e., imitation) learning on a training set of 1000 questions produced by the Turks and used the known correct answers of the original data set (and no textual feedback); next, we collected textual feedback for the bot's answers for an additional 10,000 questions. Examples from the collected data set are given in Figure 2. In light of this data set, we compare different models: RBI, FP and RBI. Knowing the correct answers to the additional questions, we can assign a positive reward to questions that the bot got right. We therefore measure the impact of the thrift of this reward signal, with a fraction of the additional examples having rewards."}, {"heading": "6 CONCLUSION", "text": "We studied the dialog learning of end-to-end models using text feedback and numerical rewards. Full online settings as well as iterative batch settings are viable approaches to political learning, as long as possible instabilities in learning algorithms are taken into account. Second, we showed for the first time that the recently introduced FP method can work both in an online environment and on real human feedback. Overall, our results suggest that it is feasible to build a practical pipeline that starts with a model trained on an initial solid dataset and then learns from interactions with people to improve oneself (semi-) online."}, {"heading": "A FURTHER SIMULATOR TASK DETAILS", "text": "The tasks in Weston (2016) were specific: - Task 1: The teacher tells the student exactly what to say (supervised baseline). - Task 2: The teacher responds with positive text feedback and reward, or with negative text feedback. - Task 3: The teacher gives the answer if the bot is wrong. - Task 4: The teacher gives a clue by giving the class the correct answer, e.g.: \"No, it's a movie\" for the question, \"the film has Forest Gump in? - Task 5: The teacher provides a reason why the student's answer is wrong by pointing out the relevant supporting fact from the knowledge base. - The teacher only gives a positive reward\" reward. \""}, {"heading": "C ADDITIONAL EXPERIMENTS", "text": "C.1 ADDITIONAL EXPERIMENTS FOR MECHANICAL TURK SETUPIn the experiment in Section 5.2, we conducted experiments with real human feedback. We compare this to a form of synthetic feedback, mostly as a health check, but also to see how much improvement we can get if the signal is simpler and cleaner (as it is). We construct synthetic feedback for the 10,000 responses using either task 2 (positive or negative feedback) or a mix (task 2)."}], "references": [{"title": "Interactional feedback and the impact of attitude and motivation on noticing l2 form", "author": ["Mohammad Amin Bassiri"], "venue": "English Language and Literature Studies,", "citeRegEx": "Bassiri.,? \\Q2011\\E", "shortCiteRegEx": "Bassiri.", "year": 2011}, {"title": "Large-scale simple question answering with memory networks", "author": ["Antoine Bordes", "Nicolas Usunier", "Sumit Chopra", "Jason Weston"], "venue": "arXiv preprint arXiv:1506.02075,", "citeRegEx": "Bordes et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Bordes et al\\.", "year": 2015}, {"title": "Counterfactual reasoning and learning systems: The example of computational advertising", "author": ["Leon Bottou", "Jonas Peters", "Denis X. Quionero-Candela", "Joaquin amd Charles", "D. Max Chickering", "Elon Portugaly", "Dipankar Ray", "Patrice Simard", "Ed Snelson"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Bottou et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Bottou et al\\.", "year": 2013}, {"title": "Evaluating prerequisite qualities for learning end-to-end dialog systems", "author": ["Jesse Dodge", "Andreea Gane", "Xiang Zhang", "Antoine Bordes", "Sumit Chopra", "Alexander Miller", "Arthur Szlam", "Jason Weston"], "venue": "arXiv preprint arXiv:1511.06931,", "citeRegEx": "Dodge et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Dodge et al\\.", "year": 2015}, {"title": "Pomdp-based dialogue manager adaptation to extended domains", "author": ["Milica Ga\u0161ic", "Catherine Breslin", "Matthew Henderson", "Dongho Kim", "Martin Szummer", "Blaise Thomson", "Pirros Tsiakoulis", "Steve Young"], "venue": "In Proceedings of SIGDIAL,", "citeRegEx": "Ga\u0161ic et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Ga\u0161ic et al\\.", "year": 2013}, {"title": "Incremental on-line adaptation of pomdp-based dialogue managers to extended domains", "author": ["Milica Ga\u0161ic", "Dongho Kim", "Pirros Tsiakoulis", "Catherine Breslin", "Matthew Henderson", "Martin Szummer", "Blaise Thomson", "Steve Young"], "venue": "In Proceedings on InterSpeech,", "citeRegEx": "Ga\u0161ic et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Ga\u0161ic et al\\.", "year": 2014}, {"title": "Teaching machines to read and comprehend", "author": ["Karl Moritz Hermann", "Tomas Kocisky", "Edward Grefenstette", "Lasse Espeholt", "Will Kay", "Mustafa Suleyman", "Phil Blunsom"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Hermann et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Hermann et al\\.", "year": 2015}, {"title": "Learning dialogue strategies within the markov decision process framework", "author": ["Esther Levin", "Roberto Pieraccini", "Wieland Eckert"], "venue": "In Automatic Speech Recognition and Understanding,", "citeRegEx": "Levin et al\\.,? \\Q1997\\E", "shortCiteRegEx": "Levin et al\\.", "year": 1997}, {"title": "A stochastic model of human-machine interaction for learning dialog strategies", "author": ["Esther Levin", "Roberto Pieraccini", "Wieland Eckert"], "venue": "IEEE Transactions on speech and audio processing,", "citeRegEx": "Levin et al\\.,? \\Q2000\\E", "shortCiteRegEx": "Levin et al\\.", "year": 2000}, {"title": "Key-value memory networks for directly reading documents", "author": ["Alexander Miller", "Adam Fisch", "Jesse Dodge", "Amir-Hossein Karimi", "Antoine Bordes", "Jason Weston"], "venue": "arXiv preprint arXiv:1606.03126,", "citeRegEx": "Miller et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Miller et al\\.", "year": 2016}, {"title": "Playing atari with deep reinforcement learning", "author": ["Volodymyr Mnih", "Koray Kavukcuoglu", "David Silver", "Alex Graves", "Ioannis Antonoglou", "Daan Wierstra", "Martin Riedmiller"], "venue": "arXiv preprint arXiv:1312.5602,", "citeRegEx": "Mnih et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Mnih et al\\.", "year": 2013}, {"title": "Are we there yet? research in commercial spoken dialog systems", "author": ["Roberto Pieraccini", "David Suendermann", "Krishna Dayanidhi", "Jackson Liscombe"], "venue": "In International Conference on Text, Speech and Dialogue,", "citeRegEx": "Pieraccini et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Pieraccini et al\\.", "year": 2009}, {"title": "Squad: 100,000+ questions for machine comprehension of text", "author": ["Pranav Rajpurkar", "Jian Zhang", "Konstantin Lopyrev", "Percy Liang"], "venue": "arXiv preprint arXiv:1606.05250,", "citeRegEx": "Rajpurkar et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Rajpurkar et al\\.", "year": 2016}, {"title": "Sequence level training with recurrent neural networks", "author": ["Marc\u2019Aurelio Ranzato", "Sumit Chopra", "Michael Auli", "Wojciech Zaremba"], "venue": "arXiv preprint arXiv:1511.06732,", "citeRegEx": "Ranzato et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Ranzato et al\\.", "year": 2015}, {"title": "A survey of statistical user simulation techniques for reinforcement-learning of dialogue management strategies", "author": ["Jost Schatzmann", "Karl Weilhammer", "Matt Stuttle", "Steve Young"], "venue": "The knowledge engineering review,", "citeRegEx": "Schatzmann et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Schatzmann et al\\.", "year": 2006}, {"title": "Empirical evaluation of a reinforcement learning spoken dialogue system", "author": ["Satinder Singh", "Michael Kearns", "Diane J Litman", "Marilyn A Walker"], "venue": "In AAAI/IAAI,", "citeRegEx": "Singh et al\\.,? \\Q2000\\E", "shortCiteRegEx": "Singh et al\\.", "year": 2000}, {"title": "Optimizing dialogue management with reinforcement learning: Experiments with the njfun system", "author": ["Satinder Singh", "Diane Litman", "Michael Kearns", "Marilyn Walker"], "venue": "Journal of Artificial Intelligence Research,", "citeRegEx": "Singh et al\\.,? \\Q2002\\E", "shortCiteRegEx": "Singh et al\\.", "year": 2002}, {"title": "Continuously learning neural dialogue management", "author": ["Pei-Hao Su", "Milica Gasic", "Nikola Mrksic", "Lina Rojas-Barahona", "Stefan Ultes", "David Vandyke", "Tsung-Hsien Wen", "Steve Young"], "venue": "arXiv preprint arXiv:1606.02689,", "citeRegEx": "Su et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Su et al\\.", "year": 2016}, {"title": "End-to-end memory networks", "author": ["Sainbayar Sukhbaatar", "Jason Weston", "Rob Fergus"], "venue": "In Advances in neural information processing systems,", "citeRegEx": "Sukhbaatar et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Sukhbaatar et al\\.", "year": 2015}, {"title": "An application of reinforcement learning to dialogue strategy selection in a spoken dialogue system for email", "author": ["Marilyn A. Walker"], "venue": "Journal of Artificial Intelligence Research,", "citeRegEx": "Walker.,? \\Q2000\\E", "shortCiteRegEx": "Walker.", "year": 2000}, {"title": "A trainable generator for recommendations in multimodal dialog", "author": ["Marilyn A Walker", "Rashmi Prasad", "Amanda Stent"], "venue": "In INTERSPEECH,", "citeRegEx": "Walker et al\\.,? \\Q2003\\E", "shortCiteRegEx": "Walker et al\\.", "year": 2003}, {"title": "Instructive feedback: Review of parameters and effects", "author": ["Margaret G Werts", "Mark Wolery", "Ariane Holcombe", "David L Gast"], "venue": "Journal of Behavioral Education,", "citeRegEx": "Werts et al\\.,? \\Q1995\\E", "shortCiteRegEx": "Werts et al\\.", "year": 1995}, {"title": "Dialog-based language learning", "author": ["Jason Weston"], "venue": "arXiv preprint arXiv:1604.06045,", "citeRegEx": "Weston.,? \\Q2016\\E", "shortCiteRegEx": "Weston.", "year": 2016}, {"title": "Towards ai-complete question answering: A set of prerequisite toy tasks", "author": ["Jason Weston", "Antoine Bordes", "Sumit Chopra", "Alexander M Rush", "Bart van Merri\u00ebnboer", "Armand Joulin", "Tomas Mikolov"], "venue": "arXiv preprint arXiv:1502.05698,", "citeRegEx": "Weston et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Weston et al\\.", "year": 2015}, {"title": "Simple statistical gradient-following algorithms for connectionist reinforcement learning", "author": ["Ronald J Williams"], "venue": "Machine learning,", "citeRegEx": "Williams.,? \\Q1992\\E", "shortCiteRegEx": "Williams.", "year": 1992}, {"title": "The hidden information state model: A practical framework for pomdp-based spoken dialogue management", "author": ["Steve Young", "Milica Ga\u0161i\u0107", "Simon Keizer", "Fran\u00e7ois Mairesse", "Jost Schatzmann", "Blaise Thomson", "Kai Yu"], "venue": "Computer Speech & Language,", "citeRegEx": "Young et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Young et al\\.", "year": 2010}, {"title": "Under review as a conference paper at ICLR", "author": ["Steve Young", "Milica Ga\u0161i\u0107", "Blaise Thomson", "Jason D Williams"], "venue": null, "citeRegEx": "Young et al\\.,? \\Q2017\\E", "shortCiteRegEx": "Young et al\\.", "year": 2017}], "referenceMentions": [{"referenceID": 0, "context": "Human (rather than machine) language learning happens during communication (Bassiri, 2011; Werts et al., 1995), and not from labeled datasets, hence making this an important subject to study.", "startOffset": 75, "endOffset": 110}, {"referenceID": 21, "context": "Human (rather than machine) language learning happens during communication (Bassiri, 2011; Werts et al., 1995), and not from labeled datasets, hence making this an important subject to study.", "startOffset": 75, "endOffset": 110}, {"referenceID": 22, "context": "We consider two types of feedback: explicit numerical rewards as in conventional reinforcement learning, and textual feedback which is more natural in human dialogue, following (Weston, 2016).", "startOffset": 177, "endOffset": 191}, {"referenceID": 19, "context": "Reinforcement learning has been widely applied to dialogue, especially in slot filling to solve domain-specific tasks (Walker, 2000; Schatzmann et al., 2006; Singh et al., 2000; 2002).", "startOffset": 118, "endOffset": 183}, {"referenceID": 14, "context": "Reinforcement learning has been widely applied to dialogue, especially in slot filling to solve domain-specific tasks (Walker, 2000; Schatzmann et al., 2006; Singh et al., 2000; 2002).", "startOffset": 118, "endOffset": 183}, {"referenceID": 15, "context": "Reinforcement learning has been widely applied to dialogue, especially in slot filling to solve domain-specific tasks (Walker, 2000; Schatzmann et al., 2006; Singh et al., 2000; 2002).", "startOffset": 118, "endOffset": 183}, {"referenceID": 7, "context": "Efforts include Markov Decision Processes (MDPs) (Levin et al., 1997; 2000; Walker et al., 2003; Pieraccini et al., 2009), POMDP models (Young et al.", "startOffset": 49, "endOffset": 121}, {"referenceID": 20, "context": "Efforts include Markov Decision Processes (MDPs) (Levin et al., 1997; 2000; Walker et al., 2003; Pieraccini et al., 2009), POMDP models (Young et al.", "startOffset": 49, "endOffset": 121}, {"referenceID": 11, "context": "Efforts include Markov Decision Processes (MDPs) (Levin et al., 1997; 2000; Walker et al., 2003; Pieraccini et al., 2009), POMDP models (Young et al.", "startOffset": 49, "endOffset": 121}, {"referenceID": 25, "context": ", 2009), POMDP models (Young et al., 2010; 2013; Ga\u0161ic et al., 2013; 2014) and policy learning (Su et al.", "startOffset": 22, "endOffset": 74}, {"referenceID": 4, "context": ", 2009), POMDP models (Young et al., 2010; 2013; Ga\u0161ic et al., 2013; 2014) and policy learning (Su et al.", "startOffset": 22, "endOffset": 74}, {"referenceID": 17, "context": ", 2013; 2014) and policy learning (Su et al., 2016).", "startOffset": 34, "endOffset": 51}, {"referenceID": 3, "context": "Our work is related to the line of research that focuses on supervised learning for question answering (QA) from dialogues (Dodge et al., 2015; Weston, 2016), either given a database of knowledge (Bordes et al.", "startOffset": 123, "endOffset": 157}, {"referenceID": 22, "context": "Our work is related to the line of research that focuses on supervised learning for question answering (QA) from dialogues (Dodge et al., 2015; Weston, 2016), either given a database of knowledge (Bordes et al.", "startOffset": 123, "endOffset": 157}, {"referenceID": 1, "context": ", 2015; Weston, 2016), either given a database of knowledge (Bordes et al., 2015; Miller et al., 2016) or short texts (Weston et al.", "startOffset": 60, "endOffset": 102}, {"referenceID": 9, "context": ", 2015; Weston, 2016), either given a database of knowledge (Bordes et al., 2015; Miller et al., 2016) or short texts (Weston et al.", "startOffset": 60, "endOffset": 102}, {"referenceID": 23, "context": ", 2016) or short texts (Weston et al., 2015; Hermann et al., 2015; Rajpurkar et al., 2016).", "startOffset": 23, "endOffset": 90}, {"referenceID": 6, "context": ", 2016) or short texts (Weston et al., 2015; Hermann et al., 2015; Rajpurkar et al., 2016).", "startOffset": 23, "endOffset": 90}, {"referenceID": 12, "context": ", 2016) or short texts (Weston et al., 2015; Hermann et al., 2015; Rajpurkar et al., 2016).", "startOffset": 23, "endOffset": 90}, {"referenceID": 22, "context": "The experiments in (Weston, 2016) involve constructing pre-built fixed datasets, rather than training the learner within a simulator, as in our work.", "startOffset": 19, "endOffset": 33}, {"referenceID": 22, "context": "In our work, when policy training is viewed as batch learning over iterations of the dataset, updating the policy on each iteration, (Weston, 2016) can be viewed as training only one iteration, whereas we perform multiple iterations.", "startOffset": 133, "endOffset": 147}, {"referenceID": 22, "context": "Finally, (Weston, 2016) only conducted experiments on synthetic or templated language, and not real language, especially the feedback from the teacher was scripted.", "startOffset": 9, "endOffset": 23}, {"referenceID": 1, "context": ", 2015; Weston, 2016), either given a database of knowledge (Bordes et al., 2015; Miller et al., 2016) or short texts (Weston et al., 2015; Hermann et al., 2015; Rajpurkar et al., 2016). In our work, the discourse includes the statements made in the past, the question and answer, and crucially the response from the teacher. The latter is what makes the setting different from the standard QA setting, i.e. we use methods that leverage this response also, not just answering questions. Further, QA works only consider fixed datasets with gold annotations, i.e. they do not consider a reinforcement learning setting. Our work is closely related to a recent work from Weston (2016) that learns through conducting conversations where supervision is given naturally in the response during the conversation.", "startOffset": 61, "endOffset": 681}, {"referenceID": 23, "context": "Following Weston (2016), we use (i) the single supporting fact problem from the bAbI datasets (Weston et al., 2015) which consists of 1000 short stories from a simulated world interspersed with questions; and (ii) the WikiMovies dataset (Weston et al.", "startOffset": 94, "endOffset": 115}, {"referenceID": 23, "context": ", 2015) which consists of 1000 short stories from a simulated world interspersed with questions; and (ii) the WikiMovies dataset (Weston et al., 2015) which consists of roughly 100k (templated) questions over 75k entities based on questions with answers in the open movie database (OMDb).", "startOffset": 129, "endOffset": 150}, {"referenceID": 22, "context": "We follow the paradigm defined in (Weston, 2016) where the teacher\u2019s feedback takes the form of either textual feedback, a numerical reward, or both, depending on the task.", "startOffset": 34, "endOffset": 48}, {"referenceID": 22, "context": "We also refer the readers to (Weston, 2016) for more detailed descriptions and the motivation behind these tasks.", "startOffset": 29, "endOffset": 43}, {"referenceID": 22, "context": "Following Weston (2016), we use (i) the single supporting fact problem from the bAbI datasets (Weston et al.", "startOffset": 10, "endOffset": 24}, {"referenceID": 22, "context": "Following Weston (2016), we use (i) the single supporting fact problem from the bAbI datasets (Weston et al., 2015) which consists of 1000 short stories from a simulated world interspersed with questions; and (ii) the WikiMovies dataset (Weston et al., 2015) which consists of roughly 100k (templated) questions over 75k entities based on questions with answers in the open movie database (OMDb). Each dialogue takes place between a teacher, scripted by the simulation, and a bot. The communication protocol is as follows: (1) the teacher first asks a question from the fixed set of questions existing in the dataset, (2) the bot answers the question, and finally (3) the teacher gives feedback on the bot\u2019s answer. We follow the paradigm defined in (Weston, 2016) where the teacher\u2019s feedback takes the form of either textual feedback, a numerical reward, or both, depending on the task. For each dataset, there are ten tasks, which are further described in Sec. A and illustrated in Figure 5 of the appendix. We also refer the readers to (Weston, 2016) for more detailed descriptions and the motivation behind these tasks. In the main text of this paper we only consider Task 6 (\u201cpartial feedback\u201d): the teacher replies with positive textual feedback (6 possible templates) when the bot answers correctly, and positive reward is given only 50% of the time. When the bot is wrong, the teacher gives textual feedback containing the answer. Descriptions and experiments on the other tasks are detailed in the appendix. Example dialogues are given in Figure 1. The difference between our simulation and the original fixed tasks of Weston (2016) is that models are trained on-the-fly.", "startOffset": 10, "endOffset": 1643}, {"referenceID": 22, "context": "We consider 10 different tasks following Weston (2016) but here describe only Task 6; other tasks are detailed in the appendix.", "startOffset": 41, "endOffset": 55}, {"referenceID": 18, "context": "1 MODEL ARCHITECTURE In our experiments, we used variants of the End-to-End Memory Network (MemN2N) model (Sukhbaatar et al., 2015) as our underlying architecture for learning from dialogue.", "startOffset": 106, "endOffset": 131}, {"referenceID": 2, "context": "Our experiments show that our model and base algorithms are very robust to the choice of batch size, alleviating the need for correction terms in the learning algorithm (Bottou et al., 2013).", "startOffset": 169, "endOffset": 190}, {"referenceID": 22, "context": "The simplest algorithm we first consider is the one employed in Weston (2016). RBI relies on positive rewards provided by the teacher.", "startOffset": 64, "endOffset": 78}, {"referenceID": 24, "context": "The second algorithm we use is the REINFORCE algorithm (Williams, 1992), which maximizes the expected cumulative reward of the episode, in our case the expected reward provided by the teacher.", "startOffset": 55, "endOffset": 71}, {"referenceID": 13, "context": "We refer the readers to (Ranzato et al., 2015; Zaremba & Sutskever, 2015) for more details.", "startOffset": 24, "endOffset": 73}, {"referenceID": 22, "context": "3 FORWARD PREDICTION (FP) FP (Weston, 2016) handles the situation where a numerical reward for a bot\u2019s answer is not available, meaning that there are no +1 or 0 labels available after a student\u2019s utterance.", "startOffset": 29, "endOffset": 43}, {"referenceID": 10, "context": "2 This is a type of experience replay (Mnih et al., 2013) but sampling with an evened distribution.", "startOffset": 38, "endOffset": 57}, {"referenceID": 21, "context": "It was shown in Weston (2016) that in an off-line setting this procedure can work either on its own, or in conjunction with a method that uses numerical rewards as well for improved performance.", "startOffset": 16, "endOffset": 30}, {"referenceID": 22, "context": "Note that supervised, rather than reinforcement learning, with gold standard labels achieves 80% accuracy on this task (Weston, 2016).", "startOffset": 119, "endOffset": 133}, {"referenceID": 23, "context": "Relation to experiments in Weston (2016) As described in detail in Section 2 the datasets we use in our experiments were introduced in (Weston et al., 2015).", "startOffset": 135, "endOffset": 156}, {"referenceID": 23, "context": "This is a key differentiator to the work of (Weston et al., 2015) where such improvement was not shown.", "startOffset": 44, "endOffset": 65}, {"referenceID": 22, "context": "Relation to experiments in Weston (2016) As described in detail in Section 2 the datasets we use in our experiments were introduced in (Weston et al.", "startOffset": 27, "endOffset": 41}, {"referenceID": 22, "context": "Relation to experiments in Weston (2016) As described in detail in Section 2 the datasets we use in our experiments were introduced in (Weston et al., 2015). However, that work involved constructing pre-built fixed policies (and hence, datasets), rather than training the learner in a reinforcement/interactive learning using a simulator, as in our work. They achieved this by choosing an omniscient (but deliberately imperfect) labeler that gets \u03c0acc examples always correct (the paper looked at values 1%, 10% and 50%). In a realistic setting one does not have access to an omniscient labeler, one has to learn a policy completely from scratch, online, starting with a random policy, as we do here. Nevertheless, it is possible to compare our learnt policies to those results because we use the same train/valid/test splits. The clearest comparison comparison is via Table 1, where the policy is learnt using batch iterations of the dataset, updating the policy on each iteration. Weston et al. (2015) can be viewed as training only one iteration, with a pre-built policy, as explained above, where 59%, 81% and 99% accuracy was obtained for RBI for \u03c0acc with 1%, 10% and 50% respectively4.", "startOffset": 27, "endOffset": 1004}, {"referenceID": 22, "context": "Relation to experiments in Weston (2016) As described in detail in Section 2 the datasets we use in our experiments were introduced in (Weston et al., 2015). However, that work involved constructing pre-built fixed policies (and hence, datasets), rather than training the learner in a reinforcement/interactive learning using a simulator, as in our work. They achieved this by choosing an omniscient (but deliberately imperfect) labeler that gets \u03c0acc examples always correct (the paper looked at values 1%, 10% and 50%). In a realistic setting one does not have access to an omniscient labeler, one has to learn a policy completely from scratch, online, starting with a random policy, as we do here. Nevertheless, it is possible to compare our learnt policies to those results because we use the same train/valid/test splits. The clearest comparison comparison is via Table 1, where the policy is learnt using batch iterations of the dataset, updating the policy on each iteration. Weston et al. (2015) can be viewed as training only one iteration, with a pre-built policy, as explained above, where 59%, 81% and 99% accuracy was obtained for RBI for \u03c0acc with 1%, 10% and 50% respectively4. While \u03c0acc of 50% is good enough to solve the task, lower values are not. In this work a random policy begins with 74% accuracy on the first iteration, but importantly on each iteration the policy is updated and improves, with values of 87%, 90% on iterations 2 and 3 respectively, and 98% on iteration 6. This is a key differentiator to the work of (Weston et al., 2015) where such improvement was not shown. We show that such online learning works for both reward-based numerical feedback and for forward prediction methods using textual feedback (as long as balancing or random exploration is performed sufficiently). The final performance outperforms most values of \u03c0acc from Weston et al. (2015) unless \u03c0 is so large that the task is already solved.", "startOffset": 27, "endOffset": 1894}, {"referenceID": 22, "context": "Relation to experiments in Weston (2016) As described in detail in Section 2 the datasets we use in our experiments were introduced in (Weston et al., 2015). However, that work involved constructing pre-built fixed policies (and hence, datasets), rather than training the learner in a reinforcement/interactive learning using a simulator, as in our work. They achieved this by choosing an omniscient (but deliberately imperfect) labeler that gets \u03c0acc examples always correct (the paper looked at values 1%, 10% and 50%). In a realistic setting one does not have access to an omniscient labeler, one has to learn a policy completely from scratch, online, starting with a random policy, as we do here. Nevertheless, it is possible to compare our learnt policies to those results because we use the same train/valid/test splits. The clearest comparison comparison is via Table 1, where the policy is learnt using batch iterations of the dataset, updating the policy on each iteration. Weston et al. (2015) can be viewed as training only one iteration, with a pre-built policy, as explained above, where 59%, 81% and 99% accuracy was obtained for RBI for \u03c0acc with 1%, 10% and 50% respectively4. While \u03c0acc of 50% is good enough to solve the task, lower values are not. In this work a random policy begins with 74% accuracy on the first iteration, but importantly on each iteration the policy is updated and improves, with values of 87%, 90% on iterations 2 and 3 respectively, and 98% on iteration 6. This is a key differentiator to the work of (Weston et al., 2015) where such improvement was not shown. We show that such online learning works for both reward-based numerical feedback and for forward prediction methods using textual feedback (as long as balancing or random exploration is performed sufficiently). The final performance outperforms most values of \u03c0acc from Weston et al. (2015) unless \u03c0 is so large that the task is already solved. This is a key contribution of our work. Similar conclusions can be made for Figures 3 and 4. Despite our initial random policy starting at close to 0% accuracy, if random exploration \u2265 0.2 is employed then after a number of epochs the performance is better than most values of \u03c0acc from Weston et al. (2015), e.", "startOffset": 27, "endOffset": 2256}], "year": 2017, "abstractText": "An important aspect of developing conversational agents is to give a bot the ability to improve through communicating with humans and to learn from the mistakes that it makes. Most research has focused on learning from fixed training sets of labeled data rather than interacting with a dialogue partner in an online fashion. In this paper we explore this direction in a reinforcement learning setting where the bot improves its question-answering ability from feedback a teacher gives following its generated responses. We build a simulator that tests various aspects of such learning in a synthetic environment, and introduce models that work in this regime. Finally, real experiments with Mechanical Turk validate the approach.", "creator": "LaTeX with hyperref package"}}}