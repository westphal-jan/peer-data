{"id": "1611.10283", "review": {"conference": "AAAI", "VERSION": "v1", "DATE_OF_SUBMISSION": "30-Nov-2016", "title": "Weighted Bandits or: How Bandits Learn Distorted Values That Are Not Expected", "abstract": "Motivated by models of human decision making proposed to explain commonly observed deviations from conventional expected value preferences, we formulate two stochastic multi-armed bandit problems with distorted probabilities on the cost distributions: the classic $K$-armed bandit and the linearly parameterized bandit. In both settings, we propose algorithms that are inspired by Upper Confidence Bound (UCB), incorporate cost distortions, and exhibit sublinear regret assuming \\holder continuous weight distortion functions. For the $K$-armed setting, we show that the algorithm, called W-UCB, achieves problem-dependent regret $O(L^2 M^2 \\log n/ \\Delta^{\\frac{2}{\\alpha}-1})$, where $n$ is the number of plays, $\\Delta$ is the gap in distorted expected value between the best and next best arm, $L$ and $\\alpha$ are the H\\\"{o}lder constants for the distortion function, and $M$ is an upper bound on costs, and a problem-independent regret bound of $O((KL^2M^2)^{\\alpha/2}n^{(2-\\alpha)/2})$. We also present a matching lower bound on the regret, showing that the regret of W-UCB is essentially unimprovable over the class of H\\\"{o}lder-continuous weight distortions. For the linearly parameterized setting, we develop a new algorithm, a variant of the Optimism in the Face of Uncertainty Linear bandit (OFUL) algorithm called WOFUL (Weight-distorted OFUL), and show that it has regret $O(d\\sqrt{n} \\; \\mbox{polylog}(n))$ with high probability, for sub-Gaussian cost distributions. Finally, numerical examples demonstrate the advantages resulting from using distortion-aware learning algorithms.", "histories": [["v1", "Wed, 30 Nov 2016 17:37:51 GMT  (101kb,D)", "http://arxiv.org/abs/1611.10283v1", "Longer version of the paper to be published as part of the proceedings of AAAI 2017"]], "COMMENTS": "Longer version of the paper to be published as part of the proceedings of AAAI 2017", "reviews": [], "SUBJECTS": "cs.LG stat.ML", "authors": ["aditya gopalan", "prashanth l a", "michael c fu", "steven i marcus"], "accepted": true, "id": "1611.10283"}, "pdf": {"name": "1611.10283.pdf", "metadata": {"source": "CRF", "title": "Weighted bandits or: How bandits learn distorted values that are not expected", "authors": ["Aditya Gopalan", "Prashanth L A", "Michael Fu", "Steve Marcus"], "emails": ["aditya@ece.iisc.ernet.in", "prashla@umd.edu", "mfu@isr.umd.edu", "marcus@umd.edu"], "sections": [{"heading": null, "text": "2 \u03b1 \u2212 1), where n is the number of games, \u2206 is the gap in the distorted expectation value between the best and the next best arm, L and \u03b1 is the lower constant for the distortion function, and M is an upper limit for costs and a problem-free limit for O ((((KL2M2) \u03b1 / 2n (2 \u2212 \u03b1) / 2). We also represent a corresponding lower limit for regret and show that the regret of the W-UCB algorithm Abbasi-Yadkori et al. [2011] is substantially less likely than the class of permanent weight distortions. For the linear parameterized setting, we are developing a new algorithm, a variant of optimism in the Face of Uncertainty Linear Bandit (OFUL) algorithm Abbasi-Yadkori et al. [2011] called WOFUL (Weight-distorted OFUL), and show that regret has to result in a high probability of demonstrating the result of polylog (O) n)."}, {"heading": "1 Introduction", "text": "Consider the following two-armed bandit problem: The rewards of Arm 1 are $1 million W.P. 1 / 106 and 0 otherwise, while Arm 2 rewards $1000 W.P. 1 / 103 and 0 otherwise. In this case, man would normally prefer a different path than Arm 2. The above example illustrates that traditional expectation values are short in explaining human preferences, and the reader is referred to the classic Allais problem, which rigorously argues against the expected utility theory. Violations of the expected value-based preferences in human decision-making can be caused by the inclusion of distortions in the underlying probabilities of the Starmer system [2000, Chapter 4]."}, {"heading": "2.1 W-UCB Algorithm", "text": "Estimating the weight-distorted costs for each arm k is a challenge, and one cannot use a Monte Carlo approach with sampling, because the costs include a distorted distribution, whereas the samples come from the undistorted distribution Fk. Therefore, one has to estimate the total distribution, and to this end, we adapt the quantitative approach of [Prashanth et al., 2016]. Estimate the number of samples Tk (m \u2212 1) for the undistorted distribution. Let us order the samples from the cost distribution Fk for arm k, where we used l to name the number of samples Tk (m \u2212 1) for notorious convenience."}, {"heading": "2.2 Main results", "text": "Under (A1) - (A2), the expected cumulative regret Rn of W-UCB is limited as follows: ERn \u2264 \u2211 {k: \u2206 k > 0} 3 (2LM) 2 / \u03b1 log n2 \u0445 2 / \u03b1 \u2212 1 k + MK (1 + 2\u03c023).Evidence. See Section 4.1.2.The above theorem contains the gapless limit. Next, we present a gapless regret bound limit in the following result: Episode 1 (Gap-independent regret). Under (A1) - (A2), the expected cumulative regret Rn of W-UCB shows the following gapless limit. There is a universal constant c > 0, so that for all n, ERn \u2264 MK\u03b1 / 2 (3 2 (2L) 2 / \u03b1 log n).Regret Rn of W-UCB there is a universal constant."}, {"heading": "3 Linearly parameterized bandit with weight distortion", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "3.1 Arm-dependent noise setting", "text": "The learning game runs as follows: In each round m = 1, 2,... the learner (a) plays an arm xm x, possibly depending on the observation history so far, and (b) observes a stochastic, non-negative cost in cm: = x T m (\u03b8 + Nm), (7), where Nm: = (N1m,.., N d m) is a vector of the standard Gaussian random variables, independent of the previous vectors N1,.., Nm \u2212 1, and the underlying model parameter Rd. Both systems and Nm, m \u00b2 1, are unknown to the learner. Algorithm 1 WOFUL Input: Regulating constant x,."}, {"heading": "3.2 The WOFUL algorithm", "text": "Algorithm 1 represents the pseudo code for the proposed algorithm, which follows the general template for linear bandit algorithms (cf. ConfidenceBall in [Dani et al., 2008] or OFUL in [Abbasi-Yadkori et al., 2011]), but differs in the step when an arm is selected. In particular, in each round of the algorithm, WOFUL uses \u00b5x (\u03b8) as the decision criterion for each arm x-X and \u03b8-Cm, which is the weight distorted value specified in (8) and Cm is the confidence ellisoid in algorithm 1. This differs from regular linear bandit algorithms that use xTTB as the cost for each arm x-X, and vice versa."}, {"heading": "3.3 Main results", "text": "So it is not like it is for everyone x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x"}, {"heading": "4 Convergence proofs", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "4.1 Proofs for K-armed bandit", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "4.1.1 Proof of Theorem 1", "text": "The proof is very similar to that of sentence 2 in [Prashanth et al., 2016]. The proof is based on the Dvoretzky-Kiefer-Wolfowitz inequality (DKW), which indicates the exponential concentration of the empirical distribution F-k, m in finite samples around the true distribution Fk, measured by the norm of the function Fk [Wasserman, 2015, chapter 2]."}, {"heading": "4.1.2 Proof of Theorem 2", "text": "Evidence. As in the case of the regular UCB, we tie the number of times a sub-optimal arm is pulled using the technique of Munos (2014). Suppose a sub-optimal arm is currently pulled m, which implies that the B value of the arm k can be smaller than the value of the upper arm if one of the following three conditions applies: p < p < p < p < p. < p. (m \u2212 1) \u2212 p. (m \u2212 1), p. (m \u2212 1), p. (m \u2212 1). The B value of the arm k may only be smaller than that of the upper arm if one of the following three conditions applies: p. < p. < p. & lt. < p. < p. < p. (m \u2212 p). < p. (m \u2212 p)."}, {"heading": "4.1.3 Proof of Corollary 1", "text": "Proof. We have established by analysis in proof of theorem 2 that for all k with \u2206 k > 0, E [Tk (n)] \u2264 3 (2LM) 2 / \u03b1 log n2 \u2206 2 / \u03b1 k + (1 + 2\u03c023). Thus we can ERn = \u2211 k \u2206 k E [Tk (n)] = \u2211 k (\u2206 k E [Tk (n)] \u03b1 2) (E [Tk (n)] 1 \u2212 \u03b1 2) \u2264 (\u2211 k \u0445 2 / \u03b1 k E [Tk (n)]) \u03b1 2 (\u2211 k E [Tk (n)]) 1 \u2212 \u03b12 (Heller's inequality with exponents 2 / \u03b1 and 2 / (2 \u2212 \u03b1)) \u2264 (3K (2LM) 2 / \u03b1 log n2 + KM2 / \u03b1 (1 + 2\u03b223)) \u03b1 2n 2 \u2212 \u03b1 2, (using exponents 2 / \u03b1 \u00b2 M 2 / \u03b1). This is proved by the result."}, {"heading": "4.1.4 Formal Regret Lower Bound", "text": "We will need the following definition: For two probability distributions p and q on R, define the relative entropy or Kullback-Leibler costs (KL)."}, {"heading": "4.1.5 Cumulative prospect theory (CPT) for K-armed bandit", "text": "As mentioned in the introduction, a popular approach that uses a weight function to distort probabilities is the so-called cumulative prospectus theory (CPT) by Tversky and Kahneman [1992]. In the following, we expand the W-UCB algorithm to include CPT-like utility functions. We define the CPT value for each arm k as follows: \u00b5k: = probability of event A for arm k, u +, u \u2212 R \u2212 R + are the utility functions and w +, w \u2212 \u221e 0 w \u2212 (Pk (X) > z)))) dz, (19) where Pk (\u00b7) is the probability of event A for arm k, u \u2212 R \u2212 R + are the utility functions and w +, w \u2212 \u00b2 [0, 1] \u2192 [0, 1] are weight functions (0, 1]. The optimal arm is one that maximizes the CPT value, i.e., use functions are R + w \u2212 and R \u2212 \u2212 functions."}, {"heading": "4.2 Proofs for linear bandit", "text": "We note some technical results in the following lemmas, which are necessary for the proof of theorem 4. The first step in the proof of theorem 4 is to tie the momentary regret rm to moment m, which is the difference in the weight-distorted value of arm x * and the arm chosen by the algorithm xm. To limit the momentary regret, it is necessary to correlate the difference in weight-distorted values of x * and xm with the difference in their means, i.e. (xT \u043d \u2212 xTm\u03b8), and Lemma 5 provides this context."}, {"heading": "4.2.1 Proof of Lemma 5", "text": "It is not as if it were not as if it were not as it is. (...) It is not as if we were. (...) It is as if we were. (...) It is as if we were. (...) It is as if we were. (...) It is as if we were. (...) It is as if we were. (...) It is as if we were. (...) It is as if we were. (...) It is as if we were. (...) It is as if we were. (...) It is as if we were. (...) It is as if we were. (...) It is as if we were. (...) It is as if we were. (...) It is as if we were. (...) It is as if we are. (...) It is as if we are. (...) It is as if we are. (...) It is as if we are. (...) It is. (...) It is as if we are. (...) It is. (...) It is."}, {"heading": "5 Numerical Experiments", "text": "For both problems, we take the S-shaped weight function w (p) = p \u03b7 (p\u03b7 + (1 \u2212 p) \u03b7) 1 / \u03b7, with \u03b7 = 0.61, to model perceived cost distortions. Tversky and Kahneman note that this distortion weight function is well suited to explain distorted value preferences in humans. 5.1 Experiments for K-armed bandits We examine two stylized 2-armed bandit problems, based in part on experiments conducted by Tversky and Kahneman on human subjects. [1992] In their studies of cumulative future theory outside the EU, we distort each problem in detail (following the convention of this paper to model costs,. \"x w.p\" is assumed to mean a loss of $x with a probability of p.)"}, {"heading": "5.2 Experiments for Linear Bandit", "text": "We are investigating the problem of optimizing a human traveler's route choice with Green Light District (GLD) for these purposes. (GLD) We are investigating the problem of optimizing a human traveler's route choice with Green Light District (GLD) for these purposes. (GLD) We are investigating the problem of optimizing a human traveler's route choice with Green Light District (GLD) for these purposes. (GLD) We are looking at the distorted value (as defined in (8))) as a performance metric.We are implementing both the OFUL and WOFUL algorithms for this problem, while traditional algorithms are minimizing the expected delay, in this work we are looking at the distorted value (as defined in (8))) as the performance metric.We are implementing both OFUL and WOFUL algorithms for this problem."}, {"heading": "6 Conclusions and Future Work", "text": "We have developed online learning algorithms to minimize weight-distorted costs - a generalization of the expected value - in both standard (unstructured) k-armed bandits and linearly parameterized bandit settings. In the future, it will be interesting to investigate the general learning problem of online amplification using weight-distorted cost metrics. Existing algorithms for expected value maximization such as UCRL [Jaksch et al., 2010] and PSRL [Osband et al., 2013] could be adapted for this purpose. Other interesting directions include consideration of contextual versions of the cost-distorted bandit problem and perceived distortions of observations."}], "references": [{"title": "Improved algorithms for linear stochastic bandits", "author": ["Yasin Abbasi-Yadkori", "D\u00e1vid P\u00e1l", "Csaba Szepesv\u00e1ri"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Abbasi.Yadkori et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Abbasi.Yadkori et al\\.", "year": 2011}, {"title": "Le comportement de l\u2019homme rationel devant le risque", "author": ["M. Allais"], "venue": "Critique des postulats et axioms de l\u2019ecole americaine. Econometrica,", "citeRegEx": "Allais.,? \\Q1953\\E", "shortCiteRegEx": "Allais.", "year": 1953}, {"title": "Exploration-exploitation tradeoff using variance estimates in multi-armed bandits", "author": ["Jean-Yves Audibert", "R\u00e9mi Munos", "Csaba Szepesv\u00e1ri"], "venue": "Theor. Comput. Sci.,", "citeRegEx": "Audibert et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Audibert et al\\.", "year": 2009}, {"title": "Finite-time analysis of the multiarmed bandit problem", "author": ["Peter Auer", "Nicol\u00f2 Cesa-Bianchi", "Paul Fischer"], "venue": "Machine Learning,", "citeRegEx": "Auer et al\\.,? \\Q2002\\E", "shortCiteRegEx": "Auer et al\\.", "year": 2002}, {"title": "The nonstochastic multiarmed bandit problem", "author": ["Peter Auer", "Nicol\u00f2 Cesa-Bianchi", "Yoav Freund", "Robert E. Schapire"], "venue": "SIAM J. Comput.,", "citeRegEx": "Auer et al\\.,? \\Q2003\\E", "shortCiteRegEx": "Auer et al\\.", "year": 2003}, {"title": "An experimental test of several generalized utility theories", "author": ["Colin F Camerer"], "venue": "Journal of Risk and Uncertainty,", "citeRegEx": "Camerer.,? \\Q1989\\E", "shortCiteRegEx": "Camerer.", "year": 1989}, {"title": "Recent tests of generalizations of expected utility theory. In Utility Theories: Measurements and Applications, pages 207\u2013251", "author": ["Colin F Camerer"], "venue": null, "citeRegEx": "Camerer.,? \\Q1992\\E", "shortCiteRegEx": "Camerer.", "year": 1992}, {"title": "New measures for performance evaluation", "author": ["Alexander Cherny", "Dilip Madan"], "venue": "Review of Financial Studies,", "citeRegEx": "Cherny and Madan.,? \\Q2009\\E", "shortCiteRegEx": "Cherny and Madan.", "year": 2009}, {"title": "Three variants on the Allais example", "author": ["John Conlisk"], "venue": "The American Economic Review, pages 392\u2013407,", "citeRegEx": "Conlisk.,? \\Q1989\\E", "shortCiteRegEx": "Conlisk.", "year": 1989}, {"title": "The price of bandit information for online optimization", "author": ["Varsha Dani", "Sham M. Kakade", "Thomas P. Hayes"], "venue": "In Advances in Neural Information Processing Systems", "citeRegEx": "Dani et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Dani et al\\.", "year": 2007}, {"title": "Stochastic linear optimization under bandit feedback", "author": ["Varsha Dani", "Thomas P Hayes", "Sham M Kakade"], "venue": "In Proceedings of the 21st Annual Conference on Learning Theory (COLT),", "citeRegEx": "Dani et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Dani et al\\.", "year": 2008}, {"title": "On the shape of the probability weighting function", "author": ["Richard Gonzalez", "George Wu"], "venue": "Cognitive Psychology,", "citeRegEx": "Gonzalez and Wu.,? \\Q1999\\E", "shortCiteRegEx": "Gonzalez and Wu.", "year": 1999}, {"title": "Near-optimal regret bounds for reinforcement learning", "author": ["T. Jaksch", "R. Ortner", "P. Auer"], "venue": "J. Mach. Learn. Res.,", "citeRegEx": "Jaksch et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Jaksch et al\\.", "year": 2010}, {"title": "Asymptotically efficient adaptive allocation rules", "author": ["T.L. Lai", "Herbert Robbins"], "venue": "Advances in Applied Mathematics,", "citeRegEx": "Lai and Robbins.,? \\Q1985\\E", "shortCiteRegEx": "Lai and Robbins.", "year": 1985}, {"title": "From Bandits to Monte-Carlo Tree Search: The Optimistic Principle Applied to Optimization and Planning", "author": ["R\u00e9mi Munos"], "venue": "Foundations and Trends in Machine Learning,", "citeRegEx": "Munos.,? \\Q2014\\E", "shortCiteRegEx": "Munos.", "year": 2014}, {"title": "More) Efficient reinforcement learning via posterior sampling", "author": ["Ian Osband", "Dan Russo", "Benjamin Van Roy"], "venue": "In Proc. Neural Information Processing Systems,", "citeRegEx": "Osband et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Osband et al\\.", "year": 2013}, {"title": "Cumulative prospect theory meets reinforcement learning: prediction and control", "author": ["L.A. Prashanth", "Jie Cheng", "Michael Fu", "Steve Marcus", "Csaba Szepesv\u00e1ri"], "venue": "Proceedings of the 33rd International Conference on Machine Learning,", "citeRegEx": "Prashanth et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Prashanth et al\\.", "year": 2016}, {"title": "The probability weighting function", "author": ["Drazen Prelec"], "venue": "Econometrica, pages 497\u2013527,", "citeRegEx": "Prelec.,? \\Q1998\\E", "shortCiteRegEx": "Prelec.", "year": 1998}, {"title": "Generalized Expected Utility Theory: The Rank-dependent", "author": ["John Quiggin"], "venue": "Model. Springer Science & Business Media,", "citeRegEx": "Quiggin.,? \\Q2012\\E", "shortCiteRegEx": "Quiggin.", "year": 2012}, {"title": "Developments in non-expected utility theory: The hunt for a descriptive theory of choice under risk", "author": ["Chris Starmer"], "venue": "Journal of Economic Literature,", "citeRegEx": "Starmer.,? \\Q2000\\E", "shortCiteRegEx": "Starmer.", "year": 2000}, {"title": "Advances in prospect theory: Cumulative representation of uncertainty", "author": ["A. Tversky", "D. Kahneman"], "venue": "Journal of Risk and Uncertainty,", "citeRegEx": "Tversky and Kahneman.,? \\Q1992\\E", "shortCiteRegEx": "Tversky and Kahneman.", "year": 1992}, {"title": "Simulation and optimization of traffic in a city", "author": ["M. Wiering", "J. Vreeken", "J. van Veenen", "A. Koopman"], "venue": "In IEEE Intelligent Vehicles Symposium,", "citeRegEx": "Wiering et al\\.,? \\Q2004\\E", "shortCiteRegEx": "Wiering et al\\.", "year": 2004}, {"title": "Curvature of the probability weighting function", "author": ["George Wu", "Richard Gonzalez"], "venue": "Management Science,", "citeRegEx": "Wu and Gonzalez.,? \\Q1996\\E", "shortCiteRegEx": "Wu and Gonzalez.", "year": 1996}], "referenceMentions": [{"referenceID": 0, "context": "For the linearly parameterized setting, we develop a new algorithm, a variant of the Optimism in the Face of Uncertainty Linear bandit (OFUL) algorithm Abbasi-Yadkori et al. [2011] called WOFUL (Weight-distorted OFUL), and show that it has regret O(d \u221a n polylog(n)) with high probability, for sub-Gaussian cost distributions.", "startOffset": 152, "endOffset": 181}, {"referenceID": 1, "context": "The above example illustrates that traditional expected value falls short in explaining human preferences, and the reader is referred to the classic Allais problem Allais [1953] that rigorously argues against expected utility theory as a model for human-based decision making systems.", "startOffset": 149, "endOffset": 178}, {"referenceID": 1, "context": "The above example illustrates that traditional expected value falls short in explaining human preferences, and the reader is referred to the classic Allais problem Allais [1953] that rigorously argues against expected utility theory as a model for human-based decision making systems. Violations of the expected value-based preferences in human-based decision making systems can be alleviated by incorporating distortions in the underlying probabilities of the system Starmer [2000] [Quiggin, 2012, Chapter 4].", "startOffset": 149, "endOffset": 483}, {"referenceID": 1, "context": "The above example illustrates that traditional expected value falls short in explaining human preferences, and the reader is referred to the classic Allais problem Allais [1953] that rigorously argues against expected utility theory as a model for human-based decision making systems. Violations of the expected value-based preferences in human-based decision making systems can be alleviated by incorporating distortions in the underlying probabilities of the system Starmer [2000] [Quiggin, 2012, Chapter 4]. Probabilistic distortions have a long history in behavioral science and economics, and we bring this idea to a multi-armed bandit setup. In particular, we base our approach on rank-dependent expected utility (RDEU) Quiggin [2012], which includes the popular cumulative prospect theory (CPT) of Tversky and Kahneman Tversky and Kahneman [1992].", "startOffset": 149, "endOffset": 741}, {"referenceID": 1, "context": "The above example illustrates that traditional expected value falls short in explaining human preferences, and the reader is referred to the classic Allais problem Allais [1953] that rigorously argues against expected utility theory as a model for human-based decision making systems. Violations of the expected value-based preferences in human-based decision making systems can be alleviated by incorporating distortions in the underlying probabilities of the system Starmer [2000] [Quiggin, 2012, Chapter 4]. Probabilistic distortions have a long history in behavioral science and economics, and we bring this idea to a multi-armed bandit setup. In particular, we base our approach on rank-dependent expected utility (RDEU) Quiggin [2012], which includes the popular cumulative prospect theory (CPT) of Tversky and Kahneman Tversky and Kahneman [1992]. \u2217aditya@ece.", "startOffset": 149, "endOffset": 854}, {"referenceID": 20, "context": "The weight function used in the figure is the one recommended by Tversky and Kahneman Tversky and Kahneman [1992] based on empirical tests involving human subjects.", "startOffset": 65, "endOffset": 114}, {"referenceID": 12, "context": "with a inverted-S shaped weight function, to model human decision making (and thus preferences) has been widely documented Prelec [1998], Wu and Gonzalez [1996], Conlisk [1989], Camerer [1989, 1992], Cherny and Madan [2009], Gonzalez and Wu [1999].", "startOffset": 123, "endOffset": 137}, {"referenceID": 12, "context": "with a inverted-S shaped weight function, to model human decision making (and thus preferences) has been widely documented Prelec [1998], Wu and Gonzalez [1996], Conlisk [1989], Camerer [1989, 1992], Cherny and Madan [2009], Gonzalez and Wu [1999].", "startOffset": 123, "endOffset": 161}, {"referenceID": 5, "context": "with a inverted-S shaped weight function, to model human decision making (and thus preferences) has been widely documented Prelec [1998], Wu and Gonzalez [1996], Conlisk [1989], Camerer [1989, 1992], Cherny and Madan [2009], Gonzalez and Wu [1999].", "startOffset": 162, "endOffset": 177}, {"referenceID": 5, "context": "with a inverted-S shaped weight function, to model human decision making (and thus preferences) has been widely documented Prelec [1998], Wu and Gonzalez [1996], Conlisk [1989], Camerer [1989, 1992], Cherny and Madan [2009], Gonzalez and Wu [1999].", "startOffset": 178, "endOffset": 224}, {"referenceID": 5, "context": "with a inverted-S shaped weight function, to model human decision making (and thus preferences) has been widely documented Prelec [1998], Wu and Gonzalez [1996], Conlisk [1989], Camerer [1989, 1992], Cherny and Madan [2009], Gonzalez and Wu [1999]. We use a traveler\u2019s route choice as a running example to illustrate the main ideas in this paper.", "startOffset": 178, "endOffset": 248}, {"referenceID": 20, "context": "We provide upper bounds on the regret of W-UCB, assuming w is H\u00f6lder continuous and provide empirical demonstrations on a setting from [Tversky and Kahneman, 1992].", "startOffset": 135, "endOffset": 163}, {"referenceID": 0, "context": "For the linear bandit setting, we propose a variant of the OFUL algorithm [Abbasi-Yadkori et al., 2011], that incorporates weight-distorted values in the arm selection step.", "startOffset": 74, "endOffset": 103}, {"referenceID": 13, "context": "The W-UCB algorithm that we propose incorporates a empirical distribution-based approach, similar to that of Prashanth et al. [2016], to estimate \u03bcx.", "startOffset": 109, "endOffset": 133}, {"referenceID": 16, "context": "Related work The closest related previous contribution is that of Prashanth et al. [2016], where the authors bring in ideas from CPT to a reinforcement learning (RL) setting.", "startOffset": 66, "endOffset": 90}, {"referenceID": 16, "context": "Related work The closest related previous contribution is that of Prashanth et al. [2016], where the authors bring in ideas from CPT to a reinforcement learning (RL) setting. In contrast, we formulate two multi-armed bandit models that incorporate weight-distortions. From a theoretical standpoint, we handle the exploration-exploitation tradeoff via UCB-inspired algorithms, while the focus of Prashanth et al. [2016] was to devise a policy-gradient scheme given biased estimates of a certain CPT-value defined for each policy.", "startOffset": 66, "endOffset": 419}, {"referenceID": 16, "context": "Related work The closest related previous contribution is that of Prashanth et al. [2016], where the authors bring in ideas from CPT to a reinforcement learning (RL) setting. In contrast, we formulate two multi-armed bandit models that incorporate weight-distortions. From a theoretical standpoint, we handle the exploration-exploitation tradeoff via UCB-inspired algorithms, while the focus of Prashanth et al. [2016] was to devise a policy-gradient scheme given biased estimates of a certain CPT-value defined for each policy. Moreover, we provide finite-time regret bounds for both bandit settings, while the guarantees for the policy gradient algorithm of Prashanth et al. [2016] are asymptotic in nature.", "startOffset": 66, "endOffset": 684}, {"referenceID": 4, "context": ", EXP3 [Auer et al., 2003], are not inherently suitable for this problem, as they do not factor in the distortion caused in (expected) reward.", "startOffset": 7, "endOffset": 26}, {"referenceID": 2, "context": "A similar observation holds for conventional stochastic bandit algorithms such as UCB, and algorithms sensitive to arm reward variances such as UCB-V [Audibert et al., 2009] \u2013 once weight distortion is incorporated, the algorithm will converge to an arm that is not weight-distorted value optimal.", "startOffset": 150, "endOffset": 173}, {"referenceID": 16, "context": "Thus, one needs to estimate the entire distribution, and for this purpose, we adapt the quantile-based approach of [Prashanth et al., 2016].", "startOffset": 115, "endOffset": 139}, {"referenceID": 20, "context": "Moreover, the popular choice for the weight function, proposed by Tversky and Kahneman [1992] and illustrated in Figure 1, is H\u00f6lder continuous.", "startOffset": 66, "endOffset": 94}, {"referenceID": 20, "context": "(CPT adaptation) As remarked in the introduction, a popular approach using a weight function to distort probabilities is the so-called cumulative prospect theory (CPT) [Tversky and Kahneman, 1992].", "startOffset": 168, "endOffset": 196}, {"referenceID": 10, "context": "ConfidenceBall in [Dani et al., 2008] or OFUL in [Abbasi-Yadkori et al.", "startOffset": 18, "endOffset": 37}, {"referenceID": 0, "context": ", 2008] or OFUL in [Abbasi-Yadkori et al., 2011]), but deviates in the step when an arm is chosen.", "startOffset": 19, "endOffset": 48}, {"referenceID": 0, "context": ", 2008] or OFUL in [Abbasi-Yadkori et al., 2011]), but deviates in the step when an arm is chosen. In particular, in any round m of the algorithm, WOFUL uses \u03bcx(\u03b8) as the decision criterion for any arm x \u2208 X and \u03b8 \u2208 Cm, where \u03bcx(\u03b8) is the weight-distorted value that is defined in (8) and Cm is the confidence ellipsoid that is specified in Algorithm 1. This is unlike regular linear bandit algorithms, which use x\u03b8 as the cost for any arm x \u2208 X and \u03b8 \u2208 Cm. Note that the \u201cin-parameter\u201d or arm-dependent noise model (7) also necessitates modifying the standard confidence ellipsoid construction of Abbasi-Yadkori et al. [2011] by rescaling with the arm size (the Am and bm variables in Algorithm 1).", "startOffset": 20, "endOffset": 627}, {"referenceID": 10, "context": "For the identity weight function w(t) = t, 0 \u2264 t \u2264 1 with L = \u03b1 = 1, we recover the stochastic linear bandit setting, and the associated \u00d5 (d \u221a n) regret bound for linear bandit algorithms such as ConfidenceBall1 and ConfidenceBall2 [Dani et al., 2008], OFUL [Abbasi-Yadkori et al.", "startOffset": 233, "endOffset": 252}, {"referenceID": 0, "context": ", 2008], OFUL [Abbasi-Yadkori et al., 2011].", "startOffset": 14, "endOffset": 43}, {"referenceID": 9, "context": "A lower bound of essentially the same order as Theorem 4 (O(d \u221a n)) holds for regret in (undistorted) linear bandits [Dani et al., 2007].", "startOffset": 117, "endOffset": 136}, {"referenceID": 16, "context": "1 Proof of Theorem 1 The proof is very similar to that of Proposition 2 in [Prashanth et al., 2016].", "startOffset": 75, "endOffset": 99}, {"referenceID": 14, "context": "As in the case of regular UCB, we bound the number of times a sub-optimal arm is pulled using the technique from Munos [2014]. Let k\u2217 denote the optimal arm.", "startOffset": 113, "endOffset": 126}, {"referenceID": 13, "context": "The key ingredient in the proof is the seminal lower-bound on the number of suboptimal arm plays derived by Lai and Robbins [1985]. Their result shows that for any algorithm that plays suboptimal arms only a subpolynomial number of times in the time horizon,", "startOffset": 108, "endOffset": 131}, {"referenceID": 20, "context": "5 Cumulative prospect theory (CPT) for K-armed bandit As remarked in the introduction, a popular approach using a weight function to distort probabilities is the so-called cumulative prospect theory (CPT) of Tversky and Kahneman [1992]. In the following, we extend the W-UCB algorithm to incorporate CPT-style utility functions.", "startOffset": 208, "endOffset": 236}, {"referenceID": 0, "context": "The proof is a consequence of Theorem 2 in [Abbasi-Yadkori et al., 2011].", "startOffset": 43, "endOffset": 72}, {"referenceID": 10, "context": "(29) Inequality (28) follows by an application of Cauchy-Schwarz inequality, while the inequality in (29) follows from (the standard by now) Lemma 9 in [Dani et al., 2008], which shows that \u2211n m=1 w 2 m \u2264 2d log n.", "startOffset": 152, "endOffset": 171}, {"referenceID": 3, "context": "We benchmark the cumulative regret of two algorithms \u2013 (a) the well-known UCB algorithm [Auer et al., 2002], and (b) W-UCB; the results are as in Figure 2.", "startOffset": 88, "endOffset": 107}, {"referenceID": 18, "context": "1 Experiments for K-armed Bandit We study two stylized 2-armed bandit problems which, in part, draw upon experiments carried out by Tversky and Kahneman [1992] on human subjects in their studies of non-EU cumulative prospect theory.", "startOffset": 132, "endOffset": 160}, {"referenceID": 21, "context": "2 Experiments for Linear Bandit We study the problem of optimizing the route choice of a human traveler using Green light district (GLD) traffic simulation software Wiering et al. [2004]. In this setup, a source-destination pair is fixed in a given road network.", "startOffset": 165, "endOffset": 187}, {"referenceID": 12, "context": "Existing algorithms for expected value maximization such as UCRL [Jaksch et al., 2010] and PSRL [Osband et al.", "startOffset": 65, "endOffset": 86}, {"referenceID": 15, "context": ", 2010] and PSRL [Osband et al., 2013] could be adapted for this purpose.", "startOffset": 17, "endOffset": 38}], "year": 2016, "abstractText": "Motivated by models of human decision making proposed to explain commonly observed deviations from conventional expected value preferences, we formulate two stochastic multi-armed bandit problems with distorted probabilities on the cost distributions: the classic K-armed bandit and the linearly parameterized bandit. In both settings, we propose algorithms that are inspired by Upper Confidence Bound (UCB) algorithms, incorporate cost distortions, and exhibit sublinear regret assuming H\u00f6lder continuous weight distortion functions. For the K-armed setting, we show that the algorithm, called W-UCB, achieves problem-dependent regret O ( LM logn/\u2206 2 \u03b1 \u22121 ) , where n is the number of plays, \u2206 is the gap in distorted expected value between the best and next best arm, L and \u03b1 are the H\u00f6lder constants for the distortion function, and M is an upper bound on costs, and a problem-independent regret bound of O((KL2M2)\u03b1/2n(2\u2212\u03b1)/2). We also present a matching lower bound on the regret, showing that the regret of W-UCB is essentially unimprovable over the class of H\u00f6lder -continuous weight distortions. For the linearly parameterized setting, we develop a new algorithm, a variant of the Optimism in the Face of Uncertainty Linear bandit (OFUL) algorithm Abbasi-Yadkori et al. [2011] called WOFUL (Weight-distorted OFUL), and show that it has regret O(d \u221a n polylog(n)) with high probability, for sub-Gaussian cost distributions. Finally, numerical examples demonstrate the advantages resulting from using distortion-aware learning algorithms.", "creator": "LaTeX with hyperref package"}}}