{"id": "1612.00094", "review": {"conference": "AAAI", "VERSION": "v1", "DATE_OF_SUBMISSION": "1-Dec-2016", "title": "Optimizing Quantiles in Preference-Based Markov Decision Processes", "abstract": "In the Markov decision process model, policies are usually evaluated by expected cumulative rewards. As this decision criterion is not always suitable, we propose in this paper an algorithm for computing a policy optimal for the quantile criterion. Both finite and infinite horizons are considered. Finally we experimentally evaluate our approach on random MDPs and on a data center control problem.", "histories": [["v1", "Thu, 1 Dec 2016 00:55:23 GMT  (430kb,D)", "http://arxiv.org/abs/1612.00094v1", "Long version of AAAI 2017 paper. arXiv admin note: text overlap witharXiv:1611.00862"]], "COMMENTS": "Long version of AAAI 2017 paper. arXiv admin note: text overlap witharXiv:1611.00862", "reviews": [], "SUBJECTS": "cs.AI", "authors": ["hugo gilbert", "paul weng", "yan xu"], "accepted": true, "id": "1612.00094"}, "pdf": {"name": "1612.00094.pdf", "metadata": {"source": "CRF", "title": "Optimizing Quantiles in Preference-based Markov Decision Processes", "authors": ["Hugo Gilbert", "Paul Weng", "Yan Xu"], "emails": ["hugo.gilbert@lip6.fr", "paweng@cmu.edu", "xuyan@cmu.edu"], "sections": [{"heading": "1 Introduction", "text": "In fact, most of them are able to play by the rules."}, {"heading": "2 Background", "text": "In this section we provide the background information necessary for the continuation."}, {"heading": "2.1 Markov Decision Process", "text": "Markov Decision Processes (MDPs) provide a general and powerful formalism for modeling and solving sequential decision-making problems (Puterman, 1994).An MDP is formally defined as a tupleMT = (S, A, P, r, s0), where T is a time horizon, S is a finite series of states, A is a finite series of actions, P: S \u00b7 A \u00b7 S \u2192 R is a transitional function with P (s, a, s), where the probability of reaching a state is s if an action is performed in the state s, r: S \u00b7 A \u2192 R is a limited reward function, and s0 \u00b2 S is a specific state designated as the initial state."}, {"heading": "2.2 Preferences over Histories", "text": "For the sake of the general public, in this work we define the reward function to take values in a specified R. Furthermore, we assume that the values of history take values in a specified W, which is called the prosperity level space, and that the value of a story ht = (s0, a0, s1,.., st) is defined by: w (h0) = w0 w (ht) = w (ht \u2212 1). \u2212 r (st \u2212 1), where ht \u2212 1 = (s0, a0, s1,.., st \u2212 1), it is a binary operation from W \u00b7 R to W, and w0 \u00b2 W is the left identity element of the group. Letter \u00b2 W is the set of welfare levels of T history. We assume three assumptions about WT: \u2022 It is ordered by an overall order W, which is defined as T histories are compared \u2022 it becomes a lowest element, denominates a minimum level, and denominates a largest element."}, {"heading": "2.3 Quantile Criterion", "text": "Intuitively, the quantitative distribution of wealth (w) = quantitative distribution of wealth (w) is preferred. (f) Intuitively, the quantitative distribution of wealth is not applied to the actual distribution of wealth (w). (f) Intuitively, the quantitative distribution of wealth is not applied to the actual distribution of wealth (w). (f) Intuitively, the quantitative distribution of wealth is not defined to the actual distribution of wealth. (f) Intuitively, the quantitative distribution of wealth is defined as such. (f) Intuitively, the quantitative distribution of wealth is defined as such. (f) Intuitively, the cumulative distribution of wealth is defined as such. (f) The quantitative distribution of wealth is then defined as such. (f) The quantitative distribution of wealth is then defined as such."}, {"heading": "3 Solving Algorithm", "text": "In this section we present a technique for calculating an \u03b5-optimal policy for the quantitative criterion. Our approach boils down to solving a sequence of MDPs that optimize the EU with target functions (see Section 3.2)."}, {"heading": "3.1 Binary Search", "text": "In order to justify our algorithm, we introduce two lemmas that characterize the optimal lower and upper quantity1: Lemma 1. The optimal lower quantity1: Lemma 1. (Lemma 1.): Quantile 1. (Lemma 1.): Quantile 2. (Lemma 1.) The optimal lower quantity1: Quantile 2. (Lemma 1.) The optimal lower quantity 4. (Lemma 1.): Quantile 1. (Lemma 1.) (Lemma 1.) The optimal lower quantity 4. (Lemma 1.) 4. (Lemma 1.): Quantile 1. (Lemma 1.). (Lemma 1.). (Lemma 1.). (Lemma 1.). (Lemma 1.). (Lemma 1.). (Lemma 1.). (Lemma 1.). (Lemma 1.). (Lemma 1.). (Lemma 1. (Lemma 1.) 4. (Lemma 1.). (Lemma 1.). (Lemma 1. (Lemma 1.). (Lemma 1.): Quantity 1. (Lemma 1.): Quantity 1. (Lemma 1.): Quantity 1. (Lemma 1.): Quantity 1. (Lemma 1. (Lemma 1.): Quantity 1. (Lemma 1.): Quantity 1. (Lemma 1. (Lemma 1.): Quantity 1. (Lemma 1.): Quantity 1. (Lemma 1.): (Lemma 1. (Lemma 1.): Quantity 1. (Lemma 1. (Lemma 1.): (Lemma 1.): Quantity 1. (Lemma 1. (Lemma 1.). (Lemma 1.). (Lemma 1.). (Lemma 1.): (Lemma 1. (Lemma 1.). (Lemma 1.): (Lemma 1. (Lemma 1.): (Lemma 1.). (Lemma 1.). (Lemma 1.): (Lemma 1. (Lemma 1.)"}, {"heading": "3.2 Dynamic Programming", "text": "In fact, the function referred to as the Target-Benefit function is defined as follows: U / w (x) = 1 if w / x and 0 otherwise. (11) Algorithm 2: FunctionalBackwardInduction Data: MDPM, wealth w Result: an optimal policy \u03c0 1 for all s \"S\" S do 2 VT + 1 (s, \".) \u2190 U / w (.) 3 for t = T do 1 4 for all s\" S do 5 Vt (s, \"\u00b7) \u2190 maxa\" s \"S P\" (s, \"s\") Vt + 1 (s, \"\u00b7 r,\" a \") 6 Return (\u03c0V1, V1 (s0, w0))) V1 = Policy corresponding to V1When optimizing the lower (or upper) quantity, the function solution (M\" w \") by solving the MDPM using EU as the decision criterion."}, {"heading": "4 Infinite Horizon", "text": "In this section, we present some results in relation to the case of the infinite horizon. Similar to setting the finite horizon, the situation for the quantile criterion is not as simple as for the standard case. In fact, in the case of the infinite horizon, it may happen that there is no stationary deterministic Markovian policy that is optimal (w.r.t. the quantile criterion) among all strategies, as opposed to standard MDPs. Example 2. Let us consider an MDP with two states s1 and s2 and two actions a2. In s1, the transition probabilities are P (s1, a1, s1) = 0.1, P (s1, s2) = 0.9 and P (s1, s2) = 1. To make this example shorter, let us assume that the rewards depend on the next states. The rewards are r (s1, a1, s1, s1, s1) = 1, s2) = r and \u2212 (sa2) = likely."}, {"heading": "5 Experimental Results", "text": "We assume that we will achieve an optimal number of programming languages and be improved by taking advantage of a multicultural architecture. Although our approach could be used in a preference-based setting, we are conducting experiments with numerical rewards for simplicity. The first shows the runtime of the functional backward induction for different state sizes on random MDPs. The second series of experiments shows the runtime of the functional backward induction for a data center problem with different servers. Finally, the third compares the cumulative distributions of an optimal policy for the quantity criteria and an optimal policy for the standard criteria on a fixed MDP. The first series of experiments was conducted on different servers."}, {"heading": "6 Related Work", "text": "In fact, most of them are able to determine for themselves what they want and what they want."}, {"heading": "7 Conclusion", "text": "In this paper, we have developed a framework for solving sequential decision-making problems in a very general environment using a quantitative criterion. By modelling these problems as MDPs, we developed an offline algorithm to calculate optimal policies and investigated the characteristics of optimal policies in cases with limited and infinite horizons. Finally, we provided experimental results by testing these two algorithms in a variety of situations. As a future work, we plan to explore how this work can be extended to the case of enhanced learning, a framework that is more involved than that of MDPs, in which the dynamics of the problems are unknown and must be learned."}, {"heading": "8 Supplementary material of \u201cOptimizing Quantiles in Preference-based Markov Decision Processes\u201d", "text": "In this section we provide the evidence of our Lemmas and Propositions. Lemma 1 = q q q q q = q q q fulfills: q \u0445 = min {w: F \u043c \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s.\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s.\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s.\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s. \"s\" s \"s\" s \"s\" s \"s\" s. \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s.\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s.\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s. \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s.\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s. \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s.\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s"}], "references": [{"title": "Mathematical Methods of Operations Research", "author": ["Nicole B\u00e4uerle", "Jonathan Ott. Markov decision processes with average value-at-risk criteria"], "venue": "74(3):361\u2013379,", "citeRegEx": "B\u00e4uerle and Ott. 2011", "shortCiteRegEx": null, "year": 2011}, {"title": "Benefits of quantile regression for the analysis of customer lifetime value in a contractual setting: An application in financial services", "author": ["D.F. Benoit", "D. Van den Poel"], "venue": "Expert Systems with Applications, 36:10475\u201310484,", "citeRegEx": "Benoit and Van den Poel. 2009", "shortCiteRegEx": null, "year": 2009}, {"title": "on Automatic Control", "author": ["V. Borkar", "Rahul Jain. Risk-constrained Markov decision processes. IEEE Trans"], "venue": "59(9):2574\u20132579,", "citeRegEx": "Borkar and Jain. 2014", "shortCiteRegEx": null, "year": 2014}, {"title": "Target-level criterion in Markov decision processes", "author": ["M. Bouakiz", "Y. Kebir"], "venue": "Journal of Optimization Theory and Applications, 86(1):1\u201315", "citeRegEx": "Bouakiz and Kebir. 1995", "shortCiteRegEx": null, "year": 1995}, {"title": "chapter Non-Standard Criteria", "author": ["Matthieu Boussard", "Maroua Bouzid", "Abdel-Illah Mouaddib", "R\u00e9gis Sabbadin", "Paul Weng. Markov Decision Processes in Artificial Intelligence"], "venue": "pages 319\u2013359. Wiley,", "citeRegEx": "Boussard et al.. 2010", "shortCiteRegEx": null, "year": 2010}, {"title": "Preference-based Reinforcement Learning: Evolutionary Direct Policy Search using a Preference-based Racing Algorithm", "author": ["R\u00f3bert Busa-Fekete", "Bal\u00e1zs Sz\u00f6renyi", "Paul Weng", "Weiwei Cheng", "Eyke H\u00fcllermeier"], "venue": "Machine Learning, 97(3):327\u2013351,", "citeRegEx": "Busa.Fekete et al.. 2014", "shortCiteRegEx": null, "year": 2014}, {"title": "Extreme bandits", "author": ["Alexandra Carpentier", "Michal Valko"], "venue": "NIPS,", "citeRegEx": "Carpentier and Valko. 2014", "shortCiteRegEx": null, "year": 2014}, {"title": "Algorithms for CVaR optimization in MDPs", "author": ["Yinlam Chow", "Mohammad Ghavamzadeh"], "venue": "NIPS,", "citeRegEx": "Chow and Ghavamzadeh. 2014", "shortCiteRegEx": null, "year": 2014}, {"title": "Dynamo: Amazon\u2019s highly available key-value store", "author": ["G. DeCandia", "D. Hastorun", "M. Jampani", "G. Kakulapati", "A. Lakshman", "A. Pilchin", "S. Sivasubramanian", "P. Vosshall", "W. Vogels"], "venue": "ACM SIGOPS Operating Systems Review, 41(6):205\u2013 220", "citeRegEx": "DeCandia et al.. 2007", "shortCiteRegEx": null, "year": 2007}, {"title": "Percentile optimization in uncertain Markov decision processes with application to efficient exploration", "author": ["E. Delage", "S. Mannor"], "venue": "ICML, pages 225\u2013232", "citeRegEx": "Delage and Mannor. 2007", "shortCiteRegEx": null, "year": 2007}, {"title": "In AAMAS", "author": ["Stefano Ermon", "Carla Gomes", "Bart Selman", "Alexander Vladimirsky. Probabilistic planning with non-linear utility functions", "worst-case guarantees"], "venue": "pages 965\u2013972,", "citeRegEx": "Ermon et al.. 2012", "shortCiteRegEx": null, "year": 2012}, {"title": "Arriving on time", "author": ["YY Fan", "RE Kalaba", "JE Moore II"], "venue": "Journal of Optimization Theory and Applications, 127(3):497\u2013513", "citeRegEx": "Fan et al.. 2005", "shortCiteRegEx": null, "year": 2005}, {"title": "Resolute choice in sequential decision problems with multiple priors", "author": ["H\u00e9l\u00e8ne Fargier", "Gildas Jeantet", "Olivier Spanjaard"], "venue": "IJCAI,", "citeRegEx": "Fargier et al.. 2011", "shortCiteRegEx": null, "year": 2011}, {"title": "Mathematics of Operations Research", "author": ["Jerzy A. Filar", "L.C.M. Kallenberg", "Huey-Miin Lee. Variancepenalized Markov decision processes"], "venue": "14:147\u2013 161,", "citeRegEx": "Filar et al.. 1989", "shortCiteRegEx": null, "year": 1989}, {"title": "Percentile performance criteria for limiting average Markov decision processes", "author": ["J.A. Filar", "D. Krass", "K.W. Ross"], "venue": "IEEE Trans. on Automatic Control, 40(1):2\u201310", "citeRegEx": "Filar et al.. 1995", "shortCiteRegEx": null, "year": 1995}, {"title": "Operations Research Letters", "author": ["Jerzy A. Filar. Percentiles", "Markovian decision processes"], "venue": "2(1):13 \u2013 15,", "citeRegEx": "Filar. 1983", "shortCiteRegEx": null, "year": 1983}, {"title": "An axiomatic characterization of skew-symmetric bilinear functionals", "author": ["P.C. Fishburn"], "venue": "with applications to utility theory. Economics Letters, 8(4):311\u2013313", "citeRegEx": "Fishburn. 1981", "shortCiteRegEx": null, "year": 1981}, {"title": "In IJCAI", "author": ["Hugo Gilbert", "Olivier Spanjaard", "Paolo Viappiani", "Paul Weng. Solving MDPs with skew symmetric bilinear utility functions"], "venue": "pages 1989\u2013 1995,", "citeRegEx": "Gilbert et al.. 2015", "shortCiteRegEx": null, "year": 2015}, {"title": "Revisiting risk-sensitive MDPs: New algorithms and results", "author": ["Ping Hou", "William Yeoh", "Pradeep Reddy Varakantham"], "venue": "ICAPS,", "citeRegEx": "Hou et al.. 2014", "shortCiteRegEx": null, "year": 2014}, {"title": "Implementing resolute choice under uncertainty", "author": ["Jean-Yves Jaffray"], "venue": "UAI,", "citeRegEx": "Jaffray. 1998", "shortCiteRegEx": null, "year": 1998}, {"title": "Value-at-Risk: The New Benchmark for Managing Financial Risk", "author": ["Philippe Jorion"], "venue": "McGraw-Hill,", "citeRegEx": "Jorion. 2006", "shortCiteRegEx": null, "year": 2006}, {"title": "Risk-sensitive planning with one-switch utility functions: Value iteration", "author": ["Y. Liu", "S. Koenig"], "venue": "AAAI, pages 993\u2013999", "citeRegEx": "Liu and Koenig. 2005", "shortCiteRegEx": null, "year": 2005}, {"title": "Functional value iteration for decisiontheoretic planning with general utility functions", "author": ["Y. Liu", "S. Koenig"], "venue": "AAAI, pages 1186\u20131193", "citeRegEx": "Liu and Koenig. 2006", "shortCiteRegEx": null, "year": 2006}, {"title": "Mean-variance optimization in Markov decision processes", "author": ["Shie Mannor", "John Tsitsiklis"], "venue": "ICML,", "citeRegEx": "Mannor and Tsitsiklis. 2011", "shortCiteRegEx": null, "year": 2011}, {"title": "Rationality and dynamic choice: Foundational explorations", "author": ["E. McClennen"], "venue": "Cambridge university press", "citeRegEx": "McClennen. 1990", "shortCiteRegEx": null, "year": 1990}, {"title": "In Journal of the Operational Research Society", "author": ["T.W. Archibald K. McKinnon", "L.C. Thomas. On the generation of Markov decision processes"], "venue": "pages 354\u2013361,", "citeRegEx": "McKinnon and Thomas. 1995", "shortCiteRegEx": null, "year": 1995}, {"title": "Optimal policy for minimizing risk models in Markov decision processes", "author": ["Y. Ohtsubo", "K. Toyonaga"], "venue": "Journal of mathematical analysis and applications, 271:66\u201381", "citeRegEx": "Ohtsubo and Toyonaga. 2002", "shortCiteRegEx": null, "year": 2002}, {"title": "In NIPS", "author": ["LA Prashanth", "Mohammad Ghavamzadeh. Actorcritic algorithms for risk-sensitive MDPs"], "venue": "pages 252\u2013260,", "citeRegEx": "Prashanth and Ghavamzadeh. 2013", "shortCiteRegEx": null, "year": 2013}, {"title": "Markov decision processes: discrete stochastic dynamic programming", "author": ["M.L. Puterman"], "venue": "Wiley", "citeRegEx": "Puterman. 1994", "shortCiteRegEx": null, "year": 1994}, {"title": "Regret based reward elicitation for Markov decision processes", "author": ["K. Regan", "C. Boutilier"], "venue": "UAI, pages 444\u2013451. Morgan Kaufmann", "citeRegEx": "Regan and Boutilier. 2009", "shortCiteRegEx": null, "year": 2009}, {"title": "Quantile maximization in decision theory", "author": ["M.J. Rostek"], "venue": "Review of Economic Studies, 77(1):339\u2013371", "citeRegEx": "Rostek. 2010", "shortCiteRegEx": null, "year": 2010}, {"title": "Qualitative multi-armed bandits: A quantile-based approach", "author": ["Bal\u00e1zs Sz\u00f6renyi", "R\u00f3bert Busa-Fekete", "Paul Weng", "Eyke H\u00fcllermeier"], "venue": "ICML, pages 1660\u20131668,", "citeRegEx": "Sz\u00f6renyi et al.. 2015", "shortCiteRegEx": null, "year": 2015}, {"title": "In IJCAI", "author": ["Paul Weng", "Bruno Zanuttini. Interactive value iteration for Markov decision processes with unknown rewards"], "venue": "pages 2415\u20132421,", "citeRegEx": "Weng and Zanuttini. 2013", "shortCiteRegEx": null, "year": 2013}, {"title": "Markov decision processes with ordinal rewards: Reference point-based preferences", "author": ["Paul Weng"], "venue": "ICAPS, volume 21, pages 282\u2013289,", "citeRegEx": "Weng. 2011", "shortCiteRegEx": null, "year": 2011}, {"title": "volume 20", "author": ["Paul Weng. Ordinal decision models for Markov decision processes. In ECAI"], "venue": "pages 828\u2013833,", "citeRegEx": "Weng. 2012", "shortCiteRegEx": null, "year": 2012}, {"title": "Utility", "author": ["D.J. White"], "venue": "probabilistic constraints, mean and variance of discounted rewards in Markov decision processes. OR Spektrum, 9:13\u201322", "citeRegEx": "White. 1987", "shortCiteRegEx": null, "year": 1987}, {"title": "Minimising a threshold probability in discounted Markov decision processes", "author": ["D.J. White"], "venue": "Journal of mathematical analysis and applications, 173(634\u2013646)", "citeRegEx": "White. 1993", "shortCiteRegEx": null, "year": 1993}, {"title": "QPRED: Using quantile predictions to improve power usage for private clouds", "author": ["R. Wolski", "J. Brevik"], "venue": "Technical report, UCSB", "citeRegEx": "Wolski and Brevik. 2014", "shortCiteRegEx": null, "year": 2014}, {"title": "Journal of mathematical analysis and applications", "author": ["Congbin Wu", "Yuanlie Lin. Minimizing risk models in Markov decision processes with policies depending on target values"], "venue": "231:41\u201367,", "citeRegEx": "Wu and Lin. 1999", "shortCiteRegEx": null, "year": 1999}, {"title": "Adaptive robust optimization for coordinated capacity and load control in data centers", "author": ["Xiaoqi Yin", "Bruno Sinopoli"], "venue": "International Conference on Decision and Control,", "citeRegEx": "Yin and Sinopoli. 2014", "shortCiteRegEx": null, "year": 2014}, {"title": "Sample complexity of riskaverse bandit-arm selection", "author": ["Jia Yuan Yu", "Evdokia Nikolova"], "venue": "IJCAI,", "citeRegEx": "Yu and Nikolova. 2013", "shortCiteRegEx": null, "year": 2013}, {"title": "Journal of mathematical analysis and applications", "author": ["Stella X. Yu", "Yuanlie Lin", "Pingfan Yan. Optimization models for the first arrival target distribution function in discrete time"], "venue": "225:193\u2013223,", "citeRegEx": "Yu et al.. 1998", "shortCiteRegEx": null, "year": 1998}], "referenceMentions": [{"referenceID": 20, "context": "For instance, the Value-at-Risk criterion [Jorion, 2006] widely used in finance is in fact a quantile.", "startOffset": 42, "endOffset": 56}, {"referenceID": 37, "context": "Moreover, in the Web industry [Wolski and Brevik, 2014; DeCandia et al., 2007], decisions about performance or Quality-Of-Service are often made based on quantiles.", "startOffset": 30, "endOffset": 78}, {"referenceID": 8, "context": "Moreover, in the Web industry [Wolski and Brevik, 2014; DeCandia et al., 2007], decisions about performance or Quality-Of-Service are often made based on quantiles.", "startOffset": 30, "endOffset": 78}, {"referenceID": 8, "context": "For instance, Amazon reports [DeCandia et al., 2007] that they optimize the 99.", "startOffset": 29, "endOffset": 52}, {"referenceID": 1, "context": "More generally, in the service industry, because of skewed distributions [Benoit and Van den Poel, 2009], one generally does not want that customers are satisfied on average, but rather that most customers (e.", "startOffset": 73, "endOffset": 104}, {"referenceID": 22, "context": "We provide a binary search algorithm using functional backward induction [Liu and Koenig, 2006] as a subroutine for computing an optimal policy.", "startOffset": 73, "endOffset": 95}, {"referenceID": 28, "context": "Markov Decision Processes (MDPs) offer a general and powerful formalism to model and solve sequential decision-making problems [Puterman, 1994].", "startOffset": 127, "endOffset": 143}, {"referenceID": 29, "context": "One can therefore try to uncover the reward function by interacting with an expert of the domain considered [Regan and Boutilier, 2009; Weng and Zanuttini, 2013].", "startOffset": 108, "endOffset": 161}, {"referenceID": 32, "context": "One can therefore try to uncover the reward function by interacting with an expert of the domain considered [Regan and Boutilier, 2009; Weng and Zanuttini, 2013].", "startOffset": 108, "endOffset": 161}, {"referenceID": 34, "context": "When the lower and upper quantiles differ, one may define the quantile as a function of the lower and upper quantiles [Weng, 2012].", "startOffset": 118, "endOffset": 130}, {"referenceID": 24, "context": "Three solutions are then possible [McClennen, 1990]: 1) adopting a consequentialist approach, i.", "startOffset": 34, "endOffset": 51}, {"referenceID": 19, "context": ", at time step t = 0 we apply an optimal policy for the problem with horizon T and initial state s0 and do not deviate from it; 3) adopting a sophisticated resolute choice approach [Jaffray, 1998; Fargier et al., 2011], i.", "startOffset": 181, "endOffset": 218}, {"referenceID": 12, "context": ", at time step t = 0 we apply an optimal policy for the problem with horizon T and initial state s0 and do not deviate from it; 3) adopting a sophisticated resolute choice approach [Jaffray, 1998; Fargier et al., 2011], i.", "startOffset": 181, "endOffset": 218}, {"referenceID": 22, "context": "Following [Liu and Koenig, 2006], this problem can be solved with a functional backward induction (Algorithm 2).", "startOffset": 10, "endOffset": 32}, {"referenceID": 22, "context": "2) by functional value iteration [Liu and Koenig, 2006] in the binary search.", "startOffset": 33, "endOffset": 55}, {"referenceID": 25, "context": "The first set of experiments was conducted on Garnets [McKinnon and Thomas, 1995], which designate random MDPs with a constrained branching factor.", "startOffset": 54, "endOffset": 81}, {"referenceID": 1, "context": "This experiment is performed on an instance of GarnetG(100, 5, dlog2 100e) whose rewards are slightly modified to make the distribution of the optimal policy skewed, as it is often the case in some real applications [Benoit and Van den Poel, 2009].", "startOffset": 216, "endOffset": 247}, {"referenceID": 4, "context": "Much work in the MDP literature [Boussard et al., 2010] considered decision criteria different to the standard ones (i.", "startOffset": 32, "endOffset": 55}, {"referenceID": 27, "context": "Recently, [Prashanth and Ghavamzadeh, 2013] and [Mannor and Tsitsiklis, 2011] provided algorithms for this mean-variance formulation.", "startOffset": 10, "endOffset": 43}, {"referenceID": 23, "context": "Recently, [Prashanth and Ghavamzadeh, 2013] and [Mannor and Tsitsiklis, 2011] provided algorithms for this mean-variance formulation.", "startOffset": 48, "endOffset": 77}, {"referenceID": 21, "context": "Additionally, in the artificial intelligence community, [Liu and Koenig, 2005; Liu and Koenig, 2006; Ermon et al., 2012] also investigated the use of EU as a decision criterion in MDPs.", "startOffset": 56, "endOffset": 120}, {"referenceID": 22, "context": "Additionally, in the artificial intelligence community, [Liu and Koenig, 2005; Liu and Koenig, 2006; Ermon et al., 2012] also investigated the use of EU as a decision criterion in MDPs.", "startOffset": 56, "endOffset": 120}, {"referenceID": 10, "context": "Additionally, in the artificial intelligence community, [Liu and Koenig, 2005; Liu and Koenig, 2006; Ermon et al., 2012] also investigated the use of EU as a decision criterion in MDPs.", "startOffset": 56, "endOffset": 120}, {"referenceID": 16, "context": "[2015] investigated the use of Skew-Symmetric Bilinear (SSB) utility [Fishburn, 1981] functions \u2014 a generalization of EU with stronger descriptive abilities \u2014 as decision criteria in finite-horizon MDPs.", "startOffset": 69, "endOffset": 85}, {"referenceID": 33, "context": "In MDPs with ordinal rewards [Weng, 2011; Weng, 2012; Filar, 1983], quantile-based decision models were proposed to compute", "startOffset": 29, "endOffset": 66}, {"referenceID": 34, "context": "In MDPs with ordinal rewards [Weng, 2011; Weng, 2012; Filar, 1983], quantile-based decision models were proposed to compute", "startOffset": 29, "endOffset": 66}, {"referenceID": 15, "context": "In MDPs with ordinal rewards [Weng, 2011; Weng, 2012; Filar, 1983], quantile-based decision models were proposed to compute", "startOffset": 29, "endOffset": 66}], "year": 2016, "abstractText": "In the Markov decision process model, policies are usually evaluated by expected cumulative rewards. As this decision criterion is not always suitable, we propose in this paper an algorithm for computing a policy optimal for the quantile criterion. Both finite and infinite horizons are considered. Finally we experimentally evaluate our approach on random MDPs and on a data center control problem.", "creator": "LaTeX with hyperref package"}}}