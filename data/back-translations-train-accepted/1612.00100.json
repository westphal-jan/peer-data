{"id": "1612.00100", "review": {"conference": "NIPS", "VERSION": "v1", "DATE_OF_SUBMISSION": "1-Dec-2016", "title": "Noise-Tolerant Life-Long Matrix Completion via Adaptive Sampling", "abstract": "We study the problem of recovering an incomplete $m\\times n$ matrix of rank $r$ with columns arriving online over time. This is known as the problem of life-long matrix completion, and is widely applied to recommendation system, computer vision, system identification, etc. The challenge is to design provable algorithms tolerant to a large amount of noises, with small sample complexity. In this work, we give algorithms achieving strong guarantee under two realistic noise models. In bounded deterministic noise, an adversary can add any bounded yet unstructured noise to each column. For this problem, we present an algorithm that returns a matrix of a small error, with sample complexity almost as small as the best prior results in the noiseless case. For sparse random noise, where the corrupted columns are sparse and drawn randomly, we give an algorithm that exactly recovers an $\\mu_0$-incoherent matrix by probability at least $1-\\delta$ with sample complexity as small as $O\\left(\\mu_0rn\\log (r/\\delta)\\right)$. This result advances the state-of-the-art work and matches the lower bound in a worst case. We also study the scenario where the hidden matrix lies on a mixture of subspaces and show that the sample complexity can be even smaller. Our proposed algorithms perform well experimentally in both synthetic and real-world datasets.", "histories": [["v1", "Thu, 1 Dec 2016 01:10:07 GMT  (822kb,D)", "http://arxiv.org/abs/1612.00100v1", "24 pages, 5 figures in NIPS 2016"]], "COMMENTS": "24 pages, 5 figures in NIPS 2016", "reviews": [], "SUBJECTS": "cs.LG stat.ML", "authors": ["maria-florina balcan", "hongyang zhang"], "accepted": true, "id": "1612.00100"}, "pdf": {"name": "1612.00100.pdf", "metadata": {"source": "CRF", "title": "Noise-Tolerant Life-Long Matrix Completion via Adaptive Sampling", "authors": ["Maria-Florina Balcan", "Hongyang Zhang"], "emails": ["ninamf@cs.cmu.edu", "hongyanz@cs.cmu.edu"], "sections": [{"heading": "1 Introduction", "text": "In fact, it is the case that we will be able to agree on a solution that is capable, that we need to find a solution, \"he told the German Press Agency.\" We are able to bring about a solution, \"he said,\" but we are not yet able to find a solution. \""}, {"heading": "2 Preliminaries", "text": "Before proceeding, we define some notations and clarify the problem in this paragraph. Notations: We use bold uppercase letters to represent the matrix, bold lowercase letters to represent the vector, and lowercase letters to represent the scalar. Specifically, we designate the noisy observation matrix with M: Rm \u00b7 n and similar to Mt: R1 \u00b7 n in the t-th line. For each set of indications, MVP stands for a subsampling of the M lines at the coordinates. Without confusion, the column space is corrupted by the matrix L. Denote by U: the noisy version of U, i.e. the subspace is corrupted by the noise."}, {"heading": "2.1 Problem Setup", "text": "We assume that each column of the underlying matrix L may restore exactly the noise that we have normalized as Unit 2 and that we can exploit over time. In retrospect, we assume that the underlying matrix is of rank r. This assumption allows us to represent L as L = US, where U is the dictionary (a.k.a. base) of size m \u00d7 r with each column representing a latent metafeature, and S is a matrix of size r, representing the weights of the linear combination for each column L: t The entire subspatial structure is covered by U and the grouping structure, e.g. the mixture of multiple spaces is covered by the column."}, {"heading": "3 Main Results", "text": "In this section, we formalize our lifelong matrix completion algorithm, develop our most important theoretical contributions, and compare our results with previous work."}, {"heading": "3.1 Bounded Deterministic Noise", "text": "To solve the problem, we need to address the question of whether the value of the estimator is greater than a given threshold, which contains the remaining entries of M: t and a few lines of U: t and U. If the value of the estimator is greater than a given threshold, the algorithm becomes the remaining entries of M: t and a few lines of U: t and a few lines of U: t the best approximation of M: t by a linear combination of columns of U: t and U. The pseudo-code of the procedure is represented in algorithm 1. We note that our algorithm is similar to the algorithm of [KS14] for the problem of the offline matrix without noise."}, {"heading": "3.1.1 Recovery Guarantee", "text": "Our analysis leads to the following guarantee for the power of the algorithm 1.Theorem 1 (K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K (K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K"}, {"heading": "3.2 Sparse Random Noise", "text": "In this area we are able to deal with the question of whether we see ourselves in a position to establish ourselves in the world. (...) We assume that the number of people who are moving in the world is very low. (...) We assume that the number of people who are moving in the world is very low. (...) We assume that the number of people who are moving in the world is very high. (...) We assume that the number of people who are moving in the world is very high. (...) We need the number of people who are moving in the world is very high. (...) We assume that the number of people who are moving in the world is very high. (...) We need the number of people who are moving in the world, in the world, in the world, in the world, in the world, in the world, in the world, in the world, in the world, in the world, in the world, in the world in the world, in the world, in the world in the world, in the world, in the world in the world, in the world, in the world, in the world in the world, in the world, in the world in the world, in the world in the world, in the world in the world, in the world in the world, in the world in the world, in the world in the world, in the world in the world, in the world in the world, in the world in the world, in the world, in the world in the world, in the world in the world, in the world in the world, in the world, in the world in the world, in the world in the world, in the world in the world, in the world in the world, in the world, in the world in the world, in the world in the world, in the world in the world, in the world in the world, in the world in the world, in the world in the world, in the world in the world, in the world in the world, in the world in the world, in the world in the world, in the world in the world, in the world in the world in the world, in the world in the world, in the world, in the world in the world in the world in the world, in the world in the world, in the world in the world, in the world in the world, in the world in the"}, {"heading": "3.2.2 Lower Bound", "text": "This lower limit corresponds to our analysis of the upper limit in Section 3.2.1.Theorem 10 (Bottom to Sample Complexity). Let's use 0 < p < p < 1 / 2) for a constant c, then with a probability of at least 1 \u2212 it is the index of the line sample. [m] That is, Ur is 0-incoherent. If the total sampling number dn < c\u00b50rn log (r / 2) for a constant c, then there is an example of M that is under the sampling model of Section 2.1 (i.e.) when a column makes the decisions, either (a) randomly or (b) the entire column."}, {"heading": "4 Experimental Results", "text": "The result shows that empirically estimated errors of our algorithms are actually true: We are the actual trends in the world. We are the imperfect errors of our algorithms. We are the imperfect errors in the world. We are the imperfect errors in the world. We are the underlying matrix L = [u11 T 200], each column of which is normalized to unit 2. Finally, we add unstructured noise in each column, with noise level = 0.6. We randomly select 20% of the entries so as not to be observed. The left figure in Figure 4 shows the comparison between our estimated errors and the true errors by our algorithm."}, {"heading": "5 Conclusions", "text": "In this paper, we examine the lifetime completion of the matrix, which aims to restore a rank r m \u00b7 n matrix online among two realistic noise models - limited deterministic noise and sparse random noise. Our result advances state-of-the-art work and is consistent with the lower limit of sparse random noise. In a more harmless environment, where the columns of the underlying matrix are on a mixture of subspaces, we show that lower sample complexity is possible to accurately restore the target matrix. It would be interesting to extend our results to other realistic noise models, including random classification noise or malignant noise, previously studied in the context of the supervised classification [ABL14, BF13], which was partially supported by NSF grants NSF CCF-1422910, NSF CCF-1535967, NSF CCF Facty-145NSIS 1177-I17714, and a Microsoft Research."}, {"heading": "A Facts on Subspace Spanned by Non-Degenerate Random Vectors", "text": "Lemma 13. Let Es-Rm \u00b7 k be a matrix consisting of corrupt vectors drawn from any non-degenerated distribution. Let Uk-Rm \u00b7 k be a fixed matrix with rank k. Then, with probability, we have 1 \u2022 rank (Es) = s for all s \u2264 m; \u2022 rank ([Es, x] = s + 1 applies to x-Uk-Rm uniformly and s \u2264 m \u2212 k, where x may even depend on It; \u2022 rank ([Es, Uk]) = s + k, provided that s + k \u2264 m; \u2022 The limit of non-degenerated distribution is not degenerated.Evidence. For the sake of simplicity, we show only the proof of fact 1. Let Es = [Es \u2212 1, e]. Since e is drawn from a non-degenerated distribution, the conditional probability fulfills Pr [rank (Es \u2212 1, e) = s \u2212 1] = by the definition of the non-generated (Pr)."}, {"heading": "B Equivalence between Bernoulli and Uniform Models", "text": ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"}, {"heading": "C A Collection of Concentration Results", "text": "Lemma 15 (Theorem 6. [KS14]). Designates by U-k a k-dimensional division in Rm. Assign the sample number d \u2265 max {83k\u00b5 (U-k) (2k\u043c), 4\u00b5 (PU-k-y) log (1). Randomly denote an index set of size d with uniform replacement of [m]. Then with a probability of at least 1 \u2212 4\u043c, for each y-y-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K."}], "references": [{"title": "The power of localization for efficiently learning linear separators with noise", "author": ["Pranjal Awasthi", "Maria Florina Balcan", "Philip M Long"], "venue": "In ACM Symposium on Theory of Computing,", "citeRegEx": "Awasthi et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Awasthi et al\\.", "year": 2014}, {"title": "Efficient representations for life-long learning and autoencoding", "author": ["Maria Florina Balcan", "Avrim Blum", "Santosh Vempala"], "venue": "In Annual Conference on Learning Theory,", "citeRegEx": "Balcan et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Balcan et al\\.", "year": 2015}, {"title": "Statistical active learning algorithms", "author": ["Maria Florina Balcan", "Vitaly Feldman"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Balcan and Feldman.,? \\Q2013\\E", "shortCiteRegEx": "Balcan and Feldman.", "year": 2013}, {"title": "Lambertian reflectance and linear subspaces", "author": ["Ronen Basri", "David W Jacobs"], "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence,", "citeRegEx": "Basri and Jacobs.,? \\Q2003\\E", "shortCiteRegEx": "Basri and Jacobs.", "year": 2003}, {"title": "Online identification and tracking of subspaces from highly incomplete information", "author": ["Laura Balzano", "Robert Nowak", "Benjamin Recht"], "venue": "In Annual Allerton Conference on Communication,", "citeRegEx": "Balzano et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Balzano et al\\.", "year": 2010}, {"title": "High-dimensional matched subspace detection when data are missing", "author": ["Laura Balzano", "Benjamin Recht", "Robert Nowak"], "venue": "In IEEE International Symposium on Information Theory,", "citeRegEx": "Balzano et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Balzano et al\\.", "year": 2010}, {"title": "Toward an architecture for never-ending language learning", "author": ["Andrew Carlson", "Justin Betteridge", "Bryan Kisiel", "Burr Settles", "Estevam R. Hruschka Jr.", "Tom M. Mitchell"], "venue": "In AAAI Conference on Artificial Intelligence,", "citeRegEx": "Carlson et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Carlson et al\\.", "year": 2010}, {"title": "A multibody factorization method for independently moving objects", "author": ["J. Costeira", "T. Kanade"], "venue": "International Journal of Computer Vision,", "citeRegEx": "Costeira and Kanade.,? \\Q1998\\E", "shortCiteRegEx": "Costeira and Kanade.", "year": 1998}, {"title": "Matrix completion with noise", "author": ["Emmanuel J Cand\u00e8s", "Yaniv Plan"], "venue": "Proceedings of the IEEE,", "citeRegEx": "Cand\u00e8s and Plan.,? \\Q2010\\E", "shortCiteRegEx": "Cand\u00e8s and Plan.", "year": 2010}, {"title": "Exact matrix completion via convex optimization", "author": ["E.J. Cand\u00e8s", "B. Recht"], "venue": "Foundations of Computational Mathematics,", "citeRegEx": "Cand\u00e8s and Recht.,? \\Q2009\\E", "shortCiteRegEx": "Cand\u00e8s and Recht.", "year": 2009}, {"title": "Robust uncertainty principles: Exact signal reconstruction from highly incomplete frequency information", "author": ["Emmanuel J Cand\u00e8s", "Justin Romberg", "Terence Tao"], "venue": "IEEE Transactions on Information Theory,", "citeRegEx": "Cand\u00e8s et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Cand\u00e8s et al\\.", "year": 2006}, {"title": "The power of convex relaxation: Near-optimal matrix completion", "author": ["E.J. Cand\u00e8s", "T. Tao"], "venue": "IEEE Transactions on Information Theory,", "citeRegEx": "Cand\u00e8s and Tao.,? \\Q2010\\E", "shortCiteRegEx": "Cand\u00e8s and Tao.", "year": 2010}, {"title": "Online matrix completion through nuclear norm regularisation", "author": ["Charanpal Dhanjal", "Romaric Gaudel", "St\u00e9phan Cl\u00e9mencon"], "venue": "In SIAM International Conference on Data Mining,", "citeRegEx": "Dhanjal et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Dhanjal et al\\.", "year": 2014}, {"title": "The spectral norm error of the na\u00efve Nystr\u00f6m extension", "author": ["Alex Gittens"], "venue": "arXiv preprint arXiv:1110.5305,", "citeRegEx": "Gittens.,? \\Q2011\\E", "shortCiteRegEx": "Gittens.", "year": 2011}, {"title": "How babies think: the science of childhood", "author": ["Alison Gopnik", "Andrew N Meltzoff", "Patricia Katherine Kuhl"], "venue": null, "citeRegEx": "Gopnik et al\\.,? \\Q2001\\E", "shortCiteRegEx": "Gopnik et al\\.", "year": 2001}, {"title": "Tail bounds for all eigenvalues of a sum of random matrices", "author": ["Alex Gittens", "Joel A Tropp"], "venue": "arXiv preprint:", "citeRegEx": "Gittens and Tropp.,? \\Q2011\\E", "shortCiteRegEx": "Gittens and Tropp.", "year": 2011}, {"title": "Metrics and models for handwritten character recognition", "author": ["Trevor Hastie", "Patrice Y Simard"], "venue": "Statistical Science,", "citeRegEx": "Hastie and Simard.,? \\Q1998\\E", "shortCiteRegEx": "Hastie and Simard.", "year": 1998}, {"title": "Low-rank matrix and tensor completion via adaptive sampling", "author": ["Akshay Krishnamurthy", "Aarti Singh"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Krishnamurthy and Singh.,? \\Q2013\\E", "shortCiteRegEx": "Krishnamurthy and Singh.", "year": 2013}, {"title": "On the power of adaptivity in matrix completion and approximation", "author": ["Akshay Krishnamurthy", "Aarti Singh"], "venue": "arXiv preprint arXiv:1407.3619,", "citeRegEx": "Krishnamurthy and Singh.,? \\Q2014\\E", "shortCiteRegEx": "Krishnamurthy and Singh.", "year": 2014}, {"title": "Online completion of ill-conditioned low-rank matrices", "author": ["Ryan Kennedy", "Camillo J Taylor", "Laura Balzano"], "venue": "In IEEE Global Conference on Signal and Information,", "citeRegEx": "Kennedy et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Kennedy et al\\.", "year": 2014}, {"title": "Online matrix completion and online robust PCA", "author": ["Brian Lois", "Namrata Vaswani"], "venue": "In IEEE International Symposium on Information Theory,", "citeRegEx": "Lois and Vaswani.,? \\Q2015\\E", "shortCiteRegEx": "Lois and Vaswani.", "year": 2015}, {"title": "`p-recovery of the most significant subspace among multiple subspaces with outliers", "author": ["Gilad Lerman", "Teng Zhang"], "venue": "Constructive Approximation,", "citeRegEx": "Lerman and Zhang.,? \\Q2014\\E", "shortCiteRegEx": "Lerman and Zhang.", "year": 2014}, {"title": "A simpler approach to matrix completion", "author": ["Benjamin Recht"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Recht.,? \\Q2011\\E", "shortCiteRegEx": "Recht.", "year": 2011}, {"title": "Matched subspace detectors", "author": ["Louis L Scharf", "Benjamin Friedlander"], "venue": "IEEE Transactions on Signal Processing,", "citeRegEx": "Scharf and Friedlander.,? \\Q1994\\E", "shortCiteRegEx": "Scharf and Friedlander.", "year": 1994}, {"title": "Randomized online pca algorithms with regret bounds that are logarithmic in the dimension", "author": ["Manfred K Warmuth", "Dima Kuzmin"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Warmuth and Kuzmin.,? \\Q2008\\E", "shortCiteRegEx": "Warmuth and Kuzmin.", "year": 2008}, {"title": "Robust PCA via outlier pursuit", "author": ["H. Xu", "C. Caramanis", "S. Sanghavi"], "venue": "IEEE Transaction on Information Theory,", "citeRegEx": "Xu et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Xu et al\\.", "year": 2012}, {"title": "Completing low-rank matrices with corrupted samples from few coefficients in general basis", "author": ["Hongyang Zhang", "Zhouchen Lin", "Chao Zhang"], "venue": "IEEE Transactions on Information Theory,", "citeRegEx": "Zhang et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Zhang et al\\.", "year": 2016}, {"title": "Exact recoverability of robust PCA via outlier pursuit with tight recovery bounds", "author": ["H. Zhang", "Z Lin", "C. Zhang", "E. Chang"], "venue": "In AAAI Conference on Artificial Intelligence,", "citeRegEx": "Zhang et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Zhang et al\\.", "year": 2015}, {"title": "Relations among some low rank subspace recovery models", "author": ["Hongyang Zhang", "Zhouchen Lin", "Chao Zhang", "Junbin Gao"], "venue": "Neural Computation,", "citeRegEx": "Zhang et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Zhang et al\\.", "year": 2015}], "referenceMentions": [], "year": 2016, "abstractText": "We study the problem of recovering an incomplete m \u00d7 n matrix of rank r with columns arriving online over time. This is known as the problem of life-long matrix completion, and is widely applied to recommendation system, computer vision, system identification, etc. The challenge is to design provable algorithms tolerant to a large amount of noises, with small sample complexity. In this work, we give algorithms achieving strong guarantee under two realistic noise models. In bounded deterministic noise, an adversary can add any bounded yet unstructured noise to each column. For this problem, we present an algorithm that returns a matrix of a small error, with sample complexity almost as small as the best prior results in the noiseless case. For sparse random noise, where the corrupted columns are sparse and drawn randomly, we give an algorithm that exactly recovers an \u03bc0-incoherent matrix by probability at least 1\u2212 \u03b4 with sample complexity as small asO (\u03bc0rn log(r/\u03b4)). This result advances the state-of-the-art work and matches the lower bound in a worst case. We also study the scenario where the hidden matrix lies on a mixture of subspaces and show that the sample complexity can be even smaller. Our proposed algorithms perform well experimentally in both synthetic and real-world datasets.", "creator": "LaTeX with hyperref package"}}}