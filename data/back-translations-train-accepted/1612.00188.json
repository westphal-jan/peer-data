{"id": "1612.00188", "review": {"conference": "ICML", "VERSION": "v1", "DATE_OF_SUBMISSION": "1-Dec-2016", "title": "Efficient Orthogonal Parametrisation of Recurrent Neural Networks Using Householder Reflections", "abstract": "Recurrent Neural Networks (RNNs) have been successfully used in many applications. However, the problem of learning long-term dependencies in sequences using these networks is still a major challenge. Recent methods have been suggested to solve this problem by constraining the transition matrix to be unitary during training, which ensures that its norm is exactly equal to one. These methods either have limited expressiveness or scale poorly with the size of the network when compared to the simple RNN case, especially in an online learning setting. Our contributions are as follows. We first show that constraining the transition matrix to be unitary is a special case of an orthogonal constraint. Therefore, it may not be necessary to work with complex valued matrices. Then we present a new parametrisation of the transition matrix which allows efficient training of an RNN while ensuring that the transition matrix is always orthogonal. Using our approach, one online gradient step can, in the worst case, be performed in time complexity $\\mathcal{O}(T n^2)$, where $T$ and $n$ are the length of the input sequence and the size of the hidden layer respectively. This time complexity is the same as the simple RNN case. Finally, we test our new parametrisation on problems with long-term dependencies. Our results suggest that the orthogonal constraint on the transition matrix has similar benefits to the unitary constraint.", "histories": [["v1", "Thu, 1 Dec 2016 09:55:10 GMT  (221kb,D)", "http://arxiv.org/abs/1612.00188v1", null], ["v2", "Tue, 6 Dec 2016 12:08:34 GMT  (221kb,D)", "http://arxiv.org/abs/1612.00188v2", "12 pages, 9 figures"], ["v3", "Mon, 12 Dec 2016 14:09:19 GMT  (233kb,D)", "http://arxiv.org/abs/1612.00188v3", "12 pages, 9 figures"], ["v4", "Mon, 6 Mar 2017 12:01:53 GMT  (137kb,D)", "http://arxiv.org/abs/1612.00188v4", "11 pages, 5 figures"], ["v5", "Tue, 13 Jun 2017 07:07:33 GMT  (152kb,D)", "http://arxiv.org/abs/1612.00188v5", "12 pages, 5 figures"]], "reviews": [], "SUBJECTS": "cs.LG", "authors": ["zakaria mhammedi", "andrew d hellicar", "ashfaqur rahman", "james bailey"], "accepted": true, "id": "1612.00188"}, "pdf": {"name": "1612.00188.pdf", "metadata": {"source": "META", "title": "Efficient Orthogonal Parametrisation of Recurrent Neural Networks  Using Householder Reflections", "authors": ["Zakaria Mhammedi", "Andrew Hellicar", "Ashfaqur Rahman"], "emails": ["ZMHAMMEDI@STUDENT.UNIMELB.EDU.AU", "ANDREW.HELLICAR@DATA61.CSIRO.AU", "ASHFAQUR.RAHMAN@DATA61.CSIRO.AU", "JAMES.BAILEY@UNIMELB.EDU.AU"], "sections": [{"heading": null, "text": "\u2020 Department of Computing and Information Systems. \u2021 Data61. Parameterization of problems with long-term dependencies. Our results suggest that the orthogonal constraint of the transition matrix has similar advantages to the uniform constraint."}, {"heading": "1. Introduction", "text": "This year it is more than ever before."}, {"heading": "2. Related Work", "text": "(It is. \"It is.\" It is. \"(It is.\" It is. \"It is.\" \"It is.\" \"It is.\" \"It is.\" \"It is.\" \"It is.\" \"It is.\" \"It is.\" \"It is.\" \"It is.\" \"It is.\" \"It is.\" \"It is.\" \"It is.\" \"It is.\" \"It is.\" (\"It is.\" \"It is.\" It is. \"\" It is. \"It is.\" (\"It is.\" \"It.\" It. \"It.\" It. \"It.\" It. \"It.\" It. \"It.\" It. \"It.\" It. \"(\" It. \"It.\" \"It.\" It. \"It.\" It. \"It.\" It. \"It.\" It. \"It.\" It. \"It.\" It. \"It.\" It. \"It is.\" It. \"It is.\" (\"It.\" It. \"It.\" It. \"It.\" It. \"It.\" It. \"It.\" It. \"It.\" It. \"It.\" It. \"It.\" It. \"It is.\" (\"It.\" It. \"It.\" It is. \") It is.\" (\"It.\" It. \"It.\" It. \"It.\" It is. \"(\" It. \"It.\" It. \"It.\" It. \"It is.\" It. \") It is.\" It is. \"(\" It is. \"(\" It. \"It.\" It. \"It is.\" It. \") It is.\" It is. (\"It is.\" It. \"It is.\") It is. (It is. (\"It is. It is. It.\" It is. It is. It is. \") It is. (It is. (\" It is. It is. \"It is. It is. It is. It is.\") It is. (\"It is. (\" It is. It is. (\"It. It. It.\" It is. \") It is. It is. It is. (\" It is. It is."}, {"heading": "3. Complex unitary versus orthogonal", "text": "It is possible to show that if the transition matrix W-Cn-n of an RNN range is considered a complex transition matrix, there is an equivalent representation of this RNN range (otherwise, there is an equivalent representation of this RNN range with an orthogonal transition matrix W-Cn-n where A-B B B-A-n exists. Furthermore, it is possible to consider the following real and imaginary parts of a complex number (< (ht) = (ht)], V-Cn (V) = (V), W-B-A), where the real and imaginary parts of a complex number are applied."}, {"heading": "4. Parametrisation of the transition matrix", "text": "Before discussing our parameterization, we need to introduce a few notations. \u2022 For n-Q and 2-K-Q \u2264 n, we define the mappingsHk: Rk \u2192 Rn \u00b7 n. \"(10), where we define the k-dimensional identity matrix. (10), where we define the k-dimensional identity matrix: Rn \u00b7 n.\" (11), where there is simply a scalar in this case that is orthogonal. (10), where we define the H1 asH1: R \u2192 Rn \u00b7 n. \"(11), where it is simply a scalar. That is when u.\" (1), H1. \"We suggest the transition matrix of an RNN with the vectors {ui} that we refer to the help."}, {"heading": "4.1. Time complexity", "text": "If we use the parameterization in (12), we do not need to calculate the matrix W explicitly. We only need the matrix vector products Wv. The cost of calculating Wv is proportional to O (nm), where m and n are the number of used reflection vectors or the size of the hidden state, respectively. Consequently, the time complexity of forward propagation (FP) is equal to a sequence of length T O (Tmn). Back Propagation Through Time (BPTT) can be achieved within the same time complexity as FP. Note that with a simple RNN, the complexity of FP and BPTT is equal to O (Tn2) for a sequence of length T. Therefore, when training an RNN using our parameterization, the same time complexity arises as with the simple RNN. In the worst case, the number of reflection vectors is equal to the hidden size of the hidden state."}, {"heading": "4.2. Space complexity", "text": "The parameterization of the transition matrix of an RNN using (12) reflections is equivalent to the addition of m \u2212 1 hidden intermediate states between two successive time steps. For the time step t, these vectors are areht, k = Hn \u2212 m + k (un \u2212 m + k)... Hn \u2212 m + 1 (un \u2212 m + 1) ht, (13) for 1 \u2264 k \u2264 m \u2212 1, using the conventions ht, m = ht + 1.0 = ht + 1.Using an algorithm such as BPTT to calculate the gradients requires the values of the hidden states, including intermediate states, to expand the gradients. A naive approach would be to store the values of ht, k for all t \u2264 T and 1 \u2264 k \u2264 m \u2212 1. This method would increase the memory storage by a factor of m, compared to the case of a simple RNN."}, {"heading": "4.3. Compact WY representation", "text": "In order to make the implementation of our method feasible, we must derive the advanced mathematical expressions of the following progressions in relation to some progressions: \"U.\" \"U.\" \"U.\" \"\" U. \"\" \"U.\" \"\" \"U.\" \".\" \"U.\" \".\" \"\" U. \"\". \"\" \".\" \"\". \"\" \".\" \"\" \".\" \"\". \"\" \"\". \"\" \"\". \"\" \"\". \"\" \"\".. \"\" \"..\" \"\" \"..\" \"\" \"\".. \"\" \"..\" \"\" \"..\" \"\".. \"\". \"\" \"\".. \"\" \"\" \"..\" \"\" \"\".. \"\" \"\" \"..\" \"\" \""}, {"heading": "5. Experiments", "text": "We implemented and tested our method with the Python library theano (Theano Development Team, 2016). We implemented a tensor operator F, which takes a matrix W and a vector h and returns the product Wh. To enable automatic differentiation, we also implemented a gradient method associated with the operator F, which calculates the gradients in equations 19 and 20 using algorithms 2, 3 and 4."}, {"heading": "5.1. Scalability", "text": "In this first experiment, we tested the scalability of our method by measuring the time required for a gradient calculation. (The input consisted of a randomly generated 2d sequence. The target quantity was a fixed scalar value at the end of the sequence. We compared our method with a na\u00efve approach in which the orthogonal matrix is generated explicitly before the FP using the compact representation W = I \u2212 UT \u2212 1UT. Note that although the matrix T is triangular and its inversion can be calculated in the time complexity O (m2), the generation of W would still require O (nm2) complexity due to the matrix products involved. Our method, on the other hand, does not require explicit compilation of W. We considered the following situations: \u2022 We fixed the length T = 10 of the input sequence and varied the number of hidden units in the RNN in the {1212, 105b} number we compared with (105b)."}, {"heading": "5.2. Application to problems with long-term dependencies", "text": "We tested the new parameterization on the three different datasets, which all have long-term dependencies. We refer to the RNN parameterization using household reflection as oRNN. We set its activation function to the leaky Rectified Linear Unit (Relu), which defines as\u03c6 (x) = {x, if x > 00,1, otherwise (21) For all experiments we used the adam method for stochastic gradient descent. We tested different learning rates in the quantity {10 \u2212 1, 10 \u2212 2, 10 \u2212 3, 10 \u2212 4} for each model and selected those which resulted in the fastest convergence of the training sets. We initialized all parameters similarly based on uniform distributions (Arjovsky et al., 2015). The distortions of all models were set to zero, with the exception of the forgetfulness distortion of the LSTM, which we set to 5 in order to facilitate learning long-term dependencies according to Krithox."}, {"heading": "5.2.1. SEQUENCE GENERATION", "text": "In this experiment, we used the title Manyrista from Cuprum's album Musica Deposita. We extracted five consecutive extracts around the beginning of the song with 800 data points each, corresponding to 18ms at a sampling rate of 44.1Hz. We trained one sRNN, LSTM, and oRNN for 5000 epochs on each of the pieces with five random seeds. For each run, the lowest normalized Mean Squared Error (NMSE) was recorded during the 5000 epochs. For each model, we tested three different hidden sizes. The total number of parameters corresponding to these hidden sizes was approximately 250, 500, and 1000. For the oRNN, we selected the number of reflection vectors to match the hidden sizes for each case, so that the transition matrix resulted in better results in the group than the complete results in the group 0.0M."}, {"heading": "5.2.2. PIXEL MNIST", "text": "We used the MNIST image dataset in this experiment. We divided the dataset into tension (55,000 instances), validation (5,000 instances) and test set (10,000 instances). To minimize cross entropy, we trained an oRNN with n = 128 hidden units and m = 16 reflections, corresponding to approximately 3,600 parameters. We set the batch size to 64 and selected the best model based on the lowest validation costs after 20 epochs. The latter was evaluated for all 20 training processes. All learning rates in this experiment were set at 0.001. Table 5.2.2 compares our test performance with the uRNN results available in the literature. The models used for the comparison all have approximately 16K parameters, which is more than four times the number of parameters of our tested model. Nevertheless, our model still achieved a test accuracy of 95.6%, which is the second best result."}, {"heading": "5.2.3. ADDING TASK", "text": "In this experiment, we follow a setting similar to (Arjovsky et al., 2015), where the goal of the RNN is to output the sum of two elements in the first dimension of a 2d sequence. The position of the two elements to be summed is determined by the entries in the second dimension of the input sequence. Specifically, the first dimension of each input sequence consists of random numbers between 0 and 1. The second dimension has all zeros except two elements that are equal to 1. The first entry is in the first half of the sequence, and the second in the second half. We tested two different delays between T = 400 and T = 800. All models were trained to minimize the Mean Squared Error (MSE). The baseline MSE for this task corresponds to 0.167 of a model that is always 1, We trained an oRNN with n = 128 hidden units and m = 16 reflections similar to the MNIST experiment."}, {"heading": "6. Discussion", "text": "In this paper, we presented a new efficient method for parameterizing transition matrices of a recurrent neural network using householder reflections, which provides a simple and computationally efficient method for enforcing an orthogonal constraint of the transition matrix that ensures that no exploding gradients occur during training. We also demonstrated that enforcing a uniform constraint of the transition matrix is a special case of orthogonal parameterization. Experimental results show that orthogonal parameterization has similar advantages to unitary parameterization."}, {"heading": "Acknowledgment", "text": "The authors would like to acknowledge the Department of State Growth Tasmania for funding this work through SenseT."}, {"heading": "A. Proof of Theorem 2", "text": "(D) T (D) T (D) T (D) T (D) T (D) T (D) T (D) T (D) T (D) T (D) T (D) T (D) T (D) T (D) T (D) T (D) T (D) T (D) T (D) T (D) T (D) T (D) T (D) T (D) T (D) T (D) T (D) T (D) T (D) T (D) T (D) T (D) T (D) T (D) T (D) T (D) T (D) T (D) T (D) T (D) T (D) T (D) T (D) T (D) T (D) T (D) T (D) T (D) T (D) T (D) T (D) T (D) T (D) T (D) T (D) T (D) T (D) T (D) T (D) T (D) T (D) T (D) T (D) T (D) T (D) T (D) T (D) T (D) T (D) T (D) T (D) T (D) T (D) T (D) T (D) T (D) T (D) T (D (D) T (D) T (D) T (D) T (D) T (D (D) T (D) T (D) T (D) T (D) T (D) T (D) T (D) T (D) T (D) T (D) T (D) T (D) T (D) T (D) T (D) T (D) T (D) T (D) T (D) T (D) T (D) T) T (D) T (D) T (D) T (D) T) T (D) T (D) T (D) T (D) T (D) T (D) T) T (D) T (D) T (D) T (D) T (D) T (D) D) T (D) T (D) T (D) T (D"}], "references": [{"title": "Unitary evolution recurrent neural networks", "author": ["Arjovsky", "Martin", "Shah", "Amar", "Bengio", "Yoshua"], "venue": "arXiv preprint arXiv:1511.06464,", "citeRegEx": "Arjovsky et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Arjovsky et al\\.", "year": 2015}, {"title": "An extended collection of matrix derivative results for forward and reverse mode automatic differentiation", "author": ["Giles", "Mike B"], "venue": null, "citeRegEx": "Giles and B.,? \\Q2008\\E", "shortCiteRegEx": "Giles and B.", "year": 2008}, {"title": "Orthogonal rnns and long-memory tasks", "author": ["Henaff", "Mikael", "Szlam", "Arthur", "LeCun", "Yann"], "venue": "arXiv preprint arXiv:1602.06662,", "citeRegEx": "Henaff et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Henaff et al\\.", "year": 2016}, {"title": "Gradient flow in recurrent nets: the difficulty of learning", "author": ["Hochreiter", "Sepp", "Bengio", "Yoshua", "Frasconi", "Paolo", "Schmidhuber", "J\u00fcrgen"], "venue": "long-term dependencies,", "citeRegEx": "Hochreiter et al\\.,? \\Q2001\\E", "shortCiteRegEx": "Hochreiter et al\\.", "year": 2001}, {"title": "Learning unitary operators with help from u (n)", "author": ["Hyland", "Stephanie L", "R\u00e4tsch", "Gunnar"], "venue": "arXiv preprint arXiv:1607.04903,", "citeRegEx": "Hyland et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Hyland et al\\.", "year": 2016}, {"title": "Improving training of deep neural networks via singular value bounding", "author": ["Jia", "Kui"], "venue": "arXiv preprint arXiv:1611.06013,", "citeRegEx": "Jia and Kui.,? \\Q2016\\E", "shortCiteRegEx": "Jia and Kui.", "year": 2016}, {"title": "Accumulating householder transformations, revisited", "author": ["Joffrain", "Thierry", "Low", "Tze Meng", "Quintana-Ort\u0131", "Enrique S", "Geijn", "Robert van de", "Zee", "Field G Van"], "venue": "ACM Transactions on Mathematical Software (TOMS),", "citeRegEx": "Joffrain et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Joffrain et al\\.", "year": 2006}, {"title": "A clockwork RNN", "author": ["Koutn\u0131\u0301k", "Jan", "Greff", "Klaus", "Gomez", "Faustino J", "Schmidhuber", "J\u00fcrgen"], "venue": "CoRR, abs/1402.3511,", "citeRegEx": "Koutn\u0131\u0301k et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Koutn\u0131\u0301k et al\\.", "year": 2014}, {"title": "A simple way to initialize recurrent networks of rectified linear units", "author": ["Le", "Quoc V", "Jaitly", "Navdeep", "Hinton", "Geoffrey E"], "venue": "arXiv preprint arXiv:1504.00941,", "citeRegEx": "Le et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Le et al\\.", "year": 2015}, {"title": "On the difficulty of training recurrent neural networks", "author": ["Pascanu", "Razvan", "Mikolov", "Tomas", "Bengio", "Yoshua"], "venue": "ICML (3),", "citeRegEx": "Pascanu et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Pascanu et al\\.", "year": 2013}, {"title": "Full-capacity unitary recurrent neural networks", "author": ["Wisdom", "Scott", "Powers", "Thomas", "Hershey", "John R", "Roux", "Jonathan Le", "Atlas", "Les"], "venue": "arXiv preprint arXiv:1611.00035,", "citeRegEx": "Wisdom et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Wisdom et al\\.", "year": 2016}], "referenceMentions": [{"referenceID": 3, "context": "In practice, however, training simple RNNs can be challenging due to the problem of exploding and vanishing gradients (Hochreiter et al., 2001).", "startOffset": 118, "endOffset": 143}, {"referenceID": 9, "context": "This results in an error surface, associated with some objective function, having very steep walls (Pascanu et al., 2013).", "startOffset": 99, "endOffset": 121}, {"referenceID": 9, "context": "One approach to solving this issue is to clip the gradients (Pascanu et al., 2013) when their norm exceeds some threshold value.", "startOffset": 60, "endOffset": 82}, {"referenceID": 0, "context": "In particular, the unitary RNN (Arjovsky et al., 2015) constrains the transition matrix to be in the unitary group U(n) using a special parametrisation.", "startOffset": 31, "endOffset": 54}, {"referenceID": 10, "context": "This parametrisation and other similar ones (Hyland & R\u00e4tsch, 2016; Wisdom et al., 2016) have some advantages and drawbacks which we will discuss in more details in the next section.", "startOffset": 44, "endOffset": 88}, {"referenceID": 8, "context": "In particular, initialising with specific structures such as the identity or an orthogonal matrix can greatly improve performance (Le et al., 2015).", "startOffset": 130, "endOffset": 147}, {"referenceID": 2, "context": "In addition to these initialisation methods, one study has also considered removing the non-linearity between the hidden-to-hidden connections (Henaff et al., 2016), i.", "startOffset": 143, "endOffset": 164}, {"referenceID": 10, "context": "It is possible to constrain the transition matrix to be orthogonal during training using more sophisticated methods (Wisdom et al., 2016).", "startOffset": 116, "endOffset": 137}, {"referenceID": 0, "context": "Recently, new parametrisations of the transition matrix have been suggested (Arjovsky et al., 2015), which ensure that its spectral norm is always equal to one.", "startOffset": 76, "endOffset": 99}, {"referenceID": 10, "context": "However, it has been shown that this parametrisation does not allow the transition matrix to span the full unitary group (Wisdom et al., 2016), which may limit the model expressiveness.", "startOffset": 121, "endOffset": 142}, {"referenceID": 0, "context": "This is one advantage over the original unitary parametrisation (Arjovsky et al., 2015).", "startOffset": 64, "endOffset": 87}, {"referenceID": 10, "context": "A more recent method (Wisdom et al., 2016) performs optimisation directly of the Stiefel manifold.", "startOffset": 21, "endOffset": 42}, {"referenceID": 0, "context": "Take for example the activation function proposed by (Arjovsky et al., 2015)", "startOffset": 53, "endOffset": 76}, {"referenceID": 0, "context": "Methods Constraint on the Time complexity of one Search space of the transition matrix online gradient step transition matrix uRNN (Arjovsky et al., 2015) \u2016W\u2016 = 1 O(Tn log(n)) A subset of U(n)", "startOffset": 131, "endOffset": 154}, {"referenceID": 10, "context": "Full-capacity uRNN \u2016W\u2016 = 1 O(Tn + n) The full U(n) group (Wisdom et al., 2016) Full-capacity uRNN \u2016W\u2016 = 1 O(Tn + n) The full U(n) group (Hyland & R\u00e4tsch, 2016) oRNN \u2016W\u2016 = 1 O(Tnm) The full O(n) group (Our approach) where m \u2264 n when m = n", "startOffset": 57, "endOffset": 78}, {"referenceID": 6, "context": "In order to write the mathematical expressions of the gradients in Equations (14) and (15), we will use the compact WY representation (Joffrain et al., 2006) of the product of Householder Reflections, which is described by the following proposition.", "startOffset": 134, "endOffset": 157}, {"referenceID": 6, "context": "(Joffrain et al., 2006) Let n \u2208 N and m \u2264 n \u2212 1.", "startOffset": 0, "endOffset": 23}, {"referenceID": 0, "context": "We initialised all the parameters using uniform distributions similar to (Arjovsky et al., 2015).", "startOffset": 73, "endOffset": 96}, {"referenceID": 7, "context": "The biases of all model were set to zero, except for the forget bias of the LSTM, which we set to 5 in order to facilitate the learning of long-term dependencies (Koutn\u0131\u0301k et al., 2014).", "startOffset": 162, "endOffset": 185}, {"referenceID": 7, "context": "In this experiment, we followed a similar setting to (Koutn\u0131\u0301k et al., 2014) where we trained RNNs to encode song excerpts.", "startOffset": 53, "endOffset": 76}, {"referenceID": 0, "context": "6 % uRNN (Arjovsky et al., 2015) 512 ' 16K - 95.", "startOffset": 9, "endOffset": 32}, {"referenceID": 0, "context": "In this experiment, we follow a similar setting to (Arjovsky et al., 2015), where the goal of the RNN is to output the sum of two elements in the first dimension of a 2d sequence.", "startOffset": 51, "endOffset": 74}, {"referenceID": 0, "context": "This is in line with the results of the unitary RNN (Arjovsky et al., 2015).", "startOffset": 52, "endOffset": 75}], "year": 2016, "abstractText": "Recurrent Neural Networks (RNNs) have been successfully used in many applications. However, the problem of learning long-term dependencies in sequences using these networks is still a major challenge. Recent methods have been suggested to solve this problem by constraining the transition matrix to be unitary during training, which ensures that its norm is exactly equal to one. These methods either have limited expressiveness or scale poorly with the size of the network when compared to the simple RNN case, especially in an online learning setting. Our contributions are as follows. We first show that constraining the transition matrix to be unitary is a special case of an orthogonal constraint. Therefore, it may not be necessary to work with complex valued matrices. Then we present a new parametrisation of the transition matrix which allows efficient training of an RNN while insuring that the transition matrix is always orthogonal. Using our approach, one online gradient step can, in the worst case, be performed in time complexity O(Tn), where T and n are the length of the input sequence and the size of the hidden layer respectively. This time complexity is the same as the simple RNN case. Finally, we test our new \u2020The Department of Computing and Information Systems. \u2021Data61. parametrisation on problems with long-term dependencies. Our results suggest that the orthogonal constraint on the transition matrix has similar benefits to the unitary constraint.", "creator": "LaTeX with hyperref package"}}}