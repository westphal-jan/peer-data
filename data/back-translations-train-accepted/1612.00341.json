{"id": "1612.00341", "review": {"conference": "iclr", "VERSION": "v1", "DATE_OF_SUBMISSION": "1-Dec-2016", "title": "A Compositional Object-Based Approach to Learning Physical Dynamics", "abstract": "We present the Neural Physics Engine (NPE), an object-based neural network architecture for learning predictive models of intuitive physics. We propose a factorization of a physical scene into composable object-based representations and also the NPE architecture whose compositional structure factorizes object dynamics into pairwise interactions. Our approach draws on the strengths of both symbolic and neural approaches: like a symbolic physics engine, the NPE is endowed with generic notions of objects and their interactions, but as a neural network it can also be trained via stochastic gradient descent to adapt to specific object properties and dynamics of different worlds. We evaluate the efficacy of our approach on simple rigid body dynamics in two-dimensional worlds. By comparing to less structured architectures, we show that our model's compositional representation of the structure in physical interactions improves its ability to predict movement, generalize to different numbers of objects, and infer latent properties of objects such as mass.", "histories": [["v1", "Thu, 1 Dec 2016 16:39:04 GMT  (7857kb,D)", "http://arxiv.org/abs/1612.00341v1", "Under review as a conference paper for ICLR 2017. 11 pages, 5 figures"], ["v2", "Sat, 4 Mar 2017 17:44:06 GMT  (6899kb,D)", "http://arxiv.org/abs/1612.00341v2", "Published as a conference paper for ICLR 2017. 15 pages, 6 figures"]], "COMMENTS": "Under review as a conference paper for ICLR 2017. 11 pages, 5 figures", "reviews": [], "SUBJECTS": "cs.AI cs.LG", "authors": ["michael b chang", "tomer ullman", "antonio torralba", "joshua b tenenbaum"], "accepted": true, "id": "1612.00341"}, "pdf": {"name": "1612.00341.pdf", "metadata": {"source": "CRF", "title": "LEARNING PHYSICAL DYNAMICS", "authors": ["Michael B. Chang", "Tomer Ullman", "Antonio Torralba", "Joshua B. Tenenbaum"], "emails": ["mbchang@mit.edu", "tomeru@mit.edu", "torralba@mit.edu", "jbt@mit.edu"], "sections": [{"heading": "1 INTRODUCTION", "text": "In fact, most people who are able to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to dance, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to dance, to dance, to"}, {"heading": "2 APPROACH", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "2.1 STATE SPACE", "text": "In our factorization of the scene, we make two observations (Fig. 1): First, because physics does not change across inertial systems, it is sufficient to separately predict the future state of each object depending on the past states of itself and the other objects in its neighbourhood, similar to Fragkiadaki et al. (2015). Second, because physics is Markovian, this prediction only has to apply to the next time step. Therefore, we choose an object-based state representation; a state vector is composed of extrinsic properties (position, velocity, orientation, angular velocity), intrinsic properties (mass, object type, object size) and global properties (gravitational, friction and pair forces) at a given time."}, {"heading": "2.2 NEURAL PHYSICS ENGINE", "text": "In fact, the fact is that most of them are able to move to another world in which they are able to find themselves."}, {"heading": "2.3 BASELINES", "text": "The purpose of comparing the NPE with the following two baselines is to illustrate the benefits of the pairwise factorization and functional composition, which are the most important architectural features of the NPE. Since the architectures for both baselines have shown that they work well in similar tasks, it is not immediately clear whether the assumptions of the NPE are useful or necessary, so that these are good baselines to compare with. Comparison with these baselines is a lesion study of the NPE, because each baseline misses an aspect of the NPE structure. No-Pairwise The No-Pairwise (NP) baseline is summarized by Fig."}, {"heading": "2.4 IMPLEMENTATION", "text": "We trained all models using the rmsprop Tieleman and Hinton (2012) back propagation algorithm with a Euclidean loss for 1,200,000 iterations with a learning rate of 0.0003 and a learning rate decay rate of 0.99 per 2,500 training iterations, starting at iteration 50,000. We used size 50 minibatches and used a 70-15-15 split for training, validation and test data. All models are implemented with the neural network libraries created by Collobert et al.; Le \u0301 onard et al.. The NPE encoder consists of a pairwise layer of 25 hidden units and a 5-layer feedback network of 50 hidden units per layer, each with reflected linear activations. As we use a binary mask to exclude non-adjacent objects, we implement the encoder layers without bias so that adjacent objects do not contribute to the network activation."}, {"heading": "3 EXPERIMENTS", "text": "Using the matter-js-physics engine, we evaluate the NPE on worlds of balls and obstacles. These worlds have a self-evident dynamics and support a variety of scenarios that reflect everyday physics. Jump balls were also of interest in cognitive science to study causality and counterfactual thinking, as in (Gerstenberg et al., 2012). We trained on 3-time frame windows in trajectories of 60 time frames (10 time frames). For a world of k objects, we generate 50,000 such trajectories. For experiments where we train on multiple worlds together, we mix the examples in all training worlds and train without a timetable. All worlds have a vertical dimension of 600 pixels and a horizontal dimension of 800 pixels, and we limit the maximum speed of an object to 60 pixels / second."}, {"heading": "3.1 PREDICTION", "text": "First, we look at simple worlds with four spheres of uniform mass (Fig. 3a). To measure performance in the simulation, we visualize the cosinal similarity between the predicted velocity and the basic truth velocity, as well as the relative error in the order of magnitude between the predicted velocity and the basic truth velocity over a period of 50 simulation phases (about 5 seconds). The models take the periods 1 and 2 as input and then use earlier predictions as input for future predictions. To measure progress through training, we also show the Euclidean loss of the normalized velocity."}, {"heading": "3.2 GENERALIZATION AND KNOWLEDGE TRANSFER", "text": "The invisible worlds (6, 7, 8 balls) in the test data are combinatively more complex and diverse than the observed worlds (3, 4, 5 balls) in the training data. All objects have the same mass. NPE's predictions are more consistent, while NP and LSTM's predictions start to diverge wildly towards the end of 50 simulation points (Fig. 3b, middle row).The NPE's performance in this extrapolation task suggests that its architectonically inductive distortions are useful for generalizing knowledge learned in Markovian domains with causal structure in object interactions. The next paragraph illustrates another aspect of the strong generalization capability of NPE."}, {"heading": "3.3 MASS INFERENCE", "text": "We now show that the NPE can derive latent properties such as mass. This proposal is motivated by the experiments in Battaglia et al. (2013), which use a probabilistic physics simulator to derive various properties of a scene configuration. While the physical rules of its simulator were set manually in advance, the NPE learns these rules from observation. We train on the same worlds used in both the prediction and generalization task, but we sample the mass for each ball from the log spaced set {1, 5, 25}. We decided to use discretely weighted masses to simplify our qualitative understanding of the model's ability to deduce from it. For future work, we would like to continuously examine weighted masses and evaluate them with binary comparisons (e.g. \"Which one is heavier?\"). As summarized by the Nig. 3c and Fig. 4a, we select scenarios from the Focus Objects, which we focus on the object we point with the truth, the object we focus."}, {"heading": "3.4 DIFFERENT SCENE CONFIGURATIONS", "text": "We demonstrate the presentation of large structures as a composition of smaller objects as building blocks in a world of balls and obstacles. These worlds contain 2 balls that jump around in variations of 4 different wall geometries. \"O\" and \"L\" geometries have no internal obstacles and are in the shape of a rectangle or \"L.\" \"U\" and \"I\" have internal obstacles. Obstacles in \"U\" are fastened linearly to the wall like a protrusion, while obstacles in \"I\" have no constraints in position. We randomly vary the position and orientation of the \"L\" cavities and the \"U\" protrusion. We randomly sample the positions of the \"I\" internal obstacle. We train on the conceptually simpler \"O\" and \"L\" worlds and test the more complex \"U\" and \"I\" worlds. Variations in wall geometry contribute to the difficulty of this exhibition task."}, {"heading": "4 RELATION TO PREVIOUS WORK", "text": "In fact, most of them will be able to play by the rules they have imposed on themselves."}, {"heading": "5 DISCUSSION", "text": "We hope that these contributions can trigger further research that builds on themes of factorization and composition that this paper proposes. We have presented a factorization of a physical scene into composable object-based representations, as well as a model architecture whose compositional structure factorizes object dynamics in pairs of interactions. We have applied the NPE to simple two-dimensional worlds that, in their complexity, can demonstrate the usefulness of these two levels of factorization and composition. We have shown that the NPE achieves low predictive errors, extrapolates learned physical knowledge to previously unseen numbers of objects and world configurations, and deduces latent properties such as mass. Further work includes generalizing beyond object counting to new object types and physical laws, such as in worlds with stacked block towers and liquids. Our results also invite questions about how much prior information and entangled structures can be used to construct physical systems, as well as to build physical systems without physical ones."}, {"heading": "ACKNOWLEDGMENTS", "text": "We thank Tejas Kulkarni for insightful discussions and advice. We thank Ilker Yidirim, Erin Reynolds, Feras Saad, Andreas Stuhlmu Mueller, Adam Lerer, Chelsea Finn, Jiajun Wu and the anonymous critics for valuable feedback. We thank Liam Brummit, Kevin Kwok and Guillermo Webster for their help with factual questions. M. Chang was kindly supported by MIT's SuperUROP and UROP programs."}], "references": [{"title": "Learning to poke by poking: Experiential learning of intuitive physics", "author": ["P. Agrawal", "A. Nair", "P. Abbeel", "J. Malik", "S. Levine"], "venue": "arXiv preprint arXiv:1606.07419,", "citeRegEx": "Agrawal et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Agrawal et al\\.", "year": 2016}, {"title": "Social lstm: Human trajectory prediction in crowded spaces. 2016", "author": ["A. Alahi", "K. Goel", "V. Ramanathan", "A. Robicquet", "L. Fei-Fei", "S. Savarese"], "venue": null, "citeRegEx": "Alahi et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Alahi et al\\.", "year": 2016}, {"title": "Cognitive psychology and its implications", "author": ["J.R. Anderson"], "venue": "WH Freeman/Times Books/Henry Holt & Co,", "citeRegEx": "Anderson.,? \\Q1990\\E", "shortCiteRegEx": "Anderson.", "year": 1990}, {"title": "Humans predict liquid dynamics using probabilistic simulation", "author": ["C.J. Bates", "I. Yildirim", "J.B. Tenenbaum", "P.W. Battaglia"], "venue": null, "citeRegEx": "Bates et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Bates et al\\.", "year": 2015}, {"title": "Interaction networks for learning about objects, relations and physics", "author": ["P. Battaglia", "R. Pascanu", "M. Lai", "D. Jimenez Rezende", "K. Koray"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Battaglia et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Battaglia et al\\.", "year": 2016}, {"title": "Simulation as an engine of physical scene understanding", "author": ["P.W. Battaglia", "J.B. Hamrick", "J.B. Tenenbaum"], "venue": "Proceedings of the National Academy of Sciences,", "citeRegEx": "Battaglia et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Battaglia et al\\.", "year": 2013}, {"title": "Torch7: A matlab-like environment for machine learning", "author": ["R. Collobert", "K. Kavukcuoglu", "C. Farabet"], "venue": "In BigLearn, NIPS Workshop, number EPFL-CONF-192376,", "citeRegEx": "Collobert et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Collobert et al\\.", "year": 2011}, {"title": "Attend, infer, repeat: Fast scene understanding with generative models", "author": ["S. Eslami", "N. Heess", "T. Weber", "Y. Tassa", "K. Kavukcuoglu", "G.E. Hinton"], "venue": "arXiv preprint arXiv:1603.08575,", "citeRegEx": "Eslami et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Eslami et al\\.", "year": 2016}, {"title": "Unsupervised learning for physical interaction through video prediction", "author": ["C. Finn", "I. Goodfellow", "S. Levine"], "venue": "arXiv preprint arXiv:1605.07157,", "citeRegEx": "Finn et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Finn et al\\.", "year": 2016}, {"title": "Learning visual predictive models of physics for playing billiards", "author": ["K. Fragkiadaki", "P. Agrawal", "S. Levine", "J. Malik"], "venue": "arXiv preprint arXiv:1511.07404,", "citeRegEx": "Fragkiadaki et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Fragkiadaki et al\\.", "year": 2015}, {"title": "Noisy newtons: Unifying process and dependency accounts of causal attribution", "author": ["T. Gerstenberg", "N. Goodman", "D.A. Lagnado", "J.B. Tenenbaum"], "venue": "In In proceedings of the 34th. Citeseer,", "citeRegEx": "Gerstenberg et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Gerstenberg et al\\.", "year": 2012}, {"title": "Probabilistic models of cognition, 2016", "author": ["N.D. Goodman", "J.B. Tenenbaum"], "venue": "URL http: //probmods.org", "citeRegEx": "Goodman and Tenenbaum.,? \\Q2016\\E", "shortCiteRegEx": "Goodman and Tenenbaum.", "year": 2016}, {"title": "Transforming auto-encoders", "author": ["G.E. Hinton", "A. Krizhevsky", "S.D. Wang"], "venue": "In Artificial Neural Networks and Machine Learning\u2013ICANN", "citeRegEx": "Hinton et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Hinton et al\\.", "year": 2011}, {"title": "Long short-term memory", "author": ["S. Hochreiter", "J. Schmidhuber"], "venue": "Neural computation,", "citeRegEx": "Hochreiter and Schmidhuber.,? \\Q1997\\E", "shortCiteRegEx": "Hochreiter and Schmidhuber.", "year": 1997}, {"title": "Spatial transformer networks", "author": ["M. Jaderberg", "K. Simonyan", "A. Zisserman"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Jaderberg et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Jaderberg et al\\.", "year": 2015}, {"title": "Picture: A probabilistic programming language for scene perception", "author": ["T.D. Kulkarni", "P. Kohli", "J.B. Tenenbaum", "V. Mansinghka"], "venue": "In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition,", "citeRegEx": "Kulkarni et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Kulkarni et al\\.", "year": 2015}, {"title": "Building machines that learn and think like people", "author": ["B.M. Lake", "T.D. Ullman", "J.B. Tenenbaum", "S.J. Gershman"], "venue": "arXiv preprint arXiv:1604.00289,", "citeRegEx": "Lake et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Lake et al\\.", "year": 2016}, {"title": "rnn: Recurrent library for torch", "author": ["N. L\u00e9onard", "S. Waghmare", "Y. Wang"], "venue": "arXiv preprint arXiv:1511.07889,", "citeRegEx": "L\u00e9onard et al\\.,? \\Q2015\\E", "shortCiteRegEx": "L\u00e9onard et al\\.", "year": 2015}, {"title": "Learning physical intuition of block towers by example", "author": ["A. Lerer", "S. Gross", "R. Fergus", "J. Malik"], "venue": "arXiv preprint arXiv:1603.01312,", "citeRegEx": "Lerer et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Lerer et al\\.", "year": 2016}, {"title": "To fall or not to fall: A visual approach to physical stability prediction", "author": ["W. Li", "S. Azimi", "A. Leonardis", "M. Fritz"], "venue": "arXiv preprint arXiv:1604.00066,", "citeRegEx": "Li et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Li et al\\.", "year": 2016}, {"title": "Newtonian image understanding: Unfolding the dynamics of objects in static images", "author": ["R. Mottaghi", "H. Bagherinezhad", "M. Rastegari", "A. Farhadi"], "venue": "arXiv preprint arXiv:1511.04048,", "citeRegEx": "Mottaghi et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Mottaghi et al\\.", "year": 2015}, {"title": "if...\u201d learning to predict the effect of forces in images", "author": ["R. Mottaghi", "M. Rastegari", "A. Gupta", "A. Farhadi"], "venue": "arXiv preprint arXiv:1603.05600,", "citeRegEx": "Mottaghi et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Mottaghi et al\\.", "year": 2016}, {"title": "Dynamics of target selection in multiple object tracking (mot)", "author": ["Z.W. Pylyshyn", "V. Annan"], "venue": "Spatial vision,", "citeRegEx": "Pylyshyn and Annan.,? \\Q2006\\E", "shortCiteRegEx": "Pylyshyn and Annan.", "year": 2006}, {"title": "Program synthesis by sketching", "author": ["A. Solar-Lezama"], "venue": "ProQuest,", "citeRegEx": "Solar.Lezama.,? \\Q2008\\E", "shortCiteRegEx": "Solar.Lezama.", "year": 2008}, {"title": "Unsupervised learning of video representations using lstms", "author": ["N. Srivastava", "E. Mansimov", "R. Salakhutdinov"], "venue": null, "citeRegEx": "Srivastava et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Srivastava et al\\.", "year": 2015}, {"title": "The recurrent temporal restricted boltzmann machine", "author": ["I. Sutskever", "G.E. Hinton", "G.W. Taylor"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Sutskever et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Sutskever et al\\.", "year": 2009}, {"title": "How to grow a mind: Statistics, structure, and abstraction", "author": ["J.B. Tenenbaum", "C. Kemp", "T.L. Griffiths", "N.D. Goodman"], "venue": null, "citeRegEx": "Tenenbaum et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Tenenbaum et al\\.", "year": 2011}, {"title": "Lecture 6.5\u2014RmsProp: Divide the gradient by a running average of its recent magnitude", "author": ["T. Tieleman", "G. Hinton"], "venue": "COURSERA: Neural Networks for Machine Learning,", "citeRegEx": "Tieleman and Hinton.,? \\Q2012\\E", "shortCiteRegEx": "Tieleman and Hinton.", "year": 2012}, {"title": "Learning physics from dynamical scenes", "author": ["T. Ullman", "A. Stuhlm\u00fcller", "N. Goodman"], "venue": null, "citeRegEx": "Ullman et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Ullman et al\\.", "year": 2014}, {"title": "Galileo: Perceiving physical object properties by integrating a physics engine with deep learning", "author": ["J. Wu", "I. Yildirim", "J.J. Lim", "B. Freeman", "J. Tenenbaum"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Wu et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Wu et al\\.", "year": 2015}], "referenceMentions": [{"referenceID": 16, "context": "Physical reasoning is a crucial part of learning, perception, planning, inference, and understanding in artificial intelligence, and can be leveraged to accelerate the learning of new tasks (Lake et al., 2016).", "startOffset": 190, "endOffset": 209}, {"referenceID": 7, "context": "Accurately modeling a scene involves reasoning about the spatial properties, identities and locations of objects (Eslami et al., 2016; Hinton et al., 2011; Jaderberg et al., 2015; Kulkarni et al., 2015), but also their physical properties, future dynamics, and causal relationships.", "startOffset": 113, "endOffset": 202}, {"referenceID": 12, "context": "Accurately modeling a scene involves reasoning about the spatial properties, identities and locations of objects (Eslami et al., 2016; Hinton et al., 2011; Jaderberg et al., 2015; Kulkarni et al., 2015), but also their physical properties, future dynamics, and causal relationships.", "startOffset": 113, "endOffset": 202}, {"referenceID": 14, "context": "Accurately modeling a scene involves reasoning about the spatial properties, identities and locations of objects (Eslami et al., 2016; Hinton et al., 2011; Jaderberg et al., 2015; Kulkarni et al., 2015), but also their physical properties, future dynamics, and causal relationships.", "startOffset": 113, "endOffset": 202}, {"referenceID": 15, "context": "Accurately modeling a scene involves reasoning about the spatial properties, identities and locations of objects (Eslami et al., 2016; Hinton et al., 2011; Jaderberg et al., 2015; Kulkarni et al., 2015), but also their physical properties, future dynamics, and causal relationships.", "startOffset": 113, "endOffset": 202}, {"referenceID": 2, "context": "Such a sense of intuitive physics can be seen as a program (Anderson, 1990; Goodman and Tenenbaum, 2016) that takes input provided by a physical scene and the past states of objects, and outputs the future states and physical properties of relevant objects for a given task.", "startOffset": 59, "endOffset": 104}, {"referenceID": 11, "context": "Such a sense of intuitive physics can be seen as a program (Anderson, 1990; Goodman and Tenenbaum, 2016) that takes input provided by a physical scene and the past states of objects, and outputs the future states and physical properties of relevant objects for a given task.", "startOffset": 59, "endOffset": 104}, {"referenceID": 5, "context": "The top-down approach (Battaglia et al., 2013; Ullman et al., 2014; Wu et al., 2015) formulates the problem as inference over the parameters of a symbolic physics engine, while the bottom-up approach (Agrawal et al.", "startOffset": 22, "endOffset": 84}, {"referenceID": 28, "context": "The top-down approach (Battaglia et al., 2013; Ullman et al., 2014; Wu et al., 2015) formulates the problem as inference over the parameters of a symbolic physics engine, while the bottom-up approach (Agrawal et al.", "startOffset": 22, "endOffset": 84}, {"referenceID": 29, "context": "The top-down approach (Battaglia et al., 2013; Ullman et al., 2014; Wu et al., 2015) formulates the problem as inference over the parameters of a symbolic physics engine, while the bottom-up approach (Agrawal et al.", "startOffset": 22, "endOffset": 84}, {"referenceID": 0, "context": ", 2015) formulates the problem as inference over the parameters of a symbolic physics engine, while the bottom-up approach (Agrawal et al., 2016; Fragkiadaki et al., 2015; Lerer et al., 2016; Li et al., 2016; Mottaghi et al., 2015; 2016; Sutskever et al., 2009) learns to directly map physical observations to motion prediction or physical judgments.", "startOffset": 123, "endOffset": 261}, {"referenceID": 9, "context": ", 2015) formulates the problem as inference over the parameters of a symbolic physics engine, while the bottom-up approach (Agrawal et al., 2016; Fragkiadaki et al., 2015; Lerer et al., 2016; Li et al., 2016; Mottaghi et al., 2015; 2016; Sutskever et al., 2009) learns to directly map physical observations to motion prediction or physical judgments.", "startOffset": 123, "endOffset": 261}, {"referenceID": 18, "context": ", 2015) formulates the problem as inference over the parameters of a symbolic physics engine, while the bottom-up approach (Agrawal et al., 2016; Fragkiadaki et al., 2015; Lerer et al., 2016; Li et al., 2016; Mottaghi et al., 2015; 2016; Sutskever et al., 2009) learns to directly map physical observations to motion prediction or physical judgments.", "startOffset": 123, "endOffset": 261}, {"referenceID": 19, "context": ", 2015) formulates the problem as inference over the parameters of a symbolic physics engine, while the bottom-up approach (Agrawal et al., 2016; Fragkiadaki et al., 2015; Lerer et al., 2016; Li et al., 2016; Mottaghi et al., 2015; 2016; Sutskever et al., 2009) learns to directly map physical observations to motion prediction or physical judgments.", "startOffset": 123, "endOffset": 261}, {"referenceID": 20, "context": ", 2015) formulates the problem as inference over the parameters of a symbolic physics engine, while the bottom-up approach (Agrawal et al., 2016; Fragkiadaki et al., 2015; Lerer et al., 2016; Li et al., 2016; Mottaghi et al., 2015; 2016; Sutskever et al., 2009) learns to directly map physical observations to motion prediction or physical judgments.", "startOffset": 123, "endOffset": 261}, {"referenceID": 25, "context": ", 2015) formulates the problem as inference over the parameters of a symbolic physics engine, while the bottom-up approach (Agrawal et al., 2016; Fragkiadaki et al., 2015; Lerer et al., 2016; Li et al., 2016; Mottaghi et al., 2015; 2016; Sutskever et al., 2009) learns to directly map physical observations to motion prediction or physical judgments.", "startOffset": 123, "endOffset": 261}, {"referenceID": 29, "context": "For example, there is much work in object detection and segmentation for extracting position and velocity, as well as work for extracting latent object properties (Wu et al., 2015).", "startOffset": 163, "endOffset": 180}, {"referenceID": 16, "context": "Therefore this paper focuses on learning dynamics in that state space, taking a small step toward emulating a general-purpose physics engine (Lake et al., 2016), with the eventual goal of building a system that exhibits the compositionality, modularity, and generality of physics engine whose internal components can be learned through observation.", "startOffset": 141, "endOffset": 160}, {"referenceID": 12, "context": "This structure serves to guide learning towards object-based reasoning as (Hinton et al., 2011) does, and, by design, allows physical knowledge to transfer across variable number of objects and for object properties to be explicitly inferred.", "startOffset": 74, "endOffset": 95}, {"referenceID": 12, "context": "This structure serves to guide learning towards object-based reasoning as (Hinton et al., 2011) does, and, by design, allows physical knowledge to transfer across variable number of objects and for object properties to be explicitly inferred. This approach \u2013 starting with a general sketch of a program and filling in the specifics \u2013 is similar to ideas presented by Solar-Lezama (2008); Tenenbaum et al.", "startOffset": 75, "endOffset": 387}, {"referenceID": 12, "context": "This structure serves to guide learning towards object-based reasoning as (Hinton et al., 2011) does, and, by design, allows physical knowledge to transfer across variable number of objects and for object properties to be explicitly inferred. This approach \u2013 starting with a general sketch of a program and filling in the specifics \u2013 is similar to ideas presented by Solar-Lezama (2008); Tenenbaum et al. (2011). The NPE\u2019s general sketch is its architectural structure, and it extends and enriches this sketch to model the specifics of a particular scene by training on observed trajectories from that scene.", "startOffset": 75, "endOffset": 412}, {"referenceID": 9, "context": "First, because physics does not change across inertial frames, it suffices to separately predict the future state of each object conditioned on the past states of itself and the other objects in its neighborhood, similar to Fragkiadaki et al. (2015). Second, because physics is Markovian, this prediction need only be for the immediate next timestep.", "startOffset": 224, "endOffset": 250}, {"referenceID": 1, "context": "The NP is also a Markovian variant of the Social LSTM (Alahi et al., 2016); it sums the encodings of context objects after encoding each object independently, similar to the Social LSTM\u2019s \u201csocial pooling.", "startOffset": 54, "endOffset": 74}, {"referenceID": 13, "context": "LSTM Long Short-Term Memory (LSTM) networks (Hochreiter and Schmidhuber, 1997) have been shown to sequentially attend to objects (Eslami et al.", "startOffset": 44, "endOffset": 78}, {"referenceID": 7, "context": "LSTM Long Short-Term Memory (LSTM) networks (Hochreiter and Schmidhuber, 1997) have been shown to sequentially attend to objects (Eslami et al., 2016), so it is interesting to test whether a LSTM is well-suited for modeling object interactions, when the object states are explicitly given as input.", "startOffset": 129, "endOffset": 150}, {"referenceID": 22, "context": "From a cognitive science viewpoint, an LSTM can be interpreted as a serial mechanism in object tracking (Pylyshyn and Annan, 2006).", "startOffset": 104, "endOffset": 130}, {"referenceID": 27, "context": "We trained all models using the rmsprop Tieleman and Hinton (2012) backpropagation algorithm with a Euclidean loss for 1,200,000 iterations with a learning rate of 0.", "startOffset": 40, "endOffset": 67}, {"referenceID": 10, "context": "Bouncing balls have also been of interest in cognitive science to study causality and counterfactual reasoning, as in (Gerstenberg et al., 2012).", "startOffset": 118, "endOffset": 144}, {"referenceID": 4, "context": "This proposal is motivated by the experiments in Battaglia et al. (2013), which uses a probabilistic physics simulator to infer various properties of a scene configuration.", "startOffset": 49, "endOffset": 73}, {"referenceID": 5, "context": "A recent set of top-down approaches (Battaglia et al., 2013; Bates et al., 2015; Ullman et al., 2014) investigate probabilistic and deterministic (Wu et al.", "startOffset": 36, "endOffset": 101}, {"referenceID": 3, "context": "A recent set of top-down approaches (Battaglia et al., 2013; Bates et al., 2015; Ullman et al., 2014) investigate probabilistic and deterministic (Wu et al.", "startOffset": 36, "endOffset": 101}, {"referenceID": 28, "context": "A recent set of top-down approaches (Battaglia et al., 2013; Bates et al., 2015; Ullman et al., 2014) investigate probabilistic and deterministic (Wu et al.", "startOffset": 36, "endOffset": 101}, {"referenceID": 29, "context": ", 2014) investigate probabilistic and deterministic (Wu et al., 2015) game physics engines as computational models for physical simulation in humans.", "startOffset": 52, "endOffset": 69}, {"referenceID": 5, "context": "However, inverting a physics engine, as the Intuitive Physics Engine (IPE) (Battaglia et al., 2013) does, requires a full specification of the physical laws and object geometries.", "startOffset": 75, "endOffset": 99}, {"referenceID": 18, "context": "Bottom-up approaches have mapped visual observations to physical judgments (Lerer et al., 2016; Li et al., 2016; Mottaghi et al., 2015; 2016) or passive (Lerer et al.", "startOffset": 75, "endOffset": 141}, {"referenceID": 19, "context": "Bottom-up approaches have mapped visual observations to physical judgments (Lerer et al., 2016; Li et al., 2016; Mottaghi et al., 2015; 2016) or passive (Lerer et al.", "startOffset": 75, "endOffset": 141}, {"referenceID": 20, "context": "Bottom-up approaches have mapped visual observations to physical judgments (Lerer et al., 2016; Li et al., 2016; Mottaghi et al., 2015; 2016) or passive (Lerer et al.", "startOffset": 75, "endOffset": 141}, {"referenceID": 18, "context": ", 2015; 2016) or passive (Lerer et al., 2016; Srivastava et al., 2015; Sutskever et al., 2009) and action-conditioned (Agrawal et al.", "startOffset": 25, "endOffset": 94}, {"referenceID": 24, "context": ", 2015; 2016) or passive (Lerer et al., 2016; Srivastava et al., 2015; Sutskever et al., 2009) and action-conditioned (Agrawal et al.", "startOffset": 25, "endOffset": 94}, {"referenceID": 25, "context": ", 2015; 2016) or passive (Lerer et al., 2016; Srivastava et al., 2015; Sutskever et al., 2009) and action-conditioned (Agrawal et al.", "startOffset": 25, "endOffset": 94}, {"referenceID": 0, "context": ", 2009) and action-conditioned (Agrawal et al., 2016; Finn et al., 2016; Fragkiadaki et al., 2015) motion prediction.", "startOffset": 31, "endOffset": 98}, {"referenceID": 8, "context": ", 2009) and action-conditioned (Agrawal et al., 2016; Finn et al., 2016; Fragkiadaki et al., 2015) motion prediction.", "startOffset": 31, "endOffset": 98}, {"referenceID": 9, "context": ", 2009) and action-conditioned (Agrawal et al., 2016; Finn et al., 2016; Fragkiadaki et al., 2015) motion prediction.", "startOffset": 31, "endOffset": 98}, {"referenceID": 29, "context": "With the exception of works as (Wu et al., 2015), the bottom-up approaches mentioned above do not infer latent properties as our model does.", "startOffset": 31, "endOffset": 48}, {"referenceID": 18, "context": "Lerer et al. (2016) used a shared network for both predicting future frames and for making physical judgments.", "startOffset": 0, "endOffset": 20}, {"referenceID": 9, "context": "This assumption is also core to Fragkiadaki et al. (2015), which conditions motion prediction on individual objects rather than the entire scene.", "startOffset": 32, "endOffset": 58}, {"referenceID": 4, "context": "Recently, in work we only learned about after implementing our approach, Battaglia et al. (2016) in parallel and independently developed a similar architecture that they call Interaction Networks for learning to model physical systems.", "startOffset": 73, "endOffset": 97}], "year": 2016, "abstractText": "We present the Neural Physics Engine (NPE), an object-based neural network architecture for learning predictive models of intuitive physics. We propose a factorization of a physical scene into composable object-based representations and also the NPE architecture whose compositional structure factorizes object dynamics into pairwise interactions. Our approach draws on the strengths of both symbolic and neural approaches: like a symbolic physics engine, the NPE is endowed with generic notions of objects and their interactions, but as a neural network it can also be trained via stochastic gradient descent to adapt to specific object properties and dynamics of different worlds. We evaluate the efficacy of our approach on simple rigid body dynamics in two-dimensional worlds. By comparing to less structured architectures, we show that our model\u2019s compositional representation of the structure in physical interactions improves its ability to predict movement, generalize to different numbers of objects, and infer latent properties of objects such as mass.", "creator": "LaTeX with hyperref package"}}}