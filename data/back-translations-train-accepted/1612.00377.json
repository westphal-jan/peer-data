{"id": "1612.00377", "review": {"conference": "EMNLP", "VERSION": "v1", "DATE_OF_SUBMISSION": "1-Dec-2016", "title": "Piecewise Latent Variables for Neural Variational Text Processing", "abstract": "Recent advances in neural variational inference have facilitated efficient training of powerful directed graphical models with continuous latent variables, such as variational autoencoders. However, these models usually assume simple, uni-modal priors - such as the multivariate Gaussian distribution - yet many real-world data distributions are highly complex and multi-modal. Examples of complex and multi-modal distributions range from topics in newswire text to conversational dialogue responses. When such latent variable models are applied to these domains, the restriction of the simple, uni-modal prior hinders the overall expressivity of the learned model as it cannot possibly capture more complex aspects of the data distribution. To overcome this critical restriction, we propose a flexible, simple prior distribution which can be learned efficiently and potentially capture an exponential number of modes of a target distribution. We develop the multi-modal variational encoder-decoder framework and investigate the effectiveness of the proposed prior in several natural language processing modeling tasks, including document modeling and dialogue modeling.", "histories": [["v1", "Thu, 1 Dec 2016 18:49:23 GMT  (192kb,D)", "http://arxiv.org/abs/1612.00377v1", "18 pages, 2 figures, 4 tables; under review at ICLR 2017"], ["v2", "Fri, 9 Dec 2016 03:18:54 GMT  (192kb,D)", "http://arxiv.org/abs/1612.00377v2", "18 pages, 2 figures, 4 tables; under review at ICLR 2017"], ["v3", "Thu, 13 Jul 2017 19:25:58 GMT  (1016kb,D)", "http://arxiv.org/abs/1612.00377v3", "19 pages, 2 figures, 8 tables; EMNLP 2017"], ["v4", "Sat, 23 Sep 2017 13:33:55 GMT  (1029kb,D)", "http://arxiv.org/abs/1612.00377v4", "19 pages, 2 figures, 8 tables; EMNLP 2017"]], "COMMENTS": "18 pages, 2 figures, 4 tables; under review at ICLR 2017", "reviews": [], "SUBJECTS": "cs.CL cs.AI cs.LG cs.NE", "authors": ["iulian vlad serban", "alexander g ororbia ii", "joelle pineau", "aaron c courville"], "accepted": true, "id": "1612.00377"}, "pdf": {"name": "1612.00377.pdf", "metadata": {"source": "CRF", "title": "MULTI-MODAL VARIATIONAL ENCODER-DECODERS", "authors": ["Iulian V. Serban", "Alexander G. Ororbia II", "Joelle Pineau", "Aaron Courville"], "emails": [], "sections": [{"heading": null, "text": "Recent advances in neural variation coding have enabled efficient training of powerful directed graphical models with continuous latent variables such as variable autoencoders. However, these models typically assume simple, unimodal priors - such as multivariate Gaussian distribution - but many real-world data distributions are highly complex and multimodal. Examples of complex and multimodal distributions range from messaging text topics to conversational dialogue answers. Applying such latent variable models to these domains impedes the general expressivity of the learned model by making it impossible to capture more complex aspects of data distribution. To overcome this critical constraint, we propose a flexible, simple predecessor distribution that can be learned efficiently and potentially capture an exponential number of modes of a target distribution. We are developing the multimodal variation coder decoder framework of previous models and examining the framework of proposed linguistic effects, including the proposed by several modellers."}, {"heading": "1 INTRODUCTION", "text": "With the development of the variable autoencoding framework (Kingma & Welling, 2013; Rezende et al., 2014), an enormous amount of progress has been made in learning large-scale, steered variable models, which has led to improved performance in applications ranging from computer vision (Gregor et al., 2015; Larsen et al., 2015) to natural language processing (Mnih & Gregor, 2014; Miao et al., 2015; Bowman et al., 2015; Serban et al., 2016b). Furthermore, these models incorporate a Bayesian modeling perspective by allowing the integration of problem-dependent knowledge in the form of a generating distribution. However, the majority of proposed models assume an extremely simple rather than a form of multivariate distribution in order to maintain mathematical and computational tractability."}, {"heading": "2 RELATED WORK", "text": "The idea is that it is a way in which people are able to determine for themselves what they want and what they want."}, {"heading": "2.1 APPROACHES FOR LEARNING MULTI-MODAL LATENT VARIABLES", "text": "In this context, it should be noted that the two models that have been developed in recent years are not a model, but a model that is a model that is capable of moving, and in this case it is an example of how the situation has developed in recent years."}, {"heading": "3 THE MULTI-MODAL VARIATIONAL ENCODER-DECODER FRAMEWORK", "text": "We begin by describing the general framework for learning neural variations, and then present our proposed earlier model, which aims to improve the model's ability to learn multiple types of data distributions. We focus on modeling discrete output variables in the context of natural speech processing applications, but the framework is easily adaptable to continuous output variables such as images, video, and audio."}, {"heading": "3.1 NEURAL VARIATIONAL LEARNING", "text": ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"}, {"heading": "3.2 THE PIECEWISE-CONSTANT PRIOR FOR LATENT VARIABLES", "text": "In this paper, we overcome the uni-modal constraint by parameterizing z with a piecemeal constant probability density function (PDF). This parameterization will allow us to represent complex aspects of data distribution in latent variable space, such as multiple modes and highly uneven regions of the probability mass. From a manifold learning perspective, the number of partial constant components results. We assume that z will expand the number of partial constant components that can be represented by the model parameters to include more nonlinear manifolds - especially manifolds in which there are separate clusters of probability mass. We assume that z will extract the number of partial constant components from the PDF: P (z) = 1K n."}, {"heading": "4 LATENT VARIABLE PARAMETRIZATIONS", "text": "In this section, we will develop the parameterizations for both the Gaussian variable and our proposed latent variable.For all parameterizations, c should be the conditioning information for the previous one. In document modeling, no conditioning information is available to the previous one, i.e. c = \u2205. In dialog modeling c is the vector representation of the dialog context, i.e. all previous statements up to the current time step. Let x be the current output sequence (observation) that the model must generate (e.g. w1,..., wN for document modeling)."}, {"heading": "4.1 GAUSSIAN PARAMETRIZATION", "text": "For Gaussian latent variables, the mean of the previous distribution and the variances are encoded using linear transformations of a hidden state. In particular, we thank Christian A. Naesseth for pointing out this assumption. Encoded as a diagonal covariance matrix using a soft-plus function: (16) where Enc (c) is an embedding / encoding of the context c (c) + b before \u00b5, (15) \u03c32, prior = diag (log (1 + exp (Hprior\u03c3 Enc (c) + b before \u03c3)))))), (Enc (c) is an embedding / encoding of the context c (e.g. given by a bag-word encoder or an LSTM encoder applied to c, \u2212 eric = posterical distribution."}, {"heading": "4.2 PIECEWISE CONSTANT PARAMETRIZATION", "text": "Similar to Gaussian variances, we propose to parameterise the piecemeal constants before parameters, using an exponential function applied to a linear transformation of the context in which embedding / encoding takes place: apriori = exp (H before a, i Enc (c) + b before a, i), i = 1,.., n, (19) where Hpriora and b before a are the parameters to be learned. We can also limit the piecemeal constant parameters as an interpolation between the previous parameters and a new estimated parameter: aposti = (1 \u2212 \u03b1a, i) a before i + \u03b1a, i exp (H post a, i Enc (c, x) + b post a, i = 1,.., n, (20) 3We experimented with more complex mechanisms to control the gating variables, including the definition of profiles (c, x + post, b, i), i = 1."}, {"heading": "5 VARIATIONAL TEXT MODELING", "text": "We now present two probabilistic models, the NVDM and the VHRED, which are extended by latent variable parameterization and used for document modeling and the dialog modeling experiments described below."}, {"heading": "5.1 NEURAL VARIATIONAL DOCUMENT MODEL (NVDM)", "text": "The NVDM framework (Mnih & Gregor, 2014; Miao et al., 2015) collapses the recursive neural encoder into a simpler word-bag model (since no symbol order is taken into account) that can be defined as multi-layered perception (MLP) for Enc (c = \u2205, x) = Enc (x). Let's transform the vocabulary into a continuous distributed representation on which the subordinate model is built. NVDM parameterization requires only learning the parameters bpriora, W post a post a, b post a post a post a, the piece variables, and learning the parameters bprior\u00b5, b post, W post, b post, b post, b post, b post, b post, b post, b post, b post, b post, b post to the variables we."}, {"heading": "5.2 VARIATIONAL HIERARCHICAL RECURRENT ENCODER-DECODER (VHRED)", "text": "The VHRED model is an extension of the hierarchical structure, which is reflected in the way it sees itself able to unfold and how it sees itself able to unfold. (...) It is not as if they saw themselves able to unfold. (...) It is not as if they saw themselves able to unfold. (...) It is not as if they saw themselves able to unfold. (...) It is not as if they saw themselves able to unfold. (...) It is not as if they were able to unfold. (...) It is not as if they saw themselves able to unfold. (...) It is not as if they are able to unfold themselves. (...). (...). (...). (...) It is not as if it is. (...). (...) It is not as if it is. (...). (...) It is not as if it is. (...). (... It is not. (...). (... It is not as if it is. (...). (... It is not. (...). (... It is.). (... It is not. (....). (... It is.)."}, {"heading": "6 EXPERIMENTS", "text": "In order to validate the ability of our piecemeal latent variables to capture complex aspects of data distribution, we conduct experiments with both the NVDM and VHRED models. All models are trained using backpropagation to obtain parameter gradients in terms of the lower limit of variation in terms of log probability or exact log probability. We used a standard firststorder gradient descend optimizer Adam (Kingma & Ba, 2015) for both models, where only hyperparameter selections vary depending on the task. The specifications of the encoder and decoder design differed between the two tasks (as described in Sections 5.1 and 5.2).For all models that used piecemeal latent variables, we opted for fixing \u03b1ai = 1, i.e. the piecemeal previous and posterior models are separated (instead of interpolation between a different distribution and the previous ones), as we found this to be 6."}, {"heading": "6.1 DOCUMENT MODELING", "text": "We follow the preparation and follow-up of Hinton & Salakhutdinov (2009), and we use the Reuters corpus (RCV1-V2), using a version that contains a selected 5,000-term vocabulary. 7 Note that the features are a log (1 + TF) that transforms the original frequency vectors. To test our document on text from another language (in this case, Brazilian Portuguese), we used the CADE12 dataset (stop word removed and stemmed) Cardoso-Cachopo (2007), where we use additional filtered terms to obtain a vocabulary of 3,736 terms (over 26,991 terms and 13,486 test documents). For all datasets, we track validation tied to a set of 100 vectors randomly drawn from each training."}, {"heading": "6.2 DIALOGUE MODELING", "text": "This year, it has reached the point where it will be able to retaliate until it is able to retaliate."}, {"heading": "7 CONCLUSIONS", "text": "In order to capture complex aspects of unknown data distributions, we have developed the step constant prior, which can be adapted efficiently and flexibly to capture distributions with many modes, such as those on topics. In document modeling and dialogue modeling experiments, we have demonstrated the effectiveness of our framework in building models that are able to learn a richer structure from data. In particular, we have shown new state-of-the-art results on various tasks of document modeling. Future work should focus on exploring other tasks of natural language processing where multimodality plays an important role, such as modeling technical auxiliary dialogues (Lowe et al., 2015) and online debates (Rosenthal & McKeown, 2015), and where additional information is available, e.g. in the semi-supervised document categorization Ororbia II et al. (2015a)."}, {"heading": "APPENDIX A: ANALYSIS OF DOCUMENT MODEL PIECEWISE VARIABLES", "text": "In order to calculate the gradient of the KL terms needed to formulate the word evaluations, we follow the approach described in subsection 6.2. However, in Table 4 we observe similar results as in subsection 6.2 - the piecemeal variables capture different aspects of the document data. It is noteworthy in this experiment that the Gaussian variables alone were originally sensitive to some of these words. In the hybrid model, however, almost all temporal words that were once more sensitive to the Gaussian variables are now more strongly influenced by the piecemeal variables, which in turn capture all words that were not originally explained."}], "references": [{"title": "Learning the structure of task-driven human\u2013human dialogs", "author": ["S. Bangalore", "G. Di Fabbrizio", "A. Stent"], "venue": "IEEE Transactions on Audio, Speech, and Language Processing,", "citeRegEx": "Bangalore et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Bangalore et al\\.", "year": 2008}, {"title": "Latent dirichlet allocation", "author": ["David M Blei", "Andrew Y Ng", "Michael I Jordan"], "venue": "Journal of machine Learning research,", "citeRegEx": "Blei et al\\.,? \\Q2003\\E", "shortCiteRegEx": "Blei et al\\.", "year": 2003}, {"title": "Reweighted wake-sleep", "author": ["J\u00f6rg Bornschein", "Yoshua Bengio"], "venue": "In ICLR 2015,", "citeRegEx": "Bornschein and Bengio.,? \\Q2014\\E", "shortCiteRegEx": "Bornschein and Bengio.", "year": 2014}, {"title": "Generating sentences from a continuous space", "author": ["S.R. Bowman", "L. Vilnis", "O. Vinyals", "A.M. Dai", "R. Jozefowicz", "S. Bengio"], "venue": "In Conference on Computational Natural Language Learning,", "citeRegEx": "Bowman et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Bowman et al\\.", "year": 2015}, {"title": "Importance weighted autoencoders", "author": ["Yuri Burda", "Roger Grosse", "Ruslan Salakhutdinov"], "venue": "arXiv preprint arXiv:1509.00519,", "citeRegEx": "Burda et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Burda et al\\.", "year": 2015}, {"title": "Improving Methods for Single-label Text Categorization", "author": ["Ana Cardoso-Cachopo"], "venue": "PdD Thesis, Instituto Superior Tecnico, Universidade Tecnica de Lisboa,", "citeRegEx": "Cardoso.Cachopo.,? \\Q2007\\E", "shortCiteRegEx": "Cardoso.Cachopo.", "year": 2007}, {"title": "One billion word benchmark for measuring progress in statistical language modeling", "author": ["C. Chelba", "T. Mikolov", "M. Schuster", "Q. Ge", "T. Brants", "P. Koehn", "T. Robinson"], "venue": "In INTERSPEECH,", "citeRegEx": "Chelba et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Chelba et al\\.", "year": 2014}, {"title": "Unsupervised classification of dialogue acts using a dirichlet process mixture model", "author": ["N. Crook", "R. Granell", "S. Pulman"], "venue": "In Proceedings of the SIGDIAL", "citeRegEx": "Crook et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Crook et al\\.", "year": 2009}, {"title": "Varieties of helmholtz machine", "author": ["Peter Dayan", "Geoffrey E Hinton"], "venue": "Neural Networks,", "citeRegEx": "Dayan and Hinton.,? \\Q1996\\E", "shortCiteRegEx": "Dayan and Hinton.", "year": 1996}, {"title": "Sample-based non-uniform random variate generation", "author": ["Luc Devroye"], "venue": "In Proceedings of the 18th conference on Winter simulation,", "citeRegEx": "Devroye.,? \\Q1986\\E", "shortCiteRegEx": "Devroye.", "year": 1986}, {"title": "Lower and upper bounds for approximation of the kullback-leibler divergence between gaussian mixture models", "author": ["J-L Durrieu", "J-Ph Thiran", "Finnian Kelly"], "venue": "IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP),", "citeRegEx": "Durrieu et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Durrieu et al\\.", "year": 2012}, {"title": "Amazon\u2019s \u2019Alexa Prize", "author": ["M. Farber"], "venue": "Will Give College Students Up To $2.5M To Create A Socialbot. Fortune,", "citeRegEx": "Farber.,? \\Q2016\\E", "shortCiteRegEx": "Farber.", "year": 2016}, {"title": "DRAW: A recurrent neural network for image generation", "author": ["K. Gregor", "I. Danihelka", "A. Graves", "D. Wierstra"], "venue": "In ICLR,", "citeRegEx": "Gregor et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Gregor et al\\.", "year": 2015}, {"title": "Replicated softmax: an undirected topic model", "author": ["Geoffrey E. Hinton", "Ruslan R Salakhutdinov"], "venue": "Advances in Neural Information Processing Systems", "citeRegEx": "Hinton and Salakhutdinov.,? \\Q2009\\E", "shortCiteRegEx": "Hinton and Salakhutdinov.", "year": 2009}, {"title": "Autoencoders, minimum description length and helmholtz free energy", "author": ["Geoffrey E. Hinton", "Richard S. Zemel"], "venue": "Advances in Neural Information Processing Systems", "citeRegEx": "Hinton and Zemel.,? \\Q1994\\E", "shortCiteRegEx": "Hinton and Zemel.", "year": 1994}, {"title": "The\" wake-sleep\" algorithm for unsupervised neural networks", "author": ["Geoffrey E Hinton", "Peter Dayan", "Brendan J Frey", "Radford M Neal"], "venue": null, "citeRegEx": "Hinton et al\\.,? \\Q1995\\E", "shortCiteRegEx": "Hinton et al\\.", "year": 1995}, {"title": "Probabilistic latent semantic indexing", "author": ["Thomas Hofmann"], "venue": "Proceedings of the 22nd annual international ACM SIGIR conference on Research and development in information retrieval,", "citeRegEx": "Hofmann.,? \\Q1999\\E", "shortCiteRegEx": "Hofmann.", "year": 1999}, {"title": "An introduction to variational methods for graphical models", "author": ["Michael I Jordan", "Zoubin Ghahramani", "Tommi S Jaakkola", "Lawrence K Saul"], "venue": "Machine learning,", "citeRegEx": "Jordan et al\\.,? \\Q1999\\E", "shortCiteRegEx": "Jordan et al\\.", "year": 1999}, {"title": "Smart Reply: Automated Response Suggestion for Email", "author": ["Anjuli Kannan", "Karol Kurach"], "venue": "In KDD,", "citeRegEx": "Kannan and Kurach,? \\Q2016\\E", "shortCiteRegEx": "Kannan and Kurach", "year": 2016}, {"title": "Adam: A method for stochastic optimization", "author": ["Diederik Kingma", "Jimmy Ba"], "venue": "In International Conference on Learning Representations,", "citeRegEx": "Kingma and Ba.,? \\Q2015\\E", "shortCiteRegEx": "Kingma and Ba.", "year": 2015}, {"title": "Auto-encoding variational bayes", "author": ["Diederik P Kingma", "Max Welling"], "venue": "arXiv preprint arXiv:1312.6114,", "citeRegEx": "Kingma and Welling.,? \\Q2013\\E", "shortCiteRegEx": "Kingma and Welling.", "year": 2013}, {"title": "Improving variational inference with inverse autoregressive flow", "author": ["Diederik P Kingma", "Tim Salimans", "Max Welling"], "venue": "arXiv preprint arXiv:1606.04934,", "citeRegEx": "Kingma et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Kingma et al\\.", "year": 2016}, {"title": "A neural autoregressive topic model", "author": ["Hugo Larochelle", "Stanislas Lauly"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Larochelle and Lauly.,? \\Q2012\\E", "shortCiteRegEx": "Larochelle and Lauly.", "year": 2012}, {"title": "Autoencoding beyond pixels using a learned similarity metric", "author": ["Anders Boesen Lindbo Larsen", "S\u00f8ren Kaae S\u00f8nderby", "Ole Winther"], "venue": "arXiv preprint arXiv:1512.09300,", "citeRegEx": "Larsen et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Larsen et al\\.", "year": 2015}, {"title": "Document neural autoregressive distribution estimation", "author": ["Stanislas Lauly", "Yin Zheng", "Alexandre Allauzen", "Hugo Larochelle"], "venue": "arXiv preprint arXiv:1603.05962,", "citeRegEx": "Lauly et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Lauly et al\\.", "year": 2016}, {"title": "A diversity-promoting objective function for neural conversation models", "author": ["Jiwei Li", "Michel Galley", "Chris Brockett", "Jianfeng Gao", "Bill Dolan"], "venue": null, "citeRegEx": "Li et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Li et al\\.", "year": 2016}, {"title": "How NOT to evaluate your dialogue system: An empirical study of unsupervised evaluation metrics for dialogue response generation", "author": ["C.-W. Liu", "R. Lowe", "I.V. Serban", "M. Noseworthy", "L. Charlin", "J. Pineau"], "venue": null, "citeRegEx": "Liu et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Liu et al\\.", "year": 2016}, {"title": "The Ubuntu Dialogue Corpus: A Large Dataset for Research in Unstructured Multi-Turn Dialogue Systems", "author": ["Ryan Lowe", "Nissan Pow", "Iulian Serban", "Joelle Pineau"], "venue": "In Proceedings of the SIGDIAL 2015 Conference,", "citeRegEx": "Lowe et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Lowe et al\\.", "year": 2015}, {"title": "Auxiliary deep generative models", "author": ["Lars Maal\u00f8e", "Casper Kaae S\u00f8nderby", "S\u00f8ren Kaae S\u00f8nderby", "Ole Winther"], "venue": "arXiv preprint arXiv:1602.05473,", "citeRegEx": "Maal\u00f8e et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Maal\u00f8e et al\\.", "year": 2016}, {"title": "Mozur. For Sympathetic Ear, More Chinese Turn to Smartphone Program", "author": ["P.J. Markoff"], "venue": "NY Times,", "citeRegEx": "Markoff,? \\Q2015\\E", "shortCiteRegEx": "Markoff", "year": 2015}, {"title": "Neural variational inference for text processing", "author": ["Yishu Miao", "Lei Yu", "Phil Blunsom"], "venue": "arXiv preprint arXiv:1511.06038,", "citeRegEx": "Miao et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Miao et al\\.", "year": 2015}, {"title": "Neural variational inference and learning in belief networks", "author": ["Andriy Mnih", "Karol Gregor"], "venue": "arXiv preprint arXiv:1402.0030,", "citeRegEx": "Mnih and Gregor.,? \\Q2014\\E", "shortCiteRegEx": "Mnih and Gregor.", "year": 2014}, {"title": "Connectionist learning of belief networks", "author": ["Radford M Neal"], "venue": "Artificial intelligence,", "citeRegEx": "Neal.,? \\Q1992\\E", "shortCiteRegEx": "Neal.", "year": 1992}, {"title": "Learning a deep hybrid model for semisupervised text classification", "author": ["Alexander G Ororbia II", "C Lee Giles", "David Reitter"], "venue": "In Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing (EMNLP),", "citeRegEx": "II et al\\.,? \\Q2015\\E", "shortCiteRegEx": "II et al\\.", "year": 2015}, {"title": "Online semi-supervised learning with deep hybrid boltzmann machines and denoising autoencoders", "author": ["Alexander G Ororbia II", "C. Lee Giles", "David Reitter"], "venue": "arXiv preprint arXiv:1511.06964,", "citeRegEx": "II et al\\.,? \\Q2015\\E", "shortCiteRegEx": "II et al\\.", "year": 2015}, {"title": "On the difficulty of training recurrent neural networks", "author": ["R. Pascanu", "T. Mikolov", "Y. Bengio"], "venue": null, "citeRegEx": "Pascanu et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Pascanu et al\\.", "year": 2012}, {"title": "Stochastic backpropagation and approximate inference in deep generative models", "author": ["D.J. Rezende", "S. Mohamed", "D. Wierstra"], "venue": "In ICML,", "citeRegEx": "Rezende et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Rezende et al\\.", "year": 2014}, {"title": "Variational inference with normalizing flows", "author": ["Danilo Jimenez Rezende", "Shakir Mohamed"], "venue": "arXiv preprint arXiv:1505.05770,", "citeRegEx": "Rezende and Mohamed.,? \\Q2015\\E", "shortCiteRegEx": "Rezende and Mohamed.", "year": 2015}, {"title": "Data-driven response generation in social media", "author": ["Alan Ritter", "Colin Cherry", "William B Dolan"], "venue": "In Proceedings of the conference on empirical methods in natural language processing,", "citeRegEx": "Ritter et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Ritter et al\\.", "year": 2011}, {"title": "I couldn\u2019t agree more: The role of conversational structure in agreement and disagreement detection in online discussions", "author": ["Sara Rosenthal", "Kathleen McKeown"], "venue": "In 16th Annual Meeting of the Special Interest Group on Discourse and Dialogue,", "citeRegEx": "Rosenthal and McKeown.,? \\Q2015\\E", "shortCiteRegEx": "Rosenthal and McKeown.", "year": 2015}, {"title": "Efficient learning of deep boltzmann machines", "author": ["Ruslan Salakhutdinov", "Hugo Larochelle"], "venue": "In AISTATs, pp", "citeRegEx": "Salakhutdinov and Larochelle.,? \\Q2010\\E", "shortCiteRegEx": "Salakhutdinov and Larochelle.", "year": 2010}, {"title": "Markov chain monte carlo and variational inference: Bridging the gap", "author": ["Tim Salimans", "Diederik P Kingma", "Max Welling"], "venue": "In International Conference on Machine Learning,", "citeRegEx": "Salimans et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Salimans et al\\.", "year": 2015}, {"title": "Neural machine translation of rare words with subword units", "author": ["Rico Sennrich", "Barry Haddow", "Alexandra Birch"], "venue": "In ACL,", "citeRegEx": "Sennrich et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Sennrich et al\\.", "year": 2016}, {"title": "Building end-to-end dialogue systems using generative hierarchical neural network models", "author": ["I.V. Serban", "A. Sordoni", "Y. Bengio", "A. Courville", "J. Pineau"], "venue": "In AAAI,", "citeRegEx": "Serban et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Serban et al\\.", "year": 2016}, {"title": "A hierarchical latent variable encoder-decoder model for generating dialogues", "author": ["Iulian Vlad Serban", "Alessandro Sordoni", "Ryan Lowe", "Laurent Charlin", "Joelle Pineau", "Aaron Courville", "Yoshua Bengio"], "venue": "arXiv preprint arXiv:1605.06069,", "citeRegEx": "Serban et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Serban et al\\.", "year": 2016}, {"title": "A neural network approach to context-sensitive generation of conversational responses", "author": ["Alessandro Sordoni", "Michel Galley", "Michael Auli", "Chris Brockett", "Yangfeng Ji", "Meg Mitchell", "JianYun Nie", "Jianfeng Gao", "Bill Dolan"], "venue": "In Conference of the North American Chapter of the Association for Computational Linguistics (NAACL-HLT", "citeRegEx": "Sordoni et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Sordoni et al\\.", "year": 2015}, {"title": "Modeling documents with deep boltzmann machines", "author": ["Nitish Srivastava", "Ruslan R Salakhutdinov", "Geoffrey E Hinton"], "venue": "arXiv preprint arXiv:1309.6865,", "citeRegEx": "Srivastava et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Srivastava et al\\.", "year": 2013}, {"title": "A deep and tractable density estimator", "author": ["Benigno Uria", "Iain Murray", "Hugo Larochelle"], "venue": "In ICML, pp", "citeRegEx": "Uria et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Uria et al\\.", "year": 2014}, {"title": "Discovering latent structure in task-oriented dialogues", "author": ["K. Zhai", "J.D. Williams"], "venue": "In ACL, pp", "citeRegEx": "Zhai and Williams.,? \\Q2014\\E", "shortCiteRegEx": "Zhai and Williams.", "year": 2014}], "referenceMentions": [{"referenceID": 36, "context": "With the development of the variational autoencoding framework (Kingma & Welling, 2013; Rezende et al., 2014), a tremendous amount of progress has been made in learning large-scale, directed latent variable models.", "startOffset": 63, "endOffset": 109}, {"referenceID": 12, "context": "This approach has lead to improved performance in applications ranging from computer vision (Gregor et al., 2015; Larsen et al., 2015) to natural language processing (Mnih & Gregor, 2014; Miao et al.", "startOffset": 92, "endOffset": 134}, {"referenceID": 23, "context": "This approach has lead to improved performance in applications ranging from computer vision (Gregor et al., 2015; Larsen et al., 2015) to natural language processing (Mnih & Gregor, 2014; Miao et al.", "startOffset": 92, "endOffset": 134}, {"referenceID": 30, "context": ", 2015) to natural language processing (Mnih & Gregor, 2014; Miao et al., 2015; Bowman et al., 2015; Serban et al., 2016b).", "startOffset": 39, "endOffset": 122}, {"referenceID": 3, "context": ", 2015) to natural language processing (Mnih & Gregor, 2014; Miao et al., 2015; Bowman et al., 2015; Serban et al., 2016b).", "startOffset": 39, "endOffset": 122}, {"referenceID": 15, "context": "The idea of using an artificial neural network to approximate an inference model dates back to the 90s (Hinton & Zemel, 1994; Hinton et al., 1995; Dayan & Hinton, 1996).", "startOffset": 103, "endOffset": 168}, {"referenceID": 32, "context": "Traditionally, researchers resorted to Markov chain Monte Carlo methods (MCMC) (Neal, 1992) which do not scale well and mix slowly, or to variational approaches which require a tractable, factored distribution to approximate the true posterior distribution, usually under-fitting it (Jordan et al.", "startOffset": 79, "endOffset": 91}, {"referenceID": 17, "context": "Traditionally, researchers resorted to Markov chain Monte Carlo methods (MCMC) (Neal, 1992) which do not scale well and mix slowly, or to variational approaches which require a tractable, factored distribution to approximate the true posterior distribution, usually under-fitting it (Jordan et al., 1999).", "startOffset": 283, "endOffset": 304}, {"referenceID": 15, "context": "The idea of using an artificial neural network to approximate an inference model dates back to the 90s (Hinton & Zemel, 1994; Hinton et al., 1995; Dayan & Hinton, 1996). However, initial attempts at such an approach were hindered by the lack of low-bias, low-variance estimators of parameter gradients. Traditionally, researchers resorted to Markov chain Monte Carlo methods (MCMC) (Neal, 1992) which do not scale well and mix slowly, or to variational approaches which require a tractable, factored distribution to approximate the true posterior distribution, usually under-fitting it (Jordan et al., 1999). Others have since proposed using feed-forward inference models to efficiently initialize the mean-field inference algorithm for incrementally training Boltzmann architectures (Salakhutdinov & Larochelle, 2010; Ororbia II et al., 2015b). However, these approaches are limited by the mean-field inference\u2019s inability to model structured posteriors. Recently, Mnih & Gregor (2014) proposed the neural variational inference and learning (NVIL) approach to match the true posterior directly without resorting to approximate inference.", "startOffset": 126, "endOffset": 987}, {"referenceID": 15, "context": "The idea of using an artificial neural network to approximate an inference model dates back to the 90s (Hinton & Zemel, 1994; Hinton et al., 1995; Dayan & Hinton, 1996). However, initial attempts at such an approach were hindered by the lack of low-bias, low-variance estimators of parameter gradients. Traditionally, researchers resorted to Markov chain Monte Carlo methods (MCMC) (Neal, 1992) which do not scale well and mix slowly, or to variational approaches which require a tractable, factored distribution to approximate the true posterior distribution, usually under-fitting it (Jordan et al., 1999). Others have since proposed using feed-forward inference models to efficiently initialize the mean-field inference algorithm for incrementally training Boltzmann architectures (Salakhutdinov & Larochelle, 2010; Ororbia II et al., 2015b). However, these approaches are limited by the mean-field inference\u2019s inability to model structured posteriors. Recently, Mnih & Gregor (2014) proposed the neural variational inference and learning (NVIL) approach to match the true posterior directly without resorting to approximate inference. NVIL allows for the joint training of an inference network and directed generative model, maximizing a variational lower-bound on the data log-likelihood and facilitating exact sampling of the variational posterior. Simultaneously with this work, the variational autoencoder framework was proposed by Kingma & Welling (2013) and Rezende et al.", "startOffset": 126, "endOffset": 1464}, {"referenceID": 15, "context": "The idea of using an artificial neural network to approximate an inference model dates back to the 90s (Hinton & Zemel, 1994; Hinton et al., 1995; Dayan & Hinton, 1996). However, initial attempts at such an approach were hindered by the lack of low-bias, low-variance estimators of parameter gradients. Traditionally, researchers resorted to Markov chain Monte Carlo methods (MCMC) (Neal, 1992) which do not scale well and mix slowly, or to variational approaches which require a tractable, factored distribution to approximate the true posterior distribution, usually under-fitting it (Jordan et al., 1999). Others have since proposed using feed-forward inference models to efficiently initialize the mean-field inference algorithm for incrementally training Boltzmann architectures (Salakhutdinov & Larochelle, 2010; Ororbia II et al., 2015b). However, these approaches are limited by the mean-field inference\u2019s inability to model structured posteriors. Recently, Mnih & Gregor (2014) proposed the neural variational inference and learning (NVIL) approach to match the true posterior directly without resorting to approximate inference. NVIL allows for the joint training of an inference network and directed generative model, maximizing a variational lower-bound on the data log-likelihood and facilitating exact sampling of the variational posterior. Simultaneously with this work, the variational autoencoder framework was proposed by Kingma & Welling (2013) and Rezende et al. (2014). This framework is the motivation of this paper, and will be discussed in detail in the next section.", "startOffset": 126, "endOffset": 1490}, {"referenceID": 1, "context": "With respect to document modeling, it has recently been demonstrated that neural architectures can outperform well-established, standard topic models such as Latent Dirichlet Allocation (LDA) (Blei et al., 2003).", "startOffset": 192, "endOffset": 211}, {"referenceID": 16, "context": "For example, it has been demonstrated that models based on the Boltzmann machine, which learn semantic binary vectors (binary latent variables), perform very well (Hofmann, 1999).", "startOffset": 163, "endOffset": 178}, {"referenceID": 46, "context": "Work involving discrete latent variables include the constrained Poisson model (Salakhutdinov & Hinton, 2009), the Replicated Softmax model (Hinton & Salakhutdinov, 2009) and the Over-Replicated Softmax model (Srivastava et al., 2013), as well as similar, auto-regressive neural architectures and deep directed graphical models (Larochelle & Lauly, 2012; Uria et al.", "startOffset": 209, "endOffset": 234}, {"referenceID": 47, "context": ", 2013), as well as similar, auto-regressive neural architectures and deep directed graphical models (Larochelle & Lauly, 2012; Uria et al., 2014; Lauly et al., 2016; Bornschein & Bengio, 2014).", "startOffset": 101, "endOffset": 193}, {"referenceID": 24, "context": ", 2013), as well as similar, auto-regressive neural architectures and deep directed graphical models (Larochelle & Lauly, 2012; Uria et al., 2014; Lauly et al., 2016; Bornschein & Bengio, 2014).", "startOffset": 101, "endOffset": 193}, {"referenceID": 1, "context": "With respect to document modeling, it has recently been demonstrated that neural architectures can outperform well-established, standard topic models such as Latent Dirichlet Allocation (LDA) (Blei et al., 2003). For example, it has been demonstrated that models based on the Boltzmann machine, which learn semantic binary vectors (binary latent variables), perform very well (Hofmann, 1999). Work involving discrete latent variables include the constrained Poisson model (Salakhutdinov & Hinton, 2009), the Replicated Softmax model (Hinton & Salakhutdinov, 2009) and the Over-Replicated Softmax model (Srivastava et al., 2013), as well as similar, auto-regressive neural architectures and deep directed graphical models (Larochelle & Lauly, 2012; Uria et al., 2014; Lauly et al., 2016; Bornschein & Bengio, 2014). In particular, Mnih & Gregor (2014) showed that using NVIL yields better generative models of documents than these previous approaches.", "startOffset": 193, "endOffset": 851}, {"referenceID": 1, "context": "With respect to document modeling, it has recently been demonstrated that neural architectures can outperform well-established, standard topic models such as Latent Dirichlet Allocation (LDA) (Blei et al., 2003). For example, it has been demonstrated that models based on the Boltzmann machine, which learn semantic binary vectors (binary latent variables), perform very well (Hofmann, 1999). Work involving discrete latent variables include the constrained Poisson model (Salakhutdinov & Hinton, 2009), the Replicated Softmax model (Hinton & Salakhutdinov, 2009) and the Over-Replicated Softmax model (Srivastava et al., 2013), as well as similar, auto-regressive neural architectures and deep directed graphical models (Larochelle & Lauly, 2012; Uria et al., 2014; Lauly et al., 2016; Bornschein & Bengio, 2014). In particular, Mnih & Gregor (2014) showed that using NVIL yields better generative models of documents than these previous approaches. The success of these discrete latent variable models \u2014 which are able to partition probability mass into separate regions \u2014 serve as the main motivation for investigating models with continuous multi-modal latent variables for document modeling. More recently, Miao et al. (2015) have proposed continuous latent variable representations for document modeling, which has achieved state-of-the-art results.", "startOffset": 193, "endOffset": 1231}, {"referenceID": 0, "context": "With respect to dialogue modeling, latent variable models were investigated by Bangalore et al. (2008), Crook et al.", "startOffset": 79, "endOffset": 103}, {"referenceID": 0, "context": "With respect to dialogue modeling, latent variable models were investigated by Bangalore et al. (2008), Crook et al. (2009) as well as others.", "startOffset": 79, "endOffset": 124}, {"referenceID": 0, "context": "With respect to dialogue modeling, latent variable models were investigated by Bangalore et al. (2008), Crook et al. (2009) as well as others. More recently, Zhai & Williams (2014) have proposed three models combining hidden Markov models and topic models.", "startOffset": 79, "endOffset": 181}, {"referenceID": 0, "context": "With respect to dialogue modeling, latent variable models were investigated by Bangalore et al. (2008), Crook et al. (2009) as well as others. More recently, Zhai & Williams (2014) have proposed three models combining hidden Markov models and topic models. The success of these discrete latent variable models also motivates our investigation into dialogue models with multi-modal latent variables. Most related to our work is the Variational Hierarchical Recurrent Encoder-Decoder (VHRED) model by Serban et al. (2016b), which is a neural architecture with latent multivariate Gaussian variables.", "startOffset": 79, "endOffset": 521}, {"referenceID": 21, "context": "This approach is similar to the inverse auto-regressive flow proposed by Kingma et al. (2016). Unfortunately, both normalizing flows and auto-regressive flow are only applicable to the approximate posterior distribution; typically these approaches require fixing the prior distri-", "startOffset": 73, "endOffset": 94}, {"referenceID": 41, "context": "A complementary approach is to combine variational inference with MCMC sampling (Salimans et al., 2015; Burda et al., 2015), however this is computationally expensive and therefore difficult to scale up to many real-world tasks.", "startOffset": 80, "endOffset": 123}, {"referenceID": 4, "context": "A complementary approach is to combine variational inference with MCMC sampling (Salimans et al., 2015; Burda et al., 2015), however this is computationally expensive and therefore difficult to scale up to many real-world tasks.", "startOffset": 80, "endOffset": 123}, {"referenceID": 4, "context": ", 2015; Burda et al., 2015), however this is computationally expensive and therefore difficult to scale up to many real-world tasks. Enriching the latent variable distributions has also been investigated by Maal\u00f8e et al. (2016).", "startOffset": 8, "endOffset": 228}, {"referenceID": 10, "context": "However, the KL divergence between two mixtures of Gaussian distributions cannot be computed in closed form (Durrieu et al., 2012).", "startOffset": 108, "endOffset": 130}, {"referenceID": 36, "context": "multivariate Gaussians) (Rezende et al., 2014).", "startOffset": 24, "endOffset": 46}, {"referenceID": 10, "context": "However, the KL divergence between two mixtures of Gaussian distributions cannot be computed in closed form (Durrieu et al., 2012). To train such a model, one would have to either resort to MCMC sampling, which may slow down and hurt the training process due to the high variance it incurs, or resort to approximations of the KL divergence, which may also hurt the training process.1 Deep Directed Models An alternative to a mixture of Gaussians parametrization is to construct a deep directed graphical model composed of multiple layers of uni-modal latent variables (e.g. multivariate Gaussians) (Rezende et al., 2014). Such models have the potential to capture highly complex, multi-modal latent variable representations through the marginal distribution of the toplayer latent variables. However, this approaches has two major drawbacks. First, the variance of the gradient estimator grows with the number of layers. This makes it difficult to learn highly multimodal latent representations. Second, it is not clear how many modes such models can represent or how their inductive biases will affect their performance on tasks containing multi-modal latent structure. The piecewise constant latent variables we propose do not suffer from either of these two drawbacks; the piecewise constant variables incur low variance in the gradient estimator, and can, in principle, represent a number of modes exponential in the number of latent variables. Discrete Latent Variables A third approach for learning multi-modal latent representations is to instead use discrete latent variables as discussed above. For example, the learning procedure proposed by Mnih & Gregor (2014) for discrete latent variables can easily be combined with the variational autoencoder framework to learn models with both discrete and continuous latent variables.", "startOffset": 109, "endOffset": 1673}, {"referenceID": 9, "context": "To do so, we employ inverse transform sampling (Devroye, 1986), which requires finding the inverse of the cumulative distribution function (CDF).", "startOffset": 47, "endOffset": 62}, {"referenceID": 30, "context": "The NVDM framework (Mnih & Gregor, 2014; Miao et al., 2015) collapses the recurrent neural encoder into a simpler bag-of-words model (since no symbol order is taken into account), which may be defined as a multi-layer perceptron (MLP) for Enc(c = \u2205, x) = Enc(x).", "startOffset": 19, "endOffset": 59}, {"referenceID": 30, "context": "The NVDM framework (Mnih & Gregor, 2014; Miao et al., 2015) collapses the recurrent neural encoder into a simpler bag-of-words model (since no symbol order is taken into account), which may be defined as a multi-layer perceptron (MLP) for Enc(c = \u2205, x) = Enc(x). Let V be the vocabulary. Let W represent a document matrix, where row wi is the 1-of-|V | binary encoding of the i\u2019th word in the document. Enc(W ) is trained to compress a document vector into a continuous distributed representation upon which the posterior model is built. The NVDM parametrization requires only learning the parameters b a ,W post a , b post a for the piecewise variables, and learning the parameters b \u03bc , b prior \u03c3 ,W post \u03bc , b post \u03bc ,W post \u03c3 , b post \u03c3 for the Gaussian variables. We initialize the bias parameters to zero, in order for the NVDM to start with a centered Gaussian prior. This prior will be adapted by the parametric encoder as learning progresses, while also learning to turn on/off latent dimensions controlled through the gating mechanism. It is important to note that our particular instantiation of the NVDM is different from that of Mnih & Gregor (2014) and Miao et al.", "startOffset": 41, "endOffset": 1163}, {"referenceID": 30, "context": "The NVDM framework (Mnih & Gregor, 2014; Miao et al., 2015) collapses the recurrent neural encoder into a simpler bag-of-words model (since no symbol order is taken into account), which may be defined as a multi-layer perceptron (MLP) for Enc(c = \u2205, x) = Enc(x). Let V be the vocabulary. Let W represent a document matrix, where row wi is the 1-of-|V | binary encoding of the i\u2019th word in the document. Enc(W ) is trained to compress a document vector into a continuous distributed representation upon which the posterior model is built. The NVDM parametrization requires only learning the parameters b a ,W post a , b post a for the piecewise variables, and learning the parameters b \u03bc , b prior \u03c3 ,W post \u03bc , b post \u03bc ,W post \u03c3 , b post \u03c3 for the Gaussian variables. We initialize the bias parameters to zero, in order for the NVDM to start with a centered Gaussian prior. This prior will be adapted by the parametric encoder as learning progresses, while also learning to turn on/off latent dimensions controlled through the gating mechanism. It is important to note that our particular instantiation of the NVDM is different from that of Mnih & Gregor (2014) and Miao et al. (2015); we jointly learn the prior mean and variance whereas in previous work it has been assumed to be a standard Gaussian.", "startOffset": 41, "endOffset": 1186}, {"referenceID": 43, "context": "More details are given by Serban et al. (2016b).", "startOffset": 26, "endOffset": 48}, {"referenceID": 43, "context": "The original VHRED model as described by Serban et al. (2016b) used only Gaussian latent variables.", "startOffset": 41, "endOffset": 63}, {"referenceID": 5, "context": "To test our document models on text from another language (in this case, Brazilian Portuguese), we made use of the CADE12 dataset (stop-word removed and stemmed) Cardoso-Cachopo (2007), where we further filtered terms that occurred less than 130 times to obtain a vocabulary of 3,736 terms (over 26,991 training and 13,486 test documents).", "startOffset": 162, "endOffset": 185}, {"referenceID": 34, "context": "We rescale large gradients by their norm Pascanu et al. (2012). Inference networks made use of 50 units in each hidden layer for 20 News-Groups and CADE and 100 for RCV1, while all performed best with 50 latent variables (chosen via preliminary experimentation with smaller models).", "startOffset": 41, "endOffset": 63}, {"referenceID": 34, "context": "We rescale large gradients by their norm Pascanu et al. (2012). Inference networks made use of 50 units in each hidden layer for 20 News-Groups and CADE and 100 for RCV1, while all performed best with 50 latent variables (chosen via preliminary experimentation with smaller models). On the 20 News-Groups, since we were able to use the same set-up (especially vocabulary) as Hinton & Salakhutdinov (2009), we also report the perplexities of a topic model (LDA, Hinton & Salakhutdinov (2009)), the Replicated Softmax (RSM, Hinton & Salakhutdinov (2009)), the document neural auto-regressive estimator (docNADE, Larochelle & Lauly (2012)), a sigmoid belief network (SBN, Mnih & Gregor (2014)), a deep auto-regressive neural network (fDARN, Mnih & Gregor (2014)), and a neural variational document model with a fixed standard Gaussian prior (NVDM, lowest reported perplexity, Miao et al.", "startOffset": 41, "endOffset": 405}, {"referenceID": 34, "context": "We rescale large gradients by their norm Pascanu et al. (2012). Inference networks made use of 50 units in each hidden layer for 20 News-Groups and CADE and 100 for RCV1, while all performed best with 50 latent variables (chosen via preliminary experimentation with smaller models). On the 20 News-Groups, since we were able to use the same set-up (especially vocabulary) as Hinton & Salakhutdinov (2009), we also report the perplexities of a topic model (LDA, Hinton & Salakhutdinov (2009)), the Replicated Softmax (RSM, Hinton & Salakhutdinov (2009)), the document neural auto-regressive estimator (docNADE, Larochelle & Lauly (2012)), a sigmoid belief network (SBN, Mnih & Gregor (2014)), a deep auto-regressive neural network (fDARN, Mnih & Gregor (2014)), and a neural variational document model with a fixed standard Gaussian prior (NVDM, lowest reported perplexity, Miao et al.", "startOffset": 41, "endOffset": 491}, {"referenceID": 34, "context": "We rescale large gradients by their norm Pascanu et al. (2012). Inference networks made use of 50 units in each hidden layer for 20 News-Groups and CADE and 100 for RCV1, while all performed best with 50 latent variables (chosen via preliminary experimentation with smaller models). On the 20 News-Groups, since we were able to use the same set-up (especially vocabulary) as Hinton & Salakhutdinov (2009), we also report the perplexities of a topic model (LDA, Hinton & Salakhutdinov (2009)), the Replicated Softmax (RSM, Hinton & Salakhutdinov (2009)), the document neural auto-regressive estimator (docNADE, Larochelle & Lauly (2012)), a sigmoid belief network (SBN, Mnih & Gregor (2014)), a deep auto-regressive neural network (fDARN, Mnih & Gregor (2014)), and a neural variational document model with a fixed standard Gaussian prior (NVDM, lowest reported perplexity, Miao et al.", "startOffset": 41, "endOffset": 552}, {"referenceID": 34, "context": "We rescale large gradients by their norm Pascanu et al. (2012). Inference networks made use of 50 units in each hidden layer for 20 News-Groups and CADE and 100 for RCV1, while all performed best with 50 latent variables (chosen via preliminary experimentation with smaller models). On the 20 News-Groups, since we were able to use the same set-up (especially vocabulary) as Hinton & Salakhutdinov (2009), we also report the perplexities of a topic model (LDA, Hinton & Salakhutdinov (2009)), the Replicated Softmax (RSM, Hinton & Salakhutdinov (2009)), the document neural auto-regressive estimator (docNADE, Larochelle & Lauly (2012)), a sigmoid belief network (SBN, Mnih & Gregor (2014)), a deep auto-regressive neural network (fDARN, Mnih & Gregor (2014)), and a neural variational document model with a fixed standard Gaussian prior (NVDM, lowest reported perplexity, Miao et al.", "startOffset": 41, "endOffset": 636}, {"referenceID": 34, "context": "We rescale large gradients by their norm Pascanu et al. (2012). Inference networks made use of 50 units in each hidden layer for 20 News-Groups and CADE and 100 for RCV1, while all performed best with 50 latent variables (chosen via preliminary experimentation with smaller models). On the 20 News-Groups, since we were able to use the same set-up (especially vocabulary) as Hinton & Salakhutdinov (2009), we also report the perplexities of a topic model (LDA, Hinton & Salakhutdinov (2009)), the Replicated Softmax (RSM, Hinton & Salakhutdinov (2009)), the document neural auto-regressive estimator (docNADE, Larochelle & Lauly (2012)), a sigmoid belief network (SBN, Mnih & Gregor (2014)), a deep auto-regressive neural network (fDARN, Mnih & Gregor (2014)), and a neural variational document model with a fixed standard Gaussian prior (NVDM, lowest reported perplexity, Miao et al.", "startOffset": 41, "endOffset": 690}, {"referenceID": 34, "context": "We rescale large gradients by their norm Pascanu et al. (2012). Inference networks made use of 50 units in each hidden layer for 20 News-Groups and CADE and 100 for RCV1, while all performed best with 50 latent variables (chosen via preliminary experimentation with smaller models). On the 20 News-Groups, since we were able to use the same set-up (especially vocabulary) as Hinton & Salakhutdinov (2009), we also report the perplexities of a topic model (LDA, Hinton & Salakhutdinov (2009)), the Replicated Softmax (RSM, Hinton & Salakhutdinov (2009)), the document neural auto-regressive estimator (docNADE, Larochelle & Lauly (2012)), a sigmoid belief network (SBN, Mnih & Gregor (2014)), a deep auto-regressive neural network (fDARN, Mnih & Gregor (2014)), and a neural variational document model with a fixed standard Gaussian prior (NVDM, lowest reported perplexity, Miao et al.", "startOffset": 41, "endOffset": 759}, {"referenceID": 30, "context": "On the 20 News-Groups, since we were able to use the same set-up (especially vocabulary) as Hinton & Salakhutdinov (2009), we also report the perplexities of a topic model (LDA, Hinton & Salakhutdinov (2009)), the Replicated Softmax (RSM, Hinton & Salakhutdinov (2009)), the document neural auto-regressive estimator (docNADE, Larochelle & Lauly (2012)), a sigmoid belief network (SBN, Mnih & Gregor (2014)), a deep auto-regressive neural network (fDARN, Mnih & Gregor (2014)), and a neural variational document model with a fixed standard Gaussian prior (NVDM, lowest reported perplexity, Miao et al. (2015)).", "startOffset": 590, "endOffset": 609}, {"referenceID": 30, "context": ", the G-NVDM), as opposed to the fixed prior of Miao et al. (2015). However, we observe that integrating our proposed piecewise variables yields even better results in our document modeling experiments, substantially improving over the baselines.", "startOffset": 48, "endOffset": 67}, {"referenceID": 38, "context": "This is a difficult problem, extensively studied in the recent literature (Ritter et al., 2011; Lowe et al., 2015; Sordoni et al., 2015; Li et al., 2016; Serban et al., 2016a).", "startOffset": 74, "endOffset": 175}, {"referenceID": 27, "context": "This is a difficult problem, extensively studied in the recent literature (Ritter et al., 2011; Lowe et al., 2015; Sordoni et al., 2015; Li et al., 2016; Serban et al., 2016a).", "startOffset": 74, "endOffset": 175}, {"referenceID": 45, "context": "This is a difficult problem, extensively studied in the recent literature (Ritter et al., 2011; Lowe et al., 2015; Sordoni et al., 2015; Li et al., 2016; Serban et al., 2016a).", "startOffset": 74, "endOffset": 175}, {"referenceID": 25, "context": "This is a difficult problem, extensively studied in the recent literature (Ritter et al., 2011; Lowe et al., 2015; Sordoni et al., 2015; Li et al., 2016; Serban et al., 2016a).", "startOffset": 74, "endOffset": 175}, {"referenceID": 11, "context": "Even more recently, Amazon has announced the Alexa Prize Challenge for the research community with the goal of developing a natural and engaging chatbot system (Farber, 2016).", "startOffset": 160, "endOffset": 174}, {"referenceID": 38, "context": "We focus on non-goal-driven dialogue modeling and use the Twitter Dialogue Corpus (Ritter et al., 2011) based on public Twitter conversations.", "startOffset": 82, "endOffset": 103}, {"referenceID": 42, "context": "(2016b), but further pre-processed using byte-pair encoding (Sennrich et al., 2016) using a vocabulary consisting of 5000 sub-words.", "startOffset": 60, "endOffset": 83}, {"referenceID": 6, "context": "9 The dialogues are substantially longer than recent large-scale language modeling corpora, such as the 1 Billion Word Language Model Benchmark (Chelba et al., 2014), which usually focus on modeling single sentences.", "startOffset": 144, "endOffset": 165}, {"referenceID": 10, "context": "Even more recently, Amazon has announced the Alexa Prize Challenge for the research community with the goal of developing a natural and engaging chatbot system (Farber, 2016). We focus on non-goal-driven dialogue modeling and use the Twitter Dialogue Corpus (Ritter et al., 2011) based on public Twitter conversations. The dataset is split into training, validation, and test sets, containing respectively 749,060, 93,633 and 9,399 dialogues each. On average, each dialogue contains about 6 utterances (dialogue turns) and about 94 words. The dataset is the same as used by Serban et al. (2016b), but further pre-processed using byte-pair encoding (Sennrich et al.", "startOffset": 161, "endOffset": 596}, {"referenceID": 35, "context": "10 We use a variant of truncated back-propagation and apply gradient clipping (Pascanu et al., 2012).", "startOffset": 78, "endOffset": 100}, {"referenceID": 43, "context": "Similar to Serban et al. (2016b), we use a bidirectional GRU RNN encoder, where the forward and backward RNNs each have 1000 hidden units.", "startOffset": 11, "endOffset": 33}, {"referenceID": 43, "context": "Similar to Serban et al. (2016b), we use a bidirectional GRU RNN encoder, where the forward and backward RNNs each have 1000 hidden units. We experiment with context RNN encoders with 500 and 1000 hidden units, and find that that 1000 hidden units reach better performance w.r.t. the variational lower-bound on the validation set. The encoder and context RNNs use layer normalization (Ba et al., 2016). We experiment with decoder RNNs with 1000, 2000 and 4000 hidden units (LSTM cells), and find that 2000 hidden units reach better performance. For the G-VHRED model, we experiment with latent multivariate Gaussian variables with 100 and 300 dimensions, and find that 100 dimensions reach better performance. For the H-VHRED model, we experiment with latent multivariate Gaussian and piecewise constant variables each with 100 and 300 dimensions, and find that 100 dimensions reach better performance. We follow the training procedure of Serban et al. (2016b): we drop words in the decoder with a fixed drop rate of 25% and multiply the KL terms in the variational lower-bound by a scalar, which starts at zero and linearly increases to 1 over the first 60,000 training batches.", "startOffset": 11, "endOffset": 961}, {"referenceID": 27, "context": "For each test dialogue, we use TF-IDF to extract 100 candidate responses (Lowe et al., 2015).", "startOffset": 73, "endOffset": 92}, {"referenceID": 25, "context": "Second, it decreases the number of generic responses, which are extremely common among generative models and which human evaluators tend to prefer despite not advancing the dialogue (Li et al., 2016).", "startOffset": 182, "endOffset": 199}, {"referenceID": 25, "context": "We follow the approach by Liu et al. (2016) by conducting an Amazon Mechanical Turk experiment to compare the G-VHRED and H-VHRED models.", "startOffset": 26, "endOffset": 44}, {"referenceID": 27, "context": "Future work should focus on exploring other natural language processing tasks, where multimodality plays an important role such as modeling technical help dialogues (Lowe et al., 2015) and online debates (Rosenthal & McKeown, 2015), and where additional information is available, such as in semi-supervised document categorization Ororbia II et al.", "startOffset": 165, "endOffset": 184}, {"referenceID": 27, "context": "Future work should focus on exploring other natural language processing tasks, where multimodality plays an important role such as modeling technical help dialogues (Lowe et al., 2015) and online debates (Rosenthal & McKeown, 2015), and where additional information is available, such as in semi-supervised document categorization Ororbia II et al. (2015a).", "startOffset": 166, "endOffset": 357}], "year": 2017, "abstractText": "Recent advances in neural variational inference have facilitated efficient training of powerful directed graphical models with continuous latent variables, such as variational autoencoders. However, these models usually assume simple, unimodal priors \u2014 such as the multivariate Gaussian distribution \u2014 yet many realworld data distributions are highly complex and multi-modal. Examples of complex and multi-modal distributions range from topics in newswire text to conversational dialogue responses. When such latent variable models are applied to these domains, the restriction of the simple, uni-modal prior hinders the overall expressivity of the learned model as it cannot possibly capture more complex aspects of the data distribution. To overcome this critical restriction, we propose a flexible, simple prior distribution which can be learned efficiently and potentially capture an exponential number of modes of a target distribution. We develop the multi-modal variational encoder-decoder framework and investigate the effectiveness of the proposed prior in several natural language processing modeling tasks, including document modeling and dialogue modeling.", "creator": "LaTeX with hyperref package"}}}