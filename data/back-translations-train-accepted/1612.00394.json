{"id": "1612.00394", "review": {"conference": "AAAI", "VERSION": "v1", "DATE_OF_SUBMISSION": "1-Dec-2016", "title": "Definition Modeling: Learning to Define Word Embeddings in Natural Language", "abstract": "Distributed representations of words have been shown to capture lexical semantics, as demonstrated by their effectiveness in word similarity and analogical relation tasks. But, these tasks only evaluate lexical semantics indirectly. In this paper, we study whether it is possible to utilize distributed representations to generate dictionary definitions of words, as a more direct and transparent representation of the embeddings' semantics. We introduce definition modeling, the task of generating a definition for a given word and its embedding. We present several definition model architectures based on recurrent neural networks, and experiment with the models over multiple data sets. Our results show that a model that controls dependencies between the word being defined and the definition words performs significantly better, and that a character-level convolution layer designed to leverage morphology can complement word-level embeddings. Finally, an error analysis suggests that the errors made by a definition model may provide insight into the shortcomings of word embeddings.", "histories": [["v1", "Thu, 1 Dec 2016 19:42:37 GMT  (133kb)", "http://arxiv.org/abs/1612.00394v1", "To appear in AAAI Conference 2017"]], "COMMENTS": "To appear in AAAI Conference 2017", "reviews": [], "SUBJECTS": "cs.CL", "authors": ["thanapon noraset", "chen liang", "larry birnbaum", "doug downey"], "accepted": true, "id": "1612.00394"}, "pdf": {"name": "1612.00394.pdf", "metadata": {"source": "CRF", "title": null, "authors": ["Doug Downey"], "emails": ["chenliang2013}@u.northwestern.edu,", "l-birnbaum@northwestern.edu", "d-downey@northwestern.edu"], "sections": [{"heading": null, "text": "ar Xiv: 161 2.00 394v 1 [cs.C L] 1D ec2 01"}, {"heading": "1 Introduction", "text": "Distributed representations of words, or word embeddings, are a key component in many natural language processing systems (NLP) that deal with demand analysis models (Turian, Ratinov, and Bengio 2010; Huang, et al. 2014). Recently, several neural definition techniques have been introduced to learn high-quality word embeddings from unspecified text files (Mikolov et al. 2013a; Pennington, Socher, and Manning 2014; Yogatama et al. 2015). Embeddings have been shown to capture lexical syntax and semantics. It is well known that nearby embeddings represent more synonymmetrical words (Landauer and Dumais 1997) or words in the same class (Dowland, Schoenmackers, and Etzioni 2007). More recently, we have shown vector offsets between embeddings (Yanalog, 2013) to reflect yanalogical relationships."}, {"heading": "2 Previous Work", "text": "Our goal is to investigate RNN models that learn to define word embeddings by training on examples of dictionary definitions. While dictionary corporas have been extensively used in NLP, to the best of our knowledge none of the previous work has attempted to create a generative definition model. Early work focused on extracting semantic information from definitions. For example, Khodorov (1985) and Klavans and Whitman (2001) constructed a taxonomy of words from dictionaries. Dolan et al. (1993) and Vanderwende et al. (2005) extracted semantic representations from definitions to populate a lexical knowledge base. In distributed semantics, words are represented by a dense vector of real numbers rather than semantic predicates. Recently, dictionary definitions have been used to learn these embeddings."}, {"heading": "3 Dictionary Definitions", "text": "In this section, we first examine the content and structure of the definition by examining existing dictionaries. We then describe our new data set and define our tasks and key figures."}, {"heading": "3.1 Definition Content and Structure", "text": "In existing dictionaries, individual definitions are often composed of genera and differentiations (Khodorov, Byrd and Heidorn 1985; Montemagni and Vanderwende 1992); the genus is a generalized class of the word that is defined, and the differentiations are what distinguish the word from others of the same class. Phosphorescence: Light emits without appreciable heat as through slow oxidation of phosphorus \"emitting light...\" is a genus, and \"without applicable heat...\" is a differentiation; moreover, definitions include common patterns such as \"the act of...\" or \"one who has...\" (Markowitz, Ahlswede and Evens 1986); however, patterns and styles are often unique for each diction; the genus + differentiation (G + D) structure is not the only pattern for definitions; the following entry shows a distinction."}, {"heading": "3.2 Corpus: Preprocessing and Analysis", "text": "Dictionaries contain undefined text to help human readers, for example, the entry for \"gradient\" in Wordnik1 contains fields (\"mathematics\") and a single definition (\"how, the line of a railway\"), and many entries contain multiple definitions that are usually (but not always) separated by \".\" We want a corpus in which each entry contains only one word to be defined, and a single definition. We analyze dictionary entries from GCIDE2 and prepare WordNet's glosses, and the fields and processing of scripts can be found at http: / github.com / northanapon / dict-definition.1https / www.wordnik.com / words / gradient 250g.u.org."}, {"heading": "3.3 Dictionary Definition Tasks", "text": "In the definition modeling task (definition ranking), we get an input word and output the probability that a given text D is a definition of the input word. In other words, we estimate P (D | w \u043a). We assume that our definition model has access to a series of word embeddings that are estimated by a corpus other than the definition corpus that is used to build the definition model.DM is a special case of language modeling, and as with language modeling, the performance of a definition model can be measured by the perplexity of a test corpus. Lower perplexity suggests that the model is more accurate in capturing the definition structures and semantics of the defined word. In addition to perplexity measurement, there are other tasks we can use to evaluate a dictionary definition model including the definition generation and the reverse and forward-directed word model we will automatically evaluate the forward definition tasks generated by our P in the definition component."}, {"heading": "4 Models", "text": "The goal of a definition model is to predict the probability of a definition (D = [w1,..., wT]) in the face of a word defined w \u043a. Our model assumes that the probability of generating the tallest word wt of a definition text depends on both the previous words and the word that is defined (Eq 1).The probability distribution is usually determined by a Softmax function (Eq 2) p (D | w \u043a) = T \u0433t = 1p (wt | w1,.., wt \u2212 1) p (1) p (wt = j) p (wt = 1) p (wt = j) p (wt = j) p (wt = j) p (wt = j) p (wt = j) p (wt = wt) p (wt = j) p (wt) p (wt = 1) p (wt), which is a vector that summarizes temperature wj = j) p (wkt) p (wj = 1, and vice versa), and vice versa. wj wt (wt) p (wt) is a mattor that combines wj wj = 1, and vice versa wt (wt) p (wt = 1, and vice versa wt) p (wt) p (wt = 1, wt)."}, {"heading": "4.1 Model Architectures", "text": "A natural method for the condition of an RNN language model is to provide the word defined in the first step q = q as a form of \"seed information.\" The seed approach has proven effective in RNs for other tasks (Kalchburner and Blunsom 2013; Karpaththy and Fei-Fei 2014) Here we follow the simple method of Sutskever et al., (2011), in which the seed is added at the beginning of the text. In our case, the word defined at the beginning of the definition is added. Note: We ignore the predicted probability distribution of the seed even in the test period. Section 3 shows that definitions have common patterns. We suspect that the word defined should be relatively more important for parts of the definition that carry semantic information, rather than for patterns or structures consisting of function and stop words."}, {"heading": "4.2 Other Features", "text": "We focus on two distinct traits: affixes and hypernym embeddings. To add these traits within DM, we simply associate the embedding v \u0445 with the additional feature vectors. Affixes Many words in English and other languages consist of composite morphemes. For example, a word \"capitalist\" contains a root word \"-is.\" A model that knows the semantics of a given root word, together with the knowledge of how affixes modify meaning, could precisely define any morphological variant of the root word. However, words are automatically broken down into morphemes and the semantics of affixes are not trivial. We try to capture prefixes and suffixes in a word by using a network of characters to recognize affixes."}, {"heading": "5 Experiments and Results", "text": "We now present our experiments to evaluate our definition models. We train several model architectures on the basis of the tension set and evaluate the model on the basis of the test set for all three tasks described in Section 3.3. We use the valid sentence to search for the learning hyperparameters. Note that the words that are defined are mutually exclusive across the three sentences, and therefore our experiments evaluate how well the models generalize to new words rather than to additional definitions or senses of the same words. All models use the same set of fixed, pre-trained word embeddings from the Word2Vec project, 3, and a 2-layer LSTM network as an RNN component (Hochreiter and Schmidhuber 1997). Embedding and the hidden LSTM layers each comprise 300 units. For the affix detector, CNN has cores of length 2-6 and size {10, 30, 40, 40} with a strip of 1. During the training, we attach the 01.0- the Logbook Objective and 0.01."}, {"heading": "5.1 Definition Modeling", "text": "The results are consistent with our hypothesis that the word that is defined is more relevant to some words in the definition than others, and the gate update can detect this. We examine the behavior of the gate further in Section 6.Next, we evaluate the contribution of linguistic characteristics. We see that the affixes (S + G + CH) continue to improve the model, suggesting that character-level information can supplement the word embeddings learned from the context. Perhaps surprisingly, the hypernym embeddings (S + G + CH + HE) make an unclear contribution to performance. We suspect that the average of multiple embeddings of the hypernym words may be a poor representation of the genre in a definition."}, {"heading": "5.2 Definition Generation", "text": "In this experiment, we evaluate the quality of the definitions generated by our models. We calculate BLEU scores between the output definitions and the dictionary definitions to measure the quality of the generation. The decoding algorithm will simply scan one token at a time from the predicted probability distribution of words. We take 40 definitions for each word that is defined, using three baseline methods that are close to a greedy algorithm (0.05 or 0.1, selected from the valid sentence) and report on the average BLEU scores. We also report on the scores for the output definitions found in education or testing. Inter, returns the definition of the test set word from the other dictionary."}, {"heading": "5.3 Reverse and Forward Dictionary", "text": "In the dictionary tasks, the models are evaluated on how well they classify words for given definitions (RVD) or definitions for words (FWD). We compare them with models from previous work on the reverse dictionary task (Hill et al. 2016). Previous models read a definition and output an embedding, and then use cosinal similarity between embedding the output and word embedding as a ranking score. There are two ways to compose the embedding of the output: BOW w2v Cosinus uses vector addition and linear projection, and RNN w2vcosinus uses a single-layer LSTM with 512 hidden units. We use two standard metrics for ranking, precision at the top k and R precision (i.e. precision of the uppermost R, where R is the number of correct definitions for the test word). Table 6 shows that our models are better trained to distinguish between the previous cosmic tasks (although they are well trained in the subdivision of the previous ones)."}, {"heading": "6 Discussion", "text": "In this section, we discuss our analysis of the generated definitions. First, we present a qualitative evaluation, followed by an analysis of the behavior of the models. Finally, we discuss error types of the generated definitions and how they might reflect the information captured in the Word embedding."}, {"heading": "6.1 Qualitative Evaluation and Analysis", "text": "First, we perform a qualitative evaluation of the results of the models by asking 6 participants to rate a series of definitions of 50 words from the test sentence. For each word w, we provide, in random order: a basic truth definition for w (dictionary), a basic truth definition of the word w \u00b2, the embedding of which is closest to that of w (NE), the standard language model (Seed *), and our complete system (S + G + CH + HE *). Inter-annotator agreement was strong (almost all interannotator correlations were above 0.6). Table 7 shows that definitions from the S + G + CH + HE * are second only to the word definitions, on average. The advantage of S + G + CH + HE * over Seed * is statistically significant (p < 0.002, t-test), and the difference between S + G + CH + HE * is limited."}, {"heading": "6.2 Error Analysis", "text": "In our manual error analysis of 200 designated definitions, we find that 140 of them contain some degree of error. Table 9 shows the primary error types using examples. Types (1) to (3) are fluid problems and probably stem from the definition model rather than from inadequacies in the embedding. We believe that the other error types are due to semantic gaps in the embedding rather than constraints in the definition model. Our reasons for placing the blame on the embedding, rather than on the definition model itself, are twofold: First, we are conducting an ablation study in which we train S + G + CH using randomized embedding, rather than by the learned Word2Vec embedding. The performance of the model is significantly deteriorated (the test confusion is 100.43, and the BLEU values are shown in Table 5), which indicates that the literature is plausible that the good performance of our secondary definition is often incorrect (4)."}, {"heading": "7 Conclusion", "text": "In this paper, we introduce the task of definition modeling and examine whether word embedding can be used to generate definitions of the corresponding words. We evaluate different architectures based on an RNN language model for definition generation and reverse and advanced dictionary tasks. We find that the gated update function, which improves the impact of the word defined in each step on the model, improves accuracy and that a coil layer at the character level further improves performance. Our error analysis shows that well-trained word embedding is critical to the models and that some error modes of the generated definitions can provide insight into the deficiencies of word embedding. In the future, we plan to investigate whether definition models can be used to improve word embedding or standard language models."}, {"heading": "8 Acknowledgments", "text": "This work was partially supported by NSF grant IIS-1351029 and the Allen Institute for Artificial Intelligence. We thank Chandra Sekhar Bhagavatula for her helpful comments."}], "references": [{"title": "M", "author": ["L. Argerich", "J. Torr\u00e9 Zaffaroni", "Cano"], "venue": "J.", "citeRegEx": "Argerich. Torr\u00e9 Zaffaroni. and Cano 2016", "shortCiteRegEx": null, "year": 2016}, {"title": "and Franz", "author": ["T. Brants"], "venue": "A.", "citeRegEx": "Brants and Franz 2006", "shortCiteRegEx": null, "year": 2006}, {"title": "Learning phrase representations using RNN encoder-decoder", "author": ["Cho"], "venue": null, "citeRegEx": "Cho,? \\Q2014\\E", "shortCiteRegEx": "Cho", "year": 2014}, {"title": "G", "author": ["M.S. Chodorow", "R.J. Byrd", "Heidorn"], "venue": "E.", "citeRegEx": "Chodorow. Byrd. and Heidorn 1985", "shortCiteRegEx": null, "year": 1985}, {"title": "and Baroni", "author": ["G. Dinu"], "venue": "M.", "citeRegEx": "Dinu and Baroni 2014", "shortCiteRegEx": null, "year": 2014}, {"title": "S", "author": ["W. Dolan", "L. Vanderwende", "Richardson"], "venue": "D.", "citeRegEx": "Dolan. Vanderwende. and Richardson 1993", "shortCiteRegEx": null, "year": 1993}, {"title": "Sparse information extraction: Unsupervised language models to the rescue", "author": ["Schoenmackers Downey", "D. Etzioni 2007] Downey", "S. Schoenmackers", "O. Etzioni"], "venue": "In ACL 2007,", "citeRegEx": "Downey et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Downey et al\\.", "year": 2007}, {"title": "J", "author": ["Elman"], "venue": "L.", "citeRegEx": "Elman 1990", "shortCiteRegEx": null, "year": 1990}, {"title": "B", "author": ["J. Gordon", "Van Durme"], "venue": "2013. Reporting bias and knowledge acquisition. In AKBC workshop,", "citeRegEx": "Gordon and Van Durme 2013", "shortCiteRegEx": null, "year": 2013}, {"title": "M", "author": ["Hearst"], "venue": "A.", "citeRegEx": "Hearst 1992", "shortCiteRegEx": null, "year": 1992}, {"title": "Learning to understand phrases by embedding the dictionary. 4:17\u201330", "author": ["Hill"], "venue": null, "citeRegEx": "Hill,? \\Q2016\\E", "shortCiteRegEx": "Hill", "year": 2016}, {"title": "and Schmidhuber", "author": ["S. Hochreiter"], "venue": "J.", "citeRegEx": "Hochreiter and Schmidhuber 1997", "shortCiteRegEx": null, "year": 1997}, {"title": "Learning representations for weakly supervised natural language processing tasks. Computational Linguistics 40(1):85\u2013120", "author": ["Huang"], "venue": null, "citeRegEx": "Huang,? \\Q2014\\E", "shortCiteRegEx": "Huang", "year": 2014}, {"title": "and Blunsom", "author": ["N. Kalchbrenner"], "venue": "P.", "citeRegEx": "Kalchbrenner and Blunsom 2013", "shortCiteRegEx": null, "year": 2013}, {"title": "and Fei-Fei", "author": ["A. Karpathy"], "venue": "L.", "citeRegEx": "Karpathy and Fei.Fei 2014", "shortCiteRegEx": null, "year": 2014}, {"title": "A", "author": ["Y. Kim", "Y. Jernite", "D. Sontag", "Rush"], "venue": "2016. Character-aware neural language models. In AAAI", "citeRegEx": "Kim et al. 2016", "shortCiteRegEx": null, "year": 2016}, {"title": "and Ba", "author": ["D.P. Kingma"], "venue": "J.", "citeRegEx": "Kingma and Ba 2014", "shortCiteRegEx": null, "year": 2014}, {"title": "and Whitman", "author": ["J. Klavans"], "venue": "B.", "citeRegEx": "Klavans and Whitman 2001", "shortCiteRegEx": null, "year": 2001}, {"title": "S", "author": ["T.K. Landauer", "Dumais"], "venue": "T.", "citeRegEx": "Landauer and Dumais 1997", "shortCiteRegEx": null, "year": 1997}, {"title": "Handwritten digit recognition with a backpropagation network", "author": ["D. L"], "venue": null, "citeRegEx": "L.,? \\Q1990\\E", "shortCiteRegEx": "L.", "year": 1990}, {"title": "D", "author": ["J. Li", "M.-T. Luong", "Jurafsky"], "venue": "2015. A hierarchical neural autoencoder for paragraphs and documents. In ACL", "citeRegEx": "Li. Luong. and Jurafsky 2015", "shortCiteRegEx": null, "year": 2015}, {"title": "Semantically significant patterns in dictionary definitions", "author": ["Ahlswede Markowitz", "J. Evens 1986] Markowitz", "T. Ahlswede", "M. Evens"], "venue": "ACL", "citeRegEx": "Markowitz et al\\.,? \\Q1986\\E", "shortCiteRegEx": "Markowitz et al\\.", "year": 1986}, {"title": "and Zweig", "author": ["T. Mikolov"], "venue": "G.", "citeRegEx": "Mikolov and Zweig 2012", "shortCiteRegEx": null, "year": 2012}, {"title": "Recurrent neural network based language model", "author": ["Mikolov"], "venue": "INTERSPEECH", "citeRegEx": "Mikolov,? \\Q2010\\E", "shortCiteRegEx": "Mikolov", "year": 2010}, {"title": "Efficient estimation of word representations in vector space", "author": ["Mikolov"], "venue": null, "citeRegEx": "Mikolov,? \\Q2013\\E", "shortCiteRegEx": "Mikolov", "year": 2013}, {"title": "Distributed representations of words and phrases and their compositionality", "author": ["Mikolov"], "venue": null, "citeRegEx": "Mikolov,? \\Q2013\\E", "shortCiteRegEx": "Mikolov", "year": 2013}, {"title": "Linguistic Regularities in Continuous Space Word Representations", "author": ["Yih Mikolov", "T. Zweig 2013] Mikolov", "W.-t. Yih", "G. Zweig"], "venue": "HLT-NAACL", "citeRegEx": "Mikolov et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Mikolov et al\\.", "year": 2013}, {"title": "and Vanderwende", "author": ["S. Montemagni"], "venue": "L.", "citeRegEx": "Montemagni and Vanderwende 1992", "shortCiteRegEx": null, "year": 1992}, {"title": "Glove: Global vectors for word representation", "author": ["Socher Pennington", "J. Manning 2014] Pennington", "R. Socher", "C. Manning"], "venue": "EMNLP", "citeRegEx": "Pennington et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Pennington et al\\.", "year": 2014}, {"title": "S", "author": ["J. Seitner", "C. Bizer", "K. Eckert", "S. Faralli", "R. Meusel", "H. Paulheim", "Ponzetto"], "venue": "2016. A large database of hypernymy relations extracted from the web. In LREC", "citeRegEx": "Seitner et al. 2016", "shortCiteRegEx": null, "year": 2016}, {"title": "G", "author": ["I. Sutskever", "J. Martens", "Hinton"], "venue": "E.", "citeRegEx": "Sutskever. Martens. and Hinton 2011", "shortCiteRegEx": null, "year": 2011}, {"title": "2014", "author": ["I. Sutskever", "O. Vinyals", "Q. V Le"], "venue": "Sequence to sequence learning with neural networks. In Ghahramani, Z.; Welling, M.; Cortes, C.; Lawrence, N. D.; and Weinberger, K. Q., eds., NIPS", "citeRegEx": "Sutskever. Vinyals. and Le 2014", "shortCiteRegEx": null, "year": 2014}, {"title": "Word representations: A simple and general method for semi-supervised learning", "author": ["Ratinov Turian", "J. Bengio 2010] Turian", "L.-A. Ratinov", "Y. Bengio"], "venue": "In Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics,", "citeRegEx": "Turian et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Turian et al\\.", "year": 2010}, {"title": "A", "author": ["L. Vanderwende", "G. Kacmarcik", "H. Suzuki", "Menezes"], "venue": "2005. Mindnet: an automaticallycreated lexical resource. In HLT-EMNLP", "citeRegEx": "Vanderwende et al. 2005", "shortCiteRegEx": null, "year": 2005}, {"title": "Learning lexical embeddings with syntactic and lexicographic knowledge", "author": ["Mohamed Wang", "T. Hirst 2015] Wang", "A. Mohamed", "G. Hirst"], "venue": "ACL", "citeRegEx": "Wang et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Wang et al\\.", "year": 2015}, {"title": "Semantically conditioned LSTM-based natural language generation for spoken dialogue systems", "author": ["Wen"], "venue": "EMNLP", "citeRegEx": "Wen,? \\Q2015\\E", "shortCiteRegEx": "Wen", "year": 2015}, {"title": "Stochastic language generation in dialogue using recurrent neural networks with convolutional sentence reranking", "author": ["Wen"], "venue": "SIGDIAL", "citeRegEx": "Wen,? \\Q2015\\E", "shortCiteRegEx": "Wen", "year": 2015}, {"title": "Learning word representations with hierarchical sparse coding", "author": ["Yogatama"], "venue": "In Proceedings of The 32nd International Conference on Machine Learning,", "citeRegEx": "Yogatama,? \\Q2015\\E", "shortCiteRegEx": "Yogatama", "year": 2015}], "referenceMentions": [], "year": 2016, "abstractText": "Distributed representations of words have been shown to capture lexical semantics, as demonstrated by their effectiveness in word similarity and analogical relation tasks. But, these tasks only evaluate lexical semantics indirectly. In this paper, we study whether it is possible to utilize distributed representations to generate dictionary definitions of words, as a more direct and transparent representation of the embeddings\u2019 semantics. We introduce definition modeling, the task of generating a definition for a given word and its embedding. We present several definition model architectures based on recurrent neural networks, and experiment with the models over multiple data sets. Our results show that a model that controls dependencies between the word being defined and the definition words performs significantly better, and that a characterlevel convolution layer designed to leverage morphology can complement word-level embeddings. Finally, an error analysis suggests that the errors made by a definition model may provide insight into the shortcomings of word embeddings.", "creator": "LaTeX with hyperref package"}}}