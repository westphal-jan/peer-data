{"id": "1612.00429", "review": {"conference": "iclr", "VERSION": "v1", "DATE_OF_SUBMISSION": "1-Dec-2016", "title": "Generalizing Skills with Semi-Supervised Reinforcement Learning", "abstract": "Deep reinforcement learning (RL) can acquire complex behaviors from low-level inputs, such as images. However, real-world applications of such methods require generalizing to the vast variability of the real world. Deep networks are known to achieve remarkable generalization when provided with massive amounts of labeled data, but can we provide this breadth of experience to an RL agent, such as a robot? The robot might continuously learn as it explores the world around it, even while deployed. However, this learning requires access to a reward function, which is often hard to measure in real-world domains, where the reward could depend on, for example, unknown positions of objects or the emotional state of the user. Conversely, it is often quite practical to provide the agent with reward functions in a limited set of situations, such as when a human supervisor is present or in a controlled setting. Can we make use of this limited supervision, and still benefit from the breadth of experience an agent might collect on its own? In this paper, we formalize this problem as semisupervised reinforcement learning, where the reward function can only be evaluated in a set of \"labeled\" MDPs, and the agent must generalize its behavior to the wide range of states it might encounter in a set of \"unlabeled\" MDPs, by using experience from both settings. Our proposed method infers the task objective in the unlabeled MDPs through an algorithm that resembles inverse RL, using the agent's own prior experience in the labeled MDPs as a kind of demonstration of optimal behavior. We evaluate our method on challenging tasks that require control directly from images, and show that our approach can improve the generalization of a learned deep neural network policy by using experience for which no reward function is available. We also show that our method outperforms direct supervised learning of the reward.", "histories": [["v1", "Thu, 1 Dec 2016 20:48:39 GMT  (1883kb,D)", "http://arxiv.org/abs/1612.00429v1", null], ["v2", "Thu, 9 Mar 2017 19:46:12 GMT  (1941kb,D)", "http://arxiv.org/abs/1612.00429v2", "ICLR 2017"]], "reviews": [], "SUBJECTS": "cs.LG cs.AI cs.RO", "authors": ["chelsea finn", "tianhe yu", "justin fu", "pieter abbeel", "sergey levine"], "accepted": true, "id": "1612.00429"}, "pdf": {"name": "1612.00429.pdf", "metadata": {"source": "META", "title": "Generalizing Skills with Semi-Supervised Reinforcement Learning", "authors": ["Chelsea Finn", "Tianhe Yu", "Justin Fu", "Pieter Abbeel", "Sergey Levine"], "emails": ["cbfinn@berkeley.edu", "tianhe.yu@berkeley.edu", "justinfu@berkeley.edu", "pabbeel@berkeley.edu", "svlevine@berkeley.edu"], "sections": [{"heading": "1 INTRODUCTION", "text": "In practice, however, the application of reinforcement methods (RL) to real scenarios in which the learned policy must be able to deal with the variability of the real world and engage with scenarios that have not previously existed. In many areas, such as robotics and dialogue systems, the variability of the real world poses a major challenge. Methods for training, which are combined with massive amounts of data, are known to enable a broad generalization of supervised learning tasks (Russakovsky et al.) Lifelong learning aims to address these."}, {"heading": "2 RELATED WORK", "text": "In fact, we see ourselves in a position to put ourselves at the top of society, in the way we do it: in the way we do it, in the way we do it, in the way we do it, in the way we do it, in the way we do it, in the way we do it, in the way we do it. \""}, {"heading": "3 SEMI-SUPERVISED REINFORCEMENT LEARNING", "text": "We first define semi-supervised or discrete learning, and the reward function is distributed among the same MPs. We would like to capture situations where the monitoring, via the reward function, is available only in a small set of limited Markov decision-making processes (MDPs), but where we want our agent to be able to continue to learn to successfully work in a much larger set of unmarked MDPs where reward tokens are not available. For example, if the task corresponds to an autonomous car learning to drive, the marked MDPs could correspond to a series of closed courses, while the unmarked MDPs could include driving on real roads and city streets. Formally, we consider a distribution p (M) over undiscounted finite-horizon MDPs, each defined as 5-tupleMi = (S, T, R) across states, actions, transition dynamics (which are universally unknown), and reward."}, {"heading": "4 SEMI-SUPERVISED SKILL GENERALIZATION", "text": "As already discussed, our goal is to learn a policy that maximizes the expected reward by using both unmarked experiences in U and the related experiences in L. We will use the formalism used in the previous section, but note that the implementation of RL in a number of MDPs can be considered a single MDP with a wide variety of initial conditions. To perform semi-supervised reinforcement learning, we use the framework of maximum entropy control (Ziebart, 2010; Kappen et al., 2012), sometimes referred to as linear-solvable MDPs (Dvijotham & Todorov, 2010).This framework is a generalization of the standard reinforcement learning formulation, in which, instead of optimizing the expected reward, we set an entropy objective of the formalized RL = argmax."}, {"heading": "5 EXPERIMENTAL EVALUATION", "text": "Since the goal of S3G is to improve the generalization performance of a learned policy by using data from unlabeled MDPs, our experiments focus on areas where generalization is critical to success. Despite the focus on generalization in many machine learning problems, the generalization capabilities of strategies trained with RL have often been overlooked. For example, the training conditions in recent RL benchmarks such as the Arcade Learning Environment (Bellemare et al., 2012) and OpenAI Gym (Brockman et al., 2016) are perfectly aligned with the test conditions. Therefore, we define our own simulated control tasks for this work, explicitly considering the types of variations that a robot might encounter in the real world. By evaluating how well semi-supervised methods can use blank experiences to improve the generalization of a deep neural network policy that is only learned in designated scenarios."}, {"heading": "5.1 TASKS", "text": "The difficulty of the task ranges from simple, low-dimensional problems1 The code will be part of the GPS code base, which usually includes a wider range of conditions. We visualize the tasks in Figure 2 and describe them in detail: Obstacle Navigation / Obstacle Height: The aim of this task is to navigate a point robot around an obstacle, but not around a greater variety of conditions. We visualize the tasks in Figure 2 and describe them in detail."}, {"heading": "5.2 EVALUATION", "text": "In our evaluation, we compare the performance of S3G with that of (i) the Directive Directive Policy \u03c0FL, which is trained only in the designated MDPs, (ii) a policy that uses a reward function endowed with supervised learning, and (iii) an oracle policy that can access the true reward function in all scenarios. The architecture of the reward function endowed with supervised learning is the same as in S3G. In order to thoroughly test the generalization capabilities of the actions learned with each method, we measure performance in a wide range of settings that represent a supervision of the unlabeled and labeled MDPs. We report on the success rate of the policies learned with each method in Table 1, and visualize the generalization performance of the 2-Link Reachers, Cheetah, and obstacle tasks in Figure 3. The sample complexity of each method is described in Annex B.In all four tasks, the RL policy shows that the generalization function cannot be improved as S3G can be improved."}, {"heading": "6 CONCLUSION & FUTURE WORK", "text": "We introduced the first method for semi-supervised reinforcement learning, motivated by lifelong learning in the real world. By deriving the reward in environments where one is not available, S3G can improve the generalization of a learned policy of neural networks trained only in the \"labeled\" environments. Furthermore, we find that compared to using supervised regression for reward labeling, we can achieve higher performance by using a reverse RL target to derive the reward underlying the previous learning experience of the agent. Interestingly, this does not directly use the reward labeling when we derive the reward of states in the unlabeled MDPs, and our results on obstacle navigation indeed suggest that the rewards learned with S3G soften the learning path to a better design. As we discussed earlier, the reward and policy optimization methods used in 2016 are semi-physics based on the work we have done with hundreds of similar physics-based systems."}, {"heading": "ACKNOWLEDGMENTS", "text": "The authors thank Anca Dragan for the insightful discussions and Aviv Tamar and Roberto Calandra for their helpful feedback on the paper. Funded by the NSF GRFP, the DARPA Simplex program and Berkeley DeepDrive."}, {"heading": "A MIRROR DESCENT GUIDED POLICY SEARCH", "text": "To optimize strategies with S3G, we decided to use mirrored policy strategies (MDGPS) for their superior example efficiency over other policy optimization methods. MDGPS belongs to a class of guided policy search methods that simplify policy search by splitting the problem into two phases: a) a trajectory oriented RL phase (C phase) and b) a supervised learning phase (S phase). During the C phase, a trajectory oriented RL method is used to train \"local\" controllers for each of the M starting positions. In the S phase, a global policy guideline (a | s) is trained using supervised learning to adjust the output of each local policy. MDGPS can be interpreted as an approximate variant of the mirror pedigree at the expected cost J (V) = pronounced Tt = 1EE guidelines (st, at)."}, {"heading": "B SAMPLE COMPLEXITY OF EXPERIMENTS", "text": "In Table 2, we specify the number of samples used for all tasks and methods in both marked and unmarked scenarios. Note that the marked samples used by the oracle come from the \"unmarked\" MDPs U, where we generally assume that reward labels are not available."}], "references": [{"title": "Apprenticeship learning via inverse reinforcement learning", "author": ["P. Abbeel", "A. Ng"], "venue": "In International Conference on Machine Learning (ICML),", "citeRegEx": "Abbeel and Ng.,? \\Q2004\\E", "shortCiteRegEx": "Abbeel and Ng.", "year": 2004}, {"title": "Concrete problems in ai safety", "author": ["Dario Amodei", "Chris Olah", "Jacob Steinhardt", "Paul Christiano", "John Schulman", "Dan Man\u00e9"], "venue": "arXiv preprint arXiv:1606.06565,", "citeRegEx": "Amodei et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Amodei et al\\.", "year": 2016}, {"title": "Maximum entropy semisupervised inverse reinforcement learning", "author": ["Julien Audiffren", "Michal Valko", "Alessandro Lazaric", "Mohammad Ghavamzadeh"], "venue": "International Joint Conference on Artificial Intelligence (IJCAI),", "citeRegEx": "Audiffren et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Audiffren et al\\.", "year": 2015}, {"title": "Transfer learning for reinforcement learning on a physical robot", "author": ["Samuel Barrett", "Matt E. Taylor", "Peter Stone"], "venue": "In Ninth International Conference on Autonomous Agents and Multiagent Systems - Adaptive Learning Agents Workshop (ALA),", "citeRegEx": "Barrett et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Barrett et al\\.", "year": 2010}, {"title": "The arcade learning environment: An evaluation platform for general agents", "author": ["Marc G Bellemare", "Yavar Naddaf", "Joel Veness", "Michael Bowling"], "venue": "Journal of Artificial Intelligence Research,", "citeRegEx": "Bellemare et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Bellemare et al\\.", "year": 2012}, {"title": "Semi-supervised reinforcement learning", "author": ["Paul Christiano"], "venue": "https://medium.com/ai-control/ semi-supervised-reinforcement-learning-cf7d5375197f,", "citeRegEx": "Christiano.,? \\Q2016\\E", "shortCiteRegEx": "Christiano.", "year": 2016}, {"title": "Learning modular neural network policies for multi-task and multi-robot transfer", "author": ["Coline Devin", "Abhishek Gupta", "Trevor Darrell", "Pieter Abbeel", "Sergey Levine"], "venue": "arXiv preprint arXiv:1609.07088,", "citeRegEx": "Devin et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Devin et al\\.", "year": 2016}, {"title": "Learning from experience in manipulation planning: Setting the right goals", "author": ["Anca Dragan", "Geoffrey Gordon", "Siddhartha Srinivasa"], "venue": "International Symposium on Experimental Robotics (ISER),", "citeRegEx": "Dragan et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Dragan et al\\.", "year": 2011}, {"title": "Inverse optimal control with linearly-solvable MDPs", "author": ["K. Dvijotham", "E. Todorov"], "venue": "In International Conference on Machine Learning (ICML),", "citeRegEx": "Dvijotham and Todorov.,? \\Q2010\\E", "shortCiteRegEx": "Dvijotham and Todorov.", "year": 2010}, {"title": "Guided cost learning: Deep inverse optimal control via policy optimization", "author": ["Chelsea Finn", "Sergey Levine", "Pieter Abbeel"], "venue": "International Conference on Machine Learning (ICML),", "citeRegEx": "Finn et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Finn et al\\.", "year": 2016}, {"title": "Model-free imitation learning with policy optimization", "author": ["Jonathan Ho", "Jayesh K. Gupta", "Stefano Ermon"], "venue": "International Conference on Machine Learning (ICML),", "citeRegEx": "Ho et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Ho et al\\.", "year": 2016}, {"title": "Optimal control as a graphical model inference problem", "author": ["Hilbert J Kappen", "Vicen\u00e7 G\u00f3mez", "Manfred Opper"], "venue": "Machine learning,", "citeRegEx": "Kappen et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Kappen et al\\.", "year": 2012}, {"title": "Semi-supervised learning with deep generative models", "author": ["Diederik P Kingma", "Shakir Mohamed", "Danilo Jimenez Rezende", "Max Welling"], "venue": "In Neural Information Processing Systems (NIPS)", "citeRegEx": "Kingma et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Kingma et al\\.", "year": 2014}, {"title": "Autonomous shaping: Knowledge transfer in reinforcement learning", "author": ["George Konidaris", "Andrew Barto"], "venue": "International Conference on Machine Learning (ICML),", "citeRegEx": "Konidaris and Barto.,? \\Q2006\\E", "shortCiteRegEx": "Konidaris and Barto.", "year": 2006}, {"title": "End-to-end training of deep visuomotor policies", "author": ["S. Levine", "C. Finn", "T. Darrell", "P. Abbeel"], "venue": "Journal of Machine Learning Research (JMLR),", "citeRegEx": "Levine et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Levine et al\\.", "year": 2016}, {"title": "Human-level control through deep reinforcement learning", "author": ["Volodymyr Mnih", "Koray Kavukcuoglu", "David Silver", "Andrei A Rusu", "Joel Veness", "Marc G Bellemare", "Alex Graves", "Martin Riedmiller", "Andreas K Fidjeland", "Georg Ostrovski"], "venue": null, "citeRegEx": "Mnih et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Mnih et al\\.", "year": 2015}, {"title": "Guided policy search as approximate mirror descent", "author": ["William Montgomery", "Sergey Levine"], "venue": "Advances in Neural Information Processing Systems (NIPS),", "citeRegEx": "Montgomery and Levine.,? \\Q2016\\E", "shortCiteRegEx": "Montgomery and Levine.", "year": 2016}, {"title": "Reset-free guided policy search: Efficient deep reinforcement learning with stochastic initial states", "author": ["William Montgomery", "Anurag Ajay", "Chelsea Finn", "Pieter Abbeel", "Sergey Levine"], "venue": "arXiv preprint arXiv:1610.01112,", "citeRegEx": "Montgomery et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Montgomery et al\\.", "year": 2016}, {"title": "Combining model-based policy search with online model learning for control of physical humanoids", "author": ["Igor Mordatch", "Nikhil Mishra", "Clemens Eppner", "Pieter Abbeel"], "venue": "International Conference on Robotics and Automation (ICRA),", "citeRegEx": "Mordatch et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Mordatch et al\\.", "year": 2016}, {"title": "Algorithms for inverse reinforcement learning", "author": ["Andrew Y Ng", "Stuart J Russell"], "venue": "International Conference on Machine Learning (ICML),", "citeRegEx": "Ng and Russell,? \\Q2000\\E", "shortCiteRegEx": "Ng and Russell", "year": 2000}, {"title": "Control of memory, active perception, and action in minecraft", "author": ["Junhyuk Oh", "Valliappa Chockalingam", "Satinder Singh", "Honglak Lee"], "venue": "International Conference on Machine Learning (ICML),", "citeRegEx": "Oh et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Oh et al\\.", "year": 2016}, {"title": "Actor-mimic: Deep multitask and transfer reinforcement learning", "author": ["Emilio Parisotto", "Jimmy Lei Ba", "Ruslan Salakhutdinov"], "venue": "International Conference on Learning Representations (ICLR),", "citeRegEx": "Parisotto et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Parisotto et al\\.", "year": 2016}, {"title": "Semi-supervised learning with ladder networks", "author": ["Antti Rasmus", "Harri Valpola", "Mikko Honkala", "Mathias Berglund", "Tapani Raiko"], "venue": "Neural Information Processing Systems (NIPS),", "citeRegEx": "Rasmus et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Rasmus et al\\.", "year": 2016}, {"title": "Imagenet large scale visual recognition challenge", "author": ["Olga Russakovsky", "Jia Deng", "Hao Su", "Jonathan Krause", "Sanjeev Satheesh", "Sean Ma", "Zhiheng Huang", "Andrej Karpathy", "Aditya Khosla", "Michael Bernstein"], "venue": "International Journal of Computer Vision (IJCV),", "citeRegEx": "Russakovsky et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Russakovsky et al\\.", "year": 2015}, {"title": "Trust region policy optimization", "author": ["John Schulman", "Sergey Levine", "Philipp Moritz", "Michael I Jordan", "Pieter Abbeel"], "venue": "International Conference on Machine Learning (ICML),", "citeRegEx": "Schulman et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Schulman et al\\.", "year": 2015}, {"title": "Knowledge transfer using local features", "author": ["M. Stolle", "C.G. Atkeson"], "venue": "Approximate Dynamic Programming and Reinforcement Learning (ADPRL),", "citeRegEx": "Stolle and Atkeson.,? \\Q2007\\E", "shortCiteRegEx": "Stolle and Atkeson.", "year": 2007}, {"title": "Information regularization with partially labeled data", "author": ["Martin Szummer", "Tommi S Jaakkola"], "venue": "In Neural Information processing systems (NIPS),", "citeRegEx": "Szummer and Jaakkola.,? \\Q2002\\E", "shortCiteRegEx": "Szummer and Jaakkola.", "year": 2002}, {"title": "Transfer learning for reinforcement learning domains: A survey", "author": ["Matthew E. Taylor", "Peter Stone"], "venue": "Journal of Machine Learning Research (JMLR),", "citeRegEx": "Taylor and Stone.,? \\Q2009\\E", "shortCiteRegEx": "Taylor and Stone.", "year": 2009}, {"title": "Tracking-based semi-supervised learning", "author": ["Alex Teichman", "Sebastian Thrun"], "venue": "Robotics: Science and Systems (RSS),", "citeRegEx": "Teichman and Thrun.,? \\Q2007\\E", "shortCiteRegEx": "Teichman and Thrun.", "year": 2007}, {"title": "Lifelong robot learning", "author": ["Sebastian Thrun", "Tom M Mitchell"], "venue": null, "citeRegEx": "Thrun and Mitchell.,? \\Q1995\\E", "shortCiteRegEx": "Thrun and Mitchell.", "year": 1995}, {"title": "Adapting deep visuomotor representations with weak pairwise constraints", "author": ["Eric Tzeng", "Coline Devin", "Judy Hoffman", "Chelsea Finn", "Pieter Abbeel", "Sergey Levine", "Kate Saenko", "Trevor Darrell"], "venue": "Workshop on the Algorithmic Foundations of Robotics (WAFR),", "citeRegEx": "Tzeng et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Tzeng et al\\.", "year": 2016}, {"title": "Augmenting supervised neural networks with unsupervised objectives for large-scale image classification", "author": ["Yuting Zhang", "Kibok Lee", "Honglak Lee"], "venue": "International Conference on Machine Learning (ICML),", "citeRegEx": "Zhang et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Zhang et al\\.", "year": 2016}, {"title": "Learning from labeled and unlabeled data with label propagation", "author": ["Xiaojin Zhu", "Zoubin Ghahramani"], "venue": "Technical report,", "citeRegEx": "Zhu and Ghahramani.,? \\Q2002\\E", "shortCiteRegEx": "Zhu and Ghahramani.", "year": 2002}, {"title": "Introduction to semi-supervised learning", "author": ["Xiaojin Zhu", "Andrew B Goldberg"], "venue": null, "citeRegEx": "Zhu and Goldberg.,? \\Q2009\\E", "shortCiteRegEx": "Zhu and Goldberg.", "year": 2009}, {"title": "Modeling purposeful adaptive behavior with the principle of maximum causal entropy", "author": ["B. Ziebart"], "venue": "PhD thesis,", "citeRegEx": "Ziebart.,? \\Q2010\\E", "shortCiteRegEx": "Ziebart.", "year": 2010}], "referenceMentions": [{"referenceID": 15, "context": "RL has been combined with deep networks to learn policies for problems such as Atari games (Mnih et al., 2015), simple Minecraft tasks (Oh et al.", "startOffset": 91, "endOffset": 110}, {"referenceID": 20, "context": ", 2015), simple Minecraft tasks (Oh et al., 2016), and simulated locomotion (Schulman et al.", "startOffset": 32, "endOffset": 49}, {"referenceID": 24, "context": ", 2016), and simulated locomotion (Schulman et al., 2015).", "startOffset": 34, "endOffset": 57}, {"referenceID": 23, "context": "Methods for training deep, flexible models combined with massive amounts of labeled data are known to enable wide generalization for supervised learning tasks (Russakovsky et al., 2015).", "startOffset": 159, "endOffset": 185}, {"referenceID": 9, "context": "Although our approach is compatible with any choice of reinforcement learning and inverse reinforcement learning algorithm, we use the guided cost learning method in our experimental evaluation, which allows us to evaluate on high-dimensional robotic manipulation tasks with unknown dynamics while using a relatively modest number of samples (Finn et al., 2016).", "startOffset": 342, "endOffset": 361}, {"referenceID": 22, "context": "Semi-supervised learning has been performed with deep models, either by blending unsupervised and supervised objectives (Rasmus et al., 2016; Zhang et al., 2016) or by using generative models, with the labels treated as missing data (Kingma et al.", "startOffset": 120, "endOffset": 161}, {"referenceID": 31, "context": "Semi-supervised learning has been performed with deep models, either by blending unsupervised and supervised objectives (Rasmus et al., 2016; Zhang et al., 2016) or by using generative models, with the labels treated as missing data (Kingma et al.", "startOffset": 120, "endOffset": 161}, {"referenceID": 12, "context": ", 2016) or by using generative models, with the labels treated as missing data (Kingma et al., 2014).", "startOffset": 79, "endOffset": 100}, {"referenceID": 3, "context": "A related but orthogonal problem is transfer learning (Taylor & Stone, 2009; Barrett et al., 2010), which attempts to use prior experience in one domain to improve training performance in another.", "startOffset": 54, "endOffset": 98}, {"referenceID": 18, "context": "Transfer learning has been applied to RL domains for transferring information across environments (Mordatch et al., 2016; Tzeng et al., 2016), robots (Devin et al.", "startOffset": 98, "endOffset": 141}, {"referenceID": 30, "context": "Transfer learning has been applied to RL domains for transferring information across environments (Mordatch et al., 2016; Tzeng et al., 2016), robots (Devin et al.", "startOffset": 98, "endOffset": 141}, {"referenceID": 6, "context": ", 2016), robots (Devin et al., 2016), and tasks (Konidaris & Barto, 2006; Stolle & Atkeson, 2007; Dragan et al.", "startOffset": 16, "endOffset": 36}, {"referenceID": 7, "context": ", 2016), and tasks (Konidaris & Barto, 2006; Stolle & Atkeson, 2007; Dragan et al., 2011; Parisotto et al., 2016; Rusu et al., 2016).", "startOffset": 19, "endOffset": 132}, {"referenceID": 21, "context": ", 2016), and tasks (Konidaris & Barto, 2006; Stolle & Atkeson, 2007; Dragan et al., 2011; Parisotto et al., 2016; Rusu et al., 2016).", "startOffset": 19, "endOffset": 132}, {"referenceID": 9, "context": "We build upon prior methods, including guided cost learning, which propose to learn a cost and a policy simultaneously (Finn et al., 2016; Ho et al., 2016).", "startOffset": 119, "endOffset": 155}, {"referenceID": 10, "context": "We build upon prior methods, including guided cost learning, which propose to learn a cost and a policy simultaneously (Finn et al., 2016; Ho et al., 2016).", "startOffset": 119, "endOffset": 155}, {"referenceID": 4, "context": ", 2016) or by using generative models, with the labels treated as missing data (Kingma et al., 2014). Semi-supervised learning is particularly relevant in robotics and control, where collecting labeled experience on real hardware is expensive. However, while semi-supervised learning has been successful in domains such as object tracking and detection (Teichman & Thrun, 2007), applications to action and control have not been applied to the objective of the task itself. The generalization capabilities of policies learned through RL (and deep RL) has been limited, as pointed out by Oh et al. Oh et al. (2016). That is, typically the settings under which the agent is tested do not vary from those under which it was trained.", "startOffset": 80, "endOffset": 613}, {"referenceID": 1, "context": "A related but orthogonal problem is transfer learning (Taylor & Stone, 2009; Barrett et al., 2010), which attempts to use prior experience in one domain to improve training performance in another. Transfer learning has been applied to RL domains for transferring information across environments (Mordatch et al., 2016; Tzeng et al., 2016), robots (Devin et al., 2016), and tasks (Konidaris & Barto, 2006; Stolle & Atkeson, 2007; Dragan et al., 2011; Parisotto et al., 2016; Rusu et al., 2016). The goal of these approaches is typically to utilize experience in a source domain to learn faster or better in the target domain. Unlike most transfer learning scenarios, we assume that supervision cannot be obtained in many scenarios. We are also not concerned with large, systematic domain shift: we assume that the labeled and unlabeled settings come from the same underlying distribution. Note, however, that the method that we develop could be used for transfer learning problems where the state and reward are consistent across domains. To the best of our knowledge, this paper is the first to provide a practical and tractable algorithm for semi-supervised RL with large, expressive function approximators, and illustrate that such learning actually improves the generalization of the learned policy. However, the idea of semi-supervised reinforcement learning procedures has been previously discussed as a compelling research direction by Christiano (2016) and Amodei et al.", "startOffset": 77, "endOffset": 1460}, {"referenceID": 1, "context": "However, the idea of semi-supervised reinforcement learning procedures has been previously discussed as a compelling research direction by Christiano (2016) and Amodei et al. (2016). To accomplish semi-supervised reinforcement learning, we propose a method that resembles an inverse reinforcement learning (IRL) algorithm, in that it imputes the reward function in the unlabeled settings by learning from the successful trials in the labeled settings.", "startOffset": 161, "endOffset": 182}, {"referenceID": 1, "context": "However, the idea of semi-supervised reinforcement learning procedures has been previously discussed as a compelling research direction by Christiano (2016) and Amodei et al. (2016). To accomplish semi-supervised reinforcement learning, we propose a method that resembles an inverse reinforcement learning (IRL) algorithm, in that it imputes the reward function in the unlabeled settings by learning from the successful trials in the labeled settings. IRL was first introduced by Ng et al. (2000) as the problem of learning reward functions from expert, human demonstrations, typically with the end goal of learning a policy that can succeed from states that are not in the set of demonstrations (Abbeel & Ng, 2004).", "startOffset": 161, "endOffset": 497}, {"referenceID": 1, "context": "However, the idea of semi-supervised reinforcement learning procedures has been previously discussed as a compelling research direction by Christiano (2016) and Amodei et al. (2016). To accomplish semi-supervised reinforcement learning, we propose a method that resembles an inverse reinforcement learning (IRL) algorithm, in that it imputes the reward function in the unlabeled settings by learning from the successful trials in the labeled settings. IRL was first introduced by Ng et al. (2000) as the problem of learning reward functions from expert, human demonstrations, typically with the end goal of learning a policy that can succeed from states that are not in the set of demonstrations (Abbeel & Ng, 2004). We use IRL to infer the reward function underlying a policy previously learned in a small set of labeled scenarios, rather than using expert demonstrations. We build upon prior methods, including guided cost learning, which propose to learn a cost and a policy simultaneously (Finn et al., 2016; Ho et al., 2016). Note that the problem that we are considering is distinct from semi-supervised inverse reinforcement learning Audiffren et al. (2015), which makes use of expert and non-expert trajectories for learning.", "startOffset": 161, "endOffset": 1165}, {"referenceID": 20, "context": "The standard paradigm in reinforcement learning is to learn a policy in the labeled MDPs and apply it directly to new MDPs from the same distribution, hoping that the original policy will generalize (Oh et al., 2016).", "startOffset": 199, "endOffset": 216}, {"referenceID": 34, "context": "In order to perform semi-supervised reinforcement learning, we use the framework of maximum entropy control (Ziebart, 2010; Kappen et al., 2012), sometimes also called linear-solvable MDPs (Dvijotham & Todorov, 2010).", "startOffset": 108, "endOffset": 144}, {"referenceID": 11, "context": "In order to perform semi-supervised reinforcement learning, we use the framework of maximum entropy control (Ziebart, 2010; Kappen et al., 2012), sometimes also called linear-solvable MDPs (Dvijotham & Todorov, 2010).", "startOffset": 108, "endOffset": 144}, {"referenceID": 9, "context": "As shown in prior work, this procedure corresponds to an inverse reinforcement learning algorithm that converges to a policy that matches the performance observed in D\u03c0RL (Finn et al., 2016).", "startOffset": 171, "endOffset": 190}, {"referenceID": 34, "context": "Reward update: Because of the entropy regularized objective in Equation 1, it follows that the samples D\u03c0RL are generated from the following maximum entropy distribution (Ziebart, 2010):", "startOffset": 170, "endOffset": 185}, {"referenceID": 17, "context": "While we could in principle use any policy optimization method in this step, our prototype uses mirror descent guided policy search (MDGPS), a sample-efficient policy optimization method suitable for training complex neural network policies that has been validated on real-world physical robots (Montgomery & Levine, 2016; Montgomery et al., 2016).", "startOffset": 295, "endOffset": 347}, {"referenceID": 9, "context": "Our method is structured similarly to the recently proposed guided cost learning method (Finn et al., 2016), and inherits its convergence properties and theoretical foundations.", "startOffset": 88, "endOffset": 107}, {"referenceID": 4, "context": "For example, in recent RL benchmarks such as the Arcade Learning Environment (Bellemare et al., 2012) and OpenAI Gym (Brockman et al.", "startOffset": 77, "endOffset": 101}, {"referenceID": 13, "context": "The vision task used 3 convolutional layers with 15 filters of size 5\u00d75 each, followed by the spatial feature point transformation proposed by Levine et al. (2016), and lastly 3 fully-connected layers of 20 units each.", "startOffset": 143, "endOffset": 164}, {"referenceID": 9, "context": "The reward function architecture mirrored the architecture as the policy, but using a quadratic norm on the output, as done by Finn et al. (2016).", "startOffset": 127, "endOffset": 146}, {"referenceID": 9, "context": "Indeed, previous work has evaluated similar methods on real physical systems, in the context of inverse RL (Finn et al., 2016) and vision-based policy learning (Levine et al.", "startOffset": 107, "endOffset": 126}, {"referenceID": 14, "context": ", 2016) and vision-based policy learning (Levine et al., 2016).", "startOffset": 41, "endOffset": 62}], "year": 2016, "abstractText": "Deep reinforcement learning (RL) can acquire complex behaviors from low-level inputs, such as images. However, real-world applications of such methods require generalizing to the vast variability of the real world. Deep networks are known to achieve remarkable generalization when provided with massive amounts of labeled data, but can we provide this breadth of experience to an RL agent, such as a robot? The robot might continuously learn as it explores the world around it, even while it is deployed and performing useful tasks. However, this learning requires access to a reward function, to tell the agent whether it is succeeding or failing at its task. Such reward functions are often hard to measure in the real world, especially in domains such as robotics and dialog systems, where the reward could depend on the unknown positions of objects or the emotional state of the user. On the other hand, it is often quite practical to provide the agent with reward functions in a limited set of situations, such as when a human supervisor is present, or in a controlled laboratory setting. Can we make use of this limited supervision, and still benefit from the breadth of experience an agent might collect in the unstructured real world? In this paper, we formalize this problem setting as semisupervised reinforcement learning (SSRL), where the reward function can only be evaluated in a set of \u201clabeled\u201d MDPs, and the agent must generalize its behavior to the wide range of states it might encounter in a set of \u201cunlabeled\u201d MDPs, by using experience from both settings. Our proposed method infers the task objective in the unlabeled MDPs through an algorithm that resembles inverse RL, using the agent\u2019s own prior experience in the labeled MDPs as a kind of demonstration of optimal behavior. We evaluate our method on challenging tasks that require control directly from images, and show that our approach can improve the generalization of a learned deep neural network policy by using experience for which no reward function is available. We also show that our method outperforms direct supervised learning of the reward.", "creator": "LaTeX with hyperref package"}}}