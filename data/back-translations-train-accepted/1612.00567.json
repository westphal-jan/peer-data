{"id": "1612.00567", "review": {"conference": "acl", "VERSION": "v1", "DATE_OF_SUBMISSION": "2-Dec-2016", "title": "Shift-Reduce Constituent Parsing with Neural Lookahead Features", "abstract": "Transition-based models can be fast and accurate for constituent parsing. Compared with chart-based models, they leverage richer features by extracting history information from a parser stack, which spans over non-local constituents. On the other hand, during incremental parsing, constituent information on the right hand side of the current word is not utilized, which is a relative weakness of shift-reduce parsing. To address this limitation, we leverage a fast neural model to extract lookahead features. In particular, we build a bidirectional LSTM model, which leverages the full sentence information to predict the hierarchy of constituents that each word starts and ends. The results are then passed to a strong transition-based constituent parser as lookahead features. The resulting parser gives 1.3% absolute improvement in WSJ and 2.3% in CTB compared to the baseline, given the highest reported accuracies for fully-supervised parsing.", "histories": [["v1", "Fri, 2 Dec 2016 04:55:24 GMT  (196kb,D)", "http://arxiv.org/abs/1612.00567v1", null]], "reviews": [], "SUBJECTS": "cs.CL", "authors": ["jiangming liu", "yue zhang"], "accepted": true, "id": "1612.00567"}, "pdf": {"name": "1612.00567.pdf", "metadata": {"source": "CRF", "title": "Shift-Reduce Constituent Parsing with Neural Lookahead Features", "authors": ["Jiangming Liu", "Yue Zhang"], "emails": ["zhang}@sutd.edu.sg"], "sections": [{"heading": "1 Introduction", "text": "In fact, most of them are able to survive on their own if they do not put themselves in a position to survive on their own."}, {"heading": "2 Baseline System", "text": "We use the parser of Zhu et al. (2013) for a baseline based on the shift-reduce process of Sagae and Lavie (2005) and the beam search strategy of Zhang and Clark (2011) with global perceptron training."}, {"heading": "2.1 The Shift-Reduce System", "text": "Shift-reducing parsers process an input set step by step from left to right. A stack is used to maintain partial phrase structures while the incoming words are arranged in a buffer. At each step, a transitional action is applied to consume an input word or construct a new phrase structure. Transitional actions are \u2022 SHIFT: fold the front word out of the buffer and move it onto the stack. \u2022 REDUCE-L / R-X: fold the top two components from the stack, combine them into a new component with the label X and move the new component onto the stack. \u2022 UNARY-X: fold the top component from the stack, lift it onto a new component X and move the new component onto the stack. \u2022 FINISH: fold the root node from the stack and move the new component onto the stack. \u2022 IDLE: no effective action to a completed state, without elements on the stack or the stack being displayed as a stack for the process."}, {"heading": "2.2 Search and Training", "text": "During initialization, the agenda contains only the initial state [\u03c6, 0, false, 0]. At each step, each state in the agenda is cracked and expanded by applying all valid transitional actions, and the resulting top k states are put back on the agenda (Zhu et al., 2013). The process repeats until the agenda is empty and the best completed state is taken as output. A state's score is the total number of transitional actions applied to its construction: C (\u03b1) = N \u0445 i = 1 \u0445 (\u03b1i) \u00b7 ~ \u03b8 (1) Here, the characteristic vector for the ith action \u03b1i is represented in state item \u03b1. N is the total number of actions in the agenda. The model parameter is trained online using the averaged perceptron algorithm with the early update strategy (Collins and Roark, 2004)."}, {"heading": "2.3 Baseline Features", "text": "Our basic characteristics are taken from Zhu et al. (2013). As shown in Table 1, they include the UNIGRAM, BIGRAM, TRIGRAM characteristics of Zhang and Clark (2009) and the extended characteristics of Zhu et al. (2013)."}, {"heading": "3 Global Lookahead Features", "text": "The basic functions, as mentioned in the introduction, are subject to two limitations: First, they are relatively local to the state and only take into account the adjacent nodes s0 (top level of the stack) and q0 (top level of the buffer); second, they do not take into account predictive information beyond s3 or the syntactic structure of the buffer and sequence. We use an LSTM to capture complete sentence information in linear time and present this global information as a constituent hierarchy for every word in the base line parser. Lookahead functions are extracted from the constituent hierarchy to provide top-down instructions for bottom-up parsing."}, {"heading": "3.1 Constituent Hierarchy", "text": "In a constituency tree, each word can begin or end a constituent hierarchy. As shown in Figure 1, the word \"Das\" ends with a constituent hierarchy \"S \u2192 NP.\" Specifically, it begins with a constituent S at the top level and then with a constituent NP. The word \"Table\" ends a constituent hierarchy \"S \u2192 VP \u2192 PP \u2192 NP.\" Specifically, it ends with a constituent S at the top level and then with a VP (starting with the word \"Like\"), a NP (starting with the noun phrase \"this book\"), a PP (starting with the word \"in\") and finally a NP (starting with the word \"that\"). The extraction of constituent hierarchies for each word is based on non-binding grammars reflecting the beginning and the end in non-binary trees. The constitutional hierarchy is empty (starting with the word \"in\") if the corresponding words do not begin as constituent \u2192 the constituent or the constituent hierarchy (the hierarchy is constituted)."}, {"heading": "3.2 Lookahead Features", "text": "To ensure that the parameter hierarchy is efficient, only simple feature templates are taken into account. The Lookahead features of a state are used for the two top elements on the stack (i.e. s0 and s1) and the buffer (i.e. q0 and q1) instantiated. The new function is defined as to issue the Lookahead feature svektor. The rating of a state in our model is simply in the form of the formula (1) widened: C (i.e. q0 and q1) = =?????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????"}, {"heading": "4 Constituent Hierarchy Prediction", "text": "We propose a novel neural model for predicting the constituent hierarchy. Inspired by the encoder decoder framework for neural machine translation (Bahdanau et al., 2014; Cho et al., 2014), we use aLSTM to capture complete sentence characteristics and another LSTM to generate the constituent hierarchies for each word. Compared to a CRF-based sequence labeling model (Roark and Hollingshead, 2009), the proposed model has three advantages: First, the global characteristics can be represented automatically. Second, the exponentially large number of labels can be avoided if constituent hierarchies are treated as distinct labels. Third, the model size is relatively small and does not have much influence on the final parser model. As shown in Figure 4, the neural network consists of three main layers, namely the input layer, the encode layer, the sign layer, the encode layer, the encode layer, the encode layer and the decode layer used to constitute the input layer and its structure."}, {"heading": "4.1 Input Layer", "text": "The input layer generates a dense vector representation for each input word. We use character embedding to alleviate OOV problems with word embedding (Ballesteros et al., 2015; Santos and Zadrozny, 2014; Kim et al., 2016), linking a characteristic embedding of the representation of a word to its word embedding. Formally, the input representation xi of the word wi is calculated by: xi = xwi, ci att = x, where xwi is a word embedding vector of the word wi according to an embedding table, ci att is a character embedding form of the word wi, cij is the jth character in wi, and \u03b1ij is the contribution of the character cij to ci att calculated by: \u03b1ij = ef (xwi, cij), k e f (xwi, cik) f is a non-linear transformation function based on the tanh function."}, {"heading": "4.2 Encoder Layer", "text": "The encoder first uses a window strategy to represent input nodes with their corresponding local context nodes. Formally, the encoder looks at the input set and generates hidden units for each input word using a recursive neural network (RNN) that represents features of the word from the global sequence. Formally, the encoder mitigates the problem of the disappearing input set x \u2032 1, x \u2032 2,..., x \u2032 n for the sentence w1, w2,..., wn, the RNN layer calculates a hidden node sequence h1, h2,..., similar. Long Short-Term Memory (LSTM) mitigates the disappearing gradient problem in RNN training by introducing gates (i.e. input i, forgot f and output o) and a cell storage vector c. We use the variation of graves and Schmidhuber (hi) by following the 1997 gradient layers (Wi-1) (gradient problems = 1 Wi-N)."}, {"heading": "4.3 Decoder Layer", "text": "The hidden layer decoder uses two different LSTMs to generate the s-type and e-type sequences of the constituent labels from each encoder hidden output, or, as shown in Figure 4. Each constituent hierarchy is generated from bottom to top. Specifically, a sequence of state vectors is generated repetitively, with each state providing a constituent label. The process starts with a ~ 0 state vector and ends when a NULL constituent is created. The recursive state transition process is achieved using the LSTM model, using the hidden vectors of the encoder layer for the context. Formally, the value of the jth state unit sij of the LSTM for the word wi is reached by: sij = f (sij \u2212 1, cij, hi) 1, compressing the context cij = k state ijels."}, {"heading": "4.4 Training", "text": "We use two separate models to assign the S-type or E-type names. To form each hierarchical predictor, we minimize the following training goal: L (\u03b8) = \u2212 T \u2211 i Zi \u0445 j log pijo + \u03bb 2 | | \u03b8 | | 2, where T is the size of the sentence, Zi is the depth of the constituent hierarchy of the word wi and pijo stands for p (yij = o), which is specified by the SOFTMAX function, and o is the golden label. We apply backpropagation by means of dynamic stochastic gradient descent (Sutskever et al., 2013) with a learning rate of \u03b7 = 0.01 for optimization and regulation parameters \u03bb = 10 \u2212 6."}, {"heading": "5 Experiments", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "5.1 Experiment Settings", "text": "English data comes from the corpus of the Wall Street Journal (WSJ) of Penn Treebank (Marcus et al., 1993). We use Sections 2-21 for training, Section 24 for system development, and Section 23 for final performance evaluation. Chinese data comes from version 5.1 of Penn Chinese Treebank (CTB) (Xue et al., 2005). We use Articles 001-270 and 440-1151 for training, Articles 301-325 for system development, and Articles 271-300 for final performance evaluation. For English and Chinese data, we use ZPar2 for POS marking and ten times jackknifing to assign auto-POS tags to training data. In addition, we use ten times jackknifing to assign auto-constituent hierarchies to training data. We use F1 score to evaluate the constituent hierarchy as a prediction."}, {"heading": "5.2 Model Settings", "text": "Hyperparameters are selected on the basis of development tests, and the values are in Table 3.For the shift-reduce parser for constituencies, we set the bar size for both education and decoding to 16, thus achieving a good compromise between efficiency2https: / / github.com / SUTDNLP / ZPar 3http: / / nlp.cs.nyu.edu / evalband accuracy (Zhu et al., 2013). The optimal training number is determined on the development sets."}, {"heading": "5.3 Results of Constituent Hierarchy Prediction", "text": "Table 4 shows the results of predicting the constituent hierarchy, where word and character embeddings are randomly initialized and refined during training; the third column shows the accuracy of analyzing the development when using labels for predictive functions; Table 4 shows that as the number of hidden hierarchies increases, both the prediction of the S-type and the prediction of the constituent hierarchy of the e-type improve; the accuracy of predicting the e-type is relatively lower due to the right-branching in the tree base, making the hierarchies of the e-type longer than the hierarchies of the e-type. In addition, a 3-layer LSTM does not bring significant improvements compared to the 2-layer LSTM. To achieve a compromise between efficiency and accuracy, we select the 2-layer LSTM as our constituent hierarchy predictableness; Table 5 shows the results of the constituent architecture are given by the different architecture, without a prediction of the constituent architecture, or a preconstituting one."}, {"heading": "5.4 Final Results", "text": "As shown in Table 64, our model achieves an improvement of 1.3% compared to the base saver with fully supervised learning (Zhu et al., 2013) by 0.6% compared to the state-of-the-art, fully supervised system (Carreras et al., 2008; Shindo et al., 2012). In addition, our fully supervised model also catches up with many state-of-the-art, semi-supervised models (Zhu et al., 2013; Huang et al., 2009; Huang et al., 2010; Durrett and Klein, 2015) by reaching 91.7% in the WSJ test kit. The size of our model is much smaller than the semi-supervised model by Zhu et al. (2013), 4We treat the methods as semi-supervised when using pre-supervised word beds."}, {"heading": "5.5 Comparison of Speed", "text": "Table 8 shows the runtimes of different parsers on test sets on an Intel 2.2 GHz processor with 16G memory. Our parsers are much faster than the related parsers using the same shift-reduce framework (Sagae and Lavie, 2005; Sagae and Lavie, 2006). Compared to the base parser, our parser offers significant improvements in accuracy (90.4% to 91.7% F1) at a speed of 79.2 sets per second, as opposed to 89.5 sets per second for the standard WSJ benchmark."}, {"heading": "6 Errors Analysis", "text": "We perform error analyses by measurement: Analysis of accuracies based on different phrase types, components of different spans and sentence lengths."}, {"heading": "6.1 Phrase Type", "text": "As the results show, the parser with lookahead functions achieves improvements on all common phrase types, but there are relatively more improvements on the constituent VP, S, SBAR and WHNP. The constituent hierarchy predictor performs relatively better on the s-type labels for constituencies VP and WHNP, which are prone to errors by the base system. The constituent hierarchy can provide the constituent parser with guidance on how to tackle the challenges. Compared to the s-type constituent hierarchy, the e-type constituent hierarchy is relatively difficult to predict, especially for constituencies with long ranges such as VP, S and SBAR. Nevertheless, the e-type constituent hierarchies also benefit from predicting constituencies with relatively low accuracy. The costs of this step are far lower than the costs of parsing and can be eliminated by shifting the constituent definition."}, {"heading": "6.2 Span Length", "text": "Figure 5 shows the comparison of the two parsers for components with different spans. As the results show, lookahead functions are useful for both large and small spans, while the performance gap between the two parsers widens as the span increases, reflecting the usefulness of long-distance information captured by the constituent hierarchy predictor and lookahead functions."}, {"heading": "6.3 Sentence Length", "text": "Figure 6 shows the comparison of the two parsers for sentences of different lengths. As the results show, the parser with lookahead functions outperforms the basic system for both short and long sentences. In addition, the performance gap between the two parsers widens the longer the sentence increases. Constituent hierarchy predictors generate hierarchical components for each input word based on global information. For longer sentences, the predictors yield deeper stock hierarchies and provide corresponding lookahead characteristics. As a result, the performance of the parser with lookahead characteristics decreases slower compared to the baseline parser as the sentences lengthen."}, {"heading": "7 Related Work", "text": "Our forward-looking features are similar in spirit to the terms of Roark and Hollingshead (2009) and Zhang et al. (2010b), which infer the maximum length of each component that a particular word can begin or end. However, our method is different in three main perspectives. First, the results are inserted into a transitional parser with sparse local word window functions, which is then used as hard constraints on a diagram. Second, not only the size of the components, but also the constituent hierarchy is identified for each word. Third, the results are added as soft features that then parse to a diagram.Our concepts of constituent hierarchies are similar to the work of supertagging. For lexicalized grammars such as Combinatory Categorial Grammar (CCG), Tree-Adjoining Grammar (TAG)."}, {"heading": "8 Conclusion", "text": "We proposed a novel constituent hierarchy predictor based on recurrent neural networks that aims to capture global sentential information, and the resulting constituent hierarchies are sent as lookahead features to a baseline shiftreduce parser that takes into account the limitations of shift-reduce parsers by not using the right-hand syntax for local decisions, but maintaining the same model size and speed. The resulting fully monitored parser outperforms the modern baseline parser by reaching 91.7% F1 in the standard WSJ rating and 85.5% F1 in the standard CTB rating."}, {"heading": "Acknowledgments", "text": "We thank the anonymous reviewers for their detailed and constructed comments. This work is supported by T2MOE 201301 from Singapore M-O-E. Yue Zhang is the corresponding author."}], "references": [{"title": "Neural Machine Translation by Jointly Learning to Align and Translate", "author": ["Dzmitry Bahdanau", "Kyunghyun Cho", "Yoshua Bengio."], "venue": "CoRR.", "citeRegEx": "Bahdanau et al\\.,? 2014", "shortCiteRegEx": "Bahdanau et al\\.", "year": 2014}, {"title": "Improved Transition-Based Parsing by Modeling Characters instead of Words with LSTMs", "author": ["Miguel Ballesteros", "Chris Dyer", "Noah A Smith."], "venue": "EMNLP, pages 349\u2013359.", "citeRegEx": "Ballesteros et al\\.,? 2015", "shortCiteRegEx": "Ballesteros et al\\.", "year": 2015}, {"title": "Supertagging: an approach to almost parsing", "author": ["Srinivas Bangalore", "Aravind K Joshi."], "venue": "Computational Linguistics, 25(2):237\u2013265, June.", "citeRegEx": "Bangalore and Joshi.,? 1999", "shortCiteRegEx": "Bangalore and Joshi.", "year": 1999}, {"title": "On the parameter space of generative lexicalized statistical parsing models", "author": ["Daniel M Bikel."], "venue": "Ph.D Thesis, University of Pennsylvania.", "citeRegEx": "Bikel.,? 2004", "shortCiteRegEx": "Bikel.", "year": 2004}, {"title": "TAG, dynamic programming, and the perceptron for efficient, feature-rich parsing", "author": ["Xavier Carreras", "Michael Collins", "Terry Koo."], "venue": "CoNLL, pages 9\u201316, Morristown, NJ, USA. Association for Computational Linguistics.", "citeRegEx": "Carreras et al\\.,? 2008", "shortCiteRegEx": "Carreras et al\\.", "year": 2008}, {"title": "Coarseto-Fine n-Best Parsing and MaxEnt Discriminative Reranking", "author": ["Eugene Charniak", "Mark Johnson."], "venue": "ACL.", "citeRegEx": "Charniak and Johnson.,? 2005", "shortCiteRegEx": "Charniak and Johnson.", "year": 2005}, {"title": "A Maximum-Entropy-Inspired Parser", "author": ["Eugene Charniak."], "venue": "ANLP, pages 132\u2013139.", "citeRegEx": "Charniak.,? 2000", "shortCiteRegEx": "Charniak.", "year": 2000}, {"title": "A Fast and Accurate Dependency Parser using Neural Networks", "author": ["Danqi Chen", "Christopher Manning."], "venue": "EMNLP, pages 740\u2013750, Stroudsburg, PA, USA. Association for Computational Linguistics.", "citeRegEx": "Chen and Manning.,? 2014", "shortCiteRegEx": "Chen and Manning.", "year": 2014}, {"title": "Learning Phrase Representations using RNN Encoder-Decoder for Statistical Machine Translation", "author": ["Kyunghyun Cho", "Bart van Merrienboer", "\u00c7aglar G\u00fcl\u00e7ehre", "Dzmitry Bahdanau", "Fethi Bougares", "Holger Schwenk", "Yoshua Bengio."], "venue": "EMNLP, pages 1724\u20131734.", "citeRegEx": "Cho et al\\.,? 2014", "shortCiteRegEx": "Cho et al\\.", "year": 2014}, {"title": "The importance of supertagging for wide-coverage CCG parsing", "author": ["Stephen Clark", "James R Curran."], "venue": "COLING, pages 282\u2013288, Morristown, NJ, USA, August. University of Edinburgh, Association for Computational Linguistics.", "citeRegEx": "Clark and Curran.,? 2004", "shortCiteRegEx": "Clark and Curran.", "year": 2004}, {"title": "Supertagging for combinatory categorial grammar", "author": ["Stephen Clark."], "venue": "Proceedings of the Sixth International Workshop on Tree Adjoining Grammar and Related Frameworks, pages 101\u2013106, Universita di Venezia.", "citeRegEx": "Clark.,? 2002", "shortCiteRegEx": "Clark.", "year": 2002}, {"title": "Incremental parsing with the perceptron algorithm", "author": ["Michael Collins", "Brian Roark."], "venue": "ACL, pages", "citeRegEx": "Collins and Roark.,? 2004", "shortCiteRegEx": "Collins and Roark.", "year": 2004}, {"title": "Head-driven statistical models for natural language parsing", "author": ["Michael Collins."], "venue": "Computational linguistics, 29(4):589\u2013637.", "citeRegEx": "Collins.,? 2003", "shortCiteRegEx": "Collins.", "year": 2003}, {"title": "Enhancing Performance of Lexicalised Grammars", "author": ["R Dridan", "V Kordoni", "J Nicholson."], "venue": "ACL.", "citeRegEx": "Dridan et al\\.,? 2008", "shortCiteRegEx": "Dridan et al\\.", "year": 2008}, {"title": "Neural CRF Parsing", "author": ["Greg Durrett", "Dan Klein."], "venue": "ACL, pages 302\u2013312.", "citeRegEx": "Durrett and Klein.,? 2015", "shortCiteRegEx": "Durrett and Klein.", "year": 2015}, {"title": "TransitionBased Dependency Parsing with Stack Long ShortTerm Memory", "author": ["Chris Dyer", "Miguel Ballesteros", "Wang Ling", "Austin Matthews", "Noah A Smith."], "venue": "ACL-IJCNLP, pages 334\u2013343.", "citeRegEx": "Dyer et al\\.,? 2015", "shortCiteRegEx": "Dyer et al\\.", "year": 2015}, {"title": "Recurrent Neural Network Grammars", "author": ["Chris Dyer", "Adhiguna Kuncoro", "Miguel Ballesteros", "Noah A Smith."], "venue": "NAACL, pages 199 \u2013 209.", "citeRegEx": "Dyer et al\\.,? 2016", "shortCiteRegEx": "Dyer et al\\.", "year": 2016}, {"title": "Parsing Inside-Out", "author": ["Joshua Goodman."], "venue": "PhD thesis.", "citeRegEx": "Goodman.,? 1998", "shortCiteRegEx": "Goodman.", "year": 1998}, {"title": "Offline Handwriting Recognition with Multidimensional Recurrent Neural Networks", "author": ["Alex Graves", "J\u00fcrgen Schmidhuber."], "venue": "NIPS, pages 545\u2013552.", "citeRegEx": "Graves and Schmidhuber.,? 2008", "shortCiteRegEx": "Graves and Schmidhuber.", "year": 2008}, {"title": "Hybrid speech recognition with Deep Bidirectional LSTM", "author": ["Alex Graves", "Navdeep Jaitly", "Abdel-rahman Mohamed."], "venue": "IEEE Workshop on Automatic Speech Recognition & Understanding (ASRU), pages 273\u2013278. IEEE.", "citeRegEx": "Graves et al\\.,? 2013", "shortCiteRegEx": "Graves et al\\.", "year": 2013}, {"title": "Long Short-Term Memory", "author": ["Sepp Hochreiter", "J\u00fcrgen Schmidhuber."], "venue": "Neural Computation, 9(8):1735\u20131780, November.", "citeRegEx": "Hochreiter and Schmidhuber.,? 1997", "shortCiteRegEx": "Hochreiter and Schmidhuber.", "year": 1997}, {"title": "SelfTraining PCFG Grammars with Latent Annotations Across Languages", "author": ["Zhongqiang Huang", "Mary P Harper."], "venue": "EMNLP, pages 832\u2013841.", "citeRegEx": "Huang and Harper.,? 2009", "shortCiteRegEx": "Huang and Harper.", "year": 2009}, {"title": "Self-Training with Products of Latent Variable Grammars", "author": ["Zhongqiang Huang", "Mary P Harper", "Slav Petrov."], "venue": "EMNLP, pages 12\u201322.", "citeRegEx": "Huang et al\\.,? 2010", "shortCiteRegEx": "Huang et al\\.", "year": 2010}, {"title": "Forest Reranking: Discriminative Parsing with Non-Local Features", "author": ["Liang Huang."], "venue": "ACL, pages 586\u2013 594.", "citeRegEx": "Huang.,? 2008", "shortCiteRegEx": "Huang.", "year": 2008}, {"title": "Character-Aware Neural Language Models", "author": ["Yoon Kim", "Yacine Jernite", "David Sontag", "Alexander M Rush."], "venue": "AAAI.", "citeRegEx": "Kim et al\\.,? 2016", "shortCiteRegEx": "Kim et al\\.", "year": 2016}, {"title": "Faster parsing by supertagger adaptation", "author": ["Jonathan K Kummerfeld", "Jessika Roesner", "Tim Dawborn", "James Haggerty", "James R Curran", "Stephen Clark."], "venue": "ACL, pages 345\u2013355. University of Cambridge, Association for Computational Linguistics, July.", "citeRegEx": "Kummerfeld et al\\.,? 2010", "shortCiteRegEx": "Kummerfeld et al\\.", "year": 2010}, {"title": "Multi-task Sequence to Sequence Learning", "author": ["Minh-Thang Luong", "Quoc V Le", "Ilya Sutskever", "Oriol Vinyals", "Lukasz Kaiser."], "venue": "ICLR.", "citeRegEx": "Luong et al\\.,? 2015", "shortCiteRegEx": "Luong et al\\.", "year": 2015}, {"title": "Building a Large Annotated Corpus of English: The Penn Treebank", "author": ["Mitchell P Marcus", "Beatrice Santorini", "Mary Ann Marcinkiewicz."], "venue": "Computational Linguistics, 19(2):313\u2013330.", "citeRegEx": "Marcus et al\\.,? 1993", "shortCiteRegEx": "Marcus et al\\.", "year": 1993}, {"title": "Effective self-training for parsing", "author": ["David McClosky", "Eugene Charniak", "Mark Johnson."], "venue": "HLTNAACL, pages 152\u2013159, Morristown, NJ, USA. Association for Computational Linguistics.", "citeRegEx": "McClosky et al\\.,? 2006", "shortCiteRegEx": "McClosky et al\\.", "year": 2006}, {"title": "Extremely lexicalized models for accurate and fast HPSG parsing", "author": ["Takashi Ninomiya", "Takuya Matsuzaki", "Yoshimasa Tsuruoka", "Yusuke Miyao", "Jun\u2019ichi Tsujii"], "venue": "In EMNLP,", "citeRegEx": "Ninomiya et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Ninomiya et al\\.", "year": 2006}, {"title": "Improved Inference for Unlexicalized Parsing", "author": ["Slav Petrov", "Dan Klein."], "venue": "HLT-NAACL, pages 404\u2013411.", "citeRegEx": "Petrov and Klein.,? 2007", "shortCiteRegEx": "Petrov and Klein.", "year": 2007}, {"title": "A Linear Observed Time Statistical Parser Based on Maximum Entropy Models", "author": ["Adwait Ratnaparkhi."], "venue": "EMNLP.", "citeRegEx": "Ratnaparkhi.,? 1997", "shortCiteRegEx": "Ratnaparkhi.", "year": 1997}, {"title": "Linear Complexity Context-Free Parsing Pipelines via Chart Constraints", "author": ["Brian Roark", "Kristy Hollingshead."], "venue": "HLT-NAACL, pages 647\u2013655.", "citeRegEx": "Roark and Hollingshead.,? 2009", "shortCiteRegEx": "Roark and Hollingshead.", "year": 2009}, {"title": "A classifier-based parser with linear run-time complexity", "author": ["Kenji Sagae", "Alon Lavie."], "venue": "IWPT, pages 125\u2013132, Morristown, NJ, USA. Association for Computational Linguistics.", "citeRegEx": "Sagae and Lavie.,? 2005", "shortCiteRegEx": "Sagae and Lavie.", "year": 2005}, {"title": "Parser combination by reparsing", "author": ["Kenji Sagae", "Alon Lavie."], "venue": "HLT-NAACL, pages 129\u2013132, Morristown, NJ, USA. Association for Computational Linguistics.", "citeRegEx": "Sagae and Lavie.,? 2006", "shortCiteRegEx": "Sagae and Lavie.", "year": 2006}, {"title": "Learning character-level representations for part-of-speech tagging", "author": ["Cicero D Santos", "Bianca Zadrozny."], "venue": "ICML, pages 1818\u20131826.", "citeRegEx": "Santos and Zadrozny.,? 2014", "shortCiteRegEx": "Santos and Zadrozny.", "year": 2014}, {"title": "Bidirectional recurrent neural networks", "author": ["Mike Schuster", "Kuldip K Paliwal."], "venue": "Signal Processing, IEEE transaction, 45(11):2673\u20132681.", "citeRegEx": "Schuster and Paliwal.,? 1997", "shortCiteRegEx": "Schuster and Paliwal.", "year": 1997}, {"title": "Bayesian Symbol-Refined Tree Substitution Grammars for Syntactic Parsing", "author": ["Hiroyuki Shindo", "Yusuke Miyao", "Akinori Fujino", "Masaaki Nagata."], "venue": "ACL, pages 440\u2013448.", "citeRegEx": "Shindo et al\\.,? 2012", "shortCiteRegEx": "Shindo et al\\.", "year": 2012}, {"title": "Parsing with Compositional Vector Grammars", "author": ["Richard Socher", "John Bauer", "Christopher D Manning", "Andrew Y Ng."], "venue": "ACL, pages 455\u2013465.", "citeRegEx": "Socher et al\\.,? 2013", "shortCiteRegEx": "Socher et al\\.", "year": 2013}, {"title": "On the importance of initialization and momentum in deep learning", "author": ["Ilya Sutskever", "James Martens", "George E Dahl", "Geoffrey E Hinton."], "venue": "ICML, pages 1139\u20131147.", "citeRegEx": "Sutskever et al\\.,? 2013", "shortCiteRegEx": "Sutskever et al\\.", "year": 2013}, {"title": "Sequence to Sequence Learning with Neural Networks", "author": ["Ilya Sutskever", "Oriol Vinyals", "Quoc V Le."], "venue": "NIPS, pages 3104\u20133112.", "citeRegEx": "Sutskever et al\\.,? 2014", "shortCiteRegEx": "Sutskever et al\\.", "year": 2014}, {"title": "Learning with Lookahead: Can History-Based Models Rival Globally Optimized Models", "author": ["Yoshimasa Tsuruoka", "Yusuke Miyao", "Jun\u2019ichi Kazama"], "venue": "In CoNLL,", "citeRegEx": "Tsuruoka et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Tsuruoka et al\\.", "year": 2011}, {"title": "Supertagging with LSTMs", "author": ["A Vaswani", "Y Bisk", "K Sagae."], "venue": "NAACL.", "citeRegEx": "Vaswani et al\\.,? 2016", "shortCiteRegEx": "Vaswani et al\\.", "year": 2016}, {"title": "Grammar as a Foreign Language", "author": ["Oriol Vinyals", "Lukasz Kaiser", "Terry Koo", "Slav Petrov", "Ilya Sutskever", "Geoffrey E Hinton."], "venue": "NIPS, pages 2773\u2013 2781.", "citeRegEx": "Vinyals et al\\.,? 2015", "shortCiteRegEx": "Vinyals et al\\.", "year": 2015}, {"title": "Joint POS Tagging and Transition-based Constituent Parsing in Chinese with Non-local Features", "author": ["Zhiguo Wang", "Nianwen Xue."], "venue": "ACL, pages 733\u2013742, Stroudsburg, PA, USA. Association for Computational Linguistics.", "citeRegEx": "Wang and Xue.,? 2014", "shortCiteRegEx": "Wang and Xue.", "year": 2014}, {"title": "Feature Optimization for Constituent Parsing via Neural Networks", "author": ["Zhiguo Wang", "Haitao Mi", "Nianwen Xue."], "venue": "ACL-IJCNLP, pages 1138\u20131147, Stroudsburg, PA, USA. Association for Computational Linguistics.", "citeRegEx": "Wang et al\\.,? 2015", "shortCiteRegEx": "Wang et al\\.", "year": 2015}, {"title": "Transitionbased Neural Constituent Parsing", "author": ["Taro Watanabe", "Eiichiro Sumita."], "venue": "ACL, pages 1169\u20131179.", "citeRegEx": "Watanabe and Sumita.,? 2015", "shortCiteRegEx": "Watanabe and Sumita.", "year": 2015}, {"title": "CCG Supertagging with a Recurrent Neural Network", "author": ["Wenduan Xu", "Michael Auli", "Stephen Clark."], "venue": "ACL-IJCNLP, pages 250\u2013255, Stroudsburg, PA, USA. Association for Computational Linguistics.", "citeRegEx": "Xu et al\\.,? 2015", "shortCiteRegEx": "Xu et al\\.", "year": 2015}, {"title": "The Penn Chinese TreeBank: Phrase structure annotation of a large corpus", "author": ["Naiwen Xue", "Fei Xia", "Fudong Chiou", "MarTa Palmer."], "venue": "Natural Language Engineering, 11(2):207\u2013238.", "citeRegEx": "Xue et al\\.,? 2005", "shortCiteRegEx": "Xue et al\\.", "year": 2005}, {"title": "Transition-based parsing of the Chinese treebank using a global discriminative model", "author": ["Yue Zhang", "Stephen Clark."], "venue": "ICPT, pages 162\u2013171, Morristown, NJ, USA. Association for Computational Linguistics.", "citeRegEx": "Zhang and Clark.,? 2009", "shortCiteRegEx": "Zhang and Clark.", "year": 2009}, {"title": "Syntactic processing using the generalized perceptron and beam search", "author": ["Yue Zhang", "Stephen Clark."], "venue": "Computational linguistics, 37(1):105\u2013151.", "citeRegEx": "Zhang and Clark.,? 2011", "shortCiteRegEx": "Zhang and Clark.", "year": 2011}, {"title": "A simple approach for HPSG supertagging using dependency information", "author": ["Yao-zhong Zhang", "Takuya Matsuzaki", "Jun\u2019ichi Tsujii"], "venue": "In NAACL-HLT,", "citeRegEx": "Zhang et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Zhang et al\\.", "year": 2010}, {"title": "Chart Pruning for Fast Lexicalised-Grammar Parsing", "author": ["Yue Zhang", "Byung-Gyu Ahn", "Stephen Clark", "Curt Van Wyk", "James R Curran", "Laura Rimell."], "venue": "COLING, pages 1471\u20131479.", "citeRegEx": "Zhang et al\\.,? 2010b", "shortCiteRegEx": "Zhang et al\\.", "year": 2010}, {"title": "A Neural Probabilistic Structured-Prediction Model for Transition-Based Dependency Parsing", "author": ["Hao Zhou", "Yue Zhang", "Shujian Huang", "Jiajun Chen."], "venue": "ACL, pages 1213\u20131222.", "citeRegEx": "Zhou et al\\.,? 2015", "shortCiteRegEx": "Zhou et al\\.", "year": 2015}, {"title": "Fast and Accurate Shift-Reduce Constituent Parsing", "author": ["Muhua Zhu", "Yue Zhang", "Wenliang Chen", "Min Zhang", "Jingbo Zhu."], "venue": "ACL, pages 434\u2013443.", "citeRegEx": "Zhu et al\\.,? 2013", "shortCiteRegEx": "Zhu et al\\.", "year": 2013}], "referenceMentions": [{"referenceID": 33, "context": "Pioneering models rely on a classifier to make local decisions, searching greedily for local transitions to build a parse tree (Sagae and Lavie, 2005).", "startOffset": 127, "endOffset": 150}, {"referenceID": 27, "context": "The model gives state-of-the-art accuracies at a speed of 89 sentences per second on the standard WSJ benchmark (Marcus et al., 1993).", "startOffset": 112, "endOffset": 133}, {"referenceID": 41, "context": "Such lookahead features (Tsuruoka et al., 2011) correspond to the outside scores in chart parsing (Goodman, 1998).", "startOffset": 24, "endOffset": 47}, {"referenceID": 17, "context": ", 2011) correspond to the outside scores in chart parsing (Goodman, 1998).", "startOffset": 58, "endOffset": 73}, {"referenceID": 31, "context": "Pioneering models rely on a classifier to make local decisions, searching greedily for local transitions to build a parse tree (Sagae and Lavie, 2005). Zhu et al. (2013) use a beam search framework, which preserves linear time complexity of greedy search, while alleviating the disadvantage of error propagation.", "startOffset": 128, "endOffset": 170}, {"referenceID": 26, "context": "The model gives state-of-the-art accuracies at a speed of 89 sentences per second on the standard WSJ benchmark (Marcus et al., 1993). Zhu et al. (2013) exploit rich features by extracting history information from a parser stack, which spans over non-local constituents.", "startOffset": 113, "endOffset": 153}, {"referenceID": 32, "context": "Our idea is inspired by the work of Roark and Hollingshead (2009) and Zhang et al.", "startOffset": 36, "endOffset": 66}, {"referenceID": 32, "context": "Our idea is inspired by the work of Roark and Hollingshead (2009) and Zhang et al. (2010b), which shows that shallow syntactic information gathered over the word sequence can be utilized for improving chart parsing speed without losing accuracies.", "startOffset": 36, "endOffset": 91}, {"referenceID": 32, "context": "Our idea is inspired by the work of Roark and Hollingshead (2009) and Zhang et al. (2010b), which shows that shallow syntactic information gathered over the word sequence can be utilized for improving chart parsing speed without losing accuracies. For example, Roark and Hollingshead (2009) predict constituent boundary information on words as a pre-processing step, and use such information to prune the chart.", "startOffset": 36, "endOffset": 291}, {"referenceID": 32, "context": "Different from that of Roark and Hollingshead (2009), we collect lookahead constituent information for shift-reduce parsing, rather than pruning information for chart parsing.", "startOffset": 23, "endOffset": 53}, {"referenceID": 20, "context": "In particular, a LSTM (Hochreiter and Schmidhuber, 1997) is used to learn global features automatically from the input words.", "startOffset": 22, "endOffset": 56}, {"referenceID": 0, "context": "For each word, a second LSTM is then used to generate the constituent hierarchies greedily using features from the hidden layer of the first LSTM, in the same way as a neural language model decoder generating output sentences for machine translation (Bahdanau et al., 2014).", "startOffset": 250, "endOffset": 273}, {"referenceID": 27, "context": "In the standard WSJ (Marcus et al., 1993) and CTB 5.", "startOffset": 20, "endOffset": 41}, {"referenceID": 48, "context": "1 tests (Xue et al., 2005), our parser gives 1.", "startOffset": 8, "endOffset": 26}, {"referenceID": 54, "context": "baseline of Zhu et al. (2013), resulting in a accuracy of 91.", "startOffset": 12, "endOffset": 30}, {"referenceID": 49, "context": "We adopt the parser of Zhu et al. (2013) for a baseline, which is based on the shift-reduce process of Sagae and Lavie (2005) and the beam search strategy of Zhang and Clark (2011) with the global perceptron training.", "startOffset": 23, "endOffset": 41}, {"referenceID": 32, "context": "(2013) for a baseline, which is based on the shift-reduce process of Sagae and Lavie (2005) and the beam search strategy of Zhang and Clark (2011) with the global perceptron training.", "startOffset": 69, "endOffset": 92}, {"referenceID": 10, "context": "(2013) for a baseline, which is based on the shift-reduce process of Sagae and Lavie (2005) and the beam search strategy of Zhang and Clark (2011) with the global perceptron training.", "startOffset": 134, "endOffset": 147}, {"referenceID": 54, "context": "At each step, each state in the agenda is popped and expanded by applying all valid transition actions, and the top k resulting states are put back onto the agenda (Zhu et al., 2013).", "startOffset": 164, "endOffset": 182}, {"referenceID": 11, "context": "The model parameter set ~ \u03b8 is trained online using the averaged perceptron algorithm with the earlyupdate strategy (Collins and Roark, 2004).", "startOffset": 116, "endOffset": 141}, {"referenceID": 51, "context": "Our baseline features are taken from Zhu et al. (2013). As shown in Table 1, they include the UNIGRAM, BIGRAM, TRIGRAM features of Zhang and Clark (2009) and the extended features of Zhu et al.", "startOffset": 37, "endOffset": 55}, {"referenceID": 10, "context": "As shown in Table 1, they include the UNIGRAM, BIGRAM, TRIGRAM features of Zhang and Clark (2009) and the extended features of Zhu et al.", "startOffset": 85, "endOffset": 98}, {"referenceID": 10, "context": "As shown in Table 1, they include the UNIGRAM, BIGRAM, TRIGRAM features of Zhang and Clark (2009) and the extended features of Zhu et al. (2013).", "startOffset": 85, "endOffset": 145}, {"referenceID": 0, "context": "Inspired by the encoder-decoder framework for neural machine translation (Bahdanau et al., 2014; Cho et al., 2014), we use a LSTM to capture full sentence features, and another LSTM to generate the constituent hierarchies for each word.", "startOffset": 73, "endOffset": 114}, {"referenceID": 8, "context": "Inspired by the encoder-decoder framework for neural machine translation (Bahdanau et al., 2014; Cho et al., 2014), we use a LSTM to capture full sentence features, and another LSTM to generate the constituent hierarchies for each word.", "startOffset": 73, "endOffset": 114}, {"referenceID": 32, "context": "Compared with a CRF-based sequence labelling model (Roark and Hollingshead, 2009), the proposed model has three advantages.", "startOffset": 51, "endOffset": 81}, {"referenceID": 0, "context": "The input layer represents each word using its characters and token information; the encoder hidden layer uses a bidirectional recurrent neural network structure to learn global features from the sentence; and the decoder layer predicts constituent hierarchies according to the encoder layer features, by using attention mechanism (Bahdanau et al., 2014) to softly compute the contribution of each hidden unit of the encoder.", "startOffset": 331, "endOffset": 354}, {"referenceID": 1, "context": "We use character embeddings to alleviate OOV problems in word embeddings (Ballesteros et al., 2015; Santos and Zadrozny, 2014; Kim et al., 2016), concatenating a characterembedding representation of a word to its word embedding.", "startOffset": 73, "endOffset": 144}, {"referenceID": 35, "context": "We use character embeddings to alleviate OOV problems in word embeddings (Ballesteros et al., 2015; Santos and Zadrozny, 2014; Kim et al., 2016), concatenating a characterembedding representation of a word to its word embedding.", "startOffset": 73, "endOffset": 144}, {"referenceID": 24, "context": "We use character embeddings to alleviate OOV problems in word embeddings (Ballesteros et al., 2015; Santos and Zadrozny, 2014; Kim et al., 2016), concatenating a characterembedding representation of a word to its word embedding.", "startOffset": 73, "endOffset": 144}, {"referenceID": 18, "context": "We use the variation of Graves and Schmidhuber (2008). Formally, the values in the LSTM hidden layers are computed as follows:", "startOffset": 24, "endOffset": 54}, {"referenceID": 36, "context": "x \u2032 n, we use a bidirectional variation (Schuster and Paliwal, 1997; Graves et al., 2013).", "startOffset": 40, "endOffset": 89}, {"referenceID": 19, "context": "x \u2032 n, we use a bidirectional variation (Schuster and Paliwal, 1997; Graves et al., 2013).", "startOffset": 40, "endOffset": 89}, {"referenceID": 0, "context": "Here, different from typical MT model (Bahdanau et al., 2014), the chain is predicted sequentially in a feed-forward way with no feedback of the prediction made.", "startOffset": 38, "endOffset": 61}, {"referenceID": 0, "context": "The weight of contribution \u03b2ijk are computed under attention mechanism (Bahdanau et al., 2014).", "startOffset": 71, "endOffset": 94}, {"referenceID": 39, "context": "We apply back-propagation, using momentum stochastic gradient descent (Sutskever et al., 2013) with a learning rate of \u03b7 = 0.", "startOffset": 70, "endOffset": 94}, {"referenceID": 27, "context": "English data come from the Wall Street Journal (WSJ) corpus of the Penn Treebank (Marcus et al., 1993).", "startOffset": 81, "endOffset": 102}, {"referenceID": 48, "context": "1 of the Penn Chinese Treebank (CTB) (Xue et al., 2005).", "startOffset": 37, "endOffset": 55}, {"referenceID": 54, "context": "and accuracy (Zhu et al., 2013).", "startOffset": 13, "endOffset": 31}, {"referenceID": 27, "context": "The baseline removing both the character embeddings and the input word Parser LR LP F1 Fully-supervised Ratnaparkhi (1997) 86.", "startOffset": 104, "endOffset": 123}, {"referenceID": 5, "context": "9 Charniak (2000) 89.", "startOffset": 2, "endOffset": 18}, {"referenceID": 5, "context": "9 Charniak (2000) 89.5 89.9 89.5 Collins (2003) 88.", "startOffset": 2, "endOffset": 48}, {"referenceID": 5, "context": "9 Charniak (2000) 89.5 89.9 89.5 Collins (2003) 88.1 88.3 88.2 Sagae and Lavie (2005)\u2020 86.", "startOffset": 2, "endOffset": 86}, {"referenceID": 5, "context": "9 Charniak (2000) 89.5 89.9 89.5 Collins (2003) 88.1 88.3 88.2 Sagae and Lavie (2005)\u2020 86.1 86.0 86.0 Sagae and Lavie (2006)\u2020 87.", "startOffset": 2, "endOffset": 125}, {"referenceID": 5, "context": "9 Charniak (2000) 89.5 89.9 89.5 Collins (2003) 88.1 88.3 88.2 Sagae and Lavie (2005)\u2020 86.1 86.0 86.0 Sagae and Lavie (2006)\u2020 87.8 88.1 87.9 Petrov and Klein (2007) 90.", "startOffset": 2, "endOffset": 165}, {"referenceID": 4, "context": "1 Carreras et al. (2008) 90.", "startOffset": 2, "endOffset": 25}, {"referenceID": 4, "context": "1 Carreras et al. (2008) 90.7 91.4 91.1 Shindo et al. (2012) N/A N/A 91.", "startOffset": 2, "endOffset": 61}, {"referenceID": 4, "context": "1 Carreras et al. (2008) 90.7 91.4 91.1 Shindo et al. (2012) N/A N/A 91.1 Zhu et al. (2013)\u2020 90.", "startOffset": 2, "endOffset": 92}, {"referenceID": 4, "context": "1 Carreras et al. (2008) 90.7 91.4 91.1 Shindo et al. (2012) N/A N/A 91.1 Zhu et al. (2013)\u2020 90.2 90.7 90.4 Socher et al. (2013)* N/A N/A 90.", "startOffset": 2, "endOffset": 129}, {"referenceID": 4, "context": "1 Carreras et al. (2008) 90.7 91.4 91.1 Shindo et al. (2012) N/A N/A 91.1 Zhu et al. (2013)\u2020 90.2 90.7 90.4 Socher et al. (2013)* N/A N/A 90.4 Vinyals et al. (2015)* N/A N/A 88.", "startOffset": 2, "endOffset": 165}, {"referenceID": 37, "context": "Ensemble Shindo et al. (2012) N/A N/A 92.", "startOffset": 9, "endOffset": 30}, {"referenceID": 37, "context": "Ensemble Shindo et al. (2012) N/A N/A 92.4 Vinyals et al. (2015)* N/A N/A 90.", "startOffset": 9, "endOffset": 65}, {"referenceID": 5, "context": "Rerank Charniak and Johnson (2005) 91.", "startOffset": 7, "endOffset": 35}, {"referenceID": 5, "context": "Rerank Charniak and Johnson (2005) 91.2 91.8 91.5 Huang (2008) 92.", "startOffset": 7, "endOffset": 63}, {"referenceID": 22, "context": "Semi-supervised McClosky et al. (2006) 92.", "startOffset": 16, "endOffset": 39}, {"referenceID": 18, "context": "3 Huang and Harper (2009) 91.", "startOffset": 2, "endOffset": 26}, {"referenceID": 18, "context": "3 Huang and Harper (2009) 91.1 91.6 91.3 Huang et al. (2010) 91.", "startOffset": 2, "endOffset": 61}, {"referenceID": 18, "context": "3 Huang and Harper (2009) 91.1 91.6 91.3 Huang et al. (2010) 91.4 91.8 91.6 Zhu et al. (2013)\u2020 91.", "startOffset": 2, "endOffset": 94}, {"referenceID": 14, "context": "3 Durrett and Klein (2015)* N/A N/A 91.", "startOffset": 2, "endOffset": 27}, {"referenceID": 14, "context": "3 Durrett and Klein (2015)* N/A N/A 91.1 Dyer et al. (2016)*\u2020 N/A N/A 92.", "startOffset": 2, "endOffset": 60}, {"referenceID": 54, "context": "3% F1 improvement compared to the baseline parser with fully-supervised learning (Zhu et al., 2013).", "startOffset": 81, "endOffset": 99}, {"referenceID": 4, "context": "Our model outperforms the state-of-the-art fullysupervised system (Carreras et al., 2008; Shindo et al., 2012) by 0.", "startOffset": 66, "endOffset": 110}, {"referenceID": 37, "context": "Our model outperforms the state-of-the-art fullysupervised system (Carreras et al., 2008; Shindo et al., 2012) by 0.", "startOffset": 66, "endOffset": 110}, {"referenceID": 54, "context": "In addition, our fullysupervised model also catches up with many stateof-the-art semi-supervised models (Zhu et al., 2013; Huang and Harper, 2009; Huang et al., 2010; Durrett and Klein, 2015) by achieving 91.", "startOffset": 104, "endOffset": 191}, {"referenceID": 21, "context": "In addition, our fullysupervised model also catches up with many stateof-the-art semi-supervised models (Zhu et al., 2013; Huang and Harper, 2009; Huang et al., 2010; Durrett and Klein, 2015) by achieving 91.", "startOffset": 104, "endOffset": 191}, {"referenceID": 22, "context": "In addition, our fullysupervised model also catches up with many stateof-the-art semi-supervised models (Zhu et al., 2013; Huang and Harper, 2009; Huang et al., 2010; Durrett and Klein, 2015) by achieving 91.", "startOffset": 104, "endOffset": 191}, {"referenceID": 14, "context": "In addition, our fullysupervised model also catches up with many stateof-the-art semi-supervised models (Zhu et al., 2013; Huang and Harper, 2009; Huang et al., 2010; Durrett and Klein, 2015) by achieving 91.", "startOffset": 104, "endOffset": 191}, {"referenceID": 4, "context": "Our model outperforms the state-of-the-art fullysupervised system (Carreras et al., 2008; Shindo et al., 2012) by 0.6% F1. In addition, our fullysupervised model also catches up with many stateof-the-art semi-supervised models (Zhu et al., 2013; Huang and Harper, 2009; Huang et al., 2010; Durrett and Klein, 2015) by achieving 91.7% F1 on WSJ test set. The size of our model is much smaller than the semi-supervised model of Zhu et al. (2013),", "startOffset": 67, "endOffset": 444}, {"referenceID": 5, "context": "Parser LR LP F1 Fully-supervised Charniak (2000) 79.", "startOffset": 33, "endOffset": 49}, {"referenceID": 3, "context": "8 Bikel (2004) 79.", "startOffset": 2, "endOffset": 15}, {"referenceID": 3, "context": "8 Bikel (2004) 79.3 82.0 80.6 Petrov and Klein (2007) 81.", "startOffset": 2, "endOffset": 54}, {"referenceID": 3, "context": "8 Bikel (2004) 79.3 82.0 80.6 Petrov and Klein (2007) 81.9 84.8 83.3 Zhu et al. (2013)\u2020 82.", "startOffset": 2, "endOffset": 87}, {"referenceID": 3, "context": "8 Bikel (2004) 79.3 82.0 80.6 Petrov and Klein (2007) 81.9 84.8 83.3 Zhu et al. (2013)\u2020 82.1 84.3 83.2 Wang et al. (2015)\u2021 N/A N/A 83.", "startOffset": 2, "endOffset": 122}, {"referenceID": 5, "context": "Rerank Charniak and Johnson (2005) 80.", "startOffset": 7, "endOffset": 35}, {"referenceID": 51, "context": "Semi-supervised Zhu et al. (2013)\u2020 84.", "startOffset": 16, "endOffset": 34}, {"referenceID": 51, "context": "Semi-supervised Zhu et al. (2013)\u2020 84.4 86.8 85.6 Wand and Xue (2014)\u2021 N/A N/A 86.", "startOffset": 16, "endOffset": 70}, {"referenceID": 43, "context": "3 Wang et al. (2015)\u2021 N/A N/A 86.", "startOffset": 2, "endOffset": 21}, {"referenceID": 15, "context": "6 Dyer et al. (2016)*\u2020 N/A N/A 82.", "startOffset": 2, "endOffset": 21}, {"referenceID": 54, "context": "3% F1 improvement compared to the state-of-the-art baseline system with fully-supervised learning (Zhu et al., 2013), which are by far the best results in the literature.", "startOffset": 98, "endOffset": 116}, {"referenceID": 54, "context": "In addition, our fully-supervised model is also comparable to many state-of-the-art semi-supervised models (Zhu et al., 2013; Wang and Xue, 2014; Wang et al., 2015; Dyer et al., 2016) by achieving 85.", "startOffset": 107, "endOffset": 183}, {"referenceID": 44, "context": "In addition, our fully-supervised model is also comparable to many state-of-the-art semi-supervised models (Zhu et al., 2013; Wang and Xue, 2014; Wang et al., 2015; Dyer et al., 2016) by achieving 85.", "startOffset": 107, "endOffset": 183}, {"referenceID": 45, "context": "In addition, our fully-supervised model is also comparable to many state-of-the-art semi-supervised models (Zhu et al., 2013; Wang and Xue, 2014; Wang et al., 2015; Dyer et al., 2016) by achieving 85.", "startOffset": 107, "endOffset": 183}, {"referenceID": 16, "context": "In addition, our fully-supervised model is also comparable to many state-of-the-art semi-supervised models (Zhu et al., 2013; Wang and Xue, 2014; Wang et al., 2015; Dyer et al., 2016) by achieving 85.", "startOffset": 107, "endOffset": 183}, {"referenceID": 15, "context": ", 2015; Dyer et al., 2016) by achieving 85.5% F1 on the CTB test set. Wang and Xue (2014) and Wang et al.", "startOffset": 8, "endOffset": 90}, {"referenceID": 15, "context": ", 2015; Dyer et al., 2016) by achieving 85.5% F1 on the CTB test set. Wang and Xue (2014) and Wang et al. (2015) do joint POS tagging and parsing.", "startOffset": 8, "endOffset": 113}, {"referenceID": 33, "context": "Our parsers are much faster than the related parser with the same shift-reduce framework (Sagae and Lavie, 2005; Sagae and Lavie, 2006).", "startOffset": 89, "endOffset": 135}, {"referenceID": 34, "context": "Our parsers are much faster than the related parser with the same shift-reduce framework (Sagae and Lavie, 2005; Sagae and Lavie, 2006).", "startOffset": 89, "endOffset": 135}, {"referenceID": 27, "context": "2 sentences per secParser #Sent/Second Ratnaparkhi (1997) Unk Collins (2003) 3.", "startOffset": 39, "endOffset": 58}, {"referenceID": 10, "context": "2 sentences per secParser #Sent/Second Ratnaparkhi (1997) Unk Collins (2003) 3.", "startOffset": 62, "endOffset": 77}, {"referenceID": 5, "context": "5 Charniak (2000) 5.", "startOffset": 2, "endOffset": 18}, {"referenceID": 5, "context": "5 Charniak (2000) 5.7 Sagae and Lavie (2005) 3.", "startOffset": 2, "endOffset": 45}, {"referenceID": 5, "context": "5 Charniak (2000) 5.7 Sagae and Lavie (2005) 3.7 Sagae and Lavie (2006) 2.", "startOffset": 2, "endOffset": 72}, {"referenceID": 5, "context": "5 Charniak (2000) 5.7 Sagae and Lavie (2005) 3.7 Sagae and Lavie (2006) 2.2 Petrov and Klein (2007) 6.", "startOffset": 2, "endOffset": 100}, {"referenceID": 4, "context": "2 Carreras et al. (2008) Unk Zhu et al.", "startOffset": 2, "endOffset": 25}, {"referenceID": 4, "context": "2 Carreras et al. (2008) Unk Zhu et al. (2013) 89.", "startOffset": 2, "endOffset": 47}, {"referenceID": 54, "context": "The running times of related parsers are taken from Zhu et al. (2013).", "startOffset": 52, "endOffset": 70}, {"referenceID": 32, "context": "Our lookahead features are similar in spirit to the pruners of Roark and Hollingshead (2009) and Zhang et al.", "startOffset": 63, "endOffset": 93}, {"referenceID": 32, "context": "Our lookahead features are similar in spirit to the pruners of Roark and Hollingshead (2009) and Zhang et al. (2010b), which infer the maximum length of constituents that a particular word can start or end.", "startOffset": 63, "endOffset": 118}, {"referenceID": 10, "context": "tactic role of the word for constraint parsing (Clark, 2002; Clark and Curran, 2004; Carreras et al., 2008; Ninomiya et al., 2006; Dridan et al., 2008).", "startOffset": 47, "endOffset": 151}, {"referenceID": 9, "context": "tactic role of the word for constraint parsing (Clark, 2002; Clark and Curran, 2004; Carreras et al., 2008; Ninomiya et al., 2006; Dridan et al., 2008).", "startOffset": 47, "endOffset": 151}, {"referenceID": 4, "context": "tactic role of the word for constraint parsing (Clark, 2002; Clark and Curran, 2004; Carreras et al., 2008; Ninomiya et al., 2006; Dridan et al., 2008).", "startOffset": 47, "endOffset": 151}, {"referenceID": 29, "context": "tactic role of the word for constraint parsing (Clark, 2002; Clark and Curran, 2004; Carreras et al., 2008; Ninomiya et al., 2006; Dridan et al., 2008).", "startOffset": 47, "endOffset": 151}, {"referenceID": 13, "context": "tactic role of the word for constraint parsing (Clark, 2002; Clark and Curran, 2004; Carreras et al., 2008; Ninomiya et al., 2006; Dridan et al., 2008).", "startOffset": 47, "endOffset": 151}, {"referenceID": 2, "context": "Under the lexicalized grammar, this supertagging can benefit the parsing with more accuracy and efficiency as almost parsing (Bangalore and Joshi, 1999).", "startOffset": 125, "endOffset": 152}, {"referenceID": 40, "context": "Our constituent hierarchy predictor is also related to sequence-to-sequence learning (Sutskever et al., 2014), which is successful in neural machine translation (Bahdanau et al.", "startOffset": 85, "endOffset": 109}, {"referenceID": 0, "context": ", 2014), which is successful in neural machine translation (Bahdanau et al., 2014).", "startOffset": 59, "endOffset": 82}, {"referenceID": 43, "context": "There has also been work that directly use sequence-to-sequence model for constituent parsing, which generates bracketed constituency tree given raw sentences (Vinyals et al., 2015; Luong et al., 2015).", "startOffset": 159, "endOffset": 201}, {"referenceID": 26, "context": "There has also been work that directly use sequence-to-sequence model for constituent parsing, which generates bracketed constituency tree given raw sentences (Vinyals et al., 2015; Luong et al., 2015).", "startOffset": 159, "endOffset": 201}, {"referenceID": 2, "context": "tactic role of the word for constraint parsing (Clark, 2002; Clark and Curran, 2004; Carreras et al., 2008; Ninomiya et al., 2006; Dridan et al., 2008). For a lexicalized grammar, supertagging can benefit the parsing in both accuracy and efficiency by offering almost-parsing information. In particular, Carreras et al. (2008) defined the concept spine in TAG, which is similar to our constituent hierarchy.", "startOffset": 85, "endOffset": 327}, {"referenceID": 2, "context": "tactic role of the word for constraint parsing (Clark, 2002; Clark and Curran, 2004; Carreras et al., 2008; Ninomiya et al., 2006; Dridan et al., 2008). For a lexicalized grammar, supertagging can benefit the parsing in both accuracy and efficiency by offering almost-parsing information. In particular, Carreras et al. (2008) defined the concept spine in TAG, which is similar to our constituent hierarchy. However, there are three differences. First, the spine is defined to describe the main syntactic tree structure with a series of unary projections, while constituent hierarchy is defined to describe how words can start or end hierarchical constituents (it is possible to be empty if the word cannot start or end constituents). Second, spines are extracted from gold trees and used to prune the search space of parsing as hard constraints. In contrast, we use constituent hierarchies as soft features. Third, Carreras et al. (2008) use spines to prune a chart parsing, while we use constituent hierarchies to improve a linear shiftreduce parser.", "startOffset": 85, "endOffset": 939}, {"referenceID": 1, "context": "Under the lexicalized grammar, this supertagging can benefit the parsing with more accuracy and efficiency as almost parsing (Bangalore and Joshi, 1999). Recently, the works on obtaining the super tags appear. Zhang et al. (2010a) proposed the efficient methods to obtain super tags for HPSG parsing using dependency information.", "startOffset": 126, "endOffset": 231}, {"referenceID": 1, "context": "Under the lexicalized grammar, this supertagging can benefit the parsing with more accuracy and efficiency as almost parsing (Bangalore and Joshi, 1999). Recently, the works on obtaining the super tags appear. Zhang et al. (2010a) proposed the efficient methods to obtain super tags for HPSG parsing using dependency information. Xu et al. (2015) and Vaswani et al.", "startOffset": 126, "endOffset": 347}, {"referenceID": 1, "context": "Under the lexicalized grammar, this supertagging can benefit the parsing with more accuracy and efficiency as almost parsing (Bangalore and Joshi, 1999). Recently, the works on obtaining the super tags appear. Zhang et al. (2010a) proposed the efficient methods to obtain super tags for HPSG parsing using dependency information. Xu et al. (2015) and Vaswani et al. (2016) turn to design recursive neural network for supertagging for CCG parsing.", "startOffset": 126, "endOffset": 373}, {"referenceID": 0, "context": ", 2014), which is successful in neural machine translation (Bahdanau et al., 2014). The neural model encodes the source-side sentence into dense vectors, and then uses them to generate target-side word by word. There has also been work that directly use sequence-to-sequence model for constituent parsing, which generates bracketed constituency tree given raw sentences (Vinyals et al., 2015; Luong et al., 2015). Compared to Vinyals et al. (2015), who predicts a full parser tree from input, our predictors tackle a much simpler task, by predicting the constituent hierarchies of each word separately.", "startOffset": 60, "endOffset": 448}, {"referenceID": 16, "context": "By integrating the neural constituent hierarchy predictor, our parser is related to neural network models for parsing, which has given competitive accuracies for both constituency parsing (Dyer et al., 2016; Watanabe and Sumita, 2015) and dependency parsing (Chen and Manning, 2014; Zhou et al.", "startOffset": 188, "endOffset": 234}, {"referenceID": 46, "context": "By integrating the neural constituent hierarchy predictor, our parser is related to neural network models for parsing, which has given competitive accuracies for both constituency parsing (Dyer et al., 2016; Watanabe and Sumita, 2015) and dependency parsing (Chen and Manning, 2014; Zhou et al.", "startOffset": 188, "endOffset": 234}, {"referenceID": 7, "context": ", 2016; Watanabe and Sumita, 2015) and dependency parsing (Chen and Manning, 2014; Zhou et al., 2015; Dyer et al., 2015).", "startOffset": 58, "endOffset": 120}, {"referenceID": 53, "context": ", 2016; Watanabe and Sumita, 2015) and dependency parsing (Chen and Manning, 2014; Zhou et al., 2015; Dyer et al., 2015).", "startOffset": 58, "endOffset": 120}, {"referenceID": 15, "context": ", 2016; Watanabe and Sumita, 2015) and dependency parsing (Chen and Manning, 2014; Zhou et al., 2015; Dyer et al., 2015).", "startOffset": 58, "endOffset": 120}, {"referenceID": 38, "context": "In particular, our parser is more closely related to neural models that integrates discrete manual features (Socher et al., 2013; Durrett and Klein, 2015).", "startOffset": 108, "endOffset": 154}, {"referenceID": 14, "context": "In particular, our parser is more closely related to neural models that integrates discrete manual features (Socher et al., 2013; Durrett and Klein, 2015).", "startOffset": 108, "endOffset": 154}, {"referenceID": 41, "context": "Tsuruoka et al. (2011) run a baseline parser for a few future steps, and use the output actions to guide the current action.", "startOffset": 0, "endOffset": 23}, {"referenceID": 50, "context": "Zhang et al. (2010b) introduced a chart pruner to accelerate a CCG parser.", "startOffset": 0, "endOffset": 21}, {"referenceID": 25, "context": "Kummerfeld et al. (2010) proposed novel self-training method focusing on increasing the speed of a CCG parser rather than its accuracy.", "startOffset": 0, "endOffset": 25}], "year": 2016, "abstractText": "Transition-based models can be fast and accurate for constituent parsing. Compared with chart-based models, they leverage richer features by extracting history information from a parser stack, which spans over non-local constituents. On the other hand, during incremental parsing, constituent information on the right hand side of the current word is not utilized, which is a relative weakness of shiftreduce parsing. To address this limitation, we leverage a fast neural model to extract lookahead features. In particular, we build a bidirectional LSTM model, which leverages the full sentence information to predict the hierarchy of constituents that each word starts and ends. The results are then passed to a strong transition-based constituent parser as lookahead features. The resulting parser gives 1.3% absolute improvement in WSJ and 2.3% in CTB compared to the baseline, given the highest reported accuracies for fully-supervised parsing.", "creator": "TeX"}}}