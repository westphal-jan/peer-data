{"id": "1612.00583", "review": {"conference": "AAAI", "VERSION": "v1", "DATE_OF_SUBMISSION": "2-Dec-2016", "title": "Active Search for Sparse Signals with Region Sensing", "abstract": "Autonomous systems can be used to search for sparse signals in a large space; e.g., aerial robots can be deployed to localize threats, detect gas leaks, or respond to distress calls. Intuitively, search algorithms may increase efficiency by collecting aggregate measurements summarizing large contiguous regions. However, most existing search methods either ignore the possibility of such region observations (e.g., Bayesian optimization and multi-armed bandits) or make strong assumptions about the sensing mechanism that allow each measurement to arbitrarily encode all signals in the entire environment (e.g., compressive sensing). We propose an algorithm that actively collects data to search for sparse signals using only noisy measurements of the average values on rectangular regions (including single points), based on the greedy maximization of information gain. We analyze our algorithm in 1d and show that it requires $\\tilde{O}(\\frac{n}{\\mu^2}+k^2)$ measurements to recover all of $k$ signal locations with small Bayes error, where $\\mu$ and $n$ are the signal strength and the size of the search space, respectively. We also show that active designs can be fundamentally more efficient than passive designs with region sensing, contrasting with the results of Arias-Castro, Candes, and Davenport (2013). We demonstrate the empirical performance of our algorithm on a search problem using satellite image data and in high dimensions.", "histories": [["v1", "Fri, 2 Dec 2016 07:44:45 GMT  (2126kb,D)", "http://arxiv.org/abs/1612.00583v1", "aaai 2017 preprint; nips exhibition of rejections"]], "COMMENTS": "aaai 2017 preprint; nips exhibition of rejections", "reviews": [], "SUBJECTS": "stat.ML cs.AI cs.LG", "authors": ["yifei ma", "roman garnett", "jeff g schneider"], "accepted": true, "id": "1612.00583"}, "pdf": {"name": "1612.00583.pdf", "metadata": {"source": "CRF", "title": "Active Search for Sparse Signals with Region Sensing", "authors": ["Yifei Ma", "Roman Garnett", "Jeff Schneider"], "emails": ["yifeim@cs.cmu.edu", "garnett@wustl.edu", "schneide@cs.cmu.edu"], "sections": [{"heading": null, "text": "This year, the time has come for an agreement to be reached, and it will only take a few days."}, {"heading": "1.1 Related Work", "text": "The authors have also shown that a passive random design, combined with a non-trivial inference algorithm, such as Lasso (Wainwright 2009) or the Dantzig selector (Candes and Tao 2007), can have similar recovery rates (up to O (log n) terms. This result has been presented as a paradox, suggesting that the active methods are not always true. Here, we show that active search can make a significant difference in recovery rates when the measurements are physically plausible, especially when the physical space is of low dimensions."}, {"heading": "3.1 Accelerations", "text": "In practice, keeping (n k) models in memory may be impractical if k is large, we can instead restore support for \u03b2 elementally by repeating RSI under the assumption k = 1. After the rear distribution \u03c0t (\u03b2 (1)) converges for better modeling at the most likely one-dimensional distribution, we report on its location and move on to by2In real-world experiments we additionally estimate that we use point measurement at the derived signal position for better modeling. Table 1: Conditions and conclusions for sample complexity.Design type Region Sensing AlgorithmPrior for Bayes RiskMin T to Guarantee \u0433T = 1 kE | S \u0441T | \u2264 Sample complexity, passive yes (any) sample complexity (any)."}, {"heading": "Ensure: S\u0302", "text": "1: initialize S = \u03b2, \u03b2 = 0 2: for k = 1 > 2,.., do 3: draw conclusions \u03c00 (\u03b2 (k)) that there is not a p (y\u03c4 | \u03b2 (k) + \u03b2, x), x (p), x (c), x (c), x (c), x (c), x (c), x (c), x (c), x (c) and x (c). We call this alternative algorithm Region Sensing IndexAny-k (RSI-A, algorithm 2) and use it in our simulations so that the computing costs are no longer exponential."}, {"heading": "4.1 Baseline Results", "text": "Here we offer lower limits on sample complexity. We show that under regional detection constraints, all passive methods require measurements and active methods. If \u00b5 1, active methods have significant potential for improvement compared to passive methods using region detection, which contradicts the view of full compressive detection by Arias-Castro, Candes and Davenport (2013). Theorem 1 (Limits of all passive methods using region detection). Assumption \u03b2 has advance 0 (uniform random to S\u00b5 (n k). Any passive method with noiseless region measurements on 1d must cause Bayes risk. n \u2212 kn \u2212 1 (1 \u2212 2Tn). To guarantee that T \u2264 n2 (1 \u2212 n \u2212 1n \u2212 k)."}, {"heading": "4.2 Main Result", "text": "For technical convenience, we express our main result directly in the form of the expected number of measurements that are actually taken to achieve a certain threshold value (DT) \u2264 in an experiment. If T = T is taken as a random variable, the expected number of actual measurements differs from the pre-determined sample budget that an algorithm consumes completely to guarantee a desirable average risk (see Section 4.1). However, it is a comparable alternative in Bayesian analysis used by e.g. Lai and Robbins (1985); Kaufmann, Korda and Munos (2012). If the goal is constant = O (1), our result implies a deterministic budget requirement of the same magnitude, T \u2264 \u2212 12 ET 2, where 2 = 2, by directly applying Markow's inequality (Theorem 3 (sample complexity of RSI \u2264), our result implies a deterministic budget requirement of the same magnitude, where T = 2, whereas T = 2 is assumed to be true)."}, {"heading": "4.3 Proof Sketch", "text": "The proof for theory 3 depends on an observation that the information gains (IG) where RSI takes measurements are consistently large before the active search ends with minimal risk. Example: the IG of any measurement in binary search with k = 1 and noiseless observations is always O (log (2)). However, IG is more difficult to approximate if the observations are noisy. Therefore, we first show an intuitive lower limit for IG. Recall notations from (4).3The original result in Malloy and Nowak (2014) is stronger; it considers the maximum probability of recovery errors, P (S 6 = S)."}, {"heading": "B.1 Basic Properties of Information Gain (IG)", "text": "Remember that the observation model is yt = x > t \u03b2 + t, where \u03b2-S\u00b5 (n k), \u03b2-\u0445 \u03c0t and t-N (0, \u03c32t) must be maximized in each step. If the time index t is omitted, the information gain (IG) is defined as I (\u03b2; y | x, \u03c0) = H (y | x, \u03c0) \u2212 E [H (y | x, \u03b2) | \u03c0] \u21d4 I (\u03b3; y | \u03bb, p) = H (y | \u03bb, p) \u2212 H (), (B.1), where f (y | \u03bb, p) = K \u2211 c = 0pc\u03c6 (y \u2212 c\u03bb) \u03bb = \u00b5wx, \u03b3 = x > \u03b2 \u03bb, pc = Pr (\u03b3 = c) = \u2211 \u03b2: x > \u03b2 = cnowt (\u03b2). (B.2) There are two basic properties: Lemma B.1, which are applied both directly in Section 3.1 accelerations and indirectly in the sketch."}, {"heading": "Basic Property 1", "text": "(Concavity.) (Concavity.) (Concavity.) (Concavity.) (Concavity.) (Concavity.) (Concavity.) (Concavity.) (Concavity.) (Concavity.) (Concavity.) (Concavity.) (Concavity.) (Concavity.) (Concavity.) (Concavity.) (Concavity.) (Concavity.) (Concavity. (Concavity.) (Concavity. (Concavity.) (Concavity.) (Concavity.) (Concavity.) (Concavity.) (Concavity. (Concavity.) (Concavity.) (Concavity. (Concavity.) (Concavity.) (Concavity. (Concavity.) (Concavity. (Concavity.) (Concavity. (Concavity.) (Concavity. (Concavity.) (Concavity. (Concavity.) (Concavity. (Concavity.) (Concavity. (Concavity.) (Concavity. (Concavity.) (Concavity.) (Concavity. (Concavity.) (Concavity.). (Concavity. (Concavity.) (Concavity.). (Concavity.). (Concavity. (Concavity.). (Concavity.). (Concavity. (Concavity.). (Concavity. (.). (.). (.) (Concavity. (.). (.) (Concavity. (.). (. (.) (Concavity.) (.). (.) (Concavity. (.) (. (.).). (. (Concavity. (.).). (. (. (.). (.). (Concavity.). (.). (. (.). (Concavity.). (.). (.). (. (. (.). (.). (.). (. (.).). (."}, {"heading": "Basic Property 2", "text": "Proposition B.2 (Proposition 4) in the main document; a lower limit for the IG of a design = size (= size).The IG score of a region senses lower limits with respect to its design parameters (\u03bb, p), asI (\u03b3; y | \u03bb, p), 2qcq, c (2), 2 (2), 2 (2), 2 (2), 2 (2), 2 (2), 3 (2), 4 (2), 2 (2), 2 (2), 2 (2), 2 (2), 2 (2), 2 (2), 2 (2), 2 (2), 3 (2), 4 (2), 2 (2), 4 (2), 4 (2), 4 (2), 4 (2), 4 (2), 4 (2), 4 (2), 4 (2), 4 (2), 4 (2), 4 (2), 2 (2), 2 (2), 3 (2), 3 (2), 3 (2), 4 (2), 4 (2), 4 (2), 4 (2), 4 (2), 5 (2), 5 (2), 5 (5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5 (5), 5, 5, 5 (5), 5, 5, 5 (5), 5, 5, 5 (5), 5 (5, 5, 5), 5 (2, 5, 5, 5, 5 (5), 5, 5 (5), 5, 5 (5), 5, 5 (5, 5, 5 (5), 5, 5 (5), 5, 5, 5, 5 (2, 5), 5 (5), 5 (2, 5, 5 (5), 5, 5 (2, 5), 5 (2, 5, 5 (5), 5, 5, 5 (5), 5, 5 (5, 5, 5 (5), 5, 5 (5), 5, 5 (5, 5), 5 (2, 5 (5), 5, 5 (5), 5, 5, 5, 5, 5 ("}, {"heading": "B.2 Minimum Information Gain of the Chosen Region in Each Iteration", "text": "# # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # #"}, {"heading": "B.3 The Proof of Theorem 3", "text": "Lemma B.4 implies that entropy in the posterior distribution, H (\u03b2 | \u03c0t) = \u2212 \u2211 \u03b2 \u03c0t (\u03b2) log \u03c0t (\u03b2), with each measurement in expectation, starting with H (\u03b2 | \u03c00) \u2264 k log n. Since the posterior entropy cannot be negative, the RSI must be terminated in finite time in expectation. Theorem B.7 (Theorem 3 in the main document; Sample complexity of the RSI): In the active search for k-sparse signals with the strength \u00b5 in the 1d physical space of size n (\u2265 2k), whereby any > 0 is given as tolerance of the posterior Bayes risk, the RSI has limited the expected number of actual measurements with the help of the region recording before the stop, T = E [min {T: \u0432 (DT) \u2264}] \u2264 50 (n \u00b52 + k2 9) log (n) = n (2 \u00b5k2), where the sensitivity is 30 before the distribution takes place (B.30)."}, {"heading": "The Simple Approach", "text": "t Definition B.8 (holding period) Definition T = minT (DT) \u2264 (\u03b2 \u03b2) = > > Reference period for an experiment up to the first yield is lower than the risk after the first yield. (Definition B.8) Definition T = > > > Reference period (DT) \u2264 (DT) = > > Reference period for an experiment up to the first yield is lower than the risk after the second yield. (Definition B.8) Definition T = > > > Reference period (DT) \u2264 (T) \u2264 (T) \u2264 1 (T) \u2264 1 (T) + > Reference period (Simple expectations of the number of measurements after the first yield after the first yield after the second yield). (Definition B.9) Definition B.9 (Definition B.9) + > Reference period after the first yield after the first yield after the second T) + > Reference period after the first yield + > Reference period after the first yield after the first yield is lower than the risk after the second yield. (Definition B.0) Definition T = > > > Reference period (DT) > Reference period after the first yield is lower than the risk after the second yield. (Definition 0) Definition T = > > > Reference period after the second yield after the second yield after the second yield (DT) \u2264 (T) \u2264 (T) \u2264 (T) \u2264 (T) \u2264 (T) \u2264 1 (T) > Reference period after the first yield after the first yield after the first yield (T) > Reference period (Simple expectations of the number of measurements after the first yield after the first yield after the first yield after the first yield after the first yield after the second yield after the second yield after the second yield). (Definition B.9) Definition B.9 Percent Definition B.9 + > Reference period after the first yield after the second yield after the second yield after the second yield after the second yield after the second yield after the second yield (Definition B.9 Percent."}, {"heading": "The Complex Approach", "text": "Let us S = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K ="}], "references": [{"title": "Example of positive discoveries in NAIP satellite", "author": ["E. Arias-Castro", "E.J. Candes", "M. Davenport"], "venue": null, "citeRegEx": "Arias.Castro et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Arias.Castro et al\\.", "year": 2013}], "referenceMentions": [], "year": 2016, "abstractText": "Autonomous systems can be used to search for sparse signals in a large space; e.g., aerial robots can be deployed to localize threats, detect gas leaks, or respond to distress calls. Intuitively, search algorithms may increase efficiency by collecting aggregate measurements summarizing large contiguous regions. However, most existing search methods either ignore the possibility of such region observations (e.g., Bayesian optimization and multi-armed bandits) or make strong assumptions about the sensing mechanism that allow each measurement to arbitrarily encode all signals in the entire environment (e.g., compressive sensing). We propose an algorithm that actively collects data to search for sparse signals using only noisy measurements of the average values on rectangular regions (including single points), based on the greedy maximization of information gain. We analyze our algorithm in 1d and show that it requires \u00d5(n/\u03bc2 +k) measurements to recover all of k signal locations with small Bayes error, where \u03bc and n are the signal strength and the size of the search space, respectively. We also show that active designs can be fundamentally more efficient than passive designs with region sensing, contrasting with the results of Arias-Castro, Candes, and Davenport (2013). We demonstrate the empirical performance of our algorithm on a search problem using satellite image data and in high dimensions.", "creator": "LaTeX with hyperref package"}}}