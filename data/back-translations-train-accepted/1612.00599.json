{"id": "1612.00599", "review": {"conference": "AAAI", "VERSION": "v1", "DATE_OF_SUBMISSION": "2-Dec-2016", "title": "Communication Lower Bounds for Distributed Convex Optimization: Partition Data on Features", "abstract": "Recently, there has been an increasing interest in designing distributed convex optimization algorithms under the setting where the data matrix is partitioned on features. Algorithms under this setting sometimes have many advantages over those under the setting where data is partitioned on samples, especially when the number of features is huge. Therefore, it is important to understand the inherent limitations of these optimization problems. In this paper, with certain restrictions on the communication allowed in the procedures, we develop tight lower bounds on communication rounds for a broad class of non-incremental algorithms under this setting. We also provide a lower bound on communication rounds for a class of (randomized) incremental algorithms.", "histories": [["v1", "Fri, 2 Dec 2016 09:01:57 GMT  (13kb)", "http://arxiv.org/abs/1612.00599v1", null]], "reviews": [], "SUBJECTS": "cs.LG stat.ML", "authors": ["zihao chen", "luo luo", "zhihua zhang"], "accepted": true, "id": "1612.00599"}, "pdf": {"name": "1612.00599.pdf", "metadata": {"source": "CRF", "title": "Communication Lower Bounds for Distributed Convex Optimization: Partition Data on Features", "authors": ["Zihao Chen"], "emails": ["z.h.chen@sjtu.edu.cn", "ricky@sjtu.edu.cn", "zhzhang@math.pku.edu.cn"], "sections": [{"heading": null, "text": "ar Xiv: 161 2.00 599v 1 [cs.L G"}, {"heading": "1 Introduction", "text": "In this paper, we look at the following distributed optimization problem over m machines: min w. \"Rd f (w). Each machine knows the form of f, but has only some partial information of p.\" In particular, we mainly look at the case of empirical risk minimization (ERM) problems. Let's be a matrix that contains n data patterns with d features and Aj. (1) In recent years, many distributed optimization algorithms have been proposed, many of which are under the setting in which data is partitioned to samples, i.e. each machine stores a subset of the data matrix. (2, 3, 15, 5, 10, 10)"}, {"heading": "2 Notations and Preliminaries", "text": "For a vector w-Rd, we define w (i) as the i-th coordinate of the vector w-Rd. We leave the coordinate index [d] in m disjunct setsS1, S2,..., Sm with p-sample m i = 1 di = d and Sj = {k setpoint i < j di < k) for j = 1, 2,.., m. For a vector w setpoint w-Rd, denounce w [j] as a vector in Rdj corresponding to the segment w on coordinates Sj. For a series of vectors V-Rd, we define V [j] = j [j] [j] [j] [j]], v-setpoint] y setpoint."}, {"heading": "3 Definitions and Framework", "text": "In this section we first describe a family of distributed optimization algorithms using m-machines and then modify them to obtain a family of incremental algorithms. Initially, the feature coordinates are divided into m-sets and each machine has the data columns corresponding to its coordinate set. We model the algorithms as iterative processes in several rounds and each round consists of a computation phase followed by a communication phase. We define a workable set for each machine and during the computation phase each machine can perform some \"cheap\" communication and add some vectors. During the communication phase, each machine can send a limited number of points to all other machines. We also assume that the communication operations are the usual operations such as Broadcast, Reduce and ReduceAll."}, {"heading": "3.1 Non-incremental Algorithm Family", "text": "It's not just the way the data is distributed around the world, it's also the way the data is distributed around the world. It's also the way the data is distributed around the world. It's the way the data is distributed around the world. It's the way the data is distributed around the world. It's the way the data is distributed around the world. It's the way the data is distributed around the world. It's the way the data is distributed around the world. It's the way the data is distributed around the world. It's the way we're distributed around the world. It's the way we're distributed around the world. It's the way we're distributed around the world. It's the way we're distributed around the world."}, {"heading": "3.2 Incremental Algorithm Family", "text": "To define the class of the incremental / stochastic algorithms I\u03bb, L under the Feature partition setting, we modify the definition of F\u03bb, L slightly, by replacing the assumption of the realizable quantity (2) with the following, while we leave the rest unchanged: Adoption of the realizable quantity for I\u03bb, L. In the k-ten round initially W (k) w (k) w (k) w (k) w. As next selects each machine for a constant number of times for a??????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????"}, {"heading": "4 Main Results", "text": "W: W: W: W: W: W: W: W: W: W: W: W: W: W: W: W: W: W: W: W: W: W: W: W: W: W: W: W: W: W: W: W: W: W: W: W: W: W: W: W: W: W: W: W: W: W: W: W: W: W: W: W: W: W: W: W: W: W: W: W: W: W: W: W: W: W: W: W: W: W: W: W: W: W: W: W: W: W: W: W: W: W: W: W: W: W: W: W: W: W: W: W: W: W: W: W: W: W: W: W: W: W: W: W: W: W: W: W: W: W: W: W: W: W: W: W: W: W: W: W: W: W: W: W: W: W: W: W: W: W: W: W: W: W: W: W: W: W: W: W: W: W: W: W: W: W: W: W: W: W: W: W: W: W: W: W: W: W: W: W: W: W: W: W: W: W: W: W: W: W: W: W: W: W: W: W: W: W: W: W: W: W: W: W: W: W: W: W: W: W: W: W: W: W: W: W: W: W: W: W: W: W: W: W: W: W: W: W: W: W: W: W: W: W: W: W: W: W: W: W: W: W: W: W: W: W: W: W: W: W: W: W: W: W: W: W: W: W: W: W: W: W: W: W: W: W: W: W: W: W: W: W: W: W: W:"}, {"heading": "5 Proof of Main Results", "text": "In this section we offer proofs for theorem 2 and theorem 4. The framework of proof of these theorems is based on [13]. Since theorem 3 can be obtained by replacing [13, Lemma 2.1.3] with the accompanying phenomenon 6 (see below) in the proof of [13, Theorem 2.1.6], we will not discuss it here for the sake of simplicity."}, {"heading": "5.1 Proof of Theorem 2", "text": "The idea is to construct a \"hard\" function so that all the algorithms in the class we have defined cannot be well optimized in a small number of rounds: In each round, only one of the machines can perform a constant level of \"progress,\" while other machines remain \"trapped\" (see Tagma 5). Without losing generality, we assume that each machine adds only one vector to its realizable number and the limit does not change. First, we construct the following function as [6] f (w) = [6 wTAw \u2212 1) 4 [12 wTAw \u2212 < e1, w >] w \u00b2 w \u00b2 w \u00b2 w \u00b2 w \u00b2 2, (7) where A constructs a tridiagonal matrix in Rd \u00b7 d \u00b7 d \u00b7 d = 2 \u2212 1 0 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 0 \u00b7 0 \u00b7 0 \u00b7 0 \u00b7 1 \u2212 1 \u00b7 1 \u00b7 0 \u00b7 1 \u00b7 0 \u00b7 0 \u00b7 0 \u00b7 0 \u00b7 1 \u00b7 1 \u00b7 1 \u00b7 1 \u00b7 1 \u00b7 1, we, 0."}, {"heading": "5.2 Proof of Theorem 4", "text": "\u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7"}, {"heading": "6 Conclusion", "text": "In this paper, we have defined two classes of distributed optimization algorithms, in terms of partitioning data by characteristics: One is a family of non-incremental algorithms, and the other is incremental. We have set narrow lower limits for communication rounds for non-incremental algorithms. We have also set a lower limit for incremental classes of algorithms, but whether it is narrow remains open. Narrowness informs us that we should break our definition when we try to design optimization algorithms with fewer communication rounds than existing algorithms. We also emphasize that our lower limits are important as they can provide a deeper understanding of the limits of some ideas or techniques used in distributed optimization algorithms, which may provide some insights for developing better algorithms. To the best of knowledge, this is the first work to examine communication from the perspective of distributed optimization algorithms from the point of view of partitioning data by characteristics."}], "references": [{"title": "Communication complexity of distributed convex learning and optimization", "author": ["Y. Arjevani", "O. Shamir"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2015}, {"title": "Distributed learning, communication complexity and privacy", "author": ["M.-F. Balcan", "A. Blum", "S. Fine", "Y. Mansour"], "venue": "arXiv preprint arXiv:1204.3514,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2012}, {"title": "Distributed optimization and statistical learning via the alternating direction method of multipliers", "author": ["S. Boyd", "N. Parikh", "E. Chu", "B. Peleato", "J. Eckstein"], "venue": "Foundations and Trends R  \u00a9 in Machine Learning,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2011}, {"title": "Mapreduce: simplified data processing on large clusters", "author": ["J. Dean", "S. Ghemawat"], "venue": "Communications of the ACM,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2008}, {"title": "Communicationefficient distributed dual coordinate ascent", "author": ["M. Jaggi", "V. Smith", "M. Tak\u00e1c", "J. Terhorst", "S. Krishnan", "T. Hofmann", "M.I. Jordan"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2014}, {"title": "An optimal randomized incremental gradient method", "author": ["G. Lan"], "venue": "arXiv preprint arXiv:1507.02000,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2015}, {"title": "Distributed box-constrained quadratic optimization for dual linear svm", "author": ["C.-p. Lee", "D. Roth"], "venue": "In Proceedings of the 32nd International Conference on Machine Learning", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2015}, {"title": "Distributed stochastic variance reduced gradient methods and a lower bound for communication complexity", "author": ["J. Lee", "T. Ma", "Q. Lin"], "venue": "arXiv preprint arXiv:1507.07595,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2015}, {"title": "Distributed inexact damped newton method: Data partitioning and loadbalancing", "author": ["C. Ma", "M. Tak\u00e1\u010d"], "venue": "arXiv preprint arXiv:1603.05191,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2016}, {"title": "Adding vs. averaging in distributed primal-dual optimization", "author": ["C. Ma", "V. Smith", "M. Jaggi", "M.I. Jordan", "P. Richt\u00e1rik", "M. Tak\u00e1\u010d"], "venue": "arXiv preprint arXiv:1502.03508,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2015}, {"title": "Distributed block coordinate descent for minimizing partially separable functions", "author": ["J. Mare\u010dek", "P. Richt\u00e1rik", "M. Tak\u00e1\u010d"], "venue": "In Numerical Analysis and Optimization,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2015}, {"title": "Parallel coordinate descent methods for composite minimization: convergence analysis and error bounds", "author": ["I. Necoara", "D. Clipici"], "venue": null, "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2013}, {"title": "Introductory lectures on convex optimization: A basic course, volume 87", "author": ["Y. Nesterov"], "venue": "Springer Science & Business Media,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2013}, {"title": "Distributed coordinate descent method for learning with big data", "author": ["P. Richt\u00e1rik", "M. Tak\u00e1\u010d"], "venue": "arXiv preprint arXiv:1310.2059,", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2013}, {"title": "Trading computation for communication: Distributed stochastic dual coordinate ascent", "author": ["T. Yang"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2013}, {"title": "Communication-efficient distributed optimization of self-concordant empirical loss", "author": ["Y. Zhang", "L. Xiao"], "venue": "arXiv preprint arXiv:1501.00263,", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2015}, {"title": "Communication-efficient algorithms for statistical optimization", "author": ["Y. Zhang", "M.J. Wainwright", "J.C. Duchi"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2012}], "referenceMentions": [{"referenceID": 15, "context": "each machine stores a subset of the data matrix A\u2019s rows [16, 17, 2, 3, 15, 8, 5, 10].", "startOffset": 57, "endOffset": 85}, {"referenceID": 16, "context": "each machine stores a subset of the data matrix A\u2019s rows [16, 17, 2, 3, 15, 8, 5, 10].", "startOffset": 57, "endOffset": 85}, {"referenceID": 1, "context": "each machine stores a subset of the data matrix A\u2019s rows [16, 17, 2, 3, 15, 8, 5, 10].", "startOffset": 57, "endOffset": 85}, {"referenceID": 2, "context": "each machine stores a subset of the data matrix A\u2019s rows [16, 17, 2, 3, 15, 8, 5, 10].", "startOffset": 57, "endOffset": 85}, {"referenceID": 14, "context": "each machine stores a subset of the data matrix A\u2019s rows [16, 17, 2, 3, 15, 8, 5, 10].", "startOffset": 57, "endOffset": 85}, {"referenceID": 7, "context": "each machine stores a subset of the data matrix A\u2019s rows [16, 17, 2, 3, 15, 8, 5, 10].", "startOffset": 57, "endOffset": 85}, {"referenceID": 4, "context": "each machine stores a subset of the data matrix A\u2019s rows [16, 17, 2, 3, 15, 8, 5, 10].", "startOffset": 57, "endOffset": 85}, {"referenceID": 9, "context": "each machine stores a subset of the data matrix A\u2019s rows [16, 17, 2, 3, 15, 8, 5, 10].", "startOffset": 57, "endOffset": 85}, {"referenceID": 13, "context": ", each machine stores a subset of A\u2019s columns [14, 11, 9, 12, 7].", "startOffset": 46, "endOffset": 64}, {"referenceID": 10, "context": ", each machine stores a subset of A\u2019s columns [14, 11, 9, 12, 7].", "startOffset": 46, "endOffset": 64}, {"referenceID": 8, "context": ", each machine stores a subset of A\u2019s columns [14, 11, 9, 12, 7].", "startOffset": 46, "endOffset": 64}, {"referenceID": 11, "context": ", each machine stores a subset of A\u2019s columns [14, 11, 9, 12, 7].", "startOffset": 46, "endOffset": 64}, {"referenceID": 6, "context": ", each machine stores a subset of A\u2019s columns [14, 11, 9, 12, 7].", "startOffset": 46, "endOffset": 64}, {"referenceID": 12, "context": ", which can be matched by a straightforward distributed version of accelerated gradient decent [13] and also DISCO-F for quadratics [9], an variant of DISCO [16] under the feature partition setting.", "startOffset": 95, "endOffset": 99}, {"referenceID": 8, "context": ", which can be matched by a straightforward distributed version of accelerated gradient decent [13] and also DISCO-F for quadratics [9], an variant of DISCO [16] under the feature partition setting.", "startOffset": 132, "endOffset": 135}, {"referenceID": 15, "context": ", which can be matched by a straightforward distributed version of accelerated gradient decent [13] and also DISCO-F for quadratics [9], an variant of DISCO [16] under the feature partition setting.", "startOffset": 157, "endOffset": 161}, {"referenceID": 0, "context": "Related Work The most revelant work should be [1], which studied lower bounds under the sample partition setting, and provided tight lower bounds on communication rounds for convex smooth optimization after putting some mild restrictions.", "startOffset": 46, "endOffset": 49}, {"referenceID": 12, "context": "Both these work as well as ours are based on some techniques used in non-distributed optimization lower bound analysis [13, 6].", "startOffset": 119, "endOffset": 126}, {"referenceID": 5, "context": "Both these work as well as ours are based on some techniques used in non-distributed optimization lower bound analysis [13, 6].", "startOffset": 119, "endOffset": 126}, {"referenceID": 3, "context": "Here we list some common MapReduce types of communication operations [4] in an abstract level: (a) One-to-all broadcast.", "startOffset": 69, "endOffset": 72}, {"referenceID": 13, "context": "Actually, almost all existing partition-on-feature algorithms satisfy this restriction [14, 11, 9, 12, 7].", "startOffset": 87, "endOffset": 105}, {"referenceID": 10, "context": "Actually, almost all existing partition-on-feature algorithms satisfy this restriction [14, 11, 9, 12, 7].", "startOffset": 87, "endOffset": 105}, {"referenceID": 8, "context": "Actually, almost all existing partition-on-feature algorithms satisfy this restriction [14, 11, 9, 12, 7].", "startOffset": 87, "endOffset": 105}, {"referenceID": 11, "context": "Actually, almost all existing partition-on-feature algorithms satisfy this restriction [14, 11, 9, 12, 7].", "startOffset": 87, "endOffset": 105}, {"referenceID": 6, "context": "Actually, almost all existing partition-on-feature algorithms satisfy this restriction [14, 11, 9, 12, 7].", "startOffset": 87, "endOffset": 105}, {"referenceID": 0, "context": "\u2022 Similar to [1], we use Wj to define the restriction on the updates.", "startOffset": 13, "endOffset": 16}, {"referenceID": 13, "context": "However, for some common loss functions \u03c6, such as (regularized) squared loss, logistic loss and squared hinge loss, computing f \u2032 j(w) for all j \u2208 [m] in total only needs a ReduceAll operation of an R vector [14].", "startOffset": 209, "endOffset": 213}, {"referenceID": 13, "context": "In some gradient (or partial gradient) based algorithms [14, 11], communication to compute partial gradients are the only need of communication in the computation phase.", "startOffset": 56, "endOffset": 64}, {"referenceID": 10, "context": "In some gradient (or partial gradient) based algorithms [14, 11], communication to compute partial gradients are the only need of communication in the computation phase.", "startOffset": 56, "endOffset": 64}, {"referenceID": 8, "context": "Besides, some algorithms like DISCO-F [9] need to compute (\u2207f(w)u)[j].", "startOffset": 38, "endOffset": 41}, {"referenceID": 8, "context": "a ReduceAll operation of an R vector [9].", "startOffset": 37, "endOffset": 40}, {"referenceID": 0, "context": "The amount of communication allowed in our partition-on-feature algorithm class is relatively small, compared with the partitionon-sample algorithm class described in [1], which allows \u00d5(md) bits of one-to-all broadcast in each round.", "startOffset": 167, "endOffset": 170}, {"referenceID": 12, "context": "For some common loss functions, this can be matched by a straightforward distributed implementation of accelerated gradient decent [13] and it is easy to verify that it satisfies our definition: let all machines compute their own partial gradients and aggregate to form a gradient.", "startOffset": 131, "endOffset": 135}, {"referenceID": 8, "context": "Recall that our definition of the algorithm class includes some types of distributed second order algorithms, for example DISCO-F [9].", "startOffset": 130, "endOffset": 133}, {"referenceID": 12, "context": "The proof framework of these theorems are based on [13].", "startOffset": 51, "endOffset": 55}, {"referenceID": 5, "context": "First, we construct the following function like [6] f(w) = \u03bb(\u03ba\u2212 1) 4 [1 2 wAw \u2212 \u3008e1, w\u3009 ]", "startOffset": 48, "endOffset": 51}, {"referenceID": 0, "context": "[ \u03bb(\u03ba\u22121) 2 A22 + \u03bbI ] x2 + \u03bb(\u03ba\u22121) 2 (A21x1 +A23x3)\u2212 \u03bb(\u03ba\u22121) 4 e [1] 1 j = 1 and f \u2032\u2032 jj(u) = \u03bb(\u03ba\u2212 1) 4 A22 + \u03bbI.", "startOffset": 63, "endOffset": 66}], "year": 2016, "abstractText": "Recently, there has been an increasing interest in designing distributed convex optimization algorithms under the setting where the data matrix is partitioned on features. Algorithms under this setting sometimes have many advantages over those under the setting where data is partitioned on samples, especially when the number of features is huge. Therefore, it is important to understand the inherent limitations of these optimization problems. In this paper, with certain restrictions on the communication allowed in the procedures, we develop tight lower bounds on communication rounds for a broad class of non-incremental algorithms under this setting. We also provide a lower bound on communication rounds for a class of (randomized) incremental algorithms.", "creator": "LaTeX with hyperref package"}}}