{"id": "1612.00814", "review": {"conference": "NIPS", "VERSION": "v1", "DATE_OF_SUBMISSION": "1-Dec-2016", "title": "Perspective Transformer Nets: Learning Single-View 3D Object Reconstruction without 3D Supervision", "abstract": "Understanding the 3D world is a fundamental problem in computer vision. However, learning a good representation of 3D objects is still an open problem due to the high dimensionality of the data and many factors of variation involved. In this work, we investigate the task of single-view 3D object reconstruction from a learning agent's perspective. We formulate the learning process as an interaction between 3D and 2D representations and propose an encoder-decoder network with a novel projection loss defined by the perspective transformation. More importantly, the projection loss enables the unsupervised learning using 2D observation without explicit 3D supervision. We demonstrate the ability of the model in generating 3D volume from a single 2D image with three sets of experiments: (1) learning from single-class objects; (2) learning from multi-class objects and (3) testing on novel object classes. Results show superior performance and better generalization ability for 3D object reconstruction when the projection loss is involved.", "histories": [["v1", "Thu, 1 Dec 2016 05:51:37 GMT  (8835kb,D)", "http://arxiv.org/abs/1612.00814v1", "published at NIPS 2016"], ["v2", "Thu, 16 Mar 2017 07:08:48 GMT  (8834kb,D)", "http://arxiv.org/abs/1612.00814v2", "published at NIPS 2016"], ["v3", "Sun, 13 Aug 2017 02:40:50 GMT  (8834kb,D)", "http://arxiv.org/abs/1612.00814v3", "published at NIPS 2016"]], "COMMENTS": "published at NIPS 2016", "reviews": [], "SUBJECTS": "cs.CV cs.GR cs.LG", "authors": ["xinchen yan", "jimei yang", "ersin yumer", "yijie guo", "honglak lee"], "accepted": true, "id": "1612.00814"}, "pdf": {"name": "1612.00814.pdf", "metadata": {"source": "CRF", "title": "Perspective Transformer Nets: Learning Single-View 3D Object Reconstruction without 3D Supervision", "authors": ["Xinchen Yan", "Jimei Yang", "Ersin Yumer", "Yijie Guo", "Honglak Lee"], "emails": ["glak}@umich.edu,", "jimyang@adobe.com", "yumer@adobe.com"], "sections": [{"heading": "1 Introduction", "text": "It is particularly important to solve this problem with the most convenient sensory impressions: 2D images. In this paper, we propose a holistic solution for the underlying shape (e.g. geometry, material), as well as the extrinsic properties that depend on the interaction with the observer."}, {"heading": "2 Related Work", "text": "In fact, most of them are able to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move and to move."}, {"heading": "3 Problem Formulation", "text": "In this section, we develop neural networks for the reconstruction of 3D objects. From the perspective of a learning agent (e.g. neural network), a natural way to understand a 3D object X from its 2D views is through transformations. By moving around the 3D object, the agent should be able to detect its unique properties and eventually create a 3D mental model of it, as illustrated in Figure 1 (a). Suppose I (k) is the 2D image from the K-th angle \u03b1 (k) by projection I (k) = P (X; \u03b1 (k))) or reproduction in graphics. An object X in a particular scene is the entanglement of shape, color, and texture (its intrinsic properties), and the image I (k) is the further entanglement with point of view and illumination (extrinsic parameters). The general goal of understanding 3D objects is the entanglement of shape, color, and texture (their intrinsic) properties."}, {"heading": "3.1 Learning to Reconstruct Volumetric 3D Shape from Single-View", "text": "We look at volumetric 3D reconstruction from a perspective as a dense prediction problem and develop a revolutionary encoder decoder network for this learning task, which is referred to by the decoder g (\u00b7). The encoder network h (\u00b7) learns a point-invariant latent representation h (I (k)), which is then used by the decoder g (\u00b7) to generate the volume V (h (k)). If the volumetric forms V are available, the problem can easily be viewed as learning volumetric 3D shapes with a regular reconstruction objective in 3D space: Lvol (I (k) = | f (I (k)) \u2212 V | 22. In practice, however, the volumetric truth with which we observe the volumetric 3D form is not available. For example, the agent observes the silhouette j through his built-in camera without accessing the volumetric 3D form."}, {"heading": "3.2 Perspective Transformer Networks", "text": "As previously defined, the 2D silhouette S (k) is achieved by perspective projection of the entered 3D volume V and the specific viewing angle of the camera \u03b1 (k). In this work, we implement the perspective projection (see Figure 1 (c)) with a transformation matrix of 4 by 4, where K is a camera calibration matrix and (R, t) are extrinsic parameters. \u2212 4 \u2212 4 = [K 0 \u2212 T 1] [R t 0T 1] (3) For each point psi = (x s i, y s i, z s i, 1) in 3D world coordinates, we calculate the corresponding point pti = (x t i, y i, d t i) in screen coordinates (plus disparity d t i) using perspective transformation: psi (4 \u00d7 4pti). Similar to the spatial transformer network that was introduced in [6] (1), we propose (1)."}, {"heading": "3.3 Training", "text": "Since the same volumetric 3D shape is expected to be generated from different images of the object, the encoder network is required to perform a 3D vision invariant latent representationh (I (1) = h (I (2)) = \u00b7 \u00b7 \u00b7 = h (I (k)) (5) This sub-problem itself is a challenging task in computer vision [23, 9]. Therefore, we apply a two-step training procedure: First, we learn the encoder network for a 3D view invariant latent representation h (I) and then we train the volume encoder with perspective transformer networks. As shown in [23], a detangled representation of 2D synthetic images can be learned from successive rotations using a recursive network by pre-training the encoder of our network using a similar curriculum strategy, so that the latent representation contains only 3D vision invariant identification information of the object. Once we obtain a 3D perspective, we obtain the encoder network relative to the identity of the concoder points, so that the next one encoder mutates the identity."}, {"heading": "4 Experiments", "text": "This dataset contains approximately 51,300 unique 3D models from 55 common object categories [1]. Each 3D model is rendered from 24 azimuth angles (with steps of 15 percent) with fixed elevation angles (30 percent) under the same camera and illumination constellation. We then cut and scale the centering region of each image to 64 x 64 milliseconds per pixel. For each reason and form of truth, we create a volume of 32 x 32 x 32 voxels from its canonical orientation (0 percent). Network architecture. As shown in Figure 2, our encoder network has three components: a revolutionary encoder, a revolutionary encoder, and a perspective transformer."}, {"heading": "4.1 Training on a single category", "text": "For model comparisons, we first perform quantitative evaluations on the generated 3D volume from the test set of single images. In view of a pair of soil-truth volumes and our generated volume (threshold is 0.5), we calculate the intersection-over-union value (IU) and the average IU value is calculated over 24 volumes of all instances generated in the test set. Furthermore, we offer a baseline method based on the search for the closest neighbor (NN). Specifically, for each of the test images, we create a VGG function from the fc6 layer (4096-dim-vector) and call up the closest training example using the Euclidean distance in the feature room. The ground-correct 3D volume corresponds to the nearest training model."}, {"heading": "4.2 Training on multiple categories", "text": "In the multi-category experiment, the training set comprised 13 main categories: airplane, bench, dresser, car, chair, screen, lamp, loudspeaker, rifle, sofa, table, telephone and vessel. We preserved 20% of the cases from each category as test data. As shown in Table 2, the quantitative results show that (1) models trained with combined loss are in most cases better than volume losses and (2) models trained with projection losses perform as well as volume / combined losses. The visualization results in Figure 5 show that all three models predict volumes reasonably well. There are only subtle differences in performance in object parts such as the wing of an aircraft."}, {"heading": "4.3 Out-of-Category Tests", "text": "Ideally, an intelligent agent should be able to generalize the knowledge he has learned from previously seen categories into invisible categories. To this end, we design out-of-category tests for both models trained in a single category and multiple categories, as described in Section 4.1 and Section 4.2 respectively. We select 5 invisible categories from ShapeNetCore: bed, bookcase, cabinet, motorcycle and train for out-of-category testing. Here, the two categories cabinet and train are relatively easier than other categories, as there may be cases of similar shapes in the training kit (e.g. chest of drawers, vessel and airplane), but bed, bookcase and motorcycle can be considered completely novel categories in terms of shape. We summarized the quantitative results in Table 3. Surprisingly, the model trained in multiple categories still achieves a relatively good overall IU. As shown in Figure 6, the proposed projection loss generates better than the model trained on combined losses or volume losses on motorcycle and motorcycle."}, {"heading": "5 Conclusions", "text": "In this paper, we examine the problem of 3D form reconstruction from the perspective of a learning agent. By formulating the learning process as an interaction between 3D form and 2D observation, we propose to learn an encoder decoder network that uses projection transformation as regularization. Experimental results show (1) the excellent performance of the proposed model in reconstructing the object even without 3D volume as supervision and (2) the generalization potential of the proposed model to invisible categories."}, {"heading": "Acknowledgments", "text": "This work was partially supported by NSF CAREER IIS-1453651, ONR N00014-13-1-0762, Sloan Research Fellowship and a gift from Adobe. We thank NVIDIA for donating GPUs. We also thank Yuting Zhang, Scott Reed, Junhyuk Oh, Ruben Villegas, Seunghoon Hong, Wenling Shang, Kibok Lee, Lajanugen Logeswaran, Rui Zhang and Yi Zhang for helpful comments and discussions."}, {"heading": "A Details regarding perspective transformer network", "text": "As defined in the main text, a two-step approach is proposed: (1) Perspective transformation is achieved by entering 3D volume V and specific camera perspectives (k). (2) Perspective projection (see Figure 7) with a 4-by-4 transformation matrix (4-4), where K is the calibration matrix and (R, t) the extrinsic parameters. (4-4) For each point psi = (x s), y s i, z s i, 1) in 3D world coordinates, we calculate the corresponding point pti = x t i, y t i) in screen coordinates (plus disparity d i) using perspective transformation: psi (4) Similar to the spatial transformer network, we propose a two-step: (1) Perspective transformation is assumed."}, {"heading": "B Details regarding learning from partial views", "text": "In our experiments, we have access to 2D projections from the entire 24 azimuth angles for each object in the training set. A natural but more difficult setting is to learn 3D reconstruction, with only partial views available for each object. To evaluate the performance gap when using partial views during training, we train the model in two different ways: 1) with a narrow azimuth range and 2) with sparse azimuths. For the first task, we limit the azimuth range to 105 percent (8 of 24 views); for the second, we offer 8 views that form a full 360 degree rotation, but with a larger step size of 45 percent. For both tasks, we perform the training based on the new limitations. Specifically, we train the encoder using the method proposed by Yang et al. [23] With a similar curriculum learning strategy: RNN-1, RNN-2, RN-7 and finally N7 views are only available during training (N-7)."}, {"heading": "C Additional visualization results on 3D volumetric shape reconstruction", "text": "As shown in Figure 8, Figure 9, Figure 10 and Figure 11, we offer additional side-by-side analysis for each of the three models we train. In each figure, each line is an independent comparison; the first column is the 2D image we used to input the model; the second and third columns show the true 3D volume (the same volume represented by two views for a better visualization purpose); similarly, we list the model trained only with projection loss (PTN-Proj), combined loss (PTN-Comb) and volume loss (CNN-Vol) from the fourth to the ninth column."}], "references": [], "referenceMentions": [], "year": 2016, "abstractText": "<lb>Understanding the 3D world is a fundamental problem in computer vision. How-<lb>ever, learning a good representation of 3D objects is still an open problem due<lb>to the high dimensionality of the data and many factors of variation involved. In<lb>this work, we investigate the task of single-view 3D object reconstruction from a<lb>learning agent\u2019s perspective. We formulate the learning process as an interaction<lb>between 3D and 2D representations and propose an encoder-decoder network with<lb>a novel projection loss defined by the perspective transformation. More importantly,<lb>the projection loss enables the unsupervised learning using 2D observation without<lb>explicit 3D supervision. We demonstrate the ability of the model in generating 3D<lb>volume from a single 2D image with three sets of experiments: (1) learning from<lb>single-class objects; (2) learning from multi-class objects and (3) testing on novel<lb>object classes. Results show superior performance and better generalization ability<lb>for 3D object reconstruction when the projection loss is involved.", "creator": "LaTeX with hyperref package"}}}