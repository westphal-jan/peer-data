{"id": "1612.01197", "review": {"conference": "acl", "VERSION": "v1", "DATE_OF_SUBMISSION": "4-Dec-2016", "title": "Neural Symbolic Machines: Learning Semantic Parsers on Freebase with Weak Supervision (Short Version)", "abstract": "Extending the success of deep neural networks to natural language understanding and symbolic reasoning requires complex operations and external memory. Recent neural program induction approaches have attempted to address this problem, but are typically limited to differentiable memory, and consequently cannot scale beyond small synthetic tasks. In this work, we propose the Manager-Programmer-Computer framework, which integrates neural networks with non-differentiable memory to support abstract, scalable and precise operations through a friendly neural computer interface. Specifically, we introduce a Neural Symbolic Machine, which contains a sequence-to-sequence neural \"programmer\", and a non-differentiable \"computer\" that is a Lisp interpreter with code assist. To successfully apply REINFORCE for training, we augment it with approximate gold programs found by an iterative maximum likelihood training process. NSM is able to learn a semantic parser from weak supervision over a large knowledge base. It achieves new state-of-the-art performance on WebQuestionsSP, a challenging semantic parsing dataset, with weak supervision. Compared to previous approaches, NSM is end-to-end, therefore does not rely on feature engineering or domain specific knowledge.", "histories": [["v1", "Sun, 4 Dec 2016 22:29:32 GMT  (74kb,D)", "http://arxiv.org/abs/1612.01197v1", "Published in NAMPI workshop at NIPS 2016. Short version ofarXiv:1611.00020"]], "COMMENTS": "Published in NAMPI workshop at NIPS 2016. Short version ofarXiv:1611.00020", "reviews": [], "SUBJECTS": "cs.CL cs.AI cs.LG", "authors": ["chen liang", "jonathan berant", "quoc le", "kenneth d forbus", "ni lao"], "accepted": true, "id": "1612.01197"}, "pdf": {"name": "1612.01197.pdf", "metadata": {"source": "CRF", "title": "Neural Symbolic Machines: Learning Semantic Parsers on Freebase with Weak Supervision (Short Version)", "authors": ["Chen Liang", "Jonathan Berant", "Quoc Le", "Kenneth Forbus", "Ni Lao"], "emails": ["chenliang2013@u.northwestern.edu", "forbus@u.northwestern.edu", "joberant@cs.tau.ac.il", "qvl@google.com", "nlao@google.com", "@NIPS"], "sections": [{"heading": "1 Introduction", "text": "In fact, most of them are able to survive on their own if they do not put themselves in a position to survive on their own."}, {"heading": "2 Neural Symbolic Machines", "text": "Now we describe in detail a neural symbolic machine that falls within the MPC framework and how it is used to learn semantic parsing from weak supervision. Semantic parsing is defined as follows: Using a knowledge base (KB) K and a question q = (w1, w2,..., wk), a program or logical form z is generated that, when executed against K, generates the correct answer y. Let E denote a set of entities (e.g. ABELINCOLN) 4 and let P denote a set of properties (or relationships, e.g. PLACEOFBIRTHOF). A knowledge base K is a set of assertions (e1, p, e2), such as (HODGENVILLE, PLACEOFBIRTHOF, ABELINCOLN)."}, {"heading": "2.1 \"Computer\": Lisp interpreter with code assist", "text": "In fact, it is as if most people who are able are able to determine themselves what they want and what they want to do, and that they have to do it. (...) In fact, it is as if most people who are able are able to determine themselves. (...) It is as if they are able to determine themselves. (...) It is as if they are able to determine themselves. (...) It is as if they are able to determine themselves. (...) It is as if they are able to determine themselves. (...) It is as if they are able to determine themselves. (...) It is as if they are able to determine themselves. (...) It is as if they are able to determine themselves what they want. (...) It is as if they do it, as if they want to do it. (...) It is as if they want to. (...)"}, {"heading": "2.2 \"Programmer\": key-variable memory augmented Seq2Seq model", "text": "The \"computer\" implements the operations (functions) and stores the values (intermediate results) in variables, simplifying the task for the \"programmer.\" The \"programmer\" only needs to include the natural language in a program that is a sequence of tokens that reference operations and values in the \"computer.\" We use a standard sequence-to-sequence model with attention and extend it with a simplified variant of LSTM to reference the values. A typical sequence-to-sequence model consists of two RNs, an encoder and a decoder. We used a 1-layer GRU [5], which is a simplified variant of LSTM, both for the encoder and the decoder."}, {"heading": "2.3 Training NSM with Weak Supervision", "text": "To efficiently protect NSM from weak monitoring, we use the REINFORCE algorithm [15, 19]. However, the REINFORCE target is known to be very difficult to optimize from the ground up. Therefore, we supplement it with approximate gold programs found with maximum probability through an iterative training process. During training, the model always places a reasonable degree of probability on the best programs found to date, and anchoring the model in these highly rewarded programs significantly speeds up training and helps to avoid a local optimum. Further details of the training process can be found in the long version."}, {"heading": "3 Experiments and analysis", "text": "Modern semantic parsers [4], which map natural language expressions to executable logical forms, have been successfully trained over large knowledge bases of weak oversight [17], but require substantial feature engineering. Recent attempts to form an end-to-end neural network for semantic parsing [6, 10] have either used strong oversight (complete logical forms) or synthetic datasets. We use NSM to learn a weak-supervised semantic parser without manual engineering. we have used the challenging semantic parsing dataset WEBQUESTIONSSP [18], which consists of 3,098 question-answer pairs for training and 1,639 for testing. These questions were collected using Google Suggest API and the answers were originally obtained with Amazon Mechanical Turk and updated by annotators familiar with the design of the frease [18]."}], "references": [{"title": "Learning to compose neural networks for question answering", "author": ["Jacob Andreas", "Marcus Rohrbach", "Trevor Darrell", "Dan Klein"], "venue": null, "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2016}, {"title": "Neural machine translation by jointly learning to align and translate", "author": ["Dzmitry Bahdanau", "Kyunghyun Cho", "Yoshua Bengio"], "venue": "CoRR, abs/1409.0473,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2014}, {"title": "Semantic parsing on freebase from question-answer pairs", "author": ["Jonathan Berant", "Andrew Chou", "Roy Frostig", "Percy Liang"], "venue": "In EMNLP,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2013}, {"title": "Learning phrase representations using rnn encoder\u2013 decoder for statistical machine translation", "author": ["Kyunghyun Cho", "Bart van Merrienboer", "Caglar Gulcehre", "Dzmitry Bahdanau", "Fethi Bougares", "Holger Schwenk", "Yoshua Bengio"], "venue": "In Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP),", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2014}, {"title": "Language to logical form with neural attention", "author": ["Li Dong", "Mirella Lapata"], "venue": "In Association for Computational Linguistics (ACL),", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2016}, {"title": "Neural turing machines", "author": ["Alex Graves", "Greg Wayne", "Ivo Danihelka"], "venue": "arXiv preprint arXiv:1410.5401,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2014}, {"title": "Hybrid computing using a neural network", "author": ["Alex Graves", "Greg Wayne", "Malcolm Reynolds", "Tim Harley", "Ivo Danihelka", "Agnieszka Grabska- Barwinska", "Sergio G. Colmenarejo", "Edward Grefenstette", "Tiago Ramalho", "John Agapiou", "Adri\u00c3 P. Badia", "Karl M. Hermann", "Yori Zwols", "Georg Ostrovski", "Adam Cain", "Helen King", "Christopher Summerfield", "Phil Blunsom", "Koray Kavukcuoglu", "Demis Hassabis"], "venue": null, "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2016}, {"title": "Deep neural networks for acoustic modeling in speech recognition: The shared views of four research groups", "author": ["Geoffrey Hinton", "Li Deng", "Dong Yu", "George E Dahl", "Abdel-rahman Mohamed", "Navdeep Jaitly", "Andrew Senior", "Vincent Vanhoucke", "Patrick Nguyen", "Tara N Sainath"], "venue": "IEEE Signal Processing Magazine,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2012}, {"title": "Data recombination for neural semantic parsing", "author": ["Robin Jia", "Percy Liang"], "venue": "In Association for Computational Linguistics (ACL),", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2016}, {"title": "Neural gpus learn algorithms", "author": ["\u0141ukasz Kaiser", "Ilya Sutskever"], "venue": "arXiv preprint arXiv:1511.08228,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2015}, {"title": "Neural programmer: Inducing latent programs with gradient descent", "author": ["Arvind Neelakantan", "Quoc V. Le", "Ilya Sutskever"], "venue": "CoRR, abs/1511.04834,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2015}, {"title": "Neural programmer-interpreters", "author": ["Scott Reed", "Nando de Freitas"], "venue": "In ICLR,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2016}, {"title": "Sequence to sequence learning with neural networks", "author": ["Ilya Sutskever", "Oriol Vinyals", "Quoc V Le"], "venue": "In Advances in neural information processing systems,", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2014}, {"title": "Simple statistical gradient-following algorithms for connectionist reinforcement learning", "author": ["Ronald J. Williams"], "venue": "In Machine Learning,", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 1992}, {"title": "Google\u2019s neural machine translation system: Bridging the gap between human and machine", "author": ["Yonghui Wu", "Mike Schuster", "Zhifeng Chen", "Quoc V. Le", "Mohammad Norouzi", "Wolfgang Macherey", "Maxim Krikun", "Yuan Cao", "Qin Gao", "Klaus Macherey", "Jeff Klingner", "Apurva Shah", "Melvin Johnson", "Xiaobing Liu", "Lukasz Kaiser", "Stephan Gouws", "Yoshikiyo Kato", "Taku Kudo", "Hideto Kazawa", "Keith Stevens", "George Kurian", "Nishant Patil", "Wei Wang", "Cliff Young", "Jason Smith", "Jason Riesa", "Alex Rudnick", "Oriol Vinyals", "Greg Corrado", "Macduff Hughes", "Jeffrey Dean"], "venue": "translation. CoRR,", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2016}, {"title": "Semantic parsing via staged query graph generation: Question answering with knowledge base", "author": ["Wen-tau Yih", "Ming-Wei Chang", "Xiaodong He", "Jianfeng Gao"], "venue": "In Association for Computational Linguistics (ACL),", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2015}, {"title": "The value of semantic parse labeling for knowledge base question answering", "author": ["Wen-tau Yih", "Matthew Richardson", "Chris Meek", "Ming-Wei Chang", "Jina Suh"], "venue": "In Association for Computational Linguistics (ACL),", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2016}, {"title": "Reinforcement learning neural turing machines", "author": ["Wojciech Zaremba", "Ilya Sutskever"], "venue": "arXiv preprint arXiv:1505.00521,", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2015}], "referenceMentions": [{"referenceID": 7, "context": "Deep neural networks have achieved impressive performance in classification and structured prediction tasks with full supervision such as speech recognition [9] and machine translation [14, 2, 16].", "startOffset": 157, "endOffset": 160}, {"referenceID": 12, "context": "Deep neural networks have achieved impressive performance in classification and structured prediction tasks with full supervision such as speech recognition [9] and machine translation [14, 2, 16].", "startOffset": 185, "endOffset": 196}, {"referenceID": 1, "context": "Deep neural networks have achieved impressive performance in classification and structured prediction tasks with full supervision such as speech recognition [9] and machine translation [14, 2, 16].", "startOffset": 185, "endOffset": 196}, {"referenceID": 14, "context": "Deep neural networks have achieved impressive performance in classification and structured prediction tasks with full supervision such as speech recognition [9] and machine translation [14, 2, 16].", "startOffset": 185, "endOffset": 196}, {"referenceID": 5, "context": "There were several recent attempts to address this problem in neural program induction [7, 12, 13, 11, 19, 8, 1], which learn programs by using a neural sequence model to control a computation component.", "startOffset": 87, "endOffset": 112}, {"referenceID": 10, "context": "There were several recent attempts to address this problem in neural program induction [7, 12, 13, 11, 19, 8, 1], which learn programs by using a neural sequence model to control a computation component.", "startOffset": 87, "endOffset": 112}, {"referenceID": 11, "context": "There were several recent attempts to address this problem in neural program induction [7, 12, 13, 11, 19, 8, 1], which learn programs by using a neural sequence model to control a computation component.", "startOffset": 87, "endOffset": 112}, {"referenceID": 9, "context": "There were several recent attempts to address this problem in neural program induction [7, 12, 13, 11, 19, 8, 1], which learn programs by using a neural sequence model to control a computation component.", "startOffset": 87, "endOffset": 112}, {"referenceID": 17, "context": "There were several recent attempts to address this problem in neural program induction [7, 12, 13, 11, 19, 8, 1], which learn programs by using a neural sequence model to control a computation component.", "startOffset": 87, "endOffset": 112}, {"referenceID": 6, "context": "There were several recent attempts to address this problem in neural program induction [7, 12, 13, 11, 19, 8, 1], which learn programs by using a neural sequence model to control a computation component.", "startOffset": 87, "endOffset": 112}, {"referenceID": 0, "context": "There were several recent attempts to address this problem in neural program induction [7, 12, 13, 11, 19, 8, 1], which learn programs by using a neural sequence model to control a computation component.", "startOffset": 87, "endOffset": 112}, {"referenceID": 17, "context": "However, the memories in these models are either low-level (such as in Neural Turing machines[19]), or differentiable so that they can be trained by backpropagation.", "startOffset": 93, "endOffset": 97}, {"referenceID": 16, "context": "On the challenging semantic parsing dataset WEBQUESTIONSSP [18], NSM achieves new state-of-the-art results with weak supervision.", "startOffset": 59, "endOffset": 63}, {"referenceID": 5, "context": "Operations learned by current neural network models with differentiable memory, such as addition or sorting, do not generalize perfectly to inputs that are larger than previously observed ones [7, 13].", "startOffset": 193, "endOffset": 200}, {"referenceID": 11, "context": "Operations learned by current neural network models with differentiable memory, such as addition or sorting, do not generalize perfectly to inputs that are larger than previously observed ones [7, 13].", "startOffset": 193, "endOffset": 200}, {"referenceID": 15, "context": "The programs that can be executed by it are equivalent to the limited subset of \u03bb-calculus in [17], but easier for a sequence-to-sequence model to generate given Lisp\u2019s simple syntax.", "startOffset": 94, "endOffset": 98}, {"referenceID": 3, "context": "We used a 1-layer GRU [5], which is a simplified variant of LSTM, for both the encoder and the decoder.", "startOffset": 22, "endOffset": 25}, {"referenceID": 4, "context": "We adopt a dot-product attention similar to that of [6].", "startOffset": 52, "endOffset": 55}, {"referenceID": 13, "context": "To efficiently train NSM from weak supervision, we apply the REINFORCE algorithm [15, 19].", "startOffset": 81, "endOffset": 89}, {"referenceID": 17, "context": "To efficiently train NSM from weak supervision, we apply the REINFORCE algorithm [15, 19].", "startOffset": 81, "endOffset": 89}, {"referenceID": 15, "context": "Modern semantic parsers [4], which map natural language utterances to executable logical forms, have been successfully trained over large knowledge bases from weak supervision[17], but require substantial feature engineering.", "startOffset": 175, "endOffset": 179}, {"referenceID": 4, "context": "Recent attempts to train an end-to-end neural network for semantic parsing [6, 10] have either used strong supervision (full logical forms), or have employed synthetic datasets.", "startOffset": 75, "endOffset": 82}, {"referenceID": 8, "context": "Recent attempts to train an end-to-end neural network for semantic parsing [6, 10] have either used strong supervision (full logical forms), or have employed synthetic datasets.", "startOffset": 75, "endOffset": 82}, {"referenceID": 16, "context": "we used the challenging semantic parsing dataset WEBQUESTIONSSP [18], which consists of 3,098 question-answer pairs for training and 1,639 for testing.", "startOffset": 64, "endOffset": 68}, {"referenceID": 2, "context": "These questions were collected using Google Suggest API and the answers were originally obtained [3] using Amazon Mechanical Turk and updated by annotators who are familiar with the design of Freebase [18].", "startOffset": 97, "endOffset": 100}, {"referenceID": 16, "context": "These questions were collected using Google Suggest API and the answers were originally obtained [3] using Amazon Mechanical Turk and updated by annotators who are familiar with the design of Freebase [18].", "startOffset": 201, "endOffset": 205}, {"referenceID": 15, "context": "The quality of the entity resolution is similar to that of [17] with about 94% of the gold root entities being included in the resolution results.", "startOffset": 59, "endOffset": 63}, {"referenceID": 4, "context": "Similar to [6], we also replaced named entity tokens with a special token \"ENT\".", "startOffset": 11, "endOffset": 14}, {"referenceID": 15, "context": "Following [17] we use the last public available snapshot of Freebase KB.", "startOffset": 10, "endOffset": 14}, {"referenceID": 16, "context": "The comparison with previous state-of-the-art [18, 17] is shown in Table 2.", "startOffset": 46, "endOffset": 54}, {"referenceID": 15, "context": "The comparison with previous state-of-the-art [18, 17] is shown in Table 2.", "startOffset": 46, "endOffset": 54}], "year": 2016, "abstractText": "Extending the success of deep neural networks to natural language understanding and symbolic reasoning requires complex operations and external memory. Recent neural program induction approaches have attempted to address this problem, but are typically limited to differentiable memory, and consequently cannot scale beyond small synthetic tasks. In this work, we propose the Manager-ProgrammerComputer framework, which integrates neural networks with non-differentiable memory to support abstract, scalable and precise operations through a friendly neural computer interface. Specifically, we introduce a Neural Symbolic Machine, which contains a sequence-to-sequence neural \"programmer\", and a nondifferentiable \"computer\" that is a Lisp interpreter with code assist. To successfully apply REINFORCE for training, we augment it with approximate gold programs found by an iterative maximum likelihood training process. NSM is able to learn a semantic parser from weak supervision over a large knowledge base. It achieves new state-of-the-art performance on WEBQUESTIONSSP, a challenging semantic parsing dataset. Compared to previous approaches, NSM is end-to-end, therefore does not rely on feature engineering or domain specific knowledge.", "creator": "LaTeX with hyperref package"}}}