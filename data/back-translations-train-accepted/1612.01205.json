{"id": "1612.01205", "review": {"conference": "ICML", "VERSION": "v1", "DATE_OF_SUBMISSION": "4-Dec-2016", "title": "Optimal and Adaptive Off-policy Evaluation in Contextual Bandits", "abstract": "We consider the problem of off-policy evaluation---estimating the value of a target policy using data collected by another policy---under the contextual bandit model. We establish a minimax lower bound on the mean squared error (MSE), and show that it is matched up to constant factors by the inverse propensity scoring (IPS) estimator. Since in the multi-armed bandit problem the IPS is suboptimal (Li et. al, 2015), our result highlights the difficulty of the contextual setting with non-degenerate context distributions. We further consider improvements on this minimax MSE bound, given access to a reward model. We show that the existing doubly robust approach, which utilizes such a reward model, may continue to suffer from high variance even when the reward model is perfect. We propose a new estimator called SWITCH which more effectively uses the reward model and achieves a superior bias-variance tradeoff compared with prior work. We prove an upper bound on its MSE and demonstrate its benefits empirically on a diverse collection of datasets, often seeing orders of magnitude improvements over a number of baselines.", "histories": [["v1", "Sun, 4 Dec 2016 23:24:17 GMT  (171kb,D)", "http://arxiv.org/abs/1612.01205v1", null]], "reviews": [], "SUBJECTS": "stat.ML cs.LG", "authors": ["yu-xiang wang", "alekh agarwal", "miroslav dud\u00edk"], "accepted": true, "id": "1612.01205"}, "pdf": {"name": "1612.01205.pdf", "metadata": {"source": "CRF", "title": "Optimal and Adaptive Off-policy Evaluation in Contextual Bandits", "authors": ["Yu-Xiang Wang", "Alekh Agarwal", "Miroslav Dud\u00edk"], "emails": [], "sections": [{"heading": "1 Introduction", "text": "In fact, the fact is that you are able to be in a position without seeing yourself in a position to be in."}, {"heading": "2 Setup and related work", "text": "As in earlier works by Dud\u00edk et al. [2014], we use the standard structure for the non-political evaluation of contextual bandits. We begin with the formal structure and then discuss the related works."}, {"heading": "2.1 Formal setup", "text": "In contextual bandit problems, the learning agent observes a context x, takes an action > a and observes a scalar reward that measures the quality of the chosen action for the context. Here, the context x is a feature vector of a finite set X, possibly {0, 1} d, or a machine-precise encoding of Rd for some large ones. The agent's decision rule is called a policy that represents a function from contexts to distributions of actions to allow a randomized action choice. We will use \u00b5 (a | x) to name the logging and target policies accordingly. The agent's decision rule is called a policy that represents a function from contexts to distributions of actions to enable a randomized action selection."}, {"heading": "2.2 Related work", "text": "There are several papers that focus on the creation of estimators for the non-political evaluation problem and indicate upper limits for the Mean Squared Error (MSE) of these estimators as discussed in the introduction. As we will show, the naive extension of their results to contextual bandits is relatively new, on the other hand. Jiang and Li [2016] initially investigated this question under a minimax setup, but focused on the multi-arm bandit problem. As we will show, the naive extension of their results to contextual bandits leads to a sub-optimal lower limit. Jiang and Li [2016] built at the head of Li et al. [2015] a Cramer-Rao style demonstrates lower limits for policy evaluation in enhancing learning, but the adoption of a small number of the observed states proceeds."}, {"heading": "3 Limits of off-policy evaluation", "text": "In this section, we will present our main finding on the lower limits of non-political evaluation, starting with the Minimax framework for examining such issues, before presenting the outcome and its implications."}, {"heading": "3.1 Minimax framework", "text": "We will examine this problem within a standard Minimax framework and try to answer the following question: What is the smallest mean square error (MSE) that any estimator can achieve in the worst case over a large class of contextual bandit problems? As is customary in the Minimax environment, we want the class of problems to be rich enough so that the estimation problem is not trivialized and must be small enough so that the lower limits are not driven by complete pathologies. In our problem, we make the choice to resolve such a case and only accept the worst case over a class of reward distributions, allowing the upper and lower limits as well as the estimators to adjust with and depending on the ratios. The family of reward distributions D (r | x, a) that we are studying is a natural generalization of the class of Li et al."}, {"heading": "3.2 Minimax lower bound for off-policy evaluation", "text": "To describe our result, we define a convenient piece of notationC: = 22 + \u00b7 max {E\u00b5), unless the upper limit is reached. For all problem cases requiring the satisfaction of acceptance 1 with and without any n maximum satisfaction, the upper limit is 2 +, E\u00b5 [(\u03c1Rmax), 2 +], 2 +, 2 +, 2 +, 2 +, 2 +, 4 +, the upper limit of the satisfaction of the lower limit 2 +} (4) Theorem 1 (Minimax lower bound). For all problem cases requiring the satisfaction of acceptance 1 with and any n maximum satisfaction of the upper limit, {5C1 / E\u00b5 / E\u00b5 [\u03c32 / R2max] reaching the lower limit of the satisfaction of the satisfaction of the lower limit of the satisfaction of the lower limit."}, {"heading": "4 Incorporating model-based approaches in policy evaluation", "text": "Among our twin goals of optimal and adaptive estimators, the discussion so far has revolved around the optimality of IPS estimators in the minimax sense. However, real data sets rarely show the worst-case behavior, and in this section we discuss approaches to using additional structure in the data when such knowledge is available. We begin with the necessary build-up before introducing our new estimator and its properties. During this section, we drop the superscript \u03c0 from the estimators as the valuation policy is determined during this discussion."}, {"heading": "4.1 The need for model-based approaches", "text": "As we have seen in the last section, the model-free approach has an information-theoretical limit that is quadratically dependent on Rmax, size, and meaning, which is good news in that it implies the existence of an optimal estimator-IPS approach. However, it also significantly limits which measures can be reliably evaluated, since the quadratic dependence on other measures is not great. The main reason for this limitation is that the number of measures so far allows for completely arbitrary reward models - E [r] x, a policy that can be reliably evaluated, cannot rely much on such measures without leading to unreliable estimates. The main reason for this limitation is that the setup allows for completely arbitrary reward models - E [r] x, a) can change arbitrarily across different actions and contexts. Real data sets are so pathological, and often we have substantial intuitions about contexts and actions based on specific applications."}, {"heading": "4.2 The SWITCH estimators to incorporate reward models", "text": "We present a class of estimators that depict all available reward models very differently from the DR approach, and the starting point for our method is the observation that insisting on maintaining unbiasednessput can drastically reduce the DR estimator at the extreme end of the bias-variance compromise threshold (see, for example, Bottou et al. [2013] for a detailed discussion that can often drastically reduce the variance in the cost of a small bias. We take intuition a step further and suggest estimating the rewards for actions differently based on whether they have a large or a small significance in light of context. If the importance is low, we continue to use our preferred unbiased estimators, but switch to using the (potentially biased) reward model for actions of great importance, defining small and large thresholds."}, {"heading": "4.3 Automatic parameter tuning", "text": "So far we have discussed the properties of the SWITCH estimators, assuming that the parameter \u03c4 = nominal value is well chosen. (On the other hand, the estimator may be as bad as DM in the worst case, if poorly chosen, as evidenced by the bias term in Theorem 2. (Unless the estimators are useful in practice), it becomes necessary to have a method to select a good value of GDP. (A natural criterion for selection is that the MSE of the resulting estimator can be minimized.) Since we do not know the exact MSE (since it is unknown), an alternative is necessary to minimize a data-dependent estimate for it. (It is assumed that the MSE can be written as the sum of the variance and quared bias, we can individually assess and bind the conditions. We can estimate the variance of the SWITCH estimators in a simple way from the data. (Let us let Yi) denote the estimated value we observe according to the SWITCH."}, {"heading": "5 Experiments", "text": "In this section, we will conduct an experimental evaluation of the proposed SWITCH estimators. We will use the same 10 UCI datasets previously used by Dud\u00edk et al. [2011] and convert the multi-class classification problem into contextual bandits by1. which means that the learner gets an example at a time and compares his label on the basis of guidelines we use is the deterministic decision of an learned logistic regression classifier, while the logging policy we apply is the probability estimate of a learned logistic regression classifier on a covaried dataset. We will simulate the covaried data using the same technique as in Dud\u00edk et al. [2011], which follows standard practice as described by Greet."}, {"heading": "6 Conclusion", "text": "In this article, we performed Minimax analyses of the non-political evaluation of context-dependent bandits and showed that IPS is optimal at worst. This result underscores the need to use ancillary information, potentially 2For clarity of presentation, we excluded SWITCH, which also performs significantly better than IPS, but from SWITCH-DR. For the same reason, we combined the trim and truncated / reweighted IPS provided by direct modeling of reward, especially if the weights of importance are too large. In light of this observation, we proposed a new class of estimators called SWITCH, which can be used to combine all key samples, including IPS and DR, with DM. The estimator includes the adaptive switch to DM when the weights of importance are large, and the switch to IPS or DR when the weights of importance are small. We showed that the new estimator has favorable theoretical properties and also works well on real data."}, {"heading": "Acknowledgments", "text": "The authors thank Lihong Li and John Langford for helpful discussions, Edward Kennedy for drawing our attention to related issues and the recent evolution of causal conclusions, and Wei He for pointing us to [Sun, 2006] for rigorous scale theory treatment of a continuum of random variables and the corresponding law of large numbers. YW was supported by the NSF Award BCS-0941518 to CMU Statistics, a Singapore NRF fellowship under its International Research Centre @ Singapore Funding Initiative, and a Baidu Fellowship."}, {"heading": "A Proof of Theorem 1", "text": "In this appendix we occupy the minimax limit of theorem 1. The result results by combining the following two lower limits: theorem 3 (Lower bound 1) For each example of a problem, so that E\u00b5 [\u03c12\u03c32] < \u221e, we haveRn (\u03c0; \u03bb, \u00b5, \u03c3, Rmax) \u2265 log 2\u00b5 (Lower bound 2) 8n1 \u2212 E\u00b5 [\u03c12\u03c32 > Rmax \u221a n\u00ba 2] / (2 log 2) E\u00b5 [2)] E\u00b5 [\u03c12\u03c32\u0445 2 (Lower bound 2) Lower bound 2) E\u00b5 (Lower bound 2) For each example of a problem, so that E\u00b5 2R2max] < we haveRn (Number 2 / Number Number Number Number Number 2 / Number 2 / Number 2 / Number 2 / Number 2 / Number 2 / Number 2 / Number 2 / Number 2"}, {"heading": "B Other proofs", "text": "The variance (therefore MSE) of weight weighting can be divided into two parts (Var (v-IPS) = E\u00b5 (v-IPS) = Var (v-IPS-x, a) + Var\u00b5 (E (v-IPS-x, a) + 1 n E\u00b5 (r-x, a) + 1 n E\u00b5 (r-x, a) + 1 n E\u00b5 (n-x) + 1 n E\u00b5 (r-x, a) + 1 n E\u00b5 (r-x, a) + 1 n E\u00b5 (r-x, a) + 1 n E\u00b5 (r-x, a). Denote Ax A to be Ax = [a-x-x-x), a-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x, a-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x, a-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x"}, {"heading": "C Utility Lemmas", "text": "Lemma 3 (Hoeffding, 1963, Theorem 2) If we allow Xi [ai, bi] and X1,..., Xn to be drawn independently of each other, we obtain the empirical mean X = 1n (X1 +... + Xn) obeys P (| X, \u2212 E [X] | \u2265 t) \u2264 2e \u2212 2n 2t2 \u2211 n i = 1 (bi \u2212 ai) 2.Lemma 4 (Bernoulli KL divergence). For 0 < p, q < 1 we have DKL (Ber (p) and Ber (q)) \u2264 (p \u2212 q) 2 (1q + 11 \u2212 q).Evidence DKL (Ber (p) and Ber (q)) = p log (p q) + (1 \u2212 p)."}], "references": [{"title": "Doubly robust estimation in missing data and causal inference models", "author": ["Heejung Bang", "James M Robins"], "venue": null, "citeRegEx": "Bang and Robins.,? \\Q2005\\E", "shortCiteRegEx": "Bang and Robins.", "year": 2005}, {"title": "Data-adaptive selection of the truncation level for inverseprobability-of-treatment-weighted estimators", "author": ["Oliver Bembom", "Mark J van der Laan"], "venue": null, "citeRegEx": "Bembom and Laan.,? \\Q2008\\E", "shortCiteRegEx": "Bembom and Laan.", "year": 2008}, {"title": "Counterfactual reasoning and learning systems: the example of computational advertising", "author": ["L\u00e9on Bottou", "Jonas Peters", "Joaquin Quinonero Candela", "Denis Xavier Charles", "Max Chickering", "Elon Portugaly", "Dipankar Ray", "Patrice Y Simard", "Ed Snelson"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Bottou et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Bottou et al\\.", "year": 2013}, {"title": "Some results on generalized difference estimation and generalized regression estimation for finite populations", "author": ["Claes M Cassel", "Carl E S\u00e4rndal", "Jan H Wretman"], "venue": null, "citeRegEx": "Cassel et al\\.,? \\Q1976\\E", "shortCiteRegEx": "Cassel et al\\.", "year": 1976}, {"title": "Learning bounds for importance weighting", "author": ["Corinna Cortes", "Yishay Mansour", "Mehryar Mohri"], "venue": "In Advances in neural information processing systems,", "citeRegEx": "Cortes et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Cortes et al\\.", "year": 2010}, {"title": "Doubly robust policy evaluation and learning", "author": ["Miroslav Dud\u00edk", "John Langford", "Lihong Li"], "venue": "In ICML,", "citeRegEx": "Dud\u00edk et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Dud\u00edk et al\\.", "year": 2011}, {"title": "Doubly robust policy evaluation and optimization", "author": ["Miroslav Dud\u00edk", "Dumitru Erhan", "John Langford", "Lihong Li"], "venue": "Statistical Science,", "citeRegEx": "Dud\u00edk et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Dud\u00edk et al\\.", "year": 2014}, {"title": "Covariate shift by kernel mean matching", "author": ["Arthur Gretton", "Alex Smola", "Jiayuan Huang", "Marcel Schmittfull", "Karsten Borgwardt", "Bernhard Sch\u00f6lkopf"], "venue": "Dataset shift in machine learning,", "citeRegEx": "Gretton et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Gretton et al\\.", "year": 2009}, {"title": "Probability inequalities for sums of bounded random variables", "author": ["Wassily Hoeffding"], "venue": "Journal of the American statistical association,", "citeRegEx": "Hoeffding.,? \\Q1963\\E", "shortCiteRegEx": "Hoeffding.", "year": 1963}, {"title": "Statistics and causal inference", "author": ["Paul W Holland"], "venue": "Journal of the American statistical Association,", "citeRegEx": "Holland.,? \\Q1986\\E", "shortCiteRegEx": "Holland.", "year": 1986}, {"title": "A generalization of sampling without replacement from a finite universe", "author": ["Daniel G Horvitz", "Donovan J Thompson"], "venue": "Journal of the American statistical Association,", "citeRegEx": "Horvitz and Thompson.,? \\Q1952\\E", "shortCiteRegEx": "Horvitz and Thompson.", "year": 1952}, {"title": "Doubly robust off-policy evaluation for reinforcement learning", "author": ["Nan Jiang", "Lihong Li"], "venue": "In ICML\u201916,", "citeRegEx": "Jiang and Li.,? \\Q2016\\E", "shortCiteRegEx": "Jiang and Li.", "year": 2016}, {"title": "Unbiased offline evaluation of contextual-banditbased news article recommendation algorithms", "author": ["Lihong Li", "Wei Chu", "John Langford", "Xuanhui Wang"], "venue": "In Proceedings of the fourth ACM international conference on Web search and data mining,", "citeRegEx": "Li et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Li et al\\.", "year": 2011}, {"title": "Toward minimax off-policy value estimation", "author": ["Lihong Li", "R\u00e9mi Munos", "Csaba Szepesv\u00e1ri"], "venue": "In AISTATS,", "citeRegEx": "Li et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Li et al\\.", "year": 2015}, {"title": "Statistical analysis with missing data", "author": ["Roderick JA Little", "Donald B Rubin"], "venue": null, "citeRegEx": "Little and Rubin.,? \\Q2002\\E", "shortCiteRegEx": "Little and Rubin.", "year": 2002}, {"title": "Weighting adjustment for unit nonresponse", "author": ["H Lock Oh", "Frederick J Scheuren"], "venue": "Incomplete data in sample surveys,", "citeRegEx": "Oh and Scheuren.,? \\Q1983\\E", "shortCiteRegEx": "Oh and Scheuren.", "year": 1983}, {"title": "Semiparametric efficiency in multivariate regression models with missing data", "author": ["James M Robins", "Andrea Rotnitzky"], "venue": "Journal of the American Statistical Association,", "citeRegEx": "Robins and Rotnitzky.,? \\Q1995\\E", "shortCiteRegEx": "Robins and Rotnitzky.", "year": 1995}, {"title": "Improved double-robust estimation in missing data and causal inference models", "author": ["Andrea Rotnitzky", "Quanhong Lei", "Mariela Sued", "James M Robins"], "venue": null, "citeRegEx": "Rotnitzky et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Rotnitzky et al\\.", "year": 2012}, {"title": "The exact law of large numbers via fubini extension and characterization of insurable risks", "author": ["Yeneng Sun"], "venue": "Journal of Economic Theory,", "citeRegEx": "Sun.,? \\Q2006\\E", "shortCiteRegEx": "Sun.", "year": 2006}, {"title": "Data-efficient off-policy policy evaluation for reinforcement learning", "author": ["Philip S Thomas", "Emma Brunskill"], "venue": "In ICML\u201916,", "citeRegEx": "Thomas and Brunskill.,? \\Q2016\\E", "shortCiteRegEx": "Thomas and Brunskill.", "year": 2016}], "referenceMentions": [{"referenceID": 13, "context": "Since in the multi-armed bandit problem the IPS is suboptimal [Li et al., 2015], our result highlights the difficulty of the contextual setting with non-degenerate context distributions.", "startOffset": 62, "endOffset": 79}, {"referenceID": 10, "context": "In particular, there are several estimators which are unbiased under mild assumptions, such as inverse propensity scoring (IPS) [Horvitz and Thompson, 1952], and sharp estimates on their mean squared error (MSE) for policy evaluation are well-known [Dud\u00edk et al.", "startOffset": 128, "endOffset": 156}, {"referenceID": 6, "context": "In particular, there are several estimators which are unbiased under mild assumptions, such as inverse propensity scoring (IPS) [Horvitz and Thompson, 1952], and sharp estimates on their mean squared error (MSE) for policy evaluation are well-known [Dud\u00edk et al., 2014].", "startOffset": 249, "endOffset": 269}, {"referenceID": 6, "context": "Some approaches, such as the doubly-robust method (DR) [Dud\u00edk et al., 2014] (also see the references therein for its origin in statistics and application in causal inference, e.", "startOffset": 55, "endOffset": 75}, {"referenceID": 0, "context": ", [Robins and Rotnitzky, 1995, Bang and Robins, 2005]), combine the model with an IPS-style unbiased estimation and remain consistent, with known estimates for their MSE. All these works focus on developing specific methods alongside upper bounds on their MSE. Little work, on the other hand, exists on the question of the fundamental statistical hardness of off-policy evaluation and the optimality (or the lack of) of the existing methods. A notable exception is the recent work of Li et al. [2015], who study off-policy evaluation in multi-armed bandits\u2014a special case of our setting, without any contexts\u2014and provide a minimax lower bound on the MSE.", "startOffset": 31, "endOffset": 501}, {"referenceID": 13, "context": "In contrast with context-free multi-armed bandits [Li et al., 2015], our lower bound matches the MSE upper bound for IPS up to constants, so long as the contexts have a non-degenerate distribution.", "startOffset": 50, "endOffset": 67}, {"referenceID": 5, "context": "We use the standard setup for off-policy evaluation in contextual bandits as in prior works Dud\u00edk et al. [2014]. We start with the formal setup and then discuss the related work.", "startOffset": 92, "endOffset": 112}, {"referenceID": 10, "context": "The simplest estimator in this setting, called Inverse Propensity Scoring (IPS) [Horvitz and Thompson, 1952] is defined as: v\u0302 IPS = n \u2211", "startOffset": 80, "endOffset": 108}, {"referenceID": 6, "context": ", 2012] as well as the references in [Dud\u00edk et al., 2014]).", "startOffset": 37, "endOffset": 57}, {"referenceID": 5, "context": "Li et al. [2015] first studied this question under a minimax setup, but focused on the multi-arm bandits problem.", "startOffset": 0, "endOffset": 17}, {"referenceID": 5, "context": "Jiang and Li [2016] built on top of Li et al.", "startOffset": 0, "endOffset": 20}, {"referenceID": 5, "context": "Jiang and Li [2016] built on top of Li et al. [2015] proves a Cramer-Rao-style lower bound for policy-evaluation in reinforcement learning, but the assumption of a small number of observed states precludes contextual bandits from being a special case.", "startOffset": 0, "endOffset": 53}, {"referenceID": 3, "context": "Cortes et al. [2010] studied the related covariate shift problem in the statistical learning setting and proved upper bounds and lower bounds of the minimax excess risk that depends on the Renyi divergence of the two covariate distributions.", "startOffset": 0, "endOffset": 21}, {"referenceID": 11, "context": ", 2011, 2014] and reinforcement learning [Jiang and Li, 2016].", "startOffset": 41, "endOffset": 61}, {"referenceID": 3, "context": "[Cassel et al., 1976, Robins and Rotnitzky, 1995], were recently used for off-policy value estimation in the contextual bandits problem [Dud\u00edk et al., 2011, 2014] and reinforcement learning [Jiang and Li, 2016]. These also provide error estimates for the proposed estimators, which we will compare with the lower bounds that we will obtain. The doubly robust techniques also have a flavor of incorporating existing reward models, an idea which we will also leverage in the development of the SWITCH estimator in Section 4. We note that similar ideas were also recently investigated in the context of reinforcement learning by Thomas and Brunskill [2016].", "startOffset": 1, "endOffset": 654}, {"referenceID": 12, "context": "The family of reward distributions D(r | x, a) that we study is a natural generalization of the class studied by Li et al. [2015] for multi-armed bandits.", "startOffset": 113, "endOffset": 130}, {"referenceID": 5, "context": ", the bound on the variance of IPS in Dud\u00edk et al. [2014], which assumes the finiteness of these second moments).", "startOffset": 38, "endOffset": 58}, {"referenceID": 10, "context": "Comparing with the upper bounds on the MSE for existing estimators, we find that this lower bound precisely matches the upper bound for the IPS estimator [Horvitz and Thompson, 1952] (defined in Equation 2).", "startOffset": 154, "endOffset": 182}, {"referenceID": 10, "context": "Lemma 1 (IPS estimator [Horvitz and Thompson, 1952]).", "startOffset": 23, "endOffset": 51}, {"referenceID": 12, "context": "Comparison with the multi-armed bandit setting of Li et al. [2015] The most closely related prior result was due to Li et al.", "startOffset": 50, "endOffset": 67}, {"referenceID": 12, "context": "Comparison with the multi-armed bandit setting of Li et al. [2015] The most closely related prior result was due to Li et al. [2015], who showed matching upper and lower bounds on the minimax risk for multi-armed bandits.", "startOffset": 50, "endOffset": 133}, {"referenceID": 12, "context": "Comparison with the multi-armed bandit setting of Li et al. [2015] The most closely related prior result was due to Li et al. [2015], who showed matching upper and lower bounds on the minimax risk for multi-armed bandits. The somewhat surprising conclusion of their work was the sub-optimality of IPS, which might appear at odds with our conclusion regarding IPS above. However, this difference actually highlights the additional challenges in contextual bandits beyond multi-armed bandits. This is best illustrated in a noiseless setting, where \u03c3 = 0 in the rewards. This makes the multi-armed bandit problem trivial, we can just measure the reward of each arm with one pull and find out the optimal choice. However, there is still a non-trivial lower bound of \u03a9(E\u03bc[\u03c1R max]/n) in the contextual bandit setting, which is exactly the upper bound on the MSE of IPS when the rewards have no noise. This difference crucially relies on \u03bb0 being suitably small relative to the sample size n. When the number of contexts is small, independent estimation for each context can be done in a noiseless setting as observed by Li et al. [2015]. However, once the context distribution is rich enough, then even with noiseless rewards, there is significant variance in the value estimates based on which contexts were observed.", "startOffset": 50, "endOffset": 1131}, {"referenceID": 12, "context": "Comparison with the multi-armed bandit setting of Li et al. [2015] The most closely related prior result was due to Li et al. [2015], who showed matching upper and lower bounds on the minimax risk for multi-armed bandits. The somewhat surprising conclusion of their work was the sub-optimality of IPS, which might appear at odds with our conclusion regarding IPS above. However, this difference actually highlights the additional challenges in contextual bandits beyond multi-armed bandits. This is best illustrated in a noiseless setting, where \u03c3 = 0 in the rewards. This makes the multi-armed bandit problem trivial, we can just measure the reward of each arm with one pull and find out the optimal choice. However, there is still a non-trivial lower bound of \u03a9(E\u03bc[\u03c1R max]/n) in the contextual bandit setting, which is exactly the upper bound on the MSE of IPS when the rewards have no noise. This difference crucially relies on \u03bb0 being suitably small relative to the sample size n. When the number of contexts is small, independent estimation for each context can be done in a noiseless setting as observed by Li et al. [2015]. However, once the context distribution is rich enough, then even with noiseless rewards, there is significant variance in the value estimates based on which contexts were observed. This distinction is further highlighted in the proof of Theorem 1, and is obtained by combining two separate lower bounds. The first lower bound considers the case of noisy rewards, and is a relatively straightforward generalization of the proof of Li et al. [2015]. The second lower bound focuses on noiseless rewards, and shows how the variance in a rich context distribution allows the environment to essentially simulate noisy rewards, even when the reward signal itself is noiseless.", "startOffset": 50, "endOffset": 1579}, {"referenceID": 6, "context": "where the DM stands for direct method, a name for this approach which has been used in prior works [Dud\u00edk et al., 2014].", "startOffset": 99, "endOffset": 119}, {"referenceID": 5, "context": "where the DM stands for direct method, a name for this approach which has been used in prior works [Dud\u00edk et al., 2014]. This approach appears attractive when r\u0302 is a close to E[r | x, a]. Crucially, r\u0302 can be evaluated for any x, a pair, meaning that there is no need to do use importance weights unlike in IPS. This suggests that we might completely eliminate any dependence on \u03c1 using this approach., and it is easily seen that given any estimator r\u0302 such th which takes values in [0, Rmax(x, a)] for any x, a, the variance satisfies: Var(v\u0302DM) \u2264 1 n E\u03c0[R max] = 1 n E\u03bc[\u03c1R max]. (6) That is, there is a linear, rather than quadratic dependence on \u03c1, unlike in the worst case minimax risk of Corollary 1. The catch, however, is that such estimators can have an uncontrolled bias of O(E\u03c0[Rmax]) in the worst case, meaning that even asymptotic consistency is not guaranteed. Prior works have addressed the bias of DM, while trying to reduce the variance of IPS by using doubly robust estimators. A concrete estimator previously used for off-policy evaluation in Dud\u00edk et al. [2014] is defined as", "startOffset": 100, "endOffset": 1082}, {"referenceID": 5, "context": "However, based on the results of Dud\u00edk et al. [2014], the MSE of the DR estimator in this special case is", "startOffset": 33, "endOffset": 53}, {"referenceID": 2, "context": "Bottou et al. [2013] for a detailed discussion), which can often reduce the variance drastically at the cost of a little bias.", "startOffset": 0, "endOffset": 21}, {"referenceID": 2, "context": "This approach, with a specific choice of \u03c4 was described in Bottou et al. [2013] and will be evaluated in the experiments under the name Trimmed IPS.", "startOffset": 60, "endOffset": 81}, {"referenceID": 2, "context": "This approach, with a specific choice of \u03c4 was described in Bottou et al. [2013] and will be evaluated in the experiments under the name Trimmed IPS. \u2022 Thomas and Brunskill [2016] study a similar estimator in the more general context of reinforcement learning.", "startOffset": 60, "endOffset": 180}, {"referenceID": 5, "context": "The analysis in Theorem 2 still applies, replacing the variance of IPS with that of DR from Dud\u00edk et al. [2014]. Since no independence was required in our analysis between the IPS and the DM parts of the estimator, the result is also robust to the use of a common data-dependent estimator r\u0302 = r\u0302\u2032 in SWITCH-DR (10).", "startOffset": 92, "endOffset": 112}, {"referenceID": 19, "context": "Finally, we should mention that this development is quite related to the MAGIC estimator of Thomas and Brunskill [2016], which was discussed following Theorem 2.", "startOffset": 92, "endOffset": 120}, {"referenceID": 5, "context": "We will be using the same 10 UCI data sets that was used previously by Dud\u00edk et al. [2011] and convert the multi-class classification problem to contextual bandits by 1.", "startOffset": 71, "endOffset": 91}, {"referenceID": 5, "context": "We simulate the covariate-shifted data set using the same technique as in Dud\u00edk et al. [2011], which follows standard practice as was described by Gretton et al.", "startOffset": 74, "endOffset": 94}, {"referenceID": 5, "context": "We simulate the covariate-shifted data set using the same technique as in Dud\u00edk et al. [2011], which follows standard practice as was described by Gretton et al. [2009]. In each data set with n rows, we treat the uniform distribution over the data set itself as a surrogate of the population distribution so that we know the ground truth of the rewards.", "startOffset": 74, "endOffset": 169}, {"referenceID": 4, "context": "This approach was suggested by Dud\u00edk et al. [2011], and is standard practice of doubly robust estimators when the direct method (or the oracle estimator) needs to be estimated from the data.", "startOffset": 31, "endOffset": 51}, {"referenceID": 2, "context": ", Bembom and van der Laan, 2008], or the terms with weights larger than \u03c4 are removed altogether as described in Bottou et al. [2013]. Note that the trimmed IPS is a special case of SWITCH when the direct estimator \u2261 0.", "startOffset": 113, "endOffset": 134}, {"referenceID": 18, "context": "The authors would like to thank Lihong Li and John Langford for helpful discussions, Edward Kennedy for bringing our attention to related problems and recent development in causal inference and Wei He for pointing us to [Sun, 2006] for rigorous a measure-theoretic treatment to a continuum of random variables and the corresponding law of large numbers.", "startOffset": 220, "endOffset": 231}], "year": 2016, "abstractText": "We consider the problem of off-policy evaluation\u2014estimating the value of a target policy using data collected by another policy\u2014under the contextual bandit model. We establish a minimax lower bound on the mean squared error (MSE), and show that it is matched up to constant factors by the inverse propensity scoring (IPS) estimator. Since in the multi-armed bandit problem the IPS is suboptimal [Li et al., 2015], our result highlights the difficulty of the contextual setting with non-degenerate context distributions. We further consider improvements on this minimax MSE bound, given access to a reward model. We show that the existing doubly robust approach, which utilizes such a reward model, may continue to suffer from high variance even when the reward model is perfect. We propose a new estimator called SWITCH which more effectively uses the reward model and achieves a superior bias-variance tradeoff compared with prior work. We prove an upper bound on its MSE and demonstrate its benefits empirically on a diverse collection of datasets, often seeing orders of magnitude improvements over a number of baselines.", "creator": "LaTeX with hyperref package"}}}