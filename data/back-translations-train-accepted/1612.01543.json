{"id": "1612.01543", "review": {"conference": "iclr", "VERSION": "v1", "DATE_OF_SUBMISSION": "5-Dec-2016", "title": "Towards the Limit of Network Quantization", "abstract": "Network quantization is one of network compression techniques employed to reduce the redundancy of deep neural networks. It compresses the size of the storage for a large number of network parameters in a neural network by quantizing them and encoding the quantized values into binary codewords of smaller sizes. In this paper, we aim to design network quantization schemes that minimize the expected loss due to quantization while maximizing the compression ratio. To this end, we analyze the quantitative relation of quantization errors to the loss function of a neural network and identify that the Hessian-weighted distortion measure is locally the right objective function that we need to optimize for minimizing the loss due to quantization. As a result, Hessian-weighted k-means clustering is proposed for clustering network parameters to quantize when fixed-length binary encoding follows. When optimal variable-length binary codes, e.g., Huffman codes, are employed for further compression of quantized values after clustering, we derive that the network quantization problem can be related to the entropy-constrained scalar quantization (ECSQ) problem in information theory and consequently propose two solutions of ECSQ for network quantization, i.e., uniform quantization and an iterative algorithm similar to Lloyd's algorithm for k-means clustering. Finally, using the simple uniform quantization followed by Huffman coding, our experiment results show that the compression ratios of 51.25, 22.17 and 40.65 are achievable (i.e., the sizes of the compressed models are 1.95%, 4.51% and 2.46% of the original model sizes) for LeNet, ResNet and AlexNet, respectively, at no or marginal performance loss.", "histories": [["v1", "Mon, 5 Dec 2016 21:04:17 GMT  (400kb)", "http://arxiv.org/abs/1612.01543v1", null]], "reviews": [], "SUBJECTS": "cs.CV cs.LG cs.NE", "authors": ["yoojin choi", "mostafa el-khamy", "jungwon lee"], "accepted": true, "id": "1612.01543"}, "pdf": {"name": "1612.01543.pdf", "metadata": {"source": "CRF", "title": null, "authors": ["Yoojin Choi"], "emails": ["yoojin.c@samsung.com", "mostafa.e@samsung.com", "jungwon2.lee@samsung.com"], "sections": [{"heading": null, "text": "ar Xiv: 161 2.01 543v 1 [cs.C V] 5D ec2 016 Revised as conference contribution at ICLR 2017"}, {"heading": "1 INTRODUCTION", "text": "This year, it will be ready to leave the country in which it is located."}, {"heading": "2 NETWORK MODEL", "text": "We consider a generic nonlinear neural network that returns the output y of input x according to toy = f (x; w), where function f is determined by the structure of the neural network, while w = [w1 \u00b7 \u00b7 \u00b7 wN] T is the vector that consists of all traceable network parameters in the network; N is the total number of traceable parameters in the network.A loss of the loss function (y, y) is defined as the objective function that we want to minimize on average by training: loss (y, y) = loss (f (x; w), y (x)). Note that y = f (x; w) is the predicted output of the network for input x and y = y (x) is the expected (bottom truth) output for input x. Cross entropy or mean square errors are typical examples of a loss function. We define the average loss function for arbitrary input x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, etc."}, {"heading": "3 NETWORK QUANTIZATION", "text": "Figure 2 (a) illustrates an example of network quantization. In the quantization of networks, network parameters are grouped into clusters. Parameters in the same cluster share their quantified value, the representative value (i.e. cluster center) of the cluster to which they belong. After quantization, lossless binary encoding follows to encode quantified parameters into binary code words to store them instead of actual parameter values. To this end, either fixed-length binary encoding or variable-length binary encoding, such as Huffman encoding, can be used. Note that you also have to run a search table to decode quantified values from your binary encoding words, as shown in Figure 2 (b)."}, {"heading": "3.1 COMPRESSION RATIO", "text": "Before quantifying, it is assumed that each network parameter is of b bits, which means that we need Nb bits for all N network parameters. Suppose we partition the network parameters into k clusters. For 1 \u2264 i \u2264 k, let Ci be the set of network parameters in cluster i and let bi be the number of bits of the code word assigned for the network parameters in cluster i. Then we need q i = 1 | Ci | bi bits total for network parameters after quantizing instead of Nb bits. For a lookup table in which k binary code words (bi-bits for 1 \u2264 i \u2264 k) and corresponding quantified values (b-bits for each) are stored, we also need k = 1 bi + kb bits. The compression ratio is then determined by compression ratio = Nb-k i = 1 (| Ci | 1) and corresponding quantified values (b-bits for each), if we are not logically weighted by the number of the cluster, but by the number of the code (if it is only)."}, {"heading": "3.2 K-MEANS CLUSTERING", "text": "In fact, the majority of them are in a position to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to"}, {"heading": "4 NETWORK QUANTIZATION USING HESSIAN-WEIGHT", "text": "In this section, we analyze the effects of quantization errors on the loss function of a neural network and deduce that the Hessian weight can be used to quantify the significance of network parameters: the square matrix, which consists of second-order subderivatives, is called the Hessian matrix or Hessian matrix; in LeCun et al. (1989); Hassibi & Stork (1993), the Hessian is used for circumcision in the selection of network parameters; in this section, we identify that the Hessian, especially the diagonals of the Hessian, can also be used to quantify networks in order to weigh the quantization errors of different network parameters differently."}, {"heading": "4.1 HESSIAN-WEIGHTED QUANTIZATION ERROR", "text": "(W) W (W) W (W) W (W) W (W) W (W) W (W) W (W) W (W) W (W) W (W) W (W) W (W) W (W) W (W) W (W) W (W) W (W) W (W) W (W) W (W) W (W) W (W) W (W) W (W) W (W) W (W) W (W) W (W) W (W) W (W) W (W) W \"W (W) W (W) W (W) W (W) W (W) W (W) W (W) W (W) W (W) W (W) W (W) W) W (W) W (W) W) W (W) W (W) W) W (W) W (W) W) W (W) W (W) W) W (W) W (W) W) W (W) W (W) W) W (W) W (W) W) W (W) W (W) W) W (W) W (W) W (W) W) W (W) W (W) W (W) W) W (W) W (W) W) W (W) W (W) W) W (W) W (W) W) W (W) W (W) W) W (W) W (W) W) W (W) W (W) W) W (W) W (W) W (W) W) W (W) W (W) W) W (W) W (W) W (W) W) W (W) W (W (W) W (W) W (W) W) W (W (W) W (W)\" (W (W) W (W (W) W (W) W) W (W) W (W (W) W (W) W (W) W (W (W) W (W (W (W) \"(W)\" (W) W \"(W\" (W \"(W\" (W) W \"(W\" (W) W \"(W\" (W \"(W\" (W) W \"(W)"}, {"heading": "4.2 HESSIAN-WEIGHTED K-MEANS CLUSTERING", "text": "In the previous subsection, we identify the local relation of the Hessian-weighted quantization errors to the average loss function of a neural network. In this subsection, we use this relation in (7) to design quantization schemes that minimize the quantization loss. To simplify the notation, we now use wi-w-i and hii-hi-hii (w-i). From (7), the optimal cluster that minimizes the Hessian-weighted mean of network parameters in Cj is given by argmin C1, C2,..., Ckk-j-j = 1-wi-cjhii-cj-cj-2, (8), where cj is the Hessian-weighted mean of network parameters in Cj, i.e. cj = wi-Cj hiiwi-wi wi wi wi-wi-wi-Chi.We call this Hessian weighted cluster formation."}, {"heading": "4.3 HESSIAN COMPUTATION", "text": "In order to obtain Hessian weights, it is necessary to evaluate the partial derivative of the second order average loss function with respect to each of the network parameters, i.e., we must evaluate the loss (f (x; w) = 2L (X; w) = 2L (X; w) = 2L (X; w) = 1 | X | 6 x x x (x; w) = Xloss (f (x; w), y (x)), and it is based on the backpropagation method, which is similar to the backpropagation method used to calculate the first order Hessian diagonals. An efficient method for calculating the Hessian diagonals is presented in Le Cun (1987); Becker & Le Cun (1988) and it is based on the backpropagation method, which is similar to the backpropagation algorithm, the partial derivatives used to calculate the first order (gradients). That is, the calculation of the Hessional data from the quantification framework is the same as the one carried out in the order of the Hessional grades."}, {"heading": "4.4 ALTERNATIVE OF HESSIAN", "text": "Although there is an efficient method to obtain the Hessian diagonal, as discussed in the previous section, the calculation of the Hessian weight is not free of charge. In this section, we present an alternative metric that can be used instead of the Hessian weight. In particular, we consider neural networks trained with the Adam SGD Optimizer (Kingma & Ba, 2014) and propose to use a function (e.g. the square root) of the second instant as an alternative to the Hessian. The Adam algorithm calculates individual adaptive learning rates for different network parameters from the estimates of the first and second moments of the gradients. Parameter updating of the Adam algorithm follows swi (t \u2212 1) \u2212 deriwswi (t) \u2212 deriwswi (t).The Adam algorithm calculates individual adaptive learning rates for different network parameters from the estimates of the first and second moments of the gradients."}, {"heading": "4.5 QUANTIZATION OF ALL LAYERS", "text": "We propose to quantify the network parameters of all layers in a neural network at once, taking into account the Hessian weight. Layer by layer has been studied in previous work (Gong et al., 2014; Han et al., 2015a). However, in Han et al. (2015a), for example, a larger number of bits (a larger number of clusters) are allocated to the Convolutionary Layers, meaning that they treat Convolutionary Layers more important. Consequently, the effects of quantification errors on performance vary significantly across layers; some layers, such as Convolutionary Layers, may be more important than the others. This concern is exactly what we want to address by using the Hessian weight, and so we propose to quantify all layers with our quantification schemes."}, {"heading": "5 ENTROPY-CONSTRAINED NETWORK QUANTIZATION", "text": "In this section, we will examine how to solve the problem of network quantization by restricting the compression ratio. In designing network quantization schemes, we will not only minimize the loss of performance, but also maximize the compression ratio. In Section 4, we will examine how to quantify and minimize the loss due to quantification. In this section, we will examine how the compression ratio can be adequately taken into account when optimizing network quantization."}, {"heading": "5.1 ENTROPY CODING", "text": "After clustering network parameters, lossless data compression with variable-length binary code can be followed by assigning short binary code words (i.e. quantified values) to the most common symbols, and inevitably assigning longer binary code words to the rarer symbols. There are a number of optimal codes that achieve the minimum average code word length for a given source. Entropy is the theoretical limit of the average code word length per symbol that we can achieve through lossless data compression, as demonstrated by Shannon (see e.g. Cover & Thomas (2012, Section 5.3)). It is known that optimal codes reach this limit with an overhead of less than 1 bit when only integer code words are allowed. Therefore, optimal encoding is also referred to as entropy encoding. Huffman encoding is one of the optimal coding schemes commonly used when covering 2012 (see section 5.6), for example, when distributing Thomas."}, {"heading": "5.2 ENTROPY-CONSTRAINED SCALAR QUANTIZATION (ECSQ)", "text": "Considering the effects of variable length binary encoding used for lossless data compression, we need to solve the optimization problem in (3) or the problem with the Hessian weight in (8) under a restriction of the compression ratio given by Compression ratio = bb + (\u2211 k = 1 bi + kb) / N > C, (12) which follows from (1) where b-value is the average code word length, i.e. b-value = 1Nk \u2211 i = 1 | Ci | bi. The solution to this optimization with a restriction of the compression ratio for any variable length binary code is generally too complex, as the average code word length may be arbitrary depending on cluster performance. However, we identify that it can be simplified if optimal codes, e.g. Huffman assumes that codes are used by Huffman after cluster encoding."}, {"heading": "5.3 UNIFORM QUANTIZATION", "text": "It is shown in Gish & Pierce (1968) that the unified quantifier is the optimal high-resolution entropy-related scalar quantization factor, independent of the source distribution for the mean square error criterion, which implies that it is asymptotically optimal when it comes to minimizing the average square quantization error for each random source with a reasonably smooth density function, since the resolution becomes infinite, i.e. the number of clusters k. This asymptotic result leads us to a very simple network quantization scheme as follows: 1. We first set uniformly delimited thresholds and divide network parameters into clusters. 2. After determining the clusters, their quantified values (cluster centers) are achieved by taking the mean of network parameters in each cluster. 3. Then follows the optimal binary coding, e.g. Huffman coding, to quantify values in quantifiable lengths."}, {"heading": "5.4 ITERATIVE ALGORITHM TO SOLVE ECSQ", "text": "Another suggestion for solving the ECSQ problem for the quantization of the network is an iterative algorithm similar to the cost of Lloyd's algorithm for clustering. Although this iterative algorithm is more complicated than a uniform quantization in Section 5.3, it finds a local optimum for a specific discrete source. The iterative algorithm for solving a general ECSQ problem is described in Chou et al. (1989) and it follows from the method of Lagrangian multipliers (Boyd & Vandenberghe, 2004, Section 5.1). To apply the same technique to our network quantization problem, we first define a Lagrangian cost function given by J\u03bb. C2, Ck) = D +.H, (15) whereD = 1Nk that we have the costs of J = 1, Cjhii | 2, H = \u2212 k."}, {"heading": "6 EXPERIMENTS", "text": "This section presents our experimental results of the proposed network quantification schemes in three exemplary Convolutionary Neural Networks: (a) LeNet (LeCun et al., 1998) for the MNIST dataset, (b) ResNet (He et al., 2015) for the CIFAR-10 dataset, and (c) AlexNet (Krizhevsky et al., 2012) for the ImageNet dataset. Our experiments can be summarized as follows: \u2022 We use the proposed network quantification methods to quantify all network parameters at once, as discussed in 4.5. Specifically, we refer to 32-layer ResNet et al., 2015) in our experiments the gain of our methods using Hessian weight for very deep convolution of neural networks. \u2022 We evaluate the performance of the proposed network quantification methods in cases with and without network pruning."}, {"heading": "6.1 EXPERIMENT MODELS", "text": "First, we evaluate our network quantization schemes for the MNIST dataset with a simplified version of LeNet5 (LeCun et al., 1998), consisting of two convolutionary layers and two fully connected layers followed by a soft-max layer. It has a total of 431,080 parameters and achieves an accuracy of 99.25%. For a truncated model, we retain only 8.55% (36,860 parameters) of the original network parameters and trim the rest. For the Hessian calculation, 50,000 samples of the training set are reused. We also evaluate the performance when Hessian is calculated using only 1,000 samples. Second, we experiment with our network quantization schemes for the CIFAR-10 dataset (Krizhevsky, 2009) with a pre-programmed 32-layer ResNet et al, 2015. The 32-layer ResNet consists of a total of 464,154 parameters and achieves an accuracy of 2.958%."}, {"heading": "6.2 EXPERIMENT RESULTS", "text": "This year, it has reached the point where it will be able to seek a solution, to find a solution that is capable of finding a solution, to find a solution that is capable, that is capable of finding a solution that is capable, that is capable of finding a solution, that is capable of finding a solution, that is capable of finding a solution, that is capable of finding a solution, that is capable of finding a solution, that is capable of finding a solution that is capable of finding a solution. \""}, {"heading": "7 CONCLUSION", "text": "This paper examines the quantization problem of network parameters in deep neural networks. The quantization of networks compresses the size of memory for a large number of network parameters by quantifying them and encoding the quantified values into binary code words of smaller bit sizes. We identify the sub-optimality of the conventional method through K-medium clustering and redesign quantization schemes so that they can maximize the compression ratio and minimize the loss due to quantization errors. In particular, we show analytically that Hessian weight can be used as a measure of the importance of network parameters and propose to minimize the quantization errors on average to quantify network parameters. Hessian weighting is advantageous in the quantization of all network parameters together as it can handle the different effects of quantization errors correctly, not only within layers, but also across layers."}, {"heading": "A APPENDIX", "text": "In fact, it is the case that the number of people registered in the EU who live in the EU, in the EU Commission and in the EU Commission, who live in the EU, in the EU, in the EU, in the EU, in the EU, in the EU, in the EU, in the EU, in the USA, in the EU, in the USA, in the USA, in the USA, in the EU, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the EU, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA,"}], "references": [{"title": "Fixed point optimization of deep convolutional neural networks for object recognition", "author": ["Sajid Anwar", "Kyuyeon Hwang", "Wonyong Sung"], "venue": "In IEEE International Conference on Acoustics, Speech and Signal Processing,", "citeRegEx": "Anwar et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Anwar et al\\.", "year": 2015}, {"title": "Improving the convergence of back-propagation learning with second order methods", "author": ["Sue Becker", "Yann Le Cun"], "venue": "In Proceedings of the Connectionist Models Summer School,", "citeRegEx": "Becker and Cun.,? \\Q1988\\E", "shortCiteRegEx": "Becker and Cun.", "year": 1988}, {"title": "Entropy-constrained vector quantization", "author": ["Philip A Chou", "Tom Lookabaugh", "Robert M Gray"], "venue": "IEEE Transactions on Acoustics, Speech, and Signal Processing,", "citeRegEx": "Chou et al\\.,? \\Q1989\\E", "shortCiteRegEx": "Chou et al\\.", "year": 1989}, {"title": "Training deep neural networks with low precision multiplications", "author": ["Matthieu Courbariaux", "Jean-Pierre David", "Yoshua Bengio"], "venue": "arXiv preprint arXiv:1412.7024,", "citeRegEx": "Courbariaux et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Courbariaux et al\\.", "year": 2014}, {"title": "Binaryconnect: Training deep neural networks with binary weights during propagations", "author": ["Matthieu Courbariaux", "Yoshua Bengio", "Jean-Pierre David"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Courbariaux et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Courbariaux et al\\.", "year": 2015}, {"title": "Elements of information theory", "author": ["Thomas M Cover", "Joy A Thomas"], "venue": null, "citeRegEx": "Cover and Thomas.,? \\Q2012\\E", "shortCiteRegEx": "Cover and Thomas.", "year": 2012}, {"title": "Predicting parameters in deep learning", "author": ["Misha Denil", "Babak Shakibi", "Laurent Dinh", "Nando de Freitas"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Denil et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Denil et al\\.", "year": 2013}, {"title": "Adaptive subgradient methods for online learning and stochastic optimization", "author": ["John Duchi", "Elad Hazan", "Yoram Singer"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Duchi et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Duchi et al\\.", "year": 2011}, {"title": "Asymptotically efficient quantizing", "author": ["Herbert Gish", "John Pierce"], "venue": "IEEE Transactions on Information Theory,", "citeRegEx": "Gish and Pierce.,? \\Q1968\\E", "shortCiteRegEx": "Gish and Pierce.", "year": 1968}, {"title": "Compressing deep convolutional networks using vector quantization", "author": ["Yunchao Gong", "Liu Liu", "Ming Yang", "Lubomir Bourdev"], "venue": "arXiv preprint arXiv:1412.6115,", "citeRegEx": "Gong et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Gong et al\\.", "year": 2014}, {"title": "Deep learning with limited numerical precision", "author": ["Suyog Gupta", "Ankur Agrawal", "Kailash Gopalakrishnan", "Pritish Narayanan"], "venue": "In Proceedings of the 32nd International Conference on Machine Learning,", "citeRegEx": "Gupta et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Gupta et al\\.", "year": 2015}, {"title": "Deep compression: Compressing deep neural networks with pruning, trained quantization and huffman coding", "author": ["Song Han", "Huizi Mao", "William J Dally"], "venue": "arXiv preprint arXiv:1510.00149,", "citeRegEx": "Han et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Han et al\\.", "year": 2015}, {"title": "Learning both weights and connections for efficient neural network", "author": ["Song Han", "Jeff Pool", "John Tran", "William Dally"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Han et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Han et al\\.", "year": 2015}, {"title": "Second order derivatives for network pruning: Optimal brain surgeon", "author": ["Babak Hassibi", "David G Stork"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Hassibi and Stork.,? \\Q1993\\E", "shortCiteRegEx": "Hassibi and Stork.", "year": 1993}, {"title": "Deep residual learning for image recognition", "author": ["Kaiming He", "Xiangyu Zhang", "Shaoqing Ren", "Jian Sun"], "venue": "arXiv preprint arXiv:1512.03385,", "citeRegEx": "He et al\\.,? \\Q2015\\E", "shortCiteRegEx": "He et al\\.", "year": 2015}, {"title": "Fixed-point feedforward deep neural network design using weights +1, 0, and -1", "author": ["Kyuyeon Hwang", "Wonyong Sung"], "venue": "In IEEE Workshop on Signal Processing Systems,", "citeRegEx": "Hwang and Sung.,? \\Q2014\\E", "shortCiteRegEx": "Hwang and Sung.", "year": 2014}, {"title": "Speeding up convolutional neural networks with low rank expansions", "author": ["Max Jaderberg", "Andrea Vedaldi", "Andrew Zisserman"], "venue": "In Proceedings of the British Machine Vision Conference,", "citeRegEx": "Jaderberg et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Jaderberg et al\\.", "year": 2014}, {"title": "Compression of deep convolutional neural networks for fast and low power mobile applications", "author": ["Yong-Deok Kim", "Eunhyeok Park", "Sungjoo Yoo", "Taelim Choi", "Lu Yang", "Dongjun Shin"], "venue": "arXiv preprint arXiv:1511.06530,", "citeRegEx": "Kim et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Kim et al\\.", "year": 2015}, {"title": "Adam: A method for stochastic optimization", "author": ["Diederik Kingma", "Jimmy Ba"], "venue": "arXiv preprint arXiv:1412.6980,", "citeRegEx": "Kingma and Ba.,? \\Q2014\\E", "shortCiteRegEx": "Kingma and Ba.", "year": 2014}, {"title": "Learning multiple layers of features from tiny images", "author": ["Alex Krizhevsky"], "venue": null, "citeRegEx": "Krizhevsky.,? \\Q2009\\E", "shortCiteRegEx": "Krizhevsky.", "year": 2009}, {"title": "Imagenet classification with deep convolutional neural networks", "author": ["Alex Krizhevsky", "Ilya Sutskever", "Geoffrey E Hinton"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Krizhevsky et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Krizhevsky et al\\.", "year": 2012}, {"title": "Mod\u00e8les connexionnistes de l\u2019apprentissage", "author": ["Yann Le Cun"], "venue": "PhD thesis, Paris", "citeRegEx": "Cun.,? \\Q1987\\E", "shortCiteRegEx": "Cun.", "year": 1987}, {"title": "Speeding-up convolutional neural networks using fine-tuned CP-decomposition", "author": ["Vadim Lebedev", "Yaroslav Ganin", "Maksim Rakhuba", "Ivan Oseledets", "Victor Lempitsky"], "venue": "arXiv preprint arXiv:1412.6553,", "citeRegEx": "Lebedev et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Lebedev et al\\.", "year": 2014}, {"title": "Optimal brain damage", "author": ["Yann LeCun", "John S Denker", "Sara A Solla", "Richard E Howard", "Lawrence D Jackel"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "LeCun et al\\.,? \\Q1989\\E", "shortCiteRegEx": "LeCun et al\\.", "year": 1989}, {"title": "Gradient-based learning applied to document recognition", "author": ["Yann LeCun", "L\u00e9on Bottou", "Yoshua Bengio", "Patrick Haffner"], "venue": "Proceedings of the IEEE,", "citeRegEx": "LeCun et al\\.,? \\Q1998\\E", "shortCiteRegEx": "LeCun et al\\.", "year": 1998}, {"title": "Fixed point quantization of deep convolutional networks", "author": ["Darryl D Lin", "Sachin S Talathi", "V Sreekanth Annapureddy"], "venue": "arXiv preprint arXiv:1511.06393,", "citeRegEx": "Lin et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Lin et al\\.", "year": 2015}, {"title": "Neural networks with few multiplications", "author": ["Zhouhan Lin", "Matthieu Courbariaux", "Roland Memisevic", "Yoshua Bengio"], "venue": "arXiv preprint arXiv:1510.03009,", "citeRegEx": "Lin et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Lin et al\\.", "year": 2015}, {"title": "Least squares quantization in pcm", "author": ["Stuart Lloyd"], "venue": "IEEE transactions on information theory,", "citeRegEx": "Lloyd.,? \\Q1982\\E", "shortCiteRegEx": "Lloyd.", "year": 1982}, {"title": "Skeletonization: A technique for trimming the fat from a network via relevance assessment", "author": ["Michael C Mozer", "Paul Smolensky"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Mozer and Smolensky.,? \\Q1989\\E", "shortCiteRegEx": "Mozer and Smolensky.", "year": 1989}, {"title": "XNOR-Net: Imagenet classification using binary convolutional neural networks", "author": ["Mohammad Rastegari", "Vicente Ordonez", "Joseph Redmon", "Ali Farhadi"], "venue": "arXiv preprint arXiv:1603.05279,", "citeRegEx": "Rastegari et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Rastegari et al\\.", "year": 2016}, {"title": "Imagenet large scale visual recognition challenge", "author": ["Olga Russakovsky", "Jia Deng", "Hao Su", "Jonathan Krause", "Sanjeev Satheesh", "Sean Ma", "Zhiheng Huang", "Andrej Karpathy", "Aditya Khosla", "Michael Bernstein"], "venue": "International Journal of Computer Vision,", "citeRegEx": "Russakovsky et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Russakovsky et al\\.", "year": 2015}, {"title": "Lowrank matrix factorization for deep neural network training with high-dimensional output targets", "author": ["Tara N Sainath", "Brian Kingsbury", "Vikas Sindhwani", "Ebru Arisoy", "Bhuvana Ramabhadran"], "venue": "In IEEE International Conference on Acoustics, Speech and Signal Processing,", "citeRegEx": "Sainath et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Sainath et al\\.", "year": 2013}, {"title": "Very deep convolutional networks for large-scale image recognition", "author": ["Karen Simonyan", "Andrew Zisserman"], "venue": "arXiv preprint arXiv:1409.1556,", "citeRegEx": "Simonyan and Zisserman.,? \\Q2014\\E", "shortCiteRegEx": "Simonyan and Zisserman.", "year": 2014}, {"title": "Going deeper with convolutions", "author": ["Christian Szegedy", "Wei Liu", "Yangqing Jia", "Pierre Sermanet", "Scott Reed", "Dragomir Anguelov", "Dumitru Erhan", "Vincent Vanhoucke", "Andrew Rabinovich"], "venue": "In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition,", "citeRegEx": "Szegedy et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Szegedy et al\\.", "year": 2015}, {"title": "Rethinking the inception architecture for computer vision", "author": ["Christian Szegedy", "Vincent Vanhoucke", "Sergey Ioffe", "Jonathon Shlens", "Zbigniew Wojna"], "venue": "arXiv preprint arXiv:1512.00567,", "citeRegEx": "Szegedy et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Szegedy et al\\.", "year": 2015}, {"title": "Convolutional neural networks with low-rank regularization", "author": ["Cheng Tai", "Tong Xiao", "Xiaogang Wang"], "venue": "arXiv preprint arXiv:1511.06067,", "citeRegEx": "Tai et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Tai et al\\.", "year": 2015}, {"title": "Lecture 6.5-rmsprop: Divide the gradient by a running average of its recent magnitude", "author": ["Tijmen Tieleman", "Geoffrey Hinton"], "venue": "COURSERA: Neural Networks for Machine Learning,", "citeRegEx": "Tieleman and Hinton.,? \\Q2012\\E", "shortCiteRegEx": "Tieleman and Hinton.", "year": 2012}, {"title": "Improving the speed of neural networks on CPUs", "author": ["Vincent Vanhoucke", "Andrew Senior", "Mark Z Mao"], "venue": "In Deep Learning and Unsupervised Feature Learning Workshop,", "citeRegEx": "Vanhoucke et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Vanhoucke et al\\.", "year": 2011}, {"title": "Restructuring of deep neural network acoustic models with singular value decomposition", "author": ["Jian Xue", "Jinyu Li", "Yifan Gong"], "venue": "In INTERSPEECH,", "citeRegEx": "Xue et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Xue et al\\.", "year": 2013}, {"title": "Deep fried convnets", "author": ["Zichao Yang", "Marcin Moczulski", "Misha Denil", "Nando de Freitas", "Alex Smola", "Le Song", "Ziyu Wang"], "venue": "In Proceedings of the IEEE International Conference on Computer Vision, pp", "citeRegEx": "Yang et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Yang et al\\.", "year": 2015}, {"title": "Adadelta: an adaptive learning rate method", "author": ["Matthew D Zeiler"], "venue": "arXiv preprint arXiv:1212.5701,", "citeRegEx": "Zeiler.,? \\Q2012\\E", "shortCiteRegEx": "Zeiler.", "year": 2012}], "referenceMentions": [{"referenceID": 6, "context": "However, it has been shown that deep neural networks typically include many redundant parameters (Denil et al., 2013) and it is of great interest in recent years to reduce the redundancy of deep neural networks.", "startOffset": 97, "endOffset": 117}, {"referenceID": 15, "context": "For example, Krizhevsky et al. (2012) came up with a deep convolutional neural network consisting of 61 million parameters and won the ImageNet competition in 2012.", "startOffset": 13, "endOffset": 38}, {"referenceID": 15, "context": "For example, Krizhevsky et al. (2012) came up with a deep convolutional neural network consisting of 61 million parameters and won the ImageNet competition in 2012. It is followed by deeper neural networks with even larger numbers of parameters, e.g., Simonyan & Zisserman (2014). The large sizes of deep neural networks make it difficult to deploy them on resource-limited devices, e.", "startOffset": 13, "endOffset": 280}, {"referenceID": 6, "context": "However, it has been shown that deep neural networks typically include many redundant parameters (Denil et al., 2013) and it is of great interest in recent years to reduce the redundancy of deep neural networks. The whole procedure to reduce the redundancy of a neural network is called as network compression. The benefit of compression is twofold: computational cost reduction and hardware resource saving. In this paper, our interest is mainly on the size reduction of the storage (memory) for a large number of network parameters (weights and biases). In particular, we focus on the network size compression by quantizing network parameters. The most related work to our investigation can be found in Gong et al. (2014); Han et al.", "startOffset": 98, "endOffset": 724}, {"referenceID": 6, "context": "However, it has been shown that deep neural networks typically include many redundant parameters (Denil et al., 2013) and it is of great interest in recent years to reduce the redundancy of deep neural networks. The whole procedure to reduce the redundancy of a neural network is called as network compression. The benefit of compression is twofold: computational cost reduction and hardware resource saving. In this paper, our interest is mainly on the size reduction of the storage (memory) for a large number of network parameters (weights and biases). In particular, we focus on the network size compression by quantizing network parameters. The most related work to our investigation can be found in Gong et al. (2014); Han et al. (2015a), where a conventional quantization method", "startOffset": 98, "endOffset": 744}, {"referenceID": 23, "context": "Network pruning (Mozer & Smolensky, 1989; LeCun et al., 1989; Hassibi & Stork, 1993; Han et al., 2015b) and low-precision fixed- or floating-point implementation of neural networks (Vanhoucke et al.", "startOffset": 16, "endOffset": 103}, {"referenceID": 37, "context": ", 2015b) and low-precision fixed- or floating-point implementation of neural networks (Vanhoucke et al., 2011; Hwang & Sung, 2014; Courbariaux et al., 2014; Anwar et al., 2015; Gupta et al., 2015; Lin et al., 2015a) have been also investigated for network compression.", "startOffset": 86, "endOffset": 215}, {"referenceID": 3, "context": ", 2015b) and low-precision fixed- or floating-point implementation of neural networks (Vanhoucke et al., 2011; Hwang & Sung, 2014; Courbariaux et al., 2014; Anwar et al., 2015; Gupta et al., 2015; Lin et al., 2015a) have been also investigated for network compression.", "startOffset": 86, "endOffset": 215}, {"referenceID": 0, "context": ", 2015b) and low-precision fixed- or floating-point implementation of neural networks (Vanhoucke et al., 2011; Hwang & Sung, 2014; Courbariaux et al., 2014; Anwar et al., 2015; Gupta et al., 2015; Lin et al., 2015a) have been also investigated for network compression.", "startOffset": 86, "endOffset": 215}, {"referenceID": 10, "context": ", 2015b) and low-precision fixed- or floating-point implementation of neural networks (Vanhoucke et al., 2011; Hwang & Sung, 2014; Courbariaux et al., 2014; Anwar et al., 2015; Gupta et al., 2015; Lin et al., 2015a) have been also investigated for network compression.", "startOffset": 86, "endOffset": 215}, {"referenceID": 31, "context": "Low-rank approximation was also heavily studied mainly for speeding up computation (Sainath et al., 2013; Xue et al., 2013; Jaderberg et al., 2014; Lebedev et al., 2014; Yang et al., 2015; Kim et al., 2015; Tai et al., 2015).", "startOffset": 83, "endOffset": 224}, {"referenceID": 38, "context": "Low-rank approximation was also heavily studied mainly for speeding up computation (Sainath et al., 2013; Xue et al., 2013; Jaderberg et al., 2014; Lebedev et al., 2014; Yang et al., 2015; Kim et al., 2015; Tai et al., 2015).", "startOffset": 83, "endOffset": 224}, {"referenceID": 16, "context": "Low-rank approximation was also heavily studied mainly for speeding up computation (Sainath et al., 2013; Xue et al., 2013; Jaderberg et al., 2014; Lebedev et al., 2014; Yang et al., 2015; Kim et al., 2015; Tai et al., 2015).", "startOffset": 83, "endOffset": 224}, {"referenceID": 22, "context": "Low-rank approximation was also heavily studied mainly for speeding up computation (Sainath et al., 2013; Xue et al., 2013; Jaderberg et al., 2014; Lebedev et al., 2014; Yang et al., 2015; Kim et al., 2015; Tai et al., 2015).", "startOffset": 83, "endOffset": 224}, {"referenceID": 39, "context": "Low-rank approximation was also heavily studied mainly for speeding up computation (Sainath et al., 2013; Xue et al., 2013; Jaderberg et al., 2014; Lebedev et al., 2014; Yang et al., 2015; Kim et al., 2015; Tai et al., 2015).", "startOffset": 83, "endOffset": 224}, {"referenceID": 17, "context": "Low-rank approximation was also heavily studied mainly for speeding up computation (Sainath et al., 2013; Xue et al., 2013; Jaderberg et al., 2014; Lebedev et al., 2014; Yang et al., 2015; Kim et al., 2015; Tai et al., 2015).", "startOffset": 83, "endOffset": 224}, {"referenceID": 35, "context": "Low-rank approximation was also heavily studied mainly for speeding up computation (Sainath et al., 2013; Xue et al., 2013; Jaderberg et al., 2014; Lebedev et al., 2014; Yang et al., 2015; Kim et al., 2015; Tai et al., 2015).", "startOffset": 83, "endOffset": 224}, {"referenceID": 0, "context": ", 2014; Anwar et al., 2015; Gupta et al., 2015; Lin et al., 2015a) have been also investigated for network compression. Some extremes of low-precision neural networks consisting of binary or ternary parameters were presented in Courbariaux et al. (2015); Lin et al.", "startOffset": 8, "endOffset": 254}, {"referenceID": 0, "context": ", 2014; Anwar et al., 2015; Gupta et al., 2015; Lin et al., 2015a) have been also investigated for network compression. Some extremes of low-precision neural networks consisting of binary or ternary parameters were presented in Courbariaux et al. (2015); Lin et al. (2015b); Rastegari et al.", "startOffset": 8, "endOffset": 274}, {"referenceID": 0, "context": ", 2014; Anwar et al., 2015; Gupta et al., 2015; Lin et al., 2015a) have been also investigated for network compression. Some extremes of low-precision neural networks consisting of binary or ternary parameters were presented in Courbariaux et al. (2015); Lin et al. (2015b); Rastegari et al. (2016). Low-rank approximation was also heavily studied mainly for speeding up computation (Sainath et al.", "startOffset": 8, "endOffset": 299}, {"referenceID": 0, "context": ", 2014; Anwar et al., 2015; Gupta et al., 2015; Lin et al., 2015a) have been also investigated for network compression. Some extremes of low-precision neural networks consisting of binary or ternary parameters were presented in Courbariaux et al. (2015); Lin et al. (2015b); Rastegari et al. (2016). Low-rank approximation was also heavily studied mainly for speeding up computation (Sainath et al., 2013; Xue et al., 2013; Jaderberg et al., 2014; Lebedev et al., 2014; Yang et al., 2015; Kim et al., 2015; Tai et al., 2015). We note that these are different types of network compression techniques, which can be employed on top of each other. A network compression framework consisting of network pruning, network quantization and finetuning is presented in Han et al. (2015a). Network pruning is employed to remove some of network parameters completely from a neural network by setting their values to be always zero.", "startOffset": 8, "endOffset": 778}, {"referenceID": 0, "context": ", 2014; Anwar et al., 2015; Gupta et al., 2015; Lin et al., 2015a) have been also investigated for network compression. Some extremes of low-precision neural networks consisting of binary or ternary parameters were presented in Courbariaux et al. (2015); Lin et al. (2015b); Rastegari et al. (2016). Low-rank approximation was also heavily studied mainly for speeding up computation (Sainath et al., 2013; Xue et al., 2013; Jaderberg et al., 2014; Lebedev et al., 2014; Yang et al., 2015; Kim et al., 2015; Tai et al., 2015). We note that these are different types of network compression techniques, which can be employed on top of each other. A network compression framework consisting of network pruning, network quantization and finetuning is presented in Han et al. (2015a). Network pruning is employed to remove some of network parameters completely from a neural network by setting their values to be always zero. For a pruned network, one only needs to keep unpruned network parameters and their respective locations (indexes) in the original model. After network pruning, unpruned network parameters are fined-tuned to recover the loss due to pruning. The values for pruned parameters remain to be zero during this fine-tuning stage. Then, network quantization follows. Network quantization reduces the number of bits needed for representing (unpruned) network parameters by quantizing them and encoding their quantized values into binary codewords with smaller bit sizes. The quantized values can be retrieved from the binary codewords stored instead of actual values by using a lookup table of a small size. Finally, the quantized values in the lookup table can be further fined-tuned to reduce the loss due to quantization. We note that the fine-tuning stages are optional but it is preferred to have them in order to recover the loss due to compression after aggressive pruning and/or quantization. For network quantization. we consider a neural network that is already trained, pruned if employed and fine-tuned before quantization. If no network pruning is employed, all parameters in a network are subject to quantization. For pruned networks, our focus is on quantization of unpruned parameters. We note that pruning and quantization methods can be designed jointly but it makes the problem more complicated. In particular, if fine-tuning of unpruned network parameters is assumed to be employed after aggressive pruning, it is difficult to predict and to consider the impact of fine-tuning in network quantization since unpruned parameters can change substantially from their original values after fine-tuning due to the fact that a large number of pruned parameters are fixed to be zero. Hence, in this paper, we treat them as two separable procedures for network compression and focus on network quantization. In Gong et al. (2014); Han et al.", "startOffset": 8, "endOffset": 2851}, {"referenceID": 0, "context": ", 2014; Anwar et al., 2015; Gupta et al., 2015; Lin et al., 2015a) have been also investigated for network compression. Some extremes of low-precision neural networks consisting of binary or ternary parameters were presented in Courbariaux et al. (2015); Lin et al. (2015b); Rastegari et al. (2016). Low-rank approximation was also heavily studied mainly for speeding up computation (Sainath et al., 2013; Xue et al., 2013; Jaderberg et al., 2014; Lebedev et al., 2014; Yang et al., 2015; Kim et al., 2015; Tai et al., 2015). We note that these are different types of network compression techniques, which can be employed on top of each other. A network compression framework consisting of network pruning, network quantization and finetuning is presented in Han et al. (2015a). Network pruning is employed to remove some of network parameters completely from a neural network by setting their values to be always zero. For a pruned network, one only needs to keep unpruned network parameters and their respective locations (indexes) in the original model. After network pruning, unpruned network parameters are fined-tuned to recover the loss due to pruning. The values for pruned parameters remain to be zero during this fine-tuning stage. Then, network quantization follows. Network quantization reduces the number of bits needed for representing (unpruned) network parameters by quantizing them and encoding their quantized values into binary codewords with smaller bit sizes. The quantized values can be retrieved from the binary codewords stored instead of actual values by using a lookup table of a small size. Finally, the quantized values in the lookup table can be further fined-tuned to reduce the loss due to quantization. We note that the fine-tuning stages are optional but it is preferred to have them in order to recover the loss due to compression after aggressive pruning and/or quantization. For network quantization. we consider a neural network that is already trained, pruned if employed and fine-tuned before quantization. If no network pruning is employed, all parameters in a network are subject to quantization. For pruned networks, our focus is on quantization of unpruned parameters. We note that pruning and quantization methods can be designed jointly but it makes the problem more complicated. In particular, if fine-tuning of unpruned network parameters is assumed to be employed after aggressive pruning, it is difficult to predict and to consider the impact of fine-tuning in network quantization since unpruned parameters can change substantially from their original values after fine-tuning due to the fact that a large number of pruned parameters are fixed to be zero. Hence, in this paper, we treat them as two separable procedures for network compression and focus on network quantization. In Gong et al. (2014); Han et al. (2015a), it is proposed to utilize the conventional k-means clustering method for quantizing network parameters.", "startOffset": 8, "endOffset": 2871}, {"referenceID": 7, "context": ", AdaGrad (Duchi et al., 2011), Adadelta (Zeiler, 2012), and RMSProp (Tieleman & Hinton, 2012).", "startOffset": 10, "endOffset": 30}, {"referenceID": 40, "context": ", 2011), Adadelta (Zeiler, 2012), and RMSProp (Tieleman & Hinton, 2012).", "startOffset": 18, "endOffset": 32}, {"referenceID": 7, "context": ", AdaGrad (Duchi et al., 2011), Adadelta (Zeiler, 2012), and RMSProp (Tieleman & Hinton, 2012). The advantage of using these metrics instead of Hessian is that they are computed while training and can be obtained at the end of training at no additional cost. \u2022 It is shown how the proposed network quantization schemes can be applied for quantizing network parameters of all layers together at once, rather than layer-by-layer network quantization in Gong et al. (2014); Han et al.", "startOffset": 11, "endOffset": 470}, {"referenceID": 7, "context": ", AdaGrad (Duchi et al., 2011), Adadelta (Zeiler, 2012), and RMSProp (Tieleman & Hinton, 2012). The advantage of using these metrics instead of Hessian is that they are computed while training and can be obtained at the end of training at no additional cost. \u2022 It is shown how the proposed network quantization schemes can be applied for quantizing network parameters of all layers together at once, rather than layer-by-layer network quantization in Gong et al. (2014); Han et al. (2015a). This follows from our investigation that Hessian-weighting can handle the different impact of quantization errors properly not only within layers but also across layers.", "startOffset": 11, "endOffset": 490}, {"referenceID": 27, "context": "The most well-known heuristic algorithm is Lloyd\u2019s algorithm (Lloyd, 1982).", "startOffset": 61, "endOffset": 74}, {"referenceID": 21, "context": "In LeCun et al. (1989); Hassibi & Stork (1993), Hessian is utilized in selecting network parameters to prune.", "startOffset": 5, "endOffset": 23}, {"referenceID": 21, "context": "In LeCun et al. (1989); Hassibi & Stork (1993), Hessian is utilized in selecting network parameters to prune.", "startOffset": 5, "endOffset": 47}, {"referenceID": 21, "context": "An efficient way of computing the diagonal of Hessian is presented in Le Cun (1987); Becker & Le Cun (1988) and it is based on the back propagation method that is similar to the back propagation algorithm used for computing first-order partial derivatives (gradients).", "startOffset": 73, "endOffset": 84}, {"referenceID": 21, "context": "An efficient way of computing the diagonal of Hessian is presented in Le Cun (1987); Becker & Le Cun (1988) and it is based on the back propagation method that is similar to the back propagation algorithm used for computing first-order partial derivatives (gradients).", "startOffset": 73, "endOffset": 108}, {"referenceID": 7, "context": ", AdaGrad (Duchi et al., 2011), Adadelta (Zeiler, 2012) and RMSProp (Tieleman & Hinton, 2012).", "startOffset": 10, "endOffset": 30}, {"referenceID": 40, "context": ", 2011), Adadelta (Zeiler, 2012) and RMSProp (Tieleman & Hinton, 2012).", "startOffset": 18, "endOffset": 32}, {"referenceID": 9, "context": "Layer-by-layer quantization was examined in the previous work (Gong et al., 2014; Han et al., 2015a).", "startOffset": 62, "endOffset": 100}, {"referenceID": 9, "context": "Layer-by-layer quantization was examined in the previous work (Gong et al., 2014; Han et al., 2015a). However, e.g., in Han et al. (2015a), a larger number of bits (a larger number of clusters) are assigned for convolutional layers than fully-connected layers, which implies that they heuristically treat convolutional layers more importantly.", "startOffset": 63, "endOffset": 139}, {"referenceID": 9, "context": "Layer-by-layer quantization was examined in the previous work (Gong et al., 2014; Han et al., 2015a). However, e.g., in Han et al. (2015a), a larger number of bits (a larger number of clusters) are assigned for convolutional layers than fully-connected layers, which implies that they heuristically treat convolutional layers more importantly. It follows from the fact that the impact of quantization errors on the performance varies significantly across layers; some layers, e.g., convolutional layers, may be more important than the others. This concern is exactly what we want to address by using Hessian-weight, and so we propose performing quantization all layers together with our quantization schemes using Hessian-weight. We note that quantization of all layers is proposed under the assumption that all of binary encoded quantized parameters in a network are simply stored in one single array. Under this assumption, if layer-by-layer quantization is employed, then we need to assign some (additional) bits to each binary codeword for layer information (layer index), and it hurts the compression ratio. If we quantize all parameters of a network together, then we can avoid such additional overhead for layer indication when storing binary encoded quantized parameters. Thus, in this case, quantizing all layers together is beneficial and Hessian-weighting can be used to address the different impact of the quantization errors across layers. For layer-by-layer quantization, it is advantageous to use separate arrays and separate lookup tables for different layers since layer information can be excluded in each of binary codewords for network parameters. Hessian-weighting can still provide gain even in this case for layer-by-layer quantization since it can address the different impact of the quantization errors of network parameters within each layer as well. Finally, note that recent neural networks are getting deeper, e.g., see Szegedy et al. (2015a;b); He et al. (2015). In such deep neural networks, quantizing network parameters of all layers together is more efficient since optimizing layer-by-layer clustering jointly across all layers requires exponential time complexity with respect to the number of layers.", "startOffset": 63, "endOffset": 1992}, {"referenceID": 2, "context": "The iterative algorithm for solving a general ECSQ problem is provided in Chou et al. (1989) and it follows from the method of Lagrangian multipliers (Boyd & Vandenberghe, 2004, Section 5.", "startOffset": 74, "endOffset": 93}, {"referenceID": 24, "context": "This section presents our experiment results of the proposed network quantization schemes in three exemplary convolutional neural networks: (a) LeNet (LeCun et al., 1998) for the MNIST data set, (b) ResNet (He et al.", "startOffset": 150, "endOffset": 170}, {"referenceID": 14, "context": ", 1998) for the MNIST data set, (b) ResNet (He et al., 2015) for the CIFAR-10 data set, and (c) AlexNet (Krizhevsky et al.", "startOffset": 43, "endOffset": 60}, {"referenceID": 20, "context": ", 2015) for the CIFAR-10 data set, and (c) AlexNet (Krizhevsky et al., 2012) for the ImageNet data set.", "startOffset": 51, "endOffset": 76}, {"referenceID": 14, "context": "In particular, we include 32-layer ResNet (He et al., 2015) in our experiments in order to see the gain of our methods using Hessian-weight for very deep convolution neural networks.", "startOffset": 42, "endOffset": 59}, {"referenceID": 11, "context": "For the index information, we compute index differences between unpruned network parameters in the original model and further compress them by Huffman coding as in Han et al. (2015a). \u2022 We experiment our network quantization methods with fixed-length coding as well as with Huffman coding.", "startOffset": 164, "endOffset": 183}, {"referenceID": 24, "context": "First, we evaluate our network quantization schemes for the MNIST data set with a simplified version of LeNet5 (LeCun et al., 1998), consisting of two convolutional layers and two fully-connected layers followed by a soft-max layer.", "startOffset": 111, "endOffset": 131}, {"referenceID": 19, "context": "Second, we experiment our network quantization schemes for the CIFAR-10 data set (Krizhevsky, 2009) with a pre-trained 32-layer ResNet (He et al.", "startOffset": 81, "endOffset": 99}, {"referenceID": 14, "context": "Second, we experiment our network quantization schemes for the CIFAR-10 data set (Krizhevsky, 2009) with a pre-trained 32-layer ResNet (He et al., 2015).", "startOffset": 135, "endOffset": 152}, {"referenceID": 30, "context": "Third, we evaluate our network quantization schemes for the ImageNet ILSVRC-2012 data set (Russakovsky et al., 2015) with AlexNet (Krizhevsky et al.", "startOffset": 90, "endOffset": 116}, {"referenceID": 20, "context": ", 2015) with AlexNet (Krizhevsky et al., 2012).", "startOffset": 21, "endOffset": 46}, {"referenceID": 11, "context": "Finally, we note that layer-by-layer quantization was evaluated in Han et al. (2015a) under the assumption that the layer information (layer index) is not needed to be encoded as additional bits for each network parameter (e.", "startOffset": 67, "endOffset": 86}], "year": 2016, "abstractText": "Network quantization is one of network compression techniques employed to reduce the redundancy of deep neural networks. It compresses the size of the storage for a large number of network parameters in a neural network by quantizing them and encoding the quantized values into binary codewords of smaller sizes. In this paper, we aim to design network quantization schemes that minimize the expected loss due to quantization while maximizing the compression ratio. To this end, we analyze the quantitative relation of quantization errors to the loss function of a neural network and identify that the Hessian-weighted distortion measure is locally the right objective function that we need to optimize for minimizing the loss due to quantization. As a result, Hessian-weighted k-means clustering is proposed for clustering network parameters to quantize when fixed-length binary encoding follows. Hessian-weighting properly handles the different impact of quantization errors not only within layers but also across layers and thus it can be employed for quantizing all layers of a network together at once; it is beneficial since one can avoid layer-by-layer compression rate optimization. When optimal variablelength binary codes, e.g., Huffman codes, are employed for further compression of quantized values after clustering, we derive that the network quantization problem can be related to the entropy-constrained scalar quantization (ECSQ) problem in information theory and consequently propose two solutions of ECSQ for network quantization, i.e., uniform quantization and an iterative algorithm similar to Lloyd\u2019s algorithm for k-means clustering. Finally, using the simple uniform quantization followed by Huffman coding, our experiment results show that the compression ratios of 51.25, 22.17 and 40.65 are achievable (i.e., the sizes of the compressed models are 1.95%, 4.51% and 2.46% of the original model sizes) for LeNet, ResNet and AlexNet, respectively, at no or marginal performance loss.", "creator": "LaTeX with hyperref package"}}}