{"id": "1612.01627", "review": {"conference": "ACL", "VERSION": "v1", "DATE_OF_SUBMISSION": "6-Dec-2016", "title": "Sequential Matching Network: A New Architecture for Multi-turn Response Selection in Retrieval-Based Chatbots", "abstract": "We study response selection for multi-turn conversation in retrieval based chatbots. Existing works either ignores relationships among utterances, or misses important information in context when matching a response with a highly abstract context vector finally. We propose a new session based matching model to address both problems. The model first matches a response with each utterance on multiple granularities, and distills important matching information from each pair as a vector with convolution and pooling operations. The vectors are then accumulated in a chronological order through a recurrent neural network (RNN) which models the relationships among the utterances. The final matching score is calculated with the hidden states of the RNN. Empirical study on two public data sets shows that our model can significantly outperform the state-of-the-art methods for response selection in multi-turn conversation.", "histories": [["v1", "Tue, 6 Dec 2016 01:57:39 GMT  (608kb,D)", "http://arxiv.org/abs/1612.01627v1", null], ["v2", "Mon, 15 May 2017 01:50:55 GMT  (600kb,D)", "http://arxiv.org/abs/1612.01627v2", "ACL 2017"]], "reviews": [], "SUBJECTS": "cs.CL", "authors": ["yu wu", "wei wu", "chen xing", "ming zhou", "zhoujun li"], "accepted": true, "id": "1612.01627"}, "pdf": {"name": "1612.01627.pdf", "metadata": {"source": "CRF", "title": "Sequential Match Network: A New Architecture for Multi-turn Response Selection in Retrieval-based Chatbots", "authors": ["Yu Wu", "Wei Wu", "Zhoujun Li", "Ming Zhou"], "emails": ["wuyu@buaa.edu.cn", "lizj@buaa.edu.cn", "wuwei@microsoft.com", "mingzhou@microsoft.com"], "sections": [{"heading": "1 Introduction", "text": "In fact, most of them are able to survive on their own, and they are able to survive on their own, \"he said.\" But it's not that they are able to survive on their own, \"he said.\" But it's not that they are able to survive on their own. \"He added,\" It's not that they are able to survive on their own, but that they are able to survive on their own. \""}, {"heading": "2 Related Work", "text": "Recently, data-driven approaches (Ritter et al., 2011; Higashinaka et al., 2014) have attracted a lot of attention. Existing work along this line includes retrieval methods and generation-based methods. The former selects an appropriate response from an index based on the agreement between the answer and an input message with or without context (Hu et al., 2014; Ji et al., 2015; Wang et al., 2015; Yan et al., 2016; Wu et al., 2016; Zhou et al.), while the latter uses statistical machine translation techniques (Ritter et al., 2011) or the sequence of a sequence framework (Shang et al., 2015; Serban et al., 2015; Vinyals and Le., 2015; Li et al., 2016; Xing et al., 2016; Zell."}, {"heading": "3 Matching Approach", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "3.1 Problem Formalization", "text": "Suppose we have a record D = {(yi, si, ri)} Ni = 1, where si = {ui, 1,.., ui, ni} is a conversation session with {ui1,.., ui, ni \u2212 1} statements in context and ui, ni is an input messag. ri is a response candidate and yi, 0, 1} stands for a label. yi = 1 means ri is an appropriate response for si, otherwise yi = 0. Our goal is to learn a matching model g (\u00b7) with D. For each session response pair (s, r), g (s, r) measures the appropriate degree between s and r."}, {"heading": "3.2 Model Overview", "text": "Figure 1 shows the architecture of our model. First, the model decomposes the session-response matching into multiple session-response pairs, and then all session matches are accumulated as session matching by a recursive neural network. Specifically, the model consists of two layers. The first layer balances each response candidate with each utterance (context and message) in the session at word and segment level. A session-response pair is then transformed into a word-word similarity matrix and a sequence-sequence-similarity matrix, and important matrix information in the pair is isolated from the two matrices and encoded in a matrix vector. The matching vectors are then fed into the second layer, where they are transformed into the hidden states of a recursive neural network with gated unites (GRU), following the chronological sequence of the matrix expressions in the session matrix, matching the sequence of the sequence of the matrices and the sequence of the response-matrices sequences, and the sequence of the sequence of the matrices are then consolidated into the sequences-sequences-sequenced-sequences-sequenced-sequences-sequences-sequenced-sequences-sequenced-sequences-sequences-sequenced-sequences-sequenced-sequences-sequenced-sequences-sequence-sequences-sequence-sequence-sequence-sequence-sequence-sequence-sequence-sequence-sequence-sequence-sequence-sequence-sequence-sequence-sequence-sequence-sequence-sequence-sequence-sequence-sequence-sequence-sequence-sequence-sequence-sequence-sequence-sequence-sequence-sequence-sequence-sequence-sequence-sequence-sequence-sequence-sequence-sequence-sequence-sequence-sequence-sequence-sequence-sequence-sequence-sequence-sequence-sequence-sequence-sequence-sequence-sequence-sequence-sequence-sequence-sequence-sequence-sequence-sequence-sequence-sequence-sequence-sequence-sequence-sequence"}, {"heading": "3.3 Utterance-Response Matching", "text": "In the first layer in which there is the M1, the M1 is defined and an answer candidate r, the M1, the M1, the M1, the M1, the M2, the M2, the M2, the M2, the M2, the M2, the M2, the M1, the M2, the M2, the M2, the M2, the M2, the M2, the, the M2, the, the M2, the M2, the M2, the M2, the M2, the, the M2, the M2, the, the M2, the M2, the M2, the M2, the M2, the M2, the M2, the M2, the M2, the M2, the M2, the M2, the M2, the M2, the M2, the M2, the M2, the M2, the M2, the M2, the M2, the M2, the M2, the M2, the M2, the M2, the M2, the M2, the M2, the M2, the M2, the M2, the M2, the M2, the M2, the M2, the M2, the M2, the M2, the M2, the M2, the M2, the M2, the M2, the M2, the M2, the M2, the M2, the M2, the M2, the M2, the M2, M2, the M2, the M2, the M2, the M2, the M2, M2, the M2, M2, the M2, the M2, the M2, the M2, the M2, M2, the M2, the M2, M2, the M2, M2, M2, the M2, M2, the M2, the M2, M2, M2, the M2, M2, the M2, the M2, M2, the M2, M2, the M2, M2, the M2, the M2, M2, the M2, M2, M2, M2, the M2, M2, M2, the M2"}, {"heading": "3.4 Matching Accumulation", "text": "Suppose [v1,.., vn] is the output of the first layer (corresponding to n pairs), on the second layer a GRU [v1,.., vn] takes as input and encodes the matching sequence into its hidden states Hm = [h1,.., hn] with a detailed parameterization similar to Equation (2). This layer has two functions: (1) it models the dependence and the temporal relationship of utterances in the session; (2) it uses the temporal relationship to monitor the accumulation of the pair matching as a session-based matching. Furthermore, from Equation (2) we can see that the reset gate (i.e. ri) and the update gate (i.e. zi) control how much information is from the previous hidden state and the current input into the current hidden state."}, {"heading": "4 Response Candidate Retrieval", "text": "In the practice of a on-demand chatbot, in order to apply the appropriate approach to the selection of answers, one must first retrieve a number of candidates from an index. While retrieving candidates is not the focus of the paper, it is an important step in a real system. In this work, we use a heuristic method to extract candidates from the index. Faced with a message triggered by {u1,..., un \u2212 1} utterances in their previous rounds, we extract the five most important keywords based on their tf-idf values1 and expand un by the keywords. Then we send the extended message to the index and retrieve candidates using the index's inline retrieval aluation algorithm. Finally, we use g (s, r) to reclassify the candidates and return the first one as a response to the session."}, {"heading": "5 Experiment", "text": "We tested our model using a public English dataset and a Chinese dataset, which we publish with this paper."}, {"heading": "5.1 Experiment setup", "text": "This year it has come to the point that it has never come as far as this year."}, {"heading": "5.2 Baseline", "text": "We considered the following basics: Basic models: Models in (Lowe et al., 2015) and (Kadlec et al., 2015), including TF-IDF, RNN, CNN, LSTM, and BiLSTM.Multi-view: the model proposed by Zhou et al. (2016), which uses a hierarchically recursive neural network to model speech relationships. Deep Reaction Learning (DL2R): the model proposed by Yan et al. (2016). Advanced single-turn matching models: Since LSTM and BiLSTM do not represent the state-of-the-art matching models, we linked the statements in a session and likened the long text to an answer candidate using more powerful models such as MV-LSTM (Wan et al., 2016), Match-LSTM (Wang and Jiang, 2015), and Multi-Channel, described in Section 3.3. Multi-Channel is a simple version of our relationship model without consideration."}, {"heading": "5.3 Parameter Tuning", "text": "All models were implemented with Theano (Theano Development Team, 2016). Word embedding was initialized by the results of word2vec 6 on the training data, and the dimension of the word vectors is 200. For MultiChannel and layer one of our model, we set the dimension of the hidden states of GRU to 200. We optimized the window size in folding and merging in {(2, 2), (3, 3) (4, 4)} and finally chose (3, 3). The number of function boards is 8. In layer two, 6https: / / code.google.com / archive / p / word2vec / we specified the dimensions of the matching vectors and the hidden states of GRU. In layer two, 6https: / code.google.com / archive / word2vec / we surpass the initial sizes of King\u03b280 and the hidden states of GRU as 50th."}, {"heading": "5.4 Evaluation Results", "text": "The results show that one cannot neglect external relations and can simply perform a multi-turn response selection by turning them into a single-turn problem. Our models achieve significant improvements over MultiView, which justifies our Matching First strategy. DL2R is also worse than our models, suggesting that rephrasing the statement with heuristic rules is not a good way to use context information. Figures on the Ubuntu data are much higher than those on the Chinese data (R10 @ 1 and P @ 1 are equivalent), and the results showed the value of our new data and supported our assertion that the Ubuntu data excessively simplifies the problem of multi-turn selection. There is no significant difference between our three models, which could be because Gate was already selected when it is no longer useful when it coincides with the last mechanism."}, {"heading": "5.5 Further Analysis", "text": "Visualization: We visualize the similarity matrices and gates of GRU in layer two using an example from the Ubuntu Corpus to further clarify how our model identifies important information in context and how it selects important matching vectors using the gate mechanism of GRU, as described in Section 3.3 and Section 3.4. Example is {u1: How to extract many RAR (number, for example) files at once; u2: Surely you can do this in bash; u3: Okay how? u4: Are the files all in the same directory? u5: Yes, they are all; r: then the command glebihan should extract them all from / into that directory. It comes from the test set and our model successfully ranked the correct response to the top position. Due to the space limitation, we visualized only M1 of u1 and r in Figure 2 (a), M1 of u3 and r in Figure 2 (b), and the update ranges are already larger (i.e.) in Figure 2."}, {"heading": "6 Conclusion", "text": "We present a new model for selecting multi-turn responses in on-demand chatbots. Experimental results from public datasets show that the model can significantly exceed current methods."}], "references": [], "referenceMentions": [], "year": 2016, "abstractText": "We study response selection for multi-turn conversation in retrieval based chatbots. Existing works either ignores relationships among utterances, or misses important information in context when matching a response with a highly abstract context vector finally. We propose a new session based matching model to address both problems. The model first matches a response with each utterance on multiple granularities, and distills important matching information from each pair as a vector with convolution and pooling operations. The vectors are then accumulated in a chronological order through a recurrent neural network (RNN) which models the relationships among the utterances. The final matching score is calculated with the hidden states of the RNN. Empirical study on two public data sets shows that our model can significantly outperform the state-of-the-art methods for response selection in multi-turn conversation.", "creator": "LaTeX with hyperref package"}}}