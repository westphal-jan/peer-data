{"id": "1612.01663", "review": {"conference": "AAAI", "VERSION": "v1", "DATE_OF_SUBMISSION": "6-Dec-2016", "title": "Efficient Non-Oblivious Randomized Reduction for Risk Minimization with Improved Excess Risk Guarantee", "abstract": "In this paper, we address learning problems for high dimensional data. Previously, oblivious random projection based approaches that project high dimensional features onto a random subspace have been used in practice for tackling high-dimensionality challenge in machine learning. Recently, various non-oblivious randomized reduction methods have been developed and deployed for solving many numerical problems such as matrix product approximation, low-rank matrix approximation, etc. However, they are less explored for the machine learning tasks, e.g., classification. More seriously, the theoretical analysis of excess risk bounds for risk minimization, an important measure of generalization performance, has not been established for non-oblivious randomized reduction methods. It therefore remains an open problem what is the benefit of using them over previous oblivious random projection based approaches. To tackle these challenges, we propose an algorithmic framework for employing non-oblivious randomized reduction method for general empirical risk minimizing in machine learning tasks, where the original high-dimensional features are projected onto a random subspace that is derived from the data with a small matrix approximation error. We then derive the first excess risk bound for the proposed non-oblivious randomized reduction approach without requiring strong assumptions on the training data. The established excess risk bound exhibits that the proposed approach provides much better generalization performance and it also sheds more insights about different randomized reduction approaches. Finally, we conduct extensive experiments on both synthetic and real-world benchmark datasets, whose dimension scales to $O(10^7)$, to demonstrate the efficacy of our proposed approach.", "histories": [["v1", "Tue, 6 Dec 2016 04:58:45 GMT  (44kb)", "http://arxiv.org/abs/1612.01663v1", null]], "reviews": [], "SUBJECTS": "cs.LG", "authors": ["yi xu", "haiqin yang", "lijun zhang 0005", "tianbao yang"], "accepted": true, "id": "1612.01663"}, "pdf": {"name": "1612.01663.pdf", "metadata": {"source": "CRF", "title": "Efficient Non-oblivious Randomized Reduction for Risk Minimization with Improved Excess Risk Guarantee", "authors": ["Yi Xu", "Haiqin Yang", "Lijun Zhang", "Tianbao Yang"], "emails": ["tianbao-yang}@uiowa.edu,", "hqyang@ieee.org,", "zhanglj@lamda.nju.edu.cn"], "sections": [{"heading": null, "text": "ar Xiv: 161 2.01 663v 1 [cs.L G] 6D ec2 01"}, {"heading": "Introduction", "text": "This year, it has reached the point where it will be able to retaliate until it is able to retaliate."}, {"heading": "Related Work", "text": "Large studies in the literature are devoted to the non-oblivious randomized reduction of matrix applications, focusing on the detection of matrix approximation errors or the restoration error of the solution (e.g. in the leastsquares regression), and few studies have examined its properties for risk mitigation in machine learning. However, for oblivious randomized reduction methods there are some theoretical papers that attempt to understand its effects on predictive performance (Blum 2005; Shi et al. 2012; Paul et al. 2013), which differs from these studies in that we rely on the statistical property (generalization property) of non-oblivious randomized reduction of non-oblivious randomized capabilities for expected risk mitigation (Blum 2005; Shi et al. 2013)."}, {"heading": "Preliminaries", "text": "Subsequently, we will focus on Y = {+ 1, \u2212 1} and Y = R. However, we emphasize that the results are applicable to other problems (e.g. multi-class and multi-class classification).We designate the risk minimization problem in machine learning by (z, \u2212 y) a non-negative loss function that measures the inconsistency between a prediction z and the designation y. If we allow w Rd, then by assuming a linear model z = w x for the prediction, the risk minimization problem in machine learning should solve the following problem: w \u00b2 = arg min w Rd EP [x, y] (1), where EP [\u00b7] is the expectation above (x, y) \u00b2 P."}, {"heading": "Oblivious Randomized Reduction", "text": "The idea of oblivious randomized reduction is to reduce a high-dimensional feature vector x-Rd to a low-dimensional vector by x-Rm, in which a random matrix exists that is independent of the data. A traditional approach is to use a Gaussian matrix with each entry independent of a normal distribution with mean null and variance values (Dasgupta and Gupta 2003). Many other types of random matrix A are proposed that lead to a much more efficient calculation of reduction, including subsampled Hadamard Transform (SRHT) (Boutsidis and Gittens 2013a) and random hashing (RH)."}, {"heading": "Then", "text": "ER (w-n, w-n), EP (w-nx, y) \u2212 EP (w-x, y) \u2264 O (\u221a r / m-wn, 2 + 1 / \u221a n) Note: In the above limit, we omit dependence on the upper limit of the data norm. Although synthesis of the set and prior analysis of recovery errors can give us a guarantee of the excess risk limit, it is based on certain assumptions of the data that may not apply in practice."}, {"heading": "Non-Oblivious Randomized Reduction", "text": "The key idea of non-oblivious randomized reduction is the calculation of a subspace U-Rd \u00b7 m from the data matrix X-Rd \u00b7 n such that the projection of the data matrix into the subspace is close to the data matrix. To calculate the subspace U-Rd \u00b7 m, we first have a random matrix Rn \u00b7 m and compute Y-Rp-Rd \u00b7 m. Then we let U-Rp be the left singular vector matrix of Y. This technique was used in the low-level matrix approximation, matrix-product approximation and approximate singular value decomposition (SVD) of a large matrix (Halko, Martinsson, and Drop 2011). Various random matrices can be used as long as the matrix approximation errors are defined below the matrix-Rp-Rp-X-Rp-Rp-R."}, {"heading": "Excess Risk Bound", "text": "The logic of the analysis is to first derive the optimization error of the approximate model w \u00b2 n = U \u00b2 v \u00b2 n and then to examine the statistical learning theory in order to determine the risk of excess. In particular, we will show that the optimization error, and thus the risk of excess, is limited by the matrix approximation error in (8). \u2212 To simplify the presentation, we present some notations: F (w) = 1nn (w \u00b2 xi, yi) + 2 (w \u00b2, yi) + 2 (w \u00b2) F \u00b2 (w \u00b2 x) = EP (w \u00b2 x, y) + 2 (11) Next, we derive the optimization error of w \u00b2 n = U \u00b2 v \u00b2 n."}, {"heading": "Matrix Approximation Error", "text": "In this subsection we will present some recent results on matrix approximation of four commonly used reduction operators. < < / p > p > p > p > p > p > p > p > p (SRHT), and random reduction (RH), and discuss their impact on excess risk. Further details of these four randomized reduction operators can be found in (Yang et al. 2015). We will first introduce some notations used in matrix approximation analyses. Let r \u2264 min (n, d) denote the order of X and k."}, {"heading": "Experiments", "text": "In this section, we provide empirical evaluations to support the proposed algorithms and theoretical analysis. We implement and compare the following algorithms: (i) NOR: ERM with non-oblivious, randomized data, (ii) the real approaches with unclear, randomized approaches, (ii) the real approaches and the real results, (ii) the real approaches, (ii) the real approaches, (ii) the real approaches and the pure random approaches (RP), (RP) and the pure random approaches (RP). We also implement and compare three randomized reduction operators for these different approaches, i.e. RH, RG and RS 1. We use only one block of the random hashing matrix (i.e., s = 1) A similar result to Thermal 5 can be produced for a constant probability of success (Nelson, but with a constant Nyen)."}, {"heading": "Conclusions", "text": "In this paper, we have established the excess risk limit for non-oblivious randomised reduction methods for risk minimisation problems. More importantly, the new excess risk limit does not require strict assumptions of data and loss functions, which are not trivial and significant theoretical results. Empirical studies on synthetic data sets and real data sets confirm our theoretical analysis and also show the effectiveness of the proposed non-oblivious randomised reduction approach."}, {"heading": "Acknowlegements", "text": "Y. Xu and T. Yang are partially supported by the National Science Foundation (IIS-1463988, IIS-1545995). L. Zhang is partially supported by NSFC (61603177) and JiangsuSF (BK20160658)."}, {"heading": "Appendix", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "Proof of Lemma 2", "text": "Since the loss function G-Lipschitz is continuous, i.e. we have F (wn) = max."}, {"heading": "Proof of Theorem 1", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "Proof.", "text": "ER (w n, w n) = EP [(w n x, y)] \u2212 EP [(w n, y) = [F (w n) \u2212 \u03bb2 w n 22] \u2212 [F (w n) \u2212 \u03bb2 w 22] = F (w n) \u2212 F (w n) \u2212 F (w n) + \u03bb2 (w n) 22 \u2212 \u03bb 2 F (w n) \u2212 F (w n) \u2212 F (1 + a) 22 \u2264 (1 + a) (F (F (w n) \u2212 F (wn)) + 8 (1 + 1 / a) G2R2 (32 + log (1 / g) n (v w n) 22 (from Lemma 3) \u2264 G 2 (1 + a) X \u2212 PY X \u2212 22 + 8 (1 + 1 / a) G2R2 (32 + g (1 / g) 2 P2 (n) (22 + log (1 / g) n) n (b p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p y) 1 (n m 2) (n m 2) (n m n m) (n m m m m m m p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p"}, {"heading": "Proof of Theorem 2", "text": "Remember that X = U1\u04451V 1 + U2\u04452V 2, and we have Y = U1\u04451V 1 + U2\u04452V 2, we denote 1 = V 1 and 2 = V 2 2, then Y = XB = U1\u04411\u04411\u04411V 1 + U2\u04412\u04412\u04412\u04412\u04412\u04412\u04412\u04412\u04411\u04411\u04411\u04411 + U1\u04451V = U1\u04451V 1 + U1\u04451V = U1\u04452V 2, then Y = U1\u04451V = U1\u04451V 1 + U1\u04451V = U1\u04452V = U1\u04452V = U1\u04451V = U1\u04451V = U1\u04451V 1 = U1\u04451V = U1\u04451V = U1V = U1\u04451V = U1V = U1\u04451V = U1\u04451V = U1V = U1\u04451V = U1\u04451V = U1x2V = U1x2V = U1x2V = U1x2x2V = U1x2x2V = U1x2x2V = U1x2V = U1x2x2x2V = U1x2x2x2V = U1x2x2V = U1x2x2x2V = U1x2x2V = U1x2x2x2x2V = U1x2x2V = U1x2x2V = U1x2x2x2x2V = U1V = U1x2x2x2x2V = U1x2x2x2V = U1x2x2x2x2V = U1x2x2x2x2V = U1x2x2x2x2V = U1x2x2x2x2V = U1x2x2x2x2x2x2x2x2V = U1V = U1x2x2x2x2x2x2x2x2x2V = U1V = U1x2x2x2x2x2x2x2x2x2x2x2"}, {"heading": "Proof of Theorem 3", "text": "In section 10 of (Halko, Martinsson and Tropp 2011) it is shown that if m = k + p with p > 4 and u, t + 1, and \"2\" is a diagonal matrix, then we have a probability of at least 1 \u2212 2t \u2212 p \u2212 e \u2212 u2 / s. Since m = 2k log k and m = k + p, we have this p \u2212 2k log k. Then we have the following inequalities: 3kp + 1 t (13) with a probability of at least 1 \u2212 2t \u2212 p \u2212 e \u2212 u2 / s. Since m = 2k log k and m = k + p, we have this p \u2212 2k log k \u2212 k. Then the following inequalities apply: 3kp + 1 \u2264 3kp \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 e \u2212 u2 / s."}, {"heading": "Proof of Theorem 4", "text": "The proof: In (Boutsidis and Gittens 2013b), Lemma 4.1 has shown that we are equally likely to \"X\" (Boutsidis and Gittens 2013b) \"X\" (PY X) \"22\" (1) (1) (Lemma 5.4 \"of (Boutsidis and Gittens 2013b) (Boutsidis and Gittens 2013b) (1) (1) (PY X) 22\" 22 \"(1) (2) (Lemma 4.8\" of (Boutsidis and Gittens 2013b) (1) (PY) 2V (2) 22 \"51\" (2) (2) (2) (2) (2) (2) (2) (V)."}, {"heading": "Proof of Theorem 5", "text": "Remember that X = U1H 1 + U2H 2V 2 and Y = XH = U-K-K-K-K-K on Theorem 4 by (Cohen, Nelson and Waldmeister 2015) Application of SVD of PY X = U-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K"}, {"heading": "An Efficient Implementation of Computing U\u0302 in NOR for Sparse Data", "text": "In this section we present an efficient implementation of NOR for sparse data. We note that if data is sparse, the time complexity of the calculation of U is sparse O (mN), where N is sparse the number of non-zero elements in X. Consequently, the calculation of the left singular vectors of Y could become a significant component of the total calculation in algorithm 2. To use the data economy, we next present a quick implementation of U sparse calculation. If Y = U sparse, V sparse is the SVD of Y, then V sparse is the single value decomposition of Km = Y sparse, then we can calculate the projection matrix of U sparse."}], "references": [{"title": "Fast randomized kernel ridge regression with", "author": ["W. M"], "venue": null, "citeRegEx": "M.,? \\Q2015\\E", "shortCiteRegEx": "M.", "year": 2015}, {"title": "Introduction to statistical learning theory", "author": ["Boucheron Bousquet", "O. Lugosi 2003] Bousquet", "S. Boucheron", "G. Lugosi"], "venue": "In Advanced Lectures on Machine Learning,", "citeRegEx": "Bousquet et al\\.,? \\Q2003\\E", "shortCiteRegEx": "Bousquet et al\\.", "year": 2003}, {"title": "Improved matrix algorithms via the subsampled randomized hadamard transform", "author": ["Boutsidis", "C. Gittens 2013a] Boutsidis", "A. Gittens"], "venue": "SIAM J. Matrix Analysis Applications 34(3):1301\u20131340", "citeRegEx": "Boutsidis et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Boutsidis et al\\.", "year": 2013}, {"title": "Improved matrix algorithms via the subsampled randomized hadamard transform", "author": ["Boutsidis", "C. Gittens 2013b] Boutsidis", "A. Gittens"], "venue": "SIAM Journal on Matrix Analysis and Applications 34(3):1301\u20131340", "citeRegEx": "Boutsidis et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Boutsidis et al\\.", "year": 2013}, {"title": "D", "author": ["M.B. Cohen", "J. Nelson", "Woodruff"], "venue": "P.", "citeRegEx": "Cohen. Nelson. and Woodruff 2015", "shortCiteRegEx": null, "year": 2015}, {"title": "and Gupta", "author": ["S. Dasgupta"], "venue": "A.", "citeRegEx": "Dasgupta and Gupta 2003", "shortCiteRegEx": null, "year": 2003}, {"title": "and Kaban", "author": ["R.J. Durrant"], "venue": "A.", "citeRegEx": "Durrant and Kaban 2013", "shortCiteRegEx": null, "year": 2013}, {"title": "and Mahoney", "author": ["A. Gittens"], "venue": "M.", "citeRegEx": "Gittens and Mahoney 2013", "shortCiteRegEx": null, "year": 2013}, {"title": "J", "author": ["N. Halko", "P.-G. Martinsson", "Tropp"], "venue": "A.", "citeRegEx": "Halko. Martinsson. and Tropp 2011", "shortCiteRegEx": null, "year": 2011}, {"title": "Improved bounds for the nystr\u00f6m method with application to kernel classification", "author": ["Jin"], "venue": "IEEE Transactions on Information Theory 59(10):6939\u20136949", "citeRegEx": "Jin,? \\Q2013\\E", "shortCiteRegEx": "Jin", "year": 2013}, {"title": "and Nelson", "author": ["D.M. Kane"], "venue": "J.", "citeRegEx": "Kane and Nelson 2014", "shortCiteRegEx": null, "year": 2014}, {"title": "and Shamir", "author": ["D. Kukliansky"], "venue": "O.", "citeRegEx": "Kukliansky and Shamir 2015", "shortCiteRegEx": null, "year": 2015}, {"title": "M", "author": ["Mahoney"], "venue": "W.", "citeRegEx": "Mahoney 2011", "shortCiteRegEx": null, "year": 2011}, {"title": "H", "author": ["J. Nelson", "Nguyen"], "venue": "L.", "citeRegEx": "Nelson and Nguyen 2012", "shortCiteRegEx": null, "year": 2012}, {"title": "Random projections for support vector machines", "author": ["Paul"], "venue": null, "citeRegEx": "Paul,? \\Q2013\\E", "shortCiteRegEx": "Paul", "year": 2013}, {"title": "M", "author": ["M. Pilanci", "Wainwright"], "venue": "J.", "citeRegEx": "Pilanci and Wainwright 2015", "shortCiteRegEx": null, "year": 2015}, {"title": "Is margin preserved after random projection", "author": ["Shi"], "venue": null, "citeRegEx": "Shi,? \\Q2012\\E", "shortCiteRegEx": "Shi", "year": 2012}, {"title": "and Franc", "author": ["S. Sonnenburg"], "venue": "V.", "citeRegEx": "Sonnenburg and Franc 2010", "shortCiteRegEx": null, "year": 2010}, {"title": "Fast rates for regularized objectives", "author": ["Sridharan"], "venue": null, "citeRegEx": "Sridharan,? \\Q2008\\E", "shortCiteRegEx": "Sridharan", "year": 2008}, {"title": "V", "author": ["Vapnik"], "venue": "N.", "citeRegEx": "Vapnik 1998", "shortCiteRegEx": null, "year": 1998}, {"title": "Nystr\u00f6m method vs random fourier features: A theoretical and empirical comparison", "author": ["Yang"], "venue": null, "citeRegEx": "Yang,? \\Q2012\\E", "shortCiteRegEx": "Yang", "year": 2012}, {"title": "Theory of dual-sparse regularized randomized reduction", "author": ["Yang"], "venue": null, "citeRegEx": "Yang,? \\Q2015\\E", "shortCiteRegEx": "Yang", "year": 2015}, {"title": "Random projections for classification: A recovery approach", "author": ["Zhang"], "venue": "IEEE Transactions on Information Theory 60(11):7300\u20137316", "citeRegEx": "Zhang,? \\Q2014\\E", "shortCiteRegEx": "Zhang", "year": 2014}], "referenceMentions": [], "year": 2016, "abstractText": "In this paper, we address learning problems for high dimensional data. Previously, oblivious random projection based approaches that project high dimensional features onto a random subspace have been used in practice for tackling highdimensionality challenge in machine learning. Recently, various non-oblivious randomized reduction methods have been developed and deployed for solving many numerical problems such as matrix product approximation, low-rank matrix approximation, etc. However, they are less explored for the machine learning tasks, e.g., classification. More seriously, the theoretical analysis of excess risk bounds for risk minimization, an important measure of generalization performance, has not been established for non-oblivious randomized reduction methods. It therefore remains an open problem what is the benefit of using them over previous oblivious random projection based approaches. To tackle these challenges, we propose an algorithmic framework for employing non-oblivious randomized reduction method for general empirical risk minimizing in machine learning tasks, where the original high-dimensional features are projected onto a random subspace that is derived from the data with a small matrix approximation error. We then derive the first excess risk bound for the proposed non-oblivious randomized reduction approach without requiring strong assumptions on the training data. The established excess risk bound exhibits that the proposed approach provides much better generalization performance and it also sheds more insights about different randomized reduction approaches. Finally, we conduct extensive experiments on both synthetic and real-world benchmark datasets, whose dimension scales to O(10), to demonstrate the efficacy of our proposed approach. Introduction Recently, the scale and dimensionality of data associated with machine learning and data mining applications have seen unprecedented growth, spurring the BIG DATA research and development. Learning from largescale ultrahigh-dimensional data remains a computationally challenging problem. The big size of data not only increases the memory footprint but also increases the computational costs pertaining to optimization. A popular approach for addressing the high-dimensionality challenge is Copyright c \u00a9 2017, Association for the Advancement of Artificial Intelligence (www.aaai.org). All rights reserved. to perform dimensionality reduction. Nowadays, randomized reduction methods are emerging to be attractive for dimensionality reduction. Compared with traditional dimensionality reduction methods (e.g., PCA and LDA), randomized reduction methods (i) can lead to simpler algorithms that are easier to analyze (Mahoney 2011); (ii) can often be organized to exploit modern computational architectures better than classical dimensional reduction methods (Halko, Martinsson, and Tropp 2011); (iii) can be more efficient without loss in efficacy (Paul et al. 2013). Generally, randomized reduction methods can be cast into two types: the first type of methods reduces a set of high-dimensional vectors into a low dimensional space independent of each other. These methods usually sample a random matrix independent of the data and then use it to reduce the dimensionality of the data. The second type of methods projects a set of vectors (in the form of a matrix) onto a subspace such that the original matrix can be well reconstructed from the projected matrix and the subspace. Therefore, the subspace to which the data is projected depends on the original data. These methods have been deployed for solving many numerical problems related to matrices, e.g., matrix product approximation, low-rank matrix approximation, approximate singular value decomposition (Boutsidis and Gittens 2013a; Halko, Martinsson, and Tropp 2011). To differentiate these two types of randomized reduction methods, we refer to the first type as oblivious randomized reduction, and refer to the second type as non-oblivious randomized reduction. We note that in literature oblivious and non-oblivious are used interchangeably with data-independent and data-dependent. Here, we use the terminology commonly appearing in matrix analysis and numerical linear algebra due to that the general excess risk bound depends on the matrix approximation error. However, we have not seen any comprehensive study on the statistical property (in particular the excess risk bound) of these randomized reduction methods applied to risk minimization in machine learning. The excess risk bound measures the generalization performance of a learned model compared to the optimal model from a class that has the best generalization performance. The excess risk bounds facilitate a better understanding of different learning algorithms and have the potential to guide us to design better algorithms (Kukliansky and Shamir 2015). It is worth noting that several studies have been devoted to understanding the theoretical properties of oblivious randomized reduction methods applied to classification and regression problems. For example, (Blum 2005; Shi et al. 2012; Paul et al. 2013) analyzed the preservation of the margin of SVM based classification methods with randomized dimension reduction. (Zhang et al. 2014; Yang et al. 2015; Pilanci and Wainwright 2015) studied the problem from the perspective of optimization. Nonetheless, these results are limited in the sense that (i) they focus on only oblivious randomized reduction where the data is projected onto a random subspace independent of the data; (ii) they depend heavily on strong assumptions of the training data or the problem, e.g., low-rank of the data matrix, linear separability of training examples, or the sparsity of optimal solution, and (iii) some of these results do not directly carry over to the excess risk bounds. To tackle the above challenges, we propose an algorithmic framework for employing non-oblivious randomized reduction (NOR) method to project the original high-dimensional features onto a random subspace that is derived from the original data. We study and establish the excess risk bound of the presented randomized algorithms for risk minimization. Different from previous results for oblivious randomized reduction methods, our theoretical analysis does not require assumptions of the training data or the problem, such as low-rank of the data matrix, linear separability of training examples, and the sparsity of optimal solution. When the data matrix is of low-rank or has a fast spectral decay, the excess risk bound of NOR is much better than that of oblivious randomized reduction based methods. Empirical studies on synthetic and real data sets corroborate the theoretical results and demonstrate the effectiveness of the proposed methods.", "creator": "LaTeX with hyperref package"}}}