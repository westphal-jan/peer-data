{"id": "1612.01848", "review": {"conference": "AAAI", "VERSION": "v1", "DATE_OF_SUBMISSION": "6-Dec-2016", "title": "Condensed Memory Networks for Clinical Diagnostic Inferencing", "abstract": "Diagnosis of a clinical condition is a challenging task, which often requires significant medical investigation. Previous work related to diagnostic inferencing problems mostly consider multivariate observational data (e.g. physiological signals, lab tests etc.). In contrast, we explore the problem using free-text medical notes recorded in an electronic health record (EHR). Complex tasks like these can benefit from structured knowledge bases, but those are not scalable. We instead exploit raw text from Wikipedia as a knowledge source. Memory networks have been demonstrated to be effective in tasks which require comprehension of free-form text. They use the final iteration of the learned representation to predict probable classes. We introduce condensed memory neural networks (C-MemNNs), a novel model with iterative condensation of memory representations that preserves the hierarchy of features in the memory. Experiments on the MIMIC-III dataset show that the proposed model outperforms other variants of memory networks to predict the most probable diagnoses given a complex clinical scenario.", "histories": [["v1", "Tue, 6 Dec 2016 15:15:27 GMT  (106kb,D)", "http://arxiv.org/abs/1612.01848v1", "Accepted to AAAI 2017"], ["v2", "Tue, 3 Jan 2017 20:41:20 GMT  (106kb,D)", "http://arxiv.org/abs/1612.01848v2", "Accepted to AAAI 2017"]], "COMMENTS": "Accepted to AAAI 2017", "reviews": [], "SUBJECTS": "cs.CL", "authors": ["aaditya prakash", "siyuan zhao", "sadid a hasan", "vivek v datla", "kathy lee", "ashequl qadir", "joey liu", "oladimeji farri"], "accepted": true, "id": "1612.01848"}, "pdf": {"name": "1612.01848.pdf", "metadata": {"source": "CRF", "title": "Condensed Memory Networks for Clinical Diagnostic Inferencing", "authors": ["Aaditya Prakash", "Siyuan Zhao", "Sadid A. Hasan", "Vivek Datla", "Kathy Lee", "Ashequl Qadir", "Joey Liu", "Oladimeji Farri"], "emails": ["aprakash@brandeis.edu", "szhao@wpi.edu", "1@philips.com", "dimeji.farri@philips.com"], "sections": [{"heading": "Introduction", "text": "This year it is more than ever before."}, {"heading": "Related Work", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "Memory Networks", "text": "In fact, it is as if most people who are able to survive themselves are able to survive themselves by going in search of themselves. (...) It is not as if they are able to survive themselves. (...) It is as if they were able to survive themselves. (...) It is as if they were able to survive themselves. (...) It is as if they were able to survive themselves. (...) It is as if they were able to survive themselves. \"(...) It is as if they were able to survive themselves.\" (...) It is as if they are able to survive themselves. (...) It is as if they are able to survive themselves. \"(...) It is as if they are able to survive themselves.\" It is as if they are able to survive themselves. \""}, {"heading": "Neural Networks for Clinical Diagnosis", "text": "The application of neural networks to medical tasks dates back more than twenty years (Baxt 1990), and the recent success of deep learning has sparked wider interest in building AI systems to support clinical decision-making. Lipton et al. (2015) train Long Short-Term Memory Networks (LSTMs) to classify 128 diagnoses out of 13 frequently but irregularly collected clinical measurements derived from the EHR. DoctorAI (Choi, Bahadori and Sun 2015) and RETAIN (Choi et al. 2016) also use time series data to classify diagnoses. Similar to this work, we formulate the problem as multi-label classification, as each medical note could be associated with multiple diagnoses. However, there are two important differences between our work and previous work. We only consider relief notes for our experiments, which are unstructured free texts and do not include time series during clinical trials."}, {"heading": "Dataset", "text": "MIMIC-III (Multi-parameter Intelligent Monitoring in Intensive Care) (Johnson et al. 2016) is a large, freely available clinical database, containing physiological signals and various measurements collected from patient monitors, as well as comprehensive clinical data obtained from hospital information systems for patients in intensive care (ICU) at over 58K. We use the MIMIC-III note table: v1.3, which contains the unstructured clinical notes in free text for patients. We use \"outlet summaries\" instead of \"admission notes,\" as previous notes contain the actual truth and the free text. Since outlet summaries are written after the diagnosis decision, we clean the notes by removing any mention of class labels from the text. As shown in Table 1, medical notes contain several details about the patient, but the sections are not consistent. We do not separate the sections from other than the NOSIS labels, which are the NOSIS labels."}, {"heading": "Knowledge Base", "text": "We use Wikipedia pages (see Table 2) that correspond to the diagnoses in the MIMIC III notes as an external source of knowledge. WikiProject Medicine is dedicated to improving the quality of medical articles on Wikipedia, and the information presented on these pages generally proves reliable (Trevena 2011). Since some diagnostic terms from MIMIC-III do not always match a Wikipedia title, we use the Wikipedia API with the diagnoses as search terms to find the most relevant Wikipedia pages. In most cases, we find an exact match, while in the rest, we select the most relevant page. We use the first paragraph and the paragraphs corresponding to the sections with the signs and symptoms for our experiments. In cases where such a section is not available, we use the second and third paragraphs of the page. This happens in the arcane diseases that have a limited content."}, {"heading": "Condensed Memory Networks", "text": "The basic structure of our model is inspired by MemNN."}, {"heading": "Network Overview", "text": "Figure 2 (b) shows the overview of the C-MemNN structure. Input x is converted to the internal state u1 using transformation matrix B. This value is converted to the corresponding storage value c1 using memory keys m1 using matrix A. In parallel, the storage state u is condensed to half its original dimension using transformation matrix D. If u is size 1 \u00d7 K, then D is size K \u00b7 K2. We call this reduced representation u the condensed storage state and this is the end of the first hop. This process is then repeated for a desired number of hops. After each hop, the condensed storage state u becomes the concatenation of its previous state and its current state, each reduced to half of its original dimension."}, {"heading": "Averaged Memory Networks", "text": "In C-MemNN, the transformation of u-MemNN adds additional parameters to the model with each jump, which is not always desirable. Therefore, we also present a simpler alternative model, which we call A-MemNN, to capture the hierarchy in the memory representation without adding any learned parameters. In this alternative model, we calculate the weighted average of u-MemNN across several hops. Instead of linking previous u-MemNN values together, we simply maintain an exponential moving average of different hops: u-k + 1 = u-k + u-k \u2212 12 + u-k \u2212 24 +.. (6) where the incipient condensed memory state is the same as the input memory state u-1 = u1."}, {"heading": "Memory Addressing", "text": "Softmax uses key value addressing as described in KV-MemNN on the product of question embedding and retrieved keys to determine a relevance probability distribution across memory locations, and the resulting representation is then the sum of the output memory representation o, weighted by these probability values. KV-MemNN is designed to select the most relevant response across a range of candidate responses, and the use of Softmax significantly reduces the estimated relevance of all but the most likely memory locations, which poses a problem for multi-mark classification, where multiple memory locations may be relevant to different target designations. We experimented with changing Softmax to Sigmoid to alleviate this problem, but this was not enough to accommodate the condensed form of internal state u resulting from previous hops. Therefore, we are investigating a novel alternative addressing scheme that we call gated address. This addressing method uses a multi-layer forward memory layer to determine each base layer of a network."}, {"heading": "Document Representation", "text": "There are a variety of models for displaying knowledge in key value stores, and choosing a model can have an impact on overall performance. We use a simple bag-of-words (BoW) model that converts every word wij in the document di = wi1, wi2, wi3,... into embedding, converts win into embedding, and summarizes these to obtain the vectors \u043a (di) = \u2211 j Awij, where A is the embedding matrix. Medical notes, memory keys, and memory values are all represented this way."}, {"heading": "Experiments", "text": "The distribution of diagnoses in our training data has a very long tail. There are 4,186 distinct diagnoses in the MIMIC-III discharge notes. However, many diagnoses (labels) are made in just a single note, which is not enough to train efficiently on these labels. We present experiments for both the 50 most common labels and the 100 most common labels covering 99.97%. For all experiments, we shorten both notes and wiki pages to 600 words. We reduce the trained vocabulary to 20K after removing common stop words. We use a common dimension of 500 for all embedding matrices. We use a memory slot of the dimension 300. A smaller embedding of the dimension 32 is used to represent the wiki titles."}, {"heading": "Training", "text": "We use Adam (Kingma and Ba 2014) stochastic gradient drop to optimize the learned parameters. The learning rate is 0.001 and the lot size for each iteration is 100 for all models. For the final prediction layer, we use a fully connected layer above the output of Equation 5 with a sigmoid activation function. The loss function is the sum of the cross entropy of prediction labels and prediction memory locations using the addressing scheme. The complexity of the model was punished by adding L2 regulation to the cross entropy loss function. We use dropout (Srivastava et al. 2014) with a 0.5 probability on the initial to decision sigmoid layer and limit the gradient norm to below 20. Models are trained to 80% of the data and validated to 10%. The remaining 10% are used as a test set, which is evaluated only once in all experiments with different models."}, {"heading": "Results and Analysis", "text": "We present experiments in which performance is evaluated on the basis of three metrics: the range below the ROC curve (AUC), average accuracy over the first five predictions, and hamming loss. AUC is calculated by taking the unweighted mean of AUC values for each label - also known as macro AUC. Average accuracy over the first five predictions is reported because it is a relevant metric for real-world applications. Hamming loss is reported instead of accuracy because it is a better metric for multiple labeling (Elisseeff and Weston 2001). As shown in Table 3, C-MemNN can outperform the results of various other storage networks in all experiments. The improvement is more pronounced with a higher number of memory hops than with a higher number of memory hops. This is due to the learning saturation of vanilla storage networks across multiple hops. While NA does not provide better results for higher memory hops, higher NN is for better results."}, {"heading": "Conclusion and Future Work", "text": "Weston, Chopra, and Bordes (2014) discussed the possibility of better memory representation for complex conclusions. We achieved better memory representation by condensing previous hops in a novel way to obtain a hierarchical representation of internal memory. We demonstrated the effectiveness of the proposed memory representation for clinical diagnostic conclusions from textual raw data. We discussed the limitations of storage networks for multi-mark classification and examined gated addresses to achieve better mapping between clinical notes and memory slots. We demonstrated that training multiple hops with compressed representation is helpful but still computationally expensive. We plan to investigate asynchronous memory updating, which will enable faster formation of memory networks. In the future, we will explore other sources of knowledge and the recently proposed biomedical word vectors (Chiu et al. 2016)."}, {"heading": "Acknowledgments", "text": "The authors would like to thank the anonymous reviewers for their valuable comments and feedback. The first author especially thanks Prof. James Storer from Brandeis University for his guidance and Nick Moran, Ryan Marcus and Solomon Garber for helpful discussions."}], "references": [{"title": "Neural machine translation by jointly learning to align and translate", "author": ["Cho Bahdanau", "D. Bengio 2014] Bahdanau", "K. Cho", "Y. Bengio"], "venue": "arXiv preprint arXiv:1409.0473", "citeRegEx": "Bahdanau et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Bahdanau et al\\.", "year": 2014}, {"title": "W", "author": ["Baxt"], "venue": "G.", "citeRegEx": "Baxt 1990", "shortCiteRegEx": null, "year": 1990}, {"title": "How to train good word embeddings for biomedical nlp", "author": ["Chiu"], "venue": null, "citeRegEx": "Chiu,? \\Q2016\\E", "shortCiteRegEx": "Chiu", "year": 2016}, {"title": "M", "author": ["Choi, E.", "Bahadori"], "venue": "T.; and Sun, J.", "citeRegEx": "Choi. Bahadori. and Sun 2015", "shortCiteRegEx": null, "year": 2015}, {"title": "W", "author": ["E. Choi", "M.T. Bahadori", "A. Schuetz", "Stewart"], "venue": "F.; and Sun, J.", "citeRegEx": "Choi et al. 2016", "shortCiteRegEx": null, "year": 2016}, {"title": "Hierarchical multiscale recurrent neural networks. arXiv preprint arXiv:1609.01704", "author": ["Ahn Chung", "J. Bengio 2016] Chung", "S. Ahn", "Y. Bengio"], "venue": null, "citeRegEx": "Chung et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Chung et al\\.", "year": 2016}, {"title": "Attention-over-attention neural networks for reading comprehension", "author": ["Cui"], "venue": "CoRR abs/1607.04423", "citeRegEx": "Cui,? \\Q2016\\E", "shortCiteRegEx": "Cui", "year": 2016}, {"title": "W", "author": ["B. Dhingra", "H. Liu", "Cohen"], "venue": "W.; and Salakhutdinov, R.", "citeRegEx": "Dhingra et al. 2016", "shortCiteRegEx": null, "year": 2016}, {"title": "and Weston", "author": ["A. Elisseeff"], "venue": "J.", "citeRegEx": "Elisseeff and Weston 2001", "shortCiteRegEx": null, "year": 2001}, {"title": "S", "author": ["Hasan"], "venue": "A.; Zhu, X.; Dong, Y.; Liu, J.; and Farri, O.", "citeRegEx": "Hasan et al. 2014", "shortCiteRegEx": null, "year": 2014}, {"title": "S", "author": ["Hasan"], "venue": "A.; Ling, Y.; Liu, J.; and Farri, O.", "citeRegEx": "Hasan et al. 2015", "shortCiteRegEx": null, "year": 2015}, {"title": "S", "author": ["Hasan"], "venue": "A.; Zhao, S.; Datla, V.; Liu, J.; Lee, K.; Qadir, A.; Prakash, A.; and Farri, O.", "citeRegEx": "Hasan et al. 2016", "shortCiteRegEx": null, "year": 2016}, {"title": "K", "author": ["Hermann"], "venue": "M.; Kocisk\u00fd, T.; Grefenstette, E.; Espeholt, L.; Kay, W.; Suleyman, M.; and Blunsom, P.", "citeRegEx": "Hermann et al. 2015", "shortCiteRegEx": null, "year": 2015}, {"title": "R", "author": ["A.E. Johnson", "T.J. Pollard", "L. Shen", "L.-w. H. Lehman", "M. Feng", "M. Ghassemi", "B. Moody", "P. Szolovits", "L.A. Celi", "Mark"], "venue": "G.", "citeRegEx": "Johnson et al. 2016", "shortCiteRegEx": null, "year": 2016}, {"title": "and Mikolov", "author": ["A. Joulin"], "venue": "T.", "citeRegEx": "Joulin and Mikolov 2015", "shortCiteRegEx": null, "year": 2015}, {"title": "2016. Bag of tricks for efficient text classification", "author": ["Joulin"], "venue": "arXiv preprint arXiv:1607.01759", "citeRegEx": "Joulin,? \\Q2016\\E", "shortCiteRegEx": "Joulin", "year": 2016}, {"title": "and Ba", "author": ["D.P. Kingma"], "venue": "J.", "citeRegEx": "Kingma and Ba 2014", "shortCiteRegEx": null, "year": 2014}, {"title": "R", "author": ["Z.C. Lipton", "D.C. Kale", "C. Elkan", "Wetzel"], "venue": "C.", "citeRegEx": "Lipton et al. 2015", "shortCiteRegEx": null, "year": 2015}, {"title": "Hierarchical question-image co-attention for visual question answering", "author": ["Lu"], "venue": "arXiv preprint arXiv:1606.00061", "citeRegEx": "Lu,? \\Q2016\\E", "shortCiteRegEx": "Lu", "year": 2016}, {"title": "Key-value memory networks for directly reading documents. CoRR abs/1606.03126", "author": ["Miller"], "venue": null, "citeRegEx": "Miller,? \\Q2016\\E", "shortCiteRegEx": "Miller", "year": 2016}, {"title": "Iterative alternating neural attention for machine reading", "author": ["Bachman Sordoni", "A. Bengio 2016] Sordoni", "P. Bachman", "Y. Bengio"], "venue": "CoRR abs/1606.02245", "citeRegEx": "Sordoni et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Sordoni et al\\.", "year": 2016}, {"title": "G", "author": ["Srivastava, N.", "Hinton"], "venue": "E.; Krizhevsky, A.; Sutskever, I.; and Salakhutdinov, R.", "citeRegEx": "Srivastava et al. 2014", "shortCiteRegEx": null, "year": 2014}, {"title": "End-to-end memory networks", "author": ["Sukhbaatar"], "venue": null, "citeRegEx": "Sukhbaatar,? \\Q2015\\E", "shortCiteRegEx": "Sukhbaatar", "year": 2015}, {"title": "Movieqa: Understanding stories in movies through question-answering", "author": ["Tapaswi"], "venue": "arXiv preprint arXiv:1512.02902", "citeRegEx": "Tapaswi,? \\Q2015\\E", "shortCiteRegEx": "Tapaswi", "year": 2015}, {"title": "Towards ai-complete question answering: A set of prerequisite toy tasks. arXiv preprint arXiv:1502.05698", "author": [], "venue": null, "citeRegEx": "2015.,? \\Q2015\\E", "shortCiteRegEx": "2015.", "year": 2015}, {"title": "R", "author": ["K. Xu", "J. Ba", "R. Kiros", "K. Cho", "A. Courville", "R. Salakhutdinov", "Zemel"], "venue": "S.; and Bengio, Y.", "citeRegEx": "Xu et al. 2015", "shortCiteRegEx": null, "year": 2015}], "referenceMentions": [], "year": 2016, "abstractText": "Diagnosis of a clinical condition is a challenging task, which often requires significant medical investigation. Previous work related to diagnostic inferencing problems mostly consider multivariate observational data (e.g. physiological signals, lab tests etc.). In contrast, we explore the problem using free-text medical notes recorded in an electronic health record (EHR). Complex tasks like these can benefit from structured knowledge bases, but those are not scalable. We instead exploit raw text from Wikipedia as a knowledge source. Memory networks have been demonstrated to be effective in tasks which require comprehension of free-form text. They use the final iteration of the learned representation to predict probable classes. We introduce condensed memory neural networks (C-MemNNs), a novel model with iterative condensation of memory representations that preserves the hierarchy of features in the memory. Experiments on the MIMIC-III dataset show that the proposed model outperforms other variants of memory networks to predict the most probable diagnoses given a complex clinical scenario.", "creator": "LaTeX with hyperref package"}}}