{"id": "1612.01859", "review": {"conference": "NIPS", "VERSION": "v1", "DATE_OF_SUBMISSION": "6-Dec-2016", "title": "Combinatorial semi-bandit with known covariance", "abstract": "The combinatorial stochastic semi-bandit problem is an extension of the classical multi-armed bandit problem in which an algorithm pulls more than one arm at each stage and the rewards of all pulled arms are revealed. One difference with the single arm variant is that the dependency structure of the arms is crucial. Previous works on this setting either used a worst-case approach or imposed independence of the arms. We introduce a way to quantify the dependency structure of the problem and design an algorithm that adapts to it. The algorithm is based on linear regression and the analysis develops techniques from the linear bandit literature. By comparing its performance to a new lower bound, we prove that it is optimal, up to a poly-logarithmic factor in the number of pulled arms.", "histories": [["v1", "Tue, 6 Dec 2016 15:28:22 GMT  (135kb,D)", "http://arxiv.org/abs/1612.01859v1", "in NIPS 2016 (Conference on Neural Information Processing Systems), Dec 2016, Barcelona, Spain"]], "COMMENTS": "in NIPS 2016 (Conference on Neural Information Processing Systems), Dec 2016, Barcelona, Spain", "reviews": [], "SUBJECTS": "cs.LG", "authors": ["r\u00e9my degenne", "vianney perchet"], "accepted": true, "id": "1612.01859"}, "pdf": {"name": "1612.01859.pdf", "metadata": {"source": "CRF", "title": "Combinatorial semi-bandit with known covariance", "authors": ["R\u00e9my Degenne", "Vianney Perchet"], "emails": ["degenne@cmla.ens-cachan.fr", "perchet@normalesup.org"], "sections": [{"heading": "1 Introduction and setting", "text": "The Multi-Arm Bandit Problem (MAB) is a sequential learning task in which an algorithm makes a decision (or \"pulls an arm\") at each level, and then receives a reward from that choice, with the goal of maximizing the cumulative reward [15]. Here, we look at its stochastic combinatorial extension, in which the algorithm selects a subset of weapons at each level [2, 5, 6, 9]. These arms could, for example, form the path from an origin to a destination in a network, in the combinatorial environment in which the interdependencies between the arms can play a role (we consider the distribution of rewards as invariant over time). Here, we examine how the covariant structure of the arms affects the difficulty of the learning task and whether it is possible to design a unique algorithm that is able to function optimally in all cases."}, {"heading": "2 Lower bound", "text": "First of all, we prove a lower limit on the regret of each algorithm by showing the connection between the subgaussian covariance matrix and the difficulty of the problem, depending on the maximum extra-diagonal correlation coefficient of the covariance matrix. This coefficient is \u03b3 = max {(i, j), i6 = j} C (ij) \u221a C (ii) C (jj). The limit applies to consistent algorithms [13], for which regretting each problem ERt = o (ta) as t \u2192 + \u221e for all a > 0.Theorem 1. Let us imagine that d is a multiple of m. Then for each, every consistent algorithm there is a problem with gaps."}, {"heading": "3 OLS-UCB Algorithm and analysis", "text": "Given the combinatorial half-bandit at stage t \u2265 1, the observations from the t \u2212 1 stages form as many linear equations, and the goal of an algorithm is to choose the best action. To find the action with the highest mean, we estimate the mean of all weapons. This can be considered a regression problem. The design of our algorithm results from this observation and is inspired by linear regression in the fixed design setting, similar to what has been done in the stochastic linear bandit literature [16, 8]. There are many estimates of linear regression and we focus on those that are simple enough and adaptive: Ordinary Least Squares (OLS)."}, {"heading": "3.1 Fixed design linear regression and OLS-UCB algorithm", "text": "For a matrix M, we have also observed a series of linear equations that are both independent of each other and have a known subgaussian constant that controls their variance. However, this is not true in our online setting, as the successive actions are not independent. At stage t, we define (i) t = 1 (i) t = 1 (i) t = 1 (i) t = 1 (i) s = 1 (i) s = 1 (i) s = 1 (i) s = 1 (i) s = 1 (i) s = 1 (i) s = 1 (i) As) s = 1 (i) s) s = 1 (ij) t = 1 (ij) t = 1 (i)."}, {"heading": "3.2 Comparison with other algorithms", "text": "Previous work assumed that the rewards of the individual arms are in [0, 1], giving them a 1 / 2 Subgaussian property. Therefore, we assume that ESCB-2 has an O (d) upper limit for our comparison, but our analysis streamlines it to O (d). In general (worst) case [12] we prove an O (dm) upper limit (which is narrow) with CombUCB1, a UCB-based algorithm introduced in [6] and using the exploration concept of independence at stage t. In general (worst) cases, we prove an O (dm) upper limit (which is narrow) with CombUCB1, a UCB-based algorithm that uses the exploration concept at stage t."}, {"heading": "3.3 Regret Decomposition", "text": "Let's say Hi, t = (\u00b5) t-\u00b5 (i) t-\u00b5 (i) | \u2265 \u0445 t2m} and Ht = (vid) i = 1Hi, t-t. Ht is the event that at least one coordinate of \u00b5-t is far from the true mean. Let's say that Gt = (A) > \u00b5 (A) > \u00b5 (A) + Et (A)} is the event that the estimate of the optimal action is far below its true mean. Let's break down the regret according to these events: RT \u2264 T (T) = 1 \u0445 tI (Gt) + T (T) + T (T) = 1 \u0445 tI (Gt) + T (T) = 1 \u0445 tI (Ht) events Gt) and Ht (Ht) are rare and result in a finite regret (see below). Let's first simplify regret due to Gt-Ht (Ht) and show that it is limited by the \"variance\" of the concept of the algorithm."}, {"heading": "3.4 Expected regret from Ht", "text": "Lemma 2. The expected regret due to the event Ht is E [\u2211 T t = 1 \u2206 tI {Ht}] \u2264 8dm2 maxi {C (ii)} \u2206 max \u0445 2min. The proof is based on Hoeffding's mean estimate of inequality on the arm and can be found in Appendix B.2 of the supplementary material."}, {"heading": "3.5 Expected regret from Gt", "text": "We want to determine the probability that the estimated reward for the optimal action is far from its mean. We show that it is sufficient to control a self-normalized sum and bind it using arguments from [14], or [1], who applied it to linear bandits. The analysis also includes an exfoliating argument, as it was made in a dimension by [10], to bind a similar quantity. Let it. Let it. (A) Let it. (A) Let it. (A) Let it. (A) Let it. (A). (A). (A). (A). (A). (A). (A). (A). (A). (A). Let it. (A). (A). (A). (A). (A). (A). (A). (A). (A). (A). (A). (A). (A). (A). (A). (A)."}, {"heading": "3.6 Bounding the variance term", "text": "The object of this section is to Et (An) under the event {t \u2264 Et (An) (An) (an) (an) (an) (an) (an) (an) (an) (an) (an) (an) (an) (an) (an) (an) (an) (an) (an) (an) (an) (an) (an) (an) (an) (an) (an) (an) (an) (an) (an) (an) (an) (an) (an) (an) (an) (an) (an) (an) (an) (an) (an) (an) (an) (an) (an) (an) (an) (an) (an) (an) (an) (an) (an) (an) (an) (an) (an) (an) (an) (an) (an) (an) (an) (an) (an) (an) (an) (an) (an) (an) (an) (an) (an) (an) (an) (an) (an) (an) (an) (an) (an) (an) (an) (an) (an) (an) (an) (an) (an) (an) (an) (an) (an) (an) (an (an) (an) (an) (an) (an (an) (an) (an) (an (an) (an) (an (an) (an (an) (an) (an (an) (an (an) (an) (an (an) (an (an) (an) (an (an) (an (an) (an (an) (an) (an) (an (an (an) (an (an) (an) (an (an) (an) (an) (an (an (an) (an) (an (an) (an (an) (an) (an) (an (an) (an) (an (an) (an) (an) (an (an) (an) (an (an) (an) (an) (an) (an) (an (an (an) (an) (an ("}, {"heading": "4 Conclusion", "text": "We defined a continuum of attitudes from general to independent armament cases suitable for the analysis of semi-bandit algorithms. We showed a lower scale with a parameter that quantifies the respective attitudes in that continuum, and proposed an algorithm inspired by linear regression with an upper limit corresponding to the lower limit up to a log2 meter long term. Finally, we showed how to use tools from the linear bandit literature to analyze algorithms for the combinatorial bandit case that are based on linear regression. It would be interesting to estimate the subgaussian covariance matrix online in order to reach good limits of regret without prior knowledge. Moreover, our algorithm is not computationally efficient as it requires the calculation of an Argmax via actions at each stage. It might be possible to calculate this Argmax less frequently and still maintain the remorse guarantee, as is the case [7 and]."}, {"heading": "Acknowledgements", "text": "The authors would like to recognize the funding by the ANR under the grant number ANR-13-JS01-0004 and EDF through the program Gaspard Monge for Optimization and the Irsdi project Tecolere."}, {"heading": "A The subgaussian covariance matrix", "text": "Property 0. An \u03b1-subgaussian variable X confirms Var [X] \u2264 \u03b12.Let \u03b7 be a noise in Rd with mean 0 and subgaussian covariance matrix C. The following properties are direct consequences of property 0 and are presented as justification for the name of the subgaussian \"covariance matrix.\" Property 1. For all i [d], Var [\u03b7 (i)] \u2264 C (ii).Property 2. For all (i, j), Var [\u03b7 (i)] + Var [\u03b7 (j) + 2Cov [\u03b7 (i), \u03b7 (j)] \u2264 C (ii) + C (jj) + 2C (ij).Property 3. If Var [\u03b7 (i)] = C (ii) and Var [\u03b7 (j)] = C (jj), Cov [\u03b7 (i), \u03b7 (j)] \u2264 C (ij)."}, {"heading": "B Missing proofs", "text": "We look at the problem where A has a series of d / m separate actions A1 = >., Ad / m and assume that these actions are independent. This is no more than a bandit problem with d / m arms, each with m forearms. We choose a noise where Jblocs is a block matrix in which each m \u00b7 m block is indexed by an action. Aj contains only ones and other coefficients. The actions are independent and the sub-arms correlate with correlations. Then C1 / 2 = Lemm 1 \u2212 g Id + 1m (1 + m \u00b2) of the optimal action."}, {"heading": "C Finding the best sequences for the sums indexed by 1.", "text": "The limitations for the four sequences (\u03b1i, 1) i \u2265 1, (\u03b1i, 2) i \u2265 1, (\u03b2i, 1) i \u2265 0 and (\u03b2i, 2) i \u2265 0 are that they are with the limit 0, \u03b20.1 = \u03b20.2 = 1 and limj \u2192 + \u221e \u03b2j, 2 / \u221a \u03b1j, 2 = 0.For i \u2265 1 we take \u03b2i, 1 = \u03b1i, 1 = \u03b2i with the limit 0, 1), then j0.1 = d logmlog 1 / \u03b2 e and l1 \u2211 j0.1 j = 1 \u03b1j, 1 \u03b2j, 1 = j20.1 / \u03b2 j20.1 / \u03b2. We take \u03b2i, 2 and \u03b1i, 2 as in [12]: \u03b2i, 2 = \u03b2i2 = \u03b2i2 with the limit, 1 \u03b2i2 = 0.236; \u03b1j, 2 = (1 \u2212 \u03b22) 2 \u03b2i, with the limit 0."}], "references": [{"title": "Improved Algorithms for Linear Stochastic Bandits", "author": ["Yasin Abbasi-Yadkori", "David Pal", "Csaba Szepesvari"], "venue": "Neural Information Processing Systems,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2011}, {"title": "Regret in online combinatorial optimization", "author": ["Jean-Yves Audibert", "S\u00e9bastien Bubeck", "G\u00e1bor Lugosi"], "venue": "Mathematics of Operations Research,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2013}, {"title": "Finite-time analysis of the multiarmed bandit problem", "author": ["Peter Auer", "Nicolo Cesa-Bianchi", "Paul Fischer"], "venue": "Machine learning,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2002}, {"title": "Bandit Theory meets Compressed Sensing for high dimensional Stochastic Linear Bandit", "author": ["Alexandra Carpentier", "R\u00e9mi Munos"], "venue": "Advances in Neural Information Processing Systems (NIPS),", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2012}, {"title": "Combinatorial bandits", "author": ["Nicolo Cesa-Bianchi", "G\u00e1bor Lugosi"], "venue": "Journal of Computer and System Sciences,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2012}, {"title": "Combinatorial multi-armed bandit: General framework and applications", "author": ["Wei Chen", "Yajun Wang", "Yang Yuan"], "venue": "Proceedings of the 30th International Conference on Machine Learning (ICML),", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2013}, {"title": "Combinatorial Bandits Revisited", "author": ["Richard Combes", "M. Sadegh Talebi", "Alexandre Proutiere", "Marc Lelarge"], "venue": "Neural Information Processing Systems,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2015}, {"title": "Parametric Bandits: The Generalized Linear Case", "author": ["Sarah Filippi", "Olivier Capp\u00e9", "Aur\u00e9lien Garivier", "Csaba Szepesv\u00e1ri"], "venue": "Neural Information Processing Systems,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2010}, {"title": "Combinatorial network optimization with unknown variables: Multi-armed bandits with linear rewards and individual observations", "author": ["Yi Gai", "Bhaskar Krishnamachari", "Rahul Jain"], "venue": "IEEE/ACM Transactions on Networking,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2012}, {"title": "Informational confidence bounds for self-normalized averages and applications", "author": ["Aur\u00e9lien Garivier"], "venue": "IEEE Information Theory Workshop,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2013}, {"title": "Optimal Regret Analysis of Thompson Sampling in Stochastic Multi-armed Bandit Problem with Multiple Plays", "author": ["Junpei Komiyama", "Junya Honda", "Hiroshi Nakagawa"], "venue": "Proceedings of the 32nd International Conference on Machine Learning,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2015}, {"title": "Tight regret bounds for stochastic combinatorial semi-bandits", "author": ["Branislav Kveton", "Zheng Wen", "Azin Ashkan", "Csaba Szepesvari"], "venue": "Proceedings of the 18th International Conference on Artificial Intelligence and Statistics,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2015}, {"title": "Asymptotically efficient adaptive allocation rules", "author": ["Tze Leung Lai", "Herbert Robbins"], "venue": "Advances in applied mathematics,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 1985}, {"title": "Self-normalized processes: Limit theory and Statistical Applications", "author": ["Victor H Pe\u00f1a", "Tze Leung Lai", "Qi-Man Shao"], "venue": "Springer Science & Business Media,", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2008}, {"title": "Some aspects of the sequential design of experiments", "author": ["Herbert Robbins"], "venue": "In Herbert Robbins Selected Papers,", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 1985}], "referenceMentions": [{"referenceID": 14, "context": "It then gets a reward from this choice, with the goal of maximizing the cumulative reward [15].", "startOffset": 90, "endOffset": 94}, {"referenceID": 1, "context": "We consider here its stochastic combinatorial extension, in which the algorithm chooses at each stage a subset of arms [2, 5, 6, 9].", "startOffset": 119, "endOffset": 131}, {"referenceID": 4, "context": "We consider here its stochastic combinatorial extension, in which the algorithm chooses at each stage a subset of arms [2, 5, 6, 9].", "startOffset": 119, "endOffset": 131}, {"referenceID": 5, "context": "We consider here its stochastic combinatorial extension, in which the algorithm chooses at each stage a subset of arms [2, 5, 6, 9].", "startOffset": 119, "endOffset": 131}, {"referenceID": 8, "context": "We consider here its stochastic combinatorial extension, in which the algorithm chooses at each stage a subset of arms [2, 5, 6, 9].", "startOffset": 119, "endOffset": 131}, {"referenceID": 4, "context": "This setting was already studied [5], most recently in [7, 12], where two different algorithms are used to tackle on one hand the case where the arms have independent rewards and on the other hand the general bounded case.", "startOffset": 33, "endOffset": 36}, {"referenceID": 6, "context": "This setting was already studied [5], most recently in [7, 12], where two different algorithms are used to tackle on one hand the case where the arms have independent rewards and on the other hand the general bounded case.", "startOffset": 55, "endOffset": 62}, {"referenceID": 11, "context": "This setting was already studied [5], most recently in [7, 12], where two different algorithms are used to tackle on one hand the case where the arms have independent rewards and on the other hand the general bounded case.", "startOffset": 55, "endOffset": 62}, {"referenceID": 10, "context": "Another algorithm for the independent arms case based on Thompson Sampling was introduced in [11].", "startOffset": 93, "endOffset": 97}, {"referenceID": 6, "context": "In the case of 1-subgaussian independent rewards, in which C can be chosen diagonal, a known lower bound on the regret appearing in [7] is d \u2206 log T , while [12] proves a dm \u2206 log T lower bound in general.", "startOffset": 132, "endOffset": 135}, {"referenceID": 11, "context": "In the case of 1-subgaussian independent rewards, in which C can be chosen diagonal, a known lower bound on the regret appearing in [7] is d \u2206 log T , while [12] proves a dm \u2206 log T lower bound in general.", "startOffset": 157, "endOffset": 161}, {"referenceID": 12, "context": "The bound is valid for consistent algorithms [13], for which the regret on any problem verifies ERt = o(t) as t\u2192 +\u221e for all a > 0.", "startOffset": 45, "endOffset": 49}, {"referenceID": 0, "context": "Then, for any \u2206 > 0, for any consistent algorithm, there is a problem with gaps \u2206, \u03c3-subgaussian arms and correlation coefficients smaller than \u03b3 \u2208 [0, 1] on which the regret is such that", "startOffset": 148, "endOffset": 154}, {"referenceID": 12, "context": "lim inf t\u2192+\u221e ERt log t \u2265 (1 + \u03b3(m\u2212 1)) (d\u2212m) \u2206 This bound is a consequence of the classical result of [13] for multi-armed bandits, applied to the problem of choosing one among d/m paths, each of which has m different successive edges (Figure 1).", "startOffset": 102, "endOffset": 106}, {"referenceID": 7, "context": "The design of our algorithm stems from this observation and is inspired by linear regression in the fixed design setting, similarly to what was done in the stochastic linear bandit literature [16, 8].", "startOffset": 192, "endOffset": 199}, {"referenceID": 12, "context": "We get confidence intervals for the estimates and can use an upper confidence bound strategy [13, 3].", "startOffset": 93, "endOffset": 100}, {"referenceID": 2, "context": "We get confidence intervals for the estimates and can use an upper confidence bound strategy [13, 3].", "startOffset": 93, "endOffset": 100}, {"referenceID": 0, "context": "2 Comparison with other algorithms Previous works supposed that the rewards of the individual arms are in [0, 1], which gives them a 1/2-subgaussian property.", "startOffset": 106, "endOffset": 112}, {"referenceID": 6, "context": "In the independent case, our algorithm is the same as ESCB-2 from [7], up to the parameter \u03bb.", "startOffset": 66, "endOffset": 69}, {"referenceID": 11, "context": "In the general (worst) case, [12] prove an O( log T \u2206 ) upper bound (which is tight) using CombUCB1, a UCB based algorithm introduced in [6] which at stage t uses the exploration term \u221a 1.", "startOffset": 29, "endOffset": 33}, {"referenceID": 5, "context": "In the general (worst) case, [12] prove an O( log T \u2206 ) upper bound (which is tight) using CombUCB1, a UCB based algorithm introduced in [6] which at stage t uses the exploration term \u221a 1.", "startOffset": 137, "endOffset": 140}, {"referenceID": 13, "context": "We show that it is sufficient to control a selfnormalized sum and do it using arguments from [14], or [1] who applied them to linear bandits.", "startOffset": 93, "endOffset": 97}, {"referenceID": 0, "context": "We show that it is sufficient to control a selfnormalized sum and do it using arguments from [14], or [1] who applied them to linear bandits.", "startOffset": 102, "endOffset": 105}, {"referenceID": 9, "context": "The analysis also involves a peeling argument, as was done in one dimension by [10] to bound a similar quantity.", "startOffset": 79, "endOffset": 83}, {"referenceID": 0, "context": "Doing so leads to the following lemma, extracted from the proof of Theorem 1 of [1].", "startOffset": 80, "endOffset": 83}, {"referenceID": 0, "context": "Let \u03b3t \u2208 [0, 1] such that for all i, j \u2208 At with i 6= j, \u0393 \u2264 \u03b3t \u221a \u0393(ii)\u0393(jj).", "startOffset": 9, "endOffset": 15}, {"referenceID": 6, "context": "We recognize here the forms of the indexes used in [7] for independent arms (left term) and [12] for general arms (right term).", "startOffset": 51, "endOffset": 54}, {"referenceID": 11, "context": "We recognize here the forms of the indexes used in [7] for independent arms (left term) and [12] for general arms (right term).", "startOffset": 92, "endOffset": 96}, {"referenceID": 6, "context": "In [7], \u03b1i,1 and \u03b2i,1 were such that the logm term was replaced by \u221a m.", "startOffset": 3, "endOffset": 6}, {"referenceID": 0, "context": "It may be possible to compute this argmax less often and still keep the regret guaranty, as was done in [1] and [7].", "startOffset": 104, "endOffset": 107}, {"referenceID": 6, "context": "It may be possible to compute this argmax less often and still keep the regret guaranty, as was done in [1] and [7].", "startOffset": 112, "endOffset": 115}, {"referenceID": 3, "context": "Or one could take advantage of a sparse covariance matrix by using sparse estimators, as was done in the linear bandit case in [4].", "startOffset": 127, "endOffset": 130}], "year": 2016, "abstractText": "The combinatorial stochastic semi-bandit problem is an extension of the classical multi-armed bandit problem in which an algorithm pulls more than one arm at each stage and the rewards of all pulled arms are revealed. One difference with the single arm variant is that the dependency structure of the arms is crucial. Previous works on this setting either used a worst-case approach or imposed independence of the arms. We introduce a way to quantify the dependency structure of the problem and design an algorithm that adapts to it. The algorithm is based on linear regression and the analysis develops techniques from the linear bandit literature. By comparing its performance to a new lower bound, we prove that it is optimal, up to a poly-logarithmic factor in the number of pulled arms.", "creator": "LaTeX with hyperref package"}}}