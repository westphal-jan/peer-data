{"id": "1612.02136", "review": {"conference": "iclr", "VERSION": "v1", "DATE_OF_SUBMISSION": "7-Dec-2016", "title": "Mode Regularized Generative Adversarial Networks", "abstract": "Although Generative Adversarial Networks achieve state-of-the-art results on a variety of generative tasks, they are regarded as highly unstable and prone to miss modes. We argue that these bad behaviors of GANs are due to the very particular functional shape of the trained discriminators in high dimensional spaces, which can easily make training stuck or push probability mass in the wrong direction, towards that of higher concentration than that of the data generating distribution.", "histories": [["v1", "Wed, 7 Dec 2016 07:45:38 GMT  (1133kb,D)", "http://arxiv.org/abs/1612.02136v1", "Under review as a conference paper at ICLR 2017"], ["v2", "Fri, 9 Dec 2016 06:08:37 GMT  (1133kb,D)", "http://arxiv.org/abs/1612.02136v2", "Under review as a conference paper at ICLR 2017"], ["v3", "Sun, 18 Dec 2016 05:55:22 GMT  (1006kb,D)", "http://arxiv.org/abs/1612.02136v3", "Under review as a conference paper at ICLR 2017"], ["v4", "Mon, 20 Feb 2017 05:01:27 GMT  (1337kb,D)", "http://arxiv.org/abs/1612.02136v4", "Published as a conference paper at ICLR 2017"], ["v5", "Thu, 2 Mar 2017 06:28:13 GMT  (1337kb,D)", "http://arxiv.org/abs/1612.02136v5", "Published as a conference paper at ICLR 2017"]], "COMMENTS": "Under review as a conference paper at ICLR 2017", "reviews": [], "SUBJECTS": "cs.LG cs.AI cs.CV cs.NE", "authors": ["tong che", "yanran li", "athul paul jacob", "yoshua bengio", "wenjie li"], "accepted": true, "id": "1612.02136"}, "pdf": {"name": "1612.02136.pdf", "metadata": {"source": "CRF", "title": null, "authors": ["Paul Jacob", "\u2020Yoshua Bengio", "\u2021Wenjie Li"], "emails": ["tong.che@umontreal.ca", "ap.jacob@umontreal.ca", "yoshua.bengio@umontreal.ca", "csyli@comp.polyu.edu.hk", "cswjli@comp.polyu.edu.hk"], "sections": [{"heading": "1 INTRODUCTION", "text": "In fact, most of them are a very rare disease that is able to infect itself, and most of them are not able to put themselves in a position to put themselves in the position they are in."}, {"heading": "2 RELATED WORK", "text": "In fact, most of them are able to survive on their own, without being able to survive on their own."}, {"heading": "3 MODE REGULARIZERS FOR GANS", "text": "The GAN training method can be seen as an uncooperative two-player game in which discriminator D tries to distinguish between real and generated examples, while generator G tries to deceive the discriminator by pushing the generated samples towards higher discriminatory values. Training discriminator D can be seen as training an evaluation metric on the sample space. Then, generator G must take advantage of the local trajectory provided by the discriminator to improve itself, namely to move towards data diversity. We will now take a closer look at the root cause of instabilities during training GANs. The discriminator will be trained on both generated and real examples. As Goodfellow et al. (2014); Denton et al. (2015); Radford et al. (2015), when data diversity and generation diversity are separated from each other (which in almost all practical training situations is similar to data manner1)."}, {"heading": "3.1 GEOMETRIC METRICS REGULARIZER", "text": "Compared to the goal for the GAN generator, the optimization goals for supervised learning are more stable from the optimization point of view. The difference is clear: The optimization goal for the GAN generator is a learned discriminator. Whereas in the supervised models the optimization goals are distance functions with beautiful geometric characteristics, the latter usually offers much easier training sequences than the former, especially in the early stages of the training. 1This problem also exists when we use logD (G (z)) as the goal for the generator, such as Denton et al. (2015) and our experiments. Inspired by this observation, we propose to integrate a supervised training signal as a regulator on the discrimination target. Let's take the generator G (z): X generates samples by starting from a fixed prior distribution in room Z, followed by a deterministically feasible transformation G into sample room X."}, {"heading": "3.2 MODE REGULARIZER", "text": "In addition to the metric regulator, we propose a mode regulator to further punish missing modes. In traditional GANs, the optimization target for the generator is the empirical sum \u2211 i-\u03b8 logD (G\u03b8 (zi)). The problem of the missing mode is caused by the combination of two facts: (1) the areas near missing modes are by definition rarely visited by the generator and therefore provide very few examples of how to improve the generator around these areas, and (2) both missing modes and not missing modes tend to correspond to a high value of D, since the generator is not perfect, so that the discriminator can make strong local decisions and achieve a high value of D even near non-missing modes. In this way, we can achieve a fair probability mass distribution across different modes. In short, our regulated optimization target for the generator and the encoder will be: TG = Ez [logD (G) (z) + D (G) + Gr) + (E) (E) (E) (E)."}, {"heading": "3.3 MANIFOLD-DIFFUSION TRAINING FOR REGULARIZED GANS", "text": "On some large datasets, such as CelebA, for example, the regulators we have discussed improve the variety of samples generated, but the quality of the samples cannot be as good without carefully tuning the hyperparameters. Here, we propose a new algorithm for forming metrically regulated GANs, which is very stable and much easier to tune to produce good specimens. The proposed algorithm splits the training process of GANs into two steps: a multiple step and a diffusion step. In the multiple step, we try to match the generation multiplicity and the real data multiplicity using an encoder and the geometric loss. In the diffusion step, we try to match the probability mass on the generation multiplicity according to the real data distribution.An example of multiplicity diffusion multiplicity and the real data multiplicity using an identical encoder and geometric loss."}, {"heading": "3.4 EVALUATION METRICS FOR MODE MISSING", "text": "To estimate both the missing modes and sample qualities in our experiments, we used several different metrics for different experiments instead of human annotators. The starting score (Salimans et al., 2016) was considered a good assessment of sample quality from a marked dataset: exp (ExKL (p (y) | p * (y) | p * (3) Where x denotes a sample, p (y) is the softmax output of a trained label classifier, and p * (y) is the general distribution of the generated samples. The intuition behind this score is that a strong classifier usually has high confidence in good samples. However, sometimes the starting point is not a good measurement for our purpose. Suppose that a generative model collapses into a very bad picture. Although the model is very bad, it can have a perfect starting point because p (y | x) has high entropy and p (y) low data quality."}, {"heading": "4.1.1 GRID SEARCH FOR MNIST GAN MODELS", "text": "To systematically examine the impact of our proposed regulators on GAN models in terms of improving stability and sample quality, we use a large-scale network search for various GAN hyperparameters on the MNIST dataset. The network search is based on a pair of randomly selected loss weights: \u03bb1 = 0.2 and \u03bb2 = 0.4. We use the same hyperparameter settings for both GAN and regulated GAN and list the search areas in Table 1. Our network search is similar to those in Zhao et al. (2016). Please refer to them for detailed explanations of these hyperparameters. For evaluation, we first form a 4-layer CNN classifier on the MNIST digits and then apply them to calculate the MODE values for the samples generated from all of these models. The resulting distribution of the MODE score is shown in Figure 3."}, {"heading": "4.1.2 COMPOSITIONAL MNIST DATA WITH 1000 MODES", "text": "To quantify the effect of our regulators on the missing modes, we link three MNIST digits to a number in [0.999] in a single 64x64 image and then train DCGAN as the base model on the 1000-mode datasets. The digits on the image are scanned with different probabilities to test the model's ability to maintain small modes in the generation. We again use a pre-trained classifier for MNIST instead of a human to evaluate the models. # Miss represents the number of missing modes reported by classifiers, i.e. the size of the number set of numbers that the model never generates. KL stands for the KL divergence between the distribution of the generated numbers reported by classifiers and the distribution of numbers in the training data (as for the Inception Score). The results are shown in Table 2 with the help of our IST suggested number as well as the missing modes."}, {"heading": "4.2 CELEBA", "text": "To test the effectiveness of our proposal for more difficult problems, we implement an encoder for the DCGAN algorithm and train our model with various hyperparameters along with the DCGAN baseline on the CelebA dataset. The detailed architecture of our regulated DCGAN is provided in Appendix B."}, {"heading": "4.2.1 MISSING MODES ESTIMATION ON CELEBA", "text": "To achieve this, we add the noise in the input layer of the discriminator network. For each estimated GAN model, we train this loud discriminator independently of each other as a mode estimator with the same architecture and hyperparameters on the generated data and training data. Then, we apply the mode estimator to the test data. The images, which have high mode estimator outputs, can still be considered to be in the missing mode.The comparison result is in Table 3. Both our proposed regularized GAN and MDGAN outperform the base models DCGAN on all settings. Specifically, MDGAN suppresses other models and shows its superiority in maintaining the modes.We also find that although we share the same architecture, the DCGAN performs significantly worse with 200-dimensional noise than the one with 100-dimensional secondary noise as input. On the contrary, our regulated GAN performs a better capture of the visual images through input."}, {"heading": "4.2.2 QUALITATIVE EVALUATION OF GENERATED SAMPLES", "text": "After quantitative evaluation, we manually examine the generated samples of our regulated GAN to see if the proposed regulator has adverse effects on sample quality. We compare our model with ALI (Dumoulin et al., 2016), VAEGAN (Larsen et al., 2015) and DCGAN (Radford et al., 2015) in terms of the visual quality and diversity of the samples. Samples from these models are shown in Figure 6. Both MDDGAN and Regularized-GAN produce clear and natural-looking facial images. Although the samples from ALI are plausible, they are significantly deformed compared to those from MDGAN. Samples from VAEGAN and DCGAN are insufficient due to insufficient sharpness. In terms of sample quality, it is worth noting that the samples from MDGAN exhibit less distortion."}, {"heading": "5 CONCLUSIONS", "text": "Although GANs are state-of-the-art in a variety of unattended learning tasks, they are considered highly unstable in training, very difficult and sensitive to hyperparameters, while missing modes in data distribution or even the breakdown of large amounts of probability mass in some modes. Successful GAN training typically requires large amounts of human and computational effort to refine the hyperparameters to stabilize the training and prevent collapse. Researchers typically rely on their own experience and published tricks and hyperparameters instead of systematic methods for training GAN. We offer systematic methods to measure and avoid the problem of missing modes and stabilize the training with the proposed autoencoder-based regulators. The basic idea is that some geometric metrics can provide more stable gradients than trained discriminators, and in combination with the encoder they can be used as regulators for training."}, {"heading": "ACKNOWLEDGEMENTS", "text": "We thank Naiyan Wang, Jianbo Ye, Yuchen Ding, Saboya Yang for their GPU support, Huiling Zhen for helpful discussions and Junbo Zhao for providing the details of web search attempts on his EBGAN model."}, {"heading": "A APPENDIX: PSEUDO CODE FOR MDGAN", "text": "In this appendix we give the detailed training of an MDGAN example, which we discuss in Section 3.3."}, {"heading": "B APPENDIX: ARCHITECTURE FOR EXPERIMENTS", "text": "We use similar architectures for Compositional MNIST and CelebA experiments. The architecture is based on that of DCGAN Radford et al. (2015). Apart from the discriminator and generator, which are identical to DCGAN, we add an encoder that represents the \"reverse\" order of the generator by reversing the order of the layers and replacing the unfolding layers with revolutionary layers. Particular attention must be paid to batch normalization layers. In DCGAN, there are batch normalization layers in both the generator and the discriminator. However, two classes of data go through the batch normalization layers in the generator. One comes from the sampled noise, the other from the encoder. In our implementation, we separate the batch statistics for these two classes of data in the generator, while the parameters of the BN layer are divided. In this way, the batch statistics of these types of batches cannot interfere with each other."}, {"heading": "C APPENDIX: ADDITIONAL SYNTHESIZED EXPERIMENTS", "text": "In order to demonstrate the effectiveness of the modular regulated GAN proposed in this paper, we train a very simple GAN architecture on synthesized 2D datasets following Metz et al. (2016). The data comes from a mixture of 6 Gaussians, with a standard derivative of 0.1. Gaussian mean lies around a radius 5 circle. The generator network has two hidden ReLU layers with 128 neurons. It generates 2D output samples from uniform 3D noise [0.1]. The discriminator consists of only one fully connected layer of ReLU neurons, which maps the 2D input to a real 1D number. Both networks are optimized with the Adam optimizer at the learning rate of 1e-4. In the regulated version, we choose \u03bb1 = \u03bb2 = 0.005. The comparison between the generator distribution from the standard GAN and our proposed regulated GAN is shown in Figure 9."}], "references": [{"title": "Deep generative image models using a laplacian pyramid of adversarial networks", "author": ["Emily L Denton", "Soumith Chintala", "Rob Fergus"], "venue": "In Advances in neural information processing systems,", "citeRegEx": "Denton et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Denton et al\\.", "year": 2015}, {"title": "Adversarial feature learning", "author": ["Jeff Donahue", "Philipp Kr\u00e4henb\u00fchl", "Trevor Darrell"], "venue": "arXiv preprint arXiv:1605.09782,", "citeRegEx": "Donahue et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Donahue et al\\.", "year": 2016}, {"title": "Generating images with perceptual similarity metrics based on deep networks", "author": ["Alexey Dosovitskiy", "Thomas Brox"], "venue": "arXiv preprint arXiv:1602.02644,", "citeRegEx": "Dosovitskiy and Brox.,? \\Q2016\\E", "shortCiteRegEx": "Dosovitskiy and Brox.", "year": 2016}, {"title": "Adversarially learned inference", "author": ["Vincent Dumoulin", "Ishmael Belghazi", "Ben Poole", "Alex Lamb", "Martin Arjovsky", "Olivier Mastropietro", "Aaron Courville"], "venue": "arXiv preprint arXiv:1606.00704,", "citeRegEx": "Dumoulin et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Dumoulin et al\\.", "year": 2016}, {"title": "Generative adversarial nets", "author": ["Ian Goodfellow", "Jean Pouget-Abadie", "Mehdi Mirza", "Bing Xu", "David Warde-Farley", "Sherjil Ozair", "Aaron Courville", "Yoshua Bengio"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Goodfellow et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Goodfellow et al\\.", "year": 2014}, {"title": "A Efros. Image-to-image translation with conditional adversarial networks", "author": ["Phillip Isola", "Jun-Yan Zhu", "Tinghui Zhou", "Alexei"], "venue": "arxiv,", "citeRegEx": "Isola et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Isola et al\\.", "year": 2016}, {"title": "Autoencoding beyond pixels using a learned similarity metric", "author": ["Anders Boesen Lindbo Larsen", "S\u00f8ren Kaae S\u00f8nderby", "Ole Winther"], "venue": "arXiv preprint arXiv:1512.09300,", "citeRegEx": "Larsen et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Larsen et al\\.", "year": 2015}, {"title": "Photo-realistic single image super-resolution using a generative adversarial network", "author": ["Christian Ledig", "Lucas Theis", "Ferenc Husz\u00e1r", "Jose Caballero", "Andrew Aitken", "Alykhan Tejani", "Johannes Totz", "Zehan Wang", "Wenzhe Shi"], "venue": "arXiv preprint arXiv:1609.04802,", "citeRegEx": "Ledig et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Ledig et al\\.", "year": 2016}, {"title": "Precomputed real-time texture synthesis with markovian generative adversarial networks", "author": ["Chuan Li", "Michael Wand"], "venue": "arXiv preprint arXiv:1604.04382,", "citeRegEx": "Li and Wand.,? \\Q2016\\E", "shortCiteRegEx": "Li and Wand.", "year": 2016}, {"title": "Deep multi-scale video prediction beyond mean square error", "author": ["Michael Mathieu", "Camille Couprie", "Yann LeCun"], "venue": "arXiv preprint arXiv:1511.05440,", "citeRegEx": "Mathieu et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Mathieu et al\\.", "year": 2015}, {"title": "Unrolled generative adversarial networks", "author": ["Luke Metz", "Ben Poole", "David Pfau", "Jascha Sohl-Dickstein"], "venue": "arXiv preprint arXiv:1611.02163,", "citeRegEx": "Metz et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Metz et al\\.", "year": 2016}, {"title": "Conditional generative adversarial nets", "author": ["Mehdi Mirza", "Simon Osindero"], "venue": "arXiv preprint arXiv:1411.1784,", "citeRegEx": "Mirza and Osindero.,? \\Q2014\\E", "shortCiteRegEx": "Mirza and Osindero.", "year": 2014}, {"title": "Plug & play generative networks: Conditional iterative generation of images in latent space", "author": ["Anh Nguyen", "Jason Yosinski", "Yoshua Bengio", "Alexey Dosovitskiy", "Jeff Clune"], "venue": "arXiv preprint arXiv:1612.00005,", "citeRegEx": "Nguyen et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Nguyen et al\\.", "year": 2016}, {"title": "Unsupervised representation learning with deep convolutional generative adversarial networks", "author": ["Alec Radford", "Luke Metz", "Soumith Chintala"], "venue": "arXiv preprint arXiv:1511.06434,", "citeRegEx": "Radford et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Radford et al\\.", "year": 2015}, {"title": "Generative adversarial text to image synthesis", "author": ["Scott Reed", "Zeynep Akata", "Xinchen Yan", "Lajanugen Logeswaran", "Bernt Schiele", "Honglak Lee"], "venue": "arXiv preprint arXiv:1605.05396,", "citeRegEx": "Reed et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Reed et al\\.", "year": 2016}, {"title": "Temporal generative adversarial nets", "author": ["Masaki Saito", "Eiichi Matsumoto"], "venue": "arXiv preprint arXiv:1611.06624,", "citeRegEx": "Saito and Matsumoto.,? \\Q2016\\E", "shortCiteRegEx": "Saito and Matsumoto.", "year": 2016}, {"title": "Improved techniques for training gans", "author": ["Tim Salimans", "Ian Goodfellow", "Wojciech Zaremba", "Vicki Cheung", "Alec Radford", "Xi Chen"], "venue": "arXiv preprint arXiv:1606.03498,", "citeRegEx": "Salimans et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Salimans et al\\.", "year": 2016}, {"title": "Amortised map inference for image super-resolution", "author": ["Casper Kaae S\u00f8nderby", "Jose Caballero", "Lucas Theis", "Wenzhe Shi", "Ferenc Husz\u00e1r"], "venue": "arXiv preprint arXiv:1610.04490,", "citeRegEx": "S\u00f8nderby et al\\.,? \\Q2016\\E", "shortCiteRegEx": "S\u00f8nderby et al\\.", "year": 2016}, {"title": "Generating videos with scene dynamics", "author": ["Carl Vondrick", "Hamed Pirsiavash", "Antonio Torralba"], "venue": "In Advances In Neural Information Processing Systems,", "citeRegEx": "Vondrick et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Vondrick et al\\.", "year": 2016}, {"title": "Generative image modeling using style and structure adversarial networks", "author": ["Xiaolong Wang", "Abhinav Gupta"], "venue": "In ECCV,", "citeRegEx": "Wang and Gupta.,? \\Q2016\\E", "shortCiteRegEx": "Wang and Gupta.", "year": 2016}, {"title": "Learning a probabilistic latent space of object shapes via 3d generative-adversarial modeling", "author": ["Jiajun Wu", "Chengkai Zhang", "Tianfan Xue", "William T Freeman", "Joshua B Tenenbaum"], "venue": "In Neural Information Processing Systems (NIPS),", "citeRegEx": "Wu et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Wu et al\\.", "year": 2016}, {"title": "Energy-based generative adversarial network", "author": ["Junbo Zhao", "Michael Mathieu", "Yann LeCun"], "venue": "arXiv preprint arXiv:1609.03126,", "citeRegEx": "Zhao et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Zhao et al\\.", "year": 2016}, {"title": "Learning temporal transformations from time-lapse videos", "author": ["Yipin Zhou", "Tamara L Berg"], "venue": "In European Conference on Computer Vision,", "citeRegEx": "Zhou and Berg.,? \\Q2016\\E", "shortCiteRegEx": "Zhou and Berg.", "year": 2016}, {"title": "Generative visual manipulation on the natural image manifold", "author": ["Jun-Yan Zhu", "Philipp Kr\u00e4henb\u00fchl", "Eli Shechtman", "Alexei A. Efros"], "venue": "In Proceedings of European Conference on Computer Vision (ECCV),", "citeRegEx": "Zhu et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Zhu et al\\.", "year": 2016}, {"title": "The data is sampled from a mixture of 6 Gaussians, with standard derivation of 0.1. The means of the Gaussians are placed around a circle with radius 5. The generator network has two ReLU hidden layers with 128 neurons. It generates 2D output samples from 3D uniform noise from [0,1]. The discriminator consists of only one fully connected layer of ReLU neurons, mapping the 2D input", "author": ["Metz"], "venue": "GAN architecture on synthesized 2D dataset,", "citeRegEx": "Metz,? \\Q2016\\E", "shortCiteRegEx": "Metz", "year": 2016}], "referenceMentions": [{"referenceID": 4, "context": "Generative adversarial networks (GAN) (Goodfellow et al., 2014) have demonstrated their potential on various tasks, such as image generation, image super-resolution, 3D object generation, and video prediction (Radford et al.", "startOffset": 38, "endOffset": 63}, {"referenceID": 13, "context": ", 2014) have demonstrated their potential on various tasks, such as image generation, image super-resolution, 3D object generation, and video prediction (Radford et al., 2015; Ledig et al., 2016; S\u00f8nderby et al., 2016; Nguyen et al., 2016; Wu et al., 2016; Mathieu et al., 2015).", "startOffset": 153, "endOffset": 278}, {"referenceID": 7, "context": ", 2014) have demonstrated their potential on various tasks, such as image generation, image super-resolution, 3D object generation, and video prediction (Radford et al., 2015; Ledig et al., 2016; S\u00f8nderby et al., 2016; Nguyen et al., 2016; Wu et al., 2016; Mathieu et al., 2015).", "startOffset": 153, "endOffset": 278}, {"referenceID": 17, "context": ", 2014) have demonstrated their potential on various tasks, such as image generation, image super-resolution, 3D object generation, and video prediction (Radford et al., 2015; Ledig et al., 2016; S\u00f8nderby et al., 2016; Nguyen et al., 2016; Wu et al., 2016; Mathieu et al., 2015).", "startOffset": 153, "endOffset": 278}, {"referenceID": 12, "context": ", 2014) have demonstrated their potential on various tasks, such as image generation, image super-resolution, 3D object generation, and video prediction (Radford et al., 2015; Ledig et al., 2016; S\u00f8nderby et al., 2016; Nguyen et al., 2016; Wu et al., 2016; Mathieu et al., 2015).", "startOffset": 153, "endOffset": 278}, {"referenceID": 20, "context": ", 2014) have demonstrated their potential on various tasks, such as image generation, image super-resolution, 3D object generation, and video prediction (Radford et al., 2015; Ledig et al., 2016; S\u00f8nderby et al., 2016; Nguyen et al., 2016; Wu et al., 2016; Mathieu et al., 2015).", "startOffset": 153, "endOffset": 278}, {"referenceID": 9, "context": ", 2014) have demonstrated their potential on various tasks, such as image generation, image super-resolution, 3D object generation, and video prediction (Radford et al., 2015; Ledig et al., 2016; S\u00f8nderby et al., 2016; Nguyen et al., 2016; Wu et al., 2016; Mathieu et al., 2015).", "startOffset": 153, "endOffset": 278}, {"referenceID": 3, "context": "The GAN approach was initially proposed by Goodfellow et al. (2014) where both the generator and the discriminator are defined by deep neural networks.", "startOffset": 43, "endOffset": 68}, {"referenceID": 3, "context": "The GAN approach was initially proposed by Goodfellow et al. (2014) where both the generator and the discriminator are defined by deep neural networks. In Goodfellow et al. (2014), the GAN is able to generate interesting local structure but globally incoherent images on various datasets.", "startOffset": 43, "endOffset": 180}, {"referenceID": 3, "context": "The GAN approach was initially proposed by Goodfellow et al. (2014) where both the generator and the discriminator are defined by deep neural networks. In Goodfellow et al. (2014), the GAN is able to generate interesting local structure but globally incoherent images on various datasets. Mirza & Osindero (2014) enlarges GAN\u2019s representation capacity by introducing an extra vector to allow the generator to produce samples conditioned on other beneficial information.", "startOffset": 43, "endOffset": 313}, {"referenceID": 3, "context": "The GAN approach was initially proposed by Goodfellow et al. (2014) where both the generator and the discriminator are defined by deep neural networks. In Goodfellow et al. (2014), the GAN is able to generate interesting local structure but globally incoherent images on various datasets. Mirza & Osindero (2014) enlarges GAN\u2019s representation capacity by introducing an extra vector to allow the generator to produce samples conditioned on other beneficial information. Motivated from this, several conditional variants of GAN has been applied to a wide range of tasks, including image prediction from a normal map Wang & Gupta (2016), image synthesis from text Reed et al.", "startOffset": 43, "endOffset": 635}, {"referenceID": 3, "context": "The GAN approach was initially proposed by Goodfellow et al. (2014) where both the generator and the discriminator are defined by deep neural networks. In Goodfellow et al. (2014), the GAN is able to generate interesting local structure but globally incoherent images on various datasets. Mirza & Osindero (2014) enlarges GAN\u2019s representation capacity by introducing an extra vector to allow the generator to produce samples conditioned on other beneficial information. Motivated from this, several conditional variants of GAN has been applied to a wide range of tasks, including image prediction from a normal map Wang & Gupta (2016), image synthesis from text Reed et al. (2016) and edge map Isola et al.", "startOffset": 43, "endOffset": 681}, {"referenceID": 3, "context": "The GAN approach was initially proposed by Goodfellow et al. (2014) where both the generator and the discriminator are defined by deep neural networks. In Goodfellow et al. (2014), the GAN is able to generate interesting local structure but globally incoherent images on various datasets. Mirza & Osindero (2014) enlarges GAN\u2019s representation capacity by introducing an extra vector to allow the generator to produce samples conditioned on other beneficial information. Motivated from this, several conditional variants of GAN has been applied to a wide range of tasks, including image prediction from a normal map Wang & Gupta (2016), image synthesis from text Reed et al. (2016) and edge map Isola et al. (2016), real-time image manipulation Zhu et al.", "startOffset": 43, "endOffset": 714}, {"referenceID": 3, "context": "The GAN approach was initially proposed by Goodfellow et al. (2014) where both the generator and the discriminator are defined by deep neural networks. In Goodfellow et al. (2014), the GAN is able to generate interesting local structure but globally incoherent images on various datasets. Mirza & Osindero (2014) enlarges GAN\u2019s representation capacity by introducing an extra vector to allow the generator to produce samples conditioned on other beneficial information. Motivated from this, several conditional variants of GAN has been applied to a wide range of tasks, including image prediction from a normal map Wang & Gupta (2016), image synthesis from text Reed et al. (2016) and edge map Isola et al. (2016), real-time image manipulation Zhu et al. (2016), temporal image generation Zhou & Berg (2016); Saito & Matsumoto (2016); Vondrick et al.", "startOffset": 43, "endOffset": 762}, {"referenceID": 3, "context": "The GAN approach was initially proposed by Goodfellow et al. (2014) where both the generator and the discriminator are defined by deep neural networks. In Goodfellow et al. (2014), the GAN is able to generate interesting local structure but globally incoherent images on various datasets. Mirza & Osindero (2014) enlarges GAN\u2019s representation capacity by introducing an extra vector to allow the generator to produce samples conditioned on other beneficial information. Motivated from this, several conditional variants of GAN has been applied to a wide range of tasks, including image prediction from a normal map Wang & Gupta (2016), image synthesis from text Reed et al. (2016) and edge map Isola et al. (2016), real-time image manipulation Zhu et al. (2016), temporal image generation Zhou & Berg (2016); Saito & Matsumoto (2016); Vondrick et al.", "startOffset": 43, "endOffset": 808}, {"referenceID": 3, "context": "The GAN approach was initially proposed by Goodfellow et al. (2014) where both the generator and the discriminator are defined by deep neural networks. In Goodfellow et al. (2014), the GAN is able to generate interesting local structure but globally incoherent images on various datasets. Mirza & Osindero (2014) enlarges GAN\u2019s representation capacity by introducing an extra vector to allow the generator to produce samples conditioned on other beneficial information. Motivated from this, several conditional variants of GAN has been applied to a wide range of tasks, including image prediction from a normal map Wang & Gupta (2016), image synthesis from text Reed et al. (2016) and edge map Isola et al. (2016), real-time image manipulation Zhu et al. (2016), temporal image generation Zhou & Berg (2016); Saito & Matsumoto (2016); Vondrick et al.", "startOffset": 43, "endOffset": 834}, {"referenceID": 3, "context": "The GAN approach was initially proposed by Goodfellow et al. (2014) where both the generator and the discriminator are defined by deep neural networks. In Goodfellow et al. (2014), the GAN is able to generate interesting local structure but globally incoherent images on various datasets. Mirza & Osindero (2014) enlarges GAN\u2019s representation capacity by introducing an extra vector to allow the generator to produce samples conditioned on other beneficial information. Motivated from this, several conditional variants of GAN has been applied to a wide range of tasks, including image prediction from a normal map Wang & Gupta (2016), image synthesis from text Reed et al. (2016) and edge map Isola et al. (2016), real-time image manipulation Zhu et al. (2016), temporal image generation Zhou & Berg (2016); Saito & Matsumoto (2016); Vondrick et al. (2016), texture synthesis, style transfer, and video stylization Li & Wand (2016).", "startOffset": 43, "endOffset": 858}, {"referenceID": 3, "context": "The GAN approach was initially proposed by Goodfellow et al. (2014) where both the generator and the discriminator are defined by deep neural networks. In Goodfellow et al. (2014), the GAN is able to generate interesting local structure but globally incoherent images on various datasets. Mirza & Osindero (2014) enlarges GAN\u2019s representation capacity by introducing an extra vector to allow the generator to produce samples conditioned on other beneficial information. Motivated from this, several conditional variants of GAN has been applied to a wide range of tasks, including image prediction from a normal map Wang & Gupta (2016), image synthesis from text Reed et al. (2016) and edge map Isola et al. (2016), real-time image manipulation Zhu et al. (2016), temporal image generation Zhou & Berg (2016); Saito & Matsumoto (2016); Vondrick et al. (2016), texture synthesis, style transfer, and video stylization Li & Wand (2016). Researchers also aim at stretching GAN\u2019s limit to generate higher-resolution, photo-realistic images.", "startOffset": 43, "endOffset": 933}, {"referenceID": 0, "context": "Denton et al. (2015) initially apply a Laplacian pyramid framework on GAN to generate images of high resolution.", "startOffset": 0, "endOffset": 21}, {"referenceID": 0, "context": "Denton et al. (2015) initially apply a Laplacian pyramid framework on GAN to generate images of high resolution. At each level of their LAPGAN, both the generator and the discriminator are convolutional networks. As an alternative to LAPGAN, Radford et al. (2015) successfully designs a class of deep convolutional generative adversarial networks which has led to significant improvements on unsupervised image representation learning.", "startOffset": 0, "endOffset": 264}, {"referenceID": 0, "context": "Denton et al. (2015) initially apply a Laplacian pyramid framework on GAN to generate images of high resolution. At each level of their LAPGAN, both the generator and the discriminator are convolutional networks. As an alternative to LAPGAN, Radford et al. (2015) successfully designs a class of deep convolutional generative adversarial networks which has led to significant improvements on unsupervised image representation learning. Another line of work aimed at improving GANs are through feature learning, including features from the latent space and image space. The motivation is that features from different spaces are complementary for generating perceptual and natural-looking images. With this perspective, some researchers use distances between learned features as losses for training objectives for generative models. Larsen et al. (2015) combine a variational autoencoder objective with a GAN and utilize the learned features from the discriminator in the GANs for better image similarity metrics.", "startOffset": 0, "endOffset": 852}, {"referenceID": 0, "context": "Denton et al. (2015) initially apply a Laplacian pyramid framework on GAN to generate images of high resolution. At each level of their LAPGAN, both the generator and the discriminator are convolutional networks. As an alternative to LAPGAN, Radford et al. (2015) successfully designs a class of deep convolutional generative adversarial networks which has led to significant improvements on unsupervised image representation learning. Another line of work aimed at improving GANs are through feature learning, including features from the latent space and image space. The motivation is that features from different spaces are complementary for generating perceptual and natural-looking images. With this perspective, some researchers use distances between learned features as losses for training objectives for generative models. Larsen et al. (2015) combine a variational autoencoder objective with a GAN and utilize the learned features from the discriminator in the GANs for better image similarity metrics. It is shown that the learned distance from the discriminator is of great help for the sample visual fidelity. Recent literature have also shown impressive results on image super-resolution to infer photo-realistic natural images for 4x upscaling factors Ledig et al. (2016); S\u00f8nderby et al.", "startOffset": 0, "endOffset": 1286}, {"referenceID": 0, "context": "Denton et al. (2015) initially apply a Laplacian pyramid framework on GAN to generate images of high resolution. At each level of their LAPGAN, both the generator and the discriminator are convolutional networks. As an alternative to LAPGAN, Radford et al. (2015) successfully designs a class of deep convolutional generative adversarial networks which has led to significant improvements on unsupervised image representation learning. Another line of work aimed at improving GANs are through feature learning, including features from the latent space and image space. The motivation is that features from different spaces are complementary for generating perceptual and natural-looking images. With this perspective, some researchers use distances between learned features as losses for training objectives for generative models. Larsen et al. (2015) combine a variational autoencoder objective with a GAN and utilize the learned features from the discriminator in the GANs for better image similarity metrics. It is shown that the learned distance from the discriminator is of great help for the sample visual fidelity. Recent literature have also shown impressive results on image super-resolution to infer photo-realistic natural images for 4x upscaling factors Ledig et al. (2016); S\u00f8nderby et al. (2016); Nguyen et al.", "startOffset": 0, "endOffset": 1310}, {"referenceID": 0, "context": "Denton et al. (2015) initially apply a Laplacian pyramid framework on GAN to generate images of high resolution. At each level of their LAPGAN, both the generator and the discriminator are convolutional networks. As an alternative to LAPGAN, Radford et al. (2015) successfully designs a class of deep convolutional generative adversarial networks which has led to significant improvements on unsupervised image representation learning. Another line of work aimed at improving GANs are through feature learning, including features from the latent space and image space. The motivation is that features from different spaces are complementary for generating perceptual and natural-looking images. With this perspective, some researchers use distances between learned features as losses for training objectives for generative models. Larsen et al. (2015) combine a variational autoencoder objective with a GAN and utilize the learned features from the discriminator in the GANs for better image similarity metrics. It is shown that the learned distance from the discriminator is of great help for the sample visual fidelity. Recent literature have also shown impressive results on image super-resolution to infer photo-realistic natural images for 4x upscaling factors Ledig et al. (2016); S\u00f8nderby et al. (2016); Nguyen et al. (2016). Despite these promising successes, GANs are notably hard to train.", "startOffset": 0, "endOffset": 1332}, {"referenceID": 0, "context": "Denton et al. (2015) initially apply a Laplacian pyramid framework on GAN to generate images of high resolution. At each level of their LAPGAN, both the generator and the discriminator are convolutional networks. As an alternative to LAPGAN, Radford et al. (2015) successfully designs a class of deep convolutional generative adversarial networks which has led to significant improvements on unsupervised image representation learning. Another line of work aimed at improving GANs are through feature learning, including features from the latent space and image space. The motivation is that features from different spaces are complementary for generating perceptual and natural-looking images. With this perspective, some researchers use distances between learned features as losses for training objectives for generative models. Larsen et al. (2015) combine a variational autoencoder objective with a GAN and utilize the learned features from the discriminator in the GANs for better image similarity metrics. It is shown that the learned distance from the discriminator is of great help for the sample visual fidelity. Recent literature have also shown impressive results on image super-resolution to infer photo-realistic natural images for 4x upscaling factors Ledig et al. (2016); S\u00f8nderby et al. (2016); Nguyen et al. (2016). Despite these promising successes, GANs are notably hard to train. Although Radford et al. (2015) provide a class of empirical architectural choices that are critical to stabilize GAN\u2019s training, it would be even better to train GANs more robustly and systematically.", "startOffset": 0, "endOffset": 1431}, {"referenceID": 0, "context": "Denton et al. (2015) initially apply a Laplacian pyramid framework on GAN to generate images of high resolution. At each level of their LAPGAN, both the generator and the discriminator are convolutional networks. As an alternative to LAPGAN, Radford et al. (2015) successfully designs a class of deep convolutional generative adversarial networks which has led to significant improvements on unsupervised image representation learning. Another line of work aimed at improving GANs are through feature learning, including features from the latent space and image space. The motivation is that features from different spaces are complementary for generating perceptual and natural-looking images. With this perspective, some researchers use distances between learned features as losses for training objectives for generative models. Larsen et al. (2015) combine a variational autoencoder objective with a GAN and utilize the learned features from the discriminator in the GANs for better image similarity metrics. It is shown that the learned distance from the discriminator is of great help for the sample visual fidelity. Recent literature have also shown impressive results on image super-resolution to infer photo-realistic natural images for 4x upscaling factors Ledig et al. (2016); S\u00f8nderby et al. (2016); Nguyen et al. (2016). Despite these promising successes, GANs are notably hard to train. Although Radford et al. (2015) provide a class of empirical architectural choices that are critical to stabilize GAN\u2019s training, it would be even better to train GANs more robustly and systematically. Salimans et al. (2016) propose feature matching technique to stabilize GAN\u2019s training.", "startOffset": 0, "endOffset": 1624}, {"referenceID": 0, "context": "Denton et al. (2015) initially apply a Laplacian pyramid framework on GAN to generate images of high resolution. At each level of their LAPGAN, both the generator and the discriminator are convolutional networks. As an alternative to LAPGAN, Radford et al. (2015) successfully designs a class of deep convolutional generative adversarial networks which has led to significant improvements on unsupervised image representation learning. Another line of work aimed at improving GANs are through feature learning, including features from the latent space and image space. The motivation is that features from different spaces are complementary for generating perceptual and natural-looking images. With this perspective, some researchers use distances between learned features as losses for training objectives for generative models. Larsen et al. (2015) combine a variational autoencoder objective with a GAN and utilize the learned features from the discriminator in the GANs for better image similarity metrics. It is shown that the learned distance from the discriminator is of great help for the sample visual fidelity. Recent literature have also shown impressive results on image super-resolution to infer photo-realistic natural images for 4x upscaling factors Ledig et al. (2016); S\u00f8nderby et al. (2016); Nguyen et al. (2016). Despite these promising successes, GANs are notably hard to train. Although Radford et al. (2015) provide a class of empirical architectural choices that are critical to stabilize GAN\u2019s training, it would be even better to train GANs more robustly and systematically. Salimans et al. (2016) propose feature matching technique to stabilize GAN\u2019s training. The generator is required to match the statistics of intermediate features of the discriminator. Similar idea is adopted by Zhao et al. (2016).", "startOffset": 0, "endOffset": 1831}, {"referenceID": 3, "context": "Furthermore, some researchers make use of information in both spaces in a unified learning procedure (Dumoulin et al., 2016; Donahue et al., 2016).", "startOffset": 101, "endOffset": 146}, {"referenceID": 1, "context": "Furthermore, some researchers make use of information in both spaces in a unified learning procedure (Dumoulin et al., 2016; Donahue et al., 2016).", "startOffset": 101, "endOffset": 146}, {"referenceID": 6, "context": "Our work is related to VAEGAN (Larsen et al., 2015) in terms of training an autoencoder or VAE jointly with the GAN model.", "startOffset": 30, "endOffset": 51}, {"referenceID": 1, "context": ", 2016; Donahue et al., 2016). In Dumoulin et al. (2016), one trains not just a generator but also an encoder, and the discriminator is trained to distinguish between two joint distributions over image and latent spaces produced either by the application of the encoder on the training data or by the application of the generator (decoder) to the latent prior.", "startOffset": 8, "endOffset": 57}, {"referenceID": 1, "context": ", 2016; Donahue et al., 2016). In Dumoulin et al. (2016), one trains not just a generator but also an encoder, and the discriminator is trained to distinguish between two joint distributions over image and latent spaces produced either by the application of the encoder on the training data or by the application of the generator (decoder) to the latent prior. This is in contrast with the regular GAN training, in which the discriminator only attempts to separate the distributions in the image space. Parallelly, Metz et al. (2016) stabilize GANs by unrolling the optimization of discriminator, which can be considered as an orthogonal work with ours.", "startOffset": 8, "endOffset": 534}, {"referenceID": 3, "context": "As pointed out by Goodfellow et al. (2014); Denton et al.", "startOffset": 18, "endOffset": 43}, {"referenceID": 0, "context": "(2014); Denton et al. (2015); Radford et al.", "startOffset": 8, "endOffset": 29}, {"referenceID": 0, "context": "(2014); Denton et al. (2015); Radford et al. (2015), when the data manifold and the generation manifold are disjoint (which is true in almost all practical situations), it is equivalent to training a characteristic function to be very close to 1 on the data manifold, and 0 on the generation manifold.", "startOffset": 8, "endOffset": 52}, {"referenceID": 0, "context": "(2014); Denton et al. (2015); Radford et al. (2015), when the data manifold and the generation manifold are disjoint (which is true in almost all practical situations), it is equivalent to training a characteristic function to be very close to 1 on the data manifold, and 0 on the generation manifold. In order to pass good gradient information to the generator, it is important that the trained discriminator produces stable and smooth gradients. However, since the discriminator objective does not directly depend on the behavior of the discriminator in other parts of the space, training can easily fail if the shape of the discriminator function is not as expected. As an example,Denton et al. (2015) noted a common failure pattern for training GANs which is the vanishing gradient problem, in which the discriminator D perfectly classifies real and fake examples, such that around the fake examples, D is nearly zero.", "startOffset": 8, "endOffset": 705}, {"referenceID": 0, "context": "This problem exists even when we use logD(G(z)) as target for the generator, as noted by Denton et al. (2015) and our experiments.", "startOffset": 89, "endOffset": 110}, {"referenceID": 3, "context": "For instance, the pixel-wise L distance, or the distance of learned features by the discriminator (Dumoulin et al., 2016) or by other networks, such as a VGG classifier.", "startOffset": 98, "endOffset": 121}, {"referenceID": 7, "context": "(Ledig et al., 2016) The geometric intuition for this regularizer is straight-forward.", "startOffset": 0, "endOffset": 20}, {"referenceID": 16, "context": "The inception score (Salimans et al., 2016) was considered as a good assessment for sample quality from a labelled dataset:", "startOffset": 20, "endOffset": 43}, {"referenceID": 4, "context": "We can view the output of the discriminator as an estimator for the quantity (See (Goodfellow et al., 2014) for proof):", "startOffset": 82, "endOffset": 107}, {"referenceID": 21, "context": "Our grid search is similar to those proposed in Zhao et al. (2016). Please refer to it for detailed explanations regarding these hyper-parameters.", "startOffset": 48, "endOffset": 67}, {"referenceID": 3, "context": "We compare our model with ALI (Dumoulin et al., 2016), VAEGAN (Larsen et al.", "startOffset": 30, "endOffset": 53}, {"referenceID": 6, "context": ", 2016), VAEGAN (Larsen et al., 2015), and DCGAN (Radford et al.", "startOffset": 16, "endOffset": 37}, {"referenceID": 13, "context": ", 2015), and DCGAN (Radford et al., 2015) in terms of sample visual quality and mode diversity.", "startOffset": 19, "endOffset": 41}], "year": 2017, "abstractText": "Although Generative Adversarial Networks achieve state-of-the-art results on a variety of generative tasks, they are regarded as highly unstable and prone to miss modes. We argue that these bad behaviors of GANs are due to the very particular functional shape of the trained discriminators in high dimensional spaces, which can easily make training stuck or push probability mass in the wrong direction, towards that of higher concentration than that of the data generating distribution. We introduce several ways of regularizing the objective, which can dramatically stabilize the training of GAN models. We also show that our regularizers can help the fair distribution of probability mass across the modes of the data generating distribution, during the early phases of training and thus providing a unified solution to the missing modes problem.", "creator": "LaTeX with hyperref package"}}}