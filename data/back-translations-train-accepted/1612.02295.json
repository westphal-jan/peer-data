{"id": "1612.02295", "review": {"conference": "ICML", "VERSION": "v1", "DATE_OF_SUBMISSION": "7-Dec-2016", "title": "Large-Margin Softmax Loss for Convolutional Neural Networks", "abstract": "Cross-entropy loss together with softmax is arguably one of the most common used supervision components in convolutional neural networks (CNNs). Despite its simplicity, popularity and excellent performance, the component does not explicitly encourage discriminative learning of features. In this paper, we propose a generalized large-margin softmax (L-Softmax) loss which explicitly encourages intra-class compactness and inter-class separability between learned features. Moreover, L-Softmax not only can adjust the desired margin but also can avoid overfitting. We also show that the L-Softmax loss can be optimized by typical stochastic gradient descent. Extensive experiments on four benchmark datasets demonstrate that the deeply-learned features with L-softmax loss become more discriminative, hence significantly boosting the performance on a variety of visual classification and verification tasks.", "histories": [["v1", "Wed, 7 Dec 2016 15:36:11 GMT  (2423kb,D)", "http://arxiv.org/abs/1612.02295v1", "Published in ICML 2016. Revised some typos"], ["v2", "Tue, 17 Jan 2017 07:28:18 GMT  (2423kb,D)", "http://arxiv.org/abs/1612.02295v2", "Published in ICML 2016. Revised some typos"], ["v3", "Mon, 23 Jan 2017 08:32:08 GMT  (2423kb,D)", "http://arxiv.org/abs/1612.02295v3", "Fixed some typos"]], "COMMENTS": "Published in ICML 2016. Revised some typos", "reviews": [], "SUBJECTS": "stat.ML cs.LG", "authors": ["weiyang liu", "yandong wen", "zhiding yu", "meng yang"], "accepted": true, "id": "1612.02295"}, "pdf": {"name": "1612.02295.pdf", "metadata": {"source": "META", "title": "Large-Margin Softmax Loss for Convolutional Neural Networks", "authors": ["Weiyang Liu", "Yandong Wen", "Zhiding Yu", "Meng Yang"], "emails": ["WYLIU@PKU.EDU.CN", "WEN.YANDONG@MAIL.SCUT.EDU.CN", "YZHIDING@ANDREW.CMU.EDU", "YANG.MENG@SZU.EDU.CN"], "sections": [{"heading": "1. Introduction", "text": "In recent years, there has been a significant increase in the number of people opting for training, both in terms of quality and in terms of the quality and quality of training, as well as in the way in which participants take part in training, and in the way in which they take part in training. (Do you look at what they are interested in training?) This also applies to the way in which participants take part in training. (Do you look at what they are interested in training?) This also applies to the way in which they take part in training. (See what they are interested in training.) This applies not only to the way in which they work, but also to the way in which they look at themselves. (See yourself) This applies to the way in which they work, in which they work and in the way in which they work. (See what they think) This applies to the way in which they work, in which they work and in the way in which they work."}, {"heading": "2. Related Work and Preliminaries", "text": "Current, widely used data loss functions in CNNs include euclidean loss, (square) hinge loss, information gain loss, contrast loss, triple loss, softmax loss, etc. To improve class-internal compactness and separability between classes, (Sun et al., 2014) CNN trains using the combination of softmax loss and contrast loss. Contrastive loss requires their removal to be greater than a margin. (Schroff et al., 2015) uses triplet loss to promote a distance restriction similar to contrastive loss. Triplet loss requires 3 (or multiples of 3) training samples as input at a specified time. Triplet loss minimizes the distance between an anchor sample and a sample (of the same identity) and the distance between the sample."}, {"heading": "3. Large-Margin Softmax Loss", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "3.1. Intuition", "text": "s look at the binary classification and we have a sample x from class 1. The original Softmax is to force W T1 x > W T 2 x (i.e. W1, W2, W2, x x x x x cos) to classify x correctly. However, we want to make the classification stricter to generate a decision margin, so instead we demand that W1, W1, x x x cos (m2) > W2, x cos (m2) > W2, x cos (2, 0) (0, 2, where m is a positive integer, because the following inequalities apply: W1, W2, x (formerly 1), W1, x cos (m2) > W2, x cos (formerly 1) > W2, x cos (formerly 2)."}, {"heading": "3.2. Definition", "text": "After the notation in the precursors, the L-Softmax loss is defined as Li = \u2212 log (e), in which we generally have the value of m (\u03b8) = cos (m\u03b8), 0 \u2264 270 m D (\u03b8), \u03c0m < \u03b8 (5), where m is an integer closely related to the classification span. With a greater m, the classification span becomes larger and the learning goal also becomes more difficult. Meanwhile, D (\u03b8) must be a monotonously decreasing function, and D (zipm) should be equal to cos (\u03c0 m). To simplify forward and backward propagation, we construct in this paper a specific combination of numbers (\u03b8i):"}, {"heading": "3.3. Geometric Interpretation", "text": "To simplify the geometric interpretation, we analyze the case of binary classification, in which there is only W1 and W2. First, we consider the scenario of the original Softmax losses, as in Fig. 4. With Fig. 4, while the result of the classification depends entirely on the angles between x and W1 (W2). In the training phase, the original Softmax loss requires a difference between 1 < 2 to classify the sample x as Class 1, while the loss of L-Softmax losses requires the same decision < 2 to make the same decision. We can see that the L-Softmax loss is stricter than the classification criteria, which results in a difference between Class 1 and Class 2. If we assume that both Softmax losses and L-Softmax losses are optimized to the same value and all training features can be perfectly classified, then the angle distance between Class 1 and Class 2 will be optimized."}, {"heading": "3.4. Discussion", "text": "By assigning different values for m, we define a flexible learning task with an adjustable degree of difficulty for CNNs. L-Softmax loss is equipped with some nice features, such as \u2022 L-Softmax loss has a clear geometric interpretation. \u2022 L-Softmax loss controls the margin between classes. \u2022 L-Softmax loss increases the ideal margin between classes with greater m (with the same loss of training) and the learning difficulty also increases. At m = 1, L-Softmax loss becomes identical to the original Softmax loss. \u2022 L-Softmax loss defines a relatively difficult learning goal with adjustable margin (difficulty). A difficult learning goal can effectively avoid overadjustment and take full advantage of the strong learning ability from deep and wide architectures. \u2022 L-Softmax loss can be used as a drop-in replacement for the standard loss function and can also be activated together with other performance-enhancing ap-1asible angles of the function i."}, {"heading": "4. Optimization", "text": "It is easy to calculate forward and backward propagation for L-Softmax loss, so it is also trivial to optimize L-Softmax loss using a typical stochastic gradient. Li says the only difference between the original Softmax loss and L-Softmax loss is in Fyi. (6) and Eq. (7), fyi is calculated only in forward and backward propagation, while fj, j \u2212 6 = yi is the same difference as the original Softmax loss. (6) and Eq. (7), fyi is written asfyi: (\u2212 1) k \u00b7 Wyxi \u2212 1 Tyxi \u2212 Tyxi m: (2) Tyxi \u2212 Tyxi m: (2) Tyxi \u2212 Tyxi m: (1) and Tyxi m: (2) Tyxi \u2212 Tyxi m: (2)."}, {"heading": "5. Experiments and Results", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "5.1. Experimental Settings", "text": "In visual classification, we use three standard benchmark datasets: MNIST (LeCun et al., 1998), CIFAR10 (Krizhevsky, 2009), and CIFAR100 (Krizhevsky, 2009). In facial verification, we use our method for the widely used LFW datasets (Huang et al., 2007). We use only one model in all baseline CNNs to compare our performance. For convenience, we use L-Softmax to denote L-Softmax loss, both Softmax and L-Softmax in the experiments."}, {"heading": "5.2. Visual Classification", "text": "The results show that the problem is not only a problem, but also an error compared to other deep CNN architectures. In Figure 2, we will also visualize the learned features using the L-Softmax loss and compare them with the original Softmax loss. Figure 2 shows the effectiveness of the large marginality within the L-Softmax loss. In fact, we get a larger analog decision than the others. CIFAR10: We use two common comparison protocols in CIFAR10 datasets. We do not compare our L-Softmax losses under any circumstances at first. For the data augmentation experiment, we follow the standard data augmentation in (Lee et al., 2015)"}, {"heading": "5.3. Face Verification", "text": "In order to further evaluate the learned features, we conduct an experiment with the famous LFW dataset (Huang et al., 2007), which collects 13,233 facial images of 5,749 people from uncontrolled conditions. Following the unrestricted labeled external data protocols (Huang et al., 2007), we train on the publicly available CASIA WebFace dataset (Yi et al., 2014) outside the dataset (490k labeled facial images of over 10,000 people) and test the 6,000 face pairs on LFW. Overlapping between the external training data and the LFW test data is excluded. As pre-processing, we use IntraFace (Asthana et al., 2014) to align the facial images and then cut them based on 5 points. Afterwards, we train a single network for feature extraction, so that we only compare the individual model performance of modern NFace's compact CNA to form a pative vector."}, {"heading": "6. Concluding Remarks", "text": "We proposed the Large Margin Softmax Loss for the Convolutionary Neural Networks. Large Margin Softmax Loss defines a flexible learning task with adjustable leeway. We can set the parameter m to control the leeway. As the m increases, the leeway between classes increases. What's more attractive is that the Large Margin Softmax Loss has a very clear intuition and geometric interpretation. Extensive experimental results on multiple benchmark datasets show clear advantages over current CNNs and all comparable baselines."}], "references": [{"title": "Incremental face alignment in the wild", "author": ["Asthana", "Akshay", "Zafeiriou", "Stefanos", "Cheng", "Shiyang", "Pantic", "Maja"], "venue": "In CVPR,", "citeRegEx": "Asthana et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Asthana et al\\.", "year": 2014}, {"title": "Robust face recognition via multimodal deep face representation", "author": ["Ding", "Changxing", "Tao", "Dacheng"], "venue": "IEEE TMM,", "citeRegEx": "Ding et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Ding et al\\.", "year": 2015}, {"title": "Dimensionality reduction by learning an invariant mapping", "author": ["Hadsell", "Raia", "Chopra", "Sumit", "LeCun", "Yann"], "venue": "In CVPR,", "citeRegEx": "Hadsell et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Hadsell et al\\.", "year": 2006}, {"title": "Deep residual learning for image recognition", "author": ["He", "Kaiming", "Zhang", "Xiangyu", "Ren", "Shaoqing", "Sun", "Jian"], "venue": "arXiv preprint arXiv:1512.03385,", "citeRegEx": "He et al\\.,? \\Q2015\\E", "shortCiteRegEx": "He et al\\.", "year": 2015}, {"title": "Delving deep into rectifiers: Surpassing human-level performance on imagenet classification", "author": ["He", "Kaiming", "Zhang", "Xiangyu", "Ren", "Shaoqing", "Sun", "Jian"], "venue": "In ICCV,", "citeRegEx": "He et al\\.,? \\Q2015\\E", "shortCiteRegEx": "He et al\\.", "year": 2015}, {"title": "Improving neural networks by preventing co-adaptation of feature detectors", "author": ["Hinton", "Geoffrey E", "Srivastava", "Nitish", "Krizhevsky", "Alex", "Sutskever", "Ilya", "Salakhutdinov", "Ruslan R"], "venue": "arXiv preprint arXiv:1207.0580,", "citeRegEx": "Hinton et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Hinton et al\\.", "year": 2012}, {"title": "Labeled faces in the wild: A database for studying face recognition in unconstrained environments", "author": ["Huang", "Gary B", "Ramesh", "Manu", "Berg", "Tamara", "LearnedMiller", "Erik"], "venue": null, "citeRegEx": "Huang et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Huang et al\\.", "year": 2007}, {"title": "Batch normalization: Accelerating deep network training by reducing internal covariate shift", "author": ["Ioffe", "Sergey", "Szegedy", "Christian"], "venue": "In ICML,", "citeRegEx": "Ioffe et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Ioffe et al\\.", "year": 2015}, {"title": "What is the best multi-stage architecture for object recognition", "author": ["Jarrett", "Kevin", "Kavukcuoglu", "Koray", "Ranzato", "Marc\u2019Aurelio", "LeCun", "Yann"], "venue": "In ICCV,", "citeRegEx": "Jarrett et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Jarrett et al\\.", "year": 2009}, {"title": "Caffe: Convolutional architecture for fast feature embedding", "author": ["Jia", "Yangqing", "Shelhamer", "Evan", "Donahue", "Jeff", "Karayev", "Sergey", "Long", "Jonathan", "Girshick", "Ross", "Guadarrama", "Sergio", "Darrell", "Trevor"], "venue": "arXiv preprint arXiv:1408.5093,", "citeRegEx": "Jia et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Jia et al\\.", "year": 2014}, {"title": "Learning multiple layers of features from tiny images", "author": ["Krizhevsky", "Alex"], "venue": "Technical Report,", "citeRegEx": "Krizhevsky and Alex.,? \\Q2009\\E", "shortCiteRegEx": "Krizhevsky and Alex.", "year": 2009}, {"title": "Imagenet classification with deep convolutional neural networks", "author": ["Krizhevsky", "Alex", "Sutskever", "Ilya", "Hinton", "Geoffrey E"], "venue": "In NIPS,", "citeRegEx": "Krizhevsky et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Krizhevsky et al\\.", "year": 2012}, {"title": "The mnist database of handwritten digits", "author": ["LeCun", "Yann", "Cortes", "Corinna", "Burges", "Christopher JC"], "venue": null, "citeRegEx": "LeCun et al\\.,? \\Q1998\\E", "shortCiteRegEx": "LeCun et al\\.", "year": 1998}, {"title": "Generalizing pooling functions in convolutional neural networks: Mixed, gated, and tree", "author": ["Lee", "Chen-Yu", "Gallagher", "Patrick W", "Tu", "Zhuowen"], "venue": null, "citeRegEx": "Lee et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Lee et al\\.", "year": 2016}, {"title": "Recurrent convolutional neural network for object recognition", "author": ["Liang", "Ming", "Hu", "Xiaolin"], "venue": "In CVPR,", "citeRegEx": "Liang et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Liang et al\\.", "year": 2015}, {"title": "Rectified linear units improve restricted boltzmann machines", "author": ["Nair", "Vinod", "Hinton", "Geoffrey E"], "venue": "In ICML,", "citeRegEx": "Nair et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Nair et al\\.", "year": 2010}, {"title": "Deep face recognition", "author": ["Parkhi", "Omkar M", "Vedaldi", "Andrea", "Zisserman", "Andrew"], "venue": "In BMVC,", "citeRegEx": "Parkhi et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Parkhi et al\\.", "year": 2015}, {"title": "Fitnets: Hints for thin deep nets", "author": ["Romero", "Adriana", "Ballas", "Nicolas", "Kahou", "Samira Ebrahimi", "Chassang", "Antoine", "Gatta", "Carlo", "Bengio", "Yoshua"], "venue": "In ICLR,", "citeRegEx": "Romero et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Romero et al\\.", "year": 2015}, {"title": "Imagenet large scale visual recognition challenge", "author": ["Russakovsky", "Olga", "Deng", "Jia", "Su", "Hao", "Krause", "Jonathan", "Satheesh", "Sanjeev", "Ma", "Sean", "Huang", "Zhiheng", "Karpathy", "Andrej", "Khosla", "Aditya", "Bernstein", "Michael"], "venue": null, "citeRegEx": "Russakovsky et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Russakovsky et al\\.", "year": 2014}, {"title": "Facenet: A unified embedding for face recognition and clustering", "author": ["Schroff", "Florian", "Kalenichenko", "Dmitry", "Philbin", "James"], "venue": "In CVPR,", "citeRegEx": "Schroff et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Schroff et al\\.", "year": 2015}, {"title": "Overfeat: Integrated recognition, localization and detection using convolutional networks", "author": ["Sermanet", "Pierre", "Eigen", "David", "Zhang", "Xiang", "Mathieu", "Micha\u00ebl", "Fergus", "Rob", "LeCun", "Yann"], "venue": null, "citeRegEx": "Sermanet et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Sermanet et al\\.", "year": 2014}, {"title": "Very deep convolutional networks for large-scale image recognition", "author": ["Simonyan", "Karen", "Zisserman", "Andrew"], "venue": "arXiv preprint arXiv:1409.1556,", "citeRegEx": "Simonyan et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Simonyan et al\\.", "year": 2014}, {"title": "Striving for simplicity", "author": ["Springenberg", "Jost Tobias", "Dosovitskiy", "Alexey", "Brox", "Thomas", "Riedmiller", "Martin"], "venue": "In ICLR,", "citeRegEx": "Springenberg et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Springenberg et al\\.", "year": 2015}, {"title": "Dropout: A simple way to prevent neural networks from overfitting", "author": ["Srivastava", "Nitish", "Hinton", "Geoffrey", "Krizhevsky", "Alex", "Sutskever", "Ilya", "Salakhutdinov", "Ruslan"], "venue": null, "citeRegEx": "Srivastava et al\\.,? \\Q1929\\E", "shortCiteRegEx": "Srivastava et al\\.", "year": 1929}, {"title": "Deep networks with internal selective attention through feedback connections", "author": ["Stollenga", "Marijn F", "Masci", "Jonathan", "Gomez", "Faustino", "Schmidhuber", "J\u00fcrgen"], "venue": "In NIPS,", "citeRegEx": "Stollenga et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Stollenga et al\\.", "year": 2014}, {"title": "Deep learning face representation by joint identificationverification", "author": ["Sun", "Yi", "Chen", "Yuheng", "Wang", "Xiaogang", "Tang", "Xiaoou"], "venue": "In NIPS,", "citeRegEx": "Sun et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Sun et al\\.", "year": 2014}, {"title": "Deeply learned face representations are sparse, selective, and robust", "author": ["Sun", "Yi", "Wang", "Xiaogang", "Tang", "Xiaoou"], "venue": "In CVPR,", "citeRegEx": "Sun et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Sun et al\\.", "year": 2015}, {"title": "Going deeper with convolutions", "author": ["Szegedy", "Christian", "Liu", "Wei", "Jia", "Yangqing", "Sermanet", "Pierre", "Reed", "Scott", "Anguelov", "Dragomir", "Erhan", "Dumitru", "Vanhoucke", "Vincent", "Rabinovich", "Andrew"], "venue": "In CVPR,", "citeRegEx": "Szegedy et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Szegedy et al\\.", "year": 2015}, {"title": "Deepface: Closing the gap to human-level performance in face verification", "author": ["Taigman", "Yaniv", "Yang", "Ming", "Ranzato", "Marc\u2019Aurelio", "Wolf", "Lars"], "venue": "In CVPR,", "citeRegEx": "Taigman et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Taigman et al\\.", "year": 2014}, {"title": "Regularization of neural networks using dropconnect", "author": ["Wan", "Li", "Zeiler", "Matthew", "Zhang", "Sixin", "Cun", "Yann L", "Fergus", "Rob"], "venue": "In ICML,", "citeRegEx": "Wan et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Wan et al\\.", "year": 2013}, {"title": "Learning face representation from scratch", "author": ["Yi", "Dong", "Lei", "Zhen", "Liao", "Shengcai", "Li", "Stan Z"], "venue": "arXiv preprint arXiv:1411.7923,", "citeRegEx": "Yi et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Yi et al\\.", "year": 2014}, {"title": "Stochastic pooling for regularization of deep convolutional neural networks", "author": ["Zeiler", "Matthew D", "Fergus", "Rob"], "venue": "arXiv preprint arXiv:1301.3557,", "citeRegEx": "Zeiler et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Zeiler et al\\.", "year": 2013}], "referenceMentions": [{"referenceID": 28, "context": ", 2015b;a), face verification (Taigman et al., 2014; Sun et al., 2014; 2015) and hand-written digit recognition (Wan et al.", "startOffset": 30, "endOffset": 76}, {"referenceID": 25, "context": ", 2015b;a), face verification (Taigman et al., 2014; Sun et al., 2014; 2015) and hand-written digit recognition (Wan et al.", "startOffset": 30, "endOffset": 76}, {"referenceID": 29, "context": ", 2014; 2015) and hand-written digit recognition (Wan et al., 2013).", "startOffset": 49, "endOffset": 67}, {"referenceID": 27, "context": "Facing the increasingly more complex data, CNNs have been continuously improved with deeper structures (Simonyan & Zisserman, 2014; Szegedy et al., 2015), smaller strides (Simonyan & Zisserman, 2014) and new non-linear activations (Goodfellow et al.", "startOffset": 103, "endOffset": 153}, {"referenceID": 18, "context": "Considerable effort such as large-scale training data (Russakovsky et al., 2014), dropout (Krizhevsky et al.", "startOffset": 54, "endOffset": 80}, {"referenceID": 11, "context": ", 2014), dropout (Krizhevsky et al., 2012), data augmentation (Krizhevsky et al.", "startOffset": 17, "endOffset": 42}, {"referenceID": 11, "context": ", 2012), data augmentation (Krizhevsky et al., 2012; Szegedy et al., 2015), regularization (Hinton et al.", "startOffset": 27, "endOffset": 74}, {"referenceID": 27, "context": ", 2012), data augmentation (Krizhevsky et al., 2012; Szegedy et al., 2015), regularization (Hinton et al.", "startOffset": 27, "endOffset": 74}, {"referenceID": 5, "context": ", 2015), regularization (Hinton et al., 2012; Srivastava et al., 2014; Wan et al., 2013; Goodfellow et al., 2013) and stochastic pooling (Zeiler & Fergus, 2013) has been put to address the issue.", "startOffset": 24, "endOffset": 113}, {"referenceID": 29, "context": ", 2015), regularization (Hinton et al., 2012; Srivastava et al., 2014; Wan et al., 2013; Goodfellow et al., 2013) and stochastic pooling (Zeiler & Fergus, 2013) has been put to address the issue.", "startOffset": 24, "endOffset": 113}, {"referenceID": 2, "context": "Inspired by such idea, the contrastive loss (Hadsell et al., 2006) and triplet loss (Schroff et al.", "startOffset": 44, "endOffset": 66}, {"referenceID": 19, "context": ", 2006) and triplet loss (Schroff et al., 2015) were proposed to enforce extra intra-class compactness and inter-class separa-", "startOffset": 25, "endOffset": 47}, {"referenceID": 25, "context": "To enhance the intra-class compactness and inter-class separability, (Sun et al., 2014) trains the CNN with the combination of softmax loss and contrastive loss.", "startOffset": 69, "endOffset": 87}, {"referenceID": 19, "context": "(Schroff et al., 2015) uses the triplet loss to encourage a distance constraint similar to the contrastive loss.", "startOffset": 0, "endOffset": 22}, {"referenceID": 25, "context": "Both (Sun et al., 2014) and (Schroff et al.", "startOffset": 5, "endOffset": 23}, {"referenceID": 19, "context": ", 2014) and (Schroff et al., 2015) suggest that enforcing such a distance constraint that encourages intraclass compactness and inter-class separability can greatly boost the feature discriminativeness, which motivates us to employ a margin constraint in the original softmax loss.", "startOffset": 12, "endOffset": 34}, {"referenceID": 12, "context": "In visual classification, we use three standard benchmark datasets: MNIST (LeCun et al., 1998), CIFAR10 (Krizhevsky, 2009), and CIFAR100 (Krizhevsky, 2009).", "startOffset": 74, "endOffset": 94}, {"referenceID": 6, "context": "In face verification, we evaluate our method on the widely used LFW dataset (Huang et al., 2007).", "startOffset": 76, "endOffset": 96}, {"referenceID": 9, "context": "We implement the CNNs using the Caffe library (Jia et al., 2014) with our modifications.", "startOffset": 46, "endOffset": 64}, {"referenceID": 8, "context": "Method Error Rate CNN (Jarrett et al., 2009) 0.", "startOffset": 22, "endOffset": 44}, {"referenceID": 29, "context": "53 DropConnect (Wan et al., 2013) 0.", "startOffset": 15, "endOffset": 33}, {"referenceID": 17, "context": "57 FitNet (Romero et al., 2015) 0.", "startOffset": 10, "endOffset": 31}, {"referenceID": 13, "context": "31 GenPool (Lee et al., 2016) 0.", "startOffset": 11, "endOffset": 29}, {"referenceID": 29, "context": "Method CIFAR10 CIFAR10+ DropConnect (Wan et al., 2013) 9.", "startOffset": 36, "endOffset": 54}, {"referenceID": 17, "context": "32 FitNet (Romero et al., 2015) N/A 8.", "startOffset": 10, "endOffset": 31}, {"referenceID": 22, "context": "97 All-CNN (Springenberg et al., 2015) 9.", "startOffset": 11, "endOffset": 38}, {"referenceID": 13, "context": "43 GenPool (Lee et al., 2016) 7.", "startOffset": 11, "endOffset": 29}, {"referenceID": 17, "context": "5% Method Error Rate FitNet (Romero et al., 2015) 35.", "startOffset": 28, "endOffset": 49}, {"referenceID": 24, "context": "57 dasNet (Stollenga et al., 2014) 33.", "startOffset": 10, "endOffset": 34}, {"referenceID": 22, "context": "78 All-CNN (Springenberg et al., 2015) 33.", "startOffset": 11, "endOffset": 38}, {"referenceID": 13, "context": "75 GenPool (Lee et al., 2016) 32.", "startOffset": 11, "endOffset": 29}, {"referenceID": 19, "context": "Method Outside Data Accuracy FaceNet (Schroff et al., 2015) 200M* 99.", "startOffset": 37, "endOffset": 59}, {"referenceID": 16, "context": "65 Deep FR (Parkhi et al., 2015) 2.", "startOffset": 11, "endOffset": 32}, {"referenceID": 26, "context": "95 DeepID2+ (Sun et al., 2015) 300K* 98.", "startOffset": 12, "endOffset": 30}, {"referenceID": 30, "context": "7 (Yi et al., 2014) WebFace 97.", "startOffset": 2, "endOffset": 19}, {"referenceID": 6, "context": "To further evaluate the learned features, we conduct an experiment on the famous LFW dataset (Huang et al., 2007).", "startOffset": 93, "endOffset": 113}, {"referenceID": 6, "context": "Following the unrestricted with labeled outside data protocol (Huang et al., 2007), we train on the publicly available CASIA-WebFace (Yi et al.", "startOffset": 62, "endOffset": 82}, {"referenceID": 30, "context": ", 2007), we train on the publicly available CASIA-WebFace (Yi et al., 2014) outside dataset (490k labeled face images belonging to over 10,000 individuals) and test on the 6,000 face pairs on LFW.", "startOffset": 58, "endOffset": 75}, {"referenceID": 0, "context": "As preprocessing, we use IntraFace (Asthana et al., 2014) to align the face images and then crop them based on 5 points.", "startOffset": 35, "endOffset": 57}], "year": 2016, "abstractText": "Cross-entropy loss together with softmax is arguably one of the most common used supervision components in convolutional neural networks (CNNs). Despite its simplicity, popularity and excellent performance, the component does not explicitly encourage discriminative learning of features. In this paper, we propose a generalized large-margin softmax (L-Softmax) loss which explicitly encourages intra-class compactness and inter-class separability between learned features. Moreover, L-Softmax not only can adjust the desired margin but also can avoid overfitting. We also show that the L-Softmax loss can be optimized by typical stochastic gradient descent. Extensive experiments on four benchmark datasets demonstrate that the deeply-learned features with L-softmax loss become more discriminative, hence significantly boosting the performance on a variety of visual classification and verification tasks.", "creator": "LaTeX with hyperref package"}}}