{"id": "1612.02741", "review": {"conference": "ICML", "VERSION": "v1", "DATE_OF_SUBMISSION": "8-Dec-2016", "title": "Coupling Distributed and Symbolic Execution for Natural Language Queries", "abstract": "Building neural networks to query a knowledge base (a table) with natural language is an emerging research topic in NLP. The neural enquirer typically necessitates multiple steps of execution because of the compositionality of queries. In previous studies, researchers have developed either distributed enquirers or symbolic ones for table querying. The distributed enquirer is end-to-end learnable, but is weak in terms of execution efficiency and explicit interpretability. The symbolic enqurier, on the contrary, is efficient during execution; but it is very difficult to train especially at initial stages. In this paper, we propose to couple distributed and symbolic execution for natural language queries. The observation is that a fully distributed executor also exhibits meaningful, albeit imperfect, interpretation. We can thus pretrain the symbolic executor with the distributed one's intermediate execution results in a step-by-step fashion. Experiments show that our approach significantly outperforms either the distributed or symbolic executor; moreover, we have recovered more than 80% execution sequences with only groundtruth denotations during training. In summary, the coupled neural enquirer takes advantages of both distributed and symbolic executors, and has high performance, high learning efficiency, high execution efficiency, and high interpretability.", "histories": [["v1", "Thu, 8 Dec 2016 17:45:16 GMT  (534kb,D)", "http://arxiv.org/abs/1612.02741v1", null], ["v2", "Thu, 16 Feb 2017 11:37:44 GMT  (1587kb,D)", "http://arxiv.org/abs/1612.02741v2", null], ["v3", "Tue, 25 Apr 2017 20:39:57 GMT  (4331kb,D)", "http://arxiv.org/abs/1612.02741v3", "ICLR-17 Workshop"], ["v4", "Fri, 16 Jun 2017 14:33:31 GMT  (1066kb,D)", "http://arxiv.org/abs/1612.02741v4", "Accepted by ICML-17; also presented at ICLR-17 Workshop"]], "reviews": [], "SUBJECTS": "cs.LG cs.AI cs.CL cs.NE cs.SE", "authors": ["lili mou", "zhengdong lu", "hang li", "zhi jin"], "accepted": true, "id": "1612.02741"}, "pdf": {"name": "1612.02741.pdf", "metadata": {"source": "CRF", "title": "Coupling Distributed and Symbolic Execution for Natural Language Queries", "authors": ["Lili Mou", "Zhengdong Lu", "Hang Li", "Zhi Jin"], "emails": ["doublepower.mou@gmail.com,", "luz@DeeplyCurious.ai", "HangLi.HL@huawei.com,", "zhijin@sei.pku.edu.cn"], "sections": [{"heading": "1 Introduction", "text": "In fact, it is a way in which people are able to determine for themselves what they want and what they want."}, {"heading": "2 Approach", "text": "In this section, we first present the fully distributed (neuralized) enquirer proposed in Yin et al. [2016b], and then design a series of operators that are fully customized to the task at hand. At each execution step, a neural network predicts a specific operator and possibly arguments for symbolic execution. Section 2.3 provides a unified view of distributed and symbolic execution. (See also Figure 1) We explain how the symbolic performer is pre-trained based on the intermediate results of the distributed model and then trained using the REINFORCE algorithm."}, {"heading": "2.1 Distributed Enquirer", "text": "In the USA, where there has been a massive increase in unemployment in the USA, Europe and the USA in the last few years, the number of unemployed in the USA, in Europe, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA"}, {"heading": "2.2 Symbolic Executor", "text": "The method of designing a symbolic executor is to define a set of primitive operators (Section 2.2.1) and then use a machine learning model to predict the operator and its arguments (Section 2.2.2)."}, {"heading": "2.2.1 Primitive Operators", "text": "We design six operators for symbolic execution, which are complete because they cover all kinds of queries in our scenario. Similar to the distributed neural finder, the result of symbolic execution in one step is a Boolean scalar of 0-1 for each row; previous execution results are used as input, with a column / field serving as argument. Table 2 summarizes our symbolic operator. In Table 1, for example, the first step of execution is an arrow across the column range, selecting all previous (initial) rows. This step yields a single line < 2008, Beijing, \u00b7, 350, \u00b7, 25 >. The second execution operator is an execution value, where an argument column = duration yields the result 25. Then, the executor (EOE) quits end. Stacked with several steps of primitive operators, the executor can answer complicated questions, such as \"How long is the last game, which has a country size smaller than the game whose host country is chosen as the first line, the one whose GDP is the first column mentioned is 250?\""}, {"heading": "2.2.2 Operator and Argument Predictors", "text": "We also use neural models, especially recurrent neural networks (RNNs), to predict the operator and its argument (a selected field / column), let us let h (t \u2212 1) be the hidden vectors of the previous state, the current vector ish (t \u2212 1) op = sigmoid (Wrech (t \u2212 1) op) (6), leaving out Wrec weight parameters and a bias term in the equation for simplicity's sake, the initial hidden state being the embedding of the query, i.e. h (0) op = q.The predicted probability of an operator i is determined by p (t) opi = softmax {w > i h (t \u2212 1) op} (7) The operator with the most predicted probability is selected for execution, our RNN has no input here, as the execution sequence does not depend on the likelihood of execution, as the previous execution sequence is not dependent on the result of the execution sequence."}, {"heading": "2.3 A Unified View", "text": "We have two worlds of execution: \u2022 The distributed caller is end-to-end learnable, but itis of low execution efficiency due to intense matrix / vector multiplication during neural information processing. \u00b7 The symbolic caller has high execution efficiency and explicit interpretation. However, he cannot be trained in an end-to-end manner, and he suffers from the cold start problem of attachment learning. We propose to combine the two worlds by using the distributed callers to define the intermedia execution results to the pretrain thedenotation execution method query type SEMPRE query type, the distributed Our Method SelectWhere 93.8 96.2 100.0 - 100.0 superlatives 97.8 - 98.8 - 98.7 WhereSuperlatives Y4.8 80.4 82.1 60.5 84.2%."}, {"heading": "3 Experiments", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "3.1 Dataset", "text": "We examined our approach using a QA dataset in Yin et al. [2016b].3 The dataset includes 25k different tables and queries. The queries can be divided into four types: SelectWhere, Superlative, WhereSuperlative, and NestQuery, each requiring 2-4 execution steps (EOE excluded).We have both basic truth labels and execution actions (including operators and fields), as the dataset is synthesized for research purposes by complicated rules and templates. However, during the training only denotations are used as labels, which is a realistic setting. Execution sequences are used only during the test to better understand our approach; this is also a strong motivation for using synthesis datasets for research purposes. For simplicity, we assume that the number of execution steps is a priori during the training (but not during the test) to ensure a better understanding of our approach; this is also a strong motivation for using synthesis datasets for research purposes."}, {"heading": "3.2 Settings", "text": "All settings of the distributed neural enquirers were derived from Yin et al. [2016b] so that we can have a fair comparison. Dimensions of all layers were in the range of 20-50 and the learning algorithm was AdaDelta with standard hyperparameters. To pre-train the symbolic enquirer, we applied the maximum probability estimate for 40 epochs to the column selection with labels predicted by the distributed enquirers. Subsequently, we used the REINFORCE algorithm to improve the guidelines, generating 10 action samples for each data point with an exploration probability of 0.1."}, {"heading": "3.3 Results", "text": "Table 3 presents the experimental results of our coupled distributed and symbolic questioner as well as the SUMPRE base execution. As we can see, both the fully distributed and our proposed approach significantly outperform the SEMPRE system. 4 The coupled distributed and symbolic questioner also significantly outperforms the fully distributed questioner in almost all types of queries. In terms of execution accuracy, we note that our approach has 83.4% correct execution sequences even without Intermediate3The dataset will soon be made available. 4http: / / nlp.stanford.edu / software / sempre / step-by-step-training signals. In contrast, a fully distributed neural questioner has no explicit interpretations.Figure 3 shows the validation learning curves further. We see that if the symbolic questioner is trained solely by REINFORCE, it will take 200 epochs to reach 50% if the initial form is poor."}, {"heading": "4 Conclusion", "text": "In this paper, we proposed a coupled view of distributed and symbolic execution for questions of natural language. By pre-training with intermediate results of the execution of a distributed questioner, we succeeded in greatly accelerating the training of the symbolic model. Our proposed approach uses both distributed and symbolic worlds, achieving high accuracy, high execution efficiency, high learning efficiency and high interpretation. In future work, we would like to design an interpretable user selection in the distributed model in order to better link the two worlds and further simplify training with REINFORCE. We would also like to return the results of the execution of the symbolic questioner in order to better train the distributed one. Such step-by-step monitoring can also be repeated in a cooperative manner."}, {"heading": "Acknowledgments", "text": "We thank Pengcheng Yin, Jiatao Gu and Hao Zhou for the helpful discussion."}], "references": [{"title": "Language to logical form with neural attention", "author": ["Li Dong", "Mirella Lapata"], "venue": "In ACL,", "citeRegEx": "Dong and Lapata.,? \\Q2016\\E", "shortCiteRegEx": "Dong and Lapata.", "year": 2016}, {"title": "Serial order: A parallel distributed processing approach", "author": ["Michael I Jordan"], "venue": "Advances in psychology,", "citeRegEx": "Jordan.,? \\Q1997\\E", "shortCiteRegEx": "Jordan.", "year": 1997}, {"title": "Neural symbolic machines: Learning semantic parsers on freebase with weak supervision", "author": ["Chen Liang", "Jonathan Berant", "Quoc Le", "Kenneth D Forbus", "Ni Lao"], "venue": "arXiv preprint arXiv:1611.00020,", "citeRegEx": "Liang et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Liang et al\\.", "year": 2016}, {"title": "Simpler context-dependent logical forms via model projections", "author": ["Reginald Long", "Panupong Pasupat", "Percy Liang"], "venue": "In ACL,", "citeRegEx": "Long et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Long et al\\.", "year": 2016}, {"title": "Investigation of recurrent-neural-network architectures and learning methods for spoken language understanding", "author": ["Gr\u00e9goire Mesnil", "Xiaodong He", "Li Deng", "Yoshua Bengio"], "venue": "In INTERSPEECH,", "citeRegEx": "Mesnil et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Mesnil et al\\.", "year": 2013}, {"title": "Distributed representations of words and phrases and their compositionality", "author": ["Tomas Mikolov", "Ilya Sutskever", "Kai Chen", "Greg S Corrado", "Jeff Dean"], "venue": "In Advances in neural information processing systems,", "citeRegEx": "Mikolov et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Mikolov et al\\.", "year": 2013}, {"title": "Neural programmer: Inducing latent programs with gradient descent", "author": ["Arvind Neelakantan", "Quoc V Le", "Ilya Sutskever"], "venue": "In ICLR,", "citeRegEx": "Neelakantan et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Neelakantan et al\\.", "year": 2016}, {"title": "Inferring logical forms from denotations", "author": ["Panupong Pasupat", "Percy Liang"], "venue": "In ACL,", "citeRegEx": "Pasupat and Liang.,? \\Q2016\\E", "shortCiteRegEx": "Pasupat and Liang.", "year": 2016}, {"title": "Sequence level training with recurrent neural networks", "author": ["MarcAurelio Ranzato", "Sumit Chopra", "Michael Auli", "Wojciech Zaremba"], "venue": "In ICLR,", "citeRegEx": "Ranzato et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Ranzato et al\\.", "year": 2016}, {"title": "Reinforcement Learning: An Introduction, volume 1", "author": ["Richard S Sutton", "Andrew G Barto"], "venue": null, "citeRegEx": "Sutton and Barto.,? \\Q1998\\E", "shortCiteRegEx": "Sutton and Barto.", "year": 1998}, {"title": "A network-based end-to-end trainable taskoriented dialogue system", "author": ["Tsung-Hsien Wen", "Milica Gasic", "Nikola Mrksic", "Lina M RojasBarahona", "Pei-Hao Su", "Stefan Ultes", "David Vandyke", "Steve Young"], "venue": "arXiv preprint arXiv:1604.04562,", "citeRegEx": "Wen et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Wen et al\\.", "year": 2016}, {"title": "Sequence-based structured prediction for semantic parsing", "author": ["Chunyang Xiao", "Marc Dymetman", "Claire Gardent"], "venue": "In ACL,", "citeRegEx": "Xiao et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Xiao et al\\.", "year": 2016}, {"title": "Neural generative question answering", "author": ["Jun Yin", "Xin Jiang", "Zhengdong Lu", "Lifeng Shang", "Hang Li", "Xiaoming Li"], "venue": "In IJCAI,", "citeRegEx": "Yin et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Yin et al\\.", "year": 2016}, {"title": "Neural enquirer: Learning to query tables with natural language", "author": ["Pengcheng Yin", "Zhengdong Lu", "Hang Li", "Ben Kao"], "venue": "In IJCAI,", "citeRegEx": "Yin et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Yin et al\\.", "year": 2016}], "referenceMentions": [{"referenceID": 10, "context": ", 2016a], human-computer conversation [Wen et al., 2016], etc.", "startOffset": 38, "endOffset": 56}, {"referenceID": 3, "context": "A typical approach is to convert a natural language sentence to an \u201cexecutable\u201d logic forms for table/knowledge base querying, known as semantic parsing [Long et al., 2016; Pasupat and Liang, 2016].", "startOffset": 153, "endOffset": 197}, {"referenceID": 7, "context": "A typical approach is to convert a natural language sentence to an \u201cexecutable\u201d logic forms for table/knowledge base querying, known as semantic parsing [Long et al., 2016; Pasupat and Liang, 2016].", "startOffset": 153, "endOffset": 197}, {"referenceID": 0, "context": "With the fast development of modern neural networks, Dong and Lapata [2016] and Xiao et al.", "startOffset": 53, "endOffset": 76}, {"referenceID": 0, "context": "With the fast development of modern neural networks, Dong and Lapata [2016] and Xiao et al. [2016] apply sequenceto-sequence (seq2seq) neural models to generate a logic form conditioned on an input sentence.", "startOffset": 53, "endOffset": 99}, {"referenceID": 6, "context": "An emerging research topic in neural semantic parsing is to query a knowledge base directly by neural networks [Yin et al., 2016b; Neelakantan et al., 2016; Liang et al., 2016].", "startOffset": 111, "endOffset": 176}, {"referenceID": 2, "context": "An emerging research topic in neural semantic parsing is to query a knowledge base directly by neural networks [Yin et al., 2016b; Neelakantan et al., 2016; Liang et al., 2016].", "startOffset": 111, "endOffset": 176}, {"referenceID": 2, "context": ", 2016; Liang et al., 2016]. Because queries can be composited in a highly complicated manner, neural enquirers necessitate multiple steps of execution. The difficulty then lies in the lack of step-by-step supervision. In other words, we only assume groundtruth denotations are available in realistic settings, and that we do not know the execution sequence or intermediate execution results. There have seen several studies addressing the problem. Yin et al. [2016b] propose a fully distributed neural enquirer, comprising several neuralized execution layers of field attention, row annotation, etc.", "startOffset": 8, "endOffset": 468}, {"referenceID": 2, "context": ", 2016; Liang et al., 2016]. Because queries can be composited in a highly complicated manner, neural enquirers necessitate multiple steps of execution. The difficulty then lies in the lack of step-by-step supervision. In other words, we only assume groundtruth denotations are available in realistic settings, and that we do not know the execution sequence or intermediate execution results. There have seen several studies addressing the problem. Yin et al. [2016b] propose a fully distributed neural enquirer, comprising several neuralized execution layers of field attention, row annotation, etc. While the model is not efficient in execution because of intensive matrix/vector operation during neural information processing and lacks explicit interpretation of execution, it can be trained in an end-to-end fashion because all components in the neural enquirer are differentiable. Neelakantan et al. [2016], on the other hand, propose a neural programmer by defining a set of symbolic operations (e.", "startOffset": 8, "endOffset": 912}, {"referenceID": 2, "context": ", 2016; Liang et al., 2016]. Because queries can be composited in a highly complicated manner, neural enquirers necessitate multiple steps of execution. The difficulty then lies in the lack of step-by-step supervision. In other words, we only assume groundtruth denotations are available in realistic settings, and that we do not know the execution sequence or intermediate execution results. There have seen several studies addressing the problem. Yin et al. [2016b] propose a fully distributed neural enquirer, comprising several neuralized execution layers of field attention, row annotation, etc. While the model is not efficient in execution because of intensive matrix/vector operation during neural information processing and lacks explicit interpretation of execution, it can be trained in an end-to-end fashion because all components in the neural enquirer are differentiable. Neelakantan et al. [2016], on the other hand, propose a neural programmer by defining a set of symbolic operations (e.g., argmax, greater than); at each step, all possible execution results are fused by a softmax layer, which predicts the probability of each operator at the current step. The step-bystep fusion is accomplished by weighted sum and the model is trained with mean square error. Hence, such approaches work with numeric tables, but may not be suited for other operations like string matching; it also suffers from the problem of \u201cexponential numbers of combinatorial states.\u201d Liang et al. [2016] train a symbolic executor by REINFORCE policy gradient, but it is known that the REINFORCE algorithm is sensitive to the", "startOffset": 8, "endOffset": 1496}, {"referenceID": 12, "context": "For example, the field attention gadget in Yin et al. [2016b] generally aligns with column selection.", "startOffset": 43, "endOffset": 62}, {"referenceID": 12, "context": "For example, the field attention gadget in Yin et al. [2016b] generally aligns with column selection. We therefore use the distributed model\u2019s intermediate execution results as supervision signals to pretrain a symbolic executor. Guided by such imperfect step-by-step supervision, the symbolic executor learns a fairly meaningful initial policy, which largely alleviates the cold start problem of the REINFORCE algorithm. We evaluate the proposed approach on the QA dataset in Yin et al. [2016b]. In our experiment, the REINFORCE algorithm alone takes long to get started; even if it does, it is stuck in a poor local optimum.", "startOffset": 43, "endOffset": 496}, {"referenceID": 12, "context": "In this section, we first introduce the fully distributed (neuralized) enquirer proposed in Yin et al. [2016b]. Then we design a set of operators that are complete to the task at hand.", "startOffset": 92, "endOffset": 111}, {"referenceID": 5, "context": "One of the most notable studies of distributed semantics is word embeddings, which map discrete words to vectors as anonymous meaning representations [Mikolov et al., 2013].", "startOffset": 150, "endOffset": 172}, {"referenceID": 12, "context": "Please refer to Yin et al. [2016b] for details of the distributed neural enquirer.", "startOffset": 16, "endOffset": 35}, {"referenceID": 1, "context": "Such architecture is known as a Jordan-type RNN [Jordan, 1997; Mesnil et al., 2013].", "startOffset": 48, "endOffset": 83}, {"referenceID": 4, "context": "Such architecture is known as a Jordan-type RNN [Jordan, 1997; Mesnil et al., 2013].", "startOffset": 48, "endOffset": 83}, {"referenceID": 12, "context": "Table 3: Accuracies (in percentage) of Sempre tookit (reported in Yin et al. [2016b]), the distributed neural enquirer, and our proposed coupled approach.", "startOffset": 66, "endOffset": 85}, {"referenceID": 9, "context": "After obtaining a meaningful, albeit imperfect, initial policy, we apply REINFORCE [Sutton and Barto, 1998] to improve the policy.", "startOffset": 83, "endOffset": 107}, {"referenceID": 8, "context": "This is a common practice for REINFORCE [Ranzato et al., 2016].", "startOffset": 40, "endOffset": 62}, {"referenceID": 12, "context": "We evaluated our approach on a QA dataset in Yin et al. [2016b].3 The dataset comprises 25k different tables and queries.", "startOffset": 45, "endOffset": 64}, {"referenceID": 12, "context": "All settings of distributed neural enquirers were derived from Yin et al. [2016b] so that we can have a fair comparison.", "startOffset": 63, "endOffset": 82}], "year": 2016, "abstractText": "Building neural networks to query a knowledge base (a table) with natural language is an emerging research topic in NLP. The neural enquirer typically necessitates multiple steps of execution because of the compositionality of queries. In previous studies, researchers have developed either distributed enquirers or symbolic ones for table querying. The distributed enquirer is end-to-end learnable, but is weak in terms of execution efficiency and explicit interpretability. The symbolic enqurier, on the contrary, is efficient during execution; but it is very difficult to train especially at initial stages. In this paper, we propose to couple distributed and symbolic execution for natural language queries. The observation is that a fully distributed executor also exhibits meaningful, albeit imperfect, interpretation. We can thus pretrain the symbolic executor with the distributed one\u2019s intermediate execution results in a step-by-step fashion. Experiments show that our approach significantly outperforms either the distributed or symbolic executor; moreover, we have recovered more than 80% execution sequences with only groundtruth denotations during training. In summary, the coupled neural enquirer takes advantages of both distributed and symbolic executors, and has high performance, high learning efficiency, high execution efficiency, and high interpretability.", "creator": "LaTeX with hyperref package"}}}