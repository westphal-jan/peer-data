{"id": "1612.03441", "review": {"conference": "AAAI", "VERSION": "v1", "DATE_OF_SUBMISSION": "11-Dec-2016", "title": "Lock-Free Optimization for Non-Convex Problems", "abstract": "Stochastic gradient descent~(SGD) and its variants have attracted much attention in machine learning due to their efficiency and effectiveness for optimization. To handle large-scale problems, researchers have recently proposed several lock-free strategy based parallel SGD~(LF-PSGD) methods for multi-core systems. However, existing works have only proved the convergence of these LF-PSGD methods for convex problems. To the best of our knowledge, no work has proved the convergence of the LF-PSGD methods for non-convex problems. In this paper, we provide the theoretical proof about the convergence of two representative LF-PSGD methods, Hogwild! and AsySVRG, for non-convex problems. Empirical results also show that both Hogwild! and AsySVRG are convergent on non-convex problems, which successfully verifies our theoretical results.", "histories": [["v1", "Sun, 11 Dec 2016 17:26:43 GMT  (326kb,D)", "http://arxiv.org/abs/1612.03441v1", null]], "reviews": [], "SUBJECTS": "stat.ML cs.LG", "authors": ["shen-yi zhao", "gong-duo zhang", "wu-jun li"], "accepted": true, "id": "1612.03441"}, "pdf": {"name": "1612.03441.pdf", "metadata": {"source": "META", "title": "Lock-Free Optimization for Non-Convex Problems", "authors": ["Shen-Yi Zhao", "Gong-Duo Zhang", "Wu-Jun Li"], "emails": ["zhanggd}@lamda.nju.edu.cn,", "liwujun@nju.edu.cn"], "sections": [{"heading": "Introduction", "text": "In this context, it should also be noted that in the past few years, in most countries of the world, the number of workers capable of being drawn into unemployment has increased. (...) The number of unemployed has also increased in the other countries of the world. (...) The number of unemployed has increased. (...) The number of unemployed has increased. (...) The number of unemployed has increased. (...) The number of unemployed has increased. (...) The number of unemployed has increased. (...) The number of unemployed has increased. (...) The number of unemployed has increased. (...) The number of unemployed has increased. (...) The number of unemployed has increased. (...) The number of unemployed has increased. (...) The number of unemployed has increased. (...) The number of unemployed has increased. (...) The number of unemployed has increased. (...) The number of unemployed has increased. (... The number of unemployed has increased. (...) The number of unemployed has increased. (...) The number of unemployed has increased. (... The number of unemployed has increased. (...) The number of unemployed has increased. (...) The number of unemployed has increased. (... The number of unemployed has. (...) The number of unemployed has increased. (... The number of unemployed has. (...) The number of unemployed has increased. (...) The number of unemployed has. (... The number."}, {"heading": "Preliminary", "text": "We use f (w) to denote the objective function in (1), which means f (w) = 1n (b) = 1 fi (w). \u2212 And we use f (c) to denote the L2 standard in (1). \u2212 Assumption 1. The function fi (\u00b7) in (1) is smooth, which means that there is a constant L > 0, a, b, fi (b) \u2264 fi (a) + fi (a) T (b \u2212 a) + L2 (a) or equivalent to the fact that there is a constant L > 0, b \u2212 fi (a). \u2212 Although the implementation of AsySG-incon in (Lian et al. 2015) is latch-free, the theoretical analysis of the convergence of AsySG-incon is based on the assumption that there is no overwriting (b) \u2212 fi (b) \u2212 fi (a) \u2212 fi (a) \u2212 fi (a)."}, {"heading": "Hogwild! for Non-Convex Problems", "text": "The Hogwild Method (w) is ineluctable in its entirety. (n) The Hogwild Method (s) is ineluctable in its entirety. (n) The Hogwild Method (s) is ineluctable. (n) The Hogwild Method (s) is ineluctable. (n) The Hogwild Method (s) is ineluctable. (n) The Hogwild Method (s) is ineluctable. (n) We are ineluctable. (n) The Hogwild Method (s) is ineluctable. (n) We are ineluctable. (n) The Hogwild Method (s) is ineluctable. (n) The Hogwild Method (s) is ineluctable. (n) The Hogwild Method (s) is ineluctable."}, {"heading": "AsySVRG for Non-Convex Problems", "text": "The AsySVRG method (Zhao and Li 2016) is listed in algorithm 2. \u2212 In this section we will prove that AsySVRG is also convergent for non-convex problems and has a faster convergence rate than Hogwild! on non-convex problems. \u2212 In this section we will prove that AsySVRG is also convergent for non-convex problems and has a faster convergence rate than Hogwild! on non-convex problems. \u2212 In this section we will show that AsySVRG is also convergent for non-convex problems and has a faster convergence rate than Hogwild! on non-convex problems. \u2212 In algorithm 2 AsySVRG initialization: p threads, initialized w0 m; for t = 0, 2,... T \u2212 1 dou0 = wt; all threads are calculated in parallel to the full gradient (0)."}, {"heading": "It easy to find that Eq(u\u0302t,m) = E[\u2016v\u0302t,m\u20162].", "text": "The difference between Hogwild! and AsySVRG is the stochastic gradient and we have the following problems, which lead to a fast convergence rate of AsySVRG (+ 1). \u2212 2 (+ 1) \u2212 4 (+ 2) (+ 2) (+ 2) (+ 2) (+ 2) (+ 2) (+) (+) (+ 2) (+ 2) (+ 2) (+ 2) (+) (+ 2) (+ 2) (+ 2) (+ 2) (+ 2) (+ 2) (+) (+) (+) (+ 2) (+ 2) (+ 2) (+ 2) (+ 2) (+ 2) (+ 2) (+ 2) (+) (+ 2) (+) (+ 2) (+ 2) (+ 2) (+ 2) (+ 2) (+ 2) (+ 2) (+ 2) (+ 2) (+ 2) (+ 2) (+ 2) (+ 2) (+ 2) (+ 2) (+ 2) (+ 2) (+ 2) (+ 2) (+ 2) (+ 2) (+ 2) (+ 2) (+ 2) (+ 2) (+ 2) (+ 2) (+ 2) (+ 2) (+ 2) (+ 2) (+ 2) (+ 2) (+ 2) (+ 2) (+ 2) (+ 2) (+ 2) (+ 2) (+ 2) (+ 2) (+ 2) (+ 2) (+ 2) (+ 2) (+ 2) (+ 2) (+ 2) (+ 2) (+ 2) (+ 2) (+ 2) (+ 2) (+ 2) (+ 2) (+ 2) (+ 2 (+ 2) (+ 2) (+ 2) (+ 2) (+ 2) (+ 2) (+ 2) (+ 2) (+ 2) (+ 2) (+ 2) (+ 2) (+ 2) (+ 2) (+ 2) (+ 2) (+ 2) (+ 2 (+ 2) (+ 2) (+ 2) (+ 2), (+ 2), (+ 2), (+ 2), (+ 2 ("}, {"heading": "Computation Complexity", "text": "In Theorem 2 we construct a sequence {cm} and need \u03b3 > 0. According to the definition of hm we can write hm as hm = gcm + f, where g = 2\u03b7\u03b2 4\u03c4\u03c12 (\u03c1\u03c4 \u2212 1) \u03c1 \u2212 1 + \u03c1, f = \u03b7L22 4\u03c4\u03c12 (\u03c1\u043e \u2212 1) \u03c1 \u2212 1 + L\u03c1 2 are constants. First we choose \u03b2 > \u03b7, then both g, f are limited positive constants. We have havecm = cm + 1 (1 + \u03b2\u03b7 + 2L 2\u03b72g) + 2L2\u03b72f.Let us leave a = \u03b2\u03b7 + 2L2\u03b72g. Because cM = 0, it is easy to c0 = 2L 2throu2f (1 + a) M-1 (1 + \u03b2\u03b7 + 2L 2throu2g), let us take M = b 1ac \u2264 1 a, then we have c0 \u2264 4L2throughs = 2 (& x\u043c) and get a compatible solution."}, {"heading": "Experiment", "text": "In fact, most of us are able to play by the rules we have set ourselves in order to make them a reality."}, {"heading": "Conclusion", "text": "In this paper, we have provided theoretical evidence for the convergence of two representative lock-free strategy based parallel SGD methods, Hogwild! and AsySVRG, for non-convex problems. Empirical results also show that both Hogwild! and AsySVRG are convergent for non-convex problems, which successfully verifies our theoretical results. To the best of our knowledge, this is the first paper to demonstrate the convergence of lock-free strategy based parallel SGD methods for non-convex problems."}, {"heading": "Acknowledgements", "text": "This work is partially supported by NSFC (No. 61472182) and a Tencent fund."}, {"heading": "Appendix", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "Proof of Lemma 2", "text": "In the above equation, we take x = w, y = w, y = w, we get: p, fi (f, f, f, f, f, f, f, f, f, f, f, f, f, f, f, f, f, f, f, f, f, f, f, f, f, f, f, f, f, f, f, f, f, f, f, f, f, f, f, f, f, f, f, f, f, f, f, f, f, f, f, f, f, f, f, f, f, f, f, f, f, f, f, f, f, f, f, f, f, f, f, f, f, f, f, f, f, f, f, f, f, f, f, f, f, f, f, f, f, f, f, f, f, f, f, f, f, f, f, f, f, f, f, f, f, f, f, f, f, f, f, f, f, f, f, f, f, f, f, f, f, f, f, f, f, f, f, f, f, f, f, f, f, f, f, f, f, f, f, f, f, f, f, f, f, f, f, f, f, f, f, f, f, f, f, f, f, f, f, f, f, f, f, f, f, f, f, f, f, f, f, f, f, f, f, f, f, f, f, f, f, f, f, f, f, f, f, f, f, f, f, f, f, f, f, f, f, f, f, f, f, f, f, f, f, f, f, f, f, f, f, f, f, f, f, f, f, f, f, f, f, f, f, f, f, f, f, f, f, f, f, f, f, f, f, f, f, f, f, f, f, f"}, {"heading": "Proof of Lemma 3", "text": "Proof: wt-w-t = wt-wt-wa (t) + p-j = a (t) Pt, j-a (t) i-fij (w-j) i-wt-wa (t) i-j = a (t) i-j (w-j) i-fij (w-j) i-j = a (t) i-j (w-j) i-wj + 1 (t) i-j (w-j) i-j (t) i-j = a (t) i-j (w-j) i-j (w-j) i-j (w-j) i-w-j (t) i-j = a (t) E-fij (w-j) i-j (w-j) i-j (w-j) i-j = a (t) i-j (w-j)."}, {"heading": "Proof of Lemma 5", "text": "fourthly, fourthly, fourthly, fourthly, fourthly, fourthly, fourthly, fourthly, fourthly, fourthly, fourthly, fourthly, fourthly, fourthly, fourthly, fourthly, fourthly, fourthly, fourthly, fourthly, fourthly, fourthly, fourthly, fourthly, fourthly, fourthly, fourthly, fourthly, fourthly, fourthly, fourthly, fourthly, fourthly, fourthly, fourthly, fourthly, fourthly, fourthly, fourthly, fourthly, fourthly, fourthly, fourthly, fourthly, fourthly, fourthly, fourthly, fourthly, fourthly, fourthly, fourthly, fourthly, fourthly, fourthly, fourthly, fourthly, fourthly, fourthly, fourthly, fourthly, fourthly, fourthly, fourthly, fourthly, fourthly, fourthly, fourthly, fourthly, fourthly, fourthly, fourthly, fourthly, fourthly, fourthly, fourthly, fourthly, fourthly, fourthly, fourthly, fourthly, fourthly, fourthly, fourthly, fourthly, fourthly, fourthly, fourthly, fourthly, fourthly, fourthly, fourthly, fourthly, fourthly, fourthly, fourthly, fourthly, fourthly, fourthly, fourthly, fourthly, fourthly, fourthly, fourthly, fourthly, fourthly, fourthly, fourthly, fourthly, fourthly, fourthly, fourthly, fourthly, fourthly, fourthly, fourthly, fourthly, fourthly, fourthly, fourthly, fourthly, fourthly, fourthly, fourthly, fourthly, fourthly, fourthly, fourthly, four"}, {"heading": "Proof of Lemma 6", "text": "Prove that it is a case in which one (m) -1 (m) -1 (m) P (t) m, j \u2212 a (m) v (t) t, j \u2212 t (m), j \u2212 ut, m (m) t, a (m) \u2212 ut, m (m) \u2212 ut, m (m) t, m (m) v (t) t, j (m) t, j (m) t, j (m) t, j \u2212 ut, j \u2212 ut, j + 1 (m) m (m) m (m) \u2212 ut, m (m) t, j (m) t, j (m) t (m) t, j (m) t (m) t, j (m) t (m) t (m), j (m) t (m), j (m) t (m), j (m), j (j) t (m), j (m), j (m) t (m), j (m), j (m) t (m), j (m), j (m) t (m), j (m), j (m) t (m), j (m) t (m), j (m), j (m) t (m), j (m), j (m) t (m), j (m), j (m), j (m), j (m), j (m) t (m), j (m), j (m), j (m (m), j (m), j (m), j (m (m), j (m), j (m), j (m), j (m (m), j (m), j (m (m), j (m), j (m (m), j (m), j (m), j (m (m), j (m), j (m), j (m (m), j (m (m), j (m), j (m), j (m (m), j (m (m), j (m (m), j (m (m), j (m (m), j (m), j (m), j (m)"}, {"heading": "Proof of Lemma 7", "text": "Proof: First, for each fixe i, for each fixe i, we have the expectation on both sides by using Lemma 6 and then adding I from 1 to n and we can add Eq (u, t, m) \u2212 Eq (u, m) \u2212 Eq (u, m) \u2212 Eq (u, t) \u2212 Eq (u, m) \u2212 Eq (u, m) \u2212 Eq (ut, m) \u2212 Eq (ut, m). \u2212 Eq (u, m). \u2212 Eq (ut, m). \u2212 Eq (ut, m). \u2212 Eq (u, m). \u2212 Eq (u, m). \u2212 pi (u, m). \u2212 pi. \u2212 pi. \u2212 pi, m. \u2212 m. \u2212 pi, m. \u2212 pi, m. \u2212 pi, m. \u2212 pi, m. \u2212 pi, m. \u2212 pi, m. \u2212 pi, m. \u2212 pi, m. \u2212 pi, m. \u2212 pi, m. \u2212 pi, m. \u2212 pi, m. \u2212 pi, m. \u2212 pi, m. \u2212 pi, m. \u2212 pi, m. \u2212 pi, m. \u2212 pi, m. \u2212 pi, m. \u2212 pi, m. \u2212 pi, m. \u2212 pi, m. \u2212 pi, m. \u2212 pi, m. \u2212 pi. \u2212 pi, m, m. \u2212 pi. \u2212 pi, m. \u2212 pi, m. \u2212 pi, m. \u2212 pi. \u2212 pi, m. \u2212 pi, m."}], "references": [{"title": "Variance reduction for faster non-convex optimization", "author": ["Z. Allen-Zhu", "E. Hazan"], "venue": "Proceedings of the 33nd International Conference on Machine Learning.", "citeRegEx": "Allen.Zhu and Hazan,? 2016", "shortCiteRegEx": "Allen.Zhu and Hazan", "year": 2016}, {"title": "Improved svrg for nonstrongly-convex or sum-of-non-convex objectives", "author": ["Z. Allen-Zhu", "Y. Yuan"], "venue": "Proceedings of the 33nd International Conference on Machine Learning.", "citeRegEx": "Allen.Zhu and Yuan,? 2016", "shortCiteRegEx": "Allen.Zhu and Yuan", "year": 2016}, {"title": "Asynchronous stochastic convex optimization: the noise is in the noise and sgd don\u2019t care", "author": ["S. Chaturapruek", "J.C. Duchi", "C. R\u00e9"], "venue": "Proceedings of the Advances in Neural Information Processing Systems.", "citeRegEx": "Chaturapruek et al\\.,? 2015", "shortCiteRegEx": "Chaturapruek et al\\.", "year": 2015}, {"title": "Stochastic first- and zeroth-order methods for nonconvex stochastic programming", "author": ["S. Ghadimi", "G. Lan"], "venue": "SIAM Journal on Optimization 23(4):2341\u20132368.", "citeRegEx": "Ghadimi and Lan,? 2013", "shortCiteRegEx": "Ghadimi and Lan", "year": 2013}, {"title": "Asynchronous stochastic gradient descent with variance reduction for non-convex optimization", "author": ["Z. Huo", "H. Huang"], "venue": "CoRR abs/1604.03584.", "citeRegEx": "Huo and Huang,? 2016", "shortCiteRegEx": "Huo and Huang", "year": 2016}, {"title": "On variance reduction in stochastic gradient descent and its asynchronous variants", "author": ["S.J. Reddi", "A. Hefny", "S. Sra", "B. Poczos", "A.J. Smola"], "venue": "Proceedings of the Advances in Neural Information Processing Systems.", "citeRegEx": "Reddi et al\\.,? 2015", "shortCiteRegEx": "Reddi et al\\.", "year": 2015}, {"title": "Communicationefficient distributed dual coordinate ascent", "author": ["M. Jaggi", "V. Smith", "M. Takac", "J. Terhorst", "S. Krishnan", "T. Hofmann", "M.I. Jordan"], "venue": "Proceedings of the Advances in Neural Information Processing Systems.", "citeRegEx": "Jaggi et al\\.,? 2014", "shortCiteRegEx": "Jaggi et al\\.", "year": 2014}, {"title": "Accelerating stochastic gradient descent using predictive variance reduction", "author": ["R. Johnson", "T. Zhang"], "venue": "Proceedings of the Advances in Neural Information Processing Systems.", "citeRegEx": "Johnson and Zhang,? 2013", "shortCiteRegEx": "Johnson and Zhang", "year": 2013}, {"title": "Matrix factorization techniques for recommender systems", "author": ["Y. Koren", "R.M. Bell", "C. Volinsky"], "venue": "IEEE Computer 42(8):30\u201337.", "citeRegEx": "Koren et al\\.,? 2009", "shortCiteRegEx": "Koren et al\\.", "year": 2009}, {"title": "Imagenet classification with deep convolutional neural networks", "author": ["A. Krizhevsky", "I. Sutskever", "G.E. Hinton"], "venue": "Proceedings of the Advances in Neural Information Processing Systems.", "citeRegEx": "Krizhevsky et al\\.,? 2012", "shortCiteRegEx": "Krizhevsky et al\\.", "year": 2012}, {"title": "Scaling distributed machine learning with the parameter", "author": ["M. Li", "D.G. Andersen", "J.W. Park", "A.J. Smola", "A. Ahmed", "V. Josifovski", "J. Long", "E.J. Shekita", "B. Su"], "venue": null, "citeRegEx": "Li et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Li et al\\.", "year": 2014}, {"title": "Stochastic variance reduced optimization for nonconvex sparse learning", "author": ["X. Li", "T. Zhao", "R. Arora", "Han", "J. Haupt"], "venue": "Proceedings of the 33nd International Conference on Machine Learning.", "citeRegEx": "Li et al\\.,? 2016", "shortCiteRegEx": "Li et al\\.", "year": 2016}, {"title": "Asynchronous parallel stochastic gradient for nonconvex optimization", "author": ["X. Lian", "Y. Huang", "Y. Li", "J. Liu"], "venue": "Proceedings of the Advances in Neural Information Processing Systems.", "citeRegEx": "Lian et al\\.,? 2015", "shortCiteRegEx": "Lian et al\\.", "year": 2015}, {"title": "Hogwild!: A lock-free approach to parallelizing stochastic gradient descent", "author": ["B. Recht", "C. Re", "S. Wright", "F. Niu"], "venue": "Proceedings of the Advances in Neural Information Processing Systems.", "citeRegEx": "Recht et al\\.,? 2011", "shortCiteRegEx": "Recht et al\\.", "year": 2011}, {"title": "Stochastic variance reduction for nonconvex optimization", "author": ["S.J. Reddi", "A. Hefny", "S. Sra", "B. Poczos", "Alex."], "venue": "Proceedings of the 33nd International Conference on Machine Learning.", "citeRegEx": "Reddi et al\\.,? 2016", "shortCiteRegEx": "Reddi et al\\.", "year": 2016}, {"title": "A stochastic gradient method with an exponential convergence rate for finite training sets", "author": ["N.L. Roux", "M.W. Schmidt", "F. Bach"], "venue": "Proceedings of the Advances in Neural Information Processing Systems.", "citeRegEx": "Roux et al\\.,? 2012", "shortCiteRegEx": "Roux et al\\.", "year": 2012}, {"title": "Global convergence of stochastic gradient descent for some non-convex matrix problems", "author": ["C.D. Sa", "C. Re", "K. Olukotun"], "venue": "Proceedings of the 32nd International Conference on Machine Learning.", "citeRegEx": "Sa et al\\.,? 2015", "shortCiteRegEx": "Sa et al\\.", "year": 2015}, {"title": "Stochastic dual coordinate ascent methods for regularized loss", "author": ["S. Shalev-Shwartz", "T. Zhang"], "venue": "Journal of Machine Learning Research 14(1):567\u2013599.", "citeRegEx": "Shalev.Shwartz and Zhang,? 2013", "shortCiteRegEx": "Shalev.Shwartz and Zhang", "year": 2013}, {"title": "A stochastic PCA and SVD algorithm with an exponential convergence rate", "author": ["O. Shamir"], "venue": "Proceedings of the 32nd International Conference on Machine Learning.", "citeRegEx": "Shamir,? 2015", "shortCiteRegEx": "Shamir", "year": 2015}, {"title": "Convergence of stochastic gradient descent for pca", "author": ["O. Shamir"], "venue": "Proceedings of the 33nd International Conference on Machine Learning.", "citeRegEx": "Shamir,? 2016a", "shortCiteRegEx": "Shamir", "year": 2016}, {"title": "Fast stochastic algorithms for svd and pca: Convergence properties and convexity", "author": ["O. Shamir"], "venue": "Proceedings of the 33nd International Conference on Machine Learning.", "citeRegEx": "Shamir,? 2016b", "shortCiteRegEx": "Shamir", "year": 2016}, {"title": "Petuum: A new platform for distributed machine learning on big data", "author": ["E.P. Xing", "Q. Ho", "W. Dai", "J.K. Kim", "J. Wei", "S. Lee", "X. Zheng", "P. Xie", "A. Kumar", "Y. Yu"], "venue": "Proceedings of the 21th ACM SIGKDD International Conference on Knowledge Discovery and Data Min-", "citeRegEx": "Xing et al\\.,? 2015", "shortCiteRegEx": "Xing et al\\.", "year": 2015}, {"title": "Asynchronous distributed semi-stochastic gradient optimization", "author": ["R. Zhang", "S. Zheng", "J.T. Kwok"], "venue": "Proceedings of the AAAI Conference on Artificial Intelligence.", "citeRegEx": "Zhang et al\\.,? 2016", "shortCiteRegEx": "Zhang et al\\.", "year": 2016}, {"title": "Fast asynchronous parallel stochastic gradient descent: A lock-free approach with convergence guarantee", "author": ["Zhao", "S.-Y.", "Li", "W.-J."], "venue": "Proceedings of the AAAI Conference on Artificial Intelligence.", "citeRegEx": "Zhao et al\\.,? 2016", "shortCiteRegEx": "Zhao et al\\.", "year": 2016}], "referenceMentions": [{"referenceID": 18, "context": "Moreover, many other machine learning models, including neural networks (Krizhevsky, Sutskever, and Hinton 2012), matrix factorization (Koren, Bell, and Volinsky 2009), and principal component analysis (PCA) (Shamir 2015) and so on, can also be formulated as that in (1).", "startOffset": 208, "endOffset": 221}, {"referenceID": 17, "context": "Many works (Roux, Schmidt, and Bach 2012; Shalev-Shwartz and Zhang 2013; Johnson and Zhang 2013) have found that SGD-based methods can achieve promising performance in large-scale learning problems.", "startOffset": 11, "endOffset": 96}, {"referenceID": 7, "context": "Many works (Roux, Schmidt, and Bach 2012; Shalev-Shwartz and Zhang 2013; Johnson and Zhang 2013) have found that SGD-based methods can achieve promising performance in large-scale learning problems.", "startOffset": 11, "endOffset": 96}, {"referenceID": 17, "context": "When the problem in (1) is convex, the SGD methods, including SSGD (Roux, Schmidt, and Bach 2012; Shalev-Shwartz and Zhang 2013; Johnson and Zhang 2013), PSGD (Recht et al.", "startOffset": 67, "endOffset": 152}, {"referenceID": 7, "context": "When the problem in (1) is convex, the SGD methods, including SSGD (Roux, Schmidt, and Bach 2012; Shalev-Shwartz and Zhang 2013; Johnson and Zhang 2013), PSGD (Recht et al.", "startOffset": 67, "endOffset": 152}, {"referenceID": 13, "context": "When the problem in (1) is convex, the SGD methods, including SSGD (Roux, Schmidt, and Bach 2012; Shalev-Shwartz and Zhang 2013; Johnson and Zhang 2013), PSGD (Recht et al. 2011) and DSGD (Jaggi et al.", "startOffset": 159, "endOffset": 178}, {"referenceID": 6, "context": "2011) and DSGD (Jaggi et al. 2014; Li et al. 2014; Xing et al. 2015; Zhang, Zheng, and Kwok 2016), have achieved very promising empirical performance.", "startOffset": 15, "endOffset": 97}, {"referenceID": 10, "context": "2011) and DSGD (Jaggi et al. 2014; Li et al. 2014; Xing et al. 2015; Zhang, Zheng, and Kwok 2016), have achieved very promising empirical performance.", "startOffset": 15, "endOffset": 97}, {"referenceID": 21, "context": "2011) and DSGD (Jaggi et al. 2014; Li et al. 2014; Xing et al. 2015; Zhang, Zheng, and Kwok 2016), have achieved very promising empirical performance.", "startOffset": 15, "endOffset": 97}, {"referenceID": 10, "context": "Because many researchers (Li et al. 2014; Xing et al. 2015) find that the SGD methods can also achieve good empirical results for nonconvex problems, theoretical proof about the convergence of SGD methods for non-convex problems has recently attracted much attention.", "startOffset": 25, "endOffset": 59}, {"referenceID": 21, "context": "Because many researchers (Li et al. 2014; Xing et al. 2015) find that the SGD methods can also achieve good empirical results for nonconvex problems, theoretical proof about the convergence of SGD methods for non-convex problems has recently attracted much attention.", "startOffset": 25, "endOffset": 59}, {"referenceID": 3, "context": "For example, the works in (Ghadimi and Lan 2013; Reddi et al. 2016; Li et al. 2016; Allen-Zhu and Hazan 2016; Allen-Zhu and Yuan 2016) have proved the convergence of the sequential SGD and its variants for non-convex problems.", "startOffset": 26, "endOffset": 134}, {"referenceID": 14, "context": "For example, the works in (Ghadimi and Lan 2013; Reddi et al. 2016; Li et al. 2016; Allen-Zhu and Hazan 2016; Allen-Zhu and Yuan 2016) have proved the convergence of the sequential SGD and its variants for non-convex problems.", "startOffset": 26, "endOffset": 134}, {"referenceID": 11, "context": "For example, the works in (Ghadimi and Lan 2013; Reddi et al. 2016; Li et al. 2016; Allen-Zhu and Hazan 2016; Allen-Zhu and Yuan 2016) have proved the convergence of the sequential SGD and its variants for non-convex problems.", "startOffset": 26, "endOffset": 134}, {"referenceID": 0, "context": "For example, the works in (Ghadimi and Lan 2013; Reddi et al. 2016; Li et al. 2016; Allen-Zhu and Hazan 2016; Allen-Zhu and Yuan 2016) have proved the convergence of the sequential SGD and its variants for non-convex problems.", "startOffset": 26, "endOffset": 134}, {"referenceID": 1, "context": "For example, the works in (Ghadimi and Lan 2013; Reddi et al. 2016; Li et al. 2016; Allen-Zhu and Hazan 2016; Allen-Zhu and Yuan 2016) have proved the convergence of the sequential SGD and its variants for non-convex problems.", "startOffset": 26, "endOffset": 134}, {"referenceID": 18, "context": "There are also some other theoretical results for some particular non-convex problems, like PCA (Shamir 2015; 2016a; 2016b) and matrix factorization (Sa, Re, and Olukotun 2015).", "startOffset": 96, "endOffset": 123}, {"referenceID": 12, "context": "There have appeared only two works (Lian et al. 2015; Huo and Huang 2016) which propose PSGD methods for non-convex problems with theoretical proof of convergence.", "startOffset": 35, "endOffset": 73}, {"referenceID": 4, "context": "There have appeared only two works (Lian et al. 2015; Huo and Huang 2016) which propose PSGD methods for non-convex problems with theoretical proof of convergence.", "startOffset": 35, "endOffset": 73}, {"referenceID": 12, "context": "However, the PSGD methods in (Lian et al. 2015) need write-lock or atomic operation for the memory to prove the convergence 2.", "startOffset": 29, "endOffset": 47}, {"referenceID": 4, "context": "Similarly, the work in (Huo and Huang 2016) also does not prove the convergence for the lockfree case in our paper.", "startOffset": 23, "endOffset": 43}, {"referenceID": 13, "context": "Recent works (Recht et al. 2011; Chaturapruek, Duchi, and R\u00e9 2015; J. Reddi et al. 2015; Zhao and Li 2016) find that lock-free strategy based parallel SGD (LF-PSGD) methods can empirically outperform lockbased PSGD methods for multi-core systems.", "startOffset": 13, "endOffset": 106}, {"referenceID": 13, "context": "In this paper, we provide the theoretical proof about the convergence of two representative LF-PSGD methods, Hogwild! (Recht et al. 2011; Chaturapruek, Duchi, and R\u00e9 2015) and AsySVRG (Zhao and Li 2016), for non-convex problems.", "startOffset": 118, "endOffset": 171}, {"referenceID": 12, "context": "Although the implementation of AsySG-incon in (Lian et al. 2015) is lock-free, the theoretical analysis about the convergence of AsySG-incon is based on an assumption that no over-writing happens, i.", "startOffset": 46, "endOffset": 64}, {"referenceID": 13, "context": "Hogwild! for Non-Convex Problems The Hogwild! method (Recht et al. 2011) is listed in Algorithm 1.", "startOffset": 53, "endOffset": 72}, {"referenceID": 13, "context": "Please note that Hogwild! in (Recht et al. 2011) has several variants with locks or lock-free.", "startOffset": 29, "endOffset": 48}, {"referenceID": 7, "context": "AsySVRG provides a lock-free parallel strategy for the original sequential SVRG (Johnson and Zhang 2013).", "startOffset": 80, "endOffset": 104}, {"referenceID": 14, "context": "In the t outer-loop, similar to (Reddi et al. 2016), we define Rt,m as follows", "startOffset": 32, "endOffset": 51}, {"referenceID": 14, "context": "As recommended in (Reddi et al. 2016), we can take \u03b7 = \u03bc/n, \u03b2 = v/n with \u03b7 < \u03b2 (assuming n is large).", "startOffset": 18, "endOffset": 37}], "year": 2016, "abstractText": "Stochastic gradient descent (SGD) and its variants have attracted much attention in machine learning due to their efficiency and effectiveness for optimization. To handle largescale problems, researchers have recently proposed several lock-free strategy based parallel SGD (LF-PSGD) methods for multi-core systems. However, existing works have only proved the convergence of these LF-PSGD methods for convex problems. To the best of our knowledge, no work has proved the convergence of the LF-PSGD methods for nonconvex problems. In this paper, we provide the theoretical proof about the convergence of two representative LF-PSGD methods, Hogwild! and AsySVRG, for non-convex problems. Empirical results also show that both Hogwild! and AsySVRG are convergent on non-convex problems, which successfully verifies our theoretical results. Introduction Many machine learning models can be formulated as the following optimization problem:", "creator": "TeX"}}}