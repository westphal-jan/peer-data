{"id": "1612.03969", "review": {"conference": "iclr", "VERSION": "v1", "DATE_OF_SUBMISSION": "12-Dec-2016", "title": "Tracking the World State with Recurrent Entity Networks", "abstract": "We introduce a new model, the Recurrent Entity Network (EntNet). It is equipped with a dynamic long-term memory which allows it to maintain and update a representation of the state of the world as it receives new data. For language understanding tasks, it can reason on-the-fly as it reads text, not just when it is required to answer a question or respond as is the case for a Memory Network (Sukhbaatar et al., 2015). Like a Neural Turing Machine or Differentiable Neural Computer (Graves et al., 2014; 2016) it maintains a fixed size memory and can learn to perform location and content-based read and write operations. However, unlike those models it has a simple parallel architecture in which several memory locations can be updated simultaneously. The EntNet sets a new state-of-the-art on the bAbI tasks, and is the first method to solve all the tasks in the 10k training examples setting. We also demonstrate that it can solve a reasoning task which requires a large number of supporting facts, which other methods are not able to solve, and can generalize past its training horizon. It can also be practically used on large scale datasets such as Children's Book Test, where it obtains competitive performance, reading the story in a single pass.", "histories": [["v1", "Mon, 12 Dec 2016 23:29:40 GMT  (59kb)", "http://arxiv.org/abs/1612.03969v1", null], ["v2", "Sat, 25 Mar 2017 03:05:14 GMT  (60kb)", "http://arxiv.org/abs/1612.03969v2", null], ["v3", "Wed, 10 May 2017 16:52:56 GMT  (60kb)", "http://arxiv.org/abs/1612.03969v3", null]], "reviews": [], "SUBJECTS": "cs.CL", "authors": ["mikael henaff", "jason weston", "arthur szlam", "antoine bordes", "yann lecun"], "accepted": true, "id": "1612.03969"}, "pdf": {"name": "1612.03969.pdf", "metadata": {"source": "CRF", "title": null, "authors": [], "emails": ["mbh305@nyu.edu,", "jase@fb.com", "aszlam@fb.com", "abordes@fb.com", "yann@fb.com"], "sections": [{"heading": null, "text": "ar Xiv: 161 2.03 969v 1 [cs.C L] 12 Dec 201 6"}, {"heading": "1 INTRODUCTION", "text": "In fact, it is a way in which people are able to determine for themselves what they want and what they want."}, {"heading": "2 MODEL", "text": "Our model is designed to process data sequentially, and consists of three main parts: an input encoder, dynamic memory, and output layer, which we now describe in detail. We developed it in the context of answering questions about short stories where the input is sequences of words, but the model could be adapted to many other contexts."}, {"heading": "2.1 INPUT ENCODER", "text": "The encoding layer summarizes an element of the input sequence with a vector of fixed length. Typically, the input element at the time t consists of a sequence of words, such as a sentence or a window of words. It is up to you to choose the encoding module so that it is any standard sequence encoder that is an active field of research. Typical choices include a word bag representation (BoW) or the final state of a recursive neural network (RNN) that runs over the sequence. In this thesis, we use a simple encoder consisting of a learned multiplicative mask followed by a summation. Specifically, we do not allow the input to be a word sequence with embedding (e1,..., ek) at any time. The vector representation of this input is then: st =.ifi-ei (1) The same set of vectors {f1,... fk, step 2015 to be used with each other parameter, and a parameter to be learned)."}, {"heading": "2.2 DYNAMIC MEMORY", "text": "\"Dynamic memory is a gated recurrent network with a (partial) block structured weight binding scheme. We divide the hidden states of the network into blocks h1,..., hm; the complete hidden state is the concatenation of hj. In the experiments below, m is in the order of 5 to 20, and each block hj is in the order of 20 to 100 units. At each step t the contents of the hidden states {hj} (which we will call the jth memory) are updated using a set of key vectors {wj} and the encrypted input st. In its most general form, the updated equations of our model are hidden by: gj (s t hj) + s T wj) (2) h (Uhj + V wj + Wst) and the encrypted input st. (3) hj + gj (4) hj \u2190 hj | hj (5) Here, a sigmoid of the network is hidden, a function of the jj, which should be any block of memory that is orgenic."}, {"heading": "2.3 OUTPUT MODULE", "text": "The output module can be considered a one-hop storage network (Sukhbaatar et al., 2015) with an additional nonlinearity between the internal state and the decoder matrix. If the memory slots match certain words (as we will describe in the following section) that contain the response, p can be considered a distribution over potential responses and can be used to make a prediction directly or feed it into a loss function, eliminating the need for the last two steps. The entire model (all three components described above) is trained by back propagation over time and receives gradients from any time the reader needs to generate an output that is then rolled through the network."}, {"heading": "3 MOTIVATING EXAMPLE OF OPERATION", "text": "We describe a motivating example of how our model is able to capture the input sequences. Let's assume that our model reads a story so that the input sequences are natural sentences, and then it is necessary to answer questions about the position of the person carrying it or the people holding it in their hands."}, {"heading": "4 RELATED WORK", "text": "In fact, it is that we see ourselves as being able to assert ourselves, that we are able, that we are able to put ourselves at the top, and that we are able, that we are able to assert ourselves, that we are able to put ourselves at the top, \"he said."}, {"heading": "5 EXPERIMENTS", "text": "In this section, we evaluate our model using three different datasets. For training details common to all experiments, see Appendix A."}, {"heading": "5.1 SYNTHETIC WORLD MODEL TASK", "text": "We first examine the characteristics of our model on a toy task designed to measure the ability to hold a world model in memory. In this task, two agents are first randomly placed on a 10 x 10 grid, and at each step a randomly selected agent either changes direction or moves forward. After a certain number of time steps, the model must supply the locations of both agents, thereby showing its internal world model (details can be found in Appendix B.) This task is difficult because the model must combine supporting facts up to T \u2212 2 to answer the question correctly, and must also keep the locations of both agents in memory and update them at different times. We compare the performance of one MemN2N, LSTM and EntNet. For the MemN2N, we set the number of hops equal to T \u2212 2 and the embedding dimension to d = 20. EntNet had the embedding dimension d = 20 and 5 memory slots, and the LSTM had 50 hidden units, which led to both models being significantly repeated."}, {"heading": "5.2 BABI TASKS", "text": "Next, we evaluate our model of bAbI tasks, which are a collection of 20 synthetic question and answer data sets first introduced in (Weston et al., 2015) to test a variety of thinking skills, which have since become a benchmark for memory-enhanced neural networks and most of the related methods described in Section 4. Performance is measured by two metrics: the average error across all tasks, and the number of failed tasks (more than 5% error). We used version 1.2 of the data set with 10k samples.Training details We used a similar training setup as (Sukhbaatar et al., 2015).All models were searched with ADAM using a learning rate of \u03b7 = 0.01, which was divided by 2 all 25 epochs, up to 200 epochs. Copying previous works (Sukhbaatar et al., 2015; Xiong et al., 2016), the memory capacity was limited to 130 sets, except for the most recent ones, for the 3 sets."}, {"heading": "5.3 CHILDREN\u2019S BOOK TEST (CBT)", "text": "We have next presented our model on the Children's Book Test (Hill et al., 2016), which requires semantic language modeling (sentence completion) of children's books that are freely available to read 20 consecutive sentences in order to accomplish this task, but does not significantly alter the key terms, although they have been violated in a few cases (see Appendix C). Therefore, we do not have them able to fill in the story in Table 22www.gutenberg.orggiven story and use this context to fill in a missing word from the 21st sentence. More specifically, each sample consists of a tuple (S, C, a), where S is the story consisting of 20 sentences, Q is the 21st sentence replaced by a special empty token, C is a set of 10 candidate answers of the same type as the missing word (for example, common nouns or named entities), and a true answer is the true answer."}, {"heading": "6 CONCLUSION", "text": "Two closely related challenges in artificial intelligence are the development of models that can maintain an estimate of the state of a world with complex dynamics over long time scales, and of models that can predict the predictive development of the state of the world using partial observation. In this essay, we introduced the Recurrent Entity Network, a new model that represents a promising step toward the first goal. Our model is able to accurately track the state of the world while reading text stories, enabling it to define a new state of the art for bAbI tasks, the competitive benchmark of understanding history by being the first model to solve them all. We also demonstrated that our model is able to capture simple dynamics over long time scales, and is competitive to be competitive on a real dataset. Although our model was able to solve all bAbI tasks with the help of 10k training samples, we found that only if we have significantly decreased the number of training samples (see 1)."}, {"heading": "A TRAINING DETAILS", "text": "All models were implemented with Torch (Collobert et al., 2011). In all experiments, we initialized our model by using weights from a Gaussian distribution with a mean zero and a standard deviation of 0.1, with the exception of the PReLU slopes and encoder weights initialized to 1. Note that the PReLU initialization is related to two of the heuristics used in (Sukhbaatar et al., 2015), namely to start with a purely linear model and to add non-linear units to half of the hidden units. Our initialization allows the model to choose when and how much to enter into the nonlinear system. Initialization of the encoder weights to 1 corresponds to the beginning of a BoW encoding that the model can then change. Initial values of the memory slots were initialized to the key values that we considered conducive to performance. Optimization was carried out with GADD or set to a standard of 40 with a miniature size and a standard of 32 meters."}, {"heading": "B DETAILS OF WORLD MODEL EXPERIMENTS", "text": "Two agents are first placed randomly on a 10 x 10 grid with 100 different locations {(1, 1), (1, 2),... (9, 10), (10, 10)}. At each step, an agent is randomly selected. There are two types of actions: the agent can face a certain direction or precede a number of steps. Actions are scanned until a legal action is found by choosing either a change of direction or a move with equal probability. If he changes direction, the direction between north, south, east and west is chosen with equal probability. If he moves, the number of steps is randomly scanned between 1 and 5. A legal action is one in which the agent is not placed off the grid. Stories are given to the network in text form, an example of this is below. The first action after each agent is placed on the grid must face a given direction."}, {"heading": "Q1: where is agent1 ?", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "Q2: where is agent2 ?", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "A1: (2,9)", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "A2: (10,4)", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "C ADDITIONAL RESULTS ON BABI TASKS", "text": "We provide some additional experiments to the bAbI tasks in order to better understand the influence of architecture, weight binding and the amount of training data. Table 5 shows results when a simple BoW encoding is used for the input. In this case, EntNet still performs better than a MemN2N that uses the same encoding scheme, suggesting that the architecture has an important effect. Binding the key vectors to entities was not helpful and impaired performance for some tasks. Table 6 shows results when only 1k training samples are used. In this setting, EntNet performs worse than MemN2N."}], "references": [{"title": "Gatedattention readers for text comprehension", "author": ["Bhuwan Dhingra", "Hanxiao Liu", "William Cohen", "Salakhutdinov", "Ruslan"], "venue": "CoRR, abs/1606.01549,", "citeRegEx": "Dhingra et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Dhingra et al\\.", "year": 2016}, {"title": "Hierarchical memory networks", "author": ["Chandar", "Sarath", "Ahn", "Sungjin", "Larochelle", "Hugo", "Vincent", "Pascal", "Tesauro", "Gerald", "Bengio", "Yoshua"], "venue": "arXiv preprint arXiv:1605.07427,", "citeRegEx": "Chandar et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Chandar et al\\.", "year": 2016}, {"title": "On the properties of neural machine translation: Encoder-decoder approaches", "author": ["Cho", "Kyunghyun", "van Merrienboer", "Bart", "Bahdanau", "Dzmitry", "Bengio", "Yoshua"], "venue": "In Proceedings of SSST@EMNLP", "citeRegEx": "Cho et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Cho et al\\.", "year": 2014}, {"title": "Torch7: A matlab-like environment for machine", "author": ["Collobert", "Ronan", "Kavukcuoglu", "Koray", "Farabet", "Clment"], "venue": null, "citeRegEx": "Collobert et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Collobert et al\\.", "year": 2011}, {"title": "Attentionover-attention neural networks for reading comprehension", "author": ["Cui", "Yiming", "Chen", "Zhipeng", "Wei", "Si", "Wang", "Shijin", "Liu", "Ting", "Hu", "Guoping"], "venue": "CoRR, abs/1607.04423,", "citeRegEx": "Cui et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Cui et al\\.", "year": 2016}, {"title": "Hybrid computing using a neural network with dynamic external memory", "author": ["Graves", "Alex", "Wayne", "Greg", "Reynolds", "Malcolm", "Harley", "Tim", "Danihelka", "Ivo", "Grabska-Barwi\u0144ska", "Agnieszka", "Colmenarejo", "Sergio G\u00f3mez", "Grefenstette", "Edward", "Ramalho", "Tiago", "Agapiou", "John"], "venue": null, "citeRegEx": "Graves et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Graves et al\\.", "year": 2016}, {"title": "Learning to transduce with unbounded memory", "author": ["Grefenstette", "Edward", "Hermann", "Karl Moritz", "Suleyman", "Mustafa", "Blunsom", "Phil"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Grefenstette et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Grefenstette et al\\.", "year": 2015}, {"title": "Dynamic neural turing machines with soft and hard addressing schemes", "author": ["Gulcehre", "Caglar", "Chandar", "Sarath", "Cho", "Kyunghyun", "Bengio", "Yoshua"], "venue": "CoRR, abs/1607.00036,", "citeRegEx": "Gulcehre et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Gulcehre et al\\.", "year": 2016}, {"title": "Delving deep into rectifiers: Surpassing human-level performance on imagenet classification", "author": ["He", "Kaiming", "Zhang", "Xiangyu", "Ren", "Shaoqing", "Sun", "Jian"], "venue": "CoRR, abs/1502.01852,", "citeRegEx": "He et al\\.,? \\Q2015\\E", "shortCiteRegEx": "He et al\\.", "year": 2015}, {"title": "The goldilocks principle: Reading children\u2019s books with explicit memory representations", "author": ["Hill", "Felix", "Bordes", "Antoine", "Chopra", "Sumit", "Weston", "Jason"], "venue": "In Proceedings of the International Conference on Learning Representations", "citeRegEx": "Hill et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Hill et al\\.", "year": 2016}, {"title": "Long short-term memory", "author": ["Hochreiter", "Sepp", "Schmidhuber", "J\u00fcrgen"], "venue": "Neural Comput.,", "citeRegEx": "Hochreiter et al\\.,? \\Q1997\\E", "shortCiteRegEx": "Hochreiter et al\\.", "year": 1997}, {"title": "Inferring algorithmic patterns with stack-augmented recurrent nets", "author": ["Joulin", "Armand", "Mikolov", "Tomas"], "venue": "arXiv preprint arXiv:1503.01007,", "citeRegEx": "Joulin et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Joulin et al\\.", "year": 2015}, {"title": "Text understanding with the attention sum reader network", "author": ["Kadlec", "Rudolf", "Schmid", "Martin", "Bajgar", "Ondrej", "Kleindienst", "Jan"], "venue": "CoRR, abs/1603.01547,", "citeRegEx": "Kadlec et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Kadlec et al\\.", "year": 2016}, {"title": "Adam: A method for stochastic optimization", "author": ["Kingma", "Diederik P", "Ba", "Jimmy"], "venue": "CoRR, abs/1412.6980,", "citeRegEx": "Kingma et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Kingma et al\\.", "year": 2014}, {"title": "Learning physical intuition of block towers by example", "author": ["Lerer", "Adam", "Gross", "Sam", "Fergus", "Rob"], "venue": "In Proceedings of the 33nd International Conference on Machine Learning,", "citeRegEx": "Lerer et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Lerer et al\\.", "year": 2016}, {"title": "Gated graph sequence neural networks", "author": ["Li", "Yujia", "Tarlow", "Daniel", "Brockschmidt", "Marc", "Zemel", "Richard S"], "venue": "CoRR, abs/1511.05493,", "citeRegEx": "Li et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Li et al\\.", "year": 2015}, {"title": "Deep multi-scale video prediction beyond mean square", "author": ["Mathieu", "Micha\u00ebl", "Couprie", "Camille", "LeCun", "Yann"], "venue": "error. CoRR,", "citeRegEx": "Mathieu et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Mathieu et al\\.", "year": 2015}, {"title": "Key-value memory networks for directly reading documents", "author": ["Miller", "Alexander", "Fisch", "Adam", "Dodge", "Jesse", "Karimi", "Amir-Hossein", "Bordes", "Antoine", "Weston", "Jason"], "venue": "arXiv preprint arXiv:1606.03126,", "citeRegEx": "Miller et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Miller et al\\.", "year": 2016}, {"title": "Reasoning with memory augmented neural networks for language comprehension", "author": ["Munkhdalai", "Tsendsuren", "Yu", "Hong"], "venue": "CoRR, abs/1610.06454,", "citeRegEx": "Munkhdalai et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Munkhdalai et al\\.", "year": 2016}, {"title": "Endto-end memory networks", "author": ["Sukhbaatar", "Sainbayar", "szlam", "arthur", "Weston", "Jason", "Fergus", "Rob"], "venue": "Advances in Neural Information Processing Systems", "citeRegEx": "Sukhbaatar et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Sukhbaatar et al\\.", "year": 2015}, {"title": "Learning multiagent communication with backpropagation", "author": ["Sukhbaatar", "Sainbayar", "Szlam", "Arthur", "Fergus", "Rob"], "venue": "CoRR, abs/1605.07736,", "citeRegEx": "Sukhbaatar et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Sukhbaatar et al\\.", "year": 2016}, {"title": "Natural language comprehension with the epireader", "author": ["Trischler", "Adam", "Ye", "Zheng", "Yuan", "Xingdi", "Suleman", "Kaheer"], "venue": "CoRR, abs/1606.02270,", "citeRegEx": "Trischler et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Trischler et al\\.", "year": 2016}, {"title": "Dialog-based language learning", "author": ["Weston", "Jason"], "venue": "CoRR, abs/1604.06045,", "citeRegEx": "Weston and Jason.,? \\Q2016\\E", "shortCiteRegEx": "Weston and Jason.", "year": 2016}, {"title": "Towards ai-complete question answering: A set of prerequisite toy tasks", "author": ["Weston", "Jason", "Bordes", "Antoine", "Chopra", "Sumit", "Mikolov", "Tomas"], "venue": "CoRR, abs/1502.05698,", "citeRegEx": "Weston et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Weston et al\\.", "year": 2015}, {"title": "Dynamic memory networks for visual and textual question answering", "author": ["Xiong", "Caiming", "Merity", "Stephen", "Socher", "Richard"], "venue": "In ICML,", "citeRegEx": "Xiong et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Xiong et al\\.", "year": 2016}], "referenceMentions": [{"referenceID": 19, "context": "For language understanding tasks, it can reason on-the-fly as it reads text, not just when it is required to answer a question or respond as is the case for a Memory Network (Sukhbaatar et al., 2015).", "startOffset": 174, "endOffset": 199}, {"referenceID": 23, "context": "The EntNet is able to solve all 20 bAbI question-answering tasks (Weston et al., 2015), a popular benchmark of story understanding, which to our knowledge sets a new state-of-the-art.", "startOffset": 65, "endOffset": 86}, {"referenceID": 19, "context": "We also introduce a new reasoning task which, unlike the bAbI tasks, requires a model to use a large number of supporting facts to answer the question, and show that the EntNet outperforms both LSTMs and Memory Networks (Sukhbaatar et al., 2015) by a significant margin.", "startOffset": 220, "endOffset": 245}, {"referenceID": 9, "context": "Finally, our model also obtains competitive results on the Childrens Book Test (Hill et al., 2016), and performs best among models that read the text in a single pass before receiving knowledge of the question.", "startOffset": 79, "endOffset": 98}, {"referenceID": 19, "context": "Note that the model can choose to adopt a standard BoW representation by setting all weights in the multiplicative mask to 1, or can choose a positional encoding model as used in (Sukhbaatar et al., 2015).", "startOffset": 179, "endOffset": 204}, {"referenceID": 8, "context": "The function \u03c6 can be chosen from any number of activation functions, in our experiments we use either parametric ReLU non-linearities (He et al., 2015) or the identity.", "startOffset": 135, "endOffset": 152}, {"referenceID": 19, "context": "The output module can be viewed as a one-hop Memory Network (Sukhbaatar et al., 2015) with an additional non-linearity \u03c6 between the internal state and the decoder matrix.", "startOffset": 60, "endOffset": 85}, {"referenceID": 2, "context": "The EntNet is related to gated recurrent models such as the LSTM (Hochreiter & Schmidhuber, 1997) and GRU (Cho et al., 2014), which also use gates to fix or modify the information stored in the hidden state.", "startOffset": 106, "endOffset": 124}, {"referenceID": 19, "context": "Our model is similar to a Memory Network and its variants (Weston et al., 2014; Sukhbaatar et al., 2015; Chandar et al., 2016; Miller et al., 2016) in the way it produces an output using a softmax over blocks of hidden states, and our encoding layer is inspired by techniques used in those works.", "startOffset": 58, "endOffset": 147}, {"referenceID": 1, "context": "Our model is similar to a Memory Network and its variants (Weston et al., 2014; Sukhbaatar et al., 2015; Chandar et al., 2016; Miller et al., 2016) in the way it produces an output using a softmax over blocks of hidden states, and our encoding layer is inspired by techniques used in those works.", "startOffset": 58, "endOffset": 147}, {"referenceID": 17, "context": "Our model is similar to a Memory Network and its variants (Weston et al., 2014; Sukhbaatar et al., 2015; Chandar et al., 2016; Miller et al., 2016) in the way it produces an output using a softmax over blocks of hidden states, and our encoding layer is inspired by techniques used in those works.", "startOffset": 58, "endOffset": 147}, {"referenceID": 24, "context": "The Dynamic Memory Network of (Xiong et al., 2016) also performs updates via a recurrent model, however it links memories to input tokens and updates them sequentially rather than in parallel.", "startOffset": 30, "endOffset": 50}, {"referenceID": 15, "context": "The weight tying scheme and the parallel gated RNNs recall the gated graph network of (Li et al., 2015).", "startOffset": 86, "endOffset": 103}, {"referenceID": 20, "context": "The CommNN model of (Sukhbaatar et al., 2016) also uses a set of parallel recurrent models with tied weights, but differs from our model in their use of inter-network communication and the lack of a gating mechanism.", "startOffset": 20, "endOffset": 45}, {"referenceID": 6, "context": "Finally, there is another class of recent models that have a writeable memory arranged as (unbounded) stacks, linked lists or queues (Joulin & Mikolov, 2015; Grefenstette et al., 2015).", "startOffset": 133, "endOffset": 184}, {"referenceID": 23, "context": "We next evaluate our model on the bAbI tasks, which are a collection of 20 synthetic questionanswering datasets first introduced in (Weston et al., 2015) designed to test a wide variety of reasoning abilities.", "startOffset": 132, "endOffset": 153}, {"referenceID": 19, "context": "Training Details We used a similar training setup as (Sukhbaatar et al., 2015).", "startOffset": 53, "endOffset": 78}, {"referenceID": 19, "context": "Copying previous works (Sukhbaatar et al., 2015; Xiong et al., 2016), the capacity of the memory was limited to the most recent 70 sentences, except for task 3 which was limited to 130 sentences.", "startOffset": 23, "endOffset": 68}, {"referenceID": 24, "context": "Copying previous works (Sukhbaatar et al., 2015; Xiong et al., 2016), the capacity of the memory was limited to the most recent 70 sentences, except for task 3 which was limited to 130 sentences.", "startOffset": 23, "endOffset": 68}, {"referenceID": 19, "context": "In Table 2 we compare our model to various other state-of-the-art models in the literature: the larger MemN2N reported in the appendix of (Sukhbaatar et al., 2015), the Dynamic Memory Network of (Xiong et al.", "startOffset": 138, "endOffset": 163}, {"referenceID": 24, "context": ", 2015), the Dynamic Memory Network of (Xiong et al., 2016), the Dynamic Neural Turing Machine (Gulcehre et al.", "startOffset": 39, "endOffset": 59}, {"referenceID": 7, "context": ", 2016), the Dynamic Neural Turing Machine (Gulcehre et al., 2016), the Neural Turing Machine (Graves et al.", "startOffset": 43, "endOffset": 66}, {"referenceID": 5, "context": ", 2014) and the Differentiable Neural Computer (Graves et al., 2016).", "startOffset": 47, "endOffset": 68}, {"referenceID": 9, "context": "We next evaluated our model on the Children\u2019s Book Test (Hill et al., 2016), which is a semantic language modeling (sentence completion) benchmark built from children\u2019s books that are freely available from Project Gutenberg 2.", "startOffset": 56, "endOffset": 75}, {"referenceID": 9, "context": "It was shown in (Hill et al., 2016) that methods with limited memory such as LSTMs perform well on more frequent, syntax based words such as prepositions and verbs, being similar to human performance, but poorly relative to humans on more semantically meaningful words such as named entities and common nouns.", "startOffset": 16, "endOffset": 35}, {"referenceID": 9, "context": "Training Details We adopted the same window memory approach used in (Hill et al., 2016), where each input corresponds to a window of text from {w(i\u2212b\u22121/2).", "startOffset": 68, "endOffset": 87}, {"referenceID": 9, "context": "This can be explained by the fact that the maximum frequency baseline model in (Hill et al., 2016) has performance which is significantly higher than random, and including the normalization step hides this useful frequency-based information.", "startOffset": 79, "endOffset": 98}, {"referenceID": 9, "context": "In Table 4, we show the performance of the general EntNet, the simplified EntNet, as well as other single-pass models taken from (Hill et al., 2016).", "startOffset": 129, "endOffset": 148}, {"referenceID": 12, "context": "630 Attention Sum Reader (Kadlec et al., 2016) 0.", "startOffset": 25, "endOffset": 46}, {"referenceID": 21, "context": "639 EpiReader (Trischler et al., 2016) 0.", "startOffset": 14, "endOffset": 38}, {"referenceID": 4, "context": "674 AoA Reader (Cui et al., 2016) 0.", "startOffset": 15, "endOffset": 33}, {"referenceID": 14, "context": "Recent works have made some progress towards the second goal of forward modeling, for instance in capturing simple physics (Lerer et al., 2016), predicting future frames in video (Mathieu et al.", "startOffset": 123, "endOffset": 143}, {"referenceID": 16, "context": ", 2016), predicting future frames in video (Mathieu et al., 2015) or responses in dialog (Weston, 2016).", "startOffset": 43, "endOffset": 65}], "year": 2016, "abstractText": "We introduce a new model, the Recurrent Entity Network (EntNet). It is equipped with a dynamic long-term memory which allows it to maintain and update a representation of the state of the world as it receives new data. For language understanding tasks, it can reason on-the-fly as it reads text, not just when it is required to answer a question or respond as is the case for a Memory Network (Sukhbaatar et al., 2015). Like a Neural Turing Machine or Differentiable Neural Computer (Graves et al., 2014; 2016) it maintains a fixed size memory and can learn to perform location and content-based read and write operations. However, unlike those models it has a simple parallel architecture in which several memory locations can be updated simultaneously. The EntNet sets a new state-of-the-art on the bAbI tasks, and is the first method to solve all the tasks in the 10k training examples setting. We also demonstrate that it can solve a reasoning task which requires a large number of supporting facts, which other methods are not able to solve, and can generalize past its training horizon. It can also be practically used on large scale datasets such as Children\u2019s Book Test, where it obtains competitive performance, reading the story in a single pass.", "creator": "LaTeX with hyperref package"}}}