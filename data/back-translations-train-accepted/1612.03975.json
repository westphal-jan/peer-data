{"id": "1612.03975", "review": {"conference": "AAAI", "VERSION": "v1", "DATE_OF_SUBMISSION": "12-Dec-2016", "title": "ConceptNet 5.5: An Open Multilingual Graph of General Knowledge", "abstract": "Machine learning about language can be improved by supplying it with specific knowledge and sources of external information. We present here a new version of the linked open data resource ConceptNet that is particularly well suited to be used with modern NLP techniques such as word embeddings.", "histories": [["v1", "Mon, 12 Dec 2016 23:54:52 GMT  (154kb,D)", "http://arxiv.org/abs/1612.03975v1", null]], "reviews": [], "SUBJECTS": "cs.CL", "authors": ["robert speer", "joshua chin", "catherine havasi"], "accepted": true, "id": "1612.03975"}, "pdf": {"name": "1612.03975.pdf", "metadata": {"source": "CRF", "title": "ConceptNet 5.5: An Open Multilingual Graph of General Knowledge", "authors": ["Robert Speer", "Joshua Chin", "Catherine Havasi"], "emails": [], "sections": [{"heading": null, "text": "ConceptNet is a knowledge diagram that combines words and phrases of natural language with marked edges. Its knowledge is gathered from many sources, including expert-created resources, crowdsourcing and games with a specific purpose. It is designed to represent general knowledge related to understanding language and improve the use of natural language by enabling the application to better understand the meanings behind the words people use. If ConceptNet is combined with word embeddings acquired from distribution semantics (such as word2vec), it provides applications with understanding that they would not acquire from distribution semantics alone or from narrower resources such as WordNet or DBPedia. We demonstrate this using state-of-the-art results on intrinsic evaluations of word contexts, which translate into improvements in word vectors, including solving SAT-style analogies."}, {"heading": "Introduction", "text": "In this context, it is worth mentioning that this project is about a project that is about researching and understanding the foundations of human thought and thought. (...) It is about a project that is about researching the foundations of human thought. (...) It is about researching and understanding the foundations of human thought. (...) It is about researching the foundations of human thought and thought. (...) It is about researching the foundations of human thought. (...) It is about researching the foundations of human thought. (...) It is about researching the foundations of human thought. (...) It is about researching the foundations of human thought. (...) It is about researching the foundations of human thought. (...) It is about researching the foundations. (...) It is about researching the foundations of human thought and thought. (...) It is about researching the foundations of human thought. (...) It is about researching the foundations."}, {"heading": "Related Work", "text": "ConceptNet is the knowledge chart version of the Open Mind Common Sense Project (Singh 2002), a Common Sensear Xiv: 161 2.03 975v 1 [cs.C L] 12 Dec 201 6Knowledge base of the most basic things a person knows. It was last published as Bedside Techniques 5.2 (Speer and Havasi 2013). Many projects strive to create lexical resources of common knowledge. Cyc (Lenat and Guha 1989) has built an ontology of common sense in predicative logic form. DBPedia (Auer et al. 2007) extracts knowledge from Wikipedia infoboxes, which provide a large number of facts focusing on named entities that have Wikipedia articles. The Google Knowledge Graph (Singhal 2012) is perhaps the largest and most general knowledge graphic graphics graphics, although its contents are not freely available, most of which Wikipedia articles do not focus on. \"ConceptNet is a knowledge diagram that can be discussed.\""}, {"heading": "Structure of ConceptNet", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "Knowledge Sources", "text": "ConceptNet 5.5 is based on the following sources: \u2022 Facts from Open Mind Common Sense (OMCS) (Singh 2002) and sister projects in other languages (Anacleto et al. 2006) \u2022 Information from parsing Wiktionary, in multiple languages, using a custom parser (\"Wikiparsec\") \u2022 \"Games with a purpose\" for collecting general knowledge (by Ahn, Kedia and Blum 2006) (Nakahara and Yamada 2011) (Kuo et al. 2009) \u2022 Open Multilingual WordNet (Bond and Foster 2013), an interlinked data representation of WordNet (Miller et al. 1998) and its parallel projects in multiple languages \u2022 JMDict (Breen 2004), a multilingual dictionary \u2022 OpenCyc, a hierarchy of hypernyms provided by Cyc (Lenat and Guha 1989), a system that represents common sense in predictive quanta million net quanta million net quanta million (1 million net quanta subset of a million), and a large subset of English (1 peha)."}, {"heading": "Relations", "text": "ConceptNet uses a closed class of selected relationships such as IsA, UsedFor, and CapableOf, which aims to represent a relationship regardless of the language or source of the terms it connects. ConceptNet 5.5 aims to align its knowledge resources with its core set of 36 relationships, which are generalized relationships similar in purpose to the relationships of WordNet such as Hyponym and Meronym, as well as the qualification of the theory of the Generative Dictionary (Pustejovsky 1991). ConceptNet's edges are directed, but as a new feature in ConceptNet 5.5 some relationships are called symmetrical, such as SimilarTo. The directness of these edges is unimportant. The core relationships are: \u2022 Symmetrical relationships: Antonym, DistinctFrom, EtymologicallyRelatedTo, LocatedNear, RelatedTo, SimilarTo, and Synonym Haservent."}, {"heading": "Term Representation", "text": "ConceptNet represents terms in standardized form. The text is standardized in NFKC form2 using Python's Unicode data implementation using the tokenizer in the Python package wordfreq (Speer et al. 2016), which builds on the standard Unicode word segmentation algorithm. The tokens are associated with underscores, and this text is prepended with the URI / c / lang, where lang is the BCP-47 language code3 for the language in which the term is located. As an example, the English term \"United States\" is prepended to / c / en / united states.Relations have a separate namespace of URIs with / r, such as / r / PartOf. \"These relationships are given artificial names in English, but apply to all languages. The statement reached in Portuguese as\" O alimento e-usado-parcomeper \"is still prefixed with the terms\" Drive 4, \"Drive of States,\" in terms 1.org."}, {"heading": "Vocabulary", "text": "When building a knowledge diagram, deciding what a node should represent has a significant impact on the way the graph is used. It also has implications that cannot make linking and importing other resources trivial, as different resources make different decisions about their representation. In ConceptNet, a node is a word or phrase of a natural language, often a common word in its unique form. \"Lead\" in English is a term in ConceptNet that is represented by URI / c / en / lead, although it has multiple meanings. The advantage of ambiguous terms is that they can be easily extracted from the natural language, which is also ambiguous. This ambiguous representation is similar to that of systems that learn distributional semantics from text."}, {"heading": "Linked Data", "text": "ConceptNet imports knowledge from some other systems, such as WordNet, into its own representation. These other systems have their own target vocabularies, which must be aligned with ConceptNet, which is usually a sub-specific, many-to-many alignment. A term imported from another knowledge graph is linked to ConceptNet nodes via the relation ExternalURL and refers to an absolute URL that represents that term in that external resource. This newly introduced relationship preserves the origin of the data and allows research into what the untransformed data was. ConceptNet terms can also be presented as absolute URLs, so that ConceptNet can connect bidirectionally to the broader Linked Open Data ecosystem."}, {"heading": "Applying ConceptNet to Word Embeddings", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "Computing ConceptNet Embeddings Using PPMI", "text": "We can present the ConceptNet graph as a sparse, symmetrical term-term matrix. Each cell contains the sum of the weights of all the edges that connect the two corresponding terms. For performance reasons, we truncate the ConceptNet graph by discarding terms that are associated with less than three edges.We consider this matrix to be a representation of terms and their contexts. In a text corpus, the context of a term would be the terms that appear near the text; here, the context is the other nodes it is linked to in ConceptNet. We can calculate word embeddings directly from this sparse matrix by following the practical recommendations of Levy, Goldberg, and Dagan (2015). As in Levy et al., we determine the acute mutual information of the matrix entries with context-related smoothing, clicking the negative values to form the net-based networked concepts (such as the mutual PMI) with each other dimensions."}, {"heading": "Combining ConceptNet with Distributional Word Embeddings", "text": "Having created embeddings solely from ConceptNet, we now want to create a more robust series of embeddings that represent both ConceptNet and text-derived distributional word embeddings. Retrofitting (Faruqui et al. 2015) is a process that adapts an existing matrix of word embeddings using a knowledge diagram. Retrofitting infects new vectors Qi with the aim of being close to their original values, and also close to their neighbors in the diagram with edges E by minimizing this objective function: \"(Q) = n\" Qi = 1 \"Qi\" Qi \"(i, j)\" Qi-Qi \"and\" 2 \"Faruqui et al. give a simple iterative process to minimize this function via the vocabulary of the original embeddings Qi = n\" Qi = 1 \"Qi\" Qi \"Qi\" \"Qi\" \"\" \"Qi\" \"and\" The Qi \"process \u2212 Qi \u2212 to expand the Qi.\""}, {"heading": "Combining Multiple Sources of Embeddings", "text": "This is particularly useful because it is possible to build on publicly published matrices of embedding whose input data is not available or difficult to acquire. As described in the \"Related Work\" section, word2vec and GloVe both offer recommended pre-trained matrices. These matrices represent slightly different text areas and have complementary strengths, and the way we can benefit most from them is by using both as embedding. To do this, we apply the retrofit to both matrices and then find a global linear projection that aligns the results to their common vocabulary. This process was inspired by Zhao, Hassan and Auli (2015). We find the projection by linking the columns of the matrices and reducing them to 300 dimensions by deriving the word from the merged SVD."}, {"heading": "Evaluation", "text": "To compare the performance of fully featured word embedding systems, we will first compare their results to intrinsic word embedding ratings, and then apply the word embedding to the downstream tasks, solve proportional analogies, and select the meaningful end of a story to assess whether better embedding results in better performance for semantic tasks. The hybrid system described above is the system we call ConceptNet Numberbatch, with version number 16.09 indicating that it was built in September 2016. We will now compare the results of ConceptNet Numberbatch 16.09 with other systems that provide their word embedding, both those used to build ConceptNet Numberbatch, and a recently released system, LexVec, which was not built. The systems we evaluate are: \u2022 word2vec SGNS (Mikolov et al. 2013, Common Net News on Google VGN, 2014, and Pennnet Glocher Manning (Pennnet)"}, {"heading": "Evaluations of Word Relatedness", "text": "One way to evaluate the intrinsic performance of a semantic space is to ask it to rate the relativity of word pairs and compare its judgments with human judgments. 4 If a word in a pair is outside the vocabulary, it is assumed that the pair has a relativity of 0. Here, we focus on MEN-3000 (Bruni, Tran, and Baroni 2014), a large crowdsourced ranking of common words as measured by its Spearman correlation. Many gold standards of word relationship are in common use. We focus on MEN-3000 (Bruni, and Baroni 2014), a large crowdsourced ranking of common words; RW (Luong, Socher, and Manning 2013), a ranking of rare words; WordSim-353 et al. 2001, a smaller rating used as a benchmark for many methods."}, {"heading": "Solving SAT-style Analogies", "text": "This year it has come to the point that it will only be a matter of time before it will happen, until it does."}, {"heading": "An Evaluation of Common-Sense Stories", "text": "The Story Cloze Test (Mostafazadeh et al. 2016) is a more recent evaluation of semantic understanding that tests whether a"}, {"heading": "Evaluation Dev Test Final", "text": "This task is characterized by the fact that it is very challenging for computers, but very simple for humans, because it is based on implicit common sense. Most systems evaluated with the Story Cloze Test achieve only slightly above the random baseline of 50%, while human agreement is close to 100%. Our first attempt to apply ConceptNet Numberbatch to the Story Cloze Test is to use a very simple \"bag-of-vectors\" model, averaging the embedding of words in the sentence and selecting the end that is closest to the average. This allows us to directly compare it to one of the original results presented by Mostafazadeh et al., in which a bag of vectors using the implementation of word2vec is 53.9% on the testset.This bag-vectors model should not only use the knowledge of another event that is not only related to another event, but is sensitive to it, such as the implementation of 2."}, {"heading": "Results and Discussion", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "Word Relatedness", "text": "Figure 2 compares the performance of the systems we compared across all ratings. To evaluate the word correlations, the Y axis represents the Spearman correlation (\u03c1) and uses the Fisher transformation to calculate a 95% confidence interval, which assumes that the given word pairs are scanned using a non-observable larger set (Bonett and Wright 2000). For analogy and history ratings, the Y axis is merely the ratio of the questions correctly answered, with 95% confidence intervals calculated using the binomial exact test. Our system's results on all of these ratings appear in Table 1, including a development / test breakdown that does not show any obvious match. The \"Final\" column is intended for comparisons with other papers and is used in the graph. It uses the standard test set that other publications use when it exists (which is the case for MEN-Cloze 3000 and the other world, the understanding or the other data)."}, {"heading": "SAT Analogies", "text": "ConceptNet Numberbatch achieved the best results among the word embedding systems for SAT analogies, with 56.1% of the questions being correct (58.8% for the half announced for final tests).These analogies slightly exceed or exceed analogies based on other word embedding when evaluated in the same context as Figure 2.The analogies results match the performance of the best-in-class systems in this evaluation. Table 2 compares our results with the other systems presented in the section \"Solving SAT-Style Analogies\": BagPack (Herdag-Delen and Baroni 2009), the previous use of ConceptNet in this evaluation; LRA (Turney 2006), the system whose record has lasted for a decade, which spends nine days searching the web during its evaluation; and SuperSim (Turney 2013), the newer system, which held the record among closed systems."}, {"heading": "Story Cloze Test", "text": "The performance of our system in the Story Cloze test was acceptable but inconspicuous. ConceptNet Numberbatch chose the correct end 59.4% of the time, which is actually slightly better than all the results reported by Mostafazadeh et al. (2016), including the task-trained neural networks. However, we could also achieve a similar result by applying the same bag-of-vectors approach to other word embeddings. LexVec achieved the best score of 59.9%, with ConceptNet Numberbatch, GloVe and word2vec all within its confidence interval. This result should perhaps be reassuring for those aiming to improve the computational understanding of stories. A bag-of vectors approach may be slightly more successful than other approaches in choosing the right end to a story, but the performance of this approach has probably reached a plateau."}, {"heading": "Conclusion", "text": "We compared word embeddings that only represent distributional semantics (word2vec, GloVe, and LexVec), word embeddings that only represent relational knowledge (ConceptNet PPMI), and the combination of both (ConceptNet Numberbatch), and we showed that the whole is more than the sum of its parts. ConceptNet continues to be important in an area that increasingly focuses on word embeddings, because word embeddings can benefit from what ConceptNet knows. ConceptNet can correlate word embeddings more robustly and more strongly with human judgments, as demonstrated by the state-of-the-art results that ConceptNet Numberbatch achieves when it comes to aligning human annotators in multiple ratings. Any technique that builds on word embeddings should consider including a source of relational knowledge, or starting from a pre-trained series of word embeddings that take relational knowledge into account."}, {"heading": "Availability of the Code and Data", "text": "The ConceptNet 5.5 code and documentation can be found on GitHub at https: / / github.com / commonsense / conceptnet5, and the knowledge chart can be viewed at http: / / conceptnet.io The full build process and evaluation chart can be reproduced using the instructions included in the README file on using Snakemake, a data science build system (Koster and Rahmann 2012), and optional Docker Compose to reproduce the system environment.The version of the repository at the time of submission of this paper was labeled aai2017. ConceptNet Numberbatch Word Embedings resulting from this build process in September 2016 will be evaluated in this paper; they can be downloaded as pre-built embedded versions of https: / / github.com / commonsense / conceptnet-numberbatch, day 16.09."}, {"heading": "Acknowledgments", "text": "We would like to thank the tens of thousands of volunteers who have provided the crowd-sourced knowledge that makes ConceptNet possible, including contributors to Open Mind Common Sense and related projects, and contributors to Wikipedia and Wiktionary, which improve the knowledge of people and computers alike."}], "references": [{"title": "Can common sense uncover cultural differences in computer applications? In Artificial intelligence in theory and practice", "author": ["Anacleto"], "venue": null, "citeRegEx": "Anacleto,? \\Q2006\\E", "shortCiteRegEx": "Anacleto", "year": 2006}, {"title": "DBpedia: A nucleus for a web of open data", "author": ["Auer"], "venue": null, "citeRegEx": "Auer,? \\Q2007\\E", "shortCiteRegEx": "Auer", "year": 2007}, {"title": "and Foster", "author": ["F. Bond"], "venue": "R.", "citeRegEx": "Bond and Foster 2013", "shortCiteRegEx": null, "year": 2013}, {"title": "T", "author": ["D.G. Bonett", "Wright"], "venue": "A.", "citeRegEx": "Bonett and Wright 2000", "shortCiteRegEx": null, "year": 2000}, {"title": "Multimodal distributional semantics", "author": ["Tran Bruni", "E. Baroni 2014] Bruni", "N.-K. Tran", "M. Baroni"], "venue": "J. Artif. Intell. Res", "citeRegEx": "Bruni et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Bruni et al\\.", "year": 2014}, {"title": "N", "author": ["M. Faruqui", "J. Dodge", "S.K. Jauhar", "C. Dyer", "E. Hovy", "Smith"], "venue": "A.", "citeRegEx": "Faruqui et al. 2015", "shortCiteRegEx": null, "year": 2015}, {"title": "Placing search in context: The concept revisited", "author": ["Finkelstein"], "venue": "In Proceedings of the 10th international conference on World Wide Web,", "citeRegEx": "Finkelstein,? \\Q2001\\E", "shortCiteRegEx": "Finkelstein", "year": 2001}, {"title": "Large-scale learning of word relatedness with constraints", "author": ["Halawi"], "venue": "In Proceedings of the 18th ACM SIGKDD international conference on Knowledge discovery and data mining,", "citeRegEx": "Halawi,? \\Q2012\\E", "shortCiteRegEx": "Halawi", "year": 2012}, {"title": "and Mihalcea", "author": ["S. Hassan"], "venue": "R.", "citeRegEx": "Hassan and Mihalcea 2009", "shortCiteRegEx": null, "year": 2009}, {"title": "and Baroni", "author": ["A. Herda\u01e7delen"], "venue": "M.", "citeRegEx": "Herda\u01e7delen and Baroni 2009", "shortCiteRegEx": null, "year": 2009}, {"title": "and Rahmann", "author": ["J. K\u00f6ster"], "venue": "S.", "citeRegEx": "K\u00f6ster and Rahmann 2012", "shortCiteRegEx": null, "year": 2012}, {"title": "J", "author": ["Y.-L. Kuo", "J.-C. Lee", "K.-Y. Chiang", "R. Wang", "E. Shen", "C.-W. Chan", "Hsu"], "venue": "Y.-J.", "citeRegEx": "Kuo et al. 2009", "shortCiteRegEx": null, "year": 2009}, {"title": "R", "author": ["D.B. Lenat", "Guha"], "venue": "V.", "citeRegEx": "Lenat and Guha 1989", "shortCiteRegEx": null, "year": 1989}, {"title": "Improving distributional similarity with lessons learned from word embeddings. Transactions of the Association for Computational Linguistics 3:211\u2013225", "author": ["Goldberg Levy", "O. Dagan 2015] Levy", "Y. Goldberg", "I. Dagan"], "venue": null, "citeRegEx": "Levy et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Levy et al\\.", "year": 2015}, {"title": "and Singh", "author": ["H. Liu"], "venue": "P.", "citeRegEx": "Liu and Singh 2004", "shortCiteRegEx": null, "year": 2004}, {"title": "C", "author": ["M.-T. Luong", "R. Socher", "Manning"], "venue": "D.", "citeRegEx": "Luong. Socher. and Manning 2013", "shortCiteRegEx": null, "year": 2013}, {"title": "Efficient estimation of word representations in vector space. CoRR abs/1301.3781", "author": ["Mikolov"], "venue": null, "citeRegEx": "Mikolov,? \\Q2013\\E", "shortCiteRegEx": "Mikolov", "year": 2013}, {"title": "A corpus and cloze evaluation for deeper understanding of commonsense stories", "author": ["Mostafazadeh"], "venue": "In Proceedings of NAACL: Human Language Technologies,", "citeRegEx": "Mostafazadeh,? \\Q2016\\E", "shortCiteRegEx": "Mostafazadeh", "year": 2016}, {"title": "and Yamada", "author": ["K. Nakahara"], "venue": "S.", "citeRegEx": "Nakahara and Yamada 2011", "shortCiteRegEx": null, "year": 2011}, {"title": "Holographic embeddings of knowledge graphs", "author": ["Rosasco Nickel", "M. Poggio 2016] Nickel", "L. Rosasco", "T. Poggio"], "venue": null, "citeRegEx": "Nickel et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Nickel et al\\.", "year": 2016}, {"title": "C", "author": ["J. Pennington", "R. Socher", "Manning"], "venue": "D.", "citeRegEx": "Pennington. Socher. and Manning 2014", "shortCiteRegEx": null, "year": 2014}, {"title": "Enhancing the LexVec distributed word representation model using positional contexts and external memory. arXiv preprint arXiv:1606.01283", "author": ["Idiart Salle", "A. Villavicencio 2016] Salle", "M. Idiart", "A. Villavicencio"], "venue": null, "citeRegEx": "Salle et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Salle et al\\.", "year": 2016}, {"title": "and Chin", "author": ["R. Speer"], "venue": "J.", "citeRegEx": "Speer and Chin 2016", "shortCiteRegEx": null, "year": 2016}, {"title": "and Havasi", "author": ["R. Speer"], "venue": "C.", "citeRegEx": "Speer and Havasi 2013", "shortCiteRegEx": null, "year": 2013}, {"title": "P", "author": ["Turney"], "venue": "D.", "citeRegEx": "Turney 2006", "shortCiteRegEx": null, "year": 2006}, {"title": "P", "author": ["Turney"], "venue": "D.", "citeRegEx": "Turney 2013", "shortCiteRegEx": null, "year": 2013}, {"title": "Verbosity: a game for collecting common-sense facts", "author": ["Kedia von Ahn", "L. Blum 2006] von Ahn", "M. Kedia", "M. Blum"], "venue": "In Proceedings of the SIGCHI conference on Human Factors in computing systems,", "citeRegEx": "Ahn et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Ahn et al\\.", "year": 2006}, {"title": "and Guo", "author": ["M. Xiao"], "venue": "Y.", "citeRegEx": "Xiao and Guo 2014", "shortCiteRegEx": null, "year": 2014}, {"title": "Learning translation models from monolingual continuous representations", "author": ["Hassan Zhao", "K. Auli 2015] Zhao", "H. Hassan", "M. Auli"], "venue": "In Proceedings of NAACL", "citeRegEx": "Zhao et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Zhao et al\\.", "year": 2015}], "referenceMentions": [], "year": 2016, "abstractText": "Machine learning about language can be improved by supplying it with specific knowledge and sources of external information. We present here a new version of the linked open data resource ConceptNet that is particularly well suited to be used with modern NLP techniques such as word embeddings. ConceptNet is a knowledge graph that connects words and phrases of natural language with labeled edges. Its knowledge is collected from many sources that include expertcreated resources, crowd-sourcing, and games with a purpose. It is designed to represent the general knowledge involved in understanding language, improving natural language applications by allowing the application to better understand the meanings behind the words people use. When ConceptNet is combined with word embeddings acquired from distributional semantics (such as word2vec), it provides applications with understanding that they would not acquire from distributional semantics alone, nor from narrower resources such as WordNet or DBPedia. We demonstrate this with state-of-the-art results on intrinsic evaluations of word relatedness that translate into improvements on applications of word vectors, including solving SAT-style analogies.", "creator": "LaTeX with hyperref package"}}}