{"id": "1612.04426", "review": {"conference": "iclr", "VERSION": "v1", "DATE_OF_SUBMISSION": "13-Dec-2016", "title": "Improving Neural Language Models with a Continuous Cache", "abstract": "We propose an extension to neural network language models to adapt their prediction to the recent history. Our model is a simplified version of memory augmented networks, which stores past hidden activations as memory and accesses them through a dot product with the current hidden activation. This mechanism is very efficient and scales to very large memory sizes. We also draw a link between the use of external memory in neural network and cache models used with count based language models. We demonstrate on several language model datasets that our approach performs significantly better than recent memory augmented networks.", "histories": [["v1", "Tue, 13 Dec 2016 23:09:49 GMT  (197kb,D)", "http://arxiv.org/abs/1612.04426v1", "Submitted to ICLR 2017"]], "COMMENTS": "Submitted to ICLR 2017", "reviews": [], "SUBJECTS": "cs.CL cs.LG", "authors": ["edouard grave", "armand joulin", "nicolas usunier"], "accepted": true, "id": "1612.04426"}, "pdf": {"name": "1612.04426.pdf", "metadata": {"source": "CRF", "title": "IMPROVING NEURAL LANGUAGE MODELS WITH A CONTINUOUS CACHE", "authors": ["Edouard Grave", "Armand Joulin", "Nicolas Usunier"], "emails": ["egrave@fb.com", "ajoulin@fb.com", "usunier@fb.com"], "sections": [{"heading": "1 INTRODUCTION", "text": "We have a core problem in processing natural language, with many applications such as machine translation (Brown et al., 1993), speech recognition (Bahl et al., 1983) or dialog agents (Stolcke et al., 2000). While traditional neural networks have received language models in this area (Jozefowicz et al., 2016; Mikolov et al., 2010), they lack the ability to adapt to their recent history and limit their application to dynamic environments (Dodge et al., 2015). A more recent approach to solving this problem is to supplement these networks with external memory (Graves et al., 2014; Grefenstette et al., 2015; Joulin & Mikolov, 2015; Sukhbaatar et al., 2015). These models can potentially use their external to store new information and adapt to a changing environment."}, {"heading": "2 LANGUAGE MODELING", "text": "A language model is a probability distribution over sequences of words. Let V be the size of the vocabulary, each word is represented by a one-hot encoding vector x in RV = V, according to its index in the vocabulary language. Using the chain rule, the probability of a word sequence is often defined as the probability of condition over words., xT can be factorized asp (x1,..., xT).This conditional probability is traditionally approxiated with non-parameteric models on counting statistics (Goodman, 2001). In particular, smoothed N-gram models (Katz, 1987; Kneser & Ney, 1995) perform good performance in practice (Mikolov et al, 2011)."}, {"heading": "3 NEURAL CACHE MODEL", "text": "It uses the hidden representations to define a probability distribution over the words in the cache. As shown in Figure 1, we can then define a probability distribution over words stored in the cache and the word generated on the basis of this representation (we remind the reader that the vector in this encrypts the story,..., x1). We then define a probability distribution over words stored in the cache and the word generated on the basis of this representation (we remind the reader that the vector in this encrypts the story)."}, {"heading": "4 RELATED WORK", "text": "Unlike previous models of speech recognition (Kuhn, 1988; Kupiec, 1989; Kuhn & De Mori, 1990), these models have been transformed by Jelinek et al. (1991) into a smoothed trigram language model that reduces both the perplexity and error rates of words. Other adaptive language models have been proposed in the past: Kneser & Steinbiss et al. (1993) and Iyer & Ostendorf (1999) dynamically adapt the parameters of their model to recent history by using different weight interpolation programs. Bellegarda (2000) and Coccaro & Jurafsky use latent semantic analyses to adapt their models to the current context."}, {"heading": "5 EXPERIMENTS", "text": "In this section, we evaluate our method using different language modeling datasets that have different sizes and characteristics. On all datasets, we train a static recursive language model of neural networks with LSTM units. We then use the hidden representations of this model to obtain our cache, which is interpolated with the static LSTM model. We also evaluate a unigram cache model interpolated with the static model as a further baseline."}, {"heading": "5.1 SMALL SCALE EXPERIMENTS", "text": "In this section we describe experiments performed on two small datasets: the Penn Tree Bank (Marcus et al., 1993) and the wikitext2 (Merity et al., 2016) datasets. The Penn Tree Bank dataset consists of articles from the Wall Street Journal, contains 929k training marks, and has a vocabulary size of 10k. The wikitext2 dataset is derived from Wikipedia articles, contains 2M training marks, and has a vocabulary size of 33k. These datasets contain non-mixed documents and therefore require models to capture inter-sentence dependencies in order to perform well. Implementation details.We train recurring neural network language models with 1024 LSTM units, regulated with dropout (probability of suspending units equals 0.65). We use the adagrad algorithm, with a learning rate of 0.2, a batch size of 20, and a net weight of 0.0000s to 0.0000s uniformly."}, {"heading": "5.2 MEDIUM SCALE EXPERIMENTS", "text": "In this section, we describe experiments performed on two medium-sized datasets: text8 and wikitext103. Both datasets are from Wikipedia, but different pre-processing methods have been used. Text8 dataset contains 17M training marks and has a vocabulary of 44k words, while the wikitext103 dataset has a training set of 103M and a vocabulary of 267k words. We use the same setting as in the previous section, except for batch size (we use 128) and failure parameters (we use 0.45 for text8 and 0.25 for wikitext103). As both datasets have large vocabularies, we use adaptive softmax (Grave et al., 2016) for faster training (we use 128) and failure parameters (we report test perplexity as a function of cache size in Figure 4, for the kit section model and an uncache section)."}, {"heading": "5.3 EXPERIMENTS ON THE LAMBADA DATASET", "text": "Finally, we report on experiments with the Lambada dataset introduced by Paperno et al. (2016), which is a dataset of short passages from novels with the goal of predicting the last word of the snippet. This dataset was constructed in such a way that human subjects perfectly solve the task in its full context (about 4.6 sentences), but do not do so if only the sentence with the target word is given. Therefore, most modern language models fail on this dataset. The Lambada training set contains about 200 million tokens and has a vocabulary size of 93, 215. We report on the results of our method in Table 3 as well as on the performance of baselines from Paperno et al. (2016). Adding a neural cache model to the LSTM baseline greatly improves performance on the Lambada dataset. In addition, we note in Figure 5 that the best interpolation parameter between the static model and the cache model for both the development of the STM-based model and the current STM-based model is not set to the STM-based model either."}, {"heading": "6 CONCLUSION", "text": "We have introduced the neural cache model to complement neural language models with longer-term memory that dynamically updates word probabilities based on the long-term context. Neural cache can be added at negligible cost beyond a pre-trained language model. Our experiments on both speech model tasks and the sophisticated LAMBADA dataset show that significant performance gains can be expected by adding this external memory component. Technically, neural cache models are similar to some current memory-enhanced neural networks, such as pointer networks, but its specific design makes it possible to avoid learning the memory component, which makes the neural cache attractive because it can use larger cache sizes than memory-enlarged networks and can be applied just as easily as conventional caches based on counting."}], "references": [{"title": "Neural machine translation by jointly learning to align and translate", "author": ["Dzmitry Bahdanau", "Kyunghyun Cho", "Yoshua Bengio"], "venue": "arXiv preprint arXiv:1409.0473,", "citeRegEx": "Bahdanau et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Bahdanau et al\\.", "year": 2014}, {"title": "A maximum likelihood approach to continuous speech recognition", "author": ["Lalit R Bahl", "Frederick Jelinek", "Robert L Mercer"], "venue": null, "citeRegEx": "Bahl et al\\.,? \\Q1983\\E", "shortCiteRegEx": "Bahl et al\\.", "year": 1983}, {"title": "Exploiting latent semantic information in statistical language modeling", "author": ["Jerome R Bellegarda"], "venue": "Proceedings of the IEEE,", "citeRegEx": "Bellegarda.,? \\Q2000\\E", "shortCiteRegEx": "Bellegarda.", "year": 2000}, {"title": "A neural probabilistic language model", "author": ["Yoshua Bengio", "R\u00e9jean Ducharme", "Pascal Vincent", "Christian Jauvin"], "venue": null, "citeRegEx": "Bengio et al\\.,? \\Q2003\\E", "shortCiteRegEx": "Bengio et al\\.", "year": 2003}, {"title": "The mathematics of statistical machine translation: Parameter estimation", "author": ["Peter F Brown", "Vincent J Della Pietra", "Stephen A Della Pietra", "Robert L Mercer"], "venue": "Computational linguistics,", "citeRegEx": "Brown et al\\.,? \\Q1993\\E", "shortCiteRegEx": "Brown et al\\.", "year": 1993}, {"title": "A thorough examination of the cnn/daily mail reading comprehension", "author": ["Danqi Chen", "Jason Bolton", "Christopher D Manning"], "venue": "task. arXiv preprint arXiv:1606.02858,", "citeRegEx": "Chen et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Chen et al\\.", "year": 2016}, {"title": "Empirical evaluation of gated recurrent neural networks on sequence modeling", "author": ["Junyoung Chung", "Caglar Gulcehre", "KyungHyun Cho", "Yoshua Bengio"], "venue": "arXiv preprint arXiv:1412.3555,", "citeRegEx": "Chung et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Chung et al\\.", "year": 2014}, {"title": "Towards better integration of semantic predictors in statistical language modeling", "author": ["Noah Coccaro", "Daniel Jurafsky"], "venue": "In ICSLP. Citeseer,", "citeRegEx": "Coccaro and Jurafsky.,? \\Q1998\\E", "shortCiteRegEx": "Coccaro and Jurafsky.", "year": 1998}, {"title": "Adaptive language modeling using minimum discriminant estimation", "author": ["Stephen Della Pietra", "Vincent Della Pietra", "Robert L Mercer", "Salim Roukos"], "venue": "In Proceedings of the workshop on Speech and Natural Language,", "citeRegEx": "Pietra et al\\.,? \\Q1992\\E", "shortCiteRegEx": "Pietra et al\\.", "year": 1992}, {"title": "Evaluating prerequisite qualities for learning end-to-end dialog systems", "author": ["Jesse Dodge", "Andreea Gane", "Xiang Zhang", "Antoine Bordes", "Sumit Chopra", "Alexander Miller", "Arthur Szlam", "Jason Weston"], "venue": "arXiv preprint arXiv:1511.06931,", "citeRegEx": "Dodge et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Dodge et al\\.", "year": 2015}, {"title": "Adaptive subgradient methods for online learning and stochastic optimization", "author": ["John Duchi", "Elad Hazan", "Yoram Singer"], "venue": null, "citeRegEx": "Duchi et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Duchi et al\\.", "year": 2011}, {"title": "Finding structure in time", "author": ["Jeffrey L Elman"], "venue": "Cognitive science,", "citeRegEx": "Elman.,? \\Q1990\\E", "shortCiteRegEx": "Elman.", "year": 1990}, {"title": "A theoretically grounded application of dropout in recurrent neural networks", "author": ["Yarin Gal", "Zoubin Ghahramani"], "venue": "arXiv preprint arXiv:1512.05287,", "citeRegEx": "Gal and Ghahramani.,? \\Q2015\\E", "shortCiteRegEx": "Gal and Ghahramani.", "year": 2015}, {"title": "A bit of progress in language modeling", "author": ["Joshua T Goodman"], "venue": "Computer Speech & Language,", "citeRegEx": "Goodman.,? \\Q2001\\E", "shortCiteRegEx": "Goodman.", "year": 2001}, {"title": "Efficient softmax approximation for gpus", "author": ["Edouard Grave", "Armand Joulin", "Moustapha Ciss\u00e9", "David Grangier", "Herv\u00e9 J\u00e9gou"], "venue": "arXiv preprint arXiv:1609.04309,", "citeRegEx": "Grave et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Grave et al\\.", "year": 2016}, {"title": "Speech recognition with deep recurrent neural networks", "author": ["A. Graves", "A. Mohamed", "G. Hinton"], "venue": "In ICASSP,", "citeRegEx": "Graves et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Graves et al\\.", "year": 2013}, {"title": "Neural turing machines", "author": ["Alex Graves", "Greg Wayne", "Ivo Danihelka"], "venue": "arXiv preprint arXiv:1410.5401,", "citeRegEx": "Graves et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Graves et al\\.", "year": 2014}, {"title": "Learning to transduce with unbounded memory", "author": ["Edward Grefenstette", "Karl Moritz Hermann", "Mustafa Suleyman", "Phil Blunsom"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Grefenstette et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Grefenstette et al\\.", "year": 2015}, {"title": "Pointing the unknown words", "author": ["Caglar Gulcehre", "Sungjin Ahn", "Ramesh Nallapati", "Bowen Zhou", "Yoshua Bengio"], "venue": "arXiv preprint arXiv:1603.08148,", "citeRegEx": "Gulcehre et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Gulcehre et al\\.", "year": 2016}, {"title": "Teaching machines to read and comprehend", "author": ["Karl Moritz Hermann", "Tomas Kocisky", "Edward Grefenstette", "Lasse Espeholt", "Will Kay", "Mustafa Suleyman", "Phil Blunsom"], "venue": "In NIPS,", "citeRegEx": "Hermann et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Hermann et al\\.", "year": 2015}, {"title": "Long short-term memory", "author": ["Sepp Hochreiter", "J\u00fcrgen Schmidhuber"], "venue": "Neural computation,", "citeRegEx": "Hochreiter and Schmidhuber.,? \\Q1997\\E", "shortCiteRegEx": "Hochreiter and Schmidhuber.", "year": 1997}, {"title": "Modeling long distance dependence in language: Topic mixtures versus dynamic cache models", "author": ["Rukmini M Iyer", "Mari Ostendorf"], "venue": "IEEE Transactions on speech and audio processing,", "citeRegEx": "Iyer and Ostendorf.,? \\Q1999\\E", "shortCiteRegEx": "Iyer and Ostendorf.", "year": 1999}, {"title": "A dynamic language model for speech recognition", "author": ["Frederick Jelinek", "Bernard Merialdo", "Salim Roukos", "Martin Strauss"], "venue": "In HLT,", "citeRegEx": "Jelinek et al\\.,? \\Q1991\\E", "shortCiteRegEx": "Jelinek et al\\.", "year": 1991}, {"title": "Inferring algorithmic patterns with stack-augmented recurrent nets", "author": ["Armand Joulin", "Tomas Mikolov"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Joulin and Mikolov.,? \\Q2015\\E", "shortCiteRegEx": "Joulin and Mikolov.", "year": 2015}, {"title": "Exploring the limits of language modeling", "author": ["Rafal Jozefowicz", "Oriol Vinyals", "Mike Schuster", "Noam Shazeer", "Yonghui Wu"], "venue": "arXiv preprint arXiv:1602.02410,", "citeRegEx": "Jozefowicz et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Jozefowicz et al\\.", "year": 2016}, {"title": "Text understanding with the attention sum reader network", "author": ["Rudolf Kadlec", "Martin Schmid", "Ondrej Bajgar", "Jan Kleindienst"], "venue": "arXiv preprint arXiv:1603.01547,", "citeRegEx": "Kadlec et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Kadlec et al\\.", "year": 2016}, {"title": "Estimation of probabilities from sparse data for the language model component of a speech recognizer", "author": ["Slava M Katz"], "venue": null, "citeRegEx": "Katz.,? \\Q1987\\E", "shortCiteRegEx": "Katz.", "year": 1987}, {"title": "Maximum entropy techniques for exploiting syntactic, semantic and collocational dependencies in language modeling", "author": ["Sanjeev Khudanpur", "Jun Wu"], "venue": "Computer Speech & Language,", "citeRegEx": "Khudanpur and Wu.,? \\Q2000\\E", "shortCiteRegEx": "Khudanpur and Wu.", "year": 2000}, {"title": "Improved backing-off for m-gram language modeling", "author": ["Reinhard Kneser", "Hermann Ney"], "venue": "In ICASSP,", "citeRegEx": "Kneser and Ney.,? \\Q1995\\E", "shortCiteRegEx": "Kneser and Ney.", "year": 1995}, {"title": "On the dynamic adaptation of stochastic language models", "author": ["Reinhard Kneser", "Volker Steinbiss"], "venue": "In ICASSP,", "citeRegEx": "Kneser and Steinbiss.,? \\Q1993\\E", "shortCiteRegEx": "Kneser and Steinbiss.", "year": 1993}, {"title": "Speech recognition and the frequency of recently used words: A modified markov model for natural language", "author": ["Roland Kuhn"], "venue": "In Proceedings of the 12th conference on Computational linguistics-Volume", "citeRegEx": "Kuhn.,? \\Q1988\\E", "shortCiteRegEx": "Kuhn.", "year": 1988}, {"title": "A cache-based natural language model for speech recognition", "author": ["Roland Kuhn", "Renato De Mori"], "venue": null, "citeRegEx": "Kuhn and Mori.,? \\Q1990\\E", "shortCiteRegEx": "Kuhn and Mori.", "year": 1990}, {"title": "Probabilistic models of short and long distance word dependencies in running text", "author": ["Julien Kupiec"], "venue": "In Proceedings of the workshop on Speech and Natural Language,", "citeRegEx": "Kupiec.,? \\Q1989\\E", "shortCiteRegEx": "Kupiec.", "year": 1989}, {"title": "Trigger-based language models: A maximum entropy approach", "author": ["Raymond Lau", "Ronald Rosenfeld", "Salim Roukos"], "venue": "In ICASSP,", "citeRegEx": "Lau et al\\.,? \\Q1993\\E", "shortCiteRegEx": "Lau et al\\.", "year": 1993}, {"title": "Building a large annotated corpus of english: The penn treebank", "author": ["Mitchell P Marcus", "Mary Ann Marcinkiewicz", "Beatrice Santorini"], "venue": "Computational linguistics,", "citeRegEx": "Marcus et al\\.,? \\Q1993\\E", "shortCiteRegEx": "Marcus et al\\.", "year": 1993}, {"title": "Pointer sentinel mixture models", "author": ["Stephen Merity", "Caiming Xiong", "James Bradbury", "Richard Socher"], "venue": "arXiv preprint arXiv:1609.07843,", "citeRegEx": "Merity et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Merity et al\\.", "year": 2016}, {"title": "Context dependent recurrent neural network language model", "author": ["Tomas Mikolov", "Geoffrey Zweig"], "venue": "In SLT,", "citeRegEx": "Mikolov and Zweig.,? \\Q2012\\E", "shortCiteRegEx": "Mikolov and Zweig.", "year": 2012}, {"title": "Recurrent neural network based language model", "author": ["Tomas Mikolov", "Martin Karafi\u00e1t", "Lukas Burget", "Jan Cernock\u1ef3", "Sanjeev Khudanpur"], "venue": "In INTERSPEECH,", "citeRegEx": "Mikolov et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Mikolov et al\\.", "year": 2010}, {"title": "Empirical evaluation and combination of advanced language modeling techniques", "author": ["Tomas Mikolov", "Anoop Deoras", "Stefan Kombrink", "Lukas Burget", "Jan Cernock\u1ef3"], "venue": "In INTERSPEECH,", "citeRegEx": "Mikolov et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Mikolov et al\\.", "year": 2011}, {"title": "Learning longer memory in recurrent neural networks", "author": ["Tomas Mikolov", "Armand Joulin", "Sumit Chopra", "Michael Mathieu", "Marc\u2019Aurelio Ranzato"], "venue": "arXiv preprint arXiv:1412.7753,", "citeRegEx": "Mikolov et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Mikolov et al\\.", "year": 2014}, {"title": "The lambada dataset: Word prediction requiring a broad discourse context", "author": ["Denis Paperno", "Germ\u00e1n Kruszewski", "Angeliki Lazaridou", "Quan Ngoc Pham", "Raffaella Bernardi", "Sandro Pezzelle", "Marco Baroni", "Gemma Boleda", "Raquel Fern\u00e1ndez"], "venue": "arXiv preprint arXiv:1606.06031,", "citeRegEx": "Paperno et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Paperno et al\\.", "year": 2016}, {"title": "A maximum entropy approach to adaptive statistical language modeling", "author": ["Ronald Rosenfeld"], "venue": "Computer, Speech and Language,", "citeRegEx": "Rosenfeld.,? \\Q1996\\E", "shortCiteRegEx": "Rosenfeld.", "year": 1996}, {"title": "Dialogue act modeling for automatic tagging and recognition of conversational speech", "author": ["Andreas Stolcke", "Noah Coccaro", "Rebecca Bates", "Paul Taylor", "Carol Van Ess-Dykema", "Klaus Ries", "Elizabeth Shriberg", "Daniel Jurafsky", "Rachel Martin", "Marie Meteer"], "venue": "Computational linguistics,", "citeRegEx": "Stolcke et al\\.,? \\Q2000\\E", "shortCiteRegEx": "Stolcke et al\\.", "year": 2000}, {"title": "End-to-end memory networks", "author": ["Sainbayar Sukhbaatar", "Szlam Arthur", "Jason Weston", "Rob Fergus"], "venue": "In NIPS,", "citeRegEx": "Sukhbaatar et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Sukhbaatar et al\\.", "year": 2015}, {"title": "Larger-context language modelling", "author": ["Tian Wang", "Kyunghyun Cho"], "venue": "arXiv preprint arXiv:1511.03729,", "citeRegEx": "Wang and Cho.,? \\Q2015\\E", "shortCiteRegEx": "Wang and Cho.", "year": 2015}, {"title": "Backpropagation through time: what it does and how to do", "author": ["Paul J Werbos"], "venue": null, "citeRegEx": "Werbos.,? \\Q1990\\E", "shortCiteRegEx": "Werbos.", "year": 1990}, {"title": "An efficient gradient-based algorithm for on-line training of recurrent network trajectories", "author": ["Ronald J Williams", "Jing Peng"], "venue": "Neural computation,", "citeRegEx": "Williams and Peng.,? \\Q1990\\E", "shortCiteRegEx": "Williams and Peng.", "year": 1990}, {"title": "Recurrent neural network regularization", "author": ["Wojciech Zaremba", "Ilya Sutskever", "Oriol Vinyals"], "venue": "arXiv preprint arXiv:1409.2329,", "citeRegEx": "Zaremba et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Zaremba et al\\.", "year": 2014}, {"title": "Recurrent highway networks", "author": ["Julian Georg Zilly", "Rupesh Kumar Srivastava", "Jan Koutn\u00edk", "J\u00fcrgen Schmidhuber"], "venue": "arXiv preprint arXiv:1607.03474,", "citeRegEx": "Zilly et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Zilly et al\\.", "year": 2016}], "referenceMentions": [{"referenceID": 4, "context": "Language modeling is a core problem in natural language processing, with many applications such as machine translation (Brown et al., 1993), speech recognition (Bahl et al.", "startOffset": 119, "endOffset": 139}, {"referenceID": 1, "context": ", 1993), speech recognition (Bahl et al., 1983) or dialogue agents (Stolcke et al.", "startOffset": 28, "endOffset": 47}, {"referenceID": 42, "context": ", 1983) or dialogue agents (Stolcke et al., 2000).", "startOffset": 27, "endOffset": 49}, {"referenceID": 24, "context": "While traditional neural networks language models have obtained stateof-the-art performance in this domain (Jozefowicz et al., 2016; Mikolov et al., 2010), they lack the capacity to adapt to their recent history, limiting their application to dynamic environments (Dodge et al.", "startOffset": 107, "endOffset": 154}, {"referenceID": 37, "context": "While traditional neural networks language models have obtained stateof-the-art performance in this domain (Jozefowicz et al., 2016; Mikolov et al., 2010), they lack the capacity to adapt to their recent history, limiting their application to dynamic environments (Dodge et al.", "startOffset": 107, "endOffset": 154}, {"referenceID": 9, "context": ", 2010), they lack the capacity to adapt to their recent history, limiting their application to dynamic environments (Dodge et al., 2015).", "startOffset": 117, "endOffset": 137}, {"referenceID": 16, "context": "A recent approach to solve this problem is to augment these networks with an external memory (Graves et al., 2014; Grefenstette et al., 2015; Joulin & Mikolov, 2015; Sukhbaatar et al., 2015).", "startOffset": 93, "endOffset": 190}, {"referenceID": 17, "context": "A recent approach to solve this problem is to augment these networks with an external memory (Graves et al., 2014; Grefenstette et al., 2015; Joulin & Mikolov, 2015; Sukhbaatar et al., 2015).", "startOffset": 93, "endOffset": 190}, {"referenceID": 43, "context": "A recent approach to solve this problem is to augment these networks with an external memory (Graves et al., 2014; Grefenstette et al., 2015; Joulin & Mikolov, 2015; Sukhbaatar et al., 2015).", "startOffset": 93, "endOffset": 190}, {"referenceID": 43, "context": "While these networks have obtained promising results on language modeling datasets (Sukhbaatar et al., 2015), they are quite computationally expensive.", "startOffset": 83, "endOffset": 108}, {"referenceID": 16, "context": "Typically, they have to learn a parametrizable mechanism to read or write to memory cells (Graves et al., 2014; Joulin & Mikolov, 2015).", "startOffset": 90, "endOffset": 135}, {"referenceID": 40, "context": "We demonstrate the quality of the Neural Cache models on several language model tasks and the LAMBADA dataset (Paperno et al., 2016).", "startOffset": 110, "endOffset": 132}, {"referenceID": 1, "context": ", 1993), speech recognition (Bahl et al., 1983) or dialogue agents (Stolcke et al., 2000). While traditional neural networks language models have obtained stateof-the-art performance in this domain (Jozefowicz et al., 2016; Mikolov et al., 2010), they lack the capacity to adapt to their recent history, limiting their application to dynamic environments (Dodge et al., 2015). A recent approach to solve this problem is to augment these networks with an external memory (Graves et al., 2014; Grefenstette et al., 2015; Joulin & Mikolov, 2015; Sukhbaatar et al., 2015). These models can potentially use their external memory to store new information and adapt to a changing environment. While these networks have obtained promising results on language modeling datasets (Sukhbaatar et al., 2015), they are quite computationally expensive. Typically, they have to learn a parametrizable mechanism to read or write to memory cells (Graves et al., 2014; Joulin & Mikolov, 2015). This may limit both the size of their usable memory as well as the quantity of data they can be trained on. In this work, we propose a very light-weight alternative that shares some of the properties of memory augmented networks, notably the capability to dynamically adapt over time. By minimizing the computation burden of the memory, we are able to use larger memory and scale to bigger datasets. We observe in practice that this allows us to surpass the perfomance of memory augmented networks on different language modeling tasks. Our model share some similarities with a model proposed by Kuhn (1988), called the cache model.", "startOffset": 29, "endOffset": 1582}, {"referenceID": 1, "context": "Language modeling is often framed as learning the conditional probability over words, given the history (Bahl et al., 1983).", "startOffset": 104, "endOffset": 123}, {"referenceID": 13, "context": "This conditional probability is traditionally approximated with non-parameteric models based on counting statistics (Goodman, 2001).", "startOffset": 116, "endOffset": 131}, {"referenceID": 26, "context": "In particular, smoothed N-gram models (Katz, 1987; Kneser & Ney, 1995) achieve good performance in practice (Mikolov et al.", "startOffset": 38, "endOffset": 70}, {"referenceID": 38, "context": "In particular, smoothed N-gram models (Katz, 1987; Kneser & Ney, 1995) achieve good performance in practice (Mikolov et al., 2011).", "startOffset": 108, "endOffset": 130}, {"referenceID": 41, "context": "Parametrized alternatives are either maximum entropy language models (Rosenfeld, 1996), feedforward networks (Bengio et al.", "startOffset": 69, "endOffset": 86}, {"referenceID": 3, "context": "Parametrized alternatives are either maximum entropy language models (Rosenfeld, 1996), feedforward networks (Bengio et al., 2003) or recurrent networks (Mikolov et al.", "startOffset": 109, "endOffset": 130}, {"referenceID": 37, "context": ", 2003) or recurrent networks (Mikolov et al., 2010).", "startOffset": 30, "endOffset": 52}, {"referenceID": 24, "context": "In particular, recurrent networks are currently the best solution to approximate this conditional probability, achieving state-of-the-arts performance on standard language modeling benchmarks (Jozefowicz et al., 2016; Zilly et al., 2016).", "startOffset": 192, "endOffset": 237}, {"referenceID": 48, "context": "In particular, recurrent networks are currently the best solution to approximate this conditional probability, achieving state-of-the-arts performance on standard language modeling benchmarks (Jozefowicz et al., 2016; Zilly et al., 2016).", "startOffset": 192, "endOffset": 237}, {"referenceID": 11, "context": "Several architecture for recurrent networks have been proposed, such as the Elman network (Elman, 1990), the long short-term memory (LSTM) (Hochreiter & Schmidhuber, 1997) or the gated recurrent unit (GRU) (Chung et al.", "startOffset": 90, "endOffset": 103}, {"referenceID": 6, "context": "Several architecture for recurrent networks have been proposed, such as the Elman network (Elman, 1990), the long short-term memory (LSTM) (Hochreiter & Schmidhuber, 1997) or the gated recurrent unit (GRU) (Chung et al., 2014).", "startOffset": 206, "endOffset": 226}, {"referenceID": 11, "context": "One of the simplest recurrent networks is the Elman network (Elman, 1990), where ht = \u03c3 (Lxt +Rht\u22121) , where \u03c3 is a non-linearity such as the logistic or tanh functions, L \u2208 Rd\u00d7V is a word embedding matrix and R \u2208 Rd\u00d7d is the recurrent matrix.", "startOffset": 60, "endOffset": 73}, {"referenceID": 24, "context": "The LSTM architecture is particularly interesting in the context of language modelling (Jozefowicz et al., 2016) and we refer the reader to Graves et al.", "startOffset": 87, "endOffset": 112}, {"referenceID": 10, "context": "This objective function is usually minimized by using the stochastic gradient descent algorithm, or variants such as Adagrad (Duchi et al., 2011).", "startOffset": 125, "endOffset": 145}, {"referenceID": 45, "context": "The gradient is computed using the truncated backpropagation through time algorithm (Werbos, 1990; Williams & Peng, 1990).", "startOffset": 84, "endOffset": 121}, {"referenceID": 6, "context": "Several architecture for recurrent networks have been proposed, such as the Elman network (Elman, 1990), the long short-term memory (LSTM) (Hochreiter & Schmidhuber, 1997) or the gated recurrent unit (GRU) (Chung et al., 2014). One of the simplest recurrent networks is the Elman network (Elman, 1990), where ht = \u03c3 (Lxt +Rht\u22121) , where \u03c3 is a non-linearity such as the logistic or tanh functions, L \u2208 Rd\u00d7V is a word embedding matrix and R \u2208 Rd\u00d7d is the recurrent matrix. The LSTM architecture is particularly interesting in the context of language modelling (Jozefowicz et al., 2016) and we refer the reader to Graves et al. (2013) for details on this architecture.", "startOffset": 207, "endOffset": 633}, {"referenceID": 47, "context": "3 LSTM (Zaremba et al., 2014) 78.", "startOffset": 7, "endOffset": 29}, {"referenceID": 48, "context": "4 Recurrent Highway Network (Zilly et al., 2016) 66.", "startOffset": 28, "endOffset": 48}, {"referenceID": 35, "context": "0 Pointer Sentinel LSTM (Merity et al., 2016) 70.", "startOffset": 24, "endOffset": 45}, {"referenceID": 30, "context": "Adding a cache to a language model was intoducted in the context of speech recognition(Kuhn, 1988; Kupiec, 1989; Kuhn & De Mori, 1990).", "startOffset": 86, "endOffset": 134}, {"referenceID": 32, "context": "Adding a cache to a language model was intoducted in the context of speech recognition(Kuhn, 1988; Kupiec, 1989; Kuhn & De Mori, 1990).", "startOffset": 86, "endOffset": 134}, {"referenceID": 21, "context": "These models were further extended by Jelinek et al. (1991) into a smoothed trigram language model, reporting reduction in both perplexity and word error rates.", "startOffset": 38, "endOffset": 60}, {"referenceID": 8, "context": "Della Pietra et al. (1992) adapt the cache to a general n-gram model such that it satisfies marginal constraints obtained from the current document.", "startOffset": 6, "endOffset": 27}, {"referenceID": 2, "context": "Bellegarda (2000) and Coccaro & Jurafsky (1998) use latent semantic analysis to adapt their models to the current context.", "startOffset": 0, "endOffset": 18}, {"referenceID": 2, "context": "Bellegarda (2000) and Coccaro & Jurafsky (1998) use latent semantic analysis to adapt their models to the current context.", "startOffset": 0, "endOffset": 48}, {"referenceID": 2, "context": "Bellegarda (2000) and Coccaro & Jurafsky (1998) use latent semantic analysis to adapt their models to the current context. Similarly, topic features have been used with either maximum entropy models (Khudanpur & Wu, 2000) or recurrent networks (Mikolov & Zweig, 2012; Wang & Cho, 2015). Finally, Lau et al. (1993) proposes to use pairs of distant of words to capture long-range dependencies.", "startOffset": 0, "endOffset": 314}, {"referenceID": 43, "context": "In the context of sequence prediction, several memory augmented neural networks have obtained promising results (Sukhbaatar et al., 2015; Graves et al., 2014; Grefenstette et al., 2015; Joulin & Mikolov, 2015).", "startOffset": 112, "endOffset": 209}, {"referenceID": 16, "context": "In the context of sequence prediction, several memory augmented neural networks have obtained promising results (Sukhbaatar et al., 2015; Graves et al., 2014; Grefenstette et al., 2015; Joulin & Mikolov, 2015).", "startOffset": 112, "endOffset": 209}, {"referenceID": 17, "context": "In the context of sequence prediction, several memory augmented neural networks have obtained promising results (Sukhbaatar et al., 2015; Graves et al., 2014; Grefenstette et al., 2015; Joulin & Mikolov, 2015).", "startOffset": 112, "endOffset": 209}, {"referenceID": 14, "context": ", 2015; Graves et al., 2014; Grefenstette et al., 2015; Joulin & Mikolov, 2015). In particular, Sukhbaatar et al. (2015) stores a representation of the recent past and accesses it using an attention mechanism Bahdanau et al.", "startOffset": 8, "endOffset": 121}, {"referenceID": 0, "context": "(2015) stores a representation of the recent past and accesses it using an attention mechanism Bahdanau et al. (2014). Sukhbaatar et al.", "startOffset": 95, "endOffset": 118}, {"referenceID": 0, "context": "(2015) stores a representation of the recent past and accesses it using an attention mechanism Bahdanau et al. (2014). Sukhbaatar et al. (2015) shows that this reduces the perplexity for language modeling.", "startOffset": 95, "endOffset": 144}, {"referenceID": 35, "context": "Model wikitext2 wikitext103 Zoneout + Variational LSTM (Merity et al., 2016) 100.", "startOffset": 55, "endOffset": 76}, {"referenceID": 35, "context": "9 Pointer Sentinel LSTM (Merity et al., 2016) 80.", "startOffset": 24, "endOffset": 45}, {"referenceID": 5, "context": "This approach has been successfully applied to question answering, when the answer is contained in a given paragraph (Chen et al., 2016; Hermann et al., 2015; Kadlec et al., 2016; Sukhbaatar et al., 2015).", "startOffset": 117, "endOffset": 204}, {"referenceID": 19, "context": "This approach has been successfully applied to question answering, when the answer is contained in a given paragraph (Chen et al., 2016; Hermann et al., 2015; Kadlec et al., 2016; Sukhbaatar et al., 2015).", "startOffset": 117, "endOffset": 204}, {"referenceID": 25, "context": "This approach has been successfully applied to question answering, when the answer is contained in a given paragraph (Chen et al., 2016; Hermann et al., 2015; Kadlec et al., 2016; Sukhbaatar et al., 2015).", "startOffset": 117, "endOffset": 204}, {"referenceID": 43, "context": "This approach has been successfully applied to question answering, when the answer is contained in a given paragraph (Chen et al., 2016; Hermann et al., 2015; Kadlec et al., 2016; Sukhbaatar et al., 2015).", "startOffset": 117, "endOffset": 204}, {"referenceID": 5, "context": "This approach has been successfully applied to question answering, when the answer is contained in a given paragraph (Chen et al., 2016; Hermann et al., 2015; Kadlec et al., 2016; Sukhbaatar et al., 2015). Similarly, Vinyals et al. (2015) explores the use of this mechanism to reorder sequences of tokens.", "startOffset": 118, "endOffset": 239}, {"referenceID": 5, "context": "This approach has been successfully applied to question answering, when the answer is contained in a given paragraph (Chen et al., 2016; Hermann et al., 2015; Kadlec et al., 2016; Sukhbaatar et al., 2015). Similarly, Vinyals et al. (2015) explores the use of this mechanism to reorder sequences of tokens. Their network uses an attention (or \u201cpointer\u201d) over the input sequence to predict which element should be selected as the next output. Gulcehre et al. (2016) have shown that a similar mechanism called pointer softmax could be used in the context of machine translation, to decide which word to copy from the source to target.", "startOffset": 118, "endOffset": 464}, {"referenceID": 5, "context": "This approach has been successfully applied to question answering, when the answer is contained in a given paragraph (Chen et al., 2016; Hermann et al., 2015; Kadlec et al., 2016; Sukhbaatar et al., 2015). Similarly, Vinyals et al. (2015) explores the use of this mechanism to reorder sequences of tokens. Their network uses an attention (or \u201cpointer\u201d) over the input sequence to predict which element should be selected as the next output. Gulcehre et al. (2016) have shown that a similar mechanism called pointer softmax could be used in the context of machine translation, to decide which word to copy from the source to target. Independently of our work, Merity et al. (2016) apply the same mechanism to recurrent network.", "startOffset": 118, "endOffset": 680}, {"referenceID": 34, "context": "In this section, we describe experiments performed on two small datasets: the Penn Tree Bank (Marcus et al., 1993) and the wikitext2 (Merity et al.", "startOffset": 93, "endOffset": 114}, {"referenceID": 35, "context": ", 1993) and the wikitext2 (Merity et al., 2016) datasets.", "startOffset": 26, "endOffset": 47}, {"referenceID": 35, "context": "Our approach is competitive with previous models, in particular with the pointer sentinel LSTM model of Merity et al. (2016). On Penn Tree Bank, we note that the improvement over the base model is similar for both methods.", "startOffset": 104, "endOffset": 125}, {"referenceID": 14, "context": "Since both datasets have large vocabularies, we use the adaptive softmax (Grave et al., 2016) for faster training.", "startOffset": 73, "endOffset": 93}, {"referenceID": 13, "context": "The fact that improvements obtained with more advanced techniques decrease when the size of training data increases has already been observed by Goodman (2001). Both wikitext datasets sharing the same test set, we also observe that the LSTM baseline, trained on 103M tokens (wikitext103), strongly outperforms more sophisticated methods, trained on 2M tokens (wikitext2).", "startOffset": 145, "endOffset": 160}, {"referenceID": 39, "context": "Model Test LSTM-500 (Mikolov et al., 2014) 156 SCRNN (Mikolov et al.", "startOffset": 20, "endOffset": 42}, {"referenceID": 39, "context": ", 2014) 156 SCRNN (Mikolov et al., 2014) 161 MemNN (Sukhbaatar et al.", "startOffset": 18, "endOffset": 40}, {"referenceID": 43, "context": ", 2014) 161 MemNN (Sukhbaatar et al., 2015) 147 LSTM-1024 (our implem.", "startOffset": 18, "endOffset": 43}, {"referenceID": 40, "context": "(a) text8 Model Dev Ctrl WB5 (Paperno et al., 2016) 3125 285 WB5+cache (Paperno et al.", "startOffset": 29, "endOffset": 51}, {"referenceID": 40, "context": ", 2016) 3125 285 WB5+cache (Paperno et al., 2016) 768 270 LSTM-512 (Paperno et al.", "startOffset": 27, "endOffset": 49}, {"referenceID": 40, "context": ", 2016) 768 270 LSTM-512 (Paperno et al., 2016) 5357 149 LSTM-1024 (our implem.", "startOffset": 25, "endOffset": 47}, {"referenceID": 40, "context": "Finally, we report experiments carried on the lambada dataset, introduced by Paperno et al. (2016). This is a dataset of short passages extracted from novels.", "startOffset": 77, "endOffset": 99}, {"referenceID": 40, "context": "Finally, we report experiments carried on the lambada dataset, introduced by Paperno et al. (2016). This is a dataset of short passages extracted from novels. The goal is to predict the last word of the excerpt. This dataset was built so that human subjects solve the task perfectly when given the full context (approx. 4.6 sentences), but fail to do so when only given the sentence with the target word. Thus, most state-of-the-art language models fail on this dataset. The lambada training set contains approximately 200M tokens and has a vocabulary size of 93, 215. We report results for our method in Table 3, as well the performance of baselines from Paperno et al. (2016). Adding a neural cache model to the LSTM baseline strongly improves the performance on the lambada dataset.", "startOffset": 77, "endOffset": 678}], "year": 2016, "abstractText": "We propose an extension to neural network language models to adapt their prediction to the recent history. Our model is a simplified version of memory augmented networks, which stores past hidden activations as memory and accesses them through a dot product with the current hidden activation. This mechanism is very efficient and scales to very large memory sizes. We also draw a link between the use of external memory in neural network and cache models used with count based language models. We demonstrate on several language model datasets that our approach performs significantly better than recent memory augmented networks.", "creator": "LaTeX with hyperref package"}}}