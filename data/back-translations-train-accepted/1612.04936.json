{"id": "1612.04936", "review": {"conference": "iclr", "VERSION": "v1", "DATE_OF_SUBMISSION": "15-Dec-2016", "title": "Learning through Dialogue Interactions by Asking Questions", "abstract": "A good dialogue agent should have the ability to interact with users. In this work, we explore this direction by designing a simulator and a set of synthetic tasks in the movie domain that allow the learner to interact with a teacher by both asking and answering questions. We investigate how a learner can benefit from asking questions in both an offline and online reinforcement learning setting. We demonstrate that the learner improves when asking questions. Our work represents a first step in developing end-to-end learned interactive dialogue agents.", "histories": [["v1", "Thu, 15 Dec 2016 05:46:27 GMT  (355kb,D)", "http://arxiv.org/abs/1612.04936v1", null], ["v2", "Thu, 29 Dec 2016 19:47:12 GMT  (356kb,D)", "http://arxiv.org/abs/1612.04936v2", null], ["v3", "Fri, 13 Jan 2017 21:07:04 GMT  (360kb,D)", "http://arxiv.org/abs/1612.04936v3", null], ["v4", "Mon, 13 Feb 2017 17:30:42 GMT  (360kb,D)", "http://arxiv.org/abs/1612.04936v4", null]], "reviews": [], "SUBJECTS": "cs.CL cs.AI", "authors": ["jiwei li", "alexander h miller", "sumit chopra", "marc'aurelio ranzato", "jason weston"], "accepted": true, "id": "1612.04936"}, "pdf": {"name": "1612.04936.pdf", "metadata": {"source": "CRF", "title": null, "authors": ["Jiwei Li", "Alexander H. Miller", "Sumit Chopra"], "emails": ["jiwel@fb.com", "ahm@fb.com", "spchopra@fb.com", "ranzato@fb.com", "jase@fb.com"], "sections": [{"heading": "1 INTRODUCTION", "text": "A good interlocutor (a learner / bot / student) should have this ability to interact with a dialogue partner (the teacher / user). However, recent efforts tend to focus on learning through firm answers provided in the learning group, rather than through interactions. In this case, when a learner encounters a confusing situation such as an unknown interface shape (phrase or structure), the agent will either make a (usually bad) guess or redirect the user to other resources (such as a search engine, as in Siri). However, humans can adapt to many situations by asking questions. We identify three categories of mistakes a learner can make during the dialogue. (1) The learner has problems understanding the interface form of the dialogue, such as the formulation of a question."}, {"heading": "2 RELATED WORK", "text": "In recent years, it has been shown that people are able to survive themselves, and that they are able to survive themselves by putting themselves in a position to survive themselves. In recent years, it has been shown that people are able to survive themselves, and that they are able to survive themselves. In recent years, it has been shown that people are able to survive themselves, and that they are able to survive themselves. In recent years, it has been shown that people are able to survive themselves, and that they are able to survive themselves."}, {"heading": "3 THE TASKS", "text": "In this section, we describe the dialog tasks we devised 2. They are tailored to the three different situations described in Section 1 that motivate the bot to ask questions: (1) clarify the question where the bot has difficulty understanding the text of its dialogue partner; (2) knowledge operation where the bot needs to ask for help to think through an existing knowledge base; and (3) knowledge acquisition where the bot's knowledge is incomplete and needs to be filled in. For our experiments, we adapt the WikiMovies data set (Weston et al., 2015), which consists of approximately 100k questions about 75k units, based on questions with answers in the open film dataset (OMDb). The training / developer / test sets each contain 181638 / 9702 / 9698 examples. The accuracy metric corresponds to the percentage of time the bot gives the correct answers to the teacher's questions."}, {"heading": "3.1 QUESTION CLARIFICATION.", "text": "In this context, the bot does not understand the teacher's question. We focus on a specific situation where the bot does not understand the teacher due to typing / spelling errors, as shown in Figure2 code, and the data is available at https: / / github.com / facebook / MemNN / tree / master / AskingQuestions. 3These QA pairs from the story can be considered a distraction and serve to test the bot's ability to separate the wheat from the chaff. For each dialogue, we insert 5 additional QA pairs (10 sentences). We intentionally misspelled some words in the questions, such as the word \"movie\" by \"movie\" or \"star\" by \"sttar.\" 4 To ensure that the bot will have trouble understanding the question, we guarantee that the bot has never encountered the misspelling before - the misspelling, which means the introduction of mechanisms in training, dev, and the phrases are spelled differently so the same word."}, {"heading": "3.2 KNOWLEDGE OPERATION", "text": "We focus on a specific case where the bot tries to understand the relevant facts. We examine two settings: Ask For Relevant Knowledge (task 3), in which the bot directly asks the teacher to point out the relevant KB fact, and Knowledge Verification (task 4), in which the bot asks if the teacher's question is relevant to a specific KB fact. In the Ask For Relevant Knowledge setting, the teacher points to the relevant KB fact or gives a positive or negative answer in the Knowledge Verification setting. Then the bot gives an answer to the teacher's original question and the teacher gives feedback to the answer."}, {"heading": "3.3 KNOWLEDGE ACQUISITION", "text": "For the tasks in this area, the bot has an incomplete KB and there are entities that it lacks, see Figure 3. For example, the missing part could be the entity that the teacher asks for in this case. It must be the teacher who answers the question. It must be the answer to the question."}, {"heading": "4 TRAIN/TEST REGIME", "text": "From the point of view of access to the simulator, we had two objectives: firstly, to test the usefulness of the questionnaire; secondly, to train our Student Bot to learn when to ask questions and what questions to ask. To achieve these two objectives, we examined the training of our models using two methods, offline supervised learning and online reinforcement learning."}, {"heading": "4.1 OFFLINE SUPERVISED LEARNING", "text": "The motivation for the training of our students is first and foremost to test the usefulness of the question. Dialogues are described as follows:"}, {"heading": "4.2 ONLINE REINFORCEMENT LEARNING (RL)", "text": "In fact, it is such that most of them are able to survive themselves without there being a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is"}, {"heading": "5 MODELS", "text": "The model takes as input the last utterance of the dialogue history (the question of the teacher's reward), as well as a number of memory contexts, including short-term memories (the dialogue history between the bot and the teacher) and long-term memories (the knowledge base facts to which the bot has access), and issues a label. We refer readers to the appendix for more details on MemN2N.Offline Supervised Settings: The first learning strategy we apply is the reward strategy (denoted vanilla-MemN2N) described in (Weston, 2016), the model maximizes the likelihood of logging the correct answers given by the student (examples with erroneous definitive answers)."}, {"heading": "6 EXPERIMENTS", "text": "In this context, it should be noted that this is a very complex matter."}, {"heading": "7 CONCLUSIONS", "text": "In this paper, we have explored how an intelligent agent can benefit from interacting with users by asking questions. We have developed tasks where interaction via asking questions is desirable. We examine both online and offline settings that mimic different situations in the real world and show that in most cases teaching a bot to interact with people facilitates speech comprehension and thus leads to a better ability to answer questions."}], "references": [{"title": "Interactional feedback and the impact of attitude and motivation on noticing l2 form", "author": ["Mohammad Amin Bassiri"], "venue": "English Language and Literature Studies,", "citeRegEx": "Bassiri.,? \\Q2011\\E", "shortCiteRegEx": "Bassiri.", "year": 2011}, {"title": "Learning end-to-end goal-oriented dialog", "author": ["Antoine Bordes", "Jason Weston"], "venue": "arXiv preprint arXiv:1605.07683,", "citeRegEx": "Bordes and Weston.,? \\Q2016\\E", "shortCiteRegEx": "Bordes and Weston.", "year": 2016}, {"title": "Large-scale simple question answering with memory networks", "author": ["Antoine Bordes", "Nicolas Usunier", "Sumit Chopra", "Jason Weston"], "venue": "arXiv preprint arXiv:1506.02075,", "citeRegEx": "Bordes et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Bordes et al\\.", "year": 2015}, {"title": "Evaluating prerequisite qualities for learning end-to-end dialog systems", "author": ["Jesse Dodge", "Andreea Gane", "Xiang Zhang", "Antoine Bordes", "Sumit Chopra", "Alexander Miller", "Arthur Szlam", "Jason Weston"], "venue": "arXiv preprint arXiv:1511.06931,", "citeRegEx": "Dodge et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Dodge et al\\.", "year": 2015}, {"title": "The conscientious consumer: Reconsidering the role of assessment feedback in student learning", "author": ["Richard Higgins", "Peter Hartley", "Alan Skelton"], "venue": "Studies in higher education,", "citeRegEx": "Higgins et al\\.,? \\Q2002\\E", "shortCiteRegEx": "Higgins et al\\.", "year": 2002}, {"title": "Learning through feedback", "author": ["Andrew S Latham"], "venue": "Educational Leadership,", "citeRegEx": "Latham.,? \\Q1997\\E", "shortCiteRegEx": "Latham.", "year": 1997}, {"title": "A diversity-promoting objective function for neural conversation models", "author": ["Jiwei Li", "Michel Galley", "Chris Brockett", "Jianfeng Gao", "Bill Dolan"], "venue": "arXiv preprint arXiv:1510.03055,", "citeRegEx": "Li et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Li et al\\.", "year": 2015}, {"title": "Key-value memory networks for directly reading documents", "author": ["Alexander Miller", "Adam Fisch", "Jesse Dodge", "Amir-Hossein Karimi", "Antoine Bordes", "Jason Weston"], "venue": "arXiv preprint arXiv:1606.03126,", "citeRegEx": "Miller et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Miller et al\\.", "year": 2016}, {"title": "Sequence level training with recurrent neural networks", "author": ["Marc\u2019Aurelio Ranzato", "Sumit Chopra", "Michael Auli", "Wojciech Zaremba"], "venue": "arXiv preprint arXiv:1511.06732,", "citeRegEx": "Ranzato et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Ranzato et al\\.", "year": 2015}, {"title": "A neural network approach to context-sensitive generation of conversational responses", "author": ["Alessandro Sordoni", "Michel Galley", "Michael Auli", "Chris Brockett", "Yangfeng Ji", "Margaret Mitchell", "Jian-Yun Nie", "Jianfeng Gao", "Bill Dolan"], "venue": "arXiv preprint arXiv:1506.06714,", "citeRegEx": "Sordoni et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Sordoni et al\\.", "year": 2015}, {"title": "Continuously learning neural dialogue management", "author": ["Pei-Hao Su", "Milica Gasic", "Nikola Mrksic", "Lina Rojas-Barahona", "Stefan Ultes", "David Vandyke", "Tsung-Hsien Wen", "Steve Young"], "venue": "arXiv preprint arXiv:1606.02689,", "citeRegEx": "Su et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Su et al\\.", "year": 2016}, {"title": "End-to-end memory networks", "author": ["Sainbayar Sukhbaatar", "Jason Weston", "Rob Fergus"], "venue": "In Advances in neural information processing systems,", "citeRegEx": "Sukhbaatar et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Sukhbaatar et al\\.", "year": 2015}, {"title": "A neural conversational model", "author": ["Oriol Vinyals", "Quoc Le"], "venue": "arXiv preprint arXiv:1506.05869,", "citeRegEx": "Vinyals and Le.,? \\Q2015\\E", "shortCiteRegEx": "Vinyals and Le.", "year": 2015}, {"title": "Learning language games through interaction", "author": ["Sida I Wang", "Percy Liang", "Christopher D Manning"], "venue": "arXiv preprint arXiv:1606.02447,", "citeRegEx": "Wang et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Wang et al\\.", "year": 2016}, {"title": "A network-based end-to-end trainable task-oriented dialogue system", "author": ["Tsung-Hsien Wen", "Milica Gasic", "Nikola Mrksic", "Lina M Rojas-Barahona", "Pei-Hao Su", "Stefan Ultes", "David Vandyke", "Steve Young"], "venue": "arXiv preprint arXiv:1604.04562,", "citeRegEx": "Wen et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Wen et al\\.", "year": 2016}, {"title": "Instructive feedback: Review of parameters and effects", "author": ["Margaret G Werts", "Mark Wolery", "Ariane Holcombe", "David L Gast"], "venue": "Journal of Behavioral Education,", "citeRegEx": "Werts et al\\.,? \\Q1995\\E", "shortCiteRegEx": "Werts et al\\.", "year": 1995}, {"title": "Dialog-based language learning", "author": ["Jason Weston"], "venue": "arXiv preprint arXiv:1604.06045,", "citeRegEx": "Weston.,? \\Q2016\\E", "shortCiteRegEx": "Weston.", "year": 2016}, {"title": "Towards ai-complete question answering: A set of prerequisite toy tasks", "author": ["Jason Weston", "Antoine Bordes", "Sumit Chopra", "Alexander M Rush", "Bart van Merri\u00ebnboer", "Armand Joulin", "Tomas Mikolov"], "venue": "arXiv preprint arXiv:1502.05698,", "citeRegEx": "Weston et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Weston et al\\.", "year": 2015}, {"title": "Simple statistical gradient-following algorithms for connectionist reinforcement learning", "author": ["Ronald J Williams"], "venue": "Machine learning,", "citeRegEx": "Williams.,? \\Q1992\\E", "shortCiteRegEx": "Williams.", "year": 1992}, {"title": "Understanding natural language", "author": ["Terry Winograd"], "venue": "Cognitive psychology,", "citeRegEx": "Winograd.,? \\Q1972\\E", "shortCiteRegEx": "Winograd.", "year": 1972}, {"title": "Philosophical investigations", "author": ["Ludwig Wittgenstein"], "venue": null, "citeRegEx": "Wittgenstein.,? \\Q2010\\E", "shortCiteRegEx": "Wittgenstein.", "year": 2010}, {"title": "Reinforcement learning neural turing machines", "author": ["Wojciech Zaremba", "Ilya Sutskever"], "venue": "arXiv preprint arXiv:1505.00521,", "citeRegEx": "Zaremba and Sutskever.,? \\Q2015\\E", "shortCiteRegEx": "Zaremba and Sutskever.", "year": 2015}], "referenceMentions": [{"referenceID": 20, "context": "games (Wittgenstein, 2010).", "startOffset": 6, "endOffset": 26}, {"referenceID": 19, "context": "The direction of interactive language learning through language games has been explored in the early seminal work of Winograd (Winograd, 1972), and in the recent SHRDLURN system (Wang et al.", "startOffset": 126, "endOffset": 142}, {"referenceID": 13, "context": "The direction of interactive language learning through language games has been explored in the early seminal work of Winograd (Winograd, 1972), and in the recent SHRDLURN system (Wang et al., 2016).", "startOffset": 178, "endOffset": 197}, {"referenceID": 0, "context": "In a broader context, the usefulness of feedback and interactions has been validated in the setting of multiple language learning, such as second language learning (Bassiri, 2011) and learning by students (Higgins et al.", "startOffset": 164, "endOffset": 179}, {"referenceID": 4, "context": "In a broader context, the usefulness of feedback and interactions has been validated in the setting of multiple language learning, such as second language learning (Bassiri, 2011) and learning by students (Higgins et al., 2002; Latham, 1997; Werts et al., 1995).", "startOffset": 205, "endOffset": 261}, {"referenceID": 5, "context": "In a broader context, the usefulness of feedback and interactions has been validated in the setting of multiple language learning, such as second language learning (Bassiri, 2011) and learning by students (Higgins et al., 2002; Latham, 1997; Werts et al., 1995).", "startOffset": 205, "endOffset": 261}, {"referenceID": 15, "context": "In a broader context, the usefulness of feedback and interactions has been validated in the setting of multiple language learning, such as second language learning (Bassiri, 2011) and learning by students (Higgins et al., 2002; Latham, 1997; Werts et al., 1995).", "startOffset": 205, "endOffset": 261}, {"referenceID": 6, "context": "These include the chit-chat type end-to-end dialogue systems (Vinyals & Le, 2015; Li et al., 2015; Sordoni et al., 2015), which directly generate a response given the previous history of user utterance.", "startOffset": 61, "endOffset": 120}, {"referenceID": 9, "context": "These include the chit-chat type end-to-end dialogue systems (Vinyals & Le, 2015; Li et al., 2015; Sordoni et al., 2015), which directly generate a response given the previous history of user utterance.", "startOffset": 61, "endOffset": 120}, {"referenceID": 14, "context": "It also include a collection of goal-oriented dialogue systems (Wen et al., 2016; Su et al., 2016; Bordes & Weston, 2016), which complete a certain task such as booking a ticket or making a reservation at a restaurant.", "startOffset": 63, "endOffset": 121}, {"referenceID": 10, "context": "It also include a collection of goal-oriented dialogue systems (Wen et al., 2016; Su et al., 2016; Bordes & Weston, 2016), which complete a certain task such as booking a ticket or making a reservation at a restaurant.", "startOffset": 63, "endOffset": 121}, {"referenceID": 3, "context": "Another line of research focuses on supervised learning for question answering from dialogues (Dodge et al., 2015; Weston, 2016), using either a given database of knowledge (Bordes et al.", "startOffset": 94, "endOffset": 128}, {"referenceID": 16, "context": "Another line of research focuses on supervised learning for question answering from dialogues (Dodge et al., 2015; Weston, 2016), using either a given database of knowledge (Bordes et al.", "startOffset": 94, "endOffset": 128}, {"referenceID": 2, "context": ", 2015; Weston, 2016), using either a given database of knowledge (Bordes et al., 2015; Miller et al., 2016) or short stories (Weston et al.", "startOffset": 66, "endOffset": 108}, {"referenceID": 7, "context": ", 2015; Weston, 2016), using either a given database of knowledge (Bordes et al., 2015; Miller et al., 2016) or short stories (Weston et al.", "startOffset": 66, "endOffset": 108}, {"referenceID": 17, "context": ", 2016) or short stories (Weston et al., 2015).", "startOffset": 25, "endOffset": 46}, {"referenceID": 0, "context": "In a broader context, the usefulness of feedback and interactions has been validated in the setting of multiple language learning, such as second language learning (Bassiri, 2011) and learning by students (Higgins et al., 2002; Latham, 1997; Werts et al., 1995). In the context of dialogue, with the recent popularity of deep learning models, many neural dialogue systems have been proposed. These include the chit-chat type end-to-end dialogue systems (Vinyals & Le, 2015; Li et al., 2015; Sordoni et al., 2015), which directly generate a response given the previous history of user utterance. It also include a collection of goal-oriented dialogue systems (Wen et al., 2016; Su et al., 2016; Bordes & Weston, 2016), which complete a certain task such as booking a ticket or making a reservation at a restaurant. Another line of research focuses on supervised learning for question answering from dialogues (Dodge et al., 2015; Weston, 2016), using either a given database of knowledge (Bordes et al., 2015; Miller et al., 2016) or short stories (Weston et al., 2015). As far as we know, current dialogue systems mostly focus on learning through fixed supervised signals rather than interacting with users. Our work is closely related to the recent work of Weston (2016), which explores the problem of learning through conducting conversations, where supervision is given naturally in the response during the conversation.", "startOffset": 165, "endOffset": 1272}, {"referenceID": 17, "context": "For our experiments we adapt the WikiMovies dataset (Weston et al., 2015), which consists of roughly 100k questions over 75k entities based on questions with answers in the open movie dataset (OMDb).", "startOffset": 52, "endOffset": 73}, {"referenceID": 11, "context": "For both offline supervised and online RL settings, we use the End-to-End Memory Network model (MemN2N) (Sukhbaatar et al., 2015) as a backbone.", "startOffset": 104, "endOffset": 129}, {"referenceID": 16, "context": "Offline Supervised Settings: The first learning strategy we adopt is the reward-based imitation strategy (denoted vanilla-MemN2N) described in (Weston, 2016), where at training time, the model maximizes the log likelihood probability of the correct answers the student gave (examples with incorrect final answers are discarded).", "startOffset": 143, "endOffset": 157}, {"referenceID": 16, "context": "The bot\u2019s answers are predicted using a vanilla-MemN2N and the teacher\u2019s feedback is predicted using the Forward Prediction (FP) model as described in (Weston, 2016).", "startOffset": 151, "endOffset": 165}, {"referenceID": 18, "context": "7 We use the REINFORCE algorithm (Williams, 1992) to update PRL(Question) and PRL(Answer).", "startOffset": 33, "endOffset": 49}, {"referenceID": 8, "context": "We refer the readers to (Ranzato et al., 2015; Zaremba & Sutskever, 2015) for more details.", "startOffset": 24, "endOffset": 73}], "year": 2017, "abstractText": "A good dialogue agent should have the ability to interact with users. In this work, we explore this direction by designing a simulator and a set of synthetic tasks in the movie domain that allow the learner to interact with a teacher by both asking and answering questions. We investigate how a learner can benefit from asking questions in both an offline and online reinforcement learning setting. We demonstrate that the learner improves when asking questions. Our work represents a first step in developing end-to-end learned interactive dialogue agents.", "creator": "LaTeX with hyperref package"}}}