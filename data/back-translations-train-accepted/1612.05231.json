{"id": "1612.05231", "review": {"conference": "ICML", "VERSION": "v1", "DATE_OF_SUBMISSION": "15-Dec-2016", "title": "Tunable Efficient Unitary Neural Networks (EUNN) and their application to RNNs", "abstract": "We present a method for implementing an Efficient Unitary Neural Network (EUNN) whose computational complexity is merely $\\mathcal{O}(1)$ per parameter and has full tunability, from spanning part of unitary space to all of it. We apply the EUNN in Recurrent Neural Networks, and test its performance on the standard copying task and the MNIST digit recognition benchmark, finding that it significantly outperforms a non-unitary RNN, an LSTM network, an exclusively partial space URNN and a projective URNN with comparable parameter numbers.", "histories": [["v1", "Thu, 15 Dec 2016 20:39:15 GMT  (1624kb,D)", "http://arxiv.org/abs/1612.05231v1", "9 pages, 4 figures"], ["v2", "Sun, 26 Feb 2017 19:00:50 GMT  (3179kb,D)", "http://arxiv.org/abs/1612.05231v2", "9 pages, 4 figures"], ["v3", "Mon, 3 Apr 2017 17:13:38 GMT  (3180kb,D)", "http://arxiv.org/abs/1612.05231v3", "9 pages, 4 figures"]], "COMMENTS": "9 pages, 4 figures", "reviews": [], "SUBJECTS": "cs.LG cs.NE stat.ML", "authors": ["li jing", "yichen shen", "tena dubcek", "john peurifoy", "scott a skirlo", "yann lecun", "max tegmark", "marin soljacic"], "accepted": true, "id": "1612.05231"}, "pdf": {"name": "1612.05231.pdf", "metadata": {"source": "META", "title": "Tunable Efficient Unitary Neural Networks (EUNN) and their application to RNN", "authors": ["Li Jing", "Yichen Shen", "John Peurifoy", "Scott Skirlo"], "emails": ["LJING@MIT.EDU", "YCSHEN@MIT.EDU", "TENAD@MIT.EDU", "JPEURIFO@MIT.EDU", "SSKIRLO@MIT.EDU", "TEGMARK@MIT.EDU", "SOLJACIC@MIT.EDU"], "sections": [{"heading": "1. Introduction", "text": "This year, it is more than ever before in the history of the city."}, {"heading": "2. Background", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "2.1. Basic Recurrent Neural Networks", "text": "A recursive neural network takes an input sequence and uses the current hidden state to create a new hidden state during each step, storing past information in the hidden layer. First, we look at the basic RNN architecture. Consider an RNN updated at regular intervals, whose input is the sequence of the vectors x (t) and a, whose hidden layer h (t) is updated according to the following rule: h (t) = \u03c3 (Ux (t) + Wh (t \u2212 1) + a), (1) where \u03c3 is the nonlinear activation function (see Figure 1) and a the hidden layer bias vector. The output is generated by y (t) = softmax (Wh (t) + b), (2) where b is the hidden layer bias vector (see Figure 1) and a the hidden layer bias vector."}, {"heading": "2.2. The Vanishing and Exploding Gradient Problems", "text": "When training the neural network to minimize a cost function C, which depends on a parameter vector a, the gradient descend method updates this vector to a \u2212 \u03bb \u2202 C \u2202 a, where \u03bb is a fixed learning rate and \u2202 C \u2202 a \u0445 C. For an RNN, the problem of disappearing or exploding gradients occurs only during the reverse propagation from hidden to hidden layers, so we focus only on the gradient for hidden layers. Training the input-to-hidden and hidden-to-output matrices is relatively trivial once the hidden to hidden layers have been successfully optimized. To evaluate this problem, first calculate the derived W layer using the chain rule: \"X\" X \"X\" X \"X\" X \"\" Y \"Y\" Y \"Y\" Y \"Y\" Y \"Y\" Y \"Y\" Y \"Y\" Y \"Y\" Y \"Y\" Y."}, {"heading": "3. Unitary RNNs", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "3.1. Partial Space Unitary RNNs", "text": "In a groundbreaking paper, Arjovsky, Shah & Benjovsky (Arjovsky et al., 2015) have shown that uniform RNNNs can overcome exploding and disappearing gradient problems and perform well in long-term memory tasks when parameterized the previously hidden matrix in the following uniform form: W = D3T2F \u2212 1D2\u0433T1FD1. (6) D1,2,3 are diagonal matrices with each element of the matrix = 1, 2 \u00b7, n. T1,2 are reflection matrices and T = I \u2212 2 are antiquity matrix matrix matrix, antiquity matrix, antiquity matrix = 1, and antiquity matrix = 1."}, {"heading": "3.2. Full Space Unitary RNNs", "text": "To maximize the power of unified RNNs, it is preferable to optimize the weight matrix W over the entire space of the unified matrices rather than over a subspace as described above. A simple method of implementation is to simply update W by default backpropagation and then project the resulting matrix (which will typically no longer be unified) back into the space of the unified matrices. If Gij is defined according to the gradient C \u2202 Wij with respect to W, this can be implemented by the procedure defined by (Wisdom et al., 2016): A (t) \u2261 G (t) \u2212 W (t) \u2212 W (t) \u2020 G (k), (7) W (t + 1). (I + 2 A (t) \u2212 1 (I \u2212 2 A (t) W (t). (8) This method shows that full-area unified networks are superior to many -RNN tasks (A + 2) wisdom (t)."}, {"heading": "4. Efficient Unitary Neural Network (EUNN) Architectures", "text": "In the following, we first describe a general parameterization method that can represent arbitrary uniform matrices with up to N2 degrees of freedom. Then, we present an efficient backpropagation algorithm in this parameterization scheme that requires only O (1) computation and memory access steps to determine the gradient for each parameter. Finally, we show that our scheme performs significantly better than the above methods on two well-known benchmarks."}, {"heading": "4.1. Unitary Matrix Parametrization", "text": "Each N \u00b7 N) -dimensional matrix UN can be represented as a product of rotational matrices = subsequent matrices {Rij} and a diagonal matrixD, so that UN = D \u00b7 N \u2212 N \u2212 2 = 1 Rij, where Rij replaces the N -dimensional identity matrix with the elements Rii, Rij, Rji and Rjj as follows (Reck et al., 1994; Clements et al.): (Rii Rij Rji Rjj) = (ei\u03c6ij cos \u03b8ij \u2212 ei\u03c6ij sin \u03b8ij cos successij cos successij). (9) Rij is thus parameterized by the two real numbers. (2) Each of these matrices Rij performs a U (2) -unified transformation on a two-dimensional subspace, which we leave unchanged a (N \u2212 2) -dimensional subspace."}, {"heading": "4.2. Efficient Implementation of Unitary Matrix", "text": "\u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7"}, {"heading": "5. Experimental test of our method", "text": "In this section, we compare the performance of our Efficient Unitary Recurrent Neural Network (EURNN) with 1. an LSTM RNN (Hochreiter & Schmidhuber, 1997), 2. a Partial Space URNN (Arjovsky et al., 2015) and 3. a Projective URNN (Wisdom et al., 2016) using the learning rate 10 \u2212 3 and the decay rate 0.9. For the LSTM, the clipping gradient is set to unit."}, {"heading": "5.1. Memory Copying Task", "text": "In fact, most people are able to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to edit, to edit, to edit, to edit, to edit, to edit."}, {"heading": "5.2. MNIST Task", "text": "The problem of MNIST handwriting recognition remains one of the most popular benchmarks for testing learning ability. 28 x 28 large grayscale images of handwritten digits are applied with a target label between 0-9. In accordance with the approach of (Arjovsky et al., 2015; Wisdom et al., 2016), we apply RNN models to MNIST handwriting records by transmitting a series of pixels to the RNN at each step. The3Our intuitive explanation for this is, to be good at this task, that the network only needs to remember each entry (while a small-capacity unit matrix is sufficient to remember), but not the correlations between adjacent centers. Thus, RNN is fed with 28 consecutive inputs of 28 lengths each, and the final results are probability distributions across the ten digitaliss.We find that for this task greater L (greater capacity) is preferred."}, {"heading": "6. Conclusion", "text": "This year, it has never been as good as it has been this year."}, {"heading": "Acknowledgment", "text": "We thank Hugo Larochelle, Yann LeCun, Luis Seoane, and David Theurel for their helpful discussions and comments. This work was partially supported by the Army Research Office through the Institute of Soldier Nanotechnologies under the contract W911NF-13-D0001. T.D. \"s work was also supported by the Institute of International Education through the Fulbright Scholarship."}], "references": [{"title": "Learning long-term dependencies with gradient descent is difficult", "author": ["Bengio", "Yoshua", "Simard", "Patrice", "Frasconi", "Paolo"], "venue": "IEEE transactions on neural networks,", "citeRegEx": "Bengio et al\\.,? \\Q1994\\E", "shortCiteRegEx": "Bengio et al\\.", "year": 1994}, {"title": "Bidirectional recurrent neural networks as generative models", "author": ["Berglund", "Mathias", "Raiko", "Tapani", "Honkala", "Mikko", "K\u00e4rkk\u00e4inen", "Leo", "Vetek", "Akos", "Karhunen", "Juha T"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Berglund et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Berglund et al\\.", "year": 2015}, {"title": "On the properties of neural machine translation: Encoder-decoder approaches", "author": ["Cho", "Kyunghyun", "Van Merri\u00ebnboer", "Bart", "Bahdanau", "Dzmitry", "Bengio", "Yoshua"], "venue": "arXiv preprint arXiv:1409.1259,", "citeRegEx": "Cho et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Cho et al\\.", "year": 2014}, {"title": "Natural language processing (almost) from scratch", "author": ["Collobert", "Ronan", "Weston", "Jason", "Bottou", "L\u00e9on", "Karlen", "Michael", "Kavukcuoglu", "Koray", "Kuksa", "Pavel"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Collobert et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Collobert et al\\.", "year": 2011}, {"title": "Orthogonal rnns and long-memory tasks", "author": ["Henaff", "Mikael", "Szlam", "Arthur", "LeCun", "Yann"], "venue": "arXiv preprint arXiv:1602.06662,", "citeRegEx": "Henaff et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Henaff et al\\.", "year": 2016}, {"title": "Untersuchungen zu dynamischen neuronalen netzen", "author": ["Hochreiter", "Sepp"], "venue": "Diploma, Technische Universita\u0308t Mu\u0308nchen, pp", "citeRegEx": "Hochreiter and Sepp.,? \\Q1991\\E", "shortCiteRegEx": "Hochreiter and Sepp.", "year": 1991}, {"title": "Long shortterm memory", "author": ["Hochreiter", "Sepp", "Schmidhuber", "J\u00fcrgen"], "venue": "Neural computation,", "citeRegEx": "Hochreiter et al\\.,? \\Q1997\\E", "shortCiteRegEx": "Hochreiter et al\\.", "year": 1997}, {"title": "Imagenet classification with deep convolutional neural networks. In Advances in neural information processing", "author": ["Krizhevsky", "Alex", "Sutskever", "Ilya", "Hinton", "Geoffrey E"], "venue": null, "citeRegEx": "Krizhevsky et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Krizhevsky et al\\.", "year": 2012}, {"title": "A simple way to initialize recurrent networks of rectified linear units", "author": ["Le", "Quoc V", "Jaitly", "Navdeep", "Hinton", "Geoffrey E"], "venue": "arXiv preprint arXiv:1504.00941,", "citeRegEx": "Le et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Le et al\\.", "year": 2015}, {"title": "Efficient orthogonal parametrisation of recurrent neural networks using householder reflections", "author": ["Mhammedi", "Zakaria", "Hellicar", "Andrew", "Rahman", "Ashfaqur", "Bailey", "James"], "venue": "arXiv preprint arXiv:1612.00188,", "citeRegEx": "Mhammedi et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Mhammedi et al\\.", "year": 2016}, {"title": "Experimental realization of any discrete unitary operator", "author": ["Reck", "Michael", "Zeilinger", "Anton", "Bernstein", "Herbert J", "Bertani", "Philip"], "venue": "Phys. Rev. Lett.,", "citeRegEx": "Reck et al\\.,? \\Q1994\\E", "shortCiteRegEx": "Reck et al\\.", "year": 1994}, {"title": "Exact solutions to the nonlinear dynamics of learning in deep linear neural networks", "author": ["Saxe", "Andrew M", "McClelland", "James L", "Ganguli", "Surya"], "venue": "arXiv preprint arXiv:1312.6120,", "citeRegEx": "Saxe et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Saxe et al\\.", "year": 2013}, {"title": "Deep learning with coherent nanophotonic circuits", "author": ["Shen", "Yichen", "Harris", "Nicholas C", "Skirlo", "Scott", "Prabhu", "Mihika", "Baehr-Jones", "Tom", "Hochberg", "Michael", "Sun", "Xin", "Zhao", "Shijie", "Larochelle", "Hugo", "Englund", "Dirk"], "venue": "arXiv preprint arXiv:1610.02365,", "citeRegEx": "Shen et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Shen et al\\.", "year": 2016}, {"title": "Sequence to sequence learning with neural networks. In Advances in neural information processing", "author": ["Sutskever", "Ilya", "Vinyals", "Oriol", "Le", "Quoc V"], "venue": null, "citeRegEx": "Sutskever et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Sutskever et al\\.", "year": 2014}, {"title": "Full-capacity unitary recurrent neural networks", "author": ["Wisdom", "Scott", "Powers", "Thomas", "Hershey", "John", "Le Roux", "Jonathan", "Atlas", "Les"], "venue": "In Advances In Neural Information Processing Systems,", "citeRegEx": "Wisdom et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Wisdom et al\\.", "year": 2016}], "referenceMentions": [{"referenceID": 7, "context": ", 2015) have been successful on numerous difficult machine learning tasks, including image recognition (Krizhevsky et al., 2012; Donahue et al., 2015), speech recognition (Hinton et al.", "startOffset": 103, "endOffset": 150}, {"referenceID": 3, "context": ", 2012) and natural language processing (Collobert et al., 2011; Bahdanau et al., 2014; Sutskever et al., 2014).", "startOffset": 40, "endOffset": 111}, {"referenceID": 13, "context": ", 2012) and natural language processing (Collobert et al., 2011; Bahdanau et al., 2014; Sutskever et al., 2014).", "startOffset": 40, "endOffset": 111}, {"referenceID": 0, "context": "However, deep neural networks have long suffered from vanishing and exploding gradient problems (Hochreiter, 1991; Bengio et al., 1994), which are known to be caused by matrix eigenvalues far from unity being raised to large powers.", "startOffset": 96, "endOffset": 135}, {"referenceID": 2, "context": "Other recently proposed methods such as GRU (Cho et al., 2014) and Bidirectional RNNs (Berglund et al.", "startOffset": 44, "endOffset": 62}, {"referenceID": 1, "context": ", 2014) and Bidirectional RNNs (Berglund et al., 2015) also perform well in numerous applications.", "startOffset": 31, "endOffset": 54}, {"referenceID": 11, "context": "A recently proposed solution strategy is using orthogonal weight matrices or their complex generalization (unitary matrices) (Saxe et al., 2013; Le et al., 2015; Arjovsky et al., 2015), because all their eigenvalues will then have absolute values of unity, and can safely be raised to large powers.", "startOffset": 125, "endOffset": 184}, {"referenceID": 8, "context": "A recently proposed solution strategy is using orthogonal weight matrices or their complex generalization (unitary matrices) (Saxe et al., 2013; Le et al., 2015; Arjovsky et al., 2015), because all their eigenvalues will then have absolute values of unity, and can safely be raised to large powers.", "startOffset": 125, "endOffset": 184}, {"referenceID": 11, "context": "This has been shown to help both when weight matrices are initialized to be unitary (Saxe et al., 2013; Le et al., 2015) and when they are kept unitary during training, either by restricting them to a more tractable matrix subspace (Arjovsky et al.", "startOffset": 84, "endOffset": 120}, {"referenceID": 8, "context": "This has been shown to help both when weight matrices are initialized to be unitary (Saxe et al., 2013; Le et al., 2015) and when they are kept unitary during training, either by restricting them to a more tractable matrix subspace (Arjovsky et al.", "startOffset": 84, "endOffset": 120}, {"referenceID": 14, "context": "This is superior to the O(N) computational complexity of the existing training method for a full-space unitary network (Wisdom et al., 2016) and O(logN) more efficient than the subspace Unitary RNN (Arjovsky et al.", "startOffset": 119, "endOffset": 140}, {"referenceID": 14, "context": "Defining Gij \u2261 \u2202C \u2202Wij as the gradient with respect to W, this can be implemented by the procedure defined by (Wisdom et al., 2016): A \u2261 G \u2020 W \u2212W \u2020 G, (7)", "startOffset": 110, "endOffset": 131}, {"referenceID": 14, "context": "This method shows that full space unitary networks are superior on many RNN tasks (Wisdom et al., 2016).", "startOffset": 82, "endOffset": 103}, {"referenceID": 12, "context": "2 shows two that are particularly convenient to implement in software (and even in neuromorphic hardware (Shen et al., 2016)).", "startOffset": 105, "endOffset": 124}, {"referenceID": 10, "context": "(Reck et al., 1994) or b) the square decomposition method of Clements et al.", "startOffset": 0, "endOffset": 19}, {"referenceID": 14, "context": "a Projective URNN (Wisdom et al., 2016).", "startOffset": 18, "endOffset": 39}, {"referenceID": 4, "context": "We first compare these networks by applying them all to the so-called Memory Copying Task, which remains the most characteristically difficult task for RNNs (Hochreiter & Schmidhuber, 1997; Arjovsky et al., 2015; Henaff et al., 2016).", "startOffset": 157, "endOffset": 233}, {"referenceID": 4, "context": "Specifically, the task is defined as follows (Hochreiter & Schmidhuber, 1997; Arjovsky et al., 2015; Henaff et al., 2016).", "startOffset": 45, "endOffset": 121}, {"referenceID": 14, "context": ", 2015) and (Wisdom et al., 2016).", "startOffset": 12, "endOffset": 33}, {"referenceID": 14, "context": ", 2015) and projective (Wisdom et al., 2016) unitary methods, and also perform more robustly.", "startOffset": 23, "endOffset": 44}, {"referenceID": 14, "context": "EURNN corresponds to our algorithm, projective URNN corresponds to algorithm presented in (Wisdom et al., 2016), URNN corresponds to the algorithm presented in (Arjovsky et al.", "startOffset": 90, "endOffset": 111}, {"referenceID": 14, "context": "Following the procedure of (Arjovsky et al., 2015; Wisdom et al., 2016), we apply RNN models to the MNIST handwriting data set by feeding one row of pixels to the RNN at each step.", "startOffset": 27, "endOffset": 71}, {"referenceID": 14, "context": "EURNN corresponds to our algorithm, projective URNN corresponds to algorithm presented in (Wisdom et al., 2016), URNN corresponds to the algorithm presented in (Arjovsky et al.", "startOffset": 90, "endOffset": 111}, {"referenceID": 9, "context": "Note added: Recently, during the final proofreading phase of our manuscript, another paper that goes in a similar direction as ours (but parametrizing the unitary matrix by using reflections rather than rotations) was posted (Mhammedi et al., 2016).", "startOffset": 225, "endOffset": 248}], "year": 2017, "abstractText": "We present a method for implementing an Efficient Unitary Neural Network (EUNN) whose computational complexity is merelyO(1) per parameter and has full tunability, from spanning part of unitary space to all of it. We apply the EUNN in Recurrent Neural Networks, and test its performance on the standard copying task and the MNIST digit recognition benchmark, finding that it significantly outperforms a non-unitary RNN, an LSTM network, an exclusively partial space URNN and a projective URNN with comparable parameter numbers.", "creator": "LaTeX with hyperref package"}}}