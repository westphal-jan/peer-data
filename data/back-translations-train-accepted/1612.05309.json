{"id": "1612.05309", "review": {"conference": "AAAI", "VERSION": "v1", "DATE_OF_SUBMISSION": "15-Dec-2016", "title": "Multi-Agent Path Finding with Delay Probabilities", "abstract": "Several recently developed Multi-Agent Path Finding (MAPF) solvers scale to large MAPF instances by searching for MAPF plans on 2 levels: The high-level search resolves collisions between agents, and the low-level search plans paths for single agents under the constraints imposed by the high-level search. We make the following contributions to solve the MAPF problem with imperfect plan execution with small average makespans: First, we formalize the MAPF Problem with Delay Probabilities (MAPF-DP), define valid MAPF-DP plans and propose the use of robust plan-execution policies for valid MAPF-DP plans to control how each agent proceeds along its path. Second, we discuss 2 classes of decentralized robust plan-execution policies (called Fully Synchronized Policies and Minimal Communication Policies) that prevent collisions during plan execution for valid MAPF-DP plans. Third, we present a 2-level MAPF-DP solver (called Approximate Minimization in Expectation) that generates valid MAPF-DP plans.", "histories": [["v1", "Thu, 15 Dec 2016 23:33:41 GMT  (178kb,D)", "http://arxiv.org/abs/1612.05309v1", "To appear in AAAI 2017"]], "COMMENTS": "To appear in AAAI 2017", "reviews": [], "SUBJECTS": "cs.AI cs.MA cs.RO", "authors": ["hang ma 0001", "t k satish kumar", "sven koenig"], "accepted": true, "id": "1612.05309"}, "pdf": {"name": "1612.05309.pdf", "metadata": {"source": "META", "title": "Multi-Agent Path Finding with Delay Probabilities", "authors": ["Hang Ma", "T. K. Satish Kumar", "Sven Koenig"], "emails": ["hangma@usc.edu", "tkskwork@gmail.com", "skoenig@usc.edu"], "sections": [{"heading": "Introduction", "text": "In fact, most of them will be able to play by the rules that they have set themselves, and they will be able to play by the rules that they have set themselves."}, {"heading": "Background and Related Work", "text": "This year it has come to the point that it has never come as far as this year."}, {"heading": "Problem Definition: Planning", "text": "A MAPF-DP instance is characterized by an undirected graph G = (V, E), whose vertices V correspond to the vertices, and whose edges E correspond to the transitions between the locations. We get m-Agent a1, a2.... Each Agentai has a unique starting point si-V, a unique destination point gi-V, and a delay probability pi-V (0, 1). A path for Agent ai is expressed by a function li associated with each time index x = 0, 1... Xi to a vertex li (x) \u0445V, so that li (0) = si, consecutive vertices li (x) and li (x + 1) are either identical (if Agent ai is intended to execute a waiting action) or are connected by an edge (if Agent ai to execute a motion action from vertex to vertex (x + 1) and li Xi () = A Mgi path consists of one MAPF path for each agent."}, {"heading": "Problem Definition: Plan Execution", "text": "\u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b0 \u00b0 \u00b0 \u00b0 \u00b0 \u00b0 \u00b0 \u00b0 \u00b0 \u00b0 \u00b0 \u00b0 \u00b0 \u00b0 \u00b0 \u00b0 \u00b0 \u00b0 \u00b0 \u00b0 \u00b0 \u00b0 \u00b0 \u00b0 \u00b0 \u00b0 \u00b0 \u00b0 \u00b0 \u00b0 \u00b0 \u00b0 \u00b0 \u00b0 \u00b0 \u00b0 \u00b0 \u00b0 \u00b0 \u00b0 \u00b0 \u00b0 \u00b0 \u00b0 \u00b0 \u00b0 \u00b0 \u00b0 \u00b0 \u00b0 \u00b0 \u00b0 \u00b0 \u00b0 \u00b0 \u00b0 \u00b0 \u00b0 \u00b0 \u00b0 \u00b0 \u00b0 \u00b0 \u00b0 \u00b0 \u00b0 \u00b0 \u00b0 \u00b0 \u00b0 \u00b0 \u00b0 \u00b0 \u00b0 \u00b0 \u00b0 \u00b0 \u00b0 \u00b0 \u00b0 \u00b0 \u00b0 \u00b0 \u00b0 \u00b0 \u00b0 \u00b0 \u00b0 \u00b0 \u00b0 \u00b0 \u00b0 \u00b0 \u00b0 \u00b0 \u00b0 \u00b0 \u00b0 \u00b0 \u00b0 \u00b0 \u00b0 \u00b0 \u00b0 \u00b0 \u00b0 \u00b0 \u00b0 \u00b0 \u00b0 \u00b0 \u00b0 \u00b0 \u00b0 \u00b0 \u00b0 \u00b0 \u00b0 \u00b0 \u00b0 \u00b0 \u00b0 \u00b0 \u00b0 \u00b0 \u00b0 \u00b0 \u00b0 \u00b0 \u00b0 \u00b0 \u00b0 \u00b0 \u00b0 \u00b0 \u00b0 \u00b0 \u00b0 \u00b0 \u00b0 \u00b0 \u00b0 \u00b0 \u00b0 \u00b0 \u00b0 \u00b0 \u00b0 \u00b0 \u00b0 \u00b0 \u00b0 \u00b0 \u00b0 \u00b0 \u00b0 \u00b0 \u00b0 \u00b0 \u00b0 \u00b0"}, {"heading": "Valid MAPF-DP Plans", "text": "Definition 1. A valid MAPF-DP plan is a plan with 2 properties: 1. DP, j, x with i 6 = j: li (x) 6 = lj (x) [two agents are never intended for the same vertex, i.e. the vertices of two agents in the same local state are different] 2. Hi, j, x with i 6 = j: li (x + 1) 6 = lj (x) [an agent is never intended for a vertex in a time index x + 1 if another agent is to be in the same vertex at the same time. Index x, i.e. the vertex of an agent in a local state x + 1 must be different from the vertex of another agent in the local state x. Figure 1 shows that a sample MAPF-DP instance where the blue agent a1, valid vertex v3 and the red agent has a perfect vertex v2."}, {"heading": "Robust Plan-Execution Policies", "text": "We examine two types of decentralized robust plan implementation guidelines for valid MAPF-DP plans. These are plan implementation guidelines that prevent any collisions during the imperfect implementation of valid MAPF-DP plans."}, {"heading": "Fully Synchronized Policies (FSPs)", "text": "Fully synchronized guidelines (FSPs) try to keep all agents in check as much as possible by providing an agent with a GO command if and only if the agent has not yet entered his last local state and all other agents have either entered their last local state or left all local states preceding the agent's local state itself. FSPs can be easily implemented if each agent sends a message to all other agents when he enters a new local state. An agent can simply implement his FSP by counting how many messages he has received from each other agent and giving himself a GO command even in the local state x if and only if he has not yet entered his last local state and received x messages from each other agent during the execution of the plan."}, {"heading": "Minimal Communication Policies (MCPs)", "text": "The first is that agents wait unnecessarily, resulting in large average Makespans. Second, each agent must always know the local states of all other agents, which leads to many messages being sent. Property 2 of Definition 1 suggests that robust plan execution guidelines for valid MAPF-DP plans can send a GO command to an agent if the agent has not yet entered his last local state and all other agents have left all local states that preceded the agent's local state itself, and the deepenings of which are the same as the vertex of the agent's next local state itself. This guarantees that the vertex of the agent's next local state is different from the deepenings of all other agents in their current local states. Minimal Communication Policies (MCPs) address these drawbacks by identifying and obeying such critical dependencies between agents during plan execution, an idea that is not more robust in the context of centralized local states."}, {"heading": "Properties of FSPs and MCPs", "text": "Both FSPs and MCPs do not cause stoppages during the execution of valid MAPF-DP plans, as there is always at least one agent who receives a GO command before all agents have entered their last local states (namely, an agent with the smallest local state of all agents who have not yet entered their last local states, since an agent can only wait for other agents with smaller local states).Both FSPs and MCPs are robust planning execution guidelines based on properties 1 and 2 of valid MAPF-DP plans. We now provide a proof sketch of the robustness of MCPs. First, we consider a valid MAPF-DF plan and assume that li (x) = lj (y) for two agents ai and aj with i = j 6 = j. Then we supply 1) y = x since li (x) for property x x and 2) y for property x."}, {"heading": "Approximate Minimization in Expectation", "text": "MCPs are robust plan execution policies for valid MAPFDP plans that do not stop agents unnecessarily and result in under-sent messages. We introduce an MAPF-DP solver, called \"Approximate Minimization in Expectation\" (AME), that sets valid MAPF-DP plans so that their combination with MCPs results in small average Makespans.AME is a 2-level MAPF-DP solver based on conflict-based search (CBS) (Sharon et al. 2015).Its high-level search imposes low-level search restrictions that resolve violations of properties 1 and 2 of Definition 1 (referred to as conflicts).Its low-threshold search plans are ways for individual agents to obey these limitations and achieve results in small average Makespans. The average macespan of a MAPF-DP plan is the expectation of the maximum (or more) time steps that occur in each of the states that fall within their local time limits."}, {"heading": "High-Level Search", "text": "Each high-level node N contains the following points: 1. A set of N.Restrictions on the limitations of form (ai, ai, x) stating that the vertex of Agent ai in the local state x must be different from the vertex l. 2. A (labeled) MAPF-DP Plan N.Plan containing a path li for each Agent ai (following the limitations of N.s) and an approximation l (x) of the respective average time step when Agent ai enters the local state x during the execution of the plan with MCPs. 3. The key of the high-level node N encoding its priority (smaller keys have a higher priority) and is equal to the approximate average Makespan of the MAPF-DP Plan N.Plan provided by ApproximateAverageMakespan (Plan) = maxi i Plan (Xi-DP in a high-level case N.P)."}, {"heading": "Low-Level Search", "text": "It uses the paths of the other agents and their labels in N.Plan, but does not update them. (The paths are empty immediately after executing line 2.) It performs a focal search with re-expansions in a state space whose states correspond to pairs of depressions and local states (except for those pairs excluded by constraints in N.E.) and whose edges connect to the state (l, x) to determine the state (l, x) (l, l) and only if l (for a wait action) or (l, l). The g value of a state (l, x) approaches the approximate average time step l (x). The start state is (si, 0) and its g value is 0."}, {"heading": "Future Work", "text": "The low-level search is currently the weakest part of the AME due to the many approaches used to minimize its duration, which is important because the high-level search performs many low-level searches. We expect future work to significantly improve the low-level search. For example, the approximate average time steps for agents that differ from agents could be updated before, during, or after the local search, which would provide more accurate values for current and future low-level searches as well as the current high-level search. Once the low-level search finds a path for agent ai, and the high-level search replaces the path for agent ai in the MAPF-DP plan in the current high-level node with this path, it could update the approximate average time steps of all agents to the ideal average time steps specified in Equations (1), for example as part of executing ApproximateA21. There are also many possible improvements to the 7 and many other periods."}, {"heading": "Experiments", "text": "In fact, it is as if most of them do not see themselves as being able to follow the rules that they have imposed on themselves. (...) It is not as if they are able to understand the rules. (...) It is not as if they are able to understand the rules. (...) It is not as if they are able to follow the rules. (...) It is as if they are able to break the rules. \"(...) It is as if they are able to follow the rules.\" (...) It is not as if they are able to follow the rules. \"(...) It is as if they are able to follow the rules.\" (...) It is as if they are able to follow the rules. \"(...) It is as if they are able to follow the rules. (...) It is as if they are able to follow the rules."}, {"heading": "50 0.94 0.166 69.32 75.19 474.62", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "100 0.68 4.668 78.48 87.29 1,554.71", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "150 0.10 134.155 81.77 96.43 2,940.40", "text": "Experiment 4: Plan-Execution PoliciesWe use with 3 plan-execution policies, namely 1) MCPs, 2) FSPs and 3) Dummy (non-robust) plan-execution policies that always provide GO commands. We repeat Experiment 1 for each plan-execution policy. Table 4 reports for each solved MAPF-DP instance and plan-execution policy the average macespan of over 1,000 plan-execution policies runs along with 95% confidence intervals, the number of messages sent for MCPs and FSPs, and the average number of collisions for dummy plan-execution policies. The number of messages sent is zero (and is therefore not displayed) for dummy plan-execution policies, as they do not prevent collisions, unlike MCPs and FSPs. The average macespan for MCPs appears to be only slightly larger than that for dummy plan-execution policies, and the average macespan for the number of messages and the SPPs to be smaller than the SPPs."}, {"heading": "Conclusions", "text": "In this paper, we formalized the Multi-Agent Path-Finding Problem with Delay Probabilities (MAPF-DP) to account for imperfect plan execution, and then developed an efficient method of solving this problem with low average macespans, namely Approximate Minimization in Expectation (a 2-step MAPF-DP solver for creating valid MAPF-DP plans) and Minimal Communication Policies (decentralized robust planning execution guidelines for executing valid MAPF-DP plans without collisions)."}], "references": [{"title": "The transitive reduction of a directed graph", "author": ["A.V. Aho", "M.R. Garey", "J.D. Ullman"], "venue": "SIAM Journal on Computing 1(2):131\u2013137.", "citeRegEx": "Aho et al\\.,? 1972", "shortCiteRegEx": "Aho et al\\.", "year": 1972}, {"title": "Solving transition independent decentralized Markov decision processes", "author": ["R. Becker", "S. Zilberstein", "V. Lesser", "C.V. Goldman"], "venue": "Journal of Artificial Intelligence Research 22(1):423\u2013455.", "citeRegEx": "Becker et al\\.,? 2004", "shortCiteRegEx": "Becker et al\\.", "year": 2004}, {"title": "Planning, learning and coordination in multiagent decision processes", "author": ["C. Boutilier"], "venue": "Conference on Theoretical Aspects of Rationality and Knowledge, 195\u2013210.", "citeRegEx": "Boutilier,? 1996", "shortCiteRegEx": "Boutilier", "year": 1996}, {"title": "ICBS: Improved conflict-based search algorithm for multi-agent pathfinding", "author": ["E. Boyarski", "A. Felner", "R. Stern", "G. Sharon", "D. Tolpin", "O. Betzalel", "S.E. Shimony"], "venue": "International Joint Conference on Artificial Intelligence, 740\u2013746.", "citeRegEx": "Boyarski et al\\.,? 2015", "shortCiteRegEx": "Boyarski et al\\.", "year": 2015}, {"title": "Improved solvers for bounded-suboptimal multiagent path finding", "author": ["L. Cohen", "T. Uras", "T.K.S. Kumar", "H. Xu", "N. Ayanian", "S. Koenig"], "venue": "International Joint Conference on Artificial Intelligence, 3067\u20133074.", "citeRegEx": "Cohen et al\\.,? 2016", "shortCiteRegEx": "Cohen et al\\.", "year": 2016}, {"title": "Enhanced Partial Expansion A", "author": ["M. Goldenberg", "A. Felner", "R. Stern", "G. Sharon", "N.R. Sturtevant", "R.C. Holte", "J. Schaeffer"], "venue": "Journal of Artificial Intelligence Research 50:141\u2013 187.", "citeRegEx": "Goldenberg et al\\.,? 2014", "shortCiteRegEx": "Goldenberg et al\\.", "year": 2014}, {"title": "Decentralized control of cooperative systems: Categorization and complexity analysis", "author": ["C.V. Goldman", "S. Zilberstein"], "venue": "Journal of Artificial Intelligence Research 22:143\u2013174.", "citeRegEx": "Goldman and Zilberstein,? 2004", "shortCiteRegEx": "Goldman and Zilberstein", "year": 2004}, {"title": "Multi-agent path finding with kinematic constraints", "author": ["W. H\u00f6nig", "T.K.S. Kumar", "L. Cohen", "H. Ma", "H. Xu", "N. Ayanian", "S. Koenig"], "venue": "International Conference on Automated Planning and Scheduling, 477\u2013485.", "citeRegEx": "H\u00f6nig et al\\.,? 2016", "shortCiteRegEx": "H\u00f6nig et al\\.", "year": 2016}, {"title": "Coordinating pebble motion on graphs, the diameter of permutation groups, and applications", "author": ["D. Kornhauser", "G. Miller", "P. Spirakis"], "venue": "Annual Symposium on Foundations of Computer Science, 241\u2013250.", "citeRegEx": "Kornhauser et al\\.,? 1984", "shortCiteRegEx": "Kornhauser et al\\.", "year": 1984}, {"title": "SARSOP: Efficient point-based POMDP planning by approximating optimally reachable belief spaces", "author": ["H. Kurniawati", "D. Hsu", "W.S. Lee"], "venue": "Robotics: Science and Systems, 65\u201372.", "citeRegEx": "Kurniawati et al\\.,? 2008", "shortCiteRegEx": "Kurniawati et al\\.", "year": 2008}, {"title": "An MDP-based approximation method for goal constrained multi-MAV planning under action uncertainty", "author": ["L. Liu", "N. Michael"], "venue": "IEEE International Conference on Robotics and Automation, 56\u201362.", "citeRegEx": "Liu and Michael,? 2016", "shortCiteRegEx": "Liu and Michael", "year": 2016}, {"title": "Push and Swap: Fast cooperative path-finding with completeness guarantees", "author": ["R. Luna", "K.E. Bekris"], "venue": "International Joint Conference on Artificial Intelligence, 294\u2013300.", "citeRegEx": "Luna and Bekris,? 2011", "shortCiteRegEx": "Luna and Bekris", "year": 2011}, {"title": "Optimal target assignment and path finding for teams of agents", "author": ["H. Ma", "S. Koenig"], "venue": "International Conference on Autonomous Agents and Multiagent Systems, 1144\u20131152.", "citeRegEx": "Ma and Koenig,? 2016", "shortCiteRegEx": "Ma and Koenig", "year": 2016}, {"title": "Information gathering and reward exploitation of subgoals for POMDPs", "author": ["H. Ma", "J. Pineau"], "venue": "AAAI Conference on Artificial Intelligence, 3320\u20133326.", "citeRegEx": "Ma and Pineau,? 2015", "shortCiteRegEx": "Ma and Pineau", "year": 2015}, {"title": "Multi-agent path finding with payload transfers and the package-exchange robot-routing problem", "author": ["H. Ma", "C. Tovey", "G. Sharon", "T.K.S. Kumar", "S. Koenig"], "venue": "AAAI Conference on Artificial Intelligence, 3166\u20133173.", "citeRegEx": "Ma et al\\.,? 2016", "shortCiteRegEx": "Ma et al\\.", "year": 2016}, {"title": "Decentralized MDPs with sparse interactions", "author": ["F.S. Melo", "M. Veloso"], "venue": "Artificial Intelligence 175(11):1757\u20131789.", "citeRegEx": "Melo and Veloso,? 2011", "shortCiteRegEx": "Melo and Veloso", "year": 2011}, {"title": "Planning, scheduling and monitoring for airport surface operations", "author": ["R. Morris", "C. Pasareanu", "K. Luckow", "W. Malik", "H. Ma", "S. Kumar", "S. Koenig"], "venue": "AAAI-16 Workshop on Planning for Hybrid Systems, 608\u2013614.", "citeRegEx": "Morris et al\\.,? 2016", "shortCiteRegEx": "Morris et al\\.", "year": 2016}, {"title": "Solving transition-independent multiagent MDPs with sparse interactions", "author": ["J. Scharpff", "D.M. Roijers", "F.A. Oliehoek", "M.T.J. Spaan", "M.M. de Weerdt"], "venue": "In AAAI Conference on Artificial Intelligence,", "citeRegEx": "Scharpff et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Scharpff et al\\.", "year": 2016}, {"title": "The increasing cost tree search for optimal multi-agent pathfinding", "author": ["G. Sharon", "R. Stern", "M. Goldenberg", "A. Felner"], "venue": "Artificial Intelligence 195:470\u2013495.", "citeRegEx": "Sharon et al\\.,? 2013", "shortCiteRegEx": "Sharon et al\\.", "year": 2013}, {"title": "Conflict-based search for optimal multi-agent pathfinding", "author": ["G. Sharon", "R. Stern", "A. Felner", "N.R. Sturtevant"], "venue": "Artificial Intelligence 219:40\u201366.", "citeRegEx": "Sharon et al\\.,? 2015", "shortCiteRegEx": "Sharon et al\\.", "year": 2015}, {"title": "Cooperative pathfinding", "author": ["D. Silver"], "venue": "Artificial Intelligence and Interactive Digital Entertainment, 117\u2013122.", "citeRegEx": "Silver,? 2005", "shortCiteRegEx": "Silver", "year": 2005}, {"title": "Finding optimal solutions to cooperative pathfinding problems", "author": ["T.S. Standley"], "venue": "AAAI Conference on Artificial Intelligence, 173\u2013178.", "citeRegEx": "Standley,? 2010", "shortCiteRegEx": "Standley", "year": 2010}, {"title": "Distributed model shaping for scaling to decentralized POMDPs with hundreds of agents", "author": ["P. Velagapudi", "P. Varakantham", "K.P. Sycara", "P. Scerri"], "venue": "International Conference on Autonomous Agents and Multi-agent Systems, 955\u2013962.", "citeRegEx": "Velagapudi et al\\.,? 2011", "shortCiteRegEx": "Velagapudi et al\\.", "year": 2011}, {"title": "CoBots: Robust symbiotic autonomous mobile service robots", "author": ["M. Veloso", "J. Biswas", "B. Coltin", "S. Rosenthal"], "venue": "International Joint Conference on Artificial Intelligence, 4423\u2013 4429.", "citeRegEx": "Veloso et al\\.,? 2015", "shortCiteRegEx": "Veloso et al\\.", "year": 2015}, {"title": "Subdimensional expansion for multirobot path planning", "author": ["G. Wagner", "H. Choset"], "venue": "Artificial Intelligence 219:1\u201324.", "citeRegEx": "Wagner and Choset,? 2015", "shortCiteRegEx": "Wagner and Choset", "year": 2015}, {"title": "Subdimensional Expansion: A Framework for Computationally Tractable Multirobot Path Planning", "author": ["G. Wagner"], "venue": "Ph.D. Dissertation, Carnegie Mellon University.", "citeRegEx": "Wagner,? 2015", "shortCiteRegEx": "Wagner", "year": 2015}, {"title": "MAPP: a scalable multi-agent path planning algorithm with tractability and completeness guarantees", "author": ["K. Wang", "A. Botea"], "venue": "Journal of Artificial Intelligence Research 42:55\u201390.", "citeRegEx": "Wang and Botea,? 2011", "shortCiteRegEx": "Wang and Botea", "year": 2011}, {"title": "Coordinating hundreds of cooperative, autonomous vehicles in warehouses. AI Magazine 29(1):9\u201320", "author": ["P.R. Wurman", "R. D\u2019Andrea", "M. Mountz"], "venue": null, "citeRegEx": "Wurman et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Wurman et al\\.", "year": 2008}], "referenceMentions": [{"referenceID": 16, "context": "MAPF problems arise for aircraft towing vehicles (Morris et al. 2016), office robots (Veloso et al.", "startOffset": 49, "endOffset": 69}, {"referenceID": 23, "context": "2016), office robots (Veloso et al. 2015), video game characters (Silver 2005) and warehouse robots (Wurman, D\u2019Andrea, and Mountz 2008), among others.", "startOffset": 21, "endOffset": 41}, {"referenceID": 20, "context": "2015), video game characters (Silver 2005) and warehouse robots (Wurman, D\u2019Andrea, and Mountz 2008), among others.", "startOffset": 29, "endOffset": 42}, {"referenceID": 14, "context": "The MAPF problem is NP-hard to solve optimally for flowtime minimization and to approximate within any constant factor less than 4/3 for makespan minimization (Ma et al. 2016).", "startOffset": 159, "endOffset": 175}, {"referenceID": 13, "context": "The MAPF-DP problem can be solved with POMDPs but this is tractable only for very few agents in very small environments since the size of the state space is proportional to the size of the environment to the power of the number of agents and the size of the belief space is proportional to the size of the state space to the power of the length of the planning horizon (Kurniawati, Hsu, and Lee 2008; Ma and Pineau 2015).", "startOffset": 369, "endOffset": 420}, {"referenceID": 1, "context": "Several specialized probabilistic planning frameworks, such as transition-independent decentralized Markov Decision Processes (DecMDPs) (Becker et al. 2004) and Multi-Agent Markov Decision Processes (MMDPs) (Boutilier 1996) can solve larger probabilistic planning problems than POMDPs.", "startOffset": 136, "endOffset": 156}, {"referenceID": 2, "context": "2004) and Multi-Agent Markov Decision Processes (MMDPs) (Boutilier 1996) can solve larger probabilistic planning problems than POMDPs.", "startOffset": 56, "endOffset": 72}, {"referenceID": 6, "context": "In transition-independent Dec-MDPs, the local state of each agent depends only on its previous local state and the action taken by it (Goldman and Zilberstein 2004).", "startOffset": 134, "endOffset": 164}, {"referenceID": 17, "context": "For example, the MAPF-DP problem can be solved with transition-independent MMDPs (Scharpff et al. 2016).", "startOffset": 81, "endOffset": 103}, {"referenceID": 10, "context": "In fact, the most closely related research to ours is that on approximating MMDPs (Liu and Michael 2016) although it handles different types of dynamics than we do.", "startOffset": 82, "endOffset": 104}, {"referenceID": 15, "context": "For example, decentralized sparse-interaction Markov Decision Processes (Dec-SIMDPs) (Melo and Veloso 2011) assume that interactions among agents occur only in well-defined interaction areas in the environment (which is not the case for MAPF-DP in general), but typically still do not scale to more than 10 agents.", "startOffset": 85, "endOffset": 107}, {"referenceID": 22, "context": "The model shaping technique for decentralized POMDPs (Velagapudi et al. 2011) can compute policies for hundreds of agents greedily and UM* (Wagner 2015) scales to larger numbers of agents (with identical delay probabilities), but the plan execution for both approaches is completely decentralized and thus cannot prevent collisions.", "startOffset": 53, "endOffset": 77}, {"referenceID": 25, "context": "2011) can compute policies for hundreds of agents greedily and UM* (Wagner 2015) scales to larger numbers of agents (with identical delay probabilities), but the plan execution for both approaches is completely decentralized and thus cannot prevent collisions.", "startOffset": 67, "endOffset": 80}, {"referenceID": 7, "context": "Minimal Communication Policies (MCPs) address these drawbacks by identifying such critical dependencies between agents and obeying them during plan execution, an idea that originated in the context of centralized non-robust plan-execution policies (H\u00f6nig et al. 2016).", "startOffset": 248, "endOffset": 267}, {"referenceID": 19, "context": "AME is a 2-level MAPF-DP solver that is based on Conflict-Based Search (CBS) (Sharon et al. 2015).", "startOffset": 77, "endOffset": 97}, {"referenceID": 11, "context": "We compare AME to 2 MAPF solvers, namely 1) Adapted CBS, a CBS variant that assumes perfect plan execution and computes valid MAPF-DP plans, minimizes maxi Xi and breaks ties toward paths with smaller Xi and thus fewer actions and 2) Push and Swap (Luna and Bekris 2011), a MAPF solver that assumes perfect plan execution and computes valid MAPF-DP plans where exactly one agent executes a move action at each time step and all other agents execute wait actions.", "startOffset": 248, "endOffset": 270}], "year": 2016, "abstractText": "Several recently developed Multi-Agent Path Finding (MAPF) solvers scale to large MAPF instances by searching for MAPF plans on 2 levels: The high-level search resolves collisions between agents, and the low-level search plans paths for single agents under the constraints imposed by the high-level search. We make the following contributions to solve the MAPF problem with imperfect plan execution with small average makespans: First, we formalize the MAPF Problem with Delay Probabilities (MAPF-DP), define valid MAPF-DP plans and propose the use of robust plan-execution policies for valid MAPF-DP plans to control how each agent proceeds along its path. Second, we discuss 2 classes of decentralized robust plan-execution policies (called Fully Synchronized Policies and Minimal Communication Policies) that prevent collisions during plan execution for valid MAPF-DP plans. Third, we present a 2-level MAPF-DP solver (called Approximate Minimization in Expectation) that generates valid MAPF-DP plans.", "creator": "TeX"}}}