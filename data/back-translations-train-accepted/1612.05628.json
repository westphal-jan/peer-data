{"id": "1612.05628", "review": {"conference": "ICML", "VERSION": "v1", "DATE_OF_SUBMISSION": "16-Dec-2016", "title": "An Alternative Softmax Operator for Reinforcement Learning", "abstract": "A softmax operator applied to a set of values acts somewhat like the maximization function and somewhat like an average. In sequential decision making, softmax is often used in settings where it is necessary to maximize utility but also to hedge against problems that arise from putting all of one's weight behind a single maximum utility decision. The Boltzmann softmax operator is the most commonly used softmax operator in this setting, but we show that this operator is prone to misbehavior. In this work, we study an alternative softmax operator that, among other properties, is both a non-expansion (ensuring convergent behavior in learning and planning) and differentiable (making it possible to improve decisions via gradient descent methods). We provide proofs of these properties and present empirical comparisons between various softmax operators.", "histories": [["v1", "Fri, 16 Dec 2016 20:49:35 GMT  (2709kb,D)", "http://arxiv.org/abs/1612.05628v1", null], ["v2", "Mon, 19 Dec 2016 19:02:06 GMT  (2709kb,D)", "http://arxiv.org/abs/1612.05628v2", null], ["v3", "Wed, 21 Dec 2016 02:05:31 GMT  (2710kb,D)", "http://arxiv.org/abs/1612.05628v3", null], ["v4", "Tue, 13 Jun 2017 05:28:04 GMT  (2296kb,D)", "http://arxiv.org/abs/1612.05628v4", null], ["v5", "Wed, 14 Jun 2017 14:29:04 GMT  (2296kb,D)", "http://arxiv.org/abs/1612.05628v5", null]], "reviews": [], "SUBJECTS": "cs.AI cs.LG stat.ML", "authors": ["kavosh asadi", "michael l littman"], "accepted": true, "id": "1612.05628"}, "pdf": {"name": "1612.05628.pdf", "metadata": {"source": "META", "title": "A New Softmax Operator for Reinforcement Learning", "authors": ["Kavosh Asadi", "Michael L. Littman"], "emails": ["KAVOSH@BROWN.EDU", "MLITTMAN@BROWN.EDU"], "sections": [{"heading": "1. Introduction", "text": "There is a fundamental tension in decision-making between choosing the measure that has the highest expected reward estimate and avoiding \"starvation\" of the other measures. However, the problem arises in the context of the dilemma between exploration and exploitation (Thrun, 1992), non-stationary decision problems (Sutton, 1990), and in interpreting the observed decisions (Baker et al., 2007). However, in the constellation of affirmation, a typical approach to addressing this tension is the use of Softmax operators for optimizing value functions and Softmax strategies for selecting measures. Examples of this commonly used approach include value-based methods such as SARSA (Rummery & Niranjan, 1994) or expected SARSA (Sutton & Barto, 1998; Van Seijen et al., 2009) and political search methods such as REINFORCE (Williams, 1992). An ideal Softmax operator is a 1. Parameterization of parameters that allow operators to have exactly one set of parameters:"}, {"heading": "2. Boltzmann Operator Is Prone to Misbehavior", "text": "First, we show that Boltzmann can lead to problematic behavior in learning. To this end, we have set SARSA with Boltzmann-Q policy (algorithm 1) on the MDP in Figure 1. The edges are labeled with a transition probability (without sign) and a reward (signed). Note, however, that in this MDP rewards are functions of states and actions, not the next states. Furthermore, the state s2 is a final state, so we consider only two action values, namely Q value (s1, a) and Q value (s2, b). Remember that Boltzmann-Softmax policy assigns the following probability to each action: \u03c0 (a-s) = e-Q value (s, a). In Figure 2, we plot state action values at the end of each episode of a single action (smoothed by averaging over ten consecutive points)."}, {"heading": "3. Background", "text": "A Markov decision-making process (Puterman, 1994), or MDP, is defined by the tuple < Q = Q = Q, A, P, Q >, where S is the totality of states and A is the totality of actions. The functions R and P denote the reward and transition dynamics of the MDP. More specifically: the expected immediate reward following an action A in a state S before it is converted into a next state S's is determined by: R (s, a, s \"s\" s) = E [Rt + 1 | s, At = a, St + 1 = s, \"and the probability of this transition is determined by: P (s, a\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s"}, {"heading": "4. Boltzmann Has Multiple Fixed Points", "text": "Although it has long been known that the Boltzmann operator is not a non-expansion (Littman, 1996), we are not aware of any published example of an MDP for which two different fixed points exist. MDP shown in Figure 1 is the first example of such an MDP. As shown in Figure 4, GVI has two fixed points under Boltz\u03b2 for \u03b2 [16,53, 16,78]. (For other \u03b2 values, we found a single fixed point.) We also show a vector field that visualizes GVI updates under Boltz\u03b2 = 16,53. Thus, the behavior of SARSA in Figure 2 is caused by the algorithm that jumps back and forth between the two different fixed points in a poignant manner."}, {"heading": "5. Mellowmax and its Properties", "text": "We advocate an alternative Softmax operator defined as mm\u03c9 (X) = log (1n \u0445 n i = 1 e \u03c9xi) \u03c9, which can be regarded as a special instance of the quasiarithmetic mean (Beliakov et al., 2016). We show that mm\u03c9 (X), which we call Mellowmax, has all four desired properties and is compared favourably with boltz\u03b2 in practice."}, {"heading": "5.1. Mellowmax is a Non-Expansion", "text": "We prove that mm\u03c9 is a non-expansion (property 2), and therefore the convergence of the GVI below mm\u03c9 to a single fixed point is guaranteed. Let's allow X = x1,.., xn and Y = y1,.., yn to be two vectors of values. Let's allow the difference of the ith components of the two vectors. Let's also allow me to be the index with the maximum component-wise difference, i = argmaxi. For simplicity's sake, let's assume that I am unique. Even without losing generality, let's assume that xi-yi, xi-yi,."}, {"heading": "5.2. Maximization", "text": "Mellowmax contains parameter settings that allow both maximization (property 1) and minimization. Note in particular that W \u2265 1 is the number of maximum values (\"winners\") in X. Then: lim \u03c9 (X) = lim \u03c9 (e\u03c9m) \u2192 \u221e log (1n \u2211 n = 1 e \u03c9xi) \u03c9 = lim \u03c9 (1ne \u03c9m \u0445 n = 1 e \u03c9 (xi \u2212 m) \u03c9 = lim \u03c9 (1 ne \u03c9xi) \u00b2 log (n) \u00b2 log (n) + log (W) \u03c9 = m + lim \u03c9 (1 n \u00b2 \u00b2) \u00b2 log (n) \u00b2 log (n) \u043c = lim \u03c9 (1 \u00b2 mW) \u043e = lim \u0438 (e\u03c9m) \u2012 log (n) \u2012 log (n). That is, the operator acts more and more like maximized (n) \u2192 the operator (N) + log (W) \u03c9 = \u043c (m + lim \u043c), the operator is minimal (n \u00b2)."}, {"heading": "5.3. Derivatives", "text": "We can take the derivative of mellowmax in relation to each of the arguments and for all other factors as null \u03c9: \u2202 mm\u03c9 (X) \u2202 xi = e\u03c9xi n = 1 e \u03c9xi \u2265 0.Note that the operator does not decrease in each component of X. Furthermore, we can take the derivative of mellowmax in relation to \u03c9. We define n\u03c9 (X) = log (1n n n = 1 e\u03c9xi) and d\u03c9 (X) = \u03c9. Then: \u2202 n\u03c9 (X) (X) \u0445 n = 1 xie \u03c9xi n = 1 e perspecxi and \u0445 d\u03c9 (X) = 1, and so on: \u0445 mm\u03c9 (X) nynynynynynynynynynynynynynynynynynynynynynynynynynynynynynynynynynynynynynynynynynynynynynynynynynynynynynynynynynynynynynynynynynynynynynynynynynynynynynynynynynynynynynynynynynynynynynynynynynynynynynynynynynynynynynynynynynynynynynynynynynynynynynynynynynynynynynynynynynynynynynynynynynynynynynynynynynynynynynynynynynynynynynynynynynynynynynynynynynynynynynynynynynynynynynynynynynynynynynynynynynynynynynynynynynynynynynynynynynynynynynynynynynynynynynynynynynynynynynynynynynynynynynynynynynynynynynynynynynynynynynyyyyyyyyyyyyyyya (X)) (X))"}, {"heading": "5.4. Averaging", "text": "Since both the counter and the denominator go to zero if \u03c9 goes to zero, we will use the L'Ho'pital rule and the derivative given in the previous section to derive the value in the limit: lim \u03c9 \u2192 0 mm\u03c9 (X) = lim \u03c9 \u2192 0log (1n \u0445 n i = 1 e \u03c9xi) \u03c9L'Ho'pital = lim\u03c9 \u2192 01 n \u0445 n \u0445 i = 1 xie \u03c9xi1 n \u0445 n \u0445 i = 1 e \u03c9xi = mean (X). That is, if we get closer to zero, mm\u03c9 (X) approaches the mean of the values in X."}, {"heading": "6. Maximum Entropy Mellowmax Policy", "text": "As described, mm\u03c9 calculates a value for a list of numbers somewhere between their minimum and maximum. However, it is often useful to create a probability distribution over the actions in such a way that (1) a non-zero probability mass is associated with each action and (2) the resulting expected value corresponds to the calculated value. (Such a probability distribution can then be used for selecting measures such as SARSA. In this section, we will address the problem of identifying such a probability distribution as the maximum entropy problem - across all distributions that meet the above properties, choose the one that maximizes the information tropy (Cover & Thomas, 2006; Peters et al., 2010). We formally define the maximum entropy policy of a state as: E (s) = argmin."}, {"heading": "7. Experiments On MDPs", "text": "Next, we repeat the experiment from Figure 2 with SARSA with the maximum entropy mellowmax policy and \u03c9 = 16.53. The results shown in Figure 6 show rapid convergence to the unique fixed point. Similar to Figure 5, we provide a vector field for GVI updates below mm\u03c9 = 16.53. As shown above, the use of mm\u03c9 ensures that GVI updates move the estimates steadily to the fixed point. Consequently, GVI can end significantly faster under mm\u03c9 than GVI under Boltz\u03b2, as shown in Figure 8. We now present two experiments on standard learning areas for amplification. The first experiment compares Softmax strategies when used in SARSA with a tabular representation. The second experiment is a policy gradient experiment using a deep neural network to directly represent policy."}, {"heading": "7.1. Multi-passenger Taxi Domain", "text": "We examined SARSA with various policies in the field of multi-passenger taxis introduced by (Dearden et al., 1998). (See Figure 9.) One challenging aspect of this area is that it allows for many locally optimal strategies, and exploration must be carefully defined to avoid over- or underexploration of state space. Also, note that Boltzmann-Softmax performs remarkably well in this area and outperforms Bayesian advanced learning algorithms for amplification. (Dearden et al., 1998) As shown in Figure 10, SARSA performs poorly with the epsilon-greedy policy. In fact, the algorithm in our experiment was rarely able to carry all passengers. However, SARSA with Boltzman-Softmax and SARSA with the maximum entropy-Millowmax policy achieved a significantly higher average reward. Maximum entropy-Millowmax policy is not inferior to Boltzman-policy here."}, {"heading": "7.2. Lunar Lander Domain", "text": "In this section, we evaluate the maximum entropy-mellowmax policy in the context of the policy gradient algorithms. Specifically, we represent politics through a deep neural network (we discuss the details of the network below). Normally, the activation function of the last layer of the network is a softmax function, and typically Boltzmann-Softmax boltz\u03b2 is used. Alternatively, we use the maximum entropy-mellowmax policy presented in Section 6, treating the input of the activation function as Q values. We used the lunar landing domain, by OpenAI Gym (Brockman et al., 2016) as the yardstick. A screenshot of the domain is presented in Figure 11, and the code of the domain is publicly available. This domain has a continuous state space with 8 dimensions, namely x-y coordinates, x-y velocities, x-y velocities, angles, and angular velocities."}, {"heading": "8. Related Work", "text": "Softmax operators play an important role in sequential decision-making algorithms. In model-free amplification learning, they can help strike a balance between exploration (mean) and exploitation (maximum). Decision-making rules based on epsilon-greedy and Boltzmann-Softmax are very simple, but often perform surprisingly well in practice and even exceed more advanced exploration techniques (Kuleshov & Precup, 2014). If you learn \"in politics,\" exploration steps (Rummery & Niranjan, 1994) can and perhaps should (John, 1994) become part of the appreciation process itself. At the political level, algorithms such as SARSA can be pushed to align themselves within the boundaries of optimal behavior if the exploration rate and the update operator are gradually shifted towards max (Singh et al., 2000)."}, {"heading": "9. Conclusion and Future Work", "text": "We have suggested the mellowmax operator as an alternative to the Boltzmann operator. We have shown that mellowmax has several desirable properties and will be advantageous.It is likely that mellowmax could be used in place of Boltzmann in the entire reinforcement learning research.Important future work is the extension of the scope of study to include the adjustment of the functional approximation in which the state space or action space is large and abstraction techniques are appliced.We expect the Mellowmax operator and its non-expansion characteristics to behave more consistently than the Boltzmann operator when estimates of the state values can be arbitrarily inaccurate. Another direction is the analysis of the fixed point of planning, reinforcement learning and game algorithms when using Softmax- and Mellowmax operators. In particular, an interesting analysis could be one that limits the suboptimality of the fixed points found by value division among each operator."}, {"heading": "10. Acknowledgments", "text": "The authors thank George Konidaris for his help."}], "references": [{"title": "Apprenticeship learning about multiple intentions", "author": ["Babes", "Monica", "Marivate", "Vukosi N", "Littman", "Michael L", "Subramanian", "Kaushik"], "venue": "In International Conference on Machine Learning,", "citeRegEx": "Babes et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Babes et al\\.", "year": 2011}, {"title": "Gradient descent for general reinforcement learning", "author": ["Baird", "Leemon", "Moore", "Andrew W"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Baird et al\\.,? \\Q1999\\E", "shortCiteRegEx": "Baird et al\\.", "year": 1999}, {"title": "Goal inference as inverse planning", "author": ["Baker", "Chris L", "Tenenbaum", "Joshua B", "Saxe", "Rebecca R"], "venue": "In Proceedings of the 29th Annual Meeting of the Cognitive Science Society,", "citeRegEx": "Baker et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Baker et al\\.", "year": 2007}, {"title": "A Practical Guide to Averaging", "author": ["Beliakov", "Gleb", "Sola", "Humberto Bustince", "S\u00e1nchez", "Tomasa Calvo"], "venue": null, "citeRegEx": "Beliakov et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Beliakov et al\\.", "year": 2016}, {"title": "Algorithms for minimization without derivatives", "author": ["Brent", "Richard P"], "venue": "Courier Corporation,", "citeRegEx": "Brent and P.,? \\Q2013\\E", "shortCiteRegEx": "Brent and P.", "year": 2013}, {"title": "Elements of Information Theory", "author": ["T.M. Cover", "J.A. Thomas"], "venue": null, "citeRegEx": "Cover and Thomas,? \\Q2006\\E", "shortCiteRegEx": "Cover and Thomas", "year": 2006}, {"title": "Bayesian Q-learning", "author": ["Dearden", "Richard", "Friedman", "Nir", "Russell", "Stuart"], "venue": "In Fifteenth National Conference on Artificial Intelligence (AAAI),", "citeRegEx": "Dearden et al\\.,? \\Q1998\\E", "shortCiteRegEx": "Dearden et al\\.", "year": 1998}, {"title": "Reinforcement learning with function approximation converges to a region, 2001. Unpublished", "author": ["Gordon", "Geoffrey J"], "venue": null, "citeRegEx": "Gordon and J.,? \\Q2001\\E", "shortCiteRegEx": "Gordon and J.", "year": 2001}, {"title": "When the best move isn\u2019t optimal: Q-learning with exploration", "author": ["John", "George H"], "venue": "In Proceedings of the Twelfth National Conference on Artificial Intelligence,", "citeRegEx": "John and H.,? \\Q1994\\E", "shortCiteRegEx": "John and H.", "year": 1994}, {"title": "Adam: A method for stochastic optimization", "author": ["Kingma", "Diederik", "Ba", "Jimmy"], "venue": "arXiv preprint arXiv:1412.6980,", "citeRegEx": "Kingma et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Kingma et al\\.", "year": 2014}, {"title": "Algorithms for multi-armed bandit problems", "author": ["Kuleshov", "Volodymyr", "Precup", "Doina"], "venue": "arXiv preprint arXiv:1402.6028,", "citeRegEx": "Kuleshov et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Kuleshov et al\\.", "year": 2014}, {"title": "A generalized reinforcement-learning model: Convergence and applications", "author": ["Littman", "Michael L", "Szepesv\u00e1ri", "Csaba"], "venue": "Proceedings of the Thirteenth International Conference on Machine Learning,", "citeRegEx": "Littman et al\\.,? \\Q1996\\E", "shortCiteRegEx": "Littman et al\\.", "year": 1996}, {"title": "Algorithms for Sequential Decision Making", "author": ["Littman", "Michael Lederman"], "venue": "PhD thesis,", "citeRegEx": "Littman and Lederman.,? \\Q1996\\E", "shortCiteRegEx": "Littman and Lederman.", "year": 1996}, {"title": "Apprenticeship learning using inverse reinforcement learning and gradient methods", "author": ["Neu", "Gergely", "Szepesv\u00e1ri", "Csaba"], "venue": "In UAI,", "citeRegEx": "Neu et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Neu et al\\.", "year": 2007}, {"title": "Algorithms for inverse reinforcement learning", "author": ["Ng", "Andrew Y", "Russell", "Stuart"], "venue": "In International Conference on Machine Learning,", "citeRegEx": "Ng et al\\.,? \\Q2000\\E", "shortCiteRegEx": "Ng et al\\.", "year": 2000}, {"title": "A convergent form of approximate policy iteration", "author": ["Perkins", "Theodore J", "Precup", "Doina"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Perkins et al\\.,? \\Q2002\\E", "shortCiteRegEx": "Perkins et al\\.", "year": 2002}, {"title": "Relative entropy policy search", "author": ["Peters", "Jan", "M\u00fclling", "Katharina", "Altun", "Yasemin"], "venue": "In AAAI. Atlanta,", "citeRegEx": "Peters et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Peters et al\\.", "year": 2010}, {"title": "Markov Decision Processes\u2014Discrete Stochastic Dynamic Programming", "author": ["Ramachandran", "Deepak", "Amir", "Eyal"], "venue": "In IJCAI,", "citeRegEx": "Ramachandran et al\\.,? \\Q1994\\E", "shortCiteRegEx": "Ramachandran et al\\.", "year": 1994}, {"title": "On-line Q-learning using connectionist systems", "author": ["G.A. Rummery", "M. Niranjan"], "venue": "Technical Report CUED/F-INFENG/TR 166,", "citeRegEx": "Rummery and Niranjan,? \\Q1994\\E", "shortCiteRegEx": "Rummery and Niranjan", "year": 1994}, {"title": "Convergence results for single-step on-policy reinforcement-learning algorithms", "author": ["Singh", "Satinder", "Jaakkola", "Tommi", "Littman", "Michael L", "Szepesv\u00e1ri", "Csaba"], "venue": "Machine Learning,", "citeRegEx": "Singh et al\\.,? \\Q2000\\E", "shortCiteRegEx": "Singh et al\\.", "year": 2000}, {"title": "Experimental evidence on players\u2019 models of other players", "author": ["Stahl", "Dale O", "Wilson", "Paul W"], "venue": "Journal of Economic Behavior and Organization,", "citeRegEx": "Stahl et al\\.,? \\Q1994\\E", "shortCiteRegEx": "Stahl et al\\.", "year": 1994}, {"title": "Integrated architectures for learning, planning, and reacting based on approximating dynamic programming", "author": ["Sutton", "Richard S"], "venue": "In Proceedings of the Seventh International Conference on Machine Learning,", "citeRegEx": "Sutton and S.,? \\Q1990\\E", "shortCiteRegEx": "Sutton and S.", "year": 1990}, {"title": "Reinforcement Learning: An Introduction", "author": ["Sutton", "Richard S", "Barto", "Andrew G"], "venue": null, "citeRegEx": "Sutton et al\\.,? \\Q1998\\E", "shortCiteRegEx": "Sutton et al\\.", "year": 1998}, {"title": "The role of exploration in learning control", "author": ["Thrun", "Sebastian B"], "venue": null, "citeRegEx": "Thrun and B.,? \\Q1992\\E", "shortCiteRegEx": "Thrun and B.", "year": 1992}, {"title": "A theoretical and empirical analysis of expected sarsa", "author": ["Van Seijen", "Harm", "Van Hasselt", "Hado", "Whiteson", "Shimon", "Wiering", "Marco"], "venue": "IEEE Symposium on Adaptive Dynamic Programming and Reinforcement Learning,", "citeRegEx": "Seijen et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Seijen et al\\.", "year": 2009}, {"title": "Simple statistical gradient-following algorithms for connectionist reinforcement learning", "author": ["Williams", "Ronald J"], "venue": "Machine Learning,", "citeRegEx": "Williams and J.,? \\Q1992\\E", "shortCiteRegEx": "Williams and J.", "year": 1992}, {"title": "Beyond equilibrium: Predicting human behavior in normal-form games", "author": ["Wright", "James R", "Leyton-Brown", "Kevin"], "venue": "In AAAI,", "citeRegEx": "Wright et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Wright et al\\.", "year": 2010}], "referenceMentions": [{"referenceID": 2, "context": "The issue arises in the context of the exploration\u2013exploitation dilemma (Thrun, 1992), non-stationary decision problems (Sutton, 1990), and when interpreting observed decisions (Baker et al., 2007).", "startOffset": 177, "endOffset": 197}, {"referenceID": 19, "context": "SARSA is known to converge in the tabular setting using -greedy exploration (Littman & Szepesv\u00e1ri, 1996), under decreasing exploration (Singh et al., 2000), and to a region in the function approximation setting (Gordon, 2001).", "startOffset": 135, "endOffset": 155}, {"referenceID": 3, "context": "which can be viewed as a particular instantiation of the quasi-arithmetic mean (Beliakov et al., 2016).", "startOffset": 79, "endOffset": 102}, {"referenceID": 16, "context": "In this section, we address the problem of identifying such a probability distribution as a maximum entropy problem\u2014over all distributions that satisfy the properties above, pick the one that maximizes information entropy (Cover & Thomas, 2006; Peters et al., 2010).", "startOffset": 222, "endOffset": 265}, {"referenceID": 6, "context": "Multi-passenger Taxi Domain We evaluated SARSA with various policies on the multi-passenger taxi domain introduced by (Dearden et al., 1998).", "startOffset": 118, "endOffset": 140}, {"referenceID": 6, "context": "(Dearden et al., 1998) As shown in Figure 10, SARSA with the epsilon-greedy policy performs poorly.", "startOffset": 0, "endOffset": 22}, {"referenceID": 19, "context": "On-policy algorithms like SARSA can be made to converge to optimal behavior in the limit when the exploration rate and the update operator is gradually moved toward max (Singh et al., 2000).", "startOffset": 169, "endOffset": 189}, {"referenceID": 0, "context": "Such methods include Bayesian IRL (Ramachandran & Amir, 2007), natural gradient IRL (Neu & Szepesv\u00e1ri, 2007), and maximum likelihood IRL (Babes et al., 2011).", "startOffset": 137, "endOffset": 157}], "year": 2017, "abstractText": "A softmax operator applied to a set of values acts somewhat like the maximization function and somewhat like an average. In sequential decision making, softmax is often used in settings where it is necessary to maximize utility but also to hedge against problems that arise from putting all of one\u2019s weight behind a single maximum utility decision. The Boltzmann softmax operator is the most commonly used softmax operator in this setting, but we show that this operator is prone to misbehavior. In this work, we study an alternative softmax operator that, among other properties, is both a non-expansion (ensuring convergent behavior in learning and planning) and differentiable (making it possible to improve decisions via gradient descent methods). We provide proofs of these properties and present empirical comparisons between various softmax operators.", "creator": "LaTeX with hyperref package"}}}