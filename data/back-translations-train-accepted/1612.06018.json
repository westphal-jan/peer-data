{"id": "1612.06018", "review": {"conference": "AAAI", "VERSION": "v1", "DATE_OF_SUBMISSION": "19-Dec-2016", "title": "Self-Correcting Models for Model-Based Reinforcement Learning", "abstract": "When an agent cannot represent a perfectly accurate model of its environment's dynamics, model-based reinforcement learning (MBRL) can fail catastrophically. Planning involves composing the predictions of the model; when flawed predictions are composed, even minor errors can compound and render the model useless for planning. Hallucinated Replay (Talvitie 2014) trains the model to \"correct\" itself when it produces errors, substantially improving MBRL with flawed models. This paper theoretically analyzes this approach, illuminates settings in which it is likely to be effective or ineffective, and presents a novel error bound, showing that a model's ability to self-correct is more tightly related to MBRL performance than one-step prediction error. These results inspire an MBRL algorithm for deterministic MDPs with performance guarantees that are robust to model class limitations.", "histories": [["v1", "Mon, 19 Dec 2016 01:09:23 GMT  (235kb,D)", "https://arxiv.org/abs/1612.06018v1", "To appear in Proceedings of the 31st AAAI Conference on Artificial Intelligence, 2017. This version incorporates the appendix into document (rather than as supplementary material)"], ["v2", "Wed, 26 Jul 2017 18:53:51 GMT  (236kb,D)", "http://arxiv.org/abs/1612.06018v2", "Original paper appeared in Proceedings of the 31st AAAI Conference on Artificial Intelligence, 2017. This version incorporates the appendix into document (rather than as supplementary material), corrects a minor error in Lemma 1, and fixes some type-os"]], "COMMENTS": "To appear in Proceedings of the 31st AAAI Conference on Artificial Intelligence, 2017. This version incorporates the appendix into document (rather than as supplementary material)", "reviews": [], "SUBJECTS": "cs.LG cs.AI", "authors": ["erik talvitie"], "accepted": true, "id": "1612.06018"}, "pdf": {"name": "1612.06018.pdf", "metadata": {"source": "CRF", "title": "Self-Correcting Models for Model-Based Reinforcement Learning", "authors": ["Erik Talvitie"], "emails": ["erik.talvitie@fandm.edu"], "sections": [{"heading": "1 Introduction", "text": "This year, it is only a matter of time before an agreement is reached."}, {"heading": "1.1 Notation and background", "text": "We focus on the Markov decision-making processes (MDP). The initial state s1 results from a distribution. At each step t, the environment is in a state st. The agent selects an action in which the environment passes into a new state, which is sampled from the transition distribution: st + 1 \u0445 P atst. The environment also gives a reward, R (st, at). For simplicity, we assume that the reward function is known and is limited within [0, M]. A policy approach indicates how to behave in the MDP. Let's let \u03c0 (a | s) = \u03c0s (a) be the probability with which the action is chosen in a state. For an action sequence, let's p (s, a1: t) = P a1: ts (s) = P a1: ts (s) be the probability to achieve s (s) by starting in s and taking actions in sequence."}, {"heading": "2 Bounding value error", "text": "We look at an MBRL architecture that uses the simple Monte Carlo single-stage planning algorithm (single-stage Q-value), rooted in the \"rollout algorithm\" (Tesauro and Galperin 1996).For each pair of states, the planner performs N-T-step \"rollouts\" in P-value, starting with s, taking measures a, and then following a rollout policy. Let Q-value (s, a) be the average discounted return on rollouts. For large-scale N-value, Q-value T (Cacade 2003) approaches. The broker greedily chooses its actions in terms of Q-value. Talvitie (2015) limits the performance of a single-stage MC in terms of model quality. For political performance and government action distribution, we let T-value be determined depending on the government's trade value."}, {"heading": "B is the Bellman operator).", "text": "The mc term stands for errors due to limitations of the planning algorithm: errors due to the average Q-value of the sample and the sub-optimality of the T-step value function with respect to \u03c1. The B-value represents errors due to the model parameters. The key factor for the usefulness of the model for planning is the accuracy of the value it assigns to the rollout policy in government measures attended by \u03c0 and \u03c0. Our goal in the next sections is to tie B-value, B-value, B-value and Tval in terms of measures of model accuracy and ultimately gain insights into how to train models that are effective for MBRL. Evidence can be found in the appendix."}, {"heading": "2.1 One-step prediction error", "text": "It's the first step in the right direction, \"he says.\" It's the first step in the right direction. \"\" It's the second step in the right direction. \"\" It's the first step in the right direction. \"\" It's the second step in the right direction. \"\" It's the second step in the right direction. \"\" It's the second step in the right direction. \"\" It's the third step in the right direction. \"\" It's the third step in the right direction. \"\" It's the third step in the right direction. \"\" It's the first step in the right direction, \"he says.\""}, {"heading": "2.2 Multi-step prediction error", "text": "Since Q\u03c0T (s, a) = \u2211 T = 1 \u03b3 t \u2212 1 E (s, \"a\") \u0445 Dts, a, \u03c0 R (s, \"a\"), it is straightforward in terms of multi-level errors. Lemma 3. The limit in Lemma 2 is dependent on 11 \u2212 \u03b3, because it effectively assumes the worst possible loss of value when the model samples an \"incorrect\" state. In contrast, Lemma 3 takes into account the model's ability to recover from an error, punishing it only for individual incorrect transitions. Unfortunately, it is difficult to directly optimize the accuracy of multi-level predictions. Nevertheless, this limit suggests that algorithms that take into account the multi-level error of a model will yield more robust MBRL results."}, {"heading": "2.3 Hallucinated one-step prediction error", "text": "This year, it has reached the point where it is able to retaliate."}, {"heading": "2.4 A Tighter Bound", "text": "In the rest of the paper, we assume that the environment is deterministic. Let's consider this model as a unique condition resulting from starting in the states and the sequence of actions a1: 1. The agent's model is still stochastic. Remember that our goal is to tie the value error under the unilateral MC rollout policy. Proposal 6 shows that hallucinated error has a loose boundary between arbitrary strategies. We are now focusing on blind strategies (Bowling et al. 2006). Blind policies depend only on the story, i.e. \u03c0 (at | st, a1: t \u2212 1) = finite strategies (at | a1: t \u2212 1) This class of strategies extends from stateless strategies to open sequences of actions. It includes the uniform random policy, a common rollout policy. For each blind policy and state's distribution of actions, Hta and model state applies, Hta and model state will apply."}, {"heading": "3 Hallucinated DAgger-MC", "text": "The Data Aggregator (DAgger) algorithm (Ross and Bagnell 2012) was the first practicable MBRL algorithm with performance guarantees for the model class, but it requires the planner to be near optimal. DAgger-MC (Talvitie 2015) has loosened this assumption by taking into account the limitations of the planner using the model (One-ply MC).This section increases the planner's ability (One-ply MC) to apply hallucinated training, leading to the hallucinated DAgger-MC algorithm1 or H-DAgger-MC (Algorithm 1).In addition to adopting a specific form of the planner (One-ply MC with a blind roll-out policy), H-DAgger-MC assumes that the model will be \"unwanted\" (similar to, for example, Abbeel et al. 2006)."}, {"heading": "4 Empirical Illustration", "text": "This year, it has reached the point where it will be able to retaliate."}, {"heading": "4.1 Impact of the unrolled model", "text": "Remember that the H-DAgger-MC algorithm assumes that the model will be \"rolled out,\" with a separate model responsible for testing each step in a rollout. This has clear practical disadvantages, but is important in theory. If a model is used over all time steps, convergence to a perfect model cannot be guaranteed, even if it is present in Figure 3c. H-DAgger-MC was trained with a single model in fixed porthole shooter, but the temporary cutting plan described above has been permanently limited to different depths."}, {"heading": "5 Conclusions and future work", "text": "The primary contribution of this paper is a deeper theoretical understanding of how to perform effective MBRL in the face of the limitations of the model class. Specifically, we have studied an MBRL algorithm that, despite errors in the model and the planner, achieves good control performance and provides strong theoretical performance guarantees. We have also seen negative results suggesting that hallucinated one-step errors in the most general environment may not be an effective optimization criterion. This is the open challenge of loosening assumptions regarding deterministic dynamics and blind policies or developing alternative approaches to improve multi-level errors in more general environments. We have further observed that hallucinated training can cause stability problems as model parameters affect both prediction errors and the training distribution itself. It would be valuable to develop techniques that take both effects into account when a poor reward for DAM selection is still possible when adjusting model parameters."}, {"heading": "Acknowledgements", "text": "Many thanks to Marc Bellemare, whose feedback positively influenced the work both in terms of content and presentation. Thanks to Drew Bagnell and Arun Venkatraman for their valuable insights. Thanks also to Joel Veness for his freely available implementations of FAC-CTW and CTS (http: / / jveness.info / software /)."}, {"heading": "Appendix: Proofs", "text": "Lemma 2: For each policy measure and each national action distribution applies: for each measure applies: for each measure applies: for each measure applies: for each measure applies: for each measure applies: for each measure applies: for each measure applies: for each measure applies: for each measure applies: for each measure applies: for each measure applies: for each measure applies: for each measure applies: for each measure applies: for each measure applies: for each measure applies: for each measure applies: for each measure: for each measure: for each measure applies: for each measure: for each measure: for each measure: for each measure: for each measure applies: for each measure: for each measure: for each measure: for each measure: for each measure: for each measure: for each measure: for each measure: for each measure: for each measure: for each measure: for each measure: for each measure: for each measure: for each measure: for each measure: for each measure: for each measure: for each measure: for each measure: for each measure: for each measure: for each measure: for each measure: for each measure: for each measure: for each measure: for each measure: for each measure: for each measure: for each measure: for each measure: for each measure: for each measure: for each measure: for each measure: for each measure: for each measure: for each measure: for each measure: for each measure: for each measure: for each measure: for each measure: for each measure: for each measure: for each measure: for each measure: for each measure: for each measure: for each measure: for each measure: for each measure: for each measure: for each measure: for each measure: for each measure: for every measure: for every measure: for every measure: for every measure: for every measure: for every measure: for every measure: for every measure: for every measure: for every measure: for every measure: for every measure: for every measure: for every measure: for every measure: for every measure: for every measure: for every measure: for every measure: for every measure: for every measure: for every measure: for every measure: for every measure: for every measure: for every measure: for every measure: for every measure: for every measure: for every measure: for every measure: for every measure: for every measure: for every measure: for every measure: for every measure: for every measure:"}], "references": [{"title": "Using inaccurate models in reinforcement learning", "author": ["P. Abbeel", "M. Quigley", "A.Y. Ng"], "venue": "In Proceedings of the 23rd International Conference on Machine Learning,", "citeRegEx": "Abbeel et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Abbeel et al\\.", "year": 2006}, {"title": "Skip context tree switching", "author": ["M.G. Bellemare", "J. Veness", "E. Talvitie"], "venue": "In Proceedings of the 31st International Conference on Machine Learning,", "citeRegEx": "Bellemare et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Bellemare et al\\.", "year": 2014}, {"title": "A neuroevolution approach to general atari game playing", "author": ["M. Hausknecht", "J. Lehman", "R. Miikkulainen", "P. Stone"], "venue": "IEEE Transactions on Computational Intelligence and AI in Games,", "citeRegEx": "Hausknecht et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Hausknecht et al\\.", "year": 2014}, {"title": "Reinforcement learning with misspecified model classes", "author": ["J. Joseph", "A. Geramifard", "J.W. Roberts", "J.P. How", "N. Roy"], "venue": "IEEE International Conference on Robotics and Automation,", "citeRegEx": "Joseph et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Joseph et al\\.", "year": 2013}, {"title": "On the sample complexity of reinforcement learning", "author": ["S.M. Kakade"], "venue": "PhD thesis, University of London,", "citeRegEx": "Kakade.,? \\Q2003\\E", "shortCiteRegEx": "Kakade.", "year": 2003}, {"title": "Actionconditional video prediction using deep networks in atari games", "author": ["J. Oh", "X. Guo", "H. Lee", "R.L. Lewis", "S. Singh"], "venue": "In Advances in Neural Information Processing Systems", "citeRegEx": "Oh et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Oh et al\\.", "year": 2015}, {"title": "Agnostic system identification for modelbased reinforcement learning", "author": ["S. Ross", "D. Bagnell"], "venue": "In Proceedings of the 29th International Conference on Machine Learning,", "citeRegEx": "Ross and Bagnell.,? \\Q2012\\E", "shortCiteRegEx": "Ross and Bagnell.", "year": 2012}, {"title": "Reward design via online gradient ascent", "author": ["J. Sorg", "R.L. Lewis", "S. Singh"], "venue": "In Advances in Neural Information Processing Systems", "citeRegEx": "Sorg et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Sorg et al\\.", "year": 2010}, {"title": "Model-based reinforcement learning with nearly tight exploration complexity bounds", "author": ["I. Szita", "C. Szepesv\u00e1ri"], "venue": "In Proceedings of the 27th International Conference on Machine Learning,", "citeRegEx": "Szita and Szepesv\u00e1ri.,? \\Q2010\\E", "shortCiteRegEx": "Szita and Szepesv\u00e1ri.", "year": 2010}, {"title": "Model regularization for stable sample rollouts", "author": ["E. Talvitie"], "venue": "In Proceedings of the 30th Conference on Uncertainty in Artificial Intelligence,", "citeRegEx": "Talvitie.,? \\Q2014\\E", "shortCiteRegEx": "Talvitie.", "year": 2014}, {"title": "Agnostic system identification for monte carlo planning", "author": ["E. Talvitie"], "venue": "In Proceedings of the 29th AAAI Conference on Artificial Intelligence,", "citeRegEx": "Talvitie.,? \\Q2015\\E", "shortCiteRegEx": "Talvitie.", "year": 2015}, {"title": "On-line policy improvement using monte-carlo search", "author": ["G. Tesauro", "G.R. Galperin"], "venue": "In Advances in Neural Information Processing Systems", "citeRegEx": "Tesauro and Galperin.,? \\Q1996\\E", "shortCiteRegEx": "Tesauro and Galperin.", "year": 1996}, {"title": "A Monte-Carlo AIXI Approximation", "author": ["J. Veness", "K.S. Ng", "M. Hutter", "W.T.B. Uther", "D. Silver"], "venue": "Journal of Artificial Intelligence Research,", "citeRegEx": "Veness et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Veness et al\\.", "year": 2011}, {"title": "Context tree switching", "author": ["J. Veness", "K.S. Ng", "M. Hutter", "M. Bowling"], "venue": "In Proceedings of the 2012 Data Compression Conference,", "citeRegEx": "Veness et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Veness et al\\.", "year": 2012}, {"title": "Improving multistep prediction of learned time series models", "author": ["A. Venkatraman", "M. Hebert", "J.A. Bagnell"], "venue": "In Proceedings of the 29th AAAI Conference on Artificial Intelligence,", "citeRegEx": "Venkatraman et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Venkatraman et al\\.", "year": 2015}], "referenceMentions": [{"referenceID": 5, "context": "For instance, Sorg et al. (2010) and Joseph et al.", "startOffset": 14, "endOffset": 33}, {"referenceID": 3, "context": "(2010) and Joseph et al. (2013) both pointed out that the most accurate model by this measure is not necessarily the best for planning.", "startOffset": 11, "endOffset": 32}, {"referenceID": 3, "context": "(2010) and Joseph et al. (2013) both pointed out that the most accurate model by this measure is not necessarily the best for planning. Both proposed optimizing model parameters for control performance using policy gradient methods. Though appealing in its directness, this approach arguably discards some of the benefits of learning a model in the first place. Talvitie (2014) pointed out that one-step prediction accuracy does not account for how the model behaves when composed with itself and introduced the Hallucinated Replay meta-algorithm to address this.", "startOffset": 11, "endOffset": 378}, {"referenceID": 3, "context": "(2010) and Joseph et al. (2013) both pointed out that the most accurate model by this measure is not necessarily the best for planning. Both proposed optimizing model parameters for control performance using policy gradient methods. Though appealing in its directness, this approach arguably discards some of the benefits of learning a model in the first place. Talvitie (2014) pointed out that one-step prediction accuracy does not account for how the model behaves when composed with itself and introduced the Hallucinated Replay meta-algorithm to address this. As illustrated in Figure 1, this approach rolls out the model and environment in parallel, training the model to predict the correct environment state (s4) even when its input is an incorrect sampled state (z3). This effectively causes the model to \u201cself-correct\u201d its rollouts. Hallucinated Replay was shown to enable meaningful planning with flawed models in examples where the standard approach failed. However, it offers no theoretical guarantees. Venkatraman et al. (2015) and Oh et al.", "startOffset": 11, "endOffset": 1041}, {"referenceID": 3, "context": "(2010) and Joseph et al. (2013) both pointed out that the most accurate model by this measure is not necessarily the best for planning. Both proposed optimizing model parameters for control performance using policy gradient methods. Though appealing in its directness, this approach arguably discards some of the benefits of learning a model in the first place. Talvitie (2014) pointed out that one-step prediction accuracy does not account for how the model behaves when composed with itself and introduced the Hallucinated Replay meta-algorithm to address this. As illustrated in Figure 1, this approach rolls out the model and environment in parallel, training the model to predict the correct environment state (s4) even when its input is an incorrect sampled state (z3). This effectively causes the model to \u201cself-correct\u201d its rollouts. Hallucinated Replay was shown to enable meaningful planning with flawed models in examples where the standard approach failed. However, it offers no theoretical guarantees. Venkatraman et al. (2015) and Oh et al. (2015) used similar approaches to improve models\u2019 long-range predictions, though not in the MBRL setting.", "startOffset": 11, "endOffset": 1062}, {"referenceID": 4, "context": "For largeN , Q\u0304will closely approximate Q\u0302\u03c1T (Kakade 2003). The agent will select its actions greedily with respect to Q\u0304. Talvitie (2015) bounds the performance of one-ply MC in terms of model quality.", "startOffset": 46, "endOffset": 139}, {"referenceID": 4, "context": "For largeN , Q\u0304will closely approximate Q\u0302\u03c1T (Kakade 2003). The agent will select its actions greedily with respect to Q\u0304. Talvitie (2015) bounds the performance of one-ply MC in terms of model quality. For a policy \u03c0 and state-action distribution \u03be, let \u03be,\u03c0,T val = E(s,a)\u223c\u03be [ |QT (s, a) \u2212 Q\u0302T (s, a)| ] be the error in the T -step state-action values the model assigns to the policy under the given distribution. Then the following result can be straightforwardly adapted from one provided by Talvitie (2015). Lemma 1.", "startOffset": 46, "endOffset": 511}, {"referenceID": 4, "context": "For largeN , Q\u0304will closely approximate Q\u0302\u03c1T (Kakade 2003). The agent will select its actions greedily with respect to Q\u0304. Talvitie (2015) bounds the performance of one-ply MC in terms of model quality. For a policy \u03c0 and state-action distribution \u03be, let \u03be,\u03c0,T val = E(s,a)\u223c\u03be [ |QT (s, a) \u2212 Q\u0302T (s, a)| ] be the error in the T -step state-action values the model assigns to the policy under the given distribution. Then the following result can be straightforwardly adapted from one provided by Talvitie (2015). Lemma 1. Let Q\u0304 be the state-action value function returned by applying one-ply Monte Carlo to the model P\u0302 with rollout policy \u03c1 and rollout depth T . Let \u03c0\u0302 be greedy w.r.t. Q\u0304. For any policy \u03c0 and state-distribution \u03bc, E s\u223c\u03bc [ V (s)\u2212 V (s) ] \u2264 4 1\u2212 \u03b3 \u03be,\u03c1,T val + mc, where we let \u03be(s, a) = 1 2D\u03bc,\u03c0\u0302(s, a) + 1 4D\u03bc,\u03c0(s, a) + 1 4 ( (1\u2212 \u03b3)\u03bc(s)\u03c0\u0302s(a) + \u03b3 \u2211 z,bD\u03bc,\u03c0(z, b)P b z (s)\u03c0\u0302s(a) ) and mc = 4 1\u2212\u03b3 \u2016Q\u0304 \u2212 Q\u0302 \u03c1 T \u2016\u221e + 2 1\u2212\u03b3 \u2016BV \u03c1 T \u2212 V \u03c1 T \u2016\u221e (here B is the Bellman operator). The mc term represents error due to limitations of the planning algorithm: error due to the sample average Q\u0304 and the sub-optimality of the T -step value function with respect to \u03c1. The \u03be,\u03c1,T val term represents error due to the model parameters. The key factor in the model\u2019s usefulness for planning is the accuracy of the value it assigns to the rollout policy in state-actions visited by \u03c0 and \u03c0\u0302. Our goal in the next sections is to bound \u03be,\u03c1,T val in terms of measures of model accuracy, ultimately deriving insight into how to train models that will be effective for MBRL. Proofs may be found in the appendix. 2.1 One-step prediction error Intuitively, the value of a policy should be accurate if the model is accurate in states that the policy would visit. We can adapt a bound from Ross and Bagnell (2012). Lemma 2.", "startOffset": 46, "endOffset": 1805}, {"referenceID": 9, "context": "Consider the \u201cShooter\u201d domain introduced by Talvitie (2015), pictured in Figure 2a.", "startOffset": 44, "endOffset": 60}, {"referenceID": 9, "context": "\u201d Talvitie (2014) presents several similar examples involving various model deficiencies.", "startOffset": 2, "endOffset": 18}, {"referenceID": 14, "context": "Venkatraman et al. (2015) provide some analysis but in the uncontrolled time series prediction setting.", "startOffset": 0, "endOffset": 26}, {"referenceID": 2, "context": "(2015) learned models of Atari 2600 games, which are fully deterministic (Hausknecht et al. 2014); human players often perceive them as stochastic due to their complexity.", "startOffset": 73, "endOffset": 97}, {"referenceID": 4, "context": "For instance, Oh et al. (2015) learned models of Atari 2600 games, which are fully deterministic (Hausknecht et al.", "startOffset": 14, "endOffset": 31}, {"referenceID": 2, "context": "(2015) learned models of Atari 2600 games, which are fully deterministic (Hausknecht et al. 2014); human players often perceive them as stochastic due to their complexity. Similarly, in synthetic RL domains stochasticity is often added to simulate complex, deterministic phenomena (e.g. robot wheels slipping on debris), not necessarily to capture inherently stochastic effects in the world. As in these examples, we shall assume that the environment is deterministic but complex, so a limited agent will learn an imperfect, stochastic model. That said, even specialized to deterministic environments, the bound in Lemma 4 is loose for arbitrary policies. Proposition 6. The hallucinated one-step error of a perfect model may be non-zero, even in a deterministic MDP. Proof. Alter the coin MDP, giving the agent two actions which fully determine the coin\u2019s orientation. The original dynamics can be recovered via a stochastic policy that randomly selects sh or st and then leaves the coin alone. Oh et al. (2015) tied action selection to the environment state only (rather than independently selecting actions in the environment and model).", "startOffset": 74, "endOffset": 1013}, {"referenceID": 6, "context": "We now analyze H-DAgger-MC, adapting Ross and Bagnell (2012)\u2019s DAgger analysis.", "startOffset": 37, "endOffset": 61}, {"referenceID": 6, "context": "Ross and Bagnell (2012) discuss extensions that address more practical loss functions, finite sample bounds, and results for \u03c0\u0302N .", "startOffset": 0, "endOffset": 24}, {"referenceID": 6, "context": "Ross and Bagnell (2012) discuss extensions that address more practical loss functions, finite sample bounds, and results for \u03c0\u0302N . The next question is, of course, when will the learned models be accurate? Following Ross and Bagnell (2012) note that \u0304prd can be interpreted as the average loss of an online learner on the problem defined by the aggregated datasets at each iteration.", "startOffset": 0, "endOffset": 240}, {"referenceID": 13, "context": "The model for each pixel was learned using Context Tree Switching (Veness et al. 2012), similar to the FAC-CTW algorithm (Veness et al.", "startOffset": 66, "endOffset": 86}, {"referenceID": 12, "context": "2012), similar to the FAC-CTW algorithm (Veness et al. 2011), and used a 7\u00d7 7 neighborhood around the pixel in the previous timestep as input.", "startOffset": 40, "endOffset": 60}, {"referenceID": 8, "context": "The experimental setup matches that of Talvitie (2015) for comparison\u2019s sake, though the qualitative comparison presented here is robust to the parameter settings.", "startOffset": 39, "endOffset": 55}, {"referenceID": 8, "context": "The experimental setup matches that of Talvitie (2015) for comparison\u2019s sake, though the qualitative comparison presented here is robust to the parameter settings. In all cases one-ply MC was used with 50 uniformly random rollouts of depth 15 at every step. The exploration distribution was generated by following the optimal policy with (1\u2212\u03b3) probability of termination at each step. The model for each pixel was learned using Context Tree Switching (Veness et al. 2012), similar to the FAC-CTW algorithm (Veness et al. 2011), and used a 7\u00d7 7 neighborhood around the pixel in the previous timestep as input. Data was shared across all positions. The discount factor was \u03b3 = 0.9. In each iteration 500 training rollouts were generated and the resulting policy was evaluated in an episode of length 30. The discounted return obtained by the policy in each iteration is reported, averaged over 50 trials. The results can be seen in Figure 3a and 3b. The shaded regions represent 95% confidence intervals for the mean performance. The benchmark lines labeled \u201cRandom\u201d and \u201cPerfect Model\u201d represent the average performance of the uniform random policy and one-ply Monte Carlo using a perfect model, respectively. In Figure 3a the bullseyes move, simulating the typical practical reality that C does not contain a perfect model. In Figure 3b the bullseyes have fixed positions, so C does contain a perfect model. As observed by Talvitie (2015), DAgger performs poorly in both versions, due to the suboptimal planner.", "startOffset": 39, "endOffset": 1439}, {"referenceID": 8, "context": "The experimental setup matches that of Talvitie (2015) for comparison\u2019s sake, though the qualitative comparison presented here is robust to the parameter settings. In all cases one-ply MC was used with 50 uniformly random rollouts of depth 15 at every step. The exploration distribution was generated by following the optimal policy with (1\u2212\u03b3) probability of termination at each step. The model for each pixel was learned using Context Tree Switching (Veness et al. 2012), similar to the FAC-CTW algorithm (Veness et al. 2011), and used a 7\u00d7 7 neighborhood around the pixel in the previous timestep as input. Data was shared across all positions. The discount factor was \u03b3 = 0.9. In each iteration 500 training rollouts were generated and the resulting policy was evaluated in an episode of length 30. The discounted return obtained by the policy in each iteration is reported, averaged over 50 trials. The results can be seen in Figure 3a and 3b. The shaded regions represent 95% confidence intervals for the mean performance. The benchmark lines labeled \u201cRandom\u201d and \u201cPerfect Model\u201d represent the average performance of the uniform random policy and one-ply Monte Carlo using a perfect model, respectively. In Figure 3a the bullseyes move, simulating the typical practical reality that C does not contain a perfect model. In Figure 3b the bullseyes have fixed positions, so C does contain a perfect model. As observed by Talvitie (2015), DAgger performs poorly in both versions, due to the suboptimal planner. DAgger-MC is able to perform well with fixed bullseyes (Figure 3b), but with moving bullseyes the model suffers from compounding errors and is not useful for planning (Figure 3a). This holds for a single model and for an \u201cunrolled\u201d model. In these experiments one practically-minded alteration was made to the H-DAgger-MC algorithm. In early training the model is highly inaccurate, and thus deep rollouts produce incoherent samples. Training with these samples is counter-productive (also, the large number of distinct, nonsensical contexts renders CTS impractical). For these experiments, training rollouts in iteration i were truncated at depth bi/10c. Planning rollouts in these early iterations use the models that have been trained so far and then repeatedly apply the deepest model in order to complete the rollout. Talvitie (2014), Venkatraman et al.", "startOffset": 39, "endOffset": 2351}, {"referenceID": 8, "context": "The experimental setup matches that of Talvitie (2015) for comparison\u2019s sake, though the qualitative comparison presented here is robust to the parameter settings. In all cases one-ply MC was used with 50 uniformly random rollouts of depth 15 at every step. The exploration distribution was generated by following the optimal policy with (1\u2212\u03b3) probability of termination at each step. The model for each pixel was learned using Context Tree Switching (Veness et al. 2012), similar to the FAC-CTW algorithm (Veness et al. 2011), and used a 7\u00d7 7 neighborhood around the pixel in the previous timestep as input. Data was shared across all positions. The discount factor was \u03b3 = 0.9. In each iteration 500 training rollouts were generated and the resulting policy was evaluated in an episode of length 30. The discounted return obtained by the policy in each iteration is reported, averaged over 50 trials. The results can be seen in Figure 3a and 3b. The shaded regions represent 95% confidence intervals for the mean performance. The benchmark lines labeled \u201cRandom\u201d and \u201cPerfect Model\u201d represent the average performance of the uniform random policy and one-ply Monte Carlo using a perfect model, respectively. In Figure 3a the bullseyes move, simulating the typical practical reality that C does not contain a perfect model. In Figure 3b the bullseyes have fixed positions, so C does contain a perfect model. As observed by Talvitie (2015), DAgger performs poorly in both versions, due to the suboptimal planner. DAgger-MC is able to perform well with fixed bullseyes (Figure 3b), but with moving bullseyes the model suffers from compounding errors and is not useful for planning (Figure 3a). This holds for a single model and for an \u201cunrolled\u201d model. In these experiments one practically-minded alteration was made to the H-DAgger-MC algorithm. In early training the model is highly inaccurate, and thus deep rollouts produce incoherent samples. Training with these samples is counter-productive (also, the large number of distinct, nonsensical contexts renders CTS impractical). For these experiments, training rollouts in iteration i were truncated at depth bi/10c. Planning rollouts in these early iterations use the models that have been trained so far and then repeatedly apply the deepest model in order to complete the rollout. Talvitie (2014), Venkatraman et al. (2015), and Oh et al.", "startOffset": 39, "endOffset": 2378}, {"referenceID": 5, "context": "(2015), and Oh et al. (2015) all similarly discarded noisy examples early in training.", "startOffset": 12, "endOffset": 29}, {"referenceID": 5, "context": "(2015), and Oh et al. (2015) all similarly discarded noisy examples early in training. This transient modification does not impact H-DAggerMC\u2019s asymptotic guarantees. In Figure 3a it is clear that H-DAgger-MC obtains a good policy despite the limitations of the model class. Hallucinated training has made MBRL possible with both a flawed model and a flawed planner while the standard approach has failed entirely. In the case that C contains a perfect model (rare in problems of genuine interest) H-DAgger-MC is outperformed by DAgger-MC. Despite the adjustment to training, deep models still receive noisy inputs. Theoretically the model should become perfectly accurate in the limit, though in practice it may do so very slowly. Source code for these experiments may be found at github. com/etalvitie/hdaggermc. 4.1 Impact of the unrolled model Recall that the H-DAgger-MC algorithm assumes the model will be \u201cunrolled,\u201d with a separate model responsible for sampling each step in a rollout. This has clear practical disadvantages, but it is important theoretically. When one model is used across all time-steps, convergence to a perfect model cannot be guaranteed, even if one exists in C. In Figure 3c, H-DAgger-MC has been trained using a single model in Shooter with fixed bullseyes. The temporary truncation schedule described above is employed, but the training rollouts have been permanently limited to various depths. First consider the learning curve marked \u201cDepth 15\u201d, where training rollouts are permitted to reach the maximum depth. While the rollouts are temporarily truncated the model does well, but performance degrades as longer rollouts are permitted even though C contains a perfect model! Recall from Section 3 that changing the model parameters impacts both prediction error and the future training distribution. Furthermore, training examples generated by deep rollouts may contain highly flawed samples as inputs. Sometimes attempting to \u201ccorrect\u201d a large error (i.e. reduce prediction error) causes additional, even worse errors in the next iteration (i.e. harms the training distribution). For instance consider a hallucinated training example with the 4th screen from Figure 2b as input and the 5th screen from Figure 2a as the target. The model would effectively learn that targets can appear out of nowhere, an error that would be even harder to correct in future iterations. With a single model across timesteps, a feedback loop can emerge: the model parameters change to attempt to correct large errors, thereby causing larger errors, and so on. This feedback loop causes the observed performance crash. With an unrolled model the parameters of each sub-model cannot impact that sub-model\u2019s own training distribution, ensuring stability. Note that none of Talvitie (2014), Venkatraman et al.", "startOffset": 12, "endOffset": 2805}, {"referenceID": 5, "context": "(2015), and Oh et al. (2015) all similarly discarded noisy examples early in training. This transient modification does not impact H-DAggerMC\u2019s asymptotic guarantees. In Figure 3a it is clear that H-DAgger-MC obtains a good policy despite the limitations of the model class. Hallucinated training has made MBRL possible with both a flawed model and a flawed planner while the standard approach has failed entirely. In the case that C contains a perfect model (rare in problems of genuine interest) H-DAgger-MC is outperformed by DAgger-MC. Despite the adjustment to training, deep models still receive noisy inputs. Theoretically the model should become perfectly accurate in the limit, though in practice it may do so very slowly. Source code for these experiments may be found at github. com/etalvitie/hdaggermc. 4.1 Impact of the unrolled model Recall that the H-DAgger-MC algorithm assumes the model will be \u201cunrolled,\u201d with a separate model responsible for sampling each step in a rollout. This has clear practical disadvantages, but it is important theoretically. When one model is used across all time-steps, convergence to a perfect model cannot be guaranteed, even if one exists in C. In Figure 3c, H-DAgger-MC has been trained using a single model in Shooter with fixed bullseyes. The temporary truncation schedule described above is employed, but the training rollouts have been permanently limited to various depths. First consider the learning curve marked \u201cDepth 15\u201d, where training rollouts are permitted to reach the maximum depth. While the rollouts are temporarily truncated the model does well, but performance degrades as longer rollouts are permitted even though C contains a perfect model! Recall from Section 3 that changing the model parameters impacts both prediction error and the future training distribution. Furthermore, training examples generated by deep rollouts may contain highly flawed samples as inputs. Sometimes attempting to \u201ccorrect\u201d a large error (i.e. reduce prediction error) causes additional, even worse errors in the next iteration (i.e. harms the training distribution). For instance consider a hallucinated training example with the 4th screen from Figure 2b as input and the 5th screen from Figure 2a as the target. The model would effectively learn that targets can appear out of nowhere, an error that would be even harder to correct in future iterations. With a single model across timesteps, a feedback loop can emerge: the model parameters change to attempt to correct large errors, thereby causing larger errors, and so on. This feedback loop causes the observed performance crash. With an unrolled model the parameters of each sub-model cannot impact that sub-model\u2019s own training distribution, ensuring stability. Note that none of Talvitie (2014), Venkatraman et al. (2015), or Oh et al.", "startOffset": 12, "endOffset": 2832}, {"referenceID": 5, "context": "(2015), and Oh et al. (2015) all similarly discarded noisy examples early in training. This transient modification does not impact H-DAggerMC\u2019s asymptotic guarantees. In Figure 3a it is clear that H-DAgger-MC obtains a good policy despite the limitations of the model class. Hallucinated training has made MBRL possible with both a flawed model and a flawed planner while the standard approach has failed entirely. In the case that C contains a perfect model (rare in problems of genuine interest) H-DAgger-MC is outperformed by DAgger-MC. Despite the adjustment to training, deep models still receive noisy inputs. Theoretically the model should become perfectly accurate in the limit, though in practice it may do so very slowly. Source code for these experiments may be found at github. com/etalvitie/hdaggermc. 4.1 Impact of the unrolled model Recall that the H-DAgger-MC algorithm assumes the model will be \u201cunrolled,\u201d with a separate model responsible for sampling each step in a rollout. This has clear practical disadvantages, but it is important theoretically. When one model is used across all time-steps, convergence to a perfect model cannot be guaranteed, even if one exists in C. In Figure 3c, H-DAgger-MC has been trained using a single model in Shooter with fixed bullseyes. The temporary truncation schedule described above is employed, but the training rollouts have been permanently limited to various depths. First consider the learning curve marked \u201cDepth 15\u201d, where training rollouts are permitted to reach the maximum depth. While the rollouts are temporarily truncated the model does well, but performance degrades as longer rollouts are permitted even though C contains a perfect model! Recall from Section 3 that changing the model parameters impacts both prediction error and the future training distribution. Furthermore, training examples generated by deep rollouts may contain highly flawed samples as inputs. Sometimes attempting to \u201ccorrect\u201d a large error (i.e. reduce prediction error) causes additional, even worse errors in the next iteration (i.e. harms the training distribution). For instance consider a hallucinated training example with the 4th screen from Figure 2b as input and the 5th screen from Figure 2a as the target. The model would effectively learn that targets can appear out of nowhere, an error that would be even harder to correct in future iterations. With a single model across timesteps, a feedback loop can emerge: the model parameters change to attempt to correct large errors, thereby causing larger errors, and so on. This feedback loop causes the observed performance crash. With an unrolled model the parameters of each sub-model cannot impact that sub-model\u2019s own training distribution, ensuring stability. Note that none of Talvitie (2014), Venkatraman et al. (2015), or Oh et al. (2015) used an unrolled model.", "startOffset": 12, "endOffset": 2853}], "year": 2017, "abstractText": "When an agent cannot represent a perfectly accurate model of its environment\u2019s dynamics, model-based reinforcement learning (MBRL) can fail catastrophically. Planning involves composing the predictions of the model; when flawed predictions are composed, even minor errors can compound and render the model useless for planning. Hallucinated Replay (Talvitie 2014) trains the model to \u201ccorrect\u201d itself when it produces errors, substantially improving MBRL with flawed models. This paper theoretically analyzes this approach, illuminates settings in which it is likely to be effective or ineffective, and presents a novel error bound, showing that a model\u2019s ability to self-correct is more tightly related to MBRL performance than one-step prediction error. These results inspire an MBRL algorithm for deterministic MDPs with performance guarantees that are robust to model class limitations.", "creator": "LaTeX with hyperref package"}}}