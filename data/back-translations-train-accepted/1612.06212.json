{"id": "1612.06212", "review": {"conference": "iclr", "VERSION": "v1", "DATE_OF_SUBMISSION": "19-Dec-2016", "title": "A recurrent neural network without chaos", "abstract": "We introduce an exceptionally simple gated recurrent neural network (RNN) that achieves performance comparable to well-known gated architectures, such as LSTMs and GRUs, on the word-level language modeling task. We prove that our model has simple, predicable and non-chaotic dynamics. This stands in stark contrast to more standard gated architectures, whose underlying dynamical systems exhibit chaotic behavior.", "histories": [["v1", "Mon, 19 Dec 2016 14:59:14 GMT  (684kb,D)", "http://arxiv.org/abs/1612.06212v1", null]], "reviews": [], "SUBJECTS": "cs.NE cs.CL cs.LG", "authors": ["thomas laurent", "james von brecht"], "accepted": true, "id": "1612.06212"}, "pdf": {"name": "1612.06212.pdf", "metadata": {"source": "CRF", "title": "A RECURRENT NEURAL NETWORK WITHOUT CHAOS", "authors": ["Thomas Laurent"], "emails": ["tlaurent@lmu.edu", "james.vonbrecht@csulb.edu"], "sections": [{"heading": "1 INTRODUCTION", "text": "Gated recurrent neural networks, as the Long Short Term Memory network (LSTM) = function (LSTM) (Hochreiter & Schmidhuber (1997) and the Gated Recurrent Unit (GRU) proposed by Cho et al. (2014), prove highly effective for machine learning patterns that include sequential data. We propose an exceptionally simple variant of these gated architects. The basic model takes the formht = \u03b8t tanh (ht \u2212 1) + \u03b7t tanh (Wxt), (1) where stands for the Hadamard product. The horizontal / forget gate (i.e.) and the vertical / input gate (i.e. \u03b7t) take the usual form used in most gated RNN architectures: = Dynamiht: = Dynamiht \u2212 1 + V\u03b8xt + bhabi) and goveri.we. \""}, {"heading": "2 CHAOS IN RECURRENT NEURAL NETWORKS", "text": "The study of RNNs of a dynamic system has brought fruitful insights into generic characteristics of RNNs (Sussillo & Barak, 2013; Pascanu et al., 2013).We will follow a brief study of CFN, LSTM and GRU networks that use this formalism, as it allows us to identify key differences between them."}, {"heading": "2.1 CHAOTIC BEHAVIOR OF LSTM AND GRU IN THE ABSENCE OF INPUT DATA", "text": "In this field, we briefly show that in the absence of input data that have led to a normal distribution, we are not able to trump ourselves. (In the absence of input data) We can only be able to trump ourselves. (In the absence of input data) We can only be able to trump ourselves. (In the absence of input data.) In the absence of \"absence.\" (In the absence of input data.) We must put ourselves in the position to trump ourselves. (In the absence of input data.) We must put ourselves in the position to trump ourselves in the absence. \"(In the absence of absence.) We must be able to trump ourselves in the absence of input data.\" (In the absence of input data.) In the absence of absence. \"In the absence. (in the absence of absence of absence.) In the absence.\" In the absence of absence."}, {"heading": "2.2 CHAOS-FREE BEHAVIOR OF THE CFN", "text": "The dynamic behavior of the CFN is dramatically different from that of the LSTM. In this subsection, we begin by showing that the hidden states of the CFN move and relax in a predictable manner towards zero, in response to the input data. This shows, on the one hand, that the CFN cannot generate non-trivial dynamics without having any influence of data. On the other hand, this leads to an interpretable model; every non-trivial activation in the hidden states of a CFN has a clear cause, which comes from the data-driven activation, which follows from a precise, quantified description of the intuitive image (3) - (4) outlined in the introduction. We start with the following simple estimation, which activates light on the hidden states of the CFN and then relaxes towards the origin."}, {"heading": "3 EXPERIMENTS", "text": "In this section, we show that despite its simplicity, the CFN network (0) performs as well as the much more complex LSTM networks (1). We use two datasets for these experiments, namely the CFNs and LSTM networks in a similar way and always compare the same number of parameters. We compare their performance with and without dropout and show that they achieve similar results in both cases. (2014), Jozefowicz et al. (2015) and Sukhbaatar al. (2015) For the concrete implementation of our model (0)."}, {"heading": "4 CONCLUSION", "text": "Despite its simple dynamics, the CFN achieves results that compare well with LSTM networks and word-level speech modeling GRUs, suggesting that it may generally be possible to build RNNs that perform well, while avoiding the complicated, non-interpretable, and potentially chaotic dynamics that can occur in LSTMs and GRUs. Of course, it remains to be seen whether dynamically simple RNNNs like the proposed CFN can handle a variety of tasks well and potentially require longer-term dependencies than those required for word-level speech modeling. Therefore, the experiments presented in Section 2 show a plausible way forward - activations in the higher layers of a multi-layer CFN decay at a lower rate than activations in the lower layers. Theoretically, complexity and long-term dependencies can be captured with a more \"feed-forward\" approach than an image (i.e., the complicated layering and layering to rely on)."}], "references": [{"title": "Real-time computation at the edge of chaos in recurrent neural networks", "author": ["Nils Bertschinger", "Thomas Natschl\u00e4ger"], "venue": "Neural computation,", "citeRegEx": "Bertschinger and Natschl\u00e4ger.,? \\Q2004\\E", "shortCiteRegEx": "Bertschinger and Natschl\u00e4ger.", "year": 2004}, {"title": "Learning phrase representations using rnn encoder-decoder for statistical machine translation", "author": ["Kyunghyun Cho", "Bart Van Merri\u00ebnboer", "Caglar Gulcehre", "Dzmitry Bahdanau", "Fethi Bougares", "Holger Schwenk", "Yoshua Bengio"], "venue": "arXiv preprint arXiv:1406.1078,", "citeRegEx": "Cho et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Cho et al\\.", "year": 2014}, {"title": "Learning to forget: Continual prediction with lstm", "author": ["Felix A Gers", "J\u00fcrgen Schmidhuber", "Fred Cummins"], "venue": "Neural computation,", "citeRegEx": "Gers et al\\.,? \\Q2000\\E", "shortCiteRegEx": "Gers et al\\.", "year": 2000}, {"title": "A two-dimensional mapping with a strange attractor", "author": ["Michel H\u00e9non"], "venue": "Communications in Mathematical Physics,", "citeRegEx": "H\u00e9non.,? \\Q1976\\E", "shortCiteRegEx": "H\u00e9non.", "year": 1976}, {"title": "Long short-term memory", "author": ["Sepp Hochreiter", "J\u00fcrgen Schmidhuber"], "venue": "Neural computation,", "citeRegEx": "Hochreiter and Schmidhuber.,? \\Q1997\\E", "shortCiteRegEx": "Hochreiter and Schmidhuber.", "year": 1997}, {"title": "An empirical exploration of recurrent network architectures", "author": ["Rafal Jozefowicz", "Wojciech Zaremba", "Ilya Sutskever"], "venue": "In Proceedings of the 32nd International Conference on Machine Learning,", "citeRegEx": "Jozefowicz et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Jozefowicz et al\\.", "year": 2015}, {"title": "Building a large annotated corpus of english: The penn treebank", "author": ["Mitchell P Marcus", "Mary Ann Marcinkiewicz", "Beatrice Santorini"], "venue": "Computational linguistics,", "citeRegEx": "Marcus et al\\.,? \\Q1993\\E", "shortCiteRegEx": "Marcus et al\\.", "year": 1993}, {"title": "Learning longer memory in recurrent neural networks", "author": ["Tomas Mikolov", "Armand Joulin", "Sumit Chopra", "Michael Mathieu", "Marc\u2019Aurelio Ranzato"], "venue": "arXiv preprint arXiv:1412.7753,", "citeRegEx": "Mikolov et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Mikolov et al\\.", "year": 2014}, {"title": "On the difficulty of training recurrent neural networks", "author": ["Razvan Pascanu", "Tomas Mikolov", "Yoshua Bengio"], "venue": "ICML (3),", "citeRegEx": "Pascanu et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Pascanu et al\\.", "year": 2013}, {"title": "Nonlinear dynamics and chaos: with applications to physics, biology, chemistry, and engineering", "author": ["Steven H Strogatz"], "venue": "Westview press,", "citeRegEx": "Strogatz.,? \\Q2014\\E", "shortCiteRegEx": "Strogatz.", "year": 2014}, {"title": "End-to-end memory networks", "author": ["Sainbayar Sukhbaatar", "Jason Weston", "Rob Fergus"], "venue": "In Advances in neural information processing systems,", "citeRegEx": "Sukhbaatar et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Sukhbaatar et al\\.", "year": 2015}, {"title": "Opening the black box: low-dimensional dynamics in highdimensional recurrent neural networks", "author": ["David Sussillo", "Omri Barak"], "venue": "Neural computation,", "citeRegEx": "Sussillo and Barak.,? \\Q2013\\E", "shortCiteRegEx": "Sussillo and Barak.", "year": 2013}, {"title": "Recurrent neural network regularization", "author": ["Wojciech Zaremba", "Ilya Sutskever", "Oriol Vinyals"], "venue": "arXiv preprint arXiv:1409.2329,", "citeRegEx": "Zaremba et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Zaremba et al\\.", "year": 2014}], "referenceMentions": [{"referenceID": 1, "context": "Gated recurrent neural networks, such as the Long Short Term Memory network (LSTM) introduced by Hochreiter & Schmidhuber (1997) and the Gated Recurrent Unit (GRU) proposed by Cho et al. (2014), prove highly effective for machine learning tasks that involve sequential data.", "startOffset": 176, "endOffset": 194}, {"referenceID": 8, "context": "The study of RNNs from a dynamical systems point-of-view has brought fruitful insights into generic features of RNNs (Sussillo & Barak, 2013; Pascanu et al., 2013).", "startOffset": 117, "endOffset": 163}, {"referenceID": 9, "context": "In this subsection we briefly show that LSTM and GRU, in the absence of input data, can lead to dynamical systems ut = \u03a6(ut\u22121) that are chaotic in the classical sense of the term (Strogatz, 2014).", "startOffset": 179, "endOffset": 195}, {"referenceID": 6, "context": "We use two datasets for these experiments, namely the Penn Treebank corpus (Marcus et al., 1993) and the Text8 corpus (Mikolov et al.", "startOffset": 75, "endOffset": 96}, {"referenceID": 7, "context": ", 1993) and the Text8 corpus (Mikolov et al., 2014).", "startOffset": 29, "endOffset": 51}, {"referenceID": 5, "context": "We use two datasets for these experiments, namely the Penn Treebank corpus (Marcus et al., 1993) and the Text8 corpus (Mikolov et al., 2014). We consider both one-layer and two-layer CFNs and LSTMs for our experiments. We train both CFN and LSTM networks in a similar fashion and always compare models that use the same number of parameters. We compare their performance with and without dropout, and show that in both cases they obtain similar results. We also provide results published in Mikolov et al. (2014), Jozefowicz et al.", "startOffset": 76, "endOffset": 513}, {"referenceID": 5, "context": "(2014), Jozefowicz et al. (2015) and Sukhbaatar et al.", "startOffset": 8, "endOffset": 33}, {"referenceID": 5, "context": "(2014), Jozefowicz et al. (2015) and Sukhbaatar et al. (2015) for the sake of comparison.", "startOffset": 8, "endOffset": 62}, {"referenceID": 5, "context": "Vanilla RNN 5M parameters Jozefowicz et al. (2015) 122.", "startOffset": 26, "endOffset": 51}, {"referenceID": 5, "context": "Vanilla RNN 5M parameters Jozefowicz et al. (2015) 122.9 GRU 5M parameters Jozefowicz et al. (2015) 108.", "startOffset": 26, "endOffset": 100}, {"referenceID": 5, "context": "Vanilla RNN 5M parameters Jozefowicz et al. (2015) 122.9 GRU 5M parameters Jozefowicz et al. (2015) 108.2 LSTM 5M parameters Jozefowicz et al. (2015) 109.", "startOffset": 26, "endOffset": 150}, {"referenceID": 7, "context": "on development set Vanilla RNN 500 hidden units Mikolov et al. (2014) 184 SCRN 500 hidden units Mikolov et al.", "startOffset": 48, "endOffset": 70}, {"referenceID": 7, "context": "on development set Vanilla RNN 500 hidden units Mikolov et al. (2014) 184 SCRN 500 hidden units Mikolov et al. (2014) 161 LSTM 500 hidden units Mikolov et al.", "startOffset": 48, "endOffset": 118}, {"referenceID": 7, "context": "on development set Vanilla RNN 500 hidden units Mikolov et al. (2014) 184 SCRN 500 hidden units Mikolov et al. (2014) 161 LSTM 500 hidden units Mikolov et al. (2014) 156 MemN2N 500 hidden units Sukhbaatar et al.", "startOffset": 48, "endOffset": 166}, {"referenceID": 7, "context": "on development set Vanilla RNN 500 hidden units Mikolov et al. (2014) 184 SCRN 500 hidden units Mikolov et al. (2014) 161 LSTM 500 hidden units Mikolov et al. (2014) 156 MemN2N 500 hidden units Sukhbaatar et al. (2015) 147 LSTM (2 layers) 46.", "startOffset": 48, "endOffset": 219}, {"referenceID": 2, "context": "The importance of a careful initialization of the forget gate was first pointed out in Gers et al. (2000) and further emphasized in Jozefowicz et al.", "startOffset": 87, "endOffset": 106}, {"referenceID": 2, "context": "The importance of a careful initialization of the forget gate was first pointed out in Gers et al. (2000) and further emphasized in Jozefowicz et al. (2015). Finally, we initialize all hidden states to zero for both models.", "startOffset": 87, "endOffset": 157}, {"referenceID": 2, "context": "The importance of a careful initialization of the forget gate was first pointed out in Gers et al. (2000) and further emphasized in Jozefowicz et al. (2015). Finally, we initialize all hidden states to zero for both models. Dataset Construction. The Penn Treebank Corpus has 1 million words and a vocabulary size of 10,000. We used the code from Zaremba et al. (2014) to construct and split the dataset into a training set (929K words), a validation set (73K words) and a test set (82K words).", "startOffset": 87, "endOffset": 368}, {"referenceID": 2, "context": "The importance of a careful initialization of the forget gate was first pointed out in Gers et al. (2000) and further emphasized in Jozefowicz et al. (2015). Finally, we initialize all hidden states to zero for both models. Dataset Construction. The Penn Treebank Corpus has 1 million words and a vocabulary size of 10,000. We used the code from Zaremba et al. (2014) to construct and split the dataset into a training set (929K words), a validation set (73K words) and a test set (82K words). The Text8 corpus has 100 million characters and a vocabulary size of 44,000. We used the script from Mikolov et al. (2014) to", "startOffset": 87, "endOffset": 617}, {"referenceID": 5, "context": "Vanilla RNN 20M parameters Jozefowicz et al. (2015) 103.", "startOffset": 27, "endOffset": 52}, {"referenceID": 5, "context": "Vanilla RNN 20M parameters Jozefowicz et al. (2015) 103.0 97.7 GRU 20M parameters Jozefowicz et al. (2015) 95.", "startOffset": 27, "endOffset": 107}, {"referenceID": 5, "context": "Vanilla RNN 20M parameters Jozefowicz et al. (2015) 103.0 97.7 GRU 20M parameters Jozefowicz et al. (2015) 95.5 91.7 LSTM 20M parameters Jozefowicz et al. (2015) 83.", "startOffset": 27, "endOffset": 162}, {"referenceID": 5, "context": "The first three rows report results published in (Jozefowicz et al., 2015) and the last four rows provide results for LSTM and CFN networks trained and initialized with the strategy previously described.", "startOffset": 49, "endOffset": 74}, {"referenceID": 5, "context": "We also report results published in Jozefowicz et al. (2015) were a vanilla RNN, a GRU and an LSTM network were trained on Penn Treebank, each of them having 5 million parameters (only the test perplexity was reported).", "startOffset": 36, "endOffset": 61}, {"referenceID": 5, "context": "We also report results published in Jozefowicz et al. (2015) were a vanilla RNN, a GRU and an LSTM network were trained on Penn Treebank, each of them having 5 million parameters (only the test perplexity was reported). Finally we report results published in Mikolov et al. (2014) and Sukhbaatar et al.", "startOffset": 36, "endOffset": 281}, {"referenceID": 5, "context": "We also report results published in Jozefowicz et al. (2015) were a vanilla RNN, a GRU and an LSTM network were trained on Penn Treebank, each of them having 5 million parameters (only the test perplexity was reported). Finally we report results published in Mikolov et al. (2014) and Sukhbaatar et al. (2015) where various networks are trained on Text8.", "startOffset": 36, "endOffset": 310}, {"referenceID": 5, "context": "We also report results published in Jozefowicz et al. (2015) were a vanilla RNN, a GRU and an LSTM network were trained on Penn Treebank, each of them having 5 million parameters (only the test perplexity was reported). Finally we report results published in Mikolov et al. (2014) and Sukhbaatar et al. (2015) where various networks are trained on Text8. Of these four networks, only the LSTM network from Mikolov et al. (2014) has the same number of parameters than the CFN and LSTM networks we trained (46.", "startOffset": 36, "endOffset": 428}], "year": 2016, "abstractText": "We introduce an exceptionally simple gated recurrent neural network (RNN) that achieves performance comparable to well-known gated architectures, such as LSTMs and GRUs, on the word-level language modeling task. We prove that our model has simple, predicable and non-chaotic dynamics. This stands in stark contrast to more standard gated architectures, whose underlying dynamical systems exhibit chaotic behavior.", "creator": "LaTeX with hyperref package"}}}