{"id": "1612.06475", "review": {"conference": "EMNLP", "VERSION": "v1", "DATE_OF_SUBMISSION": "20-Dec-2016", "title": "Span-Based Constituency Parsing with a Structure-Label System and Provably Optimal Dynamic Oracles", "abstract": "Parsing accuracy using efficient greedy transition systems has improved dramatically in recent years thanks to neural networks. Despite striking results in dependency parsing, however, neural models have not surpassed state-of-the-art approaches in constituency parsing. To remedy this, we introduce a new shift-reduce system whose stack contains merely sentence spans, represented by a bare minimum of LSTM features. We also design the first provably optimal dynamic oracle for constituency parsing, which runs in amortized O(1) time, compared to O(n^3) oracles for standard dependency parsing. Training with this oracle, we achieve the best F1 scores on both English and French of any parser that does not use reranking or external data.", "histories": [["v1", "Tue, 20 Dec 2016 01:23:00 GMT  (66kb,D)", "http://arxiv.org/abs/1612.06475v1", "EMNLP 2016"]], "COMMENTS": "EMNLP 2016", "reviews": [], "SUBJECTS": "cs.CL", "authors": ["james cross", "liang huang 0001"], "accepted": true, "id": "1612.06475"}, "pdf": {"name": "1612.06475.pdf", "metadata": {"source": "CRF", "title": "Span-Based Constituency Parsing with a Structure-Label System and Provably Optimal Dynamic Oracles", "authors": ["James Cross", "Liang Huang"], "emails": ["liang.huang.sh}@gmail.com"], "sections": [{"heading": "1 Introduction", "text": "Parsing is an important problem in the processing of natural language that has been extensively studied for decades. Between the two basic paradigms of parsing, constituency parsing, the subject of this work, has generally proven to be more difficult than dependence parsing, both in terms of accuracy and duration of parsing algorithms. Recently, there has been a huge increase in interest in the use of neural networks, and such models continue to dominate the state of the art in relation to parsing (Andor et al., 2016). However, in constituency parsing, neural approaches are still behind the state-of-the-art (Carreras et al., 2008; Shindo et al., 2012; Thang et al., 2015). To address this, we are designing a new parsing framework that is better suited for constituency parsing, and that can be precisely modelled by neural networks."}, {"heading": "2 Parsing System", "text": "It is a paradox in the way in which the individual plot lines and plot lines are interwoven with each other, how they differ in the individual plot lines and plot lines of the individual plot lines in the individual plot lines and plot lines of the individual plot lines. There are only two structural storylines: the shifting of the storylines in other transitional systems, while the linking of the storylines connects the two sentences, the latter being analogous to a reduction of the storylines, which do not directly create a tree structure and do not form direct storylines. There are only two structural storylines: the shifting of belonging to the other transistack systems, in which the two sentences are fused together."}, {"heading": "3 LSTM Span Features", "text": "It is a kind of recurrent neural network model proposed by Hochreiter and Schmidhuber (1997), which is very effective for modeling sequences; they are able to capture and generalize the interactions between their sequential inputs, even if separated by a long distance, and are therefore a natural adaptation for the analysis of natural language. LSTMs have also proven to be a powerful tool for many learning tasks in natural language, such as speech modeling (Sundermeyer et al., 2012) and translation (Sutskever et al.).LSTMs have incorporated a variety of differences into the analysis, such as the direct coding of an entire sentence (Vinyals et al., 2015), separately the modeling of the stack, and the action history (Dyer et al., 2015), to enwords based on their character forms (Ballesteros et al., 2015), and as an element in a combined structure to combine with their children's dependencies."}, {"heading": "4 Dynamic Oracle", "text": "The basic method of training our parser is what is known as a static oracle: we simply create the sequence of actions to properly analyze each training set, using short-stack heuristics (i.e., combine first whenever there is a choice between layer and combination), but this method suffers from a well-documented problem, namely that it only \"prepares\" the model for the situation in which no mistakes were made in parsing, an inevitably erroneous assumption in practice. To mitigate this problem, Goldberg and Nivre (2013) define a dynamic oracle to return the best possible actions to any configuration. In this section, we present an easy-to-calculate optimal dynamic oracle for our constituency saver. We will first define some concepts on which the dynamic oracle is built, and then show how the optimal actions can be calculated very efficiently using this framework."}, {"heading": "4.1 Preliminaries and Notations", "text": "(D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D. (D). (D). (D. (D). (D). (D. (D). (D). (D). (D). (D). (D). (D). (D. (D). (D). (D). (D). (D). (D. (D). (D). (D. (D). (D). (D). (D. (D). (D). (D). (D). (D. (D). (D). (D). (D). (D. (D). (D). (D). (D). (D). (D). (D. (D. (D). (D). (D. (D). (D. (D). (D). (D). (D). (D. (D). (D). (D. (D). (D). (D. (D. (D). (D). (D. (D). (D). (D). (D. (D). (. (D). (D). (. (D). (. (D). (D). (D). (. ("}, {"heading": "4.2 Structural and Label Oracles", "text": "In a straight configuration c = < z, \u03c3 | i | j, t > we call the next attainable gold bracket (c) pXq and define the dynamic oracle as: dyna (c) = {sh} if p = i and q > j {comb} if p < i and q = j {sh, comb} if p < i and q > j (1) As a special case dyna (< 0, [0], \u2205 >) = {sh}. Figure 5 shows examples of this policy. The most important finding is that if you follow this policy, you will not miss the next attainable bracket, but if you do not follow it, you will certainly do so. We formalize this fact below (with a reference omitted due to space constraints), which are used to prove the key results."}, {"heading": "4.3 Correctness", "text": "To show the optimality of our dynamic oracle, we start by defining a special tree t * (c) and show that it is optimal among all the trees we can achieve in terms of configuration. (c) We then show that we can extend the optimal tree t * (c) by all available gold brackets and nothing else. (c) We do not have (c). Configuration oraclestatic dynamic 0Some t * e symbol or scaled1Some text and the scaled1Some text and the scaled2Some text or the scaled2Some text and the symbol or scaled3 comb {comb} I do like 1, sh} I do like 1 and the symbol or scaled130Some text and the scaled1Some text and the scaled130Some text and the scaled2Some text and the scaled2Some text and the scaled3 comb {comb} I do like 1, sh} I do like 1 and the symbol or scaled0130Some text and the scaled0some text."}, {"heading": "4.4 Implementation and Complexity", "text": "For each configuration, our dynamic oracle can be calculated in amortized constant time, since there are only O (n) gold clasps and thus the range (c) | and the choice of the next (c). After each action, the next (c) remains either unchanged or must be updated in case of overrun by a structural action or mislabeling by a label action. This update merely repeats the superordinate link to the next smallest gold clasp until the new clasp covers the span (i, j). As there are no more than O (n) decisions of the next (c) and O (n) steps, the cost per step is constantly amortized. Therefore, our dynamic oracle is much faster than the superlinear time oracle for the arc standards dependence parsed in Goldberg et al. (2014)."}, {"heading": "5 Related Work", "text": "For example, Socher et al. (2013) use a recursive network that combines vectors representing sub-trees, Vinyals et al. (2015) adapt a sequence sequence sequence model to produce parse trees, Watanabe and Sumita (2015) use a recursive model that applies a layer reduction system to parsing constituencies with beam search, and Dyer et al. (2016) adapt the StackLSTM dependence analysis approach for this task. Durrett and Klein (2015) combine both neural and sparse features for a CKY parsing system. Our own previous work (Cross and Huang, 2016) use a recurring sentence representation in a head-driven transition system that allows greedy parsing but does not achieve state-of-the-art outcomes. The concept of \"oracle\" for parsing constituencies (as the tree most similar to any possible transition system) was first resolved by a shift from top to bottom (3)."}, {"heading": "6 Experiments", "text": "We present experiments at both the Penn English Treebank (Marcus et al., 1993) and the French Treebank (Abeille \u0301 et al., 2003). In both cases, all state training pairs are used for a given set at the same time, which greatly increases the training speed, since all examples of the same set have the same forward and reverse gear. The only regulation we apply during the training is the drop-out (Hinton et al., 2012), which is applied with a 0.5 probability to the recurring results. It is applied separately to the input to the second LSTM layer for each set, and to the input to the ReLU layer (chip characteristics) for each state pair. We use the ADADELTA method (Zeiler, 2012) to determine the learning rates for all weights."}, {"heading": "6.1 Training with Dynamic Oracle", "text": "The simplest application of dynamic oracles to train a neural network model, in which we collect all the examples of action for a particular set before updating, is \"Training with Exploration,\" as proposed by Goldberg and Nivre (2013), which involves parsing each set according to the current model and using the oracle to determine the right measures for training. However, we saw very little improvement compared to the Penn Tree Bank validation set that uses this method. Based on the analytical accuracy of the training sets, this seems to be due to the fact that the model overlaps training data too early in training, negating the benefit of training on erroneous paths. Accordingly, we also used a method recently proposed by Ballesteros et al. (2016) that specifically addresses this problem. This method introduces stochasticity into the training data parses by randomly dividing actions according to the Softmax distribution versus the action trajectories, which is also very effective in filtering the training data parses, although this results are also very effective in the higher level of error."}, {"heading": "6.2 Penn Treebank", "text": "Following the literature, we used the Wall Street Journal section of Penn Treebank, with standard splits for training (seconds 2-21), development (seconds 22) and test kits (seconds 23). As our parsing system seamlessly processes non-binary productions, minimal data pre-processing was required, and for the part-of-speech tags, which are a required input for our parser, we used the Stanford tagger with 10-way jackknives. Table 4 compares our results on PTB with a number of other leading constituency parsers. Although he is a greedy parser, when trained with dynamic oracles with exploration, he achieves the best F1 score of all closed parsers with a single model."}, {"heading": "6.3 French Treebank", "text": "We also report on the results of the French tree bank, with a small change in the network structure. Specifically, we included morphological characteristics for each word as input into the recurring network, using a small embedding for each of these characteristics to show that our parsing model is capable of using such additional characteristics. We used the predicted morphological characteristics, part-of-speech tags and lemmas (instead of word surface shapes) supplied with the SPMRL 2014 dataset (Seddah et al., 2014). Therefore, it is possible that the results can be further improved by an integrated or more precise predictor for these characteristics. Our analysis and evaluation also includes predicting POS tags for multilingual expressions, as is the standard practice for the French tree bank, although our results are similar to whether this aspect is included or not. We compare our parser with other current work in 5. We achieve highly modern results compared to those obtained in the French tree bank (even in 2014)."}, {"heading": "6.4 Notes on Experiments", "text": "We have every reason to believe that performance could be further improved by techniques such as random reboots, larger hidden layers, external embedding, and search for hyperparameters, as Weiss et al. (2015) have shown. We also note that even with greedy decoding, our parser is highly accurate, but the model is easily adaptable for bar searching, especially since the parsing system already uses a fixed number of actions. Bar searching could also be made much more efficient by caching components of the post-hidden layer function for sentence spans, essentially using the precomputation trick described by Chen and Manning (2014), but on the basis of a sentence."}, {"heading": "7 Conclusion and Future Work", "text": "We have developed a new transformation-based constituency parser based on sentence stresses, using a factor system that alternates between structural and label actions. We also describe a fast dynamic oracle for this parser that can determine the optimal set of actions related to a gold training tree in any state. Using an LSTM model and with only fewer sentence stresses as characteristics, we achieve state-of-the-art accuracy for all parsers on the Penn Treebank without re-capturing them, despite the use of strict greedy conclusions. In the future, we hope to achieve even better results by beam searching, which is relatively simple since the parsing system already uses a fixed number of actions. Dynamic programming (Huang and Sagae, 2010) may be particularly powerful in this regard, given the very simple feature representation used by our parser, as well as by Kiperwasser and Goldberg (2016b)."}, {"heading": "Acknowledgments", "text": "We thank the three anonymous reviewers Kai Zhao, Lemao Liu, Yoav Goldberg and Slav Petrov for suggestions, Juneki Hong for proofreading and Maximin Coavoux for sharing their manuscript. This project was partially supported by NSF IIS-1656051, DARPA FA8750-13-2-0041 (DEFT) and a Google Faculty Research Award."}], "references": [{"title": "Building a treebank for french", "author": ["Abeill\u00e9 et al.2003] Anne Abeill\u00e9", "Lionel Cl\u00e9ment", "Fran\u00e7ois Toussenel"], "venue": "In Treebanks,", "citeRegEx": "Abeill\u00e9 et al\\.,? \\Q2003\\E", "shortCiteRegEx": "Abeill\u00e9 et al\\.", "year": 2003}, {"title": "Globally normalized transition-based neural networks", "author": ["Andor et al.2016] Daniel Andor", "Chris Alberti", "David Weiss", "Aliaksei Severyn", "Alessandro Presta", "Kuzman Ganchev", "Slav Petrov", "Michael Collins"], "venue": "Proceedings of ACL", "citeRegEx": "Andor et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Andor et al\\.", "year": 2016}, {"title": "Improved transition-based parsing by modeling characters instead of words with lstms", "author": ["Chris Dyer", "Noah A Smith"], "venue": "arXiv preprint arXiv:1508.00657", "citeRegEx": "Ballesteros et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Ballesteros et al\\.", "year": 2015}, {"title": "Training with exploration improves a greedy stack-lstm parser. arXiv preprint arXiv:1603.03793", "author": ["Yoav Goldberg", "Chris Dyer", "Noah A Smith"], "venue": null, "citeRegEx": "Ballesteros et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Ballesteros et al\\.", "year": 2016}, {"title": "Introducing the ims-wroc\u0142aw-szeged-cis entry at the spmrl", "author": ["Ozlem Cetinoglu", "Agnieszka Falenska", "Rich\u00e1rd Farkas", "Thomas Mueller", "Wolfgang Seeker", "Zsolt Sz\u00e1nt\u00f3"], "venue": null, "citeRegEx": "Bj\u00f6rkelund et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Bj\u00f6rkelund et al\\.", "year": 2014}, {"title": "Tag, dynamic programming, and the perceptron for efficient, feature-rich parsing", "author": ["Michael Collins", "Terry Koo"], "venue": "In Proceedings of the Twelfth Conference on Computational Natural Language Learning,", "citeRegEx": "Carreras et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Carreras et al\\.", "year": 2008}, {"title": "A fast and accurate dependency parser using neural networks. In Empirical Methods in Natural Language Processing (EMNLP)", "author": ["Chen", "Manning2014] Danqi Chen", "Christopher D Manning"], "venue": null, "citeRegEx": "Chen et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Chen et al\\.", "year": 2014}, {"title": "Incremental parsing with minimal features using bi-directional lstm", "author": ["Cross", "Huang2016] James Cross", "Liang Huang"], "venue": "Proceedings of ACL", "citeRegEx": "Cross et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Cross et al\\.", "year": 2016}, {"title": "Transition-based dependency parsing with stack long short-term memory", "author": ["Dyer et al.2015] Chris Dyer", "Miguel Ballesteros", "Wang Ling", "Austin Matthews", "Noah A Smith"], "venue": "Empirical Methods in Natural Language Processing (EMNLP)", "citeRegEx": "Dyer et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Dyer et al\\.", "year": 2015}, {"title": "Recurrent neural network grammars", "author": ["Dyer et al.2016] Chris Dyer", "Adhiguna Kuncoro", "Miguel Ballesteros", "Noah A Smith"], "venue": "Proceedings of HLT-NAACL", "citeRegEx": "Dyer et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Dyer et al\\.", "year": 2016}, {"title": "Training deterministic parsers with nondeterministic oracles. Transactions of the association for Computational Linguistics, 1:403\u2013414", "author": ["Goldberg", "Nivre2013] Yoav Goldberg", "Joakim Nivre"], "venue": null, "citeRegEx": "Goldberg et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Goldberg et al\\.", "year": 2013}, {"title": "A tabular method for dynamic oracles in transition-based parsing", "author": ["Francesco Sartorio", "Giorgio Satta"], "venue": null, "citeRegEx": "Goldberg et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Goldberg et al\\.", "year": 2014}, {"title": "Discriminative training of a neural network statistical parser", "author": ["James Henderson"], "venue": "In Proceedings of ACL", "citeRegEx": "Henderson.,? \\Q2004\\E", "shortCiteRegEx": "Henderson.", "year": 2004}, {"title": "Improving neural networks by preventing co-adaptation of feature detectors. CoRR, abs/1207.0580", "author": ["Nitish Srivastava", "Alex Krizhevsky", "Ilya Sutskever", "Ruslan Salakhutdinov"], "venue": null, "citeRegEx": "Hinton et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Hinton et al\\.", "year": 2012}, {"title": "Long short-term memory", "author": ["Hochreiter", "Schmidhuber1997] Sepp Hochreiter", "J\u00fcrgen Schmidhuber"], "venue": "Neural computation,", "citeRegEx": "Hochreiter et al\\.,? \\Q1997\\E", "shortCiteRegEx": "Hochreiter et al\\.", "year": 1997}, {"title": "Dynamic programming for linear-time incremental parsing", "author": ["Huang", "Sagae2010] Liang Huang", "Kenji Sagae"], "venue": "In Proceedings of ACL", "citeRegEx": "Huang et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Huang et al\\.", "year": 2010}, {"title": "Forest reranking: Discriminative parsing with non-local features", "author": ["Liang Huang"], "venue": "In Proceedings of the ACL: HLT,", "citeRegEx": "Huang.,? \\Q2008\\E", "shortCiteRegEx": "Huang.", "year": 2008}, {"title": "2016a. Easy-first dependency parsing with hierarchical tree lstms", "author": ["Kiperwasser", "Yoav Goldberg"], "venue": "arXiv preprint arXiv:1603.00375", "citeRegEx": "Kiperwasser et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Kiperwasser et al\\.", "year": 2016}, {"title": "2016b. Simple and accurate dependency parsing using bidirectional LSTM feature representations. CoRR, abs/1603.04351", "author": ["Kiperwasser", "Yoav Goldberg"], "venue": null, "citeRegEx": "Kiperwasser et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Kiperwasser et al\\.", "year": 2016}, {"title": "Building a large annotated corpus of english: The penn treebank", "author": ["Mary Ann Marcinkiewicz", "Beatrice Santorini"], "venue": null, "citeRegEx": "Marcus et al\\.,? \\Q1993\\E", "shortCiteRegEx": "Marcus et al\\.", "year": 1993}, {"title": "Reranking and selftraining for parser adaptation", "author": ["Eugene Charniak", "Mark Johnson"], "venue": "In Proceedings of the 21st International Conference on Computational Linguistics and the 44th annual meeting of the Associ-", "citeRegEx": "McClosky et al\\.,? \\Q2006\\E", "shortCiteRegEx": "McClosky et al\\.", "year": 2006}, {"title": "Shift-reduce constituency parsing with dynamic programming and pos tag lattice", "author": ["Mi", "Huang2015] Haitao Mi", "Liang Huang"], "venue": "In Proceedings of the 2015 Conference of the North American Chapter of the Association for Computational Linguistics:", "citeRegEx": "Mi et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Mi et al\\.", "year": 2015}, {"title": "Improved inference for unlexicalized parsing", "author": ["Petrov", "Klein2007] Slav Petrov", "Dan Klein"], "venue": "In Proceedings of HLT-NAACL", "citeRegEx": "Petrov et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Petrov et al\\.", "year": 2007}, {"title": "A best-first probabilistic shift-reduce parser", "author": ["Sagae", "Lavie2006] Kenji Sagae", "Alon Lavie"], "venue": "In Proceedings of the COLING/ACL on Main conference poster sessions,", "citeRegEx": "Sagae et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Sagae et al\\.", "year": 2006}, {"title": "Introducing the spmrl 2014 shared task on parsing morphologically-rich languages", "author": ["Seddah et al.2014] Djam\u00e9 Seddah", "Sandra K\u00fcbler", "Reut Tsarfaty"], "venue": "In Proceedings of the First Joint Workshop on Statistical Parsing of Morphologically Rich Lan-", "citeRegEx": "Seddah et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Seddah et al\\.", "year": 2014}, {"title": "Bayesian symbol-refined tree substitution grammars for syntactic parsing", "author": ["Yusuke Miyao", "Akinori Fujino", "Masaaki Nagata"], "venue": "In Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics:", "citeRegEx": "Shindo et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Shindo et al\\.", "year": 2012}, {"title": "Parsing with compositional vector grammars", "author": ["John Bauer", "Christopher D Manning", "Andrew Y Ng"], "venue": "ACL", "citeRegEx": "Socher et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Socher et al\\.", "year": 2013}, {"title": "Lstm neural networks for language modeling", "author": ["Ralf Schl\u00fcter", "Hermann Ney"], "venue": "In INTERSPEECH,", "citeRegEx": "Sundermeyer et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Sundermeyer et al\\.", "year": 2012}, {"title": "Sequence to sequence learning with neural networks", "author": ["Quoc V Le."], "venue": "Advances in neural information processing systems, pages 3104\u20133112.", "citeRegEx": "Le.,? 2014", "shortCiteRegEx": "Le.", "year": 2014}, {"title": "Optimal shift-reduce constituent parsing with structured perceptron", "author": ["Thang et al.2015] Le Quang Thang", "Hiroshi Noji", "Yusuke Miyao"], "venue": null, "citeRegEx": "Thang et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Thang et al\\.", "year": 2015}, {"title": "Grammar as a foreign language", "author": ["\u0141ukasz Kaiser", "Terry Koo", "Slav Petrov", "Ilya Sutskever", "Geoffrey Hinton"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Vinyals et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Vinyals et al\\.", "year": 2015}, {"title": "Graph-based dependency parsing with bidirectional lstm", "author": ["Wang", "Chang2016] Wenhui Wang", "Baobao Chang"], "venue": "In Proceedings of ACL", "citeRegEx": "Wang et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Wang et al\\.", "year": 2016}, {"title": "Transition-based neural constituent parsing", "author": ["Watanabe", "Sumita2015] Taro Watanabe", "Eiichiro Sumita"], "venue": "Proceedings of ACL-IJCNLP", "citeRegEx": "Watanabe et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Watanabe et al\\.", "year": 2015}, {"title": "Structured training for neural network transition-based parsing", "author": ["Weiss et al.2015] David Weiss", "Chris Alberti", "Michael Collins", "Slav Petrov"], "venue": "In Proceedings of ACL", "citeRegEx": "Weiss et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Weiss et al\\.", "year": 2015}, {"title": "ADADELTA: an adaptive learning rate method. CoRR, abs/1212.5701", "author": ["Matthew D. Zeiler"], "venue": null, "citeRegEx": "Zeiler.,? \\Q2012\\E", "shortCiteRegEx": "Zeiler.", "year": 2012}, {"title": "Fast and accurate shift-reduce constituent parsing", "author": ["Zhu et al.2013] Muhua Zhu", "Yue Zhang", "Wenliang Chen", "Min Zhang", "Jingbo Zhu"], "venue": "ACL", "citeRegEx": "Zhu et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Zhu et al\\.", "year": 2013}], "referenceMentions": [{"referenceID": 1, "context": "There has recently been a huge surge of interest in using neural networks to make parsing decisions, and such models continue to dominate the state of the art in dependency parsing (Andor et al., 2016).", "startOffset": 181, "endOffset": 201}, {"referenceID": 5, "context": "In constituency parsing, however, neural approaches are still behind the state-of-the-art (Carreras et al., 2008; Shindo et al., 2012; Thang et al., 2015); see more details in Section 5.", "startOffset": 90, "endOffset": 154}, {"referenceID": 25, "context": "In constituency parsing, however, neural approaches are still behind the state-of-the-art (Carreras et al., 2008; Shindo et al., 2012; Thang et al., 2015); see more details in Section 5.", "startOffset": 90, "endOffset": 154}, {"referenceID": 29, "context": "In constituency parsing, however, neural approaches are still behind the state-of-the-art (Carreras et al., 2008; Shindo et al., 2012; Thang et al., 2015); see more details in Section 5.", "startOffset": 90, "endOffset": 154}, {"referenceID": 16, "context": "The latter is significantly more difficult than the former due to F1 being a combination of precision and recall (Huang, 2008), and yet we propose a simple and extremely efficient oracle (amortizedO(1) time).", "startOffset": 113, "endOffset": 126}, {"referenceID": 1, "context": "There has recently been a huge surge of interest in using neural networks to make parsing decisions, and such models continue to dominate the state of the art in dependency parsing (Andor et al., 2016). In constituency parsing, however, neural approaches are still behind the state-of-the-art (Carreras et al., 2008; Shindo et al., 2012; Thang et al., 2015); see more details in Section 5. To remedy this, we design a new parsing framework that is more suitable for constituency parsing, and that can be accurately modeled by neural networks. Observing that constituency parsing is primarily focused on sentence spans (rather than individual words, as is dependency parsing), we propose a novel adaptation of the shift-reduce system which reflects this focus. In this system, the stack consists of sentence spans rather than partial trees. It is also factored into two types of parser actions, structural and label actions, which alternate during a parse. The structural actions are a simplified analogue of shift-reduce actions, omitting the directionality of reduce actions, while the label actions directly assign nonterminal symbols to sentence spans. Our neural model processes the sentence once for each parse with a recurrent network. We represent parser configurations with a very small number of span features (4 for structural actions and 3 for label actions). Extending Wang and Chang (2016), each span is represented as the difference of recurrent output from multiple layers in each direction.", "startOffset": 182, "endOffset": 1403}, {"referenceID": 16, "context": "Except for the use of spans, this factored approach is similar to the odd-even parser from Mi and Huang (2015). The fact that stack elements do not have to be tree-structured, however, means that we can create productions with arbitrary arity, and no binarization is required either for training or parsing.", "startOffset": 98, "endOffset": 111}, {"referenceID": 27, "context": "LSTM models have proved to be a powerful tool for many learning tasks in natural language, such as language modeling (Sundermeyer et al., 2012) and translation (Sutskever et al.", "startOffset": 117, "endOffset": 143}, {"referenceID": 30, "context": "LSTMs have also been incorporated into parsing in a variety of ways, such as directly encoding an entire sentence (Vinyals et al., 2015), separately modeling the stack, buffer, and action history (Dyer et al.", "startOffset": 114, "endOffset": 136}, {"referenceID": 8, "context": ", 2015), separately modeling the stack, buffer, and action history (Dyer et al., 2015), to encode words based on their character forms (Ballesteros et al.", "startOffset": 67, "endOffset": 86}, {"referenceID": 2, "context": ", 2015), to encode words based on their character forms (Ballesteros et al., 2015), and as an element in a recursive structure to combine dependency subtrees with their left and right children (Kiperwasser and Goldberg, 2016a).", "startOffset": 56, "endOffset": 82}, {"referenceID": 16, "context": "See Cross and Huang (2016) for more details on this approach.", "startOffset": 14, "endOffset": 27}, {"referenceID": 28, "context": "In our experiments, the embeddings are randomly initialized and learned from scratch together with all other network weights, and we would expect further performance improvement from incorporating embeddings pretrained from a large external corpus. The network structure after the the span features consists of a separate multilayer perceptron for each type of action (structural and label). For each action we use a single hidden layer with rectified linear (ReLU) activation. The model is trained on a peraction basis using a single correct action for each parser state, with a negative log softmax loss function, as in Chen and Manning (2014).", "startOffset": 64, "endOffset": 646}, {"referenceID": 28, "context": "The baseline method of training our parser is what is known as a static oracle: we simply generate the sequence of actions to correctly parse each training sentence, using a short-stack heuristic (i.e., combine first whenever there is a choice of shift and combine). This method suffers from a well-documeted problem, however, namely that it only \u201cprepares\u201d the model for the situation where no mistakes have been made during parsing, an inevitably incorrect assumption in practice. To alleviate this problem, Goldberg and Nivre (2013) define a dynamic oracle to return the best possible action(s) at any arbitrary configuration.", "startOffset": 76, "endOffset": 536}, {"referenceID": 10, "context": "The structure of this framework follows Goldberg et al. (2014).", "startOffset": 40, "endOffset": 63}, {"referenceID": 10, "context": "The following two definitions are similar to those for dependency parsing by Goldberg et al. (2014).", "startOffset": 77, "endOffset": 100}, {"referenceID": 10, "context": "Thus our dynamic oracle is much faster than the super-linear time oracle for arc-standard dependency parsing in Goldberg et al. (2014).", "startOffset": 112, "endOffset": 135}, {"referenceID": 26, "context": "For example, Socher et al. (2013) learn a recursive network that combines vectors representing partial trees, Vinyals et al.", "startOffset": 13, "endOffset": 34}, {"referenceID": 26, "context": "For example, Socher et al. (2013) learn a recursive network that combines vectors representing partial trees, Vinyals et al. (2015) adapt a sequence-tosequence model to produce parse trees, Watanabe and Sumita (2015) use a recursive model applying a shift-reduce system to constituency parsing with Network architecture Word embeddings 50 Tag embeddings 20 Morphological embeddings\u2020 10 LSTM layers 2 LSTM units 200 / direction ReLU hidden units 200 / action type", "startOffset": 13, "endOffset": 132}, {"referenceID": 26, "context": "For example, Socher et al. (2013) learn a recursive network that combines vectors representing partial trees, Vinyals et al. (2015) adapt a sequence-tosequence model to produce parse trees, Watanabe and Sumita (2015) use a recursive model applying a shift-reduce system to constituency parsing with Network architecture Word embeddings 50 Tag embeddings 20 Morphological embeddings\u2020 10 LSTM layers 2 LSTM units 200 / direction ReLU hidden units 200 / action type", "startOffset": 13, "endOffset": 217}, {"referenceID": 11, "context": "In transition-based parsing, the dynamic oracle for shift-reduce dependency parsing costs worst-case O(n3) time (Goldberg et al., 2014).", "startOffset": 112, "endOffset": 135}, {"referenceID": 8, "context": "beam search, and Dyer et al. (2016) adapt the StackLSTM dependency parsing approach to this task.", "startOffset": 17, "endOffset": 36}, {"referenceID": 8, "context": "beam search, and Dyer et al. (2016) adapt the StackLSTM dependency parsing approach to this task. Durrett and Klein (2015) combine both neural and sparse features for a CKY parsing system.", "startOffset": 17, "endOffset": 123}, {"referenceID": 8, "context": "beam search, and Dyer et al. (2016) adapt the StackLSTM dependency parsing approach to this task. Durrett and Klein (2015) combine both neural and sparse features for a CKY parsing system. Our own previous work (Cross and Huang, 2016) use a recurrent sentence representation in a head-driven transition system which allows for greedy parsing but does not achieve state-of-the-art results. The concept of \u201coracles\u201d for constituency parsing (as the tree that is most similar to tG among all possible trees) was first defined and solved by Huang (2008) in bottom-up parsing.", "startOffset": 17, "endOffset": 550}, {"referenceID": 19, "context": "We present experiments on both the Penn English Treebank (Marcus et al., 1993) and the French Treebank (Abeill\u00e9 et al.", "startOffset": 57, "endOffset": 78}, {"referenceID": 0, "context": ", 1993) and the French Treebank (Abeill\u00e9 et al., 2003).", "startOffset": 32, "endOffset": 54}, {"referenceID": 13, "context": "The only regularization which we employ during training is dropout (Hinton et al., 2012), which is applied with probability 0.", "startOffset": 67, "endOffset": 88}, {"referenceID": 34, "context": "We use the ADADELTA method (Zeiler, 2012) to schedule learning rates for all weights.", "startOffset": 27, "endOffset": 41}, {"referenceID": 13, "context": "The only regularization which we employ during training is dropout (Hinton et al., 2012), which is applied with probability 0.5 to the recurrent outputs. It is applied separately to the input to the second LSTM layer for each sentence, and to the input to the ReLU hidden layer (span features) for each stateaction pair. We use the ADADELTA method (Zeiler, 2012) to schedule learning rates for all weights. All of these design choices are summarized in Table 2. In order to account for unknown words during training, we also adopt the strategy described by Kiperwasser and Goldberg (2016b), where words in the training set are replaced with the unknownword symbol UNK with probability punk = z z+f(w) where f(w) is the number of times the word appears in the training corpus.", "startOffset": 68, "endOffset": 590}, {"referenceID": 26, "context": "The most straightforward use of dynamic oracles to train a neural network model, where we collect all action examples for a given sentence before updating, is \u201ctraining with exploration\u201d as proposed by Goldberg and Nivre (2013). This involves parsing each sentence according to the current model and using the oracle to determine correct actions for training.", "startOffset": 44, "endOffset": 228}, {"referenceID": 2, "context": "Accordingly, we also used a method recently proposed by Ballesteros et al. (2016), which specifically addresses this problem.", "startOffset": 56, "endOffset": 82}, {"referenceID": 22, "context": "Closed Training & Single Model LR LP F1 Sagae and Lavie (2006) 88.", "startOffset": 22, "endOffset": 63}, {"referenceID": 22, "context": "Closed Training & Single Model LR LP F1 Sagae and Lavie (2006) 88.1 87.8 87.9 Petrov and Klein (2007) 90.", "startOffset": 22, "endOffset": 102}, {"referenceID": 5, "context": "2 Carreras et al. (2008) 90.", "startOffset": 2, "endOffset": 25}, {"referenceID": 5, "context": "2 Carreras et al. (2008) 90.7 91.4 91.1 Shindo et al. (2012) 91.", "startOffset": 2, "endOffset": 61}, {"referenceID": 5, "context": "2 Carreras et al. (2008) 90.7 91.4 91.1 Shindo et al. (2012) 91.1 \u2020Socher et al. (2013) 90.", "startOffset": 2, "endOffset": 88}, {"referenceID": 5, "context": "2 Carreras et al. (2008) 90.7 91.4 91.1 Shindo et al. (2012) 91.1 \u2020Socher et al. (2013) 90.4 Zhu et al. (2013) 90.", "startOffset": 2, "endOffset": 111}, {"referenceID": 5, "context": "2 Carreras et al. (2008) 90.7 91.4 91.1 Shindo et al. (2012) 91.1 \u2020Socher et al. (2013) 90.4 Zhu et al. (2013) 90.2 90.7 90.4 Mi and Huang (2015) 90.", "startOffset": 2, "endOffset": 146}, {"referenceID": 5, "context": "2 Carreras et al. (2008) 90.7 91.4 91.1 Shindo et al. (2012) 91.1 \u2020Socher et al. (2013) 90.4 Zhu et al. (2013) 90.2 90.7 90.4 Mi and Huang (2015) 90.7 90.9 90.8 \u2020Watanabe and Sumita (2015) 90.", "startOffset": 2, "endOffset": 189}, {"referenceID": 5, "context": "2 Carreras et al. (2008) 90.7 91.4 91.1 Shindo et al. (2012) 91.1 \u2020Socher et al. (2013) 90.4 Zhu et al. (2013) 90.2 90.7 90.4 Mi and Huang (2015) 90.7 90.9 90.8 \u2020Watanabe and Sumita (2015) 90.7 Thang et al. (2015) (A*) 90.", "startOffset": 2, "endOffset": 214}, {"referenceID": 5, "context": "2 Carreras et al. (2008) 90.7 91.4 91.1 Shindo et al. (2012) 91.1 \u2020Socher et al. (2013) 90.4 Zhu et al. (2013) 90.2 90.7 90.4 Mi and Huang (2015) 90.7 90.9 90.8 \u2020Watanabe and Sumita (2015) 90.7 Thang et al. (2015) (A*) 90.9 91.2 91.1 \u2020*Dyer et al. (2016) (discrim.", "startOffset": 2, "endOffset": 255}, {"referenceID": 5, "context": "2 Carreras et al. (2008) 90.7 91.4 91.1 Shindo et al. (2012) 91.1 \u2020Socher et al. (2013) 90.4 Zhu et al. (2013) 90.2 90.7 90.4 Mi and Huang (2015) 90.7 90.9 90.8 \u2020Watanabe and Sumita (2015) 90.7 Thang et al. (2015) (A*) 90.9 91.2 91.1 \u2020*Dyer et al. (2016) (discrim.) 89.8 \u2020*Cross and Huang (2016) 90.", "startOffset": 2, "endOffset": 296}, {"referenceID": 10, "context": "External/Reranking/Combo \u2020Henderson (2004) (rerank) 89.", "startOffset": 26, "endOffset": 43}, {"referenceID": 10, "context": "External/Reranking/Combo \u2020Henderson (2004) (rerank) 89.8 90.4 90.1 McClosky et al. (2006) 92.", "startOffset": 26, "endOffset": 90}, {"referenceID": 10, "context": "External/Reranking/Combo \u2020Henderson (2004) (rerank) 89.8 90.4 90.1 McClosky et al. (2006) 92.2 92.6 92.4 Zhu et al. (2013) (semi) 91.", "startOffset": 26, "endOffset": 123}, {"referenceID": 10, "context": "External/Reranking/Combo \u2020Henderson (2004) (rerank) 89.8 90.4 90.1 McClosky et al. (2006) 92.2 92.6 92.4 Zhu et al. (2013) (semi) 91.1 91.5 91.3 Huang (2008) (forest) 91.", "startOffset": 26, "endOffset": 158}, {"referenceID": 10, "context": "External/Reranking/Combo \u2020Henderson (2004) (rerank) 89.8 90.4 90.1 McClosky et al. (2006) 92.2 92.6 92.4 Zhu et al. (2013) (semi) 91.1 91.5 91.3 Huang (2008) (forest) 91.7 \u2020Vinyals et al. (2015) (WSJ)\u2021 90.", "startOffset": 26, "endOffset": 195}, {"referenceID": 10, "context": "External/Reranking/Combo \u2020Henderson (2004) (rerank) 89.8 90.4 90.1 McClosky et al. (2006) 92.2 92.6 92.4 Zhu et al. (2013) (semi) 91.1 91.5 91.3 Huang (2008) (forest) 91.7 \u2020Vinyals et al. (2015) (WSJ)\u2021 90.5 \u2020Vinyals et al. (2015) (semi) 92.", "startOffset": 26, "endOffset": 230}, {"referenceID": 10, "context": "External/Reranking/Combo \u2020Henderson (2004) (rerank) 89.8 90.4 90.1 McClosky et al. (2006) 92.2 92.6 92.4 Zhu et al. (2013) (semi) 91.1 91.5 91.3 Huang (2008) (forest) 91.7 \u2020Vinyals et al. (2015) (WSJ)\u2021 90.5 \u2020Vinyals et al. (2015) (semi) 92.8 \u2020Durrett and Klein (2015)\u2021 91.", "startOffset": 26, "endOffset": 268}, {"referenceID": 8, "context": "1 \u2020Dyer et al. (2016) (gen.", "startOffset": 3, "endOffset": 22}, {"referenceID": 4, "context": "Parser LR LP F1 Bj\u00f6rkelund et al. (2014)\u2217,\u2021 82.", "startOffset": 16, "endOffset": 41}, {"referenceID": 4, "context": "Parser LR LP F1 Bj\u00f6rkelund et al. (2014)\u2217,\u2021 82.53 Durrett and Klein (2015)\u2021 81.", "startOffset": 16, "endOffset": 75}, {"referenceID": 4, "context": "Parser LR LP F1 Bj\u00f6rkelund et al. (2014)\u2217,\u2021 82.53 Durrett and Klein (2015)\u2021 81.25 Coavoux and Crabb\u00e9 (2016) 80.", "startOffset": 16, "endOffset": 108}, {"referenceID": 24, "context": "data set (Seddah et al., 2014).", "startOffset": 9, "endOffset": 30}, {"referenceID": 4, "context": "We achieve state-of-the-art results even in comparison to Bj\u00f6rkelund et al. (2014), which utilized both external data and reranking in achieving the best results in the SPMRL 2014 shared task.", "startOffset": 58, "endOffset": 83}, {"referenceID": 28, "context": "For these experiments, we performed very little hyperparameter tuning, due to time and resource contraints. We have every reason to believe that performance could be improved still further with such techniques as random restarts, larger hidden layers, external embeddings, and hyperparameter grid search, as demonstrated by Weiss et al. (2015). We also note that while our parser is very accurate even with greedy decoding, the model is easily adaptable for beam search, particularly since the parsing system already uses a fixed number of actions.", "startOffset": 45, "endOffset": 344}, {"referenceID": 28, "context": "For these experiments, we performed very little hyperparameter tuning, due to time and resource contraints. We have every reason to believe that performance could be improved still further with such techniques as random restarts, larger hidden layers, external embeddings, and hyperparameter grid search, as demonstrated by Weiss et al. (2015). We also note that while our parser is very accurate even with greedy decoding, the model is easily adaptable for beam search, particularly since the parsing system already uses a fixed number of actions. Beam search could also be made considerably more efficient by caching post-hidden-layer feature components for sentence spans, essentially using the precomputation trick described by Chen and Manning (2014), but on a per-sentence basis.", "startOffset": 45, "endOffset": 756}, {"referenceID": 16, "context": "Dynamic programming (Huang and Sagae, 2010) could be especially powerful in this context given the very simple feature representation used by our parser, as noted also by Kiperwasser and Goldberg (2016b).", "startOffset": 21, "endOffset": 204}], "year": 2016, "abstractText": "Parsing accuracy using efficient greedy transition systems has improved dramatically in recent years thanks to neural networks. Despite striking results in dependency parsing, however, neural models have not surpassed stateof-the-art approaches in constituency parsing. To remedy this, we introduce a new shiftreduce system whose stack contains merely sentence spans, represented by a bare minimum of LSTM features. We also design the first provably optimal dynamic oracle for constituency parsing, which runs in amortized O(1) time, compared to O(n) oracles for standard dependency parsing. Training with this oracle, we achieve the best F1 scores on both English and French of any parser that does not use reranking or external data.", "creator": "LaTeX with hyperref package"}}}