{"id": "1612.06778", "review": {"conference": "EMNLP", "VERSION": "v1", "DATE_OF_SUBMISSION": "20-Dec-2016", "title": "SCDV : Sparse Composite Document Vectors using soft clustering over distributional representations", "abstract": "In this work, we present a modified feature formation technique - graded-weighted Bag of Word Vectors (gwBoWV) by Vivek Gupta, 2016 for faster and better composite document feature representation. We propose a very simple feature construction algorithm that potentially overcomes many weaknesses in current distributional vector representations and other composite document representation methods widely used for text representation. Through extensive experiments on multi-class classification on 20newsgroup dataset and multi-label text classification on Reuters-21578, we achieve better performance results and also a significant reduction in training and prediction time compared to composite document representation methods gwBoWV and TWE Liu et al., 2015b.", "histories": [["v1", "Tue, 20 Dec 2016 17:38:57 GMT  (375kb,D)", "http://arxiv.org/abs/1612.06778v1", "Submitted to proceedings of EACL 2017"], ["v2", "Sun, 8 Jan 2017 17:26:54 GMT  (376kb,D)", "http://arxiv.org/abs/1612.06778v2", "Submitted to proceedings of EACL 2017. Update : Added results of SDV with SGNS trained word-vectors (SOA performance)"], ["v3", "Fri, 12 May 2017 09:48:04 GMT  (3660kb,D)", "http://arxiv.org/abs/1612.06778v3", "10 pages, 5 figures. Update: Added results on Information Retrieval and Topic Coherence with Discussion"]], "COMMENTS": "Submitted to proceedings of EACL 2017", "reviews": [], "SUBJECTS": "cs.CL", "authors": ["dheeraj mekala", "vivek gupta", "bhargavi paranjape", "harish karnick"], "accepted": true, "id": "1612.06778"}, "pdf": {"name": "1612.06778.pdf", "metadata": {"source": "CRF", "title": "Text classification with sparse composite document vectors", "authors": ["Dheeraj Mekala", "Vivek Gupta", "Harish Karnick"], "emails": ["dheerajm@iitk.ac.in", "t-vigu@microsoft.com", "hk@iitk.ac.in"], "sections": [{"heading": "1 Introduction", "text": "Text classification and text clustering are widely used in various tasks of gathering information and processing natural language (Moulinier et al., 1996) Various machine learning algorithms are used to perform these tasks, requiring that text data be represented as a floating-point vector of fixed dimension D. Some of the most commonly used techniques are described in the next section along with their problems."}, {"heading": "2 Related Work", "text": "Le. and Mikolov et. al. (Le and Mikolov, 2014) proposed two models for the distribution representation of documents as fixed-dimensional vectors * Represents the same contribution submitted to the procedures of EACL 2017 under the name Distributed Memory Model Paragraph Vectors (PV-DM) (Le and Mikolov, 2014) and Distributed BoW Paragraph Vectors (PV-DBoW) (Le and Mikolov, 2014). However, they do not perform particularly well in document categorization for several reasons, as indicated in (Vivek Gupta, 2016)."}, {"heading": "2.1 Composite Document Representation", "text": "Recently, great efforts have been made to create a document vector from word vectors. Some of the most important methods are described below:"}, {"heading": "2.1.1 Weighted Average Embedding", "text": "Mukerjee et al. (Pranjal Singh, 2015) proposed a weighted average composite document vector, in which vectors for words appearing in the document are added after they are weighted with idf values from the training set (Robertson, 2004). This method attempts to measure the relative meaning of words by different weights.This method assumes that all words within a document have the same semantic theme. Intuitively, a paragraph often contains words coming from several semantically different topics."}, {"heading": "2.1.2 Topical Word Embedding", "text": "(Liu et al., 2015b) proposed three novel composite document representations called Topical word embeddding (TWE-1, TWE-2, and TWE-3). For each word in the vocabulary, word-topic assignments are obtained by Latent Dirichlet Allocation (Blei et al., 2003). TWE-1 learns word and topic-embedding by treating each topic as a pseudo-word, and builds the thematic embedding for each word-topic assignment. TWE-2 learns topical word-embedding of each word-topic assignment directly by treating each word-topic pair as a pseudo-word. For each word and topic, TWE-3 builds different word embedding for the topic and word-topic separately, and for each word-topic assignment Xiv: 161 2.06 778v 1 [cs.C L] 20 Dec 201 6signment, the corresponding word-embedding and topic-embedding are."}, {"heading": "2.1.3 Graded weighted Bag of Word Vectors", "text": "(Vivek Gupta, 2016) presents a method for creating a composite document vector using word vectors and tf-idf values called Graded Weighted Bag of Words Vector (gwBoWV). gwBoWV represents each document by a vector of the dimension D = K \u0445 d + K, where K represents the number of clusters, d is the dimension of word vectors. The basic idea is that semantically different words belong to different clusters and their word vectors should not be averaged, i.e. concatenated. gwBoWV also concatenates the inverse cluster frequency of each cluster (icf), which is calculated by idf values of all words in a cluster in order to capture the meaning of words in documents. (Vivek Gupta, 2016) shows that gwBoWV paragraph vector models on a hierarchical product classification task.gwV do not require high-dimensional computational efficiency and thus high-efficiency."}, {"heading": "3 Sparse Document Vectors", "text": "rf\u00fc ide rf\u00fc the rf for the rf for the rf for the rf for the rf for the rf for the rf for the rf for the rf for the rf for the rf for the rf for the rf for the rf for the rf for the rf for the rf for the rf for the rf for the rf for the rf for the rf for the rf for the rf for the rf for the rf for the rf for the rf for the rf for the rf for the rf for the rf for the rf for the rf for the rf for the rf for the rf for the rf for the rf for the rf for the rf for the rf for the rf for the rf for the rf for the rf for the rf for the rf for the rf for the rf for the rf for the rf for the rf for the rf for the rf for the rf for the rf for the rf for the rf for the rf for the rf for the rf for the rf for the rf for the rf for the rf for the rf for the rf for the rf for the rf for the rf for the rf for the rf for the rf for the rf for the rf for the rf for the rf for the rf for the rf for the rf for the rf for the rf for the rf for the rf for the rf for the rf for the rf for the rf for the rf for the rf for the rf for the rf for the rf for the rf for the rf for the rf for the rf for the rf for the rf for the rf for the rf for the rf for the rf for the rf for the rf for the rf for the rf for the rf for the rf for the rf for the rf for the rf for the rf for the rf for the rf for the rf for the rf for the rf for the rf for the rf for the rf for the rf for the rf for the rf for the rf"}, {"heading": "4 Experiments", "text": "We perform several experiments to show the effectiveness of our representation for multiclass and multi-class text classification. We perform the multi-class experiments for 20NewsGroup Dataset (Lang,) and multi-class classification experiments for Reuters-21578 Dataset (Lewis,). Experimental conditions are as follows: Pre-processing: Stop word removal; Word vector dimension: 200; GMM components: 60 with the same spherical covariance matrix for all components; Classified multi-label: Logistic regression, OneVsRest setting; Platform: Intel (R) Xeon (R) CPU E5-2670 v2 @ 2.50 GHz, 40 working cores, 128GB RAM with Linux Ubuntu 14.4, use of multiple cores for a Vsrest vsrest classifier, Word2Vector vvec, and Doc2Vec training terms in all the following cases we consider BO.04W only for a BO.WE classification."}, {"heading": "4.1 Dataset Description", "text": "Multi-class: The training: Split the test samples into the 20NewsGroup dataset is 11314: 7532. Multi-label: The training: Split the test samples into the Reuters 21578 dataset is 13734: 5887. We use the script provided by Eustache 2 to pre-process the Reuters 21578 dataset. The dataset contains 5 main categories for a total of 445 categories. Each article is assigned to several categories, i.e. a multi-label classification."}, {"heading": "5 Results", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "5.1 Multi-class classification", "text": "Table 1 shows results for 20Newsgroup and Table 2 shows the comparison between SDV and NTSG, the current state of the art on the dataset 20Newsgroup. Compared to TWE, SDV succeeds in significantly reducing training and prediction times. Table 3 shows a comparison of training and prediction times between gwBoWV, SDV and TWE models. Table 4 shows the room comparison between gwBoWV1https: / / dheeraj7596.github.io / SDV / 2 https: / / gist.github.com / herrfz / 7967781and SDV models. The pictorial comparison between training and prediction times is shown in Figure 3 and 4.We find that SDV sales vectors, average word vectors, tf-weighted average word vectors and SDV models perform significantly faster than STSG models."}, {"heading": "5.2 Multi-label classification", "text": "We evaluate performance using Precision @ K, nDCG @ k, Coverage error, Label ranking average precision score (LRAPS), weighted F-measurement. The first two metrics come from the Extreme learning repository (Bhatia et al.,) and the next four metrics are defined in Scikit-Learn multilabel loss function class (Pedregosa et al., 2011). Since we randomly divide the data into train sets and test sets, the weighted F-measurement is a suitable metric for multi-label classification as it also takes into account distortions in label samples. Tables 5 and 6 show the evaluation results of the multi-label text classification on Reuters-21578 datasets."}, {"heading": "6 Time Complexity Analysis", "text": "W = Vocabulary Size, N = Number of Documents, T = Number of Topics or Clusters, C = Window Size, M = Body Length, KW = Word Vector Length, KT = Theme Vector Length, I = Number of Theme Modeling Equipment, D = Characteristic Vector Dimension. LDA Time Complexity: O (W 2NT) GMM Time Complexity: O (NT 2D). Since W 2 > > TD, i.e. Better Time Complexity. Detailed Time Complexity Comparison is shown in Table 7."}, {"heading": "7 Conclusion", "text": "In this article, we modified gwBoWV and reduced the total time of the feature vector calculation by 8.5 x, the prediction time by 20 x and the space by 4 x for 20 newsgroup multi-class data sets. We experimentally showed that our model outperforms TWE-1 in the multi-class classification of 20 newsgroup data sets and performs as well as NTSG (accuracy). Compared to TWE-1 and NTSG, our model is 6 x faster in the feature vector calculation and 3 x in the test class prediction of 20 newsgroups. In the multi-label classification at Reuters, our model performs better on every metric with reasonable margins. Overall, we improved gwBoWV by making it simple and efficient and achieving better results for standard multi-class and multi-label data sets."}], "references": [{"title": "Latent dirichlet allocation", "author": ["Blei et al.2003] David M. Blei", "Andrew Y. Ng", "Michael I. Jordan"], "venue": "J. Mach. Learn. Res.,", "citeRegEx": "Blei et al\\.,? \\Q2003\\E", "shortCiteRegEx": "Blei et al\\.", "year": 2003}, {"title": "Improving word representations via global context and multiple word prototypes", "author": ["Huang et al.2012] Eric Huang", "Richard Socher", "Christopher Manning", "Andrew Ng"], "venue": "In Proceedings of the 50th Annual Meeting of the Association", "citeRegEx": "Huang et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Huang et al\\.", "year": 2012}, {"title": "Distributed representations of sentences and documents. arXiv preprint arXiv:1405.4053", "author": ["Le", "Mikolov2014] Quoc V Le", "Tomas Mikolov"], "venue": null, "citeRegEx": "Le et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Le et al\\.", "year": 2014}, {"title": "Learning context-sensitive word embeddings with neural tensor skip-gram model", "author": ["Liu et al.2015a] Pengfei Liu", "Xipeng Qiu", "Xuanjing Huang"], "venue": "In Proceedings of the Twenty-Fourth International Joint Conference on Artificial Intelligence", "citeRegEx": "Liu et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Liu et al\\.", "year": 2015}, {"title": "Topical word embeddings", "author": ["Liu et al.2015b] Yang Liu", "Zhiyuan Liu", "Tat-Seng Chua", "Maosong Sun"], "venue": null, "citeRegEx": "Liu et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Liu et al\\.", "year": 2015}, {"title": "Text categorization: a symbolic approach", "author": ["Gailius Raskinis", "J Ganascia"], "venue": "In proceedings of the fifth annual symposium on document analysis and information retrieval,", "citeRegEx": "Moulinier et al\\.,? \\Q1996\\E", "shortCiteRegEx": "Moulinier et al\\.", "year": 1996}, {"title": "Words are not equal: Graded weighting model for building composite document vectors", "author": [], "venue": "In Proceedings of the twelfth International Conference on Natural Language Processing (ICON-", "citeRegEx": "Singh.,? \\Q2015\\E", "shortCiteRegEx": "Singh.", "year": 2015}, {"title": "Software Framework for Topic Modelling with Large Corpora", "author": ["\u0158eh\u016f\u0159ek", "Sojka2010] Radim \u0158eh\u016f\u0159ek", "Petr Sojka"], "venue": "In Proceedings of the LREC 2010 Workshop on New Challenges for NLP Frameworks,", "citeRegEx": "\u0158eh\u016f\u0159ek et al\\.,? \\Q2010\\E", "shortCiteRegEx": "\u0158eh\u016f\u0159ek et al\\.", "year": 2010}, {"title": "Understanding inverse document frequency: on theoretical arguments for idf", "author": ["Stephen Robertson"], "venue": "Journal of documentation,", "citeRegEx": "Robertson.,? \\Q2004\\E", "shortCiteRegEx": "Robertson.", "year": 2004}, {"title": "Product classification in e-commerce using distributional semantics", "author": ["Harish Karnick"], "venue": "In Proceedings of COLING 2016,", "citeRegEx": "Gupta and Karnick.,? \\Q2016\\E", "shortCiteRegEx": "Gupta and Karnick.", "year": 2016}], "referenceMentions": [{"referenceID": 5, "context": "Text classification and text clustering are widely used in various information retrieval and natural language processing tasks (Moulinier et al., 1996).", "startOffset": 127, "endOffset": 151}, {"referenceID": 8, "context": "(Pranjal Singh, 2015) proposed a weighted average composite document vectors in which vectors for words appearing in the document are added after weighting them with idf (Robertson, 2004) values from the training set.", "startOffset": 170, "endOffset": 187}, {"referenceID": 0, "context": "For each word in the vocabulary, word-topic assignments are obtained through Latent Dirichlet Allocation (Blei et al., 2003).", "startOffset": 105, "endOffset": 124}, {"referenceID": 1, "context": "Lots of work had been done earlier to handle polysemy for word embeddings (Huang et al., 2012) but not on its effects in document representation.", "startOffset": 74, "endOffset": 94}], "year": 2017, "abstractText": "In this work, we present a modified feature formation technique gradedweighted Bag of Word Vectors (gwBoWV) by (Vivek Gupta, 2016) for faster and better composite document feature representation. We propose a very simple feature construction algorithm that potentially overcomes many weaknesses in current distributional vector representations and other composite document representation methods widely used for text representation. Through extensive experiments on multi-class classification on 20newsgroup dataset and multi-label text classification on Reuters-21578, we achieve better performance results and also significant reduction in training and prediction time compared to composite document representation methods gwBoWV and TWE(Liu et al., 2015b).", "creator": "LaTeX with hyperref package"}}}