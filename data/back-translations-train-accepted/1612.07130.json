{"id": "1612.07130", "review": {"conference": "acl", "VERSION": "v1", "DATE_OF_SUBMISSION": "21-Dec-2016", "title": "Sparse Coding of Neural Word Embeddings for Multilingual Sequence Labeling", "abstract": "In this paper we propose and carefully evaluate a sequence labeling framework which solely utilizes sparse indicator features derived from dense distributed word representations. The proposed model obtains (near) state-of-the art performance for both part-of-speech tagging and named entity recognition for a variety of languages. Our model relies only on a few thousand sparse coding-derived features, without applying any modification of the word representations employed for the different tasks. The proposed model has favorable generalization properties as it retains over 89.8% of its average POS tagging accuracy when trained at 1.2% of the total available training data, i.e.~150 sentences per language.", "histories": [["v1", "Wed, 21 Dec 2016 14:17:53 GMT  (150kb,D)", "http://arxiv.org/abs/1612.07130v1", null]], "reviews": [], "SUBJECTS": "cs.CL", "authors": ["g\\'abor berend"], "accepted": true, "id": "1612.07130"}, "pdf": {"name": "1612.07130.pdf", "metadata": {"source": "CRF", "title": "Sparse Coding of Neural Word Embeddings for Multilingual Sequence Labeling", "authors": ["G\u00e1bor Berend"], "emails": ["berendg@inf.u-szeged.hu"], "sections": [{"heading": "1 Introduction", "text": "Determining the linguistic structure of natural language texts, based on rich handcrafted features, has a long history in natural language processing. Traditional approaches have mostly focused on building linguistic analyzers for a particular type of analysis, which often result in the inclusion of extensive linguistic and / or domain-specific knowledge for the definition of the functional space. Consequently, traditional models easily become language and / or tasks that lead to inappropriate generalization characteristics. A new line of research has emerged recently aimed at developing more general models that require far less feature engineering or none at all. These advances in natural language processing, which are being promoted by Bengio et al, are being followed by laboratories, followed by Collobert and Weston."}, {"heading": "2 Related work", "text": "The research presented in this paper is based on distributed word representations (Al-Rfou et al., 2013) and dictionary learning for sparse coding (Mairal et al., 2010) and also shows a great similarity to (Faruqui et al., 2015)."}, {"heading": "2.1 Distributed word representations", "text": "In fact, it is not as if we were able to understand the world and understand it as it is. (...) It is not as if we understood the world. (...) It is not as if we understood it. (...) It is as if we understood the world. (...) It is as if we understood the world. (...) It is not as if we understood it. (...) It is as if we understood it. \"(...) It is not as if we understood it. (...) It is not as if we understood it.\" (...) It is as if we understood it. \"(...) It is as if we understood it.\""}, {"heading": "2.2 Sparse coding", "text": "The general objective of sparse coding is to express signals in the form of a sparse linear combination of base vectors, and the task of finding a suitable set of base vectors is referred to as the dictionary learning problem (Mairal et al., 2010). In general, the task is to find the formal problem of minimizing linear minimum squares with the formal D-C value, \u03b112n n n \u00b2 i = 1 value and \u03b1-Rm \u00b2 value (1), given a data matrix with its ith column xi representing the ith-k dimensional signal, where C is the convex group of linear minimum squares consisting of column vectors having a '2 standard (matrix D acts as a divided dictionary across the signals, and the columns of the hierarchy of sparse hierarchy do not arise."}, {"heading": "3 Sequence labeling framework", "text": "Since our goal is to measure the effectiveness of sparse word embedding alone, we do not apply attributes based on gazettes, uppercase patterns, or character suffixes. As previously described, word embedding methods transform a one-dimensional representation of words into a dense embedding of much smaller dimensions k.In our work, instead of using low-dimensional dense word embedding, we use a dictatorial learning approach to obtain sparse encodings for the embedded word representations. Formally, the search matrix of W containing the embedding vectors is D, Rk x x."}, {"heading": "4 Experiments", "text": "For the sparse coding of distributed word representations, we rely primarily on the SPArse Modeling Software1 (SPAMS) (Mairal et al., 2010). For dictionary learning, as formulated in Equation 1, one should choose m and \u03bb, controlling the number of base vectors and the regulatory coefficient that influences the sparseness of \u03b1. Starting from m = 256 and doubling it at each iteration, our preliminary investigations showed a steady increase in the usefulness of sparse word representations as a function of m, with m set to 1024, which is why we set m to this value for further experiments."}, {"heading": "4.1 Baseline methods", "text": "So we have defined a linear chain of CRF that is based on traits that are based on the traits of the Brown cluster as one of our basic concepts. Since Brown cluster defines a hierarchical clustering of words, we can easily act as traits. So we are dealing with a linear chain of traits that are based on traits from the Brown cluster as one of our basic concepts. We are dealing with a hierarchical clustering of words, cluster supersets. We are dealing with the commonly used approach of generating traits of length p (p)."}, {"heading": "4.2 POS tagging experiments", "text": "Although one can reasonably assume that languages share a common rough set of linguistic categories, linguistic resources used to have their own notations for parts-of-speech tags. The first notable attempt to canonize existing multiple tag sets is Google's Universal Part-of-Speech Tags, introduced by Petrov et al. (2012), which argue that the POS tags of various tagging schemes can also be mapped to 12 language-independent part-of-speech tags. There is a recent initiative of Universal Dependencies (UD) (Nivre, 2015) aimed at providing a uniform notation for several linguistic phenomena, including part-of-speech tags. The POS tag set proposed for UD includes 17 partially overlapping categories to those defined by Petrov et al. (2012)."}, {"heading": "4.2.1 Experiments using CoNLL 2006/07 data", "text": "We use 12 treebanks in CoNLL-X format from the CoNLL-2006 / 07 (Buchholz and Marsi, 2006; Nivre et al., 2007). The full list of treebanks included in our experiments is given in Table 2.We rely on the official scripts used by Petrov et al. (2012) 5 for mapping the treebank specifications to make the results comparable. For our experiments, we used the original CoNLL-X languages that we use to map the different languages of treebanks.A key factor for the efficiency of our proposed model resides in the coverage of word embeddings, i.e. the percentage of tokens / word forms with distributed representations is determined for the distributed languages. Figure 1 shows this coverage calculated results for the different languages."}, {"heading": "4.2.2 Experiments using UD treebanks", "text": "In fact, most of them are able to move, to move, to move and to move, and that they are able to move, to move, to move, to move, to move and to move."}, {"heading": "4.3 Named entity recognition experiments", "text": "In addition to the POS labeling experiments, we investigated whether exactly the same features as for POS labeling can be used in another sequence labeling task, namely, entity recognition. To evaluate our approach, we obtained the English, Spanish and Dutch data sets from the 2002 and 2003 CoNLL collection tasks (Tjong Kim Sang, 2002; Tjong Kim Sang and De Meulder, 2003). We use the train test splits provided by the organizers and report our NER results using the F1 values, which are based on the official evaluation script of the CoNLL collection task. Similar to Collobert et al. (2011), we also apply the 17-day IOBES labeling scheme during training and conclusion. The best F1 values reported by Collobert et al. (2011) for English are the best values, without additional, unlabeled texts, to improve their language model."}, {"heading": "5 Conclusion", "text": "In this paper, we have shown that it is possible to train sequence models that are (nearly) state-of-the-art in a variety of languages for both POS and NER marking. Our approach does not require any word identity characteristics to work reliably, and it is possible to achieve comparable results with traditional feature-rich models. We have also demonstrated the advantageous generalization characteristics of our model, as it retains 89.8% of its original average POS marking accuracy when trained to only 1.2% of the total training sets available. As Mikolov et al. (2013b) pointed out, we believe that our proposed model could be used not only for multilingual but for cross-lingual language analysis settings. In fact, we consider studying its feasibility as our future work. Finally, we make the sparsely coded words available for embedding vectors to facilitate reproducibility and multilingual research."}, {"heading": "Acknowledgement", "text": "The author thanks the TACL editors and the anonymous reviewers for their valuable feedback and suggestions."}], "references": [{"title": "Floresta sint\u00e1(c)tica\u201d: a treebank for Portuguese", "author": ["Susana Afonso", "Eckhard Bick", "Renato Haber", "Diana Santos."], "venue": "Proceedings of the 3rd International Conference on Language Resources and Evaluation (LREC), pages 1698\u20131703, Las Palmas, Spain.", "citeRegEx": "Afonso et al\\.,? 2002", "shortCiteRegEx": "Afonso et al\\.", "year": 2002}, {"title": "Polyglot: Distributed word representations for multilingual nlp", "author": ["Rami Al-Rfou", "Bryan Perozzi", "Steven Skiena."], "venue": "Proceedings of the Seventeenth Conference on Computational Natural Language Learning, pages 183\u2013192, Sofia, Bulgaria, August. Association", "citeRegEx": "Al.Rfou et al\\.,? 2013", "shortCiteRegEx": "Al.Rfou et al\\.", "year": 2013}, {"title": "The annotation process in the Turkish treebank", "author": ["Nart B. Atalay", "Kemal Oflazer", "Bilge Say."], "venue": "In Proceedings of the 4th International Workshop on Linguistically Interpreted Corpora (LINC), Budapest, Hungary. Association for Computational Linguistics.", "citeRegEx": "Atalay et al\\.,? 2003", "shortCiteRegEx": "Atalay et al\\.", "year": 2003}, {"title": "Don\u2019t count, predict! a systematic comparison of context-counting vs", "author": ["Marco Baroni", "Georgiana Dinu", "Germ\u00e1n Kruszewski."], "venue": "context-predicting semantic vectors. In Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics", "citeRegEx": "Baroni et al\\.,? 2014", "shortCiteRegEx": "Baroni et al\\.", "year": 2014}, {"title": "A neural probabilistic language model", "author": ["Yoshua Bengio", "R\u00e9jean Ducharme", "Pascal Vincent", "Christian Janvin."], "venue": "J. Mach. Learn. Res., 3:1137\u20131155, March.", "citeRegEx": "Bengio et al\\.,? 2003", "shortCiteRegEx": "Bengio et al\\.", "year": 2003}, {"title": "The TIGER treebank", "author": ["Sabine Brants", "Stefanie Dipper", "Silvia Hansen", "Wolfgang Lezius", "George Smith."], "venue": "Proceedings of the Workshop on Treebanks and Linguistic Theories, Sozopol.", "citeRegEx": "Brants et al\\.,? 2002", "shortCiteRegEx": "Brants et al\\.", "year": 2002}, {"title": "Classbased n-gram models of natural language", "author": ["Peter F. Brown", "Peter V. deSouza", "Robert L. Mercer", "Vincent J. Della Pietra", "Jenifer C. Lai."], "venue": "Comput. Linguist., 18(4):467\u2013479, December.", "citeRegEx": "Brown et al\\.,? 1992", "shortCiteRegEx": "Brown et al\\.", "year": 1992}, {"title": "Conll-x shared task on multilingual dependency parsing", "author": ["Sabine Buchholz", "Erwin Marsi."], "venue": "Proceedings of the Tenth Conference on Computational Natural Language Learning, CoNLL-X \u201906, pages 149\u2013 164, Stroudsburg, PA, USA. Association for Compu-", "citeRegEx": "Buchholz and Marsi.,? 2006", "shortCiteRegEx": "Buchholz and Marsi.", "year": 2006}, {"title": "Compressing neural language models by sparse word representations", "author": ["Yunchuan Chen", "Lili Mou", "Yan Xu", "Ge Li", "Zhi Jin."], "venue": "Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 226\u2013235,", "citeRegEx": "Chen et al\\.,? 2016", "shortCiteRegEx": "Chen et al\\.", "year": 2016}, {"title": "A unified architecture for natural language processing: Deep neural networks with multitask learning", "author": ["Ronan Collobert", "Jason Weston."], "venue": "Proceedings of the 25th International Conference on Machine Learning, ICML \u201908, pages 160\u2013167, New York, NY,", "citeRegEx": "Collobert and Weston.,? 2008", "shortCiteRegEx": "Collobert and Weston.", "year": 2008}, {"title": "Natural language processing (almost) from scratch", "author": ["Ronan Collobert", "Jason Weston", "L\u00e9on Bottou", "Michael Karlen", "Koray Kavukcuoglu", "Pavel Kuksa."], "venue": "J. Mach. Learn. Res., 12:2493\u20132537, November.", "citeRegEx": "Collobert et al\\.,? 2011", "shortCiteRegEx": "Collobert et al\\.", "year": 2011}, {"title": "The Szeged treebank", "author": ["D\u00f3ra Csendes", "J\u00e1nos Csirik", "Tibor Gyim\u00f3thy", "Andr\u00e1s Kocsor."], "venue": "TSD, pages 123\u2013131.", "citeRegEx": "Csendes et al\\.,? 2005", "shortCiteRegEx": "Csendes et al\\.", "year": 2005}, {"title": "Tune your brown clustering, please", "author": ["Leon Derczynski", "Sean Chester", "Kenneth B\u00f8gh."], "venue": "Pro-", "citeRegEx": "Derczynski et al\\.,? 2015", "shortCiteRegEx": "Derczynski et al\\.", "year": 2015}, {"title": "Towards a Slovene dependency treebank", "author": ["Sa\u0161o D\u017eeroski", "Toma\u017e Erjavec", "Nina Ledinek", "Petr Pajas", "Zden\u011bk \u017dabokrtsk\u00fd", "Andreja \u017dele."], "venue": "Proceedings of the Fifth International Language Resources and Evaluation Conference, LREC 2006, pages 1388\u20131391, Gen-", "citeRegEx": "D\u017eeroski et al\\.,? 2006", "shortCiteRegEx": "D\u017eeroski et al\\.", "year": 2006}, {"title": "Sparse overcomplete word vector representations", "author": ["Manaal Faruqui", "Yulia Tsvetkov", "Dani Yogatama", "Chris Dyer", "Noah A. Smith."], "venue": "Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint", "citeRegEx": "Faruqui et al\\.,? 2015", "shortCiteRegEx": "Faruqui et al\\.", "year": 2015}, {"title": "Danish dependency treebank", "author": ["Matthias T. Kromann", "Line Mikkelsen", "Stine Kern Lynge"], "venue": null, "citeRegEx": "Kromann et al\\.,? \\Q2004\\E", "shortCiteRegEx": "Kromann et al\\.", "year": 2004}, {"title": "Conditional random fields: Probabilistic models for segmenting and labeling sequence data", "author": ["John D. Lafferty", "Andrew McCallum", "Fernando C.N. Pereira."], "venue": "Proceedings of the Eighteenth International Conference on Machine Learning, ICML \u201901, pages", "citeRegEx": "Lafferty et al\\.,? 2001", "shortCiteRegEx": "Lafferty et al\\.", "year": 2001}, {"title": "Word emdeddings through hellinger PCA", "author": ["R\u00e9mi Lebret", "Ronan Collobert."], "venue": "CoRR, abs/1312.5542.", "citeRegEx": "Lebret and Collobert.,? 2013", "shortCiteRegEx": "Lebret and Collobert.", "year": 2013}, {"title": "Rehabilitation of count-based models for word vector representations", "author": ["R\u00e9mi Lebret", "Ronan Collobert."], "venue": "CoRR, abs/1412.4930.", "citeRegEx": "Lebret and Collobert.,? 2014", "shortCiteRegEx": "Lebret and Collobert.", "year": 2014}, {"title": "Improving distributional similarity with lessons learned from word embeddings", "author": ["Omer Levy", "Yoav Goldberg", "Ido Dagan."], "venue": "TACL, 3:211\u2013225.", "citeRegEx": "Levy et al\\.,? 2015", "shortCiteRegEx": "Levy et al\\.", "year": 2015}, {"title": "Semi-supervised learning for natural language", "author": ["P. Liang."], "venue": "Master\u2019s thesis, Massachusetts Institute of Technology.", "citeRegEx": "Liang.,? 2005", "shortCiteRegEx": "Liang.", "year": 2005}, {"title": "Unsupervised pos induction with word embeddings", "author": ["Chu-Cheng Lin", "Waleed Ammar", "Chris Dyer", "Lori Levin."], "venue": "Proceedings of the 2015 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Tech-", "citeRegEx": "Lin et al\\.,? 2015", "shortCiteRegEx": "Lin et al\\.", "year": 2015}, {"title": "Online learning for matrix factorization and sparse coding", "author": ["Julien Mairal", "Francis Bach", "Jean Ponce", "Guillermo Sapiro."], "venue": "J. Mach. Learn. Res., 11:19\u201360, March.", "citeRegEx": "Mairal et al\\.,? 2010", "shortCiteRegEx": "Mairal et al\\.", "year": 2010}, {"title": "Building a large annotated corpus of english: The penn treebank", "author": ["Mitchell P. Marcus", "Beatrice Santorini", "Mary Ann Marcinkiewicz."], "venue": "Computational Linguistics, 19(2):313\u2013330.", "citeRegEx": "Marcus et al\\.,? 1993", "shortCiteRegEx": "Marcus et al\\.", "year": 1993}, {"title": "Efficient estimation of word representations in vector space", "author": ["Tomas Mikolov", "Kai Chen", "Greg Corrado", "Jeffrey Dean."], "venue": "CoRR, abs/1301.3781.", "citeRegEx": "Mikolov et al\\.,? 2013a", "shortCiteRegEx": "Mikolov et al\\.", "year": 2013}, {"title": "Exploiting similarities among languages for machine translation", "author": ["Tomas Mikolov", "Quoc V. Le", "Ilya Sutskever."], "venue": "CoRR, abs/1309.4168.", "citeRegEx": "Mikolov et al\\.,? 2013b", "shortCiteRegEx": "Mikolov et al\\.", "year": 2013}, {"title": "Distributed representations of words and phrases and their compositionality", "author": ["Tomas Mikolov", "Ilya Sutskever", "Kai Chen", "Greg Corrado", "Jeffrey Dean."], "venue": "CoRR, abs/1310.4546.", "citeRegEx": "Mikolov et al\\.,? 2013c", "shortCiteRegEx": "Mikolov et al\\.", "year": 2013}, {"title": "Learning word embeddings efficiently with noise-contrastive estimation", "author": ["Andriy Mnih", "Koray Kavukcuoglu."], "venue": "Advances in Neural Information Processing Systems (NIPS 2013).", "citeRegEx": "Mnih and Kavukcuoglu.,? 2013", "shortCiteRegEx": "Mnih and Kavukcuoglu.", "year": 2013}, {"title": "Building the Italian syntactic-semantic treebank", "author": ["zotto", "Nadia Mana", "Fabio Pianesi", "Rodolfo Delmonte"], "venue": "In Anne Abeill, editor, Building and using Parsed Corpora, Language and Speech series,", "citeRegEx": "zotto et al\\.,? \\Q2003\\E", "shortCiteRegEx": "zotto et al\\.", "year": 2003}, {"title": "Learning effective and interpretable semantic models using non-negative sparse embedding", "author": ["Brian Murphy", "Partha Talukdar", "Tom Mitchell."], "venue": "Proceedings of COLING 2012, pages 1933\u20131950, Mumbai, India, December. The COLING 2012 Organizing Commit-", "citeRegEx": "Murphy et al\\.,? 2012", "shortCiteRegEx": "Murphy et al\\.", "year": 2012}, {"title": "Talbanken05: A Swedish treebank with phrase structure and dependency annotation", "author": ["Joakim Nivre", "Jens Nilsson", "Johan Hall."], "venue": "Proceedings of the Fifth International Conference on Language Resources and Evaluation (LREC 2006), Genova, Italy.", "citeRegEx": "Nivre et al\\.,? 2006", "shortCiteRegEx": "Nivre et al\\.", "year": 2006}, {"title": "The CoNLL 2007 shared task on dependency parsing", "author": ["Joakim Nivre", "Johan Hall", "Sandra K\u00fcbler", "Ryan McDonald", "Jens Nilsson", "Sebastian Riedel", "Deniz Yuret."], "venue": "Proceedings of the CoNLL Shared Task Session of EMNLP-CoNLL 2007, pages 915\u2013932,", "citeRegEx": "Nivre et al\\.,? 2007", "shortCiteRegEx": "Nivre et al\\.", "year": 2007}, {"title": "Universal dependencies 1.2", "author": ["Aaron Smith", "Jan \u0160t\u011bp\u00e1nek", "Alane Suhr", "Zsolt Sz\u00e1nt\u00f3", "Takaaki Tanaka", "Reut Tsarfaty", "Sumire Uematsu", "Larraitz Uria", "Viktor Varga", "Veronika Vincze", "Zden\u011bk \u017dabokrtsk\u00fd", "Daniel Zeman", "Hanzhi Zhu"], "venue": null, "citeRegEx": "Smith et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Smith et al\\.", "year": 2015}, {"title": "Towards a universal grammar for natural language processing", "author": ["Joakim Nivre."], "venue": "Computational Linguistics and Intelligent Text Processing - 16th International Conference, CICLing 2015, Cairo, Egypt, April 14-20, 2015, Proceedings, Part I, pages 3\u201316.", "citeRegEx": "Nivre.,? 2015", "shortCiteRegEx": "Nivre.", "year": 2015}, {"title": "CRFsuite: a fast implementation of Conditional Random Fields (CRFs)", "author": ["Naoaki Okazaki"], "venue": null, "citeRegEx": "Okazaki.,? \\Q2007\\E", "shortCiteRegEx": "Okazaki.", "year": 2007}, {"title": "Improved part-of-speech tagging for online conversational text with word clusters", "author": ["Olutobi Owoputi", "Brendan O\u2019Connor", "Chris Dyer", "Kevin Gimpel", "Nathan Schneider", "Noah A. Smith"], "venue": "In Proceedings of the 2013 Conference of the North American Chap-", "citeRegEx": "Owoputi et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Owoputi et al\\.", "year": 2013}, {"title": "GloVe: Global vectors for word representation", "author": ["Jeffrey Pennington", "Richard Socher", "Christopher D. Manning."], "venue": "Proceedings of EMNLP.", "citeRegEx": "Pennington et al\\.,? 2014", "shortCiteRegEx": "Pennington et al\\.", "year": 2014}, {"title": "A universal part-of-speech tagset", "author": ["Slav Petrov", "Dipanjan Das", "Ryan McDonald."], "venue": "Nicoletta Calzolari (Conference Chair), Khalid Choukri, Thierry Declerck, Mehmet Uur Doan, Bente Maegaard, Joseph Mariani, Asuncion Moreno, Jan Odijk, and Stelios", "citeRegEx": "Petrov et al\\.,? 2012", "shortCiteRegEx": "Petrov et al\\.", "year": 2012}, {"title": "Multilingual part-of-speech tagging with bidirectional long short-term memory models and auxiliary loss", "author": ["Barbara Plank", "Anders S\u00f8gaard", "Yoav Goldberg."], "venue": "Proceedings of the 54th Annual Meet-", "citeRegEx": "Plank et al\\.,? 2016", "shortCiteRegEx": "Plank et al\\.", "year": 2016}, {"title": "Big data small data, in domain out-of domain, known word unknown word: The impact of word representations on sequence labelling tasks", "author": ["Lizhen Qu", "Gabriela Ferraro", "Liyuan Zhou", "Weiwei Hou", "Nathan Schneider", "Timothy Baldwin."], "venue": "Proceedings", "citeRegEx": "Qu et al\\.,? 2015", "shortCiteRegEx": "Qu et al\\.", "year": 2015}, {"title": "Design challenges and misconceptions in named entity recognition", "author": ["Lev Ratinov", "Dan Roth."], "venue": "Proceedings of the Thirteenth Conference on Computational Natural Language Learning, CoNLL \u201909, pages 147\u2013155, Stroudsburg, PA, USA. Association", "citeRegEx": "Ratinov and Roth.,? 2009", "shortCiteRegEx": "Ratinov and Roth.", "year": 2009}, {"title": "Extending the annotation of BulTreeBank: Phase 2", "author": ["Kiril Simov", "Petya Osenova."], "venue": "The Fourth Workshop on Treebanks and Linguistic Theories (TLT 2005), pages 173\u2013184, Barcelona, December.", "citeRegEx": "Simov and Osenova.,? 2005", "shortCiteRegEx": "Simov and Osenova.", "year": 2005}, {"title": "Simple semisupervised pos tagging", "author": ["Karl Stratos", "Michael Collins."], "venue": "Proceedings of the 1st Workshop on Vector Space Modeling for Natural Language Processing, pages 79\u201387, Denver, Colorado, June. Association for Computational Linguistics.", "citeRegEx": "Stratos and Collins.,? 2015", "shortCiteRegEx": "Stratos and Collins.", "year": 2015}, {"title": "Sparse word embeddings using `1 regularized online learning", "author": ["Fei Sun", "Jiafeng Guo", "Yanyan Lan", "Jun Xu", "Xueqi Cheng."], "venue": "Proceedings of the TwentyFifth International Joint Conference on Artificial Intelligence, pages 2915\u20132921.", "citeRegEx": "Sun et al\\.,? 2016", "shortCiteRegEx": "Sun et al\\.", "year": 2016}, {"title": "AnCora: Multilevel annotated corpora for Catalan and Spanish", "author": ["Mariona Taul\u00e9", "Maria Ant\u00f2nia Mart\u0131", "Marta Recasens"], "venue": null, "citeRegEx": "Taul\u00e9 et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Taul\u00e9 et al\\.", "year": 2008}, {"title": "Introduction to the conll-2003 shared task: Languageindependent named entity recognition", "author": ["Erik F. Tjong Kim Sang", "Fien De Meulder."], "venue": "Proceedings of the Seventh Conference on Natural Language Learning at HLT-NAACL 2003 - Volume 4, CONLL", "citeRegEx": "Sang and Meulder.,? 2003", "shortCiteRegEx": "Sang and Meulder.", "year": 2003}, {"title": "Introduction to the conll2002 shared task: Language-independent named entity recognition", "author": ["Erik F. Tjong Kim Sang."], "venue": "Proceedings of the 6th Conference on Natural Language Learning - Volume 20, COLING02, pages 1\u20134, Stroudsburg, PA, USA. Association for", "citeRegEx": "Sang.,? 2002", "shortCiteRegEx": "Sang.", "year": 2002}, {"title": "Word representations: A simple and general method for semi-supervised learning", "author": ["Joseph Turian", "Lev Ratinov", "Yoshua Bengio."], "venue": "Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, ACL \u201910, pages 384\u2013394, Strouds-", "citeRegEx": "Turian et al\\.,? 2010", "shortCiteRegEx": "Turian et al\\.", "year": 2010}, {"title": "Chapter 5", "author": ["Leonoor van der Beek", "Gosse Bouma", "Jan Daciuk", "Tanja Gaustad", "Robert Malouf", "Gertjan van Noord", "Robbert Prins", "Begoa Villada."], "venue": "the Alpino dependency treebank. In Algorithms for Linguistic Processing NWO PIONIER Progress Report, Gronin-", "citeRegEx": "Beek et al\\.,? 2002", "shortCiteRegEx": "Beek et al\\.", "year": 2002}, {"title": "Learning word representations with hierarchical sparse coding", "author": ["Dani Yogatama", "Manaal Faruqui", "Chris Dyer", "Noah A. Smith."], "venue": "Proceedings of ICML.", "citeRegEx": "Yogatama et al\\.,? 2015", "shortCiteRegEx": "Yogatama et al\\.", "year": 2015}], "referenceMentions": [{"referenceID": 14, "context": "Even though sparse coding has been utilized in NLP prior to us (Faruqui et al., 2015; Chen et al., 2016), to the best of our knowledge, we are the first to propose a sequence labeling framework incorporating it with the following contributions:", "startOffset": 63, "endOffset": 104}, {"referenceID": 8, "context": "Even though sparse coding has been utilized in NLP prior to us (Faruqui et al., 2015; Chen et al., 2016), to the best of our knowledge, we are the first to propose a sequence labeling framework incorporating it with the following contributions:", "startOffset": 63, "endOffset": 104}, {"referenceID": 4, "context": "These advancements of natural language processing, pioneered by Bengio et al. (2003), followed by Collobert and Weston (2008), Collobert et al.", "startOffset": 64, "endOffset": 85}, {"referenceID": 4, "context": "These advancements of natural language processing, pioneered by Bengio et al. (2003), followed by Collobert and Weston (2008), Collobert et al.", "startOffset": 64, "endOffset": 126}, {"referenceID": 4, "context": "These advancements of natural language processing, pioneered by Bengio et al. (2003), followed by Collobert and Weston (2008), Collobert et al. (2011), Mikolov et al.", "startOffset": 64, "endOffset": 151}, {"referenceID": 4, "context": "These advancements of natural language processing, pioneered by Bengio et al. (2003), followed by Collobert and Weston (2008), Collobert et al. (2011), Mikolov et al. (2013a) among others, employ a different philosophy.", "startOffset": 64, "endOffset": 175}, {"referenceID": 1, "context": "The line of research introduced in this paper relies on distributed word representations (Al-Rfou et al., 2013) and dictionary learning for sparse coding (Mairal et al.", "startOffset": 89, "endOffset": 111}, {"referenceID": 22, "context": ", 2013) and dictionary learning for sparse coding (Mairal et al., 2010) and also shows close resemblance to (Faruqui et al.", "startOffset": 50, "endOffset": 71}, {"referenceID": 14, "context": ", 2010) and also shows close resemblance to (Faruqui et al., 2015).", "startOffset": 44, "endOffset": 66}, {"referenceID": 4, "context": "the word embeddings) get determined, we could distinguish a plethora of approaches (Bengio et al., 2003; Lebret and Collobert, 2013; Mnih and Kavukcuoglu, 2013; Collobert and Weston, 2008; Mikolov et al., 2013a; Pennington et al., 2014).", "startOffset": 83, "endOffset": 236}, {"referenceID": 17, "context": "the word embeddings) get determined, we could distinguish a plethora of approaches (Bengio et al., 2003; Lebret and Collobert, 2013; Mnih and Kavukcuoglu, 2013; Collobert and Weston, 2008; Mikolov et al., 2013a; Pennington et al., 2014).", "startOffset": 83, "endOffset": 236}, {"referenceID": 27, "context": "the word embeddings) get determined, we could distinguish a plethora of approaches (Bengio et al., 2003; Lebret and Collobert, 2013; Mnih and Kavukcuoglu, 2013; Collobert and Weston, 2008; Mikolov et al., 2013a; Pennington et al., 2014).", "startOffset": 83, "endOffset": 236}, {"referenceID": 9, "context": "the word embeddings) get determined, we could distinguish a plethora of approaches (Bengio et al., 2003; Lebret and Collobert, 2013; Mnih and Kavukcuoglu, 2013; Collobert and Weston, 2008; Mikolov et al., 2013a; Pennington et al., 2014).", "startOffset": 83, "endOffset": 236}, {"referenceID": 24, "context": "the word embeddings) get determined, we could distinguish a plethora of approaches (Bengio et al., 2003; Lebret and Collobert, 2013; Mnih and Kavukcuoglu, 2013; Collobert and Weston, 2008; Mikolov et al., 2013a; Pennington et al., 2014).", "startOffset": 83, "endOffset": 236}, {"referenceID": 36, "context": "the word embeddings) get determined, we could distinguish a plethora of approaches (Bengio et al., 2003; Lebret and Collobert, 2013; Mnih and Kavukcuoglu, 2013; Collobert and Weston, 2008; Mikolov et al., 2013a; Pennington et al., 2014).", "startOffset": 83, "endOffset": 236}, {"referenceID": 24, "context": "Prediction-based distributed word embedding approaches such as word2vec (Mikolov et al., 2013a) have been conjectured to have superior performance over count-based word representations (Baroni et al.", "startOffset": 72, "endOffset": 95}, {"referenceID": 3, "context": ", 2013a) have been conjectured to have superior performance over count-based word representations (Baroni et al., 2014).", "startOffset": 98, "endOffset": 119}, {"referenceID": 3, "context": ", 2013a) have been conjectured to have superior performance over count-based word representations (Baroni et al., 2014). However, as Lebret and Collobert (2014), Levy et al.", "startOffset": 99, "endOffset": 161}, {"referenceID": 3, "context": ", 2013a) have been conjectured to have superior performance over count-based word representations (Baroni et al., 2014). However, as Lebret and Collobert (2014), Levy et al. (2015) and Qu et al.", "startOffset": 99, "endOffset": 181}, {"referenceID": 3, "context": ", 2013a) have been conjectured to have superior performance over count-based word representations (Baroni et al., 2014). However, as Lebret and Collobert (2014), Levy et al. (2015) and Qu et al. (2015) point out count-based distributional models can perform on par with prediction-based distributed word embedding models.", "startOffset": 99, "endOffset": 202}, {"referenceID": 3, "context": ", 2013a) have been conjectured to have superior performance over count-based word representations (Baroni et al., 2014). However, as Lebret and Collobert (2014), Levy et al. (2015) and Qu et al. (2015) point out count-based distributional models can perform on par with prediction-based distributed word embedding models. Levy et al. (2015) illustrate that the effectiveness of neural word embeddings largely depend on the selection of model hyperparameters and other design choices.", "startOffset": 99, "endOffset": 341}, {"referenceID": 1, "context": "According to these findings, in order to avoid any hassles of tuning the hyperparameters of the word embedding model employed, we rather primarily use the publicly available pre-trained polyglot word embeddings of (Al-Rfou et al., 2013) without any task specific modification for our experiments.", "startOffset": 214, "endOffset": 236}, {"referenceID": 1, "context": "According to these findings, in order to avoid any hassles of tuning the hyperparameters of the word embedding model employed, we rather primarily use the publicly available pre-trained polyglot word embeddings of (Al-Rfou et al., 2013) without any task specific modification for our experiments. A key thing to note is that polyglot word embeddings are not tailored for any specific language analysis task such as POS tagging or NER. These word embeddings are instead trained in a manner favoring the word analogy task introduced by Mikolov et al. (2013c). The polyglot project distributes word embeddings for more than 100 languages.", "startOffset": 215, "endOffset": 557}, {"referenceID": 1, "context": "According to these findings, in order to avoid any hassles of tuning the hyperparameters of the word embedding model employed, we rather primarily use the publicly available pre-trained polyglot word embeddings of (Al-Rfou et al., 2013) without any task specific modification for our experiments. A key thing to note is that polyglot word embeddings are not tailored for any specific language analysis task such as POS tagging or NER. These word embeddings are instead trained in a manner favoring the word analogy task introduced by Mikolov et al. (2013c). The polyglot project distributes word embeddings for more than 100 languages. AlRfou et al. (2013) also report results on POS tagging, however, word representations they apply for these experiments are different from the task-agnostic representations they made publicly available.", "startOffset": 215, "endOffset": 657}, {"referenceID": 10, "context": "A final difference to (Collobert et al., 2011) is that we experiment with a much wider range of languages while they report results for English only.", "startOffset": 22, "endOffset": 46}, {"referenceID": 10, "context": "Collobert et al. (2011) propose neural network architectures to four natural language processing tasks, i.", "startOffset": 0, "endOffset": 24}, {"referenceID": 10, "context": "Collobert et al. (2011) propose neural network architectures to four natural language processing tasks, i.e. POS tagging, named entity recognition, semantic role labeling and chunking. Collobert et al. (2011) train word representations on large amounts of unannotated texts from Wikipedia, then update the pre-trained word representations for the individual tasks.", "startOffset": 0, "endOffset": 209}, {"referenceID": 10, "context": "The hand-crafted features they employ for POS tagging and NER are the same as in Collobert et al. (2011) and Turian et al.", "startOffset": 81, "endOffset": 105}, {"referenceID": 10, "context": "The hand-crafted features they employ for POS tagging and NER are the same as in Collobert et al. (2011) and Turian et al. (2010).", "startOffset": 81, "endOffset": 130}, {"referenceID": 22, "context": "The general goal of sparse coding is to express signals in the form of sparse linear combination of basis vectors and the task of finding an appropriate set of basis vectors is referred to as the dictionary learning problem (Mairal et al., 2010).", "startOffset": 224, "endOffset": 245}, {"referenceID": 14, "context": "In Section 4, we compare the effects of employing different sparse coding paradigms including the ones in (Faruqui et al., 2015).", "startOffset": 106, "endOffset": 128}, {"referenceID": 24, "context": "(2016) have proposed an online learning algorithm using regularized dual averaging to directly obtain `1 regularized continuous bag of words (CBOW) representations (Mikolov et al., 2013a) without the need to determine dense CBOW representations first.", "startOffset": 164, "endOffset": 187}, {"referenceID": 14, "context": "Performing sparse coding of word embeddings has recently been proposed by Faruqui et al. (2015), however, the objective function they optimize differs from (1).", "startOffset": 74, "endOffset": 96}, {"referenceID": 14, "context": "Performing sparse coding of word embeddings has recently been proposed by Faruqui et al. (2015), however, the objective function they optimize differs from (1). In Section 4, we compare the effects of employing different sparse coding paradigms including the ones in (Faruqui et al., 2015). In their work, Yogatama et al. (2015) proposed an efficient learning algorithm for determining hierarchically organized sparse word representations using stochastic proximal methods.", "startOffset": 74, "endOffset": 329}, {"referenceID": 14, "context": "Performing sparse coding of word embeddings has recently been proposed by Faruqui et al. (2015), however, the objective function they optimize differs from (1). In Section 4, we compare the effects of employing different sparse coding paradigms including the ones in (Faruqui et al., 2015). In their work, Yogatama et al. (2015) proposed an efficient learning algorithm for determining hierarchically organized sparse word representations using stochastic proximal methods. Most recently, Sun et al. (2016) have proposed an online learning algorithm using regularized dual averaging to directly obtain `1 regularized continuous bag of words (CBOW) representations (Mikolov et al.", "startOffset": 74, "endOffset": 507}, {"referenceID": 22, "context": "Once the dictionary matrix D is learned, the sparse linear combination coefficients \u03b1i can easily be determined for a word embedding vector wi by solving an `1-regularized linear least-squares minimization problem (Mairal et al., 2010).", "startOffset": 214, "endOffset": 235}, {"referenceID": 16, "context": "We then use the previously described set of features in a linear chain CRF (Lafferty et al., 2001) using CRFsuite (Okazaki, 2007).", "startOffset": 75, "endOffset": 98}, {"referenceID": 34, "context": ", 2001) using CRFsuite (Okazaki, 2007).", "startOffset": 23, "endOffset": 38}, {"referenceID": 22, "context": "We primarily rely on the SPArse Modeling Software1 (SPAMS) (Mairal et al., 2010) for performing sparse coding of distributed word representations.", "startOffset": 59, "endOffset": 80}, {"referenceID": 6, "context": "Brown clustering Various studies have identified Brown clustering (Brown et al., 1992) as a useful source of feature generation for sequence labeling tasks (Ratinov and Roth, 2009; Turian et al.", "startOffset": 66, "endOffset": 86}, {"referenceID": 40, "context": ", 1992) as a useful source of feature generation for sequence labeling tasks (Ratinov and Roth, 2009; Turian et al., 2010; Owoputi et al., 2013; Stratos and Collins, 2015; Derczynski et al., 2015).", "startOffset": 77, "endOffset": 196}, {"referenceID": 47, "context": ", 1992) as a useful source of feature generation for sequence labeling tasks (Ratinov and Roth, 2009; Turian et al., 2010; Owoputi et al., 2013; Stratos and Collins, 2015; Derczynski et al., 2015).", "startOffset": 77, "endOffset": 196}, {"referenceID": 35, "context": ", 1992) as a useful source of feature generation for sequence labeling tasks (Ratinov and Roth, 2009; Turian et al., 2010; Owoputi et al., 2013; Stratos and Collins, 2015; Derczynski et al., 2015).", "startOffset": 77, "endOffset": 196}, {"referenceID": 42, "context": ", 1992) as a useful source of feature generation for sequence labeling tasks (Ratinov and Roth, 2009; Turian et al., 2010; Owoputi et al., 2013; Stratos and Collins, 2015; Derczynski et al., 2015).", "startOffset": 77, "endOffset": 196}, {"referenceID": 12, "context": ", 1992) as a useful source of feature generation for sequence labeling tasks (Ratinov and Roth, 2009; Turian et al., 2010; Owoputi et al., 2013; Stratos and Collins, 2015; Derczynski et al., 2015).", "startOffset": 77, "endOffset": 196}, {"referenceID": 40, "context": "We employ the frequently used approach of generating features from the length-p (p \u2208 {4, 6, 10, 20}) prefixes of Brown cluster identifiers similar to Ratinov and Roth (2009) and Turian et al.", "startOffset": 150, "endOffset": 174}, {"referenceID": 40, "context": "We employ the frequently used approach of generating features from the length-p (p \u2208 {4, 6, 10, 20}) prefixes of Brown cluster identifiers similar to Ratinov and Roth (2009) and Turian et al. (2010).", "startOffset": 150, "endOffset": 199}, {"referenceID": 20, "context": "In our experiments we use the implementation by Liang (2005) for performing Brown clustering2.", "startOffset": 48, "endOffset": 61}, {"referenceID": 33, "context": "There is a recent initiative of universal dependencies (UD) (Nivre, 2015), which aims at providing a unified notation for multiple linguistic phenomena, including part-of-speech tags as well.", "startOffset": 60, "endOffset": 73}, {"referenceID": 36, "context": "The first notable attempt trying to canonize the multiple tag sets existing is the Google universal part-of-speech tags introduced by Petrov et al. (2012) arguing that the POS tags of various tagging schemes can be mapped to 12 language-independent part-of-speech tags.", "startOffset": 134, "endOffset": 155}, {"referenceID": 33, "context": "There is a recent initiative of universal dependencies (UD) (Nivre, 2015), which aims at providing a unified notation for multiple linguistic phenomena, including part-of-speech tags as well. The POS tag set proposed for UD has 17 partially overlapping categories to the ones defined by Petrov et al. (2012).", "startOffset": 61, "endOffset": 308}, {"referenceID": 7, "context": "We use 12 treebanks in the CoNLL-X format from the CoNLL-2006/07 (Buchholz and Marsi, 2006; Nivre et al., 2007) shared tasks.", "startOffset": 65, "endOffset": 111}, {"referenceID": 31, "context": "We use 12 treebanks in the CoNLL-X format from the CoNLL-2006/07 (Buchholz and Marsi, 2006; Nivre et al., 2007) shared tasks.", "startOffset": 65, "endOffset": 111}, {"referenceID": 7, "context": "We use 12 treebanks in the CoNLL-X format from the CoNLL-2006/07 (Buchholz and Marsi, 2006; Nivre et al., 2007) shared tasks. The complete list of the treebanks included in our experiments is presented in Table 2. We rely on the official scripts released by Petrov et al. (2012)5 for mapping the treebank specific", "startOffset": 66, "endOffset": 279}, {"referenceID": 19, "context": "Analyzing the effects of window size Hyperparameters for training word representations can largely impact their quality as also concluded by Levy et al. (2015). We thus investigate if providing a larger context window size during the training of CBOW, SG and Glove embeddings can improve their utility of being employed in our model.", "startOffset": 141, "endOffset": 160}, {"referenceID": 21, "context": "Differences are the most pronounced in case of skip-gram representation, confirming the findings of Lin et al. (2015), i.", "startOffset": 100, "endOffset": 118}, {"referenceID": 14, "context": "The recent work of Faruqui et al. (2015) formulated alternative approaches to determine sparse word representations.", "startOffset": 19, "endOffset": 41}, {"referenceID": 14, "context": "The recent work of Faruqui et al. (2015) formulated alternative approaches to determine sparse word representations. One of the objective functions Faruqui et al. (2015) apply is", "startOffset": 19, "endOffset": 170}, {"referenceID": 14, "context": "In order to implicitly control for the norms of the basis vectors Faruqui et al. (2015) apply an additional regularization term affected by an extra parameter \u03c4 in their objective function.", "startOffset": 66, "endOffset": 88}, {"referenceID": 14, "context": "In order to implicitly control for the norms of the basis vectors Faruqui et al. (2015) apply an additional regularization term affected by an extra parameter \u03c4 in their objective function. Faruqui et al. (2015) also formulated a constrained objective function of the form", "startOffset": 66, "endOffset": 212}, {"referenceID": 14, "context": "When using the objective functions introduced by Faruqui et al. (2015), we use the default \u03c4 = 10\u22125 value.", "startOffset": 49, "endOffset": 71}, {"referenceID": 14, "context": "When using the objective functions introduced by Faruqui et al. (2015), we use the default \u03c4 = 10\u22125 value. Notationally, we distinguish the sparse coding approaches based on the equation they use as their objective function, i.e. SC-i, i \u2208 {1, 3, 4}. We applied \u03bb = 0.05 for SC-1 and \u03bb = 0.5 for SC-3 and SC-4 in order to obtain word representations of comparable average sparsity levels across the 12 languages, i.e. 95.3%, 94.5% and 95.2%, respectively (cf. the left of Figure 5). The right of Figure 5 further illustrates the spread of POS tagging accuracies over the 12 CoNLL-X treebanks when using models that rely on different sparse coding strategies with comparable sparsity levels. Although Murphy et al. (2012) mentions nonnegativity as a desired property of word representations for cognitive plausibility, Figure 5 reveals that our sequence labeling model cannot benefit from it as the average POS tagging accuracy for SC-4 is 0.", "startOffset": 49, "endOffset": 721}, {"referenceID": 38, "context": "We also present in Table 5 the state-of-the-art results of the bidirectional LSTM models by Plank et al. (2016) for comparative purposes.", "startOffset": 92, "endOffset": 112}, {"referenceID": 38, "context": "The bi-LSTM results are from Plank et al. (2016).", "startOffset": 29, "endOffset": 49}, {"referenceID": 10, "context": "Similar to Collobert et al. (2011) we also apply the 17-tag IOBES tagging scheme during training and inference.", "startOffset": 11, "endOffset": 35}, {"referenceID": 10, "context": "Similar to Collobert et al. (2011) we also apply the 17-tag IOBES tagging scheme during training and inference. The best F1 scores reported for English by Collobert et al. (2011) without employing additional unlabeled texts to enhance their language model is 81.", "startOffset": 11, "endOffset": 179}, {"referenceID": 24, "context": "As Mikolov et al. (2013b) pointed out the similarities of continuous word embeddings across languages, we think that our proposed model could be employed in not just multi-lingual, but cross-lingual language analysis settings.", "startOffset": 3, "endOffset": 26}], "year": 2016, "abstractText": "In this paper we propose and carefully evaluate a sequence labeling framework which solely utilizes sparse indicator features derived from dense distributed word representations. The proposed model obtains (near) state-of-the art performance for both part-ofspeech tagging and named entity recognition for a variety of languages. Our model relies only on a few thousand sparse coding-derived features, without applying any modification of the word representations employed for the different tasks. The proposed model has favorable generalization properties as it retains over 89.8% of its average POS tagging accuracy when trained at 1.2% of the total available training data, i.e. 150 sentences per language.", "creator": "LaTeX with hyperref package"}}}