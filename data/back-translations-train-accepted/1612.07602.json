{"id": "1612.07602", "review": {"conference": "ACL", "VERSION": "v1", "DATE_OF_SUBMISSION": "22-Dec-2016", "title": "Jointly Extracting Relations with Class Ties via Effective Deep Ranking", "abstract": "In distant supervised relation extraction, the connection between relations of one entity tuple, which we call class ties, is common. Exploiting this connection may be promising for relation extraction. However, this property is seldom considered by previous work. In this work, to leverage class ties, we propose to make joint relation extraction with a unified model that integrates convolutional neural network with a general pairwise ranking framework, in which two novel ranking loss functions are introduced. Besides, an effective method is proposed to relieve the impact of relation NR (not relation) for model training and test. Experimental results on a widely used dataset show that: (1) Our model is much more superior than the baselines, achieving state-of-the-art performance; (2) Leveraging class ties, joint extraction is indeed better than separated extraction; (3) Relieving the impact of NR will significantly boost our model performance; (4) Our model can primely deal with wrong labeling problem.", "histories": [["v1", "Thu, 22 Dec 2016 14:08:08 GMT  (82kb,D)", "http://arxiv.org/abs/1612.07602v1", null], ["v2", "Thu, 2 Feb 2017 11:15:55 GMT  (512kb,D)", "http://arxiv.org/abs/1612.07602v2", null], ["v3", "Thu, 13 Apr 2017 08:21:37 GMT  (167kb,D)", "http://arxiv.org/abs/1612.07602v3", null], ["v4", "Sat, 5 Aug 2017 12:03:18 GMT  (167kb,D)", "http://arxiv.org/abs/1612.07602v4", "To appear in ACL2017"]], "reviews": [], "SUBJECTS": "cs.AI cs.CL", "authors": ["hai ye", "wenhan chao", "zhunchen luo", "zhoujun li"], "accepted": true, "id": "1612.07602"}, "pdf": {"name": "1612.07602.pdf", "metadata": {"source": "CRF", "title": "Jointly Extracting Relations with Class Ties via Effective Deep Ranking", "authors": ["Hai Ye", "Wenhan Chao", "Zhunchen Luo"], "emails": ["chaowenhan}@buaa.edu.cn,", "zhunchenluo@gmail.com"], "sections": [{"heading": "1 Introduction", "text": "On the contrary, strong class relationships have a latent effect on the relationships between two particular named entities from the natural language text. Traditionally supervised methods of machine learning require a large amount of labeled data to function properly, which involves a lot of human work at the same time. However, with the rapid growth of the volume of relationship types, traditional methods cannot keep pace with the limitations of the designated data. To narrow the gap of data economy, we propose remote supervision (DS) for relationship types that automatically generate training data by aligning a knowledge database (e.g. Freebase [Bollacker et al., 2008]) with texts. Class relationships means the connections between the relationships between an entity tuples in DS relationship. In general, we conclude that class relationships can have two types: weak class relationships and strong class relationships. Weak class relationships mainly involve the coexistence of relationships such as the place of birth and place lived."}, {"heading": "2 Related Work", "text": "We summarize related work on two main aspects: remote supervised relationship extraction and deep learning of rank."}, {"heading": "2.1 Distant Supervised Relation Extraction", "text": "Traditionally monitored methods of relationship extraction suffer from the problem of limited, labeled data that require a lot of human work to obtain it. To overcome the rarity of labeled data, [Mintz et al., 2009] propose remote supervision of relationship extraction that aligns the relationship facts with texts. To solve the problem of mislabeling, [Riedel et al., 2010] introduce multi-instance learning but cannot deal with the overlapping problem. [Hoffmann et al., 2011; Surdeanu et al., 2012] Model this problem by multilevel, multi-level learning to extract overlapping relationships. In recent years, deep learning has achieved remarkable successes in computer vision and natural language processing ([LeCun et al., 2015]). With respect to extraction, deep learning has been applied to automatically learn the characteristics of sentences ([Zeng et al. 2016; Zeng, al., 2015, al.]"}, {"heading": "2.2 Deep Learning to Rank", "text": "Rank Learning (LTR) is an important technique for obtaining information (IR) ([Liu, 2009]). Methods for learning an LTR model include meaningful, pair-wise and list-wise. We apply the pair-wise learning of rank in our paper. Deep learning of rank is common in problems with multiple label classification problems. When retrieving images, [Zhao et al., 2015] apply a deep semantic ranking for image acquisition with multiple labels. When comparing text [Severyn and Moschitti, 2015] we take the learning of rank combined with deep CNN for short text pairs. In traditional supervised relationship extraction, [Santos et al., 2015] we design a pair-wise loss function based on CNN for single label extraction. Based on the advantage of deep learning to rank in multi-label classification, we adopt this method of anchoring in 2016, by focusing on the problem of having multiple relationships."}, {"heading": "3 Methodology", "text": "We define the relationship classes as L = {1, 2, \u00b7 \u00b7, C}, entity stuples as T = {ti} Mi = 1 and mentions as X = {xi} Ni = 1. The data set is structured as follows: For entity stuples ti-T and its relationship class Li L sets all mentions Xi containing ti, the data set we use is D = {(ti, Li, Xi)} Hi = 1."}, {"heading": "3.1 Convolutional Neural Network for Sentence Embedding", "text": "The raw sentences in the format of the natural-language text cannot be encrypted directly by the neural network, so we need to transform words in sentences from raw text to real vectors. First, the words are represented by prefabricated word embedding. Then, we add the embedding of words to the word to define the spacing of words that are embedded in the sentence. (Zeng et al.) The words of the word embedding are the dimensions of the word embedding and the dimensions of the word embedding. (Zeng et al., 2015) have shown that adding the word embedding can improve the performance of the word embedding. (Zeng et al.) The position of the word embedding is represented by real vectors from V. (Zeng et al., 2014) we have shown that embedding the word embedding can improve the word embedding. (Zeng et al.)"}, {"heading": "3.2 Learning to Rank", "text": "We start by combining the information about the sentences for collective extraction, then we discuss our pair-by-pair loss ranking function = = 2015. Given the amount of data (tk, Lk, Xk) that we use (ti, Li, Xi) Hi = 1, the sentence embeddings of Xk encoded by CNN, we are called Sk = {si} | Xk | i = 1 and we use the class embedding in positive relationships. We propose two options to combine information across the sentences. \u2022 AVE (Variant-1) The first option is an average method. This method refers to all sentences equally and directly to the averages in all dimensions of the sentences. This AVE function is defined as follows: s = 1n, si si si si si si."}, {"heading": "3.3 Relieving Impact of NR", "text": "In traditional supervised relation extraction, [Santos et al., 2015] suggest omitting the NR class to alleviate the problem. In DS-based supervised relation extraction, the problem is more serious. Table 2 shows the proportion of NR samples in SemEval 2010 Task 8 dataset 1 ([Erk and Strapparava, 2010]) and [Riedel et al., 2010] dataset containing almost data on NR in [Riedel et al., 2010] dataset. The data imbalance will severely impair model training and cause the model to be sensitive only to classes with high proportionality. To mitigate the effects of NR in DS-based relation extraction, we cut off the spread of loss c in NR class c, meaning that the relationship c-class NR is comparable to [Santos et al., 2015] with the SR function that we cannot execute directly in class ATK."}, {"heading": "4 Experiments", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "4.1 Dataset and Evaluation Criteria", "text": "We conduct our experiments on a widely used dataset developed by [Riedel et al., 2010] and used by ([Hoffmann et al., 2011; Surdeanu et al., 2012; Zeng et al., 2015; 1This is a dataset for relation extraction in traditional supervision framework.Lin et al., 2016]. The dataset aligns the facts of the freebase relationship with the New York Times corpus, which contains training mentions from 2005-2006 corpus and test mentions from 2007. Following [Mintz et al., 2009] we adopt a reserved assessment framework in all experiments. Aggregate precision / recall curves are plotted and precision @ N (P @ N) is reported to illustrate model performance."}, {"heading": "4.2 Experimental Settings", "text": "Word Embeddings We use a word2vec tool called gensim2 to train word embedding on the NYT corpus.Similar to [Lin et al., 2016], we use words that occur more than 100 times to create dictionaries, and use \"UNK\" to represent the others.Hyper-parameter settings Triple validation on the training dataset is used to tune the parameters according to [Surdeanu et al., 2012].We determine the optimal hyper parameters using grid search.We select word embedding size from {50, 100, 150, 200, 250, 300}. Batch size is set from {80, 160, 320, 640}. We determine the learning rate under {0.01, 0.02, 0.03, 0.04}. The window size of the convolution is tuned from {1, 3, 5}. Other hyper parameters remain the same as Zell [because they have limited impact on performance in 2015]."}, {"heading": "4.3 Model Evaluation", "text": "In fact, it is the case that we will be able to go in search of a solution that is capable of finding the solution we need, \"he said in an interview with the German Press Agency.\" I am very satisfied, \"he said,\" but we are not yet able to find a solution. \""}, {"heading": "4.4 Model Comparison", "text": "From the evaluations above, we can conclude that the best experimental environment for our model is the use of AVE (Variant-1) to combine information across sentences, achieve common extraction and mitigate NR effects. We compare our model with baseline methods in the best experimental environment. Baseline We compare our model with the following baseline: \u2022 Mintz ([Mintz et al., 2009]) the original remote monitored model. \u2022 MultiR ([Hoffmann et al., 2011]) a multi-stage learning-based graphical model that aims to solve overlapping relationship problems. \u2022 MIML ([Surdeanu et al., 2012]) also solves overlapping relationships in a multi-label framework. \u2022 PCNN + ATT ([Lin et al., 2016]) the state-of-the-art model in datasets of [Riedel et al., 2016] solves overlapping relationships in a multi-label framework in a multi-label framework."}, {"heading": "4.5 Case Study", "text": "We randomly select an entity tuple (Cuyahoga County, Cleveland) from the test set to see its values for each relation class under common extraction and separate extraction settings. This entity tuple has two relationships: / location / us county / county town and / location / location / location / contains, which are derived from the same root class, and we can assume that they have weak class attachments because they all relate to the topic of the location. We scale the values by adding value 10. The results are shown in Figure 6, from which we can see the following: Under common extraction settings, the two gold relationships have the highest values among the other relationships, but under separate extraction settings, only / location / location / content can be distinguished from the negative relationships, which shows that a shared extraction is better than a separate extraction by capturing the class ties between relationships."}, {"heading": "5 Conclusion and Feature Works", "text": "In this paper, we propose two novel pairwise ranking loss functions in combination with CNN to integrate class connections through collective extraction, as well as an effective method to reduce the impact of NR. Experimental results show that our model significantly outperforms baselines, and we note that sentence-level attention may not be suitable to combine with rank-based frameworks. Our model can be applied not only to relation extraction, but can also be applied to other problems with multiple labels, such as multi-category text categorization and multi-category image categorization, with particular negative samples having a major impact on model training and testing."}], "references": [{"title": "Neural machine translation by jointly learning to align and translate", "author": ["Dzmitry Bahdanau", "Kyunghyun Cho", "Yoshua Bengio"], "venue": "arXiv preprint arXiv:1409.0473,", "citeRegEx": "Bahdanau et al.. 2014", "shortCiteRegEx": null, "year": 2014}, {"title": "Freebase: a collaboratively created graph database for structuring human knowledge", "author": ["Bollacker et al", "2008] Kurt D. Bollacker", "Colin Evans", "Praveen Paritosh", "Tim Sturge", "Jamie Taylor"], "venue": "In Proceedings of KDD", "citeRegEx": "al. et al\\.,? \\Q2008\\E", "shortCiteRegEx": "al. et al\\.", "year": 2008}, {"title": "editors", "author": ["Katrin Erk", "Carlo Strapparava"], "venue": "Proceedings of SemEval. The Association for Computer Linguistics,", "citeRegEx": "Erk and Strapparava. 2010", "shortCiteRegEx": null, "year": 2010}, {"title": "In Proceedings of AAAI", "author": ["Xianpei Han", "Le Sun. Global distant supervision for relation extraction"], "venue": "pages 2950\u20132956,", "citeRegEx": "Han and Sun. 2016", "shortCiteRegEx": null, "year": 2016}, {"title": "Knowl", "author": ["Haibo He", "Edwardo A. Garcia. Learning from imbalanced data. IEEE Trans"], "venue": "Data Eng., 21(9):1263\u20131284,", "citeRegEx": "He and Garcia. 2009", "shortCiteRegEx": null, "year": 2009}, {"title": "Knowledge-based weak supervision for information extraction of overlapping relations", "author": ["Hoffmann et al", "2011] Raphael Hoffmann", "Congle Zhang", "Xiao Ling", "Luke Zettlemoyer", "Daniel S Weld"], "venue": "In Proceedings of ACLHLT,", "citeRegEx": "al. et al\\.,? \\Q2011\\E", "shortCiteRegEx": "al. et al\\.", "year": 2011}, {"title": "Nature", "author": ["Yann LeCun", "Yoshua Bengio", "Geoffrey Hinton. Deep learning"], "venue": "521(7553):436\u2013 444,", "citeRegEx": "LeCun et al.. 2015", "shortCiteRegEx": null, "year": 2015}, {"title": "volume 1", "author": ["Yankai Lin", "Shiqi Shen", "Zhiyuan Liu", "Huanbo Luan", "Maosong Sun. Neural relation extraction with selective attention over instances. In Proceedings of ACL"], "venue": "pages 2124\u20132133,", "citeRegEx": "Lin et al.. 2016", "shortCiteRegEx": null, "year": 2016}, {"title": "Foundations and Trends in Information Retrieval", "author": ["Tie-Yan Liu. Learning to rank for information retrieval"], "venue": "3(3):225\u2013331,", "citeRegEx": "Liu. 2009", "shortCiteRegEx": null, "year": 2009}, {"title": "In Proceedings of EMNLP", "author": ["Thang Luong", "Hieu Pham", "Christopher D. Manning. Effective approaches to attention-based neural machine translation"], "venue": "pages 1412\u20131421,", "citeRegEx": "Luong et al.. 2015", "shortCiteRegEx": null, "year": 2015}, {"title": "pages 1003\u20131011", "author": ["Mike Mintz", "Steven Bills", "Rion Snow", "Dan Jurafsky. Distant supervision for relation extraction without labeled data. In Proceedings of ACL-IJCNLP"], "venue": "Association for Computational Linguistics,", "citeRegEx": "Mintz et al.. 2009", "shortCiteRegEx": null, "year": 2009}, {"title": "pages 148\u2013163", "author": ["Sebastian Riedel", "Limin Yao", "Andrew McCallum. Modeling relations", "their mentions without labeled text. In Proceedings of ECML-PKDD"], "venue": "Springer,", "citeRegEx": "Riedel et al.. 2010", "shortCiteRegEx": null, "year": 2010}, {"title": "Classifying relations by ranking with convolutional neural networks", "author": ["Cicero Nogueira dos Santos", "Bing Xiang", "Bowen Zhou"], "venue": "pages 626\u2013634,", "citeRegEx": "Santos et al.. 2015", "shortCiteRegEx": null, "year": 2015}, {"title": "pages 373\u2013382", "author": ["Aliaksei Severyn", "Alessandro Moschitti. Learning to rank short text pairs with convolutional deep neural networks. In Proceedings of SIGIR"], "venue": "ACM,", "citeRegEx": "Severyn and Moschitti. 2015", "shortCiteRegEx": null, "year": 2015}, {"title": "pages 455\u2013465", "author": ["Mihai Surdeanu", "Julie Tibshirani", "Ramesh Nallapati", "Christopher D Manning. Multiinstance multi-label learning for relation extraction. In Proceedings of EMNLP"], "venue": "Association for Computational Linguistics,", "citeRegEx": "Surdeanu et al.. 2012", "shortCiteRegEx": null, "year": 2012}, {"title": "In Proceedings of ACL", "author": ["Linlin Wang", "Zhu Cao", "Gerard de Melo", "Zhiyuan Liu. Relation classification via multi-level attention cnns"], "venue": "Volume 1: Long Papers,", "citeRegEx": "Wang et al.. 2016", "shortCiteRegEx": null, "year": 2016}, {"title": "WSABIE: scaling up to large vocabulary image annotation", "author": ["Jason Weston", "Samy Bengio", "Nicolas Usunier"], "venue": "Proceedings of IJCAI, pages 2764\u2013 2770,", "citeRegEx": "Weston et al.. 2011", "shortCiteRegEx": null, "year": 2011}, {"title": "Relation classification", "author": ["Zeng et al", "2014] Daojian Zeng", "Kang Liu", "Siwei Lai", "Guangyou Zhou", "Jun Zhao"], "venue": null, "citeRegEx": "al. et al\\.,? \\Q2014\\E", "shortCiteRegEx": "al. et al\\.", "year": 2014}, {"title": "In Proceedings of EMNLP", "author": ["Daojian Zeng", "Kang Liu", "Yubo Chen", "Jun Zhao. Distant supervision for relation extraction via piecewise convolutional neural networks"], "venue": "pages 17\u201321,", "citeRegEx": "Zeng et al.. 2015", "shortCiteRegEx": null, "year": 2015}, {"title": "In Proceedings of CVPR", "author": ["Fang Zhao", "Yongzhen Huang", "Liang Wang", "Tieniu Tan. Deep semantic ranking based hashing for multi-label image retrieval"], "venue": "pages 1556\u20131564,", "citeRegEx": "Zhao et al.. 2015", "shortCiteRegEx": null, "year": 2015}, {"title": "Aggregating intersentence information to enhance relation extraction", "author": ["Hao Zheng", "Zhoujun Li", "Senzhang Wang", "Zhao Yan", "Jianshe Zhou"], "venue": "Thirtieth AAAI Conference on Artificial Intelligence,", "citeRegEx": "Zheng et al.. 2016", "shortCiteRegEx": null, "year": 2016}, {"title": "In Proceeding of ACL", "author": ["Peng Zhou", "Wei Shi", "Jun Tian", "Zhenyu Qi", "Bingchen Li", "Hongwei Hao", "Bo Xu. Attentionbased bidirectional long short-term memory networks for relation classification"], "venue": "page 207,", "citeRegEx": "Zhou et al.. 2016", "shortCiteRegEx": null, "year": 2016}], "referenceMentions": [{"referenceID": 10, "context": "In order to narrow down the gap of data sparsity, [Mintz et al., 2009] propose distant supervision(DS) for relation extraction, which automatically generates training data by aligning a knowledge facts database (ie.", "startOffset": 50, "endOffset": 70}, {"referenceID": 14, "context": "DS based relation extraction suffers the challenge of relation overlapping which one entity tuple may have multiple relation facts ([Hoffmann et al., 2011; Surdeanu et al., 2012]).", "startOffset": 132, "endOffset": 178}, {"referenceID": 11, "context": "In the widely used dataset of [Riedel et al., 2010], we find that 98.", "startOffset": 30, "endOffset": 51}, {"referenceID": 14, "context": "[Hoffmann et al., 2011; Surdeanu et al., 2012] propose a multi-instance multi-label learning framework to jointly extract relations, but they only incorporate information from sentences and can not model relation ties.", "startOffset": 0, "endOffset": 46}, {"referenceID": 3, "context": "[Han and Sun, 2016] use rule-based Markov logic model to capture consistency between relation labels, but it is hard for human to define exact rules instead bringing errors to extraction.", "startOffset": 0, "endOffset": 19}, {"referenceID": 3, "context": "Different from [Han and Sun, 2016] using defined rules, we propose a general pairwise ranking framework under two variants combined with a convolutional neural network (CNN) to automatically learn the connections between relations by joint extraction.", "startOffset": 15, "endOffset": 34}, {"referenceID": 12, "context": "Similar to [Santos et al., 2015; Lin et al., 2016], we use class embeddings to represent relation classes.", "startOffset": 11, "endOffset": 50}, {"referenceID": 7, "context": "Similar to [Santos et al., 2015; Lin et al., 2016], we use class embeddings to represent relation classes.", "startOffset": 11, "endOffset": 50}, {"referenceID": 7, "context": "We first use CNN to embed sentences of one entity tuple and then combine the embedded sentences into one bag representation vector with two variant methods to aggregate information across sentences [Lin et al., 2016; Zheng et al., 2016].", "startOffset": 198, "endOffset": 236}, {"referenceID": 20, "context": "We first use CNN to embed sentences of one entity tuple and then combine the embedded sentences into one bag representation vector with two variant methods to aggregate information across sentences [Lin et al., 2016; Zheng et al., 2016].", "startOffset": 198, "endOffset": 236}, {"referenceID": 11, "context": "In dataset of [Riedel et al., 2010], almost training and test data is about NR.", "startOffset": 14, "endOffset": 35}, {"referenceID": 4, "context": "It will be really hard to train the model for data extreme imbalance ([He and Garcia, 2009]).", "startOffset": 70, "endOffset": 91}, {"referenceID": 11, "context": "Our experimental results on dataset of [Riedel et al., 2010] are evident that: (1) Our model is much more effective than the baselines; (2) Joint extraction for relations is better than separated extraction by leveraging class ties; (3) Our method is effective to relieve the impact of NR on training; (4) Wrong labeling problem can be primely solved by our ranking method.", "startOffset": 39, "endOffset": 60}, {"referenceID": 10, "context": "To overcome the sparsity of labeled data, [Mintz et al., 2009] propose distant supervision for relation extraction, which align the relation facts to texts.", "startOffset": 42, "endOffset": 62}, {"referenceID": 11, "context": "To relieve the wrong labeling problem, [Riedel et al., 2010] introduce multi-instance learning but can not deal with the relation overlapping problem.", "startOffset": 39, "endOffset": 60}, {"referenceID": 14, "context": "Afterwards, [Hoffmann et al., 2011; Surdeanu et al., 2012] model this problem by multi-instance multi-label learning to extract overlapping relations.", "startOffset": 12, "endOffset": 58}, {"referenceID": 6, "context": "Recent years, deep learning has achieved remarkable success in computer vision and natural language processing ([LeCun et al., 2015]).", "startOffset": 112, "endOffset": 132}, {"referenceID": 18, "context": "In relation extraction, deep learning has been applied to automatically learn the features of sentences ([Zeng et al., 2014; Zeng et al., 2015; Santos et al., 2015; Lin et al., 2016]), the specific deep learning architecture can be CNN ([Zeng et al.", "startOffset": 105, "endOffset": 182}, {"referenceID": 12, "context": "In relation extraction, deep learning has been applied to automatically learn the features of sentences ([Zeng et al., 2014; Zeng et al., 2015; Santos et al., 2015; Lin et al., 2016]), the specific deep learning architecture can be CNN ([Zeng et al.", "startOffset": 105, "endOffset": 182}, {"referenceID": 7, "context": "In relation extraction, deep learning has been applied to automatically learn the features of sentences ([Zeng et al., 2014; Zeng et al., 2015; Santos et al., 2015; Lin et al., 2016]), the specific deep learning architecture can be CNN ([Zeng et al.", "startOffset": 105, "endOffset": 182}, {"referenceID": 21, "context": ", 2014]), RNN ([Zhou et al., 2016]), etc.", "startOffset": 15, "endOffset": 34}, {"referenceID": 18, "context": "[Zeng et al., 2015] propose a piecewise convolutional neural network in multi-instance learning framework for DS based relation extraction, which improves the precision and recall significantly.", "startOffset": 0, "endOffset": 19}, {"referenceID": 7, "context": "Afterwards, [Lin et al., 2016] introduce the mechanism of attention ([Luong et al.", "startOffset": 12, "endOffset": 30}, {"referenceID": 9, "context": ", 2016] introduce the mechanism of attention ([Luong et al., 2015; Bahdanau et al., 2014]) to select the sentences to relieve the wrong labeling problem and use all the information across sentences.", "startOffset": 46, "endOffset": 89}, {"referenceID": 0, "context": ", 2016] introduce the mechanism of attention ([Luong et al., 2015; Bahdanau et al., 2014]) to select the sentences to relieve the wrong labeling problem and use all the information across sentences.", "startOffset": 46, "endOffset": 89}, {"referenceID": 8, "context": "Learning to rank (LTR) is an important technique in information retrieval (IR) ([Liu, 2009]).", "startOffset": 80, "endOffset": 91}, {"referenceID": 19, "context": "In image retrieval, [Zhao et al., 2015] apply deep semantic ranking for multi-label image retrieval.", "startOffset": 20, "endOffset": 39}, {"referenceID": 13, "context": "In text matching, [Severyn and Moschitti, 2015] adopt learning to rank combined with deep CNN for short text pairs matching.", "startOffset": 18, "endOffset": 47}, {"referenceID": 12, "context": "In traditional supervised relation extraction, [Santos et al., 2015] design a pairwise loss function based on CNN for single label extraction.", "startOffset": 47, "endOffset": 68}, {"referenceID": 20, "context": "[Zheng et al., 2016] also deal with this problem by learning to rank.", "startOffset": 0, "endOffset": 20}, {"referenceID": 18, "context": "\u2022 Position Embedding [Zeng et al., 2014; Zeng et al., 2015] have shown that adding the word position embedding can improve the performance.", "startOffset": 21, "endOffset": 59}, {"referenceID": 7, "context": "\u2022 ATT (Variant-2) The second one is a sentence-level attention algorithm used by [Lin et al., 2016] to measure the importance of sentences aiming to relieve the wrong labeling problem.", "startOffset": 81, "endOffset": 99}, {"referenceID": 15, "context": "In [Wang et al., 2016], they propose distance function that measures the similarity between s andW[c] by distance.", "startOffset": 3, "endOffset": 22}, {"referenceID": 12, "context": "Because score function is not an important issue in our model, we adopt dot function, also used by [Santos et al., 2015; Lin et al., 2016], as our score function.", "startOffset": 99, "endOffset": 138}, {"referenceID": 7, "context": "Because score function is not an important issue in our model, we adopt dot function, also used by [Santos et al., 2015; Lin et al., 2016], as our score function.", "startOffset": 99, "endOffset": 138}, {"referenceID": 12, "context": "Similar to [Santos et al., 2015; Wang et al., 2016], this loss function is designed to rank positive classes higher than negative ones controlled by the margin of \u03c3 \u2212 \u03c3\u2212.", "startOffset": 11, "endOffset": 51}, {"referenceID": 15, "context": "Similar to [Santos et al., 2015; Wang et al., 2016], this loss function is designed to rank positive classes higher than negative ones controlled by the margin of \u03c3 \u2212 \u03c3\u2212.", "startOffset": 11, "endOffset": 51}, {"referenceID": 16, "context": "Similar to [Weston et al., 2011; Santos et al., 2015], we update one negative class at every training round but to balance the loss between positive classes and negative ones, we multiply |Lk| before the right term in function (9) to expand the negative loss.", "startOffset": 11, "endOffset": 53}, {"referenceID": 12, "context": "Similar to [Weston et al., 2011; Santos et al., 2015], we update one negative class at every training round but to balance the loss between positive classes and negative ones, we multiply |Lk| before the right term in function (9) to expand the negative loss.", "startOffset": 11, "endOffset": 53}, {"referenceID": 12, "context": "The negative class is chosen as the one with highest score among all negative classes ([Santos et al., 2015]), i.", "startOffset": 87, "endOffset": 108}, {"referenceID": 12, "context": "In traditional supervised relation extraction, [Santos et al., 2015] propose to omit the NR class embedding to relieve the problem.", "startOffset": 47, "endOffset": 68}, {"referenceID": 2, "context": "Table 2 presents the proportion of NR samples in SemEval-2010 Task 8 dataset1 ([Erk and Strapparava, 2010]) and [Riedel et al.", "startOffset": 79, "endOffset": 106}, {"referenceID": 11, "context": "Table 2 presents the proportion of NR samples in SemEval-2010 Task 8 dataset1 ([Erk and Strapparava, 2010]) and [Riedel et al., 2010] dataset, which shows almost data is about NR in [Riedel et al.", "startOffset": 112, "endOffset": 133}, {"referenceID": 11, "context": ", 2010] dataset, which shows almost data is about NR in [Riedel et al., 2010] dataset.", "startOffset": 56, "endOffset": 77}, {"referenceID": 12, "context": "Our method is similar to [Santos et al., 2015] with sight variance.", "startOffset": 25, "endOffset": 46}, {"referenceID": 12, "context": "[Santos et al., 2015] directly omit the NR class embedding, but we keep it.", "startOffset": 0, "endOffset": 21}, {"referenceID": 11, "context": "We conduct our experiments on a widely used dataset, developed by [Riedel et al., 2010] and has been used by ([Hoffmann et al.", "startOffset": 66, "endOffset": 87}, {"referenceID": 10, "context": "Following [Mintz et al., 2009], we adopt held-out evaluation framework in all experiments.", "startOffset": 10, "endOffset": 30}, {"referenceID": 7, "context": "Similar to [Lin et al., 2016], we keep the words that appear more than 100 times to construct word dictionary and use \u201dUNK\u201d to represent the other ones.", "startOffset": 11, "endOffset": 29}, {"referenceID": 14, "context": "Hyper-parameter Settings Three-fold validation on the training dataset is adopted to tune the parameters following [Surdeanu et al., 2012].", "startOffset": 115, "endOffset": 138}, {"referenceID": 18, "context": "We keep other hyper-parameters same as [Zeng et al., 2015] because they make little effect on model performance.", "startOffset": 39, "endOffset": 58}, {"referenceID": 7, "context": "Impact of Combing Information across Sentences Previous work has proven combining information across all the sentences will improve the performance ([Lin et al., 2016; Zheng et al., 2016]), so we do not repeat experiment to verify this in this paper.", "startOffset": 149, "endOffset": 187}, {"referenceID": 20, "context": "Impact of Combing Information across Sentences Previous work has proven combining information across all the sentences will improve the performance ([Lin et al., 2016; Zheng et al., 2016]), so we do not repeat experiment to verify this in this paper.", "startOffset": 149, "endOffset": 187}, {"referenceID": 7, "context": "In the work of [Lin et al., 2016], ATT (Variant-2) can improve precision and recall significantly comparing to AVE.", "startOffset": 15, "endOffset": 33}, {"referenceID": 7, "context": "From the two results, we can see that ATT can not improve the model performance and even hurt the performance which does not correspond to the results in [Lin et al., 2016].", "startOffset": 154, "endOffset": 172}, {"referenceID": 10, "context": "Baseline We compare our model with the following baselines: \u2022 Mintz ([Mintz et al., 2009]) the original distant supervised model.", "startOffset": 69, "endOffset": 89}, {"referenceID": 14, "context": "\u2022 MIML ([Surdeanu et al., 2012]) also solving overlapping relations in a multi-instance multi-label framework.", "startOffset": 8, "endOffset": 31}, {"referenceID": 7, "context": "\u2022 PCNN+ATT ([Lin et al., 2016]) the state-of-the-art model in dataset of [Riedel et al.", "startOffset": 12, "endOffset": 30}, {"referenceID": 11, "context": ", 2016]) the state-of-the-art model in dataset of [Riedel et al., 2010] which applies sentence-level attention to measure contributions of sentences and combines all the mentions for training and prediction.", "startOffset": 50, "endOffset": 71}], "year": 2016, "abstractText": "In distant supervised relation extraction, the connection between relations of one entity tuple, which we call class ties, is common. Exploiting this connection may be promising for relation extraction. However, this property is seldom considered by previous work. In this work, to leverage class ties, we propose to make joint relation extraction with a unified model that integrates convolutional neural network with a general pairwise ranking framework, in which two novel ranking loss functions are introduced. Besides, an effective method is proposed to relieve the impact of relation NR (not relation) for model training and test. Experimental results on a widely used dataset show that: (1) Our model is much more superior than the baselines, achieving state-of-the-art performance; (2) Leveraging class ties, joint extraction is indeed better than separated extraction; (3) Relieving the impact of NR will significantly boost our model performance; (4) Our model can primely deal with wrong labeling problem.", "creator": "LaTeX with hyperref package"}}}