{"id": "1612.07771", "review": {"conference": "iclr", "VERSION": "v1", "DATE_OF_SUBMISSION": "22-Dec-2016", "title": "Highway and Residual Networks learn Unrolled Iterative Estimation", "abstract": "The past year saw the introduction of new architectures such as Highway networks and Residual networks which, for the first time, enabled the training of feedforward networks with dozens to hundreds of layers using simple gradient descent. While depth of representation has been posited as a primary reason for their success, there are indications that these architectures defy a popular view of deep learning as a hierarchical computation of increasingly abstract features at each layer.", "histories": [["v1", "Thu, 22 Dec 2016 19:57:35 GMT  (164kb,D)", "http://arxiv.org/abs/1612.07771v1", "10 + 3pages, under review for ICLR 2017"], ["v2", "Fri, 3 Mar 2017 19:52:47 GMT  (1447kb,D)", "http://arxiv.org/abs/1612.07771v2", "10 + 4 pages, accepted for ICLR 2017"], ["v3", "Tue, 14 Mar 2017 21:27:03 GMT  (1440kb,D)", "http://arxiv.org/abs/1612.07771v3", "10 + 4 pages, accepted for ICLR 2017"]], "COMMENTS": "10 + 3pages, under review for ICLR 2017", "reviews": [], "SUBJECTS": "cs.NE cs.AI cs.LG", "authors": ["klaus greff", "rupesh k srivastava", "j\\\"urgen schmidhuber"], "accepted": true, "id": "1612.07771"}, "pdf": {"name": "1612.07771.pdf", "metadata": {"source": "CRF", "title": "UNROLLED ITERATIVE ESTIMATION", "authors": ["Klaus Greff", "Rupesh K. Srivastava", "J\u00fcrgen Schmidhuber"], "emails": ["klaus@idsia.ch", "rupesh@idsia.ch", "juergen@idsia.ch"], "sections": [{"heading": null, "text": "In fact, it is a way in which one is able to put oneself at the centre, without being able to orient oneself in a particular direction."}, {"heading": "2 CHALLENGING THE REPRESENTATION VIEW", "text": "This year, more than ever before in the history of the country in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which is a country, in which it is a country, in which it is a country, in which is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which is a country, in which it is a country, in which it is a country, in which it is a country, in which is a country"}, {"heading": "3 UNROLLED ITERATIVE ESTIMATION VIEW", "text": "The point of view of representation has guided research on neural networks by providing intuitions about the \"meaning\" of their calculations. Therefore, in this section we will broaden the point of view of representation to deal with the inconsistencies and hopefully enable future research on these very deep architectures to gain the same advantages. Therefore, the aim of our modification is to map layers / blocks of the network at abstraction levels. At this point, it is interesting to note that the one-to-one mapping of neural network layers at abstraction levels is an implicit assumption rather than a specified part of the observation. A recent deep textbook Goodfellow et al. (2016) explicitly states that the depth flow diagram of calculations necessary to calculate the representation of each concept can be much deeper than the graph of the concepts themselves, which in a strict sense produce the evidence from Section 2, is not at odds with the representation of a highway and residual network."}, {"heading": "3.1 HIGHWAY AND RESIDUAL NETWORKS", "text": "Both the highway and residual networks address the problem of forming very deep architectures by improving the flow of errors through identity skip connections that allow units to copy their input to the next fixed layer. Highway networks use design principles from Long Short-Term Memory (LSTM). Essentially, they can be considered a simplified and unrolled LSTM network with unbound weights. For each unit, there is an additional estimation unit that interpolates between the normal, typically non-linear transformation and a copy of the activation of the corresponding unit in the previous layer. Let H (x) be a non-linear parametric function of the input, x, (typically an affine projection followed by pointedly non-linearity). Then, a traditional feed-forward network layer can be written like this: y (x) = H (x) = H (x) (x) (H)."}, {"heading": "3.2 DERIVING RESIDUAL NETWORKS", "text": "A traditional feedback network layer can be described as follows: y (x) = H (x), where H (x) is a nonlinear parametric function of the inputs x (typically an affine projection followed by meaningful nonlinearity). ResNets formulate the nonlinear transformation of a plane as its input x plus a residual function F (x): y (x) = F (x) + xThe reason for this is that it is easier to optimize the residual form than the original function. In the extreme case where the desired function is the identity, this boils down to the trivial task of pressing the residual amount to zero. However, it does not answer the question why a function close to the identity is desirable. Equation 1 can be used to derive the ResNet formula directly, because it immediately follows that the expected difference between two blocks is also zero: E [aki \u2212 Ai] \u2212 E [ak \u2212 Fi \u2212 Fi \u2212 0 (2) \u2212 aki = 1ak = 1k = 1k = 1i = 1ak = 1k \u2212 1 ak = thus we can get a property."}, {"heading": "3.3 DERIVING HIGHWAY NETWORKS", "text": "Motorway networks use the design principles used by Long Short-Term Memory (LSTM; Hochreiter & Schmidhuber 1997) to solve the problem of the disappearing slope. Essentially, they correspond to a simplified and unrolled LSTM network with unbound weights. For each unit, there is an additional gating unit that interpolates between the normal transformation and copying the activation from the corresponding unit to the previous layer. By adding an additional unit T (x) (the transformation gate), we can write a highway layer as follows: y (x) = H (x) \u00b7 T (x) + x \u00b7 (1 \u2212 T (x)) This formula can be derived directly as an alternative method of ensuring Equation 1, assuming that Hi is a separate estimate of Ai. At the moment, it does not matter where Hi (x) = H (x) = H (x) = H (x) (x) (x) (T) + x (T) (x) \u2212 1)."}, {"heading": "4 DISCUSSION", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "4.1 IMPLICATIONS FOR HIGHWAY NETWORKS", "text": "The exact value of the mixing determines only the variance of the new estimate. We can limit this variance to less or equal to the variance of the previous layer by classifying both mixing coefficients as positive. In motorway networks, this is done by using the logistic sigmoid activation function for the transformation gate Ti. This limitation corresponds to the assumption that \u03b11 and \u03b12 have the same sign. This assumption applies, for example, if the error of the new estimate Hi \u2212 Ai is independent of the old ak \u2212 1i \u2212 Ai. In this case, their covariance is zero and thus both alphas are positive. Furthermore, the use of the logistic sigmoid as an activation function for the transformation gate means that the pre-activation of Ti implicitly results in an estimate (cf."}, {"heading": "4.2 EXPERIMENTAL CORROBORATION OF ITERATIVE ESTIMATION VIEW", "text": "The primary prediction of the iterative estimation view is that the estimation error for highway or residual blocks within the same level should be zero in expectation. To empirically verify this assertion, we extract the intermediate layers for 5000 validation set images using the 50-layer ResNet that was trained on the ILSVRC 2015 dataset by He et al. (2015). These are then used to calculate the empirical mean and the standard deviation of the estimation error over the validation subset for all blocks in the four residual levels in the network. Finally, the mean of the empirical mean and the standard deviation over the three spatial dimensions.Figure 3 shows that the mean estimation error in the first three stages is actually close to zero, indicating that it is permissible to interpret the role of the residual blocks in this network as the role of the iterative refinement of a representation of a study, with each of the following blocks interpreting the probability variation over the three blocks."}, {"heading": "4.3 STAGES", "text": "ResNet-151 (He et al., 2015) and many other derived architectures have some common features: they are divided into stages of residual blocks that have the same dimensionality; between these stages, the input dimensionality changes, typically through down sampling and an increase in the number of channels; these stages also typically increase in length: the first stage consists of a few layers, while the last stage has many. We can now interpret these design decisions from an iterative estimation point of view; from this perspective, the level of representation within each stage remains the same, through the use of identity abbreviations; between stages, the level of representation changes through the use of a projection to change dimensionality; this means that we expect the type of characteristics detected within a stage to be very similar and jumps in the abstraction between stages; hence, it is also clear that the first few stages should be shorter, as early representations may be relatively simple and relatively easy."}, {"heading": "4.4 REVISITING THE EVIDENCE", "text": "This is why the ResNet formulation makes sense: Identity learning is difficult and often needed, and it also explains the sparse transformation activity of gate activity in trained highway networks: These networks learn to dynamically and selectively update individual features while performing most of the display intact.Lesioning. Another implication of the iteration view is that layers become incremental and somewhat interchangeable. Each layer (apart from the first) refines an already reasonable estimate for the display, so the immediate implication is that removing layers, as in the emptying experiments, should only have a mild effect on the end result. As removing these layers does not alter the overall display, the next layer only retains its quality, so the following layer can still perform the same operation, even with a somewhat noisy input."}, {"heading": "5 COMPARATIVE CASE STUDIES", "text": "The preceding sections show that we can mathematically construct both highway and residual architectures based on learning unrolled iterative estimation. What these architectures have in common is that they retain characteristic identities, and the main difference is that they have different biases about changing characteristic identities. Unfortunately, since our current understanding of the calculations needed to solve complex problems is limited, it is extremely difficult to say a priori which architecture might be better suited to what kind of problems. Therefore, in this section we will conduct two case studies in which their behavior will be experimentally compared and compared."}, {"heading": "5.1 IMAGE CLASSIFICATION", "text": "This year, the situation is so bad that there will be no further significant tightening."}, {"heading": "5.2 LANGUAGE MODELING", "text": "Next, we compare different functional forms (or variants) of highway network formulation in the case of sign-conscious speech modeling. Kim et al. (2015) have shown that the use of a few highway layers instead of conventional simple layers improves model performance for a variety of languages. Architecture consists of a stack of convolutional layers followed by highway layers and then an LSTM layer that predicts the next word based on history. Similar architectures have since been used to make significant improvements in large-format speech modeling (Jozefowicz et al., 2016) and character-level machine translation (Lee et al., 2016). Highway layers with coupled gates have been used in all of these studies to make significant improvements in the studies."}, {"heading": "6 CONCLUSION", "text": "This paper offers a new perspective on Highway and Residual networks as performing unrolled iterative estimation. As an extension of the popular representation view, it contrasts with the optimization perspective from which these architectures were originally introduced. According to the new view, successive layers (within a stage) work together to compute a single representation level. Therefore, the first layer already computes a rough estimate of this representation, which is then iteratively refined by the successive layers. Contrary to layers in a conventional neural network, from which each is computed a new representation, these layers retain their property identity. We have also shown that both residual and highway networks can be derived directly from this new perspective. This provides a unified theory from which these architectures can be understood as two approaches to the same problem. Moreover, this view provides a framework from which several surprising recent findings such as resistance to lesions, the benefits of layer fall, and the mitigation effects of layer changes are understood."}, {"heading": "ACKNOWLEDGEMENTS", "text": "The authors thank Faustino Gomez, Bas Steunebrink, Jonathan Masci, Sjoerd van Steenkiste and Christian Osendorfer for their feedback and support. This research was supported by the EU project \"INPUT\" (H2020-ICT-2015 grant number 687795)."}, {"heading": "A DERIVATION", "text": "A.1 OPTIMAL LINEAR ESTIMATORLet's assume two random variables A and B, which are both noisy measurements of a third (latent) random variable. C: E [A \u2212 C] = E [B \u2212 C] = 0 (10) We are looking for the corresponding variances Var [A \u2212 C] = \u03c32A and Var [B \u2212 C] = 0 (unbiased) and Kovarianzcov [A, B] = 2AB. We are looking for the linear estimator q (A, B) = q0 + q1A + q2B of C with E [q \u2212 C] = 0 (unbiased), the minimum variance.E [q (A, B) \u2212 C] = 0 E [q0 + q1A + q1B + q2B + q2B \u2212 q2qar [q1A + q1A] = 0E [q1A + q1A \u2212 q1C + q2C + q2C + 2C + 2C is)."}], "references": [{"title": "Multi-Residual Networks", "author": ["Abdi", "Masoud", "Nahavandi", "Saeid"], "venue": "[cs],", "citeRegEx": "Abdi et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Abdi et al\\.", "year": 2016}, {"title": "Deep Learning Methods and Applications", "author": ["Deng", "Li", "Yu", "Dong"], "venue": "Foundations and Trends in Signal Processing,", "citeRegEx": "Deng et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Deng et al\\.", "year": 2014}, {"title": "Deep Learning. Book in preparation for", "author": ["Goodfellow", "Ian", "Bengio", "Yoshua", "Courville", "Aaron"], "venue": null, "citeRegEx": "Goodfellow et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Goodfellow et al\\.", "year": 2016}, {"title": "Deep Residual Learning for Image Recognition", "author": ["He", "Kaiming", "Zhang", "Xiangyu", "Ren", "Shaoqing", "Sun", "Jian"], "venue": "[cs],", "citeRegEx": "He et al\\.,? \\Q2015\\E", "shortCiteRegEx": "He et al\\.", "year": 2015}, {"title": "Identity Mappings in Deep Residual Networks", "author": ["He", "Kaiming", "Zhang", "Xiangyu", "Ren", "Shaoqing", "Sun", "Jian"], "venue": "In Computer Vision\u2013ECCV", "citeRegEx": "He et al\\.,? \\Q2016\\E", "shortCiteRegEx": "He et al\\.", "year": 2016}, {"title": "Untersuchungen zu dynamischen neuronalen Netzen", "author": ["Hochreiter", "Sepp"], "venue": "Diploma, Technische Universita\u0308t Mu\u0308nchen, pp", "citeRegEx": "Hochreiter and Sepp.,? \\Q1991\\E", "shortCiteRegEx": "Hochreiter and Sepp.", "year": 1991}, {"title": "Long short-term memory", "author": ["Hochreiter", "Sepp", "Schmidhuber", "J\u00fcrgen"], "venue": "Neural computation,", "citeRegEx": "Hochreiter et al\\.,? \\Q1997\\E", "shortCiteRegEx": "Hochreiter et al\\.", "year": 1997}, {"title": "Densely Connected Convolutional Networks. arXiv:1608.06993 [cs], August 2016a", "author": ["Huang", "Gao", "Liu", "Zhuang", "Weinberger", "Kilian Q"], "venue": null, "citeRegEx": "Huang et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Huang et al\\.", "year": 2016}, {"title": "Deep Networks with Stochastic Depth", "author": ["Huang", "Gao", "Sun", "Yu", "Liu", "Zhuang", "Sedra", "Daniel", "Weinberger", "Kilian"], "venue": null, "citeRegEx": "Huang et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Huang et al\\.", "year": 2016}, {"title": "Receptive fields, binocular interaction and functional architecture in the cat\u2019s visual cortex", "author": ["Hubel", "David H", "Wiesel", "Torsten N"], "venue": "The Journal of physiology,", "citeRegEx": "Hubel et al\\.,? \\Q1962\\E", "shortCiteRegEx": "Hubel et al\\.", "year": 1962}, {"title": "Exploring the limits of language modeling", "author": ["Jozefowicz", "Rafal", "Vinyals", "Oriol", "Schuster", "Mike", "Shazeer", "Noam", "Wu", "Yonghui"], "venue": "arXiv preprint arXiv:1602.02410,", "citeRegEx": "Jozefowicz et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Jozefowicz et al\\.", "year": 2016}, {"title": "Character-aware neural language models", "author": ["Kim", "Yoon", "Jernite", "Yacine", "Sontag", "David", "Rush", "Alexander M"], "venue": "arXiv preprint arXiv:1508.06615,", "citeRegEx": "Kim et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Kim et al\\.", "year": 2015}, {"title": "Fully Character-Level Neural Machine Translation without Explicit Segmentation", "author": ["Lee", "Jason", "Cho", "Kyunghyun", "Hofmann", "Thomas"], "venue": "arXiv preprint arXiv:1610.03017,", "citeRegEx": "Lee et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Lee et al\\.", "year": 2016}, {"title": "Deep learning in neural networks: An overview", "author": ["Schmidhuber", "J\u00fcrgen"], "venue": "Neural Networks,", "citeRegEx": "Schmidhuber and J\u00fcrgen.,? \\Q2015\\E", "shortCiteRegEx": "Schmidhuber and J\u00fcrgen.", "year": 2015}, {"title": "Training Very Deep Networks", "author": ["Srivastava", "Rupesh K", "Greff", "Klaus", "Schmidhuber", "Juergen"], "venue": "Advances in Neural Information Processing Systems", "citeRegEx": "Srivastava et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Srivastava et al\\.", "year": 2015}, {"title": "Highway Networks. arXiv:1505.00387 [cs], May 2015b", "author": ["Srivastava", "Rupesh Kumar", "Greff", "Klaus", "Schmidhuber", "J\u00fcrgen"], "venue": null, "citeRegEx": "Srivastava et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Srivastava et al\\.", "year": 2015}, {"title": "Inception-v4, InceptionResNet and the Impact of Residual Connections on Learning", "author": ["Szegedy", "Christian", "Ioffe", "Sergey", "Vanhoucke", "Vincent", "Alemi", "Alex"], "venue": "[cs],", "citeRegEx": "Szegedy et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Szegedy et al\\.", "year": 2016}, {"title": "Residual Networks are Exponential Ensembles of Relatively Shallow Networks", "author": ["Veit", "Andreas", "Wilber", "Michael", "Belongie", "Serge"], "venue": "[cs],", "citeRegEx": "Veit et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Veit et al\\.", "year": 2016}, {"title": "Visualizing and understanding convolutional networks", "author": ["Zeiler", "Matthew D", "Fergus", "Rob"], "venue": "In European Conference on Computer Vision,", "citeRegEx": "Zeiler et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Zeiler et al\\.", "year": 2014}], "referenceMentions": [{"referenceID": 3, "context": ", 2015a) and Residual networks (He et al., 2015) which, for the first time, enabled the training of feedforward networks with dozens to hundreds of layers using simple gradient descent.", "startOffset": 31, "endOffset": 48}, {"referenceID": 2, "context": "Deep learning can be thought of as learning many levels of representation of the input which form a hierarchy of concepts (Deng & Yu, 2014; Goodfellow et al., 2016; LeCun et al., 2015) (But note that this is not the only view: cf.", "startOffset": 122, "endOffset": 184}, {"referenceID": 2, "context": "Deep learning can be thought of as learning many levels of representation of the input which form a hierarchy of concepts (Deng & Yu, 2014; Goodfellow et al., 2016; LeCun et al., 2015) (But note that this is not the only view: cf. Schmidhuber (2015)).", "startOffset": 140, "endOffset": 250}, {"referenceID": 2, "context": "Deep learning can be thought of as learning many levels of representation of the input which form a hierarchy of concepts (Deng & Yu, 2014; Goodfellow et al., 2016; LeCun et al., 2015) (But note that this is not the only view: cf. Schmidhuber (2015)). With fixed computational budget, deeper architectures are believed to possess greater representational power and, consequently, higher performance than shallower models. Intuitively, each layer of a deep neural network computes a new level of representation. For convolutional networks, Zeiler & Fergus (2014) visualized the features computed by each layer, and demonstrated that they in fact become increasingly abstract with depth.", "startOffset": 140, "endOffset": 562}, {"referenceID": 2, "context": "Deep learning can be thought of as learning many levels of representation of the input which form a hierarchy of concepts (Deng & Yu, 2014; Goodfellow et al., 2016; LeCun et al., 2015) (But note that this is not the only view: cf. Schmidhuber (2015)). With fixed computational budget, deeper architectures are believed to possess greater representational power and, consequently, higher performance than shallower models. Intuitively, each layer of a deep neural network computes a new level of representation. For convolutional networks, Zeiler & Fergus (2014) visualized the features computed by each layer, and demonstrated that they in fact become increasingly abstract with depth. We refer to this way of thinking about neural networks as the representation view, which probably dates back to Hubel & Wiesel (1962). The representation view links the layers in a network to the abstraction levels of their representations, and as such represents a pervasive assumption in many recent publications including He et al.", "startOffset": 140, "endOffset": 820}, {"referenceID": 2, "context": "Deep learning can be thought of as learning many levels of representation of the input which form a hierarchy of concepts (Deng & Yu, 2014; Goodfellow et al., 2016; LeCun et al., 2015) (But note that this is not the only view: cf. Schmidhuber (2015)). With fixed computational budget, deeper architectures are believed to possess greater representational power and, consequently, higher performance than shallower models. Intuitively, each layer of a deep neural network computes a new level of representation. For convolutional networks, Zeiler & Fergus (2014) visualized the features computed by each layer, and demonstrated that they in fact become increasingly abstract with depth. We refer to this way of thinking about neural networks as the representation view, which probably dates back to Hubel & Wiesel (1962). The representation view links the layers in a network to the abstraction levels of their representations, and as such represents a pervasive assumption in many recent publications including He et al. (2015) who describe the success of their Residual networks like this: \u201cSolely due to our extremely deep representations, we obtain a 28% relative improvement on the COCO object detection dataset.", "startOffset": 140, "endOffset": 1028}, {"referenceID": 3, "context": "The latter have been widely successful, advancing the state of the art on many benchmarks and winning several pattern recognition competitions (He et al., 2015), while Highway networks have been used to improve language modeling and translation (Kim et al.", "startOffset": 143, "endOffset": 160}, {"referenceID": 11, "context": ", 2015), while Highway networks have been used to improve language modeling and translation (Kim et al., 2015; Lee et al., 2016).", "startOffset": 92, "endOffset": 128}, {"referenceID": 12, "context": ", 2015), while Highway networks have been used to improve language modeling and translation (Kim et al., 2015; Lee et al., 2016).", "startOffset": 92, "endOffset": 128}, {"referenceID": 17, "context": "For example, it has been reported that removing almost any layer from a trained Residual or Highway network has only minimal effect on its overall performance (Srivastava et al., 2015b; Veit et al., 2016).", "startOffset": 159, "endOffset": 204}, {"referenceID": 17, "context": "It has been argued that ResNets are better understood as ensembles of shallow networks (Huang et al., 2016b; Veit et al., 2016; Abdi & Nahavandi, 2016).", "startOffset": 87, "endOffset": 151}, {"referenceID": 3, "context": "2015a, May) and Residual networks (ResNets; He et al. 2015, December). The latter have been widely successful, advancing the state of the art on many benchmarks and winning several pattern recognition competitions (He et al., 2015), while Highway networks have been used to improve language modeling and translation (Kim et al., 2015; Lee et al., 2016). Both architectures have been introduced with the explicit goal of training deeper models. There are, however, some surprising findings that seem to contradict the applicability of the representation view to these very deep networks. For example, it has been reported that removing almost any layer from a trained Residual or Highway network has only minimal effect on its overall performance (Srivastava et al., 2015b; Veit et al., 2016). This idea has been extended to a layerwise dropout, as a regularizer for ResNets (Huang et al., 2016b). But if each layer supposedly builds a new level of representation from the previous one, then removing any layer should critically disrupt the input for the following layer. So how is it possible that doing so seems to have only a negligible effect on the network output? Veit et al. (2016) even demonstrated that shuffling some of the layers in a trained ResNet barely affects performance.", "startOffset": 44, "endOffset": 1188}, {"referenceID": 14, "context": "The analysis in Srivastava et al. (2015a) shows that in trained Highway networks, the activity of the transform gates is often sparse for each individual sample, while their average activity over all training samples is non-sparse.", "startOffset": 16, "endOffset": 42}, {"referenceID": 3, "context": "We refer to the building blocks of a ResNet\u2014a few layers with an identity skip connection\u2014as a Residual block (He et al., 2015).", "startOffset": 110, "endOffset": 127}, {"referenceID": 13, "context": "This is in fact what Veit et al. (2016) find for the 15-layer VGG network on CIFAR-10: removing any layer from the trained network sets the classification error to around 90%.", "startOffset": 21, "endOffset": 40}, {"referenceID": 12, "context": "But the lesioning studies conducted on Highway networks Srivastava et al. (2015a) and ResNets Veit et al.", "startOffset": 56, "endOffset": 82}, {"referenceID": 12, "context": "But the lesioning studies conducted on Highway networks Srivastava et al. (2015a) and ResNets Veit et al. (2016) paint an entirely different picture: only a minor drop in performance is observed for any removed layer.", "startOffset": 56, "endOffset": 113}, {"referenceID": 7, "context": "Huang et al. (2016b) take lesioning one step further and drop out entire ResNet layers as a regularizer during training.", "startOffset": 0, "endOffset": 21}, {"referenceID": 17, "context": "The link between layers and representation levels may be most clearly challenged by the experiment in Veit et al. (2016) where the layers of a trained 110-layer ResNet are reshuffled.", "startOffset": 102, "endOffset": 121}, {"referenceID": 17, "context": "The link between layers and representation levels may be most clearly challenged by the experiment in Veit et al. (2016) where the layers of a trained 110-layer ResNet are reshuffled. Remarkably, error increases smoothly with the amount of reshuffling, and many re-orderings result only in a small increase in error. Note, however, that only layers within a stage are reshuffled, since the dimensionality of the swapped layers must match. Veit et al. (2016) take these results as evidence that ResNets behave as ensembles of exponentially many shallow networks.", "startOffset": 102, "endOffset": 458}, {"referenceID": 2, "context": "A recent deep learning textbook Goodfellow et al. (2016) explicitly states: \u201c[.", "startOffset": 32, "endOffset": 57}, {"referenceID": 3, "context": "To empirically test this claim, we extract the intermediate layer outputs for 5000 validation set images using the 50-layer ResNet trained on the ILSVRC-2015 dataset from He et al. (2015). These are then used to compute the empirical mean and standard deviation of the estimation error over the validation subset, for all blocks in the four Residual stages in the network.", "startOffset": 171, "endOffset": 188}, {"referenceID": 3, "context": "ResNet-151 (He et al., 2015) and many other derived architectures share some common characteristics: They are divided into stages of Residual blocks that share the same dimensionality.", "startOffset": 11, "endOffset": 28}, {"referenceID": 14, "context": "But we would expect these effects to be moderate, which is indeed what has been found in Srivastava et al. (2015a); Veit et al.", "startOffset": 89, "endOffset": 115}, {"referenceID": 14, "context": "But we would expect these effects to be moderate, which is indeed what has been found in Srivastava et al. (2015a); Veit et al. (2016).", "startOffset": 89, "endOffset": 135}, {"referenceID": 16, "context": "task\u2014shallower networks have already outperformed deep Residual networks on all original Residual network benchmarks (Huang et al., 2016a; Szegedy et al., 2016).", "startOffset": 117, "endOffset": 160}, {"referenceID": 3, "context": "Instead, our goal is to fairly compare the two architectures, and test the following claims regarding deep convolutional Highway networks (He et al., 2015; 2016; Veit et al., 2016):", "startOffset": 138, "endOffset": 180}, {"referenceID": 17, "context": "Instead, our goal is to fairly compare the two architectures, and test the following claims regarding deep convolutional Highway networks (He et al., 2015; 2016; Veit et al., 2016):", "startOffset": 138, "endOffset": 180}, {"referenceID": 3, "context": "We train a 50-layer convolutional Highway network based on the 50-layer Residual network from He et al. (2015). The design of the two networks are identical (including use of batch normalization after every convolution operation), except that unlike Residual blocks, the Highway blocks use two sets of layers to learn both H and T and then combine them using the coupled Highway formulation.", "startOffset": 94, "endOffset": 111}, {"referenceID": 3, "context": "The mismatch between the results above and claims 1 and 2 made by He et al. (2016) can be explained based on the importance of having sufficiently expressive transform gates.", "startOffset": 66, "endOffset": 83}, {"referenceID": 3, "context": "The mismatch between the results above and claims 1 and 2 made by He et al. (2016) can be explained based on the importance of having sufficiently expressive transform gates. For experiments with Highway networks (which they refer to as Residual networks with exclusive gating) He et al. (2016) used 1 \u00d7 1 convolutions for the transform gate, instead of having the same receptive fields for the gates as the primary transformation (H), as done by Srivastava et al.", "startOffset": 66, "endOffset": 295}, {"referenceID": 3, "context": "The mismatch between the results above and claims 1 and 2 made by He et al. (2016) can be explained based on the importance of having sufficiently expressive transform gates. For experiments with Highway networks (which they refer to as Residual networks with exclusive gating) He et al. (2016) used 1 \u00d7 1 convolutions for the transform gate, instead of having the same receptive fields for the gates as the primary transformation (H), as done by Srivastava et al. (2015a). This change in design appears to be the primary cause of instabilities in learning since the gates can no longer function effectively.", "startOffset": 66, "endOffset": 473}, {"referenceID": 10, "context": "Similar architectures have since been utilized for obtaining substantial improvements for large-scale language modeling (Jozefowicz et al., 2016) and character level machine translation (Lee et al.", "startOffset": 120, "endOffset": 145}, {"referenceID": 12, "context": ", 2016) and character level machine translation (Lee et al., 2016).", "startOffset": 48, "endOffset": 66}, {"referenceID": 10, "context": "Kim et al. (2015) have shown that utilizing a few Highway fully connected layers instead of conventional plain layers improves model performance for a variety of languages.", "startOffset": 0, "endOffset": 18}, {"referenceID": 10, "context": "Similar architectures have since been utilized for obtaining substantial improvements for large-scale language modeling (Jozefowicz et al., 2016) and character level machine translation (Lee et al., 2016). Highway layers with coupled gates have been used in all these studies. Only two to four Highway layers were necessary to obtain significant modeling improvements in the studies above. Thus, it is reasonable to assume that the central advantage of using Highway layers for this task is not easing of credit assignment over depth, but an improved modeling bias. To test how well Residual and other variants of Highway networks perform, we compare several language models trained on the Penn Treebank dataset using the same setup and code provided by Kim et al. (2015). We use the LSTM-Char-Large model, only changing the two Highway layers to different variants.", "startOffset": 121, "endOffset": 772}, {"referenceID": 3, "context": "Residual The Residual form from He et al. (2015), in which both transform and carry gate are always one.", "startOffset": 32, "endOffset": 49}, {"referenceID": 11, "context": "Table 1: Comparison of various variants of the Highway formulation for character-aware neural language models introduced by Kim et al. (2015).", "startOffset": 124, "endOffset": 142}], "year": 2016, "abstractText": "The past year saw the introduction of new architectures such as Highway networks (Srivastava et al., 2015a) and Residual networks (He et al., 2015) which, for the first time, enabled the training of feedforward networks with dozens to hundreds of layers using simple gradient descent. While depth of representation has been posited as a primary reason for their success, there are indications that these architectures defy a popular view of deep learning as a hierarchical computation of increasingly abstract features at each layer. In this report, we argue that this view is incomplete and does not adequately explain several recent findings. We propose an alternative viewpoint based on unrolled iterative estimation\u2014a group of successive layers iteratively refine their estimates of the same features instead of computing an entirely new representation. We demonstrate that this viewpoint directly leads to the construction of Highway and Residual networks. Finally we provide preliminary experiments to discuss the similarities and differences between the two architectures.", "creator": "LaTeX with hyperref package"}}}