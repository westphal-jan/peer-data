{"id": "1612.07837", "review": {"conference": "iclr", "VERSION": "v1", "DATE_OF_SUBMISSION": "22-Dec-2016", "title": "SampleRNN: An Unconditional End-to-End Neural Audio Generation Model", "abstract": "In this paper we propose a novel model for unconditional audio generation based on generating one audio sample at a time. We show that our model, which profits from combining memory-less modules, namely autoregressive multilayer perceptrons, and stateful recurrent neural networks in a hierarchical structure is able to capture underlying sources of variations in the temporal sequences over very long time spans, on three datasets of different nature. Human evaluation on the generated samples indicate that our model is preferred over competing models. We also show how each component of the model contributes to the exhibited performance.", "histories": [["v1", "Thu, 22 Dec 2016 23:28:47 GMT  (819kb,D)", "http://arxiv.org/abs/1612.07837v1", null], ["v2", "Sat, 11 Feb 2017 20:04:46 GMT  (819kb,D)", "http://arxiv.org/abs/1612.07837v2", "Published as a conference paper at ICLR 2017"]], "reviews": [], "SUBJECTS": "cs.SD cs.AI", "authors": ["soroush mehri", "kundan kumar", "ishaan gulrajani", "rithesh kumar", "shubham jain", "jose sotelo", "aaron courville", "yoshua bengio"], "accepted": true, "id": "1612.07837"}, "pdf": {"name": "1612.07837.pdf", "metadata": {"source": "CRF", "title": "SAMPLERNN: AN UNCONDITIONAL END-TO-END NEURAL AUDIO GENERATION MODEL", "authors": ["Soroush Mehri", "Kundan Kumar", "Ishaan Gulrajani", "Aaron Courville"], "emails": [], "sections": [{"heading": "1 INTRODUCTION", "text": "This year it has come to the point that it has never come as far as this year."}, {"heading": "2 SAMPLERNN MODEL", "text": "In this paper, we propose SampleRNN (shown in Fig. 1), a density model for individual samples. SampleRNN models the probability of a sequence of waveform samples X = {x1, x2,.., xT} (a random variable on input data sequences) as a product of the probabilities of each sample conditioned on all previous samples: p (X) = T \u2212 1 \u0441 i = 0 p (xi + 1 | x1,..., xi) (1) (1) RNNs are commonly used to model sequential data that can be formulated as follows: ht = RNNCell (ht \u2212 1, xi = t) (2) p (xi + 1 | x1,... xi) = Softmax (MLP (ht)) (3) with RNNCell as one of the known memory cells (Section 3). However, raw temporary signals are difficult to model because they contain very different structures on modules with different levels of hierarchies between them and resolution."}, {"heading": "2.1 FRAME-LEVEL MODULES", "text": "Instead of working on individual samples, the higher modules in SampleRNN work on non-overlapping frames of FS (k) (\"Frame Size\") samples at the lowest level in the hierarchy at a time (frames referred to by f (k)). Each frame-level module is a deep RNN that summarizes the history of its input into a conditioning vector for the next module down. The variable number of frames we condition up to timestep t \u2212 1 is expressed by a hidden state or memory h (k) t, where t is related to the clock rate at that level. RNN performs a memory update to timestep t as a function of the previous memory h (k) t \u2212 1 and an input. This input for top tier k = K is simply the input frame (k). For middle tiers (1 < < < < < < K < < K < K; K) this is a conditioner < < K < K < K < K; K."}, {"heading": "2.2 SAMPLE-LEVEL MODULE", "text": "The lowest module (Tier k = 1; Eqs. 7-9) in the sample RNN hierarchy outputs a distribution over a sample xi + 1, conditioned by the preceding samples FS (1) and a vector c (k = 2) i from the next higher module encoding information about the sequence before this frame. Since FS (1) is usually a small value and correlations in nearby samples can easily be modelled by a simple memory-less module, we implement it with a multi-layer perceptron (MLP) instead of RNN, which slightly accelerates training. Assuming that ei xi can achieve a conditional distribution after passing through the embedding layer (Section 2.2.1) by flattening a gap ([ei \u2212 FS (1),)."}, {"heading": "2.2.1 OUTPUT QUANTIZATION", "text": "According to van den Oord et al. (2016), the sample-level module models its output as a Q-Way discrete distribution using possible quantified values of xi (i.e., the output layer of the MLP is a Q-Way Softmax).To demonstrate the importance of a discrete output distribution, we apply the same architecture to real data by replacing the Q-Way Softmax with a Gaussian Mixture Models (GMM) output distribution.Table 2 shows that our model exceeds an RNN baseline, even if both models use real output values. However, samples from the real model are almost indistinguishable from random notes. In this work, we use linear quantization with q = 256, which results in a pro-sample bit depth of 8. Intuitively, we realized that even linear reduction of the bit depth (resolution of each audio sample) from 16 to 8 can facilitate the optimization process, while samples are still artificially high and have adequate quality."}, {"heading": "2.2.2 CONDITIONALLY INDEPENDENT SAMPLE OUTPUTS", "text": "To demonstrate the importance of an autoregressive module at the sample level, we try to replace it with \"multi-softmax\" (see Table 4), where the prediction of each sample xi depends only on the conditioning vector c from Eq. 9. In this configuration, the model outputs an entire frame of FS (1) samples at once, with all samples in a frame modeled as conditionally independent of each other. We find that this multi-softmax model (which lacks an autoregressive module at the sample level) performs significantly worse in terms of log probability and does not generate convincing samples. This suggests that modelling the common distribution of acoustic samples within each frame is very important in order to achieve good acoustic generation."}, {"heading": "2.3 TRUNCATED BPTT", "text": "Training recursive neural networks on long sequences can be very expensive in arithmetical terms. Oord et al. (2016) avoids this problem by using a stack of dilated turns instead of recursive connections. However, if they can be trained efficiently, recursive networks have proven to be very powerful and expressive sequence models. We enable efficient training of our recursive model by means of abbreviated backpropagation over time, dividing each sequence into short sub-sequences and spreading gradients only to the beginning of each sub-sequence. We experiment with different sub-sequence lengths and show that we are able to train our networks that model very long-term dependencies even though they propagate backwards through relatively short sub-sequences. Table 3 shows that by increasing the sub-sequence length, performance increases along with migratory memory usage and convergence time. Nevertheless, it is noteworthy that our best models are trained on sub-sequences of the same length as this word-sequence, which is a small 532 millisecond long structure despite having been trained."}, {"heading": "3 EXPERIMENTS AND RESULTS", "text": "This year, it has reached the point where it will be able to put itself at the top of the list."}, {"heading": "3.1 WAVENET RE-IMPLEMENTATION", "text": "We implemented the WaveNet architecture as described in Oord et al. (2016). Ideally, we would have wanted to replicate their model exactly, but due to the lack of detail of the architecture and hyperparameters and limited computing power available to us, we made our own design selections so3Courtesy of UbisoftTable 3: Effect of subsequence length on NLL (bits per audio sample) computing on the Blizzard validation set. Subsequence Length 32 64 128 256 512NLL Validation 1,575 1,468 1,412 1,391 1,364that the model would fit on one GPU while having a receptive field of around 250 milliseconds, while having a reasonable number of updates per unit time. Although our model is very similar to WaveNet, the design decisions, such as number of convolution filters in each dilated constellation layer, are length of the target sequence to train simultaneously."}, {"heading": "3.2 HUMAN EVALUATION", "text": "Apart from reporting NLL, AB conducted preference tests for random samples from four models trained on the Blizzard dataset. For the unconditional generation of language that sounds like marbles at best, this type of test is the one that is more appropriate. Competitive models were RNN, SampleRNN (2-tier), SampleRNN (3-tier) and our implementation of WaveNet. The rest of the models were excluded because the quality of the samples was definitely lower and also to keep the number of comparative tests manageable. We will also release the samples used in this test.All samples should have the same volume. Each user will then be shown a series of twenty sample pairs, each with a random pair. Each pair had samples from two different models. The human evaluator is asked to listen to the samples used in this test and had the opportunity to choose between the two or no models."}, {"heading": "3.3 QUANTIFYING INFORMATION RETENTION", "text": "For the last experiment, we are interested in measuring the memory span of the model. We trained our model, SampleRNN (3-tier), with the best hyperparameters on a dataset of 2 speakers that read audiobooks, one male and one female, with a mean basic frequency of 125.3 and 201.8Hz. Each speaker has about 10 hours of audio in the dataset, which was preprocessed similarly to Blizzard. We observed that he has learned to remain consistent by generating samples from the same speaker without knowledge of the speaker ID or other conditioning information. This effect is more apparent here compared to the unbalanced onomatopoeias, which sometimes mix two different categories of noises. Another experiment was conducted to test the effect of memory and study the effective memory horizon. We inject 1 second silence in the middle of the sampling process to see if he will or she will remember."}, {"heading": "4 RELATED WORK", "text": "Our work is related to earlier work on auto-regressive multi-layered neural networks, starting with Bengio & Bengio (1999), then NADE (Larochelle & Murray, 2011) and more recently PixelRNN (van den Oord et al., 2016). Similar to how they simulate the common distribution across units of data (e.g. words in sentences, pixels in images, etc.) by auto-regressive decomposition, we transform the common distribution of acoustic samples by equation. 1.The idea of running a part of the model at different clock speeds is related to multi-scale RNNNs (Schmidhuber, 1992; El Hihi & Bengio, 1995; Koutnik et al., 2014; Sordoni et al., 2015; Serban et al., 2016).Chung et al al. (2015) also attempts to model the raw audio waveforms that, unlike traditional approaches, use spectral features such as Tokuda al."}, {"heading": "5 DISCUSSION AND CONCLUSION", "text": "We can show that a hierarchy of time scales and frequent updates will help solve the problem of modeling extremely high-resolution temporal data, which allows us to learn the data for this particular application directly from audio samples. We show that this model can be generalized well and samples can be generated on three sets of data that are inherently different. We also show that the samples generated by this model are preferred by human raters. Success in this application with a universal solution, as proposed here, opens up room for further improvement when specific expertise is applied. However, this method, which is proposed for the application of audio generation, can be easily adapted to other tasks that require learning to display sequential data with high time resolution and far-reaching complex structure."}, {"heading": "ACKNOWLEDGMENTS", "text": "The authors thank Joa o Felipe Santos and Kyle Kastner for insightful comments and discussions. We thank the Theano Development Team (2016) 4 and the MILA staff. We thank the following research funding and computer support agencies: NSERC, Calcul Que bec, Compute Canada, the Canada Research Chairs and CIFAR. This work was done in collaboration with Ubisoft.4http: / / deeplearning.net / software / theano /"}, {"heading": "APPENDIX A", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "A MODEL VARIANT: SAMPLERNN-WAVENET HYBRID", "text": "SampleRNN WaveNet model has two modules that operate at two different clock speeds. The slower clock frequency module (Frame Level Module) sees one frame (each of which is FS size) at a time, while the component with a higher clock speed (Sample Level Component) sees one acoustic sample at a time, i.e. the ratio of clock speeds for these two modules would be the size of a single frame. The number of sequential steps for Frame Level Component would be FS times lower. We repeat the output of each step of the Frame Level Component FS times so that the number of time steps for the output of both components match. Output of these two modules is linked for each time step that is driven independently by non-linearity for each time step before we generate the final output.In our experiments we kept the size of a single frame (FS 128)."}], "references": [{"title": "Modeling high-dimensional discrete data with multi-layer neural networks", "author": ["Yoshua Bengio", "Samy Bengio"], "venue": "In NIPS,", "citeRegEx": "Bengio and Bengio.,? \\Q1999\\E", "shortCiteRegEx": "Bengio and Bengio.", "year": 1999}, {"title": "Random search for hyper-parameter optimization", "author": ["James Bergstra", "Yoshua Bengio"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Bergstra and Bengio.,? \\Q2012\\E", "shortCiteRegEx": "Bergstra and Bengio.", "year": 2012}, {"title": "Unsupervised learning of auditory filter banks using non-negative matrix factorisation", "author": ["Alexander Bertrand", "Kris Demuynck", "Veronique Stouten"], "venue": "IEEE International Conference on Acoustics, Speech and Signal Processing,", "citeRegEx": "Bertrand et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Bertrand et al\\.", "year": 2008}, {"title": "Empirical evaluation of gated recurrent neural networks on sequence modeling", "author": ["Junyoung Chung", "Caglar Gulcehre", "KyungHyun Cho", "Yoshua Bengio"], "venue": "arXiv preprint arXiv:1412.3555,", "citeRegEx": "Chung et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Chung et al\\.", "year": 2014}, {"title": "A recurrent latent variable model for sequential data", "author": ["Junyoung Chung", "Kyle Kastner", "Laurent Dinh", "Kratarth Goel", "Aaron C Courville", "Yoshua Bengio"], "venue": "In Advances in neural information processing systems,", "citeRegEx": "Chung et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Chung et al\\.", "year": 2015}, {"title": "Learning to generate chairs, tables and cars with convolutional networks. 2016", "author": ["Alexey Dosovitskiy", "Jost Springenberg", "Maxim Tatarchenko", "Thomas Brox"], "venue": null, "citeRegEx": "Dosovitskiy et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Dosovitskiy et al\\.", "year": 2016}, {"title": "Hierarchical recurrent neural networks for long-term dependencies", "author": ["Salah El Hihi", "Yoshua Bengio"], "venue": "In NIPS,", "citeRegEx": "Hihi and Bengio.,? \\Q1995\\E", "shortCiteRegEx": "Hihi and Bengio.", "year": 1995}, {"title": "Long short-term memory in recurrent neural networks", "author": ["Felix Gers"], "venue": "PhD thesis, Universita\u0308t Hannover,", "citeRegEx": "Gers.,? \\Q2001\\E", "shortCiteRegEx": "Gers.", "year": 2001}, {"title": "Generating sequences with recurrent neural networks", "author": ["Alex Graves"], "venue": "arXiv preprint arXiv:1308.0850,", "citeRegEx": "Graves.,? \\Q2013\\E", "shortCiteRegEx": "Graves.", "year": 2013}, {"title": "Delving deep into rectifiers: Surpassing human-level performance on imagenet classification", "author": ["Kaiming He", "Xiangyu Zhang", "Shaoqing Ren", "Jian Sun"], "venue": "In Proceedings of the IEEE International Conference on Computer Vision, pp", "citeRegEx": "He et al\\.,? \\Q2015\\E", "shortCiteRegEx": "He et al\\.", "year": 2015}, {"title": "Long short-term memory", "author": ["Sepp Hochreiter", "J\u00fcrgen Schmidhuber"], "venue": "Neural computation,", "citeRegEx": "Hochreiter and Schmidhuber.,? \\Q1997\\E", "shortCiteRegEx": "Hochreiter and Schmidhuber.", "year": 1997}, {"title": "Web Audio Evaluation Tool: A browser-based listening test environment", "author": ["Nicholas Jillings", "David Moffat", "Brecht De Man", "Joshua D. Reiss"], "venue": "In 12th Sound and Music Computing Conference,", "citeRegEx": "Jillings et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Jillings et al\\.", "year": 2015}, {"title": "The unreasonable effectiveness of recurrent neural networks", "author": ["Andrej Karpathy"], "venue": "Andrej Karpathy blog,", "citeRegEx": "Karpathy.,? \\Q2015\\E", "shortCiteRegEx": "Karpathy.", "year": 2015}, {"title": "Adam: A method for stochastic optimization", "author": ["Diederik Kingma", "Jimmy Ba"], "venue": "arXiv preprint arXiv:1412.6980,", "citeRegEx": "Kingma and Ba.,? \\Q2014\\E", "shortCiteRegEx": "Kingma and Ba.", "year": 2014}, {"title": "A clockwork rnn", "author": ["Jan Koutnik", "Klaus Greff", "Faustino Gomez", "Juergen Schmidhuber"], "venue": "arXiv preprint arXiv:1402.3511,", "citeRegEx": "Koutnik et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Koutnik et al\\.", "year": 2014}, {"title": "The neural autoregressive distribution estimator", "author": ["Hugo Larochelle", "Iain Murray"], "venue": "In AISTATS,", "citeRegEx": "Larochelle and Murray.,? \\Q2011\\E", "shortCiteRegEx": "Larochelle and Murray.", "year": 2011}, {"title": "Unsupervised feature learning for audio classification using convolutional deep belief networks. In Advances in neural information processing", "author": ["Honglak Lee", "Peter Pham", "Yan Largman", "Andrew Y Ng"], "venue": null, "citeRegEx": "Lee et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Lee et al\\.", "year": 2009}, {"title": "Wavenet: A generative model for raw audio", "author": ["Aaron van den Oord", "Sander Dieleman", "Heiga Zen", "Karen Simonyan", "Oriol Vinyals", "Alex Graves", "Nal Kalchbrenner", "Andrew Senior", "Koray Kavukcuoglu"], "venue": "arXiv preprint arXiv:1609.03499,", "citeRegEx": "Oord et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Oord et al\\.", "year": 2016}, {"title": "The blizzard challenge 2013\u2013 indian language task", "author": ["Kishore Prahallad", "Anandaswarup Vadapalli", "Naresh Elluru", "G Mantena", "B Pulugundla", "P Bhaskararao", "HA Murthy", "S King", "V Karaiskos", "AW Black"], "venue": "In Blizzard Challenge Workshop 2013,", "citeRegEx": "Prahallad et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Prahallad et al\\.", "year": 2013}, {"title": "Weight normalization: A simple reparameterization to accelerate training of deep neural networks", "author": ["Tim Salimans", "Diederik P Kingma"], "venue": "arXiv preprint arXiv:1602.07868,", "citeRegEx": "Salimans and Kingma.,? \\Q2016\\E", "shortCiteRegEx": "Salimans and Kingma.", "year": 2016}, {"title": "Learning complex, extended sequences using the principle of history compression", "author": ["J\u00fcrgen Schmidhuber"], "venue": "Neural Computation,", "citeRegEx": "Schmidhuber.,? \\Q1992\\E", "shortCiteRegEx": "Schmidhuber.", "year": 1992}, {"title": "Building end-to-end dialogue systems using generative hierarchical neural network models", "author": ["Iulian V Serban", "Alessandro Sordoni", "Yoshua Bengio", "Aaron Courville", "Joelle Pineau"], "venue": "In Proceedings of the 30th AAAI Conference on Artificial Intelligence", "citeRegEx": "Serban et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Serban et al\\.", "year": 2016}, {"title": "Computation beyond the turing limit", "author": ["Hava T Siegelmann"], "venue": "In Neural Networks and Analog Computation,", "citeRegEx": "Siegelmann.,? \\Q1999\\E", "shortCiteRegEx": "Siegelmann.", "year": 1999}, {"title": "A hierarchical recurrent encoder-decoder for generative context-aware query suggestion", "author": ["Alessandro Sordoni", "Yoshua Bengio", "Hossein Vahabi", "Christina Lioma", "Jakob Grue Simonsen", "Jian-Yun Nie"], "venue": "In Proceedings of the 24th ACM International on Conference on Information and Knowledge Management,", "citeRegEx": "Sordoni et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Sordoni et al\\.", "year": 2015}, {"title": "Speech synthesis based on hidden markov models", "author": ["Keiichi Tokuda", "Yoshihiko Nankaku", "Tomoki Toda", "Heiga Zen", "Junichi Yamagishi", "Keiichiro Oura"], "venue": "Proceedings of the IEEE,", "citeRegEx": "Tokuda et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Tokuda et al\\.", "year": 2013}, {"title": "Pixel recurrent neural networks", "author": ["Aaron van den Oord", "Nal Kalchbrenner", "Koray Kavukcuoglu"], "venue": "arXiv preprint arXiv:1601.06759,", "citeRegEx": "Oord et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Oord et al\\.", "year": 2016}, {"title": "Multi-scale context aggregation by dilated convolutions", "author": ["Fisher Yu", "Vladlen Koltun"], "venue": "arXiv preprint arXiv:1511.07122,", "citeRegEx": "Yu and Koltun.,? \\Q2015\\E", "shortCiteRegEx": "Yu and Koltun.", "year": 2015}, {"title": "An empirical exploration of recurrent network architectures", "author": ["Wojciech Zaremba"], "venue": null, "citeRegEx": "Zaremba.,? \\Q2015\\E", "shortCiteRegEx": "Zaremba.", "year": 2015}], "referenceMentions": [{"referenceID": 8, "context": "We believe RNNs are well suited as they have been designed and are suited solutions for these tasks (see Graves (2013), Karpathy (2015), and Siegelmann (1999)).", "startOffset": 105, "endOffset": 119}, {"referenceID": 8, "context": "We believe RNNs are well suited as they have been designed and are suited solutions for these tasks (see Graves (2013), Karpathy (2015), and Siegelmann (1999)).", "startOffset": 105, "endOffset": 136}, {"referenceID": 8, "context": "We believe RNNs are well suited as they have been designed and are suited solutions for these tasks (see Graves (2013), Karpathy (2015), and Siegelmann (1999)).", "startOffset": 105, "endOffset": 159}, {"referenceID": 8, "context": "We believe RNNs are well suited as they have been designed and are suited solutions for these tasks (see Graves (2013), Karpathy (2015), and Siegelmann (1999)). However, in practice it is a known problem of these models to not scale well at such a high temporal resolution as is found when generating acoustic signals one sample at a time, e.g., 16000 times per second. This is one of the reasons that Oord et al. (2016) profits from other neural modules such as one presented by Yu & Koltun (2015) to show extremely good performance.", "startOffset": 105, "endOffset": 421}, {"referenceID": 8, "context": "We believe RNNs are well suited as they have been designed and are suited solutions for these tasks (see Graves (2013), Karpathy (2015), and Siegelmann (1999)). However, in practice it is a known problem of these models to not scale well at such a high temporal resolution as is found when generating acoustic signals one sample at a time, e.g., 16000 times per second. This is one of the reasons that Oord et al. (2016) profits from other neural modules such as one presented by Yu & Koltun (2015) to show extremely good performance.", "startOffset": 105, "endOffset": 499}, {"referenceID": 5, "context": "It was first demonstrated to work well in Dosovitskiy et al. (2016) and is a fairly common upsampling technique.", "startOffset": 42, "endOffset": 68}, {"referenceID": 17, "context": "Following van den Oord et al. (2016), the sample-level module models its output as a q-way discrete distribution over possible quantized values of xi (that is, the output layer of the MLP is a q-way Softmax).", "startOffset": 18, "endOffset": 37}, {"referenceID": 17, "context": "Oord et al. (2016) avoid this problem by using a stack of dilated convolutions instead of any recurrent connections.", "startOffset": 0, "endOffset": 19}, {"referenceID": 18, "context": "Blizzard which is a dataset presented by Prahallad et al. (2013) for speech synthesis task, contains 315 hours of a single female voice actor in English; however, for our experiments we are using only 20.", "startOffset": 41, "endOffset": 65}, {"referenceID": 3, "context": "We particularly explored two gated variants of RNNs\u2014Gated Recurrent Units (GRUs) (Chung et al., 2014) and Long Short Term Memory Units (LSTMs) (Hochreiter & Schmidhuber, 1997).", "startOffset": 81, "endOffset": 101}, {"referenceID": 3, "context": "We particularly explored two gated variants of RNNs\u2014Gated Recurrent Units (GRUs) (Chung et al., 2014) and Long Short Term Memory Units (LSTMs) (Hochreiter & Schmidhuber, 1997). For the case of LSTMs, the forget gate bias is initialized with a large positive value of 3, as recommended by Zaremba (2015) and Gers (2001), which has been shown to be beneficial for learning long-term dependencies.", "startOffset": 82, "endOffset": 303}, {"referenceID": 3, "context": "We particularly explored two gated variants of RNNs\u2014Gated Recurrent Units (GRUs) (Chung et al., 2014) and Long Short Term Memory Units (LSTMs) (Hochreiter & Schmidhuber, 1997). For the case of LSTMs, the forget gate bias is initialized with a large positive value of 3, as recommended by Zaremba (2015) and Gers (2001), which has been shown to be beneficial for learning long-term dependencies.", "startOffset": 82, "endOffset": 319}, {"referenceID": 3, "context": "We particularly explored two gated variants of RNNs\u2014Gated Recurrent Units (GRUs) (Chung et al., 2014) and Long Short Term Memory Units (LSTMs) (Hochreiter & Schmidhuber, 1997). For the case of LSTMs, the forget gate bias is initialized with a large positive value of 3, as recommended by Zaremba (2015) and Gers (2001), which has been shown to be beneficial for learning long-term dependencies. As for models that take real-valued input, e.g. the RNN-GMM and SampleRNN-GMM (with 4 components), normalization is applied per audio sample with the global mean and standard deviation obtained from the train split. For most of our experiments where the model demands discrete input, binning was applied per audio sample. All the models have been trained with teacher forcing and stochastic gradient decent (mini-batch size 128) to minimize the Negative Log-Likelihood (NLL) in bits per dimension (per audio sample). Gradients were hard-clipped to remain in [-1, 1] range. Update rules from the Adam optimizer (Kingma & Ba, 2014) (\u03b21 = 0.9, \u03b22 = 0.999, and = 1e\u22128) with an initial learning rate of 0.001 was used to adjust the parameters. For training each model, random search over hyper-parameter values (Bergstra & Bengio, 2012) was conducted. The initial RNN state of all the RNN-based models was always learnable. Weight Normalization (Salimans & Kingma, 2016) has been used for all the linear layers in the model (except for the embedding layer) to accelerate the training procedure. Size of the embedding layer was 256 and initialized by standard normal distribution. Orthogonal weight matrices used for hidden-to-hidden connections and other weight matrices initialized similar to He et al. (2015). In final model, we found GRU to work best (slightly better than LSTM).", "startOffset": 82, "endOffset": 1701}, {"referenceID": 17, "context": "We implemented the WaveNet architecture as described in Oord et al. (2016). Ideally, we would have liked to replicate their model exactly but owing to missing details of architecture and hyperparameters, as well as limited compute power at our disposal, we made our own design choices so Courtesy of Ubisoft", "startOffset": 56, "endOffset": 75}, {"referenceID": 11, "context": "We used the online tool made publicly available by Jillings et al. (2015). Results in Fig.", "startOffset": 51, "endOffset": 74}, {"referenceID": 20, "context": "The idea of having part of the model running at different clock rates is related to multi-scale RNNs (Schmidhuber, 1992; El Hihi & Bengio, 1995; Koutnik et al., 2014; Sordoni et al., 2015; Serban et al., 2016).", "startOffset": 101, "endOffset": 209}, {"referenceID": 14, "context": "The idea of having part of the model running at different clock rates is related to multi-scale RNNs (Schmidhuber, 1992; El Hihi & Bengio, 1995; Koutnik et al., 2014; Sordoni et al., 2015; Serban et al., 2016).", "startOffset": 101, "endOffset": 209}, {"referenceID": 23, "context": "The idea of having part of the model running at different clock rates is related to multi-scale RNNs (Schmidhuber, 1992; El Hihi & Bengio, 1995; Koutnik et al., 2014; Sordoni et al., 2015; Serban et al., 2016).", "startOffset": 101, "endOffset": 209}, {"referenceID": 21, "context": "The idea of having part of the model running at different clock rates is related to multi-scale RNNs (Schmidhuber, 1992; El Hihi & Bengio, 1995; Koutnik et al., 2014; Sordoni et al., 2015; Serban et al., 2016).", "startOffset": 101, "endOffset": 209}, {"referenceID": 17, "context": "Our work is closely related to WaveNet (Oord et al., 2016), which is why we have made the above comparisons, and makes it interesting to compare the effect of adding higher-level RNN stages working at a low resolution.", "startOffset": 39, "endOffset": 58}, {"referenceID": 2, "context": "Chung et al. (2015) also attempt to model raw audio waveforms which is in contrast to traditional approaches which use spectral features as in Tokuda et al.", "startOffset": 0, "endOffset": 20}, {"referenceID": 2, "context": "Chung et al. (2015) also attempt to model raw audio waveforms which is in contrast to traditional approaches which use spectral features as in Tokuda et al. (2013), Bertrand et al.", "startOffset": 0, "endOffset": 164}, {"referenceID": 2, "context": "(2013), Bertrand et al. (2008), and Lee et al.", "startOffset": 8, "endOffset": 31}, {"referenceID": 2, "context": "(2013), Bertrand et al. (2008), and Lee et al. (2009). Our work is closely related to WaveNet (Oord et al.", "startOffset": 8, "endOffset": 54}], "year": 2016, "abstractText": "In this paper we propose a novel model for unconditional audio generation based on generating one audio sample at a time. We show that our model, which profits from combining memory-less modules, namely autoregressive multilayer perceptrons, and stateful recurrent neural networks in a hierarchical structure is able to capture underlying sources of variations in the temporal sequences over very long time spans, on three datasets of different nature. Human evaluation on the generated samples indicate that our model is preferred over competing models. We also show how each component of the model contributes to the exhibited performance.", "creator": "LaTeX with hyperref package"}}}