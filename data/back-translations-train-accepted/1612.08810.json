{"id": "1612.08810", "review": {"conference": "ICML", "VERSION": "v1", "DATE_OF_SUBMISSION": "28-Dec-2016", "title": "The Predictron: End-To-End Learning and Planning", "abstract": "One of the key challenges of artificial intelligence is to learn models that are effective in the context of planning. In this document we introduce the predictron architecture. The predictron consists of a fully abstract model, represented by a Markov reward process, that can be rolled forward multiple \"imagined\" planning steps. Each forward pass of the predictron accumulates internal rewards and values over multiple planning depths. The predictron is trained end-to-end so as to make these accumulated values accurately approximate the true value function. We applied the predictron to procedurally generated random mazes and a simulator for the game of pool. The predictron yielded significantly more accurate predictions than conventional deep neural network architectures.", "histories": [["v1", "Wed, 28 Dec 2016 06:47:15 GMT  (962kb,D)", "http://arxiv.org/abs/1612.08810v1", null], ["v2", "Fri, 20 Jan 2017 14:57:31 GMT  (1537kb,D)", "http://arxiv.org/abs/1612.08810v2", null], ["v3", "Thu, 20 Jul 2017 09:21:54 GMT  (1303kb,D)", "http://arxiv.org/abs/1612.08810v3", "Camera-ready version, ICML 2017, with supplement"]], "reviews": [], "SUBJECTS": "cs.LG cs.AI cs.NE", "authors": ["david silver", "hado van hasselt", "matteo hessel", "tom schaul", "arthur guez", "tim harley", "gabriel dulac-arnold", "david p reichert", "neil rabinowitz", "andr\u00e9 barreto", "thomas degris"], "accepted": true, "id": "1612.08810"}, "pdf": {"name": "1612.08810.pdf", "metadata": {"source": "CRF", "title": null, "authors": ["David Silver", "Hado van Hasselt", "Matteo Hessel", "Tom Schaul", "Arthur Guez", "Tim Harley", "Gabriel Dulac-Arnold", "David Reichert", "Neil Rabinowitz", "Andre Barreto", "Thomas Degris"], "emails": ["davidsilver@google.com", "hado@google.com", "mtthss@google.com", "schaul@google.com", "aguez@google.com"], "sections": [{"heading": "1 INTRODUCTION", "text": "This year, the time has come for an agreement to be reached."}, {"heading": "2 BACKGROUND", "text": "We look at environments defined by an MRP with states s-S, e.g. a common configuration of a robot, or a history of raw input sensors. MRP is defined by a function, s-S, g-S (s-S, \u03b1), where s-S is the next state, r is the reward, and \u03b3 is the discount factor that can, for example, represent the non-termination probability for this transition. The process can be stochastical, since the IID noise \u03b1 can vary. Returning an MRP is the cumulative discounted reward over a single trajectory, gt = rt + 1 + \u03b3t + 1 + 2 + \u03b3t + 1\u03b3t + 2rt + 3 +... We look at a generalization of the MRP setting that includes vector-weighted rewards r, diagonal matrix disconstant r values (and vector-weighted g-S values)."}, {"heading": "3 PREDICTRON ARCHITECTURE", "text": "The predictron is composed of three main components: First, a state representation s = f (s), which transforms the raw input s (this could be a history of observations in the partially observed environment, for example, if f is a recursive network) into an abstract (internal, hidden) state s \u00b2, r, \u03b3 = m (s, \u03b2), which translates from the abstract state s + to the subsequent abstract state s \u00b2, rewards r and discounts. Third, a value function v, which outputs an estimate v = v (s) of future cumulative discounting of rewards from the internal state s upwards. The predictron is applied by rolling up its model to several \"planning steps\" to produce rewards, discounts and values. Together, the components of the predictron can be calculated to form many different predictions of real returns. We use superscripts to show k \u2022 the internal steps of the model (to have no connection to the necessary environment)."}, {"heading": "4 PREDICTRON LEARNING UPDATES", "text": "We are now considering how we can jointly optimize the parameters \u03b8 of all components f, m, v of the predicate. First, we will discuss how we can learn from Monte Carlo returns from the real environment. Afterwards, we will discuss internal consistency updates that can also be applied without real data."}, {"heading": "4.1 SUPERVISED (MONTE-CARLO) LEARNING WITH THE PREDICTRON", "text": "We can update all k-step preturns g0,.., gK towards a target result g, such as the Monte Carlo yield from the results of local episodes by minimizing an error in the mean square, L = 12 K \u00b2 k = 0 Ep [g] -Em [gk]. (5) This loss depends on the parameters of the value function, the model, and the state representation parameters, which we collectively call \"progress,\" and we can use the gradient of L to update them, e.g. by stochastic decrease of the gradient on the sample loss l = 12 K = 0 Ep \u2212 gk \u00b2 2 using the gradient l = K \u00b2 k = 0 (g \u2212 gk). (6) In stochastic models, two independent samples for gk and g \u00b2 s are needed to obtain impartial samples for the gradient value."}, {"heading": "4.2 CONSISTENCY (SEMI-SUPERVISED) LEARNING WITH THE PREDICTRON", "text": "Ideally, the predictron (f, m, v) learns predictrons, which are all the same in expectation of the true value function of the environment, Em [gk | s] = Ep [gt | st = s] = vp (s), in this case the predictrons must be equal in expectation, Em [g0 | s] = Em [g1 | s] =... = Em [gK | s]. This can be interpreted as satisfaction of a Bellman equation that unrolls K times according to the model m. Furthermore, each k-step predictron must be equal in expectation of the precourse, Em [gk | s] = Em [g.g.g.s] for any [g.g.s] parameters. All these consistency relationships between predictrons lead to additional constraints on the predictron."}, {"heading": "5 EXPERIMENTS", "text": "The first domain consists of randomly generated 20 x 20 labyrinths in which each place is either empty or contains a wall. Two places in a labyrinth are considered connected when they are both empty, and we can reach one of them by moving horizontally or vertically through adjacent empty cells. The goal is to predict for each of the places on the diagonal from top to left to bottom to right of the labyrinth whether the bottom right corner is connected to that place, as the entire labyrinth serves as an input image. Some of these predictions will be easy, for example, for places on the diagonal that themselves contain a wall, and for places near the bottom right. Many other predictive questions seem to require a simple algorithm, such as a form of flood filling or search; our hypothesis is that an internal model can learn to emulate such algorithms, where naive approximation can struggle."}, {"heading": "5.1 EXPLORING THE PREDICTRON ARCHITECTURE", "text": "Our first series of experiments examines three binary dimensions that distinguish the predictive power of standard deep networks. We compare eight prediction variants that correspond to the corners of the cube on the left, in Figure 3. The first dimension corresponds to the question of whether or not the predictive architecture uses the structure of an MRP model. In the case of MRP, internal advantages and discounts are ignored by setting their values to rk = 0 and \u03b3k = 1. The second dimension is whether or not a predictive pool is used to aggregate predictive results over predictions. If a predictive network is used, a predictive predictive turn is calculated, as in Section 3. Otherwise, average predictions are ignored by setting their values to quotk = 1 for k < K. In this case, the overall predictive power predictive power used is calculated based on the maximum weighting K."}, {"heading": "5.2 COMPARING THE PREDICTRON TO OTHER DEEP NETWORKS", "text": "Our second series of experiments compares the prediction results with prediction results and recurring deep learning architectures (with and without skip connections). We compare the corners of a new cube, as shown on the left in Figure 4, based on three different binary dimensions. The first dimension of this second cube is whether we are using a predictron or a (non-\u03bb, non-r, \u03b3) deep network that has no internal model and does not draw any output or lessons from intermediate predictions. We are using the most effective prediction results from the previous section, i.e., the (r, \u03b3,) predictrons with usage weighting. r, w, h sh, arin g, skip connections (r,) -predictronConvNetrecurrent ConvNetrecurrent ConvNetrecurrent ConvNetrecurrent w eighting0 1M 2M 4M 4M 5M 50.0001R MS, 10.00R E d"}, {"heading": "5.3 SEMI-SUPERVISED LEARNING BY CONSISTENCY", "text": "Semi-supervised learning is important because a general bottleneck in the application of machine learning in the real world is the difficulty of collecting labeled data when large amounts of unlabeled data often exist. We trained a complete (r, \u03b3, \u03bb) predictrine by alternating standard supervised updates with consistency updates achieved by stochastic minimization of consistency loss (8) on the unlabeled samples. For each supervised update, we apply either 0, 1, or 9 consistency updates. Figure 5 shows that performance improved monotonously with the number of consistency updates measured by the number of labeled samples consumed."}, {"heading": "5.4 ANALYSIS OF ADAPTIVE DEPTH", "text": "In principle, the predictron can adjust its depth to \"think more\" about some predictions than others, perhaps depending on the complexity of the underlying goal. We examine this by looking at qualitatively different types of prediction in the pool: ball collisions, track collisions, pocket balls, and entering or remaining in quadrants. For each prediction type, we look at several different time periods (determined by the real-world discount factors associated with each pseudo-reward). Figure 6 shows depth distributions for each type of prediction. Here, the \"depth\" of a predictron is defined as the effective number of model steps. If the predictron is fully based on the very first value (i.e. 0 = 0), it counts as 0 steps. If instead, it learns to weigh all rewards and the final value of a predictron equally, it counts as 16 steps. Concretely, the depth can be used as recursive for each Dicursive (1), but each Dicursive (1)."}, {"heading": "5.5 VISUALIZING THE PREDICTIONS IN THE POOL DOMAIN", "text": "We test the quality of the predictions in the pool area to assess whether they are well suited to making decisions. For each recorded pool position, we apply a set I of different initial conditions (different angles and velocity of the white ball) and ask which ones are more likely to cause colored balls to disappear in the pockets. For each captured pool position, we apply the (r, \u03b3, \u03bb) predictron (split cores, 16 model steps, no jump connections) to obtain predictions. We sum up the predictions that match the insertion of any ball other than the white ball, and to real discounts \u03b3 = 0.98 and \u03b3 = 1. We select the condition s that maximizes this sum. We then roll the pool simulator forward from s and log the number of pocket events. Figure 2 shows a stamped rollout in which the prediction ctron is used to pick out s-tron the consequences of the choice of 128 | 6 winches and 25Q."}, {"heading": "6 RELATED WORK", "text": "Lee et al. (2015) introduced an architecture of neural networks in which classifications branch off from hidden intermediate layers. An important difference with respect to the \u03bb predictron is that the weights are adjusted by hand as hyperparameters, while in the predictron the \u03bb weights are learned and, more importantly, depend on the input. Another difference is that the loss in auxiliary classifications is used to accelerate learning, but the classifications themselves are not combined to form an aggregated prediction; the output of the model itself is the deepest prediction. Value value estimation networks (Tamar et al., 2016) use convolutionary and max-pooling layers to represent a step of value appreciation, which in a way resembles an r-predictron without any distortion or distortion."}, {"heading": "7 DISCUSSION", "text": "The prediction model is a single differentiated architecture that drives an internal model to estimate values. This internal model can maintain both the structure and semantics of traditional prediction models. However, unlike most approaches to model-based prediction models, the model is entirely abstract: it does not have to correspond to the real environment in a human comprehensible way, as long as its forward \"plans\" accurately predict the results in the real environment. The prediction model can be considered a novel network architecture that includes several separable ideas. First, the prediction values output a value by accumulating them through a series of internal planning steps. Second, each advance of the prediction values output values at multiple planning depths. Third, these values can be combined, even within a single forward step, to generate a general ensemble value. Finally, the different values output by the prediction models can encourage the prediction values to be more consistent with each other, rather than showing the internal differences in our experiments."}, {"heading": "A ARCHITECTURE", "text": "The state representation f is a two-layer revolutionary neural network (LeCun et al., 1998). There is a nucleus c, which in turn is based on convolutions and combines both the MRP model and the \u03bb network into a single reproducible module, so that sk + 1, rk + 1, \u03b3k + 1, \u03bbk = c (sk). This nucleus is deterministic and is duplicated in the prediction K times with common weights. (The prediction with undivided weights hates the cores the cores.) Finally, the value network v is a fully connected neural network that computes vk = v (sk). Specifically, the nucleus (Figure 7) consists of a revolutionary layer that moulds into a middle (hidden) layer."}, {"heading": "B TRAINING", "text": "All experiments used the supervised (Monte-Carlo) update described in Section 4.1, with the exception of the semi-supervised experiment, which used the consistency update described in Section 4.2. We update all parameters by applying the Adam Optimizer (Kingma and Adam, 2015) to stochastic gradients of the corresponding loss functions. Each return is normalized by dividing them by their standard deviation (as measured on a group of 20,000 episodes before the experiment). In all experiments, the learning rate was 0.001, and the other parameters of the Adam Optimizer were \u03b21 = 0.9, \u03b22 = 0.999 and = 10 \u2212 8. We used mini-batches of 100 samples."}, {"heading": "C COMPARING ARCHITECTURES OF DIFFERENT DEPTHS", "text": "We investigated the effects of changing the depth of networks, with and without skip lines. Figure 8 in shows that skip lines (dashed lines) make traditional architectures (black / gray lines) more robust in depth (i.e., the black / gray dashed lines almost overlap, especially at the pool), and that the predicate exceeds the corresponding forward or backward lines for all depths, with and without skip lines."}, {"heading": "D ADDITIONAL DOMAIN DETAILS", "text": "D.1 RANDOM MAZESTo create labyrinths, we first determine a number of walls using a stochastic line search, so that the upper left corner is connected to the lower right corner in about 50% of the labyrinths (both always forced to be empty), and then mix the walls evenly at random. For 20 x 20 labyrinths, this means that 70% of the locations are empty and 30% contain walls. More than one Googol of such labyrinths exist 20 x 20 (such as (398 120) > 10100).D.2 POOLTo create sequences in the pool domain, the initial locations of 4 balls of different color are randomly scanned. The white ball is the only one that moves first. Its speed has a standard that is evenly scanned between 7 and 14. The initial angle is scanned in the range (0, 2\u03c0)."}], "references": [{"title": "Dynamic programming", "author": ["R. Bellman"], "venue": null, "citeRegEx": "Bellman.,? \\Q1957\\E", "shortCiteRegEx": "Bellman.", "year": 1957}, {"title": "Recurrent environment simulators", "author": ["S. Chiappa", "S. Racaniere", "D. Wierstra", "S. Mohamed"], "venue": null, "citeRegEx": "Chiappa et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Chiappa et al\\.", "year": 2016}, {"title": "Deep sparse rectifier neural networks", "author": ["X. Glorot", "A. Bordes", "Y. Bengio"], "venue": "In Aistats,", "citeRegEx": "Glorot et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Glorot et al\\.", "year": 2011}, {"title": "Adaptive computation time for recurrent neural networks", "author": ["A. Graves"], "venue": "CoRR, abs/1603.08983,", "citeRegEx": "Graves.,? \\Q2016\\E", "shortCiteRegEx": "Graves.", "year": 2016}, {"title": "Deep residual learning for image recognition", "author": ["K. He", "X. Zhang", "S. Ren", "J. Sun"], "venue": "arXiv preprint arXiv:1512.03385,", "citeRegEx": "He et al\\.,? \\Q2015\\E", "shortCiteRegEx": "He et al\\.", "year": 2015}, {"title": "Batch normalization: Accelerating deep network training by reducing internal covariate shift", "author": ["S. Ioffe", "C. Szegedy"], "venue": "arXiv preprint arXiv:1502.03167,", "citeRegEx": "Ioffe and Szegedy.,? \\Q2015\\E", "shortCiteRegEx": "Ioffe and Szegedy.", "year": 2015}, {"title": "A method for stochastic optimization", "author": ["D.P. Kingma", "J.B. Adam"], "venue": "In International Conference on Learning Representation,", "citeRegEx": "Kingma and Adam.,? \\Q2015\\E", "shortCiteRegEx": "Kingma and Adam.", "year": 2015}, {"title": "Gradient-based learning applied to document recognition", "author": ["Y. LeCun", "L. Bottou", "Y. Bengio", "P. Haffner"], "venue": "Proceedings of the IEEE,", "citeRegEx": "LeCun et al\\.,? \\Q1998\\E", "shortCiteRegEx": "LeCun et al\\.", "year": 1998}, {"title": "Continuous control with deep reinforcement learning", "author": ["T. Lillicrap", "J. Hunt", "A. Pritzel", "N. Heess", "T. Erez", "Y. Tassa", "D. Silver", "D. Wierstra"], "venue": "In ICLR,", "citeRegEx": "Lillicrap et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Lillicrap et al\\.", "year": 2016}, {"title": "Human-level control through deep reinforcement learning", "author": ["V. Mnih", "K. Kavukcuoglu", "D. Silver", "A.A. Rusu", "J. Veness", "M.G. Bellemare", "A. Graves", "M. Riedmiller", "A.K. Fidjeland", "G. Ostrovski", "S. Petersen", "C. Beattie", "A. Sadik", "I. Antonoglou", "H. King", "D. Kumaran", "D. Wierstra", "S. Legg", "D. Hassabis"], "venue": "Nature, 518(7540):529\u2013533,", "citeRegEx": "Mnih et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Mnih et al\\.", "year": 2015}, {"title": "Asynchronous methods for deep reinforcement learning", "author": ["V. Mnih", "A.P. Badia", "M. Mirza", "A. Graves", "T. Lillicrap", "T. Harley", "D. Silver", "K. Kavukcuoglu"], "venue": "In International Conference on Machine Learning,", "citeRegEx": "Mnih et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Mnih et al\\.", "year": 2016}, {"title": "Action-conditional video prediction using deep networks in atari games", "author": ["J. Oh", "X. Guo", "H. Lee", "R.L. Lewis", "S. Singh"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Oh et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Oh et al\\.", "year": 2015}, {"title": "On learning to think: Algorithmic information theory for novel combinations of reinforcement learning controllers and recurrent neural world models", "author": ["J. Schmidhuber"], "venue": "arXiv preprint arXiv:1511.09249,", "citeRegEx": "Schmidhuber.,? \\Q2015\\E", "shortCiteRegEx": "Schmidhuber.", "year": 2015}, {"title": "Learning to predict by the methods of temporal differences", "author": ["R.S. Sutton"], "venue": "Machine Learning,", "citeRegEx": "Sutton.,? \\Q1988\\E", "shortCiteRegEx": "Sutton.", "year": 1988}, {"title": "TD models: Modeling the world at a mixture of time scales", "author": ["R.S. Sutton"], "venue": "In Proceedings of the Twelfth International Conference on Machine Learning,", "citeRegEx": "Sutton.,? \\Q1995\\E", "shortCiteRegEx": "Sutton.", "year": 1995}, {"title": "Reinforcement Learning: An Introduction", "author": ["R.S. Sutton", "A.G. Barto"], "venue": "The MIT press, Cambridge MA,", "citeRegEx": "Sutton and Barto.,? \\Q1998\\E", "shortCiteRegEx": "Sutton and Barto.", "year": 1998}, {"title": "Horde: A scalable real-time architecture for learning knowledge from unsupervised sensorimotor interaction", "author": ["R.S. Sutton", "J. Modayil", "M. Delp", "T. Degris", "P.M. Pilarski", "A. White", "D. Precup"], "venue": "In The 10th International Conference on Autonomous Agents and Multiagent Systems-Volume", "citeRegEx": "Sutton et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Sutton et al\\.", "year": 2011}, {"title": "Value iteration networks", "author": ["A. Tamar", "S. Levine", "P. Abbeel"], "venue": "arXiv preprint arXiv:1602.02867,", "citeRegEx": "Tamar et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Tamar et al\\.", "year": 2016}, {"title": "Mujoco: A physics engine for model-based control", "author": ["E. Todorov", "T. Erez", "Y. Tassa"], "venue": "In 2012 IEEE/RSJ International Conference on Intelligent Robots and Systems,", "citeRegEx": "Todorov et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Todorov et al\\.", "year": 2012}], "referenceMentions": [{"referenceID": 11, "context": "Prior deep reinforcement learning methods have successfully constructed models that can unroll near pixel-perfect reconstructions (Oh et al., 2015; Chiappa et al., 2016); but are yet to surpass state-of-the-art modelfree methods in challenging RL domains with raw inputs (e.", "startOffset": 130, "endOffset": 169}, {"referenceID": 1, "context": "Prior deep reinforcement learning methods have successfully constructed models that can unroll near pixel-perfect reconstructions (Oh et al., 2015; Chiappa et al., 2016); but are yet to surpass state-of-the-art modelfree methods in challenging RL domains with raw inputs (e.", "startOffset": 130, "endOffset": 169}, {"referenceID": 8, "context": ", 2016); but are yet to surpass state-of-the-art modelfree methods in challenging RL domains with raw inputs (e.g., Mnih et al., 2015; 2016; Lillicrap et al., 2016).", "startOffset": 109, "endOffset": 164}, {"referenceID": 16, "context": "In the vector case, these are known as general value functions (Sutton et al., 2011).", "startOffset": 63, "endOffset": 84}, {"referenceID": 0, "context": "We will say that a (general) value function v(\u00b7) is consistent with environment p if and only if v = vp which satisfies the following Bellman equation (Bellman, 1957),", "startOffset": 151, "endOffset": 166}, {"referenceID": 15, "context": "In model-based reinforcement learning (Sutton and Barto, 1998), an approximation m \u2248 p to the environment is learned.", "startOffset": 38, "endOffset": 62}, {"referenceID": 14, "context": "A (general) value function vm(\u00b7) is consistent with model m (or valid, (Sutton, 1995)), if and only if it satisfies a Bellman equation vm(s) = Em [r + \u03b3vm(s)] with respect to model m.", "startOffset": 71, "endOffset": 85}, {"referenceID": 13, "context": "This \u03bb-preturn is analogous to the \u03bb-return in the forward-view TD(\u03bb) algorithm (Sutton, 1988; Sutton and Barto, 1998).", "startOffset": 80, "endOffset": 118}, {"referenceID": 15, "context": "This \u03bb-preturn is analogous to the \u03bb-return in the forward-view TD(\u03bb) algorithm (Sutton, 1988; Sutton and Barto, 1998).", "startOffset": 80, "endOffset": 118}, {"referenceID": 3, "context": "This enables the predictron to compute to an adaptive depth (Graves, 2016) depending on the internal state and learning dynamics of the network.", "startOffset": 60, "endOffset": 74}, {"referenceID": 18, "context": "The simulator is implemented in the physics engine Mujoco (Todorov et al., 2012).", "startOffset": 58, "endOffset": 80}, {"referenceID": 4, "context": "The deep network with skip connections is a variant of ResNet (He et al., 2015).", "startOffset": 62, "endOffset": 79}, {"referenceID": 17, "context": "Value iteration networks (Tamar et al., 2016) use convolutional and max-pooling layers to represent a step of value iteration.", "startOffset": 25, "endOffset": 45}, {"referenceID": 12, "context": "Schmidhuber (2015) dicusses learning abstract models, but maintains separate losses for the model and a controller, and suggests training the model unsupervised to compactly encode the entire history of observations, through predictive coding.", "startOffset": 0, "endOffset": 19}, {"referenceID": 9, "context": "However, these ideas may transfer to the control setting, for example by using the predictron as a Q-network (Mnih et al., 2015).", "startOffset": 109, "endOffset": 128}], "year": 2016, "abstractText": "One of the key challenges of artificial intelligence is to learn models that are effective in the context of planning. In this document we introduce the predictron architecture. The predictron consists of a fully abstract model, represented by a Markov reward process, that can be rolled forward multiple \u201cimagined\u201d planning steps. Each forward pass of the predictron accumulates internal rewards and values over multiple planning depths. The predictron is trained end-to-end so as to make these accumulated values accurately approximate the true value function. We applied the predictron to procedurally generated random mazes and a simulator for the game of pool. The predictron yielded significantly more accurate predictions than conventional deep neural network architectures.", "creator": "LaTeX with hyperref package"}}}