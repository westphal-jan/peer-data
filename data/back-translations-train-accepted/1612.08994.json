{"id": "1612.08994", "review": {"conference": "EMNLP", "VERSION": "v1", "DATE_OF_SUBMISSION": "28-Dec-2016", "title": "Here's My Point: Joint Pointer Architecture for Argument Mining", "abstract": "One of the major goals in automated argumentation mining is to uncover the argument structure present in argumentative text. In order to determine this structure, one must understand how different individual components of the overall argument are linked. General consensus in this field dictates that the argument components form a hierarchy of persuasion, which manifests itself in a tree structure. This work provides the first neural network-based approach to argumentation mining, focusing on the two tasks of extracting links between argument components, and classifying types of argument components. In order to solve this problem, we propose to use a joint model that is based on a Pointer Network architecture. A Pointer Network is appealing for this task for the following reasons: 1) It takes into account the sequential nature of argument components; 2) By construction, it enforces certain properties of the tree structure present in argument relations; 3) The hidden representations can be applied to auxiliary tasks. In order to extend the contribution of the original Pointer Network model, we construct a joint model that simultaneously attempts to learn the type of argument component, as well as continuing to predict links between argument components. The proposed joint model achieves state-of-the-art results on two separate evaluation corpora, achieving far superior performance than a regular Pointer Network model. Our results show that optimizing for both tasks, and adding a fully-connected layer prior to recurrent neural network input, is crucial for high performance.", "histories": [["v1", "Wed, 28 Dec 2016 21:36:19 GMT  (860kb,D)", "http://arxiv.org/abs/1612.08994v1", "10 pages; under review for ICLR"], ["v2", "Mon, 8 May 2017 21:59:03 GMT  (848kb,D)", "http://arxiv.org/abs/1612.08994v2", "10 pages; under review for ICLR"]], "COMMENTS": "10 pages; under review for ICLR", "reviews": [], "SUBJECTS": "cs.CL", "authors": ["peter potash", "alexey romanov", "anna rumshisky"], "accepted": true, "id": "1612.08994"}, "pdf": {"name": "1612.08994.pdf", "metadata": {"source": "CRF", "title": "HERE\u2019S MY POINT: ARGUMENTATION MINING WITH POINTER NETWORKS", "authors": ["Peter Potash", "Alexey Romanov"], "emails": ["ppotash@cs.uml.edu", "aromanov@cs.uml.edu", "arum@cs.uml.edu"], "sections": [{"heading": "1 INTRODUCTION", "text": "In fact, most of them are able to determine for themselves what they want and what they want."}, {"heading": "2 RELATED WORK", "text": "Peldszus & Stede (2015) have also used classification models to predict the presence of linkages. Various authors have also proposed to model link extraction along with other subtasks from the argumentation pipeline, either using an Integrated Linear Programming (ILP) framework (Persing & Ng, 2016; Stab & Gurevych, 2016) or feeding earlier subtask predictions directly into another model; the earlier common approaches are evaluated on commented corporations of compelling essays (Stab & Gurevych, 2014a; 2016), and the latter on a corpus of microtexts that argue repeatedly (Peldszus, 2014)."}, {"heading": "3 POINTER NETWORK FOR LINK EXTRACTION", "text": "In this section we will describe how to use a PN for the problem of extracting connections between ACs. We start with a general description of the PN model."}, {"heading": "3.1 POINTER NETWORK", "text": "In fact, it is as if it is a pure formation that sees itself in a position to comply with the rules. (...) In fact, it is as if it does not comply with the rules. (...) In fact, it is as if it does not comply with the rules. (...) It is as if it does not comply with the rules. (...) It is as if it does not comply with the rules. (...) It is as if it does not comply with the rules. (...) It is as if it does not comply with the rules. (...) It is as if it complies with the rules. (...) It is as if it complies with the rules. (...) It is as if it complies with the rules. \"(...) It is as if it is. (...) It is as if it is. (...) It is as if it is. (...) It is as if it is."}, {"heading": "3.2 LINK EXTRACTION AS SEQUENCE MODELING", "text": "A particular piece of text has a series of ACs that occur in a particular order in the text (C1,..., Cn). Therefore, in the timestep i encoding, we add a representation of Ci. Because the representation is large and sparse (see Section 3.3 for details on how we represent ACs), we add a fully connected layer before the LSTM input. In the face of a representation of Ri for AC Ci, the LSTM input becomes Ai (WrepRi + brep) (5), where Wrep in turn becomes model parameters, and is the sigma function. (Likewise, the decoding network applies a fully connected layer with sigma activation to its inputs, see Figure 3). In the coding of step i, the coding of LSTM, the hidden layer becomes egg, which acts as a hidden representation of AC Ci. To make the PN applicable, we are equal to the extraction of I."}, {"heading": "3.3 REPRESENTING ARGUMENT COMPONENTS", "text": "Each alternating current is itself a sequence of symbols, similar to the recently proposed question-and-answer dataset (Weston et al., 2015) We follow the work of Stab & Gurevych (2016) and focus on three different types of characteristics to represent our alternating current networks: 1) vocabulary of alternating current; 2) embedding representation based on GloVe embedding (Pennington et al., 2014); 3) structural features: whether alternating current is the first alternating current in a paragraph or not, and whether the alternating current is in an opening, body or final paragraph. See section 6 for a deposit study of proposed characteristics."}, {"heading": "3.4 JOINT NEURAL MODEL", "text": "Up to this point, we focused on the task of extracting connections between ACs, but recent work has shown that common models that simultaneously attempt to complete multiple aspects of the subtask pipeline outperform models that focus on a single subtask (Persing & Ng, 2016; Stab & Gurevych, 2014b; Peldszus & Stede, 2015), so we will modify the architecture we proposed in Section 3 so that we can perform the AC classification (Kwon et al., 2007; Rooney et al., 2012) along with Link Prediction. Knowledge of the predictions of a single subtask (Che et al., 2015) can help with other subtask predictions."}, {"heading": "4 EXPERIMENTAL DESIGN", "text": "As we mentioned earlier, our work assumes that ACs have already been identified, that is, the symbol sequence that comprises a particular AC is already known; the sequence of ACs corresponds directly to the order in which the ACs appear in the text; since ACs are not overlapping, there is no ambiguity in that sequence; we test the effectiveness of our proposed model using a dataset of compelling essays (Stab & Gurevych, 2016) and a dataset of microtexts (Peldszus, 2014); the functional space for the compelling essay corpus has approximately 3,000 dimensions; and the microtext corpus feature space has between 2,500 and 3,000 dimensions, depending on the data distribution (see below); the compelling essay corpus contains a total of 402 essays, with a frozen set of 80 essays held for testing; there are three AC types in this main claim and premise:"}, {"heading": "5 RESULTS", "text": "The results of our experiments are presented in Tables 1 and 2. For each corpus, we present f1 scores for the AC classification experiment with a macro averaged score of each class f1 score. We also present the f1 scores for predicting the presence / absence of connections between ACs and the associated macro averages between these two values. We implement and compare four types of neural models: 1) The previously described PN-based model is presented in Figure 3 (referred to as PN in the tables); 2) The same as 1), but without the fully connected input layers; 3) The same as 1), but the model only predicts the linking task and is therefore not optimized for predicting the type."}, {"heading": "6 DISCUSSION", "text": "This year is the highest in the history of the country."}, {"heading": "7 CONCLUSION", "text": "We evaluate our models on the basis of two corpora: a corpus of compelling essays (Stab & Gurevych, 2016) and a corpus of microtext (Peldszus, 2014).The PN model records current results in the field of compelling essay writing and achieves current results for link prediction on the microtext corpus, although there are only 90 training examples.The results show that joint modelling of the two prediction tasks is critical for high performance, as is the presence of a fully connected layer prior to LSTM input. Future work may attempt to learn the AC representations themselves, as in Kumar et al. (2015).Finally, future work may integrate tasks 1 and 4 into the model."}], "references": [{"title": "TensorFlow: Large-scale machine learning on heterogeneous systems", "author": ["cent Vanhoucke", "Vijay Vasudevan", "Fernanda Vi\u00e9gas", "Oriol Vinyals", "Pete Warden", "Martin Wattenberg", "Martin Wicke", "Yuan Yu", "Xiaoqiang Zheng"], "venue": null, "citeRegEx": "Vanhoucke et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Vanhoucke et al\\.", "year": 2015}, {"title": "Neural machine translation by jointly learning to align and translate", "author": ["Dzmitry Bahdanau", "Kyunghyun Cho", "Yoshua Bengio"], "venue": "arXiv preprint arXiv:1409.0473,", "citeRegEx": "Bahdanau et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Bahdanau et al\\.", "year": 2014}, {"title": "Recursive neural networks can learn logical semantics", "author": ["Samuel R Bowman", "Christopher Potts", "Christopher D Manning"], "venue": "arXiv preprint arXiv:1406.1827,", "citeRegEx": "Bowman et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Bowman et al\\.", "year": 2014}, {"title": "Tree-structured composition in neural networks without tree-structured architectures", "author": ["Samuel R Bowman", "Christopher D Manning", "Christopher Potts"], "venue": "arXiv preprint arXiv:1506.04834,", "citeRegEx": "Bowman et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Bowman et al\\.", "year": 2015}, {"title": "A study of the impact of persuasive argumentation in political debates", "author": ["Amparo Elizabeth Cano-Basave", "Yulan He"], "venue": "In Proceedings of NAACL-HLT,", "citeRegEx": "Cano.Basave and He.,? \\Q2016\\E", "shortCiteRegEx": "Cano.Basave and He.", "year": 2016}, {"title": "Deep computational phenotyping", "author": ["Zhengping Che", "David Kale", "Wenzhe Li", "Mohammad Taha Bahadori", "Yan Liu"], "venue": "In Proceedings of the 21th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining,", "citeRegEx": "Che et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Che et al\\.", "year": 2015}, {"title": "Analyzing the structure of argumentative discourse", "author": ["Robin Cohen"], "venue": "Computational linguistics,", "citeRegEx": "Cohen.,? \\Q1987\\E", "shortCiteRegEx": "Cohen.", "year": 1987}, {"title": "Coarse-grained argumentation features for scoring persuasive essays", "author": ["Debanjan Ghosh", "Aquila Khanam", "Yubo Han", "Smaranda Muresan"], "venue": "In The 54th Annual Meeting of the Association for Computational Linguistics,", "citeRegEx": "Ghosh et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Ghosh et al\\.", "year": 2016}, {"title": "Offline handwriting recognition with multidimensional recurrent neural networks. In Advances in neural information processing", "author": ["Alex Graves", "J\u00fcrgen Schmidhuber"], "venue": null, "citeRegEx": "Graves and Schmidhuber.,? \\Q2009\\E", "shortCiteRegEx": "Graves and Schmidhuber.", "year": 2009}, {"title": "Which argument is more convincing? analyzing and predicting convincingness of web arguments using bidirectional lstm", "author": ["Ivan Habernal", "Iryna Gurevych"], "venue": "In Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (ACL),", "citeRegEx": "Habernal and Gurevych.,? \\Q2016\\E", "shortCiteRegEx": "Habernal and Gurevych.", "year": 2016}, {"title": "Long short-term memory", "author": ["Sepp Hochreiter", "J\u00fcrgen Schmidhuber"], "venue": "Neural computation,", "citeRegEx": "Hochreiter and Schmidhuber.,? \\Q1997\\E", "shortCiteRegEx": "Hochreiter and Schmidhuber.", "year": 1997}, {"title": "Adam: A method for stochastic optimization", "author": ["Diederik Kingma", "Jimmy Ba"], "venue": "arXiv preprint arXiv:1412.6980,", "citeRegEx": "Kingma and Ba.,? \\Q2014\\E", "shortCiteRegEx": "Kingma and Ba.", "year": 2014}, {"title": "Ask me anything: Dynamic memory networks for natural language processing", "author": ["Ankit Kumar", "Ozan Irsoy", "Jonathan Su", "James Bradbury", "Robert English", "Brian Pierce", "Peter Ondruska", "Ishaan Gulrajani", "Richard Socher"], "venue": "arXiv preprint arXiv:1506.07285,", "citeRegEx": "Kumar et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Kumar et al\\.", "year": 2015}, {"title": "Identifying and classifying subjective claims", "author": ["Namhee Kwon", "Liang Zhou", "Eduard Hovy", "Stuart W Shulman"], "venue": "In Proceedings of the 8th annual international conference on Digital government research: bridging disciplines & domains,", "citeRegEx": "Kwon et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Kwon et al\\.", "year": 2007}, {"title": "Context-aware argumentative relation mining", "author": ["Huy V Nguyen", "Diane J Litman"], "venue": null, "citeRegEx": "Nguyen and Litman.,? \\Q2016\\E", "shortCiteRegEx": "Nguyen and Litman.", "year": 2016}, {"title": "Argumentation mining: the detection, classification and structure of arguments in text", "author": ["Raquel Mochales Palau", "Marie-Francine Moens"], "venue": "In Proceedings of the 12th international conference on artificial intelligence and law,", "citeRegEx": "Palau and Moens.,? \\Q2009\\E", "shortCiteRegEx": "Palau and Moens.", "year": 2009}, {"title": "Towards segment-based recognition of argumentation structure in short texts", "author": ["Andreas Peldszus"], "venue": "ACL", "citeRegEx": "Peldszus.,? \\Q2014\\E", "shortCiteRegEx": "Peldszus.", "year": 2014}, {"title": "Joint prediction in mst-style discourse parsing for argumentation mining", "author": ["Andreas Peldszus", "Manfred Stede"], "venue": "In Proc. of the Conference on Empirical Methods in Natural Language Processing,", "citeRegEx": "Peldszus and Stede.,? \\Q2015\\E", "shortCiteRegEx": "Peldszus and Stede.", "year": 2015}, {"title": "Glove: Global vectors for word representation", "author": ["Jeffrey Pennington", "Richard Socher", "Christopher D Manning"], "venue": "In EMNLP,", "citeRegEx": "Pennington et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Pennington et al\\.", "year": 2014}, {"title": "End-to-end argumentation mining in student essays", "author": ["Isaac Persing", "Vincent Ng"], "venue": "In Proceedings of NAACL-HLT,", "citeRegEx": "Persing and Ng.,? \\Q2016\\E", "shortCiteRegEx": "Persing and Ng.", "year": 2016}, {"title": "An application of recurrent nets to phone probability estimation", "author": ["Anthony J Robinson"], "venue": "IEEE transactions on Neural Networks,", "citeRegEx": "Robinson.,? \\Q1994\\E", "shortCiteRegEx": "Robinson.", "year": 1994}, {"title": "Applying kernel methods to argumentation mining", "author": ["Niall Rooney", "Hui Wang", "Fiona Browne"], "venue": "In FLAIRS Conference,", "citeRegEx": "Rooney et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Rooney et al\\.", "year": 2012}, {"title": "Dropout: a simple way to prevent neural networks from overfitting", "author": ["Nitish Srivastava", "Geoffrey E Hinton", "Alex Krizhevsky", "Ilya Sutskever", "Ruslan Salakhutdinov"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Srivastava et al\\.,? \\Q1929\\E", "shortCiteRegEx": "Srivastava et al\\.", "year": 1929}, {"title": "Annotating argument components and relations in persuasive essays", "author": ["Christian Stab", "Iryna Gurevych"], "venue": "In COLING, pp", "citeRegEx": "Stab and Gurevych.,? \\Q2014\\E", "shortCiteRegEx": "Stab and Gurevych.", "year": 2014}, {"title": "Identifying argumentative discourse structures in persuasive essays", "author": ["Christian Stab", "Iryna Gurevych"], "venue": "In EMNLP, pp", "citeRegEx": "Stab and Gurevych.,? \\Q2014\\E", "shortCiteRegEx": "Stab and Gurevych.", "year": 2014}, {"title": "Parsing argumentation structures in persuasive essays", "author": ["Christian Stab", "Iryna Gurevych"], "venue": "arXiv preprint arXiv:1604.07370,", "citeRegEx": "Stab and Gurevych.,? \\Q2016\\E", "shortCiteRegEx": "Stab and Gurevych.", "year": 2016}, {"title": "Sequence to sequence learning with neural networks. In Advances in neural information processing", "author": ["Ilya Sutskever", "Oriol Vinyals", "Quoc V Le"], "venue": null, "citeRegEx": "Sutskever et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Sutskever et al\\.", "year": 2014}, {"title": "Order matters: Sequence to sequence for sets", "author": ["Oriol Vinyals", "Samy Bengio", "Manjunath Kudlur"], "venue": "arXiv preprint arXiv:1511.06391,", "citeRegEx": "Vinyals et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Vinyals et al\\.", "year": 2015}, {"title": "Grammar as a foreign language", "author": ["Oriol Vinyals", "\u0141ukasz Kaiser", "Terry Koo", "Slav Petrov", "Ilya Sutskever", "Geoffrey Hinton"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Vinyals et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Vinyals et al\\.", "year": 2015}, {"title": "Is this post persuasive? ranking argumentative comments in the online forum", "author": ["Zhongyu Wei", "Yang Liu", "Yi Li"], "venue": "In The 54th Annual Meeting of the Association for Computational Linguistics,", "citeRegEx": "Wei et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Wei et al\\.", "year": 2016}], "referenceMentions": [{"referenceID": 29, "context": "Computational approaches to argument mining/understanding have become very popular (Persing & Ng, 2016; Cano-Basave & He, 2016; Wei et al., 2016; Ghosh et al., 2016; Palau & Moens, 2009; Habernal & Gurevych, 2016).", "startOffset": 83, "endOffset": 213}, {"referenceID": 7, "context": "Computational approaches to argument mining/understanding have become very popular (Persing & Ng, 2016; Cano-Basave & He, 2016; Wei et al., 2016; Ghosh et al., 2016; Palau & Moens, 2009; Habernal & Gurevych, 2016).", "startOffset": 83, "endOffset": 213}, {"referenceID": 6, "context": "Second, we follow previous work that assumes a tree structure for the linking of ACs (Palau & Moens, 2009; Cohen, 1987; Peldszus & Stede, 2015; Stab & Gurevych, 2016) Specifically, a given AC can only have a single outgoing", "startOffset": 85, "endOffset": 166}, {"referenceID": 26, "context": "We also note that since a PN is a type of sequence-to-sequence model (Sutskever et al., 2014), it allows the entire sequence to be seen before making prediction.", "startOffset": 69, "endOffset": 93}, {"referenceID": 20, "context": "This is important because if the problem were to be approached as standard sequence modeling (Graves & Schmidhuber, 2009; Robinson, 1994), making predictions at each forward timestep, it would only allow links to ACs that have already been seen.", "startOffset": 93, "endOffset": 137}, {"referenceID": 16, "context": "Linking to the first argument component can provide a competitive baseline heuristic (Peldszus & Stede, 2015; Stab & Gurevych, 2016). Given the task at hand, we propose a modification of a Pointer Network (PN) (Vinyals et al., 2015b). A PN is a sequence-to-sequence model that outputs a distribution over the encoding indices at each decoding timestep. The PN is a promising model for link extraction in argumentative text because it inherently possesses three important characteristics: 1) it is able to model the sequential nature of ACs; 2) it constrains ACs to have a single outgoing link, thus partly enforcing the tree structure; 3) the hidden representations learned by the model can be used for jointly predicting multiple subtasks. We also note that since a PN is a type of sequence-to-sequence model (Sutskever et al., 2014), it allows the entire sequence to be seen before making prediction. This is important because if the problem were to be approached as standard sequence modeling (Graves & Schmidhuber, 2009; Robinson, 1994), making predictions at each forward timestep, it would only allow links to ACs that have already been seen. This is equivalent to only allowing backward links. We note that we do test a simplified model that only uses hidden states from an encoding network to make predictions, as opposed to the sequence-to-sequence architecture present in the PN (see Section 5). PNs were originally proposed to allow a variable length decoding sequence (Vinyals et al., 2015b). Alternatively, the PN we implement differs from the original model in that we decode for the same number of timesteps as there are input components. We also propose a joint PN for both extracting links between ACs and predicting the type of AC. The model uses the hidden representation of ACs produced during the encoding step (see Section 3.4). Aside from the partial assumption of tree structure in the argumentative text, our models do not make any additional assumptions about the AC types or connectivity, unlike the work of Peldszus (2014). We evaluate our models on the corpora of Stab & Gurevych (2016) and Peldszus (2014), and compare our results with the results of the aformentioned authors.", "startOffset": 86, "endOffset": 2051}, {"referenceID": 16, "context": "Linking to the first argument component can provide a competitive baseline heuristic (Peldszus & Stede, 2015; Stab & Gurevych, 2016). Given the task at hand, we propose a modification of a Pointer Network (PN) (Vinyals et al., 2015b). A PN is a sequence-to-sequence model that outputs a distribution over the encoding indices at each decoding timestep. The PN is a promising model for link extraction in argumentative text because it inherently possesses three important characteristics: 1) it is able to model the sequential nature of ACs; 2) it constrains ACs to have a single outgoing link, thus partly enforcing the tree structure; 3) the hidden representations learned by the model can be used for jointly predicting multiple subtasks. We also note that since a PN is a type of sequence-to-sequence model (Sutskever et al., 2014), it allows the entire sequence to be seen before making prediction. This is important because if the problem were to be approached as standard sequence modeling (Graves & Schmidhuber, 2009; Robinson, 1994), making predictions at each forward timestep, it would only allow links to ACs that have already been seen. This is equivalent to only allowing backward links. We note that we do test a simplified model that only uses hidden states from an encoding network to make predictions, as opposed to the sequence-to-sequence architecture present in the PN (see Section 5). PNs were originally proposed to allow a variable length decoding sequence (Vinyals et al., 2015b). Alternatively, the PN we implement differs from the original model in that we decode for the same number of timesteps as there are input components. We also propose a joint PN for both extracting links between ACs and predicting the type of AC. The model uses the hidden representation of ACs produced during the encoding step (see Section 3.4). Aside from the partial assumption of tree structure in the argumentative text, our models do not make any additional assumptions about the AC types or connectivity, unlike the work of Peldszus (2014). We evaluate our models on the corpora of Stab & Gurevych (2016) and Peldszus (2014), and compare our results with the results of the aformentioned authors.", "startOffset": 86, "endOffset": 2116}, {"referenceID": 16, "context": "Linking to the first argument component can provide a competitive baseline heuristic (Peldszus & Stede, 2015; Stab & Gurevych, 2016). Given the task at hand, we propose a modification of a Pointer Network (PN) (Vinyals et al., 2015b). A PN is a sequence-to-sequence model that outputs a distribution over the encoding indices at each decoding timestep. The PN is a promising model for link extraction in argumentative text because it inherently possesses three important characteristics: 1) it is able to model the sequential nature of ACs; 2) it constrains ACs to have a single outgoing link, thus partly enforcing the tree structure; 3) the hidden representations learned by the model can be used for jointly predicting multiple subtasks. We also note that since a PN is a type of sequence-to-sequence model (Sutskever et al., 2014), it allows the entire sequence to be seen before making prediction. This is important because if the problem were to be approached as standard sequence modeling (Graves & Schmidhuber, 2009; Robinson, 1994), making predictions at each forward timestep, it would only allow links to ACs that have already been seen. This is equivalent to only allowing backward links. We note that we do test a simplified model that only uses hidden states from an encoding network to make predictions, as opposed to the sequence-to-sequence architecture present in the PN (see Section 5). PNs were originally proposed to allow a variable length decoding sequence (Vinyals et al., 2015b). Alternatively, the PN we implement differs from the original model in that we decode for the same number of timesteps as there are input components. We also propose a joint PN for both extracting links between ACs and predicting the type of AC. The model uses the hidden representation of ACs produced during the encoding step (see Section 3.4). Aside from the partial assumption of tree structure in the argumentative text, our models do not make any additional assumptions about the AC types or connectivity, unlike the work of Peldszus (2014). We evaluate our models on the corpora of Stab & Gurevych (2016) and Peldszus (2014), and compare our results with the results of the aformentioned authors.", "startOffset": 86, "endOffset": 2136}, {"referenceID": 16, "context": "The former joint approaches are evaluated on annotated corpora of persuasive essays (Stab & Gurevych, 2014a; 2016), and the latter on a corpus of microtexts (Peldszus, 2014).", "startOffset": 157, "endOffset": 173}, {"referenceID": 2, "context": "(2015) experiment on an artificial entailment dataset that is specifically engineered to capture recursive logic (Bowman et al., 2014).", "startOffset": 113, "endOffset": 134}, {"referenceID": 14, "context": "Peldszus & Stede (2015) have also used classification models for predicting the presence of links.", "startOffset": 0, "endOffset": 24}, {"referenceID": 14, "context": "Peldszus & Stede (2015) have also used classification models for predicting the presence of links. Various authors have also proposed to jointly model link extraction with other subtasks from the argumentation mining pipeline, using either an Integer Linear Programming (ILP) framework (Persing & Ng, 2016; Stab & Gurevych, 2016) or directly feeding previous subtask predictions into another model. The former joint approaches are evaluated on annotated corpora of persuasive essays (Stab & Gurevych, 2014a; 2016), and the latter on a corpus of microtexts (Peldszus, 2014). The ILP framework is effective in enforcing a tree structure between ACs when predictions are made from otherwise naive base classifiers. Unrelated to argumentation mining specifically, recurrent neural networks have previously been proposed to model tree/graph structures in a linear manner. Vinyals et al. (2015c) use a sequenceto-sequence model for the task of syntactic parsing.", "startOffset": 0, "endOffset": 890}, {"referenceID": 2, "context": "Bowman et al. (2015) experiment on an artificial entailment dataset that is specifically engineered to capture recursive logic (Bowman et al.", "startOffset": 0, "endOffset": 21}, {"referenceID": 26, "context": "A PN is a sequence-to-sequence model (Sutskever et al., 2014) with attention (Bahdanau et al.", "startOffset": 37, "endOffset": 61}, {"referenceID": 1, "context": ", 2014) with attention (Bahdanau et al., 2014) that was proposed to handle decoding sequences over the encoding inputs, and can be extended to arbitrary sets (Vinyals et al.", "startOffset": 23, "endOffset": 46}, {"referenceID": 1, "context": "The PN uses a form of content-based attention (Bahdanau et al., 2014) to allow the model to produce a distribution over input elements.", "startOffset": 46, "endOffset": 69}, {"referenceID": 18, "context": "We follow the work of Stab & Gurevych (2016) and focus on three different types of features to represent our ACs: 1) Bag-of-Words of the AC; 2) Embedding representation based on GloVe embeddings (Pennington et al., 2014); 3) Structural features: Whether or not the AC is the first AC in a paragraph, and Whether the AC is in an opening, body, or closing paragraph.", "startOffset": 195, "endOffset": 220}, {"referenceID": 13, "context": "Therefore, we will modify the architecture we proposed in Section 3 so that it would allow us to perform AC classification (Kwon et al., 2007; Rooney et al., 2012) together with link prediction.", "startOffset": 123, "endOffset": 163}, {"referenceID": 21, "context": "Therefore, we will modify the architecture we proposed in Section 3 so that it would allow us to perform AC classification (Kwon et al., 2007; Rooney et al., 2012) together with link prediction.", "startOffset": 123, "endOffset": 163}, {"referenceID": 5, "context": "This can be seen as a way of regularizing the hidden representations from the encoding component (Che et al., 2015).", "startOffset": 97, "endOffset": 115}, {"referenceID": 16, "context": "We test the effectiveness of our proposed model on a dataset of persuasive essays (Stab & Gurevych, 2016), as well as a dataset of microtexts (Peldszus, 2014).", "startOffset": 142, "endOffset": 158}, {"referenceID": 16, "context": "We evaluate our models on two corpora: a corpus of persuasive essays (Stab & Gurevych, 2016), and a corpus of microtexts (Peldszus, 2014).", "startOffset": 121, "endOffset": 137}], "year": 2017, "abstractText": "One of the major goals in automated argumentation mining is to uncover the argument structure present in argumentative text. In order to determine this structure, one must understand how different individual components of the overall argument are linked. General consensus in this field dictates that the argument components form a hierarchy of persuasion, which manifests itself in a tree structure. This work provides the first neural network-based approach to argumentation mining, focusing on the two tasks of extracting links between argument components, and classifying types of argument components. In order to solve this problem, we propose to use a joint model that is based on a Pointer Network architecture. A Pointer Network is appealing for this task for the following reasons: 1) It takes into account the sequential nature of argument components; 2) By construction, it enforces certain properties of the tree structure present in argument relations; 3) The hidden representations can be applied to auxiliary tasks. In order to extend the contribution of the original Pointer Network model, we construct a joint model that simultaneously attempts to learn the type of argument component, as well as continuing to predict links between argument components. The proposed joint model achieves state-of-the-art results on two separate evaluation corpora, achieving far superior performance than a regular Pointer Network model. Our results show that optimizing for both tasks, and adding a fully-connected layer prior to recurrent neural network input, is crucial for high performance.", "creator": "LaTeX with hyperref package"}}}