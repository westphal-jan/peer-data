{"id": "1701.01722", "review": {"conference": "ICML", "VERSION": "v1", "DATE_OF_SUBMISSION": "6-Jan-2017", "title": "Follow the Compressed Leader: Faster Online Learning of Eigenvectors and Faster MMWU", "abstract": "Matrix multiplicative weight update (MMWU) is an extremely powerful algorithmic tool for computer science and related fields. However, it comes with a slow running time due to the matrix exponential and eigendecomposition computations. For this reason, many researchers studied the followed-the-perturbed-leader (FTPL) framework which is faster, but a factor $\\sqrt{d}$ worse than the optimal regret of MMWU for dimension-$d$ matrices.", "histories": [["v1", "Fri, 6 Jan 2017 18:43:53 GMT  (582kb,D)", "http://arxiv.org/abs/1701.01722v1", null], ["v2", "Wed, 14 Jun 2017 18:18:21 GMT  (1118kb,D)", "http://arxiv.org/abs/1701.01722v2", null], ["v3", "Mon, 18 Sep 2017 01:04:08 GMT  (1119kb,D)", "http://arxiv.org/abs/1701.01722v3", null]], "reviews": [], "SUBJECTS": "cs.LG cs.DS math.OC stat.ML", "authors": ["zeyuan allen-zhu", "yuanzhi li"], "accepted": true, "id": "1701.01722"}, "pdf": {"name": "1701.01722.pdf", "metadata": {"source": "CRF", "title": "Follow the Compressed Leader: Faster Algorithms for Matrix Multiplicative Weight Updates", "authors": ["Zeyuan Allen-Zhu", "Yuanzhi Li"], "emails": ["zeyuan@csail.mit.edu", "yuanzhil@cs.princeton.edu"], "sections": [{"heading": null, "text": "\u221a worse than the optimal regret of the MMWU for dimension d matrices. In this paper, we propose a framework following the compressed guide that not only corresponds to the optimal regret of the MMWU (up to polylog factors), but runs even faster than FTPL. Our main idea is to \"compress\" the matrix exponential calculation to dimension 3 in the opposite setting or dimension 1 in the stochastic setting. This result solves an open question on how to obtain both (almost) optimal and efficient algorithms for the online eigenvector problem [16].ar Xiv: 170 1.01 722v 1 [cs.L G] 6J an"}, {"heading": "1 Introduction", "text": "The Multiplicative Weight Update (MWU) method is a simple but extremely powerful algorithm tool that has been repeatedly discovered in the theory of calculation, machine learning, optimization, and game theory (see, for example, the study [9] and the book [12]). Its natural matrix extension, commonly known as the Matrix Multiplicative Weight Update (MMWU = 1), has been used in the direction of efficient algorithms for solving semi-defined programs. [3, 10, 28], balanced delimiters [7, 22], and even in the evidence of QIP = PSPACE [19]. To discuss MMWU on an abstract level, let us consider the online linear optimization proble.Online optimization optimization. Let us consider d = {U-Rd \u00b7 d \u00b7 d = 1-U = 0} the number of density matrices in dimension.1."}, {"heading": "1.1 Our Main Results", "text": "We propose a \"follow-the-compressed-leader\" (FTCL) strategy that compresses the MMU strategy at a high level to dimension m = 3, as opposed to dimension m = land use plan (1 / 2) = land use plan (T) in MMWU-3It requires additional but standard efforts to turn this into a result with high confidence. 4The best FTO algorithm runs in time O (d3) as opposed to FTO (d2). 5Specifically, the calculation W1 / 2 k Q m requires calculations from Exp (\u2212 3 / 2) that are applied to vectors, and this can be done using FTO approximation to FTO (d3) with a total runtime O (d2 \u00b7 s)."}, {"heading": "1.2 Our Side Result: Stochastic Online Eigenvector", "text": "Our idea of compression also leads to a faster algorithm for the special case of the online eigenvector problem, where the opponent is stochastic, meaning that A1,..., AT are selected from a common distribution whose expectation corresponds to a matrix B, regardless of the actions of the player. For this problem, Garber, Hazan and Ma [16] have shown that a block power method corresponds to optimal regret and enjoys an efficient O (INZ) time translation per iteration. Shamir [30] analyzed the so-called Oja algorithm, but its total regret is O (\u221a dT log (T), which is a factor that d is worse than optimal. 6In this paper we show that Oja's algorithm actually has only one total regret (\u221a T log d) for the stochastic online eigenvector problem, which is optimal to zero."}, {"heading": "1.3 Our Stronger Results in a More Refined Language", "text": "In the previous sections, we have followed tradition and discussed our results and previous work on the assumption that the worst possibility is that we regret it. Indeed, this has led to simplified notations.6In the specific case that Ak takes rank 1, the FTCL algorithm may have recently shown regret for Oja's algorithm by [6] using other techniques that differ from us.If we are much smaller than 1, our complexity limits can be improved on quantities that depend on whether the FTCL results in that language are improved. At a high level, for our FTCL work, both the contradictory and stochastic settings are improved, \u2022 the total regret formula improves from O position 1 to place 1. We refer to this as the inferior language in Table 2. We give our FTCL results for that language in Table 2. At a high level, for our FTCL work, at a higher level, the total regret formula improves from O position, for the contradictory and stoic settings in O \u2022"}, {"heading": "1.4 Other Related Works", "text": "For the online eigenvector problem, if the feedback matrices Ak are only of rank-1, the FTPL's O self-pity can be improved to O-1 (d1 / 4T 1 / 2), as first shown by Dwork et al. [13] and independently of Kot lowski and Warmuth [21]. This d1 / 4 factor for the rank-1 case and the d1 / 2 factor for the high-level case are scarce, at least for their proposed FTPL methods [18]. Abernethy et al. have shown that FTPL strategies can also be analyzed using an FTRL framework [1]. 7This is because in the same footnote 5 notations, the pro-iteration time running O-1-2 iteration time for the proposed FTPL methods is scarce. Abernethy et al. have shown that FTPL strategies can also be analyzed using an FTRL framework."}, {"heading": "1.5 Roadmap", "text": "We introduce necessary notations in Section 2 and discuss the difficulties at a high level and our techniques in Section 3. We introduce a new trace inequality in Section 4 to be used in our main evidence. In Section 5 we prove our main result for an unconscious opponent and then extend it to the opposing attitude in Section 6. We discuss how FTCL can be used quickly in Section 7. Finally, in Section 8 we provide our FTCL result for a stochastic opponent. All our results are formulated and proven directly in the refined language."}, {"heading": "2 Notations and Preliminaries", "text": "We define k def = 1 Ai for each k = 0, 1,.,., T. Since each Ak is semi-uniquely positive (PSD), we can find Pk-Rd \u00b7 d in such a way that Ak = PkP > k; we only use Pk for analysis purposes. We write A-B def = Tr (A > B). We write A-B if A, B are symmetrical matrices, and A-B is PSD. We write [A] i, j the (i, j) th input from A. We use nnz (M) to multiply the time needed for matrix M, Rd \u00b7 d with an arbitrary vector in Rd. Specifically, nnz (M) is at most d plus the number of non-zero elements in M. We call nnz (A) def = maxxxxxk [T]."}, {"heading": "3 High-Level Discussion of Our Techniques", "text": "Let us first revisit the high-level idea behind MMWU. Recall Wk = > exp (ckI + formance), where the unique constant is such that TrWk = 1. Well, the key idea behind the analysis of MMWU is to use the Golden Thompson inequality: Tr (eckI + formance) \u2264 Tr (eckI + formance) \u2264 Tr (eckI + randness) \u2212 Tr (eckI + formance) it is only possible to use the Golden Thompson inequality \u2022 Ak.8This \"equivalence\" is not a black-box reduction; one usually has to open the analysis and switch the solution of one to the other. In other words, the profit value Wk \u2022 Ak, up to a factor, is proportional to the change in the trace function. One can also use the context to show Tr (eck + 1I)."}, {"heading": "4 A New Trace Inequality", "text": "Before working on MMWU and its extensions, one relies heavily on one of the following trace inequalities [7]: Golden Thompson inequalities: Tr (eA + \u03b7B) \u2264 Tr (eAe\u03b7B) Lieb-Thirring inequalities: Tr ((A + \u03b7B) k) \u2264 Tr (Ak / 2 (I + \u03b7A \u2212 1 / 2BA \u2212 1 / 2) kAk / 2). Due to our compression framework in this paper, we need inequalities of the type \"Tr (eA + \u03b7BD) \u2264 Tr (e\u03b7A / 2DeA / 2)\" Tr (A + \u03b7B) kD) \u2264 Tr (((I + \u03b7B) \u2212 1 / 2) kAk / 2DAk / 2). \"(4.1), which look almost like\" generalizations \"of Golden-Thompson and Lieb-Thirring (by setting D = I)."}, {"heading": "5 Oblivious Online Eigenvector + Expected Regret", "text": "In this section, we will first focus on a simpler oblivious setting. A1,.., AT are PSD matrices selected in advance by the opponent, and they are not dependent on the actions of the player. (In Section 6, we generalize this result to the full adversarial setting with high confidence.) Our FTCLobl algorithm is represented in Algorithm 1. (In Section 6, we generalize this result to the full adversarial setting with high confidence.) This FTCLobl algorithm is represented in Algorithm 1. (In Section 2, we generalize this result.) We start with a learning rate of > 0."}, {"heading": "5.1 Well-Conditioning Events", "text": "Due to concentration reasons, the potential increase could only be \"appropriate\" for well-conditioned matrices U. We are now making this definition formal. In view of some parameters \u03b4 > 0, which we will select later than 1 / T 3, we introduce the following event: Definition 5.2. For each k \u00b2 event {0, 1,.., T} we define eventEk (U) def = {\u03bd > 1 U\u03bd1 \u2265 2 and vice versa. \u2212 Let E < j (U) def \u2212 j \u2212 1 k = 0 Ek (U). Intuitively, Event Ek (U) ensures that the matrix U is \"well-conditioned\" (.) in Eduk's own base: (1) it has a not so small first coordination point > 1 U. \u2212 If event Ek (U) ensures that the matrix properties are \"well-conditioned.\""}, {"heading": "5.2 First Potential Increase", "text": "There is a constant C > 1, so that if q \u2265 max {log 2\u03b4, log (3d log ed\u03b4)} and \u03b7 \u2264 13q3, E [Tr ((ckI \u2212 throuk) \u2212 (q \u2212 1) U) \u00b7 1E < k (U)] \u2264 (q \u2212 1) \u03b7 (1 + C \u00b7 throuq5 log (d / \u03b4)) E [Ak \u2022 X1 / 2k UX 1 / 2 k] + (g \u2212 T + e) T\u03b4. The proof of Lemma 5.5 is the most important technical contribution of this work and differs most from the classical analysis of MWU. It uses our trace imbalance in section 4 and is the only place in our analysis that is based on rank (U)."}, {"heading": "5.3 Second Potential Increase", "text": "For all q \u2265 2 and \u03b7 > 0, E [Tr ((ck + 1I \u2212 \u03b7\u0432\u0430k) \u2212 (q \u2212 1) U) \u00b7 1E < (k + 1) (U)] \u2212 E [Tr (((ckI \u2212 \u041a\u0430\u0441k) \u2212 (q \u2212 1) U) \u2264 \u2212 (q \u2212 1) (E [ck + 1] \u2212 E [ck]) Finally, in Appendix E, we prove that theorem 1 is a direct consequence of our two above mentioned potential increase emmats."}, {"heading": "6 Adversarial Online Eigenvector + Regret in High-Confidence", "text": "In this section we switch to the more difficult constellation of the opponent: In each iteration k > q, the opponent chooses Ak after seeing the player's strategies (max.,.., wk \u2212 1). In other words, Ak can depend on the randomness that is used as good in generating w1,.., wk \u2212 1. In such a case, we choose the same order-3 wishart distribution with D and choose a variant of FTCLobl from FTCLobl, where a new random Uk is generated from D per iteration (wk \u2212 1). In other words, instead of choosing the same order-3 only once, we choose U1,.., UT i.e. from D. Then the normalization constant ck is defined to Tr ((((ckI \u2212 qUk \u2212 1) \u2212 qUk \u2212 1) \u2212 qUk \u2212 1) = 1. We call this algorithm FTCLadCLadCLadv and we will show it in the next paragraph that the constellation is complete... '"}, {"heading": "7 Efficient Implementation", "text": "It should be remembered that our reuetheorems were based on the assumption that, in each iteration, the three vectors vj def = X 1 / 2 k uj = (i) - (i) - (ii) - (ii) - (ii) - (ii) - (ii) - (ii) - (ii) - (ii) - (ii) - (ii) - (ii) - (ii) - (ii) - (ii) - (ii) - (ii) - (ii) -) - (ii) - (ii) - (ii) - (ii) -) - (ii) - (ii) -) - (ii) - (ii) - (ii) - (ii) - (ii) -) - (ii) -) - (ii) - (ii) -) (ii)) - (ii) -) (ii) (ii) (ii))))) (ii))) (ii) (ii))) (ii)) (ii))) (ii)) (ii))) (ii))) (ii)) (ii)) (ii)))) (ii))) (ii))) (ii)))) (ii) (ii)))) (ii)) (ii))) (ii))) (ii))) (ii))) (ii))) (ii)))) (ii) (ii))) (ii))) (ii)))) (ii)))) (ii)) (ii))) (ii) (ii)))))) (ii))) (ii)))) (ii) (ii))) (ii)))) (ii))))) (ii)))) (ii)))) (ii)))) (ii)))) (ii))))))))"}, {"heading": "8 Stochastic Online Eigenvector", "text": "Consider the simplest setting when the matrices A1,.., AT i.i.d.d. are generated from a common distribution whose expectation corresponds to B. This is known as a stochastic online eigenvector problem, and we would like to minimize the regret that T k = 1w > k Akwk \u2212 T \u00b7 \u03bbmax (B).14In this setting we check Oja's algorithm: starting with a random vector u \u0439Rd, in which each ui i.i.d. is drawn from N (0, 1), playing wk with each iteration (I + \u03b7Ak \u2212 1) \u00b7 \u00b7 Choja's algorithm: after normalization. It is clear that wk can be calculated from wk \u2212 1 with an additional time nz (A). We include in Annex H a unilateral proof for the following theorem (I + \u03b7Ak \u2212 1)."}, {"heading": "Acknowledgement", "text": "We thank Yin Tat Lee for discussing the problem of how to condense MMWU 2015 to a constant dimension. We thank Elad Hazan for raising the issue and for some enlightening discussions. We thank Dan Garber and Tengyu Ma for clarifying some of the results of previous work [16].14 In principle, one can also ask to minimize regret about where T \u00b7 B is being replaced by A1 + \u00b7 \u00b7 + AT. However, due to simple concentration results, there is not much difference between the two different terms. [16] Appendix"}, {"heading": "A Proof of Lemma 4.1", "text": "Lemma 4.1. For all symmetrical matrices A, B, D, D, Rd \u00b7 d, each integer k = 1, each integer p = 1, each integer p = 0, and each integer p = 1, and each integer p = 1, and each integer p = 1, and each integer p = 0, p = 2, if A and D are commutative, then (A + p = B) k \u2022 D \u2212 Ak \u2022 D \u2212 Ak \u2212 1D + (p = 2) 2 maxp = (0, p \u00b2) k \u2022 D \u2212 A \u2212 Ak \u2022 D = k \u00b2 s (A + p \u00b2) k = 1,..., j \u00b7 Z = 0 j0 j0 + \u00b7 Jak = k = k = k \u2212 iAj1B \u00b7 \u00b7 \u00b7 BAji \u2022 DIts first order f \u2212 kkjab 0, f \u00b7 kjab 0 (A + p \u00b2) k = 1, f \u00b7 kjab 0, f \u00b7 jab jab 0 = 0 jk, jk = 1 from k, jk jk = 1 (ab, k jk = 1, k from k = 1, k from k = 1, k from k = 1, k from k = 1, jk = jk = 1"}, {"heading": "B Proof for Section 5.1", "text": "B.1 Proof of Lemma 5.3Lemma 5.3. For each k = 0, 1,.,., T, we have PrU [Ek (U)] [EK] [EK) [EK) [EK) [EK) [EK)] [EK) [EK) [EK) [EK) [EK) = EK) = EK) = EK). \u2212 K = EK = EK = EK = EK)., T, we have PrU [EK) [EK) [EK). \u2212 K,. \u2212 D, so that each eigenvector of the EK is drawn with non-increasing eigenvalues. In other words, from N (0, 1) for each i [d], j [EK]. [EK]. [EK). [EK). [EK). [EK)."}, {"heading": "C Proof for Section 5.2", "text": "Lemma 5.5. There are constant C > 1 such that if q \u2265 max {log 2\u043c, log (3d log edhion)} and \u03b7 \u2264 111q3, E [Tr ((ckI \u2212 throuq k) \u2212 (q \u2212 1) U) \u00b7 1E < k (U) \u00b7 Tr (ckI \u2212 g) (ckI \u2212 k \u2212 p (U) \u2212 p (q \u2212 P) \u2212 p (1 + C \u00b7 4))) E [Ak \u2022 X1 / 2k UX) (ckI) (ckI / 2 k) K (kI) + (kI).Proof. Let us assume the property vectors of K \u2212 1 with non-increasing eigenvalues that we assume without loss of generality that all vectors and matrices are written in this property."}, {"heading": "D Proof for Section 5.3", "text": "Lemma 5.7. For all q \u2265 2 and \u03b7 > 0, E [Tr ((ck + q + q + 1) < k (u)] \u2264 \u2212 (k + 1) (U) \u2212 E [ck]) proof. Remember that ck + 1 \u2265 ck, because all matrices are Ak PSD. Significant is (k) the eigenvectors of k with non-increasing eigenvalues. \u2212 20 we have for each U, Tr (ck + 1I - district) \u2212 (q \u2212 1) U) \u2212 (q \u2212 1) U-ck (ckI) \u2212 ck (ckI), (ckck) U-ck."}, {"heading": "E Proof of Theorem 1: Oblivious Online Eigenvector", "text": "The Evidence of Theorem 1 (E) < k + 1 (U) \u2212 E [Tr (X 1 \u2212 1 / q k U) \u00b7 q q q (Tr (X1 \u2212 1 / q k + 1 U) \u00b7 1E < k + 1 (U) \u2212 E [Tr (X 1 \u2212 1 / q k U) \u00b7 1E < k (U) \u2264 (q \u2212 1) \u2212 E (ck + 1) + (q \u2212 1) \u2212 E [Tr (X 1 \u2212 1 / q k U) < k (U) < k (U) < k (U) < k (U) < k + 1 (U) < k (U) < k (U) < k (U) < k (U) < k (U) < k (U) < k (U) < k (U) < k (U) < k (U) < k (U) < k (U) < k (U) < k (U) < k (U) < k (U) < k (U) < k (U) < k (U) < k (U) < k (U) < k (U) < k (U) < k (U) < k (U) < k (U) < k (U) < k (U) < k (U) < k (U) < k (U) < k (U (U) < k (U) < k (U) k (U) < k (U) < k (U (U) k (U), U (U) + T) + T) + T) (X + T) + T) (X) + T) (X) + T) (T) \u2212 T) (X + 1) (T)"}, {"heading": "F Proof of Theorem 2: Adversarial Online Eigenvector", "text": "The Proof of Theorem 2 (1): Before beginning our evidence, we want to emphasize that in this constellation we do not rely on the randomness of U1,. \u2212 k \u2212 k The randomness of U1,. \u2212 k The randomness of U1,. \u2212 k The randomness of U1,. \u2212 k The randomness of U1,. \u2212 k The randomness of U2,. \u2212 k The randomness of U2,. The randomness of U2,. The randomness of U1,. \u2212 k The randomness of U1,. \u2212 k The randomness of U2,. The randomness of U1,. \u2212 k The randomness of U2,."}, {"heading": "G Proof of Theorem 3: Implementation Details", "text": "(1) (1) (1) (1) (1) (1) (2) (2) (2) (2) (2) (2) (2) (2) (2) (2) (2) (2) (2) (2) (2) (2) (2) (2) (2) (2) (2) (2) (2) (2) (2) (2) (2) (2) (2) (2) (2) (2) (2) (2) (2) (2) (2) (2) (2) (2) (2) (2) (2) (2) (2) (2) (2) (2) (2) (2) (2) (2) (2) (2) (2) (2) (2) (2) (2) (2) (2) (2) (2) (2) (2) (2) (2) (2) (2) (2) (2) (2) (2) (2) (2) (2) (2) (2) (2) (2) (2) (2) (2) (2) (2) (2) (2) (2) (2) (2) (2) (2) (2) (2) (2) (2) (2) (2) (2) (2) (2) (2) (2) (2) (2) (2) (2) (2) (2) (2) (2) (2) (2) (2) (2) (2) (2) (2) (2) (2) (2) (2) (2) (2) (2) (2) (2) (2) (2) (2) (2) (2) (2) (2) (2) (2) (2) (2) (2) (2) (2) (2) (2) (2) (2) (2) (2) (2 (2) (2) (2) (2) (2) (2) (2) (2) (2) (2) (2) (2) (2) (2) (2) (2) ("}, {"heading": "H Proof of Theorem 4: Stochastic Online Eigenvector", "text": "Evidence of theory 4: Defined (Definition) Defined (Defined) Defined (Defined) Defined (Defined) (I + > Defined) Defined (I + > Defined) Defined (Defined) Defined (I + > Defined) Defined (I + > Defined) (I + > Defined) (I + > Defined) \u00b7 Defined (I + > Defined) \u00b7 \u00b7 Defined (I + > Defined) \u00b7 \u00b7 Defined (I + > Defined) (I + > Defined) (I + > Defined) (I + > Defined) (I + > Defined) (I + > Defined) (I + > Defined) (I + > Defined) (I + > Defined) (I + Defined) (I + > Defined) (I + Defined) (I + Defined) (I + Defined) (I + > Defined) (I + Defined) (I + Defined) (I + Defined) (I + > Defined) (I + Defined) (I + Defined) (I + Defined) (I + Defined) (I + Defined) (I + Defined) (I + Defined) (I + Defined) (I + Defined) (I + Defined) (I + Defined) (I + Defined) (I + Defined) (I + Defined) (I + Defined) (I + Defined) (I + Defined) (I + Defined) (I + Defined) (I + Defined) (I + Defined) (I + Defined)) (I + Defined) (I + Defined) (I + Defined) (I + Defined) (I + Defined) (I + Defined) (I + Defined) (I + Defined) (I + Defined) (I + Defined) (I + Defined (I + Defined)) (I + Defined (I + Defined) I + Defined (I + Defined (I + Defined) I + Defined) (I + Defined) (I + Defined (I + Defined) Defined (I + Defined) Defined (I + Defined) I Defined)"}, {"heading": "I A Simple Lower Bound for the \u03bb-Refined Language", "text": "We outline the proof that for the stochastic online eigenvector problem, there is a constant C > 0 for each \u03bb unit (0, 1), a PSD matrix B for the satisfaction of B \u03bbI, and a distribution D of (even rank-1) matrices with spectral standards at most 1 and expectations equal to B, so that for any learning algorithm learner, the total regret must be at least C \u00b7 270. \u2212 Such a lower limit naturally leads to the harder irreconcilable or forgotten settings. We prove this lower limit by reducing the problem to a lower limit for information theory, which has appeared in our separate paper [6]. The lower limit in [6] states that for every 1 more than 0, there is a PSD matrix B, where the largest two eigenvalues are equal and the largest eigenvalues are equal."}], "references": [{"title": "Online linear optimization via smoothing", "author": ["Jacob Abernethy", "Chansoo Lee", "Abhinav Sinha", "Ambuj Tewari"], "venue": "In COLT, pages 807\u2013823,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2014}, {"title": "Spectral smoothing via random matrix perturbations", "author": ["Jacob Abernethy", "Chansoo Lee", "Ambuj Tewari"], "venue": "ArXiv e-prints,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2015}, {"title": "Using optimization to obtain a widthindependent, parallel, simpler, and faster positive SDP solver", "author": ["Zeyuan Allen-Zhu", "Yin Tat Lee", "Lorenzo Orecchia"], "venue": "In Proceedings of the 27th ACM-SIAM Symposium on Discrete Algorithms,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2016}, {"title": "Doubly Accelerated Methods for Faster CCA and Generalized Eigendecomposition", "author": ["Zeyuan Allen-Zhu", "Yuanzhi Li"], "venue": "ArXiv e-prints,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2016}, {"title": "Even Faster SVD Decomposition Yet Without Agonizing Pain", "author": ["Zeyuan Allen-Zhu", "Yuanzhi Li"], "venue": "In NIPS,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2016}, {"title": "First Efficient Convergence for Streaming k-PCA: a Global, Gap- Free, and Near-Optimal Rate", "author": ["Zeyuan Allen-Zhu", "Yuanzhi Li"], "venue": "ArXiv e-prints,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2016}, {"title": "Spectral Sparsification and Regret Minimization Beyond Multiplicative Updates", "author": ["Zeyuan Allen-Zhu", "Zhenyu Liao", "Lorenzo Orecchia"], "venue": "In Proceedings of the 47th Annual ACM Symposium on Theory of Computing,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2015}, {"title": "Improved SVRG for Non-Strongly-Convex or Sum-of-Non-Convex Objectives", "author": ["Zeyuan Allen-Zhu", "Yang Yuan"], "venue": "In ICML,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2016}, {"title": "The Multiplicative Weights Update Method: a Meta- Algorithm and Applications", "author": ["Sanjeev Arora", "Elad Hazan", "Satyen Kale"], "venue": "Theory of Computing,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2012}, {"title": "A combinatorial, primal-dual approach to semidefinite programs", "author": ["Sanjeev Arora", "Satyen Kale"], "venue": "In Proceedings of the thirty-ninth annual ACM symposium on Theory of computing - STOC", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2007}, {"title": "Online principal components analysis", "author": ["Christos Boutsidis", "Dan Garber", "Zohar Karnin", "Edo Liberty"], "venue": "In Proceedings of the Twenty-Sixth Annual ACM-SIAM Symposium on Discrete Algorithms,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2015}, {"title": "Prediction, Learning, and Games", "author": ["Nicolo Cesa-Bianchi", "Gabor Lugosi"], "venue": null, "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2006}, {"title": "Analyze gauss: optimal bounds for privacy-preserving principal component analysis", "author": ["Cynthia Dwork", "Kunal Talwar", "Abhradeep Thakurta", "Li Zhang"], "venue": "In STOC,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2014}, {"title": "Un-regularizing: approximate proximal point and faster stochastic algorithms for empirical risk minimization", "author": ["Roy Frostig", "Rong Ge", "Sham M. Kakade", "Aaron Sidford"], "venue": "In ICML,", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2015}, {"title": "Fast and simple PCA via convex optimization", "author": ["Dan Garber", "Elad Hazan"], "venue": "ArXiv e-prints,", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2015}, {"title": "Online learning of eigenvectors", "author": ["Dan Garber", "Elad Hazan", "Tengyu Ma"], "venue": "In Proceedings of the 32nd International Conference on Machine Learning", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2015}, {"title": "The noisy power method: A meta algorithm with applications", "author": ["Moritz Hardt", "Eric Price"], "venue": "In NIPS,", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2014}, {"title": "QIP = PSPACE", "author": ["Rahul Jain", "Zhengfeng Ji", "Sarvagya Upadhyay", "John Watrous"], "venue": "Journal of the ACM (JACM),", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2011}, {"title": "Online pca with spectral bounds", "author": ["Zohar Karnin", "Edo Liberty"], "venue": "In Proceedings of the 28th Annual Conference on Computational Learning Theory (COLT),", "citeRegEx": "20", "shortCiteRegEx": "20", "year": 2015}, {"title": "Pca with gaussian perturbations", "author": ["Wojciech Kot  lowski", "Manfred K. Warmuth"], "venue": "ArXiv e-prints,", "citeRegEx": "21", "shortCiteRegEx": "21", "year": 2015}, {"title": "Constructing linear-sized spectral sparsification in almost-linear time", "author": ["Yin Tat Lee", "He Sun"], "venue": "In FOCS,", "citeRegEx": "22", "shortCiteRegEx": "22", "year": 2015}, {"title": "A Universal Catalyst for First-Order Optimization", "author": ["Hongzhou Lin", "Julien Mairal", "Zaid Harchaoui"], "venue": null, "citeRegEx": "23", "shortCiteRegEx": "23", "year": 2015}, {"title": "Introductory Lectures on Convex Programming Volume: A Basic course, volume I", "author": ["Yurii Nesterov"], "venue": "Kluwer Academic Publishers,", "citeRegEx": "24", "shortCiteRegEx": "24", "year": 2004}, {"title": "Online pca with optimal regrets", "author": ["Jiazhong Nie", "Wojciech Kot  lowski", "Manfred K Warmuth"], "venue": "In International Conference on Algorithmic Learning Theory,", "citeRegEx": "25", "shortCiteRegEx": "25", "year": 2013}, {"title": "Fast Approximation Algorithms for Graph Partitioning using Spectral and Semidefinite-Programming Techniques", "author": ["Lorenzo Orecchia"], "venue": "PhD thesis, EECS Department,", "citeRegEx": "26", "shortCiteRegEx": "26", "year": 2011}, {"title": "Approximating the exponential, the lanczos method and an \u00d5(m)-time spectral algorithm for balanced separator", "author": ["Lorenzo Orecchia", "Sushant Sachdeva", "Nisheeth K. Vishnoi"], "venue": null, "citeRegEx": "27", "shortCiteRegEx": "27", "year": 2012}, {"title": "Faster and simpler width-independent parallel algorithms for positive semidefinite programming", "author": ["Richard Peng", "Kanat Tangwongsan"], "venue": null, "citeRegEx": "28", "shortCiteRegEx": "28", "year": 2012}, {"title": "SDCA without Duality, Regularization, and Individual Convexity", "author": ["Shai Shalev-Shwartz"], "venue": "In ICML,", "citeRegEx": "29", "shortCiteRegEx": "29", "year": 2016}, {"title": "Convergence of stochastic gradient descent for pca", "author": ["Ohad Shamir"], "venue": "In ICML,", "citeRegEx": "30", "shortCiteRegEx": "30", "year": 2016}, {"title": "An introduction to the conjugate gradient method without the agonizing pain", "author": ["Jonathan Richard Shewchuk"], "venue": null, "citeRegEx": "31", "shortCiteRegEx": "31", "year": 1994}, {"title": "Note on the gamma function", "author": ["J.G. Wendel"], "venue": "The American Mathematical Monthly,", "citeRegEx": "32", "shortCiteRegEx": "32", "year": 1948}], "referenceMentions": [{"referenceID": 24, "context": "Matrix multiplicative weight update (MMWU) [26] is an extremely powerful algorithmic tool for computer science and related fields.", "startOffset": 43, "endOffset": 47}, {"referenceID": 15, "context": "This result resolves an open question regarding how to obtain both (nearly) optimal and efficient algorithms for the online eigenvector problem [16].", "startOffset": 144, "endOffset": 148}, {"referenceID": 8, "context": "The multiplicative weight update (MWU) method is a simple but extremely powerful algorithmic tool that has been repeatedly discovered in theory of computation, machine learning, optimization, and game theory (see for instance the survey [9] and the book [12]).", "startOffset": 237, "endOffset": 240}, {"referenceID": 11, "context": "The multiplicative weight update (MWU) method is a simple but extremely powerful algorithmic tool that has been repeatedly discovered in theory of computation, machine learning, optimization, and game theory (see for instance the survey [9] and the book [12]).", "startOffset": 254, "endOffset": 258}, {"referenceID": 24, "context": "Its natural matrix extension, commonly known as matrix multiplicative weight update (MMWU) [26], has been used towards efficient algorithms for solving semidefinite programs [3, 10, 28], balanced separators [27], Ramanujan sparsifiers [7, 22], and even in the proof of QIP = PSPACE [19].", "startOffset": 91, "endOffset": 95}, {"referenceID": 2, "context": "Its natural matrix extension, commonly known as matrix multiplicative weight update (MMWU) [26], has been used towards efficient algorithms for solving semidefinite programs [3, 10, 28], balanced separators [27], Ramanujan sparsifiers [7, 22], and even in the proof of QIP = PSPACE [19].", "startOffset": 174, "endOffset": 185}, {"referenceID": 9, "context": "Its natural matrix extension, commonly known as matrix multiplicative weight update (MMWU) [26], has been used towards efficient algorithms for solving semidefinite programs [3, 10, 28], balanced separators [27], Ramanujan sparsifiers [7, 22], and even in the proof of QIP = PSPACE [19].", "startOffset": 174, "endOffset": 185}, {"referenceID": 26, "context": "Its natural matrix extension, commonly known as matrix multiplicative weight update (MMWU) [26], has been used towards efficient algorithms for solving semidefinite programs [3, 10, 28], balanced separators [27], Ramanujan sparsifiers [7, 22], and even in the proof of QIP = PSPACE [19].", "startOffset": 174, "endOffset": 185}, {"referenceID": 25, "context": "Its natural matrix extension, commonly known as matrix multiplicative weight update (MMWU) [26], has been used towards efficient algorithms for solving semidefinite programs [3, 10, 28], balanced separators [27], Ramanujan sparsifiers [7, 22], and even in the proof of QIP = PSPACE [19].", "startOffset": 207, "endOffset": 211}, {"referenceID": 6, "context": "Its natural matrix extension, commonly known as matrix multiplicative weight update (MMWU) [26], has been used towards efficient algorithms for solving semidefinite programs [3, 10, 28], balanced separators [27], Ramanujan sparsifiers [7, 22], and even in the proof of QIP = PSPACE [19].", "startOffset": 235, "endOffset": 242}, {"referenceID": 20, "context": "Its natural matrix extension, commonly known as matrix multiplicative weight update (MMWU) [26], has been used towards efficient algorithms for solving semidefinite programs [3, 10, 28], balanced separators [27], Ramanujan sparsifiers [7, 22], and even in the proof of QIP = PSPACE [19].", "startOffset": 235, "endOffset": 242}, {"referenceID": 17, "context": "Its natural matrix extension, commonly known as matrix multiplicative weight update (MMWU) [26], has been used towards efficient algorithms for solving semidefinite programs [3, 10, 28], balanced separators [27], Ramanujan sparsifiers [7, 22], and even in the proof of QIP = PSPACE [19].", "startOffset": 282, "endOffset": 286}, {"referenceID": 0, "context": "The player receives a gain Ak \u2022Wk def = Tr(AkWk) \u2208 [0, 1].", "startOffset": 51, "endOffset": 57}, {"referenceID": 24, "context": "The best choice \u03b7 = \u221a log d/ \u221a T yields a total regret at most O( \u221a T log d) [26], and this is optimal up to constant [9].", "startOffset": 77, "endOffset": 81}, {"referenceID": 8, "context": "The best choice \u03b7 = \u221a log d/ \u221a T yields a total regret at most O( \u221a T log d) [26], and this is optimal up to constant [9].", "startOffset": 118, "endOffset": 121}, {"referenceID": 6, "context": "Some authors also refer to MMWU as the follow-the-regularized-leader strategy or FTRL for short, because MMWU can be analyzed from a mirror-descent view with the matrix entropy function as its regularizer [7].", "startOffset": 205, "endOffset": 208}, {"referenceID": 1, "context": "If instead of playing an arbitrary matrix in \u2206d, the player is only allowed to play a rank-1 matrix Wk = wkw > k , then this online matrix optimization becomes the well-known online eigenvector problem [2, 13, 16, 21, 25]:", "startOffset": 202, "endOffset": 221}, {"referenceID": 12, "context": "If instead of playing an arbitrary matrix in \u2206d, the player is only allowed to play a rank-1 matrix Wk = wkw > k , then this online matrix optimization becomes the well-known online eigenvector problem [2, 13, 16, 21, 25]:", "startOffset": 202, "endOffset": 221}, {"referenceID": 15, "context": "If instead of playing an arbitrary matrix in \u2206d, the player is only allowed to play a rank-1 matrix Wk = wkw > k , then this online matrix optimization becomes the well-known online eigenvector problem [2, 13, 16, 21, 25]:", "startOffset": 202, "endOffset": 221}, {"referenceID": 19, "context": "If instead of playing an arbitrary matrix in \u2206d, the player is only allowed to play a rank-1 matrix Wk = wkw > k , then this online matrix optimization becomes the well-known online eigenvector problem [2, 13, 16, 21, 25]:", "startOffset": 202, "endOffset": 221}, {"referenceID": 23, "context": "If instead of playing an arbitrary matrix in \u2206d, the player is only allowed to play a rank-1 matrix Wk = wkw > k , then this online matrix optimization becomes the well-known online eigenvector problem [2, 13, 16, 21, 25]:", "startOffset": 202, "endOffset": 221}, {"referenceID": 6, "context": "This more challenging setting is very desirable for multiple reasons: \u2022 in many applications \u2014such as graph problems [7, 22]\u2014 Ak does not depend on wk; \u2022 vector-based strategies wk can be cheaper to compute and more efficient to communicate.", "startOffset": 117, "endOffset": 124}, {"referenceID": 20, "context": "This more challenging setting is very desirable for multiple reasons: \u2022 in many applications \u2014such as graph problems [7, 22]\u2014 Ak does not depend on wk; \u2022 vector-based strategies wk can be cheaper to compute and more efficient to communicate.", "startOffset": 117, "endOffset": 124}, {"referenceID": 6, "context": "MMWU [7, 9] \u00d5( \u221a T ) O(d) \u00d5 ( d \u03b52 )", "startOffset": 5, "endOffset": 11}, {"referenceID": 8, "context": "MMWU [7, 9] \u00d5( \u221a T ) O(d) \u00d5 ( d \u03b52 )", "startOffset": 5, "endOffset": 11}, {"referenceID": 6, "context": "MMWU with JL [7, 28] \u00d5( \u221a T ) \u00d5 ( T 5 4 nnz(\u03a3) ) \u00d5 ( 1 \u03b54.", "startOffset": 13, "endOffset": 20}, {"referenceID": 26, "context": "MMWU with JL [7, 28] \u00d5( \u221a T ) \u00d5 ( T 5 4 nnz(\u03a3) ) \u00d5 ( 1 \u03b54.", "startOffset": 13, "endOffset": 20}, {"referenceID": 15, "context": "FTPL (T \u2265 d only) [16] \u00d5( \u221a dT ) \u00d5 ( T 3 4 d\u2212 1 4 nnz(\u03a3) ) \u00d5 ( d \u03b53.", "startOffset": 18, "endOffset": 22}, {"referenceID": 15, "context": "block power method [16] \u00d5( \u221a T ) O ( nnz(\u03a3) ) \u00d5 ( 1 \u03b52 nnz(\u03a3) )", "startOffset": 19, "endOffset": 23}, {"referenceID": 2, "context": "4 Some researchers [3, 7, 22, 28] use the Johnson-Lindenstrauss (JL) compression to reduce the dimension of Wk to make it more efficiently computable.", "startOffset": 19, "endOffset": 33}, {"referenceID": 6, "context": "4 Some researchers [3, 7, 22, 28] use the Johnson-Lindenstrauss (JL) compression to reduce the dimension of Wk to make it more efficiently computable.", "startOffset": 19, "endOffset": 33}, {"referenceID": 20, "context": "4 Some researchers [3, 7, 22, 28] use the Johnson-Lindenstrauss (JL) compression to reduce the dimension of Wk to make it more efficiently computable.", "startOffset": 19, "endOffset": 33}, {"referenceID": 26, "context": "4 Some researchers [3, 7, 22, 28] use the Johnson-Lindenstrauss (JL) compression to reduce the dimension of Wk to make it more efficiently computable.", "startOffset": 19, "endOffset": 33}, {"referenceID": 1, "context": "Many researchers also analyzed the so-called followthe-perturbed-leader (FTPL) strategy for this problem [2, 13, 16, 21].", "startOffset": 105, "endOffset": 120}, {"referenceID": 12, "context": "Many researchers also analyzed the so-called followthe-perturbed-leader (FTPL) strategy for this problem [2, 13, 16, 21].", "startOffset": 105, "endOffset": 120}, {"referenceID": 15, "context": "Many researchers also analyzed the so-called followthe-perturbed-leader (FTPL) strategy for this problem [2, 13, 16, 21].", "startOffset": 105, "endOffset": 120}, {"referenceID": 19, "context": "Many researchers also analyzed the so-called followthe-perturbed-leader (FTPL) strategy for this problem [2, 13, 16, 21].", "startOffset": 105, "endOffset": 120}, {"referenceID": 15, "context": "Most notably, Garber, Hazan and Ma [16] proposed to compute an (approximate) leading eigenvector of the matrix \u03a3k\u22121 +rr> at iteration k, where r is a random vector whose norm is carefully chosen.", "startOffset": 35, "endOffset": 39}, {"referenceID": 15, "context": "For this problem, Garber, Hazan, and Ma [16] showed that a block power method matches the optimum regret and enjoys an efficient O(nnz(\u03a3T ))-time implementation per iteration.", "startOffset": 40, "endOffset": 44}, {"referenceID": 28, "context": "Shamir [30] analyzed the so-called Oja\u2019s algorithm but his total regret is O( \u221a dT log(T )) which is a factor \u221a d worse than optimum.", "startOffset": 7, "endOffset": 11}, {"referenceID": 15, "context": "If nnz(\u03a3T ) = d 2 and nnz(A) = O(d), our running time is O(d) times faster than [16].", "startOffset": 80, "endOffset": 84}, {"referenceID": 5, "context": "In the special case of Ak being rank-1, the \u00d5( \u221a T ) regret for Oja\u2019s algorithm was recently shown by [6], using different techniques from us.", "startOffset": 102, "endOffset": 105}, {"referenceID": 6, "context": "MMWU [7, 9] \u00d5( \u221a \u03bbT ) O(d) \u00d5 ( \u03bbd \u03b52 )", "startOffset": 5, "endOffset": 11}, {"referenceID": 8, "context": "MMWU [7, 9] \u00d5( \u221a \u03bbT ) O(d) \u00d5 ( \u03bbd \u03b52 )", "startOffset": 5, "endOffset": 11}, {"referenceID": 6, "context": "MMWU with JL [7, 28] \u00d5( \u221a \u03bbT ) \u00d5 ( T 5 4 \u03bb\u2212 3 4 nnz(\u03a3) ) \u00d5 ( \u03bb \u03b54.", "startOffset": 13, "endOffset": 20}, {"referenceID": 26, "context": "MMWU with JL [7, 28] \u00d5( \u221a \u03bbT ) \u00d5 ( T 5 4 \u03bb\u2212 3 4 nnz(\u03a3) ) \u00d5 ( \u03bb \u03b54.", "startOffset": 13, "endOffset": 20}, {"referenceID": 15, "context": "block power method [16] \u00d5( \u221a \u03bbT ) O ( nnz(\u03a3) ) \u00d5 ( 1 \u03b52 nnz(\u03a3) )", "startOffset": 19, "endOffset": 23}, {"referenceID": 0, "context": "We denote by \u03a3 = A1 + \u00b7 \u00b7 \u00b7+ AT , by nnz(A) = maxk\u2208[T ] { nnz(Ak) } , and by \u03bb = 1 T \u03bbmax(\u03a3) \u2208 [0, 1].", "startOffset": 95, "endOffset": 101}, {"referenceID": 15, "context": "7 The block power method (for the stochastic online eigenvector problem) can also be analyzed in this \u03bb-refined language, for instance by modifying the proof in [16].", "startOffset": 161, "endOffset": 165}, {"referenceID": 12, "context": "[13] and independently shown by Kot lowski and Warmuth [21].", "startOffset": 0, "endOffset": 4}, {"referenceID": 19, "context": "[13] and independently shown by Kot lowski and Warmuth [21].", "startOffset": 55, "endOffset": 59}, {"referenceID": 0, "context": "showed that FTPL strategies can also be analyzed using a FTRL framework [1].", "startOffset": 72, "endOffset": 75}, {"referenceID": 23, "context": "studied the high-rank variant using MMWU [25], but their per-iteration running time is still O(d3) due to eigendecomposition.", "startOffset": 41, "endOffset": 45}, {"referenceID": 10, "context": "Some authors also study a very different online model for computing the top k eigenvectors[11, 20]: they are interested in outputting O(k \u00b7 poly(1/\u03b5)) vectors instead of k but with a good PCA reconstruction error.", "startOffset": 90, "endOffset": 98}, {"referenceID": 18, "context": "Some authors also study a very different online model for computing the top k eigenvectors[11, 20]: they are interested in outputting O(k \u00b7 poly(1/\u03b5)) vectors instead of k but with a good PCA reconstruction error.", "startOffset": 90, "endOffset": 98}, {"referenceID": 5, "context": "The stochastic online eigenvector problem is almost equivalent to the streaming PCA problem [6, 17].", "startOffset": 92, "endOffset": 99}, {"referenceID": 16, "context": "The stochastic online eigenvector problem is almost equivalent to the streaming PCA problem [6, 17].", "startOffset": 92, "endOffset": 99}, {"referenceID": 5, "context": "The two papers [6, 17] use different techniques from ours and do not imply our result on stochastic online eigenvector.", "startOffset": 15, "endOffset": 22}, {"referenceID": 16, "context": "The two papers [6, 17] use different techniques from ours and do not imply our result on stochastic online eigenvector.", "startOffset": 15, "endOffset": 22}, {"referenceID": 4, "context": "For the most efficient offline eigenvectors algorithms, we refer interested readers to our paper [5] (for PCA / SVD) and [4] (for CCA and generalized eigendecomposition).", "startOffset": 97, "endOffset": 100}, {"referenceID": 3, "context": "For the most efficient offline eigenvectors algorithms, we refer interested readers to our paper [5] (for PCA / SVD) and [4] (for CCA and generalized eigendecomposition).", "startOffset": 121, "endOffset": 124}, {"referenceID": 6, "context": "In a recent result, the authors of [7] generalized MMWU to `1\u22121/q regularized strategies.", "startOffset": 35, "endOffset": 38}, {"referenceID": 6, "context": "We encourage interested readers to see the introduction of [7] for more background information, but we shall make this present paper self-contained.", "startOffset": 59, "endOffset": 62}, {"referenceID": 6, "context": "Prior work on MMWU and its extensions relies heavily on one of the following trace inequalities [7]: Golden-Thompson inequality : Tr(e) \u2264 Tr ( ee )", "startOffset": 96, "endOffset": 99}, {"referenceID": 2, "context": "If this is the case, then one can compute the 3\u00d7 3 matrix ( ui Xkuj ) i,j\u2208[3] explicitly, and then we can obtain its rank-3 eigendecomposition X 1/2 k UX 1/2 k = \u22113 j=1 pj \u00b7 yjy> j in O(d) time.", "startOffset": 74, "endOffset": 77}, {"referenceID": 6, "context": "At a high level, issue (a) is not a big deal because if v\u2032 j satisfies \u2016vj \u2212 v\u2032 j\u20162 \u2264 \u03b5\u0303/poly(d, T ) and we use v\u2032 j instead of vj , then the final regret is affected by less than \u03b5\u0303; issue (b) can be dealt as long as we perform a careful binary search to find ck, similar to prior work [7]; issue (c) can be done as long as we have a good control on the condition number of the matrix ( ckI\u2212 \u03b7\u03a3k\u22121 ) .", "startOffset": 287, "endOffset": 290}, {"referenceID": 15, "context": "We thank Dan Garber and Tengyu Ma for clarifying some results of prior work [16].", "startOffset": 76, "endOffset": 80}, {"referenceID": 15, "context": "[16]", "startOffset": 0, "endOffset": 4}, {"referenceID": 2, "context": "from N (0, 1) for every i \u2208 [d], j \u2208 [3].", "startOffset": 37, "endOffset": 40}, {"referenceID": 6, "context": "1) like it was used in [7].", "startOffset": 23, "endOffset": 26}, {"referenceID": 2, "context": "Notice that TrU = 13 \u2211 i\u2208[d],j\u2208[3](uj,i) 2 so 3TrU is distributed according to chisquared distribution \u03c72(3d) whose PDF is p(x) = 2 \u2212 3d 2 e\u2212 x 2 x 3d 2 \u22121 \u0393(3d/2) .", "startOffset": 31, "endOffset": 34}, {"referenceID": 30, "context": "Wendell [32]), and the second inequality uses our assumption on q.", "startOffset": 8, "endOffset": 12}, {"referenceID": 0, "context": "Let {Zt}t=1 be a random process with respect to a filter {0,\u03a9} = F0 \u2282 F1 \u2282 \u00b7 \u00b7 \u00b7 \u2282 FT and each Zt \u2208 [0, 1] is Ft-measurable.", "startOffset": 100, "endOffset": 106}, {"referenceID": 0, "context": "(The first inequality has used (a\u2212 b)2 \u2264 a2 + b2 when a, b \u2265 0, and the second has used Zt \u2208 [0, 1].", "startOffset": 93, "endOffset": 99}, {"referenceID": 2, "context": "Moreover, this can be done in time O(d) as long as we can compute the three vectors { Xuj } j\u2208[3] to an additive \u03b5\u0303/poly(d, T ) error in Euclidean norm.", "startOffset": 94, "endOffset": 97}, {"referenceID": 29, "context": "1), so one can apply conjugate gradient [31] or Nesterov\u2019s accelerated gradient descent [24] to minimize this objective.", "startOffset": 40, "endOffset": 44}, {"referenceID": 22, "context": "1), so one can apply conjugate gradient [31] or Nesterov\u2019s accelerated gradient descent [24] to minimize this objective.", "startOffset": 88, "endOffset": 92}, {"referenceID": 7, "context": "Since \u2016\u2207fi(x)\u20162 \u2264 \u03b7k for each i, one can apply the SVRG method [8, 29] to minimize f(x) which gives running time \u00d5 ( nnz(\u03a3k\u22121) + (\u03b7k)2 maxi\u2208[k\u22121]{nnz(Ai)} ) .", "startOffset": 63, "endOffset": 70}, {"referenceID": 27, "context": "Since \u2016\u2207fi(x)\u20162 \u2264 \u03b7k for each i, one can apply the SVRG method [8, 29] to minimize f(x) which gives running time \u00d5 ( nnz(\u03a3k\u22121) + (\u03b7k)2 maxi\u2208[k\u22121]{nnz(Ai)} ) .", "startOffset": 63, "endOffset": 70}, {"referenceID": 13, "context": "Then, using the Catalyst/APPA acceleration scheme [14, 23], the above running time can be improved to \u00d5 ( nnz(\u03a3k\u22121) + \u221a \u03b7k \u00b7maxi\u2208[k\u22121]{nnz(\u03a3k\u22121)nnz(Ai)} ) .", "startOffset": 50, "endOffset": 58}, {"referenceID": 21, "context": "Then, using the Catalyst/APPA acceleration scheme [14, 23], the above running time can be improved to \u00d5 ( nnz(\u03a3k\u22121) + \u221a \u03b7k \u00b7maxi\u2208[k\u22121]{nnz(\u03a3k\u22121)nnz(Ai)} ) .", "startOffset": 50, "endOffset": 58}, {"referenceID": 14, "context": "This can be done via a \u201cbinary search\u201d procedure which was used widely for shift-and-invert based methods [15]:", "startOffset": 106, "endOffset": 110}, {"referenceID": 6, "context": "The authors of [7] have shown that the potential Tr(X 1\u22121/q k ) is robust against noise and a completely analogous (but lengthy) proof of theirs applies to this paper.", "startOffset": 15, "endOffset": 18}, {"referenceID": 14, "context": "(This requires O(1) iterations of power method applied to (cI\u2212 \u03b7\u03a3k\u22121)\u22121 [15].", "startOffset": 72, "endOffset": 76}, {"referenceID": 14, "context": "It is a simple exercise (with details given in [15]) to show that when the procedure ends, it satisfies 1 2e \u2264 c \u2212 \u03b7\u03a3k\u22121 \u2264 1e so c is a lower bound on ck.", "startOffset": 47, "endOffset": 51}, {"referenceID": 0, "context": "Above, \u00ac uses Tr(AT\u03a6T\u22121AT ) \u2264 Tr(AT\u03a6T\u22121) as well as wkw> k = \u03a6k\u22121/Tr(\u03a6k\u22121), \u00ad uses 1+x \u2264 ex, \u00ae uses 1+2x \u2265 e2x\u22122x for x \u2208 [0, 1], \u00ad uses E[\u03bd> 1 \u03a60\u03bd1] = 1, \u00b0 uses the Lieb-Thirring inequality Tr(ABAB) \u2264 Tr(A2B2),22 \u00b1 uses (I + \u03b7A1) I + (4\u03b7 + 11\u03b7)A1.", "startOffset": 122, "endOffset": 128}, {"referenceID": 5, "context": "We prove this lower bound by reducing the problem to an information-theoretic lower bound that has appeared in our separate paper [6].", "startOffset": 130, "endOffset": 133}, {"referenceID": 5, "context": "The lower bound in [6] states that, for every 1 \u2265 \u03bb \u2265 \u03bb2 \u2265 0, there exists a PSD matrix B with the largest two eigenvalues being \u03bb and \u03bb2, and a distribution D of rank-1 matrices with spectral norm at most 1 and expectation equal to D.", "startOffset": 19, "endOffset": 22}], "year": 2017, "abstractText": "Matrix multiplicative weight update (MMWU) [26] is an extremely powerful algorithmic tool for computer science and related fields. However, it comes with a slow running time due to the matrix exponential and eigendecomposition computations. For this reason, many researchers studied the followed-the-perturbed-leader (FTPL) framework which is faster, but a factor \u221a d worse than the optimal regret of MMWU for dimension-d matrices. In this paper, we propose a followed-the-compressed-leader framework which, not only matches the optimal regret of MMWU (up to polylog factors), but runs even faster than FTPL. Our main idea is to \u201ccompress\u201d the matrix exponential computation to dimension 3 in the adversarial setting, or dimension 1 in the stochastic setting. This result resolves an open question regarding how to obtain both (nearly) optimal and efficient algorithms for the online eigenvector problem [16]. ar X iv :1 70 1. 01 72 2v 1 [ cs .L G ] 6 J an 2 01 7", "creator": "LaTeX with hyperref package"}}}