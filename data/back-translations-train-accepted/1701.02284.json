{"id": "1701.02284", "review": {"conference": "iclr", "VERSION": "v1", "DATE_OF_SUBMISSION": "9-Jan-2017", "title": "DeepDSL: A Compilation-based Domain-Specific Language for Deep Learning", "abstract": "In recent years, Deep Learning (DL) has found great success in domains such as multimedia understanding. However, the complex nature of multimedia data makes it difficult to develop DL-based software. The state-of-the art tools, such as Caffe, TensorFlow, Torch7, and CNTK, while are successful in their applicable domains, are programming libraries with fixed user interface, internal representation, and execution environment. This makes it difficult to implement portable and customized DL applications.", "histories": [["v1", "Mon, 9 Jan 2017 18:02:13 GMT  (80kb,D)", "http://arxiv.org/abs/1701.02284v1", null]], "reviews": [], "SUBJECTS": "cs.PL cs.LG", "authors": ["tian zhao", "xiaobing huang", "yu cao"], "accepted": true, "id": "1701.02284"}, "pdf": {"name": "1701.02284.pdf", "metadata": {"source": "CRF", "title": "DEEPDSL: A COMPILATION-BASED DOMAIN- SPECIFIC LANGUAGE FOR DEEP LEARNING", "authors": ["Tian Zhao", "Yu Cao"], "emails": ["tzhao@uwm.edu", "xiaobing@uwm.edu", "ycao@cs.uml.edu"], "sections": [{"heading": "1 INTRODUCTION", "text": "This year, it is as far as ever in the history of the city, where it is as far as never before."}, {"heading": "2 OVERVIEW", "text": "DeepDSL directly encodes the mathematical representation of DL networks, with each layer represented as a tensor function. the entire network is then symbolically derived as a composition of these1http: / / www.jcuda.org 2http: / / www.eclipse.orgfunctions. deepDSL derives the partial derivatives of tensor functions in relation to tensor variables, so that the backward gradients of the network parameters are automatically generated. In turn, the IR expressions are passed on through a series of simplifications and optimizations in the second stage. A DeepDSL program is compiled in several steps."}, {"heading": "3 SYNTAX", "text": "In this context, it should also be mentioned that the two candidates are two candidates who have decided to stand. (...) It is about a candidate who has decided to stand. (...) It is about a candidate who has decided to stand. (...) It is about a candidate who has decided to stand. (...) It is about a candidate who has decided to stand. (...) It is about a candidate who has decided to stand. (...) It is about a candidate. \"(...) It is about a candidate.\" (...) It is about a candidate. \"(...) It is about a candidate.\" (...) It is about a candidate. \"(...) It is about a candidate.\" (...) It is about a candidate. \"(...) It is about a candidate.\" (...) It is about a candidate. \"(...) It is about a candidate.\" (...) It is about a candidate. \"(...) It is about a candidate.\" It is about a candidate. \"(...) It is about a candidate.\" It is about a candidate. \"(...) It is about a candidate.\" It is about a candidate. \"(...) It is about a candidate.\" It is about a candidate. \"(...) It is about a candidate.\" (...) It is about a candidate. \"It is about a candidate.\" (...) It is about a candidate. \"(...) It is about a candidate.\" It is about a candidate. \"(...\" It is about a candidate. \"(...) It is about a candidate.\" (...) It is about a candidate. \"It is about a candidate.\" (... \""}, {"heading": "4 INTERMEDIATE REPRESENTATION", "text": "In fact, most of them are able to feed themselves, and if they are not able to feed themselves, they are able to feed themselves."}, {"heading": "5 COMPILATION", "text": "A DeepDSL program compiles to a Java source program that uses a small Java library to invoke CUDA and CuDNN via a JNI wrapper. The compiled Java code does not depend on a DeepDSL compiler or Scala, making it more portable and easier to integrate it into other applications. Most of the current tools use platform-dependent programming languages such as C, Python, and Lua, which are compiled to specific binaries for each installation. Since our compiled program is Java, it runs directly on all platforms that support JVM. Compiling Java is trivial for most compiling platforms such as the Java source generated by DeepDSL, which can be run on a Java laptop, without any modification. While it takes effort to generate tools such as Tensorflow, Caffe, or Torrence to install the Java source code on different Java query architectures, it is required to execute the Java code."}, {"heading": "6 PERFORMANCE", "text": "The primary compilation goal of DeepDSL is the Java program that runs on Nvidia GPU through its CUDA / CuDNN library3. DeepDSL can test well-known networks such as Alexnet, Overfeat, GoogleNet, Vgg, and Deep Residual Networks (Resnet) DSDSB tests. In this section we evaluate the performance of DeepDSL with Caffe and Tensorflow with these networks. To be consistent, DeepDSL, Caffe, and Tensorflow tests all follow the same Caffe prototxt definitions. Specifically, for Alexsnet and GoogleNet, we followed the prototxt from Caffe's Website4; for Vgg-16, we followed Prototxt from this link5; for Overfeat, we followed Prototxt from IntelLabs6; and for Deep Residual Network (ResNet-50), we followed the prototxt from the author's Website7."}, {"heading": "7 RELATED WORK", "text": "In fact, it is such that most of them will be able to move to another world, in which they are able to move to another world, in which they are able to move to another world, in which they are able to move, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they, in which they, in which they live, in which they, in which they, in which they live, in which they, in which they, in which they live, in which they, in which they live."}, {"heading": "8 CONCLUSION", "text": "The compiled DeepDSL programs are very easy to use and extend, as their primary dependencies are just JCuda and CUDA libraries. DeepDSL programs are also efficient, and their runtime performance and memory usage are significantly better than caffe and tensor flow in some DL networks. DeepDSL performs static analysis to detect errors early, and provides readable intermediate representations and analyses of memory usage. DeepDSL allows compact encoding of complex networks, and because it is based on a highly typed language scale, writing DeepDSL programs is less error-prone than dynamic languages like Python. While the compiled DeepDSL programs are efficient, DeepDSL itself is not optimized. Although compiling simpler networks like Alexnet takes a few seconds, compiling complex networks like ResNet can take a few minutes. As the compiled DeepDSL programs are efficient, DeepDSL programs are not optimizing memory usage itself, while we plan to deploy DSL efficiently."}], "references": [{"title": "TensorFlow: LargeScale Machine Learning on Heterogeneous Distributed Systems", "author": ["gas", "O. Vinyals", "P. Warden", "M. Wattenberg", "M. Wicke", "Y. Yu", "X. Zheng"], "venue": null, "citeRegEx": "gas et al\\.,? \\Q2016\\E", "shortCiteRegEx": "gas et al\\.", "year": 2016}, {"title": "An introduction to computational networks and the computational network toolkit", "author": ["colm Slaney", "Andreas Stolcke", "Yongqiang Wang", "Huaming Wang", "Kaisheng Yao", "Dong Yu", "Yu Zhang", "Geoffrey Zweig"], "venue": "Technical Report MSR-TR-2014-112,", "citeRegEx": "Slaney et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Slaney et al\\.", "year": 2014}, {"title": "Theano: a CPU and GPU math expression compiler", "author": ["James Bergstra", "Olivier Breuleux", "Fr\u00e9d\u00e9ric Bastien", "Pascal Lamblin", "Razvan Pascanu", "Guillaume Desjardins", "Joseph Turian", "David Warde-Farley", "Yoshua Bengio"], "venue": "In Proceedings of the Python for Scientific Computing Conference (SciPy),", "citeRegEx": "Bergstra et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Bergstra et al\\.", "year": 2010}, {"title": "Guest editorial multimedia: The biggest big data", "author": ["Shu-Ching Chen", "Ramesh Jain", "Yonghong Tian", "Haohong Wang"], "venue": "Multimedia, IEEE Transactions on,", "citeRegEx": "Chen et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Chen et al\\.", "year": 2015}, {"title": "Mxnet: A flexible and efficient machine learning library for heterogeneous distributed systems", "author": ["Tianqi Chen", "Mu Li", "Yutian Li", "Min Lin", "Naiyan Wang", "Minjie Wang", "Tianjun Xiao", "Bing Xu", "Chiyuan Zhang", "Zheng Zhang"], "venue": "Neural Information Processing Systems, Workshop on Machine Learning Systems,", "citeRegEx": "Chen et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Chen et al\\.", "year": 2015}, {"title": "Torch7: A matlab-like environment for machine learning", "author": ["R. Collobert", "K. Kavukcuoglu", "C. Farabet"], "venue": "In BigLearn, NIPS Workshop,", "citeRegEx": "Collobert et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Collobert et al\\.", "year": 2011}, {"title": "Caffe: Convolutional architecture for fast feature embedding", "author": ["Yangqing Jia", "Evan Shelhamer", "Jeff Donahue", "Sergey Karayev", "Jonathan Long", "Ross Girshick", "Sergio Guadarrama", "Trevor Darrell"], "venue": "arXiv preprint arXiv:1408.5093,", "citeRegEx": "Jia et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Jia et al\\.", "year": 2014}, {"title": "Gradient-based learning applied to document recognition", "author": ["Yann LeCun", "L\u00e9on Bottou", "Yoshua Bengio", "Patrick Haffner"], "venue": "Proceedings of the IEEE,", "citeRegEx": "LeCun et al\\.,? \\Q1998\\E", "shortCiteRegEx": "LeCun et al\\.", "year": 1998}, {"title": "Chainer: a next-generation open source framework for deep learning", "author": ["Seiya Tokui", "Kenta Oono", "Shohei Hido", "Justin Clayton"], "venue": "In Proceedings of Workshop on Machine Learning Systems (LearningSys) in The Twenty-ninth Annual Conference on Neural Information Processing Systems (NIPS),", "citeRegEx": "Tokui et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Tokui et al\\.", "year": 2015}, {"title": "The numpy array: a structure for efficient numerical computation", "author": ["St\u00e9fan van der Walt", "S. Chris Colbert", "Ga\u00ebl Varoquaux"], "venue": "CoRR, abs/1102.1523,", "citeRegEx": "Walt et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Walt et al\\.", "year": 2011}], "referenceMentions": [{"referenceID": 3, "context": "Multimedia is increasingly becoming the \u201dbiggest big data\u201d as the most important and valuable source for insights and information Chen et al. (2015a). Recently, a new set of machine learning algorithms named \u201dDeep Learning\u201d (DL) LeCun et al.", "startOffset": 130, "endOffset": 150}, {"referenceID": 3, "context": "Multimedia is increasingly becoming the \u201dbiggest big data\u201d as the most important and valuable source for insights and information Chen et al. (2015a). Recently, a new set of machine learning algorithms named \u201dDeep Learning\u201d (DL) LeCun et al. (2015), which aims at learning multiple levels of representation and abstraction that help infer knowledge from multimedia data (e.", "startOffset": 130, "endOffset": 249}, {"referenceID": 2, "context": "However, current tools, such as Theano Bergstra et al. (2010), Torch7 Collobert et al.", "startOffset": 39, "endOffset": 62}, {"referenceID": 2, "context": "However, current tools, such as Theano Bergstra et al. (2010), Torch7 Collobert et al. (2011), Caffe Jia et al.", "startOffset": 39, "endOffset": 94}, {"referenceID": 2, "context": "However, current tools, such as Theano Bergstra et al. (2010), Torch7 Collobert et al. (2011), Caffe Jia et al. (2014), Computational Network Toolkit (CNTK) Agarwal et al.", "startOffset": 39, "endOffset": 119}, {"referenceID": 2, "context": "However, current tools, such as Theano Bergstra et al. (2010), Torch7 Collobert et al. (2011), Caffe Jia et al. (2014), Computational Network Toolkit (CNTK) Agarwal et al. (2014), and TensorFlow Abadi et al.", "startOffset": 39, "endOffset": 179}, {"referenceID": 2, "context": "However, current tools, such as Theano Bergstra et al. (2010), Torch7 Collobert et al. (2011), Caffe Jia et al. (2014), Computational Network Toolkit (CNTK) Agarwal et al. (2014), and TensorFlow Abadi et al. (2016), while are efficient in their applicable domains, are essentially application libraries with some inherent limitations.", "startOffset": 39, "endOffset": 215}, {"referenceID": 7, "context": "Figure 2 shows the complete implementation for compiling a program to train and test Lenet LeCun et al. (1998). Since DeepDSL is embedded in Scala, the program is in Scala syntax and it can be compiled and run with a programming tool such as eclipse.", "startOffset": 91, "endOffset": 111}, {"referenceID": 6, "context": "In this section, we review some popular tools: Torch7, Theano, Caffe, TensorFlow, and CNTK, and newer ones such as Chainer Tokui et al. (2015) and MXNet Chen et al.", "startOffset": 123, "endOffset": 143}, {"referenceID": 3, "context": "(2015) and MXNet Chen et al. (2015b).", "startOffset": 17, "endOffset": 37}, {"referenceID": 4, "context": "Torch7 Collobert et al. (2011) uses Lua language for integration with C program and achieves Clike performance.", "startOffset": 7, "endOffset": 31}, {"referenceID": 2, "context": "Theano Bergstra et al. (2010), hosted in Python, allows users to define symbolic variables and functions (using NumPy van der Walt et al.", "startOffset": 7, "endOffset": 30}, {"referenceID": 2, "context": "Theano Bergstra et al. (2010), hosted in Python, allows users to define symbolic variables and functions (using NumPy van der Walt et al. (2011)) to encode DL networks and compiles the symbolic expressions to C.", "startOffset": 7, "endOffset": 145}, {"referenceID": 2, "context": "Theano Bergstra et al. (2010), hosted in Python, allows users to define symbolic variables and functions (using NumPy van der Walt et al. (2011)) to encode DL networks and compiles the symbolic expressions to C. Theano performs optimization such as normalizing mathematical expressions, numerical stabilization, and code specialization during compilation and the target code can run on CPU or GPU devices. Caffe Jia et al. (2014) constructs a graph for DL network by connecting the layers with the 4D arrays that store tensors.", "startOffset": 7, "endOffset": 430}, {"referenceID": 2, "context": "Theano Bergstra et al. (2010), hosted in Python, allows users to define symbolic variables and functions (using NumPy van der Walt et al. (2011)) to encode DL networks and compiles the symbolic expressions to C. Theano performs optimization such as normalizing mathematical expressions, numerical stabilization, and code specialization during compilation and the target code can run on CPU or GPU devices. Caffe Jia et al. (2014) constructs a graph for DL network by connecting the layers with the 4D arrays that store tensors. Caffe separates its DL network model representation (using ProtocolBuffers Google) from the actual model parameter calculations. With its layered structure, Caffe computes the memory needed for each layer and reserves memory accordingly. TensorFlow Abadi et al. (2016) shares largely common design paradigms as that of Caffe.", "startOffset": 7, "endOffset": 797}, {"referenceID": 6, "context": "Comparing to the \u201cdefine-and-run\u201d paradigm (adopted by Torch7, Theano, and Caffe), Chainer Tokui et al. (2015) follows a \u201cdefine-by-run\u201d pattern, which essentially allows modifying the control flow during the execution of a computational graph.", "startOffset": 91, "endOffset": 111}, {"referenceID": 3, "context": "MXNet Chen et al. (2015b) provides both declarative and imperative programming styles and multiple language supports by embedding into multiple host languages and unifying the execution with one backend engine.", "startOffset": 6, "endOffset": 26}], "year": 2017, "abstractText": "In recent years, Deep Learning (DL) has found great success in domains such as multimedia understanding. However, the complex nature of multimedia data makes it difficult to develop DL-based software. The state-of-the-art tools, such as Caffe, TensorFlow, Torch7, and CNTK, while are successful in their applicable domains, are programming libraries with fixed user interface, internal representation, and execution environment. This makes it difficult to implement portable and customized DL applications. In this paper, we present DeepDSL, a domain specific language (DSL) embedded in Scala, that compiles deep networks written in DeepDSL to Java source code. Deep DSL provides (1) intuitive constructs to support compact encoding of deep networks; (2) symbolic gradient derivation of the networks; (3) static analysis for memory consumption and error detection; and (4) DSL-level optimization to improve memory and runtime efficiency. DeepDSL programs are compiled into compact, efficient, customizable, and portable Java source code, which operates the CUDA and CUDNN interfaces running on Nvidia GPU via a Java Native Interface (JNI) library. We evaluated DeepDSL with a number of popular DL networks. Our experiments show that the compiled programs have very competitive runtime performance and memory efficiency compared to the existing libraries.", "creator": "LaTeX with hyperref package"}}}