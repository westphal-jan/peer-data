{"id": "1701.02789", "review": {"conference": "ICML", "VERSION": "v1", "DATE_OF_SUBMISSION": "10-Jan-2017", "title": "Identifying Best Interventions through Online Importance Sampling", "abstract": "Motivated by applications in computational advertising and systems biology, we consider the problem of identifying the best out of several possible soft interventions at a source node $V$ in a causal DAG, to maximize the expected value of a target node $Y$ (downstream of $V$). There is a fixed total budget for sampling under various interventions. Also, there are cost constraints on different types of interventions. We pose this as a best arm identification problem with $K$ arms, where each arm is a soft intervention at $V$. The key difference from the classical setting is that there is information leakage among the arms. Each soft intervention is a distinct known conditional probability distribution between $V$ and its parents $pa(V)$.", "histories": [["v1", "Tue, 10 Jan 2017 21:26:03 GMT  (5604kb,D)", "https://arxiv.org/abs/1701.02789v1", "33 pages, 7 figures"], ["v2", "Wed, 8 Mar 2017 01:19:33 GMT  (3267kb,D)", "http://arxiv.org/abs/1701.02789v2", "30 pages, 11 figures"], ["v3", "Thu, 9 Mar 2017 22:50:38 GMT  (3277kb,D)", "http://arxiv.org/abs/1701.02789v3", "30 pages, 11 figures"]], "COMMENTS": "33 pages, 7 figures", "reviews": [], "SUBJECTS": "stat.ML cs.IT cs.LG math.IT", "authors": ["rajat sen", "karthikeyan shanmugam", "alexandros g dimakis", "sanjay shakkottai"], "accepted": true, "id": "1701.02789"}, "pdf": {"name": "1701.02789.pdf", "metadata": {"source": "CRF", "title": "Identifying Best Interventions through Online Importance Sampling", "authors": ["Rajat Sen", "Karthikeyan Shanmugam", "Alexandros G. Dimakis", "Sanjay Shakkottai", "Thomas J. Watson"], "emails": [], "sections": [{"heading": "1 Introduction", "text": "In fact, it is that we are able to assert ourselves, that we are able to assert ourselves in the world, and that we are able to assert ourselves in the world, that we are able to stay in the world, \"he said."}, {"heading": "1.1 Main Contributions", "text": "We have the possibility that there may be a tightening-up of the rules which are based on the principles of the EU. (...) We have the possibility to change the rules of the EU. \"(S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S."}, {"heading": "1.2 Related Work", "text": "The problem lies at the intersection of causal conclusions and the identification of the best arm in bandits. There have been many studies on the classical identification of the best arm in bandit literature, both in the fixed trust system [19, 13] and within the fixed budget [3, 10, 17, 8]. It was recently shown in [8] that the results of [3] are optimal; the crucial difference to our work is that there is no information leak among the armies in these models. [27, 16, 12, 15, 35, 29, 24, 33] there has been a lot of work on learning casual models from data and / or experiments and using them to estimate causal force issues of a counterfactual nature. One remarkable work that partly inspired our work is [7] where the causal graphics underlying a computer-based advertising system (as in Bing, Google, etc.) is known and the primary interest is to find out how some of the system would change."}, {"heading": "2 Problem Setting", "text": "In fact, it is as if most of us are able to outdo ourselves. (...) In fact, it is as if most of us are able to outdo ourselves. (...) It is not as if they are able to outdo themselves. (...) It is not as if they are able to outdo themselves. (...) It is as if they are able to outdo themselves. (...) It is not as if they are able to outdo themselves. (...) It is as if they are able to outdo themselves. (...) It is as if they are able to outdo themselves. (...) It is as if they are able to outdo themselves. (...) It is as if they are able to outdo themselves. (...) It is as if they are able to outdo themselves."}, {"heading": "3 Our Main Results", "text": "In this section, we present our most important theoretical contributions. In Section 3.2, we provide a successive reject algorithm that exploits the information leak between the arms by means of importance tests. Subsequently, we provide theoretical guarantees regarding the probability of misidentification (e (T, B)) and simple regret (r (T, B) for our algorithm in Section 3.3. To formally explain our algorithm and our results, we first describe some key ideas in our algorithm and introduce important definitions in Section 3.1."}, {"heading": "3.1 Definitions", "text": "In fact, most people are able to decide for themselves what they want and what they want."}, {"heading": "3.2 Algorithm", "text": "We will now describe our most important algorithmic contribution - algorithm 1 and 2. Algorithm 1 starts with all the K-arms taken into account and then proceeds in phases that have one or more arms at the end of each phase. A random arm among those that survive at the end of all phases is declared optimal. We will now describe the duration of different phases. Recall the parameters T - Total sample budget available and B - average cost constraint on the end of all phases. Let n (T) = dlog 10. \"Let log (n) = 1 (1 / i).We will number an algorithm with n (T) phases."}, {"heading": "3.3 Theoretical Guarantees", "text": "We give our main results as theorem 1 and theorem 2, which provide guarantees for the probability of errors and simple regret. (Our results can be interpreted as a natural generalization of the results in [3], if there is an information leak among the arms. This is the first gap that depends on the characterization. (Theorem 1. (Formally proven as theorem 5) Let us leave the results in [3] as an effective standard deviation in definition 3. The probability of an error for algorithm 1 fulfills: e (T, B) \u2264 2K2 log2 log2 (20 / \u04452) exp (\u2212 T2H) exp (n (T))))) (5), if the budget for the total number of samples is T and vice versa. The probability of an error in algorithm 1 satisfies: e (T, B) \u2264 2K2 log2 log2 log2 log2 (theorem) exp (\u2212 theorem) exp (total tlog \u2212 Tp) (T), if the sample (T) (n) (T) is the number of samples."}, {"heading": "4 Empirical Validation", "text": "We empirically validate the performance of our algorithms in two real data settings. In Section 4.1 we examine the empirical performance of our algorithm on the data set of flow cytometry [32]. In Section 4.2 we apply our algorithms for the purpose of model interpretation of the Inception Deep Network [38] in the context of image classification. Section 4.3 is devoted to synthetic experiments. In Section D (in the appendix) we provide more details on our experiments. In the appendix we show empirically that our divergence metrics are fundamental and their replacement with other divergences is suboptimal."}, {"heading": "4.1 Flow Cytometry Data-Set", "text": "In fact, in the USA, in the USA, in the USA, in Europe, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA,"}, {"heading": "4.2 Interpretability of Inception Deep Network", "text": "In this case, it is a mere diversion, a kind of diversion, a diversionary manoeuvre."}, {"heading": "4.3 Synthetic Experiments", "text": "In this section, we empirically confirm the performance of our algorithm through synthetic experiments. We carefully design our simulation setting, which is simple but at the same time sufficient to capture the various trade-offs involved in the problem. An important point is therefore that our algorithm is not aware of the actual effects of the changes on the target (gaps between expectations), but it only knows the divergence between the different experiments that we perform. Sometimes, a change with large deviations from an existing may not maximize the effect that we are looking for. Conversely, smaller divergences can sometimes lead closer to the optimum. We show that our algorithm works well in all experiments, as compared to previous work [3, 22]. We continue our experiments according to the simple causal graph in Figure 5. V is assumed that random variable values in {0, 1, 2, m \u2212 1}."}, {"heading": "5 Discussion and Future Work", "text": "In fact, it is so that most of us are able to abide by the rules that they have imposed on themselves. (...) In fact, it is so that they are able to abide by the rules. (...) In fact, it is so that they are able to outdo themselves. (...) In fact, it is so that they are able to outdo themselves. (...) \"(...)\" (...) \"(...)\" (...) \"(...)\" (...) \"(...)\" (...) \"(...) (()\" (() \"(...\") () (() (() () () () () () () () () () () () () () () () () () () () () () ()) () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () ()) () () () () () () () () () () () () () () () () () () () () () () () () ()) () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () (() () () () () (() () () (() () () () () (() ((() () () () (((() () (() () () (() (() () (() () () (() () () ((() (() ((() (() ((() () () ((() (() (() () (() (() ("}, {"heading": "B.1 Comparison with [3]", "text": "Let us assume that the error rate in the search for the optimal arm is: e (T) \u2264 O (K2 exp (K) H))) (8) An intuitive interpretation for H (3) is that it is the maximum number of samples (neglecting some protocol factors) that is required to conclude that the arm is suboptimal among the weapons that are closer to the optimal than itself. Intuitively, this is the case when there are no information leaks needed to distinguish between the optimal arm and the optimal arm."}, {"heading": "B.2 Comparison with [22]", "text": "In [22] the algorithm is based on truncated importance tests where the Clipper is always set to a static level of O (\u221a T) (without logfactors).The simple repentance guarantee in [22] scales as O (\u221a (\u03b7) / T) log T), where (\u03b7) is a global hardness parameter. The guarantees do not adapt to the problem parameters, in particular the gaps {\u2206 k} k [K].On the contrary, we provide problem-dependent limits that distinguish the arms according to their gap from the optimal arm and its effective standard deviation parameter.The terms H-k can be interpreted as hardness parameters for the rejection of arm k. Note that H-k only depends on the arms that are at least as bad as their gap from the optimal arm in terms of their gap. In addition, the guarantees are adjusted to our general budget constraints that are missing in [22].It is evident that our problem cannot be scaled on the T scale (T) as an exception (T) scale (T)."}, {"heading": "C Proofs", "text": "In this section, we present the theoretical analysis of our algorithm. Before proceeding to the proof of our main theories, we deduce some key dilemmas that are useful for analyzing truncated valuers."}, {"heading": "C.1 Clipped Importance Sampling Estimator", "text": "In Section 3.1, we introduced the concept of importance, which helps us to use the data collected under one arm to estimate the resources of other arms. As explained in Section 3.1, a naive, unbiased estimator can have potentially unlimited variances, leading to poor confidence intervals. It has been observed in the context of the importance forecast that the estimator's performance in (2) a carefully chosen value can provide better guarantees of confidence, even if the resulting estimator is slightly distorted [7]. Before introducing the precise estimator, let us define a key quantity that is useful for analysis.Definition 4: We define the term i, j () as follows:"}, {"heading": "C.3 Aggregating Heterogenous Clipped Estimators", "text": "In Section C1, we have seen how samples from one of the candidate distributions can be used to estimate the target mean value under another arm. Therefore, it is possible to obtain information about the target value under the kth arm (Ek [Y]) from the samples of all other arms. In this section, we will design such an estimator based on the insights gained in Section C.1.Recall, which will be the quantities Mkj = 1 + log (Pk [Pk] Pj = 1) Pk [K] Pk [K] Pk [K] Pk [K] s [K]. These quantities will be the key tools in designing the estimators in this section. We will obtain the total number of samples from arm i [K]."}, {"heading": "C.4 Allocation of Samples", "text": "In Section C.3, Theorem 4 states that the payer's guarantees of confidence depend on how the samples are distributed among the individual arms. Specifically, the term (Zk / \u03c4) in Equation (26) influences the payer's performance for \u00b5k (Y-k). We want to maximize (Zk-K) for all arms k-K. Let the total budget be the same. Let R be the number of arms that remain in competition for the best optimal arm. Let's look at the matrix A-RK-K such that Akj = 1 / Mkj for all k, j-K [K]. Then we decide how often arm k is pulled, i.e., how each time arm k is maximized to algorithm 3.Lemma 4. Allocation in algorithm 3 ensures that (Zk / Kj for all k, j-K)."}, {"heading": "C.5 Putting it together: Online Analysis", "text": "We analyze algorithm 1 phase by phase. \u2212 K: We define different quantities to be used in the analysis of the algorithm. \u2212 K: We analyze algorithm 1 phase by phase. \u2212 K: We define different quantities to be used in the analysis of the algorithm. \u2212 K: We analyze algorithm 1 phase by phase. \u2212 K: We define different quantities by phase. \u2212 K: The value of the estimator (in algorithm 1): The value of the estimator (in algorithm 1) for arms at the end of the phases. \u2212 K: The value of the highest estimation maxc Y: K (in algorithm 1). \u2022 A (l): The value of the estimation maxc Y (in algorithm 1)."}, {"heading": "2. The error probability is bounded as:", "text": "e (T, B) \u2264 2K2 log2 (20 / \u0445) exp (\u2212 T2H) k (< k) log (n (T)). The probability of error is only given when \"k\". k) exp (\u2212 k (\u2212 k) exp (\u2212 k). Proof. We should remember that the simple regret is defined as: \"k.\" (T, B) = \"k.\" (D). (D): \"k.\" (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). G (D). (D). (D). (D). (D). (D"}, {"heading": "D More Experiments", "text": "In this section we present our experiments in more detail."}, {"heading": "D.1 More on Flow Cytometry Experiments", "text": "In this section we will give further details on the flow cytometry experiments. As described in the main paper, we will use the causal diagram in Fig. 5 (c) in [26] (shown in Fig. 3a) as a basic truth. Then, we will fit a GLM gamma model [14] between each node in the diagram and its parents using the observational data set. The GLM model provides a highly accurate representation of the flow cytometry data. In Fig. 11a, we will draw the histogram for the activation of an internal node pip2 from the real data and samples generated from the GLM probability model. You can see that the histograms are very close to each other. In Fig. we will plot the performance of SRISv2 when the divergence metric is replaced by the KL divergence. In one of the diagrams, SRISv2 is modified by setting Mij = 1 + KL (Pj)."}, {"heading": "D.2 More on Interpretation of Inception Deep Network", "text": "In this section, we describe the methodology of our model interpretation technique in detail. In Section 4.2, we have described how the best arm algorithm can be used to select a distribution over the superpixels of an image that has the maximum probability of generating a certain classification of inception. Here, we describe how the distributions over the superpixels are generated and how they are subsequently used. Arm distributions are essentially points in the n-dimensional simplex (where n is the number of superpixels into which the image is segmented). These distributions are generated in a randomized manner using the following methods: 1. Generate a point uniformly in the n-dimensional simplex.2. Randomly select l < n superpixels. Make the distribution evenly over them and 0 elsewhere.3. Choose l < n superpixels. The probability distribution is a uniformly selected random point over the selected simplex-dimensional and 0 elsewhere.3."}], "references": [{"title": "Low-cost learning via active data procurement", "author": ["Jacob Abernethy", "Yiling Chen", "Chien-Ju Ho", "Bo Waggoner"], "venue": "In Proceedings of the Sixteenth ACM Conference on Economics and Computation,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2015}, {"title": "Best arm identification in multi-armed bandits", "author": ["Jean-Yves Audibert", "S\u00e9bastien Bubeck"], "venue": "In COLT- 23th Conference on Learning Theory-2010, pages 13\u2013p,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2010}, {"title": "Probability inequalities for the sum of independent random variables", "author": ["George Bennett"], "venue": "Journal of the American Statistical Association,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 1962}, {"title": "Causal models in the social sciences", "author": ["Hubert M Blalock"], "venue": "Transaction Publishers,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 1985}, {"title": "A predictive model for transcriptional control of physiology in a free living", "author": ["Richard Bonneau", "Marc T Facciotti", "David J Reiss", "Amy K Schmid", "Min Pan", "Amardeep Kaur", "Vesteinn Thorsson", "Paul Shannon", "Michael H Johnson", "J Christopher Bare"], "venue": "cell. Cell,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2007}, {"title": "Counterfactual reasoning and learning systems: the example of computational advertising", "author": ["L\u00e9on Bottou", "Jonas Peters", "Joaquin Quinonero Candela", "Denis Xavier Charles", "Max Chickering", "Elon Portugaly", "Dipankar Ray", "Patrice Y Simard", "Ed Snelson"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2013}, {"title": "Tight (lower) bounds for the fixed budget best arm identification bandit problem", "author": ["Alexandra Carpentier", "Andrea Locatelli"], "venue": "arXiv preprint arXiv:1605.09004,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2016}, {"title": "On the optimal sample complexity for best arm identification", "author": ["Lijie Chen", "Jian Li"], "venue": "arXiv preprint arXiv:1511.03774,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2015}, {"title": "Reconstructing causal biological networks through active learning", "author": ["Hyunghoon Cho", "Bonnie Berger", "Jian Peng"], "venue": "PloS one,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2016}, {"title": "Almost optimal intervention sets for causal discovery", "author": ["Frederick Eberhardt"], "venue": "In Proceedings of 24th Conference in Uncertainty in Artificial Intelligence (UAI),", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2008}, {"title": "Best arm identification: A unified approach to fixed budget and fixed confidence", "author": ["Victor Gabillon", "Mohammad Ghavamzadeh", "Alessandro Lazaric"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2012}, {"title": "Generalized linear models and extensions", "author": ["James William Hardin", "Joseph M Hilbe", "Joseph Hilbe"], "venue": "Stata press,", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2007}, {"title": "Two optimal strategies for active learning of causal networks from interventional data", "author": ["Alain Hauser", "Peter B\u00fchlmann"], "venue": "In Proceedings of Sixth European Workshop on Probabilistic Graphical Models,", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2012}, {"title": "Experiment selection for causal discovery", "author": ["Antti Hyttinen", "Frederick Eberhardt", "Patrik Hoyer"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2013}, {"title": "lil\u2019ucb: An optimal exploration algorithm for multi-armed bandits", "author": ["Kevin G Jamieson", "Matthew Malloy", "Robert D Nowak", "S\u00e9bastien Bubeck"], "venue": "In COLT,", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2014}, {"title": "Causal diagrams in systems epidemiology", "author": ["Michael Joffe", "Manoj Gambhir", "Marc Chadeau-Hyam", "Paolo Vineis"], "venue": "Emerging themes in epidemiology,", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2012}, {"title": "On the complexity of best arm identification in multi-armed bandit models", "author": ["Emilie Kaufmann", "Olivier Capp\u00e9", "Aur\u00e9lien Garivier"], "venue": "The Journal of Machine Learning Research,", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2015}, {"title": "Large-scale genetic perturbations reveal regulatory networks and an abundance of gene-specific repressors", "author": ["Patrick Kemmeren", "Katrin Sameith", "Loes AL van de Pasch", "Joris J Benschop", "Tineke L Lenstra", "Thanasis Margaritis", "Eoghan O?Duibhir", "Eva Apweiler", "Sake van Wageningen", "Cheuk W Ko"], "venue": null, "citeRegEx": "20", "shortCiteRegEx": "20", "year": 2014}, {"title": "Gene regulatory networks in plants: learning causality from time and perturbation", "author": ["Gabriel Krouk", "Jesse Lingeman", "Amy Marshall Colon", "Gloria Coruzzi", "Dennis Shasha"], "venue": "Genome biology,", "citeRegEx": "21", "shortCiteRegEx": "21", "year": 2013}, {"title": "Causal bandits: Learning good interventions via causal inference", "author": ["Finnian Lattimore", "Tor Lattimore", "Mark D Reid"], "venue": "In Advances In Neural Information Processing Systems,", "citeRegEx": "22", "shortCiteRegEx": "22", "year": 2016}, {"title": "Unbiased offline evaluation of contextualbandit-based news article recommendation algorithms", "author": ["Lihong Li", "Wei Chu", "John Langford", "Xuanhui Wang"], "venue": "In Proceedings of the fourth ACM international conference on Web search and data mining,", "citeRegEx": "23", "shortCiteRegEx": "23", "year": 2011}, {"title": "High-dimensional learning of linear causal networks via inverse covariance estimation", "author": ["Po-Ling Loh", "Peter B\u00fchlmann"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "24", "shortCiteRegEx": "24", "year": 2014}, {"title": "Methods for causal inference from gene perturbation experiments and validation", "author": ["Nicolai Meinshausen", "Alain Hauser", "Joris M Mooij", "Jonas Peters", "Philip Versteeg", "Peter B\u00fchlmann"], "venue": "Proceedings of the National Academy of Sciences,", "citeRegEx": "25", "shortCiteRegEx": "25", "year": 2016}, {"title": "Cyclic causal discovery from continuous equilibrium data", "author": ["Joris Mooij", "Tom Heskes"], "venue": "arXiv preprint arXiv:1309.6849,", "citeRegEx": "26", "shortCiteRegEx": "26", "year": 2013}, {"title": "Distinguishing cause from effect using observational data: methods and benchmarks", "author": ["Joris M Mooij", "Jonas Peters", "Dominik Janzing", "Jakob Zscheischler", "Bernhard Sch\u00f6lkopf"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "27", "shortCiteRegEx": "27", "year": 2016}, {"title": "Causality: Models, Reasoning and Inference", "author": ["Judea Pearl"], "venue": null, "citeRegEx": "29", "shortCiteRegEx": "29", "year": 2009}, {"title": "Kullback-leibler divergence estimation of continuous distributions", "author": ["Fernando P\u00e9rez-Cruz"], "venue": "In Information Theory,", "citeRegEx": "30", "shortCiteRegEx": "30", "year": 2008}, {"title": "Why should i trust you?: Explaining the predictions of any classifier", "author": ["Marco Tulio Ribeiro", "Sameer Singh", "Carlos Guestrin"], "venue": "In Proceedings of the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining,", "citeRegEx": "31", "shortCiteRegEx": "31", "year": 2016}, {"title": "Causal proteinsignaling networks derived from multiparameter single-cell data", "author": ["Karen Sachs", "Omar Perez", "Dana Pe\u2019er", "Douglas A Lauffenburger", "Garry P Nolan"], "venue": null, "citeRegEx": "32", "shortCiteRegEx": "32", "year": 2005}, {"title": "Learning causal graphs with small interventions", "author": ["Karthikeyan Shanmugam", "Murat Kocaoglu", "Alexandros G Dimakis", "Sriram Vishwanath"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "33", "shortCiteRegEx": "33", "year": 2015}, {"title": "Dynamic ad allocation: Bandits with budgets", "author": ["Aleksandrs Slivkins"], "venue": "arXiv preprint arXiv:1306.0155,", "citeRegEx": "34", "shortCiteRegEx": "34", "year": 2013}, {"title": "Causation, Prediction, and Search", "author": ["Peter Spirtes", "Clark Glymour", "Richard Scheines"], "venue": "A Bradford Book,", "citeRegEx": "35", "shortCiteRegEx": "35", "year": 2001}, {"title": "On the application of probability theory to agricultural experiments", "author": ["Jerzy Splawa-Neyman", "DM Dabrowska", "TP Speed"], "venue": "essay on principles", "citeRegEx": "36", "shortCiteRegEx": "36", "year": 1990}, {"title": "Covariate shift adaptation by importance weighted cross validation", "author": ["Masashi Sugiyama", "Matthias Krauledat", "Klaus-Robert M\u00c3\u017eller"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "37", "shortCiteRegEx": "37", "year": 2007}, {"title": "Going deeper with convolutions", "author": ["Christian Szegedy", "Wei Liu", "Yangqing Jia", "Pierre Sermanet", "Scott Reed", "Dragomir Anguelov", "Dumitru Erhan", "Vincent Vanhoucke", "Andrew Rabinovich"], "venue": "In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition,", "citeRegEx": "38", "shortCiteRegEx": "38", "year": 2015}], "referenceMentions": [{"referenceID": 5, "context": "Causal graphs [28] are useful for representing causal relationships among interacting variables in large systems [7].", "startOffset": 113, "endOffset": 116}, {"referenceID": 5, "context": "Over the last few decades, causal models have found use in computational advertising [7], biological systems [25], sociology [5], agriculture [36] and epidemiology [18].", "startOffset": 85, "endOffset": 88}, {"referenceID": 22, "context": "Over the last few decades, causal models have found use in computational advertising [7], biological systems [25], sociology [5], agriculture [36] and epidemiology [18].", "startOffset": 109, "endOffset": 113}, {"referenceID": 3, "context": "Over the last few decades, causal models have found use in computational advertising [7], biological systems [25], sociology [5], agriculture [36] and epidemiology [18].", "startOffset": 125, "endOffset": 128}, {"referenceID": 32, "context": "Over the last few decades, causal models have found use in computational advertising [7], biological systems [25], sociology [5], agriculture [36] and epidemiology [18].", "startOffset": 142, "endOffset": 146}, {"referenceID": 15, "context": "Over the last few decades, causal models have found use in computational advertising [7], biological systems [25], sociology [5], agriculture [36] and epidemiology [18].", "startOffset": 164, "endOffset": 168}, {"referenceID": 5, "context": "There are two important questions commonly studied with causal graphs: (i) How to learn a directed causal graph that encodes the pattern of interaction among components in a system (casual structure learning)? [28] , and (ii) Using previously acquired (partial) knowledge about the causal graph structure, how to estimate and/or to optimize the effect of a new intervention on other variables (optimization) [7, 18, 20, 6, 21]? Here, an intervention is a forcible change to the value of a variable in a system.", "startOffset": 408, "endOffset": 426}, {"referenceID": 15, "context": "There are two important questions commonly studied with causal graphs: (i) How to learn a directed causal graph that encodes the pattern of interaction among components in a system (casual structure learning)? [28] , and (ii) Using previously acquired (partial) knowledge about the causal graph structure, how to estimate and/or to optimize the effect of a new intervention on other variables (optimization) [7, 18, 20, 6, 21]? Here, an intervention is a forcible change to the value of a variable in a system.", "startOffset": 408, "endOffset": 426}, {"referenceID": 17, "context": "There are two important questions commonly studied with causal graphs: (i) How to learn a directed causal graph that encodes the pattern of interaction among components in a system (casual structure learning)? [28] , and (ii) Using previously acquired (partial) knowledge about the causal graph structure, how to estimate and/or to optimize the effect of a new intervention on other variables (optimization) [7, 18, 20, 6, 21]? Here, an intervention is a forcible change to the value of a variable in a system.", "startOffset": 408, "endOffset": 426}, {"referenceID": 4, "context": "There are two important questions commonly studied with causal graphs: (i) How to learn a directed causal graph that encodes the pattern of interaction among components in a system (casual structure learning)? [28] , and (ii) Using previously acquired (partial) knowledge about the causal graph structure, how to estimate and/or to optimize the effect of a new intervention on other variables (optimization) [7, 18, 20, 6, 21]? Here, an intervention is a forcible change to the value of a variable in a system.", "startOffset": 408, "endOffset": 426}, {"referenceID": 18, "context": "There are two important questions commonly studied with causal graphs: (i) How to learn a directed causal graph that encodes the pattern of interaction among components in a system (casual structure learning)? [28] , and (ii) Using previously acquired (partial) knowledge about the causal graph structure, how to estimate and/or to optimize the effect of a new intervention on other variables (optimization) [7, 18, 20, 6, 21]? Here, an intervention is a forcible change to the value of a variable in a system.", "startOffset": 408, "endOffset": 426}, {"referenceID": 5, "context": "An illustrative example includes online advertising [7], where there is a collection of click-through rate scoring algorithms that provide an estimate of the probability that an user clicks on an ad displayed at a specific position.", "startOffset": 52, "endOffset": 55}, {"referenceID": 4, "context": "Another example is in biological gene-regulatory networks [6], where a large number of genomes interact amongst each other and also interact with environmental factors.", "startOffset": 58, "endOffset": 61}, {"referenceID": 5, "context": "Figure 1: Computational advertising example borrowed from [7].", "startOffset": 58, "endOffset": 61}, {"referenceID": 19, "context": "Determining the best intervention in the above setting can be cast as a best arm identification bandit problem, as noted in [22].", "startOffset": 124, "endOffset": 128}, {"referenceID": 19, "context": "Our procedure yields a major improvement over the algorithm in [22] (both in theoretical guarantees and in practice), which sets the clippers and allocates samples in a static manner.", "startOffset": 63, "endOffset": 67}, {"referenceID": 1, "context": "(Gap Dependent Error Guarantees under Budget Constraints) In the classic best arm identification problem [3], Audibert et al.", "startOffset": 105, "endOffset": 108}, {"referenceID": 1, "context": "Then, it has been shown in [3] that the number of samples needed scales as (upto poly log factors) maxi(i/\u2206 2 (i)).", "startOffset": 27, "endOffset": 30}, {"referenceID": 1, "context": "In our setting, a fundamental difference from the classical best arm setting [3] is the information leakage across the arms, i.", "startOffset": 77, "endOffset": 80}, {"referenceID": 1, "context": "\u03c3i can be much smaller than \u221a i (the corresponding term in the results of [3]).", "startOffset": 74, "endOffset": 77}, {"referenceID": 5, "context": "Our theoretical guarantees quantify the improvement obtained by leveraging information leakage, which has been empirically observed in [7].", "startOffset": 135, "endOffset": 138}, {"referenceID": 2, "context": "1) act as the \u2018effective variance\u2019 term in the analysis for the estimators (similar to Bernstein\u2019s bound [4]).", "startOffset": 105, "endOffset": 108}, {"referenceID": 19, "context": "(Extensive Empirical Validation) We demonstrate that our algorithm outperforms the prior works [22, 3] on the Flow Cytometry data-set [32] (in Section 4.", "startOffset": 95, "endOffset": 102}, {"referenceID": 1, "context": "(Extensive Empirical Validation) We demonstrate that our algorithm outperforms the prior works [22, 3] on the Flow Cytometry data-set [32] (in Section 4.", "startOffset": 95, "endOffset": 102}, {"referenceID": 28, "context": "(Extensive Empirical Validation) We demonstrate that our algorithm outperforms the prior works [22, 3] on the Flow Cytometry data-set [32] (in Section 4.", "startOffset": 134, "endOffset": 138}, {"referenceID": 34, "context": "We exhibit an innovative application of our algorithm for model interpretation of the Inception Deep Network [38] for image classification (refer to Section 4.", "startOffset": 109, "endOffset": 113}, {"referenceID": 16, "context": "There have been many studies on the classical best arm identification in the bandit literature, both in the fixed confidence regime [19, 13] and in the fixed budget setting [3, 10, 17, 8].", "startOffset": 132, "endOffset": 140}, {"referenceID": 10, "context": "There have been many studies on the classical best arm identification in the bandit literature, both in the fixed confidence regime [19, 13] and in the fixed budget setting [3, 10, 17, 8].", "startOffset": 132, "endOffset": 140}, {"referenceID": 1, "context": "There have been many studies on the classical best arm identification in the bandit literature, both in the fixed confidence regime [19, 13] and in the fixed budget setting [3, 10, 17, 8].", "startOffset": 173, "endOffset": 187}, {"referenceID": 7, "context": "There have been many studies on the classical best arm identification in the bandit literature, both in the fixed confidence regime [19, 13] and in the fixed budget setting [3, 10, 17, 8].", "startOffset": 173, "endOffset": 187}, {"referenceID": 14, "context": "There have been many studies on the classical best arm identification in the bandit literature, both in the fixed confidence regime [19, 13] and in the fixed budget setting [3, 10, 17, 8].", "startOffset": 173, "endOffset": 187}, {"referenceID": 6, "context": "There have been many studies on the classical best arm identification in the bandit literature, both in the fixed confidence regime [19, 13] and in the fixed budget setting [3, 10, 17, 8].", "startOffset": 173, "endOffset": 187}, {"referenceID": 6, "context": "It was shown recently in [8] that the results of [3] are optimal.", "startOffset": 25, "endOffset": 28}, {"referenceID": 1, "context": "It was shown recently in [8] that the results of [3] are optimal.", "startOffset": 49, "endOffset": 52}, {"referenceID": 24, "context": "There has been a lot of work [27, 16, 12, 15, 35, 29, 24, 33] on learning casual models from data and/or experiments and using it to estimate causal strength questions of the counterfactual nature.", "startOffset": 29, "endOffset": 61}, {"referenceID": 13, "context": "There has been a lot of work [27, 16, 12, 15, 35, 29, 24, 33] on learning casual models from data and/or experiments and using it to estimate causal strength questions of the counterfactual nature.", "startOffset": 29, "endOffset": 61}, {"referenceID": 9, "context": "There has been a lot of work [27, 16, 12, 15, 35, 29, 24, 33] on learning casual models from data and/or experiments and using it to estimate causal strength questions of the counterfactual nature.", "startOffset": 29, "endOffset": 61}, {"referenceID": 12, "context": "There has been a lot of work [27, 16, 12, 15, 35, 29, 24, 33] on learning casual models from data and/or experiments and using it to estimate causal strength questions of the counterfactual nature.", "startOffset": 29, "endOffset": 61}, {"referenceID": 31, "context": "There has been a lot of work [27, 16, 12, 15, 35, 29, 24, 33] on learning casual models from data and/or experiments and using it to estimate causal strength questions of the counterfactual nature.", "startOffset": 29, "endOffset": 61}, {"referenceID": 25, "context": "There has been a lot of work [27, 16, 12, 15, 35, 29, 24, 33] on learning casual models from data and/or experiments and using it to estimate causal strength questions of the counterfactual nature.", "startOffset": 29, "endOffset": 61}, {"referenceID": 21, "context": "There has been a lot of work [27, 16, 12, 15, 35, 29, 24, 33] on learning casual models from data and/or experiments and using it to estimate causal strength questions of the counterfactual nature.", "startOffset": 29, "endOffset": 61}, {"referenceID": 29, "context": "There has been a lot of work [27, 16, 12, 15, 35, 29, 24, 33] on learning casual models from data and/or experiments and using it to estimate causal strength questions of the counterfactual nature.", "startOffset": 29, "endOffset": 61}, {"referenceID": 5, "context": "One notable work that partially inspired our work is [7] where the causal graph underlying a computational advertising system (like in Bing, Google etc.", "startOffset": 53, "endOffset": 56}, {"referenceID": 19, "context": "At the intersection of causality and bandits, [22] is perhaps most relevant to our setting.", "startOffset": 46, "endOffset": 50}, {"referenceID": 5, "context": "We assume soft interventions that affect the mechanism between a \u2019source\u2019 node and its parents, far away from the target (similar to the case of computational advertising considered in [7]).", "startOffset": 185, "endOffset": 188}, {"referenceID": 1, "context": "Further, we derive the first gap dependent bounds (that can be exponentially small in T ), generalizing the results of [3].", "startOffset": 119, "endOffset": 122}, {"referenceID": 19, "context": "Our formulation can handle general budget constraints on the bandit arms and also recover the problem independent bounds of [22] (orderwise).", "startOffset": 124, "endOffset": 128}, {"referenceID": 0, "context": "Budget constraints in bandit settings have been explored before in [1, 34].", "startOffset": 67, "endOffset": 74}, {"referenceID": 30, "context": "Budget constraints in bandit settings have been explored before in [1, 34].", "startOffset": 67, "endOffset": 74}, {"referenceID": 33, "context": "In the context of machine learning, importance sampling has been mostly used to recondition input data to adhere to conditions imposed by learning algorithms [37, 23, 39].", "startOffset": 158, "endOffset": 170}, {"referenceID": 20, "context": "In the context of machine learning, importance sampling has been mostly used to recondition input data to adhere to conditions imposed by learning algorithms [37, 23, 39].", "startOffset": 158, "endOffset": 170}, {"referenceID": 0, "context": "For simplicity we assume that the variables V, pa(V ) are discrete while the target variable Y may be continuous/discrete and has bounded support in [0, 1].", "startOffset": 149, "endOffset": 155}, {"referenceID": 1, "context": "Fixed Budget for Samples: In this paper, we work under the fixed budget setting of best arm identification [3].", "startOffset": 107, "endOffset": 110}, {"referenceID": 5, "context": "We find such examples in the context of online advertisement design [7].", "startOffset": 68, "endOffset": 71}, {"referenceID": 30, "context": "(ii) Cost Budget (S2): This is the most general budget setting that captures the variable costs of sampling each arm [34].", "startOffset": 117, "endOffset": 121}, {"referenceID": 1, "context": "Then the probability of error e(T,B) [3, 8] is given by, e(T,B) = P ( k\u0302(T,B) 6= k\u2217 )", "startOffset": 37, "endOffset": 43}, {"referenceID": 6, "context": "Then the probability of error e(T,B) [3, 8] is given by, e(T,B) = P ( k\u0302(T,B) 6= k\u2217 )", "startOffset": 37, "endOffset": 43}, {"referenceID": 19, "context": "(Simple Regret): Another important quantity that has been analyzed in the best arm identification setting is the simple regret [22].", "startOffset": 127, "endOffset": 131}, {"referenceID": 19, "context": "A popular method for utilizing this information leakage among different distributions is through importance sampling, which has been used in counterfactual analysis in similar causal settings [22, 7].", "startOffset": 192, "endOffset": 199}, {"referenceID": 5, "context": "A popular method for utilizing this information leakage among different distributions is through importance sampling, which has been used in counterfactual analysis in similar causal settings [22, 7].", "startOffset": 192, "endOffset": 199}, {"referenceID": 19, "context": "This has been noted in [22] in a similar setting, where a static clipper has been applied to the weighted samples to control the variance.", "startOffset": 23, "endOffset": 27}, {"referenceID": 26, "context": "We also note that estimates of the f -divergence measure can be had directly from empirical data (without the knowledge of the full distributions) using techniques like that of [30].", "startOffset": 177, "endOffset": 181}, {"referenceID": 19, "context": "2We note that the authors in [22] discuss the possibility of a multi-phase approach, where clipper levels could change across phases.", "startOffset": 29, "endOffset": 33}, {"referenceID": 1, "context": "Our results can be interpreted as a natural generalization of the results in [3], when there is information leakage among the arms.", "startOffset": 77, "endOffset": 80}, {"referenceID": 1, "context": "Comparison with the result in [3]: Let R\u0303(\u2206k) = {s : \u2206s \u2264 \u2206k}, i.", "startOffset": 30, "endOffset": 33}, {"referenceID": 1, "context": "The result in [3] can be stated as: The error in finding the optimal arm is bounded as: e(T ) \u2264 O ( K exp ( \u2212 T\u2212K log(K)H\u0303 )) .", "startOffset": 14, "endOffset": 17}, {"referenceID": 19, "context": "Comparison with the result in [22]: In [22], the simple regret scales as O(1/ \u221a T ) and does not adapt to the gaps.", "startOffset": 30, "endOffset": 34}, {"referenceID": 19, "context": "Comparison with the result in [22]: In [22], the simple regret scales as O(1/ \u221a T ) and does not adapt to the gaps.", "startOffset": 39, "endOffset": 43}, {"referenceID": 19, "context": "We provide gap dependent bounds that can be exponentially better than that of [22] (when \u2206k\u2019s are not too small and the first term in (7) is zero).", "startOffset": 78, "endOffset": 82}, {"referenceID": 28, "context": "1, we study the empirical performance of our algorithm on the flow cytometry data-set [32].", "startOffset": 86, "endOffset": 90}, {"referenceID": 34, "context": "2, we apply our algorithms for the purpose of model interpretability of the Inception Deep Network [38] in the context of image classification.", "startOffset": 99, "endOffset": 103}, {"referenceID": 23, "context": "(a) Causal Graph for Cytometry Data [26] 0 100 200 300 400 500 600 700 800 900 0.", "startOffset": 36, "endOffset": 40}, {"referenceID": 28, "context": "The flow cytometry data-set [32] consists of multi-variate measurements of protein interactions in a single cell, under different experimental conditions (soft interventions).", "startOffset": 28, "endOffset": 32}, {"referenceID": 23, "context": "5(c) in [26] (shown in Fig.", "startOffset": 8, "endOffset": 12}, {"referenceID": 22, "context": "Parametric linear models have been popularly used for causal inference on this data-set [25, 11].", "startOffset": 88, "endOffset": 96}, {"referenceID": 8, "context": "Parametric linear models have been popularly used for causal inference on this data-set [25, 11].", "startOffset": 88, "endOffset": 96}, {"referenceID": 11, "context": "We fit a GLM gamma model [14] between the activation of each node and its parents in Fig.", "startOffset": 25, "endOffset": 29}, {"referenceID": 1, "context": "Competing Algorithms: We test our algorithms on different problem parameters and compare with related prior work [3, 22].", "startOffset": 113, "endOffset": 120}, {"referenceID": 19, "context": "Competing Algorithms: We test our algorithms on different problem parameters and compare with related prior work [3, 22].", "startOffset": 113, "endOffset": 120}, {"referenceID": 26, "context": "The divergences, Df1(Pi||Pj) are estimated from sampled data using techniques from [30]; (ii)SRISv2: Algorithm 2 as detailed in Section 3.", "startOffset": 83, "endOffset": 87}, {"referenceID": 1, "context": "2; (iii) SR: Successive Rejects Algorithm from [3] adapted to the budget setting.", "startOffset": 47, "endOffset": 50}, {"referenceID": 19, "context": "The division of the total budget T into K \u2212 1 phases is identical, while the individual arm budgets are decided in each phase according to the budget restrictions; (iv) CR: Algorithm 2 from [22].", "startOffset": 190, "endOffset": 194}, {"referenceID": 34, "context": "2 Interpretability of Inception Deep Network In this section we use our algorithm for model interpretation of the pre-trained Inception-v3 network [38] for classifying images.", "startOffset": 147, "endOffset": 151}, {"referenceID": 27, "context": "Model Interpretation essentially addresses: \u2019why does a learning model classify in a certain way?\u2019, which is an important question for complicated models like deep nets [31].", "startOffset": 169, "endOffset": 173}, {"referenceID": 1, "context": "We demonstrate that our algorithm performs well in all the experiments, as compared to previous works [3, 22].", "startOffset": 102, "endOffset": 109}, {"referenceID": 19, "context": "We demonstrate that our algorithm performs well in all the experiments, as compared to previous works [3, 22].", "startOffset": 102, "endOffset": 109}, {"referenceID": 1, "context": "In conclusion, it should be noted that our algorithms perform well in all the different settings, because they are able to adapt to the problem parameters (similar to [3]) and at the same time leverage the information leakage (similar to [22]).", "startOffset": 167, "endOffset": 170}, {"referenceID": 19, "context": "In conclusion, it should be noted that our algorithms perform well in all the different settings, because they are able to adapt to the problem parameters (similar to [3]) and at the same time leverage the information leakage (similar to [22]).", "startOffset": 238, "endOffset": 242}, {"referenceID": 1, "context": "We provide the first problem dependent simple regret and error bounds for this problem, that is a natural generalization of [3], but with information leakage between arms.", "startOffset": 124, "endOffset": 127}, {"referenceID": 19, "context": "Problem Dependent Lower Bound: In [22], a problem independent lower bound of O(1/ \u221a T ) has been provided for a special causal graph.", "startOffset": 34, "endOffset": 38}, {"referenceID": 1, "context": "However, the problem parameter dependent lower bound like that of [3] still remains an open problem.", "startOffset": 66, "endOffset": 69}, {"referenceID": 5, "context": "In fact, this is explored in a non-bandit context in [7].", "startOffset": 53, "endOffset": 56}, {"referenceID": 19, "context": "An important question is: What is the most suitable cut to be used? [22] uses the cut closest to Y , i.", "startOffset": 68, "endOffset": 72}], "year": 2017, "abstractText": "Motivated by applications in computational advertising and systems biology, we consider the problem of identifying the best out of several possible soft interventions at a source node V in an acyclic causal directed graph, to maximize the expected value of a target node Y (located downstream of V ). Our setting imposes a fixed total budget for sampling under various interventions, along with cost constraints on different types of interventions. We pose this as a best arm identification bandit problem with K arms where each arm is a soft intervention at V, and leverage the information leakage among the arms to provide the first gap dependent error and simple regret bounds for this problem. Our results are a significant improvement over the traditional best arm identification results. We empirically show that our algorithms outperform the state of the art in the Flow Cytometry data-set, and also apply our algorithm for model interpretation of the Inception-v3 deep net that classifies images.", "creator": "LaTeX with hyperref package"}}}