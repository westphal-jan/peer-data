{"id": "1701.02810", "review": {"conference": "acl", "VERSION": "v1", "DATE_OF_SUBMISSION": "10-Jan-2017", "title": "OpenNMT: Open-Source Toolkit for Neural Machine Translation", "abstract": "We describe an open-source toolkit for neural machine translation (NMT). The toolkit prioritizes efficiency, modularity, and extensibility with the goal of supporting NMT research into model architectures, feature representations, and source modalities, while maintaining competitive performance and reasonable training requirements. The toolkit consists of modeling and translation support, as well as detailed pedagogical documentation about the underlying techniques.", "histories": [["v1", "Tue, 10 Jan 2017 23:32:43 GMT  (47kb,D)", "http://arxiv.org/abs/1701.02810v1", "Report forthis http URL"], ["v2", "Mon, 6 Mar 2017 15:54:27 GMT  (93kb,D)", "http://arxiv.org/abs/1701.02810v2", "Report forthis http URL"]], "COMMENTS": "Report forthis http URL", "reviews": [], "SUBJECTS": "cs.CL cs.AI cs.NE", "authors": ["guillaume klein", "yoon kim", "yuntian deng", "jean senellart", "alexander m rush"], "accepted": true, "id": "1701.02810"}, "pdf": {"name": "1701.02810.pdf", "metadata": {"source": "CRF", "title": "OpenNMT: Open-Source Toolkit for Neural Machine Translation", "authors": ["Guillaume Klein", "Yoon Kim", "Yuntian Deng", "Jean Senellart", "Alexander M. Rush"], "emails": [], "sections": [{"heading": "1 Introduction", "text": "Neural machine translation (NMT) is a new methodology for machine translation that has led to remarkable improvements, especially in terms of human evaluation, compared to rule-based and statistical machine translation systems (SMT) (Wu et al., 2016; Crego et al., 2016). Originally developed using pure sequence-to-sequence models (Sutskever et al., 2014; Cho et al., 2014) and improved in the use of attention-based variants (Bahdanau et al., 2014; Luong et al., 2015), NMT has now become a widely used technique for machine translation, as well as an effective approach for other related NLP tasks such as dialogue, parsing and summarization.As NMT approaches are standardized, it is becoming more important for machine translation and NLP community to develop open implementations for researchers."}, {"heading": "2 Background", "text": "NMT has now been extensively described in many excellent tutorials (see for example https: / / sites.google.com / site / acl16nmt / home).We give a condensed overview.NMT takes conditional linguistic modelling of the translation by modelling the probability of a target sentence w1: T in relation to a source sentence x1: S as p (w1: T | x) = ig T 1 p (wt | w1: t \u2212 1, x; \u03b8).This distribution is estimated using an attention-based encoder decoder architecture (Bahdanau et al., 2014).A source code recurrent neural network (RNN) maps each source word to a word vector and processes it to a sequence of hidden vectors h1,.., hS. The target decoder combines an RNN hidden representation of already generated words (w1,... w1) with hidden vectors that form a hidden predictor for any prediction."}, {"heading": "3 Implementation", "text": "OpenNMT is a complete library for learning, training, and deployment of neural machine translation models. It is the successor to seq2seq-attn, developed at Harvard, and has been completely rewritten to simplify efficiency, readability, and generalizability. It includes vanilla NMT models along with support for attention, gating, stacking, input power, regulation, beam search, and all other options required for cutting-edge performance. It is implemented using the Torch mathematical framework and neural network library, and can be easily expanded by Torch's standard internal neural network components. It has been fully developed on GitHub at (http: / / github.com / opennmt / opennmt) and is licensed by MIT. The first version includes primarily (intercontinental) contributions from SYSTRAN Paris and the Harvard NLP Group. Since the official beta version, the project has been actively developed by 500 users, and there has been an active response from both."}, {"heading": "4 Design Goals", "text": "As the details of low-level NMT have been covered in previous work, this report focuses on the design goals of OpenNMT, focusing in particular on three ordered criteria: system efficiency, code modularity, and model expandability."}, {"heading": "4.1 System Efficiency", "text": "Because NMT systems can take days to weeks to train, training efficiency is a primary concern. Any faster training can make the difference between plausible and impossible experiments. Optimization: Memory Sharing In the formation of GPU-based NMT models, memory size limitations are the most common limitations on batch size, so we have implemented an external storage system that exploits the known time control flow of NMT systems and aggressively divides the internal buffers between clones. Potential shared buffers are calculated by exploring the network before starting in both directions, and so we implement an external storage system that exploits the known time control mechanisms of NMT systems."}, {"heading": "4.2 Modularity for Research", "text": "A secondary goal was the desire for code readability for non-experts. We aimed to achieve this goal by explicitly separating many optimizations from the core model and by inserting tutorial documentation into the code. To test whether this approach would allow for the development of new features, we experimented with two case studies. Case Study: Factored Neural Translation In feature-based factored Neural Translation (Sennrich and Haddow, 2016), however, the system could include words and separate case features, rather than generating a word at each step. This expansion requires both the input and output of the decoder to generate multiple symbols. In OpenNMT, both of these aspects are abstracted from the core translation code, and therefore factored translation simply modifies the input network to process feature-based representation instead, and the output generator network structures its attention so that it instead generates several conditionally independent pre-productions from the network."}, {"heading": "4.3 Extensibility", "text": "Deep learning is a rapidly evolving field. Recently, work such as seq2seq variation models auto-encoder (Bowman et al., 2016) or storage networks (Weston et al., 2014) have offered interesting extensions to basic seq2seq models. Next, we will discuss a case study to show that OpenNMT is expandable to future variants. Case study: Image-to-Text Recent work has shown that NMT-like systems for picture-to-text generation tasks are effective. (Xu et al., 2015) This task is very different from standard machine translation, as the initial set is now an image (!). However, the future of translation may require this style of (multi-) model input (e.g. http: / / www.statmt.org / wmt16 / multimodal-task.html). As a case study adapting the imm2T Markup System to Rtu requires the full use of this Ubuntu model as well (2)."}, {"heading": "5 Benchmarks", "text": "The most important benchmarks are performed with an Intel (R) Core (TM) i7-5930K CPU @ 3.50GHz, 256GB meme, trained on 1 GPU GeForce GTX 1080 (Pascal) with CUDA v. 8.0 (driver 375.20) and cuDNN (v. 5005). Comparison, shown in Table 1, is performed in English-German (EN \u2192 DE) using WMT 20151 data set. Here, we compare the BLEU score and training and test speed with the publicly available Nematus system. 2We have also trained a multilingual translation model based on Johnson (2016). The model translates to and from French, Spanish, Portuguese, and Romanian."}, {"heading": "6 Conclusion", "text": "We introduce OpenNMT, a research tool for NMT that prioritizes efficiency and modularity. We hope to develop OpenNMT further to get strong MT results on the research front and create a stable and expandable framework for production use."}], "references": [{"title": "Neural Machine http://statmt.org/wmt15 https://github.com/rsennrich/nematus", "author": ["Kyunghyun Cho", "Yoshua Bengio"], "venue": null, "citeRegEx": "Bahdanau et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Bahdanau et al\\.", "year": 2014}, {"title": "Generating sentences from a continuous space", "author": ["Luke Vilnis", "Oriol Vinyals", "Andrew M. Dai", "Rafal J\u00f3zefowicz", "Samy Bengio"], "venue": "In Proceedings of the 20th SIGNLL Conference on Computational Natural", "citeRegEx": "Bowman et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Bowman et al\\.", "year": 2016}, {"title": "Learning Phrase Representations using RNN Encoder-Decoder for Statistical Machine Transla", "author": ["Cho et al.2014] Kyunghyun Cho", "Bart van Merrienboer", "Caglar Gulcehre", "Dzmitry Bahdanau", "Fethi Bougares", "Holger Schwenk", "Yoshua Bengio"], "venue": null, "citeRegEx": "Cho et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Cho et al\\.", "year": 2014}, {"title": "Empirical evaluation of gated recurrent neural networks on sequence modeling", "author": ["Chung et al.2014] Junyoung Chung", "Caglar Gulcehre", "KyungHyun Cho", "Yoshua Bengio"], "venue": "arXiv preprint arXiv:1412.3555", "citeRegEx": "Chung et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Chung et al\\.", "year": 2014}, {"title": "Systran\u2019s pure neural machine translation system", "author": ["Crego et al.2016] Josep Crego", "Jungi Kim", "Jean Senellart"], "venue": "arXiv preprint arXiv:1602.06023", "citeRegEx": "Crego et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Crego et al\\.", "year": 2016}, {"title": "What you get is what you see: A visual markup decompiler. CoRR, abs/1609.04938", "author": ["Deng et al.2016] Yuntian Deng", "Anssi Kanervisto", "Alexander M. Rush"], "venue": null, "citeRegEx": "Deng et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Deng et al\\.", "year": 2016}, {"title": "cdec: A decoder, alignment, and learning framework for finite-state and context-free", "author": ["Dyer et al.2010] Chris Dyer", "Jonathan Weese", "Hendra Setiawan", "Adam Lopez", "Ferhan Ture", "Vladimir Eidelman", "Juri Ganitkevitch", "Phil Blunsom", "Philip Resnik"], "venue": null, "citeRegEx": "Dyer et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Dyer et al\\.", "year": 2010}, {"title": "Long short-term memory", "author": ["Hochreiter", "Schmidhuber1997] Sepp Hochreiter", "J\u00fcrgen Schmidhuber"], "venue": "Neural computation,", "citeRegEx": "Hochreiter et al\\.,? \\Q1997\\E", "shortCiteRegEx": "Hochreiter et al\\.", "year": 1997}, {"title": "Effective Approaches to Attention-based Neural Machine Translation", "author": ["Hieu Pham", "Christopher D. Manning"], "venue": "In Proceedings of EMNLP", "citeRegEx": "Luong et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Luong et al\\.", "year": 2015}, {"title": "From softmax to sparsemax: A sparse model of attention and multi-label classification", "author": ["Martins", "Astudillo2016] Andr\u00e9 FT Martins", "Ram\u00f3n Fernandez Astudillo"], "venue": "arXiv preprint arXiv:1602.02068", "citeRegEx": "Martins et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Martins et al\\.", "year": 2016}, {"title": "Linguistic input features improve neural machine translation", "author": ["Sennrich", "Haddow2016] Rico Sennrich", "Barry Haddow"], "venue": "arXiv preprint arXiv:1606.02892", "citeRegEx": "Sennrich et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Sennrich et al\\.", "year": 2016}, {"title": "Sequence to Sequence Learning with Neural Networks", "author": ["Oriol Vinyals", "Quoc V. Le"], "venue": "In NIPS,", "citeRegEx": "Sutskever et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Sutskever et al\\.", "year": 2014}, {"title": "Show, attend and tell: Neural image caption generation with visual attention", "author": ["Xu et al.2015] Kelvin Xu", "Jimmy Ba", "Ryan Kiros", "Kyunghyun Cho", "Aaron C. Courville", "Ruslan Salakhutdinov", "Richard S. Zemel", "Yoshua Bengio"], "venue": null, "citeRegEx": "Xu et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Xu et al\\.", "year": 2015}, {"title": "Hierarchical attention networks for document classification", "author": ["Yang et al.2016] Zichao Yang", "Diyi Yang", "Chris Dyer", "Xiaodong He", "Alex Smola", "Eduard Hovy"], "venue": "In Proceedings of the 2016 Conference of the North American Chapter of the Association", "citeRegEx": "Yang et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Yang et al\\.", "year": 2016}], "referenceMentions": [{"referenceID": 4, "context": "Neural machine translation (NMT) is a new methodology for machine translation that has led to remarkable improvements, particularly in terms of human evaluation, compared to rule-based and statistical machine translation (SMT) systems (Wu et al., 2016; Crego et al., 2016).", "startOffset": 235, "endOffset": 272}, {"referenceID": 11, "context": "Originally developed using pure sequence-to-sequence models (Sutskever et al., 2014; Cho et al., 2014) and improved upon using attention-based variants (Bahdanau et al.", "startOffset": 60, "endOffset": 102}, {"referenceID": 2, "context": "Originally developed using pure sequence-to-sequence models (Sutskever et al., 2014; Cho et al., 2014) and improved upon using attention-based variants (Bahdanau et al.", "startOffset": 60, "endOffset": 102}, {"referenceID": 0, "context": ", 2014) and improved upon using attention-based variants (Bahdanau et al., 2014; Luong et al., 2015), NMT has now become a widely-applied technique for machine translation, as well as an effective approach for other related NLP tasks such as dialogue, parsing, and summarization.", "startOffset": 57, "endOffset": 100}, {"referenceID": 8, "context": ", 2014) and improved upon using attention-based variants (Bahdanau et al., 2014; Luong et al., 2015), NMT has now become a widely-applied technique for machine translation, as well as an effective approach for other related NLP tasks such as dialogue, parsing, and summarization.", "startOffset": 57, "endOffset": 100}, {"referenceID": 6, "context": ", 2007) for phrase-based MT and the CDec toolkit (Dyer et al., 2010) for syntax-based MT, NMT toolkits can provide a foundation for the research community.", "startOffset": 49, "endOffset": 68}, {"referenceID": 0, "context": "This distribution is estimated using an attentionbased encoder-decoder architecture (Bahdanau et al., 2014).", "startOffset": 84, "endOffset": 107}, {"referenceID": 3, "context": "In practice, there are also several other important aspects that contribute to model effectiveness: (a) It is important to use a gated RNN such as an LSTM (Hochreiter and Schmidhuber, 1997) or GRU (Chung et al., 2014) which help the model", "startOffset": 197, "endOffset": 217}, {"referenceID": 11, "context": "(b) Translation requires relatively large, stacked RNNs, which consist of several layers (2-16) of RNN at each time step (Sutskever et al., 2014).", "startOffset": 121, "endOffset": 145}, {"referenceID": 8, "context": "(c) Input feeding, where the previous attention vector is fed back into the input as well as the predicted word, has been shown to be quite helpful for machine translation (Luong et al., 2015).", "startOffset": 172, "endOffset": 192}, {"referenceID": 8, "context": "However there are many other types of attention that have recently proposed including local attention (Luong et al., 2015), sparse-max attention (Martins and Astudillo, 2016), hierarchical attention (Yang et al.", "startOffset": 102, "endOffset": 122}, {"referenceID": 13, "context": ", 2015), sparse-max attention (Martins and Astudillo, 2016), hierarchical attention (Yang et al., 2016) among others.", "startOffset": 84, "endOffset": 103}, {"referenceID": 1, "context": "Recently work such as variational seq2seq auto-encoders (Bowman et al., 2016) or memory networks (We-", "startOffset": 56, "endOffset": 77}, {"referenceID": 12, "context": "(Xu et al., 2015).", "startOffset": 0, "endOffset": 17}, {"referenceID": 5, "context": "we adapted the im2markup system (Deng et al., 2016) to use OpenNMT.", "startOffset": 32, "endOffset": 51}], "year": 2017, "abstractText": "We describe an open-source toolkit for neural machine translation (NMT). The toolkit prioritizes efficiency, modularity, and extensibility with the goal of supporting NMT research into model architectures, feature representations, and source modalities, while maintaining competitive performance and reasonable training requirements. The toolkit consists of modeling and translation support, as well as detailed pedagogical documentation about the underlying techniques.", "creator": "LaTeX with hyperref package"}}}