{"id": "1701.02815", "review": {"conference": "ICML", "VERSION": "v1", "DATE_OF_SUBMISSION": "11-Jan-2017", "title": "Stochastic Generative Hashing", "abstract": "Learning to hash plays a fundamentally important role in the efficient image and video retrieval and many other computer vision problems. However, due to the binary outputs of the hash functions, the learning of hash functions is very challenging. In this paper, we propose a novel approach to learn stochastic hash functions such that the learned hashing codes can be used to regenerate the inputs. We develop an efficient stochastic gradient learning algorithm which can avoid the notorious difficulty caused by binary output constraint, and directly optimize the parameters of the hash functions and the associated generative model jointly. The proposed method can be applied to both $L2$ approximate nearest neighbor search (L2NNS) and maximum inner product search (MIPS). Extensive experiments on a variety of large-scale datasets show that the proposed method achieves significantly better retrieval results than previous state-of-the-arts.", "histories": [["v1", "Wed, 11 Jan 2017 00:23:34 GMT  (4699kb,D)", "http://arxiv.org/abs/1701.02815v1", "19 pages, 22 figures"], ["v2", "Sat, 12 Aug 2017 21:36:09 GMT  (4441kb,D)", "http://arxiv.org/abs/1701.02815v2", "21 pages, 40 figures"]], "COMMENTS": "19 pages, 22 figures", "reviews": [], "SUBJECTS": "cs.LG cs.CV stat.ML", "authors": ["bo dai", "ruiqi guo", "sanjiv kumar", "niao he", "le song"], "accepted": true, "id": "1701.02815"}, "pdf": {"name": "1701.02815.pdf", "metadata": {"source": "CRF", "title": "Stochastic Generative Hashing", "authors": ["Sanjiv Kumar", "Niao He", "Le Song"], "emails": ["bodai@gatech.edu,", "lsong@cc.gatech.edu", "sanjivk}@google.com", "niaohe@illinois.edu"], "sections": [{"heading": "1 Introduction", "text": "Searching for similar data in the database is an important task in image and documentation technology."}, {"heading": "1.1 Main Contribution", "text": "It's a way in which we relate to each other in the way in which we relate to each other in the way in which we relate to each other in the way in which we relate to each other in the way in which we relate to each other in the way in which we relate to each other in the way in which we relate to each other in the way in which we relate to each other in the way in which we relate to each other in the way in which we relate to each other in the way in which we relate to each other in the way in which we relate to each other in the way in which we relate to each other in the way in which we relate to each other in the way in which we relate to each other in which we reciprocate in the way in which we reciprocate in which we reciprocate in the way in which we reciprocate in which we reciprocate in which we reciprocate in which we reciprocate in which we reciprocate in which we reciprocate in which we reciprocate in which we reciprocate in which we reciprocate in which we reciprocate in which we reciprocate in which we reciprocate in which we reciprocate in which we reciprocate in which we reciprocate in which we reciprocate in which we reciprocate in which we reciprocate in which we reciprocate in which we reciprocate in which we reciprocate in which we reciprocate in which we reciprocate in which we reciprocate in which we reciprocate in which we reciprocate in which we reciprocate in which we reciprocate in which we reciprocate in which we reciprocate in which we reciprocate in which we reciprocate in which we reciprocate in which we reciprocate in which we reciprocate in which we reciprocate in which we reciprocate in which we reciprocate in which we reciprocate in which we reciprocate in which we reciprocate in which we reciprocate in which we reciprocate in which we reciprocate in which we reciprocate in which"}, {"heading": "2 Stochastic Generative Hashing", "text": "In order to achieve in principle the important trade-off between the first two requirements, i.e. the preservation of the neighbourhood and the compactness of the binary codes, we propose a generative model that treats the binary hash codes h, 1, 1 l as latent variables. In contrast to most existing methods, where the hash function is modelled as a signed projection of the original data and is learned by means of some heuristic targets that prefer the binary codes that maintain the original neighbourhood, we model the process of data generation directly, i.e. in view of the binary hash codes h, the observed samples x, Rd are generated by a probabilistic model p (x | h). We will show that the binary latent variable h with a suitable p (x | h) and a theoretically founded goal actually balances these two factors in the sense of information theory. Furthermore, we have designed a delicate algorithm to solve the optimization so that the requirements iv and iv) are fulfilled and iv) fast."}, {"heading": "2.1 Reduced Gaussian-Markov Random Fields", "text": "u > u > u > u > u > u > u \"r\" u \"u\" u \"u\" u \"u\" u \"u\" u \"u\" u \"u\" u \"u\" u \"u\" u \"u\" u \"u\" u \"u\" u \"u\" u \"u\" u \"u\" u \"u\" u \"u\" u \"u\" u \"u\" u \"u\" u \"u\" u \"u\" u \"u\" u \"u\" u \"u\" u \"u\" u \"u\" u \"u\" u \"u\" u \"u\" u \"u\" u \"u\" u \"u\" u \"u\" u \"u\" u \"u\" u \"u\" u \"u\" u \"u\" u \"u\" u \"u\" u \"u\" u \"u\" u \"u\" u \"u\" u \"u\" u \"u\" u \"u\" u \"u\" u \"u\" u \"u\" u \"u\" u \"u\" u \"u\" u \"u\" u \"u\" u \"u\" u \"u\" u \"u\" u \"u\" u \"u\" u \"u\" u \"u\" u \"u\" u \"u\" u \"u\" u \"u\" u \"u\" u \"u\" u \"u\" u \"u\" u \"u\" u \"u\" u \"u\" u \"u\" u \"u\" u \"u\" u \"u\" u \"u\" u \"u\" u \"u\" u \"u\" u \"u\" u \"u\" u \"u\" u \"u\" u \"u\" u \"u\" u \"u\" u \"u\" u \"u\" u \"u\" u \"u\" u \"u\" u \"u\" u \"u\" u \"u\" u \"u\" u \"u\" u \"u\" u \"u\" u \"u\" u \"u\" u \"u\" u \"u\" u \"u\" u \"u\" u \"u\" u \"u\" u \"u\" u \"u\" u \"u\" u \"u\" u \"u\" u \"u\" u \"u\" u \"u\" u \"u\" u \"\" u \"u\" u \"u\" u \"u\" u \"u\" u \"u\" u \"u"}, {"heading": "2.2 Stochastic Encoder for Hashing", "text": "The concept of algorithms in the CD (3) leads to unacceptable costs. (3) The algorithm needs from the field of research and exploration of the world (3) are also able to establish themselves in the real world (3). (4) We apply the off-the-shelf Contrastive Divergence (CD) algorithms. (4) We will apply them directly. (4) We will use the off-the-shelf Contrastive Divergence (CD) algorithm (Hinton, 2002) will lead to unacceptable costs. (3) The algorithms we apply. (4) We will use the off-the-shelf Contrastive Divergence (CD) algorithm (Hinton, 2002) will cause unacceptable costs. (4) We will apply the algorithm needs. (4) We will use the off-the-shelf Contrastive Divergence (CD) algorithm (CD)."}, {"heading": "3 Learning with Doubly Stochastic Neuron", "text": "As shown in Section 2.2, by maximizing the negative problems of free Helmholtz energy (7) we no longer have to deal directly with the discrete latent variables, but with their distributions. However, the existing algorithms using either the repair ametrization trick (Kingma and Welling, 2013) or the REINFORCE trick (Mnih and Gregor, 2014) cannot be readily applied to the proposed model; the repair trick suffers from the difficulty of discontinuous target, whose gradient is not well defined; the REINFORCE trick suffers from a high variance of the stochastic estimator of the gradient, whose gradient WL = Eq (h | x; W) logq (h; W) p (x, h) p (9), so that it slowly or possibly does not converge (Bengio et al, 2013; Gu et al, 2015)."}, {"heading": "3.1 Doubly Stochastic Neuron", "text": "First, we perform a double stochastic neuron to restore the Bernoulli distribution B = > q (z), where z (0, 1) is defined as: = 1, if z > 1z >, if z <. (10) Note that P (f (z,) = 1) = z, i.e. f (z,) \u0445 B (z). The name \"double stochastic neuron\" comes from the fact that it requires two stochastic inputs. (10) Compared with the traditional binary stochastic neuron s1 (z,) = 1z > or s0 (z,) \u0445 B (z,) = 1z > with the difference U (0, 1), the double stochastic neuron behaves almost identically."}, {"heading": "3.2 Distributional Stochastic Gradient Descent", "text": "Since x was chosen randomly from {xi} Ni = 1, it is easy to calculate the stochastic gradient Lsn with respect to W due to the discontinuity of the stochastic neuron f (z, x), because the SGD algorithm is not readily applicable. To overcome this difficulty, we will adopt the term distribution derivative for generalized functions (a.k.a distributions) (Grubb, 2008)."}, {"heading": "3.2.1 Distributions and distributional derivative", "text": "The elements in space D are often referred to as distributions. We emphasize that this definition of distribution is more general than traditional probability distributions. Definition 3 (Grubb, 2008) If u-D elements are admitted, then a distribution v is referred to as a distribution derivative of u, which is referred to as v = Du, if it meets the requirements for a permanent linear distribution.Definition 3 (Grubb, 2008) If u-D elements are admitted, then a distribution v is referred to as a distribution derivative of u, which is referred to as v = Du, if it meets the requirements for a permanent distribution."}, {"heading": "3.2.2 Computing distributional derivative of Lsn(\u0398;x)", "text": "Based on the definition of the distribution rules and chains, we are able to calculate the distribution mechanisms (...) (...) (...) (...) (...) (...) (...) (...) (...) (...) (...) (...) (...) (...) (...) (...) (...) (...) (...) (...) (...) (...) (...) (...) (...) (...) (...) (...) (...) (...) (...) (...) (...) (...) (...) (...) (...) (...) (...) (...) (...) (...) (...) (...) (...) (...) (...) (...) (...) (...) (...) (...) (...) (...) (...) (...) (...) (...) (...) (...) (...) (... (...) (...) (...) (...) (...) (...) (... (...) (...) (...) (...) (...) (...) (... (...) (...) (...) (...) (...) (...) (...) (...) (...) (...) (...) (...) (...) (...) (...) (...) (...)...) (...) (...) (...) (...) (...)...) (...) (...) (...) (...) (...) (...)...) (...) (...) (...) (...) (...) (...)...) (...) (...) (...) (...) (...) (...) (...) (...) (...) (...) (...) (...) (...) (...) (...) (...) (...) (...) (...) (...) (...) (...) (...) (... (...) (...) (...) (...) (...) (...) (...) (...) (...) (...) (...) (...) (...) (...) (...) (...) (...) (...) (...) (...) (...) (...) (...) (...) (...) (...) (...) ("}, {"heading": "3.3 Convergence of Distributional SGD", "text": "One caveat here is that due to the potential discrepancy between the distribution derivative and the traditional gradient, whether or not the SGD algorithm converges with the distribution derivative generally remains unclear. However, for our hash problem, it is easy to show that the distribution derivative in 3 is actually the true gradient. Proposal 6 The distribution derivative DWLsn (\u0441; x) is equivalent to the traditional gradient. Proof First of all, we have by definition that Lsn (\u0432; x) = L (\u0432; x). It is easy to verify that both DWLsn (\u0432; x) and the WL (\u0432; x) are continuous. Therefore, it is sufficient to show that for each distribution method u, D) and each distribution method u is the same distribution method u."}, {"heading": "4 Connections", "text": "In this section, we will show the connections to several existing algorithms (VQ = 1). (VQ = 1). (VQ = 1). (F = 1). (F = 1). (F = 1). (F = 1). (F = 1). (F = 1). (F = 1). (F = 1). (F = 1). (F = 1). (F = 1). (F = 1). (F = 1). (F = 1). (F = 1). (F = 1). (F = 1). (F = 1). (F = 1). (F = 1). (F = 1). (F = 1). (F = 1). (F = 1. (F = 1). (F = 1. (F = 1). (F = 1). (F = 1. (F = 1). (F). (F = 1. (F = 1). (F = 1.). (F = F). (F = 1. (F = 1). (F = 1. (F = 1). (F = F = 1. (F = 1). (F). (F = F = 1. (F = 1). (F = F = 1. (F = 1). (F = 1). (F = F = 1. (F = 1). (F = 1. (F = 1). (F = F = 1. (F = F = 1.). (F = 1. (F = 1.). (F = 1. F = F = = 1. (F = F = 1.). (F = = = = F = = 1. F = 1. F = 1.). (F = F = 1. (F = F = 1. (F = 1 = 1. F = = = 1. F = 1. F = 1. F = 1. (F =. F = 1. F = 1. F = 1. (F = 1.). (F = 1 = 1 = 1. F =. F = 1. F = 1. F = 1. F =. F = 1. F = 1. F = 1. (F ="}, {"heading": "5 Experiments", "text": "In this section, we first justify the advantages of the proposed SGD in terms of convergence speed and computational costs numerically, and then demonstrate the generative capability of the reduced MRF using the codes of the stochastic coding function. Finally, we evaluate the performance of the proposed stochastic generative hashing (SGH) for both L2NNS and MIPS using multiple benchmarks. We used the following data sets for various tasks in our experiments, including i), MNIST, which consists of 28 x 28 grayscale images of handwritten digits from 0 to 9; ii), CIFAR-10, which consists of 32 x 32 natural color images in 10 categories; iii), SIFT-1M, which contains high-resolution color images represented by 128 SIFT features; iv), WORD2VEC, which consists of 200-dimensional real vectors, each representing a word generated by d2text model microlov-8."}, {"heading": "5.1 Empirical Study of Distributional SGD", "text": "We first demonstrate the convergence of the distribution derivative with Adam (Kingma and Ba, 2014) numerically on MINST and SIFT-1M from 8 bits to 64 bits. Convergence curves are shown in Figure 2 (a) and (c). We record the free Helmholtz energy. Obviously, the proposed algorithm converges quickly, regardless of how many bits we have used. It is reasonable that with more bits, the better model matches the data and the free Helmholtz energy can be further reduced. It should be emphasized that the computing costs in each iteration in the proposed distribution SGD are only O (dl). We compare the training time with the most commonly used binary autoencoder hashing (BA) (CarreiraPerpina'n and Raziperchicolaei, 2015) empirically on the machine with Intel Core i7-3770 CPBA @ 3.40GHz and comparing it with 32GB of empirical memory is more efficient."}, {"heading": "5.2 Generative Ability of reduced-MRFs", "text": "We first illustrated the learned templates U in the reduced MRFs in Figure 3. In our model, the respective binary code leads to a switch that turns the corresponding template on or off during the creation process. Therefore, it can be seen that the templates behave like a filter on both MNIST and CIFAR-10. To further demonstrate the generative ability of the reduced MRFs, we have the images with the learned model with 64 binary codes on MNIST and CIFAR-10. Specifically, the x randomly sampled from the data set is first re-coded into binary representation of the learned q (h | x) bits, and then we generate an artificial sample from p (x | h).We also compare them with PCA and ITQ for recovery."}, {"heading": "5.3 Retrieval Performance Comparison", "text": "It is only a matter of time before that happens, that it happens."}, {"heading": "6 Conclusion", "text": "In this paper, we propose a novel approach to learning the stochastic hash function in the context of the generative model of reduced MRFs. First, from a theoretical point of view, we justify that the proposed algorithm is capable of providing a good hash function that meets all requirements, i.e., to keep both L2 and the internal product environment as short as possible in order to achieve fast recall and fast learning in the meantime. We design experiments to verify our conclusion, and perform comparisons for both L2NNS- and MIPS-tasks to empirically justify the benefit of the proposed algorithm. We emphasize that the proposed generative hashing is a general framework that can be extended to semi-monitored scenarios and other learning scenarios for hash analysis in Appendix A. In addition, the proposed double stochastic neurons and distributing SGDs can be applied to general integral programming problems and may be of independent interest."}, {"heading": "A Generalization", "text": "We generalize the basic model of translation and scale expansion, semi-supervised expansion, semi-limited expansion, as well as encoding with h (1) l.A.1 translation and scale Invariant-Reduced-MRFsAs We know that the data does not mean zero, and the scale of individual samples can be completely different. To eliminate the translation and scale effects, we extend the basic model to translation and scale invariant-reduced-MRFs by separating the translation effect and the latent variance in each sample x, hence the potential function becomesE (x, h, z) = \u2212 \u03b2 > x-MR (z \u00b7 h) > (x \u2212 M > U > (z \u00b7 h) > (z \u00b7 h) > (z \u00b7 h) where \u00b7 the relevant element-wise product, z \u00b7 Rl."}], "references": [{"title": "Provable bayesian inference via particle mirror descent", "author": ["Bo Dai", "Niao He", "Hanjun Dai", "Le Song"], "venue": "In Proceedings of the 19th International Conference on Artificial Intelligence and Statistics,", "citeRegEx": "Dai et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Dai et al\\.", "year": 2016}, {"title": "Stochastic first-and zeroth-order methods for nonconvex stochastic programming", "author": ["Saeed Ghadimi", "Guanghui Lan"], "venue": "SIAM Journal on Optimization,", "citeRegEx": "Ghadimi and Lan.,? \\Q2013\\E", "shortCiteRegEx": "Ghadimi and Lan.", "year": 2013}, {"title": "Similarity search in high dimensions via hashing", "author": ["Aristides Gionis", "Piotr Indyk", "Rajeev Motwani"], "venue": "In VLDB,", "citeRegEx": "Gionis et al\\.,? \\Q1999\\E", "shortCiteRegEx": "Gionis et al\\.", "year": 1999}, {"title": "Iterative quantization: A procrustean approach to learning binary codes", "author": ["Yunchao Gong", "Svetlana Lazebnik"], "venue": "In Computer Vision and Pattern Recognition (CVPR),", "citeRegEx": "Gong and Lazebnik.,? \\Q2011\\E", "shortCiteRegEx": "Gong and Lazebnik.", "year": 2011}, {"title": "Learning binary codes for highdimensional data using bilinear projections", "author": ["Yunchao Gong", "Sanjiv Kumar", "Henry A Rowley", "Svetlana Lazebnik"], "venue": "In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition,", "citeRegEx": "Gong et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Gong et al\\.", "year": 2013}, {"title": "Deep autoregressive networks", "author": ["Karol Gregor", "Ivo Danihelka", "Andriy Mnih", "Charles Blundell", "Daan Wierstra"], "venue": "In Proceedings of The 31st International Conference on Machine Learning,", "citeRegEx": "Gregor et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Gregor et al\\.", "year": 2014}, {"title": "Distributions and operators, volume 252", "author": ["Gerd Grubb"], "venue": "Springer Science & Business Media,", "citeRegEx": "Grubb.,? \\Q2008\\E", "shortCiteRegEx": "Grubb.", "year": 2008}, {"title": "Muprop: Unbiased backpropagation for stochastic neural networks", "author": ["Shixiang Gu", "Sergey Levine", "Ilya Sutskever", "Andriy Mnih"], "venue": "arXiv preprint arXiv:1511.05176,", "citeRegEx": "Gu et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Gu et al\\.", "year": 2015}, {"title": "Quantization based fast inner product search", "author": ["Ruiqi Guo", "Sanjiv Kumar", "Krzysztof Choromanski", "David Simcha"], "venue": "arXiv preprint arXiv:1509.01469,", "citeRegEx": "Guo et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Guo et al\\.", "year": 2015}, {"title": "K-means hashing: An affinity-preserving quantization method for learning binary compact codes", "author": ["Kaiming He", "Fang Wen", "Jian Sun"], "venue": "In Proceedings of the IEEE conference on computer vision and pattern recognition,", "citeRegEx": "He et al\\.,? \\Q2013\\E", "shortCiteRegEx": "He et al\\.", "year": 2013}, {"title": "Spherical hashing", "author": ["Jae-Pil Heo", "Youngwoon Lee", "Junfeng He", "Shih-Fu Chang", "Sung-Eui Yoon"], "venue": "In Computer Vision and Pattern Recognition (CVPR),", "citeRegEx": "Heo et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Heo et al\\.", "year": 2012}, {"title": "Training products of experts by minimizing contrastive divergence", "author": ["Geoffrey E Hinton"], "venue": "Neural computation,", "citeRegEx": "Hinton.,? \\Q2002\\E", "shortCiteRegEx": "Hinton.", "year": 2002}, {"title": "Keeping the neural networks simple by minimizing the description length of the weights", "author": ["Geoffrey E Hinton", "Drew Van Camp"], "venue": "In Proceedings of the sixth annual conference on Computational learning theory,", "citeRegEx": "Hinton and Camp.,? \\Q1993\\E", "shortCiteRegEx": "Hinton and Camp.", "year": 1993}, {"title": "Autoencoders, minimum description length and helmholtz free energy", "author": ["Geoffrey E Hinton", "Richard S Zemel"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Hinton and Zemel.,? \\Q1994\\E", "shortCiteRegEx": "Hinton and Zemel.", "year": 1994}, {"title": "Online hashing. In Proceedings of the Twenty-Third international joint conference on Artificial Intelligence, pages 1422\u20131428", "author": ["Long-Kai Huang", "Qiang Yang", "Wei-Shi Zheng"], "venue": null, "citeRegEx": "Huang et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Huang et al\\.", "year": 2013}, {"title": "Approximate nearest neighbors: towards removing the curse of dimensionality", "author": ["Piotr Indyk", "Rajeev Motwani"], "venue": "In Proceedings of the thirtieth annual ACM symposium on Theory of computing,", "citeRegEx": "Indyk and Motwani.,? \\Q1998\\E", "shortCiteRegEx": "Indyk and Motwani.", "year": 1998}, {"title": "Aggregating local descriptors into a compact image representation", "author": ["Herv\u00e9 J\u00e9gou", "Matthijs Douze", "Cordelia Schmid", "Patrick P\u00e9rez"], "venue": "In Computer Vision and Pattern Recognition (CVPR),", "citeRegEx": "J\u00e9gou et al\\.,? \\Q2010\\E", "shortCiteRegEx": "J\u00e9gou et al\\.", "year": 2010}, {"title": "Product quantization for nearest neighbor search", "author": ["Herve Jegou", "Matthijs Douze", "Cordelia Schmid"], "venue": "IEEE transactions on pattern analysis and machine intelligence,", "citeRegEx": "Jegou et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Jegou et al\\.", "year": 2011}, {"title": "Adam: A method for stochastic optimization", "author": ["Diederik Kingma", "Jimmy Ba"], "venue": "arXiv preprint arXiv:1412.6980,", "citeRegEx": "Kingma and Ba.,? \\Q2014\\E", "shortCiteRegEx": "Kingma and Ba.", "year": 2014}, {"title": "Auto-encoding variational bayes", "author": ["Diederik P Kingma", "Max Welling"], "venue": "arXiv preprint arXiv:1312.6114,", "citeRegEx": "Kingma and Welling.,? \\Q2013\\E", "shortCiteRegEx": "Kingma and Welling.", "year": 2013}, {"title": "Learning multiple layers of features from tiny images", "author": ["Alex Krizhevsky"], "venue": null, "citeRegEx": "Krizhevsky.,? \\Q2009\\E", "shortCiteRegEx": "Krizhevsky.", "year": 2009}, {"title": "Online sketching hashing", "author": ["Cong Leng", "Jiaxiang Wu", "Jian Cheng", "Xiao Bai", "Hanqing Lu"], "venue": "In 2015 IEEE Conference on Computer Vision and Pattern Recognition (CVPR),", "citeRegEx": "Leng et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Leng et al\\.", "year": 2015}, {"title": "Hashing with graphs", "author": ["Wei Liu", "Jun Wang", "Sanjiv Kumar", "Shih-Fu Chang"], "venue": "In Proceedings of the 28th international conference on machine learning", "citeRegEx": "Liu et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Liu et al\\.", "year": 2011}, {"title": "Modeling pixel means and covariances using factorized thirdorder boltzmann machines", "author": ["Ranzato Marc\u2019Aurelio", "E Hinton Geoffrey"], "venue": "In Computer Vision and Pattern Recognition (CVPR),", "citeRegEx": "Marc.Aurelio and Geoffrey.,? \\Q2010\\E", "shortCiteRegEx": "Marc.Aurelio and Geoffrey.", "year": 2010}, {"title": "Efficient estimation of word representations in vector space", "author": ["Tomas Mikolov", "Kai Chen", "Greg Corrado", "Jeffrey Dean"], "venue": "arXiv preprint arXiv:1301.3781,", "citeRegEx": "Mikolov et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Mikolov et al\\.", "year": 2013}, {"title": "Neural variational inference and learning in belief networks", "author": ["Andriy Mnih", "Karol Gregor"], "venue": "arXiv preprint arXiv:1402.0030,", "citeRegEx": "Mnih and Gregor.,? \\Q2014\\E", "shortCiteRegEx": "Mnih and Gregor.", "year": 2014}, {"title": "An nmf perspective on binary hashing", "author": ["Lopamudra Mukherjee", "Sathya N Ravi", "Vamsi K Ithapu", "Tyler Holmes", "Vikas Singh"], "venue": "In Proceedings of the IEEE International Conference on Computer Vision,", "citeRegEx": "Mukherjee et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Mukherjee et al\\.", "year": 2015}, {"title": "Robust stochastic approximation approach to stochastic programming", "author": ["Arkadi Nemirovski", "Anatoli Juditsky", "Guanghui Lan", "Alexander Shapiro"], "venue": "SIAM Journal on optimization,", "citeRegEx": "Nemirovski et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Nemirovski et al\\.", "year": 2009}, {"title": "Techniques for learning binary stochastic feedforward neural networks", "author": ["Tapani Raiko", "Mathias Berglund", "Guillaume Alain", "Laurent Dinh"], "venue": "arXiv preprint arXiv:1406.2989,", "citeRegEx": "Raiko et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Raiko et al\\.", "year": 2014}, {"title": "Learning binary codes for maximum inner product search", "author": ["Fumin Shen", "Wei Liu", "Shaoting Zhang", "Yang Yang", "Heng Tao Shen"], "venue": "In 2015 IEEE International Conference on Computer Vision (ICCV),", "citeRegEx": "Shen et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Shen et al\\.", "year": 2015}, {"title": "Video google: A text retrieval approach to object matching in videos", "author": ["Josef Sivic", "Andrew Zisserman"], "venue": "In Computer Vision,", "citeRegEx": "Sivic and Zisserman.,? \\Q2003\\E", "shortCiteRegEx": "Sivic and Zisserman.", "year": 2003}, {"title": "Small codes and large image databases for recognition", "author": ["Antonio Torralba", "Rob Fergus", "Yair Weiss"], "venue": "In Computer Vision and Pattern Recognition,", "citeRegEx": "Torralba et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Torralba et al\\.", "year": 2008}, {"title": "Hashing for similarity search: A survey", "author": ["Jingdong Wang", "Heng Tao Shen", "Jingkuan Song", "Jianqiu Ji"], "venue": "arXiv preprint arXiv:1408.2927,", "citeRegEx": "Wang et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Wang et al\\.", "year": 2014}, {"title": "Spectral hashing", "author": ["Yair Weiss", "Antonio Torralba", "Rob Fergus"], "venue": "In Advances in neural information processing systems,", "citeRegEx": "Weiss et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Weiss et al\\.", "year": 2009}, {"title": "Bayesian conditionalisation and the principle of minimum information", "author": ["P.M. Williams"], "venue": "British Journal for the Philosophy of Science,", "citeRegEx": "Williams.,? \\Q1980\\E", "shortCiteRegEx": "Williams.", "year": 1980}, {"title": "Circulant binary embedding", "author": ["Felix X Yu", "Sanjiv Kumar", "Yunchao Gong", "Shih-Fu Chang"], "venue": "In International conference on machine learning,", "citeRegEx": "Yu et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Yu et al\\.", "year": 2014}, {"title": "Optimal Information Processing and Bayes\u2019s Theorem", "author": ["Arnold Zellner"], "venue": "The American Statistician,", "citeRegEx": "Zellner.,? \\Q1988\\E", "shortCiteRegEx": "Zellner.", "year": 1988}, {"title": "Supervised hashing with latent factor models", "author": ["Peichao Zhang", "Wei Zhang", "Wu-Jun Li", "Minyi Guo"], "venue": "In Proceedings of the 37th international ACM SIGIR conference on Research & development in information retrieval,", "citeRegEx": "Zhang et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Zhang et al\\.", "year": 2014}, {"title": "Composite quantization for approximate nearest neighbor search", "author": ["Ting Zhang", "Chao Du", "Jingdong Wang"], "venue": "In Proceedings of the 31st International Conference on Machine Learning", "citeRegEx": "Zhang et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Zhang et al\\.", "year": 2014}, {"title": "Deep hashing network for efficient similarity retrieval", "author": ["Han Zhu", "Mingsheng Long", "Jianmin Wang", "Yue Cao"], "venue": "In Thirtieth AAAI Conference on Artificial Intelligence,", "citeRegEx": "Zhu et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Zhu et al\\.", "year": 2016}, {"title": "2016). Specifically, we are provided the (partial) supervision information for some pairs of data", "author": ["2014a", "Zhu"], "venue": null, "citeRegEx": "2014a and Zhu,? \\Q2016\\E", "shortCiteRegEx": "2014a and Zhu", "year": 2016}], "referenceMentions": [{"referenceID": 15, "context": "Using hashing for similarity searching has been popularized by influential works such as Locality Sensitive Hashing (Indyk and Motwani, 1998; Gionis et al., 1999), and has been applied towards computer vision tasks such as large scale image retrieval (Torralba et al.", "startOffset": 116, "endOffset": 162}, {"referenceID": 2, "context": "Using hashing for similarity searching has been popularized by influential works such as Locality Sensitive Hashing (Indyk and Motwani, 1998; Gionis et al., 1999), and has been applied towards computer vision tasks such as large scale image retrieval (Torralba et al.", "startOffset": 116, "endOffset": 162}, {"referenceID": 31, "context": ", 1999), and has been applied towards computer vision tasks such as large scale image retrieval (Torralba et al., 2008; J\u00e9gou et al., 2010) and video retrieval (Sivic and Zisserman, 2003).", "startOffset": 96, "endOffset": 139}, {"referenceID": 16, "context": ", 1999), and has been applied towards computer vision tasks such as large scale image retrieval (Torralba et al., 2008; J\u00e9gou et al., 2010) and video retrieval (Sivic and Zisserman, 2003).", "startOffset": 96, "endOffset": 139}, {"referenceID": 30, "context": ", 2010) and video retrieval (Sivic and Zisserman, 2003).", "startOffset": 28, "endOffset": 55}, {"referenceID": 32, "context": "It is well recognized that the data-dependent hash functions perform better (Wang et al., 2014).", "startOffset": 76, "endOffset": 95}, {"referenceID": 3, "context": "Many works have since then focused on learning hash functions or the binary codes for either L2NNS or MIPS, including iterative quantization (Gong and Lazebnik, 2011), spectral hashing (Weiss et al.", "startOffset": 141, "endOffset": 166}, {"referenceID": 33, "context": "Many works have since then focused on learning hash functions or the binary codes for either L2NNS or MIPS, including iterative quantization (Gong and Lazebnik, 2011), spectral hashing (Weiss et al., 2009), product quantization (Jegou et al.", "startOffset": 185, "endOffset": 205}, {"referenceID": 17, "context": ", 2009), product quantization (Jegou et al., 2011), binary autoencoder (Carreira-Perpin\u00e1n and Raziperchikolaei, 2015), and many others (Liu et al.", "startOffset": 30, "endOffset": 50}, {"referenceID": 22, "context": ", 2011), binary autoencoder (Carreira-Perpin\u00e1n and Raziperchikolaei, 2015), and many others (Liu et al., 2011; Gong et al., 2013; Yu et al., 2014; Shen et al., 2015; Guo et al., 2015).", "startOffset": 92, "endOffset": 183}, {"referenceID": 4, "context": ", 2011), binary autoencoder (Carreira-Perpin\u00e1n and Raziperchikolaei, 2015), and many others (Liu et al., 2011; Gong et al., 2013; Yu et al., 2014; Shen et al., 2015; Guo et al., 2015).", "startOffset": 92, "endOffset": 183}, {"referenceID": 35, "context": ", 2011), binary autoencoder (Carreira-Perpin\u00e1n and Raziperchikolaei, 2015), and many others (Liu et al., 2011; Gong et al., 2013; Yu et al., 2014; Shen et al., 2015; Guo et al., 2015).", "startOffset": 92, "endOffset": 183}, {"referenceID": 29, "context": ", 2011), binary autoencoder (Carreira-Perpin\u00e1n and Raziperchikolaei, 2015), and many others (Liu et al., 2011; Gong et al., 2013; Yu et al., 2014; Shen et al., 2015; Guo et al., 2015).", "startOffset": 92, "endOffset": 183}, {"referenceID": 8, "context": ", 2011), binary autoencoder (Carreira-Perpin\u00e1n and Raziperchikolaei, 2015), and many others (Liu et al., 2011; Gong et al., 2013; Yu et al., 2014; Shen et al., 2015; Guo et al., 2015).", "startOffset": 92, "endOffset": 183}, {"referenceID": 35, "context": "For example, iterative quantization and its variants (Yu et al., 2014; Gong et al., 2013) introduce structure and orthogonality constraints on the parameters, and the vector quantization and its generalization, e.", "startOffset": 53, "endOffset": 89}, {"referenceID": 4, "context": "For example, iterative quantization and its variants (Yu et al., 2014; Gong et al., 2013) introduce structure and orthogonality constraints on the parameters, and the vector quantization and its generalization, e.", "startOffset": 53, "endOffset": 89}, {"referenceID": 17, "context": ", product quantization (Jegou et al., 2011) and composite quantization (Zhang et al.", "startOffset": 23, "endOffset": 43}, {"referenceID": 33, "context": ", the graph-based hashing (Weiss et al., 2009; Liu et al., 2011), or use penalty or augmented Lagrangian method to separate the binary variables, e.", "startOffset": 26, "endOffset": 64}, {"referenceID": 22, "context": ", the graph-based hashing (Weiss et al., 2009; Liu et al., 2011), or use penalty or augmented Lagrangian method to separate the binary variables, e.", "startOffset": 26, "endOffset": 64}, {"referenceID": 26, "context": ", NMF-based hashing (Mukherjee et al., 2015), binary autoencoder (Carreira-Perpin\u00e1n and Raziperchikolaei, 2015).", "startOffset": 20, "endOffset": 44}, {"referenceID": 32, "context": "1 Main Contribution As demonstrated in previous literatures such as (Wang et al., 2014), a good hash function learned from data should generate binary codes which i) preserve the neighborhood in original dataset; while ii) as short as possible.", "startOffset": 68, "endOffset": 87}, {"referenceID": 27, "context": "Then, we extend the stochastic gradient descent algorithm (Nemirovski et al., 2009) with distributional derivative (Grubb, 2008) to deal with the discontinuity in the reparametrization.", "startOffset": 58, "endOffset": 83}, {"referenceID": 6, "context": ", 2009) with distributional derivative (Grubb, 2008) to deal with the discontinuity in the reparametrization.", "startOffset": 39, "endOffset": 52}, {"referenceID": 1, "context": "Equipped with these new tools, we can directly target on the optimization containing discrete variables without relaxation and still achieve convergence in both static or online setting (Ghadimi and Lan, 2013).", "startOffset": 186, "endOffset": 209}, {"referenceID": 3, "context": ", vector quantization, iterative quantization (Gong and Lazebnik, 2011), product quantization (Jegou et al.", "startOffset": 46, "endOffset": 71}, {"referenceID": 17, "context": ", vector quantization, iterative quantization (Gong and Lazebnik, 2011), product quantization (Jegou et al., 2011), and composition quantization (Zhang et al.", "startOffset": 94, "endOffset": 114}, {"referenceID": 6, "context": "1 Reduced Gaussian-Markov Random Fields We first propose the basic model, in which the generating process is characterized by the joint distribution for x \u2208 R, h \u2208 {0, 1}, 2The distributional derivative is defined in (Grubb, 2008) and will be introduced in section 3.", "startOffset": 217, "endOffset": 230}, {"referenceID": 20, "context": "Even with such shared structure in the architecture, this model is still more flexible than Gaussian-Bernoulli restricted Boltzmann machines (GRBMs) (Krizhevsky, 2009; Marc\u2019Aurelio and Geoffrey, 2010).", "startOffset": 149, "endOffset": 200}, {"referenceID": 23, "context": "Even with such shared structure in the architecture, this model is still more flexible than Gaussian-Bernoulli restricted Boltzmann machines (GRBMs) (Krizhevsky, 2009; Marc\u2019Aurelio and Geoffrey, 2010).", "startOffset": 149, "endOffset": 200}, {"referenceID": 11, "context": "Directly applying the off-the-shelf Contrastive Divergence (CD) algorithm (Hinton, 2002) will result unacceptable computation cost.", "startOffset": 74, "endOffset": 88}, {"referenceID": 19, "context": "We provide a solution bypassing these difficulties inspired by recent work variational auto-encoder (Kingma and Welling, 2013; Mnih and Gregor, 2014; Gregor et al., 2014),.", "startOffset": 100, "endOffset": 170}, {"referenceID": 25, "context": "We provide a solution bypassing these difficulties inspired by recent work variational auto-encoder (Kingma and Welling, 2013; Mnih and Gregor, 2014; Gregor et al., 2014),.", "startOffset": 100, "endOffset": 170}, {"referenceID": 5, "context": "We provide a solution bypassing these difficulties inspired by recent work variational auto-encoder (Kingma and Welling, 2013; Mnih and Gregor, 2014; Gregor et al., 2014),.", "startOffset": 100, "endOffset": 170}, {"referenceID": 34, "context": "The key insight is that when fixing the model p(x, h), the posterior is the solution of an optimization maximizing the negative Helmholtz variational free energy (Williams, 1980; Zellner, 1988; Dai et al., 2016), i.", "startOffset": 162, "endOffset": 211}, {"referenceID": 36, "context": "The key insight is that when fixing the model p(x, h), the posterior is the solution of an optimization maximizing the negative Helmholtz variational free energy (Williams, 1980; Zellner, 1988; Dai et al., 2016), i.", "startOffset": 162, "endOffset": 211}, {"referenceID": 0, "context": "The key insight is that when fixing the model p(x, h), the posterior is the solution of an optimization maximizing the negative Helmholtz variational free energy (Williams, 1980; Zellner, 1988; Dai et al., 2016), i.", "startOffset": 162, "endOffset": 211}, {"referenceID": 13, "context": "Interestingly, the Helmholtz variational free energy has close relationship to minimum description length (MDL) principle derived from bits-back coding (Hinton and Van Camp, 1993; Hinton and Zemel, 1994; Gregor et al., 2014).", "startOffset": 152, "endOffset": 224}, {"referenceID": 5, "context": "Interestingly, the Helmholtz variational free energy has close relationship to minimum description length (MDL) principle derived from bits-back coding (Hinton and Van Camp, 1993; Hinton and Zemel, 1994; Gregor et al., 2014).", "startOffset": 152, "endOffset": 224}, {"referenceID": 19, "context": "However, the existing algorithms using either reparametrization trick (Kingma and Welling, 2013) or REINFORCE trick (Mnih and Gregor, 2014) can not be applied straightforwardly to the proposed model.", "startOffset": 70, "endOffset": 96}, {"referenceID": 25, "context": "However, the existing algorithms using either reparametrization trick (Kingma and Welling, 2013) or REINFORCE trick (Mnih and Gregor, 2014) can not be applied straightforwardly to the proposed model.", "startOffset": 116, "endOffset": 139}, {"referenceID": 7, "context": "so it converges slowly or might not converge (Bengio et al., 2013; Gu et al., 2015).", "startOffset": 45, "endOffset": 83}, {"referenceID": 28, "context": "To remedy these issues, heuristic \u201cpseudo-gradient\u201d estimators based on reparametrization trick (Bengio et al., 2013; Raiko et al., 2014) and variational reduced gradient estimator based on REINFORCE trick (Gregor et al.", "startOffset": 96, "endOffset": 137}, {"referenceID": 5, "context": ", 2014) and variational reduced gradient estimator based on REINFORCE trick (Gregor et al., 2014; Gu et al., 2015) have been proposed.", "startOffset": 76, "endOffset": 114}, {"referenceID": 7, "context": ", 2014) and variational reduced gradient estimator based on REINFORCE trick (Gregor et al., 2014; Gu et al., 2015) have been proposed.", "startOffset": 76, "endOffset": 114}, {"referenceID": 6, "context": "a distributions) (Grubb, 2008).", "startOffset": 17, "endOffset": 30}, {"referenceID": 6, "context": "Definition 3 (Grubb, 2008) Let u \u2208 D\u2032(\u03a9), then a distribution v is called to be the distributional derivative of u, denoted as v = Du, if it satisfies \u222b", "startOffset": 13, "endOffset": 26}, {"referenceID": 6, "context": "Indeed, the chain rule of distributional derivative is also valid (Grubb, 2008).", "startOffset": 66, "endOffset": 79}, {"referenceID": 6, "context": "Lemma 4 (Grubb, 2008) Let u \u2208 D\u2032(\u03a9), we have 1.", "startOffset": 8, "endOffset": 21}, {"referenceID": 6, "context": "Proof Let z = \u03c3(Wx), we have DzE ,\u03be [`(f(z, , \u03be))] limit theorem (Grubb, 2008) = E\u03be, [Dz`(f(z, , \u03be))] chain rule II = E\u03be, [\u2207f `(f(z, , \u03be))Dzf(z, , \u03be)] chain rule I = E\u03be, [\u2207f `(f(z, , \u03be))\u03b4 (z)] property of \u03b4 function = E\u03be [\u2207f `(f(z, z, \u03be))] , Hence, DWLsn(\u0398, x) = E\u03be [ \u2207f `(f(\u03c3(Wx), \u03c3(Wx), \u03be)) ] \u2207\u03c3(W>x)x>.", "startOffset": 65, "endOffset": 78}, {"referenceID": 28, "context": "Interestingly, the natural stochastic estimator of the distributional derivative we established through doubly stochastic neuron coincides with the heuristic \u201cpsudo-gradient\u201d constructed for s1(z, ) and s0(z, ) in (Raiko et al., 2014).", "startOffset": 214, "endOffset": 234}, {"referenceID": 28, "context": "While the authors in the original paper (Raiko et al., 2014) claimed that this is a biased estimator with low variance, our new analysis reveals that such estimator is indeed unbaised and meaningful.", "startOffset": 40, "endOffset": 60}, {"referenceID": 27, "context": ", (Nemirovski et al., 2009) and its variants (Kingma and Ba, 2014; Bottou et al.", "startOffset": 2, "endOffset": 27}, {"referenceID": 18, "context": ", 2009) and its variants (Kingma and Ba, 2014; Bottou et al., 2016)), which we designate as Distributional SGD.", "startOffset": 25, "endOffset": 67}, {"referenceID": 14, "context": ", (Carreira-Perpin\u00e1n and Raziperchikolaei, 2015), the proposed distributional SGD 1 is much simpler and also amenable to on-line setting (Huang et al., 2013; Leng et al., 2015).", "startOffset": 137, "endOffset": 176}, {"referenceID": 21, "context": ", (Carreira-Perpin\u00e1n and Raziperchikolaei, 2015), the proposed distributional SGD 1 is much simpler and also amenable to on-line setting (Huang et al., 2013; Leng et al., 2015).", "startOffset": 137, "endOffset": 176}, {"referenceID": 6, "context": "2 in (Grubb, 2008)), we have Du = \u2207u.", "startOffset": 5, "endOffset": 18}, {"referenceID": 1, "context": "1 in (Ghadimi and Lan, 2013), we arrive at Theorem 7 Under some mild conditions, the proposed distributional SGD converges to stationary points, i.", "startOffset": 5, "endOffset": 28}, {"referenceID": 5, "context": ", (Gregor et al., 2014; Gu et al., 2015), saving unnecessary computational and memory cost.", "startOffset": 2, "endOffset": 40}, {"referenceID": 7, "context": ", (Gregor et al., 2014; Gu et al., 2015), saving unnecessary computational and memory cost.", "startOffset": 2, "endOffset": 40}, {"referenceID": 17, "context": "which is exactly the objective of product quantization (Jegou et al., 2011).", "startOffset": 55, "endOffset": 75}, {"referenceID": 3, "context": "which is exactly the objective of iterative quantization (Gong and Lazebnik, 2011).", "startOffset": 57, "endOffset": 82}, {"referenceID": 24, "context": "We used the following datasets for different tasks in our experiments, including i), MNIST, which consists of 28 \u00d7 28 grayscale images of handwritten digits from 0 to 9; ii), CIFAR-10, which consists of 32 \u00d7 32 color natural images in 10 categories; iii), SIFT-1M, which contains with high-resolution color images represented by 128 SIFT features; iv), WORD2VEC, which consists of 200-dimensional real-valued vectors, each one representing one word generated by word2vec model (Mikolov et al., 2013) learned from text-8 corpus.", "startOffset": 477, "endOffset": 499}, {"referenceID": 18, "context": "1 Empirical Study of Distributional SGD We first demonstrate the convergence of the distributional derivative with Adam (Kingma and Ba, 2014) numerically on MINST and SIFT-1M from 8 bits to 64 bits.", "startOffset": 120, "endOffset": 141}, {"referenceID": 9, "context": "3 Retrieval Performance Comparison We compared the stochastic generative hashing on both L2NNS and MIPS tasks with several state-ofthe-art unsupervised algorithms, including K-means hashing (KMH) (He et al., 2013), iterative quantization (ITQ) (Gong and Lazebnik, 2011), spectral hashing (SH) (Weiss et al.", "startOffset": 196, "endOffset": 213}, {"referenceID": 3, "context": ", 2013), iterative quantization (ITQ) (Gong and Lazebnik, 2011), spectral hashing (SH) (Weiss et al.", "startOffset": 38, "endOffset": 63}, {"referenceID": 33, "context": ", 2013), iterative quantization (ITQ) (Gong and Lazebnik, 2011), spectral hashing (SH) (Weiss et al., 2009), spherical hashing (SpH) (Heo et al.", "startOffset": 87, "endOffset": 107}, {"referenceID": 10, "context": ", 2009), spherical hashing (SpH) (Heo et al., 2012), and binary autoencoder (BA) (Carreira-Perpin\u00e1n and Raziperchikolaei, 2015).", "startOffset": 33, "endOffset": 51}, {"referenceID": 9, "context": "We conducted the searching via the commonly used Hamming ranking follows (He et al., 2013; Gong and Lazebnik, 2011; Weiss et al., 2009).", "startOffset": 73, "endOffset": 135}, {"referenceID": 3, "context": "We conducted the searching via the commonly used Hamming ranking follows (He et al., 2013; Gong and Lazebnik, 2011; Weiss et al., 2009).", "startOffset": 73, "endOffset": 135}, {"referenceID": 33, "context": "We conducted the searching via the commonly used Hamming ranking follows (He et al., 2013; Gong and Lazebnik, 2011; Weiss et al., 2009).", "startOffset": 73, "endOffset": 135}], "year": 2017, "abstractText": "Learning to hash plays a fundamentally important role in the efficient image and video retrieval and many other computer vision problems. However, due to the binary outputs of the hash functions, the learning of hash functions is very challenging. In this paper, we propose a novel approach to learn stochastic hash functions such that the learned hashing codes can be used to regenerate the inputs. We develop an efficient stochastic gradient learning algorithm which can avoid the notorious difficulty caused by binary output constraint, and directly optimize the parameters of the hash functions and the associated generative model jointly. The proposed method can be applied to both L2 approximate nearest neighbor search (L2NNS) and maximum inner product search (MIPS). Extensive experiments on a variety of large-scale datasets show that the proposed method achieves significantly better retrieval results than previous state-of-the-arts.", "creator": "LaTeX with hyperref package"}}}