{"id": "1701.02854", "review": {"conference": "acl", "VERSION": "v1", "DATE_OF_SUBMISSION": "11-Jan-2017", "title": "Towards Decoding as Continuous Optimization in Neural Machine Translation", "abstract": "In this work, we propose a novel decoding approach for neural machine translation (NMT) based on continuous optimisation. The resulting optimisation problem can then be tackled using a whole range of continuous optimisation algorithms which have been developed and used in the literature mainly for training. Our approach is general and can be applied to other sequence-to-sequence neural models as well. We make use of this powerful decoding approach to intersect an underlying NMT with a language model, to intersect left-to-right and right-to-left NMT models, and to decode with soft constraints involving coverage and fertility of the source sentence words. The experimental results show the promise of the proposed framework.", "histories": [["v1", "Wed, 11 Jan 2017 06:02:44 GMT  (30kb)", "https://arxiv.org/abs/1701.02854v1", "v1 with preliminary results"], ["v2", "Wed, 8 Feb 2017 00:15:08 GMT  (70kb,D)", "http://arxiv.org/abs/1701.02854v2", "v2 with more results"], ["v3", "Thu, 9 Feb 2017 09:26:30 GMT  (172kb,D)", "http://arxiv.org/abs/1701.02854v3", "v2a with supplementary material"], ["v4", "Sat, 22 Jul 2017 16:35:43 GMT  (98kb,D)", "http://arxiv.org/abs/1701.02854v4", "EMNLP 2017 Camera Ready Paper"]], "COMMENTS": "v1 with preliminary results", "reviews": [], "SUBJECTS": "cs.CL cs.AI", "authors": ["cong duy vu hoang", "gholamreza haffari", "trevor cohn"], "accepted": true, "id": "1701.02854"}, "pdf": {"name": "1701.02854.pdf", "metadata": {"source": "CRF", "title": "Towards Decoding as Continuous Optimisation in Neural Machine Translation", "authors": ["Cong Duy Vu Hoang", "Gholamreza Haffari"], "emails": ["vhoang2@student.unimelb.edu.au", "gholamreza.haffari@monash.edu", "t.cohn@unimelb.edu.au"], "sections": [{"heading": "1 Introduction", "text": "The sequence of neural network sequence learning (Graves, 2013; Sutskever et al., 2014; Lipton et al., 2015) is typically associated with two phases: training and decoding (a.k.a. inference). Model parameters are learned by optimizing the educational goal, so that the model is well generalized when the unknown test data is decoded. A majority of literature has focused on developing better training paradigms or network architectures, but the decoding problem is unlikely to be investigated. Conventional heuristic approaches to approximate inferences include greedy, beam, and stochastic search. Greedy and beam searching have been empirically proven to be appropriate for many sequence tasks, and are the standard methods of decoding process in NMT. However, these approximate inference approaches have several drawbacks."}, {"heading": "2 Neural Machine Translation", "text": "In neural machine translation (NMT) (1), the probability of the target sentence is y (f, y < i, x), where f is a non-linear function of the previously generated word sequence y < i, x), x is \"softmax\" (f, y < i, x), where f is a non-linear function of the previously generated word sequence y < i, the source sentence x, and the model parameters. In this paper, we realize f as follows: f (n, y < i, x) = Where \u00b7 MLP (ci, E yi \u2212 1 T, gi = RNN)."}, {"heading": "3 Decoding as Continuous Optimisation", "text": "The best translation for a given source set solves the following optimization problem: \"We are interested in finding the most likely translation for a given source set.\" The best translation for a given source set solves the following optimization problem: \"The best translation for a given source set solves the following optimization problem: y.\" The best translation for a given source set solves the following optimization problem: y = arg min y1, y = 1. \"We assume that the maximum length of a possible translation for a source set is known and denote. i\" The best translation for a given source set solves the following optimization problem: y = arg min y1. \""}, {"heading": "3.1 Exponentiated Gradient (EG)", "text": "The exponential gradient (Kivinen and Warmuth, 1997) is an elegant algorithm for solving optimization problems with simple limitations. Remember our limited optimization problem: arg min y-1,..., y-Q (y-1,..., y-log softmax (f, y-roplt; i, x). (6) EG is an iterative algorithm that updates each distribution y-ti in the current time step t based on the distributions of the previous time step as follows: \"w-VT: y-log softmax (w) = 1 Zti y-t \u2212 1i (w) exp \u2212 roply \u2212 ropy-enti.\""}, {"heading": "3.2 Stochastic Gradient Descent (SGD)", "text": "In order to apply SGD to our optimization problem, we must ensure that the simplex constraints remain intact. One way to achieve this is to change the optimization variables from y-i to r-i through the Softmax transformation, i.e. y-i = Softmax (r-i), and the resulting unrestricted optimization problem is then derived from the Esoftmax-i = 1 Softmax (r-i) \u00b7 log Softmax (f-i, y-i < i, x)), where EyiT is replaced by the expected embedding of the target words under the distribution resulting from the Esoftmax (r-i) [E w-T] in the model. To apply SGD updates, we need the gradient of the objective function in relation to the new variables r-i, which can be derived from the chain rule backpropagation algorithm:"}, {"heading": "4 Decoding in Extended NMT", "text": "Our decoding method allows us to add additional global factors effectively and flexibly to avoid errors in transmission, allowing us to decode for more accurate global models for which there are no effective means of decoding or finding new targets. We outline several such models and their corresponding relaxed objective functions for optimizing based decoding. A compelling alternative would be the condition for each target word in its right context, i.e., generating the target word from right to left. We do not expect a right to left model to perform a left to right model, but as the left to right order reflects the natural temporal order of the spoken language."}, {"heading": "5 Experiments", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "5.1 Setup", "text": "We conducted our experiments on datasets with different scales, which are translated between Chinese \u2192 English with the BTEC corpus and German \u2192 English with the IWSLT 2015 TED Talks corpus (Cettolo et al., 2014) and WMT20162 corpus. Statistics of the datasets can be found in Table 1.NMT models. We implemented our continuous optimization decoding method on the top of the Mantidae toolkit3 (Cohn et al., 2016) and with the Dynet deep learning library4 (Neubig et al., 2017). All neural network models were configured with 512 input embedding and hidden layer dimensions, and 256 alignment dimensions, with 1 and 2 hidden layers in the source and the target, respectively. We used an LSTM data structure (target sequences and high-end quantum for both 1997 and RN)."}, {"heading": "5.2 Results and Analysis", "text": "In fact, it is so that it is about a way and a way, in which it is about a way and a way, in which it is about the question, in which it is about a way and a way, in which it is about a way and a way, in which it is about a way and a way, in which it is about a way and a way, in which it is about a way and a way, in which it is about a way and a way, in which it is about a way and a way, in which it is about a way and a way, in which it is about a way and a way, in which it is about a way and a way, in which it is about a way and a way, in which it is about a way and a way, in which it is about a way and a way, in which it is about a way and a way, in which it is about a way and a way, in which it is about a way and a way, in which it is about a way and a way in which it is about a way and a way in which it is about a way and a way in which it is about a way and a way in which it is about a way and a way in which it is about a way and a way in which it is about a way and a way in which it is about a way and a way in which it is about a way and a way in which it is about a way and a way in which it is about a way and a way in which it is about a way and a way in which it is about a way and a way in which it is about a way and a way in which it is about a way and a way in which it is about a way and a way in which it is about a way and a way in which it is about a way and a way in which it is about a way and a way in which it is about a way and a way in which it is about a way and a way in which it is about a way and a way and a way it is about a way and a way in which it is about a way and a way in which it is about a way and a way in which it is about a way and a way in which it is about a way and a way and a way in which it is about a way and a way and a way in which it is about a way in which it is about"}, {"heading": "6 Related Work", "text": "The most common inference methods include sampling (Cho, 2016), greedy and beam search (Sutskever et al., 2014; Bahdanau et al., 2015, among others) and reranking (Birch, 2016; Li and Jurafsky, 2016). Cho (2016) suggested disrupting the neural model by injecting noise (s) into the hidden transition function of the conditional recursive neural language model during greedy or officious search."}, {"heading": "7 Conclusions", "text": "This work represents the first attempt to formulate decoding in NMT as a continuous optimization problem, the basic idea being to remove integrity constraints (i.e. a hot vector) from the prediction variables and allow them soft assignments within the probability simplex while minimizing the loss function of the neural model. We have provided two optimization algorithms - exponentiated gradient (EC) and stochas-tic gradient descent (SGD) - to optimize the resulting optimization problem, with our results showing the effectiveness of EC compared to SGD. Thanks to our framework, we have been able to decode NMT models when cutting left to right and right to left, as well as source to target and Targetto source, and our results show that our decoding framework is effective and will result in significant improvements in the translation models that result from the typical search possibilities that are generated in multiple algorithms."}, {"heading": "Acknowledgments", "text": "Cong Duy Vu Hoang is supported by scholarships from the Australian Government Research Training Program at the University of Melbourne, Australia. Trevor Cohn is supported by the ARC Future Fellowship. This work is partially supported by an ARC DP scholarship to Trevor Cohn and Gholamreza Haffari.8Some comparative translation examples are shown in Figure 4.9These limitations have only been used for training in previous work (Cohn et al., 2016; Wed et al., 2016)."}], "references": [{"title": "Neural Machine Translation by Jointly Learning to Align and Translate", "author": ["References Dzmitry Bahdanau", "Kyunghyun Cho", "Yoshua Bengio."], "venue": "Proc. of 3rd International Conference on Learning Representa-", "citeRegEx": "Bahdanau et al\\.,? 2015", "shortCiteRegEx": "Bahdanau et al\\.", "year": 2015}, {"title": "Endto-End Learning for Structured Prediction Energy Networks", "author": ["D. Belanger", "B. Yang", "A. McCallum."], "venue": "ArXiv e-prints.", "citeRegEx": "Belanger et al\\.,? 2017", "shortCiteRegEx": "Belanger et al\\.", "year": 2017}, {"title": "Structured prediction energy networks", "author": ["David Belanger", "Andrew McCallum."], "venue": "Proceedings of the 33rd International Conference on International Conference on Machine Learning - Volume 48, ICML\u201916, pages 983\u2013992. JMLR.org.", "citeRegEx": "Belanger and McCallum.,? 2016", "shortCiteRegEx": "Belanger and McCallum.", "year": 2016}, {"title": "A maximum entropy approach to natural language processing", "author": ["Adam L. Berger", "Vincent J. Della Pietra", "Stephen A. Della Pietra."], "venue": "Comput. Linguist., 22(1):39\u201371.", "citeRegEx": "Berger et al\\.,? 1996", "shortCiteRegEx": "Berger et al\\.", "year": 1996}, {"title": "Edinburgh Neural Machine Translation Systems for WMT 16", "author": ["Rico Sennrich", "Barry Haddow", "Alexandra Birch."], "venue": "Proceedings of the First Conference on Machine Translation, Volume 2: Shared Task Papers. Berlin, Germany.", "citeRegEx": "Sennrich et al\\.,? 2016", "shortCiteRegEx": "Sennrich et al\\.", "year": 2016}, {"title": "Report on the 11th IWSLT Evaluation Campaign", "author": ["M. Cettolo", "J. Niehues", "S. Stuker", "L. Bentivogli", "M. Federico."], "venue": "Proc. of The International Workshop on Spoken Language Translation (IWSLT).", "citeRegEx": "Cettolo et al\\.,? 2014", "shortCiteRegEx": "Cettolo et al\\.", "year": 2014}, {"title": "Noisy Parallel Approximate Decoding for Conditional Recurrent Language Model", "author": ["K. Cho."], "venue": "ArXiv e-prints.", "citeRegEx": "Cho.,? 2016", "shortCiteRegEx": "Cho.", "year": 2016}, {"title": "Incorporating Structural Alignment Biases into an Attentional Neural Translation Model", "author": ["Trevor Cohn", "Cong Duy Vu Hoang", "Ekaterina Vymolova", "Kaisheng Yao", "Chris Dyer", "Gholamreza Haffari."], "venue": "Proceedings of the 2016 Conference of", "citeRegEx": "Cohn et al\\.,? 2016", "shortCiteRegEx": "Cohn et al\\.", "year": 2016}, {"title": "Exponentiated Gradient Algorithms for Log-linear Structured Prediction", "author": ["Amir Globerson", "Terry Y. Koo", "Xavier Carreras", "Michael Collins."], "venue": "Proceedings of the 24th International Conference on Machine Learning, ICML \u201907, pages 305\u2013312, New", "citeRegEx": "Globerson et al\\.,? 2007", "shortCiteRegEx": "Globerson et al\\.", "year": 2007}, {"title": "Differentiable scheduled sampling for credit assignment", "author": ["Kartik Goyal", "Chris Dyer", "Taylor BergKirkpatrick."], "venue": "Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short", "citeRegEx": "Goyal et al\\.,? 2017", "shortCiteRegEx": "Goyal et al\\.", "year": 2017}, {"title": "Generating Sequences With Recurrent Neural Networks", "author": ["A. Graves."], "venue": "ArXiv e-prints.", "citeRegEx": "Graves.,? 2013", "shortCiteRegEx": "Graves.", "year": 2013}, {"title": "Long Short-Term Memory", "author": ["Sepp Hochreiter", "Jurgen Schmidhuber."], "venue": "Neural Comput., 9(8):1735\u2013 1780.", "citeRegEx": "Hochreiter and Schmidhuber.,? 1997", "shortCiteRegEx": "Hochreiter and Schmidhuber.", "year": 1997}, {"title": "Exponentiated Gradient Versus Gradient Descent for Linear Predictors", "author": ["Jyrki Kivinen", "Manfred K. Warmuth."], "venue": "Inf. Comput., 132(1):1\u201363.", "citeRegEx": "Kivinen and Warmuth.,? 1997", "shortCiteRegEx": "Kivinen and Warmuth.", "year": 1997}, {"title": "Mutual Information and Diverse Decoding Improve Neural Machine Translation", "author": ["J. Li", "D. Jurafsky."], "venue": "ArXiv e-prints.", "citeRegEx": "Li and Jurafsky.,? 2016", "shortCiteRegEx": "Li and Jurafsky.", "year": 2016}, {"title": "A Simple, Fast Diverse Decoding Algorithm for Neural Generation", "author": ["J. Li", "W. Monroe", "D. Jurafsky."], "venue": "ArXiv e-prints.", "citeRegEx": "Li et al\\.,? 2016", "shortCiteRegEx": "Li et al\\.", "year": 2016}, {"title": "A Critical Review of Recurrent Neural Networks for Sequence Learning", "author": ["Z.C. Lipton", "J. Berkowitz", "C. Elkan."], "venue": "ArXiv e-prints.", "citeRegEx": "Lipton et al\\.,? 2015", "shortCiteRegEx": "Lipton et al\\.", "year": 2015}, {"title": "Word-based alignment, phrase-based translation: Whats the link", "author": ["Adam Lopez", "Philip Resnik."], "venue": "Proceedings of 7th Biennial Conference of the Association for Machine Translation in the Americas (AMTA).", "citeRegEx": "Lopez and Resnik.,? 2006", "shortCiteRegEx": "Lopez and Resnik.", "year": 2006}, {"title": "Addressing the Rare Word Problem in Neural Machine Translation", "author": ["Thang Luong", "Ilya Sutskever", "Quoc Le", "Oriol Vinyals", "Wojciech Zaremba."], "venue": "Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the", "citeRegEx": "Luong et al\\.,? 2015", "shortCiteRegEx": "Luong et al\\.", "year": 2015}, {"title": "Coverage Embedding Models for Neural Machine Translation", "author": ["Haitao Mi", "Baskaran Sankaran", "Zhiguo Wang", "Abe Ittycheriah."], "venue": "Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing, pages 955\u2013960, Austin,", "citeRegEx": "Mi et al\\.,? 2016", "shortCiteRegEx": "Mi et al\\.", "year": 2016}, {"title": "DyNet: The Dynamic Neural Network Toolkit", "author": ["Y. Oda", "M. Richardson", "N. Saphra", "S. Swayamdipta", "P. Yin."], "venue": "ArXiv e-prints.", "citeRegEx": "Oda et al\\.,? 2017", "shortCiteRegEx": "Oda et al\\.", "year": 2017}, {"title": "Discriminative training and maximum entropy models for statistical machine translation", "author": ["Franz Josef Och", "Hermann Ney."], "venue": "Proceedings of the 40th Annual Meeting on Association for Computational Linguistics, pages 295\u2013302.", "citeRegEx": "Och and Ney.,? 2002", "shortCiteRegEx": "Och and Ney.", "year": 2002}, {"title": "BLEU: A Method for Automatic Evaluation of Machine Translation", "author": ["Kishore Papineni", "Salim Roukos", "Todd Ward", "WeiJing Zhu."], "venue": "Proceedings of the 40th Annual Meeting on Association for Computational Linguistics, ACL \u201902, pages 311\u2013318,", "citeRegEx": "Papineni et al\\.,? 2002", "shortCiteRegEx": "Papineni et al\\.", "year": 2002}, {"title": "On the momentum term in gradient descent learning algorithms", "author": ["Ning Qian."], "venue": "Neural Networks, 12(1):145 \u2013 151.", "citeRegEx": "Qian.,? 1999", "shortCiteRegEx": "Qian.", "year": 1999}, {"title": "Neural Machine Translation of Rare Words with Subword Units", "author": ["Rico Sennrich", "Barry Haddow", "Alexandra Birch."], "venue": "Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 1715\u2013", "citeRegEx": "Sennrich et al\\.,? 2016", "shortCiteRegEx": "Sennrich et al\\.", "year": 2016}, {"title": "Decoding neural machine translation using gradient descent", "author": ["Emanuel Snelleman."], "venue": "Master\u2019s thesis, Chalmers University of Technology, Gothenburg, Sweden.", "citeRegEx": "Snelleman.,? 2016", "shortCiteRegEx": "Snelleman.", "year": 2016}, {"title": "Approximate Inference in Graphical Models using LP Relaxations", "author": ["David Sontag."], "venue": "Ph.D. thesis, Massachusetts Institute of Technology, Department of Electrical Engineering and Computer Science.", "citeRegEx": "Sontag.,? 2010", "shortCiteRegEx": "Sontag.", "year": 2010}, {"title": "Sequence to sequence learning with neural networks", "author": ["Ilya Sutskever", "Oriol Vinyals", "Quoc V. Le."], "venue": "Proceedings of the 27th International Conference on Neural Information Processing Systems, NIPS\u201914, pages 3104\u20133112, Cambridge, MA,", "citeRegEx": "Sutskever et al\\.,? 2014", "shortCiteRegEx": "Sutskever et al\\.", "year": 2014}, {"title": "Bidirectional Decoding for Statistical Machine Translation", "author": ["Taro Watanabe", "Eiichiro Sumita."], "venue": "Proceedings of the 19th International Conference on Computational Linguistics - Volume 1, COLING \u201902, pages 1\u20137, Stroudsburg, PA, USA. Association", "citeRegEx": "Watanabe and Sumita.,? 2002a", "shortCiteRegEx": "Watanabe and Sumita.", "year": 2002}, {"title": "Bidirectional decoding for statistical machine translation", "author": ["Taro Watanabe", "Eiichiro Sumita."], "venue": "Proceedings of the 19th international conference on Computational linguistics-Volume 1, pages 1\u20137.", "citeRegEx": "Watanabe and Sumita.,? 2002b", "shortCiteRegEx": "Watanabe and Sumita.", "year": 2002}, {"title": "Sequence-to-Sequence Learning as Beam-Search Optimization", "author": ["Sam Wiseman", "Alexander M. Rush."], "venue": "Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing, pages 1296\u20131306, Austin, Texas. Asso-", "citeRegEx": "Wiseman and Rush.,? 2016", "shortCiteRegEx": "Wiseman and Rush.", "year": 2016}], "referenceMentions": [{"referenceID": 10, "context": "Sequence to sequence learning with neural networks (Graves, 2013; Sutskever et al., 2014; Lipton et al., 2015) is typically associated with two phases: training and decoding (a.", "startOffset": 51, "endOffset": 110}, {"referenceID": 26, "context": "Sequence to sequence learning with neural networks (Graves, 2013; Sutskever et al., 2014; Lipton et al., 2015) is typically associated with two phases: training and decoding (a.", "startOffset": 51, "endOffset": 110}, {"referenceID": 15, "context": "Sequence to sequence learning with neural networks (Graves, 2013; Sutskever et al., 2014; Lipton et al., 2015) is typically associated with two phases: training and decoding (a.", "startOffset": 51, "endOffset": 110}, {"referenceID": 27, "context": "For example, when decoding the words of the target sentence in a left-to-right manner, the right context is not exploited leading potentially to inferior performance (see Watanabe and Sumita (2002a) who apply this idea in traditional statistical MT).", "startOffset": 171, "endOffset": 199}, {"referenceID": 25, "context": "This is akin to linear programming relaxation approach for approximate inference in graphical models with discrete random variables where the exact inference is NP-hard (Sontag, 2010; Belanger and McCallum, 2016).", "startOffset": 169, "endOffset": 212}, {"referenceID": 2, "context": "This is akin to linear programming relaxation approach for approximate inference in graphical models with discrete random variables where the exact inference is NP-hard (Sontag, 2010; Belanger and McCallum, 2016).", "startOffset": 169, "endOffset": 212}, {"referenceID": 0, "context": "We briefly review the attentional neural translation model proposed by Bahdanau et al. (2015) as a sequence-to-sequence neural model onto which we will apply our decoding framework.", "startOffset": 71, "endOffset": 94}, {"referenceID": 25, "context": "In the context of graphical models, the above relaxation technique gives rise to linear programming for approximate inference (Sontag, 2010; Belanger and McCallum, 2016).", "startOffset": 126, "endOffset": 169}, {"referenceID": 2, "context": "In the context of graphical models, the above relaxation technique gives rise to linear programming for approximate inference (Sontag, 2010; Belanger and McCallum, 2016).", "startOffset": 126, "endOffset": 169}, {"referenceID": 12, "context": "Exponentiated gradient (Kivinen and Warmuth, 1997) is an elegant algorithm for solving optimisation problems involving simplex constraints.", "startOffset": 23, "endOffset": 50}, {"referenceID": 3, "context": "There are intriguing parallels with the maximum entropy formulation of log-linear models (Berger et al., 1996).", "startOffset": 89, "endOffset": 110}, {"referenceID": 27, "context": "For this reason, we propose to use both models, and seek to find translations that have high probability according both models (this mirrors work on bidirectional decoding in classical statistical machine translation by Watanabe and Sumita (2002b).) Decoding under the ensemble of these models leads to an intractable search problem, not well suited to traditional greedy or beam search algorithms, which require a fixed generation order of the target words.", "startOffset": 220, "endOffset": 248}, {"referenceID": 13, "context": "This bidirectional agreement may also lead to improvement in translation diversity, as shown in (Li and Jurafsky, 2016) in a re-ranking evaluation.", "startOffset": 96, "endOffset": 119}, {"referenceID": 20, "context": "This is inspired by the direct and reverse feature functions commonly used in classical discriminative SMT (Och and Ney, 2002) which have been shown to offer some complementary benefits (although see (Lopez and Resnik, 2006)).", "startOffset": 107, "endOffset": 126}, {"referenceID": 16, "context": "This is inspired by the direct and reverse feature functions commonly used in classical discriminative SMT (Och and Ney, 2002) which have been shown to offer some complementary benefits (although see (Lopez and Resnik, 2006)).", "startOffset": 200, "endOffset": 224}, {"referenceID": 5, "context": "We conducted our experiments on datasets with different scales, translating between Chinese\u2192English using the BTEC corpus, and German\u2192English using the IWSLT 2015 TED Talks corpus (Cettolo et al., 2014) and WMT 20162 corpus.", "startOffset": 180, "endOffset": 202}, {"referenceID": 7, "context": "We implemented our continuous-optimisation based decoding method on top of the Mantidae toolkit3 (Cohn et al., 2016), and using the dynet deep learning library4 (Neubig et al.", "startOffset": 97, "endOffset": 116}, {"referenceID": 11, "context": "We used a LSTM recurrent structure (Hochreiter and Schmidhuber, 1997) for both source and target RNN sequences.", "startOffset": 35, "endOffset": 69}, {"referenceID": 4, "context": "For large-scale dataset with WMT, we applied byte-pair encoding (BPE) method (Sennrich et al., 2016) so that the neural MT system can tackle the unknown word problem (Luong et al.", "startOffset": 77, "endOffset": 100}, {"referenceID": 17, "context": ", 2016) so that the neural MT system can tackle the unknown word problem (Luong et al., 2015).", "startOffset": 73, "endOffset": 93}, {"referenceID": 21, "context": "We evaluated in terms of search error, measured using the model score of the inferred solution (either continuous or discrete), as well as measuring the end translation quality with case-insensitive BLEU (Papineni et al., 2002).", "startOffset": 204, "endOffset": 227}, {"referenceID": 22, "context": "we use momentum (Qian, 1999) to accelerate the convergence by modifying the term \u2207i,w in Algorithm 1 with a weighted moving average of past gradients:", "startOffset": 16, "endOffset": 28}, {"referenceID": 8, "context": "This concurs with previous work on learning structured prediction models with EG (Globerson et al., 2007).", "startOffset": 81, "endOffset": 105}, {"referenceID": 6, "context": "The most widely-used inference methods include sampling (Cho, 2016), greedy and beam search (Sutskever et al.", "startOffset": 56, "endOffset": 67}, {"referenceID": 13, "context": ", 2015, inter alia), and reranking (Birch, 2016; Li and Jurafsky, 2016).", "startOffset": 35, "endOffset": 71}, {"referenceID": 2, "context": "Recently, relaxation techniques have been applied to deep models for training and inference in text classification (Belanger and McCallum, 2016; Belanger et al., 2017), and fully differentiable training of sequence-to-sequence models with scheduled-sampling (Goyal et al.", "startOffset": 115, "endOffset": 167}, {"referenceID": 1, "context": "Recently, relaxation techniques have been applied to deep models for training and inference in text classification (Belanger and McCallum, 2016; Belanger et al., 2017), and fully differentiable training of sequence-to-sequence models with scheduled-sampling (Goyal et al.", "startOffset": 115, "endOffset": 167}, {"referenceID": 9, "context": ", 2017), and fully differentiable training of sequence-to-sequence models with scheduled-sampling (Goyal et al., 2017).", "startOffset": 98, "endOffset": 118}, {"referenceID": 7, "context": "These constraints have only been used for training in the previous works (Cohn et al., 2016; Mi et al., 2016).", "startOffset": 73, "endOffset": 109}, {"referenceID": 18, "context": "These constraints have only been used for training in the previous works (Cohn et al., 2016; Mi et al., 2016).", "startOffset": 73, "endOffset": 109}], "year": 2017, "abstractText": "We propose a novel decoding approach for neural machine translation (NMT) based on continuous optimisation. We convert decoding \u2013 basically a discrete optimization problem \u2013 into a continuous optimization problem. The resulting constrained continuous optimisation problem is then tackled using gradient-based methods. Our powerful decoding framework enables decoding intractable models such as the intersection of left-to-right and right-to-left (bidirectional) as well as source-to-target and target-to-source (bilingual) NMT models. Our empirical results show that our decoding framework is effective, and leads to substantial improvements in translations generated from the intersected models where the typical greedy or beam search is not feasible. We also compare our framework against reranking, and analyse its advantages and disadvantages.", "creator": "TeX"}}}