{"id": "1701.04027", "review": {"conference": "AAAI", "VERSION": "v1", "DATE_OF_SUBMISSION": "15-Jan-2017", "title": "Neural Models for Sequence Chunking", "abstract": "Many natural language understanding (NLU) tasks, such as shallow parsing (i.e., text chunking) and semantic slot filling, require the assignment of representative labels to the meaningful chunks in a sentence. Most of the current deep neural network (DNN) based methods consider these tasks as a sequence labeling problem, in which a word, rather than a chunk, is treated as the basic unit for labeling. These chunks are then inferred by the standard IOB (Inside-Outside-Beginning) labels. In this paper, we propose an alternative approach by investigating the use of DNN for sequence chunking, and propose three neural models so that each chunk can be treated as a complete unit for labeling. Experimental results show that the proposed neural sequence chunking models can achieve start-of-the-art performance on both the text chunking and slot filling tasks.", "histories": [["v1", "Sun, 15 Jan 2017 11:08:28 GMT  (224kb,D)", "http://arxiv.org/abs/1701.04027v1", "Accepted by AAAI 2017"]], "COMMENTS": "Accepted by AAAI 2017", "reviews": [], "SUBJECTS": "cs.CL", "authors": ["feifei zhai", "saloni potdar", "bing xiang", "bowen zhou"], "accepted": true, "id": "1701.04027"}, "pdf": {"name": "1701.04027.pdf", "metadata": {"source": "CRF", "title": "Neural Models for Sequence Chunking", "authors": ["Feifei Zhai", "Saloni Potdar", "Bing Xiang", "Bowen Zhou"], "emails": ["fzhai@us.ibm.com", "potdars@us.ibm.com", "bingxia@us.ibm.com", "zhou@us.ibm.com"], "sections": [{"heading": "Introduction", "text": "In fact, most people who are able to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to"}, {"heading": "Basic Neural Networks", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "Recurrent Neural Network", "text": "Recursive neural network (RNN) is a neural network suitable for modelling sequential information. Although it is theoretically able to detect dependencies over long distances, in practice it suffers from the problems of the course that disappears / explodes (Bengio, Simard and Frasconi 1994). Long-term memory networks (LSTM) were introduced to overcome these history problems and model long-distance dependencies (Hochreiter and Schmidhuber 1997) by using a memory cell. Faced with an input set x = (x1, x2,..., xT) where T is the record length, the hidden state of the LSTM is replaced by: it = \u03c3 (W ixt + U iht \u2212 1 + b) ft = (W fxt + U fht \u2212 f) ot = (W oxt + b f) ot = (h) and the element tanid (W oxt + U oht) is tangt = id."}, {"heading": "Convolutional Neural Network", "text": "Convolutional Neural Networks (CNN) were used to extract features for sentence classification (Kim 2014; Ma et al. 2015; dos Santos, Xiang and Zhou 2015). Faced with a sentence, a CNN with m-filters and a filter size of n extracts a m-dimensional feature vector from each n-gram phrase in the sentence. Over all the extracted feature vectors, a max-pooling layer is applied to create the last indicative feature vector (m-dimension) for the sentence. Following this approach, we use CNN and the maximum-pooling layer to extract features from chunks. For each identified chunk, we apply CNN to the embedding of its words (whether it is a single chunk of word or chunks), and then use the maximum-pooling layer to obtain the feature vector for the label."}, {"heading": "Proposed Models", "text": "This year, it has reached the point where it will be able to retaliate."}, {"heading": "Learning Objective", "text": "As we have described above, all of the above models solve two subtasks - segmentation and labeling. We use the crossentropy loss function for both subtasks and add the two losses to the learning goal: L (\u03b8) = Lsegmentation (\u03b8) + Llabeling (\u03b8) (8), where \u03b8 denotes the learnable parameters. Alternatively, we could also use weighted sum or multitask learning by considering segmentation and labeling as the two tasks. We will leave these extensions as future work."}, {"heading": "Experiments", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "Experimental Setup", "text": "This year, it has come to the point where it will be able to put itself at the top, \"he said in an interview with the\" Welt am Sonntag. \""}, {"heading": "Text Chunking Results", "text": "The results of the text chunking task are presented in Table 1. \"Baseline (Bi-LSTM)\" refers to a Bi-LSTM model for sequence labeling (use IOB-based labels on words as shown in Figure 1). \"F1\" is the final evaluation metric, and \"segmentF1\" refers to the segmentation F1 score. The table indicates that Model I and Model II only have comparable results with the baseline on both evaluation metrics - Segment 1 and Final Result F1 Score. Consequently, we conclude that the use of IOB labels for independent segmentation might not be a good choice. However, Model III exceeds baseline in both segmentation and labeling. We continue to compare our best result with the current published results in Table 2. In the table (Collobert et al. 2011) we found that 100 epochs are sufficient to match the model."}, {"heading": "Slot Filling Results", "text": "In this context, it should be noted that the solution to problems that have arisen in recent years is a very complex situation."}, {"heading": "Related Work", "text": "In recent years, many in-depth learning approaches have been explored to solve sequence labeling tasks. (Collobert et al. 2011) proposed an effective window-based approach in which they used a feeding neural network to classify each word and conditional random fields (CRF) to capture sequential information. CNNs are also widely used to extract effective classification features (Xu and Sarikaya 2013; Vu 2016).RNNs are an easy and more appropriate choice for these tasks as they model sequential information. (Huang, Xu, and Yu 2015) presented a BiLSTM CRF model and achieved advanced performance in multiple tasks, such as designated entity recognition and text dots using handcrafted features. (Chiu and Nichols 2015) used a BiLSTM for labeling and a CNN model to capture information at character level, in addition to handcrafted features such as Gatti and Santos."}, {"heading": "Conclusion", "text": "Our experiments show that the segmentation results of Model I and Model II are comparable to the baseline on text chunking data and ATIS data and worse than the baseline on BIG data, while Model III achieves a higher Segment F1 score than the baseline, demonstrating that the use of IOB labels is unsuitable for building segmentation models. In addition, Model I and II do not consistently improve the final F1 score - the segmentation step improves labeling when filling slots, but not when texting tasks. Finally, Model III consistently performs better than the base model and achieves state-of-the-art performance in both tasks. We also gain insights into the data sets we use by comparing the Segment F1 scores and the F1 scores of Model III."}], "references": [{"title": "Neural machine translation by jointly learning to align and translate", "author": ["Cho Bahdanau", "D. Bengio 2014] Bahdanau", "K. Cho", "Y. Bengio"], "venue": "arXiv preprint arXiv:1409.0473", "citeRegEx": "Bahdanau et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Bahdanau et al\\.", "year": 2014}, {"title": "Learning long-term dependencies with gradient descent is difficult. IEEE transactions on neural networks 5(2):157\u2013166", "author": ["Simard Bengio", "Y. Frasconi 1994] Bengio", "P. Simard", "P. Frasconi"], "venue": null, "citeRegEx": "Bengio et al\\.,? \\Q1994\\E", "shortCiteRegEx": "Bengio et al\\.", "year": 1994}, {"title": "and Nichols", "author": ["J.P. Chiu"], "venue": "E.", "citeRegEx": "Chiu and Nichols 2015", "shortCiteRegEx": null, "year": 2015}, {"title": "Natural language processing (almost) from scratch", "author": ["Collobert"], "venue": "Journal of Machine Learning Research 12(Aug):2493\u20132537", "citeRegEx": "Collobert,? \\Q2011\\E", "shortCiteRegEx": "Collobert", "year": 2011}, {"title": "Deep convolutional neural networks for sentiment analysis of short texts", "author": ["dos Santos", "C.N. Gatti 2014] dos Santos", "M. Gatti"], "venue": null, "citeRegEx": "Santos et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Santos et al\\.", "year": 2014}, {"title": "Classifying relations by ranking with convolutional neural networks. In ACL, 626\u2013634", "author": ["Xiang dos Santos", "C. Zhou 2015] dos Santos", "B. Xiang", "B. Zhou"], "venue": null, "citeRegEx": "Santos et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Santos et al\\.", "year": 2015}, {"title": "and Klein", "author": ["G. Durrett"], "venue": "D.", "citeRegEx": "Durrett and Klein 2015", "shortCiteRegEx": null, "year": 2015}, {"title": "N", "author": ["C. Dyer", "M. Ballesteros", "W. Ling", "A. Matthews", "Smith"], "venue": "A.", "citeRegEx": "Dyer et al. 2015", "shortCiteRegEx": null, "year": 2015}, {"title": "and Schmidhuber", "author": ["S. Hochreiter"], "venue": "J.", "citeRegEx": "Hochreiter and Schmidhuber 1997", "shortCiteRegEx": null, "year": 1997}, {"title": "Bidirectional lstm-crf models for sequence tagging", "author": ["Xu Huang", "Z. Yu 2015] Huang", "W. Xu", "K. Yu"], "venue": "arXiv preprint arXiv:1508.01991", "citeRegEx": "Huang et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Huang et al\\.", "year": 2015}, {"title": "Domain adaptation of recurrent neural networks for natural language understanding", "author": ["Heck Jaech", "A. Ostendorf 2016] Jaech", "L. Heck", "M. Ostendorf"], "venue": "arXiv preprint arXiv:1604.00117", "citeRegEx": "Jaech et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Jaech et al\\.", "year": 2016}, {"title": "and Matsumoto", "author": ["T. Kudo"], "venue": "Y.", "citeRegEx": "Kudo and Matsumoto 2001", "shortCiteRegEx": null, "year": 2001}, {"title": "and Matsumoto", "author": ["T. Kudoh"], "venue": "Y.", "citeRegEx": "Kudoh and Matsumoto 2000", "shortCiteRegEx": null, "year": 2000}, {"title": "Leveraging sentence-level information with encoder lstm for semantic slot filling", "author": ["Kurata"], "venue": "arXiv preprint arXiv:1601.01530", "citeRegEx": "Kurata,? \\Q2016\\E", "shortCiteRegEx": "Kurata", "year": 2016}, {"title": "C", "author": ["G. Lample", "M. Ballesteros", "K. Kawakami", "S. Subramanian", "Dyer"], "venue": "2016. Neural architectures for named entity recognition. In In proceedings of NAACL", "citeRegEx": "Lample et al. 2016", "shortCiteRegEx": null, "year": 2016}, {"title": "and Lane", "author": ["B. Liu"], "venue": "I.", "citeRegEx": "Liu and Lane 2015", "shortCiteRegEx": null, "year": 2015}, {"title": "Asgard: A portable architecture for multilingual dialogue systems", "author": ["Liu"], "venue": "In ICASSP. IEEE", "citeRegEx": "Liu,? \\Q2013\\E", "shortCiteRegEx": "Liu", "year": 2013}, {"title": "Query understanding enhanced by hierarchical parsing structures", "author": ["Liu"], "venue": "In ASRU,", "citeRegEx": "Liu,? \\Q2013\\E", "shortCiteRegEx": "Liu", "year": 2013}, {"title": "and Hovy", "author": ["X. Ma"], "venue": "E.", "citeRegEx": "Ma and Hovy 2016", "shortCiteRegEx": null, "year": 2016}, {"title": "Dependency-based convolutional neural networks for sentence embedding", "author": ["Ma"], "venue": "In ACL,", "citeRegEx": "Ma,? \\Q2015\\E", "shortCiteRegEx": "Ma", "year": 2015}, {"title": "Using recurrent neural networks for slot filling in spoken language understanding", "author": ["Mesnil"], "venue": null, "citeRegEx": "Mesnil,? \\Q2015\\E", "shortCiteRegEx": "Mesnil", "year": 2015}, {"title": "Abstractive text summarization using sequence-to-sequence rnns and beyond", "author": ["Nallapati"], "venue": "Proceedings of CoNLL", "citeRegEx": "Nallapati,? \\Q2016\\E", "shortCiteRegEx": "Nallapati", "year": 2016}, {"title": "and Yao", "author": ["B. Peng"], "venue": "K.", "citeRegEx": "Peng and Yao 2015", "shortCiteRegEx": null, "year": 2015}, {"title": "Weighting finite-state transductions with neural context", "author": ["Cotterell Rastogi", "P. Eisner 2016] Rastogi", "R. Cotterell", "J. Eisner"], "venue": "In Proceedings of NAACL", "citeRegEx": "Rastogi et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Rastogi et al\\.", "year": 2016}, {"title": "and Pereira", "author": ["F. Sha"], "venue": "F.", "citeRegEx": "Sha and Pereira 2003", "shortCiteRegEx": null, "year": 2003}, {"title": "and Sarkar", "author": ["H. Shen"], "venue": "A.", "citeRegEx": "Shen and Sarkar 2005", "shortCiteRegEx": null, "year": 2005}, {"title": "Q", "author": ["I. Sutskever", "O. Vinyals", "Le"], "venue": "V.", "citeRegEx": "Sutskever. Vinyals. and Le 2014", "shortCiteRegEx": null, "year": 2014}, {"title": "and Buchholz", "author": ["E.F. Tjong Kim Sang"], "venue": "S.", "citeRegEx": "Tjong Kim Sang and Buchholz 2000", "shortCiteRegEx": null, "year": 2000}, {"title": "N", "author": ["O. Vinyals", "M. Fortunato", "Jaitly"], "venue": "2015. Pointer networks. In NIPS, 2692\u2013", "citeRegEx": "Vinyals. Fortunato. and Jaitly 2015", "shortCiteRegEx": null, "year": 2700}, {"title": "N", "author": ["Vu"], "venue": "T.; Gupta, P.; Adel, H.; Sch, H.; et al.", "citeRegEx": "Vu et al. 2016", "shortCiteRegEx": null, "year": 2016}, {"title": "N", "author": ["Vu"], "venue": "T.", "citeRegEx": "Vu 2016", "shortCiteRegEx": null, "year": 2016}, {"title": "and Sarikaya", "author": ["P. Xu"], "venue": "R.", "citeRegEx": "Xu and Sarikaya 2013", "shortCiteRegEx": null, "year": 2013}, {"title": "Multi-task crosslingual sequence tagging from scratch. arXiv preprint arXiv:1603.06270", "author": ["Salakhutdinov Yang", "Z. Cohen 2016] Yang", "R. Salakhutdinov", "W. Cohen"], "venue": null, "citeRegEx": "Yang et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Yang et al\\.", "year": 2016}, {"title": "Recurrent neural networks for language understanding", "author": ["Yao"], "venue": null, "citeRegEx": "Yao,? \\Q2013\\E", "shortCiteRegEx": "Yao", "year": 2013}, {"title": "Spoken language understanding using long short-term memory neural networks", "author": ["Yao"], "venue": "In Spoken Language Technology Workshop (SLT),", "citeRegEx": "Yao,? \\Q2014\\E", "shortCiteRegEx": "Yao", "year": 2014}, {"title": "and Yu", "author": ["S. Zhu"], "venue": "K.", "citeRegEx": "Zhu and Yu 2016", "shortCiteRegEx": null, "year": 2016}], "referenceMentions": [], "year": 2017, "abstractText": "Many natural language understanding (NLU) tasks, such as shallow parsing (i.e., text chunking) and semantic slot filling, require the assignment of representative labels to the meaningful chunks in a sentence. Most of the current deep neural network (DNN) based methods consider these tasks as a sequence labeling problem, in which a word, rather than a chunk, is treated as the basic unit for labeling. These chunks are then inferred by the standard IOB (Inside-OutsideBeginning) labels. In this paper, we propose an alternative approach by investigating the use of DNN for sequence chunking, and propose three neural models so that each chunk can be treated as a complete unit for labeling. Experimental results show that the proposed neural sequence chunking models can achieve start-of-the-art performance on both the text chunking and slot filling tasks.", "creator": "LaTeX with hyperref package"}}}