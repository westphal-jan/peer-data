{"id": "1701.04056", "review": {"conference": "acl", "VERSION": "v1", "DATE_OF_SUBMISSION": "15-Jan-2017", "title": "Dialog Context Language Modeling with Recurrent Neural Networks", "abstract": "In this work, we propose contextual language models that incorporate dialog level discourse information into language modeling. Previous works on contextual language model treat preceding utterances as a sequence of inputs, without considering dialog interactions. We design recurrent neural network (RNN) based contextual language models that specially track the interactions between speakers in a dialog. Experiment results on Switchboard Dialog Act Corpus show that the proposed model outperforms conventional single turn based RNN language model by 3.3% on perplexity. The proposed models also demonstrate advantageous performance over other competitive contextual language models.", "histories": [["v1", "Sun, 15 Jan 2017 15:10:29 GMT  (88kb,D)", "http://arxiv.org/abs/1701.04056v1", "Accepted for publication at ICASSP 2017"]], "COMMENTS": "Accepted for publication at ICASSP 2017", "reviews": [], "SUBJECTS": "cs.CL", "authors": ["bing liu", "ian lane"], "accepted": true, "id": "1701.04056"}, "pdf": {"name": "1701.04056.pdf", "metadata": {"source": "CRF", "title": "DIALOG CONTEXT LANGUAGE MODELING WITH RECURRENT NEURAL NETWORKS", "authors": ["Bing Liu", "Ian Lane"], "emails": ["liubing@cmu.edu,", "lane@cmu.edu"], "sections": [{"heading": null, "text": "Index Terms - RNNLM, context-dependent language model, dialogue modeling, dialogue action"}, {"heading": "1. INTRODUCTION", "text": "Language models play an important role in many systems of natural language processing, such as automatic speech recognition [1, 2] and machine translation [3, 4]. Recursive neural network models [5, 6] have recently shown success in speech modeling and outperform conventional n-gram-based models. Long-term memory [7, 8] is a widely used RNN variant of speech modeling due to its superior performance in capturing long-term dependencies. Conventional RNN-based language model uses a hidden state to represent the summary of the preceding words in a sentence without taking into account context signals. Mikolov et al. proposed a context-dependent RNN language model [9] by linking a context-based vector to the hidden state. This context-dependent vector is generated by applying latent directional allocation [10] to previous texts."}, {"heading": "2. BACKGROUND", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "2.1. RNN Language Model", "text": "A language model assigns a probability to a word sequence w = (w1, w2,..., wT) after the probability distribution. With the help of the chain rule, the probability of the word sequence w can be factored in as: P (w) = P (w1, w2,..., wT) = T \u0442t = 1 P (wt | w < t) (1) At step t, the system input is the embedding of the word at index t, and the system output is the probability distribution of the word at index t + 1. The RNN hidden state ht encodes the information of the word sequence up to the current step: ht = RNN (ht \u2212 1, wt) (2) P (wt + 1 | w < t + 1) = softmax (Living + bo) (3), where Where and bo are the initial weights and distortions."}, {"heading": "2.2. Contextual RNN Language Model", "text": "Mikolov and Zweig [9] proposed a topic-based RNNLM by introducing a contextual real vector into the hidden state of the RNN. Contextual vector was created by performing LDA [10] on previous text. Wang and Cho [11] examined the introduction of Xiv: 170 1,04 056v 1 [cs.C L] 15 January 2017Corpus-level discourse information into language modeling. A number of context presentation methods were examined, including word bags, dictionaries, word bags and dictionaries with attention. Lin et al. [14] proposed using hierarchical RNN for modeling documents. Compared to using dictionaries and dictionaries for context representation, hierarchical RNN can better model the order of words in previous text, with most of the text information proposed at the code model level."}, {"heading": "3. METHODS", "text": "The previously proposed contextual language models focus on the application of context by encoding preceding texts, without taking into account interactions in dialogues. These models may not be suitable for modeling dialog languages, as they are not designed to capture dialog interactions such as clarifications and confirmations. By specifically designing in learning dialog interactions, we expect models to provide better representations of the dialog context and thus lower perplexity of the target dialog reversal or utterance. In this section, we will first explain the context-dependent RNN language model, which works at the outer or inflection level. Afterwards, we will describe the two proposed contextual language models that use the context of the dialogues."}, {"heading": "3.1. Context Dependent RNNLM", "text": "The kest move Uk = (w1, w2,..., wTk) is represented as a sequence of Tk words. Depending on the information in the previous text in the dialog, the probability of the target move Uk can be calculated as follows: P (Uk | U < k) = Tk, t = 1 P (wUkt | w Uk < t, U < k) (4), where U < k denotes all previous revolutions before Uk and wUk < t all previous words before the tallest word in turn Uk. In the context-dependent RNN language model, the context vector c is connected to the RNN hidden state, together with the input word embedding in each time step (Figure 1), which is similar to the context-dependent RNN model in the additionally suggested context Nht [n], which is not directly connected to the context (Nht = 5)."}, {"heading": "3.2. Context Representations", "text": "In neural network-based speech models, the dialog context can be represented as a dense continuous vector, which can be created in various ways. A simple approach is the use of word embedding, but word embedding does not take into account word order. An alternative approach is the use of an RNN to read the preceding text. The last hidden state of the RNN encoder can be considered the representation of the text and used as a context vector for the next round. To create a document-level context representation, you can cascade all sentences in a document by removing the sentence boundaries. The last hidden state of the RNN of the previous utterance serves as the initial RNN state of the next utterance. As in [12], we refer to this model as DRNNLM. Alternatively, in the CCDCLM model proposed in [12], the last hidden state of the previous utterance of the hidden state of the target is led to the target state."}, {"heading": "3.3. Interactive Dialog Context LM", "text": "The previously proposed contextual language models, such as DRNNLM and CCDCLM, treat the dialog history as a sequence of inputs without modelling dialog interactions. A rotation of the dialog of a speaker can not only be a direct response to the request of the other speaker, but should also be a continuation of his own previous statement. Therefore, when modelling rotation k in a dialog, we suggest linking the last RNN state of rotation k \u2212 2 directly to the incipient RNN state of rotation k, rather than linking it with the incipient RNN state of rotation k \u2212 1. The last RNN state of rotation k \u2212 1 serves as a context vector for rotation k, which is supplied to rotate k's RNN state at each step together with the word input. The model architecture is shown as in Figure 2. Context vector c and the initial RNN state of rotation Uk \u2212 the Uk turn \u2212 the previous signal Uk = 1 \u2212 the loss of the model \u2212 1 preceding signal \u2212 1"}, {"heading": "3.4. External State Interactive Dialog Context LM", "text": "The propagation of the dialog context can be considered as a series of updates of a hidden dialog context state along the growing dialog. IDCLM models this hidden dialog context state implicitly in the RNN state of the turn level. Such dialog context states can also be modeled in a separate RNN. As illustrated in the architecture in Figure 3, we use an external RNN to explicitly model the context changes. Input to the external state RNN is the vector representation of the previous dialog. The external state RNN output serves as the dialog context for the next turn: sk \u2212 1 = RNNES (sk \u2212 2, h Uk \u2212 1 Tk \u2212 1) (7), where sk \u2212 1 is the output of the external state RNN after processing the turn k \u2212 1. The context vector c and the initial RNUCN state Uk for this tuk \u2212 2 are then followed by \u2212 hk \u2212 1: \u2212 CLn \u2212 1."}, {"heading": "4. EXPERIMENTS", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "4.1. Data Set", "text": "We use the Switchboard Dialog Act Corpus (SwDA) 1 to evaluate our contextual language usage models. The SwDA Corpus extends the Switchboard-1 Telephone Speech Corpus by rotation levels and statement-level dialog files. The statements are also marked with Part-of-Speech-Tags (POS). We divide the data into sw00 folder in sw09 as a training set, sw10 folder as a test set, and sw11 folder in sw13 as a validation set. The training, validation, and test sets contain 98.7K rotations (190.0K expressions), 5.7K rotations (11.3K expressions), and 11.9K rotations (22.2K expressions). The maximum rotation length is 160. The vocabulary is defined by the most common 10K words."}, {"heading": "4.2. Baselines", "text": "We compare IDCLM and ESIDCLM with several basic methods, including N-gram-based model, single-turn RNNLM, and various context-dependent RNNLMs.5-gram KN A 5-gram language model with modified Kneser-Ney smoothing [16]. Single-turn RNNLM Contextual RNNLM, which is at single-turn level without context information.BoW context RNNLM Contextual RNLM with BoW representation of the previous text as context.DRNLM Contextual RNLM with context vector associated with the turn context vector RNNLM. We implement this model following the design in [12].To investigate the potential performance gain that can be achieved by introducing context."}, {"heading": "4.3. Model Configuration and Training", "text": "In this thesis, we use the LSTM cell [7] as a basic RNN unit for its greater ability to capture extensive dependencies in a word sequence compared to simple RNN. We use pre-formed word vectors [17] trained on Google News records to initialize word embeddings, which are fine-tuned during model training. We perform 1http: / / compprag.christopherpotts.net / swda.htmlmini-batch training using the Adam optimization method that follows the proposed parameter setup in [18]. Maximum standard is 5 for gradient cutting. For regulation, we apply suspensions (p = 0.8) to the non-recurring connections [19] of LSTM. In addition, we apply the L2 regulation (\u03bb = 10 \u2212 4) to the weights and distortions of the RN base layer."}, {"heading": "4.4. Results and Analysis", "text": "The experiments to model the perplexity for models with different dialog sizes are shown in Table 1. The K-value shows the number of turns in the dialog. BoW context RNLM and DRNLM consistently suggest the single-turnRNLM. Our implementation of the context-dependent CCDCLM performs worse than the single-turnRNLM. This could be due to the fact that the target context preference is too focused on the previous turn context vector."}, {"heading": "DA Tag IDCLM ESIDCLM DACLM", "text": "For the Dialogact tag-based results in Table 3, the three context-dependent models show a consistent increase in performance for non-opinion-based statements. The perplexity changes for other Dialogact tags vary for different models."}, {"heading": "5. CONCLUSIONS", "text": "In this paper, we propose two dialog context language models that model dialog interactions using special design. Our evaluation results on Switchboard Dialog Act Corpus show that the proposed model outperforms the traditional RNN language model by 3.3%. The proposed models also illustrate a favorable performance over several competing contextual language models. Perplexity of the proposed dialog context language models is higher than that of the model that uses real dialog stamps as context, suggesting that the proposed model could implicitly capture the dialog context status for speech modeling."}, {"heading": "6. REFERENCES", "text": "[1] Lawrence Rabiner and Biing-Hwang Juang, \"Fundamentals of speech recognition,\" 1993. [2] William Chan, Navdeep Jaitly, Quoc Le, and Oriol Vinyals, \"Listen, attend and spell: A neural network for large vocabulary conversations recognition,\" in 2016 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP). [3] Peter F Brown, John Cocke, Stephen Della Pietra, Vincent J Della Pietra, Fredrick Jelinek, John XiXiXiXiun Cho, Robert L Mercer, and Paul XiXiXiXier,. \""}], "references": [{"title": "Fundamentals of speech recognition", "author": ["Lawrence Rabiner", "Biing-Hwang Juang"], "venue": "1993.", "citeRegEx": "1", "shortCiteRegEx": null, "year": 1993}, {"title": "Listen, attend and spell: A neural network for large vocabulary conversational speech recognition", "author": ["William Chan", "Navdeep Jaitly", "Quoc Le", "Oriol Vinyals"], "venue": "2016 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP). IEEE, 2016, pp. 4960\u20134964.", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2016}, {"title": "A statistical approach to machine translation", "author": ["Peter F Brown", "John Cocke", "Stephen A Della Pietra", "Vincent J Della Pietra", "Fredrick Jelinek", "John D Lafferty", "Robert L Mercer", "Paul S Roossin"], "venue": "Computational linguistics, vol. 16, no. 2, pp. 79\u201385, 1990.", "citeRegEx": "3", "shortCiteRegEx": null, "year": 1990}, {"title": "Learning phrase representations using rnn encoder-decoder for statistical machine translation", "author": ["Kyunghyun Cho", "Bart Van Merri\u00ebnboer", "Caglar Gulcehre", "Dzmitry Bahdanau", "Fethi Bougares", "Holger Schwenk", "Yoshua Bengio"], "venue": "arXiv preprint arXiv:1406.1078, 2014.", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2014}, {"title": "Recurrent neural network based language model", "author": ["Tomas Mikolov", "Martin Karafi\u00e1t", "Lukas Burget", "Jan Cernock\u1ef3", "Sanjeev Khudanpur"], "venue": "Interspeech, 2010, vol. 2, p. 3.", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2010}, {"title": "Extensions of recurrent neural network language model", "author": ["Tom\u00e1\u0161 Mikolov", "Stefan Kombrink", "Luk\u00e1\u0161 Burget", "Jan \u010cernock\u1ef3", "Sanjeev Khudanpur"], "venue": "2011 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP). IEEE, 2011, pp. 5528\u20135531.", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2011}, {"title": "Long shortterm memory", "author": ["Sepp Hochreiter", "J\u00fcrgen Schmidhuber"], "venue": "Neural computation, vol. 9, no. 8, pp. 1735\u20131780, 1997.", "citeRegEx": "7", "shortCiteRegEx": null, "year": 1997}, {"title": "Lstm neural networks for language modeling", "author": ["Martin Sundermeyer", "Ralf Schl\u00fcter", "Hermann Ney"], "venue": "Interspeech, 2012, pp. 194\u2013197.", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2012}, {"title": "Context dependent recurrent neural network language model", "author": ["Tomas Mikolov", "Geoffrey Zweig"], "venue": "SLT, 2012, pp. 234\u2013239.", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2012}, {"title": "Latent dirichlet allocation", "author": ["David M Blei", "Andrew Y Ng", "Michael I Jordan"], "venue": "Journal of machine Learning research, vol. 3, no. Jan, pp. 993\u20131022, 2003.", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2003}, {"title": "Larger-context language modelling", "author": ["Tian Wang", "Kyunghyun Cho"], "venue": "arXiv preprint arXiv:1511.03729, 2015.", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2015}, {"title": "Document context language models", "author": ["Yangfeng Ji", "Trevor Cohn", "Lingpeng Kong", "Chris Dyer", "Jacob Eisenstein"], "venue": "arXiv preprint arXiv:1511.03962, 2015.", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2015}, {"title": "Grounding in communication", "author": ["Herbert H Clark", "Susan E Brennan"], "venue": "Perspectives on socially shared cognition, vol. 13, no. 1991, pp. 127\u2013149, 1991.", "citeRegEx": "13", "shortCiteRegEx": null, "year": 1991}, {"title": "Hierarchical recurrent neural network for document modeling", "author": ["Rui Lin", "Shujie Liu", "Muyun Yang", "Mu Li", "Ming Zhou", "Sheng Li"], "venue": "Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, 2015, pp. 899\u2013907.", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2015}, {"title": "Inter-document contextual language model", "author": ["Quan Hung Tran", "Ingrid Zukerman", "Gholamreza Haffari"], "venue": "Proceedings of NAACL-HLT, 2016, pp. 762\u2013766.", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2016}, {"title": "An empirical study of smoothing techniques for language modeling", "author": ["Stanley F Chen", "Joshua Goodman"], "venue": "Proceedings of the 34th annual meeting on Association for Computational Linguistics. Association for Computational Linguistics, 1996, pp. 310\u2013318.", "citeRegEx": "16", "shortCiteRegEx": null, "year": 1996}, {"title": "Distributed representations of words and phrases and their compositionality", "author": ["T Mikolov", "J Dean"], "venue": "Advances in neural information processing systems, 2013.", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2013}, {"title": "Adam: A method for stochastic optimization", "author": ["Diederik Kingma", "Jimmy Ba"], "venue": "arXiv preprint arXiv:1412.6980, 2014.", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2014}, {"title": "Recurrent neural network regularization", "author": ["Wojciech Zaremba", "Ilya Sutskever", "Oriol Vinyals"], "venue": "arXiv preprint arXiv:1409.2329, 2014.", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2014}], "referenceMentions": [{"referenceID": 0, "context": "Language model plays an important role in many natural language processing systems, such as in automatic speech recognition [1, 2] and machine translation systems [3, 4].", "startOffset": 124, "endOffset": 130}, {"referenceID": 1, "context": "Language model plays an important role in many natural language processing systems, such as in automatic speech recognition [1, 2] and machine translation systems [3, 4].", "startOffset": 124, "endOffset": 130}, {"referenceID": 2, "context": "Language model plays an important role in many natural language processing systems, such as in automatic speech recognition [1, 2] and machine translation systems [3, 4].", "startOffset": 163, "endOffset": 169}, {"referenceID": 3, "context": "Language model plays an important role in many natural language processing systems, such as in automatic speech recognition [1, 2] and machine translation systems [3, 4].", "startOffset": 163, "endOffset": 169}, {"referenceID": 4, "context": "Recurrent neural network (RNN) based models [5, 6] have recently shown success in language modeling, outperforming conventional n-gram based models.", "startOffset": 44, "endOffset": 50}, {"referenceID": 5, "context": "Recurrent neural network (RNN) based models [5, 6] have recently shown success in language modeling, outperforming conventional n-gram based models.", "startOffset": 44, "endOffset": 50}, {"referenceID": 6, "context": "Long short-term memory [7, 8] is a widely used RNN variant for language modeling due to its superior performance in capturing longer term dependencies.", "startOffset": 23, "endOffset": 29}, {"referenceID": 7, "context": "Long short-term memory [7, 8] is a widely used RNN variant for language modeling due to its superior performance in capturing longer term dependencies.", "startOffset": 23, "endOffset": 29}, {"referenceID": 8, "context": "proposed a context dependent RNN language model [9] by connecting a contextual vector to the RNN hidden state.", "startOffset": 48, "endOffset": 51}, {"referenceID": 9, "context": "This contextual vector is produced by applying Latent Dirichlet Allocation [10] on preceding text.", "startOffset": 75, "endOffset": 79}, {"referenceID": 10, "context": "Several other contextual language models were later proposed by using bag-of-word [11] and RNN methods [12] to learn larger context representation that beyond the target sentence.", "startOffset": 82, "endOffset": 86}, {"referenceID": 11, "context": "Several other contextual language models were later proposed by using bag-of-word [11] and RNN methods [12] to learn larger context representation that beyond the target sentence.", "startOffset": 103, "endOffset": 107}, {"referenceID": 12, "context": "Modeling utterances in a dialog as a sequence of inputs might not well capture the pauses, turntaking, and grounding phenomena [13] in a dialog.", "startOffset": 127, "endOffset": 131}, {"referenceID": 8, "context": "Mikolov and Zweig [9] proposed a topic-conditioned RNNLM by introducing a contextual real-valued vector to RNN hidden state.", "startOffset": 18, "endOffset": 21}, {"referenceID": 9, "context": "The contextual vector was created by performing LDA [10] on preceding text.", "startOffset": 52, "endOffset": 56}, {"referenceID": 10, "context": "Wang and Cho [11] studied introducing ar X iv :1 70 1.", "startOffset": 13, "endOffset": 17}, {"referenceID": 13, "context": "[14] proposed using hierarchical RNN for document modeling.", "startOffset": 0, "endOffset": 4}, {"referenceID": 14, "context": "[15] further proposed a contextual language model that consider information at interdocument level.", "startOffset": 0, "endOffset": 4}, {"referenceID": 8, "context": "This is similar to the context dependent RNN language model proposed in [9], other than that the context vector is not connected directly to the RNN output layer.", "startOffset": 72, "endOffset": 75}, {"referenceID": 11, "context": "As in [12], we refer to this model as DRNNLM.", "startOffset": 6, "endOffset": 10}, {"referenceID": 11, "context": "Alternatively, in the CCDCLM model proposed in [12], the last RNN hidden state of the previous utterance is fed to the RNN hidden state of the target utterance at each time step.", "startOffset": 47, "endOffset": 51}, {"referenceID": 15, "context": "5-gram KN A 5-gram language model with modified Kneser-Ney smoothing [16].", "startOffset": 69, "endOffset": 73}, {"referenceID": 11, "context": "We implement this model following the design in [12].", "startOffset": 48, "endOffset": 52}, {"referenceID": 6, "context": "In this work, we use LSTM cell [7] as the basic RNN unit for its stronger capability in capturing long-range dependencies in a word sequence comparing to simple RNN.", "startOffset": 31, "endOffset": 34}, {"referenceID": 16, "context": "We use pretrained word vectors [17] that are trained on Google News dataset to initialize the word embeddings.", "startOffset": 31, "endOffset": 35}, {"referenceID": 17, "context": "mini-batch training using Adam optimization method following the suggested parameter setup in [18].", "startOffset": 94, "endOffset": 98}, {"referenceID": 18, "context": "8) on the non-recurrent connections [19] of LSTM.", "startOffset": 36, "endOffset": 40}], "year": 2017, "abstractText": "In this work, we propose contextual language models that incorporate dialog level discourse information into language modeling. Previous works on contextual language model treat preceding utterances as a sequence of inputs, without considering dialog interactions. We design recurrent neural network (RNN) based contextual language models that specially track the interactions between speakers in a dialog. Experiment results on Switchboard Dialog Act Corpus show that the proposed model outperforms conventional single turn based RNN language model by 3.3% on perplexity. The proposed models also demonstrate advantageous performance over other competitive contextual language models.", "creator": "LaTeX with hyperref package"}}}