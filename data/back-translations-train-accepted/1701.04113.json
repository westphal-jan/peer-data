{"id": "1701.04113", "review": {"conference": "ICML", "VERSION": "v1", "DATE_OF_SUBMISSION": "15-Jan-2017", "title": "Near Optimal Behavior via Approximate State Abstraction", "abstract": "The combinatorial explosion that plagues planning and reinforcement learning (RL) algorithms can be moderated using state abstraction. Prohibitively large task representations can be condensed such that essential information is preserved, and consequently, solutions are tractably computable. However, exact abstractions, which treat only fully-identical situations as equivalent, fail to present opportunities for abstraction in environments where no two situations are exactly alike. In this work, we investigate approximate state abstractions, which treat nearly-identical situations as equivalent. We present theoretical guarantees of the quality of behaviors derived from four types of approximate abstractions. Additionally, we empirically demonstrate that approximate abstractions lead to reduction in task complexity and bounded loss of optimality of behavior in a variety of environments.", "histories": [["v1", "Sun, 15 Jan 2017 21:24:45 GMT  (3873kb,D)", "http://arxiv.org/abs/1701.04113v1", "Earlier version published at ICML 2016"]], "COMMENTS": "Earlier version published at ICML 2016", "reviews": [], "SUBJECTS": "cs.LG cs.AI", "authors": ["david abel", "d ellis hershkowitz", "michael l littman"], "accepted": true, "id": "1701.04113"}, "pdf": {"name": "1701.04113.pdf", "metadata": {"source": "META", "title": "Near Optimal Behavior via Approximate State Abstraction", "authors": ["David Abel", "D. Ellis Hershkowitz", "Michael L. Littman"], "emails": ["abel@brown.edu", "dhershko@cs.cmu.edu", "mlittman@cs.brown.edu"], "sections": [{"heading": "1 Introduction", "text": "This means that we need a complete knowledge that is mathematically difficult to achieve. In addition, often no two situations are identical, so that exact abstractions are often ineffective. To overcome these problems, we examine approximate abstractions that agents can treat as identical in sufficiently similar situations. This work characterizes the effects of equating \"similar states\" in the context of planning and RL in Markov Decision Processes (MDPs). The reminders of our introduction contextualizes these intuitions in MDPs.RLSolution to Abstracted Problem, MG Abstraction Error (MG) True Problem, MG Abstraction Problem Abstraction, MAoa St Approximation."}, {"heading": "2 MDPs and Sequential Decision Making", "text": "An MDP is a problem representation for sequential decision-makers, represented by a quintuple: < S, A, T, R, \u03b3 >. Here, S is a finite state space; A is a finite series of measures available to the agent; T stands for T (s, a, s), the likelihood of an agent transitioning to the state s \u00b2 S after the application of measures a) to the state s \u00b2 S; R (s, a) stands for the reward that the agent receives for executing measures a to the state s; T [0, 1] is a discount factor that determines how much the agent prefers future rewards to immediate rewards. We assume, without loss of universality, that the range of all reward functions is normalized to [0, 1]. The solution for an MDP is referred to as politics, referred to as politics: S 7 \u2192 A. The goal of an agent is to resolve the policy that maximizes its expected reward from each state."}, {"heading": "3 Related Work", "text": "Several other projects have taken up similar themes."}, {"heading": "3.1 Approximate State Abstraction", "text": "They are developing an algorithm called Interval Value Iteration (IVI) that approaches the correct boundaries of a family of abstract MDPs called Bounded MDPs. Several approaches rely on Dean et al. [9]. Ferns et al. [14, 15] examined state metrics for MDPs; they limited the value difference between soil states and abstract states for multiple bisimulation metrics that induce an abstract MDP. This differs from our work, which develops a theory of abstraction that limits the suboptimal application of an abstract MDP to its soil MDP, covering four types of state abstraction, one of which closely parallels bisimulation."}, {"heading": "3.2 Specific Abstraction Algorithms", "text": "Andre and Russell [2] investigated a method of state abstraction in hierarchical reinforcement learning using a programming language called ALISP that promotes the notion of safe state abstraction. Agents programmed with ALISP can ignore irrelevant parts of the state and achieve abstractions that maintain optimizability. Dietterich [12] developed MAXQ, a framework for composing tasks into an abstract hierarchy in which state aggregation can be applied. Bakker and Schmidhuber [4] also aim at hierarchical abstraction, focusing on the discovery of sub-targets. Jong and Stone [21] introduced a method called political irrelevance, in which agents (online) identify which state variables can be safely stratified in a factually aggregated MDP. Dayan and Hinton [7] develop \"Feudal Reinforcement Learning,\" which represents an early form of hierarchical RDP that is administered in a factorally structured MDP task."}, {"heading": "3.3 Exact Abstraction Framework", "text": "Li et al. [24] developed a framework for the exact state abstraction of MDPs. Specifically, the authors defined five types of state aggregation functions inspired by existing methods of state aggregation of MDPs. We generalize two of these five types, \u03c6Q * and \u03c6model, to the approximate abstraction case. Our generalizations correspond to theirs when using exact criteria (e.g. \u03b5 = 0). Furthermore, our limitations when using exact criteria indicate that no value is lost, which is one of the core findings of Li et al. [24]. Walsh et al. [34] build on the framework they have previously developed by empirically demonstrating how abstractions can be transferred between structurally related MDPs."}, {"heading": "4 Abstraction Notation", "text": "We build on the notation used by Li et al. [24], who introduced a unifying theoretical framework for state abstraction in MDP. Definition 1 (MG, MA): Definition 1 (MG, MA): This figure induces an abstract MDP. Let's let's let MG = < SG, A, TG, RG, \u03b3 > and MA = < SA, A, TA, RA, \u03b3 >. Definition 2 (SA, BO): The states in the abstract MDP are constructed by applying a state aggregation function to the states in the earth MDP, SA. More specifically, let's mould a state in the earth MDP into a state. Definition 2 (SA, BO): The states in the abstract MDP: The states in the abstract MDP are constructed by applying a state aggregation function to the states in the earth MDP, SA."}, {"heading": "5 Approximate State Abstraction", "text": "Here we present our formal analysis of the preliminary abstraction of states, including the results limiting the error associated with these abstraction methods. In particular, we show that abstractions based on approximate state similarity (5,2), approximate model similarity (5,3), and approximate similarity between distributions via Q * produce abstract MDPs for both Boltzmann (5,4) and multinomial (5,5) distributions, for which optimal politics produce errors based on MDP. First, we provide some additional notation.Definition 7 (\u03c0 A, \u03c0 \u0432 G): We leave \u03c0 A: SA \u2192 A and mixed states G: SG \u2192 A stand for optimal politics in the abstract and basic MDP. We are interested in how optimal politics work in the abstract MDP."}, {"heading": "5.1 Main Result", "text": "D, \"D,\" D, \"D,\" D, \"D,\" D, \"D,\" D, \"D,\" D, \"D,\" D, \"D,\" D, \"D,\" D, \"D,\" D, \"D,\" D, \"D,\" D, \"D,\" D, \"D,\" D, \"D,\" D, \"D,\" D, \"D,\" D, \"D,\" D, \"D,\" D, \"D,\" D, \"D,\" D, \"D,\" D, \"D,\" D, \"D,\" D, \"D,\" D, \"D,\" D, \"D,\" D, \"D,\" D, \"D,\" D, \"D,\" D, \"D,\" D, \"D,\" D, \"D,\" D, \"D,\" D, \"D,\" D, \"D,\" D, \"D,\" D, \"D,\" D, \"D,\" D, \"D,\" D, \"D, D,\" D, \"D,\" D, D, \"D, D,\" D, D, D, \"D, D,\" D, D, D, D, \"D, D, D, D,\" D, D, D, D, D, D, D, \"D, D, D, D, D, D, D, D,\" D, D, D, D, D, D, D, D, D, D, D, D, D, D, D \"D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D,\" D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, \"D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D,"}, {"heading": "5.2 Optimal Q Function: \u03c6\u0303Q\u2217,\u03b5", "text": "We consider an approximate version of Li et al. [24] s throught action in the abstract MDP. (D) In our abstraction, the states are summarized if their optimal Q values within \u03b5.Definition 12 (D) \u00b7 Q-Function Abstraction has the same form as Equation 9: \"Q-Value,\" \u03b5 (s1) = \"Q-Value,\" \u03b5 (s2) \u00b7 G-Value (s1) \u00b7 \"QG-Value (s2, a).\" (11) Lemma 1. If an abstraction is used to generate the abstract MDP values (s2) \u00b7 S-Value (s2) \u00b7 G (s1, a). (12) Proof Lemma 1: First, we show that the Q values in the abstract MDP are close to the Q values in the soil MDP (Claim 1)."}, {"heading": "5.3 Model Similarity: \u03c6\u0303model,\u03b5", "text": "Let us now consider an approximate version of the QII model by Li et al. [24], in which the states are summarized when their rewards and transitions lie within the QII model. \u2212 Definition 13 (\"model,\" \"model\"): We leave \"model,\" \"model,\" \"model,\" \"model,\" \"model,\" \"model,\" \"model,\" \"model,\" \"model,\" \"model,\" \"model,\" \"model,\" \"model,\" \"model,\" \"model,\" \"model. (24) Lemma 2. When SA is created using a model,\" \"model,\" \"model,\" \"model,\" \"model,\" \"\" model, \"\" \"model,\" \"model,\" \"\" model, \"\" \"model,\" \"\" model, \"\" \"model,\" \"\", \"\" \"\" \"model,\" \"\" \",\" \"\" \"\" model, \"\" \"\" \",\" \"\" \"\" \"\" model, \"\" \"\" \"\" \"\" model, \"\" \"\" \"\" \"."}, {"heading": "5.4 Boltzmann over Optimal Q: \u03c6\u0303bolt,\u03b5", "text": "Here we introduce the following types of abstractions: (s1), (s2), (s2), (s2), (s2), (s2), (s2), (s2), (s2), (s2), (s2), (s2), (s2), (s2), (s2), (s2), (s2), (s2), (s2), (s2), (s2), (s2), (s2), (s2), (s2), (s2), (s2), (s2), (s2), (s2), (s2), (s2), (s2), (2), (s2), (s2), (s2), (s2), (s2), (s2), (s2), (s2), (s2, (s2), (s2), (s2), (s2), (s2, (s2), (s2), (s2), (s2), (s2), (s2, (s2), (s2), (s2), (s2), (s2, (s2), (s2), (s2), (s2), (s2), (s2), (s2), (s2), (s2), (s2), (s2), (s2), (s2), (s2), (s2), (s2), (s2), (s2), (s2), (s2), (s2), (s2), (s2), (s2), s2, (s2), (s2), (s2, s2), (s2, s2), s2, s2, (, s2), (, (, s2), s2, s2, (, s2), s2), (, s2), (, s2), s2, (, s2), (, s2"}, {"heading": "5.5 Multinomial over Optimal Q: \u03c6\u0303mult,\u03b5", "text": "We consider approximate abstractions derived from a multinomial distribution over Q * for reasons similar to the Boltzmann distribution. Moreover, multinomial distribution is attractive because of its simplicity. Definition 15 (? mult,?): We let? mult,? define a kind of abstraction that, for fixed \u03b5,? mult,? (s1) =? mult,? (s2)? a),? bQG (s1, b) \u2212 QG (s1, a),? bQG (s1, b),? bQG (s1, b). (38) We also assume that the difference in the normalizing terms is limited by a non-negative constant, kmult, or R."}, {"heading": "6 Example Domains", "text": "We apply approximate abstraction to five sample domains - NChain, Upworld, Taxi, Minefield, and Random. These domains have been chosen for their diversity - NChain is relatively simple, Upworld is particularly vivid of the power of abstraction, Taxi is goal-based and hierarchical, Minefield is stochastical, and Random MDP has many near-optimal policies. Our code base 1 provides implementations for abstracting arbitrary MDPs and visualizing and evaluating the resulting abstract MDPs. We use the GraphStream graph visualization library [29] and the planning and RL library BURLAP2. For all experiments, we set \u03b3 to 0.95.1https: / / github.com / david-abel / state _ abstraction 2http: / / / burlap.cs.brown.edu /"}, {"heading": "6.1 Visualizations", "text": "A gray circle indicates a state, and colored arrows indicate transitions. The thickness of the arrow indicates how much reward is associated with that transition. In the soil MDPs, the states are marked with a number. In the abstract MDPs, we indicate which basic states have collapsed on each abstract state by labeling the abstract states with their basic states."}, {"heading": "6.1.1 NChain", "text": "In our implementation, we set N = 10, normalize rewards between 0 and 1 and use a slip probability of 0.2. An NChain instance (N = 10) and its abstraction are visualized in Figure 6.1.2. In all states, two actions are available to the agent: Progress along the chain or Return to State 0. The agent receives 0.2 reward for returning to State 0 and no reward for descending the chain. The exception is that when the agent passes into the last state of the chain, he receives a reward of 1.0. Transitions also have a low slip probability, so the action applied leads to an opposite dynamic. In our implementation, we set N = 10 and B = 0.2."}, {"heading": "6.1.2 Upworld", "text": "The agent receives a positive reward for the transition to any state at the top of the grid, where the agent starts in the lower left corner, and the agent receives 0 reward for all other transitions. Consequently, the ascent is always the optimal action, since the movement to the left and right does not turn the Manhatanic distance of the agent into positive reward. During the experiment, we set N = 10, M = 4. An upworld instance (N = 10, M = 4) and its abstraction are illustrated in Figure 6.1.2.Upworld illustrates a convincing property in relation to state abstraction: The optimal exact Q abstraction (if \u03b5 = 0) can always construct an abstract MDP with | SA | = N, the height of the grid, without changing the value of the optimal policy. As a result, Upworld provides an arbitrary reduction of the size of the MDP to the same value as the optimal policy."}, {"heading": "6.1.3 Taxi", "text": "Taxis have long been studied in the hierarchical RL literature [12]. The agent operating in Grid World style [30] can move left, right, up and down, pick up and drop off a passenger, and the destination is reached when the agent has brought all passengers to their destination. We visualize the compression on a simple 626 taxi instance in Figure 3. As mentioned above, we visualize the original taxi problem in a graphical representation, so that despite its unnatural appearance, we can visualize both the MDP and the abstract MDP in the same format."}, {"heading": "6.1.4 Minefield", "text": "Minefield is a test problem we introduced that uses the Grid World dynamics of Russell and Norvig [30] with a slip probability of x. The reward function is such that ascent to the top row of the grid receives a reward of 1.0; all other transitions receive a reward of 0.2, except for transitions to a random group of \u0445 mine states (which may include the top row) that receive a reward of 0."}, {"heading": "6.1.5 Random MDP", "text": "In the random MDP domain we are looking at, there are 100 states and 3 actions. For each state, each action passes into one of two randomly selected states with a 0.5 probability. The random MDP and its compression are shown in Figure 4."}, {"heading": "7 Empirical Results", "text": "We have conducted experiments with the number of abstract states, with the aggregation functions of the MDP type being roughly the same. We provide results for only DP types that need to be reported via the Q Values column, because, as our evidence in Section 5 shows, the other three functions are reducible to certain DP types. To illustrate what types of approximations are possible, we have built up each abstraction by first solving the MDP, then greedily aggregating soil states into abstract states that meet the Q values, \u03b5 criteria. Since this approach is an order-dependent approximation to the maximum amount of abstraction, we have randomized the order in which states were considered via studies. Each soil state is equally weighted in its abstract state. For each domain, we report two quantities as a function of Epsilon with 95% confidence bars. First, we compare the number of states in the abstract MDP for different values of the world, the number of the abstract states in the 6, and the smaller the number of the states in the figure of the 6."}, {"heading": "8 Conclusion", "text": "Approximate abstraction in MDPs offers considerable advantages over exact abstraction. First, approximate abstraction is based on criteria that we assume a planning or learning algorithm can learn without solving the full MDP. Second, approximate abstractions can achieve a higher degree of compression due to their relaxed equality criteria. Third, methods that use approximate aggregation techniques are able to optimize the aggressiveness of abstraction while causing limited errors at the same time. In this work, we have shown limitations on the value that is lost when we behave according to the optimal policy of abstract MDP, and empirically demonstrated that approximate abstractions can reduce the size of government space with little loss of the quality of behavior. First, we offer a code base that provides implementations to abstract, visualize, and evaluate an arbitrary MDP in order to promote future work in many non-hazardous directions."}], "references": [{"title": "Goal-based action priors", "author": ["David Abel", "David Ellis Hershkowitz", "Gabriel Barth-Maron", "Stephen Brawner", "Kevin O\u2019Farrell", "James MacGlashan", "Stefanie Tellex"], "venue": "In ICAPS,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2015}, {"title": "State abstraction for programmable reinforcement learning agents", "author": ["David Andre", "Stuart J Russell"], "venue": "In AAAI/IAAI,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2002}, {"title": "Near-optimal regret bounds for reinforcement learning", "author": ["Peter Auer", "Thomas Jaksch", "Ronald Ortner"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2009}, {"title": "Hierarchical reinforcement learning based on subgoal discovery and subpolicy specialization", "author": ["Bram Bakker", "J\u00fcrgen Schmidhuber"], "venue": "In Proc. of the 8-th Conf. on Intelligent Autonomous Systems,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2004}, {"title": "Recent advances in hierarchical reinforcement learning", "author": ["Andrew G Barto", "Sridhar Mahadevan"], "venue": "Discrete Event Dynamic Systems,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2003}, {"title": "Dynamic programming aggregation", "author": ["James C Bean", "John R Birge", "Robert L Smith"], "venue": "Operations Research,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2011}, {"title": "Feudal Reinforcement Learning", "author": ["Peter Dayan", "Geoffrey Hinton"], "venue": "Advances in neural information processing systems,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 1993}, {"title": "Model minimization in markov decision processes", "author": ["Thomas Dean", "Robert Givan"], "venue": "In AAAI/IAAI,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 1997}, {"title": "Model reduction techniques for computing approximately optimal solutions for markov decision processes", "author": ["Thomas Dean", "Robert Givan", "Sonia Leach"], "venue": "In Proceedings of the Thirteenth Conference on Uncertainty in Artificial Intelligence,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 1997}, {"title": "Abstraction and approximate decision-theoretic planning", "author": ["Richard Dearden", "Craig Boutilier"], "venue": "Artificial Intelligence,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 1997}, {"title": "Bayesian Q-learning", "author": ["Richard Dearden", "Nir Friedman", "Stuart Russell"], "venue": "In AAAI/IAAI,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 1998}, {"title": "Hierarchical reinforcement learning with the MAXQ value function decomposition", "author": ["Thomas G Dietterich"], "venue": "Journal of Artificial Intelligence Research,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2000}, {"title": "Approximate equivalence of Markov decision processes", "author": ["Eyal Even-Dar", "Yishay Mansour"], "venue": "In Learning Theory and Kernel Machines,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2003}, {"title": "Metrics for finite markov decision processes", "author": ["Norm Ferns", "Prakash Panangaden", "Doina Precup"], "venue": "In Proceedings of the 20th conference on Uncertainty in artificial intelligence,", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2004}, {"title": "Methods for computing state similarity in markov decision processes", "author": ["Norman Ferns", "Pablo Samuel Castro", "Doina Precup", "Prakash Panangaden"], "venue": "Proceedings of the 22nd conference on Uncertainty in artificial intelligence,", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2006}, {"title": "State Aggregation in Monte Carlo Tree Search", "author": ["Jesse Hostetler", "Alan Fern", "Tom Dietterich"], "venue": "Aaai 2014,", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2014}, {"title": "Extreme state aggregation beyond mdps", "author": ["Marcus Hutter"], "venue": "In International Conference on Algorithmic Learning Theory,", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2014}, {"title": "Extreme state aggregation beyond markov decision processes", "author": ["Marcus Hutter"], "venue": "Theoretical Computer Science,", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2016}, {"title": "Abstraction Selection in Model-Based Reinforcement Learning", "author": ["Nan Jiang"], "venue": "icml,", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2015}, {"title": "Improving uct planning via approximate homomorphisms", "author": ["Nan Jiang", "Satinder Singh", "Richard Lewis"], "venue": "In Proceedings of the 2014 international conference on Autonomous agents and multi-agent systems,", "citeRegEx": "20", "shortCiteRegEx": "20", "year": 2014}, {"title": "State abstraction discovery from irrelevant state variables", "author": ["Nicholas K Jong", "Peter Stone"], "venue": "In IJCAI,", "citeRegEx": "21", "shortCiteRegEx": "21", "year": 2005}, {"title": "Reinforcement learning: A survey", "author": ["Leslie Pack Kaelbling", "Michael L Littman", "Andrew W Moore"], "venue": "Journal of Artificial Intelligence Research,", "citeRegEx": "22", "shortCiteRegEx": "22", "year": 1996}, {"title": "Bandit based monte-carlo planning", "author": ["Levente Kocsis", "Csaba Szepesv\u00e1ri"], "venue": "In European conference on machine learning,", "citeRegEx": "23", "shortCiteRegEx": "23", "year": 2006}, {"title": "Towards a unified theory of state abstraction for mdps", "author": ["Lihong Li", "Thomas J Walsh", "Michael L Littman"], "venue": "In ISAIM,", "citeRegEx": "24", "shortCiteRegEx": "24", "year": 2006}, {"title": "On the complexity of solving Markov decision problems", "author": ["Michael L Littman", "Thomas L Dean", "Leslie Pack Kaelbling"], "venue": "In Proceedings of the Eleventh Conference on Uncertainty in Artificial Intelligence,", "citeRegEx": "25", "shortCiteRegEx": "25", "year": 1995}, {"title": "Efficient bayesian clustering for reinforcement learning", "author": ["Travis Mandel", "Yun-En Liu", "Emma Brunskill", "Zoran Popovic"], "venue": null, "citeRegEx": "26", "shortCiteRegEx": "26", "year": 2016}, {"title": "Adaptive aggregation for reinforcement learning in average reward Markov decision processes", "author": ["Ronald Ortner"], "venue": "Annals of Operations Research,", "citeRegEx": "27", "shortCiteRegEx": "27", "year": 2013}, {"title": "The complexity of Markov decision processes", "author": ["Christos H Papadimitriou", "John N Tsitsiklis"], "venue": "Mathematics of Operations Research,", "citeRegEx": "28", "shortCiteRegEx": "28", "year": 1987}, {"title": "Graphstream: A tool for bridging the gap between complex systems and dynamic graphs", "author": ["Yoann Pign\u00e9", "Antoine Dutot", "Fr\u00e9d\u00e9ric Guinand", "Damien Olivier"], "venue": null, "citeRegEx": "29", "shortCiteRegEx": "29", "year": 2008}, {"title": "Artificial Intelligence A Modern Approach", "author": ["Stuart Russell", "Peter Norvig"], "venue": "Prentice-Hall, Englewood Cliffs,", "citeRegEx": "30", "shortCiteRegEx": "30", "year": 1995}, {"title": "Reinforcement Learning in Finite MDPs : PAC Analysis", "author": ["Alexander L. Strehl", "Lihong Li", "Michael L. Littman"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "31", "shortCiteRegEx": "31", "year": 2009}, {"title": "Reinforcement Learning: An Introduction", "author": ["Richard S Sutton", "Andrew G Barto"], "venue": null, "citeRegEx": "32", "shortCiteRegEx": "32", "year": 1998}, {"title": "Between MDPs and semi-MDPs: A framework for temporal abstraction in reinforcement learning", "author": ["Richard S Sutton", "Doina Precup", "Satinder Singh"], "venue": "Artificial Intelligence,", "citeRegEx": "33", "shortCiteRegEx": "33", "year": 1999}, {"title": "Transferring state abstractions between mdps", "author": ["Thomas J Walsh", "Lihong Li", "Michael L Littman"], "venue": "In ICML Workshop on Structural Knowledge Transfer for Machine Learning,", "citeRegEx": "34", "shortCiteRegEx": "34", "year": 2006}], "referenceMentions": [{"referenceID": 27, "context": "Solving for optimal behavior in MDPs in a planning setting is known to be P-Complete in the size of the state space [28, 25].", "startOffset": 116, "endOffset": 124}, {"referenceID": 24, "context": "Solving for optimal behavior in MDPs in a planning setting is known to be P-Complete in the size of the state space [28, 25].", "startOffset": 116, "endOffset": 124}, {"referenceID": 30, "context": "Similarly, many RL algorithms for solving MDPs are known to require a number of samples polynomial in the size of the state space [31].", "startOffset": 130, "endOffset": 134}, {"referenceID": 0, "context": "For instance, a robot involved in a pick-and-place task might be able to employ planning algorithms to solve for how to manipulate some objects into a desired configuration in time polynomial in the number of states, but the number of states it must consider grows exponentially with the number of objects with which it is working [1].", "startOffset": 331, "endOffset": 334}, {"referenceID": 1, "context": "Thus, a key research agenda for planning and RL is leveraging abstraction to reduce large state spaces [2, 21, 10, 12, 6].", "startOffset": 103, "endOffset": 121}, {"referenceID": 20, "context": "Thus, a key research agenda for planning and RL is leveraging abstraction to reduce large state spaces [2, 21, 10, 12, 6].", "startOffset": 103, "endOffset": 121}, {"referenceID": 9, "context": "Thus, a key research agenda for planning and RL is leveraging abstraction to reduce large state spaces [2, 21, 10, 12, 6].", "startOffset": 103, "endOffset": 121}, {"referenceID": 11, "context": "Thus, a key research agenda for planning and RL is leveraging abstraction to reduce large state spaces [2, 21, 10, 12, 6].", "startOffset": 103, "endOffset": 121}, {"referenceID": 5, "context": "Thus, a key research agenda for planning and RL is leveraging abstraction to reduce large state spaces [2, 21, 10, 12, 6].", "startOffset": 103, "endOffset": 121}, {"referenceID": 23, "context": "Existing work has characterized how exact abstractions can fully maintain optimality in MDPs [24, 8].", "startOffset": 93, "endOffset": 100}, {"referenceID": 7, "context": "Existing work has characterized how exact abstractions can fully maintain optimality in MDPs [24, 8].", "startOffset": 93, "endOffset": 100}, {"referenceID": 0, "context": "Here, S is a finite state space; A is a finite set of actions available to the agent; T denotes T (s, a, s\u2032), the probability of an agent transitioning to state s\u2032 \u2208 S after applying action a \u2208 A in state s \u2208 S; R(s, a) denotes the reward received by the agent for executing action a in state s; \u03b3 \u2208 [0, 1] is a discount factor that determines how much the agent prefers future rewards over immediate rewards.", "startOffset": 300, "endOffset": 306}, {"referenceID": 0, "context": "We assume without loss of generality that the range of all reward functions is normalized to [0, 1].", "startOffset": 93, "endOffset": 99}, {"referenceID": 21, "context": "[22].", "startOffset": 0, "endOffset": 4}, {"referenceID": 8, "context": "[9] leverage the notion of bisimulation to investigate partitioning an MDP\u2019s state space into clusters of states whose transition model and reward function are within \u03b5 of each other.", "startOffset": 0, "endOffset": 3}, {"referenceID": 8, "context": "[9].", "startOffset": 0, "endOffset": 3}, {"referenceID": 13, "context": "[14, 15] investigated state similarity metrics for MDPs; they bounded the value difference of ground states and abstract states for several bisimulation metrics that induce an abstract MDP.", "startOffset": 0, "endOffset": 8}, {"referenceID": 14, "context": "[14, 15] investigated state similarity metrics for MDPs; they bounded the value difference of ground states and abstract states for several bisimulation metrics that induce an abstract MDP.", "startOffset": 0, "endOffset": 8}, {"referenceID": 12, "context": "Even-Dar and Mansour [13] analyzed different distance metrics used in identifying state space partitions subject to \u03b5-similarity, also providing value bounds (their Lemma 4) for \u03b5-homogeneity subject to the L\u221e norm, which parallels our Claim 2.", "startOffset": 21, "endOffset": 25}, {"referenceID": 26, "context": "Ortner [27] developed an algorithm for learning partitions in an online setting by taking advantage of the confidence bounds for T and R provided by UCRL [3].", "startOffset": 7, "endOffset": 11}, {"referenceID": 2, "context": "Ortner [27] developed an algorithm for learning partitions in an online setting by taking advantage of the confidence bounds for T and R provided by UCRL [3].", "startOffset": 154, "endOffset": 157}, {"referenceID": 17, "context": "Hutter [18, 17] investigates state aggregation beyond the MDP setting.", "startOffset": 7, "endOffset": 15}, {"referenceID": 16, "context": "Hutter [18, 17] investigates state aggregation beyond the MDP setting.", "startOffset": 7, "endOffset": 15}, {"referenceID": 23, "context": "[24] that Q\u2217 abstractions preserve the optimal value function.", "startOffset": 0, "endOffset": 4}, {"referenceID": 15, "context": "[16] apply state abstraction to Monte Carlo Tree Search and expectimax search, giving value bounds of applying the optimal abstract action in the ground tree(s), similarly to our setting.", "startOffset": 0, "endOffset": 4}, {"referenceID": 9, "context": "Dearden and Boutilier [10] also formalize stateabstraction for planning, focusing on abstractions that are quickly computed and offer bounded value.", "startOffset": 22, "endOffset": 26}, {"referenceID": 19, "context": "[20] analyze a similar setting, applying abstractions to the Upper Confidence Bound applied to Trees algorithm adapted for planning, introduced by Kocsis and Szepesv\u00e1ri [23].", "startOffset": 0, "endOffset": 4}, {"referenceID": 22, "context": "[20] analyze a similar setting, applying abstractions to the Upper Confidence Bound applied to Trees algorithm adapted for planning, introduced by Kocsis and Szepesv\u00e1ri [23].", "startOffset": 169, "endOffset": 173}, {"referenceID": 25, "context": "[26] advance Bayesian aggregation in RL to define Thompson Clustering for Reinforcement Learning (TCRL), an extension of which achieves near-optimal Bayesian regret bounds.", "startOffset": 0, "endOffset": 4}, {"referenceID": 18, "context": "Jiang [19] analyze the problem of choosing between two candidate abstractions.", "startOffset": 6, "endOffset": 10}, {"referenceID": 1, "context": "Andre and Russell [2] investigated a method for state abstraction in hierarchical reinforcement learning leveraging a programming language called ALISP that promotes the notion of safe state abstraction.", "startOffset": 18, "endOffset": 21}, {"referenceID": 11, "context": "Dietterich [12] developed MAXQ, a framework for composing tasks into an abstracted hierarchy where state aggregation can be applied.", "startOffset": 11, "endOffset": 15}, {"referenceID": 3, "context": "Bakker and Schmidhuber [4] also target hierarchical abstraction, focusing on subgoal discovery.", "startOffset": 23, "endOffset": 26}, {"referenceID": 20, "context": "Jong and Stone [21] introduced a method called policy-irrelevance in which agents identify (online) which state variables may be safely abstracted away in a factored-state MDP.", "startOffset": 15, "endOffset": 19}, {"referenceID": 6, "context": "Dayan and Hinton [7] develop \u201cFeudal Reinforcement Learning\u201d which presents an early form of hierarchical RL that restructures Q-Learning to manage the decomposition of a task into subtasks.", "startOffset": 17, "endOffset": 20}, {"referenceID": 23, "context": "[24], and for a survey of early works on hierarchical reinforcement learning, see Barto and Mahadevan [5].", "startOffset": 0, "endOffset": 4}, {"referenceID": 4, "context": "[24], and for a survey of early works on hierarchical reinforcement learning, see Barto and Mahadevan [5].", "startOffset": 102, "endOffset": 105}, {"referenceID": 23, "context": "[24] developed a framework for exact state abstraction in MDPs.", "startOffset": 0, "endOffset": 4}, {"referenceID": 23, "context": "[24].", "startOffset": 0, "endOffset": 4}, {"referenceID": 33, "context": "[34] build on the framework they previously developed by showing empirically how to transfer abstractions between structurally related MDPs.", "startOffset": 0, "endOffset": 4}, {"referenceID": 23, "context": "[24], who introduced a unifying theoretical framework for state abstraction in MDPs.", "startOffset": 0, "endOffset": 4}, {"referenceID": 0, "context": "The only restriction placed on the weighting scheme is that it induces a probability distribution on the ground states of each abstract state: \u2200s \u2208 SG \uf8eb\uf8ed \u2211 s\u2208G(s) \u03c9(s) \uf8f6\uf8f8 = 1 AND \u03c9(s) \u2208 [0, 1].", "startOffset": 186, "endOffset": 192}, {"referenceID": 0, "context": "Definition 5 (RA): The abstract reward function RA : SA \u00d7A 7\u2192 [0, 1] is a weighted sum of the rewards of each of the ground states that map to the same abstract state: RA(s, a) = \u2211 g\u2208G(s) RG(g, a)\u03c9(g).", "startOffset": 62, "endOffset": 68}, {"referenceID": 0, "context": "Definition 6 (TA): The abstract transition function TA : SA \u00d7 A \u00d7 SA 7\u2192 [0, 1] is a weighted sum of the transitions of each of the ground states that map to the same abstract state: TA(s, a, s\u2032) = \u2211 g\u2208G(s) \u2211 g\u2032\u2208G(s\u2032) TG(g, a, g\u2032)\u03c9(g).", "startOffset": 72, "endOffset": 78}, {"referenceID": 0, "context": "j eG2j \u2223\u2223\u2223\u2223\u2223 \u2264 kbolt \u00d7 \u03b5 Naturally, the value bound of Equation 10 is meaningless for 2\u03b5\u03b7f \u2265 RMax 1\u2212\u03b3 = 1 1\u2212\u03b3 , since this is the maximum possible value in any MDP (and we assumed the range of R is [0, 1]).", "startOffset": 198, "endOffset": 204}, {"referenceID": 23, "context": "[24]\u2019s \u03c6Q\u2217 .", "startOffset": 0, "endOffset": 4}, {"referenceID": 23, "context": "[24]\u2019s \u03c6model, where states are aggregated together when their rewards and transitions are within \u03b5.", "startOffset": 0, "endOffset": 4}, {"referenceID": 31, "context": "This type of abstractions is appealing as Boltzmman distributions balance exploration and exploitation [32].", "startOffset": 103, "endOffset": 107}, {"referenceID": 28, "context": "We use the graph-visualization library GraphStream [29] and the planning and RL library, BURLAP.", "startOffset": 51, "endOffset": 55}, {"referenceID": 10, "context": "1 NChain NChain is a simple MDP investigated in the Bayesian RL literature due to the interesting exploration problem it poses [11].", "startOffset": 127, "endOffset": 131}, {"referenceID": 11, "context": "3 Taxi Taxi has long been studied by the hierarchical RL literature [12].", "startOffset": 68, "endOffset": 72}, {"referenceID": 29, "context": "The agent, operating in a Grid World style domain [30], may move left, right, up, and down, as well as pick up a passenger and drop off a passenger.", "startOffset": 50, "endOffset": 54}, {"referenceID": 29, "context": "4 Minefield Minefield is a test problem we are introducing that uses the Grid World dynamics of Russell and Norvig [30] with slip probability of x.", "startOffset": 115, "endOffset": 119}, {"referenceID": 26, "context": "First, we are interested in extending the approach of Ortner [27] by learning the approximate abstraction functions introduced in this paper online in the planning or RL setting, particularly when the agent must solve a collection of related MDPs.", "startOffset": 61, "endOffset": 65}], "year": 2017, "abstractText": "The combinatorial explosion that plagues planning and reinforcement learning (RL) algorithms can be moderated using state abstraction. Prohibitively large task representations can be condensed such that essential information is preserved, and consequently, solutions are tractably computable. However, exact abstractions, which treat only fully-identical situations as equivalent, fail to present opportunities for abstraction in environments where no two situations are exactly alike. In this work, we investigate approximate state abstractions, which treat nearly-identical situations as equivalent. We present theoretical guarantees of the quality of behaviors derived from four types of approximate abstractions. Additionally, we empirically demonstrate that approximate abstractions lead to reduction in task complexity and bounded loss of optimality of behavior in a variety of environments.", "creator": "LaTeX with hyperref package"}}}