{"id": "1701.04128", "review": {"conference": "NIPS", "VERSION": "v1", "DATE_OF_SUBMISSION": "15-Jan-2017", "title": "Understanding the Effective Receptive Field in Deep Convolutional Neural Networks", "abstract": "We study characteristics of receptive fields of units in deep convolutional networks. The receptive field size is a crucial issue in many visual tasks, as the output must respond to large enough areas in the image to capture information about large objects. We introduce the notion of an effective receptive field, and show that it both has a Gaussian distribution and only occupies a fraction of the full theoretical receptive field. We analyze the effective receptive field in several architecture designs, and the effect of nonlinear activations, dropout, sub-sampling and skip connections on it. This leads to suggestions for ways to address its tendency to be too small.", "histories": [["v1", "Sun, 15 Jan 2017 23:52:49 GMT  (496kb,D)", "http://arxiv.org/abs/1701.04128v1", null], ["v2", "Wed, 25 Jan 2017 06:32:29 GMT  (554kb,D)", "http://arxiv.org/abs/1701.04128v2", null]], "reviews": [], "SUBJECTS": "cs.CV cs.AI cs.LG", "authors": ["wenjie luo", "yujia li", "raquel urtasun", "richard s zemel"], "accepted": true, "id": "1701.04128"}, "pdf": {"name": "1701.04128.pdf", "metadata": {"source": "CRF", "title": "Understanding the Effective Receptive Field in Deep Convolutional Neural Networks", "authors": ["Wenjie Luo", "Yujia Li", "Raquel Urtasun", "Richard Zemel"], "emails": ["zemel}@cs.toronto.edu"], "sections": [{"heading": "1 Introduction", "text": "In fact, it is the case that most people who have come to the United States in recent years are able to retreat to the United States in order to find a new home there. (...) It is not that they want to retreat to the United States. (...) It is not that they want to retreat to the United States. (...) It is that they want to retreat to the United States. (...) It is not that they want to retreat to the United States. (...) It is that they want to retreat to the United States. (...) It is that they want to retreat to the United States. \"(...) It is not that they want to retreat to the United States.\" (...) It is not that they want to go to the United States. \"(...) It is not that they want to live in the United States. (...) It is not that they want to live in the United States."}, {"heading": "2 Properties of Effective Receptive Fields", "text": "To simplify the notation, let's consider only a single channel at each level, but similar results can easily be derived for revolutionary layers with more input and output channels. Let's say the pixels at each level are indexed by (i, j), with their center at (0, 0). Let's call the (i, j) th pixel on the pth layer xpi, j, with x 0 i, j as input to the network, and yi, j n i, j as output to the nth layer. Let's call the (i, j) th pixel on the pth layer xpi, j i, j as input to the network, and yi, j as output to the nth layer. Let's measure how much each x0i, j, j, j contributes to the pth layer at y0.0. Let's define the effective receptive field (ERF) of this central output unit as having a region that does not neglect an explicit effect on a pixel."}, {"heading": "2.1 The simplest case: a stack of convolutional layers of weights all equal to one", "text": "In this analysis, we ignore the distortions at all levels. We start with the analysis of folding cores with weights that are all equal. Denote g (i, j, p) is the desired gradient image of the input. The process of propagation effectively proceeds in volume volumes g (, p) with the k \u00b7 k kernel to obtain g (, p \u2212 1) for each p. In this particular case, the kernel is a k \u00b7 k matrix of 1, so that the 2D conversion can be decomposed into the product of two 1D convolutions."}, {"heading": "2.2 Random weights", "text": "Let us now consider the case of random weights. In general, we have (i, j, p \u2212 1) = k \u2212 1 \u2211 a = 0 k \u2212 1 \u2211 b = 0 wpa, bg (i + a, i + b, p) (6) with pixel indices correctly displaced for clarity, and wpa, b is the folding weight at (a, b) in the folding core at level p. At each level, the initial weights become independent of a fixed distribution with mean and variance C. We assume that the gradients g are independent of the weights. This assumption generally does not apply if the network contains non-linearity, but for linear networks these assumptions apply. Since Ew [wpa, b] = 0, we can then calculate the expectancy Ew, the input [g (i, j, p \u2212 1) = Var, the variance as a variance."}, {"heading": "2.3 Non-uniform kernels", "text": "More generally, each point in the core window can have different weights, or as in the case of random weight, they can have different variances. Let's look again at case 1D, u (t) = \u03b4 (t) as before, and the core signal v (t) = 1 m = 0 w (m) sian (t \u2212 m) \u03b4 (t \u2212 m), where w (m) is the weight for the first pixel in the kernel. Without loss of generality, we can assume that the weights are normalized, i.e. that we apply the Fourier transformation and convolution theorem as before, we getU (n) \u00b7 V (n) \u00b7 V (n) = (k \u2212 1), m = 0 w (m) e \u2212 jm) n (9) the space domain signal o (t) is again the coefficient of the e \u2212 jaterm in expansion; the only difference is that the e \u2212 jaterm terms are weighted by w (m)."}, {"heading": "2.4 Nonlinear activation functions", "text": "Nonlinear activation functions are an integral part of any neural mesh. We use \u03c3 to represent an arbitrary nonlinear activation function. As we move forward, the pixels on each plane are guided first through \u03c3 and then intertwined with the convolution core to calculate the next plane. Although this sequence of operations is somewhat non-standard, it corresponds to the more usual sequence of entanglement first and traversing the nonlinearity and facilitates analysis. In this case, the reverse will be somewhat easier (i, j, p \u2212 1) = \u03c3pi, j \u2032 k \u2212 1 \u0445 a = 0 k \u2212 1 \u2211 b = 0 wpa, bg (i + a, i + b, p) (12), where we misused the notation and used the dynamics, j \u2032 to represent the gradient of the activation function for pixels (i, j) at the level p.For ReLU, nonlinearity has p.For the nonlinearity, we have the nonlinearity > j, where i = > i."}, {"heading": "2.5 Dropout, Subsampling, Dilated Convolution and Skip-Connections", "text": "Here we look at the impact of some common CNN approaches on the effective receiver field. Dropout is a popular technique to prevent over-matching; we show that dropout does not alter the Gaussian ERF shape. Subsampling and extended coils are effective methods to quickly increase the receptive field size, but skip connections make ERFs smaller. We present the analysis for all of these cases in the appendix."}, {"heading": "3 Experiments", "text": "In this section, we examine the ERF empirically for various deep CNN architectures. First, we use artificially constructed CNN models to verify the theoretical results of our analysis, then we present our observations of how the ERF changes on real data sets during the training of deep CNN. In all ERF studies, we place a gradient signal of 1 in the middle of the output plane and 0 anywhere else, and then propagate this gradient back over the network to obtain input gradients."}, {"heading": "3.1 Verifying theoretical results", "text": "We first verify our theoretical results in artificially constructed deep CNNs. To calculate the ERF we use random inputs and for all random weight networks we have followed [??] for a correct random initialization. In this section we check the following results: ERFs are distributed Gaussian: As shown in Fig. 1, we can find perfect Gaussian shapes for uniform and randomly weighted convolution cores without nonlinear activations and near Gaussian shapes for randomly weighted cores with nonlinearity.ReLU Tanh Sigmoid addition of ReLU nonlinearity makes the distribution a bit less Gaussian, since the ERF distribution also depends on the input. Another reason is that ReLU units output exactly zero for half of its inputs and it is very easy to get a zero output for the center pixels at the output level."}, {"heading": "3.2 How the ERF evolves during training", "text": "In this part, we will take a look at how the ERF of units in the top revolutionary layers of CNN classification and CNN semantic segmentation evolve during the training. For both tasks, we adopt the ResNet architecture, which makes extensive use of skip connections. However, as the analysis shows, the ERF of this network should be significantly smaller than the theoretical receptive field. Indeed, this is what we initially observed. At the end of the training, this network reached a test accuracy of 89%. Note that in this experiment, we do not use pooling or downsampling and focus exclusively on architectures with skip connections."}, {"heading": "4 Reduce the Gaussian Damage", "text": "The above analysis shows that the ERF occupies only a small part of the theoretical receptive field, which is undesirable for tasks that require a large receptive field. New initialization. A simple way to increase the effective receptive field is to manipulate the initial weights. We propose a new random initialization scheme that makes the weights in the center of the convolution core smaller, and the weights on the outside are larger; this distributes the concentration to the center outwardly. Practically, we can initialize the network with any initialization method, then scale the weights according to a distribution that has a lower scale in the center and a higher scale on the outside. In extreme cases, we can optimize the w (m) s to maximize the ERF size or equivalent variance in Eq. 10. Solving this optimization problem results in the weights being distributed evenly at the 4 corners of the convolution core."}, {"heading": "5 Discussion", "text": "In our analysis, we found that the effective receptive field in deep CNNs actually grows much more slowly than we previously thought, suggesting that a lot of local information remains intact even after many conventional layers. However, this finding contradicts some long-held relevant terms in deep biological networks. A popular characterization of mammalian systems involves splitting into \"what\" and \"where\" paths. Progressive movement along the what or where is a gradual shift in the nature of connectivity: receptive fields expand, and spatial organization becomes looser until there is an obvious retinotopic organization. Loss of retinotopic neurons means that they respond to objects like faces throughout the visual field."}, {"heading": "6 Conclusion", "text": "In this work, we carefully studied the receptive fields in deep CNNs and found some surprising results about the effective receptive field size. In particular, we showed that the distribution of effects within the receptive field is asymptotic Gaussian and the effective receptive field occupies only a fraction of the entire theoretical receptive field. Empirical results reflected the theory we have put forward. We believe that this is only the beginning of the investigation of the effective receptive field, which offers a new perspective to understand deep CNNs. In the future, we hope to learn more about what factors influence the effective receptive field in practice and how we can gain more control over it."}], "references": [], "referenceMentions": [], "year": 2017, "abstractText": "We study characteristics of receptive fields of units in deep convolutional networks. The receptive field size is a crucial issue in many visual tasks, as the output must respond to large enough areas in the image to capture information about large objects. We introduce the notion of an effective receptive field, and show that it both has a Gaussian distribution and only occupies a fraction of the full theoretical receptive field. We analyze the effective receptive field in several architecture designs, and the effect of nonlinear activations, dropout, sub-sampling and skip connections on it. This leads to suggestions for ways to address its tendency to be too small.", "creator": "LaTeX with hyperref package"}}}