{"id": "1701.04238", "review": {"conference": "AAAI", "VERSION": "v1", "DATE_OF_SUBMISSION": "16-Jan-2017", "title": "Thompson Sampling for Stochastic Bandits with Graph Feedback", "abstract": "We present a novel extension of Thompson Sampling for stochastic sequential decision problems with graph feedback, even when the graph structure itself is unknown and/or changing. We provide theoretical guarantees on the Bayesian regret of the algorithm, linking its performance to the underlying properties of the graph. Thompson Sampling has the advantage of being applicable without the need to construct complicated upper confidence bounds for different problems. We illustrate its performance through extensive experimental results on real and simulated networks with graph feedback. More specifically, we tested our algorithms on power law, planted partitions and Erdo's-Renyi graphs, as well as on graphs derived from Facebook and Flixster data. These all show that our algorithms clearly outperform related methods that employ upper confidence bounds, even if the latter use more information about the graph.", "histories": [["v1", "Mon, 16 Jan 2017 10:52:51 GMT  (1488kb,D)", "http://arxiv.org/abs/1701.04238v1", null]], "reviews": [], "SUBJECTS": "cs.LG cs.AI", "authors": ["aristide c y tossou", "christos dimitrakakis", "devdatt p dubhashi"], "accepted": true, "id": "1701.04238"}, "pdf": {"name": "1701.04238.pdf", "metadata": {"source": "CRF", "title": "Thompson Sampling For Stochastic Bandits with Graph Feedback", "authors": ["Aristide C. Y. Tossou", "Christos Dimitrakakis", "Devdatt Dubhashi"], "emails": ["aristide@chalmers.se", "christos.dimitrakakis@gmail.com", "dubhashi@chalmers.se"], "sections": [{"heading": "1 Introduction", "text": "In both cases, the problem is to maximize the total reward achieved over time. However, for the prediction (or full information) issue, it requires the reward of the chosen arm, but it also monitors the reward of all other decisions. In both cases, the problem is to maximize the total reward achieved over time. However, dealing with specific types of feedback requires specialized algorithms. In this paper, we show that the Thompson algorithms can be successfully applied to a number of sequential decision problems."}, {"heading": "2 Setting", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "2.1 The stochastic bandit model", "text": "The stochastic K-armed bandit problem is a well-known sequential decision problem in which an agent sequentially selects V = {1... K} from a series of K-weapons. In each turn, the agent plays an arm at-V and receives a reward rt = R (Yt, At), where Yt, At-Y is a random variable defined on some probability space (P-Y, \u03a3) and R: Y-R is a reward function. Each arm i has a mean reward \u00b5i (P) = EP R (Yt, i). Our goal is to maximize its expected cumulative reward after T-rounds. An equivalent idea is to minimize the expected regret against an oracle that P. knows. More formally, the expected regret EE of an agent policy is expressed."}, {"heading": "2.2 The graph feedback model", "text": "In this model we assume that the existence of an undirected graph G = (V, E) with the links of the limbs of the limbs of the limbs of the limbs of the limbs of the limbs of the limbs of the limbs of the limbs and the limbs of the limbs of the limbs of the limbs of the limbs of the limbs of the limbs of the limbs of the limbs of the limbs of the limbs of the limbs of the limbs of the limbs of the limbs of the limbs of the limbs of the limbs of the limbs. If the limbs are empty, then the limbs are equivalent to the Bandit problem of the limbs of the limbs of the limbs and the limbs of the limbs of the limbs of the limbs of the limbs."}, {"heading": "4 Algorithms and analysis", "text": "The second algorithm uses Thompson sampling to select an arm, and then selects the empirically best arm within that arm. At each step, it selects an arm according to the probability of its remedy. It then observes a series of rewards that it uses to update its probability distribution via the parameters. In the case where each arm has an independent parameter, we can update the distribution of all weapons separately. A particularly simple case is when all rewards are generated from Bernoulli distributions."}, {"heading": "5.1 Algorithms and hyperparameters.", "text": "In all our experiments, we tested against the UCB-MaxN and UCB-N algorithms introduced in (Caron et al. 2012). These are the analogies of our algorithms, reporting instead of Thompson sampling-greedy-D and -greedy-LP. This is based on a linear program formulation to find a lower bound form (G) that evaluates our algorithms against a variant of -greedy-LP (Buccapatnam, Eryilmaz and Shroff 2014) based on the size of the minimum dominating set (G). First, we observe that their analysis applies to any fixed dominating set D and the limit obtained is O (D | lnT). Specifically, we can use a simple greedy algorithm to compile a near-optimal dominating set D."}, {"heading": "5.2 General experimental setup.", "text": "For all our experiments, we conducted 210 independent studies and reported on the mean estimation5 of cumulative regret. It splits the studies into equal groups and returns the mean of the sample of each group. We set the number of groups to a0 = 14, so that the confidence interval is likely to hold at least 0.95. We also reported on the deviation of each algorithm based on the Gini's Mean Difference (GMD hereafter) (Gini and Pearson 1912). GMD calculates the deviation as \u2211 N j = 1 (2j \u2212 N \u2212 1) x (j) with x (j) the statistics of the j-th order of the sample (i.e. x (1) \u2264 x (2) \u2264.. \u2264 x (N). As shown in (Yitzhaki and others 2003; David 1968), the GMD provides a superior approximation of the true deviation as the standard deviation. To take into account the fact that the cumulative regret of our algorithms could not follow a symmetrical distribution of the mean above the GMD for the GMD."}, {"heading": "5.3 Simulated graphs", "text": "In our synthetic problems, unless otherwise stated, the rewards are drawn from a Bernoulli distribution, the mean of which is generated evenly randomly in [0.45, 0.55], except for the optimal arm, whose mean is randomly generated in [0.55, 0.6] The number of nodes in the diagram is 500. We tested with a sparse diagram of 2500 edges and also with a dense diagram of 62625 edges. Our first observation here is that all strategies exploit a large number of edges, as their cumulative regret is better by using the erdo-re-nyi model. Figure 2e and 2f each show the result in the sparse and dense diagram N. Our first observation here is that all strategies exploit a large number of edges, as their cumulative regret is better by using the dense diagram (Figure 2f) than the sparse diagram, which we show a dense diagram (Figure 2f each)."}, {"heading": "5.4 Social networks datasets", "text": "Our experiments on real data sets follow the methodology described in (Caron et al. 2012). We first draw a graph from the data and then define a reward function for film recommendations from users \"ratings. Missing ratings are predicted using matrix factorization, which allows us to generate rewards from the graph. We explain the data sets, reward function and graph conclusions in the full version. Results Figure 2a shows the results for the Facebook graph and Figure 2b for the Flixster graph. Once again, the sampling strategies dominate all other Facebook strategies and they are matched by the optimized greed D policy in the Flixster graph. We note that in this setting the gap between the UCB strategies and the rest is much larger than the overall regret of all strategies. This can be attributed to the larger size of this graph for the Badia graph."}], "references": [{"title": "N", "author": ["S. Agrawal", "Goyal"], "venue": "2012. Analysis of thompson sampling for the multi-armed bandit problem. In COLT", "citeRegEx": "Agrawal and Goyal 2012", "shortCiteRegEx": null, "year": 2012}, {"title": "Online learning with feedback graphs: beyond bandits", "author": ["Alon"], "venue": "In Proceedings of the 28th Annual Conference on Learning", "citeRegEx": "Alon,? \\Q2015\\E", "shortCiteRegEx": "Alon", "year": 2015}, {"title": "The space complexity of approximating the frequency moments", "author": ["Matias Alon", "N. Szegedy 1996] Alon", "Y. Matias", "M. Szegedy"], "venue": null, "citeRegEx": "Alon et al\\.,? \\Q1996\\E", "shortCiteRegEx": "Alon et al\\.", "year": 1996}, {"title": "Finite time analysis of the multiarmed bandit problem. Machine Learning 47(2/3):235\u2013256", "author": ["Cesa-Bianchi Auer", "P. Fischer 2002a] Auer", "N. CesaBianchi", "P. Fischer"], "venue": null, "citeRegEx": "Auer et al\\.,? \\Q2002\\E", "shortCiteRegEx": "Auer et al\\.", "year": 2002}, {"title": "Finite-time analysis of the multiarmed bandit problem. Machine learning 47(2-3):235\u2013256", "author": ["Cesa-Bianchi Auer", "P. Fischer 2002b] Auer", "N. CesaBianchi", "P. Fischer"], "venue": null, "citeRegEx": "Auer et al\\.,? \\Q2002\\E", "shortCiteRegEx": "Auer et al\\.", "year": 2002}, {"title": "N", "author": ["S. Buccapatnam", "A. Eryilmaz", "Shroff"], "venue": "B.", "citeRegEx": "Buccapatnam. Eryilmaz. and Shroff 2014", "shortCiteRegEx": null, "year": 2014}, {"title": "M", "author": ["A.N. Burnetas", "Katehakis"], "venue": "N.", "citeRegEx": "Burnetas and Katehakis 1997", "shortCiteRegEx": null, "year": 1997}, {"title": "Leveraging side observations in stochastic bandits. UAI", "author": ["Caron"], "venue": null, "citeRegEx": "Caron,? \\Q2012\\E", "shortCiteRegEx": "Caron", "year": 2012}, {"title": "and Lugosi", "author": ["N. Cesa-Bianchi"], "venue": "G.", "citeRegEx": "Cesa.Bianchi and Lugosi 2006", "shortCiteRegEx": null, "year": 2006}, {"title": "and Li", "author": ["O. Chapelle"], "venue": "L.", "citeRegEx": "Chapelle and Li 2011", "shortCiteRegEx": null, "year": 2011}, {"title": "T", "author": ["A. Cohen", "T. Hazan", "Koren"], "venue": "2016. Online learning with feedback graphs without the graphs. In ICML", "citeRegEx": "Cohen. Hazan. and Koren 2016", "shortCiteRegEx": null, "year": 2016}, {"title": "R", "author": ["A. Condon", "Karp"], "venue": "M.", "citeRegEx": "Condon and Karp 2001", "shortCiteRegEx": null, "year": 2001}, {"title": "Miscellanea: Gini\u2019s mean difference rediscovered", "author": ["H. David 1968] David"], "venue": null, "citeRegEx": "David,? \\Q1968\\E", "shortCiteRegEx": "David", "year": 1968}, {"title": "and Capp\u00e9", "author": ["A. Garivier"], "venue": "O.", "citeRegEx": "Garivier and Capp\u00e9 2011", "shortCiteRegEx": null, "year": 2011}, {"title": "and Pearson", "author": ["C. Gini"], "venue": "K.", "citeRegEx": "Gini and Pearson 1912", "shortCiteRegEx": null, "year": 1912}, {"title": "Universal behavior of load distribution in scale-free networks", "author": ["Kahng Goh", "K.-I. Kim 2001] Goh", "B. Kahng", "D. Kim"], "venue": "Physical Review Letters 87(27):278701", "citeRegEx": "Goh et al\\.,? \\Q2001\\E", "shortCiteRegEx": "Goh et al\\.", "year": 2001}, {"title": "and Robbins", "author": ["T.L. Lai"], "venue": "H.", "citeRegEx": "Lai and Robbins 1985", "shortCiteRegEx": null, "year": 1985}, {"title": "and Shamir", "author": ["S. Mannor"], "venue": "O.", "citeRegEx": "Mannor and Shamir 2011", "shortCiteRegEx": null, "year": 2011}, {"title": "F", "author": ["McSherry"], "venue": "2001. Spectral partitioning of random graphs. In Foundations of Computer Science,", "citeRegEx": "McSherry 2001", "shortCiteRegEx": null, "year": 2001}, {"title": "A greedy approximation for minimum connected dominating sets. Theoretical Computer Science 329(1):325\u2013330", "author": ["Ruan"], "venue": null, "citeRegEx": "Ruan,? \\Q2004\\E", "shortCiteRegEx": "Ruan", "year": 2004}, {"title": "B", "author": ["D. Russo", "Roy"], "venue": "V.", "citeRegEx": "Russo and Roy 2016", "shortCiteRegEx": null, "year": 2016}, {"title": "S", "author": ["Scott"], "venue": "L.", "citeRegEx": "Scott 2010", "shortCiteRegEx": null, "year": 2010}, {"title": "S", "author": ["Scott"], "venue": "L.", "citeRegEx": "Scott 2015", "shortCiteRegEx": null, "year": 2015}, {"title": "Gini\u2019s mean difference: A superior measure of variability for nonnormal distributions. Metron 61(2):285\u2013316", "author": ["Yitzhaki", "S others 2003] Yitzhaki"], "venue": null, "citeRegEx": "Yitzhaki and Yitzhaki,? \\Q2003\\E", "shortCiteRegEx": "Yitzhaki and Yitzhaki", "year": 2003}], "referenceMentions": [], "year": 2017, "abstractText": "We present a novel extension of Thompson Sampling for stochastic sequential decision problems with graph feedback, even when the graph structure itself is unknown and/or changing. We provide theoretical guarantees on the Bayesian regret of the algorithm, linking its performance to the underlying properties of the graph. Thompson Sampling has the advantage of being applicable without the need to construct complicated upper confidence bounds for different problems. We illustrate its performance through extensive experimental results on real and simulated networks with graph feedback. More specifically, we tested our algorithms on power law, planted partitions and Erd\u0151s\u2013R\u00e9nyi graphs, as well as on graphs derived from Facebook and Flixster data. These all show that our algorithms clearly outperform related methods that employ upper confidence bounds, even if the latter use more information about the graph.", "creator": "LaTeX with hyperref package"}}}