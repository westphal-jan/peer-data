{"id": "1701.04722", "review": {"conference": "ICML", "VERSION": "v1", "DATE_OF_SUBMISSION": "17-Jan-2017", "title": "Adversarial Variational Bayes: Unifying Variational Autoencoders and Generative Adversarial Networks", "abstract": "Variational Autoencoders (VAEs) are expressive latent variable models that can be used to learn complex probability distributions from training data. However, the quality of the resulting model crucially relies on the expressiveness of the inference model used during training. We introduce Adversarial Variational Bayes (AVB), a technique for training Variational Autoencoders with arbitrarily expressive inference models. We achieve this by introducing an auxiliary discriminative network that allows to rephrase the maximum-likelihood-problem as a two-player game, hence establishing a principled connection between VAEs and Generative Adversarial Networks (GANs). We show that in the nonparametric limit our method yields an exact maximum-likelihood assignment for the parameters of the generative model, as well as the exact posterior distribution over the latent variables given an observation. Contrary to competing approaches which combine VAEs with GANs, our approach has a clear theoretical justification, retains most advantages of standard Variational Autoencoders and is easy to implement.", "histories": [["v1", "Tue, 17 Jan 2017 15:18:31 GMT  (2048kb,D)", "http://arxiv.org/abs/1701.04722v1", null], ["v2", "Wed, 8 Mar 2017 13:46:01 GMT  (3115kb,D)", "http://arxiv.org/abs/1701.04722v2", null], ["v3", "Wed, 2 Aug 2017 12:32:57 GMT  (3107kb,D)", "http://arxiv.org/abs/1701.04722v3", null]], "reviews": [], "SUBJECTS": "cs.LG", "authors": ["lars m mescheder", "sebastian nowozin", "andreas geiger"], "accepted": true, "id": "1701.04722"}, "pdf": {"name": "1701.04722.pdf", "metadata": {"source": "META", "title": "Adversarial Variational Bayes: Unifying Variational Autoencoders and Generative Adversarial Networks", "authors": ["Lars Mescheder", "Andreas Geiger"], "emails": ["LARS.MESCHEDER@TUEBINGEN.MPG.DE", "SEBASTIAN.NOWOZIN@MICROSOFT.COM", "ANDREAS.GEIGER@TUEBINGEN.MPG.DE"], "sections": [{"heading": "1. Introduction", "text": "This year, it has reached the stage where it will be able to put itself at the forefront in order to find its way into the future."}, {"heading": "2. Background", "text": "Since our model is an extension of the Variational Autoencoders (VAEs) (Kingma & Welling, 2013; Rezende et al., 2014), we begin with a brief review of the VAEs.VAEs are specified by a parametric generative model p\u03b8 (x | z) of the visible variables relative to the latent variables, a preceding p (z) relative to the latent variables and an approximate inference model q\u03c6 (z | x) relative to the latent variables relative to the visible variables. It can be shown that the protocol p (x) \u2265 \u2212 KL (z), p (z)) + Eq\u03c6 (z) + Eq\u03c6 (z | z) log p\u03b8 (x | z) relative to the latent variables relative to the visible variables. (2,1) The right side of (2,1) is referred to as the variable lower limit or proof lower limit (ELBO), the lower limit (z)."}, {"heading": "3. Method", "text": "In this thesis we show how we can use a black box inference model q\u03c6 (z | x) instead and how we can obtain an approximate maximum probability allocation \u03b8 * to \u03b8 and an approximate approximation q\u03c6 * (z | x) to the true posterior p\u03b8 * (z | x) by means of adversarial training. This is illustrated in Figure 1: On the left side the structure of a typical variational autoencoder is shown, on the right side our flexible black box inference model."}, {"heading": "3.1. Derivation", "text": "To deduce our method, we have to use the optimization problem in (2.4) asmax. (max.) empirical (x) empirical (x) empirical) empirical (x) empirical) empirical (x) empirical (x) empirical) empirical (x) empirical (x) empirical) empirical (x) empirical (x) empirical) empirical (x) empirical (x) empirical (x) empirical (x) empirical (x) empirical (x) empirical (x) empirical (x) empirical (x) empirical (empirical) empirical (x) empirical (x) empirical (x) empirical (x) empirical (x) empirical (x) empirical (x) empirical (x) empirical (x) empirical (x) empirical (x) empirical (x) empirical (x) empirical) empirical (x) empirical (x) empirical (x) empirical (x) empirical) empirical (x) empirical (x) empirical (x) empirical) empirical (x) empirical (x) empirical (x) empirical) empirical (x) empirical (x) empirical (x) empirical) empirical (x) empirical (x) empirical (x) empirical) empirical (x) empirical (x) empirical (x) empirical (x) empirical) empirical (x) empirical (x) empirical (x) empirical (x) empirical) empirical (x) empirical () empirical) empirical ("}, {"heading": "3.2. Algorithm", "text": "Algorithm 1: Adversarial Variational Bayes (AVB) 1: i = 0 2: while not converged do 3: Samplem examples {x (1),.., x (m)} from data dis-tribution pD (x). 4: Sample m examples {z (1),.., z (m)} from prior distribution p (z). 5: Sample m noise examples {(1),., (m)} from N (0, 1). 6: Compute \u03b8-gradient (eq. 3,9): g\u03b8-gradient (eq. 3,9): g\u03b8 = preh. m i = 1 log p\u03b8 (x), (i) | z\u03c6 (x), (i))) 7: Compute \u03c6-gradient (eq. 3,9): g\u03c6 = equector (eq. i = 1 [\u2212 T\u043d (i), z\u03c6 (i), (i)."}, {"heading": "3.3. Theoretical results", "text": "In Section 3.1 we derive the AVB as a method for performing stochastic gradient descent at the lowest limit (2,4). In this section we analyze the properties of algorithm 1 from a game theory point of view. As the next sentence shows, global Nash balances of algorithm 1 result in a global optimum of the target in (2,4): sentence 3. If (270, 44, T) a Nash balance of the game for two players is defined by (3,3) and (3,9), this results in a global optimum of the target in (x, z) = log q\u03c6 (z | x) \u2212 log p (z) (3,10) and (44, T) is a global optimum of the lowest limit in (2,4). Proof this is to be found in the evidence in the appendix. Let us now consider the case in which T and q\u03c6 (z | x) both corridoms are flexible enough so that T can represent any function of the lower limit in (2,4)."}, {"heading": "4. Experiments", "text": "We tested our method both on shallow and on deeply entangled neural networks. The CNN implementation is based on two different network structures, both of which are able to identify themselves, both on the one hand, as well as on the other, on the other hand, on the other hand, on the other hand, on the other hand, on the other hand, on the other hand, on the other hand, on the other hand, on the other hand, the, on the other, on the other, the, on the other, on the other, the, the, the, the, the, the, the, the, the, the, the, the, the, the, the, the, the, the, the, the, the, the, the, the, the, the, the, the, the, the, the, the, the, the, the, the, the, on the other, the other, on the other, on the other, on the other, on the other, on the other, on the other, on the other, on the other, on the other, on the other, on the other, on the other, the other, the, the, the, the, the, the, the, the, the, the, the, the, the, the, the, the, the, the, the, the, the, the, the, the, the, the, the, the, the, the, the, the, the, the, the, the, the, the, the, the, the, the, the, the, the, the, the, the, the, the, the, the, the, the, the, the, the, the, the, the, the, the, the, the, the, the, the, the, the, the, the, the, the, the, the, the, the, the, the, the, the, the, the, the, the, the, the, the, the, the, the, the, the, the, the, the, the, the, the, the, the, the, the, the, the, the, the, the, the, the, the, the, the, the, the, the, the, the, the, the, the, the, the, the, the, the, the, the, the, the, the,"}, {"heading": "5. Related Work", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "5.1. Connection to Variational Autoencoders", "text": "Adversarial Variational Bayes strives to optimize the same goal as a standard Variational Autoencoder (Kingma & Welling, 2013; Rezende et al., 2014), but approaches the Kullback-Leibler divergence by using an adversary rather than relying on a formula in closed form. Extensive work has focused on making the class of approximate inference models more meaningful. The normalization of flows (Rezende & Mohamed, 2015; Kingma et al., 2016) makes the rear end more complex by assembling a simple Gaussian rear end with an invertable, smooth mapping for which the determinant of the Jacobite is tractable. Auxiliary Variable VAEs (Maal\u00f8e et al., 2016) add auxiliary variables to the rear end to make it more flexible."}, {"heading": "5.2. Connection to Adversarial Autoencoders", "text": "Makhzani et al. (Makhzani et al., 2015) introduced the concept of \"Adversarial Autoencoders\" (5.5%). The idea is to replace the terms KL (q\u03c6 (z | x), p (z))) (5.1) in (2.4) with an opposing loss that tries to force this in convergence. (5.2) Although the approach of Makhzani et al. is related to our approach, it modifies the variation target while our approach maintains objectivity. Interestingly, the approach of Makhzani et al. can be seen as an approximation to our approach where T (x, z) is limited to the class of functions that do not depend on x. In fact, an ideal discriminator that is only maximized by zmaximized, pD (x) q (z | x) is an approximation to the pD (z) distributions, i.e. an approximation to the class that do not depend on x."}, {"heading": "5.3. Connection to f-GANs", "text": "Nowozin et al. (Nowozin et al., 2016) proposed generalizing Generative Adversarial Networks (Goodfellow et al., 2014) to f-divergences (Ali & Silvey, 1966) based on the results of Nguyen et al. (Nguyen et al., 2010). (In this paragraph, we show that f-divergences allow to present AVB as a zero-sum game for two players. (2010) The family of f-divergences is represented by Df (p-divergences) = Epf (x) p (x)). (5,6) for some convex-functional f: R with f (1) = 0, Nguyen et al. (2010) shows that the use of the convex conjugation f of f (Hiriart-Urruty & Lemare chal, 2013)."}, {"heading": "5.4. Connection to BiGANs", "text": "BiGANs (Donahue et al., 2016; Dumoulin et al., 2016) are a newer extension of generative adversary networks with the goal of adding an inference network to the generative model. Similar to our approach, the authors present an adversary that acts on pairs (x, z) of data points and latent codes. However, while in BiGANs the adversary is used to optimize the generative and inference networks separately, our approach optimizes the generative and inference models jointly. Consequently, our approach obtains good reconstructions of the input data, whereas in BiGANs we obtain this reconstruction only indirectly."}, {"heading": "6. Conclusion", "text": "We introduced a new training method for Variational Autoencoders based on adversarial training, which allows us to make the inference model much more flexible and effectively allow almost any family of conditional distributions to be represented by latent variables. We believe that further progress can be made by studying the class of neural network architectures used for the adversary and the encoder and decoder networks."}, {"heading": "Acknowledgements", "text": "This work was supported by Microsoft Research as part of its doctoral fellowship program."}], "references": [{"title": "A general class of coefficients of divergence of one distribution from another", "author": ["Ali", "Syed Mumtaz", "Silvey", "Samuel D"], "venue": "Journal of the Royal Statistical Society. Series B (Methodological),", "citeRegEx": "Ali et al\\.,? \\Q1966\\E", "shortCiteRegEx": "Ali et al\\.", "year": 1966}, {"title": "Density estimation using real nvp", "author": ["Dinh", "Laurent", "Sohl-Dickstein", "Jascha", "Bengio", "Samy"], "venue": "arXiv preprint arXiv:1605.08803,", "citeRegEx": "Dinh et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Dinh et al\\.", "year": 2016}, {"title": "Adversarial feature learning", "author": ["Donahue", "Jeff", "Kr\u00e4henb\u00fchl", "Philipp", "Darrell", "Trevor"], "venue": "arXiv preprint arXiv:1605.09782,", "citeRegEx": "Donahue et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Donahue et al\\.", "year": 2016}, {"title": "Adversarially learned inference", "author": ["Dumoulin", "Vincent", "Belghazi", "Ishmael", "Poole", "Ben", "Lamb", "Alex", "Arjovsky", "Martin", "Mastropietro", "Olivier", "Courville", "Aaron"], "venue": "arXiv preprint arXiv:1606.00704,", "citeRegEx": "Dumoulin et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Dumoulin et al\\.", "year": 2016}, {"title": "Generative adversarial nets", "author": ["Goodfellow", "Ian", "Pouget-Abadie", "Jean", "Mirza", "Mehdi", "Xu", "Bing", "Warde-Farley", "David", "Ozair", "Sherjil", "Courville", "Aaron", "Bengio", "Yoshua"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Goodfellow et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Goodfellow et al\\.", "year": 2014}, {"title": "Deep residual learning for image recognition", "author": ["He", "Kaiming", "Zhang", "Xiangyu", "Ren", "Shaoqing", "Sun", "Jian"], "venue": "arXiv preprint arXiv:1512.03385,", "citeRegEx": "He et al\\.,? \\Q2015\\E", "shortCiteRegEx": "He et al\\.", "year": 2015}, {"title": "Convex analysis and minimization algorithms I: fundamentals, volume 305", "author": ["Hiriart-Urruty", "Jean-Baptiste", "Lemar\u00e9chal", "Claude"], "venue": "Springer science & business media,", "citeRegEx": "Hiriart.Urruty et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Hiriart.Urruty et al\\.", "year": 2013}, {"title": "Multilayer feedforward networks are universal approximators", "author": ["Hornik", "Kurt", "Stinchcombe", "Maxwell", "White", "Halbert"], "venue": "Neural networks,", "citeRegEx": "Hornik et al\\.,? \\Q1989\\E", "shortCiteRegEx": "Hornik et al\\.", "year": 1989}, {"title": "Auto-encoding variational bayes", "author": ["Kingma", "Diederik P", "Welling", "Max"], "venue": "arXiv preprint arXiv:1312.6114,", "citeRegEx": "Kingma et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Kingma et al\\.", "year": 2013}, {"title": "Improving variational inference with inverse autoregressive flow", "author": ["Kingma", "Diederik P", "Salimans", "Tim", "Welling", "Max"], "venue": "arXiv preprint arXiv:1606.04934,", "citeRegEx": "Kingma et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Kingma et al\\.", "year": 2016}, {"title": "Autoencoding beyond pixels using a learned similarity metric", "author": ["Larsen", "Anders Boesen Lindbo", "S\u00f8nderby", "S\u00f8ren Kaae", "Winther", "Ole"], "venue": "arXiv preprint arXiv:1512.09300,", "citeRegEx": "Larsen et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Larsen et al\\.", "year": 2015}, {"title": "Gradient-based learning applied to document recognition", "author": ["LeCun", "Yann", "Bottou", "L\u00e9on", "Bengio", "Yoshua", "Haffner", "Patrick"], "venue": "Proceedings of the IEEE,", "citeRegEx": "LeCun et al\\.,? \\Q1998\\E", "shortCiteRegEx": "LeCun et al\\.", "year": 1998}, {"title": "Deep learning face attributes in the wild", "author": ["Liu", "Ziwei", "Luo", "Ping", "Wang", "Xiaogang", "Tang", "Xiaoou"], "venue": "In Proceedings of International Conference on Computer Vision (ICCV),", "citeRegEx": "Liu et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Liu et al\\.", "year": 2015}, {"title": "Auxiliary deep generative models", "author": ["Maal\u00f8e", "Lars", "S\u00f8nderby", "Casper Kaae", "S\u00f8ren Kaae", "Winther", "Ole"], "venue": "arXiv preprint arXiv:1602.05473,", "citeRegEx": "Maal\u00f8e et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Maal\u00f8e et al\\.", "year": 2016}, {"title": "Plug & play generative networks: Conditional iterative generation of images in latent space", "author": ["Nguyen", "Anh", "Yosinski", "Jason", "Bengio", "Yoshua", "Dosovitskiy", "Alexey", "Clune", "Jeff"], "venue": "arXiv preprint arXiv:1612.00005,", "citeRegEx": "Nguyen et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Nguyen et al\\.", "year": 2016}, {"title": "Estimating divergence functionals and the likelihood ratio by convex risk minimization", "author": ["Nguyen", "XuanLong", "Wainwright", "Martin J", "Jordan", "Michael I"], "venue": "IEEE Transactions on Information Theory,", "citeRegEx": "Nguyen et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Nguyen et al\\.", "year": 2010}, {"title": "f-gan: Training generative neural samplers using variational divergence minimization", "author": ["Nowozin", "Sebastian", "Cseke", "Botond", "Tomioka", "Ryota"], "venue": "arXiv preprint arXiv:1606.00709,", "citeRegEx": "Nowozin et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Nowozin et al\\.", "year": 2016}, {"title": "Improved generator objectives for gans", "author": ["Poole", "Ben", "Alemi", "Alexander A", "Sohl-Dickstein", "Jascha", "Angelova", "Anelia"], "venue": "arXiv preprint arXiv:1612.02780,", "citeRegEx": "Poole et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Poole et al\\.", "year": 2016}, {"title": "Unsupervised representation learning with deep convolutional generative adversarial networks", "author": ["Radford", "Alec", "Metz", "Luke", "Chintala", "Soumith"], "venue": "arXiv preprint arXiv:1511.06434,", "citeRegEx": "Radford et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Radford et al\\.", "year": 2015}, {"title": "Variational inference with normalizing flows", "author": ["Rezende", "Danilo Jimenez", "Mohamed", "Shakir"], "venue": "arXiv preprint arXiv:1505.05770,", "citeRegEx": "Rezende et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Rezende et al\\.", "year": 2015}, {"title": "Stochastic backpropagation and approximate inference in deep generative models", "author": ["Rezende", "Danilo Jimenez", "Mohamed", "Shakir", "Wierstra", "Daan"], "venue": "arXiv preprint arXiv:1401.4082,", "citeRegEx": "Rezende et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Rezende et al\\.", "year": 2014}, {"title": "Information theoretical estimators (ite) toolbox", "author": ["Szabo", "Zolt\u00e1n"], "venue": null, "citeRegEx": "Szabo and Zolt\u00e1n.,? \\Q2013\\E", "shortCiteRegEx": "Szabo and Zolt\u00e1n.", "year": 2013}, {"title": "Conditional image generation with pixelcnn decoders", "author": ["van den Oord", "Aaron", "Kalchbrenner", "Nal", "Espeholt", "Lasse", "Vinyals", "Oriol", "Graves", "Alex"], "venue": "In Advances In Neural Information Processing Systems,", "citeRegEx": "Oord et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Oord et al\\.", "year": 2016}, {"title": "Pixel recurrent neural networks", "author": ["van den Oord", "Aaron van den", "Kalchbrenner", "Nal", "Kavukcuoglu", "Koray"], "venue": "arXiv preprint arXiv:1601.06759,", "citeRegEx": "Oord et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Oord et al\\.", "year": 2016}, {"title": "On the quantitative analysis of decoder-based generative models", "author": ["Adversarial Variational Bayes Wu", "Yuhuai", "Burda", "Yuri", "Salakhutdinov", "Ruslan", "Grosse", "Roger"], "venue": "arXiv preprint arXiv:1611.04273,", "citeRegEx": "Wu et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Wu et al\\.", "year": 2016}], "referenceMentions": [{"referenceID": 1, "context": ", 2016a), real NVP (Dinh et al., 2016) and Plug & Play generative networks (Nguyen et al.", "startOffset": 19, "endOffset": 38}, {"referenceID": 14, "context": ", 2016) and Plug & Play generative networks (Nguyen et al., 2016) have been introduced and studied, the two most prominent ones are Variational Autoencoders (VAEs) (Kingma & Welling, 2013; Rezende et al.", "startOffset": 44, "endOffset": 65}, {"referenceID": 20, "context": ", 2016) have been introduced and studied, the two most prominent ones are Variational Autoencoders (VAEs) (Kingma & Welling, 2013; Rezende et al., 2014) and Generative Adversarial Networks (GANs) (Goodfellow et al.", "startOffset": 106, "endOffset": 152}, {"referenceID": 4, "context": ", 2014) and Generative Adversarial Networks (GANs) (Goodfellow et al., 2014).", "startOffset": 51, "endOffset": 76}, {"referenceID": 24, "context": "Moreover, it was reported, that VAEs often lead to better log-likelihoods (Wu et al., 2016), making them interesting for scenarios where visual realism is not the main goal.", "startOffset": 74, "endOffset": 91}, {"referenceID": 2, "context": "The recently introduced BiGANs (Donahue et al., 2016; Dumoulin et al., 2016) add an inference model to GANs.", "startOffset": 31, "endOffset": 76}, {"referenceID": 3, "context": "The recently introduced BiGANs (Donahue et al., 2016; Dumoulin et al., 2016) add an inference model to GANs.", "startOffset": 31, "endOffset": 76}, {"referenceID": 9, "context": "Indeed, recent work shows that using more expressive model classes can lead to substantially better results (Kingma et al., 2016), both visually and in terms of log-likelihood-bounds.", "startOffset": 108, "endOffset": 129}, {"referenceID": 10, "context": "While there were some attempts at combining VAEs and GANs (Makhzani et al., 2015; Larsen et al., 2015), most of these attempts are not motivated from a maximumlikelihood point of view and therefore usually do not lead to maximum-likelihood assignments.", "startOffset": 58, "endOffset": 102}, {"referenceID": 20, "context": "As our model is an extension of Variational Autoencoders (VAEs) (Kingma & Welling, 2013; Rezende et al., 2014), we start with a brief review of VAEs.", "startOffset": 64, "endOffset": 110}, {"referenceID": 20, "context": "Usually, q\u03c6(z | x) is taken to be a Gaussian distribution with diagonal covariance matrix whose mean and variance vectors are parameterized by neural networks with x as input (Kingma & Welling, 2013; Rezende et al., 2014).", "startOffset": 175, "endOffset": 221}, {"referenceID": 10, "context": "Indeed, it was observed that applying standard Variational Autoencoders to natural images often results in blurry images (Larsen et al., 2015).", "startOffset": 121, "endOffset": 142}, {"referenceID": 4, "context": "This assumption is often referred to as the nonparametric limit (Goodfellow et al., 2014) and is justified by the fact that deep neural networks are universal function approximators (Hornik et al.", "startOffset": 64, "endOffset": 89}, {"referenceID": 7, "context": ", 2014) and is justified by the fact that deep neural networks are universal function approximators (Hornik et al., 1989).", "startOffset": 100, "endOffset": 121}, {"referenceID": 4, "context": "The proof is analogous to the proof of Proposition 1 in Goodfellow et al. (2014). See the Appendix for details.", "startOffset": 56, "endOffset": 81}, {"referenceID": 20, "context": "Using the reparameterization trick (Kingma & Welling, 2013; Rezende et al., 2014), (3.", "startOffset": 35, "endOffset": 81}, {"referenceID": 18, "context": "This is also a common problem for regular GANs (Radford et al., 2015).", "startOffset": 47, "endOffset": 69}, {"referenceID": 18, "context": "The CNN implementation is based on Radford et al. (2015) who show that deep convolutional networks can be used to parameterize the generator and discriminator for GANs.", "startOffset": 35, "endOffset": 57}, {"referenceID": 18, "context": "MNIST In addition, we trained deep convolutional networks based on the DC-GAN-architecture (Radford et al., 2015) on the MNIST-dataset (LeCun et al.", "startOffset": 91, "endOffset": 113}, {"referenceID": 11, "context": ", 2015) on the MNIST-dataset (LeCun et al., 1998).", "startOffset": 29, "endOffset": 49}, {"referenceID": 12, "context": "celebA We also trained a deep convolutional network on the celebA-dataset (Liu et al., 2015).", "startOffset": 74, "endOffset": 92}, {"referenceID": 5, "context": "Moreover, in the encoder and decoder we replace every hidden layer by three RESNETunits (He et al., 2015).", "startOffset": 88, "endOffset": 105}, {"referenceID": 20, "context": "Connection to Variational Autoencoders Adversarial Variational Bayes strives to optimize the same objective as a standard Variational Autoencoder (Kingma & Welling, 2013; Rezende et al., 2014), but approximates the Kullback-Leibler divergence using an adversary instead of relying on a closed-form formula.", "startOffset": 146, "endOffset": 192}, {"referenceID": 9, "context": "Normalizing flows (Rezende & Mohamed, 2015; Kingma et al., 2016) make the posterior more complex by composing a simple Gaussian posterior with an invertible smooth mapping for which the determinant of the Jacobian is tractable.", "startOffset": 18, "endOffset": 64}, {"referenceID": 13, "context": "Auxiliary Variable VAEs (Maal\u00f8e et al., 2016) add auxiliary variables to the posterior to make it more flexible.", "startOffset": 24, "endOffset": 45}, {"referenceID": 7, "context": "As neural network are universal function approximators (Hornik et al., 1989), our approach recovers the true posterior in the nonparametric limit.", "startOffset": 55, "endOffset": 76}, {"referenceID": 16, "context": "(Nowozin et al., 2016) proposed to generalize Generative Adversarial Networks (Goodfellow et al.", "startOffset": 0, "endOffset": 22}, {"referenceID": 4, "context": ", 2016) proposed to generalize Generative Adversarial Networks (Goodfellow et al., 2014) to f-divergences (Ali & Silvey, 1966) based on results by Nguyen et al.", "startOffset": 63, "endOffset": 88}, {"referenceID": 15, "context": "(Nguyen et al., 2010).", "startOffset": 0, "endOffset": 21}, {"referenceID": 14, "context": "Nguyen et al. (2010) show that using the convex conjugate f\u2217 of f (Hiriart-Urruty & Lemar\u00e9chal, 2013) Df (p\u2016q) = sup T Eq(x) [T (x)]\u2212 Ep(x) [f\u2217(T (x))] (5.", "startOffset": 0, "endOffset": 21}, {"referenceID": 17, "context": "(Poole et al., 2016) to improve the GAN-objective.", "startOffset": 0, "endOffset": 20}, {"referenceID": 2, "context": "BiGANs (Donahue et al., 2016; Dumoulin et al., 2016) are a recent extension of generative adversarial networks with the goal to add an inference network to the generative model.", "startOffset": 7, "endOffset": 52}, {"referenceID": 3, "context": "BiGANs (Donahue et al., 2016; Dumoulin et al., 2016) are a recent extension of generative adversarial networks with the goal to add an inference network to the generative model.", "startOffset": 7, "endOffset": 52}], "year": 2017, "abstractText": "Variational Autoencoders (VAEs) are expressive latent variable models that can be used to learn complex probability distributions from training data. However, the quality of the resulting model crucially relies on the expressiveness of the inference model used during training. We introduce Adversarial Variational Bayes (AVB), a technique for training Variational Autoencoders with arbitrarily expressive inference models. We achieve this by introducing an auxiliary discriminative network that allows to rephrase the maximum-likelihood-problem as a two-player game, hence establishing a principled connection between VAEs and Generative Adversarial Networks (GANs). We show that in the nonparametric limit our method yields an exact maximumlikelihood assignment for the parameters of the generative model, as well as the exact posterior distribution over the latent variables given an observation. Contrary to competing approaches which combine VAEs with GANs, our approach has a clear theoretical justification, retains most advantages of standard Variational Autoencoders and is easy to implement.", "creator": "LaTeX with hyperref package"}}}