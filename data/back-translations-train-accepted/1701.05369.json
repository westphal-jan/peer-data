{"id": "1701.05369", "review": {"conference": "ICML", "VERSION": "v1", "DATE_OF_SUBMISSION": "19-Jan-2017", "title": "Variational Dropout Sparsifies Deep Neural Networks", "abstract": "We explore recently proposed variational dropout technique which provided an elegant Bayesian interpretation to dropout. We extend variational dropout to the case when dropout rate is unknown and show that it can be found by optimizing evidence variational lower bound. We show that it is possible to assign and find individual dropout rates to each connection in DNN. Interestingly such assignment leads to extremely sparse solutions both in fully-connected and convolutional layers. This effect is similar to automatic relevance determination (ARD) effect in empirical Bayes but has a number of advantages. We report up to 128 fold compression of popular architectures without a large loss of accuracy providing additional evidence to the fact that modern deep architectures are very redundant.", "histories": [["v1", "Thu, 19 Jan 2017 10:44:55 GMT  (171kb,D)", "https://arxiv.org/abs/1701.05369v1", null], ["v2", "Mon, 27 Feb 2017 20:43:27 GMT  (90kb,D)", "http://arxiv.org/abs/1701.05369v2", null], ["v3", "Tue, 13 Jun 2017 11:01:55 GMT  (93kb,D)", "http://arxiv.org/abs/1701.05369v3", "Published in ICML 2017"]], "reviews": [], "SUBJECTS": "stat.ML cs.LG", "authors": ["dmitry molchanov", "arsenii ashukha", "dmitry vetrov"], "accepted": true, "id": "1701.05369"}, "pdf": {"name": "1701.05369.pdf", "metadata": {"source": "META", "title": "Variational Dropout Sparsifies Deep Neural Networks", "authors": ["Dmitry Molchanov", "Arsenii Ashukha", "Dmitry Vetrov"], "emails": ["<dmitry.molchanov@skolkovotech.ru>,", "<ars.ashuha@gmail.com>,", "<vetrovd@yandex.ru>."], "sections": [{"heading": "1. Introduction", "text": "This year, more than ever before in the history of the country in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a city, in which it is a country."}, {"heading": "2. Related Work", "text": "There are several successful techniques that have been proposed for the regulation of DNN, including Dropout (Srivastava et al., 2014), DropConnect (Wan et al., 2013), Max Norm Constraint (Srivastava et al., 2014), Batch Normalization (Ioffe & Szegedy, 2015), etc.Another way to regularize deep model is to reduce the number of parameters. One possible approach is tensor decompositions (Novikov et al., 2015), Garipov et al., 2016). Another approach is to induce thrift in weight matrices. Most current work on sparse neural networks use pruning (Han et al., 2015b), elastic net regularization (Lebedev Lempitsky, 2015; Scardapane et al al al al."}, {"heading": "3. Preliminaries", "text": "s inference and stochastic variational inference. Then we describe variational dropout, a recently proposed Bayesian regulation technique (Kingma et al., 2015)."}, {"heading": "3.1. Bayesian Inference", "text": "Consider a dataset D that predicts y-given x and w from N object pairs (xn, yn) Nn = 1. Our goal is to fine-tune the parameters w of a model p (y | x, w) that predicts y-given x and w. In Bayesian learning, we usually have some prior knowledge of weights w, expressed in the form of a previous distribution p (w). Once the data D is available, this prior distribution is normally converted to a rear distribution p (w | D) = p (D | w) p (w) / p (D). This process is called Bayesian inference. Calculation of the rear distribution using the Bayes rule usually involves calculating intractable multidimensional integral integrals, so we must apply approximation techniques. One of these techniques is variational inference. In this approach, the rear distribution p (w | D) is approximated by a parametric distribution size qw."}, {"heading": "3.2. Stochastic Variational Inference", "text": "In the case of complex models, the expectations in (1) and (2) are insoluble, and therefore the lower variable limit (1) and its gradients cannot be accurately calculated. However, it is still possible to estimate them from samples and to optimize the lower variable limit using stochastic optimization.We follow (Kingma & Welling, 2013) and use the repair trick to obtain an unbiased differentiated minibatch-based estimate of the expected log probability (3).The basic idea is to represent the parametric noise q\u03c6 (w) as a deterministic differentiation function w = f (\u03c6,) of a non-parametric noise p ().This trick enables us to obtain an unbiased estimate of the prospective log probability (3)."}, {"heading": "3.3. Variational Dropout", "text": "In this section, we consider a single, fully connected layer of I-input neurons and O-output neurons before nonlinearity. We designate an output matrix as BM + O, input matrix as AM + I, and a weight matrix as W I + O. We index the elements of these matrices as bmj, ami, and wij. Then, B = AW.Dropout is one of the most popular regulation methods for deep neural networks. (6) The original version of dropout, so called Bernoulli or Binary Dropout, was presented with the results of Bernoulli. (1 \u2212 p) (Hinton et al., 2012), with one of the most popular regulation methods for deep neural networks. (6) The original version of dropout, so-called Bernoulli or Binary Dropout, was presented with the results of Bernoulli. (1 \u2212 p) (Hinton et al., 2012) It means that each element is also reported later with a zero input."}, {"heading": "4. Sparse Variational Dropout", "text": "In the original paper, the authors reported difficulties in forming the model with high dropout rates \u03b1 (Kingma et al., 2015) and considered only the case of \u03b1 \u2264 1, which corresponds to a binary dropout rate p \u2264 0.5. However, the case of large \u03b1ij is very exciting (here we mean separate \u03b1ij per weight).High dropout rate \u03b1ij \u2192 + \u221e corresponds to a binary dropout rate approaching p = 1. It effectively means that the corresponding weight or neuron is always ignored and can be removed from the model. In this paper, we consider the case of individual \u03b1ij for each weight of the model."}, {"heading": "4.1. Additive Noise Reparameterization", "text": "The formation of neural networks with variational dropout is difficult when the dropout rates q are large, as there is an enormous variance of stochastic gradients (Kingma et al., 2015). The cause of the large gradient variance results from multiplicative noise. However, to be clear, we can describe the gradient of LSGVB w.r.t. successij as follows: The second multiplier in (9) is very noisy when \u03b1ij = repair trick. wij = repair trick trick trick trick trick trick trick trick trick trick trick trick trick trick trick trick trick trick trick trick trick trick trick trick trick trick trick trick trick trick trick trick trick trick trick trick trick trick trick trick trick trick trick trick trick trick trick trick trick trick trick trick trick trick trick trick trick trick trick trick trick trick trick trick trick trick trick trick trick trick trick trick trick trick trick trick trick trick trick trick trick trick trick trick trick trick trick trick trick trick trick trick trick trick trick trick trick trick trick trick trick trick trick trick trick trick trick trick trick trick trick trick trick trick trick trick trick trick trick trick trick trick trick trick trick trick trick trick trick trick trick trick trick trick trick trick trick trick trick trick trick trick trick trick trick trick trick trick trick trick trick trick trick trick trick trick trick."}, {"heading": "4.2. Approximation of the KL Divergence", "text": "Since the previous and approximate posterior values are fully factored, the complete KL divergence concept in the lower limit (1) can be broken down into a sum: DKL (q (W | \u03b8, \u03b1). \u2212 0.5 Approximations to the KL divergence can only be calculated up to an additive constant C (Kingma et al., 2015). \u2212 DKL (wij | empij, \u03b1ij) previous distribution is improper, so that the KL divergence can only be calculated up to an additive constant C (Kingma et al., 2015). \u2212 DKL (wij | empij, \u03b1j) previous distribution is an inadmissible prediction, so that the KL divergence can only be calculated up to an additive constant C (Kingma et al., 2015)."}, {"heading": "4.3. Sparsity", "text": "From Figure 1 is evident that \u2212 DKL term with the growth of \u03b1 > IIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIII"}, {"heading": "4.4. Sparse Variational Dropout for Fully-Connected and Convolutional Layers", "text": "In order to reduce the variance of LSGVB, we use the same notation as in Section 3.3. In this case, the variation variant Dropout is performed using the local repair method Trick and Additive Noise Repair. To improve the convergence, the optimization can be calculated as follows: bmj \u0445 N (fully connected layer). For a complete notation, we use the same notation as in Section 3.3. In this case, the variation Dropout can be calculated using the local repair method Trick and Additive Noise Repair as follows: bmj \u0445 N (fully connected layer).mj = I-Filter i = 1 amistij, \u03b4mj = I-Filter 2-Filter (2 a2mivement 2 ij (17)."}, {"heading": "4.5. Relation to RVM", "text": "The Relevance Vector Machine (RVM, (Tipping, 2001) > X > is a classic example of a frugal Bayesian model. RVM is essentially a Bayesian treatment of L2-regulated linear or logistic regression, where each weight has a separate regularization parameter \u03b1i. This effect is known as automatic relevance determination (ARD) and is a popular method of constructing sparse Bayesian models. Empirical Bayes is a somewhat counterintuitive method, as we optimize the previous distribution, and the observed data carry a risk of overadjusting the trick, indeed in (Cawley, 2010)."}, {"heading": "5. Experiments", "text": "We conduct classification experiments and use various neural network architectures, including architectures with a combination of batch normalization and dropout layers. We investigate the relevance determination power of our algorithm, as well as the classification accuracy of the ultimate sparse model. Our experiments show that Sparse Variational Dropout results in extremely sparse models. To make a Sparse Variational Dropout analogous to an existing architecture, all we need to do is remove existing dropout layers and replace all dense and convective layers with their Sparse Variational Dropout layers, as described in Section 4.4, and use LSGVB as an objective function. The value of the varying lower limit can be used to choose between multiple local optima."}, {"heading": "5.1. General Empirical Observations", "text": "We provide a general intuition about the formation of Sparse Bayesian DNNs with Sparse Variational Dropout.Since it is impossible for weights to converge exactly to zero in a stochastic setting, we explicitly set weights with high dropout rates to 0 during testing. In our neural network experiments, we use the value log\u03b1 = 3 as the threshold. This value corresponds to a binary dropout rate p > 0.95. Unlike most other methods (Han et al., 2015b; Wen et al., 2016), this trick usually does not harm the performance of our model. It means that Sparse VD does not require finetuning after the threshold \u2212.Training our random initialization model is tedious, as many weights are already truncated during training before they could possibly learn a useful representation of the data."}, {"heading": "5.2. Variance Reduction", "text": "To see how additive noise repair parameterization reduces variance, we compared it to the original parameterization. We used a fully networked architecture with two layers of 1000 neurons each. Both models were created with identical random initializations and with the same learning rate of 10 \u2212 4. We did not re-scale the KL term during the training. Interestingly, the original version of Variational Dropout also offers a sparse solution with our approximation of KL divergence and without limitation to Alphas. However, our method has a much better convergence rate and offers higher economy and a better value of the varying lower limit."}, {"heading": "5.3. LeNet-300-100 and LeNet5 on MNIST", "text": "We compare our method with other methods for forming sparse neural networks on the MNIST dataset using a fully networked architecture LeNet-300-100 and a revolutionary architecture LeNet-5-Caffe1. These networks were trained after random initialization and without data expansion. In these architectures, our method achieves a state-of-the-art level of thrift, while its accuracy is comparable to other methods. It should be noted a modified version of LeNet5 from (LeCun et al., 1998). Specification of the Caffe model: https: / / goo.gl / 4yI3dLthat we only consider the degree of thrift and not the final compression ratio."}, {"heading": "5.4. VGG-like on CIFAR-10 and CIFAR-100", "text": "To show that our method is applicable to large modern architectures, we apply it to a VGG-like network (Zagoruyko, 2015) adapted for the CIFAR-10 dataset (Krizhevsky & Hinton, 2009).The network consists of 13 revolutionary and two fully connected layers, each layer followed by pre-activation batch normalization and binary dropout. We experiment with different sizes of this architecture by scaling the number of units in each network by k = 1 (0.25, 0.5, 1.0, 1.5).We use CIFAR-10 and CIFAR-100 for evaluation. The reported error of this architecture on the CIFAR-10 dataset reaches k = 1 7.55%. Since no preset weights are available, we train our own network and achieve 7.3% error. Sparse VD also reaches 7.3% error for k = 1, but retains less than 48 x weights."}, {"heading": "5.5. Random Labels", "text": "Recently, CNNs have been shown to be able to store the data even with random labeling (Zhang et al., 2016). Standard dropout and other regulatory techniques did not prevent this behavior. Following this work, we are also experimenting with random labeling of data. We are using a fully connected network on the MNIST dataset and VGG-like networks on CIFAR-10. We are using bi-nary dropout (BD) with drop rate p = 0.5 on all fully connected layers of these networks. We observe that even with binary dropout, these architectures can fit random labeling. However, our model decides to drop each individual weight and provide a constant prediction. It is still possible that our model learns random labeling by initializing with a network that is trained on this random label, and then the 100 fine-tuning of this label is still lower than the L in this case."}, {"heading": "6. Discussion", "text": "The \"Occam's Razor\" principle states that unnecessarily complex data should not be preferred to simpler ones (MacKay, 1992). Automatic relevance determination is effectively a Bayean implementation of this principle, which occurs in different cases. Previously, the same effect was mainly studied in factorized Gaussian precursors in linear models, Gaussian processes, etc. In the Relevance Tagging Model (Molchanov et al., 2015), the same effect was achieved with beta distributions as precursors. Finally, in this work, the ARD effect is reproduced in a completely different environment. We consider a fixed advance and train the model with varying inferences. In this case, the ARD effect is caused by the particular combination of approximate posterior distributions and previous distributions, rather than by model selection. In this way, we can abandon the empirical Bayes approaches known to excel (Cawley, 2010)."}, {"heading": "Acknowledgements", "text": "We would like to thank Michael Figurnov, Ekaterina Lobacheva and Max Welling for their valuable feedback. Dmitry Molchanov was supported by the Ministry of Education and Science of the Russian Federation (funding 14,756,31,0001), Arseniy Ashukha from the HSE International lab of Deep Learning and Bayesian Methods, which is funded by the Russian Academic Excellence Project \"5-100,\" Dmitry Vetrov received funding from the Russian Science Foundation 17-11-01027. We would also like to thank the Department of Algorithms and Theory of Programming, the Faculty of Innovation and High Technology at the Moscow Institute of Physics and Technology, for the mandatory resources provided."}], "references": [{"title": "On over-fitting in model selection and subsequent selection bias in performance evaluation", "author": ["Cawley", "Nicola L.C. Talbot"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Cawley and Talbot.,? \\Q2010\\E", "shortCiteRegEx": "Cawley and Talbot.", "year": 2010}, {"title": "Gaussian kullback-leibler approximate inference", "author": ["E Challis", "D. Barber"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Challis and Barber,? \\Q2013\\E", "shortCiteRegEx": "Challis and Barber", "year": 2013}, {"title": "Dropout as a bayesian approximation: Insights and applications", "author": ["Gal", "Yarin", "Ghahramani", "Zoubin"], "venue": "In Deep Learning Workshop,", "citeRegEx": "Gal et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Gal et al\\.", "year": 2015}, {"title": "Ultimate tensorization: compressing convolutional and fc layers alike", "author": ["Garipov", "Timur", "Podoprikhin", "Dmitry", "Novikov", "Alexander", "Vetrov"], "venue": "arXiv preprint arXiv:1611.03214,", "citeRegEx": "Garipov et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Garipov et al\\.", "year": 2016}, {"title": "Dynamic network surgery for efficient dnns", "author": ["Guo", "Yiwen", "Yao", "Anbang", "Chen", "Yurong"], "venue": "In Advances In Neural Information Processing Systems,", "citeRegEx": "Guo et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Guo et al\\.", "year": 2016}, {"title": "Deep compression: Compressing deep neural networks with pruning, trained quantization and huffman coding", "author": ["Han", "Song", "Mao", "Huizi", "Dally", "William J"], "venue": "arXiv preprint arXiv:1510.00149,", "citeRegEx": "Han et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Han et al\\.", "year": 2015}, {"title": "Learning both weights and connections for efficient neural network", "author": ["Han", "Song", "Pool", "Jeff", "Tran", "John", "Dally", "William"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Han et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Han et al\\.", "year": 2015}, {"title": "Deep residual learning for image recognition", "author": ["He", "Kaiming", "Zhang", "Xiangyu", "Ren", "Shaoqing", "Sun", "Jian"], "venue": "arXiv preprint arXiv:1512.03385,", "citeRegEx": "He et al\\.,? \\Q2015\\E", "shortCiteRegEx": "He et al\\.", "year": 2015}, {"title": "Improving neural networks by preventing co-adaptation of feature detectors", "author": ["Hinton", "Geoffrey E", "Srivastava", "Nitish", "Krizhevsky", "Alex", "Sutskever", "Ilya", "Salakhutdinov", "Ruslan R"], "venue": "arXiv preprint arXiv:1207.0580,", "citeRegEx": "Hinton et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Hinton et al\\.", "year": 2012}, {"title": "Stochastic variational inference", "author": ["Hoffman", "Matthew D", "Blei", "David M", "Wang", "Chong", "Paisley", "John William"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Hoffman et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Hoffman et al\\.", "year": 2013}, {"title": "Batch normalization: Accelerating deep network training by reducing internal covariate shift", "author": ["Ioffe", "Sergey", "Szegedy", "Christian"], "venue": "arXiv preprint arXiv:1502.03167,", "citeRegEx": "Ioffe et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Ioffe et al\\.", "year": 2015}, {"title": "Adam: A method for stochastic optimization", "author": ["Kingma", "Diederik", "Ba", "Jimmy"], "venue": "arXiv preprint arXiv:1412.6980,", "citeRegEx": "Kingma et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Kingma et al\\.", "year": 2014}, {"title": "Auto-encoding variational bayes", "author": ["Kingma", "Diederik P", "Welling", "Max"], "venue": "arXiv preprint arXiv:1312.6114,", "citeRegEx": "Kingma et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Kingma et al\\.", "year": 2013}, {"title": "Learning multiple layers of features from tiny", "author": ["Krizhevsky", "Alex", "Hinton", "Geoffrey"], "venue": null, "citeRegEx": "Krizhevsky et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Krizhevsky et al\\.", "year": 2009}, {"title": "Fast convnets using group-wise brain damage", "author": ["Lebedev", "Vadim", "Lempitsky", "Victor"], "venue": "arXiv preprint arXiv:1506.02515,", "citeRegEx": "Lebedev et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Lebedev et al\\.", "year": 2015}, {"title": "Gradient-based learning applied to document recognition", "author": ["LeCun", "Yann", "Bottou", "L\u00e9on", "Bengio", "Yoshua", "Haffner", "Patrick"], "venue": "Proceedings of the IEEE,", "citeRegEx": "LeCun et al\\.,? \\Q1998\\E", "shortCiteRegEx": "LeCun et al\\.", "year": 1998}, {"title": "Sparse convolutional neural networks", "author": ["Liu", "Baoyuan", "Wang", "Min", "Foroosh", "Hassan", "Tappen", "Marshall", "Pensky", "Marianna"], "venue": "In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition,", "citeRegEx": "Liu et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Liu et al\\.", "year": 2015}, {"title": "Bayesian interpolation", "author": ["MacKay", "David JC"], "venue": "Neural computation,", "citeRegEx": "MacKay and JC.,? \\Q1992\\E", "shortCiteRegEx": "MacKay and JC.", "year": 1992}, {"title": "Bayesian nonlinear modeling for the prediction competition", "author": ["MacKay", "David JC"], "venue": "ASHRAE transactions,", "citeRegEx": "MacKay and JC,? \\Q1994\\E", "shortCiteRegEx": "MacKay and JC", "year": 1994}, {"title": "A bayesian encourages dropout", "author": ["Maeda", "Shin-ichi"], "venue": "arXiv preprint arXiv:1412.7003,", "citeRegEx": "Maeda and Shin.ichi.,? \\Q2014\\E", "shortCiteRegEx": "Maeda and Shin.ichi.", "year": 2014}, {"title": "Relevance tagging machine", "author": ["Molchanov", "Dmitry", "Kondrashkin", "Vetrov"], "venue": "Machine Learning and Data Analysis,", "citeRegEx": "Molchanov et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Molchanov et al\\.", "year": 2015}, {"title": "Bayesian learning for neural networks, volume 118", "author": ["Neal", "Radford M"], "venue": "Springer Science & Business Media,", "citeRegEx": "Neal and M.,? \\Q1996\\E", "shortCiteRegEx": "Neal and M.", "year": 1996}, {"title": "Tensorizing neural networks", "author": ["Novikov", "Alexander", "Podoprikhin", "Dmitrii", "Osokin", "Anton", "Vetrov", "Dmitry P"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Novikov et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Novikov et al\\.", "year": 2015}, {"title": "Stochastic backpropagation and approximate inference in deep generative models", "author": ["Rezende", "Danilo Jimenez", "Mohamed", "Shakir", "Wierstra", "Daan"], "venue": "arXiv preprint arXiv:1401.4082,", "citeRegEx": "Rezende et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Rezende et al\\.", "year": 2014}, {"title": "Group sparse regularization for deep neural networks", "author": ["Scardapane", "Simone", "Comminiello", "Danilo", "Hussain", "Amir", "Uncini", "Aurelio"], "venue": "arXiv preprint arXiv:1607.00485,", "citeRegEx": "Scardapane et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Scardapane et al\\.", "year": 2016}, {"title": "How to train deep variational autoencoders and probabilistic ladder networks", "author": ["S\u00f8nderby", "Casper Kaae", "Raiko", "Tapani", "Maal\u00f8e", "Lars", "S\u00f8ren Kaae", "Winther", "Ole"], "venue": "In Proceedings of the 33rd International Conference on Machine Learning", "citeRegEx": "S\u00f8nderby et al\\.,? \\Q2016\\E", "shortCiteRegEx": "S\u00f8nderby et al\\.", "year": 2016}, {"title": "The power of sparsity in convolutional neural networks", "author": ["Soravit Changpinyo", "Mark Sandler", "Andrey Zhmoginov"], "venue": "In Under review on ICLR", "citeRegEx": "Changpinyo et al\\.,? \\Q2017\\E", "shortCiteRegEx": "Changpinyo et al\\.", "year": 2017}, {"title": "Dropout: a simple way to prevent neural networks from overfitting", "author": ["Srivastava", "Nitish", "Hinton", "Geoffrey E", "Krizhevsky", "Alex", "Sutskever", "Ilya", "Salakhutdinov", "Ruslan"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Srivastava et al\\.,? \\Q1929\\E", "shortCiteRegEx": "Srivastava et al\\.", "year": 1929}, {"title": "Inception-v4, inception-resnet and the impact of residual connections on learning", "author": ["Szegedy", "Christian", "Ioffe", "Sergey", "Vanhoucke", "Vincent", "Alemi", "Alex"], "venue": "arXiv preprint arXiv:1602.07261,", "citeRegEx": "Szegedy et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Szegedy et al\\.", "year": 2016}, {"title": "Sparse bayesian learning and the relevance vector machine", "author": ["Tipping", "Michael E"], "venue": "Journal of machine learning research,", "citeRegEx": "Tipping and E.,? \\Q2001\\E", "shortCiteRegEx": "Tipping and E.", "year": 2001}, {"title": "Doubly stochastic variational bayes for non-conjugate inference", "author": ["Titsias", "Michalis", "L\u00e1zaro-Gredilla", "Miguel"], "venue": "Proceedings of The 31st International Conference on Machine Learning,", "citeRegEx": "Titsias et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Titsias et al\\.", "year": 2014}, {"title": "Soft weight-sharing for neural network compression", "author": ["Ullrich", "Karen", "Meeds", "Edward", "Welling", "Max"], "venue": "arXiv preprint arXiv:1702.04008,", "citeRegEx": "Ullrich et al\\.,? \\Q2017\\E", "shortCiteRegEx": "Ullrich et al\\.", "year": 2017}, {"title": "Automatic relevance determination for least squares support vector machine regression", "author": ["Van Gestel", "Tony", "JAK Suykens", "De Moor", "Bart", "Vandewalle", "Joos"], "venue": "In Neural Networks,", "citeRegEx": "Gestel et al\\.,? \\Q2001\\E", "shortCiteRegEx": "Gestel et al\\.", "year": 2001}, {"title": "Regularization of neural networks using dropconnect", "author": ["Wan", "Li", "Zeiler", "Matthew", "Zhang", "Sixin", "Cun", "Yann L", "Fergus", "Rob"], "venue": "In Proceedings of the 30th International Conference on Machine Learning", "citeRegEx": "Wan et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Wan et al\\.", "year": 2013}, {"title": "Learning structured sparsity in deep neural networks", "author": ["Wen", "Wei", "Wu", "Chunpeng", "Wang", "Yandan", "Chen", "Yiran", "Li", "Hai"], "venue": "Advances in Neural Information Processing Systems", "citeRegEx": "Wen et al\\.,? \\Q2082\\E", "shortCiteRegEx": "Wen et al\\.", "year": 2082}, {"title": "Understanding deep learning requires rethinking generalization", "author": ["Zhang", "Chiyuan", "Bengio", "Samy", "Hardt", "Moritz", "Recht", "Benjamin", "Vinyals", "Oriol"], "venue": "arXiv preprint arXiv:1611.03530,", "citeRegEx": "Zhang et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Zhang et al\\.", "year": 2016}], "referenceMentions": [{"referenceID": 28, "context": "Deep neural networks (DNNs) are a widely popular family of models which is currently state-of-the-art in many important problems (Szegedy et al., 2016; Silver et al., 2016).", "startOffset": 129, "endOffset": 172}, {"referenceID": 8, "context": "This makes them prone to overfitting (Hinton et al., 2012; Zhang et al., 2016) and necessitates using regularization.", "startOffset": 37, "endOffset": 78}, {"referenceID": 35, "context": "This makes them prone to overfitting (Hinton et al., 2012; Zhang et al., 2016) and necessitates using regularization.", "startOffset": 37, "endOffset": 78}, {"referenceID": 8, "context": "A commonly used regularizer is Binary Dropout (Hinton et al., 2012) that prevents co-adaptation of neurons by randomly dropping them during training.", "startOffset": 46, "endOffset": 67}, {"referenceID": 7, "context": "Modern neural networks contain hundreds of millions of parameters (Szegedy et al., 2015; He et al., 2015) and require a lot of computational and memory resources.", "startOffset": 66, "endOffset": 105}, {"referenceID": 24, "context": "Inducing sparsity during training of DNNs leads to regularization, compression, and acceleration of the resulting model (Han et al., 2015a; Scardapane et al., 2016).", "startOffset": 120, "endOffset": 164}, {"referenceID": 9, "context": "During past several years, a number of papers (Hoffman et al., 2013; Kingma & Welling, 2013; Rezende et al., 2014) on scalable variational inference have appeared.", "startOffset": 46, "endOffset": 114}, {"referenceID": 23, "context": "During past several years, a number of papers (Hoffman et al., 2013; Kingma & Welling, 2013; Rezende et al., 2014) on scalable variational inference have appeared.", "startOffset": 46, "endOffset": 114}, {"referenceID": 33, "context": ", 2014), DropConnect (Wan et al., 2013), Max Norm Constraint (Srivastava et al.", "startOffset": 21, "endOffset": 39}, {"referenceID": 22, "context": "One possible approach is to use tensor decompositions (Novikov et al., 2015; Garipov et al., 2016).", "startOffset": 54, "endOffset": 98}, {"referenceID": 3, "context": "One possible approach is to use tensor decompositions (Novikov et al., 2015; Garipov et al., 2016).", "startOffset": 54, "endOffset": 98}, {"referenceID": 16, "context": ", 2015b), elastic net regularization (Lebedev & Lempitsky, 2015; Liu et al., 2015; Scardapane et al., 2016; Wen et al., 2016) or composite techniques (Han et al.", "startOffset": 37, "endOffset": 125}, {"referenceID": 24, "context": ", 2015b), elastic net regularization (Lebedev & Lempitsky, 2015; Liu et al., 2015; Scardapane et al., 2016; Wen et al., 2016) or composite techniques (Han et al.", "startOffset": 37, "endOffset": 125}, {"referenceID": 4, "context": ", 2016) or composite techniques (Han et al., 2015a; Guo et al., 2016; Ullrich et al., 2017).", "startOffset": 32, "endOffset": 91}, {"referenceID": 31, "context": ", 2016) or composite techniques (Han et al., 2015a; Guo et al., 2016; Ullrich et al., 2017).", "startOffset": 32, "endOffset": 91}, {"referenceID": 20, "context": "In the Relevance Tagging Machine model (Molchanov et al., 2015) Beta prior distribution is used to obtain the ARD effect in a similar setting.", "startOffset": 39, "endOffset": 63}, {"referenceID": 23, "context": "Recent works on Bayesian DNNs (Kingma & Welling, 2013; Rezende et al., 2014; Scardapane et al., 2016) provide different ways to train deep models with a huge number of parameters in a Bayesian way.", "startOffset": 30, "endOffset": 101}, {"referenceID": 24, "context": "Recent works on Bayesian DNNs (Kingma & Welling, 2013; Rezende et al., 2014; Scardapane et al., 2016) provide different ways to train deep models with a huge number of parameters in a Bayesian way.", "startOffset": 30, "endOffset": 101}, {"referenceID": 31, "context": "Soft Weights Sharing (Ullrich et al., 2017) uses an approach, similar to Sparse Bayesian Learning framework, to obtain a sparse and quantized Bayesian Deep Neural Network, but utilizes a more flexible family of prior distributions.", "startOffset": 21, "endOffset": 43}, {"referenceID": 8, "context": "It injects a multiplicative random noise \u039e to the layer input A at each iteration of training procedure (Hinton et al., 2012).", "startOffset": 104, "endOffset": 125}, {"referenceID": 8, "context": "The original version of dropout, so-called Bernoulli or Binary Dropout, was presented with \u03bemi \u223c Bernoulli(1\u2212 p) (Hinton et al., 2012).", "startOffset": 113, "endOffset": 134}, {"referenceID": 25, "context": "The same problem is reported by (S\u00f8nderby et al., 2016) and is a common problem for Bayesian DNNs.", "startOffset": 32, "endOffset": 55}, {"referenceID": 25, "context": "Another way to approach this problem is to use warm-up, as described by (S\u00f8nderby et al., 2016).", "startOffset": 72, "endOffset": 95}, {"referenceID": 4, "context": ", 2015b;a), Dynamic Network Surgery (Guo et al., 2016) and Soft Weight Sharing (Ullrich et al.", "startOffset": 36, "endOffset": 54}, {"referenceID": 31, "context": ", 2016) and Soft Weight Sharing (Ullrich et al., 2017).", "startOffset": 32, "endOffset": 54}, {"referenceID": 15, "context": "A modified version of LeNet5 from (LeCun et al., 1998).", "startOffset": 34, "endOffset": 54}, {"referenceID": 4, "context": ", 2015b;a), DNS (Guo et al., 2016), SWS (Ullrich et al.", "startOffset": 16, "endOffset": 34}, {"referenceID": 31, "context": ", 2016), SWS (Ullrich et al., 2017)) on LeNet architectures.", "startOffset": 13, "endOffset": 35}, {"referenceID": 35, "context": "Recently is was shown that the CNNs are capable of memorizing the data even with random labeling (Zhang et al., 2016).", "startOffset": 97, "endOffset": 117}, {"referenceID": 20, "context": "In the Relevance Tagging Machine model (Molchanov et al., 2015) the same effect was achieved using Beta distributions as a prior.", "startOffset": 39, "endOffset": 63}, {"referenceID": 31, "context": "This result correlates with results of other works on training of sparse neural networks (Han et al., 2015a; Wen et al., 2016; Ullrich et al., 2017; Soravit Changpinyo, 2017).", "startOffset": 89, "endOffset": 174}, {"referenceID": 35, "context": "Further investigation of such redundancy may lead to an understanding of generalization properties of DNNs and explain the phenomenon, observed by (Zhang et al., 2016).", "startOffset": 147, "endOffset": 167}, {"referenceID": 31, "context": "quantization and Huffman coding (Han et al., 2015a; Ullrich et al., 2017), as they use sparsification as an intermediate step.", "startOffset": 32, "endOffset": 73}], "year": 2017, "abstractText": "We explore a recently proposed Variational Dropout technique that provided an elegant Bayesian interpretation to Gaussian Dropout. We extend Variational Dropout to the case when dropout rates are unbounded, propose a way to reduce the variance of the gradient estimator and report first experimental results with individual dropout rates per weight. Interestingly, it leads to extremely sparse solutions both in fullyconnected and convolutional layers. This effect is similar to automatic relevance determination effect in empirical Bayes but has a number of advantages. We reduce the number of parameters up to 280 times on LeNet architectures and up to 68 times on VGG-like networks with a negligible decrease of accuracy.", "creator": "LaTeX with hyperref package"}}}