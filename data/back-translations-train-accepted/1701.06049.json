{"id": "1701.06049", "review": {"conference": "ICML", "VERSION": "v1", "DATE_OF_SUBMISSION": "21-Jan-2017", "title": "Interactive Learning from Policy-Dependent Human Feedback", "abstract": "For agents and robots to become more useful, they must be able to quickly learn from non-technical users. This paper investigates the problem of interactively learning behaviors communicated by a human teacher using positive and negative feedback. Much previous work on this problem has made the assumption that people provide feedback for decisions that is dependent on the behavior they are teaching and is independent from the learner's current policy. We present empirical results that show this assumption to be false---whether human trainers give a positive or negative feedback for a decision is influenced by the learner's current policy. We argue that policy-dependent feedback, in addition to being commonplace, enables useful training strategies from which agents should benefit. Based on this insight, we introduce Convergent Actor-Critic by Humans (COACH), an algorithm for learning from policy-dependent feedback that converges to a local optimum. Finally, we demonstrate that COACH can successfully learn multiple behaviors on a physical robot, even with noisy image features.", "histories": [["v1", "Sat, 21 Jan 2017 16:37:41 GMT  (107kb,D)", "http://arxiv.org/abs/1701.06049v1", "7 pages, 2 figures"]], "COMMENTS": "7 pages, 2 figures", "reviews": [], "SUBJECTS": "cs.AI", "authors": ["james macglashan", "mark k ho", "robert tyler loftin", "bei peng", "guan wang", "david l roberts", "matthew e taylor", "michael l littman"], "accepted": true, "id": "1701.06049"}, "pdf": {"name": "1701.06049.pdf", "metadata": {"source": "CRF", "title": "Interactive Learning from Policy-Dependent Human Feedback", "authors": ["James MacGlashan", "Mark K Ho", "Robert Loftin", "Bei Peng", "David Roberts", "Matthew E. Taylor", "Michael L. Littman", "\u2217Cogitai", "\u2020Brown"], "emails": [], "sections": [{"heading": null, "text": "In this context, it should be noted that most people who are able to specify and adapt complex behavior rely on giving instructions to robots that are programmatically doomed to failure because of this complexity. Reinforcement learning (RL) from human trainers offers a compelling alternative to programming, as agents can learn complex behavior from very simple positive and negative signals. In addition, real animal training is evidence that humans can train complex behavior with these simple signals. In fact, animals have been successfully trained to guide the blind, locate mines in the oceans, detect cancers or explosions, and even solve complex, multi-level puzzles."}, {"heading": "II. BACKGROUND", "text": "For modeling the underlying decision problem of an agent taught by a human being, we take the Markov decision process (MDP).An MDP is a 5-fold: < S, A, T, R, \u03b3 >, where S is the set of possible states of the environment; A is the set of actions available to the agent; T (s) is the transition function that defines the likelihood of the transition of the environment to the state \"s\" when the actor adopts measures affecting an environmental state s; R (s, s \") is the reward function that indicates the numerical reward the actor receives for action and the transition to the state\" s, \"and Q (s\") is a discount factor that indicates how much immediate rewards are preferred over more distant rewards."}, {"heading": "III. HUMAN-CENTERED REINFORCEMENT LEARNING", "text": "In this paper, a human-centered reinforcement learning problem (HCRL) is a learning problem where an agent is in an environment described by an MDP, but where rewards are generated by a human trainer, rather than a stationary MDP reward function that the agent is supposed to maximize. The agent has a target policy. To define a learning algorithm for this problem, we first describe how human trainers typically use numerical feedback to teach target guidelines. If feedback is stationary and to be maximized, it can be treated as a reward function and as standard RL algorithms that are used. Although this approach has had some success [8, 9], there are complications that limit its applicability. In particular, a trainer must be careful that the feedback they provide does not contain unforeseen exploitation."}, {"heading": "IV. POLICY-DEPENDENT FEEDBACK", "text": "For example, it has been observed that trainers reduce their feedback during the learning process [11, 12, 9]. One explanation for declining feedback rates is policy-dependent feedback, but coach fatigue is another. We provide a stronger result showing that trainers - for the same state-action pair - choose positive or negative feedback depending on their perception of the learner's behavior. This finding serves as a warning to algorithms based on assumption-policy-independent feedback."}, {"heading": "A. Empirical Results", "text": "We had Amazon Mechanical Turk (AMT) participants teach an agent in a simple sequential task, illustrated in Figure 1. < Participants were instructed to train a virtual dog to go to the yellow target position in a grid world as quickly as possible, but without going through the green cells. They were also told that as a result of previous training, their dog was either \"bad,\" \"fine\" or \"good\" at the task and were shown examples of each behavior prior to training. In all cases, the dog would start at the location shown in Figure 1. \"Bad\" dogs went directly through the green cells into the yellow cell. \"Alright\" dogs moved first to the left, then up, and then to the goal, not taking the shortest route to yellow without going through the green phase. During training, participants saw the dog take an action from one tile to another and then gave feedback after each action with a continuously labeled pusher, as shown."}, {"heading": "B. Training Strategies", "text": "Apart from the fact that our findings to date suggest that people provide policy-based feedback, we argue that policy-based feedback provides desirable training strategies. Specifically, we consider three distinct feedback schemes that can be considered operationalizations of well-studied behavioral analysis [13]: Diminishing Returns: providing positive feedback for suboptimal measures that improve behavior, and then negative feedback after improvement. Reducing feedback is a useful strategy because it reduces the burden of how actively a trainer needs to provide feedback, and eliminates the need for explicit training and execution phases. Differential feedback is useful because it can serve to emphasize the most important behaviors in the state space, and to emphasize them as a way of communicating."}, {"heading": "V. CONVERGENT ACTOR-CRITIC BY HUMANS", "text": "In this section, we introduce Convergent Actor-Critic by Humans (COACH), an algorithm based on actors and critics that is able to learn from policy-dependent feedback. COACH is based on the realization that the benefit function is a good model of human feedback, and that Actor-Critic algorithms update a policy by using the TD error of the critic, which is an unbiased estimate of the benefit function. Consequently, an agent's policy can be changed directly through human feedback without critical components. First, we define the benefit function and describe how it relates to the three aforementioned training strategies. Then, we present the general updating rule for COACH and its convergence. Finally, we present COACH in real time, which includes mechanisms for providing different-sized feedback and learning in problems with a high-frequency decision cycle."}, {"heading": "A. The Advantage Function and Training Strategies", "text": "Since the expected value is a smooth combination of Q > Q > Q (Q > Q) - Q (s) - Q (s) - Q (s) - Q (s) - Q (s) - Q (s) - Q (s). (1) Broadly speaking, the Advantage Function describes how much better or worse an action selection is compared to the performance of the agent according to his current policy. To show that the Advantage Function follows the patterns of all three training strategies, consider what happens when the learner improves his behavior by moving to a higher action. As the probability of selecting an agent is 1, Ap (s) - (s) - (s) - (s -) - (s -) - (s - (s -) - (s -) - (s -) - (s -) - (s -) - (s -) - (s - - (s -) - (s -) - (s - (s -) - (s - (s -)."}, {"heading": "B. Convergence and Update Rule", "text": "In view of a measure of success that controls the behavior of the agent, Sutton et al. [15] derive a policy gradient algorithm from the form. Assuming that this is the discounted expected reward from a fixed distribution of the starting state, they show that this form of gradient consists in the fact that, given that states are visited according to d\u03c0 (s, a) and measures are taken accordingly (s, a), the update cannot be made in due course as follows:. (st, at) + 1p (st, at), (st, at), (2), if states are visited according to d\u03c0 (s) and measures are taken accordingly (s, a), the update cannot take place in due course as follows: (st, at). (st, at) + 1p (st, at), (st, at), (st), (st)."}, {"heading": "C. Real-time COACH", "text": "There are challenges in implementing Equation 2 for real-time use in practice. Specifically, the interface for providing variable-scale feedback needs to be addressed, and the question of how to deal with the scarcity and timing of feedback needs to be answered. At this point, we are introducing real-time COACH, shown in Algorithm 1, to address these issues. To provide variable-scale reward aggregation, we use reward aggregation [1]. In the reward aggregation, a trainer selects from a discrete set of feedback values and further increases or lowers the numerical value by giving multiple feedbacks in succession, which are summarized. While sparse feedback is not particularly problematic (because no feedback leads to no policy change), it can slow learning if the trainer is not equipped with a mechanism to allow feedback to influence a story of actions."}, {"heading": "VI. RELATED WORK", "text": "In this case, it is a reactionary group that will be able to retaliate."}, {"heading": "VII. ROBOTICS CASE STUDY", "text": "It is. (It is.) It is. (It is.) It is. (It is.) It is. (It is.) It is. (It is.) It is. (It is.) It is. (It is.) It is. (it is.) It is. (it is.) It is. (it is.) It is. (it is.) It is. (it is.) It is. (it is.) It is. (it is.) It is. (it.) It is. (it.) It is. (it.) It is. (it.) It is. (it.) It is. (it.) It is. (it.) It is. (it.) It is. (it.) It is. (it.) It is. (it.) It is. (it.) It is. (it. (it.) It is. (it.) It is. (it. (it.) It is. (it. (it.) It is. (it. (it.) It is. (it. (it.) It is. (it. (it.) It. (it. (it.) It is. (it. (it.) It. (it. (it.) It. (it. (it.) is. (it. (it.) It. (it. (it. (it.) It. (it.) It. (it. (it. (it.) It. (it. (it.) It. (it. (it. (it.) It. (it. (it.) It. (it. (it.) is. (it.) is. (it. (it.) It. (it. (it. (it. (it.)"}, {"heading": "A. Results and Discussion", "text": "COACH was able to successfully learn all five behaviors, and a video showing its learning behavior is available online at https: / / vid.me / 3h2s. Each of these behaviors was trained in less than two minutes, including the time spent checking whether a behavior works. Differential feedback and decreasing returns only allowed the behaviors that require coordination to be quickly amplified or deleted without an explicit separation between training and testing. In addition, the agent successfully benefited from compositional training methods by correctly combining subbehaviors for variety, and fast learning cylinder navigation with lure.TAMER only successfully learned the behaviors by not learning the compositively trained behaviors. In all cases, TAMER tended to forget the behavior by returning feedback for previous decisions he had learned after making a new decision."}, {"heading": "VIII. CONCLUSION", "text": "In this paper, we presented empirical results showing that the numerical feedback that people give to agents in an interactive training paradigm is influenced by the current policy of the agent and argued why such policy-dependent feedback enables useful training strategies. Finally, we introduced COACH, an algorithm that, unlike existing human-centered reinforcement learning algorithms, converges with policy-centered feedback to a local optimum. Finally, we demonstrated that COACH scales in the context of a robotic case study in which a TurtleBot has successfully taught multiple behaviors with advanced training methods. There are a number of exciting future directions to expand this work. Secondly, because COACH builds on the actor critique paradigm in particular, it should be possible to combine it simply with learning from demonstration and environmental rewards, allowing an agent to be trained in many ways. Secondly, because people could provide policy-centered feedback by examining current policies, such as greater gains from actual policy and environmental rewards."}], "references": [{"title": "Interactively shaping agents via human reinforcement: The tamer framework", "author": ["W.B. Knox", "P. Stone"], "venue": "Proceedings of the fifth international conference on Knowledge capture. ACM, 2009, pp. 9\u201316.", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2009}, {"title": "Reinforcement learning with human teachers: Evidence of feedback and guidance with implications for learning performance", "author": ["A.L. Thomaz", "C. Breazeal"], "venue": "AAAI, vol. 6, 2006, pp. 1000\u20131005.", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2006}, {"title": "Robot learning via socially guided exploration", "author": ["A. Thomaz", "C. Breazeal"], "venue": "ICDL Development on Learning, 2007.", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2007}, {"title": "Teachable robots: Understanding human teaching behavior to build more effective robot learners", "author": ["A.L. Thomaz", "C. Breazeal"], "venue": "2008.", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2008}, {"title": "Policy shaping: Integrating human feedback with reinforcement learning", "author": ["S. Griffith", "K. Subramanian", "J. Scholz", "C. Isbell", "A.L. Thomaz"], "venue": "Advances in Neural Information Processing Systems, 2013, pp. 2625\u20132633.", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2013}, {"title": "Learning behaviors via human-delivered discrete feedback: modeling implicit feedback strategies to speed up learning", "author": ["R. Loftin", "B. Peng", "J. MacGlashan", "M.L. Littman", "M.E. Taylor", "J. Huang", "D.L. Roberts"], "venue": "Autonomous Agents and Multi-Agent Systems, vol. 30, no. 1, pp. 30\u2013 59, 2015.", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2015}, {"title": "Natural actor\u2013critic algorithms", "author": ["S. Bhatnagar", "R.S. Sutton", "M. Ghavamzadeh", "M. Lee"], "venue": "Automatica, vol. 45, no. 11, pp. 2471\u20132482, 2009.", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2009}, {"title": "Online human training of a myoelectric prosthesis controller via actor-critic reinforcement learning", "author": ["P.M. Pilarski", "M.R. Dawson", "T. Degris", "F. Fahimi", "J.P. Carey", "R.S. Sutton"], "venue": "2011 IEEE International Conference on Rehabilitation Robotics. IEEE, 2011, pp. 1\u20137.", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2011}, {"title": "A social reinforcement learning agent", "author": ["C. Isbell", "C.R. Shelton", "M. Kearns", "S. Singh", "P. Stone"], "venue": "Proceedings of the fifth international conference on Autonomous agents. ACM, 2001, pp. 377\u2013384.", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2001}, {"title": "Learning from human-generated reward", "author": ["W.B. Knox"], "venue": "Ph.D. dissertation, University of Texas at Austin, 2012.", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2012}, {"title": "How humans teach agents", "author": ["W.B. Knox", "B.D. Glass", "B.C. Love", "W.T. Maddox", "P. Stone"], "venue": "International Journal of Social Robotics, vol. 4, no. 4, pp. 409\u2013421, 2012.", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2012}, {"title": "Miltenberger, Behavior modification: Principles and procedures", "author": ["G. R"], "venue": "Cengage Learning,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2011}, {"title": "Residual algorithms: Reinforcement learning with function approximation", "author": ["L. Baird"], "venue": "Proceedings of the twelfth international conference on machine learning, 1995, pp. 30\u201337.", "citeRegEx": "14", "shortCiteRegEx": null, "year": 1995}, {"title": "Policy gradient methods for reinforcement learning with function approximation.", "author": ["R.S. Sutton", "D.A. McAllester", "S.P. Singh", "Y. Mansour"], "venue": "in NIPS, vol", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 1999}, {"title": "Neuronlike adaptive elements that can solve difficult learning control problems", "author": ["A. Barto", "R. Sutton", "C. Anderson"], "venue": "Systems, Man and Cybernetics, IEEE Transactions on, vol. SMC-13, no. 5, pp. 834 \u2013846, sept.-oct. 1983.", "citeRegEx": "16", "shortCiteRegEx": null, "year": 1983}, {"title": "Learning non-myopically from human-generated reward", "author": ["W.B. Knox", "P. Stone"], "venue": "Proceedings of the 2013 international conference on Intelligent user interfaces. ACM, 2013, pp. 191\u2013202.", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2013}, {"title": "Combining manual feedback with subsequent MDP reward signals for reinforcement learning", "author": ["B. Knox", "P. Stone"], "venue": "Proc. of 9th Int. Conf. on Autonomous  Agents and Multiagent Systems, 2010.", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2010}, {"title": "Dynamic reward shaping: training a robot by voice", "author": ["A.C. Tenorio-Gonzalez", "E.F. Morales", "L. Villase\u00f1or- Pineda"], "venue": "Advances in Artificial Intelligence\u2013IBERAMIA 2010. Springer, 2010, pp. 483\u2013492.", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2010}, {"title": "A teaching method for reinforcement learning", "author": ["J.A. Clouse", "P.E. Utgoff"], "venue": "Proceedings of the Ninth International Conference on Machine Learning (ICML92), 1992, pp. 92\u2013101.", "citeRegEx": "20", "shortCiteRegEx": null, "year": 1992}, {"title": "Giving advice about preferred actions to reinforcement learners via knowledge-based kernel regression", "author": ["R. Maclin", "J. Shavlik", "L. Torrey", "T. Walker", "E. Wild"], "venue": "Proceedings of the National Conference on Artificial intelligence, vol. 20, no. 2, 2005, p. 819.", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2005}, {"title": "A survey of robot learning from demonstration", "author": ["B.D. Argall", "S. Chernova", "M. Veloso", "B. Browning"], "venue": "Robotics and autonomous systems, vol. 57, no. 5, pp. 469\u2013483, 2009.", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2009}, {"title": "Training a robot via human feedback: A case study", "author": ["W.B. Knox", "P. Stone", "C. Breazeal"], "venue": "Social Robotics. Springer, 2013, pp. 460\u2013470.", "citeRegEx": "23", "shortCiteRegEx": null, "year": 2013}, {"title": "Reinforcement learning: An introduction", "author": ["R.S. Sutton", "A.G. Barto"], "venue": "MIT press Cambridge,", "citeRegEx": "24", "shortCiteRegEx": "24", "year": 1998}], "referenceMentions": [{"referenceID": 0, "context": "This failure has led to the development of new reinforcement-learning algorithms that are designed to learn from human-generated rewards and investigations into how people give interactive feedback [1, 2, 3, 4, 5, 6].", "startOffset": 198, "endOffset": 216}, {"referenceID": 1, "context": "This failure has led to the development of new reinforcement-learning algorithms that are designed to learn from human-generated rewards and investigations into how people give interactive feedback [1, 2, 3, 4, 5, 6].", "startOffset": 198, "endOffset": 216}, {"referenceID": 2, "context": "This failure has led to the development of new reinforcement-learning algorithms that are designed to learn from human-generated rewards and investigations into how people give interactive feedback [1, 2, 3, 4, 5, 6].", "startOffset": 198, "endOffset": 216}, {"referenceID": 3, "context": "This failure has led to the development of new reinforcement-learning algorithms that are designed to learn from human-generated rewards and investigations into how people give interactive feedback [1, 2, 3, 4, 5, 6].", "startOffset": 198, "endOffset": 216}, {"referenceID": 4, "context": "This failure has led to the development of new reinforcement-learning algorithms that are designed to learn from human-generated rewards and investigations into how people give interactive feedback [1, 2, 3, 4, 5, 6].", "startOffset": 198, "endOffset": 216}, {"referenceID": 5, "context": "This failure has led to the development of new reinforcement-learning algorithms that are designed to learn from human-generated rewards and investigations into how people give interactive feedback [1, 2, 3, 4, 5, 6].", "startOffset": 198, "endOffset": 216}, {"referenceID": 0, "context": "the reward function specifying the numeric reward the agent receives for taking action a in state s and transitioning to state s\u2032; and \u03b3 \u2208 [0, 1] is a discount factor specifying how much immediate rewards are preferred to more distant rewards.", "startOffset": 139, "endOffset": 145}, {"referenceID": 0, "context": "A stochastic policy \u03c0 for an MDP is a per-state action probability distribution that defines a mode of behavior; \u03c0 : S \u00d7 A \u2192 [0, 1], where \u2211 a\u2208A \u03c0(s, a) = 1,\u2200s \u2208 S.", "startOffset": 125, "endOffset": 131}, {"referenceID": 6, "context": "[7] provides a general template for these algorithms.", "startOffset": 0, "endOffset": 3}, {"referenceID": 7, "context": "Although this approach has had some success [8, 9], there are complications", "startOffset": 44, "endOffset": 50}, {"referenceID": 8, "context": "Although this approach has had some success [8, 9], there are complications", "startOffset": 44, "endOffset": 50}, {"referenceID": 9, "context": "Indeed, prior research has shown that interpreting human feedback like a reward function often induces positive reward cycles that leads to unintended behaviors [10, 11].", "startOffset": 161, "endOffset": 169}, {"referenceID": 0, "context": "\u201d Existing HCRL work adopting this perspective includes TAMER [1], SABL [6], and Policy Shaping [5], discussed in more detail in", "startOffset": 62, "endOffset": 65}, {"referenceID": 5, "context": "\u201d Existing HCRL work adopting this perspective includes TAMER [1], SABL [6], and Policy Shaping [5], discussed in more detail in", "startOffset": 72, "endOffset": 75}, {"referenceID": 4, "context": "\u201d Existing HCRL work adopting this perspective includes TAMER [1], SABL [6], and Policy Shaping [5], discussed in more detail in", "startOffset": 96, "endOffset": 99}, {"referenceID": 10, "context": "For example, it was observed that trainers taper their feedback over the course of learning [11, 12, 9].", "startOffset": 92, "endOffset": 103}, {"referenceID": 8, "context": "For example, it was observed that trainers taper their feedback over the course of learning [11, 12, 9].", "startOffset": 92, "endOffset": 103}, {"referenceID": 11, "context": "analysis reinforcement schedules [13]: Diminishing Returns: gradually decrease positive feedback for good actions as the", "startOffset": 33, "endOffset": 37}, {"referenceID": 12, "context": "The advantage function [14] A is defined as", "startOffset": 23, "endOffset": 27}, {"referenceID": 13, "context": "[15] derive a policy gradient algorithm of the form: \u2206\u03b8 = \u03b1\u2207\u03b8\u03c1.", "startOffset": 0, "endOffset": 4}, {"referenceID": 0, "context": "For providing variable magnitude reward, we use reward aggregation [1].", "startOffset": 67, "endOffset": 70}, {"referenceID": 14, "context": "We use eligibility traces [16] to help apply feedback to the relevant transitions.", "startOffset": 26, "endOffset": 30}, {"referenceID": 9, "context": "8 seconds from the event to which they meant to give feedback [10].", "startOffset": 62, "endOffset": 66}, {"referenceID": 9, "context": "An inspiration for our work is the TAMER framework [10].", "startOffset": 51, "endOffset": 55}, {"referenceID": 15, "context": "Later work investigated nonmyopically maximizing the learned reward function with a planning algorithm [17], but this approach requires a model of the environment and special treatment of termination conditions.", "startOffset": 103, "endOffset": 107}, {"referenceID": 5, "context": "Two other closely related approaches are SABL [6] and Policy Shaping [5] (unrelated to the policy shaping feedback strategy defined above).", "startOffset": 46, "endOffset": 49}, {"referenceID": 4, "context": "Two other closely related approaches are SABL [6] and Policy Shaping [5] (unrelated to the policy shaping feedback strategy defined above).", "startOffset": 69, "endOffset": 72}, {"referenceID": 7, "context": "There have also been some domains in which treating human feedback as reward signals to maximize has had some success, such as in shaping the control for a prosthetic arm [8] and learning how to interact in an online chat room from multiple users\u2019 feedback [9].", "startOffset": 171, "endOffset": 174}, {"referenceID": 8, "context": "There have also been some domains in which treating human feedback as reward signals to maximize has had some success, such as in shaping the control for a prosthetic arm [8] and learning how to interact in an online chat room from multiple users\u2019 feedback [9].", "startOffset": 257, "endOffset": 260}, {"referenceID": 16, "context": "Some research has examined combining human feedback with more traditional environmental rewards [18, 19, 20, 21].", "startOffset": 96, "endOffset": 112}, {"referenceID": 17, "context": "Some research has examined combining human feedback with more traditional environmental rewards [18, 19, 20, 21].", "startOffset": 96, "endOffset": 112}, {"referenceID": 18, "context": "Some research has examined combining human feedback with more traditional environmental rewards [18, 19, 20, 21].", "startOffset": 96, "endOffset": 112}, {"referenceID": 19, "context": "Some research has examined combining human feedback with more traditional environmental rewards [18, 19, 20, 21].", "startOffset": 96, "endOffset": 112}, {"referenceID": 20, "context": "[22].", "startOffset": 0, "endOffset": 4}, {"referenceID": 21, "context": "We chose TAMER as a comparison because, to our knowledge, it is the only HCRL algorithm with success on a similar platform [23].", "startOffset": 123, "endOffset": 127}, {"referenceID": 22, "context": "Additionally, we used an actorcritic parameter-update rule variant in which action preference values are directly modified (along its gradient), rather than by the gradient of the policy [24].", "startOffset": 187, "endOffset": 191}, {"referenceID": 9, "context": "8 seconds, p = 0, and cmin = 1 [10].", "startOffset": 31, "endOffset": 35}], "year": 2017, "abstractText": "For agents and robots to become more useful, they must be able to quickly learn from non-technical users. This paper investigates the problem of interactively learning behaviors communicated by a human teacher using positive and negative feedback. Much previous work on this problem has made the assumption that people provide feedback for decisions that is dependent on the behavior they are teaching and is independent from the learner\u2019s current policy. We present empirical results that show this assumption to be false\u2014whether human trainers give a positive or negative feedback for a decision is influenced by the learner\u2019s current policy. We argue that policy-dependent feedback, in addition to being commonplace, enables useful training strategies from which agents should benefit. Based on this insight, we introduce Convergent Actor-Critic by Humans (COACH), an algorithm for learning from policy-dependent feedback that converges to a local optimum. Finally, we demonstrate that COACH can successfully learn multiple behaviors on a physical robot, even with noisy image features.", "creator": "LaTeX with hyperref package"}}}