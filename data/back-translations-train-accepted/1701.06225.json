{"id": "1701.06225", "review": {"conference": "AAAI", "VERSION": "v1", "DATE_OF_SUBMISSION": "22-Jan-2017", "title": "Predicting Demographics of High-Resolution Geographies with Geotagged Tweets", "abstract": "In this paper, we consider the problem of predicting demographics of geographic units given geotagged Tweets that are composed within these units. Traditional survey methods that offer demographics estimates are usually limited in terms of geographic resolution, geographic boundaries, and time intervals. Thus, it would be highly useful to develop computational methods that can complement traditional survey methods by offering demographics estimates at finer geographic resolutions, with flexible geographic boundaries (i.e. not confined to administrative boundaries), and at different time intervals. While prior work has focused on predicting demographics and health statistics at relatively coarse geographic resolutions such as the county-level or state-level, we introduce an approach to predict demographics at finer geographic resolutions such as the blockgroup-level. For the task of predicting gender and race/ethnicity counts at the blockgroup-level, an approach adapted from prior work to our problem achieves an average correlation of 0.389 (gender) and 0.569 (race) on a held-out test dataset. Our approach outperforms this prior approach with an average correlation of 0.671 (gender) and 0.692 (race).", "histories": [["v1", "Sun, 22 Jan 2017 22:16:46 GMT  (165kb,D)", "http://arxiv.org/abs/1701.06225v1", "6 pages, AAAI-17 preprint"]], "COMMENTS": "6 pages, AAAI-17 preprint", "reviews": [], "SUBJECTS": "cs.LG cs.SI stat.ML", "authors": ["omar montasser", "daniel kifer"], "accepted": true, "id": "1701.06225"}, "pdf": {"name": "1701.06225.pdf", "metadata": {"source": "CRF", "title": "Predicting Demographics of High-Resolution Geographies with Geotagged Tweets", "authors": ["Omar Montasser", "Daniel Kifer"], "emails": ["ovm5033@psu.edu,", "dkifer@cse.psu.edu"], "sections": [{"heading": "1 Introduction", "text": "In fact, most of them are able to survive on their own, without being able to survive on their own."}, {"heading": "2 Related Work", "text": "Indeed, the number of those who are able to abide by the rules is very high. (...) The number of those who are able to understand the rules is very high. (...) The number of those who are able to understand the rules is very high. (...) The number of those who are able to understand the rules is very high. (...) The number of those who are able to understand the rules is high. (...) The number of those who are able to understand the rules is very high. (...) The number of those who are able to understand the rules, to understand the rules, to understand the rules. \"(...) The number of those who are able to understand the rules, to understand the rules.\""}, {"heading": "3 Preliminaries", "text": "In this section, we set some necessary definitions, notations and formally define the problem. Our data set is a set of tweets {t1, t2,..., tm}. Each tweet ti is a tuple of the form (loci, uidi, < wi1, w i 2, w i 3,... >), where loc is the GPS position, uid is the user ID and < w1, w2, w3,... > is the order of the tokens in the tweet. We have a set of geographic units {g1, g2,..., gn}. Each gi is a tuple of the form (shapei, yi), where form is the boundary definition and y is the basic truth of a demographic variable (e.g. gender) of the geographic unit. If we have a demographic variable with k-excluding categories, then y is a k vector where yj is the number of category malj, j [Twe1] is for the unit tjj = 1 (i.e., for the unit tjj = 1)."}, {"heading": "4 Method", "text": "In this section we describe our approach to learning the model f. Our approach is based on the calculation of a characteristic vector xi-RD given for each geographical unit i-i [1, n]. Afterwards we adapt a model to predict y-i given xi. In the following we discuss possible characteristic developments and modeling possibilities."}, {"heading": "4.1 Feature Engineering", "text": "In line with previous work, we focus on functions based on lexical content, motivated by research into the most predictive linguistic patterns of demographic development. First, we discuss possible lexical characteristics, then we discuss possible normalization and transformation programs that can be applied to these characteristics. There are several possible lexical characteristics that can be used to represent geographical units. These characteristics are not mutually exclusive and can be combined with each other. Lexicons are predefined word-to-category mappings that can be used to represent each geographical unit by the frequency of each category (Schwartz et al. 2013b; Culotta 2014a). Lexicons generally have stronger domain assumptions (compared to bag-of-words) (OConnor, Bamman and Smith 2011) and are limited to specific applications such as health and personality."}, {"heading": "4.2 Modeling", "text": "We examine two variants of the problem: predicting the demography of geographical units when the population size is unknown and the population size is known. In this context, we would like to predict demographic numbers (e.g. gender) yi, j of a region gi for each category (e.g. male and female) j [1, k] without access to population size gi. We choose a linear regression model for scalability. For each category j [1, k] we optimize the following objective function: wj = argmin wj12n n \u2211 i = 1 (wj \u00b7 xi \u2212 yi, j) 2 + \u03bb | 22, where wj is the weight vector learned for category j, and \u03bb is an l2 regulation parameter to prevent overadjustment. We also examined l1 and ElasticNet regulations, but they yielded similar results. For a region gu with an unknown demographic category j, yy, y, y, y, we use the yj function in this category j, y, y, y, y, y, y, y, y, y, y, y, y, y, y, y, y, y, y, y."}, {"heading": "5 Experiments", "text": "We evaluate our approach and competing approaches (baselines) to both variants of the problem using predefined geographies with different resolutions: block, block group, tract and circle. Of the four, block level is the highest resolution and circle level the lowest resolution. In the following subsections we provide details of our experiments: baselines, data, pre-processing, training and results."}, {"heading": "5.1 Baselines", "text": "We compare our approach to an approach adopted by (Mohammady and Culotta 2014), where they used tweets to predict the ethnic composition of counties. In their approach, they used a sack-of-words representation that was normalized by Twitter users with tweeted words and words from Twitter users \"description fields as characteristics. We customize their approach by using the same types of characteristics. We calculate a sack-of-words representation using user normalization, and then we train a model with that representation and evaluate it nationally for both variants of the problem. Note that in our competing configurations we do not use characteristics from the description field of Twitter users. In the environment in which the population size is known, we also compare our models with a baseline that always uses gender and race / ethnicity proportions nationally to predict the number of blocks, block groups, tracts, and counties (12.8%) (61.6% of the population size is black, 61.6% African) or 61.6% national estimates for American."}, {"heading": "5.2 Data", "text": "We collected a large dataset of geotagged tweets using the Twitters Streaming API from June 12, 2013 to January 31, 2014. We only included tweets from the contiguous United States consisting of the 48 contiguous states and Washington, D.C., and did not include, for example, Alaska and Hawaii. We used an input field with [125,0011, 66,9326] W \u00d7 [24,9493, 49.5904] N. Based on the GPS coordinates of a tweet, we comment on it with the geographic identifier (GEOID) of the block in which it appeared. The US Census Bureau provides geographic boundary files (shapefiles) for each state in which each shape file contains the boundary definitions for all blocks in that state, allowing us to match each tweet to its specific block."}, {"heading": "5.3 Preprocessing", "text": "Twitter is full of spam and organization accounts that post content that we consider irrelevant to our application because we are interested in content produced by personal accounts. To reduce the likelihood that content from organization / spam accounts will contain content, we removed tweets from accounts with more than 1,000 followers or 1,000 friends (Lee, Eoff and Caverlee 2011; McCorriston, Jurgens and Ruths 2015) and tweets that contain URLs (Guo and Chen 2014). We also removed retweets by verifying the presence of retweet status fields or the RT token in the tweet text itself. As a result, our record was narrowed to 423,622,202 tweets with 4,027,594 unique Twitter users. To build a sack-of-words representation, we shared the text of the tweets in unigram or RT tokens in the tweet text itself. There are several things we need to consider when tweeting emoji tags, such as:"}, {"heading": "5.4 Training", "text": "For a given geographical resolution, we randomly divide the geographical units that have tweets into 90% training and 10% testing (e.g., we train 90% of the blocks and forecast the demographics of the remaining 10%).10% of the geographical units in the Training Split were randomly selected as a validation rate. Note that this splitting is performed separately for each geographical resolution.We have ntrain - ntest examples: 5,188,608 - 576,513 (block); 194,610 - 21,624 (block group); 65,239 - 7,249 (wing); 2,798 - 311 (district).Then we calculate a configuration (e.g. Raw User with TFIDF) and use all words that appear as characteristics in the Training Split (more than 22 million characteristics).Since it is a major learning problem, we use stochastic parameters of descent (SGD) to set a value of 0.001 actualization rate, which we must use in the Training SD, which is a learning rate."}, {"heading": "5.5 Results", "text": "We evaluate the fundamentals and our models (with different characteristics of construction) on both sides of the problem (population is unknown and known). In both variants, we predict demographic categories (yj for j, k) and evaluate the results of our experiments using Pearl correlations and the coefficient of determination R2. Note that R2 always compares the performance of a model in terms of the fundamentals of prediction to the average value of the test set. Table 2 summarizes the results of our experiments, which we only include in terms of the results of our best configurations."}, {"heading": "6 Conclusion", "text": "Our method can be used as an alternative or complement to survey methods. We have shown that certain feature transformations such as Anscombe, TFIDF, Logistic, and Gauss significantly improve prediction performance compared to competing baselines. We have also shown that our method is capable of learning the proportions of demographic categories and delivering accurate predictions in regions with at least 100 Twitter users. In future work, it is worth drawing attention to the effect of the sampling rate on the prediction. According to Eisenstein et al. (2014), word frequencies normalized by users are non-invariant to the sampling rate of the data. If we remove half of the tweets, these frequencies will decrease because the number of users decreases more slowly than the number of raw words. Therefore, it would be interesting to investigate methods that minimize the deviation of such normalizations to the sampling rate."}, {"heading": "7 Acknowledgments", "text": "This work was supported by NSF Prize # 1054389. We also thank Guangqing Chi for providing the Twitter data."}], "references": [], "referenceMentions": [], "year": 2017, "abstractText": "In this paper, we consider the problem of predicting demographics of geographic units given geotagged Tweets that are composed within these units. Traditional survey methods that offer demographics estimates are usually limited in terms of geographic resolution, geographic boundaries, and time intervals. Thus, it would be highly useful to develop computational methods that can complement traditional survey methods by offering demographics estimates at finer geographic resolutions, with flexible geographic boundaries (i.e. not confined to administrative boundaries), and at different time intervals. While prior work has focused on predicting demographics and health statistics at relatively coarse geographic resolutions such as the county-level or state-level, we introduce an approach to predict demographics at finer geographic resolutions such as the blockgroup-level. For the task of predicting gender and race/ethnicity counts at the blockgrouplevel, an approach adapted from prior work to our problem achieves an average correlation of 0.389 (gender) and 0.569 (race) on a held-out test dataset. Our approach outperforms this prior approach with an average correlation of 0.671 (gender) and 0.692 (race).", "creator": "LaTeX with hyperref package"}}}