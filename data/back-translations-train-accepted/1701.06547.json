{"id": "1701.06547", "review": {"conference": "EMNLP", "VERSION": "v1", "DATE_OF_SUBMISSION": "23-Jan-2017", "title": "Adversarial Learning for Neural Dialogue Generation", "abstract": "In this paper, drawing intuition from the Turing test, we propose using adversarial training for open-domain dialogue generation: the system is trained to produce sequences that are indistinguishable from human-generated dialogue utterances. We cast the task as a reinforcement learning (RL) problem where we jointly train two systems, a generative model to produce response sequences, and a discriminator---analagous to the human evaluator in the Turing test--- to distinguish between the human-generated dialogues and the machine-generated ones. The outputs from the discriminator are then used as rewards for the generative model, pushing the system to generate dialogues that mostly resemble human dialogues.", "histories": [["v1", "Mon, 23 Jan 2017 18:32:27 GMT  (4857kb)", "http://arxiv.org/abs/1701.06547v1", null], ["v2", "Thu, 26 Jan 2017 01:42:13 GMT  (4851kb)", "http://arxiv.org/abs/1701.06547v2", null], ["v3", "Fri, 10 Feb 2017 14:02:56 GMT  (4842kb)", "http://arxiv.org/abs/1701.06547v3", null], ["v4", "Wed, 22 Feb 2017 08:36:59 GMT  (4842kb)", "http://arxiv.org/abs/1701.06547v4", null], ["v5", "Sun, 24 Sep 2017 01:44:39 GMT  (259kb)", "http://arxiv.org/abs/1701.06547v5", null]], "reviews": [], "SUBJECTS": "cs.CL", "authors": ["jiwei li", "will monroe", "tianlin shi", "s\u00e9bastien jean", "alan ritter", "dan jurafsky"], "accepted": true, "id": "1701.06547"}, "pdf": {"name": "1701.06547.pdf", "metadata": {"source": "CRF", "title": "Adversarial Learning for Neural Dialogue Generation", "authors": ["Jiwei Li", "Will Monroe", "Tianlin Shi", "Alan Ritter", "Dan Jurafsky"], "emails": ["jiweil@stanford.edu", "wmonroe4@stanford.edu", "tianlins@stanford.edu", "jurafsky@stanford.edu", "ritter.1492@osu.edu"], "sections": [{"heading": null, "text": "ar Xiv: 170 1,06 547v 1 [cs.C L] 23 Jan 2017In this paper, we propose to apply adversarial training for generating open-field dialogues based on the intuition of the Turing test: the system is trained to produce sequences that cannot be distinguished from human-generated dialog expressions. We consider the task to be a reinforcement learning problem, in which we train two systems together, a generative model for generating reaction sequences and a discriminator - analogous to the human evaluator in the Turing test - for distinguishing between human-generated and machine-generated dialogues. The results of the discriminator are then used as a reward for the generative model, prompting the system to generate dialogues that largely resemble human dialogues. In addition to adversarial training, we describe a model for adversarial evaluation that uses success to avoid an adversary as a case-scale evaluation system and a higher number of potential evaluation results."}, {"heading": "1 Introduction", "text": "In fact, the majority of people who are able to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move"}, {"heading": "2 Related Work", "text": "In fact, most of them will be able to move into a different world, in which they move, in which they move, in which they move, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they, in which they live, in which they live, in which they, in which they live, in which they live, in which they, in which they live, in which they live, in which they live, in which they live."}, {"heading": "3 Adversarial Training for Dialogue Generation", "text": "In this section we describe in detail the components of the proposed learning model of adversarial reinforcement. The problem can be described as follows: In the face of a dialog history x consisting of a sequence of dialog expressions, 2 the model must generate a response y = {y1, y2,..., yT}. We consider the process of sentence generation as a sequence of actions performed according to a policy defined by an encoder decoder recurrent neural networks."}, {"heading": "3.1 Adversarial REINFORCE", "text": "The opposing REINFORCE algorithm consists of two components: a generative Q model (Q) and a discriminatory model D. Generative model The generative model G defines the policy that generates an answer y given dialog story x. It takes a form similar to SEQ2SEQ models that first use the source information for a vector representation {x, y} and then calculate the probability of generating each token in the target using a softmax function. Discriminative model D is a binary classifier that uses as input a sequence of dialog statements {x, y} and outputs a label that indicates whether the input is generated by humans or machines. The input dialog is encoded in a vector representation that uses a hierarchical coder (Li et al., 2015; Serban et al., 2016b), which is then fed into a 2-class Softmax function."}, {"heading": "3.2 Reward for Every Generation Step (REGS)", "text": "In fact, it is so that most people are able to determine for themselves what they want and what they want. (...) In fact, it is so that most people are able to determine for themselves. (...) It is not so that they feel able to act. (...) It is not so that they do it. (...) It is as if they do it. (...) It is as if they do it. (...). (...) It is as if they do it. (...). \"(...).\" (.). \"(.).\" (.). \"(.\" (.). \"(.).\" (.). \"(.).\" (.). \"(.).\" (.). \"(. (.).\" (.). \"(.).\" (. \"(.).\" (.). \"(.\" (.). \"(.\" (.). \"(.).\" (. \"(.).\" (. \"(.).\" (.). \"(.\" (.). \"(.\" (.). \"(.).\" (. \"(.).\" (.). \"(.).\" (. \"(.).\" (.). \"(.\" (.). \"(.).\" (.). \"(.\" (.). \"(.).\" (.). \"(.).\" (. \"(.).).\" (. \"(.).\" (.). \"(.\" (.). \"(.).). (.).\" (. \"(.).\" (.). \"(. (.). (.).).\" (. \"(.\" (.). \"(.). (.\" (.).). (.). \"(. (.).\" (.). \"(.).\" (.). (.).). \"(.\" (.). \"(.).\" (. \"(.).\" (.). (. \"(.).).\" (.).). \"(.). (.). (."}, {"heading": "3.3 Training Details", "text": "We trained an SEQ2SEQ model (Sutskever et al., 2014) with an attention mechanism (Bahdanau et al., 2015; Luong et al., 2015) on the OpenSubtitles Dataset. We followed the protocols recommended by Sutskever et al., 2014), such as gradient clipping, mini-batch and learning rate decay. We also generated pre-train strategies the discriminator. To generate negative examples, we decode some of the training data. Half of the negative examples are generated using straam search with mutual information as described in Li et al. (2016a) and the other half is generated using sampling. For data processing, model training and decoding (both the proposed adversarial training modelsFor the number of training siterations. For i = 1, D-steps do. Example (X, Y) is generated from real data."}, {"heading": "4 Adversarial Evaluation", "text": "In this section, we will discuss details of strategies for successful hostile evaluation. It is worth noting that the proposed hostile training and hostile evaluation are separate approaches, independent of each other and do not share common parameters. The idea of hostile evaluation, first proposed by Bowman et al. (2015), is to train a discriminatory function to separate generated and true sentences in an attempt to evaluate the model's ability to generate sentences. Since it is time-consuming and costly to ask a human to speak to a model and make judgments, instead of the human evaluator, we build a machine evaluation mechanism to distinguish the machine-generated from the human-generated texts. Since it is time-consuming and costly to ask a human to speak to a model and make judgments, we build a machine evaluation mechanism to distinguish the human dialogues and machine-generated dialogues."}, {"heading": "4.1 Adversarial Success", "text": "We define Adversarial Success (short AdverSuc) as the fraction of cases in which a model is able to deceive the evaluator. AdverSuc is the difference between 1 and the accuracy achieved by the evaluator. Higher values from AdverSuc for a dialogue generation model are better."}, {"heading": "4.2 Testing the Evaluator\u2019s Ability", "text": "We approach the human evaluator in the Turing test with an automatic evaluator and assume that the evaluator is perfect: a low accuracy of the discriminator should indicate a high quality of the answers, because we interpret this in such a way that the generated answers are indistinguishable from the human ones. Unfortunately, there is another factor that can lead to a low discriminatory accuracy: a bad discrimination model. Consider a discriminator that always gives random labels or always gives the same label. Such an evaluator always provides a high AdverSuc value of 0.5. Bowman et al. (2015) propose two different discrimination models separately, using unigrammable characteristics and neural characteristics. It is difficult to say which characteristics are more reliable. The standard strategy of testing the model at a pre-development stage is not suitable, as a model that exceeds the amount of development."}, {"heading": "4.3 Machine-vs-Random Accuracy", "text": "Unfortunately, machine-generated responses are not included in the ERE metric, and the following example illustrates the serious weakness that results from this strategy: as shown in the experimental part, when input is decrypted using greedy or radial search models, most generation systems have so far had the opposite success rate of less than 10 percent (90 percent evaluation accuracy), but when sampling samples are used to decrypt, the opposite success rate soars to about 40 percent, 8 percent less than what is required for the Turing test. A closer look at the decrypted sequences using sampling tells a different story: the responses from the sample are sometimes incoherent, irrelevant, or even ungrammatical. We therefore propose an additional random test in which we indicate the accuracy of distinguishing between machine-generated answers and randomly sampled responses using sampling."}, {"heading": "5 Experimental Results", "text": "In this section we describe experimental results on the success of adversaries and human evaluation."}, {"heading": "5.1 Adversarial Evaluation", "text": "In fact, the fact is that most of us will be able to be in a position to be in, and that they will be able to be in a position to put themselves in a position in which they are able to put themselves in a position in which they are able to be in a position to be in a position to be in a position to be in a position to be in a position to be in a position in which they are able to be in a position."}, {"heading": "5.2 Human Evaluation", "text": "For human evaluation, we follow protocols defined in Li et al. (2016d), using crowdsourcing judges to evaluate a random sample of 200 points. We present both an input message and the generated results to three judges and ask them to decide which of the two results is better (general singleturn quality).Ties are permissible. Identical strings receive the same score. We also present to the judges multiturn conversations simulated between the two actors. Each conversation consists of 3 rounds. Results are shown in Table 4. We observe a significant quality improvement in both singleturn quality and multiturn quality compared to the proposed opponent's quality.It is worth noting that the amplification learning system described in Li et al. (2016d), which simulates conversations between two bots and is trained on the basis of manually designed reward functions, only improves the quality of the rotation level with multiple inscriptions, which is also confirmed in this model."}, {"heading": "6 Conclusion and Future Work", "text": "We put the model in the context of enhanced learning and train a generator based on the signal of a discriminator to generate response sequences that are indistinguishable from human-made dialogues. We observe significant performance improvements in several key indicators from the opposing training strategy. We suspect that the opposing training model should theoretically benefit a variety of generational tasks in the NLP. Unfortunately, we have not observed a clear performance boost in preliminary experiments applying the same training paradigm to machine translation. We suspect that this is due to the fact that the opposing training strategy is more advantageous for tasks where there is a large discrepancy between the distributions of the generated sequences and the reference target sequences. In other words, the opposing approach is beneficial for tasks where the entropy of the targets is high."}], "references": [{"title": "Stochastic optimization", "author": ["V.M. Aleksandrov", "V.I. Sysoyev", "V.V. Shemeneva."], "venue": "Engineering Cybernetics 5:11\u201316.", "citeRegEx": "Aleksandrov et al\\.,? 1968", "shortCiteRegEx": "Aleksandrov et al\\.", "year": 1968}, {"title": "Online sequence-to-sequence reinforcement learning for open-domain conversational agents", "author": ["Nabiha Asghar", "Pasca Poupart", "Jiang Xin", "Hang Li."], "venue": "arXiv preprint arXiv:1612.03929 .", "citeRegEx": "Asghar et al\\.,? 2016", "shortCiteRegEx": "Asghar et al\\.", "year": 2016}, {"title": "An actor-critic algorithm for sequence prediction", "author": ["Dzmitry Bahdanau", "Philemon Brakel", "Kelvin Xu", "Anirudh Goyal", "Ryan Lowe", "Joelle Pineau", "Aaron Courville", "Yoshua Bengio."], "venue": "arXiv preprint arXiv:1607.07086 .", "citeRegEx": "Bahdanau et al\\.,? 2016", "shortCiteRegEx": "Bahdanau et al\\.", "year": 2016}, {"title": "Neural machine translation by jointly learning to align and translate", "author": ["Dzmitry Bahdanau", "Kyunghyun Cho", "Yoshua Bengio."], "venue": "Proc. of ICLR.", "citeRegEx": "Bahdanau et al\\.,? 2015", "shortCiteRegEx": "Bahdanau et al\\.", "year": 2015}, {"title": "Generating sentences from a continuous space", "author": ["Samuel R Bowman", "Luke Vilnis", "Oriol Vinyals", "Andrew M Dai", "Rafal Jozefowicz", "Samy Bengio."], "venue": "arXiv preprint arXiv:1511.06349 .", "citeRegEx": "Bowman et al\\.,? 2015", "shortCiteRegEx": "Bowman et al\\.", "year": 2015}, {"title": "Infogan: Interpretable representation learning by information maximizing generative adversarial nets", "author": ["Xi Chen", "Yan Duan", "Rein Houthooft", "John Schulman", "Ilya Sutskever", "Pieter Abbeel."], "venue": "Advances In Neural Information Processing Systems.", "citeRegEx": "Chen et al\\.,? 2016a", "shortCiteRegEx": "Chen et al\\.", "year": 2016}, {"title": "Adversarial deep averaging networks for cross-lingual sentiment classification", "author": ["Xilun Chen", "Ben Athiwaratkun", "Yu Sun", "Kilian Weinberger", "Claire Cardie."], "venue": "arXiv preprint arXiv:1606.01614 .", "citeRegEx": "Chen et al\\.,? 2016b", "shortCiteRegEx": "Chen et al\\.", "year": 2016}, {"title": "Deep generative image models using a? laplacian pyramid of adversarial networks. In Advances in neural information processing systems", "author": ["Emily L Denton", "Soumith Chintala", "Rob Fergus"], "venue": null, "citeRegEx": "Denton et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Denton et al\\.", "year": 2015}, {"title": "Likelihood ratio gradient estimation for stochastic systems", "author": ["Peter W Glynn."], "venue": "Communications of the ACM 33(10):75\u201384.", "citeRegEx": "Glynn.,? 1990", "shortCiteRegEx": "Glynn.", "year": 1990}, {"title": "Generative adversarial nets", "author": ["Ian Goodfellow", "Jean Pouget-Abadie", "Mehdi Mirza", "Bing Xu", "David Warde-Farley", "Sherjil Ozair", "Aaron Courville", "Yoshua Bengio."], "venue": "Advances in Neural Information Processing Systems. pages 2672\u20132680.", "citeRegEx": "Goodfellow et al\\.,? 2014", "shortCiteRegEx": "Goodfellow et al\\.", "year": 2014}, {"title": "Long short-term memory", "author": ["Sepp Hochreiter", "J\u00fcrgen Schmidhuber."], "venue": "Neural computation 9(8):1735\u2013 1780.", "citeRegEx": "Hochreiter and Schmidhuber.,? 1997", "shortCiteRegEx": "Hochreiter and Schmidhuber.", "year": 1997}, {"title": "Learning to classify text using support vector machines: Methods, theory and algorithms", "author": ["Thorsten Joachims."], "venue": "Kluwer Academic Publishers.", "citeRegEx": "Joachims.,? 2002", "shortCiteRegEx": "Joachims.", "year": 2002}, {"title": "Adversarial evaluation of dialogue models", "author": ["Anjuli Kannan", "Oriol Vinyals."], "venue": "NIPS 2016 Workshop on Adversarial Training.", "citeRegEx": "Kannan and Vinyals.,? 2016", "shortCiteRegEx": "Kannan and Vinyals.", "year": 2016}, {"title": "Professor forcing: A new algorithm for training recurrent networks", "author": ["Alex Lamb", "Anirudh Goyal", "Ying Zhang", "Saizheng Zhang", "Aaron Courville", "Yoshua Bengio."], "venue": "Advances In Neural Information Processing Systems. pages 4601\u20134609.", "citeRegEx": "Lamb et al\\.,? 2016", "shortCiteRegEx": "Lamb et al\\.", "year": 2016}, {"title": "A diversity-promoting objective function for neural conversation models", "author": ["Jiwei Li", "Michel Galley", "Chris Brockett", "Jianfeng Gao", "Bill Dolan."], "venue": "Proc. of NAACL-HLT.", "citeRegEx": "Li et al\\.,? 2016a", "shortCiteRegEx": "Li et al\\.", "year": 2016}, {"title": "A persona-based neural conversation model", "author": ["Jiwei Li", "Michel Galley", "Chris Brockett", "Georgios Spithourakis", "Jianfeng Gao", "Bill Dolan."], "venue": "Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 1:", "citeRegEx": "Li et al\\.,? 2016b", "shortCiteRegEx": "Li et al\\.", "year": 2016}, {"title": "A hierarchical neural autoencoder for paragraphs and documents", "author": ["Jiwei Li", "Minh-Thang Luong", "Dan Jurafsky."], "venue": "arXiv preprint arXiv:1506.01057 .", "citeRegEx": "Li et al\\.,? 2015", "shortCiteRegEx": "Li et al\\.", "year": 2015}, {"title": "A simple, fast diverse decoding algorithm for neural generation", "author": ["Jiwei Li", "Will Monroe", "Dan Jurafsky."], "venue": "arXiv preprint arXiv:1611.08562 .", "citeRegEx": "Li et al\\.,? 2016c", "shortCiteRegEx": "Li et al\\.", "year": 2016}, {"title": "Deep reinforcement learning for dialogue generation", "author": ["Jiwei Li", "Will Monroe", "Alan Ritter", "Dan Jurafsky."], "venue": "arXiv preprint arXiv:1606.01541 .", "citeRegEx": "Li et al\\.,? 2016d", "shortCiteRegEx": "Li et al\\.", "year": 2016}, {"title": "How NOT to evaluate your dialogue system: An empirical study of unsupervised evaluation metrics for dialogue response generation", "author": ["Chia-Wei Liu", "Ryan Lowe", "Iulian V Serban", "Michael Noseworthy", "Laurent Charlin", "Joelle Pineau."], "venue": "arXiv preprint", "citeRegEx": "Liu et al\\.,? 2016", "shortCiteRegEx": "Liu et al\\.", "year": 2016}, {"title": "Towards an automatic turing test: Learning to evaluate dialogue responses", "author": ["Ryan Lowe", "Michael Noseworthy", "Iulian Serban", "Nicolas Angelard-Gontier", "Yoshua Bengio", "Joelle Pineau."], "venue": "arXiv preprint arXiv:1605.05414 .", "citeRegEx": "Lowe et al\\.,? 2016a", "shortCiteRegEx": "Lowe et al\\.", "year": 2016}, {"title": "On the evaluation of dialogue systems with next utterance classification", "author": ["Ryan Lowe", "Iulian V Serban", "Mike Noseworthy", "Laurent Charlin", "Joelle Pineau."], "venue": "arXiv preprint arXiv:1605.05414 .", "citeRegEx": "Lowe et al\\.,? 2016b", "shortCiteRegEx": "Lowe et al\\.", "year": 2016}, {"title": "LSTM based conversation models", "author": ["Yi Luan", "Yangfeng Ji", "Mari Ostendorf."], "venue": "arXiv preprint arXiv:1603.09457 .", "citeRegEx": "Luan et al\\.,? 2016", "shortCiteRegEx": "Luan et al\\.", "year": 2016}, {"title": "Effective approaches to attentionbased neural machine translation", "author": ["Minh-Thang Luong", "Hieu Pham", "Christopher D Manning."], "venue": "arXiv preprint arXiv:1508.04025 .", "citeRegEx": "Luong et al\\.,? 2015", "shortCiteRegEx": "Luong et al\\.", "year": 2015}, {"title": "Unsupervised representation learning with deep convolutional generative adversarial networks", "author": ["Alec Radford", "Luke Metz", "Soumith Chintala."], "venue": "arXiv preprint arXiv:1511.06434 .", "citeRegEx": "Radford et al\\.,? 2015", "shortCiteRegEx": "Radford et al\\.", "year": 2015}, {"title": "Sequence level training with recurrent neural networks", "author": ["Marc\u2019Aurelio Ranzato", "Sumit Chopra", "Michael Auli", "Wojciech Zaremba"], "venue": null, "citeRegEx": "Ranzato et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Ranzato et al\\.", "year": 2015}, {"title": "Data-driven response generation in social media", "author": ["Alan Ritter", "Colin Cherry", "William B Dolan."], "venue": "Proceedings of EMNLP 2011. pages 583\u2013593.", "citeRegEx": "Ritter et al\\.,? 2011", "shortCiteRegEx": "Ritter et al\\.", "year": 2011}, {"title": "Improved techniques for training gans", "author": ["Tim Salimans", "Ian Goodfellow", "Wojciech Zaremba", "Vicki Cheung", "Alec Radford", "Xi Chen."], "venue": "Advances in Neural Information Processing Systems. pages 2226\u20132234.", "citeRegEx": "Salimans et al\\.,? 2016", "shortCiteRegEx": "Salimans et al\\.", "year": 2016}, {"title": "Building end-to-end dialogue systems using generative hierarchical neural network models", "author": ["Iulian V Serban", "Alessandro Sordoni", "Yoshua Bengio", "Aaron Courville", "Joelle Pineau."], "venue": "Proceedings of AAAI.", "citeRegEx": "Serban et al\\.,? 2016a", "shortCiteRegEx": "Serban et al\\.", "year": 2016}, {"title": "Building end-to-end dialogue systems using generative hierarchical neural network models", "author": ["Iulian V Serban", "Alessandro Sordoni", "Yoshua Bengio", "Aaron Courville", "Joelle Pineau."], "venue": "Proceedings of the 30th AAAI Conference on Artificial Intelligence", "citeRegEx": "Serban et al\\.,? 2016b", "shortCiteRegEx": "Serban et al\\.", "year": 2016}, {"title": "Multiresolution recurrent neural networks: An application to dialogue response generation", "author": ["Iulian Vlad Serban", "Tim Klinger", "Gerald Tesauro", "Kartik Talamadupula", "Bowen Zhou", "Yoshua Bengio", "Aaron Courville."], "venue": "arXiv preprint", "citeRegEx": "Serban et al\\.,? 2016c", "shortCiteRegEx": "Serban et al\\.", "year": 2016}, {"title": "Generative deep neural networks for dialogue: A short review", "author": ["Iulian Vlad Serban", "Ryan Lowe", "Laurent Charlin", "Joelle Pineau"], "venue": null, "citeRegEx": "Serban et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Serban et al\\.", "year": 2016}, {"title": "A hierarchical latent variable encoder-decoder model for generating dialogues", "author": ["Iulian Vlad Serban", "Alessandro Sordoni", "Ryan Lowe", "Laurent Charlin", "Joelle Pineau", "Aaron Courville", "Yoshua Bengio."], "venue": "arXiv preprint arXiv:1605.06069 .", "citeRegEx": "Serban et al\\.,? 2016e", "shortCiteRegEx": "Serban et al\\.", "year": 2016}, {"title": "Neural responding machine for short-text conversation", "author": ["Lifeng Shang", "Zhengdong Lu", "Hang Li."], "venue": "Proceedings of ACL-IJCNLP. pages 1577\u20131586.", "citeRegEx": "Shang et al\\.,? 2015", "shortCiteRegEx": "Shang et al\\.", "year": 2015}, {"title": "Generating long and diverse responses with neural conversational models", "author": ["Louis Shao", "Stephan Gouws", "Denny Britz", "Anna Goldie", "Brian Strope", "Ray Kurzweil"], "venue": null, "citeRegEx": "Shao et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Shao et al\\.", "year": 2016}, {"title": "Minimum risk training for neural machine translation", "author": ["Shiqi Shen", "Yong Cheng", "Zhongjun He", "Wei He", "Hua Wu", "Maosong Sun", "Yang Liu."], "venue": "arXiv preprint arXiv:1512.02433 .", "citeRegEx": "Shen et al\\.,? 2015", "shortCiteRegEx": "Shen et al\\.", "year": 2015}, {"title": "Mastering the game of Go with deep neural networks and tree", "author": ["David Silver", "Aja Huang", "Chris J Maddison", "Arthur Guez", "Laurent Sifre", "George Van Den Driessche", "Julian Schrittwieser", "Ioannis Antonoglou", "Veda Panneershelvam", "Marc Lanctot"], "venue": null, "citeRegEx": "Silver et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Silver et al\\.", "year": 2016}, {"title": "A neural network approach to context-sensitive generation of conversational responses", "author": ["Alessandro Sordoni", "Michel Galley", "Michael Auli", "Chris Brockett", "Yangfeng Ji", "Meg Mitchell", "JianYun Nie", "Jianfeng Gao", "Bill Dolan."], "venue": "Proceedings of", "citeRegEx": "Sordoni et al\\.,? 2015", "shortCiteRegEx": "Sordoni et al\\.", "year": 2015}, {"title": "Continuously learning neural dialogue management", "author": ["Pei-Hao Su", "Milica Gasic", "Nikola Mrksic", "Lina RojasBarahona", "Stefan Ultes", "David Vandyke", "TsungHsien Wen", "Steve Young."], "venue": "arxiv .", "citeRegEx": "Su et al\\.,? 2016", "shortCiteRegEx": "Su et al\\.", "year": 2016}, {"title": "Sequence to sequence learning with neural networks", "author": ["Ilya Sutskever", "Oriol Vinyals", "Quoc V Le."], "venue": "Advances in neural information processing systems. pages 3104\u20133112.", "citeRegEx": "Sutskever et al\\.,? 2014", "shortCiteRegEx": "Sutskever et al\\.", "year": 2014}, {"title": "Computing machinery and intelligence", "author": ["Alan M Turing."], "venue": "Mind 59(236):433\u2013460.", "citeRegEx": "Turing.,? 1950", "shortCiteRegEx": "Turing.", "year": 1950}, {"title": "A neural conversational model", "author": ["Oriol Vinyals", "Quoc Le."], "venue": "Proceedings of ICML Deep Learning Workshop.", "citeRegEx": "Vinyals and Le.,? 2015", "shortCiteRegEx": "Vinyals and Le.", "year": 2015}, {"title": "A networkbased end-to-end trainable task-oriented dialogue system", "author": ["Tsung-Hsien Wen", "Milica Gasic", "Nikola Mrksic", "Lina M Rojas-Barahona", "Pei-Hao Su", "Stefan Ultes", "David Vandyke", "Steve Young."], "venue": "arXiv preprint arXiv:1604.04562 .", "citeRegEx": "Wen et al\\.,? 2016", "shortCiteRegEx": "Wen et al\\.", "year": 2016}, {"title": "Simple statistical gradientfollowing algorithms for connectionist reinforcement learning", "author": ["Ronald J Williams."], "venue": "Machine learning 8(3-4):229\u2013256.", "citeRegEx": "Williams.,? 1992", "shortCiteRegEx": "Williams.", "year": 1992}, {"title": "Sequence-to-sequence learning as beam-search optimization", "author": ["Sam Wiseman", "Alexander M Rush."], "venue": "arXiv preprint arXiv:1606.02960 .", "citeRegEx": "Wiseman and Rush.,? 2016", "shortCiteRegEx": "Wiseman and Rush.", "year": 2016}, {"title": "Incorporating loose-structured knowledge into LSTM with recall gate for conversation modeling", "author": ["Zhen Xu", "Bingquan Liu", "Baoxun Wang", "Chengjie Sun", "Xiaolong Wang."], "venue": "arXiv preprint arXiv:1605.05110 .", "citeRegEx": "Xu et al\\.,? 2016", "shortCiteRegEx": "Xu et al\\.", "year": 2016}, {"title": "Attention with intention for a neural network conversation model", "author": ["Kaisheng Yao", "Geoffrey Zweig", "Baolin Peng."], "venue": "NIPS workshop on Machine Learning for Spoken Language Understanding and Interaction.", "citeRegEx": "Yao et al\\.,? 2015", "shortCiteRegEx": "Yao et al\\.", "year": 2015}, {"title": "Seqgan: sequence generative adversarial nets with policy gradient", "author": ["Lantao Yu", "Weinan Zhang", "Jun Wang", "Yong Yu."], "venue": "arXiv preprint arXiv:1609.05473 .", "citeRegEx": "Yu et al\\.,? 2016a", "shortCiteRegEx": "Yu et al\\.", "year": 2016}, {"title": "Strategy and policy learning for nontask-oriented conversational systems", "author": ["Zhou Yu", "Ziyu Xu", "Alan W Black", "Alex I Rudnicky."], "venue": "17th Annual Meeting of the Special Interest Group on Discourse and Dialogue. page 404.", "citeRegEx": "Yu et al\\.,? 2016b", "shortCiteRegEx": "Yu et al\\.", "year": 2016}, {"title": "Aspect-augmented adversarial networks for domain adaptation", "author": ["Yuan Zhang", "Regina Barzilay", "Tommi Jaakkola."], "venue": "arXiv preprint arXiv:1701.00188 .", "citeRegEx": "Zhang et al\\.,? 2017", "shortCiteRegEx": "Zhang et al\\.", "year": 2017}], "referenceMentions": [{"referenceID": 26, "context": ", phrase-based machine translation systems (Ritter et al., 2011; Sordoni et al., 2015) or end-to-end neural systems (Shang et al.", "startOffset": 43, "endOffset": 86}, {"referenceID": 37, "context": ", phrase-based machine translation systems (Ritter et al., 2011; Sordoni et al., 2015) or end-to-end neural systems (Shang et al.", "startOffset": 43, "endOffset": 86}, {"referenceID": 33, "context": ", 2015) or end-to-end neural systems (Shang et al., 2015; Vinyals and Le, 2015; Li et al., 2016a; Yao et al., 2015; Luan et al., 2016) approximate such a goal by predicting the next dialogue utterance given the dialogue history using the maximum likelihood estimation (MLE) objective function.", "startOffset": 37, "endOffset": 134}, {"referenceID": 41, "context": ", 2015) or end-to-end neural systems (Shang et al., 2015; Vinyals and Le, 2015; Li et al., 2016a; Yao et al., 2015; Luan et al., 2016) approximate such a goal by predicting the next dialogue utterance given the dialogue history using the maximum likelihood estimation (MLE) objective function.", "startOffset": 37, "endOffset": 134}, {"referenceID": 14, "context": ", 2015) or end-to-end neural systems (Shang et al., 2015; Vinyals and Le, 2015; Li et al., 2016a; Yao et al., 2015; Luan et al., 2016) approximate such a goal by predicting the next dialogue utterance given the dialogue history using the maximum likelihood estimation (MLE) objective function.", "startOffset": 37, "endOffset": 134}, {"referenceID": 46, "context": ", 2015) or end-to-end neural systems (Shang et al., 2015; Vinyals and Le, 2015; Li et al., 2016a; Yao et al., 2015; Luan et al., 2016) approximate such a goal by predicting the next dialogue utterance given the dialogue history using the maximum likelihood estimation (MLE) objective function.", "startOffset": 37, "endOffset": 134}, {"referenceID": 22, "context": ", 2015) or end-to-end neural systems (Shang et al., 2015; Vinyals and Le, 2015; Li et al., 2016a; Yao et al., 2015; Luan et al., 2016) approximate such a goal by predicting the next dialogue utterance given the dialogue history using the maximum likelihood estimation (MLE) objective function.", "startOffset": 37, "endOffset": 134}, {"referenceID": 37, "context": "Despite its success, many issues emerge resulting from this over-simplified training objective: responses are highly dull and generic (Sordoni et al., 2015; Serban et al., 2016a; Li et al., 2016a), repetitive and short-sighted (Li et al.", "startOffset": 134, "endOffset": 196}, {"referenceID": 28, "context": "Despite its success, many issues emerge resulting from this over-simplified training objective: responses are highly dull and generic (Sordoni et al., 2015; Serban et al., 2016a; Li et al., 2016a), repetitive and short-sighted (Li et al.", "startOffset": 134, "endOffset": 196}, {"referenceID": 14, "context": "Despite its success, many issues emerge resulting from this over-simplified training objective: responses are highly dull and generic (Sordoni et al., 2015; Serban et al., 2016a; Li et al., 2016a), repetitive and short-sighted (Li et al.", "startOffset": 134, "endOffset": 196}, {"referenceID": 18, "context": ", 2016a), repetitive and short-sighted (Li et al., 2016d).", "startOffset": 39, "endOffset": 57}, {"referenceID": 14, "context": "Solutions to these problems require answering a few fundamental questions: what are the crucial aspects that define an ideal conversation, how can we quantitatively measure them, and how can we incorporate them into a machine learning system? For example, Li et al. (2016d) manually define three types of ideal dialogue properties such as ease of answering, informativeness and coherence, and use a reinforcement-learning framework to train the model to generate highly rewarded responses.", "startOffset": 256, "endOffset": 274}, {"referenceID": 14, "context": "Solutions to these problems require answering a few fundamental questions: what are the crucial aspects that define an ideal conversation, how can we quantitatively measure them, and how can we incorporate them into a machine learning system? For example, Li et al. (2016d) manually define three types of ideal dialogue properties such as ease of answering, informativeness and coherence, and use a reinforcement-learning framework to train the model to generate highly rewarded responses. Yu et al. (2016b) use keyword retrieval confidence as a reward.", "startOffset": 256, "endOffset": 508}, {"referenceID": 40, "context": "Such a goal suggests a training objective resembling the idea of the Turing test (Turing, 1950).", "startOffset": 81, "endOffset": 95}, {"referenceID": 9, "context": "We borrow the idea of adversarial training (Goodfellow et al., 2014; Denton et al., 2015) in computer vision, in which we jointly train two models, a generator (which takes the form of the neural SEQ2SEQ model) that defines the probability of generating a dialogue sequence, and", "startOffset": 43, "endOffset": 89}, {"referenceID": 7, "context": "We borrow the idea of adversarial training (Goodfellow et al., 2014; Denton et al., 2015) in computer vision, in which we jointly train two models, a generator (which takes the form of the neural SEQ2SEQ model) that defines the probability of generating a dialogue sequence, and", "startOffset": 43, "endOffset": 89}, {"referenceID": 4, "context": "Adversarial evaluation was first employed in Bowman et al. (2015) to evaluate sentence generation quality, and preliminarily studied in the context of dialogue generation by Kannan and Vinyals (2016).", "startOffset": 45, "endOffset": 66}, {"referenceID": 4, "context": "Adversarial evaluation was first employed in Bowman et al. (2015) to evaluate sentence generation quality, and preliminarily studied in the context of dialogue generation by Kannan and Vinyals (2016). In this paper, we discuss potential pitfalls of adversarial evaluations and necessary steps to avoid them and make evaluation reliable.", "startOffset": 45, "endOffset": 200}, {"referenceID": 41, "context": "Recent progress in SEQ2SEQ models have inspired several efforts (Vinyals and Le, 2015; Serban et al., 2016a,d; Luan et al., 2016) to build end-to-end conversational systems which first apply an encoder to map a message to a distributed vector representing its semantics and then generate a response from the message vector.", "startOffset": 64, "endOffset": 129}, {"referenceID": 22, "context": "Recent progress in SEQ2SEQ models have inspired several efforts (Vinyals and Le, 2015; Serban et al., 2016a,d; Luan et al., 2016) to build end-to-end conversational systems which first apply an encoder to map a message to a distributed vector representing its semantics and then generate a response from the message vector.", "startOffset": 64, "endOffset": 129}, {"referenceID": 25, "context": "Ritter et al. (2011) frame the response generation problem as a statistical machine translation (SMT) problem.", "startOffset": 0, "endOffset": 21}, {"referenceID": 25, "context": "Ritter et al. (2011) frame the response generation problem as a statistical machine translation (SMT) problem. Sordoni et al. (2015) improved Ritter et al.", "startOffset": 0, "endOffset": 133}, {"referenceID": 42, "context": "Our work is also related to recent efforts to integrate the SEQ2SEQ and reinforcement learning paradigms, drawing on the advantages of both (Wen et al., 2016).", "startOffset": 140, "endOffset": 158}, {"referenceID": 13, "context": "Our work adapts the encoder-decoder model to RL training, and can thus be viewed as an extension of Li et al. (2016d), but with more general RL rewards.", "startOffset": 100, "endOffset": 118}, {"referenceID": 13, "context": "Our work adapts the encoder-decoder model to RL training, and can thus be viewed as an extension of Li et al. (2016d), but with more general RL rewards. Li et al. (2016d) simulate dialogues between two virtual agents, using policy gradient methods to reward sequences that display three useful conversational properties: informativity, coherence, and ease of answering.", "startOffset": 100, "endOffset": 171}, {"referenceID": 13, "context": "Our work adapts the encoder-decoder model to RL training, and can thus be viewed as an extension of Li et al. (2016d), but with more general RL rewards. Li et al. (2016d) simulate dialogues between two virtual agents, using policy gradient methods to reward sequences that display three useful conversational properties: informativity, coherence, and ease of answering. Our work is also related to recent efforts to integrate the SEQ2SEQ and reinforcement learning paradigms, drawing on the advantages of both (Wen et al., 2016). For example, Su et al. (2016) combine reinforcement learning with neural generation on tasks with real users.", "startOffset": 100, "endOffset": 560}, {"referenceID": 1, "context": "Recent work from Asghar et al. (2016) trains an end-to-end RL dialogue model using human users.", "startOffset": 17, "endOffset": 38}, {"referenceID": 19, "context": "Some recent work (Liu et al., 2016) has started to look at more flexible and reliable evaluation metrics such as humanrating prediction (Lowe et al.", "startOffset": 17, "endOffset": 35}, {"referenceID": 20, "context": ", 2016) has started to look at more flexible and reliable evaluation metrics such as humanrating prediction (Lowe et al., 2016a) and next utterance classification (Lowe et al.", "startOffset": 108, "endOffset": 128}, {"referenceID": 21, "context": ", 2016a) and next utterance classification (Lowe et al., 2016b).", "startOffset": 43, "endOffset": 63}, {"referenceID": 24, "context": "Adversarial networks The idea of generative adversarial networks has enjoyed great success in computer vision (Radford et al., 2015; Chen et al., 2016a; Salimans et al., 2016).", "startOffset": 110, "endOffset": 175}, {"referenceID": 5, "context": "Adversarial networks The idea of generative adversarial networks has enjoyed great success in computer vision (Radford et al., 2015; Chen et al., 2016a; Salimans et al., 2016).", "startOffset": 110, "endOffset": 175}, {"referenceID": 27, "context": "Adversarial networks The idea of generative adversarial networks has enjoyed great success in computer vision (Radford et al., 2015; Chen et al., 2016a; Salimans et al., 2016).", "startOffset": 110, "endOffset": 175}, {"referenceID": 11, "context": "Some recent work has begun to address this issue: Lamb et al. (2016) propose providing the discriminator with the intermediate hidden vectors of the generator rather than its sequence outputs.", "startOffset": 50, "endOffset": 69}, {"referenceID": 11, "context": "Some recent work has begun to address this issue: Lamb et al. (2016) propose providing the discriminator with the intermediate hidden vectors of the generator rather than its sequence outputs. Such a strategy makes the system differentiable and achieves promising results in tasks like character-level language modeling and handwriting generation. Yu et al. (2016a) use policy gradient reinforcement learning to backpropagate the error from the discriminator, showing improvement in multiple generation tasks such as poem generation, speech language generation and music generation.", "startOffset": 50, "endOffset": 366}, {"referenceID": 5, "context": "Outside of sequence generation, Chen et al. (2016b) apply the idea of adversarial training to sentiment analysis and Zhang et al.", "startOffset": 32, "endOffset": 52}, {"referenceID": 5, "context": "Outside of sequence generation, Chen et al. (2016b) apply the idea of adversarial training to sentiment analysis and Zhang et al. (2017) apply the idea to domain adaptation tasks.", "startOffset": 32, "endOffset": 137}, {"referenceID": 25, "context": "Ranzato et al. (2015) train RNN decoders in a SEQ2SEQ model using policy gradient, and they obtain competitive machine translation results.", "startOffset": 0, "endOffset": 22}, {"referenceID": 35, "context": "Also related is recent work (Shen et al., 2015; Wiseman and Rush, 2016) to address the issues of exposure bias and lossevaluation mismatch in neural machine translation.", "startOffset": 28, "endOffset": 71}, {"referenceID": 44, "context": "Also related is recent work (Shen et al., 2015; Wiseman and Rush, 2016) to address the issues of exposure bias and lossevaluation mismatch in neural machine translation.", "startOffset": 28, "endOffset": 71}, {"referenceID": 16, "context": "The input dialogue is encoded into a vector representation using a hierarchical encoder (Li et al., 2015; Serban et al., 2016b),3 which is then fed to a 2-class softmax function, returning the probability of the input dialogue episode being a machinegenerated dialogue (denoted Q\u2212({x, y})) or a human-generated dialogue (denoted Q+({x, y})).", "startOffset": 88, "endOffset": 127}, {"referenceID": 29, "context": "The input dialogue is encoded into a vector representation using a hierarchical encoder (Li et al., 2015; Serban et al., 2016b),3 which is then fed to a 2-class softmax function, returning the probability of the input dialogue episode being a machinegenerated dialogue (denoted Q\u2212({x, y})) or a human-generated dialogue (denoted Q+({x, y})).", "startOffset": 88, "endOffset": 127}, {"referenceID": 10, "context": "To be specific, each utterance p or q is mapped to a vector representation hp or hq using LSTM (Hochreiter and Schmidhuber, 1997).", "startOffset": 95, "endOffset": 129}, {"referenceID": 43, "context": ", Q+({x, y})) is used as a reward for the generator, which is trained to maximize the expected reward of generated utterance(s) using the REINFORCE algorithm (Williams, 1992):", "startOffset": 158, "endOffset": 174}, {"referenceID": 43, "context": "The gradient of (1) is approximated using the likelihood ratio trick (Williams, 1992; Glynn, 1990; Aleksandrov et al., 1968):", "startOffset": 69, "endOffset": 124}, {"referenceID": 8, "context": "The gradient of (1) is approximated using the likelihood ratio trick (Williams, 1992; Glynn, 1990; Aleksandrov et al., 1968):", "startOffset": 69, "endOffset": 124}, {"referenceID": 0, "context": "The gradient of (1) is approximated using the likelihood ratio trick (Williams, 1992; Glynn, 1990; Aleksandrov et al., 1968):", "startOffset": 69, "endOffset": 124}, {"referenceID": 25, "context": "4 Like Ranzato et al. (2015), we train another neural network model (the critic) to estimate the value (or future reward) of current state (i.", "startOffset": 7, "endOffset": 29}, {"referenceID": 36, "context": "To mitigate this problem, we adopt a strategy similar to when training value networks in AlphaGo (Silver et al., 2016), in which for each collection of subsequences of Y , we randomly", "startOffset": 97, "endOffset": 118}, {"referenceID": 46, "context": "A similar strategy is adopted in Yu et al. (2016a). The downside of MC is that it requires repeating the sampling process for each prefix of each sequence and is thus significantly time-consuming.", "startOffset": 33, "endOffset": 51}, {"referenceID": 13, "context": "This can be seen as having a teacher intervene with the generator some fraction of the time and force it to generate the true responses, an approach that is similar to the professor-forcing algorithm of Lamb et al. (2016).", "startOffset": 203, "endOffset": 222}, {"referenceID": 39, "context": "We trained a SEQ2SEQ model (Sutskever et al., 2014) with an attention mechanism (Bahdanau et al.", "startOffset": 27, "endOffset": 51}, {"referenceID": 3, "context": ", 2014) with an attention mechanism (Bahdanau et al., 2015; Luong et al., 2015) on the OpenSubtitles dataset.", "startOffset": 36, "endOffset": 79}, {"referenceID": 23, "context": ", 2014) with an attention mechanism (Bahdanau et al., 2015; Luong et al., 2015) on the OpenSubtitles dataset.", "startOffset": 36, "endOffset": 79}, {"referenceID": 2, "context": ", 2014) with an attention mechanism (Bahdanau et al., 2015; Luong et al., 2015) on the OpenSubtitles dataset. We followed protocols recommended by Sutskever et al. (2014), such as gradient clipping, mini-batch and learning rate decay.", "startOffset": 37, "endOffset": 171}, {"referenceID": 14, "context": "Half of the negative examples are generated using beam-search with mutual information reranking as described in Li et al. (2016a), and the other half is generated from sampling.", "startOffset": 112, "endOffset": 130}, {"referenceID": 14, "context": "7 (3) Penalizing intra-sibling ranking when doing beam search decoding to promote N-best list diversity as described in Li et al. (2016c). (4) Penalizing word types (stop words excluded) that have already been generated.", "startOffset": 120, "endOffset": 138}, {"referenceID": 4, "context": "The idea of adversarial evaluation, first proposed by Bowman et al. (2015), is to train a discriminant function to separate generated and true sentences, in an attempt to evaluate the model\u2019s sentence generation capability.", "startOffset": 54, "endOffset": 75}, {"referenceID": 4, "context": "The idea of adversarial evaluation, first proposed by Bowman et al. (2015), is to train a discriminant function to separate generated and true sentences, in an attempt to evaluate the model\u2019s sentence generation capability. The idea has been preliminarily studied by Kannan and Vinyals (2016) in the context of dialogue generation.", "startOffset": 54, "endOffset": 293}, {"referenceID": 4, "context": "Bowman et al. (2015) propose two different discriminator models separately using unigram features and neural features.", "startOffset": 0, "endOffset": 21}, {"referenceID": 34, "context": "This resembles the N-choose-1 metric described in Shao et al. (2016). Higher accuracy indicates that the generated responses are distinguishable from randomly sampled human responses, indicating that the generative model is not fooling the generator simply by introducing randomness.", "startOffset": 50, "endOffset": 69}, {"referenceID": 12, "context": "Similar results are also reported in Kannan and Vinyals (2016). 5 Experimental Results", "startOffset": 37, "endOffset": 63}, {"referenceID": 11, "context": "Trained using the SVM-Light package (Joachims, 2002).", "startOffset": 36, "endOffset": 52}, {"referenceID": 14, "context": "Baselines we consider include standard SEQ2SEQ models using greedy decoding (MLEgreedy), beam-search (MLE+BS) and sampling, as well as the mutual information reranking model of Li et al. (2016a) with two algorithmic variations: (1) MMI+p(t|s), in which a large N-best list is first generated using a pre-trained SEQ2SEQ model and then reranked by the backward probability p(s|t) and (2) MMI\u2212p(t), in which language model probability is penalized during decoding.", "startOffset": 177, "endOffset": 195}, {"referenceID": 14, "context": "This result is in line with human-evaluation results from Li et al. (2016a). The two proposed adversarial algorithms", "startOffset": 58, "endOffset": 76}, {"referenceID": 14, "context": "For human evaluation, we follow protocols defined in Li et al. (2016d), employing crowdsourced judges to evaluate a random sample of 200 items.", "startOffset": 53, "endOffset": 71}, {"referenceID": 14, "context": "For human evaluation, we follow protocols defined in Li et al. (2016d), employing crowdsourced judges to evaluate a random sample of 200 items. We present both an input message and the generated outputs to 3 judges and ask them to decide which of the two outputs is better (singleturn general quality). Ties are permitted. Identical strings are assigned the same score. We also present the judges with multi-turn conversations simulated between the two agents. Each conversation consists of 3 turns. Results are presented in Table 4. We observe a significant quality improvement on both single-turn quality and multiturn quality from the proposed adversarial model. It is worth noting that the reinforcement learning system described in Li et al. (2016d), which simulates conversations between two bots and is trained based on manually designed reward functions, only improves multi-turn dialogue quality, while the model described in this paper improves both single-turn and multi-turn dialogue genera-", "startOffset": 53, "endOffset": 755}], "year": 2017, "abstractText": "In this paper, drawing intuition from the Turing test, we propose using adversarial training for open-domain dialogue generation: the system is trained to produce sequences that are indistinguishable from human-generated dialogue utterances. We cast the task as a reinforcement learning (RL) problem where we jointly train two systems, a generative model to produce response sequences, and a discriminator\u2014analagous to the human evaluator in the Turing test\u2014 to distinguish between the human-generated dialogues and the machine-generated ones. The outputs from the discriminator are then used as rewards for the generative model, pushing the system to generate dialogues that mostly resemble human dialogues. In addition to adversarial training we describe a model for adversarial evaluation that uses success in fooling an adversary as a dialogue evaluation metric, while avoiding a number of potential pitfalls. Experimental results on several metrics, including adversarial evaluation, demonstrate that the adversarially-trained system generates higher-quality responses than previous baselines. 1", "creator": "LaTeX with hyperref package"}}}