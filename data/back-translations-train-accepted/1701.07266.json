{"id": "1701.07266", "review": {"conference": "NIPS", "VERSION": "v1", "DATE_OF_SUBMISSION": "25-Jan-2017", "title": "k*-Nearest Neighbors: From Global to Local", "abstract": "The weighted k-nearest neighbors algorithm is one of the most fundamental non-parametric methods in pattern recognition and machine learning. The question of setting the optimal number of neighbors as well as the optimal weights has received much attention throughout the years, nevertheless this problem seems to have remained unsettled. In this paper we offer a simple approach to locally weighted regression/classification, where we make the bias-variance tradeoff explicit. Our formulation enables us to phrase a notion of optimal weights, and to efficiently find these weights as well as the optimal number of neighbors efficiently and adaptively, for each data point whose value we wish to estimate. The applicability of our approach is demonstrated on several datasets, showing superior performance over standard locally weighted methods.", "histories": [["v1", "Wed, 25 Jan 2017 11:18:18 GMT  (38kb,D)", "http://arxiv.org/abs/1701.07266v1", null]], "reviews": [], "SUBJECTS": "stat.ML cs.LG", "authors": ["oren anava", "kfir y levy"], "accepted": true, "id": "1701.07266"}, "pdf": {"name": "1701.07266.pdf", "metadata": {"source": "CRF", "title": "k\u2217-Nearest Neighbors: From Global to Local", "authors": ["Oren Anava", "Kfir Y. Levy"], "emails": ["oren@voleon.com.", "yehuda.levy@inf.ethz.ch."], "sections": [{"heading": "1 Introduction", "text": "The k-nearest neighbors (k-NN) algorithm Cover and Hart (1967); Fix and Hodges Jr. (1951) and Nadarays-Watson estimation Nadaraya (1964); Watson (1964) are the cornerstones of non-parametric learning. Due to their simplicity and flexibility, these methods have become the methods of choice in many scenarios Wu et al. (2008), especially in situations where the underlying model is complex. Modern applications of the k-NN algorithm include recommendation systems Adeniyi et al. (2016), text categorization Trstenjak et al. (2014), heart disease classification Deekshatulu et al. (2013), and financial forecasting Imandoust and Bolandraftar (2013), among others. The Voleon Group. Email: oren @ voleon.com."}, {"heading": "1.1 Related Work", "text": "However, this powerful guarantee implies that, since the number of samples n goes infinite, and also k \u2192 \u221e, k / n \u2192 0, then the risk of the k-NN rule approximates the risk of the Bayes classifier for each underlying data distribution. Similar guarantees apply to weighted k-NN rules, with the additional assumptions that the risk of the k-NN rule converges approximately with the risk of the Bayes classification (1977); Devroye et al. (2013) In the regime of practical interest, where the number of samples n is limited, the use of k = b \u221a nc neighbors is a widely mentioned rule of thumb Devroye et al. (2013) Nevertheless, this rule often leads to poor results, and in the regime of finite samples it is usually recommended to choose k using cross-validation. Similar consistency results apply to local methods Devroye et al (Samrofi 1980)."}, {"heading": "2 Problem Definition", "text": "In this section, we present our attitudes and assumptions and formulate the locally weighted optimal estimation problem. We try to find the best local prediction in a sense that minimizes the distance between this prediction and the basic truth."}, {"heading": "3 Algorithm and Analysis", "text": "In this section we discuss the properties of the optimal solution for (P2), and present a greedy algorithm designed to efficiently find the exact solution to the latter goal (see Section 3.1). Faced with a decision point x0, Theorem 3.1 shows that the optimal weight of the data point xi is proportional to \u2212 d (xi, x0) (closer points are given more weight). Interestingly, this weight drop is quite slow compared to popular weight nuclei that use Sharper decay plans, e.g. exponential / inverse proportional. Theorem 3.1 also implies a cutoff effect, meaning that there is k. [n] So that only the k."}, {"heading": "3.2 Special Cases", "text": "The aim of this section is to discuss two specific cases in which the limit of our algorithm coincides with familiar limits in the literature, justifying the relaxed goal of (P2). We present here only a high-level description of both cases and move the formal details to the full version of the paper. The solution of (P2) is most likely above the limit of the true prediction error. In the first case, our algorithm includes all designations in the calculation, giving a confidence limit of 2C\u03bb = 2b \u221a (2 / n) log (2 / \u043c) log for the prediction error (with the probability of 1 \u2212 \u043c > 0). In the second case, our algorithm includes all designations in the calculation, resulting in a confidence limit of 2 / n."}, {"heading": "4 Experimental Results", "text": "The following experiments show the effectiveness of the proposed algorithm on several datasets. We start with the representation of the baselines used for the comparison."}, {"heading": "4.1 Baselines", "text": "The standard k-NN: Given k, the standard k-NN finds the k nearest data to x0 (without loss of generality assuming that these data points are x1,.., xk) and then estimates f (x0) = 1 k-k i = 1 yi.The Nadaraya-Watson estimator: This estimator assigns the data points with weights proportional to a certain similarity core K: Rd \u00b7 Rd 7 \u2192 R +. That is, f (x0) = n i = 1K (xi, x0) yi n = 1K (xi, x0).Popular choices of core functions include the Gaussian core K (xi, xj) = 1 x-xi \u2212 xj \u00b2 22\u04452; Epanechnikov kernel K (xi, xj) = 3 4 (1 \u2212 xi \u2212 xj \u00b2) 1 {xi \u2212 xj \u00b2 except}; and the triangular core that is absent above the presented kernel (xi, xi)."}, {"heading": "4.2 Datasets", "text": "In our experiments, we use 8 real data sets, all of which are available on the UCI repository website (https: / / archive.ics.uci.edu / ml /). In each of the data sets, the characteristic vector consists only of real values, while the labels take different forms: in the first 6 data sets (QSAR, Diabetes, PopFailures, Sonar, Ionosphere and Fertility), the labels are binary, and in the last two data sets (Slump and Yacht), the labels are real. Note that our algorithm (as well as the other two baselines) is applicable to all data sets without the need for adjustments. The number of samples n and the dimension of each sample d are given in Table 1 for each data set."}, {"heading": "4.3 Experimental Setup", "text": "We randomly divide each data set into two halves (one for validation and the other for testing) and in the first half (the validation set) we perform the two baselines and our algorithm with different values of k, \u03c3 and L / C (respectively), using 5-fold cross validation. Specifically, we consider values of k in {1, 2,.., 10} and values of \u03c3 and L / C in {0.001, 0.005, 0.01, 0.05, 0.1, 0.5, 1, 5, 10}. The best values of k, \u03c3 and L / C are then used in the second half of the data set (the test set) to obtain the results shown in Table 1. Our algorithm also specifies the range of k corresponding to the selection of L / C. Note that here we present the average absolute error of our prediction as a result of our theoretical guarantees."}, {"heading": "4.4 Results and Discussion", "text": "As Table 1 shows, our algorithm outperforms baselines in 7 (of 8) datasets, with significant outperformance in 3 datasets. It can also be seen that the standard k-NN is limited to selecting a value of k per dataset, but our algorithm takes full advantage of the ability to select k adaptively per dataset point, confirming our theoretical results and emphasizing the advantage of adaptive selection of k."}, {"heading": "5 Conclusions and Future Directions", "text": "We have introduced a principled approach to locally weighted optimal estimation. By explicitly formulating the trade-off between bias and variance, we have defined the notion of optimal weights and optimal number of neighbors per decision point, and thus developed an efficient method to extract them. Note that our approach could be extended to multi-class classification, as well as scenarios in which predictions of different data points correlate (and we have an estimate of their correlations).Due to the lack of space, we leave these extensions to the full version of the paper. A lack of current non-parametric methods, including our k-NN algorithm, is their limited geometric perspective. Specifically, all of these methods only take into account the distances between decision point and data points, i.e. {d (x0, xi)} ni = 1, and ignore the geometric relationship between data points, i.e. we believe that this point has an additional advantage over our local geometric information."}, {"heading": "A Hoeffding\u2019s Inequality", "text": "Theorem (Hoeffding): Let {i} ni = 1 [Li, Ui] n be a sequence of independent random variables, so that E [i] = \u00b5i. Then it is valid that P (EEEEEEEEEE-EEE-EE-EE-EE-EE-EE-EE-EE-EE-EE-EE-EE-EE-EE-EE-EE-EE-EE-EE-EE-EE-EE-EE-EE-EE-EE-EE-EE-EE-EE-EE-EE-EE-E-E-EE-E-EE-E-E-E-E-E-E-E-E-E-E-E-E-E-E-E-E-E-E-E-E-E-E-E-E-E-E-E-E-E-E-E-E-E-E-E-E-E-E-E-E-E-E-E-E-E-E-E-E-E-E-E-E-E-E-E-E-E-E-E-E-E-E-E-E-E-E-E-E-E-E-E-E-E-E-E-E-E-E-E-E-E-E-E-E-E-E-E-E-E-E-E-E-E-E-E-E-E-E-E-E-E-E-E-E-E-E-E-E-E-E-E-E-E-E-E-E-E-E-E-E-E-E-E-E-E-E-E-E-E-E-E-E-E-E-E-E-E-E-E-E-E-E-E-E-E-E-E-E-E-E-E-E-E-E-E-E-E-E-E-E-E-E-E-E-E-E-E-E-"}, {"heading": "B Proof of Lemma 3.4", "text": "Proof. First of all, it should be noted that k = k * is the problem immediately after Theorem 3.1. In the following, we set the problem for k < k *. Thus, we set k * (k) n = {\u03b1 \u00b2 n: \u03b1i = 0, \u0441i > k} and consider the following optimization problem: min \u03b1 \u00b2 (k) n (\u03b1 \u00b2 2 + \u03b1 > \u03b2) (P2k). Similar to the proof of Theorem 3.1 and sequence 3.2, it can be shown that k \u00b2 k exists in such a way that the optimal solution of (P2k) exists in the form (\u03b11,..., \u03b1k \u00b2, 0.., 0) in which the problem i > 0 and i \u2264 k \u00b2 exists. Furthermore, it can be shown that the value of (P2k) is equal to the optimum of the solution, whereby lt. = 1k \u00b2 = 1 & lt.K \u00b2 (k \u00b2) is implied."}], "references": [{"title": "On bandwidth variation in kernel estimates-a square root law", "author": ["I.S. Abramson"], "venue": "The annals of Statistics,", "citeRegEx": "Abramson.,? \\Q1982\\E", "shortCiteRegEx": "Abramson.", "year": 1982}, {"title": "Automated web usage data mining and recommendation system using k-nearest neighbor (knn) classification method", "author": ["D. Adeniyi", "Z. Wei", "Y. Yongquan"], "venue": "Applied Computing and Informatics,", "citeRegEx": "Adeniyi et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Adeniyi et al\\.", "year": 2016}, {"title": "Modification of the adaptive nadaraya-watson kernel regression estimator", "author": ["K.H. Aljuhani"], "venue": "Scientific Research and Essays,", "citeRegEx": "Aljuhani,? \\Q2014\\E", "shortCiteRegEx": "Aljuhani", "year": 2014}, {"title": "An adaptive k-nearest neighbor text categorization strategy", "author": ["L. Baoli", "L. Qin", "Y. Shiwen"], "venue": "ACM Transactions on Asian Language Information Processing (TALIP),", "citeRegEx": "Baoli et al\\.,? \\Q2004\\E", "shortCiteRegEx": "Baoli et al\\.", "year": 2004}, {"title": "Nearest neighbor pattern classification", "author": ["T.M. Cover", "P.E. Hart"], "venue": "IEEE Transactions on Information Theory,", "citeRegEx": "Cover and Hart.,? \\Q1967\\E", "shortCiteRegEx": "Cover and Hart.", "year": 1967}, {"title": "Classification of heart disease using k-nearest neighbor and genetic algorithm", "author": ["B. Deekshatulu", "P. Chandra"], "venue": "Procedia Technology,", "citeRegEx": "Deekshatulu and Chandra,? \\Q2013\\E", "shortCiteRegEx": "Deekshatulu and Chandra", "year": 2013}, {"title": "Toktami\u015f. On the adaptive nadaraya-watson kernel regression estimators", "author": ["\u00d6.S. Demir"], "venue": "Hacettepe Journal of Mathematics and Statistics,", "citeRegEx": "Demir,? \\Q2010\\E", "shortCiteRegEx": "Demir", "year": 2010}, {"title": "A probabilistic theory of pattern recognition, volume 31", "author": ["L. Devroye", "L. Gy\u00f6rfi", "G. Lugosi"], "venue": "Springer Science & Business Media,", "citeRegEx": "Devroye et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Devroye et al\\.", "year": 2013}, {"title": "Distribution-free consistency results in nonparametric discrimination and regression function estimation", "author": ["L.P. Devroye", "T. Wagner"], "venue": "The Annals of Statistics,", "citeRegEx": "Devroye and Wagner,? \\Q1980\\E", "shortCiteRegEx": "Devroye and Wagner", "year": 1980}, {"title": "Local polynomial modelling and its applications: monographs on statistics and applied probability 66, volume 66", "author": ["J. Fan", "I. Gijbels"], "venue": null, "citeRegEx": "Fan and Gijbels.,? \\Q1996\\E", "shortCiteRegEx": "Fan and Gijbels.", "year": 1996}, {"title": "Discriminatory analysis-nonparametric discrimination: consistency properties", "author": ["E. Fix", "J.L. Hodges Jr."], "venue": "Technical report, DTIC Document,", "citeRegEx": "Fix and Jr.,? \\Q1951\\E", "shortCiteRegEx": "Fix and Jr.", "year": 1951}, {"title": "On nearest neighbor classification using adaptive choice of k", "author": ["A.K. Ghosh"], "venue": "Journal of computational and graphical statistics,", "citeRegEx": "Ghosh.,? \\Q2007\\E", "shortCiteRegEx": "Ghosh.", "year": 2007}, {"title": "A distribution-free theory of nonparametric regression", "author": ["L. Gy\u00f6rfi", "M. Kohler", "A. Krzyzak", "H. Walk"], "venue": "Springer Science & Business Media,", "citeRegEx": "Gy\u00f6rfi et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Gy\u00f6rfi et al\\.", "year": 2006}, {"title": "Application of k-nearest neighbor (knn) approach for predicting economic events: Theoretical background", "author": ["S.B. Imandoust", "M. Bolandraftar"], "venue": "International Journal of Engineering Research and Applications,", "citeRegEx": "Imandoust and Bolandraftar.,? \\Q2013\\E", "shortCiteRegEx": "Imandoust and Bolandraftar.", "year": 2013}, {"title": "Approximate nearest neighbors: towards removing the curse of dimensionality", "author": ["P. Indyk", "R. Motwani"], "venue": "In Proceedings of the thirtieth annual ACM symposium on Theory of computing,", "citeRegEx": "Indyk and Motwani.,? \\Q1998\\E", "shortCiteRegEx": "Indyk and Motwani.", "year": 1998}, {"title": "Metric learning: A survey", "author": ["B. Kulis"], "venue": "Foundations and Trends in Machine Learning,", "citeRegEx": "Kulis.,? \\Q2012\\E", "shortCiteRegEx": "Kulis.", "year": 2012}, {"title": "On estimating regression", "author": ["E.A. Nadaraya"], "venue": "Theory of Probability & Its Applications,", "citeRegEx": "Nadaraya.,? \\Q1964\\E", "shortCiteRegEx": "Nadaraya.", "year": 1964}, {"title": "Optimal weighted nearest neighbour classifiers", "author": ["R.J. Samworth"], "venue": "The Annals of Statistics,", "citeRegEx": "Samworth,? \\Q2012\\E", "shortCiteRegEx": "Samworth", "year": 2012}, {"title": "Density estimation for statistics and data analysis, volume 26", "author": ["B.W. Silverman"], "venue": "CRC press,", "citeRegEx": "Silverman.,? \\Q1986\\E", "shortCiteRegEx": "Silverman.", "year": 1986}, {"title": "Consistent nonparametric regression", "author": ["C.J. Stone"], "venue": "The Annals of Statistics,", "citeRegEx": "Stone.,? \\Q1977\\E", "shortCiteRegEx": "Stone.", "year": 1977}, {"title": "An adaptive k-nearest neighbor algorithm", "author": ["S. Sun", "R. Huang"], "venue": "In 2010 Seventh International Conference on Fuzzy Systems and Knowledge Discovery", "citeRegEx": "Sun and Huang.,? \\Q2010\\E", "shortCiteRegEx": "Sun and Huang.", "year": 2010}, {"title": "Knn with tf-idf based framework for text categorization", "author": ["B. Trstenjak", "S. Mikac", "D. Donko"], "venue": "Procedia Engineering,", "citeRegEx": "Trstenjak et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Trstenjak et al\\.", "year": 2014}, {"title": "Smooth regression analysis", "author": ["G.S. Watson"], "venue": "Sankhya\u0304: The Indian Journal of Statistics,", "citeRegEx": "Watson.,? \\Q1964\\E", "shortCiteRegEx": "Watson.", "year": 1964}, {"title": "Locally adaptive nearest neighbor algorithms", "author": ["D. Wettschereck", "T.G. Dietterich"], "venue": "Advances in Neural Information Processing Systems,", "citeRegEx": "Wettschereck and Dietterich.,? \\Q1994\\E", "shortCiteRegEx": "Wettschereck and Dietterich.", "year": 1994}, {"title": "Top 10 algorithms in data mining", "author": ["X. Wu", "V. Kumar", "J.R. Quinlan", "J. Ghosh", "Q. Yang", "H. Motoda", "G.J. McLachlan", "A. Ng", "B. Liu", "S.Y. Philip"], "venue": "Knowledge and information systems,", "citeRegEx": "Wu et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Wu et al\\.", "year": 2008}], "referenceMentions": [{"referenceID": 3, "context": "The k-nearest neighbors (k-NN) algorithm Cover and Hart (1967); Fix and Hodges Jr (1951), and Nadarays-Watson estimation Nadaraya (1964); Watson (1964) are the cornerstones of non-parametric learning.", "startOffset": 41, "endOffset": 63}, {"referenceID": 3, "context": "The k-nearest neighbors (k-NN) algorithm Cover and Hart (1967); Fix and Hodges Jr (1951), and Nadarays-Watson estimation Nadaraya (1964); Watson (1964) are the cornerstones of non-parametric learning.", "startOffset": 41, "endOffset": 89}, {"referenceID": 3, "context": "The k-nearest neighbors (k-NN) algorithm Cover and Hart (1967); Fix and Hodges Jr (1951), and Nadarays-Watson estimation Nadaraya (1964); Watson (1964) are the cornerstones of non-parametric learning.", "startOffset": 41, "endOffset": 137}, {"referenceID": 3, "context": "The k-nearest neighbors (k-NN) algorithm Cover and Hart (1967); Fix and Hodges Jr (1951), and Nadarays-Watson estimation Nadaraya (1964); Watson (1964) are the cornerstones of non-parametric learning.", "startOffset": 41, "endOffset": 152}, {"referenceID": 3, "context": "The k-nearest neighbors (k-NN) algorithm Cover and Hart (1967); Fix and Hodges Jr (1951), and Nadarays-Watson estimation Nadaraya (1964); Watson (1964) are the cornerstones of non-parametric learning. Owing to their simplicity and flexibility, these procedures had become the methods of choice in many scenarios Wu et al. (2008), especially in settings where the underlying model is complex.", "startOffset": 41, "endOffset": 329}, {"referenceID": 1, "context": "Modern applications of the k-NN algorithm include recommendation systems Adeniyi et al. (2016), text categorization Trstenjak et al.", "startOffset": 73, "endOffset": 95}, {"referenceID": 1, "context": "Modern applications of the k-NN algorithm include recommendation systems Adeniyi et al. (2016), text categorization Trstenjak et al. (2014), heart disease classification Deekshatulu et al.", "startOffset": 73, "endOffset": 140}, {"referenceID": 1, "context": "Modern applications of the k-NN algorithm include recommendation systems Adeniyi et al. (2016), text categorization Trstenjak et al. (2014), heart disease classification Deekshatulu et al. (2013), and financial market prediction Imandoust and Bolandraftar (2013), amongst others.", "startOffset": 73, "endOffset": 196}, {"referenceID": 1, "context": "Modern applications of the k-NN algorithm include recommendation systems Adeniyi et al. (2016), text categorization Trstenjak et al. (2014), heart disease classification Deekshatulu et al. (2013), and financial market prediction Imandoust and Bolandraftar (2013), amongst others.", "startOffset": 73, "endOffset": 263}, {"referenceID": 7, "context": "Most of the theoretic work focuses on the asymptotic regime in which the number of samples n goes to infinity Devroye et al. (2013); Samworth et al.", "startOffset": 110, "endOffset": 132}, {"referenceID": 7, "context": "Most of the theoretic work focuses on the asymptotic regime in which the number of samples n goes to infinity Devroye et al. (2013); Samworth et al. (2012); Stone (1977), and ignores the practical regime in which n is finite.", "startOffset": 110, "endOffset": 156}, {"referenceID": 7, "context": "Most of the theoretic work focuses on the asymptotic regime in which the number of samples n goes to infinity Devroye et al. (2013); Samworth et al. (2012); Stone (1977), and ignores the practical regime in which n is finite.", "startOffset": 110, "endOffset": 170}, {"referenceID": 15, "context": "Similar guarantees hold for weighted k-NN rules, with the additional assumptions that \u2211k i=1 \u03b1i = 1 and maxi\u2264n \u03b1i \u2192 0, Stone (1977); Devroye et al.", "startOffset": 119, "endOffset": 132}, {"referenceID": 7, "context": "Similar guarantees hold for weighted k-NN rules, with the additional assumptions that \u2211k i=1 \u03b1i = 1 and maxi\u2264n \u03b1i \u2192 0, Stone (1977); Devroye et al. (2013). In the regime of practical interest where the number of samples n is finite, using k = b \u221a nc neighbors is a widely mentioned rule of thumb Devroye et al.", "startOffset": 133, "endOffset": 155}, {"referenceID": 7, "context": "Similar guarantees hold for weighted k-NN rules, with the additional assumptions that \u2211k i=1 \u03b1i = 1 and maxi\u2264n \u03b1i \u2192 0, Stone (1977); Devroye et al. (2013). In the regime of practical interest where the number of samples n is finite, using k = b \u221a nc neighbors is a widely mentioned rule of thumb Devroye et al. (2013). Nevertheless, this rule often yields poor results, and in the regime of finite samples it is usually advised to choose k using cross-validation.", "startOffset": 133, "endOffset": 318}, {"referenceID": 7, "context": "Similar guarantees hold for weighted k-NN rules, with the additional assumptions that \u2211k i=1 \u03b1i = 1 and maxi\u2264n \u03b1i \u2192 0, Stone (1977); Devroye et al. (2013). In the regime of practical interest where the number of samples n is finite, using k = b \u221a nc neighbors is a widely mentioned rule of thumb Devroye et al. (2013). Nevertheless, this rule often yields poor results, and in the regime of finite samples it is usually advised to choose k using cross-validation. Similar consistency results apply to kernel based local methods Devroye et al. (1980); Gy\u00f6rfi et al.", "startOffset": 133, "endOffset": 550}, {"referenceID": 7, "context": "Similar guarantees hold for weighted k-NN rules, with the additional assumptions that \u2211k i=1 \u03b1i = 1 and maxi\u2264n \u03b1i \u2192 0, Stone (1977); Devroye et al. (2013). In the regime of practical interest where the number of samples n is finite, using k = b \u221a nc neighbors is a widely mentioned rule of thumb Devroye et al. (2013). Nevertheless, this rule often yields poor results, and in the regime of finite samples it is usually advised to choose k using cross-validation. Similar consistency results apply to kernel based local methods Devroye et al. (1980); Gy\u00f6rfi et al. (2006). A novel study of k-NN by Samworth, Samworth et al.", "startOffset": 133, "endOffset": 572}, {"referenceID": 7, "context": "Similar guarantees hold for weighted k-NN rules, with the additional assumptions that \u2211k i=1 \u03b1i = 1 and maxi\u2264n \u03b1i \u2192 0, Stone (1977); Devroye et al. (2013). In the regime of practical interest where the number of samples n is finite, using k = b \u221a nc neighbors is a widely mentioned rule of thumb Devroye et al. (2013). Nevertheless, this rule often yields poor results, and in the regime of finite samples it is usually advised to choose k using cross-validation. Similar consistency results apply to kernel based local methods Devroye et al. (1980); Gy\u00f6rfi et al. (2006). A novel study of k-NN by Samworth, Samworth et al. (2012), derives a closed form expression for the optimal weight vector, and extracts the optimal number of neighbors.", "startOffset": 133, "endOffset": 631}, {"referenceID": 7, "context": "Similar guarantees hold for weighted k-NN rules, with the additional assumptions that \u2211k i=1 \u03b1i = 1 and maxi\u2264n \u03b1i \u2192 0, Stone (1977); Devroye et al. (2013). In the regime of practical interest where the number of samples n is finite, using k = b \u221a nc neighbors is a widely mentioned rule of thumb Devroye et al. (2013). Nevertheless, this rule often yields poor results, and in the regime of finite samples it is usually advised to choose k using cross-validation. Similar consistency results apply to kernel based local methods Devroye et al. (1980); Gy\u00f6rfi et al. (2006). A novel study of k-NN by Samworth, Samworth et al. (2012), derives a closed form expression for the optimal weight vector, and extracts the optimal number of neighbors. However, this result is only optimal under several restrictive assumptions, and only holds for the asymptotic regime where n \u2192 \u221e. Furthermore, the above optimal number of neighbors/weights do not adapt, but are rather fixed over all decision points given the dataset. In the context of kernel based methods, it is possible to extract an expression for the optimal kernel\u2019s bandwidth \u03c3 Gy\u00f6rfi et al. (2006); Fan and Gijbels (1996).", "startOffset": 133, "endOffset": 1148}, {"referenceID": 7, "context": "Similar guarantees hold for weighted k-NN rules, with the additional assumptions that \u2211k i=1 \u03b1i = 1 and maxi\u2264n \u03b1i \u2192 0, Stone (1977); Devroye et al. (2013). In the regime of practical interest where the number of samples n is finite, using k = b \u221a nc neighbors is a widely mentioned rule of thumb Devroye et al. (2013). Nevertheless, this rule often yields poor results, and in the regime of finite samples it is usually advised to choose k using cross-validation. Similar consistency results apply to kernel based local methods Devroye et al. (1980); Gy\u00f6rfi et al. (2006). A novel study of k-NN by Samworth, Samworth et al. (2012), derives a closed form expression for the optimal weight vector, and extracts the optimal number of neighbors. However, this result is only optimal under several restrictive assumptions, and only holds for the asymptotic regime where n \u2192 \u221e. Furthermore, the above optimal number of neighbors/weights do not adapt, but are rather fixed over all decision points given the dataset. In the context of kernel based methods, it is possible to extract an expression for the optimal kernel\u2019s bandwidth \u03c3 Gy\u00f6rfi et al. (2006); Fan and Gijbels (1996). Nevertheless, this bandwidth is fixed over all decision points, and is only optimal under several restrictive assumptions.", "startOffset": 133, "endOffset": 1172}, {"referenceID": 14, "context": "In Wettschereck and Dietterich (1994); Sun and Huang it is suggested to use local cross-validation in order to adapt the value of k to different decision points.", "startOffset": 3, "endOffset": 38}, {"referenceID": 6, "context": "Conversely, Ghosh Ghosh (2007) takes a Bayesian approach towards choosing k adaptively.", "startOffset": 12, "endOffset": 31}, {"referenceID": 1, "context": "Focusing on the multiclass classification setup, it is suggested in Baoli et al. (2004) to consider different values of k for each class, choosing k proportionally to the class populations.", "startOffset": 68, "endOffset": 88}, {"referenceID": 0, "context": "Similarly, there exist several attitudes towards adaptively choosing the kernel\u2019s bandwidth \u03c3, for kernel based methods Abramson (1982); Silverman (1986); Demir and Toktami\u015f (2010); Aljuhani et al.", "startOffset": 120, "endOffset": 136}, {"referenceID": 0, "context": "Similarly, there exist several attitudes towards adaptively choosing the kernel\u2019s bandwidth \u03c3, for kernel based methods Abramson (1982); Silverman (1986); Demir and Toktami\u015f (2010); Aljuhani et al.", "startOffset": 120, "endOffset": 154}, {"referenceID": 0, "context": "Similarly, there exist several attitudes towards adaptively choosing the kernel\u2019s bandwidth \u03c3, for kernel based methods Abramson (1982); Silverman (1986); Demir and Toktami\u015f (2010); Aljuhani et al.", "startOffset": 120, "endOffset": 181}, {"referenceID": 0, "context": "Similarly, there exist several attitudes towards adaptively choosing the kernel\u2019s bandwidth \u03c3, for kernel based methods Abramson (1982); Silverman (1986); Demir and Toktami\u015f (2010); Aljuhani et al. (2014). Learning the distance metric for k-NN was extensively studied throughout the last decade.", "startOffset": 120, "endOffset": 205}, {"referenceID": 0, "context": "Similarly, there exist several attitudes towards adaptively choosing the kernel\u2019s bandwidth \u03c3, for kernel based methods Abramson (1982); Silverman (1986); Demir and Toktami\u015f (2010); Aljuhani et al. (2014). Learning the distance metric for k-NN was extensively studied throughout the last decade. There are several approaches towards metric learning, which roughly divide into linear/non-linear learning methods. It was found that metric learning may significantly affect the performance of k-NN in numerous applications, including computer vision, text analysis, program analysis and more. A comprehensive survey by Kulis Kulis (2012) provides a review of the metric learning literature.", "startOffset": 120, "endOffset": 635}, {"referenceID": 0, "context": "Similarly, there exist several attitudes towards adaptively choosing the kernel\u2019s bandwidth \u03c3, for kernel based methods Abramson (1982); Silverman (1986); Demir and Toktami\u015f (2010); Aljuhani et al. (2014). Learning the distance metric for k-NN was extensively studied throughout the last decade. There are several approaches towards metric learning, which roughly divide into linear/non-linear learning methods. It was found that metric learning may significantly affect the performance of k-NN in numerous applications, including computer vision, text analysis, program analysis and more. A comprehensive survey by Kulis Kulis (2012) provides a review of the metric learning literature. Throughout this work we assume that the distance metric is fixed, and thus the focus is on finding the best (in a sense) values of k and \u03b1 for each new data point. Two comprehensive monographs, Devroye et al. (2013) and Biau and Devroye (2015), provide an extensive survey of the existing literature regarding k-NN rules, including theoretical guarantees, useful practices, limitations and more.", "startOffset": 120, "endOffset": 904}, {"referenceID": 0, "context": "Similarly, there exist several attitudes towards adaptively choosing the kernel\u2019s bandwidth \u03c3, for kernel based methods Abramson (1982); Silverman (1986); Demir and Toktami\u015f (2010); Aljuhani et al. (2014). Learning the distance metric for k-NN was extensively studied throughout the last decade. There are several approaches towards metric learning, which roughly divide into linear/non-linear learning methods. It was found that metric learning may significantly affect the performance of k-NN in numerous applications, including computer vision, text analysis, program analysis and more. A comprehensive survey by Kulis Kulis (2012) provides a review of the metric learning literature. Throughout this work we assume that the distance metric is fixed, and thus the focus is on finding the best (in a sense) values of k and \u03b1 for each new data point. Two comprehensive monographs, Devroye et al. (2013) and Biau and Devroye (2015), provide an extensive survey of the existing literature regarding k-NN rules, including theoretical guarantees, useful practices, limitations and more.", "startOffset": 120, "endOffset": 932}, {"referenceID": 14, "context": "Note that in some cases Indyk and Motwani (1998), using advanced data structures may decrease the cost of finding the nearest neighbors (i.", "startOffset": 24, "endOffset": 49}], "year": 2017, "abstractText": "The weighted k-nearest neighbors algorithm is one of the most fundamental non-parametric methods in pattern recognition and machine learning. The question of setting the optimal number of neighbors as well as the optimal weights has received much attention throughout the years, nevertheless this problem seems to have remained unsettled. In this paper we offer a simple approach to locally weighted regression/classification, where we make the biasvariance tradeoff explicit. Our formulation enables us to phrase a notion of optimal weights, and to efficiently find these weights as well as the optimal number of neighbors efficiently and adaptively, for each data point whose value we wish to estimate. The applicability of our approach is demonstrated on several datasets, showing superior performance over standard locally weighted methods.", "creator": "LaTeX with hyperref package"}}}