{"id": "1701.07481", "review": {"conference": "ACL", "VERSION": "v1", "DATE_OF_SUBMISSION": "25-Jan-2017", "title": "Learning Word-Like Units from Joint Audio-Visual Analysis", "abstract": "Given a collection of images and spoken audio captions, we present a method for discovering word-like acoustic units in the continuous speech signal and grounding them to semantically relevant image regions. For example, our model is able to detect spoken instances of the word 'lighthouse' within an utterance and associate them with image regions containing lighthouses. We do not use any form of conventional automatic speech recognition, nor do we use any text transcriptions or conventional linguistic annotations. Our model effectively implements a form of spoken language acquisition, in which the computer learns not only to recognize word categories by sound, but also to enrich the words it learns with semantics by grounding them in images.", "histories": [["v1", "Wed, 25 Jan 2017 20:40:56 GMT  (6260kb,D)", "http://arxiv.org/abs/1701.07481v1", null], ["v2", "Tue, 7 Feb 2017 15:15:41 GMT  (6260kb,D)", "http://arxiv.org/abs/1701.07481v2", null], ["v3", "Wed, 24 May 2017 22:10:25 GMT  (1862kb,D)", "http://arxiv.org/abs/1701.07481v3", null]], "reviews": [], "SUBJECTS": "cs.CL cs.CV", "authors": ["david harwath", "james r glass"], "accepted": true, "id": "1701.07481"}, "pdf": {"name": "1701.07481.pdf", "metadata": {"source": "CRF", "title": null, "authors": ["David Harwath", "James R. Glass"], "emails": ["dharwath@mit.edu", "glass@mit.edu"], "sections": [{"heading": null, "text": "Using a collection of images and spoken audio captions, we present a method for recognizing word-like acoustic units in the continuous speech signal and grounding them in semantically relevant image regions. For example, our model is capable of recognizing spoken instances of the word \"lighthouse\" within an utterance and linking them to image regions that contain lighthouses. We do not use any form of conventional automatic speech recognition, text transcriptions, or conventional linguistic annotations. Our model effectively implements a form of language acquisition where computers learn to recognize word categories not just by sound, but to enrich the words they learn through semantics by grounding them in images."}, {"heading": "1 INTRODUCTION", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "1.1 PROBLEM STATEMENT AND MOTIVATION", "text": "The automatic discovery of words and other elements of the linguistic structure from continuous speech has been a long-standing goal of computer linguists, cognitive science and other language processing fields. Virtually all people acquire language at a very young age, but this task has proven to be an incredibly difficult one for computers. While conventional automatic speech recognition (ASR) systems have a long history and have made great progress recently thanks to the revival of deep neural networks (DNNs), their reliance on heavily monitored training paradigms is essentially limited to the world's major languages, which account for a small fraction of the human languages spoken worldwide (Lewis et al., 2016). The main reason for this limitation is the fact that these monitored approaches require enormous amounts of very expensive human transcripts. Moreover, the use of the written word is a convenient but limited convention, as there are many oral languages that do not even use a written system."}, {"heading": "1.2 PREVIOUS WORK", "text": "Segmental Dynamic Time Warping (S-DTW) was introduced by Park & Glass (2008), which discovered repetitions of the same words and phrases in a collection of untranscribed acoustic data. Many subsequent efforts expanded these ideas (Jansen et al., 2010; Jansen & Van Durme, 2011; Dredze et al., 2010; Harwath et al., 2012; Zhang & Glass, 2009). Alternative approaches based on non-parametric modeling (Lee & Glass, 2012; Ondel et al., 2016) employed a generative model to group acoustic segments into phoneme-like categories, and related work aimed at either segmenting or learning phoneme-like images."}, {"heading": "2 EXPERIMENTAL DATA", "text": "We use a corpus of over 200,000 spoken captions for images taken from the Places205 dataset (Zhou et al., 2014), which is equivalent to more than 522 hours of voice data. Captions were collected using Amazon's Mechanical Turk service, where workers were shown images and asked to describe them in free form. Our data collection scheme is detailed in Harwath et al. (2016), but the experiments in this paper use almost twice as much data. To train our multimodal neural network and pattern-finding experiments, we use a subset of 214,585 image / caption pairs, and we have a set of 1,000 pairs ready to evaluate the retrievaluation capability of the multimodal network. Lacking basic truth-text transcripts for the data, we have used Google's public voice-recognition API to generate proxy transcripts that we use in the analysis of our system."}, {"heading": "3 AUDIO-VISUAL EMBEDDING NEURAL NETWORKS", "text": "We have first developed a deep multimodal embedding network, similar to that in Harwath et al. (2016), but with a more complex architecture. The model is trained to embed entire picture frames and entire spoken captions in a common embedding space; however, as we will show, the trained network can then be used to locate patterns that match words and phrases within the spectrogram, as well as visual objects within the image, by applying it to small sub-regions of the image and spectrogram. The model consists of two branches, one of which can be used as input images and the other which can be used as input spectrograms. The image network is formed by applying the off-the-shelf VGG 16 layers of the network (Simonyan & Zisserman, 2014) and replacing the softmax classification layer with a linear transform that incorporates the 4096 multidimensional activations of the second layer of our 1024 dimensional space."}, {"heading": "4 FINDING AND CLUSTERING AUDIO-VISUAL CAPTION GROUNDINGS", "text": "So far, so far, so far, so far, so far, so far, so far, so far, so far, so far, so, so, so, so, so, so, so, so, so, so, so, so, so, so, so, so, so, so, so, so, so, so, so, so, so, so, so, so, so, so, so, so, so, so, so, so, so, so, so, so, so, so, so, so, so, so, so, so, so, so, so, so, so, so, so, so, so, so, so, so, so, so, so, so, so, so, so, so, so, so, so, so, so, so, so, so, so, so, so, so, so, so, so, so, so, so, so, so, so, so, so, so, so, so, so, so, so, so, so, so, so, so, so, so, so, so, so, so, so, so, so, so, so, so, so, so, so, so, so, so, so, so, so, so, so, so, so, so, so, so, so, so, so, so, so, so, so, so, so, so, so, so, so, so, so, so, so, so, so, so, so, so, so, so, so, so, so, so, so, so, so, so, so, so, so, so, so, so, so, so, so, so, so, so, so, so, so, so, so, so, so, so, so, so, so, so, so, so, so, so, so, so, so, so, so, so, so, so, so, so, so, so, so, so, so, so, so, so, so, so, so, so, so, so, so, so, so, so, so, so, so, so, so, so, so, so, so, so, so, so, so, so, so, so, so, so, so, so, so, so,"}, {"heading": "5 EXPERIMENTS AND ANALYSIS", "text": "We are committed to the idea that we will be able to change the world and that we will be able to change the world, \"he said in an interview with\" Welt am Sonntag. \""}, {"heading": "6 CONCLUSIONS AND FUTURE WORK", "text": "In this work, we have shown that a neural network trained to associate images with the waveforms that represent their spoken audio captions can be successfully applied to detect and cluster acoustic patterns that represent words or short phrases in untranscribed audio data. An analogous process can be applied to visual images to detect visual patterns, and then the two modalities can be linked together so that the network can learn, for example, that spoken instances of the word \"train\" are associated with image regions that contain trains, without the use of a conventional automatic speech recognition system and without text transcriptions, and is therefore completely agnostic in the language in which the captions are spoken, in the O (n) time in terms of the number of spoken image / caption pairs, while previous state-of-the-art acoustic pattern finding algorithms are used solely by the O (the acoustic data used in the O)."}, {"heading": "A APPENDIX: ADDITIONAL VISUALIZATIONS OF IMAGE PATTERN CLUSTERS", "text": "beach cliff pool desert fieldchair table stairs statue stone church forest mountain skyscraper tree waterfall windmills window city bridge builder man wall archway baseball boat shelves cockpit girl nursery rock kitchen plant hallway"}], "references": [{"title": "Self-taught object localization with deep networks", "author": ["Alessandro Bergamo", "Loris Bazzani", "Dragomir Anguelov", "Lorenzo Torresani"], "venue": "CoRR, abs/1409.3964,", "citeRegEx": "Bergamo et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Bergamo et al\\.", "year": 2014}, {"title": "Unsupervised object discovery and localization in the wild: Part-based matching with bottom-up region proposals", "author": ["Minsu Cho", "Suha Kwak", "Cordelia Schmid", "Jean Ponce"], "venue": "In Proceedings of CVPR,", "citeRegEx": "Cho et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Cho et al\\.", "year": 2015}, {"title": "Weakly supervised object localization with multi-fold multiple instance learning", "author": ["Ramazan Cinbis", "Jakob Verbeek", "Cordelia Schmid"], "venue": "In IEEE Transactions on Pattern Analysis and Machine Intelligence,", "citeRegEx": "Cinbis et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Cinbis et al\\.", "year": 2016}, {"title": "NLP on spoken documents without ASR", "author": ["Mark Dredze", "Aren Jansen", "Glen Coppersmith", "Kenneth Church"], "venue": "In Proceedings of EMNLP,", "citeRegEx": "Dredze et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Dredze et al\\.", "year": 2010}, {"title": "From captions to visual concepts and back", "author": ["Hao Fang", "Saurabh Gupta", "Forrest Iandola", "Srivastava Rupesh", "Li Deng", "Piotr Dollar", "Jianfeng Gao", "Xiaodong He", "Margaret Mitchell", "Platt John C", "C. Lawrence Zitnick", "Geoffrey Zweig"], "venue": "Proceedings of CVPR,", "citeRegEx": "Fang et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Fang et al\\.", "year": 2015}, {"title": "Devise: A deep visual-semantic embedding model", "author": ["Andrea Frome", "Greg S. Corrado", "Jonathon Shlens", "Samy Bengio", "Jeffrey Dean", "Marc\u2019Aurelio Ranzato", "Tomas Mikolov"], "venue": "In Proceedings of the Neural Information Processing Society,", "citeRegEx": "Frome et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Frome et al\\.", "year": 2013}, {"title": "The TIMIT acoustic-phonetic continuous speech", "author": ["John Garofolo", "Lori Lamel", "William Fisher", "Jonathan Fiscus", "David Pallet", "Nancy Dahlgren", "Victor Zue"], "venue": null, "citeRegEx": "Garofolo et al\\.,? \\Q1993\\E", "shortCiteRegEx": "Garofolo et al\\.", "year": 1993}, {"title": "Chrupaa. From phonemes to images: levels of representation in a recurrent neural model of visually-grounded language learning", "author": ["Lieke Gelderloos", "Grzegorz"], "venue": null, "citeRegEx": "Gelderloos and Grzegorz,? \\Q2016\\E", "shortCiteRegEx": "Gelderloos and Grzegorz", "year": 2016}, {"title": "A Bayesian framework for word segmentation: exploring the effects of context", "author": ["Sharon Goldwater", "Thomas Griffiths", "Mark Johnson"], "venue": "In Cognition,", "citeRegEx": "Goldwater et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Goldwater et al\\.", "year": 2009}, {"title": "Deep multimodal semantic embeddings for speech and images", "author": ["David Harwath", "James Glass"], "venue": "In Proceedings of the IEEE Workshop on Automatic Speech Recognition and Understanding,", "citeRegEx": "Harwath and Glass.,? \\Q2015\\E", "shortCiteRegEx": "Harwath and Glass.", "year": 2015}, {"title": "Zero resource spoken audio corpus analysis", "author": ["David Harwath", "Timothy J. Hazen", "James Glass"], "venue": "In Proceedings of ICASSP,", "citeRegEx": "Harwath et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Harwath et al\\.", "year": 2012}, {"title": "Unsupervised learning of spoken language with visual context", "author": ["David Harwath", "Antonio Torralba", "James R. Glass"], "venue": "In Proceedings of NIPS,", "citeRegEx": "Harwath et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Harwath et al\\.", "year": 2016}, {"title": "Efficient spoken term discovery using randomized algorithms", "author": ["Aren Jansen", "Benjamin Van Durme"], "venue": "In Proceedings of IEEE Workshop on Automatic Speech Recognition and Understanding,", "citeRegEx": "Jansen and Durme.,? \\Q2011\\E", "shortCiteRegEx": "Jansen and Durme.", "year": 2011}, {"title": "Toward spoken term discovery at scale with zero resources", "author": ["Aren Jansen", "Kenneth Church", "Hynek Hermansky"], "venue": "In Proceedings of Interspeech,", "citeRegEx": "Jansen et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Jansen et al\\.", "year": 2010}, {"title": "Densecap: Fully convolutional localization networks for dense captioning", "author": ["Justin Johnson", "Andrej Karpathy", "Li Fei-Fei"], "venue": "In Proceedings of CVPR,", "citeRegEx": "Johnson et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Johnson et al\\.", "year": 2016}, {"title": "Unsupervised word segmentation for sesotho using adaptor grammars", "author": ["Mark Johnson"], "venue": "In Proceedings of ACL SIG on Computational Morphology and Phonology,", "citeRegEx": "Johnson.,? \\Q2008\\E", "shortCiteRegEx": "Johnson.", "year": 2008}, {"title": "Deep visual-semantic alignments for generating image descriptions", "author": ["Andrej Karpathy", "Fei-Fei Li"], "venue": "In Proceedings of CVPR,", "citeRegEx": "Karpathy and Li.,? \\Q2015\\E", "shortCiteRegEx": "Karpathy and Li.", "year": 2015}, {"title": "Deep fragment embeddings for bidirectional image sentence mapping", "author": ["Andrej Karpathy", "Armand Joulin", "Fei-Fei Li"], "venue": "In Proceedings of the Neural Information Processing Society,", "citeRegEx": "Karpathy et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Karpathy et al\\.", "year": 2014}, {"title": "A nonparametric Bayesian approach to acoustic model discovery", "author": ["Chia-Ying Lee", "James Glass"], "venue": "In Proceedings of the 2012 meeting of the Association for Computational Linguistics,", "citeRegEx": "Lee and Glass.,? \\Q2012\\E", "shortCiteRegEx": "Lee and Glass.", "year": 2012}, {"title": "Unsupervised lexicon discovery from acoustic input", "author": ["Chia-Ying Lee", "Timothy J. O\u2019Donnell", "James Glass"], "venue": "In Transactions of the Association for Computational Linguistics,", "citeRegEx": "Lee et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Lee et al\\.", "year": 2015}, {"title": "Ethnologue: Languages of the World, Nineteenth edition", "author": ["M. Paul Lewis", "Gary F. Simon", "Charles D. Fennig"], "venue": "SIL International. Online version: http://www.ethnologue.com,", "citeRegEx": "Lewis et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Lewis et al\\.", "year": 2016}, {"title": "Variational inference for acoustic unit discovery", "author": ["Lucas Ondel", "Lukas Burget", "Jan Cernocky"], "venue": "In 5th Workshop on Spoken Language Technology for Under-resourced Language,", "citeRegEx": "Ondel et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Ondel et al\\.", "year": 2016}, {"title": "Unsupervised pattern discovery in speech", "author": ["Alex Park", "James Glass"], "venue": "In IEEE Transactions on Audio, Speech, and Language Processing vol. 16,", "citeRegEx": "Park and Glass.,? \\Q2008\\E", "shortCiteRegEx": "Park and Glass.", "year": 2008}, {"title": "The Kaldi speech recognition toolkit", "author": ["Daniel Povey", "Arnab Ghoshal", "Gilles Boulianne", "Lukas Burget", "Ondrej Glembek", "Nagendra Goel", "Mirko Hannemann", "Petr Motlicek", "Yanmin Qian", "Petr Schwarz", "Jan Silovsky", "Georg Stemmer", "Karel Vesely"], "venue": "In IEEE 2011 Workshop on Automatic Speech Recognition and Understanding,", "citeRegEx": "Povey et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Povey et al\\.", "year": 2011}, {"title": "Very deep convolutional networks for large-scale image recognition", "author": ["Karen Simonyan", "Andrew Zisserman"], "venue": "CoRR, abs/1409.1556,", "citeRegEx": "Simonyan and Zisserman.,? \\Q2014\\E", "shortCiteRegEx": "Simonyan and Zisserman.", "year": 2014}, {"title": "Connecting modalities: Semi-supervised segmentation and annotation of images using unaligned text corpora", "author": ["Richard Socher", "Fei-Fei Li"], "venue": "In Proceedings of CVPR,", "citeRegEx": "Socher and Li.,? \\Q2010\\E", "shortCiteRegEx": "Socher and Li.", "year": 2010}, {"title": "Grounded compositional semantics for finding and describing images with sentences", "author": ["Richard Socher", "Andrej Karpathy", "Quoc V. Le", "Christopher D. Manning", "Andrew Y. Ng"], "venue": "In Transactions of the Association for Computational Linguistics,", "citeRegEx": "Socher et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Socher et al\\.", "year": 2014}, {"title": "Visualizing high-dimensional data using t-sne", "author": ["Laurens van der Maaten", "Geoffrey Hinton"], "venue": "In Journal of Machine Learning Research,", "citeRegEx": "Maaten and Hinton.,? \\Q2008\\E", "shortCiteRegEx": "Maaten and Hinton.", "year": 2008}, {"title": "Show and tell: A neural image caption generator", "author": ["Oriol Vinyals", "Alexander Toshev", "Samy Bengio", "Dimitru Erhan"], "venue": "In Proceedings of CVPR,", "citeRegEx": "Vinyals et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Vinyals et al\\.", "year": 2015}, {"title": "Unsupervised spoken keyword spotting via segmental DTW on Gaussian posteriorgrams", "author": ["Yaodong Zhang", "James Glass"], "venue": "In Proceedings ASRU,", "citeRegEx": "Zhang and Glass.,? \\Q2009\\E", "shortCiteRegEx": "Zhang and Glass.", "year": 2009}, {"title": "Learning deep features for scene recognition using places database", "author": ["Bolei Zhou", "Agata Lapedriza", "Jianxiong Xiao", "Antonio Torralba", "Aude Oliva"], "venue": "In Proceedings of the Neural Information Processing Society,", "citeRegEx": "Zhou et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Zhou et al\\.", "year": 2014}, {"title": "Object detectors emerge in deep scene CNNs", "author": ["Boloi Zhou", "Aditya Khosla", "Agata Lapedriza", "Aude Oliva", "Antonio Torralba"], "venue": "In Proceedings of ICLR,", "citeRegEx": "Zhou et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Zhou et al\\.", "year": 2015}], "referenceMentions": [{"referenceID": 20, "context": "While conventional automatic speech recognition (ASR) systems have a long history and have recently made great strides thanks to the revival of deep neural networks (DNNs), their reliance on highly supervised training paradigms has essentially restricted their application to the major languages of the world, accounting for a small fraction of the more than 7,000 human languages spoken worldwide (Lewis et al., 2016).", "startOffset": 398, "endOffset": 418}, {"referenceID": 13, "context": "Previous work has presented algorithms for performing acoustic pattern discovery in continuous speech (Park & Glass, 2008; Jansen et al., 2010; Jansen & Van Durme, 2011) without the use of transcriptions or another modality, but those algorithms are limited in their ability to scale by their inherent O(n) complexity, since they do an exhaustive comparison of the data against itself.", "startOffset": 102, "endOffset": 169}, {"referenceID": 13, "context": "Many subsequent efforts extended these ideas(Jansen et al., 2010; Jansen & Van Durme, 2011; Dredze et al., 2010; Harwath et al., 2012; Zhang & Glass, 2009).", "startOffset": 44, "endOffset": 155}, {"referenceID": 3, "context": "Many subsequent efforts extended these ideas(Jansen et al., 2010; Jansen & Van Durme, 2011; Dredze et al., 2010; Harwath et al., 2012; Zhang & Glass, 2009).", "startOffset": 44, "endOffset": 155}, {"referenceID": 10, "context": "Many subsequent efforts extended these ideas(Jansen et al., 2010; Jansen & Van Durme, 2011; Dredze et al., 2010; Harwath et al., 2012; Zhang & Glass, 2009).", "startOffset": 44, "endOffset": 155}, {"referenceID": 21, "context": "Alternative approaches based on Bayesian nonparametric modeling (Lee & Glass, 2012; Ondel et al., 2016) employed a generative model to cluster acoustic segments into phoneme-like categories, and related works aimed to segment and cluster either reference or learned phoneme-like tokens into word-like and higher-level units (Johnson, 2008; Goldwater et al.", "startOffset": 64, "endOffset": 103}, {"referenceID": 15, "context": ", 2016) employed a generative model to cluster acoustic segments into phoneme-like categories, and related works aimed to segment and cluster either reference or learned phoneme-like tokens into word-like and higher-level units (Johnson, 2008; Goldwater et al., 2009; Lee et al., 2015).", "startOffset": 228, "endOffset": 285}, {"referenceID": 8, "context": ", 2016) employed a generative model to cluster acoustic segments into phoneme-like categories, and related works aimed to segment and cluster either reference or learned phoneme-like tokens into word-like and higher-level units (Johnson, 2008; Goldwater et al., 2009; Lee et al., 2015).", "startOffset": 228, "endOffset": 285}, {"referenceID": 19, "context": ", 2016) employed a generative model to cluster acoustic segments into phoneme-like categories, and related works aimed to segment and cluster either reference or learned phoneme-like tokens into word-like and higher-level units (Johnson, 2008; Goldwater et al., 2009; Lee et al., 2015).", "startOffset": 228, "endOffset": 285}, {"referenceID": 5, "context": "Many works have focused on generating annotations or text captions for images (Socher & Li, 2010; Frome et al., 2013; Socher et al., 2014; Karpathy et al., 2014; Karpathy & Li, 2015; Vinyals et al., 2015; Fang et al., 2015; Johnson et al., 2016).", "startOffset": 78, "endOffset": 245}, {"referenceID": 26, "context": "Many works have focused on generating annotations or text captions for images (Socher & Li, 2010; Frome et al., 2013; Socher et al., 2014; Karpathy et al., 2014; Karpathy & Li, 2015; Vinyals et al., 2015; Fang et al., 2015; Johnson et al., 2016).", "startOffset": 78, "endOffset": 245}, {"referenceID": 17, "context": "Many works have focused on generating annotations or text captions for images (Socher & Li, 2010; Frome et al., 2013; Socher et al., 2014; Karpathy et al., 2014; Karpathy & Li, 2015; Vinyals et al., 2015; Fang et al., 2015; Johnson et al., 2016).", "startOffset": 78, "endOffset": 245}, {"referenceID": 28, "context": "Many works have focused on generating annotations or text captions for images (Socher & Li, 2010; Frome et al., 2013; Socher et al., 2014; Karpathy et al., 2014; Karpathy & Li, 2015; Vinyals et al., 2015; Fang et al., 2015; Johnson et al., 2016).", "startOffset": 78, "endOffset": 245}, {"referenceID": 4, "context": "Many works have focused on generating annotations or text captions for images (Socher & Li, 2010; Frome et al., 2013; Socher et al., 2014; Karpathy et al., 2014; Karpathy & Li, 2015; Vinyals et al., 2015; Fang et al., 2015; Johnson et al., 2016).", "startOffset": 78, "endOffset": 245}, {"referenceID": 14, "context": "Many works have focused on generating annotations or text captions for images (Socher & Li, 2010; Frome et al., 2013; Socher et al., 2014; Karpathy et al., 2014; Karpathy & Li, 2015; Vinyals et al., 2015; Fang et al., 2015; Johnson et al., 2016).", "startOffset": 78, "endOffset": 245}, {"referenceID": 11, "context": "Several recent papers have taken these ideas beyond text, and attempted to relate images to spoken audio captions directly at the waveform level (Harwath & Glass, 2015; Harwath et al., 2016).", "startOffset": 145, "endOffset": 190}, {"referenceID": 4, "context": ", 2015; Fang et al., 2015; Johnson et al., 2016). One interesting intersection between word induction from phoneme strings and multimodal modeling of images and text is that of Gelderloos & Chrupaa (2016), who uses images to segment words within captions at the phoneme string level.", "startOffset": 8, "endOffset": 205}, {"referenceID": 0, "context": "While supervised object detection is a standard problem in the vision community, several recent works have tackled the problem of weakly-supervised or unsupervised object localization (Bergamo et al., 2014; Cho et al., 2015; Zhou et al., 2015; Cinbis et al., 2016).", "startOffset": 184, "endOffset": 264}, {"referenceID": 1, "context": "While supervised object detection is a standard problem in the vision community, several recent works have tackled the problem of weakly-supervised or unsupervised object localization (Bergamo et al., 2014; Cho et al., 2015; Zhou et al., 2015; Cinbis et al., 2016).", "startOffset": 184, "endOffset": 264}, {"referenceID": 31, "context": "While supervised object detection is a standard problem in the vision community, several recent works have tackled the problem of weakly-supervised or unsupervised object localization (Bergamo et al., 2014; Cho et al., 2015; Zhou et al., 2015; Cinbis et al., 2016).", "startOffset": 184, "endOffset": 264}, {"referenceID": 2, "context": "While supervised object detection is a standard problem in the vision community, several recent works have tackled the problem of weakly-supervised or unsupervised object localization (Bergamo et al., 2014; Cho et al., 2015; Zhou et al., 2015; Cinbis et al., 2016).", "startOffset": 184, "endOffset": 264}, {"referenceID": 30, "context": "We employ a corpus of over 200,000 spoken captions for images taken from the Places205 dataset (Zhou et al., 2014), corresponding to over 522 hours of speech data.", "startOffset": 95, "endOffset": 114}, {"referenceID": 10, "context": "Our data collection scheme is described in detail in Harwath et al. (2016), but the experiments in this paper leverage nearly twice the amount of data.", "startOffset": 53, "endOffset": 75}, {"referenceID": 10, "context": "We first train a deep multimodal embedding network similar in spirit to the one described in Harwath et al. (2016), but with a more sophisticated architecture.", "startOffset": 93, "endOffset": 115}, {"referenceID": 10, "context": ", 10sec of speech) by applying truncation or zero padding; this introduces computational savings and was shown in Harwath et al. (2016) to only slightly degrade the performance.", "startOffset": 114, "endOffset": 136}, {"referenceID": 10, "context": ", 10sec of speech) by applying truncation or zero padding; this introduces computational savings and was shown in Harwath et al. (2016) to only slightly degrade the performance. Additionally, both the images and spectrograms are mean normalized before training. The overall multimodal network is formed by tying together the image and audio branches with a layer which takes both of their output vectors and computes an inner product between them, representing the similarity score between a given image/caption pair. We train the network to assign high scores to matching image/caption pairs, and lower scores to mismatched pairs. The objective function and training procedure we use is identical to that described in Harwath et al. (2016), but we briefly describe it here.", "startOffset": 114, "endOffset": 741}, {"referenceID": 6, "context": "We solve this issue by using a simple voice activity detector (VAD) which was trained on the TIMIT corpus(Garofolo et al., 1993).", "startOffset": 105, "endOffset": 128}, {"referenceID": 10, "context": "We trained our multimodal network on a set of 214,585 image/caption pairs, and vetted it with an image search (given caption, find image) and annotation (given image, find caption) task similar to the one used in Harwath et al. (2016); Karpathy et al.", "startOffset": 213, "endOffset": 235}, {"referenceID": 10, "context": "We trained our multimodal network on a set of 214,585 image/caption pairs, and vetted it with an image search (given caption, find image) and annotation (given image, find caption) task similar to the one used in Harwath et al. (2016); Karpathy et al. (2014); Karpathy & Li (2015).", "startOffset": 213, "endOffset": 259}, {"referenceID": 10, "context": "We trained our multimodal network on a set of 214,585 image/caption pairs, and vetted it with an image search (given caption, find image) and annotation (given image, find caption) task similar to the one used in Harwath et al. (2016); Karpathy et al. (2014); Karpathy & Li (2015). The image annotation and search recall scores on a 1,000 image/caption pair held-out test set are shown in Table 1, and are compared against the model architecture used in Harwath et al.", "startOffset": 213, "endOffset": 281}, {"referenceID": 10, "context": "We trained our multimodal network on a set of 214,585 image/caption pairs, and vetted it with an image search (given caption, find image) and annotation (given image, find caption) task similar to the one used in Harwath et al. (2016); Karpathy et al. (2014); Karpathy & Li (2015). The image annotation and search recall scores on a 1,000 image/caption pair held-out test set are shown in Table 1, and are compared against the model architecture used in Harwath et al. (2016). We then performed the grounding and pattern clustering steps on the entire training dataset.", "startOffset": 213, "endOffset": 476}, {"referenceID": 23, "context": "The alignments are created with the help of a Kaldi (Povey et al., 2011) speech recognizer based on the standard WSJ recipe and trained using the Google ASR hypothesis as a proxy for the transcriptions.", "startOffset": 52, "endOffset": 72}, {"referenceID": 10, "context": "The model we use in this paper is compared against the meanpool variant of the model architecture presented in Harwath et al. (2016). For both training and testing, the captions were truncated/zero-padded to 10 seconds.", "startOffset": 111, "endOffset": 133}, {"referenceID": 11, "context": "(Harwath et al., 2016) 0.", "startOffset": 0, "endOffset": 22}], "year": 2017, "abstractText": "Given a collection of images and spoken audio captions, we present a method for discovering word-like acoustic units in the continuous speech signal and grounding them to semantically relevant image regions. For example, our model is able to detect spoken instances of the words \u201clighthouse\u201d within an utterance and associate them with image regions containing lighthouses. We do not use any form of conventional automatic speech recognition, nor do we use any text transcriptions or conventional linguistic annotations. Our model effectively implements a form of spoken language acquisition, in which the computer learns not only to recognize word categories by sound, but also to enrich the words it learns with semantics by grounding them in images.", "creator": "LaTeX with hyperref package"}}}