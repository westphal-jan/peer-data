{"id": "1701.07953", "review": {"conference": "ICML", "VERSION": "v1", "DATE_OF_SUBMISSION": "27-Jan-2017", "title": "The Price of Differential Privacy for Online Learning", "abstract": "We design differentially private algorithms for the problem of online linear optimization in the full information and bandit settings with optimal $\\tilde{O}(\\sqrt{T})$ regret bounds. In the full-information setting, our results demonstrate that $(\\epsilon, \\delta)$-differential privacy may be ensured for free - in particular, the regret bounds scale as $O(\\sqrt{T})+\\tilde{O}\\big(\\frac{1}{\\epsilon}\\log \\frac{1}{\\delta}\\big)$. For bandit linear optimization, and as a special case, for non-stochastic multi-armed bandits, the proposed algorithm achieves a regret of $O\\Big(\\frac{\\sqrt{T\\log T}}{\\epsilon}\\log \\frac{1}{\\delta}\\Big)$, while the previously best known bound was $\\tilde{O}\\Big(\\frac{T^{\\frac{3}{4}}}{\\epsilon}\\Big)$.", "histories": [["v1", "Fri, 27 Jan 2017 06:17:14 GMT  (26kb)", "http://arxiv.org/abs/1701.07953v1", null], ["v2", "Tue, 13 Jun 2017 21:25:12 GMT  (36kb)", "http://arxiv.org/abs/1701.07953v2", "To appear in the Proceedings of the 34th International Conference on Machine Learning (ICML), Sydney, Australia, 2017"]], "reviews": [], "SUBJECTS": "cs.LG stat.ML", "authors": ["naman agarwal", "karan singh"], "accepted": true, "id": "1701.07953"}, "pdf": {"name": "1701.07953.pdf", "metadata": {"source": "CRF", "title": "The Price of Differential Privacy For Online Learning", "authors": ["Naman Agarwal", "Karan Singh"], "emails": ["namana@cs.princeton.edu,", "karans@cs.princeton.edu,"], "sections": [{"heading": null, "text": "ar Xiv: 170 1.07 953v 1 [cs.L G] 27 Jan 20 \u221a T) 1 Repentance limits. In the full information set, our results show that (\u03b5, \u03b4) -differential privacy can be guaranteed free of charge - in particular the repentance limits scale as O (\u221a T) + O (1 \u03b5 log 1 \u03b4). In the linear optimization of bandits and as a special case for non-stochastic, multi-armed bandits, the proposed algorithm achieves a repentance of O (\u221a T log T \u03b5 log 1 \u03b4), whereas the previously known limit was O (T 3 4\u03b5)."}, {"heading": "1 Introduction", "text": "In the paradigm of online learning, a learning algorithm makes a sequence of predictions based on (possibly incomplete) knowledge of the correct answers to past questions. In contrast to statistical learning, online learning algorithms typically offer non-distributive guarantees, i.e. no environmental distribution assumption is made. Consequently, online learning algorithms are well suited to dynamic and contrarian environments where real-time learning from changing data is indispensable. While statistical (batch) learning algorithms provide a single predictor as output, an online learning algorithm must issue a predictor for each successive time step. Consequently, ensuring the differential privacy of each data point for online learning is a challenge, as changing the data point provided to the algorithm in the tenth step changes the prediction for all subsequent time steps. In this paper, we design differentiated private linear algorithms for both online optimization and full-time optimization."}, {"heading": "1.1 Full-Information Setting: Differential Privacy for Free", "text": "For the complete information environment, we design (\u03b5, \u03b4) -differentiated private algorithms with regrets scaling as O (\u221a T) + O (1 \u03b5 log 1 \u03b4), solving an open question posed in [TS13]. Dissecting the limit on regretting this form implies that we have no additional regrets, i.e. that differential privacy is free. Furthermore, even if we show a substantial regret per round compared to the empty, constant regret per round guaranteed by existing results. \u2022 namana @ cs.princeton.edu, Computer Science, Princeton University \u2020 karans @ cs.princeton.edu, Computer Science, Princeton University 1Here hides the O-notation polylog (T) factors.As a concrete example of making predictions from expert advice with N experts and T rounds of the game, we show that it is possible to achieve O-login."}, {"heading": "1.2 Bandit Feedback: Reduction to the Non-private Setting", "text": "An important case of the general linear optimization framework for bandits is the non-private problem from [AHR12], which positively answers a question from [TS13] whether O-remorse is achievable for differentiated private linear bandits. An important case of the general linear optimization framework for bandits is the non-stochastic, multi-armed bandit problem [BCB + 12], with applications for website optimization, personalized medicine, advertising placement, and recommendations for systems for general linear optimization of bandits (here: N-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-"}, {"heading": "2 Model and Preliminaries", "text": "This section introduces the model of online learning (linear) (T), the distinction between complete and partial feedback scenarios (T), and the concept of differential privacy in this model. < T (S), the concept of differential privacy in this model. < T (S), the concept of differential privacy in this model. < T (S), the concept of success of such an algorithm, is defined, although it is defined, asRegret (T) = 1 < lt > min x (KT), where the expectation goes beyond the randomness of the algorithm."}, {"heading": "3 FTPL-based Algorithms: Differential Privacy for Free", "text": "In this section, we outline algorithms based on the \"follow-the-perturbed-leader\" template [KV05]. FTPL-based algorithms cause little remorse by disrupting the cumulative sum of loss vectors with noise from an appropriately selected distribution. We show that the noise added in the process of FTPL is sufficient to ensure a differentiated privacy. Specifically, we note that the repentance guarantees have received a scale as O (\u221a T) + O (1\u03b5 log 1). Algorithm 1 FTPL template for OLO - A (D, T) on the action table X, the loss quantity Y. 1: Initializes an empty binary tree B to differentiate private estimates of t s = 1 ls.2: Example n10,. n: Example n."}, {"heading": "3.1 Proof of (\u03b5, \u03b4)-Differential Privacy", "text": "To make formal claims about the quality of privacy, we ensure that the input differential privacy is guaranteed for the algorithm - that is, we ensure that the order of all subtotals of the loss vectors (across all T rounds) (it: t = 1 ls: t [T]) is differentially private. Theorem 3.1 (Privacy Guarantees with Gaussian Noise) confirms that the order of decisions made by the algorithm (it: t = 1 ls: t [T]) is differentiated private.Theorem 3.1 (Privacy Guarantees with Gaussian Noise). Select any number of p \u00b2 Y \u00b2 2 \u00b2 2 log 2 T log2 log T\u03b4.When the algorithm 1 A (D, T) is executed with D = N (0, 2 IN), the following assertions apply: \u2022 Privacy: The order (L \u00b2 t: t \u00b2 is clearly defined) is different from the order of private processing (T)."}, {"heading": "3.2 Regret Bounds for FTPL", "text": "While we rely on the analysis of FTPL-based algorithms in the form of a general framework that takes into account different noise structures. (For a number of subsets of linear online optimization, with different X and Y, however, we have found that Gaussian noise achieves a minimax optimal distribution (up to multiplicative constants). We use these results from [ALST14] to prove the limits of linear optimization for algorithm 1. We make a few observations before applying these results. First, algorithm 1 calls the argmin oracle to the perturbed estimates of s = 1 ls, specifically L't, as these are distributed."}, {"heading": "4 Bandit Feedback: Reduction to the Non-private Setting", "text": "We start by describing an algorithmic reduction that is used as an input of a non-private bandit algorithm. & tt; / p > p > p > p > p = p > p = p = p = p > p = p > p = p = p = p = p = p = p = p = p = p = p."}, {"heading": "4.1 Differentially Private Bandit Linear Optimization", "text": "To get an answer to this question, we have to use the SCRiBLe algorithm of [AHR12] as the internal algorithm A. < p > p > p > p > p < p > p > p > p > p > p > p < p > p > p < p > p > p > p > p > p > p < p > p < p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p \"p\" p \"p\" p. \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p.\" p. \"p.\" p. \"p.\" \"p.\" p. \"p.\" p. \"p.\" p. \"p.\" p. \"p.\" p. \"p.\" p. \"p.\" p \"p.\" p. \"p.\" p. \"p.\" p. \"p\" p \"p.\" p. \"p\" p. \"p\" p. \"p.\" p \"p.\" p. \"p.\" p. \"p\" p. \"p.\" p. \"p.\" p \"p.\" p. \"p.\" p. \"p.\" p. \"p.\" p. \"p.\" p. \"p.\" p. \"p.\" p. \"p.\" p. \"p.\" p. \"p.\" p. \"p.\" p. \"p.\" p. \"p.\" p. \"p.\" p. \"p.\" p. \"p\" p. \"p.\" p. \"p.\" p. \"p.\" p. \"p.\" p. \"p.\" p. \"p.\" p. \"p.\" p. \"p.\" p. \"p.\" p. \"p.\" p. \"p.\" p. \"p.\" p. \"p.\" p. \"p.\" p. \"p.\" p. \"p.\" p. \"p.\" p. \"p.\" p. \"p.\" p. \""}, {"heading": "4.2 Differentially Private Multi-Armed Bandits", "text": "To begin with, we would like to point out that maxt, l & # 8222; Y & # 8222;. & # 8220; & # 10; & # 10; & # 10; & # 10; & # 10; & # 10; & # 10; & # 10; & # 10; & # 10; & # 10; & # 10; & # 10; & # 10; & # 10; & # 10; & # 10; & # 10; & # 10; & # 10; & # 10; & # 10; & # 10; & # 10; & # 10; & # 10; & # 10; & # 10; & # 10; & # 10; & # 10; & # 10; & # 10; & # 10; & # 10; & # 10; & # 10; & # 10; & # 10; & # 10; & # 10; & # 10; & # 10; & # 10; & # 10; & # 10; & # 10; & # 10; & # 10; & # 10; & # 10; & # 10; & # 10; & # 10; & # 10; & # 10; & # 10; & # 10; & # 10; & # 10; & # 10; & # 10; & # 10; & # 10; & # 10; & # 10; & # 10; & # 10; & # 10; & # 10; & # 10; & # 10; & # 10; & # 10; & # 10; & # 10; & # 10; & # 10; & # 10; & # 10; & # 10; & # 10; & # 10; & # 10; & # 10; & # 10; & # 10; & # 10; & # 10; & # 10; & # 10; & # 10; & # 10; & # 10; & # 10; & # 10; & # 10."}, {"heading": "A Noisy OCO Theorem Proof", "text": "We now give the proof for Theorem 4.2Evidence. The proof for the Lemma is a simple calculation. First, it can be established that E {Zt} [EA] [T \u00b2 t = 1 (< lt, x \u00b2 t > \u2212 < lt, x \u00b2 >)] = E {Zt} [EA [T \u00b2 t = 1 (< l \u00b2 t, x \u00b2 t > \u2212 < l \u00b2 t, x \u00b2 >)] + E {Zt} [EA [T \u00b2 t = 1 < l \u00b2 t = 1 < l \u00b2 t = 1 < l \u00b2 t, x \u00b2 >]] [EA [T \u00b2 t = 1 < t = 1 < lt; t = 1 < l; t = 1 < l; l \u00b2 t = 1 < l \u00b2 t = 1 < l \u00b2 T = 1 < T = 1 < T = 1 < T = 1 < < < < T = 1 < T = 1 < T = 1 < T = 1 < T = 1 < T = 1 < T = 1 < T = 1 < T = 1 < T = 1 < T = 1 < T = 1 < T = 1 < T = 1 < T = 1 < T = 1 < T = 1 < T = 1 < T = 1 < T = 1; T = 1 < T = 1 < T = 1 < T = 1 < T = 1 < T = 1; T = 1 < T = 1 < T = 1; T = 1 < T = 1; T = 1 < T = 1; T = 1 < T = 1; T = 1 < T = 1."}, {"heading": "B Facts about Norms of Gaussian Vectors", "text": "We prove the facts in case \u03c32 = 1. The versions of the generals follow immediately. Fact B.1. Leave Z \u0445 N (0, IN), then we have that E [\u0435Z \u0445 2 \u221e] \u2264 3 logN1 \u2212 N \u2212 1Proof. The proof is a simple application of the standard moment generating function trick. Note that for each i [N], Z (i) 2 is a \u03c72 random variable. The MGF for \u03c72 distribution is (1 \u2212 2s) \u2212 1 / 2. Therefore, we have this for all s \u2264 1 / 2esE [maxi Z (i) 2] \u2264 E [esmaxi Z (i) 2] Convexity of ex = E [max iesZ (i) 2] \u2264 1 [esZ (i) \u2212 2] \u2264 iE [esZ (i) 2] = N (1 \u2212 2s), inequality (i) 2] = N (1 \u2212 2s) that is fixed."}, {"heading": "C EXP2 with Exploration \u00b5", "text": "Algorithm 4 EXP2 with exploration \u00b5Input: learning rate \u03b7; mixing coefficient \u03b3; distribution \u00b5 over the catalogue of measures S1: q1 = (1% S,.. 1% S,.. 2: for t = 1,2... T do 3: Lets pt = (1 \u2212 \u03b3) qt + \u03b3\u00b5 and plays st \u00b2 pt 4: Estimated loss vector lt by l = P + t sts T t lt, with Pt = Ept [sts T] 5: Updating exponential weights, for all s \u00b2 S, qt + 1 (s) = e \u2212 \u03b7 < s, l \u00b2 t > qt (s) \u0445 s \u00b2 S e \u2212 \u03b7 < s \u00b2, l \u00b2 t > qt (s) 6: End for"}], "references": [{"title": "Competing in the dark: An efficient algorithm for bandit linear optimization", "author": ["Jacob Abernethy", "Elad Hazan", "Alexander Rakhlin"], "venue": "In COLT,", "citeRegEx": "Abernethy et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Abernethy et al\\.", "year": 2008}, {"title": "Interior-point methods for full-information and bandit online learning", "author": ["Jacob D Abernethy", "Elad Hazan", "Alexander Rakhlin"], "venue": "IEEE Transactions on Information Theory,", "citeRegEx": "Abernethy et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Abernethy et al\\.", "year": 2012}, {"title": "Online linear optimization via smoothing", "author": ["Jacob Abernethy", "Chansoo Lee", "Abhinav Sinha", "Ambuj Tewari"], "venue": "In COLT,", "citeRegEx": "Abernethy et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Abernethy et al\\.", "year": 2014}, {"title": "Towards minimax policies for online linear optimization with bandit feedback", "author": ["S\u00e9bastien Bubeck", "Nicolo Cesa-Bianchi", "Sham M Kakade", "Shie Mannor", "Nathan Srebro", "Robert C Williamson"], "venue": "In COLT,", "citeRegEx": "Bubeck et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Bubeck et al\\.", "year": 2012}, {"title": "Calibrating noise to sensitivity in private data analysis", "author": ["Cynthia Dwork", "Frank McSherry", "Kobbi Nissim", "Adam Smith"], "venue": "In Theory of Cryptography Conference,", "citeRegEx": "Dwork et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Dwork et al\\.", "year": 2006}, {"title": "Adaptive online prediction by following the perturbed leader", "author": ["Marcus Hutter", "Jan Poland"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Hutter and Poland.,? \\Q2005\\E", "shortCiteRegEx": "Hutter and Poland.", "year": 2005}, {"title": "Differentially private online learning", "author": ["Prateek Jain", "Pravesh Kothari", "Abhradeep Thakurta"], "venue": "In COLT,", "citeRegEx": "Jain et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Jain et al\\.", "year": 2012}, {"title": "Efficient algorithms for online decision problems", "author": ["Adam Kalai", "Santosh Vempala"], "venue": "Journal of Computer and System Sciences,", "citeRegEx": "Kalai and Vempala.,? \\Q2005\\E", "shortCiteRegEx": "Kalai and Vempala.", "year": 2005}, {"title": "Online learning and online convex optimization", "author": ["Shai Shalev-Shwartz"], "venue": "Foundations and Trends in Machine Learning,", "citeRegEx": "Shalev.Shwartz.,? \\Q2011\\E", "shortCiteRegEx": "Shalev.Shwartz.", "year": 2011}, {"title": "nearly) optimal algorithms for private online learning in full-information and bandit settings", "author": ["Abhradeep Guha Thakurta", "Adam Smith"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Thakurta and Smith.,? \\Q2013\\E", "shortCiteRegEx": "Thakurta and Smith.", "year": 2013}], "referenceMentions": [], "year": 2017, "abstractText": "We design differentially private algorithms for the problem of online linear optimization in the full information and bandit settings with optimal \u00d5( \u221a T ) regret bounds. In the full-information setting, our results demonstrate that (\u03b5, \u03b4)-differential privacy may be ensured for free \u2013 in particular, the regret bounds scale as O( \u221a T ) + \u00d5 ( 1 \u03b5 log 1 \u03b4 ) . For bandit linear optimization, and as a special case, for non-stochastic multi-armed bandits, the proposed algorithm achieves a regret of O (\u221a T log T \u03b5 log 1 \u03b4 ) , while the previously best known bound was \u00d5 (", "creator": "LaTeX with hyperref package"}}}