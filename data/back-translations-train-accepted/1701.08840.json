{"id": "1701.08840", "review": {"conference": "AAAI", "VERSION": "v1", "DATE_OF_SUBMISSION": "30-Jan-2017", "title": "Spatial Projection of Multiple Climate Variables Using Hierarchical Multitask Learning", "abstract": "Future projection of climate is typically obtained by combining outputs from multiple Earth System Models (ESMs) for several climate variables such as temperature and precipitation. While IPCC has traditionally used a simple model output average, recent work has illustrated potential advantages of using a multitask learning (MTL) framework for projections of individual climate variables. In this paper we introduce a framework for hierarchical multitask learning (HMTL) with two levels of tasks such that each super-task, i.e., task at the top level, is itself a multitask learning problem over sub-tasks. For climate projections, each super-task focuses on projections of specific climate variables spatially using an MTL formulation. For the proposed HMTL approach, a group lasso regularization is added to couple parameters across the super-tasks, which in the climate context helps exploit relationships among the behavior of different climate variables at a given spatial location. We show that some recent works on MTL based on learning task dependency structures can be viewed as special cases of HMTL. Experiments on synthetic and real climate data show that HMTL produces better results than decoupled MTL methods applied separately on the super-tasks and HMTL significantly outperforms baselines for climate projection.", "histories": [["v1", "Mon, 30 Jan 2017 21:56:18 GMT  (3279kb,D)", "http://arxiv.org/abs/1701.08840v1", "Accepted for the 31st AAAI Conference on Artificial Intelligence (AAAI-17)"]], "COMMENTS": "Accepted for the 31st AAAI Conference on Artificial Intelligence (AAAI-17)", "reviews": [], "SUBJECTS": "cs.LG stat.ML", "authors": ["andr\u00e9 ricardo gon\u00e7alves", "arindam banerjee", "fernando j von zuben"], "accepted": true, "id": "1701.08840"}, "pdf": {"name": "1701.08840.pdf", "metadata": {"source": "CRF", "title": "Spatial Projection of Multiple Climate Variables Using Hierarchical Multitask Learning", "authors": ["Andr\u00e9 R. Gon\u00e7alves", "Arindam Banerjee", "Fernando J. Von Zuben"], "emails": ["{andre@cs.umn.edu,", "banerjee@cs.umn.edu,", "vonzuben@dca.fee.unicamp.br}"], "sections": [{"heading": null, "text": "Keywords - Multitask Learning, Structure Learning, Spatial Regression, Structured Regression, Earth System Models Ensemble."}, {"heading": "1 Introduction", "text": "This year it is more than ever before."}, {"heading": "2 Multitask Learning with Task Dependence Estimation", "text": "Characteristics between tasks (series of the parameter matrix) were assumed to be derived from a multivariate Gaussian distribution; the task relationship is then encoded in the inverse of the covariance matrix \u03a3 \u2212 1 =, also known as the precision matrix. Thrift is desired in such a matrix, since zero entries of the precision matrix indicate a conditional independence between the corresponding two random variables (tasks) (Friedman et al., 2008). The associated learning problem (1) consists in the common estimation of the task parameters and the precision matrix, which is carried out by an alternating optimization process. Min."}, {"heading": "3 Mathematical Formulation of ESMs Climate Projection", "text": "A common method of projection is to combine several ESMs in the smallest square sense, i.e. to estimate a set of weights for the ESMs based on past observations.For a given location k, the predicted climate variable (temperature, for example) for a given timestamp i (expected mean temperature for a given month / year) is given by the following yardsticks: y-ik = < xik, \u03b8k > + ik (2), where xik is the value predicted by the ESMs for the k-th place in the timestamp i, \u03b8k is the weight vector of each ESM for the k-th place, and ik is a residual value. The weight vector successk is estimated from training data. the combined estimate y-ik is then used as a more robust prediction of temperature for the k-place in a given month / year in the future."}, {"heading": "4 The HMTL Formulation", "text": "The HMTL formulation aims to minimise the following cost function: the following cost function C (land use plan) with land use plan (land use plan), land use plan (land use plan), land use plan (land use plan), land use plan (land use plan), land use plan (land use plan), land use plan (land use plan), land use plan (land use plan), land use plan (land use plan), land use plan (land use plan), land use plan (land use plan), land use plan (land use plan), land use plan (land use plan), land use plan (land use plan), land use plan (land use plan), (land use plan), (land use plan), (land use plan), (land use plan), (land use plan), (land use plan), (land use plan), (land use plan), (land use plan), (land use plan), (land use plan), (land use plan), (land use plan), land use plan (land use plan), land use plan (), land use plan (land use plan), land use plan (land use plan), land use plan (land use plan), land use plan, land use plan (land use plan), land use plan (land use plan), land use plan (land use plan), land use plan (land use plan), land use plan (land use plan), land use plan, land use plan), land use plan (land use plan, land use plan), land use plan (land use plan), land use plan (land use plan), land use plan (land use plan), land use plan (land use plan), land use plan (), land use plan ()"}, {"heading": "4.1 Optimization", "text": "The optimization problem (3) is not jointly convex to {\u0443} and {\u0430}, especially due to the trace term that includes both variables. We then use alternate minimization, where {\u043a} is fixed and optimized for {\u0430} (we call it \"step\") and similarly fixed and optimized for {\u27e9} (we call it \"step\"), both steps consisting of convex problems for which efficient methods were proposed. In the experiments, 20 to 30 iterations were required for convergence."}, {"heading": "4.1.1 Solving \u0398-step", "text": "The convex problem associated with this step is defined as asmin {\u044b} T \u2211 t = 1 mk \u2211 k = 1 L (X (t, k) \u03b8 (t, k), y (t, k))) + \u03bb0tr (S (t) \u0432 (t))). (5) Considering the quadratic loss function, this step consists of two quadratic terms, since these are positive semi-defined matrices. Note that the optimization for each super task weight matrix \u0443 (t) is independent and can be performed in parallel. We used the L-BFGS method in the experiments (Liu and Nocedal, 1989)."}, {"heading": "4.1.2 Solving \u2126-step", "text": "The next step is to solve the following optimization problem: (\u2212 log (t), (t), (t), (t), (t), (t), (t), (t), (t), (t), (t) 0, (t), (..., T. (6) This step corresponds to the problem of joint learning of multiple Gaussian graphic models and has recently been investigated (Honorio and Samaras, 2010; Danaher et al., 2014; Mohan et al., 2014). These formulations aim to minimize the likelihood of the punishable negative protocol in the form of (6), and they differ substantially in the penal expression R (\u00b2). Researchers have shown that the jointly estimated graphic models were able to increase the number of correctly identified edges and at the same time reduce the number of incorrectly identified edges when compared with the independently estimated ones. \u2212 An alternating directional method of multipliers (MADM) suggested."}, {"heading": "5 Related Works", "text": "A grid diagram is used to represent the regional relationship in (Subbian and Banerjee, 2013), where it is assumed that immediate neighboring sites have similar ESM weights. Weights for all geolocations are jointly estimated (in the least square sense) with a klaplatic regularization to promote spatial smoothness. In (McQuade and Monteleoni, 2013), the online ESM ensemble problem is addressed by using a grid Markov Random Field. The state of each hidden variable associated with a geographical location is the identity of the best ESM for that specific location. The marginal probabilities of the hidden variables act as weights of the ensemble. Therefore, ESMs are more likely to be the best, have a greater weight within the ensemble. At each step, marginal probabilities based on the performance of the ESMs in the previous time step (SG weights) and multiplicity algorithms are updated."}, {"heading": "6 Experiments", "text": "In this section we present experiments to compare the proposed HMTL with existing methods in the literature for both synthetic and real climate data."}, {"heading": "6.1 Synthetic Data", "text": "For comparison, we used the MTL method proposed in (Gonc, alves et al., 2014, 2016), called MSSL, which has proven to be competitive with existing MTL algorithms, including (Zhang and Yeung, 2010; Kumar and Daume III, 2012; Kang et al., 2011).For this analysis, we created 7 supertasks with 15 subtasks each with a dimension of 50. For each subtask, 100 data samples were generated that proved to be valuable. Inverse covariance matrices were derived from a Wishart distribution with a scale matrix and n = 10 degrees of freedom. Scale matrix was designed to reflect a structure with three groups of related variables."}, {"heading": "6.2 Climate Data", "text": "We collected monthly land temperature and precipitation data from 32 CMIP5 ESMs (Taylor et al., 2012) from 1901 to 2000 in South America. Observed data from (Willmott and Matsuura, 2001) were used. ESM's predictions and observational values from 250 locations in South America (raster distribution) were taken into account. From the perspective of HMTL, the problem includes: two supertasks, 250 subtasks (per supertask) with a dimension of 32. In the climate area, it is customary to work with the relative measurement of the climate variable to a reference value derived from previous information. In our experiments, we work directly on the raw data (non-detrended). We investigate the performance of the algorithm in both seasonal and annual time scales, focusing on winter and summer. All ESMs and observed data are located at the same time and spatial resolution. Temperature is in degrees Celsius and rainfall in cm."}, {"heading": "6.2.1 Experimental Setup:", "text": "Based on climate data from a particular past (training) period, model parameters are estimated and the conclusion is based on its predictions for the future (test). The duration of the training period clearly influences the performance of the algorithm. A moving window of 20, 30 and 50 years was used for training and the next 10 years for testing. Performance is measured in the form of a root-mean-square error (RMSE).Seasonality strongly influences climate data analysis. For example, winter and summer precipitation patterns are different. Seasonal data also makes it easier to identify anomalies.1Relative improvement results from the difference between MSSL and HMTL performance (RMSE) divided by MSSL performance as percentiles (%).Patterns that may be useful to characterize climate phenomena as El Nin o. We extracted summer and winter data and conducted climate variable predictions specifically for these seasons.Five baseline algorithms (multimodel algorithms) were taken into account."}, {"heading": "6.2.2 Results:", "text": "Table 3 shows the RMSE of the projections made by the algorithms and the observed values for precipitation and temperature. Firstly, we note that the simple allocation of equal weights to all ESMs does not seem to exploit the potential of the ensemble methods. Secondly, the MTL methods, MSSL and HMTL, clearly exceed the basic methods. S2M2R does not always provide better predictions than OLS. In fact, it is slightly worse for the annual dataset. As expected, the assumption of spatial neighbourhood dependence does not seem to hold for all climatic fluctuations. HMTL presented better results than the performance of MSSL for precipitation and temperature fluctuations independently in many situations. HMTL was able to significantly reduce the predictive power of precipitation projections."}, {"heading": "7 Concluding Remarks", "text": "The formulation enables two levels of information exchange: (1) model parameters (coefficients of linear regression) and (2) precision matrices that encode the relationship of linear regressors. Group lasso regularization is responsible for the acquisition of similar splinter patterns across multiple precision matrices. Experiments on joint projection of temperature and precipitation in South America showed that in many situations, the HMTL provided more accurate predictions compared to the independent execution of existing MTL methods for each climate variable. Simulations of synthetic datasets also showed that the proposed HMTL achieved higher performance as the number of internal MTL problems increased. Here, only temperature and precipitation were used as they are two of the most studied climate variables."}, {"heading": "8 Acknowledgments", "text": "We thank the anonymous critics for their valuable comments. AB was supported by NSF grants IIS-1563950, IIS-1447566, IIS-1447574, IIS-1422557, CCF-1451986, CNS-1314560, IIS-0953274, IIS-1029711, NASA grants NNX12AQ39A, and gifts from Adobe, IBM, and Yahoo. FJVZ thanks CNPq for its financial support (Process No. 309115 / 2014-0)."}], "references": [{"title": "The joint graphical lasso for inverse covariance", "author": ["P. Danaher", "P. Wang", "D.M. Witten"], "venue": null, "citeRegEx": "Danaher et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Danaher et al\\.", "year": 2014}, {"title": "Sparse inverse covariance estimation", "author": ["J. Friedman", "T. Hastie", "R. Tibshirani"], "venue": "J. R. Stat. Soc. Series B Stat. Methodol.,", "citeRegEx": "Friedman et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Friedman et al\\.", "year": 2008}, {"title": "Multi-task Sparse Structure Learning", "author": ["A.R. Gon\u00e7alves", "F.J. Von Zuben", "A. Banerjee"], "venue": "In ACM CIKM ,", "citeRegEx": "Gon\u00e7alves et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Gon\u00e7alves et al\\.", "year": 2016}, {"title": "Intergovernmental Panel on Climate Change Fifth Assessment", "author": ["Z. Kang", "K. Grauman", "F. Sha"], "venue": "IPCC", "citeRegEx": "Kang et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Kang et al\\.", "year": 2013}, {"title": "Learning task grouping and overlap in multi-task learning", "author": ["A. Kumar", "H. Daume III"], "venue": null, "citeRegEx": "Kumar and III,? \\Q2012\\E", "shortCiteRegEx": "Kumar and III", "year": 2012}, {"title": "On the limited memory BFGS method for large scale optimization", "author": ["D.C. Liu", "J. Nocedal"], "venue": "Mathematical Programming ,", "citeRegEx": "Liu and Nocedal,? \\Q1989\\E", "shortCiteRegEx": "Liu and Nocedal", "year": 1989}, {"title": "MRF-Based Spatial Expert Tracking of the Multi-Model Ensemble", "author": ["S. McQuade", "C. Monteleoni"], "venue": "In International Workshop on Climate Informatics", "citeRegEx": "McQuade and Monteleoni,? \\Q2013\\E", "shortCiteRegEx": "McQuade and Monteleoni", "year": 2013}, {"title": "High-dimensional graphs and variable selection with the lasso", "author": ["N. Meinshausen", "P. Buhlmann"], "venue": "Annals of Statistics,", "citeRegEx": "Meinshausen and Buhlmann,? \\Q2006\\E", "shortCiteRegEx": "Meinshausen and Buhlmann", "year": 2006}, {"title": "Node-based learning of multiple Gaussian graphical models", "author": ["K. Mohan", "P. London", "M. Fazel", "D. Witten", "Lee", "S.-I"], "venue": null, "citeRegEx": "Mohan et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Mohan et al\\.", "year": 2014}, {"title": "Addressing interdependency in a multimodel ensemble by interpolation of model properties", "author": ["B.M. Sanderson", "R. Knutti", "P. Caldwell"], "venue": "J. of Climate,", "citeRegEx": "Sanderson et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Sanderson et al\\.", "year": 2015}, {"title": "Climate Multi-model Regression using Spatial Smoothing", "author": ["K. Subbian", "A. Banerjee"], "venue": "In SIAM SDM ,", "citeRegEx": "Subbian and Banerjee,? \\Q2013\\E", "shortCiteRegEx": "Subbian and Banerjee", "year": 2013}, {"title": "An overview of CMIP5 and the experiment design", "author": ["K. Taylor", "R. Stouffer", "G. Meehl"], "venue": "Bull. of the Am. Met. Soc.,", "citeRegEx": "Taylor et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Taylor et al\\.", "year": 2012}, {"title": "The use of the multi-model ensemble in probabilistic climate projections", "author": ["C. Tebaldi", "R. Knutti"], "venue": "Phil. Trans. R. Soc. A,", "citeRegEx": "Tebaldi and Knutti,? \\Q2007\\E", "shortCiteRegEx": "Tebaldi and Knutti", "year": 2007}, {"title": "Terrestrial Air Temperature and Precipitation: Monthly and Annual Time Series (1950", "author": ["C.J. Willmott", "K. Matsuura"], "venue": null, "citeRegEx": "Willmott and Matsuura,? \\Q2001\\E", "shortCiteRegEx": "Willmott and Matsuura", "year": 2001}, {"title": "Multi-task learning with Gaussian matrix generalized inverse Gaussian model", "author": ["M. Yang", "Y. Li", "Z. Zhang"], "venue": "In ICML,", "citeRegEx": "Yang et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Yang et al\\.", "year": 2013}, {"title": "Model selection and estimation in regression with grouped variables", "author": ["M. Yuan", "Y. Lin"], "venue": "J. R. Stat. Soc. Series B Stat. Methodol.,", "citeRegEx": "Yuan and Lin,? \\Q2006\\E", "shortCiteRegEx": "Yuan and Lin", "year": 2006}, {"title": "Learning multiple tasks with sparse matrix-normal penalty", "author": ["Y. Zhang", "J. Schneider"], "venue": null, "citeRegEx": "Zhang and Schneider,? \\Q2010\\E", "shortCiteRegEx": "Zhang and Schneider", "year": 2010}, {"title": "A convex formulation for learning task relationships in multitask learning", "author": ["Y. Zhang", "Yeung", "D.-Y"], "venue": "In UAI ,", "citeRegEx": "Zhang et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Zhang et al\\.", "year": 2010}], "referenceMentions": [{"referenceID": 12, "context": "atmosphere, oceans, land, and sea ice (Tebaldi and Knutti, 2007; IPCC, 2013).", "startOffset": 38, "endOffset": 76}, {"referenceID": 12, "context": "A single and possibly more robust projection can be built as a combination (ensemble) of multiple ESMs simulations (Tebaldi and Knutti, 2007; McQuade and Monteleoni, 2013; Sanderson et al., 2015).", "startOffset": 115, "endOffset": 195}, {"referenceID": 6, "context": "A single and possibly more robust projection can be built as a combination (ensemble) of multiple ESMs simulations (Tebaldi and Knutti, 2007; McQuade and Monteleoni, 2013; Sanderson et al., 2015).", "startOffset": 115, "endOffset": 195}, {"referenceID": 9, "context": "A single and possibly more robust projection can be built as a combination (ensemble) of multiple ESMs simulations (Tebaldi and Knutti, 2007; McQuade and Monteleoni, 2013; Sanderson et al., 2015).", "startOffset": 115, "endOffset": 195}, {"referenceID": 16, "context": "Modeling task relationship in multitask learning has been the focus of recent research (Zhang and Schneider, 2010; Zhang and Yeung, 2010; Yang et al., 2013; Gon\u00e7alves et al., 2016).", "startOffset": 87, "endOffset": 180}, {"referenceID": 14, "context": "Modeling task relationship in multitask learning has been the focus of recent research (Zhang and Schneider, 2010; Zhang and Yeung, 2010; Yang et al., 2013; Gon\u00e7alves et al., 2016).", "startOffset": 87, "endOffset": 180}, {"referenceID": 2, "context": "Modeling task relationship in multitask learning has been the focus of recent research (Zhang and Schneider, 2010; Zhang and Yeung, 2010; Yang et al., 2013; Gon\u00e7alves et al., 2016).", "startOffset": 87, "endOffset": 180}, {"referenceID": 7, "context": "The problem of estimating statistical dependency structure of a set of random variables is known as structure learning (Meinshausen and Buhlmann, 2006).", "startOffset": 119, "endOffset": 151}, {"referenceID": 2, "context": "In the MTL case (Gon\u00e7alves et al., 2016), the random variables are tasks parameters and, depending on the ratio between dimensionality and the number of tasks, the amount of data samples may not be sufficient.", "startOffset": 16, "endOffset": 40}, {"referenceID": 1, "context": "Sparsity is desired in such matrix, as zero entries of the precision matrix indicate conditional independence between the corresponding two random variables (tasks) (Friedman et al., 2008).", "startOffset": 165, "endOffset": 188}, {"referenceID": 1, "context": "The problem associated with Step 2, known as sparse inverse covariance selection problem (Friedman et al., 2008), seeks to find some sparsity pattern in the precision matrix.", "startOffset": 89, "endOffset": 112}, {"referenceID": 16, "context": "Experimental analysis have shown that these approaches usually outperform MTL with pre-defined task dependency structure for a variety of problems (Zhang and Schneider, 2010; Gon\u00e7alves et al., 2014).", "startOffset": 147, "endOffset": 198}, {"referenceID": 16, "context": "For simplicity, we dropped the `1-penalization on the weight matrix \u0398 as is often done in MTL (Zhang and Schneider, 2010; Yang et al., 2013; Gon\u00e7alves et al., 2016).", "startOffset": 94, "endOffset": 164}, {"referenceID": 14, "context": "For simplicity, we dropped the `1-penalization on the weight matrix \u0398 as is often done in MTL (Zhang and Schneider, 2010; Yang et al., 2013; Gon\u00e7alves et al., 2016).", "startOffset": 94, "endOffset": 164}, {"referenceID": 2, "context": "For simplicity, we dropped the `1-penalization on the weight matrix \u0398 as is often done in MTL (Zhang and Schneider, 2010; Yang et al., 2013; Gon\u00e7alves et al., 2016).", "startOffset": 94, "endOffset": 164}, {"referenceID": 15, "context": "Here, we focus on the group lasso penalty (Yuan and Lin, 2006), which we denote by RG, and is defined as", "startOffset": 42, "endOffset": 62}, {"referenceID": 5, "context": "We used the L-BFGS (Liu and Nocedal, 1989) method in the experiments.", "startOffset": 19, "endOffset": 42}, {"referenceID": 0, "context": "This step corresponds to the problem of joint learning multiple Gaussian graphical models and has been recently studied (Honorio and Samaras, 2010; Danaher et al., 2014; Mohan et al., 2014).", "startOffset": 120, "endOffset": 189}, {"referenceID": 8, "context": "This step corresponds to the problem of joint learning multiple Gaussian graphical models and has been recently studied (Honorio and Samaras, 2010; Danaher et al., 2014; Mohan et al., 2014).", "startOffset": 120, "endOffset": 189}, {"referenceID": 0, "context": "An alternating direction method of multipliers (ADMM) proposed in (Danaher et al., 2014) was used to solve problem (6).", "startOffset": 66, "endOffset": 88}, {"referenceID": 10, "context": "A lattice graph is used to represent the regional relationship in (Subbian and Banerjee, 2013), where immediate neighbor locations are assumed to have similar ESMs weights.", "startOffset": 66, "endOffset": 94}, {"referenceID": 6, "context": "In (McQuade and Monteleoni, 2013) the online ESMs ensemble problem is tackled by using a lattice Markov Random Field.", "startOffset": 3, "endOffset": 33}, {"referenceID": 11, "context": "We collected monthly land temperature and precipitation data of 32 CMIP5 ESMs (Taylor et al., 2012), from 1901 to 2000, in South America.", "startOffset": 78, "endOffset": 99}, {"referenceID": 13, "context": "Observed data provided by (Willmott and Matsuura, 2001) was used.", "startOffset": 26, "endOffset": 55}, {"referenceID": 10, "context": "S2M2R (Subbian and Banerjee, 2013): can be seen as an MTL method with pre-defined location dependence given by the graph Laplacian.", "startOffset": 6, "endOffset": 34}], "year": 2017, "abstractText": "Future projection of climate is typically obtained by combining outputs from multiple Earth System Models (ESMs) for several climate variables such as temperature and precipitation. While IPCC has traditionally used a simple model output average, recent work has illustrated potential advantages of using a multitask learning (MTL) framework for projections of individual climate variables. In this paper we introduce a framework for hierarchical multitask learning (HMTL) with two levels of tasks such that each super-task, i.e., task at the top level, is itself a multitask learning problem over sub-tasks. For climate projections, each super-task focuses on projections of specific climate variables spatially using an MTL formulation. For the proposed HMTL approach, a group lasso regularization is added to couple parameters across the supertasks, which in the climate context helps exploit relationships among the behavior of different climate variables at a given spatial location. We show that some recent works on MTL based on learning task dependency structures can be viewed as special cases of HMTL. Experiments on synthetic and real climate data show that HMTL produces better results than decoupled MTL methods applied separately on the super-tasks and HMTL significantly outperforms baselines for climate projection. Keywords\u2014 Multitask Learning, Structure Learning, Spatial Regression, Structured Regression, Earth System Models Ensemble.", "creator": "LaTeX with hyperref package"}}}