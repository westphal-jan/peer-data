{"id": "1702.00071", "review": {"conference": "ICML", "VERSION": "v1", "DATE_OF_SUBMISSION": "31-Jan-2017", "title": "On orthogonality and learning recurrent networks with long term dependencies", "abstract": "It is well known that it is challenging to train deep neural networks and recurrent neural networks for tasks that exhibit long term dependencies. The vanishing or exploding gradient problem is a well known issue associated with these challenges. One approach to addressing vanishing and exploding gradients is to use either soft or hard constraints on weight matrices so as to encourage or enforce orthogonality. Orthogonal matrices preserve gradient norm during backpropagation and can therefore be a desirable property; however, we find that hard constraints on orthogonality can negatively affect the speed of convergence and model performance. This paper explores the issues of optimization convergence, speed and gradient stability using a variety of different methods for encouraging or enforcing orthogonality. In particular we propose a weight matrix factorization and parameterization strategy through which we can bound matrix norms and therein control the degree of expansivity induced during backpropagation.", "histories": [["v1", "Tue, 31 Jan 2017 22:14:59 GMT  (5131kb,D)", "http://arxiv.org/abs/1702.00071v1", null], ["v2", "Fri, 3 Mar 2017 11:09:28 GMT  (4031kb,D)", "http://arxiv.org/abs/1702.00071v2", null], ["v3", "Mon, 12 Jun 2017 23:12:14 GMT  (7139kb,D)", "http://arxiv.org/abs/1702.00071v3", null], ["v4", "Thu, 12 Oct 2017 17:18:51 GMT  (8317kb,D)", "http://arxiv.org/abs/1702.00071v4", null]], "reviews": [], "SUBJECTS": "cs.LG cs.NE", "authors": ["eugene vorontsov", "chiheb trabelsi", "samuel kadoury", "chris pal"], "accepted": true, "id": "1702.00071"}, "pdf": {"name": "1702.00071.pdf", "metadata": {"source": "CRF", "title": "ON ORTHOGONALITY AND LEARNING RECURRENT NETWORKS WITH LONG TERM DEPENDENCIES", "authors": ["Eugene Vorontsov", "Chiheb Trabelsi", "Samuel Kadoury", "Chris Pal"], "emails": ["christopher.pal}@polymtl.ca"], "sections": [{"heading": "1 INTRODUCTION", "text": "This year, it is closer than ever before to a major U-turn."}, {"heading": "1.1 VANISHING AND EXPLODING GRADIENTS", "text": "The question of disappearing and exploding gradients, as it relates to the parameterization of neural networks, can be illuminated by looking at the sequence backwards through a network.A neural network with n hidden layers has pre-activationsai (hi \u2212 1) = Wi-1 + bi, i-2, \u00b7 \u00b7, n} (1) For notational convenience we combine the parameters Wi and bi to form an affine matrix. We can see that for some loss functions L at layer n, the derivative in relation to the parameters scu-2, the derivative in relation to the parameters scu-3, the derivatives in relation to the parameters scu-3, the derivatives in relation to the parameters scu-3, the derivatives in relation to the parameters scu-3, the scu-3, the scu-points in relation to the scu-3, the scu-points in relation to the scu-3, the scu-points in relation to the scu-3, the scu-points in relation to the scu-3, the scu-points in relation to the scu-3, the scu-points in relation to the scu-3, the scu-3, the scu-points in relation to the scu-3, the scu-3-scu-scu points in relation to the scu-3, the scu-scu points in relation to the"}, {"heading": "2 OUR APPROACH", "text": "It is a question of the extent to which people are able to survive themselves, and of the extent to which they are able to survive themselves. (...) It is a question of the extent to which they are able to survive themselves. (...) It is a question of the extent to which they are able to survive themselves. (...) It is a question of the extent to which they are able to survive themselves. (...) It is a question of the extent to which they are able to survive themselves. (...) It is a question of the extent to which they are able to survive themselves. (...) It is a question of the extent to which they are able to survive themselves. (...)"}, {"heading": "3 EXPERIMENTS", "text": "In this section, we will explore hard and soft orthogonal constraints on factorized weight matrices for recursive neural networks that hide in hidden transitions. With hard orthogonal constraints on U and V, we will investigate the effects of extending the spectral range or limits on convergence and performance. Loosening these boundaries allows us to have ever greater margins in which the transition matrix W can deviate from orthogonomy. We confirm that orthogonal initialization is useful, as in Henaff, and we show that although strict orthogonal structures guarantee stable gradient standards, orthogonality can deviate from the conditions in which people live. We will begin to design the tasks that are designed: a sequential set of tasks that are tailored to a basic task (high rider and a basic additional task)."}, {"heading": "3.1 LOOSENING HARD ORTHOGONALITY CONSTRAINTS", "text": "In this section, we will experimentally examine the effects of relaxing hard orthogonal constraints by loosening the spectral range defined above for the hidden transition matrix. In all experiments, we used RMSprop (Tieleman & Hinton, 2012) without geodetic gradient descent. We used size 50 minibatches and for generated data (copying and adding tasks), we assumed an epoch length of 100 minibatches. In all of our RNN experiments, we cautiously introduced the gradient truncation of strength 100 (unless otherwise specified), although it might not be necessary, and consistently applied a small weight drop of 0.0001. Unless otherwise specified, we trained all simple recursive neural networks with the hidden to hidden factorization of the matrix as in (8) using geodesirable gradient pedigree on the basics (learning rate 10 \u2212 6) and learning rate M00S00p on the other parameters."}, {"heading": "3.1.1 CONVERGENCE ON SYNTHETIC MEMORY TASKS", "text": "For different sequence lengths T of the copy and addition tasks, we trained a factored RNN with 128 hidden units and different spectral margins m. For the copy task, we used Elman networks without transition nonlinearity as in Henaff et al. (2016). We discuss our investigations on the use of nonlinearity in the copy task in the appendix. As shown in Figure 1, we see an increase in the convergence rate as the spectral margin increases. This observation generally applies to the tested sequence lengths (T = 200, T = 500, T = 1000, T = 10000); however, large spectral margins imen the convergence at extremely long sequence lengths. At sequence lengths T = 10000, parameterizations with spectral ranges greater than 0.001 convergence apply slower than when using a margin of 0.001. Furthermore, the experiment without margin missed the convergence of the longest sequence lengths."}, {"heading": "3.1.2 PERFORMANCE ON REAL DATA", "text": "It is only a matter of time before it will happen, until it will happen."}, {"heading": "3.1.3 SPECTRAL AND GRADIENT EVOLUTION", "text": "This year, it is closer than ever before in the history of the country."}, {"heading": "3.2 EXPLORING SOFT ORTHOGONALITY CONSTRAINTS", "text": "Once we have established that it can indeed be useful to move away from orthogonality, we examine here two forms of soft constraints (rather than hard boundaries as above) on hidden, hidden transition matrix orthogonality. The first is a simple penalty that directly encourages a transition matrix W to be orthogonal, the form \u03bb | WTW \u2212 I | | 22. This is similar to the orthogonality penalty introduced by Henaff et al. (2016). In the first two sub-figures on the left of Figure 4, we examine the effects of the attenuation of this form of regulation. We have both a regular non-factored RNN on the T = 200 copy task and a factored RNN on the right side of Figure 4. For regular RNN, we had to reduce the learning rate to 10 \u2212 5."}, {"heading": "4 CONCLUSIONS", "text": "We have studied a number of methods to control the expansivity of gradients during reverse propagation-based learning in RNNs by manipulating orthogonal constraints and matrix regulation. Indeed, our experiments suggest that while orthogonal initialization can be beneficial, maintaining orthogonal constraints can be detrimental. Indeed, moving away from hard constraints on matrix orthogonality can help to improve optimization convergence rate and performance, but even in synthetic tasks, we observe that loose regulation that encourages the spectral norms of weight matrices to be close to one, or allows the limits of spectral norms of weight matrices to be too broad, reversing these gains and leading to unstable optimizations."}, {"heading": "ACKNOWLEDGMENTS", "text": "We thank the Natural Sciences and Engineeering Research Council (NSERC) of Canada and Samsung for supporting this research."}, {"heading": "5 APPENDIX", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "5.1 ADDITIONAL FIGURES", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "5.2 COPY TASK NONLINEARITY", "text": "We found that non-linearities such as a linear rectilinear unit (ReLU) (Nair & Hinton, 2010) or a hyperbolic tangent (tanh) made the copying task much more difficult to solve. It is worth noting that the use of a short sequence length (T = 100) required both a soft constraint that promotes orthogonality and thousands of epochs for training. It is worth noting that in unitary evolution, a recursive neural network of Arjovsky et al. (2015) actually initializes non-linearity (referred to as \"modReLU\") as an identity operation that can depart freely from identity during training. Furthermore, Henaff et al. (2016) derive a solution mechanism for the copying task that drops the non-linearity of an RNN. To further explore this, we experimented with a parametric leactivity called ReLU activation function (PLU), which yields a incrementable of 0.1."}, {"heading": "5.3 METHOD RUNNING TIME", "text": "Although the method proposed in Section 2 is based on a matrix inversion, an operation with O (n3) complexity for an n \u00b7 n matrix, the runtime of such a factored RNN actually remains reasonable. This runtime is summarized in Table 5 and includes all calculations in the graph, along with the matrix inversion. As this method is to be used only for analysis in this thesis, we find the runtimes acceptable for this purpose. Models were executed on an Nvidia GTX-770 GPU and executed against the T = 100 copy task."}], "references": [{"title": "Unitary evolution recurrent neural networks", "author": ["Martin Arjovsky", "Amar Shah", "Yoshua Bengio"], "venue": "arXiv preprint arXiv:1511.06464,", "citeRegEx": "Arjovsky et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Arjovsky et al\\.", "year": 2015}, {"title": "Norm-preserving orthogonal permutation linear unit activation functions (oplu)", "author": ["Artem Chernodub", "Dimitri Nowicki"], "venue": "arXiv preprint arXiv:1604.02313,", "citeRegEx": "Chernodub and Nowicki.,? \\Q2016\\E", "shortCiteRegEx": "Chernodub and Nowicki.", "year": 2016}, {"title": "Understanding the difficulty of training deep feedforward neural networks", "author": ["Xavier Glorot", "Yoshua Bengio"], "venue": "In Aistats,", "citeRegEx": "Glorot and Bengio.,? \\Q2010\\E", "shortCiteRegEx": "Glorot and Bengio.", "year": 2010}, {"title": "Delving deep into rectifiers: Surpassing human-level performance on imagenet classification", "author": ["Kaiming He", "Xiangyu Zhang", "Shaoqing Ren", "Jian Sun"], "venue": "In Proceedings of the IEEE international conference on computer vision,", "citeRegEx": "He et al\\.,? \\Q2015\\E", "shortCiteRegEx": "He et al\\.", "year": 2015}, {"title": "Orthogonal rnns and long-memory tasks", "author": ["Mikael Henaff", "Arthur Szlam", "Yann LeCun"], "venue": "arXiv preprint arXiv:1602.06662,", "citeRegEx": "Henaff et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Henaff et al\\.", "year": 2016}, {"title": "Long short-term memory", "author": ["Sepp Hochreiter", "J\u00fcrgen Schmidhuber"], "venue": "Neural computation,", "citeRegEx": "Hochreiter and Schmidhuber.,? \\Q1997\\E", "shortCiteRegEx": "Hochreiter and Schmidhuber.", "year": 1997}, {"title": "Regularizing rnns by stabilizing activations", "author": ["David Krueger", "Roland Memisevic"], "venue": "arXiv preprint arXiv:1511.08400,", "citeRegEx": "Krueger and Memisevic.,? \\Q2015\\E", "shortCiteRegEx": "Krueger and Memisevic.", "year": 2015}, {"title": "A simple way to initialize recurrent networks of rectified linear units", "author": ["Quoc V Le", "Navdeep Jaitly", "Geoffrey E Hinton"], "venue": "arXiv preprint arXiv:1504.00941,", "citeRegEx": "Le et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Le et al\\.", "year": 2015}, {"title": "Gradient-based learning applied to document recognition", "author": ["Yann LeCun", "L\u00e9on Bottou", "Yoshua Bengio", "Patrick Haffner"], "venue": "Proceedings of the IEEE,", "citeRegEx": "LeCun et al\\.,? \\Q1998\\E", "shortCiteRegEx": "LeCun et al\\.", "year": 1998}, {"title": "Building a large annotated corpus of english: The penn treebank", "author": ["Mitchell P Marcus", "Mary Ann Marcinkiewicz", "Beatrice Santorini"], "venue": "Computational linguistics,", "citeRegEx": "Marcus et al\\.,? \\Q1993\\E", "shortCiteRegEx": "Marcus et al\\.", "year": 1993}, {"title": "Rectified linear units improve restricted boltzmann machines", "author": ["Vinod Nair", "Geoffrey E Hinton"], "venue": "In Proceedings of the 27th International Conference on Machine Learning", "citeRegEx": "Nair and Hinton.,? \\Q2010\\E", "shortCiteRegEx": "Nair and Hinton.", "year": 2010}, {"title": "A note on riemannian optimization methods on the stiefel and the grassmann manifolds", "author": ["Yasunori Nishimori"], "venue": null, "citeRegEx": "Nishimori.,? \\Q2005\\E", "shortCiteRegEx": "Nishimori.", "year": 2005}, {"title": "On the difficulty of training recurrent neural networks", "author": ["Razvan Pascanu", "Tomas Mikolov", "Yoshua Bengio"], "venue": "ICML (3),", "citeRegEx": "Pascanu et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Pascanu et al\\.", "year": 2013}, {"title": "Exact solutions to the nonlinear dynamics of learning in deep linear neural networks", "author": ["Andrew M Saxe", "James L McClelland", "Surya Ganguli"], "venue": "arXiv preprint arXiv:1312.6120,", "citeRegEx": "Saxe et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Saxe et al\\.", "year": 2013}, {"title": "Notes on optimization on stiefel manifolds", "author": ["Hemant D Tagare"], "venue": "Technical report, Tech. Rep., Yale University,", "citeRegEx": "Tagare.,? \\Q2011\\E", "shortCiteRegEx": "Tagare.", "year": 2011}, {"title": "Lecture 6.5\u2014RmsProp: Divide the gradient by a running average of its recent magnitude", "author": ["T. Tieleman", "G. Hinton"], "venue": "COURSERA: Neural Networks for Machine Learning,", "citeRegEx": "Tieleman and Hinton.,? \\Q2012\\E", "shortCiteRegEx": "Tieleman and Hinton.", "year": 2012}, {"title": "Full-capacity unitary recurrent neural networks", "author": ["Scott Wisdom", "Thomas Powers", "John R. Hershey", "Jonathan Le Roux", "Les Atlas"], "venue": null, "citeRegEx": "Wisdom et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Wisdom et al\\.", "year": 2016}], "referenceMentions": [{"referenceID": 12, "context": "Typically, exploding gradients are avoided by clipping large gradients (Pascanu et al., 2013) or introducing an L2 or L1 weight norm penalty.", "startOffset": 71, "endOffset": 93}, {"referenceID": 11, "context": "At a much higher computational cost, gradient descent optimization directly along this manifold can be done via geodesic steps (Nishimori, 2005; Tagare, 2011).", "startOffset": 127, "endOffset": 158}, {"referenceID": 14, "context": "At a much higher computational cost, gradient descent optimization directly along this manifold can be done via geodesic steps (Nishimori, 2005; Tagare, 2011).", "startOffset": 127, "endOffset": 158}, {"referenceID": 16, "context": "Recent work (Wisdom et al., 2016) has proposed the optimization of unitary matrices along the Stiefel manifold using geodesic gradient descent.", "startOffset": 12, "endOffset": 33}, {"referenceID": 8, "context": "Typically, exploding gradients are avoided by clipping large gradients (Pascanu et al., 2013) or introducing an L2 or L1 weight norm penalty. The latter has the effect of bounding the spectral radius of the linear transformations, thus limiting the maximal gain across the transformation. Krueger & Memisevic (2015) attempt to stabilize the norm of propagating signals directly by penalizing differences in successive norm pairs in the forward pass and Pascanu et al.", "startOffset": 72, "endOffset": 316}, {"referenceID": 8, "context": "Typically, exploding gradients are avoided by clipping large gradients (Pascanu et al., 2013) or introducing an L2 or L1 weight norm penalty. The latter has the effect of bounding the spectral radius of the linear transformations, thus limiting the maximal gain across the transformation. Krueger & Memisevic (2015) attempt to stabilize the norm of propagating signals directly by penalizing differences in successive norm pairs in the forward pass and Pascanu et al. (2013) propose to penalize successive gradient norm pairs in the backward pass.", "startOffset": 72, "endOffset": 475}, {"referenceID": 5, "context": "Le et al. (2015) and Henaff et al.", "startOffset": 0, "endOffset": 17}, {"referenceID": 3, "context": "(2015) and Henaff et al. (2016) have respectively shown that identity initialization and orthogonal initialization can be beneficial.", "startOffset": 11, "endOffset": 32}, {"referenceID": 0, "context": "Arjovsky et al. (2015) have gone beyond initialization, building unitary recurrent neural network (RNN) models with transformations that are unitary by construction which they achieved by composing multiple basic unitary transformations.", "startOffset": 0, "endOffset": 23}, {"referenceID": 14, "context": "from Tagare (2011), combining the use of a canonical inner products and Cayley transformations.", "startOffset": 5, "endOffset": 19}, {"referenceID": 12, "context": "In (Pascanu et al., 2013), it is shown that the 2-norm of \u2202ai+1 \u2202ai is bounded by the product of the norms of the non-linearity\u2019s Jacobian and transition matrix at time t (layer i ), as follows: \u2223\u2223\u2223\u2223\u2223\u2223\u2223\u2223\u2202at+1 \u2202at \u2223\u2223\u2223\u2223\u2223\u2223\u2223\u2223 \u2264 ||Dt|| ||Wt|| \u2264 \u03bbDt \u03bbWt = \u03b7t, \u03bbDt , \u03bbWt \u2208 R.", "startOffset": 3, "endOffset": 25}, {"referenceID": 11, "context": "where M = U, M = V or M = W if so desired, we employ a Cayley transformation of the update step onto the Stiefel manifold of (semi-)orthogonal matrices, as in Nishimori (2005) and Tagare (2011).", "startOffset": 159, "endOffset": 176}, {"referenceID": 11, "context": "where M = U, M = V or M = W if so desired, we employ a Cayley transformation of the update step onto the Stiefel manifold of (semi-)orthogonal matrices, as in Nishimori (2005) and Tagare (2011). Given an orthogonally-initialized parameter matrix M and its Jacobian, G with respect to the objective function, an update is performed as follows:", "startOffset": 159, "endOffset": 194}, {"referenceID": 7, "context": "We then move on to tasks on real data that require models to capture long-range dependencies: digit classification based on sequential and permuted MNIST vectors (Le et al., 2015; LeCun et al., 1998).", "startOffset": 162, "endOffset": 199}, {"referenceID": 8, "context": "We then move on to tasks on real data that require models to capture long-range dependencies: digit classification based on sequential and permuted MNIST vectors (Le et al., 2015; LeCun et al., 1998).", "startOffset": 162, "endOffset": 199}, {"referenceID": 9, "context": "Finally, we look at a basic language modeling task using the Penn Treebank dataset (Marcus et al., 1993).", "startOffset": 83, "endOffset": 104}, {"referenceID": 4, "context": "We confirm that orthogonal initialization is useful as noted in Henaff et al. (2016), and we show that although strict orthogonality guarantees stable gradient norm, loosening orthogonality constraints can increase the rate of gradient descent convergence.", "startOffset": 64, "endOffset": 85}, {"referenceID": 4, "context": "We confirm that orthogonal initialization is useful as noted in Henaff et al. (2016), and we show that although strict orthogonality guarantees stable gradient norm, loosening orthogonality constraints can increase the rate of gradient descent convergence. We begin our analyses on tasks that are designed to stress memory: a sequence copying task and a basic addition task (Hochreiter & Schmidhuber, 1997). We then move on to tasks on real data that require models to capture long-range dependencies: digit classification based on sequential and permuted MNIST vectors (Le et al., 2015; LeCun et al., 1998). Finally, we look at a basic language modeling task using the Penn Treebank dataset (Marcus et al., 1993). The copy and adding tasks, introduced by Hochreiter & Schmidhuber (1997), are synthetic benchmarks with pathologically hard long distance dependencies that require long-term memory in models.", "startOffset": 64, "endOffset": 788}, {"referenceID": 7, "context": "The sequential MNIST task from Le et al. (2015), MNIST digits are flattened into vectors that can be traversed sequentially by a recurrent neural network.", "startOffset": 31, "endOffset": 48}, {"referenceID": 7, "context": "The sequential MNIST task from Le et al. (2015), MNIST digits are flattened into vectors that can be traversed sequentially by a recurrent neural network. The goal is to classify the digit based on the sequential input of pixels. The simple variant of this task is with a simple flattening of the image matrices; the harder variant of this task includes a random permutation of the pixels in the input vector that is determined once for an experiment. The latter formulation introduces longer distance dependencies between pixels that must be interpreted by the classification model. The English Penn Treebank (PTB) dataset from Marcus et al. (1993) is an annotated corpus of English sentences, commonly used for benchmarking language models.", "startOffset": 31, "endOffset": 650}, {"referenceID": 4, "context": "For the copy task, we used Elman networks without a transition non-linearity as in Henaff et al. (2016). We discuss our investigations into the use of a non-linearity on the copy task in the Appendix.", "startOffset": 83, "endOffset": 104}, {"referenceID": 13, "context": "Indeed, (Saxe et al., 2013) describe this as a possibly good regime for learning in deep neural networks.", "startOffset": 8, "endOffset": 27}, {"referenceID": 4, "context": "This is similar to the orthogonality penalty introduced by Henaff et al. (2016). In the first two subfigures on the left of Figure 4, we explore the effect of weakening this form of regularization.", "startOffset": 59, "endOffset": 80}], "year": 2017, "abstractText": "It is well known that it is challenging to train deep neural networks and recurrent neural networks for tasks that exhibit long term dependencies. The vanishing or exploding gradient problem is a well known issue associated with these challenges. One approach to addressing vanishing and exploding gradients is to use either soft or hard constraints on weight matrices so as to encourage or enforce orthogonality. Orthogonal matrices preserve gradient norm during backpropagation and can therefore be a desirable property; however, we find that hard constraints on orthogonality can negatively affect the speed of convergence and model performance. This paper explores the issues of optimization convergence, speed and gradient stability using a variety of different methods for encouraging or enforcing orthogonality. In particular we propose a weight matrix factorization and parameterization strategy through which we can bound matrix norms and therein control the degree of expansivity induced during backpropagation.", "creator": "LaTeX with hyperref package"}}}