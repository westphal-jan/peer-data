{"id": "1702.00196", "review": {"conference": "NIPS", "VERSION": "v1", "DATE_OF_SUBMISSION": "1-Feb-2017", "title": "Communication-Optimal Distributed Clustering", "abstract": "Clustering large datasets is a fundamental problem with a number of applications in machine learning. Data is often collected on different sites and clustering needs to be performed in a distributed manner with low communication. We would like the quality of the clustering in the distributed setting to match that in the centralized setting for which all the data resides on a single site. In this work, we study both graph and geometric clustering problems in two distributed models: (1) a point-to-point model, and (2) a model with a broadcast channel. We give protocols in both models which we show are nearly optimal by proving almost matching communication lower bounds. Our work highlights the surprising power of a broadcast channel for clustering problems; roughly speaking, to spectrally cluster $n$ points or $n$ vertices in a graph distributed across $s$ servers, for a worst-case partitioning the communication complexity in a point-to-point model is $n \\cdot s$, while in the broadcast model it is $n + s$. A similar phenomenon holds for the geometric setting as well. We implement our algorithms and demonstrate this phenomenon on real life datasets, showing that our algorithms are also very efficient in practice.", "histories": [["v1", "Wed, 1 Feb 2017 10:30:32 GMT  (1747kb,D)", "http://arxiv.org/abs/1702.00196v1", "A preliminary version of this paper appeared at the 30th Annual Conference on Neural Information Processing Systems (NIPS), 2016"]], "COMMENTS": "A preliminary version of this paper appeared at the 30th Annual Conference on Neural Information Processing Systems (NIPS), 2016", "reviews": [], "SUBJECTS": "cs.DS cs.LG", "authors": ["jiecao chen", "he sun 0001", "david p woodruff", "qin zhang 0001"], "accepted": true, "id": "1702.00196"}, "pdf": {"name": "1702.00196.pdf", "metadata": {"source": "CRF", "title": "Communication-Optimal Distributed Clustering\u2217", "authors": ["Jiecao Chen", "He Sun", "David P. Woodruff", "Qin Zhang"], "emails": ["jiecchen@indiana.edu", "h.sun@bristol.ac.uk", "dpwoodru@us.ibm.com", "qzhangcs@indiana.edu"], "sections": [{"heading": null, "text": "A preliminary version of this paper will be published at the 30th Annual Meeting of Neural Information Processing Systems (NIPS) 2016. \u2020 Department of Computer Science, Indiana University, Bloomington, USA. Work partially supported by NSF CCF1525024 and IIS-1633215. Email: jiecchen @ indiana.edu \u2021 Department of Computer Science, University of Bristol, UK. h.sun @ bristol.ac.uk \u00a7 IBM Research Almaden, San Jose, USA. dpwoodru @ us.ibm.com \u00b6 Department of Computer Science, Indiana University, Bloomington, USA. Work partially supported by NSF CCF1525024 and IIS-1633215. Email: qzhangcs @ indiana.eduar Xiv: 170 2.00 196v 1 [cs.D S] 1 Feb 201 7Contents"}, {"heading": "1 Introduction 1", "text": "1.1 Our contributions....................................................................................................."}, {"heading": "2 Preliminaries 3", "text": "2.1 Laplacian graph............................................................................................................................"}, {"heading": "3 Distributed graph clustering 5", "text": "3.1................................................................................................................."}, {"heading": "4 Distributed geometric clustering 9", "text": "4.1 The message model........................................... 10 4.2 The table model..................................................."}, {"heading": "5 Experiments 13", "text": "5.1 Data sets.........................................................................................................................."}, {"heading": "1 Introduction", "text": "In fact, it is so that most of them are able to survive themselves if they do not see themselves as being able to survive themselves. Most of them are not able to survive themselves, and most of them are able to survive themselves. Most of them are able to survive themselves, and most of them are not able to survive themselves. Most of them are able to survive themselves, and most of them are not able to survive themselves. Most of them are able to survive themselves."}, {"heading": "1.1 Our contributions", "text": "This year it is more than ever before in the history of the city."}, {"heading": "1.2 Related work", "text": "Balcan et al. [3, 4] and Feldman et al. [10] examine distributed k-median ([3] also examines k-median) and present verifiable guarantees of cluster quality. Recently, Guha et al. [11] examined distributed k-median / center / mean with outliers. Cohen et al. [7] investigate dimension reduction techniques for the input data matrices that can be used for distributed k-averages. Main emphasis is on the fact that there is no previous work that developed protocols for spectral clustering in common message transmission and table models, and also a lack of lower limits."}, {"heading": "2 Preliminaries", "text": "G = (V, E, w) is an undirected graph with n vertices, m edges and weight function V \u00b7 V \u2192 R \u2265 0. The set of neighbors of a vertex v is represented by N (v), and its degree is dv = \u2211 u \u0445 v (u, v). The maximum degree of G is defined as \"G\" = maxv {dv}. For each quantity of S V, we leave \"\u00b5\" (S), \"v\" S dv. \"For each quantity S, T V, we define w (S, T),\" u \"S, v\" T \"w\" (u, v) as the total weight of the edges that cross S and T. We define the conductivity of each quantity S by\u03c6 (S, V\\ S) \u00b5 (S).For two sets X and Y, the symmetric difference of X and Y is defined as X4Y \u2212 notoriously (Y\\ X)."}, {"heading": "2.1 Graph Laplacian", "text": "The laplac matrix of G is a n \u00b7 n matrix LG, defined by LG = DG \u2212 AG, where AG is the adjacence matrix of G, defined by AG (u, v) = w (u, v), and DG is the n \u00b7 n diagonal matrix with DG (v, v) = dv for each v-V [G]. Alternatively, we can write LG with respect to a signed edge-vertex incidence matrix: We assign each edge e = {u, v} an arbitrary orientation and leave BG (e, v) = 1 if v is the head of e, BG (e, v) = \u2212 1 if v is the tail of e, and BG (e, v) = 0 otherwise. We further define a diagonal matrix WG-Rm \u00b7 m, where WG (e, v) = 1 for each edge of e-E [G], and BG (e, otherwise v = 0)."}, {"heading": "2.2 Spectral sparsification", "text": "For each undirected and weighted diagram G = (V, E, w) we say that a subgraph H of G with correct reweighting of the edges is a (1 + \u03b5) spectral sparsifier if (1 \u2212 \u03b5) LG LH (1 + \u03b5) LG. (1) By definition, it is easy to show that if we split the set of edges of a diagram G = (V, E) into E1,.. \u2212 E 'for a constant' and Hi a spectral sparsifier of Gi = (V, Ei) for any 1 \u2264 i, then the diagram resulting from the union of the set of edges of Hi \u2212 x \u2212 H is a spectral sparsifier of G. It is known that for each undirected diagram G of n n ces there is a (1 + \u03b5) spectral sparsifier of Gi (V, Ei)."}, {"heading": "2.3 Models of computation", "text": "We examine the distributed cluster in two models of distributed data: the message delivery model and the table model. The message delivery model represents those distributed computing systems with point-to-point communication, and the table model represents those in which messages can be sent to all parties. Specifically, in the message delivery model, there are pages P1,.., Ps and a coordinator. These pages can talk to the coordinator via a bilateral private channel. In fact, this is referred to as the coordinator model in Section 1, where it is equivalent to the point-to-point model up to small factors. Input is first distributed to the s locations. Calculation is done in rounds: at the beginning of each round, the coordinator sends a message to some of the s locations, and then each of these pages contacted by the coordinator sends a message back to the coordinator."}, {"heading": "2.4 Communication complexity", "text": "For each problem A and each protocol that A solves, the communication complexity of a protocol is the maximum communication cost across all possible inputs X. If the protocol is randomized, we define the error of \u0430 bymax XP (the coordinator issues an incorrect response to X), with the maximum above all inputs X and the probability above all random strings of the coordinator and the s-sides. The g-error randomized communication complexity R\u03b4 (A) of a problem A in the message transmission model is the minimum communication complexity of any random protocol that A solves at most with error."}, {"heading": "2.5 Information complexity", "text": "We abuse the notation by using both the protocol and its transcript (its concatenation of messages) \"i (i [s])\" as a transcript (set of messages to be exchanged) between the i-th page and the coordinator. Then, \"i\" can be considered a concatenation of \"1,\" \"2...\" and \"i\" as a consequence of the timestamps of the messages. We define the information complexity of a problem \"A\" in IC\u00b5's message delivery model, \"(A) = min (B) error,\" i [s] I (X1,.., Xs; \"i), where\" I \"(\u00b7; \u00b7) is the mutual information function. It was shown in [12] that\" R (A), \"I (A),\" A) is \"\u00b5\" for each input distribution."}, {"heading": "3 Distributed graph clustering", "text": "In this section, we examine distributed graph clustering. We assume that the vertex set of the input graph G = (V, E) can be divided into k clusters, in which vertices in each cluster S are strongly interconnected and there are fewer edges between S and V\\ S. To formalize this term, we define the k path expansion constant of the diagram G by\u03c1 (k), min partition A1,..., Ak max 1 \u2264 i \u2264 k inequality (Ai). Note that a diagram G has k clusters if the value of p (k) is small. It was shown in [14] that p (k) is closely related to p-point (LG) by the following cheeger inequality of higher order:"}, {"heading": "3.1 The message passing model", "text": "We assume that the edges of the input diagram G = (V, E) are arbitrary between sites P1, \u00b7 \u00b7, Ps, and we use egg to mark the edge maintained by the side Pi. Our proposed algorithm consists of two steps: (i) each Pi calculates a linear quantity (1 + \u03b5) -spectral aggregation of algorithms based on Gi, (V, Ei), and sends the edge set of Hi, that of E \u2032 i, to the coordinator; (ii) the coordinator performs a spectral aggregation of algorithms based on the graphs received H, (V, k i = 1E). The theorem below summarizes the performance of this algorithm, and shows the approximation guarantee of this algorithm is as good as the detectable guarantee of the spectral aggregation shown in the central arrangement."}, {"heading": "3.2 The blackboard model", "text": "The result is based on the observation that a spectral sparsifier maintains the structure of the clusters used for the detection of Theorem 3.2. Thus, it is sufficient to design a distributed algorithm for the construction of a spectral sparsifying system. (K) The distribution algorithms are based on the construction of a chain of coarse sparsifiers [18], which is described as follows: for each input PSD matrix K with the construction of a spectral sparsifier (K). (K) We will define the non-zero eigenvalues of K d = dlog2) e and construct a chain of d + 1 matrices [K (0),. (1),. K (2) where (i) and K (i)."}, {"heading": "4 Distributed geometric clustering", "text": "Let P be a set of points of magnitude n in a metric space with distance function d (\u00b7, \u00b7), and let k \u2264 n be an integer. In the k-center problem, let P be a set of points of magnitude n in a metric space with distance function d (\u00b7, \u00b7), and let k \u2264 n be an integer. In the k-center problem, let us find a set of C (| C | = k), so that maxp-P d (p, C) is minimized, where d (p, C) = minc-C is d (p, c). In the k median and k-mean, let us replace the objective function maxp-P d (p, C) with \u0445 p-P d (p, C) or \u0445 p-P (d (p, C) 2."}, {"heading": "4.1 The message passing model", "text": "As already mentioned, we are able to prove a general function that we have for a constant dimensionality of Euclidean space and a constant c > 1. There are algorithms that c-approximate k-median and k-means using O (sk) bits of communication [3]. The following theory states that the above upper limits are closely related to logarithmic factors. The proof uses tools from multiparty communication complexity. We can in fact prove a stronger statement that any algorithm that can distinguish whether we have k-points or k-points in the totality of message transmission model needs. Theorem 4.1 For each c > 1, the calculation of the c-approximation for k-median, k-means or k-center correctly with the probability 0.99."}, {"heading": "4.2 The blackboard model", "text": "We can show that there is an algorithm that achieves an approximation O (1) approximation using O (s + k) bits of communication for k-median and k-mean. Our algorithm for k-median / mean is a simple adaptation of the successive sampling algorithm entered by Mettu and Plaxton [17] into the (centralized) RAM model. We first summarize their algorithm and then describe how to port it to the table model. Let X1,. Xs be the point sets at sites P1,., Pk respectively. The successive sampling algorithm runs in rounds. In each round j it does the following places: 1. s Places where O (k) point centers designated by Yj."}, {"heading": "5 Experiments", "text": "In this section, we present experimental results for clustering graphs in message and table models. We will compare the following three algorithms: (1) Baseline: Each site sends all data directly to the coordinator; (2) MsgPassing: our algorithm in the message delivery model (Section 3.1); (3) Table: our algorithm in the table model (Section 3.2). In addition to displaying the visualized results of these algorithms on different data sets, we also measure the quality of results via the standardized intersection, defined as asncut (A1,.., Ak) = 12 \u2211 i [k] w (Ai, V\\ Ai) \u00b5 (Ai), which is a standard objective function that must be minimized for spectral cluster algorithms. We implemented the algorithms using multiple languages, including Matlab, Python and C + +. Our experiments were conducted on an IBM NeXtale Scnx360 server, M32-2GB and Intel-2GB processor."}, {"heading": "5.1 Datasets.", "text": "We test the algorithms in the following real and synthetic datasets, which are shown in Figure 1. \u2022 Twomoons: This dataset contains n = 14,000 coordinates in R2. We consider each point as a vertex. For two vertex u, v we add an edge with weight w (u, v) = exp {\u2212 u \u2212 v \u00b2 22 / \u03c32} with \u03c3 = 0.1 if one vertex belongs to the next 7000 points of the other. This construction results in a graph with about 110,000, 000 edges. \u2022 Gauss: This dataset contains n = 10,000 points in R2. There are 4 clusters in this dataset, each of which is generated using a Gaussian distribution. We construct a complete graph as a similarity diagram. For two vertex u, v, we define the weight w (u, v) = exp {\u2212 v \u2212 v = 1."}, {"heading": "5.2 Results on clustering quality", "text": "We visualize the cluster results for the Twomoons, Gauss and Sculptures in Figure 2. You can see that baseline, MsgPassing and Blackboard provide results of very similar qualities. For the sake of simplicity, we present here only the visualization for s = 15. Similar results were observed when we varied the values. We also compare the normalized cut (ncut) values of the cluster results of different algorithms. The results are presented in Figure 3. In all datasets, the Ncut values of different algorithms are very close to each other. The Ncut value of MsgPassing decreases slightly when we increase the value of s, while the Ncut value of Blackboard is independent of s."}, {"heading": "5.3 Results on communication costs", "text": "We find that while we achieve clustering qualities similar to baseline, both MsgPassing and Blackboard are significantly more communication efficient (by one or two orders of magnitude in our experiments); we also note that the value of s does not affect Blackboard's communication costs, while the communication costs of MsgPassing grow almost linearly with s; when s is large, MsgPassing uses significantly more communication than Blackboard. These confirm our theory. 5.4 Parameters in MsgPassing and BlackboardFigure 5 show in MsgPassinghow that the value of ncut is affected by the number of locations and the number of edges sampled at each location. If c = 3 and s = 1, the ncut value differs from all data sets. This is because the algorithm with such a small c does not generate worthless data."}], "references": [{"title": "On sketching quadratic forms", "author": ["Alexandr Andoni", "Jiecao Chen", "Robert Krauthgamer", "Bo Qin", "David P. Woodruff", "Qin Zhang"], "venue": "In ITCS,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2016}, {"title": "k-means++: The advantages of careful seeding", "author": ["David Arthur", "Sergei Vassilvitskii"], "venue": "In SODA,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2007}, {"title": "Distributed k-means and k-median clustering on general communication topologies", "author": ["Maria-Florina Balcan", "Steven Ehrlich", "Yingyu Liang"], "venue": "In NIPS,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2013}, {"title": "Improved distributed principal component analysis", "author": ["Maria-Florina Balcan", "Vandana Kanchanapally", "Yingyu Liang", "David P. Woodruff"], "venue": "CoRR, abs/1408.5823,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2014}, {"title": "A tight bound for set disjointness in the message-passing model", "author": ["Mark Braverman", "Faith Ellen", "Rotem Oshman", "Toniann Pitassi", "Vinod Vaikuntanathan"], "venue": "In FOCS,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2013}, {"title": "Algorithms for facility location problems with outliers", "author": ["Moses Charikar", "Samir Khuller", "David M. Mount", "Giri Narasimhan"], "venue": "In SODA,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2001}, {"title": "Dimensionality reduction for k-means clustering and low rank approximation", "author": ["Michael B. Cohen", "Sam Elder", "Cameron Musco", "Christopher Musco", "Madalina Persu"], "venue": "In STOC,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2015}, {"title": "Continuous sampling from distributed streams", "author": ["Graham Cormode", "S. Muthukrishnan", "Ke Yi", "Qin Zhang"], "venue": "J. ACM,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2012}, {"title": "Conquering the divide: Continuous clustering of distributed data streams", "author": ["Graham Cormode", "S Muthukrishnan", "Wei Zhuang"], "venue": "In ICDE,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2007}, {"title": "Turning big data into tiny data: Constant-size coresets for k -means, PCA and projective clustering", "author": ["Dan Feldman", "Melanie Schmidt", "Christian Sohler"], "venue": "In SODA,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2013}, {"title": "Distributed partial clustering", "author": ["Sudipto Guha", "Yi Li", "Qin Zhang"], "venue": null, "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2017}, {"title": "Communication complexity of approximate matching in distributed graphs", "author": ["Zengfeng Huang", "Bozidar Radunovic", "Milan Vojnovic", "Qin Zhang"], "venue": "In STACS,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2015}, {"title": "Analysis of a local search heuristic for facility location problems", "author": ["Madhukar R. Korupolu", "C. Greg Plaxton", "Rajmohan Rajaraman"], "venue": "In SODA,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 1998}, {"title": "Multi-way spectral partitioning and higher-order cheeger inequalities", "author": ["James R. Lee", "Shayan Oveis Gharan", "Luca Trevisan"], "venue": "In STOC,", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2012}, {"title": "Constructing linear-sized spectral sparsification in almost-linear time", "author": ["Yin Tat Lee", "He Sun"], "venue": "In FOCS,", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2015}, {"title": "MST construction in O(log log n) communication rounds", "author": ["Zvi Lotker", "Elan Pavlov", "Boaz Patt-Shamir", "David Peleg"], "venue": "In SPAA,", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2003}, {"title": "Optimal time bounds for approximate clustering", "author": ["Ramgopal R. Mettu", "C. Greg Plaxton"], "venue": "Machine Learning,", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2004}, {"title": "Iterative approaches to row sampling", "author": ["Gary L. Miller", "Richard Peng"], "venue": "CoRR, abs/1211.2713,", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2012}, {"title": "On spectral clustering: Analysis and an algorithm", "author": ["Andrew Y. Ng", "Michael I. Jordan", "Yair Weiss"], "venue": "Advances in neural information processing systems,", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2002}, {"title": "Partitioning well-clustered graphs: Spectral clustering works", "author": ["Richard Peng", "He Sun", "Luca Zanetti"], "venue": "In COLT, pages 1423\u20131455,", "citeRegEx": "20", "shortCiteRegEx": "20", "year": 2015}, {"title": "Lower bounds for number-in-hand multiparty communication complexity, made easy", "author": ["Jeff M. Phillips", "Elad Verbin", "Qin Zhang"], "venue": "SIAM J. Comput.,", "citeRegEx": "21", "shortCiteRegEx": "21", "year": 2016}, {"title": "A tutorial on spectral clustering", "author": ["Ulrike Von Luxburg"], "venue": "Statistics and computing,", "citeRegEx": "22", "shortCiteRegEx": "22", "year": 2007}, {"title": "Tight bounds for distributed functional monitoring", "author": ["David P. Woodruff", "Qin Zhang"], "venue": "In STOC,", "citeRegEx": "23", "shortCiteRegEx": "23", "year": 2012}], "referenceMentions": [{"referenceID": 20, "context": "Also, for a number of problems the communication complexity is the same in both models, such as computing the sum of s length-n bit vectors modulo two, where each site holds one bit vector [21], or estimating large moments [23].", "startOffset": 189, "endOffset": 193}, {"referenceID": 22, "context": "Also, for a number of problems the communication complexity is the same in both models, such as computing the sum of s length-n bit vectors modulo two, where each site holds one bit vector [21], or estimating large moments [23].", "startOffset": 223, "endOffset": 227}, {"referenceID": 4, "context": "Still, for other problems like set disjointness it can save a factor of s in the communication [5].", "startOffset": 95, "endOffset": 98}, {"referenceID": 0, "context": ", [1]), from which a spectral sparsifier serves as a good preconditioner.", "startOffset": 2, "endOffset": 5}, {"referenceID": 2, "context": "Our results imply that existing algorithms [3] for k-median and k-means with \u00d5(sk) bits of communication, as well as the folklore parallel guessing algorithm for k-center with \u00d5(sk) bits of communication, are optimal up to polylogarithmic factors.", "startOffset": 43, "endOffset": 46}, {"referenceID": 1, "context": ", [2, 19, 20, 22]).", "startOffset": 2, "endOffset": 17}, {"referenceID": 18, "context": ", [2, 19, 20, 22]).", "startOffset": 2, "endOffset": 17}, {"referenceID": 19, "context": ", [2, 19, 20, 22]).", "startOffset": 2, "endOffset": 17}, {"referenceID": 21, "context": ", [2, 19, 20, 22]).", "startOffset": 2, "endOffset": 17}, {"referenceID": 2, "context": "[3, 4] and Feldman et al.", "startOffset": 0, "endOffset": 6}, {"referenceID": 3, "context": "[3, 4] and Feldman et al.", "startOffset": 0, "endOffset": 6}, {"referenceID": 9, "context": "[10] study distributed k-means ([3] also studies k-median), and present provable guarantees on the clustering quality.", "startOffset": 0, "endOffset": 4}, {"referenceID": 2, "context": "[10] study distributed k-means ([3] also studies k-median), and present provable guarantees on the clustering quality.", "startOffset": 32, "endOffset": 35}, {"referenceID": 10, "context": "[11] studied distributed k-median/center/means with outliers.", "startOffset": 0, "endOffset": 4}, {"referenceID": 6, "context": "[7] study dimensionality reduction techniques for the input data matrices that can be used for distributed k-means.", "startOffset": 0, "endOffset": 3}, {"referenceID": 2, "context": ", [3, 4, 10]), no provable lower bounds in either model existed, and our main contribution is to show that previous algorithms are optimal.", "startOffset": 2, "endOffset": 12}, {"referenceID": 3, "context": ", [3, 4, 10]), no provable lower bounds in either model existed, and our main contribution is to show that previous algorithms are optimal.", "startOffset": 2, "endOffset": 12}, {"referenceID": 9, "context": ", [3, 4, 10]), no provable lower bounds in either model existed, and our main contribution is to show that previous algorithms are optimal.", "startOffset": 2, "endOffset": 12}, {"referenceID": 14, "context": "It is known that, for any undirected graph G of n vertices, there is a (1 + \u03b5)-spectral sparsifier of G with O(n/\u03b52) edges, and it can be constructed in almost-linear time [15].", "startOffset": 172, "endOffset": 176}, {"referenceID": 4, "context": ", [5, 21, 23]).", "startOffset": 2, "endOffset": 13}, {"referenceID": 20, "context": ", [5, 21, 23]).", "startOffset": 2, "endOffset": 13}, {"referenceID": 22, "context": ", [5, 21, 23]).", "startOffset": 2, "endOffset": 13}, {"referenceID": 15, "context": "They are similar to the congested clique model [16] studied in the distributed computing community; the main difference is that in our models we do not post any bandwidth limitations at each channel but instead consider the total number of bits communicated.", "startOffset": 47, "endOffset": 51}, {"referenceID": 11, "context": "It has been shown in [12] that R\u03b4(A) \u2265 IC\u03b4,\u03bc(A) for any input distribution \u03bc.", "startOffset": 21, "endOffset": 25}, {"referenceID": 13, "context": "It was shown in [14] that \u03c1(k) closely relates to \u03bbk(LG) by the following higher-order Cheeger inequality: \u03bbk(LG) 2 \u2264 \u03c1(k) \u2264 O(k) \u221a \u03bbk(LG).", "startOffset": 16, "endOffset": 20}, {"referenceID": 19, "context": "The same assumption has been used in the literature for studying graph clustering in the centralized setting [20].", "startOffset": 109, "endOffset": 113}, {"referenceID": 19, "context": "1 ([20]).", "startOffset": 3, "endOffset": 7}, {"referenceID": 4, "context": "3 ([5]).", "startOffset": 3, "endOffset": 6}, {"referenceID": 17, "context": "Our distributed algorithm is based on constructing a chain of coarse sparsifiers [18], which is described as follows: for any input PSD matrix K with \u03bbmax(K) \u2264 \u03bbu and all the non-zero eigenvalues of K at least \u03bb`, we define d = dlog2(\u03bbu/\u03bb`)e and construct a chain of d+ 1 matrices", "startOffset": 81, "endOffset": 85}, {"referenceID": 17, "context": "6 ([18]).", "startOffset": 3, "endOffset": 7}, {"referenceID": 2, "context": "As mentioned, for constant dimensional Euclidean space and a constant c > 1, there are algorithms that c-approximate k-median and k-means using \u00d5(sk) bits of communication [3].", "startOffset": 172, "endOffset": 175}, {"referenceID": 8, "context": ", [9]) achieve a 2.", "startOffset": 2, "endOffset": 5}, {"referenceID": 5, "context": ", [6, 13]).", "startOffset": 2, "endOffset": 9}, {"referenceID": 12, "context": ", [6, 13]).", "startOffset": 2, "endOffset": 9}, {"referenceID": 16, "context": "Our algorithm for k-median/means is an easy adaptation of the successive sampling algorithm proposed by Mettu and Plaxton [17] in the (centralized) RAM model.", "startOffset": 122, "endOffset": 126}, {"referenceID": 16, "context": "In [17] it has been shown that this algorithm gives an O(1)-approximation to k-median or k-means with high probability.", "startOffset": 3, "endOffset": 7}, {"referenceID": 7, "context": "Step 1 can be done by the distributed sampling algorithm in [8] using \u00d5(k + s) bits of communication; note that at the end of this step the sampled points in Yj are written on the blackboard.", "startOffset": 60, "endOffset": 63}], "year": 2017, "abstractText": "Clustering large datasets is a fundamental problem with a number of applications in machine learning. Data is often collected on different sites and clustering needs to be performed in a distributed manner with low communication. We would like the quality of the clustering in the distributed setting to match that in the centralized setting for which all the data resides on a single site. In this work, we study both graph and geometric clustering problems in two distributed models: (1) a point-to-point model, and (2) a model with a broadcast channel. We give protocols in both models which we show are nearly optimal by proving almost matching communication lower bounds. Our work highlights the surprising power of a broadcast channel for clustering problems; roughly speaking, to spectrally cluster n points or n vertices in a graph distributed across s servers, for a worst-case partitioning the communication complexity in a point-to-point model is n \u00b7 s, while in the broadcast model it is n + s. A similar phenomenon holds for the geometric setting as well. We implement our algorithms and demonstrate this phenomenon on real life datasets, showing that our algorithms are also very efficient in practice. \u2217A preliminary version of this paper appears at the 30th Annual Conference on Neural Information Processing Systems (NIPS), 2016. \u2020Department of Computer Science, Indiana University, Bloomington, USA. Work supported in part by NSF CCF1525024 and IIS-1633215. Email: jiecchen@indiana.edu \u2021Department of Computer Science, University of Bristol, Bristol, UK. h.sun@bristol.ac.uk \u00a7IBM Research Almaden, San Jose, USA. dpwoodru@us.ibm.com \u00b6Department of Computer Science, Indiana University, Bloomington, USA. Work supported in part by NSF CCF1525024 and IIS-1633215. Email: qzhangcs@indiana.edu ar X iv :1 70 2. 00 19 6v 1 [ cs .D S] 1 F eb 2 01 7", "creator": "LaTeX with hyperref package"}}}