{"id": "1702.00500", "review": {"conference": "ACL", "VERSION": "v1", "DATE_OF_SUBMISSION": "1-Feb-2017", "title": "AMR-to-text Generation with Synchronous Node Replacement Grammar", "abstract": "This paper addresses the task of AMR-to-text generation by leveraging synchronous node replacement grammar. During training, graph-to-string rules are learned using a heuristic extraction algorithm. At test time, a graph transducer is applied to collapse input AMRs and generate output sentences. Evaluated on SemEval-2016 Task 8, our method gives a BLEU score of 25.62, which is the best reported so far.", "histories": [["v1", "Wed, 1 Feb 2017 23:36:33 GMT  (42kb,D)", "http://arxiv.org/abs/1702.00500v1", null], ["v2", "Tue, 7 Feb 2017 18:22:24 GMT  (0kb,I)", "http://arxiv.org/abs/1702.00500v2", "Also there is a crucial error in section 2.2, 2.3 and 4.1"], ["v3", "Mon, 3 Apr 2017 02:39:14 GMT  (42kb,D)", "http://arxiv.org/abs/1702.00500v3", "Accepted by ACL 2017"], ["v4", "Fri, 28 Apr 2017 13:37:00 GMT  (78kb,D)", "http://arxiv.org/abs/1702.00500v4", "camera-ready version of ACL 2017"]], "reviews": [], "SUBJECTS": "cs.CL", "authors": ["linfeng song", "xiaochang peng", "yue zhang", "zhiguo wang", "daniel gildea"], "accepted": true, "id": "1702.00500"}, "pdf": {"name": "1702.00500.pdf", "metadata": {"source": "CRF", "title": "AMR-to-text Generation with Synchronous Node Replacement Grammar", "authors": ["Linfeng Song", "Xiaochang Peng", "Yue Zhang", "Zhiguo Wang", "Daniel Gildea"], "emails": [], "sections": [{"heading": null, "text": "AMR-to-Text Generation with Synchronous Node Replacement GrammarLinfeng Song1, Xiaochang Peng1, Yue Zhang3, Zhiguo Wang2 and Daniel Gildea1 1Department of Computer Science, University of Rochester, Rochester, NY 146272IBM T.J. Watson Research Center, Yorktown Heights, NY 10598 3Singapore University of Technology and DesignAbstractThis paper focuses on the task of generating AMR tottext using synchronous node replacement grammar. During the training, graph-to-string rules are learned using a heuristic extraction algorithm. At test time, a graph converter is used to collapse input AMRs and generate output sets. Analyzed on SemEval-2016 Task 8, our method yields a BLEU score of 25.62, which is the best value to date."}, {"heading": "1 Introduction", "text": "It is a semantic formalism that encodes the meaning of a sentence as a rooted, directed graph. AMR uses a graph to represent meaning where nodes (such as \"boy,\" \"want-01\") represent concepts, and edges (such as \"ARG0,\" \"ARG1\") represent relationships between concepts. Encoding many semantic phenomena into a graph structure, AMR is useful for NLP tasks such as machine translation (Jones et al., 2012; Tamchyna et al., 2015), answering questions (Mitra and Baral, 2015), summary (Takase et al., 2016), and event recognition (Li et al., 2015).AMR-to-text generation is a challenge as function words and syntactical structures are waved away, creating an AMR graph graph graph graph graph graph graph graph graph."}, {"heading": "2 Synchronous Node Replacement Grammar", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "2.1 Grammar Definition", "text": "A synchronous node replacement grammar (NRG) is a paraphrased formalism: G = < N, \u0440, P, S >, where N is a finite series of infinite characters, \u03a3 and \u0445 are finite sets of infinite symbols for the source and destination pages, respectively. S-N is the start symbol, and P is a finite series of productions. Each instance of P takes the form of Xi \u2192 (< F, E >, \u0445), where Xi-N is a non-terminal node, F is a rooted, connected AMR fragment with edge labels over \u03a3 and node labels over N-Z, E is a corresponding target designation over N-Z, and \u0445 denotes the alignment of non-terminal symbols between F and E. A classical NRG (Engelfriet and Rozenberg, 1997, Chapter 1) also defines C, which is an embedding mechanism that defines how F is connected to the rest of the graph."}, {"heading": "2.2 Induced Rules", "text": "In our system, there are three types of rules, namely induced rules, concept rules, and graph-adhesive rules. Here, we first introduce induced rules obtained by a two-step procedure on a training corpus. In algorithm 1, the nodes first placed with Xi, which were previously connected with F, are reconnected with Xistep to extract a series of initial rules from the Training < sentence, AMR > pairs (line 2), using the Phraseto graph fragment extraction algorithm by Peng et al. (2015) (line 3). Here, a starting rule contains only terminal symbols in F and E. In the next step, we align the pairs of the initial rules ri and rj after Cai and Knight (2013), and generate Rij by breaking ri with rj if ri contains rj2 (line 6-8). When ri collapses with rj, we replace the corresponding sub-graph in a new line with a normalized.F-node and a subnode rule."}, {"heading": "2.3 Concept Rules and Glue Rules", "text": "In addition to the induced rules, we adopt concept rules (Song et al., 2016) and graph-glue rules to ensure the existence of derivatives; for a concept rule, F is a single node in the AMR input diagram, and E is a morphological string of the node concept; a concept rule is used if no induced rule can cover the node. We refer to Verbalization List 3 and AMR Guidelines 4 to create more complex concept rules. For example, we define graph-glue rules to be created from the verbalization list: \"(k / keep-01: ARG1 (p / peace)) | | Peacekeeping.\" Inspired by Chiang (2005), we define graph-glue rules to associate non-terminal nodes connected to an edge when no induced rules can be applied. Three glue rules are defined for each type of edge label."}, {"heading": "3 Model", "text": "We use a log-linear model for the evaluation of search hypotheses. Considering an input diagram for AMR, we find the highest-rated derivative t * of all possible derivatives t: t * = argmax t exp (\u2211 i wifi (g, t)), (1) where g represents the input probability for AMR, fi (\u00b7, \u00b7) and wi for a feature and the corresponding weight. The feature set we apply includes phrase-to-graph and phrase-to-phrase translation probabilities and the corresponding lexicalized translation probabilities (Section 3.1), language model evaluation, word count, rule number, reorder of model evaluation (Section 3.2) and motion distance (Section 3.3). The characteristics for language model, word count and phrase count are taken over by SMT (Koehn et al., 2003; Chiang, 2005). We perform a punctuation search for the AMR subhypothesis, which contains a current bottom-up for each subsurface result."}, {"heading": "3.1 Translation Probabilities", "text": "Production rules serve as the basis for scoring hypotheses. We associate each synchronous NRG rule n \u2192 (< F, E >, \u0445) with a set of probabilities. First, phrases-to-fragment translation probabilities are defined on the basis of maximum probability estimation (MLE) as shown in Equation 2, where c < F, E > the fraction of < F, E >.p (F | E) = c < F, E > \u2211 F \u2032 c < F \u2032, E > (2) Furthermore, lexicalized translation probabilities are defined as follows: pw (F | E) = area number of E p (l | w) (3)."}, {"heading": "3.2 Reordering Model", "text": "Although the word order is defined for induced rules, it is not the case for adhesive rules. We learn a model of reordering that helps in deciding whether the translations of the nodes should be monotonous or inverse, since the connection edge designation is directed. A probability model with smoothed counter values is defined as: p (M | h, l, t) = 1.0 + \u2211 h \u2211 t (h, l, t, M) 2.0 + \u2211 o \u0432 {M, I} \u2211 h \u2211 t (h, l, t, o) (4) c (h, l, t, M) is the number of monotonous translations of head h and tail t connected by edge l."}, {"heading": "3.3 Moving Distance", "text": "The Moving Distance feature detects the distances between the subgraph roots of two consecutive rule matches during the decoding process, thereby controlling a propensity for the nearby subgraph to collapse one after the other."}, {"heading": "4 Experiments", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "4.1 Setup", "text": "We use the SemEval-2016 Task8 dataset, which contains 16833 training, 1368 development and 1371 test instances. Rules are extracted from the training data, and the model parameters are matched to the development set. For tuning and testing, we filter out sentences with more than 30 words, resulting in 1103 development instances and 1055 test instances. We train a 4 gram language model (LM) to Gigaword (LDC2011T07) and use BLEU (Papineni et al., 2002) as a benchmark. MERT is used (Och., 2003) to match model parameters to k-best outputs to the development set where k is set. We examine the effectiveness of rules and characteristics by ablation tests: \"NoInducedRule\" does not adopt induced rules, \"NoConceptRule\" does not adopt concept rules to the development set, \"NoMovingDistance\" does not adopt the feature of the movable distance and \"NoInducedRule\" cannot be combined with the existing NoceptR model. \""}, {"heading": "4.2 Results", "text": "The results are shown in Table 2. Firstly, All exceeds all baselines. NoInducedRule leads to the largest performance decline compared to All, which shows that induced rules play a very important role in our system. On the other hand, NoConceptRule does not lead to a large performance decline. This observation is in line with the observation by Song et al. (2016) for their TSP-based system. NoMovingDistance leads to a significant performance decline, which empirically confirms that the translations of close subgraphs are also close to each other. Finally, NoReorderingModel does not significantly affect performance, which may be due to the fact that the most important reordering patterns are already covered by the hierarchically induced rules. Compared with TSP genes and JAMR genes, our final model All the BLEU improves from 22.44 and 23.00 to 25.62 and shows the advantage of our model. To our knowledge, this is the best result ever reported on the task."}, {"heading": "5 Conclusion", "text": "We have demonstrated that synchronous node replacement grammar is useful for AMR-to-text generation by developing a system that learns a synchronous NRG during training time and uses a graph converter to collapse AMR input graphs and generate output strings according to the grammar learned at test dates. Our method exceeds the current state of the art and empirically demonstrates the benefits of our graph-to-string rules."}], "references": [{"title": "Broad-coverage CCG semantic parsing with AMR", "author": ["Artzi et al.2015] Yoav Artzi", "Kenton Lee", "Luke Zettlemoyer"], "venue": "In Conference on Empirical Methods in Natural Language Processing", "citeRegEx": "Artzi et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Artzi et al\\.", "year": 2015}, {"title": "Abstract meaning representation for sembanking", "author": ["Claire Bonial", "Shu Cai", "Madalina Georgescu", "Kira Griffitt", "Ulf Hermjakob", "Kevin Knight", "Philipp Koehn", "Martha Palmer", "Nathan Schneider"], "venue": null, "citeRegEx": "Banarescu et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Banarescu et al\\.", "year": 2013}, {"title": "Smatch: an evaluation metric for semantic feature structures", "author": ["Cai", "Knight2013] Shu Cai", "Kevin Knight"], "venue": "In Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics", "citeRegEx": "Cai et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Cai et al\\.", "year": 2013}, {"title": "A hierarchical phrase-based model for statistical machine translation", "author": ["David Chiang"], "venue": "In Proceedings of the 43rd Annual Meeting of the Association for Computational Linguistics", "citeRegEx": "Chiang.,? \\Q2005\\E", "shortCiteRegEx": "Chiang.", "year": 2005}, {"title": "Node replacement graph grammars", "author": ["Engelfriet", "Rozenberg1997] J. Engelfriet", "G. Rozenberg"], "venue": "Handbook of Graph Grammars and Computing by Graph Transformation,", "citeRegEx": "Engelfriet et al\\.,? \\Q1997\\E", "shortCiteRegEx": "Engelfriet et al\\.", "year": 1997}, {"title": "A discriminative graph-based parser for the abstract meaning representation", "author": ["Sam Thomson", "Jaime Carbonell", "Chris Dyer", "Noah A. Smith"], "venue": "In Proceedings of the 52nd Annual Meeting of the Association", "citeRegEx": "Flanigan et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Flanigan et al\\.", "year": 2014}, {"title": "Generation from abstract meaning representation using tree transducers", "author": ["Chris Dyer", "Noah A. Smith", "Jaime Carbonell"], "venue": "In Proceedings of the 2016 Meeting of the North American chapter of the Association", "citeRegEx": "Flanigan et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Flanigan et al\\.", "year": 2016}, {"title": "Loosely tree-based alignment for machine translation", "author": ["Daniel Gildea"], "venue": "In Proceedings of the 41th Annual Conference of the Association for Computational Linguistics", "citeRegEx": "Gildea.,? \\Q2003\\E", "shortCiteRegEx": "Gildea.", "year": 2003}, {"title": "Noise reduction and targeted exploration in imitation learning for abstract meaning representation parsing", "author": ["Andreas Vlachos", "Jason Naradowsky"], "venue": "In Proceedings of the 54th Annual Meeting of the Associa-", "citeRegEx": "Goodman et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Goodman et al\\.", "year": 2016}, {"title": "Graph parsing with s-graph grammars", "author": ["Alexander Koller", "Christoph Teichmann"], "venue": "In Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics", "citeRegEx": "Groschwitz et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Groschwitz et al\\.", "year": 2015}, {"title": "Statistical syntax-directed translation with extended domain of locality", "author": ["Huang et al.2006] Liang Huang", "Kevin Knight", "Aravind Joshi"], "venue": "In Proceedings of Association for Machine Translation in the Americas", "citeRegEx": "Huang et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Huang et al\\.", "year": 2006}, {"title": "Semantics-based machine translation with hyperedge replacement grammars", "author": ["Jones et al.2012] Bevan Jones", "Jacob Andreas", "Daniel Bauer", "Karl Moritz Hermann", "Kevin Knight"], "venue": "In Proceedings of the International Conference on Computational", "citeRegEx": "Jones et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Jones et al\\.", "year": 2012}, {"title": "Statistical phrase-based translation", "author": ["Koehn et al.2003] Philipp Koehn", "Franz Josef Och", "Daniel Marcu"], "venue": "In Proceedings of the 2003 Meeting of the North American chapter of the Association for Computational Linguistics", "citeRegEx": "Koehn et al\\.,? \\Q2003\\E", "shortCiteRegEx": "Koehn et al\\.", "year": 2003}, {"title": "Improving event detection with abstract meaning representation", "author": ["Li et al.2015] Xiang Li", "Thien Huu Nguyen", "Kai Cao", "Ralph Grishman"], "venue": "In Proceedings of the First Workshop on Computing News Storylines,", "citeRegEx": "Li et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Li et al\\.", "year": 2015}, {"title": "Tree-to-string alignment template for statistical machine translation", "author": ["Liu et al.2006] Yang Liu", "Qun Liu", "Shouxun Lin"], "venue": "In Proceedings of the 44th Annual Meeting of the Association for Computational Linguistics", "citeRegEx": "Liu et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Liu et al\\.", "year": 2006}, {"title": "Addressing a question answering challenge by combining statistical methods with inductive rule learning and reasoning", "author": ["Mitra", "Baral2015] Arindam Mitra", "Chitta Baral"], "venue": "In Proceedings of the National Conference on Artificial Intelligence (AAAI-", "citeRegEx": "Mitra et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Mitra et al\\.", "year": 2015}, {"title": "Minimum error rate training in statistical machine translation", "author": ["Franz Josef Och"], "venue": "In Proceedings of the 41st Annual Meeting of the Association for Computational Linguistics", "citeRegEx": "Och.,? \\Q2003\\E", "shortCiteRegEx": "Och.", "year": 2003}, {"title": "Bleu: a method for automatic evaluation of machine translation", "author": ["Salim Roukos", "Todd Ward", "Wei-Jing Zhu"], "venue": "In Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics", "citeRegEx": "Papineni et al\\.,? \\Q2002\\E", "shortCiteRegEx": "Papineni et al\\.", "year": 2002}, {"title": "A synchronous hyperedge replacement grammar based approach for AMR parsing", "author": ["Peng et al.2015] Xiaochang Peng", "Linfeng Song", "Daniel Gildea"], "venue": "In Proceedings of the Nineteenth Conference on Computational Natural Language Learning", "citeRegEx": "Peng et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Peng et al\\.", "year": 2015}, {"title": "Generating english from abstract meaning representations", "author": ["Kevin Knight", "Ulf Hermjakob"], "venue": "In International Conference on Natural Language Generation", "citeRegEx": "Pourdamghani et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Pourdamghani et al\\.", "year": 2016}, {"title": "Parsing English into abstract meaning representation using syntax-based machine translation", "author": ["Pust et al.2015] Michael Pust", "Ulf Hermjakob", "Kevin Knight", "Daniel Marcu", "Jonathan May"], "venue": "In Conference on Empirical Methods in Natural Language", "citeRegEx": "Pust et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Pust et al\\.", "year": 2015}, {"title": "Amr-to-text generation as a traveling salesman problem", "author": ["Song et al.2016] Linfeng Song", "Yue Zhang", "Xiaochang Peng", "Zhiguo Wang", "Daniel Gildea"], "venue": "In Conference on Empirical Methods in Natural Language Processing", "citeRegEx": "Song et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Song et al\\.", "year": 2016}, {"title": "Neural headline generation on abstract meaning representation", "author": ["Takase et al.2016] Sho Takase", "Jun Suzuki", "Naoaki Okazaki", "Tsutomu Hirao", "Masaaki Nagata"], "venue": "In Conference on Empirical Methods in Natural Language Processing (EMNLP-", "citeRegEx": "Takase et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Takase et al\\.", "year": 2016}, {"title": "A discriminative model for semantics-to-string translation", "author": ["Chris Quirk", "Michel Galley"], "venue": "In Proceedings of the 1st Workshop on Semantics-Driven Statistical Machine Translation", "citeRegEx": "Tamchyna et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Tamchyna et al\\.", "year": 2015}, {"title": "An AMR parser for English, French, German, Spanish and Japanese and a new AMR-annotated corpus", "author": ["Arul Menezes", "Chris Quirk"], "venue": "In Proceedings of the 2015 Meeting of the North American chapter", "citeRegEx": "Vanderwende et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Vanderwende et al\\.", "year": 2015}, {"title": "A transition-based algorithm for AMR parsing", "author": ["Wang et al.2015] Chuan Wang", "Nianwen Xue", "Sameer Pradhan"], "venue": "In Proceedings of the 2015 Meeting of the North American chapter of the Association for Computational Linguistics (NAACL-15),", "citeRegEx": "Wang et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Wang et al\\.", "year": 2015}, {"title": "Stochastic inversion transduction grammars and bilingual parsing of parallel corpora", "author": ["Dekai Wu"], "venue": "Computational linguistics,", "citeRegEx": "Wu.,? \\Q1997\\E", "shortCiteRegEx": "Wu.", "year": 1997}, {"title": "A decoder for syntax-based statistical mt", "author": ["Yamada", "Knight2002] Kenji Yamada", "Kevin Knight"], "venue": "In Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics", "citeRegEx": "Yamada et al\\.,? \\Q2002\\E", "shortCiteRegEx": "Yamada et al\\.", "year": 2002}, {"title": "Amr parsing with an incremental joint model", "author": ["Zhou et al.2016] Junsheng Zhou", "Feiyu Xu", "Hans Uszkoreit", "Weiguang QU", "Ran Li", "Yanhui Gu"], "venue": "In Conference on Empirical Methods in Natural Language Processing", "citeRegEx": "Zhou et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Zhou et al\\.", "year": 2016}], "referenceMentions": [{"referenceID": 1, "context": "Abstract Meaning Representation (AMR) (Banarescu et al., 2013) is a semantic formalism encoding the meaning of a sentence as a rooted, directed graph.", "startOffset": 38, "endOffset": 62}, {"referenceID": 11, "context": "Encoding many semantic phenomena into a graph structure, AMR is useful for NLP tasks such as machine translation (Jones et al., 2012; Tamchyna et al., 2015), question answering (Mitra and Baral, 2015), summarization (Takase et al.", "startOffset": 113, "endOffset": 156}, {"referenceID": 23, "context": "Encoding many semantic phenomena into a graph structure, AMR is useful for NLP tasks such as machine translation (Jones et al., 2012; Tamchyna et al., 2015), question answering (Mitra and Baral, 2015), summarization (Takase et al.", "startOffset": 113, "endOffset": 156}, {"referenceID": 22, "context": ", 2015), question answering (Mitra and Baral, 2015), summarization (Takase et al., 2016) and event detection (Li et al.", "startOffset": 67, "endOffset": 88}, {"referenceID": 13, "context": ", 2016) and event detection (Li et al., 2015).", "startOffset": 28, "endOffset": 45}, {"referenceID": 5, "context": "Despite much literature so far on text-to-AMR parsing (Flanigan et al., 2014; Wang et al., 2015; Peng et al., 2015; Vanderwende et al., 2015; Pust et al., 2015; Artzi et al., 2015; Groschwitz et al., 2015; Goodman et al., 2016; Zhou et al., 2016), there has been little work on AMR-to-text generation (Flanigan et al.", "startOffset": 54, "endOffset": 246}, {"referenceID": 25, "context": "Despite much literature so far on text-to-AMR parsing (Flanigan et al., 2014; Wang et al., 2015; Peng et al., 2015; Vanderwende et al., 2015; Pust et al., 2015; Artzi et al., 2015; Groschwitz et al., 2015; Goodman et al., 2016; Zhou et al., 2016), there has been little work on AMR-to-text generation (Flanigan et al.", "startOffset": 54, "endOffset": 246}, {"referenceID": 18, "context": "Despite much literature so far on text-to-AMR parsing (Flanigan et al., 2014; Wang et al., 2015; Peng et al., 2015; Vanderwende et al., 2015; Pust et al., 2015; Artzi et al., 2015; Groschwitz et al., 2015; Goodman et al., 2016; Zhou et al., 2016), there has been little work on AMR-to-text generation (Flanigan et al.", "startOffset": 54, "endOffset": 246}, {"referenceID": 24, "context": "Despite much literature so far on text-to-AMR parsing (Flanigan et al., 2014; Wang et al., 2015; Peng et al., 2015; Vanderwende et al., 2015; Pust et al., 2015; Artzi et al., 2015; Groschwitz et al., 2015; Goodman et al., 2016; Zhou et al., 2016), there has been little work on AMR-to-text generation (Flanigan et al.", "startOffset": 54, "endOffset": 246}, {"referenceID": 20, "context": "Despite much literature so far on text-to-AMR parsing (Flanigan et al., 2014; Wang et al., 2015; Peng et al., 2015; Vanderwende et al., 2015; Pust et al., 2015; Artzi et al., 2015; Groschwitz et al., 2015; Goodman et al., 2016; Zhou et al., 2016), there has been little work on AMR-to-text generation (Flanigan et al.", "startOffset": 54, "endOffset": 246}, {"referenceID": 0, "context": "Despite much literature so far on text-to-AMR parsing (Flanigan et al., 2014; Wang et al., 2015; Peng et al., 2015; Vanderwende et al., 2015; Pust et al., 2015; Artzi et al., 2015; Groschwitz et al., 2015; Goodman et al., 2016; Zhou et al., 2016), there has been little work on AMR-to-text generation (Flanigan et al.", "startOffset": 54, "endOffset": 246}, {"referenceID": 9, "context": "Despite much literature so far on text-to-AMR parsing (Flanigan et al., 2014; Wang et al., 2015; Peng et al., 2015; Vanderwende et al., 2015; Pust et al., 2015; Artzi et al., 2015; Groschwitz et al., 2015; Goodman et al., 2016; Zhou et al., 2016), there has been little work on AMR-to-text generation (Flanigan et al.", "startOffset": 54, "endOffset": 246}, {"referenceID": 8, "context": "Despite much literature so far on text-to-AMR parsing (Flanigan et al., 2014; Wang et al., 2015; Peng et al., 2015; Vanderwende et al., 2015; Pust et al., 2015; Artzi et al., 2015; Groschwitz et al., 2015; Goodman et al., 2016; Zhou et al., 2016), there has been little work on AMR-to-text generation (Flanigan et al.", "startOffset": 54, "endOffset": 246}, {"referenceID": 28, "context": "Despite much literature so far on text-to-AMR parsing (Flanigan et al., 2014; Wang et al., 2015; Peng et al., 2015; Vanderwende et al., 2015; Pust et al., 2015; Artzi et al., 2015; Groschwitz et al., 2015; Goodman et al., 2016; Zhou et al., 2016), there has been little work on AMR-to-text generation (Flanigan et al.", "startOffset": 54, "endOffset": 246}, {"referenceID": 6, "context": ", 2016), there has been little work on AMR-to-text generation (Flanigan et al., 2016; Song et al., 2016; Pourdamghani et al., 2016).", "startOffset": 62, "endOffset": 131}, {"referenceID": 21, "context": ", 2016), there has been little work on AMR-to-text generation (Flanigan et al., 2016; Song et al., 2016; Pourdamghani et al., 2016).", "startOffset": 62, "endOffset": 131}, {"referenceID": 19, "context": ", 2016), there has been little work on AMR-to-text generation (Flanigan et al., 2016; Song et al., 2016; Pourdamghani et al., 2016).", "startOffset": 62, "endOffset": 131}, {"referenceID": 0, "context": ", 2015; Artzi et al., 2015; Groschwitz et al., 2015; Goodman et al., 2016; Zhou et al., 2016), there has been little work on AMR-to-text generation (Flanigan et al., 2016; Song et al., 2016; Pourdamghani et al., 2016). Flanigan et al. (2016) transform a given AMR graph into a spanning tree, before translating it #X1#", "startOffset": 8, "endOffset": 242}, {"referenceID": 21, "context": "Song et al. (2016) directly generate sentences using graph-fragment-to-string rules.", "startOffset": 0, "endOffset": 19}, {"referenceID": 16, "context": "Our system makes use of a loglinear model with real-valued features, tuned using MERT (Och, 2003), and beam search decoding.", "startOffset": 86, "endOffset": 97}, {"referenceID": 26, "context": "Our graph-to-string rules are inspired by synchronous grammars for machine translation (Wu, 1997; Yamada and Knight, 2002; Gildea, 2003; Chiang, 2005; Huang et al., 2006; Liu et al., 2006).", "startOffset": 87, "endOffset": 188}, {"referenceID": 7, "context": "Our graph-to-string rules are inspired by synchronous grammars for machine translation (Wu, 1997; Yamada and Knight, 2002; Gildea, 2003; Chiang, 2005; Huang et al., 2006; Liu et al., 2006).", "startOffset": 87, "endOffset": 188}, {"referenceID": 3, "context": "Our graph-to-string rules are inspired by synchronous grammars for machine translation (Wu, 1997; Yamada and Knight, 2002; Gildea, 2003; Chiang, 2005; Huang et al., 2006; Liu et al., 2006).", "startOffset": 87, "endOffset": 188}, {"referenceID": 10, "context": "Our graph-to-string rules are inspired by synchronous grammars for machine translation (Wu, 1997; Yamada and Knight, 2002; Gildea, 2003; Chiang, 2005; Huang et al., 2006; Liu et al., 2006).", "startOffset": 87, "endOffset": 188}, {"referenceID": 14, "context": "Our graph-to-string rules are inspired by synchronous grammars for machine translation (Wu, 1997; Yamada and Knight, 2002; Gildea, 2003; Chiang, 2005; Huang et al., 2006; Liu et al., 2006).", "startOffset": 87, "endOffset": 188}, {"referenceID": 18, "context": "step is to extract a set of initial rules from training \u3008sentence, AMR\u3009 pairs (Line 2) using the phraseto-graph fragment extraction algorithm of Peng et al. (2015) (Line 3).", "startOffset": 145, "endOffset": 164}, {"referenceID": 18, "context": "step is to extract a set of initial rules from training \u3008sentence, AMR\u3009 pairs (Line 2) using the phraseto-graph fragment extraction algorithm of Peng et al. (2015) (Line 3). Here an initial rule contains only terminal symbols in both F and E. As a next step, we match between pairs of initial rules ri and rj according to Cai and Knight (2013), and generate rij by collapsing ri with rj , if ri contains rj 2 (Line 6-8).", "startOffset": 145, "endOffset": 344}, {"referenceID": 21, "context": "In addition to induced rules, we adopt concept rules (Song et al., 2016) and graph glue rules to ensure existence of derivations.", "startOffset": 53, "endOffset": 72}, {"referenceID": 3, "context": "Inspired by Chiang (2005), we define graph glue rules to concatenate non-terminal nodes connected with an edge, when no induced rules can be applied.", "startOffset": 12, "endOffset": 26}, {"referenceID": 12, "context": "The language model score, word count and phrase count features are adopted from SMT (Koehn et al., 2003; Chiang, 2005).", "startOffset": 84, "endOffset": 118}, {"referenceID": 3, "context": "The language model score, word count and phrase count features are adopted from SMT (Koehn et al., 2003; Chiang, 2005).", "startOffset": 84, "endOffset": 118}, {"referenceID": 17, "context": "We train a 4-gram language model (LM) on gigaword (LDC2011T07), and use BLEU (Papineni et al., 2002) as the evaluation metric.", "startOffset": 77, "endOffset": 100}, {"referenceID": 16, "context": "MERT is used (Och, 2003) to tune model parameters on k-best outputs on the devset, where k is set 20.", "startOffset": 13, "endOffset": 24}, {"referenceID": 6, "context": "We also compare our method with previous works, in particular JAMR-gen (Flanigan et al., 2016) and TSP-gen (Song et al.", "startOffset": 71, "endOffset": 94}, {"referenceID": 21, "context": ", 2016) and TSP-gen (Song et al., 2016), on the same dataset.", "startOffset": 20, "endOffset": 39}, {"referenceID": 21, "context": "This observation is consistent with the observation of Song et al. (2016) for their TSP-based system.", "startOffset": 55, "endOffset": 74}], "year": 2017, "abstractText": "This paper addresses the task of AMR-totext generation by leveraging synchronous node replacement grammar. During training, graph-to-string rules are learned using a heuristic extraction algorithm. At test time, a graph transducer is applied to collapse input AMRs and generate output sentences. Evaluated on SemEval-2016 Task 8, our method gives a BLEU score of 25.62, which is the best reported so far.", "creator": "LaTeX with hyperref package"}}}