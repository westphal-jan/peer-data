{"id": "1702.00763", "review": {"conference": "ICML", "VERSION": "v1", "DATE_OF_SUBMISSION": "2-Feb-2017", "title": "Natasha: Faster Non-Convex Stochastic Optimization via Strongly Non-Convex Parameter", "abstract": "Given a non-convex function $f(x)$ that is an average of $n$ smooth functions, we design stochastic first-order methods to find its approximate stationary points. The performance of our new methods depend on the smallest (negative) eigenvalue $-\\sigma$ of the Hessian. This parameter $\\sigma$ captures how strongly non-convex $f(x)$ is, and is analogous to the strong convexity parameter for convex optimization.", "histories": [["v1", "Thu, 2 Feb 2017 17:45:09 GMT  (1325kb,D)", "http://arxiv.org/abs/1702.00763v1", null], ["v2", "Mon, 27 Feb 2017 03:50:25 GMT  (1040kb,D)", "http://arxiv.org/abs/1702.00763v2", "typos"], ["v3", "Tue, 5 Sep 2017 09:37:06 GMT  (1047kb,D)", "http://arxiv.org/abs/1702.00763v3", "Corrected several typos and polished writing"]], "reviews": [], "SUBJECTS": "math.OC cs.DS cs.LG stat.ML", "authors": ["zeyuan allen-zhu"], "accepted": true, "id": "1702.00763"}, "pdf": {"name": "1702.00763.pdf", "metadata": {"source": "CRF", "title": "Natasha: Faster Stochastic Non-Convex Optimization Via Strongly Non-Convex Parameter", "authors": ["Zeyuan Allen-Zhu"], "emails": ["zeyuan@csail.mit.edu"], "sections": [{"heading": null, "text": "Our methods exceed the best known results in a wide range of \u03c3 and can also be used to find approximate local minimums. In particular, we find an interesting dichotomy: there is a threshold \u03c30, so that the fastest methods for \u03c3 > \u03c30 and for \u03c3 < \u03c30 exhibit drastically different behaviours: the former scales with n2 / 3 and the latter scales with n3 / 4."}, {"heading": "1 Introduction", "text": "We examine the basic problem of compound nonconvex minimization: min x Rd {F (x) def = vice versa (x) + f (x) def = vice versa (x) + 1nn \u2211 i = 1fi (x)} (1,1), where each fi (x) is not convex but smooth, and vice versa (\u00b7) is properly convex and relatively simple. We are interested in finding a point x, which is an approximate local minimum of F (x). \u2022 The finite sum structure f (x) = 1n \u2211 ni = 1 fi (x) is clearly present in major problems, especially in machine learning. Especially when minimizing losses over a training set, each example i can correspond to the amount of a loss function fi (\u00b7) in the summation. This finite sum structure allows one to perform a stochastic descent in relation to a random convection. \u2022 The additional term \"Xixi\" (proximal), commonly referred to as a flexibility problem, adds."}, {"heading": "1.1 Strongly Non-Convex Optimization", "text": "We concentrate mainly on the case when each fi (x) L is smooth, which means that all eigenvalues of \u0430 2fi (x) are automatically in [\u2212 L, L], and their average f (x) = 1n \u2211 n i = 1 fi (x) is elastic-strongly non-convex, all eigenvalues of \u0432 2f (x) in [\u2212 \u03c3, L] are significant for some \u03b5 [0, L].1We emphasize here that this parameter is analogous to the strongly convex parameter \u00b5 for convex optimization, where all eigenvalues of \u0432 2f (x) in [\u00b5, L] for some \u00b5 > 0.We would like to find an \u03b5-approximate fixed point (a.k.a. critical point) of F (x), which is any point x-satisfactory that denotes G\u03b7 (x) as a minimum convex point, where G (x) is the so-called gradient mapping of F (x)."}, {"heading": "1.2 Known Results", "text": "Despite the widespread use of non-convex models in machine learning and related fields, we refer to complexity as the total number of non-convex optimizations (a total of 12x). Until recently, almost all research has focused on a point that is either f (x) or f (x) convex, or L so (x) simply L-smooth: \u2022 If the accelerated SVRG method [8, 21] finds a point x-approximate stationary point of F (x) -F (x) \u2212 F (x) \u2264 \u03b5, where gradient complexity O (n + n2 / 3L / 2 L) is simply L / \u03b5-smooth: \u2022 If the SVRG method [3] finds a point x-approximate stationary point of F (x) with gradient complexity O (n + n2 / 3L / 2 L) \u2264 \u03b5. \u2022 If we find an approximate point x-approximate point of F (if we find a point x) of gradient complexity of O (if we find a point)."}, {"heading": "1.4 Our Techniques", "text": "Let us first recall the main ideas of stochastic, variance-reduced gradient methods, such as SVRG [14]. The SVRG method provides a snapshot point x for each epoch of n iterations, and calculates the complete gradient f (x) only for snapshots. However, SVRG then defines a gradient estimator x (x) iteration point x (x) \u2212 iteration point x (x) \u2212 iteration point that satisfies egg [x] = f (xt), and performs a proximal update x (x). SVRG method x (x) is used for some learning rates x. 5Note that the epochal length of the SVRG is always n (or a constant multiple of n in practice), because this ensures the calculation of the n-iteration we use for the amortized gradient complexity O (1)."}, {"heading": "1.5 Other Related Work", "text": "The first such method is SAG by Schmidt et al. [20]. The two most popular choices for gradient estimators are the SVRG-like method we adopted in this paper (independently introduced by [14, 22], and the SAGA-like method introduced by [10]. In almost all applications, the results for SVRG-like estimators and SAGA-like estimators are easily interchangeable (hence, the results of this paper naturally generalize to SAGA-like estimators). The first \"non-convex use\" of variance reduction is by Shalev-Shwartz [21], who assumes that each fi (x) is not convex, but its average f (x) is still convex. This result has been slightly improved to several refined settings [8]."}, {"heading": "2 Preliminaries", "text": "In this paper, we refer to the Euclidean norm as uniformly random. We refer to the full gradient of the function f (x) if it is differentiable, and the subgradient f (x) if f is only continuous at point x. We let x be any minimizer of the function f (x). Let us call some definitions of strong convectivity (SC), strong nonconvectivity, and smoothness. Definition 2.1. For a function f: Rd \u2192 R, \u2022 f is a strongly convex form of F (x) + < f: convectivity f (x), f: x."}, {"heading": "In particular, if \u03c8(\u00b7) \u2261 0, then G\u03b7(x) \u2261 \u2207f(x).", "text": "For example, the following theorem for the SVRG method can be found in [8], which is based on the results [11, 16, 21]: Theorem 2.3 (SVRG). If G (y) def = more economic (y) + 1n = 1 gi (y) -strongly convex, then the SVRG method finds a point y which is G (y) -G (y) \u2264 \u03b5 \u2022 with gradient complexity O (((n + L 2\u03c32) -smooth (for '1,' 2) -smooth (for L); or \u2022 with gradient complexity O (n + '2\u04412) log 1\u03b5), if each gi (\u00b7) -smooth (for' 1, '2 \u0430). If acceleration is performed, the runtimes become O (n + n3 / 4 \u04452) and O (n + n3 / 4) -smooth (for' 1, '2 \u0430) (for' 2 '\u043c)."}, {"heading": "2.1 RepeatSVRG", "text": "We remember the idea behind a simple algorithm - which we call repeatSVRG - that finds the approximate stationary points for problem (1,1) if f (x) is strongly non-convex. The algorithm is divided into iterations. Let's consider in each iteration t a modified function Ft (x) def = F (x) + \u00b2 x \u2212 xt \u00b2 2. It is easy to see that Ft (x) is now elastically strongly convex, so that one can use the accelerated SVRG method to minimize Ft (x). Let's make xt + 1 be a sufficiently accurate approximate minimizer for Ft (x). 8Now we can (c.f. Section 4) prove that xt + 1 is an O (xt \u2212 xt + 1) approximate stationary point for F (x)."}, {"heading": "3 Our Algorithms", "text": "We present two variants of our algorithms: (1) Basic method Natasha targets the simple system, if f (x) and each fi (x) are both L-smooth, and (2) Complete method Natashafull targets the more sophisticated system, if f (x) is smooth, but each fi (x) is ('1,' 2) -smooth. Both methods follow the general idea of stochastic gradient lineage reduced in variance: in each innermost iteration, they calculate a gradient estimator, which is of the form that it is (x) -f (x) -smooth."}, {"heading": "4 A Sufficient Stopping Criterion", "text": "In this section, we represent a sufficient condition for determining approximate stationary points in a range that is not convex. (Lemma 4.1) Below is that if we use the original function with G (x) def = F (x) + x (x) + x (x) + x (2) + x (2) + x (1) + x (2) an approximate saddle point for F (x).Lemma 4.1. Assume G (y) + x (y) + x (2) + x) for any point x (2), and let x (x) be the minimizer of G (y)."}, {"heading": "5 Base Method: Analysis for One Full Epoch", "text": "In this section we will consider problem (1,1) where each fi (x) L is smooth and F (x) L approximate convex. We use our basic method Natasha to minimize F (x) and analyze its behavior for a full epoch in this section. We assume that \u03c3 \u2264 L is without loss of universality, because each L-smooth function is also L-strong convex. We will introduce some notations only for analysis purposes. \u2022 Let x-smooth be the value of x-smooth at the beginning of the sub-epoch. \u2022 Let xst be the value of xt in sub-epochs. \u2022 Let the value of i-smooth (n) in sub-epochs be iterated. \u2022 Let fs (x) def = f (x) + x-smooth f-f (x) and f-smooth (x) is x-smooth (x)."}, {"heading": "5.1 Variance Upper Bound", "text": "The following problem gives an upper limit for the variance of the gradient estimator: Lemma 5.1. We have Eist [VP] -fs (xst) -fs (xst) -2] \u2264 pL2-xst \u2212 x-s-2 + pL2-s \u2212 1 k = 0 x-k \u2212 x-k + 1-2.Proof. We have Eist [VP) -fs (xst) -x-s-2] = Eist [VP) -f (xst) -f (xst) -2] = Eist [f (xst) -f (xst) -2] = Ei-fi (xst) \u2212 fi (x-f))) \u2212 (VP (x-st) \u00b7 f (x-f)))))))."}, {"heading": "5.2 Analysis for One Sub-Epoch", "text": "The following inequality is classically known as \"repentance inequality\" for mirror deviation [7]. < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < <"}, {"heading": "5.3 Analysis for One Full Epoch", "text": "We telescope Lemma 5.3 for an entire epoch and arrive at the following problem: Lemma 5.4. If \u03b1 \u2264 12L + 4\u03c3, \u03b1 \u2265 4\u03c3m and \u03b1 \u2264 \u03c3p2L2, then we have p \u2212 1 \u2211 s = 0E [(F s (x-s) \u2212 F s (xs)))] \u2264 2E [F (x-0) \u2212 F (x-p)]. Proof. Telescoping Lemma 5.3 for all sub-epochs s = 0, 1,...., p \u2212 1, we have p \u2212 1 \u0445 s = 0E [(F s (x-s + 1) \u2212 F s (xs)))."}, {"heading": "6 Base Method: Final Theorem", "text": "We are now ready to specify our main convergence theorem for Natasha and to prove: Theorem 1: If we (1.1) assume that each fi (x) x-x-x-x-x-x-x-x-x-x-x-x-x-convex-convex-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-"}, {"heading": "7 Full Method: Analysis for One Full Epoch", "text": "In this section we will examine a more refined version of the problem (1,1) in which f (x) is smooth, each fi (x) is smooth ('1,' 2) -smooth and F (x) is approximate-convex. As explained later in note 8.1, we can assume that the parameter L in the specification of Natashafull ('1,' 2, L} is almost without loss of generality. \u2212 We use our complete method Natashafull to minimize F (x) and to analyze its behavior for a full epoch in this section."}, {"heading": "7.1 Variance Upper Bound", "text": "In this subsection, we derive a new upper limit for the variance of the gradient valuer. < < p > p > p > p > p (1) p (1) p (2) p (2) p (2) p (2) p (2) p (2) p (2) p (2) p (2) p (2) p (2) p (2) p (2) p (2) p (2) p (2) p (2) p (2) p) p (2) p (2) p) p (2) p) p (2) p) p (2) p) p (2) p) p (2) p) p (2) p (2) p) p (2) p (2) p) p (2) p) p (2) p (p) p (2) p (p) p) p (2) p) p (2) p (p) p) p (2) p) p (2) p (p) p) p (2) p) p (2) p) p (2) p) p (2) p) p (2) p) p (2) p) p) p (2 p) p) p (2 p) p) p (2 p) p) p (2 p) p) p) p (2 p) p) p). < < p < p > p > p > p > p > p > p > p > p > p (1) p (1) p (1) p (2) p (2) p (2) p (2) p (2) p (2) p (2) p (2) p (2) p (2) p (2) p) p (2) p) p (2) p) p (2) p (2) p) p) p (2) p) p (2) p) p) p (2 p) p) p) p (2 p) p) p) p (2 p) p) p (2 p) p) p)."}, {"heading": "7.2 Analysis for One Sub-Epoch", "text": "The following fact is analogous to fact 5.2, and the only difference is that in Natashafull (+ >) we have preliminary updates to the {zst} t-sequence.Fact 7.3. < \u2212 fps (+ 1 \u2212 fps), zst + 1 \u2212 fps (zst + 1), \u2212 fps (zst), (u), (u), (u), (u), (u), (u), (u), (u), (u), (u), (u), (u), (u), (u), (u), (u), (u), (u), (u), (u), (u), (u, (), (), (), (), (,), (, (,), (, (,), (,), (, (,), (, (,), (, (,), (, (,), (, (,), (,), (, (,), (,), (, (,), (,), (, (,), (,), (, (,), (,), (, (,), (,), (, (,), (,), (, (,), (, (,), (,), (, (,), (), (), (), (, (), (, (, (,), (, (,), (), (), (), (, (), (), (, (), (), (, (, (), (), (, (), (), (, (, (), (), (, (, (, (,), (, (,), (, (, (, (,), (,), (, (, (,), (, (), (, (), (, (), (, (), (, (), (, (, (, (, (,), (, (, (,), (, (, (,), (, (, (,), (,), (, (, (,"}, {"heading": "7.3 Analysis for One Full Epoch", "text": "We use the telescope Lemma 7.4 for an entire epoch and arrive at the following problem: Lemma 7.5. If \u03b1 \u2264 O (\u03c3 p2 '1' 2) and \u03b1 (1\u03c3m), we have p \u2212 1 \u00b2 s = 0E [(F s (x s) \u2212 F s (xs) \u2212 F s (xs) = 0,., p \u2212 1, we have p \u2212 1 \u00b2 s (F s (x \u00b2 s + 1) \u2212 F s (xs) = 0,., p \u2212 1, we have p \u2212 1 \u00b2 s (F s (x \u00b2 s + 1) \u2212 F (x \u00b2 s (xs)) \u2212 F \u2212 1 \u00b2 s = 0E [F s (x \u00b2 s) \u2212 fs (xs) \u2212 F s (xs (xs) \u2212 s (xs (xs), p \u00b2 s (x \u00b2 s), x \u00b2 s (x \u00b2 s), x \u00b2 s (x \u00b2 s), x \u00b2 s (x \u00b2 s), x \u00b2 s (x \u00b2 s), x \u00b2 s (x \u00b2 s), x s \u00b2 s (x \u00b2 s))."}, {"heading": "8 Full Method: Final Theorem", "text": "We are now ready to state our most important convergence theorems for Natashafull and to prove: Theorem 2. Let us assume f (x) is L-smooth, each fi (x) is (\"1,\" 2) -smooth, and F (x) is \"s-strong convex,\" because \"1,\" 2, \"L.\" If \"1\" 2, \"2\" s-smooth (\"1,\" 2 \"s-smooth\"), and F (x) is \"s-strong convex,\" for \"s-smooth\" (1, \"2,\" L. \"If\" s-smooth, \"2,\" 2. \"If\" s-smooth. \""}, {"heading": "Acknowledgements", "text": "This work is partially supported by a Microsoft Research Award, No. 0518584, and an NSF grant, No. CCF-1412958. We thank Yuanzhi Li for his insightful discussions."}, {"heading": "A From Stationary Points to Local Minima", "text": "Recently, researchers have shown that the general problem of determining (\u03b5, \u03c1) approximate local minimums under mild conditions is reduced to (repeated) finding \u03b5 approximate stationary points for an O (\u03c1) strongly non-convex function [1, 9]. We will sketch the details here for the sake of completeness, in the particular case of \u03b5 (x) approximate stationary points for an O (\u03c1) approximate local minimum function [1, 9]. We say that a point x (\u03b5, \u03b4) approximate local minimum value is when the iteration at the point xt and 2f (x) \u2212. Carmon et al. [9] showed that a (\u03b5, \u03b4) approximate minimum value for the general problem (1,1) can be solved by the following iterative procedure. In each iteration at the point xt, we can determine whether the smallest eigenvalue of the two points (xt) below \u2212 is less harmful than the smallest eigenvalue of the latter 2xt (x)."}], "references": [{"title": "Finding Approximate Local Minima for Nonconvex Optimization in Linear Time", "author": ["Naman Agarwal", "Zeyuan Allen-Zhu", "Brian Bullins", "Elad Hazan", "Tengyu Ma"], "venue": "ArXiv e-prints,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2016}, {"title": "Katyusha: The First Direct Acceleration of Stochastic Gradient Methods", "author": ["Zeyuan Allen-Zhu"], "venue": "ArXiv e-prints,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2016}, {"title": "Variance Reduction for Faster Non-Convex Optimization", "author": ["Zeyuan Allen-Zhu", "Elad Hazan"], "venue": "In NIPS,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2016}, {"title": "Doubly Accelerated Methods for Faster CCA and Generalized Eigendecomposition", "author": ["Zeyuan Allen-Zhu", "Yuanzhi Li"], "venue": "ArXiv e-prints,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2016}, {"title": "LazySVD: Even Faster SVD Decomposition Yet Without Agonizing Pain", "author": ["Zeyuan Allen-Zhu", "Yuanzhi Li"], "venue": "In NIPS,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2016}, {"title": "Follow the Compressed Leader: Faster Algorithm for Matrix Multiplicative Weight Updates", "author": ["Zeyuan Allen-Zhu", "Yuanzhi Li"], "venue": "ArXiv e-prints,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2017}, {"title": "Linear Coupling: An Ultimate Unification of Gradient and Mirror Descent", "author": ["Zeyuan Allen-Zhu", "Lorenzo Orecchia"], "venue": "In Proceedings of the 8th Innovations in Theoretical Computer Science, ITCS \u201917,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2017}, {"title": "Improved SVRG for Non-Strongly-Convex or Sum-of-Non- Convex Objectives", "author": ["Zeyuan Allen-Zhu", "Yang Yuan"], "venue": "In ICML,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2016}, {"title": "Accelerated Methods for Non-Convex Optimization", "author": ["Yair Carmon", "John C. Duchi", "Oliver Hinder", "Aaron Sidford"], "venue": "ArXiv e-prints,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2016}, {"title": "SAGA: A Fast Incremental Gradient Method With Support for Non-Strongly Convex Composite Objectives", "author": ["Aaron Defazio", "Francis Bach", "Simon Lacoste-Julien"], "venue": "In NIPS,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2014}, {"title": "Un-regularizing: approximate proximal point and faster stochastic algorithms for empirical risk minimization", "author": ["Roy Frostig", "Rong Ge", "Sham M. Kakade", "Aaron Sidford"], "venue": "In ICML,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2015}, {"title": "Robust shift-and-invert preconditioning: Faster and more sample efficient algorithms for eigenvector computation", "author": ["Dan Garber", "Elad Hazan", "Chi Jin", "Sham M. Kakade", "Cameron Musco", "Praneeth Netrapalli", "Aaron Sidford"], "venue": null, "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2016}, {"title": "Escaping from saddle points\u2014online stochastic gradient for tensor decomposition", "author": ["Rong Ge", "Furong Huang", "Chi Jin", "Yang Yuan"], "venue": "In Proceedings of the 28th Annual Conference on Learning", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2015}, {"title": "Accelerating stochastic gradient descent using predictive variance reduction", "author": ["Rie Johnson", "Tong Zhang"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2013}, {"title": "Gradient descent only converges to minimizers", "author": ["Jason D. Lee", "Max Simchowitz", "Michael I. Jordan", "Benjamin Recht"], "venue": "In Proceedings of the 29th Conference on Learning Theory, COLT 2016,", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2016}, {"title": "A Universal Catalyst for First-Order Optimization", "author": ["Hongzhou Lin", "Julien Mairal", "Zaid Harchaoui"], "venue": "In NIPS,", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2015}, {"title": "Introductory Lectures on Convex Programming Volume: A Basic course, volume I", "author": ["Yurii Nesterov"], "venue": "Kluwer Academic Publishers,", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2004}, {"title": "Accelerating the cubic regularization of newton\u2019s method on convex problems", "author": ["Yurii Nesterov"], "venue": "Mathematical Programming,", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2008}, {"title": "Stochastic variance reduction for nonconvex optimization", "author": ["Sashank J. Reddi", "Ahmed Hefny", "Suvrit Sra", "Barnabas Poczos", "Alex Smola"], "venue": "ArXiv e-prints,", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2016}, {"title": "Minimizing finite sums with the stochastic average gradient", "author": ["Mark Schmidt", "Nicolas Le Roux", "Francis Bach"], "venue": "arXiv preprint arXiv:1309.2388,", "citeRegEx": "20", "shortCiteRegEx": "20", "year": 2013}, {"title": "SDCA without Duality, Regularization, and Individual Convexity", "author": ["Shai Shalev-Shwartz"], "venue": "In ICML,", "citeRegEx": "21", "shortCiteRegEx": "21", "year": 2016}, {"title": "Linear convergence with condition number independent access of full gradients", "author": ["Lijun Zhang", "Mehrdad Mahdavi", "Rong Jin"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "22", "shortCiteRegEx": "22", "year": 2013}], "referenceMentions": [{"referenceID": 2, "context": "In many interesting practical problems \u2014such as training neural nets and classifications with sigmoid loss, see [3] for details\u2014 neither fi(x) or the overall f(x) is convex.", "startOffset": 112, "endOffset": 115}, {"referenceID": 17, "context": "In particular, second-order literatures usually find (\u03b5, \u221a \u03b5)-approximate local minima [18], and this corresponds to \u03c3 = \u221a \u03b5.", "startOffset": 87, "endOffset": 91}, {"referenceID": 7, "context": "Until recently, nearly all research papers have been mostly focusing on either \u03c3 = 0 so f(x) is convex, or \u03c3 = L so f(x) is simply L-smooth: \u2022 If \u03c3 = 0, the accelerated SVRG method [8, 21] find a point x satisfying F (x) \u2212 F (x\u2217) \u2264 \u03b5, in gradient complexity \u00d5 ( n+ n3/4 \u221a L/\u03b5 ) .", "startOffset": 181, "endOffset": 188}, {"referenceID": 20, "context": "Until recently, nearly all research papers have been mostly focusing on either \u03c3 = 0 so f(x) is convex, or \u03c3 = L so f(x) is simply L-smooth: \u2022 If \u03c3 = 0, the accelerated SVRG method [8, 21] find a point x satisfying F (x) \u2212 F (x\u2217) \u2264 \u03b5, in gradient complexity \u00d5 ( n+ n3/4 \u221a L/\u03b5 ) .", "startOffset": 181, "endOffset": 188}, {"referenceID": 2, "context": "2 \u2022 If \u03c3 = L, the SVRG method [3] finds an \u03b5-approximate stationary point of F (x) with gradient complexity O(n+ n2/3L/\u03b52).", "startOffset": 30, "endOffset": 33}, {"referenceID": 0, "context": "Very recently, it was observed by two independent groups [1, 9] \u2014although implicitly, see Section 2.", "startOffset": 57, "endOffset": 63}, {"referenceID": 8, "context": "Very recently, it was observed by two independent groups [1, 9] \u2014although implicitly, see Section 2.", "startOffset": 57, "endOffset": 63}, {"referenceID": 2, "context": "Note that our result matches that of SVRG [3] for \u03c3 = L, and has a much simpler analysis.", "startOffset": 42, "endOffset": 45}, {"referenceID": 11, "context": "Using the so-called shift-and-invert reduction [12], computing leading eigenvector reduces to a convex version of problem (1.", "startOffset": 47, "endOffset": 51}, {"referenceID": 4, "context": "Other examples include all the applications that are built on shift-and-invert, including high rank SVD/PCA [5], canonical component analysis [4], online matrix learning [6], and approximate local minima algorithms [1, 9].", "startOffset": 108, "endOffset": 111}, {"referenceID": 3, "context": "Other examples include all the applications that are built on shift-and-invert, including high rank SVD/PCA [5], canonical component analysis [4], online matrix learning [6], and approximate local minima algorithms [1, 9].", "startOffset": 142, "endOffset": 145}, {"referenceID": 5, "context": "Other examples include all the applications that are built on shift-and-invert, including high rank SVD/PCA [5], canonical component analysis [4], online matrix learning [6], and approximate local minima algorithms [1, 9].", "startOffset": 170, "endOffset": 173}, {"referenceID": 0, "context": "Other examples include all the applications that are built on shift-and-invert, including high rank SVD/PCA [5], canonical component analysis [4], online matrix learning [6], and approximate local minima algorithms [1, 9].", "startOffset": 215, "endOffset": 221}, {"referenceID": 8, "context": "Other examples include all the applications that are built on shift-and-invert, including high rank SVD/PCA [5], canonical component analysis [4], online matrix learning [6], and approximate local minima algorithms [1, 9].", "startOffset": 215, "endOffset": 221}, {"referenceID": 13, "context": "We first recall the main ideas of stochastic variance-reduced gradient methods, such as SVRG [14].", "startOffset": 93, "endOffset": 97}, {"referenceID": 0, "context": "For instance, the result of [1, 9] requires \u03b5 \u2264 \u03c3.", "startOffset": 28, "endOffset": 34}, {"referenceID": 8, "context": "For instance, the result of [1, 9] requires \u03b5 \u2264 \u03c3.", "startOffset": 28, "endOffset": 34}, {"referenceID": 2, "context": "In nearly all the aforementioned results for strongly nonconvex optimization, researchers have either directly applied SVRG [3, 19] (for the case \u03c3 = L), or repeatedly applied SVRG [1, 9] (for general \u03c3 \u2208 [0, L]).", "startOffset": 124, "endOffset": 131}, {"referenceID": 18, "context": "In nearly all the aforementioned results for strongly nonconvex optimization, researchers have either directly applied SVRG [3, 19] (for the case \u03c3 = L), or repeatedly applied SVRG [1, 9] (for general \u03c3 \u2208 [0, L]).", "startOffset": 124, "endOffset": 131}, {"referenceID": 0, "context": "In nearly all the aforementioned results for strongly nonconvex optimization, researchers have either directly applied SVRG [3, 19] (for the case \u03c3 = L), or repeatedly applied SVRG [1, 9] (for general \u03c3 \u2208 [0, L]).", "startOffset": 181, "endOffset": 187}, {"referenceID": 8, "context": "In nearly all the aforementioned results for strongly nonconvex optimization, researchers have either directly applied SVRG [3, 19] (for the case \u03c3 = L), or repeatedly applied SVRG [1, 9] (for general \u03c3 \u2208 [0, L]).", "startOffset": 181, "endOffset": 187}, {"referenceID": 1, "context": "The technique of having the gradients computed at a point xt but moving with respect to a different sequence zt is related to the Katyusha momentum recently developed for convex optimization [2].", "startOffset": 191, "endOffset": 194}, {"referenceID": 19, "context": "The first such method is SAG by Schmidt et al [20].", "startOffset": 46, "endOffset": 50}, {"referenceID": 13, "context": "The two most popular choices for gradient estimators are the SVRG-like one we adopted in this paper (independently introduced by [14, 22], and the SAGA-like one introduced by [10].", "startOffset": 129, "endOffset": 137}, {"referenceID": 21, "context": "The two most popular choices for gradient estimators are the SVRG-like one we adopted in this paper (independently introduced by [14, 22], and the SAGA-like one introduced by [10].", "startOffset": 129, "endOffset": 137}, {"referenceID": 9, "context": "The two most popular choices for gradient estimators are the SVRG-like one we adopted in this paper (independently introduced by [14, 22], and the SAGA-like one introduced by [10].", "startOffset": 175, "endOffset": 179}, {"referenceID": 20, "context": "The first \u201cnon-convex use\u201d of variance reduction is by Shalev-Shwartz [21] who assumes that each fi(x) is non-convex but their average f(x) is still convex.", "startOffset": 70, "endOffset": 74}, {"referenceID": 7, "context": "This result has been slightly improved to several more refined settings [8].", "startOffset": 72, "endOffset": 75}, {"referenceID": 2, "context": ", for f(x) being also non-convex) is independently by [3] and [19].", "startOffset": 54, "endOffset": 57}, {"referenceID": 18, "context": ", for f(x) being also non-convex) is independently by [3] and [19].", "startOffset": 62, "endOffset": 66}, {"referenceID": 17, "context": "The first such result is by cubic regularized Newton\u2019s method [18]; however, its per-iteration complexity is very slow.", "startOffset": 62, "endOffset": 66}, {"referenceID": 0, "context": "Very recently, two independent groups of authors tackled this problem from a somewhat similar viewpoint [1, 9]: if the computation of Hessian-vector multiplications (i.", "startOffset": 104, "endOffset": 110}, {"referenceID": 8, "context": "Very recently, two independent groups of authors tackled this problem from a somewhat similar viewpoint [1, 9]: if the computation of Hessian-vector multiplications (i.", "startOffset": 104, "endOffset": 110}, {"referenceID": 8, "context": "[9] only stated a complexity of \u00d5 ( n \u03b51.", "startOffset": 0, "endOffset": 3}, {"referenceID": 12, "context": "[13] where the authors showed that a noise-injected version of SGD converges to local minima instead of critical points, as long as the underlying function is \u201cstrict-saddle.", "startOffset": 0, "endOffset": 4}, {"referenceID": 14, "context": "[15] showed that gradient descent, starting from a random point, almost surely converges to a local minimum of a \u201cstrict-saddle\u201d function.", "startOffset": 0, "endOffset": 4}, {"referenceID": 7, "context": "The (`1, `2)-smoothness parameters were introduced in [8] to tackle the convex setting of problem (1.", "startOffset": 54, "endOffset": 57}, {"referenceID": 7, "context": "The notion of strong nonconvexity is also known as \u201clower smoothness [8]\u201d or \u201calmost convexity [9]\u201d.", "startOffset": 69, "endOffset": 72}, {"referenceID": 8, "context": "The notion of strong nonconvexity is also known as \u201clower smoothness [8]\u201d or \u201calmost convexity [9]\u201d.", "startOffset": 95, "endOffset": 98}, {"referenceID": 7, "context": "The following theorem for the SVRG method can be found for instance in [8], which is built on top of the results [11, 16, 21]:", "startOffset": 71, "endOffset": 74}, {"referenceID": 10, "context": "The following theorem for the SVRG method can be found for instance in [8], which is built on top of the results [11, 16, 21]:", "startOffset": 113, "endOffset": 125}, {"referenceID": 15, "context": "The following theorem for the SVRG method can be found for instance in [8], which is built on top of the results [11, 16, 21]:", "startOffset": 113, "endOffset": 125}, {"referenceID": 20, "context": "The following theorem for the SVRG method can be found for instance in [8], which is built on top of the results [11, 16, 21]:", "startOffset": 113, "endOffset": 125}, {"referenceID": 0, "context": "We remark here that the above complexity of repeatSVRG can be inferred from papers [1, 9], but is not explicitly stated.", "startOffset": 83, "endOffset": 89}, {"referenceID": 8, "context": "We remark here that the above complexity of repeatSVRG can be inferred from papers [1, 9], but is not explicitly stated.", "startOffset": 83, "endOffset": 89}, {"referenceID": 8, "context": "For instance, the paper [9] does not allow F (x) to have a non-smooth proximal term \u03c8(x), and applies accelerated gradient descent instead of accelerated SVRG.", "startOffset": 24, "endOffset": 27}, {"referenceID": 8, "context": "(Notice that when \u03c8(x) \u2261 0 this lemma is trivial, and can be found for instance in [9]).", "startOffset": 83, "endOffset": 86}, {"referenceID": 1, "context": "The idea for this second kind of retraction, and the idea of having the updates on a sequence zt but computing gradients at points xt, is largely motivated by our recent work on the Katyusha momentum and the Katyusha acceleration [2].", "startOffset": 230, "endOffset": 233}, {"referenceID": 6, "context": "The following inequality is classically known as the \u201cregret inequality\u201d for mirror descent [7].", "startOffset": 92, "endOffset": 95}, {"referenceID": 16, "context": "5 in textbook [17]).", "startOffset": 14, "endOffset": 18}, {"referenceID": 0, "context": "Recently, researchers have shown that the general problem of finding (\u03b5, \u03c1)-approximate local minima, under mild conditions, reduces to (repeatedly) finding \u03b5-approximate stationary points for an O(\u03c1)-strongly nonconvex function [1, 9].", "startOffset": 229, "endOffset": 235}, {"referenceID": 8, "context": "Recently, researchers have shown that the general problem of finding (\u03b5, \u03c1)-approximate local minima, under mild conditions, reduces to (repeatedly) finding \u03b5-approximate stationary points for an O(\u03c1)-strongly nonconvex function [1, 9].", "startOffset": 229, "endOffset": 235}, {"referenceID": 8, "context": "[9] showed that an (\u03b5, \u03b4)-approximate minimum for the general problem (1.", "startOffset": 0, "endOffset": 3}, {"referenceID": 11, "context": "(One can use for instance the shift-and-invert method [12].", "startOffset": 54, "endOffset": 58}, {"referenceID": 0, "context": "As argued in [1, 9], if the Hessian-vector multiplication ( \u2207fi(x) ) v for an arbitrary vector runs in the same time as computing \u2207fi(x) \u2014which is the case for training neural nets\u2014 the optimum trade-off is \u03b4 = \u221a L2\u03b5.", "startOffset": 13, "endOffset": 19}, {"referenceID": 8, "context": "As argued in [1, 9], if the Hessian-vector multiplication ( \u2207fi(x) ) v for an arbitrary vector runs in the same time as computing \u2207fi(x) \u2014which is the case for training neural nets\u2014 the optimum trade-off is \u03b4 = \u221a L2\u03b5.", "startOffset": 13, "endOffset": 19}], "year": 2017, "abstractText": "Given a non-convex function f(x) that is an average of n smooth functions, we design stochastic first-order methods to find its approximate stationary points. The performance of our new methods depend on the smallest (negative) eigenvalue \u2212\u03c3 of the Hessian. This parameter \u03c3 captures how strongly non-convex f(x) is, and is analogous to the strong convexity parameter for convex optimization. Our methods outperform the best known results for a wide range of \u03c3, and can also be used to find approximate local minima. In particular, we find an interesting dichotomy: there exists a threshold \u03c30 so that the fastest methods for \u03c3 > \u03c30 and for \u03c3 < \u03c30 have drastically different behaviors: the former scales with n and the latter scales with n.", "creator": "LaTeX with hyperref package"}}}