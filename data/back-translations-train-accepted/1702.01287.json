{"id": "1702.01287", "review": {"conference": "ACL", "VERSION": "v1", "DATE_OF_SUBMISSION": "4-Feb-2017", "title": "Doubly-Attentive Decoder for Multi-modal Neural Machine Translation", "abstract": "We introduce a Multi-modal Neural Machine Translation model in which a doubly-attentive decoder naturally incorporates spatial visual features obtained using pre-trained convolutional neural networks, bridging the gap between image description and translation. Our decoder learns to attend to source-language words and parts of an image independently by means of two separate attention mechanisms as it generates words in the target language. We find that our model can efficiently exploit not just back-translated in-domain multi-modal data but also large general-domain text-only MT corpora. We also report state-of-the-art results on the Multi30k data set.", "histories": [["v1", "Sat, 4 Feb 2017 13:46:53 GMT  (454kb,D)", "http://arxiv.org/abs/1702.01287v1", "8 pages (11 including references), 2 figures"]], "COMMENTS": "8 pages (11 including references), 2 figures", "reviews": [], "SUBJECTS": "cs.CL", "authors": ["iacer calixto", "qun liu", "nick campbell"], "accepted": true, "id": "1702.01287"}, "pdf": {"name": "1702.01287.pdf", "metadata": {"source": "CRF", "title": "Doubly-Attentive Decoder for Multi-modal Neural Machine Translation", "authors": ["Iacer Calixto", "Qun Liu"], "emails": ["iacer.calixto@adaptcentre.ie"], "sections": [{"heading": "1 Introduction", "text": "In recent years, it has become clear that the problem is not only a problem, but also a problem that we must solve. (...) We are dealing with a problem that we cannot solve. (...) We are dealing with a problem that we cannot solve. (...) We are dealing with a problem. (...) We are dealing with a problem. (...) We are dealing with a problem that we cannot solve. (...) We are dealing with a problem. (...) We are dealing with a problem. (...) We are dealing with a problem. (...) We are dealing with a problem. (...) We are dealing with a problem. \"(...) We are not going to solve it. (...) We are not going to solve it.\" (...) We are not going to solve it. \"(...) (...) We are not going to solve it. (...) (...) We are not going to solve it."}, {"heading": "2 Background and Notation", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "2.1 Attention-based NMT", "text": "We describe the attention-based NMT model by Bahdanau et al. (2015) in this section. Given a source sequence X = (x1, x2, \u00b7 \u00b7, xN) and its translation Y = (y1, y2, \u00b7 \u00b7, yM), an NMT model aims to build a single neural network that X translates into Y by directly learning the model p (Y | X.) The entire network consists of an encoder and a decoder with an attention mechanism typically implemented as two recursive neural networks (RNN) and a multilayered perceptron, or a multilayered perctron. Each xi is a line index in a source sequence search or word embedding Ex-R-DX, as well as any yj word that is an index in a target search or word embedding."}, {"heading": "2.2 Conditional GRU", "text": "The conditional GRU1 has three main components that are calculated at any time step t of the decoder: \u2022 REC1 calculates a hidden state suggestion s't based on the previous hidden state st \u2212 1 and the previously broadcast word y't \u2212 1; \u2022 ATTsrc2 is an attention mechanism based on the hidden states of the source language RNN and calculates ct using all source-related annotation vectors C and the hidden state suggestion s't; \u2022 REC2 calculates the final hidden state st using the hidden state suggestion s \u2032 t and the time-dependent source context vector ct.We use the conditional GRU in our text-related NMT model. First, a single-layer feed network is used to use an expected approximation suggestion esrct, i between each source vector hi and the target word y'context.We only use the GRU."}, {"heading": "3 Multi-modal NMT", "text": "Our MNMT model can be considered an extension of the attention-based NMT framework described in paragraph 2.1, with a visual component to incorporate spatial visual features, and is comparable to the model evaluated by Calixto et al. (2016). We use publicly available, pre-trained CNNs for image feature extraction. Specifically, we extract spatial image characteristics for all images in our dataset using the 50-layer residual network (ResNet-50) by He et al. (2015). These spatial characteristics are the activations of the res4f layer, which is the coding of an image in a 14 x 14 grid, in which each entry in the grid is represented by a 1024D feature vector encoding only information about that specific region of the image. We vectorize this 3-tensor into a 196 x 1024 matrix A (ale, 1 \u00b7 aor, 2 \u00b7 a24, each)."}, {"heading": "3.1 NMTSRC+IMG: decoder with two independent attention mechanisms", "text": "The NMTSRC + IMG model integrates two separate attention mechanisms via the source-language words and visual features into a single RNN decoder. Our double-attentive RNN decoder is dependent on the previous hidden state of the decoder and the previously broadcast word, as well as the source code and the image via two independent attention mechanisms, as shown in Figure 1. We are implementing this idea, which applies a time-dependent image context vector it has described in \u00a7 2.2 to a dual-conditional GRU proposal. To this end, we are introducing a new attention mechanism ATTimg to the original conditional GRU proposal. This visual attention calculates a time-dependent image context vector that it proposes in a hidden state and the image annotation vectors A = (a1, a2, aL) using the \"soft\" attention (Xu et. 2015) with a source language mechanism very similar to the one explained."}, {"heading": "4 Data", "text": "The Flickr30k dataset contains 30k images and 5 descriptions in English for each image (Younget al., 2014). In this work, we use the Multi30k dataset (Elliott et al., 2016), which consists of two multilingual extensions of the original Flickr30k: one with translated data and another with comparable data, which will henceforth be referred to as M30kT and M30kC. For each of the 30k images in the Flickr30k, the M30kT has one of the English descriptions, which will be manually translated into German by a professional translator. Training, validation and test kits contain 29k, 1.014 and 1k images, each accompanied by a set pair of sentences (the original English sentence and its translation into German). For each of the 30k images in the Flickr30k model M30k, the M30k has five descriptions in German."}, {"heading": "5 Experimental setup", "text": "Our encoder is a bidirectional RNN with GRU, a 1024D single-layer set forward and a 1024D single-layer backward RNN. Source and target word embedding are each 620D and are formed together with the model. Word embedding and other non-recurring matrices are initialized by samples from a Gaussian N layer (0, 0.012), recurring matrices are randomly orthogonal and bias vectors are all initialized to zero. Visual characteristics are obtained by feeding images to the pre-trained ResNet-50 and activation of the res4f layer (He et al., 2015). We apply dropout with a 0.5 probability in the encoder bi-directional RNN, the image characteristics, the decoder RNN and before sending a target word NBLN-4. We follow Gal and Ghahramani models, each using a dropout and the English encoder respectively."}, {"heading": "5.1 Baselines", "text": "We train a purely text-based SMT system (PBSMT) and a purely text-based NMT model for comparison. Our PBSMT baseline is based on Moses and uses a 5-gram LM with modified Kneser-Ney smoothing (Kneser and Ney, 1995). It is trained on the basis of the English-German descriptions of the M30kT, while your LM is trained only on the basis of the German descriptions. We use a minimal error rate training3We specifically calculate the character 6-gram-F3 and additionally the precision and recall of the character for comparison.Tune the model (Och, 2003) with BLEU. The text-based NMT baseline is the one described in \u00a7 2.1 and trains on the basis of the English-German descriptions of the M30kT. We also compare our model with two multimodal attention-based NMT models. The first model is the one that has been trained on the same object (Huang), the second one that has been best used in 2016 and the third."}, {"heading": "5.2 Results", "text": "This is not the case in Table 1, where we show the results for our texts NMT and PBSMT. (2016) And indeed, when it comes to the question of whether the individual models are the same as the individual models. (2016) And indeed, if your model has access to more data, the quality of the models improves as well. (2016) And indeed, if your model is not yet able to implement the comparative models from Huang et al. (2016), where there are improvements of + 1.4 BLEU and + 2.7 METEOR. (2016) And indeed, if your model has access to more data, it improves from + 0.9 METEOR, while maintaining the maintenance of BLEU4 scores, we can also conclude from Table 1 that PBSMT is managed better than that of PBSMT. (2016)"}, {"heading": "6 Related work", "text": "However, there is a considerable amount of work to be done in generating natural language from non-textual inputs. Mao et al. (2014) introduced a multimodal RNN that integrates text and visual features and applies them to the tasks of the image description generation and image sentence ranking. In their work, the authors incorporate global image features into a separate multimodal layer that merges RNN text representations and global image characteristics. Vinyals et al al al al al al al al al al al al. (2015) proposed an influential neural IDG model based on the sequence sequence sequence language that is trained end-to-end. (2015) presented a model for generating multilingual descriptions of images by learning and transferring characteristics between two independent, inattentive neural image descriptions."}, {"heading": "7 Conclusions and Future Work", "text": "We have introduced a novel attention-based, multimodal NMT model to integrate spatial visual information into the NMT. We have reported new state-of-the-art results from the M30kT test set that have improved over previous multimodal attention-based models. We have also prepared our model on a cross-domain multimodal dataset and many cross-domain text-based MT corpora, and found that it learns efficiently and is able to utilize the additional data independently of the domain. Our model also compares favorably with NMT and PBSMT baselines that are evaluated on the same training data. In the future, we will incorporate coverage into our model and examine how we can apply it to other tasks of processing natural language."}], "references": [{"title": "Neural Machine Translation by Jointly Learning to Align and Translate", "author": ["Dzmitry Bahdanau", "Kyunghyun Cho", "Yoshua Bengio."], "venue": "International Conference on Learning Representations, ICLR 2015. San Diego, California.", "citeRegEx": "Bahdanau et al\\.,? 2015", "shortCiteRegEx": "Bahdanau et al\\.", "year": 2015}, {"title": "A Neural Probabilistic Language Model", "author": ["Yoshua Bengio", "R\u00e9jean Ducharme", "Pascal Vincent", "Christian Janvin."], "venue": "J. Mach. Learn. Res. 3:1137\u20131155. http://dl.acm.org/citation.cfm?id=944919.944966.", "citeRegEx": "Bengio et al\\.,? 2003", "shortCiteRegEx": "Bengio et al\\.", "year": 2003}, {"title": "Does multimodality help human and machine for translation and image captioning", "author": ["Ozan Caglayan", "Walid Aransa", "Yaxing Wang", "Marc Masana", "Mercedes Garc\u0131\u0301a-Mart\u0131\u0301nez", "Fethi Bougares", "Lo\u0131\u0308c Barrault", "Joost van de Weijer"], "venue": null, "citeRegEx": "Caglayan et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Caglayan et al\\.", "year": 2016}, {"title": "Dcu-uva multimodal mt system report", "author": ["Iacer Calixto", "Desmond Elliott", "Stella Frank."], "venue": "Proceedings of the First Conference on Machine Translation. Berlin, Germany, pages 634\u2013638. http://www.aclweb.org/anthology/W/W16/W16-", "citeRegEx": "Calixto et al\\.,? 2016", "shortCiteRegEx": "Calixto et al\\.", "year": 2016}, {"title": "On the properties of neural machine translation: Encoder\u2013decoder approaches", "author": ["Kyunghyun Cho", "Bart van Merri\u00ebnboer", "Dzmitry Bahdanau", "Yoshua Bengio."], "venue": "Syntax, Semantics and Structure in Statistical Translation. page 103.", "citeRegEx": "Cho et al\\.,? 2014a", "shortCiteRegEx": "Cho et al\\.", "year": 2014}, {"title": "Learning phrase representations using rnn encoder\u2013 decoder for statistical machine translation", "author": ["Kyunghyun Cho", "Bart van Merrienboer", "Caglar Gulcehre", "Dzmitry Bahdanau", "Fethi Bougares", "Holger Schwenk", "Yoshua Bengio."], "venue": "In", "citeRegEx": "Cho et al\\.,? 2014b", "shortCiteRegEx": "Cho et al\\.", "year": 2014}, {"title": "Better Hypothesis Testing for Statistical Machine Translation: Controlling for Optimizer Instability", "author": ["Jonathan H. Clark", "Chris Dyer", "Alon Lavie", "Noah A. Smith."], "venue": "Proceedings of the 49th Annual Meeting of the Associa-", "citeRegEx": "Clark et al\\.,? 2011", "shortCiteRegEx": "Clark et al\\.", "year": 2011}, {"title": "Meteor Universal: Language Specific Translation Evaluation for Any Target Language", "author": ["Michael Denkowski", "Alon Lavie."], "venue": "Proceedings of the EACL 2014 Workshop on Statistical Machine Translation.", "citeRegEx": "Denkowski and Lavie.,? 2014", "shortCiteRegEx": "Denkowski and Lavie.", "year": 2014}, {"title": "Multi-Task Learning for Multiple Language Translation", "author": ["Daxiang Dong", "Hua Wu", "Wei He", "Dianhai Yu", "Haifeng Wang."], "venue": "Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint", "citeRegEx": "Dong et al\\.,? 2015", "shortCiteRegEx": "Dong et al\\.", "year": 2015}, {"title": "Multi-Language Image Description with Neural Sequence Models", "author": ["Desmond Elliott", "Stella Frank", "Eva Hasler."], "venue": "CoRR abs/1510.04709. http://arxiv.org/abs/1510.04709.", "citeRegEx": "Elliott et al\\.,? 2015", "shortCiteRegEx": "Elliott et al\\.", "year": 2015}, {"title": "Multi30K: Multilingual English-German Image Descriptions", "author": ["Desmond Elliott", "Stella Frank", "Khalil Sima\u2019an", "Lucia Specia"], "venue": "In Proceedings of the 5th Workshop on Vision and Language,", "citeRegEx": "Elliott et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Elliott et al\\.", "year": 2016}, {"title": "Multi-Way, Multilingual Neural Machine Translation with a Shared Attention Mechanism", "author": ["Orhan Firat", "Kyunghyun Cho", "Yoshua Bengio."], "venue": "Proceedings of the 2016 Conference of the North American Chapter of the Association for Com-", "citeRegEx": "Firat et al\\.,? 2016", "shortCiteRegEx": "Firat et al\\.", "year": 2016}, {"title": "A Theoretically Grounded Application of Dropout in Recurrent Neural Networks", "author": ["Yarin Gal", "Zoubin Ghahramani."], "venue": "Advances in Neural Information Processing Systems, NIPS, Barcelona, Spain, pages 1019\u20131027. http://papers.nips.cc/paper/6241-", "citeRegEx": "Gal and Ghahramani.,? 2016", "shortCiteRegEx": "Gal and Ghahramani.", "year": 2016}, {"title": "Rich Feature Hierarchies for Accurate Object Detection and Semantic Segmentation", "author": ["Ross Girshick", "Jeff Donahue", "Trevor Darrell", "Jitendra Malik."], "venue": "Proceedings of the 2014 IEEE Conference on Computer Vision and Pattern Recognition.", "citeRegEx": "Girshick et al\\.,? 2014", "shortCiteRegEx": "Girshick et al\\.", "year": 2014}, {"title": "Deep residual learning for image recognition", "author": ["Kaiming He", "Xiangyu Zhang", "Shaoqing Ren", "Jian Sun."], "venue": "arXiv preprint arXiv:1512.03385 .", "citeRegEx": "He et al\\.,? 2015", "shortCiteRegEx": "He et al\\.", "year": 2015}, {"title": "Multimodal Pivots for Image Caption Translation", "author": ["Julian Hitschler", "Shigehiko Schamoni", "Stefan Riezler."], "venue": "Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long", "citeRegEx": "Hitschler et al\\.,? 2016", "shortCiteRegEx": "Hitschler et al\\.", "year": 2016}, {"title": "Europarl: A Parallel Corpus", "author": ["Philipp Koehn"], "venue": "troit, Michigan,", "citeRegEx": "Koehn.,? \\Q2005\\E", "shortCiteRegEx": "Koehn.", "year": 2005}, {"title": "BLEU: A Method", "author": ["Wei-Jing Zhu"], "venue": null, "citeRegEx": "Zhu.,? \\Q2002\\E", "shortCiteRegEx": "Zhu.", "year": 2002}, {"title": "chrf: character n-gram f", "author": [], "venue": null, "citeRegEx": "Popovi\u0107.,? \\Q2015\\E", "shortCiteRegEx": "Popovi\u0107.", "year": 2015}, {"title": "A study of translation edit rate with targeted human annotation", "author": ["Matthew Snover", "Bonnie Dorr", "Richard Schwartz", "Linnea Micciulla", "John Makhoul."], "venue": "In Proceedings of Association for Machine Translation in the Americas. Cambridge, MA, pages", "citeRegEx": "Snover et al\\.,? 2006", "shortCiteRegEx": "Snover et al\\.", "year": 2006}, {"title": "A Shared Task on Multimodal Machine Translation and Crosslingual Image Description", "author": ["Lucia Specia", "Stella Frank", "Khalil Sima\u2019an", "Desmond Elliott"], "venue": "In Proceedings of the First Conference on Machine Translation,", "citeRegEx": "Specia et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Specia et al\\.", "year": 2016}, {"title": "Sequence to Sequence Learning with Neural Networks", "author": ["Ilya Sutskever", "Oriol Vinyals", "Quoc V Le."], "venue": "Advances in Neural Information Processing Systems. Montr\u00e9al, Canada, pages 3104\u20133112.", "citeRegEx": "Sutskever et al\\.,? 2014", "shortCiteRegEx": "Sutskever et al\\.", "year": 2014}, {"title": "Modeling Coverage for Neural Machine Translation", "author": ["Zhaopeng Tu", "Zhengdong Lu", "Yang Liu", "Xiaohua Liu", "Hang Li."], "venue": "Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume", "citeRegEx": "Tu et al\\.,? 2016", "shortCiteRegEx": "Tu et al\\.", "year": 2016}, {"title": "Sequence to sequence - video to text", "author": ["Subhashini Venugopalan", "Marcus Rohrbach", "Jeffrey Donahue", "Raymond J. Mooney", "Trevor Darrell", "Kate Saenko."], "venue": "2015 IEEE International Conference on Computer Vision,", "citeRegEx": "Venugopalan et al\\.,? 2015", "shortCiteRegEx": "Venugopalan et al\\.", "year": 2015}, {"title": "Santiago, Chile, pages 4534\u20134542", "author": ["ICCV"], "venue": "https://doi.org/10.1109/ICCV.2015.515.", "citeRegEx": "ICCV,? 2015", "shortCiteRegEx": "ICCV", "year": 2015}, {"title": "Show and tell: A neural image caption generator", "author": ["Oriol Vinyals", "Alexander Toshev", "Samy Bengio", "Dumitru Erhan."], "venue": "IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2015. Boston, Massachusetts, pages 3156\u20133164.", "citeRegEx": "Vinyals et al\\.,? 2015", "shortCiteRegEx": "Vinyals et al\\.", "year": 2015}, {"title": "Show, attend and tell: Neural image caption generation with visual attention", "author": ["Kelvin Xu", "Jimmy Ba", "Ryan Kiros", "Kyunghyun Cho", "Aaron Courville", "Ruslan Salakhudinov", "Rich Zemel", "Yoshua Bengio."], "venue": "Proceedings of", "citeRegEx": "Xu et al\\.,? 2015", "shortCiteRegEx": "Xu et al\\.", "year": 2015}, {"title": "From image descriptions to visual denotations: New similarity metrics for semantic inference over event descriptions", "author": ["Peter Young", "Alice Lai", "Micah Hodosh", "Julia Hockenmaier."], "venue": "Transactions of the Association for Computational Linguis-", "citeRegEx": "Young et al\\.,? 2014", "shortCiteRegEx": "Young et al\\.", "year": 2014}, {"title": "ADADELTA: An Adaptive Learning Rate Method", "author": ["Matthew D. Zeiler."], "venue": "CoRR abs/1212.5701. http://arxiv.org/abs/1212.5701.", "citeRegEx": "Zeiler.,? 2012", "shortCiteRegEx": "Zeiler.", "year": 2012}], "referenceMentions": [{"referenceID": 5, "context": "ing problem (Kalchbrenner and Blunsom, 2013; Cho et al., 2014b; Sutskever et al., 2014) where each training example consists of one source and one target variable-length sequences, with no prior information on the alignment between the two.", "startOffset": 12, "endOffset": 87}, {"referenceID": 21, "context": "ing problem (Kalchbrenner and Blunsom, 2013; Cho et al., 2014b; Sutskever et al., 2014) where each training example consists of one source and one target variable-length sequences, with no prior information on the alignment between the two.", "startOffset": 12, "endOffset": 87}, {"referenceID": 0, "context": "In the context of NMT, Bahdanau et al. (2015) first proposed to use an attention mechanism in the decoder, which is trained to attend to the relevant source-language words as it generates each word of the target sentence.", "startOffset": 23, "endOffset": 46}, {"referenceID": 2, "context": "models in the literature that utilised spatial visual features did not significantly improve over a comparable model that used global visual features or even only textual features (Caglayan et al., 2016; Calixto et al., 2016; Huang et al., 2016; Libovick\u00fd et al., 2016; Specia et al., 2016).", "startOffset": 180, "endOffset": 290}, {"referenceID": 3, "context": "models in the literature that utilised spatial visual features did not significantly improve over a comparable model that used global visual features or even only textual features (Caglayan et al., 2016; Calixto et al., 2016; Huang et al., 2016; Libovick\u00fd et al., 2016; Specia et al., 2016).", "startOffset": 180, "endOffset": 290}, {"referenceID": 20, "context": "models in the literature that utilised spatial visual features did not significantly improve over a comparable model that used global visual features or even only textual features (Caglayan et al., 2016; Calixto et al., 2016; Huang et al., 2016; Libovick\u00fd et al., 2016; Specia et al., 2016).", "startOffset": 180, "endOffset": 290}, {"referenceID": 0, "context": "We describe the attention-based NMT model introduced by Bahdanau et al. (2015) in this section.", "startOffset": 56, "endOffset": 79}, {"referenceID": 4, "context": "GRU (Cho et al., 2014a), where a forward RNN \u2212 \u2192 \u03a6 enc reads X word by word, from left to right, and generates a sequence of forward annota-", "startOffset": 4, "endOffset": 23}, {"referenceID": 1, "context": "These annotation vectors are in turn used by the decoder, which is essentially a neural language model (LM) (Bengio et al., 2003) conditioned on the previously emitted words and the source sentence via an attention mechanism.", "startOffset": 108, "endOffset": 129}, {"referenceID": 3, "context": "1 with the addition of a visual component to incorporate spatial visual features, and is comparable to the model evaluated by Calixto et al. (2016).", "startOffset": 126, "endOffset": 148}, {"referenceID": 14, "context": "using the 50-layer Residual network (ResNet-50) of He et al. (2015). These spatial features are the activations of the res4f layer, which can be seen as encoding an image in a 14\u00d714 grid, where each of the entries in the grid is represented by a 1024D feature vector that only encodes information about that specific region of the image.", "startOffset": 51, "endOffset": 68}, {"referenceID": 26, "context": "This visual attention computes a time-dependent image context vector it given a hidden state proposal st and the image annotation vectors A = (a1,a2, \u00b7 \u00b7 \u00b7 ,aL) using the \u201csoft\u201d attention (Xu et al., 2015).", "startOffset": 188, "endOffset": 205}, {"referenceID": 26, "context": "We use \u03b2 following Xu et al. (2015) who empirically found it to", "startOffset": 19, "endOffset": 36}, {"referenceID": 27, "context": "The Flickr30k data set contains 30k images and 5 descriptions in English for each image (Young et al., 2014).", "startOffset": 88, "endOffset": 108}, {"referenceID": 10, "context": "In this work, we use the Multi30k dataset (Elliott et al., 2016), which consists of two multilingual expansions of the original Flickr30k: one with translated data and another one with comparable data, henceforth referred to as M30kT and M30kC, respectively.", "startOffset": 42, "endOffset": 64}, {"referenceID": 16, "context": "These include the Europarl v7 (Koehn, 2005), News Commentary and", "startOffset": 30, "endOffset": 43}, {"referenceID": 14, "context": "to the pre-trained ResNet-50 and using the activations of the res4f layer (He et al., 2015).", "startOffset": 74, "endOffset": 91}, {"referenceID": 12, "context": "We follow Gal and Ghahramani (2016) and apply dropout to the encoder bidirectional and the decoder RNN using one same mask in all time steps.", "startOffset": 10, "endOffset": 36}, {"referenceID": 28, "context": "All models are trained using stochastic gradient descent with ADADELTA (Zeiler, 2012) with minibatches of size 80 (text-only NMT) or 40", "startOffset": 71, "endOffset": 85}, {"referenceID": 7, "context": ", 2002), METEOR (Denkowski and Lavie, 2014), TER (Snover et al.", "startOffset": 16, "endOffset": 43}, {"referenceID": 19, "context": ", 2002), METEOR (Denkowski and Lavie, 2014), TER (Snover et al., 2006), and chrF3 (Popovi\u0107, 2015).", "startOffset": 49, "endOffset": 70}, {"referenceID": 18, "context": ", 2006), and chrF3 (Popovi\u0107, 2015).", "startOffset": 19, "endOffset": 34}, {"referenceID": 6, "context": "nificance with approximate randomisation for the first three metrics using the MultEval tool (Clark et al., 2011).", "startOffset": 93, "endOffset": 113}, {"referenceID": 0, "context": "This is somehow expected, since the attention mechanism in NMT (Bahdanau et al., 2015) does not explicitly take attention weights from previous", "startOffset": 63, "endOffset": 86}, {"referenceID": 22, "context": "time steps into account, an thus lacks the notion of source coverage as in SMT (Koehn et al., 2003; Tu et al., 2016).", "startOffset": 79, "endOffset": 116}, {"referenceID": 16, "context": "the concatenation of the Europarl (Koehn, 2005), Common Crawl and News Commentary corpora, used in WMT 2015 (\u223c4.", "startOffset": 34, "endOffset": 47}, {"referenceID": 20, "context": "Multi-modal MT was just recently addressed by the MT community by means of a shared task (Specia et al., 2016).", "startOffset": 89, "endOffset": 110}, {"referenceID": 25, "context": "Vinyals et al. (2015) proposed an influential neural IDG model based on the sequenceto-sequence framework, which is trained end-to-", "startOffset": 0, "endOffset": 22}, {"referenceID": 9, "context": "Elliott et al. (2015) put forward a model to generate multilingual descriptions of images by", "startOffset": 0, "endOffset": 22}, {"referenceID": 23, "context": "5 Venugopalan et al. (2015) introduced a model trained end-to-end to generate textual descriptions of open-domain videos from the video frames based on the sequence-to-sequence frame-", "startOffset": 2, "endOffset": 28}, {"referenceID": 26, "context": "Finally, Xu et al. (2015) introduced the first attention-based IDG model where an attentive decoder learns to attend to different parts of an image as it generates its description in natural language.", "startOffset": 9, "endOffset": 26}, {"referenceID": 8, "context": "In the context of NMT, Dong et al. (2015) proposed a multi-task learning approach where a model is trained to translate from one source language into multiple target languages.", "startOffset": 23, "endOffset": 42}, {"referenceID": 11, "context": "Firat et al. (2016) proposed a multi-way model trained to translate between", "startOffset": 0, "endOffset": 20}, {"referenceID": 8, "context": "Instead of one attention mechanism per language pair as in Dong et al. (2015), which would lead to a quadratic number of attention mechanisms in relation to language pairs, they use a shared attention", "startOffset": 59, "endOffset": 78}, {"referenceID": 15, "context": "Although not an NMT model, Hitschler et al. (2016)", "startOffset": 27, "endOffset": 51}, {"referenceID": 20, "context": "Although no purely neural multi-modal model to date significantly improves on both text-only NMT and SMT models (Specia et al., 2016), different research groups have proposed to include global and spatial visual features in re-ranking", "startOffset": 112, "endOffset": 133}, {"referenceID": 2, "context": "n-best lists generated by an SMT system or directly in an NMT framework with some success (Caglayan et al., 2016; Calixto et al., 2016; Huang et al., 2016; Libovick\u00fd et al., 2016; Shah et al., 2016).", "startOffset": 90, "endOffset": 198}, {"referenceID": 3, "context": "n-best lists generated by an SMT system or directly in an NMT framework with some success (Caglayan et al., 2016; Calixto et al., 2016; Huang et al., 2016; Libovick\u00fd et al., 2016; Shah et al., 2016).", "startOffset": 90, "endOffset": 198}, {"referenceID": 20, "context": "Although their model has not been devised with translation as its primary goal, theirs is one of the baselines of the first shared task in multi-modal MT in WMT 2016 (Specia et al., 2016).", "startOffset": 166, "endOffset": 187}, {"referenceID": 8, "context": "when these translate from many source languages into many target languages (Dong et al., 2015; Firat et al., 2016).", "startOffset": 75, "endOffset": 114}, {"referenceID": 11, "context": "when these translate from many source languages into many target languages (Dong et al., 2015; Firat et al., 2016).", "startOffset": 75, "endOffset": 114}], "year": 2017, "abstractText": "We introduce a Multi-modal Neural Machine Translation model in which a doubly-attentive decoder naturally incorporates spatial visual features obtained using pre-trained convolutional neural networks, bridging the gap between image description and translation. Our decoder learns to attend to source-language words and parts of an image independently by means of two separate attention mechanisms as it generates words in the target language. We find that our model can efficiently exploit not just back-translated in-domain multi-modal data but also large general-domain text-only MT corpora. We also report state-of-the-art results on the Multi30k data set.", "creator": "LaTeX with hyperref package"}}}