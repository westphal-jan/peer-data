{"id": "1702.01691", "review": {"conference": "iclr", "VERSION": "v1", "DATE_OF_SUBMISSION": "6-Feb-2017", "title": "Calibrating Energy-based Generative Adversarial Networks", "abstract": "In this paper, we propose to equip Generative Adversarial Networks with the ability to produce direct energy estimates for samples.Specifically, we propose a flexible adversarial training framework, and prove this framework not only ensures the generator converges to the true data distribution, but also enables the discriminator to retain the density information at the global optimal. We derive the analytic form of the induced solution, and analyze the properties. In order to make the proposed framework trainable in practice, we introduce two effective approximation techniques. Empirically, the experiment results closely match our theoretical analysis, verifying the discriminator is able to recover the energy of data distribution.", "histories": [["v1", "Mon, 6 Feb 2017 16:30:13 GMT  (8827kb,D)", "https://arxiv.org/abs/1702.01691v1", "Accepted into ICLR 2017"], ["v2", "Fri, 24 Feb 2017 01:38:09 GMT  (9079kb,D)", "http://arxiv.org/abs/1702.01691v2", "ICLR 2017 camera ready"]], "COMMENTS": "Accepted into ICLR 2017", "reviews": [], "SUBJECTS": "cs.LG", "authors": ["zihang dai", "amjad almahairi", "philip bachman", "eduard hovy", "aaron courville"], "accepted": true, "id": "1702.01691"}, "pdf": {"name": "1702.01691.pdf", "metadata": {"source": "CRF", "title": "CALIBRATING ENERGY-BASED GENERATIVE ADVER-", "authors": ["SARIAL NETWORKS", "Zihang Dai", "Amjad Almahairi", "Philip Bachman", "Eduard Hovy", "Aaron Courville"], "emails": [], "sections": [{"heading": "1 INTRODUCTION", "text": "Generative Adversarial Networks (GANs) (Goodfellow et al., 2014) represent an important milestone on the road to more effective generative models. GANs cast generative modeling as a minimax game between a generative network (generator) that maps a random vector into the data space and a discriminatory network (discriminator) that aims to distinguish generated samples from real samples. Several researchers Radford et al. (2015); Salimans et al. (2016); Zhao et al. (2016) have shown that adversarial interaction with the discriminator can lead to a generator that produces compelling samples. Empirical success of the GAN framework has also been supported by the theoretical analysis by Goodfellow et al., who has shown that the distribution produced by the generator adapts to true data distribution, while the discriminator converts to a degenerated uniform solution."}, {"heading": "2 RELATED WORK", "text": "Following a similar motivation, the field of Inverse Reinforcement Learning (IRL) (Ng & Russell, 2000) explores ways to regain the \"intrinsic\" reward function (analogous to the discriminator) from observed expert paths (real-life examples).Taking this idea one step further, apprenticeship or imitation learning (Abbeel & Ng, 2004; Ziebart et al., 2008) aims to learn a policy (analogous to the generator) using the reward signals gained through IRL. Specifically, Ho & Ermon establish a link between imitation learning and GAN by showing that the GAN formulation can be derived through a specific regulation of the reward function. Also, in a specific case of its formulation, Ho & Ermon provide a duality-based interpretation of the problem that inspires our theoretical analysis."}, {"heading": "3 ALTERNATIVE FORMULATION OF ADVERSARIAL TRAINING", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "3.1 BACKGROUND", "text": "Before introducing the proposed formulation, we first make some basic assumptions required for the analysis and introduce notations used throughout the paper.While many of the non-parametric intuitions can be applied directly to the parametric case, our analysis focuses on the non-parametric case where all models have infinite capacities. We assume a finite data space throughout the analysis in order to avoid engineering machines outside the scope of this work. However, our results can be extended to continuous data spaces, and our experiments are actually conducted on continuous data.Let X be the data space to be considered and P = {p | p (x) \u2265 0, and P = x \u00b2 X, x \u00b2 X (x) = 1} the set of all proper distributions defined on X."}, {"heading": "3.2 PROPOSED FORMULATION", "text": "To understand the reasons for such a development, it is necessary that it sees itself in a position to put itself and the others in the center. (...) It is about the question of whether people are able to understand themselves and the world. (...) It is about the question of how far people are able to understand themselves and themselves. (...) It is about the question of how far they are able to understand the world. (...) It is about the question of how they are able to understand themselves and the world. (...) It is about the question of how far people are able to understand the world. (...) It is about the question of how far they are able to understand the world. (...) It is about the question of how they are able to understand themselves and the world. (...) It is about the question of how far they are lost in the world. (...) It is about the question of how far the world is removed from the world. (...) It is about the question of how far the world is in the world."}, {"heading": "4 PARAMETRIC INSTANTIATION WITH ENTROPY APPROXIMATION", "text": "While the discussion in the previous sections focused on the non-parametric case, in practice we limit ourselves to a finite amount of data, and the real problem concerns high-dimensional continuous spaces. Therefore, we resort to parametric representations for both the generator and the discriminator. In order to train the generator on the basis of the usual back-propagation, we do not parametrize the generator distribution directly, but parametrize a directed generator network that transforms random noise z-pz (z) in samples from a continuous data space Rn. Consequently, we do not have analytical access to the generator distribution, which is implicitly defined by the generator network. However, the regulation term K (pgen) in the training goal (1) requires the generator distribution. In the face of this problem, we focus on the max entropy formulation and use two different approximations of the regulation term K (pgen) = \u2212 pgen (H)."}, {"heading": "4.1 NEAREST-NEIGHBOR ENTROPY GRADIENT APPROXIMATION", "text": "The first proposed solution is based on an intuitive interpretation of the entropy gradient. First, since we construct pgen by applying a deterministic, differentiable transformation environment to samples z from a fixed distribution pz, we can describe the gradient of H (pgen) with respect to the generator parameters \u03b8 as follows: \u2212 approximation range H (pgen) = Ez \u00b2 log pgen (g\u03b8 (z))] = Ez \u0445 pz (z) \u0445 log pgen (g\u03b8 (z))) \u2202 gasm (z), (9) where the first equality is based on the \"repair risk trick\" \u2212 Equation 9 implies that if we directly construct the gradient of the generator log density protocol pgen (x) w.r.t. any x = g\u03b8 (z), then we can construct the Monte Carlo estimate of the entropy gradient i (pgen)."}, {"heading": "4.2 VARIATIONAL LOWER BOUND ON THE ENTROPY", "text": "Another approach we are considering is based on defining and maximizing a variable lower limit on the entropy H (pgen (x) of the generator distribution. We can define the common distribution over observed data and the noise variables as pgen (x, z) = pgen (x | z), where simply pgen (z) = pz (z) is a fixed prerequisite. We can also write the mutual information between the observed data and noise variables as: me (pgen (x); pgen (z) = H (pgen) \u2212 pgen (x) \u2212 H (pgen (z) = H (pgen (z)) \u2212 pgen (z) -qgen."}, {"heading": "5 EXPERIMENTS", "text": "In this section, we will empirically verify our theoretical results using several synthetic and real data sets. In particular, we will assess whether the discriminator obtained from entropy-regulated hostile training can capture the density information (in the form of energy) while ensuring that the generator distribution matches the data distribution. For convenience, we refer to the obtained models as an EGAN entity. Our experimental setup closely follows the recommendations of Radford et al. (2015), except in Section 5.1, where we use fully interconnected models (see Appendix B.1 for details)."}, {"heading": "5.1 SYNTHETIC LOW-DIMENSIONAL DATA", "text": "First, we look at three synthetic data sets in 2-dimensional space, drawn from the following distributions: (i) mixture of 4 casters with equal mixing weights, (ii) mixture of 200 casters arranged as two spirals (100 components of each spiral), and (iii) mixture of 2 casters with highly biased mixing weights, P (c1) = 0.1, P (c2) = 0.1. We visualize the soil probability of these distributions together with 100 K samples in Figure 1. Since the data are in 2-dimensional space, we can identify both the learned generator (by drawing samples) and the discriminator for direct comparison and evaluation."}, {"heading": "5.2 RANKING NIST DIGITS", "text": "In this experiment, we verify that the results in synthetic data sets can be translated into higher-dimensional data. While the learned energy function is not feasible in high-dimensional space, we can verify that the learned energy function learns relative densities by checking the ranking of the samples according to their assigned energies. We train on 28 x 28 images of a single handwriting 1For further details, we refer to https: / / github.com / zihangdai / cegan _ iclr2017.digit from the NIST dataset. 2 We compare the capability of EGAN-Ent-NN with both EGAN-Const and GAN in a ranking of 1,000 images, half of which are generated samples and the rest real test images. Figures 4 and 5 show the top 100 images or sub-100-ranked images for each model, after training them on number 1., we also show in Figure 7 the mean of all training samples, so that we can get the highest sense of what is most common in IST."}, {"heading": "5.3 SAMPLE QUALITY ON NATURAL IMAGE DATASETS", "text": "In this last set of experiments, we evaluate the visual quality of the samples generated by our model in two datasets of natural images, namely CIFAR-10 and CelebA. Here, we use the variational approach for entropy regularization, which can be easily transferred to high-dimensional data. Figure 6 shows samples generated by EGAN-Ent-VI. We see that despite the loud gradients offered by the variable approach, our model is capable of generating high-quality specimens. 2https: / / www.nist.gov / srd / nist-special-database-19, an extended version of MNIST with an average of over 74K examples per digital.We support the quality of the samples of our model based on the CIFAR-10 based on the Inception Score proposed by (Salimans et al., 2016). 3. Table 1 shows the values of our EGAN 2016 Enan Model VI, with only the best energy from GAN models."}, {"heading": "6 CONCLUSION", "text": "In this paper, we addressed a fundamental limitation of adversarial learning approaches, namely their inability to provide reasonable energy estimates for samples. We proposed a novel formulation of adversarial learning that leads to a discriminatory function that restores true data energy. We provided a rigorous characterization of the learned discriminator in a non-parametric environment and proposed two methods of instantiation in a typical parametric environment. Our experimental results confirm our theoretical analysis of discriminator properties and show that we can also obtain samples of the most modern quality."}, {"heading": "7 ACKNOWLEDGEMENTS", "text": "We would like to thank the developers of Theano (Theano Development Team, 2016) for developing such a powerful tool for scientific computing. Amjad Almahairi was supported with funds from Maluuba Research.3Using the evaluation script published in https: / / github.com / openai / improved-gan / Figure 7: mean digit."}, {"heading": "A SUPPLEMENTARY MATERIALS FOR SECTION 3", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "A.1 OPTIMAL DISCRIMINATOR FORM UNDER THE PROPOSED FORMULATION", "text": "Proof of Proposition 3.1. By introducing additional dual variables for probability constraints (the second and third), the new Lagrange function has the form L (pgen, c, \u00b5, \u03bb) = K (pgen) + x (x) - x (x) (pgen (x) \u2212 pdata (x) - x (x) - x (x) - x (x) - x (x) pgen (x) \u2212 1) (17), where c (x), x (x), x (x), x (x), x (x) and x (x) are the dual variables.The CCT conditions for the optimal primary and dual variables are as follows: K (pgen) - pgen (x) - asimal pgen (x) - asimal pgen (x) - asimal pgen (x) - asimal pgen (x) - asimal pgen (x) - asimal pgen (x) - asimal pgen (x) - asimal pgen (x) - asimal pgen (x) - asimal pgen (implicit) (implicit)."}, {"heading": "A.2 OPTIMAL CONDITIONS OF EBGAN", "text": "In (Zhao et al., 2016), the educational objectives of the generator and discriminator are described only as a single minimax optimization problem, since the margin structure is applied only to the discriminator's target. Furthermore, the discriminator is designed to generate the mean square reconstruction error of an auto-encoder structure. (This limits the range of discriminator output as non-negative, which is equivalent to positioning a specific discriminator distribution under the non-parametric constellation.) To characterize the optimal generator and discriminator, we adapt the same analytical logic used in the evidence sketch of the original GAN (Goodfellow et al., 2014). Specifically, a specific generator distribution is given that the optimal discriminator function can be derived from the generator distribution c (x; pgen) by examining the objective of the discriminator function."}, {"heading": "A.3 ANALYSIS OF ADDING ADDITIONAL TRAINING SIGNAL TO GAN FORMULATION", "text": "To show that simply adding the same training signal to GAN does not lead to the same result, it is more convenient to work directly with the formulation of f -GAN (Nowozin et al., 2016, Equation (6), which includes the original GAN formulation as a special case. Specifically, the general f -GAN formulation takes the following formula: c min pgen, p E, x, pgen [f? (c (x)) \u2212 E x, pdata [c (x)], (24), where the f? (\u00b7) denotes the convex conjugate (Boyd & Vandenberghe, 2004) of the f-divergence function. The optimal state of the discriminator can be found by taking the variation w.r.t. c, which gives the optimal discriminatorc (x) = f \u00b2 (pdata (x) pgen (x)), where f \u2032 pgen (\u00b7) is the derivative of first order."}, {"heading": "B SUPPLEMENTARY MATERIALS FOR SECTION 5", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "B.1 EXPERIMENT SETTING", "text": "Here we have specified the neural architectures used in Section 5.First, for the Egan-Ent-VI model, we parameterize the approximate posterior distribution qgen (z | x) with a diagonal Gaussian distribution whose middle and covariant matrix is the output of a trainable inference network, i.e.qgen (z) = N (\u00b5).For experiments with synthetic datasets, the following fully connected neural networks are used \u2022 Generator (f inferences the inference network, and I am the identity matrix.) Note: The inference network appears only in the Egan-Ent-VI form."}, {"heading": "B.2 QUANTITATIVE COMPARISON OF DIFFERENT MODELS", "text": "To quantify the quality of the recovered distributions, we calculate the paired KL divergence of the following four distributions: \u2022 The real data distribution with analytical form, called pdata \u2022 The empirical data distribution approximates the 100K training data, called pemp \u2022 The generator distribution approximates the data generated by 100K, called pgen \u2022 The discriminator distribution normalizes from the learned energy, called pdiscDa the synthetic data sets are two-dimensional, we approximate both the empirical data distribution and the generator distribution using the simple histogram. Specifically, we divide the surface into a 100-to-100 grid and assign each sample to its nearest grid cell based on euclidean distance. Then, we normalize the number of samples in each cell into an orderly distribution. When restoring the discriminator distribution from the learned energy, we assume that the digit (.ex) (i.e.) is 0."}, {"heading": "B.3 COMPARISON OF THE ENTROPY (GRADIENT) APPROXIMATION METHODS", "text": "In order to understand the difference in performance between EGAN-Ent-VI and EGAN-Ent-NN, we analyze the quality of the approach of the entropy gradient during training. To this end, we visualize some detailed training information in Figure 8. As we can see in Figure 8a, the approach of the viarational entropy gradient to the implicit contour in Fig. (1,2) is imprecise. Ideally, the direction of the entropy gradient should point from the center of its narrowest mode towards the environment, with the direction being orthogonal to the implicit contour in Fig. (1,2). However, the direction of the gradients in Fig. (2,3) does not coincide with this. \u2022 It is imprecise in magnitude. As we can see, the entropy approach gradient (Fig. (2,3) exhibits a much larger norm than the differentiation in Fig. (2,2)."}, {"heading": "B.4 RANKING NIST DIGITS", "text": "Figure 9 shows the ranking of all 1000 generated and real images (from the test kit) for three models: EGAN-Ent-NN, EGAN-Const and GAN. We can clearly see that in EGAN-Ent-NN the upper digits of the middle digit look very similar. From the upper left corner to the lower right corner, the transition trend is: the degree of rotation increases and the digits get thicker or thinner compared to the mean. Furthermore, the samples in the last lines deviate from the middle image: either high diagonally to the right or left or have a different shape: very thin or thick or typewritten. Other models are unable to achieve a similarly clear distinction for images with a high or low probability. Finally, we consistently observe the same trend in modeling other digits that are not represented in this paper due to lack of space."}, {"heading": "B.5 CLASSIFIER PERFORMANCE AS A PROXY MEASURE", "text": "As mentioned in Section 5, the quantitative evaluation of the proposed formulation on the basis of high-dimensional data is extremely difficult. In order to provide more quantitative intuitions about the learned discriminator in convergence, we take a proxy measure. Specifically, we take the activation of the last layer of the convergent discriminator network as a fixed, pre-trained feature and build on it a linear classifier. Hypothetically, if the discriminator does not degenerate, the extracted last layer feature should contain more information about the data points, especially compared to characteristics of degenerated discriminators. Following this idea, we first train EGAN-Ent-NN, EGAN-Const and GAN on the MNIST until convergence and then extract the activation of the last layer from their discriminator networks as a fixed feature input. Based on fixed characteristics, a randomly initialized linear classifier is trained to perform the classification on the MIST table based on each of the energy differences (LIST) in the three of the models."}], "references": [{"title": "Apprenticeship learning via inverse reinforcement learning", "author": ["Pieter Abbeel", "Andrew Y Ng"], "venue": "In Proceedings of the twenty-first international conference on Machine learning,", "citeRegEx": "Abbeel and Ng.,? \\Q2004\\E", "shortCiteRegEx": "Abbeel and Ng.", "year": 2004}, {"title": "Generative adversarial nets", "author": ["Ian Goodfellow", "Jean Pouget-Abadie", "Mehdi Mirza", "Bing Xu", "David Warde-Farley", "Sherjil Ozair", "Aaron Courville", "Yoshua Bengio"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Goodfellow et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Goodfellow et al\\.", "year": 2014}, {"title": "Generative adversarial imitation learning", "author": ["Jonathan Ho", "Stefano Ermon"], "venue": "arXiv preprint arXiv:1606.03476,", "citeRegEx": "Ho and Ermon.,? \\Q2016\\E", "shortCiteRegEx": "Ho and Ermon.", "year": 2016}, {"title": "Deep directed generative models with energy-based probability estimation", "author": ["Taesup Kim", "Yoshua Bengio"], "venue": "arXiv preprint arXiv:1606.03439,", "citeRegEx": "Kim and Bengio.,? \\Q2016\\E", "shortCiteRegEx": "Kim and Bengio.", "year": 2016}, {"title": "Algorithms for inverse reinforcement learning", "author": ["A. Ng", "S. Russell"], "venue": "In Icml, pp", "citeRegEx": "Ng and Russell.,? \\Q2000\\E", "shortCiteRegEx": "Ng and Russell.", "year": 2000}, {"title": "f-gan: Training generative neural samplers using variational divergence minimization", "author": ["Sebastian Nowozin", "Botond Cseke", "Ryota Tomioka"], "venue": "arXiv preprint arXiv:1606.00709,", "citeRegEx": "Nowozin et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Nowozin et al\\.", "year": 2016}, {"title": "Unsupervised representation learning with deep convolutional generative adversarial networks", "author": ["Alec Radford", "Luke Metz", "Soumith Chintala"], "venue": "arXiv preprint arXiv:1511.06434,", "citeRegEx": "Radford et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Radford et al\\.", "year": 2015}, {"title": "Improved techniques for training gans", "author": ["Tim Salimans", "Ian Goodfellow", "Wojciech Zaremba", "Vicki Cheung", "Alec Radford", "Xi Chen"], "venue": "arXiv preprint arXiv:1606.03498,", "citeRegEx": "Salimans et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Salimans et al\\.", "year": 2016}, {"title": "Energy-based generative adversarial network", "author": ["Junbo Zhao", "Michael Mathieu", "Yann LeCun"], "venue": "arXiv preprint arXiv:1609.03126,", "citeRegEx": "Zhao et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Zhao et al\\.", "year": 2016}, {"title": "Maximum entropy inverse reinforcement learning", "author": ["Brian D Ziebart", "Andrew L Maas", "J Andrew Bagnell", "Anind K Dey"], "venue": "In AAAI,", "citeRegEx": "Ziebart et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Ziebart et al\\.", "year": 2008}, {"title": "2016), the training objectives of the generator and the discriminator cannot be written", "author": ["In (Zhao"], "venue": null, "citeRegEx": ".Zhao,? \\Q2016\\E", "shortCiteRegEx": ".Zhao", "year": 2016}], "referenceMentions": [{"referenceID": 1, "context": "Generative Adversarial Networks (GANs) (Goodfellow et al., 2014) represent an important milestone on the path towards more effective generative models.", "startOffset": 39, "endOffset": 64}, {"referenceID": 1, "context": "Generative Adversarial Networks (GANs) (Goodfellow et al., 2014) represent an important milestone on the path towards more effective generative models. GANs cast generative model training as a minimax game between a generative network (generator), which maps a random vector into the data space, and a discriminative network (discriminator), whose objective is to distinguish generated samples from real samples. Multiple researchers Radford et al. (2015); Salimans et al.", "startOffset": 40, "endOffset": 456}, {"referenceID": 1, "context": "Generative Adversarial Networks (GANs) (Goodfellow et al., 2014) represent an important milestone on the path towards more effective generative models. GANs cast generative model training as a minimax game between a generative network (generator), which maps a random vector into the data space, and a discriminative network (discriminator), whose objective is to distinguish generated samples from real samples. Multiple researchers Radford et al. (2015); Salimans et al. (2016); Zhao et al.", "startOffset": 40, "endOffset": 480}, {"referenceID": 1, "context": "Generative Adversarial Networks (GANs) (Goodfellow et al., 2014) represent an important milestone on the path towards more effective generative models. GANs cast generative model training as a minimax game between a generative network (generator), which maps a random vector into the data space, and a discriminative network (discriminator), whose objective is to distinguish generated samples from real samples. Multiple researchers Radford et al. (2015); Salimans et al. (2016); Zhao et al. (2016) have shown that the adversarial interaction with the discriminator can result in a generator that produces compelling samples.", "startOffset": 40, "endOffset": 500}, {"referenceID": 1, "context": "Generative Adversarial Networks (GANs) (Goodfellow et al., 2014) represent an important milestone on the path towards more effective generative models. GANs cast generative model training as a minimax game between a generative network (generator), which maps a random vector into the data space, and a discriminative network (discriminator), whose objective is to distinguish generated samples from real samples. Multiple researchers Radford et al. (2015); Salimans et al. (2016); Zhao et al. (2016) have shown that the adversarial interaction with the discriminator can result in a generator that produces compelling samples. The empirical successes of the GAN framework were also supported by the theoretical analysis of Goodfellow et al., who showed that, under certain conditions, the distribution produced by the generator converges to the true data distribution, while the discriminator converges to a degenerate uniform solution. While GANs have excelled as compelling sample generators, their use as general purpose probabilistic generative models has been limited by the difficulty in using them to provide density estimates or even unnormalized energy values for sample evaluation. It is tempting to consider the GAN discriminator as a candidate for providing this sort of scoring function. Conceptually, it is a trainable sample evaluation mechanism that \u2013 owing to GAN training paradigm \u2013 could be closely calibrated to the distribution modeled by the generator. If the discriminator could retain fine-grained information of the relative quality of samples, measured for instance by probability density or unnormalized energy, it could be used as an evaluation metric. Such data-driven evaluators would be highly desirable for problems where it is difficult to define evaluation criteria that correlate well with human judgment. Indeed, the real-valued discriminator of the recently introduced energy-based GANs Zhao et al. (2016) might seem like an ideal candidate energy function.", "startOffset": 40, "endOffset": 1943}, {"referenceID": 9, "context": "Taking this idea one step further, apprenticeship learning or imitation learning (Abbeel & Ng, 2004; Ziebart et al., 2008) aims at learning a policy (analogous to the generator) using the reward signals recovered by IRL.", "startOffset": 81, "endOffset": 122}, {"referenceID": 8, "context": "The GAN models most closely related to our proposed framework are energy-based GAN models of Zhao et al. (2016) and Kim & Bengio (2016).", "startOffset": 93, "endOffset": 112}, {"referenceID": 8, "context": "The GAN models most closely related to our proposed framework are energy-based GAN models of Zhao et al. (2016) and Kim & Bengio (2016). In the next section, We show how one can derive both of these approaches from different assumptions regarding regularization of the generative model.", "startOffset": 93, "endOffset": 136}, {"referenceID": 1, "context": "Following the original work on GANs (Goodfellow et al., 2014), our analysis focuses on the nonparametric case, where all models are assumed to have infinite capacities.", "startOffset": 36, "endOffset": 61}, {"referenceID": 9, "context": "This elegant result has deep connections to several existing formulations, which include max-entropy imitation learning (Ziebart et al., 2008) and the directed-generator-trained energybased model (Kim & Bengio, 2016).", "startOffset": 120, "endOffset": 142}, {"referenceID": 6, "context": "Our experimental setting follows closely recommendations from Radford et al. (2015), except in Sec.", "startOffset": 62, "endOffset": 84}, {"referenceID": 7, "context": "We futher validate the quality of our model\u2019s samples on CIFAR-10 using the Inception score proposed by (Salimans et al., 2016) 3.", "startOffset": 104, "endOffset": 127}, {"referenceID": 7, "context": "We futher validate the quality of our model\u2019s samples on CIFAR-10 using the Inception score proposed by (Salimans et al., 2016) 3. Table 1 shows the scores of our EGAN-Ent-VI, the best GAN model from Salimans et al. (2016) which uses only unlabeled data, and an EGAN-Const model which has the same architecture as our model.", "startOffset": 105, "endOffset": 223}, {"referenceID": 7, "context": "We futher validate the quality of our model\u2019s samples on CIFAR-10 using the Inception score proposed by (Salimans et al., 2016) 3. Table 1 shows the scores of our EGAN-Ent-VI, the best GAN model from Salimans et al. (2016) which uses only unlabeled data, and an EGAN-Const model which has the same architecture as our model. We notice that even without employing suggested techniques in Salimans et al. (2016), energy-based models perform quite similarly to the GAN model.", "startOffset": 105, "endOffset": 410}], "year": 2017, "abstractText": "In this paper we propose equipping Generative Adversarial Networks with the ability to produce direct energy estimates for samples. Specifically, we develop a flexible adversarial training framework, and prove this framework not only ensures the generator converges to the true data distribution, but also enables the discriminator to retain the density information at the global optimum. We derive the analytic form of the induced solution, and analyze its properties. In order to make the proposed framework trainable in practice, we introduce two effective approximation techniques. Empirically, the experiment results closely match our theoretical analysis, verifying that the discriminator is able to recover the energy of data distribution.", "creator": "LaTeX with hyperref package"}}}