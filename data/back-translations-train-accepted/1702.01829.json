{"id": "1702.01829", "review": {"conference": "ACL", "VERSION": "v1", "DATE_OF_SUBMISSION": "7-Feb-2017", "title": "Neural Discourse Structure for Text Categorization", "abstract": "We show that discourse structure, as defined by Rhetorical Structure Theory and provided by an existing discourse parser, benefits text categorization. Our approach uses a recursive neural network and a newly proposed attention mechanism to compute a representation of the text that focuses on salient content, from the perspective of both RST and the task. Experiments consider variants of the approach and illustrate its strengths and weaknesses.", "histories": [["v1", "Tue, 7 Feb 2017 00:26:56 GMT  (430kb,D)", "https://arxiv.org/abs/1702.01829v1", null], ["v2", "Sat, 6 May 2017 02:08:57 GMT  (448kb,D)", "http://arxiv.org/abs/1702.01829v2", "ACL 2017 camera ready version"]], "reviews": [], "SUBJECTS": "cs.CL cs.LG", "authors": ["yangfeng ji", "noah a smith"], "accepted": true, "id": "1702.01829"}, "pdf": {"name": "1702.01829.pdf", "metadata": {"source": "CRF", "title": "Neural Discourse Structure for Text Categorization", "authors": ["Yangfeng Ji", "Noah A. Smith", "Paul G. Allen"], "emails": ["yangfeng@cs.washington.edu", "nasmith@cs.washington.edu"], "sections": [{"heading": "1 Introduction", "text": "Advances in text categorization have the potential to improve systems for analyzing sentiments, deriving authorship or author attributes, making predictions, and more. Several previous researchers have noted that methods that judge the relative emphasis or meaning of text passages can lead to improvements (Ko et al., 2004). Latent variables (Yessenalina et al., 2010), structured, sparse regulators (Yogatama and Smith, 2014), and neural attention models (Yang et al., 2016) have all been studied, and the discourse structure that depicts the organization of a text as a tree (for example, see Figure 1) could provide clues about the meaning of different parts of a text. Some promising results in sensing tasks support this idea: Bhatia et al. (2015) and Hogenboom et al. (2015) we apply handmade weight schemes to the sentences in a document based on its polarization and utility for the polarization."}, {"heading": "2 Background: Rhetorical Structure Theory", "text": "It is a theory of discourse that is popular in the NLP. RST postulates that a document can be represented by a tree whose leaves are elementary units of discourse (EDUs, typical clauses or sentences).Internal nodes in the tree correspond to the spans of sentences connected via discourse relationships. 1An example of a manually constructed RST for a restaurant overview is in most cases a discourse relationship, the adjacent span denoting \"nucleus\" and \"satellite,\" with which the author pursues essential goals."}, {"heading": "3 Model", "text": "Our model is a recursive neural network based on a discourse dependency tree. It includes a distributed representation calculated for each EDU, and a composition function that combines EDUs and sub-trees into larger trees. At the top of the tree, the representation of the entire document is used to make a categorization decision. Our approach is analogous (and inspired by) the use of recursive neural networks on syntactic dependence trees with word embedded on the leaves (Socher et al., 2014)."}, {"heading": "3.1 Representation of Sentences", "text": "Let's be the distributed representation of an EDU: We use a bidirectional LSTM for the embedding of words within each EDU (details on word embedding are listed in Section 4), which links the last hidden state vector from the forward-looking LSTM (\u2212 \u2192 e) with the backward-looking LSTM (\u2190 \u2212 e) to obtain. There is extensive recent work on architectures for embedding sentence representations and other short pieces of text, including, for example, (bi) recursive neural networks (Paulus et al., 2014) and convolutionary neural networks (Kalchbrenner et al., 2014)."}, {"heading": "3.2 Full Recursive Model", "text": "Considering the discourse dependence tree for an input text, our recursive model forms a vector representation by composition on each arc in the tree. Let's call vi the vector representation of EDU i and its descendants. In the base case where EDU i is a leaf in the tree, let's call vi = tanh (ei), which is the elementary hyperbolic tangent function. For an internal node i, the composition function considers a parent and all his children whose indices are designated by children (i). In defining this composition function, we are looking for (i) the contribution of the parent node ei tactical function to be central; and (iii) the contribution of each child node ej is determined by its content as well as the discourse relationship it holds with the parent. Therefore, we define evi = tanh ei + \u2211 j children (i), jWri, jvj, jvj, (1) where attention is related to a specific Wri by attention."}, {"heading": "3.3 Unlabeled Model", "text": "The FULL model, which is based on Equation 1, uses a dependency discourse tree with relationships. As alternative discourse relationship names have been proposed (e.g. Prasad et al., 2008), we are trying to measure the effect of these terms. Therefore, we are looking at an UNLABELED model, which is based only on the tree structure, without the relationships: vi = tanh ei + \u2211 j, Kinder (i) \u03b1i, jvj. (3) Here only attention weights are used to form the representations of the children, which significantly reduces the number of model parameters. This UNLABELED model is similar to the depth weighting scheme introduced by Bhatia et al. (2015), which also uses an unlabeled discourse dependence tree, but our attention weights are composed by a function whose parameters are learned. This approach lies directly between Bhatia et al. (2015) and the flat document structure, which is still used by Yang et al (the UNED) model."}, {"heading": "3.4 Simpler Variants", "text": "We look at two other baselines that are even simpler: the first, ROOT, uses the discourse-dependent structure only to select the root EDU used to represent the entire text: vroot = eroot. It does not require a composition function. This model variant is motivated by work on document summary (Yoshida et al., 2014), in which the most central EDU is used to represent the entire text. The second variant, ADDITIVE, uses all EDUs with a simple composition function and is not at all dependent on the discourse structure: vroot = 1 N \u2211 N i = 1 ei, where N is the total number of EDUs. This serves as a basis for testing the benefits of the discourse, controlling other design decisions and implementation options. Although sentence representations are structured differently from the work of Yang et al. (2016), this model is quite similar to their HN-AVE model for creating documents."}, {"heading": "4 Implementation Details", "text": "The parameters of all components of our model (top level classification, composition and EDU representation) are learned end-to-end using standard methods.We implement our learning procedure using the DyNet package (Neubig et al., 2017).Pre-processing. We use the same pre-processing steps for all data sets, mostly after recent work on speech modeling (e.g. Mikolov et al., 2010).We have downloaded all tokens and removed tokens that contain only punctuation symbols.We avoid numbers in documents with a special number symbol. Low frequency word types have been replaced by UNK; we reduce the vocabulary for each data set until about 5% of the tokens are mapped to UNK. Vocabulary sizes after pre-processing are also shown in Table 1.Discourse analyses. Our model requires the discourse structure for each document. We used PLP, the RST parameter from 2000, and one of the best discourse (2014)."}, {"heading": "5 Datasets", "text": "We selected five datasets of different sizes and correspondences to different categorization tasks. Some information about these datasets is summarized in Table 1.Sentiment Analysis on Yelp Ratings. Originally from the 2015 Yelp Dataset Challenge, this dataset contains 1.5 million examples. We used the pre-made datasets from Zhang et al. (2015), which have 650,000 training data and 50,000 test examples. The task is to make a proper assessment (1-5) from the text of the review. To select the best combination of hyperparameters, we have 10% training examples such as the development data. We compared with hierarchical attention networks (Yang et al.), which use the normative attention mechanism on both word and sentence structures, and we offer a flat document structure."}, {"heading": "6 Experiments", "text": "This year, more than ever before in the history of the country in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country."}, {"heading": "7 Related Work", "text": "Early work on text categorization often treated text like a bag of words (e.g., Joachims, 1998; Yang and Pedersen, 1997); representational learning, for example, through matrix decomposition (Deerwester et al., 1990) or latent theme variables (Ramage et al., 2009), was considered to avoid overmatching in the face of sparse data; the assumption that all parts of a text should affect categorization equally persists, even when stronger representational learners are taken into account. Zhang et al. (2015) treat a text as a sequence of signs and propose a deep Convolutionary Neural Network to build a text representation. Xiao and Cho (2016) expanded this architecture by inserting a recursive neural network layer between the Convolutionary Layer and the Classification Layer. In contrast, our contributions Ko et al. (2004), which sought to directly influence the different parts of a text's input function on the meaning of a text's two-weighted task, follow."}, {"heading": "8 Conclusion", "text": "We conclude that automatically derived discourse structures can be helpful for text categorization, and that the benefit increases with the accuracy of discourse sparsing. We saw no benefit in categorizing draft laws, a text genre whose discourse structure differs from the news structure, and these results motivate further improvements in discourse sparsing, especially for new genres."}, {"heading": "Acknowledgments", "text": "We thank anonymous reviewers and members of Noah's ARK for helpful feedback on this work. We thank Dallas Card and Jesse Dodge for their help in preparing the Media Frames Corpus and the Congressional Bill Corpus. This work was made possible by an Innovation Award from the University of Washington."}, {"heading": "A Supplementary Material: An example text from the Bill corpus", "text": "In the United States, the number of home burglaries has fallen by more than half over the past decade. In the United States, the number of home burglaries has fallen by more than half over the past decade. In the United States, the number of home burglaries has fallen by more than half; in the United States, the number of home burglaries has fallen by more than half; in the United States, the number of home burglaries has increased by more than half; in the United States, the number of home burglaries has increased by more than half; in the United States, the number of home burglaries has increased by more than half; in the United States, the number of burglaries has increased by more than half."}], "references": [{"title": "Argumentative text as rhetorical structure: An application of rhetorical structure theory", "author": ["Moshe Azar."], "venue": "Argumentation 13(1):97\u2013114.", "citeRegEx": "Azar.,? 1999", "shortCiteRegEx": "Azar.", "year": 1999}, {"title": "Neural machine translation by jointly learning to align and translate", "author": ["Dzmitry Bahdanau", "Kyunghyun Cho", "Yoshua Bengio."], "venue": "ICLR.", "citeRegEx": "Bahdanau et al\\.,? 2015", "shortCiteRegEx": "Bahdanau et al\\.", "year": 2015}, {"title": "Learning latent personas of film characters", "author": ["David Bamman", "Brendan O\u2019Connor", "Noah A Smith"], "venue": null, "citeRegEx": "Bamman et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Bamman et al\\.", "year": 2014}, {"title": "Better document-level sentiment analysis from RST discourse parsing", "author": ["Parminder Bhatia", "Yangfeng Ji", "Jacob Eisenstein."], "venue": "EMNLP.", "citeRegEx": "Bhatia et al\\.,? 2015", "shortCiteRegEx": "Bhatia et al\\.", "year": 2015}, {"title": "The Media Frames Corpus: Annotations of frames across issues", "author": ["Dallas Card", "Amber E. Boydstun", "Justin H. Gross", "Philip Resnik", "Noah A. Smith."], "venue": "ACL.", "citeRegEx": "Card et al\\.,? 2015", "shortCiteRegEx": "Card et al\\.", "year": 2015}, {"title": "Analyzing framing through the casts of characters in the news", "author": ["Dallas Card", "Justin Gross", "Amber E. Boydstun", "Noah A. Smith."], "venue": "EMNLP.", "citeRegEx": "Card et al\\.,? 2016", "shortCiteRegEx": "Card et al\\.", "year": 2016}, {"title": "Building a discourse-tagged corpus in the framework of Rhetorical Structure Theory", "author": ["Lynn Carlson", "Daniel Marcu", "Mary Ellen Okurowski."], "venue": "Proceedings of Second SIGdial Workshop on Discourse and Dialogue.", "citeRegEx": "Carlson et al\\.,? 2001", "shortCiteRegEx": "Carlson et al\\.", "year": 2001}, {"title": "Indexing by latent semantic analysis", "author": ["Scott Deerwester", "Susan T. Dumais", "George W. Furnas", "Thomas K. Landauer", "Richard Harshman."], "venue": "Journal of the American Society for Information Science 41(6):391.", "citeRegEx": "Deerwester et al\\.,? 1990", "shortCiteRegEx": "Deerwester et al\\.", "year": 1990}, {"title": "Using rhetorical structure in sentiment analysis", "author": ["Alexander Hogenboom", "Flavius Frasincar", "Franciska de Jong", "Uzay Kaymak."], "venue": "Communications of the ACM 58(7):69\u201377.", "citeRegEx": "Hogenboom et al\\.,? 2015", "shortCiteRegEx": "Hogenboom et al\\.", "year": 2015}, {"title": "Representation learning for document-level discourse parsing", "author": ["Yangfeng Ji", "Jacob Eisenstein."], "venue": "ACL.", "citeRegEx": "Ji and Eisenstein.,? 2014", "shortCiteRegEx": "Ji and Eisenstein.", "year": 2014}, {"title": "One vector is not enough: Entity-augmented distributed semantics for discourse relations", "author": ["Yangfeng Ji", "Jacob Eisenstein."], "venue": "Transactions of the Association of Computational Linguistics 3:329\u2013344.", "citeRegEx": "Ji and Eisenstein.,? 2015", "shortCiteRegEx": "Ji and Eisenstein.", "year": 2015}, {"title": "Text categorization with support vector machines: Learning with many relevant features", "author": ["Thorsten Joachims."], "venue": "ECML.", "citeRegEx": "Joachims.,? 1998", "shortCiteRegEx": "Joachims.", "year": 1998}, {"title": "A convolutional neural network for modelling sentences", "author": ["Nal Kalchbrenner", "Edward Grefenstette", "Phil Blunsom."], "venue": "ArXiv:1404.2188.", "citeRegEx": "Kalchbrenner et al\\.,? 2014", "shortCiteRegEx": "Kalchbrenner et al\\.", "year": 2014}, {"title": "Adam: A method for stochastic optimization", "author": ["Diederik Kingma", "Jimmy Ba."], "venue": "ICLR.", "citeRegEx": "Kingma and Ba.,? 2015", "shortCiteRegEx": "Kingma and Ba.", "year": 2015}, {"title": "Improving text categorization using the importance of sentences", "author": ["Youngjoong Ko", "Jinwoo Park", "Jungyun Seo."], "venue": "Information Processing & Management 40(1):65\u201379.", "citeRegEx": "Ko et al\\.,? 2004", "shortCiteRegEx": "Ko et al\\.", "year": 2004}, {"title": "What do recurrent neural network grammars learn about syntax? In EACL", "author": ["Adhiguna Kuncoro", "Miguel Ballesteros", "Lingpeng Kong", "Chris Dyer", "Graham Neubig", "Noah A. Smith"], "venue": null, "citeRegEx": "Kuncoro et al\\.,? \\Q2017\\E", "shortCiteRegEx": "Kuncoro et al\\.", "year": 2017}, {"title": "Rhetorical Structure Theory: Toward a functional theory of text organization", "author": ["William Mann", "Sandra Thompson."], "venue": "Text 8(3):243\u2013281.", "citeRegEx": "Mann and Thompson.,? 1988", "shortCiteRegEx": "Mann and Thompson.", "year": 1988}, {"title": "Discourse trees are good indicators of importance in text", "author": ["Daniel Marcu."], "venue": "Inderjeet Mani and Mark T. Maybury, editors, Advances in Automatic Text Summarization, pages 123\u2013136.", "citeRegEx": "Marcu.,? 1999", "shortCiteRegEx": "Marcu.", "year": 1999}, {"title": "Building a large annotated corpus of English: The Penn Treebank", "author": ["Mitchell P. Marcus", "Mary Ann Marcinkiewicz", "Beatrice Santorini."], "venue": "Computational Linguistics 19(2):313\u2013330.", "citeRegEx": "Marcus et al\\.,? 1993", "shortCiteRegEx": "Marcus et al\\.", "year": 1993}, {"title": "Recurrent neural network based language model", "author": ["Tomas Mikolov", "Martin Karafi\u00e1t", "Lukas Burget", "Jan Cernock\u1ef3", "Sanjeev Khudanpur."], "venue": "INTERSPEECH.", "citeRegEx": "Mikolov et al\\.,? 2010", "shortCiteRegEx": "Mikolov et al\\.", "year": 2010}, {"title": "Dynet: The dynamic neural network toolkit. ArXiv:1701.03980", "author": ["Graham Neubig", "Chris Dyer", "Yoav Goldberg", "Austin Matthews", "Waleed Ammar", "Antonios Anastasopoulos", "Miguel Ballesteros", "David Chiang", "Daniel Clothiaux", "Trevor Cohn"], "venue": null, "citeRegEx": "Neubig et al\\.,? \\Q2017\\E", "shortCiteRegEx": "Neubig et al\\.", "year": 2017}, {"title": "A sentimental education: Sentiment analysis using subjectivity summarization based on minimum cuts", "author": ["Bo Pang", "Lillian Lee."], "venue": "Proceedings of the 42nd annual meeting on Association for Computational Linguistics. Association for Computational", "citeRegEx": "Pang and Lee.,? 2004", "shortCiteRegEx": "Pang and Lee.", "year": 2004}, {"title": "Global belief recursive neural networks", "author": ["Romain Paulus", "Richard Socher", "Christopher D Manning."], "venue": "NIPS.", "citeRegEx": "Paulus et al\\.,? 2014", "shortCiteRegEx": "Paulus et al\\.", "year": 2014}, {"title": "GloVe: Global vectors for word representation", "author": ["Jeffrey Pennington", "Richard Socher", "Christopher D Manning."], "venue": "EMNLP.", "citeRegEx": "Pennington et al\\.,? 2014", "shortCiteRegEx": "Pennington et al\\.", "year": 2014}, {"title": "Labeled lda: A supervised topic model for credit attribution in multilabeled corpora", "author": ["Daniel Ramage", "David Hall", "Ramesh Nallapati", "Christopher D. Manning."], "venue": "EMNLP.", "citeRegEx": "Ramage et al\\.,? 2009", "shortCiteRegEx": "Ramage et al\\.", "year": 2009}, {"title": "Grounded compositional semantics for finding and describing images with sentences", "author": ["Richard Socher", "Andrej Karpathy", "Quoc V Le", "Christopher D. Manning", "Andrew Y. Ng."], "venue": "Transactions of the Association for Computational Linguistics", "citeRegEx": "Socher et al\\.,? 2014", "shortCiteRegEx": "Socher et al\\.", "year": 2014}, {"title": "Get out the vote: Determining support or opposition from Congressional floor-debate transcripts", "author": ["Matt Thomas", "Bo Pang", "Lillian Lee."], "venue": "EMNLP.", "citeRegEx": "Thomas et al\\.,? 2006", "shortCiteRegEx": "Thomas et al\\.", "year": 2006}, {"title": "Efficient character-level document classification by combining convolution and recurrent layers", "author": ["Yijun Xiao", "Kyunghyun Cho."], "venue": "ArXiv:1602.00367.", "citeRegEx": "Xiao and Cho.,? 2016", "shortCiteRegEx": "Xiao and Cho.", "year": 2016}, {"title": "Statistical dependency analysis with support vector machines", "author": ["H. Yamada", "Y. Matsumoto."], "venue": "IWPT .", "citeRegEx": "Yamada and Matsumoto.,? 2003", "shortCiteRegEx": "Yamada and Matsumoto.", "year": 2003}, {"title": "A comparative study on feature selection in text categorization", "author": ["Yiming Yang", "Jan O. Pedersen."], "venue": "ICML.", "citeRegEx": "Yang and Pedersen.,? 1997", "shortCiteRegEx": "Yang and Pedersen.", "year": 1997}, {"title": "Hierarchical attention networks for document classification", "author": ["Zichao Yang", "Diyi Yang", "Chris Dyer", "Xiaodong He", "Alex Smola", "Eduard Hovy."], "venue": "NAACL.", "citeRegEx": "Yang et al\\.,? 2016", "shortCiteRegEx": "Yang et al\\.", "year": 2016}, {"title": "Textual predictors of bill survival in congressional committees", "author": ["Tae Yano", "Noah A. Smith", "John D. Wilkerson."], "venue": "NAACL.", "citeRegEx": "Yano et al\\.,? 2012", "shortCiteRegEx": "Yano et al\\.", "year": 2012}, {"title": "Multi-level structured models for document sentiment classification", "author": ["Ainur Yessenalina", "Yisong Yue", "Claire Cardie."], "venue": "EMNLP.", "citeRegEx": "Yessenalina et al\\.,? 2010", "shortCiteRegEx": "Yessenalina et al\\.", "year": 2010}, {"title": "Linguistic structured sparsity in text categorization", "author": ["Dani Yogatama", "Noah A. Smith."], "venue": "ACL.", "citeRegEx": "Yogatama and Smith.,? 2014", "shortCiteRegEx": "Yogatama and Smith.", "year": 2014}, {"title": "Dependency-based discourse parser for single-document summarization", "author": ["Yasuhisa Yoshida", "Jun Suzuki", "Tsutomu Hirao", "Masaaki Nagata."], "venue": "EMNLP.", "citeRegEx": "Yoshida et al\\.,? 2014", "shortCiteRegEx": "Yoshida et al\\.", "year": 2014}, {"title": "Character-level convolutional networks for text classification", "author": ["Xiang Zhang", "Junbo Zhao", "Yann LeCun."], "venue": "NIPS.", "citeRegEx": "Zhang et al\\.,? 2015", "shortCiteRegEx": "Zhang et al\\.", "year": 2015}], "referenceMentions": [{"referenceID": 14, "context": "Several past researchers have noticed that methods that reason about the relative salience or importance of passages within a text can lead to improvements (Ko et al., 2004).", "startOffset": 156, "endOffset": 173}, {"referenceID": 32, "context": "Latent variables (Yessenalina et al., 2010), structured-sparse regularizers (Yogatama and Smith, 2014), and neural attention models (Yang et al.", "startOffset": 17, "endOffset": 43}, {"referenceID": 33, "context": ", 2010), structured-sparse regularizers (Yogatama and Smith, 2014), and neural attention models (Yang et al.", "startOffset": 40, "endOffset": 66}, {"referenceID": 30, "context": ", 2010), structured-sparse regularizers (Yogatama and Smith, 2014), and neural attention models (Yang et al., 2016) have all been explored.", "startOffset": 96, "endOffset": 115}, {"referenceID": 3, "context": "Some promising results on sentiment classification tasks support this idea: Bhatia et al. (2015) and Hogenboom et al.", "startOffset": 76, "endOffset": 97}, {"referenceID": 3, "context": "Some promising results on sentiment classification tasks support this idea: Bhatia et al. (2015) and Hogenboom et al. (2015) applied hand-crafted weighting schemes to the sentences in a document, based on its discourse structure, and showed benefit to sentiment polarity classification.", "startOffset": 76, "endOffset": 125}, {"referenceID": 16, "context": "Figure 1: A manually constructed example of the RST (Mann and Thompson, 1988) discourse structure on a text.", "startOffset": 52, "endOffset": 77}, {"referenceID": 9, "context": "automatically-derived document parse from a topperforming, open-source discourse parser, DPLP (Ji and Eisenstein, 2014).", "startOffset": 94, "endOffset": 119}, {"referenceID": 16, "context": "Rhetorical Structure Theory (RST; Mann and Thompson, 1988) is a theory of discourse that has enjoyed popularity in NLP.", "startOffset": 28, "endOffset": 58}, {"referenceID": 17, "context": "The information embedded in RST trees has motivated many applications in NLP research, including document summarization (Marcu, 1999), argumentation mining (Azar, 1999), and sentiment analysis (Bhatia et al.", "startOffset": 120, "endOffset": 133}, {"referenceID": 0, "context": "The information embedded in RST trees has motivated many applications in NLP research, including document summarization (Marcu, 1999), argumentation mining (Azar, 1999), and sentiment analysis (Bhatia et al.", "startOffset": 156, "endOffset": 168}, {"referenceID": 3, "context": "The information embedded in RST trees has motivated many applications in NLP research, including document summarization (Marcu, 1999), argumentation mining (Azar, 1999), and sentiment analysis (Bhatia et al., 2015).", "startOffset": 193, "endOffset": 214}, {"referenceID": 9, "context": "In this work, we use a state-of-the-art open-source RST-style discourse parser, DPLP (Ji and Eisenstein, 2014).", "startOffset": 85, "endOffset": 110}, {"referenceID": 34, "context": "2 We follow recent work that suggests transforming the RST tree into a dependency structure (Yoshida et al., 2014).", "startOffset": 92, "endOffset": 114}, {"referenceID": 25, "context": "Our approach is analogous to (and inspired by) the use of recursive neural networks on syntactic dependency trees, with word embeddings at the leaves (Socher et al., 2014).", "startOffset": 150, "endOffset": 171}, {"referenceID": 22, "context": "There is extensive recent work on architectures for embedding representations of sentences and other short pieces of text, including, for example, (bi)recursive neural networks (Paulus et al., 2014) and convolutional neural networks (Kalchbrenner et al.", "startOffset": 177, "endOffset": 198}, {"referenceID": 12, "context": ", 2014) and convolutional neural networks (Kalchbrenner et al., 2014).", "startOffset": 42, "endOffset": 69}, {"referenceID": 1, "context": "Our attention mechanism differs from prior work (Bahdanau et al., 2015), in which attention weights are normalized to sum to one across competing candidates for attention.", "startOffset": 48, "endOffset": 71}, {"referenceID": 15, "context": "It also differs from attention in composition functions used in syntactic parsing (Kuncoro et al., 2017), where attention can mimic head rules that follow from an endocentricity hypothesis of syntactic phrase representation.", "startOffset": 82, "endOffset": 104}, {"referenceID": 3, "context": "This idea of using a weighting scheme along with discourse structure is explored in prior works (Bhatia et al., 2015; Hogenboom et al., 2015), although they are manually designed, rather than learned from training data.", "startOffset": 96, "endOffset": 141}, {"referenceID": 8, "context": "This idea of using a weighting scheme along with discourse structure is explored in prior works (Bhatia et al., 2015; Hogenboom et al., 2015), although they are manually designed, rather than learned from training data.", "startOffset": 96, "endOffset": 141}, {"referenceID": 3, "context": "This UNLABELED model is similar to the depth weighting scheme introduced by Bhatia et al. (2015), which also uses an unlabeled discourse dependency tree, but our attention weights are computed by a function whose parameters are learned.", "startOffset": 76, "endOffset": 97}, {"referenceID": 3, "context": "This UNLABELED model is similar to the depth weighting scheme introduced by Bhatia et al. (2015), which also uses an unlabeled discourse dependency tree, but our attention weights are computed by a function whose parameters are learned. This approach sits squarely between Bhatia et al. (2015) and the flat document structure used by Yang et al.", "startOffset": 76, "endOffset": 294}, {"referenceID": 3, "context": "This UNLABELED model is similar to the depth weighting scheme introduced by Bhatia et al. (2015), which also uses an unlabeled discourse dependency tree, but our attention weights are computed by a function whose parameters are learned. This approach sits squarely between Bhatia et al. (2015) and the flat document structure used by Yang et al. (2016); the UNLABELED model still uses discourse to bias the model toward some content (that which is closer to the tree\u2019s root).", "startOffset": 76, "endOffset": 353}, {"referenceID": 34, "context": "This model variant is motivated by work on document summarization (Yoshida et al., 2014), where the", "startOffset": 66, "endOffset": 88}, {"referenceID": 30, "context": "Although sentence representations ei are built in a different way from the work of Yang et al. (2016), this model is quite similar to their HN-AVE model on building document representations.", "startOffset": 83, "endOffset": 102}, {"referenceID": 20, "context": "We implement our learning procedure with the DyNet package (Neubig et al., 2017).", "startOffset": 59, "endOffset": 80}, {"referenceID": 6, "context": "We used DPLP, the RST parser from Ji and Eisenstein (2014), which is one of the best discourse parsers on the RST discourse treebank benchmark (Carlson et al., 2001).", "startOffset": 143, "endOffset": 165}, {"referenceID": 18, "context": "DPLP as distributed is trained on 347 Wall Street Journal articles from the Penn Treebank (Marcus et al., 1993).", "startOffset": 90, "endOffset": 111}, {"referenceID": 8, "context": "We used DPLP, the RST parser from Ji and Eisenstein (2014), which is one of the best discourse parsers on the RST discourse treebank benchmark (Carlson et al.", "startOffset": 34, "endOffset": 59}, {"referenceID": 6, "context": "We used DPLP, the RST parser from Ji and Eisenstein (2014), which is one of the best discourse parsers on the RST discourse treebank benchmark (Carlson et al., 2001). It employs a greedy decoding algorithm for parsing, producing 2,000 parses per minute on average on a single CPU. DPLP provides discourse segmentation, breaking a text into EDUs, typically clauses or sentences, based on syntactic parses provided by Stanford CoreNLP. RST trees are converted to dependencies following the method of Yoshida et al. (2014). DPLP as distributed is trained on 347 Wall Street Journal articles from the Penn Treebank (Marcus et al.", "startOffset": 144, "endOffset": 520}, {"referenceID": 23, "context": "In cases where there are 10,000 or fewer training examples, we used pretrained GloVe word embeddings (Pennington et al., 2014), following previous work on neural discourse processing (Ji and Eisenstein, 2015).", "startOffset": 101, "endOffset": 126}, {"referenceID": 10, "context": ", 2014), following previous work on neural discourse processing (Ji and Eisenstein, 2015).", "startOffset": 64, "endOffset": 89}, {"referenceID": 30, "context": "We compared with hierarchical attention networks (Yang et al., 2016), which use the normalized attention mechanism on both word and sentence layers with a flat document structure, and provide the state-of-the-art result on this corpus.", "startOffset": 49, "endOffset": 68}, {"referenceID": 34, "context": "We used the preprocessed dataset from Zhang et al. (2015), which has 650,000 training and 50,000 test examples.", "startOffset": 38, "endOffset": 58}, {"referenceID": 4, "context": "The Media Frames Corpus (MFC; Card et al., 2015) includes around 4,200 news articles about immigration from 13 U.", "startOffset": 24, "endOffset": 48}, {"referenceID": 2, "context": "(2016), where they used logistic regression together with unigrams, bigrams and Bamman-style personas (Bamman et al., 2014) as features.", "startOffset": 102, "endOffset": 123}, {"referenceID": 5, "context": "Then, we report average accuracy across 10-fold cross validation as in (Card et al., 2016).", "startOffset": 71, "endOffset": 90}, {"referenceID": 3, "context": "The state-of-the-art result on this corpus is from Card et al. (2016), where they used logistic regression together with unigrams, bigrams and Bamman-style personas (Bamman et al.", "startOffset": 51, "endOffset": 70}, {"referenceID": 26, "context": "The corpus was originally collected by Thomas et al. (2006), and the data split we used was constructed by Yessenalina et al.", "startOffset": 39, "endOffset": 60}, {"referenceID": 26, "context": "The corpus was originally collected by Thomas et al. (2006), and the data split we used was constructed by Yessenalina et al. (2010). The goal is to predict the vote (\u201cyea\u201d or \u201cnay\u201d) for the speaker of each speech segment.", "startOffset": 39, "endOffset": 133}, {"referenceID": 26, "context": "The corpus was originally collected by Thomas et al. (2006), and the data split we used was constructed by Yessenalina et al. (2010). The goal is to predict the vote (\u201cyea\u201d or \u201cnay\u201d) for the speaker of each speech segment. The most recent work on this corpus is from Yogatama and Smith (2014), which proposed structured regularization methods based on linguistic components, e.", "startOffset": 39, "endOffset": 293}, {"referenceID": 19, "context": "This classic movie review corpus was constructed by Pang and Lee (2004) and includes 1,000 positive and 1,000 negative reviews.", "startOffset": 52, "endOffset": 72}, {"referenceID": 3, "context": "We compared with the work from both Bhatia et al. (2015) and Hogenboom et al.", "startOffset": 36, "endOffset": 57}, {"referenceID": 3, "context": "We compared with the work from both Bhatia et al. (2015) and Hogenboom et al. (2015), which are two recent works on discourse for sentiment analysis.", "startOffset": 36, "endOffset": 85}, {"referenceID": 3, "context": "We compared with the work from both Bhatia et al. (2015) and Hogenboom et al. (2015), which are two recent works on discourse for sentiment analysis. Bhatia et al. (2015) used a hand-crafted weighting scheme to bias the bag-of-word representations on sentences.", "startOffset": 36, "endOffset": 171}, {"referenceID": 3, "context": "We compared with the work from both Bhatia et al. (2015) and Hogenboom et al. (2015), which are two recent works on discourse for sentiment analysis. Bhatia et al. (2015) used a hand-crafted weighting scheme to bias the bag-of-word representations on sentences. Hogenboom et al. (2015) also considered manually-designed weighting schemes and a lexicon-based model as classifier, achieving performance inferior to fully-supervised methods like Bhatia et al.", "startOffset": 36, "endOffset": 286}, {"referenceID": 3, "context": "We compared with the work from both Bhatia et al. (2015) and Hogenboom et al. (2015), which are two recent works on discourse for sentiment analysis. Bhatia et al. (2015) used a hand-crafted weighting scheme to bias the bag-of-word representations on sentences. Hogenboom et al. (2015) also considered manually-designed weighting schemes and a lexicon-based model as classifier, achieving performance inferior to fully-supervised methods like Bhatia et al. (2015) and ours.", "startOffset": 36, "endOffset": 464}, {"referenceID": 31, "context": "This corpus, collected by Yano et al. (2012), includes 51,762 legislative bills from the 103rd to 111th U.", "startOffset": 26, "endOffset": 45}, {"referenceID": 31, "context": "This corpus, collected by Yano et al. (2012), includes 51,762 legislative bills from the 103rd to 111th U.S. Congresses. The task is to predict whether a bill will survive based on its content. We randomly sampled 10% training examples as development data to search for the best hyperparameters. To our knowledge, the best published results are due to Yogatama and Smith (2014), which is the same baseline as for the congressional floor debates corpus.", "startOffset": 26, "endOffset": 378}, {"referenceID": 26, "context": "Yang et al. (2016) 71.", "startOffset": 0, "endOffset": 19}, {"referenceID": 3, "context": "Card et al. (2016) \u2014 56.", "startOffset": 0, "endOffset": 19}, {"referenceID": 3, "context": "Card et al. (2016) \u2014 56.8 \u2014 \u2014 \u2014 3. Yogatama and Smith (2014) \u2014 \u2014 74.", "startOffset": 0, "endOffset": 61}, {"referenceID": 3, "context": "Bhatia et al. (2015) \u2014 \u2014 \u2014 82.", "startOffset": 0, "endOffset": 21}, {"referenceID": 3, "context": "Bhatia et al. (2015) \u2014 \u2014 \u2014 82.9 \u2014 5. Hogenboom et al. (2015) \u2014 \u2014 \u2014 71.", "startOffset": 0, "endOffset": 61}, {"referenceID": 9, "context": "Figure 3: Some example texts (with light revision for readability) from the Yelp Review corpus and their corresponding dependency discourse parses from DPLP (Ji and Eisenstein, 2014).", "startOffset": 157, "endOffset": 182}, {"referenceID": 30, "context": "Notably, if the RST discourse treebank were reduced to 25% of its size, our method would underperform the discourseignorant model of Yang et al. (2016). While we cannot extrapolate with certainty, these findings suggest that further improvements to discourse parsing, through larger annotated datasets or improved models, could lead to greater gains.", "startOffset": 133, "endOffset": 152}, {"referenceID": 1, "context": "In section 3, we contrasted our new attention mechanism (Equation 2), which is inspired by RST\u2019s lack of \u201ccompetition\u201d for salience among satellites, with the attention mechanism used in machine translation (Bahdanau et al., 2015).", "startOffset": 207, "endOffset": 230}, {"referenceID": 29, "context": "Early work on text categorization often treated text as a bag of words (e.g., Joachims, 1998; Yang and Pedersen, 1997).", "startOffset": 71, "endOffset": 118}, {"referenceID": 7, "context": "Representation learning, for example through matrix decomposition (Deerwester et al., 1990) or latent topic variables (Ramage et al.", "startOffset": 66, "endOffset": 91}, {"referenceID": 24, "context": ", 1990) or latent topic variables (Ramage et al., 2009), has been considered to avoid overfitting in the face of sparse data.", "startOffset": 34, "endOffset": 55}, {"referenceID": 34, "context": "Zhang et al. (2015) treat a text as a sequence of characters, proposing to a deep convolutional neural network to build text representation.", "startOffset": 0, "endOffset": 20}, {"referenceID": 27, "context": "Xiao and Cho (2016) extended that architecture by inserting a recurrent neural network layer between the convolutional layer and the classification layer.", "startOffset": 0, "endOffset": 20}, {"referenceID": 14, "context": "In contrast, our contributions follow Ko et al. (2004), who sought to weight the influence of different parts of an input text on the task.", "startOffset": 38, "endOffset": 55}, {"referenceID": 14, "context": "In contrast, our contributions follow Ko et al. (2004), who sought to weight the influence of different parts of an input text on the task. Two works that sought to learn the importance of sentences in a document are Yessenalina et al. (2010) and Yang et al.", "startOffset": 38, "endOffset": 243}, {"referenceID": 14, "context": "In contrast, our contributions follow Ko et al. (2004), who sought to weight the influence of different parts of an input text on the task. Two works that sought to learn the importance of sentences in a document are Yessenalina et al. (2010) and Yang et al. (2016). The former used a latent variable for the informativeness of each sentence, and the latter used a neural network to learn an attention function.", "startOffset": 38, "endOffset": 266}, {"referenceID": 33, "context": "presented by Yogatama and Smith (2014), who used a bag-of-words model.", "startOffset": 13, "endOffset": 39}, {"referenceID": 3, "context": "Bhatia et al. (2015) proposed two discourse-motivated models for sentiment polarity prediction.", "startOffset": 0, "endOffset": 21}], "year": 2017, "abstractText": "We show that discourse structure, as defined by Rhetorical Structure Theory and provided by an existing discourse parser, benefits text categorization. Our approach uses a recursive neural network and a newly proposed attention mechanism to compute a representation of the text that focuses on salient content, from the perspective of both RST and the task. Experiments consider variants of the approach and illustrate its strengths and weaknesses.", "creator": "LaTeX with hyperref package"}}}