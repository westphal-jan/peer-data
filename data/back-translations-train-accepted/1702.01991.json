{"id": "1702.01991", "review": {"conference": "ACL", "VERSION": "v1", "DATE_OF_SUBMISSION": "7-Feb-2017", "title": "Representations of language in a model of visually grounded speech signal", "abstract": "We present a visually grounded model of speech perception which projects spoken utterances and images to a joint semantic space. We use a multi-layer recurrent highway network to model the temporal nature of spoken speech, and show that it learns to extract both form and meaning-based linguistic knowledge from the input signal. We carry out an in-depth analysis of the representations used by different components of the trained model and show that encoding of semantic aspects tends to become richer as we go up the hierarchy of layers, whereas encoding of form-related aspects of the language input tends to initially increase and then plateau or decrease.", "histories": [["v1", "Tue, 7 Feb 2017 13:02:09 GMT  (738kb,D)", "https://arxiv.org/abs/1702.01991v1", "10 pages"], ["v2", "Thu, 1 Jun 2017 12:57:36 GMT  (742kb,D)", "http://arxiv.org/abs/1702.01991v2", "Accepted at ACL 2017"], ["v3", "Fri, 30 Jun 2017 07:34:55 GMT  (742kb,D)", "http://arxiv.org/abs/1702.01991v3", "Accepted at ACL 2017"]], "COMMENTS": "10 pages", "reviews": [], "SUBJECTS": "cs.CL cs.AI cs.LG", "authors": ["grzegorz chrupala", "lieke gelderloos", "afra alishahi"], "accepted": true, "id": "1702.01991"}, "pdf": {"name": "1702.01991.pdf", "metadata": {"source": "META", "title": "Representations of language in a model of visually grounded speech signal", "authors": ["Grzegorz Chrupa\u0142a", "Lieke Gelderloos", "Afra Alishahi"], "emails": ["g.chrupala@uvt.nl", "l.j.gelderloos@uvt.nl", "a.alishahi@uvt.nl"], "sections": [{"heading": "1 Introduction", "text": "In fact, we will be able to go in search of a solution that will enable us, that will enable us, that will enable us, that will enable us, that will enable us, that will enable us, that will enable us, that will enable us, that will enable us, that will enable us, that will enable us, that will enable us, that will enable us, that will enable us, that will enable us, that will enable us, that will enable us to put ourselves in the position we are in."}, {"heading": "2 Related work", "text": "Children learn to recognize and assign meaning to words from continuous perceptual data in an extremely loud context. Although there have been many computational studies on the acquisition of human word meanings, they typically make simplistic assumptions about the nature of the input. Speech input often takes the form of word symbols, and the context consists of a series of symbols that represent possible references (e.g. Siskind, 1996; Frank et al., 2007; Fazly et al., 2010). In contrast, several studies have presented models that learn from sensory rather than symbolic inputs that are rich in signals themselves but very limited in scope and variation (e.g. Roy and Pentland, 2002; Yu and Ballard, 2004; Lazaridou et al, 2016)."}, {"heading": "2.1 Multimodal language acquisition", "text": "Chrupa\u0142a et al. (2015) introduce a model that learns to predict the visual context using captions, based on caption pairs from MSCOCO (Lin et al., 2014), and captures both rich visual input and large-format input, but speech input is still made up of word symbols. Gelderloos and Chrupa\u0142a (2016) propose a similar architecture that uses phonemrelevant transcriptions as speech input instead, thereby incorporating the word segmentation problem into the learning task. In this work, we present an architecture that learns directly from continuous language and images. This work is related to the exploration of visual language creation. The field is large and growing, with most work devoted to the creation of written text, particularly in image-writing tasks (see Bernardi et al. (2016) for an overview."}, {"heading": "2.2 Analysis of neural representations", "text": "While the analysis of neural methods in NLP is often limited to assessing performance in the training task, methods have recently been introduced to look inside the black box and investigate what enables the model to perform this task. One approach is to look at the contribution of certain parts of the input or certain units in the model in final representations or decisions. Li et al. (2016) proposes a similar approach to outputs, a method to estimate the contribution of input tokens to the final representation by removing them from input and comparing the resulting representations with those generated by the original input. Li et al. (2016) takes a similar approach to examine the contribution of individual input tokens as well as hidden units and word embedding dimensions by erasing them from the representation and analyzing how this affects modeling."}, {"heading": "3 Models", "text": "We use a multi-layered recursive neural network (RHN) to model the temporal nature of the speech signal. Recursive neural networks are designed to model sequential data, and gated variants (GRUs, LSTMs) are widely used with language and text in both cognitive modeling and engineering contexts. RHNs are a simple generalization of GRU networks whose transformation between points of time can consist of several steps. (2015) Our multimodal model projects utterances and images into a common semantic space. The idea of projecting different modalities onto a common semantic space using a pair of coders has been used in working on language and vision (among them Vendrov et al.) The core idea is to promote inputs that represent the same meaning nearby while maintaining a distance from unrelated subjects. The model consists of two parts - an image and a coder."}, {"heading": "4 Experiments", "text": "In Section 4.2, we evaluate the effectiveness of our architecture by evaluating it against the task of evaluating images on the basis of a statement. In Section 4.3 to 4.6, we present our analyses. In Section 4.3 and 4.4, we define auxiliary tasks to examine the extent to which the network encodes information on the surface shape of an expression from speech input. In Section 4.5 and 4.6, we focus on where semantic information is encoded in the model. In the analyses, we use the following characteristics: embedding maternity: the weighted sum of unit activations on the last layer, as calculated by Equation (3). Average unit activations: hidden layer activations on average of time and L2-normalized activations for each hidden layer. Mean of unit activations on the last layer, as calculated by Equation (3)."}, {"heading": "4.1 Data", "text": "For the experiments reported in the rest of the paper, we use two datasets with captions."}, {"heading": "4.1.1 Flickr8K", "text": "The Flickr8k Audio Caption Corpus was designed by Crowdsource staff to have the captions in the original Flickr8K corpus read aloud (Hodosh et al., 2013). Details of the data acquisition process can be found in Harwath and Glass (2015). The data sets consist of 8,000 images, each image with five descriptions. One thousand images are provided for validation and another thousand for the final test set. We use the splits provided by (Karpathy and Fei-Fei, 2015). The image features come from the last fully connecting layer of VGG-16 (Simonyan and Zisserman, 2014), which was pre-trained on Imagenet (Russakovsky et al., 2014). We generate the input signal as follows: We extract 12-dimensional melting frequency cepstral coefficients (MFCC) protocol plus the overall energy."}, {"heading": "4.1.2 Synthetically spoken COCO", "text": "We created synthetic language for the captions in the MS COCO dataset (Lin et al., 2014) using the Google Text-to-Speech API.2 The audio and associated MFCC functions are released as Chrupa\u0142a et al. (2017) 3. This TTS system we use generates high-quality, realistic-sounding language. However, it is much simpler than real human language because it uses a single voice and lacks speed variations or ambient noise. The data consists of over 300,000 images, each with five spoken captions. Each image is offered for validation and testing. We use the splits and image attributes provided by Vendrov et al. (2015).4 The image attributes are also derived from the VGG-16 network, but are average values of image vectors for ten image segments each. For the MS COCO captions, we extracted only simple MFC and aggregate features, not adding to the size of the data added."}, {"heading": "4.2 Image retrieval", "text": "We evaluate our model on the basis of a spoken utterance, so that high-ranking images contain scenes described by the utterance. (The performance of this task on validation data is also used to select the best variant of the model architecture and optimize the hyperparameters.) We compare the language models to models divided into words on written sentences, but the best settings for the four models were the following: Flickr8K text RHN 300-dimensional phrases, 1 hidden layer with 1024 dimensions, 1 micro-step, initial learning rate 0.00K Speech RHN constellation layer with length 6, size 64, Stride 2, 4 hidden layers with 1024 dimensions, 2 micro-steps, atten-1We noticed that for a number of utterances the audio signal was very long: on inspection it turned out that most of these errors involved switching the microphone to the part of the worker."}, {"heading": "4.3 Predicting utterance length", "text": "Our first auxiliary task is to predict the length of the utterance, using the characteristics explained at the beginning of Section 4. However, since the length of an utterance corresponds directly to the length of its articulation, we also use the number of time steps5 as a feature and expect it to be the upper limit of our task, especially for synthetic language. We use a backbone regression model to predict the length of the utterance using each set of characteristics. The model is trained on 80% of the sentences in the validation group and tested for the remaining 20%. For all characteristics, the regulatory penalty \u03b1 = 1.0 yielded the best result.Figure 4 shows the results for this task of human speech from Flickr8K and synthetic speech from COCO. Except for the average input vectors for Flickr8K, all characteristics can explain a high proportion of the variance in the predicted utterance length. The pattern observed for the two sets of data is Patch8K, which can represent a patch 8K due to the systemic conversion of the language into 8K."}, {"heading": "4.4 Predicting word presence", "text": "The results of the previous experiment indicate that our model obtains information about higher building blocks (words) in the continuous speech signal. Here, we investigate whether it can detect the presence or absence of individual words in an utterance. We formulate the recognition of a word in an utterance as a binary classification task, for which we use a multi-layer perceptron with a single Adam-optimized layer of size 1024. Input into the model is a concatenation of the characteristic vector representing an utterance and the word representing a target word. We re-use embedding of the utterance, average unit activations on each layer and average input vectors as characteristics, and represent each target word as a vector of the MFCC characteristics that are extracted from the synthetically generated audio signal for that term. For each utterance in the validation group, we randomly select a positive expression and a target unit (i.e., a negative one)."}, {"heading": "4.5 Sentence similarity", "text": "Next, we examine the extent to which the representations of the model correspond to those of humans. We use the Sentences Involving Compositional Knowledge (SICK) datasets (STS) (Agirre et al., 2012). SICK consists of image descriptions taken from Flickr8K and Video Captions from the SemEval 2012 STS MSRVideo Description Data Set (STS). Captions were randomly assigned and modified to obtain semantically similar and contrasting counterparts, and the resulting pairs were evaluated for semantic similarity. For all sentence pairs in SICK, we generate synthetic spoken sentences and feed them to the COCO Speech RHN, and calculate the cosmic similarity between the averaged MFCC input vectors, the averaged hidden layer activation vectors, and the sentence embedding."}, {"heading": "4.6 Homonym disambiguation", "text": "Next, we simulate the task of distinguishing between pairs of homonyms, i.e. words with the same acoustic form but different meanings. We group the words in the combination of training and validation data of the COCO dataset by their phonetic transcription. We then select pairs of words that have the same pronunciation but different spelling, e.g. suite / sweet. We set the following conditions: (a) both forms appear more than 20 times, (b) the two forms have different meanings (i.e. they are not simply variant spellings like theater / theater), (c) no form is a functional word, and (d) the more common form represents less than 95% of the occurrence, giving us 34 word pairs. For each pair, we create a binary classification task by taking all expressions where either a form appears, using average input vectors, expression embedding, and average unit activations as characteristics."}, {"heading": "5 Conclusion", "text": "We present a multi-layered, recurring network model of language acquisition from visually grounded speech signals. Through detailed analysis, we uncover how information in the input signal is transformed as it flows through the network: formal aspects of language, such as word identities that are not directly present in the input signal, are detected and encoded deep down in the hierarchy, while semantic information is most strongly expressed in the upper layers. In the future, we want to compare the representations learned through our model with the brain activity of people listening to speech to determine to what extent the patterns found correspond to localized processing in the human cortex, which will hopefully lead to a better understanding of language acquisition and speech processing through artificial and neural networks."}, {"heading": "Acknowledgements", "text": "We would like to thank David Harwath for making Flickr8k Audio Caption Corpus available to the public."}], "references": [{"title": "Fine-grained analysis of sentence embeddings using auxiliary prediction tasks", "author": ["Yossi Adi", "Einat Kermany", "Yonatan Belinkov", "Ofer Lavi", "Yoav Goldberg."], "venue": "arXiv preprint arXiv:1608.04207 .", "citeRegEx": "Adi et al\\.,? 2016", "shortCiteRegEx": "Adi et al\\.", "year": 2016}, {"title": "Semeval-2012 task 6: A pilot on semantic textual similarity", "author": ["Eneko Agirre", "Mona Diab", "Daniel Cer", "Aitor Gonzalez-Agirre."], "venue": "Proceedings of the First Joint Conference on Lexical and Computational Semantics. Association for Computational", "citeRegEx": "Agirre et al\\.,? 2012", "shortCiteRegEx": "Agirre et al\\.", "year": 2012}, {"title": "Automatic description generation from images: A survey of models, datasets, and evaluation", "author": ["Raffaella Bernardi", "Ruket Cakici", "Desmond Elliott", "Aykut Erdem", "Erkut Erdem", "Nazli Ikizler-Cinbis", "Frank Keller", "Adrian Muscat", "Barbara Plank"], "venue": null, "citeRegEx": "Bernardi et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Bernardi et al\\.", "year": 2016}, {"title": "Learning language through pictures", "author": ["Grzegorz Chrupa\u0142a", "Akos K\u00e1d\u00e1r", "Afra Alishahi."], "venue": "Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics.", "citeRegEx": "Chrupa\u0142a et al\\.,? 2015", "shortCiteRegEx": "Chrupa\u0142a et al\\.", "year": 2015}, {"title": "Synthetically spoken COCO", "author": ["Grzegorz Chrupa\u0142a", "Lieke Gelderloos", "Afra Alishahi."], "venue": "https://doi.org/10.5281/zenodo.400926.", "citeRegEx": "Chrupa\u0142a et al\\.,? 2017", "shortCiteRegEx": "Chrupa\u0142a et al\\.", "year": 2017}, {"title": "A probabilistic computational model of cross-situational word learning", "author": ["Afsaneh Fazly", "Afra Alishahi", "Suzanne Stevenson."], "venue": "Cognitive Science: A Multidisciplinary Journal 34(6):1017\u20131063.", "citeRegEx": "Fazly et al\\.,? 2010", "shortCiteRegEx": "Fazly et al\\.", "year": 2010}, {"title": "A Bayesian framework for crosssituational word-learning", "author": ["Michael C. Frank", "Noah D. Goodman", "Joshua B. Tenenbaum."], "venue": "Advances in Neural Information Processing Systems. volume 20.", "citeRegEx": "Frank et al\\.,? 2007", "shortCiteRegEx": "Frank et al\\.", "year": 2007}, {"title": "From phonemes to images: levels of representation in a recurrent neural model of visually-grounded language learning", "author": ["Lieke Gelderloos", "Grzegorz Chrupa\u0142a."], "venue": "Proceedings of COLING 2016, the 26th International Conference on Computational", "citeRegEx": "Gelderloos and Chrupa\u0142a.,? 2016", "shortCiteRegEx": "Gelderloos and Chrupa\u0142a.", "year": 2016}, {"title": "Deep multimodal semantic embeddings for speech and images", "author": ["David Harwath", "James Glass."], "venue": "IEEE Automatic Speech Recognition and Understanding Workshop.", "citeRegEx": "Harwath and Glass.,? 2015", "shortCiteRegEx": "Harwath and Glass.", "year": 2015}, {"title": "Learning word-like units from joint audio-visual analysis", "author": ["David Harwath", "James R Glass."], "venue": "arXiv preprint arXiv:1701.07481 .", "citeRegEx": "Harwath and Glass.,? 2017", "shortCiteRegEx": "Harwath and Glass.", "year": 2017}, {"title": "Unsupervised learning of spoken language with visual context", "author": ["David Harwath", "Antonio Torralba", "James Glass."], "venue": "Advances in Neural Information Processing Systems. pages 1858\u20131866.", "citeRegEx": "Harwath et al\\.,? 2016", "shortCiteRegEx": "Harwath et al\\.", "year": 2016}, {"title": "Deep residual learning for image recognition", "author": ["Kaiming He", "Xiangyu Zhang", "Shaoqing Ren", "Jian Sun."], "venue": "arXiv:1512.03385 .", "citeRegEx": "He et al\\.,? 2015", "shortCiteRegEx": "He et al\\.", "year": 2015}, {"title": "Framing image description as a ranking task: Data, models and evaluation metrics", "author": ["Micah Hodosh", "Peter Young", "Julia Hockenmaier."], "venue": "Journal of Artificial Intelligence Research 47:853\u2013899.", "citeRegEx": "Hodosh et al\\.,? 2013", "shortCiteRegEx": "Hodosh et al\\.", "year": 2013}, {"title": "Representation of linguistic form and function in recurrent neural networks", "author": ["\u00c1kos K\u00e1d\u00e1r", "Grzegorz Chrupa\u0142a", "Afra Alishahi."], "venue": "CoRR abs/1602.08952.", "citeRegEx": "K\u00e1d\u00e1r et al\\.,? 2016", "shortCiteRegEx": "K\u00e1d\u00e1r et al\\.", "year": 2016}, {"title": "Deep visualsemantic alignments for generating image descriptions", "author": ["Andrej Karpathy", "Li Fei-Fei."], "venue": "Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. pages 3128\u20133137.", "citeRegEx": "Karpathy and Fei.Fei.,? 2015", "shortCiteRegEx": "Karpathy and Fei.Fei.", "year": 2015}, {"title": "Adam: A method for stochastic optimization", "author": ["Diederik P. Kingma", "Jimmy Ba."], "venue": "CoRR abs/1412.6980.", "citeRegEx": "Kingma and Ba.,? 2014", "shortCiteRegEx": "Kingma and Ba.", "year": 2014}, {"title": "Multimodal semantic learning from child-directed input", "author": ["Angeliki Lazaridou", "Grzegorz Chrupa\u0142a", "Raquel Fern\u00e1ndez", "Marco Baroni."], "venue": "The 15th Annual Conference of the North American Chapter of the Association for Computational Lin-", "citeRegEx": "Lazaridou et al\\.,? 2016", "shortCiteRegEx": "Lazaridou et al\\.", "year": 2016}, {"title": "Understanding neural networks through representation erasure", "author": ["Jiwei Li", "Will Monroe", "Dan Jurafsky."], "venue": "arXiv preprint arXiv:1612.08220 .", "citeRegEx": "Li et al\\.,? 2016", "shortCiteRegEx": "Li et al\\.", "year": 2016}, {"title": "Microsoft coco: Common objects in context", "author": ["Tsung-Yi Lin", "Michael Maire", "Serge Belongie", "James Hays", "Pietro Perona", "Deva Ramanan", "Piotr Doll\u00e1r", "C Lawrence Zitnick."], "venue": "Computer Vision\u2013 ECCV 2014, Springer, pages 740\u2013755.", "citeRegEx": "Lin et al\\.,? 2014", "shortCiteRegEx": "Lin et al\\.", "year": 2014}, {"title": "A sick cure for the evaluation of compositional distributional semantic models", "author": ["Marco Marelli", "Stefano Menini", "Marco Baroni", "Luisa Bentivogli", "Raffaella Bernardi", "Roberto Zamparelli."], "venue": "LREC. pages 216\u2013223.", "citeRegEx": "Marelli et al\\.,? 2014", "shortCiteRegEx": "Marelli et al\\.", "year": 2014}, {"title": "Simplifying long short-term memory acoustic models for fast training and decoding", "author": ["Yajie Miao", "Jinyu Li", "Yongqiang Wang", "Shi-Xiong Zhang", "Yifan Gong."], "venue": "IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP).", "citeRegEx": "Miao et al\\.,? 2016", "shortCiteRegEx": "Miao et al\\.", "year": 2016}, {"title": "Pixel recurrent neural networks", "author": ["Aaron van den Oord", "Nal Kalchbrenner", "Koray Kavukcuoglu."], "venue": "arXiv preprint arXiv:1601.06759 .", "citeRegEx": "Oord et al\\.,? 2016", "shortCiteRegEx": "Oord et al\\.", "year": 2016}, {"title": "Learning words from sights and sounds: a computational model", "author": ["Deb K Roy", "Alex P Pentland."], "venue": "Cognitive Science 26(1):113 \u2013 146.", "citeRegEx": "Roy and Pentland.,? 2002", "shortCiteRegEx": "Roy and Pentland.", "year": 2002}, {"title": "ImageNet Large Scale Visual Recognition Challenge", "author": ["Olga Russakovsky", "Jia Deng", "Hao Su", "Jonathan Krause", "Sanjeev Satheesh", "Sean Ma", "Zhiheng Huang", "Andrej Karpathy", "Aditya Khosla", "Michael Bernstein", "Alexander C. Berg", "Li Fei-Fei"], "venue": null, "citeRegEx": "Russakovsky et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Russakovsky et al\\.", "year": 2014}, {"title": "Very deep convolutional networks for large-scale image recognition", "author": ["Karen Simonyan", "Andrew Zisserman."], "venue": "CoRR abs/1409.1556.", "citeRegEx": "Simonyan and Zisserman.,? 2014", "shortCiteRegEx": "Simonyan and Zisserman.", "year": 2014}, {"title": "A computational study of cross-situational techniques for learning word-tomeaning mappings", "author": ["Jeffrey M. Siskind."], "venue": "Cognition 61(1-2):39\u201391.", "citeRegEx": "Siskind.,? 1996", "shortCiteRegEx": "Siskind.", "year": 1996}, {"title": "Learning words from images and speech", "author": ["Gabriel Synnaeve", "Maarten Versteegh", "Emmanuel Dupoux."], "venue": "NIPS Workshop on Learning Semantics, Montreal, Canada.", "citeRegEx": "Synnaeve et al\\.,? 2014", "shortCiteRegEx": "Synnaeve et al\\.", "year": 2014}, {"title": "Memory visualization for gated recurrent neural networks in speech recognition", "author": ["Zhiyuan Tang", "Ying Shi", "Dong Wang", "Yang Feng", "Shiyue Zhang."], "venue": "arXiv preprint arXiv:1609.08789 .", "citeRegEx": "Tang et al\\.,? 2016", "shortCiteRegEx": "Tang et al\\.", "year": 2016}, {"title": "Order-embeddings of images and language", "author": ["Ivan Vendrov", "Ryan Kiros", "Sanja Fidler", "Raquel Urtasun."], "venue": "arXiv preprint arXiv:1511.06361 .", "citeRegEx": "Vendrov et al\\.,? 2015", "shortCiteRegEx": "Vendrov et al\\.", "year": 2015}, {"title": "A multimodal learning interface for grounding spoken language in sensory perceptions", "author": ["Chen Yu", "Dana H Ballard."], "venue": "ACM Transactions on Applied Perception (TAP) 1(1):57\u201380.", "citeRegEx": "Yu and Ballard.,? 2004", "shortCiteRegEx": "Yu and Ballard.", "year": 2004}, {"title": "Learning deep features for scene recognition using places database", "author": ["Bolei Zhou", "Agata Lapedriza", "Jianxiong Xiao", "Antonio Torralba", "Aude Oliva."], "venue": "Z. Ghahramani, M. Welling, C. Cortes, N. D. Lawrence, and K. Q. Weinberger, editors, Advances", "citeRegEx": "Zhou et al\\.,? 2014", "shortCiteRegEx": "Zhou et al\\.", "year": 2014}, {"title": "Recurrent highway networks", "author": ["Julian Georg Zilly", "Rupesh Kumar Srivastava", "Jan Koutn\u0131\u0301k", "J\u00fcrgen Schmidhuber"], "venue": null, "citeRegEx": "Zilly et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Zilly et al\\.", "year": 2016}], "referenceMentions": [{"referenceID": 18, "context": "Gelderloos and Chrupa\u0142a (2016) use the image captioning dataset MS COCO (Lin et al., 2014) to mimic the setting of grounded language learning: the sensory input consists of images of natural scenes, while the language input are phonetically transcribed descriptions of these scenes.", "startOffset": 72, "endOffset": 90}, {"referenceID": 7, "context": "Gelderloos and Chrupa\u0142a (2016) use the image captioning dataset MS COCO (Lin et al.", "startOffset": 0, "endOffset": 31}, {"referenceID": 7, "context": "Gelderloos and Chrupa\u0142a (2016) use the image captioning dataset MS COCO (Lin et al., 2014) to mimic the setting of grounded language learning: the sensory input consists of images of natural scenes, while the language input are phonetically transcribed descriptions of these scenes. The use of such moderately large and low-level data allows the authors to train a multi-layer recurrent neural network model, and to explore the nature and localization of the emerging hierarchy of linguistic representations learned in the process. Furthermore, in a series of recent studies Harwath and Glass (2015); Harwath et al.", "startOffset": 0, "endOffset": 600}, {"referenceID": 7, "context": "Gelderloos and Chrupa\u0142a (2016) use the image captioning dataset MS COCO (Lin et al., 2014) to mimic the setting of grounded language learning: the sensory input consists of images of natural scenes, while the language input are phonetically transcribed descriptions of these scenes. The use of such moderately large and low-level data allows the authors to train a multi-layer recurrent neural network model, and to explore the nature and localization of the emerging hierarchy of linguistic representations learned in the process. Furthermore, in a series of recent studies Harwath and Glass (2015); Harwath et al. (2016); Harwath and Glass (2017) use image captioning datasets to model learning to understand spoken language from visual context with convolutional neural network models.", "startOffset": 0, "endOffset": 623}, {"referenceID": 7, "context": "Gelderloos and Chrupa\u0142a (2016) use the image captioning dataset MS COCO (Lin et al., 2014) to mimic the setting of grounded language learning: the sensory input consists of images of natural scenes, while the language input are phonetically transcribed descriptions of these scenes. The use of such moderately large and low-level data allows the authors to train a multi-layer recurrent neural network model, and to explore the nature and localization of the emerging hierarchy of linguistic representations learned in the process. Furthermore, in a series of recent studies Harwath and Glass (2015); Harwath et al. (2016); Harwath and Glass (2017) use image captioning datasets to model learning to understand spoken language from visual context with convolutional neural network models.", "startOffset": 0, "endOffset": 649}, {"referenceID": 7, "context": "Gelderloos and Chrupa\u0142a (2016) use the image captioning dataset MS COCO (Lin et al., 2014) to mimic the setting of grounded language learning: the sensory input consists of images of natural scenes, while the language input are phonetically transcribed descriptions of these scenes. The use of such moderately large and low-level data allows the authors to train a multi-layer recurrent neural network model, and to explore the nature and localization of the emerging hierarchy of linguistic representations learned in the process. Furthermore, in a series of recent studies Harwath and Glass (2015); Harwath et al. (2016); Harwath and Glass (2017) use image captioning datasets to model learning to understand spoken language from visual context with convolutional neural network models. Finally, there is a small but growing body of work dedicated to elucidating the nature of representations learned by neural networks from language data (see Section 2.2 for a brief overview). In the current work we build on these three strands of research and contribute the following advances: \u2022 We use a multi-layer gated recurrent neural network to properly model the temporal nature of speech signal and substantially improve performance compared to the convolutional architecture from Harwath and Glass (2015); \u2022 We carry out an in-depth analysis of the rep-", "startOffset": 0, "endOffset": 1304}, {"referenceID": 6, "context": "Often language input is given in the form of word symbols, and the context consists of a set of symbols representing possible referents (e.g. Siskind, 1996; Frank et al., 2007; Fazly et al., 2010).", "startOffset": 136, "endOffset": 196}, {"referenceID": 5, "context": "Often language input is given in the form of word symbols, and the context consists of a set of symbols representing possible referents (e.g. Siskind, 1996; Frank et al., 2007; Fazly et al., 2010).", "startOffset": 136, "endOffset": 196}, {"referenceID": 29, "context": "In contrast, several studies presented models that learn from sensory rather than symbolic input, which is rich with regards to the signal itself, but very limited in scale and variation (e.g. Roy and Pentland, 2002; Yu and Ballard, 2004; Lazaridou et al., 2016).", "startOffset": 187, "endOffset": 262}, {"referenceID": 16, "context": "In contrast, several studies presented models that learn from sensory rather than symbolic input, which is rich with regards to the signal itself, but very limited in scale and variation (e.g. Roy and Pentland, 2002; Yu and Ballard, 2004; Lazaridou et al., 2016).", "startOffset": 187, "endOffset": 262}, {"referenceID": 18, "context": "The model is trained on image-caption pairs from MSCOCO (Lin et al., 2014), capturing both rich visual input as well as larger scale input, but the language input still consists of word symbols.", "startOffset": 56, "endOffset": 74}, {"referenceID": 2, "context": "The field is large and growing, with most work dedicated to the grounding of written text, particularly in image captioning tasks (see Bernardi et al. (2016) for an", "startOffset": 135, "endOffset": 158}, {"referenceID": 24, "context": "Synnaeve et al. (2014) present a method of learning to recognize spoken words in isolation from cooccurrence with image fragments.", "startOffset": 0, "endOffset": 23}, {"referenceID": 8, "context": "Harwath and Glass (2015) present a model that learns to map pre-segmented spoken words in sequence to aspects of the visual context, while in Harwath and Glass (2017) the model also learns to recognize words in the unsegmented signal.", "startOffset": 0, "endOffset": 25}, {"referenceID": 8, "context": "Harwath and Glass (2015) present a model that learns to map pre-segmented spoken words in sequence to aspects of the visual context, while in Harwath and Glass (2017) the model also learns to recognize words in the unsegmented signal.", "startOffset": 0, "endOffset": 167}, {"referenceID": 30, "context": "The architecture is trained on crowd-sourced spoken captions for images from the Places dataset (Zhou et al., 2014), and evaluated on image search and caption retrieval.", "startOffset": 96, "endOffset": 115}, {"referenceID": 31, "context": "We make different architectural choices, as our models are based on recurrent highway networks (Zilly et al., 2016).", "startOffset": 95, "endOffset": 115}, {"referenceID": 8, "context": "Most closely related to our work is that of Harwath et al. (2016), as it presents an architecture that learns to project images and unsegmented spoken captions to the same embedding space.", "startOffset": 44, "endOffset": 66}, {"referenceID": 8, "context": "Most closely related to our work is that of Harwath et al. (2016), as it presents an architecture that learns to project images and unsegmented spoken captions to the same embedding space. The sentence representation is obtained by feeding the spectrogram to a convolutional network. The architecture is trained on crowd-sourced spoken captions for images from the Places dataset (Zhou et al., 2014), and evaluated on image search and caption retrieval. Unfortunately this dataset is not currently available and we were thus unable to directly compare the performance of our model to Harwath et al. (2016). We do compare to Harwath and Glass (2015) which was tested on a public dataset.", "startOffset": 44, "endOffset": 606}, {"referenceID": 8, "context": "We do compare to Harwath and Glass (2015) which was tested on a public dataset.", "startOffset": 17, "endOffset": 42}, {"referenceID": 13, "context": "K\u00e1d\u00e1r et al. (2016) propose omission scores, a method to estimate the contribution of input tokens to the fi-", "startOffset": 0, "endOffset": 20}, {"referenceID": 17, "context": "In a similar approach, Li et al. (2016) study the contribution of individual input tokens as well as hidden units and word embedding dimensions by erasing them from the representation and analyzing how this affects the model.", "startOffset": 23, "endOffset": 40}, {"referenceID": 0, "context": "Adi et al. (2016) introduce prediction tasks to analyze information encoded in sentence embeddings about word order, sentence length, and the presence of individual words.", "startOffset": 0, "endOffset": 18}, {"referenceID": 28, "context": "The idea of projecting different modalities to a shared semantic space via a pair of encoders has been used in work on language and vision (among them Vendrov et al. (2015)).", "startOffset": 151, "endOffset": 173}, {"referenceID": 31, "context": "recurrent network, specifically a Recurrent Highway Network (Zilly et al., 2016).", "startOffset": 60, "endOffset": 80}, {"referenceID": 21, "context": "This formulation tends to ease optimization in multi-layer models (cf. He et al., 2015; Oord et al., 2016).", "startOffset": 66, "endOffset": 106}, {"referenceID": 12, "context": "The Flickr8k Audio Caption Corpus was constructed by having crowdsource workers read aloud the captions in the original Flickr8K corpus (Hodosh et al., 2013).", "startOffset": 136, "endOffset": 157}, {"referenceID": 14, "context": "We use the splits provided by (Karpathy and Fei-Fei, 2015).", "startOffset": 30, "endOffset": 58}, {"referenceID": 24, "context": "The image features come from the final fully connect layer of VGG-16 (Simonyan and Zisserman, 2014) pre-trained on Imagenet (Russakovsky et al.", "startOffset": 69, "endOffset": 99}, {"referenceID": 23, "context": "The image features come from the final fully connect layer of VGG-16 (Simonyan and Zisserman, 2014) pre-trained on Imagenet (Russakovsky et al., 2014).", "startOffset": 124, "endOffset": 150}, {"referenceID": 8, "context": "For details of the data collection procedure refer to Harwath and Glass (2015). The datasets consist of 8,000 images, each image with five descriptions.", "startOffset": 54, "endOffset": 79}, {"referenceID": 18, "context": "We generated synthetic speech for the captions in the MS COCO dataset (Lin et al., 2014) via the Google Text-to-Speech API.", "startOffset": 70, "endOffset": 88}, {"referenceID": 3, "context": "2 The audio and the corresponding MFCC features are released as Chrupa\u0142a et al. (2017)3.", "startOffset": 64, "endOffset": 87}, {"referenceID": 3, "context": "2 The audio and the corresponding MFCC features are released as Chrupa\u0142a et al. (2017)3. This TTS system we used produces high-quality realistic-sounding speech. It is nevertheless much simpler than real human speech as it uses a single voice, and lacks tempo variation or ambient noise. The data consists of over 300,000 images, each with five spoken captions. Five thousand images each are held out for validation and test. We use the splits and image features provided by Vendrov et al. (2015).4 The image features also come from the VGG-16 network, but are averages of feature vectors for ten crops of each image.", "startOffset": 64, "endOffset": 497}, {"referenceID": 15, "context": "0002 All models were optimized with Adam (Kingma and Ba, 2014) with early stopping: we kept the parameters for the epoch which showed the best recall@10 on validation data.", "startOffset": 41, "endOffset": 62}, {"referenceID": 8, "context": "The Speech RHN model scores substantially higher than model of Harwath and Glass (2015) on the same data.", "startOffset": 63, "endOffset": 88}, {"referenceID": 19, "context": "We employ the Sentences Involving Compositional Knowledge (SICK) dataset (Marelli et al., 2014).", "startOffset": 73, "endOffset": 95}, {"referenceID": 1, "context": "Flickr8K and video captions from the SemEval 2012 STS MSRVideo Description data set (STS) (Agirre et al., 2012).", "startOffset": 90, "endOffset": 111}], "year": 2017, "abstractText": "We present a visually grounded model of speech perception which projects spoken utterances and images to a joint semantic space. We use a multi-layer recurrent highway network to model the temporal nature of spoken speech, and show that it learns to extract both form and meaningbased linguistic knowledge from the input signal. We carry out an in-depth analysis of the representations used by different components of the trained model and show that encoding of semantic aspects tends to become richer as we go up the hierarchy of layers, whereas encoding of formrelated aspects of the language input tends to initially increase and then plateau or decrease.", "creator": "LaTeX with hyperref package"}}}