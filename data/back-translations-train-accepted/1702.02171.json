{"id": "1702.02171", "review": {"conference": "ACL", "VERSION": "v1", "DATE_OF_SUBMISSION": "7-Feb-2017", "title": "Question Answering through Transfer Learning from Large Fine-grained Supervision Data", "abstract": "We show that the task of question answering (QA) can significantly benefit from the transfer learning of models trained on a different large, fine-grained QA dataset. We achieve the state of the art in two well-studied QA datasets, WikiQA and SemEval-2016 (Task 3A), through a basic transfer learning technique from SQuAD. For WikiQA, our model outperforms the previous best model by more than 8%. We demonstrate that finer supervision provides better guidance for learning lexical and syntactic information than coarser supervision, through quantitative results and visual analysis. We also show that a similar transfer learning procedure achieves the state of the art on an entailment task.", "histories": [["v1", "Tue, 7 Feb 2017 19:22:06 GMT  (784kb,D)", "http://arxiv.org/abs/1702.02171v1", null], ["v2", "Wed, 22 Feb 2017 17:15:18 GMT  (236kb,D)", "http://arxiv.org/abs/1702.02171v2", null], ["v3", "Thu, 23 Feb 2017 19:38:38 GMT  (0kb,I)", "http://arxiv.org/abs/1702.02171v3", "This paper has been withdrawn by the authors because of insufficient experimental results"], ["v4", "Fri, 21 Apr 2017 14:47:24 GMT  (663kb,D)", "http://arxiv.org/abs/1702.02171v4", "Published as a conference paper at ACL 2017 (short paper)"]], "reviews": [], "SUBJECTS": "cs.CL", "authors": ["sewon min", "min joon seo", "hannaneh hajishirzi"], "accepted": true, "id": "1702.02171"}, "pdf": {"name": "1702.02171.pdf", "metadata": {"source": "CRF", "title": "Question Answering through Transfer Learning from Large Fine-grained Supervision Data", "authors": ["Sewon Min", "Minjoon Seo", "Hannaneh Hajishirzi"], "emails": ["shmsw25@snu.ac.kr", "minjoon@uw.edu", "hannaneh@uw.edu"], "sections": [{"heading": "1 Introduction", "text": "Answering questions (QA) has been a longstanding challenge in NLP, and the community has introduced several paradigms and data sets for the task in recent years. These paradigms differ from each other in the type of questions and answers and the size of training data, ranging from a few hundred to millions of examples.We are particularly interested in the contextual QA paradigm, where the answer to each question can be obtained by referring to the accompanying context (paragraph or list of sentences).From this point of view, the two most notable types of oversight are coarse sentence level and fine-grained chip level. In sentence level QA, the task is to select sentences that are most relevant to the question from a list of candidates (Yang et al., 2015).In span-level QA, the task is to locate the smallest span."}, {"heading": "2 Background and Data", "text": "This year, it has reached the point where it will be able to retaliate."}, {"heading": "3 Model", "text": "\"Among numerous proposed QA tasks (Xiong et al., 2016; Wang and Jiang, 2016b), we assume an open model, BiDAF2 (Seo et al., 2016). The inputs to the model are a question q, and a context paragraph x. BiDAF uses recurring neural networks to model sequential dependencies within each question and context, and use the attention mechanism to2https: / / allenai.github.io / bi-flowmodel to model the interaction between them. The last layer of BiDAF is the response module that spans the psuedo probability distributions of the start and end positions of the answer span, ystart, yend, yend, yend [0, 1] N, where N is the length of the contextual words. Then, the best answer is span argmax (i, j) y start i y y end j, where i & ltj.Here we briefly describe the response to the transfer level."}, {"heading": "4 Experiments", "text": "This year, it is only a matter of time before an agreement is reached."}, {"heading": "5 Conclusion", "text": "In this article, we present current results on WikiQA and SemEval-2016 (Task 3A), as well as a related task, SICK, which outperforms previous results by 8%, 1% and 2%, respectively. We show that answering questions at the sentence level can greatly benefit from the standardized transfer learning of a question-answer model that has been trained on a large, cross-disciplinary supervision, and we also show that such transfer learning can also apply to other NLP tasks such as text tasks."}, {"heading": "A Training details", "text": "For the pre-training BiDAF-T on SQuAD-T, we use the same hyperparameters for all modules except the response module, for which we use the hidden state variable of 200. Learning rate is maintained by AdaDelta (Zeiler, 2012) with the initial learning rate of 0.5 and the minibatch size of 50. We keep the moving averages of all weights of the model with the exponential decay rate of 0.999 during training and use them in testing. The loss function is the cross entropy between y-k and the uniform vector of correct classification. Convergence. For all settings, we train the models until the performance on the development set continues to decline by 5k steps, and test them on the model with the best performance on the development set. Table 4 shows the selected median step on each setting."}, {"heading": "B More Analysis", "text": "Error Analysis. Table 6 shows the comparison between the answers of SQuAD-T-pretrained model and SQuAD-pretrained model on the example of WikiQA and SemEval-2016 from Table 1. On WikiQA, SQuAD-T-pretrained model selects C2 rather than the groundtruth answer C1. On SemEval-2016, SQuAD-pretrained model ranksC3 (bad comment) higher than C2 (good comment).In addition, we randomly stamped 100 examples from WikiQA and SemEval-2016, and classified them into 6 categories (Table 5). In Table 7, we compare the performance on these WikiQA examples of SQuAD-T-pretrained model and SQuADpretrained model. It shows that span supervision clearly helps to answer questions in Categories 1 and 2, which are easier to answer, with the correct answer of most of the questions in Category 1. Similarly, we show the comparison of the performance on classified examples of the model without pretrained attention."}], "references": [{"title": "Semantic parsing on freebase from question-answer pairs", "author": ["Jonathan Berant", "Andrew Chou", "Roy Frostig", "Percy Liang."], "venue": "EMNLP.", "citeRegEx": "Berant et al\\.,? 2013", "shortCiteRegEx": "Berant et al\\.", "year": 2013}, {"title": "A large annotated corpus for learning natural language inference", "author": ["Samuel R Bowman", "Gabor Angeli", "Christopher Potts", "Christopher D Manning."], "venue": "EMNLP.", "citeRegEx": "Bowman et al\\.,? 2015", "shortCiteRegEx": "Bowman et al\\.", "year": 2015}, {"title": "Domain adaptation of rule-based annotators for named-entity recognition tasks", "author": ["Laura Chiticariu", "Rajasekar Krishnamurthy", "Yunyao Li", "Frederick Reiss", "Shivakumar Vaithyanathan."], "venue": "EMNLP.", "citeRegEx": "Chiticariu et al\\.,? 2010", "shortCiteRegEx": "Chiticariu et al\\.", "year": 2010}, {"title": "Imagenet: A large-scale hierarchical image database", "author": ["Jia Deng", "Wei Dong", "Richard Socher", "Li-Jia Li", "Kai Li", "Li Fei-Fei."], "venue": "CVPR.", "citeRegEx": "Deng et al\\.,? 2009", "shortCiteRegEx": "Deng et al\\.", "year": 2009}, {"title": "Kelp at semeval-2016 task 3: Learning semantic relations between questions and answers", "author": ["Simone Filice", "Danilo Croce", "Alessandro Moschitti", "Roberto Basili."], "venue": "SemEval 16:1116\u20131123.", "citeRegEx": "Filice et al\\.,? 2016", "shortCiteRegEx": "Filice et al\\.", "year": 2016}, {"title": "Teaching machines to read and comprehend", "author": ["Karl Moritz Hermann", "Tomas Kocisky", "Edward Grefenstette", "Lasse Espeholt", "Will Kay", "Mustafa Suleyman", "Phil Blunsom."], "venue": "NIPS.", "citeRegEx": "Hermann et al\\.,? 2015", "shortCiteRegEx": "Hermann et al\\.", "year": 2015}, {"title": "Comparing measures of sparsity", "author": ["Niall Hurley", "Scott Rickard."], "venue": "IEEE Transactions on Information Theory 55(10):4723\u20134741.", "citeRegEx": "Hurley and Rickard.,? 2009", "shortCiteRegEx": "Hurley and Rickard.", "year": 2009}, {"title": "Unal-nlp: Combining soft cardinality features for semantic textual similarity, relatedness and entailment", "author": ["Sergio Jimenez", "George Duenas", "Julia Baquero", "Alexander Gelbukh", "Av Juan Dios B\u00e1tiz", "Av Mendiz\u00e1bal."], "venue": "SemEval Workshop.", "citeRegEx": "Jimenez et al\\.,? 2014", "shortCiteRegEx": "Jimenez et al\\.", "year": 2014}, {"title": "Convkn at semeval-2016 task 3: Answer and question selection for question answering on arabic and english fora", "author": ["Shafiq Joty", "Alessandro Moschitti", "Fahad A Al Obaidli", "Salvatore Romeo", "Kateryna Tymoshenko", "Antonio Uva."], "venue": "SemEval pages 896\u2013903.", "citeRegEx": "Joty et al\\.,? 2016", "shortCiteRegEx": "Joty et al\\.", "year": 2016}, {"title": "Deep visualsemantic alignments for generating image descriptions", "author": ["Andrej Karpathy", "Li Fei-Fei."], "venue": "CVPR.", "citeRegEx": "Karpathy and Fei.Fei.,? 2015", "shortCiteRegEx": "Karpathy and Fei.Fei.", "year": 2015}, {"title": "Ask me anything: Dynamic memory networks for natural language processing", "author": ["Ankit Kumar", "Ozan Irsoy", "Jonathan Su", "James Bradbury", "Robert English", "Brian Pierce", "Peter Ondruska", "Ishaan Gulrajani", "Richard Socher."], "venue": "ICML.", "citeRegEx": "Kumar et al\\.,? 2016", "shortCiteRegEx": "Kumar et al\\.", "year": 2016}, {"title": "Illinois-lh: A denotational and distributional approach to semantics", "author": ["Alice Lai", "Julia Hockenmaier."], "venue": "SemEval .", "citeRegEx": "Lai and Hockenmaier.,? 2014", "shortCiteRegEx": "Lai and Hockenmaier.", "year": 2014}, {"title": "A sick cure for the evaluation of compositional distributional semantic models", "author": ["Marco Marelli", "Stefano Menini", "Marco Baroni", "Luisa Bentivogli", "Raffaella Bernardi", "Roberto Zamparelli."], "venue": "LREC.", "citeRegEx": "Marelli et al\\.,? 2014", "shortCiteRegEx": "Marelli et al\\.", "year": 2014}, {"title": "Automatic domain adaptation for parsing", "author": ["David McClosky", "Eugene Charniak", "Mark Johnson."], "venue": "NAACL-HLT .", "citeRegEx": "McClosky et al\\.,? 2010", "shortCiteRegEx": "McClosky et al\\.", "year": 2010}, {"title": "Semanticz at semeval-2016 task 3: Ranking relevant answers in community question answering using semantic similarity based on fine-tuned word embeddings", "author": ["Todor Mihaylov", "Preslav Nakov."], "venue": "SemEval pages 879\u2013886.", "citeRegEx": "Mihaylov and Nakov.,? 2016", "shortCiteRegEx": "Mihaylov and Nakov.", "year": 2016}, {"title": "Distributed representations of words and phrases and their compositionality", "author": ["Tomas Mikolov", "Ilya Sutskever", "Kai Chen", "Greg S Corrado", "Jeff Dean."], "venue": "NIPS.", "citeRegEx": "Mikolov et al\\.,? 2013", "shortCiteRegEx": "Mikolov et al\\.", "year": 2013}, {"title": "Key-value memory networks for directly reading documents", "author": ["Alexander Miller", "Adam Fisch", "Jesse Dodge", "AmirHossein Karimi", "Antoine Bordes", "Jason Weston."], "venue": "EMNLP.", "citeRegEx": "Miller et al\\.,? 2016", "shortCiteRegEx": "Miller et al\\.", "year": 2016}, {"title": "How transferable are neural networks in nlp applications", "author": ["Lili Mou", "Zhao Meng", "Rui Yan", "Ge Li", "Yan Xu", "Lu Zhang", "Zhi Jin"], "venue": null, "citeRegEx": "Mou et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Mou et al\\.", "year": 2016}, {"title": "Semeval-2016 task 3: Community question answering", "author": ["Preslav Nakov", "Llus Mrquez", "Alessandro Moschitti", "Walid Magdy Mubarak Hamdy Hamdy", "abed Alhakim Freihat", "Jim Glass", "Bilal Randeree"], "venue": null, "citeRegEx": "Nakov et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Nakov et al\\.", "year": 2016}, {"title": "Ms marco: A human generated machine reading comprehension dataset", "author": ["Tri Nguyen", "Mir Rosenberg", "Xia Song", "Jianfeng Gao", "Saurabh Tiwary", "Rangan Majumder", "Li Deng."], "venue": "NIPS Workshop.", "citeRegEx": "Nguyen et al\\.,? 2016", "shortCiteRegEx": "Nguyen et al\\.", "year": 2016}, {"title": "Squad: 100,000+ questions for machine comprehension of text", "author": ["Pranav Rajpurkar", "Jian Zhang", "Konstantin Lopyrev", "Percy Liang."], "venue": "EMNLP.", "citeRegEx": "Rajpurkar et al\\.,? 2016", "shortCiteRegEx": "Rajpurkar et al\\.", "year": 2016}, {"title": "Bidirectional attention flow for machine comprehension", "author": ["Minjoon Seo", "Aniruddha Kembhavi", "Ali Farhadi", "Hannaneh Hajishirzi."], "venue": "arXiv preprint arXiv:1611.01603 .", "citeRegEx": "Seo et al\\.,? 2016", "shortCiteRegEx": "Seo et al\\.", "year": 2016}, {"title": "Newsqa: A machine comprehension dataset", "author": ["Adam Trischler", "Tong Wang", "Xingdi Yuan", "Justin Harris", "Alessandro Sordoni", "Philip Bachman", "Kaheer Suleman."], "venue": "arXiv preprint arXiv:1611.09830 .", "citeRegEx": "Trischler et al\\.,? 2016", "shortCiteRegEx": "Trischler et al\\.", "year": 2016}, {"title": "Convolutional neural networks vs", "author": ["Kateryna Tymoshenko", "Daniele Bonadiman", "Alessandro Moschitti."], "venue": "convolution kernels: Feature engineering for answer sentence reranking. In NAACL-HLT .", "citeRegEx": "Tymoshenko et al\\.,? 2016", "shortCiteRegEx": "Tymoshenko et al\\.", "year": 2016}, {"title": "Building a question answering test collection", "author": ["Ellen M Voorhees", "Dawn M Tice."], "venue": "ACM SIGIR.", "citeRegEx": "Voorhees and Tice.,? 2000", "shortCiteRegEx": "Voorhees and Tice.", "year": 2000}, {"title": "A compareaggregate model for matching text sequences", "author": ["Shuohang Wang", "Jing Jiang."], "venue": "arXiv preprint arXiv:1611.01747 .", "citeRegEx": "Wang and Jiang.,? 2016a", "shortCiteRegEx": "Wang and Jiang.", "year": 2016}, {"title": "Machine comprehension using match-lstm and answer pointer", "author": ["Shuohang Wang", "Jing Jiang."], "venue": "arXiv preprint arXiv:1608.07905 .", "citeRegEx": "Wang and Jiang.,? 2016b", "shortCiteRegEx": "Wang and Jiang.", "year": 2016}, {"title": "Dynamic coattention networks for question answering", "author": ["Caiming Xiong", "Victor Zhong", "Richard Socher."], "venue": "arXiv preprint arXiv:1611.01604 .", "citeRegEx": "Xiong et al\\.,? 2016", "shortCiteRegEx": "Xiong et al\\.", "year": 2016}, {"title": "Wikiqa: A challenge dataset for open-domain question answering", "author": ["Yi Yang", "Wen-tau Yih", "Christopher Meek."], "venue": "EMNLP.", "citeRegEx": "Yang et al\\.,? 2015", "shortCiteRegEx": "Yang et al\\.", "year": 2015}, {"title": "Abcnn: Attention-based convolutional neural network for modeling sentence pairs", "author": ["Wenpeng Yin", "Hinrich Sch\u00fctze", "Bing Xiang", "Bowen Zhou."], "venue": "TACL .", "citeRegEx": "Yin et al\\.,? 2016", "shortCiteRegEx": "Yin et al\\.", "year": 2016}, {"title": "Adadelta: an adaptive learning rate method", "author": ["Matthew D Zeiler."], "venue": "arXiv preprint arXiv:1212.5701 .", "citeRegEx": "Zeiler.,? 2012", "shortCiteRegEx": "Zeiler.", "year": 2012}, {"title": "Visualizing and understanding convolutional networks", "author": ["Matthew D Zeiler", "Rob Fergus."], "venue": "ECCV .", "citeRegEx": "Zeiler and Fergus.,? 2014", "shortCiteRegEx": "Zeiler and Fergus.", "year": 2014}, {"title": "Ecnu: One stone two birds: Ensemble of heterogenous measures for semantic relatedness and textual entailment", "author": ["Jiang Zhao", "Tian Tian Zhu", "Man Lan."], "venue": "SemEval pages 271\u2013277.", "citeRegEx": "Zhao et al\\.,? 2014", "shortCiteRegEx": "Zhao et al\\.", "year": 2014}], "referenceMentions": [{"referenceID": 28, "context": "In sentence-level QA, the task is to pick sentences that are most relevant to the question among a list of candidates (Yang et al., 2015).", "startOffset": 118, "endOffset": 137}, {"referenceID": 20, "context": "In span-level QA, the task is to locate the smallest span in the given paragraph that answers the question (Rajpurkar et al., 2016).", "startOffset": 107, "endOffset": 131}, {"referenceID": 20, "context": "For the source dataset, we pretrain on SQuAD (Rajpurkar et al., 2016), a recentlyreleased, span-supervised QA dataset.", "startOffset": 45, "endOffset": 69}, {"referenceID": 21, "context": "For the source and target models, we adopt BiDAF (Seo et al., 2016), one of the top-performing models in the dataset\u2019s leaderboard.", "startOffset": 49, "endOffset": 67}, {"referenceID": 28, "context": "For the target datasets, we evaluate on two recent QA datasets, WikiQA (Yang et al., 2015) and SemEval 2016 (Task 3A) (Nakov et al.", "startOffset": 71, "endOffset": 90}, {"referenceID": 18, "context": ", 2015) and SemEval 2016 (Task 3A) (Nakov et al., 2016), which possess sufficiently different characteristics from that of SQuAD.", "startOffset": 35, "endOffset": 55}, {"referenceID": 12, "context": "In addition, we report state-of-the-art results on recognizing textual entailment (RTE) in SICK (Marelli et al., 2014) with a similar transfer learning procedure.", "startOffset": 96, "endOffset": 118}, {"referenceID": 3, "context": "In computer vision, deep convolutional neural networks trained on a large image classification dataset such as ImageNet (Deng et al., 2009) have proved to be useful for initializing models on other vision tasks, such as object detection (Zeiler and Fergus, 2014).", "startOffset": 120, "endOffset": 139}, {"referenceID": 31, "context": ", 2009) have proved to be useful for initializing models on other vision tasks, such as object detection (Zeiler and Fergus, 2014).", "startOffset": 105, "endOffset": 130}, {"referenceID": 17, "context": "1 The borderline between transfer learning and domain adaptation is often ambiguous (Mou et al., 2016).", "startOffset": 84, "endOffset": 102}, {"referenceID": 13, "context": "parsing (McClosky et al., 2010) and named entity recognition (Chiticariu et al.", "startOffset": 8, "endOffset": 31}, {"referenceID": 2, "context": ", 2010) and named entity recognition (Chiticariu et al., 2010), among others.", "startOffset": 37, "endOffset": 62}, {"referenceID": 15, "context": "With the popularity of distributed representation, pre-trained word embedding models such as word2vec (Mikolov et al., 2013) are also widely used for natural language tasks (Karpathy and FeiFei, 2015; Kumar et al.", "startOffset": 102, "endOffset": 124}, {"referenceID": 10, "context": ", 2013) are also widely used for natural language tasks (Karpathy and FeiFei, 2015; Kumar et al., 2016).", "startOffset": 56, "endOffset": 103}, {"referenceID": 0, "context": "This context can range from structured and confined knowledge bases (Berant et al., 2013) to unstructured and unbounded natural language form (e.", "startOffset": 68, "endOffset": 89}, {"referenceID": 24, "context": ", documents on the web (Voorhees and Tice, 2000)) and unstructured, but restricted in size (e.", "startOffset": 23, "endOffset": 48}, {"referenceID": 5, "context": ", a paragraph or multiple sentences (Hermann et al., 2015)).", "startOffset": 36, "endOffset": 58}, {"referenceID": 20, "context": "The recent advances in neural question answering lead to numerous datasets and successful models in these paradigms (Rajpurkar et al., 2016; Yang et al., 2015; Nguyen et al., 2016; Trischler et al., 2016).", "startOffset": 116, "endOffset": 204}, {"referenceID": 28, "context": "The recent advances in neural question answering lead to numerous datasets and successful models in these paradigms (Rajpurkar et al., 2016; Yang et al., 2015; Nguyen et al., 2016; Trischler et al., 2016).", "startOffset": 116, "endOffset": 204}, {"referenceID": 19, "context": "The recent advances in neural question answering lead to numerous datasets and successful models in these paradigms (Rajpurkar et al., 2016; Yang et al., 2015; Nguyen et al., 2016; Trischler et al., 2016).", "startOffset": 116, "endOffset": 204}, {"referenceID": 22, "context": "The recent advances in neural question answering lead to numerous datasets and successful models in these paradigms (Rajpurkar et al., 2016; Yang et al., 2015; Nguyen et al., 2016; Trischler et al., 2016).", "startOffset": 116, "endOffset": 204}, {"referenceID": 20, "context": "SQuAD (Rajpurkar et al., 2016) is a recent spanbased QA dataset, containing 100k/10k train/dev examples.", "startOffset": 6, "endOffset": 30}, {"referenceID": 28, "context": "WikiQA (Yang et al., 2015) is a sentence-level QA dataset, containing 1.", "startOffset": 7, "endOffset": 26}, {"referenceID": 18, "context": "SemEval 2016 (Task 3A) (Nakov et al., 2016) is a sentence-level QA dataset, containing 1.", "startOffset": 23, "endOffset": 43}, {"referenceID": 12, "context": "SICK (Marelli et al., 2014) is a dataset for recognizing textual entailment (RTE), containing 4.", "startOffset": 5, "endOffset": 27}, {"referenceID": 27, "context": "Among numerous models proposed for spanlevel QA tasks (Xiong et al., 2016; Wang and Jiang, 2016b), we adopt an open-sourced model, BiDAF2 (Seo et al.", "startOffset": 54, "endOffset": 97}, {"referenceID": 26, "context": "Among numerous models proposed for spanlevel QA tasks (Xiong et al., 2016; Wang and Jiang, 2016b), we adopt an open-sourced model, BiDAF2 (Seo et al.", "startOffset": 54, "endOffset": 97}, {"referenceID": 21, "context": ", 2016; Wang and Jiang, 2016b), we adopt an open-sourced model, BiDAF2 (Seo et al., 2016).", "startOffset": 71, "endOffset": 89}, {"referenceID": 20, "context": "For WikiQA, they are from Wang and Jiang (2016a); Tymoshenko et al.", "startOffset": 26, "endOffset": 49}, {"referenceID": 19, "context": "For WikiQA, they are from Wang and Jiang (2016a); Tymoshenko et al. (2016); Miller et al.", "startOffset": 50, "endOffset": 75}, {"referenceID": 13, "context": "(2016); Miller et al. (2016), respectively.", "startOffset": 8, "endOffset": 29}, {"referenceID": 4, "context": "For SemEval2016, they are from Filice et al. (2016); Joty et al.", "startOffset": 31, "endOffset": 52}, {"referenceID": 4, "context": "For SemEval2016, they are from Filice et al. (2016); Joty et al. (2016); Mihaylov and Nakov (2016).", "startOffset": 31, "endOffset": 72}, {"referenceID": 4, "context": "For SemEval2016, they are from Filice et al. (2016); Joty et al. (2016); Mihaylov and Nakov (2016).", "startOffset": 31, "endOffset": 99}, {"referenceID": 26, "context": "22 Yin et al. (2016) 86.", "startOffset": 3, "endOffset": 21}, {"referenceID": 10, "context": "2 Lai and Hockenmaier (2014) 84.", "startOffset": 2, "endOffset": 29}, {"referenceID": 10, "context": "2 Lai and Hockenmaier (2014) 84.57 Zhao et al. (2014) 83.", "startOffset": 2, "endOffset": 54}, {"referenceID": 7, "context": "64 Jimenez et al. (2014) 83.", "startOffset": 3, "endOffset": 25}, {"referenceID": 7, "context": "64 Jimenez et al. (2014) 83.05 Mou et al. (2016) 70.", "startOffset": 3, "endOffset": 49}, {"referenceID": 7, "context": "64 Jimenez et al. (2014) 83.05 Mou et al. (2016) 70.9 Mou et al. (2016) (pretrained on SNLI) 77.", "startOffset": 3, "endOffset": 72}, {"referenceID": 12, "context": "Table 3 shows the transfer learning results of BiDAF-T on SICK dataset (Marelli et al., 2014), with various pretraining routines.", "startOffset": 71, "endOffset": 93}, {"referenceID": 1, "context": "Note that SNLI (Bowman et al., 2015) is a similar task to SICK and is significantly larger (150K/10K/10K train/dev/test examples).", "startOffset": 15, "endOffset": 36}, {"referenceID": 1, "context": "Note that SNLI (Bowman et al., 2015) is a similar task to SICK and is significantly larger (150K/10K/10K train/dev/test examples). Here we highlight three observations. First, BiDAFT pretrained on SQuAD outperforms that without any pretraining by 6% and that pretrained on SQuAD-T by 2%, which demonstrates that the transfer learning from large span-based QA gives a clear improvement. Second, pretraining on SQuAD+SNLI outperforms pretraining on SNLI only. Given that SNLI is larger than SQuAD, the difference in their performance is a strong indicator that we are benefiting from not only the scale of SQuAD, but also the fine-grained supervision that it provides. Lastly, we outperform the previous state of the art by 2% with the ensemble of SQuAD+SNLI pretraining routine. It is worth noting that Mou et al. (2016) also shows improvement on SICK by pretraining on SNLI.", "startOffset": 16, "endOffset": 820}], "year": 2017, "abstractText": "We show that the task of question answering (QA) can significantly benefit from the transfer learning of models trained on a different large, fine-grained QA dataset. We achieve the state of the art in two well-studied QA datasets, WikiQA and SemEval-2016 (Task 3A), through a basic transfer learning technique from SQuAD. For WikiQA, our model outperforms the previous best model by more than 8%. We demonstrate that finer supervision provides better guidance for learning lexical and syntactic information than coarser supervision, through quantitative results and visual analysis. We also show that a similar transfer learning procedure achieves the state of the art on an entailment task.", "creator": "LaTeX with hyperref package"}}}