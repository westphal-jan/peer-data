{"id": "1702.02206", "review": {"conference": "ACL", "VERSION": "v1", "DATE_OF_SUBMISSION": "7-Feb-2017", "title": "Semi-Supervised QA with Generative Domain-Adaptive Nets", "abstract": "We study the problem of semi-supervised question answering----utilizing unlabeled text to boost the performance of question answering models. We propose a novel training framework, the Generative Domain-Adaptive Nets. In this framework, we train a generative model to generate questions based on the unlabeled text, and combine model-generated questions with human-generated questions for training question answering models. We develop novel domain adaptation algorithms, based on reinforcement learning, to alleviate the discrepancy between the model-generated data distribution and the human-generated data distribution. Experiments show that our proposed framework obtains substantial improvement from unlabeled text.", "histories": [["v1", "Tue, 7 Feb 2017 21:23:01 GMT  (154kb,D)", "https://arxiv.org/abs/1702.02206v1", null], ["v2", "Sat, 22 Apr 2017 20:31:01 GMT  (185kb,D)", "http://arxiv.org/abs/1702.02206v2", "Accepted as a long paper at ACL2017"]], "reviews": [], "SUBJECTS": "cs.CL cs.LG", "authors": ["zhilin yang", "junjie hu", "ruslan salakhutdinov", "william w cohen"], "accepted": true, "id": "1702.02206"}, "pdf": {"name": "1702.02206.pdf", "metadata": {"source": "CRF", "title": "Semi-Supervised QA with Generative Domain-Adaptive Nets", "authors": ["Zhilin Yang", "Junjie Hu", "Ruslan Salakhutdinov", "William W. Cohen"], "emails": ["zhiliny@cs.cmu.edu", "junjieh@cs.cmu.edu", "rsalakhu@cs.cmu.edu", "wcohen@cs.cmu.edu"], "sections": [{"heading": "1 Introduction", "text": "In fact, most of them are able to survive themselves if they don't put themselves in a position to survive themselves. Most of them are able to survive themselves, and most of them are able to survive themselves. Most of them are able to survive themselves, and most of them are not able to survive themselves. Most of them are able to survive themselves, and most of them are not able to survive themselves."}, {"heading": "2 Semi-Supervised Question Answering", "text": "Let us first introduce the problem of semi-supervised answering of questions. Let L = {q (i), a (i), p (i)} Ni = 1 stand for a question that answers the row of N instances, where q (i), a (i) and p (i) are the question, the answer and the paragraph of the i instance. The aim of answering questions is to create the answer a (i) in the face of question q (i) together with the paragraph p (i). We will drop the high sentence \u00b7 (i) if the context is clear. In our formulation, following the setting in SQuAD (Rajpurkar et al., 2016), we focus specifically on the extractive answering of questions where a is always a sequential part of text in p. Formally, let us leave p = (p1, p2, pT) as the model of answering word tokens."}, {"heading": "2.1 A Simple Baseline", "text": "We now present a simple starting method for semi-supervised answer of questions: If we extract a paragraph p = (p1, p2, \u00b7 \u00b7, pT) and the answer a = (pj, pj + 1, \u00b7 \u00b7 \u00b7, pk \u2212 1, pk) from the paragraph (pj \u2212 W, pj \u2212 W + 1, \u00b7 \u00b7, pj \u2212 1, pk + 1, pk + 2, pk + W), we treat it as a question. Here, W is the window size and is set to 5 in our experiments, so that the length of the questions is similar to the length of the human-generated questions. Context-based question-answer pairs on U are combined with human-generated pairs on L to train the discriminatory model. Intuitively, this method extracts the contexts around the blocks of answers to serve as clusions for the question-answer model."}, {"heading": "3 Generative Domain-Adaptive Nets", "text": "Although the simple method described in Section 2.1 can lead to significant improvements, we aim to develop a learning-based model to go further. In this section we will describe the model architecture and training algorithms for the GDANs. We will use notation in answering questions under Section 2, but we should be able to extend the term GDAN to other applications as well. GDAN framework consists of two models, a discriminatory model and a generative model. We will first discuss the two models in detail in the context of answering questions, and then present an algorithm based on enhanced learning to combine the two models."}, {"heading": "3.1 Discriminative Model", "text": "The discriminatory model learns the conditional probability of an answer in view of the paragraph and question, i.e. P (a | p, q). In this thesis, we use a reader with gated attention (GA) (Dhingra et al., 2016) as the base model, but our framework does not assume any assumptions about the base models used. The discriminatory model is called D. The GA model consists of K layers with K as the hyperparameter. Let Hkp be the intermediate paragraph representation at level k and Hq the question representation. The paragraph representation Hkp is a T \u2212 d matrix, and the question representation Hq is a T \u2032 \u00d7 d matrix, where d is the dimensionality of the representations at level k, and Hq the question representation at level p. Given paragraph p, we apply a bi-directional GRecurrent Unit (GRU) network (Chet, 2014)."}, {"heading": "3.1.1 Domain Adaptation with Tags", "text": "To mitigate this problem, we propose to consider model-generated data distribution and human-generated data distribution as two distinct data ranges, explicitly including domain matching in the discriminatory model. Specifically, we use a domain tag as an additional input to the discriminatory model. We use the tag \"d true\" to represent the domain of human-generated data (i.e., the true data), and \"d gen\" for domain matching. Following a practice in domain matching (Johnson et al., 2016; Chu et al., 2017), we append the domain tag to the end of both questions and paragraphs. By introducing domain tags, we expect the d model to end the discriminatory factor of time-specific domain testing."}, {"heading": "3.2 Generative Model", "text": "The generative model learns the conditional probability of generating a question in the light of the paragraph and the answer, i.e. P (q | p, a). We implement the generative model as a sequence sequence model (Sutskever et al., 2014) with a copy mechanism (Gu et al., 2016; Gulcehre et al., 2016). The generative model consists of an encoder and a decoder. An encoder is a GRU that encodes the input paragraph into a sequence of hidden states. We inject the response information by adding an additional vocabulary to the word stem by adding a pseudo-coding to the word stem tokens; i.e., when a word stem appears in the answer, the feature is set to a word, otherwise all word types are set to zero. The decoder is another GRU with an attention mechanism over the encoded states."}, {"heading": "3.3 Training Algorithm", "text": "We first define the objective function of the GDANs and then present an algorithm to optimize the given objective function (G). Similar to the Generative Adversarial Nets (GANs) (Goodfellow et al., 2014) and the Adversarial Domain Adaptation (Ganin and Lempitsky, 2014), the discriminatory model and the generative model have different goals within our framework. However, instead of formulating the goal as an adversarial game between the two models (Goodfellow et al., 2014), the discriminatory model can be based on the data generated by the generative model, while the generative model aims to compare the model-generated data distribution with the signals of the discriminatory model. We enter a labeled datasetL = {p (i), an (i)} Ni = 1, which defines the objective function of a discriminatory model."}, {"heading": "4 Experiments", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "4.1 Answer Extraction", "text": "As discussed in Section 2, our model assumes that answers are available for unlabeled data. In this section, we will introduce how we use linguistic tags and rules to extract answers from unlabeled text. To extract answers from massive unlabeled Wikipedia articles, we first try out 205,511 Wikipedia articles that are not used in training, development, and test records in the SQuAD dataset. We extract the paragraphs from each article and limit the length of each paragraph at word level to less than 850. In total, we obtain 950,612 paragraphs from unlabeled articles. Answers in the SQuAD dataset can be categorized into ten types, i.e. \"date,\" \"other numeric,\" \"person,\" \"other entity,\" \"common noun phrase,\" \"adjective phrase,\" \"verb phrase,\" \"and\" other terms, \"we\" Rajkar, \"\" each year. \""}, {"heading": "4.2 Settings and Comparison Methods", "text": "The original SQuAD dataset consists of 87,636 training instances and 10,600 development instances. Since the test set is not published, we divide 10% of the training set as a test set, and the remaining 90% as an actual training set. Instances are divided by article; i.e., paragraphs in an article always appear in only one sentence. We compare the following methods. SL is the supervised learning scenario, in which we train Model D exclusively on the specified data. Context is the simple context-based method described in Section 2.1. Context + Domain is the \"context\" method with domain markers as described in Section 3.1.1. Gene is to develop a generative learning scenario and use the generated questions as additional training data. GAN + Gain receiver method is not combined with the Gan.e. dual method."}, {"heading": "4.3 Results and Analysis", "text": "In fact, it is a purely reactionary project, which is a reactionary project, which is a reactionary project, which is a reactionary project, which is a reactionary project, which is a reactionary project, and which is a reactionary project."}, {"heading": "5 Related Work", "text": "A number of novel models have recently been proposed for semi-monitored learning based on representation techniques, such as generative learning (Kingma et al., 2014), conductor networks (Rasmus et al., 2015), and chart embedding (Yang et al., 2016a). However, most of the semi-monitored learning methods are based on combinations of monitored loss (y | x) and unmonitored loss (x). In the context of reading comprehension, the likelihood of a paragraph is not necessarily improved. Furthermore, traditional semi-monitored learning (Zhu and Ghahramani, 2002) is not simply extended to modelling the blank response. Domain adaptation. Domain adaptation has been successfully applied to various tasks."}, {"heading": "6 Conclusions", "text": "We propose a novel neural framework called Generative Domain-Adaptive Nets, which combines domain adaptation techniques with generative models for semi-monitored learning. Empirically, we show that our approach leads to significant improvements over monitored learning models and outperforms several strong baselines, including GANs and dual learning. In the future, we plan to apply our approach to more question-answering datasets in different areas. It will also be interesting to generalize GDANs to other application areas. Well-known examples: This work was funded by the Office of Naval Research N000141512791 andN000141310721 and NVIDIA."}], "references": [{"title": "Unsupervised domain adaptation by domain invariant projection", "author": ["Mahsa Baktashmotlagh", "Mehrtash T Harandi", "Brian C Lovell", "Mathieu Salzmann."], "venue": "ICCV . pages 769\u2013776.", "citeRegEx": "Baktashmotlagh et al\\.,? 2013", "shortCiteRegEx": "Baktashmotlagh et al\\.", "year": 2013}, {"title": "Semantic parsing on freebase from question-answer pairs", "author": ["Jonathan Berant", "Andrew Chou", "Roy Frostig", "Percy Liang."], "venue": "EMNLP.", "citeRegEx": "Berant et al\\.,? 2013", "shortCiteRegEx": "Berant et al\\.", "year": 2013}, {"title": "A thorough examination of the cnn/daily mail reading comprehension task", "author": ["Danqi Chen", "Jason Bolton", "Christopher D Manning."], "venue": "arXiv preprint arXiv:1606.02858 .", "citeRegEx": "Chen et al\\.,? 2016", "shortCiteRegEx": "Chen et al\\.", "year": 2016}, {"title": "An empirical comparison of simple domain adaptation methods for neural machine translation", "author": ["Chenhui Chu", "Raj Dabre", "Sadao Kurohashi."], "venue": "arXiv preprint arXiv:1701.03214 .", "citeRegEx": "Chu et al\\.,? 2017", "shortCiteRegEx": "Chu et al\\.", "year": 2017}, {"title": "Empirical evaluation of gated recurrent neural networks on sequence modeling", "author": ["Junyoung Chung", "Caglar Gulcehre", "KyungHyun Cho", "Yoshua Bengio."], "venue": "arXiv preprint arXiv:1412.3555 .", "citeRegEx": "Chung et al\\.,? 2014", "shortCiteRegEx": "Chung et al\\.", "year": 2014}, {"title": "Attention-overattention neural networks for reading comprehension", "author": ["Yiming Cui", "Zhipeng Chen", "Si Wei", "Shijin Wang", "Ting Liu", "Guoping Hu."], "venue": "arXiv preprint arXiv:1607.04423 .", "citeRegEx": "Cui et al\\.,? 2016", "shortCiteRegEx": "Cui et al\\.", "year": 2016}, {"title": "Gated-attention readers for text comprehension", "author": ["Bhuwan Dhingra", "Hanxiao Liu", "Zhilin Yang", "William W Cohen", "Ruslan Salakhutdinov."], "venue": "arXiv preprint arXiv:1606.01549 .", "citeRegEx": "Dhingra et al\\.,? 2016", "shortCiteRegEx": "Dhingra et al\\.", "year": 2016}, {"title": "Incorporating non-local information into information extraction systems by gibbs sampling", "author": ["Jenny Rose Finkel", "Trond Grenager", "Christopher Manning."], "venue": "ACL. Association for Computational Linguistics, pages 363\u2013370.", "citeRegEx": "Finkel et al\\.,? 2005", "shortCiteRegEx": "Finkel et al\\.", "year": 2005}, {"title": "Unsupervised domain adaptation by backpropagation", "author": ["Yaroslav Ganin", "Victor Lempitsky."], "venue": "arXiv preprint arXiv:1409.7495 .", "citeRegEx": "Ganin and Lempitsky.,? 2014", "shortCiteRegEx": "Ganin and Lempitsky.", "year": 2014}, {"title": "Domain adaptation for large-scale sentiment classification: A deep learning approach", "author": ["Xavier Glorot", "Antoine Bordes", "Yoshua Bengio."], "venue": "ICML. pages 513\u2013520.", "citeRegEx": "Glorot et al\\.,? 2011", "shortCiteRegEx": "Glorot et al\\.", "year": 2011}, {"title": "Geodesic flow kernel for unsupervised domain adaptation", "author": ["Boqing Gong", "Yuan Shi", "Fei Sha", "Kristen Grauman."], "venue": "CVPR. IEEE, pages 2066\u2013 2073.", "citeRegEx": "Gong et al\\.,? 2012", "shortCiteRegEx": "Gong et al\\.", "year": 2012}, {"title": "Generative adversarial nets", "author": ["Ian Goodfellow", "Jean Pouget-Abadie", "Mehdi Mirza", "Bing Xu", "David Warde-Farley", "Sherjil Ozair", "Aaron Courville", "Yoshua Bengio."], "venue": "NIPS. pages 2672\u20132680.", "citeRegEx": "Goodfellow et al\\.,? 2014", "shortCiteRegEx": "Goodfellow et al\\.", "year": 2014}, {"title": "Domain adaptation for object recognition: An unsupervised approach", "author": ["Raghuraman Gopalan", "Ruonan Li", "Rama Chellappa."], "venue": "ICCV . IEEE, pages 999\u20131006.", "citeRegEx": "Gopalan et al\\.,? 2011", "shortCiteRegEx": "Gopalan et al\\.", "year": 2011}, {"title": "Incorporating copying mechanism in sequence-to-sequence learning", "author": ["Jiatao Gu", "Zhengdong Lu", "Hang Li", "Victor OK Li."], "venue": "arXiv preprint arXiv:1603.06393 .", "citeRegEx": "Gu et al\\.,? 2016", "shortCiteRegEx": "Gu et al\\.", "year": 2016}, {"title": "Pointing the unknown words", "author": ["Caglar Gulcehre", "Sungjin Ahn", "Ramesh Nallapati", "Bowen Zhou", "Yoshua Bengio."], "venue": "arXiv preprint arXiv:1603.08148 .", "citeRegEx": "Gulcehre et al\\.,? 2016", "shortCiteRegEx": "Gulcehre et al\\.", "year": 2016}, {"title": "On using monolingual corpora in neural machine translation", "author": ["Caglar Gulcehre", "Orhan Firat", "Kelvin Xu", "Kyunghyun Cho", "Loic Barrault", "Huei-Chi Lin", "Fethi Bougares", "Holger Schwenk", "Yoshua Bengio."], "venue": "arXiv preprint arXiv:1503.03535 .", "citeRegEx": "Gulcehre et al\\.,? 2015", "shortCiteRegEx": "Gulcehre et al\\.", "year": 2015}, {"title": "Google\u2019s multilingual neural machine translation system: Enabling zero-shot translation", "author": ["Melvin Johnson", "Mike Schuster", "Quoc V Le", "Maxim Krikun", "Yonghui Wu", "Zhifeng Chen", "Nikhil Thorat", "Fernanda Vi\u00e9gas", "Martin Wattenberg", "Greg Corrado"], "venue": null, "citeRegEx": "Johnson et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Johnson et al\\.", "year": 2016}, {"title": "Text understanding with the attention sum reader network", "author": ["Rudolf Kadlec", "Martin Schmid", "Ondrej Bajgar", "Jan Kleindienst."], "venue": "arXiv preprint arXiv:1603.01547 .", "citeRegEx": "Kadlec et al\\.,? 2016", "shortCiteRegEx": "Kadlec et al\\.", "year": 2016}, {"title": "Semi-supervised learning with deep generative models", "author": ["Diederik P Kingma", "Shakir Mohamed", "Danilo Jimenez Rezende", "Max Welling."], "venue": "NIPS. pages 3581\u20133589.", "citeRegEx": "Kingma et al\\.,? 2014", "shortCiteRegEx": "Kingma et al\\.", "year": 2014}, {"title": "Learning transferable features with deep adaptation networks", "author": ["Mingsheng Long", "Yue Cao", "Jianmin Wang", "Michael I Jordan."], "venue": "ICML. pages 97\u2013105.", "citeRegEx": "Long et al\\.,? 2015", "shortCiteRegEx": "Long et al\\.", "year": 2015}, {"title": "Ms marco: A human generated machine reading comprehension dataset", "author": ["Tri Nguyen", "Mir Rosenberg", "Xia Song", "Jianfeng Gao", "Saurabh Tiwary", "Rangan Majumder", "Li Deng."], "venue": "arXiv preprint arXiv:1611.09268 .", "citeRegEx": "Nguyen et al\\.,? 2016", "shortCiteRegEx": "Nguyen et al\\.", "year": 2016}, {"title": "Domain adaptation via transfer component analysis", "author": ["Sinno Jialin Pan", "Ivor W Tsang", "James T Kwok", "Qiang Yang."], "venue": "IEEE Transactions on Neural Networks 22(2):199\u2013210.", "citeRegEx": "Pan et al\\.,? 2011", "shortCiteRegEx": "Pan et al\\.", "year": 2011}, {"title": "Squad: 100,000+ questions for machine comprehension of text", "author": ["Pranav Rajpurkar", "Jian Zhang", "Konstantin Lopyrev", "Percy Liang."], "venue": "EMNLP.", "citeRegEx": "Rajpurkar et al\\.,? 2016", "shortCiteRegEx": "Rajpurkar et al\\.", "year": 2016}, {"title": "Semisupervised learning with ladder networks", "author": ["Antti Rasmus", "Mathias Berglund", "Mikko Honkala", "Harri Valpola", "Tapani Raiko."], "venue": "NIPS. pages 3546\u20133554.", "citeRegEx": "Rasmus et al\\.,? 2015", "shortCiteRegEx": "Rasmus et al\\.", "year": 2015}, {"title": "Mctest: A challenge dataset for the open-domain machine comprehension of text", "author": ["Matthew Richardson", "Christopher JC Burges", "Erin Renshaw."], "venue": "EMNLP. volume 3.", "citeRegEx": "Richardson et al\\.,? 2013", "shortCiteRegEx": "Richardson et al\\.", "year": 2013}, {"title": "Bidirectional attention flow for machine comprehension", "author": ["Minjoon Seo", "Aniruddha Kembhavi", "Ali Farhadi", "Hannaneh Hajishirzi."], "venue": "arXiv preprint arXiv:1611.01603 .", "citeRegEx": "Seo et al\\.,? 2016", "shortCiteRegEx": "Seo et al\\.", "year": 2016}, {"title": "Iterative alternating neural attention for machine reading", "author": ["Alessandro Sordoni", "Philip Bachman", "Adam Trischler", "Yoshua Bengio."], "venue": "arXiv preprint arXiv:1606.02245 .", "citeRegEx": "Sordoni et al\\.,? 2016", "shortCiteRegEx": "Sordoni et al\\.", "year": 2016}, {"title": "Sequence to sequence learning with neural networks", "author": ["Ilya Sutskever", "Oriol Vinyals", "Quoc V Le."], "venue": "NIPS. pages 3104\u20133112.", "citeRegEx": "Sutskever et al\\.,? 2014", "shortCiteRegEx": "Sutskever et al\\.", "year": 2014}, {"title": "Feature-rich part-ofspeech tagging with a cyclic dependency network", "author": ["Kristina Toutanova", "Dan Klein", "Christopher D Manning", "Yoram Singer."], "venue": "NAACL. Association for Computational Linguistics, pages 173\u2013180.", "citeRegEx": "Toutanova et al\\.,? 2003", "shortCiteRegEx": "Toutanova et al\\.", "year": 2003}, {"title": "Newsqa: A machine comprehension dataset", "author": ["Adam Trischler", "Tong Wang", "Xingdi Yuan", "Justin Harris", "Alessandro Sordoni", "Philip Bachman", "Kaheer Suleman."], "venue": "arXiv preprint arXiv:1611.09830 .", "citeRegEx": "Trischler et al\\.,? 2016a", "shortCiteRegEx": "Trischler et al\\.", "year": 2016}, {"title": "Natural language comprehension with the epireader", "author": ["Adam Trischler", "Zheng Ye", "Xingdi Yuan", "Kaheer Suleman."], "venue": "arXiv preprint arXiv:1606.02270 .", "citeRegEx": "Trischler et al\\.,? 2016b", "shortCiteRegEx": "Trischler et al\\.", "year": 2016}, {"title": "Modeling coverage for neural machine translation", "author": ["Zhaopeng Tu", "Zhengdong Lu", "Yang Liu", "Xiaohua Liu", "Hang Li."], "venue": "ACL.", "citeRegEx": "Tu et al\\.,? 2016", "shortCiteRegEx": "Tu et al\\.", "year": 2016}, {"title": "Stacked denoising autoencoders: Learning useful representations in a deep network with a local denoising criterion", "author": ["Pascal Vincent", "Hugo Larochelle", "Isabelle Lajoie", "Yoshua Bengio", "Pierre-Antoine Manzagol."], "venue": "JMLR 11(Dec):3371\u20133408.", "citeRegEx": "Vincent et al\\.,? 2010", "shortCiteRegEx": "Vincent et al\\.", "year": 2010}, {"title": "Building a question answering test collection", "author": ["Ellen M Voorhees", "Dawn M Tice."], "venue": "SIGIR. ACM, pages 200\u2013207.", "citeRegEx": "Voorhees and Tice.,? 2000", "shortCiteRegEx": "Voorhees and Tice.", "year": 2000}, {"title": "Machine comprehension using match-lstm and answer pointer", "author": ["Shuohang Wang", "Jing Jiang."], "venue": "arXiv preprint arXiv:1608.07905 .", "citeRegEx": "Wang and Jiang.,? 2016", "shortCiteRegEx": "Wang and Jiang.", "year": 2016}, {"title": "Multi-perspective context matching for machine comprehension", "author": ["Zhiguo Wang", "Haitao Mi", "Wael Hamza", "Radu Florian."], "venue": "arXiv preprint arXiv:1612.04211 .", "citeRegEx": "Wang et al\\.,? 2016", "shortCiteRegEx": "Wang et al\\.", "year": 2016}, {"title": "Simple statistical gradientfollowing algorithms for connectionist reinforcement learning", "author": ["Ronald J Williams."], "venue": "Machine learning 8(3-4):229\u2013256.", "citeRegEx": "Williams.,? 1992", "shortCiteRegEx": "Williams.", "year": 1992}, {"title": "Dual learning for machine translation", "author": ["Yingce Xia", "Di He", "Tao Qin", "Liwei Wang", "Nenghai Yu", "Tie-Yan Liu", "Wei-Ying Ma."], "venue": "arXiv preprint arXiv:1611.00179 .", "citeRegEx": "Xia et al\\.,? 2016", "shortCiteRegEx": "Xia et al\\.", "year": 2016}, {"title": "Dynamic coattention networks for question answering", "author": ["Caiming Xiong", "Victor Zhong", "Richard Socher."], "venue": "arXiv preprint arXiv:1611.01604 .", "citeRegEx": "Xiong et al\\.,? 2016", "shortCiteRegEx": "Xiong et al\\.", "year": 2016}, {"title": "Wikiqa: A challenge dataset for open-domain question answering", "author": ["Yi Yang", "Wen-tau Yih", "Christopher Meek."], "venue": "EMNLP. Citeseer, pages 2013\u2013 2018.", "citeRegEx": "Yang et al\\.,? 2015", "shortCiteRegEx": "Yang et al\\.", "year": 2015}, {"title": "Revisiting semi-supervised learning with graph embeddings", "author": ["Zhilin Yang", "William Cohen", "Ruslan Salakhutdinov."], "venue": "ICML.", "citeRegEx": "Yang et al\\.,? 2016a", "shortCiteRegEx": "Yang et al\\.", "year": 2016}, {"title": "Words or characters? fine-grained gating for reading comprehension", "author": ["Zhilin Yang", "Bhuwan Dhingra", "Ye Yuan", "Junjie Hu", "William W Cohen", "Ruslan Salakhutdinov."], "venue": "ICLR.", "citeRegEx": "Yang et al\\.,? 2017", "shortCiteRegEx": "Yang et al\\.", "year": 2017}, {"title": "Review networks for caption generation", "author": ["Zhilin Yang", "Ye Yuan", "Yuexin Wu", "William W Cohen", "Ruslan R Salakhutdinov."], "venue": "NIPS. pages 2361\u2013 2369.", "citeRegEx": "Yang et al\\.,? 2016b", "shortCiteRegEx": "Yang et al\\.", "year": 2016}, {"title": "Semi-supervised learning literature survey", "author": ["Xiaojin Zhu"], "venue": null, "citeRegEx": "Zhu.,? \\Q2005\\E", "shortCiteRegEx": "Zhu.", "year": 2005}, {"title": "Learning from labeled and unlabeled data with label propagation", "author": ["Xiaojin Zhu", "Zoubin Ghahramani"], "venue": null, "citeRegEx": "Zhu and Ghahramani.,? \\Q2002\\E", "shortCiteRegEx": "Zhu and Ghahramani.", "year": 2002}], "referenceMentions": [{"referenceID": 38, "context": "Recently, various neural network models were proposed and successfully applied to the tasks of questions answering (QA) and/or reading comprehension (Xiong et al., 2016; Dhingra et al., 2016; Yang et al., 2017).", "startOffset": 149, "endOffset": 210}, {"referenceID": 6, "context": "Recently, various neural network models were proposed and successfully applied to the tasks of questions answering (QA) and/or reading comprehension (Xiong et al., 2016; Dhingra et al., 2016; Yang et al., 2017).", "startOffset": 149, "endOffset": 210}, {"referenceID": 41, "context": "Recently, various neural network models were proposed and successfully applied to the tasks of questions answering (QA) and/or reading comprehension (Xiong et al., 2016; Dhingra et al., 2016; Yang et al., 2017).", "startOffset": 149, "endOffset": 210}, {"referenceID": 1, "context": "Historically, many of the question answering datasets have only thousands of question answering pairs, such as WebQuestions (Berant et al., 2013), MCTest (Richardson et al.", "startOffset": 124, "endOffset": 145}, {"referenceID": 24, "context": ", 2013), MCTest (Richardson et al., 2013), WikiQA (Yang et al.", "startOffset": 16, "endOffset": 41}, {"referenceID": 39, "context": ", 2013), WikiQA (Yang et al., 2015), and TREC-QA (Voorhees and Tice, 2000).", "startOffset": 16, "endOffset": 35}, {"referenceID": 33, "context": ", 2015), and TREC-QA (Voorhees and Tice, 2000).", "startOffset": 21, "endOffset": 46}, {"referenceID": 22, "context": "Although larger question answering datasets with hundreds of thousands of question-answer pairs have been collected, including SQuAD (Rajpurkar et al., 2016), MSMARCO (Nguyen et al.", "startOffset": 133, "endOffset": 157}, {"referenceID": 20, "context": ", 2016), MSMARCO (Nguyen et al., 2016), and NewsQA (Trischler et al.", "startOffset": 17, "endOffset": 38}, {"referenceID": 29, "context": ", 2016), and NewsQA (Trischler et al., 2016a), the data collection process is expensive and time-consuming in practice.", "startOffset": 20, "endOffset": 45}, {"referenceID": 44, "context": "able? The problem is challenging because conventional manifold-based semi-supervised learning algorithms (Zhu and Ghahramani, 2002; Yang et al., 2016a) cannot be straightforwardly applied.", "startOffset": 105, "endOffset": 151}, {"referenceID": 40, "context": "able? The problem is challenging because conventional manifold-based semi-supervised learning algorithms (Zhu and Ghahramani, 2002; Yang et al., 2016a) cannot be straightforwardly applied.", "startOffset": 105, "endOffset": 151}, {"referenceID": 15, "context": "answering tasks are extraction rather than generation, it is also not sensible to use unlabeled text to improve language modeling as in machine translation (Gulcehre et al., 2015).", "startOffset": 156, "endOffset": 179}, {"referenceID": 22, "context": "We experiment on the SQuAD dataset (Rajpurkar et al., 2016) with various labeling rates and various amounts of unlabeled data.", "startOffset": 35, "endOffset": 59}, {"referenceID": 8, "context": "tal results show that our GDAN framework consistently improves over both the supervised learning setting and the baseline methods, including adversarial domain adaptation (Ganin and Lempitsky, 2014) and dual learning (Xia et al.", "startOffset": 171, "endOffset": 198}, {"referenceID": 37, "context": "tal results show that our GDAN framework consistently improves over both the supervised learning setting and the baseline methods, including adversarial domain adaptation (Ganin and Lempitsky, 2014) and dual learning (Xia et al., 2016).", "startOffset": 217, "endOffset": 235}, {"referenceID": 22, "context": "In our formulation, following the setting in SQuAD (Rajpurkar et al., 2016), we specifically focus on extractive question answering, where a is always a consecutive chunk of text in p.", "startOffset": 51, "endOffset": 75}, {"referenceID": 4, "context": "Given the paragraph p, we apply a bidirectional Gated Recurrent Unit (GRU) network (Chung et al., 2014) on top of the embeddings of the sequence (p1, p2, \u00b7 \u00b7 \u00b7 , pT ), and obtain the initial paragraph representation Hp.", "startOffset": 83, "endOffset": 103}, {"referenceID": 6, "context": "The question and paragraph representations are combined with the gated-attention (GA) mechanism (Dhingra et al., 2016).", "startOffset": 96, "endOffset": 118}, {"referenceID": 39, "context": "max layers on top of Hp to predict the start and end indices of a, following Yang et al. (2017).", "startOffset": 77, "endOffset": 96}, {"referenceID": 16, "context": "Following a practice in domain adaptation (Johnson et al., 2016; Chu et al., 2017), we append the", "startOffset": 42, "endOffset": 82}, {"referenceID": 3, "context": "Following a practice in domain adaptation (Johnson et al., 2016; Chu et al., 2017), we append the", "startOffset": 42, "endOffset": 82}, {"referenceID": 27, "context": "We implement the generative model as a sequence-tosequence model (Sutskever et al., 2014) with a copy mechanism (Gu et al.", "startOffset": 65, "endOffset": 89}, {"referenceID": 13, "context": ", 2014) with a copy mechanism (Gu et al., 2016; Gulcehre et al., 2016).", "startOffset": 30, "endOffset": 70}, {"referenceID": 14, "context": ", 2014) with a copy mechanism (Gu et al., 2016; Gulcehre et al., 2016).", "startOffset": 30, "endOffset": 70}, {"referenceID": 13, "context": "Both pvocab and pcopy are defined as a function of the current hidden state ht and the attention results (Gu et al., 2016).", "startOffset": 105, "endOffset": 122}, {"referenceID": 11, "context": "Similar to the Generative Adversarial Nets (GANs) (Goodfellow et al., 2014) and adversarial domain adaptation (Ganin and Lempitsky, 2014), the discriminative model and the generative model have different objectives in our framework.", "startOffset": 50, "endOffset": 75}, {"referenceID": 8, "context": ", 2014) and adversarial domain adaptation (Ganin and Lempitsky, 2014), the discriminative model and the generative model have different objectives in our framework.", "startOffset": 42, "endOffset": 69}, {"referenceID": 11, "context": "However, rather than formulating the objective as an adversarial game between the two models (Goodfellow et al., 2014; Ganin and Lempitsky, 2014), in our framework, the discriminative model relies on the data generated by the generative model, while the generative model aims to match the model-generated data distribu-", "startOffset": 93, "endOffset": 145}, {"referenceID": 8, "context": "However, rather than formulating the objective as an adversarial game between the two models (Goodfellow et al., 2014; Ganin and Lempitsky, 2014), in our framework, the discriminative model relies on the data generated by the generative model, while the generative model aims to match the model-generated data distribu-", "startOffset": 93, "endOffset": 145}, {"referenceID": 37, "context": "Similar to the dual learning (Xia et al., 2016) framework, one can define an autoencoder objective.", "startOffset": 29, "endOffset": 47}, {"referenceID": 32, "context": "plete representation of the answers (Vincent et al., 2010).", "startOffset": 36, "endOffset": 58}, {"referenceID": 36, "context": "Since the output of G is discrete and non-differentiable, we use the Reinforce algorithm (Williams, 1992) to update G.", "startOffset": 89, "endOffset": 105}, {"referenceID": 22, "context": ", \u201cDate\u201d, \u201cOther Numeric\u201d, \u201cPerson\u201d, \u201cLocation\u201d, \u201cOther Entity\u201d, \u201cCommon Noun Phrase\u201d, \u201cAdjective Phrase\u201d, \u201cVerb Phrase\u201d, \u201cClause\u201d and \u201cOther\u201d (Rajpurkar et al., 2016).", "startOffset": 143, "endOffset": 167}, {"referenceID": 28, "context": "For each paragraph from the unlabeled articles, we utilize Stanford Part-Of-Speech (POS) tagger (Toutanova et al., 2003) to label each word with the corresponding POS tag, and implement a simple constituency parser to extract the noun phrase, verb phrase, adjective and clause based on a small set of constituency grammars.", "startOffset": 96, "endOffset": 120}, {"referenceID": 7, "context": "Next, we use Stanford Named Entity Recognizer (NER) (Finkel et al., 2005) to assign each word with one of the seven labels, i.", "startOffset": 52, "endOffset": 73}, {"referenceID": 22, "context": "We tune the hyper-parameters and perform early stopping on the development set using the F1 scores, and the performance is evaluated on the test set using both F1 scores and exact matching (EM) scores (Rajpurkar et al., 2016).", "startOffset": 201, "endOffset": 225}, {"referenceID": 8, "context": "Gen + GAN refers to the domain adaptation method using GANs (Ganin and Lempitsky, 2014); in contrast to the original work, the generative model is updated using Reinforce.", "startOffset": 60, "endOffset": 87}, {"referenceID": 37, "context": "Gen + dual refers to the dual learning method (Xia et al., 2016).", "startOffset": 46, "endOffset": 64}, {"referenceID": 8, "context": "We use our own implementation of \u201cGen + GAN\u201d and \u201cGen + dual\u201d, since the GAN model (Ganin and Lempitsky, 2014) does not handle discrete features and the dual learning model (Xia et al.", "startOffset": 83, "endOffset": 110}, {"referenceID": 37, "context": "We use our own implementation of \u201cGen + GAN\u201d and \u201cGen + dual\u201d, since the GAN model (Ganin and Lempitsky, 2014) does not handle discrete features and the dual learning model (Xia et al., 2016) cannot be directly applied to question", "startOffset": 173, "endOffset": 191}, {"referenceID": 8, "context": "When implementing these two baselines, we adopt the learning schedule introduced by Ganin and Lempitsky (2014), i.", "startOffset": 84, "endOffset": 111}, {"referenceID": 22, "context": "We report two metrics, the F1 scores and the exact matching (EM) scores (Rajpurkar et al., 2016), on the test set.", "startOffset": 72, "endOffset": 96}, {"referenceID": 43, "context": "Semi-supervised learning has been extensively studied in literature (Zhu, 2005).", "startOffset": 68, "endOffset": 79}, {"referenceID": 18, "context": "been recently proposed for semi-supervised learning based on representation learning techniques, such as generative models (Kingma et al., 2014), ladder networks (Rasmus et al.", "startOffset": 123, "endOffset": 144}, {"referenceID": 23, "context": ", 2014), ladder networks (Rasmus et al., 2015) and graph embeddings (Yang et al.", "startOffset": 25, "endOffset": 46}, {"referenceID": 40, "context": ", 2015) and graph embeddings (Yang et al., 2016a).", "startOffset": 29, "endOffset": 49}, {"referenceID": 44, "context": "Moreover, traditional graph-based semisupervised learning (Zhu and Ghahramani, 2002) cannot be easily extended to modeling the unlabeled answer chunks.", "startOffset": 58, "endOffset": 84}, {"referenceID": 8, "context": "Domain adaptation has been successfully applied to various tasks, such as classification (Ganin and Lempitsky, 2014) and machine translation (Johnson et al.", "startOffset": 89, "endOffset": 116}, {"referenceID": 16, "context": "Domain adaptation has been successfully applied to various tasks, such as classification (Ganin and Lempitsky, 2014) and machine translation (Johnson et al., 2016; Chu et al., 2017).", "startOffset": 141, "endOffset": 181}, {"referenceID": 3, "context": "Domain adaptation has been successfully applied to various tasks, such as classification (Ganin and Lempitsky, 2014) and machine translation (Johnson et al., 2016; Chu et al., 2017).", "startOffset": 141, "endOffset": 181}, {"referenceID": 9, "context": "tation (Glorot et al., 2011) focus on learning distribution invariant features by sharing the intermediate representations for downstream tasks.", "startOffset": 7, "endOffset": 28}, {"referenceID": 19, "context": "Another line of research on domain adaptation attempt to match the distance between different domain distributions in a low dimensional space (Long et al., 2015; Baktashmotlagh et al., 2013).", "startOffset": 142, "endOffset": 190}, {"referenceID": 0, "context": "Another line of research on domain adaptation attempt to match the distance between different domain distributions in a low dimensional space (Long et al., 2015; Baktashmotlagh et al., 2013).", "startOffset": 142, "endOffset": 190}, {"referenceID": 11, "context": "2012; Gopalan et al., 2011; Pan et al., 2011). Our work gets inspiration from a practice in Johnson et al. (2016) and Chu et al.", "startOffset": 6, "endOffset": 114}, {"referenceID": 3, "context": "(2016) and Chu et al. (2017) based on appending domain tags.", "startOffset": 11, "endOffset": 29}, {"referenceID": 11, "context": "GANs (Goodfellow et al., 2014) formulated a adversarial game between a discriminative model and a gener-", "startOffset": 5, "endOffset": 30}, {"referenceID": 8, "context": "Ganin and Lempitsky (Ganin and Lempitsky, 2014) employed a similar idea to use two models for domain adaptation.", "startOffset": 20, "endOffset": 47}, {"referenceID": 42, "context": "Review networks (Yang et al., 2016b) employ a discriminative model as a regularizer for training a generative model.", "startOffset": 16, "endOffset": 36}, {"referenceID": 31, "context": "text of machine translation, given a language pair, various recent work studied jointly training models to learn the mappings in both directions (Tu et al., 2016; Xia et al., 2016).", "startOffset": 145, "endOffset": 180}, {"referenceID": 37, "context": "text of machine translation, given a language pair, various recent work studied jointly training models to learn the mappings in both directions (Tu et al., 2016; Xia et al., 2016).", "startOffset": 145, "endOffset": 180}], "year": 2017, "abstractText": "We study the problem of semi-supervised question answering\u2014-utilizing unlabeled text to boost the performance of question answering models. We propose a novel training framework, the Generative Domain-Adaptive Nets. In this framework, we train a generative model to generate questions based on the unlabeled text, and combine model-generated questions with human-generated questions for training question answering models. We develop novel domain adaptation algorithms, based on reinforcement learning, to alleviate the discrepancy between the modelgenerated data distribution and the humangenerated data distribution. Experiments show that our proposed framework obtains substantial improvement from unlabeled text.", "creator": "LaTeX with hyperref package"}}}