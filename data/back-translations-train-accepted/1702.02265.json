{"id": "1702.02265", "review": {"conference": "EMNLP", "VERSION": "v1", "DATE_OF_SUBMISSION": "8-Feb-2017", "title": "Neural Machine Translation with Source-Side Latent Graph Parsing", "abstract": "This paper presents a novel neural machine translation model which jointly learns translation and source-side latent graph representations of sentences. Unlike existing pipelined approaches using syntactic parsers, our end-to-end model learns a latent graph parser as part of the encoder of an attention-based neural machine translation model, so the parser is optimized according to the translation objective. Experimental results show that our model significantly outperforms the previous best results on the standard English-to-Japanese translation dataset.", "histories": [["v1", "Wed, 8 Feb 2017 03:32:23 GMT  (131kb,D)", "http://arxiv.org/abs/1702.02265v1", null], ["v2", "Mon, 20 Feb 2017 01:47:16 GMT  (279kb,D)", "http://arxiv.org/abs/1702.02265v2", "Add results of using \"Pre-training\" in Table 2 and 3, with analysis on the learned latent graphs and translations"], ["v3", "Sun, 16 Apr 2017 22:46:08 GMT  (377kb,D)", "http://arxiv.org/abs/1702.02265v3", "Reported additional results"], ["v4", "Mon, 24 Jul 2017 14:52:06 GMT  (383kb,D)", "http://arxiv.org/abs/1702.02265v4", "Accepted as a full paper at the 2017 Conference on Empirical Methods in Natural Language Processing (EMNLP 2017)"]], "reviews": [], "SUBJECTS": "cs.CL", "authors": ["kazuma hashimoto", "yoshimasa tsuruoka"], "accepted": true, "id": "1702.02265"}, "pdf": {"name": "1702.02265.pdf", "metadata": {"source": "CRF", "title": "Neural Machine Translation with Source-Side Latent Graph Parsing", "authors": ["Kazuma Hashimoto", "Yoshimasa Tsuruoka"], "emails": ["hassy@logos.t.u-tokyo.ac.jp", "tsuruoka@logos.t.u-tokyo.ac.jp"], "sections": [{"heading": "1 Introduction", "text": "Due to its outstanding empirical results (Bahdanau et al., 2015; Luong et al., 2015; Sutskever et al., 2014), Neural Machine Translation (NMT) is an active field of research. Most of the existing NMT models treat each sentence as a sequence of symbols, but recent studies suggest that syntactic information can contribute to improving translation accuracy (Eriguchi et al., 2016b; Sennrich and Haddow, 2016; Stahlberg et al., 2016). Existing syntax-based NMT models use a syntactic parser trained through supervised learning and therefore based on human-commented trees. An alternative approach to using syntactic structures in a language processing task is to learn syntactic trees of sentences together with the target task (Socher et al., 2011; Yogatama et al., 2016)."}, {"heading": "2 Latent Graph Parser", "text": "We model the latent graph parser on the basis of dependency analysis. In dependency analysis, a sentence is represented as a tree structure in which each node corresponds to a word in the sentence and a unique root node (ROOT) is added. Suppose a sentence of length N, parent nodes Hwi {w1,..., wN, ROOT} (Hwi 6 = wi) of the word wi (1 \u2264 i \u2264 N) is called a header. Then, the sentence is represented as a set of tuples (wi, Hwi, 'wi), where \"wi is a dependency designation. In this essay, we remove the limitation on the use of tree structures and represent the sentence as a set of tuples (wi, p (Hwi | wi), p (' wi | wi)), where p (Hwi | wi) is the probability distribution of the parent nodes of wi, and p ('wi | wi) is the probability distribution of parasized names."}, {"heading": "2.1 Word Representations", "text": "The i-th input word wi is represented by concatenating its word for embedding vdp (wi) and its character n-gram for embedding c (wi) and Rd1: x (wi) = [vdp (wi); c (wi)], where d1 is the dimensionality of the embeds. c (wi) is calculated as an average of the embedding of the characters n-grams in wi."}, {"heading": "2.2 POS Tagging Layer", "text": "The latent graph parser is based on multi-layer bidirectional recurrent neural networks (RNNs) with long short-term memory (LSTM) units (Graves and Schmidhuber, 2005).In the first layer, the POS tagging is performed by calculating a hidden state h (1) i = [\u2212 \u2192 h (1) i; \u2190 \u2212 h (1) i] enhanced R2d1 for wi, where \u2212 \u2192 h (1) i = LSTM (\u2212 h (1) i \u2212 1, x (wi))), \u2212 Rd1 and \u2190 \u2212 h (1) i = LSTM (\u2190 \u2212 h (1) i + 1, x (wi), \u2012 Rd1 are hiding places of forward and backward LSTMs. h (1) i is then fed into a softmax classifier to predict a probability distribution p (1) i RC (1) for word levels where C (the number of these human layers cannot be described on the basis of only 1)."}, {"heading": "2.3 Dependency Parsing Layer", "text": "A hidden state h (2) i-R2d1 is calculated by \u2212 \u2192 h (2) i = LSTM (\u2212 \u2192 h (2) i = LSTM (\u2212 \u2192 h (2) i-1, [x (wi); y (wi); \u2212 \u2192 h (1) i) and \u2190 \u2212 h (2) i = LSTM (\u2190 \u2212 h (2) i + 1, [x (wi); y (wi); \u2190 \u2212 h (1) i)), where y (wi) = W (1) 'p (1) i-Rd2 are the output information from the first layer, and W (1) \"i-Rd2 \u00b7 C (1) is a weight matrix.Then the latent graph is calculated by calculating the probability number (Hwi = wj | wi) i-Rd2 = exp (i, j), k 6 = i exp (m (i, k)."}, {"heading": "3 NMT with Latent Graph Parser", "text": "The latent graph representation described in Section 2 can be ideally used for all tasks at the sentence level, and here we apply it to an attention-based NMT (ANMT) model (Luong et al., 2015). We modify the encoder and decoder in the ANMT model to learn latent graph representation."}, {"heading": "3.1 Encoder with Dependency Composition", "text": "The ANMT model first encodes the information about the input set and then generates a sentence in another language. The encoder represents the word wi with one word, the venc (wi) and the hidden state h (2) i as input to a unidirectional LSMT: h (enc) i = LSTM (h (enc) i \u2212 1, [venc (wi); h (2) i]. That is, the encoder of our model is a three-layer LSTM where the first two layers are bidirectional. In the sequential LSTMs, relationships between words in distant positions are not explicitly taken into account. In our model, we explicitly include such relationships in the encoder by hiding a definition of dependency (Wi) (wi = decomposition (wi) (wi) h = decomposition (wi) (wi) h =."}, {"heading": "3.2 Decoder with Attention Mechanism", "text": "The decoder of our model is a single-layer LSTM, and the initial state is effective with h (enc) N + 1 and its corresponding memory cell. Given the t-th hidden state h (dec) t (enc) i of the encoder: s (i, t) = exp (dec) t \u00b7 h (enc) i) n + 1j = 1 exp (h) t \u00b7 h (enc) j) i of the encoder: s (i, t) = exp (dec) t \u00b7 h (enc) i) n (enc) n + 1 exp (h) t \u00b7 h (enc) t \u00b7 h (enc) i = 1 s (i, t) h (enc) i, (enc) where s (i, t) is a scoring function that indicates how much each source-side hidden state contributes to the word prediction."}, {"heading": "4 Experimental Settings", "text": "We briefly describe our experimental settings and further details can be found in the appendix."}, {"heading": "4.1 Data", "text": "We used an English-Japanese translation assignment from the Asian Scientific Paper Excerpt Corpus (ASPEC) (Nakazawa et al., 2016b) because syntactical information in the English-Japanese translation has been shown to be useful (Eriguchi et al., 2016b; Neubig et al., 2015). We created a small training dataset with 100,000 translation pairs and a large training dataset with 1,346,946 pairs. Development datasets include 1,790 pairs and test datasets 1,812 pairs."}, {"heading": "4.2 Parameter Optimization and Translation", "text": "We set (d1, d2) = (100, 50) for the latent graph parser, d3 = 256 for the small training dataset and d3 = 512 for the large training dataset. The training was done by stochastic gradient descent with impulse in the minibatch. At test date we used a beam search algorithm with statistics on set lengths (Eriguchi et al., 2016b) and length normalization (Cho et al., 2014)."}, {"heading": "5 Results and Discussion", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "5.1 Results on Small Training Data", "text": "Table 1 shows the effects of learning the latent graph parser. Average values with standard deviations over five different runs of model training are shown. Assessment yardsticks are BLEU, RIBES (Isozaki et al., 2010) and perplexity values. Note that lower perplexity values indicate better accuracy. The result of the \"w / pre-training\" is achieved by training the latent parser prior to training using the widely used training data of the Wall Street Journal (WSJ). We follow Chen and Manning (2014) to generate the training data for the dependency parser. The parser, including the POS tagger, is first trained for 10 epochs in advance according to the multi-task learning method of Hashimoto et al. (2016b), and then the entire NMT model is trained. The result suggests that the pre-training can improve translation accuracy by removing baseline A, whereby the difference is constructed into an equilibrium with the NMT."}, {"heading": "5.2 Results on Large Training Data", "text": "The majority of them are able to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance,"}, {"heading": "6 Conclusion and Future Work", "text": "We have introduced an end-to-end NMT model by collectively learning translations and source latent graphs. In translating from English to Japanese, our NMT model significantly outperforms previous best models. In future work, we will examine the effectiveness of our approach in various types of target tasks. 2Our training time is over five days on an Amazon Web Service c4.8x machine using our CPU-based C + + code, while it is stated that the training time in Cromieres et al. (2016) is more than two weeks according to their GPU code."}, {"heading": "Acknowledgments", "text": "We thank Yuchen Qiao and Kenjiro Taura for their help in speeding up our training code. We also thank Akiko Eriguchi for providing the pre-processed data and helpful comments. This work was supported by CREST, JST."}, {"heading": "A Supplemental Material", "text": "A.1 Data PreprocessingWe follow the data preprocessing instruction for the English-to-Japanese task in Eriguchi et al. (2016b). The English sentences were symbolized by the tokenizer in the Enju parser (Miyao and Tsujii, 2008), and the Japanese sentences were segmented by the KyTea Tool3. Among the first 1,500,000 translation pairs in the training data, we selected 1,346,946 pairs, where the maximum sentence length is 50. This is the great training technique, and we continued to pitch 100,000 pairs to construct the small training data sets. For the small data sets, we built the vocabulary with words whose minimum frequency is two, and for the large data sets, we used words whose minimum frequency is three for English and five for Japanese. As a result, the target language vocabularies were calculated with 23.532 for the small data sets, and 65.680 for the large data sets."}], "references": [], "referenceMentions": [], "year": 2017, "abstractText": "This paper presents a novel neural machine translation model which jointly learns translation and source-side latent graph representations of sentences. Unlike existing pipelined approaches using syntactic parsers, our end-to-end model learns a latent graph parser as part of the encoder of an attention-based neural machine translation model, so the parser is optimized according to the translation objective. Experimental results show that our model significantly outperforms the previous best results on the standard Englishto-Japanese translation dataset.", "creator": "LaTeX with hyperref package"}}}