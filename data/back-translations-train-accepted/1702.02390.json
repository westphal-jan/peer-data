{"id": "1702.02390", "review": {"conference": "EMNLP", "VERSION": "v1", "DATE_OF_SUBMISSION": "8-Feb-2017", "title": "A Hybrid Convolutional Variational Autoencoder for Text Generation", "abstract": "In this paper we explore the effect of architectural choices on learning a Variational Autoencoder (VAE) for text generation. In contrast to the previously introduced VAE model for text where both the encoder and decoder are RNNs, we propose a novel hybrid architecture that blends fully feed-forward convolutional and deconvolutional components with a recurrent language model. Our architecture exhibits several attractive properties such as faster run time and convergence, ability to better handle long sequences and, more importantly, it helps to avoid some of the major difficulties posed by training VAE models on textual data.", "histories": [["v1", "Wed, 8 Feb 2017 12:11:41 GMT  (468kb,D)", "http://arxiv.org/abs/1702.02390v1", null]], "reviews": [], "SUBJECTS": "cs.CL", "authors": ["stanislau semeniuta", "aliaksei severyn", "erhardt barth"], "accepted": true, "id": "1702.02390"}, "pdf": {"name": "1702.02390.pdf", "metadata": {"source": "CRF", "title": "A Hybrid Convolutional Variational Autoencoder for Text Generation", "authors": ["Stanislau Semeniuta", "Aliaksei Severyn", "Erhardt Barth"], "emails": ["stas@inb.uni-luebeck.de", "barth@inb.uni-luebeck.de", "severyn@google.com", "@userid", "@userid", "@userid", "@userid", "@userid", "@userid", "@userid", "@userid", "@userid", "@userid", "@userid", "@userid", "@userid", "@userid", "@userid", "@userid", "@userid", "@userid", "@userid", "@userid", "@userid"], "sections": [{"heading": "1 Introduction", "text": "This year, it has reached the point where it will be able to retaliate in order to pave the way for the future."}, {"heading": "2 Related Work", "text": "Currently, there are three major streams of approaches to generative modeling: the Variational Autoencoder (Kingma and Welling, 2013; Rezende et al., 2014), autoregressive models (Larochelle and Murray, 2011; van den Oord et al., 2016), and Generative Adversarial Networks (GAN) (Goodfellow et al., 2014). Autoregressive models are based on the assumption that the current data element can be accurately predicted given the sufficient history of the elements produced to date. Conventional RNN-based language models fall into this category and currently dominate speech modeling and the generational problem in NLP. Neural architectures are based on recurring (Jo'zefowicz et al., 2016; Ha et al., 2016) or conventional decoders (Kalchbrenner et al., 2016; Dauphin et al., 2016) offer an effective solution to this problem."}, {"heading": "3 Model", "text": "In this section, we first briefly explain the Kingma and Welling (2013) UAE framework, then describe our hybrid architecture, where the feedback part consists of a fully twisted encoder and a decoder that combines deconventional layers and a conventional RNN. Finally, we discuss optimization recipes that help UAE respect latent variables, which is critical to smoothing out latent space and trying out realistic phrases."}, {"heading": "3.1 Variational Autoencoder", "text": "This year, it has come to the point where there is only one person who is able to move around the world."}, {"heading": "3.2 Deconvolutional Networks", "text": "A deconvolutionary layer (also referred to as transposed convolutions (Gulrajani et al., 2016) and fractional striped convolutions (Radford et al., 2015) perform a spatial upward scan of their inputs and are an integral part of latent variable generative image models (Radford et al., 2015; Gulrajani et al., 2016) and semantic segmentation algorithms (Noh et al., 2015).Their goal is to perform an \"inverse\" folding operation and increase the spatial size of the input while decreasing the number of characteristic maps. This operation can be considered a backward-running layer of a revolutionary layer and can be implemented by simply switching the folding operation. In the context of generative modeling based on global representations, the deconfrontations are typically used as follows: the global representation is applied to another representation with small spatial resolution and large image area."}, {"heading": "3.3 Hybrid Convolutional-Recurrent VAE", "text": "The first component is a standard UAE, in which the encoders and decoder modules are parameterized by revolutionary and deconvolutionary layers (see Figure 2 (a)). This architecture is attractive for its computational efficiency and simplicity of training. The second component is a recursive language model that links activations from the deconvolutionary decoder to the previous output characters. We consider two types of recurrent functions: a conventional LSTM network (Figure 2 (b) and a stack of masked convolutions, also known as ByteNet decoders from Kalchbrenner et al. (Figure 2 (c))) The primary reason for a recursive component in the decoder is to capture dependencies between elements of the text sequences."}, {"heading": "3.4 Optimization Difficulties", "text": "The addition of the recursive component leads to optimization difficulties similar to those described by Bowman et al. (2016). In most cases, the model converges to a solution with an infinitesimal KL term and thus effectively reverts to a conventional language model. Bowman et al. (2016) have proposed to use input dropout and KL term annealing to encourage their model to encode meaningful representations in the z vector. We have found that these techniques also help our model to achieve solutions with a KL term that is not zero. KL-Term annealing can be viewed as a gradual transition from conventional deterministic autoencoder to a full VAE. In this work, we use linear hursialing from 0 to 1. We have experimented with other time schedules, but found no significant effects on the end result. As long as the KL-term total weight slowly begins to mutate the terex-weight, we do not seem to influence the terex-velocity of its growth."}, {"heading": "4 Experiments", "text": "All models were trained with the Adam optimization algorithm (Kingma and Ba, 2014) with decreasing learning rate. To make our results easily reproducible, we have released the source code of all our experiments 1.Data. Our first task is character generation at character level, which is performed on the basis of the standard Penn Treebank dataset (Marcus et al., 1993). One of the goals is to test the ability of the models to successfully learn the representations of long sequences."}, {"heading": "4.1 Comparison with LSTM VAE", "text": "We start with an experiment in which the decoder is forced to ignore history and must rely entirely on the latent vector. By applying the decoder only to the latent vector, we can directly compare the expressivity of the compared models for the LSTM1https: / / github.com / stas-semeniuta / textvaeVAE models, which are identical to the expression of the input elements. We compare it to our fullyfeedforward model without the recursive layer in the decoder (Figure 2). Both networks are parameterized to have the same number of parameters. To test how well both models can cope with the stochasticity of the latent vectors, we only minimize the reconstruction concept of Eq."}, {"heading": "4.2 Controlling the KL term", "text": "We study the effects of various training techniques that help to control the KL concept that is critical to the formation of a generative VAE model. Aux cost-weight. First, we provide a detailed overview of how optimization tricks discussed in Section 3.4 affect the performance of our hybrid model. Figure 5 presents the results of our model, which was trained with other values of \u03b1 from Eq. (3). Note that the inclusion of auxiliary reconstruction losses slightly damages the limit on the probability of the data, but helps the model to rely more on the latent vector than \u03b1. A similar effect on the limit of the model was observed by Bowman et al. (2016): Increased input dropout rates force their model to put more information into the z-vector, but at the cost of increased end-loss values. This is a trade-off that allows sampling of outputs in the AVE framework, not our model to consider that you can find a trilateral solution."}, {"heading": "4.3 Generating Tweets", "text": "In this section, we present qualitative results on the task of generating tweets.Data. We use 1M Tweets 2 to train our model and test it on a held-up dataset of 10k samples. We prepare tweets minimally by replacing only user ID and URLs with \"@ userid\" and \"url.\" We use 5 revolutionary layers with ReLU nonlinearity, core size 3 and step 2 in the encoder. The number of feature maps is [128, 256, 512, 512, 512] for each layer. The decoder is configured equally, with the number of feature maps decreasing in each successive layer. The top layer is an LSTM with 1000 units. We did not observe any significant revision. The baseline LSTM-VAE model contained two different LSTMs with 1000 cells each. The models have a comparable number of parameters of our model 10.5M for the LSTE: 8M hybrid model."}, {"heading": "5 Conclusions", "text": "We have introduced a novel generative model of nature texts based on the VAE framework. Its core components are a revolutionary encoder and a deconvolutionary decoder combined with a recursive layer. We have shown that the upstream part of our model architecture makes it easier to train a VAE and avoid the problem of a KL term collapsing to zero, with the decoder reverting to a standard language model, thereby inhibiting the scanning ability of VAE. Furthermore, we propose a more natural way to encourage the model to rely on the latent vector by introducing an additional cost term into the training target. We observe that it works well on long sequences that are difficult to achieve with purely RNN-based VAEs, by applying the previously proposed tricks such as KL-Term Annealing and Input Dropout. Finally, we have evaluated the target conflict between the receptor size and specifically the impact of the receptor loss on the different types."}, {"heading": "Acknowledgments", "text": "We thank Enrique Alfonseca, Katja Filippova, Sylvain Gelly, Jason Lee and David Wei\u00df for their useful feedback in compiling this draft. This project was funded by the European Union Framework Programme for Research and Innovation HORIZON 2020 (2014-2020) under Marie Skodowska-Curie Agreement No. 641805. Stanislau Semeniuta thanks the support of Pattern Recognition Company GmbH. We thank NVIDIA Corporation for supporting the Titan X GPU used for this research."}, {"heading": "A Supplementary Material", "text": "In this supplementary material, we include additional random samples from our hybrid UAE model and LSTM UAE (Bowman et al., 2016) to show that they lead to smooth and syntactically correct transitions between generated texts.A.1 Random samples Table 4 contains random samples generated by our hybrid model with LSTM decoder, while Table 5 contains samples from the LSTM UAE. As our experiments in Section 4.3 have already shown, it is much more difficult for LSTM VAE model to find solutions with KL terms that are significantly different from zero, resulting in redundant and uninteresting samples generated by the model. We have experimented with higher drop-out rates, but found that this did not result in better samples. In contrast, samples from our model both in length and in content indicate significantly more textual variations generated by the translate2 space generated by bowls (STE)."}], "references": [{"title": "Layer normalization", "author": ["Lei Jimmy Ba", "Ryan Kiros", "Geoffrey E. Hinton."], "venue": "CoRR abs/1607.06450.", "citeRegEx": "Ba et al\\.,? 2016", "shortCiteRegEx": "Ba et al\\.", "year": 2016}, {"title": "An architecture for deep, hierarchical generative models", "author": ["Philip Bachman."], "venue": "D. D. Lee, M. Sugiyama, U. V. Luxburg, I. Guyon, and R. Garnett, editors, NIPS, pages 4826\u20134834.", "citeRegEx": "Bachman.,? 2016", "shortCiteRegEx": "Bachman.", "year": 2016}, {"title": "Neural machine translation by jointly learning to align and translate", "author": ["Dzmitry Bahdanau", "Kyunghyun Cho", "Yoshua Bengio."], "venue": "CoRR abs/1409.0473.", "citeRegEx": "Bahdanau et al\\.,? 2014", "shortCiteRegEx": "Bahdanau et al\\.", "year": 2014}, {"title": "Generating sentences from a continuous space", "author": ["Samuel R. Bowman", "Luke Vilnis", "Oriol Vinyals", "Andrew M. Dai", "Rafal J\u00f3zefowicz", "Samy Bengio."], "venue": "CONLL. pages 10\u201321.", "citeRegEx": "Bowman et al\\.,? 2016", "shortCiteRegEx": "Bowman et al\\.", "year": 2016}, {"title": "Language modeling with gated convolutional networks", "author": ["Yann N. Dauphin", "Angela Fan", "Michael Auli", "David Grangier."], "venue": "CoRR abs/1612.08083.", "citeRegEx": "Dauphin et al\\.,? 2016", "shortCiteRegEx": "Dauphin et al\\.", "year": 2016}, {"title": "Deep generative image models using a laplacian pyramid of adversarial networks", "author": ["Emily L. Denton", "Soumith Chintala", "Arthur Szlam", "Robert Fergus."], "venue": "CoRR abs/1506.05751.", "citeRegEx": "Denton et al\\.,? 2015", "shortCiteRegEx": "Denton et al\\.", "year": 2015}, {"title": "Sequential neural models with stochastic layers", "author": ["Marco Fraccaro", "S\u00f8ren Kaae S\u00f8 nderby", "Ulrich Paquet", "Ole Winther."], "venue": "NIPS, pages 2199\u20132207.", "citeRegEx": "Fraccaro et al\\.,? 2016", "shortCiteRegEx": "Fraccaro et al\\.", "year": 2016}, {"title": "Generative adversarial networks", "author": ["Ian J. Goodfellow", "Jean Pouget-Abadie", "Mehdi Mirza", "Bing Xu", "David Warde-Farley", "Sherjil Ozair", "Aaron C. Courville", "Yoshua Bengio."], "venue": "CoRR abs/1406.2661.", "citeRegEx": "Goodfellow et al\\.,? 2014", "shortCiteRegEx": "Goodfellow et al\\.", "year": 2014}, {"title": "Pixelvae: A latent variable model for natural images", "author": ["Ishaan Gulrajani", "Kundan Kumar", "Faruk Ahmed", "Adrien Ali Taiga", "Francesco Visin", "David V\u00e1zquez", "Aaron C. Courville."], "venue": "CoRR abs/1611.05013.", "citeRegEx": "Gulrajani et al\\.,? 2016", "shortCiteRegEx": "Gulrajani et al\\.", "year": 2016}, {"title": "Hypernetworks", "author": ["David Ha", "Andrew M. Dai", "Quoc V. Le."], "venue": "CoRR abs/1609.09106.", "citeRegEx": "Ha et al\\.,? 2016", "shortCiteRegEx": "Ha et al\\.", "year": 2016}, {"title": "Long short-term memory", "author": ["Sepp Hochreiter", "J\u00fcrgen Schmidhuber."], "venue": "Neural Computation pages 1735\u20131780.", "citeRegEx": "Hochreiter and Schmidhuber.,? 1997", "shortCiteRegEx": "Hochreiter and Schmidhuber.", "year": 1997}, {"title": "Batch normalization: Accelerating deep network training by reducing internal covariate shift", "author": ["Sergey Ioffe", "Christian Szegedy."], "venue": "ICML. pages 448\u2013456.", "citeRegEx": "Ioffe and Szegedy.,? 2015", "shortCiteRegEx": "Ioffe and Szegedy.", "year": 2015}, {"title": "Exploring the limits of language modeling", "author": ["Rafal J\u00f3zefowicz", "Oriol Vinyals", "Mike Schuster", "Noam Shazeer", "Yonghui Wu."], "venue": "CoRR abs/1602.02410.", "citeRegEx": "J\u00f3zefowicz et al\\.,? 2016", "shortCiteRegEx": "J\u00f3zefowicz et al\\.", "year": 2016}, {"title": "Neural machine translation in linear time", "author": ["Nal Kalchbrenner", "Lasse Espeholt", "Karen Simonyan", "A\u00e4ron van den Oord", "Alex Graves", "Koray Kavukcuoglu."], "venue": "CoRR abs/1610.10099.", "citeRegEx": "Kalchbrenner et al\\.,? 2016", "shortCiteRegEx": "Kalchbrenner et al\\.", "year": 2016}, {"title": "Adam: A method for stochastic optimization", "author": ["Diederik P. Kingma", "Jimmy Ba."], "venue": "CoRR abs/1412.6980.", "citeRegEx": "Kingma and Ba.,? 2014", "shortCiteRegEx": "Kingma and Ba.", "year": 2014}, {"title": "Autoencoding variational bayes", "author": ["Diederik P. Kingma", "Max Welling."], "venue": "CoRR abs/1312.6114.", "citeRegEx": "Kingma and Welling.,? 2013", "shortCiteRegEx": "Kingma and Welling.", "year": 2013}, {"title": "The neural autoregressive distribution estimator", "author": ["Hugo Larochelle", "Iain Murray."], "venue": "AISTATS. pages 29\u201337.", "citeRegEx": "Larochelle and Murray.,? 2011", "shortCiteRegEx": "Larochelle and Murray.", "year": 2011}, {"title": "Autoencoding beyond pixels using a learned similarity metric", "author": ["Anders Boesen Lindbo Larsen", "S\u00f8ren Kaae S\u00f8nderby", "Ole Winther."], "venue": "CoRR abs/1512.09300. http://arxiv.org/abs/1512.09300.", "citeRegEx": "Larsen et al\\.,? 2015", "shortCiteRegEx": "Larsen et al\\.", "year": 2015}, {"title": "Building a large annotated corpus of english: The penn treebank", "author": ["Mitchell P. Marcus", "Mary Ann Marcinkiewicz", "Beatrice Santorini."], "venue": "Computational Linguistics 19(2):313\u2013330.", "citeRegEx": "Marcus et al\\.,? 1993", "shortCiteRegEx": "Marcus et al\\.", "year": 1993}, {"title": "Neural variational inference for text processing", "author": ["Yishu Miao", "Lei Yu", "Phil Blunsom."], "venue": "CoRR abs/1511.06038.", "citeRegEx": "Miao et al\\.,? 2015", "shortCiteRegEx": "Miao et al\\.", "year": 2015}, {"title": "Learning deconvolution network for semantic segmentation", "author": ["Hyeonwoo Noh", "Seunghoon Hong", "Bohyung Han."], "venue": "CoRR abs/1505.04366.", "citeRegEx": "Noh et al\\.,? 2015", "shortCiteRegEx": "Noh et al\\.", "year": 2015}, {"title": "On the difficulty of training recurrent neural networks", "author": ["Razvan Pascanu", "Tomas Mikolov", "Yoshua Bengio."], "venue": "ICML. pages 1310\u20131318.", "citeRegEx": "Pascanu et al\\.,? 2013", "shortCiteRegEx": "Pascanu et al\\.", "year": 2013}, {"title": "Unsupervised representation learning with deep convolutional generative adversarial networks", "author": ["Alec Radford", "Luke Metz", "Soumith Chintala."], "venue": "CoRR abs/1511.06434.", "citeRegEx": "Radford et al\\.,? 2015", "shortCiteRegEx": "Radford et al\\.", "year": 2015}, {"title": "Techniques for learning binary stochastic feedforward neural networks", "author": ["Tapani Raiko", "Mathias Berglund", "Guillaume Alain", "Laurent Dinh."], "venue": "CoRR abs/1406.2989.", "citeRegEx": "Raiko et al\\.,? 2014", "shortCiteRegEx": "Raiko et al\\.", "year": 2014}, {"title": "Stochastic backpropagation and approximate inference in deep generative models", "author": ["Danilo Jimenez Rezende", "Shakir Mohamed", "Daan Wierstra."], "venue": "ICML. pages 1278\u20131286.", "citeRegEx": "Rezende et al\\.,? 2014", "shortCiteRegEx": "Rezende et al\\.", "year": 2014}, {"title": "A neural attention model for abstractive sentence summarization", "author": ["Alexander M. Rush", "Sumit Chopra", "Jason Weston."], "venue": "CoRR abs/1509.00685.", "citeRegEx": "Rush et al\\.,? 2015", "shortCiteRegEx": "Rush et al\\.", "year": 2015}, {"title": "Improved techniques for training gans", "author": ["Tim Salimans", "Ian J. Goodfellow", "Wojciech Zaremba", "Vicki Cheung", "Alec Radford", "Xi Chen."], "venue": "CoRR abs/1606.03498.", "citeRegEx": "Salimans et al\\.,? 2016", "shortCiteRegEx": "Salimans et al\\.", "year": 2016}, {"title": "A hierarchical latent variable encoder-decoder model for generating dialogues", "author": ["Iulian Vlad Serban", "Alessandro Sordoni", "Ryan Lowe", "Laurent Charlin", "Joelle Pineau", "Aaron C. Courville", "Yoshua Bengio."], "venue": "CoRR abs/1605.06069.", "citeRegEx": "Serban et al\\.,? 2016", "shortCiteRegEx": "Serban et al\\.", "year": 2016}, {"title": "Ladder variational autoencoders", "author": ["Casper Kaae S\u00f8nderby", "Tapani Raiko", "Lars Maal\u00f8e", "S\u00f8ren Kaae S\u00f8nderby", "Ole Winther."], "venue": "CoRR abs/1602.02282. https://arxiv.org/abs/1602.02282.", "citeRegEx": "S\u00f8nderby et al\\.,? 2016", "shortCiteRegEx": "S\u00f8nderby et al\\.", "year": 2016}, {"title": "Pixel recurrent neural networks", "author": ["A\u00e4ron van den Oord", "Nal Kalchbrenner", "Koray Kavukcuoglu."], "venue": "CoRR abs/1601.06759.", "citeRegEx": "Oord et al\\.,? 2016", "shortCiteRegEx": "Oord et al\\.", "year": 2016}, {"title": "Show and tell: A neural image caption generator", "author": ["Oriol Vinyals", "Alexander Toshev", "Samy Bengio", "Dumitru Erhan."], "venue": "CoRR abs/1411.4555.", "citeRegEx": "Vinyals et al\\.,? 2014", "shortCiteRegEx": "Vinyals et al\\.", "year": 2014}, {"title": "Attribute2image: Conditional image generation from visual attributes", "author": ["Xinchen Yan", "Jimei Yang", "Kihyuk Sohn", "Honglak Lee."], "venue": "CoRR abs/1512.00570. http://arxiv.org/abs/1512.00570.", "citeRegEx": "Yan et al\\.,? 2015", "shortCiteRegEx": "Yan et al\\.", "year": 2015}, {"title": "Epitomic variational autoencoder", "author": ["Serena Yeung", "Anitha Kannan", "Yann Dauphin", "Li Fei-Fei."], "venue": "submission to ICLR 2017 http://ai.stanford.edu/ syyeung/resources/YeuKanDauLi16.pdf.", "citeRegEx": "Yeung et al\\.,? 2016", "shortCiteRegEx": "Yeung et al\\.", "year": 2016}, {"title": "Seqgan: Sequence generative adversarial nets with policy gradient", "author": ["Lantao Yu", "Weinan Zhang", "Jun Wang", "Yong Yu."], "venue": "CoRR abs/1609.05473.", "citeRegEx": "Yu et al\\.,? 2016", "shortCiteRegEx": "Yu et al\\.", "year": 2016}, {"title": "Deconvolutional networks", "author": ["Matthew D. Zeiler", "Dilip Krishnan", "Graham W. Taylor", "Robert Fergus."], "venue": "CVPR. pages 2528\u20132535.", "citeRegEx": "Zeiler et al\\.,? 2010", "shortCiteRegEx": "Zeiler et al\\.", "year": 2010}, {"title": "Variational neural machine translation", "author": ["Biao Zhang", "Deyi Xiong", "Jinsong Su."], "venue": "CoRR abs/1605.07869.", "citeRegEx": "Zhang et al\\.,? 2016", "shortCiteRegEx": "Zhang et al\\.", "year": 2016}, {"title": "Generating text via", "author": ["Yizhe Zhang", "Zhe Gan"], "venue": null, "citeRegEx": "Zhang and Gan.,? \\Q2016\\E", "shortCiteRegEx": "Zhang and Gan.", "year": 2016}], "referenceMentions": [{"referenceID": 2, "context": "Generative models of texts are currently at the cornerstone of natural language understanding enabling recent breakthroughs in machine translation (Bahdanau et al., 2014; Wu et al., 2016), dialogue modelling (Serban et al.", "startOffset": 147, "endOffset": 187}, {"referenceID": 27, "context": ", 2016), dialogue modelling (Serban et al., 2016), abstractive summarization (Rush et al.", "startOffset": 28, "endOffset": 49}, {"referenceID": 25, "context": ", 2016), abstractive summarization (Rush et al., 2015), etc.", "startOffset": 35, "endOffset": 54}, {"referenceID": 12, "context": "Currently, RNN-based generative models hold state of the art results in both unconditional (J\u00f3zefowicz et al., 2016; Ha et al., 2016) and conditional (Vinyals et al.", "startOffset": 91, "endOffset": 133}, {"referenceID": 9, "context": "Currently, RNN-based generative models hold state of the art results in both unconditional (J\u00f3zefowicz et al., 2016; Ha et al., 2016) and conditional (Vinyals et al.", "startOffset": 91, "endOffset": 133}, {"referenceID": 30, "context": ", 2016) and conditional (Vinyals et al., 2014) text generation.", "startOffset": 24, "endOffset": 46}, {"referenceID": 15, "context": "Variational Autoencoders (VAE), recently introduced by (Kingma and Welling, 2013; Rezende et al., 2014), offer a different approach to genera-", "startOffset": 55, "endOffset": 103}, {"referenceID": 24, "context": "Variational Autoencoders (VAE), recently introduced by (Kingma and Welling, 2013; Rezende et al., 2014), offer a different approach to genera-", "startOffset": 55, "endOffset": 103}, {"referenceID": 31, "context": "Similar to compelling examples from image generation, where it is possible to condition generated human faces on various attributes such as hair, skin color and style (Yan et al., 2015; Larsen et al., 2015), in", "startOffset": 167, "endOffset": 206}, {"referenceID": 17, "context": "Similar to compelling examples from image generation, where it is possible to condition generated human faces on various attributes such as hair, skin color and style (Yan et al., 2015; Larsen et al., 2015), in", "startOffset": 167, "endOffset": 206}, {"referenceID": 1, "context": "While training VAE-based models seems to pose little difficulty when applied to the tasks of generating natural images (Bachman, 2016; Gulrajani et al., 2016) and speech (Fraccaro et al.", "startOffset": 119, "endOffset": 158}, {"referenceID": 8, "context": "While training VAE-based models seems to pose little difficulty when applied to the tasks of generating natural images (Bachman, 2016; Gulrajani et al., 2016) and speech (Fraccaro et al.", "startOffset": 119, "endOffset": 158}, {"referenceID": 6, "context": ", 2016) and speech (Fraccaro et al., 2016), their application to natural text generation requires additional care (Bowman et al.", "startOffset": 19, "endOffset": 42}, {"referenceID": 3, "context": ", 2016), their application to natural text generation requires additional care (Bowman et al., 2016; Miao et al., 2015).", "startOffset": 79, "endOffset": 119}, {"referenceID": 19, "context": ", 2016), their application to natural text generation requires additional care (Bowman et al., 2016; Miao et al., 2015).", "startOffset": 79, "endOffset": 119}, {"referenceID": 1, "context": "While training VAE-based models seems to pose little difficulty when applied to the tasks of generating natural images (Bachman, 2016; Gulrajani et al., 2016) and speech (Fraccaro et al., 2016), their application to natural text generation requires additional care (Bowman et al., 2016; Miao et al., 2015). As discussed by Bowman et al. (2016), the core difficulty of training VAE models is the collapse of the latent loss (represented by the KL divergence term) to zero.", "startOffset": 120, "endOffset": 344}, {"referenceID": 34, "context": "lutional and deconvolutional (Zeiler et al., 2010) layers.", "startOffset": 29, "endOffset": 50}, {"referenceID": 15, "context": "Currently, there are three major streams of approaches to generative modeling: the Variational Autoencoder (Kingma and Welling, 2013; Rezende et al., 2014), autoregressive models (Larochelle and Murray, 2011; van den Oord et al.", "startOffset": 107, "endOffset": 155}, {"referenceID": 24, "context": "Currently, there are three major streams of approaches to generative modeling: the Variational Autoencoder (Kingma and Welling, 2013; Rezende et al., 2014), autoregressive models (Larochelle and Murray, 2011; van den Oord et al.", "startOffset": 107, "endOffset": 155}, {"referenceID": 16, "context": ", 2014), autoregressive models (Larochelle and Murray, 2011; van den Oord et al., 2016) and Generative Adversarial Networks (GAN) (Goodfellow et al.", "startOffset": 31, "endOffset": 87}, {"referenceID": 7, "context": ", 2016) and Generative Adversarial Networks (GAN) (Goodfellow et al., 2014).", "startOffset": 50, "endOffset": 75}, {"referenceID": 3, "context": ", 2016) or convolutional decoders Figure 1: LSTM VAE model of (Bowman et al., 2016)", "startOffset": 62, "endOffset": 83}, {"referenceID": 13, "context": "(Kalchbrenner et al., 2016; Dauphin et al., 2016) provide an effective solution to this problem.", "startOffset": 0, "endOffset": 49}, {"referenceID": 4, "context": "(Kalchbrenner et al., 2016; Dauphin et al., 2016) provide an effective solution to this problem.", "startOffset": 0, "endOffset": 49}, {"referenceID": 22, "context": "GANs have proven to be very effective in the Computer Vision domain (Radford et al., 2015; Denton et al., 2015; Salimans et al., 2016) but so far have gained little traction in the NLP community (Yu et al.", "startOffset": 68, "endOffset": 134}, {"referenceID": 5, "context": "GANs have proven to be very effective in the Computer Vision domain (Radford et al., 2015; Denton et al., 2015; Salimans et al., 2016) but so far have gained little traction in the NLP community (Yu et al.", "startOffset": 68, "endOffset": 134}, {"referenceID": 26, "context": "GANs have proven to be very effective in the Computer Vision domain (Radford et al., 2015; Denton et al., 2015; Salimans et al., 2016) but so far have gained little traction in the NLP community (Yu et al.", "startOffset": 68, "endOffset": 134}, {"referenceID": 33, "context": ", 2016) but so far have gained little traction in the NLP community (Yu et al., 2016; Zhang and Gan, 2016).", "startOffset": 68, "endOffset": 106}, {"referenceID": 36, "context": ", 2016) but so far have gained little traction in the NLP community (Yu et al., 2016; Zhang and Gan, 2016).", "startOffset": 68, "endOffset": 106}, {"referenceID": 3, "context": "Bowman et al. (2016) is a recent work that tackles language generation problem within the VAE framework.", "startOffset": 0, "endOffset": 21}, {"referenceID": 10, "context": "Although their model is slightly outperformed by a traditional LSTM (Hochreiter and Schmidhuber, 1997) language model, their model achieves a similar effect as in computer vision where one can (i) sam-", "startOffset": 68, "endOffset": 102}, {"referenceID": 19, "context": "Miao et al. (2015) apply VAE to bagof-words representations of documents and the an-", "startOffset": 0, "endOffset": 19}, {"referenceID": 3, "context": "Various techniques to improve training of VAE models where the total cost represents a trade-off between the reconstruction cost and KL term have been used so far: KL-term annealing and input dropout (Bowman et al., 2016; S\u00f8nderby et al., 2016), imposing structured sparsity on latent variables (Yeung et al.", "startOffset": 200, "endOffset": 244}, {"referenceID": 28, "context": "Various techniques to improve training of VAE models where the total cost represents a trade-off between the reconstruction cost and KL term have been used so far: KL-term annealing and input dropout (Bowman et al., 2016; S\u00f8nderby et al., 2016), imposing structured sparsity on latent variables (Yeung et al.", "startOffset": 200, "endOffset": 244}, {"referenceID": 32, "context": ", 2016), imposing structured sparsity on latent variables (Yeung et al., 2016).", "startOffset": 58, "endOffset": 78}, {"referenceID": 31, "context": "Zhang et al. (2016) and Serban et al.", "startOffset": 0, "endOffset": 20}, {"referenceID": 26, "context": "(2016) and Serban et al. (2016) apply VAE to sequence-to-sequence problems, improving over deterministic alternatives.", "startOffset": 11, "endOffset": 32}, {"referenceID": 15, "context": "framework of Kingma and Welling (2013), then describe our hybrid architecture where the feedforward part is composed of a fully convolutional encoder and a decoder that combines deconvolutional layers and a conventional RNN.", "startOffset": 13, "endOffset": 39}, {"referenceID": 15, "context": "The prior is typically chosen to be also a Gaussian with zero mean and unit variance, such that the KL term between posterior and prior can be computed in closed form (Kingma and Welling, 2013).", "startOffset": 167, "endOffset": 193}, {"referenceID": 23, "context": "The most straight-forward way to achieve a good reconstruction error in this case is to predict a very sharp probability distribution effectively corresponding to a single point in the latent space (Raiko et al., 2014).", "startOffset": 198, "endOffset": 218}, {"referenceID": 3, "context": "Bowman et al. (2016) demonstrate that this does not work in the fully deterministic Autoencoder framework .", "startOffset": 0, "endOffset": 21}, {"referenceID": 8, "context": "A deconvolutional layer (also referred to as transposed convolutions (Gulrajani et al., 2016) and fractionally strided convolutions (Radford et al.", "startOffset": 69, "endOffset": 93}, {"referenceID": 22, "context": "2015)) performs spatial up-sampling of its inputs and is an integral part of latent variable generative models of images (Radford et al., 2015; Gulrajani et al., 2016) and semantic segmentation algorithms (Noh et al.", "startOffset": 121, "endOffset": 167}, {"referenceID": 8, "context": "2015)) performs spatial up-sampling of its inputs and is an integral part of latent variable generative models of images (Radford et al., 2015; Gulrajani et al., 2016) and semantic segmentation algorithms (Noh et al.", "startOffset": 121, "endOffset": 167}, {"referenceID": 20, "context": ", 2016) and semantic segmentation algorithms (Noh et al., 2015).", "startOffset": 45, "endOffset": 63}, {"referenceID": 22, "context": "A notable example of such a model is the Deep Network of (Radford et al., 2015) trained with adversarial objective.", "startOffset": 57, "endOffset": 79}, {"referenceID": 13, "context": "We consider two flavors of recurrent functions: a conventional LSTM network (Figure 2(b)) and a stack of masked convolutions also known as the ByteNet decoder from Kalchbrenner et al. (2016) (Figure 2(c)).", "startOffset": 164, "endOffset": 191}, {"referenceID": 4, "context": "Note that the feed-forward part of our model is different from the existing fully convolutional approaches of Dauphin et al. (2016) and Kalchbrenner et al.", "startOffset": 110, "endOffset": 132}, {"referenceID": 4, "context": "Note that the feed-forward part of our model is different from the existing fully convolutional approaches of Dauphin et al. (2016) and Kalchbrenner et al. (2016) in two respects: firstly, while being fully parallelizable during training, these models still require predictions from previous time steps during inference and thus behave as a variant of recurrent networks.", "startOffset": 110, "endOffset": 163}, {"referenceID": 3, "context": "The addition of the recurrent component results in optimization difficulties that are similar to those described by Bowman et al. (2016). In most cases the model converges to a solution with a vanishingly small KL term, thus effectively falling back", "startOffset": 116, "endOffset": 137}, {"referenceID": 3, "context": "Bowman et al. (2016) have proposed to use input dropout and KL term annealing to encourage their model to encode meaningful representations into the z vector.", "startOffset": 0, "endOffset": 21}, {"referenceID": 14, "context": "All models were trained with the Adam optimization algorithm (Kingma and Ba, 2014) with decaying learning rate.", "startOffset": 61, "endOffset": 82}, {"referenceID": 3, "context": "training the LSTM VAE models from Bowman et al. (2016) and KL term annealing and regularized objective function from Eq (3) when training our models.", "startOffset": 34, "endOffset": 55}, {"referenceID": 0, "context": "Normalization (Ba et al., 2016) in LSTM layers and Batch Normalization (Ioffe and Szegedy, 2015) in convolutional and deconvolutional layers.", "startOffset": 14, "endOffset": 31}, {"referenceID": 11, "context": ", 2016) in LSTM layers and Batch Normalization (Ioffe and Szegedy, 2015) in convolutional and deconvolutional layers.", "startOffset": 47, "endOffset": 72}, {"referenceID": 18, "context": "Our first task is character-level language generation performed on the standard Penn Treebank dataset (Marcus et al., 1993).", "startOffset": 102, "endOffset": 123}, {"referenceID": 21, "context": "gradients (Pascanu et al., 2013), we have searched over learning rates, gradient clipping thresholds and sizes of LSTM layers but were only able to get results comparable to those shown in Figure 3.", "startOffset": 10, "endOffset": 32}, {"referenceID": 0, "context": "Note that LSTM networks make use of Layer Normalization (Ba et al., 2016) which has been shown to make training of such networks easier.", "startOffset": 56, "endOffset": 73}, {"referenceID": 3, "context": "A similar effect on model\u2019s bound was observed by Bowman et al. (2016): increased input dropout rates force their model to put more information into the z vector but at the cost of increased final loss values.", "startOffset": 50, "endOffset": 71}, {"referenceID": 13, "context": "periment with masked convolutions (Figure 2(c)), which is similar to the decoder in ByteNet model from Kalchbrenner et al. (2016). We fix the size of the convolutional kernels to 2 and do not use dilated convolutions and skip connections as in the original ByteNet.", "startOffset": 103, "endOffset": 130}], "year": 2017, "abstractText": "In this paper we explore the effect of architectural choices on learning a Variational Autoencoder (VAE) for text generation. In contrast to the previously introduced VAE model for text where both the encoder and decoder are RNNs, we propose a novel hybrid architecture that blends fully feed-forward convolutional and deconvolutional components with a recurrent language model. Our architecture exhibits several attractive properties such as faster run time and convergence, ability to better handle long sequences and, more importantly, it helps to avoid some of the major difficulties posed by training VAE models", "creator": "LaTeX with hyperref package"}}}