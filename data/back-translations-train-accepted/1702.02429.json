{"id": "1702.02429", "review": {"conference": "EMNLP", "VERSION": "v1", "DATE_OF_SUBMISSION": "8-Feb-2017", "title": "Trainable Greedy Decoding for Neural Machine Translation", "abstract": "Recent research in neural machine translation has largely focused on two aspects; neural network architectures and end-to-end learning algorithms. The problem of decoding, however, has received relatively little attention from the research community. In this paper, we solely focus on the problem of decoding given a trained neural machine translation model. Instead of trying to build a new decoding algorithm for any specific decoding objective, we propose the idea of trainable decoding algorithm in which we train a decoding algorithm to find a translation that maximizes an arbitrary decoding objective. More specifically, we design an actor that observes and manipulates the hidden state of the neural machine translation decoder and propose to train it using a variant of deterministic policy gradient. We extensively evaluate the proposed algorithm using four language pairs and two decoding objectives and show that we can indeed train a trainable greedy decoder that generates a better translation (in terms of a target decoding objective) with minimal computational overhead.", "histories": [["v1", "Wed, 8 Feb 2017 13:56:16 GMT  (868kb,D)", "http://arxiv.org/abs/1702.02429v1", "10 pages"]], "COMMENTS": "10 pages", "reviews": [], "SUBJECTS": "cs.CL cs.LG", "authors": ["jiatao gu", "kyunghyun cho", "victor o k li"], "accepted": true, "id": "1702.02429"}, "pdf": {"name": "1702.02429.pdf", "metadata": {"source": "CRF", "title": "Trainable Greedy Decoding for Neural Machine Translation", "authors": ["Jiatao Gu", "Kyunghyun Cho", "Victor O.K. Li"], "emails": ["vli}@eee.hku.hk", "kyunghyun.cho@nyu.edu"], "sections": [{"heading": "1 Introduction", "text": "In fact, most of them are able to survive on their own."}, {"heading": "2 Background", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "2.1 Neural Machine Translation", "text": "Let's take X = {x1,.., xTs} and Y = {y1,.., yT} to denote source and target sentences. Neural machine translation then models the target sentence with respect to the source sentence as: p (Y | X) = 1 p (yt | y < t, X). Each term in the above equation is modeled as a composite of two parametric functions: p (yt | y < t, X), p (yt, zt; \u03b8g), where zt = f (zt \u2212 1, yt \u2212 1, et (X; \u03b8e); p (c); g is a read function that transforms the hidden state zt into the distribution across all possible symbols. < and f is a recursive function that forces all attention to the previous sentence."}, {"heading": "2.2 Decoding", "text": "Once the model is trained, either by learning the maximum probability or by any other recently proposed algorithms (Wiseman and Rush, 2016; Shen et al., 2015; Bahdanau et al., 2016; Ranzato et al., 2015), we can have the model translate a particular sentence by finding a translation that maximizes Y = argmax Ylog p\u03b8 (Y | X), where such an approximate decoding algorithm is greedy decoding. However, in greedy decoding, we follow the path of conditional dependence and choose the symbol with the highest conditional probability so far on each node. This is equivalent to selecting the best symbol one by one in conditional language modeling. < A decrypted translation of the greedy decoding follows the path and selects the symbol with the highest conditional probability so far on each node."}, {"heading": "3 Trainable Greedy Decoding", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "3.1 Many Decoding Objectives", "text": "Although we have described neural machine translation decoding as a maximum a posteriori estimate in log p (Y | X), this is not necessarily the only or desirable objective decoding. First, any potential scenario in which neural machine translation is used requires a unique decoding target. On the other hand, when a machine translation system is used as part of a larger information extraction system, it is more important to translate named entities and events correctly than to translate syntactic function words. The decoding target in this case must take into account how the translation is likely to be used in subsequent modules."}, {"heading": "3.2 Trainable Greedy Decoding", "text": "The basic idea behind the NPAD algorithm is that a better translation can be found with a higher log probability by injecting unstructured noise into the transition function of a recursive network, that is, zt = f (zt \u2212 1 + t, yt \u2212 1, et (X; 2), where t-N (0, (0 / t) 2).NPAD avoids potential degradation of the translation quality by performing such a greedy decoding process several times in parallel. An important lesson of the NPAD algorithm is that there is a decoding strategy with asymptotically the same computational complexity that leads to better translation quality, and that such a better translation manipulates the hidden state of the recursive network."}, {"heading": "3.3 Learning and Challenges", "text": "While all the parameters of the underlying neural translation model are set, we merely update the parameters \u03c6 of the agent \u03c0. This ensures the universality of the pre-formed translation model and allows us to train multiple trainable greedy decryption agents with different decryption goals, maximizing the benefits of a single trained translation model. Let's call R our arbitrary decryption target a function that achieves a translation generated from traceable greedy decryption goals. Then, there are two major challenges in learning an agent with such a goal. First, the decoding target R [R] D [R (Y)]], where we used G\u03c0 (X) as an abbreviation for tradable greedy decryption with an agent. There are two major challenges in learning an agent with such a goal. First, the decoding target R may not be distinguishable from the agent."}, {"heading": "4 Deterministic Policy Gradient with Critic-Aware Actor Learning", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "4.1 Deterministic Policy Gradient for Trainable Greedy Decoding", "text": "It is highly unlikely that we have access to the gradient of an arbitrary deciphering object Y in relation to the agent \u03c0 or its parameters \u03c6. Furthermore, we cannot assess it stochastically, because our policy \u03c0 is defined as deterministic, without a predefined or learned distribution over the plot. Instead, following (Silver et al., 2014; Lillicrap et al., 2015), we use a parametric, differentiable critic called Critic Rc for the non-differentiable goal R. We train the critic by minimizingJC (eg) = EY-Episode = G\u03c0 (X) X-D [RcB: T) \u2212 R (Y)] -2. The critic observes the state action sequence of Agent Rc via the modified hidden states (z1, zT) of the recursive network and objectively predicts the associated decoding."}, {"heading": "4.2 Critic-Aware Actor Learning", "text": "The most obvious challenge for the training of such a deterministic actor with a large scope for action is that most action configurations will lead to zero return. (It is also not trivial to develop an efficient exploration strategy with a deterministic actor with reality-evaluated actions.) However, this problem has turned out to be less problematic than in a usual reinforcement learning environment, since the state and action spaces are well structured, thanks to pre-training by maximizing the likelihood of learning. (Association 1: Educable Greedy decision processes: NMT, actors, critics, Nc, Na, Sc, Sa, \u03c41: train that apply MLE to training. (2: training and working method). (3: Shuffle D twice in D.4: while stopping criterion is not met. (for t = 1: Nc do 6: drawing of a translation pair: (X, Y) (7: rDEc = CODE)."}, {"heading": "5 Experimental Settings", "text": "Empirically, we evaluate the proposed traceable greedy decoding using four language pairs - En-De, En-Ru, En-Cs and En-Fi - with a standard attention-based neural machine translation system (Bahdanau et al., 2014). We train the underlying neural translation systems using the parallel corpora provided by WMT '15.1. The same corpora are also used for traceable greedy decoding. All corpora are tokenized using byte pair encoding (BPE) and segmented into subword symbols (Sennrich et al., 2015)."}, {"heading": "5.1 Model Architectures and Learning", "text": "In fact, the majority of them are able to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to move, to move, to move, to move, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to move, to move, to move, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move."}, {"heading": "5.2 Results and Analysis", "text": "It is clear that the best result is achieved when the trainable greedy decoder has been trained to maximize the goal of decoding. If the decoder has been trained to maximize the lack of perspective, we see only the improvement in the perplexity, but often the degradation in the perplexity. This confirms our earlier assertion that it is necessary and desirable to achieve the goal of decoding. If the actor has been trained to minimize the perplexity, we only see the improvement in the perplexity."}, {"heading": "6 Conclusion", "text": "The proposed traceable greedy decoder observes and manipulates the hidden state of a trained neural translation system and is trained by a novel variant of the deterministic policy gradient, so-called critical-conscious actor learning. Our extensive experiments on eight language pairs and two objectives confirmed its validity and usefulness. The proposed traceable greedy decoding is a general idea that can be applied to all recurring language modeling, and we expect future research on both the basics of traceable decoding and the applications for more diverse tasks such as image labeling and dialogue modeling."}, {"heading": "Acknowledgement", "text": "KC would like to thank Facebook, Google (Google Faculty Award 2016) and NVidia (GPU Center of Excellence 2015-2016) for their support. We would like to thank Martin Arjovsky, Zihang Dai, Graham Neubig, Pengcheng Yin and Chunting Zhou for their helpful conversations and insightful feedback."}], "references": [], "referenceMentions": [], "year": 2017, "abstractText": "Recent research in neural machine translation has largely focused on two aspects; neural network architectures and end-toend learning algorithms. The problem of decoding, however, has received relatively little attention from the research community. In this paper, we solely focus on the problem of decoding given a trained neural machine translation model. Instead of trying to build a new decoding algorithm for any specific decoding objective, we propose the idea of trainable decoding algorithm in which we train a decoding algorithm to find a translation that maximizes an arbitrary decoding objective. More specifically, we design an actor that observes and manipulates the hidden state of the neural machine translation decoder and propose to train it using a variant of deterministic policy gradient. We extensively evaluate the proposed algorithm using four language pairs and two decoding objectives, and show that we can indeed train a trainable greedy decoder that generates a better translation (in terms of a target decoding objective) with minimal computational overhead.", "creator": "LaTeX with hyperref package"}}}