{"id": "1702.02535", "review": {"conference": "ACL", "VERSION": "v1", "DATE_OF_SUBMISSION": "8-Feb-2017", "title": "Exploiting Domain Knowledge via Grouped Weight Sharing with Application to Text Categorization", "abstract": "A fundamental advantage of neural models for NLP is their ability to learn representations from scratch. However, in practice this often means ignoring existing external linguistic resources, e.g., WordNet or domain specific ontologies such as the Unified Medical Language System (UMLS). We propose a general, novel method for exploiting such resources via weight sharing. Prior work on weight sharing in neural networks has considered it largely as a means of model compression. In contrast, we treat weight sharing as a flexible mechanism for incorporating prior knowledge into neural models. We show that this approach consistently yields improved performance on classification tasks compared to baseline strategies that do not exploit weight sharing.", "histories": [["v1", "Wed, 8 Feb 2017 17:30:51 GMT  (60kb,D)", "http://arxiv.org/abs/1702.02535v1", null], ["v2", "Wed, 19 Apr 2017 20:40:43 GMT  (61kb,D)", "http://arxiv.org/abs/1702.02535v2", "This paper is accepted by ACL 2017"], ["v3", "Tue, 25 Apr 2017 16:33:52 GMT  (61kb,D)", "http://arxiv.org/abs/1702.02535v3", "This paper is accepted by ACL 2017"]], "reviews": [], "SUBJECTS": "cs.CL", "authors": ["ye zhang", "matthew lease", "byron c wallace"], "accepted": true, "id": "1702.02535"}, "pdf": {"name": "1702.02535.pdf", "metadata": {"source": "CRF", "title": "Exploiting Domain Knowledge via Grouped Weight Sharing with Application to Text Categorization", "authors": ["Ye Zhang", "Matthew Lease", "Byron C. Wallace"], "emails": ["yezhang@utexas.edu,", "ml@utexas.edu,", "byron@ccs.neu.edu"], "sections": [{"heading": "1 Introduction", "text": "Neural models are powerful in part because of their ability to learn good representations of raw text input, which reduces the need for extensive task-specific feature engineering tasks (Collobert et al., 2011). However, one downside of learning from scratch is that we do not capitalize on previous linguistic or semantic knowledge, often encoded in existing resources such as ontologies. Such prior knowledge can be particularly valuable when evaluating highly flexible models. In this work, we look at the use of known relationships between words when we train neural models for NLP tasks. We suggest using the feature hashing trick that was originally proposed as a means of neural network compression (Chen et al., 2015). Here, instead, we look at the partial parameter distribution induced by feature hashing as a flexible mechanism for merging network node weights that we believe are similar to a."}, {"heading": "2 Grouped Weight Sharing", "text": "In fact, it is such that most of us will be able to move into a different world, in which they are able to understand the world, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they, in which they live, in which they, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live."}, {"heading": "3 Experimental Setup", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "3.1 Datasets", "text": "We use three sentiment datasets: a film review dataset (MR) (Pang and Lee, 2005) 2; a customer review dataset (CR) (Hu and Liu, 2004) 3; and an opinion dataset (MPQA) (Wiebe et al., 2005) 4. We also use four biomedical datasets relating to systematic review, with the task of classifying published articles describing clinical trials as relevant or not relevant to a well-specified clinical question. Articles deemed relevant are included in the corresponding review, which provides a synthesis of all relevant evidence (Wallace et al., 2010). We use data from reviews relating to the following: clopidogrel (CL) for cardiovascular disease (Dahabreh et al., 2013); biomarkers for assessing iron deficiency in anemia (AN) in patients with kidney disease (Chung et al., 2012 Statin; (Cohen) (et)."}, {"heading": "3.2 Implementation Details and Baselines", "text": "We use SentiWordNet (Baccianella et al., 2010) 5 for the sentiment tasks. SentiWordNet assigns three sentiment scores to each synset of WordNet: positivity, negativity and objectivity, limited to the sum of 1. We only keep the synsets with positivity or negativity scores greater than 0, i.e., we remove synsets as a target. The synsets in SentiWordNet constitute our groups. We also use the Brown Clustering Algorithm6 on the three sentiment datasets. We generate 1000 clusters and treat each as a group. For the biomedical datasets, we use the Medical Subject Headings (MeSH) Terms 7 to classify them. Each MeSH term has a tree number indicating the path from the root in the UMLS. For example, \"Alagille syndrome\" has tree number \"C06,552,150,125; rows of tree splits we are."}, {"heading": "4 Results", "text": "We report on results (averages of 10-fold cross-validation) on mood and biomedical corpora in Tables 211 and 3, respectively, which use different external resources to induce the word groups, which in turn influence weight distribution. We report AUC for the biomedical datasets, as they are highly unbalanced (see Table 1). Our method improves performance relative to all relevant baselines (including an approach that also utilizes external knowledge through retrofitting) in six out of seven cases. Information about weight initialization with external resources improves performance independently, but additional gains are made by also forcing sharing during training. We note that our goal here is not necessarily to obtain current results from a given dataset, 10bio.nlplab.org / 11Sentiment task results are not directly comparable toprior work due to different preprocessing steps. Rather, we evaluate the proposed method of incorporating external resources via neural models."}, {"heading": "5 Related Work", "text": "Recently, there has been a huge interest in neural models for NLP in general (Collobert et al., 2011; Goldberg, 2016). Most relevant to this work are simple CNN-based models (on which we are building here) that have proven highly effective for text categorization (Kim, 2014; Zhang and Wallace, 2015); the exploitation of linguistic resources; a potential disadvantage of learning from the ground up in holistic neural models is the failure to capitalize on existing knowledge sources; efforts have been made to target these resources to induce better word vectors (Yu and Dredze, 2014; Faruqui et al., 2014; Yu et al., 2016; Xu et al., 2014); but these models do not attempt to exploit external resources collectively during training for a specific downstream task (which uses word embedding as inputs)."}, {"heading": "6 Conclusions", "text": "We have proposed a new method to integrate earlier semantic knowledge into neural models using stochastic weight distribution. We have shown that this improves text classification performance in general, compared to model variants that do not exhaust external resources, and with an approach based on pre-training retrofitting. In future work, we hope to generalize the approach beyond classification tasks and inform weight distribution using other variants and sources of linguistic knowledge."}], "references": [{"title": "Sentiwordnet 3.0: An enhanced lexical resource for sentiment analysis and opinion mining", "author": ["Stefano Baccianella", "Andrea Esuli", "Fabrizio Sebastiani"], "venue": "In LREC", "citeRegEx": "Baccianella et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Baccianella et al\\.", "year": 2010}, {"title": "The unified medical language system (umls): integrating biomedical terminology", "author": ["Olivier Bodenreider."], "venue": "Nucleic acids research 32(suppl 1):D267\u2013 D270.", "citeRegEx": "Bodenreider.,? 2004", "shortCiteRegEx": "Bodenreider.", "year": 2004}, {"title": "Class-based n-gram models of natural language", "author": ["Peter F Brown", "Peter V Desouza", "Robert L Mercer", "Vincent J Della Pietra", "Jenifer C Lai."], "venue": "Computational linguistics 18(4):467\u2013479.", "citeRegEx": "Brown et al\\.,? 1992", "shortCiteRegEx": "Brown et al\\.", "year": 1992}, {"title": "Compressing neural networks with the hashing trick", "author": ["Wenlin Chen", "James T Wilson", "Stephen Tyree", "Kilian Q Weinberger", "Yixin Chen."], "venue": "ICML. pages 2285\u20132294.", "citeRegEx": "Chen et al\\.,? 2015", "shortCiteRegEx": "Chen et al\\.", "year": 2015}, {"title": "Biomarkers for assessing and managing iron deficiency anemia in late-stage chronic kidney disease", "author": ["Mei Chung", "Denish Moorthy", "Nira Hadar", "Priyanka Salvi", "Ramon C Iovin", "Joseph Lau"], "venue": null, "citeRegEx": "Chung et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Chung et al\\.", "year": 2012}, {"title": "Reducing workload in systematic review preparation using automated citation classification", "author": ["Aaron M Cohen", "William R Hersh", "K Peterson", "PoYin Yen."], "venue": "Journal of the American Medical Informatics Association 13(2):206\u2013219.", "citeRegEx": "Cohen et al\\.,? 2006", "shortCiteRegEx": "Cohen et al\\.", "year": 2006}, {"title": "Natural language processing (almost) from scratch", "author": ["Ronan Collobert", "Jason Weston", "L\u00e9on Bottou", "Michael Karlen", "Koray Kavukcuoglu", "Pavel Kuksa."], "venue": "Journal of Machine Learning Research 12(Aug):2493\u20132537.", "citeRegEx": "Collobert et al\\.,? 2011", "shortCiteRegEx": "Collobert et al\\.", "year": 2011}, {"title": "Testing of cyp2c19 variants and platelet reactivity for guiding antiplatelet treatment", "author": ["Issa J Dahabreh", "Denish Moorthy", "Jenny L Lamont", "Minghua L Chen", "David M Kent", "Joseph Lau"], "venue": null, "citeRegEx": "Dahabreh et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Dahabreh et al\\.", "year": 2013}, {"title": "Graph-sparse lda: A topic model with structured sparsity", "author": ["Finale Doshi-Velez", "Byron C Wallace", "Ryan Adams."], "venue": "AAAI Conference on Artificial Intelligence. pages 2575\u20132581.", "citeRegEx": "Doshi.Velez et al\\.,? 2015", "shortCiteRegEx": "Doshi.Velez et al\\.", "year": 2015}, {"title": "Retrofitting word vectors to semantic lexicons", "author": ["Manaal Faruqui", "Jesse Dodge", "Sujay K Jauhar", "Chris Dyer", "Eduard Hovy", "Noah A Smith."], "venue": "arXiv preprint arXiv:1411.4166 .", "citeRegEx": "Faruqui et al\\.,? 2014", "shortCiteRegEx": "Faruqui et al\\.", "year": 2014}, {"title": "A primer on neural network models for natural language processing", "author": ["Yoav Goldberg."], "venue": "Journal of Artificial Intelligence Research 57:345\u2013420.", "citeRegEx": "Goldberg.,? 2016", "shortCiteRegEx": "Goldberg.", "year": 2016}, {"title": "Mining and summarizing customer reviews", "author": ["Minqing Hu", "Bing Liu."], "venue": "Proceedings of the tenth ACM SIGKDD international conference on Knowledge discovery and data mining. ACM, pages 168\u2013 177.", "citeRegEx": "Hu and Liu.,? 2004", "shortCiteRegEx": "Hu and Liu.", "year": 2004}, {"title": "Convolutional neural networks for sentence classification", "author": ["Yoon Kim."], "venue": "arXiv preprint arXiv:1408.5882 .", "citeRegEx": "Kim.,? 2014", "shortCiteRegEx": "Kim.", "year": 2014}, {"title": "Efficient estimation of word representations in vector space", "author": ["Tomas Mikolov", "Kai Chen", "Greg Corrado", "Jeffrey Dean."], "venue": "arXiv preprint arXiv:1301.3781 .", "citeRegEx": "Mikolov et al\\.,? 2013", "shortCiteRegEx": "Mikolov et al\\.", "year": 2013}, {"title": "Wordnet: a lexical database for english", "author": ["George A Miller."], "venue": "Communications of the ACM 38(11):39\u2013", "citeRegEx": "Miller.,? 1995", "shortCiteRegEx": "Miller.", "year": 1995}, {"title": "Distributional semantics resources for biomedical text processing", "author": ["SPFGH Moen"], "venue": null, "citeRegEx": "Moen and Ananiadou.,? \\Q2013\\E", "shortCiteRegEx": "Moen and Ananiadou.", "year": 2013}, {"title": "Seeing stars: Exploiting class relationships for sentiment categorization with respect to rating scales", "author": ["Bo Pang", "Lillian Lee."], "venue": "Proceedings of the ACL.", "citeRegEx": "Pang and Lee.,? 2005", "shortCiteRegEx": "Pang and Lee.", "year": 2005}, {"title": "Learning representations by backpropagating errors", "author": ["D.E. Rumelhart", "G.E. Hintont", "R.J. Williams."], "venue": "Nature 323(6088):533\u2013536.", "citeRegEx": "Rumelhart et al\\.,? 1986", "shortCiteRegEx": "Rumelhart et al\\.", "year": 1986}, {"title": "Hash kernels for structured data", "author": ["Qinfeng Shi", "James Petterson", "Gideon Dror", "John Langford", "Alex Smola", "SVN Vishwanathan."], "venue": "Journal of Machine Learning Research 10(Nov):2615\u20132637.", "citeRegEx": "Shi et al\\.,? 2009", "shortCiteRegEx": "Shi et al\\.", "year": 2009}, {"title": "Charged Particle Radiation Therapy for Cancer: A Systematic Review", "author": ["T. Terasawa", "T. Dvorak", "S. Ip", "G. Raman", "J. Lau", "T.A. Trikalinos."], "venue": "Ann. Intern. Med. .", "citeRegEx": "Terasawa et al\\.,? 2009", "shortCiteRegEx": "Terasawa et al\\.", "year": 2009}, {"title": "Semi-automated screening of biomedical citations for systematic reviews", "author": ["Byron C Wallace", "Thomas A Trikalinos", "Joseph Lau", "Carla Brodley", "Christopher H Schmid."], "venue": "BMC bioinformatics 11(1):55.", "citeRegEx": "Wallace et al\\.,? 2010", "shortCiteRegEx": "Wallace et al\\.", "year": 2010}, {"title": "Feature hashing for large scale multitask learning", "author": ["Kilian Weinberger", "Anirban Dasgupta", "John Langford", "Alex Smola", "Josh Attenberg."], "venue": "Proceedings of the 26th Annual International Conference on Machine Learning. ACM, pages 1113\u2013", "citeRegEx": "Weinberger et al\\.,? 2009", "shortCiteRegEx": "Weinberger et al\\.", "year": 2009}, {"title": "Annotating expressions of opinions and emotions in language", "author": ["Janyce Wiebe", "Theresa Wilson", "Claire Cardie."], "venue": "Language resources and evaluation 39(2):165\u2013210.", "citeRegEx": "Wiebe et al\\.,? 2005", "shortCiteRegEx": "Wiebe et al\\.", "year": 2005}, {"title": "Rcnet: A general framework for incorporating knowledge into word representations", "author": ["Chang Xu", "Yalong Bai", "Jiang Bian", "Bin Gao", "Gang Wang", "Xiaoguang Liu", "Tie-Yan Liu."], "venue": "Proceedings of the 23rd ACM International Conference on Confer-", "citeRegEx": "Xu et al\\.,? 2014", "shortCiteRegEx": "Xu et al\\.", "year": 2014}, {"title": "Linguistic structured sparsity in text categorization", "author": ["Dani Yogatama", "Noah A Smith."], "venue": "Meeting of the Association for Computational Linguistics. pages 786\u2013796.", "citeRegEx": "Yogatama and Smith.,? 2014", "shortCiteRegEx": "Yogatama and Smith.", "year": 2014}, {"title": "Improving lexical embeddings with semantic knowledge", "author": ["Mo Yu", "Mark Dredze."], "venue": "ACL. pages 545\u2013550.", "citeRegEx": "Yu and Dredze.,? 2014", "shortCiteRegEx": "Yu and Dredze.", "year": 2014}, {"title": "Retrofitting word vectors of mesh terms to improve semantic similarity measures", "author": ["Zhiguo Yu", "Trevor Cohen", "Elmer V Bernstam", "Byron C Wallace."], "venue": "International Workshop on Health Text Mining and Information Analysis at EMNLP pages 43\u2013", "citeRegEx": "Yu et al\\.,? 2016", "shortCiteRegEx": "Yu et al\\.", "year": 2016}, {"title": "Adadelta: An adaptive learning rate method", "author": ["Matthew D. Zeiler"], "venue": null, "citeRegEx": "Zeiler.,? \\Q2012\\E", "shortCiteRegEx": "Zeiler.", "year": 2012}, {"title": "MGNC-CNN: A simple approach to exploiting multiple word embeddings for sentence classification pages 1522\u20131527", "author": ["Ye Zhang", "Stephen Roller", "Byron C Wallace"], "venue": null, "citeRegEx": "Zhang et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Zhang et al\\.", "year": 2016}, {"title": "A sensitivity analysis of (and practitioners\u2019 guide to) convolutional neural networks for sentence classification", "author": ["Ye Zhang", "Byron C Wallace."], "venue": "arXiv preprint arXiv:1510.03820 .", "citeRegEx": "Zhang and Wallace.,? 2015", "shortCiteRegEx": "Zhang and Wallace.", "year": 2015}], "referenceMentions": [{"referenceID": 6, "context": "Neural models are powerful in part due to their ability to learn good representations of raw textual inputs, mitigating the need for extensive task-specific feature engineering (Collobert et al., 2011).", "startOffset": 177, "endOffset": 201}, {"referenceID": 3, "context": "We propose exploiting the feature-hashing trick, originally proposed as a means of neural network compression (Chen et al., 2015).", "startOffset": 110, "endOffset": 129}, {"referenceID": 2, "context": "More specifically, as external resources we use Brown clusters (Brown et al., 1992), WordNet (Miller, 1995) and the Unified Medical Language System (UMLS) (Bodenreider, 2004).", "startOffset": 63, "endOffset": 83}, {"referenceID": 14, "context": ", 1992), WordNet (Miller, 1995) and the Unified Medical Language System (UMLS) (Bodenreider, 2004).", "startOffset": 17, "endOffset": 31}, {"referenceID": 1, "context": ", 1992), WordNet (Miller, 1995) and the Unified Medical Language System (UMLS) (Bodenreider, 2004).", "startOffset": 79, "endOffset": 98}, {"referenceID": 9, "context": "Results on seven diverse classification tasks (three sentiment and four biomedical) show that our method consistently improves performance over (1) baselines that fail to capitalize on domain knowledge, and (2) an approach that uses retrofitting (Faruqui et al., 2014) as a preprocessing step to encode doar X iv :1 70 2.", "startOffset": 246, "endOffset": 268}, {"referenceID": 28, "context": "To exploit both grouped and independent word weights, we adopt a two-channel CNN model (Zhang et al., 2016).", "startOffset": 87, "endOffset": 107}, {"referenceID": 21, "context": "Following (Weinberger et al., 2009; Shi et al., 2009), we use a second hash function b to remove bias induced by hashing.", "startOffset": 10, "endOffset": 53}, {"referenceID": 18, "context": "Following (Weinberger et al., 2009; Shi et al., 2009), we use a second hash function b to remove bias induced by hashing.", "startOffset": 10, "endOffset": 53}, {"referenceID": 17, "context": "During training, we update Ep as usual using back-propagation (Rumelhart et al., 1986).", "startOffset": 62, "endOffset": 86}, {"referenceID": 3, "context": "We update Es and group embeddings g in a manner similar to Chen et al. (2015). In the forward propagation before each training step (mini-batch), we derive the value of ei,j from g:", "startOffset": 59, "endOffset": 78}, {"referenceID": 16, "context": "We use three sentiment datasets: a movie review (MR) dataset (Pang and Lee, 2005)2; a customer review (CR) dataset (Hu and Liu, 2004)3; and an opinion dataset (MPQA) (Wiebe et al.", "startOffset": 61, "endOffset": 81}, {"referenceID": 11, "context": "We use three sentiment datasets: a movie review (MR) dataset (Pang and Lee, 2005)2; a customer review (CR) dataset (Hu and Liu, 2004)3; and an opinion dataset (MPQA) (Wiebe et al.", "startOffset": 115, "endOffset": 133}, {"referenceID": 22, "context": "We use three sentiment datasets: a movie review (MR) dataset (Pang and Lee, 2005)2; a customer review (CR) dataset (Hu and Liu, 2004)3; and an opinion dataset (MPQA) (Wiebe et al., 2005)4.", "startOffset": 166, "endOffset": 186}, {"referenceID": 20, "context": "Articles deemed relevant are included in the corresponding review, which is a synthesis of all pertinent evidence (Wallace et al., 2010).", "startOffset": 114, "endOffset": 136}, {"referenceID": 7, "context": "We use data from reviews that concerned: clopidogrel (CL) for cardiovascular conditions (Dahabreh et al., 2013); biomarkers for assessing iron deficiency in anemia (AN) experienced by patients with kidney disease (Chung et al.", "startOffset": 88, "endOffset": 111}, {"referenceID": 4, "context": ", 2013); biomarkers for assessing iron deficiency in anemia (AN) experienced by patients with kidney disease (Chung et al., 2012); statins (ST) (Cohen et al.", "startOffset": 109, "endOffset": 129}, {"referenceID": 5, "context": ", 2012); statins (ST) (Cohen et al., 2006); and proton beam (PB) therapy (Terasawa et al.", "startOffset": 22, "endOffset": 42}, {"referenceID": 19, "context": ", 2006); and proton beam (PB) therapy (Terasawa et al., 2009).", "startOffset": 38, "endOffset": 61}, {"referenceID": 0, "context": "We use SentiWordNet (Baccianella et al., 2010)5 for the sentiment tasks.", "startOffset": 20, "endOffset": 46}, {"referenceID": 9, "context": "All use pre-trained embeddings to initialize Ep, but we explore several approaches to exploiting Es: (1) randomly initialize Es; (2) initialize Es to reflect the group embedding g, but do not share weights thereafter; (3) use the linguistic resources to retro-fit (Faruqui et al., 2014) the pre-trained embeddings, and use these to initialize Es.", "startOffset": 264, "endOffset": 286}, {"referenceID": 13, "context": "For the sentiment datasets, we use Google word2vec (Mikolov et al., 2013)9 to", "startOffset": 51, "endOffset": 73}, {"referenceID": 15, "context": "word2vec trained on biomedical texts (Moen and Ananiadou, 2013)10 to initialize Ep.", "startOffset": 37, "endOffset": 63}, {"referenceID": 27, "context": "For parameter estimation, we use Adadelta (Zeiler, 2012).", "startOffset": 42, "endOffset": 56}, {"referenceID": 6, "context": "Recently there has been enormous interest in neural models for NLP generally (Collobert et al., 2011; Goldberg, 2016).", "startOffset": 77, "endOffset": 117}, {"referenceID": 10, "context": "Recently there has been enormous interest in neural models for NLP generally (Collobert et al., 2011; Goldberg, 2016).", "startOffset": 77, "endOffset": 117}, {"referenceID": 12, "context": "els (which we have built on here) have proven extremely effective for text categorization (Kim, 2014; Zhang and Wallace, 2015).", "startOffset": 90, "endOffset": 126}, {"referenceID": 29, "context": "els (which we have built on here) have proven extremely effective for text categorization (Kim, 2014; Zhang and Wallace, 2015).", "startOffset": 90, "endOffset": 126}, {"referenceID": 25, "context": "There have been efforts to exploit such resources specifically to induce better word vectors (Yu and Dredze, 2014; Faruqui et al., 2014; Yu et al., 2016; Xu et al., 2014).", "startOffset": 93, "endOffset": 170}, {"referenceID": 9, "context": "There have been efforts to exploit such resources specifically to induce better word vectors (Yu and Dredze, 2014; Faruqui et al., 2014; Yu et al., 2016; Xu et al., 2014).", "startOffset": 93, "endOffset": 170}, {"referenceID": 26, "context": "There have been efforts to exploit such resources specifically to induce better word vectors (Yu and Dredze, 2014; Faruqui et al., 2014; Yu et al., 2016; Xu et al., 2014).", "startOffset": 93, "endOffset": 170}, {"referenceID": 23, "context": "There have been efforts to exploit such resources specifically to induce better word vectors (Yu and Dredze, 2014; Faruqui et al., 2014; Yu et al., 2016; Xu et al., 2014).", "startOffset": 93, "endOffset": 170}, {"referenceID": 22, "context": "For example, Yogatama and Smith (2014) used external resources to inform structured, grouped regularization of loglinear text classification models, yielding improvements over standard regularization approaches.", "startOffset": 13, "endOffset": 39}, {"referenceID": 7, "context": "Elsewhere, Doshi-Velez et al. (2015) proposed a variant of LDA that exploits a priori known treestructured relations between tokens (e.", "startOffset": 11, "endOffset": 37}, {"referenceID": 3, "context": "Notably, Chen et al. (2015) proposed randomly sharing weights in neural networks.", "startOffset": 9, "endOffset": 28}], "year": 2017, "abstractText": "A fundamental advantage of neural models for NLP is their ability to learn representations from scratch. However, in practice this often means ignoring existing external linguistic resources, e.g., WordNet or domain specific ontologies such as the Unified Medical Language System (UMLS). We propose a general, novel method for exploiting such resources via weight sharing. Prior work on weight sharing in neural networks has considered it largely as a means of model compression. In contrast, we treat weight sharing as a flexible mechanism for incorporating prior knowledge into neural models. We show that this approach consistently yields improved performance on classification tasks compared to baseline strategies that do not exploit weight sharing.", "creator": "LaTeX with hyperref package"}}}