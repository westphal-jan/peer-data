{"id": "1702.02661", "review": {"conference": "AAAI", "VERSION": "v1", "DATE_OF_SUBMISSION": "9-Feb-2017", "title": "Inductive Pairwise Ranking: Going Beyond the n log(n) Barrier", "abstract": "We study the problem of ranking a set of items from nonactively chosen pairwise preferences where each item has feature information with it. We propose and characterize a very broad class of preference matrices giving rise to the Feature Low Rank (FLR) model, which subsumes several models ranging from the classic Bradley-Terry-Luce (BTL) (Bradley and Terry 1952) and Thurstone (Thurstone 1927) models to the recently proposed blade-chest (Chen and Joachims 2016) and generic low-rank preference (Rajkumar and Agarwal 2016) models. We use the technique of matrix completion in the presence of side information to develop the Inductive Pairwise Ranking (IPR) algorithm that provably learns a good ranking under the FLR model, in a sample-efficient manner. In practice, through systematic synthetic simulations, we confirm our theoretical findings regarding improvements in the sample complexity due to the use of feature information. Moreover, on popular real-world preference learning datasets, with as less as 10% sampling of the pairwise comparisons, our method recovers a good ranking.", "histories": [["v1", "Thu, 9 Feb 2017 00:17:39 GMT  (771kb,D)", "http://arxiv.org/abs/1702.02661v1", null]], "reviews": [], "SUBJECTS": "cs.LG cs.IT math.IT stat.ML", "authors": ["u n niranjan", "arun rajkumar"], "accepted": true, "id": "1702.02661"}, "pdf": {"name": "1702.02661.pdf", "metadata": {"source": "CRF", "title": "Inductive Pairwise Ranking: Going Beyond the n log(n) Barrier", "authors": ["U.N. Niranjan", "Arun Rajkumar"], "emails": ["un.niranjan@uci.edu", "arun.rajkumar@xerox.com"], "sections": [{"heading": "Introduction", "text": "In fact, the fact is that most people are able to decide whether they want to be able or not, will be able to play by the rules."}, {"heading": "Related Work and Background", "text": "It is a question of the extent to which it is actually a matter of a way in which people are able to survive themselves. (...) It is a question of the extent to which people are able to survive themselves. (...) It is a question of the extent to which they are able to survive themselves. (...) It is a question of the extent to which they are able to survive themselves. (...) It is a question of the extent to which they are able to survive themselves. (...) It is a question of the extent to which they are able to survive themselves. (...) It is a question of the extent to which they are able to survive themselves. (...)"}, {"heading": "Feature-aware Ranking", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "Notation and Preliminaries", "text": "Unless we use lowercase letters for scalars, uppercase letters for universal constants, lowercase letters for vectors, and uppercase letters for matrices."}, {"heading": "Feature Low Rank Model", "text": "The underlying commonality in these models is that the difference between two independent standard variables follows the distribution."}, {"heading": "Problem Setup and Solution Approach", "text": "Once we have the generative ranking model developed in the previous section, the goal in our learning problem is to find the permutation of n items that minimizes the number of violations related to the true underlying preference matrix P, i.e. to find the best ranking of n items in the sense that each user with index k assigns the paired comparison data set S = {(i, j, ykij)}, which consists of comparison results of pairs (i, j) from a survey of K users in which each user with index k assigns ykij = 1 if he prefers i j and y k ij = 0 if he prefers j i. Note that it is not necessary to compare all itemaries; our algorithm is able to process loud and incomplete data. Since the true preference matrix P is unknown, our algorithm instead proceeds using the empirical preference matrix further available from the P."}, {"heading": "Algorithm", "text": "We present our main inductive ranking algorithm in algorithm 3. Input data consists of a set of paired comparison results S = (i, j, {ykij}), (i, j), (n], k [K], ykij [0, 1} and the attribute matrix F-Rd \u00b7 n. The algorithm assumes the link function and rank as input parameters. The subroutines used are: 1. Noisy matrix completion with characteristics (subroutine 1): Note that in order to solve our ranking problem and derive the associated recovery guarantee, it is sufficient, as we have done, to use the specified trace standard program as a black box method; therefore, we assume that we have access to an oracle that gives us the solution for the convex program."}, {"heading": "Analysis", "text": "In this area, we are able to verify and substantiate our main results. (...) In this area, we are able to hide ourselves. \"(...) (...) (...) (...) (...) (...) (...) (...) (...) (...) (...) (...) (...) (...) (...) (...) (...) (...) (...) (...) (...) (...) (...) (...) (...) (...) (...) (...) (...) (...) (...) (...) (...) (...) (...) (...) (...) (...) (...) (...) (...) (...) (...) (...) (...) (...) (... (...) (...) (...) (...). (...) (...) (...) (...). (...) (...). (...) (...). (... (...). (...) (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...).). (...). (...). (...).). (...). (...). (...).). (...). (...).). (...).). (...).). (...). (...).). (...). (...).).). (...).). (...). (...).). (...).). (...). (...). (...).). (...).). (...).). (...). (...). (...).).). (...). (...).). (...). (...). (...).).). (...).). (...).). (...). (...). (...).). (...). (...).). (...).). (...).). (...)."}, {"heading": "Experimental Results", "text": "In this section, we conduct a systematic empirical investigation of the performance of our ranking method and justify our theoretical claim in the previous section. The aim of this study is to achieve two things: (a) verify the accuracy of our algorithm and (b) demonstrate that our IPR algorithm has a better sampling complexity through the use of features and feature correlations, thus improving the LRPR algorithm that does not take into account the available ancillary information."}, {"heading": "Synthetic Simulations", "text": "For a given set of n = 500 items, we consider three main problem parameters: (1) m - the number of item pairs compared (Figure 1), (2) K - the number of comparisons per pair (Figure 2), (3) d - the dimensionality of the characteristics (Figure 3). We examine the performance of both the IPR and LRPR algorithms by varying each of the problem parameters while determining the others. We find that IPR exceeds LRPR by using page information in all cases, as shown in the sample complexity diagrams. All presented accuracy results are achieved by averaging over five rounds. Data Generation: We look at three representative preference matrices derived from Equation (1): (a) Model-1: we set W = 0, (b) Model-2: we construct the Model-2: we construct a general W; we generate an IJ-1, we generate an IU-1, we all (IU)."}, {"heading": "Real-data Simulations", "text": "We apply our method to two popular preference learning datasets. We briefly describe the data and results (Figure 4) obtained below: 1. Sushi: These data (Kamishima and Akaho 2009) come from a survey of 5,000 customers. Each customer orders 10 sushi dishes according to their preferences, and the goal is then to estimate a global ranking of these sushi dishes based on these observations of customers. Each sushi has six characteristics such as price, taste, etc. We construct the full preference matrix P [0, 1] 10 \u00d7 10 using the preferences of all customers and consider this to be a preference matrix for truth. An interesting observation was that over five runs of the algorithms, the IPR receives two of the four best sushi dishes directly from most times, namely \"amaebi\" and \"ikura.\""}, {"heading": "Discussion and Future Directions", "text": "In this paper, we proposed and characterized the FLR model together with the IPR Guaranteed Algorithm, which uses available page information of the items to be evaluated to demonstrably reduce the sampling complexity for ranking from \u0430 (n log n) to as low as possible (log n).One future research direction is to determine whether ranking blending models such as the recently proposed topic modeling approach (Ding, Ishwar and Saligrama 2015) could fit into our framework, while allowing sample-efficient estimation algorithms."}, {"heading": "Proof of Proposition 1", "text": "Proof: We prove this by showing that any P-Pn model in Pn-Pn-Pn-Pn-Pn-Pr-Pr-Pr-Pn-Pr-Pr-Pr-Pn-Pn-Pn-Pn-Pn-Pn-Pn-Pn-Pn-Pn-Pn-Pn-Pn-Pn-Pn-Pn-Pn-Pn-Pn-Pn-Pn-Pn-Pn-Pn-n-Pn-n-Pn-n-Pn-n-Pn-n-Pn-Pn-Pn-Pn-Pn-Pn-Pn-Pn-Pn-Pn-Pn-Pn-Pn-Pn-Pn-Pn-Pn-Pn-Pn-Pn-Pn-Pn-Pn-Pn-Pn-Pn-Pn-Pn-Pn-Pn-Pn-Pn-n-Pn-Pn-Pn-Pn-Pn-Pn-Pn-Pn-Pn-n-Pn-Pn-n-Pn-Pn-Pn-Pn-Pn-Pn-Pn-Pn-Pn-Pn-Pn-Pn-Pn-Pn-Pn-Pn-Pn-Pn-Pn-Pn-Pn-Pn-Pn-Pn-Pn-Pn-Pn-Pn-Pn-Pn-Pn-Pn-Pn-Pn-Pn-Pn-Pn-Pn-Pn-Pn-n-Pn-Pn-n-n-Pn-Pn-Pn-Pn-n-Pn-Pn-n-Pn-Pn-Pn-Pn-Pn-n-Pn-n-Pn-Pn-Pn-n-n-n-Pn-Pn-Pn-Pn-Pn-Pn-Pn-Pn-Pn-n-Pn-Pn-Pn-"}, {"heading": "Proof of Proposition 2", "text": "Proof. Let w be the simple score vector in RU models. Then, the result follows by setting the energy function of item i in relation to item j in the FLR model so that it corresponds to item i in the RU model, i.e. by simply setting F = I and W = 0, which leads to Eij = wi."}, {"heading": "Proof of Proposition 3", "text": "If we put W = 0 in Equation 1, we get Pij = e \u2212 w > fie \u2212 w > fi + e \u2212 w > fjNote that Pij = w > fj \u2212 w > fi is a rank-2-skew-symmetrical matrix. If we write this in matrix notation, it should be noted that this is a rank-2-skew-symmetrical matrix. Note L: = (V) > (P) = (P) = U > (1w > \u2212 w1 >) U. Now, let us note that L'Rd \u00b7 d is also a rank-2-skew-symmetrical matrix."}, {"heading": "Proof of Lemma 1", "text": "Proof: For each support, you define the following event: GJ: = (1) (1) (1) (1) (1) (1) (2) (1) (2) (2) (1) (2) (2) (2) (2) (2) (2) (2) (2) (2) (2) (2) (2) (2) (2) (2) (2) (2) (2) (2) (2) (2) (2) (2) (2) (2) (2) (2) (2) (2) (2) (2) (2) (2) (2) (2) (2) (2) (2) (2) (2) (2) (2) (2) (2) (2) (2) (2) (2) (2) (2) (2) (2) (2) (2) (2) (2) (2) (2) (2) (2) (2) (2) (2) (2) (2) (2) (2) (2) (2) (2) (2) (2) (2) (2) (2) (2) (2) (2) (2) (2) (2) (2) (2) (2) (2) (2) (2) (2) (2) (2) (2) (2) (2) (2) (2) (2) (2) (2) (2) (2) (2) (2) (2) (2) (2) (2) (2) (2) (2) (2 (2) (2) (2) (2) (2) (2) (2) (2) (2) (2 (2) (2) (2) (2) (2) (2) (2 (2) (2) (2) (2) (2) (2 (2) (2) (2) (2) (2) (2 (2) (2) (2) (2) (2) (2) (2) (2) (2 (2) (2) (2) (2) (2) (2) (2) ("}], "references": [{"title": "E", "author": ["E. Abbasnejad", "S. Sanner", "Bonilla"], "venue": "V.; Poupart, P.; et al.", "citeRegEx": "Abbasnejad et al. 2013", "shortCiteRegEx": null, "year": 2013}, {"title": "M", "author": ["R.A. Bradley", "Terry"], "venue": "E.", "citeRegEx": "Bradley and Terry 1952", "shortCiteRegEx": null, "year": 1952}, {"title": "and Recht", "author": ["E.J. Cand\u00e8s"], "venue": "B.", "citeRegEx": "Cand\u00e8s and Recht 2009", "shortCiteRegEx": null, "year": 2009}, {"title": "E", "author": ["Cand\u00e8s"], "venue": "J.; Li, X.; Ma, Y.; and Wright, J.", "citeRegEx": "Cand\u00e8s et al. 2011", "shortCiteRegEx": null, "year": 2011}, {"title": "A", "author": ["Caragiannis, I.", "Procaccia"], "venue": "D.; and Shah, N.", "citeRegEx": "Caragiannis. Procaccia. and Shah 2013", "shortCiteRegEx": null, "year": 2013}, {"title": "and Joachims", "author": ["S. Chen"], "venue": "T.", "citeRegEx": "Chen and Joachims 2016", "shortCiteRegEx": null, "year": 2016}, {"title": "I", "author": ["K.-Y. Chiang", "C.J. Hsieh", "Dhillon"], "venue": "S.", "citeRegEx": "Chiang. Hsieh. and Dhillon 2015", "shortCiteRegEx": null, "year": 2015}, {"title": "A", "author": ["Copeland"], "venue": "H.", "citeRegEx": "Copeland 1951", "shortCiteRegEx": null, "year": 1951}, {"title": "Ordering by weighted number of wins gives a good ranking for weighted tournaments", "author": ["Fleischer Coppersmith", "D. Rudra 2006] Coppersmith", "L. Fleischer", "A. Rudra"], "venue": "In Proceedings of the seventeenth annual ACM-SIAM symposium on Discrete algorithm,", "citeRegEx": "Coppersmith et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Coppersmith et al\\.", "year": 2006}, {"title": "A topic modeling approach to ranking", "author": ["Ishwar Ding", "W. Saligrama 2015] Ding", "P. Ishwar", "V. Saligrama"], "venue": "In AISTATS", "citeRegEx": "Ding et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Ding et al\\.", "year": 2015}, {"title": "I", "author": ["P. Jain", "Dhillon"], "venue": "S.", "citeRegEx": "Jain and Dhillon 2013", "shortCiteRegEx": null, "year": 2013}, {"title": "and Ye", "author": ["S. Ji"], "venue": "J.", "citeRegEx": "Ji and Ye 2009", "shortCiteRegEx": null, "year": 2009}, {"title": "and Akaho", "author": ["T. Kamishima"], "venue": "S.", "citeRegEx": "Kamishima and Akaho 2009", "shortCiteRegEx": null, "year": 2009}, {"title": "and Boutilier", "author": ["T. Lu"], "venue": "C.", "citeRegEx": "Lu and Boutilier 2011", "shortCiteRegEx": null, "year": 2011}, {"title": "R", "author": ["Luce"], "venue": "D.", "citeRegEx": "Luce 1959", "shortCiteRegEx": null, "year": 1959}, {"title": "Binary choice constraints on random utility indicators", "author": ["Marschak", "J others 1959] Marschak"], "venue": "Technical report,", "citeRegEx": "Marschak and Marschak,? \\Q1959\\E", "shortCiteRegEx": "Marschak and Marschak", "year": 1959}, {"title": "and Agarwal", "author": ["A. Rajkumar"], "venue": "S.", "citeRegEx": "Rajkumar and Agarwal 2016", "shortCiteRegEx": null, "year": 2016}, {"title": "L", "author": ["Thurstone"], "venue": "L.", "citeRegEx": "Thurstone 1927", "shortCiteRegEx": null, "year": 1927}, {"title": "Speedup matrix completion with side information: Application to multi-label learning", "author": ["Jin Xu", "M. Zhou 2013] Xu", "R. Jin", "Z.-H. Zhou"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Xu et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Xu et al\\.", "year": 2013}], "referenceMentions": [], "year": 2017, "abstractText": "We study the problem of ranking a set of items from nonactively chosen pairwise preferences where each item has feature information with it. We propose and characterize a very broad class of preference matrices giving rise to the Feature Low Rank (FLR) model, which subsumes several models ranging from the classic Bradley\u2013Terry\u2013Luce (BTL) (Bradley and Terry 1952) and Thurstone (Thurstone 1927) models to the recently proposed blade-chest (Chen and Joachims 2016) and generic low-rank preference (Rajkumar and Agarwal 2016) models. We use the technique of matrix completion in the presence of side information to develop the Inductive Pairwise Ranking (IPR) algorithm that provably learns a good ranking under the FLR model, in a sample-efficient manner. In practice, through systematic synthetic simulations, we confirm our theoretical findings regarding improvements in the sample complexity due to the use of feature information. Moreover, on popular real-world preference learning datasets, with as less as 10% sampling of the pairwise comparisons, our method recovers a good ranking.", "creator": "LaTeX with hyperref package"}}}