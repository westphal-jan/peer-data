{"id": "1702.03044", "review": {"conference": "iclr", "VERSION": "v1", "DATE_OF_SUBMISSION": "10-Feb-2017", "title": "Incremental Network Quantization: Towards Lossless CNNs with Low-Precision Weights", "abstract": "This paper presents incremental network quantization (INQ), a novel method, targeting to efficiently convert any pre-trained full-precision convolutional neural network (CNN) model into a low-precision version whose weights are constrained to be either powers of two or zero. Unlike existing methods which are struggled in noticeable accuracy loss, our INQ has the potential to resolve this issue, as benefiting from two innovations. On one hand, we introduce three interdependent operations, namely weight partition, group-wise quantization and re-training. A well-proven measure is employed to divide the weights in each layer of a pre-trained CNN model into two disjoint groups. The weights in the first group are responsible to form a low-precision base, thus they are quantized by a variable-length encoding method. The weights in the other group are responsible to compensate for the accuracy loss from the quantization, thus they are the ones to be re-trained. On the other hand, these three operations are repeated on the latest re-trained group in an iterative manner until all the weights are converted into low-precision ones, acting as an incremental network quantization and accuracy enhancement procedure. Extensive experiments on the ImageNet classification task using almost all known deep CNN architectures including AlexNet, VGG-16, GoogleNet and ResNets well testify the efficacy of the proposed method. Specifically, at 5-bit quantization, our models have improved accuracy than the 32-bit floating-point references. Taking ResNet-18 as an example, we further show that our quantized models with 4-bit, 3-bit and 2-bit ternary weights have improved or very similar accuracy against its 32-bit floating-point baseline. Besides, impressive results with the combination of network pruning and INQ are also reported. The code will be made publicly available.", "histories": [["v1", "Fri, 10 Feb 2017 02:30:22 GMT  (204kb)", "http://arxiv.org/abs/1702.03044v1", "Accepted as a conference track paper by ICLR 2017"], ["v2", "Fri, 25 Aug 2017 13:21:18 GMT  (204kb)", "http://arxiv.org/abs/1702.03044v2", "Published by ICLR 2017, and the code is available atthis https URL"]], "COMMENTS": "Accepted as a conference track paper by ICLR 2017", "reviews": [], "SUBJECTS": "cs.CV cs.AI cs.NE", "authors": ["aojun zhou", "anbang yao", "yiwen guo", "lin xu", "yurong chen"], "accepted": true, "id": "1702.03044"}, "pdf": {"name": "1702.03044.pdf", "metadata": {"source": "CRF", "title": null, "authors": ["Anbang Yao", "Yiwen Guo"], "emails": ["yurong.chen}@intel.com"], "sections": [{"heading": null, "text": "ar Xiv: 170 2.03 044v 1 [cs.C V] 10 Feb 2017 Published as conference report at ICLR 2017"}, {"heading": "1 INTRODUCTION", "text": "This is why most of them are unable to abide by the rules which they have imposed on themselves. (See why they are unable to understand the rules which they have imposed on themselves.) This proves that the rules of the rules and principles of the principles of the principles of the principles and principles of the principles of the principles and principles of the principles of the principles and principles of the principles of the principles and principles of the principles of the principles and principles of the principles of the principles and principles of the principles of the principles and principles of the principles of the principles and principles of the principles of the principles of the principles of the principles and principles of the principles of the principles of the principles and principles of the principles of the principles of the principles and principles of the principles of the principles of the principles of the principles and principles of the principles of the principles of the principles of the principles and principles of the principles of the principles of the principles of the principles of the principles and principles of the principles of the principles of the principles of the principles of the principles of the principles of the principles of the principles of the principles of the principles of the principles of the principles of the principles of the principles of the principles of the principles and principles of the principles of the principles of the principles of the principles of the principles of the principles of the principles of the principles of the principles of the principles of the principles and principles of the principles of the principles of the principles of the principles of the principles of the principles of the principles of the principles of the principles of the principles of the principles of the principles of the principles of the principles of the principles of the principles of the principles of the principles of the principles of the principles of the principles of the principles of the principles of the principles of the principles of the principles of the principles of the principles of the principles of the principles of the principles of the principles of the principles of the principles of the principles of the principles of the principles of the principles of the principles of the principles of the principles of the principles of the principles of the principles of the principles of the principles of the principles of the principles of the principles of the principles of the principles of the principles of the principles of the principles of the principles of the principles of the principles of the principles of the principles of the principles of the principles of the principles of the principles of the principles of the principles of the principles of the principles of the principles of the principles of the principles of the principles of the principles of the principles of the principles of the principles of the"}, {"heading": "2 INCREMENTAL NETWORK QUANTIZATION", "text": "In this section, we explain the findings of our INQ, describe its key components and explain its implementation."}, {"heading": "2.1 WEIGHT QUANTIZATION WITH VARIABLE-LENGTH ENCODING", "text": "Suppose we also try to explore the limit of the expected bit width under the premise of the network guarantee."}, {"heading": "2.2 INCREMENTAL QUANTIZATION STRATEGY", "text": "We can, of course, use the method described above to quantify any upstream complete precision method, which is also considered from a quantitative point of view. In the literature, there are many existing quantification processes, such as the quantification method of HashedNet (Chen et al., 2015), BinaryConnect (Courbariaux et al., 2015), BinaryNet (Courbariaux et al., 2016), XNOR-Net (Rastegari et al., 2016), TWN (Li & Liu, 2016), DoReFa-Net (Zhou et al., 2016) and QNN et al."}, {"heading": "2.3 INCREMENTAL NETWORK QUANTIZATION ALGORITHM", "text": "Now we come to the training method. Taking the lth layer as an example, the basic optimization problem of making its weights (Q-L) arises: either powers of two or zero can be expressed asmin Wl E (Wl) = L (Wl) + \u03bbR (Wl) s.t. Wl (i, j).Pl, 1 \u2264 l \u2264 L, (6) where L (Wl) is the network loss, R (Wl) is the regulation problem, \u03bb is a positive coefficient, and the limitation term shows any weight entryWl (i, j) should be selected from the setPl, which consists of a fixed number of powers of two plus zero. Direct solution of the above mentioned optimization problem in training is a challenge, since it is very easy to undertake convergence problems.By performing weight distribution and group-wise quantization operations beforehand, the optimization problem can be redesigned into a simpler version."}, {"heading": "3 EXPERIMENTAL RESULTS", "text": "In order to analyze the performance of our INQ, we conduct extensive experiments with the ImageNet classification task on a large scale, which is so far known as the most difficult benchmark for image classification. ImageNet data set comprises approximately 1.2 million training images and 50,000 validation images. Each image is commented as one of 1000 object classes. We apply our INQ to AlexNet, VGG-16, GoogleNet, ResNet-18 and ResNet-50 and cover almost all known deep CNN architectures. Using the middle sections of the validation images, we report the results using two standard measures: top-1 error rate and top-5 error rate. For fair comparison, all pre-trained, fully precise (i.e. 32-bit floatingpoint) CNN models except ResNet-18 are taken from the caffe model zoo2."}, {"heading": "3.1 RESULTS ON IMAGENET", "text": "It is expected that the first series of experiments will be conducted to demonstrate the effectiveness of our net connections on different CNN architectures. In terms of weight distribution, there are several candidate strategies, as we tried in our previous work on efficient network circumcision (Guo et al., 2016). In the random strategy, weights in each layer of an upstream model are randomly divided into two separate groups, and in this paper we will directly compare these two strategies for weight distribution. Weights in each layer of an upstream model are randomly divided into two separate groups, with weights divided into two separate layers. Absolute values are divided by comparison with absolute values values values values values."}, {"heading": "3.2 ANALYSIS OF WEIGHT PARTITION STRATEGIES", "text": "In our INQ, the first operation is the weight distribution, the result of which will have a direct impact on the following group-by-group quantization and retraining operations. As opposed to random strategies, where all weights are equally likely to fall into the two disjointed groups, the sectional-inspired strategy considers that weights with larger absolute values are more important than the smaller ones to form a low-precision basis for the original CNN model. We use ResNet-18 as a test case to compare the performance of these two strategies. In the experiments, the parameter settings are completely the same as described in Section 3.1. We define 4 eras for weight retraining. Table 2 summarizes the results of our INQ with 5-bit quantization. It can be seen that our INQ has a top-1 error rate of 32.11% and a top-5 error rate of 11.73% from random, inspired 1-division."}, {"heading": "3.3 THE TRADE-OFF BETWEEN EXPECTED BIT-WIDTH AND MODEL ACCURACY", "text": "The third set of experiments is performed to explore the limit of the expected bit width below which our INQ can still achieve a loss-free quantization of the network. Similar to the second set of experiments, we also use ResNet-18 as a test case, and the parameter settings for the lot size are completely equal to the weight drop and the momentum. Finally, models with lower precision are generated with 4-bit, 3-bit and even 2-bit ternary weights for comparisons. As the expected bit width decreases, the number of quantum values is significantly reduced, so we will increase the number of iterative steps accordingly to improve the accuracy of the final model with low precision. Specifically, we set the cumulative proportions of quantified weights in iterative steps to decrease as {0.3, 0.5, 0.8, 0.9, 0.95, 1} and bit in 2016."}, {"heading": "3.4 LOW-BIT DEEP COMPRESSION", "text": "In the recently published literature, the method of compression (Han et al., 2016) is replaced as far as possible by network compression without loss of model accuracy. Therefore, the last series of experiments will be conducted to explore the potential of our INQ for much better depth compression."}, {"heading": "4 CONCLUSIONS", "text": "In this essay we introduce INQ, a new network quantization method to address the problem of how to calculate all pre-trained full-precision (i.e. 32-bit floating-point) CNN models in a lossless version with low precision, whose weights are limited to being either potencies of two or zero. In contrast to existing methods that usually quantify all network weights at the same time, INQ is a more compact quantization framework. It includes three interdependent operations: weight allocation, group-wise quantization and retraining. Weight allocation we calculate the weights in each layer of a pre-trained full-precision CNN model in two disjunction-dependent groups, the complementary roles we play in INQ. The weights in the first group are quantified directly by a method of coding with variable length and form a low precision basis for the original CNN model, the weights we have calculated on the network based on the weights we have calculated on. The weights in the other group, the weights in INQ are all quantified during the quantification process."}, {"heading": "A APPENDIX 1: STATISTICAL ANALYSIS OF THE QUANTIZED WEIGHTS", "text": "Using our 5-bit AlexNet model as an example, we analyze the distribution of the quantified weights. Detailed statistical results are summarized in Table 6. We can determine: (1) in the 1st and 2nd Convolutionary Layer the values of {\u2212 2 \u2212 6, \u2212 2 \u2212 5, \u2212 2 \u2212 4, 2 \u2212 6, 2 \u2212 5, 2 \u2212 4} and {\u2212 2 \u2212 8, \u2212 2 \u2212 7, \u2212 2 \u2212 6, \u2212 2 \u2212 5, 0 \u2212 8, 2 \u2212 7, 2 \u2212 6, 2 \u2212 5} each claim over 60% and 94% of all quantified weights respectively; (2) the distribution of the quantified weights in the 3rd, 4th and 5th Convolutionary Layer are similar to those of the 2nd Convolutionary Layer, and other weights are quantified in the 2nd, 3rd, 4th, 4th and 5th Convolutionary Layer, respectively."}, {"heading": "B APPENDIX 2: LOSSLESS CNNS WITH LOW-PRECISION WEIGHTS AND LOW-PRECISION ACTIVATIONS", "text": "Recently, we have made some good progress in developing our INQ for lossless CNNs, both with low precision weights and with low precision activations. According to the results summarized in Table 7, our VGG-16 model with 5-bit weights and 4-bit activations has improved top-5 and top-1 detection rates compared to the previous references with 32-bit floating-point weights and 32-bit floating-point activations. To our knowledge, these should be the best results reported so far on the VGG-16 architecture."}], "references": [{"title": "Semantic image segmentation with deep convolutional nets and fully connected crfs", "author": ["Liang-Chieh Chen", "George Papandreou", "Iasonas Kokkinos", "Kevin Murphy", "L. Yuille Alan"], "venue": "In ICLR,", "citeRegEx": "Chen et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Chen et al\\.", "year": 2015}, {"title": "Compressing neural networks with the hashing trick", "author": ["Wenlin Chen", "James T. Wilson", "Stephen Tyree", "Kilian Q. Weinberger", "Yixin Chen"], "venue": "In ICML,", "citeRegEx": "Chen et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Chen et al\\.", "year": 2015}, {"title": "Binaryconnect: Training deep neural networks with binary weights during propagations", "author": ["Matthieu Courbariaux", "Bengio Yoshua", "David Jean-Pierre"], "venue": "In NIPS,", "citeRegEx": "Courbariaux et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Courbariaux et al\\.", "year": 2015}, {"title": "Binarized neural networks: Training deep neural networks with weights and activations constrained to +1 or -1", "author": ["Matthieu Courbariaux", "Itay Hubara", "Daniel Soudry", "Ran El-Yaniv", "Yoshua Bengio"], "venue": "arXiv preprint arXiv:1602.02830v3,", "citeRegEx": "Courbariaux et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Courbariaux et al\\.", "year": 2016}, {"title": "Fast r-cnn", "author": ["Ross Girshick"], "venue": "In ICCV,", "citeRegEx": "Girshick.,? \\Q2015\\E", "shortCiteRegEx": "Girshick.", "year": 2015}, {"title": "Compressing deep concolutional networks using vector quantization", "author": ["Yunchao Gong", "Liu Liu", "Ming Yang", "Lubomir Bourdev"], "venue": "arXiv preprint arXiv:1412.6115v1,", "citeRegEx": "Gong et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Gong et al\\.", "year": 2014}, {"title": "Dynamic network surgery for efficient dnns", "author": ["Yiwen Guo", "Anbang Yao", "Yurong Chen"], "venue": "In NIPS,", "citeRegEx": "Guo et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Guo et al\\.", "year": 2016}, {"title": "Deep learning with limited numerical precision", "author": ["Suyog Gupta", "Ankur Agrawal", "Kailash Gopalakrishnan", "Pritish Narayanan"], "venue": "In ICML,", "citeRegEx": "Gupta et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Gupta et al\\.", "year": 2015}, {"title": "Learning both weights and connections for efficient neural networks", "author": ["Song Han", "Jeff Pool", "John Tran", "William J. Dally"], "venue": "In NIPS,", "citeRegEx": "Han et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Han et al\\.", "year": 2015}, {"title": "Deep compression: Compressing deep neural networks with pruning, trained quantization and huffman coding", "author": ["Song Han", "Jeff Pool", "John Tran", "William J. Dally"], "venue": "In ICLR,", "citeRegEx": "Han et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Han et al\\.", "year": 2016}, {"title": "Deep residual learning for image recognition", "author": ["Kaiming He", "Zhang Xiangyu", "Ren Shaoqing", "Sun Jian"], "venue": null, "citeRegEx": "He et al\\.,? \\Q2016\\E", "shortCiteRegEx": "He et al\\.", "year": 2016}, {"title": "Quantized neural networks: Training neural networks with low precision weights and activations", "author": ["Itay Hubara", "Matthieu Courbariaux", "Daniel Soudry", "Ran El-Yaniv", "Yoshua Bengio"], "venue": "arXiv preprint arXiv:1609.07061v1,", "citeRegEx": "Hubara et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Hubara et al\\.", "year": 2016}, {"title": "Imagenet classification with deep convolutional neural networks", "author": ["Alex Krizhevsky", "Sutskever Ilya", "E. Hinton Geoffrey"], "venue": "In NIPS,", "citeRegEx": "Krizhevsky et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Krizhevsky et al\\.", "year": 2012}, {"title": "Gradient-based learning applied to documentrecognition", "author": ["Yann LeCun", "Bottou Leon", "Yoshua Bengio", "Patrick Hadner"], "venue": "In NIPS,", "citeRegEx": "LeCun et al\\.,? \\Q1998\\E", "shortCiteRegEx": "LeCun et al\\.", "year": 1998}, {"title": "Ternary weight networks", "author": ["Fengfu Li", "Bin Liu"], "venue": "arXiv preprint arXiv:1605.04711v1,", "citeRegEx": "Li and Liu.,? \\Q2016\\E", "shortCiteRegEx": "Li and Liu.", "year": 2016}, {"title": "Fully convolutional networks for semantic segmentation", "author": ["Jonathan Long", "Evan Shelhamer", "Trevor Darrell"], "venue": "In CVPR,", "citeRegEx": "Long et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Long et al\\.", "year": 2015}, {"title": "Xnor-net: Imagenet classification using binary convolutional neural networks", "author": ["Mohammad Rastegari", "Vicente Ordonez", "Joseph Redmon", "Ali Farhadi"], "venue": "arXiv preprint arXiv:1603.05279v4,", "citeRegEx": "Rastegari et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Rastegari et al\\.", "year": 2016}, {"title": "Faster r-cnn: Towards real-time object detection with region proposal networks", "author": ["Shaoqing Ren", "Kaiming He", "Ross Girshick", "Jian Sun"], "venue": "In NIPS,", "citeRegEx": "Ren et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Ren et al\\.", "year": 2015}, {"title": "Very deep convolutional networks for large-scale image recognition", "author": ["Karen Simonyan", "Andrew Zisserman"], "venue": "In ICLR,", "citeRegEx": "Simonyan and Zisserman.,? \\Q2015\\E", "shortCiteRegEx": "Simonyan and Zisserman.", "year": 2015}, {"title": "Expectation backpropagation: Parameter-free training of multilayer neural networks with continuous or discrete weights", "author": ["Daniel Soudry", "Itay Hubara", "Ron Meir"], "venue": "In NIPS,", "citeRegEx": "Soudry et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Soudry et al\\.", "year": 2014}, {"title": "Deep learning face representation from predicting 10,000 classes", "author": ["Yi Sun", "Xiaogang Wang", "Xiaoou Tang"], "venue": "In CVPR,", "citeRegEx": "Sun et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Sun et al\\.", "year": 2014}, {"title": "Going deeper with convolutions", "author": ["Christian Szegedy", "Wei Liu", "Yangqing Jia", "Pierre Sermanet", "Scott Reed", "Dragomir Anguelov", "Dumitru Erhan", "Vincent Vanhoucke", "Andrew Rabinovich"], "venue": "In CVPR,", "citeRegEx": "Szegedy et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Szegedy et al\\.", "year": 2015}, {"title": "Inception-v4, inception-resnet and the impact of residual connections on learning", "author": ["Christian Szegedy", "Sergey Ioffe", "Vincent Vanhoucke"], "venue": "arXiv preprint arXiv:1602.07261v1,", "citeRegEx": "Szegedy et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Szegedy et al\\.", "year": 2016}, {"title": "Deepface: Closing the gap to human-level performance in face verification", "author": ["Yaniv Taigman", "Ming Yang", "Marc\u2019 Aurelio Ranzato", "Lior Wolf"], "venue": "In CVPR,", "citeRegEx": "Taigman et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Taigman et al\\.", "year": 2014}, {"title": "Improving the speed of neural networks on cpus", "author": ["Vincent Vanhoucke", "Andrew Senior", "Mark Z. Mao"], "venue": "In Deep Learning and Unsupervised Feature Learning Workshop,", "citeRegEx": "Vanhoucke et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Vanhoucke et al\\.", "year": 2011}, {"title": "Dorefa-net: Training low bitwidth convolutional neural networks with low bitwidth gradients", "author": ["Shuchang Zhou", "Zekun Ni", "Xinyu Zhou", "He Wen", "Yuxin Wu", "Yuheng Zou"], "venue": "arXiv preprint arXiv:1605.04711v1,", "citeRegEx": "Zhou et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Zhou et al\\.", "year": 2016}, {"title": "AlexNet model, the required bit-width for each layer", "author": ["Gong"], "venue": null, "citeRegEx": "Gong,? \\Q2014\\E", "shortCiteRegEx": "Gong", "year": 2014}], "referenceMentions": [{"referenceID": 12, "context": "Deep convolutional neural networks (CNNs) have demonstrated record breaking results on a variety of computer vision tasks such as image classification (Krizhevsky et al., 2012; Simonyan & Zisserman, 2015), face recognition (Taigman et al.", "startOffset": 151, "endOffset": 204}, {"referenceID": 23, "context": ", 2012; Simonyan & Zisserman, 2015), face recognition (Taigman et al., 2014; Sun et al., 2014), semantic segmentation (Long et al.", "startOffset": 54, "endOffset": 94}, {"referenceID": 20, "context": ", 2012; Simonyan & Zisserman, 2015), face recognition (Taigman et al., 2014; Sun et al., 2014), semantic segmentation (Long et al.", "startOffset": 54, "endOffset": 94}, {"referenceID": 15, "context": ", 2014), semantic segmentation (Long et al., 2015; Chen et al., 2015a) and object detection (Girshick, 2015; Ren et al.", "startOffset": 31, "endOffset": 70}, {"referenceID": 4, "context": ", 2015a) and object detection (Girshick, 2015; Ren et al., 2015).", "startOffset": 30, "endOffset": 64}, {"referenceID": 17, "context": ", 2015a) and object detection (Girshick, 2015; Ren et al., 2015).", "startOffset": 30, "endOffset": 64}, {"referenceID": 10, "context": "Recent progress further shows clear evidence that CNNs could easily enjoy the accuracy gain from the increased network depth and width (He et al., 2016; Szegedy et al., 2015; 2016).", "startOffset": 135, "endOffset": 180}, {"referenceID": 21, "context": "Recent progress further shows clear evidence that CNNs could easily enjoy the accuracy gain from the increased network depth and width (He et al., 2016; Szegedy et al., 2015; 2016).", "startOffset": 135, "endOffset": 180}, {"referenceID": 12, "context": "(2014) address the storage problem of AlexNet (Krizhevsky et al., 2012) with vector quantization techniques.", "startOffset": 46, "endOffset": 71}, {"referenceID": 8, "context": "(2016) present deep compression method which combines the pruning (Han et al., 2015), vector quantization and Huffman coding, and reduce the model storage by 35\u00d7 on AlexNet and 49\u00d7 on VGG-16 (Simonyan & Zisserman, 2015).", "startOffset": 66, "endOffset": 84}, {"referenceID": 2, "context": "BinaryConnect (Courbariaux et al., 2015) further extends the idea behind EBP to binarize network weights during training phase directly.", "startOffset": 14, "endOffset": 40}, {"referenceID": 13, "context": "BinaryConnect achieves state-of-the-art accuracy using shallow CNNs for small datasets such as MNIST (LeCun et al., 1998) and CIFAR-10.", "startOffset": 101, "endOffset": 121}, {"referenceID": 3, "context": "Later on, a series of efforts have been invested to train CNNs with low-precision weights, low-precision activations and even low-precision gradients, including but not limited to BinaryNet (Courbariaux et al., 2016), XNOR-Net (Rastegari et al.", "startOffset": 190, "endOffset": 216}, {"referenceID": 16, "context": ", 2016), XNOR-Net (Rastegari et al., 2016), ternary weight network (TWN) (Li & Liu, 2016), DoReFa-Net (Zhou et al.", "startOffset": 18, "endOffset": 42}, {"referenceID": 25, "context": ", 2016), ternary weight network (TWN) (Li & Liu, 2016), DoReFa-Net (Zhou et al., 2016) and quantized neural network (QNN) (Hubara et al.", "startOffset": 67, "endOffset": 86}, {"referenceID": 11, "context": ", 2016) and quantized neural network (QNN) (Hubara et al., 2016).", "startOffset": 43, "endOffset": 64}, {"referenceID": 8, "context": "Weight partition uses a pruning-inspired measure (Han et al., 2015; Guo et al., 2016) to divide the weights in each layer of a pre-trained full-precision CNN model into two disjoint groups which play complementary roles in our INQ.", "startOffset": 49, "endOffset": 85}, {"referenceID": 6, "context": "Weight partition uses a pruning-inspired measure (Han et al., 2015; Guo et al., 2016) to divide the weights in each layer of a pre-trained full-precision CNN model into two disjoint groups which play complementary roles in our INQ.", "startOffset": 49, "endOffset": 85}, {"referenceID": 1, "context": "Gong et al. (2014) address the storage problem of AlexNet (Krizhevsky et al.", "startOffset": 0, "endOffset": 19}, {"referenceID": 0, "context": "HashedNet (Chen et al., 2015b) uses a hash function to randomly map pre-trained weights into hash buckets, and all the weights in the same hash bucket are constrained to share a single floating-point value. In HashedNet, only the fully connected layers of several shallow CNN models are considered. For better compression, Han et al. (2016) present deep compression method which combines the pruning (Han et al.", "startOffset": 11, "endOffset": 341}, {"referenceID": 0, "context": "HashedNet (Chen et al., 2015b) uses a hash function to randomly map pre-trained weights into hash buckets, and all the weights in the same hash bucket are constrained to share a single floating-point value. In HashedNet, only the fully connected layers of several shallow CNN models are considered. For better compression, Han et al. (2016) present deep compression method which combines the pruning (Han et al., 2015), vector quantization and Huffman coding, and reduce the model storage by 35\u00d7 on AlexNet and 49\u00d7 on VGG-16 (Simonyan & Zisserman, 2015). Vanhoucke et al. (2011) use an SSE 8-bit fixed-point implementation to improve the computation of neural networks on the modern Intel x86 CPUs in feed-forward test, yielding 3\u00d7 speed-up over an optimized floating-point baseline.", "startOffset": 11, "endOffset": 579}, {"referenceID": 0, "context": "HashedNet (Chen et al., 2015b) uses a hash function to randomly map pre-trained weights into hash buckets, and all the weights in the same hash bucket are constrained to share a single floating-point value. In HashedNet, only the fully connected layers of several shallow CNN models are considered. For better compression, Han et al. (2016) present deep compression method which combines the pruning (Han et al., 2015), vector quantization and Huffman coding, and reduce the model storage by 35\u00d7 on AlexNet and 49\u00d7 on VGG-16 (Simonyan & Zisserman, 2015). Vanhoucke et al. (2011) use an SSE 8-bit fixed-point implementation to improve the computation of neural networks on the modern Intel x86 CPUs in feed-forward test, yielding 3\u00d7 speed-up over an optimized floating-point baseline. Training CNNs by substituting the 32-bit floating-point representation with the 16-bit fixed-point representation has also been explored in Gupta et al. (2015). Other seminal works attempt to restrict CNNs into low-precision versions during training phase.", "startOffset": 11, "endOffset": 944}, {"referenceID": 0, "context": "HashedNet (Chen et al., 2015b) uses a hash function to randomly map pre-trained weights into hash buckets, and all the weights in the same hash bucket are constrained to share a single floating-point value. In HashedNet, only the fully connected layers of several shallow CNN models are considered. For better compression, Han et al. (2016) present deep compression method which combines the pruning (Han et al., 2015), vector quantization and Huffman coding, and reduce the model storage by 35\u00d7 on AlexNet and 49\u00d7 on VGG-16 (Simonyan & Zisserman, 2015). Vanhoucke et al. (2011) use an SSE 8-bit fixed-point implementation to improve the computation of neural networks on the modern Intel x86 CPUs in feed-forward test, yielding 3\u00d7 speed-up over an optimized floating-point baseline. Training CNNs by substituting the 32-bit floating-point representation with the 16-bit fixed-point representation has also been explored in Gupta et al. (2015). Other seminal works attempt to restrict CNNs into low-precision versions during training phase. Soudry et al. (2014) propose expectation backpropagation (EBP) to estimate the posterior distribution of deterministic network weights.", "startOffset": 11, "endOffset": 1062}, {"referenceID": 9, "context": "(4) Taking AlexNet as an example, the combination of our network pruning and INQ outperforms deep compression method (Han et al., 2016) with significant margins.", "startOffset": 117, "endOffset": 135}, {"referenceID": 5, "context": ", 2015b), vector quantization (Gong et al., 2014), fixed-point representation (Vanhoucke et al.", "startOffset": 30, "endOffset": 49}, {"referenceID": 24, "context": ", 2014), fixed-point representation (Vanhoucke et al., 2011; Gupta et al., 2015), BinaryConnect (Courbariaux et al.", "startOffset": 36, "endOffset": 80}, {"referenceID": 7, "context": ", 2014), fixed-point representation (Vanhoucke et al., 2011; Gupta et al., 2015), BinaryConnect (Courbariaux et al.", "startOffset": 36, "endOffset": 80}, {"referenceID": 2, "context": ", 2015), BinaryConnect (Courbariaux et al., 2015), BinaryNet (Courbariaux et al.", "startOffset": 23, "endOffset": 49}, {"referenceID": 3, "context": ", 2015), BinaryNet (Courbariaux et al., 2016), XNOR-Net (Rastegari et al.", "startOffset": 19, "endOffset": 45}, {"referenceID": 16, "context": ", 2016), XNOR-Net (Rastegari et al., 2016), TWN (Li & Liu, 2016), DoReFa-Net (Zhou et al.", "startOffset": 18, "endOffset": 42}, {"referenceID": 25, "context": ", 2016), TWN (Li & Liu, 2016), DoReFa-Net (Zhou et al., 2016) and QNN (Hubara et al.", "startOffset": 42, "endOffset": 61}, {"referenceID": 11, "context": ", 2016) and QNN (Hubara et al., 2016).", "startOffset": 16, "endOffset": 37}, {"referenceID": 8, "context": "We are partially inspired by the latest progress in network pruning (Han et al., 2015; Guo et al., 2016).", "startOffset": 68, "endOffset": 104}, {"referenceID": 6, "context": "We are partially inspired by the latest progress in network pruning (Han et al., 2015; Guo et al., 2016).", "startOffset": 68, "endOffset": 104}, {"referenceID": 8, "context": "That is, only the weights still keep with floating-point values are updated, akin to the latest pruning methods (Han et al., 2015; Guo et al., 2016) in which only the weights that are not currently removed are re-trained to enhance network accuracy.", "startOffset": 112, "endOffset": 148}, {"referenceID": 6, "context": "That is, only the weights still keep with floating-point values are updated, akin to the latest pruning methods (Han et al., 2015; Guo et al., 2016) in which only the weights that are not currently removed are re-trained to enhance network accuracy.", "startOffset": 112, "endOffset": 148}, {"referenceID": 10, "context": "Note that He et al. (2016) do not release their pre-trained ResNet-18 model to the public, so we use a publicly available re-implementation by Facebook.", "startOffset": 10, "endOffset": 27}, {"referenceID": 6, "context": "Regarding weight partition, there are several candidate strategies as we tried in our previous work for efficient network pruning (Guo et al., 2016).", "startOffset": 130, "endOffset": 148}, {"referenceID": 6, "context": "Regarding weight partition, there are several candidate strategies as we tried in our previous work for efficient network pruning (Guo et al., 2016). In Guo et al. (2016), we found random partition and pruning-inspired partition are the two best choices compared with the others.", "startOffset": 131, "endOffset": 171}, {"referenceID": 16, "context": "56% decrease in top-5 error rate in comparison to the pre-trained full-precision reference, its accuracy is considerably better than stateof-the-art results reported for binary-weight network (BWN) (Rastegari et al., 2016) and ternary weight network (TWN) (Li & Liu, 2016).", "startOffset": 198, "endOffset": 222}, {"referenceID": 16, "context": "Method Bit-width Top-1 error Top-5 error BWN(Rastegari et al., 2016) 1 39.", "startOffset": 44, "endOffset": 68}, {"referenceID": 9, "context": "In the literature, recently proposed deep compression method (Han et al., 2016) reports so far best results on network compression without loss of model accuracy.", "startOffset": 61, "endOffset": 79}, {"referenceID": 8, "context": "(2016) is a hybrid network compression solution combining three different techniques, namely network pruning (Han et al., 2015), vector quantization (Gong et al.", "startOffset": 109, "endOffset": 127}, {"referenceID": 5, "context": ", 2015), vector quantization (Gong et al., 2014) and Huffman coding.", "startOffset": 29, "endOffset": 48}, {"referenceID": 6, "context": "We solved this problem by our dynamic network surgery (DNS) method (Guo et al., 2016) which achieves about 7\u00d7 speed-up in training and improves the performance of network pruning from 9\u00d7 to 17.", "startOffset": 67, "endOffset": 85}, {"referenceID": 5, "context": "Besides, we also perform a set of experiments on AlexNet to compare the performance of our INQ and vector quantization (Gong et al., 2014).", "startOffset": 119, "endOffset": 138}, {"referenceID": 5, "context": "This is consistent with the results reported in (Gong et al., 2014).", "startOffset": 48, "endOffset": 67}, {"referenceID": 6, "context": "In the literature, recently proposed deep compression method (Han et al., 2016) reports so far best results on network compression without loss of model accuracy. Therefore, the last set of experiments is conducted to explore the potential of our INQ for much better deep compression. Note that Han et al. (2016) is a hybrid network compression solution combining three different techniques, namely network pruning (Han et al.", "startOffset": 62, "endOffset": 313}, {"referenceID": 5, "context": ", 2015), vector quantization (Gong et al., 2014) and Huffman coding. Taking AlexNet as an example, network pruning gets 9\u00d7 compression, however this result is mainly obtained from the fully connected layers. Actually its compression performance on the convolutional layers is less than 3\u00d7 (as can be seen in the Table 4 of Han et al. (2016)).", "startOffset": 30, "endOffset": 341}, {"referenceID": 5, "context": ", 2015), vector quantization (Gong et al., 2014) and Huffman coding. Taking AlexNet as an example, network pruning gets 9\u00d7 compression, however this result is mainly obtained from the fully connected layers. Actually its compression performance on the convolutional layers is less than 3\u00d7 (as can be seen in the Table 4 of Han et al. (2016)). Besides, network pruning is realized by separately performing pruning and re-training in an iterative way, which is very time-consuming. It will cost at least several weeks for compressing AlexNet. We solved this problem by our dynamic network surgery (DNS) method (Guo et al., 2016) which achieves about 7\u00d7 speed-up in training and improves the performance of network pruning from 9\u00d7 to 17.7\u00d7. In Han et al. (2016), after network pruning, vector quantization further improves compression ratio from 9\u00d7 to 27\u00d7, and Huffman coding finally boosts compression ratio up to 35\u00d7.", "startOffset": 30, "endOffset": 759}, {"referenceID": 5, "context": ", 2015), vector quantization (Gong et al., 2014) and Huffman coding. Taking AlexNet as an example, network pruning gets 9\u00d7 compression, however this result is mainly obtained from the fully connected layers. Actually its compression performance on the convolutional layers is less than 3\u00d7 (as can be seen in the Table 4 of Han et al. (2016)). Besides, network pruning is realized by separately performing pruning and re-training in an iterative way, which is very time-consuming. It will cost at least several weeks for compressing AlexNet. We solved this problem by our dynamic network surgery (DNS) method (Guo et al., 2016) which achieves about 7\u00d7 speed-up in training and improves the performance of network pruning from 9\u00d7 to 17.7\u00d7. In Han et al. (2016), after network pruning, vector quantization further improves compression ratio from 9\u00d7 to 27\u00d7, and Huffman coding finally boosts compression ratio up to 35\u00d7. For fair comparison, we combine our proposed INQ and DNS, and compare the resulting method with Han et al. (2016). Detailed results are summarized in Table 5.", "startOffset": 30, "endOffset": 1031}, {"referenceID": 5, "context": ", 2015), vector quantization (Gong et al., 2014) and Huffman coding. Taking AlexNet as an example, network pruning gets 9\u00d7 compression, however this result is mainly obtained from the fully connected layers. Actually its compression performance on the convolutional layers is less than 3\u00d7 (as can be seen in the Table 4 of Han et al. (2016)). Besides, network pruning is realized by separately performing pruning and re-training in an iterative way, which is very time-consuming. It will cost at least several weeks for compressing AlexNet. We solved this problem by our dynamic network surgery (DNS) method (Guo et al., 2016) which achieves about 7\u00d7 speed-up in training and improves the performance of network pruning from 9\u00d7 to 17.7\u00d7. In Han et al. (2016), after network pruning, vector quantization further improves compression ratio from 9\u00d7 to 27\u00d7, and Huffman coding finally boosts compression ratio up to 35\u00d7. For fair comparison, we combine our proposed INQ and DNS, and compare the resulting method with Han et al. (2016). Detailed results are summarized in Table 5. When combing our proposed INQ and DNS, we achieve much better compression results compared with Han et al. (2016). Specifically, with 5-bit quantization, we can achieve 53\u00d7 compression with slightly larger gains both in top-5 and top-1 recognition rates, yielding 51.", "startOffset": 30, "endOffset": 1190}, {"referenceID": 5, "context": ", 2015), vector quantization (Gong et al., 2014) and Huffman coding. Taking AlexNet as an example, network pruning gets 9\u00d7 compression, however this result is mainly obtained from the fully connected layers. Actually its compression performance on the convolutional layers is less than 3\u00d7 (as can be seen in the Table 4 of Han et al. (2016)). Besides, network pruning is realized by separately performing pruning and re-training in an iterative way, which is very time-consuming. It will cost at least several weeks for compressing AlexNet. We solved this problem by our dynamic network surgery (DNS) method (Guo et al., 2016) which achieves about 7\u00d7 speed-up in training and improves the performance of network pruning from 9\u00d7 to 17.7\u00d7. In Han et al. (2016), after network pruning, vector quantization further improves compression ratio from 9\u00d7 to 27\u00d7, and Huffman coding finally boosts compression ratio up to 35\u00d7. For fair comparison, we combine our proposed INQ and DNS, and compare the resulting method with Han et al. (2016). Detailed results are summarized in Table 5. When combing our proposed INQ and DNS, we achieve much better compression results compared with Han et al. (2016). Specifically, with 5-bit quantization, we can achieve 53\u00d7 compression with slightly larger gains both in top-5 and top-1 recognition rates, yielding 51.43%/96.30% absolute improvement in compression performance compared with full version/fair version (i.e., the combination of network pruning and vector quantization) of Han et al. (2016), respectively.", "startOffset": 30, "endOffset": 1530}, {"referenceID": 8, "context": "Method Bit-width(Conv/FC) Compression ratio Decrease in top-1/top5 error Han et al. (2016) (P+Q) 8/5 27\u00d7 0.", "startOffset": 73, "endOffset": 91}, {"referenceID": 8, "context": "Method Bit-width(Conv/FC) Compression ratio Decrease in top-1/top5 error Han et al. (2016) (P+Q) 8/5 27\u00d7 0.00%/0.03% Han et al. (2016) (P+Q+H) 8/5 35\u00d7 0.", "startOffset": 73, "endOffset": 135}, {"referenceID": 8, "context": "Method Bit-width(Conv/FC) Compression ratio Decrease in top-1/top5 error Han et al. (2016) (P+Q) 8/5 27\u00d7 0.00%/0.03% Han et al. (2016) (P+Q+H) 8/5 35\u00d7 0.00%/0.03% Han et al. (2016) (P+Q+H) 8/4 -0.", "startOffset": 73, "endOffset": 181}, {"referenceID": 8, "context": "Method Bit-width(Conv/FC) Compression ratio Decrease in top-1/top5 error Han et al. (2016) (P+Q) 8/5 27\u00d7 0.00%/0.03% Han et al. (2016) (P+Q+H) 8/5 35\u00d7 0.00%/0.03% Han et al. (2016) (P+Q+H) 8/4 -0.01%/0.00% Our method (P+Q) 5/5 53\u00d7 0.08%/0.03% Han et al. (2016) (P+Q+H) 4/2 -1.", "startOffset": 73, "endOffset": 261}, {"referenceID": 5, "context": "Last but not least, the final weights for vector quantization (Gong et al., 2014), network pruning (Han et al.", "startOffset": 62, "endOffset": 81}, {"referenceID": 8, "context": ", 2014), network pruning (Han et al., 2015) and deep compression (Han et al.", "startOffset": 25, "endOffset": 43}, {"referenceID": 9, "context": ", 2015) and deep compression (Han et al., 2016) are still floating-point values, but the final weights for our INQ are in the form of either powers of two or zero.", "startOffset": 29, "endOffset": 47}], "year": 2017, "abstractText": "This paper presents incremental network quantization (INQ), a novel method, targeting to efficiently convert any pre-trained full-precision convolutional neural network (CNN) model into a low-precision version whose weights are constrained to be either powers of two or zero. Unlike existing methods which are struggled in noticeable accuracy loss, our INQ has the potential to resolve this issue, as benefiting from two innovations. On one hand, we introduce three interdependent operations, namely weight partition, group-wise quantization and re-training. A wellproven measure is employed to divide the weights in each layer of a pre-trained CNN model into two disjoint groups. The weights in the first group are responsible to form a low-precision base, thus they are quantized by a variable-length encoding method. The weights in the other group are responsible to compensate for the accuracy loss from the quantization, thus they are the ones to be re-trained. On the other hand, these three operations are repeated on the latest re-trained group in an iterative manner until all the weights are converted into low-precision ones, acting as an incremental network quantization and accuracy enhancement procedure. Extensive experiments on the ImageNet classification task using almost all known deep CNN architectures including AlexNet, VGG-16, GoogleNet and ResNets well testify the efficacy of the proposedmethod. Specifically, at 5-bit quantization (a variable-length encoding: 1 bit for representing zero value, and the remaining 4 bits represent at most 16 different values for the powers of two) , our models have improved accuracy than the 32-bit floating-point references. Taking ResNet-18 as an example, we further show that our quantized models with 4-bit, 3-bit and 2-bit ternary weights have improved or very similar accuracy against its 32-bit floating-point baseline. Besides, impressive results with the combination of network pruning and INQ are also reported. We believe that our method sheds new insights on how to make deep CNNs to be applicable on mobile or embedded devices. The code will be made publicly available.", "creator": "LaTeX with hyperref package"}}}