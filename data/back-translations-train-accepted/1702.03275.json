{"id": "1702.03275", "review": {"conference": "nips", "VERSION": "v1", "DATE_OF_SUBMISSION": "10-Feb-2017", "title": "Batch Renormalization: Towards Reducing Minibatch Dependence in Batch-Normalized Models", "abstract": "Batch Normalization is quite effective at accelerating and improving the training of deep models. However, its effectiveness diminishes when the training minibatches are small, or do not consist of independent samples. We hypothesize that this is due to the dependence of model layer inputs on all the examples in the minibatch, and different activations being produced between training and inference. We propose Batch Renormalization, a simple and effective extension to ensure that the training and inference models generate the same outputs that depend on individual examples rather than the entire minibatch. Models trained with Batch Renormalization perform substantially better than batchnorm when training with small or non-i.i.d. minibatches. At the same time, Batch Renormalization retains the benefits of batchnorm such as insensitivity to initialization and training efficiency.", "histories": [["v1", "Fri, 10 Feb 2017 18:27:17 GMT  (72kb)", "http://arxiv.org/abs/1702.03275v1", null], ["v2", "Thu, 30 Mar 2017 17:58:32 GMT  (73kb)", "http://arxiv.org/abs/1702.03275v2", null]], "reviews": [], "SUBJECTS": "cs.LG", "authors": ["sergey ioffe"], "accepted": true, "id": "1702.03275"}, "pdf": {"name": "1702.03275.pdf", "metadata": {"source": "CRF", "title": "Batch Renormalization: Towards Reducing Minibatch Dependence in Batch-Normalized Models", "authors": ["Sergey Ioffe"], "emails": ["sioffe@google.com"], "sections": [{"heading": null, "text": "ar Xiv: 170 2.03 275v 1 [cs.L G] February 10, 2017Batch normalization is very effective in accelerating and improving the training of deep models, but its effectiveness decreases when the training minibatches are small or do not consist of independent samples. We suspect that this is due to the dependence on model layer input on all examples in the minibatch and on different activations generated between training and inference. We propose batch renormalization, a simple and effective extension to ensure that the training and inference models produce the same results that depend on individual examples and not on the whole minibatch. Models trained with batch renormalization perform much better than batch norms when trained with small or non-i.i.d. minibatches. At the same time, batch renormalization retains the advantages of the batch standard, such as insensitivity to initialization and training efficiency."}, {"heading": "1 Introduction", "text": "In fact, it is a way in which people are able to determine for themselves what they want and what they want."}, {"heading": "2 Prior Work: Batch Normalization", "text": "The task is to minimize the loss that dissolves beyond the training chain. At each training step, a minibatch of m examples is used to calculate the gradient 1m values, and if these changes also affect the distribution of the x values. Since x itself affects the loss through all layers above, this change complicates the formation of the above layers. We observe that x depends on all the model parameters used for its calculation, and when the distribution of the x values itself is changed."}, {"heading": "3 Batch Renormalization", "text": "s observe that if we have an estimate of the mean value of x and normalize a particular node x using the minibatch statistics or their moving averages, then the results of these two normalizations are related by affine transformation. Specifically, let us have an estimate of the mean value of x, and be an estimate of its standard deviation, which may be calculated as a moving average over the last minibatches. Then, we have: xi \u2212 \u00b5B + d, where r = will be an average of x, and an estimate of its standard deviation."}, {"heading": "4 Results", "text": "Our basic model is Inception v3 [12], trained on 1000 classes of the ImageNet training set [8], and evaluated on the ImageNet validation data. In the base model, batch norm was used after folding and before ReLU [7]. To apply Batch Renorm, we simply exchanged it for the model instead of batch norm. Both methods normalize each function card using examples as well as spatial locations. We fix the scale \u03b3 = 1, since it could be propagated by the ReLU and absorbed into the next shift. Training has 50 synchronized workers [3]. Each worker processed a minibatch of 32 examples per training step. Gradients, calculated for all 50 minibatches, were aggregated and then used by the RMSProp optimizers."}, {"heading": "4.1 Baseline", "text": "Specifically, batch norm was applied to each of the 50 minibatches; each example was normalized using 32 examples, but the resulting gradients were aggregated over 50 minibatches. This model achieved the top-1 validation accuracy of 78.3% after 130k training steps. To ensure that Batch Renorm did not reduce the performance of such minibatches, we also trained the model with Batch Renorm, see Figure 1. The test accuracy of this model closely tracked the baseline and achieved a slightly higher test accuracy after the same number of steps (78.5%)."}, {"heading": "4.2 Small minibatches", "text": "To examine the effectiveness of Batch Renorm in training on small minibatches, we reduced the number of examples used for normalization to 4. Each minibatch of size 32 was therefore divided into \"microbatches\" with 4 examples each; each microbatch was normalized independently, but the loss for each minibatch was calculated as before. In other words, the gradient was still aggregated over 1600 examples per step, but the normalization included groups of 4 examples instead of 32 as in the baseline. Figure 2 shows the results. The validation accuracy of the batch standard model is significantly lower than the baseline, which normalized over minibatches of size 32, and the training is slow and reaches 74.2% at 210k steps. We achieve a significant improvement much faster (76.5% at 130k steps) by replacing Batch Renorm with batch, with the resulting test accuracy remaining unspecified when we apply the minibatch to either what we get below the norm or if we do not apply the minibatch to the minibatch."}, {"heading": "4.3 Non-i.i.d. minibatches", "text": "In fact, if examples in a minibatch are not independently scanned, the batch norm may have little effect. However, scanning with dependencies may be necessary for tasks such as metric learning [4, 11]. We may want to ensure that images with the same label have more similar representations than usual, and to learn this, we need to find an appropriate number of image pairs with the same label within the same minibatch. In this experiment (Figure 3), we selected each minibatch of size 32 by randomly scanning 16 labels (out of a total of 1000) with substitutes, then randomly selected 2 images for each of these labels. In batch norm training, the test accuracy is much lower than in minibatches, which only reach 67%. Surprisingly, even the training accuracy of RenRenRenRenRenRenRenRenReni.8% is lower than the test accuracy of labels in the i.i.i.i.i.i.i.i.e case, there is a drop that is consistent with the adjustment."}, {"heading": "5 Conclusions", "text": "We have shown that batch normalization, while effective, is not well suited for small or non-i.i.d. training minibatches. We have hypothesized that these disadvantages are due to the fact that the activations in the model, which in turn are used by layers other than inputs, are calculated differently during training than during inference. We deal with batch renormalization, which replaces the batch norm and ensures that the outputs calculated by the model depend only on the individual examples and not the entire minibatch, both during training and during inferencing. Batch renormalization expands batch norms with a perdimension correction to ensure that the activations between the training and inference networks match. This correction is identical in expectation; its parameters are calculated from the minibatch but are treated as constant by the optimizer."}], "references": [{"title": "Normalization propagation: A parametric technique for removing internal covariate shift in deep networks", "author": ["D. Arpit", "Y. Zhou", "B.U. Kota", "V. Govindaraju"], "venue": "arXiv preprint arXiv:1603.01431", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2016}, {"title": "Layer normalization", "author": ["J.L. Ba", "J.R. Kiros", "G.E. Hinton"], "venue": "arXiv preprint arXiv:1607.06450", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2016}, {"title": "Revisiting distributed synchronous sgd", "author": ["J. Chen", "R. Monga", "S. Bengio", "R. Jozefowicz"], "venue": "arXiv preprint arXiv:1604.00981", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2016}, {"title": "Neighbourhood components analysis", "author": ["J. Goldberger", "S. Roweis", "G. Hinton", "R. Salakhutdinov"], "venue": "Advances in Neural Information Processing Systems 17", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2004}, {"title": "Deep residual learning for image recognition", "author": ["K. He", "X. Zhang", "S. Ren", "J. Sun"], "venue": "Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 770\u2013778", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2016}, {"title": "Batch normalization: Accelerating deep network training by reducing internal covariate shift", "author": ["S. Ioffe", "C. Szegedy"], "venue": "Proceedings of the 32nd International Conference on Machine Learning (ICML-15), pages 448\u2013456", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2015}, {"title": "Rectified linear units improve restricted boltzmann machines", "author": ["V. Nair", "G.E. Hinton"], "venue": "ICML, pages 807\u2013814. Omnipress", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2010}, {"title": "and L", "author": ["O. Russakovsky", "J. Deng", "H. Su", "J. Krause", "S. Satheesh", "S. Ma", "Z. Huang", "A. Karpathy", "A. Khosla", "M. Bernstein", "A.C. Berg"], "venue": "Fei-Fei. ImageNet Large Scale Visual Recognition Challenge", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2014}, {"title": "Improved techniques for training gans", "author": ["T. Salimans", "I. Goodfellow", "W. Zaremba", "V. Cheung", "A. Radford", "X. Chen"], "venue": "Advances in Neural Information Processing Systems, pages 2226\u20132234", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2016}, {"title": "Weight normalization: A simple reparameterization to accelerate training of deep neural networks", "author": ["T. Salimans", "D.P. Kingma"], "venue": "Advances in Neural Information Processing Systems, pages 901\u2013901", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2016}, {"title": "Facenet: A unified embedding for face recognition and clustering", "author": ["F. Schroff", "D. Kalenichenko", "J. Philbin"], "venue": "CoRR, abs/1503.03832", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2015}, {"title": "Rethinking the inception architecture for computer vision", "author": ["C. Szegedy", "V. Vanhoucke", "S. Ioffe", "J. Shlens", "Z. Wojna"], "venue": "Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 2818\u20132826", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2016}], "referenceMentions": [{"referenceID": 5, "context": "Batch Normalization (\u201cbatchnorm\u201d [6]) has recently become a part of the standard toolkit for training deep networks.", "startOffset": 33, "endOffset": 36}, {"referenceID": 4, "context": "Batchnorm has been successfully used to enable state-of-the-art architectures such as residual networks [5].", "startOffset": 104, "endOffset": 107}, {"referenceID": 3, "context": "[4]), it is common to bias the minibatch sampling to include sets of examples that are known to be related.", "startOffset": 0, "endOffset": 3}, {"referenceID": 0, "context": "Several approaches [1, 2, 10] have been proposed to alleviate this.", "startOffset": 19, "endOffset": 29}, {"referenceID": 1, "context": "Several approaches [1, 2, 10] have been proposed to alleviate this.", "startOffset": 19, "endOffset": 29}, {"referenceID": 9, "context": "Several approaches [1, 2, 10] have been proposed to alleviate this.", "startOffset": 19, "endOffset": 29}, {"referenceID": 8, "context": "Another alternative [9] is to use a separate and fixed minibatch to compute the normalization parameters, but this makes the training more expensive.", "startOffset": 20, "endOffset": 23}, {"referenceID": 5, "context": "Batch Normalization [6] addresses it by considering the values of x in a minibatch B = {x1.", "startOffset": 20, "endOffset": 23}, {"referenceID": 5, "context": "The backpropagation formulas for batchnorm are easy to derive by chain rule and are given in [6].", "startOffset": 93, "endOffset": 96}, {"referenceID": 5, "context": "As argued in [6], such use of moving averages would cause the gradient optimization and the normalization to counteract each other.", "startOffset": 13, "endOffset": 16}, {"referenceID": 11, "context": "Our baseline model is Inception v3 [12], trained on 1000 classes from ImageNet training set [8], and evaluated on the ImageNet validation data.", "startOffset": 35, "endOffset": 39}, {"referenceID": 7, "context": "Our baseline model is Inception v3 [12], trained on 1000 classes from ImageNet training set [8], and evaluated on the ImageNet validation data.", "startOffset": 92, "endOffset": 95}, {"referenceID": 6, "context": "In the baseline model, batchnorm was used after convolution and before the ReLU [7].", "startOffset": 80, "endOffset": 83}, {"referenceID": 2, "context": "The training used 50 synchronized workers [3].", "startOffset": 42, "endOffset": 45}, {"referenceID": 3, "context": "However, sampling with dependencies may be necessary for tasks such as for metric learning [4, 11].", "startOffset": 91, "endOffset": 98}, {"referenceID": 10, "context": "However, sampling with dependencies may be necessary for tasks such as for metric learning [4, 11].", "startOffset": 91, "endOffset": 98}, {"referenceID": 4, "context": "This includes Residual Networks [5].", "startOffset": 32, "endOffset": 35}, {"referenceID": 8, "context": "Another application is Generative Adversarial Networks [9], where the non-determinism introduced by batchnorm has been found to be an issue, and Batch Renormmay provide a solution.", "startOffset": 55, "endOffset": 58}], "year": 2017, "abstractText": "Batch Normalization is quite effective at accelerating and improving the training of deep models. However, its effectiveness diminishes when the training minibatches are small, or do not consist of independent samples. We hypothesize that this is due to the dependence of model layer inputs on all the examples in the minibatch, and different activations being produced between training and inference. We propose Batch Renormalization, a simple and effective extension to ensure that the training and inference models generate the same outputs that depend on individual examples rather than the entire minibatch. Models trained with Batch Renormalization perform substantially better than batchnorm when training with small or non-i.i.d. minibatches. At the same time, Batch Renormalization retains the benefits of batchnorm such as insensitivity to initialization and training efficiency.", "creator": "LaTeX with hyperref package"}}}