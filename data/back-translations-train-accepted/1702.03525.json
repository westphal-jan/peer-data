{"id": "1702.03525", "review": {"conference": "ACL", "VERSION": "v1", "DATE_OF_SUBMISSION": "12-Feb-2017", "title": "Learning to Parse and Translate Improves Neural Machine Translation", "abstract": "There has been relatively little attention to incorporating linguistic prior to neural machine translation. Much of the previous work was further constrained to considering linguistic prior on the source side. In this paper, we propose a hybrid model, called NMT+RG, that learns to parse and translate by combining the recurrent neural network grammar into the attention-based neural machine translation. Our approach encourages the neural machine translation model to incorporate linguistic prior during training, and lets it translate on its own afterward. Extensive experiments with four language pairs show the effectiveness of the proposed NMT+RG.", "histories": [["v1", "Sun, 12 Feb 2017 13:19:03 GMT  (135kb,D)", "https://arxiv.org/abs/1702.03525v1", null], ["v2", "Sun, 23 Apr 2017 16:52:03 GMT  (78kb,D)", "http://arxiv.org/abs/1702.03525v2", "Accepted as a short paper at the 55th Annual Meeting of the Association for Computational Linguistics (ACL 2017)"]], "reviews": [], "SUBJECTS": "cs.CL", "authors": ["akiko eriguchi", "yoshimasa tsuruoka", "kyunghyun cho"], "accepted": true, "id": "1702.03525"}, "pdf": {"name": "1702.03525.pdf", "metadata": {"source": "CRF", "title": "Learning to Parse and Translate Improves Neural Machine Translation", "authors": ["Akiko Eriguchi", "Yoshimasa Tsuruoka", "Kyunghyun Cho"], "emails": ["tsuruoka}@logos.t.u-tokyo.ac.jp", "kyunghyun.cho@nyu.edu"], "sections": [{"heading": "1 Introduction", "text": "We have shown in recent years that NMT systems work similarly to other systems, even if the source and target sentences are simply given as flat sequences of characters (Lee et al., 2016; Chung et al., 2016) or statistically, that they are not linguistically, but as subwords in the form of Sennrich et al., 2016; Shi et al., 2016; recently an observation was made that the encoder of NMT exhibits syntactic properties of a source set that indirectly indicate that explicit linguistic approaches are not necessary. There are only a few recent studies that show the potential benefits of the linguistic approach."}, {"heading": "2 Neural Machine Translation", "text": "Neural machine translation is a recently proposed framework for building a machine translation system based solely on neural networks. It is often constructed as an attention-based encoder decoder network (Cho et al., 2015) with two recurring networks - encoder and decoder - and an attention model. Often implemented as a bi-directional recursive network with long-term storage units (LSTM, Hochreiter and Schmidhuber, 1997) or gated recurrent units (GRU, Cho et al., 2014), the encoder first reads a source set that is represented as a sequence of words x = (x1, x2,., xN).The encoder returns a sequence of hidden states h = (h1, h2,., hN).Each hidden state hi is a concatenation of hi or the vector consisting of forward and backward networks \u2212 h = \u2212 h \u2212 h; hi \u2212 f."}, {"heading": "3 Recurrent Neural Network Grammars", "text": "A recursive neural network grammar (RNNG, Dyer et al., 2016) is a probabilistic syntax-based language model. In contrast to a common recursive language model (see e.g. Mikolov et al., 2010), an RNNG simultaneously models both tokens and their tree-based composition by giving an (output) buffer, a stack, and an action history, each of which is implemented as a stack LSTM (sLSTM, Dyer et al., 2015). At each step, the action sLSTM predicts the next action based on the (current) hidden states of the buffer, stack, and action sLSTM. That is, p (at = a < t) becomes a fraction (h stack t, h stack t, h action t), (4) with Wa being the vector of the action."}, {"heading": "4 Learning to Parse and Translate", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "4.1 NMT+RNNG", "text": "This year it has come to the point where it will be able to retaliate, \"he said in an interview with the\" Welt am Sonntag. \""}, {"heading": "4.2 Knowledge Distillation for Parsing", "text": "A major challenge in developing the proposed hybrid model is that there is no parallel corpus extended with a gold-standard target corpus, and vice versa. In other words, we must either analyze the target-side sentences of an existing parallel corpus or translate sentences with existing gold-standard parses. Since the proposed model's goal is translation, we start with a parallel corpus and comment on the target sentences. However, it is costly to comment manually on each appropriate-sized corpus (Table 6 in Alonso et al., 2016). Instead, we resort to loud but automated annotations using an existing parser. This approach of automated annotation can be prepared along the lines of the recently proposed techniques of knowledge distillation (Hinton et al., 2015) and remote monitoring (Mintz et al., 2009). In knowledge distillation, a teacher network is purely prepared for a training set of notes of truth."}, {"heading": "5 Experiments", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "5.1 Language Pairs and Corpora", "text": "We compare the proposed NMT + RNNG with the basic model on four different language pairs - Jp-En, Cs-En, De-En and Ru-En. The basic statistics of the training data are presented in Table 1. We have mapped all low frequency words to the unique symbol \"UNK\" and inserted a special symbol \"EOS\" at the end of the starting and finishing sentences. Yes We use the ASPEC corpus (\"train1.txt\") from the WAT '16 Jp-En translation task. We use each Japanese sentence with KyTea (Neubig et al., 2011) and pre-processing according to the recommendations from WAT' 16 (WAT, 2016). We use the first 100K sentence pairs shorter than 50 for the training. The vocabulary is constructed with all unique vocabulary that appear at least twice in the training corpus."}, {"heading": "5.2 Models, Learning and Inference", "text": "In all our experiments, each recursive network has a single layer of LSTM units of 256 dimensions, and the word vectors and action vectors are of 256 and 128 dimensions, respectively. To reduce the computational effort, we use BlackOut (Ji et al., 2015) with 2000 negative samples and \u03b1 = 0.4. When using BlackOut, we split the negative samples of each target word into a sentence in the training time (Hashimoto and Tsuruoka, 2017), which is similar to the previous work (Zoph et al., 2016). For the proposed NMT + RNG, we divide the target word vectors between the decoder and the stack sLSTM. Each weight is initialized from the uniform distribution [\u2212 0.1, 0.1]. The bias vectors and weights of Softmax and BlackOut are set to zero."}, {"heading": "5.3 Results and Analysis", "text": "In Table 2 we report on the translation qualities of the tested models on all four language pairs. We report on both BLEU (Papineni et al., 2002) and RIBES (Isozaki et al., 2010). With the exception of DeEn, measured in BLEU, we observe the statistically significant improvement by the proposed NMT + RNNG over the base model. It is worth noting that these significant improvements were achieved without additional parameters or computational effort in the inference time. Ablation Since any component in RNG can be omitted, we remove each component in the proposed NMT + RNNG to verify its need.5 As shown in Table 3, we see that the best performance could only be achieved if all three components were present."}, {"heading": "6 Conclusion", "text": "We propose a hybrid model, called NMT + RNNG, which combines the decoder of an attention-based neural translation model with the RNNG. This model learns to parse and translate simultaneously, and trains both the encoder and the decoder to better integrate linguistic precursors. Our experiments confirmed its effectiveness on four language pairs ({JP, Cs, De, Ru} En). In principle, the RNNG can be trained without ground truth parses, making external parsers completely unnecessary."}, {"heading": "Acknowledgments", "text": "We thank Yuchen Qiao and Kenjiro Taura for their help in accelerating the implementation of training and Kazuma Hashimoto for his valuable comments and discussions. This work was supported by JST CREST Grant Number JPMJCR1513 and JSPS KAKENHI Grant Numbers 15J12597 and16H01715. KC thanks the support of eBay, Facebook, Google and NVIDIA."}], "references": [{"title": "Towards string-to-tree neural machine translation", "author": ["Roee Aharoni", "Yoav Goldberg."], "venue": "Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics. to appear.", "citeRegEx": "Aharoni and Goldberg.,? 2017", "shortCiteRegEx": "Aharoni and Goldberg.", "year": 2017}, {"title": "From noisy questions to minecraft texts: Annotation challenges in extreme syntax scenario", "author": ["H\u00e9ctor Mart\u0131\u0301nez Alonso", "Djam\u00e9 Seddah", "Beno\u0131\u0302t Sagot"], "venue": "In Proceedings of the 2nd Workshop on Noisy User-generated Text (WNUT)", "citeRegEx": "Alonso et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Alonso et al\\.", "year": 2016}, {"title": "Tree-structured decoding with doubly-recurrent neural networks", "author": ["David Alvarez-Melis", "Tommi S. Jaakkola."], "venue": "Proceedings of International Conference on Learning Representations 2017.", "citeRegEx": "Alvarez.Melis and Jaakkola.,? 2017", "shortCiteRegEx": "Alvarez.Melis and Jaakkola.", "year": 2017}, {"title": "Globally normalized transition-based neural networks", "author": ["Daniel Andor", "Chris Alberti", "David Weiss", "Aliaksei Severyn", "Alessandro Presta", "Kuzman Ganchev", "Slav Petrov", "Michael Collins."], "venue": "Proceedings of the 54th Annual Meeting of the Asso-", "citeRegEx": "Andor et al\\.,? 2016", "shortCiteRegEx": "Andor et al\\.", "year": 2016}, {"title": "Multitask learning", "author": ["Rich Caruana."], "venue": "Learning to learn, Springer, pages 95\u2013133.", "citeRegEx": "Caruana.,? 1998", "shortCiteRegEx": "Caruana.", "year": 1998}, {"title": "Describing multimedia content using attention-based encoder-decoder networks", "author": ["Kyunghyun Cho", "Aaron Courville", "Yoshua Bengio."], "venue": "IEEE Transactions on Multimedia 17(11):1875\u20131886.", "citeRegEx": "Cho et al\\.,? 2015", "shortCiteRegEx": "Cho et al\\.", "year": 2015}, {"title": "Learning phrase representations using rnn encoder\u2013decoder for statistical machine translation", "author": ["Kyunghyun Cho", "Bart van Merrienboer", "Caglar Gulcehre", "Dzmitry Bahdanau", "Fethi Bougares", "Holger Schwenk", "Yoshua Bengio."], "venue": "Proceedings of", "citeRegEx": "Cho et al\\.,? 2014", "shortCiteRegEx": "Cho et al\\.", "year": 2014}, {"title": "A character-level decoder without explicit segmentation for neural machine translation", "author": ["Junyoung Chung", "Kyunghyun Cho", "Yoshua Bengio."], "venue": "Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics. Associ-", "citeRegEx": "Chung et al\\.,? 2016", "shortCiteRegEx": "Chung et al\\.", "year": 2016}, {"title": "Natural language processing (almost) from scratch", "author": ["Ronan Collobert", "Jason Weston", "L\u00e9on Bottou", "Michael Karlen", "Koray Kavukcuoglu", "Pavel Kuksa."], "venue": "Journal of Machine Learning Research 12:2493\u20132537.", "citeRegEx": "Collobert et al\\.,? 2011", "shortCiteRegEx": "Collobert et al\\.", "year": 2011}, {"title": "Transitionbased dependency parsing with stack long shortterm memory", "author": ["Chris Dyer", "Miguel Ballesteros", "Wang Ling", "Austin Matthews", "A. Noah Smith."], "venue": "Proceedings of the 53rd Annual", "citeRegEx": "Dyer et al\\.,? 2015", "shortCiteRegEx": "Dyer et al\\.", "year": 2015}, {"title": "Recurrent neural network grammars", "author": ["Chris Dyer", "Adhiguna Kuncoro", "Miguel Ballesteros", "A. Noah Smith."], "venue": "Proceedings of the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Lan-", "citeRegEx": "Dyer et al\\.,? 2016", "shortCiteRegEx": "Dyer et al\\.", "year": 2016}, {"title": "Tree-to-sequence attentional neural machine translation", "author": ["Akiko Eriguchi", "Kazuma Hashimoto", "Yoshimasa Tsuruoka."], "venue": "Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics. pages 823\u2013833.", "citeRegEx": "Eriguchi et al\\.,? 2016", "shortCiteRegEx": "Eriguchi et al\\.", "year": 2016}, {"title": "Neural Machine Translation with SourceSide Latent Graph Parsing", "author": ["Kazuma Hashimoto", "Yoshimasa Tsuruoka."], "venue": "arXiv preprint arXiv:1702.02265 .", "citeRegEx": "Hashimoto and Tsuruoka.,? 2017", "shortCiteRegEx": "Hashimoto and Tsuruoka.", "year": 2017}, {"title": "Distilling the knowledge in a neural network", "author": ["Geoffrey Hinton", "Oriol Vinyals", "Jeff Dean."], "venue": "arXiv preprint arXiv:1503.02531 .", "citeRegEx": "Hinton et al\\.,? 2015", "shortCiteRegEx": "Hinton et al\\.", "year": 2015}, {"title": "Long short-term memory", "author": ["Sepp Hochreiter", "J\u00fcrgen Schmidhuber."], "venue": "Neural Comput. 9(8):1735\u2013 1780.", "citeRegEx": "Hochreiter and Schmidhuber.,? 1997", "shortCiteRegEx": "Hochreiter and Schmidhuber.", "year": 1997}, {"title": "Automatic evaluation of translation quality for distant language pairs", "author": ["Hideki Isozaki", "Tsutomu Hirao", "Kevin Duh", "Katsuhito Sudoh", "Hajime Tsukada."], "venue": "Proceedings of the 2010 Conference on Empirical Methods in Natural Language Process-", "citeRegEx": "Isozaki et al\\.,? 2010", "shortCiteRegEx": "Isozaki et al\\.", "year": 2010}, {"title": "Blackout: Speeding up recurrent neural network language models with very large vocabularies", "author": ["Shihao Ji", "S.V.N. Vishwanathan", "Nadathur Satish", "Michael J. Anderson", "Pradeep Dubey."], "venue": "Proceedings of International Conference on Learning", "citeRegEx": "Ji et al\\.,? 2015", "shortCiteRegEx": "Ji et al\\.", "year": 2015}, {"title": "An empirical exploration of recurrent network architectures", "author": ["Rafal J\u00f3zefowicz", "Wojciech Zaremba", "Ilya Sutskever."], "venue": "Proceedings of the 32nd International Conference on Machine Learning. pages 2342\u20132350.", "citeRegEx": "J\u00f3zefowicz et al\\.,? 2015", "shortCiteRegEx": "J\u00f3zefowicz et al\\.", "year": 2015}, {"title": "Statistical significance tests for machine translation evaluation", "author": ["Philipp Koehn."], "venue": "Proceedings of the 2004 Conference on Empirical Methods in Natural Language Processing. pages 388\u2013395.", "citeRegEx": "Koehn.,? 2004", "shortCiteRegEx": "Koehn.", "year": 2004}, {"title": "What do recurrent neural network grammars learn about syntax", "author": ["Adhiguna Kuncoro", "Miguel Ballesteros", "Lingpeng Kong", "Chris Dyer", "Graham Neubig", "Noah A. Smith"], "venue": "In Proceedings of the 15th Conference of the European Chapter", "citeRegEx": "Kuncoro et al\\.,? \\Q2017\\E", "shortCiteRegEx": "Kuncoro et al\\.", "year": 2017}, {"title": "Fully character-level neural machine translation without explicit segmentation", "author": ["Jason Lee", "Kyunghyun Cho", "Thomas Hofmann."], "venue": "arXiv preprint arXiv:1610.03017 .", "citeRegEx": "Lee et al\\.,? 2016", "shortCiteRegEx": "Lee et al\\.", "year": 2016}, {"title": "Effective approaches to attention-based neural machine translation", "author": ["Thang Luong", "Hieu Pham", "Christopher D. Manning."], "venue": "Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing. pages 1412\u20131421.", "citeRegEx": "Luong et al\\.,? 2015", "shortCiteRegEx": "Luong et al\\.", "year": 2015}, {"title": "Recurrent neural network based language model", "author": ["Tom\u00e1\u0161 Mikolov", "Martin Karafi\u00e1t", "Luk\u00e1\u0161 Burget", "Jan \u010cernock\u00fd", "Sanjeev Khudanpur."], "venue": "Proceedings of the 11th Annual Conference of the International Speech Communication Association (INTER-", "citeRegEx": "Mikolov et al\\.,? 2010", "shortCiteRegEx": "Mikolov et al\\.", "year": 2010}, {"title": "Distant supervision for relation extraction without labeled data", "author": ["Mike Mintz", "Steven Bills", "Rion Snow", "Dan Jurafsky."], "venue": "Proceedings of the Joint Conference of the 47th Annual Meeting of the ACL and the 4th International Joint Conference on", "citeRegEx": "Mintz et al\\.,? 2009", "shortCiteRegEx": "Mintz et al\\.", "year": 2009}, {"title": "Pointwise prediction for robust, adaptable japanese morphological analysis", "author": ["Graham Neubig", "Yosuke Nakata", "Shinsuke Mori."], "venue": "Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technolo-", "citeRegEx": "Neubig et al\\.,? 2011", "shortCiteRegEx": "Neubig et al\\.", "year": 2011}, {"title": "Bleu: A method for automatic evaluation of machine translation", "author": ["Kishore Papineni", "Salim Roukos", "Todd Ward", "WeiJing Zhu."], "venue": "Proceedings of the 40th Annual Meeting on Association for Computational Linguistics. pages 311\u2013318.", "citeRegEx": "Papineni et al\\.,? 2002", "shortCiteRegEx": "Papineni et al\\.", "year": 2002}, {"title": "Understanding the exploding gradient problem", "author": ["Razvan Pascanu", "Tomas Mikolov", "Yoshua Bengio."], "venue": "arXiv preprint arXiv:1211.5063 abs/1211.5063.", "citeRegEx": "Pascanu et al\\.,? 2012", "shortCiteRegEx": "Pascanu et al\\.", "year": 2012}, {"title": "Linguistic input features improve neural machine translation", "author": ["Rico Sennrich", "Barry Haddow."], "venue": "Proceedings of the First Conference on Machine Translation. pages 83\u201391.", "citeRegEx": "Sennrich and Haddow.,? 2016", "shortCiteRegEx": "Sennrich and Haddow.", "year": 2016}, {"title": "Neural machine translation of rare words with subword units", "author": ["Rico Sennrich", "Barry Haddow", "Alexandra Birch."], "venue": "Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics. pages 1715\u20131725.", "citeRegEx": "Sennrich et al\\.,? 2016", "shortCiteRegEx": "Sennrich et al\\.", "year": 2016}, {"title": "Does string-based neural mt learn source syntax? In Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing", "author": ["Xing Shi", "Inkit Padhi", "Kevin Knight."], "venue": "pages 1526\u2013 1534.", "citeRegEx": "Shi et al\\.,? 2016", "shortCiteRegEx": "Shi et al\\.", "year": 2016}, {"title": "Syntactically guided neural machine translation", "author": ["Felix Stahlberg", "Eva Hasler", "Aurelien Waite", "Bill Byrne."], "venue": "Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics. pages 299\u2013305.", "citeRegEx": "Stahlberg et al\\.,? 2016", "shortCiteRegEx": "Stahlberg et al\\.", "year": 2016}, {"title": "Simple, Fast Noise-Contrastive Estimation for Large RNN Vocabularies", "author": ["Barret Zoph", "Ashish Vaswani", "Jonathan May", "Kevin Knight."], "venue": "Proceedings of the 2016 Conference of the North American Chapter of the Association for Computational", "citeRegEx": "Zoph et al\\.,? 2016", "shortCiteRegEx": "Zoph et al\\.", "year": 2016}], "referenceMentions": [{"referenceID": 20, "context": "Some of the most recent studies have for instance demonstrated that NMT systems work comparably to other systems even when the source and target sentences are given simply as flat sequences of characters (Lee et al., 2016; Chung et al., 2016) or statistically, not linguistically, motivated subword units (Sennrich et al.", "startOffset": 204, "endOffset": 242}, {"referenceID": 7, "context": "Some of the most recent studies have for instance demonstrated that NMT systems work comparably to other systems even when the source and target sentences are given simply as flat sequences of characters (Lee et al., 2016; Chung et al., 2016) or statistically, not linguistically, motivated subword units (Sennrich et al.", "startOffset": 204, "endOffset": 242}, {"referenceID": 28, "context": ", 2016) or statistically, not linguistically, motivated subword units (Sennrich et al., 2016; Wu et al., 2016).", "startOffset": 70, "endOffset": 110}, {"referenceID": 7, "context": ", 2016; Chung et al., 2016) or statistically, not linguistically, motivated subword units (Sennrich et al., 2016; Wu et al., 2016). Shi et al. (2016) recently made an observation that the encoder of NMT captures syntactic properties of a source sentence automatically, indirectly suggesting that explicit linguistic prior may not be necessary.", "startOffset": 8, "endOffset": 150}, {"referenceID": 26, "context": "Sennrich and Haddow (2016) for instance proposed to augment each source word with its corresponding part-of-speech tag, lemmatized form and dependency label.", "startOffset": 0, "endOffset": 27}, {"referenceID": 11, "context": "Eriguchi et al. (2016) instead replaced the sequential encoder with a tree-based encoder which computes the representation of the source sentence following its parse tree.", "startOffset": 0, "endOffset": 23}, {"referenceID": 11, "context": "Eriguchi et al. (2016) instead replaced the sequential encoder with a tree-based encoder which computes the representation of the source sentence following its parse tree. Stahlberg et al. (2016) let the lattice from a hierarchical phrase-based system guide the decod-", "startOffset": 0, "endOffset": 196}, {"referenceID": 2, "context": "Alvarez-Melis and Jaakkola (2017) have proposed a doubly-recurrent neural network that", "startOffset": 0, "endOffset": 34}, {"referenceID": 0, "context": "Aharoni and Goldberg (2017) introduced a method to serialize a parsed tree and to train the serialized parsed sentences.", "startOffset": 0, "endOffset": 28}, {"referenceID": 4, "context": "We propose to implicitly incorporate linguistic prior based on the idea of multi-task learning (Caruana, 1998; Collobert et al., 2011).", "startOffset": 95, "endOffset": 134}, {"referenceID": 8, "context": "We propose to implicitly incorporate linguistic prior based on the idea of multi-task learning (Caruana, 1998; Collobert et al., 2011).", "startOffset": 95, "endOffset": 134}, {"referenceID": 3, "context": "We use an external parser (Andor et al., 2016) to generate target parse actions, but unlike the previous explicit approaches, we do not need it during test time.", "startOffset": 26, "endOffset": 46}, {"referenceID": 5, "context": "It is often built as an attention-based encoder-decoder network (Cho et al., 2015) with two recurrent networks\u2014encoder and decoder\u2014and an attention model.", "startOffset": 64, "endOffset": 82}, {"referenceID": 21, "context": "The attention model first compares the current hidden state sj against each of the hidden states and assigns a scalar score: \u03b2i,j = exp(hi Wdsj) (Luong et al., 2015).", "startOffset": 145, "endOffset": 165}, {"referenceID": 9, "context": "where rd and rp are the corresponding vectors of the parent and dependent phrases, respectively (Dyer et al., 2015).", "startOffset": 96, "endOffset": 115}, {"referenceID": 10, "context": "Note that the original paper of RNNG (Dyer et al., 2016) uses constituency trees, but we employ dependency trees in this paper.", "startOffset": 37, "endOffset": 56}, {"referenceID": 13, "context": "This approach of automated annotation can be considered along the line of recently proposed techniques of knowledge distillation (Hinton et al., 2015) and distant supervision (Mintz et al.", "startOffset": 129, "endOffset": 150}, {"referenceID": 23, "context": ", 2015) and distant supervision (Mintz et al., 2009).", "startOffset": 32, "endOffset": 52}, {"referenceID": 3, "context": "Specifically, we use SyntaxNet, released by Andor et al. (2016), on a target sentence.", "startOffset": 44, "endOffset": 64}, {"referenceID": 24, "context": "We tokenize each Japanese sentence with KyTea (Neubig et al., 2011) and preprocess according to the recommendations from WAT\u201916 (WAT, 2016).", "startOffset": 46, "endOffset": 67}, {"referenceID": 16, "context": "To reduce computational overhead, we use BlackOut (Ji et al., 2015) with 2000 negative samples and \u03b1 =", "startOffset": 50, "endOffset": 67}, {"referenceID": 12, "context": "When employing BlackOut, we shared the negative samples of each target word in a sentence in training time (Hashimoto and Tsuruoka, 2017), which is similar to the previous work (Zoph et al.", "startOffset": 107, "endOffset": 137}, {"referenceID": 31, "context": "When employing BlackOut, we shared the negative samples of each target word in a sentence in training time (Hashimoto and Tsuruoka, 2017), which is similar to the previous work (Zoph et al., 2016).", "startOffset": 177, "endOffset": 196}, {"referenceID": 17, "context": "The forget gate biases of LSTMs and Stack-LSTMs are initialized to 1 as recommended in J\u00f3zefowicz et al. (2015). We use stochastic gradient descent with minibatches of 128 examples.", "startOffset": 87, "endOffset": 112}, {"referenceID": 26, "context": "We clip the norm of the gradient (Pascanu et al., 2012) with the threshold set to 3.", "startOffset": 33, "endOffset": 55}, {"referenceID": 18, "context": "We use the bootstrap resampling method from Koehn (2004) to compute the statistical significance.", "startOffset": 44, "endOffset": 57}, {"referenceID": 25, "context": "We report both BLEU (Papineni et al., 2002) and", "startOffset": 20, "endOffset": 43}, {"referenceID": 15, "context": "RIBES (Isozaki et al., 2010).", "startOffset": 6, "endOffset": 28}, {"referenceID": 19, "context": "Removing the stack had the most adverse effect, which was found to be the case for parsing as well by Kuncoro et al. (2017).", "startOffset": 102, "endOffset": 124}], "year": 2017, "abstractText": "There has been relatively little attention to incorporating linguistic prior to neural machine translation. Much of the previous work was further constrained to considering linguistic prior on the source side. In this paper, we propose a hybrid model, called NMT+RNNG, that learns to parse and translate by combining the recurrent neural network grammar into the attention-based neural machine translation. Our approach encourages the neural machine translation model to incorporate linguistic prior during training, and lets it translate on its own afterward. Extensive experiments with four language pairs show the effectiveness of the proposed NMT+RNNG.", "creator": "LaTeX with hyperref package"}}}