{"id": "1702.03859", "review": {"conference": "iclr", "VERSION": "v1", "DATE_OF_SUBMISSION": "13-Feb-2017", "title": "Offline bilingual word vectors, orthogonal transformations and the inverted softmax", "abstract": "Usually bilingual word vectors are trained \"online\". Mikolov et al. showed they can also be found \"offline\", whereby two pre-trained embeddings are aligned with a linear transformation, using dictionaries compiled from expert knowledge. In this work, we prove that the linear transformation between two spaces should be orthogonal. This transformation can be obtained using the singular value decomposition. We introduce a novel \"inverted softmax\" for identifying translation pairs, with which we improve the precision @1 of Mikolov's original mapping from 34% to 43%, when translating a test set composed of both common and rare English words into Italian. Orthogonal transformations are more robust to noise, enabling us to learn the transformation without expert bilingual signal by constructing a \"pseudo-dictionary\" from the identical character strings which appear in both languages, achieving 40% precision on the same test set. Finally, we extend our method to retrieve the true translations of English sentences from a corpus of 200k Italian sentences with a precision @1 of 68%.", "histories": [["v1", "Mon, 13 Feb 2017 16:31:06 GMT  (340kb,D)", "http://arxiv.org/abs/1702.03859v1", "Accepted to conference track at ICLR 2017"]], "COMMENTS": "Accepted to conference track at ICLR 2017", "reviews": [], "SUBJECTS": "cs.CL cs.AI cs.IR", "authors": ["samuel l smith", "david h p turban", "steven hamblin", "nils y hammerla"], "accepted": true, "id": "1702.03859"}, "pdf": {"name": "1702.03859.pdf", "metadata": {"source": "CRF", "title": null, "authors": ["INVERTED SOFTMAX", "Samuel L. Smith", "David H. P. Turban", "Steven Hamblin", "Nils Y. Hammerla"], "emails": ["nils.hammerla}@babylonhealth.com", "dt382@cam.ac.uk", "@1", "@1"], "sections": [{"heading": "1 INTRODUCTION", "text": "This year, more than ever before in the history of the country in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is not a country, but in which it is a country, a country, a city and a country."}, {"heading": "2 OFFLINE BILINGUAL LANGUAGE VECTORS", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "2.1 PREVIOUS WORK", "text": "Offline bilingual word vectors were first proposed by Mikolov et al. (2013a), who received a small dictionary of word pairs from Google Translate, whose word vectors we identified as {yi, xi} ni = 1. Next, they applied a linear transformation W to the source language and used stochastic origin of words to minimize the square reconstruction error. Surprisingly, the similarity between a source vector x and a target vector yt can then be assigned to the target by the cosmic similarity cos (successte) = yTt ye / | ye. Surprisingly, this simple method was calculated by calculating ye = Wx. The similarity between a source vector x and a target vector yt can then be evaluated by the cosmic similarity cos."}, {"heading": "2.2 THE SIMILARITY MATRIX AND THE ORTHOGONAL TRANSFORM", "text": "To prove that a self-consistent linear mapping between semantic spaces must be orthogonal, we must therefore form the similarity matrix, S = YWXT. X and Y are word vector matrices for each language, in which each line contains a single word denoted by lowercase letters x and y. The matrix element, Sij = y T i i Wxj (2) = yi \u00b7 (Wxj), (3) evaluates the similarity between the jth source word and the target word. The matrix W maps the source language into the target language. The largest value in a column of the similarity matrix gives the most similar target word to a given source word, while the largest value in one line gives the most similar source word to a given target word. However, we could also form a second similarity matrix S, so that the matrix Q forms the target language back into the source language."}, {"heading": "2.3 THE SVD AND CCA", "text": "Our method is very similar to Faruqui & Dyer's CCA method (2014), which can also be achieved with the help of SVD (Press, 2011). First, we obtain the source dictionary matrix XD and subtract the mean from each column of that matrix to X \"D. We then perform our first SVD to obtain X\" D = QDXV T X. This last step is identical to the alignment method we introduced above. Finally, we obtain X \"and Y\" by subtracting the mean of each column in XD and YD, and then another SVD on the product M \"= QTDWD = U.\" This last step is identical to the alignment method we introduced above. Finally, we obtain X \"and Y\" by subtracting the mean of each column in XD and YD, before creating a new pair of aligned representations of the complete vocabularies, Qaligned = X \u2212 U and \"Vigned = Y.\""}, {"heading": "2.4 THE INVERTED SOFTMAX", "text": "Mikolov et al. (2013a) have predicted the translation of a source word xj by finding the target word yi that comes closest to Wxj. In our formalism, this corresponds to the search for the largest entry in the jth column of the similarity matrix. To estimate our confidence in this prediction, we could form the Softmax, Pj \u2192 i = e\u03b2Sij \u2211 m e \u03b2Smj. (10) To learn the \"reverse temperature\" \u03b2, we maximize the likelihood of the protocol via the training dictionary, max \u03b2 pairs ij ln (Pj \u2192 i). (11) This sum should only be done using valid translation pairs. Dinu et al. (2014) have shown that the next neighbor restoration is flawed because it suffers from the presence of \"hubs.\" Hubs are words that appear to be the nearest target word maximized and reduce the translation performance."}, {"heading": "2.5 PSEUDO DICTIONARIES", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "2.5.1 IDENTICAL CHARACTER STRINGS", "text": "Our method requires a training dictionary of paired vectors, which is used to derive the orthogonal map O and the inverse temperature \u03b2, and also as a validation sentence during dimensionality reduction. Typically, this dictionary is obtained by translating common source words into the target language using Google Translate, constructed with human expertise. However, most European languages share a large number of words consisting of identical strings. Words such as \"London,\" \"DNA\" and \"Tortilla.\" Identical strings in two languages probably have similar meanings. We can extract these strings and form a \"pseudo-dictionary\" created without any bilingual expertise. Below, we show that this pseudo-dictionary is sufficient to successfully translate between English and Italian with high precision."}, {"heading": "2.5.2 ALIGNED SENTENCES", "text": "Chandar et al. (2014) have shown that such corpus can be used alongside monolingual text sources to learn online bilingual vectors, but to date offline bilingual vectors have only been obtained from dictionaries. To learn the orthogonal transformation from aligned sentences, we define the vector q of a source word sentence by a normalized sum of the existing word vectors, q = \u2211 i xi / | \u2211 i xi |. The vector w of a target-language sentence is defined by a normalized sum of word vectors yi. We consider the aligned corpus as a dictionary of compound sentences {wi, qi} from which we form two dictionaries, the matrices WD and QD. We obtain the transformation O from an SVD on the matrix M = WTDQD, and use this transformation to translate individual words into composite sentences."}, {"heading": "3 EXPERIMENTS", "text": "We conduct our experiments with the same word vectors by analysing the dictionary and the test dictionary provided by Dinu et al. (2014), which were formed using word2vec, and then extracted the 200k most common words in both the English and Italian corpuss. the training dictionary includes 5k common English words and their Italian translations, while the test set consists of 1500 English words and their Italian translations, which is divided into five sentences of 300 each. The first 300 words are derived from the most common 5k words in the English corpus, the next 300 from the 5k-20k most common words and their Italian translations, while the test set consists of 1500 English words and their Italian translations. The first 300 words are derived from the most common 5k words in the English corpus, the next 300 from the 5k-20k most common words, followed by bins for the 20k-100k, and 100k-100k words."}, {"heading": "3.1 EXPERIMENTS USING THE EXPERT TRAINING DICTIONARY", "text": "In Tables 1 and 2 we present the translation performance of our methods for translating the test sentence between English and Italian, using the expert dictionary provided by Dinu. We rate Mikolov and Dinu's methods for comparison, as well as CCA suggested by Faruqui & Dyer (2014). All methods are more accurate when translating from English to Italian. This is not surprising since some English words in the test sentence can be translated into either the male or female form of the Italian word. In the fourth column, we first evaluate the performance of our SVD method with the nearest call. This already represents a significant improvement in Mikolov's mapping, especially when translating from Italian to English. As expected, the performance of the SVD CCA is very similar. In the following two columns, we first apply the reverse Softmax method, and then the dimensionality reduction to the vector space obtained from SVD."}, {"heading": "3.2 EXPERIMENTS USING IDENTICAL CHARACTER STRINGS", "text": "In the previous section, we reported on our performance when we do not use this dictionary and instead construct a pseudo-dictionary consisting of exactly the same string from the word list that occurs in both English and Italian vocabulary. Remarkably, 47074 such identical strings appear in both words. There would be fewer identical entries for more different language pairs, but our main goal here is to demonstrate the superior robustness of orthogonal transformations into low-quality dictionaries. Our results are shown in Table 4, where we evaluate our method (SVD + inverted Softmax + dimensionality reduction) when we translate either from English to Italian or from Italian to English. Even if we use this pseudo-dictionary without bilingual expertise, we still achieve an average translation performance @ 1% of Italian + dimensionality reduction when we translate from either English to Italian or from Italian."}, {"heading": "3.3 EXPERIMENTS ON THE EUROPARL CORPUS OF ALIGNED SENTENCES", "text": "In fact, the fact is that most of them are not a mere formation, but a group of people who are able to assert their interests."}, {"heading": "4 SUMMARY", "text": "We proved that the optimal linear transformation between word vector spaces should be orthogonal and can be achieved by a single application of the SVD to a dictionary of translation pairs, as independently proposed by Artetxe et al. (2016). We used the SVD to obtain bilingual word vectors from which we can predict the translations of previously invisible words. We introduced a novel \"inverted Softmax,\" which significantly increased the accuracy of our predicted translations. By combining the SVD with the inverted Softmax and dimensionality reduction, we improved the translation accuracy of Mikolov's original linear mapping from 34% to 43%, when translating a test sentence composed of ordinary and rare English words into Italian. This was achieved by using a training dictionary of 5k English words and their Italian translations. Replacing this training dictionary with a pseudo-lectorial dictionary, we can still achieve the same number of words we use in both stringent languages."}, {"heading": "ACKNOWLEDGMENTS", "text": "We thank Dinu et al. for providing their source code, pre-formed word vectors and a training and test dictionary for English and Italian words, as well as Philipp Koehn for compiling the Europarl corpus."}, {"heading": "A THE ORTHOGONAL PROCRUSTES PROBLEM", "text": "There is no intuitive analytical solution to the cost function in Eq.6, but there is an analytical solution to the closely related \"orthogonal procrust problem,\" which minimizes the quadratic reconstruction error that is subject to orthogonal constraint (Scho Sch\u00f6nemann, 1966), min O n \u2211 i = 1 | | yi \u2212 Oxi | | 2, subject to OTO = I. (13) However, both X and Y are normalized while O maintains the vector norm. We note that the solution of the orthogonal procrust problem is presented in the main text."}, {"heading": "B ADDITIONAL EXPERIMENTS", "text": "In Tables 7 and 8, we provide results with accuracy @ 5 and @ 10, for the same experiments as shown in Table 6 of the main text @ 1. Once again, the inverted Softmax performs well when retrieving Italian translations of English sentences, but is less effective when translating Italian sentences into English. However, the performance of Dinus Method seems to increase faster than that of other methods when we go from precision @ 1 to @ 5 to @ 10. In addition, Dinus Method performs better when using the dictionary @ 1, but prefers the vocabulary dictionaries @ 5 and @ 10."}], "references": [{"title": "Massively multilingual word embeddings", "author": ["Waleed Ammar", "George Mulcaire", "Yulia Tsvetkov", "Guillaume Lample", "Chris Dyer", "Noah A Smith"], "venue": null, "citeRegEx": "Ammar et al\\.,? \\Q1925\\E", "shortCiteRegEx": "Ammar et al\\.", "year": 1925}, {"title": "Learning principled bilingual mappings of word embeddings while preserving monolingual invariance", "author": ["M Artetxe", "G Labaka", "E Agirre"], "venue": "Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing,", "citeRegEx": "Artetxe et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Artetxe et al\\.", "year": 2016}, {"title": "An autoencoder approach to learning bilingual word representations", "author": ["Sarath Chandar", "Stanislas Lauly", "Hugo Larochelle", "Mitesh Khapra", "Balaraman Ravindran", "Vikas C Raykar", "Amrita Saha"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Chandar et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Chandar et al\\.", "year": 2014}, {"title": "Improving zero-shot learning by mitigating the hubness problem", "author": ["Georgiana Dinu", "Angeliki Lazaridou", "Marco Baroni"], "venue": null, "citeRegEx": "Dinu et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Dinu et al\\.", "year": 2014}, {"title": "Improving vector space word representations using multilingual correlation", "author": ["Manaal Faruqui", "Chris Dyer"], "venue": "In Proceedings of the 2014 conference of the Association for Computational Linguistics. Association for Computational Linguistics,", "citeRegEx": "Faruqui and Dyer.,? \\Q2014\\E", "shortCiteRegEx": "Faruqui and Dyer.", "year": 2014}, {"title": "Bilbowa: Fast bilingual distributed representations without word alignments", "author": ["Stephan Gouws", "Yoshua Bengio", "Greg Corrado"], "venue": "In Proceedings of The 32nd International Conference on Machine Learning,", "citeRegEx": "Gouws et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Gouws et al\\.", "year": 2015}, {"title": "Learning bilingual lexicons from monolingual corpora", "author": ["Aria Haghighi", "Percy Liang", "Taylor Berg-Kirkpatrick", "Dan Klein"], "venue": "In ACL,", "citeRegEx": "Haghighi et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Haghighi et al\\.", "year": 2008}, {"title": "Multilingual distributed representations without word alignment", "author": ["Karl Moritz Hermann", "Phil Blunsom"], "venue": "arXiv preprint arXiv:1312.6173,", "citeRegEx": "Hermann and Blunsom.,? \\Q2013\\E", "shortCiteRegEx": "Hermann and Blunsom.", "year": 2013}, {"title": "Convolutional neural networks for sentence classification", "author": ["Yoon Kim"], "venue": null, "citeRegEx": "Kim.,? \\Q2014\\E", "shortCiteRegEx": "Kim.", "year": 2014}, {"title": "Skip-thought vectors. In Advances in neural information processing", "author": ["Ryan Kiros", "Yukun Zhu", "Ruslan R Salakhutdinov", "Richard Zemel", "Raquel Urtasun", "Antonio Torralba", "Sanja Fidler"], "venue": null, "citeRegEx": "Kiros et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Kiros et al\\.", "year": 2015}, {"title": "Inducing crosslingual distributed representations of words", "author": ["Alexandre Klementiev", "Ivan Titov", "Binod Bhattarai"], "venue": null, "citeRegEx": "Klementiev et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Klementiev et al\\.", "year": 2012}, {"title": "Europarl: A parallel corpus for statistical machine translation", "author": ["Philipp Koehn"], "venue": "In MT summit,", "citeRegEx": "Koehn.,? \\Q2005\\E", "shortCiteRegEx": "Koehn.", "year": 2005}, {"title": "Distributed representations of sentences and documents", "author": ["Quoc V Le", "Tomas Mikolov"], "venue": "In ICML,", "citeRegEx": "Le and Mikolov.,? \\Q2014\\E", "shortCiteRegEx": "Le and Mikolov.", "year": 2014}, {"title": "Deep multilingual correlation for improved word embeddings", "author": ["Ang Lu", "Weiran Wang", "Mohit Bansal", "Kevin Gimpel", "Karen Livescu"], "venue": "In Proceedings of NAACL,", "citeRegEx": "Lu et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Lu et al\\.", "year": 2015}, {"title": "Exploiting similarities among languages for machine translation", "author": ["Tomas Mikolov", "Quoc V Le", "Ilya Sutskever"], "venue": null, "citeRegEx": "Mikolov et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Mikolov et al\\.", "year": 2013}, {"title": "Distributed representations of words and phrases and their compositionality", "author": ["Tomas Mikolov", "Ilya Sutskever", "Kai Chen", "Greg S Corrado", "Jeff Dean"], "venue": "In Advances in neural information processing systems,", "citeRegEx": "Mikolov et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Mikolov et al\\.", "year": 2013}, {"title": "Canonical correlation clarified by singular value decomposition", "author": ["William H Press"], "venue": null, "citeRegEx": "Press.,? \\Q2011\\E", "shortCiteRegEx": "Press.", "year": 2011}, {"title": "A generalized solution of the orthogonal procrustes problem", "author": ["Peter H Sch\u00f6nemann"], "venue": null, "citeRegEx": "Sch\u00f6nemann.,? \\Q1966\\E", "shortCiteRegEx": "Sch\u00f6nemann.", "year": 1966}, {"title": "Normalized word embedding and orthogonal transform for bilingual word translation", "author": ["Chao Xing", "Dong Wang", "Chao Liu", "Yiye Lin"], "venue": "In Proceedings of the 2015 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies,", "citeRegEx": "Xing et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Xing et al\\.", "year": 2015}, {"title": "Bilingual word embeddings for phrase-based machine translation", "author": ["Will Y Zou", "Richard Socher", "Daniel M Cer", "Christopher D Manning"], "venue": "In EMNLP,", "citeRegEx": "Zou et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Zou et al\\.", "year": 2013}], "referenceMentions": [{"referenceID": 14, "context": "Mikolov et al. (2013a) showed they can also be found \u201coffline\u201d; whereby two pre-trained embeddings are aligned with a linear transformation, using dictionaries compiled from expert knowledge.", "startOffset": 0, "endOffset": 23}, {"referenceID": 8, "context": "They enable us to train sophisticated classifiers to interpret free flowing text (Kim, 2014), but they require independent models to be trained for each language.", "startOffset": 81, "endOffset": 92}, {"referenceID": 19, "context": "Such vectors may drive improvements in machine translation (Zou et al., 2013), and enable language-agnostic text classifiers (Klementiev et al.", "startOffset": 59, "endOffset": 77}, {"referenceID": 10, "context": ", 2013), and enable language-agnostic text classifiers (Klementiev et al., 2012).", "startOffset": 55, "endOffset": 80}, {"referenceID": 2, "context": "Bilingual vectors are normally trained \u201conline\u201d, whereby both languages are learnt together in a shared space (Chandar et al., 2014; Hermann & Blunsom, 2013).", "startOffset": 110, "endOffset": 157}, {"referenceID": 2, "context": "Bilingual vectors are normally trained \u201conline\u201d, whereby both languages are learnt together in a shared space (Chandar et al., 2014; Hermann & Blunsom, 2013). Typically these algorithms exploit two sources of monolingual text alongside a smaller bilingual corpus of aligned sentences. This bilingual signal provides a regularisation term, which penalises the embeddings if similar words in the two languages do not lie nearby in the vector space. However Mikolov et al. (2013a) showed that bilingual word vectors can also be obtained \u201coffline\u201d.", "startOffset": 111, "endOffset": 478}, {"referenceID": 3, "context": "We build on the work of Dinu et al. (2014), by introducing a novel \u201cinverted softmax\u201d to combat the hubness problem.", "startOffset": 24, "endOffset": 43}, {"referenceID": 14, "context": "Offline bilingual word vectors were first proposed by Mikolov et al. (2013a). They obtained a small dictionary of paired words from Google Translate, whose word vectors we denote {yi, xi}i=1.", "startOffset": 54, "endOffset": 77}, {"referenceID": 6, "context": "CCA had previously been used to iteratively extract translation pairs directly from monolingual corpora (Haghighi et al., 2008).", "startOffset": 104, "endOffset": 127}, {"referenceID": 13, "context": ", 2016), and non-linear \u201cdeep CCA\u201d has been introduced (Lu et al., 2015).", "startOffset": 55, "endOffset": 72}, {"referenceID": 1, "context": "We note that the cost function above is solved by the method of least squares, as realised by Dinu et al. (2014). They did not modify this cost function, but proposed an adapted method of retrieving translation pairs which was more accurate when translating words from English to Italian.", "startOffset": 94, "endOffset": 113}, {"referenceID": 1, "context": "We note that the cost function above is solved by the method of least squares, as realised by Dinu et al. (2014). They did not modify this cost function, but proposed an adapted method of retrieving translation pairs which was more accurate when translating words from English to Italian. Faruqui & Dyer (2014) obtained bilingual word vectors using CCA.", "startOffset": 94, "endOffset": 311}, {"referenceID": 1, "context": "We note that the cost function above is solved by the method of least squares, as realised by Dinu et al. (2014). They did not modify this cost function, but proposed an adapted method of retrieving translation pairs which was more accurate when translating words from English to Italian. Faruqui & Dyer (2014) obtained bilingual word vectors using CCA. They did not attempt any translation tasks, but showed that the combination of CCA and dimensionality reduction improved the performance of monolingual vectors on standard evaluation tasks. CCA had previously been used to iteratively extract translation pairs directly from monolingual corpora (Haghighi et al., 2008). More recently, Xing et al. (2015) argued that Mikolov\u2019s linear matrix should be orthogonal, and introduced an approximate procedure composed of gradient descent updates and repeated applications of the SVD.", "startOffset": 94, "endOffset": 707}, {"referenceID": 0, "context": "CCA has been extended to map 59 languages into a single shared space (Ammar et al., 2016), and non-linear \u201cdeep CCA\u201d has been introduced (Lu et al., 2015). A theoretical analysis of bilingual word vectors similar to this paper was recently published by Artetxe et al. (2016).", "startOffset": 70, "endOffset": 275}, {"referenceID": 1, "context": "It was recently independently proposed by Artetxe et al. (2016), and provides a numerically exact solution to the cost function proposed by Xing et al.", "startOffset": 42, "endOffset": 64}, {"referenceID": 1, "context": "It was recently independently proposed by Artetxe et al. (2016), and provides a numerically exact solution to the cost function proposed by Xing et al. (2015), just as the method of least squares provides a numerically exact solution to the cost function of Mikolov et al.", "startOffset": 42, "endOffset": 159}, {"referenceID": 1, "context": "It was recently independently proposed by Artetxe et al. (2016), and provides a numerically exact solution to the cost function proposed by Xing et al. (2015), just as the method of least squares provides a numerically exact solution to the cost function of Mikolov et al. (2013a). Our procedure did not use the singular values \u03a3, but these values do carry relevant information.", "startOffset": 42, "endOffset": 281}, {"referenceID": 16, "context": "Our method is very similar to the CCA procedure proposed by Faruqui & Dyer (2014), which can also be obtained using the SVD (Press, 2011).", "startOffset": 124, "endOffset": 137}, {"referenceID": 3, "context": "Dinu et al. (2014) demonstrated that nearest neighbour retrieval is flawed, since it suffers from the presence of \u201chubs\u201d.", "startOffset": 0, "endOffset": 19}, {"referenceID": 11, "context": "The Europarl corpus is composed of aligned sentences in a number of European languages (Koehn, 2005).", "startOffset": 87, "endOffset": 100}, {"referenceID": 2, "context": "Chandar et al. (2014) showed that such corpora can be used alongside monolingual text sources to learn online bilingual vectors, but to date, offline bilingual vectors have only been obtained from dictionaries.", "startOffset": 0, "endOffset": 22}, {"referenceID": 3, "context": "We perform our experiments using the same word vectors, training dictionary and test dictionary provided by Dinu et al. (2014)1.", "startOffset": 108, "endOffset": 127}, {"referenceID": 6, "context": "Previously translation pairs have been extracted from monolingual corpora using CCA by bootstrapping a small seed lexicon (Haghighi et al., 2008).", "startOffset": 122, "endOffset": 145}, {"referenceID": 11, "context": "The English-Italian Europarl corpus comprises 2 million English sentences and their Italian translations, taken from the proceedings of the European parliament (Koehn, 2005).", "startOffset": 160, "endOffset": 173}, {"referenceID": 5, "context": "However our performance appears competitive with Bilbowa, a leading method for learning bilingual vectors online from monolingual corpora and aligned text (Gouws et al., 2015).", "startOffset": 155, "endOffset": 175}, {"referenceID": 9, "context": "It is likely that we could improve on these results if we used higher quality sentence vectors (Le & Mikolov, 2014; Kiros et al., 2015), although we might lose the ability to simultaneously align the underlying word vector space.", "startOffset": 95, "endOffset": 135}, {"referenceID": 1, "context": "We proved that the optimal linear transformation between word vector spaces should be orthogonal, and can be obtained by a single application of the SVD on a dictionary of translation pairs, as proposed independently by Artetxe et al. (2016). We used the SVD to obtain bilingual word vectors, from which we can predict the translations of previously unseen words.", "startOffset": 220, "endOffset": 242}], "year": 2017, "abstractText": "Usually bilingual word vectors are trained \u201conline\u201d. Mikolov et al. (2013a) showed they can also be found \u201coffline\u201d; whereby two pre-trained embeddings are aligned with a linear transformation, using dictionaries compiled from expert knowledge. In this work, we prove that the linear transformation between two spaces should be orthogonal. This transformation can be obtained using the singular value decomposition. We introduce a novel \u201cinverted softmax\u201d for identifying translation pairs, with which we improve the precision @1 of Mikolov\u2019s original mapping from 34% to 43%, when translating a test set composed of both common and rare English words into Italian. Orthogonal transformations are more robust to noise, enabling us to learn the transformation without expert bilingual signal by constructing a \u201cpseudo-dictionary\u201d from the identical character strings which appear in both languages, achieving 40% precision on the same test set. Finally, we extend our method to retrieve the true translations of English sentences from a corpus of 200k Italian sentences with a precision @1 of 68%.", "creator": "LaTeX with hyperref package"}}}