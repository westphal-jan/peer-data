{"id": "1702.04510", "review": {"conference": "AAAI", "VERSION": "v1", "DATE_OF_SUBMISSION": "15-Feb-2017", "title": "A Dependency-Based Neural Reordering Model for Statistical Machine Translation", "abstract": "In machine translation (MT) that involves translating between two languages with significant differences in word order, determining the correct word order of translated words is a major challenge. The dependency parse tree of a source sentence can help to determine the correct word order of the translated words. In this paper, we present a novel reordering approach utilizing a neural network and dependency-based embeddings to predict whether the translations of two source words linked by a dependency relation should remain in the same order or should be swapped in the translated sentence. Experiments on Chinese-to-English translation show that our approach yields a statistically significant improvement of 0.57 BLEU point on benchmark NIST test sets, compared to our prior state-of-the-art statistical MT system that uses sparse dependency-based reordering features.", "histories": [["v1", "Wed, 15 Feb 2017 09:08:21 GMT  (2774kb,D)", "http://arxiv.org/abs/1702.04510v1", "7 pages, 3 figures, Proceedings of AAAI-17"]], "COMMENTS": "7 pages, 3 figures, Proceedings of AAAI-17", "reviews": [], "SUBJECTS": "cs.CL", "authors": ["christian hadiwinoto", "hwee tou ng"], "accepted": true, "id": "1702.04510"}, "pdf": {"name": "1702.04510.pdf", "metadata": {"source": "META", "title": "A Dependency-Based Neural Reordering Model for Statistical Machine Translation", "authors": ["Christian Hadiwinoto", "Hwee Tou Ng"], "emails": ["nght}@comp.nus.edu.sg"], "sections": [{"heading": "Introduction", "text": "In a machine translation (MT) system, determining the correct order of words in translated words is crucial, as the order of words reflects the meaning. As different languages have different orders of words, reordering the words is necessary to achieve the correct translation performance. Reordering in MT remains a major challenge for language pairs with a significant difference in the order of words. Phrase-based MT systems (Koehn, Och and Marcu 2003), which perform state-of-the-art, generally adopt a reordering model based on the span of a phrase and the span of its adjacent phrase (Tillmann 2004; Koehn et al. 2005; Galley and Manning 2008). Including the dependency savings tree of an input (source) sentence is advantageous for reordering, as the dependency tree captures the relationships between words in a sentence, through the dependency model of linking two words."}, {"heading": "A Neural Classifier for Dependency-Based Reordering", "text": "We propose two neural classifiers, one to predict the correct order of the translated target words of two source words with a head-child relationship, and the other for two source words with a sibling relationship. Each binary classifier takes a set of attributes related to the two source words as input and predicts whether the translated words should be interchanged (positive) or kept in order (negative)."}, {"heading": "Input Representation", "text": "The head-child classifier predicts the order of the translated words of a source word xc and its header word xh (where xg is the header of xh) using the following input characteristics: \u2022 The header word xh, its part-of-speech (POS) tag T (xh), and the dependency name L (xh) linking xh with xg \u2022 The child word xc, its POS tag T (xc), and the dependency name L (xc) linking xc with xh \u2022 The sign spacing d (xh, xc) between the head and the child in the original source sentence, with the following possible values: - 2 if xc is to the left of xh and there is at least one other child between them - 1, if xc is to the left of xh and there is none of them - 1, if xh is to the left of xh, and there is at least one other child between them - - 1, if xc is to the left of xh and there is none of them - xl, if there is at least one word xh on the left side of xh (where there is no xh on the left side of xh), and there is at least one child xh between them - 1, if xl is one of the other word xh is one of the child xh, and there is one of the other xh on the left side of xh (if there is none of them)."}, {"heading": "Feed-Forward Layers", "text": "As shown in Figure 1a, the classifier is an advanced neural network whose input layer > contains the characteristics, each characteristic is mapped to a continuous vector representation by a lookup table, and the resulting vectors are concatenated and fed into a series of hidden embedding vectors (weight matrices) (multiplied by). Considering the hidden embedding vector x, a weight vector W, and a bias value b, the prediction output is defined as: z = W \u00b7 x + b (1) tape function, relative (z) = 11 + e \u2212 z (2), we initialize the hidden layers and embedding layer for non-word characteristics (POS tags, dependency labels, and boolean indicators) defined as: z = W \u00b7 x + b (1) tape model, where the xolean, xxh, and xolean word characteristics are."}, {"heading": "Neural Network Training", "text": "The training instances for the neural classifiers are derived from a word-achived parallel corpus. Figure 1 shows the training instances extracted with their corresponding characteristics; for the head-child classifier, the characteristics that contain the child information are distinguished by the position of the child to the left or right of the head; the NN classifiers are trained using backpropagation to minimize cross-entropy lens function: L = \u2212 1 T-i = 1 yi log y-i + (1 \u2212 yi) log (1 \u2212 y-i) (3), where xi is the i-th training instance, yi is the corresponding designation (1 for reversed and 0 for in sequence), and y-i is the probability for classifier prediction. To prevent overadjustment of the model, we use the failure layer (snietawa)."}, {"heading": "Reordering in Phrase-Based SMT", "text": "We apply the phrase-based SMT approach using a beam search decoding algorithm (Koehn 2004a). Each source phrase and one of its possible translations represent an alternative to searching for the translated sentence. While the search produces translations from left to right in the output order of the translation, it selects the source phrases in any order to allow reordering for a language pair with different word sequences.The translation output is based on a score calculated by a log-linear model that includes the weighted sum of the feature function values (Och and Ney 2002).A phrase-based SMT system typically includes a distance-based reordering penalty (Koehn, Och and Marcu 2003) to discourage reordering over long distances, and phrase-based reordering models (PBRM). The latter includes the 2005 based phrasebased reordering model (Tilllexmann) and the 2004 reordering model (Tilllexmann)."}, {"heading": "Dependency-Based Decoding Features", "text": "To foster structural cohesion during translation, we add a Dependency Distortion Penalty (DDP) feature (Cherry 2008) to discourage translation output, in which the translated words of a phrase are split into a source code dependency parse. (The features include taking into account a source word x that is translated during the bar search, and each of the untranslated source words x \u2032, where x \u2032 p is the head, child, or siblings of x in the source code dependency parse. x \u2032 can be placed on the left side of the x-word in the source sentence, resulting in x and x \u2032 in the translation output; or x \u2032 hels on the right side xxxxxxs, which results in xs in the same order."}, {"heading": "Incorporating Neural Classifier", "text": "We integrate the neural classifier by defining a decoding function for the head-child classifier and another decoding function for the sibling classifier. In addition, we use an ensemble of models by training several head-child and sibling classifiers, each of which has a different random seed for initializing hidden layers. Within the log-linear model, the value of each function of the neural classifier is its predictive protocol probability. Each function is assigned a different weight, resulting from the matching of developmental data."}, {"heading": "Experimental Setup", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "Data Set and Toolkits", "text": "In fact, most of them are able to play by the rules that they have imposed on themselves, and they are able to play by the rules that they have imposed on themselves."}, {"heading": "Baseline System", "text": "Our Phrase-based Base SMT system includes the traditional reorder models, i.e. distance-based reorder penalties (DBR) and phrase-based reorder models (PBRM), both of which are phrase-based lexicalized reorder functions (PBLR) and hierarchical reorder functions (HR). We also use the dependency-based reorder functions, including distortion functions (DDP) and sparse dependency swaps (DS). To limit the decryption process, we use punctuation symbols as reorder constraints, within which phrases cannot be reordered, as they form the natural boundaries between different clauses. In addition, a distortion limit is set so that a reorder may not be longer than a certain distance. To select the translation performance, we also use n-best minimal Bayes decoding (Kumar and Byrne 2004) instead of the standard maximum postal decoding (AP)."}, {"heading": "Neural Reordering", "text": "We replaced DS traits with our dependency-based neural reorder classifier, in which we set the word vocabulary to the 100,000 most common words in our parallel training corpora and replaced other words with a special UNK token, in addition to all POS tags, dependency designations and Boolean traits. We set the embedding size to 100, the bottom dimension of the hidden layer to 200, and the top dimension of the hidden layer to 100. We trained for 100 epochs, with 128 mini-batches per epoch, and used a drop-out rate of 0.5. For the model ensemble, we trained 10 classifiers for the reordering of head children and 10 for the reordering of siblings, each of which formed a function."}, {"heading": "Experimental Results", "text": "The translation quality of the system is measured by case-insensitive BLEU (Papineni et al. 2002), for which the short-term penalty is calculated on the basis of the shortest reference (NIST-BLEU) 9. The statistical significance check between systems is performed by bootstrap resampling (Koehn 2004b).Table 1 shows the experimental results. The translation system with individual neural classifiers is able to improve our strong baseline system (DBR + PDP + DS + DS) when the word is initialized with dependency context (Bansal, Gimpel, and Livescu 2014), which is our standard scheme, our translation system with a single neural classifier capable of improving itself via our strong baseline system (DBR + DDP + DS + DS) point, while an ensemble model of 10 neural classifiers improves the system."}, {"heading": "Discussion", "text": "The dependency swap features capture the dependency label and the POS tag of the two reordered words, but not the actual words themselves. While using words as sparse features can lead to many parameters, continuous word representation in our neural approach defines this problem. In addition, the neural network model can also learn useful combinations of individual features. While the dependence on swap features (Hadiwinoto, Liu and Ng 2016) defines the features as pairs of dependency labels and POS tags, the hidden layer of information can be used to account for the relocation of decisions."}, {"heading": "Related Work", "text": "In fact, most people who are able to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move."}, {"heading": "Conclusion", "text": "We have presented a dependency-based reordering approach for phrase-based SMTs, guided by predictions of neural classifiers. It shows that MT can be improved through a neural networking approach, by not requiring explicit combinations of attributes and using dependency-driven continuous word representation. Our experiments also show that our neural reordering approach exceeds our previous approach with sparse dependency-based attributes."}], "references": [{"title": "Globally normalized transition-based neural networks", "author": ["D. Andor", "C. Alberti", "D. Weiss", "A. Severyn", "A. Presta", "K. Ganchev", "S. Petrov", "M. Collins"], "venue": "ACL 2016.", "citeRegEx": "Andor et al\\.,? 2016", "shortCiteRegEx": "Andor et al\\.", "year": 2016}, {"title": "Tailoring continuous word representation for dependency parsing", "author": ["M. Bansal", "K. Gimpel", "K. Livescu"], "venue": "ACL 2014 Short Papers.", "citeRegEx": "Bansal et al\\.,? 2014", "shortCiteRegEx": "Bansal et al\\.", "year": 2014}, {"title": "A transition-based system for joint part-of-speech tagging and labeled non-projective dependency parsing", "author": ["B. Bohnet", "J. Nivre"], "venue": "EMNLP-CoNLL 2012.", "citeRegEx": "Bohnet and Nivre,? 2012", "shortCiteRegEx": "Bohnet and Nivre", "year": 2012}, {"title": "The mathematics of statistical machine translation: parameter estimation", "author": ["P.F. Brown", "V.J. Della Pietra", "S.A. Della Pietra", "R.L. Mercer"], "venue": "Computational Linguistics 19(2).", "citeRegEx": "Brown et al\\.,? 1993", "shortCiteRegEx": "Brown et al\\.", "year": 1993}, {"title": "Discriminative reordering with Chinese grammatical relations features", "author": ["P.-C. Chang", "H. Tseng", "D. Jurafsky", "C.D. Manning"], "venue": "SSST-3.", "citeRegEx": "Chang et al\\.,? 2009", "shortCiteRegEx": "Chang et al\\.", "year": 2009}, {"title": "A fast and accurate dependency parser using neural networks", "author": ["D. Chen", "C.D. Manning"], "venue": "EMNLP 2014.", "citeRegEx": "Chen and Manning,? 2014", "shortCiteRegEx": "Chen and Manning", "year": 2014}, {"title": "Cohesive phrase-based decoding for statistical machine translation", "author": ["C. Cherry"], "venue": "ACL-08: HLT.", "citeRegEx": "Cherry,? 2008", "shortCiteRegEx": "Cherry", "year": 2008}, {"title": "Hierarchical phrase-based translation", "author": ["D. Chiang"], "venue": "Computational Linguistics 33(2).", "citeRegEx": "Chiang,? 2007", "shortCiteRegEx": "Chiang", "year": 2007}, {"title": "LSTM neural reordering feature for statistical machine translation", "author": ["Y. Cui", "S. Wang", "J. Li"], "venue": "NAACL HLT 2016.", "citeRegEx": "Cui et al\\.,? 2016", "shortCiteRegEx": "Cui et al\\.", "year": 2016}, {"title": "Fast and accurate preordering for SMT using neural networks", "author": ["A. de Gispert", "G. Iglesias", "B. Byrne"], "venue": "In NAACL HLT", "citeRegEx": "Gispert et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Gispert et al\\.", "year": 2015}, {"title": "Fast and robust neural network joint models for statistical machine translation", "author": ["J. Devlin", "R. Zbib", "Z. Huang", "T. Lamar", "R. Schwartz", "J. Makhoul"], "venue": "ACL 2014.", "citeRegEx": "Devlin et al\\.,? 2014", "shortCiteRegEx": "Devlin et al\\.", "year": 2014}, {"title": "A simple and effective hierarchical phrase reordering model", "author": ["M. Galley", "C.D. Manning"], "venue": "EMNLP 2008.", "citeRegEx": "Galley and Manning,? 2008", "shortCiteRegEx": "Galley and Manning", "year": 2008}, {"title": "To swap or not to swap? Exploiting dependency word pairs for reordering in statistical machine translation", "author": ["C. Hadiwinoto", "Y. Liu", "H.T. Ng"], "venue": "AAAI-16.", "citeRegEx": "Hadiwinoto et al\\.,? 2016", "shortCiteRegEx": "Hadiwinoto et al\\.", "year": 2016}, {"title": "Tuning as ranking", "author": ["M. Hopkins", "J. May"], "venue": "EMNLP 2011.", "citeRegEx": "Hopkins and May,? 2011", "shortCiteRegEx": "Hopkins and May", "year": 2011}, {"title": "Source-side preordering for translation using logistic regression and depth-first branch-and-bound search", "author": ["L. Jehl", "A. de Gispert", "M. Hopkins", "W. Byrne"], "venue": null, "citeRegEx": "Jehl et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Jehl et al\\.", "year": 2014}, {"title": "Is neural machine translation ready for deployment? A case study on 30 translation directions", "author": ["M. Junczys-Dowmunt", "T. Dwojak", "H. Hoang"], "venue": "CoRR abs/1610.01108.", "citeRegEx": "Junczys.Dowmunt et al\\.,? 2016", "shortCiteRegEx": "Junczys.Dowmunt et al\\.", "year": 2016}, {"title": "Edinburgh system description for the 2005 IWSLT speech translation evaluation", "author": ["P. Koehn", "A. Axelrod", "A.B. Mayne", "C. Callison-Burch", "M. Osborne", "D. Talbot"], "venue": "IWSLT 2005.", "citeRegEx": "Koehn et al\\.,? 2005", "shortCiteRegEx": "Koehn et al\\.", "year": 2005}, {"title": "Moses: Open source toolkit for statistical machine translation", "author": ["P. Koehn", "H. Hoang", "A. Birch", "C. Callison-Burch", "M. Federico", "N. Bertoldi", "B. Cowan", "W. Shen", "C. Moran", "R. Zens", "C. Dyer", "O. Bojar", "A. Constantin", "E. Herbst"], "venue": "the ACL 2007 Demo and Poster Sessions.", "citeRegEx": "Koehn et al\\.,? 2007", "shortCiteRegEx": "Koehn et al\\.", "year": 2007}, {"title": "Statistical phrasebased translation", "author": ["P. Koehn", "F.J. Och", "D. Marcu"], "venue": "HLT-NAACL 2003.", "citeRegEx": "Koehn et al\\.,? 2003", "shortCiteRegEx": "Koehn et al\\.", "year": 2003}, {"title": "Pharaoh: A beam search decoder for phrase-based statistical machine translation models", "author": ["P. Koehn"], "venue": "AMTA 2004.", "citeRegEx": "Koehn,? 2004a", "shortCiteRegEx": "Koehn", "year": 2004}, {"title": "Statistical significance tests for machine translation evaluation", "author": ["P. Koehn"], "venue": "EMNLP 2004.", "citeRegEx": "Koehn,? 2004b", "shortCiteRegEx": "Koehn", "year": 2004}, {"title": "Minimum Bayes-risk decoding for statistical machine translation", "author": ["S. Kumar", "W. Byrne"], "venue": "HLT-NAACL 2004.", "citeRegEx": "Kumar and Byrne,? 2004", "shortCiteRegEx": "Kumar and Byrne", "year": 2004}, {"title": "Source-side classifier preordering for machine translation", "author": ["U. Lerner", "S. Petrov"], "venue": "EMNLP 2013.", "citeRegEx": "Lerner and Petrov,? 2013", "shortCiteRegEx": "Lerner and Petrov", "year": 2013}, {"title": "A neural reordering model for phrase-based translation", "author": ["P. Li", "Y. Liu", "M. Sun", "T. Izuha", "D. Zhang"], "venue": "COLING 2014.", "citeRegEx": "Li et al\\.,? 2014", "shortCiteRegEx": "Li et al\\.", "year": 2014}, {"title": "Tree-to-string alignment template for statistical machine translation", "author": ["Y. Liu", "Q. Liu", "S. Lin"], "venue": "ACLCOLING 2006.", "citeRegEx": "Liu et al\\.,? 2006", "shortCiteRegEx": "Liu et al\\.", "year": 2006}, {"title": "A maximum entropy approach to Chinese word segmentation", "author": ["J.K. Low", "H.T. Ng", "W. Guo"], "venue": "SIGHAN4.", "citeRegEx": "Low et al\\.,? 2005", "shortCiteRegEx": "Low et al\\.", "year": 2005}, {"title": "SPMT: statistical machine translation with syntactified target language phrases", "author": ["D. Marcu", "W. Wang", "A. Echihabi", "K. Knight"], "venue": "EMNLP 2006.", "citeRegEx": "Marcu et al\\.,? 2006", "shortCiteRegEx": "Marcu et al\\.", "year": 2006}, {"title": "Non-projective dependency-based pre-reordering with recurrent neural network for machine translation", "author": ["A.V. Miceli-Barone", "G. Attardi"], "venue": "ACL 2015.", "citeRegEx": "Miceli.Barone and Attardi,? 2015", "shortCiteRegEx": "Miceli.Barone and Attardi", "year": 2015}, {"title": "Efficient estimation of word representation in vector space", "author": ["T. Mikolov", "K. Chen", "G. Corrado", "J. Dean"], "venue": "ICLR 2013 Workshop.", "citeRegEx": "Mikolov et al\\.,? 2013", "shortCiteRegEx": "Mikolov et al\\.", "year": 2013}, {"title": "Discriminative training and maximum entropy models for statistical machine translation", "author": ["F.J. Och", "H. Ney"], "venue": "ACL 2002.", "citeRegEx": "Och and Ney,? 2002", "shortCiteRegEx": "Och and Ney", "year": 2002}, {"title": "A systematic comparison of various statistical alignment models", "author": ["F.J. Och", "H. Ney"], "venue": "Computational Linguistics 29(1).", "citeRegEx": "Och and Ney,? 2003", "shortCiteRegEx": "Och and Ney", "year": 2003}, {"title": "BLEU: A method for automatic evaluation of machine translation", "author": ["K. Papineni", "S. Roukos", "T. Ward", "W.-J. Zhu"], "venue": "ACL 2002.", "citeRegEx": "Papineni et al\\.,? 2002", "shortCiteRegEx": "Papineni et al\\.", "year": 2002}, {"title": "Dropout: A simple way to prevent neural networks from overfitting", "author": ["N. Srivastava", "G. Hinton", "A. Krizhevsky", "I. Sutskever", "R. Salakhutdinov"], "venue": "Journal of Machine Learning Research 15(Jun).", "citeRegEx": "Srivastava et al\\.,? 2014", "shortCiteRegEx": "Srivastava et al\\.", "year": 2014}, {"title": "A unigram orientation model for statistical machine translation", "author": ["C. Tillmann"], "venue": "HLT-NAACL 2004: Short Papers.", "citeRegEx": "Tillmann,? 2004", "shortCiteRegEx": "Tillmann", "year": 2004}, {"title": "Decoding with large-scale neural language models improves translation", "author": ["A. Vaswani", "Y. Zhao", "V. Fossum", "D. Chiang"], "venue": "EMNLP 2013.", "citeRegEx": "Vaswani et al\\.,? 2013", "shortCiteRegEx": "Vaswani et al\\.", "year": 2013}, {"title": "Memoryenhanced decoder for neural machine translation", "author": ["M. Wang", "Z. Lu", "H. Li", "L. Qun"], "venue": "EMNLP 2016.", "citeRegEx": "Wang et al\\.,? 2016", "shortCiteRegEx": "Wang et al\\.", "year": 2016}, {"title": "Simple but effective approaches to improving tree-to-tree model", "author": ["F. Zhai", "J. Zhang", "Y. Zhou", "C. Zong"], "venue": "MT Summit XIII.", "citeRegEx": "Zhai et al\\.,? 2011", "shortCiteRegEx": "Zhai et al\\.", "year": 2011}, {"title": "Variational neural machine translation", "author": ["B. Zhang", "D. Xiong", "J. Su", "H. Duan", "M. Zhang"], "venue": "EMNLP 2016.", "citeRegEx": "Zhang et al\\.,? 2016", "shortCiteRegEx": "Zhang et al\\.", "year": 2016}], "referenceMentions": [{"referenceID": 33, "context": "Phrase-based MT systems (Koehn, Och, and Marcu 2003), which achieve state-of-the-art performance, generally adopt a reordering model based on the span of a phrase and the span of its adjacent phrase (Tillmann 2004; Koehn et al. 2005; Galley and Manning 2008).", "startOffset": 199, "endOffset": 258}, {"referenceID": 16, "context": "Phrase-based MT systems (Koehn, Och, and Marcu 2003), which achieve state-of-the-art performance, generally adopt a reordering model based on the span of a phrase and the span of its adjacent phrase (Tillmann 2004; Koehn et al. 2005; Galley and Manning 2008).", "startOffset": 199, "endOffset": 258}, {"referenceID": 11, "context": "Phrase-based MT systems (Koehn, Och, and Marcu 2003), which achieve state-of-the-art performance, generally adopt a reordering model based on the span of a phrase and the span of its adjacent phrase (Tillmann 2004; Koehn et al. 2005; Galley and Manning 2008).", "startOffset": 199, "endOffset": 258}, {"referenceID": 4, "context": "The dependency parse tree of a source sentence can be utilized in reordering integrated within phrasebased statistical MT (SMT), by defining dependency-based features in the SMT log-linear model (Chang et al. 2009; Hadiwinoto, Liu, and Ng 2016).", "startOffset": 195, "endOffset": 244}, {"referenceID": 23, "context": "has found application in MT reordering, applied in the reranking of translation output candidates (Li et al. 2014; Cui, Wang, and Li 2016) and in the pre-ordering approach (reordering the source sentence before translation) (de Gispert, Iglesias, and Byrne 2015; Miceli-Barone and Attardi 2015).", "startOffset": 98, "endOffset": 138}, {"referenceID": 27, "context": "2014; Cui, Wang, and Li 2016) and in the pre-ordering approach (reordering the source sentence before translation) (de Gispert, Iglesias, and Byrne 2015; Miceli-Barone and Attardi 2015).", "startOffset": 115, "endOffset": 185}, {"referenceID": 28, "context": "This scheme is a modified skip-gram model, which given an input word, predicts its context (surrounding words), resulting in a mapping such that words with similar surrounding words have similar continuous vector representations (Mikolov et al. 2013).", "startOffset": 229, "endOffset": 250}, {"referenceID": 32, "context": "To prevent model overfitting, we used the dropout strategy (Srivastava et al. 2014) on the input embedding layer.", "startOffset": 59, "endOffset": 83}, {"referenceID": 19, "context": "We adopt the phrase-based SMT approach, using a beam search decoding algorithm (Koehn 2004a).", "startOffset": 79, "endOffset": 92}, {"referenceID": 29, "context": "The translation output is picked based on a score computed by a log-linear model, comprising the weighted sum of feature function values (Och and Ney 2002).", "startOffset": 137, "endOffset": 155}, {"referenceID": 33, "context": "The latter comprises the phrase-based lexicalized reordering (PBLR) model (Tillmann 2004; Koehn et al. 2005) and the hierarchical reordering (HR) model (Galley and Manning 2008).", "startOffset": 74, "endOffset": 108}, {"referenceID": 16, "context": "The latter comprises the phrase-based lexicalized reordering (PBLR) model (Tillmann 2004; Koehn et al. 2005) and the hierarchical reordering (HR) model (Galley and Manning 2008).", "startOffset": 74, "endOffset": 108}, {"referenceID": 11, "context": "2005) and the hierarchical reordering (HR) model (Galley and Manning 2008).", "startOffset": 49, "endOffset": 74}, {"referenceID": 6, "context": "To encourage structural cohesion during translation, we add a dependency distortion penalty (DDP) feature (Cherry 2008) to discourage translation output in which the translated words of a phrase in a source dependency parse subtree are split.", "startOffset": 106, "endOffset": 119}, {"referenceID": 17, "context": "We conducted experiments on a phrase-based Chinese-toEnglish SMT system built using Moses (Koehn et al. 2007).", "startOffset": 90, "endOffset": 109}, {"referenceID": 30, "context": "Then the parallel corpus is word-aligned by GIZA++ (Och and Ney 2003) using IBM Models 1, 3, and 4 (Brown et al.", "startOffset": 51, "endOffset": 69}, {"referenceID": 3, "context": "Then the parallel corpus is word-aligned by GIZA++ (Och and Ney 2003) using IBM Models 1, 3, and 4 (Brown et al. 1993)4.", "startOffset": 99, "endOffset": 118}, {"referenceID": 13, "context": "Weight tuning is done by using the pairwise ranked optimization (PRO) algorithm (Hopkins and May 2011).", "startOffset": 80, "endOffset": 102}, {"referenceID": 2, "context": "We parse the Chinese sentences by the Mate parser, which jointly performs POS tagging and dependency parsing (Bohnet and Nivre 2012), trained on Chinese Treebank (CTB) version 8.", "startOffset": 109, "endOffset": 132}, {"referenceID": 21, "context": "To pick the translation output, we also use n-best minimum Bayes risk (MBR) decoding (Kumar and Byrne 2004) instead of the default maximum a-posteriori (MAP) decoding.", "startOffset": 85, "endOffset": 107}, {"referenceID": 31, "context": "The translation quality of the system output is measured by case-insensitive BLEU (Papineni et al. 2002), for which the brevity penalty is computed based on the shortest reference (NIST-BLEU)9.", "startOffset": 82, "endOffset": 104}, {"referenceID": 20, "context": "Statistical significance testing between systems is conducted by bootstrap resampling (Koehn 2004b).", "startOffset": 86, "endOffset": 99}, {"referenceID": 28, "context": "Additional experiments use two other initialization schemes: (1) random initialization and (2) the original skip-gram model of (Mikolov et al. 2013) with a window size of 5.", "startOffset": 127, "endOffset": 148}, {"referenceID": 33, "context": "We used the following prior reordering features as baseline: (1) distance-based reordering (DBR) (Koehn, Och, and Marcu 2003); (2) phrase-based reordering models (PBRM), comprising phrase-based lexicalized reordering (Tillmann 2004; Koehn et al. 2005) and hierarchical reordering (Galley and Manning 2008); (3) dependency distortion penalty (DDP) (Cherry 2008); and (4) sparse dependency swap features (DS) (Hadiwinoto, Liu, and Ng 2016).", "startOffset": 217, "endOffset": 251}, {"referenceID": 16, "context": "We used the following prior reordering features as baseline: (1) distance-based reordering (DBR) (Koehn, Och, and Marcu 2003); (2) phrase-based reordering models (PBRM), comprising phrase-based lexicalized reordering (Tillmann 2004; Koehn et al. 2005) and hierarchical reordering (Galley and Manning 2008); (3) dependency distortion penalty (DDP) (Cherry 2008); and (4) sparse dependency swap features (DS) (Hadiwinoto, Liu, and Ng 2016).", "startOffset": 217, "endOffset": 251}, {"referenceID": 11, "context": "2005) and hierarchical reordering (Galley and Manning 2008); (3) dependency distortion penalty (DDP) (Cherry 2008); and (4) sparse dependency swap features (DS) (Hadiwinoto, Liu, and Ng 2016).", "startOffset": 34, "endOffset": 59}, {"referenceID": 6, "context": "2005) and hierarchical reordering (Galley and Manning 2008); (3) dependency distortion penalty (DDP) (Cherry 2008); and (4) sparse dependency swap features (DS) (Hadiwinoto, Liu, and Ng 2016).", "startOffset": 101, "endOffset": 114}, {"referenceID": 7, "context": "This is different from prior approaches requiring chart parsing decoding such as the hierarchical phrasebased (Chiang 2007), tree-to-string (Liu, Liu, and Lin 2006), string-to-tree (Marcu et al.", "startOffset": 110, "endOffset": 123}, {"referenceID": 26, "context": "This is different from prior approaches requiring chart parsing decoding such as the hierarchical phrasebased (Chiang 2007), tree-to-string (Liu, Liu, and Lin 2006), string-to-tree (Marcu et al. 2006), and tree-to-tree (Zhai et al.", "startOffset": 181, "endOffset": 200}, {"referenceID": 36, "context": "2006), and tree-to-tree (Zhai et al. 2011) SMT approaches.", "startOffset": 24, "endOffset": 42}, {"referenceID": 35, "context": "However, the most recent NMT papers tested on the same NIST Chinese-to-English test sets (Wang et al. 2016; Zhang et al. 2016) show lower absolute BLEU scores (by 2 to 7 points) compared to our scores.", "startOffset": 89, "endOffset": 126}, {"referenceID": 37, "context": "However, the most recent NMT papers tested on the same NIST Chinese-to-English test sets (Wang et al. 2016; Zhang et al. 2016) show lower absolute BLEU scores (by 2 to 7 points) compared to our scores.", "startOffset": 89, "endOffset": 126}, {"referenceID": 4, "context": "Chang et al. (2009) utilized the traversed paths of dependency labels to guide phrase reordering.", "startOffset": 0, "endOffset": 20}, {"referenceID": 4, "context": "Chang et al. (2009) utilized the traversed paths of dependency labels to guide phrase reordering. Hadiwinoto, Liu, and Ng (2016) introduced a technique to determine the order of two translated words with corresponding source words that are related through the dependency parse during beam search.", "startOffset": 0, "endOffset": 129}, {"referenceID": 23, "context": "Li et al. (2014) introduced a recursive auto-encoder model to represent phrases and determine the phrase orientation probability.", "startOffset": 0, "endOffset": 17}, {"referenceID": 23, "context": "Li et al. (2014) introduced a recursive auto-encoder model to represent phrases and determine the phrase orientation probability. Cui, Wang, and Li (2016) introduced long short-term memory (LSTM) recurrent neural networks to predict the translation word orientation probability.", "startOffset": 0, "endOffset": 155}, {"referenceID": 22, "context": "While the preordering step typically utilizes a classifier with feature combinations (Lerner and Petrov 2013; Jehl et al. 2014), a neural network can replace the classifier to avoid feature combination.", "startOffset": 85, "endOffset": 127}, {"referenceID": 14, "context": "While the preordering step typically utilizes a classifier with feature combinations (Lerner and Petrov 2013; Jehl et al. 2014), a neural network can replace the classifier to avoid feature combination.", "startOffset": 85, "endOffset": 127}, {"referenceID": 14, "context": "While the preordering step typically utilizes a classifier with feature combinations (Lerner and Petrov 2013; Jehl et al. 2014), a neural network can replace the classifier to avoid feature combination. De Gispert, Iglesias, and Byrne (2015) introduced a feed-forward neural network to pre-order the dependency parse tree nodes (words).", "startOffset": 110, "endOffset": 242}, {"referenceID": 14, "context": "While the preordering step typically utilizes a classifier with feature combinations (Lerner and Petrov 2013; Jehl et al. 2014), a neural network can replace the classifier to avoid feature combination. De Gispert, Iglesias, and Byrne (2015) introduced a feed-forward neural network to pre-order the dependency parse tree nodes (words). However, they did not explore dependency-driven embeddings and model ensemble. Miceli-Barone and Attardi (2015) treat pre-ordering as a traversal on the dependency parse tree, guided by a recurrent neural network.", "startOffset": 110, "endOffset": 449}, {"referenceID": 34, "context": ", neural language model (Vaswani et al. 2013) and neural joint model (Devlin et al.", "startOffset": 24, "endOffset": 45}, {"referenceID": 10, "context": "2013) and neural joint model (Devlin et al. 2014), a source-augmented language model.", "startOffset": 29, "endOffset": 49}, {"referenceID": 28, "context": "While continuous representation of words is originally defined for words (Mikolov et al. 2013), we also define continuous representation for POS tags, dependency labels, and indicator features.", "startOffset": 73, "endOffset": 94}, {"referenceID": 5, "context": "Extending continuous representation to non-word features is also done in neural dependency parsing (Chen and Manning 2014; Andor et al. 2016), which shows better performance by using continuous feature representation over the traditional discrete representation.", "startOffset": 99, "endOffset": 141}, {"referenceID": 0, "context": "Extending continuous representation to non-word features is also done in neural dependency parsing (Chen and Manning 2014; Andor et al. 2016), which shows better performance by using continuous feature representation over the traditional discrete representation.", "startOffset": 99, "endOffset": 141}], "year": 2017, "abstractText": "In machine translation (MT) that involves translating between two languages with significant differences in word order, determining the correct word order of translated words is a major challenge. The dependency parse tree of a source sentence can help to determine the correct word order of the translated words. In this paper, we present a novel reordering approach utilizing a neural network and dependency-based embeddings to predict whether the translations of two source words linked by a dependency relation should remain in the same order or should be swapped in the translated sentence. Experiments on Chinese-to-English translation show that our approach yields a statistically significant improvement of 0.57 BLEU point on benchmark NIST test sets, compared to our prior state-of-theart statistical MT system that uses sparse dependency-based reordering features.", "creator": "TeX"}}}