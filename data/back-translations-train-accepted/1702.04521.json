{"id": "1702.04521", "review": {"conference": "iclr", "VERSION": "v1", "DATE_OF_SUBMISSION": "15-Feb-2017", "title": "Frustratingly Short Attention Spans in Neural Language Modeling", "abstract": "Neural language models predict the next token using a latent representation of the immediate token history. Recently, various methods for augmenting neural language models with an attention mechanism over a differentiable memory have been proposed. For predicting the next token, these models query information from a memory of the recent history which can facilitate learning mid- and long-range dependencies. However, conventional attention mechanisms used in memory-augmented neural language models produce a single output vector per time step. This vector is used both for predicting the next token as well as for the key and value of a differentiable memory of a token history. In this paper, we propose a neural language model with a key-value attention mechanism that outputs separate representations for the key and value of a differentiable memory, as well as for encoding the next-word distribution. This model outperforms existing memory-augmented neural language models on two corpora. Yet, we found that our method mainly utilizes a memory of the five most recent output representations. This led to the unexpected main finding that a much simpler model based only on the concatenation of recent output representations from previous time steps is on par with more sophisticated memory-augmented neural language models.", "histories": [["v1", "Wed, 15 Feb 2017 09:45:23 GMT  (260kb,D)", "http://arxiv.org/abs/1702.04521v1", "Published as a conference paper at ICLR 2017"]], "COMMENTS": "Published as a conference paper at ICLR 2017", "reviews": [], "SUBJECTS": "cs.CL cs.AI cs.LG cs.NE", "authors": ["micha{\\l} daniluk", "tim rockt\\\"aschel", "johannes welbl", "sebastian riedel"], "accepted": true, "id": "1702.04521"}, "pdf": {"name": "1702.04521.pdf", "metadata": {"source": "CRF", "title": "FRUSTRATINGLY SHORT ATTENTION SPANS IN NEURAL LANGUAGE MODELING", "authors": ["Micha\u0142 Daniluk", "Tim Rockt\u00e4schel", "Johannes Welbl", "Sebastian Riedel"], "emails": ["michal.daniluk.15@ucl.ac.uk,", "t.rocktaschel@cs.ucl.ac.uk", "j.welbl@cs.ucl.ac.uk", "s.riedel@cs.ucl.ac.uk"], "sections": [{"heading": "1 INTRODUCTION", "text": "At the core of language models (LMs) lies their ability to infer the next word in a particular context, which requires the representation of context-specific dependencies in a sequence on different time scales. On the one hand, classical N-gram language models explicitly capture relevant dependencies between words at short intervals, but suffer from data sparseness. Neural language models, on the other hand, maintain and update a dense vector representation via a sequence in which time dependencies are implicitly captured (Mikolov et al., 2010). A recent extension of neural sequence models are attention mechanisms (Bahdanau et al, 2015) that can capture long-term connections."}, {"heading": "2 METHODS", "text": "In the following, we will discuss methods for extending neural language models with differentiated memory. First, we will present a standard attention mechanism for speech modeling (\u00a7 2.1). Then, we will present two methods for separating the use of output vectors in the attention mechanism: (i) use of a dedicated key and value (\u00a7 2.2) and (ii) further separation of the value into a memory value and a representation that encodes the distribution of the next word (\u00a7 2.3). Finally, we will describe a very simple method that links earlier output representations to predict the next token (\u00a7 2.4)."}, {"heading": "2.1 ATTENTION FOR NEURAL LANGUAGE MODELING", "text": "Adding attention to a neural language model (Bahdanau et al., 2015) is straightforward. We simply take the previous L output vectors as memory Yt = [ht \u2212 L \u00b7 \u00b7 ht \u2212 1], where k is the output dimension of a Long Short-Term Memory (LSTM) unit (Hochreiter & Schmidhuber, 1997). In principle, this memory could contain all previous output representations, but for practical reasons we keep only a sliding window of the previous L output. Let ht Rk represent the output in due time step t and 1 RL be a vector of entries. Attention weights are calculated by comparing the current and previous LSTM outputs. Subsequently, the context vector rt-Rk is calculated from a sum over previous output vectors weighted by their respective attention value."}, {"heading": "2.2 KEY-VALUE ATTENTION", "text": "Inspired by Miller et al. (2016); Ba et al. (2016); Reed & de Freitas (2015); Gulcehre et al. (2016), we are introducing a key-value-attention model that separates output vectors into keys for calculating the attention distribution \u03b1t and a value part for encoding the distribution of the next word and the context representation, which is shown in Figure 1b. Formally, we rewrite equations 1-4 as follows: [kt vt] = ht: R2k (6) Mt = tanh (W Y [kt \u2212 L \u00b7 \u00b7 kt \u2212 1] + (W hkt) 1T): Rk \u00b7 L (7) \u03b1t = softmax (wTMt): R1 \u00b7 L (8) rt = [vt \u2212 L \u00b7 \u00b7 vt \u2212 1]."}, {"heading": "2.3 KEY-VALUE-PREDICT ATTENTION", "text": "Even with a key-value separation, a potential problem is that the same representation vt is still used both for encoding the probability distribution of the next word and for later recall from memory of attention. Therefore, we experimented with a further extension of this model, in which we further split ht into a key, a value, and a predictive representation in which the latter is used only to encode the distribution of the next word (see Figure 1c). To this end, equations 6 and 10 are divided by [kt vt pt] = ht-R3k (11) h-t = tanh (W rrt + W xpt) -Rk (12) more precisely, the output vector ht is divided into three equal parts: key, value, and prediction. In our implementation, we simply divide the output vector ht into kt, vt, and pt. For this purpose, the hidden dimension of the key-value prediction model must be a multiplication of three out of 100, each of which is a sequence of one dimension."}, {"heading": "2.4 N -GRAM RECURRENT NEURAL NETWORK", "text": "Neural language models often work best in combination with traditional N-gram models (Mikolov et al., 2011; Chelba et al., 2013; Williams et al., 2015; Ji et al., 2016; Shazeer et al., 2015), as the former are characterized by generalization, while the latter provide memorization. Furthermore, in initial experiments with memory-enhanced neural language models, we found that normally only the previous five output steps are used, in line with observations by Tran et al. (2016). Therefore, we are experimenting with a much simpler architecture depicted in Figure 1d. Instead of an attention mechanism, the output representations from the previous N-1 time steps are used directly to calculate the probabilities of the next word. Specifically, at each step, we divide the LSTM output into N \u2212 1 vectors [h1t,.,.,.,., replace the output representations from the previous N-1 time steps in order to calculate the probabilities of the next word. \u2212 N = \u2212 N \u2212 1 = equal \u2212 N \u2212 N."}, {"heading": "3 RELATED WORK", "text": "In fact, most of them are able to survive on their own if they do not put themselves in a position to survive on their own."}, {"heading": "4 EXPERIMENTS", "text": "The first is a subset of the Wikipedia corpus. It consists of 7,500 English Wikipedia articles (dump dated February 6, 2015), which fall into one of the following categories: People, Cities, Countries, Universities, and Novels. We chose these categories because we expect articles in these categories to often contain references to the aforementioned units. We then divided this corpus into a train, a development, and a test section, resulting in a corpus of 22.5 million words, 1.2 million words, and 1.2 million words, respectively. We assign all numbers to a dedicated numeric symbol N, and limit the vocabulary to the 77,000 most common words, which comprise 97% of the training vocabulary. All other words are replaced by the UNK symbol. The average length of sentences is 25 characters. In addition to this Wikipedia corpus, we are conducting experiments with the Children's Book Test (CBT Hill al. 2016), while this corpus is well suited for use of paper."}, {"heading": "4.1 TRAINING PROCEDURE", "text": "We use ADAM (Kingma & Ba, 2015) with an initial learning rate of 0.001 and a mini batch size of 64. In addition, we use gradient clipping at a gradient standard of 5 (Pascanu et al., 2013).The bias of the LSTM forget is initialized to 1 (Jozefowicz et al., 2016), while other parameters are uniformly initialized from the range (\u2212 0.1, 0.1).Backpropagation Through Time (Rumelhart et al., 1985; Werbos et al., 1990) was used to train the network with 20 steps of unrolling. We reset the hidden states between articles for the Wikipedia corpus or between stories for the CBT."}, {"heading": "5 RESULTS", "text": "In the first series of experiments, we will examine how well the proposed models and Tran et al. \"s recurrent memory model can draw on stories of different lengths. Perplex results for different attention window sizes on the Wikipedia corpus are summarized in Figure 2a. The average attention these models pay to specific positions in the story is illustrated in Figure 3. We observed that although our models have had more tokens than the recurrent memory model in the past, participation over a longer period of time does not significantly improve the perplexity of each attentive model. The much simpler N gram RNN model achieves comparable results (Figure 2b) and seems to work best with a history of the previous three output vectors (4 gram RNN). As a result, we choose the 4 gram model for the following N gram RNN experiments."}, {"heading": "5.1 COMPARISON WITH STATE-OF-THE-ART MODELS", "text": "In the next round, we will be able to find a solution that will enable us to find a solution that will enable us to find a solution."}, {"heading": "6 CONCLUSION", "text": "In this paper, we observed that the use of an attention mechanism to model neural language by dividing output vectors into a key, value, and predictive part exceeds simpler attention mechanisms on a Wikipedia corpus and the Children Book Test (CBT, Hill et al., 2016). However, we found that all attentive neural language models primarily use a recent memory and fail to take advantage of extensive dependencies. In fact, a much simpler N-gram RNN model, which uses only a concatenation of output representations from the previous three time steps, appears to be on par with more complex memory-enhanced neural language models. Training neural language models that take into account extensive dependencies appears notoriously difficult and requires further investigation. Therefore, for future work, we want to explore ways to foster participation in a longer history by, for example, forcing the model to ignore the more local context and continue to pay attention to the local history."}, {"heading": "ACKNOWLEDGMENTS", "text": "This work has been supported by Microsoft Research and the Engineering and Physical Sciences Research Council through PhD scholarships, an Allen Distinguished Investigator Award, and a Marie Curie Career Integration Award."}], "references": [{"title": "Using fast weights to attend to the recent past", "author": ["Jimmy Ba", "Geoffrey E Hinton", "Volodymyr Mnih", "Joel Z Leibo", "Catalin Ionescu"], "venue": "In NIPS,", "citeRegEx": "Ba et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Ba et al\\.", "year": 2016}, {"title": "Neural machine translation by jointly learning to align and translate", "author": ["Dzmitry Bahdanau", "Kyunghyun Cho", "Yoshua Bengio"], "venue": "In ICLR,", "citeRegEx": "Bahdanau et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Bahdanau et al\\.", "year": 2015}, {"title": "One billion word benchmark for measuring progress in statistical language modeling", "author": ["Ciprian Chelba", "Tomas Mikolov", "Mike Schuster", "Qi Ge", "Thorsten Brants", "Phillipp Koehn", "Tony Robinson"], "venue": "arXiv preprint arXiv:1312.3005,", "citeRegEx": "Chelba et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Chelba et al\\.", "year": 2013}, {"title": "Long short-term memory-networks for machine reading", "author": ["Jianpeng Cheng", "Li Dong", "Mirella Lapata"], "venue": "In EMNLP,", "citeRegEx": "Cheng et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Cheng et al\\.", "year": 2016}, {"title": "Attention-based models for speech recognition", "author": ["Jan K Chorowski", "Dzmitry Bahdanau", "Dmitriy Serdyuk", "Kyunghyun Cho", "Yoshua Bengio"], "venue": "In NIPS,", "citeRegEx": "Chorowski et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Chorowski et al\\.", "year": 2015}, {"title": "Attention-over-attention neural networks for reading comprehension", "author": ["Yiming Cui", "Zhipeng Chen", "Si Wei", "Shijin Wang", "Ting Liu", "Guoping Hu"], "venue": "arXiv preprint arXiv:1607.04423,", "citeRegEx": "Cui et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Cui et al\\.", "year": 2016}, {"title": "Consensus attention-based neural networks for chinese reading comprehension", "author": ["Yiming Cui", "Ting Liu", "Zhipeng Chen", "Shijin Wang", "Guoping Hu"], "venue": "arXiv preprint arXiv:1607.02250,", "citeRegEx": "Cui et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Cui et al\\.", "year": 2016}, {"title": "Gated-attention readers for text comprehension", "author": ["Bhuwan Dhingra", "Hanxiao Liu", "William W Cohen", "Ruslan Salakhutdinov"], "venue": "arXiv preprint arXiv:1606.01549,", "citeRegEx": "Dhingra et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Dhingra et al\\.", "year": 2016}, {"title": "Neural turing machines", "author": ["Alex Graves", "Greg Wayne", "Ivo Danihelka"], "venue": "arXiv preprint arXiv:1410.5401,", "citeRegEx": "Graves et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Graves et al\\.", "year": 2014}, {"title": "Dynamic neural turing machine with soft and hard addressing schemes", "author": ["Caglar Gulcehre", "Sarath Chandar", "Kyunghyun Cho", "Yoshua Bengio"], "venue": "arXiv preprint arXiv:1607.00036,", "citeRegEx": "Gulcehre et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Gulcehre et al\\.", "year": 2016}, {"title": "The goldilocks principle: Reading children\u2019s books with explicit memory representations", "author": ["Felix Hill", "Antoine Bordes", "Sumit Chopra", "Jason Weston"], "venue": "In ICLR,", "citeRegEx": "Hill et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Hill et al\\.", "year": 2016}, {"title": "Long short-term memory", "author": ["Sepp Hochreiter", "J\u00fcrgen Schmidhuber"], "venue": "Neural computation,", "citeRegEx": "Hochreiter and Schmidhuber.,? \\Q1997\\E", "shortCiteRegEx": "Hochreiter and Schmidhuber.", "year": 1997}, {"title": "Blackout: Speeding up recurrent neural network language models with very large vocabularies", "author": ["Shihao Ji", "SVN Vishwanathan", "Nadathur Satish", "Michael J Anderson", "Pradeep Dubey"], "venue": "In ICLR,", "citeRegEx": "Ji et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Ji et al\\.", "year": 2016}, {"title": "Exploring the limits of language modeling", "author": ["Rafal Jozefowicz", "Oriol Vinyals", "Mike Schuster", "Noam Shazeer", "Yonghui Wu"], "venue": "arXiv preprint arXiv:1602.02410,", "citeRegEx": "Jozefowicz et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Jozefowicz et al\\.", "year": 2016}, {"title": "Text understanding with the attention sum reader network", "author": ["Rudolf Kadlec", "Martin Schmid", "Ondrej Bajgar", "Jan Kleindienst"], "venue": "In ACL,", "citeRegEx": "Kadlec et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Kadlec et al\\.", "year": 2016}, {"title": "Adam: A method for stochastic optimization", "author": ["Diederik Kingma", "Jimmy Ba"], "venue": "In ICLR,", "citeRegEx": "Kingma and Ba.,? \\Q2015\\E", "shortCiteRegEx": "Kingma and Ba.", "year": 2015}, {"title": "Recurrent neural network based language model", "author": ["Tomas Mikolov", "Martin Karafi\u00e1t", "Lukas Burget", "Jan Cernock\u1ef3", "Sanjeev Khudanpur"], "venue": "In Interspeech,", "citeRegEx": "Mikolov et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Mikolov et al\\.", "year": 2010}, {"title": "Empirical evaluation and combination of advanced language modeling techniques", "author": ["Tomas Mikolov", "Anoop Deoras", "Stefan Kombrink", "Lukas Burget", "Jan Cernock\u1ef3"], "venue": "In Interspeech,", "citeRegEx": "Mikolov et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Mikolov et al\\.", "year": 2011}, {"title": "Key-value memory networks for directly reading documents", "author": ["Alexander Miller", "Adam Fisch", "Jesse Dodge", "Amir-Hossein Karimi", "Antoine Bordes", "Jason Weston"], "venue": "arXiv preprint arXiv:1606.03126,", "citeRegEx": "Miller et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Miller et al\\.", "year": 2016}, {"title": "On the difficulty of training recurrent neural networks", "author": ["Razvan Pascanu", "Tomas Mikolov", "Yoshua Bengio"], "venue": "In ICML, pp", "citeRegEx": "Pascanu et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Pascanu et al\\.", "year": 2013}, {"title": "Reasoning about entailment with neural attention", "author": ["Tim Rockt\u00e4schel", "Edward Grefenstette", "Karl Moritz Hermann", "Tomas Kocisky", "Phil Blunsom"], "venue": "In ICLR,", "citeRegEx": "Rockt\u00e4schel et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Rockt\u00e4schel et al\\.", "year": 2016}, {"title": "Learning internal representations by error propagation", "author": ["David E Rumelhart", "Geoffrey E Hinton", "Ronald J Williams"], "venue": "Technical report, DTIC Document,", "citeRegEx": "Rumelhart et al\\.,? \\Q1985\\E", "shortCiteRegEx": "Rumelhart et al\\.", "year": 1985}, {"title": "A neural attention model for abstractive sentence summarization", "author": ["Alexander M. Rush", "Sumit Chopra", "Jason Weston"], "venue": "In EMNLP, pp", "citeRegEx": "Rush et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Rush et al\\.", "year": 2015}, {"title": "Sparse non-negative matrix language modeling for skip-grams", "author": ["Noam Shazeer", "Joris Pelemans", "Ciprian Chelba"], "venue": "In Interspeech, pp", "citeRegEx": "Shazeer et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Shazeer et al\\.", "year": 2015}, {"title": "Higher order recurrent neural networks", "author": ["Rohollah Soltani", "Hui Jiang"], "venue": "arXiv preprint arXiv:1605.00064,", "citeRegEx": "Soltani and Jiang.,? \\Q2016\\E", "shortCiteRegEx": "Soltani and Jiang.", "year": 2016}, {"title": "Learning matrices and their applications", "author": ["Karl Steinbuch", "UAW Piske"], "venue": "IEEE Transactions on Electronic Computers,", "citeRegEx": "Steinbuch and Piske.,? \\Q1963\\E", "shortCiteRegEx": "Steinbuch and Piske.", "year": 1963}, {"title": "End-to-end memory networks", "author": ["Sainbayar Sukhbaatar", "Jason Weston", "Rob Fergus"], "venue": "In NIPS, pp. 2440\u20132448,", "citeRegEx": "Sukhbaatar et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Sukhbaatar et al\\.", "year": 2015}, {"title": "Pattern recognition by means of automatic analogue apparatus", "author": ["WK Taylor"], "venue": "Proceedings of the IEE-Part B: Radio and Electronic Engineering,", "citeRegEx": "Taylor.,? \\Q1959\\E", "shortCiteRegEx": "Taylor.", "year": 1959}, {"title": "Recurrent memory networks for language modeling", "author": ["Ke Tran", "Arianna Bisazza", "Christof Monz"], "venue": "In NAACL-HLT,", "citeRegEx": "Tran et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Tran et al\\.", "year": 2016}, {"title": "Natural language comprehension with the epireader", "author": ["Adam Trischler", "Zheng Ye", "Xingdi Yuan", "Kaheer Suleman"], "venue": "arXiv preprint arXiv:1606.02270,", "citeRegEx": "Trischler et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Trischler et al\\.", "year": 2016}, {"title": "Separating answers from queries for neural reading comprehension", "author": ["Dirk Weissenborn"], "venue": "arXiv preprint arXiv:1607.03316,", "citeRegEx": "Weissenborn.,? \\Q2016\\E", "shortCiteRegEx": "Weissenborn.", "year": 2016}, {"title": "Backpropagation through time: what it does and how to do it", "author": ["Paul J Werbos"], "venue": "Proceedings of the IEEE,", "citeRegEx": "Werbos.,? \\Q1990\\E", "shortCiteRegEx": "Werbos.", "year": 1990}, {"title": "Scaling recurrent neural network language models", "author": ["Will Williams", "Niranjani Prasad", "David Mrva", "Tom Ash", "Tony Robinson"], "venue": "In 2015 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP),", "citeRegEx": "Williams et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Williams et al\\.", "year": 2015}, {"title": "Show, attend and tell: Neural image caption generation with visual attention", "author": ["Kelvin Xu", "Jimmy Ba", "Ryan Kiros", "Kyunghyun Cho", "Aaron Courville", "Ruslan Salakhutdinov", "Richard S Zemel", "Yoshua Bengio"], "venue": "In ICML,", "citeRegEx": "Xu et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Xu et al\\.", "year": 2015}, {"title": "Reference-aware language models", "author": ["Zichao Yang", "Phil Blunsom", "Chris Dyer", "Wang Ling"], "venue": "arXiv preprint arXiv:1611.01628,", "citeRegEx": "Yang et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Yang et al\\.", "year": 2016}], "referenceMentions": [{"referenceID": 16, "context": "Neural language models, on the other hand, maintain and update a dense vector representation over a sequence where time dependencies are captured implicitly (Mikolov et al., 2010).", "startOffset": 157, "endOffset": 179}, {"referenceID": 1, "context": "A recent extension of neural sequence models are attention mechanisms (Bahdanau et al., 2015), which can capture long-range connections more directly.", "startOffset": 70, "endOffset": 93}, {"referenceID": 0, "context": "A recent extension of neural sequence models are attention mechanisms (Bahdanau et al., 2015), which can capture long-range connections more directly. However, we argue that applying such an attention mechanism directly to neural language models requires output vectors to fulfill several purposes at the same time: they need to (i) encode a distribution for predicting the next token, (ii) serve as a key to compute the attention vector, as well as (iii) encode relevant content to inform future predictions. We hypothesize that such overloaded use of output representations makes training the model difficult and propose a modification to the attention mechanism which separates these functions explicitly, inspired by Miller et al. (2016); Ba et al.", "startOffset": 71, "endOffset": 742}, {"referenceID": 0, "context": "(2016); Ba et al. (2016); Reed & de Freitas (2015); Gulcehre et al.", "startOffset": 8, "endOffset": 25}, {"referenceID": 0, "context": "(2016); Ba et al. (2016); Reed & de Freitas (2015); Gulcehre et al.", "startOffset": 8, "endOffset": 51}, {"referenceID": 0, "context": "(2016); Ba et al. (2016); Reed & de Freitas (2015); Gulcehre et al. (2016). Specifically, at every time step our neural language model outputs three vectors.", "startOffset": 8, "endOffset": 75}, {"referenceID": 1, "context": "Augmenting a neural language model with attention (Bahdanau et al., 2015) is straight-forward.", "startOffset": 50, "endOffset": 73}, {"referenceID": 16, "context": "Inspired by Miller et al. (2016); Ba et al.", "startOffset": 12, "endOffset": 33}, {"referenceID": 0, "context": "(2016); Ba et al. (2016); Reed & de Freitas (2015); Gulcehre et al.", "startOffset": 8, "endOffset": 25}, {"referenceID": 0, "context": "(2016); Ba et al. (2016); Reed & de Freitas (2015); Gulcehre et al.", "startOffset": 8, "endOffset": 51}, {"referenceID": 0, "context": "(2016); Ba et al. (2016); Reed & de Freitas (2015); Gulcehre et al. (2016), we introduce a key-value attention model that separates output vectors into keys used for calculating the attention distribution \u03b1t, and a value part used for encoding the next-word distribution and context representation.", "startOffset": 8, "endOffset": 75}, {"referenceID": 17, "context": "Neural language models often work best in combination with traditional N -gram models (Mikolov et al., 2011; Chelba et al., 2013; Williams et al., 2015; Ji et al., 2016; Shazeer et al., 2015), since the former excel at generalization while the latter ensure memorization.", "startOffset": 86, "endOffset": 191}, {"referenceID": 2, "context": "Neural language models often work best in combination with traditional N -gram models (Mikolov et al., 2011; Chelba et al., 2013; Williams et al., 2015; Ji et al., 2016; Shazeer et al., 2015), since the former excel at generalization while the latter ensure memorization.", "startOffset": 86, "endOffset": 191}, {"referenceID": 32, "context": "Neural language models often work best in combination with traditional N -gram models (Mikolov et al., 2011; Chelba et al., 2013; Williams et al., 2015; Ji et al., 2016; Shazeer et al., 2015), since the former excel at generalization while the latter ensure memorization.", "startOffset": 86, "endOffset": 191}, {"referenceID": 12, "context": "Neural language models often work best in combination with traditional N -gram models (Mikolov et al., 2011; Chelba et al., 2013; Williams et al., 2015; Ji et al., 2016; Shazeer et al., 2015), since the former excel at generalization while the latter ensure memorization.", "startOffset": 86, "endOffset": 191}, {"referenceID": 23, "context": "Neural language models often work best in combination with traditional N -gram models (Mikolov et al., 2011; Chelba et al., 2013; Williams et al., 2015; Ji et al., 2016; Shazeer et al., 2015), since the former excel at generalization while the latter ensure memorization.", "startOffset": 86, "endOffset": 191}, {"referenceID": 2, "context": ", 2011; Chelba et al., 2013; Williams et al., 2015; Ji et al., 2016; Shazeer et al., 2015), since the former excel at generalization while the latter ensure memorization. In addition, from initial experiments with memory-augmented neural language models, we found that usually only the previous five output representations are utilized. This is in line with observations by Tran et al. (2016). Hence, we experiment with a much simpler architecture depicted in Figure 1d.", "startOffset": 8, "endOffset": 393}, {"referenceID": 1, "context": "Attending over previous state outputs on top of an RNN encoder has improved performances in a wide range of tasks, including machine translation (Bahdanau et al., 2015), recognizing textual entailment (Rockt\u00e4schel et al.", "startOffset": 145, "endOffset": 168}, {"referenceID": 20, "context": ", 2015), recognizing textual entailment (Rockt\u00e4schel et al., 2016), sentence summarization (Rush et al.", "startOffset": 40, "endOffset": 66}, {"referenceID": 22, "context": ", 2016), sentence summarization (Rush et al., 2015), image captioning (Xu et al.", "startOffset": 32, "endOffset": 51}, {"referenceID": 33, "context": ", 2015), image captioning (Xu et al., 2015) and speech recognition (Chorowski et al.", "startOffset": 26, "endOffset": 43}, {"referenceID": 4, "context": ", 2015) and speech recognition (Chorowski et al., 2015).", "startOffset": 31, "endOffset": 55}, {"referenceID": 20, "context": "Early attempts of using memory in neural networks have been undertaken by Taylor (1959) and Steinbuch & Piske (1963) by performing nearest-neighbor operations on input vectors and fitting parametric models to the retrieved sets.", "startOffset": 74, "endOffset": 88}, {"referenceID": 20, "context": "Early attempts of using memory in neural networks have been undertaken by Taylor (1959) and Steinbuch & Piske (1963) by performing nearest-neighbor operations on input vectors and fitting parametric models to the retrieved sets.", "startOffset": 74, "endOffset": 117}, {"referenceID": 20, "context": "Early attempts of using memory in neural networks have been undertaken by Taylor (1959) and Steinbuch & Piske (1963) by performing nearest-neighbor operations on input vectors and fitting parametric models to the retrieved sets. The dedicated use of external memory in neural architectures has more recently witnessed increased interest. Weston et al. (2015) introduced Memory Networks to explicitly segregate memory storage from the computation of the neural network, and Sukhbaatar et al.", "startOffset": 74, "endOffset": 359}, {"referenceID": 20, "context": "(2015) introduced Memory Networks to explicitly segregate memory storage from the computation of the neural network, and Sukhbaatar et al. (2015) trained this model end-to-end with an attention-based memory addressing mechanism.", "startOffset": 121, "endOffset": 146}, {"referenceID": 5, "context": "The Neural Turing Machines by Graves et al. (2014) add an external differentiable memory with read-write functions to a controller recurrent neural network, and has shown promising results in simple sequence tasks such as copying and sorting.", "startOffset": 30, "endOffset": 51}, {"referenceID": 1, "context": "Attending over previous state outputs on top of an RNN encoder has improved performances in a wide range of tasks, including machine translation (Bahdanau et al., 2015), recognizing textual entailment (Rockt\u00e4schel et al., 2016), sentence summarization (Rush et al., 2015), image captioning (Xu et al., 2015) and speech recognition (Chorowski et al., 2015). Recently, Cheng et al. (2016) proposed an architecture that modifies the standard LSTM by replacing the memory cell with a memory network (Weston et al.", "startOffset": 146, "endOffset": 387}, {"referenceID": 28, "context": "Another class of models that include memory into sequence modeling are Recurrent Memory Networks (RMNs) (Tran et al., 2016).", "startOffset": 104, "endOffset": 123}, {"referenceID": 18, "context": "Finally, the functional separation of look-up keys and memory content has been found useful for Memory Networks (Miller et al., 2016), Neural Programmer-Interpreters (Reed & de Freitas, 2015), Dynamic Neural Turing Machines (Gulcehre et al.", "startOffset": 112, "endOffset": 133}, {"referenceID": 9, "context": ", 2016), Neural Programmer-Interpreters (Reed & de Freitas, 2015), Dynamic Neural Turing Machines (Gulcehre et al., 2016), and Fast Associative Memory (Ba et al.", "startOffset": 98, "endOffset": 121}, {"referenceID": 0, "context": ", 2016), and Fast Associative Memory (Ba et al., 2016).", "startOffset": 37, "endOffset": 54}, {"referenceID": 19, "context": "Furthermore, we apply gradient clipping at a gradient norm of 5 (Pascanu et al., 2013).", "startOffset": 64, "endOffset": 86}, {"referenceID": 13, "context": "The bias of the LSTM\u2019s forget gate is initialized to 1 (Jozefowicz et al., 2016), while other parameters are initialized uniformly from the range (\u22120.", "startOffset": 55, "endOffset": 80}, {"referenceID": 21, "context": "Backpropagation Through Time (Rumelhart et al., 1985; Werbos, 1990) was used to train the network with 20 steps of unrolling.", "startOffset": 29, "endOffset": 67}, {"referenceID": 31, "context": "Backpropagation Through Time (Rumelhart et al., 1985; Werbos, 1990) was used to train the network with 20 steps of unrolling.", "startOffset": 29, "endOffset": 67}, {"referenceID": 28, "context": "Model Attention Window Size 1 5 10 15 RM(+tM-g) (Tran et al., 2016) 83.", "startOffset": 48, "endOffset": 67}, {"referenceID": 28, "context": "1 RM(+tM-g) (Tran et al., 2016) 300 300 15 93.", "startOffset": 12, "endOffset": 31}, {"referenceID": 10, "context": "(d) Results on CBT; those marked with \u2021 are taken from Hill et al. (2016).", "startOffset": 55, "endOffset": 74}, {"referenceID": 14, "context": "703 AS Reader, avg ensemble (Kadlec et al., 2016) 0.", "startOffset": 28, "endOffset": 49}, {"referenceID": 14, "context": "689 \u2212 \u2212 AS Reader, greedy ensemble (Kadlec et al., 2016) 0.", "startOffset": 35, "endOffset": 56}, {"referenceID": 30, "context": "675 \u2212 \u2212 QANN, 4 hops, GloVe (Weissenborn, 2016) 0.", "startOffset": 28, "endOffset": 47}, {"referenceID": 7, "context": "657 \u2212 \u2212 GA Reader, ensemble (Dhingra et al., 2016) 0.", "startOffset": 28, "endOffset": 50}, {"referenceID": 29, "context": "694 \u2212 \u2212 EpiReader, ensemble (Trischler et al., 2016) 0.", "startOffset": 28, "endOffset": 52}, {"referenceID": 28, "context": "774 RM(+tM-g) (Tran et al., 2016) 0.", "startOffset": 14, "endOffset": 33}, {"referenceID": 28, "context": "We reimplemented the Recurrent-Memory model by Tran et al. (2016) with the temporal matrix and gating composition function (RM+tM-g).", "startOffset": 47, "endOffset": 66}], "year": 2017, "abstractText": "Neural language models predict the next token using a latent representation of the immediate token history. Recently, various methods for augmenting neural language models with an attention mechanism over a differentiable memory have been proposed. For predicting the next token, these models query information from a memory of the recent history which can facilitate learning midand long-range dependencies. However, conventional attention mechanisms used in memoryaugmented neural language models produce a single output vector per time step. This vector is used both for predicting the next token as well as for the key and value of a differentiable memory of a token history. In this paper, we propose a neural language model with a key-value attention mechanism that outputs separate representations for the key and value of a differentiable memory, as well as for encoding the next-word distribution. This model outperforms existing memoryaugmented neural language models on two corpora. Yet, we found that our method mainly utilizes a memory of the five most recent output representations. This led to the unexpected main finding that a much simpler model based only on the concatenation of recent output representations from previous time steps is on par with more sophisticated memory-augmented neural language models.", "creator": "LaTeX with hyperref package"}}}