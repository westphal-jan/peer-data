{"id": "1702.04595", "review": {"conference": "iclr", "VERSION": "v1", "DATE_OF_SUBMISSION": "15-Feb-2017", "title": "Visualizing Deep Neural Network Decisions: Prediction Difference Analysis", "abstract": "This article presents the prediction difference analysis method for visualizing the response of a deep neural network to a specific input. When classifying images, the method highlights areas in a given input image that provide evidence for or against a certain class. It overcomes several shortcoming of previous methods and provides great additional insight into the decision making process of classifiers. Making neural network decisions interpretable through visualization is important both to improve models and to accelerate the adoption of black-box classifiers in application areas such as medicine. We illustrate the method in experiments on natural images (ImageNet data), as well as medical images (MRI brain scans).", "histories": [["v1", "Wed, 15 Feb 2017 13:25:26 GMT  (7723kb,D)", "http://arxiv.org/abs/1702.04595v1", "ICLR2017"]], "COMMENTS": "ICLR2017", "reviews": [], "SUBJECTS": "cs.CV cs.AI", "authors": ["luisa m zintgraf", "taco s cohen", "tameem adel", "max welling"], "accepted": true, "id": "1702.04595"}, "pdf": {"name": "1702.04595.pdf", "metadata": {"source": "CRF", "title": "VISUALIZING DEEP NEURAL NETWORK DECISIONS: PREDICTION DIFFERENCE ANALYSIS", "authors": ["Luisa M Zintgraf", "Taco S Cohen", "Tameem Adel", "Max Welling"], "emails": ["lmzintgraf@gmail.com,", "tameem.hesham@gmail.com,", "m.welling}@uva.nl"], "sections": [{"heading": "1 INTRODUCTION", "text": "In recent years, deep neural networks (DNNs) have emerged as the method of choice for perceptual tasks such as speech recognition and image classification. Essentially, a DNN is a highly complex nonlinear function that makes it difficult to understand how a particular classification comes about. This lack of transparency is a major obstacle to the adoption of deep learning in areas of industry, government, and healthcare where the costs of mistakes are high. In order to realize the societal promise of deep learning - for example, through self-driving cars or personalized medicine - it is imperative that classifiers learn to explain their decisions, whether in the laboratory, in the clinic, or in the courtroom. In scientific applications, a better understanding of the complex dependencies learned through deep networks could lead to new insights and theories in poorly understood areas. In this paper, we present a new, probably well-founded methodology for explaining classification decisions made through deep neural networks."}, {"heading": "2 RELATED WORK", "text": "Broadly speaking, there are two approaches to understanding DCNNs through visualization that are examined in the literature: Find an input image that maximally activates a given unit or class rating to visualize what the network is looking for (Erhan et al., 2009; Simonyan et al., 2013; Yosinski et al., 2015), or visualize how the network reacts to a given input image to explain a specific classification that is made by the network, which will be the subject of this paper.One such example-specific method is that of Simonyan et al. (2013), which measures how sensitively the classification value responds to small changes in pixel values by calculating the partial derivation of the class point with respect to the input characteristics using standard backpropagation. They also show that there is a close link to the use of deconvolutionary networks for visualization, proposed by Zeiler & Fergus (2014)."}, {"heading": "3 APPROACH", "text": "Our method is based on the method presented by Robnik-\u0160ikonja & Kononenko (2008), which we will now review. To make a given prediction, the method assigns a relevance value to each input characteristic in relation to a class. The basic idea is that the relevance of a characteristic xi can be estimated by measuring how the prediction changes when the characteristic is unknown, i.e. the difference between p (c) x (x) and p (c) x (x) x (i), where x\\ i is the set of all input characteristics except xi.To find out how the prediction changes when a characteristic is unknown, the authors propose three strategies: the first is to designate the characteristic as unknown (which few classifiers allow), and the second is to train the classifier with the characteristic that is clearly impracticable for DNNs and high-dimensional data such as images."}, {"heading": "3.1 CONDITIONAL SAMPLING", "text": "In Equation (3), the conditional probability p (xi | x\\ i) of a character xi is approximated using the boundary distribution p (xi), which is a very rough approximation. In images, for example, the value of a pixel is strongly dependent on other pixels. We propose a much more precise approximation based on the following two observations: A pixel depends most heavily on a small neighborhood around it, and the condition of a pixel in its neighborhood does not depend on the position of the pixel in the image. Thus, for a pixel xi, we can find a spot x-i of size l that contains xi, and the condition for the remaining pixels in that spot: p (xi | x\\ i)."}, {"heading": "3.2 MULTIVARIATE ANALYSIS", "text": "Robnik-\u0160ikonja & Kononenko (2008) takes a univariable approach: only one feature is removed at a time, but we would expect a neural network to be relatively robust if only one feature of a high-dimensional input is unknown, such as a pixel in an image. Therefore, we will remove several features at once, using our knowledge of images again by strategically selecting these features: patches of interconnected pixels. Instead of going through each individual pixel, we will go through all patches of the size k \u00b7 k in the image (k \u00b7 k \u00b7 3 for RGB images and k \u00b7 k for 3D images such as MRI scans), which will be implemented in a sliding window manner. The patches overlap so that ultimately the relevance of a single pixel is obtained by taking the average relevance obtained from the various patches in which it was. Algorithm 1 and Figure 2 illustrate how the method can be implemented, taking the proposed improvements into account."}, {"heading": "3.3 DEEP VISUALIZATION OF HIDDEN LAYERS", "text": "If we try to understand neural networks and how they make decisions, it is not only interesting to analyze the input-output relationship of the classifier, but also to look at what is going on within the hidden layers of the network. We can adjust the method to see how the units of any layer of the network affect a node from a deeper layer. Mathematically, we can formulate this as follows: Let h be the vector representation of the values in a layer H in the network (after we propagated the input up to that layer forward). Furthermore, let z = z (h) [z (h)] = hi p (h\\ i) \u2212 z (now a node in a subsequent layer. Then, the analogue of the equation (2) is given by the expectation: g (z | h\\ i) instead of Ep (hi | h\\ i) [z (h)] [z (h) = hi p (h\\ i) \u2212 z (now we are evaluated as the distribution of the equation h (when we are expressed as the equation h), if the equation is the equation (i) h (now we are expressed as the distribution of the equation, if the equation is the equation h (i)."}, {"heading": "4 EXPERIMENTS", "text": "In this section, we will demonstrate how the proposed visualization method can be applied on the ImageNet dataset of natural images using DCNNs (Section 4.1) and on a medical imaging dataset of MRI scans using a logistical regression classifier (Section 4.2). For the edge sample, we always use empirical distribution, i.e. we replace a feature (patch) with samples taken directly from other images in the same place. We use a multivariate normal distribution for conditional sampling. For both sample methods, we use 10 samples to estimate p (c | x\\ i) (since no significant difference was observed with more samples). Note that all images are best viewed digitally and in color.Our implementation is available at github.com / lmzintgraf / DeepVis-PredDiff."}, {"heading": "4.1 IMAGENET: UNDERSTANDING HOW A DCNN MAKES DECISIONS", "text": "In fact, it is such that most of them will be able to move into another world, in which they are able to put themselves into another world, in which they are able to move, in which they move, in which they move, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they, in which they, in which they live, in which they, in which they, in which they live, in which they, in which they are able to move, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they, in which they, in which they live, in which they, in which they live, in which they, in which they live, in which they, in which they, in which they live, in which they, in which they, in which they, in which they, in which they, in which they, in which they live, in which they, in which they, in which they, in which they, in which they, live, in which they, in which they, in which they, live, in which they, in which they, in which they, live, in which they, in which they, in which they, in which they, in which they, live, in which they, in which they, in"}, {"heading": "4.2 MRI DATA: EXPLAINING CLASSIFIER DECISIONS IN MEDICAL IMAGING", "text": "This year, we will be able to look for a solution that is capable, that we are able to find a solution, that we are able to find a solution, that we are able to find a solution, that we are able to find a solution, that we are able to find a solution, that we are able to find a solution, that we are able to find a solution, that we are able to find a solution, that we are able to find a solution, that we are able to find a solution, that we are able to find a solution. \""}, {"heading": "5 FUTURE WORK", "text": "In our experiments, we used a simple multivariate normal distribution for conditional samples. We can imagine that the use of more complex generative models will lead to better results: pixels that are easily predictable by their environment will be even more weighted, but this will also significantly increase the computational resources needed to create the explanations. Similarly, we could try to modify Equation (4) to achieve an even better approximation by using a conditional distribution that takes more information about the overall picture into account (such as adding spatial information for MRI scans).To make the method applicable for clinical analysis and practice, a better classification algorithm is needed, and software that visualizes the results as an interactive 3D model will improve the system's usability."}, {"heading": "6 CONCLUSION", "text": "We have presented a new method for visualizing deep neural networks, which improves on previous methods by using a more powerful conditional, multivariate model, which shows which pixels of an input image are evidence for or against a node in the network. Significant information provides new insights - for network exploration as well as for adoption and usability in areas such as healthcare. While our method requires considerable computing resources, 3D visualization in real time is possible when visualizations are pre-calculated. Further optimization and powerful GPUs can further reduce the pre-computing time. In our experiments, we have presented several ways in which the visualization method can be used to analyze the decision-making of DCNNs."}, {"heading": "ACKNOWLEDGMENTS", "text": "This work was supported by AWS in Education Grant award. We thank Facebook and Google for their financial support, and our reviewers for their time and valuable, constructive feedback. This work was also partially supported by: Innoviris, the Brussels Institute for Research and Innovation, Brussels, Belgium; the Nuts-OHRA Foundation HIV (grant no. HIV (grant no. 1003-026), Amsterdam, The Netherlands; The Netherlands Organization for Health Research and Development (ZonMW) together with the AIDS Fund (grant no 300020007 and 2009063). Other unlimited scientific grants were provided by Gilead Sciences, ViiV Healthcare, Janssen Pharmaceutica N.V., The Netherlands Organization for Health Research and Development N.V., Bristol-Myers Squibb, Boehringer Ingelheim, and Merck & Co.We thank Barbara Elsenga, Jane Berkel, Sandra Moll, Maja Tott\u00e9, and Marjolein Martens Martens Martens for running the AG.Eh.IV program, study and capturing data and care."}, {"heading": "A RANDOM RESULTS", "text": "It is not the first time that the EU Commission has taken such a step."}], "references": [{"title": "Non-linear optimisation. fmrib technical report tr07ja1", "author": ["Jesper LR Andersson", "Mark Jenkinson", "Stephen Smith"], "venue": "University of Oxford FMRIB Centre: Oxford,", "citeRegEx": "Andersson et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Andersson et al\\.", "year": 2007}, {"title": "On pixel-wise explanations for non-linear classifier decisions by layer-wise relevance propagation", "author": ["Sebastian Bach", "Alexander Binder", "Gr\u00e9goire Montavon", "Frederick Klauschen", "Klaus-Robert M\u00fcller", "Wojciech Samek"], "venue": "PloS one,", "citeRegEx": "Bach et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Bach et al\\.", "year": 2015}, {"title": "Describing the brain in autism in five dimensions\u2014magnetic resonance imaging-assisted diagnosis of autism spectrum disorder using a multiparameter classification approach", "author": ["Christine Ecker", "Andre Marquand", "Janaina Mour\u00e3o-Miranda", "Patrick Johnston", "Eileen M Daly", "Michael J Brammer", "Stefanos Maltezos", "Clodagh M Murphy", "Dene Robertson", "Steven C Williams"], "venue": "The Journal of Neuroscience,", "citeRegEx": "Ecker et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Ecker et al\\.", "year": 2010}, {"title": "Visualizing higher-layer features of a deep network", "author": ["Dumitru Erhan", "Yoshua Bengio", "Aaron Courville", "Pascal Vincent"], "venue": "Dept. IRO, Universite\u0301 de Montre\u0301al, Tech. Rep,", "citeRegEx": "Erhan et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Erhan et al\\.", "year": 2009}, {"title": "Analytic estimation of statistical significance maps for support vector machine based multi-variate image analysis and classification", "author": ["Bilwaj Gaonkar", "Christos Davatzikos"], "venue": null, "citeRegEx": "Gaonkar and Davatzikos.,? \\Q2013\\E", "shortCiteRegEx": "Gaonkar and Davatzikos.", "year": 2013}, {"title": "On the interpretation of weight vectors of linear models in multivariate neuroimaging", "author": ["Stefan Haufe", "Frank Meinecke", "Kai G\u00f6rgen", "Sven D\u00e4hne", "John-Dylan Haynes", "Benjamin Blankertz", "Felix Bie\u00dfmann"], "venue": null, "citeRegEx": "Haufe et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Haufe et al\\.", "year": 2014}, {"title": "Caffe: Convolutional architecture for fast feature embedding", "author": ["Yangqing Jia", "Evan Shelhamer", "Jeff Donahue", "Sergey Karayev", "Jonathan Long", "Ross Girshick", "Sergio Guadarrama", "Trevor Darrell"], "venue": "arXiv preprint arXiv:1408.5093,", "citeRegEx": "Jia et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Jia et al\\.", "year": 2014}, {"title": "Automatic classification of mr scans in alzheimer\u2019s disease", "author": ["Stefan Kl\u00f6ppel", "Cynthia M Stonnington", "Carlton Chu", "Bogdan Draganski", "Rachael I Scahill", "Jonathan D Rohrer", "Nick C Fox", "Clifford R Jack", "John Ashburner", "Richard SJ Frackowiak"], "venue": null, "citeRegEx": "Kl\u00f6ppel et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Kl\u00f6ppel et al\\.", "year": 2008}, {"title": "Imagenet classification with deep convolutional neural networks. In Advances in neural information processing", "author": ["Alex Krizhevsky", "Ilya Sutskever", "Geoffrey E Hinton"], "venue": null, "citeRegEx": "Krizhevsky et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Krizhevsky et al\\.", "year": 2012}, {"title": "Classifying brain states and determining the discriminating activation patterns: Support vector machine on functional mri data", "author": ["Janaina Mourao-Miranda", "Arun LW Bokde", "Christine Born", "Harald Hampel", "Martin Stetter"], "venue": null, "citeRegEx": "Mourao.Miranda et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Mourao.Miranda et al\\.", "year": 2005}, {"title": "Explaining classifications for individual instances", "author": ["Marko Robnik-\u0160ikonja", "Igor Kononenko"], "venue": "Knowledge and Data Engineering, IEEE Transactions on,", "citeRegEx": "Robnik.\u0160ikonja and Kononenko.,? \\Q2008\\E", "shortCiteRegEx": "Robnik.\u0160ikonja and Kononenko.", "year": 2008}, {"title": "ImageNet Large Scale Visual Recognition Challenge", "author": ["Olga Russakovsky", "Jia Deng", "Hao Su", "Jonathan Krause", "Sanjeev Satheesh", "Sean Ma", "Zhiheng Huang", "Andrej Karpathy", "Aditya Khosla", "Michael Bernstein", "Alexander C. Berg", "Li Fei-Fei"], "venue": "International Journal of Computer Vision (IJCV),", "citeRegEx": "Russakovsky et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Russakovsky et al\\.", "year": 2015}, {"title": "A data-centric neuroscience gateway: design, implementation, and experiences", "author": ["Shayan Shahand", "Ammar Benabdelkader", "Mohammad Mahdi Jaghoori", "Mostapha al Mourabit", "Jordi Huguet", "Matthan WA Caan", "Antoine HC Kampen", "S\u00edlvia D Olabarriaga"], "venue": "Concurrency and Computation: Practice and Experience,", "citeRegEx": "Shahand et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Shahand et al\\.", "year": 2015}, {"title": "Not just a black box: Learning important features through propagating activation differences", "author": ["Avanti Shrikumar", "Peyton Greenside", "Anna Shcherbina", "Anshul Kundaje"], "venue": "arXiv preprint arXiv:1605.01713,", "citeRegEx": "Shrikumar et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Shrikumar et al\\.", "year": 2016}, {"title": "Very deep convolutional networks for large-scale image recognition", "author": ["Karen Simonyan", "Andrew Zisserman"], "venue": "arXiv preprint arXiv:1409.1556,", "citeRegEx": "Simonyan and Zisserman.,? \\Q2014\\E", "shortCiteRegEx": "Simonyan and Zisserman.", "year": 2014}, {"title": "Deep inside convolutional networks: Visualising image classification models and saliency maps", "author": ["Karen Simonyan", "Andrea Vedaldi", "Andrew Zisserman"], "venue": "arXiv preprint arXiv:1312.6034,", "citeRegEx": "Simonyan et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Simonyan et al\\.", "year": 2013}, {"title": "Going deeper with convolutions", "author": ["Christian Szegedy", "Wei Liu", "Yangqing Jia", "Pierre Sermanet", "Scott Reed", "Dragomir Anguelov", "Dumitru Erhan", "Vincent Vanhoucke", "Andrew Rabinovich"], "venue": "In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition,", "citeRegEx": "Szegedy et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Szegedy et al\\.", "year": 2015}, {"title": "Support vector machine learning-based fmri data group analysis", "author": ["Ze Wang", "Anna R Childress", "Jiongjiong Wang", "John A Detre"], "venue": null, "citeRegEx": "Wang et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Wang et al\\.", "year": 2007}, {"title": "Understanding neural networks through deep visualization", "author": ["Jason Yosinski", "Jeff Clune", "Anh Nguyen", "Thomas Fuchs", "Hod Lipson"], "venue": "arXiv preprint arXiv:1506.06579,", "citeRegEx": "Yosinski et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Yosinski et al\\.", "year": 2015}, {"title": "Visualizing and understanding convolutional networks", "author": ["Matthew D Zeiler", "Rob Fergus"], "venue": "vision\u2013ECCV", "citeRegEx": "Zeiler and Fergus.,? \\Q2014\\E", "shortCiteRegEx": "Zeiler and Fergus.", "year": 2014}, {"title": "Learning deep features for discriminative localization", "author": ["Bolei Zhou", "Aditya Khosla", "Agata Lapedriza", "Aude Oliva", "Antonio Torralba"], "venue": "In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition,", "citeRegEx": "Zhou et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Zhou et al\\.", "year": 2016}], "referenceMentions": [{"referenceID": 3, "context": "Broadly speaking, there are two approaches for understanding DCNNs through visualization investigated in the literature: find an input image that maximally activates a given unit or class score to visualize what the network is looking for (Erhan et al., 2009; Simonyan et al., 2013; Yosinski et al., 2015), or visualize how the network responds to a specific input image in order to explain a particular classification made by the network.", "startOffset": 239, "endOffset": 305}, {"referenceID": 15, "context": "Broadly speaking, there are two approaches for understanding DCNNs through visualization investigated in the literature: find an input image that maximally activates a given unit or class score to visualize what the network is looking for (Erhan et al., 2009; Simonyan et al., 2013; Yosinski et al., 2015), or visualize how the network responds to a specific input image in order to explain a particular classification made by the network.", "startOffset": 239, "endOffset": 305}, {"referenceID": 18, "context": "Broadly speaking, there are two approaches for understanding DCNNs through visualization investigated in the literature: find an input image that maximally activates a given unit or class score to visualize what the network is looking for (Erhan et al., 2009; Simonyan et al., 2013; Yosinski et al., 2015), or visualize how the network responds to a specific input image in order to explain a particular classification made by the network.", "startOffset": 239, "endOffset": 305}, {"referenceID": 7, "context": "In the field of medical image classification specifically, a widely used method for visualizing feature importances is to simply plot the weights of a linear classifier (Kl\u00f6ppel et al., 2008; Ecker et al., 2010), or the p-values of these weights (determined by permutation testing) (Mourao-Miranda et al.", "startOffset": 169, "endOffset": 211}, {"referenceID": 2, "context": "In the field of medical image classification specifically, a widely used method for visualizing feature importances is to simply plot the weights of a linear classifier (Kl\u00f6ppel et al., 2008; Ecker et al., 2010), or the p-values of these weights (determined by permutation testing) (Mourao-Miranda et al.", "startOffset": 169, "endOffset": 211}, {"referenceID": 9, "context": ", 2010), or the p-values of these weights (determined by permutation testing) (Mourao-Miranda et al., 2005; Wang et al., 2007).", "startOffset": 78, "endOffset": 126}, {"referenceID": 17, "context": ", 2010), or the p-values of these weights (determined by permutation testing) (Mourao-Miranda et al., 2005; Wang et al., 2007).", "startOffset": 78, "endOffset": 126}, {"referenceID": 1, "context": "Broadly speaking, there are two approaches for understanding DCNNs through visualization investigated in the literature: find an input image that maximally activates a given unit or class score to visualize what the network is looking for (Erhan et al., 2009; Simonyan et al., 2013; Yosinski et al., 2015), or visualize how the network responds to a specific input image in order to explain a particular classification made by the network. The latter will be the subject of this paper. One such instance-specific method is class saliency visualization proposed by Simonyan et al. (2013) who measure how sensitive the classification score is to small changes in pixel values, by computing the partial derivative of the class score with respect to the input features using standard backpropagation.", "startOffset": 240, "endOffset": 587}, {"referenceID": 1, "context": "Broadly speaking, there are two approaches for understanding DCNNs through visualization investigated in the literature: find an input image that maximally activates a given unit or class score to visualize what the network is looking for (Erhan et al., 2009; Simonyan et al., 2013; Yosinski et al., 2015), or visualize how the network responds to a specific input image in order to explain a particular classification made by the network. The latter will be the subject of this paper. One such instance-specific method is class saliency visualization proposed by Simonyan et al. (2013) who measure how sensitive the classification score is to small changes in pixel values, by computing the partial derivative of the class score with respect to the input features using standard backpropagation. They also show that there is a close connection to using deconvolutional networks for visualization, proposed by Zeiler & Fergus (2014). Other methods include Shrikumar et al.", "startOffset": 240, "endOffset": 933}, {"referenceID": 1, "context": "Broadly speaking, there are two approaches for understanding DCNNs through visualization investigated in the literature: find an input image that maximally activates a given unit or class score to visualize what the network is looking for (Erhan et al., 2009; Simonyan et al., 2013; Yosinski et al., 2015), or visualize how the network responds to a specific input image in order to explain a particular classification made by the network. The latter will be the subject of this paper. One such instance-specific method is class saliency visualization proposed by Simonyan et al. (2013) who measure how sensitive the classification score is to small changes in pixel values, by computing the partial derivative of the class score with respect to the input features using standard backpropagation. They also show that there is a close connection to using deconvolutional networks for visualization, proposed by Zeiler & Fergus (2014). Other methods include Shrikumar et al. (2016), who compare the activation of a unit when a specific input is fed forward through the net to a reference activation for that unit.", "startOffset": 240, "endOffset": 980}, {"referenceID": 1, "context": "Broadly speaking, there are two approaches for understanding DCNNs through visualization investigated in the literature: find an input image that maximally activates a given unit or class score to visualize what the network is looking for (Erhan et al., 2009; Simonyan et al., 2013; Yosinski et al., 2015), or visualize how the network responds to a specific input image in order to explain a particular classification made by the network. The latter will be the subject of this paper. One such instance-specific method is class saliency visualization proposed by Simonyan et al. (2013) who measure how sensitive the classification score is to small changes in pixel values, by computing the partial derivative of the class score with respect to the input features using standard backpropagation. They also show that there is a close connection to using deconvolutional networks for visualization, proposed by Zeiler & Fergus (2014). Other methods include Shrikumar et al. (2016), who compare the activation of a unit when a specific input is fed forward through the net to a reference activation for that unit. Zhou et al. (2016) and Bach et al.", "startOffset": 240, "endOffset": 1131}, {"referenceID": 1, "context": "(2016) and Bach et al. (2015) also generate interesting visualization results for individual inputs, but are both not as closely related to our method as the two papers mentioned above.", "startOffset": 11, "endOffset": 30}, {"referenceID": 1, "context": "(2016) and Bach et al. (2015) also generate interesting visualization results for individual inputs, but are both not as closely related to our method as the two papers mentioned above. The idea of our method is similar to another analysis Zeiler & Fergus (2014) make: they estimate the importance of input pixels by visualizing the probability of the (correct) class as a function of a gray patch occluding parts of the image.", "startOffset": 11, "endOffset": 263}, {"referenceID": 1, "context": "(2016) and Bach et al. (2015) also generate interesting visualization results for individual inputs, but are both not as closely related to our method as the two papers mentioned above. The idea of our method is similar to another analysis Zeiler & Fergus (2014) make: they estimate the importance of input pixels by visualizing the probability of the (correct) class as a function of a gray patch occluding parts of the image. In this paper, we take a more rigorous approach at both removing information from the image and evaluating the effect of this. In the field of medical image classification specifically, a widely used method for visualizing feature importances is to simply plot the weights of a linear classifier (Kl\u00f6ppel et al., 2008; Ecker et al., 2010), or the p-values of these weights (determined by permutation testing) (Mourao-Miranda et al., 2005; Wang et al., 2007). These are independent of the input image, and, as argued by Gaonkar & Davatzikos (2013) and Haufe et al.", "startOffset": 11, "endOffset": 975}, {"referenceID": 1, "context": "(2016) and Bach et al. (2015) also generate interesting visualization results for individual inputs, but are both not as closely related to our method as the two papers mentioned above. The idea of our method is similar to another analysis Zeiler & Fergus (2014) make: they estimate the importance of input pixels by visualizing the probability of the (correct) class as a function of a gray patch occluding parts of the image. In this paper, we take a more rigorous approach at both removing information from the image and evaluating the effect of this. In the field of medical image classification specifically, a widely used method for visualizing feature importances is to simply plot the weights of a linear classifier (Kl\u00f6ppel et al., 2008; Ecker et al., 2010), or the p-values of these weights (determined by permutation testing) (Mourao-Miranda et al., 2005; Wang et al., 2007). These are independent of the input image, and, as argued by Gaonkar & Davatzikos (2013) and Haufe et al. (2014), interpreting these weights can be misleading in general.", "startOffset": 11, "endOffset": 999}, {"referenceID": 11, "context": "We use images from the ILSVRC challenge (Russakovsky et al., 2015) (a large dataset of natural images from 1000 categories) and three DCNNs: the AlexNet (Krizhevsky et al.", "startOffset": 40, "endOffset": 66}, {"referenceID": 8, "context": ", 2015) (a large dataset of natural images from 1000 categories) and three DCNNs: the AlexNet (Krizhevsky et al., 2012), the GoogLeNet (Szegedy et al.", "startOffset": 94, "endOffset": 119}, {"referenceID": 16, "context": ", 2012), the GoogLeNet (Szegedy et al., 2015) and the (16-layer) VGG network (Simonyan & Zisserman, 2014).", "startOffset": 23, "endOffset": 45}, {"referenceID": 6, "context": "We used the publicly available pre-trained models that were implemented using the deep learning framework caffe (Jia et al., 2014).", "startOffset": 112, "endOffset": 130}, {"referenceID": 6, "context": "We used the publicly available pre-trained models that were implemented using the deep learning framework caffe (Jia et al., 2014). Analyzing one image took us on average 20, 30 and 70 minutes for the respective classifiers AlexNet, GoogLeNet and VGG (using the GPU implementation of caffe and mini-batches with the standard settings of 10 samples and a window size of k = 10). The results shown here are chosen from among a small set of images in order to show a range of behavior of the algorithm. The shown images are quite representative of the performance of the method in general. Examples on randomly selected images, including a comparison to the sensitivity analysis of Simonyan et al. (2013), can be seen in appendix A.", "startOffset": 113, "endOffset": 702}, {"referenceID": 11, "context": "Preprocessing of the data was performed with software developed in-house, using the HPCN-UvA Neuroscience Gateway and using resources of the Dutch e-Science Grid Shahand et al. (2015). As a result, Fractional Anisotropy (FA) maps were computed.", "startOffset": 162, "endOffset": 184}, {"referenceID": 0, "context": "FA images were spatially normalized to standard space Andersson et al. (2007), resulting in volumes with 91\u00d7 109\u00d7 91 = 902, 629 voxels.", "startOffset": 54, "endOffset": 78}], "year": 2017, "abstractText": "This article presents the prediction difference analysis method for visualizing the response of a deep neural network to a specific input. When classifying images, the method highlights areas in a given input image that provide evidence for or against a certain class. It overcomes several shortcoming of previous methods and provides great additional insight into the decision making process of classifiers. Making neural network decisions interpretable through visualization is important both to improve models and to accelerate the adoption of black-box classifiers in application areas such as medicine. We illustrate the method in experiments on natural images (ImageNet data), as well as medical images (MRI brain scans).", "creator": "LaTeX with hyperref package"}}}