{"id": "1702.04767", "review": {"conference": "nips", "VERSION": "v1", "DATE_OF_SUBMISSION": "15-Feb-2017", "title": "Efficient Computation of Moments in Sum-Product Networks", "abstract": "Bayesian online learning algorithms for Sum-Product Networks (SPNs) need to compute moments of model parameters under the one-step update posterior distribution. The best existing method for computing such moments scales quadratically in the size of the SPN, although it scales linearly for trees. We propose a linear-time algorithm that works even when the SPN is a directed acyclic graph (DAG). We achieve this goal by reducing the moment computation problem into a joint inference problem in SPNs and by taking advantage of a special structure of the one-step update posterior distribution: it is a multilinear polynomial with exponentially many monomials, and we can evaluate moments by differentiating. The latter is known as the \\emph{differential trick}. We apply the proposed algorithm to develop a linear time assumed density filter (ADF) for SPN parameter learning. As an additional contribution, we conduct extensive experiments comparing seven different online learning algorithms for SPNs on 20 benchmark datasets. The new linear-time ADF method consistently achieves low runtime due to the efficient linear-time algorithm for moment computation; however, we discover that two other methods (CCCP and SMA) typically perform better statistically, while a third (BMM) is comparable to ADF. Interestingly, CCCP can be viewed as implicitly using the same differentiation trick that we make explicit here. The fact that two of the top four fastest methods use this trick suggests that the same trick might find other uses for SPN learning in the future.", "histories": [["v1", "Wed, 15 Feb 2017 20:40:12 GMT  (42kb,D)", "http://arxiv.org/abs/1702.04767v1", null]], "reviews": [], "SUBJECTS": "cs.LG cs.AI", "authors": ["han zhao", "geoff gordon"], "accepted": true, "id": "1702.04767"}, "pdf": {"name": "1702.04767.pdf", "metadata": {"source": "CRF", "title": "Efficient Computation of Moments in Sum-Product Networks", "authors": ["Han Zhao", "Geoff Gordon"], "emails": ["HAN.ZHAO@CS.CMU.EDU", "GGORDON@CS.CMU.EDU"], "sections": [{"heading": "1. Introduction", "text": "It is only a matter of time before such a process occurs. It is a matter of time before such a process occurs. It has been shown that discrete SPNs are equivalent to arithmetic circuits (ACs). They are universal inference machines through which exact common, marginal and conditional queries can be performed in the size of the network. It has been shown that discrete SPNs are equivalent to arithmetic circuits (ACs). They are in the sense that each SPN can be transformed into an equivalent AC and vice versa into an equivalent space with respect for network size (Rooshenas and Lowd, 2014). SPNs are also closely linked to probable graphical models that can transform each SPN into an equivalent AC and vice versa."}, {"heading": "2. Preliminaries", "text": "We use [n] to abbreviate {1, 2,.., n}. We use an uppercase letter X to denote a random variable, and a bold uppercase letter X to denote a series of random variables (random vector). Similarly, a lowercase letter x is used to denote a value taken from X, and a bold lowercase letter x denotes a common value taken from the random vector X. We use calligraphic letters to denote graphs (e.g. G). Specifically, we reserve S to represent a SPN, and we use h to mean that the height of S | S | is the size of an SPN, i.e. the number of edges plus the number of nodes in the graph."}, {"heading": "2.1 Sum-Product Networks", "text": "A sum product network S is a computational circle over a series of random variables X = {X1,.., Xn}. It is a rooted, directed acyclic graph. The internal nodes of S are sums or products and the leaves are univariate distributions over Xi. In their simplest form, the leaves of S are indicator variables IX = x, which can also be understood as categorical distributions whose total probability mass is based on a single value. Edges of sum nodes are parameterized with positive weights. Let x be an instantiation of the random vector X. We associate an unnormalized root probability or density with each node k when input to the network x with network weights to be w: Vk (x | w) is an instantiation of the random vector X."}, {"heading": "2.2 Bayesian Networks and Mixture Models", "text": "In fact, most of them are in a position to put themselves in the world in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they, in which they live, in which they, in which they live, in which they, in which they live, in which they live, in which they, in which they live, in which they live, in which they live, in which they live."}, {"heading": "2.3 Differential Approach for Inference in Bayesian Networks", "text": "This novel idea can be traced back to Darwiche (2003). We also refer to Darwiche (2003) for a more complete and detailed discussion of this approach. We start with a simple BN with 2 binary random variables. Using variable notations, we can represent the common probability distribution of a BN as follows: g (X1 = j) = p00Ix (2 + p01Ix) = p10Ix1Ix (2 + p11Ix1Ix): the common probability distribution of a BN as follows: g (X2 = j) = p00Ix (2) = p00Ix (2 + p10Ix1Ix)."}, {"heading": "3. Linear Time Exact Moment Computation", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "3.1 Exact Posterior Has Exponentially Many Modes", "text": "It is worth pointing out the fully factored prior distribution justified by the two-part graph structure of the BN equivalent that we introduced in Section 2.2. We are interested in calculating the moments of the posterior distribution after we have received an observation from the world. Essentially, this is the Bayean online learning environment in which we update the belief about the distribution of model parameters as we observe data from the world sequentially. Note that wk corresponds to the weight vector associated with the sum node k, so that wk is a vector that is wk > 0 and 1Twk = 1. For the simplicity of the discussion, let us assume that the previous distribution is for each wk dirichlet, i.e., p0 x x x x x x."}, {"heading": "3.2 The Hardness of Computing Moments", "text": "In order to find an approximate distribution corresponding to the moments of the exact rear part, we need to be able to calculate these moments below the exact rear level. This is not a problem for traditional blending models, where the effective number of blending components in this section will show how the number of blending components in these models is assumed to be small. However, this is not the case for SPNs where the effective number of blending components is. In this section, we will show how we use the polynomial network of S to reduce this complexity to O."}, {"heading": "3.3 Efficient Polynomial Evaluation by Differentiation", "text": "(c). (c). (c). (c). (c). (c). (c). (c). (c). (c). (c). (c). (c). (c). (c). (c). (c). (c). (c). (c). (c). (c). (c). (c). (c). (c). (c). (c). (c). (c). (c). (c). (c). (c). (c). (c). (c). (c). (c). (c). (c). (c) (c). (c). (c). (c). (c). (c). (c). (c). (c). (c). (c). (c). (c). (c). (c). (c). (c). (c). (c). (c). (c). (c). (c). (c). (c). (c). (c). (c). (c). (c). (c). (c). (c). (c). (c). (c). (c). ((c). (c). (c). ((((c). (((c). ((c). ((c). (c). (c). ((c). (c). (((c). ((c). (. ((((. (((.).). (. ((((((((.).). (.). (.). (. (.). (. (. (.). (. (.). (.). (. (.). (. (. (.). (. (.). (.). (. (.). (.). (.). (. (.). (. (.). (. (. (.). (.). (.). (. ("}, {"heading": "4. Online Moment Matching", "text": "In this section, we will use Alg. 1 as a sub-routine to develop a new Bayesian online learning algorithm for SPNs based on assumed density filtering (Sorenson and Stubberud, 1968). To do this, we will find an approximate distribution by minimizing the KL divergence between the posterior one-step update and the approximate distribution. Note that since p0 (w; \u03b1) p = 1 Dir (wk; \u03b2k)}, i.e. P is the space of the product of the dirichlet densities, which are decompatible across all sum nodes in S. Note: Since p0 (w; \u03b1), the PLC distribution is fully decompatible, we have a natural choice to find an approximate distribution q-P, so that q is the KL divergence between p (w | x) and q, i.e., p = arg min."}, {"heading": "5. Experiments", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "5.1 Experimental Setting", "text": "We are conducting experiments on 20 real data sets that have been used as benchmarks to evaluate the effectiveness of learning algorithms for SPNs (Gens and Domingos, 2013; Rooshenas and Lowd, 2014; Zhao et al., 2016a; Adel et al., 2015; Vergari et al., 2015) Statistics on the 20 data sets and their corresponding SPN models are shown in Table 1. All random variables in these 20 data sets are binary. We are using LearnSPN as a structure learning algorithm to build structures for the 20 data sets. Since LearnSPN, along with other structure learning algorithms for SPNs (Adel et al., 2015; Rooshenas and Lowd, 2014), we will only return structured networks that we constructed by merging leaf nodes with the same distributions, resulting in general networks with DAG structures."}, {"heading": "5.2 Results", "text": "All SPNs used in our experiments are DAGs, using the merging techniques introduced in the last section. Consequently, the moment calculation algorithm introduced in Rashwan et al. (2016) for trees is not applicable here. A naive algorithm for calculating moments on DAGs scales square in the size of the network, and it does not end within 24 hours for even the smallest SPN (NLTCS). For comparison, by calculating the exact moments of posterior distribution after each observation, both BMM and ADF can be executed in linear time in the size of the network as well as the training data."}, {"heading": "6. Conclusion", "text": "We propose a linear time algorithm to efficiently calculate the moments of model parameters in SPNs. The key technique in designing our algorithm is the differential trick, which is able to efficiently evaluate a multilinear function because the network polynomial allows tractable factorization. With the proposed algorithm as a subroutine, we are able to improve the temporal complexity of the BMM from square to linear to general SPNs with DAG structures. We also use this subroutine to design a new online algorithm, ADF. Extensive experiments are being conducted to evaluate the previously proposed online learning algorithms. We show that ADF and BMM are faster than all competitors except CCCP by using the proposed moment calculation algorithm. As a future direction, we hope to use the proposed moment calculation algorithm in designing efficient structure learning algorithms SPNC for future applications, we could also expect the same algorithm here for NC."}, {"heading": "Appendix A. Details about Solving Moment Matching Equations in BMM and ADF", "text": "In the appendix, we show that minimizing the KL divergence between the one-step update of the posterior distribution and the approximate distribution results in a moment-matching equation that must be solved when the approximate q distribution starts from the exponential family distributions. We will also detail the system of approximate equations to be solved in BMM and ADF for SPNs. Starting from the approximate q-w-p distribution with the natural parameter distribution, since q is assumed to be exponential family distribution, we have an approximate equation (w) g (p) exp (p) exp (w)), where T (w) is the sufficient statistic of the exponential family distribution. Considering the minimization problem about q, we havemin q. P KL (w): p (w) x) = min q-p (w) exp (w) x) log-p (w)."}], "references": [{"title": "Learning the structure of sum-product networks via an svdbased algorithm", "author": ["T. Adel", "D. Balduzzi", "A. Ghodsi"], "venue": "In Proceedings of UAI,", "citeRegEx": "Adel et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Adel et al\\.", "year": 2015}, {"title": "Context-specific independence in Bayesian networks", "author": ["C. Boutilier", "N. Friedman", "M. Goldszmidt", "D. Koller"], "venue": "In Proceedings of the Twelfth international conference on Uncertainty in artificial intelligence,", "citeRegEx": "Boutilier et al\\.,? \\Q1996\\E", "shortCiteRegEx": "Boutilier et al\\.", "year": 1996}, {"title": "Information projections revisited", "author": ["I. Csisz\u00e1r", "F. Matus"], "venue": "IEEE Transactions on Information Theory,", "citeRegEx": "Csisz\u00e1r and Matus.,? \\Q2003\\E", "shortCiteRegEx": "Csisz\u00e1r and Matus.", "year": 2003}, {"title": "A differential approach to inference in bayesian networks", "author": ["A. Darwiche"], "venue": "Journal of the ACM (JACM),", "citeRegEx": "Darwiche.,? \\Q2003\\E", "shortCiteRegEx": "Darwiche.", "year": 2003}, {"title": "Greedy structure search for sum-product networks", "author": ["A. Dennis", "D. Ventura"], "venue": "In International Joint Conference on Artificial Intelligence,", "citeRegEx": "Dennis and Ventura.,? \\Q2015\\E", "shortCiteRegEx": "Dennis and Ventura.", "year": 2015}, {"title": "Discriminative learning of sum-product networks", "author": ["R. Gens", "P. Domingos"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Gens and Domingos.,? \\Q2012\\E", "shortCiteRegEx": "Gens and Domingos.", "year": 2012}, {"title": "Learning the structure of sum-product networks", "author": ["R. Gens", "P. Domingos"], "venue": "In Proceedings of The 30th International Conference on Machine Learning,", "citeRegEx": "Gens and Domingos.,? \\Q2013\\E", "shortCiteRegEx": "Gens and Domingos.", "year": 2013}, {"title": "Theory of the backpropagation neural network", "author": ["R. Hecht-Nielsen"], "venue": "Neural Networks,", "citeRegEx": "Hecht.Nielsen,? \\Q1988\\E", "shortCiteRegEx": "Hecht.Nielsen", "year": 1988}, {"title": "Online algorithms for sum-product networks with continuous variables", "author": ["P. Jaini", "A. Rashwan", "H. Zhao", "Y. Liu", "E. Banijamali", "Z. Chen", "P. Poupart"], "venue": "In Proceedings of the Eighth International Conference on Probabilistic Graphical Models,", "citeRegEx": "Jaini et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Jaini et al\\.", "year": 2016}, {"title": "Exponentiated gradient versus gradient descent for linear predictors", "author": ["J. Kivinen", "M.K. Warmuth"], "venue": "Information and Computation,", "citeRegEx": "Kivinen and Warmuth.,? \\Q1997\\E", "shortCiteRegEx": "Kivinen and Warmuth.", "year": 1997}, {"title": "Stochastic expectation propagation", "author": ["Y. Li", "J.M. Hern\u00e1ndez-Lobato", "R.E. Turner"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Li et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Li et al\\.", "year": 2015}, {"title": "A differential semantics for jointree algorithms", "author": ["J.D. Park", "A. Darwiche"], "venue": "Artificial Intelligence,", "citeRegEx": "Park and Darwiche.,? \\Q2004\\E", "shortCiteRegEx": "Park and Darwiche.", "year": 2004}, {"title": "Foundations of Sum-Product Networks for Probabilistic Modeling", "author": ["R. Peharz"], "venue": "PhD thesis, Graz University of Technology,", "citeRegEx": "Peharz.,? \\Q2015\\E", "shortCiteRegEx": "Peharz.", "year": 2015}, {"title": "On theoretical properties of sum-product networks", "author": ["R. Peharz", "S. Tschiatschek", "F. Pernkopf", "P. Domingos"], "venue": "In AISTATS,", "citeRegEx": "Peharz et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Peharz et al\\.", "year": 2015}, {"title": "On the latent variable interpretation in sumproduct networks", "author": ["R. Peharz", "R. Gens", "F. Pernkopf", "P. Domingos"], "venue": "arXiv preprint arXiv:1601.06180,", "citeRegEx": "Peharz et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Peharz et al\\.", "year": 2016}, {"title": "Sum-product networks: A new deep architecture", "author": ["H. Poon", "P. Domingos"], "venue": "In Proc. 12th Conf. on Uncertainty in Artificial Intelligence,", "citeRegEx": "Poon and Domingos.,? \\Q2011\\E", "shortCiteRegEx": "Poon and Domingos.", "year": 2011}, {"title": "Merging strategies for sum-product networks: From trees to graphs", "author": ["T. Rahman", "V. Gogate"], "venue": "In Proceedings of the Thirty-Second Conference on Uncertainty in Artificial Intelligence,", "citeRegEx": "Rahman and Gogate.,? \\Q2016\\E", "shortCiteRegEx": "Rahman and Gogate.", "year": 2016}, {"title": "Online and distributed bayesian moment matching for parameter learning in sum-product networks", "author": ["A. Rashwan", "H. Zhao", "P. Poupart"], "venue": "In Proceedings of the 19th International Conference on Artificial Intelligence and Statistics,", "citeRegEx": "Rashwan et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Rashwan et al\\.", "year": 2016}, {"title": "Learning sum-product networks with direct and indirect variable interactions", "author": ["A. Rooshenas", "D. Lowd"], "venue": "In ICML,", "citeRegEx": "Rooshenas and Lowd.,? \\Q2014\\E", "shortCiteRegEx": "Rooshenas and Lowd.", "year": 2014}, {"title": "Non-linear filtering by approximation of the a posteriori density", "author": ["H.W. Sorenson", "A.R. Stubberud"], "venue": "International Journal of Control,", "citeRegEx": "Sorenson and Stubberud.,? \\Q1968\\E", "shortCiteRegEx": "Sorenson and Stubberud.", "year": 1968}, {"title": "Simplifying, regularizing and strengthening sum-product network structure learning", "author": ["A. Vergari", "N. Di Mauro", "F. Esposito"], "venue": "In Joint European Conference on Machine Learning and Knowledge Discovery in Databases,", "citeRegEx": "Vergari et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Vergari et al\\.", "year": 2015}, {"title": "On the Relationship between Sum-Product Networks and Bayesian Networks", "author": ["H. Zhao", "M. Melibari", "P. Poupart"], "venue": "In ICML,", "citeRegEx": "Zhao et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Zhao et al\\.", "year": 2015}, {"title": "Collapsed variational inference for sum-product networks", "author": ["H. Zhao", "T. Adel", "G. Gordon", "B. Amos"], "venue": "EFFICIENT COMPUTATION OF MOMENTS IN SUM-PRODUCT", "citeRegEx": "Zhao et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Zhao et al\\.", "year": 2016}], "referenceMentions": [{"referenceID": 15, "context": "Introduction Sum-Product Networks (SPNs) have recently attracted some interest because of their flexibility in modeling complex distributions as well as the tractability of performing exact marginal inference (Poon and Domingos, 2011; Gens and Domingos, 2012, 2013; Peharz et al., 2015; Zhao et al., 2015, 2016a,b; Peharz et al., 2016).", "startOffset": 209, "endOffset": 335}, {"referenceID": 13, "context": "Introduction Sum-Product Networks (SPNs) have recently attracted some interest because of their flexibility in modeling complex distributions as well as the tractability of performing exact marginal inference (Poon and Domingos, 2011; Gens and Domingos, 2012, 2013; Peharz et al., 2015; Zhao et al., 2015, 2016a,b; Peharz et al., 2016).", "startOffset": 209, "endOffset": 335}, {"referenceID": 14, "context": "Introduction Sum-Product Networks (SPNs) have recently attracted some interest because of their flexibility in modeling complex distributions as well as the tractability of performing exact marginal inference (Poon and Domingos, 2011; Gens and Domingos, 2012, 2013; Peharz et al., 2015; Zhao et al., 2015, 2016a,b; Peharz et al., 2016).", "startOffset": 209, "endOffset": 335}, {"referenceID": 3, "context": "It has been shown that discrete SPNs are equivalent to arithmetic circuits (ACs) (Darwiche, 2003; Park and Darwiche, 2004) in the sense that one can transform each SPN into an equivalent AC and vice versa in linear time and space with respect to the network size (Rooshenas and Lowd, 2014).", "startOffset": 81, "endOffset": 122}, {"referenceID": 11, "context": "It has been shown that discrete SPNs are equivalent to arithmetic circuits (ACs) (Darwiche, 2003; Park and Darwiche, 2004) in the sense that one can transform each SPN into an equivalent AC and vice versa in linear time and space with respect to the network size (Rooshenas and Lowd, 2014).", "startOffset": 81, "endOffset": 122}, {"referenceID": 18, "context": "It has been shown that discrete SPNs are equivalent to arithmetic circuits (ACs) (Darwiche, 2003; Park and Darwiche, 2004) in the sense that one can transform each SPN into an equivalent AC and vice versa in linear time and space with respect to the network size (Rooshenas and Lowd, 2014).", "startOffset": 263, "endOffset": 289}, {"referenceID": 1, "context": "SPNs are also closely connected to probabilistic graphical models: by interpreting each sum node in the network as a hidden variable and each product node as a rule encoding context-specific conditional independence (Boutilier et al., 1996), every SPN can be equivalently converted into a Bayesian network where compact data structures are used to represent the local probability distributions (Zhao et al.", "startOffset": 216, "endOffset": 240}, {"referenceID": 21, "context": ", 1996), every SPN can be equivalently converted into a Bayesian network where compact data structures are used to represent the local probability distributions (Zhao et al., 2015).", "startOffset": 161, "endOffset": 180}, {"referenceID": 19, "context": "At a high level BMM can be understood as an instance of the general assumed density filtering framework (Sorenson and Stubberud, 1968) where the algorithm finds an approximate posterior distribution within a tractable family of distributions by the method of moments.", "startOffset": 104, "endOffset": 134}, {"referenceID": 17, "context": "An essential sub-routine of the above two algorithms (Rashwan et al., 2016; Jaini et al., 2016) is to efficiently compute the exact first and second order moments of the one-step update posterior distribution (cf.", "startOffset": 53, "endOffset": 95}, {"referenceID": 8, "context": "An essential sub-routine of the above two algorithms (Rashwan et al., 2016; Jaini et al., 2016) is to efficiently compute the exact first and second order moments of the one-step update posterior distribution (cf.", "startOffset": 53, "endOffset": 95}, {"referenceID": 3, "context": "The key technique in the design of our algorithm dates back to the differential approach (Darwiche, 2003) used for exact inference in Bayesian networks.", "startOffset": 89, "endOffset": 105}, {"referenceID": 15, "context": "Recently Rashwan et al. (2016) proposed an online Bayesian Moment Matching (BMM) algorithm to learn the probability distribution of the model parameters of SPNs based on the method of moments.", "startOffset": 9, "endOffset": 31}, {"referenceID": 7, "context": "Later Jaini et al. (2016) extended this algorithm to the continuous case where the leaf nodes in the network are assumed to be Gaussian distributions.", "startOffset": 6, "endOffset": 26}, {"referenceID": 7, "context": "Later Jaini et al. (2016) extended this algorithm to the continuous case where the leaf nodes in the network are assumed to be Gaussian distributions. Empirically they show that BMM is superior to online extensions of projected gradient descent and exponentiated gradient. At a high level BMM can be understood as an instance of the general assumed density filtering framework (Sorenson and Stubberud, 1968) where the algorithm finds an approximate posterior distribution within a tractable family of distributions by the method of moments. Specifically, BMM for SPNs works by matching the first and second order moments of the approximate tractable posterior distribution to the exact but intractable posterior. An essential sub-routine of the above two algorithms (Rashwan et al., 2016; Jaini et al., 2016) is to efficiently compute the exact first and second order moments of the one-step update posterior distribution (cf. 3.2). Rashwan et al. (2016) designed a recursive algorithm to achieve this goal in linear time when the underlying network structure is a tree, and this algorithm is also used by Jaini et al.", "startOffset": 6, "endOffset": 955}, {"referenceID": 7, "context": "Later Jaini et al. (2016) extended this algorithm to the continuous case where the leaf nodes in the network are assumed to be Gaussian distributions. Empirically they show that BMM is superior to online extensions of projected gradient descent and exponentiated gradient. At a high level BMM can be understood as an instance of the general assumed density filtering framework (Sorenson and Stubberud, 1968) where the algorithm finds an approximate posterior distribution within a tractable family of distributions by the method of moments. Specifically, BMM for SPNs works by matching the first and second order moments of the approximate tractable posterior distribution to the exact but intractable posterior. An essential sub-routine of the above two algorithms (Rashwan et al., 2016; Jaini et al., 2016) is to efficiently compute the exact first and second order moments of the one-step update posterior distribution (cf. 3.2). Rashwan et al. (2016) designed a recursive algorithm to achieve this goal in linear time when the underlying network structure is a tree, and this algorithm is also used by Jaini et al. (2016) in the continuous case.", "startOffset": 6, "endOffset": 1126}, {"referenceID": 19, "context": "With the linear time sub-routine for computing moments, we are able to design an assumed density filter (Sorenson and Stubberud, 1968) (ADF) to learn the parameters of SPNs in an online fashion.", "startOffset": 104, "endOffset": 134}, {"referenceID": 21, "context": ", Xn} can be converted into a bipartite BN withO(n|S|) size (Zhao et al., 2015).", "startOffset": 60, "endOffset": 79}, {"referenceID": 4, "context": "The second perspective is to view an SPN S as a mixture model with exponentially many mixture components (Dennis and Ventura, 2015; Zhao et al., 2016b).", "startOffset": 105, "endOffset": 151}, {"referenceID": 3, "context": "(Darwiche, 2003) Let B be a Bayesian network representing probability distribution Pr(X) and having polynomial g.", "startOffset": 0, "endOffset": 16}, {"referenceID": 3, "context": "This novel idea is due to Darwiche (2003). We also refer readers to Darwiche (2003) for a more complete and detailed discussion of this approach.", "startOffset": 26, "endOffset": 42}, {"referenceID": 3, "context": "This novel idea is due to Darwiche (2003). We also refer readers to Darwiche (2003) for a more complete and detailed discussion of this approach.", "startOffset": 26, "endOffset": 84}, {"referenceID": 3, "context": "Formally, the circuit complexity (Darwiche, 2003) of a BN B is the size of the smallest arithmetic circuit that computes the network polynomial of B.", "startOffset": 33, "endOffset": 49}, {"referenceID": 19, "context": "Assumed density filtering (Sorenson and Stubberud, 1968) is such a framework: the algorithm chooses an approximate distribution from a tractable family of distributions after observing each instance.", "startOffset": 26, "endOffset": 56}, {"referenceID": 17, "context": "A linear time algorithm that uses dynamic programming to compute these moments has been designed by Rashwan et al. (2016) when the underlying structure of S is a tree.", "startOffset": 100, "endOffset": 122}, {"referenceID": 21, "context": "The key update in each iteration of CCCP (Zhao et al. (2016b) Eq.", "startOffset": 42, "endOffset": 62}, {"referenceID": 19, "context": "1 as a sub-routine to develop a new Bayesian online learning algorithm for SPNs based on assumed density filtering (Sorenson and Stubberud, 1968).", "startOffset": 115, "endOffset": 145}, {"referenceID": 2, "context": "This principle of finding an approximate distribution is also known as reverse information projection in the literature of information theory (Csisz\u00e1r and Matus, 2003).", "startOffset": 142, "endOffset": 167}, {"referenceID": 10, "context": "As a note, we can also extend the above ADF algorithm to the batch setting via a recently proposed technique known as stochastic expectation propagation (Li et al., 2015).", "startOffset": 153, "endOffset": 170}, {"referenceID": 2, "context": "This principle of finding an approximate distribution is also known as reverse information projection in the literature of information theory (Csisz\u00e1r and Matus, 2003).2 By utilizing our efficient linear time algorithm for exact moment computation, we propose a Bayesian online learning algorithm for SPNs based on the above moment matching principle, called ADF. The pseudocode is shown in Alg. 2. Details on how to solve the moment matching equation (Eq. 15) are presented in appendix. As a note, we can also extend the above ADF algorithm to the batch setting via a recently proposed technique known as stochastic expectation propagation (Li et al., 2015). 2. As a comparison, information projection corresponds to minimizing KL(q || p(w | x)) within the same family of distributions q \u2208 P . Finding an approximate distribution for SPNs based on information projection has recently been studied by Zhao et al. (2016a), and an algorithm called CVB-SPN is proposed therein.", "startOffset": 143, "endOffset": 921}, {"referenceID": 6, "context": "1 Experimental Setting We conduct experiments on 20 real-world data sets that have been used as benchmarks to evaluate the effectiveness of learning algorithms for SPNs (Gens and Domingos, 2013; Rooshenas and Lowd, 2014; Zhao et al., 2016a; Adel et al., 2015; Vergari et al., 2015).", "startOffset": 169, "endOffset": 281}, {"referenceID": 18, "context": "1 Experimental Setting We conduct experiments on 20 real-world data sets that have been used as benchmarks to evaluate the effectiveness of learning algorithms for SPNs (Gens and Domingos, 2013; Rooshenas and Lowd, 2014; Zhao et al., 2016a; Adel et al., 2015; Vergari et al., 2015).", "startOffset": 169, "endOffset": 281}, {"referenceID": 0, "context": "1 Experimental Setting We conduct experiments on 20 real-world data sets that have been used as benchmarks to evaluate the effectiveness of learning algorithms for SPNs (Gens and Domingos, 2013; Rooshenas and Lowd, 2014; Zhao et al., 2016a; Adel et al., 2015; Vergari et al., 2015).", "startOffset": 169, "endOffset": 281}, {"referenceID": 20, "context": "1 Experimental Setting We conduct experiments on 20 real-world data sets that have been used as benchmarks to evaluate the effectiveness of learning algorithms for SPNs (Gens and Domingos, 2013; Rooshenas and Lowd, 2014; Zhao et al., 2016a; Adel et al., 2015; Vergari et al., 2015).", "startOffset": 169, "endOffset": 281}, {"referenceID": 0, "context": "Since LearnSPN, along with other structure learning algorithms for SPNs (Adel et al., 2015; Rooshenas and Lowd, 2014), will only return tree structured networks, we post-process the constructed networks by merging leaf nodes with same distributions, leading to general networks with DAG structures.", "startOffset": 72, "endOffset": 117}, {"referenceID": 18, "context": "Since LearnSPN, along with other structure learning algorithms for SPNs (Adel et al., 2015; Rooshenas and Lowd, 2014), will only return tree structured networks, we post-process the constructed networks by merging leaf nodes with same distributions, leading to general networks with DAG structures.", "startOffset": 72, "endOffset": 117}, {"referenceID": 16, "context": "Note that more complicated and compression-efficient post-processing strategy exists for SPNs (Rahman and Gogate, 2016).", "startOffset": 94, "endOffset": 119}, {"referenceID": 15, "context": "On the other hand, we would also like to have a fair and extensive empirical comparison among all the online parameter learning algorithms for SPNs proposed to date, including projected gradient descent (PGD) (Poon and Domingos, 2011; Gens and Domingos, 2012), exponentiated gradient (EG) (Kivinen and Warmuth, 1997), sequential monomial approximation (SMA) (Zhao et al.", "startOffset": 209, "endOffset": 259}, {"referenceID": 5, "context": "On the other hand, we would also like to have a fair and extensive empirical comparison among all the online parameter learning algorithms for SPNs proposed to date, including projected gradient descent (PGD) (Poon and Domingos, 2011; Gens and Domingos, 2012), exponentiated gradient (EG) (Kivinen and Warmuth, 1997), sequential monomial approximation (SMA) (Zhao et al.", "startOffset": 209, "endOffset": 259}, {"referenceID": 9, "context": "On the other hand, we would also like to have a fair and extensive empirical comparison among all the online parameter learning algorithms for SPNs proposed to date, including projected gradient descent (PGD) (Poon and Domingos, 2011; Gens and Domingos, 2012), exponentiated gradient (EG) (Kivinen and Warmuth, 1997), sequential monomial approximation (SMA) (Zhao et al.", "startOffset": 289, "endOffset": 316}, {"referenceID": 12, "context": ", 2016b), concave-convex procedure/expectation maximization (CCCP) (Peharz, 2015; Zhao et al., 2016b), collapsed variational Bayes (CVB) (Zhao et al.", "startOffset": 67, "endOffset": 101}, {"referenceID": 17, "context": ", 2016a), online Bayesian moment matching (BMM) (Rashwan et al., 2016) and assumed density filtering (ADF).", "startOffset": 48, "endOffset": 70}, {"referenceID": 0, "context": ", 2016a; Adel et al., 2015; Vergari et al., 2015). Statistics about the 20 data sets and their corresponding SPN models are shown in Table 1. All the random variables in these 20 data sets are binary. We use LearnSPN as the structure learning algorithm to build structures for the 20 data sets. Since LearnSPN, along with other structure learning algorithms for SPNs (Adel et al., 2015; Rooshenas and Lowd, 2014), will only return tree structured networks, we post-process the constructed networks by merging leaf nodes with same distributions, leading to general networks with DAG structures. Note that more complicated and compression-efficient post-processing strategy exists for SPNs (Rahman and Gogate, 2016). We emphasize how we are not interested in obtaining the most compact SPN representations. Instead, we would like to investigate how the proposed linear time moment computation algorithm scales on general SPNs with DAG structures, compared with the existing quadratic moment computation algorithm. As the last step, we equivalently transform the network structures by removing consecutive sum nodes or product nodes, using the same technique introduced by Vergari et al. (2015). This step helps to reduce the size of the networks while keeping the distributions unchanged.", "startOffset": 9, "endOffset": 1192}, {"referenceID": 17, "context": "As a result, the moment computation algorithm introduced in Rashwan et al. (2016) for trees cannot be applied here.", "startOffset": 60, "endOffset": 82}], "year": 2017, "abstractText": "Bayesian online learning algorithms for Sum-Product Networks (SPNs) need to compute moments of model parameters under the one-step update posterior distribution. The best existing method for computing such moments scales quadratically in the size of the SPN, although it scales linearly for trees. We propose a linear-time algorithm that works even when the SPN is a directed acyclic graph (DAG). We achieve this goal by reducing the moment computation problem into a joint inference problem in SPNs and by taking advantage of a special structure of the one-step update posterior distribution: it is a multilinear polynomial with exponentially many monomials, and we can evaluate moments by differentiating. The latter is known as the differential trick. We apply the proposed algorithm to develop a linear time assumed density filter (ADF) for SPN parameter learning. As an additional contribution, we conduct extensive experiments comparing seven different online learning algorithms for SPNs on 20 benchmark datasets. The new linear-time ADF method consistently achieves low runtime due to the efficient linear-time algorithm for moment computation; however, we discover that two other methods (CCCP and SMA) typically perform better statistically, while a third (BMM) is comparable to ADF. Interestingly, CCCP can be viewed as implicitly using the same differentiation trick that we make explicit here. The fact that two of the top four fastest methods use this trick suggests that the same trick might find other uses for SPN learning in the future.", "creator": "LaTeX with hyperref package"}}}