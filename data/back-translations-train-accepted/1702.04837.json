{"id": "1702.04837", "review": {"conference": "ICML", "VERSION": "v1", "DATE_OF_SUBMISSION": "16-Feb-2017", "title": "Sketched Ridge Regression: Optimization Perspective, Statistical Perspective, and Model Averaging", "abstract": "We address the statistical and optimization impacts of using classical or Hessian sketch to approximately solve the Matrix Ridge Regression (MRR) problem. Prior research has considered the effects of classical sketch on least squares regression (LSR), a strictly simpler problem. We establish that classical sketch has a similar effect upon the optimization properties of MRR as it does on those of LSR -- namely, it recovers nearly optimal solutions. In contrast, Hessian sketch does not have this guarantee; instead, the approximation error is governed by a subtle interplay between the \"mass\" in the responses and the optimal objective value.", "histories": [["v1", "Thu, 16 Feb 2017 02:01:26 GMT  (997kb,D)", "https://arxiv.org/abs/1702.04837v1", null], ["v2", "Sat, 25 Feb 2017 02:59:41 GMT  (1007kb,D)", "http://arxiv.org/abs/1702.04837v2", null], ["v3", "Sat, 10 Jun 2017 17:52:18 GMT  (909kb,D)", "http://arxiv.org/abs/1702.04837v3", "A short version will appear in the thirty-fourth International Conference on Machine Learning (ICML 2017)"]], "reviews": [], "SUBJECTS": "stat.ML cs.LG cs.NA", "authors": ["shusen wang", "alex gittens", "michael w mahoney"], "accepted": true, "id": "1702.04837"}, "pdf": {"name": "1702.04837.pdf", "metadata": {"source": "CRF", "title": "Sketched Ridge Regression: Optimization Perspective, Statistical Perspective, and Model Averaging", "authors": ["Shusen Wang", "Alex Gittens", "Michael W. Mahoney"], "emails": ["shusen@berkeley.edu", "gittea@rpi.edu", "mmahoney@stat.berkeley.edu"], "sections": [{"heading": null, "text": "For both types of approximations, the regulation of the outlined MRR problem gives the outlined LSR problem significantly different statistical properties from the outlined LSR problem. In particular, the outlined MRR has a bias variance trade-off that does not exist in outlined LSR sketches. We offer upper and lower limits for the distortions and deviations of the outlined MRR problem; these determine that the variance is significantly greater when using classic sketches, while the bias is significantly increased when using Hessian sketches. Empirically, outlined MRR solutions can have risks that are orders of magnitude higher than those of the optimal MRR solutions. Theoretically and empirically, we find that the averages of this model significantly reduce this gap."}, {"heading": "1. Introduction", "text": "In fact, it is the case that most of us are in a position to go to another world, in which they go to another world, in which they find their way in another world, in which they live in another world, in which they live in another world, in which they live in another world, in which they live in another world, in which they live in another world, in which they live in another world, in which they live in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they do, in which they do, in which they do, in which they do, in which they do, in which they do, in which they do, in which they do, in which they do, in which they do, in which they do, in which they live, in which they do, in which they do, in which they do, in which they do, in which they do, in which they live, in which they do, in which they do, in which they do, in which they do, in which they live, in which they do, in which they do, in which they do, in which they do, in which they do, in which they live, in which they do, in which they do, in which they do, in which they live, in which they do, in which they do, in which they do, in which they live, in which they do, in which they live, in which they do, in which they do, in which they do, in which they do, in which they do, in which they do, in which they do, in which they do, in which they do, in which they do, in which they do, in which they do, in which they do, in which they do, in which they do, in which they do, in which they do, in which they do, in which they do, in which they do, in which they do"}, {"heading": "1.1 Main Results and Contributions", "text": "We grasp all our risks R (W) = E (W).W (W).W (W).W (W).W (W).W (W).W (W).W (S).S (W).S (W).S (W).S (S).S (S).S (S).S (S).S (S).S (S).S (S).S (W).S (W).S (W).S (W).S (S).S (S).S (S).S (S).S (S).S (S).S (S).S (S).S (S).S (W).S (W).S (W).S (W).S (W).S (W).S (W).S (S).S (S).S (S).S (S).S (S).S (S).S (S).S (S).S (S).S (S).S (S).S (S).S (S (S).S (S).S (S).S (S (S).S (S).S (S).S (S (S).S (W).S (W (W).S (W).S (W (W).S (W).S (W).S (W (W).S (W).S (W (W).S (W (W).S (W).S (W).S (W (W).S (W (W).S (W (W).S (W (W).S (W (W).S (W (W (W (W).S (W (W).S (W).S (W).S (W (W).S (W (W).S (W).S (W (W (W).S (W (W).S (.S (.S (W).S (W).S (.S (.S (W).S (.S (W).S (.S ("}, {"heading": "1.2 Prior Work", "text": "The body of work on the outlined LSR in this paper is similar to the spirit of classification. (Thanw.b, 2006b, 2011, Clarkson and Woodruff, 2013, Meng and Mahoney, 2013, Nelson and Nguye, 2013) shares many similarities with our results. But the theories of the outlined LSR do not obviously evolve from the optimization perspective on MRR, and the statistical analysis of LSR and MRR differs: among other differences, LSR is unbiased, while MRR has a non-trivial bias and therefore has a bias variance that needs to be taken into account. (2013) has considered another application of sketching to crest regression: they assume that the number of features in X is reduced with sketches, and perform statistical analyses."}, {"heading": "1.3 Paper Organization", "text": "Section 2 defines our notation and presents the sketch schemes we are considering. Section 3 presents our theoretical results. Section 4 and 5 conduct experiments to test our theories and demonstrate the usefulness of the model averaging. Section 6 shows the sketch of the proof. The proofs of the theorems are in the appendix."}, {"heading": "2. Preliminaries", "text": "We're not able to determine the number of points, and we're not able to define the number of points, and we're not able to define the number of points, and we're not able to define the number of points. We're not able to capture the number of points. We're not able to capture the number of points. We're not able to capture the number of points. We're not able to capture the number of points. We're not able to capture the number of points. We're not able to capture the number of points. We're not able to capture the number of points. We're not able to capture the number of points. We're not able to capture the number of points. We're not able to capture the numbers. We're not able to capture the numbers. We're not able to capture the numbers. We're not able to capture the numbers. We're not able to capture the numbers. We're not able to capture the numbers."}, {"heading": "3. Main Results", "text": "Sections 3.1 and 3.2 analyze outlined MRR from an optimization or statistical perspective, respectively. Sections 3.3 and 3.4 capture the effects of model averaging on the optimization or statistical properties of outlined MRR, respectively. In Section 2, we described six sketching methods. For convenience, in this section, we refer to leverage score sampling, shorted leverage score sampling, Gaussian projection, and SRHT as the four sketching methods; and we will explicitly mention uniform sampling and CountSketch."}, {"heading": "3.1 Sketched MRR: Optimization Perspective", "text": "Theorem 1 shows that f (Wc), the objective value of the classic sketch, is very close to the optimal objective value f (W?) definition. (W?) The approximate definition improves when it comes to a uniform sketch. (W?) Definition (W?) Definition (W?) Definition (W?) Definition (W?) Definition (W?) Definition (W?) Definition (W?) Definition (W?) Definition (W?) Definition (W?) Definition (W?) Definition (W?) Definition (W?) Definition (W?) Definition (W?) Definition (W?) Definition (W?) Definition (W?) Definition (W?) Definition (W?) Definition (W?) Definition (W?) Definition (W?) Definition (W?) Definition (W?) Definition (W?) Definition (W?) Definition (W?) Definition (W?) Definition (W?) Definition (W?) Definition (W?) Definition (W?) Definition (W?) Definition (W?) Definition (W?) Definition (W?) Definition (W?) Definition (W?) Definition (W?) Definition (W?) Definition (W?) Definition (W? (W?) Definition (W?) Definition (W? (W?) Definition (W?) Definition (W? (W?) Definition (W? (W? (W?) Definition (W?) Definition (W? (W? (W?) Definition (W? (W?) Definition (W? (W? (W?) Definition (W?) Definition (W? (W? (W?) Definition (W?) Definition (W? (W? (W?) Definition (W?) Definition (W? (W"}, {"heading": "3.2 Sketched MRR: Statistical Perspective", "text": "We consider the following model as inevitable. (W) We consider the model as inevitable. (W) We assume that it is inevitable. (W). (W). (W). (W). (W). (W). (W). (W). (W). (W). (W). (W). (W). (W). (W). (W). (W). (W). (W). (W). (W). (W). (W). (W). (W). (W). (W). (W). (W). (W). (W). (W). (W). (W). (W). (W). (W). (W). (W). (W). (W). (W). (W). (W). (W). (W). (W). (W). (W). (W). (W). (W). (W). (W). (W). (W). (W). (W). (W). (W). (W). (W). (W). (W). (W). (W). (W). (W). (W). (W). (W). (W). (W). (W). (W). (W). (W). (W). (W). (W). (W). (W). (W). (W). (W). (W). (W). (W). (W). (W). (W). (W). (W). (W). (W). (W). (W). (W). (W). (W). (W). (W. (W). (W). (W). (W). (W). (W). (W). (W). (W). (W). (W). (W). (W). (W). (W). (W). (W). (W). (W)"}, {"heading": "3.3 Model Averaging: Optimization Perspective", "text": "We consider model averaging as an approach to increasing the accuracy of the outlined MRR solutions. The method of model averaging is simple: Draw independently g outlining matrices S1, \u00b7 \u00b7 \u00b7, Sg-Rn-S, use these to form g outlined MRR solutions designated by {Wci} g i = 1 or {Whi} g i = 1, and calculate these solutions on average to obtain the final estimate. \u2212 Wc = 1g \u2211 g = 1 W c i or W h = 1g \u2211 g = 1 W h. Practical applications of model averaging are listed in Section 1.1.Theorems 8 and 9. \u2212 d Guarantees for the optimization accuracy of the use of model averaging to combine the classic / Hessian sketch solutions. We can provide these guarantees for the outlined MRR with the outlined models f-sketches 1 and 2."}, {"heading": "3.4 Model Averaging: Statistical Perspective", "text": "The first result of the study shows that the risk function in (8), those in (8), (4), (4), (4), (5), (5), (5), (5), (5), (5), (5), (5), (5), (5), (5), (5), (5), (5), (5), (5), (5), (5), (5), (5), (5), (5), (5), (5), (5), (5), (5), (5), (5), (5), (5), (5), (5), (5), (5), (5), (5), (5), (5), (5), (5), (5), (5), (5), (5, (5), (5), (5, (5), (5), (5), (5), (5), (5), (5), (5), (5), (5), (5), (5), (5), (5, (5), (5), (5), (5), (5, (5), (5), (5), (5, (5), (5), (5), (5, (5), (5), (5, (5), (5), (5, (5), (5), (5, (5), (5), (5), (5, (5), (5, (5), (5, (5), (5, (5), (5), (5, (5), (5), (5), (5, (5, (5), (5), (5, (5, (5), (5), (5, (5), (5), (5, (5), (5, (5), (5), (5, (5), (5, (5), (5), (5, (5), (5), (5, ("}, {"heading": "4. Experiments on Synthetic Data", "text": "We conduct experiments with synthetic data to verify our main theories. Section 4.1 describes the data model and experimental settings. Section 4.2 and 4.3 examine classical and Hessian sketches from the optimization and statistical perspectives, respectively, to verify theorems 1, 2, 5, 7. Section 4.4 and 4.5 examine the model on average from the optimization and statistical perspectives, respectively, to confirm theorems 8, 9, 11, 13."}, {"heading": "4.1 Settings", "text": "According to (Ma et al., 2015, Yang et al., 2016) we construct X = Udiag (VT) VT-D and y = Xw0 + \u03b5-Rn in the following way. \u2022 Let's leave the lines of A-Rn-D i.i.d sE's has a high line t distribution with covariance matrix C and v = 2 degrees of freedom where the (i, j) -th input of C-Rm \u00d7 n 2 \u00d7 0.5 | i-J. Let's leave the orthonormal bases of A. \u2022 Let's leave the lines of b-Rp-10-10 etRp-10 etSp-10 etSp-10 etSp-10 etSp-10 etSp-10 10. \u2022 Let's leave V-0FT-D the orthonorthonormal bases of A. \u2022 Let's leave the lines of b-Rp-10-10 etSp-10 etp between 0 and \u2212 6."}, {"heading": "4.2 Sketched MRR: Optimization Perspective", "text": "We try to verify theorems 1 and 2, which look at classical and Hessian sketches from the optimization perspective. In Figure 2, we plot the objective function value f (w) = 1 n-Xw \u2212 y-y-y-2 + \u03b3-w-22 under different settings of noise intensity. The black curves correspond to the optimal solution w?; the color curves are classical or Hessian sketches with different sketching methods. The results confirm our theory: The classical sketch wc is always close to the optimum; the Hessian sketch wh is much worse than the optimal one if \u03b3 is small and y is mostly in the slit space of X."}, {"heading": "4.3 Sketched MRR: Statistical Perspective", "text": "Figure 3 shows the analytical expressions for the squared biases, variance and risk that occur in theorem 4 against the regularization parameter \u03b3. Since the analytical expressions contain the random sketch of the matrix S, we randomly generate S, repeat this procedure 10 times and report on the average of calculated risks, risks and risks. We fix the risk that the risks of the classic 10-10-10-10-10-10-10-10-10-10-10-10-10-10-10-10-10-10-10-10-10-10-10-10-10-10-10-10-10-10-10-10-10-10-10-10-10-10-10-10-10-10-10-10-10-10-10-10-10-10-10-10-10-10-10-10-10-10-10-10-10-10-10-10-10-10-10-10-10-10-10-10-10-10-10-10-10-10-10-10-10-10-10-10-10-10-10-10-10-10-10-10-10-10-10-10-10-10-10-10-10-10-10-10-10-10-10-10-10-10-10-10-10-10-10-10-10-10-10-10-10-10-10-10-10-10-10-10-10-10-10-10-10-10-10-10-10-10-10-10-10-10-10-10-10-10-10-10-10-10-10-10-10-10-10-10-10-10-10-10-10-10-10-10-10-10-10-10-10-10-10-10-10-10-10-10-10-10-10-10-10-10-10-10-10-10-10-10-10-10-10-10-10-10-10-10-10-10-10-10-10-10-10-10-10-10-10-10-10-10"}, {"heading": "4.4 Model Averaging: Optimization Objective", "text": "We use different intensities of rushing - we use a 10-10-1-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-10-3-3-3-3-3-3-3-3-3-3-3-3-3-3-3-3-3-3-3-3-3-3-3-3-3-3-3-3-3-3-3-3-3-3-3-3-3-3-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-3-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-players."}, {"heading": "4.5 Model Averaging: Statistical Perspective", "text": "We examine the model from a statistical perspective. We calculate bias (w?), var (w?) (optimal solution) according to Theorem 4 and bias (wc [g]), var (w c [g]) (classical sketch) and bias (wh [g]), var (w h [g]) (Hessian sketch) according to Theorem 10."}, {"heading": "4.5.1 Classical Sketch", "text": "Se \"n\" S \"S\" S \"S\" S \"S\" S \"S\" S \"S\" S. \"S.\" S \"S.\" S. \"S.\" S. \"S.\" S. \"S.\" S. \"S.\" S. \"S.\" S. \"S.\" S. \"S.\" S. \"S.\" S. \"S.\" S. \"S.\" S. \"S.\" S. \"S.\" S \"S\" S \"S\" S. \"S\" S \"S\" S \"S.\" S. \"S.\" S. \"S.\" S \"S.\" S. \"S\" S. \"S.\" S \"S.\" S. \"S\" S. \"S.\" S. \"S\" S. \"S.\" S \"S.\" S. \"S.\" S \"S\" S \"S\" S \"S\" S \"S\" S \"S\" S \"S\" S \"S\" S \"S\" S \"S.\" S. \"S\" S. \"S\" S \"S.\" S \"S\" S \"S\" S. \"S\" S \"S\" S \"S.\" S \"S\" S \"S\" S \"S\" S. \"S\" S \"S\" S \"S\" S \"S\" S. \"S\" S \"S\" S \"S\" S \"S\" S \"S.\" S \"S\" S \"S\" S \"S\" S \"S\" S \"S\" S. \"S\" S \"S\" S \"S\" S \"S\" S \"S\" S \"S\" S \"S\" S \"S\" S \"S.\" S \"S\" S \"S\" S \"S\" S \"S\" S \"S\" S \"S\" S \"S\" S \"S\" S. \"S\" S \"S\" S \"S\" S \"S\" S \"S\" S \"S\" S \"S\" S \"S\" S \"S\" S \"S\" S \"S\" S \"S\" S \"S\" S \"S.\" S \"S\" S \"S\" S \"S\" S \"S\" S \"S\" S \"S\" S \"S\" S \"S\" S \"S\" S \"S\" S"}, {"heading": "4.5.2 Hessian Sketch", "text": "Theorem 13 shows that sufficiently large s, e.g. the Gaussian projection with s = O inclination (d 2), the distortion of inequality (wh [g]), the distortion of inequality (w?) \u2264 1 + (g + 2) and the distortion of inequality (wh [g]) \u2264 1 + are highly likely to apply. Theoretically, the model averaging improves the distortion without exacerbating the distortion. Limit distortion (wh [g]) \u2212 distortion (w?) \u2212 distortion (w?) \u2264 + (g + 2) \u0445 X-22 ng indicates that if (1) ng is much smaller than Figure X-22 and (2) \u2264 1g, equivalent s is at least O-distortion (dg2), then the ratio is proportional to g. To verify Theorem 13, we set the very small - g = 10 \u2212 12 - and variable s and g. In Figure 6, we draw the distortion w (dg2) not equal to large (dgw =)."}, {"heading": "5. Model Averaging Experiments on Real-World Data", "text": "In Section 1, we mentioned that in the distributed environment where the feature response pairs (x1, y1), \u00b7 \u00b7 \u00b7, (xn, yn) and Rd \u00b7 m are stored across machines, the classic model mean sketch requires only one round of communication and is therefore a communication-efficient algorithm that can be used to: (1) obtain an approximate solution to the MRR problem with a risk comparable to a batch solution, and (2) obtain a low solution to the MRR optimization problem that can be used as an initiator of more communication-intensive optimization algorithms. In this section, we demonstrate both areas of application. We use the Million Song Year Prediction dataset, which includes 463, 715 training samples and 51, 630 test samples with 90 features and one response. We normalize the data by moving the responses to zero mean and scaling the range of each feature to [1] \u2212 1."}, {"heading": "5.1 Prediction Error", "text": "We tested the predictive performance of the ridge regression outlined by implementing the classic sketch averaging the model in PySpark (Zaharia et al., 2010).5 We conducted our experiments with PySpark in local mode; the experiments consisted of three steps: (1) using a quintuple cross-validation to determine the regularization parameter \u03b3; (2) learning the model etc. 5. The code is available at https: / / github.com / wangshusen / SketchedRidgeRegression.git-w-2. of the selected \u03b3; and (3) using w to predict and record the average square errors (MSEs).These steps correspond well to the programming model Map-Reduce used by PySpark. We draw g = ns against the MSE in Figure 7. As g grows, the sketch size s = n decreases, so the performance of the classic sketch deteriorates."}, {"heading": "5.2 Optimization Error", "text": "If w is initialized with zero-mean random variables or deterministically with zeros, then optimization algorithms can be initialized to solve the MRR. Any w with the above ratio substantially smaller than 1 provides a better initialization. We implemented the classic sketch with and without model averaging in Python and calculated the above ratio on the training set of the yearly prediction dataset; to estimate the expectation, we repeated the procedure 100 times and reported the average of the ratio. In Figure 8, we plot g against the average of the ratio, with different settings of the regulation parameter. In contrast, the classic sketch does not yield a very good initialization unless g is small (equivalent, the sketch size s = ng is large)."}, {"heading": "6. Sketch of Proof", "text": "In this section we give an overview of the evidence of our main results. Detailed evidence can be found in the appendix. Section 6.1 shows some key properties of the matrix sketch. Section 6.2 considers the consideration of the average of the outlined matrices; the results are applied to analyze outlined MRR with model averaging. Section 6.3 to 6.6 specifies the structural results of the outlined MRR with or without model mean. Our main results in Section 3 (theorems 1, 2, 5, 7, 8, 9, 11, 13) are directly derived from the key properties of the matrix sketch and the structural results of the outlined MRR. Table 4 summarizes the dependence relationship between the theorems. For example, Theorem 1, which examines classical sketches from an optimization perspective, is one of our main theorems and can be proven by theorems 16 and 21."}, {"heading": "6.1 Properties of Matrix Sketching", "text": "Analysis of the S & Rn spectral standard is around ns.Assumption 1 (1)."}, {"heading": "6.2 Matrix Sketching with Averaging", "text": "We have shown that sketches can be applied to approximate matrix multiplication; see assumptions 1.1 and 1.2. What happens if we independently draw g sketches, approximate the multiplications, and represent the g products on average? Intuitively, the average product should come closer to the true product. Let's formally justify the intuition. Let S1, \u00b7 \u00b7 \u00b7, Sg-Rn-s be some sketch matrices, and A and B are arbitrary fixed matrices with a reasonable size. It's not difficult to show that 1g matrices i = 1 ATSiS T i B = A TSSTB, where S = 1 matrices are g [S1, \u00b7 \u00b7 \u00b7, Sg] n matrices. Clearly S is a sketch matrix larger than any of S1, \u00b7, Sg matrix."}, {"heading": "Assume Si and S satisfy", "text": "2.1. The conjecture UTSiSTi \u2212 I\u03c1, 2 \u2264 \u03b7 for all i. [g] and the conjecture that UTSSTU \u2212 I\u03c1, 2 \u2264 \u03b7g; 2.2 (1 g, g i = 1), UTSiSTi, B \u2212 UTB, F), 2 \u2264 B, 2F and the conjecture that UTSSTB \u2212 UTB, 2F, and Gaussian, and Gaussian, matrices meet the assumptions, 2.1, 2.2, 2.3. We prove Theorem 19 in Appendix B. Theorem 19 Leave S1, \u00b7, Sg, Rn, s, the same type of random matrices that can be drawn independently. \u2212 The conjecture, SRHT, or Gaussian, is a problem of low probability."}, {"heading": "6.3 Sketched MRR: Optimization Perspective", "text": "Theorem 21 applies under the partial space embedding property and the matrix multiplication property (assumptions 1,1 and 1,2). We prove theorems 21 in Appendix C.Theorem 21 (classical sketch) Let us leave assumptions 1.1 and 1.2 for the sketching matrix S-Rn \u00b7 s. Let us apply \u03b7 and be defined in Assumption 1. Let us leave \u03b1 = 2max {, \u03b7 2} 1 \u2212 \u03b7 and\u03b2 = x-X-22, X-22 + n. Thenf (Wc) \u2212 f (W?) \u2264 f (W?).Theorem 22 applies under the partial noise embedding property (Assumption 1,1). Let us prove theorems 22 in Appendix C.Theorem 22 (Hessian sketch) Let us apply assumption 1.1 to the sketching matrix S-Rn \u00b7 s. \u2212 Let us leave it in Assumption 1 and \u03b2 = x-X-22 + n."}, {"heading": "6.4 Sketched MRR: Statistical Perspective", "text": "Theorem 23 specifies the property of sub-space under assumption 1,1, but the limit of variance is very weak. Theorem 23 we prove in Appendix D. Theorem 23 (classical sketch) Let us define ourselves under assumption 1,1. Theorem 24 specifies a lower limit for the variance of bias (W?) \u2264 1 \u2212 \u03b7. Under assumptions 1,1 and 1,3 Thatvar (Wc) var (W?) \u2264 (1 + \u03b7) 2 \u0445n s. Theorem 24 specifies a lower limit for the variance of the classical sketch. We prove Theorem 24 in Appendix D. Theorem 24 (lower Bound variance)."}, {"heading": "6.5 Model Averaging: Optimization Perspective", "text": "Theorem 26 includes the embedding of the property (assumption 2,1) and the matrix multiplication property (assumption 2,2). We prove theorems 26 in appendix E.Theorem 26 (classic sketch with model mean). Let it be admitted and defined in assumption 2. Let it be: \u03b1 = 2 [max {\u221a g, \u03b7 g} + 2\u03b2 max {\u03b7, \u03b72}] 2 and \u03b2 = 2 x 22 x 22 + n\u03b3 \u2264 1. Assuming 2,1 and 2,2 we have the (Wc) \u2212 f (W?) \u2264 \u03b1\u03b2f (W?).Theorem 27 applies under the subspace embedded in the property (assumption 2,1). We prove theorems 27 in appendix E.Theorem 27 (Hessian sketch with model mean) Let it be defined in assumption 2."}, {"heading": "6.6 Model Averaging: Statistical Perspective", "text": "Theorem 28 proves that theorem 28 in Appendix F. Theorem 28 (classical sketch with model averaging) under assumption 2.1 states that it maintains the bias (Wc) \u2264 1 \u2212 \u03b7. Under assumption 2.1 it maintains the bias (Wc) \u2264 1 \u2212 \u03b7. Under assumptions 2.1 and 2.3 it maintains the bias (Wc) var (W?) \u2264 \u03b8n s (\u221a 1 + \u03b7 / g \u221a g + dependence 1 \u2212 \u03b7) 2. Theorem 28 (classical sketch with model averaging)."}, {"heading": "Here \u03b7 and \u03b8 are defined in Assumption 2.", "text": "Theorem 29 is shown in Appendix F. Theorem 29 (Hessian sketch with model mean) under assumption 2.1 that: bias (Wh) bias (W?) \u2264 1 \u2212 \u03b7 + (\u03b7 g + \u03b72 1 \u2212 \u03b7) \u0445 X-22 n\u03b3, var (Wh) var (W?) \u2264 1 \u2212 \u03b7. Here \u03b7 is defined in assumption 2."}, {"heading": "7. Conclusions", "text": "We examined the outlined matrix backbone regression (MRR) from an optimization and statistical perspective. Using the classical sketch, a sufficiently large sketch can provide an approximate solution. In contrast to the classical sketch, the relative error of the Hessian sketch increases because the answers Y are better approximated by linear combinations of columns X. Both the classical and the Hessian sketch can have statistical risks that exceed the risk of the optimal solution by an order of magnitude. We suggested the use of model averages to achieve better optimizations and statistical properties. We have shown that averaging the model leads to significant improvements in the theoretical error limits, suggesting applications in distributed optimization and machine learning. We have also empirically verified its practical usefulness."}, {"heading": "Appendix A. Risk of Fixed Design Model", "text": "Let the risk R (W) be defined in (8). Risk R (W) determines how well the learned W generalizes the test data, which is why we take care of the risk. We explain this formally below. Under the specified design model, a test sample x is uniformly drawn from the sentence {x1, \u00b7 \u00b7, xn} \"Rd.\" The corresponding answer is y = < W0, x > + \"Rm,\" in which the Rm detects random noise; assume that E [x1] = 0 and that the Rm is independent of W, W0 and x."}, {"heading": "Appendix B. Properties of Matrix Sketching: Proofs", "text": "In section B.1 we prove theorem 16. In section B.2 we prove theorem 17. In section B.3 we prove theorem 19."}, {"heading": "B.1 Proof of Theorem 16", "text": "In section B.1.1 we show that the six sketching methods meet assumptions 1.1 and 1.2. In section B.1.2 we show that the six sketching methods meet assumptions 1.3."}, {"heading": "B.1.1 Proof of Assumptions 1.1 and 1.2", "text": "For uniform samples, leverage score sampling, Gauss projection, SRHT, and CountSketch, the subspace in which property and matrix multiplication property are embedded was determined by previous work (Drineas et al., 2008, 2011, Meng and Mahoney, 2013, Nelson and Nguye, 2013, Tropp, 2011, Woodruff, 2014). See also (Wang et al., 2016c) for the summary. Below, we only show the shrunken leverage score sampling. We cite the following problem (Wang et al., 2016b); the problem was first documented by work (Drineas et al., 2008, Gittens, 2011, Woodruff, 2014). Lemma 30 (Wang et al. (2016b)) Let U-Rn-p sampling be a fixed matrix with orthonorthonmal columns."}, {"heading": "When s \u2265 \u03b1 \u03b42 , it holds that", "text": "As a consequence of Markov's imbalance, P & UB-UTSSTB-2F-2F-2F-2F-2F-2F-2F-2F-2F-2F-2F-2F-2F-2F-2F-2F-2F-2F-2F-2F-2F-2F-2F-2F-2F-2F-2F-2F-2F-2F-2F-2F-2F-2F-2F-2F-2F-2F-2F-2F-2F-2F-2F-2F-2F-2F-2F-2F-2F-2F-2F-2F-2F-2F-2F-2F-2F-2F-2F-2F-2F-2F-2F-2F-2F-2F-2F-2F-2F-2F-2F-2F-2F-2F-2F-2F-2F-2F-2F-2F-2F-2F-2F-2F-2F-2F-2F-2F-2F-2F-2F-2F-2F-2F-2F-2F-2F-2F-2F-2F-2F-2F-2F-2F-2F-2F-2F-2F-2F-2F-2F-2F-2F-2F-2F-2F-2F-2F-2F-2F-2F-2F-2F-2F-F-2F-2F-2F-2F-2F-2F-F-2F-2F-2F-F-F-2F-2F-F-2F-2F-F-2F-2F-F-2F-2F-F-2F-F-2F-F-2F-2F-2F."}, {"heading": "B.1.2 Proof of Assumption 1.3", "text": "For Uniform Sampling and SRHT, it is easy to show that STS = ns is the same. Thus, for the sampling probabilities, the leverage score is sampling and the uniform sampling probabilities. Obviously, psi \u2265 12p and i.e is for the shrunk leverage score sampling at least 1 \u2212 2e \u2212 t2 / 2. Vershynin (2010) showed that the largest singular value of the standard gaussian matrix G-Rn \u00b7 s is at most n + \u221a s + t with probability 1 \u2212 2e \u2212 t2 / 2. Thus, for the Gaussian projection matrix S, the highest singular value is the standard gaussian matrix G-Rn \u00b7 s + t) 2 thresholds with probability at least 1 \u2212 2e \u2212 t2 \u2212 t2 / 2. If S is the CountSketch matrix, then each row of S has exactly one zero input."}, {"heading": "B.2 Proof of Theorem 17", "text": "For uniform sampling and SRHT, STS = ns. For uneven sampling with the probabilities p1, \u00b7 \u00b7 \u00b7, pn (\u2211 i pi = 1), leave pmax = maxi pi. The smallest entry in S is 1 \u221a spmax, and hence S TS 1spmax Is. For sampling the leverage effect pmax = \u00b5 n. For sampling the shrunken lever effect pmax = 1 + \u00b5 2n. The lowest limit on the S 2 2 is fixed. Vershynin (2010) showed that the smallest singular value of all n \u00b7 s standard Gaussian matrix G is at least n \u2212 p \u2212 s with a probability of at least 1 \u2212 2e \u2212 t2 / 2. If S = 1 \u00b0 s is the Gaussian projection matrix, the smallest eigenvalue of the STS \u2212 p \u2212 s is equal to a probability of at least 1 \u2212 2e \u2212 t2 / 2. If S = 1 \u00b0 Scetonal G = 1 \u00b0 G = 1 S = 1)."}, {"heading": "B.3 Proof of Theorem 19", "text": "Isolation 2.1. After Theorem 16 and the union-bound Isolation 2.2. Since the Isolation 2.2. is the same type of matrix as the Skizzierungsmatrix, Theorem 16 results in the assumption 2.2. With the same proof for Theorem 16 it is easy to show that the assumption UTB \u2212 UTSiSTi B \u00b2 2F \u00b2 2F \u00b2 2F \u00b2 2F \u00b2 2F \u00b2 2F \u00b2 2F \u00b2 2F \u00b2 2F \u00b2 2F \u00b2 2F \u00b2 S \u00b2 S \u00b2 S \u00b2 S \u00b2 S \u00b2 S \u00b2 S S S \u00b2 S S S S S S S S S S S S S S S S S S S S \u00b2 S S S S S S \u00b2 S S S S S S \u00b2 S S S S \u00b2 S S S S \u00b2 S S S \u00b2 S S S \u00b2 S S S S S S S \u00b2 S S S S S S \u00b2 S S S S S S \u00b2 S S S S S S S \u00b2 S S \u00b2 S S S S S S \u00b2 S S S S S S \u00b2 S S S S S S \u00b2 S S S S S S S S S S \u00b2 S S S S S S S \u00b2 S S S S S S \u00b2 S S S S \u00b2 S S S \u00b2 S S S S S S \u00b2 S S \u00b2 S S S S \u00b2 S S S S \u00b2 S S S S S \u00b2 S S \u00b2 S S S \u00b2 S S \u00b2 S S S \u00b2 S S \u00b2 S S S S \u00b2 S S S \u00b2 S S S \u00b2 S S \u00b2 S S S \u00b2 S S \u00b2 S S \u00b2 S S \u00b2 S \u00b2 S S \u00b2 S \u00b2 S S S \u00b2 S \u00b2 S S S S S \u00b2 S \u00b2 S \u00b2 S S \u00b2 S S S \u00b2 S \u00b2 S S S S \u00b2 S \u00b2 S S \u00b2 S \u00b2 S S S S \u00b2 S \u00b2 S S \u00b2 S \u00b2 S \u00b2 S S S S \u00b2 S \u00b2 S S S S S \u00b2 S \u00b2 S S S \u00b2 S S S S \u00b2 S \u00b2 S \u00b2 S \u00b2 S S S \u00b2 S S S \u00b2 S \u00b2 S \u00b2 S S \u00b2 S S \u00b2 S S S \u00b2 S \u00b2 S \u00b2 S S S S S \u00b2 S \u00b2 S S S S S \u00b2 S \u00b2 S S S S S S \u00b2 S S S S S S \u00b2 S S S S S S S S \u00b2 S S S S S S S S"}, {"heading": "Appendix C. Sketched MRR from Optimization Perspective: Proofs", "text": "In section C.1 we find a key problem. In section C.2 we prove theorem 21. In section C.3 we prove theorem 22."}, {"heading": "C.1 Key Lemma", "text": "Remember that the objective function of matrix chamber regression (MRR) isf (W) isf (W), 1n (W), 1n (W), 1n (W), 1n (W), 1n (W), 1n (W), 1n (W), 1n (W), 1n (W), 1n (W), 1n (W), 1n (W), 1n (W), 1n (W), 1n (W), 1n (W), 1n (W), 1n (W), 2n (W), 2F (W), 2F (W), 2F (W), 2F (W), 2F (W), 2F (W), 1F (W), 1F (W), 1F (W), 1W (W), 1W (W), 1W, 1W, 1W, 1W, 1W, 1W (W), 1W, 1W (W), 1W (W), 1W (W), 1W (W), 1W (W), 1W (W), 1W (W, 1W (W), 1W (W, 1W), 1W (W (W), 1W (W, 1W), 1W (W, 1W, 1W (W), 1W (W, 1W), 1W (W (W), 1W (W (W), 1W (W), 1W (W, 1W (W), 1W (W), 1W (W (W, 1W), 1W (W (W), 1W (W (W), 1W (W, 1W (W), 1W (W, 1W), 1W (W (W), 1W, 1W (W (W), 1W, 1W (W), 1W (W (W, 1W), 1W (W (W), 1W (W (W), 1W (W (W), 1W (W (W), 1W (W), 1W, 1W, 1W (W (W), 1W, 1W"}, {"heading": "C.2 Proof of Theorem 21", "text": "It follows from the definition of W? and Wc \u2212 W? = (XTSSTX + n\u0421Id) \u2212 1XTSSTY \u2212 (XTX + n\u0421Id) \u2212 1XTY.It follows that (XTSSTX + n\u0421Id) (W c \u2212 W?) = XTSSTY (XTSSTXX + n\u0421Id) \u2212 1TSSTId (XTSSTX + n\u0421Id) \u2212 1XTY (XTSSTXX + n\u0421Id) \u2212 1TSSTId (XTSSTSTSTX + n\u0421Id) \u2212 1TSSTId (XTSSTId) \u2212 1\u0421Id (XTSSTId + n\u0421Id) \u2212 XTId (\u2212 XTXTD) that we (XTXTXT) \u2212 XTXTXT (XTSSTId) that we (XTSSTId) \u2212 XTXTXT (XTXTXT) \u2212 1STId (XTSSTId + n\u0421Id) that we (XTSSTId) \u2212 XTXTXTXT (XTSSTId) that we (XTSSTId + n\u0421Id)."}, {"heading": "C.3 Proof of Theorem 22", "text": "The detection After the definition of Wh and W? \u2212 n (XTX + n\u03b3Id) 1 / 2 (Wh \u2212 W?) 1 / 2 (Wh \u2212 n) 1 / 2 (XTSSTX \u2212 n) 1 / 2 (VT \u2212 n) 1 / 2 (VT \u2212 n) 1 / 2 (VT \u2212 n) 1 / 2 (VT \u2212 n) 1 / 2 (VT \u2212 n) 1 / 2 (VT \u2212 n) 1 / 2 (VT \u2212 n) 2 (VT \u2212 n) 2 (VT \u2212 n) (VT \u2212 n) 2 (VT \u2212 n) 1 / 2 (VT \u2212 n) (VT \u2212 2) 1 / 2 (VT \u2212 n) 1 / 2 (VT \u2212 n) (VT \u2212 n) (VT \u2212 n) 1 / 2 (VT \u2212 2) 1 / 2 (VT \u2212 n) 1 / 2 (VT \u2212 n) 1 / 2 (VT \u2212 n) 1 / 2 (VT \u2212 n) 1 / 2 (VT \u2212 n) 1 / 2 (VT \u2212 n) 1 / 2 (VT \u2212 n) 1 / 2 (VT \u2212 n)."}, {"heading": "Appendix D. Sketched MRR from Statistical Perspective: Proofs", "text": "In section D.1, we prove theorem 4. In section D.2, we prove theorem 23. In section D.3, we prove theorem 24. In section B.2, we prove theorem 17. In section D.4, we prove theorem 25. Remember that the solid construction model is Y = XW0 + \u0443, where Ds is random, E = 0, and E [HB-T] = 2In."}, {"heading": "D.1 Proofs of Theorem 4", "text": "The Frobenius standard and matrix trace fulfil this risk for each matrix A and B \u2212 and thus for each fixed A \u2212 \u2212 and B \u2212 and random matrix of the correct size, E \u2212 tr (AAT) + tr (BBT) \u2212 2tr (ABT). The trace is a linear function and thus for each fixed A \u2212 and B \u2212 and random matrix of the correct size, E \u2212 tr (AE) = Tr [A (EE) TSB], where the expectation is met."}, {"heading": "D.2 Proof of Theorem 23", "text": "Evidence of the assumption 1,1 ensures that (1 \u2212 Spot \u2212 Spot \u2212 Spot \u2212 Spot \u2212 Spot \u2212 Spot \u2212 Spot \u2212 Spot \u2212 Spot \u2212 Spot \u2212 Spot \u2212 Spot \u2212 Spot \u2212 Spot \u2212 Spot \u2212 Spot \u2212 Spot \u2212 Spot \u2212 Spot \u2212 Spot \u2212 Spot \u2212 Spot \u2212 Spot \u2212 Spot \u2212 Spot \u2212 Spot \u2212 Spot \u2212 Spot \u2212 Spot \u2212 Spot \u2212 Spot \u2212 Spot \u2212 Spot \u2212 Spot \u2212 Spot \u2212 Spot \u2212 Spot \u2212 Spot \u2212 Spot \u2212 Spot \u2212 Spot \u2212 Spot \u2212 Spot \u2212 Spot \u2212 Spot \u2212 Spot \u2212 Spot \u2212 Spot \u2212 Spot \u2212 Spot \u2212 Spot \u2212 Spot \u2212 Spot \u2212 Spot \u2212 Spot \u2212 Spot \u2212 Spot \u2212 Spot \u2212 Spot \u2212 Spot \u2212 Spot \u2212 Spot \u2212 Spot \u2212 Spot \u2212 Spot \u2212 Spot \u2212 Spot \u2212 Spot \u2212 Spot \u2212 Spot \u2212 Spot \u2212 Spot \u2212 Spot \u2212 Spot \u2212 Spot \u2212 Spot \u2212 Spot \u2212 Spot \u2212 Spot \u2212 Spot \u2212 Spot \u2212 Spot \u2212 Spot \u2212 Spot \u2212 Spot \u2212 Spot \u2212 Spot \u2212 Spot \u2212 Spot \u2212 Spot \u2212 Spot \u2212 Spot \u2212 Spot \u2212 Spot \u2212 Spot \u2212 Spot \u2212 Spot \u2212 Spot \u2212 Spot \u2212 Spot \u2212 Spot \u2212 Spot \u2212 Spot \u2212 Spot \u2212 Spot \u2212 Spot \u2212 Spot \u2212 Spot \u2212 Spot \u2212 Spot \u2212 Spot \u2212 Spot \u2212 Spot \u2212 Spot \u2212 Spot \u2212 Spot \u2212 Spot \u2212 Spot \u2212 Spot \u2212 Spot \u2212 Spot \u2212 Spot \u2212 Spot \u2212 Spot \u2212 Spot \u2212 Spot \u2212 Spot \u2212 Spot \u2212 Spot \u2212 Spot \u2212 Spot \u2212 Spot \u2212 Spot \u2212 Spot \u2212 Spot \u2212 Spot \u2212 Spot \u2212 Spot \u2212 Spot \u2212 Spot \u2212 Spot \u2212 Spot \u2212"}, {"heading": "D.3 Proof of Theorem 24", "text": "Proof Leave B = (UTSSTU + n\u03b3\u03a3 \u2212 2) \u2020 UTS-R\u03c1 \u00b7 s. In the proof of theorem 5 we show that thatvar (Wc) = 0n-0n-0BST-02F. If STS-0ns is then it holds thatvar (Wc) = 0n-0n-0BST-02F-0n-0n-0B-0n-0B-0N (1 + \u03b7) 2 var (W?). This established the lower limits on the variance."}, {"heading": "D.4 Proof of Theorem 25", "text": "The proof of theory 4 shows that the proof of the equality (Wh) = Previous equality (Wh) = Previous equality (Wh) = Previous equality (Wh) = Previous equality (Wh) = Previous equality (Wh) = Previous equality (2 + UTSSTU \u2212 Previous equality (Wh) \u2212 Previous equality (Wh) \u2212 Previous equality (Wh) \u2212 Previous equality (Wh) \u2212 Previous equality (Wh) \u2212 Previous equality (Wh) \u2212 Previous equality (Wh) \u2212 Previous equality (Wh) \u2212 Previous equality (Wh \u2212 Previous equality) \u2212 Previous equality (Wh \u2212 Previous equality) \u2212 Previous (Wh \u2212 Previous equality) \u2212 Previous (Wh \u2212 Previous equality) \u2212 (STU) \u2212 Previous (Wh \u2212 Previous equality (Wh \u2212 Previous equality) \u2212 Previous (Wh) \u2212 Previous (Wh) \u2212 Previous equality (Wh) \u2212 Previous (Wh) \u2212 Previous (Wh) \u2212 Previous equality (Wh) \u2212 Previous (Wh) \u2212 Previous equality (Wh) \u2212 Previous (Wh)"}, {"heading": "Appendix E. Model Averaging from Optimization Perspective: Proofs", "text": "In section E.1 we prove theorem 26. In section E.2 we prove theorem 27."}, {"heading": "E.1 Proof of Theorem 26", "text": "In the proof of Theorem 21 we show that (XTX + n3) 1 / 2 (Wci \u2212 W?) = (XTX + n3) = (XTX + n3) = (XTX + n4) = (XTX + n4) \u2212 1 / 2 (XTSiSTi X + n4) \u2212 1 (XTSiSTi X + n4) \u2212 1 (XTSiSTi X + n4) \u2212 1 (XTSiSTi X + n5) \u2212 1 (XTSiSTi X + n5) \u2212 2 (XTX + n4) \u2212 2 (XTSiSTi) \u2212 1 (XTSiSTi) \u2212 1 / 2 (XTSiSTi) \u2212 1 (XTI), Bi \u2212 2 (V2) \u2212 2 (UTSiSTi)."}, {"heading": "E.2 Proof of Theorem 27", "text": "Proof By Lemon 31 we only have to prove that (XTX + nGP Id) 1 / 2 (Wh \u2212 W?), (Whi \u2212 W?) = VAiBiC, whereas Ai = (\u03a3 2 + nGP IR) \u2212 1 / 2 (IR \u2212 UTSiSTi U), (\u04212 + nGP IR) \u2212 1 / 2, Bi = (\u04212 + nGP IR) 1 / 2 (\u04212 + nGP IR) \u2212 1 (\u04212 + nGP IR) 1 / 2, C = (\u04212 + nGP IR) \u2212 1 / 2 (\u04212 + nGP IR) \u2212 1 / 2 (\u04212 + nGP IR), hence the assumption 2,1 \u2212 2 that for all ICT \u2212 P \u2212 11 + nGP IR (ICT), X + nGP IR (\u04212 + nGP), (\u04212 + nGP), (\u04212 + nGP IR) that (\u04212 + nGP IR)."}, {"heading": "It follows that\u2225\u2225\u2225(XTX + n\u03b3Id)1/2(Wh \u2212W?)\u2225\u2225\u2225", "text": "F: 1 g g g g: 1 g: 1 g: 1 g: 1 g: 1 g: 1 g: 1 g: 1 g: 1 g: 1 g: 1 g: 1 g: 1 g: 1 g: 1 g: 1 g: 1 g: 1 g: 1 g: 1 g: 1 g: 1 g: 1 g: 1 g: 1 g: 1 g: 1 g: 1 g: 1 g: 1 g: 1 g: 1 g: 1 g: 1 g: 1 g: 1 g: 1 g: 1) 2: 1 (1 g: 1) 2: 1 (1 g: 2) 4: 1 (1: 2) 4: 1 (2: 2) 4: 2 (2: 2) 4: 2 (2: 2) 4 (2: 2) 4 (2: 2) 4 (2: 4) 4 (2: 4) 4 (2: 2) 4 (4) 4: 2 (2: 2) 4 (4) 4 (2: 4) 4 (4) 4) 4 (2: 2 (2) 4) 4: 2 (2) 4 (2: 2) 4 (2) 4: 2 (2) 4: 2 (2) 4: 2 (2) 4) (4) (4) (4) (4) (4) (4) (2: 2: 2) 4) (2: 2: 2) 4 (2: 2) 4 (2: 2) 4 (4) (4) (4) (4) (4) (2: 2: 2: 4) (4) (4) (4) (4) (2: 2: 4) (2: 2: 2: 2) 4) (2: 2: 2: 2: 2) 4) 4 (2: 2: 2: 2: 2) 4) 4 (4) (4) (4) (4) (4) (4) (4) (4) (4) (4) (4) (4) (4) (2: 4) (2: 2: 2: 4) (4) (4) (4) (2: 2: 4) (4) (4) (4) (2: 2: 4) (4) (4) (4) (4) (4) (4) (2:"}, {"heading": "Appendix F. Model Averaging from Statistical Perspective: Proofs", "text": "In section F. 1 we prove theorem 28. In section F. 2 we prove theorem 29."}, {"heading": "F.1 Proof of Theorem 28", "text": "The proof of limitation on inclinations (Wc) can be provided in the same way as the proof of limitation on inclinations (Wc) on inclinations (Wc) on inclinations (Wc) on inclinations (Wc) on inclinations (Wc) on inclinations (Wc) on inclinations (Wc) on inclinations (Wc) on inclinations (Wc) on inclinations (Wc) on inclinations (Wc) on inclinations (Wc) on inclinations (Wc) on inclinations (Wc) on inclinations (Wc) on inclinations (Wc) on inclinations (Wc) on inclinations (Wc) on inclinations (Wc) on inclinations (Wc) on inclinations (Wc)."}, {"heading": "F.2 Proof of Theorem 29", "text": "The proof that the limit on Var (Wh) + VII (Wh) + VII (Wh) + VII (Wh) + VII (Wh) + VII (Wh) + VII (Wh) + VII (Wh) + VII (Wh) + VII (Wh) + VII (Wh) + VII (Wh) + VII (Wh) + VII (Wh) + VII (Wh) + VII (Wh) + VII) + VII (Wh) + VII (Wh) + VII (Wh) + VII (Wh) + VII) + VII (Wh) + VII + VII (Wh) + VII + VII (Wh) + VII = (WV) + VII = (VII) (VII) (VV) + VII = VII) (VII) (VII) + VII (VV) = VII (VII) = VII (VII) (VII) = VII (VII) = VII (VII) = VII (VII) + VII (VII) + VII (Vh) + VII (Wh) + VII (Wh) + VII (VII) + VII (VII) + VII (VII) + VII (VII) + VII (VII) = VII = VII (VII) = VII (VII) = VII (VII) = VII (VII) = VII = VII (VII) = VII (VII) = VII = VII (VII) = VII (VII) = VII = VII (VV) = VII (VV) = VII (VV) = VII = VII (VV) = VII (VV) = VII (VII) = VII (VII) = VII (VII) = VII (VII) (VV) = VII (VII) = VII (VII) = VII (VII) = VII) = VII (VII) = VII (VII) = VII (V"}], "references": [{"title": "Sharper bounds for regression and low-rank approximation with regularization", "author": ["Haim Avron", "Kenneth L. Clarkson", "David P. Woodruff"], "venue": "arXiv preprint arXiv:1611.03225,", "citeRegEx": "Avron et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Avron et al\\.", "year": 2016}, {"title": "Pasting small votes for classification in large databases and on-line", "author": ["Leo Breiman"], "venue": "Machine Learning,", "citeRegEx": "Breiman.,? \\Q1999\\E", "shortCiteRegEx": "Breiman.", "year": 1999}, {"title": "Finding frequent items in data streams", "author": ["Moses Charikar", "Kevin Chen", "Martin Farach-Colton"], "venue": "Theoretical Computer Science,", "citeRegEx": "Charikar et al\\.,? \\Q2004\\E", "shortCiteRegEx": "Charikar et al\\.", "year": 2004}, {"title": "Low rank approximation and regression in input sparsity time", "author": ["Kenneth L. Clarkson", "David P. Woodruff"], "venue": "In Annual ACM Symposium on theory of computing (STOC),", "citeRegEx": "Clarkson and Woodruff.,? \\Q2013\\E", "shortCiteRegEx": "Clarkson and Woodruff.", "year": 2013}, {"title": "Fast Monte Carlo algorithms for matrices I: Approximating matrix multiplication", "author": ["Petros Drineas", "Ravi Kannan", "Michael W. Mahoney"], "venue": "SIAM Journal on Computing,", "citeRegEx": "Drineas et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Drineas et al\\.", "year": 2006}, {"title": "Sampling algorithms for `2 regression and applications", "author": ["Petros Drineas", "Michael W. Mahoney", "S. Muthukrishnan"], "venue": "In Annual ACM-SIAM Symposium on Discrete Algorithm (SODA),", "citeRegEx": "Drineas et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Drineas et al\\.", "year": 2006}, {"title": "Relative-error CUR matrix decompositions", "author": ["Petros Drineas", "Michael W. Mahoney", "S. Muthukrishnan"], "venue": "SIAM Journal on Matrix Analysis and Applications,", "citeRegEx": "Drineas et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Drineas et al\\.", "year": 2008}, {"title": "Faster least squares approximation", "author": ["Petros Drineas", "Michael W. Mahoney", "S. Muthukrishnan", "Tam\u00e1s Sarl\u00f3s"], "venue": "Numerische Mathematik,", "citeRegEx": "Drineas et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Drineas et al\\.", "year": 2011}, {"title": "Fast approximation of matrix coherence and statistical leverage", "author": ["Petros Drineas", "Malik Magdon-Ismail", "Michael W. Mahoney", "David P. Woodruff"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Drineas et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Drineas et al\\.", "year": 2012}, {"title": "The spectral norm error of the naive Nystr\u00f6m extension", "author": ["Alex Gittens"], "venue": "arXiv preprint arXiv:1110.5305,", "citeRegEx": "Gittens.,? \\Q2011\\E", "shortCiteRegEx": "Gittens.", "year": 2011}, {"title": "Extensions of Lipschitz mappings into a Hilbert space", "author": ["William B. Johnson", "Joram Lindenstrauss"], "venue": "Contemporary mathematics,", "citeRegEx": "Johnson and Lindenstrauss.,? \\Q1984\\E", "shortCiteRegEx": "Johnson and Lindenstrauss.", "year": 1984}, {"title": "Faster ridge regression via the subsampled randomized Hadamard transform", "author": ["Yichao Lu", "Paramveer Dhillon", "Dean P Foster", "Lyle Ungar"], "venue": "In Advances in Neural Information Processing Systems (NIPS),", "citeRegEx": "Lu et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Lu et al\\.", "year": 2013}, {"title": "A statistical perspective on algorithmic leveraging", "author": ["Ping Ma", "Michael W Mahoney", "Bin Yu"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Ma et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Ma et al\\.", "year": 2015}, {"title": "Randomized algorithms for matrices and data", "author": ["Michael W. Mahoney"], "venue": "Foundations and Trends in Machine Learning,", "citeRegEx": "Mahoney.,? \\Q2011\\E", "shortCiteRegEx": "Mahoney.", "year": 2011}, {"title": "Low-distortion subspace embeddings in inputsparsity time and applications to robust linear regression", "author": ["Xiangrui Meng", "Michael W Mahoney"], "venue": "In Annual ACM Symposium on Theory of Computing (STOC),", "citeRegEx": "Meng and Mahoney.,? \\Q2013\\E", "shortCiteRegEx": "Meng and Mahoney.", "year": 2013}, {"title": "Osnap: Faster numerical linear algebra algorithms via sparser subspace embeddings", "author": ["John Nelson", "Huy L Nguy\u00ean"], "venue": "In IEEE Annual Symposium on Foundations of Computer Science (FOCS),", "citeRegEx": "Nelson and Nguy\u00ean.,? \\Q2013\\E", "shortCiteRegEx": "Nelson and Nguy\u00ean.", "year": 2013}, {"title": "The power of simple tabulation-based hashing", "author": ["Mihai Patrascu", "Mikkel Thorup"], "venue": "Journal of the ACM,", "citeRegEx": "Patrascu and Thorup.,? \\Q2012\\E", "shortCiteRegEx": "Patrascu and Thorup.", "year": 2012}, {"title": "Fast and scalable polynomial kernels via explicit feature maps", "author": ["Ninh Pham", "Rasmus Pagh"], "venue": "In ACM SIGKDD International Conference on Knowledge Discovery and Data Mining (KDD),", "citeRegEx": "Pham and Pagh.,? \\Q2013\\E", "shortCiteRegEx": "Pham and Pagh.", "year": 2013}, {"title": "Iterative Hessian sketch: Fast and accurate solution approximation for constrained least-squares", "author": ["Mert Pilanci", "Martin J Wainwright"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Pilanci and Wainwright.,? \\Q2015\\E", "shortCiteRegEx": "Pilanci and Wainwright.", "year": 2015}, {"title": "A statistical perspective on randomized sketching for ordinary least-squares", "author": ["Garvesh Raskutti", "Michael W Mahoney"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Raskutti and Mahoney.,? \\Q2016\\E", "shortCiteRegEx": "Raskutti and Mahoney.", "year": 2016}, {"title": "Random projections for large-scale regression", "author": ["Gian-Andrea Thanei", "Christina Heinze", "Nicolai Meinshausen"], "venue": "arXiv preprint arXiv:1701.05325,", "citeRegEx": "Thanei et al\\.,? \\Q2017\\E", "shortCiteRegEx": "Thanei et al\\.", "year": 2017}, {"title": "Improved analysis of the subsampled randomized Hadamard transform", "author": ["Joel A Tropp"], "venue": "Advances in Adaptive Data Analysis,", "citeRegEx": "Tropp.,? \\Q2011\\E", "shortCiteRegEx": "Tropp.", "year": 2011}, {"title": "Large scale kernel learning using block coordinate descent", "author": ["Stephen Tu", "Rebecca Roelofs", "Shivaram Venkataraman", "Benjamin Recht"], "venue": "arXiv preprint arXiv:1602.05310,", "citeRegEx": "Tu et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Tu et al\\.", "year": 2016}, {"title": "Introduction to the non-asymptotic analysis of random matrices", "author": ["Roman Vershynin"], "venue": "arXiv preprint arXiv:1011.3027,", "citeRegEx": "Vershynin.,? \\Q2010\\E", "shortCiteRegEx": "Vershynin.", "year": 2010}, {"title": "Sketching meets random projection in the dual: A provable recovery algorithm for big and highdimensional data", "author": ["Jialei Wang", "Jason D Lee", "Mehrdad Mahdavi", "Mladen Kolar", "Nathan Srebro"], "venue": "arXiv preprint arXiv:1610.03045,", "citeRegEx": "Wang et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Wang et al\\.", "year": 2016}, {"title": "SPSD matrix approximation vis column selection: Theories, algorithms, and extensions", "author": ["Shusen Wang", "Luo Luo", "Zhihua Zhang"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Wang et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Wang et al\\.", "year": 2016}, {"title": "Towards more efficient SPSD matrix approximation and CUR matrix decomposition", "author": ["Shusen Wang", "Zhihua Zhang", "Tong Zhang"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Wang et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Wang et al\\.", "year": 2016}, {"title": "Computationally feasible near-optimal subset selection for linear regression under measurement constraints", "author": ["Yining Wang", "Adams Wei Yu", "Aarti Singh"], "venue": "arXiv preprint arXiv:1601.02068,", "citeRegEx": "Wang et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Wang et al\\.", "year": 2016}, {"title": "Feature hashing for large scale multitask learning", "author": ["Kilian Weinberger", "Anirban Dasgupta", "John Langford", "Alex Smola", "Josh Attenberg"], "venue": "In International Conference on Machine Learning (ICML),", "citeRegEx": "Weinberger et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Weinberger et al\\.", "year": 2009}, {"title": "A fast randomized algorithm for the approximation of matrices", "author": ["Franco Woolfe", "Edo Liberty", "Vladimir Rokhlin", "Mark Tygert"], "venue": "Applied and Computational Harmonic Analysis,", "citeRegEx": "Woolfe et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Woolfe et al\\.", "year": 2008}, {"title": "Implementing randomized matrix algorithms in parallel and distributed environments", "author": ["Jiyan Yang", "Xiangrui Meng", "Michael W Mahoney"], "venue": "Proceedings of the IEEE,", "citeRegEx": "Yang et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Yang et al\\.", "year": 2016}, {"title": "Spark: Cluster computing with working", "author": ["Matei Zaharia", "Mosharaf Chowdhury", "Michael J Franklin", "Scott Shenker", "Ion Stoica"], "venue": "sets. HotCloud,", "citeRegEx": "Zaharia et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Zaharia et al\\.", "year": 2010}, {"title": "Communication-efficient algorithms for statistical optimization", "author": ["Yuchen Zhang", "John C. Duchi", "Martin J. Wainwright"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Zhang et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Zhang et al\\.", "year": 2013}, {"title": "Divide and conquer kernel ridge regression: a distributed algorithm with minimax optimal rates", "author": ["Yuchen Zhang", "John Duchi", "Martin Wainwright"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Zhang et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Zhang et al\\.", "year": 2015}], "referenceMentions": [{"referenceID": 3, "context": ", 2006b, 2011, Clarkson and Woodruff, 2013, Meng and Mahoney, 2013, Nelson and Nguy\u00ean, 2013) by solving the sketched LSR problem minw \u2016STXw \u2212 Sy\u20162 instead of the original LSR problem. Solving sketched LSR costs either O(sd2 + Ts) time using the QR decomposition or O(sdt+ Ts) time using accelerated gradient descent algorithms, where t is as defined previously1 and Ts is the time cost of sketching. For example, Ts = O(nd log s) when S is the subsampled randomized Hadamard transform Drineas et al. (2011), and Ts = O(nd) when S is a CountSketch matrix Clarkson and Woodruff (2013).", "startOffset": 15, "endOffset": 507}, {"referenceID": 3, "context": ", 2006b, 2011, Clarkson and Woodruff, 2013, Meng and Mahoney, 2013, Nelson and Nguy\u00ean, 2013) by solving the sketched LSR problem minw \u2016STXw \u2212 Sy\u20162 instead of the original LSR problem. Solving sketched LSR costs either O(sd2 + Ts) time using the QR decomposition or O(sdt+ Ts) time using accelerated gradient descent algorithms, where t is as defined previously1 and Ts is the time cost of sketching. For example, Ts = O(nd log s) when S is the subsampled randomized Hadamard transform Drineas et al. (2011), and Ts = O(nd) when S is a CountSketch matrix Clarkson and Woodruff (2013). There has been much work in RLA on analyzing the quality of sketched LSR with different sketching methods and different objectives; see the reviews Mahoney (2011), Woodruff (2014) and the references therein.", "startOffset": 15, "endOffset": 583}, {"referenceID": 3, "context": ", 2006b, 2011, Clarkson and Woodruff, 2013, Meng and Mahoney, 2013, Nelson and Nguy\u00ean, 2013) by solving the sketched LSR problem minw \u2016STXw \u2212 Sy\u20162 instead of the original LSR problem. Solving sketched LSR costs either O(sd2 + Ts) time using the QR decomposition or O(sdt+ Ts) time using accelerated gradient descent algorithms, where t is as defined previously1 and Ts is the time cost of sketching. For example, Ts = O(nd log s) when S is the subsampled randomized Hadamard transform Drineas et al. (2011), and Ts = O(nd) when S is a CountSketch matrix Clarkson and Woodruff (2013). There has been much work in RLA on analyzing the quality of sketched LSR with different sketching methods and different objectives; see the reviews Mahoney (2011), Woodruff (2014) and the references therein.", "startOffset": 15, "endOffset": 747}, {"referenceID": 3, "context": ", 2006b, 2011, Clarkson and Woodruff, 2013, Meng and Mahoney, 2013, Nelson and Nguy\u00ean, 2013) by solving the sketched LSR problem minw \u2016STXw \u2212 Sy\u20162 instead of the original LSR problem. Solving sketched LSR costs either O(sd2 + Ts) time using the QR decomposition or O(sdt+ Ts) time using accelerated gradient descent algorithms, where t is as defined previously1 and Ts is the time cost of sketching. For example, Ts = O(nd log s) when S is the subsampled randomized Hadamard transform Drineas et al. (2011), and Ts = O(nd) when S is a CountSketch matrix Clarkson and Woodruff (2013). There has been much work in RLA on analyzing the quality of sketched LSR with different sketching methods and different objectives; see the reviews Mahoney (2011), Woodruff (2014) and the references therein.", "startOffset": 15, "endOffset": 764}, {"referenceID": 3, "context": ", 2006b, 2011, Clarkson and Woodruff, 2013, Meng and Mahoney, 2013, Nelson and Nguy\u00ean, 2013) by solving the sketched LSR problem minw \u2016STXw \u2212 Sy\u20162 instead of the original LSR problem. Solving sketched LSR costs either O(sd2 + Ts) time using the QR decomposition or O(sdt+ Ts) time using accelerated gradient descent algorithms, where t is as defined previously1 and Ts is the time cost of sketching. For example, Ts = O(nd log s) when S is the subsampled randomized Hadamard transform Drineas et al. (2011), and Ts = O(nd) when S is a CountSketch matrix Clarkson and Woodruff (2013). There has been much work in RLA on analyzing the quality of sketched LSR with different sketching methods and different objectives; see the reviews Mahoney (2011), Woodruff (2014) and the references therein. The concept of sketched LSR originated in the theoretical computer science literature, e.g., Drineas et al. (2006b, 2011), where the behavior of sketched LSR was studied from an optimization perspective. Let w? be the optimal LSR solution and w\u0303 be the solution to sketched LSR. This line of work established that if s = O(d/ +poly(d)), then the objective function value \u2016Xw\u0303 \u2212 y \u2225\u22252 2 is at most times worse than \u2016Xw? \u2212 y \u2225\u22252 2 . These works also bounded \u2016w\u0303 \u2212 w\u20162 in terms of the difference in the objective function values and the condition number of XTX. A more recent line of work has studied sketched LSR from a statistical perspective: Ma et al. (2015), Raskutti and Mahoney (2016), Pilanci and Wainwright (2015), Wang et al.", "startOffset": 15, "endOffset": 1452}, {"referenceID": 3, "context": ", 2006b, 2011, Clarkson and Woodruff, 2013, Meng and Mahoney, 2013, Nelson and Nguy\u00ean, 2013) by solving the sketched LSR problem minw \u2016STXw \u2212 Sy\u20162 instead of the original LSR problem. Solving sketched LSR costs either O(sd2 + Ts) time using the QR decomposition or O(sdt+ Ts) time using accelerated gradient descent algorithms, where t is as defined previously1 and Ts is the time cost of sketching. For example, Ts = O(nd log s) when S is the subsampled randomized Hadamard transform Drineas et al. (2011), and Ts = O(nd) when S is a CountSketch matrix Clarkson and Woodruff (2013). There has been much work in RLA on analyzing the quality of sketched LSR with different sketching methods and different objectives; see the reviews Mahoney (2011), Woodruff (2014) and the references therein. The concept of sketched LSR originated in the theoretical computer science literature, e.g., Drineas et al. (2006b, 2011), where the behavior of sketched LSR was studied from an optimization perspective. Let w? be the optimal LSR solution and w\u0303 be the solution to sketched LSR. This line of work established that if s = O(d/ +poly(d)), then the objective function value \u2016Xw\u0303 \u2212 y \u2225\u22252 2 is at most times worse than \u2016Xw? \u2212 y \u2225\u22252 2 . These works also bounded \u2016w\u0303 \u2212 w\u20162 in terms of the difference in the objective function values and the condition number of XTX. A more recent line of work has studied sketched LSR from a statistical perspective: Ma et al. (2015), Raskutti and Mahoney (2016), Pilanci and Wainwright (2015), Wang et al.", "startOffset": 15, "endOffset": 1481}, {"referenceID": 3, "context": ", 2006b, 2011, Clarkson and Woodruff, 2013, Meng and Mahoney, 2013, Nelson and Nguy\u00ean, 2013) by solving the sketched LSR problem minw \u2016STXw \u2212 Sy\u20162 instead of the original LSR problem. Solving sketched LSR costs either O(sd2 + Ts) time using the QR decomposition or O(sdt+ Ts) time using accelerated gradient descent algorithms, where t is as defined previously1 and Ts is the time cost of sketching. For example, Ts = O(nd log s) when S is the subsampled randomized Hadamard transform Drineas et al. (2011), and Ts = O(nd) when S is a CountSketch matrix Clarkson and Woodruff (2013). There has been much work in RLA on analyzing the quality of sketched LSR with different sketching methods and different objectives; see the reviews Mahoney (2011), Woodruff (2014) and the references therein. The concept of sketched LSR originated in the theoretical computer science literature, e.g., Drineas et al. (2006b, 2011), where the behavior of sketched LSR was studied from an optimization perspective. Let w? be the optimal LSR solution and w\u0303 be the solution to sketched LSR. This line of work established that if s = O(d/ +poly(d)), then the objective function value \u2016Xw\u0303 \u2212 y \u2225\u22252 2 is at most times worse than \u2016Xw? \u2212 y \u2225\u22252 2 . These works also bounded \u2016w\u0303 \u2212 w\u20162 in terms of the difference in the objective function values and the condition number of XTX. A more recent line of work has studied sketched LSR from a statistical perspective: Ma et al. (2015), Raskutti and Mahoney (2016), Pilanci and Wainwright (2015), Wang et al.", "startOffset": 15, "endOffset": 1512}, {"referenceID": 3, "context": ", 2006b, 2011, Clarkson and Woodruff, 2013, Meng and Mahoney, 2013, Nelson and Nguy\u00ean, 2013) by solving the sketched LSR problem minw \u2016STXw \u2212 Sy\u20162 instead of the original LSR problem. Solving sketched LSR costs either O(sd2 + Ts) time using the QR decomposition or O(sdt+ Ts) time using accelerated gradient descent algorithms, where t is as defined previously1 and Ts is the time cost of sketching. For example, Ts = O(nd log s) when S is the subsampled randomized Hadamard transform Drineas et al. (2011), and Ts = O(nd) when S is a CountSketch matrix Clarkson and Woodruff (2013). There has been much work in RLA on analyzing the quality of sketched LSR with different sketching methods and different objectives; see the reviews Mahoney (2011), Woodruff (2014) and the references therein. The concept of sketched LSR originated in the theoretical computer science literature, e.g., Drineas et al. (2006b, 2011), where the behavior of sketched LSR was studied from an optimization perspective. Let w? be the optimal LSR solution and w\u0303 be the solution to sketched LSR. This line of work established that if s = O(d/ +poly(d)), then the objective function value \u2016Xw\u0303 \u2212 y \u2225\u22252 2 is at most times worse than \u2016Xw? \u2212 y \u2225\u22252 2 . These works also bounded \u2016w\u0303 \u2212 w\u20162 in terms of the difference in the objective function values and the condition number of XTX. A more recent line of work has studied sketched LSR from a statistical perspective: Ma et al. (2015), Raskutti and Mahoney (2016), Pilanci and Wainwright (2015), Wang et al. (2016d) considered statistical properties of sketched LSR like the bias and variance.", "startOffset": 15, "endOffset": 1533}, {"referenceID": 3, "context": ", 2006b, 2011, Clarkson and Woodruff, 2013, Meng and Mahoney, 2013, Nelson and Nguy\u00ean, 2013) by solving the sketched LSR problem minw \u2016STXw \u2212 Sy\u20162 instead of the original LSR problem. Solving sketched LSR costs either O(sd2 + Ts) time using the QR decomposition or O(sdt+ Ts) time using accelerated gradient descent algorithms, where t is as defined previously1 and Ts is the time cost of sketching. For example, Ts = O(nd log s) when S is the subsampled randomized Hadamard transform Drineas et al. (2011), and Ts = O(nd) when S is a CountSketch matrix Clarkson and Woodruff (2013). There has been much work in RLA on analyzing the quality of sketched LSR with different sketching methods and different objectives; see the reviews Mahoney (2011), Woodruff (2014) and the references therein. The concept of sketched LSR originated in the theoretical computer science literature, e.g., Drineas et al. (2006b, 2011), where the behavior of sketched LSR was studied from an optimization perspective. Let w? be the optimal LSR solution and w\u0303 be the solution to sketched LSR. This line of work established that if s = O(d/ +poly(d)), then the objective function value \u2016Xw\u0303 \u2212 y \u2225\u22252 2 is at most times worse than \u2016Xw? \u2212 y \u2225\u22252 2 . These works also bounded \u2016w\u0303 \u2212 w\u20162 in terms of the difference in the objective function values and the condition number of XTX. A more recent line of work has studied sketched LSR from a statistical perspective: Ma et al. (2015), Raskutti and Mahoney (2016), Pilanci and Wainwright (2015), Wang et al. (2016d) considered statistical properties of sketched LSR like the bias and variance. In particular, Pilanci and Wainwright (2015) showed that sketched LSR has much higher variance than the optimal solution.", "startOffset": 15, "endOffset": 1656}, {"referenceID": 18, "context": "Following the convention of Pilanci and Wainwright (2015), Wang et al.", "startOffset": 28, "endOffset": 58}, {"referenceID": 18, "context": "Following the convention of Pilanci and Wainwright (2015), Wang et al. (2016a), we call Wc classical sketch and Wh Hessian sketch.", "startOffset": 28, "endOffset": 79}, {"referenceID": 1, "context": "Note that classical sketch with uniform sampling and model averaging is essentially bagging (synonym bootstrap aggregating) (Breiman, 1996) (or a variant called pasting (Breiman, 1999)) for ridge regression.", "startOffset": 169, "endOffset": 184}, {"referenceID": 32, "context": "The model averaging analyzed in this paper is similar in spirit to the AvgM algorithm of (Zhang et al., 2013).", "startOffset": 89, "endOffset": 109}, {"referenceID": 32, "context": "However, our results do not follow from those of (Zhang et al., 2013).", "startOffset": 49, "endOffset": 69}, {"referenceID": 22, "context": "For example, the conjugate gradient method satisfies \u2016W\u2212W\u2016F \u2016W(0)\u2212W?\u20162 F \u2264 \u03b8 1; the stochastic block coordinate descent (Tu et al., 2016) satisfies Ef(W )\u2212f(W) f(W(0))\u2212f(W?) \u2264 \u03b8 t 2.", "startOffset": 120, "endOffset": 137}, {"referenceID": 3, "context": ", 2006b, 2011, Clarkson and Woodruff, 2013, Meng and Mahoney, 2013, Nelson and Nguy\u00ean, 2013) shares many similarities with our results. However, the theories of sketched LSR developed from the optimization perspective do not obviously extend to MRR, and the statistical analysis of LSR and MRR differ: among other differences, LSR is unbiased while MRR has a nontrivial bias and therefore has a bias-variance tradeoff that must be considered. Lu et al. (2013) has considered a different application of sketching to ridge regression: they assume d n, reduce the number of features in X using sketching, and conduct statistical analysis.", "startOffset": 15, "endOffset": 460}, {"referenceID": 32, "context": "Our results clearly indicate that the performance critically depends on the row coherence of X; this dependence is not captured in (Zhang et al., 2013).", "startOffset": 131, "endOffset": 151}, {"referenceID": 33, "context": "For similar reasons, our work is different from the divide-and-conquer kernel ridge regression algorithm of (Zhang et al., 2015).", "startOffset": 108, "endOffset": 128}, {"referenceID": 17, "context": "Iterative Hessian sketch has been studied by Pilanci and Wainwright (2015), Wang et al.", "startOffset": 45, "endOffset": 75}, {"referenceID": 17, "context": "Iterative Hessian sketch has been studied by Pilanci and Wainwright (2015), Wang et al. (2016a). By way of comparison, all the algorithms in this paper are \u201cone-shot\u201d rather than iterative.", "startOffset": 45, "endOffset": 96}, {"referenceID": 0, "context": "Upon completion of this paper, we noticed the contemporary works (Avron et al., 2016, Thanei et al., 2017). Avron et al. (2016) studied classical sketch from the optimization perspective, and Thanei et al.", "startOffset": 66, "endOffset": 128}, {"referenceID": 0, "context": "Upon completion of this paper, we noticed the contemporary works (Avron et al., 2016, Thanei et al., 2017). Avron et al. (2016) studied classical sketch from the optimization perspective, and Thanei et al. (2017) studied LSR with model averaging.", "startOffset": 66, "endOffset": 213}, {"referenceID": 8, "context": "Leverage score sampling sets pi proportional to the (exact or approximate (Drineas et al., 2012)) row leverage scores li of X.", "startOffset": 74, "endOffset": 96}, {"referenceID": 8, "context": "Leverage scores can be efficiently approximated by the algorithms of (Drineas et al., 2012).", "startOffset": 69, "endOffset": 91}, {"referenceID": 10, "context": "Gaussian projection is also well-known as the prototypical Johnson-Lindenstrauss transform (Johnson and Lindenstrauss, 1984).", "startOffset": 91, "endOffset": 124}, {"referenceID": 12, "context": "In fact, pi can be any convex combination of li \u2211n j=1 lj and 1 n (Ma et al., 2015).", "startOffset": 66, "endOffset": 83}, {"referenceID": 29, "context": "In practice, the subsampled randomized Fourier transform (SRFT) (Woolfe et al., 2008) is often used in lieu of the SRHT, because the SRFT exists for all values of n, whereas Hn exists only for some values of n.", "startOffset": 64, "endOffset": 85}, {"referenceID": 31, "context": "1 Prediction Error We tested the prediction performance of sketched ridge regression by implementing classical sketch with model averaging in PySpark (Zaharia et al., 2010).", "startOffset": 150, "endOffset": 172}, {"referenceID": 13, "context": "Remark 14 Note that the first two assumptions were identified in (Mahoney, 2011) and are the relevant structural conditions needed to be satisfied to obtain strong results from the optimization perspective.", "startOffset": 65, "endOffset": 80}, {"referenceID": 12, "context": "The third condition is new, but Ma et al. (2015), Raskutti and Mahoney (2016) demonstrated that some sort of additional condition is needed to obtain strong results from the statistical perspective.", "startOffset": 32, "endOffset": 49}, {"referenceID": 12, "context": "The third condition is new, but Ma et al. (2015), Raskutti and Mahoney (2016) demonstrated that some sort of additional condition is needed to obtain strong results from the statistical perspective.", "startOffset": 32, "endOffset": 78}, {"referenceID": 4, "context": "1 can actually be expressed in the form of an approximate matrix multiplication bound (Drineas et al., 2006a). We call it the Subspace Embedding Property since, as first highlighted in Drineas et al. (2006b), this subspace embedding property is the key result to obtain high-quality sketching algorithms for regression and related problems.", "startOffset": 87, "endOffset": 208}, {"referenceID": 24, "context": "Lemma 30 (Wang et al. (2016b)) Let U \u2208 Rn\u00d7\u03c1 be any fixed matrix with orthonormal columns.", "startOffset": 10, "endOffset": 30}, {"referenceID": 23, "context": "Vershynin (2010) showed that the greatest singular value of the standard Gaussian matrix G \u2208 Rn\u00d7s is at most \u221a n + \u221a s + t with probability at least 1 \u2212 2e\u2212t/2.", "startOffset": 0, "endOffset": 17}, {"referenceID": 16, "context": "Patrascu and Thorup (2012) showed that the maximal number of balls in any bin is at most n/s+O (\u221a n/s log n ) with probability at least 1\u2212 1 n , where c = O(1).", "startOffset": 0, "endOffset": 27}, {"referenceID": 23, "context": "Vershynin (2010) showed that the smallest singular value of any n\u00d7s standard Gaussian matrix G is at least \u221a n\u2212 \u221a s \u2212 t with probability at least 1 \u2212 2e\u2212t/2.", "startOffset": 0, "endOffset": 17}], "year": 2017, "abstractText": "We address the statistical and optimization impacts of using classical sketch versus Hessian sketch to solve approximately the Matrix Ridge Regression (MRR) problem. Prior research has considered the effects of classical sketch on least squares regression (LSR), a strictly simpler problem. We establish that classical sketch has a similar effect upon the optimization properties of MRR as it does on those of LSR\u2014namely, it recovers nearly optimal solutions. In contrast, Hessian sketch does not have this guarantee; instead, the approximation error is governed by a subtle interplay between the \u201cmass\u201d in the responses and the optimal objective value. For both types of approximations, the regularization in the sketched MRR problem gives it significantly different statistical properties from the sketched LSR problem. In particular, there is a bias-variance trade-off in sketched MRR that is not present in sketched LSR. We provide upper and lower bounds on the biases and variances of sketched MRR; these establish that the variance is significantly increased when classical sketches are used, while the bias is significantly increased when using Hessian sketches. Empirically, sketched MRR solutions can have risks that are higher by an order-of-magnitude than those of the optimal MRR solutions. We establish theoretically and empirically that model averaging greatly decreases this gap. Thus, in the distributed setting, sketching combined with model averaging is a powerful technique that quickly obtains near-optimal solutions to the MRR problem while greatly mitigating the statistical risks incurred by sketching.", "creator": "LaTeX with hyperref package"}}}