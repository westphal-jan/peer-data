{"id": "1702.06054", "review": {"conference": "iclr", "VERSION": "v1", "DATE_OF_SUBMISSION": "20-Feb-2017", "title": "Learning to Repeat: Fine Grained Action Repetition for Deep Reinforcement Learning", "abstract": "Reinforcement Learning algorithms can learn complex behavioral patterns for sequential decision making tasks wherein an agent interacts with an environment and acquires feedback in the form of rewards sampled from it. Traditionally, such algorithms make decisions, i.e., select actions to execute, at every single time step of the agent-environment interactions. In this paper, we propose a novel framework, Fine Grained Action Repetition (FiGAR), which enables the agent to decide the action as well as the time scale of repeating it. FiGAR can be used for improving any Deep Reinforcement Learning algorithm which maintains an explicit policy estimate by enabling temporal abstractions in the action space. We empirically demonstrate the efficacy of our framework by showing performance improvements on top of three policy search algorithms in different domains: Asynchronous Advantage Actor Critic in the Atari 2600 domain, Trust Region Policy Optimization in Mujoco domain and Deep Deterministic Policy Gradients in the TORCS car racing domain.", "histories": [["v1", "Mon, 20 Feb 2017 16:32:07 GMT  (2697kb,D)", "http://arxiv.org/abs/1702.06054v1", "24 pages"]], "COMMENTS": "24 pages", "reviews": [], "SUBJECTS": "cs.LG cs.AI cs.NE", "authors": ["sahil sharma", "aravind s lakshminarayanan", "balaraman ravindran"], "accepted": true, "id": "1702.06054"}, "pdf": {"name": "1702.06054.pdf", "metadata": {"source": "CRF", "title": "DEEP REINFORCEMENT LEARNING", "authors": ["Sahil Sharma", "Aravind S. Lakshminarayanan", "Balaraman Ravindran"], "emails": ["ravi}@cse.iitm.ac.in", "aravindsrinivas@gmail.com"], "sections": [{"heading": "1 INTRODUCTION", "text": "In fact, it is a purely mental game, in which it is a question of finding a path to follow, to find a path to walk, to find a path to walk, to find a path to walk, to walk, to walk it, to walk it, to walk it, to walk it, to walk it, to walk it, and to walk it, to walk it, to walk it, to walk it, to hasten it."}, {"heading": "2 RELATED WORK", "text": "It is the time in which we must consider the question of what the future of the world is like, and what the future of the world is like, and what the future of the world is like, what the future of the world is like, what the future of the world is like, what the future of the world is like, what the future of the world is like, what the future of the world is like, what the future of the world is like, what the future is like, what the future is like, what the future is like, what the future is like, what the future is like, what the future is like, what the future is like, what the future is like, what the future is like, what the future is like, what the future is like, what the future is like, what the future is like, what the future is like, what the future is like, what the future is like, what the future is like, what the future is like, what the future is like, what the future is like, what the future is like, what the future is going to be like, what the future is going to be like, what the future is going to be like, what is going to be like, what is going to be like, what is going to be like, what is going to be like, what is going to be like, what is going to be like, what is going to be like, what is going to be like, what is going to be like the future, what is going to be like the future, what is going to be like the future, what is going to be like, what is going to be like, what is going to be like, what is going to be like, what is going to be like the future, what is going to be like, what is going to be like, what is going to be like, what is going to be like, what is going to be like, what is going to be like the future, what is going to be like, what is going to be like, what is going to be like, what is going to be like, what is going to be like, what is going to be like, what is going to be like, what is going to be like, what is going to be like, what is going to be like, what is going to be like, what is going to be like, what is going to be like, what is going to be like,"}, {"heading": "3 BACKGROUND", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "3.1 ASYNCHRONOUS ADVANTAGE ACTOR CRITIC", "text": "Actor-critic algorithms perform policy updates by maintaining parametric estimates for policy course updates. Asynchronous Advantage Actor Critic (A3C) [Mnih et al. (2016)] learns policy based on an asynchronous n-step decline. The k-learner performs k-copies of policy course updates asynchronously and the parameter updates are sent to a central parameter server at regular intervals, ensuring that time correlations between subsequent updates are interrupted as the different strands may explore different parts of the state space in parallel. The objective function for policy improvement in A3C is: L (dependent a) = logged (at | st)."}, {"heading": "3.2 TRUST REGION POLICY OPTIMIZATION", "text": "TRPO [Schulman et al. (2015)] is a political optimization algorithm. It proposes a limited optimization of a surrogate loss function, with theoretical guarantees for monotonous policy improvements. The TRPO surrogate loss function L for potential next policies (\u03c0) is: Limed (\u03b8) = \u03b7 (\u03c0) + \u2211 s \u03c1\u03c0 (s) \u2022 a \u03c0 (a | s) A\u03c0 (s, a), where histolds are the parameters of policy, and \u03b8 are parameters of the next policy. This surrogate loss function is optimized according to the limitation: DmaxKL (\u03c0, \u03c0) \u2264 \u043c, which ensures that policy improvement can be carried out in non-trivial steps and at the same time the new policy does not differ much from current policy due to the KL divergence limitation."}, {"heading": "3.3 DEEP DETERMINISTIC POLICY GRADIENTS", "text": "According to the Deterministic Policy Gradient (DPG) Theorem [Lever (2014)], the gradient of the performance objective (J) of deterministic politics (\u00b5) in continuous spheres of action in terms of political parameters (\u03b8) is defined by: Achieved by: Achieved by: Achieved by: Achieved by: Achieved by: Achieved by: Achieved by: Achieved by: Achieved by: Achieved by a suitably defined performance target J (s, a). Achieved by: Achieved by a suitably defined performance target J. The DPG model constructed according to this theorem consists of an actor issuing an action vector in the continuous sphere of action, and a critical model Q (s, a) evaluating the action chosen in a state. The DPG algorithm [Lillicrap et al. (2015)] expands the DPG algorithm by introducing a non-linear neural network for the critic and the approximators."}, {"heading": "4 FIGAR: FINE GRAINED ACTION REPETITION", "text": "FiGAR provides a DRL algorithm with the ability to model temporal abstractions by adding the ability to predict the number of time steps for which an action chosen for execution is to be repeated. This prediction depends on the current state in the environment. The FiGAR framework can be used to expand any DRL algorithm (say Z) that has an explicit policy. Let Z denote the extension of Z under FiGAR. Z has two independent sections: Create FiGAR \u2212 Z 1: Function MAKEFIGAR (DRL algorithm Z, ActionRepetitionSet W) 2: st: Achieve a state at a time t 3: Measures taken at a time t 4: Achieve an action policy at a time t 5: Create a (st) action network to realize the action policy."}, {"heading": "4.1 HOW FIGAR OPERATES", "text": "The following procedure describes how FiGAR variant navigates the MDP, which solves it: 1. In the very first state s0 seen by Z \u2032, a tuple (a0, x0) of the action to be performed and the number of time steps for which it is to be performed is decided. a0 is decided on the basis of \u03c0\u03b8a (s0), whereas x0 is decided on the basis of \u03c0\u03b8x (s0). Each such tuple is known as an action decision. 2. We refer to the state of the agent after such action decisions have been taken. Likewise, xj and aj refer to the repetition of action and the action chosen after such action decisions. Note that xj refers to the state of the agent after such action decisions have been taken."}, {"heading": "4.2 FIGAR-A3C", "text": "A3C uses f\u03b8a (sj) and f\u03b8c (sj), which represent the politics \u03c0 (a | sj) and the value function V (sj) respectively, \u03c0 (a | sj) is a vector of magnitude corresponding to the scope of action of the underlying MDP, while V (sj) is a scalar. FiGAR extends the A3C algorithm as follows: 1. Note: This neural network outputs a | W | dimensional vector representing the probability distribution over the elements of the set W. The sampled time scale from this multinomic distribution determines how long the action (sj) decided with f\u03b8a (sj) x is repeated."}, {"heading": "4.3 FIGAR-TRPO", "text": "Although f\u03b8a (sj) in A3C is general enough to output continuous or discrete actions, we consider A3C only for discrete action spaces. Maintaining the notation from the previous subsection, we describe FiGAR-TRPO, where we consider the case of power generated by the network f\u03b8a (sj) to be A-dimensional, each dimension being independent and describing a continuously evaluated action. Therefore, stochastic policy is modeled as a multivariant Gaussian with diagonal covariance matrix. Mean parameters and covariance matrix are collectively represented by \u03b8a and the linked mean covariance vector by the function f\u03b8a (sj). FiGAR-TRPO is constructed as follows: 1. In TRPO, the objective function L\u03b8old (\u03b8) is formed on the basis of trajectories executed according to current policy."}, {"heading": "4.4 FIGAR-DDPG", "text": "In this subsection, we present an extension of the DDPG within the FiGAR framework. The DDPG consists of f\u03b8a (sj), which denotes a deterministic policy \u00b5 (s) and is a size vector corresponding to the scope of action of the underlying MDP; and f\u03b8c (sj, saj), which denotes the critic network, whose output is a single number, the estimated state action value function Q sj, aj. The FiGAR framework expands the DDPG algorithm as follows: 1. f\u03b8c is introduced, similar to FiGAR-A3C. This implies that the complete policy for FiGARDDPG (\u03c0\u03b8a, a, a DDDPG algorithm is introduced, similar to FiGAR-A3C."}, {"heading": "5 EXPERIMENTAL SETUP AND RESULTS", "text": "The experiments are designed to understand the answers to the following questions: 1. Can FiGAR extensions for different DRL algorithms learn to use dynamic repetition of actions? 2. How does FiGAR affect the performance of different algorithms for different tasks? 3. Is FiGAR able to learn control over different types of repetition of actions W? In the next three subsections, we experiment with the simplest possible repetition of actions W = {1, 2, \u00b7 \u00b7, | W |}. In the fourth subsection, we understand the impact that changing the repetition of actions W has on the learned guidelines."}, {"heading": "5.1 FIGAR-A3C ON ATARI 2600", "text": "The hyperparameters were adjusted to a subset of games (Beamrider, Breakout, Pong, Seaquest and Space Invaders) and kept constant across all games. W is perhaps the most important hyperparameter and shows our confidence in the ability of a DRL agent to predict the future. Such choice must depend on the domain in which the DRL agent operates. We just wanted to demonstrate the ability of FiGAR to learn temporal abstractions and conclusions, rather than focusing on an optimal | W |, it was chosen to be 30, arbitrary. The specific timescales we choose are 1, 2, \u00b7 \u00b7, 30. FiGAR-A3C and A3C were trained for 100 million decision steps and were evaluated in terms of the final policy that was learned. FiGAR-A3C algorithm as a basis (b), calculated percentage improvement (3x)."}, {"heading": "5.2 FIGAR-TRPO ON MUJOCO TASKS", "text": "In this subsection, we show that FiGAR-TRPO can learn to solve the physical tasks simulated by Mujoco to some extent successfully. Similar to FiGAR-A3C, | W | is selected to be arbitrary 30. Complete guidelines (f\u03b8a, f\u03b8x) are trained together. Guidelines learned after each TRPO optimization step (details in Appendix C) are compared with current best known guidelines in order to arrive at the overall best policy. Results in this subsection relate to this best policy. Table 1 compares the performance of TRPO and FiGAR-TRPO. The number in parentheses is the chosen average repetition of measures. As shown in the table, FiGAR either learns guidelines that are to be implemented much faster, albeit at a slight loss of optimism, or it learns guidelines that are similar to the non-repeated cases, with performance competing with the basic algorithm. This best policy is then evaluated by 100 episodes, to arrive at the average score in the table 1."}, {"heading": "5.3 FIGAR-DDPG ON TORCS", "text": "FiGAR-DDPG was trained and tested on the TORCS domain. | W | was randomly selected as 15. FIGAR-DDPG completed the race task flawlessly and completed 20 laps of the track, after which the simulator stopped. The total reward FiGAR-DDPG received was 557929.68 versus 59519.70 that DDPG received. We also observed that FiGAR-DDPG learned policies that were smoother than those DDPG had learned. A video showing the learned driving behavior of the FiGAR-DDPG agent can be found at https: / / youtu.be / dX8J-sF-WX4. See Appendix D for experimental and architectural details."}, {"heading": "5.4 EFFECT OF ACTION REPETITION SET ON FIGAR", "text": "This subsection answers in the affirmative the third question raised at the beginning of this section. We show that the amount of action repetitions W = {1, 2, \u00b7 \u00b7, 30} in which FiGAR-A3C performed well, and that the good performance is transferable to other action repetitions. To demonstrate the generality of FiGAR in relation to W, we chose a variety of action repetitions W, trained and rated FiGAR-A3C variants, which learn to repeat themselves in relation to their respective action repetitions. Table 3 describes the different FiGAR variants that were considered for these experiments in relation to their action repetitions W. Note that the hyperparameters of the different variants of FiGAR-A3C were not coordinated, but rather the same ones that were obtained by voting for FiGAR-30. Table 2 contains a comparison of the raw values obtained by the different FiGAR-A3C variants in comparison to the base line of FiGAR-A3C, which shows that the GAR-30 is well tuned for gameplay variants."}, {"heading": "6 CONCLUSION, SHORTCOMINGS AND FUTURE WORK", "text": "We propose a lightweight framework (FiGAR) to improve the current Deep Reinforcement Learning algorithms for policy optimization, learning temporal abstractions in the political space. It is general and applicable to DRL algorithms that deal with policy gradients for continuous and discrete action spaces. However, our results show that FiGAR can be used to significantly improve current policy gradients and actor-critical algorithms, thereby learning better control strategies in multiple areas by detecting optimal sequences of time-extended macro activities. Our results show that FiGAR can be used to significantly improve current policy gradients and actor-critical algorithms by detecting the optimal sequences of time-extended macro activities. Atari, TORCS and MuJoCo represent environments that are largely deterministic, with minimal levels of environmental stoicism."}, {"heading": "ACKNOWLEDGMENTS", "text": "We thank Volodymr Mnih for providing valuable hyperparameter information. We thank Aravind Rajeswaran (University of Washington) for very helpful discussions and feedback on the MuJoCo domain tasks. The TRPO implementation was a modification of https: / / github.com / aravindr93 / robustRL. The DDPG implementation was a modification of https: / / github.com / yanpanlau / DPG Keras-Torcs. We thank ILDS (http: / / web.iitm.ac.in / ilds /) for the resources we used to conduct A3C experiments."}, {"heading": "APPENDIX A: EXPERIMENTAL DETAILS FOR FIGAR-A3C", "text": "It was the first and last action we carried out this year. (2016) It was the first and second that we applied in the second and third round of the competition. (2016) It was the first and second that we applied in the third and fourth round of the competition. (2016) It was the second and third round of the competition. (2016) It was the third and fourth round of the competition. (2016) It was the second and fourth round of the competition. (2016) It was the third and fourth round of the competition. (2016) It was the second and fourth round of the competition. (2016) It was the first and fourth round of the competition. (2016) It was the second and fourth round of the competition. (2016) It was the first and second round of the competition. (2016) It was the first and second round of the competition. (2016) It was the second and third round of the competition. (2016)"}, {"heading": "APPENDIX C: EXPERIMENTAL SETUP FOR FIGAR-TRPO", "text": "In the first phase (P1), the KL divergence contracts to the new policy are examined. In our experiments, 500 such policy improvement steps were performed. In the second phase (P2), a policy improvement step is performed by performing an optimization step on the surrogate loss function, with the KL divergence contraction being limited to the new policy. In our experiments, 500 such policy improvement steps were performed. K-Varies with the learning progress and the timing of what value K would take in the next iteration of P1 is defined in relation to the return in the last iteration of P1. Hence, if the return was large in the previous iteration of P1, a small number of episodes is used to construct the replacement function in the current iteration."}, {"heading": "APPENDIX D: EXPERIMENTAL DETAILS FOR FIGAR-DDPG", "text": "The DDPG algorithm also works on the low-dimensional (29-dimensional) feature vector observations. The range consists of 3 continuous actions, acceleration, pause and control. The W hyper parameter, which was used in main experiments, was chosen to be 15 arbitrary. In contrast to Lillicrap et al. (2015), we did not find it useful to use batch normalization and therefore it was not used. However, a replay memory of the size 10000 was used. Target networks were also used with soft updates that are applied with \u03c4 = 0.001. Sine DPG is an off-policy actor-critic method that we need to ensure that sufficient exploration takes place. Use of an Ornstein-Uhlenbeck process (refer to Lillicrap et al. (2015) for details that export was carried out in the action policy space."}, {"heading": "APPENDIX E: DETAILS FOR FIGAR-VARIANTS", "text": "Figure 6 clearly shows that while FiGAR A3C must be explored in two separate action spaces (primitive actions and action repetitions), the training progress will not be slowed down as a result of this exploration. Table 2 contains the final evaluation results obtained by different FiGAR variants. Figure 7 contains a bar chart of the same table to demonstrate the advantage of all FiGAR variants over baseline values. APPENDIX F: IMPORTANT FITNESS OF \u03c0\u03b8xOne could potentially use FiGAR in the evaluation phase (after completion of training) with an action repetition rate of 1 by selecting each action in accordance with E-GAR and completely discarding the learned repetition guidelines E-Gx. Such a FiGAR variant is referred to as FiGAR-where-perception x. We show that FiGAR-where-perception games are not inferior to the result of most of FiGAR and GAR-time games."}, {"heading": "APPENDIX G: SHARED REPRESENTATION EXPERIMENTS FOR FIGAR-TRPO", "text": "During these experiments with FiGAR-TRPO, the political components f\u03b8a and f\u03b8x do not share representations. This appendix contains experimental results in the environment in which (f\u03b8a and f\u03b8x) share all levels except the last one. This agent / network is called FiGAR-shared-TRPO. All hyperparameters are the same as those in Appendix C, except \u03b2ar and \u03b2KL, which were obtained by a grid search similar to Appendix C. These were matched to all 5 tasks. Values for these hyperparameters, which we considered optimal, are \u03b2ar = 1.28 and \u03b2KL = 0.16. The same training and evaluation regime as Appendix C. The performance of the best learned policies is tabulated in Table 10FiFiGAR-shared-TRPO, which we do not apply much better in relation to the totality of the TRO layers than we use in relation to the two layers of the TRO layers."}], "references": [{"title": "The arcade learning environment: An evaluation platform for general agents", "author": ["Marc G. Bellemare", "Yavar Naddaf", "Joel Veness", "Michael Bowling"], "venue": "Journal of Artificial Intelligence Research,", "citeRegEx": "Bellemare et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Bellemare et al\\.", "year": 2013}, {"title": "Transition point dynamic programming", "author": ["Kenneth M Buckland", "Peter D Lawrence"], "venue": "Advances in neural information processing systems,", "citeRegEx": "Buckland and Lawrence.,? \\Q1994\\E", "shortCiteRegEx": "Buckland and Lawrence.", "year": 1994}, {"title": "Hierarchical reinforcement learning with the maxq value function decomposition", "author": ["Thomas G Dietterich"], "venue": null, "citeRegEx": "Dietterich.,? \\Q2000\\E", "shortCiteRegEx": "Dietterich.", "year": 2000}, {"title": "Reinforcement learning methods for continuous-time markov decision problems", "author": ["Steven J Duff"], "venue": null, "citeRegEx": "Duff.,? \\Q1995\\E", "shortCiteRegEx": "Duff.", "year": 1995}, {"title": "Deep reinforcement learning with macro-actions", "author": ["Ishan P Durugkar", "Clemens Rosenbaum", "Stefan Dernbach", "Sridhar Mahadevan"], "venue": "arXiv preprint arXiv:1606.04615,", "citeRegEx": "Durugkar et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Durugkar et al\\.", "year": 2016}, {"title": "Learning to translate in real-time with neural machine translation", "author": ["Jiatao Gu", "Graham Neubig", "Kyunghyun Cho", "Victor OK Li"], "venue": "arXiv preprint arXiv:1610.00388,", "citeRegEx": "Gu et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Gu et al\\.", "year": 2016}, {"title": "Deep reinforcement learning in parametrized action space", "author": ["Matthew Hausknecht", "Peter Stone"], "venue": "4th International Conference on Learning Representations,", "citeRegEx": "Hausknecht and Stone.,? \\Q2016\\E", "shortCiteRegEx": "Hausknecht and Stone.", "year": 2016}, {"title": "Half field offense: An environment for multiagent learning and ad hoc teamwork", "author": ["Matthew Hausknecht", "Prannoy Mupparaju", "Sandeep Subramanian", "Shivaram Kalyanakrishnan", "Peter Stone"], "venue": "In AAMAS Adaptive Learning Agents (ALA) Workshop,", "citeRegEx": "Hausknecht et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Hausknecht et al\\.", "year": 2016}, {"title": "Delving deep into rectifiers: Surpassing human-level performance on imagenet classification", "author": ["Kaiming He", "Xiangyu Zhang", "Shaoqing Ren", "Jian Sun"], "venue": "In Proceedings of the IEEE International Conference on Computer Vision, pp", "citeRegEx": "He et al\\.,? \\Q2015\\E", "shortCiteRegEx": "He et al\\.", "year": 2015}, {"title": "Dynamic frame skip deep q network", "author": ["Aravind S Lakshminarayanan", "Sahil Sharma", "Balaraman Ravindran"], "venue": "arXiv preprint arXiv:1605.05365,", "citeRegEx": "Lakshminarayanan et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Lakshminarayanan et al\\.", "year": 2016}, {"title": "Deterministic policy gradient algorithms", "author": ["Guy Lever"], "venue": null, "citeRegEx": "Lever.,? \\Q2014\\E", "shortCiteRegEx": "Lever.", "year": 2014}, {"title": "Continuous control with deep reinforcement learning", "author": ["Timothy P Lillicrap", "Jonathan J Hunt", "Alexander Pritzel", "Nicolas Heess", "Tom Erez", "Yuval Tassa", "David Silver", "Daan Wierstra"], "venue": "arXiv preprint arXiv:1509.02971,", "citeRegEx": "Lillicrap et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Lillicrap et al\\.", "year": 2015}, {"title": "Self-improving factory simulation using continuous-time average-reward reinforcement learning", "author": ["Sridhar Mahadevan", "Nicholas Marchalleck", "Tapas K Das", "Abhijit Gosavi"], "venue": null, "citeRegEx": "Mahadevan et al\\.,? \\Q1997\\E", "shortCiteRegEx": "Mahadevan et al\\.", "year": 1997}, {"title": "Human-level control through deep reinforcement learning", "author": ["Volodymyr Mnih", "Koray Kavukcuoglu", "David Silver", "Andrei A. Rusu", "Joel Veness", "Marc G. Bellemare", "Alex Graves", "Martin Riedmiller", "Andreas K. Fidjeland", "Georg Ostrovski", "Stig Petersen", "Charles Beattie", "Amir Sadik", "Ioannis Antonoglou", "Helen King", "Dharshan Kumaran", "Daan Wierstra", "Shane Legg", "Demis Hassabis"], "venue": null, "citeRegEx": "Mnih et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Mnih et al\\.", "year": 2015}, {"title": "Asynchronous methods for deep reinforcement learning", "author": ["Volodymyr Mnih", "Adria Puigdomenech Badia", "Mehdi Mirza", "Alex Graves", "Timothy P Lillicrap", "Tim Harley", "David Silver", "Koray Kavukcuoglu"], "venue": "In International Conference on Machine Learning,", "citeRegEx": "Mnih et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Mnih et al\\.", "year": 2016}, {"title": "Simultaneous machine translation using deep reinforcement learning", "author": ["Harsh Satija", "Joelle Pineau"], "venue": "ICML 2016 Workshop on Abstraction in Reinforcement Learning,", "citeRegEx": "Satija and Pineau.,? \\Q2016\\E", "shortCiteRegEx": "Satija and Pineau.", "year": 2016}, {"title": "Prioritized experience replay", "author": ["Tom Schaul", "John Quan", "Ioannis Antonoglou", "David Silver"], "venue": "4th International Conference on Learning Representations,", "citeRegEx": "Schaul et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Schaul et al\\.", "year": 2015}, {"title": "Trust region policy optimization", "author": ["John Schulman", "Sergey Levine", "Philipp Moritz", "Michael I Jordan", "Pieter Abbeel"], "venue": "CoRR, abs/1502.05477,", "citeRegEx": "Schulman et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Schulman et al\\.", "year": 2015}, {"title": "Introduction to reinforcement learning", "author": ["Richard S. Sutton", "Andrew G. Barto"], "venue": null, "citeRegEx": "Sutton and Barto.,? \\Q1998\\E", "shortCiteRegEx": "Sutton and Barto.", "year": 1998}, {"title": "Mujoco: A physics engine for model-based control", "author": ["Emanuel Todorov", "Tom Erez", "Yuval Tassa"], "venue": "In 2012 IEEE/RSJ International Conference on Intelligent Robots and Systems,", "citeRegEx": "Todorov et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Todorov et al\\.", "year": 2012}, {"title": "Strategic attentive writer for learning macro-actions", "author": ["Alexander Vezhnevets", "Volodymyr Mnih", "Simon Osindero", "Alex Graves", "Oriol Vinyals", "John Agapiou"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Vezhnevets et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Vezhnevets et al\\.", "year": 2016}, {"title": "Torcs, the open racing car simulator. Software available at http://torcs", "author": ["Bernhard Wymann", "E Espi\u00e9", "C Guionneau", "C Dimitrakakis", "R Coulom", "A Sumner"], "venue": "sourceforge. net,", "citeRegEx": "Wymann et al\\.,? \\Q2000\\E", "shortCiteRegEx": "Wymann et al\\.", "year": 2000}, {"title": "The initial learning rate used was 10\u22123 and it was linearly annealed to 0 over 100 million steps. The n used in n-step returns was 20. Entropy regularization was used to encourage exploration, similar to", "author": ["Mnih"], "venue": "Mnih et al", "citeRegEx": "Mnih,? \\Q2016\\E", "shortCiteRegEx": "Mnih", "year": 2016}, {"title": "2015), except that the pre-LSTM hidden layer had size", "author": ["Mnih"], "venue": null, "citeRegEx": "Mnih,? \\Q2015\\E", "shortCiteRegEx": "Mnih", "year": 2015}, {"title": "Similar to Mnih et al. (2016) the Actor and Critic share all but one", "author": ["Mnih"], "venue": null, "citeRegEx": "Mnih,? \\Q2016\\E", "shortCiteRegEx": "Mnih", "year": 2016}, {"title": "2015), we did not find it useful to use batch normalization and hence it was not used. However, a replay memory was used of size 10000. Target networks were also used with soft updates being applied with \u03c4 = 0.001. Sine DDPG is an off-policy actor-critic method, we need to ensure that sufficient exploration takes place. Use of an Ornstein-Uhlenbeck process", "author": ["Lillicrap"], "venue": null, "citeRegEx": "Lillicrap,? \\Q2015\\E", "shortCiteRegEx": "Lillicrap", "year": 2015}], "referenceMentions": [{"referenceID": 0, "context": "Such a combination of representation learning through deep neural networks with reinforcement learning objectives has shown promising results in many sequential decision making domains such as the Atari 2600 domain [Bellemare et al. (2013); Mnih et al.", "startOffset": 216, "endOffset": 240}, {"referenceID": 0, "context": "Such a combination of representation learning through deep neural networks with reinforcement learning objectives has shown promising results in many sequential decision making domains such as the Atari 2600 domain [Bellemare et al. (2013); Mnih et al. (2015); Schaul et al.", "startOffset": 216, "endOffset": 260}, {"referenceID": 0, "context": "Such a combination of representation learning through deep neural networks with reinforcement learning objectives has shown promising results in many sequential decision making domains such as the Atari 2600 domain [Bellemare et al. (2013); Mnih et al. (2015); Schaul et al. (2015); Mnih et al.", "startOffset": 216, "endOffset": 282}, {"referenceID": 0, "context": "Such a combination of representation learning through deep neural networks with reinforcement learning objectives has shown promising results in many sequential decision making domains such as the Atari 2600 domain [Bellemare et al. (2013); Mnih et al. (2015); Schaul et al. (2015); Mnih et al. (2016)], Mujoco simulated physics tasks domain [Todorov et al.", "startOffset": 216, "endOffset": 302}, {"referenceID": 0, "context": "Such a combination of representation learning through deep neural networks with reinforcement learning objectives has shown promising results in many sequential decision making domains such as the Atari 2600 domain [Bellemare et al. (2013); Mnih et al. (2015); Schaul et al. (2015); Mnih et al. (2016)], Mujoco simulated physics tasks domain [Todorov et al. (2012); Lillicrap et al.", "startOffset": 216, "endOffset": 365}, {"referenceID": 0, "context": "Such a combination of representation learning through deep neural networks with reinforcement learning objectives has shown promising results in many sequential decision making domains such as the Atari 2600 domain [Bellemare et al. (2013); Mnih et al. (2015); Schaul et al. (2015); Mnih et al. (2016)], Mujoco simulated physics tasks domain [Todorov et al. (2012); Lillicrap et al. (2015)], the Robosoccer domain [Hausknecht et al.", "startOffset": 216, "endOffset": 390}, {"referenceID": 0, "context": "Such a combination of representation learning through deep neural networks with reinforcement learning objectives has shown promising results in many sequential decision making domains such as the Atari 2600 domain [Bellemare et al. (2013); Mnih et al. (2015); Schaul et al. (2015); Mnih et al. (2016)], Mujoco simulated physics tasks domain [Todorov et al. (2012); Lillicrap et al. (2015)], the Robosoccer domain [Hausknecht et al. (2016)] and the TORCS domain [Wymann et al.", "startOffset": 216, "endOffset": 440}, {"referenceID": 0, "context": "Such a combination of representation learning through deep neural networks with reinforcement learning objectives has shown promising results in many sequential decision making domains such as the Atari 2600 domain [Bellemare et al. (2013); Mnih et al. (2015); Schaul et al. (2015); Mnih et al. (2016)], Mujoco simulated physics tasks domain [Todorov et al. (2012); Lillicrap et al. (2015)], the Robosoccer domain [Hausknecht et al. (2016)] and the TORCS domain [Wymann et al. (2000); Mnih et al.", "startOffset": 216, "endOffset": 484}, {"referenceID": 0, "context": "Such a combination of representation learning through deep neural networks with reinforcement learning objectives has shown promising results in many sequential decision making domains such as the Atari 2600 domain [Bellemare et al. (2013); Mnih et al. (2015); Schaul et al. (2015); Mnih et al. (2016)], Mujoco simulated physics tasks domain [Todorov et al. (2012); Lillicrap et al. (2015)], the Robosoccer domain [Hausknecht et al. (2016)] and the TORCS domain [Wymann et al. (2000); Mnih et al. (2016)].", "startOffset": 216, "endOffset": 504}, {"referenceID": 0, "context": "Such a combination of representation learning through deep neural networks with reinforcement learning objectives has shown promising results in many sequential decision making domains such as the Atari 2600 domain [Bellemare et al. (2013); Mnih et al. (2015); Schaul et al. (2015); Mnih et al. (2016)], Mujoco simulated physics tasks domain [Todorov et al. (2012); Lillicrap et al. (2015)], the Robosoccer domain [Hausknecht et al. (2016)] and the TORCS domain [Wymann et al. (2000); Mnih et al. (2016)]. Often, MDP settings consist of an agent interacting with the environment at discrete time steps. A common feature shared by all the Deep Reinforcement Learning (DRL) algorithms above is that they repeatedly execute a chosen action for a fixed number of time steps k. If at represents the action taken at time step t, then for the said algorithms, a1 = a2 = \u00b7 \u00b7 \u00b7 = ak, ak+1 = ak+2 = \u00b7 \u00b7 \u00b7 = a2k and in general aik+1 = aik+2 = \u00b7 \u00b7 \u00b7 = a(i+1)k, i \u2265 0. Action repetition allows these algorithms to compute the action once every k time steps and hence operate at higher speeds, thus achieving real-time performance. This also offers other advantages such as smooth action policies. More importantly, as shown in Lakshminarayanan et al. (2016) and Durugkar et al.", "startOffset": 216, "endOffset": 1245}, {"referenceID": 0, "context": "Such a combination of representation learning through deep neural networks with reinforcement learning objectives has shown promising results in many sequential decision making domains such as the Atari 2600 domain [Bellemare et al. (2013); Mnih et al. (2015); Schaul et al. (2015); Mnih et al. (2016)], Mujoco simulated physics tasks domain [Todorov et al. (2012); Lillicrap et al. (2015)], the Robosoccer domain [Hausknecht et al. (2016)] and the TORCS domain [Wymann et al. (2000); Mnih et al. (2016)]. Often, MDP settings consist of an agent interacting with the environment at discrete time steps. A common feature shared by all the Deep Reinforcement Learning (DRL) algorithms above is that they repeatedly execute a chosen action for a fixed number of time steps k. If at represents the action taken at time step t, then for the said algorithms, a1 = a2 = \u00b7 \u00b7 \u00b7 = ak, ak+1 = ak+2 = \u00b7 \u00b7 \u00b7 = a2k and in general aik+1 = aik+2 = \u00b7 \u00b7 \u00b7 = a(i+1)k, i \u2265 0. Action repetition allows these algorithms to compute the action once every k time steps and hence operate at higher speeds, thus achieving real-time performance. This also offers other advantages such as smooth action policies. More importantly, as shown in Lakshminarayanan et al. (2016) and Durugkar et al. (2016), macro-actions constituting the same action repeated k times could be interpreted as introducing temporal abstractions in the induced policies thereby enabling transitions between temporally distant advantageous states.", "startOffset": 216, "endOffset": 1272}, {"referenceID": 11, "context": "The time scale for action repetition has largely been static in DRL algorithms until now [Mnih et al. (2015; 2016); Schaul et al. (2015)].", "startOffset": 90, "endOffset": 137}, {"referenceID": 9, "context": "Lakshminarayanan et al. (2016) are the first to explore dynamic time scales for action repetition in the DRL setting and show that it leads to significant improvement in performance on a few Atari 2600 games.", "startOffset": 0, "endOffset": 31}, {"referenceID": 9, "context": "Lakshminarayanan et al. (2016) are the first to explore dynamic time scales for action repetition in the DRL setting and show that it leads to significant improvement in performance on a few Atari 2600 games. However, they choose only two time scales and the experiments are limited to a few representative games. Moreover the method is limited to tasks with a discrete action space. We propose FiGAR, a framework that enables any DRL algorithm regardless of whether its action space is continuous or discrete, to learn temporal abstractions in the form of temporally extended macro-actions. FiGAR uses a structured and factored representation of the policy whereby the policy for choosing the action is decoupled from that for the action repetition selection. Note that deciding actions and the action repetitions independently enables us to find temporal abstractions without blowing up the action space, unlike Vezhnevets et al. (2016) and Lakshminarayanan et al.", "startOffset": 0, "endOffset": 939}, {"referenceID": 9, "context": "Lakshminarayanan et al. (2016) are the first to explore dynamic time scales for action repetition in the DRL setting and show that it leads to significant improvement in performance on a few Atari 2600 games. However, they choose only two time scales and the experiments are limited to a few representative games. Moreover the method is limited to tasks with a discrete action space. We propose FiGAR, a framework that enables any DRL algorithm regardless of whether its action space is continuous or discrete, to learn temporal abstractions in the form of temporally extended macro-actions. FiGAR uses a structured and factored representation of the policy whereby the policy for choosing the action is decoupled from that for the action repetition selection. Note that deciding actions and the action repetitions independently enables us to find temporal abstractions without blowing up the action space, unlike Vezhnevets et al. (2016) and Lakshminarayanan et al. (2016). The contribution of this work is twofold.", "startOffset": 0, "endOffset": 974}, {"referenceID": 9, "context": "Lakshminarayanan et al. (2016) are the first to explore dynamic time scales for action repetition in the DRL setting and show that it leads to significant improvement in performance on a few Atari 2600 games. However, they choose only two time scales and the experiments are limited to a few representative games. Moreover the method is limited to tasks with a discrete action space. We propose FiGAR, a framework that enables any DRL algorithm regardless of whether its action space is continuous or discrete, to learn temporal abstractions in the form of temporally extended macro-actions. FiGAR uses a structured and factored representation of the policy whereby the policy for choosing the action is decoupled from that for the action repetition selection. Note that deciding actions and the action repetitions independently enables us to find temporal abstractions without blowing up the action space, unlike Vezhnevets et al. (2016) and Lakshminarayanan et al. (2016). The contribution of this work is twofold. First, we propose a generic extension to DRL algorithms by coming up with a factored policy representation for temporal abstractions (see figure 1 for sequences of macro actions learnt in 2 Atari 2600 games). Second, we empirically demonstrate FiGAR\u2019s efficiency in improving policy gradient DRL algorithms with improvements in performance over several domains: 31 Atari 2600 games with Asynchronous Advantage Actor Critic [Mnih et al. (2016)], 5 tasks in MuJoCo Simulated physics tasks domain with Trust Region Policy Optimization [Schulman et al.", "startOffset": 0, "endOffset": 1460}, {"referenceID": 9, "context": "Lakshminarayanan et al. (2016) are the first to explore dynamic time scales for action repetition in the DRL setting and show that it leads to significant improvement in performance on a few Atari 2600 games. However, they choose only two time scales and the experiments are limited to a few representative games. Moreover the method is limited to tasks with a discrete action space. We propose FiGAR, a framework that enables any DRL algorithm regardless of whether its action space is continuous or discrete, to learn temporal abstractions in the form of temporally extended macro-actions. FiGAR uses a structured and factored representation of the policy whereby the policy for choosing the action is decoupled from that for the action repetition selection. Note that deciding actions and the action repetitions independently enables us to find temporal abstractions without blowing up the action space, unlike Vezhnevets et al. (2016) and Lakshminarayanan et al. (2016). The contribution of this work is twofold. First, we propose a generic extension to DRL algorithms by coming up with a factored policy representation for temporal abstractions (see figure 1 for sequences of macro actions learnt in 2 Atari 2600 games). Second, we empirically demonstrate FiGAR\u2019s efficiency in improving policy gradient DRL algorithms with improvements in performance over several domains: 31 Atari 2600 games with Asynchronous Advantage Actor Critic [Mnih et al. (2016)], 5 tasks in MuJoCo Simulated physics tasks domain with Trust Region Policy Optimization [Schulman et al. (2015)] and the TORCS domain with Deep Deterministic Policy Gradients [Lillicrap et al.", "startOffset": 0, "endOffset": 1573}, {"referenceID": 9, "context": "Lakshminarayanan et al. (2016) are the first to explore dynamic time scales for action repetition in the DRL setting and show that it leads to significant improvement in performance on a few Atari 2600 games. However, they choose only two time scales and the experiments are limited to a few representative games. Moreover the method is limited to tasks with a discrete action space. We propose FiGAR, a framework that enables any DRL algorithm regardless of whether its action space is continuous or discrete, to learn temporal abstractions in the form of temporally extended macro-actions. FiGAR uses a structured and factored representation of the policy whereby the policy for choosing the action is decoupled from that for the action repetition selection. Note that deciding actions and the action repetitions independently enables us to find temporal abstractions without blowing up the action space, unlike Vezhnevets et al. (2016) and Lakshminarayanan et al. (2016). The contribution of this work is twofold. First, we propose a generic extension to DRL algorithms by coming up with a factored policy representation for temporal abstractions (see figure 1 for sequences of macro actions learnt in 2 Atari 2600 games). Second, we empirically demonstrate FiGAR\u2019s efficiency in improving policy gradient DRL algorithms with improvements in performance over several domains: 31 Atari 2600 games with Asynchronous Advantage Actor Critic [Mnih et al. (2016)], 5 tasks in MuJoCo Simulated physics tasks domain with Trust Region Policy Optimization [Schulman et al. (2015)] and the TORCS domain with Deep Deterministic Policy Gradients [Lillicrap et al. (2015)].", "startOffset": 0, "endOffset": 1661}, {"referenceID": 5, "context": "For instance, Gu et al. (2016) and Satija & Pineau (2016) explore Real Time Neural Machine Translation where the action at every time step is to decide whether to output a new token in the target language or not based on current context.", "startOffset": 14, "endOffset": 31}, {"referenceID": 5, "context": "For instance, Gu et al. (2016) and Satija & Pineau (2016) explore Real Time Neural Machine Translation where the action at every time step is to decide whether to output a new token in the target language or not based on current context.", "startOffset": 14, "endOffset": 58}, {"referenceID": 5, "context": "The Dynamic Frameskip Deep Q-network [Lakshminarayanan et al. (2016)] proposes to use multiple time scales of action repetition by augmenting the Deep Q Network (DQN) [Mnih et al.", "startOffset": 38, "endOffset": 69}, {"referenceID": 5, "context": "The Dynamic Frameskip Deep Q-network [Lakshminarayanan et al. (2016)] proposes to use multiple time scales of action repetition by augmenting the Deep Q Network (DQN) [Mnih et al. (2015)] with separate streams of the same primitive actions corresponding to each time scale.", "startOffset": 38, "endOffset": 187}, {"referenceID": 2, "context": "Durugkar et al. (2016) also explore learning macro-actions composed using the same action repeated for different time scales.", "startOffset": 0, "endOffset": 23}, {"referenceID": 2, "context": "Durugkar et al. (2016) also explore learning macro-actions composed using the same action repeated for different time scales. However, their framework is limited to discrete action spaces and performance improvements are not significant. Learning temporally extended actions and abstractions have been of interest in RL for a long time. Vezhnevets et al. (2016) propose Strategic Attentive Writer (STRAW) for learning macro-actions and building dynamic action-plans directly from reinforcement learning signals.", "startOffset": 0, "endOffset": 362}, {"referenceID": 2, "context": "Durugkar et al. (2016) also explore learning macro-actions composed using the same action repeated for different time scales. However, their framework is limited to discrete action spaces and performance improvements are not significant. Learning temporally extended actions and abstractions have been of interest in RL for a long time. Vezhnevets et al. (2016) propose Strategic Attentive Writer (STRAW) for learning macro-actions and building dynamic action-plans directly from reinforcement learning signals. Instead of outputting a single action after each observation, STRAW maintains a multi-step action plan. The agent periodically updates the plan based on observations and commits to the plan between the replanning steps. Although the STRAW framework represents a more general temporal abstraction than FiGAR, FiGAR should be seen as a framework that can compliment STRAW whereby the decision to repeat could now be hierarchical at plan and base action levels. FiGAR is a framework that has a structured policy representation where the time scale of execution could be thought as parameterizing the chosen action. The only other work that explores parameterized policies in DRL is Hausknecht & Stone (2016) where discrete actions are parameterized by continuous values.", "startOffset": 0, "endOffset": 1217}, {"referenceID": 2, "context": "Durugkar et al. (2016) also explore learning macro-actions composed using the same action repeated for different time scales. However, their framework is limited to discrete action spaces and performance improvements are not significant. Learning temporally extended actions and abstractions have been of interest in RL for a long time. Vezhnevets et al. (2016) propose Strategic Attentive Writer (STRAW) for learning macro-actions and building dynamic action-plans directly from reinforcement learning signals. Instead of outputting a single action after each observation, STRAW maintains a multi-step action plan. The agent periodically updates the plan based on observations and commits to the plan between the replanning steps. Although the STRAW framework represents a more general temporal abstraction than FiGAR, FiGAR should be seen as a framework that can compliment STRAW whereby the decision to repeat could now be hierarchical at plan and base action levels. FiGAR is a framework that has a structured policy representation where the time scale of execution could be thought as parameterizing the chosen action. The only other work that explores parameterized policies in DRL is Hausknecht & Stone (2016) where discrete actions are parameterized by continuous values. In our case, discrete/continuous actions are parameterized by discrete values. The state spaces in Atari are also more sophisticated than the kind explored in Hausknecht et al. (2016). FiGAR is also very naturally connected to the Semi-MDPs (SMDPs) framework.", "startOffset": 0, "endOffset": 1464}, {"referenceID": 2, "context": "The assumption in SMDPs is that actions take some holding time to complete [Duff (1995); Mahadevan et al.", "startOffset": 76, "endOffset": 88}, {"referenceID": 2, "context": "The assumption in SMDPs is that actions take some holding time to complete [Duff (1995); Mahadevan et al. (1997); Dietterich (2000)].", "startOffset": 76, "endOffset": 113}, {"referenceID": 2, "context": "(1997); Dietterich (2000)].", "startOffset": 8, "endOffset": 26}, {"referenceID": 13, "context": "Asynchronous Advantage Actor Critic (A3C) [Mnih et al. (2016)] learns policies based on an asynchronous n-step returns.", "startOffset": 43, "endOffset": 62}, {"referenceID": 17, "context": "TRPO [Schulman et al. (2015)] is a policy optimization algorithm.", "startOffset": 6, "endOffset": 29}, {"referenceID": 10, "context": "According to the Deterministic Policy Gradient (DPG) Theorem [Lever (2014)], the gradient of the performance objective (J) of the deterministic policy (\u03bc) in continuous action spaces with respect to the policy parameters (\u03b8) is given by:", "startOffset": 62, "endOffset": 75}, {"referenceID": 11, "context": "The DDPG algorithm [Lillicrap et al. (2015)] extends the DPG algorithm by introducing non-linear neural network based function approximators for the actor and critic.", "startOffset": 20, "endOffset": 44}, {"referenceID": 9, "context": "If one were to extend the action space in a naive way by coupling the actions and the action repetitions, one would end up suffering the kind of action-space blowup as seen in [Lakshminarayanan et al. (2016); Vezhnevets et al.", "startOffset": 177, "endOffset": 208}, {"referenceID": 9, "context": "If one were to extend the action space in a naive way by coupling the actions and the action repetitions, one would end up suffering the kind of action-space blowup as seen in [Lakshminarayanan et al. (2016); Vezhnevets et al. (2016)] wherein for being able to control with respect to |W | different action repetition levels (or |W |-length policy plans in the case of STRAW) , one would need to model |A|\u00d7 |W | actions or action-values which would blow up the final layer size |W | times.", "startOffset": 177, "endOffset": 234}, {"referenceID": 11, "context": "Similar to DDPG [Lillicrap et al. (2015)], FiGAR-DDPG has no loss function for the actor.", "startOffset": 17, "endOffset": 41}, {"referenceID": 11, "context": "Similar to DDPG [Lillicrap et al. (2015)], FiGAR-DDPG has no loss function for the actor. The actor receives gradients from the critic. This is because the actors proposed policy is directly fed to the critic and the critic provides the actor with gradients which the proposed policy follows for improvement. In FiGAR-DDPG the total policy \u03c0 is a concatenation of vectors \u03c0a and \u03c0x. Hence the gradients for the total policy are also simply the concatenation of the gradients for the policies \u03c0a and \u03c0x. 2. To ensure sufficient exploration, the exploration policy for action repetition is an -greedy version of the behavioral action repetition policy. The action part of the policy, (f\u03b8a(sj)), continues to use temporally correlated noise for exploration, generated by an Ornstein-Uhlenbeck process (see Lillicrap et al. (2015) for details).", "startOffset": 17, "endOffset": 827}], "year": 2017, "abstractText": "Reinforcement Learning algorithms can learn complex behavioral patterns for sequential decision making tasks wherein an agent interacts with an environment and acquires feedback in the form of rewards sampled from it. Traditionally, such algorithms make decisions, i.e., select actions to execute, at every single time step of the agent-environment interactions. In this paper, we propose a novel framework, Fine Grained Action Repetition (FiGAR), which enables the agent to decide the action as well as the time scale of repeating it. FiGAR can be used for improving any Deep Reinforcement Learning algorithm which maintains an explicit policy estimate by enabling temporal abstractions in the action space. We empirically demonstrate the efficacy of our framework by showing performance improvements on top of three policy search algorithms in different domains: Asynchronous Advantage Actor Critic in the Atari 2600 domain, Trust Region Policy Optimization in Mujoco domain and Deep Deterministic Policy Gradients in the TORCS car racing domain.", "creator": "LaTeX with hyperref package"}}}