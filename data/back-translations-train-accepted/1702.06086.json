{"id": "1702.06086", "review": {"conference": "nips", "VERSION": "v1", "DATE_OF_SUBMISSION": "20-Feb-2017", "title": "Label Distribution Learning Forests", "abstract": "Label distribution learning (LDL) is a general learning framework, which assigns a distribution over a set of labels to an instance rather than a single label or multiple labels. Current LDL methods have either restricted assumptions on the expression form of the label distribution or limitations in representation learning. This paper presents label distribution learning forests (LDLFs) - a novel label distribution learning algorithm based on differentiable decision trees, which have several advantages: 1) Decision trees have the potential to model any general form of label distributions by the mixture of leaf node predictions. 2) The learning of differentiable decision trees can be combined with representation learning, e.g., to learn deep features in an end-to-end manner. We define a distribution-based loss function for forests, enabling all the trees to be learned jointly, and show that an update function for leaf node predictions, which guarantees a strict decrease of the loss function, can be derived by variational bounding. The effectiveness of the proposed LDLFs is verified on two LDL problems, including age estimation and crowd opinion prediction on movies, showing significant improvements to the state-of-the-art LDL methods.", "histories": [["v1", "Mon, 20 Feb 2017 18:04:31 GMT  (523kb,D)", "http://arxiv.org/abs/1702.06086v1", "Submitted to IJCAI2017"], ["v2", "Tue, 11 Jul 2017 19:17:32 GMT  (475kb,D)", "http://arxiv.org/abs/1702.06086v2", "Submitted to NIPS2017"], ["v3", "Wed, 20 Sep 2017 06:48:22 GMT  (480kb,D)", "http://arxiv.org/abs/1702.06086v3", "Accepted by NIPS2017"], ["v4", "Mon, 16 Oct 2017 21:05:45 GMT  (469kb,D)", "http://arxiv.org/abs/1702.06086v4", "Accepted by NIPS2017"]], "COMMENTS": "Submitted to IJCAI2017", "reviews": [], "SUBJECTS": "cs.LG cs.CV", "authors": ["wei shen", "kai zhao", "yilu guo", "alan yuille"], "accepted": true, "id": "1702.06086"}, "pdf": {"name": "1702.06086.pdf", "metadata": {"source": "CRF", "title": "Label Distribution Learning Forests", "authors": ["Wei Shen", "Kai Zhao", "Yilu Guo", "Alan Yuille"], "emails": ["shenwei1231@gmail.com", "zhaok1206@gmail.com", "gyl.luan0@gmail.com", "alan.l.yuille@gmail.com"], "sections": [{"heading": "1 Introduction", "text": "In fact, most of them are able to determine for themselves what they want and what they want."}, {"heading": "2 Related Work", "text": "In fact, it is that we see ourselves as being able to assert ourselves, that we are able to assert ourselves, that we are able to put ourselves at the top, and that we are able to assert ourselves, that we are able to assert ourselves, that we are able to assert ourselves, that we are able to put ourselves at the top, \"he said."}, {"heading": "3 Label Distribution Learning Forests", "text": "Forests are an overall ensemble of decision trees. We first introduce how to learn a single decision tree through labeling learning, and then describe how to learn forests."}, {"heading": "3.1 Problem Formulation", "text": "Leave X = Rm the input space and Y = {y1, y2, y2,. \u00b7 Each q function (each q function) represents the complete set of labels, where C is the number of possible label values. We are looking at a label distribution problem (LDL problem) where for each input sample x \u00b7 X a label distribution d = (dy1x, d y2 x,..., d yC x = 1. The aim of the LDL problem is to create a mapping function g: x \u2192 d between an input sample x and its corresponding label distribution d.Here we want to learn the mapping function g (x)."}, {"heading": "3.2 Tree Optimization", "text": "In view of the fact that the two countries are the countries where distributional justice has been achieved, it is not possible to minimize the following distributional justice: R (q) = 1 N (K-L) = 1 C (K-L) = 1 C (K-L) = 1 C (K-L) = 1 C (K-L) = 1 C (S) = 1 C (S) = 1 C (S) = 1 C (S) = 1 C (S) = 1 C (S) = 1 C (S) = 1 C) = 1 C (K) = 1 C (K) = 1 C)."}, {"heading": "3.3 Learning Forests", "text": "Forests are a total ensemble of decision trees F = {T1,.., TK}. In the training phase of our LDLFs, all trees have the same parameters, but each tree has independent leaf node forecasts q. The loss function for forests is given by averaging the loss functions for all individual trees: RF = 1K \u2211 k = 1 RTk, (14) where RTk is the loss function for tree Tk defined by Eqn. 3. To learn by determining the leaf node forecasts q of all trees in the forests, based on the derivation in Sec. 3.2 and with reference to Fig. 2, we have the RF results from the Eqn phase."}, {"heading": "4 Experimental Results", "text": "Our implementation of LDLFs is based on \"Caffe\" [Jia et al., 2014], which is modular and implemented as a standard neural network layer. We can either use it as a shallow standalone model (sLDLFs) or integrate it into arbitrary deep networks (dLDLFs). We evaluate both sLDLFs and dLDLFs on different LDL tasks and compare them with other LDL methods. Default settings for the parameters of our forests are: tree number (5), tree depth (7), output unit number of the feature learning function (64), iteration times for updating leaf node predictions (20)."}, {"heading": "4.1 Crowd Opinion Prediction on Movies", "text": "To accomplish this task, we use a dataset [Geng and Hou, 2015], which has 7,755 movies and 54,242,292 reviews from 478,656 different users. Ratings come from Netflix, which is located on a scale from 1 to 5 integral stars. Each movie has an average of 6,994 ratings. The distribution of ratings is calculated for each movie as an indicator of the audience's opinion on this movie. The characteristics of each film are numerical and categorical attributes provided by IMDB. We evaluate our flat model sLDLFs on this dataset and compare it with other state of the art stand-alone LDL methods, including AOSO-LDLogitBoost [Xing et al., 2016], LDLogitBoost et al al al al al al al al al al al al al., 2016], LDSVR [Geng and Hou, 2015], BFGS-LDL [Geng, 2016] and IIS-LDL Genal [2013]."}, {"heading": "4.2 Facial Age Estimation", "text": "We are conducting facial estimation experiments on Morph [Ricanek and Tesafaye, 2006], which contain more than 50,000 facial images of approximately 13,000 people of different races. As Morph provides raw facial images, we are also evaluating our deep model dLDLF's, which can benefit from end-to-end learning. Each facial image is commented on with a chronological age. As the distribution of gender and ethnic origin in Morph, which consists of 20,160 selected facial images to avoid the influence of an unbalanced facial distribution, we will evaluate 2017 on a subset of Morph, called Morph Sub. To generate an age distribution for each facial image, we are following the same strategy used in [Geng et al., 2010; Yang et al., 2016], which uses a Gaussian distribution."}, {"heading": "4.3 Parameter Discussion", "text": "Now we will discuss the influence of parameter settings on performance. We report on the results of the opinion forecast for films (measured by K-L) and the age estimate for Morph Sub with a training rate of 60% (measured by MAE) for various parameter settings in this section. Since forests are an ensemble model, it is necessary to investigate how performance differs from ours by varying the number of trees used in forests. Note that, as we discussed in Section 2, the ensemble strategy for learning forests proposed in dNDFs [Kontschieder et al., 2015] is also necessary to make a diagnosis to see which ensemble strategy is better for learning forests. We replace our ensemble strategy in dLDLFs with that proposed in dNDFs [Kontschieder et al., 2015] and rename this method dNDFs-LDL. The corresponding flat model is called sDFs-LDL."}, {"heading": "5 Conclusion", "text": "We present learning forests for label distribution, a novel learning algorithm for label distribution inspired by differentiated decision trees. We defined a distribution-based loss function for forests and found that leaf node predictions can be optimized by variation boundaries, allowing all trees and the features they use to be learned end-to-end together. Experimental results showed the superiority of our algorithm in several LDL problems, including age estimation and opinion prediction in movies."}], "references": [{"title": "Neural Computation", "author": ["Yali Amit", "Donald Geman. Shape quantization", "recognition with randomized trees"], "venue": "9(7):1545\u20131588,", "citeRegEx": "Amit and Geman. 1997", "shortCiteRegEx": null, "year": 1997}, {"title": "Computational Linguistics", "author": ["Adam L. Berger", "Stephen Della Pietra", "Vincent J. Della Pietra. A maximum entropy approach to natural language processing"], "venue": "22(1):39\u201371,", "citeRegEx": "Berger et al.. 1996", "shortCiteRegEx": null, "year": 1996}, {"title": "Machine Learning", "author": ["Leo Breiman. Random forests"], "venue": "45(1):5\u201332,", "citeRegEx": "Breiman. 2001", "shortCiteRegEx": null, "year": 2001}, {"title": "Decision Forests for Computer Vision and Medical Image Analysis", "author": ["Antonio Criminisi", "Jamie Shotton"], "venue": "Springer,", "citeRegEx": "Criminisi and Shotton. 2013", "shortCiteRegEx": null, "year": 2013}, {"title": "IJCAI", "author": ["Xin Geng", "Peng Hou. Pre-release prediction of crowd opinion on movies by label distribution learning. In Pro"], "venue": "pages 3511\u20133517,", "citeRegEx": "Geng and Hou. 2015", "shortCiteRegEx": null, "year": 2015}, {"title": "CVPR", "author": ["Xin Geng", "Yu Xia. Head pose estimation based on multivariate label distribution. In Proc"], "venue": "pages 1837\u20131842,", "citeRegEx": "Geng and Xia. 2014", "shortCiteRegEx": null, "year": 2014}, {"title": "In Proc", "author": ["Xin Geng", "Kate Smith-Miles", "ZhiHua Zhou. Facial age estimation by learning from label distributions"], "venue": "AAAI,", "citeRegEx": "Geng et al.. 2010", "shortCiteRegEx": null, "year": 2010}, {"title": "Mach", "author": ["Xin Geng", "Chao Yin", "Zhi-Hua Zhou. Facial age estimation by learning from label distributions. IEEE Trans. Pattern Anal"], "venue": "Intell., 35(10):2401\u2013 2412,", "citeRegEx": "Geng et al.. 2013", "shortCiteRegEx": null, "year": 2013}, {"title": "ICPR", "author": ["Xin Geng", "Qin Wang", "Yu Xia. Facial age estimation by adaptive label distribution learning. In Proc"], "venue": "pages 4465\u20134470,", "citeRegEx": "Geng et al.. 2014", "shortCiteRegEx": null, "year": 2014}, {"title": "Knowl", "author": ["Xin Geng. Label distribution learning. IEEE Trans"], "venue": "Data Eng., 28(7):1734\u20131748,", "citeRegEx": "Geng. 2016", "shortCiteRegEx": null, "year": 2016}, {"title": "Human age estimation: What is the influence across race and gender? In CVPR Workshops", "author": ["Guodong Guo", "Guowang Mu"], "venue": "pages 71\u201378,", "citeRegEx": "Guo and Mu. 2010", "shortCiteRegEx": null, "year": 2010}, {"title": "CVPR", "author": ["Guodong Guo", "Chao Zhang. A study on cross-population age estimation. In Proc"], "venue": "pages 4257\u20134263,", "citeRegEx": "Guo and Zhang. 2014", "shortCiteRegEx": null, "year": 2014}, {"title": "Image Processing", "author": ["Guodong Guo", "Yun Fu", "Charles R. Dyer", "Thomas S. Huang. Image-based human age estimation by manifold learning", "locally adjusted robust regression. IEEE Trans"], "venue": "17(7):1178\u20131188,", "citeRegEx": "Guo et al.. 2008", "shortCiteRegEx": null, "year": 2008}, {"title": "IEEE Trans", "author": ["Zhouzhou He", "Xi Li", "Zhongfei Zhang", "Fei Wu", "Xin Geng", "Yaqing Zhang", "Ming-Hsuan Yang", "Yueting Zhuang. Data-dependent label distribution learning for age estimation"], "venue": "on Image Processing,", "citeRegEx": "He et al.. 2017", "shortCiteRegEx": null, "year": 2017}, {"title": "ICDAR", "author": ["Tin Kam Ho. Random decision forests. In Proc"], "venue": "pages 278\u2013282,", "citeRegEx": "Ho. 1995", "shortCiteRegEx": null, "year": 1995}, {"title": "Mach", "author": ["Tin Kam Ho. The random subspace method for constructing decision forests. IEEE Trans. Pattern Anal"], "venue": "Intell., 20(8):832\u2013844,", "citeRegEx": "Ho. 1998", "shortCiteRegEx": null, "year": 1998}, {"title": "In CVPR Workshops", "author": ["Zeng-Wei Huo", "Xu Yang", "Chao Xing", "Ying Zhou", "Peng Hou", "Jiaqi Lv", "Xin Geng. Deep age distribution learning for apparent age estimation"], "venue": "pages 722\u2013729,", "citeRegEx": "Huo et al.. 2016", "shortCiteRegEx": null, "year": 2016}, {"title": "Caffe: Convolutional architecture for fast feature embedding", "author": ["Yangqing Jia", "Evan Shelhamer", "Jeff Donahue", "Sergey Karayev", "Jonathan Long", "Ross Girshick", "Sergio Guadarrama", "Trevor Darrell"], "venue": "arXiv preprint arXiv:1408.5093,", "citeRegEx": "Jia et al.. 2014", "shortCiteRegEx": null, "year": 2014}, {"title": "Machine Learning", "author": ["Michael I. Jordan", "Zoubin Ghahramani", "Tommi S. Jaakkola", "Lawrence K. Saul. An introduction to variational methods for graphical models"], "venue": "37(2):183\u2013233,", "citeRegEx": "Jordan et al.. 1999", "shortCiteRegEx": null, "year": 1999}, {"title": "ICCV", "author": ["Peter Kontschieder", "Madalina Fiterau", "Antonio Criminisi", "Samuel Rota Bul\u00f2. Deep neural decision forests. In Proc"], "venue": "pages 1467\u20131475,", "citeRegEx": "Kontschieder et al.. 2015", "shortCiteRegEx": null, "year": 2015}, {"title": "NIPS", "author": ["Alex Krizhevsky", "Ilya Sutskever", "Geoffrey E. Hinton. Imagenet classification with deep convolutional neural networks. In Proc"], "venue": "pages 1106\u20131114,", "citeRegEx": "Krizhevsky et al.. 2012", "shortCiteRegEx": null, "year": 2012}, {"title": "IEEE Trans", "author": ["Andreas Lanitis", "Chrisina Draganova", "Chris Christodoulou. Comparing different classifiers for automatic age estimation"], "venue": "on Cybernetics,, 34(1):621\u2013628,", "citeRegEx": "Lanitis et al.. 2004", "shortCiteRegEx": null, "year": 2004}, {"title": "CVPR", "author": ["Zhenxing Niu", "Mo Zhou", "Le Wang", "Xinbo Gao", "Gang Hua. Ordinal regression with multiple output CNN for age estimation. In Proc"], "venue": "pages 4920\u2013 4928,", "citeRegEx": "Niu et al.. 2016", "shortCiteRegEx": null, "year": 2016}, {"title": "MORPH: A longitudinal image database of normal adult age-progression", "author": ["Karl Ricanek", "Tamirat Tesafaye"], "venue": "Proc. FG, pages 341\u2013345,", "citeRegEx": "Ricanek and Tesafaye. 2006", "shortCiteRegEx": null, "year": 2006}, {"title": "Multi-label classification: An overview", "author": ["Grigorios Tsoumakas", "Ioannis Katakis"], "venue": "International Journal of Data Warehousing and Mining, 3(3):1\u201313,", "citeRegEx": "Tsoumakas and Katakis. 2007", "shortCiteRegEx": null, "year": 2007}, {"title": "CVPR", "author": ["Chao Xing", "Xin Geng", "Hui Xue. Logistic boosting regression for label distribution learning. In Proc"], "venue": "pages 4489\u20134497,", "citeRegEx": "Xing et al.. 2016", "shortCiteRegEx": null, "year": 2016}, {"title": "In ICCV Workshops", "author": ["Xu Yang", "Bin-Bin Gao", "Chao Xing", "Zeng-Wei Huo", "Xiu-Shen Wei", "Ying Zhou", "Jianxin Wu", "Xin Geng. Deep label distribution learning for apparent age estimation"], "venue": "pages 344\u2013350,", "citeRegEx": "Yang et al.. 2015", "shortCiteRegEx": null, "year": 2015}, {"title": "IJCAI", "author": ["Xu Yang", "Xin Geng", "Deyu Zhou. Sparsity conditional energy label distribution learning for age estimation. In Proc"], "venue": "pages 2259\u20132265,", "citeRegEx": "Yang et al.. 2016", "shortCiteRegEx": null, "year": 2016}, {"title": "Neural Computation", "author": ["Alan L. Yuille", "Anand Rangarajan. The concave-convex procedure"], "venue": "15(4):915\u2013936,", "citeRegEx": "Yuille and Rangarajan. 2003", "shortCiteRegEx": null, "year": 2003}, {"title": "MM", "author": ["Ying Zhou", "Hui Xue", "Xin Geng. Emotion distribution recognition from facial expressions. In Proc"], "venue": "pages 1247\u20131250,", "citeRegEx": "Zhou et al.. 2015", "shortCiteRegEx": null, "year": 2015}], "referenceMentions": [{"referenceID": 9, "context": "Label distribution learning (LDL) [Geng, 2016; Geng et al., 2013] is a learning strategy to deal with problems of label ambiguity.", "startOffset": 34, "endOffset": 65}, {"referenceID": 7, "context": "Label distribution learning (LDL) [Geng, 2016; Geng et al., 2013] is a learning strategy to deal with problems of label ambiguity.", "startOffset": 34, "endOffset": 65}, {"referenceID": 24, "context": "Unlike single-label learning (SLL) and multi-label learning (MLL) [Tsoumakas and Katakis, 2007], which assume an instance is assigned to a single label or multiple labels, LDL aims at learning the relative importance of each label involved in the description of an instance, i.", "startOffset": 66, "endOffset": 95}, {"referenceID": 6, "context": "An example is facial age estimation [Geng et al., 2010].", "startOffset": 36, "endOffset": 55}, {"referenceID": 4, "context": "Another example is predicting crowd opinion on movies [Geng and Hou, 2015].", "startOffset": 54, "endOffset": 74}, {"referenceID": 1, "context": "Many LDL methods assume the label distribution can be represented by a maximum entropy model [Berger et al., 1996] and learn it by optimizing an energy function based on the model [Geng et al.", "startOffset": 93, "endOffset": 114}, {"referenceID": 6, "context": ", 1996] and learn it by optimizing an energy function based on the model [Geng et al., 2010; Geng et al., 2013; Yang et al., 2016; Geng, 2016].", "startOffset": 73, "endOffset": 142}, {"referenceID": 7, "context": ", 1996] and learn it by optimizing an energy function based on the model [Geng et al., 2010; Geng et al., 2013; Yang et al., 2016; Geng, 2016].", "startOffset": 73, "endOffset": 142}, {"referenceID": 27, "context": ", 1996] and learn it by optimizing an energy function based on the model [Geng et al., 2010; Geng et al., 2013; Yang et al., 2016; Geng, 2016].", "startOffset": 73, "endOffset": 142}, {"referenceID": 9, "context": ", 1996] and learn it by optimizing an energy function based on the model [Geng et al., 2010; Geng et al., 2013; Yang et al., 2016; Geng, 2016].", "startOffset": 73, "endOffset": 142}, {"referenceID": 4, "context": "g, by boosting and support vector regression, to deal with label distributions [Geng and Hou, 2015; Xing et al., 2016], which avoid making this assumption, but have limitations in representation learning, e.", "startOffset": 79, "endOffset": 118}, {"referenceID": 25, "context": "g, by boosting and support vector regression, to deal with label distributions [Geng and Hou, 2015; Xing et al., 2016], which avoid making this assumption, but have limitations in representation learning, e.", "startOffset": 79, "endOffset": 118}, {"referenceID": 19, "context": "In this paper, we present label distribution learning forests (LDLFs) - a novel label distribution learning algorithm inspired by differentiable decision trees [Kontschieder et al., 2015].", "startOffset": 160, "endOffset": 187}, {"referenceID": 18, "context": "By fixing split nodes, we show that the optimization of leaf node predictions to minimize the loss function of the tree can be addressed by variational bounding [Jordan et al., 1999; Yuille and Rangarajan, 2003], in which the original loss function to be minimized gets iteratively replaced by upper bounds.", "startOffset": 161, "endOffset": 211}, {"referenceID": 28, "context": "By fixing split nodes, we show that the optimization of leaf node predictions to minimize the loss function of the tree can be addressed by variational bounding [Jordan et al., 1999; Yuille and Rangarajan, 2003], in which the original loss function to be minimized gets iteratively replaced by upper bounds.", "startOffset": 161, "endOffset": 211}, {"referenceID": 14, "context": "Random forests or randomized decision trees [Ho, 1995; Amit and Geman, 1997; Breiman, 2001; Criminisi and Shotton, 2013], are a popular ensemble predictive model suitable for many machine learning tasks.", "startOffset": 44, "endOffset": 120}, {"referenceID": 0, "context": "Random forests or randomized decision trees [Ho, 1995; Amit and Geman, 1997; Breiman, 2001; Criminisi and Shotton, 2013], are a popular ensemble predictive model suitable for many machine learning tasks.", "startOffset": 44, "endOffset": 120}, {"referenceID": 2, "context": "Random forests or randomized decision trees [Ho, 1995; Amit and Geman, 1997; Breiman, 2001; Criminisi and Shotton, 2013], are a popular ensemble predictive model suitable for many machine learning tasks.", "startOffset": 44, "endOffset": 120}, {"referenceID": 3, "context": "Random forests or randomized decision trees [Ho, 1995; Amit and Geman, 1997; Breiman, 2001; Criminisi and Shotton, 2013], are a popular ensemble predictive model suitable for many machine learning tasks.", "startOffset": 44, "endOffset": 120}, {"referenceID": 0, "context": "In the past, learning of a decision tree was based on heuristics such as a greedy algorithm where locally-optimal hard decisions are made at each split node [Amit and Geman, 1997], and thus, cannot be integrated into in a deep learning framework, i.", "startOffset": 157, "endOffset": 179}, {"referenceID": 19, "context": "The newly proposed deep neural decision forests (dNDFs) [Kontschieder et al., 2015] overcomes this problem by introducing a soft differentiable decision function at the split nodes and a global loss function defined on a tree.", "startOffset": 56, "endOffset": 83}, {"referenceID": 6, "context": "A number of specialized algorithms have been proposed to address the LDL task, and have shown their effectiveness in many real applications, such as facial age estimation [Geng et al., 2010; Geng et al., 2013; Yang et al., 2016; Huo et al., 2016], crowd opinion prediction on movies [Geng and Hou, 2015], expression recognition [Zhou et al.", "startOffset": 171, "endOffset": 246}, {"referenceID": 7, "context": "A number of specialized algorithms have been proposed to address the LDL task, and have shown their effectiveness in many real applications, such as facial age estimation [Geng et al., 2010; Geng et al., 2013; Yang et al., 2016; Huo et al., 2016], crowd opinion prediction on movies [Geng and Hou, 2015], expression recognition [Zhou et al.", "startOffset": 171, "endOffset": 246}, {"referenceID": 27, "context": "A number of specialized algorithms have been proposed to address the LDL task, and have shown their effectiveness in many real applications, such as facial age estimation [Geng et al., 2010; Geng et al., 2013; Yang et al., 2016; Huo et al., 2016], crowd opinion prediction on movies [Geng and Hou, 2015], expression recognition [Zhou et al.", "startOffset": 171, "endOffset": 246}, {"referenceID": 16, "context": "A number of specialized algorithms have been proposed to address the LDL task, and have shown their effectiveness in many real applications, such as facial age estimation [Geng et al., 2010; Geng et al., 2013; Yang et al., 2016; Huo et al., 2016], crowd opinion prediction on movies [Geng and Hou, 2015], expression recognition [Zhou et al.", "startOffset": 171, "endOffset": 246}, {"referenceID": 4, "context": ", 2016], crowd opinion prediction on movies [Geng and Hou, 2015], expression recognition [Zhou et al.", "startOffset": 44, "endOffset": 64}, {"referenceID": 29, "context": ", 2016], crowd opinion prediction on movies [Geng and Hou, 2015], expression recognition [Zhou et al., 2015] and hand orientation estimation [Geng and Xia, 2014].", "startOffset": 89, "endOffset": 108}, {"referenceID": 5, "context": ", 2015] and hand orientation estimation [Geng and Xia, 2014].", "startOffset": 40, "endOffset": 60}, {"referenceID": 6, "context": "[Geng et al., 2010] defined the label distribution for an instance as a vector containing the probabilities of the instance having each label.", "startOffset": 0, "endOffset": 19}, {"referenceID": 27, "context": "[Yang et al., 2016] then defined a three-layer energy based model, called SCE-LDL, in which the ability to perform representation learning is improved by adding the extra hidden layer and sparsity constraints are also incorporated to ameliorate the model.", "startOffset": 0, "endOffset": 19}, {"referenceID": 9, "context": "Geng [Geng, 2016] developed an accelerated version of IIS-LLD, called BFGS-LDL, by using quasi-Newton optimization.", "startOffset": 5, "endOffset": 17}, {"referenceID": 1, "context": "label distribution can be represented by a maximum entropy model [Berger et al., 1996].", "startOffset": 65, "endOffset": 86}, {"referenceID": 4, "context": "Geng and Hou [Geng and Hou, 2015] proposed LDSVR, a LDL method by extending support vector regressor, which fit a sigmoid function to each component of the distribution simultaneously by a support vector machine.", "startOffset": 13, "endOffset": 33}, {"referenceID": 25, "context": "[Xing et al., 2016] then extended boosting to address the LDL task by additive weighted regressors.", "startOffset": 0, "endOffset": 19}, {"referenceID": 26, "context": "There are two deep learning based LDL methods [Yang et al., 2015; Huo et al., 2016], but both of them focused on how to boost the performance by combing multiple deep models trained on different datasets.", "startOffset": 46, "endOffset": 83}, {"referenceID": 16, "context": "There are two deep learning based LDL methods [Yang et al., 2015; Huo et al., 2016], but both of them focused on how to boost the performance by combing multiple deep models trained on different datasets.", "startOffset": 46, "endOffset": 83}, {"referenceID": 19, "context": "To build a differentiable decision tree, following [Kontschieder et al., 2015], we use a probabilistic split function sn(x; \u0398) = \u03c3(f\u03c6(n)(x; \u0398)), where \u03c3(\u00b7) is a sigmoid function, \u03c6(\u00b7) is an index function to bring the \u03c6(n)-th output of function f(x; \u0398) in correspondence with split node n, and f : x\u2192 R is a real-valued feature learning function depending on the sample x and the parameter \u0398, and can be with any form.", "startOffset": 51, "endOffset": 78}, {"referenceID": 18, "context": "Here, we propose to address this optimization problem by variational bounding [Jordan et al., 1999; Yuille and Rangarajan, 2003].", "startOffset": 78, "endOffset": 128}, {"referenceID": 28, "context": "Here, we propose to address this optimization problem by variational bounding [Jordan et al., 1999; Yuille and Rangarajan, 2003].", "startOffset": 78, "endOffset": 128}, {"referenceID": 15, "context": "This strategy is somewhat similar to the random subspace method [Ho, 1998], which increases the randomness in training to reduce the risk of overfitting.", "startOffset": 64, "endOffset": 74}, {"referenceID": 17, "context": "Our realization of LDLFs is based on \u201cCaffe\u201d [Jia et al., 2014], which is modular and implemented as a standard neural network layer.", "startOffset": 45, "endOffset": 63}, {"referenceID": 4, "context": "To perform this task, we use a dataset [Geng and Hou, 2015], which has 7755 movies and 54,242,292 ratings from 478,656 different users.", "startOffset": 39, "endOffset": 59}, {"referenceID": 25, "context": "We evaluate our shallow model sLDLFs on this dataset and compare it with other state-of-the-art stand-alone LDL methods, including AOSO-LDLogitBoost [Xing et al., 2016], LDLogitBoost [Xing et al.", "startOffset": 149, "endOffset": 168}, {"referenceID": 25, "context": ", 2016], LDLogitBoost [Xing et al., 2016], LDSVR [Geng and Hou, 2015], BFGS-LDL [Geng, 2016] and IIS-LDL [Geng et al.", "startOffset": 22, "endOffset": 41}, {"referenceID": 4, "context": ", 2016], LDSVR [Geng and Hou, 2015], BFGS-LDL [Geng, 2016] and IIS-LDL [Geng et al.", "startOffset": 15, "endOffset": 35}, {"referenceID": 9, "context": ", 2016], LDSVR [Geng and Hou, 2015], BFGS-LDL [Geng, 2016] and IIS-LDL [Geng et al.", "startOffset": 46, "endOffset": 58}, {"referenceID": 7, "context": ", 2016], LDSVR [Geng and Hou, 2015], BFGS-LDL [Geng, 2016] and IIS-LDL [Geng et al., 2013].", "startOffset": 71, "endOffset": 90}, {"referenceID": 4, "context": "Following [Geng and Hou, 2015; Xing et al., 2016], we use 6 measures to evaluate the performances of LDL methods, which compute the average similarity/distance between the predicted rating distributions and the real rating distributions, including 4 distance measures (K-L, Euclidean, S\u03c6rensen, Squared \u03c7) and two similarity measures (Fidelity, Intersection).", "startOffset": 10, "endOffset": 49}, {"referenceID": 25, "context": "Following [Geng and Hou, 2015; Xing et al., 2016], we use 6 measures to evaluate the performances of LDL methods, which compute the average similarity/distance between the predicted rating distributions and the real rating distributions, including 4 distance measures (K-L, Euclidean, S\u03c6rensen, Squared \u03c7) and two similarity measures (Fidelity, Intersection).", "startOffset": 10, "endOffset": 49}, {"referenceID": 25, "context": "Following [Xing et al., 2016], we also do ten-fold cross validation, which represents the result by \u201cmean\u00b1standard deviation\u201d.", "startOffset": 10, "endOffset": 29}, {"referenceID": 25, "context": "The results of sLDLFs and the competitors are summarized in Table 1, where the results of these competitors are quoted from [Xing et al., 2016].", "startOffset": 124, "endOffset": 143}, {"referenceID": 23, "context": "We conduct facial age estimation experiments on Morph [Ricanek and Tesafaye, 2006], which contains more than 50,000 facial images from about 13,000 people of different races.", "startOffset": 54, "endOffset": 82}, {"referenceID": 10, "context": "As the distribution of gender and ethnicity is very unbalanced in Morph, many age estimation methods [Guo and Mu, 2010; Guo and Zhang, 2014; He et al., 2017] are evaluated on a subset of Morph, called Morph Sub for short, which consists of 20,160 selected facial images to avoid the influence of unbalanced distribution.", "startOffset": 101, "endOffset": 157}, {"referenceID": 11, "context": "As the distribution of gender and ethnicity is very unbalanced in Morph, many age estimation methods [Guo and Mu, 2010; Guo and Zhang, 2014; He et al., 2017] are evaluated on a subset of Morph, called Morph Sub for short, which consists of 20,160 selected facial images to avoid the influence of unbalanced distribution.", "startOffset": 101, "endOffset": 157}, {"referenceID": 13, "context": "As the distribution of gender and ethnicity is very unbalanced in Morph, many age estimation methods [Guo and Mu, 2010; Guo and Zhang, 2014; He et al., 2017] are evaluated on a subset of Morph, called Morph Sub for short, which consists of 20,160 selected facial images to avoid the influence of unbalanced distribution.", "startOffset": 101, "endOffset": 157}, {"referenceID": 6, "context": "To generate an age distribution for each face image, we follow the same strategy used in [Geng et al., 2010; Yang et al., 2016], which uses a Gaussian distribution whose mean is the chronological age of the face image.", "startOffset": 89, "endOffset": 127}, {"referenceID": 27, "context": "To generate an age distribution for each face image, we follow the same strategy used in [Geng et al., 2010; Yang et al., 2016], which uses a Gaussian distribution whose mean is the chronological age of the face image.", "startOffset": 89, "endOffset": 127}, {"referenceID": 13, "context": "As far as we know, the state-of-the-art LDL method on Morph Sub is D2LDL [He et al., 2017], which proposed a data-dependent method to learn label distributions.", "startOffset": 73, "endOffset": 90}, {"referenceID": 20, "context": "As they used the output of the \u201cfc7\u201d layer in AlexNet [Krizhevsky et al., 2012] as the face image features, we also build our dLDLFs on AlexNet, by replacing the softmax layer in AlexNet by our LDLFs.", "startOffset": 54, "endOffset": 79}, {"referenceID": 21, "context": "Besides D2LDL, other competitors are AAS [Lanitis et al., 2004], LARR [Guo et al.", "startOffset": 41, "endOffset": 63}, {"referenceID": 12, "context": ", 2004], LARR [Guo et al., 2008], IIS-LLD [Geng et al.", "startOffset": 14, "endOffset": 32}, {"referenceID": 7, "context": ", 2008], IIS-LLD [Geng et al., 2013], IIS-ALDL [Geng et al.", "startOffset": 17, "endOffset": 36}, {"referenceID": 8, "context": ", 2013], IIS-ALDL [Geng et al., 2014], including both SLL and LDL based methods.", "startOffset": 18, "endOffset": 37}, {"referenceID": 16, "context": "This configuration was also adopted in [Huo et al., 2016] to finetune different deep networks for age estimation.", "startOffset": 39, "endOffset": 57}, {"referenceID": 13, "context": "We quote the results of the competitors (except for dLDLME) from [He et al., 2017], and compare our results with them in Table.", "startOffset": 65, "endOffset": 82}, {"referenceID": 22, "context": "We also integrate it with another network, called OR-CNN, proposed in [Niu et al., 2016], which obtains the state-of-the-art result on the whole Morph dataset.", "startOffset": 70, "endOffset": 88}, {"referenceID": 22, "context": "Following [Niu et al., 2016], all these methods are trained on 80% of the whole Morph dataset and tested on the rest.", "startOffset": 10, "endOffset": 28}, {"referenceID": 19, "context": "2, the ensemble strategy to learn forests proposed in dNDFs [Kontschieder et al., 2015] is different from ours.", "startOffset": 60, "endOffset": 87}], "year": 2017, "abstractText": "Label distribution learning (LDL) is a general learning framework, which assigns a distribution over a set of labels to an instance rather than a single label or multiple labels. Current LDL methods have either restricted assumptions on the expression form of the label distribution or limitations in representation learning. This paper presents label distribution learning forests (LDLFs) a novel label distribution learning algorithm based on differentiable decision trees, which have several advantages: 1) Decision trees have the potential to model any general form of label distributions by the mixture of leaf node predictions. 2) The learning of differentiable decision trees can be combined with representation learning, e.g., to learn deep features in an end-to-end manner. We define a distributionbased loss function for forests, enabling all the trees to be learned jointly, and show that an update function for leaf node predictions, which guarantees a strict decrease of the loss function, can be derived by variational bounding. The effectiveness of the proposed LDLFs is verified on two LDL problems, including age estimation and crowd opinion prediction on movies, showing significant improvements to the state-of-the-art LDL methods.", "creator": "LaTeX with hyperref package"}}}