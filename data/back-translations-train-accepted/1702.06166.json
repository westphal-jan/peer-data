{"id": "1702.06166", "review": {"conference": "ICML", "VERSION": "v1", "DATE_OF_SUBMISSION": "20-Feb-2017", "title": "Bayesian Boolean Matrix Factorisation", "abstract": "Boolean matrix factorisation (BooMF) infers interpretable decompositions of a binary data matrix into a pair of low-rank, binary matrices: One containing meaningful patterns, the other quantifying how the observations can be expressed as a combination of these patterns. We introduce the OrMachine, a probabilistic generative model for BooMF and derive a Metropolised Gibbs sampler that facilitates very efficient parallel posterior inference. Our method outperforms all currently existing approaches for Boolean Matrix factorization and completion, as we show on simulated and real world data. This is the first method to provide full posterior inference for BooMF which is relevant in applications, e.g. for controlling false positive rates in collaborative filtering, and crucially it improves the interpretability of the inferred patterns. The proposed algorithm scales to large datasets as we demonstrate by analysing single cell gene expression data in 1.3 million mouse brain cells across 11,000 genes on commodity hardware.", "histories": [["v1", "Mon, 20 Feb 2017 20:31:39 GMT  (355kb,D)", "http://arxiv.org/abs/1702.06166v1", null], ["v2", "Sat, 25 Feb 2017 14:17:44 GMT  (414kb,D)", "http://arxiv.org/abs/1702.06166v2", null]], "reviews": [], "SUBJECTS": "stat.ML cs.LG cs.NA q-bio.GN q-bio.QM stat.ME", "authors": ["tammo rukat", "christopher c holmes", "michalis k titsias", "christopher yau"], "accepted": true, "id": "1702.06166"}, "pdf": {"name": "1702.06166.pdf", "metadata": {"source": "CRF", "title": "Bayesian Boolean Matrix Factorisation", "authors": ["Tammo Rukat", "Chris C. Holmes", "Michalis K. Titsias", "Christopher Yau"], "emails": ["tammo.rukat@stats.ox.ac.uk", "cholmes@stats.ox.ac.uk", "mtitsias@aueb.gr", "c.yau@bham.ac.uk"], "sections": [{"heading": "1 Introduction", "text": "The Boolean Matrix Factorization (BooMF) aims to split a binary data matrix into an approximate Boolean product of two low-ranking matrices. (1) The Boolean product is a special case of a matrix product between binary matrices in which all values greater than zero are set to one. (1) The Boolean product is a special case of a matrix product between binary matrices in which all values greater than zero are set to one. (1) The tammo.rukat @ stats.ox.ac.uk \u2020 cholmes @ stats.ox.ac.uk The mtitsias @ aueb.gr \u00a7 c.yau @ bham.ukar Xiv: 170 2.06 166v 1 [stat.ML] 2 0BooMF provides a framework for learning from binary data in which the derived codes U-Indicators provide a basis and the indicator variable Z-Codes."}, {"heading": "2 Related Work", "text": "The Discrete Basis Problem (Miettinen et al., 2006) provides a greedy heuristic algorithm for solving BooMF without recourse to an underlying probabilistic model. It is based on association rule mining (Agrawal et al., 1994) and has been expanded more recently to automatically select the optimal dimensionality of latent space based on the principle of minimum description length (Miettinen & Vreeken, 2014). In contrast, multi-assignment clustering for Boolean data (Streich et al., 2009) uses a probabilistic model for BooMF and adds another global source of noise to the generative process. Point estimates are inferred by deterministic glow. Similarly, Wood et al. (2012) develop a probabilistic model to determine hidden causes. Unlike Boolean OR, the probability of observation increases with the number of active hidden codes. The Indian buffet process does not serve as an indigenous process."}, {"heading": "3 The OrMachine", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "3.1 Model Formulation", "text": "The OrMachine is a probabilistic generative model for Boolean matrix factorization. A matrix of N binary observations is merely a discrete mixture of L binary codes ul- {0, 1} D. Binary latent variables znl denote whether code l is used in generating a particular observation or not. The probability that this observation is one is greater than 1 / 2. The exact magnitude of this probability is derived from the data and, for later notational convenience, is parameterized if there is no dimension in which codes and latent variables are both equal, the probability that the observation is one."}, {"heading": "3.2 Fast Posterior Inference", "text": "The complete common distribution of all data and random variables is of p (X, U, Z, \u03bb) = p (X, Z, \u03bb) p (U) p (Z). (4) The complete condition for znl (and analogous for uld) is: p (znl | rest) = p (2). (5) Notice that the independent Bernoulli before entering the expression as an additional term within the sigmoid function. This term disappears for the uninformative Bernoulli before p (z) = 1 / 2. The form of eq. (5) allows a computer-efficient evaluation of the conditions. The underlying principle is that once certain conditions are met, the result of the complete condition is known without taking into account the rest of a variable condition."}, {"heading": "3.3 Dealing with Missing Data", "text": "We can deal with unobserved data by marginalizing the probability of missing observations. Specifically, if X = (Xobs, Xmis) is the decomposition of the full matrix into the observed part Xobs and the missing part Xmis, after marginalization the original probability p (X | U, Z, \u03bb) of eq. (4) simplifies to p (Xobs | U, Z, \u03bb). Then a standard implementation can be based on indexing the observed components within matrix X and modifying the inference method so that the posterior conditions of znl and uld only include sums of observed elements in rows and columns of X. A simpler and perfectly equivalent conversion that we follow in our experiments is to present the data as x-nd-prediction. (\u2212 1, 0, 1} where missing observations are encoded as zeros, each contributing the constant factor xnd (0), to the probability X-Z, thus contributing to the full atp."}, {"heading": "3.4 Multi-Layer OrMachine", "text": "In analogy to multilayered neural networks, we can build a hierarchy of correlations by applying another layer of factorisation to factor matrix Z. This is reminiscent of the idea of deep exponential families introduced by Ranganath et al. (2015) The ability to learn features at different levels of abstraction is often cited as an explanation for the success that deep neural networks have in many areas of application (Lin & Tegmark, 2016; Goodfellow et al., 2016). In the current environment, with stochasticity at every step of the generative process and posterior inference, we are able to follow truly significant and interpretable hierarchies of abstraction. To give an example, we determine the optimal multilayered architecture to represent the computational toy dataset as introduced in Fig. 1. We observe 50 digits and consider 70% of the data as disregarded."}, {"heading": "3.5 Practical Implementation and Speed", "text": "The algorithm is implemented in Python, using the core sample routines in the compiled cython. Binary data is represented as {\u2212 1, 1}, missing data is encoded as 0. This economic representation of data and variables as integral types simplifies calculations considerably. Algorithm 1 is implemented in parallel via the observations [n] = {1,.., N} and vice versa, updates for uld are implemented in parallel across all characteristics [d] = {1,.., D}. Calculation time scales linearly in each dimension. A single sweep through high-resolution arithmetic digits with toy data sets with ND = 1.7 x 106 data points and L = 7 latent dimensions takes about 1 second on a desktop computer. A single sweep through the approximately 1.4 x 1010 data points with L = 2 latent dimensions presented in the biological example in Section 5.2 takes about 5 minutes, executed on 24 arithmetic cores."}, {"heading": "4 Experiments on Simulated Data", "text": "In this section, we examine the performance of OrMachine (OrM) in random matrix factoration and completion tasks. Message Passing (MP) has proven advantageous to BooMF compared to other state-of-the-art methods (Ravanbakhsh et al., 2016) and is therefore the focus of our comparison. The following settings for MP and OrM are used in our experiments: For MP, we use the Python implementation provided by the authors. We also conduct an experiment with their selection of hyper-parameter settings with different learning rates and maximum number of iterations. For OrMachine, we initialize the parameters uniformly randomly and perform 100 iterations of the Metropolised Gibbs Sampler as burn-in phase. We then perform 100 samples based on our posterior averages and MAP estimates."}, {"heading": "4.1 Random Matrix Factorisation", "text": "We create a quadratic matrix X, {0, 1} N \u00b7 N of rank L by taking the Boolean product of two random N \u00b7 L factor matrices. Boolean product X of two rank L binary matrices sampled from a Bernoulli distribution has an expected value of E (X) = 1 \u2212 (1 \u2212 p2) L. Since we generally prefer X neither sparse nor dense, we fix its expected density to d = 0.5, unless otherwise stated, ensuring that a simple bias is not met with reward in either method. Bits in the data are randomly reversed with probabilities ranging from 5% to 45%. Factor matrices of the correct underlying dimension are inferred and the data are reconstructed from the inferred factoration. An example of this problem is shown in the figure. 3. Results for the reconstruction error defined as a fraction of correctly reconstructed data points are displayed in Figure 4."}, {"heading": "4.2 Random Matrix Completion", "text": "Using the method outlined in Section 4.1, we generate random matrices of rank 5 and size 250 x 250. We observe only a random subset of data ranging from 0.5% to 3.5%. Missing data is reconstructed from the derived factor matrices. As shown in Fig. 5, the OrMachine exceeds the continuous message. The diagram shows mean and standard deviations of 10 repetitions of the entire experiment. It is noteworthy that the OrMachine not only provides a MAP estimate, but also an estimate of the posterior probability for each xnd. Fig. 5 (below) shows an estimate of the posterior mean density for the correctly and incorrectly filled matrix entries. The distribution of false predictions reaches a probability of 1 / 2. This indicates that the OrMachine may mean a false uncertainty about the construction used for the subsequent instance by providing further information about the uncertainty used."}, {"heading": "5 Experiments on Real World Data", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "5.1 MovieLens Matrix Completion", "text": "The MovieLens-1M dataset contains 106 integer movie ratings from 1 to 5 of 6000 users for 4000 movies, i.e. 1 / 24 of the possible ratings are available. Likewise, the MovieLens 100k dataset contains 943 users and 1682 movies. Following Ravanbakhsh et al. (2016), we binarize the data using the global mean as a threshold. We observe only a fraction of the available data, which vary from 1% to 95%, and reconstruct the remaining available data using the procedure in Section 4.2 with L = 2 latent dimensions. Reconstruction accuracy is calculated as a fraction of correctly reconstructed unobserved ratings in Table 1.The values given are averages of 10 randomly initialized runs of each algorithm. The corresponding standard deviations are always less than 0.2%."}, {"heading": "5.2 Explorative Analysis of Single Cell Gene Expression Profiles", "text": "In recent years, this has led to the discovery of new cell types and a better understanding of tissue heterogeneity (Trapnell, 2015), which is particularly relevant in cancer research, where it helps to understand the cellular composition of a tumor and its relationship to disease progression and treatment (Patel et al., 2014).Here, we apply the Orb Machine to binarized gene expression profiles of about 1.3 million cells per cell, where it helps to understand the cellular composition of a tumor and its relationship to disease progression and treatment (Patel et al., 2014).The data are publicly available2. Only 7% of the data points are not zero."}, {"heading": "6 Conclusion", "text": "We have developed the OrMachine, a probabilistic model for Boolean matrix factorization. The extremely efficient Metropolised Gibbs sampler surpasses state-of-the-art methods of matrix factorization and completion. It is the first method that undermines posterior distributions for Boolean matrix factorization, which is highly relevant in practical applications. Despite complete posterior inference, the proposed methods scale to very large data sets. We have shown that tens of billions of data points can be handled on raw material hardware. OrMachine can easily accommodate missing data and prior knowledge. Layers of OrMachine can be stacked similar to deep belief networks, resulting in representations at various levels of abstraction, resulting in improved reconstruction performance in simulated and real data. Further experiments include the ability to learn deep true abstractions."}, {"heading": "7 Acknowledgements", "text": "T.R. is supported by a grant from the UK Engineering and Physical Sciences Research Council and F. Hoffman La-Roche. C.Y. is supported by a UK Medical Research Council New Investigator Research Grant (Ref. No. MR / L001411 / 1), the Wellcome Trust Core Award Grant Number 090532 / Z / 09 / Z, the John Fell Oxford University Press (OUP) Research Fund. We thank Dr. Satu Nahkuri for his support and advice in developing this work."}], "references": [{"title": "Fast algorithms for mining association rules", "author": ["Agrawal", "Rakesh", "Srikant", "Ramakrishnan"], "venue": "In Proc. 20th Int. Conf. Very Large Data Bases, VLDB,", "citeRegEx": "Agrawal et al\\.,? \\Q1994\\E", "shortCiteRegEx": "Agrawal et al\\.", "year": 1994}, {"title": "Dissecting genomic diversity, one cell at a time", "author": ["Blainey", "Paul C", "Quake", "Stephen R"], "venue": "Nat. Methods,", "citeRegEx": "Blainey et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Blainey et al\\.", "year": 2014}, {"title": "Probabilistic topic models", "author": ["Blei", "David M"], "venue": "Communications of the ACM,", "citeRegEx": "Blei and M.,? \\Q2012\\E", "shortCiteRegEx": "Blei and M.", "year": 2012}, {"title": "Enrichr: Interactive and collaborative html5 gene list enrichment analysis tool", "author": ["Chen", "Edward Y", "Tan", "Christopher M", "Kou", "Yan", "Duan", "Qiaonan", "Wang", "Zichen", "Meirelles", "Gabriela", "Clark", "Neil R", "Ma\u2019ayan", "Avi"], "venue": "BMC Bioinformatics,", "citeRegEx": "Chen et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Chen et al\\.", "year": 2013}, {"title": "L1 and chl1 cooperate in thalamocortical axon targeting", "author": ["G.P. Demyanenko", "P.F. Siesser", "A.G. Wright", "L.H. Brennaman", "U. Bartsch", "M. Schachner", "P.F. Maness"], "venue": "Cerebral Cortex,", "citeRegEx": "Demyanenko et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Demyanenko et al\\.", "year": 2010}, {"title": "Deep learning. Book in preparation for", "author": ["Goodfellow", "Ian", "Bengio", "Yoshua", "Courville", "Aaron"], "venue": null, "citeRegEx": "Goodfellow et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Goodfellow et al\\.", "year": 2016}, {"title": "Hierarchical compositional feature learning", "author": ["L\u00e1zaro-Gredilla", "Miguel", "Liu", "Yi", "Phoenix", "D Scott", "George", "Dileep"], "venue": "arXiv preprint arXiv:1611.02252,", "citeRegEx": "L\u00e1zaro.Gredilla et al\\.,? \\Q2016\\E", "shortCiteRegEx": "L\u00e1zaro.Gredilla et al\\.", "year": 2016}, {"title": "Why does deep and cheap learning work so well? 2016", "author": ["Lin", "Henry W", "Tegmark", "Max"], "venue": null, "citeRegEx": "Lin et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Lin et al\\.", "year": 2016}, {"title": "peskun\u2019s theorem and a modified discrete-state gibbs sampler", "author": ["Liu", "J. Miscellanea"], "venue": null, "citeRegEx": "Liu and Miscellanea.,? \\Q1996\\E", "shortCiteRegEx": "Liu and Miscellanea.", "year": 1996}, {"title": "Robo1 and robo2 cooperate to control the guidance of major axonal tracts in the mammalian forebrain", "author": ["G. Lopez-Bendito", "N. Flames", "L. Ma", "C. Fouquet", "Meglio", "T. Di", "A. Chedotal", "M. Tessier-Lavigne", "O. Marin"], "venue": "Journal of Neuroscience,", "citeRegEx": "Lopez.Bendito et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Lopez.Bendito et al\\.", "year": 2007}, {"title": "Mdl4bmf: Minimum description length for boolean matrix factorization", "author": ["Miettinen", "Pauli", "Vreeken", "Jilles"], "venue": "ACM Trans. Knowl. Discov. Data,", "citeRegEx": "Miettinen et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Miettinen et al\\.", "year": 2014}, {"title": "The Discrete Basis Problem, pp. 335\u2013346", "author": ["Miettinen", "Pauli", "Mielik\u00e4inen", "Taneli", "Gionis", "Aristides", "Das", "Gautam", "Mannila", "Heikki"], "venue": null, "citeRegEx": "Miettinen et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Miettinen et al\\.", "year": 2006}, {"title": "Single-cell rna-seq highlights intratumoral heterogeneity in primary glioblastoma", "author": ["A.P. Patel", "I. Tirosh", "J.J. Trombetta", "A.K. Shalek", "S.M. Gillespie", "H. Wakimoto", "D.P. Cahill", "B.V. Nahed", "W.T. Curry", "R.L. Martuza", "D.N. Louis", "O. Rozenblatt-Rosen", "M.L. Suva", "A. Regev", "B.E. Bernstein"], "venue": "Science, 344(6190):1396\u20131401,", "citeRegEx": "Patel et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Patel et al\\.", "year": 2014}, {"title": "Optimum monte-carlo sampling using markov chains", "author": ["Peskun", "Peter H"], "venue": "Biometrika, 60(3):607\u2013612,", "citeRegEx": "Peskun and H.,? \\Q1973\\E", "shortCiteRegEx": "Peskun and H.", "year": 1973}, {"title": "Selenoprotein w expression and regulation in mouse brain and neurons", "author": ["Raman", "Arjun V", "Pitts", "Matthew W", "Seyedali", "Ali", "Hashimoto", "Ann C", "Bellinger", "Frederick P", "Berry", "Marla J"], "venue": "Brain and Behavior,", "citeRegEx": "Raman et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Raman et al\\.", "year": 2013}, {"title": "Deep exponential families", "author": ["Ranganath", "Rajesh", "Tang", "Linpeng", "Charlin", "Laurent", "Blei", "David M"], "venue": "In AISTATS,", "citeRegEx": "Ranganath et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Ranganath et al\\.", "year": 2015}, {"title": "Boolean matrix factorization and noisy completion via message passing", "author": ["Ravanbakhsh", "Siamak", "P\u00f3czos", "Barnab\u00e1s", "Greiner", "Russell"], "venue": "In Proceedings of The 33rd International Conference on Machine Learning,", "citeRegEx": "Ravanbakhsh et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Ravanbakhsh et al\\.", "year": 2016}, {"title": "Multi-assignment clustering for boolean data", "author": ["Streich", "Andreas P", "Frank", "Mario", "Basin", "David", "Buhmann", "Joachim M"], "venue": "In Proceedings of the 26th Annual International Conference on Machine Learning - ICML \u201909,", "citeRegEx": "Streich et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Streich et al\\.", "year": 2009}, {"title": "A gene atlas of the mouse and human protein-encoding transcriptomes", "author": ["A.I. Su", "T. Wiltshire", "S. Batalov", "H. Lapp", "K.A. Ching", "D. Block", "J. Zhang", "R. Soden", "M. Hayakawa", "G. Kreiman", "M.P. Cooke", "J.R. Walker", "J.B. Hogenesch"], "venue": "Proceedings of the National Academy of Sciences,", "citeRegEx": "Su et al\\.,? \\Q2004\\E", "shortCiteRegEx": "Su et al\\.", "year": 2004}, {"title": "A survey of collaborative filtering techniques", "author": ["Su", "Xiaoyuan", "Khoshgoftaar", "Taghi M"], "venue": "Adv. in Artif. Intell.,", "citeRegEx": "Su et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Su et al\\.", "year": 2009}, {"title": "Defining cell types and states with single-cell genomics", "author": ["Trapnell", "Cole"], "venue": "Genome Research,", "citeRegEx": "Trapnell and Cole.,? \\Q2015\\E", "shortCiteRegEx": "Trapnell and Cole.", "year": 2015}, {"title": "Expression profiling reveals differential gene induction underlying specific and nonspecific memory for pheromones in mice", "author": ["Upadhya", "Sudarshan C", "Smith", "Thuy K", "Brennan", "Peter A", "Mychaleckyj", "Josyf C", "Hegde", "Ashok N"], "venue": "Neurochemistry International,", "citeRegEx": "Upadhya et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Upadhya et al\\.", "year": 2011}, {"title": "A non-parametric bayesian method for inferring hidden causes", "author": ["Wood", "Frank", "Griffiths", "Thomas", "Ghahramani", "Zoubin"], "venue": null, "citeRegEx": "Wood et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Wood et al\\.", "year": 2012}, {"title": "Gene expression patterns of hippocampus and cerebral cortex of senescence-accelerated mouse treated with huang-lian-jie-du decoction", "author": ["Zheng", "Yue", "Cheng", "Xiao-Rui", "Zhou", "Wen-Xia", "Zhang", "Yong-Xiang"], "venue": "Neuroscience Letters,", "citeRegEx": "Zheng et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Zheng et al\\.", "year": 2008}, {"title": "2016), who also provide comparison to other state-of the art", "author": ["Ravanbakhsh"], "venue": null, "citeRegEx": "Ravanbakhsh,? \\Q2016\\E", "shortCiteRegEx": "Ravanbakhsh", "year": 2016}], "referenceMentions": [{"referenceID": 6, "context": "BooMF can have many real-world applications ranging from topic modelling (Blei, 2012) to collaborating filtering (Su & Khoshgoftaar, 2009) and computer vision (L\u00e1zaro-Gredilla et al., 2016).", "startOffset": 159, "endOffset": 189}, {"referenceID": 16, "context": "We show that this formulation significantly outperforms the previous state-of-the-art message passing approaches for learning BooMF models (Ravanbakhsh et al., 2016).", "startOffset": 139, "endOffset": 165}, {"referenceID": 11, "context": "The Discrete Basis Problem (Miettinen et al., 2006) provides a greedy heuristic algorithm to solve BooMF without recourse to an underlying probabilistic model.", "startOffset": 27, "endOffset": 51}, {"referenceID": 0, "context": "It is based on association rule mining (Agrawal et al., 1994) and has more recently been extended to automatically select the optimal dimensionality of the latent space based on the minimum description length principle (Miettinen & Vreeken, 2014).", "startOffset": 39, "endOffset": 61}, {"referenceID": 17, "context": "In contrast, multi assignment clustering for Boolean data (Streich et al., 2009) leverages on a probabilistic model for BooMF, adding a further global noise source to the generative process.", "startOffset": 58, "endOffset": 80}, {"referenceID": 0, "context": "It is based on association rule mining (Agrawal et al., 1994) and has more recently been extended to automatically select the optimal dimensionality of the latent space based on the minimum description length principle (Miettinen & Vreeken, 2014). In contrast, multi assignment clustering for Boolean data (Streich et al., 2009) leverages on a probabilistic model for BooMF, adding a further global noise source to the generative process. Point estimates are inferred by deterministic annealing. Similarly, Wood et al. (2012) develop a probabilistic model to infer hidden causes.", "startOffset": 40, "endOffset": 526}, {"referenceID": 0, "context": "It is based on association rule mining (Agrawal et al., 1994) and has more recently been extended to automatically select the optimal dimensionality of the latent space based on the minimum description length principle (Miettinen & Vreeken, 2014). In contrast, multi assignment clustering for Boolean data (Streich et al., 2009) leverages on a probabilistic model for BooMF, adding a further global noise source to the generative process. Point estimates are inferred by deterministic annealing. Similarly, Wood et al. (2012) develop a probabilistic model to infer hidden causes. In contrast to the Boolean OR, the likelihood of an observation increases with the number of active hidden codes. The Indian Buffet process serves as non-parametric prior over the latent space and a Gibbs sampler infers the distribution over the unbounded number of hidden causes. A similar approach to ours is the work by Ravanbakhsh et al. (2016). The authors tackle BooMF using a probabilistic graphical model and derive a message passing algorithm to perform MAP inference.", "startOffset": 40, "endOffset": 929}, {"referenceID": 0, "context": "It is based on association rule mining (Agrawal et al., 1994) and has more recently been extended to automatically select the optimal dimensionality of the latent space based on the minimum description length principle (Miettinen & Vreeken, 2014). In contrast, multi assignment clustering for Boolean data (Streich et al., 2009) leverages on a probabilistic model for BooMF, adding a further global noise source to the generative process. Point estimates are inferred by deterministic annealing. Similarly, Wood et al. (2012) develop a probabilistic model to infer hidden causes. In contrast to the Boolean OR, the likelihood of an observation increases with the number of active hidden codes. The Indian Buffet process serves as non-parametric prior over the latent space and a Gibbs sampler infers the distribution over the unbounded number of hidden causes. A similar approach to ours is the work by Ravanbakhsh et al. (2016). The authors tackle BooMF using a probabilistic graphical model and derive a message passing algorithm to perform MAP inference. Their method is shown to have state-of-the-art performance for BooMF and completion. It therefore serves us as baseline benchmark in these task. The message passing approach has recently been employed by L\u00e1zaro-Gredilla et al. (2016) in a hierarchical network combined with pooling", "startOffset": 40, "endOffset": 1292}, {"referenceID": 5, "context": "The ability to learn features at different levels of abstraction is commonly cited as an explanation for the success that deep neural networks have across many domains of application (Lin & Tegmark, 2016; Goodfellow et al., 2016).", "startOffset": 183, "endOffset": 229}, {"referenceID": 14, "context": "This is reminiscent of the idea of deep exponential families, as introduced by Ranganath et al. (2015). The ability to learn features at different levels of abstraction is commonly cited as an explanation for the success that deep neural networks have across many domains of application (Lin & Tegmark, 2016; Goodfellow et al.", "startOffset": 79, "endOffset": 103}, {"referenceID": 16, "context": "Message passing (MP) has been shown to compare favourably with other state-of-the-art methods for BooMF (Ravanbakhsh et al., 2016) and is therefore the focus of our comparison.", "startOffset": 104, "endOffset": 130}, {"referenceID": 16, "context": "2 in Ravanbakhsh et al. (2016) and shows that the OrMachine consistently enables error-free reconstruction of a 1000\u00d71000 matrix of rank L=5 for up to 30% bit flip probability.", "startOffset": 5, "endOffset": 31}, {"referenceID": 16, "context": "2 in Ravanbakhsh et al. (2016) and shows that the OrMachine consistently enables error-free reconstruction of a 1000\u00d71000 matrix of rank L=5 for up to 30% bit flip probability. Notably, MP performs worse for smaller noise levels. It was hypothesised by Ravanbakhsh et al. (2016) that symmetry breaking at higher noise levels helps message passage to converge to a better solution.", "startOffset": 5, "endOffset": 279}, {"referenceID": 16, "context": "Following Ravanbakhsh et al. (2016), we binarise the data taking the global mean as threshold.", "startOffset": 10, "endOffset": 36}, {"referenceID": 12, "context": "The latter is particularly relevant in cancer research where it helps to understand the cellular composition of a tumour and its relationship to disease progression and treatment (Patel et al., 2014).", "startOffset": 179, "endOffset": 199}, {"referenceID": 3, "context": "This is done using the Enrichr analysis tool (Chen et al., 2013) and a mouse gene atlas (Su et al.", "startOffset": 45, "endOffset": 64}, {"referenceID": 18, "context": ", 2013) and a mouse gene atlas (Su et al., 2004).", "startOffset": 31, "endOffset": 48}, {"referenceID": 9, "context": "Typical genes for each of the biological states are annotated (Lopez-Bendito et al., 2007; Zheng et al., 2008; Demyanenko et al., 2010; Upadhya et al., 2011; Raman et al., 2013).", "startOffset": 62, "endOffset": 177}, {"referenceID": 23, "context": "Typical genes for each of the biological states are annotated (Lopez-Bendito et al., 2007; Zheng et al., 2008; Demyanenko et al., 2010; Upadhya et al., 2011; Raman et al., 2013).", "startOffset": 62, "endOffset": 177}, {"referenceID": 4, "context": "Typical genes for each of the biological states are annotated (Lopez-Bendito et al., 2007; Zheng et al., 2008; Demyanenko et al., 2010; Upadhya et al., 2011; Raman et al., 2013).", "startOffset": 62, "endOffset": 177}, {"referenceID": 21, "context": "Typical genes for each of the biological states are annotated (Lopez-Bendito et al., 2007; Zheng et al., 2008; Demyanenko et al., 2010; Upadhya et al., 2011; Raman et al., 2013).", "startOffset": 62, "endOffset": 177}, {"referenceID": 14, "context": "Typical genes for each of the biological states are annotated (Lopez-Bendito et al., 2007; Zheng et al., 2008; Demyanenko et al., 2010; Upadhya et al., 2011; Raman et al., 2013).", "startOffset": 62, "endOffset": 177}], "year": 2017, "abstractText": "Boolean matrix factorisation (BooMF) infers interpretable decompositions of a binary data matrix into a pair of low-rank, binary matrices: One containing meaningful patterns, the other quantifying how the observations can be expressed as a combination of these patterns. We introduce the OrMachine, a probabilistic generative model for BooMF and derive a Metropolised Gibbs sampler that facilitates very efficient parallel posterior inference. Our method outperforms all currently existing approaches for Boolean Matrix factorization and completion, as we show on simulated and real world data. This is the first method to provide full posterior inference for BooMF which is relevant in applications, e.g. for controlling false positive rates in collaborative filtering, and crucially it improves the interpretability of the inferred patterns. The proposed algorithm scales to large datasets as we demonstrate by analysing single cell gene expression data in 1.3 million mouse brain cells across 11,000 genes on commodity hardware.", "creator": "LaTeX with hyperref package"}}}