{"id": "1702.06976", "review": {"conference": "AAAI", "VERSION": "v1", "DATE_OF_SUBMISSION": "22-Feb-2017", "title": "Heavy-Tailed Analogues of the Covariance Matrix for ICA", "abstract": "Independent Component Analysis (ICA) is the problem of learning a square matrix $A$, given samples of $X=AS$, where $S$ is a random vector with independent coordinates. Most existing algorithms are provably efficient only when each $S_i$ has finite and moderately valued fourth moment. However, there are practical applications where this assumption need not be true, such as speech and finance. Algorithms have been proposed for heavy-tailed ICA, but they are not practical, using random walks and the full power of the ellipsoid algorithm multiple times. The main contributions of this paper are:", "histories": [["v1", "Wed, 22 Feb 2017 19:27:38 GMT  (909kb,D)", "http://arxiv.org/abs/1702.06976v1", "16 Pages, 9 Figures, AAAI 2017"]], "COMMENTS": "16 Pages, 9 Figures, AAAI 2017", "reviews": [], "SUBJECTS": "cs.LG stat.ML", "authors": ["joseph anderson", "navin goyal", "anupama nandi", "luis rademacher"], "accepted": true, "id": "1702.06976"}, "pdf": {"name": "1702.06976.pdf", "metadata": {"source": "CRF", "title": "Heavy-Tailed Analogues of the Covariance Matrix for ICA", "authors": ["Joseph Anderson", "Navin Goyal", "Anupama Nandi", "Luis Rademacher"], "emails": [], "sections": [{"heading": null, "text": "(1) A practical algorithm for low-waisted ICA, which we call HTICA. We offer theoretical guarantees and show that in some low-waisted systems it outperforms other algorithms, both on real and synthetic data. As with the current state of the art, the new algorithm is based on the centrifugal body (an analogue of the first moment to the covariance matrix). Unlike the current state of the art, our algorithm is practically efficient. To achieve this, we use explicit analytical representations of the centrifugal body that circumvent the use of the ellipsoid method and random steps. (2) We study how heavy tails affect various ICA algorithms, including HTICA. Somewhat surprisingly, we show that some algorithms that use the covariance matrix or higher moments can successfully solve a number of ICA instances with an infinite second moment."}, {"heading": "1 Introduction", "text": "Independent Component Analysis (ICA) is a computerized and statistical technique with applications ranging from signal processing to machine learning and more. Formally, if S is a n-dimensional random vector with independent coordinates and A-Rn-n, then the ICA problem is directly due to access to i.i.d. samples of mixed signals X = AS. We say that the distributions of the random variables Si do not have to be Gaussian (except possibly one of them).Since their origin was in the eighties (see [CJ10] for historical remarks), ICA has been thoroughly studied and a huge literature exists (e.g. [HKO01, CJ10]."}, {"heading": "2 Preliminaries", "text": "They are characterized by the slow decay of their tails. Examples of heavy-tailed distributions are the pareto and log normal distributions. As observed in [AGNR15], this is without loss of universality for our purposes. This is due to the fact that X = AS is a stable model, and if we let X \u2032 = AS \u2032 be a copy of the same model, then X \u2212 X \u2032 = A (S \u2032) is an ICA model with components of S \u2032 that are symmetrical."}, {"heading": "3 HTICA and experiments", "text": "In this section, we demonstrate experimentally that heavy tail data pose a significant challenge to current ICA algorithms, and compare it with HTICA in different environments. We observe some clear situations where heavy tail data seriously impair standard ICA algorithms, and that these problems are often avoided by using the heavy tail ICA framework. In some cases, HTICA does not help much, but retains the same performance of simple FastICA. To generate the synthetic data, we generate a simple heavy tail density function f\u03b7 (x) proportional to (| x | + 1.5) \u2212 \u03b7, which is symmetrical, and for \u03b7 > 1 f\u03b7 is the density of a distribution that has finite k < \u03b7 \u2212 1 moment. Signal S is generated with any Si distributed independently of f\u03b7i. The mixed matrix A, Rn \u00d7 n, is compared with any coordinate of Frog values, which are not comparable to the estimated length of the norm i.id. (N)."}, {"heading": "3.1 Heavy-tailed ICA when A is orthogonal: Gaussian damping and experiments", "text": "As proposed in [AGNR15], Gaussian damping is a pre-processing technique that converts data from an ICA model X = AS, where A is uniform (columns are orthogonal with unit l2-norm) to data from a related ICA model XR = ASR, where R > 0 is a parameter to be choosed.The independent components of the SR have finite moments of all jobs, so existing algorithms A.Using samples from X can construct the damped random variable XR, where R > 0 is a parameter to be choosed.The independent components of the SR have finite moments of all jobs, so existing algorithms A.Using samples from X can construct the damped random variable XR, with pdf XR (x) a better convergence (\u2212 x) exp (\u2212 x) exp (\u2212 R)."}, {"heading": "3.2 Experiments on synthetic heavy-tailed data", "text": "We will now present the results of the HTICA using various orthogonalization techniques: (1) orthogonalization via covariance (section 4.2 (2) orthogonalization via the central body (section 4.1) (3) the basic truth that directly inverts the mixed matrix (oracle), and (4) no orthogonalization or damping (as compared to simple FastICA) (identity). The \"mixed\" regime in the left and middle side of Figure 2 (where some signals are not heavy-waisted) shows a very dramatic contrast between different orthogonalization methods, even if only two heavy-waisted signals are present. In the experiment with different methods of orthogonalization, it was observed that if all exponents are equal or very close, orthogonalization via covariance works better than orthogonalization via centric and true mixing matrix, as in Figure 2. A partial explanation (no iconogonization) without any connotation (and no configuration)."}, {"heading": "3.3 ICA on speech data", "text": "While the above study on synthetic data offers interesting situations in which heavy tails can cause problems for ICA, here are some results that use real data, especially human speech. To examine the performance of HTICA on speech data, we first examine whether the data is cumbersome. The motivation to use speech data comes from observations by the signal processing community (e.g. [Kid00]) that speech data can be modeled through alpha-stable distributions. For an alpha-stable distribution with \u03b1 (0, 2), only the moments of order under \u03b1 will be finite. Here, we present some results on a data set of human language according to the standard cocktail party model."}, {"heading": "4 New approach to orthogonalization and a new analysis of em-", "text": "Pirical CovarianceAs mentioned above, the technique in [AGNR15], although proven to be efficient and correct, suffers from practical implementation problems. Here, we discuss two alternatives: orthogonalization by centric body scaling and orthogonalization by using empirical covariance. The former, orthogonalization by centric body scaling, uses the samples already present in the algorithm rather than relying on a random path to draw samples that are roughly uniform in the approach of the algorithm to the centric body (as happens in [AGNR15]), eliminating the dependence on random walks and the ellipsoid algorithm; instead, we use samples that are scaled according to the original heavy tail distribution but not linearly to lie within the centric body (as in [AGNR15]). In Lemma 3, we prove that the covariance of this subset of samples is sufficient to mix the samples."}, {"heading": "4.1 Orthogonalization via centroid body scaling", "text": "In [AGNR15], another orthogonalization procedure, namely the orthogonalization of the actual subjects, which we then estimate. (AGNR15), another orthogonalization procedure, namely the orthogonalization of the actual subjects (QQ = Q = Q = Q = Q = Q = Q = Q Q = Q (Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q ="}, {"heading": "4.2 Orthogonalization via covariance", "text": "Here we show the somewhat surprising fact that the orthogonalization of sword-waisted signals is sometimes possible by using the \"standard\" approach: reversing the empirical covariance matrix. The advantage here is that it is mathematically very simple, especially that with sword-waisted data there is very little computational effort for the process of orthogonalization alone. It is standard to use the covariance matrix for whitening when the second moments of all independent components exist [HKO01]: Given the samples from the ICA model X = AS, we calculate the empirical covariance matrix that tends to the true covariance matrix as we take and determine it."}, {"heading": "5 Membership oracle for the centroid body, without polarity", "text": "We start with an informal description of the algorithm and its correction. The algorithm implementing the oracle (subroutine 2) is as follows: Let q q q = q = query point. Let X1,.., XN be a sample of the random vector X. If q is not included in the sample, then there is a hyperplane that separates q from (Xi X). Let {x: aTx = b} be the hyperplane that makes Y satisfactory, otherwise there is the no.Idea of the correctness of the algorithm: If q is not in sample X, then there is a hyperplane that separates q from (Xi X). Let {x: aTx = b} be the hyperplane that is a satisfaction of probability."}, {"heading": "5.1 Formal Argument", "text": "Let Rn be an absolutely symmetrically distributed random vector (< Si |). Let X be a random vector on Rn. Let A: Rn \u2192 Rn be an immutable linear transformation. Then (AX). Let S = (AGNR15). Let X be a random vector on Rn. Let A: Rn be an immutable linear transformation. Then (AX). Let S = (S1,.,., Sn). Let Rn be an absolutely symmetrically distributed random vector, so that E (Si). Let S = (AX). Let S = (S1,.,., Sn). Let Rn be an absolutely symmetrically distributed random vector."}], "references": [{"title": "A new learning algorithm for blind signal separation", "author": ["Shun Amari", "Andrzej Cichocki", "Howard H Yang"], "venue": "Advances in neural information processing systems,", "citeRegEx": "Amari et al\\.,? \\Q1996\\E", "shortCiteRegEx": "Amari et al\\.", "year": 1996}, {"title": "Heavy-tailed independent component analysis", "author": ["Joseph Anderson", "Navin Goyal", "Anupama Nandi", "Luis Rademacher"], "venue": "In 56th Annual IEEE Symposium on Foundations of Computer Science (FOCS 2015),", "citeRegEx": "Anderson et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Anderson et al\\.", "year": 2015}, {"title": "Approximate likelihood for noisy mixtures", "author": ["Olivier Bermond", "Jean-Fran\u00e7ois Cardoso"], "venue": "In Proc. ICA,", "citeRegEx": "Bermond and Cardoso.,? \\Q1999\\E", "shortCiteRegEx": "Bermond and Cardoso.", "year": 1999}, {"title": "Inference with multivariate heavy-tails in linear models", "author": ["Danny Bickson", "Carlos Guestrin"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Bickson and Guestrin.,? \\Q2010\\E", "shortCiteRegEx": "Bickson and Guestrin.", "year": 2010}, {"title": "Functional analysis, Sobolev spaces and partial differential equations. Universitext", "author": ["Haim Brezis"], "venue": null, "citeRegEx": "Brezis.,? \\Q2011\\E", "shortCiteRegEx": "Brezis.", "year": 2011}, {"title": "Source separation using higher order moments", "author": ["J.F. Cardoso"], "venue": "In International Conference on Acoustics, Speech, and Signal Processing,", "citeRegEx": "Cardoso.,? \\Q1989\\E", "shortCiteRegEx": "Cardoso.", "year": 1989}, {"title": "Robustness of prewhitening against heavy-tailed sources. In Independent Component Analysis and Blind Signal Separation", "author": ["Aiyou Chen", "Peter J. Bickel"], "venue": "Fifth International Conference,", "citeRegEx": "Chen and Bickel.,? \\Q2004\\E", "shortCiteRegEx": "Chen and Bickel.", "year": 2004}, {"title": "Consistent independent component analysis and prewhitening", "author": ["Aiyou Chen", "Peter J Bickel"], "venue": "Signal Processing, IEEE Transactions on,", "citeRegEx": "Chen and Bickel.,? \\Q2005\\E", "shortCiteRegEx": "Chen and Bickel.", "year": 2005}, {"title": "Handbook of Blind Source Separation", "author": ["Pierre Comon", "Christian Jutten", "editors"], "venue": null, "citeRegEx": "Comon et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Comon et al\\.", "year": 2010}, {"title": "Blind beamforming for non-gaussian signals", "author": ["J.-F. Cardoso", "A. Souloumiac"], "venue": "In Radar and Signal Processing, IEE Proceedings F,", "citeRegEx": "Cardoso and Souloumiac.,? \\Q1993\\E", "shortCiteRegEx": "Cardoso and Souloumiac.", "year": 1993}, {"title": "On portfolio selection under extreme risk measure: The heavy-tailed ica model", "author": ["Stephan Clemencon", "Skander Slim"], "venue": "International Journal of Theoretical and Applied Finance,", "citeRegEx": "Clemencon and Slim.,? \\Q2007\\E", "shortCiteRegEx": "Clemencon and Slim.", "year": 2007}, {"title": "The infinite factorial hidden markov model", "author": ["Jurgen V Gael", "Yee W Teh", "Zoubin Ghahramani"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Gael et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Gael et al\\.", "year": 2009}, {"title": "Independent Component Analysis", "author": ["Aapo Hyvarinen", "Juha Karhunen", "Erkki Oja"], "venue": null, "citeRegEx": "Hyvarinen et al\\.,? \\Q2001\\E", "shortCiteRegEx": "Hyvarinen et al\\.", "year": 2001}, {"title": "Fast and robust fixed-point algorithms for independent component analysis", "author": ["A. Hyvarinen"], "venue": "Neural Networks, IEEE Transactions on,", "citeRegEx": "Hyvarinen.,? \\Q1999\\E", "shortCiteRegEx": "Hyvarinen.", "year": 1999}, {"title": "Novel characteristic function based criteria for ica", "author": ["A. Kankainen J. Eriksson", "V. Koivunen"], "venue": "In Proceedings ICA 2001,", "citeRegEx": "Eriksson and Koivunen.,? \\Q2001\\E", "shortCiteRegEx": "Eriksson and Koivunen.", "year": 2001}, {"title": "Alpha-stable distributions in signal processing of audio signals", "author": ["Preben Kidmose"], "venue": "In 41st Conference on Simulation and Modelling, Scandinavian Simulation Society,", "citeRegEx": "Kidmose.,? \\Q2000\\E", "shortCiteRegEx": "Kidmose.", "year": 2000}, {"title": "Blind Separation of Heavy Tail Signals", "author": ["Preben Kidmose"], "venue": "PhD thesis, Technical University of Denmark,", "citeRegEx": "Kidmose.,? \\Q2001\\E", "shortCiteRegEx": "Kidmose.", "year": 2001}, {"title": "Independent component analysis using the spectral measure for alpha-stable distributions", "author": ["Preben Kidmose"], "venue": "In Proceedings of IEEE-EURASIP Workshop on Nonlinear Signal and Image Processing,", "citeRegEx": "Kidmose.,? \\Q2001\\E", "shortCiteRegEx": "Kidmose.", "year": 2001}, {"title": "On zonotopes", "author": ["P. McMullen"], "venue": "Trans. Amer. Math. Soc.,", "citeRegEx": "McMullen.,? \\Q1971\\E", "shortCiteRegEx": "McMullen.", "year": 1971}, {"title": "Stable Distributions - Models for Heavy Tailed Data", "author": ["J.P. Nolan"], "venue": null, "citeRegEx": "Nolan.,? \\Q2015\\E", "shortCiteRegEx": "Nolan.", "year": 2015}, {"title": "Blind source separation of noisy mixtures using a semi-parametric approach with application to heavy-tailed signals", "author": ["M. Sahmoudi", "K. Abed-Meraim", "M. Lavielle", "E. Kuhn", "Ph. Ciblat"], "venue": "In Proc. of EUSIPCO", "citeRegEx": "Sahmoudi et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Sahmoudi et al\\.", "year": 2005}, {"title": "Super-efficiency in blind signal separation of symmetric heavy-tailed sources", "author": ["Yoav Shereshevski", "Arie Yeredor", "Hagit Messer"], "venue": "In Statistical Signal Processing,", "citeRegEx": "Shereshevski et al\\.,? \\Q2001\\E", "shortCiteRegEx": "Shereshevski et al\\.", "year": 2001}, {"title": "Ica by maximizing non-stability", "author": ["Baijie Wang", "Ercan E Kuruoglu", "Junying Zhang"], "venue": "In Independent Component Analysis and Signal Separation,", "citeRegEx": "Wang et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Wang et al\\.", "year": 2009}, {"title": "Learning sparse topographic representations with products of student-t distributions", "author": ["Max Welling", "Simon Osindero", "Geoffrey E Hinton"], "venue": "In Advances in neural information processing systems,", "citeRegEx": "Welling et al\\.,? \\Q2002\\E", "shortCiteRegEx": "Welling et al\\.", "year": 2002}], "referenceMentions": [], "year": 2017, "abstractText": "Independent Component Analysis (ICA) is the problem of learning a square matrix A, given samples of X = AS, where S is a random vector with independent coordinates. Most existing algorithms are provably efficient only when each Si has finite and moderately valued fourth moment. However, there are practical applications where this assumption need not be true, such as speech and finance. Algorithms have been proposed for heavy-tailed ICA, but they are not practical, using random walks and the full power of the ellipsoid algorithm multiple times. The main contributions of this paper are: (1) A practical algorithm for heavy-tailed ICA that we call HTICA. We provide theoretical guarantees and show that it outperforms other algorithms in some heavy-tailed regimes, both on real and synthetic data. Like the current state-of-the-art, the new algorithm is based on the centroid body (a first moment analogue of the covariance matrix). Unlike the state-of-the-art, our algorithm is practically efficient. To achieve this, we use explicit analytic representations of the centroid body, which bypasses the use of the ellipsoid method and random walks. (2) We study how heavy tails affect different ICA algorithms, including HTICA. Somewhat surprisingly, we show that some algorithms that use the covariance matrix or higher moments can successfully solve a range of ICA instances with infinite second moment. We study this theoretically and experimentally, with both synthetic and real-world heavy-tailed data.", "creator": "LaTeX with hyperref package"}}}