{"id": "1702.07015", "review": {"conference": "aaai", "VERSION": "v1", "DATE_OF_SUBMISSION": "22-Feb-2017", "title": "Unsupervised Learning of Morphological Forests", "abstract": "This paper focuses on unsupervised modeling of morphological families, collectively comprising a forest over the language vocabulary. This formulation enables us to capture edgewise properties reflecting single-step morphological derivations, along with global distributional properties of the entire forest. These global properties constrain the size of the affix set and encourage formation of tight morphological families. The resulting objective is solved using Integer Linear Programming (ILP) paired with contrastive estimation. We train the model by alternating between optimizing the local log-linear model and the global ILP objective. We evaluate our system on three tasks: root detection, clustering of morphological families and segmentation. Our experiments demonstrate that our model yields consistent gains in all three tasks compared with the best published results.", "histories": [["v1", "Wed, 22 Feb 2017 21:44:02 GMT  (3595kb,D)", "http://arxiv.org/abs/1702.07015v1", "12 pages, 5 figures, accepted by TACL 2017"]], "COMMENTS": "12 pages, 5 figures, accepted by TACL 2017", "reviews": [], "SUBJECTS": "cs.CL", "authors": ["jiaming luo", "karthik narasimhan", "regina barzilay"], "accepted": true, "id": "1702.07015"}, "pdf": {"name": "1702.07015.pdf", "metadata": {"source": "CRF", "title": "Unsupervised Learning of Morphological Forests", "authors": ["Jiaming Luo", "Karthik Narasimhan", "Regina Barzilay"], "emails": ["j_luo@mit.edu", "karthikn@mit.edu", "regina@csail.mit.edu"], "sections": [{"heading": null, "text": "This paper focuses on unattended modeling of morphological families that together form a forest above the language vocabulary. This formulation allows us to capture edge characteristics that reflect single-stage morphological derivatives, along with global distribution characteristics of the entire forest. These global characteristics limit the size of the appendage and promote the formation of close morphological families. The resulting goal is solved by Integrated Linear Programming (ILP) paired with contrasting estimation. We train the model by alternately optimizing the local loglinear model and the global ILP target. We evaluate our system based on three tasks: root recognition, clustering of morphological families, and segmentation. Our experiments show that our model yields consistent gains over the best published results in all three tasks."}, {"heading": "1 Introduction", "text": "The morphological study of a language that inherently refers to the existence of related words can be derived from a common root, whether in the form of infantry or in the form of derivatives."}, {"heading": "2 Related Work", "text": "Unsupervised morphological segmentation Most of the most powerful algorithms for unattended segmentation today focus on the modelling of individual derivatives (Poon et al., 2009; Naradowsky and Toutanova, 2011; Narasimhan et al., 2015). A commonly used log-linear formulation allows these models to take into account a number of characteristics ranging from orthographic patterns to semantic relationships. However, these models generally circumvent global limitations (Narasimhan et al., 2015) or require conclusions about very large spaces (Poon et al., 2009). As we show in our analysis (Section 5), this omission has a negative impact on model performance. In contrast, previous work has focused on modelling global morphological mappings using generative probability models (Creutz and Lagus, 2007; Snyder and Barzilay, 2008; Goldwater et al, 2013)."}, {"heading": "3 Model", "text": "Our model considers a complete morphological mapping for all words of a language that they represent as a forest. F = (V, E) is a directed graph in which each word corresponds to a node v \u0394V. A directed edge e = (vc, vp) \u0433E encodes a single morphological derivative from a parent word vp to a child word vc. Edges also reflect the nature of the underlying derivative (e.g. prefixation) and the associated probability Pr (e). Note that the root of a tree is always marked with a self-directed (i.e. vc = vp) edge associated with the mark stop. Figure 1 illustrates a single tree in the forest."}, {"heading": "3.1 Inducing morphological forests", "text": "We postulate that a valid mapping produces forests with the following characteristics: 1. Increased edge weights | Edge weights reflect the likelihood of one-step derivatives based on local characteristics such as orthographic patterns and semantic kinship. This local information helps to recognize that the edge (painter, color) should be preferred over (painter, pain) because \u2212 it is a valid suffix and color is semantically closer to painting. 2. Minimized number of affixes Prior research has shown that local models tend to greatly overestimate the number of suffixes. For example, the model by Narasimhan et al. (2015) produces 617 unique affixes when segmenting 10,000 English words."}, {"heading": "3.2 Computing local probabilities", "text": "We now describe how to parameterise Pr (e), which captures the probability of a one-step morphological derivative between two words. (2), where the number of parameters to be learned is high, and (w, z), where the attribute vector is extracted from the words w and z. Each candidate z is a tuple (string, label), where the label refers to the label of the potential label. (3), where the marginal probability is Pr (w) = parent word extracted from the words w and z. (4), each candidate z is a tuple (string, label), where the label refers to the label of the potential label. (4), the marginal probability is Pr (w) = parent word extracted from the words w and z."}, {"heading": "3.3 ILP formulation", "text": "Minimizing the target in Equation (1) is a challenge because the second and third terms capture discrete global properties of the forest, which prevents us from performing a gradient descent directly. Instead, we formulate this optimization problem as Integer Linear Programming (ILP), using these two terms as constraints.3For each child, we have a limited set of its outgoing edges C (vi) = {zji}, where zji is the j candidate for vi. C (vi) is the same set as defined in Section 3.2. Each edge is associated with pij, which is calculated as Log Pr (zji | vi). Let xij be a binary variable that has a value when and only when zji is selected to be in the forest. Without loss of universality, we assume that the first edge of the candidate is always the self-edge (or stop-case), i.e. zi (stop-case) vi."}, {"heading": "3.4 Alternating training", "text": "The objective function contains two sets of parameters: a continuous weight vector \u03b8, which parameterizes the edge probabilities, and binary variables {xij} and {yk} in the ILP. Due to the discordance between continuous and discrete variables, we must alternately optimize the target. Algorithm 1 describes the training procedure. After automatically extracting affixes from the corpus, we switch between learning the local edge probabilities (line 3) and solving the ILP (line 4). The feedback from solving the ILP with the global constraints can help us refine the learning of local probabilities by removing false affixes (line 5). For example, the automatic extraction based on frequencies may include -ers as an English suffix. This will probably be eliminated by ILP, as all the precursors of -ers can be removed without adding a new affixes, by concatenating -and -two very frequent suffixes."}, {"heading": "4 Experiments", "text": "We evaluate our model based on three tasks: segmentation, morphological family clustering and root detection. While the first task has been extensively investigated in the earlier literature, we consider two additional tasks to assess the flexibility of the derived representation."}, {"heading": "4.1 Morphological segmentation", "text": "It is a question of whether the two candidates are the two candidates running for President of the Federal Republic of Germany. (...) It is a question of whether the candidate is a candidate who wants to run for President of the Federal Republic of Germany. (...) It is a question of a candidate who wants to run for President of the Federal Republic of Germany. (...) It is a question of a candidate who wants to run for President of the Federal Republic of Germany. (...) It is a question of a candidate who wants to run for President of the Federal Republic of Germany. (...) It is a question of a candidate who wants to run for President of the Federal Republic of Germany. (...) It is a question of a candidate who is running for President of the Federal Republic of Germany. (...) It is about a candidate. (...) It is about a candidate. (...) It is about a candidate. (...) It is about a candidate. (...) It is about a candidate. (...) It is about a candidate. (...) It is about a candidate. (...) It is about a candidate. (...) It is about a candidate."}, {"heading": "4.2 Morphological family clustering", "text": "Morphological family clustering is the task of grouping morphologically related word forms. For example, we want to group color, color and pain into two clusters: \"paint, paints\" and \"pain.\" To derive clusters from the representation of the forest, we assume that all words in the same tree form a cluster. To obtain gold information about morphological clusters, we use CELEX (Baayen et al., 1993). Data statistics are summarized in Table 2. We remove words without strains from CELEX.8Baseline We compare our model with the NBJ Imp described above. We select the best variant of our model and the base model based on their respective performance based on the segmentation task. Rating We use the metrics proposed by Schone and Jurafsky (2000). Specifically, we leave W W W W W W W W W W W W W W W W W W W W W W W W W W http: / www.gurobi.com / 8W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W"}, {"heading": "4.3 Root detection", "text": "In addition, we evaluate how accurately our model can predict the root of any word. Data We report on the results of the Chipmunk dataset (Cotterell et al., 2015), which was used to evaluate monitored root detection models. Since our model is unattended, we report on performance both on the test set and on the entire dataset, combining the traction / test split. Data set statistics are shown in Table 3."}, {"heading": "5 Results", "text": "In the following sections, we report on the exemplary performance of each of the three evaluation tasks."}, {"heading": "5.1 Segmentation", "text": "We used cosmic similarity characteristics in all experiments, but the root forms of German verbs are rarely used, except in mandatory sentences, so they have little formed word vectors that contribute to the low memory value. We suspect that better treatment with word vectors can further improve the results. [http: / / www.mathcracker.com / sign-test. phpFrom Table 4, we observe that our model consistently outperforms the fundamentals of all four versions.] Compared to NBJ '15, our model has a higher F1 score of 3.7%, 4.4%, 2.9% and 27.7% in English, Arabic and German. While improved implementation of the NBJ Imp brings benefits from compounding and sibling features, our model still delivers an absolute increase in F1 score, ranging from 1.8% to 7.7% above NBJImp. Note that our model reaches higher values even without the K threshold or the number of linkages while we have reached optimal hypoformations during the performations."}, {"heading": "5.2 Morphological family clustering", "text": "The results of morphological family clustering are shown in Table 6. For both languages, our model significantly increases precision, with a modest boost for memory as well. This confirms our results in the segmentation task, where our model can effectively remove erroneous attachments while encouraging words to form close, coherent families."}, {"heading": "5.3 Root detection", "text": "Table 7 summarizes the results for the root recognition task. Our model shows consistent improvements over the baseline in all three languages. We also include the results in the test set of two monitored systems: Morfette (Chrupala et al., 2008) and Chipmunk (Cotterell et al., 2015). Morfette is a string converter, while Chipmunk is a segmentator. Both systems have access to morphologically annotated corporations. Our model is quite competitive compared to Morfette. In fact, it achieves higher accuracy for English and Turkish. Compared to Chipmunk, our model achieves 0.65 versus 0.70 for English and bridges the gap significantly. However, the high accuracy for morphologically complex languages such as Turkish and German suggests that unattended root recognition remains a difficult task."}, {"heading": "6 Conclusions", "text": "In this paper, we focus on the unattended modeling of morphological families and jointly define a forest above the language vocabulary. This formulation allows us to consider both local and global properties of morphological mappings. The resulting goal is solved by Integer Linear Programming (ILP) paired with contrasting estimation. Our experiments show that our model achieves consistent gains over the best published results for three morphological tasks."}, {"heading": "Acknowledgement", "text": "We thank Tao Lei, Yuan Zhang and the members of MIT's NLP group for helpful discussions and feedback, as well as anonymous reviewers for their insightful comments."}], "references": [{"title": "Polyglot: Distributed word representations for multilingual nlp", "author": ["Rami Al-Rfou", "Bryan Perozzi", "Steven Skiena."], "venue": "Proceedings of the Seventeenth Conference on Computational Natural Language Learning, pages 183\u2013192, Sofia, Bulgaria, August. Association", "citeRegEx": "Al.Rfou et al\\.,? 2013", "shortCiteRegEx": "Al.Rfou et al\\.", "year": 2013}, {"title": "The CELEX lexical data base on CD-ROM", "author": ["R Harald Baayen", "Richard Piepenbrock", "Rijn van H"], "venue": null, "citeRegEx": "Baayen et al\\.,? \\Q1993\\E", "shortCiteRegEx": "Baayen et al\\.", "year": 1993}, {"title": "Global learning of typed entailment rules", "author": ["Jonathan Berant", "Ido Dagan", "Jacob Goldberger."], "venue": "Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies-Volume 1, pages 610\u2013619. Asso-", "citeRegEx": "Berant et al\\.,? 2011", "shortCiteRegEx": "Berant et al\\.", "year": 2011}, {"title": "Learning morphology with Morfette", "author": ["Grzegorz Chrupala", "Georgiana Dinu", "Josef van Genabith."], "venue": "LREC.", "citeRegEx": "Chrupala et al\\.,? 2008", "shortCiteRegEx": "Chrupala et al\\.", "year": 2008}, {"title": "On shortest arborescence of a directed graph", "author": ["Yoeng-Jin Chu", "Tseng-Hong Liu."], "venue": "Scientia Sinica, 14(10):1396.", "citeRegEx": "Chu and Liu.,? 1965", "shortCiteRegEx": "Chu and Liu.", "year": 1965}, {"title": "Global inference for sentence compression: An integer linear programming approach", "author": ["James Clarke", "Mirella Lapata."], "venue": "Journal of Artificial Intelligence Research, pages 399\u2013429.", "citeRegEx": "Clarke and Lapata.,? 2008", "shortCiteRegEx": "Clarke and Lapata.", "year": 2008}, {"title": "Labeled morphological segmentation with semi-Markov models", "author": ["Ryan Cotterell", "Thomas M\u00fcller", "Alexander Fraser", "Hinrich Sch\u00fctze."], "venue": "CoNLL 2015, page 164.", "citeRegEx": "Cotterell et al\\.,? 2015", "shortCiteRegEx": "Cotterell et al\\.", "year": 2015}, {"title": "Inducing the morphological lexicon of a natural language from unannotated text", "author": ["Mathias Creutz", "Krista Lagus."], "venue": "Proceedings of the International and Interdisciplinary Conference on Adaptive Knowledge Representation and Reasoning (AKRR05), vol-", "citeRegEx": "Creutz and Lagus.,? 2005", "shortCiteRegEx": "Creutz and Lagus.", "year": 2005}, {"title": "Unsupervised models for morpheme segmentation and morphology learning", "author": ["Mathias Creutz", "Krista Lagus."], "venue": "ACM Transactions on Speech and Language Processing (TSLP), 4(1):3.", "citeRegEx": "Creutz and Lagus.,? 2007", "shortCiteRegEx": "Creutz and Lagus.", "year": 2007}, {"title": "Graphical models over multiple strings", "author": ["Markus Dreyer", "Jason Eisner."], "venue": "Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing: Volume 1, pages 101\u2013110. Association for Computational Linguistics.", "citeRegEx": "Dreyer and Eisner.,? 2009", "shortCiteRegEx": "Dreyer and Eisner.", "year": 2009}, {"title": "Optimum branchings", "author": ["Jack Edmonds."], "venue": "Journal of Research of the National Bureau of Standards B, 71(4):233\u2013240.", "citeRegEx": "Edmonds.,? 1967", "shortCiteRegEx": "Edmonds.", "year": 1967}, {"title": "Morpho-syntactic lexicon generation using graph-based semi-supervised learning", "author": ["Manaal Faruqui", "Ryan McDonald", "Radu Soricut."], "venue": "Transactions of the Association for Computational Linguistics, 4:1\u2013", "citeRegEx": "Faruqui et al\\.,? 2016", "shortCiteRegEx": "Faruqui et al\\.", "year": 2016}, {"title": "Priors in Bayesian learning of phonological rules", "author": ["Sharon Goldwater", "Mark Johnson."], "venue": "Proceedings of the 7th Meeting of the ACL Special Interest Group in Computational Phonology: Current Themes in Computational Phonology and Morphology, pages", "citeRegEx": "Goldwater and Johnson.,? 2004", "shortCiteRegEx": "Goldwater and Johnson.", "year": 2004}, {"title": "A Bayesian framework for word segmentation: Exploring the effects of context", "author": ["Sharon Goldwater", "Thomas L Griffiths", "Mark Johnson."], "venue": "Cognition, 112(1):21\u201354.", "citeRegEx": "Goldwater et al\\.,? 2009", "shortCiteRegEx": "Goldwater et al\\.", "year": 2009}, {"title": "Adam: A method for stochastic optimization", "author": ["Diederik Kingma", "Jimmy Ba."], "venue": "arXiv preprint arXiv:1412.6980.", "citeRegEx": "Kingma and Ba.,? 2014", "shortCiteRegEx": "Kingma and Ba.", "year": 2014}, {"title": "Modeling syntactic context improves morphological segmentation", "author": ["Yoong Keok Lee", "Aria Haghighi", "Regina Barzilay."], "venue": "Proceedings of the Fifteenth Conference on Computational Natural Language Learning, pages 1\u20139. Association for Computa-", "citeRegEx": "Lee et al\\.,? 2011", "shortCiteRegEx": "Lee et al\\.", "year": 2011}, {"title": "Arabic treebank: Part 1 v 2.0. Distributed by the Linguistic Data Consortium. LDC Catalog No.: LDC2003T06", "author": ["Mohamed Maamouri", "Ann Bies", "Hubert Jin", "Tim Buckwalter"], "venue": null, "citeRegEx": "Maamouri et al\\.,? \\Q2003\\E", "shortCiteRegEx": "Maamouri et al\\.", "year": 2003}, {"title": "Efficient estimation of word representations in vector space", "author": ["Tomas Mikolov", "Kai Chen", "Greg Corrado", "Jeffrey Dean."], "venue": "arXiv preprint arXiv:1301.3781.", "citeRegEx": "Mikolov et al\\.,? 2013", "shortCiteRegEx": "Mikolov et al\\.", "year": 2013}, {"title": "Unsupervised bilingual morpheme segmentation and alignment with context-rich hidden semi-Markov models", "author": ["Jason Naradowsky", "Kristina Toutanova."], "venue": "Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Lan-", "citeRegEx": "Naradowsky and Toutanova.,? 2011", "shortCiteRegEx": "Naradowsky and Toutanova.", "year": 2011}, {"title": "An unsupervised method for uncovering morphological chains", "author": ["Karthik Narasimhan", "Regina Barzilay", "Tommi Jaakkola."], "venue": "Transactions of the Association for Computational Linguistics, 3:157\u2013167.", "citeRegEx": "Narasimhan et al\\.,? 2015", "shortCiteRegEx": "Narasimhan et al\\.", "year": 2015}, {"title": "Arabic gigaword fifth edition ldc2011t11", "author": ["Robert Parker", "David Graff", "Ke Chen", "Junbo Kong", "Kazuaki Maeda."], "venue": "Philadelphia: Linguistic Data Consortium.", "citeRegEx": "Parker et al\\.,? 2011", "shortCiteRegEx": "Parker et al\\.", "year": 2011}, {"title": "Dual decomposition inference for graphical models over strings", "author": ["Nanyun Peng", "Ryan Cotterell", "Jason Eisner."], "venue": "Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, pages 917\u2013927, Lisbon, Portugal, September. As-", "citeRegEx": "Peng et al\\.,? 2015", "shortCiteRegEx": "Peng et al\\.", "year": 2015}, {"title": "Unsupervised morphological segmentation with log-linear models", "author": ["Hoifung Poon", "Colin Cherry", "Kristina Toutanova."], "venue": "Proceedings of Human Language Technologies: The 2009 Annual Conference of the North American Chapter of the Association for", "citeRegEx": "Poon et al\\.,? 2009", "shortCiteRegEx": "Poon et al\\.", "year": 2009}, {"title": "Relational learning via propositional algorithms: An information extraction case study", "author": ["Dan Roth", "Wen-tau Yih."], "venue": "International Joint Conference on Artificial Intelligence, volume 17, pages 1257\u20131263. LAWRENCE ERLBAUM ASSOCIATES LTD.", "citeRegEx": "Roth and Yih.,? 2001", "shortCiteRegEx": "Roth and Yih.", "year": 2001}, {"title": "Turkish language resources: Morphological parser, morphological disambiguator and web corpus", "author": ["Ha\u015fim Sak", "Tunga G\u00fcng\u00f6r", "Murat Sara\u00e7lar."], "venue": "Advances in natural language processing, pages 417\u2013 427. Springer.", "citeRegEx": "Sak et al\\.,? 2008", "shortCiteRegEx": "Sak et al\\.", "year": 2008}, {"title": "Knowledgefree induction of morphology using latent semantic analysis", "author": ["Patrick Schone", "Daniel Jurafsky."], "venue": "Proceedings of the 2nd workshop on Learning language in logic and the 4th conference on Computational natural language learning-Volume 7,", "citeRegEx": "Schone and Jurafsky.,? 2000", "shortCiteRegEx": "Schone and Jurafsky.", "year": 2000}, {"title": "Minimallysupervised morphological segmentation using adaptor grammars", "author": ["Kairit Sirts", "Sharon Goldwater."], "venue": "Transactions of the Association for Computational Linguistics, 1:255\u2013266.", "citeRegEx": "Sirts and Goldwater.,? 2013", "shortCiteRegEx": "Sirts and Goldwater.", "year": 2013}, {"title": "Contrastive estimation: Training log-linear models on unlabeled data", "author": ["Noah A Smith", "Jason Eisner."], "venue": "Proceedings of the 43rd Annual Meeting on Association for Computational Linguistics, pages 354\u2013362. Association for Computational Linguistics.", "citeRegEx": "Smith and Eisner.,? 2005", "shortCiteRegEx": "Smith and Eisner.", "year": 2005}, {"title": "Unsupervised multilingual learning for morphological segmentation", "author": ["Benjamin Snyder", "Regina Barzilay."], "venue": "ACL, pages 737\u2013745.", "citeRegEx": "Snyder and Barzilay.,? 2008", "shortCiteRegEx": "Snyder and Barzilay.", "year": 2008}, {"title": "Unsupervised morphology induction using word embeddings", "author": ["Radu Soricut", "Franz Och."], "venue": "Proc. NAACL.", "citeRegEx": "Soricut and Och.,? 2015", "shortCiteRegEx": "Soricut and Och.", "year": 2015}, {"title": "Unsupervised morphology rivals supervised morphology for Arabic MT", "author": ["David Stallard", "Jacob Devlin", "Michael Kayser", "Yoong Keok Lee", "Regina Barzilay."], "venue": "Proceedings of the 50th Annual Meeting of the Association for Computational", "citeRegEx": "Stallard et al\\.,? 2012", "shortCiteRegEx": "Stallard et al\\.", "year": 2012}, {"title": "Empirical comparison of evaluation methods for unsupervised learning of morphology", "author": ["Sami Virpioja", "Ville T. Turunen", "Sebastian Spiegler", "Oskar Kohonen", "Mikko Kurimo."], "venue": "Traitement Automatique des Langues, 52(2):45\u201390.", "citeRegEx": "Virpioja et al\\.,? 2011", "shortCiteRegEx": "Virpioja et al\\.", "year": 2011}, {"title": "Morfessor 2.0: Python implementation and extensions for Morfessor baseline", "author": ["Sami Virpioja", "Peter Smit", "Stig-Arne Gr\u00f6nroos", "Mikko Kurimo"], "venue": null, "citeRegEx": "Virpioja et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Virpioja et al\\.", "year": 2013}, {"title": "Dsolvemorphological segmentation for German using conditional random fields", "author": ["Kay-Michael W\u00fcrzner", "Bryan Jurish."], "venue": "Systems and Frameworks for Computational Morphology, pages 94\u2013103. Springer.", "citeRegEx": "W\u00fcrzner and Jurish.,? 2015", "shortCiteRegEx": "W\u00fcrzner and Jurish.", "year": 2015}], "referenceMentions": [{"referenceID": 19, "context": "7%, relative to the best published results (Narasimhan et al., 2015).", "startOffset": 43, "endOffset": 68}, {"referenceID": 22, "context": "Unsupervised morphological segmentation Most top performing algorithms for unsupervised segmentation today center around modeling singlestep derivations (Poon et al., 2009; Naradowsky and Toutanova, 2011; Narasimhan et al., 2015).", "startOffset": 153, "endOffset": 229}, {"referenceID": 18, "context": "Unsupervised morphological segmentation Most top performing algorithms for unsupervised segmentation today center around modeling singlestep derivations (Poon et al., 2009; Naradowsky and Toutanova, 2011; Narasimhan et al., 2015).", "startOffset": 153, "endOffset": 229}, {"referenceID": 19, "context": "Unsupervised morphological segmentation Most top performing algorithms for unsupervised segmentation today center around modeling singlestep derivations (Poon et al., 2009; Naradowsky and Toutanova, 2011; Narasimhan et al., 2015).", "startOffset": 153, "endOffset": 229}, {"referenceID": 19, "context": "However, these models generally bypass global constraints (Narasimhan et al., 2015) or require performing inference over very large spaces (Poon et al.", "startOffset": 58, "endOffset": 83}, {"referenceID": 22, "context": ", 2015) or require performing inference over very large spaces (Poon et al., 2009).", "startOffset": 63, "endOffset": 82}, {"referenceID": 8, "context": "In contrast, earlier work focuses on modeling global morphological assignment, using generative probabilistic models (Creutz and Lagus, 2007; Snyder and Barzilay, 2008; Goldwater et al., 2009; Sirts and Goldwater, 2013).", "startOffset": 117, "endOffset": 219}, {"referenceID": 28, "context": "In contrast, earlier work focuses on modeling global morphological assignment, using generative probabilistic models (Creutz and Lagus, 2007; Snyder and Barzilay, 2008; Goldwater et al., 2009; Sirts and Goldwater, 2013).", "startOffset": 117, "endOffset": 219}, {"referenceID": 13, "context": "In contrast, earlier work focuses on modeling global morphological assignment, using generative probabilistic models (Creutz and Lagus, 2007; Snyder and Barzilay, 2008; Goldwater et al., 2009; Sirts and Goldwater, 2013).", "startOffset": 117, "endOffset": 219}, {"referenceID": 26, "context": "In contrast, earlier work focuses on modeling global morphological assignment, using generative probabilistic models (Creutz and Lagus, 2007; Snyder and Barzilay, 2008; Goldwater et al., 2009; Sirts and Goldwater, 2013).", "startOffset": 117, "endOffset": 219}, {"referenceID": 9, "context": "Graph-based representations in computational morphology Variants of a graph-based representation have been used to model various morphological phenomena (Dreyer and Eisner, 2009; Peng et al., 2015; Soricut and Och, 2015; Faruqui et al., 2016).", "startOffset": 153, "endOffset": 242}, {"referenceID": 21, "context": "Graph-based representations in computational morphology Variants of a graph-based representation have been used to model various morphological phenomena (Dreyer and Eisner, 2009; Peng et al., 2015; Soricut and Och, 2015; Faruqui et al., 2016).", "startOffset": 153, "endOffset": 242}, {"referenceID": 29, "context": "Graph-based representations in computational morphology Variants of a graph-based representation have been used to model various morphological phenomena (Dreyer and Eisner, 2009; Peng et al., 2015; Soricut and Och, 2015; Faruqui et al., 2016).", "startOffset": 153, "endOffset": 242}, {"referenceID": 11, "context": "Graph-based representations in computational morphology Variants of a graph-based representation have been used to model various morphological phenomena (Dreyer and Eisner, 2009; Peng et al., 2015; Soricut and Och, 2015; Faruqui et al., 2016).", "startOffset": 153, "endOffset": 242}, {"referenceID": 23, "context": "ILP for capturing global properties Integer Linear Programming has been successfully employed to capture global constraints across multiple applications such as information extraction (Roth and Yih, 2001), sentence compression (Clarke and Lapata, 2008), and textual entailment (Berant et al.", "startOffset": 184, "endOffset": 204}, {"referenceID": 5, "context": "ILP for capturing global properties Integer Linear Programming has been successfully employed to capture global constraints across multiple applications such as information extraction (Roth and Yih, 2001), sentence compression (Clarke and Lapata, 2008), and textual entailment (Berant et al.", "startOffset": 227, "endOffset": 252}, {"referenceID": 2, "context": "ILP for capturing global properties Integer Linear Programming has been successfully employed to capture global constraints across multiple applications such as information extraction (Roth and Yih, 2001), sentence compression (Clarke and Lapata, 2008), and textual entailment (Berant et al., 2011).", "startOffset": 277, "endOffset": 298}, {"referenceID": 19, "context": "For instance, the model of Narasimhan et al. (2015) produces 617 unique affixes when segmenting 10000 English words.", "startOffset": 27, "endOffset": 52}, {"referenceID": 19, "context": "work (Narasimhan et al., 2015), we model this probability using a log-linear model:", "startOffset": 5, "endOffset": 30}, {"referenceID": 27, "context": "Instead, we make use of contrastive estimation (Smith and Eisner, 2005), substituting \u03a3\u2217 with a (limited) set of neighbor strings N(w) that are orthographically close to w.", "startOffset": 47, "endOffset": 71}, {"referenceID": 19, "context": "We obtain N(w) by transposing characters in w, following the method described in Narasimhan et al. (2015). Now for the forest over the set of nodes V , the log-likelihood loss function is defined as:", "startOffset": 81, "endOffset": 106}, {"referenceID": 12, "context": "In addition to suffixation and prefixation, we also consider three types of transformations introduced in Goldwater and Johnson (2004): repetition, deletion, and modification.", "startOffset": 106, "endOffset": 135}, {"referenceID": 19, "context": "Features We use the same set of features shown to be effective in prior work (Narasimhan et al., 2015), including word vector similarity, beginning and ending character bigrams, word frequencies and affixes.", "startOffset": 77, "endOffset": 102}, {"referenceID": 4, "context": "If we had prior knowledge of words belonging to the same family, we can frame the problem as growing a Minimum Spanning Tree (MST), and use Chu-Liu-Edmonds algorithm (Chu and Liu, 1965; Edmonds, 1967) to solve it.", "startOffset": 166, "endOffset": 200}, {"referenceID": 10, "context": "If we had prior knowledge of words belonging to the same family, we can frame the problem as growing a Minimum Spanning Tree (MST), and use Chu-Liu-Edmonds algorithm (Chu and Liu, 1965; Edmonds, 1967) to solve it.", "startOffset": 166, "endOffset": 200}, {"referenceID": 17, "context": "In addition, we train word vectors (Mikolov et al., 2013) to obtain cosine similarity features.", "startOffset": 35, "endOffset": 57}, {"referenceID": 18, "context": "Following Narasimhan et al. (2015), we reduce the noise by truncating the training word list to the top K frequent words.", "startOffset": 10, "endOffset": 35}, {"referenceID": 7, "context": "(2015) which outperforms a number of alternative approaches (Creutz and Lagus, 2005; Virpioja et al., 2013; Sirts and Goldwater, 2013; Lee et al., 2011; Stallard et al., 2012; Poon et al., 2009).", "startOffset": 60, "endOffset": 194}, {"referenceID": 32, "context": "(2015) which outperforms a number of alternative approaches (Creutz and Lagus, 2005; Virpioja et al., 2013; Sirts and Goldwater, 2013; Lee et al., 2011; Stallard et al., 2012; Poon et al., 2009).", "startOffset": 60, "endOffset": 194}, {"referenceID": 26, "context": "(2015) which outperforms a number of alternative approaches (Creutz and Lagus, 2005; Virpioja et al., 2013; Sirts and Goldwater, 2013; Lee et al., 2011; Stallard et al., 2012; Poon et al., 2009).", "startOffset": 60, "endOffset": 194}, {"referenceID": 15, "context": "(2015) which outperforms a number of alternative approaches (Creutz and Lagus, 2005; Virpioja et al., 2013; Sirts and Goldwater, 2013; Lee et al., 2011; Stallard et al., 2012; Poon et al., 2009).", "startOffset": 60, "endOffset": 194}, {"referenceID": 30, "context": "(2015) which outperforms a number of alternative approaches (Creutz and Lagus, 2005; Virpioja et al., 2013; Sirts and Goldwater, 2013; Lee et al., 2011; Stallard et al., 2012; Poon et al., 2009).", "startOffset": 60, "endOffset": 194}, {"referenceID": 22, "context": "(2015) which outperforms a number of alternative approaches (Creutz and Lagus, 2005; Virpioja et al., 2013; Sirts and Goldwater, 2013; Lee et al., 2011; Stallard et al., 2012; Poon et al., 2009).", "startOffset": 60, "endOffset": 194}, {"referenceID": 16, "context": "Baselines We compare our approach against the state-of-the-art unsupervised method of Narasimhan et al. (2015) which outperforms a number of alternative approaches (Creutz and Lagus, 2005; Virpioja et al.", "startOffset": 86, "endOffset": 111}, {"referenceID": 24, "context": "Table 1: Data statistics: MC-10 = MorphoChallenge 2010 , MC:05-10 = aggregated from MorphoChallenge 2005-2010, BOUN = BOUN corpus (Sak et al., 2008), Gigaword = Arabic Gigaword corpus (Parker et al.", "startOffset": 130, "endOffset": 148}, {"referenceID": 20, "context": ", 2008), Gigaword = Arabic Gigaword corpus (Parker et al., 2011), ATB = Arabic Treebank (Maamouri et al.", "startOffset": 43, "endOffset": 64}, {"referenceID": 16, "context": ", 2011), ATB = Arabic Treebank (Maamouri et al., 2003).", "startOffset": 31, "endOffset": 54}, {"referenceID": 0, "context": "Dsolve is the dataset released by W\u00fcrzner and Jurish (2015), and for training German vectors, we use the pre-processed Wikipedia dump from (Al-Rfou et al., 2013).", "startOffset": 139, "endOffset": 161}, {"referenceID": 15, "context": ", 2011), ATB = Arabic Treebank (Maamouri et al., 2003). Duplicates in Arabic test set are filtered. Dsolve is the dataset released by W\u00fcrzner and Jurish (2015), and for training German vectors, we use the pre-processed Wikipedia dump from (Al-Rfou et al.", "startOffset": 32, "endOffset": 160}, {"referenceID": 31, "context": "Evaluation metric Following prior work (Virpioja et al., 2011), we evaluate all models using the standard boundary precision and recall (BPR).", "startOffset": 39, "endOffset": 62}, {"referenceID": 14, "context": "Training For unsupervised training, we use the gradient descent method ADAM (Kingma and Ba, 2014) and optimize over the whole batch of training words.", "startOffset": 76, "endOffset": 97}, {"referenceID": 1, "context": "Data To obtain gold information about morphological clusters, we use CELEX (Baayen et al., 1993).", "startOffset": 75, "endOffset": 96}, {"referenceID": 25, "context": "Evaluation We use the metrics proposed by Schone and Jurafsky (2000). Specifically, let Xw", "startOffset": 42, "endOffset": 69}, {"referenceID": 6, "context": "Data We report the results on the Chipmunk dataset (Cotterell et al., 2015) which has been used for evaluating supervised models for root detection.", "startOffset": 51, "endOffset": 75}, {"referenceID": 19, "context": "Table 4: Segmentation results for the supervised model and three unsupervised models: the state-ofthe-art system NBJ\u201915 (Narasimhan et al., 2015), our improved implementation of their system NBJImp and our model.", "startOffset": 120, "endOffset": 145}, {"referenceID": 19, "context": "Narasimhan et al. (2015) observe that after including more than K = 10000 words, the performance of the unsupervised model drops noticeably.", "startOffset": 0, "endOffset": 25}, {"referenceID": 3, "context": "We also include the results on the test set of two supervised systems: Morfette (Chrupala et al., 2008) and Chipmunk (Cotterell et al.", "startOffset": 80, "endOffset": 103}, {"referenceID": 6, "context": ", 2008) and Chipmunk (Cotterell et al., 2015).", "startOffset": 21, "endOffset": 45}, {"referenceID": 6, "context": "Numbers for Morfette and Chipmunk are reported by Cotterell et al. (2015).", "startOffset": 50, "endOffset": 74}], "year": 2017, "abstractText": "This paper focuses on unsupervised modeling of morphological families, collectively comprising a forest over the language vocabulary. This formulation enables us to capture edgewise properties reflecting single-step morphological derivations, along with global distributional properties of the entire forest. These global properties constrain the size of the affix set and encourage formation of tight morphological families. The resulting objective is solved using Integer Linear Programming (ILP) paired with contrastive estimation. We train the model by alternating between optimizing the local log-linear model and the global ILP objective. We evaluate our system on three tasks: root detection, clustering of morphological families and segmentation. Our experiments demonstrate that our model yields consistent gains in all three tasks compared with the best published results.1", "creator": "LaTeX with hyperref package"}}}