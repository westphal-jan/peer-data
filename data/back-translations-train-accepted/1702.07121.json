{"id": "1702.07121", "review": {"conference": "ICML", "VERSION": "v1", "DATE_OF_SUBMISSION": "23-Feb-2017", "title": "Consistent On-Line Off-Policy Evaluation", "abstract": "The problem of on-line off-policy evaluation (OPE) has been actively studied in the last decade due to its importance both as a stand-alone problem and as a module in a policy improvement scheme. However, most Temporal Difference (TD) based solutions ignore the discrepancy between the stationary distribution of the behavior and target policies and its effect on the convergence limit when function approximation is applied. In this paper we propose the Consistent Off-Policy Temporal Difference (COP-TD($\\lambda$, $\\beta$)) algorithm that addresses this issue and reduces this bias at some computational expense. We show that COP-TD($\\lambda$, $\\beta$) can be designed to converge to the same value that would have been obtained by using on-policy TD($\\lambda$) with the target policy. Subsequently, the proposed scheme leads to a related and promising heuristic we call log-COP-TD($\\lambda$, $\\beta$). Both algorithms have favorable empirical results to the current state of the art on-line OPE algorithms. Finally, our formulation sheds some new light on the recently proposed Emphatic TD learning.", "histories": [["v1", "Thu, 23 Feb 2017 07:44:43 GMT  (427kb)", "http://arxiv.org/abs/1702.07121v1", null]], "reviews": [], "SUBJECTS": "stat.ML cs.LG", "authors": ["assaf hallak", "shie mannor"], "accepted": true, "id": "1702.07121"}, "pdf": {"name": "1702.07121.pdf", "metadata": {"source": "META", "title": "Consistent On-Line Off-Policy Evaluation", "authors": ["Assaf Hallak", "Shie Mannor"], "emails": ["<ifogph@gmail.com>,", "<shie@ee.technion.ac.il>."], "sections": [{"heading": null, "text": "ar Xiv: 170 2.07 121v 1 [stat.ML] 2 3Fe b20 17 (OPE) has been actively studied over the past decade due to its importance both as a standalone problem and as a module of a policy improvement program. However, most solutions based on Temporal Difference (TD) ignore the discrepancy between the stationary distribution of behavior and target policy and its impact on the convergence boundary of functional convergence. In this paper, we propose the Consistent Off-Policy Temporal Difference (COP-TD (\u03bb, \u03b2) algorithm that addresses this problem and reduces this distortion to a certain amount of computational effort. We show that COP-TD (\u03bb, \u03b2) can be designed to approximate the same value that would have been achieved by using on-policy TD (\u03bb) with target policy."}, {"heading": "1. Introduction", "text": "Reinforcement Learning (RL) techniques have been used successfully in areas such as robotics, games, marketing and more (Kober et al., 2013; Al-Rawi et al., 2015; Barrett et al., 2013).We look at the problem of Offpolicy Evaluation (OPE) - assessing the performance of a complex strategy without applying it. OPE formulation is often considered in areas with limited sampling capacities, for example marketing and recommendation systems (Theocharous & Hallak, 2013; Theocharous et al., 2015) are directly related to policy without applying it. A more extreme example is drug administration, as there are few patients in the test population, and suboptimal policies. 1The Technion, Haifa, Israel. Correspondence to: Assaf Hallak < ifogph @ gmail.com >, Shie Mannor < shonee.techni.acil >."}, {"heading": "2. Notations and Background", "text": "We consider the standard Discounted Markov Decision Process (MDP) formulation (Bertsekas & Tsitsiklis, 1996) with a single long trajectory. Let M = (S, A, P, R) be an MDP in which S is the finite state space and A is the finite action space. Parameter P sets the transition probabilities Pr (s, a), with the discount factor being the exponential reduction of the reward with time.The process proceeds as follows: A state s0 is determined by the distribution. Parameter R sets the reward distribution r (s, a), which is achieved by taking measures a in the state s and p, is the discount factor, which is the exponential reduction of the reward with time.The state s0 is sampled by the distribution (s).Then, in each step t = 0 the agent draws an action according to the stochastic behavior policy."}, {"heading": "3. Previous Work", "text": "We can roughly categorize the existing OPE algorithms into two main families, gradient-based methods that exhibit a very slow gradient descent to error terms they want to minimize, including GTD (Sutton et al., 2009a), GTD2, TDC (Sutton et al., 2009b) and HTD (White & White, 2016). The main disadvantages of gradient-based methods are (A) they usually update an additional error-correcting term, which means that another time-step parameter needs to be controlled; and (B) they rely on the estimation of non-trivial terms, an estimate that tends to converge slowly; the other family uses meaning-sampling (IS) methods that correct the gains between policies and non-political updates using IS ratios, including full IS terms (Precup et al., 2001) and ETton terms (Sutton, 2015)."}, {"heading": "4. Motivation", "text": "Here we offer a motivating example which shows that even in simple cases with \"rigid\" behavioural and target targets the two induced stationary distributions can differ greatly from each other. The selection of a specific linear parameterisation further underlines the difference between the application of the on-political TD with the on-policy and the application of inconsistent off-policy TD.Suppose that a chain MDP with numbered states 1, 2, 2, 3, 4, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5"}, {"heading": "5.1. Relation to ETD(\u03bb, \u03b2)", "text": "Recently Sutton et al. (2015) proposed an algorithm for the external evaluation of resources, which White & White (2016) called Emphatic TD. This algorithm was later extended by Hallak et al. (2015) and renamed ETD (\u03bb, \u03b2), which proved to be very good empirically. ETD (0, \u03b2) can be represented as: Ft = (1 \u2212 \u03b2) \u221e n = 0\u03b2nnt, V-t + 1 (st) = V-t (st) + \u03b1tFt\u03c1t (rt + \u03b8 t (0 \u2212 1) \u2212 (st)). (8) In contrast to the aforementioned, ETD (\u03bb, \u03b2) converges to the fixed point of \u0445fT-II (Yu, 2015), where f = E [Ft | st] = (I \u2212 \u03b2P\u03c0) \u2212 p-effective."}, {"heading": "6. The Logarithm Approach for Handling Long Products", "text": "We now present a heuristic algorithm that works in a similar way to COP-TD (\u03bb, \u03b2). Before we present the algorithm, we explain the motivation that underlies it."}, {"heading": "6.1. Statistical Interpretation of TD(\u03bb)", "text": "Konidaris et al. (2011) suggested a statistical interpretation of TD (\u03bb \u03b2), showing that among several assumptions, the TD (\u03bb) estimate R\u03bbst is the maximum probability estimate of V (st) vis-\u00e0-vis R n st: (1) Each Rnst is an unbiased estimator of V (st); (2) The random variables R n st are independent and specifically uncorrelated; (3) The random variables Rnst are jointly normally distributed; and (4) The variance of each Rnst is proportional to \u03bb. Among assumptions 1-3, the maximum probability estimate of V (s) st is represented by its previous estimate as a linear convex combination of Rnst with weights: = [Var (n) st) st) st) st)] - 1 (Var (m) st = 0 (Var (m) st) st) st) st) st)) st = 1. \u2212 1."}, {"heading": "6.2. Variance Weighted \u0393nt", "text": "As Konidaris et al. (2011) have shown, we can use state-dependent weights instead of \u03b2-exponents to get better estimations.The second moments are explicitly given as: 3: E [(amounnt) 2 | st] = d'\u00b5 P-n-1estd\u00b5 (st), where [P] s, s \"= \u2211 a-A \u03c02 (a-s) \u00b5 (a-s) P (s-s, a).These can be estimated separately for each state. Note that the deviations increase exponentially depending on the largest eigenvalue of P-s (as Assumption 4 prescribes), but this is merely an asymptotic behavior and can only be relevant if the weights are already negligible. Therefore, implementing this solution online should not be a problem with the different weights, as usually only the first few of them are not zero. While this solution may be similar to problems with large states that have these parameters parametric or specific deviations (Thomas et al)."}, {"heading": "6.3. Log-COP-TD(\u03bb, \u03b2)", "text": "Assumption 3 in the previous paragraph is that the random estimators (R (n), \u03b2), \u03b2 (n) and \u03b2 (n) are normally distributed, because in politics TD (\u03b2) this assumption might not seem too hard, since the estimators represent R (n) growing sums of random variables. In our case, however, the estimators (\u03b2) growing products of random variables. To solve this problem, we can define new estimators using a logarithm for each of these variables. (E [E [n) [E [n] d (st \u2212 m) t \u2212 k = t \u2212 t st]. (n) Log [n]. (n) Log [p \u2212 m)."}, {"heading": "6.4. Using the Original Features", "text": "An interesting phenomenon occurs when the behavioral and target guidelines use a characteristic based on the Boltzmann distribution to select the actions: \u00b5 (a | s) = exp (\u03b8 a, \u00b5\u03c6 (s)) and \u03c0 (a | s) = exp (\u03b8 a, \u03c0\u03c6 (s)), adding a constant characteristic to remove the (possibly different) normalizing constant. Thus, log (\u03c1t) = (\u03b8a, \u03c0 \u2212 \u03b8a, \u00b5) \u03c6 (st) and LogCOP-TD (\u03bb, \u03b2) take a parametric form depending on the original characteristics rather than another proposition."}, {"heading": "6.5. Approximation Hardness", "text": "Since we propose to use linear functional approximation for \u03c1d (s) and log (\u03c1d (s), one cannot help but wonder how difficult it is to approximate these quantities, especially compared to the value function. The comparison between V (s) and \u03c1d (s) is problematic for several reasons: 1. The ultimate goal is to estimate V \u03c0 (s), approximation errors in \u03c1d (s) are second-order terms. 2. The value function V \u03c0 (s) depends on the policy-induced reward function and transition probability matrix, while \u03c1d (s) depends on the stationary distributions induced by both strategies. Since each depends on at least one unique factor - we can expect different arrangements to lead to different approximation hardness."}, {"heading": "7. Experiments", "text": "We have performed 3 types of experiments, while our first series of experiments (Figure 1) shows the accuracy of predicting the behavior of two cars (\u03b2 \u03b2 \u03b2 \u03b2 \u03b2) estimated by both COP-TD (\u03bb, \u03b2) and log COP-TD (\u03bb, \u03b2). We show two types of settings in which the visualization of \u03c1d is relatively clear - the chain MDP example mentioned, so they are simply set to 0, we show the estimated speed after 106 iterations. For the chain MDP (top two plots, note the logarithmic scale) we first of all have no functional approximation (top-left) and we can see COP-TDmanages for convergence to the correct value."}, {"heading": "8. Conclusion", "text": "While a plethora of algorithms have been proposed so far, ETD (\u03bb, \u03b2) by Hallak et al. (2015) has perhaps the simplest formulation and the most theoretical properties. Unfortunately, ETD (\u03bb, \u03b2) does not converge to the same point achieved by online TD when using linear functional approximations. We address this problem with COP-TD (\u03bb, \u03b2) and have proven that, when used correctly, it can achieve consistency or at least compensate for some of the bias by adding or removing features. Although we require a new set of features and calibrate an additional updating function, the performance of COP-TD (\u03bb, \u03b2) does not depend as much on \u03b2 as ETD (\u03bb, \u03b2) and shows promising empirical results. We provide a link to the statistical interpretation of TD (\u03bb) that motivates our entire formulation. This interpretation leads to two additional approaches: an observation of actual deviations (and) deviations."}, {"heading": "9.1. Proof of Lemma 1", "text": "If the step variables \u03b1t = 0 \u03b1t = \u221e, \u2211 \u221e t = 0 \u03b1 2 t < \u221e then the process described in Eq.4 almost certainly relates to the fixed point \u03b5\u03c0T\u03c0V = V.Proof. Similar to practical politics TD, we define A and b, the fixed point is the solution for A\u03b8 = b. Firstly, we find A and show stability: A = lim t \u2192 \u221e E\u00b5 (s) \u03c6t (\u03c6t \u2212 \u03b3t + 1) (s) \u03c1d (s) \u03c1d (s) E\u00b5 (\u03c6k \u2212 \u03b3k + 1). This is exactly the same policy as in politics TD (0) and it is negative in politics."}, {"heading": "9.2. Proof of Lemma 2", "text": "Let us make an unbiased estimate of what we are doing, and for each n = 0, 1,.., t define what we need to do: E\u00b5 [...] n (st \u2212 n) n t. Then: E\u00b5 [...] n (st) n (st) n (st). Evidence. For each function on the state territory u (s): E\u00b5 [... n (st \u2212 n) | st] = \u2211 (si) t \u2212 1 i = t \u2212 nPr \u00b5 (((si) t \u2212 1 i = t \u2212 n | st) n (st \u2212 n) t u (st \u2212 n) (si) t (si) t \u2212 n (si) t \u2212 1 i = t \u2212 n, st) Pr\u00b5 (st \u2212 n, st) t (si) t \u2212 n (st \u2212 n) t (st \u2212 n) t (si) t \u2212 n (st \u2212 n) t (st \u2212 n) t \u2212 t (s) t \u2212 n (s)."}, {"heading": "9.3. Proof of Lemma 3", "text": "Under the occupational assumption, we define the eigenvalues of P\u03c0 as 0 \u2264 \u00b7 \u00b7 \u2264 | 2 | < 1 = 1. Then Y \u03b2 is a maximum of 6 = 1 (1 \u2212 \u03b2) | 1 \u2212 \u03b2\u0421i | contraction in the L2 norm on the orthogonal subspace to \u03c1d, and \u03c1d is a fixed point of Y \u03b2.We show first that \u03c1d is a fixed point of Y \u03b2: Y \u03b2: Y \u03b2\u03c1d = (1 \u2212 \u03b2) D \u2212 1 \u00b5 (1 \u2212 \u03b2) P'\u03c0 (I \u2212 \u03b2P \u03c0) \u2212 1D\u00b5 (D \u2212 1\u00b5 d\u03c0) = (1 \u2212 \u03b2) D \u2212 1\u00b5 P \u03c0 (I \u2212 \u03b2P \u03c0) \u2212 1d\u03c0 = (1 \u2212 \u03b2) D \u2212 1\u00b5 (1 \u2212 1 \u00b2), the eigenvalues of Y \u2212 \u03b2u \u03b2u are the same as those of P (1 \u2212 1 \u2212 1 \u2212 2), with eigenvalues ranging from 1 to \u2212 1."}, {"heading": "9.4. Proof of Theorem 1", "text": "If the step sizes are not sufficient to mitigate the effects of climate change, we will not ignore the effects of climate change on the world population. (...) If the step sizes are not sufficient (...), we will not be able to compensate the step sizes. (...) We assume that the step sizes are not sufficient. (...) We assume that the step sizes are not sufficient. (...) We assume that the step sizes are not sufficient. (...) We assume that the step sizes are not sufficient. (...) The fastest process is d (...), which is of course converted with the step sizes O (...). (...) We assume that the step sizes are not sufficient. (...) We do not assume that the step sizes will be interpreted. (...)"}, {"heading": "9.5. Proof of Theorem 2", "text": "If the step sizes apply the following yardsticks: d = 1 t = 1 t = 1 t = 1 t = 1 t = 2 t = 2 t = 2 t = 1 p = 2 p = 3 p = 4 p = 4 p = 4 p = 4 p = 5 p = 5 p = 5 p = 5 p = 5 p = 5 p = 5 p = 5 p = 5 p = 5 p = 5 p = 5 p = 5 p = 5 p = 5 p = 5 p = 5 p = 5 p = 5 p = 5 p p = 5 p p = 5 p p = 5 p p = 5 p = 5 p = 5 p 5 p = 5 p = 5 p 5 p = 5 p 5 p = 5 p 5 p = 5 p 5 p = 5 p 5 p = 5 p 5 p = 5 p 5 p = 5 p 5 p = 5 p 5 p = 5 p 5 p 5 p = 5 p 5 p 5 p = 5 p 5 p = 5 p 5 p 5 p = 5 p 5 p = 5 p 5 p = 5 p 5 p = 5 p 5 p = 5 p 5 p = 5 p 5 p = 5 p 5 p = 5 p 5 p = 5 p - 5 p - 5 p = 5 p - 5 p = 5 p = 5 p 5 p = 5 p 5 p = 5 p = 5 p 5 p = 5 p 5 p = 5 p 5 p = 5 p 5 p = 5 p = 5 p 5 p = 5 p 5 p 5 p = 5 p 5 p - 5 p - 5 p - 5 p - 5 p - 5 p = 5 p - 5 p - 5 p - 5 p - 5 p - 5 p = 5 p - 5 p - 5 p - 5 p - 5 p - 5 p = 5 p - 5 p - 5 p - 5 p - 5 p - 5 p - 5 p - 5 p - 5 p - 5 p = 5 p - 5 p - 5 p - 5 p - 5 p - 5 p - 5 p - 5 p - 5 p - 5 p - 5 p - 5 p - 5 p - 5 p - 5 p - 5 p - 5 p - 5 p - 5 p - 5 p - 5 p - 5 p - 5 p - 5 p - 5 p - 5 p - 5 p - 5 p - 5 p - 5 p - 5 p - 5 p - 5 p - 5 p"}, {"heading": "9.6. Proof of Corollary 1", "text": "Let's go 0 < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < <"}, {"heading": "9.7. More details on the experiments", "text": "Experiments for Figure 1: 100 state chain MDP, with the probability of moving 0.51 to the left / right for the behavior / target policy. Results were taken after T = \u03b2-6 iterations. For COP-TD, we used \u03b2 = 0 and a constant step size 0.5. For log-COP-TD, we used \u03b2 = 0, \u03b3log = 0.9999 and constant step size 0.5. The experiment was performed 10 times and the standard deviation is shown as shading in the diagram.For the experiment with the Mountaincar, we used the simulator of https: / / jamh-web.appspot.com / download.htm. The state aggregation was achieved by executing kmeans with 100 clusters and using the centers as representative states. Behavioral policy was taken to be consistent across the 3 possible measures (-1, 0, 1, 1), 1, and the target policy chose these measures with probabilities (CO1 / 1-3.1 / 2) applied independently of the state."}], "references": [{"title": "Application of reinforcement learning to routing in distributed wireless networks: a review", "author": ["Al-Rawi", "Hasan AA", "Ng", "Ming Ann", "Yau", "KokLim Alvin"], "venue": "Artificial Intelligence Review,", "citeRegEx": "Al.Rawi et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Al.Rawi et al\\.", "year": 2015}, {"title": "Applying reinforcement learning towards automating resource allocation and application scalability in the cloud", "author": ["Barrett", "Enda", "Howley", "Duggan", "Jim"], "venue": "Concurrency and Computation: Practice and Experience,", "citeRegEx": "Barrett et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Barrett et al\\.", "year": 2013}, {"title": "Dynamic Programming and Optimal Control, Vol II", "author": ["D. Bertsekas"], "venue": "Athena Scientific,", "citeRegEx": "Bertsekas,? \\Q2012\\E", "shortCiteRegEx": "Bertsekas", "year": 2012}, {"title": "Projection onto a simplex", "author": ["Chen", "Yunmei", "Ye", "Xiaojing"], "venue": "arXiv preprint arXiv:1101.6081,", "citeRegEx": "Chen et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Chen et al\\.", "year": 2011}, {"title": "Off-policy learning with eligibility traces: A survey", "author": ["Geist", "Matthieu", "Scherrer", "Bruno"], "venue": "The Journal of Machine Learning Research,", "citeRegEx": "Geist et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Geist et al\\.", "year": 2014}, {"title": "Importance-weighted least-squares probabilistic classifier for covariate shift adaptation with application to human activity", "author": ["Hachiya", "Hirotaka", "Sugiyama", "Masashi", "Ueda", "Naonori"], "venue": "recognition. Neurocomputing,", "citeRegEx": "Hachiya et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Hachiya et al\\.", "year": 2012}, {"title": "Generalized emphatic temporal difference learning: Bias-variance analysis", "author": ["Hallak", "Assaf", "Tamar", "Aviv", "Munos", "Remi", "Mannor", "Shie"], "venue": "arXiv preprint arXiv:1509.05172,", "citeRegEx": "Hallak et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Hallak et al\\.", "year": 2015}, {"title": "Reinforcement learning in robotics: A survey", "author": ["Kober", "Jens", "Bagnell", "J Andrew", "Peters", "Jan"], "venue": "The International Journal of Robotics Research,", "citeRegEx": "Kober et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Kober et al\\.", "year": 2013}, {"title": "Td-gamma: Re-evaluating complex backups in temporal difference learning", "author": ["Konidaris", "George", "Niekum", "Scott", "Thomas", "Philip S"], "venue": "In Advances in Neural Information Processing Systems", "citeRegEx": "Konidaris et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Konidaris et al\\.", "year": 2011}, {"title": "Stochastic approximation and recursive algorithms and applications, volume 35", "author": ["Kushner", "Harold", "Yin", "G George"], "venue": "Springer Science & Business Media,", "citeRegEx": "Kushner et al\\.,? \\Q2003\\E", "shortCiteRegEx": "Kushner et al\\.", "year": 2003}, {"title": "Off-policy learning based on weighted importance sampling with linear computational complexity", "author": ["Mahmood", "A Rupam", "Sutton", "Richard S"], "venue": "In Conference on Uncertainty in Artificial Intelligence,", "citeRegEx": "Mahmood et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Mahmood et al\\.", "year": 2015}, {"title": "Off-policy temporal-difference learning with function approximation", "author": ["Precup", "Doina", "Sutton", "Richard S", "Dasgupta", "Sanjoy"], "venue": "In ICML,", "citeRegEx": "Precup et al\\.,? \\Q2001\\E", "shortCiteRegEx": "Precup et al\\.", "year": 2001}, {"title": "Stochastic approximation: A dynamical systems viewpoint", "author": ["Schuss", "Zeev", "Borkar", "Vivek S"], "venue": null, "citeRegEx": "Schuss et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Schuss et al\\.", "year": 2009}, {"title": "Reinforcement learning: An introduction", "author": ["R.S. Sutton", "A. Barto"], "venue": null, "citeRegEx": "Sutton and Barto,? \\Q1998\\E", "shortCiteRegEx": "Sutton and Barto", "year": 1998}, {"title": "An emphatic approach to the problem of off-policy temporaldifference learning", "author": ["R.S. Sutton", "A.R. Mahmood", "M. White"], "venue": null, "citeRegEx": "Sutton et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Sutton et al\\.", "year": 2015}, {"title": "A new q (lambda) with interim forward view and monte carlo equivalence", "author": ["Sutton", "Rich", "Mahmood", "Ashique R", "Precup", "Doina", "Hasselt", "Hado V"], "venue": "In Proceedings of the 31st International Conference on Machine Learning", "citeRegEx": "Sutton et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Sutton et al\\.", "year": 2014}, {"title": "Learning to predict by the methods of temporal differences", "author": ["Sutton", "Richard S"], "venue": "Machine learning,", "citeRegEx": "Sutton and S.,? \\Q1988\\E", "shortCiteRegEx": "Sutton and S.", "year": 1988}, {"title": "A convergent o(n) temporal-difference algorithm for offpolicy learning with linear function approximation", "author": ["Sutton", "Richard S", "Maei", "Hamid R", "Szepesv\u00e1ri", "Csaba"], "venue": "In Advances in neural information processing systems,", "citeRegEx": "Sutton et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Sutton et al\\.", "year": 2009}, {"title": "Lifetime value marketing using reinforcement learning", "author": ["Theocharous", "Georgios", "Hallak", "Assaf"], "venue": "RLDM", "citeRegEx": "Theocharous et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Theocharous et al\\.", "year": 2013}, {"title": "High confidence policy improvement", "author": ["Thomas", "Philip", "Theocharous", "Georgios", "Ghavamzadeh", "Mohammad"], "venue": "In Proceedings of the 32nd International Conference on Machine Learning", "citeRegEx": "Thomas et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Thomas et al\\.", "year": 2015}, {"title": "Policy evaluation using the omega-return", "author": ["Thomas", "Philip S", "Niekum", "Scott", "Theocharous", "Georgios", "Konidaris", "George"], "venue": "In Advances in Neural Information Processing Systems", "citeRegEx": "Thomas et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Thomas et al\\.", "year": 2015}, {"title": "An analysis of temporal-difference learning with function approximation", "author": ["Tsitsiklis", "John N", "Van Roy", "Benjamin"], "venue": "Automatic Control, IEEE Transactions on,", "citeRegEx": "Tsitsiklis et al\\.,? \\Q1997\\E", "shortCiteRegEx": "Tsitsiklis et al\\.", "year": 1997}, {"title": "Off-policy td (\u03bb) with a true online equivalence", "author": ["van Hasselt", "Hado", "Mahmood", "A Rupam", "Sutton", "Richard S"], "venue": "In Proceedings of the 30th Conference on Uncertainty in Artificial Intelligence, Quebec City,", "citeRegEx": "Hasselt et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Hasselt et al\\.", "year": 2014}, {"title": "Investigating practical, linear temporal difference learning", "author": ["White", "Adam", "Martha"], "venue": "arXiv preprint arXiv:1602.08771,", "citeRegEx": "White et al\\.,? \\Q2016\\E", "shortCiteRegEx": "White et al\\.", "year": 2016}, {"title": "On convergence of emphatic temporal-difference learning", "author": ["H. Yu"], "venue": "In COLT,", "citeRegEx": "Yu,? \\Q2015\\E", "shortCiteRegEx": "Yu", "year": 2015}], "referenceMentions": [{"referenceID": 7, "context": "Reinforcement Learning (RL) techniques were successfully applied in fields such as robotics, games, marketing and more (Kober et al., 2013; Al-Rawi et al., 2015; Barrett et al., 2013).", "startOffset": 119, "endOffset": 183}, {"referenceID": 0, "context": "Reinforcement Learning (RL) techniques were successfully applied in fields such as robotics, games, marketing and more (Kober et al., 2013; Al-Rawi et al., 2015; Barrett et al., 2013).", "startOffset": 119, "endOffset": 183}, {"referenceID": 1, "context": "Reinforcement Learning (RL) techniques were successfully applied in fields such as robotics, games, marketing and more (Kober et al., 2013; Al-Rawi et al., 2015; Barrett et al., 2013).", "startOffset": 119, "endOffset": 183}, {"referenceID": 14, "context": "Our algorithm resembles (Sutton et al., 2015)\u2019s Emphatic TD that was extended by (Hallak et al.", "startOffset": 24, "endOffset": 45}, {"referenceID": 6, "context": ", 2015)\u2019s Emphatic TD that was extended by (Hallak et al., 2015) to the general parametric form ETD(\u03bb,\u03b2).", "startOffset": 43, "endOffset": 64}, {"referenceID": 2, "context": "Notice that Equation 1 does not specify an on-line implementation since R (n) t,st depends on future observations, however there exists a compact on-line implementation using eligibility traces (Bertsekas & Tsitsiklis (1996) for on-line TD(\u03bb), and Sutton et al.", "startOffset": 195, "endOffset": 225}, {"referenceID": 2, "context": "Notice that Equation 1 does not specify an on-line implementation since R (n) t,st depends on future observations, however there exists a compact on-line implementation using eligibility traces (Bertsekas & Tsitsiklis (1996) for on-line TD(\u03bb), and Sutton et al. (2014), Sutton et al.", "startOffset": 195, "endOffset": 269}, {"referenceID": 2, "context": "Notice that Equation 1 does not specify an on-line implementation since R (n) t,st depends on future observations, however there exists a compact on-line implementation using eligibility traces (Bertsekas & Tsitsiklis (1996) for on-line TD(\u03bb), and Sutton et al. (2014), Sutton et al. (2015) for off-policy TD(\u03bb)).", "startOffset": 195, "endOffset": 291}, {"referenceID": 2, "context": "and is a \u03b3(1\u2212\u03bb) 1\u2212\u03bb\u03b3 -contraction (Bertsekas, 2012).", "startOffset": 34, "endOffset": 51}, {"referenceID": 5, "context": "we call \u03c1d the covariate shift ratio (as denoted under different settings by (Hachiya et al., 2012)).", "startOffset": 77, "endOffset": 99}, {"referenceID": 11, "context": "Among these are full IS (Precup et al., 2001) and ETD(\u03bb,\u03b2) (Sutton et al.", "startOffset": 24, "endOffset": 45}, {"referenceID": 14, "context": ", 2001) and ETD(\u03bb,\u03b2) (Sutton et al., 2015).", "startOffset": 21, "endOffset": 42}, {"referenceID": 11, "context": "For example, full IS-TD by (Precup et al., 2001) examines the ratio between the probabilities of the trajectory under both policies:", "startOffset": 27, "endOffset": 48}, {"referenceID": 24, "context": "Notice that a theorem is only given for \u03bb = 0, convergence results for general \u03bb should follow the work by Yu (2015). A possible criticism on COP-TD(0,\u03b2) is that it is not actually consistent, since in order to be consistent the original state space has to be small, in which case every off-policy algorithm is consistent as well.", "startOffset": 107, "endOffset": 117}, {"referenceID": 13, "context": "Recently, Sutton et al. (2015) had suggested an algorithm for off-policy evaluation called Emphatic TD.", "startOffset": 10, "endOffset": 31}, {"referenceID": 6, "context": "Their algorithm was later on extended by Hallak et al. (2015) and renamed ETD(\u03bb, \u03b2), which was shown to perform extremely well empirically by White & White (2016).", "startOffset": 41, "endOffset": 62}, {"referenceID": 6, "context": "Their algorithm was later on extended by Hallak et al. (2015) and renamed ETD(\u03bb, \u03b2), which was shown to perform extremely well empirically by White & White (2016). ETD(0, \u03b2) can be represented as:", "startOffset": 41, "endOffset": 163}, {"referenceID": 24, "context": "As mentioned before, ETD(\u03bb, \u03b2) converges to the fixed point of \u03a0fT \u03bb \u03c0 (Yu, 2015), where f = E [Ft|st] = (I \u2212 \u03b2P\u03c0) d\u03bc.", "startOffset": 71, "endOffset": 81}, {"referenceID": 6, "context": "Error bounds can be achieved by showing that the operator \u03a0fT \u03bb \u03c0 is a contraction under certain requirements on \u03b2 and that the variance of Ft is directly related to \u03b2 as well (Hallak et al., 2015) (and thus affects the convergence rate of the process).", "startOffset": 176, "endOffset": 197}, {"referenceID": 8, "context": "Subsequently, in Konidaris et al. (2011) Assumption 4 was relaxed and instead a closed form approximation of the variance was proposed.", "startOffset": 17, "endOffset": 41}, {"referenceID": 8, "context": "Subsequently, in Konidaris et al. (2011) Assumption 4 was relaxed and instead a closed form approximation of the variance was proposed. In a follow-up paper by Thomas et al. (2015b), the second assumption was also removed and the weights were instead given as: wn = We have conducted several experiments with an altered ETD and indeed obtained better results compared with the original, these experiments are outside the scope of the paper.", "startOffset": 17, "endOffset": 182}, {"referenceID": 8, "context": "Variance Weighted \u0393t As was shown by Konidaris et al. (2011), we can use statedependent weights instead of \u03b2 exponents to obtain better estimates.", "startOffset": 37, "endOffset": 61}, {"referenceID": 19, "context": "While this solution is impractical in problems with large state spaces parameterizing or approximating these variances (similarly to Thomas et al. (2015b)) could improve performance in specific applications.", "startOffset": 133, "endOffset": 155}, {"referenceID": 6, "context": "While a plethora of algorithms were suggested so far, ETD(\u03bb, \u03b2) by Hallak et al. (2015) has perhaps the simplest formulation and theoretical properties.", "startOffset": 67, "endOffset": 88}], "year": 2017, "abstractText": "The problem of on-line off-policy evaluation (OPE) has been actively studied in the last decade due to its importance both as a stand-alone problem and as a module in a policy improvement scheme. However, most Temporal Difference (TD) based solutions ignore the discrepancy between the stationary distribution of the behavior and target policies and its effect on the convergence limit when function approximation is applied. In this paper we propose the Consistent Off-Policy Temporal Difference (COP-TD(\u03bb, \u03b2)) algorithm that addresses this issue and reduces this bias at some computational expense. We show that COP-TD(\u03bb, \u03b2) can be designed to converge to the same value that would have been obtained by using on-policy TD(\u03bb) with the target policy. Subsequently, the proposed scheme leads to a related and promising heuristic we call logCOP-TD(\u03bb, \u03b2). Both algorithms have favorable empirical results to the current state of the art online OPE algorithms. Finally, our formulation sheds some new light on the recently proposed Emphatic TD learning.", "creator": "LaTeX with hyperref package"}}}