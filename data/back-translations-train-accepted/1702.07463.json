{"id": "1702.07463", "review": {"conference": "ICML", "VERSION": "v1", "DATE_OF_SUBMISSION": "24-Feb-2017", "title": "Sequence Modeling via Segmentations", "abstract": "Segmental structure is a common pattern in many types of sequences such as phrases in human languages. In this paper, we present a probabilistic model for sequences via their segmentations. The probability of a segmented sequence is calculated as the product of the probabilities of all its segments, where each segment is modeled using existing tools such as recurrent neural networks. Since the segmentation of a sequence is usually unknown in advance, we sum over all valid segmentations to obtain the final probability for the sequence. An efficient dynamic programming algorithm is developed for forward and backward computations without resorting to any approximation. We demonstrate our approach on text segmentation and speech recognition tasks. In addition to quantitative results, we also show that our approach can discover meaningful segments in their respective application contexts.", "histories": [["v1", "Fri, 24 Feb 2017 04:55:44 GMT  (350kb,D)", "https://arxiv.org/abs/1702.07463v1", "recurrent neural networks, dynamic programming, structured prediction"], ["v2", "Mon, 27 Feb 2017 01:16:23 GMT  (350kb,D)", "http://arxiv.org/abs/1702.07463v2", "recurrent neural networks, dynamic programming, structured prediction"], ["v3", "Sun, 19 Mar 2017 17:17:06 GMT  (359kb,D)", "http://arxiv.org/abs/1702.07463v3", "recurrent neural networks, dynamic programming, structured prediction (the latest version fixed a bug in Figure 3.)"], ["v4", "Wed, 7 Jun 2017 04:25:34 GMT  (471kb,D)", "http://arxiv.org/abs/1702.07463v4", "recurrent neural networks, dynamic programming, structured prediction"], ["v5", "Mon, 19 Jun 2017 17:45:31 GMT  (860kb,D)", "http://arxiv.org/abs/1702.07463v5", "recurrent neural networks, dynamic programming, structured prediction"]], "COMMENTS": "recurrent neural networks, dynamic programming, structured prediction", "reviews": [], "SUBJECTS": "stat.ML cs.LG", "authors": ["chong wang", "yining wang", "po-sen huang", "abdelrahman mohamed", "dengyong zhou", "li deng"], "accepted": true, "id": "1702.07463"}, "pdf": {"name": "1702.07463.pdf", "metadata": {"source": "META", "title": "Sequence Modeling via Segmentations", "authors": ["Chong Wang", "Yining Wang", "Po-Sen Huang", "Abdelrahman Mohamed", "Dengyong Zhou", "Li Deng"], "emails": ["<chowang@microsoft.com>."], "sections": [{"heading": "1. Introduction", "text": "Segment structure is a common pattern in many types of sequences, typically phrases in human languages and letter combinations in phonotactic rules. For instances, \u2022 Phrase structure is \"machine learning part of artificial intelligence\" \u2192 [machine learning] [is] [part] [artificial intelligence]. In this paper we hope to integrate this type of segment structure information into sequence modeling. 1Microsoft Research 2Carnegie Mellon University 3Amazon 4Citadel Securities LLC. \"Work performed when the authors were using the original sequences. Correspondence to: Chong Wang < chowang @ microsoft.com > Proceedings of the 34th International Conference on Machine Learning, Sydney, Australia, PMLR 70, 2017."}, {"heading": "2. Sequence modeling via segmentations", "text": "In this section, we present our formulation of sequence modeling using segmentation. In our model, the output is always a sequence, while the input may or may not be a sequence. We first look at the case of non-sequence input, then switch to the case of sequence input, and then show how information can be transmitted across segments if required."}, {"heading": "2.1. Case I: Mapping from non-sequence to sequence", "text": "Suppose the input x is a vector of fixed length. Suppose the output sequence is y1: T. We are interested in modelling the probability p (y1: T | x) via the segments of y1: T. Use Sy to denote the set that contains all valid segments of y1: T. Then for each segmentation a1: \u03c4a \u00b2 Sy we have the number of segments in this segmentation. Suppose T = 5, ar Xiv: 170 2.07 463v 5 [stat.ML] 1 9Ju n20 17, where \u03c0 (\u00b7) is the chain operator and \u03c4a \u00b2 p is the number of segments in this segmentation. Suppose the segmentation probability is not x."}, {"heading": "2.2. Case II: Mapping from sequence to sequence", "text": "Now we assume that the input is also a sequence x1: T and the output remains as yq: T. We assume a monotonous alignment assumption - each input element xt sends out a segment, which is then concatenated as \u03c0 (a1: T) to y1: T. Unlike in the case when the input is not a sequence, we allow empty segments in the emission, i.e. at = {$} forsome t, so that any segmentation of y1: T will always consist of exactly T segments with possibly empty. In other words, all valid segmentations for the output are in Sy, {a1: T: \u03c0 (a1: T) = y1: T}. Since an input element can choose to output an empty segment, we call this particular method \"sleep wake networks\" (SWAN)."}, {"heading": "2.3. Carrying over information across segments", "text": "Note that we do not assume that the segments in a segment are conditionally independent. Let's take Equation 2 as an example: The probability of a segment at default xt is defined as p (at | xt, \u03c0 (a1: t \u2212 1)), which also depends on the concatenation of all previous segments \u03c0 (a1: t \u2212 1). We take an approach inspired by the sequence converter (Graves, 2012) to use a separate RNN to model \u03c0 (a1: t \u2212 1). The hidden state of this RNN and the input xt are used as the initial state of the RNN for segment at. (We simply add them together in our speech recognition experiment.) This allows all previously broadcast outputs to influence this segment. Figure 3 illustrates this idea. The importance of this approach is that it still allows the exact dynamic programming algorithm as we will describe in Section 3."}, {"heading": "2.4. Related work", "text": "The approach, especially SWAN, is inspired by the connectionist temporal classification beyond the CTC (JaTC et al., 2006) and the sequence transducer (Graves, 2012). CTC defines a distribution over the output sequence that is no longer than the input sequence. In order to adequately capture the input sequence, CTC marginalizes all possible alignments by means of dynamic programming. Since CTC does not model the interdependencies between the output sequences, the sequence transducer creates a separate network for output-output dependencies in which the prediction network functions like a language modeling. SWAN can be seen as a generalization of the output sequences dependencies to enable segmented outputs. Neither the CTC nor the sequence transducer introduces a separate RNN as a prediction network to make output-output-output-dependencies dependent on where the output dependencies are dependent."}, {"heading": "3. Forward, backward and decoding", "text": "In this section, we first present the details of forward and backward calculations using dynamic programming, and then describe the beam search decoding algorithm. These algorithms turn our approach into a stand-alone loss function that can be used in many applications. [1] Here, we focus on developing the algorithm in case the input is a sequence. [2] If the input is not a sequence, the corresponding algorithms can be derived in a similar way. [3] We plan to publish this package in an in-depth learning framework."}, {"heading": "3.1. Forward and backward propagations", "text": "Let us first consider the calculation of the result for Eq. 2. We first define the forward and backward probability, 2\u03b1t (j) = p (y1: j | x1: t), \u03b2t (j) = p (yj + 1, T | xt + 1: T \u2032, y1: j), where forward \u03b1t (j) represents the probability that input x1: t represents output y1: j and backward (j) represents the probability that input xt + 1: T \u2032 emits, although + 1: T. Using \u03b1t (j) and \u03b2t (j) we can verify the following for each t = 0, 1 \u2032, p (y1): T \u2032, p (y1: T)."}, {"heading": "3.2. Beam search decoding", "text": "Although it is possible to calculate the output probability using dynamic programming during training, it is impossible to do something similar during decoding because the output is unknown, so we resort to beam search. SWAN beam search is more complex than the simple left beam search algorithm used in standard sequence models (Sutskever et al., 2014). Indeed, for each input element xt we do a simple left-to-right search. Furthermore, different segmentations could imply the same output sequence, and we need to include this information in the beam search as well. To achieve this, we need to pass 3 1 2 3 $3 $3 3 $3 $3 $3 forward."}, {"heading": "4. Experiments", "text": "In this section, we apply our method to two applications, one unattended and the other supervised, including 1) content-based text segmentation, where the input into our distribution is a vector (constructed with a variable autoencoder for text), and 2) speech recognition, where the input into our distribution is a sequence (acoustic characteristics)."}, {"heading": "4.1. Content-based text segmentation", "text": "This text segmentation corresponds to a simplified version of the non-sequenced input model in Section 2.1, where we use the term \"p\" (\"p\") (\"p\") (\"p\") (\"p\") (\"p\") (\"p\") (\"p\") (\"p\") (\"p\") (\"p\") (\"p\") (\"p\") (\"p\") (\"p\") (\"p\") (\"p\") (\"p\") (\"p\") (\"p\") (\"p\") (\"p\") (\"p\") (\"p\") (\"p\") (\"p\") (p) (\"p) (\" p \"(p) (\") (p) (\"(p) (\" (p) (\"(p) (p) (\") (p) (\"(p) (\") (p) (\"(p)) (p) (\" (p)) (p) (\"p) (\" (\"p) (\" p) (\"p) (\" (\"p) (\" p) (\"(\" p) (\"p) (\" (\"p) (\" p) (\"p) (\" (\"p) (p) (p) (p) (p) (p) (\" (p) (p) (p) (p) (p) (p) (p) (p) (p) (p (p) (p) (p) (p) (p (p) (p) (p) (p) (p) (p) (p) (p) (p) (p) (p) (p) (p) (p) (p) (p (p) (p) (p) (p) (p) (p) (p) (p) (p) (p (p) (p) (p) (p) (p) (p) (p (p) (p) (p) (p) (p) (p) (p) (p (p) (p)) (p (p (p))) (p (p (p)) (p (p (p (p) (p))) (p"}, {"heading": "4.2. Speech recognition", "text": "In fact, most of them will be able to play by the rules that they have shown in recent years, and they will be able to play by the rules that they have set for their policies."}, {"heading": "5. Conclusion and Future work", "text": "In this paper, we present a new probability distribution for sequence modeling and demonstrate its usefulness for two different tasks: Because of its generality, it can be used as a loss function in many sequence modeling tasks. We plan to investigate the following directions in future work: The first is to validate our approach to large-scale language records; the second is machine translation, where segmentation can be considered as \"phrases.\" We believe that this approach has the potential to merge the advantages of traditional phrase-based translation (Koehn et al., 2003) and more recent neural machine translation (Sutskever et al., 2014; Bahdanau et al., 2014)."}], "references": [{"title": "Neural machine translation by jointly learning to align and translate", "author": ["Bahdanau", "Dzmitry", "Cho", "Kyunghyun", "Bengio", "Yoshua"], "venue": "arXiv preprint arXiv:1409.0473,", "citeRegEx": "Bahdanau et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Bahdanau et al\\.", "year": 2014}, {"title": "Latent Dirichlet allocation", "author": ["D. Blei", "A. Ng", "M. Jordan"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Blei et al\\.,? \\Q2003\\E", "shortCiteRegEx": "Blei et al\\.", "year": 2003}, {"title": "Statistical methods for dna sequence segmentation", "author": ["Braun", "Jerome V", "Muller", "Hans-Georg"], "venue": "Statistical Science,", "citeRegEx": "Braun et al\\.,? \\Q1998\\E", "shortCiteRegEx": "Braun et al\\.", "year": 1998}, {"title": "Attention-based models for speech recognition", "author": ["Chorowski", "Jan K", "Bahdanau", "Dzmitry", "Serdyuk", "Dmitriy", "Cho", "Kyunghyun", "Bengio", "Yoshua"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Chorowski et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Chorowski et al\\.", "year": 2015}, {"title": "Empirical evaluation of gated recurrent neural networks on sequence modeling", "author": ["Chung", "Junyoung", "Gulcehre", "Caglar", "Cho", "KyungHyun", "Bengio", "Yoshua"], "venue": "arXiv preprint arXiv:1412.3555,", "citeRegEx": "Chung et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Chung et al\\.", "year": 2014}, {"title": "Deep discriminative and generative models for speech pattern recognition. Chapter 2 in Handbook of Pattern Recognition and Computer Vision (Ed", "author": ["Deng", "Li", "Jaitly", "Navdeep"], "venue": null, "citeRegEx": "Deng et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Deng et al\\.", "year": 2015}, {"title": "Structured speech modeling", "author": ["Deng", "Li", "Yu", "Dong", "Acero", "Alex"], "venue": "IEEE Trans. Audio, Speech, and Language Processing,", "citeRegEx": "Deng et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Deng et al\\.", "year": 2006}, {"title": "Sequence transduction with recurrent neural networks", "author": ["Graves", "Alex"], "venue": "arXiv preprint arXiv:1211.3711,", "citeRegEx": "Graves and Alex.,? \\Q2012\\E", "shortCiteRegEx": "Graves and Alex.", "year": 2012}, {"title": "Generating sequences with recurrent neural networks", "author": ["Graves", "Alex"], "venue": "arXiv preprint arXiv:1308.0850,", "citeRegEx": "Graves and Alex.,? \\Q2013\\E", "shortCiteRegEx": "Graves and Alex.", "year": 2013}, {"title": "Speech recognition with deep recurrent neural networks", "author": ["Graves", "Alex", "Mohamed", "Abdel-rahman", "Hinton", "Geoffrey"], "venue": "In IEEE International Conference on Acoustics Speech and Signal Processing (ICASSP),,", "citeRegEx": "Graves et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Graves et al\\.", "year": 2013}, {"title": "Long shortterm memory", "author": ["Hochreiter", "Sepp", "Schmidhuber", "J\u00fcrgen"], "venue": "Neural Computation,", "citeRegEx": "Hochreiter et al\\.,? \\Q1997\\E", "shortCiteRegEx": "Hochreiter et al\\.", "year": 1997}, {"title": "Stochastic variational inference", "author": ["M. Hoffman", "D. Blei", "C. Wang", "J. Paisley"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Hoffman et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Hoffman et al\\.", "year": 2013}, {"title": "An online sequence-to-sequence model using partial conditioning", "author": ["Jaitly", "Navdeep", "Le", "Quoc V", "Vinyals", "Oriol", "Sutskever", "Ilya", "Sussillo", "David", "Bengio", "Samy"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Jaitly et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Jaitly et al\\.", "year": 2016}, {"title": "Learning in Graphical Models", "author": ["Jordan", "Michael (ed"], "venue": null, "citeRegEx": "Jordan and .ed...,? \\Q1999\\E", "shortCiteRegEx": "Jordan and .ed...", "year": 1999}, {"title": "Adam: A method for stochastic optimization", "author": ["Kingma", "Diederik", "Ba", "Jimmy"], "venue": "arXiv preprint arXiv:1412.6980,", "citeRegEx": "Kingma et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Kingma et al\\.", "year": 2014}, {"title": "Auto-encoding variational Bayes", "author": ["Kingma", "Diederik P", "Welling", "Max"], "venue": "arXiv preprint arXiv:1312.6114,", "citeRegEx": "Kingma et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Kingma et al\\.", "year": 2013}, {"title": "Statistical phrase-based translation", "author": ["Koehn", "Philipp", "Och", "Franz Josef", "Marcu", "Daniel"], "venue": "In Proceedings of the 2003 Conference of the North American Chapter of the Association for Computational Linguistics on Human Language Technology-Volume", "citeRegEx": "Koehn et al\\.,? \\Q2003\\E", "shortCiteRegEx": "Koehn et al\\.", "year": 2003}, {"title": "Segmental recurrent neural networks", "author": ["Kong", "Lingpeng", "Dyer", "Chris", "Smith", "Noah A"], "venue": "arXiv preprint arXiv:1511.06018,", "citeRegEx": "Kong et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Kong et al\\.", "year": 2015}, {"title": "Speaker-independent phone recognition using hidden markov models", "author": ["Lee", "Kai-Fu", "Hon", "Hsiao-Wuen"], "venue": "IEEE Transactions on Acoustics, Speech, and Signal Processing,", "citeRegEx": "Lee et al\\.,? \\Q1989\\E", "shortCiteRegEx": "Lee et al\\.", "year": 1989}, {"title": "Neural variational inference for text processing", "author": ["Miao", "Yishu", "Yu", "Lei", "Blunsom", "Phil"], "venue": "In Proceedings of the 33rd International Conference on Machine Learning,", "citeRegEx": "Miao et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Miao et al\\.", "year": 2016}, {"title": "Stochastic backpropagation and approximate inference in deep generative models", "author": ["Rezende", "Danilo Jimenez", "Mohamed", "Shakir", "Wierstra", "Daan"], "venue": "In Proceedings of the 31st International Conference on Machine Learning,", "citeRegEx": "Rezende et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Rezende et al\\.", "year": 2014}, {"title": "Semi-markov conditional random fields for information extraction", "author": ["Sarawagi", "Sunita", "Cohen", "William W"], "venue": "Advances in Neural Information Processing Systems", "citeRegEx": "Sarawagi et al\\.,? \\Q2004\\E", "shortCiteRegEx": "Sarawagi et al\\.", "year": 2004}, {"title": "Sequence to sequence learning with neural networks", "author": ["Sutskever", "Ilya", "Vinyals", "Oriol", "Le", "Quoc V"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Sutskever et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Sutskever et al\\.", "year": 2014}, {"title": "Collaborative topic modeling for recommending scientific articles", "author": ["Wang", "Chong", "Blei", "David M"], "venue": "In ACM International Conference on Knowledge Discovery and Data Mining,", "citeRegEx": "Wang et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Wang et al\\.", "year": 2011}, {"title": "Online segment to segment neural transduction", "author": ["Yu", "Lei", "Buys", "Jan", "Blunsom", "Phil"], "venue": "arXiv preprint arXiv:1609.08194,", "citeRegEx": "Yu et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Yu et al\\.", "year": 2016}, {"title": "Towards end-to-end speech recognition with deep convolutional neural networks", "author": ["Zhang", "Ying", "Pezeshki", "Mohammad", "Brakel", "Phil\u00e9mon", "Saizheng", "Laurent", "C\u00e9sar", "Bengio", "Yoshua", "Courville", "Aaron"], "venue": "arXiv preprint arXiv:1701.02720,", "citeRegEx": "Zhang et al\\.,? \\Q2017\\E", "shortCiteRegEx": "Zhang et al\\.", "year": 2017}], "referenceMentions": [{"referenceID": 4, "context": "Then the probability of this sequence is calculated as the product of the probabilities of its segments, each of which is modeled using existing tools such as recurrent neural networks (RNNs), long-short term memory (LSTM) (Hochreiter & Schmidhuber, 1997), or gated recurrent units (GRU) (Chung et al., 2014).", "startOffset": 288, "endOffset": 308}, {"referenceID": 12, "context": "Although it is suspected that using a separate RNN could result in a loosely-coupled model (Graves, 2013; Jaitly et al., 2016) that might hinder the performance, we do not find it to be an issue in our approach.", "startOffset": 91, "endOffset": 126}, {"referenceID": 12, "context": "SWAN itself is most similar to the recent work on the neural transducer (Jaitly et al., 2016), although we start with a different motivation.", "startOffset": 72, "endOffset": 93}, {"referenceID": 12, "context": "SWAN itself is most similar to the recent work on the neural transducer (Jaitly et al., 2016), although we start with a different motivation. The motivation of the neural transducer is to allow incremental predictions as input streamingly arrives, for example in speech recognition. From the modeling perspective, it also assumes that the output is decomposed into several segments and the alignments are unknown in advance. However, its assumption that hidden states are carried over across the segments prohibits exact marginalizing all valid segmentations and alignments. So they resorted to find an approximate \u201cbest\u201d alignment with a dynamic programming-like algorithm during training or they might need a separate GMM-HMM model to generate alignments in advance to achieve better results. Otherwise, without carrying information across segments results in sub-optimal performance as shown in Jaitly et al. (2016). In contrast, our method of connecting the segments described in Section 2.", "startOffset": 73, "endOffset": 919}, {"referenceID": 24, "context": "Another closely related work is the online segment to segment neural transduction (Yu et al., 2016).", "startOffset": 82, "endOffset": 99}, {"referenceID": 17, "context": "Our work is also related to semi-Markov conditional random fields (Sarawagi & Cohen, 2004), segmental recurrent neural networks (Kong et al., 2015) and segmental hidden dynamic model (Deng & Jaitly, 2015), where the segmentation is applied to the input sequence instead of the output sequence.", "startOffset": 128, "endOffset": 147}, {"referenceID": 22, "context": "The beam search for SWAN is more complex than the simple left-toright beam search algorithm used in standard sequence-tosequence models (Sutskever et al., 2014).", "startOffset": 136, "endOffset": 160}, {"referenceID": 1, "context": "To this end, we build a simple model inspired by latent Dirichlet allocation (LDA) (Blei et al., 2003) and neural variational inference for texts (Miao et al.", "startOffset": 83, "endOffset": 102}, {"referenceID": 19, "context": ", 2003) and neural variational inference for texts (Miao et al., 2016).", "startOffset": 51, "endOffset": 70}, {"referenceID": 20, "context": "For variational distribution q(\u03b6), we use variational autoencoder to model it as an inference network (Kingma & Welling, 2013; Rezende et al., 2014).", "startOffset": 102, "endOffset": 148}, {"referenceID": 19, "context": "We use the form similar to Miao et al. (2016), where the inference network is a feed-forward neural network and its input is the BoW of the document \u2014 q(\u03b6) , q(\u03b6|BoW(y1:T )).", "startOffset": 27, "endOffset": 46}, {"referenceID": 1, "context": "We use two datasets including AP (Associated Press, 2, 246 documents) from Blei et al. (2003) and CiteULike3 scientific article abstracts (16, 980 documents) from Wang & Blei", "startOffset": 75, "endOffset": 94}, {"referenceID": 11, "context": "We use the evaluation setup from Hoffman et al. (2013) for comparing two different models in terms of predictive log likelihood on a heldout set.", "startOffset": 33, "endOffset": 55}, {"referenceID": 5, "context": "We evaluate SWAN on the TIMIT corpus following the setup in Deng et al. (2006). The audio data is encoded using a Fourier-transform-based filter-bank with 40 coefficients (plus energy) distributed on a mel-scale, together with their first and second temporal derivatives.", "startOffset": 60, "endOffset": 79}, {"referenceID": 25, "context": "For optimization, we largely followed the strategy in Zhang et al. (2017). We use Adam (Kingma & Ba, 2014) with learning rate 4e\u2212 4.", "startOffset": 54, "endOffset": 74}, {"referenceID": 9, "context": "BiLSTM-5L-250H (Graves et al., 2013) 18.", "startOffset": 15, "endOffset": 36}, {"referenceID": 9, "context": "4 TRANS-3L-250H (Graves et al., 2013) 18.", "startOffset": 16, "endOffset": 37}, {"referenceID": 3, "context": "3 Attention RNN (Chorowski et al., 2015) 17.", "startOffset": 16, "endOffset": 40}, {"referenceID": 12, "context": "6 Neural Transducer (Jaitly et al., 2016) 18.", "startOffset": 20, "endOffset": 41}, {"referenceID": 25, "context": "2 CNN-10L-maxout (Zhang et al., 2017) 18.", "startOffset": 17, "endOffset": 37}, {"referenceID": 16, "context": "\u201d We believe this approach has the potential to bring together the merits of traditional phrasebased translation (Koehn et al., 2003) and recent neural machine translation (Sutskever et al.", "startOffset": 113, "endOffset": 133}, {"referenceID": 22, "context": ", 2003) and recent neural machine translation (Sutskever et al., 2014; Bahdanau et al., 2014).", "startOffset": 46, "endOffset": 93}, {"referenceID": 0, "context": ", 2003) and recent neural machine translation (Sutskever et al., 2014; Bahdanau et al., 2014).", "startOffset": 46, "endOffset": 93}], "year": 2017, "abstractText": "Segmental structure is a common pattern in many types of sequences such as phrases in human languages. In this paper, we present a probabilistic model for sequences via their segmentations. The probability of a segmented sequence is calculated as the product of the probabilities of all its segments, where each segment is modeled using existing tools such as recurrent neural networks. Since the segmentation of a sequence is usually unknown in advance, we sum over all valid segmentations to obtain the final probability for the sequence. An efficient dynamic programming algorithm is developed for forward and backward computations without resorting to any approximation. We demonstrate our approach on text segmentation and speech recognition tasks. In addition to quantitative results, we also show that our approach can discover meaningful segments in their respective application contexts.", "creator": "LaTeX with hyperref package"}}}