{"id": "1702.07817", "review": {"conference": "nips", "VERSION": "v1", "DATE_OF_SUBMISSION": "25-Feb-2017", "title": "Unsupervised Sequence Classification using Sequential Output Statistics", "abstract": "We address a class of unsupervised learning problems where the same goal of supervised learning is aimed except with no output labels provided for training classifiers. This type of unsupervised learning is highly valuable in machine learning practice since obtaining labels in training data is often costly. Instead of pairing input-output samples, we exploit sequential statistics of output labels, in the form of N-gram language models, which can be obtained independently of input data and thus with low or no cost. We introduce a novel cost function in this unsupervised learning setting, whose profiles are analyzed and shown to be highly non-convex with large barriers near the global optimum. A new stochastic primal-dual gradient method is developed to optimize this very difficult type of cost function via the use of dual variables to reduce the barriers. We demonstrate in experimental evaluation, with both synthetic and real-world data sets, that the new method for unsupervised learning gives drastically lower errors and higher learning efficiency than the standard stochastic gradient descent, reaching classification errors about twice of those obtained by fully supervised learning. We also show the crucial role of labels' sequential statistics exploited for label-free training with the new method, reflected by the significantly lower classification errors when higher-order language models are used in unsupervised learning than low-order ones.", "histories": [["v1", "Sat, 25 Feb 2017 01:55:38 GMT  (8541kb,D)", "https://arxiv.org/abs/1702.07817v1", null], ["v2", "Fri, 26 May 2017 18:30:24 GMT  (8590kb,D)", "http://arxiv.org/abs/1702.07817v2", "All authors contributed equally to the paper. 17 pages, 7 figures and 2 tables"]], "reviews": [], "SUBJECTS": "cs.LG", "authors": ["yu liu", "jianshu chen", "li deng"], "accepted": true, "id": "1702.07817"}, "pdf": {"name": "1702.07817.pdf", "metadata": {"source": "CRF", "title": "Unsupervised Sequence Classification using Sequential Output Statistics", "authors": ["Yu Liu", "Jianshu Chen", "Li Deng"], "emails": ["jianshuc@microsoft.com", "deng@microsoft.com"], "sections": [{"heading": "1 Introduction", "text": "It is often formulated as a model of how the world works without requiring a large amount of manpower to obtain expensive data. To achieve this goal, we need to solve the learning of a sequencer classifier without labels. However, the problem is very valuable because sequencer statistics, such as the language models, can be obtained independently of the data generated. The problem is that we obtain the learning of a sequencer classifier without labels."}, {"heading": "2 Empirical-ODM: An unsupervised learning cost for sequence classifiers", "text": "In this section, we expand on the earlier work of [30] and propose an unattended learning effort called Empirical Output Distribution Match (Empirical-ODM) for training classifiers without labeled data. First, we formulate the unattended learning problem with sequential output structures. Then, we present the empirical-ODM costs and discuss their important properties, which are closely related to unattended learning."}, {"heading": "2.1 Problem formulation", "text": "We look at the problem of learning a sequence classification that predicts an output sequence (y1,., yT0). We look at the problem of learning a sequence classification that predicts an output sequence (y1,.,., yTn) from an input sequence (x1,., xnTn) in which the learning algorithms do not have access to a labeled image sequence (xn1,.,., xnTn) in which the length of the n-th image sequence is not specified but a collection of input sequences denoted as DX.,., xnTn): n = 1., M}, where we assume that the sequential output statistics (or the sequence probability before) are available."}, {"heading": "2.2 The Empirical-ODM", "text": "It is mainly inspired by the approach of breaking the Caesar cipher (one of the simplest forms of encryption [23]. Caesar cipher is a substitution ship in which each letter in the original message is replaced by a letter corresponding to a certain number of letters up or down in the alphabet. In this way, the original message that was legible ends up being less intelligible. The amount of this shift is also known to the intended recipient of the message, who can decrypt the message by pushing back every letter in the encrypted message. However, Caesar cipher could also be broken by an accidental message (without knowing the shift) when analyzing the frequency of the letters."}, {"heading": "2.3 Coverage-seeking versus mode-seeking", "text": "We discuss an important property of the proposed empirical ODM costs (1) by comparing them with the costs proposed in [7]. We show that the empirical ODM costs have a containment-seeking property that makes them more suitable for uncontrolled learning than the mode-seeking costs in [7]. In [7], the authors propose the expected negative log probability as the uncontrolled learning cost function that exploits the results of sequential statistics. Intuition was to maximize the aggregate log probability of all output sequences generated by the stochastic images (y n t). In Appendix A of the supplementary material, we show that their costs are equivalent to \u2212 1,... iN \u2212 1."}, {"heading": "2.4 The difficulties of optimizing J (\u03b8)", "text": "However, there are two key challenges in optimizing the empirical ODM costs J (\u03b8) in (1): The first is that the sample average (over the entire training data set) for the expression of p\u03b8 (\u00b7) (see (2)) is within the logarithmic loss that differs from traditional machine learning problems where the mean consists of external loss functions (e.g. p t ft (\u03b8)).This functional form prevents us from applying the stochastic gradient descent (SGD) to minimize (1), since the stochastic gradients would be intrinsically distorted (see Appendix C for a detailed discussion and see Section 4 for the test results).The second challenge is that the cost function J (ziped) is strongly non-convex even with linear classifiers. To see this, we visualize the profile of the cost function J (2001) (limited to 1.3, where there is an optimal two-dimensional solution in addition to local monitoring difficulties)."}, {"heading": "3 The Stochastic Primal-Dual Gradient (SPDG) Algorithm", "text": "To address the first difficulty in Section 2.4, we transform the initial cost (1) into an equivalent minimum-maximum problem (1) to get the sample average out of the logarithmic loss. Then, we could get unbiased stochastic gradients to solve the problem. To this end, we first introduce the concept of conjugated functions. [6, pp.90-95] For a given convex function f (u), its conjugated function is defined as f (u), supu (us) [6, pp.90] where u and \u03bd are designated as primary and dual variables, respectively. For a scalar function f (u) = \u2212 lnu, its conjugate function can be calculated as f? (ex.) = \u2212 ln (\u2212 ln) with the conjugated function."}, {"heading": "4 Experiments", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "4.1 Experimental setup", "text": "We evaluate our unguarded learning scheme described in previous sections with two classification tasks, unguarded character level OCR and unguarded English spell correction (Spell Corr), both of which have no labels during the training, so they are both unguarded. For the OCR task, we get our data set from a public database UWIII English Document Image Database [27], which contains images for each line of text with the associated Groudtruth. We first use Tesseract [19] to segment the image into character tiles for each line of text and assign a character to each tile. We verify the sequencing result by training a simple neural network classifier to the segmented results and achieve 0.9% error rate on the test set. Then we select sentence segments that are longer than 100 and contain only lowercase English letters and letters."}, {"heading": "4.2 Results: Comparing optimization algorithms", "text": "In the first group of experiments, we want to evaluate the effectiveness of the SPDG method as described in Section 3, which is intended to optimize the empirical ODM costs in Section 2. The analysis provided in Sections 2 and 3 provides insights into why SPDG is superior to the method in Section 7, and in relation to the standard stochastic descent (SGD). Coverage-seeking behavior of the proposed empirical ODM costs helps to avoid trivial solutions, and the simultaneous optimization of the primary-dual variables lowers the barriers in the highly non-convex profile of the cost function. Furthermore, we do not include the methods from [30] because their approaches do not achieve satisfactory results without a few labeled data, while we only consider completely unattended learning situations."}, {"heading": "4.3 Results: Comparing orders of language modeling", "text": "In the second group of experiments, we examine the extent to which the use of sequential statistics (e.g. 2- and 3-gram LMs) in unattended learning performs better than the Unigram LM (no sequential information).The unattended prediction results are presented in Table 2, using various data sources to estimate N-gram LM parameters. Consistent with all four types of estimation of reliable N-gram LMs, we find significantly lower error rates when unattended learning evaluates 2-gram LM and 3-gram LM as sequential statistics, compared with the use of the previous non-sequential statistics (i.e. 1-gram).In three out of four cases, the use of a 3-gram LM yields better results than a 2-gram LM. Furthermore, the comparable error rate associated with the use of 3-gram LM data outside of the domain (i.e. 10.2%) shows that the effectiveness is overstated in this table."}, {"heading": "5 Conclusions and future work", "text": "In this paper, we explore the problem of learning a sequence classifier without the need to label training data, and the practical benefits of such unsupervised learning are enormous. For example, the currently dominant supervised learning methods typically require a few thousand hours of training data, with every utterance having to be labeled by humans in acoustic form. Although millions of hours of natural language data are available for training, labeling all of them for supervised learning is less practical. To enable effective use of these vast amounts of acoustic data, the practical, unsupervised learning approach discussed in this paper would be required. Other potential applications such as machine translation, image and video labeling could also benefit from our paradigm, largely due to their shared natural language output structure, from which we could exploit the sequence structures for learning the classifier without labels."}, {"heading": "A Derivation of the equivalent form of the cost in [7]", "text": "The cost function in [7] can be expressed as follows: E [\u2212 M \u2211 n = 1 ln pLM = | n | n = | n |., y n Tn) | x n \u00b7 \u00b7 N \u00b7 N \u00b7 n \u00b7 n \u00b7 n \u00b7 n \u00b7 n \u00b7 n \u00b7 n \u00b7 n \u00b7 n \u00b7 n \u00b7 n \u00b7 n \u00b7 n \u00b7 n \u00b7 n \u00b7 n \u00b7 n \u00b7 n \u00b7 n \u00b7 n \u00b7 n \u00b7 n \u00b7 n \u00b7 n \u00b7 n \u00b7 n \u00b7 n \u00b7 n \u00b7 n \u00b7 n \u00b7 n \u00b7 n \u00b7 n \u00b7 n \u00b7 n \u00b7 n \u00b7 n \u00b7 n \u00b7 n \u00b7 n \u00b7 n \u00b7 n \u00b7 n \u00b7 n \u00b7 n \u00b7 n \u00b7 n \u00b7 n \u00b7 n \u00b7 n \u00b7 n \u00b7 n \u00b7 n \u00b7 n \u00b7 n (n) \u00b7 n \u00b7 n \u00b7 n \u00b7 n \u00b7 n \u00b7 n (n)."}, {"heading": "C Optimizing Empirical-ODM by SGD is intrinsically biased", "text": "In this section we show that the stochastic gradient of Empirical-ODM is distorted in itself. To see this, we can use the (full) gradient of J (\u03b8) in the form of: 1,..., iNpLM (i1,.., iN) 1, 2, 3, 4, 5, 5, 5, 6, 6, 7, 7, 7, 8, 8, 8, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11 11, 11, 11, 11, 11, 11, 11, 11, 11, 11 11, 11, 11, 11 11, 11, 11, 11, 11, 11, 11, 11"}, {"heading": "D Experiment Details", "text": "In the experiment, we implement the model with Python 2.7 and Tensorflow 0.12. In the training of models on both OCR and Spell-Corr task, we initialize the parameters of the linear model (primary variable) with winit = 1 / dim (x) and \u03b3 = 10, where dim (x) is the dimension of the input, and we initialize the dual parameters vinit with evenly distributed random variables v \u00b2 U (\u2212 1, 0). We set the learning rate for the primary parameter \u00b5\u03b8 = 10 \u2212 6 and the learning rate for the dual parameter \u00b5v = 10 \u2212 4. We use Adam optimization to train our model.The test set of OCR is also generated from the UWIII database, but avoids overlapping with the training set. The size of the test set of OCR is 15000. In addition, the size of the test set of Spell-Corr is also 15000, without overlapping with the training set."}, {"heading": "E The details of visualizing the high-dimensional cost functions", "text": "Since J (\u03b8) is a high-dimensional function, it is difficult to visualize its complete profile. Instead, we use the following method to partially visualize J (\u03b8). Firstly, because supervised learning of linear classifiers is a convex optimization problem from which we could obtain its global optimal solution. 7 Then, we randomly generate two parameter vectors \u03b81 and \u03b82 and plot the two-dimensional function J (\u03b8, V) in (5), similar to the case of J (\u03b8) + \u03bb2 (\u03b82 \u2212 \u03b8?))) in relation to \u03bb1, \u03bb2 \u0445 R, which is a slice of the cost function on a two-dimensional plane. For the profile of L (\u03b8, V) in (5), similar to that of J (\u03b8), we first solve the supervised learning problem in order to visualize the profile of L (\u03b8, V) in relation to negative elements."}, {"heading": "F Additional visualization of J (\u03b8)", "text": "In Figures 3, 4, and 5, we show three visualization examples of J (\u03b8) for the OCR dataset in three different affinity spaces, with part of the first example included in Figure 1. The six partial figures in each example show the same profile at six different angles, with the profiles of (a) - (f) rotating clockwise, the red dots indicating the global minimum. In Figure 6, we show the same type of profile as above, except for synthetic data for a binary classification problem. First, we sequentially generated a sequence of states of 0, 1 by a hidden Markov model, and then stamped the corresponding data points from two separate 2-dimensional Gaussian models."}, {"heading": "G Additional visualization of L(\u03b8, V )", "text": "Figure 7 shows the profile of L (\u03b8, V) for the OCR dataset in a two-dimensional affine space viewed from nine different angles. The red dots show the saddle points of the profile, one for each angle. 7Note that we solve supervised learning only for the purpose of understanding our proposed unsupervised learning costs J (\u03b8). In our implementation of the unsupervised learning algorithm, we do not use information about the training label or supervised learning algorithms."}], "references": [{"title": "Learning deep architectures for AI", "author": ["Yoshua Bengio"], "venue": "Foundations and Trends in Machine Learning,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2009}, {"title": "Greedy layer-wise training of deep networks", "author": ["Yoshua Bengio", "Pascal Lamblin", "Dan Popovici", "Hugo Larochelle"], "venue": "In Proceedings of the Advances in Neural Information Processing Systems (NIPS),", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2007}, {"title": "Unsupervised transcription of historical documents", "author": ["Taylor Berg-Kirkpatrick", "Greg Durrett", "Dan Klein"], "venue": "In Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2013}, {"title": "Latent dirichlet allocation", "author": ["David M. Blei", "Andrew Y. Ng", "Michael I. Jordan"], "venue": "The Journal of Machine Learning Research,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2003}, {"title": "Unsupervised learning of predictors from unpaired input-output samples", "author": ["Jianshu Chen", "Po-Sen Huang", "Xiaodong He", "Jianfeng Gao", "Li Deng"], "venue": null, "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2016}, {"title": "A path to unsupervised learning through adversarial networks", "author": ["Soumith Chintala", "Yann LeCun"], "venue": "In https://code.facebook.com/posts/1587249151575490/a-path-to-unsupervisedlearning-through-adversarial-networks/,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2016}, {"title": "Context-dependent pre-trained deep neural networks for large-vocabulary speech recognition. Audio, Speech, and Language Processing", "author": ["George E Dahl", "Dong Yu", "Li Deng", "Alex Acero"], "venue": "IEEE Transactions on,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2012}, {"title": "Semi-supervised sequence learning", "author": ["Andrew M Dai", "Quoc V Le"], "venue": "In Proceedings of the Advances in Neural Information Processing Systems (NIPS),", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2015}, {"title": "Deep learning for speech and language processing", "author": ["Li Deng"], "venue": "In Tutorial at Interspeech Conf,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2015}, {"title": "Generative adversarial nets", "author": ["Ian Goodfellow"], "venue": "In Tutorial at NIPS, http://www.cs.toronto.edu/", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2016}, {"title": "Deep Learning, by MIT Press. 2016", "author": ["Ian Goodfellow", "Yoshua Bengio", "Aaron Courville"], "venue": null, "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2016}, {"title": "Generative adversarial nets", "author": ["Ian Goodfellow", "Jean Pouget-Abadie", "Mehdi Mirza", "Bing Xu", "David Warde-Farley", "Sherjil Ozair", "Aaron Courville", "Yoshua Bengio"], "venue": "In Proceedings of the Advances in Neural Information Processing Systems (NIPS),", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2014}, {"title": "Sequence transduction with recurrent neural networks", "author": ["Alex Graves"], "venue": "arXiv preprint arXiv:1211.3711,", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2012}, {"title": "Deep neural networks for acoustic modeling in speech recognition: The shared views of four research groups", "author": ["Geoffrey Hinton", "Li Deng", "Dong Yu", "George E Dahl", "Abdel-Rahman Mohamed", "Navdeep Jaitly", "Andrew Senior", "Vincent Vanhoucke", "Patrick Nguyen", "Tara N. Sainath", "B. Kingsbury"], "venue": "IEEE Signal Processing Magazine,", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2012}, {"title": "A fast learning algorithm for deep belief nets", "author": ["Geoffrey E Hinton", "Simon Osindero", "Yee-Whye Teh"], "venue": "Neural computation,", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2006}, {"title": "Reducing the dimensionality of data with neural networks", "author": ["Geoffrey E Hinton", "Ruslan R Salakhutdinov"], "venue": null, "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2006}, {"title": "Tesseract: An open-source optical character recognition", "author": ["Anthony Kay"], "venue": "engine. Linux Journal,", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2007}, {"title": "Auto-encoding variational bayes", "author": ["Diederik P Kingma", "Max Welling"], "venue": "arXiv preprint arXiv:1312.6114,", "citeRegEx": "20", "shortCiteRegEx": "20", "year": 2013}, {"title": "Unsupervised analysis for decipherment problems", "author": ["Kevin Knight", "Anish Nair", "Nishit Rathod", "Kenji Yamada"], "venue": "In Proceedings of the COLING/ACL,", "citeRegEx": "21", "shortCiteRegEx": "21", "year": 2006}, {"title": "Building high-level features using large scale unsupervised learning", "author": ["Quoc Le", "Marc\u2019Aurelio Ranzato", "Rajat Monga", "Matthieu Devin", "Kai Chen", "Greg Corrado", "Jeff Dean", "Andrew Ng"], "venue": "In International Conference in Machine Learning,", "citeRegEx": "22", "shortCiteRegEx": "22", "year": 2012}, {"title": "Cryptology: From caesar ciphers to public-key cryptosystems", "author": ["Dennis Luciano", "Gordon Prichett"], "venue": "The College Mathematics Journal,", "citeRegEx": "23", "shortCiteRegEx": "23", "year": 1987}, {"title": "Efficient estimation of word representations in vector space", "author": ["Tomas Mikolov", "Kai Chen", "Greg Corrado", "Jeffrey Dean"], "venue": "arXiv preprint arXiv:1301.3781,", "citeRegEx": "24", "shortCiteRegEx": "24", "year": 2013}, {"title": "Divergence measures and message passing", "author": ["Tom Minka"], "venue": "Technical report, Technical report, Microsoft Research,", "citeRegEx": "25", "shortCiteRegEx": "25", "year": 2005}, {"title": "English gigaword fourth edition ldc2009t13", "author": ["Robert et al Parker"], "venue": "Philadelphia: Linguistic Data Consortium,", "citeRegEx": "26", "shortCiteRegEx": "26", "year": 2009}, {"title": "Parallel distributed processing: Explorations in the microstructure of cognition, vol. 1. chapter Information Processing in Dynamical Systems: Foundations of Harmony Theory, pages 194\u2013281", "author": ["P. Smolensky"], "venue": null, "citeRegEx": "28", "shortCiteRegEx": "28", "year": 1986}, {"title": "Label-free supervision of neural networks with physics and domain knowledge", "author": ["Russell Stewart", "Stefano Ermon"], "venue": "In Proceedings of AAAI,", "citeRegEx": "29", "shortCiteRegEx": "29", "year": 2017}, {"title": "Towards principled unsupervised learning", "author": ["Ilya Sutskever", "Rafal Jozefowicz", "Karol Gregor", "Danilo Rezende", "Tim Lillicrap", "Oriol Vinyals"], "venue": "arXiv preprint arXiv:1511.06440,", "citeRegEx": "30", "shortCiteRegEx": "30", "year": 2015}], "referenceMentions": [{"referenceID": 5, "context": "[8].", "startOffset": 0, "endOffset": 3}, {"referenceID": 10, "context": "The problem we consider here is different from most studies on unsupervised learning, which concern automatic discovery of inherent regularities of the input data to learn their representations [13, 28, 18, 17, 5, 1, 31, 20, 14, 12].", "startOffset": 194, "endOffset": 232}, {"referenceID": 24, "context": "The problem we consider here is different from most studies on unsupervised learning, which concern automatic discovery of inherent regularities of the input data to learn their representations [13, 28, 18, 17, 5, 1, 31, 20, 14, 12].", "startOffset": 194, "endOffset": 232}, {"referenceID": 15, "context": "The problem we consider here is different from most studies on unsupervised learning, which concern automatic discovery of inherent regularities of the input data to learn their representations [13, 28, 18, 17, 5, 1, 31, 20, 14, 12].", "startOffset": 194, "endOffset": 232}, {"referenceID": 14, "context": "The problem we consider here is different from most studies on unsupervised learning, which concern automatic discovery of inherent regularities of the input data to learn their representations [13, 28, 18, 17, 5, 1, 31, 20, 14, 12].", "startOffset": 194, "endOffset": 232}, {"referenceID": 3, "context": "The problem we consider here is different from most studies on unsupervised learning, which concern automatic discovery of inherent regularities of the input data to learn their representations [13, 28, 18, 17, 5, 1, 31, 20, 14, 12].", "startOffset": 194, "endOffset": 232}, {"referenceID": 0, "context": "The problem we consider here is different from most studies on unsupervised learning, which concern automatic discovery of inherent regularities of the input data to learn their representations [13, 28, 18, 17, 5, 1, 31, 20, 14, 12].", "startOffset": 194, "endOffset": 232}, {"referenceID": 17, "context": "The problem we consider here is different from most studies on unsupervised learning, which concern automatic discovery of inherent regularities of the input data to learn their representations [13, 28, 18, 17, 5, 1, 31, 20, 14, 12].", "startOffset": 194, "endOffset": 232}, {"referenceID": 11, "context": "The problem we consider here is different from most studies on unsupervised learning, which concern automatic discovery of inherent regularities of the input data to learn their representations [13, 28, 18, 17, 5, 1, 31, 20, 14, 12].", "startOffset": 194, "endOffset": 232}, {"referenceID": 9, "context": "The problem we consider here is different from most studies on unsupervised learning, which concern automatic discovery of inherent regularities of the input data to learn their representations [13, 28, 18, 17, 5, 1, 31, 20, 14, 12].", "startOffset": 194, "endOffset": 232}, {"referenceID": 19, "context": "When these methods are applied in prediction tasks, either the learned representations are used as feature vectors [22] or the learned unsupervised models are used to initialize a supervised learning algorithm [9, 18, 2, 24, 10].", "startOffset": 115, "endOffset": 119}, {"referenceID": 6, "context": "When these methods are applied in prediction tasks, either the learned representations are used as feature vectors [22] or the learned unsupervised models are used to initialize a supervised learning algorithm [9, 18, 2, 24, 10].", "startOffset": 210, "endOffset": 228}, {"referenceID": 15, "context": "When these methods are applied in prediction tasks, either the learned representations are used as feature vectors [22] or the learned unsupervised models are used to initialize a supervised learning algorithm [9, 18, 2, 24, 10].", "startOffset": 210, "endOffset": 228}, {"referenceID": 1, "context": "When these methods are applied in prediction tasks, either the learned representations are used as feature vectors [22] or the learned unsupervised models are used to initialize a supervised learning algorithm [9, 18, 2, 24, 10].", "startOffset": 210, "endOffset": 228}, {"referenceID": 21, "context": "When these methods are applied in prediction tasks, either the learned representations are used as feature vectors [22] or the learned unsupervised models are used to initialize a supervised learning algorithm [9, 18, 2, 24, 10].", "startOffset": 210, "endOffset": 228}, {"referenceID": 7, "context": "When these methods are applied in prediction tasks, either the learned representations are used as feature vectors [22] or the learned unsupervised models are used to initialize a supervised learning algorithm [9, 18, 2, 24, 10].", "startOffset": 210, "endOffset": 228}, {"referenceID": 8, "context": "Recently, various solutions have been proposed to address the input-to-output prediction problem without using labeled training data, all without demonstrated successes [11, 30, 7].", "startOffset": 169, "endOffset": 180}, {"referenceID": 26, "context": "Recently, various solutions have been proposed to address the input-to-output prediction problem without using labeled training data, all without demonstrated successes [11, 30, 7].", "startOffset": 169, "endOffset": 180}, {"referenceID": 4, "context": "Recently, various solutions have been proposed to address the input-to-output prediction problem without using labeled training data, all without demonstrated successes [11, 30, 7].", "startOffset": 169, "endOffset": 180}, {"referenceID": 4, "context": "Similar to this work, the authors in [7] proposed an unsupervised cost that also exploits the sequence prior of the output samples to train classifiers.", "startOffset": 37, "endOffset": 40}, {"referenceID": 18, "context": "The power of such a strong prior in the form of language models in unsupervised learning was also demonstrated in earlier studies in [21, 3].", "startOffset": 133, "endOffset": 140}, {"referenceID": 2, "context": "The power of such a strong prior in the form of language models in unsupervised learning was also demonstrated in earlier studies in [21, 3].", "startOffset": 133, "endOffset": 140}, {"referenceID": 4, "context": "For example, it was shown in [7] that optimizing \u2217All the authors contributed equally to the paper.", "startOffset": 29, "endOffset": 32}, {"referenceID": 8, "context": "The solution provided in this paper fundamentally improves these prior works in [11, 30, 7] in following aspects.", "startOffset": 80, "endOffset": 91}, {"referenceID": 26, "context": "The solution provided in this paper fundamentally improves these prior works in [11, 30, 7] in following aspects.", "startOffset": 80, "endOffset": 91}, {"referenceID": 4, "context": "The solution provided in this paper fundamentally improves these prior works in [11, 30, 7] in following aspects.", "startOffset": 80, "endOffset": 91}, {"referenceID": 4, "context": "First, we propose a novel cost function for unsupervised learning, and find that it has a desired coverage-seeking property that makes the learning algorithm less inclined to be stuck in trivial solutions than the cost function in [7].", "startOffset": 231, "endOffset": 234}, {"referenceID": 26, "context": "Second, we develop a special empirical formulation of this cost function that avoids the need for a strong generative model as in [30].", "startOffset": 130, "endOffset": 134}, {"referenceID": 26, "context": "2 Empirical-ODM: An unsupervised learning cost for sequence classifiers In this section, we extend the earlier work of [30] and propose an unsupervised learning cost named Empirical Output Distribution Match (Empirical-ODM) for training classifiers without labeled data.", "startOffset": 119, "endOffset": 123}, {"referenceID": 25, "context": "A recent work that shares the same motivations as our work is [29], which also recognizes the high cost of obtaining labeled data and seeks label-free prediction.", "startOffset": 62, "endOffset": 66}, {"referenceID": 12, "context": "Finally, our problem is fundamentally different from the sequence transduction method in [15], although it also exploits language models for sequence prediction.", "startOffset": 89, "endOffset": 93}, {"referenceID": 12, "context": "Specifically, the method in [15] is a fully supervised learning in that it requires supervision at the sequence level; that is, for each input sequence, a corresponding output sequence (of possibly different length) is provided as a label.", "startOffset": 28, "endOffset": 32}, {"referenceID": 12, "context": "The use of language model in [15] only serves the purpose of regularization in the sequence-level supervised learning.", "startOffset": 29, "endOffset": 33}, {"referenceID": 20, "context": "It is mainly inspired by the approach to breaking the Caesar cipher, one of the simplest forms of encryption [23].", "startOffset": 109, "endOffset": 113}, {"referenceID": 26, "context": "In [30], the authors proposed to minimize an output distribution match (ODM) cost, defined as the KL-divergence between the prior output distribution and the marginalized output distribution, D(pLM(y)||p\u03b8(y)), where p\u03b8(y) , \u222b p\u03b8(y|x)p(x)dx.", "startOffset": 3, "endOffset": 7}, {"referenceID": 26, "context": "Our proposed Empirical-ODM cost is different from the ODM cost in [30] in three key aspects.", "startOffset": 66, "endOffset": 70}, {"referenceID": 26, "context": ", yN ) (N -gram) whereas in [30] y = yt (unigram, i.", "startOffset": 28, "endOffset": 32}, {"referenceID": 26, "context": "It might also explain why the method in [30] failed as it does not exploit the sequence structure.", "startOffset": 40, "endOffset": 44}, {"referenceID": 26, "context": "This is critical in that it allows us to directly minimize the divergence between two output distributions without the need for a generative model, which [30] could not do.", "startOffset": 154, "endOffset": 158}, {"referenceID": 4, "context": "3 Coverage-seeking versus mode-seeking We now discuss an important property of the proposed Empirical-ODM cost (1) by comparing it with the cost proposed in [7].", "startOffset": 157, "endOffset": 160}, {"referenceID": 4, "context": "We show that the Empirical-ODM cost has a coverage-seeking property, which makes it more suitable for unsupervised learning than the mode-seeking cost in [7].", "startOffset": 154, "endOffset": 157}, {"referenceID": 4, "context": "In [7], the authors proposed the expected negative log-likelihood as the unsupervised learning cost function that exploits the output sequential statistics.", "startOffset": 3, "endOffset": 6}, {"referenceID": 4, "context": "We show that the cost in the form of (3) proposed in [7] is a mode-seeking divergence between two distributions, while by swapping p\u03b8(\u00b7) and pLM(\u00b7), our cost in (4) becomes a coverage-seeking divergence (see [25] for a detailed discussion on divergences with these two different behaviors).", "startOffset": 53, "endOffset": 56}, {"referenceID": 22, "context": "We show that the cost in the form of (3) proposed in [7] is a mode-seeking divergence between two distributions, while by swapping p\u03b8(\u00b7) and pLM(\u00b7), our cost in (4) becomes a coverage-seeking divergence (see [25] for a detailed discussion on divergences with these two different behaviors).", "startOffset": 208, "endOffset": 212}, {"referenceID": 22, "context": "That is, the classifier is encouraged to predict a single output mode with high probability in pLM(\u00b7), a behavior called \u201cmodeseeking\u201d in [25].", "startOffset": 138, "endOffset": 142}, {"referenceID": 4, "context": "This probably explains the phenomena observed in [7]: the training process easily converges to a trivial solution of predicting the same output that has the largest probability in pLM(\u00b7).", "startOffset": 49, "endOffset": 52}, {"referenceID": 22, "context": "That is, this cost will encourage p\u03b8(y|x) to cover as much of pLM(\u00b7) as possible, a behavior called \u201ccoverage-seeking\u201d in [25].", "startOffset": 122, "endOffset": 126}, {"referenceID": 4, "context": "Therefore, training the classifier using (4) will make it less inclined to learn trivial solutions than that in [7] since it will be heavily penalized.", "startOffset": 112, "endOffset": 115}, {"referenceID": 4, "context": "In summary, our proposed cost (1) is more suitable for unsupervised learning than that in [7].", "startOffset": 90, "endOffset": 93}, {"referenceID": 16, "context": "We first use Tesseract [19] to segment the image for each line of text into characters tiles and assign each tile with one character.", "startOffset": 23, "endOffset": 27}, {"referenceID": 23, "context": "The out-of-domain data sources are completely different databases, including three different language partitions (CNA, NYT, XIN) in the English Gigaword database [26].", "startOffset": 162, "endOffset": 166}, {"referenceID": 4, "context": "The analysis provided in Sections 2 and 3 sheds insight to why SPDG is superior to the method in [7] and to the standard stochastic gradient descent (SGD) method.", "startOffset": 97, "endOffset": 100}, {"referenceID": 26, "context": "Furthermore, we do not include the methods from [30] because their approaches could not achieve satisfactory results without a few labeled data, while we only consider fully unsupervised learning setting.", "startOffset": 48, "endOffset": 52}, {"referenceID": 26, "context": "In addition, the methods in [30] are not optimizing the ODM cost and do not exploit the output sequential statistics.", "startOffset": 28, "endOffset": 32}, {"referenceID": 4, "context": "Table 1 provides strong experimental evidence demonstrating the substantially greater effectiveness of the primal-dual method over the SGD and the method in [7] on both tasks.", "startOffset": 157, "endOffset": 160}, {"referenceID": 4, "context": "Furthermore, the method from [7] does not perform well no matter how we tune the hyperparameters for the generative regularization.", "startOffset": 29, "endOffset": 32}, {"referenceID": 4, "context": "Data sets SPDG (Ours) Method from [7] SGD \u300810\u3009 SGD \u3008100\u3009 SGD \u30081k\u3009 SGD \u300810k\u3009 Supervised Learning Majority Guess OCR 9.", "startOffset": 34, "endOffset": 37}, {"referenceID": 4, "context": "Furthermore, our proposed Empirical-ODM cost function significantly improves over the one in [7] by emphasizing the coverage-seeking behavior.", "startOffset": 93, "endOffset": 96}, {"referenceID": 13, "context": ", deep neural nets [16]) in our future work.", "startOffset": 19, "endOffset": 23}, {"referenceID": 0, "context": "References [1] Yoshua Bengio.", "startOffset": 11, "endOffset": 14}, {"referenceID": 1, "context": "[2] Yoshua Bengio, Pascal Lamblin, Dan Popovici, and Hugo Larochelle.", "startOffset": 0, "endOffset": 3}, {"referenceID": 2, "context": "[3] Taylor Berg-Kirkpatrick, Greg Durrett, and Dan Klein.", "startOffset": 0, "endOffset": 3}, {"referenceID": 3, "context": "[5] David M.", "startOffset": 0, "endOffset": 3}, {"referenceID": 4, "context": "[7] Jianshu Chen, Po-Sen Huang, Xiaodong He, Jianfeng Gao, and Li Deng.", "startOffset": 0, "endOffset": 3}, {"referenceID": 5, "context": "[8] Soumith Chintala and Yann LeCun.", "startOffset": 0, "endOffset": 3}, {"referenceID": 6, "context": "[9] George E Dahl, Dong Yu, Li Deng, and Alex Acero.", "startOffset": 0, "endOffset": 3}, {"referenceID": 7, "context": "[10] Andrew M Dai and Quoc V Le.", "startOffset": 0, "endOffset": 4}, {"referenceID": 8, "context": "[11] Li Deng.", "startOffset": 0, "endOffset": 4}, {"referenceID": 9, "context": "[12] Ian Goodfellow.", "startOffset": 0, "endOffset": 4}, {"referenceID": 10, "context": "[13] Ian Goodfellow, Yoshua Bengio, and Aaron Courville.", "startOffset": 0, "endOffset": 4}, {"referenceID": 11, "context": "[14] Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron Courville, and Yoshua Bengio.", "startOffset": 0, "endOffset": 4}, {"referenceID": 12, "context": "[15] Alex Graves.", "startOffset": 0, "endOffset": 4}, {"referenceID": 13, "context": "[16] Geoffrey Hinton, Li Deng, Dong Yu, George E Dahl, Abdel-Rahman Mohamed, Navdeep Jaitly, Andrew Senior, Vincent Vanhoucke, Patrick Nguyen, Tara N.", "startOffset": 0, "endOffset": 4}, {"referenceID": 14, "context": "[17] Geoffrey E Hinton, Simon Osindero, and Yee-Whye Teh.", "startOffset": 0, "endOffset": 4}, {"referenceID": 15, "context": "[18] Geoffrey E Hinton and Ruslan R Salakhutdinov.", "startOffset": 0, "endOffset": 4}, {"referenceID": 16, "context": "[19] Anthony Kay.", "startOffset": 0, "endOffset": 4}, {"referenceID": 17, "context": "[20] Diederik P Kingma and Max Welling.", "startOffset": 0, "endOffset": 4}, {"referenceID": 18, "context": "[21] Kevin Knight, Anish Nair, Nishit Rathod, and Kenji Yamada.", "startOffset": 0, "endOffset": 4}, {"referenceID": 19, "context": "[22] Quoc Le, Marc\u2019Aurelio Ranzato, Rajat Monga, Matthieu Devin, Kai Chen, Greg Corrado, Jeff Dean, and Andrew Ng.", "startOffset": 0, "endOffset": 4}, {"referenceID": 20, "context": "[23] Dennis Luciano and Gordon Prichett.", "startOffset": 0, "endOffset": 4}, {"referenceID": 21, "context": "[24] Tomas Mikolov, Kai Chen, Greg Corrado, and Jeffrey Dean.", "startOffset": 0, "endOffset": 4}, {"referenceID": 22, "context": "[25] Tom Minka.", "startOffset": 0, "endOffset": 4}, {"referenceID": 23, "context": "[26] Robert et al Parker.", "startOffset": 0, "endOffset": 4}, {"referenceID": 24, "context": "[28] P.", "startOffset": 0, "endOffset": 4}, {"referenceID": 25, "context": "[29] Russell Stewart and Stefano Ermon.", "startOffset": 0, "endOffset": 4}, {"referenceID": 26, "context": "[30] Ilya Sutskever, Rafal Jozefowicz, Karol Gregor, Danilo Rezende, Tim Lillicrap, and Oriol Vinyals.", "startOffset": 0, "endOffset": 4}, {"referenceID": 4, "context": "Supplementary Material for \u201cUnsupervised Sequence Classification using Sequential Output Statistics\u201d A Derivation of the equivalent form of the cost in [7]", "startOffset": 152, "endOffset": 155}, {"referenceID": 4, "context": "The cost function in [7] can be expressed as:", "startOffset": 21, "endOffset": 24}], "year": 2017, "abstractText": "We consider learning a sequence classifier without labeled data by using sequential output statistics. The problem is highly valuable since obtaining labels in training data is often costly, while the sequential output statistics (e.g., language models) could be obtained independently of input data and thus with low or no cost. To address the problem, we propose an unsupervised learning cost function and study its properties. We show that, compared to earlier works, it is less inclined to be stuck in trivial solutions and avoids the need for a strong generative model. Although it is harder to optimize in its functional form, a stochastic primal-dual gradient method is developed to effectively solve the problem. Experiment results on real-world datasets demonstrate that the new unsupervised learning method gives drastically lower errors than other baseline methods. Specifically, it reaches test errors about twice of those obtained by fully supervised learning.", "creator": "LaTeX with hyperref package"}}}