{"id": "1702.07825", "review": {"conference": "ICML", "VERSION": "v1", "DATE_OF_SUBMISSION": "25-Feb-2017", "title": "Deep Voice: Real-time Neural Text-to-Speech", "abstract": "We present Deep Voice, a production-quality text-to-speech system constructed entirely from deep neural networks. Deep Voice lays the groundwork for truly end-to-end neural speech synthesis. The system comprises five major building blocks: a segmentation model for locating phoneme boundaries, a grapheme-to-phoneme conversion model, a phoneme duration prediction model, a fundamental frequency prediction model, and an audio synthesis model. For the segmentation model, we propose a novel way of performing phoneme boundary detection with deep neural networks using connectionist temporal classification (CTC) loss. For the audio synthesis model, we implement a variant of WaveNet that requires fewer parameters and trains faster than the original. By using a neural network for each component, our system is simpler and more flexible than traditional text-to-speech systems, where each component requires laborious feature engineering and extensive domain expertise. Finally, we show that inference with our system can be performed faster than real time and describe optimized WaveNet inference kernels on both CPU and GPU that achieve up to 400x speedups over existing implementations.", "histories": [["v1", "Sat, 25 Feb 2017 03:11:04 GMT  (123kb,D)", "http://arxiv.org/abs/1702.07825v1", "Submitted to ICML 2017"], ["v2", "Tue, 7 Mar 2017 23:09:23 GMT  (123kb,D)", "http://arxiv.org/abs/1702.07825v2", "Submitted to ICML 2017"]], "COMMENTS": "Submitted to ICML 2017", "reviews": [], "SUBJECTS": "cs.CL cs.LG cs.NE cs.SD", "authors": ["sercan \u00f6mer arik", "mike chrzanowski", "adam coates", "gregory frederick diamos", "andrew gibiansky", "yongguo kang", "xian li", "john miller", "andrew y ng", "jonathan raiman", "shubho sengupta", "mohammad shoeybi"], "accepted": true, "id": "1702.07825"}, "pdf": {"name": "1702.07825.pdf", "metadata": {"source": "META", "title": "Deep Voice: Real-time Neural Text-to-Speech", "authors": ["Sercan \u00d6. Ar\u0131k", "Mike Chrzanowski", "Adam Coates", "Gregory Diamos", "Andrew Gibiansky", "Yongguo Kang", "Xian Li", "John Miller", "Jonathan Raiman", "Shubho Sengupta", "Mohammad Shoeybi"], "emails": ["SERCANARIK@BAIDU.COM", "MIKECHRZANOWSKI@BAIDU.COM", "ADAMCOATES@BAIDU.COM", "GREGDIAMOS@BAIDU.COM", "GIBIANSKYANDREW@BAIDU.COM", "KANGYONGGUO@BAIDU.COM", "LIXIAN05@BAIDU.COM", "MILLERJOHN@BAIDU.COM", "JONATHANRAIMAN@BAIDU.COM", "SSENGUPTA@BAIDU.COM", "MOHAMMAD@BAIDU.COM"], "sections": [{"heading": null, "text": "\u2020 Authors are listed alphabetically by last name. Submitted on 24 February 2017 for review by the International Conference on Machine Learning (ICML) 2017. Copyright 2017 by the author (s)."}, {"heading": "1. Introduction", "text": "Synthesis of artificial human speech from text, commonly known as Text-to-Speech (TTS), is an essential component in many applications, such as speech-enabled devices, navigation systems, and accessibility for the visually impaired. Basically, it enables interaction between humans and technology without the need for visual interfaces. Modern TTS systems are based on complex, multi-level processing channels, each based on artisanal features and heuristics. Developing new TTS systems can be labor-intensive and difficult. Deep Voice is inspired by traditional text-to-speed pipelines and adopts the same structure, while all components are replaced by neural networks and the use of simple features. First, we convert text into phonemes, and then use an audio synthesis model to turn linguistic features into speech (Taylor, 2009)."}, {"heading": "2. Related Work", "text": "This year, it has reached the point where it will be able to retaliate."}, {"heading": "3. TTS System Components", "text": "As shown in Fig. 1, the TTS system consists of five important components: \u2022 The phoneme-to-phoneme model converts from written text (English characters) to phonemes (encoded with a phonetic alphabet such as ARPABET). \u2022 The phoneme-to-phoneme model locates phoneme boundaries in the voice record. Faced with an audio file and a phoneme-by-phoneme transcription of audio, the segmentation model determines where in the audio each phoneme begins and ends. \u2022 The phoneme duration model predicts the duration of each phoneme in a phoneme sequence (an utterance). \u2022 The basic frequency model predicts whether a phoneme is pronounced. If it is, the model predicts the basic frequency (F0) in the entire phoneme model."}, {"heading": "3.1. Grapheme-to-Phoneme Model", "text": "Our graphene-to-phoneme model is based on the encoder architecture developed by Yao & Zweig (Yao & Zweig, 2015), but uses a multi-layer bidirectional encoder with a gated recurrent unit (GRU) nonlinearity and an equally deep unidirectional GRU decoder (Chung et al., 2014).The initial state of each decoder layer is initialized to the final hidden state of the corresponding encoder forward layer. Architecture is trained with a teacher who coerces and decrypts by beam search.We use 3 bidirectional layers, each with 1024 units in the encoder and 3 unidirectional layers of the same size in the decoder, a beam search with a width of 5 candidates. During training, we use drop-outs with a probability of 0.95 after each recurring layer."}, {"heading": "3.2. Segmentation Model", "text": "This task is similar to the problem of aligning speech with written output in speech recognition. In this area, the connectionist time classification (CTC) loss function has proven to be a focus on character alignment to learn a mapping between sound and text (Graves et al., 2006). We are adapting the revolutionary recurrent neural network architecture from a state-of-the-art speech recognition system (Amodei et al., 2015) for phoneme boundary detection (Graves et al., 2015). A network trained with CTC to generate sequences of phonemes produces short spikes for each output phoneme. Although this is sufficient to roughly align the phonemes with the audio, it is insufficient to detect precise phoneme boundaries. To overcome this, we train to predict sequences of phoneme pairs rather than individual phonemes."}, {"heading": "3.3. Phoneme Duration and Fundamental Frequency Model", "text": "We use a single architecture to jointly predict the phoneme duration and the time-dependent fundamental frequency. Input into the model consists of a sequence of voltage phonemes, each phoneme and each stress being encoded as a hot vector; the architecture consists of two fully connected layers, each with 256 units, followed by two unidirectionally recurring layers, each with 128 GRU cells, and finally a fully connected initial layer. After the initial fully connected layers and the last recurring layer, a dropout is applied with a probability of 0.8. The last layer provides three estimates for each inserted phoneme: the phoneme duration, the probability that the phoneme is expressed (i.e. has a fundamental frequency), and 20 time-dependent F0 values, which are uniformly sampled over the predicted duration; the model is optimized by minimizing joint loss, which has a phoneme optimization, i.e. a fundamental frequency, over the predicted time-dependent F20 values."}, {"heading": "3.4. Audio Synthesis Model", "text": "Our audio synthesis model is a variant of WaveNet. WaveNet consists of a conditioning network that raises linguistic features to the desired frequency, and an auto-regressive network that generates a probability distribution P (y) via discredited audio samples y (0, 1,.., 255). We vary the number of layers, \"the number of residual channels r (dimension of the hidden state of each layer), and the number of skip channels s (the dimension on which the outputs of the layer before the output layer are projected).WaveNet consists of an upsampling and conditioning network followed by\" 2 \u00d7 1 conversion layers with residual output channels and gated tanh nonlinearity. We break up the conversion into two matrix multiplications per time step with Wprev and Wcur. These layers are connected to residual connections."}, {"heading": "4. Results", "text": "We train our models on an internal English language database containing approximately 20 hours of speech data, divided into 13,079 expressions. In addition, we present the results of audio synthesis for our models, which were trained on a subset of Blizzard data from 2013 (Prahallad et al., 2013). Both sets of data are spoken by a professional speaker. All of our models are implemented using the TensorFlow framework (Abadi et al., 2015)."}, {"heading": "4.1. Segmentation Results", "text": "We train on 8 TitanX Maxwell GPUs, dividing each batch evenly among the GPUs and using a ring that reduces them all to average gradients calculated on different GPUs, with each iteration taking about 1300 milliseconds. After about 14,000 iterations, the model converges to a phoneme pair error rate of 7%. We also find that phoneme boundaries need not be precise, and the random shifting of the phoneme boundaries by 10-30 milliseconds makes no difference in audio quality, and therefore assume that the audio quality is insensitive to the phoneme pair error rate at some point."}, {"heading": "4.2. Grapheme-to-Phoneme Results", "text": "We train a graph-to-phoneme model based on data from CMUDict (Weide, 2008), remove any words that do not begin with a letter, contain numbers, or have multiple pronunciations, leaving 124,978 of the original 133,854 graph-phoneme sequence pairs. We train on a single TitanX Maxwell GPU, with each iteration taking about 150 milliseconds. After about 20,000 iterations, the model converges to a phoneme error rate of 5.8% and a word error rate of 28.7%, which is on par with previous results (Yao & Zweig, 2015). Unlike previous work, we do not use a language model during decoding and do not include words with multiple pronunciations in our dataset."}, {"heading": "4.3. Phoneme Duration and Fundamental Frequency Results", "text": "We train on a single TitanX Maxwell GPU, with each iteration taking about 120 milliseconds. After about 20,000 iterations, the model converges to an average absolute error of 38 milliseconds (phoneme duration) and 29.4 Hz (fundamental frequency)."}, {"heading": "4.4. Audio Synthesis Results", "text": "We divide the statements in our audio data sets into one second, in which we find a quarter of a second of context for each part of the world, in which each individual utterance is confronted with a quarter of a second of silence at the beginning. We have the opportunity to explore and evaluate the quality of models with different depth, including 10, 20, 30 and 40 layers. We find that models with less than 20 layers lead to poor audio quality. Indeed, the importance of receptive field size in determining model quality is emphasized by the 20 layer models, but the 40 layer models have less noise than the layer models, which can be detected with high quality over the ears."}, {"heading": "4.5. Blizzard Results", "text": "To demonstrate the flexibility of our system, we retrained all of our models with identical hyperparameters on the Blizzard 2013 dataset (Prahallad et al., 2013), using a 20.5-hour subset of the dataset segmented into 9,741 statements, and evaluating the model using the process described in Section 4.4, which encourages rating agencies to directly compare synthetic audio to the truth. On the set at hand, 16 kHz compiled and enhanced audio received an MOS score of 4.65 \u00b1 0.13, while our synthetic audio received an MOS score of 2.67 \u00b1 0.37."}, {"heading": "5. Optimizing Inference", "text": "This year, we have reached the stage where, in the first half of the year, there will be a significant increase in unemployment in the second half of the year."}, {"heading": "5.1. CPU Implementation", "text": "In fact, it is not as if it is a matter of a way in which it is a matter of a way in which it is a matter of a way in which it is a matter of a way in which it is about a way in which it is about a way in which it is about a way in which it is about a way in which it is about a way in which it is about a way and in which it is about a way in which it is about a way and in which it is about a way in which it is about a way and in which it is about a way in which it is about a way and in which it is about a way and in which it is about a way and in which it is about a way and in which it is about a way in which it is about a way and in which it is about a way in which it is about a way and in which it is about a way in which it is about a way and in which it is about a way in which it is about a way in which it is about a way and in which it is about a way in which it is about a way in which it is about a way and in which it is about a way in which it is about a way in which it is about a way and in which it is about a way in which it is about a way in which it is about a way and in which it is about a way in which it is about a way in which it is about a way in which it is about a way and in which it is about which it is about a way in which it is about a way and in which it is about which it is about which it is about which it is about a way in which it is about a way and in which it is about which it is about which it is about which it is about which it is about which it is about which it is about which it is about which it is about which it is about which it is about which it is about which it is about which it is about which it is about which it is about which it is about which it is about which it is about which it is about which it is about which it is about which it is about which it is about which it is about which it is about which it is about which it is about which it is about which it is about which it is about which it is about which it is about which it is about which it is about which it is about which it is"}, {"heading": "5.2. GPU Implementation", "text": "Because of their computing intensity, many neural models are ultimately used on GPUs that can have much higher computing throughput than CPUs. Because our model is memory bandwidth and FLOP-bound, it may seem like a natural choice to run inferences on a GPU, but it turns out that this comes with a different set of challenges. Normally, code on the GPU is executed in a sequence of kernel calls, with each matrix multiplying or vector operation being its own kernel. However, the latency for a CUDA kernel startup (which can be up to 50\u00b5s) combined with the time it takes to load the entire model from the GPU memory, for an approach like this one is prohibitively large. An inference core of this style ends up being about 1000 times slower than the real-time kernel. To get close to real-time on a GPU, we are instead building a kernel that will specialize in using the 2016 NNNNT techniques for transferring the single NT data (the NT)."}, {"heading": "6. Conclusion", "text": "In this paper, we demonstrate that current deep learning approaches are practicable for all components of a high-quality text-to-speech engine by building a fully neural system. We optimize inferences for speeds that are faster than real-time, and demonstrate that these techniques can be applied to generate audio in real-time in a flowing manner. Our system is trainable without human involvement and dramatically simplifies the process of building TTS systems. Our work opens many new possible directions for exploration. Inference performance can be further improved by careful optimization, model quantification on GPU, and Int-8 quantization on CPU, as well as by experimenting with other architectures such as the Xeon Phi. Another natural direction is the elimination of the separation between phases and the merging of segmentation, duration prediction, and basic prediction models directly into the audio synthesis model, allowing the sequence to be transformed into a full-time sequence model, with no single-frequency influencing of the TS."}, {"heading": "A. WaveNet Architecture and Details", "text": "The WaveNet consists of a conditioning network c = C (v), which predicts low-frequency linguistic features v = 11 x to the native audio frequency and an auto-regressive process p (yi | c, yi \u2212 1,.., yi \u2212 R), which predicts the next audio sample based on the conditioning for the current timeframe c and a context of R audio examples. R is the receptive field size and is a property determined by the structure of the network. A sketch of the WaveNet architecture is shown in Figure 3. Network details are described in the following subsections. A.1. Auto-regressive WaveNetworks The structure of the auto-regressive network is parameterized by the number of layers., \"the number of sketches of the skip channels s s, and the number of residual channels r.Audio is quantified to a = 6 values, as described in section 2.e2 of WaveNet."}, {"heading": "B. Phoneme Model Loss", "text": "The loss for the n-th phoneme isLn = | t-n-tn | + \u03bb1CE (p-n, pn) + \u03bb2 T-1-t = 0 | F-0n, t-F0n, t-n, t-2-t = 0 | F-0n, t + 1-F-0n, t |, (27), which are target constants, t-n and tn are the estimated and ground truth durations of the n-phoneme, p-n and pn the estimated and ground truth probabilities that the n-phoneme is pronounced, CE the cross entropy function, F-n, t and Fn, t the estimated and ground truth values of the fundamental frequency of the n-phoneme at the time. T time samples are evenly distributed along the phoneme duration."}, {"heading": "C. Nonlinearity Approximation Details", "text": "In the conclusion, we replace exact implementations of the nonlinearity of the neural mesh with highly precise rational approximations. In this appendix, we describe the derivation of these approximations. \u2212 tanh \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 tanh \u2212 and sigmoid approximationIf we use e (x) as an approximation to e | x |, we use the following approximate values for tanh (x) and \u03c3: tanh (x) + e-characters (x) - x \u2212 1 / e-bits (x) (28) \u03c3 (x) 1 + e-bits (x) x + e-bits (x) x 0 11 + e-characters (x) x \u2212 b \u2264 0 (29) We choose a fourth-order polynomial to e-bits (x) (x) (28). The following adjustment results in exact values for both tanh (x) and \u03c3 (x): e-bits (x) 1 + e-bits (x) (x) (x) bits x-x)."}, {"heading": "D. Persistent GPU Kernels", "text": "This year it is more than ever before."}, {"heading": "E. Performance model", "text": "We present a performance model for the autoregressive WaveNet architecture, as described in Appendix A.1. In our model, a point product takes between two vectors of dimension r 2r FLOPS - r multiplications and r addition. This means that a matrix vector between W, a r \u00b7 r matrix and x, needs a r \u00b7 1 vector 2r \u00b7 r = 2r2 FLOPs. Therefore, the calculation takes h \u2032 (i) usesCost (h \u2032 (i))) = (2r \u00b7 2r) + 2r + 2r + 2r FLOPs (34) Let the division and the exponence fd and fe FLOPs take respectively. This means tanh + 2fe + 1) FLOPs. Thus, the calculation takes h (i) (i) 2r \u00b7 2r \u00b7 FLOPs + 2 (fd + 2fe + 1) + for FLOPs."}], "references": [{"title": "TensorFlow: Large-scale machine learning", "author": ["Fernanda", "Vinyals", "Oriol", "Warden", "Pete", "Wattenberg", "Martin", "Wicke", "Yu", "Yuan", "Zheng", "Xiaoqiang"], "venue": "on heterogeneous systems,", "citeRegEx": "Fernanda et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Fernanda et al\\.", "year": 2015}, {"title": "Deep speech 2: End-to-end speech recognition in english and mandarin", "author": ["Amodei", "Dario", "Anubhai", "Rishita", "Battenberg", "Eric", "Case", "Carl", "Casper", "Jared", "Catanzaro", "Bryan", "Chen", "Jingdong", "Chrzanowski", "Mike", "Coates", "Adam", "Diamos", "Greg"], "venue": "arXiv preprint arXiv:1512.02595,", "citeRegEx": "Amodei et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Amodei et al\\.", "year": 2015}, {"title": "Praat, a system for doing phonetics by computer", "author": ["Boersma", "Paulus Petrus Gerardus"], "venue": "Glot international,", "citeRegEx": "Boersma and Gerardus,? \\Q2002\\E", "shortCiteRegEx": "Boersma and Gerardus", "year": 2002}, {"title": "Empirical evaluation of gated recurrent neural networks on sequence modeling", "author": ["Chung", "Junyoung", "Gulcehre", "Caglar", "Cho", "KyungHyun", "Bengio", "Yoshua"], "venue": "arXiv preprint arXiv:1412.3555,", "citeRegEx": "Chung et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Chung et al\\.", "year": 2014}, {"title": "Peachpy meets opcodes: direct machine code generation from python", "author": ["Dukhan", "Marat"], "venue": "In Proceedings of the 5th Workshop on Python for High-Performance and Scientific Computing,", "citeRegEx": "Dukhan and Marat.,? \\Q2015\\E", "shortCiteRegEx": "Dukhan and Marat.", "year": 2015}, {"title": "Adam: A method for stochastic optimization", "author": ["D. Kingma", "J. Ba"], "venue": "arXiv preprint arXiv:1412.6980,", "citeRegEx": "Kingma and Ba,? \\Q2014\\E", "shortCiteRegEx": "Kingma and Ba", "year": 2014}, {"title": "Samplernn: An unconditional end-to-end neural audio generation model", "author": ["Mehri", "Soroush", "Kumar", "Kundan", "Gulrajani", "Ishaan", "Rithesh", "Jain", "Shubham", "Sotelo", "Jose", "Courville", "Aaron", "Bengio", "Yoshua"], "venue": "arXiv preprint arXiv:1612.07837,", "citeRegEx": "Mehri et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Mehri et al\\.", "year": 2016}, {"title": "World: a vocoder-based high-quality speech synthesis system for real-time applications", "author": ["Morise", "Masanori", "Yokomori", "Fumiya", "Ozawa", "Kenji"], "venue": "IEICE TRANSACTIONS on Information and Systems,", "citeRegEx": "Morise et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Morise et al\\.", "year": 2016}, {"title": "Pixel recurrent neural networks", "author": ["Oord", "Aaron van den", "Kalchbrenner", "Nal", "Kavukcuoglu", "Koray"], "venue": "arXiv preprint arXiv:1601.06759,", "citeRegEx": "Oord et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Oord et al\\.", "year": 2016}, {"title": "Multi-output rnn-lstm for multiple speaker speech synthesis with \u03b1interpolation model", "author": ["Pascual", "Santiago", "Bonafonte", "Antonio"], "venue": null, "citeRegEx": "Pascual et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Pascual et al\\.", "year": 2016}, {"title": "The blizzard challenge 2013indian language task", "author": ["Prahallad", "Kishore", "Vadapalli", "Anandaswarup", "Elluru", "Naresh"], "venue": "In In Blizzard Challenge Workshop", "citeRegEx": "Prahallad et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Prahallad et al\\.", "year": 2013}, {"title": "Grapheme-to-phoneme conversion using long short-term memory recurrent neural networks", "author": ["Rao", "Kanishka", "Peng", "Fuchun", "Sak", "Ha\u015fim", "Beaufays", "Fran\u00e7oise"], "venue": "In Acoustics, Speech and Signal Processing (ICASSP),", "citeRegEx": "Rao et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Rao et al\\.", "year": 2015}, {"title": "Crowdmos: An approach for crowdsourcing mean opinion score studies", "author": ["Ribeiro", "Fl\u00e1vio", "Flor\u00eancio", "Dinei", "Zhang", "Cha", "Seltzer", "Michael"], "venue": "In Acoustics, Speech and Signal Processing (ICASSP),", "citeRegEx": "Ribeiro et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Ribeiro et al\\.", "year": 2011}, {"title": "A template-based approach for speech synthesis intonation generation using lstms", "author": ["Ronanki", "Srikanth", "Henter", "Gustav Eje", "Wu", "Zhizheng", "King", "Simon"], "venue": "Interspeech 2016,", "citeRegEx": "Ronanki et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Ronanki et al\\.", "year": 2016}, {"title": "Char2wav: End-to-end speech synthesis", "author": ["Sotelo", "Jose", "Mehri", "Soroush", "Kumar", "Kundan", "Santos", "Joao Felipe", "Kastner", "Kyle", "Courville", "Aaron", "Bengio", "Yoshua"], "venue": "In ICLR 2017 workshop submission,", "citeRegEx": "Sotelo et al\\.,? \\Q2017\\E", "shortCiteRegEx": "Sotelo et al\\.", "year": 2017}, {"title": "Production Rendering, Design and Implementation", "author": ["Stephenson", "Ian"], "venue": null, "citeRegEx": "Stephenson and Ian.,? \\Q2005\\E", "shortCiteRegEx": "Stephenson and Ian.", "year": 2005}, {"title": "Text-to-Speech Synthesis", "author": ["Taylor", "Paul"], "venue": "USA, 1st edition,", "citeRegEx": "Taylor and Paul.,? \\Q2009\\E", "shortCiteRegEx": "Taylor and Paul.", "year": 2009}, {"title": "A note on the evaluation of generative models", "author": ["Theis", "Lucas", "Oord", "A\u00e4ron van den", "Bethge", "Matthias"], "venue": "arXiv preprint arXiv:1511.01844,", "citeRegEx": "Theis et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Theis et al\\.", "year": 2015}, {"title": "Wavenet: A generative model for raw audio", "author": ["van den Oord", "A\u00e4ron", "Dieleman", "Sander", "Zen", "Heiga", "Simonyan", "Karen", "Vinyals", "Oriol", "Graves", "Alex", "Kalchbrenner", "Nal", "Senior", "Andrew", "Kavukcuoglu", "Koray"], "venue": "CoRR abs/1609.03499,", "citeRegEx": "Oord et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Oord et al\\.", "year": 2016}, {"title": "The CMU pronunciation dictionary 0.7", "author": ["R. Weide"], "venue": "Carnegie Mellon University,", "citeRegEx": "Weide,? \\Q2008\\E", "shortCiteRegEx": "Weide", "year": 2008}, {"title": "Sequence-tosequence neural net models for grapheme-to-phoneme conversion", "author": ["Yao", "Kaisheng", "Zweig", "Geoffrey"], "venue": "arXiv preprint arXiv:1506.00196,", "citeRegEx": "Yao et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Yao et al\\.", "year": 2015}, {"title": "Unidirectional long short-term memory recurrent neural network with recurrent output layer for low-latency speech synthesis", "author": ["Zen", "Heiga", "Sak", "Ha\u015fim"], "venue": "In Acoustics, Speech and Signal Processing (ICASSP),", "citeRegEx": "Zen et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Zen et al\\.", "year": 2015}, {"title": "Statistical parametric speech synthesis using deep neural networks", "author": ["Zen", "Heiga", "Senior", "Andrew", "Schuster", "Mike"], "venue": "In Proceedings of the IEEE International Conference on Acoustics, Speech, and Signal Processing (ICASSP),", "citeRegEx": "Zen et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Zen et al\\.", "year": 2013}], "referenceMentions": [{"referenceID": 11, "context": "Previous work uses neural networks as substitutes for several TTS system components, including grapheme-tophoneme conversion models (Rao et al., 2015; Yao & Zweig, 2015), phoneme duration prediction models (Zen & Sak, 2015), fundamental frequency prediction models (Pascual & Bonafonte, 2016; Ronanki et al.", "startOffset": 132, "endOffset": 169}, {"referenceID": 13, "context": ", 2015; Yao & Zweig, 2015), phoneme duration prediction models (Zen & Sak, 2015), fundamental frequency prediction models (Pascual & Bonafonte, 2016; Ronanki et al., 2016), and audio synthesis models (van den Oord et al.", "startOffset": 122, "endOffset": 171}, {"referenceID": 6, "context": ", 2016), and audio synthesis models (van den Oord et al., 2016; Mehri et al., 2016).", "startOffset": 36, "endOffset": 83}, {"referenceID": 6, "context": "Most recently, there has been a lot of work in parametric audio synthesis, notably WaveNet, SampleRNN, and Char2Wav (van den Oord et al., 2016; Mehri et al., 2016; Sotelo et al., 2017).", "startOffset": 116, "endOffset": 184}, {"referenceID": 14, "context": "Most recently, there has been a lot of work in parametric audio synthesis, notably WaveNet, SampleRNN, and Char2Wav (van den Oord et al., 2016; Mehri et al., 2016; Sotelo et al., 2017).", "startOffset": 116, "endOffset": 184}, {"referenceID": 22, "context": "WaveNet uses several features from a TTS system (Zen et al., 2013), that include values such as the number of syllables in a word, position of syllables in the phrase, position of the current frame in the phoneme, and dynamic features of the speech spectrum like spectral and excitation parameters, as well as their time derivatives.", "startOffset": 48, "endOffset": 66}, {"referenceID": 7, "context": "Char2Wav relies on vocoder features from the WORLD TTS system (Morise et al., 2016) for pre-training their alignment module which include F0, spectral envelope, and aperiodic parameters.", "startOffset": 62, "endOffset": 83}, {"referenceID": 3, "context": "However, we use a multi-layer bidirectional encoder with a gated recurrent unit (GRU) nonlinearity and an equally deep unidirectional GRU decoder (Chung et al., 2014).", "startOffset": 146, "endOffset": 166}, {"referenceID": 1, "context": "We adapt the convolutional recurrent neural network architecture from a state-of-the-art speech recognition system (Amodei et al., 2015) for phoneme boundary detection.", "startOffset": 115, "endOffset": 136}, {"referenceID": 10, "context": "audio synthesis results for our models trained on a subset of the Blizzard 2013 data (Prahallad et al., 2013).", "startOffset": 85, "endOffset": 109}, {"referenceID": 19, "context": "We train a grapheme-to-phoneme model on data obtained from CMUDict (Weide, 2008).", "startOffset": 67, "endOffset": 80}, {"referenceID": 17, "context": "As is common with high-dimensional generative models (Theis et al., 2015), model loss is somewhat uncorrelated with perceptual quality of individual samples.", "startOffset": 53, "endOffset": 73}, {"referenceID": 12, "context": "To estimate perceptual quality of the individual stages of our TTS pipeline, we crowdsourced mean opinion score (MOS) ratings (ratings between one and five, higher values being better) from Mechanical Turk using the CrowdMOS toolkit and methodology (Ribeiro et al., 2011).", "startOffset": 249, "endOffset": 271}, {"referenceID": 10, "context": "To demonstrate the flexibility of our system, we retrained all of our models with identical hyperparameters on the Blizzard 2013 dataset (Prahallad et al., 2013).", "startOffset": 137, "endOffset": 161}, {"referenceID": 8, "context": "These same techniques could be used to accelerate image synthesis with PixelCNN (Oord et al., 2016) to fractions of a second per image.", "startOffset": 80, "endOffset": 99}], "year": 2017, "abstractText": "We present Deep Voice, a production-quality text-to-speech system constructed entirely from deep neural networks. Deep Voice lays the groundwork for truly end-to-end neural speech synthesis. The system comprises five major building blocks: a segmentation model for locating phoneme boundaries, a grapheme-tophoneme conversion model, a phoneme duration prediction model, a fundamental frequency prediction model, and an audio synthesis model. For the segmentation model, we propose a novel way of performing phoneme boundary detection with deep neural networks using connectionist temporal classification (CTC) loss. For the audio synthesis model, we implement a variant of WaveNet that requires fewer parameters and trains faster than the original. By using a neural network for each component, our system is simpler and more flexible than traditional text-tospeech systems, where each component requires laborious feature engineering and extensive domain expertise. Finally, we show that inference with our system can be performed faster than real time and describe optimized WaveNet inference kernels on both CPU and GPU that achieve up to 400x speedups over existing implementations. \u2020Authors are listed alphabetically by last name. Submitted February 24, 2017 for review for the International Conference on Machine Learning (ICML) 2017. Copyright 2017 by the author(s).", "creator": "LaTeX with hyperref package"}}}