{"id": "1702.07944", "review": {"conference": "ICML", "VERSION": "v1", "DATE_OF_SUBMISSION": "25-Feb-2017", "title": "Stochastic Variance Reduction Methods for Policy Evaluation", "abstract": "Policy evaluation is a crucial step in many reinforcement-learning procedures, which estimates a value function that predicts states' long-term value under a given policy. In this paper, we focus on policy evaluation with linear function approximation over a fixed dataset. We first transform the empirical policy evaluation problem into a (quadratic) convex-concave saddle point problem, and then present a primal-dual batch gradient method, as well as two stochastic variance reduction methods for solving the problem. These algorithms scale linearly in both sample size and feature dimension. Moreover, they achieve linear convergence even when the saddle-point problem has only strong concavity in the dual variables but no strong convexity in the primal variables. Numerical experiments on benchmark problems demonstrate the effectiveness of our methods.", "histories": [["v1", "Sat, 25 Feb 2017 20:15:55 GMT  (636kb,D)", "https://arxiv.org/abs/1702.07944v1", null], ["v2", "Fri, 9 Jun 2017 06:02:47 GMT  (641kb,D)", "http://arxiv.org/abs/1702.07944v2", "Accepted by ICML 2017"]], "reviews": [], "SUBJECTS": "cs.LG cs.AI cs.SY math.OC stat.ML", "authors": ["simon s du", "jianshu chen", "lihong li", "lin xiao", "dengyong zhou"], "accepted": true, "id": "1702.07944"}, "pdf": {"name": "1702.07944.pdf", "metadata": {"source": "META", "title": "Stochastic Variance Reduction Methods for Policy Evaluation", "authors": ["Simon S. Du", "Jianshu Chen", "Lihong Li", "Lin Xiao", "Dengyong Zhou"], "emails": ["<ssdu@cs.cmu.edu>,", "shuc@microsoft.com>,", "<lihongli@microsoft.com>,", "<lin.xiao@microsoft.com>,", "zho@microsoft.com>."], "sections": [{"heading": "1 Introduction", "text": "In fact, it is such that most of them will be able to move into another world, in which they are able to move into another world, in which they are able to move into another world, in which they are able to move, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they, in which they live, in which they, in which they live, in which they live, in which they, in which they live, in which they, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live in which they live, in which they live, in which they live in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they, in which they live, in which they, in which they live, in which they live, in which they, in which they live, in which they, in which they, in which they, in which they, in which they live, in which they, in which they, in which they, in which they live, in which they, in which they, in which they, in which they live, in which they, in which they, in which they, in which they, in which they live, in which they, in which they, in which they, in which they are able to live, in which they, in which they, in which they, in which they, in which they, in which they are"}, {"heading": "2 Preliminaries", "text": "We consider a Markov decision-making process (MDP) (Puterman, 2005) described by (S, A, Pass \u2032, R, \u03b3), where S is the set of states, A is the set of actions, pass \u2032 the transition probability from state s \u2032 to state s \u2032 after we have taken action, a, R (s, a) the reward we receive after we have taken action, a in state s, and \u03b3 [0, 1) a discount factor. The goal of an agent is to find an action policy \u03c0 so that the long-term reward is maximized within the framework of this policy. To make it easier, we assume that S is finite, but none of our results is based on this assumption. An important step in many algorithms in RL is to estimate the value function of a given policy \u03c0, defined as V (s), E [ells. T = 0 \u03b3tR (st, at) | s0 = s, \u03c0 s]. Let us leave a vector, which is fixed by a point \u03c0 (1)."}, {"heading": "2.1 Mean squared projected Bellman error (MSPBE)", "text": "One approach to scale when the spatial size of the state is large or infinite is to use a linear approximation for V \u03c0 (BE). Formally, we use a characteristic map \u03c6: S \u2212 1944 and approach the value function by V \u2212 \u03c0 (s) = \u03c6 (s) T \u03b8, where the model parameter to be estimated is \"Rd.\" Here, we want to find a characteristic parameter which is the fixed distribution induced by politics via S \u2212 Then the weighted projection matrix on linear space which extends through V (1) is..., Celsius (| S |), which is a diagonal matrix with diagonal elements, the stationary distribution via S \u2212 Then the weighted projection matrix on linear space which extends through V (1) is..., Celsius (| S |), i.e. a diagonal matrix with diagonal elements, the fixed distribution via S \u2212 Then the weighted projection matrix is via the IV and vice versa \u2212 space."}, {"heading": "2.2 Empirical MSPBE", "text": "In practice, quantities in (4) are often unknown, and we only have access to finite data sets with n transitions D = (st, at, rt, st + 1) nt = 1. By replacing the unknown statistics with their finite sample estimates, we obtain the empirical MSPBE, or EM-MSPBE. Specifically, letA, 1n n \u00b2 t = 1 At, b \u00b2, 1 n \u00b2 t = 1 bt, C \u00b2, 1 n \u00b2 t = 1 Ct, (5) where for t = 1,.., n, At, (t \u00b2) T, bt, rt \u00b2 t, Ct \u00b2 n \u00b2 Tt. (6) EM-MSPBE with an optional \"2-regulation\" t = 1 Ct, (5) where for t = 1, n, At, (t \u2212 3) T, (b \u00b2) T, erta \u2212 t, Ct \u00b2, Ct. \"(6) EM-MSPBE with an optional\" 2-regulation. \""}, {"heading": "3 Saddle-Point Formulation of EM-MSPBE", "text": "So it is not that we can be brought into such a form, even if the matrices A and C in the finite sum structure are given in (5). We immediately notice that the EM-MSPBE in (7) cannot be brought into such a form, even if the matrices A and C in the finite sum structure are given in (5). Nevertheless, we will show that minimizing the EMMSPBE is equivalent to a convex concave saddlepoint structure that actually has the desired structure."}, {"heading": "4 A Primal-Dual Batch Gradient Method", "text": "Before diving into the stochastic reduction of variance (S), we also define the algorithms (S) that we introduce first (A > A) that have a primary and negative double gradient (PDBG) (PDBG) algorithms to solve the saddlepoint problem (10). In step 2, the vector B (V + C) is achieved by stacking the primary and negative double gradients. (12) Some notations are needed to characterize the convergence rate of algorithms 1. (12) For each symmetrical and positive defined matrix S, we let the maximum and minimum eigenvalues (S) be denoted and define their conditional number (S)."}, {"heading": "5 Stochastic Variance Reduction Methods", "text": "If we replace B (\u03b8, w) in algorithm 1 (line 2) with the stochastic gradient Bt (\u03b8, w) in (16), then we recover from the GTD2 algorithm of Sutton et al. (2009b), which is applied to a fixed dataset, possibly with multiple passes. It has low periteration costs but a slow, sublinear convergence rate. In this section, we offer two stochastic methods for reducing variance and show that they achieve fast linear convergence."}, {"heading": "5.1 SVRG for policy evaluation", "text": "In the outer loop, the algorithm calculates a complete gradient B (v, w, w, w) using (v, w, w, w, w), the algorithm 2 SVRG for the evaluation of guidelines inputs: starting point (v, w), step quantities (v, w), number of outer iterations M, and number of inner iterations M, and number of inner iterations N. 1: for m = 1 to M, 2: starting point (v, w) and calculation B (v, w, w), step quantities (v, v), number of outer iterations M, and number of inner iterations N. 1: for m = 1 to M, 2: starting point (v, w) and calculation B (v, w, w). 3: for j (, w, w, w), w, w, w, and N number of inner iterations"}, {"heading": "5.2 SAGA for policy evaluation", "text": "The second stochastic method for reducing variance in policy assessment is adapted to SAGA (Defazio et al., 2014); see Algorithm 3. It uses a single loop and maintains a single set of parameters (\u03b8, w). Algorithm 3 starts by first calculating the gradients of each component gt = Bt (\u03b8, w) at the starting point and also establishes its average B = \u2211 n t gt. For each iteration, the algorithm randomly selects an index tm (1,..., n) and calculates the stochastic gradient htm = Btm (\u03b8, w). Subsequently, it updates the index using a stochastic gradient reduced in variance: B + htm \u2212 gtm, where gtm is the previously calculated stochastic gradient (using the tm sample htm = Btm, w)."}, {"heading": "5.3 Theoretical analyses of SVRG and SAGA", "text": "To examine the convergence characteristics of SVRG and SAGA for policy evaluation, we present a flatness parameter LG based on the stochastic gradients Bt (\u03b8, w). Let us define the relationship between the primary and dual step variables and define a pair of weighted Euclidean standard variables. (\u00b7) Upper limits of error in optimizing solutions: (\u00b7) Upper limits of error in optimizing solutions: (\u00b7) Upper limits of error (\u00b7 Upper limits of error in optimizing solutions?) Upper limits of error (\u00b7 Upper limits of error in optimizing solutions?) Upper limits of error (\u00b7 Upper limits of error in optimizing solutions?, w \u2212 w \u2212 Lower limits of error?) Upper limits of error (\u00b7 Upper limits of error): (\u00b7 Upper limits of error). Upper limits of error (\u00b7 Upper limits of error)."}, {"heading": "6 Comparison of Different Algorithms", "text": "This section compares the calculation complexities of several representative policy assessment algorithms, the EM-MSPBE, as shown in Table 1. The upper part of the table lists algorithms whose complexity is linear in the attribute dimension d, including the two new algorithms presented in the previous section. We can also apply GTD2 to a finite data set (see e.g. Lazaric et al. (2010)), which would result in an overall complexity for GTD2, but a sublinear convergence rate in relation to it. In practice, people can choose GTD2 for generalization reasons (see e.g. Lazaric et al. (2010), resulting in an overall complexity for GTD2, where a conditional number is associated with the algorithm. However, the limitations in the table show that our SVRG / SAGA-based algorithms are much faster than their effective state numbers disappear when they grow large."}, {"heading": "7 Extensions", "text": "It is possible to expand our approach to accelerate optimization of other targets such as MSBE and NEU (Dann et al., 2014). In this section we briefly describe two extensions of the previously developed algorithms."}, {"heading": "7.1 Off-policy learning", "text": "In some cases, we would like to estimate the value function of a policy \u03c0 using a dataset D generated by another \"behavioral policy,\" \u03c0b. This is referred to as non-political learning (Sutton & Barto, 1998, Chapter 8). In the non-political case, samples are generated from the distribution induced by the behavioral policy \u03c0b, not from the target policy \u03c0. While such mismatch often leads to stochastically approximated methods (Tsitsiklis & Van Roy, 1997), our gradient-based algorithms remain consistent with the same (fast) convergence rate. If we look at the RL framework outlined in Section 2., for each state-shareholder pair (st, at), we define the importance profile (at | st) > 0 instead of converging the ratio between meaning (at | st), performance (at | st) / performance (at | st) / performance (at | converge)."}, {"heading": "7.2 Learning with eligibility traces", "text": "Authorization paths are a useful technique for balancing bias and variance in TD learning (Singh & Sutton, 1996; Kearns & Singh, 2000).When used, we can pre-calculate zt in Table 2 before executing our new algorithms.Note that EM-MSPBE with authorization paths has the same form as (7), where At, bt, and Ct are differently defined according to the last row of Table 2. In the m-th step of the learning process, the algorithm randomly extracts ztm, \u03c6tm, \u03c6 \u2032 tm, and rtm from the fixed dataset and calculates the corresponding stochastic gradients, where the index tm is evenly distributed over {1,...., n} and independently for different values of m. In this case, the algorithms 1-3 work immediately and enjoy a similar linear convergence rate and computative complexity linearly in n and d."}, {"heading": "8 Experiments", "text": "In this section, we compare the following algorithms with two benchmark problems: (i) PDBG (algorithm 1); (ii) GTD2 with samples randomly drawn from a data set by substitution; (iii) TD: the fLSTD-SA algorithm by Prashanth et al. (2014); (iv) SVRG (algorithm 2); and (v) SAGA (algorithm 3). Note that if the TD solution and the EM-MSPBE minimizer differ so that we do not include the TD. For step size matching, the suggested methods are taken from {10 \u2212 1, 10 \u2212 2,."}, {"heading": "9 Conclusions", "text": "In this paper, we have reformulated the EM-MSPBE minimization problem in policy assessment as an empirical saddle point problem and developed and analyzed a stack gradient method and two first-order stochastic variance reduction methods to solve the problem. An important result we have achieved is that even if the newly formulated saddle point problem lacks strong convexity in primary variables and the proposed algorithms exhibit only a strong concavity in dual variables, a linear convergence rate can be achieved. We are not aware of similar results for primary dualbatch gradient methods or stochastic variance reduction methods. Furthermore, we have shown that if both the trait dimension d and the number of samples n are large, the developed stochastic methods for reducing variance are more efficient than any other methods for converging variance in non-political environments. This work leads to a near-linear approach, which we believe to several interesting paradigm problems in 2009."}, {"heading": "A Eigen-analysis of G", "text": "In this section we give a thorough analysis of the spectral properties of MatrixG = > Q1 = > Q1 = > Property Q = > Property Q = > Property Q (1 / 2A), (20), which is crucial for policy evaluation when analyzing the convergence of the PDBG, SAGA and SVRG algorithms. (Here, the ratio between the dual and primary step variables in these algorithms is crucial. For convenience, we use the following notation: L, \u03bbmax (A-T C-1A), \u00b5, \u03bbmin (A-T-1A).Assuming 1 that they are well defined and we have L-P > 0.A.1 diagonalizability of GFirst, we examine the state of \u03b2 that ensures the diagonalizability of matrix G. We quote the following result (Shen et al., 2008).Lemma 1."}, {"heading": "B Linear convergence of PDBG", "text": "We remember the saddle point problem that we have to solve: Minimum requirement max w L (\u03b8, w), where the Lagrangian system is defined (\"Q\" = \"Q\" = \"Q\" = \"Q\" = \"Q\" = \"Q\" = \"Q\" (\"Q\" = \"Q\" = \"Q\" = \"Q\" = \"Q\" = \"Q\" = \"Q\" = \"Q\" = \"Q\" = \"Q\" = \"Q\" = \"Q\" (\"Q\" = \"Q\" = \"Q\" = \"Q\"). The optimal solution can be expressed in this way: (\"A\" Q \"\u2212\" Q \"=\" Q \"). The optimal solution can be expressed in this way: (A\" Q \"\u2212 C\" \u2212 1A \"\u2212 1A\" = \"Q\"). \""}, {"heading": "C Analysis of SVRG", "text": "Here we establish the linear convergence of the SVRG algorithm for policy assessment, which is described in the algorithm-Q analysis. (Q-1) We reconstruct the finite sum structure in A-1, b-2 and C-2, as well as the gradient operator B-1. (44) Algorithm 2 has both an outer loop and an inner loop. We use the index m for the outer iteration and the inner iteration. (44) We use the index m for the outer iteration and an inner loop. (We use the index m for the outer iteration and j for the inner iteration."}, {"heading": "D Analysis of SAGA", "text": "SAGA in algorithm 3 maintains a table of previously calculated gradients. SAGA in algorithm 3 maintains a table of previously determined gradients. SAGA in algorithm 3 maintains a table of previously determined gradients. SAGA in algorithm 3 maintains a table of previously determined gradients. SAGA in algorithm 3 maintains a table of previously determined gradients. SAGA in algorithm 4: Q = > Q in algorithm 4: Q = > Q in algorithm 4: Q = > Q in algorithm 4: Q = > Q in algorithm 4: Q in algorithm 4: Q = > Q in algorithm 4: Q in algorithm 4: Q in algorithm 4: Q = > Q in algorithm 4: Q in algorithm 4: Q in algorithm 4: Q = Q in algorithm 4 Q: Q in algorithm 4: Q = Q in algorithm 4 Q = Q = Q in algorithm 4"}], "references": [], "referenceMentions": [], "year": 2017, "abstractText": "Policy evaluation is concerned with estimating the value function that predicts long-term values of states under a given policy. It is a crucial step in many reinforcement-learning algorithms. In this paper, we focus on policy evaluation with linear function approximation over a fixed dataset. We first transform the empirical policy evaluation problem into a (quadratic) convex-concave saddle-point problem, and then present a primal-dual batch gradient method, as well as two stochastic variance reduction methods for solving the problem. These algorithms scale linearly in both sample size and feature dimension. Moreover, they achieve linear convergence even when the saddle-point problem has only strong concavity in the dual variables but no strong convexity in the primal variables. Numerical experiments on benchmark problems demonstrate the effectiveness of our methods.", "creator": "LaTeX with hyperref package"}}}