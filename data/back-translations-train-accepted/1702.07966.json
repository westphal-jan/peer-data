{"id": "1702.07966", "review": {"conference": "ICML", "VERSION": "v1", "DATE_OF_SUBMISSION": "26-Feb-2017", "title": "Globally Optimal Gradient Descent for a ConvNet with Gaussian Inputs", "abstract": "Deep learning models are often successfully trained using gradient descent, despite the worst case hardness of the underlying non-convex optimization problem. The key question is then under what conditions can one prove that optimization will succeed. Here we provide a strong result of this kind. We consider a neural net with one hidden layer and a convolutional structure with no overlap and a ReLU activation function. For this architecture we show that learning is NP-complete in the general case, but that when the input distribution is Gaussian, gradient descent converges to the global optimum in polynomial time. To the best of our knowledge, this is the first global optimality guarantee of gradient descent on a convolutional neural network with ReLU activations.", "histories": [["v1", "Sun, 26 Feb 2017 01:12:20 GMT  (337kb,D)", "http://arxiv.org/abs/1702.07966v1", null]], "reviews": [], "SUBJECTS": "cs.LG math.OC stat.ML", "authors": ["alon brutzkus", "amir globerson"], "accepted": true, "id": "1702.07966"}, "pdf": {"name": "1702.07966.pdf", "metadata": {"source": "CRF", "title": "Globally Optimal Gradient Descent for a ConvNet with Gaussian Inputs", "authors": ["Alon Brutzkus", "Amir Globerson"], "emails": ["alonbrutzkus@mail.tau.ac.il", "gamir@cs.tau.ac.il"], "sections": [{"heading": "1 Introduction", "text": "We anticipate that there will be a fundamental improvement in the quality of learning in 2016 (Wu et al., 2016), computer vision (Krizhevsky et al., 2012) and speech recognition (Hinton et al., 2012). However, the training of such networks is often successful by minimizing a high-dimensional, non-convex objective function by using simple first-order methods such as stochastic gradient descent. Nevertheless, the success of deep learning from an optimization perspective is poorly understood. Current results are largely pessimistic, suggesting that even the formation of a 3-dimensional neural network is NP-hard (Blum & Rivest, 1993), and that the objective function of a single neuron is exponentially permissible for many local minima (Auer et al., 1996; Safran & Shamir, 2016). There are current attempts to bridge this gap between theory and practice."}, {"heading": "2 Related Work", "text": "This year, it has come to the point that it has never come as far as it has this year."}, {"heading": "3 Preliminaries", "text": "In our analysis in Section 5 and Section 7.1, we assume that the main focus is on the distribution question, but we start with a more general, hidden neural network structure with a completely hidden layer parameterized by W-Rk, d followed by an average pooling layer. Network performance is then: f (x; W) = 1k. (4) Where the distribution gap () is, we consider the realizable ReLU function, in which there is a real W-Rk that generates the training data. Population risks (see Eq. 3) are then: \"(W) where the realizable ReLU function is."}, {"heading": "6 Empirical Illustration of Tractability Gap", "text": "The results in the previous sections showed that optimizing the meshes without overlapping is generally difficult, but understandable for the Gaussian inputs. Here, we show empirically both the simple and the hard cases. The training data for the two cases is obtained by using the same w *, but different distributions over x. To create the \"hard\" case, we start with a set splitting problem. In particular, we consider a set S with 40 elements and a collection C of 760 subsets of S, each of size 20. We select Cj so that there are subsets S1, S2 that divide the subsets Cj. We use the reduction in section 4 to convert them into a set of meshes without overlapping, resulting in a training set of size 800. Knowing that the set splitting problem solves, we can use it to mark data from a different distribution."}, {"heading": "7 Networks with Overlapping Filters", "text": "A natural question is what happens if overlaps are allowed (namely, that the step is smaller than the filter size). Will the gradient descent still find a global optimum? At this point, we show that this is indeed not the case and that the gradient descent is likely to be stuck in a sub-optimal region with a probability of more than 14 percent. In Section 7.1, we analyze this setting using a two-dimensional example and provide boundaries at the level of suboptimality. In Section 7.2, we report on an empirical study on the optimization of networks with overlapping filters. Our results suggest that this setting will most likely approach the global minimum by restarting the gradient descent. Full evidence of the results can be found in Appendix D."}, {"heading": "7.1 Suboptimality of Gradient Descent for R2", "text": "We look at an instance in which there are k = d \u2212 1 neurons and matrices, why we should seek the analysis of two different cases. (...) We look at an instance in which there are k \u2212 1 neurons and matrices. (...) We look at an instance in which there are k \u2212 1 neurons and matrices. (...) The instance in which there are k \u2212 1 neurons and matrices. (...) The instance. (...) The instance. (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...).). (...). (...). (...). (...). (...).). (...). (...). (...). (...). (...). (...).). (...). (...). (...). (...). (...). (...). (...). (...). (...).). (...). (...).). (...). (...). (...). (...). (...). (...).). (...). (...).). (...). (...). (...). (...).). (...). (...). (...). (...)"}, {"heading": "7.2 Empirical study of Gradient Descent for m > 2", "text": "In Section 7.1, we showed that already for m = 2 networks with w-Rm and filter overlap have more complex behavior than those without overlap, leaving open the question of what generally happens under the Gaussian assumption with different values of d, m and overlap. We will leave the theoretical analysis of this question to future work, but report here on empirical results that indicate what the solution should look like. We experimented with a bandwidth of d, m and overlap values (see Appendix E for details of the experimental setup). For each value of d, m and overlap, we stitched 90 values of w * from different uniform input distributions with different carriers and several predefined deterministic values, resulting in more than 1,200 different samples. For each such w \u00b2, we performed multiple derivation of global mass, each randomly initialized by a different w0."}, {"heading": "8 Discussion", "text": "It is clear that a response to specific constellations where deep learning has been proven to work is needed. Thus, our specific characterization is both in terms of architecture (no overlaps, individual hidden layers and average distributions) and in terms of input distribution. We show that learning in non-overlapping architectures is difficult, so some input distribution constraints are necessary for tractability. However, note that it is certainly possible that other non-hidden layers and average distributions may also lead to tractability. Some candidates would find themselves in sub-Gaussian and protocol distributions. Our derivative addressed the population risk that can be calculated in the Gaussian case."}, {"heading": "A Proof of Lemma 3.2", "text": "First, we assume that we (V), that we (V), (V), (V), (V), (V), (V), (V), (V), (V), (V), (V), (V), (V), (V), (V), (V), (V), (V), (V), (V), (V), (V), (V), (V), (V), (V), (V), (V), (V), (V), (V), (V), (V), (V), (V), (V), (V), (V), (V), (V), (V), (V), (V), (V), (V), (V)."}, {"heading": "B Proof of Proposition 4.1", "text": "We will prove that there can be a new clause of form (x) for new variables x and y. Furthermore, we can show a reduction in the number of variables and clauses of 3SAT. Faced with a formula specified with n variables and m clauses, we can increase the number of variables and m clauses by adding a new clause of form (x) for new variables x and y. We can add n \u2212 m by adding two new identical clauses of form (z) for a new variable, e.g."}, {"heading": "C Missing Proofs for Section 5", "text": "C.1 Proof for Lemma 5.11. For w 6 = 0, Lemma 3.2 results in the claim: p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p p = p = p = p = p p"}, {"heading": "D Missing Proofs for Section 7.1", "text": "D.1 Proposition 7.1Define wp = Define wp = Define wp = Define wp = Define wp = Define wp = Define wp = Define wp = Define wp = Define wp = Define wp = Define wp = Define wp = Define wp = Define wp = Define wp = Define wp = Define wp (0, \u2212 w) definition as in Eq. 16. Then l (w) definition as in Eq wp \u2212 w (w) = 1 Define wp (k + k2 \u2212 3k + 2 definitions) w + Define wp (k \u2212 1) Define wp \u2212 wr, Define wp \u2212 wp (k \u2212 1 Define wp = Define wp = Define wp."}, {"heading": "E Experimental Setup for Section 7.2", "text": "In our experiments, we estimated the probability of convergence to the global minimum of randomly initialized gradient lineage for many different truths about the ground w \u043c of a revolutionary neural network with overlapping filters. For each value of the number of hidden neurons, filter size, stride length, and truth distribution over the ground, we randomly selected 30 different truths about the ground w \u043c with respect to the given distribution. We tested with all the value combinations shown in Table 1. In addition, for each combination of values of the number of hidden neurons, filter size, and stride length that we tested with deterministic truths about the ground: Truth about the ground with all entries equal to 1, all entries equal to -1, and with entries that have an increasing sequence of -1 to 1, -2 to 0 to 2, or decreasing sequence from 1 to -1, 0 to -2, and 2 to 0.For each truth about the ground, we ranked a gradient decline equal to -1, and with entries that have an increasing sequence of -1 to 1, -2 to 0 to 0 to 0 to 0, and 2 to 0.For each truth about the ground with all entries equal to 1, all entries equal to 1, all entries equal to -1, and with entries that have an increasing sequence of the number of hidden neurons, filter size, filter size, filter size, and stride length over the ground with decreasing sequence from 1 to 1 to -1, filter size, filter size, and step length to 0.For each truth about the ground with all entries equal 1, all entries equal to 1, all entries equal to 1, all entries equal to 1, all entries equal to 1, all entries equal to 1, all entries equal to 1, all entries equal to 1, 1, 1, 1 to 1, 1, 1 to 1, 1, 1 to 1, 1 to 1, 1, 0 to 1, 1 to 1, 1, 1, 1, 1 to 2 to 1, 1, 1, 1, 1, 1, 1 to 2 to 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1"}, {"heading": "F Uniqueness of Global Minimum in the Population Risk", "text": "Without loss of universality, we assume that the filter is of size 2 and the increment is 1. The proof for the general case follows the same lines. Let's assume that \"(w) = 0 and w = (w1, w2), w * = (w * 1, w * 2). Remember that\" (w) = EG [(f (x; W) \u2212 f (x; W *))) 2] where f (x; W) = 1k \u00b2 i \u03c3 (wi \u00b7 x) and for all 1 \u2264 i \u2264 i wi = (0i \u2212 1, w, 0d \u2212 i \u2212 1). By equating \"(w) to 0 we get the (f (x; W) \u2212 f (x; W \u0445))) 2 = 0 almost certainly. Since (f (x; W) \u2212 f (x; W) \u2212 f (x; W) \u2212 f (x; W) \u2212 f (x) is a continuous function, it results that f (x; W) \u2212 f (x; W) \u2212 f (x; W) \u2212 f (0) = implied."}], "references": [], "referenceMentions": [], "year": 2017, "abstractText": "Deep learning models are often successfully trained using gradient descent, despite the worst<lb>case hardness of the underlying non-convex optimization problem. The key question is then under<lb>what conditions can one prove that optimization will succeed. Here we provide a strong result<lb>of this kind. We consider a neural net with one hidden layer and a convolutional structure with<lb>no overlap and a ReLU activation function. For this architecture we show that learning is NP-<lb>complete in the general case, but that when the input distribution is Gaussian, gradient descent<lb>converges to the global optimum in polynomial time. To the best of our knowledge, this is the<lb>first global optimality guarantee of gradient descent on a convolutional neural network with ReLU<lb>activations.", "creator": "LaTeX with hyperref package"}}}