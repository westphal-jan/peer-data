{"id": "1702.08139", "review": {"conference": "ICML", "VERSION": "v1", "DATE_OF_SUBMISSION": "27-Feb-2017", "title": "Improved Variational Autoencoders for Text Modeling using Dilated Convolutions", "abstract": "Recent work on generative modeling of text has found that variational auto-encoders (VAE) incorporating LSTM decoders perform worse than simpler LSTM language models (Bowman et al., 2015). This negative result is so far poorly understood, but has been attributed to the propensity of LSTM decoders to ignore conditioning information from the encoder. In this paper, we experiment with a new type of decoder for VAE: a dilated CNN. By changing the decoder's dilation architecture, we control the effective context from previously generated words. In experiments, we find that there is a trade off between the contextual capacity of the decoder and the amount of encoding information used. We show that with the right decoder, VAE can outperform LSTM language models. We demonstrate perplexity gains on two datasets, representing the first positive experimental result on the use VAE for generative modeling of text. Further, we conduct an in-depth investigation of the use of VAE (with our new decoding architecture) for semi-supervised and unsupervised labeling tasks, demonstrating gains over several strong baselines.", "histories": [["v1", "Mon, 27 Feb 2017 04:16:01 GMT  (1084kb,D)", "http://arxiv.org/abs/1702.08139v1", "12 pages"], ["v2", "Sun, 18 Jun 2017 00:31:34 GMT  (6060kb,D)", "http://arxiv.org/abs/1702.08139v2", "camera ready"]], "COMMENTS": "12 pages", "reviews": [], "SUBJECTS": "cs.NE cs.CL cs.LG", "authors": ["zichao yang", "zhiting hu", "ruslan salakhutdinov", "taylor berg-kirkpatrick"], "accepted": true, "id": "1702.08139"}, "pdf": {"name": "1702.08139.pdf", "metadata": {"source": "META", "title": "Improved Variational Autoencoders for Text Modeling using Dilated Convolutions", "authors": ["Zichao Yang", "Zhiting Hu", "Ruslan Salakhutdinov", "Taylor Berg-Kirkpatrick"], "emails": ["<zichaoy@cs.cmu.edu>,", "<zhitingh@cs.cmu.edu>,", "<rsalakhu@cs.cmu.edu>,", "<tbergkir@cs.cmu.edu>."], "sections": [{"heading": "1. Introduction", "text": "In recent years, it has become clear that the subject is not only a question of reason, but also a question of reason, of reason, of reason, of reason, of reason, of reason, of reason, of reason, of reason, of reason, of reason, of reason, of reason, of reason, of reason, of reason, of reason, of reason, of reason, of reason, of reason, of reason, of reason, of reason, of reason, of reason, of reason, of reason, of reason, of reason, of reason, of reason, of reason, of reason, of reason, of reason, of reason, of reason, of reason, of reason, of reason, of reason, of reason, of reason, of reason, of reason, of reason, of reason, of reason, of reason, of reason, of reason, of reason, of reason, of reason, of reason, of reason, of reason, of reason, of reason, of reason, of reason, of reason, of reason, of reason, of reason, of reason, of reason, of reason, of reason, of reason, of reason, of reason, of reason, of reason, of reason, of reason, of reason, of reason, of reason, of reason, of reason, of reason, of reason, of reason, of reason, of reason, of reason, of reason, of reason, of reason, of reason, of reason, of reason, of reason, of reason, of reason, of reason, of reason, of reason, of reason, of reason, of reason, of reason, of, of reason, of reason, of reason, of reason, of reason, of reason, of reason, of reason, of reason, of reason, of reason, of reason, of reason, of reason, of reason, of reason, of reason, of, of reason, of reason, of reason, of reason, of, of reason, of reason, of reason, of reason, of reason, of reason, of reason, of reason, of reason, of reason, of reason, of reason, of reason, of reason, of, of reason, of reason, of, of reason, of reason, of, of reason, of, of, of reason, of reason, of, of, of reason, of reason, of reason, of reason, of reason, of reason, of reason, of reason, of, of reason,"}, {"heading": "2. Model", "text": "In this section, we begin with background information on the use of variable autoencoders for voice modeling, then introduce the advanced CNN architecture that we will use in experiments as a new decoder for UAE, and finally describe the generalization of UAE that we will use for experiments on semi-supervised classification and unattended clustering."}, {"heading": "2.1. Variational Autoencoder for Language Modeling", "text": "(1) State-of-the-art language models that generally parameterise these conditional probabilities using RNNs (2015).Bowman et al. (2015) suggests a different approach to generative text modeling. Instead of directly modelling probabilities as in Equation 1, we specify a generative process for which p x x) is a generative distribution."}, {"heading": "2.2. Dilated Convolutional Decoder", "text": "The CNN size used for text modeling (Kalchburner et al., 2016a) is comparable to the double size used for images (Krizhevsky et al., 2012; He et al., 2016), but with the folding used in one dimension. A dimensional folding: Note that xt can only be condition on past tokens x < t by using the traditional folding, it is broken and Token x strength is used as inputs to predict the text. We can either avoid this by applying a mask to the folding filter or by moving the input through multiple slots (van den Oord et al., 2016b). Here, we take the second approach and use Token x strength as inputs to predict the text."}, {"heading": "2.3. Semi-supervised VAE", "text": "In this section we briefly review VAEs of (Kingma et al., 2014) that may contain labels. Since the label (x, y) \u0445 DL and the blank label (x, y, z, y) p (z) p (x, y, z) p (x, z) p (z). (3) The semi-supervised UAE model forms a discriminatory network q (y, x), an inference network q (z, y) and a generative network p (x, y, z) p (z) p (x, z). For the described data (x, y) a discriminatory network q (x, y) is formed."}, {"heading": "3. Experiments", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "3.1. Data sets", "text": "Since we want to study UAEs for language modeling and semi-supervised classification, the datasets should be suitable for both purposes. We use two large datasets for document classification: Yahoo Answer and Yelp15 Review, which each represent the datasets for topic classification and mood classification (Tang et al., 2015; Yang et al., 2016; Zhang et al., 2015).The original datasets contain millions of samples, of which we select 100k for training and 10k for validation and testing of the respective partitions.The detailed statistics of both datasets can be found in Table 1. Yahoo Answer contains 10 topics, including Society & Culture, Science & Mathematics, etc. Yelp15 contains 5 evaluation levels, with the evaluation being better."}, {"heading": "3.2. Model configurations and Training details", "text": "We use an LSTM as an encoder for VAE and explore LSTMs and CNNs as decoder. For CNNs we explore several different configurations. We set the confrontation filter size to 3 and gradually increase the depth and dilatation of [1, 2, 4, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 30, 30, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50"}, {"heading": "3.3. Language modeling results", "text": "The results for speech modeling are presented in Table 2. We report on the negative log probability (NLL) and the perplexity (PPL) of the test series. For the NLL of VAEs, we break it down into reconstruction losses and KL divergence and report the KL divergence in brackets. To better visualize these results, we present the results of the Yahoo database (Table 2a) in Figure 4.We first look at the LM results for CNN data set. As we estimate the effective filter size of CNN, MCNN to LCNN, LCNN and LCNN from 345.3, 338.3 to 335.4. The NLL of LCNN-LM is very close to the NLL of LSTM-LM 334.9. But VLCNN-LM is a bit worse than LCNN-LM, which points to a bit more than LCNN-LM."}, {"heading": "3.4. Semi-supervised VAE results", "text": "Motivated by the success of VAEs for language modeling, we continue the research of VAEs for semi-supervised learning. Following the (Kingma et al., 2014), we set the number of designated samples at 100, 500, 1000 and 2000. Ablation Study: First, we want to investigate the effects of different decoders for semi-supervised classification. We fix the number of designated samples at 500 and report both the classification accuracy and NLL of the Yahoo dataset's test set in the table. 5. We can see that SCNN-VAESemi has the best classification accuracy of 65.5. Accuracy decreases as we gradually increase the decoder contextual capacity. On the other hand, LCNN-VAE-Semi has the best NLL result. This classification accuracy and NLL trade off once again confirms our guess."}, {"heading": "3.5. Unsupervised clustering results", "text": "We compare the baselines that extract the feature with existing models and then run the Gaussian Mixture Model (GMM) on these features. We find empirically that the simple use of the features does not work well because the features are high-dimensional. We find that a PCA runs on these features, the dimension of the PCA is selected from [8, 16, 32]. Since GMM can easily get stuck in a bad local optimum, we run each model ten times and report the best result.We directly find that the optimization of U (x) for unattended clustering does not work well and we need to initialize the encoder with the LSTM language model. The model only works well for Yahoo datasets. This is potentially because Figure 5b shows that latent mood representations do not fall into clusters."}, {"heading": "3.6. Conditional text generation", "text": "With the semi-monitored UAE, we are able to generate text that is tied to the label. For space reasons, we only show an example of generated ratings that are conditioned to evaluate ratings in Table 6. For more examples of text-generated conditioning on topic and rating, see the appendix. For each group of generated text, we set z and vary the y label. We use the Size 10 bar search in the creation process."}, {"heading": "4. Related work", "text": "Our work is consistent with previous works that deal with the combination of UAE and text modeling (Bodo Bodo Bodo Bodo Bodo, 2015; Yan et al., 2016; Salimans et al., 2015; Zhang et al., 2016). (Bodo Bodo Bodo Bodo Bodo Bodo Bodo Bodo Bodo Bodo Bodo Bodo Bodo Bodo Bodo Bodo Bodo Bodo Bodo Bodo Bodo Bodo Bodo Bodo Bodo Bodo Bodo Bodo Bodo Bodo Bodo Bodo Bodo Bodo Bodo Bodo Bodo Bodo Bodo Bodo Bodo Bodo Bodo Bodo, 2016). (Bodo Bodo Bodo Bodo Bodo Bodo Bodo Bodo Bodo Bodo Bodo Bodo Bodo Bodo Bodo Bodo Bodo Bodo Bodo Bodo Bodo Bodo Bodo Bodo Bodo Bodo Bodo Bodo Bodo Bodo Bodo Bodo Bodo Bodo Bodo Bodo Bodo Bodo Bodo Bodo Bodo Bodo Bodo Bodo Bodo Bodo Bodo Bodo Bodo Bodo Bodo Bodo Bodo Bodo Bodo Bodo Bodo Bodo Bodo Bodo Bodo Bodo Bodo Bodo Bodo Bodo, 2016). (Bodo Bodo Bodo Bodo Bodo Bodo Bodo Bodo Bodo Bodo Bodo Bodo Bodo Bodo Bodo Bodo Bodo Bodo Bodo Bodo Bodo Bodo Bodo Bodo Bodo Bodo Bodo Bodo Bodo Bodo Bodo Bodo Bodo Bodo Bodo Bodo Bodo Bodo Bodo Bodo Bodo Bodo Bodo Bodo Bodo Bodo Bodo Bodo Bodo Bodo Bodo Bodo Bodo Bodo Bodo Bodo Bodo Bodo Bodo Bodo Bodo Bodo, Bodo Bodo Bodo Bodo Bodo Bodo Bodo Bodo Bodo Bodo Bodo Bodo Bodo Bodo Bodo Bodo 2016). Combination Bodo Bodo Bodo Bodo Bodo Bodo Bodo Bodo Bodo Bodo Bodo Bodo Bodo Bodo Bodo Bodo Bodo Bodo Bodo Bodo Bodo Bodo Bodo Bodo Bodo Bodo Bodo Bodo"}, {"heading": "5. Conclusion", "text": "We have examined the context information and latent representation compromises by varying the contextual capacity of the decoder by modifying CNN architectures. We find that the UAE with a decoder with a small context window is forced to use information from the latent representation. By selecting a suitable decoder, the UAE can perform better than simple LSTM language models. We find a similar compromise between classification accuracy and NLL for semi-monitored UAEs. We show that our semi-monitored UAEs perform better than selecting strong baselines with suitable decoders. Based on our work, there are several future directions to explore, the first being to use more complex previous / posterior probability representations such as inverse auto-regressive flows to further improve the UAE results."}], "references": [{"title": "Learning stochastic recurrent networks", "author": ["References Bayer", "Justin", "Osendorfer", "Christian"], "venue": "arXiv preprint arXiv:1411.7610,", "citeRegEx": "Bayer et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Bayer et al\\.", "year": 2014}, {"title": "Generating sentences from a continuous space", "author": ["Bowman", "Samuel R", "Vilnis", "Luke", "Vinyals", "Oriol", "Dai", "Andrew M", "Jozefowicz", "Rafal", "Bengio", "Samy"], "venue": "arXiv preprint arXiv:1511.06349,", "citeRegEx": "Bowman et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Bowman et al\\.", "year": 2015}, {"title": "Variational lossy autoencoder", "author": ["Chen", "Xi", "Kingma", "Diederik P", "Salimans", "Tim", "Duan", "Yan", "Dhariwal", "Prafulla", "Schulman", "John", "Sutskever", "Ilya", "Abbeel", "Pieter"], "venue": "arXiv preprint arXiv:1611.02731,", "citeRegEx": "Chen et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Chen et al\\.", "year": 2016}, {"title": "A recurrent latent variable model for sequential data", "author": ["Chung", "Junyoung", "Kastner", "Kyle", "Dinh", "Laurent", "Goel", "Kratarth", "Courville", "Aaron C", "Bengio", "Yoshua"], "venue": "In Advances in neural information processing systems,", "citeRegEx": "Chung et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Chung et al\\.", "year": 2015}, {"title": "Semi-supervised sequence learning", "author": ["Dai", "Andrew M", "Le", "Quoc V"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Dai et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Dai et al\\.", "year": 2015}, {"title": "Sequential neural models with stochastic layers", "author": ["Fraccaro", "Marco", "S\u00f8nderby", "S\u00f8ren Kaae", "Paquet", "Ulrich", "Winther", "Ole"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Fraccaro et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Fraccaro et al\\.", "year": 2016}, {"title": "Draw: A recurrent neural network for image generation", "author": ["Gregor", "Karol", "Danihelka", "Ivo", "Graves", "Alex", "Rezende", "Danilo Jimenez", "Wierstra", "Daan"], "venue": "arXiv preprint arXiv:1502.04623,", "citeRegEx": "Gregor et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Gregor et al\\.", "year": 2015}, {"title": "Towards conceptual compression", "author": ["Gregor", "Karol", "Besse", "Frederic", "Rezende", "Danilo Jimenez", "Danihelka", "Ivo", "Wierstra", "Daan"], "venue": "In Advances In Neural Information Processing Systems,", "citeRegEx": "Gregor et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Gregor et al\\.", "year": 2016}, {"title": "Deep residual learning for image recognition", "author": ["He", "Kaiming", "Zhang", "Xiangyu", "Ren", "Shaoqing", "Sun", "Jian"], "venue": "In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition,", "citeRegEx": "He et al\\.,? \\Q2016\\E", "shortCiteRegEx": "He et al\\.", "year": 2016}, {"title": "Categorical reparameterization with gumbel-softmax", "author": ["Jang", "Eric", "Gu", "Shixiang", "Poole", "Ben"], "venue": "arXiv preprint arXiv:1611.01144,", "citeRegEx": "Jang et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Jang et al\\.", "year": 2016}, {"title": "Neural machine translation in linear time", "author": ["Kalchbrenner", "Nal", "Espeholt", "Lasse", "Simonyan", "Karen", "Oord", "Aaron van den", "Graves", "Alex", "Kavukcuoglu", "Koray"], "venue": "arXiv preprint arXiv:1610.10099,", "citeRegEx": "Kalchbrenner et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Kalchbrenner et al\\.", "year": 2016}, {"title": "Video pixel networks", "author": ["Kalchbrenner", "Nal", "Oord", "Aaron van den", "Simonyan", "Karen", "Danihelka", "Ivo", "Vinyals", "Oriol", "Graves", "Alex", "Kavukcuoglu", "Koray"], "venue": "arXiv preprint arXiv:1610.00527,", "citeRegEx": "Kalchbrenner et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Kalchbrenner et al\\.", "year": 2016}, {"title": "Adam: A method for stochastic optimization", "author": ["Kingma", "Diederik", "Ba", "Jimmy"], "venue": "arXiv preprint arXiv:1412.6980,", "citeRegEx": "Kingma et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Kingma et al\\.", "year": 2014}, {"title": "Auto-encoding variational bayes", "author": ["Kingma", "Diederik P", "Welling", "Max"], "venue": "arXiv preprint arXiv:1312.6114,", "citeRegEx": "Kingma et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Kingma et al\\.", "year": 2013}, {"title": "Semi-supervised learning with deep generative models", "author": ["Kingma", "Diederik P", "Mohamed", "Shakir", "Rezende", "Danilo Jimenez", "Welling", "Max"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Kingma et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Kingma et al\\.", "year": 2014}, {"title": "Improving variational inference with inverse autoregressive flow", "author": ["Kingma", "Diederik P", "Salimans", "Tim", "Welling", "Max"], "venue": "arXiv preprint arXiv:1606.04934,", "citeRegEx": "Kingma et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Kingma et al\\.", "year": 2016}, {"title": "Skip-thought vectors. In Advances in neural information processing", "author": ["Kiros", "Ryan", "Zhu", "Yukun", "Salakhutdinov", "Ruslan R", "Zemel", "Richard", "Urtasun", "Raquel", "Torralba", "Antonio", "Fidler", "Sanja"], "venue": null, "citeRegEx": "Kiros et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Kiros et al\\.", "year": 2015}, {"title": "Imagenet classification with deep convolutional neural networks. In Advances in neural information processing", "author": ["Krizhevsky", "Alex", "Sutskever", "Ilya", "Hinton", "Geoffrey E"], "venue": null, "citeRegEx": "Krizhevsky et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Krizhevsky et al\\.", "year": 2012}, {"title": "A neural autoregressive topic model", "author": ["Larochelle", "Hugo", "Lauly", "Stanislas"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Larochelle et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Larochelle et al\\.", "year": 2012}, {"title": "Distributed representations of sentences and documents", "author": ["Le", "Quoc V", "Mikolov", "Tomas"], "venue": "In ICML,", "citeRegEx": "Le et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Le et al\\.", "year": 2014}, {"title": "The concrete distribution: A continuous relaxation of discrete random variables", "author": ["Maddison", "Chris J", "Mnih", "Andriy", "Teh", "Yee Whye"], "venue": "arXiv preprint arXiv:1611.00712,", "citeRegEx": "Maddison et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Maddison et al\\.", "year": 2016}, {"title": "Neural variational inference for text processing", "author": ["Miao", "Yishu", "Yu", "Lei", "Blunsom", "Phil"], "venue": "In Proc. ICML,", "citeRegEx": "Miao et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Miao et al\\.", "year": 2016}, {"title": "Recurrent neural network based language model", "author": ["Mikolov", "Tomas", "Karafi\u00e1t", "Martin", "Burget", "Lukas", "Cernock\u1ef3", "Jan", "Khudanpur", "Sanjeev"], "venue": "In Interspeech,", "citeRegEx": "Mikolov et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Mikolov et al\\.", "year": 2010}, {"title": "Neural variational inference and learning in belief networks", "author": ["Mnih", "Andriy", "Gregor", "Karol"], "venue": "arXiv preprint arXiv:1402.0030,", "citeRegEx": "Mnih et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Mnih et al\\.", "year": 2014}, {"title": "Variational inference with normalizing flows", "author": ["Rezende", "Danilo Jimenez", "Mohamed", "Shakir"], "venue": "arXiv preprint arXiv:1505.05770,", "citeRegEx": "Rezende et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Rezende et al\\.", "year": 2015}, {"title": "Stochastic backpropagation and approximate inference in deep generative models", "author": ["Rezende", "Danilo Jimenez", "Mohamed", "Shakir", "Wierstra", "Daan"], "venue": "arXiv preprint arXiv:1401.4082,", "citeRegEx": "Rezende et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Rezende et al\\.", "year": 2014}, {"title": "Markov chain monte carlo and variational inference: Bridging the gap", "author": ["Salimans", "Tim", "Kingma", "Diederik P", "Welling", "Max"], "venue": "In ICML,", "citeRegEx": "Salimans et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Salimans et al\\.", "year": 2015}, {"title": "A hierarchical latent variable encoderdecoder model for generating dialogues", "author": ["Serban", "Iulian Vlad", "Sordoni", "Alessandro", "Lowe", "Ryan", "Charlin", "Laurent", "Pineau", "Joelle", "Courville", "Aaron", "Bengio", "Yoshua"], "venue": "arXiv preprint arXiv:1605.06069,", "citeRegEx": "Serban et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Serban et al\\.", "year": 2016}, {"title": "Document modeling with gated recurrent neural network for sentiment classification", "author": ["Tang", "Duyu", "Qin", "Bing", "Liu", "Ting"], "venue": "In EMNLP,", "citeRegEx": "Tang et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Tang et al\\.", "year": 2015}, {"title": "Wavenet: A generative model for raw audio", "author": ["van den Oord", "A\u00e4ron", "Dieleman", "Sander", "Zen", "Heiga", "Simonyan", "Karen", "Vinyals", "Oriol", "Graves", "Alex", "Kalchbrenner", "Nal", "Senior", "Andrew", "Kavukcuoglu", "Koray"], "venue": "CoRR abs/1609.03499,", "citeRegEx": "Oord et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Oord et al\\.", "year": 2016}, {"title": "Conditional image generation with pixelcnn decoders", "author": ["van den Oord", "Aaron", "Kalchbrenner", "Nal", "Espeholt", "Lasse", "Vinyals", "Oriol", "Graves", "Alex"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Oord et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Oord et al\\.", "year": 2016}, {"title": "Attribute2image: Conditional image generation from visual attributes", "author": ["Yan", "Xinchen", "Yang", "Jimei", "Sohn", "Kihyuk", "Lee", "Honglak"], "venue": "In European Conference on Computer Vision,", "citeRegEx": "Yan et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Yan et al\\.", "year": 2016}, {"title": "Hierarchical attention networks for document classification", "author": ["Yang", "Zichao", "Diyi", "Dyer", "Chris", "He", "Xiaodong", "Smola", "Alex", "Hovy", "Eduard"], "venue": "In Proceedings of NAACL-HLT,", "citeRegEx": "Yang et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Yang et al\\.", "year": 2016}, {"title": "Multi-scale context aggregation by dilated convolutions", "author": ["Yu", "Fisher", "Koltun", "Vladlen"], "venue": "arXiv preprint arXiv:1511.07122,", "citeRegEx": "Yu et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Yu et al\\.", "year": 2015}, {"title": "Variational neural machine translation", "author": ["Zhang", "Biao", "Xiong", "Deyi", "Su", "Jinsong", "Duan", "Hong", "Min"], "venue": "arXiv preprint arXiv:1605.07869,", "citeRegEx": "Zhang et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Zhang et al\\.", "year": 2016}, {"title": "Characterlevel convolutional networks for text classification", "author": ["Zhang", "Xiang", "Zhao", "Junbo", "LeCun", "Yann"], "venue": "In Advances in neural information processing systems,", "citeRegEx": "Zhang et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Zhang et al\\.", "year": 2015}], "referenceMentions": [{"referenceID": 1, "context": "Recent work on generative modeling of text has found that variational autoencoders (VAE) incorporating LSTM decoders perform worse than simpler LSTM language models (Bowman et al., 2015).", "startOffset": 165, "endOffset": 186}, {"referenceID": 6, "context": "Since effective variational techniques have been developed for learning VAEs (their namesake) (Kingma & Welling, 2013), these models have been successfully applied to image modeling and generation (Gregor et al., 2015; Salimans et al., 2015; Yan et al., 2016).", "startOffset": 197, "endOffset": 259}, {"referenceID": 26, "context": "Since effective variational techniques have been developed for learning VAEs (their namesake) (Kingma & Welling, 2013), these models have been successfully applied to image modeling and generation (Gregor et al., 2015; Salimans et al., 2015; Yan et al., 2016).", "startOffset": 197, "endOffset": 259}, {"referenceID": 31, "context": "Since effective variational techniques have been developed for learning VAEs (their namesake) (Kingma & Welling, 2013), these models have been successfully applied to image modeling and generation (Gregor et al., 2015; Salimans et al., 2015; Yan et al., 2016).", "startOffset": 197, "endOffset": 259}, {"referenceID": 1, "context": "However, the application of VAEs to text data has been far less successful (Bowman et al., 2015; Miao et al., 2016).", "startOffset": 75, "endOffset": 115}, {"referenceID": 21, "context": "However, the application of VAEs to text data has been far less successful (Bowman et al., 2015; Miao et al., 2016).", "startOffset": 75, "endOffset": 115}, {"referenceID": 21, "context": "Related work (Miao et al., 2016; Larochelle & Lauly, 2012; Mnih & Gregor, 2014) has used simpler decoders that model text as a bag of words.", "startOffset": 13, "endOffset": 79}, {"referenceID": 1, "context": "However, the application of VAEs to text data has been far less successful (Bowman et al., 2015; Miao et al., 2016). The obvious choice for decoding architecture for a textual VAE is an LSTM, a typical workhorse in the language processing community. Bowman et al. (2015) demonstrated negative results using VAEs for text modeling, finding that they perform worse than LSTM language models.", "startOffset": 76, "endOffset": 271}, {"referenceID": 2, "context": "We demonstrate that when this trade off is correctly managed, textual VAEs can perform substantially better than simple LSTM language models, a finding consistent with recent image modeling experiments using variational lossy autoencoders (Chen et al., 2016).", "startOffset": 239, "endOffset": 258}, {"referenceID": 22, "context": "Language models (Mikolov et al., 2010) typically generate each token xt conditioned on the entire history of previously generated tokens:", "startOffset": 16, "endOffset": 38}, {"referenceID": 1, "context": "Such models, though effective in modeling text, do not learn a vector that represents the full sequence (Bowman et al., 2015).", "startOffset": 104, "endOffset": 125}, {"referenceID": 1, "context": "For text, a RNN such as a LSTM is used as in (Bowman et al., 2015).", "startOffset": 45, "endOffset": 66}, {"referenceID": 17, "context": ", 2016a) is similar to that used for images (Krizhevsky et al., 2012; He et al., 2016), but with the convolution applied in one dimension.", "startOffset": 44, "endOffset": 86}, {"referenceID": 8, "context": ", 2016a) is similar to that used for images (Krizhevsky et al., 2012; He et al., 2016), but with the convolution applied in one dimension.", "startOffset": 44, "endOffset": 86}, {"referenceID": 8, "context": "Residual Connection: Residual connection (He et al., 2016) is used in the decoder to speed up convergence and enable us to train deep models.", "startOffset": 41, "endOffset": 58}, {"referenceID": 12, "context": "In this section, we briefly review semi-supervised VAEs of (Kingma et al., 2014) that can incorporate labels.", "startOffset": 59, "endOffset": 80}, {"referenceID": 12, "context": "(Kingma et al., 2014) proposed a semi-supervised VAE model whose latent representation contains both continuous variable z and discrete label y:", "startOffset": 0, "endOffset": 21}, {"referenceID": 9, "context": "Gumbel-Softmax: (Jang et al., 2016; Maddison et al., 2016) propose a continuous approximation to the samples of categorical distribution.", "startOffset": 16, "endOffset": 58}, {"referenceID": 20, "context": "Gumbel-Softmax: (Jang et al., 2016; Maddison et al., 2016) propose a continuous approximation to the samples of categorical distribution.", "startOffset": 16, "endOffset": 58}, {"referenceID": 28, "context": "We use two large scale document classification data sets: Yahoo Answer and Yelp15 review, representing topic classification and sentiment classification data sets respectively (Tang et al., 2015; Yang et al., 2016; Zhang et al., 2015).", "startOffset": 176, "endOffset": 234}, {"referenceID": 32, "context": "We use two large scale document classification data sets: Yahoo Answer and Yelp15 review, representing topic classification and sentiment classification data sets respectively (Tang et al., 2015; Yang et al., 2016; Zhang et al., 2015).", "startOffset": 176, "endOffset": 234}, {"referenceID": 35, "context": "We use two large scale document classification data sets: Yahoo Answer and Yelp15 review, representing topic classification and sentiment classification data sets respectively (Tang et al., 2015; Yang et al., 2016; Zhang et al., 2015).", "startOffset": 176, "endOffset": 234}, {"referenceID": 1, "context": "\u2217\u2217 is from (Bowman et al., 2015).", "startOffset": 11, "endOffset": 32}, {"referenceID": 1, "context": "For the LSTM decoder, we follow (Bowman et al., 2015) to use it as the initial state of LSTM and feed it to every step of LSTM.", "startOffset": 32, "endOffset": 53}, {"referenceID": 1, "context": "Following (Bowman et al., 2015), we also use drop word for the LSTM decoder, the drop word ratio is selected from [0, 0.", "startOffset": 10, "endOffset": 31}, {"referenceID": 1, "context": "Following (Bowman et al., 2015), we use KL cost annealing strategy.", "startOffset": 10, "endOffset": 31}, {"referenceID": 1, "context": "We can see that LSTM-VAE is worse than LSTMLM in terms of NLL and the KL term is nearly zero, which verifies the finding of (Bowman et al., 2015).", "startOffset": 124, "endOffset": 145}, {"referenceID": 12, "context": "Following that of (Kingma et al., 2014), we set the number of labeled samples to be 100, 500, 1000 and 2000 respectively.", "startOffset": 18, "endOffset": 39}, {"referenceID": 25, "context": "Variational inference through re-parameterization trick was initially proposed by (Kingma & Welling, 2013; Rezende et al., 2014) and since then, VAE has been widely adopted as generative model for images (Gregor et al.", "startOffset": 82, "endOffset": 128}, {"referenceID": 6, "context": ", 2014) and since then, VAE has been widely adopted as generative model for images (Gregor et al., 2015; Yan et al., 2016; Salimans et al., 2015; Gregor et al., 2016).", "startOffset": 83, "endOffset": 166}, {"referenceID": 31, "context": ", 2014) and since then, VAE has been widely adopted as generative model for images (Gregor et al., 2015; Yan et al., 2016; Salimans et al., 2015; Gregor et al., 2016).", "startOffset": 83, "endOffset": 166}, {"referenceID": 26, "context": ", 2014) and since then, VAE has been widely adopted as generative model for images (Gregor et al., 2015; Yan et al., 2016; Salimans et al., 2015; Gregor et al., 2016).", "startOffset": 83, "endOffset": 166}, {"referenceID": 7, "context": ", 2014) and since then, VAE has been widely adopted as generative model for images (Gregor et al., 2015; Yan et al., 2016; Salimans et al., 2015; Gregor et al., 2016).", "startOffset": 83, "endOffset": 166}, {"referenceID": 1, "context": "Our work is in line with previous works on combining variational inferences with text modeling (Bowman et al., 2015; Miao et al., 2016; Serban et al., 2016; Zhang et al., 2016).", "startOffset": 95, "endOffset": 176}, {"referenceID": 21, "context": "Our work is in line with previous works on combining variational inferences with text modeling (Bowman et al., 2015; Miao et al., 2016; Serban et al., 2016; Zhang et al., 2016).", "startOffset": 95, "endOffset": 176}, {"referenceID": 27, "context": "Our work is in line with previous works on combining variational inferences with text modeling (Bowman et al., 2015; Miao et al., 2016; Serban et al., 2016; Zhang et al., 2016).", "startOffset": 95, "endOffset": 176}, {"referenceID": 34, "context": "Our work is in line with previous works on combining variational inferences with text modeling (Bowman et al., 2015; Miao et al., 2016; Serban et al., 2016; Zhang et al., 2016).", "startOffset": 95, "endOffset": 176}, {"referenceID": 1, "context": "(Bowman et al., 2015) is the first work to combine VAE with language model and they use LSTM as the decoder and find some negative results.", "startOffset": 0, "endOffset": 21}, {"referenceID": 21, "context": "On the other hand, (Miao et al., 2016) models text as bag of words, though improvement has been found, the model can not be used to generate text.", "startOffset": 19, "endOffset": 38}, {"referenceID": 27, "context": "(Serban et al., 2016; Zhang et al., 2016) applies variational inference to dialogue modeling and machine translation and found some improvement in terms of generated text quality, but no language modeling results are reported.", "startOffset": 0, "endOffset": 41}, {"referenceID": 34, "context": "(Serban et al., 2016; Zhang et al., 2016) applies variational inference to dialogue modeling and machine translation and found some improvement in terms of generated text quality, but no language modeling results are reported.", "startOffset": 0, "endOffset": 41}, {"referenceID": 3, "context": "(Chung et al., 2015; Bayer & Osendorfer, 2014; Fraccaro et al., 2016) embedded variational units in every step of a RNN, which is different from our model in using global latent variables to learn high level features.", "startOffset": 0, "endOffset": 69}, {"referenceID": 5, "context": "(Chung et al., 2015; Bayer & Osendorfer, 2014; Fraccaro et al., 2016) embedded variational units in every step of a RNN, which is different from our model in using global latent variables to learn high level features.", "startOffset": 0, "endOffset": 69}, {"referenceID": 2, "context": "Our work is closed related the recently proposed variational lossy autoencoder (Chen et al., 2016) which is used to predict image pixels.", "startOffset": 79, "endOffset": 98}, {"referenceID": 15, "context": "Much (Rezende & Mohamed, 2015; Kingma et al., 2016; Chen et al., 2016) has been done to come up more powerful prior/posterior distribution representations with techniques such as normalizing flows.", "startOffset": 5, "endOffset": 70}, {"referenceID": 2, "context": "Much (Rezende & Mohamed, 2015; Kingma et al., 2016; Chen et al., 2016) has been done to come up more powerful prior/posterior distribution representations with techniques such as normalizing flows.", "startOffset": 5, "endOffset": 70}, {"referenceID": 16, "context": "There are many previous works that explore unsupervised sentence encoding such as skip-thought vectors (Kiros et al., 2015), paragraph vector (Le & Mikolov, 2014) and sequence autoencoder (Dai & Le, 2015).", "startOffset": 103, "endOffset": 123}], "year": 2017, "abstractText": "Recent work on generative modeling of text has found that variational autoencoders (VAE) incorporating LSTM decoders perform worse than simpler LSTM language models (Bowman et al., 2015). This negative result is so far poorly understood, but has been attributed to the propensity of LSTM decoders to ignore conditioning information from the encoder. In this paper, we experiment with a new type of decoder for VAE: a dilated CNN. By changing the decoder\u2019s dilation architecture, we control the effective context from previously generated words. In experiments, we find that there is a trade off between the contextual capacity of the decoder and the amount of encoding information used. We show that with the right decoder, VAE can outperform LSTM language models. We demonstrate perplexity gains on two datasets, representing the first positive experimental result on the use VAE for generative modeling of text. Further, we conduct an in-depth investigation of the use of VAE (with our new decoding architecture) for semi-supervised and unsupervised labeling tasks, demonstrating gains over several strong baselines.", "creator": "LaTeX with hyperref package"}}}