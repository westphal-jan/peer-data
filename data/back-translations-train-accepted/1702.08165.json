{"id": "1702.08165", "review": {"conference": "ICML", "VERSION": "v1", "DATE_OF_SUBMISSION": "27-Feb-2017", "title": "Reinforcement Learning with Deep Energy-Based Policies", "abstract": "We propose a method for learning expressive energy-based policies for continuous states and actions, which has been feasible only in tabular domains before. We apply our method to learning maximum entropy policies, resulting into a new algorithm, called soft Q-learning, that expresses the optimal policy via a Boltzmann distribution. We use the recently proposed amortized Stein variational gradient descent to learn a stochastic sampling network that approximates samples from this distribution. The benefits of the proposed algorithm include improved exploration and compositionality that allows transferring skills between tasks, which we confirm in simulated experiments with swimming and walking robots. We also draw a connection to actor-critic methods, which can be viewed performing approximate inference on the corresponding energy-based model.", "histories": [["v1", "Mon, 27 Feb 2017 07:16:41 GMT  (4137kb,D)", "http://arxiv.org/abs/1702.08165v1", null], ["v2", "Fri, 21 Jul 2017 20:25:54 GMT  (4966kb,D)", "http://arxiv.org/abs/1702.08165v2", null]], "reviews": [], "SUBJECTS": "cs.LG cs.AI", "authors": ["tuomas haarnoja", "haoran tang", "pieter abbeel", "sergey levine"], "accepted": true, "id": "1702.08165"}, "pdf": {"name": "1702.08165.pdf", "metadata": {"source": "META", "title": "Reinforcement Learning with Deep Energy-Based Policies", "authors": ["Tuomas Haarnoja", "Haoran Tang", "Pieter Abbeel", "Sergey Levine"], "emails": ["<haarnoja@berkeley.edu>,", "<hrtang@berkeley.edu>."], "sections": [{"heading": "1. Introduction", "text": "This year, it has reached the point where it will be able to retaliate."}, {"heading": "2. Preliminaries", "text": "In this section, we will define the problem of enhanced learning that we are addressing and briefly summarize the maximum search goal of entropy politics. We will also present some useful identities on which we will build in our algorithm presented in Section 3."}, {"heading": "2.1. Maximum Entropy Reinforcement Learning", "text": "Our affirmation problem can be defined as a political search in an endless horizon Markov decision-making process (MDP) consisting of the tupel (S, A, ps, r) that actually exhibits objective discounting problems (S, A, ps, r), the state space S and the action space A are considered continuous, and the state transition probability ps: S \u00d7 S \u00d7 A \u2192 [0, 1] represents the probability density of the next state + 1 \"S given the current state of the S and the action space A.\" The environment emits a reward r: S \u00d7 A \u2192 [rmin, rmax] on each transition that we will abbreviate as rt, r (st, at) in order to simplify notation. We will also use the conventional method that (st) and (st) to distribute the state and the state's margins of government.Our goal is to learn a policy (st) that we are explicitly decentralized."}, {"heading": "2.2. Soft Value Functions and Energy-Based Models", "text": "Optimizing the maximum entropy target in Equation 2 provides a framework for the formation of stochastic Q = Q functions, but we have yet to select a representation for these strategies. However, decisions made in previous work include discrete multinomial distributions (O'Donoghue et al., 2016) and Gaussian distributions (Rawlik et al., 2012). However, if we want to use a very general class of distributions that can represent complex, multimodal behaviors, we can instead opt for the use of general energy-based strategies of formatics (at | st), Exp (\u2212 E (st, at), (3) where E (st, at) is an energy function that could, for example, be represented by a deep network. If we use a universal function approximator for E, we can represent any distribution method (at st)."}, {"heading": "3. Training Expressive Energy-Based Models via Soft Q-Learning", "text": "In this section, we will present our proposed reinforcement learning algorithm, which is based on the Soft-Q function described in the previous section, but can be implemented via a tractable stochastic gradient descendant method with approximate sampling. We will first describe the general case of Soft-Q learning and then present the inference method, which makes it tractable to use deep neural network representations in high-dimensional continuous state and action spaces. We will correlate this Qlearning method with conclusions in energy-based models and action-critical algorithms."}, {"heading": "3.1. Soft Q-Iteration", "text": "This results in a fixed-point titeration similar to Q iteration: Theorem 3. Soft Q-iteration. LetQsoft (\u00b7, \u00b7) and Vsoft (\u00b7) are limited and assume that \"A exp (1 \u03b1Qsoft (\u00b7, a \u00b2) da\" < and that \"Q \u00b2 soft (st, a \u00b2) exists. Then, the fixed-point titerationQsoft (st, at) \u2190 rt + \u03b3 Est + 1% ps [Vsoft (st + 1)],\" st, at (8) converges Vsoft (st)."}, {"heading": "3.2. Soft Q-Learning", "text": "This section discusses how the Bellman backup can be converted into a practical Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-"}, {"heading": "3.3. Approximate Sampling and Stein Variational Gradient Descent (SVGD)", "text": "In this section, we describe how to create an approximate sample from the Soft-Q function. Existing approaches derived from energy-based distributions typically fall into two categories: methods based on the Markov Monte Carlo chain (MCMC) based on sampling (Sallans & Hinton, 2004) and methods that learn a stochastic sampling network based on approximate samples from target distribution (Zhao et al., 2016; Kim & Bengio, 2016). Since sampling via MCMC is incomprehensible when the conclusion has to be made online (e.g. when a policy is being implemented), we will use a sampling network based on stone variations (SVGD) and an amortized SVGD (Wang & Liu, 2016). Amortized SVGD has several fascinating properties: First, it provides us with a stochastic sampling network that we can use for extremely fast sampling."}, {"heading": "3.4. Algorithm Summary", "text": "In summary, we propose the Soft-Q Learning algorithm for learning maximum entropy strategies in continuous areas, which alternately works by gathering new experiences from the environment and updating the Soft-Q function and scanning network parameters; the experiences are stored in a replay memory buffer D as the standard in Deep Q Learning (Mnih et al., 2013), and the parameters are updated from this memory by means of random minibatches; the Soft-Q Function updates use a delayed version of the target values, characterized by a smoothing factor \u03c4, to improve stability (Mnih et al., 2013); for optimization, we use the ADAM (Kingma & Ba, 2015) optimizer and empirical estimates of the gradients, which we call \"Soft-Learning.\" The exact formulas for calculating the gradient estimates are moved to Appendix C, but we also discuss other details of the implementation in the Soft-Q algorithm."}, {"heading": "4. Related Work", "text": "Sample the new experience in the replay memory: D (st, r, at).Sample a minibatch from the replay memory: (i) t, a (i) t, s (i) t, s (i).Sample the new experience in the replay memory: D (st, r, at).Sample a minibatch from the replay memory: (i) t, a (i) t, s (i) t, s (i).Sample the new experience in the replay memory: D (st, r, at).Sample a minibatch from the replay memory: (i) t, s (i) t, s (i) t, s (i).Sample the new experience in the replay memory: D (st, r, at).Sample a minibatch from the replay memory: (s, i) t.Sample a minibatch from the replay memory: (i) t, a (i) t, s (i) t, s (i)."}, {"heading": "5. Experiments", "text": "Our experiments aim to answer the following questions: (1) Does our soft Q-learning method accurately capture multimodal policy distribution? (2) Can soft Q-learning with energy-based strategies support exploration for complex tasks requiring multi-mode tracking? (3) Can a maximum entropy policy serve as a good initialization for fine-tuning different tasks, compared to pre-training with a standard deterministic goal? We compare our algorithm with DDPG (Lillicrap et al., 2015), which showed that better sampling efficiency is achieved in the continuous control problems that we use as other newer techniques such as REINFORCE (Williams, 1992), TRPO (Schulman et al., 2015a) and A3C (Mnih et al., 2016). This comparison is particularly interesting because, as discussed in Section 4, the DPG closely coincides with a deterrent maximum variation of our postministerial method."}, {"heading": "5.1. Didactic Example: Multi-Goal Environment", "text": "To verify that amortized SVGD samples can be correctly taken from energy-based strategies of the form exp (Q\u03b8soft (s, a)), and that our complete algorithm can successfully learn to represent multimodal behavior, we have designed a simple \"multi-target\" environment in which the agent is a 2D point mass that attempts to achieve one of four symmetrically placed targets. Reward is defined as a mixture of Gaussians, with the means placed at target positions. An optimal strategy is to arrive at any target, but the optimal maximum entropy policy should be able to randomly select each of the four targets. The final policy achieved with our method is illustrated in Figure 1. Q values do indeed have complicated shapes, as they are at s = (\u2212 2, 0) unimodal, convex s = 0, and bichal (2.5) to follow the different energy samples."}, {"heading": "5.2. Learning Multi-Modal Policies for Exploration", "text": "Although not all environments have a clear multimodal reward landscape, as in the \"multi-target\" example, multimodality prevails in a variety of tasks: a chess player might try different strategies before adjusting to one that seems most effective, and an agent controlling a maze must try different ways before finding the exit (Lai & Robbins, 1985), but deep RL algorithms for continuous control are often best to try several available options until the agent is sure that one of them is the best (Lai & Robbins, 1985)."}, {"heading": "5.3. Accelerating Training on Complex Tasks with Pretrained Maximum Entropy Policies", "text": "A standard method for accelerating the neural network is task-specific initialization (Goodfellow et al., 2016), in which a network is used for one task as initialization for another. The first task could be something very general, such as classifying a large image dataset, while the second task could be more specific, such as fine-grained classification with a small dataset. Pretraining has also been explored in the context of RL (Shelhamer et al., 2016)."}, {"heading": "6. Discussion and Future Work", "text": "We presented a method for learning stochastic energy-based strategies with approximate conclusions about Stein Variational Gradient Descent (SVGD). Our approach can also be seen as a kind of soft Q-learning method, with the additional contribution of using approximate conclusions about complex multimodal strategies. SVGD-trained sample networks can also be seen as linking the role of an actor in an actor-critic algorithm. Our experimental results show that our method can effectively capture complex multimodal behavior, from problems ranging from toy point mass tasks to complex torque control of simulated walking and swimming robots. Applications of training such stochastic strategies include improved exploration in the case of multimodal goals and comfort through pre-training of universal stochastic strategies that can then efficiently incorporate task-specific behaviors. While our work may consist of assessing potential final applications for this non-hazardous strategy, we may be exploring a more complex possibility of finding these strategies."}, {"heading": "A. Policy Improvement Proofs", "text": "In this appendix, we present evidence for the theorems that allow us to show that soft Q-learning leads to an improvement in policy regarding the maximum entropy target. First, we define a slightly more nuanced version of the maximum entropy target that allows us to integrate a discount factor. This definition is complicated by the fact that when we use a discount factor for political gradients, we do not normally discount the government distribution, but only the rewards that optimize under a discount factor. Instead, they optimize the average reward, with the discount factor serving to reduce the variance, as discussed by Thomas (2014). However, for the purposes of derivation, we can define the target that is optimized under a discount factor. MaxEnt = arg max."}, {"heading": "B. Connection between Policy Gradient and Q-Learning", "text": "We show that Entropie-regulated political gradient as soft Q-Learning (Q-Learning) (Q-Learning) (Q-Learning) (Q-Learning) (Q-Learning) (Q-Learning) (Q-Learning) (Q-Learning) (Q-Learning) (Q-Learning) (Q-Learning) (Q-Learning) (Q-Learning) (Q-Learning) (Q-Learning) (Q-Learning) (Q-Learning) (Q-Learning) (Q-Learning) (Q-Learning) (Q-Learning) (Q-Learning) (Q-Learning) (Q-Learning) (Q-Learning) (Q-Learning) (Q-Learning) (Q-Learning) (Q-Learning) (Q-Learning) (Q-Learning) (Q-Learning)) (Q-Learning) (Q-Learning) (Q-Learning) (Q-Learning) (Q-Learning) (Q-Learning) (Q-Learning) (Q-Learning) (Q-Learning) (Q-Learning) (Q-Learning) (Q-Learning) (Q-Learning) (Q-Learning) (Q-Learning) (Q-Learning) (Q-Learning) (Q-Learning) (Q-Learning) (Q-Learning) (Q-Learning) (Q-Learning) (Q-Learning) (Q-Learning) (Q-Learning) (Q-Learning) (Q-Learning) (Q-Learning) (Q-Learning (Q-Learning) (Q-Learning) (Q-Learning) (Q-Learning) (Q-Learning) (Q-Learning (Q-Learning) (Q-Learning) (Q-Learning) (Q-Learning (Q-Learning) (Q-Learning) (Q-Learning) (Q-Learning (Q-Learning) (Q-Learning) (Q-Learning) (Q-Learning) (Q-Learning (Q-Learning) (Q-Learning) (Q-"}, {"heading": "D. Experiments", "text": "The Q-values are updated with ADAM with the learning rate 0.001. The DPG policy and the Soft Q Learning network use ADAM with a learning rate of 0.0001. The algorithm uses a replay pool of one million. Training begins only when the replay pool has at least 10,000 samples. Each mini-batch is size 64. Each training unit consists of 10,000 time steps, and both the Q values and the Policy / Sampling network are trained at each time step. All experiments are conducted for 500 epochs, except that the multi-target task uses 100 epochs and the fine-tuning tasks are trained for 200 epochs. Both the Q value and the Policy / Sampling networks are neural networks of two hidden layers, with 200 hidden units at each level and ReLU non-linearity. Both DPG and Soft Q-Learning use additional OU networks, which we improve."}], "references": [{"title": "Linear Bellman combination for control of character animation", "author": ["M. Da Silva", "F. Durand", "J. Popovi\u0107"], "venue": "ACM Trans. on Graphs,", "citeRegEx": "Silva et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Silva et al\\.", "year": 2009}, {"title": "Freeenergy based reinforcement learning for vision-based navigation with high-dimensional sensory inputs", "author": ["S. Elfwing", "M. Otsuka", "E. Uchibe", "K. Doya"], "venue": "In Int. Conf. on Neural Information Processing,", "citeRegEx": "Elfwing et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Elfwing et al\\.", "year": 2010}, {"title": "Stochastic neural networks for hierarchical reinforcement learning", "author": ["C. Florensa", "Y. Duan", "Abbeel"], "venue": "In Int. Conf. on Learning Representations,", "citeRegEx": "Florensa et al\\.,? \\Q2017\\E", "shortCiteRegEx": "Florensa et al\\.", "year": 2017}, {"title": "Taming the noise in reinforcement learning via soft updates", "author": ["R. Fox", "A. Pakman", "N. Tishby"], "venue": "In Conf. on Uncertainty in Artificial Intelligence,", "citeRegEx": "Fox et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Fox et al\\.", "year": 2016}, {"title": "Deep learning. chapter 8.7.4", "author": ["Goodfellow", "Ian", "Bengio", "Yoshua", "Courville", "Aaron"], "venue": null, "citeRegEx": "Goodfellow et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Goodfellow et al\\.", "year": 2016}, {"title": "Q-prop: Sample-efficient policy gradient with an off-policy critic", "author": ["S. Gu", "T. Lillicrap", "Z. Ghahramani", "R.E. Turner", "S. Levine"], "venue": "arXiv preprint arXiv:1611.02247,", "citeRegEx": "Gu et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Gu et al\\.", "year": 2016}, {"title": "Continuous deep Q-learning with model-based acceleration", "author": ["S. Gu", "T. Lillicrap", "I. Sutskever", "S. Levine"], "venue": "In Int. Conf. on Machine Learning,", "citeRegEx": "Gu et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Gu et al\\.", "year": 2016}, {"title": "Reinforcement learning in feedback control", "author": ["R. Hafner", "M. Riedmiller"], "venue": "Machine Learning,", "citeRegEx": "Hafner and Riedmiller,? \\Q2011\\E", "shortCiteRegEx": "Hafner and Riedmiller", "year": 2011}, {"title": "Actor-critic reinforcement learning with energy-based policies", "author": ["N. Heess", "D. Silver", "Y.W. Teh"], "venue": "In Workshop on Reinforcement Learning,", "citeRegEx": "Heess et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Heess et al\\.", "year": 2012}, {"title": "Learning and transfer of modulated locomotor controllers", "author": ["N. Heess", "G. Wayne", "Y. Tassa", "T. Lillicrap", "M. Riedmiller", "D. Silver"], "venue": "arXiv preprint arXiv:1610.05182,", "citeRegEx": "Heess et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Heess et al\\.", "year": 2016}, {"title": "Reinforcement learning with unsupervised auxiliary tasks", "author": ["M. Jaderberg", "V. Mnih", "W.M. Czarnecki", "T. Schaul", "J.Z. Leibo", "D. Silver", "K. Kavukcuoglu"], "venue": "arXiv preprint arXiv:1611.05397,", "citeRegEx": "Jaderberg et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Jaderberg et al\\.", "year": 2016}, {"title": "Reinforcement learning: A survey", "author": ["L.P. Kaelbling", "M.L. Littman", "A.W. Moore"], "venue": "Journal of artificial intelligence research,", "citeRegEx": "Kaelbling et al\\.,? \\Q1996\\E", "shortCiteRegEx": "Kaelbling et al\\.", "year": 1996}, {"title": "A natural policy gradient", "author": ["S. Kakade"], "venue": "Advances in Neural Information Processing Systems,", "citeRegEx": "Kakade,? \\Q2002\\E", "shortCiteRegEx": "Kakade", "year": 2002}, {"title": "Path integrals and symmetry breaking for optimal control theory", "author": ["H.J. Kappen"], "venue": "Journal of Statistical Mechanics: Theory And Experiment,", "citeRegEx": "Kappen,? \\Q2005\\E", "shortCiteRegEx": "Kappen", "year": 2005}, {"title": "Deep directed generative models with energy-based probability estimation", "author": ["T. Kim", "Y. Bengio"], "venue": "arXiv preprint arXiv:1606.03439,", "citeRegEx": "Kim and Bengio,? \\Q2016\\E", "shortCiteRegEx": "Kim and Bengio", "year": 2016}, {"title": "Adam: A method for stochastic optimization", "author": ["D. Kingma", "J. Ba"], "venue": null, "citeRegEx": "Kingma and Ba,? \\Q2015\\E", "shortCiteRegEx": "Kingma and Ba", "year": 2015}, {"title": "Asymptotically efficient adaptive allocation rules", "author": ["Lai", "Tze Leung", "Robbins", "Herbert"], "venue": "Advances in Applied Mathematics,", "citeRegEx": "Lai et al\\.,? \\Q1985\\E", "shortCiteRegEx": "Lai et al\\.", "year": 1985}, {"title": "Learning neural network policies with guided policy search under unknown dynamics", "author": ["S. Levine", "P. Abbeel"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Levine and Abbeel,? \\Q2014\\E", "shortCiteRegEx": "Levine and Abbeel", "year": 2014}, {"title": "End-to-end training of deep visuomotor policies", "author": ["S. Levine", "C. Finn", "T. Darrell", "P. Abbeel"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Levine et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Levine et al\\.", "year": 2016}, {"title": "Continuous control with deep reinforcement learning", "author": ["T.P. Lillicrap", "J.J. Hunt", "A. Pritzel", "N. Heess", "T. Erez", "Y. Tassa", "D. Silver", "D. Wierstra"], "venue": "arXiv preprint arXiv:1509.02971,", "citeRegEx": "Lillicrap et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Lillicrap et al\\.", "year": 2015}, {"title": "Stein variational gradient descent: A general purpose bayesian inference algorithm", "author": ["Q. Liu", "D. Wang"], "venue": "In Advances In Neural Information Processing Systems,", "citeRegEx": "Liu and Wang,? \\Q2016\\E", "shortCiteRegEx": "Liu and Wang", "year": 2016}, {"title": "Playing atari with deep reinforcement learning", "author": ["V. Mnih", "K. Kavukcuoglu", "D. Silver", "A. Graves", "I. Antonoglou", "D. Wierstra", "M. Riedmiller"], "venue": "arXiv preprint arXiv:1312.5602,", "citeRegEx": "Mnih et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Mnih et al\\.", "year": 2013}, {"title": "Human-level control through deep reinforcement learning", "author": ["V. Mnih", "K. Kavukcuoglu", "D. Silver", "Rusu", "A. A", "J. Veness", "M.G. Bellemare", "A. Graves", "M. Riedmiller", "A.K. Fidjeland", "G Ostrovski"], "venue": null, "citeRegEx": "Mnih et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Mnih et al\\.", "year": 2015}, {"title": "Asynchronous methods for deep reinforcement learning", "author": ["V. Mnih", "A.P. Badia", "M. Mirza", "A. Graves", "T.P. Lillicrap", "T. Harley", "D. Silver", "K. Kavukcuoglu"], "venue": "In Int. Conf. on Machine Learning,", "citeRegEx": "Mnih et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Mnih et al\\.", "year": 2016}, {"title": "Variational inference for policy search in changing situations", "author": ["G. Neumann"], "venue": "In Int. Conf. on Machine Learning,", "citeRegEx": "Neumann,? \\Q2011\\E", "shortCiteRegEx": "Neumann", "year": 2011}, {"title": "PGQ: Combining policy gradient and Q-learning", "author": ["B. O\u2019Donoghue", "R. Munos", "K. Kavukcuoglu", "V. Mnih"], "venue": "arXiv preprint arXiv:1611.01626,", "citeRegEx": "O.Donoghue et al\\.,? \\Q2016\\E", "shortCiteRegEx": "O.Donoghue et al\\.", "year": 2016}, {"title": "Free-energybased reinforcement learning in a partially observable environment", "author": ["M. Otsuka", "J. Yoshimoto", "K. Doya"], "venue": "In ESANN,", "citeRegEx": "Otsuka et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Otsuka et al\\.", "year": 2010}, {"title": "Relative entropy policy search", "author": ["J. Peters", "K. M\u00fclling", "Y. Altun"], "venue": "In AAAI Conf. on Artificial Intelligence,", "citeRegEx": "Peters et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Peters et al\\.", "year": 2010}, {"title": "On stochastic optimal control and reinforcement learning by approximate inference", "author": ["K. Rawlik", "M. Toussaint", "S. Vijayakumar"], "venue": "Proceedings of Robotics: Science and Systems VIII,", "citeRegEx": "Rawlik et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Rawlik et al\\.", "year": 2012}, {"title": "Reinforcement learning with factored states and actions", "author": ["B. Sallans", "G.E. Hinton"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Sallans and Hinton,? \\Q2004\\E", "shortCiteRegEx": "Sallans and Hinton", "year": 2004}, {"title": "Trust region policy optimization", "author": ["J. Schulman", "S. Levine", "P. Abbeel", "M.I. Jordan", "P. Moritz"], "venue": "In Int. Conf on Machine Learning,", "citeRegEx": "Schulman et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Schulman et al\\.", "year": 2015}, {"title": "High-dimensional continuous control using generalized advantage estimation", "author": ["Schulman", "John", "Moritz", "Philipp", "Levine", "Sergey", "Jordan", "Michael", "Abbeel", "Pieter"], "venue": "arXiv preprint arXiv:1506.02438,", "citeRegEx": "Schulman et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Schulman et al\\.", "year": 2015}, {"title": "Loss is its own reward: Self-supervision for reinforcement learning", "author": ["E. Shelhamer", "P. Mahmoudieh", "M. Argus", "T. Darrell"], "venue": "arXiv preprint arXiv:1612.07307,", "citeRegEx": "Shelhamer et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Shelhamer et al\\.", "year": 2016}, {"title": "Deterministic policy gradient algorithms", "author": ["D. Silver", "G. Lever", "N. Heess", "T. Degris", "D. Wierstra", "M. Riedmiller"], "venue": "In Int. Conf on Machine Learning,", "citeRegEx": "Silver et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Silver et al\\.", "year": 2014}, {"title": "Mastering the game of go with deep neural networks and tree", "author": ["D. sabis"], "venue": "search. Nature,", "citeRegEx": "sabis,? \\Q2016\\E", "shortCiteRegEx": "sabis", "year": 2016}, {"title": "Reinforcement learning: An introduction, volume 1", "author": ["R.S. Sutton", "A.G. Barto"], "venue": "MIT press Cambridge,", "citeRegEx": "Sutton and Barto,? \\Q1998\\E", "shortCiteRegEx": "Sutton and Barto", "year": 1998}, {"title": "Bias in natural actor-critic algorithms", "author": ["P. Thomas"], "venue": "In Int. Conf. on Machine Learning,", "citeRegEx": "Thomas,? \\Q2014\\E", "shortCiteRegEx": "Thomas", "year": 2014}, {"title": "Linearly-solvable Markov decision problems", "author": ["E. Todorov"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Todorov,? \\Q2007\\E", "shortCiteRegEx": "Todorov", "year": 2007}, {"title": "General duality between optimal control and estimation", "author": ["E. Todorov"], "venue": "In IEEE Conf. on Decision and Control,", "citeRegEx": "Todorov,? \\Q2008\\E", "shortCiteRegEx": "Todorov", "year": 2008}, {"title": "Compositionality of optimal control laws", "author": ["E. Todorov"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Todorov,? \\Q2009\\E", "shortCiteRegEx": "Todorov", "year": 2009}, {"title": "Robot trajectory optimization using approximate inference", "author": ["M. Toussaint"], "venue": "In Int. Conf. on Machine Learning,", "citeRegEx": "Toussaint,? \\Q2009\\E", "shortCiteRegEx": "Toussaint", "year": 2009}, {"title": "On the theory of the brownian motion", "author": ["Uhlenbeck", "George E", "Ornstein", "Leonard S"], "venue": "Physical review,", "citeRegEx": "Uhlenbeck et al\\.,? \\Q1930\\E", "shortCiteRegEx": "Uhlenbeck et al\\.", "year": 1930}, {"title": "Learning to draw samples: With application to amortized mle for generative adversarial learning", "author": ["D. Wang", "Q. Liu"], "venue": "arXiv preprint arXiv:1611.01722,", "citeRegEx": "Wang and Liu,? \\Q2016\\E", "shortCiteRegEx": "Wang and Liu", "year": 2016}, {"title": "Simple statistical gradient-following algorithms for connectionist reinforcement learning", "author": ["Williams", "Ronald J"], "venue": "Machine learning,", "citeRegEx": "Williams and J.,? \\Q1992\\E", "shortCiteRegEx": "Williams and J.", "year": 1992}, {"title": "Energybased generative adversarial network", "author": ["J. Zhao", "M. Mathieu", "Y. LeCun"], "venue": "arXiv preprint arXiv:1609.03126,", "citeRegEx": "Zhao et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Zhao et al\\.", "year": 2016}, {"title": "Modeling purposeful adaptive behavior with the principle of maximum causal entropy", "author": ["B.D. Ziebart"], "venue": "PhD thesis,", "citeRegEx": "Ziebart,? \\Q2010\\E", "shortCiteRegEx": "Ziebart", "year": 2010}, {"title": "Maximum entropy inverse reinforcement learning", "author": ["B.D. Ziebart", "A.L. Maas", "J.A. Bagnell", "A.K. Dey"], "venue": "In AAAI Conference on Artificial Intelligence,", "citeRegEx": "Ziebart et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Ziebart et al\\.", "year": 2008}, {"title": "This objective corresponds to maximizing the discounted expected reward and entropy for future states originating from every state-action tuple (st,at) weighted by its probability \u03c1\u03c0 under the current policy. Note that this objective still takes into account the entropy of the policy at future states, in contrast to greedy objectives", "author": ["O\u2019Donoghue"], "venue": null, "citeRegEx": "O.Donoghue,? \\Q2016\\E", "shortCiteRegEx": "O.Donoghue", "year": 2016}], "referenceMentions": [{"referenceID": 22, "context": "Deep reinforcement learning (deep RL) has emerged as a promising direction for autonomous acquisition of complex behaviors (Mnih et al., 2015; Silver et al., 2016), due to its ability to process complex sensory input (Jaderberg et al.", "startOffset": 123, "endOffset": 163}, {"referenceID": 10, "context": ", 2016), due to its ability to process complex sensory input (Jaderberg et al., 2016) and to acquire elaborate behavior skills using general-purpose neural network representations (Levine et al.", "startOffset": 61, "endOffset": 85}, {"referenceID": 18, "context": ", 2016) and to acquire elaborate behavior skills using general-purpose neural network representations (Levine et al., 2016).", "startOffset": 102, "endOffset": 123}, {"referenceID": 19, "context": "Deep reinforcement learning methods can be used to optimize deterministic (Lillicrap et al., 2015) and stochastic (Schulman et al.", "startOffset": 74, "endOffset": 98}, {"referenceID": 23, "context": ", 2015) and stochastic (Schulman et al., 2015a; Mnih et al., 2016) policies.", "startOffset": 23, "endOffset": 66}, {"referenceID": 33, "context": "Although stochastic policies are desirable for exploration, this exploration is typically attained heuristically, for example by injecting noise (Silver et al., 2014; Lillicrap et al., 2015; Mnih et al., 2015) or initializing a stochastic policy with", "startOffset": 145, "endOffset": 209}, {"referenceID": 19, "context": "Although stochastic policies are desirable for exploration, this exploration is typically attained heuristically, for example by injecting noise (Silver et al., 2014; Lillicrap et al., 2015; Mnih et al., 2015) or initializing a stochastic policy with", "startOffset": 145, "endOffset": 209}, {"referenceID": 22, "context": "Although stochastic policies are desirable for exploration, this exploration is typically attained heuristically, for example by injecting noise (Silver et al., 2014; Lillicrap et al., 2015; Mnih et al., 2015) or initializing a stochastic policy with", "startOffset": 145, "endOffset": 209}, {"referenceID": 12, "context": "high entropy (Kakade, 2002; Schulman et al., 2015a; Mnih et al., 2016).", "startOffset": 13, "endOffset": 70}, {"referenceID": 23, "context": "high entropy (Kakade, 2002; Schulman et al., 2015a; Mnih et al., 2016).", "startOffset": 13, "endOffset": 70}, {"referenceID": 45, "context": "Other benefits include robustness in the face of uncertain dynamics (Ziebart, 2010), imitation learning (Ziebart et al.", "startOffset": 68, "endOffset": 83}, {"referenceID": 46, "context": "Other benefits include robustness in the face of uncertain dynamics (Ziebart, 2010), imitation learning (Ziebart et al., 2008), and improved convergence and computational properties (Gu et al.", "startOffset": 104, "endOffset": 126}, {"referenceID": 38, "context": "As discussed in prior work, a stochastic policy emerges as the optimal answer when we consider the connection between optimal control and probabilistic inference (Todorov, 2008).", "startOffset": 162, "endOffset": 177}, {"referenceID": 40, "context": "The solution can be shown to optimize an entropy-augmented reinforcement learning objective or to correspond to the solution to a maximum entropy learning problem (Toussaint, 2009).", "startOffset": 163, "endOffset": 180}, {"referenceID": 37, "context": "A number of methods have been proposed, including Z-learning (Todorov, 2007), maximum entropy inverse RL (Ziebart et al.", "startOffset": 61, "endOffset": 76}, {"referenceID": 46, "context": "A number of methods have been proposed, including Z-learning (Todorov, 2007), maximum entropy inverse RL (Ziebart et al., 2008), approximate inference using message passing (Toussaint, 2009), \u03a8-learning (Rawlik et al.", "startOffset": 105, "endOffset": 127}, {"referenceID": 40, "context": ", 2008), approximate inference using message passing (Toussaint, 2009), \u03a8-learning (Rawlik et al.", "startOffset": 53, "endOffset": 70}, {"referenceID": 28, "context": ", 2008), approximate inference using message passing (Toussaint, 2009), \u03a8-learning (Rawlik et al., 2012), and G-learning (Fox et al.", "startOffset": 83, "endOffset": 104}, {"referenceID": 3, "context": ", 2012), and G-learning (Fox et al., 2016), as well as more recent proposals in deep RL such as PGQ (O\u2019Donoghue et al.", "startOffset": 24, "endOffset": 42}, {"referenceID": 25, "context": ", 2016), as well as more recent proposals in deep RL such as PGQ (O\u2019Donoghue et al., 2016), but these generally operate either on simple tabular representations, which are difficult to apply to continuous or high-dimensional domains, or employ a simple parametric representation of the policy distribution, such as a conditional Gaussian.", "startOffset": 65, "endOffset": 90}, {"referenceID": 25, "context": "Therefore, although the policy is optimized to perform the desired skill in many different ways and, in the case of deep RL methods (O\u2019Donoghue et al., 2016), might even use an expressive high-capacity model for representing the mean action, the resulting distribution is typically very limited in terms of its representational power, even if the parameters of that distribution are represented by an expressive function approximator, such as a neural network.", "startOffset": 132, "endOffset": 157}, {"referenceID": 33, "context": "We explore this connection further in the paper, and in the course of this discuss connections to popular deep RL methods such as deterministic policy gradient (DPG) (Silver et al., 2014; Lillicrap et al., 2015), normalized advantage functions (NAF) (Gu et al.", "startOffset": 166, "endOffset": 211}, {"referenceID": 19, "context": "We explore this connection further in the paper, and in the course of this discuss connections to popular deep RL methods such as deterministic policy gradient (DPG) (Silver et al., 2014; Lillicrap et al., 2015), normalized advantage functions (NAF) (Gu et al.", "startOffset": 166, "endOffset": 211}, {"referenceID": 25, "context": ", 2016b), and PGQ (O\u2019Donoghue et al., 2016).", "startOffset": 18, "endOffset": 43}, {"referenceID": 25, "context": "Note that this objective differs qualitatively from the behavior of Boltzmann exploration (Sallans & Hinton, 2004) and PGQ (O\u2019Donoghue et al., 2016), which greedily maximize entropy at the current time step, but do not explicitly optimize for policies that aim to reach states where they will have high entropy in the future.", "startOffset": 123, "endOffset": 148}, {"referenceID": 46, "context": "This distinction is crucial, since the maximum entropy objective can be shown to maximize the entropy of the entire trajectory distribution for the policy \u03c0, while the greedy Boltzmann exploration approach does not (Ziebart et al., 2008; Levine & Abbeel, 2014).", "startOffset": 215, "endOffset": 260}, {"referenceID": 36, "context": "In the context of policy search algorithms, the use of a discount factor is actually a somewhat nuanced choice, and writing down the precise objective that is optimized when using the discount factor is non-trivial (Thomas, 2014).", "startOffset": 215, "endOffset": 229}, {"referenceID": 25, "context": "The choices in prior work include discrete multinomial distributions (O\u2019Donoghue et al., 2016) and Gaussian distributions (Rawlik et al.", "startOffset": 69, "endOffset": 94}, {"referenceID": 28, "context": ", 2016) and Gaussian distributions (Rawlik et al., 2012).", "startOffset": 35, "endOffset": 56}, {"referenceID": 45, "context": "1 as well as (Ziebart, 2010).", "startOffset": 13, "endOffset": 28}, {"referenceID": 45, "context": "2, as well as (Ziebart, 2010).", "startOffset": 14, "endOffset": 29}, {"referenceID": 44, "context": "Existing approaches that sample from energy-based distributions generally fall into two categories: methods that use Markov chain Monte Carlo (MCMC) based sampling (Sallans & Hinton, 2004), and methods that learn a stochastic sampling network trained to output approximate samples from the target distribution (Zhao et al., 2016; Kim & Bengio, 2016).", "startOffset": 310, "endOffset": 349}, {"referenceID": 21, "context": "The experience is stored in a replay memory buffer D as standard in deep Q-learning (Mnih et al., 2013), and the parameters are updated using random minibatches from this memory.", "startOffset": 84, "endOffset": 103}, {"referenceID": 21, "context": "The soft Q-function updates use a delayed version of the target values, characterized by a smoothing factor \u03c4 , to improve stability (Mnih et al., 2013).", "startOffset": 133, "endOffset": 152}, {"referenceID": 38, "context": "policy (Todorov, 2008), which has been exploited to construct practical path planning methods based on iterative linearization and probabilistic inference techniques (Toussaint, 2009).", "startOffset": 7, "endOffset": 22}, {"referenceID": 40, "context": "policy (Todorov, 2008), which has been exploited to construct practical path planning methods based on iterative linearization and probabilistic inference techniques (Toussaint, 2009).", "startOffset": 166, "endOffset": 183}, {"referenceID": 37, "context": "This has been explored in the context of linearly solvable MDPs (Todorov, 2007) and, in the case of inverse reinforcement learning, MaxEnt IRL (Ziebart et al.", "startOffset": 64, "endOffset": 79}, {"referenceID": 46, "context": "This has been explored in the context of linearly solvable MDPs (Todorov, 2007) and, in the case of inverse reinforcement learning, MaxEnt IRL (Ziebart et al., 2008).", "startOffset": 143, "endOffset": 165}, {"referenceID": 13, "context": "In continuous systems and continuous time, path integral control studies maximum entropy policies and maximum entropy planning (Kappen, 2005).", "startOffset": 127, "endOffset": 141}, {"referenceID": 27, "context": "A number of related methods have also used maximum entropy policy optimization as an intermediate step for optimizing policies under a standard expected reward objective (Peters et al., 2010; Neumann, 2011; Rawlik et al., 2012; Fox et al., 2016).", "startOffset": 170, "endOffset": 245}, {"referenceID": 24, "context": "A number of related methods have also used maximum entropy policy optimization as an intermediate step for optimizing policies under a standard expected reward objective (Peters et al., 2010; Neumann, 2011; Rawlik et al., 2012; Fox et al., 2016).", "startOffset": 170, "endOffset": 245}, {"referenceID": 28, "context": "A number of related methods have also used maximum entropy policy optimization as an intermediate step for optimizing policies under a standard expected reward objective (Peters et al., 2010; Neumann, 2011; Rawlik et al., 2012; Fox et al., 2016).", "startOffset": 170, "endOffset": 245}, {"referenceID": 3, "context": "A number of related methods have also used maximum entropy policy optimization as an intermediate step for optimizing policies under a standard expected reward objective (Peters et al., 2010; Neumann, 2011; Rawlik et al., 2012; Fox et al., 2016).", "startOffset": 170, "endOffset": 245}, {"referenceID": 3, "context": ", 2012; Fox et al., 2016). Among these, the work of Rawlik et al. (2012) re-", "startOffset": 8, "endOffset": 73}, {"referenceID": 2, "context": "The form of our sampler resembles the stochastic networks proposed in recent work on hierarchical learning (Florensa et al., 2017).", "startOffset": 107, "endOffset": 130}, {"referenceID": 11, "context": "A closely related concept to maximum entropy policies is Boltzmann exploration, which uses the exponential of the standard Q-function as the probability of an action (Kaelbling et al., 1996).", "startOffset": 166, "endOffset": 190}, {"referenceID": 1, "context": "A number of prior works have also explored representing policies as energy-based models, with the Q-value obtained from an energy model such as a restricted Boltzmann machine (RBM) (Sallans & Hinton, 2004; Elfwing et al., 2010; Otsuka et al., 2010; Heess et al., 2012).", "startOffset": 181, "endOffset": 268}, {"referenceID": 26, "context": "A number of prior works have also explored representing policies as energy-based models, with the Q-value obtained from an energy model such as a restricted Boltzmann machine (RBM) (Sallans & Hinton, 2004; Elfwing et al., 2010; Otsuka et al., 2010; Heess et al., 2012).", "startOffset": 181, "endOffset": 268}, {"referenceID": 8, "context": "A number of prior works have also explored representing policies as energy-based models, with the Q-value obtained from an energy model such as a restricted Boltzmann machine (RBM) (Sallans & Hinton, 2004; Elfwing et al., 2010; Otsuka et al., 2010; Heess et al., 2012).", "startOffset": 181, "endOffset": 268}, {"referenceID": 1, "context": "A number of prior works have also explored representing policies as energy-based models, with the Q-value obtained from an energy model such as a restricted Boltzmann machine (RBM) (Sallans & Hinton, 2004; Elfwing et al., 2010; Otsuka et al., 2010; Heess et al., 2012). Although these methods are closely related, they have not, to our knowledge, been extended to the case of deep network models, have not made extensive use of approximate inference techniques, and have not been demonstrated on the complex continuous tasks. More recently, O\u2019Donoghue et al. (2016) drew a connection between Boltzmann exploration and entropy-regularized policy gradient, though in a theoretical framework that differs from maximum entropy policy search: unlike the full maximum entropy framework, the approach of O\u2019Donoghue et al.", "startOffset": 206, "endOffset": 566}, {"referenceID": 1, "context": "A number of prior works have also explored representing policies as energy-based models, with the Q-value obtained from an energy model such as a restricted Boltzmann machine (RBM) (Sallans & Hinton, 2004; Elfwing et al., 2010; Otsuka et al., 2010; Heess et al., 2012). Although these methods are closely related, they have not, to our knowledge, been extended to the case of deep network models, have not made extensive use of approximate inference techniques, and have not been demonstrated on the complex continuous tasks. More recently, O\u2019Donoghue et al. (2016) drew a connection between Boltzmann exploration and entropy-regularized policy gradient, though in a theoretical framework that differs from maximum entropy policy search: unlike the full maximum entropy framework, the approach of O\u2019Donoghue et al. (2016) only optimizes for maximizing entropy at the current time step, rather than planning for visiting future states where entropy will be further maximized.", "startOffset": 206, "endOffset": 822}, {"referenceID": 19, "context": "It is particularly instructive to observe the connection between our approach and the deep deterministic policy gradient method (DDPG) (Lillicrap et al., 2015), which updates a Qfunction critic according to (hard) Bellman updates, and then backpropagates the Q-value gradient into the actor, similarly to NFQCA (Hafner & Riedmiller, 2011).", "startOffset": 135, "endOffset": 159}, {"referenceID": 19, "context": "Our experiments aim to answer the following questions: (1) Does our soft Q-learning method accurately capture a multi-modal policy distribution? (2) Can soft Q-learning with energy-based policies aid exploration for complex tasks that require tracking multiple modes? (3) Can a maximum entropy policy serve as a good initialization for finetuning on different tasks, when compared to pretraining with a standard deterministic objective? We compare our algorithm to DDPG (Lillicrap et al., 2015), which has been shown to achieve better sample efficiency on the continuous control problems that we consider than other recent techniques such as REINFORCE (Williams, 1992), TRPO (Schulman et al.", "startOffset": 470, "endOffset": 494}, {"referenceID": 23, "context": ", 2015a), and A3C (Mnih et al., 2016).", "startOffset": 18, "endOffset": 37}, {"referenceID": 30, "context": "3D robot (adapted from Schulman et al. (2015b)) needs to find a path through a maze to a target position (see Figure 2).", "startOffset": 23, "endOffset": 47}, {"referenceID": 4, "context": "A standard way to accelerate deep neural network training is task-specific initialization (Goodfellow et al., 2016), where a network trained for one task is used as initialization for another task.", "startOffset": 90, "endOffset": 115}, {"referenceID": 32, "context": "Pretraining has also been explored in the context of RL (Shelhamer et al., 2016).", "startOffset": 56, "endOffset": 80}, {"referenceID": 9, "context": "This pretraining is similar in some ways to recent work on modulated controllers (Heess et al., 2016) and hierarchical models (Florensa et al.", "startOffset": 81, "endOffset": 101}, {"referenceID": 2, "context": ", 2016) and hierarchical models (Florensa et al., 2017).", "startOffset": 32, "endOffset": 55}, {"referenceID": 39, "context": "In the context of linearly solvable MDPs, several prior works have shown that policies trained for different tasks can be composed to create new optimal policies (Da Silva et al., 2009; Todorov, 2009).", "startOffset": 162, "endOffset": 200}], "year": 2017, "abstractText": "We propose a method for learning expressive energy-based policies for continuous states and actions, which has been feasible only in tabular domains before. We apply our method to learning maximum entropy policies, resulting into a new algorithm, called soft Q-learning, that expresses the optimal policy via a Boltzmann distribution. We use the recently proposed amortized Stein variational gradient descent to learn a stochastic sampling network that approximates samples from this distribution. The benefits of the proposed algorithm include improved exploration and compositionality that allows transferring skills between tasks, which we confirm in simulated experiments with swimming and walking robots. We also draw a connection to actorcritic methods, which can be viewed performing approximate inference on the corresponding energy-based model.", "creator": "LaTeX with hyperref package"}}}