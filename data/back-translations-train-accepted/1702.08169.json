{"id": "1702.08169", "review": {"conference": "ICML", "VERSION": "v1", "DATE_OF_SUBMISSION": "27-Feb-2017", "title": "Communication-efficient Algorithms for Distributed Stochastic Principal Component Analysis", "abstract": "We study the fundamental problem of Principal Component Analysis in a statistical distributed setting in which each machine out of $m$ stores a sample of $n$ points sampled i.i.d. from a single unknown distribution. We study algorithms for estimating the leading principal component of the population covariance matrix that are both communication-efficient and achieve estimation error of the order of the centralized ERM solution that uses all $mn$ samples. On the negative side, we show that in contrast to results obtained for distributed estimation under convexity assumptions, for the PCA objective, simply averaging the local ERM solutions cannot guarantee error that is consistent with the centralized ERM. We show that this unfortunate phenomena can be remedied by performing a simple correction step which correlates between the individual solutions, and provides an estimator that is consistent with the centralized ERM for sufficiently-large $n$. We also introduce an iterative distributed algorithm that is applicable in any regime of $n$, which is based on distributed matrix-vector products. The algorithm gives significant acceleration in terms of communication rounds over previous distributed algorithms, in a wide regime of parameters.", "histories": [["v1", "Mon, 27 Feb 2017 07:45:58 GMT  (36kb)", "http://arxiv.org/abs/1702.08169v1", null]], "reviews": [], "SUBJECTS": "cs.LG", "authors": ["dan garber", "ohad shamir", "nathan srebro"], "accepted": true, "id": "1702.08169"}, "pdf": {"name": "1702.08169.pdf", "metadata": {"source": "CRF", "title": "Communication-efficient Algorithms for Distributed Stochastic Principal Component Analysis", "authors": ["Dan Garber"], "emails": ["dangar@technion.ac.il", "ohad.shamir@weizmann.ac.il", "nati@ttic.edu"], "sections": [{"heading": null, "text": "ar Xiv: 170 2.08 169v 1 [cs.L G] 27 Feb 20"}, {"heading": "1 Introduction", "text": "This year, it has reached the point where it will be able to retaliate until it is able to retaliate."}, {"heading": "2 Preliminaries", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "2.1 Notation and problem setting", "text": "We write vectors in Rd in bold lowercase letters (e.g. v), matrices in bold uppercase letters (e.g. X), and scalars are written as light-faced letters (e.g. c). We have the standard euclidean standard for vectors and the spectral standard for matrices marked. We consider an environment in which m machines, numbered 1... m, each receive a dataset of n samples, which are i.i.d. extracted from D. We have v1 a leading eigenvector of the population covariance X = Ex property D [xx's], whose property i.i."}, {"heading": "2.1.1 The centralized solution", "text": "Our primary measure of performance will be the centralized empirical risk minimizer, which is the leading eigenvector of the aggregated empirical covariance matrix X. Suppose that \"p\" (0, 1) and \"v\" (1) represent the leading eigenvector of the centralized ERM. (3) \"Lemma\" 1 is a direct consequence of the following standard concentration arguments for random matrices (1) and Davis-Kahan sin (1). (3) \"X\" (X) is a direct consequence of the following standard concentration arguments for random matrices (1) and Davis-Kahan sin (2). \"X\" X \"(X) and\" X \"(X)."}, {"heading": "2.2 Informal statement of main results and previous algorithms", "text": "We now informally describe our main results, followed by a detailed description of previous approaches that are directly applicable to our setting. Algorithmic results (both new and old) are summarized in Table 1."}, {"heading": "2.2.1 Main results", "text": "We show that a natural approach of simply averaging the individual leading eigenvectors of the empirical covariance matrices X-i (and normalizing the preservation of a unit vector) cannot significantly improve the performance of the individual eigenvectors (beyond logarithmic factors). Specifically, if we designate v-i-1 the leading eigenvector of X-i for any i-m factors and we designate their average as v-1 = 1 m-m i = 1 v-i-1, then there is a distribution D over vectors with size O-1 and the covariant display gap g-1-1, so that we say: ED [1 \u2212 (v-1 v1-v1-v-1-v-1] -2] = 1 n, see theorem 3 in section 3 for the complete and formal reasoning."}, {"heading": "A successful single communication round algorithm via correlation of individual", "text": "ERM solutions We show that if, before averaging the local ERM solutions, as indicated above, we correlate their statements (ERM factors) with the results (ERM factors) by adapting them to each individual machine (say machine number 1), i.e., we leave v-p = 1 m + 1 character (v-v) 1 v-v (1) 1) v-v (i) 1, then this guarantees that for each individual machine (0, 1), w-p at least 1 \u2212 p, 1 \u2212 p, 1 \u2212 (v-v) v-v, 1-v-v, 1-v-v, 2 (dm-p) 2 (dm-p) 2 (dm-p) 2 (dm-p)."}, {"heading": "2.2.2 Previous algorithms", "text": "Distributed versions of classic iterative algorithms: Classically fast iterative algorithms for calculating the leading eigenvector of a positive semidefinitive matrix (such as the well-known Power Method and Lanczos algorithm) also require iterative multiplications of the input matrix (in our case X) with the current estimate. Therefore, it is easy to implement these algorithms in our distributed environment by multiplying the same vector by the covariance matrices on each machine and calculating the result. Thus, by known convergence guarantees of these two methods, we will have a unit vector w generated for any p matrix on each machine (0, 1), 1 \u2212 (w v 1), 2 \u2212 p, after which convergence guarantees of these two methods will guide the final communication on the ship matrix."}, {"heading": "3 Single Communication Round Algorithms via ERM on Each", "text": "MachineIn this section we look at distributed algorithms that require only a single round of communication. Of course, all algorithms in this regime are based on the aggregation of the ERM solutions of the individual machines, i.e. each machine sends only the leading eigenvector of its empirical covariance matrix X-i to a centralized machine (without loss of universality, machine 1), which in turn combines it in some way into a single unit vector."}, {"heading": "3.1 Simple averaging of eigenvectors fail", "text": "Perhaps the simplest method of aggregating the individual eigenvectors of each machine is to intersect them and then normalize them to obtain a unit vector. For example, in the distributed statistical environment considered in [26], where the target is strongly convex, it has been shown that a simple averaging of the individual ERM solutions in a meaningful parameter regime results in an estimation error of the order of magnitude of the centralized ERM solution. However, we show here that for the PCA, where the target is certainly not convex, this approach fails in virtually any mode, in the sense that the error of the returned aggregated solution cannot be better than that of the one returned by a single machine. Theorem 3. There is a distribution over vectors in R2 with a boundary limited by a universal constant, where the eigengap in the covariance matrix is 1 (i.e. vice versa = 1), so that if each machine returns i an estimated value v (1), it is traceable."}, {"heading": "3.2 Averaging with Sign Fixing", "text": "As can be seen from the statement of Theorem 3, an important assumption is that each machine produces an unbiased estimate, in the sense that the character of the result is uniform and independent of the other machines. This suggests that the correlation of the characters of the various estimates can bypass the lower boundary result in Theorem 3. It turns out that this is actually the case, as captured by the following theorem: Theorem 4: Let's get the leading eigenvector of X-i for every possible i-gap (m], and consider the unit vectorw = 1 character (w)."}, {"heading": "3.3 Lower Bound for Sign Fixing", "text": "We now show that the result of Theorem 4 is closely linked to poly-logarithmic factors and generally cannot be improved: Theorem 5. For any \u043c (0, 1) and d > 1 there is a distribution via vectors in Rd (of at most one universal constant) with eigengap \u03b4 in the covariance matrix, so that for any number of machines m and for any n sample size per machine each n is sufficiently larger than 1 / 2 the aggregated vector v-1 = 1 m \u00b2 m \u00b2 m i = 1 v \u00b2 (i) 1 (even after specifying the number of characters with the basic eigenvector v1) satisfy E [1 \u2212 v-1], e1 2] = 1 (1\u04412mn + 1\u03b44n2) The proof is supplied in the appendix."}, {"heading": "4 A Multi-round Algorithm based on Shift-and-Invert Itera-", "text": "In addition to improving some poly-logarithmic factors in the estimation error, the main motivation is to achieve a result that does not require the machine-specific sample size n to grow with the number of machines m, as in Theorem 4.To this end, we will consider the use of the shift-and-invert meta algorithm originally described in [6, 7] to explicitly solve the central ERM target, i.e. to find a unit vector that is an approximate solution for maxv. In this section, we will mention the leading eigenvalue or eigengap of X. Furthermore, without loss of universality, we assume b = 1 (i.e., all data points are in the unit of the Euclidean sphere), since our approach is to determine population risk by approximating the empirical risk."}, {"heading": "4.1 The Shift-and-Invert meta-algorithm", "text": "The shift-and-invert algorithm [6, 7] efficiently reduces the problem of calculating the leading eigenvector of a positive semidefinitive matrix X (1) to the approximate solution of a polylogarithmic number of linear systems, i.e., finding approximate minimizers of the convex square optimization problems of the formal z-Rd {1, w (z): = 1 z \u00b2 (2) z \u2212 z \u00b2 w (12), where it is a displacement parameter. Essentially, the algorithm is based on applying potentiality iterations to a shifted and inverted matrix (2). \u2212 X \u2212 1, where the displacement parameters are carefully selected, and the algorithm implementing this reduction is described below (see algorithm 1)."}, {"heading": "4.2 Faster Distributed Approximation of Linear Systems via Local Preconditioning", "text": "Now consider that if y is the optimal solution to problem (13), i.e., y = C1 / 2M \u2212 1C1 / 2C \u2212 1 / 2W = C1 / 2M \u2212 1w, then z: = C \u2212 1 / 2y. The idea behind choosing C in this way is very intuitive. Ideally, we could have chosen C = M, so the number of conditions F = 0 / 2Y \u2212 p would be equal to the conditionality."}, {"heading": "4.2.1 Solving the pre-conditioned linear systems", "text": "We are now discussing the application of gradient-based algorithms to determine an approximate minimisation of the pre-conditioned problem, problem (13), in our distributed environment. \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212"}, {"heading": "4.3 Putting it all together", "text": "We now give our main result for this section, which is a simple consequence of the previous lemmings. Full proof is given in the Annex.Theorem 6. fix-and-fix algorithm (0, 1) and p-inverted algorithm 1, with parameters p / 3, and application of the algorithm in lem 7 with the parameter \u00b5 = 4 \u221a ln (3d / p) / n. In order to solve linear systems approximately, there is a probability of at least 1 \u2212 p of a unity vector admitting that (w-v-1) 2 \u2212 0, after execution at mostO \u221a ln (d / p) n [dp\u0430) lpn (Shift Vector), which admits that (w-v-1) 2 \u2212 0, after execution at mostO \u221a ln (d / p) n [dpn) lpn (Shift Vector)"}, {"heading": "5 Experiments", "text": "For both distributions we used the covariance matrix X = UsuccessU with an accidental error in the orthonormal matrix, i.e., we generated the diagrams 1, 1, 2, 3, 4, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 6, 6, 7, 6, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7"}, {"heading": "A Proofs Omitted from Section 3", "text": "The proof of theorem 3proof. Let us consider the following distribution over R2.x = V = V = V = V = V = V = V = V = V (V = V = V = V = V = V = V = V = V = V = V = V V = V V = V V = V = V - V = V = V - V = V = V = V = V = V - V = V = V = V = V - V = V = V = V - V = V = V - V = V (V = V = V = V - V = V = V = V = V = V V = V V = V = V V = V V = V = V - V = V = V = V = V - V = V = V = V = V = V - V = V = V = V = V = V = V = V - V = V = V = V = V = V = V = V = V = V = V - V = V = V = V = V = V = V = V = V - V = V = V = V = V = V = V = V = V = V - V = V = V = V = V = V = V = V - V = V = V = V = V = V = V - V = V = V = V = V = V - V = V = V = V = V = V = V - V = V = V = V = V = V = V - V = V = V = V = V = V - V = V = V = V = V = V - V = V = V = V = V = V = V - V = V = V = V = V - V = V = V = V = V = V = V = V = V - V = V = V = V = V = V - V = V = V = V = V = V = V = V - V = V = V = V = V = V = V = V - V = V = V = V = V = V - V = V = V = V = V = V = V = V - V = V = V = V = V = V = V = V = V = V = V - V = V = V = V = V = V = V = V = V = V = V = V = V = V"}, {"heading": "B Proofs Omitted from Section 4", "text": "It applies that (w'v1) 2 \u2212 v1 \u2212 v1 (w'v1) 2 \u2212 v1 (w'v1) 2 \u2212 v1 (w'v1) 2 \u2212 v1 (w'v1) 2 \u2212 v1 (w'v1) 2) (w'v1) 2 \u2212 v1 (w'v1) 2 \u2212 v1 (w'v1) 2 \u2212 v2 (w'v1) 2 \u2212 v2 (w'v1) 2 \u2212 v2 (w'v1) 2 \u2212 v2 (v'v1).B.2 Proof of Lemma 6Proof. Note that C = M + (X'v1) + v1) + v2 \u2212 v1)."}, {"heading": "C Proof of the Davis-Kahan sin\u03b8 Theorem", "text": "We prove Theorem 2 to be more universally valid."}], "references": [{"title": "Even faster SVD decomposition yet without agonizing pain", "author": ["Zeyuan Allen Zhu", "Yuanzhi Li"], "venue": "In Advances in Neural Information Processing Systems 29: Annual Conference on Neural Information Processing Systems", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2016}, {"title": "Fast global convergence of online", "author": ["Zeyuan Allen Zhu", "Yuanzhi Li"], "venue": "PCA. CoRR,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2016}, {"title": "The fast convergence of incremental PCA", "author": ["Akshay Balsubramani", "Sanjoy Dasgupta", "Yoav Freund"], "venue": "In Advances in Neural Information Processing Systems", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2013}, {"title": "Optimal principal component analysis in distributed and streaming models", "author": ["Christos Boutsidis", "David P Woodruff", "Peilin Zhong"], "venue": "In Proceedings of the 48th Annual ACM SIGACT Symposium on Theory of Computing,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2016}, {"title": "Fast and simple pca via convex optimization", "author": ["Dan Garber", "Elad Hazan"], "venue": "arXiv preprint arXiv:1509.05647,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2015}, {"title": "Faster eigenvector computation via shift-and-invert", "author": ["Dan Garber", "Elad Hazan", "Chi Jin", "Sham M. Kakade", "Cameron Musco", "Praneeth Netrapalli", "Aaron Sidford"], "venue": "preconditioning. CoRR,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2016}, {"title": "The differentiation of pseudo-inverses and nonlinear least squares problems whose variables separate", "author": ["Gene H Golub", "Victor Pereyra"], "venue": "SIAM Journal on numerical analysis,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 1973}, {"title": "Analysis of a complex of statistical variables into principal components", "author": ["H. Hotelling"], "venue": "J. Educ. Psych., 24", "citeRegEx": "9", "shortCiteRegEx": null, "year": 1933}, {"title": "Communication-efficient distributed dual coordinate ascent", "author": ["Martin Jaggi", "Virginia Smith", "Martin Tak\u00e1c", "Jonathan Terhorst", "Sanjay Krishnan", "Thomas Hofmann", "Michael I Jordan"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2014}, {"title": "Matching matrix bernstein with little memory: Near-optimal finite sample guarantees for oja\u2019s algorithm", "author": ["Prateek Jain", "Chi Jin", "Sham M Kakade", "Praneeth Netrapalli", "Aaron Sidford"], "venue": "arXiv preprint arXiv:1602.06929,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2016}, {"title": "Matching matrix bernstein with little memory: Near-optimal finite sample guarantees for oja\u2019s algorithm", "author": ["Prateek Jain", "Chi Jin", "Sham M Kakade", "Praneeth Netrapalli", "Aaron Sidford"], "venue": "arXiv preprint arXiv:1602.06929,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2016}, {"title": "Principal component analysis", "author": ["IT Jolliffe"], "venue": "2002. Spring-verlag, New York", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2002}, {"title": "Distributed stochastic variance reduced gradient methods", "author": ["Jason D. Lee", "Tengyu Ma", "Qihang Lin"], "venue": "CoRR, abs/1507.07595,", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2015}, {"title": "Improved distributed principal component analysis", "author": ["Yingyu Liang", "Maria-Florina F Balcan", "Vandana Kanchanapally", "David Woodruff"], "venue": "In NIPS,", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2014}, {"title": "On differentiating eigenvalues and eigenvectors", "author": ["Jan R Magnus"], "venue": "Econometric Theory,", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 1985}, {"title": "On lines and planes of closest fit to systems of points in space", "author": ["K. Pearson"], "venue": "Philosophical Magazine, 2(6):559\u2013572", "citeRegEx": "17", "shortCiteRegEx": null, "year": 1901}, {"title": "AIDE: fast and communication efficient distributed optimization", "author": ["Sashank J. Reddi", "Jakub Konecn\u00fd", "Peter Richt\u00e1rik", "Barnab\u00e1s P\u00f3czos", "Alexander J. Smola"], "venue": null, "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2016}, {"title": "A stochastic PCA and SVD algorithm with an exponential convergence rate", "author": ["Ohad Shamir"], "venue": "In Proceedings of the 32nd International Conference on Machine Learning, ICML 2015,", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2015}, {"title": "Convergence of stochastic gradient descent for PCA", "author": ["Ohad Shamir"], "venue": "In Proceedings of the 33nd International Conference on Machine Learning,", "citeRegEx": "20", "shortCiteRegEx": "20", "year": 2016}, {"title": "Fast stochastic algorithms for svd and pca: Convergence properties and convexity", "author": ["Ohad Shamir"], "venue": "In Proceedings of The 33rd International Conference on Machine Learning,", "citeRegEx": "21", "shortCiteRegEx": "21", "year": 2016}, {"title": "Without-replacement sampling for stochastic gradient methods", "author": ["Ohad Shamir"], "venue": "In Advances in Neural Information Processing Systems 29: Annual Conference on Neural Information Processing Systems", "citeRegEx": "22", "shortCiteRegEx": "22", "year": 2016}, {"title": "Communication-efficient distributed optimization using an approximate newton-type method", "author": ["Ohad Shamir", "Nathan Srebro", "Tong Zhang"], "venue": "In Proceedings of the 31th International Conference on Machine Learning,", "citeRegEx": "23", "shortCiteRegEx": "23", "year": 2014}, {"title": "User-friendly tail bounds for sums of random matrices", "author": ["Joel A. Tropp"], "venue": "Foundations of Computational Mathematics,", "citeRegEx": "24", "shortCiteRegEx": "24", "year": 2012}, {"title": "Samworth. A useful variant of the davis\u2013kahan theorem for statisticians", "author": ["Yi Yu", "Tengyao Wang", "Richard J"], "venue": null, "citeRegEx": "25", "shortCiteRegEx": "25", "year": 2015}, {"title": "Communication-efficient algorithms for statistical optimization", "author": ["Yuchen Zhang", "John C Duchi", "Martin J Wainwright"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "26", "shortCiteRegEx": "26", "year": 2013}], "referenceMentions": [{"referenceID": 15, "context": "1 Introduction Principal Component Analysis (PCA) [17, 9, 13] is one of the most celebrated and popular techniques in data analysis and machine learning.", "startOffset": 50, "endOffset": 61}, {"referenceID": 7, "context": "1 Introduction Principal Component Analysis (PCA) [17, 9, 13] is one of the most celebrated and popular techniques in data analysis and machine learning.", "startOffset": 50, "endOffset": 61}, {"referenceID": 11, "context": "1 Introduction Principal Component Analysis (PCA) [17, 9, 13] is one of the most celebrated and popular techniques in data analysis and machine learning.", "startOffset": 50, "endOffset": 61}, {"referenceID": 2, "context": "This for instance was the case in a recent wave of results that applied concepts such as stochastic gradient updates [4, 20, 11, 3] and variance reduction [19, 21, 6, 7, 2] to the PCA problem.", "startOffset": 117, "endOffset": 131}, {"referenceID": 18, "context": "This for instance was the case in a recent wave of results that applied concepts such as stochastic gradient updates [4, 20, 11, 3] and variance reduction [19, 21, 6, 7, 2] to the PCA problem.", "startOffset": 117, "endOffset": 131}, {"referenceID": 9, "context": "This for instance was the case in a recent wave of results that applied concepts such as stochastic gradient updates [4, 20, 11, 3] and variance reduction [19, 21, 6, 7, 2] to the PCA problem.", "startOffset": 117, "endOffset": 131}, {"referenceID": 1, "context": "This for instance was the case in a recent wave of results that applied concepts such as stochastic gradient updates [4, 20, 11, 3] and variance reduction [19, 21, 6, 7, 2] to the PCA problem.", "startOffset": 117, "endOffset": 131}, {"referenceID": 17, "context": "This for instance was the case in a recent wave of results that applied concepts such as stochastic gradient updates [4, 20, 11, 3] and variance reduction [19, 21, 6, 7, 2] to the PCA problem.", "startOffset": 155, "endOffset": 172}, {"referenceID": 19, "context": "This for instance was the case in a recent wave of results that applied concepts such as stochastic gradient updates [4, 20, 11, 3] and variance reduction [19, 21, 6, 7, 2] to the PCA problem.", "startOffset": 155, "endOffset": 172}, {"referenceID": 4, "context": "This for instance was the case in a recent wave of results that applied concepts such as stochastic gradient updates [4, 20, 11, 3] and variance reduction [19, 21, 6, 7, 2] to the PCA problem.", "startOffset": 155, "endOffset": 172}, {"referenceID": 5, "context": "This for instance was the case in a recent wave of results that applied concepts such as stochastic gradient updates [4, 20, 11, 3] and variance reduction [19, 21, 6, 7, 2] to the PCA problem.", "startOffset": 155, "endOffset": 172}, {"referenceID": 0, "context": "This for instance was the case in a recent wave of results that applied concepts such as stochastic gradient updates [4, 20, 11, 3] and variance reduction [19, 21, 6, 7, 2] to the PCA problem.", "startOffset": 155, "endOffset": 172}, {"referenceID": 24, "context": "For instance, [26] proposed communication-efficient algorithms for a distributed statistical estimation settings, similar to ours, but under convexity assumptions.", "startOffset": 14, "endOffset": 18}, {"referenceID": 24, "context": "Much like the results of [26], this result only holds in the regime when the per-machine sample size n is sufficiently large.", "startOffset": 25, "endOffset": 29}, {"referenceID": 21, "context": "A second line of results for distributed estimation under convexity assumptions consider iterative algorithms that perform multiple communication rounds and are based on distributed gradient computations (some examples include [23, 27, 14, 22, 10, 18]).", "startOffset": 227, "endOffset": 251}, {"referenceID": 12, "context": "A second line of results for distributed estimation under convexity assumptions consider iterative algorithms that perform multiple communication rounds and are based on distributed gradient computations (some examples include [23, 27, 14, 22, 10, 18]).", "startOffset": 227, "endOffset": 251}, {"referenceID": 20, "context": "A second line of results for distributed estimation under convexity assumptions consider iterative algorithms that perform multiple communication rounds and are based on distributed gradient computations (some examples include [23, 27, 14, 22, 10, 18]).", "startOffset": 227, "endOffset": 251}, {"referenceID": 8, "context": "A second line of results for distributed estimation under convexity assumptions consider iterative algorithms that perform multiple communication rounds and are based on distributed gradient computations (some examples include [23, 27, 14, 22, 10, 18]).", "startOffset": 227, "endOffset": 251}, {"referenceID": 16, "context": "A second line of results for distributed estimation under convexity assumptions consider iterative algorithms that perform multiple communication rounds and are based on distributed gradient computations (some examples include [23, 27, 14, 22, 10, 18]).", "startOffset": 227, "endOffset": 251}, {"referenceID": 4, "context": "Towards designing efficient distributed iterative methods for our PCA setting, we consider the application of the recently proposed method of Shift-and-Invert power iterations (S&I) for PCA [6, 7].", "startOffset": 190, "endOffset": 196}, {"referenceID": 5, "context": "Towards designing efficient distributed iterative methods for our PCA setting, we consider the application of the recently proposed method of Shift-and-Invert power iterations (S&I) for PCA [6, 7].", "startOffset": 190, "endOffset": 196}, {"referenceID": 13, "context": "Beyond the results described so far, [15, 5] studied distributed algorithms for PCA in a deterministic setting in which the partition of the data across machines is arbitrary and communication is measured in terms of number of transmitted bits.", "startOffset": 37, "endOffset": 44}, {"referenceID": 3, "context": "Beyond the results described so far, [15, 5] studied distributed algorithms for PCA in a deterministic setting in which the partition of the data across machines is arbitrary and communication is measured in terms of number of transmitted bits.", "startOffset": 37, "endOffset": 44}, {"referenceID": 22, "context": "(3) Lemma 1 is a direct consequence of the following standard concentration argument for random matrices, and the Davis-Kahan sin(\u03b8) theorem (whose proof is given in the appendix for completeness): Theorem 1 (Matrix Hoeffding, see [24]).", "startOffset": 231, "endOffset": 235}, {"referenceID": 4, "context": "A multi communication round algorithm We present a distributed algorithm based on the Shift-and-Invert framework for leading eigenvector computation [6, 7] which is applied to explicitly solving the centralized ERM problem.", "startOffset": 149, "endOffset": 155}, {"referenceID": 5, "context": "A multi communication round algorithm We present a distributed algorithm based on the Shift-and-Invert framework for leading eigenvector computation [6, 7] which is applied to explicitly solving the centralized ERM problem.", "startOffset": 149, "endOffset": 155}, {"referenceID": 2, "context": "SGD for PCA was studied in several results in recent years [4, 20, 21, 12, 3].", "startOffset": 59, "endOffset": 77}, {"referenceID": 18, "context": "SGD for PCA was studied in several results in recent years [4, 20, 21, 12, 3].", "startOffset": 59, "endOffset": 77}, {"referenceID": 19, "context": "SGD for PCA was studied in several results in recent years [4, 20, 21, 12, 3].", "startOffset": 59, "endOffset": 77}, {"referenceID": 10, "context": "SGD for PCA was studied in several results in recent years [4, 20, 21, 12, 3].", "startOffset": 59, "endOffset": 77}, {"referenceID": 1, "context": "SGD for PCA was studied in several results in recent years [4, 20, 21, 12, 3].", "startOffset": 59, "endOffset": 77}, {"referenceID": 10, "context": "For instance applying the result of [12] in this way will result in a final estimate w satisfying 1\u2212 (wv1) = O ( b2 ln d \u03b42mn )", "startOffset": 36, "endOffset": 40}, {"referenceID": 24, "context": "For instance, in the distributed statistical setting considered in [26], in which the objective is strongly convex, it was shown that simply averaging the individual ERM solutions leads, in a meaningful regime of parameters, to estimation error of the order of the centralized ERM solution.", "startOffset": 67, "endOffset": 71}, {"referenceID": 14, "context": "By Theorem 1 in [16], we have that both \u03bb(t) and v(t) are infinitely differentiable at any t \u2208 [0, 1], and satisfy3 \u03bb\u2032(t) = v(t)\u22a4Ev(t) , v\u2032(t) = (\u03bb(t)I \u2212A(t))\u2020Ev(t) .", "startOffset": 16, "endOffset": 20}, {"referenceID": 6, "context": "3 in [8]) \u2212B\u2020 ( \u2202 \u2202t B )", "startOffset": 5, "endOffset": 8}, {"referenceID": 14, "context": "Since v(t),v(t),v(t) are all vectors, this is a direct consequence of the standard Taylor expansion of the scalar function t 7\u2192 v(t)j , mapping t to the j-th coordinate of v(t), using the fact that this mapping is differentiable to any order (see Theorem 1 in [16], and in particular twice continuously differentiable.", "startOffset": 260, "endOffset": 264}, {"referenceID": 23, "context": "To handle the second part, note that by a variant of the Davis-Kahan sin\u03b8 theorem (see Corollary 1 in [25]), if maxi \u2016X\u0302i \u2212 X\u2016 \u2264 \u03b4/12, then the leading eigenvectors v\u0302i 1 of X\u0302i (after choosing the sign appropriately, i.", "startOffset": 102, "endOffset": 106}, {"referenceID": 4, "context": "Towards this end we consider the use of the Shift-and-Invert meta-algorithm, originally described in [6, 7], to explicitly solve the centralized ERM objective, i.", "startOffset": 101, "endOffset": 107}, {"referenceID": 5, "context": "Towards this end we consider the use of the Shift-and-Invert meta-algorithm, originally described in [6, 7], to explicitly solve the centralized ERM objective, i.", "startOffset": 101, "endOffset": 107}, {"referenceID": 4, "context": "1 The Shift-and-Invert meta-algorithm The Shift-and-Invert algorithm [6, 7] efficiently reduces the problem of computing the leading eigenvector of a positive semidefinite matrix X\u0302 to that of approximately-solving a polylogarithmic number of linear systems, i.", "startOffset": 69, "endOffset": 75}, {"referenceID": 5, "context": "1 The Shift-and-Invert meta-algorithm The Shift-and-Invert algorithm [6, 7] efficiently reduces the problem of computing the leading eigenvector of a positive semidefinite matrix X\u0302 to that of approximately-solving a polylogarithmic number of linear systems, i.", "startOffset": 69, "endOffset": 75}, {"referenceID": 4, "context": "The algorithm that implements this reduction, originally described in [6], is given below (see Algorithm 1).", "startOffset": 70, "endOffset": 73}, {"referenceID": 4, "context": "2 in [6]).", "startOffset": 5, "endOffset": 8}], "year": 2017, "abstractText": "We study the fundamental problem of Principal Component Analysis in a statistical distributed setting in which each machine out of m stores a sample of n points sampled i.i.d. from a single unknown distribution. We study algorithms for estimating the leading principal component of the population covariance matrix that are both communication-efficient and achieve estimation error of the order of the centralized ERM solution that uses all mn samples. On the negative side, we show that in contrast to results obtained for distributed estimation under convexity assumptions, for the PCA objective, simply averaging the local ERM solutions cannot guarantee error that is consistent with the centralized ERM. We show that this unfortunate phenomena can be remedied by performing a simple correction step which correlates between the individual solutions, and provides an estimator that is consistent with the centralized ERM for sufficiently-large n. We also introduce an iterative distributed algorithm that is applicable in any regime of n, which is based on distributed matrix-vector products. The algorithm gives significant acceleration in terms of communication rounds over previous distributed algorithms, in a wide regime of parameters.", "creator": "dvips(k) 5.996 Copyright 2016 Radical Eye Software"}}}