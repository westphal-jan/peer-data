{"id": "1702.08359", "review": {"conference": "ICML", "VERSION": "v1", "DATE_OF_SUBMISSION": "27-Feb-2017", "title": "Dynamic Word Embeddings", "abstract": "We present a probabilistic language model for time-stamped text data which tracks the semantic evolution of individual words over time. The model represents words and contexts by latent trajectories in an embedding space. At each moment in time, the embedding vectors are inferred from a probabilistic version of word2vec [Mikolov, 2013]. These embedding vectors are connected in time through a latent diffusion process. We describe two scalable variational inference algorithms---skip-gram smoothing and skip-gram filtering---that allow us to train the model jointly over all times; thus learning on all data while simultaneously allowing word and context vectors to drift. Experimental results on three different corpora demonstrate that our dynamic model infers word embedding trajectories that are more interpretable and lead to higher predictive likelihoods than competing methods that are based on static models trained separately on time slices.", "histories": [["v1", "Mon, 27 Feb 2017 16:31:48 GMT  (1421kb,D)", "http://arxiv.org/abs/1702.08359v1", "10 pages + supplement"], ["v2", "Mon, 17 Jul 2017 23:45:06 GMT  (1390kb,D)", "http://arxiv.org/abs/1702.08359v2", "In the proceedings of the International Conference on Machine Learning (ICML 2017); 8 pages + references and supplement"]], "COMMENTS": "10 pages + supplement", "reviews": [], "SUBJECTS": "stat.ML cs.LG", "authors": ["robert bamler", "stephan mandt"], "accepted": true, "id": "1702.08359"}, "pdf": {"name": "1702.08359.pdf", "metadata": {"source": "CRF", "title": "Dynamic Word Embeddings via Skip-Gram Filtering", "authors": ["Robert Bamler", "Stephan Mandt"], "emails": ["ROBERT.BAMLER@DISNEYRESEARCH.COM", "STEPHAN.MANDT@DISNEYRESEARCH.COM"], "sections": [{"heading": "1. Introduction", "text": "This year, it is only a matter of time before an agreement is reached."}, {"heading": "2. Related Work", "text": "In fact, most of them are able to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to rush, to rush, to rush, to rush, to rush, to rush, to rush, to rush, to rage, to rage, to rage, to rage, to rage, to rage, to rage, to rage, to rage, to rage, to rage, to rage, to rage, to rage, to rage, to rage, to rage."}, {"heading": "3. Model", "text": "We propose the dynamic Skip-gram model, a generalization of the Skip-gram model (word2vec) (Mikolov et al., 2013b) to sequential text data. The model finds text embedding vectors that drift continuously over time, allowing changes in language and word usage to be tracked over short and long periods of time. Dynamic Skip-gram is a probabilistic model that combines a Bayesian version of the Skip-gram model (Barkan, 2016) with a latent time series. It is jointly trained and scaled to massive data using approximate Bayesian conclusions. The observed data consists of word sequences from a finite vocabulary of size L. In Section 3.1, all sequences (sets of books, articles, or tweets) are considered to be time independent; in Section 3.2, they are associated with different time stamps. The goal is to maximize the probability of any word that occurs in its context, with the words given below being multicolored in the context."}, {"heading": "3.1. Bayesian Skip-Gram Model", "text": "The Bayesian Skip-gram model (Barkan, 2016) is a probable version of word2vec = > chasian examples of word2vec (Mikolov et al., 2013b) and forms the basis of our approach. The graphic model is shown in Figure 2a. For each word pair i, j in the vocabulary, the model assigns probabilities that word i appears in the context of the word j. This probability is p (u > i vj) with the sigmoid function \u03c3 (x) = 1 / (1 + e \u2212 x). Let the model be a variable indicating a drawing from this probability distribution, i.e. p (zij = 1) = a > i vj). The generative model assumes that many word-word pairs (i, j) are extracted from the vocabulary and tested to be a word-context pair."}, {"heading": "3.2. Dynamic Skip-Gram Model", "text": "The key extension of our approach is to use a Kalman filter as a precursor for the time evolution of the latent embeddings over time (Welch & Bishop, 1995), which allows us to exchange information over time while allowing the embeddings to drift away at the same time. We are looking at a corpus of T documents that become timestamps \u03c41 <. < \u03c4T. For each time step t- {1,.., T} the sufficient statistics of the word-context pairs are in the L \u00b7 L matrices n + t, n \u2212 t of the positive and negative matrix counts n + ij, t and n \u2212 ij, t, respectively. Denote Ut = (u1, \u00b7 uL, t) \u00b7 L matrices n, n \u2212 L of the matrix of the word embeddings at time t, n \u2212 t of the positive and negative matrix counts with matrix elements n + j, ij, ij, it \u2212 it, ij, it \u2212 ij, and \u2212 ij respectively."}, {"heading": "4. Inference", "text": "We will discuss two scalable approximate inference algorithms for our model, taking into account different versions of the inference: filtering, which uses only information from the past, as required in streaming applications, and smoothing, which learns better embedding, but requires complete knowledge of the order of the documents. In Bayesian inference, we start by formulating a common distribution (Eq.7) of observations n \u00b1 and parameters U and V, and we are interested in the downstream distribution of parameters that depend on observations, p (U, V | n \u00b1) = p (n \u00b1, U, V)."}, {"heading": "4.1. Skip-Gram Filtering", "text": "In many applications such as streaming, the data arrives sequentially, so we can condition our model only on the basis of past observations and not on the basis of future observations; we will first describe the conclusion in such a (Kalman) filter system (Kalman et \u2212 \u2212 al., 1960; Welch & Bishop, 1995); in the filter scenario, we will iteratively update the variation distribution from each time the results are available, using a variation distribution that is factored over all times, q (U, V) = 1 q (Ut, Vt) and we will update the variation factor t at a given time based on the evidence t and the approximate background of the previous time step; in addition, we will use a fully factored distribution at each time: q (Ut, Vt) = L-dynamic distribution; and we will update the variation factor at a given time."}, {"heading": "4.2. Skip-Gram Smoothing", "text": "In contrast to the filter models, where conclusions about past observations up to a given time t, (Kalman) we smooth out conclusions about the entire sequence of observations n \u00b1 1: T. This approach results in smoother trajectories and typically higher probabilities than with filters, because evidence from future and past observations is used. In addition to the new derivative scheme, we also use a different variational distribution. As the model is applied to all time steps, we are no longer limited to a variational distribution that is factored in time. For simplicity, we will focus here on the variational distribution of word embeddings U; context embeddings V are treated identically. We use a factored distribution over both time steps embedded in space and vocabulary space, q (U1: T) = L = 1d (uik, 1: T). (14) In the time domain, our variational approximation is structured."}, {"heading": "5. Experiments", "text": "We show that our algorithms find smoother embedding paths than methods based on a static model, which allows us to track semantic changes in individual words by following the closest adjacent relationships over time. In our quantitative analysis, we find higher predictive probabilities on held random data compared to our baseline data. We report the results of our proposed algorithms from Section 4 and compare them with baselines from Section 2: \u2022 SGI refers to the non-Bayesian selection results as independent random initializations of word vectors (Mikolov et al, 2013b). We used our own implementation of the model by comparing Kalman filtering before and dot estimation of embedding vectors near times."}, {"heading": "6. Conclusions", "text": "We introduced the dynamic Skip-gram model: a Bayesian probability model that combines word2vec with a latent continuous time series. We demonstrated experimentally that both Skip-gram filtering (which only takes into account previous observations) and the smoothing of the Skip-gram model (which uses all data) result in a smooth change in embedding vectors that are better able to predict word-context statistics in scheduled time windows. The most drastic benefits are when the data is small in individual time steps, making it difficult to fit into a static word-embedding model. Our approach can be used as data mining and as a tool to detect abnormalities in the streaming of text on social media, as well as a tool for historians and social scientists interested in the evolution of language."}, {"heading": "1. Dimensionality Reduction in Figure 1", "text": "In order to generate the word clouds in Figure 1 of the main text, we mapped the adjusted word embedding of Rd to the two-dimensional level using dynamic t-SNE (Rauber et al., 2016). Dynamic t-SNE is a non-parametric dimensional algorithm for reducing sequential data. In addition, the algorithm finds a projection into a lower dimension by solving a non-convex optimization problem that aims to obtain close neighboring relationships in each individual time step. Furthermore, projections in adjacent time steps are reconciled by a square penalty with the prefactor \u03bb 0 for sudden movements. There is a goal conflict between finding good local predictions for each individual time step (\u03bb \u2192 0) and finding smooth predictions (large \u03bb).Since we create the smoothness of word embedding with prefactor \u03bb 0 for the embedding of paths, we want to pre-analyze each of these distortion options as we do not want to have 00x distortion in each case."}, {"heading": "2. Hyperparemeters and Construction of n\u00b11:T", "text": "For the Google books, we used the same context window size cmax and embedding dimension d as in (Kim et al., 2014). We reduced d for the SoU and Twitter corpora to avoid overadjustment to these much smaller data. Contrary to the Word2vec values, we construct our positive and negative number matrices n \u00b1 ij, t deterministically in a pre-processing step. As detailed as possible, it resembles the stochastic approach in word2vec (Mikolov et al., 2013). In each update step, word2vec stochastically samples of a context window size uniformly at an interval [1, \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7, cmax] so that the context size fluctuates and appears more frequently near words than words that are far apart."}, {"heading": "3. Skip-gram Filtering Algorithm", "text": "The filter algorithm of the skip program is described in section 4 of the main text. We provide a formulation in pseudo code in algorithm 1. Algorithm 2 Skip gram smoothing (batch version); see section 4. Note: As in algorithm 1, we concentrate on a single word vector u, and we drop indexes i and k; we also consider only a single sample (S = 1), and we describe the basic algorithm without minibatch scanning. Input: Number of time steps T, timestamp \u03c41: T, positive and negative examples n \u00b1 1: T, hyperparameters. Find upper bidiagonal matrixB0, which is the cholesky decomposition of the previous precision matrix. S11. Initialize the variation parameters \u04221: T: S1: S1 S1: S1: S1 S1: S1: S1: S1 S1: S1: S0. Initialize the variation parameter of B0."}, {"heading": "4. Skip-gram Smoothing Algorithm", "text": "In this section we will give details of the skip gram smoothing algorithm, see section 4 of the main text. A summary is provided in Algorithm 2.Variation Distribution. For now we are focusing on the word embedding, and we simplify the notation by dropping the indexes for the vocabulary and the embedding of dimensions. (S4) Here is the variation distribution for a single embedding dimension of a single word embedding a single word. (u1: T) = N (B > u Bu) \u2212 1). (S4) Here is \u00b5u, 1: T the vector of the averages, and Bu is the vector of the cholesky decomposition of the precision matrix. We restrict the latter to be bidiagonal, Bu = vector u, 1, 2."}], "references": [{"title": "Mirror Descent and Nonlinear Projected Subgradient Methods for Convex Optimization", "author": ["References Beck", "Amir", "Teboulle", "Marc"], "venue": "Operations Research Letters,", "citeRegEx": "Beck et al\\.,? \\Q2003\\E", "shortCiteRegEx": "Beck et al\\.", "year": 2003}, {"title": "The Ordered Subsets Mirror Descent Optimization Method with Applications to Tomography", "author": ["Ben-Tal", "Aharon", "Margalit", "Tamar", "Nemirovski", "Arkadi"], "venue": "SIAM Journal on Optimization,", "citeRegEx": "Ben.Tal et al\\.,? \\Q2001\\E", "shortCiteRegEx": "Ben.Tal et al\\.", "year": 2001}, {"title": "Dynamic Topic Models", "author": ["Blei", "David M", "Lafferty", "John D"], "venue": "In Proceedings of the 23rd international conference on Machine learning,", "citeRegEx": "Blei et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Blei et al\\.", "year": 2006}, {"title": "Stochastic Variational Inference", "author": ["Hoffman", "Matthew D", "Blei", "David M", "Wang", "Chong", "Paisley", "John William"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Hoffman et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Hoffman et al\\.", "year": 2013}, {"title": "Temporal Analysis of Language Through Neural Language Models", "author": ["Kim", "Yoon", "Chiu", "Yi-I", "Hanaki", "Kentaro", "Hegde", "Darshan", "Petrov", "Slav"], "venue": "arXiv preprint arXiv:1405.3515,", "citeRegEx": "Kim et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Kim et al\\.", "year": 2014}, {"title": "Adam: A Method for Stochastic Optimization", "author": ["Kingma", "Diederik", "Ba", "Jimmy"], "venue": "In International Conference on Learning Representations,", "citeRegEx": "Kingma et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Kingma et al\\.", "year": 2015}, {"title": "Auto-Encoding Variational Bayes", "author": ["Kingma", "Diederik P", "Welling", "Max"], "venue": "In International Conference on Learning Representations,", "citeRegEx": "Kingma et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Kingma et al\\.", "year": 2014}, {"title": "Distributed Representations of Words and Phrases and their Compositionality", "author": ["Mikolov", "Tomas", "Sutskever", "Ilya", "Chen", "Kai", "Corrado", "Greg S", "Dean", "Jeff"], "venue": "In Advances in Neural Information Processing Systems", "citeRegEx": "Mikolov et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Mikolov et al\\.", "year": 2013}, {"title": "Visualizing Time-Dependent Data Using Dynamic t-SNE", "author": ["Rauber", "Paulo E", "Falc\u00e3o", "Alexandre X", "Telea", "Alexandru C"], "venue": "In EuroVis 2016 - Short Papers,", "citeRegEx": "Rauber et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Rauber et al\\.", "year": 2016}, {"title": "Stochastic Backpropagation and Approximate Inference in Deep Generative Models", "author": ["Rezende", "Danilo Jimenez", "Mohamed", "Shakir", "Wierstra", "Daan"], "venue": "In Proceedings of the 23rd international conference on Machine learning,", "citeRegEx": "Rezende et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Rezende et al\\.", "year": 2014}], "referenceMentions": [{"referenceID": 4, "context": "Current approaches to learning word embeddings in a dynamic context rely on grouping the data into time bins and training the embeddings separately on these bins (Kim et al., 2014; Kulkarni et al., 2015; Hamilton et al., 2016).", "startOffset": 162, "endOffset": 226}, {"referenceID": 8, "context": "We used dynamic t-SNE (Rauber et al., 2016) for dimensionality reduction.", "startOffset": 22, "endOffset": 43}, {"referenceID": 9, "context": "\u2022 We propose two scalable black-box variational inference algorithms (Ranganath et al., 2014; Rezende et al., 2014) for filtering and smoothing.", "startOffset": 69, "endOffset": 115}, {"referenceID": 4, "context": "Several authors have analyzed different statistics of text data to analyze semantic changes of words over time (Mihalcea & Nastase, 2012; Sagi et al., 2011; Kim et al., 2014; Kulkarni et al., 2015; Hamilton et al., 2016).", "startOffset": 111, "endOffset": 220}, {"referenceID": 4, "context": "Kim et al. (2014) fit word2vec separately on different time bins, where the word vectors obtained for the previous bin are used to initialize the algorithm for the next time bin.", "startOffset": 0, "endOffset": 18}, {"referenceID": 4, "context": "Kim et al. (2014) fit word2vec separately on different time bins, where the word vectors obtained for the previous bin are used to initialize the algorithm for the next time bin. The bins have to be sufficiently large and the found trajectories are not as smooth as ours, as we demonstrate in this paper. Hamilton et al. (2016) also trained word2vec separately on several large corpora from different decades.", "startOffset": 0, "endOffset": 328}, {"referenceID": 7, "context": "This is exactly the objective of the (non-Bayesian) skipgram model, see (Mikolov et al., 2013b). The count matrices n and n\u2212 are either pre-computed for the entire corpus, or estimated based on stochastic subsamples from the data in a sequential way, as done by word2vec. Barkan (2016) give an approximate Bayesian treatment of the model with Gaussian priors on the embeddings.", "startOffset": 73, "endOffset": 286}, {"referenceID": 3, "context": "For a restricted class of models, the ELBO can be computed in closed-form (Hoffman et al., 2013).", "startOffset": 74, "endOffset": 96}, {"referenceID": 9, "context": "Our model is non-conjugate and requires instead black-box VI using the reparameterization trick, where one maximizes L(\u03bb) with stochastic gradient ascent and estimates the gradient \u2207\u03bbL by sampling from the variational distribution (Rezende et al., 2014; Kingma & Welling, 2014).", "startOffset": 231, "endOffset": 277}, {"referenceID": 3, "context": "As the computational complexity of an update step scales as \u0398(L), we first pretrain the model by drawing minibatches of L\u2032 < L random words and L\u2032 random contexts from the vocabulary (Hoffman et al., 2013).", "startOffset": 183, "endOffset": 205}, {"referenceID": 1, "context": "We use mirror ascent (Ben-Tal et al., 2001; Beck & Teboulle, 2003) to enforce positive definiteness of B.", "startOffset": 21, "endOffset": 66}, {"referenceID": 7, "context": "\u2022 SGI denotes the non-Bayesian skip-gram model with independent random initializations of word vectors (Mikolov et al., 2013b). We used our own implementation of the model by dropping the Kalman filtering prior and point-estimating embedding vectors. Word vectors at nearby times are made comparable by approximate orthogonal transformations, which corresponds to Hamilton et al. (2016).", "startOffset": 104, "endOffset": 387}, {"referenceID": 4, "context": "\u2022 SGP denotes the same approach as above, but with word and context vectors being pre-initialized with the values from the previous year, as in Kim et al. (2014).", "startOffset": 144, "endOffset": 162}, {"referenceID": 4, "context": "In contrast, in SGP (middle) (Kim et al., 2014) and SGI (bottom) (Hamilton et al.", "startOffset": 29, "endOffset": 47}, {"referenceID": 4, "context": "For the Google books corpus, we chose the embedding dimension d = 200, which was also used in Kim et al. (2014). We set d = 100 for SoU and Twitter, as d = 200 resulted in overfitting on these much smaller corpora.", "startOffset": 94, "endOffset": 112}, {"referenceID": 8, "context": "We mapped the normalized embedding vectors to two dimensions using dynamic t-SNE (Rauber et al., 2016) (see supplement for details).", "startOffset": 81, "endOffset": 102}, {"referenceID": 4, "context": ", 2016) and SGP (Kim et al., 2014) on three different corpora (high values are better).", "startOffset": 16, "endOffset": 34}], "year": 2017, "abstractText": "We present a probabilistic language model for time-stamped text data which tracks the semantic evolution of individual words over time. The model represents words and contexts by latent trajectories in an embedding space. At each moment in time, the embedding vectors are inferred from a probabilistic version of word2vec (Mikolov et al., 2013b). These embedding vectors are connected in time through a latent diffusion process. We describe two scalable variational inference algorithms\u2014skipgram smoothing and skip-gram filtering\u2014that allow us to train the model jointly over all times; thus learning on all data while simultaneously allowing word and context vectors to drift. Experimental results on three different corpora demonstrate that our dynamic model infers word embedding trajectories that are more interpretable and lead to higher predictive likelihoods than competing methods that are based on static models trained separately on time slices.", "creator": "LaTeX with hyperref package"}}}