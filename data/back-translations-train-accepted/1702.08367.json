{"id": "1702.08367", "review": {"conference": "nips", "VERSION": "v1", "DATE_OF_SUBMISSION": "27-Feb-2017", "title": "Differentiable Learning of Logical Rules for Knowledge Base Reasoning", "abstract": "Learned models composed of probabilistic logical rules are useful for many tasks, such as knowledge base completion. Unfortunately this learning problem is difficult, since determining the structure of the theory normally requires solving a discrete optimization problem. In this paper, we propose an alternative approach: a completely differentiable model for learning sets of first-order rules. The approach is inspired by a recently-developed differentiable logic, i.e. a subset of first-order logic for which inference tasks can be compiled into sequences of differentiable operations. Here we describe a neural controller system which learns how to sequentially compose the these primitive differentiable operations to solve reasoning tasks, and in particular, to perform knowledge base completion. The long-term goal of this work is to develop integrated, end-to-end systems that can learn to perform high-level logical reasoning as well as lower-level perceptual tasks.", "histories": [["v1", "Mon, 27 Feb 2017 16:44:38 GMT  (156kb,D)", "http://arxiv.org/abs/1702.08367v1", null], ["v2", "Sun, 4 Jun 2017 04:17:58 GMT  (419kb,D)", "http://arxiv.org/abs/1702.08367v2", null]], "reviews": [], "SUBJECTS": "cs.AI", "authors": ["fan yang", "zhilin yang", "william w cohen"], "accepted": true, "id": "1702.08367"}, "pdf": {"name": "1702.08367.pdf", "metadata": {"source": "META", "title": "Differentiable Learning of Logical Rules for Knowledge Base Completion", "authors": ["Fan Yang", "Zhilin Yang", "William W. Cohen"], "emails": ["<fanyang1@cs.cmu.edu>,", "<zhiliny@cs.cmu.edu>,", "<wcohen@cs.cmu.edu>."], "sections": [{"heading": "1. Introduction", "text": "In fact, it is so that most of them are able to survive themselves, and that they are able to survive themselves, \"he said in an interview with the\" New York Times, \"in which he said the role of the\" New York Times, \"the\" New York Times, \"the\" New York Times, \"the\" New York Times, \"the\" New York Times, \"the\" New York Times, \"the\" New York Times, \"the\" New York Times, \"the\" New York Times, \"the\" New York Times, \"the\" New York Times, \"the\" New York Times, \"the\" New York Times, \"the\" New York Times, \"the\" New York Times, \"the\" New York Times, \"the\" the \"New York Times,\" the \"the\" New York Times, \"the\" the \"the\" New York Times, \"the\" the \"New York Times,\" the \"the\" New York Times, \"the\" the \"the\" New York Times, \"the\" the \"the\" New York Times, \"the\" the \"the\" New York Times, \"the\" the \"the\" the \"New York Times,\" the \"the\" the \"the\" New York Times, \"the\" the \"the\" the \"New York Times,\" the \"the\" the \"the\" the \"New York Times,\" the \"the\" the \"the\" the \"New York Times,\" the \"the\" the \"the\" the \"New York Times,\" the \"the\" the \"the\" the \"New York Times,\" the \"the\" the \"the\" the \"New York Times,\" the \"the\" the \"the\" New York Times, \"the\" the \"the\" the \"the\" New York Times, \"the\" the \"the\" the \"New York Times,\" the \"the\" the \"the\" the \"New York Times,\" the \"the\" the \"the\" the \"New York Times,\" the \"New York Times,\" the \"the\" the \"the\" New York Times, the \"the\" the \"the\" the \"the\" the \"New York Times,\" the \"New York Times,\" the \"the\" the \"the\" the \"the\" New York Times, the \"the\" the \"the\" the \"the\" New York Times, the \"the\" the \"the\" the \""}, {"heading": "2. Knowledge Base Completion", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "2.1. Definitions", "text": "Knowledge Databases (KB) are compilations of facts about real-world entities (such as people, places and things).The facts in the Knowledge Database are usually stored in RDF format, i.e. each fact is three times the format (head, relationship, tail) or equivalent a relationship (head, tail).Such a large amount of structured data has promising applications in many problems, such as information gathering, understanding natural languages and biological data mining. Knowledge databases such as Freebase (Bollacker et al., 2008) usually contain incomplete facts about entities that are not in the database. Therefore, an important task in Knowledge Database is to automatically discover the missing facts, i.e. the completion of the Knowledge Database is independent. In some scenarios, the missing facts can be deduced by arguing from the existing facts."}, {"heading": "2.2. Logic rules for KB completion", "text": "We are interested in learning chain logic rules in the following form: RA1 (y1, x), RAn (z, yn) = \u21d2 RC (z, x) (1), where n \u2264 N, and N is any fixed integer. RA1,.., RAn and RC are relations in the knowledge base, and y1,.., yn, x, z are units. These rules can be used to complete the knowledge base, because if we want to derive the missing fact RC (z, x), this can be reduced to checking whether there is y1,..., so that RA1 (y1, x),..., RAn (z, yn) are all facts in the knowledge base. Given the complexity of modern knowledge bases and the fact that they often contain noisy information, it is unlikely to find rules that are consistent with all data."}, {"heading": "3. Differentiable Rule Learning", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "3.1. TensorLog representation of KB and logic rule inference", "text": "To make rule learning differentiable, we first introduce a matrix representation of the knowledge base and logical rule conclusion, as used in TensorLog (Cohen, 2016). In this representation, entities are numerically encoded. We represent each entity e with a single hot vector ve (0, 1) N, where N is the number of entities, and only the e-th component is 1. For each relationship C in the KB, we use a MatrixMC (0, 1) N \u00b7 N to store all facts about this relationship. Then, the input is MCi, j is 1, if and only if RC (i, j) is a fact in the knowledge base. Using this matrix representation, we can imitate the rule conclusion RA (y, x) that we RB (z, y) = \u21d2 RC (z, x) for all y of computinguz, B, A, vTzM Avx."}, {"heading": "3.2. Learning the logic rules", "text": "We will now discuss the rule learning process, including learnable parameters and model architecture. As shown in Eq.8, the maximum length of the query is RC (\u00b7, x), we must learn the rule sets that imply it and the trust forces associated with these rules. As these parameters are actually dependent on the query reference, we expand the notation with raising indexes to make them explicitly. The learnable parameters of our model are {\u03b1Cl, \u03b2Cl}, for each relationship C.However, it is difficult to formulate a differentiated process to learn the parameters directly {\u03b1Cl} (Lao & Cohen, 2010). This is because each parameter is associated with a particular rule, and renumbering rules is a inherently discrete task. To overcome this difficulty, we observe that an almost equivalent way is to write Eq.8 to change the summation and the product, the following formula results in different parameters."}, {"heading": "4. Experiments", "text": "To investigate the effectiveness of our differentiated regulator learning model, we apply it to the task of knowledge base completion on four knowledge bases: we compare the experimental results of our model with those of an iterated structural gradient (ISG) (Wang et al., 2014) based on ProPPR, an efficient and scalable first-order probabilistic logic program (Wang et al., 2013); and the ProPPR-based ISG method has been compared with other popular ILP methods such as FOIL (Quinlan, 1990) or pseudo-probability-based structural learning methods for MLNs (Richardson & Domingos, 2006)."}, {"heading": "4.1. Data sets", "text": "In our experiment we use the following four knowledge bases: Unified Medical Language System (UMLS) (Kok & Domingos, 2007), Alyawarra clan (Kok & Domingos, 2007), Wordsworth (Miller, 1995) and a subset of Freebase (Bordes et al., 2013).The statistics of the databases are in Table 1.The UMLS knowledge base contains facts about medically related objects and symptoms such as (bacteria, affects, anatomical abnormality).The Alyawarra clan knowledge base contains facts about demographic information of an indigenous tribe in Central Australia. The Wordsworth knowledge base contains lexical information about English words. The Freebase15k knowledge base is a subset of the freebase, it contains general facts about movies, sports, etc."}, {"heading": "4.2. Experiment setup", "text": "We now describe the experimental setup of rule learning to complete the knowledge base. Completion of the task involves three types of data during training and testing. First, we need a database in which the already known facts are stored. Second, we need training samples and names of the form (query, answer) for the completion task. Each (query, answer) example can be derived from a fact (e.g., RC, x). The query is RC (?, x) and the answer is e.g. The model uses the database to find the answer to the query. There are several ways to divide the knowledge base facts into databases, training examples and test examples. We describe the data splitting and training / test protocols used in our experiments in the next section 4.2.1."}, {"heading": "4.2.1. DATA SPLITTING AND TRAINING/TESTING PROTOCOLS", "text": "The first protocol divides the facts into four disjoint sets: train database DBtrain, train examples (qtrain, atrain), test database DBtest and test examples (qtest, atest).During the training (test), the model uses DBtrain (DBtest) to answer queries qtrain (qtest).The second protocol first divides the knowledge base into two disjoint sets: DBall and test examples (qtest, atest).During each iteration in the training, a mini batch of facts ftrain is removed from the database DBall to derive from it (qtrain, atrain).And the database during this training session is DBall\\ ftrain. During the test, the database DBall is used by the model to find answers to qtest. This protocol allows more training examples than the first one."}, {"heading": "4.3. Result and analysis", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "4.3.1. BASELINE", "text": "See Section 5.1 for details of this method. The structure of data sharing and training / testing used in this method is the first protocol described in Section 4.2.1. Since our method aims to learn logical rules, we only compare them with logic-based inference methods."}, {"heading": "4.3.2. IMPLEMENTATION", "text": "In our differentiated controller model, we can either use the setup described in Section 4.2.1 and conduct experiments for both protocols. For the optimization part of the model, we use a stochastic gradient descendant algorithm ADAM (Kingma & Ba, 2014) with a learning rate of 0.001. We set the maximum control length to two. Input to the neural controller at each step is a continuous representation of the query relationship by looking up the embedding matrix of the relationship. This embedding matrix is randomly initialized.The hidden state size and continuous query size are both 128. The minibatch size is 128 or 64, depending on whether the minibatch can fit into the GPU memory. The model is implemented in TensorFlow (Abadi et al., 2016).We store the matrix M's in Equation 10 matrices as sparse matrices on the PPU, which is also supported by the model running on the PX."}, {"heading": "4.3.3. EXPERIMENT RESULTS", "text": "In the context of completing the knowledge base, the response to a query is usually a ranking of entities. Therefore, we use Hits @ 10 (Bordes et al., 2013) as the benchmark. This metric checks whether the target unit ranks in the top 10. We show the evaluation results on the four sets of data in Table 2 and Table 3. The top half of the table are experiments using the first data splitting protocol. The bottom half of the table are experiments using the second data splitting protocol. The baseline iterated structural gradient (ISG) method is only implemented using the first protocol. Diff Rule refers to our differentiable rule-splitting method. Diff Rule Embed refers to where we also use entity embedding to atter the hidden states of the neural controller, as in Equation 15. Using both data splits and training / test protocols will serve as a better differentiator."}, {"heading": "5. Related Work", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "5.1. Inductive logic programming", "text": "This year, it has come to the point where you see yourself able to go to a place where you can go to a place where you are able to move, to move, to move and to move."}, {"heading": "5.2. Neural programmer and Differentiable Neural Computer", "text": "Our learning approach is based on previous work in learning neural controllers that learn to perform sequences of (normally distinguishable) operations on memory. For example, the Neural Programmer (Neelakantan et al., 2015; 2016) is a similar neural controller model that uses an attention mechanism to \"gently\" decide which actions to perform at each step. Our method differs from the Neural Programmer in several details, but most importantly in the selection of operators. The goal of the Neural Programmer is to analyze natural linguistic expressions in action sequences, and therefore each of their actions is an independent task, such as searching in the table or summing. In contrast, we choose operations (such as the sparse matrix multiplication) and a database that encodes operations so that, properly sequenced, can be used to implement logical reasoning. The 2016 operators we use are based on logic, which is a deductive process developed internally for Cohen."}, {"heading": "5.3. Representation learning", "text": "The completion of the knowledge base, the task considered in the experimental part of this work, can also be approached with methods of representative learning (Bordes et al., 2013; Socher et al., 2013; Lin et al., 2015; Shen et al., 2016), in which knowledge base units and relationships are presented as low-dimensional continuous vectors, so that similarities in this space can be used to draw certain conclusions. A common approach to embedding knowledge base models is to learn representations of relationships (referred to as WR) and units (referred to as ve) in such a way that for some measurement functions f the value f (ve1, WR, ve2) is maximized for all R (e1, e2) facts. To derive from this, whether some facts R (e1, e2) and units (referred to as ve) are held so that we calculate the value of entities f (ve1, WR, ve2) and if necessary."}, {"heading": "6. Conclusions and Future Work", "text": "We present a differentiated method for learning rules for the task of completing the knowledge base. Our method builds on a recently developed differentiated deductive database TensorLog. Input and output of our controller learning method are real vectors that can be interpreted as hidden states and probability distributions, that is, objects that can be conveniently linked to \"upstream\" and \"downstream\" learning tasks. We hope that this differentiated rule learning method can enable various integrations of thought and pattern recognition tasks. In the future, we plan to work on more problems where the composition of logical operators is indispensable and complementary to pattern recognition, such as data integration, reading comprehension, etc."}], "references": [{"title": "Learning to compose neural networks for question answering", "author": ["Andreas", "Jacob", "Rohrbach", "Marcus", "Darrell", "Trevor", "Klein", "Dan"], "venue": "In Proceedings of NAACL-HLT,", "citeRegEx": "Andreas et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Andreas et al\\.", "year": 2016}, {"title": "Freebase: a collaboratively created graph database for structuring human knowledge", "author": ["Bollacker", "Kurt", "Evans", "Colin", "Paritosh", "Praveen", "Sturge", "Tim", "Taylor", "Jamie"], "venue": "In Proceedings of the 2008 ACM SIGMOD international conference on Management of data,", "citeRegEx": "Bollacker et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Bollacker et al\\.", "year": 2008}, {"title": "Translating embeddings for modeling multi-relational data", "author": ["Bordes", "Antoine", "Usunier", "Nicolas", "Garcia-Duran", "Alberto", "Weston", "Jason", "Yakhnenko", "Oksana"], "venue": "In Advances in neural information processing systems,", "citeRegEx": "Bordes et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Bordes et al\\.", "year": 2013}, {"title": "Fast effective rule induction", "author": ["Cohen", "William W"], "venue": "In Proceedings of the twelfth international conference on machine learning,", "citeRegEx": "Cohen and W.,? \\Q1995\\E", "shortCiteRegEx": "Cohen and W.", "year": 1995}, {"title": "Tensorlog: A differentiable deductive database", "author": ["Cohen", "William W"], "venue": "arXiv preprint arXiv:1605.06523,", "citeRegEx": "Cohen and W.,? \\Q2016\\E", "shortCiteRegEx": "Cohen and W.", "year": 2016}, {"title": "Polynomial learnability and inductive logic programming: Methods and results", "author": ["Cohen", "William W", "Page", "C David"], "venue": "New Generation Computing,", "citeRegEx": "Cohen et al\\.,? \\Q1995\\E", "shortCiteRegEx": "Cohen et al\\.", "year": 1995}, {"title": "Gated-attention readers for text comprehension", "author": ["Dhingra", "Bhuwan", "Liu", "Hanxiao", "Cohen", "William W", "Salakhutdinov", "Ruslan"], "venue": "arXiv preprint arXiv:1606.01549,", "citeRegEx": "Dhingra et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Dhingra et al\\.", "year": 2016}, {"title": "Incorporating vector space similarity in random walk inference over knowledge bases", "author": ["Gardner", "Matt", "Talukdar", "Partha", "Krishnamurthy", "Jayant", "Mitchell", "Tom"], "venue": "In Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP),", "citeRegEx": "Gardner et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Gardner et al\\.", "year": 2014}, {"title": "Introduction to statistical relational learning", "author": ["Getoor", "Lise"], "venue": "MIT press,", "citeRegEx": "Getoor and Lise.,? \\Q2007\\E", "shortCiteRegEx": "Getoor and Lise.", "year": 2007}, {"title": "Long shortterm memory", "author": ["Hochreiter", "Sepp", "Schmidhuber", "J\u00fcrgen"], "venue": "Neural computation,", "citeRegEx": "Hochreiter et al\\.,? \\Q1997\\E", "shortCiteRegEx": "Hochreiter et al\\.", "year": 1997}, {"title": "Adam: A method for stochastic optimization", "author": ["Kingma", "Diederik", "Ba", "Jimmy"], "venue": "arXiv preprint arXiv:1412.6980,", "citeRegEx": "Kingma et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Kingma et al\\.", "year": 2014}, {"title": "Learning the structure of markov logic networks", "author": ["Kok", "Stanley", "Domingos", "Pedro"], "venue": "In Proceedings of the 22nd international conference on Machine learning,", "citeRegEx": "Kok et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Kok et al\\.", "year": 2005}, {"title": "Statistical predicate invention", "author": ["Kok", "Stanley", "Domingos", "Pedro"], "venue": "In Proceedings of the 24th international conference on Machine learning,", "citeRegEx": "Kok et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Kok et al\\.", "year": 2007}, {"title": "Relational retrieval using a combination of path-constrained random walks", "author": ["Lao", "Ni", "Cohen", "William W"], "venue": "Machine learning,", "citeRegEx": "Lao et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Lao et al\\.", "year": 2010}, {"title": "Random walk inference and learning in a large scale knowledge base", "author": ["Lao", "Ni", "Mitchell", "Tom", "Cohen", "William W"], "venue": "In Proceedings of the Conference on Empirical Methods in Natural Language Processing,", "citeRegEx": "Lao et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Lao et al\\.", "year": 2011}, {"title": "Modeling relation paths for representation learning of knowledge bases", "author": ["Lin", "Yankai", "Liu", "Zhiyuan", "Luan", "Huanbo", "Sun", "Maosong", "Rao", "Siwei", "Song"], "venue": "arXiv preprint arXiv:1506.00379,", "citeRegEx": "Lin et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Lin et al\\.", "year": 2015}, {"title": "Wordnet: a lexical database for english", "author": ["Miller", "George A"], "venue": "Communications of the ACM,", "citeRegEx": "Miller and A.,? \\Q1995\\E", "shortCiteRegEx": "Miller and A.", "year": 1995}, {"title": "Inverse entailment and progol", "author": ["Muggleton", "Stephen"], "venue": "New generation computing,", "citeRegEx": "Muggleton and Stephen.,? \\Q1995\\E", "shortCiteRegEx": "Muggleton and Stephen.", "year": 1995}, {"title": "Efficient induction of logic programs", "author": ["Muggleton", "Stephen", "Feng", "Cao"], "venue": "Citeseer,", "citeRegEx": "Muggleton et al\\.,? \\Q1990\\E", "shortCiteRegEx": "Muggleton et al\\.", "year": 1990}, {"title": "Inductive logic programming, volume 38", "author": ["Muggleton", "Stephen", "Otero", "Ramon", "TamaddoniNezhad", "Alireza"], "venue": null, "citeRegEx": "Muggleton et al\\.,? \\Q1992\\E", "shortCiteRegEx": "Muggleton et al\\.", "year": 1992}, {"title": "Stochastic logic programs", "author": ["Muggleton", "Stephen"], "venue": "Advances in inductive logic programming,", "citeRegEx": "Muggleton and Stephen,? \\Q1996\\E", "shortCiteRegEx": "Muggleton and Stephen", "year": 1996}, {"title": "Metainterpretive learning of higher-order dyadic datalog: Predicate invention revisited", "author": ["Muggleton", "Stephen H", "Lin", "Dianhuan"], "venue": "In Twenty-Third International Joint Conference on Artificial Intelligence,", "citeRegEx": "Muggleton et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Muggleton et al\\.", "year": 2013}, {"title": "Neural programmer: Inducing latent programs with gradient descent", "author": ["Neelakantan", "Arvind", "Le", "Quoc V", "Sutskever", "Ilya"], "venue": "arXiv preprint arXiv:1511.04834,", "citeRegEx": "Neelakantan et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Neelakantan et al\\.", "year": 2015}, {"title": "Learning a natural language interface with neural programmer", "author": ["Neelakantan", "Arvind", "Le", "Quoc V", "Abadi", "Martin", "McCallum", "Andrew", "Amodei", "Dario"], "venue": "arXiv preprint arXiv:1611.08945,", "citeRegEx": "Neelakantan et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Neelakantan et al\\.", "year": 2016}, {"title": "Learning logical definitions from relations", "author": ["Quinlan", "J. Ross"], "venue": "Machine learning,", "citeRegEx": "Quinlan and Ross.,? \\Q1990\\E", "shortCiteRegEx": "Quinlan and Ross.", "year": 1990}, {"title": "Foil: A midterm report", "author": ["Quinlan", "J Ross", "Cameron-Jones", "R Mike"], "venue": "In European conference on machine learning,", "citeRegEx": "Quinlan et al\\.,? \\Q1993\\E", "shortCiteRegEx": "Quinlan et al\\.", "year": 1993}, {"title": "Implicit reasonet: Modeling large-scale structured relationships with shared memory", "author": ["Shen", "Yelong", "Huang", "Po-Sen", "Chang", "Ming-Wei", "Gao", "Jianfeng"], "venue": "arXiv preprint arXiv:1611.04642,", "citeRegEx": "Shen et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Shen et al\\.", "year": 2016}, {"title": "Reasoning with neural tensor networks for knowledge base completion", "author": ["Socher", "Richard", "Chen", "Danqi", "Manning", "Christopher D", "Ng", "Andrew"], "venue": "In Advances in neural information processing systems,", "citeRegEx": "Socher et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Socher et al\\.", "year": 2013}, {"title": "Learning first-order logic embeddings via matrix factorization", "author": ["Wang", "William Yang", "Cohen", "William W"], "venue": "In Proceedings of the 25th International Joint Conference on Artificial Intelligence (IJCAI", "citeRegEx": "Wang et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Wang et al\\.", "year": 2015}, {"title": "Structure learning via parameter learning", "author": ["Wang", "William Yang", "Mazaitis", "Kathryn", "Cohen", "William W"], "venue": "In CIKM 2014,", "citeRegEx": "Wang et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Wang et al\\.", "year": 2014}, {"title": "Efficient inference and learning in a large knowledge base", "author": ["Wang", "William Yang", "Mazaitis", "Kathryn", "Lao", "Ni", "Cohen", "William W"], "venue": "Machine Learning,", "citeRegEx": "Wang et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Wang et al\\.", "year": 2015}, {"title": "Efficient inference and learning in a large knowledge base", "author": ["Wang", "William Yang", "Mazaitis", "Kathryn", "Lao", "Ni", "Cohen", "William W"], "venue": "Machine Learning,", "citeRegEx": "Wang et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Wang et al\\.", "year": 2015}, {"title": "Words or characters? fine-grained gating for reading comprehension", "author": ["Yang", "Zhilin", "Dhingra", "Bhuwan", "Yuan", "Ye", "Hu", "Junjie", "Cohen", "William W", "Salakhutdinov", "Ruslan"], "venue": "arXiv preprint arXiv:1611.01724,", "citeRegEx": "Yang et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Yang et al\\.", "year": 2016}], "referenceMentions": [{"referenceID": 19, "context": "This problem is often called inductive logic programming (ILP) (Muggleton et al., 1992) or statistical relational learning (SRL) (Getoor, 2007) and typically the underlying logic is a probabilistic logic, such as Markov Logic Networks (Richardson & Domingos, 2006) or ProPPR (Wang et al.", "startOffset": 63, "endOffset": 87}, {"referenceID": 14, "context": "knowledge base completion (Lao et al., 2011; Wang et al., 2015b).", "startOffset": 26, "endOffset": 64}, {"referenceID": 29, "context": "Most past learning systems have thus used local-search based optimization methods that interleave moves in a discrete space with moves in parameter space (Kok & Domingos, 2007; Lao & Cohen, 2010; Wang et al., 2014).", "startOffset": 154, "endOffset": 214}, {"referenceID": 22, "context": "In this paper, we describe a neural controller system (Neelakantan et al., 2015; Andreas et al., 2016) which learns how to sequentially compose the primitive operations used ar X iv :1 70 2.", "startOffset": 54, "endOffset": 102}, {"referenceID": 0, "context": "In this paper, we describe a neural controller system (Neelakantan et al., 2015; Andreas et al., 2016) which learns how to sequentially compose the primitive operations used ar X iv :1 70 2.", "startOffset": 54, "endOffset": 102}, {"referenceID": 29, "context": "Our differentiable rule learning method outperforms a strong iterated structure gradient (Wang et al., 2014) baseline.", "startOffset": 89, "endOffset": 108}, {"referenceID": 1, "context": "Modern knowledge bases such as Freebase (Bollacker et al., 2008) contains millions of entities and billions of facts.", "startOffset": 40, "endOffset": 64}, {"referenceID": 27, "context": "Representation based knowledge base completion methods rely on learning embeddings for entities (Socher et al., 2013).", "startOffset": 96, "endOffset": 117}, {"referenceID": 6, "context": "The idea is to use the entity to control how much information flow from the hidden state ht to the relation weights at by adding a gate on the hidden states (Dhingra et al., 2016; Yang et al., 2016).", "startOffset": 157, "endOffset": 198}, {"referenceID": 32, "context": "The idea is to use the entity to control how much information flow from the hidden state ht to the relation weights at by adding a gate on the hidden states (Dhingra et al., 2016; Yang et al., 2016).", "startOffset": 157, "endOffset": 198}, {"referenceID": 29, "context": "We compare the experiment results from our model and those from an iterated structural gradient (ISG) (Wang et al., 2014) structure learning method based on ProPPR, an efficient and scalable first-order probabilistic logic program (Wang et al.", "startOffset": 102, "endOffset": 121}, {"referenceID": 2, "context": "We use the following four knowledge bases in our experiment: Unified Medical Language System (UMLS) (Kok & Domingos, 2007), Alyawarra kinship (Kok & Domingos, 2007), Wordnet (Miller, 1995), and a subset of Freebase (Bordes et al., 2013).", "startOffset": 215, "endOffset": 236}, {"referenceID": 2, "context": "Therefore we use Hits@10 (Bordes et al., 2013) as the evaluation metric.", "startOffset": 25, "endOffset": 46}, {"referenceID": 18, "context": "Other discrete search methods used to search for logical theories include searches that exploit special operators, such as finding the least general generalization of two candidate rules (Muggleton et al., 1990) or inverse entailment (Muggleton, 1995).", "startOffset": 187, "endOffset": 211}, {"referenceID": 14, "context": "For instance, the Path Ranking Algorithm (PRA) (Lao et al., 2011) first enumerates a subset of all possible rules, consisting of chain rules of limited length.", "startOffset": 47, "endOffset": 65}, {"referenceID": 7, "context": "PRA has since been extended in several ways, for instance, by adopting \u201csofter paths\u201d (Gardner et al., 2014) based on vector space similarity.", "startOffset": 86, "endOffset": 108}, {"referenceID": 22, "context": "For instance, the Neural Programmer (Neelakantan et al., 2015; 2016) is a similar neural controller model that uses an attention mechanism to \u201csoftly\u201d choose which actions to take at each step.", "startOffset": 36, "endOffset": 68}, {"referenceID": 2, "context": "Knowledge base completion, the task considered in the experimental part of this paper, can also be addressed with representation learning methods (Bordes et al., 2013; Socher et al., 2013; Lin et al., 2015; Shen et al., 2016), in which knowledge bases entities and relations are represented as low-dimensional continuous vectors, so that similarity in this space can be used to make certain inferences.", "startOffset": 146, "endOffset": 225}, {"referenceID": 27, "context": "Knowledge base completion, the task considered in the experimental part of this paper, can also be addressed with representation learning methods (Bordes et al., 2013; Socher et al., 2013; Lin et al., 2015; Shen et al., 2016), in which knowledge bases entities and relations are represented as low-dimensional continuous vectors, so that similarity in this space can be used to make certain inferences.", "startOffset": 146, "endOffset": 225}, {"referenceID": 15, "context": "Knowledge base completion, the task considered in the experimental part of this paper, can also be addressed with representation learning methods (Bordes et al., 2013; Socher et al., 2013; Lin et al., 2015; Shen et al., 2016), in which knowledge bases entities and relations are represented as low-dimensional continuous vectors, so that similarity in this space can be used to make certain inferences.", "startOffset": 146, "endOffset": 225}, {"referenceID": 26, "context": "Knowledge base completion, the task considered in the experimental part of this paper, can also be addressed with representation learning methods (Bordes et al., 2013; Socher et al., 2013; Lin et al., 2015; Shen et al., 2016), in which knowledge bases entities and relations are represented as low-dimensional continuous vectors, so that similarity in this space can be used to make certain inferences.", "startOffset": 146, "endOffset": 225}], "year": 2017, "abstractText": "Learned models composed of probabilistic logical rules are useful for many tasks, such as knowledge base completion. Unfortunately this learning problem is difficult, since determining the structure of the theory normally requires solving a discrete optimization problem. In this paper, we propose an alternative approach: a completely differentiable model for learning sets of first-order rules. The approach is inspired by a recently-developed differentiable logic, i.e. a subset of first-order logic for which inference tasks can be compiled into sequences of differentiable operations. Here we describe a neural controller system which learns how to sequentially compose the these primitive differentiable operations to solve reasoning tasks, and in particular, to perform knowledge base completion. The long-term goal of this work is to develop integrated, end-to-end systems that can learn to perform high-level logical reasoning as well as lower-level perceptual tasks.", "creator": "LaTeX with hyperref package"}}}