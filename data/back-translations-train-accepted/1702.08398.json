{"id": "1702.08398", "review": {"conference": "ICML", "VERSION": "v1", "DATE_OF_SUBMISSION": "27-Feb-2017", "title": "McGan: Mean and Covariance Feature Matching GAN", "abstract": "We introduce new families of Integral Probability Metrics (IPM) for training Generative Adversarial Networks (GAN). Our IPMs are based on matching statistics of distributions embedded in a finite dimensional feature space. Mean and covariance feature matching IPMs allow for stable training of GANs, which we will call McGan. McGan minimizes a meaningful loss between distributions.", "histories": [["v1", "Mon, 27 Feb 2017 17:46:30 GMT  (3643kb,D)", "http://arxiv.org/abs/1702.08398v1", "15 pages; under review"], ["v2", "Thu, 8 Jun 2017 23:45:25 GMT  (3887kb,D)", "http://arxiv.org/abs/1702.08398v2", "15 pages; published at ICML 2017"]], "COMMENTS": "15 pages; under review", "reviews": [], "SUBJECTS": "cs.LG stat.ML", "authors": ["youssef mroueh", "tom sercu", "vaibhava goel"], "accepted": true, "id": "1702.08398"}, "pdf": {"name": "1702.08398.pdf", "metadata": {"source": "META", "title": "McGan: Mean and Covariance Feature Matching GAN", "authors": ["Youssef Mroueh", "Tom Sercu", "Vaibhava Goel"], "emails": ["<mroueh@us.ibm.com>."], "sections": [{"heading": "1. Introduction", "text": "The classic approach to learning distributions is to explicitly parameterise the probability of data and adapt this model by maximising the probability of real data. An alternative current approach is to learn a generative model of data distribution without explicitly parameterising probability. We focus on the GAN approach of probability. Variational AutoEncoders (UAE) (Kingma & Welling, 2013) and Generative Adversarial Networks (GAN) (Goodfellow et al., 2014) fall into this category. We focus on the GAN approach of distributing data via a min-max game between the generator and a discriminator that distinguishes between \"real\" and \"fake\" samples. In this paper, we focus on the objective function that is minimised between the learned distribution and the real data."}, {"heading": "2. Integral Probability Metrics", "text": "In this section, we define IPMs as the distance between distributions. Intuitively, each IPM finds a \"critic\" f (Arjovsky et al., 2017) who distinguishes maximum between distributions."}, {"heading": "2.1. IPM Definition", "text": "Consider a compact space X in Rd. Consider F as a set of measurable and limited real value functions on X. Consider P (X) as the set of measurable probability distributions on X. Faced with two probability distributions P, Q, P (X), the Integrated Probability Metric (IPM) indexed by the function space F is defined as follows (Muller, 1997): dF (P, Q) = sup f, F x, P f (x) \u2212 E x, Q f (x).In this paper, we are interested in symmetrical function spaces F, i.e. f, F, \u2212 f, F, hence, in this case, we can write the IPM without the absolute value: dF (P, Q) = sup f, F {E, P f (x) \u2212 P (x)."}, {"heading": "2.2. Learning Generative Models with IPM", "text": "To learn a generative model of a distribution Pr-P (X), we learn a function distribution: Z-Rnz \u2192 X, so that for z-pz the distribution of g\u03b8 (z) is close to the real data distribution Pr, where pz is a fixed distribution to Z (for example z-N (0, Ik)). Let P\u03b8 be the distribution of g\u03b8 (z), z-pz. With the help of an IPM indexed by a function class F, we will solve the following problem: min-g\u03b8dF (Pr, P\u03b8) (2). Consequently, this boils down to solving the following min-max problem: min-g\u03b8 f-F E x-Pr f (x) \u2212 E-pz f (g\u0432 (z))) Given the samples {xi, 1... N} from Pr and the samples {zi, 1... M} from pz, we will solve the following empirical problem: min-sup-F1i-Ni (M) for the city (xi) x (N)."}, {"heading": "3. Mean Feature Matching GAN", "text": "In this section, we present a class of functions F in the form < v, \u03a6\u03c9 (x) >, where the vector v-Rm and \u03a6\u03c9-Rm: X-Rm is a nonlinear character card (typically parameterized by a neural network), and we show in this section that the IPM defined by this function class corresponds to the distance between the mean of the distribution in \u03a6\u03c9 space."}, {"heading": "3.1. IPM\u00b5,q: Mean Matching IPM", "text": "Formally, we consider the following finite Hilbert space: Fv, \u03c9, p = {f (x) q = q (q) = \u00b2 \u00b2 \u00b2 (p) = \u00b2 \u00b2 \u00b2 \u00b2 (p) = \u00b2 \u00b2 \u00b2 \u00b2 (p) = \u00b2 \u00b2 \u00b2 \u00b2 (p) = \u00b2 \u00b2 \u00b2 \u00b2 (p) = \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 (p) = \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 (p) = \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 (p) = \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 (p) = \u00b2 \u00b2 \u00b2 (p) = \u00b2 (p) = \u00b2) = \u00b2 (p) = \u00b2 (p) = = = \u00b2 (p). We remember here simple definitions of the double standards that will be necessary for the analysis in this section. Leave p, q [1] is typically a multi-layered neural network."}, {"heading": "3.2. Mean Feature Matching GAN", "text": "We now turn to the problem of learning generative models with IPM\u00b5, q. If we set F to Fv, \u03c9, p in Equation (2), the following minimum-maximum problem for learning generative models arises: min g\u03b8 max."}, {"heading": "3.3. Related Work", "text": "We show in this section that several previous papers on GAN, written within the \"q-characteristic matching IPM = Q = Q-characteristic (IPM\u00b5, q) minimization framework: a) Wasserstein GAN (WGAN): (Arjovsky et al., 2017) recently introduced Wasserstein GAN. While the main motivation of this work is to consider the functions indexed by Lipchitz to X, we show that the particular parameterization in (Arjovsky et al., 2017) corresponds to an average characteristic that corresponds to IPM. In fact (Arjovsky et al., 2017) we consider the function parameterized by a revolutionary neural network with a linear output layer and weight circumcision."}, {"heading": "4. Covariance Feature Matching GAN", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "4.1. IPM\u03a3: Covariance Matching IPM", "text": "As follows from our discussion of the mean of the two distributions: F = > Q = > Q = > F = > F = > F = > F = > F = > F = > F = > F = (F = > F = > F = > F (F = > F = > F = > F (F = > F = >)) In this section, we will provide a functional space in which the IPM captures second-order information in Equation (1). Similarly, we will define the k-guidelines that maximize the discrimination between the two covariances. In this section, the addition of second-order information would increase the discriminatory power of the feature space (see Figure 1)."}, {"heading": "4.2. Covariance Matching GAN", "text": "Let us now address the problem of learning a generative model g\u03b8 of Pr-P (X) > Pr-P (X) using IPM\u043d. We will use the following primary formulations: min-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-"}, {"heading": "4.3. Mean and Covariance Matching GAN", "text": "In order to assign first and second order statistics, we propose the following simple extension: min g\u043d max \u043e, v, | | v | | p \u2264 1 U, V-Om, k L\u00b5 (v, \u03c9, \u03b8) + L\u03c3 (U, V, \u03c9, \u03b8), which has a simple double opposing game interpretation min g\u03b8 max.: 0-1 U, V-Om, k-L\u00b5 (v, \u03c9, \u03b8) + L\u03c3 (U, V, \u03c9, \u03b8), in which the differentiator finds a space of features distinguishing between means and deviations from real and fake, and the generator tries to match the real statistics. We can also give empirical estimates of the original formulation, which is similar to the expressions in the essay."}, {"heading": "5. Algorithms", "text": "In this section we present our algorithms for intermediate and covariance, the GAN (McGan) with IPM\u00b5, q and IPM.Mean Matching GAN. Primal P\u00b5: We give in algorithm 1 an algorithm for solving the primary IPM\u00b5, q GAN (P\u00b5). Algorithm 1 is adapted by (Arjovsky et al., 2017) and corresponds to their algorithm for p = \u221e. The main difference is that we allow the projection of v on different \"p-balls\" and maintain the circumcision of \"p\" to ensure the bounce of \"p.\" For example, p = 2, projB '2 (v) = min (v = 2) v. Clipping in (Arjovsky et al al., 2017) projB. \""}, {"heading": "6. Experiments", "text": "We produce McGan for the production of images with both Mean Matching and Covariance Matching objectives. We produce \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7"}, {"heading": "7. Discussion", "text": "We noticed the influence of clipping on the capacity of the critic: a greater number of feature maps was needed to compensate for clipping. It remains to be seen what alternatives to clipping \u03a6\u03c9 can ensure boundedness. So we successfully applied a \"2 penalty on the weights of \u03a6\u03c9. Other directions are the exploration of geodesic distances between covariances (Arsigny et al., 2006) and the extension of the IPM framework to multimodal setting (Isola et al., 2016)."}, {"heading": "A. Subspace Matching Interpretation of Covariance Matching GAN", "text": "This is a symmetrical matrix, but not PSD, which relates the eigenvalues \u03bbj to their singular values as given by: \u03c3j = | \u03bbj | and its left and right singular vectors. We could ask here whether we can avoid having both U, V in the definition of IPM\u03a3, because at the optimum uj = \u00b1 vj. We could consider the following equality: E\u03c9 (Pr, PTB) defines as follows: max., U, k E x. Pr. - 2Energy in the subspace of real data \u2212 E for example:"}, {"heading": "B. Mean and Covariance Matching Loss Combinations", "text": "We report below on examples of McGan with different IPMmicro, q and IPM\u03a3 combinations. All results are reported for the same architecture selection for generator and discriminator that yielded high-quality samples with IPM\u03a3 (the same was reported in Section 6 in the main article). Note that in Figure 7 with the same hyperparameters and architecture selection, WGAN did not produce a good sample. In other configurations, the training converged."}], "references": [{"title": "Optimization Algorithms on Matrix Manifolds", "author": ["Absil", "P.-A", "R. Mahony", "R. Sepulchre"], "venue": null, "citeRegEx": "Absil et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Absil et al\\.", "year": 2007}, {"title": "Log-euclidean metrics for fast and simple calculus on diffusion tensors", "author": ["Arsigny", "Vincent", "Fillard", "Pierre", "Pennec", "Xavier", "Ayache", "Nicholas"], "venue": "In Magnetic Resonance in Medicine,", "citeRegEx": "Arsigny et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Arsigny et al\\.", "year": 2006}, {"title": "Infogan: Interpretable representation learning by information maximizing generative adversarial nets", "author": ["Chen", "Xi", "Duan", "Yan", "Houthooft", "Rein", "Schulman", "John", "Sutskever", "Ilya", "Abbeel", "Pieter"], "venue": null, "citeRegEx": "Chen et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Chen et al\\.", "year": 2016}, {"title": "Training generative neural networks via maximum mean discrepancy optimization", "author": ["Dziugaite", "Gintare Karolina", "Roy", "Daniel M", "Ghahramani", "Zoubin"], "venue": "In UAI,", "citeRegEx": "Dziugaite et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Dziugaite et al\\.", "year": 2015}, {"title": "Generative adversarial nets", "author": ["Goodfellow", "Ian", "Pouget-Abadie", "Jean", "Mirza", "Mehdi", "Xu", "Bing", "Warde-Farley", "David", "Ozair", "Sherjil", "Courville", "Aaron", "Bengio", "Yoshua"], "venue": "In NIPS", "citeRegEx": "Goodfellow et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Goodfellow et al\\.", "year": 2014}, {"title": "A kernel two-sample test", "author": ["Gretton", "Arthur", "Borgwardt", "Karsten M", "Rasch", "Malte J", "Sch\u00f6lkopf", "Bernhard", "Smola", "Alexander"], "venue": null, "citeRegEx": "Gretton et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Gretton et al\\.", "year": 2012}, {"title": "Labeled faces in the wild: A database for studying face recognition in unconstrained environments", "author": ["Huang", "Gary B", "Ramesh", "Manu", "Berg", "Tamara", "Learned-Miller", "Erik"], "venue": "Technical report,", "citeRegEx": "Huang et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Huang et al\\.", "year": 2007}, {"title": "Batch normalization: Accelerating deep network training by reducing internal covariate shift", "author": ["Ioffe", "Sergey", "Szegedy", "Christian"], "venue": "Proc. ICML,", "citeRegEx": "Ioffe et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Ioffe et al\\.", "year": 2015}, {"title": "Image-to-image translation with conditional adversarial networks", "author": ["Isola", "Phillip", "Zhu", "Jun-Yan", "Zhou", "Tinghui", "Efros", "Alexei A"], "venue": "arxiv,", "citeRegEx": "Isola et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Isola et al\\.", "year": 2016}, {"title": "Auto-encoding variational bayes", "author": ["Kingma", "Diederik P", "Welling", "Max"], "venue": null, "citeRegEx": "Kingma et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Kingma et al\\.", "year": 2013}, {"title": "Learning multiple layers of features from tiny images", "author": ["A. Krizhevsky", "G. Hinton"], "venue": "Master\u2019s thesis,", "citeRegEx": "Krizhevsky and Hinton,? \\Q2009\\E", "shortCiteRegEx": "Krizhevsky and Hinton", "year": 2009}, {"title": "Generative moment matching networks", "author": ["Li", "Yujia", "Swersky", "Kevin", "Zemel", "Richard"], "venue": "In ICML,", "citeRegEx": "Li et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Li et al\\.", "year": 2015}, {"title": "Conditional generative adversarial nets", "author": ["Mirza", "Mehdi", "Osindero", "Simon"], "venue": "Arxiv,", "citeRegEx": "Mirza et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Mirza et al\\.", "year": 2014}, {"title": "Kernel mean embedding of distributions: A review and beyond", "author": ["Muandet", "Krikamol", "Fukumizu", "Kenji", "Sriperumbudur", "Bharath", "Schlkopf", "Bernhard"], "venue": null, "citeRegEx": "Muandet et al\\.,? \\Q2017\\E", "shortCiteRegEx": "Muandet et al\\.", "year": 2017}, {"title": "Integral probability metrics and their generating classes of functions", "author": ["Muller", "Alfred"], "venue": "Advances in Applied Probability,", "citeRegEx": "Muller and Alfred.,? \\Q1997\\E", "shortCiteRegEx": "Muller and Alfred.", "year": 1997}, {"title": "f-gan: Training generative neural samplers using variational divergence minimization", "author": ["Nowozin", "Sebastian", "Cseke", "Botond", "Tomioka", "Ryota"], "venue": "In NIPS,", "citeRegEx": "Nowozin et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Nowozin et al\\.", "year": 2016}, {"title": "Conditional image synthesis with auxiliary classifier gans", "author": ["Odena", "Augustus", "Olah", "Christopher", "Shlens", "Jonathon"], "venue": null, "citeRegEx": "Odena et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Odena et al\\.", "year": 2016}, {"title": "Unsupervised representation learning with deep convolutional generative adversarial networks", "author": ["Radford", "Alec", "Metz", "Luke", "Chintala", "Soumith"], "venue": "Arxiv,", "citeRegEx": "Radford et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Radford et al\\.", "year": 2015}, {"title": "Random features for large-scale kernel machines", "author": ["Rahimi", "Ali", "Recht", "Benjamin"], "venue": "In NIPS", "citeRegEx": "Rahimi et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Rahimi et al\\.", "year": 2008}, {"title": "Improved techniques for training gans", "author": ["Salimans", "Tim", "Goodfellow", "Ian", "Zaremba", "Wojciech", "Cheung", "Vicki", "Radford", "Alec", "Chen", "Xi"], "venue": null, "citeRegEx": "Salimans et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Salimans et al\\.", "year": 2016}, {"title": "On integral probability metrics, phi -divergences and binary classification", "author": ["Sriperumbudur", "Bharath K", "Fukumizu", "Kenji", "Gretton", "Arthur", "Schlkopf", "Bernhard", "Lanckriet", "Gert R. G"], "venue": null, "citeRegEx": "Sriperumbudur et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Sriperumbudur et al\\.", "year": 2009}, {"title": "On the empirical estimation of integral probability metrics", "author": ["Sriperumbudur", "Bharath K", "Fukumizu", "Kenji", "Gretton", "Arthur", "Schlkopf", "Bernhard", "Lanckriet", "Gert R. G"], "venue": "Electronic Journal of Statistics,", "citeRegEx": "Sriperumbudur et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Sriperumbudur et al\\.", "year": 2012}, {"title": "A note on the evaluation of generative models", "author": ["Theis", "Lucas", "Oord", "A\u00e4ron van den", "Bethge", "Matthias"], "venue": null, "citeRegEx": "Theis et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Theis et al\\.", "year": 2016}, {"title": "Lsun: Construction of a large-scale image dataset using deep learning with humans in the loop", "author": ["Yu", "Fisher", "Zhang", "Yinda", "Song", "Shuran", "Seff", "Ari", "Xiao", "Jianxiong"], "venue": "arXiv preprint arXiv:1506.03365,", "citeRegEx": "Yu et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Yu et al\\.", "year": 2015}, {"title": "Energy based generative adversarial networks", "author": ["Zhao", "Junbo", "Mathieu", "Michael", "Lecun", "Yann"], "venue": null, "citeRegEx": "Zhao et al\\.,? \\Q2017\\E", "shortCiteRegEx": "Zhao et al\\.", "year": 2017}], "referenceMentions": [{"referenceID": 4, "context": "Variational AutoEncoders (VAE) (Kingma & Welling, 2013) and Generative Adversarial Networks (GAN) (Goodfellow et al., 2014) fall under this category.", "startOffset": 98, "endOffset": 123}, {"referenceID": 4, "context": "The original work of (Goodfellow et al., 2014) showed that in GAN this objective is the Jensen-Shannon divergence.", "startOffset": 21, "endOffset": 46}, {"referenceID": 15, "context": "(Nowozin et al., 2016) showed that other\u03c6-divergences can be successfully used.", "startOffset": 0, "endOffset": 22}, {"referenceID": 11, "context": "The Maximum Mean Discrepancy objective (MMD) for GAN training was proposed in (Li et al., 2015; Dziugaite et al., 2015).", "startOffset": 78, "endOffset": 119}, {"referenceID": 3, "context": "The Maximum Mean Discrepancy objective (MMD) for GAN training was proposed in (Li et al., 2015; Dziugaite et al., 2015).", "startOffset": 78, "endOffset": 119}, {"referenceID": 19, "context": "in (Salimans et al., 2016), one can train the GAN discriminator using the objective of (Goodfellow et al.", "startOffset": 3, "endOffset": 26}, {"referenceID": 4, "context": ", 2016), one can train the GAN discriminator using the objective of (Goodfellow et al., 2014) while training the generator using mean feature matching.", "startOffset": 68, "endOffset": 93}, {"referenceID": 24, "context": "An energy based objective for GANs was also developed recently (Zhao et al., 2017).", "startOffset": 63, "endOffset": 82}, {"referenceID": 13, "context": "In this paper, inspired by the MMD distance and the kernel mean embedding of distributions (Muandet et al., 2017) we propose to embed distributions in a finite dimensional feature space and to match them based on their mean and covariance feature statistics.", "startOffset": 91, "endOffset": 113}, {"referenceID": 19, "context": "While mean matching was empirically used in (Salimans et al., 2016), we show in this work that it is theoretically grounded: similarly to the EM distance in (Arjovsky et al.", "startOffset": 44, "endOffset": 67}, {"referenceID": 21, "context": "By choosing F appropriately (Sriperumbudur et al., 2012; 2009), various distances between probability measures can be defined.", "startOffset": 28, "endOffset": 62}, {"referenceID": 11, "context": "In the next subsection following (Arjovsky et al., 2017; Li et al., 2015; Dziugaite et al., 2015) we show how to use IPM to learn generative models of distributions, we then specify a special set of functions F that makes the learning tractable.", "startOffset": 33, "endOffset": 97}, {"referenceID": 3, "context": "In the next subsection following (Arjovsky et al., 2017; Li et al., 2015; Dziugaite et al., 2015) we show how to use IPM to learn generative models of distributions, we then specify a special set of functions F that makes the learning tractable.", "startOffset": 33, "endOffset": 97}, {"referenceID": 13, "context": "where \u03bc(P) = E x\u223cP \u03a6(x) \u2208H is the so called kernel mean embedding (Muandet et al., 2017).", "startOffset": 66, "endOffset": 88}, {"referenceID": 5, "context": "dF in this case is the so called Maximum kernel Mean Discrepancy (MMD) (Gretton et al., 2012) .", "startOffset": 71, "endOffset": 93}, {"referenceID": 11, "context": "(Li et al., 2015; Dziugaite et al., 2015) showed that GANs can be learned using MMD with a fixed gaussian kernel.", "startOffset": 0, "endOffset": 41}, {"referenceID": 3, "context": "(Li et al., 2015; Dziugaite et al., 2015) showed that GANs can be learned using MMD with a fixed gaussian kernel.", "startOffset": 0, "endOffset": 41}, {"referenceID": 4, "context": "c) Improved GAN: Building on the pioneering work of (Goodfellow et al., 2014), (Salimans et al.", "startOffset": 52, "endOffset": 77}, {"referenceID": 19, "context": ", 2014), (Salimans et al., 2016) suggested to learn the discriminator with the binary cross entropy criterium of GAN while learning the generator with `2 mean feature matching.", "startOffset": 9, "endOffset": 32}, {"referenceID": 0, "context": "We maintain clipping on \u03c9 to ensure boundedness of \u03a6\u03c9 , and perform a QR retraction on the Stiefel manifoldOm,k (Absil et al., 2007), maintaining orthonormality of U and V .", "startOffset": 112, "endOffset": 132}, {"referenceID": 6, "context": "(Huang et al., 2007), LSUN bedrooms (Yu et al.", "startOffset": 0, "endOffset": 20}, {"referenceID": 23, "context": ", 2007), LSUN bedrooms (Yu et al., 2015), and cifar-10 (Krizhevsky & Hinton, 2009) datasets.", "startOffset": 23, "endOffset": 40}, {"referenceID": 22, "context": "It is well-established that evaluating generative models is hard (Theis et al., 2016).", "startOffset": 65, "endOffset": 85}, {"referenceID": 17, "context": "The design of g\u03b8 and \u03a6\u03c9 are following DCGAN principles (Radford et al., 2015), with both g\u03b8 and \u03a6\u03c9 being a convolutional network with batch normalization (Ioffe & Szegedy, 2015) and ReLU activations.", "startOffset": 55, "endOffset": 77}, {"referenceID": 2, "context": "The way we incorporate label information is following InfoGAN (Chen et al., 2016) and AC-GAN (Odena et al.", "startOffset": 62, "endOffset": 81}, {"referenceID": 16, "context": ", 2016) and AC-GAN (Odena et al., 2016), where both real and generated data are labeled, but the label is not given as input to the discriminator.", "startOffset": 19, "endOffset": 39}, {"referenceID": 1, "context": "Other directions are to explore geodesic distances between the covariances (Arsigny et al., 2006), and extensions of the IPM framework to the multimodal setting (Isola et al.", "startOffset": 75, "endOffset": 97}, {"referenceID": 8, "context": ", 2006), and extensions of the IPM framework to the multimodal setting (Isola et al., 2016).", "startOffset": 71, "endOffset": 91}], "year": 2017, "abstractText": "We introduce new families of Integral Probability Metrics (IPM) for training Generative Adversarial Networks (GAN). Our IPMs are based on matching statistics of distributions embedded in a finite dimensional feature space. Mean and covariance feature matching IPMs allow for stable training of GANs, which we will call McGan. McGan minimizes a meaningful loss between distributions.", "creator": "LaTeX with hyperref package"}}}