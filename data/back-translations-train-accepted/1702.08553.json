{"id": "1702.08553", "review": {"conference": "ICML", "VERSION": "v1", "DATE_OF_SUBMISSION": "27-Feb-2017", "title": "Diameter-Based Active Learning", "abstract": "To date, the tightest upper and lower-bounds for the active learning of general concept classes have been in terms of a parameter of the learning problem called the splitting index. We provide, for the first time, an efficient algorithm that is able to realize this upper bound, and we empirically demonstrate its good performance.", "histories": [["v1", "Mon, 27 Feb 2017 21:59:24 GMT  (576kb,D)", "https://arxiv.org/abs/1702.08553v1", "18 pages, 2 figures"], ["v2", "Fri, 9 Jun 2017 00:39:57 GMT  (574kb,D)", "http://arxiv.org/abs/1702.08553v2", "16 pages, 2 figures"]], "COMMENTS": "18 pages, 2 figures", "reviews": [], "SUBJECTS": "cs.LG stat.ML", "authors": ["christopher tosh", "sanjoy dasgupta"], "accepted": true, "id": "1702.08553"}, "pdf": {"name": "1702.08553.pdf", "metadata": {"source": "CRF", "title": "Diameter-Based Active Learning", "authors": ["Christopher Tosh", "Sanjoy Dasgupta"], "emails": ["ctosh@cs.ucsd.edu", "dasgupta@cs.ucsd.edu"], "sections": [{"heading": "1 Introduction", "text": "In many situations where a classifier is an aggressive strategy of complexity, it is easy to collect unmarked data but costly to obtain labels, which has motivated the pool-based active learning model, in which a learner has access to a collection of unmarked data points and is allowed to ask for individual labels in an adaptive manner. It is hoped that the selection of these terms will quickly lead to a slightly flawed classifier, much faster than with random query. A central focus of active learning is the development of efficient query strategies and understanding of their label complexity. In the past decade or two, there has been significant progress in developing such rigorously justified active learning schemes for general concept classes. For most parts, these schemes can be described as mellow: instead of focusing on informative points, they ask any point whose label cannot reasonably be derived from the information they receive. It is of interest to develop more aggressive strategies with better label complexity."}, {"heading": "2 Related work", "text": "The theory of active learning has developed along several fronts, one of which is non-parametric active learning, where the learner begins with a pool of blank dots, questions some of them adaptively, and then fills in the remaining labels, with the goal of doing so with as few errors as possible. (In particular, the learner does not return a classifier from a predefined parametrized class.) One scheme begins by building a neighborhood diagram on the unlabeled data and spreading queried labels along the edges of that diagram [24, 7, 10]; another begins with a hierarchical clustering of the data and moves down the tree, randomly finding clusters that are relatively pure in their labels [13]. The label complexity of such methods is typically applied in terms of the suppleness of the properties of the underlying data distribution [6, 22].Another line of work has focused on active decision making at current lines, applying relatively close to the boundary of the 14]."}, {"heading": "3 Preliminaries", "text": "Consider a binary hypotheses class H, a data space X, and a distribution D over X. For mathematical convenience, we limit ourselves to finite hypotheses classes. (We can do this without loss of universality if H has a finite VC dimension, since we use the predictions of hypotheses only on a pool of blank points; however, we do not write the details of this reduction here.) The hypotheses distance induced from D over H is the pseudometricd (h, h \u2032): = Prx \u0445 D (h (h (x) 6 = h \u2032 (x)). At a point x-X and a subset V-H, the induced conversion space is the series of hypotheses that are consistent with the target."}, {"heading": "3.1 Diameter and the Splitting Index", "text": "The diameter of a set of hypotheses V-H is the maximum distance between two hypotheses in V, i.e.diam (V): = max h, h-ig, h-ig Vd (h, h).Without prior information, any hypothesis in the version space could be the target. Therefore, the worst error of a hypothesis in the version space is the diameter of the version space. The splitting index roughly describes the number of queries required for an active learning algorithm to reduce the diameter of the version space below it. As we reduce the diameter of a version space V-H, we will sometimes identify pairs of hypotheses h, h-L-V, which are far apart from each other and therefore need to be separated. We refer to {h, h-H-H-H}} as an edge. In view of a series of edges E = {h1, h-H-H-H-H, h-H-H-H-H-H, h-H-H-H, h-th-tenth-h-n-n-tenth-h-n-n-tenth-n-n-n-n-n-n-n-n-tenth-n-n-n-n-n-tenth-n-n-th-th-n-th-n-n-th-th-th-n-n-th-n-th-n-th-n-n-th-th-n."}, {"heading": "3.2 An Average Notion of Diameter", "text": "\u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2"}, {"heading": "3.3 An Average Notion of Splitting", "text": "Let us now turn to the definition of an average term of splitting. A data point x \u03c1-average splits V ifmax {\u03c0 (V + x) 2\u03c0 (V + x), \u03c0 (V \u2212 x) 2\u0445 (V) 2 \u0445 (V \u2212 x)) \u2264 (1 \u2212 x) \u0445 (1 \u2212 x) \u0445 (V \u2212 x).And we say that a set S \u00b2 -H has an average splitting index (V \u00b2).E has average splitting indices (V \u00b2), if for each subset V \u00b2 -S such splits (V) >, Prx \u00b2 D (x \u00b2 -average splits V).Intuitively, the average splitting refers to the ability to significantly reduce the potential functional part (V) 2\u043c (V) = Eh \u00b2 -shaped splits (V) >, h \u00b2 -average splits V).D (-average splitting ability to significantly increase the potential splitting V)."}, {"heading": "4 An Average Splitting Index Algorithm", "text": "Suppose we get a version space V with an average splitting index. If we draw O points (1 / \u03c4) from the data distribution, then one of these points will most likely lead to an average splitting V. If we question this point, we get a version space V with significantly lower potential \u03c0 (V \u2032) 2\u03a6 (V \u2032). If we knew the value \u03c1 a priori, then Lemma 4 in combination with standard concentration limits [21, 1] would give us a relatively simple procedure to find a good query point: 1. Draw E \u2032 (V) 2 \u00d7 M and calculate the empirical estimate. For suitable M and N it will be the case that with a high probability, for some x, 1N max \u00b2 (E + x), for others (E \u2212 x) 2 \u00d7 N for N depending on what happens. 3. For suitable M and N it will be the case that we change the next splitting version with a high probability."}, {"heading": "4.1 Finding a Good Query Point", "text": "It takes as input a sequence of data points (x1,., xm, at least one of which is divided into the current version space, and with a high probability a data point will then be found that divides the version space. To make this algorithm successful, we must make an optimistic estimate of the number of points we specify with a high probability (1), and then gradually halve them until we are certain that we have found a point at which the version space is divided up. In order for this algorithm to be successful, we must choose nt and mt such a point with a high probability (1) that is an accurate estimate of the number (V), and (2) our holding condition will be true if the constant factor is not within a constant space and is incorrectly different. The following problem, the proof of which lies in the appendix, provides such a choice for nt and mt.Lemma 5. Let us give an answer given in version V > 0."}, {"heading": "4.2 Active Learning Strategy", "text": "Using the Select method as a subroutine, algorithm 1, henceforth DBAL for diameter-based active learning (32), is our active learning strategy. Given a hypothesis class with average splitting index (\u03c1, / 2, \u03c4), DBAL queries data points provided by Select until it is certain that it is (V) <.Denote by Vt the versioning space in the t-th round of DBAL. The following log version, which is proven in the appendix, shows that the holding state (that is, E) < 3 n / 4, where E consists of n pairs sampled in the t-th round of DBAL."}, {"heading": "5 Proof of Lemma 3", "text": "In this section, we will give the proof of the following relationship between the original splitting index and our average splitting index (subsequently)."}, {"heading": "6 Simulations", "text": "We compared DBAL with the baseline of passive linear separators and two other generic active learning strategies: Gildex and QBC units. CAL randomly scans a data point and queries it when its label cannot be derived from previously queried data points. QBC uses a previous distribution \u03c0 | V and asks x if h (x) 6 = h \u2032 (x).We tested on two hypotheses categories: homogeneous, or by origin, linear separators, and k-sparse monotonous disjunctions. In each of our simulations, we drew our target h from the previous distribution. After each query, we estimated the average diameter of the version space. We repeated each simulation several times and plotted the average performance of each algorithm. < Homogeneous linear separators."}, {"heading": "Acknowledgments", "text": "The authors would like to thank NSF for its support under the funding programmes IIS-1162581 and DGE-1144086. Part of this work was carried out at the Simons Institute for Theoretical Computer Science in Berkeley as part of a programme on the basics of machine learning. CT would also like to thank Daniel Hsu and Stefanos Poulis for their helpful discussions."}, {"heading": "Appendix: Proof Details", "text": "Remark from section 3: The remark according to the definition of the average split states that there are hypotheses classes V (max.) for which there are many points, the 1 / 4-divided E (V 2), but for each x-x-satisfaction (V + x), each x-x-satisfaction (V \u2212 x), each x-x-x-satisfaction (V \u2212 x), each x-x-x-x-satisfaction (V + x), each x-x-x-x-x-x (V \u2212 x), each x-x-x-x-x-x-x-x-x), each x-x-x-x-x-x-x-x (V + x-x), each x-x-x-x-x-x-x-x (V x-x-x-x-x-x-x), each x-x-x-x-x-x-x-x-x-x (V + x-x-x-x), each x-x-x-x-x-x-x-x-x-x-x-x), each x-x-x-x-x-x-x-x-x-x-x-x-x-x-x (V + x + x-x-x), each x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x, x-x-x-x-x-x-x-x-x-x-x-x-x-x-x, x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x"}], "references": [{"title": "Fast probabilistic algorithms for hamiltonian circuits and matchings", "author": ["Dana Angluin", "Leslie G Valiant"], "venue": "In Proceedings of the ninth annual ACM symposium on Theory of computing,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 1977}, {"title": "Agnostic active learning", "author": ["Maria-Florina Balcan", "Alina Beygelzimer", "John Langford"], "venue": "Journal of Computer and System Sciences,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2009}, {"title": "Margin based active learning", "author": ["Maria-Florina Balcan", "Andrei Broder", "Tong Zhang"], "venue": "In International Conference on Computational Learning Theory,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2007}, {"title": "Active and passive learning of linear separators under log-concave distributions", "author": ["Maria-Florina Balcan", "Phil Long"], "venue": "In Proceedings of the 26th Conference on Learning Theory,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2013}, {"title": "Importance weighted active learning", "author": ["Alina Beygelzimer", "Sanjoy Dasgupta", "John Langford"], "venue": "In Proceedings of the 26th Annual International Conference on Machine Learning,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2009}, {"title": "Minimax bounds for active learning", "author": ["Rui M Castro", "Robert D Nowak"], "venue": "IEEE Transactions on Information Theory,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2008}, {"title": "Learning unknown graphs", "author": ["Nicolo Cesa-Bianchi", "Claudio Gentile", "Fabio Vitale"], "venue": "In International Conference on Algorithmic Learning Theory,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2009}, {"title": "Worst-case analysis of selective sampling for linear classification", "author": ["Nicolo Cesa-Bianchi", "Claudio Gentile", "Luca Zaniboni"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2006}, {"title": "Improving generalization with active learning", "author": ["David Cohn", "Les Atlas", "Richard Ladner"], "venue": "Machine learning,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 1994}, {"title": "An efficient graph based active learning algorithm with application to nonparametric classification", "author": ["Gautam Dasarathy", "Robert Nowak", "Xiaojin Zhu"], "venue": "In Proceedings of The 28th Conference on Learning Theory,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2015}, {"title": "Analysis of a greedy active learning strategy", "author": ["Sanjoy Dasgupta"], "venue": "In Advances in neural information processing systems,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2004}, {"title": "Coarse sample complexity bounds for active learning", "author": ["Sanjoy Dasgupta"], "venue": "In Advances in neural information processing systems,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2005}, {"title": "Hierarchical sampling for active learning", "author": ["Sanjoy Dasgupta", "Daniel Hsu"], "venue": "In Proceedings of the 25th international conference on Machine learning,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2008}, {"title": "Analysis of perceptron-based active learning", "author": ["Sanjoy Dasgupta", "Adam Tauman Kalai", "Claire Monteleoni"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2009}, {"title": "A general agnostic active learning algorithm", "author": ["Sanjoy Dasgupta", "Claire Monteleoni", "Daniel J Hsu"], "venue": "In Advances in neural information processing systems,", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2007}, {"title": "Selective sampling using the query by committee algorithm", "author": ["Yoav Freund", "H Sebastian Seung", "Eli Shamir", "Naftali Tishby"], "venue": "Machine learning,", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 1997}, {"title": "Query by committee made real", "author": ["Ran Gilad-Bachrach", "Amir Navot", "Naftali Tishby"], "venue": "In Proceedings of the 18th International Conference on Neural Information Processing Systems,", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2005}, {"title": "Near-optimal bayesian active learning with noisy observations", "author": ["Daniel Golovin", "Andreas Krause", "Debajyoti Ray"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2010}, {"title": "Average-case active learning with costs", "author": ["Andrew Guillory", "Jeff Bilmes"], "venue": "In International Conference on Algorithmic Learning Theory,", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2009}, {"title": "A bound on the label complexity of agnostic active learning", "author": ["Steve Hanneke"], "venue": "In Proceedings of the 24th international conference on Machine learning,", "citeRegEx": "20", "shortCiteRegEx": "20", "year": 2007}, {"title": "Probability inequalities for sums of bounded random variables", "author": ["Wassily Hoeffding"], "venue": "Journal of the American statistical association,", "citeRegEx": "21", "shortCiteRegEx": "21", "year": 1963}, {"title": "Hierarchical label queries with data-dependent partitions", "author": ["Samory Kpotufe", "Ruth Urner", "Shai Ben-David"], "venue": "In Proceedings of The 28th Conference on Learning Theory,", "citeRegEx": "22", "shortCiteRegEx": "22", "year": 2015}, {"title": "Beyond disagreement-based agnostic active learning", "author": ["Chicheng Zhang", "Kamalika Chaudhuri"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "23", "shortCiteRegEx": "23", "year": 2014}], "referenceMentions": [{"referenceID": 11, "context": "An exception to this general trend is the aggressive strategy of [12], whose label complexity is known to be optimal in its dependence on a key parameter called the splitting index.", "startOffset": 65, "endOffset": 69}, {"referenceID": 11, "context": "But what is the right notion of size? Dasgupta [12] pointed out that the diameter of the version space is what matters, where the distance between two classifiers is taken to be the fraction of points on which they make different predictions.", "startOffset": 47, "endOffset": 51}, {"referenceID": 11, "context": "Dasgupta [12] pointed out that the label complexity of active learning depends on the underlying distribution, the amount of unlabeled data (since more data means greater potential for highly-informative points), and also the target classifier h\u2217.", "startOffset": 9, "endOffset": 13}, {"referenceID": 6, "context": ") One scheme begins by building a neighborhood graph on the unlabeled data, and propagating queried labels along the edges of this graph [24, 7, 10].", "startOffset": 137, "endOffset": 148}, {"referenceID": 9, "context": ") One scheme begins by building a neighborhood graph on the unlabeled data, and propagating queried labels along the edges of this graph [24, 7, 10].", "startOffset": 137, "endOffset": 148}, {"referenceID": 12, "context": "Another starts with a hierarchical clustering of the data and moves down the tree, sampling at random until it finds clusters that are relatively pure in their labels [13].", "startOffset": 167, "endOffset": 171}, {"referenceID": 5, "context": "The label complexity of such methods have typically be given in terms of smoothness properties of the underlying data distribution [6, 22].", "startOffset": 131, "endOffset": 138}, {"referenceID": 21, "context": "The label complexity of such methods have typically be given in terms of smoothness properties of the underlying data distribution [6, 22].", "startOffset": 131, "endOffset": 138}, {"referenceID": 2, "context": "Another line of work has focused on active learning of linear separators, by querying points close to the current guess at the decision boundary [3, 14, 4].", "startOffset": 145, "endOffset": 155}, {"referenceID": 13, "context": "Another line of work has focused on active learning of linear separators, by querying points close to the current guess at the decision boundary [3, 14, 4].", "startOffset": 145, "endOffset": 155}, {"referenceID": 3, "context": "Another line of work has focused on active learning of linear separators, by querying points close to the current guess at the decision boundary [3, 14, 4].", "startOffset": 145, "endOffset": 155}, {"referenceID": 7, "context": "Interestingly, regret guarantees for online algorithms of this sort can be shown under far weaker conditions [8].", "startOffset": 109, "endOffset": 112}, {"referenceID": 8, "context": "Some of these schemes [9, 15, 5, 2, 23] are fairly mellow in the sense described earlier, using generalization bounds to gauge which labels can be inferred from those obtained so far.", "startOffset": 22, "endOffset": 39}, {"referenceID": 14, "context": "Some of these schemes [9, 15, 5, 2, 23] are fairly mellow in the sense described earlier, using generalization bounds to gauge which labels can be inferred from those obtained so far.", "startOffset": 22, "endOffset": 39}, {"referenceID": 4, "context": "Some of these schemes [9, 15, 5, 2, 23] are fairly mellow in the sense described earlier, using generalization bounds to gauge which labels can be inferred from those obtained so far.", "startOffset": 22, "endOffset": 39}, {"referenceID": 1, "context": "Some of these schemes [9, 15, 5, 2, 23] are fairly mellow in the sense described earlier, using generalization bounds to gauge which labels can be inferred from those obtained so far.", "startOffset": 22, "endOffset": 39}, {"referenceID": 22, "context": "Some of these schemes [9, 15, 5, 2, 23] are fairly mellow in the sense described earlier, using generalization bounds to gauge which labels can be inferred from those obtained so far.", "startOffset": 22, "endOffset": 39}, {"referenceID": 19, "context": "The label complexity of these methods can be bounded in terms of a quantity known as the disagreement coefficient [20].", "startOffset": 114, "endOffset": 118}, {"referenceID": 8, "context": "In the realizable case, the canonical such algorithm is that of [9], henceforth referred to as CAL.", "startOffset": 64, "endOffset": 67}, {"referenceID": 10, "context": "These methods typically aim to shrink the mass of the version space under \u03c0, either greedily and explicitly [11, 19, 18] or implicitly [16].", "startOffset": 108, "endOffset": 120}, {"referenceID": 18, "context": "These methods typically aim to shrink the mass of the version space under \u03c0, either greedily and explicitly [11, 19, 18] or implicitly [16].", "startOffset": 108, "endOffset": 120}, {"referenceID": 17, "context": "These methods typically aim to shrink the mass of the version space under \u03c0, either greedily and explicitly [11, 19, 18] or implicitly [16].", "startOffset": 108, "endOffset": 120}, {"referenceID": 15, "context": "These methods typically aim to shrink the mass of the version space under \u03c0, either greedily and explicitly [11, 19, 18] or implicitly [16].", "startOffset": 135, "endOffset": 139}, {"referenceID": 11, "context": "This motivates using the diameter of the version space as a yardstick, which was first proposed in [12] and is taken up again here.", "startOffset": 99, "endOffset": 103}, {"referenceID": 11, "context": "The following theorem, due to Dasgupta [12], bounds the sample complexity of active learning in terms of the splitting index.", "startOffset": 39, "endOffset": 43}, {"referenceID": 11, "context": "Dasgupta [12] derived the splitting indices for several hypothesis classes, including intervals and homogeneous linear separators.", "startOffset": 9, "endOffset": 13}, {"referenceID": 20, "context": "If we knew the value \u03c1 a priori, then Lemma 4 combined with standard concentration bounds [21, 1] would give us a relatively straightforward procedure to find a good query point: 1.", "startOffset": 90, "endOffset": 97}, {"referenceID": 0, "context": "If we knew the value \u03c1 a priori, then Lemma 4 combined with standard concentration bounds [21, 1] would give us a relatively straightforward procedure to find a good query point: 1.", "startOffset": 90, "endOffset": 97}, {"referenceID": 20, "context": "Then Hoeffding\u2019s inequality [21], combined with Lemma 4, tells us that there exist sequences n, \u03b4n \u2198 0 such that with probability at least 1\u2212 3\u03b4n, the following hold simultaneously:", "startOffset": 28, "endOffset": 32}, {"referenceID": 16, "context": "[17] demonstrated that using samples generated by the hit-and-run Markov chain works well in practice.", "startOffset": 0, "endOffset": 4}], "year": 2017, "abstractText": "To date, the tightest upper and lower-bounds for the active learning of general concept classes have been in terms of a parameter of the learning problem called the splitting index. We provide, for the first time, an efficient algorithm that is able to realize this upper bound, and we empirically demonstrate its good performance.", "creator": "LaTeX with hyperref package"}}}