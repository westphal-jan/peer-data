{"id": "1702.08591", "review": {"conference": "ICML", "VERSION": "v1", "DATE_OF_SUBMISSION": "28-Feb-2017", "title": "The Shattered Gradients Problem: If resnets are the answer, then what is the question?", "abstract": "A long-standing obstacle to progress in deep learning is the problem of vanishing and exploding gradients. The problem has largely been overcome through the introduction of carefully constructed initializations and batch normalization. Nevertheless, architectures incorporating skip-connections such as resnets perform much better than standard feedforward architectures despite well-chosen initialization and batch normalization. In this paper, we identify the shattered gradients problem. Specifically, we show that the correlation between gradients in standard feedforward networks decays exponentially with depth resulting in gradients that resemble white noise. In contrast, the gradients in architectures with skip-connections are far more resistant to shattering decaying sublinearly. Detailed empirical evidence is presented in support of the analysis, on both fully-connected networks and convnets. Finally, we present a new \"looks linear\" (LL) initialization that prevents shattering. Preliminary experiments show the new initialization allows to train very deep networks without the addition of skip-connections.", "histories": [["v1", "Tue, 28 Feb 2017 01:06:13 GMT  (1304kb,D)", "http://arxiv.org/abs/1702.08591v1", "14 pages, 6 figures"]], "COMMENTS": "14 pages, 6 figures", "reviews": [], "SUBJECTS": "cs.NE cs.LG stat.ML", "authors": ["david balduzzi", "marcus frean", "lennox leary", "j p lewis", "kurt wan-duo ma", "brian mcwilliams"], "accepted": true, "id": "1702.08591"}, "pdf": {"name": "1702.08591.pdf", "metadata": {"source": "CRF", "title": "The Shattered Gradients Problem: If resnets are the answer, then what is the question?", "authors": ["David Balduzzi", "Marcus Frean", "Lennox Leary", "JP Lewis", "Kurt Wan-Duo", "Brian McWilliams"], "emails": ["<dbalduzzi@gmail.com>,", "<brian@disneyresearch.com>."], "sections": [{"heading": null, "text": "Specifically, we show that the correlation between gradients in standard feedback networks decreases exponentially with depth, resulting in gradients that resemble white noise. In contrast, gradients in architectures with skip connection are significantly more resistant to breaking sublinears. Detailed empirical evidence is presented to support analysis, both for fully connected networks and for convective networks. Finally, we present a new \"look linear\" (LL) initialization that avoids fragmentation. Initial experiments show that the new initialization makes it possible to train very deep networks without additional skip connections."}, {"heading": "1. Introduction", "text": "Reducing the tendency of gradients to disappear or explode with depth (Hochreiter, 1991; Bengio et al., 1994) was essential for this advance.The combination of careful initialization (Glorot & Bengio, 2010; He et al., 2015) with the normalization of stacks (Ioffe & Szegedy, * authors writing alphabetically 1Victoria University of Wellington, New Zealand 2Disney Research, Zurich, Switzerland; correspondence to: David Balduzzi < dbalduzzi @ gmail.com >, Brian McWilliams < brian @ disneyresearch.com >.2015) bakes two solutions for disappearing / exploding gradients in a single architecture."}, {"heading": "1.1. The Shattered Gradients Problem", "text": "The first step is simply to look at the gradients of the neural networks (1). The gradients are averaged by minibatches depending on the losses and random samples of the data, and they are extremely high dimensional, making several confusion factors and visualizations difficult (but see section 4). Therefore, the model is not intended to be applied to real data. Rather, it is a laboratory where gradients can be isolated and studied. We are interested in how the gradients vary, to what extent the confusion is undermined: 170 2.08 591v 1 [cs.N E] 28 Feb 2017, as a function of input: dfW dx (x x) where x (i)."}, {"heading": "1.2. Outline", "text": "Section 2 shows that the normalization of stacks increases neural efficiency. We examine how the normalization of stacks in feedback and resnet behaves differently and highlight facts relevant to the main outcomes. The main results are in Section 3. They explain why gradients break and how skip connections reduce fragmentation. Evidence suggests a mathematically accessible model: fully networked rectifier networks with the same number of hidden neurons in each layer. Section 4 presents empirical results showing that gradients in convector networks break similarly for real data. It also shows that fragmentation causes the average gradients to decrease with depth via minibatches (relative to the average variance of gradients).Finally, Section 5 proposes the LL-init (\"looks like linear initialization\"), which avoids fragmentation. Preliminary experiments show that the L-200 allows the training of the skiinit without an extreme layer."}, {"heading": "1.3. Related work", "text": "The careful initialization of neural networks has led to a series of performance breakthroughs that (at least) can be traced back to unattended pretraining at Hinton et al. (2006); Bengio et al. (2006). Glorot & Bengio's (2010) finding is that controlling the variance of distributions from which weights are taken makes it possible to control how layers progressively amplify or attenuate the variance of activations and error signals. More recently, Glorot et al. (2015) have refined the approach to take into account rectifiers. Rectifiers effectively halve the variance as they are active during initialization and on average for half of their inputs. Orthogonization of weight matrices can lead to further improvements, albeit with compression costs (Saxe et al., 2014; Mishkin & Matas, 2016). Observing that the weights standards form a random walk was used by Susso & Abbott (2015) to adjust the gains."}, {"heading": "2. Observations on batch normalization", "text": "The effects of batch normalization are examined during initialization (i.e. when it relates to unit variance).We first examine the effects of batch normalization on neuronal activations.Neurons are on average active at half of their inputs, Figure 3, with or without batch normalization. Figure 3 also shows how often neurons are coactive at two inputs. Batch normalization causes neurons to become coactive at 1 4 distinct input pairs, which occurs when activations are decided by unbiased coin-structure flips. Figure 3 also shows how often neurons are coactive at two inputs. Batch normalization causes neurons to become coactive at 1 4 distinct input pairs, which means that activations are decided by unbiased coin-structure flips. Without batch normalization, the coactive portion increases with depth, indicating that neurons are increasingly redundant. Resnet normalization with batch networks does not behave the same as eforward ()."}, {"heading": "3. Analysis", "text": "The most important ideas and results are presented, with the details in Section A3.Perhaps the easiest way to investigate the structure of a random process is to measure the first few moments: the mean, the variance, and the covariance. We examine how the correlation between typical datapoints (defined below) exhibits changes with network structure and depth. Weaker correlations correspond to random variables with mean zeros and finite second moments. The analysis refers to fully connected networks. Expansion to conventions involves (significant) additional accounting. The covariance defines an internal product on the vector space of real-evaluated random variables with mean zeros and finite second moments. It has been shown in Balduzzi et al that the gradients in neural networks are sums of path weights across active paths. The first step in the evidence is the observation that path weights are expressed in relation to the inner product."}, {"heading": "3.1. Feedforward networks", "text": "The first main result is theorem 1 (covariance of gradients in the feedforward networks). Suppose the weights are initialized according to He et al. (2015) with variance \u03c32 = 2N. Thena) The gradient variance at x (i) is C fnn (i) = 1.b) The covariance is C fnn (i, j) = 1 2L. Part (a) restores the observation in He et al. (2015) that setting \u03c32 = 2N maintains the variance across layers in rectifier networks. Part (b) is new. It explains the empirical observation, Figure 2a, that gradients in feedforward networks bleach with depth. Intuitively pale gradients, because the number of paths through the network with depth grows exponentially faster than the proportion of coactive paths, see section A3 for details."}, {"heading": "3.2. Residual networks", "text": "The residual modules introduced in He et al. (2016a) are arexl = xl \u2212 1 + W l\u03c1BN (Vl\u03c1BN (xl \u2212 1), where \u03c1BN (a) = \u03c1 (BN (a))). We analyze the striped variant xl = \u03b1 \u00b7 (xl \u2212 1 + \u03b2 \u00b7 Wl\u03c1BN (xl \u2212 1))) (2), where \u03b1 and \u03b2 are recalculation factors. Declining Vl\u03c1BN makes no significant difference for the analysis. \u03b2rescaling was introduced in Szegedy et al. (2016), where it was observed by \u03b2 [0,1, 0,3] reduces instability. We include \u03b1 for reasons of symmetry. Theorem 2 (covariance of gradients in resnets). Let us consider a rescale deactivated with batch normalization and \u03b1 = \u03b2 = 1. Suppose \u03c32 = 2N as above."}, {"heading": "3.3. Rescaling in Resnets", "text": "A solution for the exploding variance of resnets is the rescaling of layers to \u03b2 = 1 \u221a 2, the Cres \u03b1 = \u221a 2 (i) = 1 and Rres \u03b1 = \u221a 2 (i, j) = (34) Land thus controls the variance, but the correlation between gradients still decreases exponentially with depth. Both theoretical predictions apply empirically. In practice, alpha rescaling is not applied. The effect is dramatic: Theorem 3 (covariance of gradients in resnets with BN and rescaling) and, more recently, the setting \u03b2 [0.1, 0.3] per Szegedy et al. (2016) Resalpha after resalpha after resalpha after resalpha after resalpha after resalpha after resalpha after resalpha after resalpha after resalpha after resalpha resalpha. Under the above assumptions, resalpha resalpha after resalpha resalpha after resalpha after resalpha after resalpha after resalpha after resalpha after a resalpha after resalpha after a resalpha after a resalpha after a resalpha after a resalpha."}, {"heading": "3.4. Highway networks", "text": "The standard road network (Srivastava et al., 2015; Greff et al., 2017) has layers of formalxl = (1 \u2212 T (xl \u2212 1)) \u00b7 xl \u2212 1 + T (xl \u2212 1) \u00b7 H (xl \u2212 1). Let us consider the following modification, where \u04211 and \u04212 are scalars, which are \u042121 + \u03b3 2 = 1: xl = \u03b31 \u00b7 xl \u2212 1 + \u03b32 \u00b7 Wl\u03c1 (xl \u2212 1) The module can be restored by careful selection of \u03b1 and \u03b2 in Equation (2). However, it is worth investigating the correlation itself: Correlation 1 (covariance of slopes in road networks). Under the above assumptions, the variance of slopes CHN\u03b3 (i) = 1; and b) the correlation is RHN\u03b3 (i, j) (2,2)."}, {"heading": "4. Gradients shatter in convnets", "text": "This year we have the opportunity to establish ourselves in the region, \"he said.\" We have to stick to the limits, \"he said.\" We have to stick to the limits, \"he said.\" We have to stick to the limits of our possibilities. \"\" We have to stick to the limits of our possibilities, \"he said.\" We have to stick to the limits of our possibilities, \"he said.\" We have to stick to the limits of our possibilities, \"he said.\" We have to stick to the limits of our possibilities. \"We have to stick to the limits,\" he said. \"We have to stick to the limits.\""}, {"heading": "5. The \u201clooks linear\u201d initialization", "text": "LL-init combines the best of linear and rectifier networks by initializing rectifiers to look linear. Multiple implementations are possible. We use bundled rectifiers or CReLUs (Shang et al., 2016): x 7 \u2192 (\u03c1 (x) \u03c1 (\u2212 x))) The most important observation is that initialization weights with a mirrored block structure are linear outputs (W \u2212 W) \u00b7 (\u03c1 (x) \u03c1 (\u2212 x) = Wx.The output will cease to be linear as soon as weight updates cause the two blocks to diverge. An alternative architecture is based on the PReLU: PReLU:.et. introduced in He et al. (2015)."}, {"heading": "5.1. Experiments", "text": "We investigated the empirical performance of the LL-init on very deep networks. Performance was evaluated on CIFAR-10. We performed a series of proof-of-concept experiments, the aim of which was not to correspond to the state of the art, but rather to investigate whether the LL-init allows training of deeper networks than standard initializations. We compared a CReLU architecture with an orthogonal LL-init against an equivalent CReLU network, resnet and a standardized forward-routed ReLU network. The other networks were initialized according to He et al. (2015) The architectures are thin with the number of filters per layer in the ReLU networks, ranging from 8 on the input layer to 64, see Section A4. Doubling with each reduction in spatial expansion makes it particularly difficult, the slimness of the architecture makes it particularly difficult to expand in depth. The reduction is accomplished by the reduction of the last layer of the magnification, followed by the conversion of the last one of the stripe and the reduction of the last."}, {"heading": "6. Conclusion", "text": "It was shown in Montufar et al. (2014) that the number of linear regions can grow exponentially with depth (but only polynomially with width), so deep neural networks are able to perform much richer mapping than flat ones (Telgarsky, 2016). An underestimated consequence of exponential growth in linear regions is the prevalence of discontinuities in the course of rectifier networks. This paper has identified and analyzed a previously unnoticed problem with course in deep networks: in a randomly initialized network, the gradients of deeper layers are increasingly uncorrelated. Fragmented gradients play havoc with the optimization methods currently in use and may explain the difficulties in forming deep feedforward networks."}, {"heading": "A1. Backprop and Brownian Motion", "text": "Brownian motion is a stochastic process {Bt: t \u2265 0} such that \u2022 B0 = 0 \u2022 (Bt2 \u2212 Bt1) \u0445 N (0, t2 \u2212 t1) for any 0 \u2264 t1 < t2. \u2022 (Bt2 \u2212 Bt1) and (Bt4 \u2212 Bt3) are independent for any 0 \u2264 t1 < t2 \u2264 t3 < t4.The Shattered Gradients Problem \u2022 the sample function t 7 \u2192 Bt (s) is continuous for almost all perspectives. Some important properties of Brownian motion are that \u2022 Bt \u0445 N (0, t)."}, {"heading": "A2. The Karhunen-Loeve theorem", "text": "The covariance function is K (s, t) = Cov (Xs, Xt) = E [XsXt].Define the corresponding integral operator TK: L2 (R) \u2192 L2 (R) asTK (\u03c6) (t) = tically 10K (t, s) \u03c6 (t) dsIf K (t, s) is continuous in t and s. According to Mercer's theorem, the operator TK has the orthonormal basis of the eigenvectors ei (t) with the corresponding eigenvalues \u03bbi. Theorem (Karhunen-Loeve). LetFi = \u0439 10Xtei (t) dtThen E [Fi] = 0, E [FiFj] = 0 for i 6 = j, Var [Fi] = \u03bbi, and Xt = emergi = 1Fiei (t) with uniform convergence in the mean (t) and in comparison to eigenvectors (2) and eigenvectors (12)."}, {"heading": "A3. Details of the Analysis", "text": "A basic tool is the extension of a function in relation to an orthonormal base f (x) = \u2211 k \u03b1kek (x), where the basis < ej (x), \u03b1 (x) > = 1j = k. A classic example is the Fourier expansion; a more recent example is wave analysis. A powerful tool for analyzing random processes based on the same philosophy is the Karhunen-Loeve transformation. The idea is to represent random processes as linear combinations of orthogonal vectors or functions. The main component analysis is a specific case of KarhunenLoeve transformation.The weights of a neural network in initialization are random variables, so we can indexThe Shattered Gradients results as a random process."}, {"heading": "A special case is the variance:", "text": "The mean is zero, because E [W\u03b1] = 0. The crossterms E [W\u03b11 \u00b7 W\u03b2] = 0. The crossterms E [W\u03b11 \u00b7 W\u03b2] = 0. [W\u03b1B\u03b2] = 0. [W2\u03b1] \u00b7 The covariance simplifies the result.A3.2. Gradients are path sumsConsider a network of L + 1 layers number 0, 1. L, where each layer contains neurons. LetsL = xL, 1 + xL, the sum of the outputs of neurons in the last layer. Lemma A2."}, {"heading": "A4. Details on architecture for figure 6", "text": "r-module with 8 filters per downsampling module with 16 filters r \u2212 1 module with 16 filters per downsampling module with 32 filters r \u2212 1 module with 32 filters per downsampling module with 64 filters r \u2212 1 module with 64 filters per downsampling module with 64 filters Layer FC layer flatten for output (width 10)"}], "references": [{"title": "Deep Online Convex Optimization with Gated Games", "author": ["Balduzzi", "David"], "venue": "In arXiv:1604.01952,", "citeRegEx": "Balduzzi and David.,? \\Q2016\\E", "shortCiteRegEx": "Balduzzi and David.", "year": 2016}, {"title": "Kickback cuts Backprop\u2019s red-tape: Biologically plausible credit assignment in neural networks", "author": ["Balduzzi", "David", "Vanchinathan", "Hastagiri", "Buhmann", "Joachim"], "venue": "In AAAI Conference on Artificial Intelligence (AAAI),", "citeRegEx": "Balduzzi et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Balduzzi et al\\.", "year": 2015}, {"title": "Neural Taylor Approximations: Convergence and Exploration in Rectifier Networks", "author": ["Balduzzi", "David", "McWilliams", "Brian", "Butler-Yeoman", "Tony"], "venue": "In arXiv:1611.02345,", "citeRegEx": "Balduzzi et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Balduzzi et al\\.", "year": 2016}, {"title": "Greedy Layer-Wise Training of Deep Networks", "author": ["Y Bengio", "P Lamblin", "D. Popovici", "H. Larochelle"], "venue": "In NIPS,", "citeRegEx": "Bengio et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Bengio et al\\.", "year": 2006}, {"title": "Learning long-term dependencies with gradient descent is difficult", "author": ["Bengio", "Yoshua", "P Simard", "P. Frasconi"], "venue": "IEEE Trans. Neur. Net.,", "citeRegEx": "Bengio et al\\.,? \\Q1994\\E", "shortCiteRegEx": "Bengio et al\\.", "year": 1994}, {"title": "The loss surface of multilayer networks", "author": ["A Choromanska", "M Henaff", "M Mathieu", "G B Arous", "Y. LeCun"], "venue": "In Journal of Machine Learning Research: Workshop and Conference Proceeedings,", "citeRegEx": "Choromanska et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Choromanska et al\\.", "year": 2015}, {"title": "Understanding the difficulty of training deep feedforward neural networks", "author": ["Glorot", "Xavier", "Bengio", "Yoshua"], "venue": "In AISTATS,", "citeRegEx": "Glorot et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Glorot et al\\.", "year": 2010}, {"title": "Highway and Residual Networks learn Unrolled Iterative Estimation", "author": ["Greff", "Klaus", "Srivastava", "Rupesh Kumar", "Schmidhuber", "Juergen"], "venue": "In ICLR,", "citeRegEx": "Greff et al\\.,? \\Q2017\\E", "shortCiteRegEx": "Greff et al\\.", "year": 2017}, {"title": "Identity Matters in Deep Learning", "author": ["Hardt", "Moritz", "Ma", "Tengyu"], "venue": "In ICLR,", "citeRegEx": "Hardt et al\\.,? \\Q2017\\E", "shortCiteRegEx": "Hardt et al\\.", "year": 2017}, {"title": "Delving Deep into Rectifiers: Surpassing Human-Level Performance on ImageNet Classification", "author": ["He", "Kaiming", "Zhang", "Xiangyu", "Ren", "Shaoqing", "Sun", "Jian"], "venue": "In ICCV,", "citeRegEx": "He et al\\.,? \\Q2015\\E", "shortCiteRegEx": "He et al\\.", "year": 2015}, {"title": "Identity Mappings in Deep Residual Networks", "author": ["He", "Kaiming", "Zhang", "Xiangyu", "Ren", "Shaoqing", "Sun", "Jian"], "venue": "In ECCV,", "citeRegEx": "He et al\\.,? \\Q2016\\E", "shortCiteRegEx": "He et al\\.", "year": 2016}, {"title": "Deep Residual Learning for Image Recognition", "author": ["He", "Kaiming", "Zhang", "Xiangyu", "Ren", "Shaoqing", "Sun", "Jian"], "venue": "In CVPR,", "citeRegEx": "He et al\\.,? \\Q2016\\E", "shortCiteRegEx": "He et al\\.", "year": 2016}, {"title": "A Fast Learning Algorithm for Deep Belief Nets", "author": ["GE Hinton", "S Osindero", "Teh", "Y W"], "venue": "Neural Computation,", "citeRegEx": "Hinton et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Hinton et al\\.", "year": 2006}, {"title": "Untersuchungen zu dynamischen neuronalen Netzen", "author": ["Hochreiter", "Sepp"], "venue": "Master\u2019s thesis, Technische Universita\u0308t Mu\u0308nchen,", "citeRegEx": "Hochreiter and Sepp.,? \\Q1991\\E", "shortCiteRegEx": "Hochreiter and Sepp.", "year": 1991}, {"title": "Batch normalization: Accelerating deep network training by reducing internal covariate shift", "author": ["Ioffe", "Sergey", "Szegedy", "Christian"], "venue": "In ICML,", "citeRegEx": "Ioffe et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Ioffe et al\\.", "year": 2015}, {"title": "Adam: A method for stochastic optimization", "author": ["Kingma", "Diederik P", "Ba", "Jimmy Lei"], "venue": "In ICLR,", "citeRegEx": "Kingma et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Kingma et al\\.", "year": 2015}, {"title": "Imagenet classification with deep convolutional neural networks", "author": ["A Krizhevsky", "I Sutskever", "Hinton", "G E"], "venue": "In Advances in Neural Information Processing Systems (NIPS),", "citeRegEx": "Krizhevsky et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Krizhevsky et al\\.", "year": 2012}, {"title": "All you need is a good init", "author": ["D Mishkin", "J. Matas"], "venue": "In ICLR,", "citeRegEx": "Mishkin and Matas,? \\Q2016\\E", "shortCiteRegEx": "Mishkin and Matas", "year": 2016}, {"title": "On the number of linear regions of deep neural networks", "author": ["Montufar", "Guido F", "Pascanu", "Razvan", "Cho", "Kyunghyun", "Bengio", "Yoshua"], "venue": "In Advances in neural information processing systems,", "citeRegEx": "Montufar et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Montufar et al\\.", "year": 2014}, {"title": "Skip Connections as Effective SymmetryBreaking", "author": ["Orhan", "A Emin"], "venue": "In arXiv:1701.09175,", "citeRegEx": "Orhan and Emin.,? \\Q2017\\E", "shortCiteRegEx": "Orhan and Emin.", "year": 2017}, {"title": "Exact solutions to the nonlinear dynamics of learning in deep linear neural networks", "author": ["Saxe", "Andrew M", "McClelland", "James L", "Ganguli", "Surya"], "venue": "In ICLR,", "citeRegEx": "Saxe et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Saxe et al\\.", "year": 2014}, {"title": "Understanding and Improving Convolutional Neural Networks via Concatenated Rectified Linear Units", "author": ["Shang", "Wenling", "Sohn", "Kihyuk", "Almeida", "Diogo", "Lee", "Honglak"], "venue": null, "citeRegEx": "Shang et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Shang et al\\.", "year": 2016}, {"title": "Random Walk Initialization for Training Very Deep Feedforward Networks", "author": ["Sussillo", "David", "Abbott", "L F"], "venue": "In ICLR,", "citeRegEx": "Sussillo et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Sussillo et al\\.", "year": 2015}, {"title": "On the importance of initialization and momentum in deep learning", "author": ["Sutskever", "Ilya", "Martens", "James", "Dahl", "George", "Hinton", "Geoffrey"], "venue": "In Proceedings of the 30th International Conference on Machine Learning", "citeRegEx": "Sutskever et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Sutskever et al\\.", "year": 2013}, {"title": "Going Deeper With Convolutions", "author": ["Szegedy", "Christian", "Liu", "Wei", "Jia", "Yangqing", "Sermanet", "Pierre", "Reed", "Scott", "Anguelov", "Dragomir", "Erhan", "Dumitru", "Vanhoucke", "Vincent", "Rabinovich", "Andrew"], "venue": "In CVPR,", "citeRegEx": "Szegedy et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Szegedy et al\\.", "year": 2015}, {"title": "Inception-v4, Inception-ResNet and the Impact of Residual Connections on Learning", "author": ["Szegedy", "Christian", "Ioffe", "Sergey", "Vanhoucke", "Vincent"], "venue": "In arXiv:1602.07261,", "citeRegEx": "Szegedy et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Szegedy et al\\.", "year": 2016}, {"title": "Benefits of depth in neural networks", "author": ["Telgarsky", "Matus"], "venue": "In COLT,", "citeRegEx": "Telgarsky and Matus.,? \\Q2016\\E", "shortCiteRegEx": "Telgarsky and Matus.", "year": 2016}, {"title": "Residual Networks Behave Like Ensembles of Relatively Shallow Networks", "author": ["Veit", "Andreas", "Wilber", "Michael J", "Belongie", "Serge"], "venue": "In NIPS,", "citeRegEx": "Veit et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Veit et al\\.", "year": 2016}, {"title": "Introduction to the non-asymptotic analysis of random matrices", "author": ["Vershynin", "Roman"], "venue": "In Compressed sensing,", "citeRegEx": "Vershynin and Roman.,? \\Q2012\\E", "shortCiteRegEx": "Vershynin and Roman.", "year": 2012}], "referenceMentions": [{"referenceID": 16, "context": "Introduction Deep neural networks have achieved outstanding performance (Krizhevsky et al., 2012; Szegedy et al., 2015; He et al., 2016b).", "startOffset": 72, "endOffset": 137}, {"referenceID": 24, "context": "Introduction Deep neural networks have achieved outstanding performance (Krizhevsky et al., 2012; Szegedy et al., 2015; He et al., 2016b).", "startOffset": 72, "endOffset": 137}, {"referenceID": 4, "context": "Reducing the tendency of gradients to vanish or explode with depth (Hochreiter, 1991; Bengio et al., 1994) has been essential to this progress.", "startOffset": 67, "endOffset": 106}, {"referenceID": 9, "context": "Combining careful initialization (Glorot & Bengio, 2010; He et al., 2015) with batch normalization (Ioffe & Szegedy, Authors listed alphabetically Victoria University of Wellington, New Zealand Disney Research, Z\u00fcrich, Switzerland.", "startOffset": 33, "endOffset": 73}, {"referenceID": 23, "context": "Shattered gradients undermine the effectiveness of algorithms that assume gradients at nearby points are similar such as momentum-based and accelerated methods (Sutskever et al., 2013; Balduzzi et al., 2016).", "startOffset": 160, "endOffset": 207}, {"referenceID": 2, "context": "Shattered gradients undermine the effectiveness of algorithms that assume gradients at nearby points are similar such as momentum-based and accelerated methods (Sutskever et al., 2013; Balduzzi et al., 2016).", "startOffset": 160, "endOffset": 207}, {"referenceID": 7, "context": "Introducing skip-connections allows much deeper networks to be trained (Srivastava et al., 2015; He et al., 2016b;a; Greff et al., 2017).", "startOffset": 71, "endOffset": 136}, {"referenceID": 25, "context": "Figure 2c shows the dramatic effect of recently proposed \u03b2-rescaling (Szegedy et al., 2016): the ACF of even the 50 layer network resemble brown-noise.", "startOffset": 69, "endOffset": 91}, {"referenceID": 20, "context": "Orthogonalizing weight matrices can yield further improvements albeit at a computational cost (Saxe et al., 2014; Mishkin & Matas, 2016).", "startOffset": 94, "endOffset": 136}, {"referenceID": 7, "context": "Related work Carefully initializing neural networks has led to a series of performance breakthroughs dating back (at least) to the unsupervised pretraining in Hinton et al. (2006); Bengio et al.", "startOffset": 159, "endOffset": 180}, {"referenceID": 3, "context": "(2006); Bengio et al. (2006). The insight of Glorot & Bengio (2010) is that controlling the variance of the distributions from which weights are sampled allows to control how layers progressively amplify or dampen the variance of activations and error signals.", "startOffset": 8, "endOffset": 29}, {"referenceID": 3, "context": "(2006); Bengio et al. (2006). The insight of Glorot & Bengio (2010) is that controlling the variance of the distributions from which weights are sampled allows to control how layers progressively amplify or dampen the variance of activations and error signals.", "startOffset": 8, "endOffset": 68}, {"referenceID": 3, "context": "(2006); Bengio et al. (2006). The insight of Glorot & Bengio (2010) is that controlling the variance of the distributions from which weights are sampled allows to control how layers progressively amplify or dampen the variance of activations and error signals. More recently, He et al. (2015) refined the approach to take rectifiers into account.", "startOffset": 8, "endOffset": 293}, {"referenceID": 3, "context": "(2006); Bengio et al. (2006). The insight of Glorot & Bengio (2010) is that controlling the variance of the distributions from which weights are sampled allows to control how layers progressively amplify or dampen the variance of activations and error signals. More recently, He et al. (2015) refined the approach to take rectifiers into account. Rectifiers effectively halve the variance since, at initialization and on average, they are active for half their inputs. Orthogonalizing weight matrices can yield further improvements albeit at a computational cost (Saxe et al., 2014; Mishkin & Matas, 2016). The observation that the norms of weights form a random walk was used by Sussillo & Abbott (2015) to tune the gains of neurons.", "startOffset": 8, "endOffset": 705}, {"referenceID": 3, "context": "(2006); Bengio et al. (2006). The insight of Glorot & Bengio (2010) is that controlling the variance of the distributions from which weights are sampled allows to control how layers progressively amplify or dampen the variance of activations and error signals. More recently, He et al. (2015) refined the approach to take rectifiers into account. Rectifiers effectively halve the variance since, at initialization and on average, they are active for half their inputs. Orthogonalizing weight matrices can yield further improvements albeit at a computational cost (Saxe et al., 2014; Mishkin & Matas, 2016). The observation that the norms of weights form a random walk was used by Sussillo & Abbott (2015) to tune the gains of neurons. In short, it has proven useful to treat weights and gradients as random variables, and carefully examine their effect on the variance of the signals propagated through the network. This paper presents a more detailed analysis that considers correlations between gradients at different datapoints. The closest work to ours is Veit et al. (2016), which shows resnets behave like ensembles of shallow networks.", "startOffset": 8, "endOffset": 1079}, {"referenceID": 1, "context": "It was shown in Balduzzi et al. (2015); Balduzzi (2016) that the gradients in neural nets are sums of path-weights over active paths.", "startOffset": 16, "endOffset": 39}, {"referenceID": 1, "context": "It was shown in Balduzzi et al. (2015); Balduzzi (2016) that the gradients in neural nets are sums of path-weights over active paths.", "startOffset": 16, "endOffset": 56}, {"referenceID": 8, "context": "The initialization in He et al. (2015) assumes datapoints activate half the neurons per layer.", "startOffset": 22, "endOffset": 39}, {"referenceID": 5, "context": "The assumption on co-activations is implied by (and so weaker than) the assumption in Choromanska et al. (2015) that activations are Bernoulli random variables independent of the inputs.", "startOffset": 86, "endOffset": 112}, {"referenceID": 9, "context": "Suppose weights are initialized with variance \u03c3 = 2 N following He et al. (2015). Then a) The variance of the gradient at x is C fnn(i) = 1.", "startOffset": 64, "endOffset": 81}, {"referenceID": 9, "context": "Part (a) recovers the observation in He et al. (2015) that setting \u03c3 = 2 N preserves the variance across layers in rectifier networks.", "startOffset": 37, "endOffset": 54}, {"referenceID": 9, "context": "Residual networks The residual modules introduced in He et al. (2016a) are", "startOffset": 53, "endOffset": 71}, {"referenceID": 24, "context": "The \u03b2rescaling was introduced in Szegedy et al. (2016) where it was observed setting \u03b2 \u2208 [0.", "startOffset": 33, "endOffset": 55}, {"referenceID": 24, "context": "3] per Szegedy et al. (2016). The effect is dramatic: Theorem 3 (covariance of gradients in resnets with BN and rescaling).", "startOffset": 7, "endOffset": 29}, {"referenceID": 7, "context": "Highway networks The standard highway network (Srivastava et al., 2015; Greff et al., 2017) has layers of the form", "startOffset": 46, "endOffset": 91}, {"referenceID": 9, "context": "Using a slight modification of the \u201cbottleneck\u201d architecture in He et al. (2016a), we introduce one skip-connection for every two convolutional layers and both network architectures use batch normalization.", "startOffset": 64, "endOffset": 82}, {"referenceID": 21, "context": "We use concatenated rectifiers or CReLUs (Shang et al., 2016):", "startOffset": 41, "endOffset": 61}, {"referenceID": 9, "context": "An alternative architecture is based on the PReLU introduced in He et al. (2015):", "startOffset": 64, "endOffset": 81}, {"referenceID": 20, "context": "A detailed analysis of learning in linear neural networks by Saxe et al. (2014) showed, theoretically and experimentally, that arbitrarily deep linear networks can be trained when initialized with orthogonal weights.", "startOffset": 61, "endOffset": 80}, {"referenceID": 9, "context": "The other networks were initialized according to He et al. (2015). The architectures are thin with the number of filters per layer in the ReLU networks ranging from 8 at the input layer to 64, see section A4.", "startOffset": 49, "endOffset": 66}, {"referenceID": 9, "context": "The other networks were initialized according to He et al. (2015). The architectures are thin with the number of filters per layer in the ReLU networks ranging from 8 at the input layer to 64, see section A4. Doubling with each spatial extent reduction. The thinness of the architecture makes it particularly difficult for gradients to propagate at high depth. The reduction is performed by convolutional layers with strides of 2, and following the last reduction the representation is passed to a fully connected layer with 10 neurons for classification. The numbers of filters per layer of the CReLU models were adjusted by a factor of 1/ \u221a 2 to achieve parameter parity with the ReLU models. The Resnet version of the model is the same as the basic ReLU model with skip-connections after every two modules following He et al. (2016a). Updates were performed with Adam (Kingma & Ba, 2015).", "startOffset": 49, "endOffset": 837}, {"referenceID": 18, "context": "It was shown in Montufar et al. (2014) that the number of linear regions can grow exponentially with depth (but only polynomially with width).", "startOffset": 16, "endOffset": 39}], "year": 2017, "abstractText": "A long-standing obstacle to progress in deep learning is the problem of vanishing and exploding gradients. The problem has largely been overcome through the introduction of carefully constructed initializations and batch normalization. Nevertheless, architectures incorporating skip-connections such as resnets perform much better than standard feedforward architectures despite well-chosen initialization and batch normalization. In this paper, we identify the shattered gradients problem. Specifically, we show that the correlation between gradients in standard feedforward networks decays exponentially with depth resulting in gradients that resemble white noise. In contrast, the gradients in architectures with skipconnections are far more resistant to shattering decaying sublinearly. Detailed empirical evidence is presented in support of the analysis, on both fully-connected networks and convnets. Finally, we present a new \u201clooks linear\u201d (LL) initialization that prevents shattering. Preliminary experiments show the new initialization allows to train very deep networks without the addition of skip-connections.", "creator": "LaTeX with hyperref package"}}}