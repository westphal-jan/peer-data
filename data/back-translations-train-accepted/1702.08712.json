{"id": "1702.08712", "review": {"conference": "ICML", "VERSION": "v1", "DATE_OF_SUBMISSION": "28-Feb-2017", "title": "Algorithmic Stability and Hypothesis Complexity", "abstract": "We introduce a notion of algorithmic stability of learning algorithms---that we term hypothesis stability---that captures stability of the hypothesis output by the learning algorithm in the normed space of functions from which hypotheses are selected. The main result of the paper bounds the generalization error of any learning algorithm in terms of its hypothesis stability. The bounds are based on martingale inequalities in the Banach space to which the hypotheses belong. We apply the general bounds to bound the performance of some learning algorithms based on empirical risk minimization and stochastic gradient descent.", "histories": [["v1", "Tue, 28 Feb 2017 09:39:03 GMT  (12kb)", "http://arxiv.org/abs/1702.08712v1", null], ["v2", "Thu, 3 Aug 2017 11:45:15 GMT  (38kb)", "http://arxiv.org/abs/1702.08712v2", null]], "reviews": [], "SUBJECTS": "stat.ML cs.LG", "authors": ["tongliang liu", "g\u00e1bor lugosi", "gergely neu", "dacheng tao"], "accepted": true, "id": "1702.08712"}, "pdf": {"name": "1702.08712.pdf", "metadata": {"source": "CRF", "title": null, "authors": ["Tongliang Liu", "G\u00e1bor Lugosi", "Gergely Neu", "Dacheng Tao"], "emails": ["tliang.liu@gmail.com,", "dacheng.tao@sydney.edu.au", "bor.lugosi@upf.edu", "gergely.neu@gmail.com"], "sections": [{"heading": null, "text": "ar Xiv: 170 2.08 712v 1 [stat.ML] 2 8We introduce a concept of the algorithmic stability of learning algorithms - which we call hypotheses stability - that captures the stability of the hypotheses output by the learning algorithm in the normalized functional space from which the hypotheses are selected. The main result of the paper limits the generalization error of any learning algorithm with regard to its hypotheses stability. The limits are based on marginal inequalities in the Banach space to which the hypotheses belong. We apply the general limits to tie the performance of some learning algorithms based on empirical risk minimization and stochastic gradient decrease."}, {"heading": "1 Introduction", "text": "This year, it is time for us to set out to find a solution that paves the way to the future."}, {"heading": "2 Algorithmic Stability and Hypothesis Class", "text": "We look at the classic statistical learning problem, where the value of a real random variable Y must be predicted on the basis of observing another random variable X. < S is a training sample of n i.i.d. pairs of random variables Z1 = (X1, Y1),.., Zn = (Xn, Yn), drawn from a fixed distribution P on a setZ = X \u00b7 Y, where X is the so-called attribute space. We focus on linear prediction problems, that is, if h (x) is a linear function theory of x. We write h (x) = < h, x >. In other words, we assume that the attribute space X is the algebraic dual of Banach space."}, {"heading": "3 Algorithmic Rademacher Complexity and General-", "text": "The result of the generalization of Lemma 1 justifies the following definition of the \"algorithmic hypotheses category\": < Type hS focuses on their expectations. < Type hS focuses on their expectations. < Type hS focuses on their complexity and not on the entire hypotheses category H. Observation can lead to significantly improved performance guarantees. < Type hB (algorithmic hypotheses category). For a sample size n and confidence parameters. < Type r = r = r (n,) = DD (n) 2n log (2 / 2) and defines the algorithmic hypotheses category of a stable learning algorithm category."}, {"heading": "4 Applications", "text": "Various learning algorithms have been shown to have some stability. We refer the reader to Devroye and Wagner [1979], Lugosi and Pawlak [1994], Bousquet and Elisseeff [2002], Zhang [2003], Wibisono et al. [2009], Hardt et al. [2015], Liu et al. [2017] for such examples, including stochastic gradient parentage methods, empirical risk minimization, and non-parametric learning algorithms such as k-next neighbor rules and kernel regression."}, {"heading": "4.1 Empirical Risk Minimization", "text": "Regularized empirical risk mitigation is known to be uniformly stable [Bousquet and Elisseeff, 2002]. Here we consider regulated empirical risk mitigation (RERM) algorithms of the following forms. e empirical risk (or objective function) of the RERM is formulated as RS (h) = 1nn \u2211 i = 1 (h, Xi) + n (h), where N: h, H 7 \u2192 N (h), R + is a convex function. Its correspondingly expected counterpart is defined as R\u03bb (h) = E (h, X) \u2212 fictive ability i (h).Bousquet and Elisseeff [2002] have proven that 2-regulated learning algorithms are uniformly stable."}, {"heading": "4.2 Stochastic Gradient Descent", "text": "The results of this study are based on the assumptions that the loss function applies to both convex and non-convex learning problems and provides insights into why SGD performs well in practice in this section, especially for deep learning algorithms. The results are based on the assumptions that the loss function can be avoided for both convex and non-convex learning effects in general Hilbert spaces. In this section we assume that the d-dimensional Euclidean function (h, \u00b7) s-smooth when defined for all h derivatives in general Hilbert spaces."}, {"heading": "5 Conlusion", "text": "We have proposed a general probabilistic framework for using local estimates of the complexity of the hypotheses class to achieve rapid convergence rates for stable learning algorithms. Specifically, we have defined the algorithmic hypotheses class by observing that the output of stable learning algorithms is concentrated around EhS. Rademacher complexity defined on the hypotheses class then converges at the same rate as that of uniform hypotheses stability in the Hilbert space, which are for different learning algorithms of order O (1 / n), such as empirical risk minimization and stochastic gradient descent. We have derived rapid convergence rates of order O (1 / n) for their deformed generalization errors. In contrast to previously published guarantees of similar flavors, our limits are most likely to hold, rather than just in this hypotheses descent."}], "references": [{"title": "Rademacher and gaussian complexities: Risk bounds and structural results", "author": ["Peter L Bartle", "Shahar Mendelson"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Bartle\u008a and Mendelson.,? \\Q2003\\E", "shortCiteRegEx": "Bartle\u008a and Mendelson.", "year": 2003}, {"title": "Local rademacher complexities", "author": ["Peter L Bartle", "Olivier Bousquet", "ShaharMendelson"], "venue": "Annals of Statistics,", "citeRegEx": "Bartle\u008a et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Bartle\u008a et al\\.", "year": 2005}, {"title": "Perturbation analysis of optimization problems", "author": ["J Fr\u00e9d\u00e9ric Bonnans", "Alexander Shapiro"], "venue": "Springer Science & Business Media,", "citeRegEx": "Bonnans and Shapiro.,? \\Q2013\\E", "shortCiteRegEx": "Bonnans and Shapiro.", "year": 2013}, {"title": "Concentration inequalities: A nonasymptotic theory of independence", "author": ["St\u00e9phane Boucheron", "G\u00e1bor Lugosi", "Pascal Massart"], "venue": "OUP Oxford,", "citeRegEx": "Boucheron et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Boucheron et al\\.", "year": 2013}, {"title": "Stability and generalization", "author": ["Olivier Bousquet", "Andr\u00e9 Elisseeff"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Bousquet and Elisseeff.,? \\Q2002\\E", "shortCiteRegEx": "Bousquet and Elisseeff.", "year": 2002}, {"title": "Distribution-free inequalities for the deleted and holdout error estimates. Information \u008aeory", "author": ["Luc Devroye", "Terry J Wagner"], "venue": "IEEE Transactions on,", "citeRegEx": "Devroye and Wagner.,? \\Q1979\\E", "shortCiteRegEx": "Devroye and Wagner.", "year": 1979}, {"title": "Train faster, generalize be\u008aer: Stability of stochastic gradient descent", "author": ["Moritz Hardt", "Benjamin Recht", "Yoram Singer"], "venue": "arXiv preprint arXiv:1509.01240,", "citeRegEx": "Hardt et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Hardt et al\\.", "year": 2015}, {"title": "Probability in Banach Spaces: isoperimetry and processes", "author": ["Michel Ledoux", "Michel Talagrand"], "venue": "Springer Science & Business Media,", "citeRegEx": "Ledoux and Talagrand.,? \\Q2013\\E", "shortCiteRegEx": "Ledoux and Talagrand.", "year": 2013}, {"title": "Algorithm-dependent generalization bounds for multi-task learning", "author": ["Tongliang Liu", "DachengTao", "Mingli Song", "Stephen J. Maybank"], "venue": "IEEE Transactions on Pa\u0088ern Analysis and Machine Intelligence,", "citeRegEx": "Liu et al\\.,? \\Q2017\\E", "shortCiteRegEx": "Liu et al\\.", "year": 2017}, {"title": "On the posterior-probability estimate of the error rate of nonparametric classification rules. Information \u008aeory", "author": ["G\u00e1bor Lugosi", "Miroslaw Pawlak"], "venue": "IEEE Transactions on,", "citeRegEx": "Lugosi and Pawlak.,? \\Q1994\\E", "shortCiteRegEx": "Lugosi and Pawlak.", "year": 1994}, {"title": "Learning theory: stability is sufficient for generalization and necessary and sufficient for consistency of empirical risk minimization", "author": ["Sayan Mukherjee", "Partha Niyogi", "Tomaso Poggio", "Ryan Ri\u0088in"], "venue": "Advances in Computational Mathematics,", "citeRegEx": "Mukherjee et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Mukherjee et al\\.", "year": 2006}, {"title": "Optimum bounds for the distributions of martingales in Banach spaces", "author": ["Iosif Pinelis"], "venue": "\u008ae Annals of Probability,", "citeRegEx": "Pinelis.,? \\Q1994\\E", "shortCiteRegEx": "Pinelis.", "year": 1994}, {"title": "Martingales in Banach spaces (in connection with type and cotype)", "author": ["Gilles Pisier"], "venue": "IHP course notes,", "citeRegEx": "Pisier.,? \\Q2011\\E", "shortCiteRegEx": "Pisier.", "year": 2011}, {"title": "On equivalence of martingale tail bounds and deterministic regret inequalities", "author": ["Alexander Rakhlin", "Karthik Sridharan"], "venue": "arXiv preprint arXiv:1510.03925,", "citeRegEx": "Rakhlin and Sridharan.,? \\Q2015\\E", "shortCiteRegEx": "Rakhlin and Sridharan.", "year": 2015}, {"title": "Learnability, stability and uniform convergence", "author": ["Shai Shalev-Shwartz", "Ohad Shamir", "Nathan Srebro", "Karthik Sridharan"], "venue": "Journal ofMachine Learning Research,", "citeRegEx": "Shalev.Shwartz et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Shalev.Shwartz et al\\.", "year": 2010}, {"title": "Sufficient conditions for uniform stability of regularization algorithms", "author": ["Andre Wibisono", "Lorenzo Rosasco", "Tomaso Poggio"], "venue": "Techincal Report MIT-CSAIL-TR-2009-060,", "citeRegEx": "Wibisono et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Wibisono et al\\.", "year": 2009}, {"title": "Leave-one-out bounds for kernel methods", "author": ["Tong Zhang"], "venue": "Neural Computation,", "citeRegEx": "Zhang.,? \\Q2003\\E", "shortCiteRegEx": "Zhang.", "year": 2003}], "referenceMentions": [{"referenceID": 2, "context": "Intuitively, a learning algorithm is said to be stable if slight perturbations in the training data result in small changes in the output of the algorithm, and these changes vanish as the data set grows bigger and bigger [Bonnans and Shapiro, 2013].", "startOffset": 221, "endOffset": 248}, {"referenceID": 2, "context": "Intuitively, a learning algorithm is said to be stable if slight perturbations in the training data result in small changes in the output of the algorithm, and these changes vanish as the data set grows bigger and bigger [Bonnans and Shapiro, 2013]. For example, Devroye and Wagner [1979], Lugosi and Pawlak [1994], and Zhang [2003] showed that several non-parametric learning algorithms are stable; Bousquet and Elisseeff [2002] proved that l2 regularized learning algorithms are uniformly stable; Wibisono et al.", "startOffset": 222, "endOffset": 289}, {"referenceID": 2, "context": "Intuitively, a learning algorithm is said to be stable if slight perturbations in the training data result in small changes in the output of the algorithm, and these changes vanish as the data set grows bigger and bigger [Bonnans and Shapiro, 2013]. For example, Devroye and Wagner [1979], Lugosi and Pawlak [1994], and Zhang [2003] showed that several non-parametric learning algorithms are stable; Bousquet and Elisseeff [2002] proved that l2 regularized learning algorithms are uniformly stable; Wibisono et al.", "startOffset": 222, "endOffset": 315}, {"referenceID": 2, "context": "Intuitively, a learning algorithm is said to be stable if slight perturbations in the training data result in small changes in the output of the algorithm, and these changes vanish as the data set grows bigger and bigger [Bonnans and Shapiro, 2013]. For example, Devroye and Wagner [1979], Lugosi and Pawlak [1994], and Zhang [2003] showed that several non-parametric learning algorithms are stable; Bousquet and Elisseeff [2002] proved that l2 regularized learning algorithms are uniformly stable; Wibisono et al.", "startOffset": 222, "endOffset": 333}, {"referenceID": 2, "context": "Intuitively, a learning algorithm is said to be stable if slight perturbations in the training data result in small changes in the output of the algorithm, and these changes vanish as the data set grows bigger and bigger [Bonnans and Shapiro, 2013]. For example, Devroye and Wagner [1979], Lugosi and Pawlak [1994], and Zhang [2003] showed that several non-parametric learning algorithms are stable; Bousquet and Elisseeff [2002] proved that l2 regularized learning algorithms are uniformly stable; Wibisono et al.", "startOffset": 222, "endOffset": 430}, {"referenceID": 2, "context": "Intuitively, a learning algorithm is said to be stable if slight perturbations in the training data result in small changes in the output of the algorithm, and these changes vanish as the data set grows bigger and bigger [Bonnans and Shapiro, 2013]. For example, Devroye and Wagner [1979], Lugosi and Pawlak [1994], and Zhang [2003] showed that several non-parametric learning algorithms are stable; Bousquet and Elisseeff [2002] proved that l2 regularized learning algorithms are uniformly stable; Wibisono et al. [2009] generalized Bousquet and Elisseeff\u2019s results and proved that regularized learning algorithmswith strongly convex penalty functions on bounded domains, e.", "startOffset": 222, "endOffset": 522}, {"referenceID": 2, "context": "Intuitively, a learning algorithm is said to be stable if slight perturbations in the training data result in small changes in the output of the algorithm, and these changes vanish as the data set grows bigger and bigger [Bonnans and Shapiro, 2013]. For example, Devroye and Wagner [1979], Lugosi and Pawlak [1994], and Zhang [2003] showed that several non-parametric learning algorithms are stable; Bousquet and Elisseeff [2002] proved that l2 regularized learning algorithms are uniformly stable; Wibisono et al. [2009] generalized Bousquet and Elisseeff\u2019s results and proved that regularized learning algorithmswith strongly convex penalty functions on bounded domains, e.g., lp regularized learning algorithms for 1 < p \u2264 2, are also uniformly stable; Hardt et al. [2015] showed that parametric models trained by stochastic gradient descent algorithms are uniformly stable; and Liu et al.", "startOffset": 222, "endOffset": 776}, {"referenceID": 2, "context": "Intuitively, a learning algorithm is said to be stable if slight perturbations in the training data result in small changes in the output of the algorithm, and these changes vanish as the data set grows bigger and bigger [Bonnans and Shapiro, 2013]. For example, Devroye and Wagner [1979], Lugosi and Pawlak [1994], and Zhang [2003] showed that several non-parametric learning algorithms are stable; Bousquet and Elisseeff [2002] proved that l2 regularized learning algorithms are uniformly stable; Wibisono et al. [2009] generalized Bousquet and Elisseeff\u2019s results and proved that regularized learning algorithmswith strongly convex penalty functions on bounded domains, e.g., lp regularized learning algorithms for 1 < p \u2264 2, are also uniformly stable; Hardt et al. [2015] showed that parametric models trained by stochastic gradient descent algorithms are uniformly stable; and Liu et al. [2017] proved that tasks in multi-task learning can act as regularizers and that multi-task learning in a very general se\u008aing will therefore be uniformly stable under mild assumptions.", "startOffset": 222, "endOffset": 900}, {"referenceID": 2, "context": "Intuitively, a learning algorithm is said to be stable if slight perturbations in the training data result in small changes in the output of the algorithm, and these changes vanish as the data set grows bigger and bigger [Bonnans and Shapiro, 2013]. For example, Devroye and Wagner [1979], Lugosi and Pawlak [1994], and Zhang [2003] showed that several non-parametric learning algorithms are stable; Bousquet and Elisseeff [2002] proved that l2 regularized learning algorithms are uniformly stable; Wibisono et al. [2009] generalized Bousquet and Elisseeff\u2019s results and proved that regularized learning algorithmswith strongly convex penalty functions on bounded domains, e.g., lp regularized learning algorithms for 1 < p \u2264 2, are also uniformly stable; Hardt et al. [2015] showed that parametric models trained by stochastic gradient descent algorithms are uniformly stable; and Liu et al. [2017] proved that tasks in multi-task learning can act as regularizers and that multi-task learning in a very general se\u008aing will therefore be uniformly stable under mild assumptions. \u008ce notion of algorithmic stability has been an important tool in deriving theoretical guarantees of the generalization abilities of learning algorithms. Various notions of stability have been introduced and have been exploited to derive generalization bounds. For some examples, Mukherjee et al. [2006] proved that a statistical form of leave-oneout stability is a sufficient and necessary condition for the generalization and learnability of empirical risk minimization learning algorithms; Shalev-Shwartz et al.", "startOffset": 222, "endOffset": 1381}, {"referenceID": 2, "context": "Intuitively, a learning algorithm is said to be stable if slight perturbations in the training data result in small changes in the output of the algorithm, and these changes vanish as the data set grows bigger and bigger [Bonnans and Shapiro, 2013]. For example, Devroye and Wagner [1979], Lugosi and Pawlak [1994], and Zhang [2003] showed that several non-parametric learning algorithms are stable; Bousquet and Elisseeff [2002] proved that l2 regularized learning algorithms are uniformly stable; Wibisono et al. [2009] generalized Bousquet and Elisseeff\u2019s results and proved that regularized learning algorithmswith strongly convex penalty functions on bounded domains, e.g., lp regularized learning algorithms for 1 < p \u2264 2, are also uniformly stable; Hardt et al. [2015] showed that parametric models trained by stochastic gradient descent algorithms are uniformly stable; and Liu et al. [2017] proved that tasks in multi-task learning can act as regularizers and that multi-task learning in a very general se\u008aing will therefore be uniformly stable under mild assumptions. \u008ce notion of algorithmic stability has been an important tool in deriving theoretical guarantees of the generalization abilities of learning algorithms. Various notions of stability have been introduced and have been exploited to derive generalization bounds. For some examples, Mukherjee et al. [2006] proved that a statistical form of leave-oneout stability is a sufficient and necessary condition for the generalization and learnability of empirical risk minimization learning algorithms; Shalev-Shwartz et al. [2010] defined a weaker notion, the so-called \u201con-average-replace-one-example stability\u201d, and showed that this condition is both sufficient and necessary for the generalization and learnability of a general learning se\u008aing.", "startOffset": 222, "endOffset": 1599}, {"referenceID": 2, "context": "Intuitively, a learning algorithm is said to be stable if slight perturbations in the training data result in small changes in the output of the algorithm, and these changes vanish as the data set grows bigger and bigger [Bonnans and Shapiro, 2013]. For example, Devroye and Wagner [1979], Lugosi and Pawlak [1994], and Zhang [2003] showed that several non-parametric learning algorithms are stable; Bousquet and Elisseeff [2002] proved that l2 regularized learning algorithms are uniformly stable; Wibisono et al. [2009] generalized Bousquet and Elisseeff\u2019s results and proved that regularized learning algorithmswith strongly convex penalty functions on bounded domains, e.g., lp regularized learning algorithms for 1 < p \u2264 2, are also uniformly stable; Hardt et al. [2015] showed that parametric models trained by stochastic gradient descent algorithms are uniformly stable; and Liu et al. [2017] proved that tasks in multi-task learning can act as regularizers and that multi-task learning in a very general se\u008aing will therefore be uniformly stable under mild assumptions. \u008ce notion of algorithmic stability has been an important tool in deriving theoretical guarantees of the generalization abilities of learning algorithms. Various notions of stability have been introduced and have been exploited to derive generalization bounds. For some examples, Mukherjee et al. [2006] proved that a statistical form of leave-oneout stability is a sufficient and necessary condition for the generalization and learnability of empirical risk minimization learning algorithms; Shalev-Shwartz et al. [2010] defined a weaker notion, the so-called \u201con-average-replace-one-example stability\u201d, and showed that this condition is both sufficient and necessary for the generalization and learnability of a general learning se\u008aing. In this paper we study learning algorithms that select a hypothesis (i.e., a function used for prediction) from a certain fixed class of functions belonging to a separable Banach space. We introduce a notion of hypothesis stability which measures the impact of changing a single training example on the hypothesis selected by the learning algorithm. \u008cis notion of stability is stronger than uniform algorithmic stability of Bousquet and Elisseeff [2002] that is only concerned about the change in the loss but not the hypothesis itself.", "startOffset": 222, "endOffset": 2270}, {"referenceID": 2, "context": "Intuitively, a learning algorithm is said to be stable if slight perturbations in the training data result in small changes in the output of the algorithm, and these changes vanish as the data set grows bigger and bigger [Bonnans and Shapiro, 2013]. For example, Devroye and Wagner [1979], Lugosi and Pawlak [1994], and Zhang [2003] showed that several non-parametric learning algorithms are stable; Bousquet and Elisseeff [2002] proved that l2 regularized learning algorithms are uniformly stable; Wibisono et al. [2009] generalized Bousquet and Elisseeff\u2019s results and proved that regularized learning algorithmswith strongly convex penalty functions on bounded domains, e.g., lp regularized learning algorithms for 1 < p \u2264 2, are also uniformly stable; Hardt et al. [2015] showed that parametric models trained by stochastic gradient descent algorithms are uniformly stable; and Liu et al. [2017] proved that tasks in multi-task learning can act as regularizers and that multi-task learning in a very general se\u008aing will therefore be uniformly stable under mild assumptions. \u008ce notion of algorithmic stability has been an important tool in deriving theoretical guarantees of the generalization abilities of learning algorithms. Various notions of stability have been introduced and have been exploited to derive generalization bounds. For some examples, Mukherjee et al. [2006] proved that a statistical form of leave-oneout stability is a sufficient and necessary condition for the generalization and learnability of empirical risk minimization learning algorithms; Shalev-Shwartz et al. [2010] defined a weaker notion, the so-called \u201con-average-replace-one-example stability\u201d, and showed that this condition is both sufficient and necessary for the generalization and learnability of a general learning se\u008aing. In this paper we study learning algorithms that select a hypothesis (i.e., a function used for prediction) from a certain fixed class of functions belonging to a separable Banach space. We introduce a notion of hypothesis stability which measures the impact of changing a single training example on the hypothesis selected by the learning algorithm. \u008cis notion of stability is stronger than uniform algorithmic stability of Bousquet and Elisseeff [2002] that is only concerned about the change in the loss but not the hypothesis itself. However, as we will show, the new notion is still quite natural and holds for a variety of learning algorithms. On the other hand, it allows one to exploit martingale inequalities Boucheron et al. [2013] in the Banach space of the hypotheses.", "startOffset": 222, "endOffset": 2557}, {"referenceID": 11, "context": "\u008ce property we need is described in the following result of [Pinelis, 1994]:", "startOffset": 60, "endOffset": 75}, {"referenceID": 4, "context": "Here we recall the notion of uniform stability defined by Bousquet and Elisseeff [2002] for comparison purposes.", "startOffset": 58, "endOffset": 88}, {"referenceID": 4, "context": "Here we recall the notion of uniform stability defined by Bousquet and Elisseeff [2002] for comparison purposes. \u008cis notion of stability relies on the altered sample S = {Z1, . . . , Zi\u22121, Z \u2032 i, Zi+1, . . . , Zn}, the sample S with the i-th example being replaced by an independent copy of Zi. Definition 1 (Uniform Stability). A learning algorithm A is \u03b2(n)-uniformly stable with respect to the loss function l if for all i \u2208 {1, . . . , n}, |l(hS, Z)\u2212 l(hSi, Z)| \u2264 \u03b2(n) , with probability one, where \u03b2(n) \u2208 R+. We propose the following, similar, notion that \u201cacts\u201d on the hypotheses directly, as opposed to the losses. Definition 2 (Uniform Hypothesis Stability). A learning algorithm A is \u03b1(n)-uniformly hypothesis stable if for all i \u2208 {1, . . . , n}, \u2016hS \u2212 hSi\u2016 \u2264 \u03b1(n) . with probability one, where \u03b1(n) \u2208 R+. \u008ce two notions of stability are closely related. Indeed, since \u2016hS \u2212 hSi\u2016 = sup x\u2208X :\u2016x\u2016\u2217\u22641 (\u3008hS, x\u3009 \u2212 \u3008hSi, x\u3009) , if the loss function is Lipschitz in the sense that |l(h, z)\u2212 l(h, z)| \u2264 L |\u3008h, x\u3009 \u2212 \u3008h, x\u3009| for all z \u2208 Z and h, h \u2208 H and \u2016X\u2016\u2217 is bounded by some B > 0 with probability one, then an \u03b1(n)-uniformly hypothesis stable learning algorithm is uniformly stable with \u03b2(n) = LB\u03b1(n). However, the reverse implication need not necessarily hold and hence uniform hypothesis stability is a stronger notion. \u008ce relationship between hypothesis stability and generalization performance hinges on a property of the Banach space B that is closely related to the martingale type of the space\u2014see Pisier [2011] for a comprehensive account.", "startOffset": 58, "endOffset": 1524}, {"referenceID": 13, "context": "We refer to Rakhlin and Sridharan [2015] for more information of martingale inequalities of this kind.", "startOffset": 12, "endOffset": 41}, {"referenceID": 0, "context": "We bound the generalization error (1) in terms of the Rademacher complexity Bartle\u008a and Mendelson [2003] of the algorithmic hypothesis class.", "startOffset": 76, "endOffset": 105}, {"referenceID": 7, "context": "By the Lipschitz property of the loss function and a standard contraction argument, we have [Ledoux and Talagrand, 2013], R(l \u25e6Br) \u2264 L \u00b7R(Br) \u2264 LB \u221a", "startOffset": 92, "endOffset": 120}, {"referenceID": 6, "context": "For the notion of uniform stability, such bounds appear in Lugosi and Pawlak [1994], Bousquet and Elisseeff [2002], Wibisono et al.", "startOffset": 59, "endOffset": 84}, {"referenceID": 4, "context": "For the notion of uniform stability, such bounds appear in Lugosi and Pawlak [1994], Bousquet and Elisseeff [2002], Wibisono et al.", "startOffset": 85, "endOffset": 115}, {"referenceID": 4, "context": "For the notion of uniform stability, such bounds appear in Lugosi and Pawlak [1994], Bousquet and Elisseeff [2002], Wibisono et al. [2009], Hardt et al.", "startOffset": 85, "endOffset": 139}, {"referenceID": 4, "context": "For the notion of uniform stability, such bounds appear in Lugosi and Pawlak [1994], Bousquet and Elisseeff [2002], Wibisono et al. [2009], Hardt et al. [2015], Liu et al.", "startOffset": 85, "endOffset": 160}, {"referenceID": 4, "context": "For the notion of uniform stability, such bounds appear in Lugosi and Pawlak [1994], Bousquet and Elisseeff [2002], Wibisono et al. [2009], Hardt et al. [2015], Liu et al. [2017]. As we will show in the examples below, many of these learning algorithms even have uniform hypothesis stability of order O(1/n).", "startOffset": 85, "endOffset": 179}, {"referenceID": 1, "context": "\u008ce next theorem derives such a bound, relying on techniques developed by Bartle\u008a et al. [2005]. \u008cis result improves essentially on earlier stability-based bounds.", "startOffset": 73, "endOffset": 95}, {"referenceID": 1, "context": "\u008ce proof of \u008ceorem 2 relies on techniques developed by Bartle\u008a et al. [2005]. In particular, we make use of the following result.", "startOffset": 55, "endOffset": 77}, {"referenceID": 1, "context": "\u008ce following lemma is proven in [Bartle\u008a et al., 2005].", "startOffset": 32, "endOffset": 54}, {"referenceID": 0, "context": ", Bartle\u008a and Mendelson [2003]), H \u2032 \u2282 H impliesR(H ) \u2264 R(H).", "startOffset": 2, "endOffset": 31}, {"referenceID": 4, "context": "1 Empirical Risk Minimization Regularized empirical riskminimization has beenknown to be uniformly stable [Bousquet and Elisseeff, 2002].", "startOffset": 106, "endOffset": 136}, {"referenceID": 4, "context": "We refer the reader toDevroye and Wagner [1979], Lugosi and Pawlak [1994], Bousquet and Elisseeff [2002], Zhang [2003], Wibisono et al.", "startOffset": 22, "endOffset": 48}, {"referenceID": 4, "context": "We refer the reader toDevroye and Wagner [1979], Lugosi and Pawlak [1994], Bousquet and Elisseeff [2002], Zhang [2003], Wibisono et al.", "startOffset": 22, "endOffset": 74}, {"referenceID": 4, "context": "We refer the reader toDevroye and Wagner [1979], Lugosi and Pawlak [1994], Bousquet and Elisseeff [2002], Zhang [2003], Wibisono et al.", "startOffset": 75, "endOffset": 105}, {"referenceID": 4, "context": "We refer the reader toDevroye and Wagner [1979], Lugosi and Pawlak [1994], Bousquet and Elisseeff [2002], Zhang [2003], Wibisono et al.", "startOffset": 75, "endOffset": 119}, {"referenceID": 4, "context": "We refer the reader toDevroye and Wagner [1979], Lugosi and Pawlak [1994], Bousquet and Elisseeff [2002], Zhang [2003], Wibisono et al. [2009], Hardt et al.", "startOffset": 75, "endOffset": 143}, {"referenceID": 4, "context": "We refer the reader toDevroye and Wagner [1979], Lugosi and Pawlak [1994], Bousquet and Elisseeff [2002], Zhang [2003], Wibisono et al. [2009], Hardt et al. [2015], Liu et al.", "startOffset": 75, "endOffset": 164}, {"referenceID": 4, "context": "We refer the reader toDevroye and Wagner [1979], Lugosi and Pawlak [1994], Bousquet and Elisseeff [2002], Zhang [2003], Wibisono et al. [2009], Hardt et al. [2015], Liu et al. [2017] for such examples, including stochastic gradient descent methods, empirical risk minimization, and non-parametric learning algorithms such as k-nearest neighbor rules and kernel regression.", "startOffset": 75, "endOffset": 183}, {"referenceID": 4, "context": "Bousquet and Elisseeff [2002] proved that l2-regularized learning algorithms are \u03b2(n)uniformly stable.", "startOffset": 0, "endOffset": 30}, {"referenceID": 4, "context": "Bousquet and Elisseeff [2002] proved that l2-regularized learning algorithms are \u03b2(n)uniformly stable. Wibisono et al. [2009] extended the result and studied a sufficient condition of the penalty term N(h) to ensure uniform \u03b2(n)-stability.", "startOffset": 0, "endOffset": 126}, {"referenceID": 15, "context": "\u008ce proof of \u008ceorem 3 relies on the following result implied by Wibisono et al. [2009]. Proposition 3.", "startOffset": 63, "endOffset": 86}, {"referenceID": 6, "context": "Hardt et al. [2015] showed that parametric models trained by SGD methods are uniformly stable.", "startOffset": 0, "endOffset": 20}, {"referenceID": 4, "context": "When the loss function is strongly convex, the stability of SGD is consistent with the result in [Bousquet and Elisseeff, 2002].", "startOffset": 97, "endOffset": 127}, {"referenceID": 4, "context": "Bousquet and Elisseeff [2002] studied the stability of batch methods.", "startOffset": 0, "endOffset": 30}, {"referenceID": 4, "context": "Bousquet and Elisseeff [2002] studied the stability of batch methods. When the loss function is strongly convex, the stability of SGD is consistent with the result in [Bousquet and Elisseeff, 2002]. \u008ceorem 4 implies that SGD generalizeswell with an early stop. \u008cis partially explains why deep learning algorithms employing SGD with a certain small number of iterations perform well. However, early stopped SGD may have a large empirical risk RS(hT ). \u008ce proof of \u008ceorem 4 follows immediately from \u008ceorem 2, combined with the following result implied by Hardt et al. [2015] (which is a collection of the results of \u008ceorems 3.", "startOffset": 0, "endOffset": 573}], "year": 2017, "abstractText": "We introduce a notion of algorithmic stability of learning algorithms\u2014that we term hypothesis stability\u2014that captures stability of the hypothesis output by the learning algorithm in the normed space of functions from which hypotheses are selected. \u008ce main result of the paper bounds the generalization error of any learning algorithm in terms of its hypothesis stability. \u008ce bounds are based on martingale inequalities in the Banach space to which the hypotheses belong. We apply the general bounds to bound the performance of some learning algorithms based on empirical risk minimization and stochastic gradient descent. Parts of the work were done when Tongliang Liu was a visiting PhD student at Pompeu Fabra University. School of Information Technologies, Faculty Engineering and Information Technologies, University of Sydney, Sydney, Australia, tliang.liu@gmail.com, dacheng.tao@sydney.edu.au Department of Economics and Business, Pompeu Fabra University, Barcelona, Spain, gabor.lugosi@upf.edu ICREA, Pg. Llus Companys 23, 08010 Barcelona, Spain Barcelona Graduate School of Economics AI group, DTIC, Universitat Pompeu Fabra, Barcelona, Spain, gergely.neu@gmail.com 1", "creator": "LaTeX with hyperref package"}}}