{"id": "1702.08887", "review": {"conference": "ICML", "VERSION": "v1", "DATE_OF_SUBMISSION": "28-Feb-2017", "title": "Stabilising Experience Replay for Deep Multi-Agent Reinforcement Learning", "abstract": "Many real-world problems, such as network packet routing and urban traffic control, are naturally modeled as multi-agent reinforcement learning (RL) problems. However, existing multi-agent RL methods typically scale poorly in the problem size. Therefore, a key challenge is to translate the success of deep learning on single-agent RL to the multi-agent setting. A key stumbling block is that independent Q-learning, the most popular multi-agent RL method, introduces nonstationarity that makes it incompatible with the experience replay memory on which deep RL relies. This paper proposes two methods that address this problem: 1) conditioning each agent's value function on a footprint that disambiguates the age of the data sampled from the replay memory and 2) using a multi-agent variant of importance sampling to naturally decay obsolete data. Results on a challenging decentralised variant of StarCraft unit micromanagement confirm that these methods enable the successful combination of experience replay with multi-agent RL.", "histories": [["v1", "Tue, 28 Feb 2017 17:56:41 GMT  (346kb,D)", "http://arxiv.org/abs/1702.08887v1", null], ["v2", "Mon, 12 Jun 2017 22:00:56 GMT  (1940kb,D)", "http://arxiv.org/abs/1702.08887v2", "Camera-ready version, International Conference of Machine Learning 2017"]], "reviews": [], "SUBJECTS": "cs.AI cs.LG cs.MA", "authors": ["jakob n foerster", "nantas nardelli", "gregory farquhar", "triantafyllos afouras", "philip h s torr", "pushmeet kohli", "shimon whiteson"], "accepted": true, "id": "1702.08887"}, "pdf": {"name": "1702.08887.pdf", "metadata": {"source": "META", "title": "Stabilising Experience Replay for Deep Multi-Agent Reinforcement Learning", "authors": ["Jakob Foerster", "Nantas Nardelli", "Gregory Farquhar", "Philip. H. S. Torr", "Pushmeet Kohli", "Shimon Whiteson"], "emails": ["<jakob.foerster@cs.ox.ac.uk>,", "<nantas@robots.ox.ac.uk>."], "sections": [{"heading": "1. Introduction", "text": "In fact, it is such that the majority of them are able to survive themselves without there being a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is in which there is a process in which there is a process in which there is a process in which there is a process"}, {"heading": "2. Related Work", "text": "This year, it has reached the stage where it will be able to take the lead."}, {"heading": "3. Background", "text": "We start with background information on the learning of single and multi-agent reinforcements."}, {"heading": "3.1. Single-Agent Reinforcement Learning", "text": "In a traditional RL problem, the agent aims to maximize his expected discounted return (Sutton & Barto, 1998).In a fully observable environment, the agent observes the true state of environmental memory (Rt | st = s, ut = u].The Bellman optimality operator T Q (s, u) selects an action to be performed according to a policy preference (u | s).The action value function Q of a policy preference is Q\u03c0 (s, u) = E [Rt | st = s, ut = u].The Bellman optimality operator T Q (s, u) = It value function Q (s, u \u2032 s) is a contraction operator in the highest standard with a unique fixed point, the optimal Q function Q (s, u) = maxp function and the ability (s, u)."}, {"heading": "3.2. Multi-Agent Reinforcement Learning", "text": "We are considering a fully cooperative multi-agent setting in which n agents are identified by an \"A-A\" problem. (..., n) Qua-Q story (\"Q\") is involved in a stochastic game, G, described by a \"Tupel\" G = < S, U, P, R, Z, O, n, \u03b3 >. The environment occupies the states of \"S\" S \"S\" S \"S\" S \"S\" S \"S\" S. \"Since the agents are fully cooperative, they share the same reward function r\" U \"s.\" (\"S\" U \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s. (\"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s.\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s. (\"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \""}, {"heading": "4. Methods", "text": "To avoid the difficulty of combining IQL with experience reproduction, previous work on Deep Multi-Agent RL limited the use of experience reproduction to short, up-to-date buffers (Leibo et al., 2017) or simply disabled reproduction altogether (Foerster et al., 2016). However, these workarounds limit sample efficiency and threaten the stability of Multiagent RL. In this section, we propose two approaches to effectively integrate experience reproduction into Multiagent RL."}, {"heading": "4.1. Multi-Agent RL with Fingerprints", "text": "The weakness of IQL is that by treating other agents as part of the environment, it ignores the fact that the strategies of these agents change over time, which means that their own Q function does not become stationary. This implies that the Q function could be made stationary if it is dependent on the strategies of the other agents. However, this is precisely the philosophy behind the Hyper Q Learning episodes (Tesauro, 2003): the state space of each agent is reinforced by an estimate of the strategies of the other agents, calculated using Bayesian conclusions. Intuitively, this reduces the learning problems of each agent to a standard, single-agent episode problem in a stationary, but much larger, environment. The practical difficulty of Hyper Q Learning is, of course, that the dimensionality of the Q function has increased, making it potentially unfeasible to learn. This problem is exacerbated in deep learning when the strategies of the other networks consist of highly neural agents."}, {"heading": "4.2. Multi-Agent Importance Sampling", "text": "Instead of preventing the non-stationarity of IQL by extending the input of each agent with a fingerprint, we can instead develop a meaning-sampling scheme for multi-agent setting, just as an RL agent can use the meaning of sampling to learn from the data collected when its own policy was different, we can also learn from the off-environment (Ciosek & Whiteson, 2017) from the data collected in another environment. Since IQL treats the policies of other agents as part of the environment, we can use the meaning of sampling corrections to stabilize a repetition of the experience. First, consider a fully observable multi-agent setting. Since the Q functions can now be tailored directly to the true state, we can write the Bellman optimality equation for a single agent: the policies of all other agents: Q-A (s, ua-A) is important."}, {"heading": "5. Experiments", "text": "In this section, we describe our experiments in which we apply fingerprint repetitions (XP-FP) and meaning tests (XP-IS) to the StarCraft domain. Since the network must be able to represent Q-values for a number of different policies of the other agents, we assume that the IS will exceed them if the network's representation capacity is sufficiently limited. We also investigate whether, in some environments and with sufficient recurring capacity, the agent can derive the episode of a given training sample solely from its action observation history without needing a fingerprint. To test these hypotheses, we must be able to manipulate the informativity of the action observation history and the capacity of the RNN. As a substitute for the capacity of the RNN, we vary the number of neurons in their hidden state between a \"small\" and \"large\" setting of 16 or 128 units respectively, the NNN is only affected by the informativeness in two."}, {"heading": "5.1. Decentralised StarCraft Micromanagement", "text": "This differs from standard multi-agent settings such as Packet World (Weyns et al., 2005) and simulated RoboCup (Hausknecht et al., 2016), where whole episodes can often be reproduced and analyzed in full. This difficulty is typical of real-world problems and is well suited to the model-free approaches that occur in deep RL. In StarCraft, micro-management refers to the subtask of controlling individual or grouped units to move them around the map and fight enemy units. In our multi-agent variant of StarCraft micro-management, the centralized player is replaced by a group of agents, each assigned to a unit on the map."}, {"heading": "5.2. Architecture", "text": "We use the recurring DQN architecture described by Foerster et al. (2016) with a few changes. As we do not take communicating agents into account, the number of bits in the message is set to 0. As mentioned above, we use two different values for the hidden state of the RNN, a \"small setting\" of 16 units and a \"large setting\" of 128. We start the training with er = 50 completely random exploratory episodes, then we use an incandescent plan of 1 / (1 + \u221a e \u2212 er), with an exponential smoothing window of 20 episodes. We train the network for emax = 600 training episodes. In the standard training loop, we collect 5 full episodes and add them to the replay memory at each training step. During the standard training, we consistently try from the playback memory and perform the complete unrolling of episodes. In \"short unroll\" experiments, we unroll a second set of 10 steps from the initial state to the beginning of the neural state of the network."}, {"heading": "6. Results", "text": "In this section we present the results of our StarCraft experiments."}, {"heading": "6.1. Disambiguate and Represent", "text": "This year, it has reached the stage where it will be able to take the lead."}, {"heading": "7. Conclusion", "text": "In this paper, two methods for stabilizing repetitive experiences in the field of multi-agent reinforcement learning were presented: we demonstrated that the use of fingerprints can help agents distinguish between episodes from different parts of the training process, thereby partially restoring performance in the face of instationality; we also demonstrated that an important sample for multi-agent setting with repeat memory restores most of the performance lost due to non-stationary training samples; and we presented ablation studies on the input and architecture of RNN to illustrate the importance of making a clear statement between episodes from different phases of training when using recurrent learning in non-stationary environments."}, {"heading": "Acknowledgements", "text": "This work was supported by the Oxford-Google DeepMind Graduate Scholarship, the Microsoft Research PhD Scolarship Program and EPSRC. We thank Nando de Freitas, Yannis Assael and Brendan Shillingford for their helpful comments and discussions. We also thank Gabriel Synnaeve, Zeming Lin and the rest of the TorchCraft team at FAIR for their help with the UI."}], "references": [{"title": "A comprehensive survey of multiagent reinforcement learning", "author": ["Busoniu", "Lucian", "Babuska", "Robert", "De Schutter", "Bart"], "venue": "IEEE Transactions on Systems Man and Cybernetics Part C Applications and Reviews,", "citeRegEx": "Busoniu et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Busoniu et al\\.", "year": 2008}, {"title": "Empirical evaluation of gated recurrent neural networks on sequence modeling", "author": ["Chung", "Junyoung", "Gulcehre", "Caglar", "Cho", "KyungHyun", "Bengio", "Yoshua"], "venue": "arXiv preprint arXiv:1412.3555,", "citeRegEx": "Chung et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Chung et al\\.", "year": 2014}, {"title": "Torch7: A matlab-like environment for machine learning", "author": ["R. Collobert", "K. Kavukcuoglu", "C. Farabet"], "venue": "In BigLearn, NIPS Workshop,", "citeRegEx": "Collobert et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Collobert et al\\.", "year": 2011}, {"title": "Awesome: A general multiagent learning algorithm that converges in self-play and learns a best response against stationary opponents", "author": ["Conitzer", "Vincent", "Sandholm", "Tuomas"], "venue": "Machine Learning,", "citeRegEx": "Conitzer et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Conitzer et al\\.", "year": 2007}, {"title": "Dealing with non-stationary environments using context detection", "author": ["Da Silva", "Bruno C", "Basso", "Eduardo W", "Bazzan", "Ana LC", "Engel", "Paulo M"], "venue": "In Proceedings of the 23rd international conference on Machine learning,", "citeRegEx": "Silva et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Silva et al\\.", "year": 2006}, {"title": "Learning to communicate with deep multi-agent reinforcement learning", "author": ["Foerster", "Jakob", "Assael", "Yannis M", "de Freitas", "Nando", "Whiteson", "Shimon"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Foerster et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Foerster et al\\.", "year": 2016}, {"title": "Deep recurrent qlearning for partially observable mdps", "author": ["Hausknecht", "Matthew", "Stone", "Peter"], "venue": "arXiv preprint arXiv:1507.06527,", "citeRegEx": "Hausknecht et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Hausknecht et al\\.", "year": 2015}, {"title": "Half field offense: an environment for multiagent learning and ad hoc teamwork", "author": ["Hausknecht", "Matthew", "Mupparaju", "Prannoy", "Subramanian", "Sandeep", "S Kalyanakrishnan", "P. Stone"], "venue": "In AAMAS Adaptive Learning Agents (ALA) Workshop,", "citeRegEx": "Hausknecht et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Hausknecht et al\\.", "year": 2016}, {"title": "Opponent modeling in deep reinforcement learning", "author": ["He", "Boyd-Graber", "Jordan", "Kwok", "Kevin", "Daum\u00e9 III", "Hal"], "venue": "In Proceedings of The 33rd International Conference on Machine Learning,", "citeRegEx": "He et al\\.,? \\Q2016\\E", "shortCiteRegEx": "He et al\\.", "year": 2016}, {"title": "Long shortterm memory", "author": ["Hochreiter", "Sepp", "Schmidhuber", "J\u00fcrgen"], "venue": "Neural computation,", "citeRegEx": "Hochreiter et al\\.,? \\Q1997\\E", "shortCiteRegEx": "Hochreiter et al\\.", "year": 1997}, {"title": "Learning to play guess who? and inventing a grounded language as a consequence", "author": ["Jorge", "Emilio", "K\u00e5geb\u00e4ck", "Mikael", "Gustavsson", "Emil"], "venue": "arXiv preprint arXiv:1611.03218,", "citeRegEx": "Jorge et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Jorge et al\\.", "year": 2016}, {"title": "Collaborative multiagent reinforcement learning by payoff propagation", "author": ["Kok", "Jelle R", "Vlassis", "Nikos"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Kok et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Kok et al\\.", "year": 2006}, {"title": "Multiagent reinforcement learning for urban traffic control using coordination graphs", "author": ["Kuyer", "Lior", "Whiteson", "Shimon", "Bakker", "Bram", "Vlassis", "Nikos"], "venue": "In ECML 2008: Proceedings of the Nineteenth European Conference on Machine Learning,", "citeRegEx": "Kuyer et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Kuyer et al\\.", "year": 2008}, {"title": "An algorithm for distributed reinforcement learning in cooperative multiagent systems", "author": ["Lauer", "Martin", "Riedmiller"], "venue": "Proceedings of the Seventeenth International Conference on Machine Learning. Citeseer,", "citeRegEx": "Lauer et al\\.,? \\Q2000\\E", "shortCiteRegEx": "Lauer et al\\.", "year": 2000}, {"title": "Multi-agent reinforcement learning in sequential social dilemmas", "author": ["Leibo", "Joel Z", "Zambaldi", "Vinicius", "Lanctot", "Marc", "Marecki", "Janusz", "Graepel", "Thore"], "venue": "arXiv preprint arXiv:1702.03037,", "citeRegEx": "Leibo et al\\.,? \\Q2017\\E", "shortCiteRegEx": "Leibo et al\\.", "year": 2017}, {"title": "Hierarchical multi-agent reinforcement learning", "author": ["Makar", "Rajbala", "Mahadevan", "Sridhar", "Ghavamzadeh", "Mohammad"], "venue": "In Proceedings of the fifth international conference on Autonomous agents,", "citeRegEx": "Makar et al\\.,? \\Q2001\\E", "shortCiteRegEx": "Makar et al\\.", "year": 2001}, {"title": "Using communication to reduce locality in distributed multiagent learning", "author": ["Mataric", "Maja J"], "venue": "Journal of experimental & theoretical artificial intelligence,", "citeRegEx": "Mataric and J.,? \\Q1998\\E", "shortCiteRegEx": "Mataric and J.", "year": 1998}, {"title": "Independent reinforcement learners in cooperative markov games: a survey regarding coordination problems", "author": ["Matignon", "Laetitia", "Laurent", "Guillaume J", "Le Fort-Piat", "Nadine"], "venue": "The Knowledge Engineering Review,", "citeRegEx": "Matignon et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Matignon et al\\.", "year": 2012}, {"title": "Multiagent Systems: Algorithmic, Game-Theoretic, and Logical Foundations", "author": ["Y. Shoham", "K. Leyton-Brown"], "venue": null, "citeRegEx": "Shoham and Leyton.Brown,? \\Q2009\\E", "shortCiteRegEx": "Shoham and Leyton.Brown", "year": 2009}, {"title": "Learning multiagent communication with backpropagation", "author": ["Sukhbaatar", "Sainbayar", "Fergus", "Rob"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Sukhbaatar et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Sukhbaatar et al\\.", "year": 2016}, {"title": "Reinforcement learning: An introduction, volume 1", "author": ["Sutton", "Richard S", "Barto", "Andrew G"], "venue": "MIT press Cambridge,", "citeRegEx": "Sutton et al\\.,? \\Q1998\\E", "shortCiteRegEx": "Sutton et al\\.", "year": 1998}, {"title": "Torchcraft: a library for machine learning research on real-time strategy games", "author": ["Synnaeve", "Gabriel", "Nardelli", "Nantas", "Auvolat", "Alex", "Chintala", "Soumith", "Lacroix", "Timoth\u00e9e", "Lin", "Zeming", "Richoux", "Florian", "Usunier", "Nicolas"], "venue": "arXiv preprint arXiv:1611.00625,", "citeRegEx": "Synnaeve et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Synnaeve et al\\.", "year": 2016}, {"title": "Multiagent cooperation and competition with deep reinforcement learning", "author": ["Tampuu", "Ardi", "Matiisen", "Tambet", "Kodelja", "Dorian", "Kuzovkin", "Ilya", "Korjus", "Kristjan", "Aru", "Juhan", "Jaan", "Vicente", "Raul"], "venue": "arXiv preprint arXiv:1511.08779,", "citeRegEx": "Tampuu et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Tampuu et al\\.", "year": 2015}, {"title": "Multi-agent reinforcement learning: Independent vs. cooperative agents", "author": ["Tan", "Ming"], "venue": "In Proceedings of the tenth international conference on machine learning,", "citeRegEx": "Tan and Ming.,? \\Q1993\\E", "shortCiteRegEx": "Tan and Ming.", "year": 1993}, {"title": "Extending q-learning to general adaptive multi-agent systems", "author": ["Tesauro", "Gerald"], "venue": "In NIPS,", "citeRegEx": "Tesauro and Gerald.,? \\Q2003\\E", "shortCiteRegEx": "Tesauro and Gerald.", "year": 2003}, {"title": "Episodic exploration for deep deterministic policies: An application to starcraft micromanagement tasks", "author": ["Usunier", "Nicolas", "Synnaeve", "Gabriel", "Lin", "Zeming", "Chintala", "Soumith"], "venue": "arXiv preprint arXiv:1609.02993,", "citeRegEx": "Usunier et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Usunier et al\\.", "year": 2016}, {"title": "Coordinated deep reinforcement learners for traffic light control", "author": ["Van der Pol", "Elise", "Oliehoek", "Frans A"], "venue": "In NIPS\u201916 Workshop on Learning, Inference and Control of Multi-Agent Systems,", "citeRegEx": "Pol et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Pol et al\\.", "year": 2016}, {"title": "Sample efficient actor-critic with experience", "author": ["Wang", "Ziyu", "Bapst", "Victor", "Heess", "Nicolas", "Mnih", "Volodymyr", "Munos", "Remi", "Kavukcuoglu", "Koray", "de Freitas", "Nando"], "venue": "replay. arXiv preprint arXiv:1611.01224,", "citeRegEx": "Wang et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Wang et al\\.", "year": 2016}, {"title": "Learning from delayed rewards", "author": ["Watkins", "Christopher John Cornish Hellaby"], "venue": "PhD thesis,", "citeRegEx": "Watkins and Hellaby.,? \\Q1989\\E", "shortCiteRegEx": "Watkins and Hellaby.", "year": 1989}, {"title": "The packet-world: A test bed for investigating situated multi-agent systems. In Software Agent-Based Applications, Platforms and Development Kits", "author": ["Weyns", "Danny", "Helleboogh", "Alexander", "Holvoet", "Tom"], "venue": null, "citeRegEx": "Weyns et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Weyns et al\\.", "year": 2005}, {"title": "Multiagent reinforcement learning for multi-robot systems: A survey", "author": ["Yang", "Erfu", "Gu", "Dongbing"], "venue": "Technical report, tech. rep,", "citeRegEx": "Yang et al\\.,? \\Q2004\\E", "shortCiteRegEx": "Yang et al\\.", "year": 2004}, {"title": "A multiagent framework for packet routing in wireless sensor", "author": ["Ye", "Dayong", "Zhang", "Minjie", "Yang", "Yun"], "venue": "networks. sensors,", "citeRegEx": "Ye et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Ye et al\\.", "year": 2015}, {"title": "Empirically evaluating multiagent learning algorithms", "author": ["E. Zawadzki", "A. Lipson", "K. Leyton-Brown"], "venue": "arXiv preprint 1401.8074,", "citeRegEx": "Zawadzki et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Zawadzki et al\\.", "year": 2014}], "referenceMentions": [{"referenceID": 31, "context": "However, many real-world problems, such as network packet delivery (Ye et al., 2015), rubbish removal (Makar et al.", "startOffset": 67, "endOffset": 84}, {"referenceID": 15, "context": ", 2015), rubbish removal (Makar et al., 2001), and urban traffic control (Kuyer et al.", "startOffset": 25, "endOffset": 45}, {"referenceID": 12, "context": ", 2001), and urban traffic control (Kuyer et al., 2008; Van der Pol & Oliehoek, 2016), are naturally modeled as cooperative multi-agent systems.", "startOffset": 35, "endOffset": 85}, {"referenceID": 17, "context": "Fortunately, substantial empirical evidence has shown that IQL often works well in practice (Matignon et al., 2012).", "startOffset": 92, "endOffset": 115}, {"referenceID": 14, "context": "To avoid this problem, previous work on deep multi-agent RL has limited the use of experience replay to short, recent buffers (Leibo et al., 2017) or simply disabled replay altogether (Foerster et al.", "startOffset": 126, "endOffset": 146}, {"referenceID": 5, "context": ", 2017) or simply disabled replay altogether (Foerster et al., 2016).", "startOffset": 45, "endOffset": 68}, {"referenceID": 0, "context": "Multi-agent RL has a rich history (Busoniu et al., 2008; Yang & Gu, 2004) but has mostly focused on tabular settings and simple environments.", "startOffset": 34, "endOffset": 73}, {"referenceID": 32, "context": "The most commonly used method is independent Qlearning(Tan, 1993; Shoham & Leyton-Brown, 2009; Zawadzki et al., 2014), which we discuss further in Section 3.", "startOffset": 54, "endOffset": 117}, {"referenceID": 4, "context": "2 - and AWESOME (Conitzer & Sandholm, 2007) try to tackle the nonstationarity by tracking and conditioning each agents\u2019 learning process on their teammates\u2019 current policy, while Da Silva et al. (2006) propose detecting and tracking different classes of traces on which to condition policy learning.", "startOffset": 182, "endOffset": 202}, {"referenceID": 4, "context": "2 - and AWESOME (Conitzer & Sandholm, 2007) try to tackle the nonstationarity by tracking and conditioning each agents\u2019 learning process on their teammates\u2019 current policy, while Da Silva et al. (2006) propose detecting and tracking different classes of traces on which to condition policy learning. Kok & Vlassis (2006) show that coordination can be learnt by estimating a global Q-function in the classical distributed setting", "startOffset": 182, "endOffset": 321}, {"referenceID": 17, "context": "Tampuu et al. (2015) apply a framework that combines DQN with independent Q-learning to two-player pong.", "startOffset": 0, "endOffset": 21}, {"referenceID": 5, "context": "Foerster et al. (2016) propose DIAL, a end-to-end differentiable architecture that allows agents to learn to communicate and has since used by Jorge et al.", "startOffset": 0, "endOffset": 23}, {"referenceID": 5, "context": "Foerster et al. (2016) propose DIAL, a end-to-end differentiable architecture that allows agents to learn to communicate and has since used by Jorge et al. (2016) in a similar setting.", "startOffset": 0, "endOffset": 163}, {"referenceID": 5, "context": "Foerster et al. (2016) propose DIAL, a end-to-end differentiable architecture that allows agents to learn to communicate and has since used by Jorge et al. (2016) in a similar setting. Sukhbaatar et al. (2016) also show that it is possible to learn to communicate by backpropagation.", "startOffset": 0, "endOffset": 210}, {"referenceID": 5, "context": "Foerster et al. (2016) propose DIAL, a end-to-end differentiable architecture that allows agents to learn to communicate and has since used by Jorge et al. (2016) in a similar setting. Sukhbaatar et al. (2016) also show that it is possible to learn to communicate by backpropagation. Leibo et al. (2017) analyse the emergence of cooperation and defection when using multiagent RL in mixed-cooperation environments such as the wolfpack problem.", "startOffset": 0, "endOffset": 304}, {"referenceID": 5, "context": "Foerster et al. (2016) propose DIAL, a end-to-end differentiable architecture that allows agents to learn to communicate and has since used by Jorge et al. (2016) in a similar setting. Sukhbaatar et al. (2016) also show that it is possible to learn to communicate by backpropagation. Leibo et al. (2017) analyse the emergence of cooperation and defection when using multiagent RL in mixed-cooperation environments such as the wolfpack problem. He et al. (2016) addresses multi-agent learning by explicitly marginalising the opponents strategy using a mixture of experts in the DQN.", "startOffset": 0, "endOffset": 461}, {"referenceID": 25, "context": "Finally, in the context of StarCraft micromanagement, Usunier et al. (2016) demonstrated some success learning a centralised policy using standard single-agent RL.", "startOffset": 54, "endOffset": 76}, {"referenceID": 1, "context": "In deep RL, this is accomplished by modelling the Q-function with a recurrent neural network (Hausknecht & Stone, 2015), utilising a gated architecture such as LSTM (Hochreiter & Schmidhuber, 1997) or GRU (Chung et al., 2014).", "startOffset": 205, "endOffset": 225}, {"referenceID": 17, "context": "On the one hand, the conventional wisdom is that this problem is not severe in practice, and substantial empirical results have demonstrated success with IQL (Matignon et al., 2012).", "startOffset": 158, "endOffset": 181}, {"referenceID": 14, "context": "To avoid the difficulty of combining IQL with experience replay, previous work on deep multi-agent RL has limited the use of experience replay to short, recent buffers (Leibo et al., 2017) or simply disabled replay altogether (Foerster et al.", "startOffset": 168, "endOffset": 188}, {"referenceID": 5, "context": ", 2017) or simply disabled replay altogether (Foerster et al., 2016).", "startOffset": 45, "endOffset": 68}, {"referenceID": 29, "context": "This differs from standard multi-agent settings such as Packet World (Weyns et al., 2005) and simulated RoboCup (Hausknecht et al.", "startOffset": 69, "endOffset": 89}, {"referenceID": 7, "context": ", 2005) and simulated RoboCup (Hausknecht et al., 2016), where often entire episodes can be fully replayed and analysed.", "startOffset": 30, "endOffset": 55}, {"referenceID": 25, "context": "This is a variation of a naturally arising battle signal, comparable with the one used by Usunier et al. (2016). A few timesteps after the agents are spawned, the agents are attacked by opponent units (of the same type).", "startOffset": 90, "endOffset": 112}, {"referenceID": 21, "context": "(2011), and we run our StarCraft experiments with TorchCraft (Synnaeve et al., 2016), a library that provides the functionality to enact the standard reinforcement learning step in StarCraft: BroodWar in a centralised way (which we extend to enable multi-agent control).", "startOffset": 61, "endOffset": 84}, {"referenceID": 2, "context": "We build our models in Torch7 Collobert et al. (2011), and we run our StarCraft experiments with TorchCraft (Synnaeve et al.", "startOffset": 30, "endOffset": 54}, {"referenceID": 5, "context": "We use the recurrent DQN architecture described by Foerster et al. (2016) with a few modifications.", "startOffset": 51, "endOffset": 74}, {"referenceID": 5, "context": "All other hyperparameters are identical to Foerster et al. (2016).", "startOffset": 43, "endOffset": 66}], "year": 2017, "abstractText": "Many real-world problems, such as network packet routing and urban traffic control, are naturally modeled as multi-agent reinforcement learning (RL) problems. However, existing multi-agent RL methods typically scale poorly in the problem size. Therefore, a key challenge is to translate the success of deep learning on singleagent RL to the multi-agent setting. A key stumbling block is that independent Q-learning, the most popular multi-agent RL method, introduces nonstationarity that makes it incompatible with the experience replay memory on which deep RL relies. This paper proposes two methods that address this problem: 1) conditioning each agent\u2019s value function on a footprint that disambiguates the age of the data sampled from the replay memory and 2) using a multi-agent variant of importance sampling to naturally decay obsolete data. Results on a challenging decentralised variant of StarCraft unit micromanagement confirm that these methods enable the successful combination of experience replay with multi-agent RL.", "creator": "LaTeX with hyperref package"}}}