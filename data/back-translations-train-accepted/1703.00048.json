{"id": "1703.00048", "review": {"conference": "ICML", "VERSION": "v1", "DATE_OF_SUBMISSION": "28-Feb-2017", "title": "Provably Optimal Algorithms for Generalized Linear Contextual Bandits", "abstract": "Contextual bandits are widely used in Internet services from news recommendation to advertising. Generalized linear models (logistical regression in particular) have demonstrated stronger performance than linear models in many applications. However, most theoretical analyses on contextual bandits so far are on linear bandits. In this work, we propose an upper confidence bound based algorithm for generalized linear contextual bandits, which achieves an $\\tilde{O}(\\sqrt{dT})$ regret over $T$ rounds with $d$ dimensional feature vectors. This regret matches the minimax lower bound, up to logarithmic terms, and improves on the best previous result by a $\\sqrt{d}$ factor, assuming the number of arms is fixed. A key component in our analysis is to establish a new, sharp finite-sample confidence bound for maximum-likelihood estimates in generalized linear models, which may be of independent interest. We also analyze a simpler upper confidence bound algorithm, which is useful in practice, and prove it to have optimal regret for the certain cases.", "histories": [["v1", "Tue, 28 Feb 2017 20:39:44 GMT  (39kb)", "http://arxiv.org/abs/1703.00048v1", "15 pages"], ["v2", "Sun, 18 Jun 2017 04:07:45 GMT  (39kb)", "http://arxiv.org/abs/1703.00048v2", "Published at ICML 2017"]], "COMMENTS": "15 pages", "reviews": [], "SUBJECTS": "cs.LG cs.AI stat.ML", "authors": ["lihong li", "yu lu", "dengyong zhou"], "accepted": true, "id": "1703.00048"}, "pdf": {"name": "1703.00048.pdf", "metadata": {"source": "META", "title": "Provable Optimal Algorithms for Generalized Linear Contextual Bandits", "authors": ["Lihong Li", "Yu Lu", "Dengyong Zhou"], "emails": [], "sections": [{"heading": null, "text": "ar Xiv: 170 3.00 048v 1 [cs.L G] February 28, 2017Services ranging from news recommendation to advertising. Generalized linear models (especially logistic regression) have shown greater performance than linear models in many applications. However, most theoretical analyses of context-dependent bandits have so far focused on linear bandits. In this paper, we propose an algorithm based on upper confidence limits for generalized linear competing bandits that achieves O-repentance compared to T-rounds with d-dimensional feature vectors. This regret corresponds up to logarithmic terms of the minimax lower limit and improves the best previous result by a \u221a d factor, provided the number of arms is fixed. A key component in our analysis is the establishment of a new, sharp finite sample linked to maximum probability estimates in generalized linear models that may be of independent interest."}, {"heading": "1. Introduction", "text": "If standard treatment and a new treatment are available for a particular disease, the physician must decide in a sequential manner which of them should be used based on patient profiles such as age, general physical status or drug history. However, with the development of modern technologies, contextual bandit problems must have more applications, especially in web-based recommendations, advertising and search (Agarwal et al., 2009; Li et al., 2012). In the issue of personalized news recommendations, the website must recommend news articles that are most interesting to users visiting the website. The problem is particularly challenging for Breaking News because little data is available. 1Microsoft Research, Redmond, WA 98052 2Department of Statistics, Yale University, New Haven, CT, USA.to make good prediction about user interest."}, {"heading": "2. Problem Setting", "text": "We considered the stochastic K-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-"}, {"heading": "3. Generalized Linear Models", "text": "To motivate the algorithms proposed in this paper, we first briefly review the classical probability theory of generalized linear models. In the canonical generalized linear model (McCullagh & Nelder, 1989), the conditional distribution of Y numbers is derived from the exponential family, and its density is determined by the parameters of generalization; m, g, and h are three normalization functions from R to R. The exponential family (3) is a very broad family of distributions, including the Gaussian, binomial, Poisson, Gamma, and inverse-Gaussian distributions. It follows from the standard properties of exponential families (Brown, 1986) that m numbers are unsatisfactory."}, {"heading": "4. Algorithms and Main Results", "text": "In this section we will present two algorithms: While the first algorithm is more efficient in terms of computation, the second algorithm has a proven optimum limit of remorse."}, {"heading": "4.1. Algorithm UCB-GLM", "text": "The idea of upper confidence limits (UCB) is highly effective in exploring and exploiting universal dependence in many parametric bandit problems, including K-arm bandits (Auer et al., 2002a) and linear bandits (Abbasi-Yadkori et al., 2011; Auer, 2003; Chu et al., 2011; Dani et al., 2008). For the generalized linear model considered here, as it is a strictly increasing function, our goal is tantamount to adopting a maximum approach. (Auer, 2003; Chu et al.) is our current estimate of post-round exploitation. An exploitation action is one that maximizes the estimated mean, while an exploration action selects the one that has the greatest variance. Thus, we can simply balance the action that maximizes the sum of estimated averages and variance."}, {"heading": "4.2. Algorithm SupCB-GLM", "text": "While the UCB-GLM algorithm works well in practice (Li et al., 2012), it is unclear whether it can achieve the optimum rate of O (\u221a dT logK) when it is small. \u2212 As mentioned in Section 4.1, the main technical difficulty in analyzing UCB-GLM is the interdependence between samples. \u2212 Like the algorithm CB-GLM (algorithm 2) as subroutine.Algorithm 2 CB-GLMInput: Parameter set (t) and candidate set A.1. Let us leave the solution of i-GLSch (t) [Yi-M) Xi = 02."}, {"heading": "End For", "text": "Normal result (5) and thus our regret about the algorithm SupCB-GLM.Theorem 3. For each 0 < \u03b4 < 1, if we use the SupCB-GLM algorithm with \u03c4 = \u221a dT and \u03b1 = 3\u03c3\u0445 \u221a 2 log (TC / \u03b4) for"}, {"heading": "T \u2265 T0 rounds, where", "text": "The TheTheTheory of TheTheTheTheTheTheTheTheTheory demonstrates an O (D) reason for the algorithm SupCB-GLM. It has been shown that the algorithm is GLM-GLM-GLM-GLM-GLM-GLM-GLM-GLM-GLM-GLM-GLM-GLM-GLM-GLM-GLM-GLM-GLM-GLM-GLM-GLM-GLM-GLM-GLM-GLM-GLM-GLM-GLM-GLM-GLM-GLM-GLM-GLM-GLM-GLM-GLM-GLM-GLM-GLM-GLM-GLM-GLM"}, {"heading": "5. Discussions", "text": "In this article, we propose two algorithms that equip armed bandits with generalized linear models: While the first algorithm, UCB-GLM, achieves the optimal rate for the finite number of actions, the second algorithm, SupCBGLM, has been proven to be optimal for the finite actions in each round. Whether UCB-GLM can achieve the optimal rate for small K, however, remains open."}, {"heading": "5.1. A better regret bound for UCB-GLM", "text": "A key parameter in determining the remorse of the UCBGLM algorithm is the minimum eigenvalue of Vt. If we add up the minimum eigenvalue of Vt, we will be able to prove an O (\u221a dT) remorse, which for UCB-"}, {"heading": "GLM.", "text": "Theorem 4: We perform the UCB-GLM algorithm in a range of problems. If it is large enough, we assume that there is a large gap."}, {"heading": "5.2. Open Questions", "text": "Computationally efficient algorithms. While UCB-GLM and SupCB-GLM have good theoretical properties, they can be expensive in some applications. Firstly, they require the inversion of a d \u00d7 d matrix at each step, a costly operation if d is large. Secondly, the MLE in step t is calculated using area samples, which means that the complexity per step increases analogously to the approach of Agarwal et al. (2014) with t for easy implementation of the algorithms. Therefore, it is interesting to investigate more scalable alternatives. It is possible to use a first-order iterative optimization method to amortize the costs, analogous to the approach of Roy et al. (2014).K-dependent lower limit. Currently, all subordinate results on (generalized) linear bandits cannot be dependent on the number of actionsK. The minimax subordinate boundary will depend on this subordinate technical boundary, because all subordinate results may be of interest in (although) subordinate situations."}, {"heading": "A. Proof of Theorem 1", "text": "In the following, we will drop the subscript n for simplicity if there is no ambiguity. (Therefore, Vn is called V and so on. (To prove normality-type-results of the maximum probability estimator, we can obtain the desired normality of effectiveness-effectiveness-effectiveness-effectiveness-effectiveness-effectiveness-effectiveness-effectiveness-effectiveness-effectiveness-effectiveness-effectiveness-effectiveness-effectiveness-effectiveness-effectiveness-effectiveness-effectiveness-effectiveness-effectiveness-effectiveness-effectiveness-effectiveness-effectiveness-effectiveness-effectiveness-effectiveness-effectiveness-effectiveness-effectiveness-effectiveness-effectiveness-effectiveness-effectiveness-effectiveness-effectiveness-effectiveness-effectiveness-effectiveness-effectiveness-effectiveness-effectiveness-effectiveness-effectiveness-effectiveness-effectiveness-effectiveness-effectiveness-effectiveness-effectiveness-effectiveness-effectiveness-effectiveness-effectiveness-effectiveness-effectiveness-effectiveness-effectiveness-effectiveness-effectiveness-effectiveness-effectiveness-effectiveness-effectiveness-effectiveness-effectiveness-effectiveness-effectiveness-effectiveness-effectiveness-effectiveness-effectiveness-effectiveness-effectiveness-effectiveness-effectiveness-effectiveness-effectiveness-effectiveness-effectiveness-effectiveness-effectiveness-effectiveness-effectiveness-effectiveness-effectiveness-effectiveness-effectiveness-effectiveness-effectiveness-effectiveness-effectiveness-effectiveness-effectiveness-effectiveness-effectiveness-effectiveness-effectiveness-effectiveness-effectiveness-effectiveness-effectiveness-effectiveness-effectiveness-effectiveness-effectiveness-effectiveness-effectiveness-"}, {"heading": "B. Proof of Proposition 1", "text": "Let X be a random vector drawn from the distribution \u03bd. Let us define Z: = \u03a3 \u2212 1 / 2X. Then Z is isotropic, namely E [ZZ \u2032] = Id. Let us define U = \u2211 n = 1 ZtZ \u2032 t = \u03a3 \u2212 1 / 2V\u03a3 \u2212 1 / 2. From Lemma 1 we have this for each t, with a probability of at least 1 \u2212 2 Exp (\u2212 C2t2), \u03bbmin (U) \u2265 n \u2212 C1\u03c32 \u221a nd \u2212 \u03c32t \u221a n.where the sub-Gaussian parameter Z is limited by the uppermost expression Z, which is highest by probability."}, {"heading": "C. Technical Lemmas and Proofs", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "C.1. Proof of Lemma 7", "text": "Considering that \"Z\" V \u2212 1 = \"V \u2212 1 / 2Z\" 2 = \"A\" 2 \u2264 1 < a, V \u2212 1 / 2Z >, \"B\" should be a 1 / 2 net of the unit sphere Bd, for each x-Bd there is an x-B net, so that x-x, V \u2212 1 / 2Z > + 2. \"Consequently < x, V \u2212 1 / 2Z > = < x, V \u2212 1 / 2Z > + < x, V \u2212 1 / 2Z > + < x, V \u2212 1 / 2Z > + < x, V \u2212 1 / 2Z > < x, V \u2212 1 / 2Z > < V \u2212 1 / 2Z > < V \u2212 2 / 2Z > < V \u2212 1 / 2Z > (V \u2212 1 / 2Z)."}, {"heading": "C.2. Proof of Lemma 2", "text": "According to Abbasi-Yadkori et al. (2011, Lemma 11) we have + n = m + 1 = Xt = 2V \u2212 1t \u2264 2 log detVm + n + 1 \u2264 2d log (tr (Vm + 1) + n d) \u2212 2 log detVm + 1.Note that tr (Vm + 1) = 1 tr (XtX) = 1 tr (XtX) = 1% Xt = 2% Xt \u00b2 m and that det Vm + 1 = 1% Xt \u00b2 dmin (Vm + 1) \u2265 1, where {1} are the eigenvalues of Vm + 1. Applying the Cauchy-Black inequality results in m + n = m + 1% Xt = V \u2212 1t = 1% Xt = m + 1% Xt = m + 1% Xt = 2V \u2212 1t \u2264 2% (n + md)."}, {"heading": "C.3. Proof of Lemma 3", "text": "Define Gt (\u03b8) = \u2264 \u2212 V \u2212 Z \u2212 Z \u2212 Z \u2212 Z \u2212 Z \u2212 Z \u2212 Z \u2212 Z \u2212 Z \u2212 Z \u2212 Z \u2212 Z \u2212 Z \u2212 Z \u2212 Z \u2212 Z \u2212 Z \u2212 Z \u2212 Z \u2212 Z \u2212 Z \u2212 Z \u2212 Z \u2212 Z \u2212 Z \u2212 Z \u2212 Z \u2212 Z \u2212 Z \u2212 Z \u2212 Z \u2212 Z \u2212 Z \u2212 Z \u2212 Z \u2212 Z \u2212 Z \u2212 Z \u2212 Z \u2212 Z \u2212 Z \u2212 Z \u2212 Z \u2212 Z \u2212 Z \u2212 Z \u2212 Z \u2212 Z \u2212 Z \u2212 Z \u2212 Z \u2212 Z \u2212 Z \u2212 Z \u2212 Z \u2212 Z \u2212 Z \u2212 Z \u2212 Z \u2212 Z \u2212 Z \u2212 Z \u2212 Z \u2212 Z \u2212 Z \u2212 Z \u2212 Z \u2212 Z \u2212 Z \u2212 Z \u2212 Z \u2212 Z \u2212 Z \u2212 Z \u2212 Z \u2212 Z \u2212 Z \u2212 Z \u2212 Z \u2212 Z \u2212 Z \u2212 Z \u2212 Z \u2212 Z \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 Z \u2212 Z \u2212 Z \u2212 Z \u2212 Z \u2212 Z \u2212 Z \u2212 Z \u2212 Z \u2212 Z \u2212 Z \u2212 Z \u2212 Z \u2212 Z \u2212 Z \u2212 Z \u2212 Z \u2212 Z \u2212 Z \u2212 Z \u2212 Z \u2212 Z \u2212 Z \u2212 Z \u2212 Z \u2212 Z \u2212 Z \u2212 Z \u2212 Z \u2212 Z \u2212 Z \u2212 Z \u2212 Z \u2212 Z \u2212 Z \u2212 Z \u2212 Z \u2212 Z \u2212 Z \u2212 Z \u2212 Z \u2212 Z \u2212 Z \u2212 Z \u2212 \u2212 Z \u2212 Z \u2212 Z \u2212 Z \u2212 \u2212 \u2212 Z \u2212 \u2212 Z \u2212 Z \u2212 Z \u2212 Z \u2212 Z \u2212 Z \u2212 Z \u2212 Z \u2212 Z \u2212 Z \u2212 Z \u2212 Z \u2212 Z \u2212 Z \u2212 Z \u2212 Z \u2212 Z \u2212 Z \u2212 Z \u2212 Z \u2212 Z \u2212 Z \u2212 Z \u2212 Z \u2212 Z \u2212 Z \u2212 Z \u2212 Z \u2212 Z \u2212 Z \u2212 Z \u2212 Z \u2212 Z \u2212 Z \u2212 Z \u2212 Z \u2212 Z \u2212 Z \u2212 Z \u2212 Z \u2212 Z \u2212 Z \u2212 Z \u2212 Z \u2212 Z \u2212 Z \u2212 Z \u2212 Z \u2212 Z \u2212 Z \u2212 Z \u2212 Z \u2212 Z \u2212 Z \u2212 Z \u2212 Z \u2212 Z \u2212 Z \u2212 Z \u2212 Z \u2212 Z \u2212 Z \u2212 Z \u2212 Z \u2212 Z \u2212 Z \u2212 Z \u2212 Z \u2212 Z \u2212 Z \u2212 Z \u2212 Z \u2212 Z \u2212 Z \u2212 Z \u2212 Z \u2212 Z \u2212 Z \u2212 Z \u2212 Z \u2212 Z \u2212 Z \u2212 Z \u2212 Z \u2212 Z \u2212 Z \u2212 Z \u2212 Z \u2212 Z \u2212 Z \u2212 Z \u2212 Z \u2212 Z \u2212 Z \u2212 Z \u2212 Z \u2212 Z \u2212 Z \u2212 Z \u2212 Z \u2212 Z \u2212 Z \u2212 Z \u2212 Z \u2212 Z \u2212 Z \u2212 Z \u2212 Z \u2212 Z \u2212"}, {"heading": "C.4. Proof of Lemma 6", "text": "We will prove the first part of the dilemma by induction. - It is easy to verify the dilemma for s = 1. Suppose we have a dilemma and we want to prove a dilemma + 1. - Since the algorithm passes to step s + 1, we know from step 2b that | m (s) t, a \u2212 x \u00b2 t, a \u2264 w (s) t, a \u2264 \u2212 2 \u2212 sfor all a \u2212 as. Specifically, it applies to a = a \u00b2 t, since a \u00b2 t is like our induction step. Then, it implies the optimality of a dilemma t (s) t, a \u00b2 t, a \u00b2 t, a \u00b2 t, a \u00b2 t, a \u00b2 t, \"a \u00b2 t, a \u00b2 t, a \u00b2 t,\" a \u00b2 t, \"a \u00b2 t, a \u00b2 t,\" c) t, \"c), c) t, b (s) t, b (s t, b (s) t, b (c (c) t, c, c, c, c) t, c, c, c, c, c, c) t, t, t, t, c, t, t, t, c, t, t, t, t, b (t, t, t, t, t, t, t, t, t, t, c, t, t, c, t, t, t, t, c, t, t, t, t, t, t, c, t, t, t, t, t, t, c, t, t, t, t, t, t, t, t, c, t, t, t, t, t, t, t, c, t, t, t, t, t, t, t, c, t, t, t, c, t, t, t, t, t, c, t, t, t, t, t, c, t, t, t, t, t, t, c, t, t, t, t, t, t, t, t, c, t, t, t, t, t, t, t, t, c, t, t, t, t, t, t, t, t, t, t, t, t, t, t, t, t, t, t"}, {"heading": "C.5. Proof of Lemma 9", "text": "Lemma 9. Let a and b be two positive constants. Ifm \u2265 a2 + 2b, thenm \u2212 a \u221a m \u2212 b \u2265 0.Proof. Supposedly \u2265 a2 + 2b, thenm \u2212 a \u221a m \u2212 b = a2 + b \u2212 a \u221a a2 + 2b \u2265 a2 + b \u2212 a \u221a a2 + 2b + b2 / a2 = a2 + b \u2212 a \u221a (a + b / a) 2 = a2 + b \u2212 a (a + b / a) = 0."}], "references": [{"title": "Improved algorithms for linear stochastic bandits", "author": ["Abbasi-Yadkori", "Yasin", "P\u00e1l", "D\u00e1vid", "Szepesv\u00e1ri", "Csaba"], "venue": "In Advances in Neural Information Processing Systems", "citeRegEx": "Abbasi.Yadkori et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Abbasi.Yadkori et al\\.", "year": 2011}, {"title": "Taming the monster: A fast and simple algorithm for contextual bandits", "author": ["Agarwal", "Alekh", "Hsu", "Daniel", "Kale", "Satyen", "Langford", "John", "Li", "Lihong", "Schapire", "Robert E"], "venue": "In Proceedings of the 31th International Conference on Machine Learning (ICML),", "citeRegEx": "Agarwal et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Agarwal et al\\.", "year": 2014}, {"title": "Online models for content optimization", "author": ["Agarwal", "Deepak", "Chen", "Bee-Chung", "Elango", "Pradheep", "Motgi", "Nitin", "Park", "Seung-Taek", "Ramakrishnan", "Raghu", "Roy", "Scott", "Zachariah", "Joe"], "venue": "In Advances in Neural Information Processing Systems", "citeRegEx": "Agarwal et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Agarwal et al\\.", "year": 2009}, {"title": "Using confidence bounds for exploitation-exploration trade-offs", "author": ["Auer", "Peter"], "venue": "The Journal of Machine Learning Research,", "citeRegEx": "Auer and Peter.,? \\Q2003\\E", "shortCiteRegEx": "Auer and Peter.", "year": 2003}, {"title": "Finite-time analysis of the multiarmed bandit problem", "author": ["Auer", "Peter", "Cesa-Bianchi", "Nicolo", "Fischer", "Paul"], "venue": "Machine learning,", "citeRegEx": "Auer et al\\.,? \\Q2002\\E", "shortCiteRegEx": "Auer et al\\.", "year": 2002}, {"title": "The nonstochastic multiarmed bandit problem", "author": ["Auer", "Peter", "Cesa-Bianchi", "Nicolo", "Freund", "Yoav", "Schapire", "Robert E"], "venue": "SIAM Journal on Computing,", "citeRegEx": "Auer et al\\.,? \\Q2002\\E", "shortCiteRegEx": "Auer et al\\.", "year": 2002}, {"title": "Contextual bandit algorithms with supervised learning guarantees", "author": ["Beygelzimer", "Alina", "Langford", "John", "Li", "Lihong", "Reyzin", "Lev", "Schapire", "Robert E"], "venue": "In Proceedings of the 14th International Conference on Artificial Intelligence and Statistics (AISTATS),", "citeRegEx": "Beygelzimer et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Beygelzimer et al\\.", "year": 2010}, {"title": "Simultaneous analysis of Lasso and Dantzig selector", "author": ["Bickel", "Peter J", "Ritov", "Ya\u2019acov", "Tsybakov", "Alexandre B"], "venue": "The Annals of Statistics,", "citeRegEx": "Bickel et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Bickel et al\\.", "year": 2009}, {"title": "Fundamentals of Statistical Exponential Families with Applications in Statistical Decision Theory, volume 9 of Lecture Notes-Monograph Series", "author": ["Brown", "Lawrence D"], "venue": "Institute of Mathematical Statistics,", "citeRegEx": "Brown and D.,? \\Q1986\\E", "shortCiteRegEx": "Brown and D.", "year": 1986}, {"title": "Regret analysis of stochastic and nonstochastic multi-armed bandit problems", "author": ["Bubeck", "S\u00e9bastien", "Cesa-Bianchi", "Nicolo"], "venue": "Foundations and Trends in Machine Learning,", "citeRegEx": "Bubeck et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Bubeck et al\\.", "year": 2012}, {"title": "An empirical evaluation of Thompson sampling", "author": ["Chapelle", "Olivier", "Li", "Lihong"], "venue": "In Advances in Neural Information Processing Systems", "citeRegEx": "Chapelle et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Chapelle et al\\.", "year": 2012}, {"title": "Strong consistency of maximum quasi-likelihood estimators in generalized linear models with fixed and adaptive designs", "author": ["Chen", "Kani", "Hu", "Inchi", "Ying", "Zhiliang"], "venue": "The Annals of Statistics,", "citeRegEx": "Chen et al\\.,? \\Q1999\\E", "shortCiteRegEx": "Chen et al\\.", "year": 1999}, {"title": "Contextual bandits with linear payoff functions", "author": ["Chu", "Wei", "Li", "Lihong", "Reyzin", "Lev", "Schapire", "Robert E"], "venue": "In Proceedings of the 14th International Conference on Artificial Intelligence and Statistics (AISTATS),", "citeRegEx": "Chu et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Chu et al\\.", "year": 2011}, {"title": "Stochastic linear optimization under bandit feedback", "author": ["Dani", "Varsha", "Hayes", "Thomas P", "Kakade", "ShamM"], "venue": "In Proceedings of the 21st Annual Conference on Learning Theory (COLT),", "citeRegEx": "Dani et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Dani et al\\.", "year": 2008}, {"title": "Consistency and asymptotic normality of the maximum likelihood estimator in generalized linear models", "author": ["Fahrmeir", "Ludwig", "Kaufmann", "Heinz"], "venue": "The Annals of Statistics,", "citeRegEx": "Fahrmeir et al\\.,? \\Q1985\\E", "shortCiteRegEx": "Fahrmeir et al\\.", "year": 1985}, {"title": "Parametric bandits: The generalized linear case", "author": ["Filippi", "Sarah", "Cappe", "Olivier", "Garivier", "Aur\u00e9lien", "Szepesv\u00e1ri", "Csaba"], "venue": "In Advances in Neural Information Processing Systems", "citeRegEx": "Filippi et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Filippi et al\\.", "year": 2010}, {"title": "Asymptotically efficient adaptive allocation rules", "author": ["Lai", "Tze Leung", "Robbins", "Herbert"], "venue": "Advances in Applied Mathematics,", "citeRegEx": "Lai et al\\.,? \\Q1985\\E", "shortCiteRegEx": "Lai et al\\.", "year": 1985}, {"title": "Least squares estimates in stochastic regression models with applications to identification and control of dynamic systems", "author": ["Lai", "Tze Leung", "Wei", "Ching Zong"], "venue": "The Annals of Statistics,", "citeRegEx": "Lai et al\\.,? \\Q1982\\E", "shortCiteRegEx": "Lai et al\\.", "year": 1982}, {"title": "Theory of Point Estimation, volume 31 of Springer Texts in Statistics", "author": ["Lehmann", "Erich Leo", "Casella", "George"], "venue": "Springer Science & Business Media,", "citeRegEx": "Lehmann et al\\.,? \\Q1998\\E", "shortCiteRegEx": "Lehmann et al\\.", "year": 1998}, {"title": "A contextual-bandit approach to personalized news article recommendation", "author": ["Li", "Lihong", "Chu", "Wei", "Langford", "John", "Schapire", "Robert E"], "venue": "In Proceedings of the 19th International Conference on World Wide Web (WWW),", "citeRegEx": "Li et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Li et al\\.", "year": 2010}, {"title": "An unbiased offline evaluation of contextual bandit algorithms with generalized linear models", "author": ["Li", "Lihong", "Chu", "Wei", "Langford", "John", "Moon", "Taesup", "Wang", "Xuanhui"], "venue": "JMLR Workshop and Conference Proceedings,", "citeRegEx": "Li et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Li et al\\.", "year": 2012}, {"title": "Generalized Linear Models, volume 37", "author": ["McCullagh", "Peter", "Nelder", "John A"], "venue": "CRC press,", "citeRegEx": "McCullagh et al\\.,? \\Q1989\\E", "shortCiteRegEx": "McCullagh et al\\.", "year": 1989}, {"title": "Empirical processes: Theory and applications. In NSF-CBMS regional conference series in probability and statistics, pp. i\u201386", "author": ["Pollard", "David"], "venue": "JSTOR,", "citeRegEx": "Pollard and David.,? \\Q1990\\E", "shortCiteRegEx": "Pollard and David.", "year": 1990}, {"title": "Linearly parameterized bandits", "author": ["Rusmevichientong", "Paat", "Tsitsiklis", "John N"], "venue": "Mathematics of Operations Research,", "citeRegEx": "Rusmevichientong et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Rusmevichientong et al\\.", "year": 2010}, {"title": "Learning to optimize via posterior sampling", "author": ["Russo", "Daniel", "Van Roy", "Benjamin"], "venue": "Mathematics of Operations Research,", "citeRegEx": "Russo et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Russo et al\\.", "year": 2014}, {"title": "One-armed bandit problems with covariates", "author": ["Sarkar", "Jyotirmoy"], "venue": "The Annals of Statistics,", "citeRegEx": "Sarkar and Jyotirmoy.,? \\Q1991\\E", "shortCiteRegEx": "Sarkar and Jyotirmoy.", "year": 1991}, {"title": "On the likelihood that one unknown probability exceeds another in view of the evidence of two samples", "author": ["Thompson", "William R"], "venue": null, "citeRegEx": "Thompson and R.,? \\Q1933\\E", "shortCiteRegEx": "Thompson and R.", "year": 1933}, {"title": "Spectral bandits for smooth graph functions", "author": ["Valko", "Michal", "Munos", "R\u00e9mi", "Kveton", "Branislav", "Koc\u00e1k", "Tom\u00e1\u0161"], "venue": "In Proceedings of the 31th International Conference on Machine Learning (ICML),", "citeRegEx": "Valko et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Valko et al\\.", "year": 2014}, {"title": "Introduction to the non-asymptotic analysis of random matrices", "author": ["Vershynin", "Roman"], "venue": "Compressed Sensing: Theory and Applications,", "citeRegEx": "Vershynin and Roman.,? \\Q2012\\E", "shortCiteRegEx": "Vershynin and Roman.", "year": 2012}, {"title": "A one-armed bandit problem with a concomitant variable", "author": ["Woodroofe", "Michael"], "venue": "Journal of the American Statistical Association,", "citeRegEx": "Woodroofe and Michael.,? \\Q1979\\E", "shortCiteRegEx": "Woodroofe and Michael.", "year": 1979}, {"title": "\u03b8\u2217\u2016 , where the last inequality is due to the fact that F (\u03b8\u0304", "author": ["\u2265 \u03ba\u03b7\u03bbmin(V ) \u2016\u03b8"], "venue": "\u03ba\u03b7V . On the other hand, Lemma A of Chen et al", "citeRegEx": "\u2212,? \\Q1999\\E", "shortCiteRegEx": "\u2212", "year": 1999}, {"title": "Z\u3009, let B\u0302 be a 1/2-net of the unit ball B. Then |B\u0302| \u2264 6 (Pollard, 1990, Lemma 4.1), and for any x \u2208 B, there is a x\u0302 \u2208 B\u0302 such that \u2016x\u2212 x\u0302", "author": ["V \u3008a"], "venue": null, "citeRegEx": ".a,? \\Q1990\\E", "shortCiteRegEx": ".a", "year": 1990}], "referenceMentions": [{"referenceID": 2, "context": "With the development of modern technologies, contextual bandit problems have more applications, especially in web-based recommendation, advertising and search (Agarwal et al., 2009; Li et al., 2010; 2012).", "startOffset": 159, "endOffset": 204}, {"referenceID": 19, "context": "With the development of modern technologies, contextual bandit problems have more applications, especially in web-based recommendation, advertising and search (Agarwal et al., 2009; Li et al., 2010; 2012).", "startOffset": 159, "endOffset": 204}, {"referenceID": 13, "context": "The most studied model in contextual bandits literature is the linear model (Auer, 2003; Dani et al., 2008; Rusmevichientong & Tsitsiklis, 2010; Chu et al., 2011; Abbasi-Yadkori et al., 2011), in which the expected rewards at each round is a linear combination of features in the context vector.", "startOffset": 76, "endOffset": 191}, {"referenceID": 12, "context": "The most studied model in contextual bandits literature is the linear model (Auer, 2003; Dani et al., 2008; Rusmevichientong & Tsitsiklis, 2010; Chu et al., 2011; Abbasi-Yadkori et al., 2011), in which the expected rewards at each round is a linear combination of features in the context vector.", "startOffset": 76, "endOffset": 191}, {"referenceID": 0, "context": "The most studied model in contextual bandits literature is the linear model (Auer, 2003; Dani et al., 2008; Rusmevichientong & Tsitsiklis, 2010; Chu et al., 2011; Abbasi-Yadkori et al., 2011), in which the expected rewards at each round is a linear combination of features in the context vector.", "startOffset": 76, "endOffset": 191}, {"referenceID": 20, "context": "Logistic regression model based algorithms have been shown to have substantial improvements over linear models (Li et al., 2012).", "startOffset": 111, "endOffset": 128}, {"referenceID": 0, "context": ", 2002a; Bubeck & Cesa-Bianchi, 2012) to linear bandits (Auer, 2003; Abbasi-Yadkori et al., 2011).", "startOffset": 56, "endOffset": 97}, {"referenceID": 20, "context": "While some UCB-type algorithms using GLMs perform well empirically (Li et al., 2012), there is little theoretical study of them.", "startOffset": 67, "endOffset": 84}, {"referenceID": 0, "context": ", 2011; Abbasi-Yadkori et al., 2011), in which the expected rewards at each round is a linear combination of features in the context vector. The linear model is theoretically convenient to work on. However, in practice, we usually have binary rewards (click or not, treatment working or not). Logistic regression model based algorithms have been shown to have substantial improvements over linear models (Li et al., 2012). Hence, we consider generalized linear models (GLM) in the contextual bandit setting, in which linear, logistic and probit regression serve as three important special cases. The celebrated work of Lai & Robbins (1985) first introduces the upper confidence bound (UCB) approach to efficient exploration.", "startOffset": 8, "endOffset": 640}, {"referenceID": 6, "context": "Later variants of the EXP4 algorithm (Beygelzimer et al., 2010; Agarwal et al., 2014) give an \u00d5( \u221a dKT ) regret that is near-optimal with respect to T .", "startOffset": 37, "endOffset": 85}, {"referenceID": 1, "context": "Later variants of the EXP4 algorithm (Beygelzimer et al., 2010; Agarwal et al., 2014) give an \u00d5( \u221a dKT ) regret that is near-optimal with respect to T .", "startOffset": 37, "endOffset": 85}, {"referenceID": 10, "context": "This rate improves the state-of-the-art results of Filippi et al. (2010) by a \u221a d factor, assuming the number of arms is fixed.", "startOffset": 51, "endOffset": 73}, {"referenceID": 10, "context": "This rate improves the state-of-the-art results of Filippi et al. (2010) by a \u221a d factor, assuming the number of arms is fixed. Moreover, it matches the GLM bandits problem\u2019s minimax lower bound indicated by the linear bandits problem and thus is optimal. SupCB-GLM is inspired by the seminal work of Auer (2003), which introduced a technique to construct independence samples in linear contextual bandits.", "startOffset": 51, "endOffset": 313}, {"referenceID": 10, "context": "This rate improves the state-of-the-art results of Filippi et al. (2010) by a \u221a d factor, assuming the number of arms is fixed. Moreover, it matches the GLM bandits problem\u2019s minimax lower bound indicated by the linear bandits problem and thus is optimal. SupCB-GLM is inspired by the seminal work of Auer (2003), which introduced a technique to construct independence samples in linear contextual bandits. A key observation in proving this result is that the l2 confidence ball of the unknown parameter is insufficient to calculate a sharp upper confidence bound, yet what we need is the confidence interval in all directions. Thus, we prove a finite sample normality type confidence bound for the maximum likelihood estimator of GLM. To our best knowledge, this is the first non-asymptotic normality type result for the GLM and might be of its own theoretical value. We also analyze a simple version of UCB algorithm called UCB-GLM that is widely used in practice. We prove it also achieves the optimal regret bound under a reasonable assumption. These results shed light on explaining the good empirical performance of GLM bandits in practice. Related Work The study of GLM bandits problem goes back at least to Sarkar (1991), who considered discounted regrets rather than cumulative regerts.", "startOffset": 51, "endOffset": 1229}, {"referenceID": 10, "context": "This rate improves the state-of-the-art results of Filippi et al. (2010) by a \u221a d factor, assuming the number of arms is fixed. Moreover, it matches the GLM bandits problem\u2019s minimax lower bound indicated by the linear bandits problem and thus is optimal. SupCB-GLM is inspired by the seminal work of Auer (2003), which introduced a technique to construct independence samples in linear contextual bandits. A key observation in proving this result is that the l2 confidence ball of the unknown parameter is insufficient to calculate a sharp upper confidence bound, yet what we need is the confidence interval in all directions. Thus, we prove a finite sample normality type confidence bound for the maximum likelihood estimator of GLM. To our best knowledge, this is the first non-asymptotic normality type result for the GLM and might be of its own theoretical value. We also analyze a simple version of UCB algorithm called UCB-GLM that is widely used in practice. We prove it also achieves the optimal regret bound under a reasonable assumption. These results shed light on explaining the good empirical performance of GLM bandits in practice. Related Work The study of GLM bandits problem goes back at least to Sarkar (1991), who considered discounted regrets rather than cumulative regerts. They prove that a myopic rule without exploration is asymptotically optimal. Recently, Filippi et al. (2010) study the same stochastic GLM bandit problem considered here.", "startOffset": 51, "endOffset": 1405}, {"referenceID": 15, "context": "Note that this assumption is weaker than Assumption 1 in Filippi et al. (2010), as it only requires to control the local behavior of \u03bc\u0307(x\u03b8) near \u03b8.", "startOffset": 57, "endOffset": 79}, {"referenceID": 11, "context": ", 2009) and generalized linear models (Fahrmeir & Kaufmann, 1985; Chen et al., 1999).", "startOffset": 38, "endOffset": 84}, {"referenceID": 30, "context": "We give a proof sketch here, and the full proof is found in the appendix. In the following, for simplicity, we will drop the subscript n when there is no ambiguity. Therefore,Vn is denotedV and so on. We will need a technical lemma, which is an existing result in randommatrix theory. The version we presented here is adapted from Equation (5.23) of Theorem 5.39 from Vershynin (2012). Lemma 1.", "startOffset": 7, "endOffset": 385}, {"referenceID": 30, "context": "\u2225 = \u03bb \u22121/2 min (\u03a3) (see, e.g., Vershynin (2012)).", "startOffset": 6, "endOffset": 48}, {"referenceID": 0, "context": ", 2002a) and linear bandits (Abbasi-Yadkori et al., 2011; Auer, 2003; Chu et al., 2011; Dani et al., 2008).", "startOffset": 28, "endOffset": 106}, {"referenceID": 12, "context": ", 2002a) and linear bandits (Abbasi-Yadkori et al., 2011; Auer, 2003; Chu et al., 2011; Dani et al., 2008).", "startOffset": 28, "endOffset": 106}, {"referenceID": 13, "context": ", 2002a) and linear bandits (Abbasi-Yadkori et al., 2011; Auer, 2003; Chu et al., 2011; Dani et al., 2008).", "startOffset": 28, "endOffset": 106}, {"referenceID": 14, "context": ", the distribution \u03bd) is only needed to ensure V\u03c4+1 is invertable (similar to the first phase in the algorithm of Filippi et al. (2010)); the rest of our analysis does not depend on this stochastic assumption.", "startOffset": 114, "endOffset": 136}, {"referenceID": 0, "context": ", Abbasi-Yadkori et al. (2011)).", "startOffset": 2, "endOffset": 31}, {"referenceID": 0, "context": "We instead use results on self-normalized martingales (Abbasi-Yadkori et al., 2011), together with a finitetime normality result like Theorem 1, to prove the next theorem.", "startOffset": 54, "endOffset": 83}, {"referenceID": 13, "context": "Indeed, this rate matches the minimax lower bound up to logarithm factor for the infinite actions contextual bandit problems (Dani et al., 2008).", "startOffset": 125, "endOffset": 144}, {"referenceID": 13, "context": "Indeed, this rate matches the minimax lower bound up to logarithm factor for the infinite actions contextual bandit problems (Dani et al., 2008). By choosing \u03b4 = 1/T and using the fact that RT \u2264 T , this highprobability result implies a bound on the expected regret: E[RT ] = \u00d5(d \u221a T ). Our result improves the previous regret bound of Filippi et al. (2010) by a \u221a logT factor.", "startOffset": 126, "endOffset": 358}, {"referenceID": 13, "context": "Indeed, this rate matches the minimax lower bound up to logarithm factor for the infinite actions contextual bandit problems (Dani et al., 2008). By choosing \u03b4 = 1/T and using the fact that RT \u2264 T , this highprobability result implies a bound on the expected regret: E[RT ] = \u00d5(d \u221a T ). Our result improves the previous regret bound of Filippi et al. (2010) by a \u221a logT factor. Moreover, the algorithm proposed in Filippi et al. (2010) involves a projection step, which is computationally more expensive comparing to UCB-GLM.", "startOffset": 126, "endOffset": 436}, {"referenceID": 20, "context": "Algorithm SupCB-GLM While the algorithm UCB-GLM performs sufficiently well in practice (Li et al., 2012), it is unclear whether it can achieve the optimal rates of O( \u221a dT logK), when K is small.", "startOffset": 87, "endOffset": 104}, {"referenceID": 19, "context": "Algorithm SupCB-GLM While the algorithm UCB-GLM performs sufficiently well in practice (Li et al., 2012), it is unclear whether it can achieve the optimal rates of O( \u221a dT logK), when K is small. As mentioned in Section 4.1, the key technical difficulty in analyzing UCB-GLM is the dependence between samples. Inspired by a technique developed by Auer (2003) to create independent samples for linear contextual bandits, we propose another algorithm SupCB-GLM (Algorithm 3), which uses algorithm CB-GLM (Algorithm 2) as a subroutine.", "startOffset": 88, "endOffset": 359}, {"referenceID": 27, "context": "Also, we would like to point out that, unlike SpectralEliminator (Valko et al., 2014), the algorithm can easily handle a changing action set.", "startOffset": 65, "endOffset": 85}, {"referenceID": 29, "context": "If w (s) t,a > 2 \u2212s for some a, we need to do more exploration on xt,a and thus we choose this action. Otherwise, the actions are filtered in step 2d such that the actions passed to the next stage are close enough to the optimal action. Since all the widths are smaller than 2, if m (s) t,a < m (s) t,j \u2212 2 \u00b7 2 for some j \u2208 As, the action a can not be the optimal action. The filter process terminates when we have already got accurate estimate of all xt,a\u03b8 \u2217 up to the 1/ \u221a T level and we do not need to do exploration. Thus in step 2c we just choose the action that maximizes the estimated mean value. Our algorithm is different from the algorithm SupLinRel in Auer (2003) that we directly maximize the mean, rather than the upper confidence bound, in steps c and d.", "startOffset": 17, "endOffset": 675}, {"referenceID": 15, "context": "Both in Theorem 2 and in Filippi et al. (2010), |x(\u03b8\u0302n \u2212 \u03b8\u2217)| is upper bounded by using the Cauchy-Schwartz inequality, |x(\u03b8\u0302n \u2212 \u03b8)| \u2264 \u2016x\u2016V \u22121 n \u2225", "startOffset": 25, "endOffset": 47}, {"referenceID": 31, "context": "This will lead to an extra \u221a d factor compared to (5). By using Cauchy-Schwartz (10), we only make use of the fact that \u03b8\u0302n is close to \u03b8 \u2217 in the l2 sense. However, (5) tells us that actually \u03b8\u0302n is close to \u03b8 \u2217 in every direction. This is the reason why we are able to remove the extra \u221a d factor to achieve a near-optimal regret. It also explains why the bound in Theorem 2 is tight when K is large. As K goes large, it is likely there is a direction x for which (10) is tight. Proof of Theorem 3. To facilitate our proof, we first present two technical lemmas. Lemma 4, Theorem 1, Theorem 5.39 of Vershynin (2012) together with a union bound yield Lemma 5.", "startOffset": 11, "endOffset": 618}], "year": 2017, "abstractText": "Contextual bandits are widely used in Internet services from news recommendation to advertising. Generalized linear models (logistical regression in particular) have demonstrated stronger performance than linear models in many applications. However, most theoretical analyses on contextual bandits so far are on linear bandits. In this work, we propose an upper confidence bound based algorithm for generalized linear contextual bandits, which achieves an \u00d5( \u221a dT ) regret over T rounds with d dimensional feature vectors. This regret matches the minimax lower bound, up to logarithmic terms, and improves on the best previous result by a \u221a d factor, assuming the number of arms is fixed. A key component in our analysis is to establish a new, sharp finite-sample confidence bound for maximumlikelihood estimates in generalized linear models, which may be of independent interest. We also analyze a simpler upper confidence bound algorithm, which is useful in practice, and prove it to have optimal regret for the certain cases.", "creator": "LaTeX with hyperref package"}}}