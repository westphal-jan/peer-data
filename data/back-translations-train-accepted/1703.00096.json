{"id": "1703.00096", "review": {"conference": "ICML", "VERSION": "v1", "DATE_OF_SUBMISSION": "1-Mar-2017", "title": "Gram-CTC: Automatic Unit Selection and Target Decomposition for Sequence Labelling", "abstract": "Most existing sequence labelling models rely on a fixed decomposition of a target sequence into a sequence of basic units. These methods suffer from two major drawbacks: 1) the set of basic units is fixed, such as the set of words, characters or phonemes in speech recognition, and 2) the decomposition of target sequences is fixed. These drawbacks usually result in sub-optimal performance of modeling sequences. In this pa- per, we extend the popular CTC loss criterion to alleviate these limitations, and propose a new loss function called Gram-CTC. While preserving the advantages of CTC, Gram-CTC automatically learns the best set of basic units (grams), as well as the most suitable decomposition of tar- get sequences. Unlike CTC, Gram-CTC allows the model to output variable number of characters at each time step, which enables the model to capture longer term dependency and improves the computational efficiency. We demonstrate that the proposed Gram-CTC improves CTC in terms of both performance and efficiency on the large vocabulary speech recognition task at multiple scales of data, and that with Gram-CTC we can outperform the state-of-the-art on a standard speech benchmark.", "histories": [["v1", "Wed, 1 Mar 2017 00:59:17 GMT  (1555kb,D)", "http://arxiv.org/abs/1703.00096v1", null], ["v2", "Sat, 12 Aug 2017 00:02:26 GMT  (1445kb,D)", "http://arxiv.org/abs/1703.00096v2", "Published at ICML 2017"]], "reviews": [], "SUBJECTS": "cs.CL cs.LG cs.NE", "authors": ["hairong liu", "zhenyao zhu", "xiangang li", "sanjeev satheesh"], "accepted": true, "id": "1703.00096"}, "pdf": {"name": "1703.00096.pdf", "metadata": {"source": "META", "title": "Gram-CTC: Automatic Unit Selection and Target Decomposition for Sequence Labelling", "authors": ["Hairong Liu", "Zhenyao Zhu", "Xiangang Li", "Sanjeev Satheesh"], "emails": ["LIUHAIRONG@BAIDU.COM", "ZHENYAOZHU@BAIDU.COM", "LIXIANGANG@BAIDU.COM", "SANJEEVSATHEESH@BAIDU.COM"], "sections": [{"heading": "1. Introduction", "text": "In fact, most of them will be able to play by the rules they have set themselves in order to play by the rules."}, {"heading": "2. Related Work", "text": "The basic text units previously used for text prediction tasks (e.g. automatic speech recognition, handwriting recognition, machine translation and caption) can generally be divided into two categories: handmade and learning-based units. Handmade base units. Fixed character sets (Graves et al., 2006; Amodei et al., 2015), word pieces (Wu et al., 2016b; Collobert et al., 2016a), words (Soltau et al., 2016; S\u00e9bastien et al., 2015) and phonemes (Lee and Hon, 1988; Goel et al al al, 2016) have been widely used as basic units for text prediction, but all have drawbacks."}, {"heading": "3. Gram-CTC", "text": "In this section we describe the proposed Gram-CTC algorithm. As it is very similar to the CTC criteria (Graves et al., 2006), we emphasize the differences between them."}, {"heading": "3.1. Probability of a Target Sequence", "text": "Let G be a set of n-grams of the base unit C of the target sequence, and we refer to TC networks (TC = TC) for the length of the longest gram in G. A gram-CTC network has a softmax output layer with | G | + 1 units, that is, the probability of all grams in G and an additional symbol, blank. To simplify the problem, we also assume that there can be no valid degradation steps for some target sequences if C is 6 G. Since Gram-CTC will find out the ideal degradation of target sequences in grams during training, this condition is harmless, and only the preservation that there is at least one valid degradation step for each target sequence. For an input sequence x of the length T, let y = Nw (x) is the sequence of network outputs, denoted by ytk as the probability of the k-th gram at the time t where the index is the gram in G."}, {"heading": "3.2. The Forward-Backward Algorithm", "text": "In our case, the state must contain all the information necessary to identify all valid extensions of an incomplete path in such a way that the collapsing function will ultimately collapse the whole chain back to l. For Gram-CTC, this can be done by collapsing all but the last element of the path. Thus, the state is a tuple (l1: i, j), where the first element is a collapsed path representing a prefix of the target tag, and j {0,.) is the length of the last gram (li \u2212 j + 1: i) used to make the prefix. j = 0 is valid and means that Blank was used."}, {"heading": "3.3. BackPropagation", "text": "Similar to the CTC, we have the following expression: p (l | x) = \"s\" s \"S\" t (s) \u03b2t (s) yts \"s\" t \"(1,.., T.\" (11) The derivative in relation to \"ytk\" is: \"s\" s \"s\" s \"s\" s \"lab\" (l, k) \u03b2t \"(s) (12), where\" lab \"(l, k) is the sentence of states in S. whose corresponding gram is k. This is because there may be several states corresponding to the same gram. For reproduction, the most important formula is the partial derivation of the loss in relation to the unnormalized output.\" \"s\" ln \"p\" s. \"(l | x)\" fs. \"utk\" s. \"\u2212 1 ytk\" s \"s\" s \"s\" lab \"(l, k)\" s \"s.\""}, {"heading": "4. Methodology", "text": "Here we describe additional techniques that we found useful in practice to make the Gram-CTC work both efficiently and effectively."}, {"heading": "4.1. Iterative Gram Selection", "text": "Although Gram-CTC can automatically select useful grams, it is difficult to train with a large G. The total number of possible grams is usually huge. In English, for example, we have 26 characters, then the total number of bi-grams is 262 = 676, the total number of tri-grams is 263 = 17576,... that grows exponentially and quickly becomes insoluble. However, it is unnecessary to consider many grams like \"aaaa,\" which are obviously useless. In our experiments, we first remove the most useless grams from the statistics of a huge corpus, that is, we count the frequency of each gram in the corpus and drop those grams with rare frequencies. Then, we train a model with Gram-CTC on all remaining grams. By applying (decrypting) the trained model to a large language record (decrypting), we get the real statistics of the use of grams. Ultimately, we choose radio frequency along with all our Uni-gram."}, {"heading": "4.2. Joint Training with Vanilla CTC", "text": "Gram-CTC has to solve both decomposition and alignment tasks, which is more difficult for a model to learn than CTC. This often manifests itself in unstable training curves that force us to lower the learning rate, which in turn leads to models converging to a worse optima. To overcome this difficulty, we found it beneficial to train a model with both Gram-CTC and vanilla CTC loss (similar to the joint training of multiple sequence labeling targets mentioned in (Sak et al., 2015) (Kim et al., 2016; Kim and Rush, 2016)."}, {"heading": "5. Experiments", "text": "We test the Gram-CTC loss on the ASR task, while both the CTC and the introduced Gram-CTC are generic techniques for other sequence marking tasks. In all experiments, the model specification and training procedure are the same as in (Amodei et al., 2015) - The model is a recursive neural network (RNN) with two two two two-dimensional Convolutionary input layers followed by K forward (Fwd) or bidirectional (Bidi) gated recurrent layers, each N cells and a fully connected layer in front of a Softmax layer. In short, such a model is written as \"2x2D Conv - KxN GRU.\" The network is practiced end-to-end with the CTC, GramCTC or a weighted combination of both layers, which is described in the previous section."}, {"heading": "5.1. Data and Setup", "text": "The Wall Street Journal (WSJ). This corpora consists primarily of read speeches with texts drawn from a machine-readable corpus of the Wall Street Journal news text, and contains approximately 80 hours of voice data. We used the default configuration of the train data set si284 for training, dev93 for validation, and Evalu92 for testing. This is a relatively \"clean\" task that is often used for model prototyping (Miao et al., 2015; Bahdanau et al., 2016; Zhang et al., 2016; Chan et al., 2016b).Fisher switchboard. This is a commonly used telephone call company, which contains 2,300 hours of CTS data. Following previous work (Zweig et al., 2016b; Povey et al., 2016; Xiong et al., 2016b; Sercu et al., 2016, and Goel test method, both the SWB SWB and SWT 10000 data collection is performed on the basis of the SWT-SWT."}, {"heading": "5.2. Gram Selection", "text": "We use the WSJ dataset to show various strategies for selecting gram by gram CTC, as it is a widely used dataset that is also small enough to quickly verify the idea. However, because it is small, we cannot use large grams here due to the problem of data economy. Thus, the auto-refined gram set to WSJ is not optimal for other larger datasets in which larger grams could be used effectively, but the process of refinement is the same for them. We first train a model that uses all the unigrams and bi-grams (29 unigrams and 262 = 676 bi-grams, a total of 705 grams), and then decode with the obtained model on another language dataset to obtain the statistics on the use of grams. The top 100 bi-grams along with all 29 unigrams (auto-refined grams) are used only for the second round of training."}, {"heading": "5.3. Sequence Labelling in Large Stride", "text": "Using a large time step for sequence tagging with RNNs can greatly increase the overall efficiency of the calculation, since it effectively reduces the time steps for the recurring calculation, thus speeding up the process of both forward and backward propagation. However, the largest step that can be used is limited by the gram we use. The (unique) CTC must work in a high time resolution (small step) to have enough frames to output each character. This is very inefficient as we know that the same acoustic feature could correspond to several grams of different length (e.g. \"i,\" \"\" degree, \"\" eye \"). The larger the grams, the larger we are able to output each character. DS2 (Amodei et al., 2015) did not deal with overlapping bigrams to allow a larger step."}, {"heading": "5.4. Decoding Examples", "text": "Figure 3 illustrates the maximum decoding results of both CTC and gram-CTC using nine expressions. Here, GramCTC uses the set of all characters, and the set for GramCTC is an automatically refined set of grams containing all unigrams and some high-frequency higher-value grammars. GramCTC uses step 4 here, while CTC uses step 2.From Figure 3, we can see the following: 1) gram-CTC automatically finds many intuitive and meaningful grammars, such as \"those,\" \"ng\" and \"are.\" 2) In addition, it splits the sentences each time into segments that are significant in terms of pronunciation. This decoding is similar to phonetic decomposition, but is more granular and probably more natural. 3) Since gram-CTC predicts some of the characters (one gram), each prediction will use a larger context, and these characters in the same predicted chunk are therefore potentially more robust."}, {"heading": "5.5. Comparison with Other Methods", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "5.5.1. WSJ DATASET", "text": "The model used here is [2x2D conv, 3x1280 Bidi GRU] with a CTC or gram CTC loss. The results are in Table 3. For all models we train, the language model can significantly improve their performance, in the sense of WHO. Although this dataset contains only a very limited amount of text data for the selection and decomposition of learning programs, GramCTC can still significantly improve the vanilla CTC."}, {"heading": "5.5.2. FISHER-SWITCHBOARD", "text": "The acoustic model trained here consists of two 2D turns and six bidirectional GRU layers in the dimension 2048. The corresponding labels are used for training N-gram speech models. \u2022 Switchboard English speech 97S62 \u2022 Fisher English speech Part 1 - 2004S13, 2004T19 \u2022 Fisher English speech Part 2 - 2005S13, 2005T19We use an example of the Switchboard 1 part of the NIST 2002 dataset (2004S11 RT-02) to adjust hyperparameters of the language model. The evaluation is based on the NIST 2000 set. This configuration is a standard benchmark for evaluating ASR models. The results are in Table 4.We compare our model with the best published results on domain data. These results can often be improved by using data outside the domain for training the language model and sometimes also the acoustic model."}, {"heading": "5.5.3. 10K SPEECH DATASET", "text": "Finally, we are experimenting with a large noisy dataset collected by ourselves to build extensive Continuous Speech Recognition (LVCSR) systems. This dataset contains approximately 10,000 hours of speech in a variety of scenarios, such as far field, background noise, accents. In all cases, the model [2x2D Conv, 3x2560 Fwd GRU, LA Conv] refers to a predictive folding layer as in Amodei et al., 2015) that works with forward-facing RNNNs only for use. As with the Fisher Switchboard dataset, the optimal step 4 for Gram-CTC and 2 for Vanilla CTC on this dataset is therefore, in both experiments, both Gram-CTC and Vanilla CTC + Gram-CTC are more effectively reduced to 27.5% by additional training with Cilla-CTC joints."}, {"heading": "6. Conclusions and Future Work", "text": "Our extensive experiments show that the proposed Gram-CTC enables models to work more efficiently than the vanilla CTC by using larger increments while achieving better performance in sequence marking. Comparative experiments on multi-scale datasets show that the proposed Gram-CTC achieves current results in various ASR tasks. We will continue to explore techniques to improve the optimization of Gram-CTC loss and the application of Gram-CTC to other sequence labeling tasks."}], "references": [{"title": "Learning phrase representations using rnn encoder-decoder for statistical machine translation", "author": ["Kyunghyun Cho", "Bart Van Merri\u00ebnboer", "Caglar Gulcehre", "Dzmitry Bahdanau", "Fethi Bougares", "Holger Schwenk", "Yoshua Bengio"], "venue": "arXiv preprint arXiv:1406.1078,", "citeRegEx": "Cho et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Cho et al\\.", "year": 2014}, {"title": "Sequence to sequence learning with neural networks", "author": ["Ilya Sutskever", "Oriol Vinyals", "Quoc V Le"], "venue": "In Advances in neural information processing systems,", "citeRegEx": "Sutskever et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Sutskever et al\\.", "year": 2014}, {"title": "Listen, attend and spell: A neural network for large vocabulary conversational speech recognition", "author": ["William Chan", "Navdeep Jaitly", "Quoc Le", "Oriol Vinyals"], "venue": "IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP),", "citeRegEx": "Chan et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Chan et al\\.", "year": 2016}, {"title": "Deep speech: Scaling up end-to-end speech recognition", "author": ["Awni Y. Hannun", "Carl Case", "Jared Casper", "Bryan Catanzaro", "Greg Diamos", "Erich Elsen", "Ryan Prenger", "Sanjeev Satheesh", "Shubho Sengupta", "Adam Coates", "Andrew Y. Ng"], "venue": "CoRR, abs/1412.5567,", "citeRegEx": "Hannun et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Hannun et al\\.", "year": 2014}, {"title": "End-to-end attention-based large vocabulary speech recognition", "author": ["Dzmitry Bahdanau", "Jan Chorowski", "Dmitriy Serdyuk", "Yoshua Bengio"], "venue": "In 2016 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP),", "citeRegEx": "Bahdanau et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Bahdanau et al\\.", "year": 2016}, {"title": "On using very large target vocabulary for neural machine translation", "author": ["Jean S\u00e9bastien", "Kyunghyun Cho", "Roland Memisevic", "Yoshua Bengio"], "venue": null, "citeRegEx": "S\u00e9bastien et al\\.,? \\Q2015\\E", "shortCiteRegEx": "S\u00e9bastien et al\\.", "year": 2015}, {"title": "Grammar as a foreign language", "author": ["Oriol Vinyals", "\u0141ukasz Kaiser", "Terry Koo", "Slav Petrov", "Ilya Sutskever", "Geoffrey Hinton"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Vinyals et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Vinyals et al\\.", "year": 2015}, {"title": "Attention-based models for speech recognition", "author": ["Jan K Chorowski", "Dzmitry Bahdanau", "Dmitriy Serdyuk", "Kyunghyun Cho", "Yoshua Bengio"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Chorowski et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Chorowski et al\\.", "year": 2015}, {"title": "The microsoft 2016 conversational speech recognition system", "author": ["W Xiong", "J Droppo", "X Huang", "F Seide", "M Seltzer", "A Stolcke", "D Yu", "G Zweig"], "venue": "arXiv preprint arXiv:1609.03528,", "citeRegEx": "Xiong et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Xiong et al\\.", "year": 2016}, {"title": "Latent sequence decompositions", "author": ["William Chan", "Yu Zhang", "Quoc Le", "Navdeep Jaitly"], "venue": "In Arxiv,", "citeRegEx": "Chan et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Chan et al\\.", "year": 2016}, {"title": "Wav2letter: an end-to-end convnet-based speech recognition system", "author": ["Ronan Collobert", "Christian Puhrsch", "Gabriel Synnaeve"], "venue": "arXiv preprint arXiv:1609.03193,", "citeRegEx": "Collobert et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Collobert et al\\.", "year": 2016}, {"title": "Advances in all-neural speech recognition", "author": ["Geoffrey Zweig", "Chengzhu Yu", "Jasha Droppo", "Andreas Stolcke"], "venue": "arXiv preprint arXiv:1609.05935,", "citeRegEx": "Zweig et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Zweig et al\\.", "year": 2016}, {"title": "Neural speech recognizer: Acoustic-to-word lstm model for large vocabulary speech recognition", "author": ["Hagen Soltau", "Hank Liao", "Hasim Sak"], "venue": "arXiv preprint arXiv:1610.09975,", "citeRegEx": "Soltau et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Soltau et al\\.", "year": 2016}, {"title": "Large-vocabulary speakerindependent continuous speech recognition using hmm", "author": ["K-F Lee", "H-W Hon"], "venue": "In Acoustics, Speech, and Signal Processing,", "citeRegEx": "Lee and Hon.,? \\Q1988\\E", "shortCiteRegEx": "Lee and Hon.", "year": 1988}, {"title": "Dense prediction on sequences with time-dilated convolutions for speech recognition", "author": ["Tom Sercu", "Vaibhava Goel"], "venue": "arXiv preprint arXiv:1611.09288,", "citeRegEx": "Sercu and Goel.,? \\Q2016\\E", "shortCiteRegEx": "Sercu and Goel.", "year": 2016}, {"title": "Achieving human parity in conversational speech recognition", "author": ["Wayne Xiong", "Jasha Droppo", "Xuedong Huang", "Frank Seide", "Mike Seltzer", "Andreas Stolcke", "Dong Yu", "Geoffrey Zweig"], "venue": "arXiv preprint arXiv:1610.05256,", "citeRegEx": "Xiong et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Xiong et al\\.", "year": 2016}, {"title": "Listen, attend and spell", "author": ["William Chan", "Navdeep Jaitly", "Quoc V Le", "Oriol Vinyals"], "venue": "arXiv preprint arXiv:1508.01211,", "citeRegEx": "Chan et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Chan et al\\.", "year": 2015}, {"title": "Phoneme-grapheme based speech recognition system", "author": ["Mathew Magimai Doss", "Todd A Stephenson", "Herv\u00e9 Bourlard", "Samy Bengio"], "venue": "In Automatic Speech Recognition and Understanding,", "citeRegEx": "Doss et al\\.,? \\Q2003\\E", "shortCiteRegEx": "Doss et al\\.", "year": 2003}, {"title": "Hierarchical multiscale recurrent neural networks", "author": ["Junyoung Chung", "Sungjin Ahn", "Yoshua Bengio"], "venue": "arXiv preprint arXiv:1609.01704,", "citeRegEx": "Chung et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Chung et al\\.", "year": 2016}, {"title": "Achieving open vocabulary neural machine translation with hybrid word-character models", "author": ["Minh-Thang Luong", "Christopher D Manning"], "venue": "arXiv preprint arXiv:1604.00788,", "citeRegEx": "Luong and Manning.,? \\Q2016\\E", "shortCiteRegEx": "Luong and Manning.", "year": 2016}, {"title": "Fast and accurate recurrent neural network acoustic models for speech recognition", "author": ["Hasim Sak", "Andrew W. Senior", "Kanishka Rao", "Fran\u00c3\u011foise Beaufays"], "venue": "CoRR, abs/1507.06947,", "citeRegEx": "Sak et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Sak et al\\.", "year": 2015}, {"title": "Joint ctc-attention based end-to-end speech recognition using multi-task learning", "author": ["Suyoun Kim", "Takaaki Hori", "Shinji Watanabe"], "venue": "arXiv preprint arXiv:1609.06773,", "citeRegEx": "Kim et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Kim et al\\.", "year": 2016}, {"title": "Sequence-level knowledge distillation", "author": ["Yoon Kim", "Alexander M Rush"], "venue": "arXiv preprint arXiv:1606.07947,", "citeRegEx": "Kim and Rush.,? \\Q2016\\E", "shortCiteRegEx": "Kim and Rush.", "year": 2016}, {"title": "Eesen: End-to-end speech recognition using deep rnn models and wfst-based decoding", "author": ["Yajie Miao", "Mohammad Gowayyed", "Florian Metze"], "venue": "In Automatic Speech Recognition and Understanding (ASRU),", "citeRegEx": "Miao et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Miao et al\\.", "year": 2015}, {"title": "Very deep convolutional networks for end-to-end speech recognition", "author": ["Yu Zhang", "William Chan", "Navdeep Jaitly"], "venue": "arXiv preprint arXiv:1610.03022,", "citeRegEx": "Zhang et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Zhang et al\\.", "year": 2016}, {"title": "Advances in all-neural speech recognition", "author": ["Geoffery Zweig", "Ghengzhu Yu", "Jasha Droppo", "Andreas Stolcke"], "venue": "arXiv preprint arXiv:1609.05935,", "citeRegEx": "Zweig et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Zweig et al\\.", "year": 2016}, {"title": "Purely sequence-trained neural networks for asr based on lattice-free mmi", "author": ["Daniel Povey", "Vijayaditya Peddinti", "Daniel Galvez", "Pegah Ghahrmani", "Vimal Manohar", "Xingyu Na", "Yiming Wang", "Sanjeev Khudanpur"], "venue": null, "citeRegEx": "Povey et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Povey et al\\.", "year": 2016}], "referenceMentions": [{"referenceID": 0, "context": ", 2006) and Sequenceto-sequence (seq2seq) models (Cho et al., 2014; Sutskever et al., 2014) present powerful approaches to multiple applications, such as Automatic Speech Recognition (ASR)", "startOffset": 49, "endOffset": 91}, {"referenceID": 1, "context": ", 2006) and Sequenceto-sequence (seq2seq) models (Cho et al., 2014; Sutskever et al., 2014) present powerful approaches to multiple applications, such as Automatic Speech Recognition (ASR)", "startOffset": 49, "endOffset": 91}, {"referenceID": 3, "context": "(Chan et al., 2016a; Hannun et al., 2014; Bahdanau et al., 2016), machine translation (S\u00e9bastien et al.", "startOffset": 0, "endOffset": 64}, {"referenceID": 4, "context": "(Chan et al., 2016a; Hannun et al., 2014; Bahdanau et al., 2016), machine translation (S\u00e9bastien et al.", "startOffset": 0, "endOffset": 64}, {"referenceID": 5, "context": ", 2016), machine translation (S\u00e9bastien et al., 2015), and parsing (Vinyals et al.", "startOffset": 29, "endOffset": 53}, {"referenceID": 6, "context": ", 2015), and parsing (Vinyals et al., 2015).", "startOffset": 21, "endOffset": 43}, {"referenceID": 1, "context": "These methods are based on 1) a fixed and carefully chosen set of basic units, such as words (Sutskever et al., 2014), phonemes (Chorowski et al.", "startOffset": 93, "endOffset": 117}, {"referenceID": 7, "context": ", 2014), phonemes (Chorowski et al., 2015) or characters (Chan et al.", "startOffset": 18, "endOffset": 42}, {"referenceID": 10, "context": ", 2015), word-pieces (Wu et al., 2016b; Collobert et al., 2016; Zweig et al., 2016a), words (Soltau et al.", "startOffset": 21, "endOffset": 84}, {"referenceID": 12, "context": ", 2016a), words (Soltau et al., 2016; S\u00e9bastien et al., 2015), and phonemes (Lee and Hon, 1988; Sercu and Goel, 2016; Xiong et al.", "startOffset": 16, "endOffset": 61}, {"referenceID": 5, "context": ", 2016a), words (Soltau et al., 2016; S\u00e9bastien et al., 2015), and phonemes (Lee and Hon, 1988; Sercu and Goel, 2016; Xiong et al.", "startOffset": 16, "endOffset": 61}, {"referenceID": 13, "context": ", 2015), and phonemes (Lee and Hon, 1988; Sercu and Goel, 2016; Xiong et al., 2016b) have been widely used as basic units for text prediction, but all of them have drawbacks.", "startOffset": 22, "endOffset": 84}, {"referenceID": 14, "context": ", 2015), and phonemes (Lee and Hon, 1988; Sercu and Goel, 2016; Xiong et al., 2016b) have been widely used as basic units for text prediction, but all of them have drawbacks.", "startOffset": 22, "endOffset": 84}, {"referenceID": 12, "context": "7 million in (Soltau et al., 2016)), out-ofvocabulary words (Soltau et al.", "startOffset": 13, "endOffset": 34}, {"referenceID": 12, "context": ", 2016)), out-ofvocabulary words (Soltau et al., 2016; S\u00e9bastien et al., 2015) and data sparsity problems (Soltau et al.", "startOffset": 33, "endOffset": 78}, {"referenceID": 5, "context": ", 2016)), out-ofvocabulary words (Soltau et al., 2016; S\u00e9bastien et al., 2015) and data sparsity problems (Soltau et al.", "startOffset": 33, "endOffset": 78}, {"referenceID": 12, "context": ", 2015) and data sparsity problems (Soltau et al., 2016).", "startOffset": 35, "endOffset": 56}, {"referenceID": 16, "context": "g, 26 for English and thousands for Chinese), but it requires much longer contexts compared to using words or word-pieces and poses the challenge of composing characters to words (Graves et al., 2006; Chan et al., 2015), which is very noisy for languages like English.", "startOffset": 179, "endOffset": 219}, {"referenceID": 13, "context": "\u2022 For the ASR task, the use of phonemes was popular in the past few decades as it eases acoustic modeling (Lee and Hon, 1988) and good results were reported with phonemic models (Xiong et al.", "startOffset": 106, "endOffset": 125}, {"referenceID": 14, "context": "\u2022 For the ASR task, the use of phonemes was popular in the past few decades as it eases acoustic modeling (Lee and Hon, 1988) and good results were reported with phonemic models (Xiong et al., 2016b; Sercu and Goel, 2016).", "startOffset": 178, "endOffset": 221}, {"referenceID": 17, "context": "However, it introduces the uncertainties of mapping phonemes to words during decoding (Doss et al., 2003), which becomes less robust especially for accented speech data.", "startOffset": 86, "endOffset": 105}, {"referenceID": 18, "context": "(Chung et al., 2016) leveraged a multiscale RNN architecture to model the characters and words hierarchically, by building word-level representations on top of character-level representations.", "startOffset": 0, "endOffset": 20}, {"referenceID": 19, "context": "(Luong and Manning, 2016) proposed a hybrid Word-Character model what translates mostly at the word level and consults the character components for rare words.", "startOffset": 0, "endOffset": 25}, {"referenceID": 20, "context": "To overcome this difficulty, we found it beneficial to train a model with both the Gram-CTC, as well as the vanilla CTC loss (similar to joint-training CTC together with CE loss as mentioned in (Sak et al., 2015)).", "startOffset": 194, "endOffset": 212}, {"referenceID": 21, "context": "Joint training of multiple objectives for sequence labelling has also been explored in previous works (Kim et al., 2016; Kim and Rush, 2016).", "startOffset": 102, "endOffset": 140}, {"referenceID": 22, "context": "Joint training of multiple objectives for sequence labelling has also been explored in previous works (Kim et al., 2016; Kim and Rush, 2016).", "startOffset": 102, "endOffset": 140}, {"referenceID": 23, "context": "This is a relatively \u2018clean\u2019 task and often used for model prototyping (Miao et al., 2015; Bahdanau et al., 2016; Zhang et al., 2016; Chan et al., 2016b).", "startOffset": 71, "endOffset": 153}, {"referenceID": 4, "context": "This is a relatively \u2018clean\u2019 task and often used for model prototyping (Miao et al., 2015; Bahdanau et al., 2016; Zhang et al., 2016; Chan et al., 2016b).", "startOffset": 71, "endOffset": 153}, {"referenceID": 24, "context": "This is a relatively \u2018clean\u2019 task and often used for model prototyping (Miao et al., 2015; Bahdanau et al., 2016; Zhang et al., 2016; Chan et al., 2016b).", "startOffset": 71, "endOffset": 153}, {"referenceID": 26, "context": "Following the previous works (Zweig et al., 2016b; Povey et al., 2016; Xiong et al., 2016b; Sercu and Goel, 2016), evaluation is carried out on the NIST 2000 CTS test set, which comprises both Switchboard (SWB) and CallHome (CH) subsets.", "startOffset": 29, "endOffset": 113}, {"referenceID": 14, "context": "Following the previous works (Zweig et al., 2016b; Povey et al., 2016; Xiong et al., 2016b; Sercu and Goel, 2016), evaluation is carried out on the NIST 2000 CTS test set, which comprises both Switchboard (SWB) and CallHome (CH) subsets.", "startOffset": 29, "endOffset": 113}, {"referenceID": 23, "context": "Phoneme CTC + trigram LM (Miao et al., 2015) 7.", "startOffset": 25, "endOffset": 44}, {"referenceID": 23, "context": "3 Grapheme CTC + trigram LM (Miao et al., 2015) 9.", "startOffset": 28, "endOffset": 47}, {"referenceID": 4, "context": "0 Attention + trigram LM (Bahdanau et al., 2016) 9.", "startOffset": 25, "endOffset": 48}, {"referenceID": 24, "context": "3 DeepConv LAS + no LM (Zhang et al., 2016) 10.", "startOffset": 23, "endOffset": 43}, {"referenceID": 26, "context": "7 BLSTM + LF MMI (Povey et al., 2016) 8.", "startOffset": 17, "endOffset": 37}, {"referenceID": 14, "context": "8 Dilated convolutions (Sercu and Goel, 2016) 7.", "startOffset": 23, "endOffset": 45}], "year": 2017, "abstractText": "Most existing sequence labelling models rely on a fixed decomposition of a target sequence into a sequence of basic units. These methods suffer from two major drawbacks: 1) the set of basic units is fixed, such as the set of words, characters or phonemes in speech recognition, and 2) the decomposition of target sequences is fixed. These drawbacks usually result in sub-optimal performance of modeling sequences. In this paper, we extend the popular CTC loss criterion to alleviate these limitations, and propose a new loss function called Gram-CTC. While preserving the advantages of CTC, Gram-CTC automatically learns the best set of basic units (grams), as well as the most suitable decomposition of target sequences. Unlike CTC, Gram-CTC allows the model to output variable number of characters at each time step, which enables the model to capture longer term dependency and improves the computational efficiency. We demonstrate that the proposed Gram-CTC improves CTC in terms of both performance and efficiency on the large vocabulary speech recognition task at multiple scales of data, and that with Gram-CTC we can outperform the state-of-the-art on a standard speech benchmark.", "creator": "LaTeX with hyperref package"}}}