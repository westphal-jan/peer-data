{"id": "1703.00102", "review": {"conference": "ICML", "VERSION": "v1", "DATE_OF_SUBMISSION": "1-Mar-2017", "title": "SARAH: A Novel Method for Machine Learning Problems Using Stochastic Recursive Gradient", "abstract": "In this paper, we propose a StochAstic Recursive grAdient algoritHm (SARAH), as well as its practical variant SARAH+, as a novel approach to the finite-sum minimization problems. Different from the vanilla SGD and other modern stochastic methods such as SVRG, S2GD, SAG and SAGA, SARAH admits a simple recursive framework for updating stochastic gradient estimates; when comparing to SAG/SAGA, SARAH does not require a storage of past gradients. The linear convergence rate of SARAH is proven under strong convexity assumption. We also prove a linear convergence rate (in the strongly convex case) for an inner loop of SARAH, the property that SVRG does not possess. Numerical experiments demonstrate the efficiency of our algorithm.", "histories": [["v1", "Wed, 1 Mar 2017 02:08:32 GMT  (1589kb)", "https://arxiv.org/abs/1703.00102v1", null], ["v2", "Sat, 3 Jun 2017 07:30:20 GMT  (1862kb)", "http://arxiv.org/abs/1703.00102v2", null]], "reviews": [], "SUBJECTS": "stat.ML cs.LG math.OC", "authors": ["lam m nguyen", "jie liu", "katya scheinberg", "martin tak\u00e1c"], "accepted": true, "id": "1703.00102"}, "pdf": {"name": "1703.00102.pdf", "metadata": {"source": "META", "title": "SARAH: A Novel Method for Machine Learning Problems  Using Stochastic Recursive Gradient", "authors": ["LamM. Nguyen", "Jie Liu", "Katya Scheinberg", "Martin Tak\u00e1\u010d"], "emails": ["guyen.mltd@gmail.com>,", "<jie.liu.2018@gmail.com>,", "<katyas@lehigh.edu>,", "<Takac.MT@gmail.com>."], "sections": [{"heading": null, "text": "ar Xiv: 170 3.00 102v 2 [stat.ML] 3 Jun 201 7sive grAdient algoritHm (SARAH), as well as its practical variant SARAH +, as a novel approach to minimizing finite sums. Unlike vanilla SGD and other modern stochastic methods such as SVRG, S2GD, SAG and SAGA, SARAH allows a simple recursive framework for updating stochastic gradient estimates; compared to SAG / SAGA, SARAH does not require storage of past gradients. SARAH's linear convergence rate is proven under strict convergence assumptions. We also demonstrate a linear convergence rate (in the strongly convex case) for an inner loop of SARAH, the property SVRG does not possess. Numerical experiments prove the efficiency of our algorithm."}, {"heading": "1. Introduction", "text": "We are interested in solving a problem that the RdP (w) def = 1n (n) i (n) fi (w), (1), (1), (1), (1), (1), (1), (2), (2), (1), (1), (1), (1), (1), (2), (2), (2), (2), (2), (1), (2), (2), (2), (2), (1), (2), (2), (2), (2), (2), (2), (2), (2), (2), (2), (2), (2), (2), (2), (2), (2), (2), (2), (3), (3), (3), (3), (3), (3), (3), (3), (2), (2), (2), (2), (2), (2), (2), (2), (2), (2), (2), (2), (2), (2), (1), (1), (1), (1), (1), (1), (1), (1), (1), (1), (1), (1), (1), (1), (1), (1), (1), (1), (1), (1), (1), (2), (2), (2), (2), (1), (2), (2), (2), (2), (2), (2), (2), (2), (2), (2), (2, (2), (2), (2, (2), (2), (2), (2, (2), (2), (2, (2), (2, (2), (2), (2), (2, (2), (, (2), (2), ("}, {"heading": "2. Stochastic Recursive Gradient Algorithm", "text": "The key step of the algorithm is a recursive update of the stochastic gradient estimate (SARAH update) vt = \"fit\" (wt) \u2212 \"fit\" (wt) \u2212 \"fit\" (wt) \u2212 \"fit\" (wt) \u2212 \"fit\" (w0) + \"fit\" (w0). (4) Algorithm 1 \"SARAHParameter: the learning rate (wt). (4) SARAHParameter: the learning rate (w0) and the inner loop size (m). (4) SARAHParameter: the learning rate (w0) and the inner loop size (m). (4) SARAHParameter: the learning rate (w0). (4) SARAHParameter: the learning rate (w0) and the inner loop size (w0). (4) SARAHParameter: the learning rate (w0)."}, {"heading": "3. Theoretical Analysis", "text": "In order to proceed with the analysis of the proposed algorithm, we will make the following general assumptions: adoption 1 (L-smooth), adoption 1 (L-smooth), adoption 2 (R-smooth), adoption 2 (R-smooth), adoption 2 (R-smooth), adoption 2 (R-smooth), adoption 3 (R-smooth), adoption 3 (R-smooth), adoption 3 (R-smooth), adoption 3 (R-smooth), adoption 2 (W-smooth), adoption 3 (R-smooth), adoption 2 (R-smooth), adoption 2 (R-smooth), adoption 3 (R-smooth), adoption 2 (R-accept), adoption 3 (R-smooth), 2 (R-accept) (R-3)."}, {"heading": "3.1. Linearly Diminishing Step-Size in a Single Inner Loop", "text": "The most important feature of the SVRG algorithm is the reduction of variance in the steps toward the optimal solution. This property holds as the number of external iterations grows, but it does not hold if only the number of internal iterations increases. In other words, if we simply traverse the inner loop for many iterations (without performing additional outer loops), the variance of steps does not decrease in the case of SVRG and SARAH to a sum of 5 square functions in a two-dimensional space in which the optimal lines and black dots point to the path of each algorithm and the red dot."}, {"heading": "3.2. Convergence Analysis", "text": "In this section, we derive the results of the general convergence rate for algorithm 1. First, we present two important lemmas as the basis of our theory, and then proceed to demonstrate the sublinear convergence rate of a single outer iteration when applying it to general convex functions. Finally, we prove that the algorithm with multiple outer iterations exhibits a linear convergence rate in the strongly convex case. 4In the diagrams of figure 2, since the data is loud for SVRG, we smooth it by using moving average filters with span 100 for the left case and 10 for the right case. Let's start with the detection of two useful lemmas that do not require convergence; the first lemma 1 limits the sum of the expected values for VRG (wt); the second lemma 2, limits E [VRP (wt) - vvt."}, {"heading": "3.2.1. GENERAL CONVEX CASE", "text": "Following the study of Lemma 2 we can set the following upper limit for E (wt) -P (wt) -P (wt) -P (wt) -P (wt) -P (wt) -P (wt) -P (wt) -P (wt) -P (wt) -wt) -P (wt) -P (wt) -P (wt) -P (wt) -P (wt) -P (wt) -P (wt) -P (wt) -P (wt) -P (wt) -P (wt) -P (wt) -P (wt) -P (wt) -P (wt) (wp) -P (wt) (wp) -P (wt) (wt) (p) -P (wt) (P) -P (wt (wt) -P (wp) -P (wt) -P (wp) -P (wt) -P (wt (p) -P (wt) -P (wt) -P (wt) -P (wt) -P (wt (p) -P (wt) -P (wt) -P (wt (p) -P (wt) -P (wt) -P (wt (p) -P (wt) -P (wt) -P (wt) -P (wt) -P (wt) -P (wt (wt) -P (wt) -P (wt) -P (wt) -P (wt) -P (wt) -P (wt (wt) -P (wt (wt) -P (wt) -P (wt) -P (wt (wt) -P (wt) -P (wt) -P (wt (wt) -P (wt) -P (wt) -P (wt) -P (wt) -P (wt) -P (wt) -P (wt) -P (wt) -P (wt) -P (wt) -P (w"}, {"heading": "3.2.2. STRONGLY CONVEX CASE", "text": "We now turn to the discussion of SARAH's linear convergence rate under the strong convergence assumption on P. < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < <"}, {"heading": "4. A Practical Variant", "text": "While SVRG is an efficient variance-reducing stochastic gradient method, one of its main drawbacks is the sensitivity of practical performance in terms of choosing m = 1. It is known that m should be around O (3), 5 while it is still unclear what is the exact best choice. However, in this section we propose a practical variant of SARAH as5, when n is large, P (w) is often considered a regulated empirical loss minimization problem with regulation parameters (n).SARAH + (n).SARAH + considering an automatic and adaptive choice of the inner loop sizem. Guided by the linear convergence of steps in the inner loop shown in Figure 2, we provide a stopping criterion based on the values of vvt 2, while the total number of steps is around a large number of burobustness."}, {"heading": "5. Numerical Experiments", "text": "To support the theoretical analyses and insights, we present our empirical experiments in which we compare SARAH and SARAH. (SARAH) SARAH + SARAH + SARAH + SARAH + SARAH + SARAH + SARAH + SARAH + SARA + SARA + SARA + SARA + SARA + SARA + SARA + SARA + SARA + SARA + SARA + SARA + SARA + SARA + SARA + SARA + SARA + SARA + SARA + SARA SARA + SARA SARA + SARA SARA + SARA SARA + SARA SARA SARA + SARA SARA + SARA SARA SARA + SARA SARA + SARA SARA + SARA SARA + SARA SARA + SARA + SARA SARA + SARA SARA + SARA SARA + SARA + SARA SARA + SARA + SARA SARA + SARA + SARA SARA + SARA + SARA SARA + SARA + SARA + SARA + SARA SARA + SARA + SARA SARA + SARA + SARA + SARA + SARA + SARA SARA + SARA SARA + SARA + SARA + SARA + SARA SARA + SARA SARA + SARA SARA + SARA SARA + SARA + SARA + SARA + SARA + SARA SARA + SARA + SARA SARA SARA + SARA + SARA SARA + SARA + SARA + SARA SARA + SARA SARA + SARA SARA + SARA + SARA + SARA SARA + SARA SARA + SARA"}, {"heading": "SARAH/SVRG.", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "6. Conclusion", "text": "We propose a new variance-reducing stochastic recursive gradient algorithm SARAH that combines some of the properties of well-known existing algorithms such as SAGA and SVRG. For smooth convex functions, we show a sublinear convergence rate, while for strongly convex cases, we demonstrate linear convergence rate and computational complexity such as those of SVRG and SAG. However, compared to SVRG, the convergence rate of SARAH is smaller and the algorithms are both theoretically and numerically more stable. Furthermore, we demonstrate the linear convergence for internal loops of SARAH that support the assertion of stability. Based on this convergence, we derive a practical version of SARAH, with a simple stop criterion for internal loops."}, {"heading": "Acknowledgements", "text": "The authors would like to thank the reviewers for their useful suggestions, which have helped to improve the exposure in the essay."}, {"heading": "A. Technical Results", "text": "Lemma 4 (Theorem 2.1.5 in (Nesterov, 2004))) Suppose that f is convex and L-smooth. Suppose then that f (w) w (w) p (w) p (w) p (w) p (w) p (w) p (w) p (w) p (w) p (2) p (16) p (w) p (w) p (w) p (w) p (w) p (w) p (18) p (w) p (f) p (f) p (w) p (17) p (w) p (w) p (w) p (w) p (w) p (w) p (w) p (18) p (16) p (f) p (f) p (f) p (f) p (f) p) p (w w w w w w p (p) p \"p (p) p\" p \"p (p) (p) (w w w w w w w w w) p\" p \"p (p)\" p (p) \"p (p) p\" p (w) p \"p (w) p\" p (p) p) p (w) p (p) p (p) p) p (w) p (w) p (w) p (w w w w w w w w w w w w w w w w) p (p \"p\" p (p) p \"p\" p \"p\" p (p) p \"p (p) p\" p (p) p \"p (p (p) p\" p (p) p (p) p (p \"p (p) p\" p (p) p (p) p (p) p (p) p (w w w w w w w w w w w w w w w w w w w w w w w) p (p (p \"p\" p \"p\" p (p) p \"p (p) p (p\" p (p) p) p (p \"p (p) p (p\" p (p) p) p (p \"p (p) p\" p (p) p) p (p (p) p (p (p) p) p (p (p) p) p (p (p"}, {"heading": "B. Proofs", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "B.1. Proof of Lemma 1", "text": "After assuming 1 and wt + 1 = wt \u2212 v = v = v = v = v = v = v = v = v = v = v = v = v = v = v = v = v = v = v [p (wt)] (16) \u2264 E [p (wt)] (P (wt)] \u2212 v [p (wt)] (2) + v [p (wt) \u2212 p (wt) \u2212 v [v) (2 \u2212 v) 2] \u2212 (p (2 \u2212 v) 22) E [v [v [v v) 2] (where the last equality follows from the fact that aT b = 12 [p (a) = 2 + v (b) 2 + v (2 \u2212 b) 2 \u2212 p (2 \u2212 b) 2].Summing over t = 0,.. \u2212 m we have E [P (wm + 1) \u2264 E [P (w0) \u2212 v = v (m) = v (w0)."}, {"heading": "B.2. Proof of Lemma 2", "text": "It is noted that Fj \u2212 jj \u2212 jj \u2212 jj \u2212 jj (jj \u2212 jj) v0,., vj \u2212 jj (jj \u2212 jj) vE (jj \u2212 jj) v0,., vj \u2212 jj (jj \u2212 jj) vj \u2212 1) + [jj \u2212 P (wj \u2212 1)] [jj \u2212 vj \u2212 jp (jj \u2212 jj) vj (jj \u2212 jp) vj (vj \u2212 jp) vj (jp) vj \u2212 vj (jj \u2212 jp) vj (jp) vj \u2212 vj (jj \u2212 jp) vj (jp) vj (jp) vj \u2212 jp (jp) jp (jp) jp (jp) vj \u2212 jp (jp) vj \u2212 jp (jp) vj \u2212 jp (jp) vj \u2212 jp (jp) v0, vj \u2212 jj \u2212 jp) v0 (jj \u2212 jj \u2212 jj) v0, v0 \u2212 jj \u2212 jj (jj \u2212 jj \u2212 jj) v0, v0 \u2212 jj \u2212 jj (jj \u2212 jj \u2212 jj) v0, vj \u2212 jj \u2212 jj (jj \u2212 jj \u2212 jj) vj \u2212 jp (jj \u2212 jp) vj \u2212 jp (jp) vj \u2212 jp (jj \u2212 jp (jp) vj \u2212 jp (jp \u2212 jp) vj \u2212 jp (jp \u2212 jp (jp) vj \u2212 jp (jp \u2212 jp \u2212 jp) vj \u2212 jp \u2212 jp (jp \u2212 jp) vj \u2212 jp \u2212 jp \u2212 jp (jp \u2212 jp \u2212 jp \u2212 jp) vj \u2212 jp \u2212 jp \u2212 jp \u2212 jp \u2212 jp (jp \u2212 jp \u2212 jp \u2212 jp) vj \u2212 jp \u2212 jp \u2212 jp \u2212 jp \u2212 jp \u2212 jp \u2212 jp \u2212 jp \u2212 jp \u2212 jp \u2212 jp \u2212 vj \u2212 vj \u2212 vj \u2212 vj \u2212 jp \u2212"}, {"heading": "B.3. Proof of Lemma 3", "text": "\u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2"}, {"heading": "B.4. Proof of Lemma 6", "text": "The convergence rate \u03b1m can be written with \u03b7 = 1 / (\u03b8L) and \u043c = L / \u00b5 as \u03c3m (15) = 1\u00b5\u03b7 (m + 1) + \u03b7L = \u03b8L \u00b5 (m + 1) + 1 / \u03b8 2 \u2212 1 / \u03b8 = (\u0443 m + 1) \u03b8 + 1 2\u03b8 \u2212 1, which Tom (\u03b8) def = \u03b8 (2\u03b8 \u2212 1) \u03c3m (2\u03b8 \u2212 1) \u2212 1 \u0445 \u2212 1. Since \u03c3m is considered fixed, the optimal m-choice can be solved with respect to \u03b8 from min\u03b8 m (\u03b8), or equivalent 0 = (\u2202 m) / (GOP) = m \u2032 (\u03b8), and therefore we have the equation with the optimal climate satisfying\u03c3m = (4\u03b8 - 1) / (2\u043c - 1), (21) and by closing it intom (\u03b8) = m (K - 2), we close the equation with the optimal climate."}, {"heading": "B.5. Proof of Theorem 1a", "text": "\u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2"}, {"heading": "B.6. Proof of Theorem 1b", "text": "Obviously we have E [2] = E [v3] = E [4] v0 [4] v2 | F0] = E [5] (5] v0 [5] v2 | Ft] (2) = E [5] v2 \u2212 1 (5) v2 | Ft] (3) = V2 + E [6] v2 + E [6] v2 \u2212 2 + E [7] v2 \u2212 2 [8] v2 \u2212 2 [8] v2 \u2212 1 [8] v2 \u2212 1) v2 (v8 (v8) v8) v8 (v8) v8 (v8) v8 (v8) v8 (v8) v8 (8) v8 (8) v8 (8) (8) (8) (8) (8) (8) (8) (8) (8) (8) (8) (8) (8) (8) (8) (8) (8) (8) (8) (8) (8) (8) (8) (8) (8) (8) (8) (8) (8) (8) (8) (8) (8) (8) (8) (8) (8) (8) (8) (8) (8) (8) (8) (8) (8) (8) (8) (8) (8) (8) (8) (8) (8) (8) (8) (8) (8) (8) (8) (8) (8) (8) (8) (8) (8) (8) (8) (8) (8) (8) (8) (8) (8) (8) (8) (8) (8) (8 (8) (8) (8) (8) (8) (8) (8) (8) (8 (8) (8) (8) (8) (8) (8 (8) (8) (8) (8) (8) (8 (8) (8) (8 (8) (8) (8 (8) (8) (8) (8) (8) (8 (8) (8) (8) (8) (8"}, {"heading": "B.7. Proof of Theorem 3", "text": "According to Theorem 2, we have E [VP (W) 2] \u2264 2\u03b7 (m + 1) E [P (W) s (1) \u2212 P (W)] + \u03b7L2 \u2212 \u03b7LE [P (W) s (1) s [S) s \u2212 1 + \u03b1E [P (W) s \u2212 1) [P (W) s \u2212 2 + \u00b7 \u00b7 + S (S) s \u2212 1 + s \u2212 2 + \u00b7 P (W) s (W) s (P (0) s (2)) s (W) s (1) s (P) s (1) s (P) s (1)) s (1) s (P) s (W) s (W) s (0) s (W) s (W) s (2) p (W) s (W) s (W) p (W) s (0) s (W) p (W) s (W) s (W) s (W) s (W) s (2) s (2)."}, {"heading": "B.8. Proof of Corollary 2", "text": "Based on Theorem 3, if we want a solution with the highest accuracy, we can choose whether we want? = 1 / 4 and? = 1 / 2 (with p = 2 / (3L). To achieve convergence to a solution with the highest accuracy, we must have? = O (B) or equivalent? m = O (1 / A). Then we must have? E? P (W)? 2 (13)? 2 + 1 2 E [W? s \u2212 1) 2] \u2264 2 + 22 + 1 22 E [E? P (W? s \u2212 2) 2] (12 + 1 22 + \u00b7 + 1 2s) + 12 s \u00b2 P (W? 0) + 1 2s \u00b2 P (W? 0)."}, {"heading": "B.9. Proof of Corollary 3", "text": "Based on Lemma 6 and Theorem 4, let us select the following points: \u03b8 = 2, i.e., then we have m \u00b2 = 4.5 \u00b2 \u2212 1. Let us run SARAH with \u03b7 = 1 / (2L) and m = 4.5 \u00b2, then we can run p \u00b2 in (15) a \u00b2 m = 1 (m + 1) + 1 (L 2 \u2212 2) (4.5L + 1) + 1 / 2 2 2 \u2212 1 / 2 < 4 9 + 1 3 = 7 9.According to Theorem 4, if we run SARAH for T iterations, where T = log (P (w \u00b2 0), 2 / 7) / log (9 / 7) = log7 / 9 (w \u00b2 0), then we have P (P \u00b2 2), if we run p \u00b2 (p \u00b2 2) (p) \u00b2 (2) (p) (p) (2) (p) (2) (p)."}], "references": [{"title": "Katyusha: The First Direct Acceleration of Stochastic Gradient Methods", "author": ["Allen-Zhu", "Zeyuan"], "venue": "Proceedings of the 49th Annual ACM on Symposium on Theory of Computing (to appear),", "citeRegEx": "Allen.Zhu and Zeyuan.,? \\Q2017\\E", "shortCiteRegEx": "Allen.Zhu and Zeyuan.", "year": 2017}, {"title": "Improved SVRG for Non-Strongly-Convex or Sum-of-Non-Convex Objectives", "author": ["Allen-Zhu", "Zeyuan", "Yuan", "Yang"], "venue": "In ICML,", "citeRegEx": "Allen.Zhu et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Allen.Zhu et al\\.", "year": 2016}, {"title": "A fast iterative shrinkagethresholding algorithm for linear inverse problems", "author": ["Beck", "Amir", "Teboulle", "Marc"], "venue": "SIAM J. Imaging Sciences,", "citeRegEx": "Beck et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Beck et al\\.", "year": 2009}, {"title": "Online learning and stochastic approximations", "author": ["Bottou", "L\u00e9on"], "venue": null, "citeRegEx": "Bottou and L\u00e9on.,? \\Q1998\\E", "shortCiteRegEx": "Bottou and L\u00e9on.", "year": 1998}, {"title": "Optimization methods for large-scale machine learning", "author": ["Bottou", "L\u00e9on", "Curtis", "Frank E", "Nocedal", "Jorge"], "venue": null, "citeRegEx": "Bottou et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Bottou et al\\.", "year": 2016}, {"title": "Better mini-batch algorithms via accelerated gradient methods", "author": ["Cotter", "Andrew", "Shamir", "Ohad", "Srebro", "Nati", "Sridharan", "Karthik"], "venue": "In NIPS, pp", "citeRegEx": "Cotter et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Cotter et al\\.", "year": 2011}, {"title": "SAGA: A fast incremental gradient method with support for non-strongly convex composite objectives", "author": ["Defazio", "Aaron", "Bach", "Francis", "Lacoste-Julien", "Simon"], "venue": "In NIPS,", "citeRegEx": "Defazio et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Defazio et al\\.", "year": 2014}, {"title": "The Elements of Statistical Learning: Data Mining, Inference, and Prediction", "author": ["Hastie", "Trevor", "Tibshirani", "Robert", "Friedman", "Jerome"], "venue": "Springer Series in Statistics,", "citeRegEx": "Hastie et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Hastie et al\\.", "year": 2009}, {"title": "Accelerating stochastic gradient descent using predictive variance reduction", "author": ["Johnson", "Rie", "Zhang", "Tong"], "venue": "In NIPS, pp", "citeRegEx": "Johnson et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Johnson et al\\.", "year": 2013}, {"title": "Mini-batch semi-stochastic gradient descent in the proximal setting", "author": ["Kone\u010dn\u00fd", "Jakub", "Liu", "Jie", "Richt\u00e1rik", "Peter", "Tak\u00e1\u010d", "Martin"], "venue": "IEEE Journal of Selected Topics in Signal Processing,", "citeRegEx": "Kone\u010dn\u00fd et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Kone\u010dn\u00fd et al\\.", "year": 2016}, {"title": "Semi-stochastic gradient descent methods", "author": ["Kone\u010dn\u00fd", "Jakub", "Richt\u00e1rik", "Peter"], "venue": null, "citeRegEx": "Kone\u010dn\u00fd et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Kone\u010dn\u00fd et al\\.", "year": 2013}, {"title": "A stochastic gradient method with an exponential convergence rate for finite training sets", "author": ["Le Roux", "Nicolas", "Schmidt", "Mark", "Bach", "Francis"], "venue": "In NIPS,", "citeRegEx": "Roux et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Roux et al\\.", "year": 2012}, {"title": "Optimization with first-order surrogate functions", "author": ["Mairal", "Julien"], "venue": "In ICML, pp", "citeRegEx": "Mairal and Julien.,? \\Q2013\\E", "shortCiteRegEx": "Mairal and Julien.", "year": 2013}, {"title": "Introductory lectures on convex optimization : a basic course", "author": ["Nesterov", "Yurii"], "venue": null, "citeRegEx": "Nesterov and Yurii.,? \\Q2004\\E", "shortCiteRegEx": "Nesterov and Yurii.", "year": 2004}, {"title": "Stochastic variance reduction for nonconvex optimization", "author": ["Reddi", "Sashank J", "Hefny", "Ahmed", "Sra", "Suvrit", "P\u00f3czos", "Barnab\u00e1s", "Smola", "Alexander J"], "venue": "In ICML,", "citeRegEx": "Reddi et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Reddi et al\\.", "year": 2016}, {"title": "A stochastic approximation method", "author": ["Robbins", "Herbert", "Monro", "Sutton"], "venue": "The Annals of Mathematical Statistics,", "citeRegEx": "Robbins et al\\.,? \\Q1951\\E", "shortCiteRegEx": "Robbins et al\\.", "year": 1951}, {"title": "Minimizing finite sums with the stochastic average gradient", "author": ["Schmidt", "Mark", "Le Roux", "Nicolas", "Bach", "Francis"], "venue": "Mathematical Programming,", "citeRegEx": "Schmidt et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Schmidt et al\\.", "year": 2016}, {"title": "Stochastic dual coordinate ascent methods for regularized loss", "author": ["Shalev-Shwartz", "Shai", "Zhang", "Tong"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Shalev.Shwartz et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Shalev.Shwartz et al\\.", "year": 2013}, {"title": "Pegasos: Primal estimated sub-gradient solver for SVM", "author": ["Shalev-Shwartz", "Shai", "Singer", "Yoram", "Srebro", "Nathan"], "venue": "In ICML, pp", "citeRegEx": "Shalev.Shwartz et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Shalev.Shwartz et al\\.", "year": 2007}, {"title": "Pegasos: Primal estimated sub-gradient solver for SVM", "author": ["Shalev-Shwartz", "Shai", "Singer", "Yoram", "Srebro", "Nathan", "Cotter", "Andrew"], "venue": "Mathematical Programming,", "citeRegEx": "Shalev.Shwartz et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Shalev.Shwartz et al\\.", "year": 2011}, {"title": "Mini-batch primal and dual methods for SVMs", "author": ["Tak\u00e1\u010d", "Martin", "Bijral", "Avleen Singh", "Richt\u00e1rik", "Peter", "Srebro", "Nathan"], "venue": "In ICML,", "citeRegEx": "Tak\u00e1\u010d et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Tak\u00e1\u010d et al\\.", "year": 2013}, {"title": "A proximal stochastic gradient method with progressive variance reduction", "author": ["Xiao", "Lin", "Zhang", "Tong"], "venue": "SIAM Journal on Optimization,", "citeRegEx": "Xiao et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Xiao et al\\.", "year": 2014}], "referenceMentions": [{"referenceID": 7, "context": "Problems of this type arise frequently in supervised learning applications (Hastie et al., 2009).", "startOffset": 75, "endOffset": 96}, {"referenceID": 19, "context": "Using diminishing sequence {\u03b7t} is used to control the variance (Shalev-Shwartz et al., 2011; Bottou et al., 2016), but the practical convergence of SGD is known to be very sensitive to the choice of this sequence, which needs to be hand-picked.", "startOffset": 64, "endOffset": 114}, {"referenceID": 4, "context": "Using diminishing sequence {\u03b7t} is used to control the variance (Shalev-Shwartz et al., 2011; Bottou et al., 2016), but the practical convergence of SGD is known to be very sensitive to the choice of this sequence, which needs to be hand-picked.", "startOffset": 64, "endOffset": 114}, {"referenceID": 6, "context": "The examples of these methods are SAG/SAGA (Le Roux et al., 2012; Defazio et al., 2014), SDCA (Shalev-Shwartz & Zhang, 2013), SVRG (Johnson & Zhang, 2013; Xiao & Zhang, 2014), DIAG (Mokhtari et al.", "startOffset": 43, "endOffset": 87}, {"referenceID": 16, "context": "In addition, theoretical results for complexity of the methods or their variants when applied to general convex functions have been derived (Schmidt et al., 2016; Defazio et al., 2014; Reddi et al., 2016; Allen-Zhu & Yuan, 2016; Allen-Zhu, 2017).", "startOffset": 140, "endOffset": 245}, {"referenceID": 6, "context": "In addition, theoretical results for complexity of the methods or their variants when applied to general convex functions have been derived (Schmidt et al., 2016; Defazio et al., 2014; Reddi et al., 2016; Allen-Zhu & Yuan, 2016; Allen-Zhu, 2017).", "startOffset": 140, "endOffset": 245}, {"referenceID": 14, "context": "In addition, theoretical results for complexity of the methods or their variants when applied to general convex functions have been derived (Schmidt et al., 2016; Defazio et al., 2014; Reddi et al., 2016; Allen-Zhu & Yuan, 2016; Allen-Zhu, 2017).", "startOffset": 140, "endOffset": 245}, {"referenceID": 9, "context": "Note that like SVRG/S2GD and SAG/SAGA, SARAH also allows an efficient sparse implementation named \u201clazy updates\u201d (Kone\u010dn\u00fd et al., 2016).", "startOffset": 113, "endOffset": 135}], "year": 2017, "abstractText": "In this paper, we propose a StochAstic Recursive grAdient algoritHm (SARAH), as well as its practical variant SARAH+, as a novel approach to the finite-sum minimization problems. Different from the vanilla SGD and other modern stochastic methods such as SVRG, S2GD, SAG and SAGA, SARAH admits a simple recursive framework for updating stochastic gradient estimates; when comparing to SAG/SAGA, SARAH does not require a storage of past gradients. The linear convergence rate of SARAH is proven under strong convexity assumption. We also prove a linear convergence rate (in the strongly convex case) for an inner loop of SARAH, the property that SVRG does not possess. Numerical experiments demonstrate the efficiency of our algorithm.", "creator": "LaTeX with hyperref package"}}}