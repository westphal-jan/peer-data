{"id": "1703.00119", "review": {"conference": "ICML", "VERSION": "v1", "DATE_OF_SUBMISSION": "1-Mar-2017", "title": "Dual Iterative Hard Thresholding: From Non-convex Sparse Minimization to Non-smooth Concave Maximization", "abstract": "Iterative Hard Thresholding (IHT) is a class of projected gradient descent methods for optimizing sparsity-constrained minimization models, with the best known efficiency and scalability in practice. As far as we know, the existing IHT-style methods are designed for sparse minimization in primal form. It remains open to explore duality theory and algorithms in such a non-convex and NP-hard setting. In this article, we bridge the gap by establishing a duality theory for sparsity-constrained minimization with $\\ell_2$-regularized objective and proposing an IHT-style algorithm for dual maximization. Our sparse duality theory provides a set of sufficient and necessary conditions under which the original NP-hard/non-convex problem can be equivalently solved in a dual space. The proposed dual IHT algorithm is a super-gradient method for maximizing the non-smooth dual objective. An interesting finding is that the sparse recovery performance of dual IHT is invariant to the Restricted Isometry Property (RIP), which is required by all the existing primal IHT without sparsity relaxation. Moreover, a stochastic variant of dual IHT is proposed for large-scale stochastic optimization. Numerical results demonstrate that dual IHT algorithms can achieve more accurate model estimation given small number of training data and have higher computational efficiency than the state-of-the-art primal IHT-style algorithms.", "histories": [["v1", "Wed, 1 Mar 2017 03:30:47 GMT  (122kb,D)", "https://arxiv.org/abs/1703.00119v1", null], ["v2", "Wed, 21 Jun 2017 02:14:17 GMT  (141kb,D)", "http://arxiv.org/abs/1703.00119v2", null]], "reviews": [], "SUBJECTS": "cs.LG stat.ML", "authors": ["bo liu 0005", "xiao-tong yuan", "lezi wang", "qingshan liu", "dimitris n metaxas"], "accepted": true, "id": "1703.00119"}, "pdf": {"name": "1703.00119.pdf", "metadata": {"source": "CRF", "title": "Dual Iterative Hard Thresholding: From Non-convex Sparse Minimization to Non-smooth Concave Maximization", "authors": ["Bo Liu", "Xiao-Tong-Yuan", "Lezi Wang", "Qingshan Liu", "Dimitris N. Metaxas"], "emails": [], "sections": [{"heading": null, "text": "Key words: Distributed Optimization, Structured-Sparsity, ADMM, Image Segmentation."}, {"heading": "1 Introduction", "text": "It has become apparent that it is about a way and a way in which it is about a way and a way in which it is about a way and a way in which it is about a way and a way in which it is about a way and a way in which it is about a way and a way in which it is about a way and a way in which it is about a way and a way in which it is about a way and a way in which it is about a way and a way in which it is about a way and a way in which it is about a way and a way in which it is about a way and a way in which it is about a way and a way in which it is about a way and a way in which it is about a way and a way in which it is about a way and a way in which it is about a way and a way in which it is about a way in which it is about a way and a way in which it is about a way in which it is about and in which it is about a way in which it is about a way in which it is about a way and a way in which it is about a way in which it is about a way in which it is about a way and a way in which it is about a way in which it is about a way in which it is about a way and a way in which it is about a way in which it is about a way in which it is about a way in which it is about a way and a way in which it is about a way in which it is about a way in which it is about a way in which it is about a way in which it is about a way in which it is about a way in which it is about a way and a way and a way it is about which it is about a way in which it is about a way in which it is about a way in which it is about a way in which it is about a way and a way in which it is about which it is about a way in which it is about a way in which it is about a way and a way in which it is not only in which it is about which it is about which it is about which it is about which it is about which it is about which it is about which it is about which it is about which it is about which it is about which it is about which it is about which it is about which it is about which it is not"}, {"heading": "2 Related Work", "text": "In (Jain et al., 2014) several relaxed variants of IHT-like algorithms were presented for which the estimation consistency can be determined without requiring the RIP conditions. In (Bahmani et al., 2013) a gradient support tracking algorithm is proposed and analyzed. In large-scale environments, where a complete gradient assessment of all data becomes a bottleneck, stochastic and variance reduction techniques have been used to improve the computing efficiency of IHT objectives (Nguyen et al., 2014; Li et al., 2016; Chen & Gu, 2016)."}, {"heading": "3 A Sparse Lagrangian Duality Theory", "text": "In this section we present a weak and strong duality theory that guarantees the original non-convex and non-convex existence (1,1)."}, {"heading": "4 Dual Iterative Hard Thresholding", "text": "In general, D (\u03b1) is a non-smooth function, since: 1) the conjugate function l \u0443 i of an arbitrary convex loss li is generally not smooth, and 2) the term \u0430 w (\u03b1) \u0445 2 is not smooth in relation to \u03b1 due to the section process associated with the calculation of w (\u03b1). Therefore, smooth optimization methods are not directly applicable here, and we resort to sub-gradient type methods to solve the problem of non-smooth dual maximization in (3.4)."}, {"heading": "4.1 Algorithm", "text": "The Dual Iterative Hard Thresholding (DIHT) algorithm as outlined in Algorithm 1 is essentially a projected supergradient method to maximize D (\u03b1). The method generates a sequence of primary-dual pairs (w (0), \u03b1 (0)), (w (1), \u03b1 (1),..... from an initial pair (0) = 0 and (0) = 0. At the tth iteration, the dual update block S1 performs the projected supergradient ascent in (4.1) to update the method (t) of \u03b1 (t \u2212 1) and w (t \u2212 1). In the primary update stage S2, the primary variable w (t) is then constructed from a k-sparse supergradient approach (t)."}, {"heading": "4.2 Convergence analysis", "text": "We analyze the non-asymptotic convergence behavior of DIHT and SDIHT = = > SDIHT. In the following analysis, we will name the non-asymptotic convergence behavior of DIHT and SDIHT = = = dual = = dual = error (max.) (max.).Let us call r = maxa (max.) = maxa (max.) = maxa (max.) = maxa (max.) = implicit (max.) = implicit (max.) = implicit (max.) = maxa (max.) = implicit (max.) = implicit (max.): max. (max.) implicit (max.) implicit (max.) implicit (max.) implicit (max.) implicit (max.) max. (max.) max. (max.) max. (max.) max. (max. (max.) implicit (max. (max.) max. (max.) max. (max. (max.) implicit (max. (max.) max. (max. (max.). (max.)"}, {"heading": "5 Experiments", "text": "This section is dedicated to demonstrating the accuracy and efficiency of the proposed algorithms. We first show the model estimation performance of the DIHT when applied to sparse burr regression models on synthetic datasets. Then we evaluate the efficiency of the DIHT / SDIHT for sparsely regulated Huber loss and hinge loss minimization tasks using real datasets."}, {"heading": "5.1 Model parameter estimation accuracy evaluation", "text": "A synthetic model is created with sparse model parameters, w = [1, 1, \u00b7 \u00b7 q = > Error rate = > q = = indirectly solved; the first component is the top-k function, which is drawn from the multivariate Gaussian distribution N (\u00b51, \u03a3); each entry in \u00b51 \"Rk\" follows the standard normal distribution independently; the first component is the top-k function, which is drawn from the multivariate Gaussian distribution N (\u00b51 \"Rk\"); the second component consists of the left d \"-k\" function, the dimensions follow N (\u00b52 \"Rk\"), where each entry in \u00b52 \"Rd\" k \"is drawn from the standard normal distribution; we simulate two data parameter settings: (1) d\" 500, k \"100.\""}, {"heading": "5.2 Model training efficiency evaluation", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "5.2.1 Huber loss model learning", "text": "We are now evaluating the algorithms considered for the evaluation of algorithm efficiency and comparison. We are selecting 0.5 million samples for loss reduction (VKI) for the following \"2-regulated sparse Huber loss minimization problem\": min-w-0 \u2264 k1N N \u2211 i = 1 lHuber (yix > i = 1 lHuber (yix > i w < 1 \u2212 \u03b31 2\u03b3 (1 \u2212 yix > i w) 2 otherwise. It is known that Shalev-Shwartz & Zhang (2013b) l \u0445 1 \u2212 yix > i + 2 yix > i if < 1 \u2212 g > i + others.Two binary benchmark data sets from LibSVM data repository1, RCV1 (d = 47, 236) and News20 (d = 1, 355, 191).We are selecting 0.5 million samples for the reduction of VKI data."}, {"heading": "5.2.2 Hinge loss model learning", "text": "We continue to test the performance of our algorithms when applying them to the following \"2-regulated sparse loss minimization problem with hinges,\" whereby lHinge (yix > i w) = max (0, 1 \u2212 yiw > xi). It is standard to know Hsieh et al. (2008) l \u0445 hinge (\u03b1i) = {yi\u03b1i if yi\u03b1i [\u2212 1, 0] + \u221e otherwise. We follow the same experimental protocol as in \u00a7 5.2.1 to compare the algorithms considered on the benchmark datasets, the time cost comparison is illustrated in Figure 5.4 and the primary dual-gap sub-optimism is illustrated in Figure 5.5. This group of results shows that DIHT and SDIHT still have remarkable efficiency advantages over the IHT algorithms considered to be primary, even if the loss function is not smooth."}, {"heading": "6 Conclusion", "text": "In this paper, we systematically examine the theory of duality and algorithms for solving the sparsely limited minimization problem, which is NP-hard and non-convex in its original form. As a theoretical contribution, we develop a sparse Lagrange duality theory, which guarantees strong duality in sparse environments, under mild, sufficient and necessary conditions. This theory opens the door to solving the original NP-hard / non-convex problem equally in a dual space. We then propose DIHT as a first-order method to maximize the uneven double concave formulation, characterized by two supergradient ascent and original hard threshold. In order to further improve iteration efficiency in large-scale environments, we propose SDIHT as a block-stochastic variant of DIHT. For both algorithms, we have demonstrated a sublinear Ur-dual-gap convergence rate, if we have Ur-loss-free that is DIHT."}, {"heading": "Acknowledgements", "text": "Xiao-Tong Yuan is partially supported by the Natural Science Foundation of China (NSFC) with grant 61402232, 61522308 and partially by the Natural Science Foundation of Jiangsu Province of China (NSFJPC) with grant BK20141003. Qingshan Liu is partially supported by NSFC with grant 61532009."}, {"heading": "A Technical Proofs", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "A.1 Proof of Theorem 1", "text": "Proof. \": If the pair (w \u00b2, q \u00b2) is a sparse saddle point for L \u00b2, then it follows from the definition of conjugate convexity and inequality (3.2) that for each x \u00b2 condition (w \u00b2) = max \u00b2 -FN L (w \u00b2, \u03b1 \u00b2) \u2264 L (w \u00b2, \u03b1 \u00b2) = min \u00b2 -W (w \u00b2). On the other hand, we know that for each x \u00b2 condition (w \u00b2) \u00b2 -k and \u03b1 \u00b2 -FNL (w \u00b2) \u2264 \u00b2 -FNL (w \u00b2, \u03b1 \u00b2) = P (w \u00b2) = P (w \u00b2).By combining the two preceding ineties, we obtain P (w \u00b2) \u2212 xi \u00b2 -k \u00b2 condition (w \u00b2)."}, {"heading": "A.2 Proof of Theorem 2", "text": "Prove that the following applies to any k-z-z-z-z-z-z-z-z-z-z-z-z-z-z-z-z-z-z-z-z-z-z-z-z-z-z-z-z-z-z-z-z-z-z-z-z-z-z-z-z-z-z-z-z-z-z-z-z-z-z-z-z-z-z-z-z-z-z-z-z-z-z-z-z-z-z-z-z-z-z-z-z-z-z-z-z-z-z-z-z-z-z-z-z-z-z-z-z-z-z-z-z-z-z-z-z-z-z-z-z-z-z-z-z-z-z-z-z-z-z-z-z-z-z-z-z-z-z-z-z-z-z-z-z-z-z-z-z-z-z-z-z-z-z-z-z-z-z-z-z-z-z-z-z-z-z-z-z-z-z-z-z-z-z-z-z-z-z-z-z-z-z-z-z-z-z-z-z-z-z-z-z-z-z-z-z-z-z-z-z-z-z-z-z-z-z-z-z-z-z-z-z-z-z-z-z-z-z-z-z-z-z-z-z-z-z-z-z-z-z-z-z-z-z-z-z-z-z-z-z-z-z-z-z-z-z-z-z-z-z-z-z-z-z-z-z-z-z-z-z-z-z-z-z-z-z-z-z-z-z-"}, {"heading": "A.3 Proof of Lemma 1", "text": "The proof: For all fixed \u03b1-FN it is easy to verify that the k-sparse minimum of L (w, \u03b1) in relation to w is reached at the following point: w (\u03b1) = arg min (p), w (p), 0 \u2264 k L (w, \u03b1) = Hk (\u2212 1 N). Thus, we have D (\u03b1) = min (w) 0 \u2264 k L (w) = L (w (p), \u03b1) = 1N N N (p) = 1 (p) > xi \u2212 l (p) i (p)) + \u03bb 2 (p). Let us now consider two arbitrary dual variables: a (p) \u00b2 1 = 1N (p)."}, {"heading": "A.4 Proof of Theorem 3", "text": "Proof. \"\u21d2\": Due to the conditions in theorem, Theorem 1 can show that the pair (w, \u03b1) forms a sparse saddle point of L. On the basis of the definitions of the sparse saddle point and the dual function D (\u03b1), we can show that D (\u03b1) = min-w-0 \u2264 kL (w, \u03b1) \u2265 L (w-W, \u03b1-L) \u2265 L (w-W, \u03b1). Moreover, theorem 2 guarantees that the following D (\u03b1) = max FN min-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W"}, {"heading": "A.5 Proof of Theorem 4", "text": "We need a number of technical lemmas to prove this theorem. The following lemmas show that w (\u03b1) under correct conditions is locally smooth around w = w (\u03b1). Lemma 2. Let X = [x1,..., xN] and Rd \u00b7 N be the data matrix. Suppose that {li} i = 1,..., N are differentiable and thus equivalent: = w-min \u2212 1 \u03bb-P \u2032 (w-P)."}, {"heading": "If \u2016\u03b1\u2212 \u03b1\u0304\u2016 \u2264 \u03bbN\u03042\u03c3max(X) , then supp(w(\u03b1)) = supp(w\u0304) and", "text": "This suggests that the F-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-"}, {"heading": "The primal-dual gap PD(w,\u03b1) can be expressed as:", "text": "It is derived directly from the definitions of P (w) and D (f) that P (w) \u2212 D (f) \u2212 l (f), that P (w) \u2212 D (f) \u2212 l (f), that P (f) \u2212 D (f), (f), (f), (f), (f), (f), (f), (f), (f), (f), (f), (f), (f), (f), (f), (f), (f), (f), (f), (f), (f), (f), (f), (f), (f), (f), (f), (f), (f), (f), (f), (f), (f), (f), (f), (f), (f), (f), (f)."}, {"heading": "A.6 Proof of Theorem 5", "text": "Proof (a): Let us g (t) with g (t) with g (t) with g (p) with g (p) with v (p) with v (p) with v (p) with v (p) with v (p) with v (p) = < g (t) with v (t) with v (t) with v (t) with v (t) with v (t) with v (t); let us g (t) with v (t) with v (t) with v (t) with v (t) with v (p) with v (p) with v) with v (t) with v \u2212 Bi: = < g (t) with v (t) with v (t) with v (t) with v (t) with v (p) with v (p) with v (p) with v (v) with v (p) with v (v) with v (p) with v (t) with v (t) with v (t)."}], "references": [{"title": "Sparse prediction with the k-support norm", "author": ["Argyriou", "Andreas", "Foygel", "Rina", "Srebro", "Nathan"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Argyriou et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Argyriou et al\\.", "year": 2012}, {"title": "Greedy sparsity-constrained optimization", "author": ["Bahmani", "Sohail", "Raj", "Bhiksha", "Boufounos", "Petros T"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Bahmani et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Bahmani et al\\.", "year": 2013}, {"title": "Sparsity constrained nonlinear optimization: Optimality conditions and algorithms", "author": ["Beck", "Amir", "Eldar", "Yonina C"], "venue": "SIAM Journal on Optimization,", "citeRegEx": "Beck et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Beck et al\\.", "year": 2013}, {"title": "Compressed sensing with nonlinear observations and related nonlinear optimization problems", "author": ["Blumensath", "Thomas"], "venue": "IEEE Transactions on Information Theory,", "citeRegEx": "Blumensath and Thomas.,? \\Q2013\\E", "shortCiteRegEx": "Blumensath and Thomas.", "year": 2013}, {"title": "Iterative hard thresholding for compressed sensing", "author": ["Blumensath", "Thomas", "Davies", "Mike E"], "venue": "Applied and Computational Harmonic Analysis,", "citeRegEx": "Blumensath et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Blumensath et al\\.", "year": 2009}, {"title": "A first-order primal-dual algorithm for convex problems with applications to imaging", "author": ["Chambolle", "Antonin", "Pock", "Thomas"], "venue": "Journal of Mathematical Imaging and Vision,", "citeRegEx": "Chambolle et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Chambolle et al\\.", "year": 2011}, {"title": "Accelerated stochastic block coordinate gradient descent for sparsity constrained nonconvex optimization", "author": ["Chen", "Jinghui", "Gu", "Quanquan"], "venue": "In Conference on Uncertainty in Artificial Intelligence,", "citeRegEx": "Chen et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Chen et al\\.", "year": 2016}, {"title": "Hard thresholding pursuit: an algorithm for compressive sensing", "author": ["Foucart", "Simon"], "venue": "SIAM Journal on Numerical Analysis,", "citeRegEx": "Foucart and Simon.,? \\Q2011\\E", "shortCiteRegEx": "Foucart and Simon.", "year": 2011}, {"title": "A dual coordinate descent method for large-scale linear svm", "author": ["Hsieh", "Cho-Jui", "Chang", "Kai-Wei", "Lin", "Chih-Jen", "Keerthi", "S Sathiya", "Sundararajan", "Sellamanickam"], "venue": "In International conference on Machine learning,", "citeRegEx": "Hsieh et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Hsieh et al\\.", "year": 2008}, {"title": "On iterative hard thresholding methods for high-dimensional m-estimation", "author": ["Jain", "Prateek", "Tewari", "Ambuj", "Kar", "Purushottam"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Jain et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Jain et al\\.", "year": 2014}, {"title": "Structured sparse regression via greedy hardthresholding", "author": ["Jain", "Prateek", "Rao", "Nikhil", "Dhillon", "Inderjit"], "venue": "arXiv preprint arXiv:1602.06042,", "citeRegEx": "Jain et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Jain et al\\.", "year": 2016}, {"title": "Scalable multitask representation learning for scene classification", "author": ["Lapin", "Maksim", "Schiele", "Bernt", "Hein", "Matthias"], "venue": "In IEEE Conference on Computer Vision and Pattern Recognition,", "citeRegEx": "Lapin et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Lapin et al\\.", "year": 2014}, {"title": "Fast saddle-point algorithm for generalized dantzig selector and fdr control with the ordered `1-norm", "author": ["Lee", "Sangkyun", "Brzyski", "Damian", "Bogdan", "Malgorzata"], "venue": "In International Conference on Artificial Intelligence and Statistics,", "citeRegEx": "Lee et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Lee et al\\.", "year": 2016}, {"title": "Stochastic variance reduced optimization for nonconvex sparse learning", "author": ["Li", "Xingguo", "Zhao", "Tuo", "Arora", "Raman", "Liu", "Han", "Haupt", "Jarvis"], "venue": "In International Conference on Machine Learning,", "citeRegEx": "Li et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Li et al\\.", "year": 2016}, {"title": "Linear convergence of stochastic iterative greedy algorithms with sparse constraints", "author": ["Nguyen", "Nam", "Needell", "Deanna", "Woolf", "Tina"], "venue": "arXiv preprint arXiv:1407.0088,", "citeRegEx": "Nguyen et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Nguyen et al\\.", "year": 2014}, {"title": "Accelerated mini-batch stochastic dual coordinate ascent", "author": ["Shalev-Shwartz", "Shai", "Zhang", "Tong"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Shalev.Shwartz et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Shalev.Shwartz et al\\.", "year": 2013}, {"title": "Stochastic dual coordinate ascent methods for regularized loss", "author": ["Shalev-Shwartz", "Shai", "Zhang", "Tong"], "venue": "The Journal of Machine Learning Research,", "citeRegEx": "Shalev.Shwartz et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Shalev.Shwartz et al\\.", "year": 2013}, {"title": "Regression shrinkage and selection via the lasso", "author": ["Tibshirani", "Robert"], "venue": "Journal of the Royal Statistical Society. Series B (Methodological), pp", "citeRegEx": "Tibshirani and Robert.,? \\Q1996\\E", "shortCiteRegEx": "Tibshirani and Robert.", "year": 1996}, {"title": "Doubly stochastic primal-dual coordinate method for empirical risk minimization and bilinear saddle-point problem", "author": ["Yu", "Adams Wei", "Lin", "Qihang", "Yang", "Tianbao"], "venue": "arXiv preprint arXiv:1508.03390,", "citeRegEx": "Yu et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Yu et al\\.", "year": 2015}, {"title": "Gradient hard thresholding pursuit for sparsityconstrained optimization", "author": ["Yuan", "Xiao-Tong", "Li", "Ping", "Zhang", "Tong"], "venue": "In International Conference on Machine Learning,", "citeRegEx": "Yuan et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Yuan et al\\.", "year": 2014}, {"title": "Exact recovery of hard thresholding pursuit", "author": ["Yuan", "Xiao-Tong", "Li", "Ping", "Zhang", "Tong"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Yuan et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Yuan et al\\.", "year": 2016}, {"title": "Stochastic primal-dual coordinate method for regularized empirical risk minimization", "author": ["Zhang", "Yuchen", "Xiao", "Lin"], "venue": "In International Conference on Machine Learning,", "citeRegEx": "Zhang et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Zhang et al\\.", "year": 2015}, {"title": "Stochastic parallel block coordinate descent for large-scale saddle point problems", "author": ["Zhu", "Zhanxing", "Storkey", "Amos J"], "venue": "In AAAI Conference on Artificial Intelligence,", "citeRegEx": "Zhu et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Zhu et al\\.", "year": 2016}], "referenceMentions": [{"referenceID": 0, "context": ", `1 norm (Tibshirani, 1996) and k-support norm (Argyriou et al., 2012), as an alternative of the cardinality constraint.", "startOffset": 48, "endOffset": 71}, {"referenceID": 19, "context": "More recently, IHT-style methods have been generalized to handle generic convex loss functions (Beck & Eldar, 2013; Yuan et al., 2014; Jain et al., 2014) as well as structured sparsity constraints (Jain et al.", "startOffset": 95, "endOffset": 153}, {"referenceID": 9, "context": "More recently, IHT-style methods have been generalized to handle generic convex loss functions (Beck & Eldar, 2013; Yuan et al., 2014; Jain et al., 2014) as well as structured sparsity constraints (Jain et al.", "startOffset": 95, "endOffset": 153}, {"referenceID": 10, "context": ", 2014) as well as structured sparsity constraints (Jain et al., 2016).", "startOffset": 51, "endOffset": 70}, {"referenceID": 9, "context": "In (Jain et al., 2014), several relaxed variants of IHT-style algorithms were presented for which the estimation consistency can be established without requiring the RIP conditions.", "startOffset": 3, "endOffset": 22}, {"referenceID": 1, "context": "In (Bahmani et al., 2013), a gradient support pursuit algorithm is proposed and analyzed.", "startOffset": 3, "endOffset": 25}, {"referenceID": 14, "context": "In large-scale settings where a full gradient evaluation on all data becomes a bottleneck, stochastic and variance reduction techniques have been adopted to improve the computational efficiency of IHT (Nguyen et al., 2014; Li et al., 2016; Chen & Gu, 2016).", "startOffset": 201, "endOffset": 256}, {"referenceID": 13, "context": "In large-scale settings where a full gradient evaluation on all data becomes a bottleneck, stochastic and variance reduction techniques have been adopted to improve the computational efficiency of IHT (Nguyen et al., 2014; Li et al., 2016; Chen & Gu, 2016).", "startOffset": 201, "endOffset": 256}, {"referenceID": 8, "context": "Dual optimization algorithms have been widely used in various learning tasks including SVMs (Hsieh et al., 2008) and multi-task learning (Lapin et al.", "startOffset": 92, "endOffset": 112}, {"referenceID": 11, "context": ", 2008) and multi-task learning (Lapin et al., 2014).", "startOffset": 32, "endOffset": 52}, {"referenceID": 12, "context": "The successful examples of primal-dual methods include learning total variation regularized model (Chambolle & Pock, 2011) and generalized Dantzig selector (Lee et al., 2016).", "startOffset": 156, "endOffset": 174}, {"referenceID": 18, "context": "More recently, a number of stochastic variants (Zhang & Xiao, 2015; Yu et al., 2015) and parallel variants (Zhu & Storkey, 2016) were developed to make the primal-dual algorithms more scalable and efficient.", "startOffset": 47, "endOffset": 84}, {"referenceID": 19, "context": "An interesting observation is that these convergence results on (t) P are not relying on the Restricted Isometry Property (RIP) (or restricted strong condition number) which is required in most existing analysis of IHT-style algorithms (Blumensath & Davies, 2009; Yuan et al., 2014).", "startOffset": 236, "endOffset": 282}, {"referenceID": 9, "context": "In (Jain et al., 2014), several relaxed variants of IHT-style algorithms are presented for which the estimation consistency can be established without requiring the RIP conditions.", "startOffset": 3, "endOffset": 22}, {"referenceID": 9, "context": "In contrast to the RIP-free sparse recovery analysis in (Jain et al., 2014), our Theorem 4 does not require the sparsity level k to be relaxed.", "startOffset": 56, "endOffset": 75}, {"referenceID": 13, "context": "We evaluate the algorithm efficiency of DIHT and SDIHT by comparing their running time against three primal baseline algorithms: IHT, HTP and gradient hard thresholding with stochastic variance reduction (SVR-GHT) Li et al. (2016). We first run IHT by setting its convergence criterion to be |P (w (t))\u2212P (w(t\u22121))| P (w(t)) \u2264 10\u22124 or maximum number of iteration is reached.", "startOffset": 214, "endOffset": 231}, {"referenceID": 8, "context": "It is standard to know Hsieh et al. (2008) l\u2217 Hinge(\u03b1i) = { yi\u03b1i if yi\u03b1i \u2208 [\u22121, 0] +\u221e otherwise .", "startOffset": 23, "endOffset": 43}], "year": 2017, "abstractText": "Iterative Hard Thresholding (IHT) is a class of projected gradient descent methods for optimizing sparsity-constrained minimization models, with the best known efficiency and scalability in practice. As far as we know, the existing IHT-style methods are designed for sparse minimization in primal form. It remains open to explore duality theory and algorithms in such a non-convex and NP-hard problem setting. In this paper, we bridge this gap by establishing a duality theory for sparsity-constrained minimization with `2-regularized loss function and proposing an IHT-style algorithm for dual maximization. Our sparse duality theory provides a set of sufficient and necessary conditions under which the original NP-hard/non-convex problem can be equivalently solved in a dual formulation. The proposed dual IHT algorithm is a supergradient method for maximizing the non-smooth dual objective. An interesting finding is that the sparse recovery performance of dual IHT is invariant to the Restricted Isometry Property (RIP), which is required by virtually all the existing primal IHT algorithms without sparsity relaxation. Moreover, a stochastic variant of dual IHT is proposed for large-scale stochastic optimization. Numerical results demonstrate the superiority of dual IHT algorithms to the state-of-the-art primal IHT-style algorithms in model estimation accuracy and computational efficiency.", "creator": "LaTeX with hyperref package"}}}