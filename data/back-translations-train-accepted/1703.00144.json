{"id": "1703.00144", "review": {"conference": "ICML", "VERSION": "v1", "DATE_OF_SUBMISSION": "1-Mar-2017", "title": "Theoretical Properties for Neural Networks with Weight Matrices of Low Displacement Rank", "abstract": "Recently low displacement rank (LDR) matrices, or so-called structured matrices, have been proposed to compress large-scale neural networks. Empirical results have shown that neural networks with weight matrices of LDR matrices, referred as LDR neural networks, can achieve significant reduction in space and computational complexity while retaining high accuracy. We formally study LDR matrices in deep learning. First, we prove the universal approximation property of LDR neural networks with a mild condition on the displacement operators. We then show that the error bounds of LDR neural networks are as efficient as general neural networks with both single-layer and multiple-layer structure. Finally, we propose back-propagation based training algorithm for general LDR neural networks.", "histories": [["v1", "Wed, 1 Mar 2017 05:38:16 GMT  (152kb,D)", "https://arxiv.org/abs/1703.00144v1", "13 pages, 1 figure"], ["v2", "Mon, 1 May 2017 16:15:40 GMT  (312kb,D)", "http://arxiv.org/abs/1703.00144v2", "13 pages, 1 figure"], ["v3", "Fri, 19 May 2017 15:57:19 GMT  (195kb,D)", "http://arxiv.org/abs/1703.00144v3", "13 pages, 1 figure"], ["v4", "Fri, 22 Sep 2017 01:53:39 GMT  (152kb,D)", "http://arxiv.org/abs/1703.00144v4", "13 pages, 1 figure"]], "COMMENTS": "13 pages, 1 figure", "reviews": [], "SUBJECTS": "cs.LG cs.CV stat.ML", "authors": ["liang zhao", "siyu liao", "yanzhi wang", "zhe li 0001", "jian tang", "bo yuan"], "accepted": true, "id": "1703.00144"}, "pdf": {"name": "1703.00144.pdf", "metadata": {"source": "CRF", "title": "Theoretical Properties for Neural Networks with Weight Matrices of Low Displacement Rank", "authors": ["Liang Zhao", "Siyu Liao", "Yanzhi Wang", "Zhe Li", "Jian Tang", "Victor Pan", "Bo Yuan"], "emails": ["lzhao1@gradcenter.cuny.edu", "sliao2@gradcenter.cuny.edu", "ywang393@syr.edu", "zli89@syr.edu", "jtang02@syr.edu", "victor.pan@lehman.cuny.edu", "byuan@ccny.cuny.edu"], "sections": [{"heading": null, "text": "Keywords: deep learning, matrix displacement, structured matrices"}, {"heading": "1 Introduction", "text": "This year, it has reached the point where it will be able to retaliate until it is able to retaliate."}, {"heading": "2 Related Work", "text": "Universal Approximation & Error Bound Analysis: For forward-facing neural networks with a hidden layer, [4] and [11] separately demonstrated the universal approximation property, which guarantees that for each given continuous function or decision function and each error-bound > 0 layer, there is always a one-sided hidden neural network that approximates the function within an integrated error. However, this property does not specify the number of neurons required to construct such a neural network. In practice, there must be a limit to the maximum number of neurons based on the computational limit. Furthermore, the magnitude of the coefficients cannot be too large or too small. [10] In order to solve these problems for general neural networks, [11] it has been proven that it is sufficient to approximate functions with weights and distortions whose absolute values can be limited by a constant (depending on the activation function).Neurons [10] This result has been expanded to include a small number of neurons randomly directed to a number of neurons that can have a lineality."}, {"heading": "3 Preliminaries on LDR Matrices and Neural Networks", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "3.1 Matrix Displacement", "text": "A n \u00b7 n matrix M is called a structured matrix if it has a low displacement q = displacement q. [18] Specifically, with the right choice of operation matrices A and B, if the Sylvester displacement of A, B (M): = M \u2212 AMB (2) of matrix M has a rank limited by a value independent of the size of M, then matrix M is called a low displacement matrix [18]. In this paper, we will refer to these matrices as LDR matrices. Even a full-rank matrix independent of the size of M, matrix M can be called a low displacement matrix. Figure 1 illustrates a number of commonly used structured matrices, including a circulated matrix, a cauchy matrix, a toeplitz matrix matrix matrix matrix matrix matrix matrix, a Hanrix matrix matrix matrix matrix."}, {"heading": "3.2 LDR Neural Networks", "text": "In this paper, we examine the viability of using LDR matrices in neural networks. Without losing generality, we focus on a forward-facing neural network with a fully connected (hidden) layer similar to a network structure like [4]. Here, it is assumed that the input layer (with n neurons) and the hidden layer (with kn neurons) 1 are completely connected to a weight matrix W-Rn-kn of the displacement state, where r n is the domain for the input vector x of the n-dimensional hypercube In: = [0, 1] n, and the output layer contains only one neuron. The neural network can be expressed as: y = GW, TB (x) = kn-j-j = 1-forward techniques (wjD x + dependent j). (4) 1Please note that this assumption does not have generality because the format of the \u00b7 w-in the \u00b7 n can be translated overnight."}, {"heading": "3.3 Problem Statement", "text": "In this paper, we aim to provide theoretical support for the accuracy of function approximation using neural LDR networks, which represent the \"effectiveness\" of neural LDR networks compared to the original neural networks. In view of a continuous function f (x) defined at [0, 1] n, we will examine the following tasks: \u2022 Find for each > 0 an LDR weight matrix W, so that the function defined by Equation (4) has a bias vector and an LDR matrix with no more than n rows of satisfactory equation (5). \u2022 Find a multilayer neural LDR network that is error-bound (5) but has an upper limit with fewer parameters. The first task is covered in Section 4, which represents the universal approximation of neural LDR networks."}, {"heading": "4 The Universal Approximation Property of LDR Neural Networks", "text": "In this section, we will first present a Theorem for Matrix shifts. Based on the Theorem = Q = q = q = q = q = q = q = q = = = = = = = = = = = = = = = = = = = \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7"}, {"heading": "5 Error Bounds on LDR Neural Networks", "text": "With the universal approximation property, of course, we have found ways to provide error-bound estimates for LDR neural networks = = function of the following kind. We are able to prove that for LDR matrices defined by O (n) parameters, where n is the number of rows and has the same order as the number of columns), the corresponding structured neural network is able to achieve an integrated error of the order O (1 / n), where n is the number of parameters. This result is asymptotically equivalent to Barron's predicted result on general neural networks, indicating that there is essentially no loss of the restriction to LDR matrices. The functions we would like to approximate are those defined on an n-dimensional sphere, Br = {x] defined in such a way that the number of columns is so large that Br is such that we have the function of Lx (the DR x)."}, {"heading": "6 Training LDR Neural Networks", "text": "In this section we will reformulate the course calculation of the LDR networks. Calculation for propagation through a fully connected layer can be written in such a way that it is (WTx + \u03b8), (25) where there is an LDR matrix with operators (Ai, Bi), which is the weight matrix, then it is essentially determined by two matrices, the Gi, Rn, r, Hi, Rn, r, asWi = [q \u2212 1), k = 0 AkiGiH T i k] (I \u2212 aBqi) \u2212 1. In order to adapt the rear propagation algorithms, our goal is to calculate the derivatives."}, {"heading": "7 Conclusion", "text": "In this work, we have demonstrated that the universal approximation characteristics of neural LDR networks are at least as efficient as general unstructured neural networks. In addition, we are also developing the back-propagation-based training algorithm for universal neural LDR networks. Our study provides the theoretical basis for the empirical success of neural LDR networks."}], "references": [{"title": "Universal approximation bounds for superpositions of a sigmoidal function", "author": ["Andrew R Barron"], "venue": "IEEE Transactions on Information theory,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 1993}, {"title": "Polynomial and matrix computations volume 1: Fundamental algorithms", "author": ["Dario Bini", "Victor Pan", "Wayne Eberly"], "venue": "SIAM Review,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 1996}, {"title": "An exploration of parameter redundancy in deep networks with circulant projections", "author": ["Yu Cheng", "Felix X Yu", "Rogerio S Feris", "Sanjiv Kumar", "Alok Choudhary", "Shi-Fu Chang"], "venue": "In Proceedings of the IEEE International Conference on Computer Vision,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2015}, {"title": "Approximation by superpositions of a sigmoidal function", "author": ["George Cybenko"], "venue": "Mathematics of Control, Signals, and Systems (MCSS),", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 1989}, {"title": "Shallow vs. deep sum-product networks", "author": ["Olivier Delalleau", "Yoshua Bengio"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2011}, {"title": "Imagenet: A large-scale hierarchical image database", "author": ["Jia Deng", "Wei Dong", "Richard Socher", "Li-Jia Li", "Kai Li", "Li Fei-Fei"], "venue": "In Computer Vision and Pattern Recognition,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2009}, {"title": "Exploiting linear structure within convolutional networks for efficient evaluation", "author": ["Emily L Denton", "Wojciech Zaremba", "Joan Bruna", "Yann LeCun", "Rob Fergus"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2014}, {"title": "Compressing deep convolutional networks using vector quantization", "author": ["Yunchao Gong", "Liu Liu", "Ming Yang", "Lubomir Bourdev"], "venue": "arXiv preprint arXiv:1412.6115,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2014}, {"title": "Deep compression: Compressing deep neural networks with pruning, trained quantization and huffman coding", "author": ["Song Han", "Huizi Mao", "William J Dally"], "venue": "arXiv preprint arXiv:1510.00149,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2015}, {"title": "Approximation capabilities of multilayer feedforward networks", "author": ["Kurt Hornik"], "venue": "Neural networks,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 1991}, {"title": "Multilayer feedforward networks are universal approximators", "author": ["Kurt Hornik", "Maxwell Stinchcombe", "Halbert White"], "venue": "Neural networks,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 1989}, {"title": "Speeding up convolutional neural networks with low rank expansions", "author": ["Max Jaderberg", "Andrea Vedaldi", "Andrew Zisserman"], "venue": "arXiv preprint arXiv:1405.3866,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2014}, {"title": "Learning multiple layers of features from tiny images", "author": ["Alex Krizhevsky", "Geoffrey Hinton"], "venue": null, "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2009}, {"title": "Imagenet classification with deep convolutional neural networks. In Advances in neural information processing", "author": ["Alex Krizhevsky", "Ilya Sutskever", "Geoffrey E Hinton"], "venue": null, "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2012}, {"title": "Why deep neural networks", "author": ["Shiyu Liang", "R Srikant"], "venue": "arXiv preprint arXiv:1610.04161,", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2016}, {"title": "Sparse convolutional neural networks", "author": ["Baoyuan Liu", "Min Wang", "Hassan Foroosh", "Marshall Tappen", "Marianna Pensky"], "venue": "In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition,", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2015}, {"title": "On the number of linear regions of deep neural networks", "author": ["Guido F Montufar", "Razvan Pascanu", "Kyunghyun Cho", "Yoshua Bengio"], "venue": "In Advances in neural information processing systems,", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2014}, {"title": "Structured matrices and polynomials: unified superfast algorithms", "author": ["Victor Pan"], "venue": "Springer Science & Business Media,", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2001}, {"title": "Estimating the norms of random circulant and toeplitz matrices and their inverses", "author": ["Victor Y Pan", "John Svadlenka", "Liang Zhao"], "venue": "Linear algebra and its applications,", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2015}, {"title": "Structured transforms for small-footprint deep learning", "author": ["Vikas Sindhwani", "Tara Sainath", "Sanjiv Kumar"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "20", "shortCiteRegEx": "20", "year": 2015}, {"title": "Sequence to sequence learning with neural networks", "author": ["Ilya Sutskever", "Oriol Vinyals", "Quoc V Le"], "venue": "In Advances in neural information processing systems,", "citeRegEx": "21", "shortCiteRegEx": "21", "year": 2014}, {"title": "Benefits of depth in neural networks", "author": ["Matus Telgarsky"], "venue": "arXiv preprint arXiv:1602.04485,", "citeRegEx": "22", "shortCiteRegEx": "22", "year": 2016}, {"title": "Learning structured sparsity in deep neural networks", "author": ["Wei Wen", "Chunpeng Wu", "Yandan Wang", "Yiran Chen", "Hai Li"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "23", "shortCiteRegEx": "23", "year": 2016}], "referenceMentions": [{"referenceID": 4, "context": "Theoretical Properties for Neural Networks with Weight Matrices of Low Displacement Rank Liang Zhao, Siyu Liao, Yanzhi Wang, Zhe Li, Jian Tang, Victor Pan , and Bo Yuan [5],[g] [1] Department of Mathematics, Graduate Center of the City University of New York [2] Department of Computer Science, Graduate Center of the City University of New York [3] Department of Electrical Engineering and Computer Science, Syracuse Univ.", "startOffset": 169, "endOffset": 172}, {"referenceID": 0, "context": "Theoretical Properties for Neural Networks with Weight Matrices of Low Displacement Rank Liang Zhao, Siyu Liao, Yanzhi Wang, Zhe Li, Jian Tang, Victor Pan , and Bo Yuan [5],[g] [1] Department of Mathematics, Graduate Center of the City University of New York [2] Department of Computer Science, Graduate Center of the City University of New York [3] Department of Electrical Engineering and Computer Science, Syracuse Univ.", "startOffset": 177, "endOffset": 180}, {"referenceID": 1, "context": "Theoretical Properties for Neural Networks with Weight Matrices of Low Displacement Rank Liang Zhao, Siyu Liao, Yanzhi Wang, Zhe Li, Jian Tang, Victor Pan , and Bo Yuan [5],[g] [1] Department of Mathematics, Graduate Center of the City University of New York [2] Department of Computer Science, Graduate Center of the City University of New York [3] Department of Electrical Engineering and Computer Science, Syracuse Univ.", "startOffset": 259, "endOffset": 262}, {"referenceID": 2, "context": "Theoretical Properties for Neural Networks with Weight Matrices of Low Displacement Rank Liang Zhao, Siyu Liao, Yanzhi Wang, Zhe Li, Jian Tang, Victor Pan , and Bo Yuan [5],[g] [1] Department of Mathematics, Graduate Center of the City University of New York [2] Department of Computer Science, Graduate Center of the City University of New York [3] Department of Electrical Engineering and Computer Science, Syracuse Univ.", "startOffset": 346, "endOffset": 349}, {"referenceID": 3, "context": "[4] Departments of Mathematics and Computer Science, Lehman College and the Graduate Center of the City University of New York [5] Department of Electrical Engineering, City College of the City University of New York [a] lzhao1@gradcenter.", "startOffset": 0, "endOffset": 3}, {"referenceID": 4, "context": "[4] Departments of Mathematics and Computer Science, Lehman College and the Graduate Center of the City University of New York [5] Department of Electrical Engineering, City College of the City University of New York [a] lzhao1@gradcenter.", "startOffset": 127, "endOffset": 130}, {"referenceID": 13, "context": "[14][21].", "startOffset": 0, "endOffset": 4}, {"referenceID": 20, "context": "[14][21].", "startOffset": 4, "endOffset": 8}, {"referenceID": 8, "context": "To overcome these limitations, many studies and approaches, such as connection pruning [9][8], low rank approximation [7][12], sparsity regularization [23][16] etc.", "startOffset": 87, "endOffset": 90}, {"referenceID": 7, "context": "To overcome these limitations, many studies and approaches, such as connection pruning [9][8], low rank approximation [7][12], sparsity regularization [23][16] etc.", "startOffset": 90, "endOffset": 93}, {"referenceID": 6, "context": "To overcome these limitations, many studies and approaches, such as connection pruning [9][8], low rank approximation [7][12], sparsity regularization [23][16] etc.", "startOffset": 118, "endOffset": 121}, {"referenceID": 11, "context": "To overcome these limitations, many studies and approaches, such as connection pruning [9][8], low rank approximation [7][12], sparsity regularization [23][16] etc.", "startOffset": 121, "endOffset": 125}, {"referenceID": 22, "context": "To overcome these limitations, many studies and approaches, such as connection pruning [9][8], low rank approximation [7][12], sparsity regularization [23][16] etc.", "startOffset": 151, "endOffset": 155}, {"referenceID": 15, "context": "To overcome these limitations, many studies and approaches, such as connection pruning [9][8], low rank approximation [7][12], sparsity regularization [23][16] etc.", "startOffset": 155, "endOffset": 159}, {"referenceID": 17, "context": "Since an LDR matrix typically requires O(n) independent parameters and exhibits fast matrix operation algorithms [18], an immense space for network model and computational complexity reduction can be enabled.", "startOffset": 113, "endOffset": 117}, {"referenceID": 2, "context": "Pioneering work in this direction [3][20] applied special types of LDR matrices (structured matrices), such as circulant", "startOffset": 34, "endOffset": 37}, {"referenceID": 19, "context": "Pioneering work in this direction [3][20] applied special types of LDR matrices (structured matrices), such as circulant", "startOffset": 37, "endOffset": 41}, {"referenceID": 8, "context": "First, unlike heuristic weight-pruning methods [9][8] that produce irregular pruned networks, the LDR construction approach always guarantees the strong structure of the trained network, thereby avoiding the storage space and computation time overhead incurred by the complicated indexing process.", "startOffset": 47, "endOffset": 50}, {"referenceID": 7, "context": "First, unlike heuristic weight-pruning methods [9][8] that produce irregular pruned networks, the LDR construction approach always guarantees the strong structure of the trained network, thereby avoiding the storage space and computation time overhead incurred by the complicated indexing process.", "startOffset": 50, "endOffset": 53}, {"referenceID": 17, "context": "Different from other network compression approaches that can only provide a heuristic compression factor, the LDR construction can enable the model reduction and computational complexity reduction in Big-O complexity: The storage requirement is reduced fromO(n) toO(n), and the computational complexity can be reduced from O(n) to O(n log n) or O(n log n) because of the existence of fast matrix-vector multiplication algorithm [18][2] for LDR matrices.", "startOffset": 428, "endOffset": 432}, {"referenceID": 1, "context": "Different from other network compression approaches that can only provide a heuristic compression factor, the LDR construction can enable the model reduction and computational complexity reduction in Big-O complexity: The storage requirement is reduced fromO(n) toO(n), and the computational complexity can be reduced from O(n) to O(n log n) or O(n log n) because of the existence of fast matrix-vector multiplication algorithm [18][2] for LDR matrices.", "startOffset": 432, "endOffset": 435}, {"referenceID": 5, "context": "For example, when applying structured matrices to the fully-connected layers of AlexNet using ImageNet dataset [6], the storage requirement can be reduced by more than 4,000X while incurring negligible degradation in overall accuracy [3].", "startOffset": 111, "endOffset": 114}, {"referenceID": 2, "context": "For example, when applying structured matrices to the fully-connected layers of AlexNet using ImageNet dataset [6], the storage requirement can be reduced by more than 4,000X while incurring negligible degradation in overall accuracy [3].", "startOffset": 234, "endOffset": 237}, {"referenceID": 2, "context": "Although [3][20] have already shown that using LDR construction still results the same accuracy or minor degradation on various datasets, such as ImageNet [6], CIFAR [13] etc.", "startOffset": 9, "endOffset": 12}, {"referenceID": 19, "context": "Although [3][20] have already shown that using LDR construction still results the same accuracy or minor degradation on various datasets, such as ImageNet [6], CIFAR [13] etc.", "startOffset": 12, "endOffset": 16}, {"referenceID": 5, "context": "Although [3][20] have already shown that using LDR construction still results the same accuracy or minor degradation on various datasets, such as ImageNet [6], CIFAR [13] etc.", "startOffset": 155, "endOffset": 158}, {"referenceID": 12, "context": "Although [3][20] have already shown that using LDR construction still results the same accuracy or minor degradation on various datasets, such as ImageNet [6], CIFAR [13] etc.", "startOffset": 166, "endOffset": 170}, {"referenceID": 2, "context": "The proposed algorithm is the generalization of the training process in [3][20] that restricts the structure of weight matrices to circulant matrices or Toeplitz matrices.", "startOffset": 72, "endOffset": 75}, {"referenceID": 19, "context": "The proposed algorithm is the generalization of the training process in [3][20] that restricts the structure of weight matrices to circulant matrices or Toeplitz matrices.", "startOffset": 75, "endOffset": 79}, {"referenceID": 3, "context": "2 Related Work Universal Approximation & Error Bound Analysis: For feedforward neural networks with one hidden layer, [4] and [11] proved separately the universal approximation property, which guarantees that for any given continuous function or decision function and any error bound > 0, there always exists a single-hidden layer neural network that approximates the function within integrated error.", "startOffset": 118, "endOffset": 121}, {"referenceID": 10, "context": "2 Related Work Universal Approximation & Error Bound Analysis: For feedforward neural networks with one hidden layer, [4] and [11] proved separately the universal approximation property, which guarantees that for any given continuous function or decision function and any error bound > 0, there always exists a single-hidden layer neural network that approximates the function within integrated error.", "startOffset": 126, "endOffset": 130}, {"referenceID": 10, "context": "To address these issues for general neural networks, [11] proved that it is sufficient to approximate functions with weights and biases whose absolute values are bounded by a constant (depending on the activation function).", "startOffset": 53, "endOffset": 57}, {"referenceID": 9, "context": "[10] further extended this result to an arbitrarily small bound.", "startOffset": 0, "endOffset": 4}, {"referenceID": 0, "context": "[1] showed that feedforward networks with one layer of sigmoidal nonlinearities achieve an integrated squared error with order of O(1/n), where n is the number of neurons.", "startOffset": 0, "endOffset": 3}, {"referenceID": 4, "context": "[5] have shown that there exist certain functions that can be approximated by three-layer neural networks with a polynomial amount of neurons, while two-layer neural networks require exponentially larger amount to achieve the same error.", "startOffset": 0, "endOffset": 3}, {"referenceID": 16, "context": "[17] and [22] have shown the exponential increase of linear regions as neural networks grow deeper.", "startOffset": 0, "endOffset": 4}, {"referenceID": 21, "context": "[17] and [22] have shown the exponential increase of linear regions as neural networks grow deeper.", "startOffset": 9, "endOffset": 13}, {"referenceID": 14, "context": "[15] proved that with log(1/ ) layers, the neural network can achieve the error bound for any continuous function with O(polylog( )) parameters in each layer.", "startOffset": 0, "endOffset": 4}, {"referenceID": 2, "context": "LDR Matrices in Neural Networks: [3] have analyzed the effectiveness of replacing conventional weight matrices in fully-connected layers with circulant matrices, which can reduce the time complexity fromO(n) to O(n log n), and the space complexity from O(n) to O(n), respectively.", "startOffset": 33, "endOffset": 36}, {"referenceID": 19, "context": "[20] have demonstrated significant benefits of using Toeplitz-like matrices to tackle the issue of large space and computation requirement for neural networks training and inference.", "startOffset": 0, "endOffset": 4}, {"referenceID": 17, "context": "1 Matrix Displacement An n\u00d7n matrix M is called a structured matrix when it has a low displacement rank \u03b3 [18].", "startOffset": 106, "endOffset": 110}, {"referenceID": 19, "context": "[20]).", "startOffset": 0, "endOffset": 4}, {"referenceID": 17, "context": "of matrix M have a rank \u03b3 bounded by a value that is independent of the size of M, then matrix M is referred to as a matrix with a low displacement rank [18].", "startOffset": 153, "endOffset": 157}, {"referenceID": 17, "context": "7 in [18].", "startOffset": 5, "endOffset": 9}, {"referenceID": 18, "context": ", the block-circulant matrices [19].", "startOffset": 31, "endOffset": 35}, {"referenceID": 3, "context": "Without loss of generality, we focus on a feed-forward neural network with one fully-connected (hidden) layer, which is similar network setup as [4].", "startOffset": 145, "endOffset": 148}, {"referenceID": 0, "context": "The domain for the input vector x is the n-dimensional hypercube I := [0, 1], and the output layer only contains one neuron.", "startOffset": 70, "endOffset": 76}, {"referenceID": 2, "context": "1Please note that this assumption does not sacrifice any generality because the n-by-m case can be transformed to n-by-kn format with the nearest k using zero padding [3].", "startOffset": 167, "endOffset": 170}, {"referenceID": 0, "context": "Given a continuous function f(x) defined on [0, 1], we study the following tasks: \u2022 For any > 0, find an LDR weight matrix W so that the function defined by equation (4) satisfies", "startOffset": 44, "endOffset": 50}, {"referenceID": 0, "context": "max x\u2208[0,1]n |f(x)\u2212GW,\u03b8(x)| < .", "startOffset": 6, "endOffset": 11}, {"referenceID": 3, "context": "[4]) Definition 4.", "startOffset": 0, "endOffset": 3}, {"referenceID": 3, "context": "The statement of this lemma and its proof is included in [4].", "startOffset": 57, "endOffset": 60}, {"referenceID": 2, "context": "Reference work [3], [20] have utilized a circulant matrix or a Toeplitz matrix for weight representation in deep neural networks.", "startOffset": 15, "endOffset": 18}, {"referenceID": 19, "context": "Reference work [3], [20] have utilized a circulant matrix or a Toeplitz matrix for weight representation in deep neural networks.", "startOffset": 20, "endOffset": 24}, {"referenceID": 2, "context": "Please note that for the general case of n-by-m weight matrices, either the more general Block-circulant matrices should be utilized or padding extra columns or rows of zeroes are needed [3].", "startOffset": 187, "endOffset": 190}, {"referenceID": 2, "context": "Circulant matrices and Topelitz matrices are both special form of LDR matrices, and thus we could apply the above universal approximation property of LDR neural networks and provide theoretical support for the use of circulant and Toeplitz matrices in [3], [20].", "startOffset": 252, "endOffset": 255}, {"referenceID": 19, "context": "Circulant matrices and Topelitz matrices are both special form of LDR matrices, and thus we could apply the above universal approximation property of LDR neural networks and provide theoretical support for the use of circulant and Toeplitz matrices in [3], [20].", "startOffset": 257, "endOffset": 261}, {"referenceID": 0, "context": "[1] considered the following set of bounded multiples of a sigmoidal function composed with linear functions: G\u03c3 = {\u03b1\u03c3(yx + \u03b8) : |\u03b1| \u2264 2C,y \u2208 R, \u03b8 \u2208 R}.", "startOffset": 0, "endOffset": 3}, {"referenceID": 0, "context": "1 ([1]).", "startOffset": 3, "endOffset": 6}, {"referenceID": 14, "context": "The next theorem naturally extended the result from [15] to LDR neural networks, indicating that LDR neural networks can also benefit a parameter reduction if one uses more than one layers.", "startOffset": 52, "endOffset": 56}, {"referenceID": 0, "context": "Let f be a continuous function on [0, 1] and is 2n + 1 times differentiable in (0, 1) for n = dlog 1 + 1]e.", "startOffset": 34, "endOffset": 40}, {"referenceID": 14, "context": "The theorem with better bounds and without assumption of being LDR neural network is proved in [15] as Theorem 4.", "startOffset": 95, "endOffset": 99}], "year": 2017, "abstractText": "Recently low displacement rank (LDR) matrices, or so-called structured matrices, have been proposed to compress large-scale neural networks. Empirical results have shown that neural networks with weight matrices of LDR matrices, referred as LDR neural networks, can achieve significant reduction in space and computational complexity while retaining high accuracy. We formally study LDR matrices in deep learning. First, we prove the universal approximation property of LDR neural networks with a mild condition on the displacement operators. We then show that the error bounds of LDR neural networks are as efficient as general neural networks with both single-layer and multiple-layer structure. Finally, we propose backpropagation based training algorithm for general LDR neural networks.", "creator": "LaTeX with hyperref package"}}}