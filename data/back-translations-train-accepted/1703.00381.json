{"id": "1703.00381", "review": {"conference": "ICML", "VERSION": "v1", "DATE_OF_SUBMISSION": "1-Mar-2017", "title": "The Statistical Recurrent Unit", "abstract": "Sophisticated gated recurrent neural network architectures like LSTMs and GRUs have been shown to be highly effective in a myriad of applications. We develop an un-gated unit, the statistical recurrent unit (SRU), that is able to learn long term dependencies in data by only keeping moving averages of statistics. The SRU's architecture is simple, un-gated, and contains a comparable number of parameters to LSTMs; yet, SRUs perform favorably to more sophisticated LSTM and GRU alternatives, often outperforming one or both in various tasks. We show the efficacy of SRUs as compared to LSTMs and GRUs in an unbiased manner by optimizing respective architectures' hyperparameters in a Bayesian optimization scheme for both synthetic and real-world tasks.", "histories": [["v1", "Wed, 1 Mar 2017 16:50:54 GMT  (570kb,D)", "http://arxiv.org/abs/1703.00381v1", null]], "reviews": [], "SUBJECTS": "cs.LG cs.AI stat.ML", "authors": ["junier b oliva", "barnab\u00e1s p\u00f3czos", "jeff g schneider"], "accepted": true, "id": "1703.00381"}, "pdf": {"name": "1703.00381.pdf", "metadata": {"source": "CRF", "title": "The Statistical Recurrent Unit", "authors": ["Junier B. Oliva", "Barnab\u00e1s P\u00f3czos", "Jeff Schneider"], "emails": ["liva@cs.cmu.edu>."], "sections": [{"heading": "1. Introduction", "text": "Analysing the shortcomings mentioned is indeed one of the greatest challenges in the history of the European Union."}, {"heading": "2. Model", "text": "As the name implies, statisticians often use summary statistics when trying to represent a dataset. Naturally, we then use an algorithm that learns to represent data that has previously been seen in a similar way to a neural statistician (Edwards & Storkey, 2016). Of course, sequence information, unlike disordered i.i.d. samples, loses valuable temporal information: first, we generate recurring statistics that depend on a context of previously seen data; second, we generate moving averages on multiple scales, allowing the model to distinguish the kind of data that could be seen at different points in the past."}, {"heading": "2.1. Recurrent Statistics", "text": "We will consider an input sequence of real evaluation points x1, x2,.., xT, Rd. As seen in the second row of Table 1, we can calculate a vector of statistics \u03c6 (xi) \u0445 RD for each point. This approach amounts to treating the sequence as a set of i.i.d. points drawn by some distributions and marginalizing time. Sure, here, one will lose temporal information that will be useful for many sequence-related ML tasks. It is interesting to note that global average pooling operations have gained a lot of current traction in evolutionary networks (Lin et al., 2013; Iandola et al., 2016). Analogous to the i.i.d. statistical approximation, we will lose spatial information."}, {"heading": "2.2. Update Equations", "text": "We have discussed in broad outlines how to generate time-conscious summary statistics using multi-scale recurring statistics. In the following, we will discuss specifically how the SRU generates and uses summary statistics for sequences. Remember that our input is a sequence of ordered points: {x1, x2,..}, xt-Rd. Throughout the process, we apply an elementary nonlinearity f (\u00b7), which we consider ReLU (Jarrett et al., 2009; Nair & Hinton, 2010): f (\u00b7) = max (\u00b7, 0). The SRU operates on exponential moving averages f (\u00b5). \u2212 Rs (7), held on various scales \u03b1 A = {\u03b11,."}, {"heading": "2.3. Intuitions from Mean Map Embeddings", "text": "The design of the SRU has been deliberately chosen so that long-term dependencies can be learned. To better explain the design and its intuition, let's take a quick detour to another use of (summary) statistics in machine learning to represent data: mean map embedding (MMEs) of distributions (Smola et al., 2007). At the core of the concept of MMEs is that you can embed a distribution through statistics (such as moments) and thus represent it. However, the MME for a distribution D with a positive, semi-defined core k is: \u00b5 [D] = EX-D [inspk (X)], (9) which are the reproducing characteristics of Hilbert space (RKHS) that can be infinitely dimensional. To represent a set Y = {y1,.,., one would use a sequence of sequences as sequences of sequences."}, {"heading": "2.3.1. DATA-DRIVEN STATISTICS", "text": "First of all, we point out the clear analogy between the mean embedding of a set Y, \u00b5 [Y] (10) and the moving average \u00b5 (\u03b1) (7). The moving averages \u00b5 (\u03b1) clearly serve as summary statistics of already seen data. However, the statistics we determine for \u00b5 (\u03b1), \u0442 (6) do not consist of apriori RKHS characteristics as is typical for MMEs, but are learned non-linear characteristics. This has the advantage of data-based statistics and can be interpreted as the use of a linear core in the learned characteristics."}, {"heading": "2.3.2. RECURSIVE STATISTICS FROM THE PAST", "text": "Secondly, remember that typical MME's use statistics that depend only on a single point x, \u03c6k (x). As mentioned above, this is fine for i.i.d. data, but loses sequential information when averaged. Instead, we want to assign statistics that depend on the data we have seen so far, as they provide context for the current point in the sequence. For example, you might want to have a statistic that tracks the difference between the current point and the mean of previous data. We provide context based on previous data by considering the statistics considered at the time t, t (6) not only as a function of xt, but also from {x1,..., xt \u2212 1} to rt (5). rt can be interpreted as a condensation of the sequence seen so far and allows us to retain sequential information even through an average operation."}, {"heading": "2.3.3. MULTI-SCALED STATISTICS", "text": "Third, the use of multi-scale moving averages in statistics gives the SRU a simple and powerful comprehensive view of past data that is typical only of this recurring unit. In short, by keeping moving averages on different scales, we are able to detect differences in statistics at different times in the past. Note that we have moving averages as: \u00b5 (\u03b1) t = (1 \u2212 \u03b1) (t + \u03b1t \u2212 1 + \u03b1 2t \u2212 2 +....) (11) Therefore, a smaller \u03b1 current statistic weighs more than older statistics; therefore, a concatenated vector \u00b5 = (\u00b5 (\u03b11),..., \u00b5 (\u03b1m) itself provides a multi-scale view of statistics over time (see Figure 2). For example, the short and long-term preservation of statistics already provides information about the evolution of the sequence over time."}, {"heading": "2.4. Viewpoints of the Past", "text": "An interesting and useful feature in maintaining multiple scales for each statistic is that simple linear combinations of statistics can be used to obtain a combinatorial number of perspectives from the past. For example, for properly selected wj, wk, wj\u00b5 (\u03b1j) \u2212 wk\u00b5 (\u03b1k), a set of past statistics can be created for \u03b1j > \u03b1k (Figure 3). Of course, more complicated linear combinations can be performed to obtain richer perspectives consisting of multiple windows. Furthermore, by using a linear projection of our statistics \u00b5t, as in ot (8), we are able to calculate output characteristics of combined viewpoints from multiple statistics. This type of multi-perspective perspective perspective of previously seen data is difficult to generate in traditional gated recurrent units, since they must encode in the sequence in which they are currently located, and then activate combined viewpoints for future use."}, {"heading": "2.5. Vanishing Gradients", "text": "As mentioned above, disappearing gradients have been shown to make it difficult to learn recursive units due to the inability to spread error gradients over time. Despite its simple, non-gated structure, the SRU has several defenses against disappearing gradients. First, units and statistics consist of ReLUs. ReLUs are easier to train for general deep networks (Nair & Hinton, 2010) and have succeeded in recursive units (Le et al., 2015). Intuitively, ReLUs allow errors to spread on positive inputs without saturation and disappearing gradients as with traditional sigmoid units. The ability of the SRU to use ReLUs (without special initialization) makes them particularly adept at learning long-term dependencies over time. In addition, the explicit moving average of statistics allows for longer-term learning. Consider the following derivation of error signal E w.r.t. an element [\u03b1 -d = 0, -\u03b1 = 0) \u03b1 = \u03b1 (1 p) (1 - \u03b1 = \u03b1 p (-p) (0.1) p (p)."}, {"heading": "3. Experiments", "text": "All experiments were conducted in Tensorflow (Abadi et al., 2016) and used the standard implementations of GRUCell and BasicLSTMCell for GRUs and LSTMs respectively. To perform a fair, unbiased comparison of the recurring units and their hyperparameters that greatly affect performance (Bergstra & Bengio, 2012), we used the Bayesian optimization package Hyperopt (Bergstra et al., 2015). We believe that such an approach gives every algorithm a fair chance to succeed without injecting distortions from experimenters or considering gross constraints on architectures. In all experiments, we used SGD for optimization using gradient clipping (Pascanu et al., 2013)."}, {"heading": "3.1. Synthetic Recurrent Unit Generated Data", "text": "We show the relative inefficiency of traditional gated units in learning long-term statistical dependencies by considering 1d synthetic data from a basic truth SRU. We start the sequences with x1 iid \u0445 N (0, 1002), and xt is the result of a projection of ot. \u2212 \u2212 sequence total of 176 points per sequence for 3200 training sequences, 400 validation sequences and 400 test sequences. We start the sequences with x1 iid recurrent statistical unit has three statistical realities (6): the positive part of the inputs (x) +, the negative part of the inputs (x) \u2212 and an internal statistic, e.g. We use \u03b1 \u00b2 5i = 1 = {0.0, 0.5, 0.9, 0.999, 0.999}."}, {"heading": "3.2. MNIST Image Classification", "text": "Next, we examine the ability of relapsing units to use long-term dependencies in data with a synthetic task using a real dataset (Le et al., 2015).In this synthetic task, each 28 x 28 gray MNIST digit image is flattened and considered as a sequence {x1,..., x784}, where xi [0, 1] (see Figure 5).The task is based on the output observed after x784 has been fed through the network to classify the digit of the corresponding image in {0,.., 9}.Consequently, we project the output after x784 of each relapsing unit to 10 dimensions and use a Softmax activation. We report the optimized results below in Table 3; due to resource constraints, each study consisted of only 10K units, which consider these dependencies to be unsurprising."}, {"heading": "3.2.1. DISSECTIVE STUDY", "text": "Next, we examine the behavior of the recurrent statistical unit with a dissective study in which we consider various parameters of the architecture. We consider different variants of the base model with: num stats = 200; r dims = 0.5; num units = 0.5. We consider the parameters initial learning rate, lr decay fixed at the optimal values found (0.1, 0.99) unless we find no learning, in which case we also try learning rates of 0.01 and 0.001. The need for multiple scaled recurring statistics. Recall that we have designed the statistics used by the SRU to capture the long-term dependencies in sequences. We did this both with recursive statistics, i.e. statistics that themselves depend on previous statistics, as well as with multi-scaled averages. We show below that both of these time-dependent design decisions are vital to capture long-term dependencies in data."}, {"heading": "3.3. Polyphonic Music Modeling", "text": "First, we used the polyphonic music data sets of Boulanger-Lewandowski et al. (2012). Each time step is a binary vector representing the notes played in each time step. As we had to predict binary vectors, we used the elemental sigmoid \u03c3. That is, the binary vector of the notes xt + 1 was modelled as \u03c3 (pt), where pt is the output after entering xt (and previous values x1,..., xt \u2212 1) through the recursive mesh. It is interesting to mention in Table 8 that the SRU is able to exceed one of the traditional gated units in each data set and exceed both in two datasets."}, {"heading": "3.4. Electronica-Genre Music MFCC", "text": "In the following experiment, we modeled the Mel frequency cepstrum coefficients (MFCCs) in a dataset of nearly 18,000 scratched-out 30 \"s sound clips of songs from the electronica genre. MFCCs are perceptually based on spectral characteristics positioned logarithmically on the mel scale, which resembles the response of the human auditory system (Muller, 2007). We examined the 13 real coefficients using the recursive units by modeling xt + 1 as a projection of the performance of a recursive unit after using x1,..., x.As can be seen in Table 9, SRUs again perform better than gated architectures, especially beating GRUs by a greater distance."}, {"heading": "3.5. Climate Data", "text": "Next, we look at weather data forecasts using the North America Regional Reanalysis (NARR) Project. The data set provides a long-term set of consistent climate data on a regional scale for the North American area. The period of reanalysis is from October 1978 to today and the analyses were carried out 8 times a day (3-hour intervals). We consider our input sequences to be years-long sequences of weather variables in one place for 2006. I.e. one input sequence will be a sequence of 2920 weather variables at a given lat / lon coordinate. We consider the following 7 variables: pres10m, 10 m pressure (pa); tcdc, total cloud cover (%); rh2 m, relative humidity 2 m (%); tmpsfc, surface temperature (k); snow, snow height (m); ugrd10m, u-component of wind 10 m above ground; 200 gr. We consider the base of the wind factor, the base of the 51-point variable of the weather component under s6."}, {"heading": "3.6. SportVu NBA Tracking data", "text": "The optical tracking data for this project was provided by STATS LLC from their SportVU product and obtained from (NBA). The data is composed of X and Y coordinates for each of the 10 players and the ball. We again minimize the square error standard for predictions. In Table 11, we observed a large margin for improvement for SRUs over gated architectures, reminiscent of the synthetic data experiment in paragraph 3.1, which indicates that this data set contains long-term dependencies that the SRU can exploit."}, {"heading": "4. Discussion", "text": "We believe that the use of summary statistics in modern recurrent units has been insufficiently researched. Although recent studies in revolutionary networks have considered global average pooling, which essentially uses high-level summary statistics to represent images, there has been little research into summary statistics for modern recurrent networks to date. To this end, we introduce the Statistical Recurrent Unit, a novel architecture that attempts to capture long-term dependencies in data using only simple moving averages and corrected-linear units. SRU was motivated by the success of Mean Map embedding to represent disordered data sets, and can be interpreted as a change in MMEs for sequential data. The main modifications are the following: Firstly, the SRU uses data-based statistics as opposed to typical MMMEs that use RKHS characteristics from a priori-selected classes of kernel units, so that the recurrent units are not subject to a recurrent point, but to a recurrent point, and secondly, to a recurrent point."}], "references": [{"title": "Random search for hyper-parameter optimization", "author": ["Bergstra", "James", "Bengio", "Yoshua"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Bergstra et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Bergstra et al\\.", "year": 2012}, {"title": "Hyperopt: a python library for model selection and hyperparameter optimization", "author": ["Bergstra", "James", "Komer", "Brent", "Eliasmith", "Chris", "Yamins", "Dan", "Cox", "David D"], "venue": "Computational Science & Discovery,", "citeRegEx": "Bergstra et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Bergstra et al\\.", "year": 2015}, {"title": "Modeling temporal dependencies in high-dimensional sequences: Application to polyphonic music generation and transcription", "author": ["Boulanger-Lewandowski", "Nicolas", "Bengio", "Yoshua", "Vincent", "Pascal"], "venue": "arXiv preprint arXiv:1206.6392,", "citeRegEx": "Boulanger.Lewandowski et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Boulanger.Lewandowski et al\\.", "year": 2012}, {"title": "On the properties of neural machine translation: Encoder-decoder approaches", "author": ["Cho", "Kyunghyun", "Van Merri\u00ebnboer", "Bart", "Bahdanau", "Dzmitry", "Bengio", "Yoshua"], "venue": "arXiv preprint arXiv:1409.1259,", "citeRegEx": "Cho et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Cho et al\\.", "year": 2014}, {"title": "Empirical evaluation of gated recurrent neural networks on sequence modeling", "author": ["Chung", "Junyoung", "Gulcehre", "Caglar", "Cho", "KyungHyun", "Bengio", "Yoshua"], "venue": "arXiv preprint arXiv:1412.3555,", "citeRegEx": "Chung et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Chung et al\\.", "year": 2014}, {"title": "Towards a neural statistician", "author": ["Edwards", "Harrison", "Storkey", "Amos"], "venue": "arXiv preprint arXiv:1606.02185,", "citeRegEx": "Edwards et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Edwards et al\\.", "year": 2016}, {"title": "Finding structure in time", "author": ["Elman", "Jeffrey L"], "venue": "Cognitive science,", "citeRegEx": "Elman and L.,? \\Q1990\\E", "shortCiteRegEx": "Elman and L.", "year": 1990}, {"title": "Towards end-to-end speech recognition with recurrent neural networks", "author": ["Graves", "Alex", "Jaitly", "Navdeep"], "venue": "In ICML,", "citeRegEx": "Graves et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Graves et al\\.", "year": 2014}, {"title": "Speech recognition with deep recurrent neural networks. In Acoustics, speech and signal processing", "author": ["Graves", "Alex", "Mohamed", "Abdel-rahman", "Hinton", "Geoffrey"], "venue": "(icassp), 2013 ieee international conference on,", "citeRegEx": "Graves et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Graves et al\\.", "year": 2013}, {"title": "Long shortterm memory", "author": ["Hochreiter", "Sepp", "Schmidhuber", "J\u00fcrgen"], "venue": "Neural computation,", "citeRegEx": "Hochreiter et al\\.,? \\Q1997\\E", "shortCiteRegEx": "Hochreiter et al\\.", "year": 1997}, {"title": "Gradient flow in recurrent nets: the difficulty of learning", "author": ["Hochreiter", "Sepp", "Bengio", "Yoshua", "Frasconi", "Paolo", "Schmidhuber", "J\u00fcrgen"], "venue": "long-term dependencies,", "citeRegEx": "Hochreiter et al\\.,? \\Q2001\\E", "shortCiteRegEx": "Hochreiter et al\\.", "year": 2001}, {"title": "Squeezenet: Alexnet-level accuracy with 50x fewer parameters and\u00a1 0.5 mb model size", "author": ["Iandola", "Forrest N", "Han", "Song", "Moskewicz", "Matthew W", "Ashraf", "Khalid", "Dally", "William J", "Keutzer", "Kurt"], "venue": "arXiv preprint arXiv:1602.07360,", "citeRegEx": "Iandola et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Iandola et al\\.", "year": 2016}, {"title": "What is the best multi-stage architecture for object recognition", "author": ["Jarrett", "Kevin", "Kavukcuoglu", "Koray", "LeCun", "Yann"], "venue": "In Computer Vision,", "citeRegEx": "Jarrett et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Jarrett et al\\.", "year": 2009}, {"title": "A simple way to initialize recurrent networks of rectified linear units", "author": ["Le", "Quoc V", "Jaitly", "Navdeep", "Hinton", "Geoffrey E"], "venue": "arXiv preprint arXiv:1504.00941,", "citeRegEx": "Le et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Le et al\\.", "year": 2015}, {"title": "Marc\u2019Aurelio. Learning longer memory in recurrent neural networks", "author": ["Mikolov", "Tomas", "Joulin", "Armand", "Chopra", "Sumit", "Mathieu", "Michael", "Ranzato"], "venue": "arXiv preprint arXiv:1412.7753,", "citeRegEx": "Mikolov et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Mikolov et al\\.", "year": 2015}, {"title": "Kernel mean embedding of distributions: A review and beyonds", "author": ["Muandet", "Krikamol", "Fukumizu", "Kenji", "Sriperumbudur", "Bharath", "Sch\u00f6lkopf", "Bernhard"], "venue": "arXiv preprint arXiv:1605.09522,", "citeRegEx": "Muandet et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Muandet et al\\.", "year": 2016}, {"title": "Information retrieval for music and motion, volume", "author": ["M\u00fcller", "Meinard"], "venue": null, "citeRegEx": "M\u00fcller and Meinard.,? \\Q2007\\E", "shortCiteRegEx": "M\u00fcller and Meinard.", "year": 2007}, {"title": "Rectified linear units improve restricted boltzmann machines", "author": ["Nair", "Vinod", "Hinton", "Geoffrey E"], "venue": "In Proceedings of the 27th international conference on machine learning", "citeRegEx": "Nair et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Nair et al\\.", "year": 2010}, {"title": "On the difficulty of training recurrent neural networks", "author": ["Pascanu", "Razvan", "Mikolov", "Tomas", "Bengio", "Yoshua"], "venue": "ICML (3),", "citeRegEx": "Pascanu et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Pascanu et al\\.", "year": 2013}, {"title": "Applied functional data analysis: methods and case studies, volume 77", "author": ["J.O. Ramsay", "B.W. Silverman"], "venue": null, "citeRegEx": "Ramsay and Silverman,? \\Q2002\\E", "shortCiteRegEx": "Ramsay and Silverman", "year": 2002}, {"title": "A hilbert space embedding for distributions", "author": ["Smola", "Alex", "Gretton", "Arthur", "Song", "Le", "Sch\u00f6lkopf", "Bernhard"], "venue": "In International Conference on Algorithmic Learning Theory,", "citeRegEx": "Smola et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Smola et al\\.", "year": 2007}, {"title": "Grammar as a foreign language", "author": ["Vinyals", "Oriol", "Kaiser", "\u0141ukasz", "Koo", "Terry", "Petrov", "Slav", "Sutskever", "Ilya", "Hinton", "Geoffrey"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Vinyals et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Vinyals et al\\.", "year": 2015}, {"title": "Recurrent neural network regularization", "author": ["Zaremba", "Wojciech", "Sutskever", "Ilya", "Vinyals", "Oriol"], "venue": "arXiv preprint arXiv:1409.2329,", "citeRegEx": "Zaremba et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Zaremba et al\\.", "year": 2014}], "referenceMentions": [{"referenceID": 22, "context": "Domain areas like natural language (Zaremba et al., 2014; Vinyals et al., 2015), speech (Graves et al.", "startOffset": 35, "endOffset": 79}, {"referenceID": 21, "context": "Domain areas like natural language (Zaremba et al., 2014; Vinyals et al., 2015), speech (Graves et al.", "startOffset": 35, "endOffset": 79}, {"referenceID": 8, "context": ", 2015), speech (Graves et al., 2013; Graves & Jaitly, 2014), music (Chung et al.", "startOffset": 16, "endOffset": 60}, {"referenceID": 4, "context": ", 2013; Graves & Jaitly, 2014), music (Chung et al., 2014), and video (Donahue et al.", "startOffset": 38, "endOffset": 58}, {"referenceID": 10, "context": "ents through time (Hochreiter et al., 2001).", "startOffset": 18, "endOffset": 43}, {"referenceID": 18, "context": "While exploding gradients can be mitigated with techniques like gradient clipping and normalization (Pascanu et al., 2013), vanishing gradients may be harder to deal with.", "startOffset": 100, "endOffset": 122}, {"referenceID": 3, "context": "As a result, sophisticated gated architectures like Long-Short Term Memory (LSTM) networks (Hochreiter & Schmidhuber, 1997) and Gated Recurrent Units (GRU) networks (Cho et al., 2014) have been developed.", "startOffset": 165, "endOffset": 183}, {"referenceID": 13, "context": "Notwithstanding, there are still challenges in capturing long term dependencies in gated architectures (Le et al., 2015).", "startOffset": 103, "endOffset": 120}, {"referenceID": 11, "context": "It is interesting to note that global average pooling operations have gained a lot of recent traction in convolutional networks (Lin et al., 2013; Iandola et al., 2016).", "startOffset": 128, "endOffset": 168}, {"referenceID": 12, "context": "wise non-linearity f (\u00b7), which we take to be the ReLU (Jarrett et al., 2009; Nair & Hinton, 2010): f (\u00b7) = max(\u00b7, 0).", "startOffset": 55, "endOffset": 98}, {"referenceID": 14, "context": "It is worth noting that exponential averages of inputs has been considered previously (Mikolov et al., 2015).", "startOffset": 86, "endOffset": 108}, {"referenceID": 13, "context": "The use of ReLUs in recurrent units has also been recently explored by Le et al. (2015), however there no statistics are kept and their use is limited to the simple RNN when initialized in a special manner.", "startOffset": 71, "endOffset": 88}, {"referenceID": 20, "context": "To better elucidate the design and its intuition, let us take a brief excursion to another use of (summary) statistics in machine learning for the representation of data: mean map embeddings (MMEs) of distributions (Smola et al., 2007).", "startOffset": 215, "endOffset": 235}, {"referenceID": 15, "context": "Numerous works have shown success in representing distributions and sets through MMEs (Muandet et al., 2016).", "startOffset": 86, "endOffset": 108}, {"referenceID": 13, "context": "ReLUs have been observed to be easier to train for general deep networks (Nair & Hinton, 2010) and have had success in recurrent units (Le et al., 2015).", "startOffset": 135, "endOffset": 152}, {"referenceID": 1, "context": "In order to perform a fair, unbiased comparison of the recurrent units and their hyper-parameters, which greatly affect performance (Bergstra & Bengio, 2012), we used the Hyperopt (Bergstra et al., 2015) Bayesian optimization package.", "startOffset": 180, "endOffset": 203}, {"referenceID": 18, "context": "In all experiments we used SGD for optimization using gradient clipping (Pascanu et al., 2013) with a norm of 1 on all algorithms.", "startOffset": 72, "endOffset": 94}, {"referenceID": 13, "context": "It has been observed that LSTMs perform poorly in classifying a long pixel-by-pixel sequence of MNIST digits (Le et al., 2015).", "startOffset": 109, "endOffset": 126}, {"referenceID": 13, "context": "Although the previous uses of ReLUs in RNN required careful initialization (Le et al., 2015), SRUs are able to use ReLUs for better learning without an special considerations.", "startOffset": 75, "endOffset": 92}, {"referenceID": 2, "context": "First, we used the polyphonic music datasets from Boulanger-Lewandowski et al. (2012). Each time-step is a binary vector representing the notes played at the respective time-step.", "startOffset": 50, "endOffset": 86}], "year": 2017, "abstractText": "Sophisticated gated recurrent neural network architectures like LSTMs and GRUs have been shown to be highly effective in a myriad of applications. We develop an un-gated unit, the statistical recurrent unit (SRU), that is able to learn long term dependencies in data by only keeping moving averages of statistics. The SRU\u2019s architecture is simple, un-gated, and contains a comparable number of parameters to LSTMs; yet, SRUs perform favorably to more sophisticated LSTM and GRU alternatives, often outperforming one or both in various tasks. We show the efficacy of SRUs as compared to LSTMs and GRUs in an unbiased manner by optimizing respective architectures\u2019 hyperparameters in a Bayesian optimization scheme for both synthetic and realworld tasks.", "creator": "LaTeX with hyperref package"}}}