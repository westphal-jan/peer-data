{"id": "1703.00426", "review": {"conference": "iclr", "VERSION": "v1", "DATE_OF_SUBMISSION": "1-Mar-2017", "title": "HolStep: A Machine Learning Dataset for Higher-order Logic Theorem Proving", "abstract": "Large computer-understandable proofs consist of millions of intermediate logical steps. The vast majority of such steps originate from manually selected and manually guided heuristics applied to intermediate goals. So far, machine learning has generally not been used to filter or generate these steps. In this paper, we introduce a new dataset based on Higher-Order Logic (HOL) proofs, for the purpose of developing new machine learning-based theorem-proving strategies. We make this dataset publicly available under the BSD license. We propose various machine learning tasks that can be performed on this dataset, and discuss their significance for theorem proving. We also benchmark a set of simple baseline machine learning models suited for the tasks (including logistic regression, convolutional neural networks and recurrent neural networks). The results of our baseline models show the promise of applying machine learning to HOL theorem proving.", "histories": [["v1", "Wed, 1 Mar 2017 18:20:19 GMT  (712kb,D)", "http://arxiv.org/abs/1703.00426v1", null]], "reviews": [], "SUBJECTS": "cs.AI", "authors": ["cezary kaliszyk", "fran\\c{c}ois chollet", "christian szegedy"], "accepted": true, "id": "1703.00426"}, "pdf": {"name": "1703.00426.pdf", "metadata": {"source": "CRF", "title": "HOLSTEP: A MACHINE LEARNING DATASET FOR HIGHER-ORDER LOGIC THEOREM PROVING", "authors": ["Cezary Kaliszyk", "Fran\u00e7ois Chollet", "Christian Szegedy"], "emails": ["cezary.kaliszyk@uibk.ac.at", "fchollet@google.com", "szegedy@google.com"], "sections": [{"heading": "1 INTRODUCTION", "text": "The fact is that we see ourselves as being able to be in a position, and that we are able, we will be able to be in a position, we will be in a position, we will be able to be in a position, we will be able to put ourselves in a position, we will be able to put ourselves in a position, we will be able to put ourselves in a position, we will be able to put ourselves in a position, we will be able to put ourselves in a position, we will be in a position, we will be in a position."}, {"heading": "1.1 CONTRIBUTION AND OVERVIEW", "text": "First, we are developing a machine learning dataset based on the evidence used in a large interactive proof section 2. We are focusing on the HOL Light (Harrison, 2009) ITP, its multivariate analysis library (Harrison, 2013), and the formal evidence for the Kepler conjecture (Hales et al., 2010), which represents a diversified dataset containing basic mathematics, analysis, trigonometry, and thinking about data structures such as graphics. Furthermore, these formal developments of evidence have been used as benchmarks for automated reasoning techniques (Kaliszyk & Urban, 2014). The dataset consists of 2,013,046 training examples and 196,030 test examples drawn from 11,400 proofs. Exactly half of the examples are statements used in the contexts and semi-steps currently being tested."}, {"heading": "1.2 RELATED WORK", "text": "The use of machine learning in interactive and automated theorem testing has so far focused on three tasks: presumption selection, strategy selection, and internal guidance, which we explain briefly. In light of a large library of proven facts and a conjecture given by the user, the multi-step classification problem of selecting the facts most likely to lead to successful proof of conjecture was commonly called relevance filtering or presumption selection (Alama et al., 2014), which is critical to the efficiency of modern automation techniques for ITPs (Blanchette et al., 2016), which today can typically solve 40-50% of conjectures in theorem-testing libraries. Similarly, most competitive ATPs today (Sutcliffe, 2016) use the SInE classifier (Hoder & Voronkov, 2011). A second theorem that proves the task in machine-learning strategy selection has been selective."}, {"heading": "2 DATASET EXTRACTION", "text": "This year, it is more than ever before in the history of the city."}, {"heading": "3 MACHINE LEARNING TASKS", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "3.1 TASKS DESCRIPTION", "text": "This data set allows for several tasks that are well suited for machine learning, most of which are highly relevant for theoretical proof: \u2022 Predicting whether a statement is useful to prove a given presumption; \u2022 Predicting the dependencies of a statement (presumption selection); \u2022 Predicting whether a statement is an important statement (human name); \u2022 Predicting what presumption a particular interim statement will be made from; \u2022 Predicting the name of a statement; \u2022 Predicting interim statements that are useful to prove a given presumption; \u2022 Generating the presumption to which the current evidence will lead. Below, we will focus on the first task: classifying evidence steps as useful or not in the context of a given evidence. This task can be further specialized in two different tasks: \u2022 Unconditional classification of evidence steps: Determination of how likely a given proof is to prove the evidence in which it occurred, based solely on the content of the statement itself (i.e., the statement itself)."}, {"heading": "3.2 RELEVANCE TO INTERACTIVE AND AUTOMATED THEOREM PROVING", "text": "Combined with an interactive theory tester, the tasks that require the most human time are: finding good intermediate steps; finding automation techniques that can justify the individual steps; and searching theory test libraries for the necessary simpler facts. These three problems correspond directly to the machine learning tasks proposed in the previous subsection; the ability to predict the usefulness of a statement will greatly improve many automation techniques; generating good intermediate lemmas or intermediate steps can improve the degree of granularity of the evidence steps; understanding the correspondence between statements and their names can allow the user to search the libraries for statements more efficiently (Aspinall & Kaliszyk, 2016)."}, {"heading": "4 BASELINE MODELS", "text": "For each task (conditional and conditional classification), we propose three distinct deep-learning architectures to serve as the basis for the classification performance that can be achieved on this dataset. Our models cover a range of architectural features (from Convolutionary Networks to Recursive Networks) to determine which characteristics of the data are most useful for usefulness classification.Our models are implemented in TensorFlow (Abadi et al., 2015) using the Keras framework (Chollet, 2015).Each model has been trained on a single Nvidia K80 GPU. Training lasts only a few hours per model, making conducting these experiments accessible to most people (they could even run on a laptop CPU).We publish our entire benchmark code as open source software 2 so that others can reproduce our results and improve our models."}, {"heading": "4.1 UNCONDITIONED CLASSIFICATION MODELS", "text": "Our three models for this task are: \u2022 Logistic regression over learned token embeddings. This minimal model aims to determine to what extent simple differences in token distribution between useful and non-useful statements can be used to distinguish them. \u2022 2-Layer 1D Convolutionary Neural Network (CNN) with global maxpooling for sequence reduction. This model aims to determine the meaning of local pattern of tokens. \u2022 2-Layer 1D CNN with LSTM (Hochreiter & Schmidhuber, 1997) Sequence reduction. This model aims to determine the meaning of order in feature sequences.See Figure 1 for a layer-by-layer description of these models."}, {"heading": "4.2 CONDITIONED CLASSIFICATION MODELS", "text": "For this task, we use versions of the models above, which have two Siamese branches (identical branches with split weights), with one branch processing the evidence step statement and the other branch processing the guess. Each branch outputs an embedding; these two embedding (step-by-step embedding and embedding conjecture) are then linked and classified by a fully connected network. See Figure 2 for a layer-by-layer description of these models."}, {"heading": "4.3 INPUT STATEMENTS ENCODING", "text": "It should be noted that all our models start with an embedding layer that maps symbols or characters in the instructions to dense vectors in a low-dimensional space. We consider two possible encodings for displaying the input instructions (evidence steps and conjectures) on the embedding layers of our models: \u2022 Character encoding of human-readable versions of the instructions at character level, mapping each character (out of a set of 86 unique characters) in the beautifully printed instructions to a 256-dimensional dense vector. This encoding results in longer instructions (training instructions are on average 308 characters long). \u2022 Tokencoding of the versions of the instructions rendered with our proposed high-grade tokenization scheme. This encoding leads to shorter instructions (training instructions are on average 60 characters long), while the size of the group of unique tokens increases considerably (1993 total tokens in the training set)."}, {"heading": "5 RESULTS", "text": "The experimental results are shown in Tables 2 and 3 as well as in Figures 3 to 6."}, {"heading": "5.1 INFLUENCE OF MODEL ARCHITECTURE", "text": "Our unconditioned logistic regression model delivers 71% accuracy in both character encoding and token encoding (Tables 2 and 3), showing that differences in token or character distribution between useful and non-useful steps alone, without any context, are sufficient to adequately distinguish between useful and non-useful statements; this also shows that token encoding is not fundamentally more informative than raw character statements at the character level. Furthermore, our unconditioned 1D CNN model delivers an accuracy of 82% to 83% in both character encoding and token encoding (Tables 2 and 3), indicating that patterns of characters or patterns of tokens for usefulness classification purposes are substantially more informative than individual tokens. Finally, our unconditional convolutional recursive model does not improve the results of 1D CNN, suggesting that our sequences of tokens are not able to use the sequences in the sequence that are meaningful in the order in which they are in use."}, {"heading": "5.2 INFLUENCE OF INPUT ENCODING", "text": "For the logistic regression model and the 2-layer 1D CNN model, the choice of input coding seems to have little impact. For the convolutional-relapsing model, the use of high-level tokenization appears to cause a large reduction in model performance (Figs. 4 and 6), which may be due to the fact that the token coding yields shorter sequences, making the use of an LSTM less relevant."}, {"heading": "5.3 INFLUENCE OF CONDITIONING ON THE CONJECTURE", "text": "None of our conditioned models seems to be able to improve on the conditioned models, suggesting that our architectures are unable to use the information provided by the conjecture. However, the presence of conditioning affects the training profile of our models, particularly by converging the 1D-CNN model faster and overfitting it significantly faster (Figs. 5 and 6)."}, {"heading": "6 CONCLUSIONS", "text": "These methods already help automated first-order examiners (Kaliszyk & Urban, 2015a), and since the branching factor in HOL is higher, predictions are valuable for a number of practical test applications, including making tableaux-based (Paulson, 1999) and superposition-based (Hurd, 2003) internal ITP proof searches significantly more efficient to simplify formalization. However, our models do not appear to be able to use the sequence in the input sequences or condition the guesses, due to the fact that these models do not perform a form of logical reasoning on their input statements, but perform simple pattern matching at the level of n-gram characters or tokens, demonstrating the need to focus future efforts on different models that perform reasoning or alternatively on systems that perform explicit reasoning (e.g. graph search) with deep learning at the level of characters or tokens."}, {"heading": "6.1 FUTURE WORK", "text": "The data set focuses on an interactive theorem tester. It would be interesting if the proposed techniques were to generalize, primarily across ITPs that use the same basic logic, e.g. through OpenTheory (Hurd, 2011), and secondly across radically different ITPs or even ATPs. A significant portion of the unused steps come from trying to fulfill the conditions for rewriting, and from calls to intuitionist tableaus. However, the emphasis is on the human evidence found, so that the trained predictions could to some extent mimic the bias regarding the usefulness of human evidence. Since ATPs are very week at the moment compared to human intuition, improving them would be an important gain even for the many evidences that people do not find difficult. Finally, two of the proposed tasks for the data set are premises selection and interim statute generation. It would be interesting to define more ATP-based methods to evaluate the premises selected (however, if the calculations are too large), as well as the calculations are relatively large."}, {"heading": "ACKNOWLEDGEMENTS", "text": "The first author was partially supported by ERC Start-up Funding 714034."}], "references": [{"title": "TensorFlow: Large-scale machine learning on heterogeneous systems", "author": ["cent Vanhoucke", "Vijay Vasudevan", "Fernanda Vi\u00e9gas", "Oriol Vinyals", "Pete Warden", "Martin Wattenberg", "Martin Wicke", "Yuan Yu", "Xiaoqiang Zheng"], "venue": null, "citeRegEx": "Vanhoucke et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Vanhoucke et al\\.", "year": 2015}, {"title": "Premise selection for mathematics by corpus analysis and kernel methods", "author": ["Jesse Alama", "Tom Heskes", "Daniel K\u00fchlwein", "Evgeni Tsivtsivadze", "Josef Urban"], "venue": "J. Autom. Reasoning,", "citeRegEx": "Alama et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Alama et al\\.", "year": 2014}, {"title": "DeepMath \u2013 Deep sequence models for premise selection", "author": ["Alex A. Alemi", "Fran\u00e7ois Chollet", "Geoffrey Irving", "Christian Szegedy", "Josef Urban"], "venue": "Advances in Neural Information Processing Systems (NIPS 2016),", "citeRegEx": "Alemi et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Alemi et al\\.", "year": 2016}, {"title": "What\u2019s in a theorem name", "author": ["David Aspinall", "Cezary Kaliszyk"], "venue": "Interactive Theorem Proving (ITP 2016),", "citeRegEx": "Aspinall and Kaliszyk.,? \\Q2016\\E", "shortCiteRegEx": "Aspinall and Kaliszyk.", "year": 2016}, {"title": "Term rewriting and all that", "author": ["Franz Baader", "Tobias Nipkow"], "venue": null, "citeRegEx": "Baader and Nipkow.,? \\Q1998\\E", "shortCiteRegEx": "Baader and Nipkow.", "year": 1998}, {"title": "Hammering towards QED", "author": ["Jasmin C. Blanchette", "Cezary Kaliszyk", "Lawrence C. Paulson", "Josef Urban"], "venue": "J. Formalized Reasoning,", "citeRegEx": "Blanchette et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Blanchette et al\\.", "year": 2016}, {"title": "Mining the Archive of Formal Proofs", "author": ["Jasmin Christian Blanchette", "Maximilian P.L. Haslbeck", "Daniel Matichuk", "Tobias Nipkow"], "venue": "Intelligent Computer Mathematics (CICM 2015),", "citeRegEx": "Blanchette et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Blanchette et al\\.", "year": 2015}, {"title": "Machine learning for first-order theorem proving - learning to select a good heuristic", "author": ["James P. Bridge", "Sean B. Holden", "Lawrence C. Paulson"], "venue": "J. Autom. Reasoning,", "citeRegEx": "Bridge et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Bridge et al\\.", "year": 2014}, {"title": "Using crash Hoare logic for certifying the FSCQ file system", "author": ["Haogang Chen", "Daniel Ziegler", "Tej Chajed", "Adam Chlipala", "M. Frans Kaashoek", "Nickolai Zeldovich"], "venue": "USENIX Association,", "citeRegEx": "Chen et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Chen et al\\.", "year": 2016}, {"title": "A formulation of the simple theory of types", "author": ["Alonzo Church"], "venue": "J. Symb. Log.,", "citeRegEx": "Church.,? \\Q1940\\E", "shortCiteRegEx": "Church.", "year": 1940}, {"title": "ImageNet: A Large-Scale Hierarchical Image Database", "author": ["J. Deng", "W. Dong", "R. Socher", "L.-J. Li", "K. Li", "L. Fei-Fei"], "venue": "In CVPR09,", "citeRegEx": "Deng et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Deng et al\\.", "year": 2009}, {"title": "The Use of Data-Mining for the Automatic Formation of Tactics", "author": ["Hazel Duncan"], "venue": "PhD thesis, University of Edinburgh,", "citeRegEx": "Duncan.,? \\Q2007\\E", "shortCiteRegEx": "Duncan.", "year": 2007}, {"title": "Internal guidance for Satallax", "author": ["Michael F\u00e4rber", "Chad E. Brown"], "venue": "International Joint Conference on Automated Reasoning (IJCAR 2016),", "citeRegEx": "F\u00e4rber and Brown.,? \\Q2016\\E", "shortCiteRegEx": "F\u00e4rber and Brown.", "year": 2016}, {"title": "A machine-checked proof of the odd order theorem", "author": ["Georges Gonthier", "Andrea Asperti", "Jeremy Avigad", "Yves Bertot", "Cyril Cohen", "Fran\u00e7ois Garillot", "St\u00e9phane Le Roux", "Assia Mahboubi", "Russell O\u2019Connor", "Sidi Ould Biha", "Ioana Pasca", "Laurence Rideau", "Alexey Solovyev", "Enrico Tassi", "Laurent Th\u00e9ry"], "venue": "Interactive Theorem Proving (ITP 2013),", "citeRegEx": "Gonthier et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Gonthier et al\\.", "year": 2013}, {"title": "Edinburgh LCF, volume", "author": ["Michael J.C. Gordon", "Robin Milner", "Christopher P. Wadsworth"], "venue": "Lecture Notes in Computer Science. Springer,", "citeRegEx": "Gordon et al\\.,? \\Q1979\\E", "shortCiteRegEx": "Gordon et al\\.", "year": 1979}, {"title": "Mizar in a nutshell", "author": ["Adam Grabowski", "Artur Kornilowicz", "Adam Naumowicz"], "venue": "J. Formalized Reasoning,", "citeRegEx": "Grabowski et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Grabowski et al\\.", "year": 2010}, {"title": "A revision of the proof of the Kepler Conjecture", "author": ["Thomas Hales", "John Harrison", "Sean McLaughlin", "Tobias Nipkow", "Steven Obua", "Roland Zumkeller"], "venue": "Discrete & Computational Geometry,", "citeRegEx": "Hales et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Hales et al\\.", "year": 2010}, {"title": "A formal proof of the Kepler conjecture", "author": ["Thomas C. Hales", "Mark Adams", "Gertrud Bauer", "Dat Tat Dang", "John Harrison", "Truong Le Hoang", "Cezary Kaliszyk", "Victor Magron", "Sean McLaughlin", "Thang Tat Nguyen", "Truong Quang Nguyen", "Tobias Nipkow", "Steven Obua", "Joseph Pleso", "Jason Rute", "Alexey Solovyev", "An Hoai Thi Ta", "Trung Nam Tran", "Diep Thi Trieu", "Josef Urban", "Ky Khac Vu", "Roland Zumkeller"], "venue": "CoRR, abs/1501.02155,", "citeRegEx": "Hales et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Hales et al\\.", "year": 2015}, {"title": "HOL Light: An overview", "author": ["John Harrison"], "venue": "Theorem Proving in Higher Order Logics (TPHOLs 2009),", "citeRegEx": "Harrison.,? \\Q2009\\E", "shortCiteRegEx": "Harrison.", "year": 2009}, {"title": "The HOL Light theory of Euclidean space", "author": ["John Harrison"], "venue": "J. Autom. Reasoning,", "citeRegEx": "Harrison.,? \\Q2013\\E", "shortCiteRegEx": "Harrison.", "year": 2013}, {"title": "History of interactive theorem proving", "author": ["John Harrison", "Josef Urban", "Freek Wiedijk"], "venue": "Handbook of the History of Logic vol. 9 (Computational Logic),", "citeRegEx": "Harrison et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Harrison et al\\.", "year": 2014}, {"title": "The principal type-scheme of an object in combinatory logic", "author": ["R. Hindley"], "venue": "Transactions of the american mathematical society,", "citeRegEx": "Hindley.,? \\Q1969\\E", "shortCiteRegEx": "Hindley.", "year": 1969}, {"title": "Long short-term memory", "author": ["Sepp Hochreiter", "J\u00fcrgen Schmidhuber"], "venue": "Neural computation,", "citeRegEx": "Hochreiter and Schmidhuber.,? \\Q1997\\E", "shortCiteRegEx": "Hochreiter and Schmidhuber.", "year": 1997}, {"title": "Sine qua non for large theory reasoning. In Nikolaj Bj\u00f8rner and Viorica Sofronie-Stokkermans (eds.), CADE-23, volume 6803", "author": ["Kry\u0161tof Hoder", "Andrei Voronkov"], "venue": null, "citeRegEx": "Hoder and Voronkov.,? \\Q2011\\E", "shortCiteRegEx": "Hoder and Voronkov.", "year": 2011}, {"title": "First-order proof tactics in higher-order logic theorem provers", "author": ["Joe Hurd"], "venue": "Technical Reports,", "citeRegEx": "Hurd.,? \\Q2003\\E", "shortCiteRegEx": "Hurd.", "year": 2003}, {"title": "The OpenTheory standard theory library", "author": ["Joe Hurd"], "venue": "NASA Formal Methods (NFM 2011),", "citeRegEx": "Hurd.,? \\Q2011\\E", "shortCiteRegEx": "Hurd.", "year": 2011}, {"title": "Scalable LCF-style proof translation", "author": ["Cezary Kaliszyk", "Alexander Krauss"], "venue": "Interactive Theorem Proving (ITP 2013),", "citeRegEx": "Kaliszyk and Krauss.,? \\Q2013\\E", "shortCiteRegEx": "Kaliszyk and Krauss.", "year": 2013}, {"title": "Learning-assisted automated reasoning with Flyspeck", "author": ["Cezary Kaliszyk", "Josef Urban"], "venue": "J. Autom. Reasoning,", "citeRegEx": "Kaliszyk and Urban.,? \\Q2014\\E", "shortCiteRegEx": "Kaliszyk and Urban.", "year": 2014}, {"title": "FEMaLeCoP: Fairly efficient machine learning connection prover", "author": ["Cezary Kaliszyk", "Josef Urban"], "venue": "20th International Conference on Logic for Programming, Artificial Intelligence, and Reasoning (LPAR 2015),", "citeRegEx": "Kaliszyk and Urban.,? \\Q2015\\E", "shortCiteRegEx": "Kaliszyk and Urban.", "year": 2015}, {"title": "Learning-assisted theorem proving with millions of lemmas", "author": ["Cezary Kaliszyk", "Josef Urban"], "venue": "J. Symbolic Computation,", "citeRegEx": "Kaliszyk and Urban.,? \\Q2015\\E", "shortCiteRegEx": "Kaliszyk and Urban.", "year": 2015}, {"title": "Learning to parse on aligned corpora", "author": ["Cezary Kaliszyk", "Josef Urban", "Ji\u0159\u00ed Vysko\u010dil"], "venue": "Proc. 6h Conference on Interactive Theorem Proving (ITP\u201915),", "citeRegEx": "Kaliszyk et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Kaliszyk et al\\.", "year": 2015}, {"title": "Comprehensive formal verification of an OS microkernel", "author": ["Gerwin Klein", "June Andronick", "Kevin Elphinstone", "Toby C. Murray", "Thomas Sewell", "Rafal Kolanski", "Gernot Heiser"], "venue": "ACM Trans. Comput. Syst.,", "citeRegEx": "Klein et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Klein et al\\.", "year": 2014}, {"title": "First-order theorem proving and Vampire", "author": ["Laura Kov\u00e1cs", "Andrei Voronkov"], "venue": "Computer-Aided Verification (CAV 2013),", "citeRegEx": "Kov\u00e1cs and Voronkov.,? \\Q2013\\E", "shortCiteRegEx": "Kov\u00e1cs and Voronkov.", "year": 2013}, {"title": "MaLeS: A framework for automatic tuning of automated theorem provers", "author": ["Daniel K\u00fchlwein", "Josef Urban"], "venue": "J. Autom. Reasoning,", "citeRegEx": "K\u00fchlwein and Urban.,? \\Q2015\\E", "shortCiteRegEx": "K\u00fchlwein and Urban.", "year": 2015}, {"title": "Formal verification of a realistic compiler", "author": ["Xavier Leroy"], "venue": "Commun. ACM,", "citeRegEx": "Leroy.,? \\Q2009\\E", "shortCiteRegEx": "Leroy.", "year": 2009}, {"title": "Importing HOL into Isabelle/HOL", "author": ["Steven Obua", "Sebastian Skalberg"], "venue": "International Joint Conference on Automated Reasoning (IJCAR 2006),", "citeRegEx": "Obua and Skalberg.,? \\Q2006\\E", "shortCiteRegEx": "Obua and Skalberg.", "year": 2006}, {"title": "A generic tableau prover and its integration with Isabelle", "author": ["Lawrence C. Paulson"], "venue": "J. Universal Computer Science,", "citeRegEx": "Paulson.,? \\Q1999\\E", "shortCiteRegEx": "Paulson.", "year": 1999}, {"title": "Mastering the game of go with deep neural networks and tree", "author": ["David Silver", "Aja Huang", "Christopher J. Maddison", "Arthur Guez", "Laurent Sifre", "George van den Driessche", "Julian Schrittwieser", "Ioannis Antonoglou", "Veda Panneershelvam", "Marc Lanctot", "Sander Dieleman", "Dominik Grewe", "John Nham", "Nal Kalchbrenner", "Ilya Sutskever", "Timothy Lillicrap", "Madeleine Leach", "Koray Kavukcuoglu", "Thore Graepel", "Demis Hassabis"], "venue": null, "citeRegEx": "Silver et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Silver et al\\.", "year": 2016}, {"title": "Reasoning with neural tensor networks for knowledge base completion", "author": ["Richard Socher", "Danqi Chen", "Christopher D. Manning", "Andrew Y. Ng"], "venue": "In Advances in Neural Information Processing Systems", "citeRegEx": "Socher et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Socher et al\\.", "year": 2013}, {"title": "Recursive deep models for semantic compositionality over a sentiment treebank", "author": ["Richard Socher", "Alex Perelygin", "Jean Wu", "Jason Chuang", "Christopher D. Manning", "Andrew Y. Ng", "Christopher Potts"], "venue": "In Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing,", "citeRegEx": "Socher et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Socher et al\\.", "year": 2013}, {"title": "End-to-end memory networks", "author": ["Sainbayar Sukhbaatar", "Jason Weston", "Rob Fergus"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Sukhbaatar et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Sukhbaatar et al\\.", "year": 2015}, {"title": "The CADE ATP system competition - CASC", "author": ["Geoff Sutcliffe"], "venue": "AI Magazine,", "citeRegEx": "Sutcliffe.,? \\Q2016\\E", "shortCiteRegEx": "Sutcliffe.", "year": 2016}, {"title": "MaLeCoP: Machine learning connection prover", "author": ["Josef Urban", "Ji\u0159\u00ed Vysko\u010dil", "Petr \u0160t\u011bp\u00e1nek"], "venue": "TABLEAUX 2011,", "citeRegEx": "Urban et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Urban et al\\.", "year": 2011}], "referenceMentions": [{"referenceID": 20, "context": "As the usability of interactive theorem proving (ITP) systems (Harrison et al., 2014) grows, its use becomes a more common way of establishing the correctness of software as well as mathematical proofs.", "startOffset": 62, "endOffset": 85}, {"referenceID": 34, "context": "Today, ITPs are used for software certification projects ranging from compilers (Leroy, 2009) and operating system components (Chen et al.", "startOffset": 80, "endOffset": 93}, {"referenceID": 8, "context": "Today, ITPs are used for software certification projects ranging from compilers (Leroy, 2009) and operating system components (Chen et al., 2016; Klein et al., 2014), to establishing the absolute correctness of large proofs in mathematics such as the Kepler conjecture (Hales et al.", "startOffset": 126, "endOffset": 165}, {"referenceID": 31, "context": "Today, ITPs are used for software certification projects ranging from compilers (Leroy, 2009) and operating system components (Chen et al., 2016; Klein et al., 2014), to establishing the absolute correctness of large proofs in mathematics such as the Kepler conjecture (Hales et al.", "startOffset": 126, "endOffset": 165}, {"referenceID": 17, "context": ", 2014), to establishing the absolute correctness of large proofs in mathematics such as the Kepler conjecture (Hales et al., 2015) and the Feit-Thomson Theorem (Gonthier et al.", "startOffset": 111, "endOffset": 131}, {"referenceID": 13, "context": ", 2015) and the Feit-Thomson Theorem (Gonthier et al., 2013).", "startOffset": 37, "endOffset": 60}, {"referenceID": 15, "context": "This means that the size of many ITP libraries can be measured in dozens of thousands of theorems (Grabowski et al., 2010; Blanchette et al., 2015) and billions of individual proof steps.", "startOffset": 98, "endOffset": 147}, {"referenceID": 6, "context": "This means that the size of many ITP libraries can be measured in dozens of thousands of theorems (Grabowski et al., 2010; Blanchette et al., 2015) and billions of individual proof steps.", "startOffset": 98, "endOffset": 147}, {"referenceID": 40, "context": "At the same time, fast progress has been unfolding in machine learning applied to tasks that involve logical inference, such as natural language question answering (Sukhbaatar et al., 2015), knowledge base completion (Socher et al.", "startOffset": 164, "endOffset": 189}, {"referenceID": 2, "context": ", 2016), and premise selection in the context of theorem proving (Alemi et al., 2016).", "startOffset": 65, "endOffset": 85}, {"referenceID": 37, "context": "Remarkably, it has recently become possible to build a system, AlphaGo (Silver et al., 2016), blending classical AI techniques such as Monte-Carlo tree search and modern deep learning techniques, capable of playing the game of Go at super-human levels.", "startOffset": 71, "endOffset": 92}, {"referenceID": 10, "context": "the ImageNet dataset for large-scale image classification (Deng et al., 2009)) serving as an experimental testbed and public benchmark of current progress, thus focusing the efforts of the research community.", "startOffset": 58, "endOffset": 77}, {"referenceID": 18, "context": "We focus on the HOL Light (Harrison, 2009) ITP, its multivariate analysis library (Harrison, 2013), as well as the formal proof of the Kepler conjecture (Hales et al.", "startOffset": 26, "endOffset": 42}, {"referenceID": 19, "context": "We focus on the HOL Light (Harrison, 2009) ITP, its multivariate analysis library (Harrison, 2013), as well as the formal proof of the Kepler conjecture (Hales et al.", "startOffset": 82, "endOffset": 98}, {"referenceID": 16, "context": "We focus on the HOL Light (Harrison, 2009) ITP, its multivariate analysis library (Harrison, 2013), as well as the formal proof of the Kepler conjecture (Hales et al., 2010).", "startOffset": 153, "endOffset": 173}, {"referenceID": 1, "context": "Given a large library of proven facts and a user given conjecture, the multi-label classification problem of selecting the facts that are most likely to lead to a successful proof of the conjecture has been usually called relevance filtering or premise selection (Alama et al., 2014).", "startOffset": 263, "endOffset": 283}, {"referenceID": 5, "context": "This is crucial for the efficiency of modern automation techniques for ITPs (Blanchette et al., 2016), which today can usually solve 40\u201350% of the conjectures in theorem proving libraries.", "startOffset": 76, "endOffset": 101}, {"referenceID": 41, "context": "Similarly most competitive ATPs today (Sutcliffe, 2016) implement the SInE classifier (Hoder & Voronkov, 2011).", "startOffset": 38, "endOffset": 55}, {"referenceID": 7, "context": "For this some frameworks use machine learning (Bridge et al., 2014; K\u00fchlwein & Urban, 2015).", "startOffset": 46, "endOffset": 91}, {"referenceID": 42, "context": "It has been shown to significantly reduce the proof search in first-order tableaux by the selection of extension steps to use (Urban et al., 2011), and has been also successfully applied in monomorphic higher-order logic proving (F\u00e4rber & Brown, 2016).", "startOffset": 126, "endOffset": 146}, {"referenceID": 11, "context": "Data/proof mining has also been applied on the level of interactive theorem proving tactics (Duncan, 2007) to extract and reuse repeating patterns.", "startOffset": 92, "endOffset": 106}, {"referenceID": 9, "context": "Second, HOL Light implements higher-order logic (Church, 1940) as its foundation, which on the one hand is powerful enough to encode most of today\u2019s formal proofs, and on the other hand allows for an easy integration of many powerful automation mechanisms (Baader & Nipkow, 1998; Paulson, 1999).", "startOffset": 48, "endOffset": 62}, {"referenceID": 36, "context": "Second, HOL Light implements higher-order logic (Church, 1940) as its foundation, which on the one hand is powerful enough to encode most of today\u2019s formal proofs, and on the other hand allows for an easy integration of many powerful automation mechanisms (Baader & Nipkow, 1998; Paulson, 1999).", "startOffset": 256, "endOffset": 294}, {"referenceID": 9, "context": "Second, HOL Light implements higher-order logic (Church, 1940) as its foundation, which on the one hand is powerful enough to encode most of today\u2019s formal proofs, and on the other hand allows for an easy integration of many powerful automation mechanisms (Baader & Nipkow, 1998; Paulson, 1999). When selecting the theorems to record, we choose an intermediate approach between HOL Light ProofRecording (Obua & Skalberg, 2006) and the HOL/Import one (Kaliszyk & Krauss, 2013). The theorems that are derived by most common proof functions are extracted by patching these functions like in the former approach, and the remaining theorems are extracted from the underlying OCaml programming language interpreter. In certain cases decision procedures derive theorems to be reused in subsequent invocations. We detect such values by looking at theorems used across proof blocks and avoid extracting such reused unrelated subproofs. All kernel-level inferences are recorded together with their respective arguments in a trace file. The trace is processed offline to extract the dependencies of the facts, detect used proof boundaries, mark the used and unused steps, and mark the training and testing examples. Only proofs that have sufficiently many used and unused steps are considered useful for the dataset. The annotated proof trace is processed again by a HOL kernel saving the actual training and testing examples originating from non-trivial reasoning steps. Training and testing examples are grouped by proof: for each proof the conjecture (statement that is finally proved), the dependencies of the theorem are constant, and a list of used and not used intermediate statements is provided. This means that the conjectures used in the training and testing sets are normally disjoint. For each statement, whether it is the conjecture, a proof dependency, or an intermediate statement, both a fully parenthesised HOL Light human-like printout is provided, as well as a predefined tokenization. The standard HOL Light printer uses parentheses and operator priorities to make its notations somewhat similar to textbook-style mathematics, while at the same time preserving the complete unambiguity of the order of applications (this is particularly visible for associative operators). The tokenization that we propose attempts to reduce the number of parentheses. To do this we compute the maximum number of arguments that each symbol needs to be applied to, and only mark partial application. This means that fully applied functions (more than 90% of the applications) do not require neither application operators nor parentheses. Top-level universal quantifications are eliminated, bound variables are represented by their de Bruijn indices (the distance from the corresponding abstraction in the parse tree of the term) and free variables are renamed canonically. Since the Hindley-Milner type inference Hindley (1969) mechanisms will be sufficient to reconstruct the most-general types of the expressions well enough for automated-reasoning techniques Kaliszyk et al.", "startOffset": 49, "endOffset": 2920}, {"referenceID": 9, "context": "Second, HOL Light implements higher-order logic (Church, 1940) as its foundation, which on the one hand is powerful enough to encode most of today\u2019s formal proofs, and on the other hand allows for an easy integration of many powerful automation mechanisms (Baader & Nipkow, 1998; Paulson, 1999). When selecting the theorems to record, we choose an intermediate approach between HOL Light ProofRecording (Obua & Skalberg, 2006) and the HOL/Import one (Kaliszyk & Krauss, 2013). The theorems that are derived by most common proof functions are extracted by patching these functions like in the former approach, and the remaining theorems are extracted from the underlying OCaml programming language interpreter. In certain cases decision procedures derive theorems to be reused in subsequent invocations. We detect such values by looking at theorems used across proof blocks and avoid extracting such reused unrelated subproofs. All kernel-level inferences are recorded together with their respective arguments in a trace file. The trace is processed offline to extract the dependencies of the facts, detect used proof boundaries, mark the used and unused steps, and mark the training and testing examples. Only proofs that have sufficiently many used and unused steps are considered useful for the dataset. The annotated proof trace is processed again by a HOL kernel saving the actual training and testing examples originating from non-trivial reasoning steps. Training and testing examples are grouped by proof: for each proof the conjecture (statement that is finally proved), the dependencies of the theorem are constant, and a list of used and not used intermediate statements is provided. This means that the conjectures used in the training and testing sets are normally disjoint. For each statement, whether it is the conjecture, a proof dependency, or an intermediate statement, both a fully parenthesised HOL Light human-like printout is provided, as well as a predefined tokenization. The standard HOL Light printer uses parentheses and operator priorities to make its notations somewhat similar to textbook-style mathematics, while at the same time preserving the complete unambiguity of the order of applications (this is particularly visible for associative operators). The tokenization that we propose attempts to reduce the number of parentheses. To do this we compute the maximum number of arguments that each symbol needs to be applied to, and only mark partial application. This means that fully applied functions (more than 90% of the applications) do not require neither application operators nor parentheses. Top-level universal quantifications are eliminated, bound variables are represented by their de Bruijn indices (the distance from the corresponding abstraction in the parse tree of the term) and free variables are renamed canonically. Since the Hindley-Milner type inference Hindley (1969) mechanisms will be sufficient to reconstruct the most-general types of the expressions well enough for automated-reasoning techniques Kaliszyk et al. (2015) we erase all type information.", "startOffset": 49, "endOffset": 3077}, {"referenceID": 14, "context": "For more details see (Gordon et al., 1979).", "startOffset": 21, "endOffset": 42}, {"referenceID": 36, "context": "This includes making tableaux-based (Paulson, 1999) and superposition-based (Hurd, 2003) internal ITP proof search significantly more efficient in turn", "startOffset": 36, "endOffset": 51}, {"referenceID": 24, "context": "This includes making tableaux-based (Paulson, 1999) and superposition-based (Hurd, 2003) internal ITP proof search significantly more efficient in turn", "startOffset": 76, "endOffset": 88}, {"referenceID": 25, "context": "It would be interesting if the proposed techniques generalize, primarily across ITPs that use the same foundational logic, for example using OpenTheory (Hurd, 2011), and secondarily across fundamentally different ITPs or even ATPs.", "startOffset": 152, "endOffset": 164}, {"referenceID": 30, "context": "It would be interesting to define more ATP-based ways to evaluate the selected premises, as well as to evaluate generated sentences (Kaliszyk et al., 2015).", "startOffset": 132, "endOffset": 155}], "year": 2017, "abstractText": "Large computer-understandable proofs consist of millions of intermediate logical steps. The vast majority of such steps originate from manually selected and manually guided heuristics applied to intermediate goals. So far, machine learning has generally not been used to filter or generate these steps. In this paper, we introduce a new dataset based on Higher-Order Logic (HOL) proofs, for the purpose of developing new machine learning-based theorem-proving strategies. We make this dataset publicly available under the BSD license. We propose various machine learning tasks that can be performed on this dataset, and discuss their significance for theorem proving. We also benchmark a set of simple baseline machine learning models suited for the tasks (including logistic regression, convolutional neural networks and recurrent neural networks). The results of our baseline models show the promise of applying machine learning to HOL theorem proving.", "creator": "LaTeX with hyperref package"}}}