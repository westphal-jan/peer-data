{"id": "1703.00443", "review": {"conference": "ICML", "VERSION": "v1", "DATE_OF_SUBMISSION": "1-Mar-2017", "title": "OptNet: Differentiable Optimization as a Layer in Neural Networks", "abstract": "This paper presents OptNet, a network architecture that integrates optimization problems (here, specifically in the form of quadratic programs) as individual layers in larger end-to-end trainable deep networks. These layers allow complex dependencies between the hidden states to be captured that traditional convolutional and fully-connected layers are not able to capture. In this paper, we develop the foundations for such an architecture: we derive the equations to perform exact differentiation through these layers and with respect to layer parameters; we develop a highly efficient solver for these layers that exploits fast GPU-based batch solves within a primal-dual interior point method, and which provides backpropagation gradients with virtually no additional cost on top of the solve; and we highlight the application of these approaches in several problems. In one particularly standout example, we show that the method is capable of learning to play Sudoku given just input and output games, with no a priori information about the rules of the game; this task is virtually impossible for other neural network architectures that we have experimented with, and highlights the representation capabilities of our approach.", "histories": [["v1", "Wed, 1 Mar 2017 18:58:48 GMT  (445kb,D)", "http://arxiv.org/abs/1703.00443v1", "Submitted to ICML 2017"], ["v2", "Wed, 14 Jun 2017 17:59:07 GMT  (966kb,D)", "http://arxiv.org/abs/1703.00443v2", "ICML 2017"]], "COMMENTS": "Submitted to ICML 2017", "reviews": [], "SUBJECTS": "cs.LG cs.AI math.OC stat.ML", "authors": ["brandon amos", "j zico kolter"], "accepted": true, "id": "1703.00443"}, "pdf": {"name": "1703.00443.pdf", "metadata": {"source": "META", "title": "OptNet: Differentiable Optimization as a Layer in Neural Networks", "authors": ["Brandon Amos", "J. Zico Kolter"], "emails": ["<bamos@cs.cmu.edu>,", "<zkolter@cs.cmu.edu>."], "sections": [{"heading": "1. Introduction", "text": "In fact, most of them are able to survive on their own."}, {"heading": "2. Background and related work", "text": "In fact, most of them are able to survive on their own, without being able to survive on their own."}, {"heading": "3. OptNet: solving optimization within a neural network", "text": "In its most general form, an OptNet layer is an optimization problem of formzi + 1 = argmin z f\u03b8 (z, zi) which is subject to g\u03c6 (z, zi) \u2264 0 h\u0445 (z, zi) = 0 (2), where zi and zi + 1 are the previous and current layer, z is the optimization variable, and \u03b8, \u03c6 and \u0442 are parameters. Although most of the techniques presented here can easily be extended to the case of a convex optimization problem, the rest of this post will focus on the special case mentioned in the introduction where (2) is a convex square program. General convex optimization problem can be solved, for example, by consistent quadratic programming which makes a relatively small number of calls to our QP optimizer within an internal loop via the larger optimization procedure."}, {"heading": "3.1. QP layers and backpropagation", "text": "In order to formally define these optimization problems, we will slightly simplify the previous notation and simply take into account the problem of the equality of all data with respect to the underlying network environments with respect to the underlying network environments. (As is known, these problems can be solved in polynomial time using a variety of methods, if an exact (numerical precision oriented) solution to these problems is desired, then the primary-dual interpretation methods, as we will apply them in a later section, will represent the current state of the solution methods.In the neural network setup, the optimal solution (or general subset of the optimal solution) of these optimization problems, we will depend on the results of our layer, denzi + 1, and the problems of the underlying system architecture, which we depend on the underlying network setup, which we consider the optimal solution to the underlying set of the optimization problems (the optimal subset of these problems)."}, {"heading": "3.2. An efficient batched QP solver", "text": "Without mini-batching on the GPU, many modern deep-learning architectures are insoluble for all practical purposes. However, today's state-of-the-art QP solvers such as Gurobi and CPLEX are unable to solve multiple optimization problems on the GPU in parallel across the entire minibatch, making larger OptNet layers quickly insoluble compared to a fully connected layer with the same number of parameters. To overcome this performance bottleneck in our square program layers, we have implemented a GPU-based Primal Dual Interior Point Method (PDIPM) based on (Mattingley & Boyd, 2012), which solves a batch of square programs and provides the gradients needed to train them in an end-to-end mode."}, {"heading": "3.2.1. EFFICIENTLY COMPUTING GRADIENTS", "text": "A central point of the particular form of the primary-dual method of the inner point that we use is that it is possible to calculate the backward gradients \"free of charge\" after solving the original QP, without any additional matrix factorization or solution. Specifically, in each iteration of the primary-dual inner point, we perform an LU decomposition of the matrix by eliminating variables to produce only one p-p matrix (the number of inequality constraints) that must be taken into account during each iteration of the primary-dual algorithm, and an m-dual decomposition of a certain subset of the matrix formed by eliminating variables that form only a p-p matrix matrix matrix matrix (the number of inequality constraints) that must be taken into account during each iteration of the primary-dual algorithm, and an m-dual algorithm and an n-times matrix we leave the beginning of the D-detail here."}, {"heading": "3.3. Limitation of the method", "text": "While we will shortly show that the OptNet layer has several strengths, we would also like to highlight the potential drawbacks of this approach. First, while integrating an OptNet layer into existing deep learning architectures is potentially practicable, we note that solving optimization problems, just like here, has a cubic complexity in the number of variables and / or limitations, in contrast to the quadratic complexity of standard feedback layers. This means that we are ultimately limited to settings where the number of hidden variables in an OptNet layer is not too large (fewer than 1,000 dimensions seem to be the limits of what we currently consider practicable, and considerably less if you want real-time results for an architecture). Second, there are many improvements to the OptNet layers that are still possible."}, {"heading": "4. Experimental results", "text": "In this section, we present several experimental results that highlight the capabilities of the OptNet layer. Specifically, we look at 1) computing power versus leaking solvers; 2) the ability to improve existing convex problems used in signal denosis; 3) the integration of the architecture into a generic deep-learning architecture; and 3) the performance of our approach to a problem that is very challenging for current approaches. Sometimes, the performance of our approach can be significantly improved over existing deep-architects architectures. In particular, we would like to highlight the results of our system in learning the game (4x4) Sudoku, a well-known logical puzzle; to the best of our knowledge, our in the first type of end-to-end differentiation architecture that can learn problems like this, based solely on examples without an-priori knowledge of the rules of Sudoku. The code and data for our experiments are open source at https: / giubtloctop.com / usnet."}, {"heading": "4.1. Batch QP solver performance", "text": "Our first experiment illustrates why standard QP solvers like CPLEX and Gurobi are accountable for OptNet layers without batch support. We run our solver on an unloaded Titan X GPU and Gurobi on an unloaded Intel Core i7-5960X CPU @ 3.00GHz. We set up the same random QP of the form (1) across all three frameworks and vary the number of variables, constraints and batch sizes. 3Figure 1 shows the means and standard deviations that are executed 10 times in each attempt, and shows that our solver, itself a highly tuned solver, outperforms Gurobi in all batch instances."}, {"heading": "4.2. Total variation denoising", "text": "Our next experiment explores how we can use the OptNet architecture to improve signal processing techniques that currently use convex optimization as a basis. Specifically, our goal in this case is to denounce a noisy 1D signal given the consistency of training data from noisy and clean signals generated from the same distribution. Specifically, such problems are often addressed by convex optimization procedures, and (1D) total variation of denoizing approaches is a particularly common and simple approach. Specifically, total variation of denoizing approaches attempts to smooth out some noisy observed signals by solving the optimization problem. The number of change points of the signal is small, and we approach y at the end by a (coarse) constant function. To test this approach and competing approaches of signal difference, we encourage this difference to be sparse, i.e. the number of change points at the end of the signal is unconstant, and the number of change points at the end is ununiform."}, {"heading": "4.2.1. BASELINE: TOTAL VARIATION DENOISING", "text": "In order to define a baseline for the denociation of power with total variation, we perform the above mentioned optimization problem with varying values between \u03bb and 100. The method works best with a selection of \u03bb \u2248 13 and achieves a minimum test MSE for our task of about 16.5 (the units here are not important, the only relevant quantity is the relative performance of the different algorithms)."}, {"heading": "4.2.2. BASELINE: LEARNING WITH A FULLY-CONNECTED NEURAL NETWORK", "text": "An alternative approach to denocialization is to learn from data. To predict the original signal, a function f\u03b8 (x) can be used. The optimal \u03b8 can be learned by using the mean square error between the true and the predicted signal. Denociation is typically a difficult function to learn, and Figure 2 shows that a fully networked neural network performs significantly worse in this denociation task than the convex optimization problem."}, {"heading": "4.2.3. LEARNING THE DIFFERENCING OPERATOR WITH OPTNET", "text": "Between the feedforward neural network approach and the convex overall variation optimization, we could instead use a generic OptNet layer that effectively allows us to (14) resolve any denocialization matrix that we randomly initialize. Although the accuracy here is much lower than even the fully connected case, this is largely the result of learning an over-regulated solution for D. This is indeed a point that should be taken into account in future work (we refer to our comments in the previous section on the fundamental challenges in forming these layers), but the point we would like to emphasize here is that the OptNet layer seems to learn something very explicable and understandable. Specifically, Figure 3 shows the D matrix of our solution before and after learning (we permutate the lines to make them ordered by the magnitude of what the largeabsolute value entries occur). What is interesting about this image is that the learned matrix is essentially generated by the same D-type matrix as the other intuition."}, {"heading": "4.2.4. FINE-TUNING AND IMPROVING THE TOTAL VARIATION SOLUTION", "text": "Finally, to emphasize the ability of the OptNet methods to improve the results of a convex program specifically tailored to the data. In this case, we use the same OptNet architecture as in the previous section, but initialize D as the differentiation matrix as in the total variation solution. As shown in Figure 4, the method can improve both training and testing of MSE compared to the TV solution, especially compared to the MSE test by 12%."}, {"heading": "4.3. MNIST", "text": "In this section, we look at the integration of OptNet layers into a traditional, fully connected network for the MNIST problem; the results here show only a very marginal improvement, if any, over a fully connected layer (MNIST is, after all, very well solved by a fully connected network, let alone a folded network), but our main point of this comparison is simply to show that we can incorporate these layers into existing network architectures and spread the gradients efficiently through the layer. Specifically, we use a fully connected FC600-FC10-FC10-SoftMax network and compare it to an FC600-FC10Optnet10-SoftMax network, where the numbers after each layer indicate the layer size, and the OptNet network in this case has only inequality limitations. As shown in Figure 5, the results for both networks are similar, with slightly fewer errors and less variation in the OptNet network."}, {"heading": "4.4. Sudoku", "text": "Finally, we present the most important illustrative example of the representational power of our approach, the task of learning the game of Sudoku. Sudoku is a popular logical puzzle in which a (typically 9x9) grid of dots must be arranged in the face of an initial point, so that each line, each line, and each 3x3 grid of dots must contain one. We consider the slightly simpler case of 4x4 sudoku puzzles, using the number in 1 to 4, as in Figure 4.3.Sudoku is basically a satisfaction problem, and is trivial for computers that explain the rules of the game."}, {"heading": "5. Conclusion", "text": "We have introduced OptNet, an architecture of neural networks where we use optimization problems as a single layer in the network. We have derived the algorithmic formulation for differentiation through these layers, which allows for backpropagation into end-to-end architectures. We have also developed an efficient batch solver for these optimizations, based on a primary dual method of the inner point, and developed a method to get the necessary gradient information \"free of charge\" from this approach. Our experiments highlight the potential performance of these networks and show that they can solve problems where existing networks are very ill-suited, such as Sudoku problems, to learn purely from data. There are many future research directions for these approaches, but we believe that they add another important primitive to the toolbox of neural networking practitioners."}], "references": [{"title": "Input convex neural networks", "author": ["Amos", "Brandon", "Xu", "Lei", "Kolter", "J Zico"], "venue": "arXiv preprint arXiv:1609.07152,", "citeRegEx": "Amos et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Amos et al\\.", "year": 2016}, {"title": "Structured prediction energy networks", "author": ["Belanger", "David", "McCallum", "Andrew"], "venue": "In Proceedings of the International Conference on Machine Learning,", "citeRegEx": "Belanger et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Belanger et al\\.", "year": 2016}, {"title": "Generic methods for optimization-based modeling", "author": ["Domke", "Justin"], "venue": "In AISTATS,", "citeRegEx": "Domke and Justin.,? \\Q2012\\E", "shortCiteRegEx": "Domke and Justin.", "year": 2012}, {"title": "Generative adversarial nets", "author": ["Goodfellow", "Ian", "Pouget-Abadie", "Jean", "Mirza", "Mehdi", "Xu", "Bing", "Warde-Farley", "David", "Ozair", "Sherjil", "Courville", "Aaron", "Bengio", "Yoshua"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Goodfellow et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Goodfellow et al\\.", "year": 2014}, {"title": "On differentiating parameterized argmin and argmax problems with application to bi-level optimization", "author": ["Gould", "Stephen", "Fernando", "Basura", "Cherian", "Anoop", "Anderson", "Peter", "Santa Cruz", "Rodrigo", "Guo", "Edison"], "venue": "arXiv preprint arXiv:1607.05447,", "citeRegEx": "Gould et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Gould et al\\.", "year": 2016}, {"title": "Composing graphical models with neural networks for structured representations and fast inference", "author": ["Johnson", "Matthew", "Duvenaud", "David K", "Wiltschko", "Alex", "Adams", "Ryan P", "Datta", "Sandeep R"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Johnson et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Johnson et al\\.", "year": 2016}, {"title": "Probabilistic graphical models: principles and techniques", "author": ["Koller", "Daphne", "Friedman", "Nir"], "venue": "MIT press,", "citeRegEx": "Koller et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Koller et al\\.", "year": 2009}, {"title": "Numerical simulation of time-dependent contact and friction problems in rigid body mechanics", "author": ["L\u00f6tstedt", "Per"], "venue": "SIAM journal on scientific and statistical computing,", "citeRegEx": "L\u00f6tstedt and Per.,? \\Q1984\\E", "shortCiteRegEx": "L\u00f6tstedt and Per.", "year": 1984}, {"title": "Matrix differential calculus", "author": ["X Magnus", "Neudecker", "Heinz"], "venue": "New York,", "citeRegEx": "Magnus et al\\.,? \\Q1988\\E", "shortCiteRegEx": "Magnus et al\\.", "year": 1988}, {"title": "Task-driven dictionary learning", "author": ["Mairal", "Julien", "Bach", "Francis", "Ponce", "Jean"], "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence,", "citeRegEx": "Mairal et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Mairal et al\\.", "year": 2012}, {"title": "Cvxgen: A code generator for embedded convex optimization", "author": ["Mattingley", "Jacob", "Boyd", "Stephen"], "venue": "Optimization and Engineering,", "citeRegEx": "Mattingley et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Mattingley et al\\.", "year": 2012}, {"title": "Unrolled generative adversarial networks", "author": ["Metz", "Luke", "Poole", "Ben", "Pfau", "David", "Sohl-Dickstein", "Jascha"], "venue": "arXiv preprint arXiv:1611.02163,", "citeRegEx": "Metz et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Metz et al\\.", "year": 2016}, {"title": "Model predictive control: past, present and future", "author": ["Morari", "Manfred", "Lee", "Jay H"], "venue": "Computers & Chemical Engineering,", "citeRegEx": "Morari et al\\.,? \\Q1999\\E", "shortCiteRegEx": "Morari et al\\.", "year": 1999}, {"title": "Learning structured prediction models: A large margin approach", "author": ["Taskar", "Ben", "Chatalbashev", "Vassil", "Koller", "Daphne", "Guestrin", "Carlos"], "venue": "In Proceedings of the 22nd International Conference on Machine Learning,", "citeRegEx": "Taskar et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Taskar et al\\.", "year": 2005}, {"title": "Large margin methods for structured and interdependent output variables", "author": ["Tsochantaridis", "Ioannis", "Joachims", "Thorsten", "Hofmann", "Thomas", "Altun", "Yasemin"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Tsochantaridis et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Tsochantaridis et al\\.", "year": 2005}, {"title": "Primal-dual interior-point methods", "author": ["Wright", "Stephen J"], "venue": null, "citeRegEx": "Wright and J.,? \\Q1997\\E", "shortCiteRegEx": "Wright and J.", "year": 1997}], "referenceMentions": [{"referenceID": 0, "context": "For instance, recent work on structured prediction (Belanger & McCallum, 2016; Amos et al., 2016) has used optimization within energy-based neural network models, and (Metz et al.", "startOffset": 51, "endOffset": 97}, {"referenceID": 11, "context": ", 2016) has used optimization within energy-based neural network models, and (Metz et al., 2016) used unrolled optimization within a network to stabilize the convergence of generative adversarial networks (Goodfellow et al.", "startOffset": 77, "endOffset": 96}, {"referenceID": 3, "context": ", 2016) used unrolled optimization within a network to stabilize the convergence of generative adversarial networks (Goodfellow et al., 2014).", "startOffset": 116, "endOffset": 141}, {"referenceID": 13, "context": "There are two broad classes of learning approaches in structured prediction: methods that use probabilistic inference techniques (typically exploiting the fact that the gradient of the log likelihood is given by the actual feature expectations minus their expectation under the learned model (Koller & Friedman, 2009, Ch 20)), and methods that rely solely upon MAP inference (such as max-margin structured prediction (Taskar et al., 2005; Tsochantaridis et al., 2005)).", "startOffset": 417, "endOffset": 467}, {"referenceID": 14, "context": "There are two broad classes of learning approaches in structured prediction: methods that use probabilistic inference techniques (typically exploiting the fact that the gradient of the log likelihood is given by the actual feature expectations minus their expectation under the learned model (Koller & Friedman, 2009, Ch 20)), and methods that rely solely upon MAP inference (such as max-margin structured prediction (Taskar et al., 2005; Tsochantaridis et al., 2005)).", "startOffset": 417, "endOffset": 467}, {"referenceID": 4, "context": "In the case of (Gould et al., 2016), the authors describe general techniques for differentiation through optimization problems, but only describe the case of exact equality constraints rather than both equality and inequality constraints (in the case inequality constraints, they add these via a barrier function).", "startOffset": 15, "endOffset": 35}, {"referenceID": 5, "context": "Other work (Johnson et al., 2016; Amos et al., 2016) (the later with some inequality constraints) uses argmin differentiation within the context of a specific optimization problem, but doesn\u2019t consider a particularly general setting; similarly, the older work of (Mairal et al.", "startOffset": 11, "endOffset": 52}, {"referenceID": 0, "context": "Other work (Johnson et al., 2016; Amos et al., 2016) (the later with some inequality constraints) uses argmin differentiation within the context of a specific optimization problem, but doesn\u2019t consider a particularly general setting; similarly, the older work of (Mairal et al.", "startOffset": 11, "endOffset": 52}, {"referenceID": 9, "context": ", 2016) (the later with some inequality constraints) uses argmin differentiation within the context of a specific optimization problem, but doesn\u2019t consider a particularly general setting; similarly, the older work of (Mairal et al., 2012) considered argmin differentiation for a LASSO problem, deriving specific rules for this case, and presenting an efficient algorithm based upon our ability to solve the LASSO problem efficiently.", "startOffset": 218, "endOffset": 239}, {"referenceID": 4, "context": "Although the previous papers mentioned above have considered similar argmin differentiation techniques (Gould et al., 2016), to the best of our knowledge this is the first case of a general formulation for argmin differentiation in the presence of exact equality and inequality constraints.", "startOffset": 103, "endOffset": 123}], "year": 2017, "abstractText": "This paper presents OptNet, a network architecture that integrates optimization problems (here, specifically in the form of quadratic programs) as individual layers in larger end-to-end trainable deep networks. These layers allow complex dependencies between the hidden states to be captured that traditional convolutional and fully-connected layers are not able to capture. In this paper, we develop the foundations for such an architecture: we derive the equations to perform exact differentiation through these layers and with respect to layer parameters; we develop a highly efficient solver for these layers that exploits fast GPU-based batch solves within a primal-dual interior point method, and which provides backpropagation gradients with virtually no additional cost on top of the solve; and we highlight the application of these approaches in several problems. In one particularly standout example, we show that the method is capable of learning to play Sudoku given just input and output games, with no a priori information about the rules of the game; this task is virtually impossible for other neural network architectures that we have experimented with, and highlights the representation capabilities of our approach.", "creator": "LaTeX with hyperref package"}}}