{"id": "1703.00522", "review": {"conference": "ICML", "VERSION": "v1", "DATE_OF_SUBMISSION": "1-Mar-2017", "title": "Understanding Synthetic Gradients and Decoupled Neural Interfaces", "abstract": "When training neural networks, the use of Synthetic Gradients (SG) allows layers or modules to be trained without update locking - without waiting for a true error gradient to be backpropagated - resulting in Decoupled Neural Interfaces (DNIs). This unlocked ability of being able to update parts of a neural network asynchronously and with only local information was demonstrated to work empirically in Jaderberg et al (2016). However, there has been very little demonstration of what changes DNIs and SGs impose from a functional, representational, and learning dynamics point of view. In this paper, we study DNIs through the use of synthetic gradients on feed-forward networks to better understand their behaviour and elucidate their effect on optimisation. We show that the incorporation of SGs does not affect the representational strength of the learning system for a neural network, and prove the convergence of the learning system for linear and deep linear models. On practical problems we investigate the mechanism by which synthetic gradient estimators approximate the true loss, and, surprisingly, how that leads to drastically different layer-wise representations. Finally, we also expose the relationship of using synthetic gradients to other error approximation techniques and find a unifying language for discussion and comparison.", "histories": [["v1", "Wed, 1 Mar 2017 21:41:09 GMT  (2521kb,D)", "http://arxiv.org/abs/1703.00522v1", null]], "reviews": [], "SUBJECTS": "cs.LG cs.NE", "authors": ["wojciech marian czarnecki", "grzegorz swirszcz", "max jaderberg", "simon osindero", "oriol vinyals", "koray kavukcuoglu"], "accepted": true, "id": "1703.00522"}, "pdf": {"name": "1703.00522.pdf", "metadata": {"source": "META", "title": "Understanding Synthetic Gradients and Decoupled Neural Interfaces", "authors": ["Wojciech Marian Czarnecki", "Grzegorz \u015awirszcz", "Max Jaderberg", "Simon Osindero", "Oriol Vinyals", "Koray Kavukcuoglu"], "emails": ["<lejlot@google.com>."], "sections": [{"heading": "1. Introduction", "text": "This year it is more than ever before in the history of the city."}, {"heading": "2. DNI using Synthetic Gradients", "text": "The key idea of synthetic gradients and DNI is to approximate the true gradient of loss with a learned model that predicts gradients without performing complete backpropagation. Consider a forward-facing network consisting of N layers fn, n layers fn,..., N layer, each taking an input hn-1i and producing an output hni = fn (h n-1 i), where h layer fn = xi is the input data point xi. A loss is defined on the output of the net Li = L (hNi, yi), where yi is the given label or monitoring for xi (which comes from some unknown P (y-x)). Each layer fn has parameters that can be used jointly to minimize Li with the gradient-based update Li hni SG."}, {"heading": "Assumptions and notation", "text": "In the course of this work, we will consider the use of a single synthetic gradient module on a single layer k and for a generic data sample j, and therefore refer to h = hj = hkj; unless otherwise stated, we will drop the superscript k and the subscript j. This model is shown in Figure 1 (b). We will also focus on SG modules that consider the true label / value of the point as conditioning SG (h, y) as opposed to SG (h). Note that an SG module without label conditioning does not attempt to understand synthetic gradients and DNIssue, but rather EP (y | x) Yankeh, since L is a function of both input and label. Theoretically, the lack of a label is sufficient parameterization, but learning becomes more difficult because the SG module must learn P (y | x) in addition."}, {"heading": "Synthetic gradients in operation", "text": "Consider an N-layer feed network with a single SG module on layer k. This network can effectively be split into two sub-nets: the first takes an input x and produces output h = Fh (x) = fk (fk \u2212 1)) and causes a loss L = L (p, y) based on a label y. With regular back propagation, the learning signal for the first network Fh (h) = fN (h) becomes a total signal specifying how the input to Fp should be changed to reduce the loss. If we attach a linear SG between these two networks, the first sub-network Fh does not receive an exact learning signal from Fp, but an approximation SG (h, y), implying that Fh should reduce the loss."}, {"heading": "3. Critical points", "text": "We look at the SG effect on critical points of the optimization problem. Specifically, it seems natural to ask whether a model enhanced with SG is capable of learning the same functions as the original model. We ask this question under the assumption of a locally converging training method, so that we always end up at a critical point. < In the case of an SG-based model, however, this implies a number of parameters that imply regulation of the model class, or whether it merely introduces a modification of learning dynamics but maintains the same set of critical points. < h = 0 and 6 points of critical SG = 0. In other words, we try to determine whether SG introduces a regulation of the model class that changes the critical points, or whether it merely introduces a modification of learning dynamics but maintains the same critical points. In general, the answer is positive: SG induces a regulating effect."}, {"heading": "4. Learning dynamics", "text": "Having shown that important critical points can be preserved and that new ones can be created, we need a better characterization of the attraction forces and to understand when an approximation of a good solution can be expected both in theory and in practice."}, {"heading": "Artificial Data", "text": "We perform an empirical analysis of learning dynamics on easily analyzable artificial data, which is often the function of logistic regression. (We create 2 and 100 dimensional versions of four basic data sets (details in the supplementary materials section C) and train four simple models (one linear model and one deep linear with 10 hidden layers, trained to minimize MSE and log loss) with regular backprop and with an SG-based alternative to see2In this case, our gradient approach must be reasonable at any point by optimizing, not just the critical ones. Whether it is converted (numerically) to the same solution, it is unclear whether it is necessary to convert both flat and deep linear architectures of the SG-based model to the global optimum (exact numerical results in the supplementary materials table 2). However, this is not the case for logistic regression."}, {"heading": "Convergence", "text": "It is trivial to say that a SG module that is used is globally convergent to the true gradient, and we only use its results after it has converged, then the whole model behaves like a model that has been trained with regular backprop. In practice, however, we never do this and instead train the two models in parallel, without waiting for the convergence of the SG module. We will now discuss some of the consequences of this convergence, and start by showing that as long as a synthetic gradient is produced, the entire system is close enough to the true critical points. Namely, only if the error introduced by SG is propagated backwards on all parameters is consistently smaller than the norm of the true gradient multiplied by some positive convergence smaller than one, the whole system converges. So, essentially, we need the SG error to circumvent critical points. Let's say that a SG module is trained in each iteration."}, {"heading": "5. Trained models", "text": "This year it is more than ever before."}, {"heading": "6. SG and conspiring networks", "text": "We are moving our attention and looking at a unified view of several different learning principles that work by replacing real gradients with surrogates. < < < < < < < < < < < < / p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \""}, {"heading": "7. Conclusions", "text": "First, we have shown that the introduction of SG does not necessarily change the critical points of the original problem, but at the same time can bring new critical points into the learning process. This is an important result that shows that SG, despite the simplification of error signals, does not act like a typical regulator. Second, we have shown that (despite the modification of learning dynamics) SG-based models converge to the true model under some additional assumptions with analog solutions. We have demonstrated exact convergence for a simple class of models, and for more complex situations, we have shown that the implicit loss model captures the characteristics of the true loss area. It remains an open question how to characterize learning dynamics in more general cases. Third, we have shown that despite these convergence characteristics, the tracked networks can be qualitatively different from those trained with backpropagation. Although this is not necessarily a disadvantage, one should be aware of this when using synthetic processes in practice."}, {"heading": "Acknowledgments", "text": "The authors thank James Martens and Ross Goroshin for their valuable comments and discussions."}, {"heading": "Supplementary Materials", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "A. Additional examples", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "Critical points", "text": "Consider a small one-dimensional training data set {\u2212 2, \u2212 1, 1, 2}, and consider a simple system where the model f: R \u2192 R is parameterized with two scalars, a and b, and produces axe + b. We train it to minimize L (a, b) = 4 i = 1 | axi + b |. This module has a unique minimum achieved for a = b = 0, and standard gradient-based methods will approach this solution. Now, let's add an SG module between f and L. This module generates a (traceable) scalar value c-R (i.e. it generates a single number, regardless of the input value). Regardless of the value a, we have a critical point of the SG module when b = 0 and c = 0 are introduced. However, solutions with a = 1 and c = 0 are clearly not critical points of the original system. Figure 6 shows the loss of the surface and the SG module when it introduces a critical point."}, {"heading": "B. Proofs", "text": "# # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # #"}, {"heading": "C. Technical details", "text": "All experiments were performed with TensorFlow (Abadi et al., 2016). In all experiments, SG loss is the MSE between synthetic and true gradients. Since all SGs studied were linear, weights were initialized to zeros, so SG initially produces zero gradients and does not affect convergence (since linear regression is convex)."}, {"heading": "Datasets", "text": "Each of the artificial data sets is a classification problem, consisting of X from the k-dimensional Gaussian distribution with zero mean and standard deviation of the unit. For k = 2 we take 100 points and for k = 100 1000 points. Labels y are generated depending on the record name: \u2022 linear - we randomly sample an origin crossing hyperplane (by scanning its parameters from standard Gaussians) and label points accordingly, \u2022 noisyk - we label points according to linear and then randomly swap labels from 10% of the samples, \u2022 randomk - points are labeled completely randomly. We used a uniform coding of binary labels to maintain compatibility with Softmax-based models, which is consistent with the rest of the experiments. However, we also tested the same things with a single output neuron and a regular moid-based network and analog results obtained."}, {"heading": "Optimisation", "text": "Optimization is performed with the Adam Optimizer (Kingma & Ba, 2014) with a learning rate of 3e \u2212 5. This applies to both the main model and the SG module."}, {"heading": "Artificial datasets", "text": "Table 2 shows the results for the formation of linear regression (flat MSE), 10 covert linear regression of deep layers (deep MSE), logistic regression (flat log loss) and 10 covert linear classification of deep layers (deep log loss).Since all these problems converge to the global optimum (if properly initialized), we report the difference between the final loss achieved for SG-enriched models and the true global optimum."}, {"heading": "MNIST experiments", "text": "The networks used are simple feed networks with h-layers of 512 hidden relays followed by batch normalization layers. The last layer is a regular 10-class softmax layer. The inputs were scaled to [0, 1] interval, and no pre-processing was applied."}, {"heading": "Representational Dissimilarity Matrices", "text": "To form RSMs for one layer h, we try 400 points (sorted by their designation) from the MNIST dataset, {xi} 400i = 1, and record activations for each of these points, hi = h (xi). Then we calculate a matrix RSM so that RSMij = 1 \u2212 corr (hi, hj) is not correlated. Consequently, a perfect RSM is a block diagonal matrix, so that elements of the same class have a high correlation representation and the representations of points from two different classes are not correlated. Figure 7 is the advanced version of the analog Figure 3 from the main paper, in which we show RDMs for backpropagation, a single SG, SG between all two layers, and also the DFA model, when 20 hidden layers of deep relief network.Understanding Synthetic Gradients and DNIsdataset model losslineable 0.00000 0.000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000001"}, {"heading": "Linear classifier/regression probes", "text": "One way to verify the actual classification problem at each level of a feedback network is to attach linear classifiers to each hidden layer and train them on the main task, without propagating them backwards through the rest of the network. In this way, we can build a chart of tensile accuracy from the representation on each layer. As shown in Figure 8 (left), there is not much difference between such a backpropagation analysis and a single SG module, which confirms our claim in the work that despite different representations in both sections of the SG-based module - both are good enough to solve the main problem. We can also find that DFA is trying to solve the bottom-up classification problem as opposed to top-down - note that we can have 100% accuracy for DFA after the very first hidden layer, which does not even apply to backpropagation."}, {"heading": "Loss estimation", "text": "In the main article, we show how SG modules that use both activations and labels can describe the loss area relatively well for most of the training, with different data sets and losses. For the sake of completeness, we include the same experiment for SG modules that do not use label information (Figure 9 (a) - (d)) and for a module that does not use activations at all (Figure 9 (e) - (h))). There are two important observations here: First, neither approach provides loss estimation fidelity comparable to the full SG (due to activations and labels), which provides further empirical confirmation for the correct conditioning of the module. Second, models that only use labels did not achieve a good solution after 100k iterations, whereas SG could achieve this without the label (but took much longer and was much louder)."}], "references": [{"title": "Understanding intermediate layers using linear classifier probes", "author": ["Alain", "Guillaume", "Bengio", "Yoshua"], "venue": "arXiv preprint arXiv:1610.01644,", "citeRegEx": "Alain et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Alain et al\\.", "year": 2016}, {"title": "Learning in the machine: Random backpropagation and the learning channel", "author": ["Baldi", "Pierre", "Sadowski", "Peter", "Lu", "Zhiqin"], "venue": "arXiv preprint arXiv:1612.02734,", "citeRegEx": "Baldi et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Baldi et al\\.", "year": 2016}, {"title": "Kickback cuts backprop\u2019s red-tape: Biologically plausible credit assignment in neural networks", "author": ["Balduzzi", "David", "Vanchinathan", "Hastagiri", "Buhmann", "Joachim"], "venue": "arXiv preprint arXiv:1411.6191,", "citeRegEx": "Balduzzi et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Balduzzi et al\\.", "year": 2014}, {"title": "Towards biologically plausible deep learning", "author": ["Bengio", "Yoshua", "Lee", "Dong-Hyun", "Bornschein", "Jorg", "Mesnard", "Thomas", "Lin", "Zhouhan"], "venue": "arXiv preprint arXiv:1502.04156,", "citeRegEx": "Bengio et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Bengio et al\\.", "year": 2015}, {"title": "How much gradient noise does a gradient-based linesearch method tolerate", "author": ["Gratton", "Serge", "Toint", "Philippe L", "Tr\u00f6ltzsch", "Anke"], "venue": "Technical report, Citeseer,", "citeRegEx": "Gratton et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Gratton et al\\.", "year": 2011}, {"title": "Decoupled neural interfaces using synthetic gradients", "author": ["Jaderberg", "Max", "Czarnecki", "Wojciech Marian", "Osindero", "Simon", "Vinyals", "Oriol", "Graves", "Alex", "Kavukcuoglu", "Koray"], "venue": "arXiv preprint arXiv:1608.05343,", "citeRegEx": "Jaderberg et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Jaderberg et al\\.", "year": 2016}, {"title": "Adam: A method for stochastic optimization", "author": ["Kingma", "Diederik", "Ba", "Jimmy"], "venue": "arXiv preprint arXiv:1412.6980,", "citeRegEx": "Kingma et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Kingma et al\\.", "year": 2014}, {"title": "Representational similarity analysis-connecting the branches of systems neuroscience", "author": ["Kriegeskorte", "Nikolaus", "Mur", "Marieke", "Bandettini", "Peter A"], "venue": "Frontiers in systems neuroscience,", "citeRegEx": "Kriegeskorte et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Kriegeskorte et al\\.", "year": 2008}, {"title": "Random synaptic feedback weights support error backpropagation for deep learning", "author": ["Lillicrap", "Timothy P", "Cownden", "Daniel", "Tweed", "Douglas B", "Akerman", "Colin J"], "venue": "Nature Communications,", "citeRegEx": "Lillicrap et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Lillicrap et al\\.", "year": 2016}, {"title": "Toward an integration of deep learning and neuroscience", "author": ["Marblestone", "Adam H", "Wayne", "Greg", "Kording", "Konrad P"], "venue": "Frontiers in Computational Neuroscience,", "citeRegEx": "Marblestone et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Marblestone et al\\.", "year": 2016}, {"title": "Direct feedback alignment provides learning in deep neural networks", "author": ["N\u00f8kland", "Arild"], "venue": "Advances in Neural Information Processing Systems", "citeRegEx": "N\u00f8kland and Arild.,? \\Q2016\\E", "shortCiteRegEx": "N\u00f8kland and Arild.", "year": 2016}, {"title": "\u03b2ky + \u03b3k)", "author": ["Jaderberg"], "venue": null, "citeRegEx": "Jaderberg,? \\Q2016\\E", "shortCiteRegEx": "Jaderberg", "year": 2016}], "referenceMentions": [{"referenceID": 5, "context": "This unlocked ability of being able to update parts of a neural network asynchronously and with only local information was demonstrated to work empirically in Jaderberg et al. (2016). However, there has been very little demonstration of what changes DNIs and SGs impose from a functional, representational, and learning dynamics point of view.", "startOffset": 159, "endOffset": 183}, {"referenceID": 9, "context": "However, this scheme has many potential drawbacks, as well as lacking biological plausibility (Marblestone et al., 2016; Bengio et al., 2015).", "startOffset": 94, "endOffset": 141}, {"referenceID": 3, "context": "However, this scheme has many potential drawbacks, as well as lacking biological plausibility (Marblestone et al., 2016; Bengio et al., 2015).", "startOffset": 94, "endOffset": 141}, {"referenceID": 5, "context": "One way of overcoming this issue is to apply Synthetic Gradients (SGs) to build Decoupled Neural Interfaces (DNIs) (Jaderberg et al., 2016).", "startOffset": 115, "endOffset": 139}, {"referenceID": 5, "context": "While the empirical evidence in Jaderberg et al. (2016) suggests that SGs do not have a negative impact and that this potential is attainable, this paper will dig deeper and analyse the result of using SGs to accurately answer the question of the impact of synthetic gradients on learning systems.", "startOffset": 32, "endOffset": 56}, {"referenceID": 8, "context": "In addition, in Section 6 we look at formalising the connection between SGs and other forms of approximate error propagation such as Feedback Alignment (Lillicrap et al., 2016), Direct Feedback Alignment (N\u00f8kland, 2016; Baldi et al.", "startOffset": 152, "endOffset": 176}, {"referenceID": 1, "context": ", 2016), Direct Feedback Alignment (N\u00f8kland, 2016; Baldi et al., 2016), and Kickback (Balduzzi et al.", "startOffset": 35, "endOffset": 70}, {"referenceID": 2, "context": ", 2016), and Kickback (Balduzzi et al., 2014), and show that all these error approximation schemes can be captured in a unified framework, but crucially only using synthetic gradients can one achieve unlocked training.", "startOffset": 22, "endOffset": 45}, {"referenceID": 5, "context": "Jaderberg et al. (2016) introduces a learnt prediction of the error gradient, the synthetic gradient SG(hi , yi) = \u0302 \u2202Li/\u2202hi ' \u2202Li/\u2202hi resulting in the update", "startOffset": 0, "endOffset": 24}, {"referenceID": 5, "context": "the topmost layers), which matches observations from Jaderberg et al. (2016) and the intuition that the lower layers solve a more complex problem (since they bootstrap their targets).", "startOffset": 53, "endOffset": 77}, {"referenceID": 4, "context": "Proof follows from showing that, under the assumptions, effectively we are training with noisy gradients, where the noise is small enough for convergence guarantees given by Zoutendijk (1970); Gratton et al. (2011) to apply.", "startOffset": 193, "endOffset": 215}, {"referenceID": 7, "context": "One approach is to compute and visualise Representational Dissimilarity Matrices (Kriegeskorte et al., 2008) for our data.", "startOffset": 81, "endOffset": 108}, {"referenceID": 8, "context": "We focus on three such approaches: Feedback Alignment (FA) (Lillicrap et al., 2016), Direct Feedback Alignment (DFA) (N\u00f8kland, 2016), and Kickback (KB) (Balduzzi et al.", "startOffset": 59, "endOffset": 83}, {"referenceID": 2, "context": ", 2016), Direct Feedback Alignment (DFA) (N\u00f8kland, 2016), and Kickback (KB) (Balduzzi et al., 2014).", "startOffset": 76, "endOffset": 99}], "year": 2017, "abstractText": "When training neural networks, the use of Synthetic Gradients (SG) allows layers or modules to be trained without update locking \u2013 without waiting for a true error gradient to be backpropagated \u2013 resulting in Decoupled Neural Interfaces (DNIs). This unlocked ability of being able to update parts of a neural network asynchronously and with only local information was demonstrated to work empirically in Jaderberg et al. (2016). However, there has been very little demonstration of what changes DNIs and SGs impose from a functional, representational, and learning dynamics point of view. In this paper, we study DNIs through the use of synthetic gradients on feed-forward networks to better understand their behaviour and elucidate their effect on optimisation. We show that the incorporation of SGs does not affect the representational strength of the learning system for a neural network, and prove the convergence of the learning system for linear and deep linear models. On practical problems we investigate the mechanism by which synthetic gradient estimators approximate the true loss, and, surprisingly, how that leads to drastically different layer-wise representations. Finally, we also expose the relationship of using synthetic gradients to other error approximation techniques and find a unifying language for discussion and comparison.", "creator": "LaTeX with hyperref package"}}}