{"id": "1703.00560", "review": {"conference": "ICML", "VERSION": "v1", "DATE_OF_SUBMISSION": "2-Mar-2017", "title": "An Analytical Formula of Population Gradient for two-layered ReLU network and its Applications in Convergence and Critical Point Analysis", "abstract": "In this paper, we explore theoretical properties of training a two-layered ReLU network $g(\\mathbf{x}; \\mathbf{w}) = \\sum_{j=1}^K \\sigma(\\mathbf{w}_j^T\\mathbf{x})$ with centered $d$-dimensional spherical Gaussian input $\\mathbf{x}$ ($\\sigma$=ReLU). We train our network with gradient descent on $\\mathbf{w}$ to mimic the output of a teacher network with the same architecture and fixed parameters $\\mathbf{w}^*$. We show that its population gradient has an analytical formula, leading to interesting theoretical analysis of critical points and convergence behaviors. First, we prove that critical points outside the hyperplane spanned by the teacher parameters (\"out-of-plane\") are not isolated and form manifolds, and characterize in-plane critical-point-free regions for two ReLU case. On the other hand, convergence to $\\mathbf{w}^*$ for one ReLU node is guaranteed with at least $(1-\\epsilon)/2$ probability, if weights are initialized randomly with standard deviation upper-bounded by $O(\\epsilon/\\sqrt{d})$, consistent with empirical practice. For network with many ReLU nodes, we prove that an infinitesimal perturbation of weight initialization results in convergence towards $\\mathbf{w}^*$ (or its permutation), a phenomenon known as spontaneous symmetric-breaking (SSB) in physics. We assume no independence of ReLU activations. Simulation verifies our findings.", "histories": [["v1", "Thu, 2 Mar 2017 00:15:00 GMT  (5040kb,D)", "http://arxiv.org/abs/1703.00560v1", null], ["v2", "Wed, 24 May 2017 18:42:08 GMT  (5040kb,D)", "http://arxiv.org/abs/1703.00560v2", "International Conference on Machine Learning (ICML) 2017"]], "reviews": [], "SUBJECTS": "cs.LG", "authors": ["yuandong tian"], "accepted": true, "id": "1703.00560"}, "pdf": {"name": "1703.00560.pdf", "metadata": {"source": "META", "title": "An Analytical Formula of Population Gradient for two-layered ReLU network and its Applications in Convergence and Critical Point Analysis", "authors": ["Yuandong Tian"], "emails": ["<yuandong@fb.com>."], "sections": [{"heading": null, "text": "We train our gradient drop network to w in order to mimic the performance of a teacher network with the same architecture and fixed parameters w *. We show that its population gradient has an analytical formula that leads to interesting theoretical analyses of critical points and convergence behavior. First, we prove that critical points outside the out-of-plane spanned by the teacher parameters are not isolated and form manifolds, and characterize flat, point-free regions for two ReLU cases. On the other hand, convergence to w \u00b2 is guaranteed for a ReLU node with at least (1 \u2212) / 2 probability if weights are randomly initialized with standard deviations above the limit of O (/ \u221a d) consistent with empirical practice. For networks with many ReLU nodes, we point out that a mutation in physical convergence (We assume convergence) does not result in convergence."}, {"heading": "1. Introduction", "text": "reactionary, reactionary, reactionary, reactionary, reactionary, reactionary, reactionary, reactionary, reactionary, reactionary, reactionary, reactionary, reactionary, reactionary, reactionary, reactionary, reactionary, reactionary, reactionary, reactionary, reactionary, reactionary, reactionary, reactionary, reactionary, reactionary, reactionary, reactionary, reactionary, reactionary, reactionary, reactionary, reactionary, reactionary, reactionary, reactionary, reactionary, reactionary, reactionary, reactionary, reactionary, reactionary, reactionary, reactionary, reactionary, reactionary, reactionary, reactionary, reactionary, reactionary, reactionary, reactionary, reactionary, reactionary, reactionary, reactionary, reactionary, reactionary, reactionary, reactionary, reactionary, reactionary,"}, {"heading": "2. Related Works", "text": "For multilayered linear network, many papers analyze their critical points and convergence behavior. (Saxe et al., 2013) analyzes its dynamics of gradient parentage and (Kawaguchi, 2016) shows that each local minimum is global. On the other hand, very little theoretical work has been done for nonlinear networks. (Mei et al., 2016) Also, the global convergence for a single nonlinear node, whose derivatives of activation \u03c3 \u2032, are limited and \u03c3 \u2032. Similar to our approach, (Saad & Solla, 1996) uses the student-teacher attitude and analyzes the dynamics when the teacher parameters w \u0445 are orthonormal."}, {"heading": "3. Problem Definition", "text": "Describe N as the number of samples and d as the input dimension. The N-by-d matrix X is the input data and w * is the fixed parameter of the teacher network. In view of the current estimate w, the l2 loss is as follows: J (w) = 12% g (X; w *) \u2212 g (X; w) 2, (2) Here, we focus on the population loss EX [J], assuming that input X follows the spherical Gaussian distribution N (0, I). Its gradient is the population gradient EX [... Jw (w)] (abbreviation E [... J]). In this paper, we examine critical points E [... J] = 0 and the dynamics of the vanilla gradient wt + 1 = wt \u2212 \u03b7E [... J (wt)], where \u03b7 is the learning rate."}, {"heading": "4. The Analytical Formula", "text": "We define the gating functionD (w).D (w).D (w).D (w).D (w).D (w).D (w).D (w).D (w).D).D (w).D (w).D).D (w).D (w).D).D (w).D (w).D (w).D (w).D).D (w).D).D (w).D).D (w).D).D (w).D (w).D).D (w).D).D (w).D (w).D).D (w).D).D (w).D (w).D).D (.D).D).D (w).D).D (.D).D).D (.D).D).D).D).D (w).D).D).D).D).D (w).D).D).D).D (w).D).D).D).D).D).D).D (w).D).D).D).D).D).D (w).D).D).D).D).D).D (w).D).D).D).D).D (w).D).D).D).D (w).D).D).D).D (.D).D).D).D).D (w).D).D).D (.D).D).D).D) (.D).D) (.D).D).D) (.D).D).D) (.D) (.D).D) (.D) (.D).D).D) (.D) (w).D).D) (.D).D) (.D) (.D) (w).D) (.D).D) (.D).D) (.D).D)"}, {"heading": "5. Critical Point Analysis", "text": "By solving Equation 8 (the normal equation, E [\u0435wjJ] = 0) we could identify all critical points of g (x). However, it is highly nonlinear and cannot be solved simply. In this paper we provide conditions for critical points using the structure of Equation 8. Following the analysis, the case study forK = 2 gives examples of saddle points and regions without critical points. For convenience, we define the main hyperplane spanned by K-ground truth weight vectors. Note that the equation is most likely K-dimensional. {wj} Kj = 1 is called in-plane if all are in-plane. Otherwise, it is outside the plane."}, {"heading": "5.1. Normal Equation", "text": "The normal equation {E [\u0433wjJ] = 0} Kj = 1 contains Kd scalp equations and can be written as follows: Y E = B \u0435 \"(9), where Y = diag (sin, w, w, w, p, p, w, p, p, p, p, p, p, p, p, p, p, w, w, w, w, w, w, w, w, w, w, w, w, w, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p"}, {"heading": "5.2. In-Plane Normal Equation", "text": "In order to analyze the critical points on the plane, it is sufficient to examine the gradient projections on the plane. If {wj} is fully weighted, the projections can be achieved by multiplying both sides by {ej}, which leads to K2 equations: M (\u0432) w = M *,. This in turn shows the decomposition of angles and magnitudes and linearity in relation to the norms of the weight vectors. Here w \u00b2 = [w1] w \u00b2, [w2] w \u00b2,. This shows whether the properties matter. M and M \u00b2 are K2-by-K matrices that depend only on the angles. The entries of M and M \u00b2 are: mjj \u00b2, k = (kj \u00b2), the properties are significant."}, {"heading": "5.3. Case study: K = 2 network", "text": "In this case, Mr and M \u043a r are matrices of the order of 2 x 2. Here we discuss the case that both w1 and w2 are located within a network. Saddle points. If \u03b812 = 0 (w1 and w2 are collinear), Mr = \u03c011is singular, since e1 and e2 are identical. Starting from Eq.9, the current solution is a saddle point. Note that this results in a limitation for two weights and therefore unlimited solutions exist. Region without critical points. We rely on the following conjecture, which is confirmed empirically in an exhaustive manner (Sec. 7,2)."}, {"heading": "6. Convergence Analysis", "text": "The application of Eq.5 also yields an interesting convergence analysis. We concentrate on infinitesimal analysis, i.e. when the learning rate \u03b7 \u2192 0 and the gradient update becomes a differential equation of the first order: dw / dt = \u2212 EX [\u0394wJ (w)] (15) Then the populated target EX [J] does not increase: dE [J] / dt = \u2212 E [\u0394J] dw / dt = \u2212 E [\u0394J] E [\u0394J] \u2264 0 (16) The aim of the convergence analysis is to determine specific weight initializations w0 that lead to convergence according to the gradient drop dynamics (Eq.15)."}, {"heading": "6.1. Single ReLU case", "text": "Using the Lyapunov method (LaSalle & Lefschetz, 1961), we show that the gradient dynamics (Eqn. 15) converge to w * if it follows the dynamics of Eqn *. \u2212 The Lyapunov function V (w) = 12 * w \u00b2 2 has dV / dt < 0 and the system is asymptotically stable and therefore wt \u00b2 w \u00b2 n \u00b2. Intuition should represent dV / dt as a two-dimensional form of the vector."}, {"heading": "6.2. Multiple ReLU case", "text": "This means that if all wj and w, we are symmetrical among group actions, we are also their population gradients as identity. wyJ, that is, we are symmetrical among group actions, so their population gradients. wyJ, that is, we are symmetrical among group actions, so their population gradients. wyJ, that is, we are symmetrical among group activities. wyJ, that is, we are symmetrical among group activities. wyJ, that is, we are symmetrical among group actions, that is, we are symmetrical among group activities. wjJ, that is, we are symmetrical among group activities. wyJ, that is, we are symmetrical among group activities."}, {"heading": "7. Simulations", "text": "7.1. The analytical solution for F (e, w) We check E [F (e, w)] = E [XD (e) D (w) Xw] (Eqn. 5) with simulation. We randomly select e and w so that their angle (e, w) is evenly distributed in [0, \u03c0]. The analytical formula E [F (e, w)] is compared with F (e, w), which is calculated by scanning input X, which follows the spherical Gaussian distribution. We use relative RMS errors: err = E [F (e, w) \u2212 F (e, w) - and F (e, w). Figure 7 (a) shows the error distribution with respect to angles. For small circuits, the gating function D (w) and D (e) mostly show overlaps and a reliable estimate."}, {"heading": "7.2. Empirical Results in critical point analysis K = 2", "text": "Suggestion 1 can be reduced to enumerate a complicated but 2D function by exhaustive scanning. In comparison, a full optimization of the 2-ReLU network, which is limited to the main hyperplane, includes 6 parameters (8 parameters minus 2 degrees symmetry) and is more difficult to handle. Figure 10 shows that empirically L12 has no additional zero-crossing function except e \u043a = e1 or e2. As shown in Figure 10 (c), we have densely listed \u044512 [0, \u03c0] and e \u0445 on a 104 x 104 grid without finding counter-examples."}, {"heading": "7.3. Convergence analysis for multiple ReLU nodes", "text": "Fig. 8 (a) and (b) show the 2D vector field in Thm 7. Fig. 8 (c) shows the 2D path to convergence with the parameters of the teacher w *. Interestingly enough, the descent of the gradient still leads to the target via detours. This is because at the beginning of optimization all ReLU nodes explain the training error in the same way (both x and y increase); when the \"obvious\" component is explained, the error forces some nodes to explain other components. Consequently, the specialization follows (x increases but y decreases). Fig. 9 shows empirical convergence for K \u2265 2 when the initialization deviates from the initialization [x, y, y] in Thm. 7. Provided the deviation is not large, w converts to w \u00b2."}, {"heading": "8. Extension to multilayer ReLU network", "text": "A natural question is whether the proposed method can be extended to the multi-layer ReLU network. In this case, there is a similar subtraction structure for the gradient to Equation 3. Sentence 2. Denotes [c] as all nodes in layer c. Denotes u \u0445 j and uj as the output of the node j to layer c of the teacher and pupil network, then the gradient of the parameters wj is immediately below node j [c] N - after N diagonal matrices. For each node k [c + 1] (Qj \u2032 uj \"\u2212 Q \u0445 j\") (19), where Xc is the data fed into node j, Qj and Q \u0445 j, N - after N diagonal matrices. For each node k [c + 1], Qk = \u00b2 j [c] wjkDjQj [c] and similarly for Q \u00b2 k.The two-layer network in this essay is a special case with a mathematical group Q = Q = I. Despite the difficulty, the Qj = Q is not necessary."}, {"heading": "9. Conclusion and Future Work", "text": "In this thesis, we examine the downward dynamics of a two-layer, distortion-free ReLU network. The network is trained on the basis of downward gradients to reproduce the results of a teacher network with fixed parameters w \u0445 in the sense of the l2 standard. We propose a novel analytical formula for the population gradient when the input follows the zero-mean Spherical Gaussian distribution. This formula leads to interesting critical points and convergence analyses. In particular, we show that critical points are not isolated from the hyperplane spanned by w \u0445 and form manifolds. For two ReLU cases, we characterize regions that do not contain critical points. For the convergence analysis, we show guaranteed convergence for a single ReLU case with random initialization, whose standard deviation is in the order O (1 / 0,000 d). For several ReLU cases, we show that an infinite change in the weighting initialization leads to different convergence."}], "references": [{"title": "Reconnaissance de la parole par reseaux connexionnistes", "author": ["Bottou", "L\u00e9on"], "venue": "In Proceedings of Neuro Nimes", "citeRegEx": "Bottou and L\u00e9on.,? \\Q1988\\E", "shortCiteRegEx": "Bottou and L\u00e9on.", "year": 1988}, {"title": "Symmetries in physics: philosophical reflections", "author": ["Brading", "Katherine", "Castellani", "Elena"], "venue": null, "citeRegEx": "Brading et al\\.,? \\Q2003\\E", "shortCiteRegEx": "Brading et al\\.", "year": 2003}, {"title": "The loss surfaces of multilayer networks", "author": ["Choromanska", "Anna", "Henaff", "Mikael", "Mathieu", "Michael", "Arous", "G\u00e9rard Ben", "LeCun", "Yann"], "venue": "In AISTATS,", "citeRegEx": "Choromanska et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Choromanska et al\\.", "year": 2015}, {"title": "Open problem: The landscape of the loss surfaces of multilayer networks", "author": ["Choromanska", "Anna", "LeCun", "Yann", "Arous", "G\u00e9rard Ben"], "venue": "In Proceedings of The 28th Conference on Learning Theory, COLT", "citeRegEx": "Choromanska et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Choromanska et al\\.", "year": 2015}, {"title": "Local minima and plateaus in hierarchical structures of multilayer perceptrons", "author": ["Fukumizu", "Kenji", "Amari", "Shun-ichi"], "venue": "Neural Networks,", "citeRegEx": "Fukumizu et al\\.,? \\Q2000\\E", "shortCiteRegEx": "Fukumizu et al\\.", "year": 2000}, {"title": "Understanding the difficulty of training deep feedforward neural networks", "author": ["Glorot", "Xavier", "Bengio", "Yoshua"], "venue": "In Aistats,", "citeRegEx": "Glorot et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Glorot et al\\.", "year": 2010}, {"title": "Delving deep into rectifiers: Surpassing humanlevel performance on imagenet classification", "author": ["He", "Kaiming", "Zhang", "Xiangyu", "Ren", "Shaoqing", "Sun", "Jian"], "venue": "In Proceedings of the IEEE International Conference on Computer Vision, pp", "citeRegEx": "He et al\\.,? \\Q2015\\E", "shortCiteRegEx": "He et al\\.", "year": 2015}, {"title": "Deep residual learning for image recognition", "author": ["He", "Kaiming", "Zhang", "Xiangyu", "Ren", "Shaoqing", "Sun", "Jian"], "venue": "Computer Vision anad Pattern Recognition", "citeRegEx": "He et al\\.,? \\Q2016\\E", "shortCiteRegEx": "He et al\\.", "year": 2016}, {"title": "Simplifying neural nets by discovering flat minima", "author": ["Hochreiter", "Sepp", "Schmidhuber", "J\u00fcrgen"], "venue": "Advances in Neural Information Processing Systems,", "citeRegEx": "Hochreiter et al\\.,? \\Q1995\\E", "shortCiteRegEx": "Hochreiter et al\\.", "year": 1995}, {"title": "Beating the perils of non-convexity: Guaranteed training of neural networks using tensor methods", "author": ["Janzamin", "Majid", "Sedghi", "Hanie", "Anandkumar", "Anima"], "venue": "CoRR abs/1506.08473,", "citeRegEx": "Janzamin et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Janzamin et al\\.", "year": 2015}, {"title": "Deep learning without poor local minima", "author": ["Kawaguchi", "Kenji"], "venue": "Advances in Neural Information Processing Systems,", "citeRegEx": "Kawaguchi and Kenji.,? \\Q2016\\E", "shortCiteRegEx": "Kawaguchi and Kenji.", "year": 2016}, {"title": "Imagenet classification with deep convolutional neural networks. In Advances in neural information processing", "author": ["Krizhevsky", "Alex", "Sutskever", "Ilya", "Hinton", "Geoffrey E"], "venue": null, "citeRegEx": "Krizhevsky et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Krizhevsky et al\\.", "year": 2012}, {"title": "Stability by lyapunov\u2019s second method with applications", "author": ["J.P. LaSalle", "S. Lefschetz"], "venue": "New York: Academic Press.,", "citeRegEx": "LaSalle and Lefschetz,? \\Q1961\\E", "shortCiteRegEx": "LaSalle and Lefschetz", "year": 1961}, {"title": "Efficient backprop", "author": ["LeCun", "Yann A", "Bottou", "L\u00e9on", "Orr", "Genevieve B", "M\u00fcller", "Klaus-Robert"], "venue": "In Neural networks: Tricks of the trade,", "citeRegEx": "LeCun et al\\.,? \\Q2012\\E", "shortCiteRegEx": "LeCun et al\\.", "year": 2012}, {"title": "The landscape of empirical risk for non-convex losses", "author": ["Mei", "Song", "Bai", "Yu", "Montanari", "Andrea"], "venue": "arXiv preprint arXiv:1607.06534,", "citeRegEx": "Mei et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Mei et al\\.", "year": 2016}, {"title": "Dynamics of on-line gradient descent learning for multilayer neural networks", "author": ["Saad", "David", "Solla", "Sara A"], "venue": "Advances in Neural Information Processing Systems,", "citeRegEx": "Saad et al\\.,? \\Q1996\\E", "shortCiteRegEx": "Saad et al\\.", "year": 1996}, {"title": "Exact solutions to the nonlinear dynamics of learning in deep linear neural networks", "author": ["Saxe", "Andrew M", "McClelland", "James L", "Ganguli", "Surya"], "venue": "arXiv preprint arXiv:1312.6120,", "citeRegEx": "Saxe et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Saxe et al\\.", "year": 2013}, {"title": "Very deep convolutional networks for large-scale image recognition", "author": ["Simonyan", "Karen", "Zisserman", "Andrew"], "venue": "International Conference on Learning Representations (ICLR),", "citeRegEx": "Simonyan et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Simonyan et al\\.", "year": 2015}, {"title": "No bad local minima: Data independent training error guarantees for multilayer neural networks", "author": ["Soudry", "Daniel", "Carmon", "Yair"], "venue": "arXiv preprint arXiv:1605.08361,", "citeRegEx": "Soudry et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Soudry et al\\.", "year": 2016}, {"title": "Sequence to sequence learning with neural networks. In Advances in neural information processing", "author": ["Sutskever", "Ilya", "Vinyals", "Oriol", "Le", "Quoc V"], "venue": null, "citeRegEx": "Sutskever et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Sutskever et al\\.", "year": 2014}, {"title": "Going deeper with convolutions", "author": ["Szegedy", "Christian", "Liu", "Wei", "Jia", "Yangqing", "Sermanet", "Pierre", "Reed", "Scott", "Anguelov", "Dragomir", "Erhan", "Dumitru", "Vanhoucke", "Vincent", "Rabinovich", "Andrew"], "venue": "In Computer Vision and Pattern Recognition (CVPR),", "citeRegEx": "Szegedy et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Szegedy et al\\.", "year": 2015}, {"title": "Electron-proton dynamics in deep learning", "author": ["Zhang", "Qiuyi", "Panigrahy", "Rina", "Sachdeva", "Sushant"], "venue": "arXiv preprint arXiv:1702.00458,", "citeRegEx": "Zhang et al\\.,? \\Q2017\\E", "shortCiteRegEx": "Zhang et al\\.", "year": 2017}], "referenceMentions": [{"referenceID": 7, "context": ", Computer Vision (He et al., 2016; Simonyan & Zisserman, 2015; Szegedy et al., 2015; Krizhevsky et al., 2012), Natural Language Processing (Sutskever et al.", "startOffset": 18, "endOffset": 110}, {"referenceID": 20, "context": ", Computer Vision (He et al., 2016; Simonyan & Zisserman, 2015; Szegedy et al., 2015; Krizhevsky et al., 2012), Natural Language Processing (Sutskever et al.", "startOffset": 18, "endOffset": 110}, {"referenceID": 11, "context": ", Computer Vision (He et al., 2016; Simonyan & Zisserman, 2015; Szegedy et al., 2015; Krizhevsky et al., 2012), Natural Language Processing (Sutskever et al.", "startOffset": 18, "endOffset": 110}, {"referenceID": 19, "context": ", 2012), Natural Language Processing (Sutskever et al., 2014) and Speech Recognition (Hinton et al.", "startOffset": 37, "endOffset": 61}, {"referenceID": 6, "context": "Using Lyapunov method (LaSalle & Lefschetz, 1961), for single ReLU case we prove that gradient descent converges to w\u2217 with at least (1 \u2212 )/2 probability, if initialized randomly with standard deviation upper-bounded by O( / \u221a d), verifying common initialization techniques (Bottou, 1988; Glorot & Bengio, 2010; He et al., 2015; LeCun et al., 2012),.", "startOffset": 274, "endOffset": 348}, {"referenceID": 13, "context": "Using Lyapunov method (LaSalle & Lefschetz, 1961), for single ReLU case we prove that gradient descent converges to w\u2217 with at least (1 \u2212 )/2 probability, if initialized randomly with standard deviation upper-bounded by O( / \u221a d), verifying common initialization techniques (Bottou, 1988; Glorot & Bengio, 2010; He et al., 2015; LeCun et al., 2012),.", "startOffset": 274, "endOffset": 348}, {"referenceID": 16, "context": "(Saxe et al., 2013) analyzes its dynamics of gradient descent and (Kawaguchi, 2016) shows every local minimum is global.", "startOffset": 0, "endOffset": 19}, {"referenceID": 14, "context": "(Mei et al., 2016) shows the global convergence for a single nonlinear node whose derivatives of activation \u03c3\u2032, \u03c3\u2032\u2032, \u03c3\u2032\u2032\u2032 are bounded and \u03c3\u2032 > 0.", "startOffset": 0, "endOffset": 18}, {"referenceID": 21, "context": "Recent paper (Zhang et al., 2017) analyzes a similar teacher-student setting on 2-layered network when the involved function is harmonic, but it is unclear how the conclusion is generalized to ReLU case.", "startOffset": 13, "endOffset": 33}, {"referenceID": 9, "context": "(Janzamin et al., 2015) gives guarantees for parameter recovery of a 2-layered network learnt with tensor decomposition.", "startOffset": 0, "endOffset": 23}, {"referenceID": 8, "context": "The theorem also explains why we have flat minima (Hochreiter et al., 1995; Dauphin et al., 2014) often occuring in practice.", "startOffset": 50, "endOffset": 97}, {"referenceID": 14, "context": "Note that the global convergence claim in (Mei et al., 2016) for l2 loss does not apply to ReLU, since it requires \u03c3\u2032(x) > 0.", "startOffset": 42, "endOffset": 60}, {"referenceID": 6, "context": "6 gives an explanation for common initialization techniques (Glorot & Bengio, 2010; He et al., 2015; LeCun et al., 2012; Bottou, 1988) that uses random variables with O(1/ \u221a d) standard deviation.", "startOffset": 60, "endOffset": 134}, {"referenceID": 13, "context": "6 gives an explanation for common initialization techniques (Glorot & Bengio, 2010; He et al., 2015; LeCun et al., 2012; Bottou, 1988) that uses random variables with O(1/ \u221a d) standard deviation.", "startOffset": 60, "endOffset": 134}], "year": 2017, "abstractText": "In this paper, we explore theoretical properties of training a two-layered ReLU network g(x;w) = \u2211K j=1 \u03c3(w T j x) with centered d-dimensional spherical Gaussian input x (\u03c3=ReLU). We train our network with gradient descent on w to mimic the output of a teacher network with the same architecture and fixed parameters w\u2217. We show that its population gradient has an analytical formula, leading to interesting theoretical analysis of critical points and convergence behaviors. First, we prove that critical points outside the hyperplane spanned by the teacher parameters (\u201cout-of-plane\u201c) are not isolated and form manifolds, and characterize inplane critical-point-free regions for two ReLU case. On the other hand, convergence to w\u2217 for one ReLU node is guaranteed with at least (1 \u2212 )/2 probability, if weights are initialized randomly with standard deviation upper-bounded by O( / \u221a d), consistent with empirical practice. For network with many ReLU nodes, we prove that an infinitesimal perturbation of weight initialization results in convergence towards w\u2217 (or its permutation), a phenomenon known as spontaneous symmetric-breaking (SSB) in physics. We assume no independence of ReLU activations. Simulation verifies our findings.", "creator": "LaTeX with hyperref package"}}}