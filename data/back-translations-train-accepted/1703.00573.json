{"id": "1703.00573", "review": {"conference": "ICML", "VERSION": "v1", "DATE_OF_SUBMISSION": "2-Mar-2017", "title": "Generalization and Equilibrium in Generative Adversarial Nets (GANs)", "abstract": "This paper makes progress on several open theoretical issues related to Generative Adversarial Networks. A definition is provided for what it means for the training to generalize, and it is shown that generalization is not guaranteed for the popular distances between distributions such as Jensen-Shannon or Wasserstein. We introduce a new metric called neural net distance for which generalization does occur. We also show that an approximate pure equilibrium in the 2-player game exists for a natural training objective (Wasserstein). Showing such a result has been an open problem (for any training objective).", "histories": [["v1", "Thu, 2 Mar 2017 01:14:03 GMT  (677kb,D)", "http://arxiv.org/abs/1703.00573v1", null], ["v2", "Fri, 3 Mar 2017 16:19:00 GMT  (677kb,D)", "http://arxiv.org/abs/1703.00573v2", null], ["v3", "Tue, 4 Apr 2017 00:41:13 GMT  (694kb,D)", "http://arxiv.org/abs/1703.00573v3", null], ["v4", "Sat, 17 Jun 2017 22:04:07 GMT  (2726kb,D)", "http://arxiv.org/abs/1703.00573v4", "To appear in ICML 2017; Major update of exposition in Section 1-3"], ["v5", "Tue, 1 Aug 2017 19:51:56 GMT  (2727kb,D)", "http://arxiv.org/abs/1703.00573v5", "This is an updated version of an ICML'17 paper with the same title. The main difference is that in the ICML'17 version the pure equilibrium result was only proved for Wasserstein GAN. In the current version the result applies to most reasonable training objectives. In particular, Theorem 4.3 now applies to both original GAN and Wasserstein GAN"]], "reviews": [], "SUBJECTS": "cs.LG cs.NE stat.ML", "authors": ["sanjeev arora", "rong ge 0001", "yingyu liang", "tengyu ma", "yi zhang"], "accepted": true, "id": "1703.00573"}, "pdf": {"name": "1703.00573.pdf", "metadata": {"source": "CRF", "title": "Generalization and Equilibrium in Generative Adversarial Nets (GANs)", "authors": ["Sanjeev Arora", "Rong Ge", "Yingyu Liang", "Tengyu Ma", "Yi Zhang"], "emails": ["arora@cs.princeton.edu", "rongge@cs.duke.edu", "yingyul@cs.princeton.edu", "tengyu@cs.princeton.edu", "yz7@cs.princeton.edu"], "sections": [{"heading": null, "text": "Finally, the above theoretical ideas lead us to propose a new training protocol, mix + gan, which can be combined with any existing method. We present experiments that show that it stabilizes and improves some existing methods."}, {"heading": "1 Introduction", "text": "In fact, it is not that it is a real scenario in which we address a general deficiency of the original idea that leads to stable education and realistic generative models in practice, but many fundamental questions remain unresolved, as we are now discussing. We remember that the basic scenario, that we want to form a generative deep net whose input is a standard Gaussian, and whose output is an example of a certain distribution D to < d We want samples from D that are closely aligned with the real distribution Dreal (which could be real images represented with raw pixels)."}, {"heading": "1.1 This paper", "text": "The reason for this is that these formalities include an ideal discriminator that can have exponentially high UK dimensions (or any other suitable measure of complexity), so we are introducing a new measurement of distributions, the net neural distance, for which we show generalizations are happening. To investigate the existence of equilibriums, in Section 5 we turn to infinite mixtures of generators of deep meshes, which are much more expressive than a single generator network: e.g. a standard result in Bayesian nonparametries says that any probability density can be moved near targets by an infinite mixture of gasses. [Ghosh et al., 2003]"}, {"heading": "2 Preliminaries", "text": "In section 3, we use m for the number of samples. (...) We assume that the number of possible meshes is abstracted as a class of functions. (...) We assume that hyperparameters - number of layers, number of nodes, convolution structure, etc. - have been prefixed, as is usually the case. (...) Without loss of generality, we assume that U is a subset of the d dimensional unit. (...) We assume that hyperparameters - number of layers, number of nodes, constellation structure, etc. - have designated the class of generators and {Dv}."}, {"heading": "3 Distance Metric and Generalization", "text": "This interpretation of the GAN target in terms of minimizing metrics such as JS divergence or Waterstone is actually standard, but it is based on two key assumptions: (i) the network has enough capacity to make the ideal distinction and (ii) we have enough examples to estimate the expectations in objectives (1) or (2). Both assumptions are not met in practice, and we now show that this concerns the generalization. (i) The training actually uses only finite number of samples of Dreal and Gu (Dh): let h1, hm \"Dh\" be the inputs of the generator, and we leave x2,... xm \"Dreal\" the distribution 3. \"We use D\" real \"and Gu\" h \"to denote the empirical distribution.\""}, {"heading": "3.1 New distance metric with generalization bounds", "text": "We now define a new distance measurement quantity that captures the actual optimization and which leads to a generalization that then leads to a generalization. (Dv) + E x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x. (Dv) + E x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x (Dv)."}, {"heading": "4 Expressive power and existence of equilibrium", "text": "Section 3 clarified the concept of generalization for GANs: namely, the neural net divergence between the generated distribution D and Dreal on the empirical samples closely traces the divergence to the full distribution (i.e., invisible samples), but this does not explain why the generator usually \"wins\" in practice, so that the discriminator cannot perform much better than the random guess at the end. In other words, it was pure luck that so many real distributions of DREAL in neuronal net distance to a distribution generated by a relatively compact neuronal net than is necessary? This section indicates that no luck is needed. The explanation begins with a thought experiment. Imagine that a much more powerful generator, namely an infinite mixture of deep nets, each of the size n. as long as the deep net class is able to generic simple Gaussisians, ansisisiansiansiansiansiansiansisisiansisisiansisisiansisisiansisiansisiansisiansisisiansisisiansisiansisiansisisisiansisiansisisisiansisisisisiansisisisiansisisiansisisisiansiansisisisisiansisiansisisisisiansisisiansisisisiansisisisiansisiansisisisiansiansisisiansisisisisiansisisisiansisisisisiansisiansisisisiansiansiansisisisiansisisisisisisiansisisisisisiansiansisisisiansisisisiansisisisiansisisisisisiansisisisiansisisisisisisisiansisisisisisiansisisisiansiansisiansisisiansisisisisisisisisisisisisisisiansisisisisisisisiansisisisisiansisisisisisisisisisisisisisisisiansisisisisisisisisisisisisisisisisisisisisisisisisi"}, {"heading": "4.1 General f : Mixed Equilibrium", "text": "For a general measuring function f we can only indicate the existence of a mixed balance, where we consider discrimination and the generator as finite mixtures of deep net.For a class of generators {Gu, u, u, u, u, u, u, u, u, u, u, u, u, u, u, u, u, u, u, u, u, u, u, u, u, u, u, u, u, u, u, u, u, u, u, u, u, u, u, u, u, u, u, u, u, u, u, u, u, u, u, u, u, u, u, u, u, u, u, u, u, u, u, u, u, u, u, u, u, u, u, u, u, u, u, u, u, u, u, u, u, u, u, u, u, u, u, u, u, u, u, u, u, u, u, u, u, u, u, u, u, u, u, u, u, u, u, u, u, u, u, u, u, u, u, u, u, u, u, u, u, u, u, u, u, u, u, u, u, u, u, u, u, u, u"}, {"heading": "5 Expressive power and existence of equilibrium", "text": "Section 3 clarified the concept of generalization for GANs: namely, the neural net divergence between the generated distribution D and Dreal on the empirical samples closely traces the divergence to the full distribution (i.e., invisible samples), but this does not explain why the generator usually \"wins\" in practice, so that the discriminator cannot perform much better than the random guess at the end. In other words, it was pure luck that so many real distributions of DREAL in neuronal net distance to a distribution generated by a relatively compact neuronal net than is necessary? This section indicates that no luck is needed. The explanation begins with a thought experiment. Imagine that a much more powerful generator, namely an infinite mixture of deep nets, each of the size n. as long as the deep net class is able to generic simple Gaussisians, ansisisiansiansiansiansiansiansisisiansisisiansisisiansisisiansisiansisiansisiansisisiansisisiansisiansisiansisisisiansisiansisisisiansisisisisiansisisisiansisisiansisisisiansiansisisisisiansisiansisisisisiansisisiansisisisiansisisisiansisiansisisisiansiansisisiansisisisisiansisisisiansisisisisiansisiansisisisiansiansiansisisisiansisisisisisisiansisisisisisiansiansisisisiansisisisiansisisisiansisisisisisiansisisisiansisisisisisisisiansisisisisisiansisisisiansiansisiansisisiansisisisisisisisisisisisisisisiansisisisisisisisiansisisisisiansisisisisisisisisisisisisisisisiansisisisisisisisisisisisisisisisisisisisisisisisisi"}, {"heading": "5.1 General f : Mixed Equilibrium", "text": "For a general measuring function f, we can only indicate the existence of a mixed balance in which we favor discrimination and the generator as finite mixtures of deep networks. For a class of generators {Gu, u, u, u, u, u, u, u, u, u, u, u, u, u, u, u, u, u, u, u, u, u, u, u, u, u, u, u, u, u, u, u, u, u, u, u, u, u, u, u, u, u, u, u, u, u, u, u, u, u, u, u, u, u, u, u, u, u, u, u, u, u, u, u, u, u, u, u, u, u, u, u, u, u, u, u, u, u, u, u, u, u, u, u, u, u, u, u, u, u, u, u, u, u, u, u, u, u, u, u, u, u, u, u, u, u, u, u, u, u, u, u, u, u, u, u, u, u, u, u, u, u, u, u, u, u, u, u, u, u, u, u, u, u, u, u, u, u, u, u, u, u, u, u, u, u, u, u, u, u, u, u, u, u, u, u, u, u, u, u, u, u, u, u, u, u, u, u, u, u, u, u, u, u, u, u, u, u, u, u, u, u, u, u, u, u, u, u, u, u, u, u, u, u, u, u, u, u, u, u, u, u, u, u, u, u, u, u, u, u, u, u, u, u, u, u, u, u, u, u, u, u, u, u, u, u, u, u, u, u, u, u, u,"}, {"heading": "6 MIX+GANs", "text": "Theorem 5.2 and Theorem 5.3 show that using a mixture of (not too many) generators and discriminators guarantees the existence of an approximate equilibrium, suggesting that using a mixture can lead to more stable training. Of course, it is impractical to use very large mixtures, so we suggest using a mixture of T components, where T is as large as the size of the GPU memory allows (usually T \u2264 5), namely a mixture of T generators {Gui, i [T] and T discriminators {Dvi, i [T]}), which share the same network architecture but have their own traceable parameters. Of course, maintaining a mixture means maintaining a wui weight for the generator Gui, which corresponds to the probability of selecting the output of Gui. These weights are also updated via backpropagation."}, {"heading": "7 Experiments", "text": "In this section, we will first examine the qualitative advantages of mix + gan in image generation: MNIST dataset [LeCun et al., 1998] of handwritten digits and CeleA [Liu et al., 2015] dataset of human faces. Then, for a more quantitative evaluation, we will use the CIFAR-10 dataset [Krizhevsky and Hinton, 2009] and the Inception Score introduced in Salimans et al. [2016]. MNIST contains 60,000 labeled 28 x 28-sized images of handwritten digits, CeleA contains over 200K 108 x 108-sized images of human faces (we cut out the center 64 x 64 pixels for our experiments), and CIFAR-10 has 60,000 labeled 32 x 32-sized RGB nature images that fall into 10 categories. To support the point that this technique works inhouse, no extensive hyperparameter search or tuning search has been performed (and the code is being released for further work)."}, {"heading": "7.1 Qualitative Results", "text": "The DCGAN architecture [Radford et al., 2016] uses deep revolutionary networks as generators and discriminators. We trained mix + dcgan on MNIST and CeleA using the author code as a black box and compared visual qualities of generated images with DCGAN. The results on MNIST are shown in Figure 2. In this experiment, the base model DCGAN consists of a pair of generators and a discriminator consisting of 5-layer deconvoluitonal neural networks conditioned on image captions. Our MIX + DCGAN model consists of a mixture of such DCGANs, so it has 3 generators and 3 discriminators. We observe that mix + dcgan produces slightly cleaner digits than dcgan (note the blur in the latter). Interestingly, each component of our mixture learns a slightly different style of strokes. Figure 3 shows the results on CeleA models are different from the DCGAN models, although the differences between the two DCGAN models are not the same for the MIST architecture."}, {"heading": "7.2 Quantitative Results", "text": "At first glance, the comparison between dcgan and mix + dcgan seems unfair, since the latter consumes as much capacity as 5 dcgan's, with a corresponding penalty in the run-time per epoch. On the other hand, in deep net training, increased capacity is not always a good idea, since the training can use it poorly (e.g. to exceed it). To construct MIX + DCGAN, we build on the top of the DCGAN, which was trained with losses that of Huang et al. [2016], namely opposite loss, entropy loss and conditional loss, which is so far the best DCGAN without improved training techniques. The same hyperparameters are used for a fair comparison. See Huang et al. [2016] for the MIX + Waterstone GAN-GAN comparison is identical."}, {"heading": "8 Conclusions", "text": "The concept of generalization for GANs has been clarified by the introduction of a new concept of distance between distributions, the net neural distance. (While popular distances such as Waterstone and JS may not be generalized.) Assuming that the visual cortex is also a deep net, generalization of this measurement is of course sufficient to make the final samples appear realistic to humans. Also, the paper made progress on other unresolved questions about GANs by showing that there is a pure approximate equilibrium for a particular natural training target (Waterstone) and that the generator wins the game. It is not an assumption about distribution of spin. Assuming that a pure equilibrium may not exist for all goals, in practice we recommend our Mix + gan protocol with a small mix of discriminators and generators. Our experiments show that it improves the quality of several existing GAN training methods."}, {"heading": "A Omitted Proofs", "text": "In this section we provide detailed evidence for the theorems in the main documentation."}], "references": [{"title": "Learning to protect communications with adversarial neural cryptography", "author": ["Mart\u0301\u0131n Abadi", "David G Andersen"], "venue": "arXiv preprint arXiv:1610.06918,", "citeRegEx": "Abadi and Andersen.,? \\Q2016\\E", "shortCiteRegEx": "Abadi and Andersen.", "year": 2016}, {"title": "Bayesian nonparametrics", "author": ["Jayanta K Ghosh", "RVJK Ghosh", "RV Ramamoorthi"], "venue": "Technical report,", "citeRegEx": "Ghosh et al\\.,? \\Q2003\\E", "shortCiteRegEx": "Ghosh et al\\.", "year": 2003}, {"title": "Generative adversarial nets", "author": ["Ian Goodfellow", "Jean Pouget-Abadie", "Mehdi Mirza", "Bing Xu", "David Warde-Farley", "Sherjil Ozair", "Aaron Courville", "Yoshua Bengio"], "venue": "In Advances in neural information processing systems,", "citeRegEx": "Goodfellow et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Goodfellow et al\\.", "year": 2014}, {"title": "Stacked generative adversarial networks", "author": ["xun Huang", "Yixuan Li", "Omid Poursaeed", "John Hopcroft", "Serge Belongie"], "venue": "Technical report,", "citeRegEx": "Huang et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Huang et al\\.", "year": 2016}, {"title": "Adam: A method for stochastic optimization", "author": ["Diederik Kingma", "Jimmy Ba"], "venue": "In International Conference on Learning Representations,", "citeRegEx": "Kingma and Ba.,? \\Q2015\\E", "shortCiteRegEx": "Kingma and Ba.", "year": 2015}, {"title": "Exponentiated gradient versus gradient descent for linear predictors", "author": ["Jyrki Kivinen", "Manfred K Warmuth"], "venue": "Information and Computation,", "citeRegEx": "Kivinen and Warmuth.,? \\Q1997\\E", "shortCiteRegEx": "Kivinen and Warmuth.", "year": 1997}, {"title": "Learning multiple layers of features from tiny images", "author": ["Alex Krizhevsky", "Geoffrey Hinton"], "venue": "Technical report,", "citeRegEx": "Krizhevsky and Hinton.,? \\Q2009\\E", "shortCiteRegEx": "Krizhevsky and Hinton.", "year": 2009}, {"title": "The mnist database of handwritten digits", "author": ["Yann LeCun", "Corinna Cortes", "Christopher JC Burges"], "venue": null, "citeRegEx": "LeCun et al\\.,? \\Q1998\\E", "shortCiteRegEx": "LeCun et al\\.", "year": 1998}, {"title": "Playing large games using simple strategies", "author": ["Richard J Lipton", "Evangelos Markakis", "Aranyak Mehta"], "venue": "In Proceedings of the 4th ACM conference on Electronic commerce,", "citeRegEx": "Lipton et al\\.,? \\Q2003\\E", "shortCiteRegEx": "Lipton et al\\.", "year": 2003}, {"title": "Deep learning face attributes in the wild", "author": ["Ziwei Liu", "Ping Luo", "Xiaogang Wang", "Xiaoou Tang"], "venue": "In Proceedings of the IEEE International Conference on Computer Vision,", "citeRegEx": "Liu et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Liu et al\\.", "year": 2015}, {"title": "Unsupervised representation learning with deep convolutional generative adversarial networks", "author": ["Alec Radford", "Luke Metz", "Soumith Chintala"], "venue": "In International Conference on Learning Representations,", "citeRegEx": "Radford et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Radford et al\\.", "year": 2016}, {"title": "Improved techniques for training gans", "author": ["Tim Salimans", "Ian Goodfellow", "Wojciech Zaremba", "Vicki Cheung", "Alec Radford", "Xi Chen"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Salimans et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Salimans et al\\.", "year": 2016}, {"title": "Adagan: Boosting generative models", "author": ["Ilya Tolstikhin", "Sylvain Gelly", "Olivier Bousquet", "Carl-Johann Simon-Gabriel", "Bernhard Sch\u00f6lkopf"], "venue": "arXiv preprint arXiv:1701.02386,", "citeRegEx": "Tolstikhin et al\\.,? \\Q2017\\E", "shortCiteRegEx": "Tolstikhin et al\\.", "year": 2017}, {"title": "Learning to draw samples: With application to amortized mle for generative adversarial learning", "author": ["Dilin Wang", "Qiang Liu"], "venue": "Technical report,", "citeRegEx": "Wang and Liu.,? \\Q2016\\E", "shortCiteRegEx": "Wang and Liu.", "year": 2016}], "referenceMentions": [{"referenceID": 2, "context": "Generative Adversarial Networks (GANs) [Goodfellow et al., 2014] have become the dominant method for fitting generative models to complicated real-life data, and even found unusual uses such as designing good cryptographic primitives [Abadi and Andersen, 2016].", "startOffset": 39, "endOffset": 64}, {"referenceID": 0, "context": ", 2014] have become the dominant method for fitting generative models to complicated real-life data, and even found unusual uses such as designing good cryptographic primitives [Abadi and Andersen, 2016].", "startOffset": 177, "endOffset": 203}, {"referenceID": 0, "context": ", 2014] have become the dominant method for fitting generative models to complicated real-life data, and even found unusual uses such as designing good cryptographic primitives [Abadi and Andersen, 2016]. Various novel architectures and training objectives were introduced to address perceived shortcomings of the original idea, leading to more stable training and more realistic generative models in practice. But many basic issues remain unresolved, as we now discuss. Let\u2019s recall that the basic scenario is that we wish to train a generator deep net whose input is a standard Gaussian, and whose output is a sample from some distribution D on <d. We wish the samples from D to closely resemble those drawn from some real-life distribution Dreal (which could be, say, real-life images represented using raw pixels). Towards this end, a discriminator deep net is trained alongside the generator net, and it is trained to maximise its ability to distinguish between samples from Dreal and D. So long as the discriminator is successful at this task with nonzero probability, its success can be used to generate a feedback (using backpropagation) to the generator, thus improving its distribution D. This basic iterative framework has been tried with many training objectives; see Section 2. Recently Arjovsky et al. [2017] proposed another variant called Wasserstein GAN that appears to lead to more stable training.", "startOffset": 178, "endOffset": 1323}, {"referenceID": 1, "context": ", a standard result in bayesian nonparametrics says that every probability density is closely approximable by an infinite mixture of Gaussians [Ghosh et al., 2003].", "startOffset": 143, "endOffset": 163}, {"referenceID": 2, "context": "The standard GAN training [Goodfellow et al., 2014] consists of training u, v so as to optimize an objective such as:", "startOffset": 26, "endOffset": 51}, {"referenceID": 8, "context": "We show that (as is folklore in game theory[Lipton et al., 2003]) that we can approximate this min-max solution with mixture of finitely many generators and discriminators.", "startOffset": 43, "endOffset": 64}, {"referenceID": 8, "context": "We show that (as is folklore in game theory[Lipton et al., 2003]) that we can approximate this min-max solution with mixture of finitely many generators and discriminators.", "startOffset": 43, "endOffset": 64}, {"referenceID": 5, "context": "We use exponentiated gradient[Kivinen and Warmuth, 1997]: store the log-probabilities {\u03b1ui , i \u2208 [T ]}, and then obtain the weights by applying soft-max function on them:", "startOffset": 29, "endOffset": 56}, {"referenceID": 12, "context": "AdaGAN[Tolstikhin et al., 2017] uses ideas similar to boosting and maintains weights on training examples.", "startOffset": 6, "endOffset": 31}, {"referenceID": 4, "context": "In our experiments we alternatively update generators\u2019 and discriminators\u2019 parameters as well as their corresponding log-probabilities using ADAM [Kingma and Ba, 2015], with learning rate lr = 0.", "startOffset": 146, "endOffset": 167}, {"referenceID": 7, "context": "In this section, we first explore the qualitative benefits of mix+gan on image generation tasks: MNIST dataset [LeCun et al., 1998] of hand-written digits and the CeleA [Liu et al.", "startOffset": 111, "endOffset": 131}, {"referenceID": 9, "context": ", 1998] of hand-written digits and the CeleA [Liu et al., 2015] dataset of human faces.", "startOffset": 45, "endOffset": 63}, {"referenceID": 6, "context": "Then for more quantitative evaluation we use the CIFAR-10 dataset [Krizhevsky and Hinton, 2009] and use the Inception Score introduced in Salimans et al.", "startOffset": 66, "endOffset": 95}, {"referenceID": 6, "context": "Then for more quantitative evaluation we use the CIFAR-10 dataset [Krizhevsky and Hinton, 2009] and use the Inception Score introduced in Salimans et al. [2016]. MNIST contains 60,000 labeled 28\u00d728-sized images of hand-written digits, CeleA contains over 200K 108\u00d7108-sized images of human faces (we crop the center 64\u00d7 64 pixels for our experiments), and CIFAR-10 has 60,000 labeled 32\u00d7 32-sized RGB natural images which fall into 10 categories.", "startOffset": 67, "endOffset": 161}, {"referenceID": 10, "context": "The DCGAN architecture [Radford et al., 2016] uses deep convolutional nets as generators and discriminators.", "startOffset": 23, "endOffset": 45}, {"referenceID": 11, "context": "Now we turn to quantitative measurement using Inception Score [Salimans et al., 2016].", "startOffset": 62, "endOffset": 85}, {"referenceID": 3, "context": "To construct MIX+DCGAN, we build on top of the DCGAN trained with losses proposed by Huang et al. [2016], namely adversarial loss, entropy loss and conditional loss, which is the best DCGAN so far without improved training techniques.", "startOffset": 85, "endOffset": 105}, {"referenceID": 3, "context": "To construct MIX+DCGAN, we build on top of the DCGAN trained with losses proposed by Huang et al. [2016], namely adversarial loss, entropy loss and conditional loss, which is the best DCGAN so far without improved training techniques. The same hyper-parameters are used for fair comparison. See Huang et al. [2016] for more details.", "startOffset": 85, "endOffset": 315}, {"referenceID": 3, "context": "To construct MIX+DCGAN, we build on top of the DCGAN trained with losses proposed by Huang et al. [2016], namely adversarial loss, entropy loss and conditional loss, which is the best DCGAN so far without improved training techniques. The same hyper-parameters are used for fair comparison. See Huang et al. [2016] for more details. Similarly, for the MIX+WassersteinGAN, the base GAN is identical to that proposed by Arjovsky et al. [2017] using their hyper-parameter scheme.", "startOffset": 85, "endOffset": 441}, {"referenceID": 13, "context": "58 SteinGAN [Wang and Liu, 2016] 6.", "startOffset": 12, "endOffset": 32}, {"referenceID": 11, "context": "35 Improved GAN [Salimans et al., 2016] 8.", "startOffset": 16, "endOffset": 39}, {"referenceID": 11, "context": "Method Score DCGAN (as reported in Wang and Liu [2016]) 6.", "startOffset": 35, "endOffset": 55}, {"referenceID": 3, "context": "07 DCGAN (best variant in Huang et al. [2016]) 7.", "startOffset": 26, "endOffset": 46}], "year": 2017, "abstractText": "This paper makes progress on several open theoretical issues related to Generative Adversarial Networks. A definition is provided for what it means for the training to generalize, and it is shown that generalization is not guaranteed for the popular distances between distributions such as Jensen-Shannon or Wasserstein. We introduce a new metric called neural net distance for which generalization does occur. We also show that an approximate pure equilibrium in the 2-player game exists for a natural training objective (Wasserstein). Showing such a result has been an open problem (for any training objective). Finally, the above theoretical ideas lead us to propose a new training protocol, mix+gan, which can be combined with any existing method. We present experiments showing that it stabilizes and improves some existing methods.", "creator": "LaTeX with hyperref package"}}}