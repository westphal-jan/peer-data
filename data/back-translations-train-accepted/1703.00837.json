{"id": "1703.00837", "review": {"conference": "ICML", "VERSION": "v1", "DATE_OF_SUBMISSION": "2-Mar-2017", "title": "Meta Networks", "abstract": "Deep neural networks have been successfully applied in applications with a large amount of labeled data. However, there are major drawbacks of the neural networks that are related to rapid generalization with small data and continual learning of new concepts without forgetting. We present a novel meta learning method, Meta Networks (MetaNet), that acquires a meta-level knowledge across tasks and shifts its inductive bias via fast parameterization for the rapid generalization. When tested on the standard one-shot learning benchmarks, our MetaNet models achieved near human-level accuracy. We demonstrated several appealing properties of MetaNet relating to generalization and continual learning.", "histories": [["v1", "Thu, 2 Mar 2017 15:52:55 GMT  (250kb,D)", "http://arxiv.org/abs/1703.00837v1", "initial submission"], ["v2", "Thu, 8 Jun 2017 16:12:40 GMT  (254kb,D)", "http://arxiv.org/abs/1703.00837v2", "Accepted at ICML 2017 - rewrote: the main section; added: MetaNet algorithmic procedure; performed: Mini-ImageNet evaluation"]], "COMMENTS": "initial submission", "reviews": [], "SUBJECTS": "cs.LG stat.ML", "authors": ["tsendsuren munkhdalai", "hong yu"], "accepted": true, "id": "1703.00837"}, "pdf": {"name": "1703.00837.pdf", "metadata": {"source": "META", "title": "Meta Networks", "authors": ["Tsendsuren Munkhdalai", "Hong Yu"], "emails": ["suren.munkhdalai@umassmed.edu>."], "sections": [{"heading": "1. Introduction", "text": "This year, it is only a matter of time before agreement is reached."}, {"heading": "2. Related Work", "text": "Rapid learning and generalization refers to a one-sided learning scenario in which a learner is introduced to a sequence of tasks, each with a single or a few named examples from many classes. A major challenge in this context is that the classes or concepts vary across the tasks. In this context, a one-sided learning process is set in motion by generative models and metric learning methods. Remarkable success is achieved by a likely programming approach (Lake et al., 2015). They have used specific knowledge of the way the characters of different alphabets are produced."}, {"heading": "3. Meta Networks", "text": "The model consists of two main learning modules (Figure 1): The meta-learner, who works across tasks, is responsible for the fast weight determination. These fast weights shift the inductive bias of a neural network and are integrated into the basic learner and the meta-learner. In particular, the basic learner, parameterized by the fast weights, is now able to generalize well for the new task. We propose a novel method of layer magnification to integrate the standard slow weights and the task or the example of specific quick weights into a neural network."}, {"heading": "3.1. Base Learner", "text": "The basic learner, referred to as b, is a function or neural network that maps an input example x (i.e. image) to an output y (i.e. label). However, unlike the standard neural network, b is parameterized by slow weightW and exemplary fast weightW *. Slow weightW * values are updated during the training by a learning algorithm, while fast weightW values are parameterized for each input.By using some kind of meta information, the basic learner should be able to explain his setting in the current task area so that the meta learner can generate the fast weights.We obtain such meta information in the form of weight by first performing a forward step and then spreading backwards as follows: Li = lossb (W, xi), yi) (1)."}, {"heading": "3.2. Meta Learner", "text": "The meta-learners process the meta-information together with the task support and generate the fast Q-values. (Above all, it transforms the loss gradient (1). (1) The fast acquisition of the data by a dynamic key-embedding function u, where the embedding function u is a slow weighting. (2) The memory M is with the task-dependent representations R = {ri} Ni = 1 of the support set {xi} Ni = 1, obtained by a dynamic key embedding function u. The embedding function u is a slow weight and task level fast weights Q: ri = u (Q, Q), xi) where the parameters Q and Q (5) are integrated."}, {"heading": "3.3. Layer Augmentation", "text": "An example of the layer augmentation approach applied to a MLP with a single hidden layer is shown in Figure 2. Input into each layer is first transformed by slow and fast weights and then guided through a nonlinearity (i.e. ReLU), resulting in two separate activation vectors. Finally, the activation vectors are aggregated by an elementary vector addition. For the last Softmax layer, we aggregate two transformed inputs and then normalize them for the classification output. Overall, the building block of layer augmentation is similar to that of the ResNet architecture (He et al., 2016). However, ResNet exhibits an identity transformation of a previous activation (i.e. short section connection) and a nonlinearity after elemental additive aggregation."}, {"heading": "4. Results", "text": "We performed unique classification experiments with two different image datasets: Omniglot and MNIST. The Omniglot dataset consists of images from 1623 classes with only 20 images per class, from 50 different alphabets (Lake et al., 2015). It also has a standard split of 30 training and 20 evaluation alphabets. Subsequently (Santoro et al., 2016) we expanded the dataset by 90, 180 and 270 degrees rotations. The images are resized to 28 x 28 pixels for efficiency of calculation."}, {"heading": "4.1. Training Details", "text": "In order to train and test MetaNet on One-Shot-Learning, we have adapted the training method introduced by Vinyals et al. (2016). First, we divide the Omniglot data into training and test sets consisting of two disjunct classes. Afterwards, we formulate a series of tasks from the training set. Each task has a support set of N images and N different classes, which results in an N-Way One-Shot classification problem. In addition to the support set, we also include L number of labeled examples in each task to update the parameters during the training. For testing, we follow the same procedure to form a series of test tasks from the disjunctive classes. Now, however, MetaNet assigns class names only on the basis of the labeled support set to each test task. For the one-shot benchmarks specified in the following sections, we use a CNN as the base learner b. This CNN has 5 revolutionary M-layers of Replicity, each of which is followed by a single filter x of 3."}, {"heading": "4.2. One-shot Learning Test", "text": "In this section we will report on three different sets of benchmark experiments: Omniglot previous split, MNIST as out-of-domain data, and Omniglot standard split."}, {"heading": "4.2.1. OMNIGLOT PREVIOUS SPLIT", "text": "After the previous setup Vinyals et al. (2016), we divided the Omniglot classes into 1200 and 423 classes for training and testing purposes. To show how fast parameterization influences network dynamics, we also examined three variants of MetaNet as an ablation experiment. In Table 1, we compared the performance of our models with all published models (as baseline), but the first group of methods are the previously published models. The next group are MetaNet variations. MetaNet is the main architecture described in Section 3. MetaNet - is a variant without fast weighting at task level in the embedding function u, while MetaNet + has additional task-level weights for the base learner in addition to W \u00b2. Our MetaNet model improves the previous best results by 0.5% to 2% accuracy. As the number of classes increases (by decreasing from 5 to net level, the net score is relatively 15%), it is not an improvement on the overall performance."}, {"heading": "4.2.2. MNIST AS OUT-OF-DOMAIN DATA", "text": "In Figure 3, we plotted the results of this experiment: MetaNet achieved an accuracy of 71.6%, which was 0.6% and 3.2% lower than the other variants with fast weights, which is not surprising since MetaNet is unable to adjust its parameters to MNIST images without dynamic embedding; the standard MetaNet model reached 74.8%, while MetaNet reached + 72.3%, while Matching Net (Vinyals et al., 2016) in this setup had an accuracy of 72.0%. Again, we did not see any improvement with the MetaNet + model; the standard MetaNet model performed best by effectively adjusting its bias to the new data distribution."}, {"heading": "4.2.3. OMNIGLOT STANDARD SPLIT", "text": "Omniglot data comes with a standard split of 30 training alphabets with 964 classes and 20 evaluation alphabets with 659 classes. In this setup, we trained and tested only the standard MetaNet model. To best comply with the evaluation protocol of Lake et al. (2015), we created 400 tasks (studies) from the evaluation classes to test the model. In Table 2, we listed the MetaNet results along with the previous models and human performance. Our MetaNet slightly exceeded human performance, but underperformed the likely programming approach. However, the performance gap between these three top baselines is rather small. In addition, the likely programming scores slightly better than MetaNet, but our model is not based on additional knowledge about how characters and strokes are composed. Comparing the results on two Omniglot splits in Tables 1 and 2, MetaNet performs slightly worse than the standard."}, {"heading": "4.3. Generalization Test", "text": "We conducted a series of experiments to test the generalization of MetaNet from several aspects: the first experiment is to test whether a MetaNet model trained for a simple task can be generalized to another K-way task (where N 6 = K) without actually practicing the second task; the second experiment is to test whether a Meta-learner trained for the rapid parameterization of a base-learner train could parameterize other base learners during the evaluation; and the last group of experiments we conducted as part of the MetaNet generalization test is to show that MetaNet supports continuous learning at the meta level."}, {"heading": "4.3.1. N-WAY TRAINING AND K-WAY TESTING", "text": "We first trained MetaNet on N-Way One-Shot Classification Task and then tested it on K-Way One-Shot Task. The number of training and test classes is varied (i.e. N 6 = K). To accomplish this, we inserted a Softmax layer into the base learner and then expanded it with the fast weights generated by the meta-learner. If the meta-learner is ingenious enough, he should be able to parameterise the new Softmax layer on the fly. The new shift weights remained fixed as no parameter update was made for that layer. K-Way test tasks were formed from the 423 invisible classes in the test. MetaNet models were trained on one of 5, 15 and 20-way one-shot tasks and evaluated on the rest."}, {"heading": "4.3.2. RAPID PARAMETERIZATION OF FIXED WEIGHT BASE LEARNER", "text": "The fast weights are generated by the meta-learner, who is trained to parameterise the old base learner and extend the fixed slow weights.We tested one small CNN and one large CNN for the base learner. The small CNN has 32 filters and the large CNN has over 128 filters. These CNNs consist of the same number of levels as the base learner (described in Section 4.1).Figure 4 compares the test performance of these CNNs. The optimized base learner (Target CNN) within the model performed better than the specified CNNs. The performance difference between these models is large in previous training processes. However, as the meta learner sees more one-sided learning attempts, their test accuracy must match with the base learners. These results show that MetaNet effectively learns to parameterise a neural network with fixed weights."}, {"heading": "4.3.3. META-LEVEL CONTINUAL LEARNING", "text": "MetaNet works in two areas: input problem space and meta (gradient) space. If the meta-space is problem-free, MetaNet meta-level should support continuous learning or lifelong learning, an experiment designed to investigate this in case of loss gradients.After previous work on catastrophic forgetting in neural networks (Srivastava et al., 2013; Goodfellow et al., 2014; Kirkpatrick et al., 2016), we have formulated two problems sequentially."}, {"heading": "400 2800 5200 7600", "text": "The model on the Omniglot sets and then we switched over and continued the training on the MNIST data. After training on a number of MNIST one-shot tasks, we re-evaluated the model on the same Omniglot test set and compared the two performances. A decreasing performance indicates that the meta weights Z and G of the neural networks m and mLSTM are prone to catastrophic forgetfulness, and MetaNet supports reverse learning. We shared separate parameters for the weights W and Q when we changed the problems, so the only meta weights were updated. We used three layers of MLPs with 64 hidden units as an embedding function and the base learner. The MNIST image and the classes were augmented."}, {"heading": "5. Discussion and Future Work", "text": "It is as if it is a reactionary, but not a reactionary, reactionary-eaJrh, in which it is able to unite."}], "references": [{"title": "Learning to learn by gradient descent by gradient descent", "author": ["Andrychowicz", "Marcin", "Denil", "Misha", "Gomez", "Sergio", "Hoffman", "Matthew W", "Pfau", "David", "Schaul", "Tom", "de Freitas", "Nando"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Andrychowicz et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Andrychowicz et al\\.", "year": 2016}, {"title": "Using fast weights to attend to the recent past", "author": ["Ba", "Jimmy", "Hinton", "Geoffrey E", "Mnih", "Volodymyr", "Leibo", "Joel Z", "Ionescu", "Catalin"], "venue": "In Advances In Neural Information Processing Systems,", "citeRegEx": "Ba et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Ba et al\\.", "year": 2016}, {"title": "Learning a synaptic learning", "author": ["Bengio", "Yoshua", "Samy", "Cloutier", "Jocelyn"], "venue": "rule. Universite\u0301 de Montre\u0301al, De\u0301partement d\u2019informatique et de recherche ope\u0301rationnelle,", "citeRegEx": "Bengio et al\\.,? \\Q1990\\E", "shortCiteRegEx": "Bengio et al\\.", "year": 1990}, {"title": "Learning a similarity metric discriminatively, with application to face verification", "author": ["Chopra", "Sumit", "Hadsell", "Raia", "LeCun", "Yann"], "venue": "In Computer Vision and Pattern Recognition,", "citeRegEx": "Chopra et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Chopra et al\\.", "year": 2005}, {"title": "Dynamic filter networks", "author": ["De Brabandere", "Bert", "Jia", "Xu", "Tuytelaars", "Tinne", "Van Gool", "Luc"], "venue": "In Neural Information Processing Systems (NIPS),", "citeRegEx": "Brabandere et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Brabandere et al\\.", "year": 2016}, {"title": "Evolving modular fast-weight networks for control", "author": ["Gomez", "Faustino", "Schmidhuber", "J\u00fcrgen"], "venue": "In International Conference on Artificial Neural Networks,", "citeRegEx": "Gomez et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Gomez et al\\.", "year": 2005}, {"title": "An empirical investigation of catastrophic forgetting in gradient-based neural networks", "author": ["Goodfellow", "Ian J", "Mirza", "Mehdi", "Xiao", "Da", "Courville", "Aaron", "Bengio", "Yoshua"], "venue": null, "citeRegEx": "Goodfellow et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Goodfellow et al\\.", "year": 2014}, {"title": "Neural turing machines", "author": ["Graves", "Alex", "Wayne", "Greg", "Danihelka", "Ivo"], "venue": "arXiv preprint arXiv:1410.5401,", "citeRegEx": "Graves et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Graves et al\\.", "year": 2014}, {"title": "The formation of learning sets", "author": ["Harlow", "Harry F"], "venue": "Psychological review,", "citeRegEx": "Harlow and F.,? \\Q1949\\E", "shortCiteRegEx": "Harlow and F.", "year": 1949}, {"title": "Deep residual learning for image recognition", "author": ["He", "Kaiming", "Zhang", "Xiangyu", "Ren", "Shaoqing", "Sun", "Jian"], "venue": "In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition,", "citeRegEx": "He et al\\.,? \\Q2016\\E", "shortCiteRegEx": "He et al\\.", "year": 2016}, {"title": "Using fast weights to deblur old memories", "author": ["Hinton", "Geoffrey E", "Plaut", "David C"], "venue": "In Proceedings of the ninth annual conference of the Cognitive Science Society,", "citeRegEx": "Hinton et al\\.,? \\Q1987\\E", "shortCiteRegEx": "Hinton et al\\.", "year": 1987}, {"title": "Learning to learn using gradient descent", "author": ["Hochreiter", "Sepp", "Younger", "A Steven", "Conwell", "Peter R"], "venue": "In International Conference on Artificial Neural Networks,", "citeRegEx": "Hochreiter et al\\.,? \\Q2001\\E", "shortCiteRegEx": "Hochreiter et al\\.", "year": 2001}, {"title": "Learning to remember rare events", "author": ["Kaiser", "Lukasz", "Nachum", "Ofir", "Roy", "Aurko", "Bengio", "Samy"], "venue": "In ICLR", "citeRegEx": "Kaiser et al\\.,? \\Q2017\\E", "shortCiteRegEx": "Kaiser et al\\.", "year": 2017}, {"title": "Siamese neural networks for one-shot image recognition", "author": ["Koch", "Gregory"], "venue": "PhD thesis, University of Toronto,", "citeRegEx": "Koch and Gregory.,? \\Q2015\\E", "shortCiteRegEx": "Koch and Gregory.", "year": 2015}, {"title": "One-shot learning by inverting a compositional causal process", "author": ["Lake", "Brenden M", "Salakhutdinov", "Ruslan R", "Tenenbaum", "Josh"], "venue": "In Advances in neural information processing systems,", "citeRegEx": "Lake et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Lake et al\\.", "year": 2013}, {"title": "Human-level concept learning through probabilistic program induction", "author": ["Lake", "Brenden M", "Salakhutdinov", "Ruslan", "Tenenbaum", "Joshua B"], "venue": null, "citeRegEx": "Lake et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Lake et al\\.", "year": 2015}, {"title": "Crafting papers on machine learning", "author": ["P. Langley"], "venue": "Proceedings of the 17th International Conference on Machine Learning (ICML", "citeRegEx": "Langley,? \\Q2000\\E", "shortCiteRegEx": "Langley", "year": 2000}, {"title": "Learning to optimize", "author": ["Li", "Ke", "Malik", "Jitendra"], "venue": "In ICLR 2017,", "citeRegEx": "Li et al\\.,? \\Q2017\\E", "shortCiteRegEx": "Li et al\\.", "year": 2017}, {"title": "Explanationbased neural network learning for robot control", "author": ["Mitchell", "Tom M", "Thrun", "Sebastian B"], "venue": "Advances in neural information processing systems,", "citeRegEx": "Mitchell et al\\.,? \\Q1993\\E", "shortCiteRegEx": "Mitchell et al\\.", "year": 1993}, {"title": "Optimization as a model for few-shot learning", "author": ["Ravi", "Sachin", "Larochell", "Hugo"], "venue": "In ICLR", "citeRegEx": "Ravi et al\\.,? \\Q2017\\E", "shortCiteRegEx": "Ravi et al\\.", "year": 2017}, {"title": "Meta-learning with memory-augmented neural networks", "author": ["Santoro", "Adam", "Bartunov", "Sergey", "Botvinick", "Matthew", "Wierstra", "Daan", "Lillicrap", "Timothy"], "venue": "In Proceedings of The 33rd International Conference on Machine Learning,", "citeRegEx": "Santoro et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Santoro et al\\.", "year": 2016}, {"title": "Learning to control fast-weight memories: An alternative to dynamic recurrent networks", "author": ["Schmidhuber", "J\u00fcrgen"], "venue": "Neural Computation,", "citeRegEx": "Schmidhuber and J\u00fcrgen.,? \\Q1992\\E", "shortCiteRegEx": "Schmidhuber and J\u00fcrgen.", "year": 1992}, {"title": "A neural network that embeds its own meta-levels", "author": ["Schmidhuber", "J\u00fcrgen"], "venue": "In Neural Networks,", "citeRegEx": "Schmidhuber and J\u00fcrgen.,? \\Q1993\\E", "shortCiteRegEx": "Schmidhuber and J\u00fcrgen.", "year": 1993}, {"title": "Compete to compute", "author": ["Srivastava", "Rupesh K", "Masci", "Jonathan", "Kazerounian", "Sohrob", "Gomez", "Faustino", "Schmidhuber", "J\u00fcrgen"], "venue": "In Advances in neural information processing systems,", "citeRegEx": "Srivastava et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Srivastava et al\\.", "year": 2013}, {"title": "A perspective view and survey of meta-learning", "author": ["Vilalta", "Ricardo", "Drissi", "Youssef"], "venue": "Artificial Intelligence Review,", "citeRegEx": "Vilalta et al\\.,? \\Q2002\\E", "shortCiteRegEx": "Vilalta et al\\.", "year": 2002}, {"title": "Matching networks for one shot learning", "author": ["Vinyals", "Oriol", "Blundell", "Charles", "Lillicrap", "Tim", "Wierstra", "Daan"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Vinyals et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Vinyals et al\\.", "year": 2016}, {"title": "Memory networks", "author": ["Weston", "Jason", "Chopra", "Sumit", "Bordes", "Antoine"], "venue": "Proceedings Of The International Conference on Representation Learning (ICLR", "citeRegEx": "Weston et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Weston et al\\.", "year": 2015}, {"title": "Fixed-weight on-line learning", "author": ["Younger", "A Steven", "Conwell", "Peter R", "Cotter", "Neil E"], "venue": "IEEE Transactions on Neural Networks,", "citeRegEx": "Younger et al\\.,? \\Q1999\\E", "shortCiteRegEx": "Younger et al\\.", "year": 1999}], "referenceMentions": [{"referenceID": 18, "context": "Previous work on meta learning has formulated the problem as a two-level learning, a slow learning of a meta-level model performing across tasks and a rapid learning of a base-level model acting within each task (Mitchell et al., 1993; Vilalta & Drissi, 2002).", "startOffset": 212, "endOffset": 259}, {"referenceID": 2, "context": "The base and the meta-level models can be framed in a single learner (Schmidhuber, 1993) or in separate learners (Bengio et al., 1990; Hochreiter et al., 2001).", "startOffset": 113, "endOffset": 159}, {"referenceID": 11, "context": "The base and the meta-level models can be framed in a single learner (Schmidhuber, 1993) or in separate learners (Bengio et al., 1990; Hochreiter et al., 2001).", "startOffset": 113, "endOffset": 159}, {"referenceID": 15, "context": "One notable success is reported by a probabilistic programming approach (Lake et al., 2015).", "startOffset": 72, "endOffset": 91}, {"referenceID": 7, "context": "(2016) proposed a memory-based approach and trained Neural Turing Machines (Graves et al., 2014) for one-shot learning, although the meta-learner and the one-shot learner in this work are not separable explicitly.", "startOffset": 75, "endOffset": 96}, {"referenceID": 12, "context": "One notable success is reported by a probabilistic programming approach (Lake et al., 2015). They used specific knowledge of how pen strokes are composed to produce characters of different alphabets. Koch (2015) applied Siamese Networks to perform one-shot classification.", "startOffset": 73, "endOffset": 212}, {"referenceID": 12, "context": "One notable success is reported by a probabilistic programming approach (Lake et al., 2015). They used specific knowledge of how pen strokes are composed to produce characters of different alphabets. Koch (2015) applied Siamese Networks to perform one-shot classification. Recently, Vinyals et al. (2016) unified the training and testing of a one-shot learner under the same procedure and developed an end-to-end differentiable nearest neighbor method for one-shot learning.", "startOffset": 73, "endOffset": 305}, {"referenceID": 12, "context": "One notable success is reported by a probabilistic programming approach (Lake et al., 2015). They used specific knowledge of how pen strokes are composed to produce characters of different alphabets. Koch (2015) applied Siamese Networks to perform one-shot classification. Recently, Vinyals et al. (2016) unified the training and testing of a one-shot learner under the same procedure and developed an end-to-end differentiable nearest neighbor method for one-shot learning. Santoro et al. (2016) proposed a memory-based approach and trained Neural Turing Machines (Graves et al.", "startOffset": 73, "endOffset": 497}, {"referenceID": 7, "context": "(2016) proposed a memory-based approach and trained Neural Turing Machines (Graves et al., 2014) for one-shot learning, although the meta-learner and the one-shot learner in this work are not separable explicitly. Their training procedure adapted the work of Hochreiter et al. (2001) in which they used LSTMs as the meta-level model.", "startOffset": 76, "endOffset": 284}, {"referenceID": 11, "context": "There is a line of work on building meta optimizers (Hochreiter et al., 2001; Andrychowicz et al., 2016; Li & Malik, 2017).", "startOffset": 52, "endOffset": 122}, {"referenceID": 0, "context": "There is a line of work on building meta optimizers (Hochreiter et al., 2001; Andrychowicz et al., 2016; Li & Malik, 2017).", "startOffset": 52, "endOffset": 122}, {"referenceID": 18, "context": "By following the success of the previous work (Mitchell et al., 1993; Younger et al., 1999; Andrychowicz et al., 2016; Ravi & Larochell, 2017), we studied the meta information in the form of the loss gradient of neural nets.", "startOffset": 46, "endOffset": 142}, {"referenceID": 27, "context": "By following the success of the previous work (Mitchell et al., 1993; Younger et al., 1999; Andrychowicz et al., 2016; Ravi & Larochell, 2017), we studied the meta information in the form of the loss gradient of neural nets.", "startOffset": 46, "endOffset": 142}, {"referenceID": 0, "context": "By following the success of the previous work (Mitchell et al., 1993; Younger et al., 1999; Andrychowicz et al., 2016; Ravi & Larochell, 2017), we studied the meta information in the form of the loss gradient of neural nets.", "startOffset": 46, "endOffset": 142}, {"referenceID": 0, "context": ", 2001; Andrychowicz et al., 2016; Li & Malik, 2017). As the main interest here is to train an optimization algorithm within the meta learning framework, the previous research have mainly focused on tasks with large datasets. In contrast, with the absent of large datasets, we emphasize the difficulties to optimize a neural network with a large number of parameters to generalize with limited examples of new concept. Our work instead proposes a novel rapid parameterization approach by employing the meta information. By following the success of the previous work (Mitchell et al., 1993; Younger et al., 1999; Andrychowicz et al., 2016; Ravi & Larochell, 2017), we studied the meta information in the form of the loss gradient of neural nets. The ideas of fast weights and generating parameters for one neural network with another have been explored individually in separate contexts. Hinton & Plaut (1987) suggested the usage of fast weights for rapid learning.", "startOffset": 8, "endOffset": 909}, {"referenceID": 0, "context": ", 2001; Andrychowicz et al., 2016; Li & Malik, 2017). As the main interest here is to train an optimization algorithm within the meta learning framework, the previous research have mainly focused on tasks with large datasets. In contrast, with the absent of large datasets, we emphasize the difficulties to optimize a neural network with a large number of parameters to generalize with limited examples of new concept. Our work instead proposes a novel rapid parameterization approach by employing the meta information. By following the success of the previous work (Mitchell et al., 1993; Younger et al., 1999; Andrychowicz et al., 2016; Ravi & Larochell, 2017), we studied the meta information in the form of the loss gradient of neural nets. The ideas of fast weights and generating parameters for one neural network with another have been explored individually in separate contexts. Hinton & Plaut (1987) suggested the usage of fast weights for rapid learning. Ba et al. (2016) recently used fast weights to replace soft attention mechanism.", "startOffset": 8, "endOffset": 982}, {"referenceID": 4, "context": "De Brabandere et al. (2016) used one network to generate slow filter weights for a convolutional neural net whereas more recently David Ha & Le (2017) generated slow weights for recurrent nets.", "startOffset": 3, "endOffset": 28}, {"referenceID": 4, "context": "De Brabandere et al. (2016) used one network to generate slow filter weights for a convolutional neural net whereas more recently David Ha & Le (2017) generated slow weights for recurrent nets.", "startOffset": 3, "endOffset": 151}, {"referenceID": 7, "context": "MANNs have shown promising results on a range of tasks starting from small programming problems (Graves et al., 2014) to large-scale language tasks (Weston et al.", "startOffset": 96, "endOffset": 117}, {"referenceID": 26, "context": ", 2014) to large-scale language tasks (Weston et al., 2015; Munkhdalai & Yu, 2017).", "startOffset": 38, "endOffset": 82}, {"referenceID": 3, "context": "However, when there are more than one examples per class available, contrastive loss (Chopra et al., 2005) is a natural choice for lossu since both positive and negative samples can be formed.", "startOffset": 85, "endOffset": 106}, {"referenceID": 9, "context": "Overall the layer augmentation building block is similar to that of ResNet architecture (He et al., 2016).", "startOffset": 88, "endOffset": 105}, {"referenceID": 15, "context": "The Omniglot dataset consists of images across 1623 classes with only 20 images per class, from 50 different alphabets (Lake et al., 2015).", "startOffset": 119, "endOffset": 138}, {"referenceID": 20, "context": "Following (Santoro et al., 2016), we augmented the data set through 90, 180 and 270 degrees rotations.", "startOffset": 10, "endOffset": 32}, {"referenceID": 25, "context": "To train and test MetaNet on one-shot learning, we adapted the training procedure introduced by Vinyals et al. (2016). First we split the Omniglot data into training and test sets consisting of two disjoint classes.", "startOffset": 96, "endOffset": 118}, {"referenceID": 0, "context": "As in Andrychowicz et al. (2016), the parameters G and Z of m and m are shared across the coordinates of the gradients \u2207 and the gradients are normalized using the same preprocessing rule (with p = 7) before it is given to the meta learner.", "startOffset": 6, "endOffset": 33}, {"referenceID": 12, "context": "Pixel kNN (Kaiser et al., 2017) 41.", "startOffset": 10, "endOffset": 31}, {"referenceID": 20, "context": "1 MANN (Santoro et al., 2016) 82.", "startOffset": 7, "endOffset": 29}, {"referenceID": 25, "context": "8 Matching Nets (Vinyals et al., 2016) 98.", "startOffset": 16, "endOffset": 38}, {"referenceID": 12, "context": "8 Siamese Net with Memory (Kaiser et al., 2017) 98.", "startOffset": 26, "endOffset": 47}, {"referenceID": 25, "context": "Following the previous setup Vinyals et al. (2016), we split the Omniglot classes into 1200 and 423 classes for training and testing.", "startOffset": 29, "endOffset": 51}, {"referenceID": 25, "context": "3% while Matching Net (Vinyals et al., 2016) reported 72.", "startOffset": 22, "endOffset": 44}, {"referenceID": 14, "context": "In order to best match the evaluation protocol of Lake et al. (2015), we form 400 tasks", "startOffset": 50, "endOffset": 69}, {"referenceID": 15, "context": "Human performance (Lake et al., 2015) - 95.", "startOffset": 18, "endOffset": 37}, {"referenceID": 14, "context": "Pixel kNN (Lake et al., 2013) - 21.", "startOffset": 10, "endOffset": 29}, {"referenceID": 14, "context": "7 Affine model (Lake et al., 2013) - 81.", "startOffset": 15, "endOffset": 34}, {"referenceID": 14, "context": "8 Deep Boltzmann Machines (Lake et al., 2013) - 62.", "startOffset": 26, "endOffset": 45}, {"referenceID": 15, "context": "0 Hierarchial Bayesian Program Learning (Lake et al., 2015) - 96.", "startOffset": 40, "endOffset": 59}, {"referenceID": 23, "context": "Following the previous work on catastrophic forgetting in neural networks (Srivastava et al., 2013; Goodfellow et al., 2014; Kirkpatrick et al., 2016), we formulated two problems in a sequential manner.", "startOffset": 74, "endOffset": 150}, {"referenceID": 6, "context": "Following the previous work on catastrophic forgetting in neural networks (Srivastava et al., 2013; Goodfellow et al., 2014; Kirkpatrick et al., 2016), we formulated two problems in a sequential manner.", "startOffset": 74, "endOffset": 150}], "year": 2017, "abstractText": "Deep neural networks have been successfully applied in applications with a large amount of labeled data. However, there are major drawbacks of the neural networks that are related to rapid generalization with small data and continual learning of new concepts without forgetting. We present a novel meta learning method, Meta Networks (MetaNet), that acquires a meta-level knowledge across tasks and shifts its inductive bias via fast parameterization for the rapid generalization. When tested on the standard oneshot learning benchmarks, our MetaNet models achieved near human-level accuracy. We demonstrated several appealing properties of MetaNet relating to generalization and continual learning.", "creator": "LaTeX with hyperref package"}}}