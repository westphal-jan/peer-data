{"id": "1703.00848", "review": {"conference": "nips", "VERSION": "v1", "DATE_OF_SUBMISSION": "2-Mar-2017", "title": "Unsupervised Image-to-Image Translation Networks", "abstract": "Most of the existing image-to-image translation frameworks---mapping an image in one domain to a corresponding image in another---are based on supervised learning, i.e., pairs of corresponding images in two domains are required for learning the translation function. This largely limits their applications, because capturing corresponding images in two different domains is often a difficult task. To address the issue, we propose the UNsupervised Image-to-image Translation (UNIT) framework, which is based on variational autoencoders and generative adversarial networks. The proposed framework can learn the translation function without any corresponding images in two domains. We enable this learning capability by combining a weight-sharing constraint and an adversarial training objective. Through visualization results from various unsupervised image translation tasks, we verify the effectiveness of the proposed framework. An ablation study further reveals the critical design choices. Moreover, we apply the UNIT framework to the unsupervised domain adaptation task and achieve better results than competing algorithms do in benchmark datasets.", "histories": [["v1", "Thu, 2 Mar 2017 16:29:30 GMT  (3274kb,D)", "http://arxiv.org/abs/1703.00848v1", "19 pages, 19 figures"], ["v2", "Tue, 3 Oct 2017 17:55:21 GMT  (4519kb,D)", "http://arxiv.org/abs/1703.00848v2", "11 pages, 6 figures"], ["v3", "Fri, 6 Oct 2017 03:14:21 GMT  (4519kb,D)", "http://arxiv.org/abs/1703.00848v3", "11 pages, 6 figures, The paper will be published in NIPS 2017"], ["v4", "Mon, 9 Oct 2017 18:14:27 GMT  (4642kb,D)", "http://arxiv.org/abs/1703.00848v4", "NIPS 2017, 11 pages, 6 figures"]], "COMMENTS": "19 pages, 19 figures", "reviews": [], "SUBJECTS": "cs.CV cs.AI", "authors": ["ming-yu liu", "thomas breuel", "jan kautz"], "accepted": true, "id": "1703.00848"}, "pdf": {"name": "1703.00848.pdf", "metadata": {"source": "META", "title": "Unsupervised Image-to-Image Translation Networks", "authors": ["Ming-Yu Liu", "Thomas Breuel", "Jan Kautz"], "emails": ["MINGYUL@NVIDIA.COM", "TBREUEL@NVIDIA.COM", "JKAUTZ@NVIDIA.COM"], "sections": [{"heading": "1. Introduction", "text": "One can translate an image in a modality that is difficult to understand in order to obtain a corresponding color image for better visualization; one can translate labeled images in a domain into corresponding images in a target domain, creating a training data set that could be used to train classifiers in the target domain. Most of the existing picture-to-picture translation approaches are based on supervised learning; they require training data sets consisting of pairs of corresponding images in two domains (Ledig et al., 2016; Isola et al., 2016). Although they can achieve good performance in multiple image translation tasks (e.g. superresolution and coloration), the required image pairs are difficult to obtain for many applications; certain applications require complex setups involving multiple sensors, mirrors, and beamers."}, {"heading": "2. Mathematical Motivation", "text": "Let X1 and X2 be two different image areas. In a monitored picture-to-picture translation problem, we are given a training sample (x1, x2), which we have drawn from a common distribution PX1, X2 (x1, x2). However, in the unattended environment, we will receive training samples only from the marginal distributions PX1 (x1) and PX2 (x2). Without other assumptions, we cannot follow anything about the common distribution from the marginal distributions.ar Xiv: 170 3.00 848v 1 [cs.C V] 2M ar2 017To illustrate the motivation and idea behind our approach, we consider a very simple model in which there is an almost 1-1 correspondence between images in the two domains. (Of course, this is generally not true, like the correspondence between thermal and RGB images, so we will talk about general cases in the next section.) Recall in which we try to discover the relationship between the two domains."}, {"heading": "3. The UNIT Framework", "text": "The framework, as illustrated in Figure 1, is motivated by newer, deep generative models, including variational autoencoders (VAEs) (Kingma & Welling, 2013; Rezende et al., 2014; Larsen et al., 2016) and generative networks (GANs) (Goodfellow et al., 2014; Liu & Tuzel, 2016). It consists of 6 subnetworks: including two domain image encoders E1 and E2, two domain generators G1 and G2, and two domain adversarial discriminators D1 and D2. Multiple ways exist to interpret the roles of the subnetworks as summarize in Table 1. We note that the UNIT network learns two ways of translation in one shot."}, {"heading": "4. Implementations", "text": "We are responsible for mapping the images to the latent space that represents image diversity. However, the deeper the encoders become, the more difficult it becomes to preserve image details after layers of neural information processing, resulting in blurred image reconstruction and translation. To overcome the problem, we use skip connections to send intermedia images from the encoders to the decoders. Skip connections are applied to all layers where the weights are shared by the two encoders, creating channels for transmitting images of varying granularity. Note that we do not apply skip connections to the first layers of the encoders where the weights are not calculated by the layers that are not available during the test.The skip connections in the UNIT network represent stochastic sampling operations that compensate for the number of the Lp using the VAE design principle."}, {"heading": "5. Experiments", "text": "In fact, the fact is that you will be able to move to another world, where you have to move to another world, where you have to move to another world, where you can move to another world, where you have to move to another world, where you have to move to another world, where you have to move to another world, where you have to move to another world, where you have to move to another world, where you can move to another world."}, {"heading": "6. Related Works", "text": "Several deep generative models have recently been proposed for image gneration, including GANs (Goodfellow et al., 2014), UAEs (Kingma & Welling, 2013; Rezende et al., 2014), Moment Matching Networks (Li et al., 2015), PixelCNN (van den Oord et al., 2016), and Plug & Play Generative Networks (Nguyen et al., 2016).The UNIT framework is based on GANs and VAEs, but it is designed for the unattended picture-to-picture translation task. Below, we will first review several current GAN and VAE works and then discuss related image translations Works.GANs to learn how to generate images by playing a zero-sum game between a generator and a discriminator. The quality of images generated by GANs has dramatically improved. (Denton et al., 2015) suggested a laponical pyramid implementation of GANs."}, {"heading": "7. Conclusion and Future Work", "text": "We introduced the UNIT framework - a general framework for unattended picture-to-picture translation. We showed that it has learned to translate an image from one area to another without corresponding images in two areas of the training data set. In the future, we plan to extend the framework to the semi-supervised picture-to-picture translation task, where monitoring of domain correspondence is provided either by a set of rules or a few pairs of corresponding images. We are also interested in extending the framework to the task of unattended translation from language to language."}, {"heading": "A. Training", "text": "We present the learning algorithm for the UNIT framework in algorithm 1. The algorithm can be considered an extension of the learning algorithm for generative adversarial networks (GAN), the convergence property follows the results presented in (Goodfellow et al., 2014). Algorithm 1 The alternating stochastic gradient update algorithm for the formation of unattended picture-to-picture translation networks. 1: Initialize E1 E2 G1 G2 D1 and D2 with the same values. 2: for t = 0, 1, 2,..., MAX ITER do 3: for k = 0, 1, 2,..., K do 4: Draw N samples from the first domain trainingdataset, X1 = {x (1) 1, x (N) 1, x (N) 1}, 5: DrawN samples from the second domain training dataset, X2 = {x (1),..."}, {"heading": "B. Quantitative Analysis", "text": "We created a toy dataset with the MNIST dataset to analyze various components within the UNIT frame. We divided the MNIST training into two equally sized image areas. For each handwritten image in the first row, we randomly colored the lines with a color that was either red, green or blue. For each image in the second row, we first calculated the edge map and then randomly colored the edges with a color that was either cyan, yellow or magenta. Some examples from the two sentences are shown in Figure 9. The two sentences formed two different image areas. Our goal was to translate a digital image from the 1st domain into the corresponding digital image and visualize it in the second domain. As there were no corresponding images in the two sentences, the translation function had to be performed in an untrained manner."}, {"heading": "C. Unsupervised Domain Adaptation", "text": "In fact, most of them are able to survive by themselves if they do not follow the rules they have imposed on themselves. (...) Most of them are able to survive by themselves. (...) Most of them are not able to survive by themselves. (...) Most of them are not able to survive by themselves. (...) Most of them are not able to survive by themselves. \"(...) Most of them are not able to survive by themselves.\" (...) Most of them are not able to survive by themselves. \"(...) Most of them are not able to survive by themselves.\" (...) Most of them are not able to survive by themselves. \"(...) Most of them are not able to survive by themselves.\" (...)"}, {"heading": "D. Network Architecture", "text": "The UNIT architecture for translating natural images, presented in the Experiments section, is shown in Table 10."}, {"heading": "E. Additional Translation Results", "text": "Other unattended picture-to-picture translation results are shown in Figures 14,15, 16, 17 18 and 19."}], "references": [{"title": "Infogan: Interpretable representation learning by information maximizing generative adversarial nets", "author": ["Chen", "Xi", "Duan", "Yan", "Houthooft", "Rein", "Schulman", "John", "Sutskever", "Ilya", "Abbeel", "Pieter"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Chen et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Chen et al\\.", "year": 2016}, {"title": "Deep generative image models using a laplacian pyramid of adversarial networks", "author": ["Denton", "Emily L", "Chintala", "Soumith", "Fergus", "Rob"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Denton et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Denton et al\\.", "year": 2015}, {"title": "Unsupervised visual domain adaptation using subspace alignment", "author": ["Fernando", "Basura", "Habrard", "Amaury", "Sebban", "Marc", "Tuytelaars", "Tinne"], "venue": "In International Conference on Computer Vision,", "citeRegEx": "Fernando et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Fernando et al\\.", "year": 2013}, {"title": "Domainadversarial training of neural networks", "author": ["Ganin", "Yaroslav", "Ustinova", "Evgeniya", "Ajakan", "Hana", "Germain", "Pascal", "Larochelle", "Hugo", "Laviolette", "Fran\u00e7ois", "Marchand", "Mario", "Lempitsky", "Victor"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Ganin et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Ganin et al\\.", "year": 2016}, {"title": "Image style transfer using convolutional neural networks", "author": ["Gatys", "Leon A", "Ecker", "Alexander S", "Bethge", "Matthias"], "venue": "In Conference on Computer Vision and Pattern Recognition", "citeRegEx": "Gatys et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Gatys et al\\.", "year": 2016}, {"title": "Generative adversarial nets", "author": ["Goodfellow", "Ian", "Pouget-Abadie", "Jean", "Mirza", "Mehdi", "Xu", "Bing", "Warde-Farley", "David", "Ozair", "Sherjil", "Courville", "Aaron", "Bengio", "Yoshua"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Goodfellow et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Goodfellow et al\\.", "year": 2014}, {"title": "Multispectral pedestrian detection: Benchmark dataset and baseline", "author": ["Hwang", "Soonmin", "Park", "Jaesik", "Kim", "Namil", "Choi", "Yukyung", "So Kweon", "In"], "venue": "In Conference on Computer Vision and Pattern Recognition (CVPR),", "citeRegEx": "Hwang et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Hwang et al\\.", "year": 2015}, {"title": "Image-to-image translation with conditional adversarial networks", "author": ["Isola", "Phillip", "Zhu", "Jun-Yan", "Zhou", "Tinghui", "Efros", "Alexei A"], "venue": "arXiv preprint arXiv:1611.07004,", "citeRegEx": "Isola et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Isola et al\\.", "year": 2016}, {"title": "Perceptual losses for real-time style transfer and superresolution", "author": ["Johnson", "Justin", "Alahi", "Alexandre", "Fei-Fei", "Li"], "venue": "In European Conference in Computer Vision,", "citeRegEx": "Johnson et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Johnson et al\\.", "year": 2016}, {"title": "Adam: A method for stochastic optimization", "author": ["Kingma", "Diederik", "Ba", "Jimmy"], "venue": "In International Conference on Learning Representations,", "citeRegEx": "Kingma et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Kingma et al\\.", "year": 2015}, {"title": "Auto-encoding variational bayes", "author": ["Kingma", "Diederik P", "Welling", "Max"], "venue": "arXiv preprint arXiv:1312.6114,", "citeRegEx": "Kingma et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Kingma et al\\.", "year": 2013}, {"title": "Improving variational inference with inverse autoregressive flow", "author": ["Kingma", "Diederik P", "Salimans", "Tim", "Welling", "Max"], "venue": "arXiv preprint arXiv:1606.04934,", "citeRegEx": "Kingma et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Kingma et al\\.", "year": 2016}, {"title": "Autoencoding beyond pixels using a learned similarity metric", "author": ["Larsen", "Anders Boesen Lindbo", "S\u00f8nderby", "S\u00f8ren Kaae", "Larochelle", "Hugo", "Winther", "Ole"], "venue": "International Conference on Machine Learning,", "citeRegEx": "Larsen et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Larsen et al\\.", "year": 2016}, {"title": "Generative moment matching networks", "author": ["Li", "Yujia", "Swersky", "Kevin", "Zemel", "Richard S"], "venue": "In International Conference on Machine Learning,", "citeRegEx": "Li et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Li et al\\.", "year": 2015}, {"title": "Coupled generative adversarial networks", "author": ["Liu", "Ming-Yu", "Tuzel", "Oncel"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Liu et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Liu et al\\.", "year": 2016}, {"title": "Deep learning face attributes in the wild", "author": ["Liu", "Ziwei", "Luo", "Ping", "Wang", "Xiaogang", "Tang", "Xiaoou"], "venue": "In International Conference on Computer Vision,", "citeRegEx": "Liu et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Liu et al\\.", "year": 2015}, {"title": "Auxiliary deep generative models", "author": ["Maal\u00f8e", "Lars", "S\u00f8nderby", "Casper Kaae", "S\u00f8ren Kaae", "Winther", "Ole"], "venue": "arXiv preprint arXiv:1602.05473,", "citeRegEx": "Maal\u00f8e et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Maal\u00f8e et al\\.", "year": 2016}, {"title": "Reading digits in natural images with unsupervised feature learning", "author": ["Netzer", "Yuval", "Wang", "Tao", "Coates", "Adam", "Bissacco", "Alessandro", "Wu", "Bo", "Ng", "Andrew Y"], "venue": "In Advances in Neural Information Processing Systems workshop,", "citeRegEx": "Netzer et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Netzer et al\\.", "year": 2011}, {"title": "Plug & play generative networks: Conditional iterative generation of images in latent space", "author": ["Nguyen", "Anh", "Yosinski", "Jason", "Bengio", "Yoshua", "Dosovitskiy", "Alexey", "Clune", "Jeff"], "venue": "arXiv preprint arXiv:1612.00005,", "citeRegEx": "Nguyen et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Nguyen et al\\.", "year": 2016}, {"title": "Unsupervised representation learning with deep convolutional generative adversarial networks", "author": ["Radford", "Alec", "Metz", "Luke", "Chintala", "Soumith"], "venue": "In International Conference on Learning Representations,", "citeRegEx": "Radford et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Radford et al\\.", "year": 2016}, {"title": "Stochastic backpropagation and variational inference in deep latent gaussian models", "author": ["Rezende", "Danilo Jimenez", "Mohamed", "Shakir", "Wierstra", "Daan"], "venue": "In International Conference on Machine Learning,", "citeRegEx": "Rezende et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Rezende et al\\.", "year": 2014}, {"title": "Improved techniques for training gans", "author": ["Salimans", "Tim", "Goodfellow", "Ian", "Zaremba", "Wojciech", "Cheung", "Vicki", "Radford", "Alec", "Chen", "Xi"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Salimans et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Salimans et al\\.", "year": 2016}, {"title": "Learning from simulated and unsupervised images through adversarial training", "author": ["Shrivastava", "Ashish", "Pfister", "Tomas", "Tuzel", "Oncel", "Susskind", "Josh", "Wang", "Wenda", "Webb", "Russ"], "venue": "arXiv preprint arXiv:1612.07828,", "citeRegEx": "Shrivastava et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Shrivastava et al\\.", "year": 2016}, {"title": "Unsupervised cross-domain image generation", "author": ["Taigman", "Yaniv", "Polyak", "Adam", "Wolf", "Lior"], "venue": "International Conference on Learning Representations,", "citeRegEx": "Taigman et al\\.,? \\Q2017\\E", "shortCiteRegEx": "Taigman et al\\.", "year": 2017}, {"title": "A note on the evaluation of generative models", "author": ["Theis", "Lucas", "Oord", "A\u00e4ron van den", "Bethge", "Matthias"], "venue": "International Conference on Learning Representations,", "citeRegEx": "Theis et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Theis et al\\.", "year": 2016}, {"title": "Conditional image generation with pixelcnn decoders", "author": ["van den Oord", "Aaron", "Kalchbrenner", "Nal", "Espeholt", "Lasse", "Vinyals", "Oriol", "Graves", "Alex"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Oord et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Oord et al\\.", "year": 2016}, {"title": "Alignment by maximization of mutual information", "author": ["Viola", "Paul", "Wells III", "William M"], "venue": "International journal of computer vision,", "citeRegEx": "Viola et al\\.,? \\Q1997\\E", "shortCiteRegEx": "Viola et al\\.", "year": 1997}, {"title": "Attribute2image: Conditional image generation from visual attributes", "author": ["Yan", "Xinchen", "Yang", "Jimei", "Sohn", "Kihyuk", "Lee", "Honglak"], "venue": "European Conference in Computer Vision,", "citeRegEx": "Yan et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Yan et al\\.", "year": 2016}, {"title": "Stackgan: Text to photo-realistic image synthesis with stacked generative adversarial networks", "author": ["Zhang", "Han", "Xu", "Tao", "Li", "Hongsheng", "Shaoting", "Huang", "Xiaolei", "Wang", "Xiaogang", "Metaxas", "Dimitris"], "venue": "arXiv preprint arXiv:1612.03242,", "citeRegEx": "Zhang et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Zhang et al\\.", "year": 2016}], "referenceMentions": [{"referenceID": 7, "context": "They require training datasets consisting of pairs of corresponding images in two domains (Ledig et al., 2016; Isola et al., 2016).", "startOffset": 90, "endOffset": 130}, {"referenceID": 20, "context": "The framework, as illustrated in Figure 1, is motivated by recent deep generative models including variational autoencoders (VAEs) (Kingma & Welling, 2013; Rezende et al., 2014; Larsen et al., 2016) and generative adversarial networks (GANs) (Goodfellow et al.", "startOffset": 131, "endOffset": 198}, {"referenceID": 12, "context": "The framework, as illustrated in Figure 1, is motivated by recent deep generative models including variational autoencoders (VAEs) (Kingma & Welling, 2013; Rezende et al., 2014; Larsen et al., 2016) and generative adversarial networks (GANs) (Goodfellow et al.", "startOffset": 131, "endOffset": 198}, {"referenceID": 5, "context": ", 2016) and generative adversarial networks (GANs) (Goodfellow et al., 2014; Liu & Tuzel, 2016).", "startOffset": 51, "endOffset": 95}, {"referenceID": 12, "context": "{E1, G1} VAE for X1 {E2, G2} VAE for X2 {E1, G2} X1 \u2192 X2 Image Translator {E2, G1} X2 \u2192 X1 Image Translator {G1, D1} GAN for X1 {G2, D2} GAN for X2 {E1, G1, D1} VAE-GAN (Larsen et al., 2016) {G1, G2, D1, D2} CoGAN (Liu & Tuzel, 2016)", "startOffset": 169, "endOffset": 190}, {"referenceID": 5, "context": "We apply an alternating gradient update scheme similar to the one described in (Goodfellow et al., 2014) to solve (1).", "startOffset": 79, "endOffset": 104}, {"referenceID": 12, "context": "2) Sampling from the VAE latent space admits a simple form, allowing a seamless integration with GANs (Larsen et al., 2016).", "startOffset": 102, "endOffset": 123}, {"referenceID": 12, "context": "An image generator trained using the GAN loss can generate crisp sharp images (Larsen et al., 2016; Ledig et al., 2016).", "startOffset": 78, "endOffset": 119}, {"referenceID": 19, "context": "999, respectively, as in (Radford et al., 2016).", "startOffset": 25, "endOffset": 47}, {"referenceID": 6, "context": "In the first experiment, we trained a UNIT network using the KAIST multispectral pedestrian detection benchmark (Hwang et al., 2015) for translating between daytime and night time images and between thermal IR and RGB images.", "startOffset": 112, "endOffset": 132}, {"referenceID": 15, "context": "We used the CelebFaces Attributes dataset (Liu et al., 2015) for translating face images based on attributes.", "startOffset": 42, "endOffset": 60}, {"referenceID": 24, "context": "Quantitative Evaluation: Quantitative evaluation of generative models is known to be a challenging task and the popular metrics are all subject to flaws (Theis et al., 2016).", "startOffset": 153, "endOffset": 173}, {"referenceID": 2, "context": "Early UDA works have explored ideas from subspace learning (Fernando et al., 2013) to deep learning (Ganin et al.", "startOffset": 59, "endOffset": 82}, {"referenceID": 3, "context": ", 2013) to deep learning (Ganin et al., 2016; Liu & Tuzel, 2016; Taigman et al., 2017).", "startOffset": 25, "endOffset": 86}, {"referenceID": 23, "context": ", 2013) to deep learning (Ganin et al., 2016; Liu & Tuzel, 2016; Taigman et al., 2017).", "startOffset": 25, "endOffset": 86}, {"referenceID": 23, "context": "The results of the other algorithms were duplicated from (Taigman et al., 2017)", "startOffset": 57, "endOffset": 79}, {"referenceID": 2, "context": "SA (Fernando et al., 2013) 59.", "startOffset": 3, "endOffset": 26}, {"referenceID": 3, "context": "32% DANN (Ganin et al., 2016) 73.", "startOffset": 9, "endOffset": 29}, {"referenceID": 23, "context": "85% DTN (Taigman et al., 2017) 84.", "startOffset": 8, "endOffset": 30}, {"referenceID": 17, "context": "We applied the above approach to the task of adapting a classifier from the Street View House Number (SVHN) dataset (Netzer et al., 2011) to the MNIST dataset.", "startOffset": 116, "endOffset": 137}, {"referenceID": 23, "context": "88% achieved by the previous state-of-the-art method (Taigman et al., 2017).", "startOffset": 53, "endOffset": 75}, {"referenceID": 5, "context": "Several deep generative models were recently proposed for image gneration including GANs (Goodfellow et al., 2014), VAEs (Kingma & Welling, 2013; Rezende et al.", "startOffset": 89, "endOffset": 114}, {"referenceID": 20, "context": ", 2014), VAEs (Kingma & Welling, 2013; Rezende et al., 2014), moment matching networks (Li et al.", "startOffset": 14, "endOffset": 60}, {"referenceID": 13, "context": ", 2014), moment matching networks (Li et al., 2015), PixelCNN (van den Oord et al.", "startOffset": 34, "endOffset": 51}, {"referenceID": 18, "context": ", 2016), and Plug&Play Generative Networks (Nguyen et al., 2016).", "startOffset": 43, "endOffset": 64}, {"referenceID": 1, "context": "(Denton et al., 2015) proposed a Laplacian pyramid implementation of GANs.", "startOffset": 0, "endOffset": 21}, {"referenceID": 19, "context": "(Radford et al., 2016) used a deeper convolution network architecture.", "startOffset": 0, "endOffset": 22}, {"referenceID": 28, "context": "(Zhang et al., 2016) stacks two generators to progressively render realistic images.", "startOffset": 0, "endOffset": 20}, {"referenceID": 0, "context": "InfoGAN (Chen et al., 2016) learned to a more interpretable latent representation.", "startOffset": 8, "endOffset": 27}, {"referenceID": 21, "context": "(Salimans et al., 2016) proposed several GAN training tricks.", "startOffset": 0, "endOffset": 23}, {"referenceID": 16, "context": "By improving the variational approximation, better image generation results were achieved (Maal\u00f8e et al., 2016; Kingma et al., 2016).", "startOffset": 90, "endOffset": 132}, {"referenceID": 11, "context": "By improving the variational approximation, better image generation results were achieved (Maal\u00f8e et al., 2016; Kingma et al., 2016).", "startOffset": 90, "endOffset": 132}, {"referenceID": 12, "context": "In (Larsen et al., 2016), a VAEGAN architecture was proposed to improve image generation quality of VAEs.", "startOffset": 3, "endOffset": 24}, {"referenceID": 27, "context": "VAEs were applied to translate face image attribute in (Yan et al., 2016).", "startOffset": 55, "endOffset": 73}, {"referenceID": 7, "context": "Most of the existing works were based on supervised learning (Ledig et al., 2016; Isola et al., 2016), requiring corresponding images in two domains.", "startOffset": 61, "endOffset": 101}, {"referenceID": 23, "context": "Recently, (Taigman et al., 2017) proposed the domain transformation network (DTN) and achieved promising results on translating small resolution face and digit images.", "startOffset": 10, "endOffset": 32}, {"referenceID": 22, "context": "In (Shrivastava et al., 2016), a conditional generative adversarial network-based approach was proposed to translate a rendering images to a real image for gaze estimation.", "startOffset": 3, "endOffset": 29}, {"referenceID": 4, "context": "We note that image translation is different to style transfer (Gatys et al., 2016; Johnson et al., 2016) in that while style transfer focuses on translate a natural image to an abstract, artistic image, image translation also considers translating a natural image to another natural image where loss of image details is unacceptable.", "startOffset": 62, "endOffset": 104}, {"referenceID": 8, "context": "We note that image translation is different to style transfer (Gatys et al., 2016; Johnson et al., 2016) in that while style transfer focuses on translate a natural image to an abstract, artistic image, image translation also considers translating a natural image to another natural image where loss of image details is unacceptable.", "startOffset": 62, "endOffset": 104}], "year": 2017, "abstractText": "Most of the existing image-to-image translation frameworks\u2014mapping an image in one domain to a corresponding image in another\u2014are based on supervised learning, i.e., pairs of corresponding images in two domains are required for learning the translation function. This largely limits their applications, because capturing corresponding images in two different domains is often a difficult task. To address the issue, we propose the UNsupervised Image-to-image Translation (UNIT) framework, which is based on variational autoencoders and generative adversarial networks. The proposed framework can learn the translation function without any corresponding images in two domains. We enable this learning capability by combining a weight-sharing constraint and an adversarial training objective. Through visualization results from various unsupervised image translation tasks, we verify the effectiveness of the proposed framework. An ablation study further reveals the critical design choices. Moreover, we apply the UNIT framework to the unsupervised domain adaptation task and achieve better results than competing algorithms do in benchmark datasets.", "creator": "LaTeX with hyperref package"}}}