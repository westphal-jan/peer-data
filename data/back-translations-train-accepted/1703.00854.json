{"id": "1703.00854", "review": {"conference": "ICML", "VERSION": "v1", "DATE_OF_SUBMISSION": "2-Mar-2017", "title": "Learning the Structure of Generative Models without Labeled Data", "abstract": "Curating labeled training data has become the primary bottleneck in machine learning. Recent frameworks address this bottleneck with generative models to synthesize labels at scale from weak supervision sources. The generative model's dependency structure directly affects the quality of the estimated labels, but selecting a structure automatically without any labeled data is a distinct challenge. We propose a structure estimation method that maximizes the $\\ell_1$-regularized marginal pseudolikelihood of the observed data. Our analysis shows that the amount of unlabeled data required to identify the true structure scales sublinearly in the number of possible dependencies for a broad class of models. Experiments on synthetic data show that our method is 100$\\times$ faster than a maximum likelihood approach and selects $1/4$ as many extraneous dependencies. We also show that our method provides an average of 1.5 F1 points of improvement over existing, user-developed information extraction applications on real-world data such as PubMed journal articles.", "histories": [["v1", "Thu, 2 Mar 2017 16:52:09 GMT  (196kb,D)", "https://arxiv.org/abs/1703.00854v1", null], ["v2", "Sat, 9 Sep 2017 21:22:57 GMT  (280kb,D)", "http://arxiv.org/abs/1703.00854v2", null]], "reviews": [], "SUBJECTS": "cs.LG stat.ML", "authors": ["stephen h bach", "bryan dawei he", "alexander ratner", "christopher r\u00e9"], "accepted": true, "id": "1703.00854"}, "pdf": {"name": "1703.00854.pdf", "metadata": {"source": "META", "title": "Learning the Structure of Generative Models without Labeled Data", "authors": ["Stephen H. Bach", "Bryan He", "Alexander Ratner", "Christopher R\u00e9"], "emails": ["<bach@cs.stanford.edu>."], "sections": [{"heading": "1. Introduction", "text": "In fact, the fact is that most of them are able to outdo themselves, and that they are able to outdo themselves, \"he said in an interview with the British newspaper The Sun.\" I don't think they're able to outdo themselves, \"he said.\" I don't think they're able to outdo themselves. \"\" I think so, \"he said,\" but I don't think they're able to outdo me. \"I think, he said,\" but I don't think I can outdo myself. \""}, {"heading": "2. Background", "text": "In the development of machine learning systems, a sufficient amount of labeled training data is often used to differentiate them. Manual labeling of training data is expensive, time-consuming, and often requires specialized knowledge. (See Section 6 for a summary.) We base our work on a framework in which data is programmed (Ratner et al., 2016) that generalizes many approaches to literacy. In data programming, weak supervision sources are encoded as labeling functions that label (or abstain) the data. A generative probability model is suitable for estimating the accuracy of labeling functions and the strength of user-specified statistical dependencies between their outputs. In this model, the true class designation for a data point is a latent variable that generates the labeling functions outputs."}, {"heading": "3. Structure Learning without Labels", "text": "In data programming, users often write labeling functions with directly correlated outputs or even labeling functions that are deliberately designed to amplify others with narrow, more precise heuristics. To solve this problem, we generalize the conditionally independent model as a factor graph with additional dependencies, including higher-order factors that combine multiple labeling functions Outputs for each data point xi and each label yi. (3) Here, T is the set of interest types of dependencies, and St is a set of index tuples that indicate the labeling functions involved in each dependency of type t-T. We begin by defining the default correlation dependencies of the formal correlation corjk (i, yi): = 1 {ichichichichichichichichik}. We refer to such dependencies that are involved in each dependency of type t-T."}, {"heading": "3.1. Learning Objective", "text": "We can expand learning about many potentially irrelevant dependencies by optimizing another goal: the logmarginal pseudo-probability of the results of a single markup function \u03bbj, i.e., conditioned by the results of the other \u03bb\\ j, by using \"1 regularization to achieve sparseness; the objective Isarg minimum requirement \u2212 minimum requirement \u2212 1 (4) = Arg minimum requirement \u2212 m minimum requirement \u2212 i minimum requirement \u2212 1, where > 0 is a hyperparameter; by conditioning on all other markup functions in each term protocol."}, {"heading": "3.2. Implementation", "text": "We implement our method as algorithm 1, a stochastic gradient descent (SGD). At each step of the descent, the gradient (5) is estimated for a single data point that can be calculated in a closed form. SGD has two advantages: First, it only requires first-order gradient information. Other methods for \"1-regulated regression\" such as in-point methods (Koh et al., 2007) usually require a second-order calculation. Second, the observations can be processed step by step. Since data programming is based on unlabeled data, which is often abundant, scalability is crucial. To implement \"1 regularization as part of the SGD, we use an online method with truncated gradients (Langford et al., 2009). In practice, we find that the only parameter that needs to be matched is the threshold and regularization algorithm 1 structuring."}, {"heading": "4. Analysis", "text": "We provide guarantees for the probability that algorithm 1 will successfully restore the exact dependency structure. First, we provide a general recovery guarantee for all kinds of possible dependencies, including both pairs and higher dependencies. In this case, as a result of our general result, we show that the number of samples required is sublinear in the number of possible dependencies, specifically O (n log n).Previence analyses for the parent dependencies and pair-wise correlations are not transferable because the problem is no longer convex. For example, the analysis of an analog method is for parent models (Ravikumar et al, 2010)."}, {"heading": "5. Experiments", "text": "We implement our method as part of the open source framework Snorkel1 and evaluate it in three ways: First, we measure how the probability of returning the exact correlation structure is influenced by the problem parameters by using synthetic data, which confirms our analysis that sample complexity is sublinear in terms of the number of possible dependencies. In fact, we find that sample complexity is lower in practice than the theoretically guaranteed rate, which corresponds to the rate of fully monitored structural learning seen in practice. Second, we compare our method with estimating structures by means of parameter learning across all possible dependencies. Using synthetic data, we show that our method is 100 x faster and more accurate, selecting an average of 1 / 4 as many minor correlations. Third, we apply our method to real-world applications created with the help of data programming, such as the information extraction from PubMed Journal abstracts, explaining how they do not have a multiplicity function between these hardware and the specification sheets."}, {"heading": "5.1. Sample Complexity", "text": "We test how the probability that algorithm 1 returns the correct correlation structure depends on the actual distribution. Our analysis in Section 4 guarantees that the complexity of the sample at worst grows by the order O (n log n) for n-indicators. In practice, we find that structural learning is better than this guaranteed rate, depending on the number of true correlations and logarithmic on the number of possible correlations. This is consistent with the observed behavior for fully monitored structural learning for issuing indicators (Ravikumar et al., 2010), which is also more accurate than the best known theoretical guarantees. To demonstrate this behavior, we try to restore the true dependence structure of indicators on a number of defined assimilation values: = 750 \u00b7 g \u00b7 d-D-Log n (9), where d-dependencies are the maximum number of dependencies that affect each individual indicators Indicators Indicators Indicators-Indicators-Indicators-Indicators-Indicators-Indicators-Indicators-Indicators-Indicators-Indicators-Indicators-Indicators-Indicators-Indicators-Indicators-Indicators-Indicators-Indicators-Indicators-Indicators-Indicators-Indicators-Indicators-Indicators-Indicators-Dependencies-Indicators-Indicators-Indicators-Indicators-Indicators-Dependencies-Indicators-Indicators-Indicators-Indicators-Indicators-Indicators-Indicators-Dependencies-Indicators-Indicators-Indicators-Indicators-Indicators-Indicators-Indicators-Indicators-Indicators-Indicators-Indicators-Indicators-Indicators-Indicators-Indicators-Indicators-Indicators-Indicators-Indicators-Indicators-Indicators-Indicators-Indicators-Indicators-Indicators-Indicators-Indicators-Indicators-Indicators-Indicators-Indicators-Indicators-Indicators-Indicat"}, {"heading": "5.2. Comparison with Maximum Likelihood", "text": "Next, we compare algorithm 1 with an alternative approach. However, without an efficient method of learning structures, one could maximize the marginal probability of the observations while taking into account all possible dependencies. To measure the benefit of maximizing the marginal pseudo-probability, we compare its performance with an analog maximum probability routine that also uses stochastic gradient descent, but instead uses Gibbs scanning to estimate the intractable gradient of the object. We generate different distributions via n-labeling functions by selecting 0.05 pairs of label functions with the probability to correlate. Again, the strength of the correlation with VictorCorjk = 0.25 is determined and the accuracy is determined with Medicare Acc j = 1.0. We generate 100 distributions for n functions {25, 30, 35,., 100}. For each distribution, we generate 10,000 samples and try to restore the true correlation. We first compare the runtime between the two methods."}, {"heading": "5.3. Real-World Applications", "text": "We evaluate our method on several real information extraction applications by comparing the performance of data programming with dependencies selected by our method with the conditionally independent model (Table 1). In the data programming method, users express a variety of weak monitoring rules and sources, such as regular expression patterns, remote monitoring of words and existing knowledge bases, and other heuristics as labeling functions. Due to the noisy and overlapping nature of these labeling functions, correlations arise. This correlation structure leads to an average improvement of 1.5 F1 points, with structured information from unstructured texts being a challenging task to study in the context of weak supervision. Alfonseca et al., 2012; Ratner et al., 2016; we consider three tasks: gathering specific diseases from the scientific literature (Disease Tagging)."}, {"heading": "5.4. Accelerating Application Development", "text": "Our method is largely motivated by the new programming model introduced by weak monitoring and the novel hurdles developers face. For example, in the disease management application above, we observed that developers were significantly slower to use the rich disease ontologies and match heuristics that were available to them without introducing too many dependencies between their labeling functions. Additionally, we observed that developers fell into significant pitfalls due to unnoticed correlations between their weak surveillance sources. In the application of one employee, for each labeling function that referenced the words in a sentence, a corresponding labeling function was pointed to the lemmas, which were often identical, and this resulted in significant deterioration in performance."}, {"heading": "6. Related Work", "text": "As a matter of fact, most of them will be able to play by the rules they have imposed on themselves. - Most of them are able to play by the rules. - Most of them are able to play by the rules. - Most of them are not able to play by the rules. - Most of them are not able to play by the rules. - Most of them are not able to play by the rules. - Most of them are not able to play by the rules. - Most of them are able to play by the rules. - Most of them are able to play by the rules. - Most of them are able to play by the rules themselves."}, {"heading": "7. Conclusion and Future Directions", "text": "We have shown that learning the structure of a generative model leads to higher quality results in data programming. Our method of structural learning is also 100 times faster than an approach with maximum probability. If data programming and other forms of weak monitoring are to facilitate the development of machine learning tools, selecting precise structures for generative models with minimal user intervention is a necessary skill. Interesting questions remain. Can the guarantee of Theorem 1 for dependencies of higher order be tightened to match the paired case of episode 2? Initial experiments show that they converge at a similar speed in practice."}, {"heading": "Acknowledgements", "text": "Thanks to Christopher De Sa for helpful discussions, and to Henry Ehrenberg and Sen Wu for supporting experiments. We thank the Defense Advanced Research Projects Agency (DARPA) for supporting the SIMPLEX program under the number N66001-15-C-4043, the DARPA D3M program under the number FA8750-17-2-0095, the CAREER Award of the National Science Foundation (NSF) under the number IIS-1353606, the Office of Naval Research (ONR) under the number N000141210041 and the number N000141310129, a Sloan Research Fellowship, the Moore Foundation, an Okawa Research Grant, Toshiba and Intel. Any opinions, findings, conclusions or recommendations expressed in this material are those of the authors and do not necessarily reflect the views of DARPA, NSF, ONR or the U.S. government."}, {"heading": "A. Proofs", "text": "In this appendix we show that the negative marginal log-pseudolikelihood, that the negative marginal log-pseudolikelihood, we show that the negative marginal log-pseudolikelihood, we show that the negative log-pseudolikelihood, we show that the negative marginal log-pseudolikelihood, we show that the negative log-pseudolikelihood, we show that the negative log-pseudolikelihood, we show that the pseudo-pseudo-pseudo-we show that the negative log-pseudolikelihood, we show that the negative log-pseudolikelihood, we show that the negative log-pseudolikelihood, we show that the pseudodo-pseudo-pseudo-pseudo, we show that the negative log-pseudolikelihood, we show that the negative log-pseudolilikelihood, we show that the pseudolog-pseudolilikelihood, we show that the celdolikelihood, we show that the celdolikelihood, we show that we show the negative log-pseudolikelihood, we show that we show the pseudolikelihood-pseudolikelihood, we show that we show the negative log-pseudolikelihood, we show that we show the negative log-pseudolilikelihood, we show that we show that we show the negative log-pseudolilikelihood, we show that we show the negative log-pseudolikelihood, we show that we show the pseudo-pseudo-pseudo-pseudo-pseudo-pseudolikelihood, we show that we show the negative log-pseudo-pseudo-pseudo-pseudo-pseudo-pseudo-pseudolikelihood, we show that the negative log-pseudo-pseudo-we-pseudo-pseudo-we, we show that the negative log-pseudolikelihood, we show that the negative log-pseudo-pseudo-pseudo-we, we show that we show that the negative log-pseu"}], "references": [{"title": "Pattern learning for relation extraction with a hierarchical topic model", "author": ["E. Alfonseca", "K. Filippova", "Delort", "J.-Y", "G. Garrido"], "venue": "In Annual Meeting of the Association for Computational Linguistics (ACL),", "citeRegEx": "Alfonseca et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Alfonseca et al\\.", "year": 2012}, {"title": "Learning to extract relations from the Web using minimal supervision", "author": ["R.C. Bunescu", "R.J. Mooney"], "venue": "In Annual Meeting of the Association for Computational Linguistics (ACL),", "citeRegEx": "Bunescu and Mooney,? \\Q2007\\E", "shortCiteRegEx": "Bunescu and Mooney", "year": 2007}, {"title": "The Dantzig selector: Statistical estimation when p is much larger than n", "author": ["E. Candes", "T. Tao"], "venue": "The Annals of Statistics,", "citeRegEx": "Candes and Tao,? \\Q2007\\E", "shortCiteRegEx": "Candes and Tao", "year": 2007}, {"title": "Latent variable graphical model selection via convex optimization", "author": ["V. Chandrasekaran", "P.A. Parrilo", "A.S. Willsky"], "venue": "The Annals of Statistics,", "citeRegEx": "Chandrasekaran et al\\.,? \\Q1935\\E", "shortCiteRegEx": "Chandrasekaran et al\\.", "year": 1935}, {"title": "Constructing biological knowledge bases by extracting information from text sources", "author": ["M. Craven", "J. Kumlien"], "venue": "In International Conference on Intelligent Systems for Molecular Biology (ISMB),", "citeRegEx": "Craven and Kumlien,? \\Q1999\\E", "shortCiteRegEx": "Craven and Kumlien", "year": 1999}, {"title": "Aggregating crowdsourced binary ratings", "author": ["N. Dalvi", "A. Dasgupta", "R. Kumar", "V. Rastogi"], "venue": "In International World Wide Web Conference (WWW),", "citeRegEx": "Dalvi et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Dalvi et al\\.", "year": 2013}, {"title": "Maximum likelihood estimation of observer error-rates using the EM algorithm", "author": ["A.P. Dawid", "A.M. Skene"], "venue": "Journal of the Royal Statistical Society C,", "citeRegEx": "Dawid and Skene,? \\Q1979\\E", "shortCiteRegEx": "Dawid and Skene", "year": 1979}, {"title": "Optimally sparse representation in general (nonorthogonal) dictionaries via ` minimization", "author": ["D. Donoho", "M. Elad"], "venue": "Proceedings of the National Academy of Sciences of the USA,", "citeRegEx": "Donoho and Elad,? \\Q2003\\E", "shortCiteRegEx": "Donoho and Elad", "year": 2003}, {"title": "Look ma, no hands: Analyzing the monotonic feature abstraction for text classification", "author": ["D. Downey", "O. Etzioni"], "venue": "In Neural Information Processing Systems (NIPS),", "citeRegEx": "Downey and Etzioni,? \\Q2008\\E", "shortCiteRegEx": "Downey and Etzioni", "year": 2008}, {"title": "Learning hidden variable networks: The information bottleneck approach", "author": ["G. Elidan", "N. Friedman"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Elidan and Friedman,? \\Q2005\\E", "shortCiteRegEx": "Elidan and Friedman", "year": 2005}, {"title": "Training products of experts by minimizing contrastive divergence", "author": ["G.E. Hinton"], "venue": "Neural Computation,", "citeRegEx": "Hinton,? \\Q2002\\E", "shortCiteRegEx": "Hinton", "year": 2002}, {"title": "Knowledge-based weak supervision for information extraction of overlapping relations", "author": ["R. Hoffmann", "C. Zhang", "X. Ling", "L. Zettlemoyer", "D.S. Weld"], "venue": "In Annual Meeting of the Association for Computational Linguistics (ACL),", "citeRegEx": "Hoffmann et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Hoffmann et al\\.", "year": 2011}, {"title": "Comprehensive and reliable crowd assessment algorithms", "author": ["M. Joglekar", "H. Garcia-Molina", "A. Parameswaran"], "venue": "In International Conference on Data Engineering (ICDE),", "citeRegEx": "Joglekar et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Joglekar et al\\.", "year": 2015}, {"title": "An interior-point method for large-scale `1-regularized logistic regression", "author": ["K. Koh", "S.J. Kim", "S. Boyd"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Koh et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Koh et al\\.", "year": 2007}, {"title": "Sparse online learning via truncated gradient", "author": ["J. Langford", "L. Li", "T. Zhang"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Langford et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Langford et al\\.", "year": 2009}, {"title": "High-dimensional graphs and variable selection with the lasso", "author": ["N. Meinshausen", "P. B\u00fchlmann"], "venue": "The Annals of Statistics,", "citeRegEx": "Meinshausen and B\u00fchlmann,? \\Q2006\\E", "shortCiteRegEx": "Meinshausen and B\u00fchlmann", "year": 2006}, {"title": "Google\u2019s hand-fed AI now gives answers, not just search results, 2016", "author": ["C. Metz"], "venue": "Wired [Online; posted 29November-2016]", "citeRegEx": "Metz,? \\Q2016\\E", "shortCiteRegEx": "Metz", "year": 2016}, {"title": "Distant supervision for relation extraction without labeled data", "author": ["M. Mintz", "S. Bills", "R. Snow", "D. Jurafsky"], "venue": "In Annual Meeting of the Association for Computational Linguistics (ACL),", "citeRegEx": "Mintz et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Mintz et al\\.", "year": 2009}, {"title": "Feature selection, l1 vs. l2 regularization, and rotational invariance", "author": ["A.Y. Ng"], "venue": "In International Conference on Machine Learning (ICML),", "citeRegEx": "Ng,? \\Q2004\\E", "shortCiteRegEx": "Ng", "year": 2004}, {"title": "Ranking and combining multiple predictors without labeled data", "author": ["F. Parisi", "F. Strino", "B. Nadler", "Y. Kluger"], "venue": "Proceedings of the National Academy of Sciences of the USA,", "citeRegEx": "Parisi et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Parisi et al\\.", "year": 2014}, {"title": "Grafting: Fast, incremental feature selection by gradient descent in function space", "author": ["S. Perkins", "K. Lacker", "J. Theiler"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Perkins et al\\.,? \\Q2003\\E", "shortCiteRegEx": "Perkins et al\\.", "year": 2003}, {"title": "Data programming: Creating large training sets, quickly", "author": ["A. Ratner", "C. De Sa", "S. Wu", "D. Selsam", "C. R\u00e9"], "venue": "In Neural Information Processing Systems (NIPS),", "citeRegEx": "Ratner et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Ratner et al\\.", "year": 2016}, {"title": "Highdimensional Ising model selection using `1-regularized logistic regression", "author": ["P. Ravikumar", "M.J. Wainwright", "J.D. Lafferty"], "venue": "The Annals of Statistics,", "citeRegEx": "Ravikumar et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Ravikumar et al\\.", "year": 2010}, {"title": "Modeling relations and their mentions without labeled text", "author": ["S. Riedel", "L. Yao", "A. McCallum"], "venue": "In European Conference on Machine Learning and Knowledge Discovery in Databases (ECML PKDD),", "citeRegEx": "Riedel et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Riedel et al\\.", "year": 2010}, {"title": "Feature-based models for improving the quality of noisy training data for relation extraction", "author": ["B. Roth", "D. Klakow"], "venue": "In Conference on Information and Knowledge Management (CIKM),", "citeRegEx": "Roth and Klakow,? \\Q2013\\E", "shortCiteRegEx": "Roth and Klakow", "year": 2013}, {"title": "Combining generative and discriminative model scores for distant supervision", "author": ["B. Roth", "D. Klakow"], "venue": "In Conference on Empirical Methods on Natural Language Processing (EMNLP),", "citeRegEx": "Roth and Klakow,? \\Q2013\\E", "shortCiteRegEx": "Roth and Klakow", "year": 2013}, {"title": "Incremental knowledge base construction using DeepDive", "author": ["J. Shin", "S. Wu", "F. Wang", "C. De Sa", "C. Zhang", "C. R\u00e9"], "venue": "Proceedings of the VLDB Endowment,", "citeRegEx": "Shin et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Shin et al\\.", "year": 2015}, {"title": "Reducing wrong labels in distant supervision for relation extraction", "author": ["S. Takamatsu", "I. Sato", "H. Nakagawa"], "venue": "In Annual Meeting of the Association for Computational Linguistics (ACL),", "citeRegEx": "Takamatsu et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Takamatsu et al\\.", "year": 2012}, {"title": "Regression shrinkage and selection via the lasso", "author": ["R. Tibshirani"], "venue": "Journal of the Royal Statistical Society B,", "citeRegEx": "Tibshirani,? \\Q1996\\E", "shortCiteRegEx": "Tibshirani", "year": 1996}, {"title": "Just relax: Convex programming methods for identifying sparse signals in noise", "author": ["J. Tropp"], "venue": "IEEE Transactions on Information Theory,", "citeRegEx": "Tropp,? \\Q2006\\E", "shortCiteRegEx": "Tropp", "year": 2006}, {"title": "Sharp thresholds for high-dimensional and noisy sparsity recovery using `1-constrained quadratic programming (lasso)", "author": ["M.J. Wainwright"], "venue": "IEEE Transactions on Information Theory,", "citeRegEx": "Wainwright,? \\Q2009\\E", "shortCiteRegEx": "Wainwright", "year": 2009}, {"title": "DimmWitted: A study of mainmemory statistical analytics", "author": ["C. Zhang", "C. R\u00e9"], "venue": "Proceedings of the VLDB Endowment,", "citeRegEx": "Zhang and R\u00e9,? \\Q2014\\E", "shortCiteRegEx": "Zhang and R\u00e9", "year": 2014}, {"title": "On model selection consistency of lasso", "author": ["P. Zhao", "B. Yu"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Zhao and Yu,? \\Q2006\\E", "shortCiteRegEx": "Zhao and Yu", "year": 2006}, {"title": "Grafting-Light: Fast, incremental feature selection and structure learning of Markov random fields", "author": ["J. Zhu", "N. Lao", "E.P. Xing"], "venue": "In International Conference on Knowledge Discovery and Data Mining (KDD),", "citeRegEx": "Zhu et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Zhu et al\\.", "year": 2010}], "referenceMentions": [{"referenceID": 16, "context": "Collecting these labels is often prohibitively expensive, especially when specialized domain expertise is required, and major technology companies are investing heavily in hand-curating labeled training data (Metz, 2016; Eadicicco, 2017).", "startOffset": 208, "endOffset": 237}, {"referenceID": 0, "context": "The structure of such generative models directly affects the inferred labels, and prior work assumes that the structure is user-specified (Alfonseca et al., 2012; Takamatsu et al., 2012; Roth & Klakow, 2013b; Ratner et al., 2016).", "startOffset": 138, "endOffset": 229}, {"referenceID": 27, "context": "The structure of such generative models directly affects the inferred labels, and prior work assumes that the structure is user-specified (Alfonseca et al., 2012; Takamatsu et al., 2012; Roth & Klakow, 2013b; Ratner et al., 2016).", "startOffset": 138, "endOffset": 229}, {"referenceID": 21, "context": "The structure of such generative models directly affects the inferred labels, and prior work assumes that the structure is user-specified (Alfonseca et al., 2012; Takamatsu et al., 2012; Roth & Klakow, 2013b; Ratner et al., 2016).", "startOffset": 138, "endOffset": 229}, {"referenceID": 21, "context": ") We ground our work in one framework, data programming (Ratner et al., 2016), that generalizes many approaches in the literature.", "startOffset": 56, "endOffset": 77}, {"referenceID": 10, "context": "a small step after each sample of each variable \u039bij or yi, similarly to contrastive divergence (Hinton, 2002).", "startOffset": 95, "endOffset": 109}, {"referenceID": 13, "context": "Other methods for `1-regularized regression like interior-point methods (Koh et al., 2007) usually require computing second-order information.", "startOffset": 72, "endOffset": 90}, {"referenceID": 14, "context": "To implement `1 regularization as part of SGD, we use an online truncated gradient method (Langford et al., 2009).", "startOffset": 90, "endOffset": 113}, {"referenceID": 22, "context": "For example, analysis of an analogous method for supervised Ising models (Ravikumar et al., 2010) relies on Lagrangian duality and a tight duality gap, which does not hold for our estimation problem.", "startOffset": 73, "endOffset": 97}, {"referenceID": 22, "context": "The rate of Theorem 1 rate is therefore not directly comparable to that of Ravikumar et al. (2010), which applies to Ising models with pairwise dependencies.", "startOffset": 75, "endOffset": 99}, {"referenceID": 22, "context": "Whereas the rate of Corollary 2 depends on the maximum number of dependencies that could affect a variable in the model class, the rate of Ravikumar et al. (2010) depends cubically on the maximum number of dependencies that actually affect any variable in the true model and only logarithmically in the maximum possible degree.", "startOffset": 139, "endOffset": 163}, {"referenceID": 22, "context": "This matches the observed behavior for fully supervised structure learning for Ising models (Ravikumar et al., 2010), which is also tighter than the best known theoretical guarantees.", "startOffset": 92, "endOffset": 116}, {"referenceID": 0, "context": "Extracting structured information from unstructured text by identifying mentioned entities and relations is a challenging task that is well studied in the context of weak supervision (Bunescu & Mooney, 2007; Alfonseca et al., 2012; Ratner et al., 2016).", "startOffset": 183, "endOffset": 252}, {"referenceID": 21, "context": "Extracting structured information from unstructured text by identifying mentioned entities and relations is a challenging task that is well studied in the context of weak supervision (Bunescu & Mooney, 2007; Alfonseca et al., 2012; Ratner et al., 2016).", "startOffset": 183, "endOffset": 252}, {"referenceID": 28, "context": "The lasso (Tibshirani, 1996), linear regression with `1 regularization, is a classic technique.", "startOffset": 10, "endOffset": 28}, {"referenceID": 29, "context": "`1 regularization has also been used as a prior for compressed sensing (e.g., Donoho & Elad, 2003; Tropp, 2006; Wainwright, 2009).", "startOffset": 71, "endOffset": 129}, {"referenceID": 30, "context": "`1 regularization has also been used as a prior for compressed sensing (e.g., Donoho & Elad, 2003; Tropp, 2006; Wainwright, 2009).", "startOffset": 71, "endOffset": 129}, {"referenceID": 27, "context": "The lasso (Tibshirani, 1996), linear regression with `1 regularization, is a classic technique. Zhao & Yu (2006) showed that the lasso is a consistent structure estimator.", "startOffset": 11, "endOffset": 113}, {"referenceID": 18, "context": "Ng (2004) showed that `1-regularized logistic regression has sample complexity logarithmic in the number of features.", "startOffset": 0, "endOffset": 10}, {"referenceID": 20, "context": "Other techniques for learning the structure of graphical models include grafting (Perkins et al., 2003; Zhu et al., 2010) and the information bottleneck approach for learning Bayesian networks with latent variables (Elidan & Friedman, 2005).", "startOffset": 81, "endOffset": 121}, {"referenceID": 33, "context": "Other techniques for learning the structure of graphical models include grafting (Perkins et al., 2003; Zhu et al., 2010) and the information bottleneck approach for learning Bayesian networks with latent variables (Elidan & Friedman, 2005).", "startOffset": 81, "endOffset": 121}, {"referenceID": 17, "context": "Treating labels from a single heuristic source as gold labels is called distant supervision (Craven & Kumlien, 1999; Mintz et al., 2009).", "startOffset": 92, "endOffset": 136}, {"referenceID": 23, "context": "Some methods use multi-instance learning to reduce the noise in a distant supervision source (Riedel et al., 2010; Hoffmann et al., 2011).", "startOffset": 93, "endOffset": 137}, {"referenceID": 11, "context": "Some methods use multi-instance learning to reduce the noise in a distant supervision source (Riedel et al., 2010; Hoffmann et al., 2011).", "startOffset": 93, "endOffset": 137}, {"referenceID": 26, "context": "Previous methods that support heuristics for weak supervision (e.g., Bunescu & Mooney, 2007; Shin et al., 2015) do not model the noise inherent in these sources.", "startOffset": 62, "endOffset": 111}, {"referenceID": 12, "context": "Many methods for crowdsourcing estimate the accuracy of workers without hand-labeled data (e.g., Dalvi et al., 2013; Joglekar et al., 2015; Zhang et al., 2016).", "startOffset": 90, "endOffset": 159}, {"referenceID": 14, "context": "Most similar to our proposed estimator, Ravikumar et al. (2010) propose a fully supervised pseudolikelihood estimator for Ising models.", "startOffset": 40, "endOffset": 64}, {"referenceID": 2, "context": "Also related is the work of Chandrasekaran et al. (2012), which considers learning the structure of Gaussian graphical models with latent variables.", "startOffset": 28, "endOffset": 57}, {"referenceID": 0, "context": "Others use hierarchical topic models to generate additional training data for weak supervision, but they do not support user-provided heuristics (Alfonseca et al., 2012; Takamatsu et al., 2012; Roth & Klakow, 2013a;b). Previous methods that support heuristics for weak supervision (e.g., Bunescu & Mooney, 2007; Shin et al., 2015) do not model the noise inherent in these sources. Also, Downey & Etzioni (2008) showed that PAC learning is possible without hand-labeled data if the features monotonically order data by class probability.", "startOffset": 146, "endOffset": 411}, {"referenceID": 0, "context": "Others use hierarchical topic models to generate additional training data for weak supervision, but they do not support user-provided heuristics (Alfonseca et al., 2012; Takamatsu et al., 2012; Roth & Klakow, 2013a;b). Previous methods that support heuristics for weak supervision (e.g., Bunescu & Mooney, 2007; Shin et al., 2015) do not model the noise inherent in these sources. Also, Downey & Etzioni (2008) showed that PAC learning is possible without hand-labeled data if the features monotonically order data by class probability. Estimating the accuracy of multiple label sources without a gold standard is a classic problem (Dawid & Skene, 1979), and many proposed approaches are generalized in the data programming framework. Parisi et al. (2014) proposed a spectral approach to estimating the accuracy of members of classifier ensembles.", "startOffset": 146, "endOffset": 756}], "year": 2017, "abstractText": "Curating labeled training data has become the primary bottleneck in machine learning. Recent frameworks address this bottleneck with generative models to synthesize labels at scale from weak supervision sources. The generative model\u2019s dependency structure directly affects the quality of the estimated labels, but selecting a structure automatically without any labeled data is a distinct challenge. We propose a structure estimation method that maximizes the `1regularized marginal pseudolikelihood of the observed data. Our analysis shows that the amount of unlabeled data required to identify the true structure scales sublinearly in the number of possible dependencies for a broad class of models. Simulations show that our method is 100\u00d7 faster than a maximum likelihood approach and selects 1/4 as many extraneous dependencies. We also show that our method provides an average of 1.5 F1 points of improvement over existing, user-developed information extraction applications on real-world data such as PubMed journal abstracts.", "creator": "LaTeX with hyperref package"}}}