{"id": "1703.00887", "review": {"conference": "ICML", "VERSION": "v1", "DATE_OF_SUBMISSION": "2-Mar-2017", "title": "How to Escape Saddle Points Efficiently", "abstract": "This paper shows that a perturbed form of gradient descent converges to a second-order stationary point in a number iterations which depends only poly-logarithmically on dimension (i.e., it is almost \"dimension-free\"). The convergence rate of this procedure matches the well-known convergence rate of gradient descent to first-order stationary points, up to log factors. When all saddle points are non-degenerate, all second-order stationary points are local minima, and our result thus shows that perturbed gradient descent can escape saddle points almost for free. Our results can be directly applied to many machine learning applications, including deep learning. As a particular concrete example of such an application, we show that our results can be used directly to establish sharp global convergence rates for matrix factorization. Our results rely on a novel characterization of the geometry around saddle points, which may be of independent interest to the non-convex optimization community.", "histories": [["v1", "Thu, 2 Mar 2017 18:35:24 GMT  (693kb)", "http://arxiv.org/abs/1703.00887v1", null]], "reviews": [], "SUBJECTS": "cs.LG math.OC stat.ML", "authors": ["chi jin", "rong ge 0001", "praneeth netrapalli", "sham m kakade", "michael i jordan"], "accepted": true, "id": "1703.00887"}, "pdf": {"name": "1703.00887.pdf", "metadata": {"source": "CRF", "title": "How to Escape Saddle Points Efficiently", "authors": ["Chi Jin", "Rong Ge", "Praneeth Netrapalli", "Sham M. Kakade", "Michael I. Jordan"], "emails": ["chijin@cs.berkeley.edu", "rongge@cs.duke.edu", "praneeth@microsoft.com", "sham@cs.washington.edu", "jordan@cs.berkeley.edu"], "sections": [{"heading": null, "text": "ar Xiv: 170 3.00 887v 1 [cs.L G] 2M arOur results can be applied directly to many machine learning applications, including deep learning. As a concrete example of such an application, we show that our results can be used directly to determine sharp global convergence rates for matrix factorization. Our results are based on a novel characterization of saddle geometry that could be of independent interest to the non-convex optimization community."}, {"heading": "1 Introduction", "text": "This year, it is only a matter of time before the first two candidates are shortlisted."}, {"heading": "1.1 Our Contributions", "text": "This paper presents the first sharp analysis that shows that (disturbed) gradient descent finds an approximate second-order stationary point in most polylog (d) iterations, efficiently escaping all saddle points. Our main technical contributions are as follows: \u2022 For \"Gradient Lipschitz\" functions (possibly non-convex), \"Gradient Descent\" with corresponding perturbations, a \"Gradient Descent\" finds a stationary second-order point in O iterations (f (x0) -f (2) iterations. This rate corresponds to the known convergence rate from \"Gradient Descent\" to \"Firstorder stationary points up to log factors.\" \u2022 Under strict saddle condition (see Assumption A2), this convergence result applies directly to locating local minima. This means that \"Gradient Descent\" can escape all saddle points with only logarithmic overhang at runtime. \u2022 If the function has a very precise attribute, see 2), this condense can be achieved."}, {"heading": "1.2 Related Work", "text": "This year it has come to the point that it has never come as far as this year."}, {"heading": "2 Preliminaries", "text": "In this section, we will first present our notation and then present some definitions and existing optimization results that will be used later."}, {"heading": "2.1 Notation", "text": "We use upper case letters A, B to denote matrices, and lower case letters x, y to denote vectors. Aij means the (i, j) th entry of matrix A. For vectors, we use to denote the \"2 standard,\" and for matrices, we use to denote the spectral standard or the Frobenius standard. We use \u03c3max (\u00b7), \u03c3min (\u00b7), \u03c3i (\u00b7) to denote the largest, smallest, and i-th singular value, and \u03bbmax (\u00b7), \u03bbmin (\u00b7), \u03bbi (\u00b7) for corresponding eigenvalues.For a function f: Rd \u2192 R, we use f (\u00b7) and \u04412f (\u00b7) to denote the largest, smallest, and i-th singular value, and f: to denote the global minimum of f (\u00b7)."}, {"heading": "2.2 Gradient Descent", "text": "The theory of the gradient is often a starting point to be the study of the gradient. Assume (\"We are strong.\") The theory of the gradient is often a starting point to be the function of the gradient. Assume (\"We are strong.\") The theory of the gradient shows that gradient develops into a global optimum (see e.g.).The theory of the gradient is often a starting point to be the starting point of the gradient. Assume (\"We are strongly dependent on these two properties.\" The theory of the gradient shows that gradient converts into a global optimum (see e.g.).The theory of the gradient is often a starting point for the study of the gradient \"We are strong.\" The theory of the gradient shows that gradient has evolved into a global optimum x (see e.g.).The theory of the gradient is a global optimum."}, {"heading": "3 Main Result", "text": "In this section, we show that it is possible to modify gradients in a simple way so that the resulting algorithms are demonstrably quickly converted to a second gradient. - The algorithms are based on gradients-gradations-gradations-gradations-gradations-gradations-gradations-gradations-gradations-gradations-gradations-gradations-gradations-gradations-gradations-gradations-gradations-gradations-gradations-gradations-gradations-gradations-gradations-gradations-gradations-gradations-gradations-gradations-gradations-gradations-gradations-gradations-gradations-gradations-gradations-gradations-gradations-gradations-gradations-gradations-gradations-gradations-gradations-gradations-gradations-gradations-gradations-gradations-gradations-gradations-gradations-gradations-gradations-gradations-gradations-gradations-gradations-graddings-graddings-gradations-gradgradations-gradations-graddings"}, {"heading": "3.1 Functions with Strict Saddle Property", "text": "In many real-world applications, objective functions still allow the property that all saddle points are strict [Ge et al., 2015, Sun et al., 2016a, b, Bhojanapalli et al., 2016, Ge et al., 2016]. In this case, all fixed points of second order are local minimums, and therefore convergence to fixed points of second order (theorem 3) is equivalent to convergence to fixed minimages.To formally determine this result, we are introducing a robust version of strict saddle ownership [cf. Ge et al., 2015]: Adoption A2. Function f (\u00b7) is (ctua) -strict saddle. That is, for each x, at least one of the following values applies: \u2022 Adoption threshold f (x)."}, {"heading": "3.2 Functions with Strong Local Structure", "text": "The convergence rate in Theorem 3 is not always the same as the global convergence parameters. (...) However, we are worse than the rate of Theorem 1 due to the lack of strong convergence. (...) Although global convergence is not kept in the non-convex environment that is our focus, objective convergence can lead to local minimal structures. (...) Such a property ensures that such convergence ensures a local form of convergence and strong convergence. (...) In a pronounced neighborhood of local minimal convergence, it can lead to much faster convergence (...). (...) The function f (...) is strongly convex, and \u03b2-smooth.Here we use different letters \u03b2 to local convergence (...)."}, {"heading": "4 Example \u2014 Matrix Factorization", "text": "As a simple example to illustrate how we can apply our general theorems to specific non-convex optimization problems, we consider a symmetric, low matrix factoring problem, based on the following objective function: min U-Rd \u00b7 rf (U) = 1 2, 2, 2, (2), where M-Rd \u00b7 d, (2), we assume that the global minimum of the function is zero, which is reached at V-RD = 2, where TDT is the SVD of the symmetric real matrix M. The following two lemmas show that the objective function in Eq (2) is the geometric assumptions A1, A2, and A3.b."}, {"heading": "5 Proof Sketch for Theorem 3", "text": "In this section, we will present the key ideas underlying the main result of this work (Theorem 3). First, we will discuss the correctness of Theorem 3 taking into account two important intermediate objectives, and then we will turn to the main dilemma, which is that a gradient can quickly escape from saddle points. Full proof of all these results will be presented in Appendix A. During this section, we will use \u03b7, r, gthres, fthres and tthres as defined in Algorithm 2."}, {"heading": "5.1 Exploiting Large Gradient or Negative Curvature", "text": "Remember that a stationary point of second order is a point with a small gradient, and that the Hesse does not have a significant negative eigenvalue. Suppose we are currently at a point that is not a stationary point of second order; i.e., it does not fulfill the above-mentioned characteristics. There are two possibilities: 1. The gradient is large: 1. The gradient is large: 2. The functional value decreases in both scenarios by the gradient value. 2. Lemma 9 (gradient) and 3. Min. (2f (xt)). The following two lemmata deal with these two cases. They guarantee that the gradient-gradient value decreases in both scenarios."}, {"heading": "5.2 Main Lemma: Escaping from Saddle Points Quickly", "text": "The proof of Lemma 9 is simple and follows from traditional analyses. The most important technical contribution of this work is the proof of Lemma 10, which gives a new characterization of the geometry around the saddle points. - Consider a point x, which fulfills the requirements of Lemma 10 (x), but we can come to x0 as from a uniform distribution over Bx (r), which we call the Perturbation Ball. - We can divide this Perturbation Ball Bx (r) into two separate regions: (1) an escaping region Xescape, which consists of all points x (r), whose functional value decreases after at least fthres steps; (2) a frozen region Xp (r)."}, {"heading": "6 Conclusion", "text": "This paper presents the first (nearly) dimension-free result for gradient descent in a general non-convex environment. We present a general convergence result and show how it can be further enhanced in combination with other structures such as strict saddle conditions and / or local regularity / convexity. There are still many outstanding problems associated with this. Firstly, it is worth investigating whether gradient descent still allows for similar sharp convergence results. Another important question is whether similar techniques can be applied to accelerated gradient descent. We hope that this result could serve as a first step towards a more general theory with strong, almost dimension-free guarantees for non-convex optimization."}, {"heading": "A Detailed Proof of Main Theorem", "text": "In this section, we will give detailed evidence for the most important theorems (+ 1). We will first cite two key lemmas that show how we can make progress when the gradient is large or near a saddle point, and show how the most important theorems are derived from the two lemmas. Then, we will focus on the new technique that either has a large gradient or is near a saddle point. This idea is similar to previous work (e.g. [Ge et al., 2015]. We will first show a standard theorem Lemma that shows that the gradient is large, then we will make progress in the value.Lemma 12 (Lemma 9) function. Assume f () meets A1, then for gradients with increasing size."}, {"heading": "C Geometric Structures of Matrix Factorization Problem", "text": "In this section we will examine the global geometric structures of the matrix factorization problem. (...) These properties are summarized in terms of terms 6 and 7. (...) Such structures allow us to apply our most important theorems and achieve rapid convergence (as shown in Theorem 8). (...) Note: Our main results are for functions f (...) whose input x is a vector. (...) We still write everything in matrix form (without explicit vectorization), while the reader should be able to vectorize everything we use. (...) Recall for vectors we use the 2-norm, and for matrices we use the 2-norm. (...)"}], "references": [{"title": "Finding approximate local minima for nonconvex optimization in linear time", "author": ["Naman Agarwal", "Zeyuan Allen-Zhu", "Brian Bullins", "Elad Hazan", "Tengyu Ma"], "venue": "arXiv preprint arXiv:1611.01146,", "citeRegEx": "Agarwal et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Agarwal et al\\.", "year": 2016}, {"title": "Global optimality of local search for low rank matrix recovery", "author": ["Srinadh Bhojanapalli", "Behnam Neyshabur", "Nathan Srebro"], "venue": "arXiv preprint arXiv:1605.07221,", "citeRegEx": "Bhojanapalli et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Bhojanapalli et al\\.", "year": 2016}, {"title": "Phase retrieval via wirtinger flow: Theory and algorithms", "author": ["Emmanuel J Candes", "Xiaodong Li", "Mahdi Soltanolkotabi"], "venue": "IEEE Transactions on Information Theory,", "citeRegEx": "Candes et al\\.,? \\Q1985\\E", "shortCiteRegEx": "Candes et al\\.", "year": 1985}, {"title": "Gradient descent efficiently finds the cubic-regularized non-convex newton step", "author": ["Yair Carmon", "John C Duchi"], "venue": "arXiv preprint arXiv:1612.00547,", "citeRegEx": "Carmon and Duchi.,? \\Q2016\\E", "shortCiteRegEx": "Carmon and Duchi.", "year": 2016}, {"title": "Accelerated methods for nonconvex optimization", "author": ["Yair Carmon", "John C Duchi", "Oliver Hinder", "Aaron Sidford"], "venue": "arXiv preprint arXiv:1611.00756,", "citeRegEx": "Carmon et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Carmon et al\\.", "year": 2016}, {"title": "The loss surface of multilayer networks", "author": ["Anna Choromanska", "Mikael Henaff", "Michael Mathieu", "G\u00e9rard Ben Arous", "Yann LeCun"], "venue": null, "citeRegEx": "Choromanska et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Choromanska et al\\.", "year": 2014}, {"title": "A trust region algorithm with a worst-case iteration complexity of\\ mathcal {O}(\\ epsilon\u02c6{-3/2}) for nonconvex optimization", "author": ["Frank E Curtis", "Daniel P Robinson", "Mohammadreza Samadi"], "venue": "Mathematical Programming,", "citeRegEx": "Curtis et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Curtis et al\\.", "year": 2014}, {"title": "Identifying and attacking the saddle point problem in high-dimensional non-convex optimization", "author": ["Yann N Dauphin", "Razvan Pascanu", "Caglar Gulcehre", "Kyunghyun Cho", "Surya Ganguli", "Yoshua Bengio"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Dauphin et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Dauphin et al\\.", "year": 2014}, {"title": "Escaping from saddle points\u2014online stochastic gradient for tensor decomposition", "author": ["Rong Ge", "Furong Huang", "Chi Jin", "Yang Yuan"], "venue": "In COLT,", "citeRegEx": "Ge et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Ge et al\\.", "year": 2015}, {"title": "Matrix completion has no spurious local minimum", "author": ["Rong Ge", "Jason D Lee", "Tengyu Ma"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Ge et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Ge et al\\.", "year": 2016}, {"title": "On decompositional algorithms for uniform sampling from n-spheres and n-balls", "author": ["Radoslav Harman", "Vladim\u0131\u0301r Lacko"], "venue": "Journal of Multivariate Analysis,", "citeRegEx": "Harman and Lacko.,? \\Q2010\\E", "shortCiteRegEx": "Harman and Lacko.", "year": 2010}, {"title": "Computing matrix squareroot via non convex local search", "author": ["Prateek Jain", "Chi Jin", "Sham M Kakade", "Praneeth Netrapalli"], "venue": "arXiv preprint arXiv:1507.05854,", "citeRegEx": "Jain et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Jain et al\\.", "year": 2015}, {"title": "Linear convergence of gradient and proximalgradient methods under the Polyak-Lojasiewicz condition", "author": ["Hamed Karimi", "Julie Nutini", "Mark Schmidt"], "venue": "In Joint European Conference on Machine Learning and Knowledge Discovery in Databases,", "citeRegEx": "Karimi et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Karimi et al\\.", "year": 2016}, {"title": "Deep learning without poor local minima", "author": ["Kenji Kawaguchi"], "venue": "In Advances In Neural Information Processing Systems,", "citeRegEx": "Kawaguchi.,? \\Q2016\\E", "shortCiteRegEx": "Kawaguchi.", "year": 2016}, {"title": "Gradient descent only converges to minimizers", "author": ["Jason D Lee", "Max Simchowitz", "Michael I Jordan", "Benjamin Recht"], "venue": "In Conference on Learning Theory,", "citeRegEx": "Lee et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Lee et al\\.", "year": 2016}, {"title": "The power of normalization: Faster evasion of saddle points", "author": ["Kfir Y Levy"], "venue": "arXiv preprint arXiv:1611.04831,", "citeRegEx": "Levy.,? \\Q2016\\E", "shortCiteRegEx": "Levy.", "year": 2016}, {"title": "Introductory lectures on convex programming volume", "author": ["Yu Nesterov"], "venue": "i: Basic course. Lecture notes,", "citeRegEx": "Nesterov.,? \\Q1998\\E", "shortCiteRegEx": "Nesterov.", "year": 1998}, {"title": "Cubic regularization of newton method and its global performance", "author": ["Yurii Nesterov", "Boris T Polyak"], "venue": "Mathematical Programming,", "citeRegEx": "Nesterov and Polyak.,? \\Q2006\\E", "shortCiteRegEx": "Nesterov and Polyak.", "year": 2006}, {"title": "Phase retrieval using alternating minimization", "author": ["Praneeth Netrapalli", "Prateek Jain", "Sujay Sanghavi"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Netrapalli et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Netrapalli et al\\.", "year": 2013}, {"title": "Non-square matrix sensing without spurious local minima via the burer-monteiro approach", "author": ["Dohyung Park", "Anastasios Kyrillidis", "Constantine Caramanis", "Sujay Sanghavi"], "venue": "arXiv preprint arXiv:1609.03240,", "citeRegEx": "Park et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Park et al\\.", "year": 2016}, {"title": "Gradient methods for the minimisation of functionals", "author": ["Boris T Polyak"], "venue": "USSR Computational Mathematics and Mathematical Physics,", "citeRegEx": "Polyak.,? \\Q1963\\E", "shortCiteRegEx": "Polyak.", "year": 1963}, {"title": "Learning representations by back-propagating errors", "author": ["David E Rumelhart", "Geoffrey E Hinton", "Ronald J Williams"], "venue": "Cognitive modeling,", "citeRegEx": "Rumelhart et al\\.,? \\Q1988\\E", "shortCiteRegEx": "Rumelhart et al\\.", "year": 1988}, {"title": "Complete dictionary recovery over the sphere i: Overview and the geometric picture", "author": ["Ju Sun", "Qing Qu", "John Wright"], "venue": "IEEE Transactions on Information Theory,", "citeRegEx": "Sun et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Sun et al\\.", "year": 2016}, {"title": "A geometric analysis of phase retrieval", "author": ["Ju Sun", "Qing Qu", "John Wright"], "venue": "In Information Theory (ISIT),", "citeRegEx": "Sun et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Sun et al\\.", "year": 2016}, {"title": "Guaranteed matrix completion via non-convex factorization", "author": ["Ruoyu Sun", "Zhi-Quan Luo"], "venue": "IEEE Transactions on Information Theory,", "citeRegEx": "Sun and Luo.,? \\Q2016\\E", "shortCiteRegEx": "Sun and Luo.", "year": 2016}, {"title": "Convergence analysis for rectangular matrix completion using burer-monteiro factorization and gradient descent", "author": ["Qinqing Zheng", "John Lafferty"], "venue": "arXiv preprint arXiv:1605.07051,", "citeRegEx": "Zheng and Lafferty.,? \\Q2016\\E", "shortCiteRegEx": "Zheng and Lafferty.", "year": 2016}], "referenceMentions": [{"referenceID": 21, "context": "This is notably true in the deep learning setting, where gradients can be computed efficiently via backpropagation [Rumelhart et al., 1988].", "startOffset": 115, "endOffset": 139}, {"referenceID": 16, "context": "within l(f(x0)\u2212 f\u22c6)/\u01eb2 iterations [Nesterov, 1998], where x0 is the initial point and f\u22c6 is the optimal value of f .", "startOffset": 34, "endOffset": 50}, {"referenceID": 8, "context": ", in tensor decomposition [Ge et al., 2015], dictionary learning [Sun et al.", "startOffset": 26, "endOffset": 43}, {"referenceID": 9, "context": ", 2016], matrix completion [Ge et al., 2016], and certain classes of deep neural networks [Kawaguchi, 2016]).", "startOffset": 27, "endOffset": 44}, {"referenceID": 13, "context": ", 2016], and certain classes of deep neural networks [Kawaguchi, 2016]).", "startOffset": 53, "endOffset": 70}, {"referenceID": 5, "context": "Moreover, there are suggestions that in more general deep newtorks most of the local minima are as good as global minima [Choromanska et al., 2014].", "startOffset": 121, "endOffset": 147}, {"referenceID": 17, "context": "For \u03c1-Hessian Lipschitz functions (see Definition 5), these points are defined as [Nesterov and Polyak, 2006]:", "startOffset": 82, "endOffset": 109}, {"referenceID": 1, "context": ", 2016b], matrix sensing [Bhojanapalli et al., 2016, Park et al., 2016], matrix completion [Ge et al., 2016], and certain classes of deep neural networks [Kawaguchi, 2016]). Moreover, there are suggestions that in more general deep newtorks most of the local minima are as good as global minima [Choromanska et al., 2014]. On the other hand, saddle points (and local maxima) can correspond to highly suboptimal solutions in many problems [see, e.g., Jain et al., 2015, Sun et al., 2016b]. Furthermore, Dauphin et al. [2014] argue that saddle points are ubiquitous in high-dimensional, non-convex optimization problems, and are thus the main bottleneck in training neural networks.", "startOffset": 26, "endOffset": 524}, {"referenceID": 1, "context": ", 2016b], matrix sensing [Bhojanapalli et al., 2016, Park et al., 2016], matrix completion [Ge et al., 2016], and certain classes of deep neural networks [Kawaguchi, 2016]). Moreover, there are suggestions that in more general deep newtorks most of the local minima are as good as global minima [Choromanska et al., 2014]. On the other hand, saddle points (and local maxima) can correspond to highly suboptimal solutions in many problems [see, e.g., Jain et al., 2015, Sun et al., 2016b]. Furthermore, Dauphin et al. [2014] argue that saddle points are ubiquitous in high-dimensional, non-convex optimization problems, and are thus the main bottleneck in training neural networks. Standard analysis of gradient descent cannot distinguish between saddle points and local minima, leaving open the possibility that gradient descent may get stuck at saddle points, either asymptotically or for a sufficiently long time so as to make training times for arriving at a local minimum infeasible. Ge et al. [2015] showed that by adding noise at each step, gradient descent can escape all saddle points in a polynomial number of iterations, provided that the objective function satisfies the strict saddle property (see Assumption A2).", "startOffset": 26, "endOffset": 1005}, {"referenceID": 1, "context": ", 2016b], matrix sensing [Bhojanapalli et al., 2016, Park et al., 2016], matrix completion [Ge et al., 2016], and certain classes of deep neural networks [Kawaguchi, 2016]). Moreover, there are suggestions that in more general deep newtorks most of the local minima are as good as global minima [Choromanska et al., 2014]. On the other hand, saddle points (and local maxima) can correspond to highly suboptimal solutions in many problems [see, e.g., Jain et al., 2015, Sun et al., 2016b]. Furthermore, Dauphin et al. [2014] argue that saddle points are ubiquitous in high-dimensional, non-convex optimization problems, and are thus the main bottleneck in training neural networks. Standard analysis of gradient descent cannot distinguish between saddle points and local minima, leaving open the possibility that gradient descent may get stuck at saddle points, either asymptotically or for a sufficiently long time so as to make training times for arriving at a local minimum infeasible. Ge et al. [2015] showed that by adding noise at each step, gradient descent can escape all saddle points in a polynomial number of iterations, provided that the objective function satisfies the strict saddle property (see Assumption A2). Lee et al. [2016] proved that under similar conditions, gradient descent with random initialization avoids saddle points even without adding noise.", "startOffset": 26, "endOffset": 1244}, {"referenceID": 1, "context": ", 2016b], matrix sensing [Bhojanapalli et al., 2016, Park et al., 2016], matrix completion [Ge et al., 2016], and certain classes of deep neural networks [Kawaguchi, 2016]). Moreover, there are suggestions that in more general deep newtorks most of the local minima are as good as global minima [Choromanska et al., 2014]. On the other hand, saddle points (and local maxima) can correspond to highly suboptimal solutions in many problems [see, e.g., Jain et al., 2015, Sun et al., 2016b]. Furthermore, Dauphin et al. [2014] argue that saddle points are ubiquitous in high-dimensional, non-convex optimization problems, and are thus the main bottleneck in training neural networks. Standard analysis of gradient descent cannot distinguish between saddle points and local minima, leaving open the possibility that gradient descent may get stuck at saddle points, either asymptotically or for a sufficiently long time so as to make training times for arriving at a local minimum infeasible. Ge et al. [2015] showed that by adding noise at each step, gradient descent can escape all saddle points in a polynomial number of iterations, provided that the objective function satisfies the strict saddle property (see Assumption A2). Lee et al. [2016] proved that under similar conditions, gradient descent with random initialization avoids saddle points even without adding noise. However, this result does not bound the number of steps needed to reach a local minimum. Though these results establish that gradient descent can find local minima in a polynomial number of iterations, they are still far from being efficient. For instance, the number of iterations required in Ge et al. [2015] is at least \u03a9(d4), where d is the underlying dimension.", "startOffset": 26, "endOffset": 1685}, {"referenceID": 16, "context": "of gradient descent to first-order stationary points [Nesterov, 1998], up to log factors.", "startOffset": 53, "endOffset": 69}, {"referenceID": 6, "context": "Trust region algorithms [Curtis et al., 2014] can also achieve the same performance if the parameters are chosen carefully.", "startOffset": 24, "endOffset": 45}, {"referenceID": 0, "context": ", 2015, Sun and Luo, 2016, Bhojanapalli et al., 2016]. While there are not many results that show global convergence for non-convex problems, Jain et al. [2015] show that gradient descent yields global convergence rates for matrix square-root problems.", "startOffset": 27, "endOffset": 161}, {"referenceID": 0, "context": ", 2015, Sun and Luo, 2016, Bhojanapalli et al., 2016]. While there are not many results that show global convergence for non-convex problems, Jain et al. [2015] show that gradient descent yields global convergence rates for matrix square-root problems. Although these results give strong guarantees, the analyses are heavily tailored to specific problems, and it is unclear how to generalize them to a wider class of non-convex functions. For general non-convex optimization, there are a few previous results on finding second-order stationary points. These results can be divided into the following three categories, where, for simplicity of presentation, we only highlight dependence on dimension d and \u01eb, assuming that all other problem parameters are constant from the point of view of iteration complexity: Hessian-based: Traditionally, only second-order optimization methods were known to converge to second-order stationary points. These algorithms rely on computing the Hessian to distinguish between first- and second-order stationary points. Nesterov and Polyak [2006] designed a cubic regularization algorithm which converges to an \u01eb-second-order stationary point in O(1/\u01eb1.", "startOffset": 27, "endOffset": 1079}, {"referenceID": 0, "context": "Agarwal et al. [2016] and Carmon et al.", "startOffset": 0, "endOffset": 22}, {"referenceID": 0, "context": "Agarwal et al. [2016] and Carmon et al. [2016] presented accelerated algorithms that can find an \u01eb-second-order stationary point in O(log d/\u01eb7/4) steps.", "startOffset": 0, "endOffset": 47}, {"referenceID": 0, "context": "Agarwal et al. [2016] and Carmon et al. [2016] presented accelerated algorithms that can find an \u01eb-second-order stationary point in O(log d/\u01eb7/4) steps. Also, Carmon and Duchi [2016] showed by running gradient descent as a subroutine to solve the subproblem of cubic regularization (which requires Hessian-vector product oracle), it is possible to find an \u01eb-second-order stationary pointin O(log d/\u01eb2) iterations.", "startOffset": 0, "endOffset": 183}, {"referenceID": 8, "context": "Ge et al. [2015] showed that stochastic gradient descent could converge to a second-order stationary point in poly(d/\u01eb) iterations, with polynomial of order at least four.", "startOffset": 0, "endOffset": 17}, {"referenceID": 8, "context": "Ge et al. [2015] showed that stochastic gradient descent could converge to a second-order stationary point in poly(d/\u01eb) iterations, with polynomial of order at least four. This was improved in Levy [2016] to O(d3 \u00b7poly(1/\u01eb)) using normalized gradient descent.", "startOffset": 0, "endOffset": 205}, {"referenceID": 16, "context": "Theorem 2 ([Nesterov, 1998]).", "startOffset": 11, "endOffset": 27}, {"referenceID": 16, "context": "We instead follow the convention of Nesterov and Polyak [2006] by choosing \u01ebH = \u221a \u03c1\u01ebg to reflect the natural relations between the gradient and the Hessian.", "startOffset": 36, "endOffset": 63}, {"referenceID": 16, "context": "We instead follow the convention of Nesterov and Polyak [2006] by choosing \u01ebH = \u221a \u03c1\u01ebg to reflect the natural relations between the gradient and the Hessian. This definition of \u01eb-second-order stationary point can also differ by reparametrization (and scaling), e.g. Nesterov and Polyak [2006] use \u01eb\u2032 = \u221a \u01eb/\u03c1.", "startOffset": 36, "endOffset": 292}, {"referenceID": 10, "context": "Note that uniform sampling from a d-dimensional ball can be done efficiently by sampling U 1 d \u00d7 Y \u2016Y\u2016 where U \u223c Uniform([0, 1]) and Y \u223c N (0, Id) [Harman and Lacko, 2010].", "startOffset": 147, "endOffset": 171}, {"referenceID": 1, "context": "This regularity condition commonly appears in low-rank problems such as matrix sensing and matrix completion, and has been used in Bhojanapalli et al. [2016], Zheng and Lafferty [2016], where local minima form a connected set, and where the Hessian is strictly positive only with respect to directions pointing outside the set of local minima.", "startOffset": 131, "endOffset": 158}, {"referenceID": 1, "context": "This regularity condition commonly appears in low-rank problems such as matrix sensing and matrix completion, and has been used in Bhojanapalli et al. [2016], Zheng and Lafferty [2016], where local minima form a connected set, and where the Hessian is strictly positive only with respect to directions pointing outside the set of local minima.", "startOffset": 131, "endOffset": 185}, {"referenceID": 12, "context": "The interested reader can refer to Karimi et al. [2016] for other relaxed and alternative notions of convexity, which can also be potentially combined with Assumptions A1andA2 to yield convergence results of a similar flavor as that of Theorem 5.", "startOffset": 35, "endOffset": 56}, {"referenceID": 8, "context": "[Ge et al., 2015]).", "startOffset": 0, "endOffset": 17}, {"referenceID": 8, "context": "[Ge et al., 2015]) do not work when the step size and perturbation do not depend polynomially in dimension d.", "startOffset": 0, "endOffset": 17}], "year": 2017, "abstractText": "This paper shows that a perturbed form of gradient descent converges to a second-order stationary point in a number iterations which depends only poly-logarithmically on dimension (i.e., it is almost \u201cdimension-free\u201d). The convergence rate of this procedure matches the wellknown convergence rate of gradient descent to first-order stationary points, up to log factors. When all saddle points are non-degenerate, all second-order stationary points are local minima, and our result thus shows that perturbed gradient descent can escape saddle points almost for free. Our results can be directly applied to many machine learning applications, including deep learning. As a particular concrete example of such an application, we show that our results can be used directly to establish sharp global convergence rates for matrix factorization. Our results rely on a novel characterization of the geometry around saddle points, which may be of independent interest to the non-convex optimization community.", "creator": "LaTeX with hyperref package"}}}