{"id": "1703.00893", "review": {"conference": "ICML", "VERSION": "v1", "DATE_OF_SUBMISSION": "2-Mar-2017", "title": "Being Robust (in High Dimensions) Can Be Practical", "abstract": "Robust estimation is much more challenging in high dimensions than it is in one dimension: Most techniques either lead to intractable optimization problems or estimators that can tolerate only a tiny fraction of errors. Recent work in theoretical computer science has shown that, in appropriate distributional models, it is possible to robustly estimate the mean and covariance with polynomial time algorithms that can tolerate a constant fraction of corruptions, independent of the dimension. However, the sample and time complexity of these algorithms is prohibitively large for high-dimensional applications. In this work, we address both of these issues by establishing sample complexity bounds that are optimal, up to logarithmic factors, as well as giving various refinements that allow the algorithms to tolerate a much larger fraction of corruptions. Finally, we show on both synthetic and real data that our algorithms have state-of-the-art performance and suddenly make high-dimensional robust estimation a realistic possibility.", "histories": [["v1", "Thu, 2 Mar 2017 18:50:33 GMT  (1689kb,D)", "http://arxiv.org/abs/1703.00893v1", null]], "reviews": [], "SUBJECTS": "cs.LG cs.DS cs.IT math.IT stat.ML", "authors": ["ilias diakonikolas", "gautam kamath", "daniel m kane", "jerry li 0001", "ankur moitra", "alistair stewart"], "accepted": true, "id": "1703.00893"}, "pdf": {"name": "1703.00893.pdf", "metadata": {"source": "CRF", "title": "Being Robust (in High Dimensions) Can Be Practical", "authors": ["Ilias Diakonikolas", "Gautam Kamath", "Daniel M. Kane", "Jerry Li", "Ankur Moitra", "Alistair Stewart"], "emails": ["diakonik@usc.edu", "g@csail.mit.edu", "dakane@cs.ucsd.edu", "jerryzli@mit.edu", "moitra@mit.edu", "alistais@usc.edu"], "sections": [{"heading": "1 Introduction", "text": "This year, it has reached the stage where it will be able to take the lead."}, {"heading": "1.1 Robustness in a Generative Model", "text": "The starting point for our work is the work of [DKK + 16], which has given an efficient algorithm for the problem of agnostic learning of a Gaussian: Given a polynomial number of samples from a high-dimensional Gaussian N (\u00b5, \u03a3), where an adversary has arbitrarily corrupted an \u03b5 fraction, you will find a number of parameters N \u2032 that satisfy dTV (N, N \u2032). The total variation distance is the natural metric to measure the proximity of the parameters, since a fraction of the samples observed came from a Gaussian model. [DKK + 16] gave an algorithm for the above problem (note that the guarantees are dimensionally independent), whose runtime and sample complexity are polynomial in dimension. [DKK + 16] gave an algorithm for the above problems (note that the guarantees are dimensionally independent)."}, {"heading": "1.2 Our Results", "text": "The aim of this work is to show that high-dimensional robust estimates can be highly practical. However, there are two major obstacles to achieving this: First, the complexity of the examples and the runtime of the algorithms in the world in which we find ourselves is far too great for high-dimensional applications. We simply would not be able to store as many samples as we need to calculate accurate estimates in which we have a high-dimensional application.Our first major contribution is to show essentially narrow limits on the complexity of the filtering algorithms of [DKK + 16], as we achieve with a new definition of good equipment embedded in the existing analysis, showing that it is possible to estimate the mean values with O (d / 2) samples (if the covariance with O is known) and the covariance with O (d2 / 2) samples."}, {"heading": "2 Formal Framework", "text": "If M is a matrix, we will allow M-2 to denote its spectral norm and M-F to denote its Frobenius norm. We will write X-u-S to indicate that X is drawn from the empirical distribution defined by S. Robust Estimation. We will consider the following powerful model of robust estimation, which generalizes many other existing models, including Huber's contamination model: Definition 2.1. Given \u03b5 > 0 and a distribution family D, the adversary works as follows: The algorithm specifies a certain number of samples m. The adversary generates m samples X1, X2, Xm from some (unknown) D-D. It then extracts m \u00b2 from an appropriate distribution. This distribution is allowed by X1, X2, Xm \u00b2, but when marginalized."}, {"heading": "3 Nearly Sample-optimal Efficient Robust Learning", "text": "In this section we present an optimal estimation method, but we rely on the mean and covariance of the high-dimensional distributions that we both achieve. (The first is a simple eigenvalue calculation), but the corresponding sample complexity in [DKK + 16] is polynomically inferior to the minimal information theoretical method. (Filtering techniques seemed to be available for practical implementation (since it only uses simple eigenvalue calculations), but the corresponding sample complexity in [DKK + 16] is polynomically inferior to the information theoretical minimum. On the other hand, the convex programming technique of [DKK + 16] is a better example complexity (e.g., close to a robust mean estimate), but based on the elliptical method."}, {"heading": "4 Filtering", "text": "We now describe the filter technique more strictly and describe some additional heuristics that we found useful in practice."}, {"heading": "4.1 Robust Mean Estimation", "text": "The algorithms that reach theorems 3.1 and 3.2 both follow the general recipe in algorithm 1. We must specify three parameter functions: \u2022 Thres (\u03b5) is a threshold function - we terminate if the covariance has spectral norm limited by Thres (\u03b5). \u2022 Tail (T, d, \u03b5, \u03c4) is a univariate tail function that would only be violated by a fraction of the points if they were not corrupted, but is violated by much more of the current score. \u2022 Tail (T, s) is a gap function that we need for technical reasons. In view of these objects, our filter is relatively easy to specify: first, we calculate the empirical covariance. Then we check whether the spectral norm of empirical covariance exceeds the empirical covariance."}, {"heading": "4.2 Robust Covariance Estimation", "text": "Our algorithm for robust covariance follows exactly the recipe outlined above, with one crucial difference - we check for discrepancies in the empirical tensor of the fourth moment. Intuitively, just as in the robust mean, we have used grade-2 information to detect outliers for the mean (the grade-1 moment), here we need to use grade-4 information to detect outliers for the degree of covariance (the grade-2 moment). More specifically, this corresponds to the finding of a normalized grade-2 polynomial whose empirical variance is too large. By then filtering along this polynomial, with a suitable selection of thres (\u03b5), \u043c (\u03b5, s) and tail, we reach the desired limits. See section A.3 for the formal pseudo code and more details."}, {"heading": "4.3 Better Univariate Tests", "text": "In the robust mean estimation algorithms described above, after projecting on one dimension, we center the points on the empirical mean in that direction. This is sufficient in theory, but introduces additional constant factors, since the empirical mean can be damaged in that direction. Instead, we can use a robust estimate for the mean in one direction. It is generally known that the median is an optimal robust statistic for the mean in one dimension [DKW56, DK14]. By centering the points on the median rather than on the mean, we are able to achieve better errors in practice."}, {"heading": "4.4 Adaptive Tail Bounding", "text": "In our empirical evaluation, we found that it was important to find a good choice of tail in order to achieve good error rates, especially for robust covariance estimates. Specifically, in this setting, our tail limit is given by tail (T, d, \u03b5, \u03b4, \u03c4) = C1 exp (\u2212 C2T) + Tail2 (T, d, \u03b5, \u043c), for a function Tail2, and constants C1, C2. We found that the term that was always the first term on the RHS for reasonable settings, and that Tail2 is less significant. Therefore, we focused on optimizing the first term. We found that it was useful to change the constant C2 depending on the setting. Especially in low dimensions, we could be stricter and force a stronger tail limit (corresponding to a higher C2), but in higher dimensions, we need to be more lax with the tail limit. If we do this in a principled way, we can adjust our goal, we have a C2 this year with a coarse value."}, {"heading": "5 Experiments", "text": "We performed an empirical evaluation of the above algorithms on synthetic and real data sets with and without synthetic noise. All experiments were performed on a laptop with 2.7 GHz Intel Core i5 CPU and 8 GB RAM. The focus of this evaluation was on statistical accuracy, not time efficiency. In this measurement, our algorithm performs the best of all the algorithms we tested. In all synthetic experiments, our algorithm consistently showed the least error. In fact, our error was orders of magnitude better than any other algorithm in some of the synthetic benchmarks. In the semi-synthetic benchmark, our algorithm also (probably) performs best, although there is no way to say for sure, as there is no fundamental truth."}, {"heading": "5.1 Synthetic Data", "text": "In both cases, the experiments have confirmed the accuracy and usefulness of our algorithm, and for the dimension d = [100, 150,. 400], we generate n = 10d\u03b52 samples, where a (1 \u2212) fraction of our synthetic averages comes from Figure 1, and a -fraction comes from a noise distribution. Our goal is to produce an estimator that minimizes the estimator's error to the truth. As a baseline, we calculate the error that is achieved only by the uncorrupted samples."}, {"heading": "5.2 Semi-synthetic Data", "text": "In this study, the authors examined the data collected as part of the Population Reference Sample (POPRES) project, which consists of the genotyping of thousands of individuals using the Affymetrix 500K Single Nucleotide Polymorphism (SNP). The authors truncated the data set to obtain the genetic data of over 1387 European individuals, using principles to analyze a two-dimensional summary of genetic variation that bears striking resemblance to the map of Europe."}, {"heading": "A Omitted Details from Section 3", "text": "In this section, we use our filtering technique to provide an approximate sample-based, efficient calculation method for robust estimation of the mean of a sub-Gaussian density with a known covariance matrix, the only difference being a weaker definition of the \"good sample\" (Definition A.4) and a simple concentration reasoning (Lemma A.5) showing that a random set of uncorrupted samples of the appropriate size is good with a high probability. In light of these results, the analysis of this subsection follows directly from the analysis in Section 8.1 of the modified parameters."}, {"heading": "B Omitted Details from Section 5", "text": "This year, it has reached the stage where it will be able to take the lead."}], "references": [{"title": "The complexity and approximability of finding maximum feasible subsystems of linear relations", "author": ["E. Amaldi", "V. Kann"], "venue": "Theoretical Computer Science,", "citeRegEx": "Amaldi and Kann.,? \\Q1995\\E", "shortCiteRegEx": "Amaldi and Kann.", "year": 1995}, {"title": "Approximating center points with iterated radon points", "author": ["K.L. Clarkson", "D. Eppstein", "G.L. Miller", "C. Sturtivant", "S.-H. Teng"], "venue": "In Proceedings of the Ninth Annual Symposium on Computational Geometry,", "citeRegEx": "Clarkson et al\\.,? \\Q1993\\E", "shortCiteRegEx": "Clarkson et al\\.", "year": 1993}, {"title": "A general decision theory for huber\u2019s \u03b5-contamination model", "author": ["M. Chen", "C. Gao", "Z. Ren"], "venue": "CoRR, abs/1511.04144,", "citeRegEx": "Chen et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Chen et al\\.", "year": 2015}, {"title": "An optimal randomized algorithm for maximum tukey depth", "author": ["T.M. Chan"], "venue": "In Proceedings of the Fifteenth Annual ACM-SIAM Symposium on Discrete Algorithms (SODA),", "citeRegEx": "Chan.,? \\Q2004\\E", "shortCiteRegEx": "Chan.", "year": 2004}, {"title": "Robust principal component analysis", "author": ["E.J. Cand\u00e8s", "X. Li", "Y. Ma", "J. Wright"], "venue": "J. ACM,", "citeRegEx": "Cand\u00e8s et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Cand\u00e8s et al\\.", "year": 2011}, {"title": "Computationally efficient robust estimation of sparse functionals", "author": ["S.S. Du", "S. Balakrishnan", "A. Singh"], "venue": null, "citeRegEx": "Du et al\\.,? \\Q2017\\E", "shortCiteRegEx": "Du et al\\.", "year": 2017}, {"title": "Breakdown properties of location estimates based on halfspace depth and projected outlyingness", "author": ["D.L. Donoho", "M. Gasko"], "venue": "Ann. Statist., 20(4):1803\u20131827,", "citeRegEx": "Donoho and Gasko.,? \\Q1992\\E", "shortCiteRegEx": "Donoho and Gasko.", "year": 1992}, {"title": "Faster and sample near-optimal algorithms for proper learning mixtures of gaussians", "author": ["C. Daskalakis", "G. Kamath"], "venue": "In Proceedings of The 27th Conference on Learning Theory, COLT", "citeRegEx": "Daskalakis and Kamath.,? \\Q2014\\E", "shortCiteRegEx": "Daskalakis and Kamath.", "year": 2014}, {"title": "Robust estimators in high dimensions without the computational intractability", "author": ["I. Diakonikolas", "G. Kamath", "D.M. Kane", "J. Li", "A. Moitra", "A. Stewart"], "venue": "In Proceedings of FOCS\u201916,", "citeRegEx": "Diakonikolas et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Diakonikolas et al\\.", "year": 2016}, {"title": "Efficient and optimally robust learning of high-dimensional gaussians", "author": ["I. Diakonikolas", "G. Kamath", "D.M. Kane", "J. Li", "A. Moitra", "A. Stewart"], "venue": "In Manuscript,", "citeRegEx": "Diakonikolas et al\\.,? \\Q2017\\E", "shortCiteRegEx": "Diakonikolas et al\\.", "year": 2017}, {"title": "Robust learning of fixed-structure bayesian networks", "author": ["I. Diakonikolas", "D.M. Kane", "A. Stewart"], "venue": null, "citeRegEx": "Diakonikolas et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Diakonikolas et al\\.", "year": 2016}, {"title": "Statistical query lower bounds for robust estimation of high-dimensional gaussians and gaussian mixtures", "author": ["I. Diakonikolas", "D.M. Kane", "A. Stewart"], "venue": null, "citeRegEx": "Diakonikolas et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Diakonikolas et al\\.", "year": 2016}, {"title": "Asymptotic minimax character of the sample distribution function and of the classical multinomial estimator", "author": ["A. Dvoretzky", "J. Kiefer", "J. Wolfowitz"], "venue": "Ann. Mathematical Statistics,", "citeRegEx": "Dvoretzky et al\\.,? \\Q1956\\E", "shortCiteRegEx": "Dvoretzky et al\\.", "year": 1956}, {"title": "Combinatorial methods in density estimation", "author": ["L. Devroye", "G. Lugosi"], "venue": null, "citeRegEx": "Devroye and Lugosi.,? \\Q2001\\E", "shortCiteRegEx": "Devroye and Lugosi.", "year": 2001}, {"title": "Robust statistics. The approach based on influence functions", "author": ["F.R. Hampel", "E.M. Ronchetti", "P.J. Rousseeuw", "W.A. Stahel"], "venue": null, "citeRegEx": "Hampel et al\\.,? \\Q1986\\E", "shortCiteRegEx": "Hampel et al\\.", "year": 1986}, {"title": "Robust estimation of a location parameter", "author": ["P.J. Huber"], "venue": "The Annals of Mathematical Statistics,", "citeRegEx": "Huber.,? \\Q1964\\E", "shortCiteRegEx": "Huber.", "year": 1964}, {"title": "The densest hemisphere problem", "author": ["D.S. Johnson", "F.P. Preparata"], "venue": "Theoretical Computer Science,", "citeRegEx": "Johnson and Preparata.,? \\Q1978\\E", "shortCiteRegEx": "Johnson and Preparata.", "year": 1978}, {"title": "Robust sparse estimation tasks in high", "author": ["J. Li"], "venue": "dimensions. CoRR,", "citeRegEx": "Li.,? \\Q2017\\E", "shortCiteRegEx": "Li.", "year": 2017}, {"title": "Agnostic estimation of mean and covariance", "author": ["K.A. Lai", "A.B. Rao", "S. Vempala"], "venue": "In Proceedings of FOCS\u201916,", "citeRegEx": "Lai et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Lai et al\\.", "year": 2016}, {"title": "Approximate centerpoints with proofs", "author": ["G.L. Miller", "D. Sheehy"], "venue": "Comput. Geom.,", "citeRegEx": "Miller and Sheehy.,? \\Q2010\\E", "shortCiteRegEx": "Miller and Sheehy.", "year": 2010}, {"title": "Genes mirror geography within europe", "author": ["J. Novembre", "T. Johnson", "K. Bryc", "Z. Kutalik", "A.R. Boyko", "A. Auton", "A. Indap", "K.S. King", "S. Bergmann", "M.R. Nelson"], "venue": null, "citeRegEx": "Novembre et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Novembre et al\\.", "year": 2008}, {"title": "Multivariate estimation with high breakdown point", "author": ["P. Rousseeuw"], "venue": "Mathematical Statistics and Applications,", "citeRegEx": "Rousseeuw.,? \\Q1985\\E", "shortCiteRegEx": "Rousseeuw.", "year": 1985}, {"title": "Computing location depth and regression depth in higher dimensions", "author": ["P.J. Rousseeuw", "A. Struyf"], "venue": "Statistics and Computing,", "citeRegEx": "Rousseeuw and Struyf.,? \\Q1998\\E", "shortCiteRegEx": "Rousseeuw and Struyf.", "year": 1998}, {"title": "An introduction to matrix concentration inequalities", "author": ["J.A. Tropp"], "venue": "Foundations and Trends in Machine Learning,", "citeRegEx": "Tropp,? \\Q2015\\E", "shortCiteRegEx": "Tropp", "year": 2015}, {"title": "A survey of sampling from contaminated distributions", "author": ["J.W. Tukey"], "venue": "Contributions to probability and statistics,", "citeRegEx": "Tukey.,? \\Q1960\\E", "shortCiteRegEx": "Tukey.", "year": 1960}, {"title": "Minimum volume ellipsoid", "author": ["S. Van Aelst", "P. Rousseeuw"], "venue": "Wiley Interdisciplinary Reviews: Computational Statistics,", "citeRegEx": "Aelst and Rousseeuw.,? \\Q2009\\E", "shortCiteRegEx": "Aelst and Rousseeuw.", "year": 2009}, {"title": "Introduction to the non-asymptotic analysis of random matrices", "author": ["R. Vershynin"], "venue": null, "citeRegEx": "Vershynin.,? \\Q2010\\E", "shortCiteRegEx": "Vershynin.", "year": 2010}, {"title": "Robust pca via outlier pursuit", "author": ["H. Xu", "C. Caramanis", "S. Sanghavi"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Xu et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Xu et al\\.", "year": 2010}], "referenceMentions": [], "year": 2017, "abstractText": "Robust estimation is much more challenging in high dimensions than it is in one dimension: Most techniques either lead to intractable optimization problems or estimators that can tolerate only a tiny fraction of errors. Recent work in theoretical computer science has shown that, in appropriate distributional models, it is possible to robustly estimate the mean and covariance with polynomial time algorithms that can tolerate a constant fraction of corruptions, independent of the dimension. However, the sample and time complexity of these algorithms is prohibitively large for high-dimensional applications. In this work, we address both of these issues by establishing sample complexity bounds that are optimal, up to logarithmic factors, as well as giving various refinements that allow the algorithms to tolerate a much larger fraction of corruptions. Finally, we show on both synthetic and real data that our algorithms have state-of-the-art performance and suddenly make high-dimensional robust estimation a realistic possibility.", "creator": "LaTeX with hyperref package"}}}