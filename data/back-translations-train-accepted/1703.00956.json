{"id": "1703.00956", "review": {"conference": "ICML", "VERSION": "v1", "DATE_OF_SUBMISSION": "2-Mar-2017", "title": "A Laplacian Framework for Option Discovery in Reinforcement Learning", "abstract": "Representation learning and option discovery are two of the biggest challenges in reinforcement learning (RL). Proto-RL is a well known approach for representation learning in MDPs. The representations learned with this framework are called proto-value functions (PVFs). In this paper we address the option discovery problem by showing how PVFs implicitly define options. We do it by introducing eigenpurposes, intrinsic reward functions derived from the learned representations. The options discovered from eigenpurposes traverse the principal directions of the state space. They are useful for multiple tasks because they are independent of the agents' intentions. Moreover, by capturing the diffusion process of a random walk, different options act at different time scales, making them helpful for exploration strategies. We demonstrate features of eigenpurposes in traditional tabular domains as well as in Atari 2600 games.", "histories": [["v1", "Thu, 2 Mar 2017 21:31:29 GMT  (3600kb,D)", "https://arxiv.org/abs/1703.00956v1", "Version submitted to the 34th International Conference on Machine Learning (ICML)"], ["v2", "Fri, 16 Jun 2017 02:52:21 GMT  (3684kb,D)", "http://arxiv.org/abs/1703.00956v2", "Appearing in the Proceedings of the 34th International Conference on Machine Learning (ICML)"]], "COMMENTS": "Version submitted to the 34th International Conference on Machine Learning (ICML)", "reviews": [], "SUBJECTS": "cs.LG cs.AI", "authors": ["marlos c machado", "marc g bellemare", "michael h bowling"], "accepted": true, "id": "1703.00956"}, "pdf": {"name": "1703.00956.pdf", "metadata": {"source": "META", "title": "A Laplacian Framework for Option Discovery in Reinforcement Learning", "authors": ["Marlos C. Machado", "Marc G. Bellemare", "Michael Bowling"], "emails": ["<machado@ualberta.ca>."], "sections": [{"heading": "1. Introduction", "text": "Proto-value functions (PVFs) are a well-known solution to the problem of representation learning (Mahadevan, 2005; Mahadevan & Maggioni, 2007); while the problem of skill discovery is generally placed under the option framework (Sutton et al., 1999; Precup, 2000), which models skills as options. In this paper, we combine representation learning and option discovery by showing how PVFs implicitly define options. One of our most important contributions is the introduction of the concepts of attachment and self-behavior. Self-purposes are intrinsic reward functions that provide incentives for agents to traverse the state by following the main directions of learned representation. Each intrinsic reward function leads to a different self-behavior, which is the optimal reward function for this policy."}, {"heading": "2. Background", "text": "Random variables are generally denoted by uppercase letters (e.g. Rt), vectors by bold letters (e.g. \u03b8), functions by lowercase letters (e.g. v), and sets by calligraphic writing (e.g. S)."}, {"heading": "2.1. Reinforcement Learning", "text": "In the RL framework (Sutton & Barto, 1998), an agent aims to maximize cumulative reward by taking action in an environment that influences the next state and the rewards he receives. We use the MDP formalism in this essay. An MDP is a 5x < S, A, r, p, \u03b3 >. Currently, the agent is in the state st \u00b2 S, where he takes action toward the next state st + 1% S according to the transition probability kernel p (s, a) encoding Pr (St + 1 = s, At = a). The agent also observes a reward Rt + 1% r (s, a). The agent's goal is to learn a policy: S \u00b7 A \u2192 qqqs that maximizes such a policy."}, {"heading": "2.2. The Options Framework", "text": "The Options Framework expands RL by introducing extended measures known as Skills or Options. One option \u03c9 is a 3-fold \u03c9 = < I, \u03c0, T >, where I \u0441S denotes the introductory amount of the option, \u03c0: A \u00b7 S \u2192 [0, 1] denotes the option policy and T \u0441S denotes the termination amount of the option. After the agent decides to pursue the option from a state in I, the measures are selected by \u03c0 until the agent reaches a state in T. Intuitively, options are superordinate actions that extend over several time steps and generalize MDPs to semiMarkov decision processes (SMDPs) (Puterman, 1994). Traditionally, options are sought that are able to move agents into bottleneck states. Bottleneck states are those states that connect various densely connected regions of the state (e.g., gateways) (S-ims, 2004; Solent et al, 2014) that have proven to be most efficient for planning these states."}, {"heading": "2.3. Proto-Value Functions", "text": "Protocol value functions (PVFs) are learned representations that capture large-scale temporal properties of an environment (Mahadevan, 2005; Mahadevan & Maggioni, 2007) and are achieved by diagonalizing a diffusion model constructed from the transition matrix of the MDP. A diffusion model captures the flow of information on a graph and is commonly defined by the combinatorial graph laplacial matrix L = D \u2212 A, where A is the adjacence matrix of the chart and D is the diagonal matrix whose entries capture the line totals of A. Note that the adjacency matrix A can be easily generalized to a weight matrix W. PVFs are defined as eigenvectors obtained after the intrinsic decomposition of L. Various diffusion models can be used to generate PVFs, such as the normalized graph Laplazian, we use the paper A = D \u2212 D \u2212 D in this paper."}, {"heading": "3. Option Discovery through the Laplacian", "text": "This is exactly what PVFs represent in Figure 2 (left). Instead, we can capture a function for interpreting the environment (left)."}, {"heading": "4. Empirical Evaluation", "text": "In our empirical study, we used three MDPs (see Figure 1): an open space, an I-labyrinth and the 4-room domain. Their transitions are deterministic and gray squares mark walls. Agents have access to four actions: top, bottom, right and left. If an action is chosen that would have led the agent into a wall, the state of the agent does not change. We show three aspects of our framework: 1 \u2022 How the property options represent specific purposes. Interestingly, options that lead to bottlenecks are not the first we discover. \u2022 How property options improve exploration by reducing the expected number of steps needed to navigate between two states. \u2022 How property options help agents accumulate rewards more quickly. We show how few options can affect agent performance while enough options speed up learning."}, {"heading": "4.1. Discovered Options", "text": "eigenvectors that correspond to the smallest eigenvalues are preferred in PVF theory (Mahadevan & Maggioni, 2007).The same intuition applies to eigenoptions, with eigenvalues corresponding to the smallest eigenvalues. Figures 3, 4 and 5 are the first eigenoptions discovered in the three areas used for evaluation. Eigenoptions do not necessarily look for bottlenecks, 1Python code can be found at: https: / / github.com / mcmachado / options.This enables us to apply our algorithm in many environments where there are no obvious or significant bottlenecks. In these environments we find meaningful options, such as walking down a corner or going into the corners of an open space. Interestingly, doorways are not the first options we discover in the 4-room domain (the fifth eigenoption is the first one that ends at the door)."}, {"heading": "4.2. Exploration", "text": "This year, more than ever before in the history of the country in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country and in which it is a country."}, {"heading": "4.3. Accumulating Rewards", "text": "We are now demonstrating the usefulness of our options when the agent's goal is to accumulate reward options. We are also investigating the effects of an increasing number of options in such a task. In these experiments, the agent starts at the bottom left corner and his goal is to reach the top right corner. The agent observes a reward of 0 until the goal is reached when he observes a reward of + 1. We used Q-Learning (Watkins & Dayan, 1992) (\u03b1 = 0.1, \u03b3 = 0.9) to learn a policy on primitive actions. Behavioral policy consistently selects primitive actions and options and follows them to completion. Figure 7 shows, after learning for a certain number of episodes, the average of over 100 studies on the final performance of the agents. Episodes were 100 time steps long, and we learned for 250 episodes in the 10-10 grid and in the I-labyrinth, and for 500 episodes the use of performance in the domain 4 is improved, such as the property options."}, {"heading": "5. Approximate Option Discovery", "text": "So far, we have assumed that agents have access to the adjacence matrix that represents the underlying MDP, but in practice, this is generally not true. In fact, the number of states in these environments is often so large that agents rarely visit the same state twice. Generally, these problems are addressed using sample-based methods and some kind of functional approximation. In this section, we propose a sample-based approach to option discovery that asymptotically detects own options. We then extend this algorithm to linear functional approximation. In Atari 2600 games, we provide anecdotal evidence that this relatively naive sample-based approach to functional approximation reveals targeted options."}, {"heading": "5.1. Sample-based Option Discovery", "text": "In the online setting, agents must scan the trajectories. \u2212 Of course, you can scan the trajectories > > until you are able to perfectly construct the adjacence matrix of the MDP, as suggested by Mahadevan & Maggioni (2007). However, this approach does not simply extend to an approximation of the linear function. In this section, we provide an approach that does not build the adjacence matrix and allows us to extend the concept of the proper purpose to the approximation of the linear function. In our algorithm, a sample transition to a matrix T is added if it has not previously been encountered. The transition is added as the difference between current and previous observations, i.e., the addition of the proper purpose to the approximation of the linear function (s)."}, {"heading": "5.2. Function Approximation", "text": "In fact, it is as if we cannot understand the intentions that we have pursued in recent years simply because we are able to get a grip on them. (...) In fact, we cannot understand the intentions that we have pursued in the past. (...) It is as if we have been able to surpass them. (...) It is as if we have been able to surpass them. (...) It is as if we have been able to surpass them. (...) It is as if they are able to surpass them. (...) (...) It is as if they have been able to surpass themselves. (...) It is as if they are able to surpass themselves. (...) It is as if they are able to surpass themselves. (...) It is as if they are able to surpass themselves. (...) It is as if they are able to surpass themselves. (...)"}, {"heading": "6. Related Work", "text": "In fact, most of them are able to move to another world, in which they are able to move, and in which they are able to move."}, {"heading": "7. Conclusion", "text": "The ability to correctly abstract MDPs in SMDPs can reduce the total cost of learning (Sutton et al., 1999; Solway et al., 2014), especially when the learned options are reused in multiple tasks. On the other hand, the wrong hierarchy can impede the agent's learning process and move the agent away from the desired target states. Current option finding algorithms often depend on an initial informative reward signal that may not be readily available in large MDPs. In this paper, we have introduced an approach that is effective for a variety of tasks in different environments. Our algorithm uses the Laplacian graph, which is directly related to the concept of proto-value functions. The learned presentation informs the agent of what meaningful options are to look for. The options discovered can be seen as traversing every single dimension in the learned representation. We believe that successful algorithms will be able to discover the future of options, and options simultaneously."}, {"heading": "Acknowledgements", "text": "The authors thank Will Dabney, Re \ufffd mi Munos and Csaba Szepesva \ufffd ri for their useful discussions. This work was supported by grants from Alberta Innovates Technology Futures and the Alberta Machine Intelligence Institute (Amii)."}, {"heading": "Appendix: Supplementary Material", "text": "This supplementary material contains details omitted from the main text for reasons of space. Below is a list of contents: \u2022 Support of Lemmata and their respective proofs and a more detailed proofs for Theorem 3.1; \u2022 Description of how to easily calculate the diffusion time in tabular MDPs; \u2022 The options leading to bottlenecks (gateways) that we have used in our experiments; \u2022 Performance comparisons between proprietary options and options generated to achieve randomly selected states; \u2022 Demonstration of the applicability of proprietary options in multiple tasks with a new set of experiments; \u2022 More details on the empirical framework used in the arcade learning environment."}, {"heading": "A. Lemmas and Proofs", "text": "Lemma (I + A) \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 v (I + A) \u2212 v (I + A) \u2212 v (I + A) \u2212 v (I + A) \u2212 v (I + A) \u2212 v (I + A) \u2212 v (I + A) \u2212 v (I + A) \u2212 v (I + A) \u2212 v (I + A) \u2212 v (I + A) \u2212 v (I + A) \u2212 v (I + A) \u2212 v (I + A) \u2212 v (I + A) \u2212 v (I + A) \u2212 v (I + A) \u2212 v (I + V)."}, {"heading": "B. Diffusion Time Computation", "text": "In the main work, we have introduced diffusion time as a new measurement to evaluate the exploration steps, but we have not discussed how to calculate it. Diffusion time encodes the expected number of time steps required to navigate between any two states in the MDP following a random course. In tabular areas, we can easily calculate the diffusion time using dynamic programming. To do this, we define a new MDP in such a way that the value function of a state encodes the expected number of steps needed to navigate between states and a selected target state within the framework of a uniform random policy. We can then calculate the expected number of steps between any two states by assigning the value of all other states.The MDP, in which the value function of the state s encodes the expected number of time steps from s to a target state, has inverted = 1 and a reward function in which the agent + 1 does not comply with the target state at any point in time."}, {"heading": "C. Options Leading to Doorways in the 4-room Domain", "text": "Figure 10 shows the four options that we refer to in Section 4 as options that lead to bootleneck states, i.e. doors. Each option is defined in a room and moves the agent toward the next door. These options were inspired by the discussion by Solway et al. (2014) about the optimal options discovered by their algorithm."}, {"heading": "D. Comparison to Random Options", "text": "In this section, we show the importance of using information about diffusion in the environment to define the options that have been selected independently, and this information influences the order of the sub-targets that the options are looking for, as well as the timescales in which they are operating; the order in which the self-options are discovered and the different timescales in which they are operating can have a major impact on the performance of the agents; we show the importance of using the diffusion information in the environment by comparing our approach to the random options, a simple baseline in which this information is not used; this baseline defines an option that is defined throughout the state, ending in a randomly selected state of the environment; we conducted our experiments in the tabular case because it is not clear how we can extend this baseline to settings in which the states cannot be enumerated; Figure 11a shows the diffusion time (c.f. B) of random options and eigendomains becoming 4."}, {"heading": "D. Empirical Evaluation of the Agent\u2019s Performance in Multiple Tasks", "text": "In Section 4, we argued that self-options are useful for multiple tasks, based on results that show that self-options allow us to find and accumulate rewards more quickly. Here, we explicitly demonstrate the uselessness of self-options for multiple tasks. We evaluate the performance of agents for different start and finish states in the 4-room area. As in Section 4.3, we use Q-Learning (\u03b1 = 0.1, \u03b3 = 0.9) to learn a policy on primitive actions. Behavioral policy consistently selects primitive actions and options and follows them to completion. Episodes were 100 time steps long, and we learned for 250 episodes. For clarity, we zoom in on the diagrams to the interval in which the agents are still learning. Figure 14 shows, after learning for a predetermined number of episodes, goals and options that the average over 100 studies on the final performance of the agents as well as the initial (S) and target states (G) are set to nearly the same number of options we have opted on our previous ones (64)."}, {"heading": "E. Experimental Setup in the Arcade Learning Environment", "text": "We have defined six different starting states in each Atari 2600 game, allowing the agent to perform random actions from this point to the end. The agent follows a predetermined sequence of actions that lead him to each starting state. We store the observed transitions that lead the agent to the starting state, as well as those achieved through random actions. In the main paper, we provide results for FREEWAY and MONTEZUMA'S REVENGE. In this section, we also provide results for MS PAC-MAN. Initial states for all three games are shown in Figure 12. The agent plays rounds of six episodes, with each episode starting from a different starting state until he observes at least 25,000 new transitions. The last incident matrix in which we executed the SVD had 25,000 rows, which we consistently sampled from the series of observed transitions. The agent plays rounds of six episodes, with each episode starting from a different starting state, from at least 25,000 new transitions, to 25,000."}], "references": [{"title": "On the Bottleneck Concept for Options Discovery: Theoretical Underpinnings and Extension in Continuous State Spaces", "author": ["Bacon", "Pierre-Luc"], "venue": null, "citeRegEx": "Bacon and Pierre.Luc.,? \\Q2013\\E", "shortCiteRegEx": "Bacon and Pierre.Luc.", "year": 2013}, {"title": "The option-critic architecture", "author": ["Bacon", "Pierre-Luc", "Harb", "Jean", "Precup", "Doina"], "venue": "In Proceedings of the National Conference on Artificial Intelligence (AAAI),", "citeRegEx": "Bacon et al\\.,? \\Q2017\\E", "shortCiteRegEx": "Bacon et al\\.", "year": 2017}, {"title": "Active learning of inverse models with intrinsically motivated goal exploration in robots", "author": ["Baranes", "Adrien", "Oudeyer", "Pierre-Yves"], "venue": "Robotics and Autonomous Systems,", "citeRegEx": "Baranes et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Baranes et al\\.", "year": 2013}, {"title": "The Arcade Learning Environment: An Evaluation Platform for General Agents", "author": ["Bellemare", "Marc G", "Naddaf", "Yavar", "Veness", "Joel", "Bowling", "Michael"], "venue": "Journal of Artificial Intelligence Research,", "citeRegEx": "Bellemare et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Bellemare et al\\.", "year": 2013}, {"title": "Dynamic Programming", "author": ["Bellman", "Richard E"], "venue": null, "citeRegEx": "Bellman and E.,? \\Q1957\\E", "shortCiteRegEx": "Bellman and E.", "year": 1957}, {"title": "Using Relative Novelty to Identify Useful Temporal Abstractions in Reinforcement Learning", "author": ["\u015eim\u015fek", "\u00d6zg\u00fcr", "Barto", "Andrew G"], "venue": "In Proceedings of the International Conference on Machine Learning (ICML),", "citeRegEx": "\u015eim\u015fek et al\\.,? \\Q2004\\E", "shortCiteRegEx": "\u015eim\u015fek et al\\.", "year": 2004}, {"title": "Skill Characterization Based on Betweenness", "author": ["\u015eim\u015fek", "\u00d6zg\u00fcr", "Barto", "Andrew G"], "venue": "In Proceedings of Advances in Neural Information Processing Systems (NIPS),", "citeRegEx": "\u015eim\u015fek et al\\.,? \\Q2008\\E", "shortCiteRegEx": "\u015eim\u015fek et al\\.", "year": 2008}, {"title": "Identifying Useful Subgoals in Reinforcement Learning by Local Graph Partitioning", "author": ["\u015eim\u015fek", "\u00d6zg\u00fcr", "Wolfe", "Alicia P", "Barto", "Andrew G"], "venue": "In Proceedings of the International Conference on Machine Learning (ICML),", "citeRegEx": "\u015eim\u015fek et al\\.,? \\Q2005\\E", "shortCiteRegEx": "\u015eim\u015fek et al\\.", "year": 2005}, {"title": "Probabilistic Inference for Determining Options in Reinforcement Learning", "author": ["Daniel", "Christian", "van Hoof", "Herke", "Peters", "Jan", "Neumann", "Gerhard"], "venue": "Machine Learning,", "citeRegEx": "Daniel et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Daniel et al\\.", "year": 2016}, {"title": "Hierarchical Reinforcement Learning with the MAXQ Value Function Decomposition", "author": ["Dietterich", "Thomas G"], "venue": "Journal of Artificial Intelligence Research (JAIR),", "citeRegEx": "Dietterich and G.,? \\Q2000\\E", "shortCiteRegEx": "Dietterich and G.", "year": 2000}, {"title": "Discovering Hierarchy in Reinforcement Learning with HEXQ", "author": ["Hengst", "Bernhard"], "venue": "In Proceedings of the International Conference on Machine Learning (ICML),", "citeRegEx": "Hengst and Bernhard.,? \\Q2002\\E", "shortCiteRegEx": "Hengst and Bernhard.", "year": 2002}, {"title": "Incremental Slow Feature Analysis", "author": ["Kompella", "Varun Raj", "Luciw", "Matthew D", "Schmidhuber", "J\u00fcrgen"], "venue": "In Proceedings of the International Joint Conference on Artificial Intelligence (IJCAI),", "citeRegEx": "Kompella et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Kompella et al\\.", "year": 2011}, {"title": "Continual Curiosity-Driven Skill Acquisition from High-Dimensional Video Inputs for Humanoid Robots", "author": ["Kompella", "Varun Raj", "Stollenga", "Marijn", "Luciw", "Matthew", "Schmidhuber", "Juergen"], "venue": "Artificial Intelligence,", "citeRegEx": "Kompella et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Kompella et al\\.", "year": 2015}, {"title": "Skill Discovery in Continuous Reinforcement Learning Domains using Skill Chaining", "author": ["Konidaris", "George", "Barto", "Andrew"], "venue": "In Proceedings of Advances in Neural Information Processing Systems (NIPS),", "citeRegEx": "Konidaris et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Konidaris et al\\.", "year": 2009}, {"title": "Hierarchical Deep Reinforcement Learning: Integrating Temporal Abstraction and Intrinsic Motivation", "author": ["Kulkarni", "Tejas D", "Narasimhan", "Karthik R", "Saeedi", "Ardavan", "Tenenbaum", "Joshua B"], "venue": "ArXiv e-prints,", "citeRegEx": "Kulkarni et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Kulkarni et al\\.", "year": 2016}, {"title": "Option Discovery in Hierarchical Reinforcement Learning using Spatio-Temporal Clustering", "author": ["Lakshminarayanan", "Aravind", "Krishnamurthy", "Ramnandan", "Kumar", "Peeyush", "Ravindran", "Balaraman"], "venue": "CoRR, abs/1605.05359,", "citeRegEx": "Lakshminarayanan et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Lakshminarayanan et al\\.", "year": 2016}, {"title": "Learning Purposeful Behaviour in the Absence of Rewards", "author": ["Machado", "Marlos C", "Bowling", "Michael"], "venue": "CoRR, abs/1410.4604,", "citeRegEx": "Machado et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Machado et al\\.", "year": 2016}, {"title": "Proto-Value Functions: Developmental Reinforcement Learning", "author": ["Mahadevan", "Sridhar"], "venue": "In Proceedings of the International Conference on Machine Learning (ICML),", "citeRegEx": "Mahadevan and Sridhar.,? \\Q2005\\E", "shortCiteRegEx": "Mahadevan and Sridhar.", "year": 2005}, {"title": "Proto-value Functions: A Laplacian Framework for Learning Representation and Control in Markov Decision Processes", "author": ["Mahadevan", "Sridhar", "Maggioni", "Mauro"], "venue": "Journal of Machine Learning Research (JMLR),", "citeRegEx": "Mahadevan et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Mahadevan et al\\.", "year": 2007}, {"title": "Adaptive Skills Adaptive Partitions (ASAP)", "author": ["Mankowitz", "Daniel J", "Mann", "Timothy Arthur", "Mannor", "Shie"], "venue": "In Proceedings of Advances in Neural Information Processing Systems (NIPS),", "citeRegEx": "Mankowitz et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Mankowitz et al\\.", "year": 2016}, {"title": "Dynamic Abstraction in Reinforcement Learning via Clustering", "author": ["Mannor", "Shie", "Menache", "Ishai", "Hoze", "Amit", "Klein", "Uri"], "venue": "In Proceedings of the International Conference on Machine Learning (ICML),", "citeRegEx": "Mannor et al\\.,? \\Q2004\\E", "shortCiteRegEx": "Mannor et al\\.", "year": 2004}, {"title": "Automatic Discovery of Subgoals in Reinforcement Learning using Diverse Density", "author": ["McGovern", "Amy", "Barto", "Andrew G"], "venue": "In Proceedings of the International Conference on Machine Learning (ICML),", "citeRegEx": "McGovern et al\\.,? \\Q2001\\E", "shortCiteRegEx": "McGovern et al\\.", "year": 2001}, {"title": "QCut - Dynamic Discovery of Sub-goals in Reinforcement Learning", "author": ["Menache", "Ishai", "Mannor", "Shie", "Shimkin", "Nahum"], "venue": "In Proceedings of the European Conference on Machine Learning (ECML),", "citeRegEx": "Menache et al\\.,? \\Q2002\\E", "shortCiteRegEx": "Menache et al\\.", "year": 2002}, {"title": "Control of Memory, Active Perception, and Action in Minecraft", "author": ["Oh", "Junhyuk", "Chockalingam", "Valliappa", "Singh", "Satinder P", "Lee", "Honglak"], "venue": "In Proceedings of the International Conference on Machine Learning (ICML),", "citeRegEx": "Oh et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Oh et al\\.", "year": 2016}, {"title": "Generalization and Exploration via Randomized Value Functions", "author": ["Osband", "Ian", "Roy", "Benjamin Van", "Wen", "Zheng"], "venue": "In Proceedings of the International Conference on Machine Learning (ICML),", "citeRegEx": "Osband et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Osband et al\\.", "year": 2016}, {"title": "Temporal Abstraction in Reinforcement Learning", "author": ["Precup", "Doina"], "venue": "PhD thesis, University of Massachusetts Amherst,", "citeRegEx": "Precup and Doina.,? \\Q2000\\E", "shortCiteRegEx": "Precup and Doina.", "year": 2000}, {"title": "Markov Decision Processes: Discrete Stochastic Dynamic Programming", "author": ["Puterman", "Martin L"], "venue": null, "citeRegEx": "Puterman and L.,? \\Q1994\\E", "shortCiteRegEx": "Puterman and L.", "year": 1994}, {"title": "Empowerment \u2013 An Introduction", "author": ["Salge", "Christoph", "Glackin", "Cornelius", "Polani", "Daniel"], "venue": "In Guided SelfOrganization: Inception,", "citeRegEx": "Salge et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Salge et al\\.", "year": 2014}, {"title": "Optimal Behavioral Hierarchy", "author": ["Solway", "Alec", "Diuk", "Carlos", "C\u00f3rdova", "Natalia", "Yee", "Debbie", "Barto", "Andrew G", "Niv", "Yael", "Botvinick", "Matthew M"], "venue": "PLOS Computational Biology,", "citeRegEx": "Solway et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Solway et al\\.", "year": 2014}, {"title": "On the Relation of Slow Feature Analysis and Laplacian Eigenmaps", "author": ["Sprekeler", "Henning"], "venue": "Neural Computation,", "citeRegEx": "Sprekeler and Henning.,? \\Q2011\\E", "shortCiteRegEx": "Sprekeler and Henning.", "year": 2011}, {"title": "Reinforcement Learning: An Introduction", "author": ["Sutton", "Richard S", "Barto", "Andrew G"], "venue": null, "citeRegEx": "Sutton et al\\.,? \\Q1998\\E", "shortCiteRegEx": "Sutton et al\\.", "year": 1998}, {"title": "Between MDPs and semi-MDPs: A Framework for Temporal Abstraction in Reinforcement Learning", "author": ["Sutton", "Richard S", "Precup", "Doina", "Singh", "Satinder"], "venue": "Artificial Intelligence,", "citeRegEx": "Sutton et al\\.,? \\Q1999\\E", "shortCiteRegEx": "Sutton et al\\.", "year": 1999}, {"title": "Algorithms for Reinforcement Learning. Synthesis Lectures on Artificial Intelligence and Machine Learning", "author": ["Szepesv\u00e1ri", "Csaba"], "venue": null, "citeRegEx": "Szepesv\u00e1ri and Csaba.,? \\Q2010\\E", "shortCiteRegEx": "Szepesv\u00e1ri and Csaba.", "year": 2010}, {"title": "Technical Note: Q-Learning", "author": ["Watkins", "Christopher J.C. H", "Dayan", "Peter"], "venue": "Machine Learning,", "citeRegEx": "Watkins et al\\.,? \\Q1992\\E", "shortCiteRegEx": "Watkins et al\\.", "year": 1992}, {"title": "Perron Cluster Analysis and Its Connection to Graph Partitioning for Noisy Data", "author": ["Weber", "Marcus", "Rungsarityotin", "Wasinee", "Schliep", "Alexander"], "venue": "Technical Report 04-39, ZIB,", "citeRegEx": "Weber et al\\.,? \\Q2004\\E", "shortCiteRegEx": "Weber et al\\.", "year": 2004}, {"title": "Slow Feature Analysis: Unsupervised Learning of Invariances", "author": ["Wiskott", "Laurenz", "Sejnowski", "Terrence J"], "venue": "Neural Computation,", "citeRegEx": "Wiskott et al\\.,? \\Q2002\\E", "shortCiteRegEx": "Wiskott et al\\.", "year": 2002}, {"title": "Options Leading to Doorways in the 4-room Domain Figure 10 depicts the four options we refer to in Section 4 as the options leading to bootleneck states, i.e., doorways. Each option is defined in a room and it moves the agent toward the closest doorway", "author": ["C. MDP"], "venue": "These options were inspired by Solway et al", "citeRegEx": "MDP.,? \\Q2014\\E", "shortCiteRegEx": "MDP.", "year": 2014}], "referenceMentions": [{"referenceID": 31, "context": "Proto-value functions (PVFs) are a well-known solution for the problem of representation learning (Mahadevan, 2005; Mahadevan & Maggioni, 2007); while the problem of skill discovery is generally posed under the options framework (Sutton et al., 1999; Precup, 2000), which models skills as options.", "startOffset": 229, "endOffset": 264}, {"referenceID": 7, "context": "Exploration, while traditionally a separate problem from option discovery, can also be addressed through the careful construction of options (McGovern & Barto, 2001; \u015eim\u015fek et al., 2005; Solway et al., 2014; Kulkarni et al., 2016).", "startOffset": 141, "endOffset": 230}, {"referenceID": 28, "context": "Exploration, while traditionally a separate problem from option discovery, can also be addressed through the careful construction of options (McGovern & Barto, 2001; \u015eim\u015fek et al., 2005; Solway et al., 2014; Kulkarni et al., 2016).", "startOffset": 141, "endOffset": 230}, {"referenceID": 14, "context": "Exploration, while traditionally a separate problem from option discovery, can also be addressed through the careful construction of options (McGovern & Barto, 2001; \u015eim\u015fek et al., 2005; Solway et al., 2014; Kulkarni et al., 2016).", "startOffset": 141, "endOffset": 230}, {"referenceID": 28, "context": ", doorways) (\u015eim\u015fek & Barto, 2004; Solway et al., 2014).", "startOffset": 12, "endOffset": 55}, {"referenceID": 28, "context": "They have been shown to be very efficient for planning as these states are the states most frequently visited when considering the shortest distance between any two states in an MDP (Solway et al., 2014).", "startOffset": 182, "endOffset": 203}, {"referenceID": 24, "context": "A major challenge for agents to explore an environment is to be decisive, avoiding the dithering commonly observed in random walks (Machado & Bowling, 2016; Osband et al., 2016).", "startOffset": 131, "endOffset": 177}, {"referenceID": 36, "context": "Naturally, one can sample trajectories until one is able to perfectly construct the MDP\u2019s adjacency matrix, as suggested by Mahadevan & Maggioni (2007). However, this approach does not easily extend to linear function approximation.", "startOffset": 84, "endOffset": 152}, {"referenceID": 3, "context": "We tested our method in the ALE (Bellemare et al., 2013).", "startOffset": 32, "endOffset": 56}, {"referenceID": 14, "context": "Interestingly, the options we discover are very similar to those handcrafted by Kulkarni et al. (2016) when evaluating the usefulness of options to tackle such a game.", "startOffset": 80, "endOffset": 103}, {"referenceID": 22, "context": "There are many approaches based on this principle, such as methods that use the observed rewards to generate intrinsic rewards leading to new value functions (e.g., McGovern & Barto, 2001; Menache et al., 2002; Konidaris & Barto, 2009), methods that use the observed rewards to climb a gradient (e.", "startOffset": 158, "endOffset": 235}, {"referenceID": 1, "context": ", 2002; Konidaris & Barto, 2009), methods that use the observed rewards to climb a gradient (e.g., Mankowitz et al., 2016; Vezhnevets et al., 2016; Bacon et al., 2017), or to do https://youtu.", "startOffset": 92, "endOffset": 167}, {"referenceID": 8, "context": "probabilistic inference (Daniel et al., 2016).", "startOffset": 24, "endOffset": 45}, {"referenceID": 7, "context": "Among the approaches to discover options without using extrinsic rewards are the use of global or local graph centrality measures (\u015eim\u015fek & Barto, 2004; \u015eim\u015fek et al., 2005; \u015eim\u015fek & Barto, 2008) and clustering of states (Mannor et al.", "startOffset": 130, "endOffset": 195}, {"referenceID": 20, "context": ", 2005; \u015eim\u015fek & Barto, 2008) and clustering of states (Mannor et al., 2004; Bacon, 2013; Lakshminarayanan et al., 2016).", "startOffset": 55, "endOffset": 120}, {"referenceID": 15, "context": ", 2005; \u015eim\u015fek & Barto, 2008) and clustering of states (Mannor et al., 2004; Bacon, 2013; Lakshminarayanan et al., 2016).", "startOffset": 55, "endOffset": 120}, {"referenceID": 5, "context": "Among the approaches to discover options without using extrinsic rewards are the use of global or local graph centrality measures (\u015eim\u015fek & Barto, 2004; \u015eim\u015fek et al., 2005; \u015eim\u015fek & Barto, 2008) and clustering of states (Mannor et al., 2004; Bacon, 2013; Lakshminarayanan et al., 2016). Interestingly, \u015eim\u015fek et al. (2005) and Lakshminarayanan et al.", "startOffset": 153, "endOffset": 324}, {"referenceID": 5, "context": "Among the approaches to discover options without using extrinsic rewards are the use of global or local graph centrality measures (\u015eim\u015fek & Barto, 2004; \u015eim\u015fek et al., 2005; \u015eim\u015fek & Barto, 2008) and clustering of states (Mannor et al., 2004; Bacon, 2013; Lakshminarayanan et al., 2016). Interestingly, \u015eim\u015fek et al. (2005) and Lakshminarayanan et al. (2016) also use the graph Laplacian in their algorithm, but to identify bottleneck states.", "startOffset": 153, "endOffset": 359}, {"referenceID": 28, "context": "Recently, Solway et al. (2014) proved that \u201coptimal hierarchy minimizes the geometric mean number of trial-and-error attempts necessary for the agent to discover the optimal policy for any selected task (.", "startOffset": 10, "endOffset": 31}, {"referenceID": 27, "context": "(2016) proposed an algorithm in which agents discover options by maximizing a notion of empowerment (Salge et al., 2014), where the agent aims at getting to states with a maximal set of available intrinsic options.", "startOffset": 100, "endOffset": 120}, {"referenceID": 11, "context": "While we use PVFs, CCSA uses Incremental Slow Feature Analysis (SFA) (Kompella et al., 2011) to define the intrinsic reward function.", "startOffset": 69, "endOffset": 92}, {"referenceID": 11, "context": "Continual Curiosity driven Skill Acquisition (CCSA) (Kompella et al., In Press) is the closest approach to ours. CCSA also discovers skills that maximize an intrinsic reward obtained by some extracted representation. While we use PVFs, CCSA uses Incremental Slow Feature Analysis (SFA) (Kompella et al., 2011) to define the intrinsic reward function. Sprekeler (2011) has shown that, given a specific choice of adjacency function, PVFs are equivalent to SFA (Wiskott & Sejnowski, 2002).", "startOffset": 53, "endOffset": 368}, {"referenceID": 31, "context": "Being able to properly abstract MDPs into SMDPs can reduce the overall expense of learning (Sutton et al., 1999; Solway et al., 2014), mainly when the learned options are reused in multiple tasks.", "startOffset": 91, "endOffset": 133}, {"referenceID": 28, "context": "Being able to properly abstract MDPs into SMDPs can reduce the overall expense of learning (Sutton et al., 1999; Solway et al., 2014), mainly when the learned options are reused in multiple tasks.", "startOffset": 91, "endOffset": 133}], "year": 2017, "abstractText": "Representation learning and option discovery are two of the biggest challenges in reinforcement learning (RL). Proto-value functions (PVFs) are a well-known approach for representation learning in MDPs. In this paper we address the option discovery problem by showing how PVFs implicitly define options. We do it by introducing eigenpurposes, intrinsic reward functions derived from the learned representations. The options discovered from eigenpurposes traverse the principal directions of the state space. They are useful for multiple tasks because they are discovered without taking the environment\u2019s rewards into consideration. Moreover, different options act at different time scales, making them helpful for exploration. We demonstrate features of eigenpurposes in traditional tabular domains as well as in Atari 2600 games.", "creator": "LaTeX with hyperref package"}}}