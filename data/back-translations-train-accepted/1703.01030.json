{"id": "1703.01030", "review": {"conference": "ICML", "VERSION": "v1", "DATE_OF_SUBMISSION": "3-Mar-2017", "title": "Deeply AggreVaTeD: Differentiable Imitation Learning for Sequential Prediction", "abstract": "Researchers have demonstrated state-of-the-art performance in sequential decision making problems (e.g., robotics control, sequential prediction) with deep neural network models. One often has access to near-optimal oracles that achieve good performance on the task during training. We demonstrate that AggreVaTeD --- a policy gradient extension of the Imitation Learning (IL) approach of (Ross &amp; Bagnell, 2014) --- can leverage such an oracle to achieve faster and better solutions with less training data than a less-informed Reinforcement Learning (RL) technique. Using both feedforward and recurrent neural network predictors, we present stochastic gradient procedures on a sequential prediction task, dependency-parsing from raw image data, as well as on various high dimensional robotics control problems. We also provide a comprehensive theoretical study of IL that demonstrates we can expect up to exponentially lower sample complexity for learning with AggreVaTeD than with RL algorithms, which backs our empirical findings. Our results and theory indicate that the proposed approach can achieve superior performance with respect to the oracle when the demonstrator is sub-optimal.", "histories": [["v1", "Fri, 3 Mar 2017 04:12:03 GMT  (283kb,D)", "http://arxiv.org/abs/1703.01030v1", "17 pages"]], "COMMENTS": "17 pages", "reviews": [], "SUBJECTS": "cs.LG", "authors": ["wen sun", "arun venkatraman", "geoffrey j gordon", "byron boots", "j andrew bagnell"], "accepted": true, "id": "1703.01030"}, "pdf": {"name": "1703.01030.pdf", "metadata": {"source": "META", "title": "Deeply AggreVaTeD: Differentiable Imitation Learning for Sequential Prediction", "authors": ["Wen Sun", "Arun Venkatraman", "Geoffrey J. Gordon", "Byron Boots", "J. Andrew Bagnell"], "emails": ["WENSUN@CS.CMU.EDU", "ARUNVENK@CS.CMU.EDU", "GGORDON@CS.CMU.EDU", "BBOOTS@CC.GATECH.EDU", "DBAGNELL@RI.CMU.EDU"], "sections": [{"heading": "1. Introduction", "text": "A fundamental challenge in the field of artificial intelligence, robotics, and language processing is to reason, plan, and predict the future prediction policy. A fundamental challenge in the field of artificial intelligence, early detection, and predictive power is to reason, plan, and make decisions to minimize accumulated costs, achieve a long-term goal, or optimize a loss that was acquired only after many predictions. (RL), especially deep RL learning processes, have, however, dramatically advanced the state of the art in sequential prediction problems in high-dimensional robot control tasks. (RL learning methods) Researchers are beginning to use deep RL methods to achieve high performance (Ranzato et al., 2015; Bahdanau et al., 2016; Li et al, 2016). Frequently in sequential prediction tasks, future predictions depend on the learner's previous history."}, {"heading": "2. Preliminaries", "text": "In order to develop algorithms that justify long-term decision-making, it is convenient to throw the problem into the Markov decision-making process (MDP). The MDP framework consists of a set of states, measures (which emanate from a policy), costs (loss), and a model that defines transition processes in certain states. Although most sequential predictions can be thrown into the same framework (thumb), the configuration of the robot is the configuration of the state, the controls (e.g. common torques) are the actions, and the costs relate to achieving a task (e.g. distance). Although most sequential predictions can be thrown into the same framework (thumb) III et al., 2009 The actions are the predictions of the learner (e.g. RNN). The state is then the result of all predictions made so far (e.g. dependence on a tree constructed so far or the words translated so far). The cumulative costs are eAS (e.i.2) negative."}, {"heading": "3. Differentiable Imitation Learning", "text": "Political imitation aims to learn a political approach that approaches the performance of the expert approach in times when such methods are no longer available. To learn rich strategies such as LSTMs or deep networks (Schulman et al., 2015), we derive a method for mimicking learning processes and sequential predictions. The basic idea in Ross & Bagnell (2014) is to use an online learner to update strategies that use the following loss function in each episode n: \"n (Ross & Bagnell, 2014)\" n H \"to learn strategies represented by expressive differentiated function approximations. The basic idea in Ross & Bagnell (2014) is to update strategies that include the following loss function in each episode n:\" n \"n.\""}, {"heading": "3.1. Online Gradient Descent", "text": "(3) For continuous spaces of action, we cannot simply replace summation with integration, since in practice it is impossible to evaluate Q; t (s, a) for an infinite number of a, so instead we use a weighting of importance to rephrase 'n (st, a) as' n (s, a), since in practice it is impossible to evaluate Q; t (s, a) for an infinite number of a, so we use a weighting to rephrase 'n (g; n) as' n (s, a) = 1H H H H; appendix; t = 1 E; d; n); one (a; s; s)."}, {"heading": "3.2. Policy Updates with Natural Gradient Descent", "text": "We derive a natural gradient updating process for imitation acquisition inspired by the success of the natural gradient downgrade in RL (Kakade, 2002; Bagnell & Schneider, 2003; Schulman et al., 2015). First, we show that Exponential Gradient Descent (EC) can be used to accelerate imitation acquisition in discrete MDPs. Then, we extend EC to continuous MDPs, where we show that EC leads to a natural gradient updating process with three steps of approximation."}, {"heading": "3.2.1. EXPONENTIAL GRADIENT IN DISCRETE MDPS", "text": "For the simplification of notation, we represent the policy of discrete probability vectors (A). We also represent d\u03c0t as a probability vector of the S-d simplex dimension, consisting of d\u03c0t (s), d\u03c0t (s), d\u03c0t (s), d\u03c0t (s), d\u03c0stststststststststststststststststststststststststststststststststststststststststststststststststststststststststststststststststststststststststststststststststststststststststststststststststststststststststststststststststststststststststststststststststststststststststststststststststststststststststststststststststststststststststststststststststststststststststststststststststststststststststststststststststststststststststststststststststststststststststststststststststststststststststststststststststststststststststststststststststststststststststststststststststststststststststststststststststststststststststststststststststststststststststststststststststststststststststststststststststststststststststststststststststststststststststststststststststststststststststststststststststststststststststststststststststststststststststststststst"}, {"heading": "3.2.2. CONTINUOUS MDPS", "text": "If one replaces sums with integrals, Eq. 6 can be written as follows: predetermined (predetermined) breaking point (predetermined) n predetermined (predetermined) breaking point (predetermined) n predetermined (predetermined) breaking point (predetermined) n predetermined (predetermined) breaking point (predetermined) n predetermined (predetermined) breaking point (predetermined) n predetermined (predetermined) n predetermined (predetermined) breaking point (predetermined) n predetermined) n predetermined (predetermined) breaking point (predetermined) n predetermined) n (predetermined) (predetermined) breaking point (predetermined) n predetermined) (predetermined)"}, {"heading": "4. Sample-Based Practical Algorithms", "text": "In the previous section, we derived a regular gradient update procedure and a natural gradient update procedure for IL. Note that all calculations of gradients and Fisher information matrices assume that it is possible to accurately calculate expectations, including Es \u0445 d\u03c0 and Ea \u0445 \u03c0 (a | s). In this section, we provide practical algorithms that approximate the gradients and Fisher information matrices based on finite samples collected during policy execution."}, {"heading": "4.1. Gradient Estimation and Variance Reduction", "text": "We consider an episodic framework in which a policy is given to sequence n, we roll in place of Q (\"Q\") n (\"Q\") n (\"K\") n (\"K\") n (\"K\") n (\"K\") n (\"K\") n (\"K\") n (\"K\") n (\"K\") n (\"K\" n \"n (\" K \"n) n (\" K \") n (\" K \") n (\" K \") n (\" K \"n\" n (\"K\" n) n (\"K\" n \"n) n (\" K \"n\" n) n (\"K\" n \"n) n (\" K \"n\" n) n (\"n\" n) n (\"n) n (\" n) n (\"n) n (K\" n) n (K \"n) n (K\" n) n (\"n) n (K\" n) n (K \"n) n (K\" n) n (K \"n) n (K\" n) n (K \"n) n (K\" n) n (K \"n) n (K\" n) n (K \"n) n (K\" n) n (K \"n) n) n (K\" n) n (K \"n) n) n (K\" n) n (K \"n) n) n (K\" n) n) n (K \"n) n) n (K\" n) n) n (K \"n) n) n (K\" n) n (K \"n) n) n (K\" n) n) n (K \"n\" n) n (K \"n) n) n (K\" n) n (K \"n) n) n (K\" n) n (K \"n) n) n (K\" n) n (K \"n) n (K\" n \"n\" n \"n) n\" n \"n\" n (K \"n) n (K\" n) n) n (K \"n) n\" n (K \"n) n (K\" n) n (K \"n\" n \"n) n\" n \"n\" n \"n\" n \"n (K\" n) n (K \"n\" n \""}, {"heading": "4.2. Differentiable Imitation Learning: AggreVaTeD", "text": "We present the differentiated imitation learning system AggreVaTeD in Alg. 1. For each iteration n, the roll-in policy \u03c0 n is a mixture of the expert policy \u03c0 * and the current policy \u03c0\u03b8n, with a mixing rate \u03b1 (\u03b1n \u2192 0, n \u2192 \u221e): at each step, with the probability \u03b1, \u03c0 n picks. (In line 6, you can either choose Equation 10 or the corresponding reduced variance estimate Eq. 12 (Eq. 11 and Eq. 13 for continuous measures) for IL and later in the sequence forecast (Bengio et al., 2015). In line 6, you can choose Equation 10 or the corresponding variant reduced estimate Eq. 11 and Eq. 13 for continuous measures to perform a regular gradient decrease, and choose CG to perform natural gradient deviations. In line 6, you can choose equations: Eq. 11 and sequential prediction algorithms (Rossgorithm AggreTeD)."}, {"heading": "5. Quantify the Gap: An Analysis of IL vs RL", "text": "How much faster can IL learn a good policy than RL? In this section, we quantify the gap in discrete MDPs when IL (1) can ask for an optimal Q * or (2) for a loud but unbiased estimate of Q *. To measure the rate of learning, we consider the cumulative regret of the entire learning process, defined as RN = \u2211 N = 1 (\u00b5 (\u03c0n) \u2212 \u00b5 (\u043c)). A lower regret rate indicates faster learning. In this section, we assume that the expert \u03c0 is optimal. We look at the finite horizon, episodic IL and RL algorithms."}, {"heading": "5.1. Exponential Gap", "text": "The transition is deterministic and the initial state s0 (root) is fixed. However, the cost of each non-leaf state is zero; the cost of each leaf is i.i.d sampled from a given distribution (possibly different distributions per leaf). In the following, we show that forM, IL can be exponentially more efficient than RL.Theorem 5.1. ForM, regretting RN of any finite horizon, episodic RL algorithm is at least: E [RN] \u2265 SN). (15) The expectation is with respect to random cost generation and internal randomness of the algorithm. However, regretting RN of any finite horizon, episodic RL algorithm is not mandatory."}, {"heading": "5.2. Polynomial Gap and Near-Optimality", "text": "Next, we quantify the gap in general discrete MDPs and also show that AggreVaTeD is close to optimal. We consider the more difficult case where we can only access an unbiased estimate of Q * t, for any t and state action pair. Policy \u03c0 is defined as a series of probability vectors \u03c0s, t \u00b2 (A) for all s \u00b2 S and t \u00b2 s \u00b2 S, t \u00b2 s \u00b2 S, t \u00b2 [H].Theorem 5.4. Accessing unbiased estimates of Q \u00b2 t, AggreVaTeD reaches the upper limit of regret with EG: RN \u2264 O (HQemax \u00b2 S ln (A) N). (18) Here, Qemax is the maximum cost-to-go gap of the expert. (3) The total regret shown in Equality 18 allows us to compare IL algorithms with RL algorithms. For example, the upper pro-confidence gap (UCB 2.0) to an optimal lupe (H)."}, {"heading": "6. Experiments", "text": "We evaluate our algorithms using robotics simulations from OpenAI Gym (Brockman et al., 2016) and Handwritten Algebra Dependency Parsing (Duyck & Gordon, 2015) and report on reward rather than cost, since OpenAI Gym uses reward and dependency analysis by default to maximize the UAS score. As our approach only promises that there is a policy for all learned policies that can work just as well as the expert, we report on the performance of the best policy so far: max {\u00b5 (\u03c01),..., \u00b5 (\u03c0i)}. For regular gradient descent, we use ADAM (Kingma & Ba, 2014), a first-order no-regret algorithm, and for the natural gradient, we use CG to calculate the downhill direction. For RL, we use REINFORCE (Williams, 1992) and Truncated Natural Policy Gradient (TPG) (Duan, 2016)."}, {"heading": "6.1. Robotics Simulations", "text": "We look at CartPole Balancing, Acrobot Swing-up, Hopper and Walker. To create an expert network, similar to previous work (Ho & Ermon, 2016), we used a Deep Q-Network (DQN) to generate a faster improvement for CartPole and Acrobot (e.g. to simulate the settings in which Q * is available), while using the publicly available TRPO implementations for generate3Here, we assume that Qemax is a constant compared to H. If Qemax = (H), then the expert is no better than a random policy whose cost-to-go is around (H). We need to simulate the settings for Hopper and Walker in which one needs to estimate Q from Monte-Carlo. Discrete Action Setting We use a single-layer (16 hidden units) neural networks with ReLu activation functions to represent the policy for the Ccropole and Acrochmarks."}, {"heading": "6.2. Dependency Parsing on Handwritten Algebra", "text": "We look at a sequential prediction problem: transitionbased dependency parsing for handwritten algebra with raw image data (Duyck & Gordon, 2015); the parsing task for algebra is similar to the classic dependency parsing for natural language (Chang et al., 2015a) where the problem is modeled in the IL setting and the state-of-the-art is achieved by AggreVaTe with FTRL (using data Aggregation); the additional challenge is that the inputs are handwritten algebra symbols in raw images; we learn directly to predict parse trees of low level image features (Histogram of Gradient features (HoG)); during the training, the expert is constructed with the ground-truth dependencies in training data; the complete state s during parsing consists of three data structures: stack, buffer and arcs, which store raw images of algebraic symbols."}, {"heading": "7. Conclusion", "text": "We introduced AggreVaTeD, a differentiated learning algorithm for the imitation of neural networks, which trains strategies for sequential prediction tasks such as continuous robot control and dependency analysis of raw image data. We showed that IL can learn in theory and practice much faster than RL with access to optimal cost-to-cost oracles. The learned strategies could reach expert and sometimes super expert levels in both fully observable and partially observable environments. Theoretical and experimental results indicate that IL is significantly more effective than RL for sequential predictions with near-optimal cost-to-cost oracles."}, {"heading": "Appendix: Proofs and Detailed Bounds", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "A. Derivation of Eq. 4", "text": "Based on Equation 1 with parametrized policy we have: \"n (\u03b8) = 1H \u0445 t = 1 E st \u0445 d \u03c0\u03b8n t [\u00b7 st; \u03b8) [Q \u0445 t (st, at)] = 1H H H \u0445 t = 1 E st d n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n in n n n n n n n in n n n n n in n in n n n n n n n n n in n in n n n n n in"}, {"heading": "B. Derivation of Exponential Gradient Update in Discrete MDP", "text": "We show the detailed lead of Eq.7 for AggreVaTeD with EG in discreet MDP. Remember that with KL-Divergenz as punishment the policy is updated in each episode as: {\u03c0sn + 1} s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s s \u00b2 s \u00b2 s \u00b2 s s s \u00b2 s s s \u00b2 s s \u00b2 s \u00b2 s \u00b2 s s \u00b2 s \u00b2 s \u00b2 s s \u00b2 s s \u00b2 s \u00b2 s s s s s \u00b2 s s s \u00b2 s"}, {"heading": "C. Lemmas", "text": "Before proving the theorems, we first present the difference in performance Lemma (Kakade & Langford, 2002; Ross & Bagnell, 2014), which will be used later: Lemma C.1. For any two strategies \u03c01 and \u03c02 we have: \u00b5 (\u03c01) \u2212 \u00b5 (\u03c02) = H \u0445 t = 1 Est \u0445 d\u03c01t [Eat \u0445 \u03c01 (\u00b7 | st) [Q \u03c02 t (st, at) \u2212 V \u03c02 t (st)]. (25) For detailed proof of the above mentioned Lemma, we refer readers to Ross & Bagnell, 2014. The second known result we will use is the analysis of the weighted majority algorithm. Let's define the linear loss function as' n (w) = w \u00b7 yn, for each yn-Rd, and w (d) from a probability calculation (d)."}, {"heading": "D. Proof of Theorem 5.1", "text": "A stochastic MAB is defined by S-arms referred to as I1,..., IS. Each arm costs to be queried at any time step t from a fixed but unknown distribution. A bandit algorithm selects an arm and then receives an unbiased sample of the selected arm cost cIt. For each bandit algorithm that selects the arms I1, I2,... IN N rounds, the expected regret is defined as: E [RN] = E [N = 1 cIn] \u2212 min i [S] N = 1 c), where the expectation is met regarding the randomness of the cost sampling process and possibly the randomness of the bandit algorithm. It has been shown that there are a number of distributions from which the cost of the weapons' sampled."}, {"heading": "E. Proof of Theorem 5.2", "text": "We assume that the most advanced policy has the lowest total cost (e.g., s3 in Figure 1 has the lowest average cost). We consider the deterministic political class, which contains all political consequences: S \u2192 {al, ar}. Since there are S-states and 2 measures, the total number of measures in the political class is 2S. To prove the top limit RN \u2264 O (Log (S)), we claim that for each e-K, at the end of episode e, AggreVaTe with FTL identifies the e'th state on the political class, i, e, the most advanced paths s0, s1, s3, s (2K \u2212 1). We can select the assertion by induktion.At episode e = 1, based on the original policy, AggreVaTe."}, {"heading": "F. Proof of Theorem 5.3", "text": "Since we assume in theory 5.3 that we only have access to the sober, but unbiased estimation of the Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q"}, {"heading": "G. Proof of Theorem 5.4", "text": "The proof for theorem 5.4 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s s \u00b2 s s \u00b2 s s \u00b2 s s \u00b2 s s \u00b2 s s \u00b2 s s \u00b2 s s \u00b2 s s \u00b2 s s \u00b2 s s \u00b2 s s \u00b2 s s \u00b2 s s \u00b2 s s \u00b2 s \u00b2 s \u00b2 s s \u00b2 s \u00b2 s s \u00b2 s \u00b2 s s \u00b2 s \u00b2 s s \u00b2 s \u00b2 s s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s s \u00b2 s \u00b2 s \u00b2 s s \u00b2 s \u00b2 s \u00b2 s \u00b2 s s \u00b2 s \u00b2 s \u00b2 s s \u00b2 s \u00b2 s \u00b2 s s s \u00b2 s \u00b2 s s \u00b2 s s \u00b2 s \u00b2 s s \u00b2 s \u00b2 s \u00b2 s s s \u00b2 s s s \u00b2 s s \u00b2 s \u00b2 s \u00b2 s s \u00b2 s \u00b2 s s \u00b2 s \u00b2 s s s \u00b2 s \u00b2 s s \u00b2 s \u00b2 s s s \u00b2 s s \u00b2 s \u00b2 s s s \u00b2 s s \u00b2 s s \u00b2 s s s s s \u00b2 s \u00b2 s \u00b2 s \u00b2 s s \u00b2 s s \u00b2 s s s \u00b2 s s \u00b2 s s s s s s \u00b2 s s \u00b2 s s s s s s s \u00b2 s \u00b2 s s s \u00b2 s s s \u00b2 s \u00b2 s s s \u00b2 s s s s s s s s s s s \u00b2 s s s s s s \u00b2 s s s s s s \u00b2 s s \u00b2 s s s s s s s s s \u00b2 s s s s s s s s s s s s s s s s s s s s s s s s s s s s"}, {"heading": "H. Proof of Theorem 5.5", "text": "s use Q (s) to make the sober but unbiased estimate of Q (s) Q (s) Q = Q (s).Proof. To simplify the description, we use E (n). We consider a finite MDP with time horizon H = 1. The initial distribution is 0 = {1 / S,..., 1 / S). We consider the algorithm setting in which at each episode n, a state sn (s) of H = 1. The algorithms use their current guidelines (A). (A) To choose an action, an action A (n) is chosen and then gets a sober but unbiased estimate Q (sn). (sn) The algorithms then update their policies."}, {"heading": "I. Details of Dependency Parsing for Handwritten Algebra", "text": "In Fig. 4, we show an example of a series of handwritten algebra equations and their dependency tree from an Arc-Hybird sequence slssssrrllslssrrssrssrssrssssrrssrssrssrssrssrssrr. The process cut individual symbols one by one from left to right and from the top equation downwards, centered them, scaled symbols to 40 by 40 images, and finally shaped them as an image sequence. Since in the most common dependency analysis setting there is no immediate reward at each parsing step, the reward togo Q \u0445 (s, a) is calculated by using UAS as follows: Start with s and apply action a, and then use expert dip to roll it out to the end of the parsing process; Q \u0445 (s, a) is the UAS score of the final configuration. Therefore, AggreVaTeD can be considered as a direct maximization of the UAS score, not representing the experts \"approach to the final configuration (such as SMM or DAM)."}], "references": [{"title": "Apprenticeship learning via inverse reinforcement learning", "author": ["Abbeel", "Pieter", "Ng", "Andrew Y"], "venue": "In ICML, pp", "citeRegEx": "Abbeel et al\\.,? \\Q2004\\E", "shortCiteRegEx": "Abbeel et al\\.", "year": 2004}, {"title": "An actor-critic algorithm for sequence prediction", "author": ["Bahdanau", "Dzmitry", "Brakel", "Philemon", "Xu", "Kelvin", "Goyal", "Anirudh", "Lowe", "Ryan", "Pineau", "Joelle", "Courville", "Aaron", "Bengio", "Yoshua"], "venue": "arXiv preprint arXiv:1607.07086,", "citeRegEx": "Bahdanau et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Bahdanau et al\\.", "year": 2016}, {"title": "Scheduled sampling for sequence prediction with recurrent neural networks", "author": ["Bengio", "Samy", "Vinyals", "Oriol", "Jaitly", "Navdeep", "Shazeer", "Noam"], "venue": "In NIPS,", "citeRegEx": "Bengio et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Bengio et al\\.", "year": 2015}, {"title": "Learning to search for dependencies", "author": ["Chang", "Kai-Wei", "He", "Daum\u00e9 III", "Hal", "Langford", "John"], "venue": "arXiv preprint arXiv:1503.05615,", "citeRegEx": "Chang et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Chang et al\\.", "year": 2015}, {"title": "Learning to search better than your teacher", "author": ["Chang", "Kai-wei", "Krishnamurthy", "Akshay", "Agarwal", "Alekh", "Daume", "Hal", "Langford", "John"], "venue": "In ICML,", "citeRegEx": "Chang et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Chang et al\\.", "year": 2015}, {"title": "Searchbased structured prediction", "author": ["Daum\u00e9 III", "Hal", "Langford", "John", "Marcu", "Daniel"], "venue": "Machine learning,", "citeRegEx": "III et al\\.,? \\Q2009\\E", "shortCiteRegEx": "III et al\\.", "year": 2009}, {"title": "Benchmarking deep reinforcement learning for continuous control", "author": ["Duan", "Yan", "Chen", "Xi", "Houthooft", "Rein", "Schulman", "John", "Abbeel", "Pieter"], "venue": "In ICML,", "citeRegEx": "Duan et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Duan et al\\.", "year": 2016}, {"title": "Predicting structure in handwritten algebra data from low level features", "author": ["Duyck", "James A", "Gordon", "Geoffrey J"], "venue": "Data Analysis Project Report, MLD,", "citeRegEx": "Duyck et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Duyck et al\\.", "year": 2015}, {"title": "Guided cost learning: Deep inverse optimal control via policy optimization", "author": ["Finn", "Chelsea", "Levine", "Sergey", "Abbeel", "Pieter"], "venue": "In ICML,", "citeRegEx": "Finn et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Finn et al\\.", "year": 2016}, {"title": "Variance reduction techniques for gradient estimates in reinforcement learning", "author": ["Greensmith", "Evan", "Bartlett", "Peter L", "Baxter", "Jonathan"], "venue": null, "citeRegEx": "Greensmith et al\\.,? \\Q2004\\E", "shortCiteRegEx": "Greensmith et al\\.", "year": 2004}, {"title": "Generative adversarial imitation learning", "author": ["Ho", "Jonathan", "Ermon", "Stefano"], "venue": "In NIPS,", "citeRegEx": "Ho et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Ho et al\\.", "year": 2016}, {"title": "Near-optimal regret bounds for reinforcement learning", "author": ["Jaksch", "Thomas", "Ortner", "Ronald", "Auer", "Peter"], "venue": null, "citeRegEx": "Jaksch et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Jaksch et al\\.", "year": 2010}, {"title": "Plato: Policy learning using adaptive trajectory optimization", "author": ["Kahn", "Gregory", "Zhang", "Tianhao", "Levine", "Sergey", "Abbeel", "Pieter"], "venue": "arXiv preprint arXiv:1603.00622,", "citeRegEx": "Kahn et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Kahn et al\\.", "year": 2016}, {"title": "A natural policy gradient", "author": ["Kakade", "Sham"], "venue": null, "citeRegEx": "Kakade and Sham.,? \\Q2002\\E", "shortCiteRegEx": "Kakade and Sham.", "year": 2002}, {"title": "Approximately optimal approximate reinforcement learning", "author": ["Kakade", "Sham", "Langford", "John"], "venue": "In ICML,", "citeRegEx": "Kakade et al\\.,? \\Q2002\\E", "shortCiteRegEx": "Kakade et al\\.", "year": 2002}, {"title": "Adam: A method for stochastic optimization", "author": ["Kingma", "Diederik", "Ba", "Jimmy"], "venue": "arXiv preprint arXiv:1412.6980,", "citeRegEx": "Kingma et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Kingma et al\\.", "year": 2014}, {"title": "Deep reinforcement learning for dialogue generation", "author": ["Li", "Jiwei", "Monroe", "Will", "Ritter", "Alan", "Galley", "Michel", "Gao", "Jianfeng", "Jurafsky", "Dan"], "venue": "arXiv preprint arXiv:1606.01541,", "citeRegEx": "Li et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Li et al\\.", "year": 2016}, {"title": "Human-level control through deep reinforcement learning", "author": ["Mnih", "Volodymyr"], "venue": "Nature,", "citeRegEx": "Mnih and Volodymyr,? \\Q2015\\E", "shortCiteRegEx": "Mnih and Volodymyr", "year": 2015}, {"title": "Exploiting the connection between pls, lanczos methods and conjugate gradients: alternative proofs of some properties of pls", "author": ["Phatak", "Aloke", "de Hoog", "Frank"], "venue": "Journal of Chemometrics,", "citeRegEx": "Phatak et al\\.,? \\Q2002\\E", "shortCiteRegEx": "Phatak et al\\.", "year": 2002}, {"title": "Sequence level training with recurrent neural networks", "author": ["Ranzato", "Marc\u2019Aurelio", "Chopra", "Sumit", "Auli", "Michael", "Zaremba", "Wojciech"], "venue": "ICLR 2016,", "citeRegEx": "Ranzato et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Ranzato et al\\.", "year": 2015}, {"title": "Maximum margin planning", "author": ["Ratliff", "Nathan D", "Bagnell", "J Andrew", "Zinkevich", "Martin A"], "venue": "In ICML,", "citeRegEx": "Ratliff et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Ratliff et al\\.", "year": 2006}, {"title": "Visual chunking: A list prediction framework for region-based object detection", "author": ["Rhinehart", "Nicholas", "Zhou", "Jiaji", "Hebert", "Martial", "Bagnell", "J Andrew"], "venue": "In ICRA. IEEE,", "citeRegEx": "Rhinehart et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Rhinehart et al\\.", "year": 2015}, {"title": "Efficient reductions for imitation learning", "author": ["Ross", "St\u00e9phane", "Bagnell", "J. Andrew"], "venue": "In AISTATS, pp", "citeRegEx": "Ross et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Ross et al\\.", "year": 2010}, {"title": "Reinforcement and imitation learning via interactive no-regret learning", "author": ["Ross", "Stephane", "Bagnell", "J Andrew"], "venue": "arXiv preprint arXiv:1406.5979,", "citeRegEx": "Ross et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Ross et al\\.", "year": 2014}, {"title": "A reduction of imitation learning and structured prediction to noregret online learning", "author": ["Ross", "St\u00e9phane", "Gordon", "Geoffrey J", "Bagnell", "J.Andrew"], "venue": "In AISTATS,", "citeRegEx": "Ross et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Ross et al\\.", "year": 2011}, {"title": "Learning policies for contextual submodular prediction", "author": ["Ross", "Stephane", "Zhou", "Jiaji", "Yue", "Yisong", "Dey", "Debadeepta", "Bagnell", "Drew"], "venue": "In ICML,", "citeRegEx": "Ross et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Ross et al\\.", "year": 2013}, {"title": "Trust region policy optimization", "author": ["Schulman", "John", "Levine", "Sergey", "Abbeel", "Pieter", "Jordan", "Michael I", "Moritz", "Philipp"], "venue": "In ICML, pp", "citeRegEx": "Schulman et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Schulman et al\\.", "year": 2015}, {"title": "Mastering the game of go with deep neural networks and tree search", "author": ["Silver", "David"], "venue": null, "citeRegEx": "Silver and David,? \\Q2016\\E", "shortCiteRegEx": "Silver and David", "year": 2016}, {"title": "Sequence to sequence learning with neural networks", "author": ["Sutskever", "Ilya", "Vinyals", "Oriol", "Le", "Quoc V"], "venue": "In NIPS,", "citeRegEx": "Sutskever et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Sutskever et al\\.", "year": 2014}, {"title": "Apprenticeship learning using linear programming", "author": ["Syed", "Umar", "Bowling", "Michael", "Schapire", "Robert E"], "venue": "In ICML,", "citeRegEx": "Syed et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Syed et al\\.", "year": 2008}, {"title": "Improving multi-step prediction of learned time series models", "author": ["Venkatraman", "Arun", "Hebert", "Martial", "Bagnell", "J Andrew"], "venue": null, "citeRegEx": "Venkatraman et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Venkatraman et al\\.", "year": 2015}, {"title": "Simple statistical gradient-following algorithms for connectionist reinforcement learning", "author": ["Williams", "Ronald J"], "venue": "Machine learning,", "citeRegEx": "Williams and J.,? \\Q1992\\E", "shortCiteRegEx": "Williams and J.", "year": 1992}, {"title": "Maximum entropy inverse reinforcement learning", "author": ["Ziebart", "Brian D", "Maas", "Andrew L", "Bagnell", "J Andrew", "Dey", "Anind K"], "venue": "In AAAI,", "citeRegEx": "Ziebart et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Ziebart et al\\.", "year": 2008}, {"title": "Online Convex Programming and Generalized Infinitesimal Gradient Ascent", "author": ["Zinkevich", "Martin"], "venue": "In ICML,", "citeRegEx": "Zinkevich and Martin.,? \\Q2003\\E", "shortCiteRegEx": "Zinkevich and Martin.", "year": 2003}, {"title": "Proof of Theorem 5.1 Proof. We construct a reduction from stochastic Multi-Arm Bandits (MAB) to the MDP M\u0303. A stochastic MAB is defined by S arms denoted as I, ..., I . Each arm I\u2019s cost ci at any time step t is sampled from a fixed but unknown distribution", "author": ["D. proof"], "venue": null, "citeRegEx": "proof.,? \\Q2012\\E", "shortCiteRegEx": "proof.", "year": 2012}], "referenceMentions": [{"referenceID": 26, "context": "Reinforcement Learning (RL), especially deep RL, has dramatically advanced the state of the art in sequential decision making in high-dimensional robotics control tasks as well as in playing video and board games (Schulman et al., 2015; Silver et al., 2016).", "startOffset": 213, "endOffset": 257}, {"referenceID": 19, "context": "Though conventional supervised learning of deep models has been pivotal in advancing performance in sequential prediction problems, researchers are beginning to utilize deep RL methods to achieve high performance (Ranzato et al., 2015; Bahdanau et al., 2016; Li et al., 2016).", "startOffset": 213, "endOffset": 275}, {"referenceID": 1, "context": "Though conventional supervised learning of deep models has been pivotal in advancing performance in sequential prediction problems, researchers are beginning to utilize deep RL methods to achieve high performance (Ranzato et al., 2015; Bahdanau et al., 2016; Li et al., 2016).", "startOffset": 213, "endOffset": 275}, {"referenceID": 16, "context": "Though conventional supervised learning of deep models has been pivotal in advancing performance in sequential prediction problems, researchers are beginning to utilize deep RL methods to achieve high performance (Ranzato et al., 2015; Bahdanau et al., 2016; Li et al., 2016).", "startOffset": 213, "endOffset": 275}, {"referenceID": 30, "context": ", 2009), DaD (Venkatraman et al., 2015), AggreVaTe (Ross & Bagnell, 2014), and LOLS (Chang et al.", "startOffset": 13, "endOffset": 39}, {"referenceID": 24, "context": "For robotics control problems, this oracle may come from a human expert guiding the robot during the training phase (Abbeel & Ng, 2004) or from an optimal MDP solver (Ross et al., 2011; Kahn et al., 2016) that either may be too slow to use at test time or leverages information unavailable at test time (e.", "startOffset": 166, "endOffset": 204}, {"referenceID": 12, "context": "For robotics control problems, this oracle may come from a human expert guiding the robot during the training phase (Abbeel & Ng, 2004) or from an optimal MDP solver (Ross et al., 2011; Kahn et al., 2016) that either may be too slow to use at test time or leverages information unavailable at test time (e.", "startOffset": 166, "endOffset": 204}, {"referenceID": 25, "context": ", beam search) or by a clairvoyant greedy algorithm (Daum\u00e9 III et al., 2009; Ross et al., 2013; Rhinehart et al., 2015; Chang et al., 2015a) that is near-optimal on the task specific performance metric (e.", "startOffset": 52, "endOffset": 140}, {"referenceID": 21, "context": ", beam search) or by a clairvoyant greedy algorithm (Daum\u00e9 III et al., 2009; Ross et al., 2013; Rhinehart et al., 2015; Chang et al., 2015a) that is near-optimal on the task specific performance metric (e.", "startOffset": 52, "endOffset": 140}, {"referenceID": 29, "context": "These methods (Abbeel & Ng, 2004; Syed et al., 2008; Ratliff et al., 2006; Ziebart et al., 2008; Finn et al., 2016; Ho & Ermon, 2016) learn either a policy \u03c0\u0302\u2217 or Q\u0302\u2217 from a fixed-size dataset pre-collected from the oracle.", "startOffset": 14, "endOffset": 133}, {"referenceID": 20, "context": "These methods (Abbeel & Ng, 2004; Syed et al., 2008; Ratliff et al., 2006; Ziebart et al., 2008; Finn et al., 2016; Ho & Ermon, 2016) learn either a policy \u03c0\u0302\u2217 or Q\u0302\u2217 from a fixed-size dataset pre-collected from the oracle.", "startOffset": 14, "endOffset": 133}, {"referenceID": 32, "context": "These methods (Abbeel & Ng, 2004; Syed et al., 2008; Ratliff et al., 2006; Ziebart et al., 2008; Finn et al., 2016; Ho & Ermon, 2016) learn either a policy \u03c0\u0302\u2217 or Q\u0302\u2217 from a fixed-size dataset pre-collected from the oracle.", "startOffset": 14, "endOffset": 133}, {"referenceID": 8, "context": "These methods (Abbeel & Ng, 2004; Syed et al., 2008; Ratliff et al., 2006; Ziebart et al., 2008; Finn et al., 2016; Ho & Ermon, 2016) learn either a policy \u03c0\u0302\u2217 or Q\u0302\u2217 from a fixed-size dataset pre-collected from the oracle.", "startOffset": 14, "endOffset": 133}, {"referenceID": 24, "context": ", 2009), DAgger (Ross et al., 2011), and AggreVaTe (Ross & Bagnell, 2014) interleave learning and testing procedures to overcome the data mismatch issue and, as a result, work well in practical applications.", "startOffset": 16, "endOffset": 35}, {"referenceID": 6, "context": "We use an LSTM-based policy (Duan et al., 2016) where the LSTM\u2019s hidden states provide a compressed feature of the history.", "startOffset": 28, "endOffset": 47}, {"referenceID": 26, "context": "In order to learn rich policies such as with LSTMs or deep networks (Schulman et al., 2015), we derive a policy gradient method for imitation learning and sequential prediction.", "startOffset": 68, "endOffset": 91}, {"referenceID": 26, "context": "In order to learn rich policies such as with LSTMs or deep networks (Schulman et al., 2015), we derive a policy gradient method for imitation learning and sequential prediction. To do this, we leverage the reduction of IL and sequential prediction to online learning as shown in (Ross & Bagnell, 2014) to learn policies represented by expressive differentiable function approximators. The fundamental idea in Ross & Bagnell (2014) is to use a no-regret online learner to update policies using the following loss function at each episode n:", "startOffset": 69, "endOffset": 431}, {"referenceID": 26, "context": "Policy Updates with Natural Gradient Descent We derive a natural gradient update procedure for imitation learning inspired by the success of natural gradient descent in RL (Kakade, 2002; Bagnell & Schneider, 2003; Schulman et al., 2015).", "startOffset": 172, "endOffset": 236}, {"referenceID": 26, "context": "Second, we replace KL(\u03c0\u03b8||\u03c0\u03b8n) by KL(\u03c0\u03b8n ||\u03c0\u03b8), which is a local approximation since KL(q||p) and KL(p||q) are equal up to the second order (Kakade & Langford, 2002; Schulman et al., 2015).", "startOffset": 140, "endOffset": 188}, {"referenceID": 9, "context": "11 by the state-action advantage function At (s i,n t , a) = Q \u2217 t (s i,n t , a) \u2212 V \u2217 t (s i,n t ), which leads to the following two unbiased and variancereduced gradient estimations (Greensmith et al., 2004):", "startOffset": 184, "endOffset": 209}, {"referenceID": 26, "context": "This approach is used in TRPO (Schulman et al., 2015).", "startOffset": 30, "endOffset": 53}, {"referenceID": 24, "context": "This mixing strategy with decay rate was first introduced in (Ross et al., 2011) for IL, and later on was used in sequence prediction (Bengio et al.", "startOffset": 61, "endOffset": 80}, {"referenceID": 2, "context": ", 2011) for IL, and later on was used in sequence prediction (Bengio et al., 2015).", "startOffset": 61, "endOffset": 82}, {"referenceID": 11, "context": "For example, the Upper Confidence Bound (UCB) based, near-optimal optimistic RL algorithms from (Jaksch et al., 2010), specifically designed for efficient exploration, admit regret \u00d5(HS \u221a HAN), leading to a gap of approximately \u221a HAS compared to the regret bound of imitation learning shown in Eq.", "startOffset": 96, "endOffset": 117}, {"referenceID": 6, "context": "For RL we use REINFORCE (Williams, 1992) and Truncated Natural Policy Gradient (TNPG) (Duan et al., 2016).", "startOffset": 86, "endOffset": 105}, {"referenceID": 26, "context": "Following the neural network settings described in Schulman et al. (2015), the expert policy \u03c0\u2217 is obtained from TRPO with one hidden layer (64 hidden states), which is the same structure that we use to represent our policies \u03c0\u03b8.", "startOffset": 51, "endOffset": 74}, {"referenceID": 28, "context": "The RNN policy follows the design from (Sutskever et al., 2014).", "startOffset": 39, "endOffset": 63}], "year": 2017, "abstractText": "Researchers have demonstrated state-of-the-art performance in sequential decision making problems (e.g., robotics control, sequential prediction) with deep neural network models. One often has access to near-optimal oracles that achieve good performance on the task during training. We demonstrate that AggreVaTeD \u2014 a policy gradient extension of the Imitation Learning (IL) approach of (Ross & Bagnell, 2014) \u2014 can leverage such an oracle to achieve faster and better solutions with less training data than a less-informed Reinforcement Learning (RL) technique. Using both feedforward and recurrent neural predictors, we present stochastic gradient procedures on a sequential prediction task, dependency-parsing from raw image data, as well as on various high dimensional robotics control problems. We also provide a comprehensive theoretical study of IL that demonstrates we can expect up to exponentially lower sample complexity for learning with AggreVaTeD than with RL algorithms, which backs our empirical findings. Our results and theory indicate that the proposed approach can achieve superior performance with respect to the oracle when the demonstrator is sub-optimal.", "creator": "LaTeX with hyperref package"}}}