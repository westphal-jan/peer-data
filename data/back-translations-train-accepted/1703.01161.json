{"id": "1703.01161", "review": {"conference": "ICML", "VERSION": "v1", "DATE_OF_SUBMISSION": "3-Mar-2017", "title": "FeUdal Networks for Hierarchical Reinforcement Learning", "abstract": "We introduce FeUdal Networks (FuNs): a novel architecture for hierarchical reinforcement learning. Our approach is inspired by the feudal reinforcement learning proposal of Dayan and Hinton, and gains power and efficacy by decoupling end-to-end learning across multiple levels -- allowing it to utilise different resolutions of time. Our framework employs a Manager module and a Worker module. The Manager operates at a lower temporal resolution and sets abstract goals which are conveyed to and enacted by the Worker. The Worker generates primitive actions at every tick of the environment. The decoupled structure of FuN conveys several benefits -- in addition to facilitating very long timescale credit assignment it also encourages the emergence of sub-policies associated with different goals set by the Manager. These properties allow FuN to dramatically outperform a strong baseline agent on tasks that involve long-term credit assignment or memorisation. We demonstrate the performance of our proposed system on a range of tasks from the ATARI suite and also from a 3D DeepMind Lab environment.", "histories": [["v1", "Fri, 3 Mar 2017 14:05:11 GMT  (1754kb,D)", "https://arxiv.org/abs/1703.01161v1", null], ["v2", "Mon, 6 Mar 2017 18:17:18 GMT  (1748kb,D)", "http://arxiv.org/abs/1703.01161v2", null]], "reviews": [], "SUBJECTS": "cs.AI", "authors": ["alexander sasha vezhnevets", "simon osindero", "tom schaul", "nicolas heess", "max jaderberg", "david silver", "koray kavukcuoglu"], "accepted": true, "id": "1703.01161"}, "pdf": {"name": "1703.01161.pdf", "metadata": {"source": "META", "title": "FeUdal Networks for Hierarchical Reinforcement Learning", "authors": ["Alexander Sasha Vezhnevets", "Simon Osindero", "Tom Schaul", "Nicolas Heess", "Koray Kavukcuoglu"], "emails": ["VEZHNICK@GOOGLE.COM", "OSINDERO@GOOGLE.COM", "SCHAUL@GOOGLE.COM", "HEESS@GOOGLE.COM", "JADERBERG@GOOGLE.COM", "DAVIDSILVER@GOOGLE.COM", "KORAYK@GOOGLE.COM"], "sections": [{"heading": "1. Introduction", "text": "In recent years it has been shown that it can come to exaggerations not only in the USA, but also in many other countries of the world. (...) Also in other countries of the world it is so that people are able to survive themselves. (...) In other countries of the world it is so that they are able to survive themselves. (...) In other countries of the world it is so that they are able to survive themselves. (...) In other countries of the world it is so that they are able to survive themselves. (...) In other countries of the world it is as if they are able to survive themselves. (...). (...) In other countries of the world it is as if they are able to survive themselves. (...) In other countries of the world it is as if they are able to survive themselves. (...). (...). (...). (...). (...) The world. (...). (...) The world. (...). (... \"The world. (...). (...\" The world. (...). (... \"The world. (...). (...\" The world. (...). (... \"The world. (...). (...\" The world. (). (... \"The world. (...\" The world. (...). (... The world. (...). \"The. (). (... The world. (). (). (... The. (... The. (... The.). (. (... The. (.). (... The.). (.) The. (. (... The. (.) The. (.) The. (.) The. (. (. (. (.) The. (. (.) The. (. (.) The.) The. (. (. (. (.) The.) The. (. (.) The. (.) The. (. (.) The. (. (.) The. (. (.) The. (. (. (. (.) The. (.) The. (. (.) The. (.) The. (. (.) The. (. (. (. (.) The. (.) The. (. (. (. (.) The. (. ("}, {"heading": "2. Related Work", "text": "In fact, it is so that most people are able to determine for themselves what they want and what they do not want. (...) In fact, it is so that most people are able to decide what they want and what they do not want. (...) In fact, it is so that most people are able to decide. (...) It is so that most people are able to decide. (...) It is so that most people are able to decide what they want. (...) It is so, as if they do not want it. (...) It is so. (...) It is so. (...) It is so. (...) It is so. (...) It is so. (...) It is so. (...) It is. (...) It is. (...) It is. (...) It is. (...) It is. It is. (...) It is. It is. (...) It is. It is. It is. (...) It is. It is. It is. (...) It is. It is. It is. It is. (...) It is. It is. It is. It is. (...) It is. It is. It is. It is. It is. It is. (...) It is. It is. It is. It is. It is. It is. It is. (...). It is. It is. It is. It is. It is. It is. It is. (...). It is. It is. It is. It is. It is. It is. It is. It is. (...). It is. It is. It is. It is. It is. It is. It is. It is. It is. (...). It is. It is. It is. (). It is. It is. It is. It is. It is. It is. It is. (). It is. It is. It is. It is."}, {"heading": "3. The model", "text": "What is FuN? FuN is a modular neural network that consists of two modules - the worker and the manager. Internally, the manager computes a latent state representation st and outputs a target vector gt. The worker produces actions based on external observation, his own state and the manager's goal. The manager and the worker share a perception module that takes an observation from the environment xt and calculates a common mean representation zt. The manager's goals are trained based on an approximate transition policy gradient. This is a particularly efficient form of policy gradient training that utilizes the knowledge that the worker's behavior ultimately coincides with the goals he has set. The worker is then trained via an intrinsic reward to produce actions that lead to achieving these goals. Figure 1a illustrates the overall design and the following equations describe the forward dynamics of our network: zt = f (xt) (1) = Mtwzt (f) (2) (f)"}, {"heading": "3.1. Goal embedding", "text": "The target g modulates the policy through a multiplicative interaction in a low-dimensional target embedding space Rk, k < < < d. To include the goals of the manager, the last c goals are first summarized and then embedded by a linear projection \u03c6 (eq. 4) in a vector w-Rk. The projection \u03c6 is linear, without distortion, and is learned with gradients from the worker's actions. The embedding matrix U is then combined with the target w via a matrix vector product (eq. 6). As it has no distortion, it can never produce a constant, non-zero vector - which is the only way the setup could ignore the manager's input, ensuring that the target output by the manager always influences the final goals over time."}, {"heading": "3.2. Learning", "text": "We consider a standard reinforcement of the actions taken by the managers. At each step, the agent receives an observation xt from the environment and selects an action from a finite series of possible actions. The objective of the agent is to maximize the discounted return as defined in eq. Conventional wisdom would be to train the entire architecture monolithically through gradients directly or via TD learning. Notice that since FuN is fully differentiable is that it operates at the end of a political gradient."}, {"heading": "3.3. Transition Policy Gradients", "text": "We motivate our proposed updating rule for the manager as a novel form of policy differentiation in relation to a model of worker behavior. Let's consider a policy at the highest level ot = \u00b5 (st, \u03b8) that selects between the sub-policies (possibly from a continuous sentence), where we currently assume that these sub-policies are fixed duration behaviors (lasting for c steps). Corresponding to each sub-policy is a transitional distribution, p (st + c | st, ot) that describes the distribution of states that we end at the end of the sub-policy, given the start state and the sub-policy that has been adopted. Top-level policy can be composed with the transitional distribution to give a \"transitional policy\" TP (st + c | st) that describes the distribution (st + c | st) that we end up at the end of the sub-policy, given the start state and the sub-policy that is being implemented."}, {"heading": "4. Architecture details", "text": "The CNN has a first layer of 16 8x8 filters of step 4, followed by a layer of 32 4x4 filters of step 2. The fully connected layer consists of 256 hidden units. Each curved and fully connected layer is followed by a non-linear layer of the rectifier. The state space, which the manager implicitly models in formulating his goals, is calculated using fMspace, which is another fully connected layer, followed by a non-linearity of the rectifier. The dimensionality of the embedding vectors, w, is on k = 16. In order to promote exploration in the transition policy, we emit a random target at each step with low probability, which we derive from a uni-variable Gaussian. The recurring network of the worker fWrnn is a novel LSTM (pre-policy)."}, {"heading": "4.1. Dilated LSTM", "text": "We propose a novel RNN architecture for the manager that operates at a lower time resolution than the data stream. We define an extended LSTM analogous to extended Convolutionary Networks (Yu & Koltun, 2016). For an extension radius, r can be the complete state of the network h = {h, i} ri = 1, i.e. it consists of separate groups of sub-states or \"cores.\" Currently, the network is governed by the following equations: h, t% rt, gt = LSTM (st, h, h, t% r t \u2212 1; \u03b8LSTM), where% denotes module operation and allows us to specify which group of cores are currently being updated. We make the parameters of the LSTM network LSTM explicit to emphasize that the same parameters govern the update for each of the r groups within the dLSTM."}, {"heading": "5. Experiments", "text": "The aim of our experiments is to show that FuN learns non-trivial, helpful and interpretable sub-policies and sub-goals and also validates components of the architecture. We start by describing technical details of the experimental setup and then present results on Montezuma's Revenge - an infamously hard ATARI game - in Section 5.1. Section 5.2 presents results on further ATARI games and extensively compares FuN to LSTM baselines with different discount factors and BPTT lengths. In Section 5.3 we present results on a series of visual memorization tasks in a 3D environment. Section 5.4 presents an unloading study by FuN that compares our Design Choices.Baseline. Our main foundation is a recursive LSTM network on top of a representation we learned from a CNN. The LSTM (Hochreiter & Schmidhuber, 1997) architecture is a widely used recurrent network and has been demonstrated to work well on a number of stronger ones."}, {"heading": "5.1. Montezuma\u2019s revenge", "text": "Montezuma's Revenge is one of the most difficult games available through the ALE (Bellemare et al., 2012). The game is notorious for challenging agents with deadly traps and scanty rewards. We had to expand and intensify our search for hyperparameters for the LSTM baseline in order to see any progress for this model. We have experimented with many different hyperparameter configurations for the LSTM baseline, for example by extending the search for learning rates to LogUniform (10 \u2212 3, 10 \u2212 2), and we report on the configuration that worked best. We use a small discount of 0.99 for LSTM; for FuN we use 0.99 in Worker and 0.999 in Manager. Figure 2b analyzes the sub-targets that FuN learned in the first room. They prove to be significant milestones that bridge the agents to their first extrinsic reward."}, {"heading": "5.2. ATARI", "text": "This year, it is only a matter of time before an agreement is reached."}, {"heading": "5.3. Memory in Labyrinth", "text": "DeepMind Lab (Beattie et al., 2016) is a first-person 3D gaming platform expanded by OpenArena. It is a visually complex 3D environment with actions that correspond to movement and orientation. We use 4 different levels that test long-term lending and visual memory: The Water Labyrinth is a reproduction of the Morris Labyrinth Experiment (Morris, 1981) from the behavioral literature; an agent is dropped into a circular pool with a hidden platform at an unknown random location; the agent can move on the platform and gets a reward and the study is restarted; the platform stays in the same place for the rest of the episode, while the agent begins each study at a random location; the walls of the pool are decorated with visual keywords to localisation.T Labyrinth is another classic animal cognition test."}, {"heading": "5.4. Ablative analysis", "text": "This means that the most important political gradients for the training of the manager are regarded rather as absolute objectives; lower temporal resolution for the manager; intrinsic motivation for the workers who have decided for the training activities of the manager, however, it is not to be expected that the training activities of the manager are extended as far as possible. (The training activities of the manager are not sufficiently pronounced). (The training activities of the training activities of the training activities of the training activities of 50 000 000 000 000 Euro. (The training activities of the training activities of the training activities of the training activities of 50 000 000 Euro.) The training activities of the training activities of the training activities are the training activities of the training activities of the training activities.) The training activities of the training activities of the training activities of the training activities are (the training activities of the training activities of the training activities of the training activities of the training activities of 50 000 000 Euro.)"}, {"heading": "5.5. ATARI action repeat transfer", "text": "One of the advantages of FuN is the clear separation of the duties between manager and worker. The manager learns a transition policy, while the worker learns to take primitive measures to accomplish these transitions. This transition policy is invariable to the underlying embodiment of the agent - the way his primitive actions translate into transitions into state space. Potentially, the transition policy can be transferred between agents with different embodiments - e.g. robot models with physical design or different operational frequency. We provide evidence of this possibility by putting strategies between agents with different repetitions of actions on ATARI.3To perform the transfer, we initialize the FuN system with parameters taken from an agent with a repetition of action of 4, and then make the following adjustments: (i) we adjust the discounts accordingly for all rewards; (ii) we increase the dilation of the dLSTM by a factor of 4; (iii) we increase the horizon of a factor of 4."}, {"heading": "6. Discussion and future work", "text": "Solving this question could be an important step toward agents with general intelligence and competence. In this paper, FeUdal Networks was introduced, a novel architecture that formulates partial goals as instructions in the latent state that, when followed, lead to meaningful behavioral primitives. FuN clearly separates the module that detects and sets partial goals from the module that generates behavior through primitive actions, creating a natural hierarchy that is stable and allows both modules to learn in a complementary way. Our experiments clearly show that this makes long-term lending and memorization more tractable, and opens up many opportunities for further research, for example: Deeper hierarchies can be constructed by setting goals on multiple time scales by scaling agents to truly large environments with sparse rewards and partial observability."}, {"heading": "7. Acknowledgements", "text": "We thank Alex Graves, Daan Wierstra, Olivier Pietquin, Oriol Vinyals, Joseph Modayil and Vlad Mnih for many helpful discussions, suggestions and comments on the article."}], "references": [{"title": "The option-critic architecture", "author": ["Bacon", "Pierre-Luc", "Precup", "Doina", "Harb", "Jean"], "venue": "In AAAI,", "citeRegEx": "Bacon et al\\.,? \\Q2017\\E", "shortCiteRegEx": "Bacon et al\\.", "year": 2017}, {"title": "Unifying count-based exploration and intrinsic motivation", "author": ["Bellemare", "Marc", "Srinivasan", "Sriram", "Ostrovski", "Georg", "Schaul", "Tom", "Saxton", "David", "Munos", "Remi"], "venue": "In NIPS,", "citeRegEx": "Bellemare et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Bellemare et al\\.", "year": 2016}, {"title": "Increasing the action gap: New operators for reinforcement learning", "author": ["Bellemare", "Marc G", "Ostrovski", "Georg", "Guez", "Arthur", "Thomas", "Philip S", "Munos", "R\u00e9mi"], "venue": "In Proceedings of the AAAI Conference on Artificial Intelligence,", "citeRegEx": "Bellemare et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Bellemare et al\\.", "year": 2016}, {"title": "Prioritized goal decomposition of markov decision processes: Toward a synthesis of classical and decision theoretic planning", "author": ["Boutilier", "Craig", "Brafman", "Ronen I", "Geib", "Christopher"], "venue": "In IJCAI,", "citeRegEx": "Boutilier et al\\.,? \\Q1997\\E", "shortCiteRegEx": "Boutilier et al\\.", "year": 1997}, {"title": "Improving generalization for temporal difference learning: The successor representation", "author": ["Dayan", "Peter"], "venue": "Neural Computation,", "citeRegEx": "Dayan and Peter.,? \\Q1993\\E", "shortCiteRegEx": "Dayan and Peter.", "year": 1993}, {"title": "Feudal reinforcement learning", "author": ["Dayan", "Peter", "Hinton", "Geoffrey E"], "venue": "In NIPS. Morgan Kaufmann Publishers,", "citeRegEx": "Dayan et al\\.,? \\Q1993\\E", "shortCiteRegEx": "Dayan et al\\.", "year": 1993}, {"title": "Hierarchical reinforcement learning with the maxq value function decomposition", "author": ["Dietterich", "Thomas G"], "venue": "J. Artif. Intell. Res.(JAIR),", "citeRegEx": "Dietterich and G.,? \\Q2000\\E", "shortCiteRegEx": "Dietterich and G.", "year": 2000}, {"title": "Long shortterm memory", "author": ["Hochreiter", "Sepp", "Schmidhuber", "J\u00fcrgen"], "venue": "Neural computation,", "citeRegEx": "Hochreiter et al\\.,? \\Q1997\\E", "shortCiteRegEx": "Hochreiter et al\\.", "year": 1997}, {"title": "Reinforcement learning with unsupervised auxiliary tasks", "author": ["Jaderberg", "Max", "Mnih", "Volodymyr", "Czarnecki", "Wojciech Marian", "Schaul", "Tom", "Leibo", "Joel Z", "Silver", "David", "Kavukcuoglu", "Koray"], "venue": "arXiv preprint arXiv:1611.05397,", "citeRegEx": "Jaderberg et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Jaderberg et al\\.", "year": 2016}, {"title": "Hierarchical learning in stochastic domains: Preliminary results", "author": ["Kaelbling", "Leslie Pack"], "venue": "In ICML,", "citeRegEx": "Kaelbling and Pack.,? \\Q2014\\E", "shortCiteRegEx": "Kaelbling and Pack.", "year": 2014}, {"title": "A clockwork rnn", "author": ["Koutn\u0131\u0301k", "Jan", "Greff", "Klaus", "Gomez", "Faustino", "Schmidhuber", "J\u00fcrgen"], "venue": "In ICML,", "citeRegEx": "Koutn\u0131\u0301k et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Koutn\u0131\u0301k et al\\.", "year": 2014}, {"title": "Hierarchical deep reinforcement learning: Integrating temporal abstraction and intrinsic motivation", "author": ["Kulkarni", "Tejas D", "Narasimhan", "Karthik R", "Saeedi", "Ardavan", "Tenenbaum", "Joshua B"], "venue": "arXiv preprint arXiv:1604.06057,", "citeRegEx": "Kulkarni et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Kulkarni et al\\.", "year": 2016}, {"title": "Building machines that learn and think like people", "author": ["Lake", "Brenden M", "Ullman", "Tomer D", "Tenenbaum", "Joshua B", "Gershman", "Samuel J"], "venue": "arXiv preprint arXiv:1604.00289,", "citeRegEx": "Lake et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Lake et al\\.", "year": 2016}, {"title": "End-to-end training of deep visuomotor policies", "author": ["Levine", "Sergey", "Finn", "Chelsea", "Darrell", "Trevor", "Abbeel", "Pieter"], "venue": "arXiv preprint arXiv:1504.00702,", "citeRegEx": "Levine et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Levine et al\\.", "year": 2015}, {"title": "Continuous control with deep reinforcement learning", "author": ["Lillicrap", "Timothy P", "Hunt", "Jonathan J", "Pritzel", "Alexander", "Heess", "Nicolas", "Erez", "Tom", "Tassa", "Yuval", "Silver", "David", "Wierstra", "Daan"], "venue": "arXiv preprint arXiv:1509.02971,", "citeRegEx": "Lillicrap et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Lillicrap et al\\.", "year": 2015}, {"title": "Human-level control through deep reinforcement learning", "author": ["Dharshan", "Wierstra", "Daan", "Legg", "Shane", "Hassabis", "Demis"], "venue": "Nature, 518(7540):529\u2013533,", "citeRegEx": "Dharshan et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Dharshan et al\\.", "year": 2015}, {"title": "Asynchronous methods for deep reinforcement learning", "author": ["Mnih", "Volodymyr", "Badia", "Adria Puigdomenech", "Mirza", "Mehdi", "Graves", "Alex", "Lillicrap", "Timothy P", "Harley", "Tim", "Silver", "David", "Kavukcuoglu", "Koray"], "venue": null, "citeRegEx": "Mnih et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Mnih et al\\.", "year": 2016}, {"title": "Spatial localization does not require the presence of local cues", "author": ["Morris", "Richard GM"], "venue": "Learning and motivation,", "citeRegEx": "Morris and GM.,? \\Q1981\\E", "shortCiteRegEx": "Morris and GM.", "year": 1981}, {"title": "A focused back-propagation algorithm for temporal pattern recognition", "author": ["Mozer", "Michael C"], "venue": "Complex systems,", "citeRegEx": "Mozer and C.,? \\Q1989\\E", "shortCiteRegEx": "Mozer and C.", "year": 1989}, {"title": "Reinforcement learning with hierarchies of machines", "author": ["Parr", "Ronald", "Russell", "Stuart"], "venue": null, "citeRegEx": "Parr et al\\.,? \\Q1998\\E", "shortCiteRegEx": "Parr et al\\.", "year": 1998}, {"title": "Temporal abstraction in reinforcement learning", "author": ["Precup", "Doina"], "venue": "PhD thesis, University of Massachusetts,", "citeRegEx": "Precup and Doina.,? \\Q2000\\E", "shortCiteRegEx": "Precup and Doina.", "year": 2000}, {"title": "Planning with closed-loop macro actions", "author": ["Precup", "Doina", "Sutton", "Richard S", "Singh", "Satinder P"], "venue": "Technical report,", "citeRegEx": "Precup et al\\.,? \\Q1997\\E", "shortCiteRegEx": "Precup et al\\.", "year": 1997}, {"title": "Theoretical results on reinforcement learning with temporally abstract options", "author": ["Precup", "Doina", "Sutton", "Richard S", "Singh", "Satinder"], "venue": "In European Conference on Machine Learning (ECML). Springer,", "citeRegEx": "Precup et al\\.,? \\Q1998\\E", "shortCiteRegEx": "Precup et al\\.", "year": 1998}, {"title": "Universal value function approximators", "author": ["Schaul", "Tom", "Horgan", "Dan", "Gregor", "Karol", "Silver", "David"], "venue": null, "citeRegEx": "Schaul et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Schaul et al\\.", "year": 2015}, {"title": "Neural sequence chunkers", "author": ["Schmidhuber", "J\u00fcrgen"], "venue": "Technical report,", "citeRegEx": "Schmidhuber and J\u00fcrgen.,? \\Q1991\\E", "shortCiteRegEx": "Schmidhuber and J\u00fcrgen.", "year": 1991}, {"title": "Trust region policy optimization", "author": ["Schulman", "John", "Levine", "Sergey", "Moritz", "Philipp", "Jordan", "Michael I", "Abbeel", "Pieter"], "venue": "In ICML,", "citeRegEx": "Schulman et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Schulman et al\\.", "year": 2015}, {"title": "High-dimensional continuous control using generalized advantage estimation", "author": ["Schulman", "John", "Moritz", "Philipp", "Levine", "Sergey", "Jordan", "Michael", "Abbeel", "Pieter"], "venue": null, "citeRegEx": "Schulman et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Schulman et al\\.", "year": 2016}, {"title": "Td models: Modeling the world at a mixture of time scales", "author": ["Sutton", "Richard S"], "venue": "In ICML,", "citeRegEx": "Sutton and S.,? \\Q1995\\E", "shortCiteRegEx": "Sutton and S.", "year": 1995}, {"title": "Between mdps and semi-mdps: A framework for temporal abstraction in reinforcement learning", "author": ["Sutton", "Richard S", "Precup", "Doina", "Singh", "Satinder"], "venue": "Artificial intelligence,", "citeRegEx": "Sutton et al\\.,? \\Q1999\\E", "shortCiteRegEx": "Sutton et al\\.", "year": 1999}, {"title": "A deep hierarchical approach to lifelong learning in minecraft", "author": ["Tessler", "Chen", "Givony", "Shahar", "Zahavy", "Tom", "Mankowitz", "Daniel J", "Mannor", "Shie"], "venue": "arXiv preprint arXiv:1604.07255,", "citeRegEx": "Tessler et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Tessler et al\\.", "year": 2016}, {"title": "Strategic attentive writer for learning macro-actions", "author": ["Vezhnevets", "Alexander", "Mnih", "Volodymyr", "Osindero", "Simon", "Graves", "Alex", "Vinyals", "Oriol", "Agapiou", "John", "kavukcuoglu", "koray"], "venue": null, "citeRegEx": "Vezhnevets et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Vezhnevets et al\\.", "year": 2016}, {"title": "Multi-scale context aggregation by dilated convolutions", "author": ["Yu", "Fisher", "Koltun", "Vladlen"], "venue": null, "citeRegEx": "Yu et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Yu et al\\.", "year": 2016}], "referenceMentions": [{"referenceID": 25, "context": "Deep reinforcement learning has recently enjoyed successes in many domains (Mnih et al., 2015; Schulman et al., 2015; Levine et al., 2015; Mnih et al., 2016; Lillicrap et al., 2015).", "startOffset": 75, "endOffset": 181}, {"referenceID": 13, "context": "Deep reinforcement learning has recently enjoyed successes in many domains (Mnih et al., 2015; Schulman et al., 2015; Levine et al., 2015; Mnih et al., 2016; Lillicrap et al., 2015).", "startOffset": 75, "endOffset": 181}, {"referenceID": 16, "context": "Deep reinforcement learning has recently enjoyed successes in many domains (Mnih et al., 2015; Schulman et al., 2015; Levine et al., 2015; Mnih et al., 2016; Lillicrap et al., 2015).", "startOffset": 75, "endOffset": 181}, {"referenceID": 14, "context": "Deep reinforcement learning has recently enjoyed successes in many domains (Mnih et al., 2015; Schulman et al., 2015; Levine et al., 2015; Mnih et al., 2016; Lillicrap et al., 2015).", "startOffset": 75, "endOffset": 181}, {"referenceID": 28, "context": "Building hierarchical agents is a long standing topic in reinforcement learning (Sutton et al., 1999; Precup, 2000; Dayan & Hinton, 1993; Dietterich, 2000; Boutilier et al., 1997; Dayan, 1993; Kaelbling, 2014; Parr & Russell, 1998; Precup et al., 1997; 1998; Schmidhuber, 1991; Sutton, 1995; Wiering & Schmidhuber, 1997; Vezhnevets et al., 2016; Bacon et al., 2017).", "startOffset": 80, "endOffset": 365}, {"referenceID": 3, "context": "Building hierarchical agents is a long standing topic in reinforcement learning (Sutton et al., 1999; Precup, 2000; Dayan & Hinton, 1993; Dietterich, 2000; Boutilier et al., 1997; Dayan, 1993; Kaelbling, 2014; Parr & Russell, 1998; Precup et al., 1997; 1998; Schmidhuber, 1991; Sutton, 1995; Wiering & Schmidhuber, 1997; Vezhnevets et al., 2016; Bacon et al., 2017).", "startOffset": 80, "endOffset": 365}, {"referenceID": 21, "context": "Building hierarchical agents is a long standing topic in reinforcement learning (Sutton et al., 1999; Precup, 2000; Dayan & Hinton, 1993; Dietterich, 2000; Boutilier et al., 1997; Dayan, 1993; Kaelbling, 2014; Parr & Russell, 1998; Precup et al., 1997; 1998; Schmidhuber, 1991; Sutton, 1995; Wiering & Schmidhuber, 1997; Vezhnevets et al., 2016; Bacon et al., 2017).", "startOffset": 80, "endOffset": 365}, {"referenceID": 30, "context": "Building hierarchical agents is a long standing topic in reinforcement learning (Sutton et al., 1999; Precup, 2000; Dayan & Hinton, 1993; Dietterich, 2000; Boutilier et al., 1997; Dayan, 1993; Kaelbling, 2014; Parr & Russell, 1998; Precup et al., 1997; 1998; Schmidhuber, 1991; Sutton, 1995; Wiering & Schmidhuber, 1997; Vezhnevets et al., 2016; Bacon et al., 2017).", "startOffset": 80, "endOffset": 365}, {"referenceID": 0, "context": "Building hierarchical agents is a long standing topic in reinforcement learning (Sutton et al., 1999; Precup, 2000; Dayan & Hinton, 1993; Dietterich, 2000; Boutilier et al., 1997; Dayan, 1993; Kaelbling, 2014; Parr & Russell, 1998; Precup et al., 1997; 1998; Schmidhuber, 1991; Sutton, 1995; Wiering & Schmidhuber, 1997; Vezhnevets et al., 2016; Bacon et al., 2017).", "startOffset": 80, "endOffset": 365}, {"referenceID": 28, "context": "The options framework (Sutton et al., 1999; Precup, 2000) is a popular formulation for considering the problem with a two level hierarchy.", "startOffset": 22, "endOffset": 57}, {"referenceID": 28, "context": "Options are typically learned using sub-goals and \u2018pseudo-rewards\u2019 that are provided explicitly (Sutton et al., 1999; Dietterich, 2000; Dayan & Hinton, 1993).", "startOffset": 96, "endOffset": 157}, {"referenceID": 23, "context": "For a simple, tabular case (Wiering & Schmidhuber, 1997; Schaul et al., 2015), each state can be used as a sub-goal.", "startOffset": 27, "endOffset": 77}, {"referenceID": 29, "context": "Recently (Tessler et al., 2016; Kulkarni et al., 2016) have demonstrated that combining deep learning with predefined sub-goals delivers promising results in challenging environments like Minecraft and Atari, however sub-goal discovery was not addressed.", "startOffset": 9, "endOffset": 54}, {"referenceID": 11, "context": "Recently (Tessler et al., 2016; Kulkarni et al., 2016) have demonstrated that combining deep learning with predefined sub-goals delivers promising results in challenging environments like Minecraft and Atari, however sub-goal discovery was not addressed.", "startOffset": 9, "endOffset": 54}, {"referenceID": 0, "context": "A recent work of (Bacon et al., 2017) shows the possibility of learning options jointly with a policy-over-options in an end-to-end fashion by extending the policy gradient theorem to options.", "startOffset": 17, "endOffset": 37}, {"referenceID": 0, "context": "Consequently, regularisers (Bacon et al., 2017; Vezhnevets et al., 2016) are usually introduced to steer the solution towards multiple options of extended length.", "startOffset": 27, "endOffset": 72}, {"referenceID": 30, "context": "Consequently, regularisers (Bacon et al., 2017; Vezhnevets et al., 2016) are usually introduced to steer the solution towards multiple options of extended length.", "startOffset": 27, "endOffset": 72}, {"referenceID": 8, "context": "The recently proposed UNREAL agent (Jaderberg et al., 2016) also demonstrates a strong improvement by using unsupervised auxiliary tasks to help refine its internal representations.", "startOffset": 35, "endOffset": 59}, {"referenceID": 16, "context": "Here we use an advantage actor critic (Mnih et al., 2016):", "startOffset": 38, "endOffset": 57}, {"referenceID": 16, "context": "This is substantially the same CNN as in (Mnih et al., 2016; 2015), the only difference is that in the pre-processing stage we retain all colour channels.", "startOffset": 41, "endOffset": 66}, {"referenceID": 10, "context": "This idea is similar to clockwork RNNs (Koutn\u0131\u0301k et al., 2014), however there the top level \u201cticks\u201d at a fixed, slow pace, whereas the dLSTM observes all the available training data instead.", "startOffset": 39, "endOffset": 62}, {"referenceID": 16, "context": "The LSTM (Hochreiter & Schmidhuber, 1997) architecture is a widely used recurrent network and it was demonstrated to perform very well on a suite of reinforcement learning problems (Mnih et al., 2016).", "startOffset": 181, "endOffset": 200}, {"referenceID": 16, "context": "We use the A3C method (Mnih et al., 2016) for all reinforcement learning experiments.", "startOffset": 22, "endOffset": 41}, {"referenceID": 16, "context": "It was shown to achieve state-of-the-art results on several challenging benchmarks (Mnih et al., 2016).", "startOffset": 83, "endOffset": 102}, {"referenceID": 11, "context": "Interestingly, two of the learnt sub-goals correspond to roughly the same locations as the ones hand-crafted in (Kulkarni et al., 2016) (ladder and key), but here they are learnt by the agent itself.", "startOffset": 112, "endOffset": 135}, {"referenceID": 30, "context": "Frostbite is a hard game (Vezhnevets et al., 2016; Lake et al., 2016) that requires both long-term credit assignment and good exploration.", "startOffset": 25, "endOffset": 69}, {"referenceID": 12, "context": "Frostbite is a hard game (Vezhnevets et al., 2016; Lake et al., 2016) that requires both long-term credit assignment and good exploration.", "startOffset": 25, "endOffset": 69}, {"referenceID": 0, "context": "Option-critic architecture (Bacon et al., 2017) is, to the best of our knowledge, the only other end-to-end trainable system with sub-policies.", "startOffset": 27, "endOffset": 47}, {"referenceID": 0, "context": "The experimental results for Option-Critic on 4 ATARI (Bacon et al., 2017) games show scores similar those from a flat DQN (Mnih et al.", "startOffset": 54, "endOffset": 74}, {"referenceID": 16, "context": "Notice that our baseline (Mnih et al., 2016) is much stronger than DQN.", "startOffset": 25, "endOffset": 44}, {"referenceID": 0, "context": "Transition policy gradient First we consider a \u2018nonFeudal\u2019 FuN \u2013 it has exactly the same network architecture as FuN, but the Managers output g is trained with gradients coming directly from the Worker and no intrinsic reward is used, much like in Option-Critic architecture (Bacon et al., 2017).", "startOffset": 275, "endOffset": 295}, {"referenceID": 26, "context": "Second, g is learnt using a standard policy gradient approach with the Manager emitting the mean of a Gaussian distribution from which goals are sampled (as if the Manager were solving a continuous control problem (Schulman et al., 2016; Mnih et al., 2016; Lillicrap et al., 2015)).", "startOffset": 214, "endOffset": 280}, {"referenceID": 16, "context": "Second, g is learnt using a standard policy gradient approach with the Manager emitting the mean of a Gaussian distribution from which goals are sampled (as if the Manager were solving a continuous control problem (Schulman et al., 2016; Mnih et al., 2016; Lillicrap et al., 2015)).", "startOffset": 214, "endOffset": 280}, {"referenceID": 14, "context": "Second, g is learnt using a standard policy gradient approach with the Manager emitting the mean of a Gaussian distribution from which goals are sampled (as if the Manager were solving a continuous control problem (Schulman et al., 2016; Mnih et al., 2016; Lillicrap et al., 2015)).", "startOffset": 214, "endOffset": 280}, {"referenceID": 30, "context": "(These modifications adapt all the \u201chard-wired\u201d Action repeat is a heuristic used in all successful agents (Mnih et al., 2015; 2016; Bellemare et al., 2016b; Vezhnevets et al., 2016).", "startOffset": 107, "endOffset": 182}], "year": 2017, "abstractText": "We introduce FeUdal Networks (FuNs): a novel architecture for hierarchical reinforcement learning. Our approach is inspired by the feudal reinforcement learning proposal of Dayan and Hinton, and gains power and efficacy by decoupling end-to-end learning across multiple levels \u2013 allowing it to utilise different resolutions of time. Our framework employs a Manager module and a Worker module. The Manager operates at a lower temporal resolution and sets abstract goals which are conveyed to and enacted by the Worker. The Worker generates primitive actions at every tick of the environment. The decoupled structure of FuN conveys several benefits \u2013 in addition to facilitating very long timescale credit assignment it also encourages the emergence of sub-policies associated with different goals set by the Manager. These properties allow FuN to dramatically outperform a strong baseline agent on tasks that involve long-term credit assignment or memorisation. We demonstrate the performance of our proposed system on a range of tasks from the ATARI suite and also from a 3D DeepMind Lab environment.", "creator": "LaTeX with hyperref package"}}}