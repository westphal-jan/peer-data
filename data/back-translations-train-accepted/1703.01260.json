{"id": "1703.01260", "review": {"conference": "nips", "VERSION": "v1", "DATE_OF_SUBMISSION": "3-Mar-2017", "title": "EX2: Exploration with Exemplar Models for Deep Reinforcement Learning", "abstract": "Efficient exploration in high-dimensional environments remains a key challenge in reinforcement learning (RL). Deep reinforcement learning methods have demonstrated the ability to learn with highly general policy classes for complex tasks with high-dimensional inputs, such as raw images. However, many of the most effective exploration techniques rely on tabular representations, or on the ability to construct a generative model over states and actions. Both are exceptionally difficult when these inputs are complex and high dimensional. On the other hand, it is comparatively easy to build discriminative models on top of complex states such as images using standard deep neural networks. This paper introduces a novel approach, EX2, which approximates state visitation densities by training an ensemble of discriminators, and assigns reward bonuses to rarely visited states. We demonstrate that EX2 achieves comparable performance to the state-of-the-art methods on low-dimensional tasks, and its effectiveness scales into high-dimensional state spaces such as visual domains without hand-designing features or density models.", "histories": [["v1", "Fri, 3 Mar 2017 17:38:59 GMT  (2908kb,D)", "https://arxiv.org/abs/1703.01260v1", null], ["v2", "Sat, 27 May 2017 05:09:09 GMT  (4087kb,D)", "http://arxiv.org/abs/1703.01260v2", null]], "reviews": [], "SUBJECTS": "cs.LG", "authors": ["justin fu", "john d co-reyes", "sergey levine"], "accepted": true, "id": "1703.01260"}, "pdf": {"name": "1703.01260.pdf", "metadata": {"source": "CRF", "title": "EX: Exploration with Exemplar Models for Deep Reinforcement Learning", "authors": ["Justin Fu", "John D. Co-Reyes", "Sergey Levine"], "emails": ["justinfu@eecs.berkeley.edu", "jcoreyes@eecs.berkeley.edu", "svlevine@eecs.berkeley.edu"], "sections": [{"heading": "1 Introduction", "text": "In fact, most of them are able to play by the rules they have set themselves."}, {"heading": "2 Related Work", "text": "In fact, most of them are able to survive themselves if they do not put themselves in a position to survive themselves. Most of them are able to survive themselves, and most of them are not able to survive themselves. Most of them are able to survive themselves, and most of them are not able to survive themselves. Most of them are able to survive themselves, and most of them are not able to survive themselves. Most of them are able to survive themselves, and most of them are able to survive themselves, and most of them are able to survive themselves."}, {"heading": "3 Preliminaries", "text": "In this paper, we consider a Markov decision-making process (MDP) defined by the tuple (S, A, T, R, \u03b3, \u03c10).S, A are the state and the scope of action. The transition distribution T (s \u2032 | a, s), the initial state distribution 0 (s) and the reward function R (s, a) are unknown in the constellation of consolidation and can only be called into question by interaction with the MDP. The goal of consolidation learning is to find the optimal political distribution method that maximizes the expected sum of discounted rewards, \u03c0 = arg max\u03c0 E\u03c4; \u03c0 T = 0 \u03b3tR (st, at)], where it refers to a path (s0, a0,... sT, aT) and refers to maximizing the expected sum of discounted rewards, GDP = arg max\u03c0 (s0)."}, {"heading": "4 Exemplar Models and Density Estimation", "text": "We begin by describing our discrimination model to predict the novelty of states visited during the training, highlighting a link between this particular form of discrimination model and density estimation, and in Section 5 we describe how this model can be used to generate reward bonuses."}, {"heading": "4.1 Exemplar Models", "text": "In order to avoid the need for explicit generative models, our novelty estimation method uses discrimination models. In the face of a data set X = {x1,... xn}, a copy model consists of a set of n classifiers or discriminators {Dx1,.... Dxn}, one for each data point. Each individual discriminator Dxi is trained to distinguish a single positive data point xi, the \"specimen,\" from the other points in data set X. However, we borrow the term \"specimen model\" by Malisiewicz et al. (2011), which coined the term \"specimen SVM\" to refer to a specific linear model trained to classify each instance against all others. However, our work is the first to apply this idea to exploration for the purpose of enhancing learning. In practice, we avoid the need to train n different classifiers x by obtaining a single copy of X-conditional network (as discussed in the X section)."}, {"heading": "4.2 Exemplar Models as Implicit Density Estimation", "text": "To show how the example model can be used for implicit density estimation, we begin by considering an infinitely strong, optimal discriminator, for which we can establish an explicit link between the discriminator and the underlying data distribution PX (x): Proposal 1. (Optimal discriminator) q q includes the optimal discriminator Dx (x) for a discrete distribution PX (x). The proof is provided by taking the derivation of the loss in Eq (x) in relation to D (x), setting it to zero and solving it for D (x). It follows that if the discriminator is optimal, we can restore x the probability of a data point x by evaluating the discriminator by its own example."}, {"heading": "4.3 Latent Space Smoothing with Noisy Discriminators", "text": "In the previous section, we discussed how adding noise can provide smoothed density estimates, which is particularly important in complex or continuous spaces where all states could be distinguishable with a sufficiently strong discriminator. Unfortunately, in this section, we discuss how we can learn smoothing distribution by injecting the noise directly into a learned latent space, rather than adding it to the original states. Formally, we are introducing a latent variable z. We want an encoder distribution q (z | x) and a latent spatial classification p (y | z) = D (z) y (1 \u2212 D) y (z) 1 \u2212 y, where y = 1, if x = x and y = 0 x (z).We additionally regulate the noise distribution against a previous unit we obtain."}, {"heading": "4.4 Smoothing from Suboptimal Discriminators", "text": "In our previous derivatives, we assume an optimal, infinitely powerful discriminator that can generate a different value D (x) for each input x. However, this is typically not possible except for small, countable ranges. A secondary but important source of density smoothing occurs when the discriminator has difficulty distinguishing between two states x and x. In this case, the discriminator is used on average over the results of the infinitely powerful discriminator. This form of smoothing results from the discriminator's inductive bias, which is difficult to quantify. In practice, we have typically found this effect to be beneficial to our model and not harmful. An example of such smoothed density estimates is shown in Figure 2. Due to this effect, the addition of noise is not strictly necessary to benefit from smoothing, although it provides significantly better control over the degree of smoothing."}, {"heading": "6 Model Architecture", "text": "To process complex observations such as images, we implement our pattern model using neural networks, using revolutionary models for image-based domains. To reduce the cost of training such large classifiers per copy, we examine two methods for amortizing the calculation using multiple examples."}, {"heading": "6.1 Amortized Multi-Exemplar Model", "text": "Instead of building a separate classifier for each specimen, we can instead train a single model based on the specimen x *. Using the latent spatial formula, we condition the latent spatial discriminator p (y | z) on an encoded version of x * given by q (z * | x *), resulting in a classifier for the form p (y | z, z *) = D (z, z *) y (1 \u2212 D (z, z *) 1 \u2212 y. The advantage of this amortized model is that we do not need to build new discriminators from scratch for each iteration and provide a certain degree of generalization for density estimation in new states. A diagram of this architecture is shown in Figure 1. The amortized architecture has the appearance of a comparison operator: it is trained to output 0 when x * 6 = x, and the optimal discriminator values covered in Section 4 are subjected to the smoothing that is provided by the noise lattice."}, {"heading": "6.2 K-Exemplar Model", "text": "As long as the distribution of positive examples is known, we can restore density estimates via Equation (3). Therefore, we can also consider a number of examples x1,..., xK and samples from this group evenly during training. We call this model a \"K-specimen\" model, which allows us to smoothly interpolate between a more powerful model with one discriminator per state (K = 1) and a weaker model that uses a single discriminator for all states (K = # states). A more detailed discussion of this method is included in Appendix A.2. In our experiments, we place adjacent states in an orbit into the same discriminator, which corresponds to a form of temporal regulation that assumes that adjacent states are similar over time. We also share the majority of layers between discriminators in the neural networks similarly (Osband et al., 2016), leaving only the final linear layer among discriminators that are similar in architecture."}, {"heading": "6.3 Relationship to Generative Adverserial Networks (GANs)", "text": "Our exploration algorithm has an interesting interpretation of GANs (Goodfellow et al., 2014). Politics can be considered a generator of a GAN, and the pattern model serves as a discriminator that attempts to classify states from the current group of trajectories against previous states. Using the K pattern version of our algorithm, we can train a single discriminator for all states in the current group (rather than one for each state) that mirrors the GAN structure. In GANs, the generator plays a cooperative game with the discriminator by trying to generate indistinguishable samples to deceive the discriminator. However, in our algorithm, the generator is rewarded for helping the discriminator rather than deceiving him, so that our algorithm plays a cooperative game with the discriminator instead of playing an adequate game. Instead, they compete with the progression of time: if a new state is visited frequently, the desired buffer will lose its ownership of the state as it continues to be explored and explored by the new state."}, {"heading": "7 Experimental Evaluation", "text": "Dre rf\u00fc ide rf\u00fc ide rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rfu the rf\u00fc the rf\u00fc the rfu the rf\u00fc the rf\u00fc the rf\u00fc the rfu the rf\u00fc the rfu the rfu the rfu the rfu the rfu the rfu the"}, {"heading": "8 Conclusion and Future Work", "text": "We introduced EX2, a scalable exploration strategy based on the creation of discriminatory copy models to allocate novelty bonuses. We also demonstrated a novel link between copy models and density estimation that motivates our algorithm to approach the exploration of pseudo-numbers. Also, this density estimation technology does not require reconstruction of samples to train them, unlike most methods for generative or energy-based models. Our empirical results show that EX2 tends to achieve comparable results to previous state-of-the-art for continuous control tasks in low-dimensional environments, and can gracefully scale to handle rich sensory inputs such as images. Since our method avoids generative modeling of complex image-based observations, it exceeds the performance of previous generative methods in domains with more complex observation functions, such as egocentric doom navigation, which differentiates the constant from the magical task."}, {"heading": "A Appendix", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "A.1 Noisy Discriminators", "text": "Our noisy latent space discriminator of section 4.3 optimizes the target: max py (= q = q (= q) (= q) (= q) (= x) (= x) (= q) (= q) (= q (= q) (= q) (= x) (= x) (= x) (= x) (= x) (= x) (= x) (= x) (= x) (= x) (= x) (= x) (= x) (= x) (= x) (= x) (= x) (= x) (= x) (= x) (= x) (= x) (= x) (= x) x) (= x) x) (= x) (= x) (= x) (= x)."}, {"heading": "A.2 K-Exemplar Model", "text": "In the K-copy model, each discriminator is associated with a stack of K-positive examples B = {x1,.. xK}. In this case, we randomly sample positives from the B-pile and do not always use a single copy. If we give PB (x) a uniform distribution over B, we can characterize the optimal discriminator for the nonsignificant K-copy model: Proposition 4. (K-copy Optimal Discriminator) For a discriminator trained with K-positives, we can use the same argument as the single-copy model, characterize the optimal discriminator for the nonsignificant K-copy model: Proposition 4. (K-copy Optimal Discriminator) For a discriminator trained with K-positives, we follow the K-copy model (xK) exactly the (K-copy version)."}, {"heading": "A.3 Task Descriptions", "text": "In this section we describe the tasks to be used in our experiments.Sample images of these tasks are included in Figure 4.2D Maze. This task involves navigating through a 2D maze, using the (x, y) coordinate of the agent as an observation. The challenge arises from the sparse reward that runs only in a small box around the target. The agent must therefore figure out how to achieve novel parts of the maze to eventually get the reward regionally.SparseHalfCheetah. This task involves a 6-DoF robot moving forward as quickly as possible. However, this task has been modified to obtain a sparse reward, such as that the agent agent-agent-agent receives reward only when he reaches a certain position threshold, and receives a permanent reward following thereafter. SwimmerGather. This motion task, which was originally proposed as an Agent-agent-agent hierarchical task by Duet al at 2016, includes an Agent-agent-the Naway."}, {"heading": "A.4 Experiment Hyperparameters", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "A.4.1 Policy Model Parameters", "text": "For non-image tasks we used a 2-layer neural network with 32 hidden units per layer and relay nonlinearities. For Doom we used 2 Convolutionary layers (16 4x4 filters, step 2) followed by 2 fully connected layers with 32 units each. All nonlinearities were relus. We reduced the input screen to an RGB 32 x 32 image. For Atari we used 2 Convolutionary layers (32 8x8 filters, step 4, 16 4x4 filter step 2) followed by 2 fully connected layers with 256 units each. All nonlinearities were relus. For Atari we used the last 4 images each in the size of a grayscale image 42 x 42."}, {"heading": "A.4.2 Exemplar Model Parameters", "text": "We used an identical, fully connected sample architecture for all non-image tasks and a convoluted architecture for the image task. For non-image tasks, we used a common neural network with 2 layers of tanh nonlinearity and 16 units per layer. The last non-split layer was a linear layer. For image-based tasks, we used a common network consisting of 2 convoluted layers (16 4x4 filters, step 2) followed by 2 fully connected layers of 16 units each. The Constitutional layers used relative nonlinearity and the fully connected used tanh. The common network architecture is identical to the political architecture. The last non-split layer was a linear layer. We also found it useful to lower the learning rate for the common network, as it has many more gradients that propose themselves through it than the non-divided layer. Therefore, we optimized our model with ADAM with a learning rate of 5 \u00d7 10 \u2212 4 for the shared layers and non-divided layers \u2212 3."}, {"heading": "A.4.3 Amortized Model Parameters", "text": "For each encoder, we use a 2-layer neural network with 32 hidden units per layer and tanh nonlinearity, which represents the mean and log variance of the latent representation of the size 41. The encoder's latent codes are linked and fed into the discriminator, which is another 2-layer neural network with 32 hidden units per layer and tanh nonlinearity.For image-based tasks, we use the input with 2 revolutionary layers (16 4x4 filters, step 2) before feeding into the encoder and the discriminator we use the same architecture as above, except we use 64 hidden units and a latent size of 32. We use a learning rate of 1 x 10 \u2212 4 and optimize the model with ADAM. We found it important to match the weight to the KL divergence, which affects the discrimination, how well the discriminator can discriminate."}], "references": [{"title": "Exploratory gradient boosting for reinforcement learning in complex domains", "author": ["Abel", "David", "Agarwal", "Alekh", "Diaz", "Fernando", "Krishnamurthy", "Akshay", "Schapire", "Robert E"], "venue": "In Advances in Neural Information Processing Systems (NIPS),", "citeRegEx": "Abel et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Abel et al\\.", "year": 2016}, {"title": "Surprise-based intrinsic motivation for deep reinforcement learning", "author": ["Achiam", "Joshua", "Sastry", "Shankar"], "venue": null, "citeRegEx": "Achiam et al\\.,? \\Q2017\\E", "shortCiteRegEx": "Achiam et al\\.", "year": 2017}, {"title": "Recent advances in hierarchical reinforcement learning", "author": ["Barto", "Andrew G", "Mahadevan", "Sridhar"], "venue": "Discrete Event Dynamic Systems,", "citeRegEx": "Barto et al\\.,? \\Q2003\\E", "shortCiteRegEx": "Barto et al\\.", "year": 2003}, {"title": "Unifying count-based exploration and intrinsic motivation", "author": ["Bellemare", "Marc G", "Srinivasan", "Sriram", "Ostrovski", "Georg", "Schaul", "Tom", "Saxton", "David", "Munos", "Remi"], "venue": "In Advances in Neural Information Processing Systems (NIPS),", "citeRegEx": "Bellemare et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Bellemare et al\\.", "year": 2016}, {"title": "R-max \u2013 a general polynomial time algorithm for near-optimal reinforcement learning", "author": ["Brafman", "Ronen I", "Tennenholtz", "Moshe"], "venue": "Journal of Machine Learning Research (JMLR),", "citeRegEx": "Brafman et al\\.,? \\Q2002\\E", "shortCiteRegEx": "Brafman et al\\.", "year": 2002}, {"title": "An empirical evaluation of thompson sampling", "author": ["O. Chapelle", "Li", "Lihong"], "venue": "In Advances in Neural Information Processing Systems (NIPS),", "citeRegEx": "Chapelle et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Chapelle et al\\.", "year": 2011}, {"title": "Intrinsically Motivated Reinforcement Learning", "author": ["Chentanez", "Nuttapong", "Barto", "Andrew G", "Singh", "Satinder P"], "venue": "In Advances in Neural Information Processing Systems (NIPS). MIT Press,", "citeRegEx": "Chentanez et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Chentanez et al\\.", "year": 2005}, {"title": "Benchmarking deep reinforcement learning for continuous control", "author": ["Duan", "Yan", "Chen", "Xi", "Houthooft", "Rein", "Schulman", "John", "Abbeel", "Pieter"], "venue": "In International Conference on Machine Learning (ICML),", "citeRegEx": "Duan et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Duan et al\\.", "year": 2016}, {"title": "Stochastic neural networks for hierarchical reinforcement learning", "author": ["Florensa", "Carlos Campo", "Duan", "Yan", "Abbeel", "Pieter"], "venue": "In International Conference on Learning Representations (ICLR),", "citeRegEx": "Florensa et al\\.,? \\Q2017\\E", "shortCiteRegEx": "Florensa et al\\.", "year": 2017}, {"title": "Generative adversarial nets", "author": ["Goodfellow", "Ian", "Pouget-Abadie", "Jean", "Mirza", "Mehdi", "Xu", "Bing", "Warde-Farley", "David", "Ozair", "Sherjil", "Courville", "Aaron", "Bengio", "Yoshua"], "venue": "In Advances in Neural Information Processing Systems (NIPS)", "citeRegEx": "Goodfellow et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Goodfellow et al\\.", "year": 2014}, {"title": "Learning and transfer of modulated locomotor", "author": ["Heess", "Nicolas", "Wayne", "Gregory", "Tassa", "Yuval", "Lillicrap", "Timothy P", "Riedmiller", "Martin A", "Silver", "David"], "venue": "controllers. CoRR,", "citeRegEx": "Heess et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Heess et al\\.", "year": 2016}, {"title": "Vime: Variational information maximizing exploration", "author": ["Houthooft", "Rein", "Chen", "Xi", "Duan", "Yan", "Schulman", "John", "Turck", "Filip De", "Abbeel", "Pieter"], "venue": "In Advances in Neural Information Processing Systems (NIPS),", "citeRegEx": "Houthooft et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Houthooft et al\\.", "year": 2016}, {"title": "Exploration in metric state spaces", "author": ["Kakade", "Sham", "Kearns", "Michael", "Langford", "John"], "venue": "In International Conference on Machine Learning (ICML),", "citeRegEx": "Kakade et al\\.,? \\Q2003\\E", "shortCiteRegEx": "Kakade et al\\.", "year": 2003}, {"title": "Near-optimal reinforcement learning in polynomial time", "author": ["Kearns", "Michael", "Singh", "Satinder"], "venue": "Machine Learning,", "citeRegEx": "Kearns et al\\.,? \\Q2002\\E", "shortCiteRegEx": "Kearns et al\\.", "year": 2002}, {"title": "Near-bayesian exploration in polynomial time", "author": ["Kolter", "J. Zico", "Ng", "Andrew Y"], "venue": "In International Conference on Machine Learning (ICML),", "citeRegEx": "Kolter et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Kolter et al\\.", "year": 2009}, {"title": "Hierarchical deep reinforcement learning: Integrating temporal abstraction and intrinsic motivation", "author": ["Kulkarni", "Tejas D", "Narasimhan", "Karthik", "Saeedi", "Ardavan", "Tenenbaum", "Josh"], "venue": "In Advances in Neural Information Processing Systems (NIPS)", "citeRegEx": "Kulkarni et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Kulkarni et al\\.", "year": 2016}, {"title": "Continuous control with deep reinforcement learning", "author": ["Lillicrap", "Timothy P", "Hunt", "Jonathan J", "Pritzel", "Alexander", "Heess", "Nicolas", "Erez", "Tom", "Tassa", "Yuval", "Silver", "David", "Wierstra", "Daan"], "venue": "In International Conference on Learning Representations (ICLR),", "citeRegEx": "Lillicrap et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Lillicrap et al\\.", "year": 2015}, {"title": "Ensemble of exemplar-svms for object detection and beyond", "author": ["Malisiewicz", "Tomasz", "Gupta", "Abhinav", "Efros", "Alexei A"], "venue": "In International Conference on Computer Vision (ICCV),", "citeRegEx": "Malisiewicz et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Malisiewicz et al\\.", "year": 2011}, {"title": "Deep multi-scale video prediction beyond mean square", "author": ["Mathieu", "Micha\u00ebl", "Couprie", "Camille", "LeCun", "Yann"], "venue": "error. CoRR,", "citeRegEx": "Mathieu et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Mathieu et al\\.", "year": 2015}, {"title": "Human-level control through deep reinforcement learning", "author": ["Mnih", "Volodymyr", "Kavukcuoglu", "Koray", "Silver", "David", "Rusu", "Andrei A", "Veness", "Joel", "Bellemare", "Marc G", "Graves", "Alex", "Riedmiller", "Martin", "Fidjeland", "Andreas K", "Ostrovski", "Georg", "Petersen", "Stig", "Beattie", "Charles", "Sadik", "Amir", "Antonoglou", "Ioannis", "King", "Helen", "Kumaran", "Dharshan", "Wierstra", "Daan", "Legg", "Shane", "Hassabis", "Demis"], "venue": "Nature, 518(7540):529\u2013533,", "citeRegEx": "Mnih et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Mnih et al\\.", "year": 2015}, {"title": "Action-conditional video prediction using deep networks in atari games", "author": ["Oh", "Junhyuk", "Guo", "Xiaoxiao", "Lee", "Honglak", "Lewis", "Richard", "Singh", "Satinder"], "venue": "In Advances in Neural Information Processing Systems (NIPS),", "citeRegEx": "Oh et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Oh et al\\.", "year": 2015}, {"title": "Deep exploration via bootstrapped DQN", "author": ["Osband", "Ian", "Blundell", "Charles", "Alexander Pritzel", "Benjamin Van Roy"], "venue": "In Advances in Neural Information Processing Systems (NIPS),", "citeRegEx": "Osband et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Osband et al\\.", "year": 2016}, {"title": "Curiosity-driven exploration by self-supervised prediction", "author": ["Pathak", "Deepak", "Agrawal", "Pulkit", "Efros", "Alexei A", "Darrell", "Trevor"], "venue": "In International Conference on Machine Learning (ICML),", "citeRegEx": "Pathak et al\\.,? \\Q2017\\E", "shortCiteRegEx": "Pathak et al\\.", "year": 2017}, {"title": "Pac optimal exploration in continuous space markov decision processes", "author": ["Pazis", "Jason", "Parr", "Ronald"], "venue": "In AAAI Conference on Artificial Intelligence (AAAI),", "citeRegEx": "Pazis et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Pazis et al\\.", "year": 2013}, {"title": "Improved techniques for training gans", "author": ["Salimans", "Tim", "Goodfellow", "Ian J", "Zaremba", "Wojciech", "Cheung", "Vicki", "Radford", "Alec", "Chen", "Xi"], "venue": "In Advances in Neural Information Processing Systems (NIPS),", "citeRegEx": "Salimans et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Salimans et al\\.", "year": 2016}, {"title": "A possibility for implementing curiosity and boredom in model-building neural controllers", "author": ["Schmidhuber", "J\u00fcrgen"], "venue": "In Proceedings of the First International Conference on Simulation of Adaptive Behavior on From Animals to Animats,", "citeRegEx": "Schmidhuber and J\u00fcrgen.,? \\Q1990\\E", "shortCiteRegEx": "Schmidhuber and J\u00fcrgen.", "year": 1990}, {"title": "Trust region policy optimization", "author": ["Schulman", "John", "Levine", "Sergey", "Moritz", "Philipp", "Jordan", "Michael I", "Abbeel", "Pieter"], "venue": "In International Conference on Machine Learning (ICML),", "citeRegEx": "Schulman et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Schulman et al\\.", "year": 2015}, {"title": "Incentivizing exploration in reinforcement learning with deep predictive models", "author": ["Stadie", "Bradly C", "Levine", "Sergey", "Abbeel", "Pieter"], "venue": "CoRR, abs/1507.00814,", "citeRegEx": "Stadie et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Stadie et al\\.", "year": 2015}, {"title": "Learning Options in Reinforcement Learning", "author": ["Stolle", "Martin", "Precup", "Doina"], "venue": "ISBN 978-3-540-45622-3", "citeRegEx": "Stolle et al\\.,? \\Q2002\\E", "shortCiteRegEx": "Stolle et al\\.", "year": 2002}, {"title": "An analysis of model-based interval estimation for markov decision processes", "author": ["Strehl", "Alexander L", "Littman", "Michael L"], "venue": "Journal of Computer and System Sciences,", "citeRegEx": "Strehl et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Strehl et al\\.", "year": 2009}, {"title": "exploration: A study of count-based exploration for deep reinforcement learning", "author": ["Tang", "Haoran", "Houthooft", "Rein", "Foote", "Davis", "Stooke", "Adam", "Chen", "Xi", "Duan", "Yan", "Schulman", "John", "Turck", "Filip De", "Abbeel", "Pieter"], "venue": null, "citeRegEx": "Tang et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Tang et al\\.", "year": 2016}, {"title": "The agent therefore has to figure out how to reach novel parts of the maze in order to eventually find the reward region. SparseHalfCheetah. This task involves making a 6-DoF robot run forward", "author": ["Houthooft"], "venue": null, "citeRegEx": "Houthooft,? \\Q2016\\E", "shortCiteRegEx": "Houthooft", "year": 2016}], "referenceMentions": [{"referenceID": 19, "context": "Recent work has shown that methods that combine reinforcement learning with rich function approximators, such as deep neural networks, can solve a range of complex tasks, from playing Atari games (Mnih et al., 2015) to controlling simulated robots (Schulman et al.", "startOffset": 196, "endOffset": 215}, {"referenceID": 26, "context": ", 2015) to controlling simulated robots (Schulman et al., 2015).", "startOffset": 40, "endOffset": 63}, {"referenceID": 19, "context": "Standard exploration strategies, such as -greedy strategies (Mnih et al., 2015) or Gaussian noise (Lillicrap et al.", "startOffset": 60, "endOffset": 79}, {"referenceID": 16, "context": ", 2015) or Gaussian noise (Lillicrap et al., 2015), are undirected and do not explicitly seek out interesting states.", "startOffset": 26, "endOffset": 50}, {"referenceID": 27, "context": "A promising avenue for more directed exploration is to explicitly estimate the novelty of a state, using predictive models that generate future states (Schmidhuber, 1990; Stadie et al., 2015; Achiam & Sastry, 2017) or model state densities (Bellemare et al.", "startOffset": 151, "endOffset": 214}, {"referenceID": 3, "context": ", 2015; Achiam & Sastry, 2017) or model state densities (Bellemare et al., 2016; Tang et al., 2016; Abel et al., 2016).", "startOffset": 56, "endOffset": 118}, {"referenceID": 30, "context": ", 2015; Achiam & Sastry, 2017) or model state densities (Bellemare et al., 2016; Tang et al., 2016; Abel et al., 2016).", "startOffset": 56, "endOffset": 118}, {"referenceID": 0, "context": ", 2015; Achiam & Sastry, 2017) or model state densities (Bellemare et al., 2016; Tang et al., 2016; Abel et al., 2016).", "startOffset": 56, "endOffset": 118}, {"referenceID": 11, "context": "Related concepts such as count-based bonuses have been shown to provide substantial speedups in classic reinforcement learning (Strehl & Littman, 2009; Kolter & Ng, 2009), and several recent works have proposed information-theoretic or probabilistic approaches to exploration based on this idea (Houthooft et al., 2016; Chentanez et al., 2005) by drawing on formal results in simpler discrete or linear systems (Bubeck & Cesa-Bianchi, 2012).", "startOffset": 295, "endOffset": 343}, {"referenceID": 6, "context": "Related concepts such as count-based bonuses have been shown to provide substantial speedups in classic reinforcement learning (Strehl & Littman, 2009; Kolter & Ng, 2009), and several recent works have proposed information-theoretic or probabilistic approaches to exploration based on this idea (Houthooft et al., 2016; Chentanez et al., 2005) by drawing on formal results in simpler discrete or linear systems (Bubeck & Cesa-Bianchi, 2012).", "startOffset": 295, "endOffset": 343}, {"referenceID": 24, "context": "When the observations are complex and high-dimensional, such as in the case of raw images, these models can be difficult to train, since generating and predicting images and other high-dimensional objects is still an open problem, despite recent progress (Salimans et al., 2016).", "startOffset": 255, "endOffset": 278}, {"referenceID": 3, "context": "Though successful results with generative novelty models have been reported with simple synthetic images, such as in Atari games (Bellemare et al., 2016; Tang et al., 2016), we show in our \u2217equal contribution.", "startOffset": 129, "endOffset": 172}, {"referenceID": 30, "context": "Though successful results with generative novelty models have been reported with simple synthetic images, such as in Atari games (Bellemare et al., 2016; Tang et al., 2016), we show in our \u2217equal contribution.", "startOffset": 129, "endOffset": 172}, {"referenceID": 12, "context": "Exploring in such state spaces has typically involved strategies such as introducing distance metrics over the state space (Pazis & Parr, 2013; Kakade et al., 2003), and approximating the quantities used in classical exploration methods.", "startOffset": 123, "endOffset": 164}, {"referenceID": 30, "context": "Prior works have employed approximations for the state-visitation count (Tang et al., 2016; Bellemare et al., 2016; Abel et al., 2016), information gain, or prediction error based on a learned dynamics model (Houthooft et al.", "startOffset": 72, "endOffset": 134}, {"referenceID": 3, "context": "Prior works have employed approximations for the state-visitation count (Tang et al., 2016; Bellemare et al., 2016; Abel et al., 2016), information gain, or prediction error based on a learned dynamics model (Houthooft et al.", "startOffset": 72, "endOffset": 134}, {"referenceID": 0, "context": "Prior works have employed approximations for the state-visitation count (Tang et al., 2016; Bellemare et al., 2016; Abel et al., 2016), information gain, or prediction error based on a learned dynamics model (Houthooft et al.", "startOffset": 72, "endOffset": 134}, {"referenceID": 11, "context": ", 2016), information gain, or prediction error based on a learned dynamics model (Houthooft et al., 2016; Stadie et al., 2015; Achiam & Sastry, 2017).", "startOffset": 81, "endOffset": 149}, {"referenceID": 27, "context": ", 2016), information gain, or prediction error based on a learned dynamics model (Houthooft et al., 2016; Stadie et al., 2015; Achiam & Sastry, 2017).", "startOffset": 81, "endOffset": 149}, {"referenceID": 21, "context": "For example, bootstrapped DQN (Osband et al., 2016) avoids the need to construct a generative model of the state by instead training multiple, randomized value functions and performs exploration by sampling a value function, and executing the greedy policy with respect to the value function.", "startOffset": 30, "endOffset": 51}, {"referenceID": 8, "context": "A few recent works in deep RL have used hierarchies to explore in sparse reward environments (Florensa et al., 2017; Heess et al., 2016).", "startOffset": 93, "endOffset": 136}, {"referenceID": 10, "context": "A few recent works in deep RL have used hierarchies to explore in sparse reward environments (Florensa et al., 2017; Heess et al., 2016).", "startOffset": 93, "endOffset": 136}, {"referenceID": 15, "context": "However, learning a hierarchy is difficult and has generally required curriculum learning or manually designed subgoals (Kulkarni et al., 2016).", "startOffset": 120, "endOffset": 143}, {"referenceID": 0, "context": ", 2016; Abel et al., 2016), information gain, or prediction error based on a learned dynamics model (Houthooft et al., 2016; Stadie et al., 2015; Achiam & Sastry, 2017). Bellemare et al. (2016) show that count-based methods in some sense bound the bonuses produced by exploration incentives based on intrinsic motivation, such as model uncertainty or information gain, making count-based or density-based bonuses an appealing and simple option.", "startOffset": 8, "endOffset": 194}, {"referenceID": 22, "context": "Concurrently with this work, Pathak et al. (2017) proposed to use discriminatively trained exploration bonuses by learning state features which are trained to predict the action from state transition pairs.", "startOffset": 29, "endOffset": 50}, {"referenceID": 3, "context": "In domains where explicit counting is impractical, pseudo-counts can be used based on a density estimate p(s, a), which typically is done using some sort of generatively trained density estimation model (Bellemare et al., 2016).", "startOffset": 203, "endOffset": 227}, {"referenceID": 17, "context": "We borrow the term \u201cexemplar model\u201d from Malisiewicz et al. (2011), which coined the term \u201cexemplar SVM\u201d to refer to a particular linear model trained to classify each instance against all others.", "startOffset": 41, "endOffset": 67}, {"referenceID": 30, "context": "For discrete domains, we can also use a count-based 1/ \u221a N(s) (Tang et al., 2016), where N(s) = nP (s), and n being the size of the replay buffer B.", "startOffset": 62, "endOffset": 81}, {"referenceID": 21, "context": "We also share the majority of layers between discriminators in the neural networks similar to (Osband et al., 2016), and only allow the final linear layer to vary amongst discriminators, which forces the shared layers to learn a joint feature representation, similarly to the amortized model.", "startOffset": 94, "endOffset": 115}, {"referenceID": 9, "context": "Our exploration algorithm has an interesting interpretation related to GANs (Goodfellow et al., 2014).", "startOffset": 76, "endOffset": 101}, {"referenceID": 26, "context": "We use TRPO (Schulman et al., 2015) for policy optimization, because it operates on both continuous and discrete action spaces, and due to its relative robustness to hyperparameter choices (Duan et al.", "startOffset": 12, "endOffset": 35}, {"referenceID": 7, "context": ", 2015) for policy optimization, because it operates on both continuous and discrete action spaces, and due to its relative robustness to hyperparameter choices (Duan et al., 2016).", "startOffset": 161, "endOffset": 180}, {"referenceID": 20, "context": "In fact, prior work on video prediction for Atari games easily achieves accurate predictions hundreds of frames into the future (Oh et al., 2015), while video prediction on natural images is challenging even a couple of frames into the future (Mathieu et al.", "startOffset": 128, "endOffset": 145}, {"referenceID": 18, "context": ", 2015), while video prediction on natural images is challenging even a couple of frames into the future (Mathieu et al., 2015).", "startOffset": 105, "endOffset": 127}, {"referenceID": 11, "context": "We compare the two variants of our method (K-exemplar and amortized) to standard random exploration, kernel density estimation (KDE) with RBF kernels, a method based on Bayesian neural network generative models called VIME (Houthooft et al., 2016), and exploration bonuses based on hashing of latent spaces learned via an autoencoder (Tang et al.", "startOffset": 223, "endOffset": 247}, {"referenceID": 30, "context": ", 2016), and exploration bonuses based on hashing of latent spaces learned via an autoencoder (Tang et al., 2016).", "startOffset": 94, "endOffset": 113}, {"referenceID": 11, "context": "Continuous Control: SwimmerGather and SparseHalfCheetah SwimmerGather and SparseHalfCheetah are two challenging continuous control tasks proposed by Houthooft et al. (2016). Both environments feature sparse reward and medium-dimensional observations (33 and 20 dimensions respectively).", "startOffset": 149, "endOffset": 173}, {"referenceID": 11, "context": "195 1 Houthooft et al. (2016) 2 Schulman et al.", "startOffset": 6, "endOffset": 30}, {"referenceID": 11, "context": "195 1 Houthooft et al. (2016) 2 Schulman et al. (2015) 3 Tang et al.", "startOffset": 6, "endOffset": 55}, {"referenceID": 11, "context": "195 1 Houthooft et al. (2016) 2 Schulman et al. (2015) 3 Tang et al. (2016)", "startOffset": 6, "endOffset": 76}, {"referenceID": 11, "context": "Table 1: Mean scores (higher is better) of our algorithm (both K-exemplar and amortized) versus VIME (Houthooft et al., 2016), baseline TRPO, Hashing, and kernel density estimation (KDE).", "startOffset": 101, "endOffset": 125}], "year": 2017, "abstractText": "Deep reinforcement learning algorithms have been shown to learn complex tasks using highly general policy classes. However, sparse reward problems remain a significant challenge. Exploration methods based on novelty detection have been particularly successful in such settings but typically require generative or predictive models of the observations, which can be difficult to train when the observations are very high-dimensional and complex, as in the case of raw images. We propose a novelty detection algorithm for exploration that is based entirely on discriminatively trained exemplar models, where classifiers are trained to discriminate each visited state against all others. Intuitively, novel states are easier to distinguish against other states seen during training. We show that this kind of discriminative modeling corresponds to implicit density estimation, and that it can be combined with countbased exploration to produce competitive results on a range of popular benchmark tasks, including state-of-the-art results on challenging egocentric observations in the vizDoom benchmark.", "creator": "LaTeX with hyperref package"}}}