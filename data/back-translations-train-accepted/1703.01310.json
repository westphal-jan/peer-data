{"id": "1703.01310", "review": {"conference": "ICML", "VERSION": "v1", "DATE_OF_SUBMISSION": "3-Mar-2017", "title": "Count-Based Exploration with Neural Density Models", "abstract": "Bellemare et al. (2016) introduced the notion of a pseudo-count to generalize count-based exploration to non-tabular reinforcement learning. This pseudo-count is derived from a density model which effectively replaces the count table used in the tabular setting. Using an exploration bonus based on this pseudo-count and a mixed Monte Carlo update applied to a DQN agent was sufficient to achieve state-of-the-art on the Atari 2600 game Montezuma's Revenge.", "histories": [["v1", "Fri, 3 Mar 2017 19:07:53 GMT  (2260kb,D)", "http://arxiv.org/abs/1703.01310v1", null], ["v2", "Wed, 14 Jun 2017 13:56:28 GMT  (2253kb,D)", "http://arxiv.org/abs/1703.01310v2", null]], "reviews": [], "SUBJECTS": "cs.AI", "authors": ["georg ostrovski", "marc g bellemare", "a\u00e4ron van den oord", "r\u00e9mi munos"], "accepted": true, "id": "1703.01310"}, "pdf": {"name": "1703.01310.pdf", "metadata": {"source": "META", "title": "Count-Based Exploration with Neural Density Models", "authors": ["Georg Ostrovski", "Marc G. Bellemare", "A\u00e4ron van den Oord", "R\u00e9mi Munos"], "emails": ["<ostrovski@google.com>."], "sections": [{"heading": null, "text": "In this paper, we look at two questions left open by their work: First, how important is the quality of the density model for exploration; second, what role does the Monte Carlo update play in exploration? We answer the first question by using PixelCNN, an advanced neural density model for images, to provide a pseudo-count count. In particular, we examine the intrinsic difficulties in adapting Bellemare et al. \"s approach when assumptions about the model are violated. The result is a more accurate and general algorithm that does not require any special apparatus. We combine PixelCNN pseudo-count with other agent architectures to dramatically improve the state of art on several hard Atari games. A surprising finding is that the mixed Monte Carlo update is a powerful promoter of exploration in the scantest of settings, including Montezuma's Replen.1. Induction is an exploration process learned through its environment."}], "references": [{"title": "Skip context tree switching", "author": ["Bellemare", "Marc", "Veness", "Joel", "Talvitie", "Erik"], "venue": "Proceedings of the International Conference on Machine Learning,", "citeRegEx": "Bellemare et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Bellemare et al\\.", "year": 2014}, {"title": "The arcade learning environment: An evaluation platform for general agents", "author": ["Bellemare", "Marc G", "Naddaf", "Yavar", "Veness", "Joel", "Bowling", "Michael"], "venue": "Journal of Artificial Intelligence Research,", "citeRegEx": "Bellemare et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Bellemare et al\\.", "year": 2013}, {"title": "Unifying count-based exploration and intrinsic motivation", "author": ["Bellemare", "Marc G", "Srinivasan", "Sriram", "Ostrovski", "Georg", "Schaul", "Tom", "Saxton", "David", "Munos", "R\u00e9mi"], "venue": "Advances in Neural Information Processing Systems,", "citeRegEx": "Bellemare et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Bellemare et al\\.", "year": 2016}, {"title": "PILCO: A model-based and data-efficient approach to policy search", "author": ["Deisenroth", "Marc P", "Rasmussen", "Carl E"], "venue": "In Proceedings of the International Conference on Machine Learning,", "citeRegEx": "Deisenroth et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Deisenroth et al\\.", "year": 2011}, {"title": "Catastrophic forgetting in connectionist networks", "author": ["French", "Robert M"], "venue": "Trends in cognitive sciences,", "citeRegEx": "French and M.,? \\Q1999\\E", "shortCiteRegEx": "French and M.", "year": 1999}, {"title": "Generative adversarial nets", "author": ["Goodfellow", "Ian", "Pouget-Abadie", "Jean", "Mirza", "Mehdi", "Xu", "Bing", "Warde-Farley", "David", "Ozair", "Sherjil", "Courville", "Aaron", "Bengio", "Yoshua"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Goodfellow et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Goodfellow et al\\.", "year": 2014}, {"title": "Draw: A recurrent neural network for image generation", "author": ["Gregor", "Karol", "Danihelka", "Ivo", "Graves", "Alex", "Rezende", "Danilo", "Wierstra", "Daan"], "venue": "In Proceedings of the International Conference on Machine Learning,", "citeRegEx": "Gregor et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Gregor et al\\.", "year": 2015}, {"title": "The Reactor: A sample-efficient actor-critic architecture", "author": ["Gruslys", "Audrunas", "Bellemare", "Marc G", "Azar", "Mohammad Gheshlaghi", "Munos", "R\u00e9mi"], "venue": "arXiv preprint,", "citeRegEx": "Gruslys et al\\.,? \\Q2017\\E", "shortCiteRegEx": "Gruslys et al\\.", "year": 2017}, {"title": "Efficient bayes-adaptive reinforcement learning using samplebased search", "author": ["Guez", "Arthur", "Silver", "David", "Dayan", "Peter"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Guez et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Guez et al\\.", "year": 2012}, {"title": "Variational information maximizing exploration", "author": ["Houthooft", "Rein", "Chen", "Xi", "Duan", "Yan", "Schulman", "John", "De Turck", "Filip", "Abbeel", "Pieter"], "venue": "In Advances in Neural Information Processing Systems (NIPS),", "citeRegEx": "Houthooft et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Houthooft et al\\.", "year": 2016}, {"title": "Nearoptimal regret bounds for reinforcement learning", "author": ["Jaksch", "Thomas", "Ortner", "Ronald", "Auer", "Peter"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Jaksch et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Jaksch et al\\.", "year": 2010}, {"title": "Auto-encoding variational bayes", "author": ["Kingma", "Diederik P", "Welling", "Max"], "venue": "In Proceedings of the International Conference on Learning Representations,", "citeRegEx": "Kingma et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Kingma et al\\.", "year": 2013}, {"title": "Exploration in model-based reinforcement learning by empirically estimating learning progress", "author": ["Lopes", "Manuel", "Lang", "Tobias", "Toussaint", "Marc", "Oudeyer", "Pierre-Yves"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Lopes et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Lopes et al\\.", "year": 2012}, {"title": "Safe and efficient off-policy reinforcement learning", "author": ["Munos", "R\u00e9mi", "Stepleton", "Tom", "Harutyunyan", "Anna", "Bellemare", "Marc G"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Munos et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Munos et al\\.", "year": 2016}, {"title": "Generalization and exploration via randomized value functions", "author": ["Osband", "Ian", "van Roy", "Benjamin", "Wen", "Zheng"], "venue": "In Proceedings of the International Conference on Machine Learning,", "citeRegEx": "Osband et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Osband et al\\.", "year": 2016}, {"title": "Intrinsic motivation systems for autonomous mental development", "author": ["Oudeyer", "Pierre-Yves", "Kaplan", "Fr\u00e9d\u00e9ric", "Hafner", "Verena V"], "venue": "IEEE Transactions on Evolutionary Computation,", "citeRegEx": "Oudeyer et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Oudeyer et al\\.", "year": 2007}, {"title": "Stochastic backpropagation and approximate inference in deep generative models", "author": ["Rezende", "Danilo Jimenez", "Mohamed", "Shakir", "Wierstra", "Daan"], "venue": "In Proceedings of The International Conference on Machine Learning,", "citeRegEx": "Rezende et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Rezende et al\\.", "year": 2014}, {"title": "An analysis of model-based interval estimation for Markov decision processes", "author": ["Strehl", "Alexander L", "Littman", "Michael L"], "venue": "Journal of Computer and System Sciences,", "citeRegEx": "Strehl et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Strehl et al\\.", "year": 2008}, {"title": "Generalization in reinforcement learning: Successful examples using sparse coarse coding", "author": ["Sutton", "Richard S"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Sutton and S.,? \\Q1996\\E", "shortCiteRegEx": "Sutton and S.", "year": 1996}, {"title": "Reinforcement learning: An introduction", "author": ["Sutton", "Richard S", "Barto", "Andrew G"], "venue": null, "citeRegEx": "Sutton et al\\.,? \\Q1998\\E", "shortCiteRegEx": "Sutton et al\\.", "year": 1998}, {"title": "Exploration: A study of count-based exploration for deep reinforcement learning", "author": ["Filip", "Abbeel", "Pieter"], "venue": "arXiv preprint arXiv:1611.04717,", "citeRegEx": "Filip et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Filip et al\\.", "year": 2016}, {"title": "A note on the evaluation of generative models", "author": ["Theis", "Lucas", "van den Oord", "A\u00e4ron", "Bethge", "Matthias"], "venue": "In Proceedings of the International Conference on Learning Representations,", "citeRegEx": "Theis et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Theis et al\\.", "year": 2016}, {"title": "RMSProp: divide the gradient by a running average of its recent magnitude. COURSERA", "author": ["Tieleman", "Tijmen", "Hinton", "Geoffrey"], "venue": "Neural Networks for Machine Learning,", "citeRegEx": "Tieleman et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Tieleman et al\\.", "year": 2012}, {"title": "An analysis of temporal-difference learning with function approximation", "author": ["Tsitsiklis", "John N", "van Roy", "Benjamin"], "venue": "IEEE Transactions on Automatic Control,", "citeRegEx": "Tsitsiklis et al\\.,? \\Q1997\\E", "shortCiteRegEx": "Tsitsiklis et al\\.", "year": 1997}, {"title": "Conditional image generation with PixelCNN decoders", "author": ["van den Oord", "Aaron", "Kalchbrenner", "Nal", "Espeholt", "Lasse", "Vinyals", "Oriol", "Graves", "Alex"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Oord et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Oord et al\\.", "year": 2016}, {"title": "Pixel recurrent neural networks", "author": ["van den Oord", "Aaron", "Kalchbrenner", "Nal", "Kavukcuoglu", "Koray"], "venue": "In Proceedings of the International Conference on Machine Learning,", "citeRegEx": "Oord et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Oord et al\\.", "year": 2016}, {"title": "Deep reinforcement learning with Double Q-learning", "author": ["van Hasselt", "Hado", "Guez", "Arthur", "Silver", "David"], "venue": "In Proceedings of the AAAI Conference on Artificial Intelligence,", "citeRegEx": "Hasselt et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Hasselt et al\\.", "year": 2016}, {"title": "Context tree switching", "author": ["Veness", "Joel", "Ng", "Kee Siong", "Hutter", "Marcus", "Bowling", "Michael H"], "venue": "In Proceedings of the Data Compression Conference,", "citeRegEx": "Veness et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Veness et al\\.", "year": 2012}, {"title": "Dueling network architectures for deep reinforcement learning", "author": ["Wang", "Ziyu", "Schaul", "Tom", "Hessel", "Matteo", "van Hasselt", "Hado", "Lanctot", "Marc", "de Freitas", "Nando"], "venue": "In Proceedings of The 33rd International Conference on Machine Learning,", "citeRegEx": "Wang et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Wang et al\\.", "year": 2016}, {"title": "The Hardest Exploration Games Table 1 reproduces Bellemare et al. (2016)\u2019s taxonomy of games available through the ALE according to their exploration difficulty. \u201cHuman-Optimal", "author": ["D. exploration"], "venue": null, "citeRegEx": "exploration.,? \\Q2016\\E", "shortCiteRegEx": "exploration.", "year": 2016}], "referenceMentions": [{"referenceID": 10, "context": "From a theoretical perspective, exploration is now wellunderstood (e.g. Strehl & Littman, 2008; Jaksch et al., 2010; Osband et al., 2016), and Bayesian methods have Google DeepMind, London, UK.", "startOffset": 66, "endOffset": 137}, {"referenceID": 14, "context": "From a theoretical perspective, exploration is now wellunderstood (e.g. Strehl & Littman, 2008; Jaksch et al., 2010; Osband et al., 2016), and Bayesian methods have Google DeepMind, London, UK.", "startOffset": 66, "endOffset": 137}, {"referenceID": 8, "context": "been successfully demonstrated in a number of settings (Deisenroth & Rasmussen, 2011; Guez et al., 2012).", "startOffset": 55, "endOffset": 104}, {"referenceID": 0, "context": "As a practical application the authors used the pseudocounts derived from the simple CTS density model (Bellemare et al., 2014) to incentivize exploration in Atari 2600 agents.", "startOffset": 103, "endOffset": 127}, {"referenceID": 0, "context": "Bellemare 1 A\u00e4ron van den Oord 1 R\u00e9mi Munos 1 Abstract Bellemare et al. (2016) introduced the notion of a pseudo-count to generalize count-based exploration to non-tabular reinforcement learning.", "startOffset": 55, "endOffset": 79}, {"referenceID": 0, "context": "Bellemare 1 A\u00e4ron van den Oord 1 R\u00e9mi Munos 1 Abstract Bellemare et al. (2016) introduced the notion of a pseudo-count to generalize count-based exploration to non-tabular reinforcement learning. This pseudo-count is derived from a density model which effectively replaces the count table used in the tabular setting. Using an exploration bonus based on this pseudo-count and a mixed Monte Carlo update applied to a DQN agent was sufficient to achieve state-of-the-art on the Atari 2600 game Montezuma\u2019s Revenge. In this paper we consider two questions left open by their work: First, how important is the quality of the density model for exploration? Second, what role does the Monte Carlo update play in exploration? We answer the first question by demonstrating the use of PixelCNN, an advanced neural density model for images, to supply a pseudo-count. In particular, we examine the intrinsic difficulties in adapting Bellemare et al.\u2019s approach when assumptions about the model are violated. The result is a more practical and general algorithm requiring no special apparatus. We combine PixelCNN pseudo-counts with different agent architectures to dramatically improve the state of the art on several hard Atari games. One surprising finding is that the mixed Monte Carlo update is a powerful facilitator of exploration in the sparsest of settings, including Montezuma\u2019s Revenge. 1. Introduction Exploration is the process by which an agent learns about its environment. In the reinforcement learning framework, this involves reducing the agent\u2019s uncertainty about the environment\u2019s transition dynamics and attainable rewards. From a theoretical perspective, exploration is now wellunderstood (e.g. Strehl & Littman, 2008; Jaksch et al., 2010; Osband et al., 2016), and Bayesian methods have Google DeepMind, London, UK. Correspondence to: Georg Ostrovski <ostrovski@google.com>. been successfully demonstrated in a number of settings (Deisenroth & Rasmussen, 2011; Guez et al., 2012). On the other hand, practical algorithms for the general case remain scarce; fully Bayesian approaches are usually intractable in large state spaces, and the count-based method typical of theoretical results is not applicable in the presence of value function approximation. Recently, Bellemare et al. (2016) proposed the notion of pseudo-count as a reasonable generalization of the tabular setting considered in the theory literature.", "startOffset": 55, "endOffset": 2300}, {"referenceID": 2, "context": "Pseudo-Count and Prediction Gain Here we briefly introduce notation and results, referring the reader to (Bellemare et al., 2016) for technical details.", "startOffset": 105, "endOffset": 129}, {"referenceID": 12, "context": "Quantities related to prediction gain have been used for similar purposes in the intrinsic motivation literature (Lopes et al., 2012), where they measure an agent\u2019s learning progress (Oudeyer et al.", "startOffset": 113, "endOffset": 133}, {"referenceID": 15, "context": ", 2012), where they measure an agent\u2019s learning progress (Oudeyer et al., 2007).", "startOffset": 57, "endOffset": 79}, {"referenceID": 0, "context": "Density Models for Images The CTS density model (Bellemare et al., 2014) is based on the namesake algorithm, Context Tree Switching (Veness et al.", "startOffset": 48, "endOffset": 72}, {"referenceID": 27, "context": ", 2014) is based on the namesake algorithm, Context Tree Switching (Veness et al., 2012), a Bayesian variable-order Markov model.", "startOffset": 67, "endOffset": 88}, {"referenceID": 16, "context": "In recent years, neural generative models for images have achieved impressive successes in their ability to generate diverse images in various domains (Kingma & Welling, 2013; Rezende et al., 2014; Gregor et al., 2015; Goodfellow et al., 2014).", "startOffset": 151, "endOffset": 243}, {"referenceID": 6, "context": "In recent years, neural generative models for images have achieved impressive successes in their ability to generate diverse images in various domains (Kingma & Welling, 2013; Rezende et al., 2014; Gregor et al., 2015; Goodfellow et al., 2014).", "startOffset": 151, "endOffset": 243}, {"referenceID": 5, "context": "In recent years, neural generative models for images have achieved impressive successes in their ability to generate diverse images in various domains (Kingma & Welling, 2013; Rezende et al., 2014; Gregor et al., 2015; Goodfellow et al., 2014).", "startOffset": 151, "endOffset": 243}, {"referenceID": 15, "context": "Count-Based Exploration with Neural Density Models In particular, we explore the use of PixelCNN (van den Oord et al., 2016b;a), a state-of-the-art neural density model. We examine the challenges posed by this approach: Model choice. Performing two evaluations and one model update at each agent step (to compute \u03c1(x) and \u03c1\u2032(x)) can be prohibitively expensive. This requires the design of a simplified \u2013 yet sufficiently expressive and accurate \u2013 PixelCNN architecture. Model training. A CTS model can naturally be trained from sequentially presented, correlated data samples. Training a neural model in this online fashion requires more careful attention to the optimization procedure to prevent overfitting and catastrophic forgetting (French, 1999). Model use. The theory of pseudo-counts requires the density model\u2019s rate of learning to decay over time. Optimization of a neural model, however, imposes constraints on the step-size regime which cannot be violated without deteriorating effectiveness and stability of training. The concept of intrinsic motivation has made a recent resurgence in reinforcement learning research, in great part due to a dissatisfaction with -greedy and Boltzmann policies. Of note, Tang et al. (2016) maintain an approximate count by means of hash tables over features, which in the pseudo-count framework corresponds to a hash-based density model.", "startOffset": 106, "endOffset": 1236}, {"referenceID": 4, "context": "Houthooft et al. (2016) used a second-order Taylor approximation of the prediction gain to drive exploration in continuous control.", "startOffset": 0, "endOffset": 24}, {"referenceID": 0, "context": "Pseudo-Count and Prediction Gain Here we briefly introduce notation and results, referring the reader to (Bellemare et al., 2016) for technical details. Let \u03c1 be a density model on a finite space X , and \u03c1n(x) the probability assigned by the model to x after being trained on a sequence of states x1, . . . , xn. Assume \u03c1n(x) > 0 for all x, n. The recoding probability \u03c1n(x) is then the probability the model would assign to x if it were trained on that same x one more time. We call \u03c1 learning-positive if \u03c1n(x) \u2265 \u03c1n(x) for all x1, . . . , xn, x \u2208 X . The prediction gain (PG) of \u03c1 is PGn(x) = log \u03c1 \u2032 n(x)\u2212 log \u03c1n(x). (1) A learning-positive \u03c1 implies PGn(x) \u2265 0 for all x \u2208 X . For learning-positive \u03c1, we define the pseudo-count as N\u0302n(x) = \u03c1n(x)(1\u2212 \u03c1n(x)) \u03c1n(x)\u2212 \u03c1n(x) , derived from postulating that a single observation of x \u2208 X should lead to a unit increase in pseudo-count: \u03c1n(x) = N\u0302n(x) n\u0302 , \u03c1n(x) = N\u0302n(x) + 1 n\u0302+ 1 , where n\u0302 is the pseudo-count total. The pseudo-count generalizes the usual state visitation count function Nn(x). Under certain assumptions on \u03c1n, pseudo-counts grow approximately linearly with real counts. Crucially, the pseudo-count can be approximated using the prediction gain of the density model: N\u0302n(x) \u2248 ( en \u2212 1 )\u22121 . Its main use is to define an exploration bonus. We consider a reinforcement learning (RL) agent interacting with an environment that provides observations and extrinsic rewards (see Sutton & Barto, 1998, for a thorough exposition of the RL framework). To the reward at step n we add the bonus r(x) := (N\u0302n(x)) \u22121/2, which incentivizes the agent to try to re-experience surprising situations. Quantities related to prediction gain have been used for similar purposes in the intrinsic motivation literature (Lopes et al., 2012), where they measure an agent\u2019s learning progress (Oudeyer et al., 2007). Although the pseudo-count bonus is close to the prediction gain, it is asymptotically more conservative and supported by stronger theoretical guarantees. 2.2. Density Models for Images The CTS density model (Bellemare et al., 2014) is based on the namesake algorithm, Context Tree Switching (Veness et al., 2012), a Bayesian variable-order Markov model. In its simplest form, the model takes as input a 2D image and assigns to it a probability according to the product of location-dependent L-shaped filters, where the prediction of each filter is given by a CTS algorithm trained on past images. In Bellemare et al. (2016), this model was applied to 3-bit greyscale, 42 \u00d7 42 downsampled Atari 2600 frames (Fig.", "startOffset": 106, "endOffset": 2481}, {"referenceID": 2, "context": "Atari frame preprocessing (Bellemare et al., 2016).", "startOffset": 26, "endOffset": 50}, {"referenceID": 13, "context": "A better multi-step method is the recent Retrace(\u03bb) algorithm (Munos et al., 2016).", "startOffset": 62, "endOffset": 82}, {"referenceID": 1, "context": "We investigate how to best resolve these tensions in the context of the Arcade Learning Environment (Bellemare et al., 2013), a suite of benchmark Atari 2600 games.", "startOffset": 100, "endOffset": 124}, {"referenceID": 0, "context": "To achieve their success on the hardest Atari 2600 games, Bellemare et al. (2016) used the mixed Monte-Carlo update (MMC) Q(x, a)\u2190 Q(x, a) + \u03b1 [(1\u2212 \u03b2)\u03b4(x, a) + \u03b2\u03b4MC(x, a)] , with \u03b2 \u2208 [0, 1].", "startOffset": 58, "endOffset": 82}, {"referenceID": 2, "context": "The DQN-CTS agent we compare against is derived from the one in (Bellemare et al., 2016).", "startOffset": 64, "endOffset": 88}, {"referenceID": 7, "context": "To demonstrate the wide applicability of the PixelCNN exploration bonus, we also evaluate it with the more recent Reactor agent3 (Gruslys et al., 2017).", "startOffset": 129, "endOffset": 151}, {"referenceID": 21, "context": "However, we are not using the generative function of the models when computing an exploration bonus, and a better generative model does not necessarily give rise to better probability estimates (Theis et al., 2016).", "startOffset": 194, "endOffset": 214}, {"referenceID": 0, "context": "The Hardest Exploration Games Table 1 reproduces Bellemare et al. (2016)\u2019s taxonomy of games available through the ALE according to their exploration difficulty.", "startOffset": 49, "endOffset": 73}, {"referenceID": 2, "context": ", 2015), A3C-CTS (\u201cA3C+\u201d in (Bellemare et al., 2016)), Prioritized Dueling DQN (Wang et al.", "startOffset": 28, "endOffset": 52}, {"referenceID": 28, "context": ", 2016)), Prioritized Dueling DQN (Wang et al., 2016), and the basic versions of DQN-CTS and DQN-PixelCNN from Section 4.", "startOffset": 34, "endOffset": 53}], "year": 2017, "abstractText": "Bellemare et al. (2016) introduced the notion of a pseudo-count to generalize count-based exploration to non-tabular reinforcement learning. This pseudo-count is derived from a density model which effectively replaces the count table used in the tabular setting. Using an exploration bonus based on this pseudo-count and a mixed Monte Carlo update applied to a DQN agent was sufficient to achieve state-of-the-art on the Atari 2600 game Montezuma\u2019s Revenge.", "creator": "LaTeX with hyperref package"}}}