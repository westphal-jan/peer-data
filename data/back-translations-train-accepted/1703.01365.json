{"id": "1703.01365", "review": {"conference": "ICML", "VERSION": "v1", "DATE_OF_SUBMISSION": "4-Mar-2017", "title": "Axiomatic Attribution for Deep Networks", "abstract": "We study the problem of attributing the prediction of a deep network to its input features, a problem previously studied by several other works. We identify two fundamental axioms---Sensitivity and Implementation Invariance that attribution methods ought to satisfy. We show that they are not satisfied by most known attribution methods, which we consider to be a fundamental weakness of those methods. We use the axioms to guide the design of a new attribution method called Integrated Gradients. Our method requires no modification to the original network and is extremely simple to implement; it just needs a few calls to the standard gradient operator. We apply this method to a couple of image models, a couple of text models and a chemistry model, demonstrating its ability to debug networks, to extract rules from a deep network, and to enable users to engage with models better.", "histories": [["v1", "Sat, 4 Mar 2017 00:18:49 GMT  (5478kb,D)", "http://arxiv.org/abs/1703.01365v1", null], ["v2", "Tue, 13 Jun 2017 01:52:38 GMT  (7018kb,D)", "http://arxiv.org/abs/1703.01365v2", null]], "reviews": [], "SUBJECTS": "cs.LG", "authors": ["mukund sundararajan", "ankur taly", "qiqi yan"], "accepted": true, "id": "1703.01365"}, "pdf": {"name": "1703.01365.pdf", "metadata": {"source": "META", "title": "Axiomatic Attribution for Deep Networks", "authors": ["Mukund Sundararajan", "Ankur Taly", "Qiqi Yan"], "emails": ["MUKUNDS@GOOGLE.COM", "ATALY@GOOGLE.COM", "QIQIYAN@GOOGLE.COM"], "sections": [{"heading": "1. Motivation and Summary of Results", "text": "In fact, most of them are able to play by the rules that they play by the rules."}, {"heading": "2. Two Fundamental Axioms", "text": "We are now discussing two axioms (desirable properties) for mapping methods. We note that other methods of mapping features in the literature break at least one of the two axioms: DeepLift (Shrikumar et al., 2016), Layer-wise relevance propagation (LRP) (Binder et al., 2016), Deconvolutional networks (Zeiler & Fergus, 2014), and Guided back-propagation (Springenberg et al., 2014). As we will see in Section 3, these axioms also guide the design of our method. Gradients. In linear models, ML practitioners regularly inspect the coefficients of the model to debug it. Gradients (of output in relation to input) are a natural analogy of these coefficients for a deep network and therefore represent a reasonable starting point for a mapping method (Baehrens et al., 2010; Simonyan et al., 2013)."}, {"heading": "2.1. Axiom: Sensitivity(a)", "text": "In fact, the majority of people who are able to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to fight, to fight, to fight, to fight, to fight, to fight, to move, to move, to move, to fight, to fight, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move"}, {"heading": "2.2. Axiom: Implementation Invariance", "text": "Two networks are functionally equivalent if their results are the same for all inputs, even though they have very different implementations. (Attribution methods should satisfy the implementation invariance, i.e. the attributions are always identical for two functionally equivalent networks.) To motivate this, note that attributions can be colloquially defined as assigning the debt (or credit) to the input characteristics. Such a definition does not refer to implementation details. We will now discuss the intuition for why DeepLift and LRP BreakImplementation Invariance; a concrete example is provided in Appendix B. First, note that gradients are invariant for implementation. Indeed, the chain rule for gradients is f projected."}, {"heading": "3. Our Method: Integrated Gradients", "text": "We are ready to describe our technique. Intuitively, our technique combines the implementation of gradients with the sensitivity of techniques such as LRP or DeepLift. Formally, we have a function that represents a deep network. In particular, we look at the straight path (in Rn) from baseline x to baseline x, and calculate the gradients at all points along the path. The integrated gradients are achieved by accumulating these gradients. Specifically, integrated gradients are defined as paths that define the gradients along the straight lines along the straight lines. Integrated gradients are defined as paths between the gradients along the straight lines."}, {"heading": "4. Uniqueness of Integrated Gradients", "text": "Previous literature has been based on empirical evaluation of the mapping technique. For example, in an object recognition task (Samek et al., 2015), we suggest that we select the uppermost k-pixels by mapping and vary their intensity randomly, and then measure the decline in score. If the mapping method is good, then the decrease in score should be large. However, the images resulting from the pixel error may be unnatural, and it may be that the scores drop simply because the network has never seen anything like it in training. (It is probably less about linear or logistical models, where the simplicity of the model ensures that the removal of a feature does not cause strange interactions.) Another evaluation technique looks at images with human-drawn bounding boxes around objects and calculates the percentage of pixel mapping within the bounding box."}, {"heading": "4.1. Path Methods", "text": "Integrated gradients aggregate the gradients along the inputs that fall on the linearity between the baseline and the input. Clearly, there are many other (non-rectilinear) paths that monotonously interpolate between the two points, and each such path will produce a different mapping method. Consider, for example, the simple case where the input is two-dimensional. Figure 1 has examples of three paths, each corresponding to a different mapping method. Formally, let it come to a different mapping method.: [0, 1] \u2192 Rn be a smooth function specifying a path in Rn from the baseline x \u2032 to the input x, i.e..."}, {"heading": "4.2. Integrated gradients is Symmetry-Preserving", "text": "In this section, we will formalize why the rectilinear path chosen by integrated gradients is canonical. First, we observe that it is the simplest path that can be defined mathematically. Second, a natural property of assignment methods is to obtain symmetry in the following sense.Symmetry conservation. Two input variables are symmetrical if their swapping does not change the function. For example, x and y are symmetrical w.r.t. F if and only if F (x, y) = F (y, x) for all values of x and y. An assignment method is symmetrical if for all inputs that have identical values for symmetrical variables and baselines, identical values for symmetrical variables that have identical attributes. For example, consider the logistic model Sigmoid (x1 + x2 +)."}, {"heading": "5. Applications", "text": "The integrated gradient technique can be applied to a variety of deep networks. As proof of concept, we apply the technique to two image models, two models of natural language and one chemical model."}, {"heading": "5.1. An Object Recognition Network", "text": "We examine the mapping in an object recognition network built using the GoogleNet architecture (Szegedy et al., 2014) and trained using the ImageNet object recognition dataset (Russakovsky et al., 2015). We use the built-in gradient method to examine the meaning of pixels in predictions made by this network. Gradients are calculated for the output of the highest scoring class in relation to pixels of the input image. Basically, the gradients are the black image, i.e. all pixel intensities are zero. Integrated gradients essentially aggregate the gradients of images obtained by interpolating between the original image and this black image. Integrated gradients can be visualized by aggregating along the color channel and scaling the pixels in the actual image."}, {"heading": "5.2. Diabetic retinopathy prediction", "text": "Diabetic retinopathy (DR) is a complication of diabetes that affects the eyes. Recently, a deep network was proposed (V et al., 2016) to predict the severity of DR in the fundus images of the retina. The model has good predictive accuracy on various validation datasets. We use integrated gradients to investigate the significance of features for this network; as in the case of object recognition, the baseline is the black image. Explanations of the importance of features are important for this network as they are available to retinal specialists to promote clinical acceptance. Retinal specialists can use them to build confidence in the predictions of the network, determine the degree of boundary cases, and obtain insights for further testing and screenings.Figure 3 shows a visualization of integrated gradients for a fundus image of the retina. Visualization is achieved by integrating the integrated gradient scales into the study and appearing to be located along the actual gradient of the study as a clue of the gradient that some gradient levels appear to be integrated into the study."}, {"heading": "5.3. Question classification", "text": "A common approach is to semantically analyze the question in its logical form (Liang, 2016; Pasupat & Liang, 2015) with a set of man-made grammar rules, the downside being that it is harder to remember, because there are often multiple formulations of the same question in natural language, such as how many people work at Walmart."}, {"heading": "5.4. Penn Treebank model", "text": "We apply our technique to linguistic modeling at the word level of the Penn Treebank dataset (Marcus et al., 1993) and apply a sequence model based on LSTM (Zaremba et al., 2014). For such a network, we select 20 randomly selected sections of the test data and check the prediction value of the next word for each of the first 10 words. Here, the baseline is reached by zeros of the embedded vectors, just like the question classification model. In Table 5, we show a comparison of the integrated gradients with the gradients. Due to saturation, the orders of magnitude of the gradients are so small compared to the forecast values that it is difficult to understand them. In comparison, integrated gradients have an overall value that is closest to the forecast value, so that \"the second most common prediction is unambiguous.\""}, {"heading": "5.5. Chemistry Models", "text": "This year, we will be able to put ourselves at the top, \"he said in an interview with\" Welt am Sonntag. \""}, {"heading": "6. Other Related work", "text": "In recent years, there has been a lot of work on demystifying the inner workings of deep networks. Most of this work concerned networks trained in computer vision tasks, and deals with understanding what a particular neuron calculates (Erhan et al., 2009; Le, 2013) and interpreting the representations captured by neurons during a prediction (Mahendran & Vedaldi, 2015; Dosovitskiy & Brox, 2015; Yosinski et al., 2015). In contrast, we focus on understanding the behavior of the network based on a specific input in relation to the input characteristics at the base level. Our technique quantifies the significance of each feature in the prediction. An approach to the mapping problem first proposed (Ribeiro et al., 2016a; b) is to describe the behavior of the network near the input, which is explained, using a simpler, more interpretative model."}, {"heading": "7. Conclusion", "text": "The primary contribution of this paper is a method called Integrated Gradients, which ascribes the prediction of a deep network to its inputs. It can be implemented with a few calls to the gradient operator, can be applied to a variety of deep networks, and has a strong theoretical justification. A secondary contribution of this paper is to clarify desirable features of a mapping method using an axiomatic framework inspired by the cost-sharing literature from economics. Without the axiomatic approach, it is difficult to say whether the mapping method is influenced by data artefacts, network artefacts, or artefacts of the method. The axiomatic approach excludes artefacts of the last font. While our and other work have made some progress in understanding the relative importance of input functions in a deep network, we have not dealt with the interactions between the input functions or the logic used by the network. Thus, many questions remain unanswered regarding the debiting I / O."}, {"heading": "ACKNOWLEDGMENTS", "text": "We thank Patrick Riley and Christian Szegedy for their helpful feedback."}, {"heading": "A. Proof of Theorem 1", "text": "Proof. Let us consider a non-linear path \u03b3: [0, 1] \u2192 Rn from the baseline to the input. W.l.o.g. There is t0 [0, 1] so that for two dimensions i, j, \u03b3i (t0) > \u03b3j (t0), let us (t1, t2) be the maximum real open interval containing t0 so that \u03b3i (t) > \u03b3j (t) for all t in (t1, t2), and let us let a = \u03b3i (t1) = \u03b3j (t1) and b = \u03b3i (t2) = \u03b3j (t2), define the function f: x [0, 1] n \u2192 R as 0 if min (xi, xj) \u2264 a, as (b \u2212 a) 2 if max (xi, xj) = \u03b3i (t1), and as (xi \u2212 a). Next, let us calculate the attributions of f = < < < < and the baseline < < < and;"}, {"heading": "B. Attribution Counter-Examples", "text": "We show that the DeepLift and Layer-wise Relevance Propagation (LRP) methods break the implementation invariance axiom, and the DeepLift and Guided Repropagation methods break the sensitivity axiom. Figure 7 provides an example of two equivalent networks f (x1, x2) and g (x1, x2) for which DeepLift and Guided Repropagation yield different attributions. First, note that the networks f (x1, x2) = ReLU (h (x1, x2) and f (x1, x2) = ReLU (x1, x2) = ReLU (x2) = ReLU (x1, x2) = ReLU (x1) \u2212 ReLU."}], "references": [{"title": "Values of Non-Atomic Games", "author": ["R.J. Aumann", "L.S. Shapley"], "venue": null, "citeRegEx": "Aumann and Shapley,? \\Q1974\\E", "shortCiteRegEx": "Aumann and Shapley", "year": 1974}, {"title": "How to explain individual classification decisions", "author": ["Baehrens", "David", "Schroeter", "Timon", "Harmeling", "Stefan", "Kawanabe", "Motoaki", "Hansen", "Katja", "M\u00fcller", "KlausRobert"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Baehrens et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Baehrens et al\\.", "year": 2010}, {"title": "Layerwise relevance propagation for neural networks with local renormalization layers", "author": ["Binder", "Alexander", "Montavon", "Gr\u00e9goire", "Bach", "Sebastian", "M\u00fcller", "Klaus-Robert", "Samek", "Wojciech"], "venue": null, "citeRegEx": "Binder et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Binder et al\\.", "year": 2016}, {"title": "Inverting visual representations with convolutional networks", "author": ["Dosovitskiy", "Alexey", "Brox", "Thomas"], "venue": null, "citeRegEx": "Dosovitskiy et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Dosovitskiy et al\\.", "year": 2015}, {"title": "Visualizing higher-layer features of a deep network", "author": ["Erhan", "Dumitru", "Bengio", "Yoshua", "Courville", "Aaron", "Vincent", "Pascal"], "venue": "Technical Report 1341,", "citeRegEx": "Erhan et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Erhan et al\\.", "year": 2009}, {"title": "Paths and consistency in additive cost sharing", "author": ["Friedman", "Eric J"], "venue": "International Journal of Game Theory,", "citeRegEx": "Friedman and J.,? \\Q2004\\E", "shortCiteRegEx": "Friedman and J.", "year": 2004}, {"title": "Molecular graph convolutions: moving beyond fingerprints", "author": ["Kearnes", "Steven", "McCloskey", "Kevin", "Berndl", "Marc", "Pande", "Vijay", "Riley", "Patrick"], "venue": "Journal of Computer-Aided Molecular Design,", "citeRegEx": "Kearnes et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Kearnes et al\\.", "year": 2016}, {"title": "Convolutional neural networks for sentence classification", "author": ["Kim", "Yoon"], "venue": "In ACL,", "citeRegEx": "Kim and Yoon.,? \\Q2014\\E", "shortCiteRegEx": "Kim and Yoon.", "year": 2014}, {"title": "Building high-level features using large scale unsupervised learning", "author": ["Le", "Quoc V"], "venue": "In International Conference on Acoustics, Speech, and Signal Processing (ICASSP),", "citeRegEx": "Le and V.,? \\Q2013\\E", "shortCiteRegEx": "Le and V.", "year": 2013}, {"title": "Learning executable semantic parsers for natural language understanding", "author": ["Liang", "Percy"], "venue": "Commun. ACM,", "citeRegEx": "Liang and Percy.,? \\Q2016\\E", "shortCiteRegEx": "Liang and Percy.", "year": 2016}, {"title": "Understanding deep image representations by inverting them", "author": ["Mahendran", "Aravindh", "Vedaldi", "Andrea"], "venue": "In Conference on Computer Vision and Pattern Recognition (CVPR),", "citeRegEx": "Mahendran et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Mahendran et al\\.", "year": 2015}, {"title": "Building a large annotated corpus of english: The penn treebank", "author": ["Marcus", "Mitchell P", "Santorini", "Beatrice", "Marcinkiewicz", "Mary Ann"], "venue": "Computational Linguistics,", "citeRegEx": "Marcus et al\\.,? \\Q1993\\E", "shortCiteRegEx": "Marcus et al\\.", "year": 1993}, {"title": "Compositional semantic parsing on semi-structured tables", "author": ["Pasupat", "Panupong", "Liang", "Percy"], "venue": "In ACL,", "citeRegEx": "Pasupat et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Pasupat et al\\.", "year": 2015}, {"title": "why should I trust you?\u201d: Explaining the predictions of any classifier", "author": ["Ribeiro", "Marco T\u00falio", "Sameer Singh", "Guestrin", "Carlos"], "venue": "In 22nd ACM International Conference on Knowledge Discovery and Data Mining,", "citeRegEx": "Ribeiro et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Ribeiro et al\\.", "year": 2016}, {"title": "Model-agnostic interpretability of machine learning", "author": ["Ribeiro", "Marco T\u00falio", "Sameer Singh", "Guestrin", "Carlos"], "venue": null, "citeRegEx": "Ribeiro et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Ribeiro et al\\.", "year": 2016}, {"title": "Evaluating the visualization of what a deep neural network has learned", "author": ["Samek", "Wojciech", "Binder", "Alexander", "Montavon", "Gr\u00e9goire", "Bach", "Sebastian", "M\u00fcller", "Klaus-Robert"], "venue": null, "citeRegEx": "Samek et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Samek et al\\.", "year": 2015}, {"title": "Not just a black box: Learning important features through propagating activation differences", "author": ["Shrikumar", "Avanti", "Greenside", "Peyton", "Shcherbina", "Anna", "Kundaje", "Anshul"], "venue": null, "citeRegEx": "Shrikumar et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Shrikumar et al\\.", "year": 2016}, {"title": "Deep inside convolutional networks: Visualising image classification models and saliency", "author": ["Simonyan", "Karen", "Vedaldi", "Andrea", "Zisserman", "Andrew"], "venue": null, "citeRegEx": "Simonyan et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Simonyan et al\\.", "year": 2013}, {"title": "Striving for simplicity: The all convolutional net", "author": ["Springenberg", "Jost Tobias", "Dosovitskiy", "Alexey", "Brox", "Thomas", "Riedmiller", "Martin A"], "venue": null, "citeRegEx": "Springenberg et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Springenberg et al\\.", "year": 2014}, {"title": "Development and validation of a deep learning algorithm for detection of diabetic retinopathy in retinal fundus", "author": ["L Gulshan", "M Peng", "Coram"], "venue": "photographs. JAMA,", "citeRegEx": "V et al\\.,? \\Q2016\\E", "shortCiteRegEx": "V et al\\.", "year": 2016}, {"title": "Understanding neural networks through deep visualization", "author": ["Yosinski", "Jason", "Clune", "Jeff", "Nguyen", "Anh Mai", "Fuchs", "Thomas", "Lipson", "Hod"], "venue": null, "citeRegEx": "Yosinski et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Yosinski et al\\.", "year": 2015}, {"title": "Recurrent neural network regularization", "author": ["Zaremba", "Wojciech", "Sutskever", "Ilya", "Vinyals", "Oriol"], "venue": null, "citeRegEx": "Zaremba et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Zaremba et al\\.", "year": 2014}, {"title": "Visualizing and understanding convolutional networks", "author": ["Zeiler", "Matthew D", "Fergus", "Rob"], "venue": "In ECCV,", "citeRegEx": "Zeiler et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Zeiler et al\\.", "year": 2014}], "referenceMentions": [{"referenceID": 1, "context": "The attribution problem was previously studied by various papers (Baehrens et al., 2010; Simonyan et al., 2013; Shrikumar et al., 2016; Binder et al., 2016; Springenberg et al., 2014).", "startOffset": 65, "endOffset": 183}, {"referenceID": 17, "context": "The attribution problem was previously studied by various papers (Baehrens et al., 2010; Simonyan et al., 2013; Shrikumar et al., 2016; Binder et al., 2016; Springenberg et al., 2014).", "startOffset": 65, "endOffset": 183}, {"referenceID": 16, "context": "The attribution problem was previously studied by various papers (Baehrens et al., 2010; Simonyan et al., 2013; Shrikumar et al., 2016; Binder et al., 2016; Springenberg et al., 2014).", "startOffset": 65, "endOffset": 183}, {"referenceID": 2, "context": "The attribution problem was previously studied by various papers (Baehrens et al., 2010; Simonyan et al., 2013; Shrikumar et al., 2016; Binder et al., 2016; Springenberg et al., 2014).", "startOffset": 65, "endOffset": 183}, {"referenceID": 18, "context": "The attribution problem was previously studied by various papers (Baehrens et al., 2010; Simonyan et al., 2013; Shrikumar et al., 2016; Binder et al., 2016; Springenberg et al., 2014).", "startOffset": 65, "endOffset": 183}, {"referenceID": 16, "context": "The need for a baseline has also been pointed out by prior work on attribution (Shrikumar et al., 2016; Binder et al., 2016).", "startOffset": 79, "endOffset": 124}, {"referenceID": 2, "context": "The need for a baseline has also been pointed out by prior work on attribution (Shrikumar et al., 2016; Binder et al., 2016).", "startOffset": 79, "endOffset": 124}, {"referenceID": 16, "context": "These methods include DeepLift (Shrikumar et al., 2016), Layer-wise relevance propagation (LRP) (Binder et al.", "startOffset": 31, "endOffset": 55}, {"referenceID": 2, "context": ", 2016), Layer-wise relevance propagation (LRP) (Binder et al., 2016), Deconvolutional networks (Zeiler & Fergus, 2014), and Guided back-propagation (Springenberg et al.", "startOffset": 48, "endOffset": 69}, {"referenceID": 18, "context": ", 2016), Deconvolutional networks (Zeiler & Fergus, 2014), and Guided back-propagation (Springenberg et al., 2014).", "startOffset": 87, "endOffset": 114}, {"referenceID": 1, "context": "Gradients (of the output with respect to the input) are a natural analog of these coefficients for a deep network, and therefore is a reasonable starting point for an attribution method (Baehrens et al., 2010; Simonyan et al., 2013); see the third column of Figure 2 for examples.", "startOffset": 186, "endOffset": 232}, {"referenceID": 17, "context": "Gradients (of the output with respect to the input) are a natural analog of these coefficients for a deep network, and therefore is a reasonable starting point for an attribution method (Baehrens et al., 2010; Simonyan et al., 2013); see the third column of Figure 2 for examples.", "startOffset": 186, "endOffset": 232}, {"referenceID": 16, "context": "This phenomenon has been reported in previous work (Shrikumar et al., 2016).", "startOffset": 51, "endOffset": 75}, {"referenceID": 16, "context": "These include DeepLift (Shrikumar et al., 2016), Layer-wise relevance propagation (LRP) (Binder et al.", "startOffset": 23, "endOffset": 47}, {"referenceID": 2, "context": ", 2016), Layer-wise relevance propagation (LRP) (Binder et al., 2016), Deconvolutional networks (DeConvNets) (Zeiler & Fergus, 2014), and Guided backpropagation (Springenberg et al.", "startOffset": 48, "endOffset": 69}, {"referenceID": 18, "context": ", 2016), Deconvolutional networks (DeConvNets) (Zeiler & Fergus, 2014), and Guided backpropagation (Springenberg et al., 2014).", "startOffset": 99, "endOffset": 126}, {"referenceID": 18, "context": "Unfortunately, Deconvolution networks (DeConvNets) (Zeiler & Fergus, 2014), and Guided backpropagation (Springenberg et al., 2014) violate Sensitivity(a).", "startOffset": 103, "endOffset": 130}, {"referenceID": 15, "context": "For instance, in the context of an object recognition task, (Samek et al., 2015) suggests that we select the top k pixels by attribution and randomly vary their intensities and then measure the drop in score.", "startOffset": 60, "endOffset": 80}, {"referenceID": 15, "context": "Pixel ablations: Based on a method proposed by (Samek et al., 2015), we ablate (i.", "startOffset": 47, "endOffset": 67}, {"referenceID": 19, "context": "Recently, a deep network (V et al., 2016) has been proposed to predict the severity grade for DR in retinal fundus images.", "startOffset": 25, "endOffset": 41}, {"referenceID": 19, "context": "The evaluated integrated gradients as part of wider study with retina specilists and found that integrated gradients help the specialist confirm the predicted DR grade on 28 out of 33 images from the EyePACs-1 dataset (V et al., 2016) chosen for diversity.", "startOffset": 218, "endOffset": 234}, {"referenceID": 11, "context": "We apply our technique to word-level language modeling of the Penn Treebank dataset (Marcus et al., 1993), and apply an LSTM-based sequence model based on (Zaremba et al.", "startOffset": 84, "endOffset": 105}, {"referenceID": 21, "context": ", 1993), and apply an LSTM-based sequence model based on (Zaremba et al., 2014).", "startOffset": 57, "endOffset": 79}, {"referenceID": 6, "context": "In particular, we consider a network based on the molecular graph convolution architecture proposed by (Kearnes et al., 2016).", "startOffset": 103, "endOffset": 125}, {"referenceID": 6, "context": "Figure 6: Attribution for a molecule under the W2N2 network (Kearnes et al., 2016).", "startOffset": 60, "endOffset": 82}, {"referenceID": 6, "context": "We now discuss how attributions helped us spot an anomaly in the W1N2 architecture in (Kearnes et al., 2016).", "startOffset": 86, "endOffset": 108}, {"referenceID": 4, "context": "Most of this work has been on networks trained on computer vision tasks, and deals with understanding what a specific neuron computes (Erhan et al., 2009; Le, 2013) and interpreting the representations captured by neurons during a prediction (Mahendran & Vedaldi, 2015; Dosovitskiy & Brox, 2015; Yosinski et al.", "startOffset": 134, "endOffset": 164}, {"referenceID": 20, "context": ", 2009; Le, 2013) and interpreting the representations captured by neurons during a prediction (Mahendran & Vedaldi, 2015; Dosovitskiy & Brox, 2015; Yosinski et al., 2015).", "startOffset": 95, "endOffset": 171}], "year": 2017, "abstractText": "We study the problem of attributing the prediction of a deep network to its input features, a problem previously studied by several other works. We identify two fundamental axioms\u2014 Sensitivity and Implementation Invariance that attribution methods ought to satisfy. We show that they are not satisfied by most known attribution methods, which we consider to be a fundamental weakness of those methods. We use the axioms to guide the design of a new attribution method called Integrated Gradients. Our method requires no modification to the original network and is extremely simple to implement; it just needs a few calls to the standard gradient operator. We apply this method to a couple of image models, a couple of text models and a chemistry model, demonstrating its ability to debug networks, to extract rules from a deep network, and to enable users to engage with models better. 1. Motivation and Summary of Results We study the problem of attributing the prediction of a deep network to its input features. Definition 1. Formally, suppose we have a function F : R \u2192 [0, 1] that represents a deep network, and an input x = (x1, . . . , xn) \u2208 R. An attribution of the prediction at input x relative to a baseline input x\u2032 is a vector AF (x, x \u2032) = (a1, . . . , an) \u2208 R where ai is the contribution of xi to the prediction F (x). For instance, in an object recognition network, an attribution method could tell us which pixels of the image were responsible for a certain label being picked (see Figure 2). The attribution problem was previously studied by various papers (Baehrens et al., 2010; Simonyan et al., 2013; Shrikumar et al., 2016; Binder et al., 2016; Springenberg et al., 2014). The intention of these works is to understand the inputoutput behavior of the deep network, which gives us the ability to improve it. Such understandability is critical to all computer programs, including machine learning models. There are also other applications of attribution. They could be used within a product driven by machine learning to provide a rationale for the recommendation. For instance, a deep network that predicts a condition based on imaging could help inform the doctor of the part of the image that resulted in the recommendation. This could help the doctor understand the strengths and weaknesses of a model and compensate for it. We give such an example in Section 5.2. Attributions could also be used by developers in an exploratory sense. For instance, we could use a deep network to extract insights that could be then used in a rulebased system. In Section 5.3, we give such an example. A significant challenge in designing an attribution technique is that they are hard to evaluate empirically. As we discuss in Section 4, it is hard to tease apart errors that stem from the misbehavior of the model versus the misbehavior of the attribution method. To compensate for this shortcoming, we take an axiomatic approach. In Section 2 we identify two axioms that every attribution method must satisfy. Unfortunately most previous methods do not satisfy one of these two axioms. In Section 3, we use the axioms to identify a new method, called integrated gradients. Unlike previously proposed methods, integrated gradients do not need any instrumentation of the network, and can be computed easily using a few calls to the gradient operaar X iv :1 70 3. 01 36 5v 1 [ cs .L G ] 4 M ar 2 01 7 Axiomatic Attribution for Deep Networks tion, allowing even novice practitioners to easily apply the technique. In Section 5, we demonstrate the ease of applicability over several deep networks, including two images networks, two text processing networks, and a chemistry network. These applications demonstrate the use of our technique in either improving our understanding of the network, performing debugging, performing rule extraction, or aiding an end user in understanding the network\u2019s prediction. Remark 1. Let us briefly examine the need for the baseline in the definition of the attribution problem. A common way for humans to perform attribution relies on counterfactual intuition. When we assign blame to a certain cause we implicitly consider the absence of the cause as a baseline for comparing outcomes. In a deep network, we model the absence using a single baseline input. For most deep networks, a natural baseline exists in the input space where the prediction is neutral. For instance, in object recognition networks, it is the black image. The need for a baseline has also been pointed out by prior work on attribution (Shrikumar et al., 2016; Binder et al., 2016). 2. Two Fundamental Axioms We now discuss two axioms (desirable characteristics) for attribution methods. We find that other feature attribution methods in literature break at least one of the two axioms. These methods include DeepLift (Shrikumar et al., 2016), Layer-wise relevance propagation (LRP) (Binder et al., 2016), Deconvolutional networks (Zeiler & Fergus, 2014), and Guided back-propagation (Springenberg et al., 2014). As we will see in Section 3, these axioms will also guide the design of our method. Gradients. For linear models, ML practitioners regularly inspect coefficients of the model in order to debug it. Gradients (of the output with respect to the input) are a natural analog of these coefficients for a deep network, and therefore is a reasonable starting point for an attribution method (Baehrens et al., 2010; Simonyan et al., 2013); see the third column of Figure 2 for examples. The problem with gradients is that they break sensitivity, a property that all attribution methods should satisfy. 2.1. Axiom: Sensitivity(a) An attribution method satisfies Sensitivity(a) if for every input and baseline that differ in one feature but have different predictions then the differing feature should be given a non-zero attribution. (Later in the paper, we will have a part (b) to this definition.) Gradients violate Sensitivity(a): For a concrete example, consider a one variable, one ReLU network, f(x) = 1 \u2212 ReLU(1\u2212x). Suppose the baseline is x = 0 and the input is x = 2. The function changes from 0 to 1, but because f becomes flat at x = 1, the gradient method gives attribution of 0 to x. Intuitively, gradients break Sensitivity because the prediction function may flatten at the input and thus have zero gradient despite the function value at the input being different from that at the baseline. This phenomenon has been reported in previous work (Shrikumar et al., 2016). Practically, the lack of sensitivity causes gradients to focus on irrelevant features (see the \u201cfireboat\u201d example in Figure 2). Other back-propagation based approaches. A second set of approaches involve back-propagating the final prediction score through each layer of the network down to the individual features. These include DeepLift (Shrikumar et al., 2016), Layer-wise relevance propagation (LRP) (Binder et al., 2016), Deconvolutional networks (DeConvNets) (Zeiler & Fergus, 2014), and Guided backpropagation (Springenberg et al., 2014). These methods differ in the specific backpropagation logic for various activation functions (e.g., ReLU, MaxPool, etc.). Unfortunately, Deconvolution networks (DeConvNets) (Zeiler & Fergus, 2014), and Guided backpropagation (Springenberg et al., 2014) violate Sensitivity(a). This is because these methods back-propogate through a ReLU node only if the ReLU is turned on at the input. This makes the method similar to gradients, in that, the attribution is zero for features with zero gradient at the input despite a non-zero gradient at the baseline. We defer the specific counterexamples to the Appendix B. Methods like DeepLift and LRP tackle the Sensitivity issue by employing a baseline, and in some sense try to compute \u201cdiscrete gradients\u201d instead of (instantaeneous) gradients at the input. (The two methods differ in the specifics of how they compute the discrete gradient). But the idea is that a large, discrete step will avoid flat regions, avoiding a breakage of sensitivity. Unfortunately, these methods violate a different requirement that attribution methods should satisfy. 2.2. Axiom: Implementation Invariance Two networks are functionally equivalent if their outputs are equal for all inputs, despite having very different implementations. Attribution methods should satisfy Implementation Invariance, i.e., the attributions are always identical for two functionally equivalent networks. To motivate this, notice that attribution can be colloquially defined as assigning the blame (or credit) for the output to the input features. Such a definition does not refer to implementation details. We now discuss intuition for why DeepLift and LRP break Axiomatic Attribution for Deep Networks Implementation Invariance; a concrete example is provided in Appendix B. First, notice that gradients are invariant to implementation. In fact, the chain-rule for gradients \u2202f \u2202g = \u2202f \u2202h \u00b7 \u2202h \u2202g is essentially about implementation invariance. To see this, think of g and f as the input and output of a system, and h being some implementation detail of the system. The gradient of output f to input g can be computed either directly by \u2202f \u2202g , ignoring the intermediate function h (implementation detail), or by invoking the chain rule via h. This is exactly how backpropagation works. Methods like LRP and DeepLift replace gradients with discrete gradients and still use a modified form of backpropagation to compose discrete gradients into attributions. 1Unfortunately, the chain rule does not hold for discrete gradients in general. Formally f(x1)\u2212f(x0) g(x1)\u2212g(x0) 6= f(x1)\u2212f(x0) h(x1)\u2212h(x0) \u00b7 h(x1)\u2212h(x0) g(x1)\u2212g(x0) , and therefore these methods fail to satisfy implementation invariance. If an attribution method fails to satisfy Implementation Invariance, the attributions are potentially sensitive to unimportant aspects of the models. For instance, in the example in the appendix, the network architecture has more degrees of freedom than needed for representing the function, and as a result there are two set of values for the network parameters that lead to the same function. The training procedure can converge at either set of values depending on the initializtion or for other reasons, but the underlying network function would remain the same. It is undesirable that attributions differ for such reasons. 3. Our Method: Integrated Gradients We are now ready to describe our technique. Intuitively, our technique combines the Implementation Invariance of Gradients along with the Sensitivity of techniques like LRP or DeepLift. Formally, suppose we have a function F : R \u2192 [0, 1] that represents a deep network. Specifically, let x \u2208 R be the input at hand, and x\u2032 \u2208 R be the baseline input. For image networks, the baseline could be the black image, while for text models it could be the zero embedding vector. We consider the straightline path (in R) from the baseline x\u2032 to the input x, and compute the gradients at all points along the path. Integrated gradients are obtained by cumulating these gradients. Specifically, integrated gradients are defined as the path intergral of the gradients along the straightline path from the baseline x\u2032 to the input x. The integrated gradient along the i dimension for an input x and baseline x\u2032 is defined as follows. IntegratedGradsi(x) ::= (x\u2212x\u2032)\u00d7 \u222b 1 \u03b1=0 \u2202F (x\u2032+\u03b1\u00d7(x\u2212x\u2032)) \u2202xi d\u03b1 (1) where \u2202F (x) \u2202xi is the gradient of F along the i th dimension at x. Axiom: Completeness. Integrated gradients satisfy an axiom called completeness that the attributions add up to the difference between the output of F at the input x and the baseline x\u2032. This axiom is identified as being desirable by Deeplift and LRP. It is a sanity check that the attribution method is somewhat comprehensive in its accounting, a property that is clearly desirable if the networks score is used in a numeric sense, and not just to pick the top label, for e.g., a model estimating insurance premiums from credit features of individuals. This is formalized by the proposition below, which is an instantiation of the fundamental theorem of calculus for path integrals. Proposition 1. If F : R \u2192 R is differentiable almost everywhere 1 then \u03a3i=1IntegratedGradsi(x) = F (x)\u2212 F (x\u2032) For most deep networks, it is possible to choose a baseline such that the prediction at the baseline is near zero (F (x\u2032) \u2248 0). (For image models, the black image baseline indeed satisfies this property.) In such cases, there is an intepretation of the resulting attributions that ignores the baseline and amounts to distributing the output to the individual input features. Remark 2. Integrated Gradients satisfies Sensivity(a) because Completeness implies Sensivity(a) and is thus a strengthening of the Sensitivity(a) axiom. This is because Sensitivity(a) refers to a case where the baseline and the input differ only in one variable, for which Completeness asserts that the difference in the two output values is equal to the attribution to this variable. Attributions generated by integrated gradients satisfy Implementation Invariance since they are based only on the gradients of the function represented by the network. Computing integrated gradients. The integrated gradients can be efficiently approximated by Riemann sum, wherein, we simply sum the gradients at points occurring at sufficiently small intervals along the straightline path from the baseline x\u2032 to the input x. IntegratedGrads i (x) ::= \u03a3 m k=1 \u2202F (x\u2032+ k m\u00d7(x\u2212x \u2032))) \u2202xi \u00d7 1 m (2) Formally, this means that the partial derivative of F along each input dimension satisfies Lebesgue\u2019s integrability condition, i.e., the set of discontinuous points has measure zero. Deep networks built out of Sigmoids, ReLUs, and pooling operators should satisfy this condition. Axiomatic Attribution for Deep Networks Here m is the number of steps in the Riemman approximation of the integral. Notice that the approximation simply involves computing the gradient in a for loop which should be straightforward and efficient in most deep learning frameworks. For instance, in TensorFlow, it essentially amounts to calling tf.gradients in a loop over the set of inputs (i.e., x\u2032+ k m \u00d7 (x\u2212x \u2032) for k = 1, . . . ,m), which could also be batched. In practice, we find that m = 50 calls are more than enough to approximate the integral to within 5%. Going forward, we abuse the term \u201cintegrated gradients\u201d to refer to the approximation described above. 4. Uniqueness of Integrated Gradients Prior literature has relied on empirically evaluating the attribution technique. For instance, in the context of an object recognition task, (Samek et al., 2015) suggests that we select the top k pixels by attribution and randomly vary their intensities and then measure the drop in score. If the attribution method is good, then the drop in score should be large. However, the images resulting from pixel perturbation could be unnatural, and it could be that the scores drop simply because the network has never seen anything like it in training. (This is probably less of a concern with linear or logistic models where the simplicity of the model ensures that ablating a feature does not cause strange interactions.) A different evaluation technique considers images with human-drawn bounding boxes around objects, and computes the percentage of pixel attribution inside the bounding box. While for most objects, one would expect the pixels located on the object to be most important for the prediction, in some cases the context in which the object occurs may also contribute to the prediction. The cabbage butterfly image from Figure 2 is a good example of this where the pixels on the leaf are also surfaced by the integrated gradients. Roughly, we found that every empirical evaluation technique we could think of could not differentiate between artifacts that stem from perturbing the data, a misbehaving model, and a misbehaving attribution method. This was why we turned to an axiomatic approach in designing a good attribution method (Section 2). While our method satisfies Sensitivity and Implementation Invariance, it certainly isn\u2019t the unique method to do so. We now justify the selection of the integrated gradients method in two steps. First, we identify a class of methods called Path methods that generalize integrated gradients. We discuss that path methods are the only methods to satisfy certain desirable axioms. Second, we argue why integrated gradients is somehow canonical among the different path methods. r1,r2 s1,s2", "creator": "LaTeX with hyperref package"}}}